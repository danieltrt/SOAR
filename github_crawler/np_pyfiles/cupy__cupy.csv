file_path,api_count,code
cupy_setup_build.py,0,"b'import argparse\nimport copy\nfrom distutils import ccompiler\nfrom distutils import errors\nfrom distutils import msvccompiler\nfrom distutils import sysconfig\nfrom distutils import unixccompiler\nimport glob\nimport os\nfrom os import path\nimport shutil\nimport sys\n\nimport pkg_resources\nimport setuptools\nfrom setuptools.command import build_ext\nfrom setuptools.command import sdist\n\nfrom install import build\nfrom install.build import PLATFORM_DARWIN\nfrom install.build import PLATFORM_LINUX\nfrom install.build import PLATFORM_WIN32\n\n\nrequired_cython_version = pkg_resources.parse_version(\'0.28.0\')\nignore_cython_versions = [\n]\nuse_hip = bool(int(os.environ.get(\'CUPY_INSTALL_USE_HIP\', \'0\')))\n\n\n# The value of the key \'file\' is a list that contains extension names\n# or tuples of an extension name and a list of other souces files\n# required to build the extension such as .cpp files and .cu files.\n#\n#   <extension name> | (<extension name>, a list of <other source>)\n#\n# The extension name is also interpreted as the name of the Cython\n# source file required to build the extension with appending \'.pyx\'\n# file extension.\nMODULES = []\n\ncuda_files = [\n    \'cupy.core._carray\',\n    \'cupy.core._dtype\',\n    \'cupy.core._kernel\',\n    \'cupy.core._memory_range\',\n    \'cupy.core._optimize_config\',\n    \'cupy.core._reduction\',\n    \'cupy.core._routines_indexing\',\n    \'cupy.core._routines_logic\',\n    \'cupy.core._routines_manipulation\',\n    \'cupy.core._routines_math\',\n    \'cupy.core._routines_sorting\',\n    \'cupy.core._routines_statistics\',\n    \'cupy.core._scalar\',\n    \'cupy.core.core\',\n    \'cupy.core.dlpack\',\n    \'cupy.core.flags\',\n    \'cupy.core.internal\',\n    \'cupy.core.fusion\',\n    \'cupy.core.raw\',\n    \'cupy.cuda.cublas\',\n    \'cupy.cuda.cufft\',\n    \'cupy.cuda.curand\',\n    \'cupy.cuda.cusparse\',\n    \'cupy.cuda.device\',\n    \'cupy.cuda.driver\',\n    \'cupy.cuda.memory\',\n    \'cupy.cuda.memory_hook\',\n    \'cupy.cuda.nvrtc\',\n    \'cupy.cuda.pinned_memory\',\n    \'cupy.cuda.profiler\',\n    \'cupy.cuda.function\',\n    \'cupy.cuda.stream\',\n    \'cupy.cuda.runtime\',\n    \'cupy.cuda.texture\',\n    \'cupy.util\',\n]\n\nif use_hip:\n    MODULES.append({\n        \'name\': \'cuda\',\n        \'file\': cuda_files,\n        \'include\': [\n            \'hip/hip_runtime_api.h\',\n            \'hip/hiprtc.h\',\n            \'hipblas.h\',\n            \'hiprand/hiprand.h\',\n        ],\n        \'libraries\': [\n            \'hiprtc\',\n            \'hip_hcc\',\n            \'hipblas\',\n            \'hiprand\',\n        ],\n    })\nelse:\n    MODULES.append({\n        \'name\': \'cuda\',\n        \'file\': cuda_files,\n        \'include\': [\n            \'cublas_v2.h\',\n            \'cuda.h\',\n            \'cuda_profiler_api.h\',\n            \'cuda_runtime.h\',\n            \'cufft.h\',\n            \'curand.h\',\n            \'cusparse.h\',\n            \'nvrtc.h\',\n        ],\n        \'libraries\': [\n            \'cublas\',\n            \'cuda\',\n            \'cudart\',\n            \'cufft\',\n            \'curand\',\n            \'cusparse\',\n            \'nvrtc\',\n        ],\n        \'check_method\': build.check_cuda_version,\n        \'version_method\': build.get_cuda_version,\n    })\n\nif use_hip:\n    MODULES.append({\n        \'name\': \'cusolver\',\n        \'file\': [\n            \'cupy.cuda.cusolver\',\n        ],\n        \'include\': [],\n        \'libraries\': [],\n    })\nelse:\n    MODULES.append({\n        \'name\': \'cusolver\',\n        \'file\': [\n            \'cupy.cuda.cusolver\',\n        ],\n        \'include\': [\n            \'cusolverDn.h\',\n        ],\n        \'libraries\': [\n            \'cusolver\',\n        ],\n        \'check_method\': build.check_cuda_version,\n    })\n\nif not use_hip:\n    MODULES.append({\n        \'name\': \'cudnn\',\n        \'file\': [\n            \'cupy.cuda.cudnn\',\n            \'cupy.cudnn\',\n        ],\n        \'include\': [\n            \'cudnn.h\',\n        ],\n        \'libraries\': [\n            \'cudnn\',\n        ],\n        \'check_method\': build.check_cudnn_version,\n        \'version_method\': build.get_cudnn_version,\n    })\n\n    MODULES.append({\n        \'name\': \'nccl\',\n        \'file\': [\n            \'cupy.cuda.nccl\',\n        ],\n        \'include\': [\n            \'nccl.h\',\n        ],\n        \'libraries\': [\n            \'nccl\',\n        ],\n        \'check_method\': build.check_nccl_version,\n        \'version_method\': build.get_nccl_version,\n    })\n\n    MODULES.append({\n        \'name\': \'nvtx\',\n        \'file\': [\n            \'cupy.cuda.nvtx\',\n        ],\n        \'include\': [\n            \'nvToolsExt.h\',\n        ],\n        \'libraries\': [\n            \'nvToolsExt\' if not PLATFORM_WIN32 else \'nvToolsExt64_1\',\n        ],\n        \'check_method\': build.check_nvtx,\n    })\n\n    MODULES.append({\n        \'name\': \'cutensor\',\n        \'file\': [\n            \'cupy.cuda.cutensor\',\n        ],\n        \'include\': [\n            \'cutensor.h\',\n        ],\n        \'libraries\': [\n            \'cutensor\',\n            \'cublas\',\n        ],\n        \'check_method\': build.check_cutensor_version,\n        \'version_method\': build.get_cutensor_version,\n    })\n\n    MODULES.append({\n        \'name\': \'cub\',\n        \'file\': [\n            (\'cupy.cuda.cub\', [\'cupy/cuda/cupy_cub.cu\']),\n        ],\n        \'include\': [\n            \'cub/util_namespace.cuh\',  # dummy\n        ],\n        \'libraries\': [\n            \'cudart\',\n        ],\n        \'check_method\': build.check_cuda_version,\n    })\n\nif bool(int(os.environ.get(\'CUPY_SETUP_ENABLE_THRUST\', 1))):\n    if use_hip:\n        MODULES.append({\n            \'name\': \'thrust\',\n            \'file\': [\n                (\'cupy.cuda.thrust\', [\'cupy/cuda/cupy_thrust.cu\']),\n            ],\n            \'include\': [\n                \'thrust/version.h\',\n            ],\n            \'libraries\': [\n                \'hiprtc\',\n                \'hip_hcc\',\n            ],\n        })\n    else:\n        MODULES.append({\n            \'name\': \'thrust\',\n            \'file\': [\n                (\'cupy.cuda.thrust\', [\'cupy/cuda/cupy_thrust.cu\']),\n            ],\n            \'include\': [\n                \'thrust/device_ptr.h\',\n                \'thrust/sequence.h\',\n                \'thrust/sort.h\',\n            ],\n            \'libraries\': [\n                \'cudart\',\n            ],\n            \'check_method\': build.check_cuda_version,\n        })\n\n\ndef ensure_module_file(file):\n    if isinstance(file, tuple):\n        return file\n    else:\n        return file, []\n\n\ndef module_extension_name(file):\n    return ensure_module_file(file)[0]\n\n\ndef module_extension_sources(file, use_cython, no_cuda):\n    pyx, others = ensure_module_file(file)\n    base = path.join(*pyx.split(\'.\'))\n    if use_cython:\n        pyx = base + \'.pyx\'\n        if not os.path.exists(pyx):\n            use_cython = False\n            print(\n                \'NOTICE: Skipping cythonize as {} does not exist.\'.format(pyx))\n    if not use_cython:\n        pyx = base + \'.cpp\'\n\n    # If CUDA SDK is not available, remove CUDA C files from extension sources\n    # and use stubs defined in header files.\n    if no_cuda:\n        others1 = []\n        for source in others:\n            base, ext = os.path.splitext(source)\n            if ext == \'.cu\':\n                continue\n            others1.append(source)\n        others = others1\n\n    return [pyx] + others\n\n\ndef check_readthedocs_environment():\n    return os.environ.get(\'READTHEDOCS\', None) == \'True\'\n\n\ndef check_library(compiler, includes=(), libraries=(),\n                  include_dirs=(), library_dirs=(), define_macros=None,\n                  extra_compile_args=()):\n\n    source = \'\'.join([\'#include <%s>\\n\' % header for header in includes])\n    source += \'int main() {return 0;}\'\n    try:\n        # We need to try to build a shared library because distutils\n        # uses different option to build an executable and a shared library.\n        # Especially when a user build an executable, distutils does not use\n        # LDFLAGS environment variable.\n        build.build_shlib(compiler, source, libraries,\n                          include_dirs, library_dirs, define_macros,\n                          extra_compile_args)\n    except Exception as e:\n        print(e)\n        sys.stdout.flush()\n        return False\n    return True\n\n\ndef preconfigure_modules(compiler, settings):\n    """"""Returns a list of modules buildable in given environment and settings.\n\n    For each module in MODULES list, this function checks if the module\n    can be built in the current environment and reports it.\n    Returns a list of module names available.\n    """"""\n\n    nvcc_path = build.get_nvcc_path()\n    summary = [\n        \'\',\n        \'************************************************************\',\n        \'* CuPy Configuration Summary                               *\',\n        \'************************************************************\',\n        \'\',\n        \'Build Environment:\',\n        \'  Include directories: {}\'.format(str(settings[\'include_dirs\'])),\n        \'  Library directories: {}\'.format(str(settings[\'library_dirs\'])),\n        \'  nvcc command       : {}\'.format(\n            nvcc_path if nvcc_path else \'(not found)\'),\n        \'\',\n        \'Environment Variables:\',\n    ]\n\n    for key in [\'CFLAGS\', \'LDFLAGS\', \'LIBRARY_PATH\',\n                \'CUDA_PATH\', \'NVTOOLSEXT_PATH\', \'NVCC\',\n                \'ROCM_HOME\']:\n        summary += [\'  {:<16}: {}\'.format(key, os.environ.get(key, \'(none)\'))]\n\n    summary += [\n        \'\',\n        \'Modules:\',\n    ]\n\n    ret = []\n    for module in MODULES:\n        installed = False\n        status = \'No\'\n        errmsg = []\n\n        if module[\'name\'] == \'cutensor\':\n            cuda_version = build.get_cuda_version()\n            cuda_version = str(cuda_version // 1000) + \'.\' + \\\n                str((cuda_version // 10) % 100)\n            cutensor_path = os.environ.get(\'CUTENSOR_PATH\', \'\')\n            inc_path = os.path.join(cutensor_path, \'include\')\n            if os.path.exists(inc_path):\n                settings[\'include_dirs\'].append(inc_path)\n            lib_path = os.path.join(cutensor_path, \'lib\', cuda_version)\n            if os.path.exists(lib_path):\n                settings[\'library_dirs\'].append(lib_path)\n\n        print(\'\')\n        print(\'-------- Configuring Module: {} --------\'.format(\n            module[\'name\']))\n        sys.stdout.flush()\n        if not check_library(\n                compiler,\n                includes=module[\'include\'],\n                include_dirs=settings[\'include_dirs\'],\n                define_macros=settings[\'define_macros\'],\n                extra_compile_args=settings[\'extra_compile_args\']):\n            errmsg = [\'Include files not found: %s\' % module[\'include\'],\n                      \'Check your CFLAGS environment variable.\']\n        elif not check_library(\n                compiler,\n                libraries=module[\'libraries\'],\n                library_dirs=settings[\'library_dirs\'],\n                define_macros=settings[\'define_macros\'],\n                extra_compile_args=settings[\'extra_compile_args\']):\n            errmsg = [\'Cannot link libraries: %s\' % module[\'libraries\'],\n                      \'Check your LDFLAGS environment variable.\']\n        elif (\'check_method\' in module and\n                not module[\'check_method\'](compiler, settings)):\n            # Fail on per-library condition check (version requirements etc.)\n            installed = True\n            errmsg = [\'The library is installed but not supported.\']\n        elif module[\'name\'] == \'thrust\' and nvcc_path is None:\n            installed = True\n            errmsg = [\'nvcc command could not be found in PATH.\',\n                      \'Check your PATH environment variable.\']\n        elif module[\'name\'] == \'cub\' and nvcc_path is None:\n            installed = True\n            errmsg = [\'nvcc command could not be found in PATH.\',\n                      \'Check your PATH environment variable.\']\n        else:\n            installed = True\n            status = \'Yes\'\n            ret.append(module[\'name\'])\n\n        if installed and \'version_method\' in module:\n            status += \' (version {})\'.format(module[\'version_method\'](True))\n\n        summary += [\n            \'  {:<10}: {}\'.format(module[\'name\'], status)\n        ]\n\n        # If error message exists...\n        if len(errmsg) != 0:\n            summary += [\'    -> {}\'.format(m) for m in errmsg]\n\n            # Skip checking other modules when CUDA is unavailable.\n            if module[\'name\'] == \'cuda\':\n                break\n\n    if len(ret) != len(MODULES):\n        if \'cuda\' in ret:\n            lines = [\n                \'WARNING: Some modules could not be configured.\',\n                \'CuPy will be installed without these modules.\',\n            ]\n        else:\n            lines = [\n                \'ERROR: CUDA could not be found on your system.\',\n            ]\n        summary += [\n            \'\',\n        ] + lines + [\n            \'Please refer to the Installation Guide for details:\',\n            \'https://docs-cupy.chainer.org/en/stable/install.html\',\n            \'\',\n        ]\n\n    summary += [\n        \'************************************************************\',\n        \'\',\n    ]\n\n    print(\'\\n\'.join(summary))\n    return ret, settings\n\n\ndef _rpath_base():\n    if PLATFORM_LINUX:\n        return \'$ORIGIN\'\n    elif PLATFORM_DARWIN:\n        return \'@loader_path\'\n    else:\n        raise Exception(\'not supported on this platform\')\n\n\ndef make_extensions(options, compiler, use_cython):\n    """"""Produce a list of Extension instances which passed to cythonize().""""""\n\n    no_cuda = options[\'no_cuda\']\n    use_hip = not no_cuda and options[\'use_hip\']\n    settings = build.get_compiler_setting(use_hip)\n\n    include_dirs = settings[\'include_dirs\']\n\n    settings[\'include_dirs\'] = [\n        x for x in include_dirs if path.exists(x)]\n    settings[\'library_dirs\'] = [\n        x for x in settings[\'library_dirs\'] if path.exists(x)]\n\n    # Adjust rpath to use CUDA libraries in `cupy/.data/lib/*.so`) from CuPy.\n    use_wheel_libs_rpath = (\n        0 < len(options[\'wheel_libs\']) and not PLATFORM_WIN32)\n\n    # In the environment with CUDA 7.5 on Ubuntu 16.04, gcc5.3 does not\n    # automatically deal with memcpy because string.h header file has\n    # been changed. This is a workaround for that environment.\n    # See details in the below discussions:\n    # https://github.com/BVLC/caffe/issues/4046\n    # https://groups.google.com/forum/#!topic/theano-users/3ihQYiTRG4E\n    settings[\'define_macros\'].append((\'_FORCE_INLINES\', \'1\'))\n\n    if options[\'linetrace\']:\n        settings[\'define_macros\'].append((\'CYTHON_TRACE\', \'1\'))\n        settings[\'define_macros\'].append((\'CYTHON_TRACE_NOGIL\', \'1\'))\n    if no_cuda:\n        settings[\'define_macros\'].append((\'CUPY_NO_CUDA\', \'1\'))\n    if use_hip:\n        settings[\'define_macros\'].append((\'CUPY_USE_HIP\', \'1\'))\n        settings[\'define_macros\'].append((\'__HIP_PLATFORM_HCC__\', \'1\'))\n\n    available_modules = []\n    if no_cuda:\n        available_modules = [m[\'name\'] for m in MODULES]\n    else:\n        available_modules, settings = preconfigure_modules(compiler, settings)\n        if \'cuda\' not in available_modules:\n            raise Exception(\'Your CUDA environment is invalid. \'\n                            \'Please check above error log.\')\n\n    ret = []\n    for module in MODULES:\n        if module[\'name\'] not in available_modules:\n            continue\n\n        s = settings.copy()\n        if not no_cuda:\n            s[\'libraries\'] = module[\'libraries\']\n\n        compile_args = s.setdefault(\'extra_compile_args\', [])\n        link_args = s.setdefault(\'extra_link_args\', [])\n\n        if module[\'name\'] == \'cusolver\':\n            compile_args = s.setdefault(\'extra_compile_args\', [])\n            link_args = s.setdefault(\'extra_link_args\', [])\n            # openmp is required for cusolver\n            if use_hip:\n                pass\n            elif compiler.compiler_type == \'unix\' and not PLATFORM_DARWIN:\n                # In mac environment, openmp is not required.\n                compile_args.append(\'-fopenmp\')\n                link_args.append(\'-fopenmp\')\n            elif compiler.compiler_type == \'msvc\':\n                compile_args.append(\'/openmp\')\n\n        original_s = s\n        for f in module[\'file\']:\n            s = copy.deepcopy(original_s)\n            name = module_extension_name(f)\n\n            rpath = []\n            if not options[\'no_rpath\']:\n                # Add library directories (e.g., `/usr/local/cuda/lib64`) to\n                # RPATH.\n                rpath += s[\'library_dirs\']\n\n            if use_wheel_libs_rpath:\n                # Add `cupy/.data/lib` (where shared libraries included in\n                # wheels reside) to RPATH.\n                # The path is resolved relative to the module, e.g., use\n                # `$ORIGIN/.data/lib` for `cupy/cudnn.so` and\n                # `$ORIGIN/../.data/lib` for `cupy/cuda/cudnn.so`.\n                depth = name.count(\'.\') - 1\n                rpath.append(\n                    \'{}{}/.data/lib\'.format(_rpath_base(), \'/..\' * depth))\n\n            if not PLATFORM_WIN32 and not PLATFORM_LINUX:\n                s[\'runtime_library_dirs\'] = rpath\n            if (PLATFORM_LINUX and s[\'library_dirs\']) or PLATFORM_DARWIN:\n                ldflag = \'-Wl,\'\n                if PLATFORM_LINUX:\n                    ldflag += \'--disable-new-dtags,\'\n                ldflag += \',\'.join(\'-rpath,\' + p for p in rpath)\n                args = s.setdefault(\'extra_link_args\', [])\n                args.append(ldflag)\n                if PLATFORM_DARWIN:\n                    # -rpath is only supported when targeting Mac OS X 10.5 or\n                    # later\n                    args.append(\'-mmacosx-version-min=10.5\')\n\n            sources = module_extension_sources(f, use_cython, no_cuda)\n            extension = setuptools.Extension(name, sources, **s)\n            ret.append(extension)\n\n    return ret\n\n\n# TODO(oktua): use enviriment variable\ndef parse_args():\n    parser = argparse.ArgumentParser(add_help=False)\n\n    parser.add_argument(\n        \'--cupy-package-name\', type=str, default=\'cupy\',\n        help=\'alternate package name\')\n    parser.add_argument(\n        \'--cupy-long-description\', type=str, default=None,\n        help=\'path to the long description file\')\n    parser.add_argument(\n        \'--cupy-wheel-lib\', type=str, action=\'append\', default=[],\n        help=\'shared library to copy into the wheel \'\n             \'(can be specified for multiple times)\')\n    parser.add_argument(\n        \'--cupy-wheel-include\', type=str, action=\'append\', default=[],\n        help=\'An include file to copy into the wheel. \'\n             \'Delimited by a colon. \'\n             \'The former part is a full path of the source include file and \'\n             \'the latter is the relative path within cupy wheel. \'\n             \'(can be specified for multiple times)\')\n    parser.add_argument(\n        \'--cupy-no-rpath\', action=\'store_true\', default=False,\n        help=\'disable adding default library directories to RPATH\')\n    parser.add_argument(\n        \'--cupy-profile\', action=\'store_true\', default=False,\n        help=\'enable profiling for Cython code\')\n    parser.add_argument(\n        \'--cupy-coverage\', action=\'store_true\', default=False,\n        help=\'enable coverage for Cython code\')\n    parser.add_argument(\n        \'--cupy-no-cuda\', action=\'store_true\', default=False,\n        help=\'build CuPy with stub header file\')\n    # parser.add_argument(\n    #     \'--cupy-use-hip\', action=\'store_true\', default=False,\n    #     help=\'build CuPy with HIP\')\n\n    opts, sys.argv = parser.parse_known_args(sys.argv)\n\n    arg_options = {\n        \'package_name\': opts.cupy_package_name,\n        \'long_description\': opts.cupy_long_description,\n        \'wheel_libs\': opts.cupy_wheel_lib,  # list\n        \'wheel_includes\': opts.cupy_wheel_include,  # list\n        \'no_rpath\': opts.cupy_no_rpath,\n        \'profile\': opts.cupy_profile,\n        \'linetrace\': opts.cupy_coverage,\n        \'annotate\': opts.cupy_coverage,\n        \'no_cuda\': opts.cupy_no_cuda,\n        \'use_hip\': use_hip  # opts.cupy_use_hip,\n    }\n    if check_readthedocs_environment():\n        arg_options[\'no_cuda\'] = True\n    return arg_options\n\n\ncupy_setup_options = parse_args()\nprint(\'Options:\', cupy_setup_options)\n\n\ndef get_package_name():\n    return cupy_setup_options[\'package_name\']\n\n\ndef get_long_description():\n    path = cupy_setup_options[\'long_description\']\n    if path is None:\n        return None\n    with open(path) as f:\n        return f.read()\n\n\ndef prepare_wheel_libs():\n    """"""Prepare shared libraries and include files for wheels.\n\n    On Windows, DLLs will be placed under `cupy/cuda`.\n    On other platforms, shared libraries are placed under `cupy/.data/lib` and\n    RUNPATH will be set to this directory later.\n    Include files are placed under `cupy/.data/include`.\n    """"""\n    data_dir = \'.data\'\n    if os.path.exists(data_dir):\n        print(\'Removing directory: {}\'.format(data_dir))\n        shutil.rmtree(data_dir)\n\n    if PLATFORM_WIN32:\n        lib_dirname = \'cuda\'\n        # Clean up existing libraries.\n        libfiles = glob.glob(\'cupy/{}/*.dll\'.format(lib_dirname))\n        for libfile in libfiles:\n            print(\'Removing file: {}\'.format(libfile))\n            os.remove(libfile)\n    else:\n        lib_dirname = os.path.join(data_dir, \'lib\')\n\n    include_dirname = os.path.join(data_dir, \'include\')\n\n    # Collect files to copy\n    files_to_copy = []\n\n    # Library files\n    lib_base_path = os.path.join(\'cupy\', lib_dirname)\n    for srcpath in cupy_setup_options[\'wheel_libs\']:\n        relpath = os.path.basename(srcpath)\n        dstpath = path.join(lib_base_path, relpath)\n        files_to_copy.append((\n            srcpath,\n            dstpath,\n            path.join(lib_dirname, relpath)))\n\n    # Include files\n    include_base_path = os.path.join(\'cupy\', include_dirname)\n    for include_path_spec in cupy_setup_options[\'wheel_includes\']:\n        # TODO(niboshi): Consider using platform-dependent path delimiter.\n        srcpath, relpath = include_path_spec.rsplit(\':\', 1)\n        dstpath = os.path.join(include_base_path, relpath)\n        files_to_copy.append((\n            srcpath,\n            dstpath,\n            path.join(include_dirname, relpath)))\n\n    # Copy\n    package_data = []\n    for srcpath, dstpath, package_path in files_to_copy:\n        # Note: symlink is resolved by shutil.copy2.\n        print(\'Copying file for wheel: {}\'.format(srcpath))\n        dirpath = os.path.dirname(dstpath)\n        if not os.path.isdir(dirpath):\n            os.makedirs(dirpath)\n        shutil.copy2(srcpath, dstpath)\n        package_data.append(package_path)\n\n    return package_data\n\n\ntry:\n    import Cython\n    import Cython.Build\n    cython_version = pkg_resources.parse_version(Cython.__version__)\n    cython_available = (\n        cython_version >= required_cython_version and\n        cython_version not in ignore_cython_versions)\nexcept ImportError:\n    cython_available = False\n\n\ndef cythonize(extensions, arg_options):\n    directive_keys = (\'linetrace\', \'profile\')\n    directives = {key: arg_options[key] for key in directive_keys}\n\n    # Embed signatures for Sphinx documentation.\n    directives[\'embedsignature\'] = True\n\n    cythonize_option_keys = (\'annotate\',)\n    cythonize_options = {key: arg_options[key]\n                         for key in cythonize_option_keys}\n\n    return Cython.Build.cythonize(\n        extensions, verbose=True, language_level=3,\n        compiler_directives=directives, **cythonize_options)\n\n\ndef check_extensions(extensions):\n    for x in extensions:\n        for f in x.sources:\n            if not path.isfile(f):\n                raise RuntimeError(\'\'\'\\\nMissing file: {}\nPlease install Cython {} or later. Please also check the version of Cython.\nSee https://docs-cupy.chainer.org/en/stable/install.html for details.\n\'\'\'.format(f, required_cython_version))\n\n\ndef get_ext_modules(use_cython=False):\n    arg_options = cupy_setup_options\n\n    # We need to call get_config_vars to initialize _config_vars in distutils\n    # see #1849\n    sysconfig.get_config_vars()\n    compiler = ccompiler.new_compiler()\n    sysconfig.customize_compiler(compiler)\n\n    extensions = make_extensions(arg_options, compiler, use_cython)\n\n    return extensions\n\n\ndef _nvcc_gencode_options(cuda_version):\n    """"""Returns NVCC GPU code generation options.""""""\n\n    if sys.argv == [\'setup.py\', \'develop\']:\n        return []\n\n    envcfg = os.getenv(\'CUPY_NVCC_GENERATE_CODE\', None)\n    if envcfg:\n        return [\'--generate-code={}\'.format(arch)\n                for arch in envcfg.split(\';\') if len(arch) > 0]\n\n    # The arch_list specifies virtual architectures, such as \'compute_61\', and\n    # real architectures, such as \'sm_61\', for which the CUDA input files are\n    # to be compiled.\n    #\n    # The syntax of an entry of the list is\n    #\n    #     entry ::= virtual_arch | (virtual_arch, real_arch)\n    #\n    # where virtual_arch is a string which means a virtual architecture and\n    # real_arch is a string which means a real architecture.\n    #\n    # If a virtual architecture is supplied, NVCC generates a PTX code for the\n    # virtual architecture. If a pair of a virtual architecture and a real\n    # architecture is supplied, NVCC generates a PTX code for the virtual\n    # architecture as well as a cubin code for the real architecture.\n    #\n    # For example, making NVCC generate a PTX code for \'compute_60\' virtual\n    # architecture, the arch_list has an entry of \'compute_60\'.\n    #\n    #     arch_list = [\'compute_60\']\n    #\n    # For another, making NVCC generate a PTX code for \'compute_61\' virtual\n    # architecture and a cubin code for \'sm_61\' real architecture, the\n    # arch_list has an entry of (\'compute_61\', \'sm_61\').\n    #\n    #     arch_list = [(\'compute_61\', \'sm_61\')]\n\n    arch_list = [\'compute_30\',\n                 \'compute_50\']\n\n    if cuda_version >= 10000:\n        arch_list += [(\'compute_60\', \'sm_60\'),\n                      (\'compute_61\', \'sm_61\'),\n                      (\'compute_70\', \'sm_70\'),\n                      (\'compute_75\', \'sm_75\'),\n                      \'compute_70\']\n    elif cuda_version >= 9000:\n        arch_list += [(\'compute_60\', \'sm_60\'),\n                      (\'compute_61\', \'sm_61\'),\n                      (\'compute_70\', \'sm_70\'),\n                      \'compute_70\']\n    elif cuda_version >= 8000:\n        arch_list += [(\'compute_60\', \'sm_60\'),\n                      (\'compute_61\', \'sm_61\'),\n                      \'compute_60\']\n\n    options = []\n    for arch in arch_list:\n        if type(arch) is tuple:\n            virtual_arch, real_arch = arch\n            options.append(\'--generate-code=arch={},code={}\'.format(\n                virtual_arch, real_arch))\n        else:\n            options.append(\'--generate-code=arch={},code={}\'.format(\n                arch, arch))\n\n    return options\n\n\nclass _UnixCCompiler(unixccompiler.UnixCCompiler):\n    src_extensions = list(unixccompiler.UnixCCompiler.src_extensions)\n    src_extensions.append(\'.cu\')\n\n    def _compile(self, obj, src, ext, cc_args, extra_postargs, pp_opts):\n        # For sources other than CUDA C ones, just call the super class method.\n        if os.path.splitext(src)[1] != \'.cu\':\n            return unixccompiler.UnixCCompiler._compile(\n                self, obj, src, ext, cc_args, extra_postargs, pp_opts)\n\n        if use_hip:\n            return self._comiple_unix_hipcc(\n                obj, src, ext, cc_args, extra_postargs, pp_opts)\n\n        # For CUDA C source files, compile them with NVCC.\n        _compiler_so = self.compiler_so\n        try:\n            nvcc_path = build.get_nvcc_path()\n            base_opts = build.get_compiler_base_options()\n            self.set_executable(\'compiler_so\', nvcc_path)\n\n            cuda_version = build.get_cuda_version()\n            postargs = _nvcc_gencode_options(cuda_version) + [\n                \'-O2\', \'--compiler-options=""-fPIC""\', \'--std=c++11\']\n            print(\'NVCC options:\', postargs)\n\n            return unixccompiler.UnixCCompiler._compile(\n                self, obj, src, ext, base_opts + cc_args, postargs, pp_opts)\n        finally:\n            self.compiler_so = _compiler_so\n\n    def _comiple_unix_hipcc(self,\n                            obj, src, ext, cc_args, extra_postargs, pp_opts):\n        # For CUDA C source files, compile them with HIPCC.\n        _compiler_so = self.compiler_so\n        try:\n            rcom_path = build.get_hipcc_path()\n            base_opts = build.get_compiler_base_options()\n            self.set_executable(\'compiler_so\', rcom_path)\n\n            postargs = [\'-O2\', \'-fPIC\']\n            print(\'HIPCC options:\', postargs)\n\n            return unixccompiler.UnixCCompiler._compile(\n                self, obj, src, ext, base_opts + cc_args, postargs, pp_opts)\n        finally:\n            self.compiler_so = _compiler_so\n\n    def link(self, target_desc, objects, output_filename, *args):\n        use_hipcc = False\n        if use_hip:\n            for i in objects:\n                if \'cupy_thrust.o\' in i:\n                    use_hipcc = True\n        if use_hipcc:\n            _compiler_cxx = self.compiler_cxx\n            try:\n                rcom_path = build.get_hipcc_path()\n                self.set_executable(\'compiler_cxx\', rcom_path)\n\n                return unixccompiler.UnixCCompiler.link(\n                    self, target_desc, objects, output_filename, *args)\n            finally:\n                self.compiler_cxx = _compiler_cxx\n        else:\n            return unixccompiler.UnixCCompiler.link(\n                self, target_desc, objects, output_filename, *args)\n\n\nclass _MSVCCompiler(msvccompiler.MSVCCompiler):\n    _cu_extensions = [\'.cu\']\n\n    src_extensions = list(unixccompiler.UnixCCompiler.src_extensions)\n    src_extensions.extend(_cu_extensions)\n\n    def _compile_cu(self, sources, output_dir=None, macros=None,\n                    include_dirs=None, debug=0, extra_preargs=None,\n                    extra_postargs=None, depends=None):\n        # Compile CUDA C files, mainly derived from UnixCCompiler._compile().\n\n        macros, objects, extra_postargs, pp_opts, _build = \\\n            self._setup_compile(output_dir, macros, include_dirs, sources,\n                                depends, extra_postargs)\n\n        compiler_so = build.get_nvcc_path()\n        cc_args = self._get_cc_args(pp_opts, debug, extra_preargs)\n        cuda_version = build.get_cuda_version()\n        postargs = _nvcc_gencode_options(cuda_version) + [\'-O2\']\n        postargs += [\'-Xcompiler\', \'/MD\']\n        print(\'NVCC options:\', postargs)\n\n        for obj in objects:\n            try:\n                src, ext = _build[obj]\n            except KeyError:\n                continue\n            try:\n                self.spawn(compiler_so + cc_args + [src, \'-o\', obj] + postargs)\n            except errors.DistutilsExecError as e:\n                raise errors.CompileError(str(e))\n\n        return objects\n\n    def compile(self, sources, **kwargs):\n        # Split CUDA C sources and others.\n        cu_sources = []\n        other_sources = []\n        for source in sources:\n            if os.path.splitext(source)[1] == \'.cu\':\n                cu_sources.append(source)\n            else:\n                other_sources.append(source)\n\n        # Compile source files other than CUDA C ones.\n        other_objects = msvccompiler.MSVCCompiler.compile(\n            self, other_sources, **kwargs)\n\n        # Compile CUDA C sources.\n        cu_objects = self._compile_cu(cu_sources, **kwargs)\n\n        # Return compiled object filenames.\n        return other_objects + cu_objects\n\n\nclass sdist_with_cython(sdist.sdist):\n\n    """"""Custom `sdist` command with cyhonizing.""""""\n\n    def __init__(self, *args, **kwargs):\n        if not cython_available:\n            raise RuntimeError(\'Cython is required to make sdist.\')\n        ext_modules = get_ext_modules(True)  # get .pyx modules\n        cythonize(ext_modules, cupy_setup_options)\n        sdist.sdist.__init__(self, *args, **kwargs)\n\n\nclass custom_build_ext(build_ext.build_ext):\n\n    """"""Custom `build_ext` command to include CUDA C source files.""""""\n\n    def run(self):\n        if build.get_nvcc_path() is not None:\n            def wrap_new_compiler(func):\n                def _wrap_new_compiler(*args, **kwargs):\n                    try:\n                        return func(*args, **kwargs)\n                    except errors.DistutilsPlatformError:\n                        if not PLATFORM_WIN32:\n                            CCompiler = _UnixCCompiler\n                        else:\n                            CCompiler = _MSVCCompiler\n                        return CCompiler(\n                            None, kwargs[\'dry_run\'], kwargs[\'force\'])\n                return _wrap_new_compiler\n            ccompiler.new_compiler = wrap_new_compiler(ccompiler.new_compiler)\n            # Intentionally causes DistutilsPlatformError in\n            # ccompiler.new_compiler() function to hook.\n            self.compiler = \'nvidia\'\n        if cython_available:\n            ext_modules = get_ext_modules(True)  # get .pyx modules\n            cythonize(ext_modules, cupy_setup_options)\n        check_extensions(self.extensions)\n        build_ext.build_ext.run(self)\n'"
setup.py,0,"b'#!/usr/bin/env python\n\nimport os\nfrom setuptools import setup\nimport sys\n\nimport cupy_setup_build\n\n\nif sys.version_info[:3] == (3, 5, 0):\n    if not int(os.getenv(\'CUPY_PYTHON_350_FORCE\', \'0\')):\n        msg = """"""\nCuPy does not work with Python 3.5.0.\n\nWe strongly recommend to use another version of Python.\nIf you want to use CuPy with Python 3.5.0 at your own risk,\nset 1 to CUPY_PYTHON_350_FORCE environment variable.""""""\n        print(msg)\n        sys.exit(1)\n\n\nrequirements = {\n    \'setup\': [\n        \'fastrlock>=0.3\',\n    ],\n    \'install\': [\n        \'numpy>=1.15\',\n        \'fastrlock>=0.3\',\n    ],\n    \'stylecheck\': [\n        \'autopep8==1.3.5\',\n        \'flake8==3.5.0\',\n        \'pbr==4.0.4\',\n        \'pycodestyle==2.3.1\',\n    ],\n    \'test\': [\n        \'pytest<4.2.0\',  # 4.2.0 is slow collecting tests and times out on CI.\n        \'attrs<19.2.0\',  # pytest 4.1.1 does not run with attrs==19.2.0\n        \'mock\',\n    ],\n    \'doctest\': [\n        \'matplotlib\',\n        \'theano\',\n    ],\n    \'docs\': [\n        \'sphinx==3.0.4\',\n        \'sphinx_rtd_theme\',\n    ],\n    \'travis\': [\n        \'-r stylecheck\',\n        \'-r docs\',\n    ],\n    \'appveyor\': [\n        \'-r test\',\n    ],\n    \'jenkins\': [\n        \'-r test\',\n        \'pytest-timeout\',\n        \'pytest-cov\',\n        \'coveralls\',\n        \'codecov\',\n    ],\n}\n\n\ndef reduce_requirements(key):\n    # Resolve recursive requirements notation (-r)\n    reqs = requirements[key]\n    resolved_reqs = []\n    for req in reqs:\n        if req.startswith(\'-r\'):\n            depend_key = req[2:].lstrip()\n            reduce_requirements(depend_key)\n            resolved_reqs += requirements[depend_key]\n        else:\n            resolved_reqs.append(req)\n    requirements[key] = resolved_reqs\n\n\nfor k in requirements.keys():\n    reduce_requirements(k)\n\n\nextras_require = {k: v for k, v in requirements.items() if k != \'install\'}\n\n\nsetup_requires = requirements[\'setup\']\ninstall_requires = requirements[\'install\']\ntests_require = requirements[\'test\']\n\n\npackage_data = {\n    \'cupy\': [\n        \'core/include/cupy/complex/arithmetic.h\',\n        \'core/include/cupy/complex/catrig.h\',\n        \'core/include/cupy/complex/catrigf.h\',\n        \'core/include/cupy/complex/ccosh.h\',\n        \'core/include/cupy/complex/ccoshf.h\',\n        \'core/include/cupy/complex/cexp.h\',\n        \'core/include/cupy/complex/cexpf.h\',\n        \'core/include/cupy/complex/clog.h\',\n        \'core/include/cupy/complex/clogf.h\',\n        \'core/include/cupy/complex/complex.h\',\n        \'core/include/cupy/complex/complex_inl.h\',\n        \'core/include/cupy/complex/cpow.h\',\n        \'core/include/cupy/complex/cproj.h\',\n        \'core/include/cupy/complex/csinh.h\',\n        \'core/include/cupy/complex/csinhf.h\',\n        \'core/include/cupy/complex/csqrt.h\',\n        \'core/include/cupy/complex/csqrtf.h\',\n        \'core/include/cupy/complex/ctanh.h\',\n        \'core/include/cupy/complex/ctanhf.h\',\n        \'core/include/cupy/complex/math_private.h\',\n        \'core/include/cupy/carray.cuh\',\n        \'core/include/cupy/complex.cuh\',\n        \'core/include/cupy/atomics.cuh\',\n        \'core/include/cupy/cuComplex_bridge.h\',\n        \'core/include/cupy/_cuda/cuda-*/*.h\',\n        \'core/include/cupy/_cuda/cuda-*/*.hpp\',\n        \'cuda/cupy_thrust.cu\',\n    ],\n}\n\npackage_data[\'cupy\'] += cupy_setup_build.prepare_wheel_libs()\n\npackage_name = cupy_setup_build.get_package_name()\nlong_description = cupy_setup_build.get_long_description()\next_modules = cupy_setup_build.get_ext_modules()\nbuild_ext = cupy_setup_build.custom_build_ext\nsdist = cupy_setup_build.sdist_with_cython\n\nhere = os.path.abspath(os.path.dirname(__file__))\n# Get __version__ variable\nexec(open(os.path.join(here, \'cupy\', \'_version.py\')).read())\n\nCLASSIFIERS = """"""\\\nDevelopment Status :: 5 - Production/Stable\nIntended Audience :: Science/Research\nIntended Audience :: Developers\nLicense :: OSI Approved :: MIT License\nProgramming Language :: Python\nProgramming Language :: Python :: 3\nProgramming Language :: Python :: 3.5\nProgramming Language :: Python :: 3.6\nProgramming Language :: Python :: 3.7\nProgramming Language :: Python :: 3 :: Only\nProgramming Language :: Cython\nTopic :: Software Development\nTopic :: Scientific/Engineering\nOperating System :: Microsoft :: Windows\nOperating System :: POSIX\nOperating System :: MacOS\n""""""\n\n\nsetup(\n    name=package_name,\n    version=__version__,  # NOQA\n    description=\'CuPy: NumPy-like API accelerated with CUDA\',\n    long_description=long_description,\n    author=\'Seiya Tokui\',\n    author_email=\'tokui@preferred.jp\',\n    url=\'https://cupy.chainer.org/\',\n    license=\'MIT License\',\n    project_urls={\n        ""Bug Tracker"": ""https://github.com/cupy/cupy/issues"",\n        ""Documentation"": ""https://docs-cupy.chainer.org/"",\n        ""Source Code"": ""https://github.com/cupy/cupy"",\n    },\n    classifiers=[_f for _f in CLASSIFIERS.split(\'\\n\') if _f],\n    packages=[\n        \'cupy\',\n        \'cupy.binary\',\n        \'cupy.core\',\n        \'cupy.creation\',\n        \'cupy.cuda\',\n        \'cupy.cuda.memory_hooks\',\n        \'cupy.fft\',\n        \'cupy.indexing\',\n        \'cupy.io\',\n        \'cupy.lib\',\n        \'cupy.linalg\',\n        \'cupy.logic\',\n        \'cupy.manipulation\',\n        \'cupy.math\',\n        \'cupy.misc\',\n        \'cupy.padding\',\n        \'cupy.prof\',\n        \'cupy.random\',\n        \'cupy._sorting\',\n        \'cupy.sparse\',\n        \'cupy.sparse.linalg\',\n        \'cupy.statistics\',\n        \'cupy.testing\',\n        \'cupyx\',\n        \'cupyx.fallback_mode\',\n        \'cupyx.scipy\',\n        \'cupyx.scipy.fft\',\n        \'cupyx.scipy.fftpack\',\n        \'cupyx.scipy.ndimage\',\n        \'cupyx.scipy.sparse\',\n        \'cupyx.scipy.sparse.linalg\',\n        \'cupyx.scipy.special\',\n        \'cupyx.scipy.linalg\',\n        \'cupyx.linalg\',\n        \'cupyx.linalg.sparse\'\n    ],\n    package_data=package_data,\n    zip_safe=False,\n    python_requires=\'>=3.5.0\',\n    setup_requires=setup_requires,\n    install_requires=install_requires,\n    tests_require=tests_require,\n    extras_require=extras_require,\n    ext_modules=ext_modules,\n    cmdclass={\'build_ext\': build_ext,\n              \'sdist\': sdist},\n)\n'"
cupy/__init__.py,0,"b'import functools\nimport sys\nimport warnings\n\nimport numpy\n\nfrom cupy import _environment\nfrom cupy import _version\n\n\nif sys.platform.startswith(\'win32\') and (3, 8) <= sys.version_info:  # NOQA\n    _environment._setup_win32_dll_directory()  # NOQA\n\n\ntry:\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\'ignore\', category=ImportWarning,\n                                message=\'can\\\'t resolve package from __spec__\')\n        from cupy import core  # NOQA\nexcept ImportError as e:\n    # core is a c-extension module.\n    # When a user cannot import core, it represents that CuPy is not correctly\n    # built.\n    exc_info = sys.exc_info()\n    msg = (\'\'\'\\\nCuPy is not correctly installed.\n\nIf you are using wheel distribution (cupy-cudaXX), make sure that the version of CuPy you installed matches with the version of CUDA on your host.\nAlso, confirm that only one CuPy package is installed:\n  $ pip freeze\n\nIf you are building CuPy from source, please check your environment, uninstall CuPy and reinstall it with:\n  $ pip install cupy --no-cache-dir -vvvv\n\nCheck the Installation Guide for details:\n  https://docs-cupy.chainer.org/en/latest/install.html\n\noriginal error: {}\'\'\'.format(exc_info[1]))  # NOQA\n\n    raise ImportError(msg) from e\n\n\nfrom cupy import cuda\n# Do not make `cupy.cupyx` available because it is confusing.\nimport cupyx as _cupyx\n\n\ndef is_available():\n    return cuda.is_available()\n\n\n__version__ = _version.__version__\n\n\nfrom cupy import binary  # NOQA\nimport cupy.core.fusion  # NOQA\nfrom cupy import creation  # NOQA\nfrom cupy import fft  # NOQA\nfrom cupy import indexing  # NOQA\nfrom cupy import io  # NOQA\nfrom cupy import linalg  # NOQA\nfrom cupy import manipulation  # NOQA\nfrom cupy import padding  # NOQA\nfrom cupy import random  # NOQA\nfrom cupy import _sorting  # NOQA\nfrom cupy import sparse  # NOQA\nfrom cupy import statistics  # NOQA\nfrom cupy import testing  # NOQA  # NOQA\nfrom cupy import util  # NOQA\nfrom cupy import lib  # NOQA\n\n\n# import class and function\nfrom cupy.core import ndarray  # NOQA\nfrom cupy.core import ufunc  # NOQA\n\n\n# =============================================================================\n# Constants (borrowed from NumPy)\n# =============================================================================\nfrom numpy import e  # NOQA\nfrom numpy import euler_gamma  # NOQA\nfrom numpy import Inf  # NOQA\nfrom numpy import inf  # NOQA\nfrom numpy import Infinity  # NOQA\nfrom numpy import infty  # NOQA\nfrom numpy import NAN  # NOQA\nfrom numpy import NaN  # NOQA\nfrom numpy import nan  # NOQA\nfrom numpy import newaxis  # == None  # NOQA\nfrom numpy import NINF  # NOQA\nfrom numpy import NZERO  # NOQA\nfrom numpy import pi  # NOQA\nfrom numpy import PINF  # NOQA\nfrom numpy import PZERO  # NOQA\n\n\n# =============================================================================\n# Data types (borrowed from NumPy)\n#\n# The order of these declarations are borrowed from the NumPy document:\n# https://docs.scipy.org/doc/numpy/reference/arrays.scalars.html\n# =============================================================================\n\n# -----------------------------------------------------------------------------\n# Generic types\n# -----------------------------------------------------------------------------\nfrom numpy import complexfloating  # NOQA\nfrom numpy import floating  # NOQA\nfrom numpy import generic  # NOQA\nfrom numpy import inexact  # NOQA\nfrom numpy import integer  # NOQA\nfrom numpy import number  # NOQA\nfrom numpy import signedinteger  # NOQA\nfrom numpy import unsignedinteger  # NOQA\n\n# Not supported by CuPy:\n# from numpy import flexible\n# from numpy import character\n\n# -----------------------------------------------------------------------------\n# Booleans\n# -----------------------------------------------------------------------------\nfrom numpy import bool_  # NOQA\n\nfrom numpy import bool8  # NOQA\n\n# -----------------------------------------------------------------------------\n# Integers\n# -----------------------------------------------------------------------------\nfrom numpy import byte  # NOQA\n\nfrom numpy import short  # NOQA\n\nfrom numpy import intc  # NOQA\n\nfrom numpy import int_  # NOQA\n\nfrom numpy import longlong  # NOQA\n\nfrom numpy import intp  # NOQA\n\nfrom numpy import int8  # NOQA\n\nfrom numpy import int16  # NOQA\n\nfrom numpy import int32  # NOQA\n\nfrom numpy import int64  # NOQA\n\n# -----------------------------------------------------------------------------\n# Unsigned integers\n# -----------------------------------------------------------------------------\nfrom numpy import ubyte  # NOQA\n\nfrom numpy import ushort  # NOQA\n\nfrom numpy import uintc  # NOQA\n\nfrom numpy import uint  # NOQA\n\nfrom numpy import ulonglong  # NOQA\n\nfrom numpy import uintp  # NOQA\n\nfrom numpy import uint8  # NOQA\n\nfrom numpy import uint16  # NOQA\n\nfrom numpy import uint32  # NOQA\n\nfrom numpy import uint64  # NOQA\n\n# -----------------------------------------------------------------------------\n# Floating-point numbers\n# -----------------------------------------------------------------------------\nfrom numpy import half  # NOQA\n\nfrom numpy import single  # NOQA\n\nfrom numpy import double  # NOQA\n\nfrom numpy import float_  # NOQA\n\nfrom numpy import longfloat  # NOQA\n\nfrom numpy import float16  # NOQA\n\nfrom numpy import float32  # NOQA\n\nfrom numpy import float64  # NOQA\n\n# Not supported by CuPy:\n# from numpy import float96\n# from numpy import float128\n\n# -----------------------------------------------------------------------------\n# Complex floating-point numbers\n# -----------------------------------------------------------------------------\nfrom numpy import csingle  # NOQA\n\nfrom numpy import complex_  # NOQA\n\nfrom numpy import complex64  # NOQA\n\nfrom numpy import complex128  # NOQA\n\n# Not supported by CuPy:\n# from numpy import complex192\n# from numpy import complex256\n# from numpy import clongfloat\n\n# -----------------------------------------------------------------------------\n# Any Python object\n# -----------------------------------------------------------------------------\n\n# Not supported by CuPy:\n# from numpy import object_\n# from numpy import bytes_\n# from numpy import unicode_\n# from numpy import void\n\n# -----------------------------------------------------------------------------\n# Built-in Python types\n# -----------------------------------------------------------------------------\n\nfrom numpy import int  # NOQA\n\nfrom numpy import bool  # NOQA\n\nfrom numpy import float  # NOQA\n\nfrom numpy import complex  # NOQA\n\n# Not supported by CuPy:\n# from numpy import object\n# from numpy import unicode\n# from numpy import str\n\n# =============================================================================\n# Routines\n#\n# The order of these declarations are borrowed from the NumPy document:\n# https://docs.scipy.org/doc/numpy/reference/routines.html\n# =============================================================================\n\n# -----------------------------------------------------------------------------\n# Array creation routines\n# -----------------------------------------------------------------------------\nfrom cupy.creation.basic import empty  # NOQA\nfrom cupy.creation.basic import empty_like  # NOQA\nfrom cupy.creation.basic import eye  # NOQA\nfrom cupy.creation.basic import full  # NOQA\nfrom cupy.creation.basic import full_like  # NOQA\nfrom cupy.creation.basic import identity  # NOQA\nfrom cupy.creation.basic import ones  # NOQA\nfrom cupy.creation.basic import ones_like  # NOQA\nfrom cupy.creation.basic import zeros  # NOQA\nfrom cupy.creation.basic import zeros_like  # NOQA\n\nfrom cupy.creation.from_data import copy  # NOQA\nfrom cupy.creation.from_data import array  # NOQA\nfrom cupy.creation.from_data import asanyarray  # NOQA\nfrom cupy.creation.from_data import asarray  # NOQA\nfrom cupy.creation.from_data import ascontiguousarray  # NOQA\nfrom cupy.creation.from_data import fromfile  # NOQA\n\nfrom cupy.creation.ranges import arange  # NOQA\nfrom cupy.creation.ranges import linspace  # NOQA\nfrom cupy.creation.ranges import logspace  # NOQA\nfrom cupy.creation.ranges import meshgrid  # NOQA\nfrom cupy.creation.ranges import mgrid  # NOQA\nfrom cupy.creation.ranges import ogrid  # NOQA\n\nfrom cupy.creation.matrix import diag  # NOQA\nfrom cupy.creation.matrix import diagflat  # NOQA\nfrom cupy.creation.matrix import tri  # NOQA\nfrom cupy.creation.matrix import tril  # NOQA\nfrom cupy.creation.matrix import triu  # NOQA\n\n# -----------------------------------------------------------------------------\n# Array manipulation routines\n# -----------------------------------------------------------------------------\nfrom cupy.manipulation.basic import copyto  # NOQA\n\nfrom cupy.manipulation.shape import ravel  # NOQA\nfrom cupy.manipulation.shape import reshape  # NOQA\n\nfrom cupy.manipulation.transpose import moveaxis  # NOQA\nfrom cupy.manipulation.transpose import rollaxis  # NOQA\nfrom cupy.manipulation.transpose import swapaxes  # NOQA\nfrom cupy.manipulation.transpose import transpose  # NOQA\n\nfrom cupy.manipulation.dims import atleast_1d  # NOQA\nfrom cupy.manipulation.dims import atleast_2d  # NOQA\nfrom cupy.manipulation.dims import atleast_3d  # NOQA\nfrom cupy.manipulation.dims import broadcast  # NOQA\nfrom cupy.manipulation.dims import broadcast_arrays  # NOQA\nfrom cupy.manipulation.dims import broadcast_to  # NOQA\nfrom cupy.manipulation.dims import expand_dims  # NOQA\nfrom cupy.manipulation.dims import squeeze  # NOQA\n\nfrom cupy.manipulation.join import column_stack  # NOQA\nfrom cupy.manipulation.join import concatenate  # NOQA\nfrom cupy.manipulation.join import dstack  # NOQA\nfrom cupy.manipulation.join import hstack  # NOQA\nfrom cupy.manipulation.join import stack  # NOQA\nfrom cupy.manipulation.join import vstack  # NOQA\n\nfrom cupy.manipulation.kind import asfortranarray  # NOQA\nfrom cupy.manipulation.kind import require  # NOQA\n\nfrom cupy.manipulation.split import array_split  # NOQA\nfrom cupy.manipulation.split import dsplit  # NOQA\nfrom cupy.manipulation.split import hsplit  # NOQA\nfrom cupy.manipulation.split import split  # NOQA\nfrom cupy.manipulation.split import vsplit  # NOQA\n\nfrom cupy.manipulation.tiling import repeat  # NOQA\nfrom cupy.manipulation.tiling import tile  # NOQA\n\nfrom cupy.manipulation.add_remove import unique  # NOQA\nfrom cupy.manipulation.add_remove import trim_zeros  # NOQA\n\nfrom cupy.manipulation.rearrange import flip  # NOQA\nfrom cupy.manipulation.rearrange import fliplr  # NOQA\nfrom cupy.manipulation.rearrange import flipud  # NOQA\nfrom cupy.manipulation.rearrange import roll  # NOQA\nfrom cupy.manipulation.rearrange import rot90  # NOQA\n\n# -----------------------------------------------------------------------------\n# Binary operations\n# -----------------------------------------------------------------------------\nfrom cupy.binary.elementwise import bitwise_and  # NOQA\nfrom cupy.binary.elementwise import bitwise_or  # NOQA\nfrom cupy.binary.elementwise import bitwise_xor  # NOQA\nfrom cupy.binary.elementwise import bitwise_not  # NOQA\nfrom cupy.binary.elementwise import invert  # NOQA\nfrom cupy.binary.elementwise import left_shift  # NOQA\nfrom cupy.binary.elementwise import right_shift  # NOQA\n\nfrom cupy.binary.packing import packbits  # NOQA\nfrom cupy.binary.packing import unpackbits  # NOQA\n\n\ndef binary_repr(num, width=None):\n    """"""Return the binary representation of the input number as a string.\n\n    .. seealso:: :func:`numpy.binary_repr`\n    """"""\n    return numpy.binary_repr(num, width)\n\n\n# -----------------------------------------------------------------------------\n# Data type routines (borrowed from NumPy)\n# -----------------------------------------------------------------------------\ndef can_cast(from_, to, casting=\'safe\'):\n    """"""Returns True if cast between data types can occur according to the\n    casting rule. If from is a scalar or array scalar, also returns True if the\n    scalar value can be cast without overflow or truncation to an integer.\n\n    .. seealso:: :func:`numpy.can_cast`\n    """"""\n    from_ = from_.dtype if isinstance(from_, cupy.ndarray) else from_\n    return numpy.can_cast(from_, to, casting=casting)\n\n\ndef common_type(*arrays):\n    """"""Return a scalar type which is common to the input arrays.\n\n    .. seealso:: :func:`numpy.common_type`\n    """"""\n    if len(arrays) == 0:\n        return numpy.float16\n\n    default_float_dtype = numpy.dtype(\'float64\')\n    dtypes = []\n    for a in arrays:\n        if a.dtype.kind == \'b\':\n            raise TypeError(\'can\\\'t get common type for non-numeric array\')\n        elif a.dtype.kind in \'iu\':\n            dtypes.append(default_float_dtype)\n        else:\n            dtypes.append(a.dtype)\n\n    return functools.reduce(numpy.promote_types, dtypes).type\n\n\ndef result_type(*arrays_and_dtypes):\n    """"""Returns the type that results from applying the NumPy type promotion\n    rules to the arguments.\n\n    .. seealso:: :func:`numpy.result_type`\n    """"""\n    dtypes = [a.dtype if isinstance(a, cupy.ndarray)\n              else a for a in arrays_and_dtypes]\n    return numpy.result_type(*dtypes)\n\n\nfrom numpy import min_scalar_type  # NOQA\nfrom numpy import obj2sctype  # NOQA\nfrom numpy import promote_types  # NOQA\n\nfrom numpy import dtype  # NOQA\nfrom numpy import format_parser  # NOQA\n\nfrom numpy import finfo  # NOQA\nfrom numpy import iinfo  # NOQA\nfrom numpy import MachAr  # NOQA\n\nfrom numpy import find_common_type  # NOQA\nfrom numpy import issctype  # NOQA\nfrom numpy import issubclass_  # NOQA\nfrom numpy import issubdtype  # NOQA\nfrom numpy import issubsctype  # NOQA\n\nfrom numpy import mintypecode  # NOQA\nfrom numpy import sctype2char  # NOQA\nfrom numpy import typename  # NOQA\n\n# -----------------------------------------------------------------------------\n# Optionally Scipy-accelerated routines\n# -----------------------------------------------------------------------------\n# TODO(beam2d): Implement it\n\n# -----------------------------------------------------------------------------\n# Discrete Fourier Transform\n# -----------------------------------------------------------------------------\n# TODO(beam2d): Implement it\n\n# -----------------------------------------------------------------------------\n# Indexing routines\n# -----------------------------------------------------------------------------\nfrom cupy.indexing.generate import c_  # NOQA\nfrom cupy.indexing.generate import indices  # NOQA\nfrom cupy.indexing.generate import ix_  # NOQA\nfrom cupy.indexing.generate import r_  # NOQA\nfrom cupy.indexing.generate import ravel_multi_index  # NOQA\nfrom cupy.indexing.generate import unravel_index  # NOQA\n\nfrom cupy.indexing.indexing import choose  # NOQA\nfrom cupy.indexing.indexing import compress  # NOQA\nfrom cupy.indexing.indexing import diagonal  # NOQA\nfrom cupy.indexing.indexing import extract  # NOQA\nfrom cupy.indexing.indexing import select  # NOQA\nfrom cupy.indexing.indexing import take  # NOQA\nfrom cupy.indexing.indexing import take_along_axis  # NOQA\n\nfrom cupy.indexing.insert import place  # NOQA\nfrom cupy.indexing.insert import put  # NOQA\nfrom cupy.indexing.insert import putmask  # NOQA\nfrom cupy.indexing.insert import fill_diagonal  # NOQA\nfrom cupy.indexing.insert import diag_indices  # NOQA\nfrom cupy.indexing.insert import diag_indices_from  # NOQA\n\nfrom cupy.indexing.iterate import flatiter  # NOQA\n\n# -----------------------------------------------------------------------------\n# Input and output\n# -----------------------------------------------------------------------------\nfrom cupy.io.npz import load  # NOQA\nfrom cupy.io.npz import save  # NOQA\nfrom cupy.io.npz import savez  # NOQA\nfrom cupy.io.npz import savez_compressed  # NOQA\n\nfrom cupy.io.formatting import array_repr  # NOQA\nfrom cupy.io.formatting import array_str  # NOQA\n\n\ndef base_repr(number, base=2, padding=0):  # NOQA (needed to avoid redefinition of `number`)\n    """"""Return a string representation of a number in the given base system.\n\n    .. seealso:: :func:`numpy.base_repr`\n    """"""\n    return numpy.base_repr(number, base, padding)\n\n\n# -----------------------------------------------------------------------------\n# Linear algebra\n# -----------------------------------------------------------------------------\nfrom cupy.linalg.einsum import einsum  # NOQA\n\nfrom cupy.linalg.product import cross  # NOQA\nfrom cupy.linalg.product import dot  # NOQA\nfrom cupy.linalg.product import inner  # NOQA\nfrom cupy.linalg.product import kron  # NOQA\nfrom cupy.linalg.product import matmul  # NOQA\nfrom cupy.linalg.product import outer  # NOQA\nfrom cupy.linalg.product import tensordot  # NOQA\nfrom cupy.linalg.product import vdot  # NOQA\n\nfrom cupy.linalg.norms import trace  # NOQA\n\n# -----------------------------------------------------------------------------\n# Logic functions\n# -----------------------------------------------------------------------------\nfrom cupy.logic.comparison import allclose  # NOQA\nfrom cupy.logic.comparison import array_equal  # NOQA\nfrom cupy.logic.comparison import isclose  # NOQA\n\nfrom cupy.logic.content import isfinite  # NOQA\nfrom cupy.logic.content import isinf  # NOQA\nfrom cupy.logic.content import isnan  # NOQA\n\nfrom cupy.logic.truth import in1d  # NOQA\nfrom cupy.logic.truth import isin  # NOQA\n\nfrom cupy.logic.type_test import iscomplex  # NOQA\nfrom cupy.logic.type_test import iscomplexobj  # NOQA\nfrom cupy.logic.type_test import isfortran  # NOQA\nfrom cupy.logic.type_test import isreal  # NOQA\nfrom cupy.logic.type_test import isrealobj  # NOQA\n\nfrom cupy.logic.truth import in1d  # NOQA\nfrom cupy.logic.truth import isin  # NOQA\n\n\ndef isscalar(element):\n    """"""Returns True if the type of num is a scalar type.\n\n    .. seealso:: :func:`numpy.isscalar`\n    """"""\n    return numpy.isscalar(element)\n\n\nfrom cupy.logic.ops import logical_and  # NOQA\nfrom cupy.logic.ops import logical_not  # NOQA\nfrom cupy.logic.ops import logical_or  # NOQA\nfrom cupy.logic.ops import logical_xor  # NOQA\n\nfrom cupy.logic.comparison import equal  # NOQA\nfrom cupy.logic.comparison import greater  # NOQA\nfrom cupy.logic.comparison import greater_equal  # NOQA\nfrom cupy.logic.comparison import less  # NOQA\nfrom cupy.logic.comparison import less_equal  # NOQA\nfrom cupy.logic.comparison import not_equal  # NOQA\n\nfrom cupy.logic.truth import all  # NOQA\nfrom cupy.logic.truth import any  # NOQA\n\n# -----------------------------------------------------------------------------\n# Mathematical functions\n# -----------------------------------------------------------------------------\nfrom cupy.math.trigonometric import arccos  # NOQA\nfrom cupy.math.trigonometric import arcsin  # NOQA\nfrom cupy.math.trigonometric import arctan  # NOQA\nfrom cupy.math.trigonometric import arctan2  # NOQA\nfrom cupy.math.trigonometric import cos  # NOQA\nfrom cupy.math.trigonometric import deg2rad  # NOQA\nfrom cupy.math.trigonometric import degrees  # NOQA\nfrom cupy.math.trigonometric import hypot  # NOQA\nfrom cupy.math.trigonometric import rad2deg  # NOQA\nfrom cupy.math.trigonometric import radians  # NOQA\nfrom cupy.math.trigonometric import sin  # NOQA\nfrom cupy.math.trigonometric import tan  # NOQA\nfrom cupy.math.trigonometric import unwrap  # NOQA\n\nfrom cupy.math.hyperbolic import arccosh  # NOQA\nfrom cupy.math.hyperbolic import arcsinh  # NOQA\nfrom cupy.math.hyperbolic import arctanh  # NOQA\nfrom cupy.math.hyperbolic import cosh  # NOQA\nfrom cupy.math.hyperbolic import sinh  # NOQA\nfrom cupy.math.hyperbolic import tanh  # NOQA\n\nfrom cupy.math.rounding import around  # NOQA\nfrom cupy.math.rounding import ceil  # NOQA\nfrom cupy.math.rounding import fix  # NOQA\nfrom cupy.math.rounding import floor  # NOQA\nfrom cupy.math.rounding import rint  # NOQA\nfrom cupy.math.rounding import round_  # NOQA\nfrom cupy.math.rounding import trunc  # NOQA\n\nfrom cupy.math.sumprod import prod  # NOQA\nfrom cupy.math.sumprod import sum  # NOQA\nfrom cupy.math.sumprod import cumprod  # NOQA\nfrom cupy.math.sumprod import cumsum  # NOQA\nfrom cupy.math.sumprod import nansum  # NOQA\nfrom cupy.math.sumprod import nanprod  # NOQA\nfrom cupy.math.sumprod import diff  # NOQA\nfrom cupy.math.window import bartlett  # NOQA\nfrom cupy.math.window import blackman  # NOQA\nfrom cupy.math.window import hamming  # NOQA\nfrom cupy.math.window import hanning  # NOQA\nfrom cupy.math.window import kaiser  # NOQA\n\nfrom cupy.math.explog import exp  # NOQA\nfrom cupy.math.explog import exp2  # NOQA\nfrom cupy.math.explog import expm1  # NOQA\nfrom cupy.math.explog import log  # NOQA\nfrom cupy.math.explog import log10  # NOQA\nfrom cupy.math.explog import log1p  # NOQA\nfrom cupy.math.explog import log2  # NOQA\nfrom cupy.math.explog import logaddexp  # NOQA\nfrom cupy.math.explog import logaddexp2  # NOQA\n\nfrom cupy.math.special import i0  # NOQA\nfrom cupy.math.special import sinc  # NOQA\n\nfrom cupy.math.floating import copysign  # NOQA\nfrom cupy.math.floating import frexp  # NOQA\nfrom cupy.math.floating import ldexp  # NOQA\nfrom cupy.math.floating import nextafter  # NOQA\nfrom cupy.math.floating import signbit  # NOQA\n\nfrom cupy.math.rational import gcd  # NOQA\nfrom cupy.math.rational import lcm  # NOQA\n\nfrom cupy.math.arithmetic import add  # NOQA\nfrom cupy.math.arithmetic import divide  # NOQA\nfrom cupy.math.arithmetic import divmod  # NOQA\nfrom cupy.math.arithmetic import floor_divide  # NOQA\nfrom cupy.math.arithmetic import fmod  # NOQA\nfrom cupy.math.arithmetic import modf  # NOQA\nfrom cupy.math.arithmetic import multiply  # NOQA\nfrom cupy.math.arithmetic import negative  # NOQA\nfrom cupy.math.arithmetic import power  # NOQA\nfrom cupy.math.arithmetic import reciprocal  # NOQA\nfrom cupy.math.arithmetic import remainder  # NOQA\nfrom cupy.math.arithmetic import remainder as mod  # NOQA\nfrom cupy.math.arithmetic import subtract  # NOQA\nfrom cupy.math.arithmetic import true_divide  # NOQA\n\nfrom cupy.math.arithmetic import angle  # NOQA\nfrom cupy.math.arithmetic import conjugate as conj  # NOQA\nfrom cupy.math.arithmetic import conjugate  # NOQA\nfrom cupy.math.arithmetic import imag  # NOQA\nfrom cupy.math.arithmetic import real  # NOQA\n\nfrom cupy.math.misc import absolute as abs  # NOQA\nfrom cupy.math.misc import absolute  # NOQA\nfrom cupy.math.misc import cbrt  # NOQA\nfrom cupy.math.misc import clip  # NOQA\nfrom cupy.math.misc import fmax  # NOQA\nfrom cupy.math.misc import fmin  # NOQA\nfrom cupy.math.misc import maximum  # NOQA\nfrom cupy.math.misc import minimum  # NOQA\nfrom cupy.math.misc import nan_to_num  # NOQA\nfrom cupy.math.misc import sign  # NOQA\nfrom cupy.math.misc import sqrt  # NOQA\nfrom cupy.math.misc import square  # NOQA\n\n# -----------------------------------------------------------------------------\n# Miscellaneous routines\n# -----------------------------------------------------------------------------\nfrom cupy.misc import may_share_memory  # NOQA\nfrom cupy.misc import shares_memory  # NOQA\nfrom cupy.misc import who  # NOQA\n\n\n# -----------------------------------------------------------------------------\n# Padding\n# -----------------------------------------------------------------------------\npad = padding.pad.pad\n\n\n# -----------------------------------------------------------------------------\n# Sorting, searching, and counting\n# -----------------------------------------------------------------------------\nfrom cupy._sorting.count import count_nonzero  # NOQA\n\nfrom cupy._sorting.search import argmax  # NOQA\nfrom cupy._sorting.search import argmin  # NOQA\nfrom cupy._sorting.search import argwhere  # NOQA\nfrom cupy._sorting.search import flatnonzero  # NOQA\nfrom cupy._sorting.search import nanargmax  # NOQA\nfrom cupy._sorting.search import nanargmin  # NOQA\nfrom cupy._sorting.search import nonzero  # NOQA\nfrom cupy._sorting.search import searchsorted  # NOQA\nfrom cupy._sorting.search import where  # NOQA\n\nfrom cupy._sorting.sort import argpartition  # NOQA\nfrom cupy._sorting.sort import argsort  # NOQA\nfrom cupy._sorting.sort import lexsort  # NOQA\nfrom cupy._sorting.sort import msort  # NOQA\nfrom cupy._sorting.sort import sort_complex  # NOQA\nfrom cupy._sorting.sort import partition  # NOQA\nfrom cupy._sorting.sort import sort  # NOQA\n\n# -----------------------------------------------------------------------------\n# Statistics\n# -----------------------------------------------------------------------------\nfrom cupy.statistics.correlation import corrcoef  # NOQA\nfrom cupy.statistics.correlation import cov  # NOQA\n\nfrom cupy.statistics.order import amax  # NOQA\nfrom cupy.statistics.order import amax as max  # NOQA\nfrom cupy.statistics.order import amin  # NOQA\nfrom cupy.statistics.order import amin as min  # NOQA\nfrom cupy.statistics.order import nanmax  # NOQA\nfrom cupy.statistics.order import nanmin  # NOQA\nfrom cupy.statistics.order import percentile  # NOQA\nfrom cupy.statistics.order import ptp  # NOQA\n\nfrom cupy.statistics.meanvar import median  # NOQA\nfrom cupy.statistics.meanvar import average  # NOQA\nfrom cupy.statistics.meanvar import mean  # NOQA\nfrom cupy.statistics.meanvar import std  # NOQA\nfrom cupy.statistics.meanvar import var  # NOQA\nfrom cupy.statistics.meanvar import nanmean  # NOQA\nfrom cupy.statistics.meanvar import nanstd  # NOQA\nfrom cupy.statistics.meanvar import nanvar  # NOQA\n\nfrom cupy.statistics.histogram import bincount  # NOQA\nfrom cupy.statistics.histogram import digitize  # NOQA\nfrom cupy.statistics.histogram import histogram  # NOQA\n\n# -----------------------------------------------------------------------------\n# Undocumented functions\n# -----------------------------------------------------------------------------\nfrom cupy.core import size  # NOQA\n\n# -----------------------------------------------------------------------------\n# CuPy specific functions\n# -----------------------------------------------------------------------------\n\nfrom cupy.util import clear_memo  # NOQA\nfrom cupy.util import memoize  # NOQA\n\nfrom cupy.core import ElementwiseKernel  # NOQA\nfrom cupy.core import RawKernel  # NOQA\nfrom cupy.core import RawModule  # NOQA\nfrom cupy.core._reduction import ReductionKernel  # NOQA\n\n# -----------------------------------------------------------------------------\n# DLPack\n# -----------------------------------------------------------------------------\n\nfrom cupy.core import fromDlpack  # NOQA\n\n\ndef asnumpy(a, stream=None, order=\'C\'):\n    """"""Returns an array on the host memory from an arbitrary source array.\n\n    Args:\n        a: Arbitrary object that can be converted to :class:`numpy.ndarray`.\n        stream (cupy.cuda.Stream): CUDA stream object. If it is specified, then\n            the device-to-host copy runs asynchronously. Otherwise, the copy is\n            synchronous. Note that if ``a`` is not a :class:`cupy.ndarray`\n            object, then this argument has no effect.\n        order ({\'C\', \'F\', \'A\'}): The desired memory layout of the host\n            array. When ``order`` is \'A\', it uses \'F\' if ``a`` is\n            fortran-contiguous and \'C\' otherwise.\n    Returns:\n        numpy.ndarray: Converted array on the host memory.\n\n    """"""\n    if isinstance(a, ndarray):\n        return a.get(stream=stream, order=order)\n    else:\n        return numpy.asarray(a, order=order)\n\n\n_cupy = sys.modules[__name__]\n\n\ndef get_array_module(*args):\n    """"""Returns the array module for arguments.\n\n    This function is used to implement CPU/GPU generic code. If at least one of\n    the arguments is a :class:`cupy.ndarray` object, the :mod:`cupy` module is\n    returned.\n\n    Args:\n        args: Values to determine whether NumPy or CuPy should be used.\n\n    Returns:\n        module: :mod:`cupy` or :mod:`numpy` is returned based on the types of\n        the arguments.\n\n    .. admonition:: Example\n\n       A NumPy/CuPy generic function can be written as follows\n\n       >>> def softplus(x):\n       ...     xp = cupy.get_array_module(x)\n       ...     return xp.maximum(0, x) + xp.log1p(xp.exp(-abs(x)))\n\n    """"""\n    for arg in args:\n        if isinstance(arg, (ndarray, sparse.spmatrix,\n                            cupy.core.fusion._FusionVarScalar,\n                            cupy.core.fusion._FusionVarArray)):\n            return _cupy\n    return numpy\n\n\nfuse = cupy.core.fusion.fuse\n\ndisable_experimental_feature_warning = False\n\n\n# set default allocator\n_default_memory_pool = cuda.MemoryPool()\n_default_pinned_memory_pool = cuda.PinnedMemoryPool()\n\ncuda.set_allocator(_default_memory_pool.malloc)\ncuda.set_pinned_memory_allocator(_default_pinned_memory_pool.malloc)\n\n\ndef get_default_memory_pool():\n    """"""Returns CuPy default memory pool for GPU memory.\n\n    Returns:\n        cupy.cuda.MemoryPool: The memory pool object.\n\n    .. note::\n       If you want to disable memory pool, please use the following code.\n\n       >>> cupy.cuda.set_allocator(None)\n\n    """"""\n    return _default_memory_pool\n\n\ndef get_default_pinned_memory_pool():\n    """"""Returns CuPy default memory pool for pinned memory.\n\n    Returns:\n        cupy.cuda.PinnedMemoryPool: The memory pool object.\n\n    .. note::\n       If you want to disable memory pool, please use the following code.\n\n       >>> cupy.cuda.set_pinned_memory_allocator(None)\n\n    """"""\n    return _default_pinned_memory_pool\n\n\ndef show_config():\n    """"""Prints the current runtime configuration to standard output.""""""\n    sys.stdout.write(str(_cupyx.get_runtime_info()))\n    sys.stdout.flush()\n'"
cupy/_environment.py,0,"b'""""""\nThis file must not depend on any other CuPy modules.\n""""""\n\nimport os\nimport os.path\nimport shutil\n\n\n_cuda_path = None\n_nvcc_path = None\n\n\ndef get_cuda_path():\n    # Returns the CUDA installation path or None if not found.\n    global _cuda_path\n    if _cuda_path is None:\n        _cuda_path = _get_cuda_path()\n    return _cuda_path\n\n\ndef get_nvcc_path():\n    # Returns the path to the nvcc command or None if not found.\n    global _nvcc_path\n    if _nvcc_path is None:\n        _nvcc_path = _get_nvcc_path()\n    return _nvcc_path\n\n\ndef _get_cuda_path():\n    # Use environment variable\n    cuda_path = os.environ.get(\'CUDA_PATH\', \'\')  # Nvidia default on Windows\n    if os.path.exists(cuda_path):\n        return cuda_path\n\n    # Use nvcc path\n    nvcc_path = shutil.which(\'nvcc\')\n    if nvcc_path is not None:\n        return os.path.dirname(os.path.dirname(nvcc_path))\n\n    # Use typical path\n    if os.path.exists(\'/usr/local/cuda\'):\n        return \'/usr/local/cuda\'\n\n    return None\n\n\ndef _get_nvcc_path():\n    # Honor the ""NVCC"" env var\n    nvcc_path = os.environ.get(\'NVCC\', None)\n    if nvcc_path is not None:\n        return nvcc_path\n\n    # Lookup <CUDA>/bin\n    cuda_path = get_cuda_path()\n    if cuda_path is None:\n        return None\n\n    return shutil.which(\'nvcc\', path=os.path.join(cuda_path, \'bin\'))\n\n\ndef _setup_win32_dll_directory():\n    cuda_path = get_cuda_path()\n    if cuda_path is None:\n        raise RuntimeError(\'CUDA path could not be detected.\')\n    os.add_dll_directory(os.path.join(cuda_path, \'bin\'))\n'"
cupy/_version.py,0,"b""__version__ = '8.0.0b3'\n"""
cupy/cusolver.py,0,"b'import numpy\n\nimport cupy\nfrom cupy.cuda import cusolver\nfrom cupy.cuda import runtime\nfrom cupy.cuda import device\n\n\n_available_cuda_version = {\n    \'gesvdj\': (9000, None),\n    \'gesvda\': (10010, None),\n}\n\n\ndef check_availability(name):\n    if name not in _available_cuda_version:\n        msg = \'No available version information specified for {}\'.name\n        raise ValueError(msg)\n    version_added, version_removed = _available_cuda_version[name]\n    cuda_version = runtime.runtimeGetVersion()\n    if version_added is not None and cuda_version < version_added:\n        return False\n    if version_removed is not None and cuda_version >= version_removed:\n        return False\n    return True\n\n\ndef gesvdj(a, full_matrices=True, compute_uv=True, overwrite_a=False):\n    """"""Singular value decomposition using cusolverDn<t>gesvdj().\n\n    Factorizes the matrix ``a`` into two unitary matrices ``u`` and ``v`` and\n    a singular values vector ``s`` such that ``a == u @ diag(s) @ v*``.\n\n    Args:\n        a (cupy.ndarray): The input matrix with dimension ``(M, N)``.\n        full_matrices (bool): If True, it returns u and v with dimensions\n            ``(M, M)`` and ``(N, N)``. Otherwise, the dimensions of u and v\n            are respectively ``(M, K)`` and ``(K, N)``, where\n            ``K = min(M, N)``.\n        compute_uv (bool): If ``False``, it only returns singular values.\n        overwrite_a (bool): If ``True``, matrix ``a`` might be overwritten.\n\n    Returns:\n        tuple of :class:`cupy.ndarray`:\n            A tuple of ``(u, s, v)``.\n    """"""\n    if not check_availability(\'gesvdj\'):\n        raise RuntimeError(\'gesvdj is not available.\')\n\n    if a.ndim == 3:\n        return _gesvdj_batched(a, full_matrices, compute_uv, overwrite_a)\n\n    assert a.ndim == 2\n\n    if a.dtype == \'f\':\n        helper = cusolver.sgesvdj_bufferSize\n        solver = cusolver.sgesvdj\n        s_dtype = \'f\'\n    elif a.dtype == \'d\':\n        helper = cusolver.dgesvdj_bufferSize\n        solver = cusolver.dgesvdj\n        s_dtype = \'d\'\n    elif a.dtype == \'F\':\n        helper = cusolver.cgesvdj_bufferSize\n        solver = cusolver.cgesvdj\n        s_dtype = \'f\'\n    elif a.dtype == \'D\':\n        helper = cusolver.zgesvdj_bufferSize\n        solver = cusolver.zgesvdj\n        s_dtype = \'d\'\n    else:\n        raise TypeError\n\n    handle = device.get_cusolver_handle()\n    m, n = a.shape\n    a = cupy.array(a, order=\'F\', copy=not overwrite_a)\n    lda = m\n    mn = min(m, n)\n    s = cupy.empty(mn, dtype=s_dtype)\n    ldu = m\n    ldv = n\n    if compute_uv:\n        jobz = cusolver.CUSOLVER_EIG_MODE_VECTOR\n    else:\n        jobz = cusolver.CUSOLVER_EIG_MODE_NOVECTOR\n        full_matrices = False\n    if full_matrices:\n        econ = 0\n        u = cupy.empty((ldu, m), dtype=a.dtype, order=\'F\')\n        v = cupy.empty((ldv, n), dtype=a.dtype, order=\'F\')\n    else:\n        econ = 1\n        u = cupy.empty((ldu, mn), dtype=a.dtype, order=\'F\')\n        v = cupy.empty((ldv, mn), dtype=a.dtype, order=\'F\')\n    params = cusolver.createGesvdjInfo()\n    lwork = helper(handle, jobz, econ, m, n, a.data.ptr, lda, s.data.ptr,\n                   u.data.ptr, ldu, v.data.ptr, ldv, params)\n    work = cupy.empty(lwork, dtype=a.dtype)\n    info = cupy.empty(1, dtype=numpy.int32)\n    solver(handle, jobz, econ, m, n, a.data.ptr, lda, s.data.ptr,\n           u.data.ptr, ldu, v.data.ptr, ldv, work.data.ptr, lwork,\n           info.data.ptr, params)\n    cupy.linalg.util._check_cusolver_dev_info_if_synchronization_allowed(\n        gesvdj, info)\n\n    cusolver.destroyGesvdjInfo(params)\n    if compute_uv:\n        return u, s, v\n    else:\n        return s\n\n\ndef _gesvdj_batched(a, full_matrices, compute_uv, overwrite_a):\n    if a.dtype == \'f\':\n        helper = cusolver.sgesvdjBatched_bufferSize\n        solver = cusolver.sgesvdjBatched\n        s_dtype = \'f\'\n    elif a.dtype == \'d\':\n        helper = cusolver.dgesvdjBatched_bufferSize\n        solver = cusolver.dgesvdjBatched\n        s_dtype = \'d\'\n    elif a.dtype == \'F\':\n        helper = cusolver.cgesvdjBatched_bufferSize\n        solver = cusolver.cgesvdjBatched\n        s_dtype = \'f\'\n    elif a.dtype == \'D\':\n        helper = cusolver.zgesvdjBatched_bufferSize\n        solver = cusolver.zgesvdjBatched\n        s_dtype = \'d\'\n    else:\n        raise TypeError\n\n    handle = device.get_cusolver_handle()\n    batch_size, m, n = a.shape\n    a = cupy.array(a.swapaxes(-2, -1), order=\'C\', copy=not overwrite_a)\n    lda = m\n    mn = min(m, n)\n    s = cupy.empty((batch_size, mn), dtype=s_dtype)\n    ldu = m\n    ldv = n\n    if compute_uv:\n        jobz = cusolver.CUSOLVER_EIG_MODE_VECTOR\n    else:\n        jobz = cusolver.CUSOLVER_EIG_MODE_NOVECTOR\n        # if not batched, `full_matrices = False` could speedup.\n\n    u = cupy.empty((batch_size, m, ldu), dtype=a.dtype).swapaxes(-2, -1)\n    v = cupy.empty((batch_size, n, ldv), dtype=a.dtype).swapaxes(-2, -1)\n    params = cusolver.createGesvdjInfo()\n    lwork = helper(handle, jobz, m, n, a.data.ptr, lda, s.data.ptr,\n                   u.data.ptr, ldu, v.data.ptr, ldv, params, batch_size)\n    work = cupy.empty(lwork, dtype=a.dtype)\n    info = cupy.empty(1, dtype=numpy.int32)\n    solver(handle, jobz, m, n, a.data.ptr, lda, s.data.ptr,\n           u.data.ptr, ldu, v.data.ptr, ldv, work.data.ptr, lwork,\n           info.data.ptr, params, batch_size)\n    cupy.linalg.util._check_cusolver_dev_info_if_synchronization_allowed(\n        gesvdj, info)\n\n    cusolver.destroyGesvdjInfo(params)\n    if not full_matrices:\n        u = u[..., :mn]\n        v = v[..., :mn]\n    if compute_uv:\n        return u, s, v\n    else:\n        return s\n\n\ndef gesvda(a, compute_uv=True):\n    """"""Singular value decomposition using cusolverDn<t>gesvdaStridedBatched().\n\n    Factorizes the matrix ``a`` into two unitary matrices ``u`` and ``v`` and\n    a singular values vector ``s`` such that ``a == u @ diag(s) @ v*``.\n\n    Args:\n        a (cupy.ndarray): The input matrix with dimension ``(.., M, N)``.\n        compute_uv (bool): If ``False``, it only returns singular values.\n\n    Returns:\n        tuple of :class:`cupy.ndarray`:\n            A tuple of ``(u, s, v)``.\n    """"""\n    if not check_availability(\'gesvda\'):\n        raise RuntimeError(\'gesvda is not available.\')\n\n    assert a.ndim >= 2\n    a_ndim = a.ndim\n    a_shape = a.shape\n    m, n = a_shape[-2:]\n    assert m >= n\n\n    if a.dtype == \'f\':\n        helper = cusolver.sgesvdaStridedBatched_bufferSize\n        solver = cusolver.sgesvdaStridedBatched\n        s_dtype = \'f\'\n    elif a.dtype == \'d\':\n        helper = cusolver.dgesvdaStridedBatched_bufferSize\n        solver = cusolver.dgesvdaStridedBatched\n        s_dtype = \'d\'\n    elif a.dtype == \'F\':\n        helper = cusolver.cgesvdaStridedBatched_bufferSize\n        solver = cusolver.cgesvdaStridedBatched\n        s_dtype = \'f\'\n    elif a.dtype == \'D\':\n        helper = cusolver.zgesvdaStridedBatched_bufferSize\n        solver = cusolver.zgesvdaStridedBatched\n        s_dtype = \'d\'\n    else:\n        raise TypeError\n\n    handle = device.get_cusolver_handle()\n    if compute_uv:\n        jobz = cusolver.CUSOLVER_EIG_MODE_VECTOR\n    else:\n        jobz = cusolver.CUSOLVER_EIG_MODE_NOVECTOR\n    rank = min(m, n)\n    if a_ndim == 2:\n        batch_size = 1\n    else:\n        batch_size = numpy.array(a_shape[:-2]).prod().item()\n    a = a.reshape((batch_size, m, n))\n    a = cupy.ascontiguousarray(a.transpose(0, 2, 1))\n    lda = m\n    stride_a = lda * n\n    s = cupy.empty((batch_size, rank), dtype=s_dtype)\n    stride_s = rank\n    ldu = m\n    ldv = n\n    u = cupy.empty((batch_size, rank, ldu), dtype=a.dtype, order=\'C\')\n    v = cupy.empty((batch_size, rank, ldv), dtype=a.dtype, order=\'C\')\n    stride_u = rank * ldu\n    stride_v = rank * ldv\n    lwork = helper(handle, jobz, rank, m, n, a.data.ptr, lda, stride_a,\n                   s.data.ptr, stride_s, u.data.ptr, ldu, stride_u,\n                   v.data.ptr, ldv, stride_v, batch_size)\n    work = cupy.empty((lwork,), dtype=a.dtype)\n    info = cupy.empty((batch_size,), dtype=numpy.int32)\n    r_norm = numpy.empty((batch_size,), dtype=numpy.float64)\n    solver(handle, jobz, rank, m, n, a.data.ptr, lda, stride_a, s.data.ptr,\n           stride_s, u.data.ptr, ldu, stride_u, v.data.ptr, ldv, stride_v,\n           work.data.ptr, lwork, info.data.ptr, r_norm.ctypes.data, batch_size)\n\n    s = s.reshape(a_shape[:-2] + (s.shape[-1],))\n    if not compute_uv:\n        return s\n\n    u = u.transpose(0, 2, 1)\n    v = v.transpose(0, 2, 1)\n    u = u.reshape(a_shape[:-2] + (u.shape[-2:]))\n    v = v.reshape(a_shape[:-2] + (v.shape[-2:]))\n    return u, s, v\n'"
cupy/cusparse.py,0,"b'import functools\n\nimport numpy\nimport platform\n\nimport cupy\nfrom cupy.cuda import cusparse\nfrom cupy.cuda import runtime\nfrom cupy.cuda import device\nfrom cupy import util\nimport cupyx.scipy.sparse\n\n\nclass MatDescriptor(object):\n\n    def __init__(self, descriptor):\n        self.descriptor = descriptor\n\n    @classmethod\n    def create(cls):\n        descr = cusparse.createMatDescr()\n        return MatDescriptor(descr)\n\n    def __reduce__(self):\n        return self.create, ()\n\n    def __del__(self, is_shutting_down=util.is_shutting_down):\n        if is_shutting_down():\n            return\n        if self.descriptor:\n            cusparse.destroyMatDescr(self.descriptor)\n            self.descriptor = None\n\n    def set_mat_type(self, typ):\n        cusparse.setMatType(self.descriptor, typ)\n\n    def set_mat_index_base(self, base):\n        cusparse.setMatIndexBase(self.descriptor, base)\n\n\ndef _cast_common_type(*xs):\n    dtypes = [x.dtype for x in xs if x is not None]\n    dtype = functools.reduce(numpy.promote_types, dtypes)\n    return [x.astype(dtype) if x is not None and x.dtype != dtype else x\n            for x in xs]\n\n\ndef _transpose_flag(trans):\n    if trans:\n        return cusparse.CUSPARSE_OPERATION_TRANSPOSE\n    else:\n        return cusparse.CUSPARSE_OPERATION_NON_TRANSPOSE\n\n\ndef _call_cusparse(name, dtype, *args):\n    if dtype == \'f\':\n        prefix = \'s\'\n    elif dtype == \'d\':\n        prefix = \'d\'\n    elif dtype == \'F\':\n        prefix = \'c\'\n    elif dtype == \'D\':\n        prefix = \'z\'\n    else:\n        raise TypeError\n    f = getattr(cusparse, prefix + name)\n    return f(*args)\n\n\ndef _dtype_to_DataType(dtype):\n    if dtype == \'f\':\n        return runtime.CUDA_R_32F\n    elif dtype == \'d\':\n        return runtime.CUDA_R_64F\n    elif dtype == \'F\':\n        return runtime.CUDA_C_32F\n    elif dtype == \'D\':\n        return runtime.CUDA_C_64F\n    else:\n        raise TypeError\n\n\n_available_cuda_version = {\n    \'csrmv\': (8000, 11000),\n    \'csrmvEx\': (8000, None),\n    \'csrmm\': (8000, 11000),\n    \'csrmm2\': (8000, 11000),\n    \'csrgeam\': (8000, 11000),\n    \'csrgeam2\': (9020, None),\n    \'csrgemm\': (8000, 11000),\n    \'csrgemm2\': (8000, None),\n    # TODO(anaruse): check availability on Windows when CUDA 11 is relased\n    \'spmv\': ({\'Linux\': 10010, \'Windows\': 11000}, None),\n    \'spmm\': ({\'Linux\': 10010, \'Windows\': 11000}, None),\n}\n\n\ndef _get_version(x):\n    if isinstance(x, dict):\n        os_name = platform.system()\n        if os_name not in x:\n            msg = \'No version information specified for the OS {}\'.os_name\n            raise ValueError(msg)\n        return x[os_name]\n    return x\n\n\ndef check_availability(name):\n    if name not in _available_cuda_version:\n        msg = \'No available version information specified for {}\'.name\n        raise ValueError(msg)\n    version_added, version_removed = _available_cuda_version[name]\n    version_added = _get_version(version_added)\n    version_removed = _get_version(version_removed)\n    cuda_version = runtime.runtimeGetVersion()\n    if version_added is not None and cuda_version < version_added:\n        return False\n    if version_removed is not None and cuda_version >= version_removed:\n        return False\n    return True\n\n\ndef csrmv(a, x, y=None, alpha=1, beta=0, transa=False):\n    """"""Matrix-vector product for a CSR-matrix and a dense vector.\n\n    .. math::\n\n       y = \\\\alpha * o_a(A) x + \\\\beta y,\n\n    where :math:`o_a` is a transpose function when ``transa`` is ``True`` and\n    is an identity function otherwise.\n\n    Args:\n        a (cupy.cusparse.csr_matrix): Matrix A.\n        x (cupy.ndarray): Vector x.\n        y (cupy.ndarray or None): Vector y. It must be F-contiguous.\n        alpha (float): Coefficient for x.\n        beta (float): Coefficient for y.\n        transa (bool): If ``True``, transpose of ``A`` is used.\n\n    Returns:\n        cupy.ndarray: Calculated ``y``.\n\n    """"""\n    assert y is None or y.flags.f_contiguous\n\n    a_shape = a.shape if not transa else a.shape[::-1]\n    if a_shape[1] != len(x):\n        raise ValueError(\'dimension mismatch\')\n\n    handle = device.get_cusparse_handle()\n    m, n = a_shape\n    a, x, y = _cast_common_type(a, x, y)\n    dtype = a.dtype\n    if y is None:\n        y = cupy.zeros(m, dtype)\n    alpha = numpy.array(alpha, dtype).ctypes\n    beta = numpy.array(beta, dtype).ctypes\n\n    _call_cusparse(\n        \'csrmv\', dtype,\n        handle, _transpose_flag(transa),\n        a.shape[0], a.shape[1], a.nnz, alpha.data, a._descr.descriptor,\n        a.data.data.ptr, a.indptr.data.ptr, a.indices.data.ptr,\n        x.data.ptr, beta.data, y.data.ptr)\n\n    return y\n\n\ndef csrmvExIsAligned(a, x, y=None):\n    """"""Check if the pointers of arguments for csrmvEx are aligned or not\n\n    Args:\n        a (cupy.cusparse.csr_matrix): Matrix A.\n        x (cupy.ndarray): Vector x.\n        y (cupy.ndarray or None): Vector y.\n\n        Check if a, x, y pointers are aligned by 128 bytes as\n        required by csrmvEx.\n\n    Returns:\n        bool: ``True`` if all pointers are aligned.\n              ``False`` if otherwise.\n\n    """"""\n\n    if a.data.data.ptr % 128 != 0:\n        return False\n    if a.indptr.data.ptr % 128 != 0:\n        return False\n    if a.indices.data.ptr % 128 != 0:\n        return False\n    if x.data.ptr % 128 != 0:\n        return False\n    if y is not None and y.data.ptr % 128 != 0:\n        return False\n    return True\n\n\ndef csrmvEx(a, x, y=None, alpha=1, beta=0, merge_path=True):\n    """"""Matrix-vector product for a CSR-matrix and a dense vector.\n\n    .. math::\n\n       y = \\\\alpha * A x + \\\\beta y,\n\n    Args:\n        a (cupy.cusparse.csr_matrix): Matrix A.\n        x (cupy.ndarray): Vector x.\n        y (cupy.ndarray or None): Vector y. It must be F-contiguous.\n        alpha (float): Coefficient for x.\n        beta (float): Coefficient for y.\n        merge_path (bool): If ``True``, merge path algorithm is used.\n\n        All pointers must be aligned with 128 bytes.\n\n    Returns:\n        cupy.ndarray: Calculated ``y``.\n\n    """"""\n    assert y is None or y.flags.f_contiguous\n\n    if a.shape[1] != len(x):\n        raise ValueError(\'dimension mismatch\')\n\n    handle = device.get_cusparse_handle()\n    m, n = a.shape\n\n    a, x, y = _cast_common_type(a, x, y)\n    dtype = a.dtype\n    if y is None:\n        y = cupy.zeros(m, dtype)\n\n    datatype = _dtype_to_DataType(dtype)\n    algmode = cusparse.CUSPARSE_ALG_MERGE_PATH if \\\n        merge_path else cusparse.CUSPARSE_ALG_NAIVE\n    transa_flag = cusparse.CUSPARSE_OPERATION_NON_TRANSPOSE\n\n    alpha = numpy.array(alpha, dtype).ctypes\n    beta = numpy.array(beta, dtype).ctypes\n\n    assert csrmvExIsAligned(a, x, y)\n\n    bufferSize = cusparse.csrmvEx_bufferSize(\n        handle, algmode, transa_flag,\n        a.shape[0], a.shape[1], a.nnz, alpha.data, datatype,\n        a._descr.descriptor, a.data.data.ptr, datatype,\n        a.indptr.data.ptr, a.indices.data.ptr,\n        x.data.ptr, datatype, beta.data, datatype,\n        y.data.ptr, datatype, datatype)\n\n    buf = cupy.empty(bufferSize, \'b\')\n    assert buf.data.ptr % 128 == 0\n\n    cusparse.csrmvEx(\n        handle, algmode, transa_flag,\n        a.shape[0], a.shape[1], a.nnz, alpha.data, datatype,\n        a._descr.descriptor, a.data.data.ptr, datatype,\n        a.indptr.data.ptr, a.indices.data.ptr,\n        x.data.ptr, datatype, beta.data, datatype,\n        y.data.ptr, datatype, datatype, buf.data.ptr)\n    return y\n\n\ndef csrmm(a, b, c=None, alpha=1, beta=0, transa=False):\n    """"""Matrix-matrix product for a CSR-matrix and a dense matrix.\n\n    .. math::\n\n       C = \\\\alpha o_a(A) B + \\\\beta C,\n\n    where :math:`o_a` is a transpose function when ``transa`` is ``True`` and\n    is an identity function otherwise.\n\n    Args:\n        a (cupyx.scipy.sparse.csr): Sparse matrix A.\n        b (cupy.ndarray): Dense matrix B. It must be F-contiguous.\n        c (cupy.ndarray or None): Dense matrix C. It must be F-contiguous.\n        alpha (float): Coefficient for AB.\n        beta (float): Coefficient for C.\n        transa (bool): If ``True``, transpose of A is used.\n\n    Returns:\n        cupy.ndarray: Calculated C.\n\n    """"""\n    assert a.ndim == b.ndim == 2\n    assert b.flags.f_contiguous\n    assert c is None or c.flags.f_contiguous\n\n    a_shape = a.shape if not transa else a.shape[::-1]\n    if a_shape[1] != b.shape[0]:\n        raise ValueError(\'dimension mismatch\')\n\n    handle = device.get_cusparse_handle()\n    m, k = a_shape\n    n = b.shape[1]\n\n    a, b, c = _cast_common_type(a, b, c)\n    if c is None:\n        c = cupy.zeros((m, n), a.dtype, \'F\')\n\n    ldb = k\n    ldc = m\n\n    alpha = numpy.array(alpha, a.dtype).ctypes\n    beta = numpy.array(beta, a.dtype).ctypes\n    _call_cusparse(\n        \'csrmm\', a.dtype,\n        handle, _transpose_flag(transa),\n        a.shape[0], n, a.shape[1], a.nnz,\n        alpha.data, a._descr.descriptor, a.data.data.ptr,\n        a.indptr.data.ptr, a.indices.data.ptr,\n        b.data.ptr, ldb, beta.data, c.data.ptr, ldc)\n    return c\n\n\ndef csrmm2(a, b, c=None, alpha=1.0, beta=0.0, transa=False, transb=False):\n    """"""Matrix-matrix product for a CSR-matrix and a dense matrix.\n\n    .. math::\n\n       C = \\\\alpha o_a(A) o_b(B) + \\\\beta C,\n\n    where :math:`o_a` and :math:`o_b` are transpose functions when ``transa``\n    and ``tranb`` are ``True`` respectively. And they are identity functions\n    otherwise.\n    It is forbidden that both ``transa`` and ``transb`` are ``True`` in\n    cuSPARSE specification.\n\n    Args:\n        a (cupyx.scipy.sparse.csr): Sparse matrix A.\n        b (cupy.ndarray): Dense matrix B. It must be F-contiguous.\n        c (cupy.ndarray or None): Dense matrix C. It must be F-contiguous.\n        alpha (float): Coefficient for AB.\n        beta (float): Coefficient for C.\n        transa (bool): If ``True``, transpose of A is used.\n        transb (bool): If ``True``, transpose of B is used.\n\n    Returns:\n        cupy.ndarray: Calculated C.\n\n    """"""\n    assert a.ndim == b.ndim == 2\n    assert a.has_canonical_format\n    assert b.flags.f_contiguous\n    assert c is None or c.flags.f_contiguous\n    assert not (transa and transb)\n\n    a_shape = a.shape if not transa else a.shape[::-1]\n    b_shape = b.shape if not transb else b.shape[::-1]\n    if a_shape[1] != b_shape[0]:\n        raise ValueError(\'dimension mismatch\')\n\n    handle = device.get_cusparse_handle()\n    m, k = a_shape\n    n = b_shape[1]\n\n    a, b, c = _cast_common_type(a, b, c)\n    if c is None:\n        c = cupy.zeros((m, n), a.dtype, \'F\')\n\n    ldb = b.shape[0]\n    ldc = c.shape[0]\n    op_a = _transpose_flag(transa)\n    op_b = _transpose_flag(transb)\n    alpha = numpy.array(alpha, a.dtype).ctypes\n    beta = numpy.array(beta, a.dtype).ctypes\n    _call_cusparse(\n        \'csrmm2\', a.dtype,\n        handle, op_a, op_b, a.shape[0], n, a.shape[1], a.nnz,\n        alpha.data, a._descr.descriptor, a.data.data.ptr,\n        a.indptr.data.ptr, a.indices.data.ptr,\n        b.data.ptr, ldb, beta.data, c.data.ptr, ldc)\n    return c\n\n\ndef csrgeam(a, b, alpha=1, beta=1):\n    """"""Matrix-matrix addition.\n\n    .. math::\n        C = \\\\alpha A + \\\\beta B\n\n    Args:\n        a (cupyx.scipy.sparse.csr_matrix): Sparse matrix A.\n        b (cupyx.scipy.sparse.csr_matrix): Sparse matrix B.\n        alpha (float): Coefficient for A.\n        beta (float): Coefficient for B.\n\n    Returns:\n        cupyx.scipy.sparse.csr_matrix: Result matrix.\n\n    """"""\n    if not check_availability(\'csrgeam\'):\n        raise RuntimeError(\'csrgeam is not available.\')\n\n    if not isinstance(a, cupyx.scipy.sparse.csr_matrix):\n        raise TypeError(\'unsupported type (actual: {})\'.format(type(a)))\n    if not isinstance(b, cupyx.scipy.sparse.csr_matrix):\n        raise TypeError(\'unsupported type (actual: {})\'.format(type(b)))\n    assert a.has_canonical_format\n    assert b.has_canonical_format\n    if a.shape != b.shape:\n        raise ValueError(\'inconsistent shapes\')\n\n    handle = device.get_cusparse_handle()\n    m, n = a.shape\n    a, b = _cast_common_type(a, b)\n    nnz = numpy.empty((), \'i\')\n    cusparse.setPointerMode(\n        handle, cusparse.CUSPARSE_POINTER_MODE_HOST)\n\n    c_descr = MatDescriptor.create()\n    c_indptr = cupy.empty(m + 1, \'i\')\n\n    cusparse.xcsrgeamNnz(\n        handle, m, n,\n        a._descr.descriptor, a.nnz, a.indptr.data.ptr, a.indices.data.ptr,\n        b._descr.descriptor, b.nnz, b.indptr.data.ptr, b.indices.data.ptr,\n        c_descr.descriptor, c_indptr.data.ptr, nnz.ctypes.data)\n\n    c_indices = cupy.empty(int(nnz), \'i\')\n    c_data = cupy.empty(int(nnz), a.dtype)\n    alpha = numpy.array(alpha, a.dtype).ctypes\n    beta = numpy.array(beta, a.dtype).ctypes\n    _call_cusparse(\n        \'csrgeam\', a.dtype,\n        handle, m, n, alpha.data,\n        a._descr.descriptor, a.nnz, a.data.data.ptr,\n        a.indptr.data.ptr, a.indices.data.ptr, beta.data,\n        b._descr.descriptor, b.nnz, b.data.data.ptr,\n        b.indptr.data.ptr, b.indices.data.ptr,\n        c_descr.descriptor, c_data.data.ptr, c_indptr.data.ptr,\n        c_indices.data.ptr)\n\n    c = cupyx.scipy.sparse.csr_matrix(\n        (c_data, c_indices, c_indptr), shape=a.shape)\n    c._has_canonical_format = True\n    return c\n\n\ndef csrgeam2(a, b, alpha=1, beta=1):\n    """"""Matrix-matrix addition.\n\n    .. math::\n        C = \\\\alpha A + \\\\beta B\n\n    Args:\n        a (cupyx.scipy.sparse.csr_matrix): Sparse matrix A.\n        b (cupyx.scipy.sparse.csr_matrix): Sparse matrix B.\n        alpha (float): Coefficient for A.\n        beta (float): Coefficient for B.\n\n    Returns:\n        cupyx.scipy.sparse.csr_matrix: Result matrix.\n\n    """"""\n    if not check_availability(\'csrgeam2\'):\n        raise RuntimeError(\'csrgeam2 is not available.\')\n\n    if not isinstance(a, cupyx.scipy.sparse.csr_matrix):\n        raise TypeError(\'unsupported type (actual: {})\'.format(type(a)))\n    if not isinstance(b, cupyx.scipy.sparse.csr_matrix):\n        raise TypeError(\'unsupported type (actual: {})\'.format(type(b)))\n    assert a.has_canonical_format\n    assert b.has_canonical_format\n    if a.shape != b.shape:\n        raise ValueError(\'inconsistent shapes\')\n\n    handle = device.get_cusparse_handle()\n    m, n = a.shape\n    a, b = _cast_common_type(a, b)\n    nnz = numpy.empty((), \'i\')\n    cusparse.setPointerMode(\n        handle, cusparse.CUSPARSE_POINTER_MODE_HOST)\n\n    alpha = numpy.array(alpha, a.dtype).ctypes\n    beta = numpy.array(beta, a.dtype).ctypes\n    c_descr = MatDescriptor.create()\n    c_indptr = cupy.empty(m + 1, \'i\')\n\n    null_ptr = 0\n    buff_size = _call_cusparse(\n        \'csrgeam2_bufferSizeExt\', a.dtype,\n        handle, m, n, alpha.data, a._descr.descriptor, a.nnz, a.data.data.ptr,\n        a.indptr.data.ptr, a.indices.data.ptr, beta.data, b._descr.descriptor,\n        b.nnz, b.data.data.ptr, b.indptr.data.ptr, b.indices.data.ptr,\n        c_descr.descriptor, null_ptr, c_indptr.data.ptr, null_ptr)\n    buff = cupy.empty(buff_size, numpy.int8)\n    cusparse.xcsrgeam2Nnz(\n        handle, m, n, a._descr.descriptor, a.nnz, a.indptr.data.ptr,\n        a.indices.data.ptr, b._descr.descriptor, b.nnz, b.indptr.data.ptr,\n        b.indices.data.ptr, c_descr.descriptor, c_indptr.data.ptr,\n        nnz.ctypes.data, buff.data.ptr)\n    c_indices = cupy.empty(int(nnz), \'i\')\n    c_data = cupy.empty(int(nnz), a.dtype)\n    _call_cusparse(\n        \'csrgeam2\', a.dtype,\n        handle, m, n, alpha.data, a._descr.descriptor, a.nnz, a.data.data.ptr,\n        a.indptr.data.ptr, a.indices.data.ptr, beta.data, b._descr.descriptor,\n        b.nnz, b.data.data.ptr, b.indptr.data.ptr, b.indices.data.ptr,\n        c_descr.descriptor, c_data.data.ptr, c_indptr.data.ptr,\n        c_indices.data.ptr, buff.data.ptr)\n\n    c = cupyx.scipy.sparse.csr_matrix(\n        (c_data, c_indices, c_indptr), shape=a.shape)\n    c._has_canonical_format = True\n    return c\n\n\ndef csrgemm(a, b, transa=False, transb=False):\n    """"""Matrix-matrix product for CSR-matrix.\n\n    math::\n       C = op(A) op(B),\n\n    Args:\n        a (cupyx.scipy.sparse.csr_matrix): Sparse matrix A.\n        b (cupyx.scipy.sparse.csr_matrix): Sparse matrix B.\n        transa (bool): If ``True``, transpose of A is used.\n        transb (bool): If ``True``, transpose of B is used.\n\n    Returns:\n        cupyx.scipy.sparse.csr_matrix: Calculated C.\n\n    """"""\n    assert a.ndim == b.ndim == 2\n    assert a.has_canonical_format\n    assert b.has_canonical_format\n    a_shape = a.shape if not transa else a.shape[::-1]\n    b_shape = b.shape if not transb else b.shape[::-1]\n    if a_shape[1] != b_shape[0]:\n        raise ValueError(\'dimension mismatch\')\n\n    handle = device.get_cusparse_handle()\n    m, k = a_shape\n    n = b_shape[1]\n\n    a, b = _cast_common_type(a, b)\n\n    if a.nnz == 0 or b.nnz == 0:\n        return cupyx.scipy.sparse.csr_matrix((m, n), dtype=a.dtype)\n\n    op_a = _transpose_flag(transa)\n    op_b = _transpose_flag(transb)\n\n    nnz = numpy.empty((), \'i\')\n    cusparse.setPointerMode(\n        handle, cusparse.CUSPARSE_POINTER_MODE_HOST)\n\n    c_descr = MatDescriptor.create()\n    c_indptr = cupy.empty(m + 1, \'i\')\n\n    cusparse.xcsrgemmNnz(\n        handle, op_a, op_b, m, n, k, a._descr.descriptor, a.nnz,\n        a.indptr.data.ptr, a.indices.data.ptr, b._descr.descriptor, b.nnz,\n        b.indptr.data.ptr, b.indices.data.ptr, c_descr.descriptor,\n        c_indptr.data.ptr, nnz.ctypes.data)\n\n    c_indices = cupy.empty(int(nnz), \'i\')\n    c_data = cupy.empty(int(nnz), a.dtype)\n    _call_cusparse(\n        \'csrgemm\', a.dtype,\n        handle, op_a, op_b, m, n, k, a._descr.descriptor, a.nnz,\n        a.data.data.ptr, a.indptr.data.ptr, a.indices.data.ptr,\n        b._descr.descriptor, b.nnz, b.data.data.ptr, b.indptr.data.ptr,\n        b.indices.data.ptr,\n        c_descr.descriptor, c_data.data.ptr, c_indptr.data.ptr,\n        c_indices.data.ptr)\n\n    c = cupyx.scipy.sparse.csr_matrix(\n        (c_data, c_indices, c_indptr), shape=(m, n))\n    c._has_canonical_format = True\n    return c\n\n\ndef csrgemm2(a, b, d=None, alpha=1, beta=1):\n    """"""Matrix-matrix product for CSR-matrix.\n\n    math::\n       C = alpha * A * B + beta * D\n\n    Args:\n        a (cupyx.scipy.sparse.csr_matrix): Sparse matrix A.\n        b (cupyx.scipy.sparse.csr_matrix): Sparse matrix B.\n        d (cupyx.scipy.sparse.csr_matrix or None): Sparse matrix D.\n        alpha (scalar): Coefficient\n        beta (scalar): Coefficient\n\n    Returns:\n        cupyx.scipy.sparse.csr_matrix\n\n    """"""\n    if not check_availability(\'csrgemm2\'):\n        raise RuntimeError(\'csrgemm2 is not available.\')\n\n    assert a.ndim == b.ndim == 2\n    if not isinstance(a, cupyx.scipy.sparse.csr_matrix):\n        raise TypeError(\'unsupported type (actual: {})\'.format(type(a)))\n    if not isinstance(b, cupyx.scipy.sparse.csr_matrix):\n        raise TypeError(\'unsupported type (actual: {})\'.format(type(b)))\n    assert a.has_canonical_format\n    assert b.has_canonical_format\n    if a.shape[1] != b.shape[0]:\n        raise ValueError(\'mismatched shape\')\n    if d is not None:\n        assert d.ndim == 2\n        if not isinstance(d, cupyx.scipy.sparse.csr_matrix):\n            raise TypeError(\'unsupported type (actual: {})\'.format(type(d)))\n        assert d.has_canonical_format\n        if a.shape[0] != d.shape[0] or b.shape[1] != d.shape[1]:\n            raise ValueError(\'mismatched shape\')\n\n    handle = device.get_cusparse_handle()\n    m, k = a.shape\n    _, n = b.shape\n\n    if d is None:\n        a, b = _cast_common_type(a, b)\n    else:\n        a, b, d = _cast_common_type(a, b, d)\n\n    info = cusparse.createCsrgemm2Info()\n    alpha = numpy.array(alpha, a.dtype).ctypes\n    null_ptr = 0\n    if d is None:\n        beta_data = null_ptr\n        d_descr = MatDescriptor.create()\n        d_nnz = 0\n        d_data = null_ptr\n        d_indptr = null_ptr\n        d_indices = null_ptr\n    else:\n        beta = numpy.array(beta, a.dtype).ctypes\n        beta_data = beta.data\n        d_descr = d._descr\n        d_nnz = d.nnz\n        d_data = d.data.data.ptr\n        d_indptr = d.indptr.data.ptr\n        d_indices = d.indices.data.ptr\n\n    buff_size = _call_cusparse(\n        \'csrgemm2_bufferSizeExt\', a.dtype,\n        handle, m, n, k, alpha.data, a._descr.descriptor, a.nnz,\n        a.indptr.data.ptr, a.indices.data.ptr, b._descr.descriptor, b.nnz,\n        b.indptr.data.ptr, b.indices.data.ptr, beta_data, d_descr.descriptor,\n        d_nnz, d_indptr, d_indices, info)\n    buff = cupy.empty(buff_size, numpy.int8)\n\n    c_nnz = numpy.empty((), \'i\')\n    cusparse.setPointerMode(handle, cusparse.CUSPARSE_POINTER_MODE_HOST)\n\n    c_descr = MatDescriptor.create()\n    c_indptr = cupy.empty(m + 1, \'i\')\n    cusparse.xcsrgemm2Nnz(\n        handle, m, n, k, a._descr.descriptor, a.nnz, a.indptr.data.ptr,\n        a.indices.data.ptr, b._descr.descriptor, b.nnz, b.indptr.data.ptr,\n        b.indices.data.ptr, d_descr.descriptor, d_nnz, d_indptr, d_indices,\n        c_descr.descriptor, c_indptr.data.ptr, c_nnz.ctypes.data, info,\n        buff.data.ptr)\n\n    c_indices = cupy.empty(int(c_nnz), \'i\')\n    c_data = cupy.empty(int(c_nnz), a.dtype)\n    _call_cusparse(\n        \'csrgemm2\', a.dtype,\n        handle, m, n, k, alpha.data, a._descr.descriptor, a.nnz,\n        a.data.data.ptr, a.indptr.data.ptr, a.indices.data.ptr,\n        b._descr.descriptor, b.nnz, b.data.data.ptr, b.indptr.data.ptr,\n        b.indices.data.ptr, beta_data, d_descr.descriptor, d_nnz, d_data,\n        d_indptr, d_indices, c_descr.descriptor, c_data.data.ptr,\n        c_indptr.data.ptr, c_indices.data.ptr, info, buff.data.ptr)\n\n    c = cupyx.scipy.sparse.csr_matrix(\n        (c_data, c_indices, c_indptr), shape=(m, n))\n    c._has_canonical_format = True\n    cusparse.destroyCsrgemm2Info(info)\n    return c\n\n\ndef csr2dense(x, out=None):\n    """"""Converts CSR-matrix to a dense matrix.\n\n    Args:\n        x (cupyx.scipy.sparse.csr_matrix): A sparse matrix to convert.\n        out (cupy.ndarray or None): A dense metrix to store the result.\n            It must be F-contiguous.\n\n    Returns:\n        cupy.ndarray: Converted result.\n\n    """"""\n    dtype = x.dtype\n    assert dtype.char in \'fdFD\'\n    if out is None:\n        out = cupy.empty(x.shape, dtype=dtype, order=\'F\')\n    else:\n        assert out.flags.f_contiguous\n\n    handle = device.get_cusparse_handle()\n    _call_cusparse(\n        \'csr2dense\', x.dtype,\n        handle, x.shape[0], x.shape[1], x._descr.descriptor,\n        x.data.data.ptr, x.indptr.data.ptr, x.indices.data.ptr,\n        out.data.ptr, x.shape[0])\n\n    return out\n\n\ndef csc2dense(x, out=None):\n    """"""Converts CSC-matrix to a dense matrix.\n\n    Args:\n        x (cupyx.scipy.sparse.csc_matrix): A sparse matrix to convert.\n        out (cupy.ndarray or None): A dense metrix to store the result.\n            It must be F-contiguous.\n\n    Returns:\n        cupy.ndarray: Converted result.\n\n    """"""\n    dtype = x.dtype\n    assert dtype.char in \'fdFD\'\n    if out is None:\n        out = cupy.empty(x.shape, dtype=dtype, order=\'F\')\n    else:\n        assert out.flags.f_contiguous\n\n    handle = device.get_cusparse_handle()\n    _call_cusparse(\n        \'csc2dense\', x.dtype,\n        handle, x.shape[0], x.shape[1], x._descr.descriptor,\n        x.data.data.ptr, x.indices.data.ptr, x.indptr.data.ptr,\n        out.data.ptr, x.shape[0])\n\n    return out\n\n\ndef csrsort(x):\n    """"""Sorts indices of CSR-matrix in place.\n\n    Args:\n        x (cupyx.scipy.sparse.csr_matrix): A sparse matrix to sort.\n\n    """"""\n    nnz = x.nnz\n    if nnz == 0:\n        return\n    handle = device.get_cusparse_handle()\n    m, n = x.shape\n\n    buffer_size = cusparse.xcsrsort_bufferSizeExt(\n        handle, m, n, nnz, x.indptr.data.ptr,\n        x.indices.data.ptr)\n    buf = cupy.empty(buffer_size, \'b\')\n    P = cupy.empty(nnz, \'i\')\n    data_orig = x.data.copy()\n    cusparse.createIdentityPermutation(handle, nnz, P.data.ptr)\n    cusparse.xcsrsort(\n        handle, m, n, nnz, x._descr.descriptor, x.indptr.data.ptr,\n        x.indices.data.ptr, P.data.ptr, buf.data.ptr)\n    _call_cusparse(\n        \'gthr\', x.dtype,\n        handle, nnz, data_orig.data.ptr, x.data.data.ptr,\n        P.data.ptr, cusparse.CUSPARSE_INDEX_BASE_ZERO)\n\n\ndef cscsort(x):\n    """"""Sorts indices of CSC-matrix in place.\n\n    Args:\n        x (cupyx.scipy.sparse.csc_matrix): A sparse matrix to sort.\n\n    """"""\n    nnz = x.nnz\n    if nnz == 0:\n        return\n    handle = device.get_cusparse_handle()\n    m, n = x.shape\n\n    buffer_size = cusparse.xcscsort_bufferSizeExt(\n        handle, m, n, nnz, x.indptr.data.ptr,\n        x.indices.data.ptr)\n    buf = cupy.empty(buffer_size, \'b\')\n    P = cupy.empty(nnz, \'i\')\n    data_orig = x.data.copy()\n    cusparse.createIdentityPermutation(handle, nnz, P.data.ptr)\n    cusparse.xcscsort(\n        handle, m, n, nnz, x._descr.descriptor, x.indptr.data.ptr,\n        x.indices.data.ptr, P.data.ptr, buf.data.ptr)\n    _call_cusparse(\n        \'gthr\', x.dtype,\n        handle, nnz, data_orig.data.ptr, x.data.data.ptr,\n        P.data.ptr, cusparse.CUSPARSE_INDEX_BASE_ZERO)\n\n\ndef coosort(x):\n    """"""Sorts indices of COO-matrix in place.\n\n    Args:\n        x (cupyx.scipy.sparse.coo_matrix): A sparse matrix to sort.\n\n    """"""\n    nnz = x.nnz\n    if nnz == 0:\n        return\n    handle = device.get_cusparse_handle()\n    m, n = x.shape\n\n    buffer_size = cusparse.xcoosort_bufferSizeExt(\n        handle, m, n, nnz, x.row.data.ptr, x.col.data.ptr)\n    buf = cupy.empty(buffer_size, \'b\')\n    P = cupy.empty(nnz, \'i\')\n    data_orig = x.data.copy()\n    cusparse.createIdentityPermutation(handle, nnz, P.data.ptr)\n    cusparse.xcoosortByRow(\n        handle, m, n, nnz, x.row.data.ptr, x.col.data.ptr,\n        P.data.ptr, buf.data.ptr)\n    _call_cusparse(\n        \'gthr\', x.dtype,\n        handle, nnz, data_orig.data.ptr, x.data.data.ptr,\n        P.data.ptr, cusparse.CUSPARSE_INDEX_BASE_ZERO)\n\n\ndef coo2csr(x):\n    handle = device.get_cusparse_handle()\n    m = x.shape[0]\n    indptr = cupy.empty(m + 1, \'i\')\n    cusparse.xcoo2csr(\n        handle, x.row.data.ptr, x.nnz, m,\n        indptr.data.ptr, cusparse.CUSPARSE_INDEX_BASE_ZERO)\n    return cupyx.scipy.sparse.csr.csr_matrix(\n        (x.data, x.col, indptr), shape=x.shape)\n\n\ndef csr2coo(x, data, indices):\n    """"""Converts a CSR-matrix to COO format.\n\n    Args:\n        x (cupyx.scipy.sparse.csr_matrix): A matrix to be converted.\n        data (cupy.ndarray): A data array for converted data.\n        indices (cupy.ndarray): An index array for converted data.\n\n    Returns:\n        cupyx.scipy.sparse.coo_matrix: A converted matrix.\n\n    """"""\n    handle = device.get_cusparse_handle()\n    m = x.shape[0]\n    nnz = len(x.data)\n    row = cupy.empty(nnz, \'i\')\n    cusparse.xcsr2coo(\n        handle, x.indptr.data.ptr, nnz, m, row.data.ptr,\n        cusparse.CUSPARSE_INDEX_BASE_ZERO)\n    # data and indices did not need to be copied already\n    return cupyx.scipy.sparse.coo_matrix(\n        (data, (row, indices)), shape=x.shape)\n\n\ndef csr2csc(x):\n    handle = device.get_cusparse_handle()\n    m, n = x.shape\n    nnz = x.nnz\n    data = cupy.empty(nnz, x.dtype)\n    indptr = cupy.empty(n + 1, \'i\')\n    indices = cupy.empty(nnz, \'i\')\n\n    _call_cusparse(\n        \'csr2csc\', x.dtype,\n        handle, m, n, nnz, x.data.data.ptr,\n        x.indptr.data.ptr, x.indices.data.ptr,\n        data.data.ptr, indices.data.ptr, indptr.data.ptr,\n        cusparse.CUSPARSE_ACTION_NUMERIC,\n        cusparse.CUSPARSE_INDEX_BASE_ZERO)\n    return cupyx.scipy.sparse.csc_matrix(\n        (data, indices, indptr), shape=x.shape)\n\n\ndef dense2csc(x):\n    """"""Converts a dense matrix in CSC format.\n\n    Args:\n        x (cupy.ndarray): A matrix to be converted.\n\n    Returns:\n        cupyx.scipy.sparse.csc_matrix: A converted matrix.\n\n    """"""\n    assert x.ndim == 2\n    x = cupy.asfortranarray(x)\n    nnz = numpy.empty((), dtype=\'i\')\n    handle = device.get_cusparse_handle()\n    m, n = x.shape\n\n    descr = MatDescriptor.create()\n    nnz_per_col = cupy.empty(m, \'i\')\n    _call_cusparse(\n        \'nnz\', x.dtype,\n        handle, cusparse.CUSPARSE_DIRECTION_COLUMN, m, n, descr.descriptor,\n        x.data.ptr, m, nnz_per_col.data.ptr, nnz.ctypes.data)\n\n    nnz = int(nnz)\n    data = cupy.empty(nnz, x.dtype)\n    indptr = cupy.empty(n + 1, \'i\')\n    indices = cupy.empty(nnz, \'i\')\n\n    _call_cusparse(\n        \'dense2csc\', x.dtype,\n        handle, m, n, descr.descriptor,\n        x.data.ptr, m, nnz_per_col.data.ptr,\n        data.data.ptr, indices.data.ptr, indptr.data.ptr)\n    # Note that a desciptor is recreated\n    csc = cupyx.scipy.sparse.csc_matrix((data, indices, indptr), shape=x.shape)\n    csc._has_canonical_format = True\n    return csc\n\n\ndef dense2csr(x):\n    """"""Converts a dense matrix in CSR format.\n\n    Args:\n        x (cupy.ndarray): A matrix to be converted.\n\n    Returns:\n        cupyx.scipy.sparse.csr_matrix: A converted matrix.\n\n    """"""\n    assert x.ndim == 2\n    x = cupy.asfortranarray(x)\n    nnz = numpy.empty((), dtype=\'i\')\n    handle = device.get_cusparse_handle()\n    m, n = x.shape\n\n    descr = MatDescriptor.create()\n    nnz_per_row = cupy.empty(m, \'i\')\n    _call_cusparse(\n        \'nnz\', x.dtype,\n        handle, cusparse.CUSPARSE_DIRECTION_ROW, m, n, descr.descriptor,\n        x.data.ptr, m, nnz_per_row.data.ptr, nnz.ctypes.data)\n\n    nnz = int(nnz)\n    data = cupy.empty(nnz, x.dtype)\n    indptr = cupy.empty(m + 1, \'i\')\n    indices = cupy.empty(nnz, \'i\')\n\n    _call_cusparse(\n        \'dense2csr\', x.dtype,\n        handle, m, n, descr.descriptor,\n        x.data.ptr, m, nnz_per_row.data.ptr,\n        data.data.ptr, indptr.data.ptr, indices.data.ptr)\n    # Note that a desciptor is recreated\n    csr = cupyx.scipy.sparse.csr_matrix((data, indices, indptr), shape=x.shape)\n    csr._has_canonical_format = True\n    return csr\n\n\ndef csr2csr_compress(x, tol):\n    assert x.dtype.char in \'fdFD\'\n\n    handle = device.get_cusparse_handle()\n    m, n = x.shape\n\n    nnz_per_row = cupy.empty(m, \'i\')\n    nnz = _call_cusparse(\n        \'nnz_compress\', x.dtype,\n        handle, m, x._descr.descriptor,\n        x.data.data.ptr, x.indptr.data.ptr, nnz_per_row.data.ptr, tol)\n    data = cupy.zeros(nnz, x.dtype)\n    indptr = cupy.empty(m + 1, \'i\')\n    indices = cupy.zeros(nnz, \'i\')\n    _call_cusparse(\n        \'csr2csr_compress\', x.dtype,\n        handle, m, n, x._descr.descriptor,\n        x.data.data.ptr, x.indices.data.ptr, x.indptr.data.ptr,\n        x.nnz, nnz_per_row.data.ptr, data.data.ptr, indices.data.ptr,\n        indptr.data.ptr, tol)\n\n    return cupyx.scipy.sparse.csr_matrix(\n        (data, indices, indptr), shape=x.shape)\n\n\ndef _dtype_to_IndexType(dtype):\n    if dtype == \'uint16\':\n        return cusparse.CUSPARSE_INDEX_16U\n    elif dtype == \'int32\':\n        return cusparse.CUSPARSE_INDEX_32I\n    elif dtype == \'int64\':\n        return cusparse.CUSPARSE_INDEX_64I\n    else:\n        raise TypeError\n\n\nclass BaseDescriptor(object):\n\n    def __init__(self, descriptor, get=None, destroyer=None):\n        self.desc = descriptor\n        self.get = get\n        self.destroy = destroyer\n\n    def __del__(self, is_shutting_down=util.is_shutting_down):\n        if is_shutting_down():\n            return\n        if self.destroy is None:\n            self.desc = None\n        elif self.desc is not None:\n            self.destroy(self.desc)\n            self.desc = None\n\n    def __getattr__(self, name):\n        if self.get is not None:\n            return getattr(self.get(self.desc), name)\n        raise AttributeError\n\n\nclass SpMatDescriptor(BaseDescriptor):\n\n    @classmethod\n    def create(cls, a):\n        assert cupyx.scipy.sparse.issparse(a)\n        rows, cols = a.shape\n        idx_base = cusparse.CUSPARSE_INDEX_BASE_ZERO\n        cuda_dtype = _dtype_to_DataType(a.dtype)\n        if a.format == \'csr\':\n            desc = cusparse.createCsr(\n                rows, cols, a.nnz, a.indptr.data.ptr, a.indices.data.ptr,\n                a.data.data.ptr, _dtype_to_IndexType(a.indptr.dtype),\n                _dtype_to_IndexType(a.indices.dtype), idx_base, cuda_dtype)\n            get = cusparse.csrGet\n        elif a.format == \'coo\':\n            desc = cusparse.createCoo(\n                rows, cols, a.nnz, a.row.data.ptr, a.col.data.ptr,\n                a.data.data.ptr, _dtype_to_IndexType(a.row.dtype),\n                idx_base, cuda_dtype)\n            get = cusparse.cooGet\n        else:\n            raise ValueError(\'csr and coo format are supported \'\n                             \'(actual: {}).\'.format(a.format))\n        destroy = cusparse.destroySpMat\n        return SpMatDescriptor(desc, get, destroy)\n\n\nclass DnVecDescriptor(BaseDescriptor):\n\n    @classmethod\n    def create(cls, x):\n        cuda_dtype = _dtype_to_DataType(x.dtype)\n        desc = cusparse.createDnVec(x.size, x.data.ptr, cuda_dtype)\n        get = cusparse.dnVecGet\n        destroy = cusparse.destroyDnVec\n        return DnVecDescriptor(desc, get, destroy)\n\n\nclass DnMatDescriptor(BaseDescriptor):\n\n    @classmethod\n    def create(cls, a):\n        assert a.ndim == 2\n        assert a.flags.f_contiguous\n        rows, cols = a.shape\n        ld = rows\n        cuda_dtype = _dtype_to_DataType(a.dtype)\n        desc = cusparse.createDnMat(rows, cols, ld, a.data.ptr, cuda_dtype,\n                                    cusparse.CUSPARSE_ORDER_COL)\n        get = cusparse.dnMatGet\n        destroy = cusparse.destroyDnMat\n        return DnMatDescriptor(desc, get, destroy)\n\n\ndef spmv(a, x, y=None, alpha=1, beta=0, transa=False):\n    """"""Multiplication of sparse matrix and dense vector.\n\n    .. math::\n\n        y = \\\\alpha * op(A) x + \\\\beta * y\n\n    Args:\n        a (cupyx.scipy.sparse.csr_matrix, csc_matrix or coo_matrix):\n            Sparse matrix A\n        x (cupy.ndarray): Dense vector x\n        y (cupy.ndarray or None): Dense vector y\n        alpha (scalar): Coefficent\n        beta (scalar): Coefficent\n        transa (bool): If ``True``, op(A) = transpose of A.\n\n    Returns:\n        cupy.ndarray\n    """"""\n    if not check_availability(\'spmv\'):\n        raise RuntimeError(\'spmv is not available.\')\n\n    if isinstance(a, cupyx.scipy.sparse.csc_matrix):\n        aT = a.T\n        if not isinstance(aT, cupyx.scipy.sparse.csr_matrix):\n            msg = \'aT must be csr_matrix (actual: {})\'.format(type(aT))\n            raise TypeError(msg)\n        a = aT\n        transa = not transa\n    if not (isinstance(a, cupyx.scipy.sparse.csr_matrix) or\n            isinstance(a, cupyx.scipy.sparse.coo_matrix)):\n        raise TypeError(\'unsupported type (actual: {})\'.format(type(a)))\n    a_shape = a.shape if not transa else a.shape[::-1]\n    if a_shape[1] != len(x):\n        raise ValueError(\'dimension mismatch\')\n    assert a.has_canonical_format\n\n    m, n = a_shape\n    a, x, y = _cast_common_type(a, x, y)\n    if y is None:\n        y = cupy.zeros(m, a.dtype)\n    elif len(y) != m:\n        raise ValueError(\'dimension mismatch\')\n    if a.nnz == 0:\n        y[...] = 0\n        return y\n\n    desc_a = SpMatDescriptor.create(a)\n    desc_x = DnVecDescriptor.create(x)\n    desc_y = DnVecDescriptor.create(y)\n\n    handle = device.get_cusparse_handle()\n    op_a = _transpose_flag(transa)\n    alpha = numpy.array(alpha, a.dtype).ctypes\n    beta = numpy.array(beta, a.dtype).ctypes\n    cuda_dtype = _dtype_to_DataType(a.dtype)\n    alg = cusparse.CUSPARSE_MV_ALG_DEFAULT\n    buff_size = cusparse.spMV_bufferSize(handle, op_a, alpha.data,\n                                         desc_a.desc, desc_x.desc, beta.data,\n                                         desc_y.desc, cuda_dtype, alg)\n    buff = cupy.empty(buff_size, cupy.int8)\n    cusparse.spMV(handle, op_a, alpha.data, desc_a.desc, desc_x.desc,\n                  beta.data, desc_y.desc, cuda_dtype, alg, buff.data.ptr)\n\n    return y\n\n\ndef spmm(a, b, c=None, alpha=1, beta=0, transa=False, transb=False):\n    """"""Multiplication of sparse matrix and dense matrix.\n\n    .. math::\n\n        C = \\\\alpha * op(A) op(B) + \\\\beta * C\n\n    Args:\n        a (cupyx.scipy.sparse.csr_matrix, csc_matrix or coo_matrix):\n            Sparse matrix A\n        b (cupy.ndarray): Dense matrix B\n        c (cupy.ndarray or None): Dense matrix C\n        alpha (scalar): Coefficent\n        beta (scalar): Coefficent\n        transa (bool): If ``True``, op(A) = transpose of A.\n        transb (bool): If ``True``, op(B) = transpose of B.\n\n    Returns:\n        cupy.ndarray\n    """"""\n    if not check_availability(\'spmm\'):\n        raise RuntimeError(\'spmm is not available.\')\n\n    assert a.ndim == b.ndim == 2\n    assert b.flags.f_contiguous\n    assert c is None or c.flags.f_contiguous\n\n    if isinstance(a, cupyx.scipy.sparse.csc_matrix):\n        aT = a.T\n        if not isinstance(aT, cupyx.scipy.sparse.csr_matrix):\n            msg = \'aT must be csr_matrix (actual: {})\'.format(type(aT))\n            raise TypeError(msg)\n        a = aT\n        transa = not transa\n    if not (isinstance(a, cupyx.scipy.sparse.csr_matrix) or\n            isinstance(a, cupyx.scipy.sparse.coo_matrix)):\n        raise TypeError(\'unsupported type (actual: {})\'.format(type(a)))\n    a_shape = a.shape if not transa else a.shape[::-1]\n    b_shape = b.shape if not transb else b.shape[::-1]\n    if a_shape[1] != b_shape[0]:\n        raise ValueError(\'dimension mismatch\')\n    assert a.has_canonical_format\n\n    m, k = a_shape\n    _, n = b_shape\n    a, b, c = _cast_common_type(a, b, c)\n    if c is None:\n        c = cupy.zeros((m, n), a.dtype, \'F\')\n    elif c.shape[0] != m or c.shape[1] != n:\n        raise ValueError(\'dimension mismatch\')\n    if a.nnz == 0:\n        c[...] = 0\n        return c\n\n    desc_a = SpMatDescriptor.create(a)\n    desc_b = DnMatDescriptor.create(b)\n    desc_c = DnMatDescriptor.create(c)\n\n    handle = device.get_cusparse_handle()\n    op_a = _transpose_flag(transa)\n    op_b = _transpose_flag(transb)\n    alpha = numpy.array(alpha, a.dtype).ctypes\n    beta = numpy.array(beta, a.dtype).ctypes\n    cuda_dtype = _dtype_to_DataType(a.dtype)\n    alg = cusparse.CUSPARSE_MM_ALG_DEFAULT\n    buff_size = cusparse.spMM_bufferSize(handle, op_a, op_b, alpha.data,\n                                         desc_a.desc, desc_b.desc, beta.data,\n                                         desc_c.desc, cuda_dtype, alg)\n    buff = cupy.empty(buff_size, cupy.int8)\n    buff_size = cusparse.spMM(handle, op_a, op_b, alpha.data, desc_a.desc,\n                              desc_b.desc, beta.data, desc_c.desc,\n                              cuda_dtype, alg, buff.data.ptr)\n\n    return c\n'"
cupy/cutensor.py,0,"b'import numpy\nimport warnings\n\nimport cupy\nfrom cupy.cuda import cutensor\nfrom cupy.cuda import device\nfrom cupy.cuda import runtime\nfrom cupy import util\n\n_handles = {}\n_tensor_descriptors = {}\n_contraction_descriptors = {}\n_contraction_finds = {}\n_contraction_plans = {}\n\n\nclass Descriptor(object):\n\n    def __init__(self, descriptor, destroyer=None):\n        self.value = descriptor\n        self.destroy = destroyer\n\n    def __del__(self, is_shutting_down=util.is_shutting_down):\n        if is_shutting_down():\n            return\n        if self.destroy is None:\n            self.value = None\n        elif self.value is not None:\n            self.destroy(self.value)\n            self.value = None\n\n\ndef get_handle():\n    dev = device.get_device_id()\n    if dev not in _handles:\n        handle = cutensor.Handle()\n        cutensor.init(handle)\n        _handles[dev] = handle\n    return _handles[dev]\n\n\ndef get_cuda_dtype(numpy_dtype):\n    if numpy_dtype == numpy.float16:\n        return runtime.CUDA_R_16F\n    elif numpy_dtype == numpy.float32:\n        return runtime.CUDA_R_32F\n    elif numpy_dtype == numpy.float64:\n        return runtime.CUDA_R_64F\n    elif numpy_dtype == numpy.complex64:\n        return runtime.CUDA_C_32F\n    elif numpy_dtype == numpy.complex128:\n        return runtime.CUDA_C_64F\n    else:\n        raise TypeError(\'Dtype {} is not supported\'.format(numpy_dtype))\n\n\ndef get_cutensor_dtype(numpy_dtype):\n    if numpy_dtype == numpy.float16:\n        return cutensor.R_MIN_16F\n    elif numpy_dtype == numpy.float32:\n        return cutensor.R_MIN_32F\n    elif numpy_dtype == numpy.float64:\n        return cutensor.R_MIN_64F\n    elif numpy_dtype == numpy.complex64:\n        return cutensor.C_MIN_32F\n    elif numpy_dtype == numpy.complex128:\n        return cutensor.C_MIN_64F\n    else:\n        raise TypeError(\'Dtype {} is not supported\'.format(numpy_dtype))\n\n\ndef _convert_mode(mode):\n    return numpy.array([ord(x) if isinstance(x, str) else x for x in mode],\n                       dtype=numpy.int32)\n\n\ndef _set_compute_dtype(array_dtype, compute_dtype=None):\n    if compute_dtype is None:\n        if array_dtype == numpy.float16:\n            compute_dtype = numpy.float32\n        else:\n            compute_dtype = array_dtype\n    return compute_dtype\n\n\ndef create_tensor_descriptor(a, uop=cutensor.OP_IDENTITY):\n    """"""Create a tensor descriptor\n\n    Args:\n        a (cupy.ndarray): tensor for which a descritpor are created.\n        uop (cutensorOperator_t): unary operator that will be applied to each\n            element of the corresponding tensor in a lazy fashion (i.e., the\n            algorithm uses this tensor as its operand only once). The\n            original data of this tensor remains unchanged.\n\n    Returns:\n        (Descriptor): A instance of class Descriptor which holds a pointer to\n            tensor descriptor and its destructor.\n    """"""\n    handle = get_handle()\n    key = (handle.ptr, a.dtype, tuple(a.shape), tuple(a.strides), uop)\n    if key in _tensor_descriptors:\n        desc = _tensor_descriptors[key]\n        return desc\n    num_modes = a.ndim\n    extent = numpy.array(a.shape, dtype=numpy.int64)\n    stride = numpy.array(a.strides, dtype=numpy.int64) // a.itemsize\n    cuda_dtype = get_cuda_dtype(a.dtype)\n    desc = cutensor.TensorDescriptor()\n    cutensor.initTensorDescriptor(\n        handle, desc, num_modes, extent.ctypes.data, stride.ctypes.data,\n        cuda_dtype, uop)\n    _tensor_descriptors[key] = desc\n    return desc\n\n\ndef elementwise_trinary(alpha, A, desc_A, mode_A,\n                        beta, B, desc_B, mode_B,\n                        gamma, C, desc_C, mode_C,\n                        out=None,\n                        op_AB=cutensor.OP_ADD, op_ABC=cutensor.OP_ADD,\n                        compute_dtype=None):\n    """"""Element-wise tensor operation for three input tensors\n\n    This function performs a element-wise tensor operation of the form:\n\n        D_{Pi^C(i_0,i_1,...,i_nc)} =\n            op_ABC(op_AB(alpha * uop_A(A_{Pi^A(i_0,i_1,...,i_na)}),\n                         beta  * uop_B(B_{Pi^B(i_0,i_1,...,i_nb)})),\n                         gamma * uop_C(C_{Pi^C(i_0,i_1,...,i_nc)}))\n\n    See cupy/cuda/cutensor.elementwiseTrinary() for details.\n\n    Args:\n        alpha: Scaling factor for tensor A.\n        A (cupy.ndarray): Input tensor.\n        desc_A (class Descriptor): A descriptor that holds the information\n            about the data type, modes, and strides of tensor A.\n        mode_A (tuple of int/str): A tuple that holds the labels of the modes\n            of tensor A (e.g., if A_{x,y,z}, mode_A = {\'x\',\'y\',\'z\'})\n        beta: Scaling factor for tensor B.\n        B (cupy.ndarray): Input tensor.\n        desc_B (class Descriptor): A descriptor that holds the information\n            about the data type, modes, and strides of tensor B.\n        mode_B (tuple of int/str): A tuple that holds the labels of the modes\n            of tensor B.\n        gamma: Scaling factor for tensor C.\n        C (cupy.ndarray): Input tensor.\n        desc_C (class Descriptor): A descriptor that holds the information\n            about the data type, modes, and strides of tensor C.\n        mode_C (tuple of int/str): A tuple that holds the labels of the modes\n            of tensor C.\n        out (cupy.ndarray): Output tensor.\n        op_AB (cutensorOperator_t): Element-wise binary operator.\n        op_ABC (cutensorOperator_t): Element-wise binary operator.\n        compute_dtype (numpy.dtype): Compute type for the intermediate\n            computation.\n\n    Returns:\n        out (cupy.ndarray): Output tensor.\n\n    Examples:\n        See examples/cutensor/elementwise_trinary.py\n    """"""\n    if not (A.dtype == B.dtype == C.dtype):\n        raise ValueError(\n            \'dtype mismatch: ({}, {}, {})\'.format(A.dtype, B.dtype, C.dtype))\n    if not (A.flags.c_contiguous\n            and B.flags.c_contiguous\n            and C.flags.c_contiguous):\n        raise ValueError(\'The inputs should be contiguous arrays.\')\n\n    if out is None:\n        out = cupy.ndarray(C.shape, dtype=C.dtype)\n    elif C.dtype != out.dtype:\n        raise ValueError(\'dtype mismatch: {} != {}\'.format(C.dtype, out.dtype))\n    elif C.shape != out.shape:\n        raise ValueError(\'shape mismatch: {} != {}\'.format(C.shape, out.shape))\n    elif not out.flags.c_contiguous:\n        raise ValueError(\'`out` should be a contiguous array.\')\n\n    if A.ndim != len(mode_A):\n        raise ValueError(\'ndim mismatch: {} != {}\'.format(A.ndim, len(mode_A)))\n    if B.ndim != len(mode_B):\n        raise ValueError(\'ndim mismatch: {} != {}\'.format(B.ndim, len(mode_B)))\n    if C.ndim != len(mode_C):\n        raise ValueError(\'ndim mismatch: {} != {}\'.format(C.ndim, len(mode_C)))\n\n    mode_A = _convert_mode(mode_A)\n    mode_B = _convert_mode(mode_B)\n    mode_C = _convert_mode(mode_C)\n\n    if compute_dtype is None:\n        compute_dtype = A.dtype\n    alpha = numpy.array(alpha, compute_dtype)\n    beta = numpy.array(beta, compute_dtype)\n    gamma = numpy.array(gamma, compute_dtype)\n    handle = get_handle()\n    cuda_dtype = get_cuda_dtype(compute_dtype)\n    cutensor.elementwiseTrinary(\n        handle,\n        alpha.ctypes.data,\n        A.data.ptr, desc_A, mode_A.ctypes.data,\n        beta.ctypes.data,\n        B.data.ptr, desc_B, mode_B.ctypes.data,\n        gamma.ctypes.data,\n        C.data.ptr, desc_C, mode_C.ctypes.data,\n        out.data.ptr, desc_C, mode_C.ctypes.data,\n        op_AB, op_ABC, cuda_dtype)\n    return out\n\n\ndef elementwise_binary(alpha, A, desc_A, mode_A,\n                       gamma, C, desc_C, mode_C,\n                       out=None,\n                       op_AC=cutensor.OP_ADD,\n                       compute_dtype=None):\n    """"""Element-wise tensor operation for two input tensors\n\n    This function performs a element-wise tensor operation of the form:\n\n        D_{Pi^C(i_0,i_1,...,i_n)} =\n            op_AC(alpha * uop_A(A_{Pi^A(i_0,i_1,...,i_n)}),\n                  gamma * uop_C(C_{Pi^C(i_0,i_1,...,i_n)}))\n\n    See elementwise_trinary() for details.\n\n    Examples:\n        See examples/cutensor/elementwise_binary.py\n    """"""\n    if A.dtype != C.dtype:\n        raise ValueError(\'dtype mismatch: {} != {}\'.format(A.dtype, C.dtype))\n    if not (A.flags.c_contiguous and C.flags.c_contiguous):\n        raise ValueError(\'The inputs should be contiguous arrays.\')\n\n    if out is None:\n        out = cupy.ndarray(C.shape, dtype=C.dtype)\n    elif C.dtype != out.dtype:\n        raise ValueError(\'dtype mismatch: {} != {}\'.format(C.dtype, out.dtype))\n    elif C.shape != out.shape:\n        raise ValueError(\'shape mismatch: {} != {}\'.format(C.shape, out.shape))\n    elif not out.flags.c_contiguous:\n        raise ValueError(\'`out` should be a contiguous array.\')\n\n    if A.ndim != len(mode_A):\n        raise ValueError(\'ndim mismatch: {} != {}\'.format(A.ndim, len(mode_A)))\n    if C.ndim != len(mode_C):\n        raise ValueError(\'ndim mismatch: {} != {}\'.format(C.ndim, len(mode_C)))\n\n    mode_A = _convert_mode(mode_A)\n    mode_C = _convert_mode(mode_C)\n\n    if compute_dtype is None:\n        compute_dtype = A.dtype\n    alpha = numpy.array(alpha, compute_dtype)\n    gamma = numpy.array(gamma, compute_dtype)\n    handle = get_handle()\n    cuda_dtype = get_cuda_dtype(compute_dtype)\n    cutensor.elementwiseBinary(\n        handle,\n        alpha.ctypes.data,\n        A.data.ptr, desc_A, mode_A.ctypes.data,\n        gamma.ctypes.data,\n        C.data.ptr, desc_C, mode_C.ctypes.data,\n        out.data.ptr, desc_C, mode_C.ctypes.data,\n        op_AC, cuda_dtype)\n    return out\n\n\ndef _create_contraction_descriptor(A, desc_A, mode_A, B, desc_B, mode_B,\n                                   C, desc_C, mode_C, compute_dtype=None):\n    """"""Create a contraction descriptor""""""\n    assert A.dtype == B.dtype == C.dtype\n    assert A.ndim == len(mode_A)\n    assert B.ndim == len(mode_B)\n    assert C.ndim == len(mode_C)\n    compute_dtype = _set_compute_dtype(A.dtype, compute_dtype)\n    handle = get_handle()\n    alignment_req_A = cutensor.getAlignmentRequirement(\n        handle, A.data.ptr, desc_A)\n    alignment_req_B = cutensor.getAlignmentRequirement(\n        handle, B.data.ptr, desc_B)\n    alignment_req_C = cutensor.getAlignmentRequirement(\n        handle, C.data.ptr, desc_C)\n    key = (handle.ptr, compute_dtype,\n           desc_A.ptr, tuple(mode_A), alignment_req_A,\n           desc_B.ptr, tuple(mode_B), alignment_req_B,\n           desc_C.ptr, tuple(mode_C), alignment_req_C)\n    if key in _contraction_descriptors:\n        desc = _contraction_descriptors[key]\n        return desc\n    mode_A = _convert_mode(mode_A)\n    mode_B = _convert_mode(mode_B)\n    mode_C = _convert_mode(mode_C)\n    cutensor_dtype = get_cutensor_dtype(compute_dtype)\n    desc = cutensor.ContractionDescriptor()\n    cutensor.initContractionDescriptor(\n        handle,\n        desc,\n        desc_A, mode_A.ctypes.data, alignment_req_A,\n        desc_B, mode_B.ctypes.data, alignment_req_B,\n        desc_C, mode_C.ctypes.data, alignment_req_C,\n        desc_C, mode_C.ctypes.data, alignment_req_C,\n        cutensor_dtype)\n    _contraction_descriptors[key] = desc\n    return desc\n\n\ndef _create_contraction_plan(desc, algo, ws_pref):\n    """"""Create a contraction plan""""""\n    handle = get_handle()\n    key = (handle.ptr, algo)\n    if key in _contraction_finds:\n        find = _contraction_finds[key]\n    else:\n        find = cutensor.ContractionFind()\n        cutensor.initContractionFind(handle, find, algo)\n        _contraction_finds[key] = find\n\n    ws_allocation_success = False\n    for pref in (ws_pref, cutensor.WORKSPACE_MIN):\n        ws_size = cutensor.contractionGetWorkspace(handle, desc, find, pref)\n        try:\n            ws = cupy.ndarray((ws_size,), dtype=numpy.int8)\n            ws_allocation_success = True\n        except Exception:\n            warnings.warn(\'cuTENSOR: failed to allocate memory of workspace \'\n                          \'with preference ({}) and size ({}).\'\n                          \'\'.format(pref, ws_size))\n        if ws_allocation_success:\n            break\n    if not ws_allocation_success:\n        raise RuntimeError(\'cuTENSOR: failed to allocate memory of workspace.\')\n\n    key = (handle.ptr, desc.ptr, find.ptr, ws_size)\n    if key in _contraction_plans:\n        plan = _contraction_plans[key]\n    else:\n        plan = cutensor.ContractionPlan()\n        cutensor.initContractionPlan(handle, plan, desc, find, ws_size)\n        _contraction_plans[key] = plan\n\n    return plan, ws, ws_size\n\n\ndef contraction(alpha, A, desc_A, mode_A, B, desc_B, mode_B,\n                beta, C, desc_C, mode_C, compute_dtype=None,\n                algo=cutensor.ALGO_DEFAULT,\n                ws_pref=cutensor.WORKSPACE_RECOMMENDED):\n    """"""General tensor contraction\n\n    This routine computes the tensor contraction:\n\n        C = alpha * uop_A(A) * uop_B(B) + beta * uop_C(C)\n\n    See cupy/cuda/cutensor.contraction for details.\n\n    Args:\n        alpha: Scaling factor for A * B.\n        A (cupy.ndarray): Input tensor.\n        desc_A (class Descriptor): A descriptor that holds the information\n            about the data type, modes, and strides of tensor A.\n        mode_A (tuple of int/str): A tuple that holds the labels of the modes\n            of tensor A (e.g., if A_{x,y,z}, mode_A = {\'x\',\'y\',\'z\'})\n        B (cupy.ndarray): Input tensor.\n        desc_B (class Descriptor): A descriptor that holds the information\n            about the data type, modes, and strides of tensor B.\n        mode_B (tuple of int/str): A tuple that holds the labels of the modes\n            of tensor B.\n        beta: Scaling factor for C.\n        C (cupy.ndarray): Input/output tensor.\n        desc_C (class Descriptor): A descriptor that holds the information\n            about the data type, modes, and strides of tensor C.\n        mode_C (tuple of int/str): A tuple that holds the labels of the modes\n            of tensor C.\n        compute_dtype (numpy.dtype): Compute type for the intermediate\n            computation.\n        algo (cutenorAlgo_t): Allows users to select a specific algorithm.\n            ALGO_DEFAULT lets the heuristic choose the algorithm.\n            Any value >= 0 selects a specific GEMM-like algorithm and\n            deactivates the heuristic. If a specified algorithm is not\n            supported, STATUS_NOT_SUPPORTED is returned.\n        ws_pref (cutensorWorksizePreference_t): User preference for the\n            workspace of cuTensor.\n\n    Returns:\n        out (cupy.ndarray): Output tensor.\n\n    Examples:\n        See examples/cutensor/contraction.py\n    """"""\n    if not (A.dtype == B.dtype == C.dtype):\n        raise ValueError(\n            \'dtype mismatch: ({}, {}, {})\'.format(A.dtype, B.dtype, C.dtype))\n    if not (A.flags.c_contiguous\n            and B.flags.c_contiguous\n            and C.flags.c_contiguous):\n        raise ValueError(\'The inputs should be contiguous arrays.\')\n\n    if A.ndim != len(mode_A):\n        raise ValueError(\'ndim mismatch: {} != {}\'.format(A.ndim, len(mode_A)))\n    if B.ndim != len(mode_B):\n        raise ValueError(\'ndim mismatch: {} != {}\'.format(B.ndim, len(mode_B)))\n    if C.ndim != len(mode_C):\n        raise ValueError(\'ndim mismatch: {} != {}\'.format(C.ndim, len(mode_C)))\n\n    out = C\n    compute_dtype = _set_compute_dtype(A.dtype, compute_dtype)\n    handle = get_handle()\n    alpha = numpy.array(alpha, compute_dtype)\n    beta = numpy.array(beta, compute_dtype)\n    desc = _create_contraction_descriptor(A, desc_A, mode_A,\n                                          B, desc_B, mode_B,\n                                          C, desc_C, mode_C,\n                                          compute_dtype=compute_dtype)\n    plan, ws, ws_size = _create_contraction_plan(desc, algo, ws_pref)\n    cutensor.contraction(handle, plan,\n                         alpha.ctypes.data, A.data.ptr, B.data.ptr,\n                         beta.ctypes.data, C.data.ptr, out.data.ptr,\n                         ws.data.ptr, ws_size)\n    return out\n\n\ndef contraction_max_algos():\n    """"""Returns the maximum number of algorithms for cutensor()\n\n    See cupy/cuda/cutensor.contractionMaxAlgos() for details.\n    """"""\n    return cutensor.contractionMaxAlgos()\n\n\ndef reduction(alpha, A, desc_A, mode_A, beta, C, desc_C, mode_C,\n              reduce_op=cutensor.OP_ADD, compute_dtype=None):\n    """"""Tensor reduction\n\n    This routine computes the tensor reduction:\n\n        C = alpha * reduce_op(uop_A(A)) + beta * uop_C(C))\n\n    See :func:`cupy.cuda.cutensor.reduction` for details.\n\n    Args:\n        alpha: Scaling factor for A.\n        A (cupy.ndarray): Input tensor.\n        desc_A (class Descriptor): A descriptor that holds the information\n            about the data type, modes, strides and unary operator (uop_A) of\n            tensor A.\n        mode_A (tuple of int/str): A tuple that holds the labels of the modes\n            of tensor A (e.g., if A_{x,y,z}, mode_A = {\'x\',\'y\',\'z\'})\n        beta: Scaling factor for C.\n        C (cupy.ndarray): Input/output tensor.\n        desc_C (class Descriptor): A descriptor that holds the information\n            about the data type, modes, strides and unary operator (uop_C) of\n            tensor C.\n        mode_C (tuple of int/str): A tuple that holds the labels of the modes\n            of tensor C.\n        reduce_op (cutensorOperator_t): Binary operator used to reduce A.\n        compute_dtype (numpy.dtype): Compute type for the intermediate\n            computation.\n\n    Returns:\n        out (cupy.ndarray): Output tensor.\n\n    Examples:\n        See examples/cutensor/reduction.py\n    """"""\n    if A.dtype != C.dtype:\n        raise ValueError(\'dtype mismatch: {} != {}\'.format(A.dtype, C.dtype))\n    if not (A.flags.c_contiguous and C.flags.c_contiguous):\n        raise ValueError(\'The inputs should be contiguous arrays.\')\n\n    if A.ndim != len(mode_A):\n        raise ValueError(\'ndim mismatch: {} != {}\'.format(A.ndim, len(mode_A)))\n    if C.ndim != len(mode_C):\n        raise ValueError(\'ndim mismatch: {} != {}\'.format(C.ndim, len(mode_C)))\n\n    mode_A = _convert_mode(mode_A)\n    mode_C = _convert_mode(mode_C)\n    out = C\n    compute_dtype = _set_compute_dtype(A.dtype, compute_dtype)\n    alpha = numpy.array(alpha, compute_dtype)\n    beta = numpy.array(beta, compute_dtype)\n    handle = get_handle()\n    cutensor_dtype = get_cutensor_dtype(compute_dtype)\n    ws_size = cutensor.reductionGetWorkspace(\n        handle,\n        A.data.ptr, desc_A, mode_A.ctypes.data,\n        C.data.ptr, desc_C, mode_C.ctypes.data,\n        out.data.ptr, desc_C, mode_C.ctypes.data,\n        reduce_op, cutensor_dtype)\n    try:\n        ws = cupy.ndarray((ws_size,), dtype=numpy.int8)\n    except cupy.cuda.memory.OutOfMemoryError:\n        warnings.warn(\'cuTENSOR: failed to allocate memory of workspace \'\n                      \'(size: {}).\'.format(ws_size))\n        ws_size = 0\n        ws = cupy.ndarray((ws_size,), dtype=numpy.int8)\n    cutensor.reduction(handle,\n                       alpha.ctypes.data,\n                       A.data.ptr, desc_A, mode_A.ctypes.data,\n                       beta.ctypes.data,\n                       C.data.ptr, desc_C, mode_C.ctypes.data,\n                       out.data.ptr, desc_C, mode_C.ctypes.data,\n                       reduce_op, cutensor_dtype, ws.data.ptr, ws_size)\n    return out\n'"
cupyx/__init__.py,0,"b'# ""NOQA"" to suppress flake8 warning\nfrom cupyx.rsqrt import rsqrt  # NOQA\nfrom cupyx.runtime import get_runtime_info  # NOQA\nfrom cupyx.scatter import scatter_add  # NOQA\nfrom cupyx.scatter import scatter_max  # NOQA\nfrom cupyx.scatter import scatter_min  # NOQA\n\nfrom cupyx import linalg  # NOQA\nfrom cupyx import time  # NOQA\nfrom cupyx import scipy  # NOQA\n\nfrom cupyx._ufunc_config import errstate  # NOQA\nfrom cupyx._ufunc_config import geterr  # NOQA\nfrom cupyx._ufunc_config import seterr  # NOQA\n\nfrom cupy.core.syncdetect import allow_synchronize  # NOQA\nfrom cupy.core.syncdetect import DeviceSynchronized  # NOQA\n'"
cupyx/_ufunc_config.py,0,"b'import contextlib\nimport threading\n\n\n_config = threading.local()\n\n\ndef get_config_divide():\n    try:\n        value = _config.divide\n    except AttributeError:\n        value = _config.divide = None\n    return value\n\n\ndef get_config_over():\n    try:\n        value = _config.over\n    except AttributeError:\n        value = _config.over = None\n    return value\n\n\ndef get_config_under():\n    try:\n        value = _config.under\n    except AttributeError:\n        value = _config.under = None\n    return value\n\n\ndef get_config_invalid():\n    try:\n        value = _config.invalid\n    except AttributeError:\n        value = _config.invalid = None\n    return value\n\n\ndef get_config_linalg():\n    # In favor of performance, the `devInfo` input/output from cuSOLVER routine\n    # calls that is necessary to check the validity of the other outputs, are\n    # ignored, as D2H copy incurring device synchronizations would otherwise be\n    # required.\n    try:\n        value = _config.linalg\n    except AttributeError:\n        value = _config.linalg = \'ignore\'\n    return value\n\n\ndef get_config_fallback_mode():\n    try:\n        value = _config.fallback_mode\n    except AttributeError:\n        value = _config.fallback_mode = \'ignore\'\n    return value\n\n\n@contextlib.contextmanager\ndef errstate(*, divide=None, over=None, under=None,\n             invalid=None, linalg=None, fallback_mode=None):\n    """"""\n    TODO(hvy): Write docs.\n    """"""\n    old_state = seterr(\n        divide=divide, over=over, under=under,\n        invalid=invalid, linalg=linalg, fallback_mode=fallback_mode)\n    try:\n        yield  # Return `None` similar to `numpy.errstate`.\n    finally:\n        seterr(**old_state)\n\n\ndef seterr(*, divide=None, over=None, under=None,\n           invalid=None, linalg=None, fallback_mode=None):\n    """"""\n    TODO(hvy): Write docs.\n    """"""\n    old_state = geterr()\n\n    if divide is not None:\n        raise NotImplementedError()\n    if over is not None:\n        raise NotImplementedError()\n    if under is not None:\n        raise NotImplementedError()\n    if invalid is not None:\n        raise NotImplementedError()\n    if linalg is not None:\n        if linalg not in (\'ignore\', \'raise\'):\n            raise NotImplementedError()\n    if fallback_mode is not None:\n        if fallback_mode in [\'print\', \'warn\', \'ignore\', \'raise\']:\n            _config.fallback_mode = fallback_mode\n        elif fallback_mode in [\'log\', \'call\']:\n            raise NotImplementedError\n        else:\n            raise ValueError(\n                \'{} is not a valid dispatch type\'.format(fallback_mode))\n\n    _config.divide = divide\n    _config.under = under\n    _config.over = over\n    _config.invalid = invalid\n    _config.linalg = linalg\n\n    return old_state\n\n\ndef geterr():\n    """"""\n    TODO(hvy): Write docs.\n    """"""\n    return dict(\n        divide=get_config_divide(),\n        over=get_config_over(),\n        under=get_config_under(),\n        invalid=get_config_invalid(),\n        linalg=get_config_linalg(),\n        fallback_mode=get_config_fallback_mode(),\n    )\n'"
cupyx/rsqrt.py,0,"b""from cupy.core.core import create_ufunc\n\nrsqrt = create_ufunc(\n    'cupy_rsqrt',\n    ('e->e', 'f->f', 'd->d', 'F->F', 'D->D'),\n    'out0 = rsqrt(in0)',\n    doc='''Returns the reciprocal square root.''')\n"""
cupyx/runtime.py,0,"b'import inspect\nimport io\nimport os\n\nimport cupy\n\ntry:\n    import cupy.cuda.cudnn as cudnn\nexcept ImportError:\n    cudnn = None\n\ntry:\n    import cupy.cuda.nccl as nccl\nexcept ImportError:\n    nccl = None\n\ntry:\n    import cupy.cuda.cub as cub\nexcept ImportError:\n    cub = None\n\ntry:\n    import cupy.cuda.cutensor as cutensor\nexcept ImportError:\n    cutensor = None\n\n\ndef _eval_or_error(func, errors):\n    # Evaluates `func` and return the result.\n    # If an error specified by `errors` occured, it returns a string\n    # representing the error.\n    try:\n        return func()\n    except errors as e:\n        return repr(e)\n\n\nclass _InstallInfo(object):\n\n    # TODO(niboshi): Add is_binary_distribution\n\n    def __init__(self):\n        cupy_package_root = self._get_cupy_package_root()\n        if cupy_package_root is not None:\n            data_root = os.path.join(cupy_package_root, \'.data\')\n            data_paths = {\n                \'lib\': _dir_or_none(os.path.join(data_root, \'lib\')),\n                \'include\': _dir_or_none(os.path.join(data_root, \'include\')),\n            }\n        else:\n            data_paths = {\n                \'lib\': None,\n                \'include\': None,\n            }\n\n        self.cupy_package_root = cupy_package_root\n        self.data_paths = data_paths\n\n    def get_data_path(self, data_type):\n        if data_type not in self.data_paths:\n            raise ValueError(\'Invalid data type: {}\'.format(data_type))\n        return self.data_paths[data_type]\n\n    def _get_cupy_package_root(self):\n        try:\n            cupy_path = inspect.getfile(cupy)\n        except TypeError:\n            return None\n        return os.path.dirname(cupy_path)\n\n\nclass _RuntimeInfo(object):\n\n    cupy_version = None\n    cuda_path = None\n\n    # CUDA Driver\n    cuda_build_version = None\n    cuda_driver_version = None\n\n    # CUDA Runtime\n    cuda_runtime_version = None\n\n    # CUDA Toolkit\n    cublas_version = None\n    cufft_version = None\n    curand_version = None\n    cusolver_version = None\n    cusparse_version = None\n    nvrtc_version = None\n\n    # Optional Libraries\n    cudnn_build_version = None\n    cudnn_version = None\n    nccl_build_version = None\n    nccl_runtime_version = None\n    cub_version = None\n    cutensor_version = None\n\n    def __init__(self):\n        self.cupy_version = cupy.__version__\n\n        self.cuda_path = cupy.cuda.get_cuda_path()\n\n        self.cuda_build_version = cupy.cuda.driver.get_build_version()\n        self.cuda_driver_version = _eval_or_error(\n            cupy.cuda.runtime.driverGetVersion,\n            cupy.cuda.runtime.CUDARuntimeError)\n\n        self.cuda_runtime_version = _eval_or_error(\n            cupy.cuda.runtime.runtimeGetVersion,\n            cupy.cuda.runtime.CUDARuntimeError)\n\n        self.cublas_version = _eval_or_error(\n            lambda: cupy.cuda.cublas.getVersion(\n                cupy.cuda.device.get_cublas_handle()),\n            cupy.cuda.cublas.CUBLASError)\n        self.cufft_version = _eval_or_error(\n            cupy.cuda.cufft.getVersion,\n            cupy.cuda.cufft.CuFFTError)\n        self.curand_version = _eval_or_error(\n            cupy.cuda.curand.getVersion,\n            cupy.cuda.curand.CURANDError)\n        self.cusolver_version = _eval_or_error(\n            cupy.cuda.cusolver._getVersion,\n            cupy.cuda.cusolver.CUSOLVERError)\n        self.cusparse_version = _eval_or_error(\n            lambda: cupy.cuda.cusparse.getVersion(\n                cupy.cuda.device.get_cusparse_handle()),\n            cupy.cuda.cusparse.CuSparseError)\n        self.nvrtc_version = _eval_or_error(\n            cupy.cuda.nvrtc.getVersion,\n            cupy.cuda.nvrtc.NVRTCError)\n\n        if cudnn is not None:\n            self.cudnn_build_version = cudnn.get_build_version()\n            self.cudnn_version = _eval_or_error(\n                cudnn.getVersion, cudnn.CuDNNError)\n\n        if nccl is not None:\n            self.nccl_build_version = nccl.get_build_version()\n            nccl_runtime_version = nccl.get_version()\n            if nccl_runtime_version == 0:\n                nccl_runtime_version = \'(unknown)\'\n            self.nccl_runtime_version = nccl_runtime_version\n\n        if cub is not None:\n            # There is no API in CUB to retrieve the current version\n            # We show if its enabled or disabled\n            self.cub_version = \'Enabled\'\n\n        if cutensor is not None:\n            self.cutensor_version = cutensor.get_version()\n\n    def __str__(self):\n        records = [\n            (\'CuPy Version\', self.cupy_version),\n            (\'CUDA Root\', self.cuda_path),\n\n            (\'CUDA Build Version\', self.cuda_build_version),\n            (\'CUDA Driver Version\', self.cuda_driver_version),\n\n            (\'CUDA Runtime Version\', self.cuda_runtime_version),\n        ]\n\n        records += [\n            (\'cuBLAS Version\', self.cublas_version),\n            (\'cuFFT Version\', self.cufft_version),\n            (\'cuRAND Version\', self.curand_version),\n            (\'cuSOLVER Version\', self.cusolver_version),\n            (\'cuSPARSE Version\', self.cusparse_version),\n            (\'NVRTC Version\', self.nvrtc_version),\n        ]\n\n        records += [\n            (\'cuDNN Build Version\', self.cudnn_build_version),\n            (\'cuDNN Version\', self.cudnn_version),\n            (\'NCCL Build Version\', self.nccl_build_version),\n            (\'NCCL Runtime Version\', self.nccl_runtime_version),\n            (\'CUB Version\', self.cub_version),\n            (\'cuTENSOR Version\', self.cutensor_version),\n        ]\n\n        width = max([len(r[0]) for r in records]) + 2\n        fmt = \'{:\' + str(width) + \'}: {}\\n\'\n        s = io.StringIO()\n        for k, v in records:\n            s.write(fmt.format(k, v))\n\n        return s.getvalue()\n\n\ndef get_runtime_info():\n    return _RuntimeInfo()\n\n\ndef get_install_info():\n    return _InstallInfo()\n\n\ndef _dir_or_none(path):\n    """"""Returns None if path does not exist.""""""\n    if os.path.isdir(path):\n        return path\n    return None\n'"
cupyx/scatter.py,0,"b'def scatter_add(a, slices, value):\n    """"""Adds given values to specified elements of an array.\n\n    It adds ``value`` to the specified elements of ``a``.\n    If all of the indices target different locations, the operation of\n    :func:`scatter_add` is equivalent to ``a[slices] = a[slices] + value``.\n    If there are multiple elements targeting the same location,\n    :func:`scatter_add` uses all of these values for addition. On the other\n    hand, ``a[slices] = a[slices] + value`` only adds the contribution from one\n    of the indices targeting the same location.\n\n    Note that just like an array indexing, negative indices are interpreted as\n    counting from the end of an array.\n\n    Also note that :func:`scatter_add` behaves identically\n    to :func:`numpy.add.at`.\n\n    Example\n    -------\n    >>> import numpy\n    >>> import cupy\n    >>> a = cupy.zeros((6,), dtype=numpy.float32)\n    >>> i = cupy.array([1, 0, 1])\n    >>> v = cupy.array([1., 1., 1.])\n    >>> cupyx.scatter_add(a, i, v);\n    >>> a\n    array([1., 2., 0., 0., 0., 0.], dtype=float32)\n\n    Args:\n        a (ndarray): An array that gets added.\n        slices: It is integer, slices, ellipsis, numpy.newaxis,\n            integer array-like, boolean array-like or tuple of them.\n            It works for slices used for\n            :func:`cupy.ndarray.__getitem__` and\n            :func:`cupy.ndarray.__setitem__`.\n        v (array-like): Values to increment ``a`` at referenced locations.\n\n    .. note::\n        It only supports types that are supported by CUDA\'s atomicAdd when\n        an integer array is included in ``slices``.\n        The supported types are ``numpy.float32``, ``numpy.int32``,\n        ``numpy.uint32``, ``numpy.uint64`` and ``numpy.ulonglong``.\n\n    .. note::\n        :func:`scatter_add` does not raise an error when indices exceed size of\n        axes. Instead, it wraps indices.\n\n    .. note::\n        As of v4, this function is moved from ``cupy`` package to ``cupyx``\n        package.\n        ``cupy.scatter_add`` is still available for backward compatibility.\n\n    .. seealso:: :meth:`numpy.ufunc.at`.\n\n    """"""\n    a.scatter_add(slices, value)\n\n\ndef scatter_max(a, slices, value):\n    """"""Stores a maximum value of elements specified by indices to an array.\n\n    It stores the maximum value of elements in ``value`` array indexed by\n    ``slices`` to ``a``. If all of the indices target different locations,\n    the operation of :func:`scatter_max` is equivalent to\n    ``a[slices] = cupy.maximum(a[slices], value)``.\n    If there are multiple elements targeting the same location,\n    :func:`scatter_max` stores the maximum of all of these values to the given\n    index of ``a``, the initial element of ``a`` is also taken in account.\n\n    Note that just like an array indexing, negative indices are interpreted as\n    counting from the end of an array.\n\n    Also note that :func:`scatter_max` behaves identically\n    to :func:`numpy.maximum.at`.\n\n    Example\n    -------\n    >>> import numpy\n    >>> import cupy\n    >>> a = cupy.zeros((6,), dtype=numpy.float32)\n    >>> i = cupy.array([1, 0, 1, 2])\n    >>> v = cupy.array([1., 2., 3., -1.])\n    >>> cupyx.scatter_max(a, i, v);\n    >>> a\n    array([2., 3., 0., 0., 0., 0.], dtype=float32)\n\n    Args:\n        a (ndarray): An array to store the results.\n        slices: It is integer, slices, ellipsis, numpy.newaxis,\n            integer array-like, boolean array-like or tuple of them.\n            It works for slices used for\n            :func:`cupy.ndarray.__getitem__` and\n            :func:`cupy.ndarray.__setitem__`.\n        v (array-like): An array used for reference.\n    """"""\n    a.scatter_max(slices, value)\n\n\ndef scatter_min(a, slices, value):\n    """"""Stores a minimum value of elements specified by indices to an array.\n\n    It stores the minimum value of elements in ``value`` array indexed by\n    ``slices`` to ``a``. If all of the indices target different locations,\n    the operation of :func:`scatter_min` is equivalent to\n    ``a[slices] = cupy.minimum(a[slices], value)``.\n    If there are multiple elements targeting the same location,\n    :func:`scatter_min` stores the minimum of all of these values to the given\n    index of ``a``, the initial element of ``a`` is also taken in account.\n\n    Note that just like an array indexing, negative indices are interpreted as\n    counting from the end of an array.\n\n    Also note that :func:`scatter_min` behaves identically\n    to :func:`numpy.minimum.at`.\n\n    Example\n    -------\n    >>> import numpy\n    >>> import cupy\n    >>> a = cupy.zeros((6,), dtype=numpy.float32)\n    >>> i = cupy.array([1, 0, 1, 2])\n    >>> v = cupy.array([1., 2., 3., -1.])\n    >>> cupyx.scatter_min(a, i, v);\n    >>> a\n    array([ 0.,  0., -1.,  0.,  0.,  0.], dtype=float32)\n\n    Args:\n        a (ndarray): An array to store the results.\n        slices: It is integer, slices, ellipsis, numpy.newaxis,\n            integer array-like, boolean array-like or tuple of them.\n            It works for slices used for\n            :func:`cupy.ndarray.__getitem__` and\n            :func:`cupy.ndarray.__setitem__`.\n        v (array-like): An array used for reference.\n    """"""\n    a.scatter_min(slices, value)\n'"
cupyx/time.py,0,"b""import math\nimport time\n\nimport numpy\n\nimport cupy\nfrom cupy import util\n\n\nclass _PerfCaseResult(object):\n    def __init__(self, name, ts):\n        assert ts.ndim == 2\n        assert ts.shape[0] == 2\n        assert ts.shape[1] > 0\n        self.name = name\n        self._ts = ts\n\n    @property\n    def cpu_times(self):\n        return self._ts[0]\n\n    @property\n    def gpu_times(self):\n        return self._ts[1]\n\n    @staticmethod\n    def _to_str_per_item(device_name, t):\n        assert t.ndim == 1\n        assert t.size > 0\n        t_us = t * 1e6\n\n        s = '    {}:{:9.03f} us'.format(device_name, t_us.mean())\n        if t.size > 1:\n            s += '   +/-{:6.03f} (min:{:9.03f} / max:{:9.03f}) us'.format(\n                t_us.std(), t_us.min(), t_us.max())\n        return s\n\n    def to_str(self, show_gpu=False):\n        results = [self._to_str_per_item('CPU', self._ts[0])]\n        if show_gpu:\n            results.append(self._to_str_per_item('GPU', self._ts[1]))\n        return '{:<20s}:{}'.format(self.name, ' '.join(results))\n\n    def __str__(self):\n        return self.to_str(show_gpu=True)\n\n\ndef repeat(\n        func, args=(), kwargs={}, n_repeat=10000, *,\n        name=None, n_warmup=10, max_duration=math.inf):\n    util.experimental('cupyx.time.repeat')\n    if name is None:\n        name = func.__name__\n\n    if not callable(func):\n        raise ValueError('`func` should be a callable object.')\n    if not isinstance(args, tuple):\n        raise ValueError('`args` should be of tuple type.')\n    if not isinstance(kwargs, dict):\n        raise ValueError('`kwargs` should be of dict type.')\n    if not isinstance(n_repeat, int):\n        raise ValueError('`n_repeat` should be an integer.')\n    if not isinstance(name, str):\n        raise ValueError('`str` should be a string.')\n    if not isinstance(n_warmup, int):\n        raise ValueError('`n_warmup` should be an integer.')\n\n    ev1 = cupy.cuda.stream.Event()\n    ev2 = cupy.cuda.stream.Event()\n\n    for i in range(n_warmup):\n        func(*args, **kwargs)\n\n    ev1.record()\n    ev1.synchronize()\n\n    cpu_times = []\n    gpu_times = []\n    duration = 0\n    for i in range(n_repeat):\n        ev1.record()\n        t1 = time.perf_counter()\n\n        func(*args, **kwargs)\n\n        t2 = time.perf_counter()\n        ev2.record()\n        ev2.synchronize()\n        cpu_time = t2 - t1\n        gpu_time = cupy.cuda.get_elapsed_time(ev1, ev2) * 1e-3\n        cpu_times.append(cpu_time)\n        gpu_times.append(gpu_time)\n\n        duration += time.perf_counter() - t1\n        if duration > max_duration:\n            break\n\n    ts = numpy.asarray([cpu_times, gpu_times], dtype=numpy.float64)\n    return _PerfCaseResult(name, ts)\n"""
install/__init__.py,0,b''
install/build.py,0,"b'import contextlib\nimport distutils.util\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport tempfile\n\nfrom install import utils\n\n\nPLATFORM_DARWIN = sys.platform.startswith(\'darwin\')\nPLATFORM_LINUX = sys.platform.startswith(\'linux\')\nPLATFORM_WIN32 = sys.platform.startswith(\'win32\')\n\nminimum_cuda_version = 8000\nminimum_cudnn_version = 5000\nmaximum_cudnn_version = 7999\n\n_cuda_path = \'NOT_INITIALIZED\'\n_rocm_path = \'NOT_INITIALIZED\'\n_compiler_base_options = None\n\n\n# Using tempfile.TemporaryDirectory would cause an error during cleanup\n# due to a bug: https://bugs.python.org/issue26660\n@contextlib.contextmanager\ndef _tempdir():\n    temp_dir = tempfile.mkdtemp()\n    try:\n        yield temp_dir\n    finally:\n        shutil.rmtree(temp_dir, ignore_errors=True)\n\n\ndef get_rocm_path():\n    global _rocm_path\n\n    # Use a magic word to represent the cache not filled because None is a\n    # valid return value.\n    if _rocm_path != \'NOT_INITIALIZED\':\n        return _rocm_path\n\n    _rocm_path = os.environ.get(\'ROCM_HOME\', \'\')\n    return _rocm_path\n\n\ndef get_cuda_path():\n    global _cuda_path\n\n    # Use a magic word to represent the cache not filled because None is a\n    # valid return value.\n    if _cuda_path != \'NOT_INITIALIZED\':\n        return _cuda_path\n\n    nvcc_path = utils.search_on_path((\'nvcc\', \'nvcc.exe\'))\n    cuda_path_default = None\n    if nvcc_path is not None:\n        cuda_path_default = os.path.normpath(\n            os.path.join(os.path.dirname(nvcc_path), \'..\'))\n\n    cuda_path = os.environ.get(\'CUDA_PATH\', \'\')  # Nvidia default on Windows\n    if len(cuda_path) > 0 and cuda_path != cuda_path_default:\n        utils.print_warning(\n            \'nvcc path != CUDA_PATH\',\n            \'nvcc path: %s\' % cuda_path_default,\n            \'CUDA_PATH: %s\' % cuda_path)\n\n    if os.path.exists(cuda_path):\n        _cuda_path = cuda_path\n    elif cuda_path_default is not None:\n        _cuda_path = cuda_path_default\n    elif os.path.exists(\'/usr/local/cuda\'):\n        _cuda_path = \'/usr/local/cuda\'\n    else:\n        _cuda_path = None\n\n    return _cuda_path\n\n\ndef get_nvcc_path():\n    nvcc = os.environ.get(\'NVCC\', None)\n    if nvcc:\n        return distutils.util.split_quoted(nvcc)\n\n    cuda_path = get_cuda_path()\n    if cuda_path is None:\n        return None\n\n    if PLATFORM_WIN32:\n        nvcc_bin = \'bin/nvcc.exe\'\n    else:\n        nvcc_bin = \'bin/nvcc\'\n\n    nvcc_path = os.path.join(cuda_path, nvcc_bin)\n    if os.path.exists(nvcc_path):\n        return [nvcc_path]\n    else:\n        return None\n\n\ndef get_hipcc_path():\n    hipcc = os.environ.get(\'HIPCC\', None)\n    if hipcc:\n        return distutils.util.split_quoted(hipcc)\n\n    rocm_path = get_rocm_path()\n    if rocm_path is None:\n        return None\n\n    if PLATFORM_WIN32:\n        hipcc_bin = \'bin/hipcc.exe\'\n    else:\n        hipcc_bin = \'bin/hipcc\'\n\n    hipcc_path = os.path.join(rocm_path, hipcc_bin)\n    if os.path.exists(hipcc_path):\n        return [hipcc_path]\n    else:\n        return None\n\n\ndef get_compiler_setting(use_hip):\n    cuda_path = None\n    rocm_path = None\n\n    if use_hip:\n        rocm_path = get_rocm_path()\n    else:\n        cuda_path = get_cuda_path()\n\n    include_dirs = []\n    library_dirs = []\n    define_macros = []\n    extra_compile_args = []\n\n    if cuda_path:\n        include_dirs.append(os.path.join(cuda_path, \'include\'))\n        if PLATFORM_WIN32:\n            library_dirs.append(os.path.join(cuda_path, \'bin\'))\n            library_dirs.append(os.path.join(cuda_path, \'lib\', \'x64\'))\n        else:\n            library_dirs.append(os.path.join(cuda_path, \'lib64\'))\n            library_dirs.append(os.path.join(cuda_path, \'lib\'))\n\n    if rocm_path:\n        include_dirs.append(os.path.join(rocm_path, \'include\'))\n        include_dirs.append(os.path.join(rocm_path, \'rocrand\', \'include\'))\n        library_dirs.append(os.path.join(rocm_path, \'lib\'))\n        library_dirs.append(os.path.join(rocm_path, \'rocrand\', \'lib\'))\n\n    if use_hip:\n        extra_compile_args.append(\'-std=c++11\')\n\n    if PLATFORM_DARWIN:\n        library_dirs.append(\'/usr/local/cuda/lib\')\n\n    if PLATFORM_WIN32:\n        nvtoolsext_path = os.environ.get(\'NVTOOLSEXT_PATH\', \'\')\n        if os.path.exists(nvtoolsext_path):\n            include_dirs.append(os.path.join(nvtoolsext_path, \'include\'))\n            library_dirs.append(os.path.join(nvtoolsext_path, \'lib\', \'x64\'))\n        else:\n            define_macros.append((\'CUPY_NO_NVTX\', \'1\'))\n\n    cub_path = os.environ.get(\'CUB_PATH\', \'\')\n    if os.path.exists(cub_path):\n        # for <cupy/complex.cuh>\n        cupy_header = os.path.join(os.path.dirname(os.path.realpath(__file__)),\n                                   \'../cupy/core/include\')\n        include_dirs.append(cupy_header)\n        include_dirs.append(cub_path)\n\n    return {\n        \'include_dirs\': include_dirs,\n        \'library_dirs\': library_dirs,\n        \'define_macros\': define_macros,\n        \'language\': \'c++\',\n        \'extra_compile_args\': extra_compile_args,\n    }\n\n\ndef _match_output_lines(output_lines, regexs):\n    # Matches regular expressions `regexs` against `output_lines` and finds the\n    # consecutive matching lines from `output_lines`.\n    # `None` is returned if no match is found.\n    if len(output_lines) < len(regexs):\n        return None\n\n    matches = [None] * len(regexs)\n    for i in range(len(output_lines) - len(regexs)):\n        for j in range(len(regexs)):\n            m = re.match(regexs[j], output_lines[i + j])\n            if not m:\n                break\n            matches[j] = m\n        else:\n            # Match found\n            return matches\n\n    # No match\n    return None\n\n\ndef get_compiler_base_options():\n    """"""Returns base options for nvcc compiler.\n\n    """"""\n    global _compiler_base_options\n    if _compiler_base_options is None:\n        _compiler_base_options = _get_compiler_base_options()\n    return _compiler_base_options\n\n\ndef _get_compiler_base_options():\n    # Try compiling a dummy code.\n    # If the compilation fails, try to parse the output of compilation\n    # and try to compose base options according to it.\n    nvcc_path = get_nvcc_path()\n    with _tempdir() as temp_dir:\n        test_cu_path = os.path.join(temp_dir, \'test.cu\')\n        test_out_path = os.path.join(temp_dir, \'test.out\')\n        with open(test_cu_path, \'w\') as f:\n            f.write(\'int main() { return 0; }\')\n        proc = subprocess.Popen(\n            nvcc_path + [\'-o\', test_out_path, test_cu_path],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE)\n        stdoutdata, stderrdata = proc.communicate()\n        stderrlines = stderrdata.split(b\'\\n\')\n        if proc.returncode != 0:\n\n            # No supported host compiler\n            matches = _match_output_lines(\n                stderrlines,\n                [\n                    b\'^ERROR: No supported gcc/g\\\\+\\\\+ host compiler found, \'\n                    b\'but .* is available.$\',\n                    b\'^ *Use \\\'nvcc (.*)\\\' to use that instead.$\',\n                ])\n            if matches is not None:\n                base_opts = matches[1].group(1)\n                base_opts = base_opts.decode(\'utf8\').split(\' \')\n                return base_opts\n\n            # Unknown error\n            raise RuntimeError(\n                \'Encountered unknown error while testing nvcc:\\n\' +\n                stderrdata.decode(\'utf8\'))\n\n    return []\n\n\n_cuda_version = None\n_cudnn_version = None\n_nccl_version = None\n\n\ndef check_cuda_version(compiler, settings):\n    global _cuda_version\n    try:\n        out = build_and_run(compiler, \'\'\'\n        #include <cuda.h>\n        #include <stdio.h>\n        int main() {\n          printf(""%d"", CUDA_VERSION);\n          return 0;\n        }\n        \'\'\', include_dirs=settings[\'include_dirs\'])\n\n    except Exception as e:\n        utils.print_warning(\'Cannot check CUDA version\', str(e))\n        return False\n\n    _cuda_version = int(out)\n\n    if _cuda_version < minimum_cuda_version:\n        utils.print_warning(\n            \'CUDA version is too old: %d\' % _cuda_version,\n            \'CUDA v7.0 or newer is required\')\n        return False\n\n    return True\n\n\ndef _format_cuda_version(version):\n    return str(version)\n\n\ndef get_cuda_version(formatted=False):\n    """"""Return CUDA Toolkit version cached in check_cuda_version().""""""\n    global _cuda_version\n    if _cuda_version is None:\n        msg = \'check_cuda_version() must be called first.\'\n        raise RuntimeError(msg)\n    if formatted:\n        return _format_cuda_version(_cuda_version)\n    return _cuda_version\n\n\ndef check_cudnn_version(compiler, settings):\n    global _cudnn_version\n    try:\n        out = build_and_run(compiler, \'\'\'\n        #include <cudnn.h>\n        #include <stdio.h>\n        int main() {\n          printf(""%d"", CUDNN_VERSION);\n          return 0;\n        }\n        \'\'\', include_dirs=settings[\'include_dirs\'])\n\n    except Exception as e:\n        utils.print_warning(\'Cannot check cuDNN version\\n{0}\'.format(e))\n        return False\n\n    _cudnn_version = int(out)\n\n    if not minimum_cudnn_version <= _cudnn_version <= maximum_cudnn_version:\n        min_major = _format_cuda_version(minimum_cudnn_version)\n        max_major = _format_cuda_version(maximum_cudnn_version)\n        utils.print_warning(\n            \'Unsupported cuDNN version: {}\'.format(\n                _format_cuda_version(_cudnn_version)),\n            \'cuDNN v{}= and <=v{} is required\'.format(min_major, max_major))\n        return False\n\n    return True\n\n\ndef get_cudnn_version(formatted=False):\n    """"""Return cuDNN version cached in check_cudnn_version().""""""\n    global _cudnn_version\n    if _cudnn_version is None:\n        msg = \'check_cudnn_version() must be called first.\'\n        raise RuntimeError(msg)\n    if formatted:\n        return _format_cuda_version(_cudnn_version)\n    return _cudnn_version\n\n\ndef check_nccl_version(compiler, settings):\n    global _nccl_version\n\n    # NCCL 1.x does not provide version information.\n    try:\n        out = build_and_run(compiler, \'\'\'\n        #include <nccl.h>\n        #include <stdio.h>\n        #ifdef NCCL_MAJOR\n        #ifndef NCCL_VERSION_CODE\n        #  define NCCL_VERSION_CODE \\\n                (NCCL_MAJOR * 1000 + NCCL_MINOR * 100 + NCCL_PATCH)\n        #endif\n        #else\n        #  define NCCL_VERSION_CODE 0\n        #endif\n        int main() {\n          printf(""%d"", NCCL_VERSION_CODE);\n          return 0;\n        }\n        \'\'\', include_dirs=settings[\'include_dirs\'])\n\n    except Exception as e:\n        utils.print_warning(\'Cannot include NCCL\\n{0}\'.format(e))\n        return False\n\n    _nccl_version = int(out)\n\n    return True\n\n\ndef get_nccl_version(formatted=False):\n    """"""Return NCCL version cached in check_nccl_version().""""""\n    global _nccl_version\n    if _nccl_version is None:\n        msg = \'check_nccl_version() must be called first.\'\n        raise RuntimeError(msg)\n    if formatted:\n        if _nccl_version == 0:\n            return \'1.x\'\n        return _format_cuda_version(_nccl_version)\n    return _nccl_version\n\n\ndef check_nvtx(compiler, settings):\n    if PLATFORM_WIN32:\n        path = os.environ.get(\'NVTOOLSEXT_PATH\', None)\n        if path is None:\n            utils.print_warning(\n                \'NVTX unavailable: NVTOOLSEXT_PATH is not set\')\n        elif not os.path.exists(path):\n            utils.print_warning(\n                \'NVTX unavailable: NVTOOLSEXT_PATH is set but the directory \'\n                \'does not exist\')\n        elif utils.search_on_path([\'nvToolsExt64_1.dll\']) is None:\n            utils.print_warning(\n                \'NVTX unavailable: nvToolsExt64_1.dll not found in PATH\')\n        else:\n            return True\n        return False\n    return True\n\n\ndef check_cutensor_version(compiler, settings):\n    global _cutensor_version\n    try:\n        out = build_and_run(compiler, \'\'\'\n        #include <cutensor.h>\n        #include <stdio.h>\n        #ifdef CUTENSOR_MAJOR\n        #ifndef CUTENSOR_VERSION\n        #define CUTENSOR_VERSION \\\n                (CUTENSOR_MAJOR * 1000 + CUTENSOR_MINOR * 100 + CUTENSOR_PATCH)\n        #endif\n        #else\n        #  define CUTENSOR_VERSION 0\n        #endif\n        int main(int argc, char* argv[]) {\n          printf(""%d"", CUTENSOR_VERSION);\n          return 0;\n        }\n        \'\'\', include_dirs=settings[\'include_dirs\'])\n\n    except Exception as e:\n        utils.print_warning(\'Cannot check cuTENSOR version\\n{0}\'.format(e))\n        return False\n\n    _cutensor_version = int(out)\n\n    if _cutensor_version < 1000:\n        utils.print_warning(\n            \'Unsupported cuTENSOR version: {}\'.format(_cutensor_version)\n        )\n        return False\n\n    return True\n\n\ndef get_cutensor_version(formatted=False):\n    """"""Return cuTENSOR version cached in check_cutensor_version().""""""\n    global _cutensor_version\n    if _cutensor_version is None:\n        msg = \'check_cutensor_version() must be called first.\'\n        raise RuntimeError(msg)\n    return _cutensor_version\n\n\ndef build_shlib(compiler, source, libraries=(),\n                include_dirs=(), library_dirs=(), define_macros=None,\n                extra_compile_args=()):\n    with _tempdir() as temp_dir:\n        fname = os.path.join(temp_dir, \'a.cpp\')\n        with open(fname, \'w\') as f:\n            f.write(source)\n        objects = compiler.compile([fname], output_dir=temp_dir,\n                                   include_dirs=include_dirs,\n                                   macros=define_macros,\n                                   extra_postargs=list(extra_compile_args))\n\n        try:\n            postargs = [\'/MANIFEST\'] if PLATFORM_WIN32 else []\n            compiler.link_shared_lib(objects,\n                                     os.path.join(temp_dir, \'a\'),\n                                     libraries=libraries,\n                                     library_dirs=library_dirs,\n                                     extra_postargs=postargs,\n                                     target_lang=\'c++\')\n        except Exception as e:\n            msg = \'Cannot build a stub file.\\nOriginal error: {0}\'.format(e)\n            raise Exception(msg)\n\n\ndef build_and_run(compiler, source, libraries=(),\n                  include_dirs=(), library_dirs=(), define_macros=None,\n                  extra_compile_args=()):\n    with _tempdir() as temp_dir:\n        fname = os.path.join(temp_dir, \'a.cpp\')\n        with open(fname, \'w\') as f:\n            f.write(source)\n\n        objects = compiler.compile([fname], output_dir=temp_dir,\n                                   include_dirs=include_dirs,\n                                   macros=define_macros,\n                                   extra_postargs=list(extra_compile_args))\n\n        try:\n            postargs = [\'/MANIFEST\'] if PLATFORM_WIN32 else []\n            compiler.link_executable(objects,\n                                     os.path.join(temp_dir, \'a\'),\n                                     libraries=libraries,\n                                     library_dirs=library_dirs,\n                                     extra_postargs=postargs,\n                                     target_lang=\'c++\')\n        except Exception as e:\n            msg = \'Cannot build a stub file.\\nOriginal error: {0}\'.format(e)\n            raise Exception(msg)\n\n        try:\n            out = subprocess.check_output(os.path.join(temp_dir, \'a\'))\n            return out\n\n        except Exception as e:\n            msg = \'Cannot execute a stub file.\\nOriginal error: {0}\'.format(e)\n            raise Exception(msg)\n'"
install/utils.py,0,"b""import os\n\n\ndef print_warning(*lines):\n    print('**************************************************')\n    for line in lines:\n        print('*** WARNING: %s' % line)\n    print('**************************************************')\n\n\ndef get_path(key):\n    return os.environ.get(key, '').split(os.pathsep)\n\n\ndef search_on_path(filenames):\n    for p in get_path('PATH'):\n        for filename in filenames:\n            full = os.path.join(p, filename)\n            if os.path.exists(full):\n                return os.path.abspath(full)\n    return None\n"""
tests/conftest.py,0,"b'import os\nimport subprocess\nimport sys\n\n\ndef _is_pip_installed():\n    try:\n        import pip  # NOQA\n        return True\n    except ImportError:\n        return False\n\n\ndef _is_in_ci():\n    ci_name = os.environ.get(\'CUPY_CI\', \'\')\n    return ci_name != \'\'\n\n\ndef pytest_configure(config):\n    # Print installed packages\n    if _is_in_ci() and _is_pip_installed():\n        print(""***** Installed packages *****"", flush=True)\n        subprocess.check_call([sys.executable, \'-m\', \'pip\', \'freeze\', \'--all\'])\n'"
cupy/_sorting/__init__.py,0,"b'# Functions from the following NumPy document\n# https://docs.scipy.org/doc/numpy/reference/routines.sort.html\n\n# ""NOQA"" to suppress flake8 warning\nfrom cupy._sorting import count  # NOQA\nfrom cupy._sorting import search  # NOQA\nfrom cupy._sorting import sort  # NOQA\n'"
cupy/_sorting/count.py,0,"b'from cupy import core\n\n\ndef count_nonzero(a, axis=None):\n    """"""Counts the number of non-zero values in the array.\n\n    .. note::\n\n       :func:`numpy.count_nonzero` returns `int` value when `axis=None`,\n       but :func:`cupy.count_nonzero` returns zero-dimensional array to reduce\n       CPU-GPU synchronization.\n\n    Args:\n        a (cupy.ndarray): The array for which to count non-zeros.\n        axis (int or tuple, optional): Axis or tuple of axes along which to\n            count non-zeros. Default is None, meaning that non-zeros will be\n            counted along a flattened version of ``a``\n    Returns:\n        cupy.ndarray of int: Number of non-zero values in the array\n            along a given axis. Otherwise, the total number of non-zero values\n            in the array is returned.\n    """"""\n\n    return _count_nonzero(a, axis=axis)\n\n\n_count_nonzero = core.create_reduction_func(\n    \'cupy_count_nonzero\',\n    (\'?->l\', \'B->l\', \'h->l\', \'H->l\', \'i->l\', \'I->l\', \'l->l\', \'L->l\',\n     \'q->l\', \'Q->l\', \'e->l\', \'f->l\', \'d->l\', \'F->l\', \'D->l\'),\n    (\'in0 != type_in0_raw(0)\', \'a + b\', \'out0 = a\', None), 0)\n'"
cupy/_sorting/search.py,0,"b'import cupy\nfrom cupy import core\nfrom cupy.core import fusion\nfrom cupy import util\n\nfrom cupy.core import _routines_indexing as _indexing\nfrom cupy.core import _routines_statistics as _statistics\n\nimport warnings\n\n\ndef argmax(a, axis=None, dtype=None, out=None, keepdims=False):\n    """"""Returns the indices of the maximum along an axis.\n\n    Args:\n        a (cupy.ndarray): Array to take argmax.\n        axis (int): Along which axis to find the maximum. ``a`` is flattened by\n            default.\n        dtype: Data type specifier.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the axis ``axis`` is preserved as an axis\n            of length one.\n\n    Returns:\n        cupy.ndarray: The indices of the maximum of ``a`` along an axis.\n\n    .. note::\n       ``dtype`` and ``keepdim`` arguments are specific to CuPy. They are\n       not in NumPy.\n\n    .. note::\n       ``axis`` argument accepts a tuple of ints, but this is specific to\n       CuPy. NumPy does not support it.\n\n    .. seealso:: :func:`numpy.argmax`\n\n    """"""\n    # TODO(okuta): check type\n    return a.argmax(axis=axis, dtype=dtype, out=out, keepdims=keepdims)\n\n\ndef nanargmax(a, axis=None, dtype=None, out=None, keepdims=False):\n    """"""Return the indices of the maximum values in the specified axis ignoring\n    NaNs. For all-NaN slice ``-1`` is returned.\n    Subclass cannot be passed yet, subok=True still unsupported\n\n    Args:\n        a (cupy.ndarray): Array to take nanargmax.\n        axis (int): Along which axis to find the maximum. ``a`` is flattened by\n            default.\n\n    Returns:\n        cupy.ndarray: The indices of the maximum of ``a``\n            along an axis ignoring NaN values.\n\n    .. note:: For performance reasons, ``cupy.nanargmax`` returns\n            ``out of range values`` for all-NaN slice\n            whereas ``numpy.nanargmax`` raises ``ValueError``\n    .. seealso:: :func:`numpy.nanargmax`\n    """"""\n    if a.dtype.kind in \'biu\':\n        return argmax(a, axis=axis)\n\n    return _statistics._nanargmax(a, axis, dtype, out, keepdims)\n\n\ndef argmin(a, axis=None, dtype=None, out=None, keepdims=False):\n    """"""Returns the indices of the minimum along an axis.\n\n    Args:\n        a (cupy.ndarray): Array to take argmin.\n        axis (int): Along which axis to find the minimum. ``a`` is flattened by\n            default.\n        dtype: Data type specifier.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the axis ``axis`` is preserved as an axis\n            of length one.\n\n    Returns:\n        cupy.ndarray: The indices of the minimum of ``a`` along an axis.\n\n    .. note::\n       ``dtype`` and ``keepdim`` arguments are specific to CuPy. They are\n       not in NumPy.\n\n    .. note::\n       ``axis`` argument accepts a tuple of ints, but this is specific to\n       CuPy. NumPy does not support it.\n\n    .. seealso:: :func:`numpy.argmin`\n\n    """"""\n    # TODO(okuta): check type\n    return a.argmin(axis=axis, dtype=dtype, out=out, keepdims=keepdims)\n\n\ndef nanargmin(a, axis=None, dtype=None, out=None, keepdims=False):\n    """"""Return the indices of the minimum values in the specified axis ignoring\n    NaNs. For all-NaN slice ``-1`` is returned.\n    Subclass cannot be passed yet, subok=True still unsupported\n\n    Args:\n        a (cupy.ndarray): Array to take nanargmin.\n        axis (int): Along which axis to find the minimum. ``a`` is flattened by\n            default.\n\n    Returns:\n        cupy.ndarray: The indices of the minimum of ``a``\n            along an axis ignoring NaN values.\n\n    .. note:: For performance reasons, ``cupy.nanargmin`` returns\n            ``out of range values`` for all-NaN slice\n            whereas ``numpy.nanargmin`` raises ``ValueError``\n    .. seealso:: :func:`numpy.nanargmin`\n    """"""\n    if a.dtype.kind in \'biu\':\n        return argmin(a, axis=axis)\n\n    return _statistics._nanargmin(a, axis, dtype, out, keepdims)\n\n# TODO(okuta): Implement argwhere\n\n\ndef nonzero(a):\n    """"""Return the indices of the elements that are non-zero.\n\n    Returns a tuple of arrays, one for each dimension of a,\n    containing the indices of the non-zero elements in that dimension.\n\n    Args:\n        a (cupy.ndarray): array\n\n    Returns:\n        tuple of arrays: Indices of elements that are non-zero.\n\n    .. warning::\n\n        This function may synchronize the device.\n\n    .. seealso:: :func:`numpy.nonzero`\n\n    """"""\n    util.check_array(a, arg_name=\'a\')\n    return a.nonzero()\n\n\ndef flatnonzero(a):\n    """"""Return indices that are non-zero in the flattened version of a.\n\n    This is equivalent to a.ravel().nonzero()[0].\n\n    Args:\n        a (cupy.ndarray): input array\n\n    Returns:\n        cupy.ndarray: Output array,\n        containing the indices of the elements of a.ravel() that are non-zero.\n\n    .. warning::\n\n        This function may synchronize the device.\n\n    .. seealso:: :func:`numpy.flatnonzero`\n    """"""\n    util.check_array(a, arg_name=\'a\')\n    return a.ravel().nonzero()[0]\n\n\n_where_ufunc = core.create_ufunc(\n    \'cupy_where\',\n    (\'???->?\', \'?bb->b\', \'?BB->B\', \'?hh->h\', \'?HH->H\', \'?ii->i\', \'?II->I\',\n     \'?ll->l\', \'?LL->L\', \'?qq->q\', \'?QQ->Q\', \'?ee->e\', \'?ff->f\',\n     # On CUDA 6.5 these combinations don\'t work correctly (on CUDA >=7.0, it\n     # works).\n     # See issue #551.\n     \'?hd->d\', \'?Hd->d\',\n     \'?dd->d\', \'?FF->F\', \'?DD->D\'),\n    \'out0 = in0 ? in1 : in2\')\n\n\ndef where(condition, x=None, y=None):\n    """"""Return elements, either from x or y, depending on condition.\n\n    If only condition is given, return ``condition.nonzero()``.\n\n    Args:\n        condition (cupy.ndarray): When True, take x, otherwise take y.\n        x (cupy.ndarray): Values from which to choose on ``True``.\n        y (cupy.ndarray): Values from which to choose on ``False``.\n\n    Returns:\n        cupy.ndarray: Each element of output contains elements of ``x`` when\n            ``condition`` is ``True``, otherwise elements of ``y``. If only\n            ``condition`` is given, return the tuple ``condition.nonzero()``,\n            the indices where ``condition`` is True.\n\n    .. warning::\n\n        This function may synchronize the device if both ``x`` and ``y`` are\n        omitted.\n\n    .. seealso:: :func:`numpy.where`\n\n    """"""\n\n    missing = (x is None, y is None).count(True)\n\n    if missing == 1:\n        raise ValueError(\'Must provide both \\\'x\\\' and \\\'y\\\' or neither.\')\n    if missing == 2:\n        return nonzero(condition)  # may synchronize\n\n    if fusion._is_fusing():\n        return fusion._call_ufunc(_where_ufunc, condition, x, y)\n    return _where_ufunc(condition.astype(\'?\'), x, y)\n\n\ndef argwhere(a):\n    """"""Return the indices of the elements that are non-zero.\n\n    Returns a (N, ndim) dimantional array containing the\n    indices of the non-zero elements. Where `N` is number of\n    non-zero elements and `ndim` is dimention of the given array.\n\n    Args:\n        a (cupy.ndarray): array\n\n    Returns:\n        cupy.ndarray: Indices of elements that are non-zero.\n\n    .. seealso:: :func:`numpy.argwhere`\n\n    """"""\n    util.check_array(a, arg_name=\'a\')\n    if a.ndim == 0:\n        warnings.warn(\n            \'calling argwhere on 0d arrays is deprecated\',\n            DeprecationWarning)\n    return _indexing._ndarray_argwhere(a)\n\n\n# This is to allow using the same kernels for all dtypes, ints & floats\n# as nan is a special case\n_preamble = \'\'\'\ntemplate<typename T>\n__device__ bool _isnan(T val) {\n    return val != val;\n}\n\'\'\'\n\n\n_searchsorted_kernel = core.ElementwiseKernel(\n    \'S x, raw T bins, int64 n_bins, bool side_is_right, \'\n    \'bool assume_increassing\',\n    \'int64 y\',\n    \'\'\'\n    // Array is assumed to be monotonically\n    // increasing unless a check is requested with the\n    // `assume_increassing = False` parameter.\n    // `digitize` allows increasing and decreasing arrays.\n    bool inc = true;\n    if (!assume_increassing && n_bins >= 2) {\n        // In the case all the bins are nan the array is considered\n        // to be decreasing in numpy\n        inc = (bins[0] <= bins[n_bins-1])\n              || (!_isnan<T>(bins[0]) && _isnan<T>(bins[n_bins-1]));\n    }\n\n    if (_isnan<S>(x)) {\n        long long pos = (inc ? n_bins : 0);\n        if (!side_is_right) {\n            if (inc) {\n                while (pos > 0 && _isnan<T>(bins[pos-1])) {\n                    --pos;\n                }\n            } else {\n                while (pos < n_bins && _isnan<T>(bins[pos])) {\n                    ++pos;\n                }\n            }\n        }\n        y = pos;\n        return;\n    }\n\n    bool greater = false;\n    if (side_is_right) {\n        greater = inc && x >= bins[n_bins-1];\n    } else {\n        greater = (inc ? x > bins[n_bins-1] : x <= bins[n_bins-1]);\n    }\n    if (greater) {\n        y = n_bins;\n        return;\n    }\n\n    long long left = 0;\n    // In the case the bins is all NaNs, digitize\n    // needs to place all the valid values to the right\n    if (!inc) {\n        while (_isnan<T>(bins[left]) && left < n_bins) {\n            ++left;\n        }\n        if (left == n_bins) {\n            y = n_bins;\n            return;\n        }\n        if (side_is_right\n                && !_isnan<T>(bins[n_bins-1]) && !_isnan<S>(x)\n                && bins[n_bins-1] > x) {\n            y = n_bins;\n            return;\n        }\n    }\n\n    long long right = n_bins-1;\n    while (left < right) {\n        long long m = left + (right - left) / 2;\n        bool look_right = true;\n        if (side_is_right) {\n            look_right = (inc ? bins[m] <= x : bins[m] > x);\n        } else {\n            look_right = (inc ? bins[m] < x : bins[m] >= x);\n        }\n        if (look_right) {\n            left = m + 1;\n        } else {\n            right = m;\n        }\n    }\n    y = right;\n    \'\'\', preamble=_preamble)\n\n\ndef searchsorted(a, v, side=\'left\', sorter=None):\n    """"""Finds indices where elements should be inserted to maintain order.\n\n    Find the indices into a sorted array ``a`` such that,\n    if the corresponding elements in ``v`` were inserted before the indices,\n    the order of ``a`` would be preserved.\n\n    Args:\n        a (cupy.ndarray): Input array. If ``sorter`` is ``None``, then\n            it must be sorted in ascending order,\n            otherwise ``sorter`` must be an array of indices that sort it.\n        v (cupy.ndarray): Values to insert into ``a``.\n        side : {\'left\', \'right\'}\n            If ``left``, return the index of the first suitable location found\n            If ``right``, return the last such index.\n            If there is no suitable index, return either 0 or length of ``a``.\n        sorter : 1-D array_like\n            Optional array of integer indices that sort array ``a`` into\n            ascending order. They are typically the result of\n            :func:`~cupy.argsort`.\n\n    Returns:\n        cupy.ndarray: Array of insertion points with the same shape as ``v``.\n\n    .. note:: When a is not in ascending order, behavior is undefined.\n\n    .. seealso:: :func:`numpy.searchsorted`\n\n    """"""\n    return _searchsorted(a, v, side, sorter, True)\n\n\ndef _searchsorted(a, v, side, sorter, assume_increasing):\n    """"""`assume_increasing` is used in the kernel to\n    skip monotonically increasing or decreasing verification\n    inside the cuda kernel.\n    """"""\n    if not isinstance(a, cupy.ndarray):\n        raise NotImplementedError(\'Only int or ndarray are supported for a\')\n\n    if not isinstance(v, cupy.ndarray):\n        raise NotImplementedError(\'Only int or ndarray are supported for v\')\n\n    if a.ndim > 1:\n        raise ValueError(\'object too deep for desired array\')\n    if a.ndim < 1:\n        raise ValueError(\'object of too small depth for desired array\')\n    if a.size == 0:\n        return cupy.zeros(v.shape, dtype=cupy.int64)\n\n    a_iscomplex = a.dtype.kind == \'c\'\n    v_iscomplex = v.dtype.kind == \'c\'\n\n    if a_iscomplex and not v_iscomplex:\n        v = v.astype(a.dtype)\n    elif v_iscomplex and not a_iscomplex:\n        a = a.astype(v.dtype)\n\n    # Numpy does not check if the array is monotonic inside searchsorted\n    # which leds to undefined behavior in such cases.\n    if sorter is not None:\n        if sorter.dtype.kind not in (\'i\', \'u\'):\n            raise TypeError(\'sorter must be of integer type\')\n        if sorter.size != a.size:\n            raise ValueError(\'sorter.size must equal a.size\')\n        a = a.take(sorter)\n\n    y = cupy.zeros(v.shape, dtype=cupy.int64)\n\n    _searchsorted_kernel(v, a, a.size, side == \'right\', assume_increasing, y)\n    return y\n\n\n# TODO(okuta): Implement extract\n'"
cupy/_sorting/sort.py,0,"b'import cupy\nimport numpy\n\nif cupy.cuda.thrust_enabled:\n    from cupy.cuda import thrust\n\n\ndef sort(a, axis=-1):\n    """"""Returns a sorted copy of an array with a stable sorting algorithm.\n\n    Args:\n        a (cupy.ndarray): Array to be sorted.\n        axis (int or None): Axis along which to sort. Default is -1, which\n            means sort along the last axis. If None is supplied, the array is\n            flattened before sorting.\n\n    Returns:\n        cupy.ndarray: Array of the same type and shape as ``a``.\n\n    .. note::\n       For its implementation reason, ``cupy.sort`` currently does not support\n       ``kind`` and ``order`` parameters that ``numpy.sort`` does\n       support.\n\n    .. seealso:: :func:`numpy.sort`\n\n    """"""\n    if axis is None:\n        ret = a.flatten()\n        axis = -1\n    else:\n        ret = a.copy()\n    ret.sort(axis=axis)\n    return ret\n\n\ndef lexsort(keys):\n    """"""Perform an indirect sort using an array of keys.\n\n    Args:\n        keys (cupy.ndarray): ``(k, N)`` array containing ``k`` ``(N,)``-shaped\n            arrays. The ``k`` different ""rows"" to be sorted. The last row is\n            the primary sort key.\n\n    Returns:\n        cupy.ndarray: Array of indices that sort the keys.\n\n    .. note::\n        For its implementation reason, ``cupy.lexsort`` currently supports only\n        keys with their rank of one or two and does not support ``axis``\n        parameter that ``numpy.lexsort`` supports.\n\n    .. seealso:: :func:`numpy.lexsort`\n\n    """"""\n\n    # TODO(takagi): Support axis argument.\n\n    if not cupy.cuda.thrust_enabled:\n        raise RuntimeError(\'Thrust is needed to use cupy.lexsort. Please \'\n                           \'install CUDA Toolkit with Thrust then reinstall \'\n                           \'CuPy after uninstalling it.\')\n\n    if keys.ndim == ():\n        # as numpy.lexsort() raises\n        raise TypeError(\'need sequence of keys with len > 0 in lexsort\')\n\n    if keys.ndim == 1:\n        return 0\n\n    # TODO(takagi): Support ranks of three or more.\n    if keys.ndim > 2:\n        raise NotImplementedError(\'Keys with the rank of three or more is not \'\n                                  \'supported in lexsort\')\n\n    # thrust.lexsort() assumes a C-contiguous array\n    if not keys.flags.c_contiguous:\n        keys = keys.copy(\'C\')\n\n    idx_array = cupy.ndarray(keys._shape[1:], dtype=numpy.intp)\n    k = keys._shape[0]\n    n = keys._shape[1]\n    thrust.lexsort(keys.dtype, idx_array.data.ptr, keys.data.ptr, k, n)\n\n    return idx_array\n\n\ndef argsort(a, axis=-1):\n    """"""Returns the indices that would sort an array with a stable sorting.\n\n    Args:\n        a (cupy.ndarray): Array to sort.\n        axis (int or None): Axis along which to sort. Default is -1, which\n            means sort along the last axis. If None is supplied, the array is\n            flattened before sorting.\n\n    Returns:\n        cupy.ndarray: Array of indices that sort ``a``.\n\n    .. note::\n        For its implementation reason, ``cupy.argsort`` does not support\n        ``kind`` and ``order`` parameters.\n\n    .. seealso:: :func:`numpy.argsort`\n\n    """"""\n    return a.argsort(axis=axis)\n\n\ndef msort(a):\n    """"""Returns a copy of an array sorted along the first axis.\n\n    Args:\n        a (cupy.ndarray): Array to be sorted.\n\n    Returns:\n        cupy.ndarray: Array of the same type and shape as ``a``.\n\n    .. note:\n        ``cupy.msort(a)``, the CuPy counterpart of ``numpy.msort(a)``, is\n        equivalent to ``cupy.sort(a, axis=0)``.\n\n    .. seealso:: :func:`numpy.msort`\n\n    """"""\n\n    return sort(a, axis=0)\n\n\ndef sort_complex(a):\n    """"""Sort a complex array using the real part first,\n    then the imaginary part.\n\n    Args:\n        a (cupy.ndarray): Array to be sorted.\n\n    Returns:\n        cupy.ndarray: sorted complex array.\n\n    .. seealso:: :func:`numpy.sort_complex`\n\n    """"""\n    if a.dtype.char in \'bhBHF\':\n        a = a.astype(\'F\')\n    else:\n        a = a.astype(\'D\')\n    a.sort()\n    return a\n\n\ndef partition(a, kth, axis=-1):\n    """"""Returns a partitioned copy of an array.\n\n    Creates a copy of the array whose elements are rearranged such that the\n    value of the element in k-th position would occur in that position in a\n    sorted array. All of the elements before the new k-th element are less\n    than or equal to the elements after the new k-th element.\n\n    Args:\n        a (cupy.ndarray): Array to be sorted.\n        kth (int or sequence of ints): Element index to partition by. If\n            supplied with a sequence of k-th it will partition all elements\n            indexed by k-th of them into their sorted position at once.\n        axis (int or None): Axis along which to sort. Default is -1, which\n            means sort along the last axis. If None is supplied, the array is\n            flattened before sorting.\n\n    Returns:\n        cupy.ndarray: Array of the same type and shape as ``a``.\n\n    .. seealso:: :func:`numpy.partition`\n\n    """"""\n    if axis is None:\n        ret = a.flatten()\n        axis = -1\n    else:\n        ret = a.copy()\n    ret.partition(kth, axis=axis)\n    return ret\n\n\ndef argpartition(a, kth, axis=-1):\n    """"""Returns the indices that would partially sort an array.\n\n    Args:\n        a (cupy.ndarray): Array to be sorted.\n        kth (int or sequence of ints): Element index to partition by. If\n            supplied with a sequence of k-th it will partition all elements\n            indexed by k-th of them into their sorted position at once.\n        axis (int or None): Axis along which to sort. Default is -1, which\n            means sort along the last axis. If None is supplied, the array is\n            flattened before sorting.\n\n    Returns:\n        cupy.ndarray: Array of the same type and shape as ``a``.\n\n    .. note::\n        For its implementation reason, `cupy.argpartition` fully sorts the\n        given array as `cupy.argsort` does. It also does not support ``kind``\n        and ``order`` parameters that ``numpy.argpartition`` supports.\n\n    .. seealso:: :func:`numpy.argpartition`\n\n    """"""\n    return a.argpartition(kth, axis=axis)\n'"
cupy/binary/__init__.py,0,"b'# Functions from the following NumPy document\n# https://docs.scipy.org/doc/numpy/reference/routines.bitwise.html\n\n# ""NOQA"" to suppress flake8 warning\nfrom cupy.binary import elementwise  # NOQA\nfrom cupy.binary import packing  # NOQA\n'"
cupy/binary/elementwise.py,0,b'from cupy import core\n\n\nbitwise_and = core.bitwise_and\n\n\nbitwise_or = core.bitwise_or\n\n\nbitwise_xor = core.bitwise_xor\n\n\nbitwise_not = core.invert\n\n\ninvert = core.invert\n\n\nleft_shift = core.left_shift\n\n\nright_shift = core.right_shift\n'
cupy/binary/packing.py,0,"b'import cupy\nfrom cupy import core\n\n\n_packbits_kernel = core.ElementwiseKernel(\n    \'raw T myarray, raw int32 myarray_size\', \'uint8 packed\',\n    \'\'\'for (int j = 0; j < 8; ++j) {\n        int k = i * 8 + j;\n        int bit = k < myarray_size && myarray[k] != 0;\n        packed |= bit << (7 - j);\n    }\'\'\',\n    \'packbits_kernel\'\n)\n\n\ndef packbits(myarray):\n    """"""Packs the elements of a binary-valued array into bits in a uint8 array.\n\n    This function currently does not support ``axis`` option.\n\n    Args:\n        myarray (cupy.ndarray): Input array.\n\n    Returns:\n        cupy.ndarray: The packed array.\n\n    .. note::\n        When the input array is empty, this function returns a copy of it,\n        i.e., the type of the output array is not necessarily always uint8.\n        This exactly follows the NumPy\'s behaviour (as of version 1.11),\n        alghough this is inconsistent to the documentation.\n\n    .. seealso:: :func:`numpy.packbits`\n    """"""\n    if myarray.dtype.kind not in \'biu\':\n        raise TypeError(\n            \'Expected an input array of integer or boolean data type\')\n\n    myarray = myarray.ravel()\n    packed_size = (myarray.size + 7) // 8\n    packed = cupy.zeros((packed_size,), dtype=cupy.uint8)\n    return _packbits_kernel(myarray, myarray.size, packed)\n\n\n_unpackbits_kernel = core.ElementwiseKernel(\n    \'raw uint8 myarray\', \'T unpacked\',\n    \'unpacked = (myarray[i / 8] >> (7 - i % 8)) & 1;\',\n    \'unpackbits_kernel\'\n)\n\n\ndef unpackbits(myarray):\n    """"""Unpacks elements of a uint8 array into a binary-valued output array.\n\n    This function currently does not support ``axis`` option.\n\n    Args:\n        myarray (cupy.ndarray): Input array.\n\n    Returns:\n        cupy.ndarray: The unpacked array.\n\n    .. seealso:: :func:`numpy.unpackbits`\n    """"""\n    if myarray.dtype != cupy.uint8:\n        raise TypeError(\'Expected an input array of unsigned byte data type\')\n\n    unpacked = cupy.ndarray((myarray.size * 8), dtype=cupy.uint8)\n    return _unpackbits_kernel(myarray, unpacked)\n'"
cupy/core/__init__.py,0,"b""from cupy.core import core  # NOQA\nfrom cupy.core import internal  # NOQA\n\n\n# import class and function\nfrom cupy.core._kernel import create_ufunc  # NOQA\nfrom cupy.core._kernel import ElementwiseKernel  # NOQA\nfrom cupy.core._kernel import ufunc  # NOQA\nfrom cupy.core._reduction import create_reduction_func  # NOQA\nfrom cupy.core._reduction import ReductionKernel  # NOQA\nfrom cupy.core._routines_manipulation import array_split  # NOQA\nfrom cupy.core._routines_manipulation import broadcast  # NOQA\nfrom cupy.core._routines_manipulation import broadcast_to  # NOQA\nfrom cupy.core._routines_manipulation import concatenate_method  # NOQA\nfrom cupy.core._routines_manipulation import moveaxis  # NOQA\nfrom cupy.core._routines_manipulation import rollaxis  # NOQA\nfrom cupy.core._routines_manipulation import size  # NOQA'\nfrom cupy.core._routines_math import absolute  # NOQA\nfrom cupy.core._routines_math import add  # NOQA\nfrom cupy.core._routines_math import angle  # NOQA\nfrom cupy.core._routines_math import conjugate  # NOQA\nfrom cupy.core._routines_math import divide  # NOQA\nfrom cupy.core._routines_math import floor_divide  # NOQA\nfrom cupy.core._routines_math import imag  # NOQA\nfrom cupy.core._routines_math import multiply  # NOQA\nfrom cupy.core._routines_math import negative  # NOQA\nfrom cupy.core._routines_math import power  # NOQA\nfrom cupy.core._routines_math import real  # NOQA\nfrom cupy.core._routines_math import remainder  # NOQA\nfrom cupy.core._routines_math import sqrt  # NOQA\nfrom cupy.core._routines_math import subtract  # NOQA\nfrom cupy.core._routines_math import true_divide  # NOQA\nfrom cupy.core._routines_statistics import nanmax  # NOQA\nfrom cupy.core._routines_statistics import nanmin  # NOQA\nfrom cupy.core.core import _internal_ascontiguousarray  # NOQA\nfrom cupy.core.core import _internal_asfortranarray  # NOQA\nfrom cupy.core.core import array  # NOQA\nfrom cupy.core.core import ascontiguousarray  # NOQA\nfrom cupy.core.core import asfortranarray  # NOQA\nfrom cupy.core.core import bitwise_and  # NOQA\nfrom cupy.core.core import bitwise_or  # NOQA\nfrom cupy.core.core import bitwise_xor  # NOQA\nfrom cupy.core.core import create_comparison  # NOQA\nfrom cupy.core.core import divmod  # NOQA\nfrom cupy.core.core import dot  # NOQA\nfrom cupy.core.core import elementwise_copy  # NOQA\nfrom cupy.core.core import elementwise_copy_where  # NOQA\nfrom cupy.core.core import equal  # NOQA\nfrom cupy.core.core import greater  # NOQA\nfrom cupy.core.core import greater_equal  # NOQA\nfrom cupy.core.core import invert  # NOQA\nfrom cupy.core.core import left_shift  # NOQA\nfrom cupy.core.core import less  # NOQA\nfrom cupy.core.core import less_equal  # NOQA\nfrom cupy.core.core import matmul  # NOQA\nfrom cupy.core.core import ndarray  # NOQA\nfrom cupy.core.core import not_equal  # NOQA\nfrom cupy.core.core import right_shift  # NOQA\nfrom cupy.core.core import tensordot_core  # NOQA\nfrom cupy.core.dlpack import fromDlpack  # NOQA\nfrom cupy.core.internal import complete_slice  # NOQA\nfrom cupy.core.internal import get_size  # NOQA\nfrom cupy.core.raw import RawKernel  # NOQA\nfrom cupy.core.raw import RawModule  # NOQA\n"""
cupy/core/_ufuncs.py,0,"b""from cupy.core._kernel import create_ufunc\n\n\n_complex_cast_copy = '''\ntemplate<typename T, typename U>\n__device__ void cast_copy(const U& x, T& y) {y = T(x);}\ntemplate<typename T, typename U>\n__device__ void cast_copy(const complex<U>& x, complex<T>& y) {\n    y = complex<T>(x);\n}\ntemplate<typename T, typename U>\n__device__ void cast_copy(const complex<U>& x, T& y) {y = T(x.real());}\n'''\n\n\nelementwise_copy = create_ufunc(\n    'cupy_copy',\n    ('?->?', 'b->b', 'B->B', 'h->h', 'H->H', 'i->i', 'I->I', 'l->l', 'L->L',\n     'q->q', 'Q->Q', 'e->e', 'f->f', 'd->d', 'F->F', 'D->D'),\n    'cast_copy(in0, out0)',\n    preamble=_complex_cast_copy, default_casting='unsafe')\n\n\nelementwise_copy_where = create_ufunc(\n    'cupy_copy_where',\n    ('??->?', 'b?->b', 'B?->B', 'h?->h', 'H?->H', 'i?->i', 'I?->I', 'l?->l',\n     'L?->L', 'q?->q', 'Q?->Q', 'e?->e', 'f?->f', 'd?->d', 'F?->F', 'D?->D'),\n    'if (in1) cast_copy(in0, out0)',\n    preamble=_complex_cast_copy, default_casting='unsafe')\n# complex numbers requires out0 = complex<T>(in0)\n"""
cupy/core/syncdetect.py,0,"b'import contextlib\nimport threading\n\nfrom cupy import util\n\n\n_thread_local = threading.local()\n\n\nclass DeviceSynchronized(RuntimeError):\n    """"""Raised when device synchronization is detected while disallowed.\n\n    .. seealso:: :func:`cupyx.allow_synchronize`\n\n    """"""\n\n    def __init__(self, message=None):\n        if message is None:\n            message = \'Device synchronization was detected while disallowed.\'\n        super().__init__(message)\n\n\ndef _is_allowed():\n    # Returns whether device synchronization is allowed in the current thread.\n    try:\n        return _thread_local.allowed\n    except AttributeError:\n        _thread_local.allowed = True\n        return True\n\n\ndef _declare_synchronize():\n    # Raises DeviceSynchronized if device synchronization is disallowed in\n    # the current thread.\n    if not _is_allowed():\n        raise DeviceSynchronized()\n\n\n@contextlib.contextmanager\ndef allow_synchronize(allow):\n    """"""Allows or disallows device synchronization temporarily in the current \\\nthread.\n\n    If device synchronization is detected, :class:`cupyx.DeviceSynchronized`\n    will be raised.\n\n    Note that there can be false negatives and positives.\n    Device synchronization outside CuPy will not be detected.\n    """"""\n    util.experimental(\'cupyx.allow_synchronize\')\n    old = _is_allowed()\n    _thread_local.allowed = allow\n    try:\n        yield\n    finally:\n        _thread_local.allowed = old\n'"
cupy/creation/__init__.py,0,"b'# Functions from the following NumPy document\n# https://docs.scipy.org/doc/numpy/reference/routines.array-creation.html\n\n# ""NOQA"" to suppress flake8 warning\nfrom cupy.creation import basic  # NOQA\nfrom cupy.creation import from_data  # NOQA\nfrom cupy.creation import matrix  # NOQA\nfrom cupy.creation import ranges  # NOQA\n'"
cupy/creation/basic.py,0,"b'import cupy\nimport numpy\n\nfrom cupy.core.core import _update_order_char, _get_strides_for_order_K\n\n\ndef empty(shape, dtype=float, order=\'C\'):\n    """"""Returns an array without initializing the elements.\n\n    Args:\n        shape (int or tuple of ints): Dimensionalities of the array.\n        dtype: Data type specifier.\n        order ({\'C\', \'F\'}): Row-major (C-style) or column-major\n            (Fortran-style) order.\n\n    Returns:\n        cupy.ndarray: A new array with elements not initialized.\n\n    .. seealso:: :func:`numpy.empty`\n\n    """"""\n    return cupy.ndarray(shape, dtype, order=order)\n\n\ndef _new_like_order_and_strides(a, dtype, order, shape=None):\n    """"""\n    Determine order and strides as in NumPy\'s PyArray_NewLikeArray.\n\n    (see: numpy/core/src/multiarray/ctors.c)\n    """"""\n    order = order.upper()\n    if order not in [\'C\', \'F\', \'K\', \'A\']:\n        raise TypeError(\'order not understood: {}\'.format(order))\n    if numpy.isscalar(shape):\n        shape = (shape,)\n\n    # Fallback to c_contiguous if keep order and number of dimensions\n    # of new shape mismatch\n    if order == \'K\' and shape is not None and len(shape) != a.ndim:\n        return \'C\', None, None\n\n    order = chr(_update_order_char(a, ord(order)))\n\n    if order == \'K\':\n        strides = _get_strides_for_order_K(a, numpy.dtype(dtype), shape)\n        order = \'C\'\n        memptr = cupy.empty(a.size, dtype=dtype).data\n        return order, strides, memptr\n    else:\n        return order, None, None\n\n\ndef empty_like(a, dtype=None, order=\'K\', subok=None, shape=None):\n    """"""Returns a new array with same shape and dtype of a given array.\n\n    This function currently does not support ``subok`` option.\n\n    Args:\n        a (cupy.ndarray): Base array.\n        dtype: Data type specifier. The data type of ``a`` is used by default.\n        order ({\'C\', \'F\', \'A\', or \'K\'}): Overrides the memory layout of the\n            result. \'C\' means C-order, \'F\' means F-order, \'A\' means \'F\' if\n            ``a`` is Fortran contiguous, \'C\' otherwise. \'K\' means match the\n            layout of ``a`` as closely as possible.\n        subok: Not supported yet, must be None.\n        shape (int or tuple of ints): Overrides the shape of the result. If\n            order=\'K\' and the number of dimensions is unchanged, will try to\n            keep order, otherwise, order=\'C\' is implied.\n\n\n    Returns:\n        cupy.ndarray: A new array with same shape and dtype of ``a`` with\n        elements not initialized.\n\n    .. seealso:: :func:`numpy.empty_like`\n\n    """"""\n    if subok is not None:\n        raise TypeError(\'subok is not supported yet\')\n    if dtype is None:\n        dtype = a.dtype\n\n    order, strides, memptr = _new_like_order_and_strides(a, dtype, order,\n                                                         shape)\n    shape = shape if shape else a.shape\n    return cupy.ndarray(shape, dtype, memptr, strides, order)\n\n\ndef eye(N, M=None, k=0, dtype=float):\n    """"""Returns a 2-D array with ones on the diagonals and zeros elsewhere.\n\n    Args:\n        N (int): Number of rows.\n        M (int): Number of columns. M == N by default.\n        k (int): Index of the diagonal. Zero indicates the main diagonal,\n            a positive index an upper diagonal, and a negative index a lower\n            diagonal.\n        dtype: Data type specifier.\n\n    Returns:\n        cupy.ndarray: A 2-D array with given diagonals filled with ones and\n        zeros elsewhere.\n\n    .. seealso:: :func:`numpy.eye`\n\n    """"""\n    if M is None:\n        M = N\n    ret = zeros((N, M), dtype)\n    ret.diagonal(k)[:] = 1\n    return ret\n\n\ndef identity(n, dtype=float):\n    """"""Returns a 2-D identity array.\n\n    It is equivalent to ``eye(n, n, dtype)``.\n\n    Args:\n        n (int): Number of rows and columns.\n        dtype: Data type specifier.\n\n    Returns:\n        cupy.ndarray: A 2-D identity array.\n\n    .. seealso:: :func:`numpy.identity`\n\n    """"""\n    return eye(n, dtype=dtype)\n\n\ndef ones(shape, dtype=float):\n    """"""Returns a new array of given shape and dtype, filled with ones.\n\n    This function currently does not support ``order`` option.\n\n    Args:\n        shape (int or tuple of ints): Dimensionalities of the array.\n        dtype: Data type specifier.\n\n    Returns:\n        cupy.ndarray: An array filled with ones.\n\n    .. seealso:: :func:`numpy.ones`\n\n    """"""\n    # TODO(beam2d): Support ordering option\n    a = cupy.ndarray(shape, dtype)\n    a.fill(1)\n    return a\n\n\ndef ones_like(a, dtype=None, order=\'K\', subok=None, shape=None):\n    """"""Returns an array of ones with same shape and dtype as a given array.\n\n    This function currently does not support ``subok`` option.\n\n    Args:\n        a (cupy.ndarray): Base array.\n        dtype: Data type specifier. The dtype of ``a`` is used by default.\n        order ({\'C\', \'F\', \'A\', or \'K\'}): Overrides the memory layout of the\n            result. \'C\' means C-order, \'F\' means F-order, \'A\' means \'F\' if\n            ``a`` is Fortran contiguous, \'C\' otherwise. \'K\' means match the\n            layout of ``a`` as closely as possible.\n        subok: Not supported yet, must be None.\n        shape (int or tuple of ints): Overrides the shape of the result. If\n            order=\'K\' and the number of dimensions is unchanged, will try to\n            keep order, otherwise, order=\'C\' is implied.\n\n    Returns:\n        cupy.ndarray: An array filled with ones.\n\n    .. seealso:: :func:`numpy.ones_like`\n\n    """"""\n    if subok is not None:\n        raise TypeError(\'subok is not supported yet\')\n    if dtype is None:\n        dtype = a.dtype\n\n    order, strides, memptr = _new_like_order_and_strides(a, dtype, order,\n                                                         shape)\n    shape = shape if shape else a.shape\n    a = cupy.ndarray(shape, dtype, memptr, strides, order)\n    a.fill(1)\n    return a\n\n\ndef zeros(shape, dtype=float, order=\'C\'):\n    """"""Returns a new array of given shape and dtype, filled with zeros.\n\n    Args:\n        shape (int or tuple of ints): Dimensionalities of the array.\n        dtype: Data type specifier.\n        order ({\'C\', \'F\'}): Row-major (C-style) or column-major\n            (Fortran-style) order.\n\n    Returns:\n        cupy.ndarray: An array filled with zeros.\n\n    .. seealso:: :func:`numpy.zeros`\n\n    """"""\n    a = cupy.ndarray(shape, dtype, order=order)\n    a.data.memset_async(0, a.nbytes)\n    return a\n\n\ndef zeros_like(a, dtype=None, order=\'K\', subok=None, shape=None):\n    """"""Returns an array of zeros with same shape and dtype as a given array.\n\n    This function currently does not support ``subok`` option.\n\n    Args:\n        a (cupy.ndarray): Base array.\n        dtype: Data type specifier. The dtype of ``a`` is used by default.\n        order ({\'C\', \'F\', \'A\', or \'K\'}): Overrides the memory layout of the\n            result. \'C\' means C-order, \'F\' means F-order, \'A\' means \'F\' if\n            ``a`` is Fortran contiguous, \'C\' otherwise. \'K\' means match the\n            layout of ``a`` as closely as possible.\n        subok: Not supported yet, must be None.\n        shape (int or tuple of ints): Overrides the shape of the result. If\n            order=\'K\' and the number of dimensions is unchanged, will try to\n            keep order, otherwise, order=\'C\' is implied.\n\n    Returns:\n        cupy.ndarray: An array filled with zeros.\n\n    .. seealso:: :func:`numpy.zeros_like`\n\n    """"""\n    if subok is not None:\n        raise TypeError(\'subok is not supported yet\')\n    if dtype is None:\n        dtype = a.dtype\n\n    order, strides, memptr = _new_like_order_and_strides(a, dtype, order,\n                                                         shape)\n    shape = shape if shape else a.shape\n    a = cupy.ndarray(shape, dtype, memptr, strides, order)\n    a.data.memset_async(0, a.nbytes)\n    return a\n\n\ndef full(shape, fill_value, dtype=None):\n    """"""Returns a new array of given shape and dtype, filled with a given value.\n\n    This function currently does not support ``order`` option.\n\n    Args:\n        shape (int or tuple of ints): Dimensionalities of the array.\n        fill_value: A scalar value to fill a new array.\n        dtype: Data type specifier.\n\n    Returns:\n        cupy.ndarray: An array filled with ``fill_value``.\n\n    .. seealso:: :func:`numpy.full`\n\n    """"""\n    # TODO(beam2d): Support ordering option\n    if dtype is None:\n        if isinstance(fill_value, cupy.ndarray):\n            dtype = fill_value.dtype\n        else:\n            dtype = numpy.array(fill_value).dtype\n    a = cupy.ndarray(shape, dtype)\n    a.fill(fill_value)\n    return a\n\n\ndef full_like(a, fill_value, dtype=None, order=\'K\', subok=None, shape=None):\n    """"""Returns a full array with same shape and dtype as a given array.\n\n    This function currently does not support ``subok`` option.\n\n    Args:\n        a (cupy.ndarray): Base array.\n        fill_value: A scalar value to fill a new array.\n        dtype: Data type specifier. The dtype of ``a`` is used by default.\n        order ({\'C\', \'F\', \'A\', or \'K\'}): Overrides the memory layout of the\n            result. \'C\' means C-order, \'F\' means F-order, \'A\' means \'F\' if\n            ``a`` is Fortran contiguous, \'C\' otherwise. \'K\' means match the\n            layout of ``a`` as closely as possible.\n        subok: Not supported yet, must be None.\n        shape (int or tuple of ints): Overrides the shape of the result. If\n            order=\'K\' and the number of dimensions is unchanged, will try to\n            keep order, otherwise, order=\'C\' is implied.\n\n    Returns:\n        cupy.ndarray: An array filled with ``fill_value``.\n\n    .. seealso:: :func:`numpy.full_like`\n\n    """"""\n    if subok is not None:\n        raise TypeError(\'subok is not supported yet\')\n    if dtype is None:\n        dtype = a.dtype\n\n    order, strides, memptr = _new_like_order_and_strides(a, dtype, order,\n                                                         shape)\n    shape = shape if shape else a.shape\n    a = cupy.ndarray(shape, dtype, memptr, strides, order)\n    a.fill(fill_value)\n    return a\n'"
cupy/creation/from_data.py,0,"b'import numpy\n\nfrom cupy import core\nfrom cupy.core import fusion\n\n\ndef array(obj, dtype=None, copy=True, order=\'K\', subok=False, ndmin=0):\n    """"""Creates an array on the current device.\n\n    This function currently does not support the ``subok`` option.\n\n    Args:\n        obj: :class:`cupy.ndarray` object or any other object that can be\n            passed to :func:`numpy.array`.\n        dtype: Data type specifier.\n        copy (bool): If ``False``, this function returns ``obj`` if possible.\n            Otherwise this function always returns a new array.\n        order ({\'C\', \'F\', \'A\', \'K\'}): Row-major (C-style) or column-major\n            (Fortran-style) order.\n            When ``order`` is \'A\', it uses \'F\' if ``a`` is column-major and\n            uses \'C\' otherwise.\n            And when ``order`` is \'K\', it keeps strides as closely as\n            possible.\n            If ``obj`` is :class:`numpy.ndarray`, the function returns \'C\' or\n            \'F\' order array.\n        subok (bool): If True, then sub-classes will be passed-through,\n            otherwise the returned array will be forced to be a base-class\n            array (default).\n        ndmin (int): Minimum number of dimensions. Ones are inserted to the\n            head of the shape if needed.\n\n    Returns:\n        cupy.ndarray: An array on the current device.\n\n\n\n    .. note::\n       This method currently does not support ``subok`` argument.\n\n    .. seealso:: :func:`numpy.array`\n\n    """"""\n    return core.array(obj, dtype, copy, order, subok, ndmin)\n\n\ndef asarray(a, dtype=None, order=None):\n    """"""Converts an object to array.\n\n    This is equivalent to ``array(a, dtype, copy=False)``.\n    This function currently does not support the ``order`` option.\n\n    Args:\n        a: The source object.\n        dtype: Data type specifier. It is inferred from the input by default.\n        order ({\'C\', \'F\'}):\n            Whether to use row-major (C-style) or column-major (Fortran-style)\n            memory representation. Defaults to \'C\'. ``order`` is ignored for\n            objects that are not :class:`cupy.ndarray`, but have the\n            ``__cuda_array_interface__`` attribute.\n\n    Returns:\n        cupy.ndarray: An array on the current device. If ``a`` is already on\n        the device, no copy is performed.\n\n    .. seealso:: :func:`numpy.asarray`\n\n    """"""\n    return core.array(a, dtype, False, order)\n\n\ndef asanyarray(a, dtype=None, order=None):\n    """"""Converts an object to array.\n\n    This is currently equivalent to :func:`~cupy.asarray`, since there is no\n    subclass of ndarray in CuPy. Note that the original\n    :func:`numpy.asanyarray` returns the input array as is if it is an instance\n    of a subtype of :class:`numpy.ndarray`.\n\n    .. seealso:: :func:`cupy.asarray`, :func:`numpy.asanyarray`\n\n    """"""\n    return core.array(a, dtype, False, order)\n\n\ndef ascontiguousarray(a, dtype=None):\n    """"""Returns a C-contiguous array.\n\n    Args:\n        a (cupy.ndarray): Source array.\n        dtype: Data type specifier.\n\n    Returns:\n        cupy.ndarray: If no copy is required, it returns ``a``. Otherwise, it\n        returns a copy of ``a``.\n\n    .. seealso:: :func:`numpy.ascontiguousarray`\n\n    """"""\n    return core.ascontiguousarray(a, dtype)\n\n\n# TODO(okuta): Implement asmatrix\n\n\ndef copy(a, order=\'K\'):\n    """"""Creates a copy of a given array on the current device.\n\n    This function allocates the new array on the current device. If the given\n    array is allocated on the different device, then this function tries to\n    copy the contents over the devices.\n\n    Args:\n        a (cupy.ndarray): The source array.\n        order ({\'C\', \'F\', \'A\', \'K\'}): Row-major (C-style) or column-major\n            (Fortran-style) order.\n            When `order` is \'A\', it uses \'F\' if `a` is column-major and\n            uses `C` otherwise.\n            And when `order` is \'K\', it keeps strides as closely as\n            possible.\n\n    Returns:\n        cupy.ndarray: The copy of ``a`` on the current device.\n\n    See: :func:`numpy.copy`, :meth:`cupy.ndarray.copy`\n\n    """"""\n    if fusion._is_fusing():\n        if order != \'K\':\n            raise NotImplementedError(\n                \'cupy.copy does not support `order` in fusion yet.\')\n        return fusion._call_ufunc(core.elementwise_copy, a)\n\n    # If the current device is different from the device of ``a``, then this\n    # function allocates a new array on the current device, and copies the\n    # contents over the devices.\n    return a.copy(order=order)\n\n\n# TODO(okuta): Implement frombuffer\n\n\ndef fromfile(*args, **kwargs):\n    """"""Reads an array from a file.\n\n    .. note::\n        Uses NumPy\'s ``fromfile`` and coerces the result to a CuPy array.\n\n    .. seealso:: :func:`numpy.fromfile`\n    """"""\n\n    return asarray(numpy.fromfile(*args, **kwargs))\n\n\n# TODO(okuta): Implement fromfunction\n\n\n# TODO(okuta): Implement fromiter\n\n\n# TODO(okuta): Implement fromstring\n\n\n# TODO(okuta): Implement loadtxt\n'"
cupy/creation/matrix.py,0,"b'import numpy\n\nimport cupy\nfrom cupy import core\n\n\ndef diag(v, k=0):\n    """"""Returns a diagonal or a diagonal array.\n\n    Args:\n        v (array-like): Array or array-like object.\n        k (int): Index of diagonals. Zero indicates the main diagonal, a\n            positive value an upper diagonal, and a negative value a lower\n            diagonal.\n\n    Returns:\n        cupy.ndarray: If ``v`` indicates a 1-D array, then it returns a 2-D\n        array with the specified diagonal filled by ``v``. If ``v`` indicates a\n        2-D array, then it returns the specified diagonal of ``v``. In latter\n        case, if ``v`` is a :class:`cupy.ndarray` object, then its view is\n        returned.\n\n    .. seealso:: :func:`numpy.diag`\n\n    """"""\n    if isinstance(v, cupy.ndarray):\n        ndim = v.ndim\n    else:\n        ndim = numpy.ndim(v)\n        if ndim == 1:\n            v = cupy.array(v)\n        if ndim == 2:\n            # to save bandwidth, don\'t copy non-diag elements to GPU\n            v = numpy.array(v)\n\n    if ndim == 1:\n        size = v.size + abs(k)\n        ret = cupy.zeros((size, size), dtype=v.dtype)\n        ret.diagonal(k)[:] = v\n        return ret\n    elif ndim == 2:\n        return cupy.array(v.diagonal(k))\n    else:\n        raise ValueError(\'Input must be 1- or 2-d.\')\n\n\ndef diagflat(v, k=0):\n    """"""Creates a diagonal array from the flattened input.\n\n    Args:\n        v (array-like): Array or array-like object.\n        k (int): Index of diagonals. See :func:`cupy.diag` for detail.\n\n    Returns:\n        cupy.ndarray: A 2-D diagonal array with the diagonal copied from ``v``.\n\n    """"""\n    if numpy.isscalar(v):\n        v = numpy.asarray(v)\n\n    return cupy.diag(v.ravel(), k)\n\n\n_tri_kernel = core.ElementwiseKernel(\n    \'int32 m, int32 k\',\n    \'T out\',\n    \'\'\'\n    int row = i / m;\n    int col = i % m;\n    out = (col <= row + k);\n    \'\'\',\n    \'tri\',\n)\n\n\ndef tri(N, M=None, k=0, dtype=float):\n    """"""Creates an array with ones at and below the given diagonal.\n\n    Args:\n        N (int): Number of rows.\n        M (int): Number of columns. M == N by default.\n        k (int): The sub-diagonal at and below which the array is filled. Zero\n            is the main diagonal, a positive value is above it, and a negative\n            value is below.\n        dtype: Data type specifier.\n\n    Returns:\n        cupy.ndarray: An array with ones at and below the given diagonal.\n\n    .. seealso:: :func:`numpy.tri`\n\n    """"""\n    if M is None:\n        M = N\n    out = cupy.empty((N, M), dtype=dtype)\n\n    return _tri_kernel(M, k, out)\n\n\ndef tril(m, k=0):\n    """"""Returns a lower triangle of an array.\n\n    Args:\n        m (array-like): Array or array-like object.\n        k (int): The diagonal above which to zero elements. Zero is the main\n            diagonal, a positive value is above it, and a negative value is\n            below.\n\n    Returns:\n        cupy.ndarray: A lower triangle of an array.\n\n    .. seealso:: :func:`numpy.tril`\n\n    """"""\n    m = cupy.asarray(m)\n    mask = tri(*m.shape[-2:], k=k, dtype=bool)\n\n    return cupy.where(mask, m, m.dtype.type(0))\n\n\ndef triu(m, k=0):\n    """"""Returns an upper triangle of an array.\n\n    Args:\n        m (array-like): Array or array-like object.\n        k (int): The diagonal below which to zero elements. Zero is the main\n            diagonal, a positive value is above it, and a negative value is\n            below.\n\n    Returns:\n        cupy.ndarray: An upper triangle of an array.\n\n    .. seealso:: :func:`numpy.triu`\n\n    """"""\n    m = cupy.asarray(m)\n    mask = tri(*m.shape[-2:], k=k-1, dtype=bool)\n\n    return cupy.where(mask, m.dtype.type(0), m)\n\n\n# TODO(okuta): Implement vander\n\n\n# TODO(okuta): Implement mat\n\n\n# TODO(okuta): Implement bmat\n'"
cupy/creation/ranges.py,0,"b'import math\n\nimport numpy\n\nimport cupy\nfrom cupy import core\n\n\ndef arange(start, stop=None, step=1, dtype=None):\n    """"""Returns an array with evenly spaced values within a given interval.\n\n    Values are generated within the half-open interval [start, stop). The first\n    three arguments are mapped like the ``range`` built-in function, i.e. start\n    and step are optional.\n\n    Args:\n        start: Start of the interval.\n        stop: End of the interval.\n        step: Step width between each pair of consecutive values.\n        dtype: Data type specifier. It is inferred from other arguments by\n            default.\n\n    Returns:\n        cupy.ndarray: The 1-D array of range values.\n\n    .. seealso:: :func:`numpy.arange`\n\n    """"""\n    if dtype is None:\n        if any(numpy.dtype(type(val)).kind == \'f\'\n               for val in (start, stop, step)):\n            dtype = float\n        else:\n            dtype = int\n\n    if stop is None:\n        stop = start\n        start = 0\n\n    if step is None:\n        step = 1\n\n    size = int(numpy.ceil((stop - start) / step))\n    if size <= 0:\n        return cupy.empty((0,), dtype=dtype)\n\n    if numpy.dtype(dtype).type == numpy.bool_:\n        if size > 2:\n            raise ValueError(\'no fill-function for data-type.\')\n        if size == 2:\n            return cupy.array([start, start - step], dtype=numpy.bool_)\n        else:\n            return cupy.array([start], dtype=numpy.bool_)\n\n    ret = cupy.empty((size,), dtype=dtype)\n    typ = numpy.dtype(dtype).type\n    _arange_ufunc(typ(start), typ(step), ret, dtype=dtype)\n    return ret\n\n\ndef _linspace_scalar(start, stop, num=50, endpoint=True, retstep=False,\n                     dtype=None):\n    """"""Returns an array with evenly-spaced values within a given interval.\n\n    Instead of specifying the step width like :func:`cupy.arange`, this\n    function requires the total number of elements specified.\n\n    Args:\n        start: Start of the interval.\n        stop: End of the interval.\n        num: Number of elements.\n        endpoint (bool): If ``True``, the stop value is included as the last\n            element. Otherwise, the stop value is omitted.\n        retstep (bool): If ``True``, this function returns (array, step).\n            Otherwise, it returns only the array.\n        dtype: Data type specifier. It is inferred from the start and stop\n            arguments by default.\n\n    Returns:\n        cupy.ndarray: The 1-D array of ranged values.\n\n    """"""\n    if dtype is None:\n        # In actual implementation, only float is used\n        dtype = float\n\n    ret = cupy.empty((num,), dtype=dtype)\n    div = (num - 1) if endpoint else num\n    if div <= 0:\n        if num > 0:\n            ret.fill(start)\n        step = float(\'nan\')\n    else:\n        step = float(stop - start) / div\n        stop = float(stop)\n\n        if step == 0.0:\n            # for underflow\n            _linspace_ufunc_underflow(start, stop - start, div, ret,\n                                      casting=\'unsafe\')\n        else:\n            _linspace_ufunc(start, step, ret, casting=\'unsafe\')\n\n        if endpoint:\n            # Here num == div + 1 > 1 is ensured.\n            ret[-1] = stop\n\n    if retstep:\n        return ret, step\n    else:\n        return ret\n\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,\n             axis=0):\n    """"""Returns an array with evenly-spaced values within a given interval.\n\n    Instead of specifying the step width like :func:`cupy.arange`, this\n    function requires the total number of elements specified.\n\n    Args:\n        start (scalar or array_like): Starting value(s) of the sequence.\n        stop (scalar or array_like): Ending value(s) of the sequence, unless\n            `endpoint` is set to False. In that case, the sequence consists of\n            all but the last of ``num + 1`` evenly spaced samples, so that\n            `stop` is excluded.  Note that the step size changes when\n            `endpoint` is False.\n        num: Number of elements.\n        endpoint (bool): If ``True``, the stop value is included as the last\n            element. Otherwise, the stop value is omitted.\n        retstep (bool): If ``True``, this function returns (array, step).\n            Otherwise, it returns only the array.\n        dtype: Data type specifier. It is inferred from the start and stop\n            arguments by default.\n        axis (int):  The axis in the result to store the samples.  Relevant\n            only if start or stop are array-like.  By default (0), the samples\n            will be along a new axis inserted at the beginning. Use -1 to get\n            an axis at the end.\n\n    Returns:\n        cupy.ndarray: The 1-D array of ranged values.\n\n    """"""\n    if num < 0:\n        raise ValueError(\'linspace with num<0 is not supported\')\n    div = (num - 1) if endpoint else num\n\n    scalar_start = cupy.isscalar(start)\n    scalar_stop = cupy.isscalar(stop)\n    if scalar_start and scalar_stop:\n        return _linspace_scalar(start, stop, num, endpoint, retstep, dtype)\n\n    if not scalar_start:\n        if not (isinstance(start, cupy.ndarray) and start.dtype.kind == \'f\'):\n            start = cupy.asarray(start) * 1.0\n\n    if not scalar_stop:\n        if not (isinstance(stop, cupy.ndarray) and stop.dtype.kind == \'f\'):\n            stop = cupy.asarray(stop) * 1.0\n\n    dt = cupy.result_type(start, stop, float(num))\n    if dtype is None:\n        # In actual implementation, only float is used\n        dtype = dt\n\n    delta = stop - start\n\n    # ret = cupy.arange(0, num, dtype=dt).reshape((-1,) + (1,) * delta.ndim)\n    ret = cupy.empty((num,), dtype=dt)\n    _arange_ufunc(0.0, 1.0, ret, dtype=dt)\n    ret = ret.reshape((-1,) + (1,) * delta.ndim)\n\n    # In-place multiplication y *= delta/div is faster, but prevents the\n    # multiplicant from overriding what class is produced, and thus prevents,\n    # e.g. use of Quantities, see numpy#7142. Hence, we multiply in place only\n    # for standard scalar types.\n    if num > 1:\n        step = delta / div\n        if cupy.any(step == 0):\n            # Special handling for denormal numbers, numpy#5437\n            ret /= div\n            ret = ret * delta\n        else:\n            ret = ret * step\n    else:\n        # 0 and 1 item long sequences have an undefined step\n        step = float(\'nan\')\n        # Multiply with delta to allow possible override of output class.\n        ret = ret * delta\n\n    ret += start\n    if endpoint and num > 1:\n        ret[-1] = stop\n\n    if axis != 0:\n        ret = cupy.moveaxis(ret, 0, axis)\n\n    if retstep:\n        return ret.astype(dtype, copy=False), step\n    else:\n        return ret.astype(dtype, copy=False)\n\n\ndef logspace(start, stop, num=50, endpoint=True, base=10.0, dtype=None):\n    """"""Returns an array with evenly-spaced values on a log-scale.\n\n    Instead of specifying the step width like :func:`cupy.arange`, this\n    function requires the total number of elements specified.\n\n    Args:\n        start: Start of the interval.\n        stop: End of the interval.\n        num: Number of elements.\n        endpoint (bool): If ``True``, the stop value is included as the last\n            element. Otherwise, the stop value is omitted.\n        base (float): Base of the log space. The step sizes between the\n            elements on a log-scale are the same as ``base``.\n        dtype: Data type specifier. It is inferred from the start and stop\n            arguments by default.\n\n    Returns:\n        cupy.ndarray: The 1-D array of ranged values.\n\n    """"""\n    y = linspace(start, stop, num=num, endpoint=endpoint)\n    if dtype is None:\n        return core.power(base, y)\n    return core.power(base, y).astype(dtype)\n\n\ndef meshgrid(*xi, **kwargs):\n    """"""Return coordinate matrices from coordinate vectors.\n\n    Given one-dimensional coordinate arrays x1, x2, ..., xn, this function\n    makes N-D grids.\n\n    For one-dimensional arrays x1, x2, ..., xn with lengths ``Ni = len(xi)``,\n    this function returns ``(N1, N2, N3, ..., Nn)`` shaped arrays\n    if indexing=\'ij\' or ``(N2, N1, N3, ..., Nn)`` shaped arrays\n    if indexing=\'xy\'.\n\n    Unlike NumPy, CuPy currently only supports 1-D arrays as inputs.\n\n    Args:\n        xi (tuple of ndarrays): 1-D arrays representing the coordinates\n            of a grid.\n        indexing ({\'xy\', \'ij\'}, optional): Cartesian (\'xy\', default) or\n            matrix (\'ij\') indexing of output.\n        sparse (bool, optional): If ``True`` a sparse grid is returned in order\n            to conserve memory. Default is False.\n        copy (bool, optional): If ``False``, a view\n            into the original arrays are returned. Default is True.\n\n    Returns:\n        list of cupy.ndarray\n\n    .. seealso:: :func:`numpy.meshgrid`\n\n    """"""\n\n    indexing = kwargs.pop(\'indexing\', \'xy\')\n    copy = bool(kwargs.pop(\'copy\', True))\n    sparse = bool(kwargs.pop(\'sparse\', False))\n    if kwargs:\n        raise TypeError(\n            \'meshgrid() got an unexpected keyword argument \\\'{}\\\'\'.format(\n                list(kwargs)[0]))\n    if indexing not in [\'xy\', \'ij\']:\n        raise ValueError(\'Valid values for `indexing` are \\\'xy\\\' and \\\'ij\\\'.\')\n\n    for x in xi:\n        if x.ndim != 1:\n            raise ValueError(\'input has to be 1d\')\n        if not isinstance(x, cupy.ndarray):\n            raise ValueError(\'input has to be cupy.ndarray\')\n    if len(xi) <= 1:\n        return list(xi)\n\n    meshes = []\n    for i, x in enumerate(xi):\n        if indexing == \'xy\' and i == 0:\n            left_none = 1\n        elif indexing == \'xy\' and i == 1:\n            left_none = 0\n        else:\n            left_none = i\n\n        expand_slices = ((None,) * left_none +\n                         (slice(None),) +\n                         (None,) * (len(xi) - (left_none + 1)))\n        meshes.append(x[expand_slices])\n\n    if sparse:\n        meshes_br = meshes\n    else:\n        meshes_br = list(cupy.broadcast_arrays(*meshes))\n\n    if copy:\n        for i in range(len(meshes_br)):\n            meshes_br[i] = meshes_br[i].copy()\n    return meshes_br\n\n\nclass nd_grid(object):\n    """"""Construct a multi-dimensional ""meshgrid"".\n\n    ``grid = nd_grid()`` creates an instance which will return a mesh-grid\n    when indexed.  The dimension and number of the output arrays are equal\n    to the number of indexing dimensions.  If the step length is not a\n    complex number, then the stop is not inclusive.\n\n    However, if the step length is a **complex number** (e.g. 5j), then the\n    integer part of its magnitude is interpreted as specifying the\n    number of points to create between the start and stop values, where\n    the stop value **is inclusive**.\n\n    If instantiated with an argument of ``sparse=True``, the mesh-grid is\n    open (or not fleshed out) so that only one-dimension of each returned\n    argument is greater than 1.\n\n    Args:\n        sparse (bool, optional): Whether the grid is sparse or not.\n            Default is False.\n\n    .. seealso:: :data:`numpy.mgrid` and :data:`numpy.ogrid`\n\n    """"""\n\n    def __init__(self, sparse=False):\n        self.sparse = sparse\n\n    def __getitem__(self, key):\n        if isinstance(key, slice):\n            step = key.step\n            stop = key.stop\n            start = key.start\n            if start is None:\n                start = 0\n            if isinstance(step, complex):\n                step = abs(step)\n                length = int(step)\n                if step != 1:\n                    step = (key.stop - start) / float(step - 1)\n                stop = key.stop + step\n                return cupy.arange(0, length, 1, float) * step + start\n            else:\n                return cupy.arange(start, stop, step)\n\n        size = []\n        typ = int\n        for k in range(len(key)):\n            step = key[k].step\n            start = key[k].start\n            if start is None:\n                start = 0\n            if step is None:\n                step = 1\n            if isinstance(step, complex):\n                size.append(int(abs(step)))\n                typ = float\n            else:\n                size.append(\n                    int(math.ceil((key[k].stop - start) / (step * 1.0))))\n            if (isinstance(step, float) or\n                    isinstance(start, float) or\n                    isinstance(key[k].stop, float)):\n                typ = float\n        if self.sparse:\n            nn = [cupy.arange(_x, dtype=_t)\n                  for _x, _t in zip(size, (typ,) * len(size))]\n        else:\n            nn = cupy.indices(size, typ)\n        for k in range(len(size)):\n            step = key[k].step\n            start = key[k].start\n            if start is None:\n                start = 0\n            if step is None:\n                step = 1\n            if isinstance(step, complex):\n                step = int(abs(step))\n                if step != 1:\n                    step = (key[k].stop - start) / float(step - 1)\n            nn[k] = (nn[k] * step + start)\n        if self.sparse:\n            slobj = [cupy.newaxis] * len(size)\n            for k in range(len(size)):\n                slobj[k] = slice(None, None)\n                nn[k] = nn[k][slobj]\n                slobj[k] = cupy.newaxis\n        return nn\n\n    def __len__(self):\n        return 0\n\n\nmgrid = nd_grid(sparse=False)\nogrid = nd_grid(sparse=True)\n\n\n_arange_ufunc = core.create_ufunc(\n    \'cupy_arange\',\n    (\'bb->b\', \'BB->B\', \'hh->h\', \'HH->H\', \'ii->i\', \'II->I\', \'ll->l\', \'LL->L\',\n     \'qq->q\', \'QQ->Q\', \'ee->e\', \'ff->f\', \'dd->d\',\n     (\'FF->F\', \'out0 = in0 + float(i) * in1\'),\n     (\'DD->D\', \'out0 = in0 + double(i) * in1\')),\n    \'out0 = in0 + i * in1\')\n\n_linspace_ufunc = core.create_ufunc(\n    \'cupy_linspace\',\n    (\'dd->d\',),\n    \'out0 = in0 + i * in1\')\n\n_linspace_ufunc_underflow = core.create_ufunc(\n    \'cupy_linspace\',\n    (\'ddd->d\',),\n    \'out0 = in0 + i * in1 / in2\')\n'"
cupy/cuda/__init__.py,0,"b'import contextlib\nimport os\n\nfrom cupy._environment import get_cuda_path, get_nvcc_path  # NOQA\nfrom cupy.cuda import compiler  # NOQA\nfrom cupy.cuda import device  # NOQA\nfrom cupy.cuda import driver  # NOQA\nfrom cupy.cuda import function  # NOQA\nfrom cupy.cuda import memory  # NOQA\nfrom cupy.cuda import memory_hook  # NOQA\nfrom cupy.cuda import memory_hooks  # NOQA\nfrom cupy.cuda import pinned_memory  # NOQA\nfrom cupy.cuda import profiler  # NOQA\nfrom cupy.cuda import runtime  # NOQA\nfrom cupy.cuda import stream  # NOQA\nfrom cupy.cuda import texture  # NOQA\n\n\n_available = None\n_cub_disabled = None\n\n\nfrom cupy.cuda import cusolver  # NOQA\n# This flag is kept for backward compatibility.\n# It is always True as cuSOLVER library is always available in CUDA 8.0+.\ncusolver_enabled = True\n\ntry:\n    from cupy.cuda import nvtx  # NOQA\n    nvtx_enabled = True\nexcept ImportError:\n    nvtx_enabled = False\n\ntry:\n    from cupy.cuda import thrust  # NOQA\n    thrust_enabled = True\nexcept ImportError:\n    thrust_enabled = False\n\ncub_enabled = False\nif int(os.getenv(\'CUB_DISABLED\', 0)) == 0:\n    try:\n        from cupy.cuda import cub  # NOQA\n        cub_enabled = True\n    except ImportError:\n        pass\n\ntry:\n    from cupy.cuda import nccl  # NOQA\n    nccl_enabled = True\nexcept ImportError:\n    nccl_enabled = False\n\ntry:\n    from cupy.cuda import cutensor  # NOQA\n    cutensor_enabled = True\nexcept ImportError:\n    cutensor_enabled = False\n\n\ndef is_available():\n    global _available\n    if _available is None:\n        _available = False\n        try:\n            _available = runtime.getDeviceCount() > 0\n        except Exception as e:\n            if (e.args[0] !=\n                    \'cudaErrorNoDevice: no CUDA-capable device is detected\'):\n                raise\n    return _available\n\n\n# import class and function\nfrom cupy.cuda.compiler import compile_with_cache  # NOQA\nfrom cupy.cuda.device import Device  # NOQA\nfrom cupy.cuda.device import get_cublas_handle  # NOQA\nfrom cupy.cuda.device import get_device_id  # NOQA\nfrom cupy.cuda.function import Function  # NOQA\nfrom cupy.cuda.function import Module  # NOQA\nfrom cupy.cuda.memory import alloc  # NOQA\nfrom cupy.cuda.memory import BaseMemory  # NOQA\nfrom cupy.cuda.memory import malloc_managed  # NOQA\nfrom cupy.cuda.memory import ManagedMemory  # NOQA\nfrom cupy.cuda.memory import Memory  # NOQA\nfrom cupy.cuda.memory import MemoryPointer  # NOQA\nfrom cupy.cuda.memory import MemoryPool  # NOQA\nfrom cupy.cuda.memory import set_allocator  # NOQA\nfrom cupy.cuda.memory import get_allocator  # NOQA\nfrom cupy.cuda.memory import UnownedMemory  # NOQA\nfrom cupy.cuda.memory_hook import MemoryHook  # NOQA\nfrom cupy.cuda.pinned_memory import alloc_pinned_memory  # NOQA\nfrom cupy.cuda.pinned_memory import PinnedMemory  # NOQA\nfrom cupy.cuda.pinned_memory import PinnedMemoryPointer  # NOQA\nfrom cupy.cuda.pinned_memory import PinnedMemoryPool  # NOQA\nfrom cupy.cuda.pinned_memory import set_pinned_memory_allocator  # NOQA\nfrom cupy.cuda.stream import Event  # NOQA\nfrom cupy.cuda.stream import get_current_stream  # NOQA\nfrom cupy.cuda.stream import get_elapsed_time  # NOQA\nfrom cupy.cuda.stream import Stream  # NOQA\nfrom cupy.cuda.stream import ExternalStream  # NOQA\n\n\n@contextlib.contextmanager\ndef using_allocator(allocator=None):\n    """"""Sets a thread-local allocator for GPU memory inside\n       context manager\n\n    Args:\n        allocator (function): CuPy memory allocator. It must have the same\n            interface as the :func:`cupy.cuda.alloc` function, which takes the\n            buffer size as an argument and returns the device buffer of that\n            size. When ``None`` is specified, raw memory allocator will be\n            used (i.e., memory pool is disabled).\n    """"""\n    # Note: cupy/memory.pyx would be the better place to implement this\n    # function but `contextmanager` decoration doesn\'t behave well in Cython.\n    if allocator is None:\n        allocator = memory._malloc\n    previous_allocator = memory._get_thread_local_allocator()\n    memory._set_thread_local_allocator(allocator)\n    try:\n        yield\n    finally:\n        memory._set_thread_local_allocator(previous_allocator)\n\n\n@contextlib.contextmanager\ndef profile():\n    """"""Enable CUDA profiling during with statement.\n\n    This function enables profiling on entering a with statement, and disables\n    profiling on leaving the statement.\n\n    >>> with cupy.cuda.profile():\n    ...    # do something you want to measure\n    ...    pass\n\n    """"""\n    profiler.start()\n    try:\n        yield\n    finally:\n        profiler.stop()\n'"
cupy/cuda/compiler.py,0,"b'import hashlib\nimport math\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport tempfile\n\nfrom cupy.cuda import device\nfrom cupy.cuda import function\nfrom cupy.cuda import nvrtc\nfrom cupy.cuda import runtime\nfrom cupy import util\n\n\n_nvrtc_version = None\n_win32 = sys.platform.startswith(\'win32\')\n_rdc_flags = (\'--device-c\', \'-dc\', \'-rdc=true\',\n              \'--relocatable-device-code=true\')\n_cudadevrt = None\n\n\nclass NVCCException(Exception):\n    pass\n\n\ndef _run_nvcc(cmd, cwd):\n    try:\n        return subprocess.check_output(cmd, cwd=cwd, stderr=subprocess.STDOUT)\n    except subprocess.CalledProcessError as e:\n        msg = (\'`nvcc` command returns non-zero exit status. \\n\'\n               \'command: {0}\\n\'\n               \'return-code: {1}\\n\'\n               \'stdout/stderr: \\n\'\n               \'{2}\'.format(e.cmd,\n                            e.returncode,\n                            e.output.decode(encoding=\'UTF-8\',\n                                            errors=\'replace\')))\n        raise NVCCException(msg)\n    except OSError as e:\n        msg = \'Failed to run `nvcc` command. \' \\\n              \'Check PATH environment variable: \' \\\n              + str(e)\n        raise OSError(msg)\n\n\ndef _get_nvrtc_version():\n    global _nvrtc_version\n    if _nvrtc_version is None:\n        _nvrtc_version = nvrtc.getVersion()\n\n    return _nvrtc_version\n\n\n# Known archs for Tegra/Jetson/Xavier/etc\n_tegra_archs = (\'53\', \'62\', \'72\')\n\n\n@util.memoize(for_each_device=True)\ndef _get_arch():\n    # See Supported Compile Options section of NVRTC User Guide for\n    # the maximum value allowed for `--gpu-architecture`.\n    major, minor = _get_nvrtc_version()\n    if major < 9:\n        # CUDA 8.0\n        _nvrtc_max_compute_capability = \'52\'\n    elif major < 10 or (major == 10 and minor == 0):\n        # CUDA 9.x / 10.0\n        _nvrtc_max_compute_capability = \'70\'\n    else:\n        # CUDA 10.1 / 10.2\n        _nvrtc_max_compute_capability = \'75\'\n\n    arch = device.Device().compute_capability\n    if arch in _tegra_archs:\n        return arch\n    else:\n        return min(arch, _nvrtc_max_compute_capability)\n\n\ndef _is_cudadevrt_needed(options):\n    return any(o for o in options if o in _rdc_flags)\n\n\ndef _get_cudadevrt_path():\n    global _cudadevrt\n    if _cudadevrt is not None:\n        return _cudadevrt\n\n    # defer import to here to avoid circular dependency\n    from cupy.cuda import get_cuda_path\n    global _win32\n\n    cudadevrt = get_cuda_path()\n    if cudadevrt is None:\n        raise RuntimeError(\'CUDA is not found.\')\n\n    if _win32:\n        # rely on os.altsep\n        cudadevrt += \'/lib/x64/cudadevrt.lib\'\n    else:  # linux & osx: search twice as in cupy/install/build.py\n        cudadevrt64 = cudadevrt + \'/lib64/libcudadevrt.a\'\n        if not os.path.isfile(cudadevrt64):\n            cudadevrt += \'/lib/libcudadevrt.a\'\n        else:\n            cudadevrt = cudadevrt64\n    if not os.path.isfile(cudadevrt):\n        raise RuntimeError(\n            \'Relocatable PTX code is requested, but cudadevrt \'\n            \'is not found.\')\n    return cudadevrt\n\n\ndef _remove_rdc_option(options):\n    return tuple(o for o in options if o not in _rdc_flags)\n\n\ndef _get_bool_env_variable(name, default):\n    val = os.environ.get(name)\n    if val is None or len(val) == 0:\n        return default\n    try:\n        return int(val) == 1\n    except ValueError:\n        return False\n\n\ndef compile_using_nvrtc(source, options=(), arch=None, filename=\'kern.cu\',\n                        name_expressions=None):\n    if not arch:\n        arch = _get_arch()\n\n    options += (\'-arch=compute_{}\'.format(arch),)\n\n    with tempfile.TemporaryDirectory() as root_dir:\n        cu_path = os.path.join(root_dir, filename)\n\n        with open(cu_path, \'w\') as cu_file:\n            cu_file.write(source)\n\n        prog = _NVRTCProgram(source, cu_path,\n                             name_expressions=name_expressions)\n        try:\n            ptx, mapping = prog.compile(options)\n        except CompileException as e:\n            dump = _get_bool_env_variable(\n                \'CUPY_DUMP_CUDA_SOURCE_ON_ERROR\', False)\n            if dump:\n                e.dump(sys.stderr)\n            raise\n\n        return ptx, mapping\n\n\ndef compile_using_nvcc(source, options=(), arch=None,\n                       filename=\'kern.cu\', code_type=\'cubin\',\n                       separate_compilation=False):\n    # defer import to here to avoid circular dependency\n    from cupy.cuda import get_nvcc_path\n\n    if not arch:\n        arch = _get_arch()\n\n    if code_type not in (\'cubin\', \'ptx\'):\n        raise ValueError(\'Invalid code_type %s. Should be cubin or ptx\')\n    if code_type == \'ptx\':\n        assert not separate_compilation\n\n    arch_str = \'-gencode=arch=compute_{cc},code=sm_{cc}\'.format(cc=arch)\n    _nvcc = get_nvcc_path()\n    # split() is needed because _nvcc could come from the env var NVCC\n    cmd = _nvcc.split()\n    cmd.append(arch_str)\n\n    with tempfile.TemporaryDirectory() as root_dir:\n        first_part = filename.split(\'.\')[0]\n\n        path = os.path.join(root_dir, first_part)\n        cu_path = \'%s.cu\' % path\n        result_path = \'%s.%s\' % (path, code_type)\n\n        with open(cu_path, \'w\') as cu_file:\n            cu_file.write(source)\n\n        if not separate_compilation:  # majority cases\n            cmd.append(\'--%s\' % code_type)\n            cmd += list(options)\n            cmd.append(cu_path)\n\n            try:\n                _run_nvcc(cmd, root_dir)\n            except NVCCException as e:\n                cex = CompileException(str(e), source, cu_path, options,\n                                       \'nvcc\')\n\n                dump = _get_bool_env_variable(\n                    \'CUPY_DUMP_CUDA_SOURCE_ON_ERROR\', False)\n                if dump:\n                    cex.dump(sys.stderr)\n\n                raise cex\n        else:  # two steps: compile to object and device-link\n            cmd_partial = cmd.copy()\n            cmd_partial.append(\'--cubin\')\n\n            obj = path + \'.o\'\n            cmd += list(options + (\'-o\', obj))\n            cmd.append(cu_path)\n\n            try:\n                _run_nvcc(cmd, root_dir)\n            except NVCCException as e:\n                cex = CompileException(str(e), source, cu_path, options,\n                                       \'nvcc\')\n\n                dump = _get_bool_env_variable(\n                    \'CUPY_DUMP_CUDA_SOURCE_ON_ERROR\', False)\n                if dump:\n                    cex.dump(sys.stderr)\n\n                raise cex\n\n            options = _remove_rdc_option(options)\n            options += (\'--device-link\', obj, \'-o\', path + \'.cubin\')\n            cmd = cmd_partial + list(options)\n\n            try:\n                _run_nvcc(cmd, root_dir)\n            except NVCCException as e:\n                cex = CompileException(str(e), \'\', \'\', options, \'nvcc\')\n                raise cex\n\n        if code_type == \'ptx\':\n            with open(result_path, \'rb\') as ptx_file:\n                return ptx_file.read().decode(\'utf-8\')\n        elif code_type == \'cubin\':\n            with open(result_path, \'rb\') as bin_file:\n                return bin_file.read()\n        else:\n            assert False, code_type\n\n\ndef _preprocess(source, options, arch, backend):\n    if backend == \'nvrtc\':\n        options += (\'-arch=compute_{}\'.format(arch),)\n\n        prog = _NVRTCProgram(source, \'\')\n        try:\n            result, _ = prog.compile(options)\n        except CompileException as e:\n            dump = _get_bool_env_variable(\n                \'CUPY_DUMP_CUDA_SOURCE_ON_ERROR\', False)\n            if dump:\n                e.dump(sys.stderr)\n            raise\n    elif backend == \'nvcc\':\n        try:\n            result = compile_using_nvcc(source, options, arch, \'preprocess.cu\',\n                                        code_type=\'ptx\')\n        except CompileException as e:\n            dump = _get_bool_env_variable(\n                \'CUPY_DUMP_CUDA_SOURCE_ON_ERROR\', False)\n            if dump:\n                e.dump(sys.stderr)\n            raise\n    else:\n        raise ValueError(\'Invalid backend %s\' % backend)\n\n    assert isinstance(result, str)\n    return result\n\n\n_default_cache_dir = os.path.expanduser(\'~/.cupy/kernel_cache\')\n\n\ndef get_cache_dir():\n    return os.environ.get(\'CUPY_CACHE_DIR\', _default_cache_dir)\n\n\n_empty_file_preprocess_cache = {}\n\n\ndef compile_with_cache(\n        source, options=(), arch=None, cache_dir=None, extra_source=None,\n        backend=\'nvrtc\', *, enable_cooperative_groups=False,\n        name_expressions=None):\n\n    if enable_cooperative_groups:\n        if backend != \'nvcc\':\n            raise ValueError(\n                \'Cooperative groups is supported only in NVCC backend.\')\n        if runtime.is_hip:\n            raise ValueError(\n                \'Cooperative groups is not supported in HIP.\')\n\n    if name_expressions is not None:\n        if runtime.is_hip or backend != \'nvrtc\':\n            raise NotImplementedError\n\n    if runtime.is_hip:\n        return _compile_with_cache_hipcc(\n            source, options, arch, cache_dir, extra_source)\n    else:\n        return _compile_with_cache_cuda(\n            source, options, arch, cache_dir, extra_source, backend,\n            enable_cooperative_groups, name_expressions)\n\n\ndef _compile_with_cache_cuda(\n        source, options, arch, cache_dir, extra_source=None, backend=\'nvrtc\',\n        enable_cooperative_groups=False, name_expressions=None):\n    # NVRTC does not use extra_source. extra_source is used for cache key.\n    global _empty_file_preprocess_cache\n    if cache_dir is None:\n        cache_dir = get_cache_dir()\n    if arch is None:\n        arch = _get_arch()\n\n    options += (\'-ftz=true\',)\n\n    if enable_cooperative_groups:\n        # `cooperative_groups` requires `-rdc=true`.\n        # The three latter flags are to resolve linker error.\n        # (https://devtalk.nvidia.com/default/topic/1023604/linker-error/)\n        options += (\'-rdc=true\', \'-Xcompiler\', \'-fPIC\', \'-shared\')\n\n    if _get_bool_env_variable(\'CUPY_CUDA_COMPILE_WITH_DEBUG\', False):\n        options += (\'--device-debug\', \'--generate-line-info\')\n\n    env = (arch, options, _get_nvrtc_version(), backend)\n    base = _empty_file_preprocess_cache.get(env, None)\n    if base is None:\n        # This is checking of NVRTC compiler internal version\n        base = _preprocess(\'\', options, arch, backend)\n        _empty_file_preprocess_cache[env] = base\n\n    key_src = \'%s %s %s %s\' % (env, base, source, extra_source)\n\n    key_src = key_src.encode(\'utf-8\')\n    name = \'%s_2.cubin\' % hashlib.md5(key_src).hexdigest()\n\n    if not os.path.isdir(cache_dir):\n        try:\n            os.makedirs(cache_dir)\n        except OSError:\n            if not os.path.isdir(cache_dir):\n                raise\n\n    mod = function.Module()\n    # To handle conflicts in concurrent situation, we adopt lock-free method\n    # to avoid performance degradation.\n    # We force recompiling to retrieve C++ mangled names if so desired.\n    path = os.path.join(cache_dir, name)\n    if os.path.exists(path) and not name_expressions:\n        with open(path, \'rb\') as file:\n            data = file.read()\n        if len(data) >= 32:\n            hash = data[:32]\n            cubin = data[32:]\n            cubin_hash = hashlib.md5(cubin).hexdigest().encode(\'ascii\')\n            if hash == cubin_hash:\n                mod.load(cubin)\n                return mod\n\n    if backend == \'nvrtc\':\n        ptx, mapping = compile_using_nvrtc(\n            source, options, arch, name + \'.cu\', name_expressions)\n        ls = function.LinkState()\n        ls.add_ptr_data(ptx, \'cupy.ptx\')\n        # for separate compilation\n        if _is_cudadevrt_needed(options):\n            _cudadevrt = _get_cudadevrt_path()\n            ls.add_ptr_file(_cudadevrt)\n        cubin = ls.complete()\n        mod._set_mapping(mapping)\n    elif backend == \'nvcc\':\n        rdc = _is_cudadevrt_needed(options)\n        cubin = compile_using_nvcc(source, options, arch, name + \'.cu\',\n                                   code_type=\'cubin\', separate_compilation=rdc)\n    else:\n        raise ValueError(\'Invalid backend %s\' % backend)\n\n    cubin_hash = hashlib.md5(cubin).hexdigest().encode(\'ascii\')\n\n    # shutil.move is not atomic operation, so it could result in a corrupted\n    # file. We detect it by appending md5 hash at the beginning of each cache\n    # file. If the file is corrupted, it will be ignored next time it is read.\n    with tempfile.NamedTemporaryFile(dir=cache_dir, delete=False) as tf:\n        tf.write(cubin_hash)\n        tf.write(cubin)\n        temp_path = tf.name\n    shutil.move(temp_path, path)\n\n    # Save .cu source file along with .cubin\n    if _get_bool_env_variable(\'CUPY_CACHE_SAVE_CUDA_SOURCE\', False):\n        with open(path + \'.cu\', \'w\') as f:\n            f.write(source)\n\n    mod.load(cubin)\n    return mod\n\n\nclass CompileException(Exception):\n\n    def __init__(self, msg, source, name, options, backend=\'nvrtc\'):\n        self._msg = msg\n        self.source = source\n        self.name = name\n        self.options = options\n        self.backend = backend\n        super(CompileException, self).__init__()\n\n    def __reduce__(self):\n        return (type(self), (self._msg, self.source, self.name,\n                             self.options, self.backend))\n\n    def __repr__(self):\n        return str(self)\n\n    def __str__(self):\n        return self.get_message()\n\n    def get_message(self):\n        return self._msg\n\n    def dump(self, f):\n        lines = self.source.split(\'\\n\')\n        digits = int(math.floor(math.log10(len(lines)))) + 1\n        linum_fmt = \'{{:0{}d}} \'.format(digits)\n        f.write(\'{} \'.format(self.backend.upper()))\n        f.write(\'compilation error: {}\\n\'.format(self))\n        f.write(\'-----\\n\')\n        f.write(\'Name: {}\\n\'.format(self.name))\n        f.write(\'Options: {}\\n\'.format(\' \'.join(self.options)))\n        f.write(\'CUDA source:\\n\')\n        for i, line in enumerate(lines):\n            f.write(linum_fmt.format(i + 1) + line.rstrip() + \'\\n\')\n        f.write(\'-----\\n\')\n        f.flush()\n\n\nclass _NVRTCProgram(object):\n\n    def __init__(self, src, name=\'default_program\', headers=(),\n                 include_names=(), name_expressions=None):\n        self.ptr = None\n\n        if isinstance(src, bytes):\n            src = src.decode(\'UTF-8\')\n        if isinstance(name, bytes):\n            name = name.decode(\'UTF-8\')\n\n        self.src = src\n        self.name = name\n        self.ptr = nvrtc.createProgram(src, name, headers, include_names)\n        self.name_expressions = name_expressions\n\n    def __del__(self, is_shutting_down=util.is_shutting_down):\n        if is_shutting_down():\n            return\n        if self.ptr:\n            nvrtc.destroyProgram(self.ptr)\n\n    def compile(self, options=()):\n        try:\n            if self.name_expressions:\n                for ker in self.name_expressions:\n                    nvrtc.addAddNameExpression(self.ptr, ker)\n            nvrtc.compileProgram(self.ptr, options)\n            mapping = None\n            if self.name_expressions:\n                mapping = {}\n                for ker in self.name_expressions:\n                    mapping[ker] = nvrtc.getLoweredName(self.ptr, ker)\n            return nvrtc.getPTX(self.ptr), mapping\n        except nvrtc.NVRTCError:\n            log = nvrtc.getProgramLog(self.ptr)\n            raise CompileException(log, self.src, self.name, options, \'nvrtc\')\n\n\ndef is_valid_kernel_name(name):\n    return re.match(\'^[a-zA-Z_][a-zA-Z_0-9]*$\', name) is not None\n\n\n_hipcc_version = None\n\n\ndef _get_hipcc_version():\n    global _hipcc_version\n    if _hipcc_version is None:\n        cmd = [\'hipcc\', \'--version\']\n        _hipcc_version = _run_hipcc(cmd)\n    return _hipcc_version\n\n\ndef _run_hipcc(cmd, cwd=\'.\', env=None):\n    try:\n        return subprocess.check_output(cmd, stderr=subprocess.STDOUT, cwd=cwd,\n                                       env=env)\n    except subprocess.CalledProcessError as e:\n        # TODO(leofang): raise an ""HIPCCException""?\n        raise RuntimeError(\n            \'`hipcc` command returns non-zero exit status. \\n\'\n            \'command: {0}\\n\'\n            \'return-code: {1}\\n\'\n            \'stdout/stderr: \\n\'\n            \'{2}\'.format(e.cmd, e.returncode, e.output.decode(\'utf-8\')))\n    except OSError as e:\n        raise OSError(\'Failed to run `hipcc` command. \'\n                      \'Check PATH environment variable: \'\n                      + str(e))\n\n\ndef _hipcc(source, options, arch):\n    cmd = [\'hipcc\', \'--genco\', \'--targets=\' + arch,\n           \'--flags=""%s""\' % \' \'.join(options)]\n\n    with tempfile.TemporaryDirectory() as root_dir:\n        path = os.path.join(root_dir, \'kern\')\n        in_path = path + \'.cpp\'\n        out_path = path + \'.hsaco\'\n\n        with open(in_path, \'w\') as f:\n            f.write(source)\n\n        cmd += [in_path, \'-o\', out_path]\n\n        env = os.environ.copy()\n\n        output = _run_hipcc(cmd, root_dir, env)\n        if not os.path.isfile(out_path):\n            raise RuntimeError(\n                \'`hipcc` command does not generate output file. \\n\'\n                \'command: {0}\\n\'\n                \'stdout/stderr: \\n\'\n                \'{1}\'.format(cmd, output))\n        with open(out_path, \'rb\') as f:\n            return f.read()\n\n\ndef _preprocess_hipcc(source, options):\n    cmd = [\'hipcc\', \'--preprocess\'] + list(options)\n    with tempfile.TemporaryDirectory() as root_dir:\n        path = os.path.join(root_dir, \'kern\')\n        cu_path = \'%s.cpp\' % path\n\n        with open(cu_path, \'w\') as cu_file:\n            cu_file.write(source)\n\n        cmd.append(cu_path)\n        pp_src = _run_hipcc(cmd, root_dir)\n        assert isinstance(pp_src, bytes)\n        return re.sub(b\'(?m)^#.*$\', b\'\', pp_src)\n\n\ndef _convert_to_hip_source(source):\n    table = [\n        (\'threadIdx.\', \'hipThreadIdx_\'),\n        (\'blockIdx.\', \'hipBlockIdx_\'),\n        (\'blockDim.\', \'hipBlockDim_\'),\n        (\'gridDim.\', \'hipGridDim_\'),\n    ]\n    for i, j in table:\n        source = source.replace(i, j)\n\n    return ""#include <hip/hip_runtime.h>\\n"" + source\n\n\ndef _compile_with_cache_hipcc(source, options, arch, cache_dir, extra_source,\n                              use_converter=True):\n    global _empty_file_preprocess_cache\n    if cache_dir is None:\n        cache_dir = get_cache_dir()\n    if arch is None:\n        arch = os.environ.get(\'HCC_AMDGPU_TARGET\')\n        if arch is None:\n            raise RuntimeError(\'HCC_AMDGPU_TARGET is not set\')\n    if use_converter:\n        source = _convert_to_hip_source(source)\n\n    env = (arch, options, _get_hipcc_version())\n    base = _empty_file_preprocess_cache.get(env, None)\n    if base is None:\n        # This is checking of HIPCC compiler internal version\n        base = _preprocess_hipcc(\'\', options)\n        _empty_file_preprocess_cache[env] = base\n    key_src = \'%s %s %s %s\' % (env, base, source, extra_source)\n\n    key_src = key_src.encode(\'utf-8\')\n    name = \'%s.hsaco\' % hashlib.md5(key_src).hexdigest()\n\n    if not os.path.isdir(cache_dir):\n        try:\n            os.makedirs(cache_dir)\n        except OSError:\n            if not os.path.isdir(cache_dir):\n                raise\n\n    mod = function.Module()\n    # To handle conflicts in concurrent situation, we adopt lock-free method\n    # to avoid performance degradation.\n    path = os.path.join(cache_dir, name)\n    if os.path.exists(path):\n        with open(path, \'rb\') as file:\n            data = file.read()\n        if len(data) >= 32:\n            hash_value = data[:32]\n            binary = data[32:]\n            binary_hash = hashlib.md5(binary).hexdigest().encode(\'ascii\')\n            if hash_value == binary_hash:\n                mod.load(binary)\n                return mod\n\n    # TODO(leofang): catch HIPCCException and convert it to CompileException\n    # with backend=\'hipcc\'\n    binary = _hipcc(source, options, arch)\n    binary_hash = hashlib.md5(binary).hexdigest().encode(\'ascii\')\n\n    # shutil.move is not atomic operation, so it could result in a corrupted\n    # file. We detect it by appending md5 hash at the beginning of each cache\n    # file. If the file is corrupted, it will be ignored next time it is read.\n    with tempfile.NamedTemporaryFile(dir=cache_dir, delete=False) as tf:\n        tf.write(binary_hash)\n        tf.write(binary)\n        temp_path = tf.name\n    shutil.move(temp_path, path)\n\n    # Save .cu source file along with .hsaco\n    if _get_bool_env_variable(\'CUPY_CACHE_SAVE_CUDA_SOURCE\', False):\n        with open(path + \'.cu\', \'w\') as f:\n            f.write(source)\n\n    mod.load(binary)\n    return mod\n'"
cupy/fft/__init__.py,0,b'from cupy.fft.fft import fft  # NOQA\nfrom cupy.fft.fft import fft2  # NOQA\nfrom cupy.fft.fft import fftfreq  # NOQA\nfrom cupy.fft.fft import fftn  # NOQA\nfrom cupy.fft.fft import fftshift  # NOQA\nfrom cupy.fft.fft import hfft  # NOQA\nfrom cupy.fft.fft import ifft  # NOQA\nfrom cupy.fft.fft import ifft2  # NOQA\nfrom cupy.fft.fft import ifftn  # NOQA\nfrom cupy.fft.fft import ifftshift  # NOQA\nfrom cupy.fft.fft import ihfft  # NOQA\nfrom cupy.fft.fft import irfft  # NOQA\nfrom cupy.fft.fft import irfft2  # NOQA\nfrom cupy.fft.fft import irfftn  # NOQA\nfrom cupy.fft.fft import rfft  # NOQA\nfrom cupy.fft.fft import rfft2  # NOQA\nfrom cupy.fft.fft import rfftfreq  # NOQA\nfrom cupy.fft.fft import rfftn  # NOQA\nfrom cupy.fft import config  # NOQA\n'
cupy/fft/config.py,0,"b'from cupy import util\n\n\nenable_nd_planning = True\nuse_multi_gpus = False\n_devices = None\n\n\ndef set_cufft_gpus(gpus):\n    \'\'\'Set the GPUs to be used in multi-GPU FFT.\n\n    Args:\n        gpus (int or list of int): The number of GPUs or a list of GPUs\n            to be used. For the former case, the first ``gpus`` GPUs\n            will be used.\n\n    .. warning::\n        This API is currently experimental and may be changed in the future\n        version.\n\n    .. seealso:: `Multiple GPU cuFFT Transforms`_\n\n    .. _Multiple GPU cuFFT Transforms:\n        https://docs.nvidia.com/cuda/cufft/index.html#multiple-GPU-cufft-transforms\n    \'\'\'\n    util.experimental(\'cupy.fft.config.set_cufft_gpus\')\n    global _devices\n\n    if isinstance(gpus, int):\n        devs = [i for i in range(gpus)]\n    elif isinstance(gpus, list):\n        devs = gpus\n    else:\n        raise ValueError(""gpus must be an int or a list of int."")\n    if len(devs) <= 1:\n        raise ValueError(""Must use at least 2 GPUs."")\n\n    _devices = devs\n'"
cupy/fft/fft.py,21,"b'import functools\nimport math\nimport warnings\n\nimport numpy as np\n\nimport cupy\nfrom cupy.cuda import cufft\nfrom cupy.fft import config\n\n\n_reduce = functools.reduce\n_prod = cupy.core.internal.prod\n\n\n@cupy.util.memoize()\ndef _output_dtype(dtype, value_type):\n    if value_type != \'R2C\':\n        if dtype in [np.float16, np.float32]:\n            return np.complex64\n        elif dtype not in [np.complex64, np.complex128]:\n            return np.complex128\n    else:\n        if dtype in [np.complex64, np.complex128]:\n            return np.dtype(dtype.char.lower())\n        elif dtype == np.float16:\n            return np.float32\n        elif dtype not in [np.float32, np.float64]:\n            return np.float64\n    return dtype\n\n\ndef _convert_dtype(a, value_type):\n    out_dtype = _output_dtype(a.dtype, value_type)\n    if out_dtype != a.dtype:\n        a = a.astype(out_dtype)\n    return a\n\n\ndef _cook_shape(a, s, axes, value_type, order=\'C\'):\n    if s is None or s == a.shape:\n        return a\n    if (value_type == \'C2R\') and (s[-1] is not None):\n        s = list(s)\n        s[-1] = s[-1] // 2 + 1\n    for sz, axis in zip(s, axes):\n        if (sz is not None) and (sz != a.shape[axis]):\n            shape = list(a.shape)\n            if shape[axis] > sz:\n                index = [slice(None)] * a.ndim\n                index[axis] = slice(0, sz)\n                a = a[index]\n            else:\n                index = [slice(None)] * a.ndim\n                index[axis] = slice(0, shape[axis])\n                shape[axis] = sz\n                z = cupy.zeros(shape, a.dtype.char, order=order)\n                z[index] = a\n                a = z\n    return a\n\n\ndef _convert_fft_type(dtype, value_type):\n    if value_type == \'C2C\' and dtype == np.complex64:\n        return cufft.CUFFT_C2C\n    elif value_type == \'R2C\' and dtype == np.float32:\n        return cufft.CUFFT_R2C\n    elif value_type == \'C2R\' and dtype == np.complex64:\n        return cufft.CUFFT_C2R\n    elif value_type == \'C2C\' and dtype == np.complex128:\n        return cufft.CUFFT_Z2Z\n    elif value_type == \'R2C\' and dtype == np.float64:\n        return cufft.CUFFT_D2Z\n    elif value_type == \'C2R\' and dtype == np.complex128:\n        return cufft.CUFFT_Z2D\n    else:\n        raise ValueError\n\n\ndef _exec_fft(a, direction, value_type, norm, axis, overwrite_x,\n              out_size=None, out=None, plan=None):\n    fft_type = _convert_fft_type(a.dtype, value_type)\n\n    if axis % a.ndim != a.ndim - 1:\n        a = a.swapaxes(axis, -1)\n\n    if a.base is not None or not a.flags.c_contiguous:\n        a = a.copy()\n\n    if out_size is None:\n        out_size = a.shape[-1]\n\n    batch = a.size // a.shape[-1]\n    curr_plan = cufft.get_current_plan()\n    if curr_plan is not None:\n        if plan is None:\n            plan = curr_plan\n        else:\n            raise RuntimeError(\'Use the cuFFT plan either as a context manager\'\n                               \' or as an argument.\')\n    if plan is None:\n        devices = None if not config.use_multi_gpus else config._devices\n        plan = cufft.Plan1d(out_size, fft_type, batch, devices=devices)\n    else:\n        # check plan validity\n        if not isinstance(plan, cufft.Plan1d):\n            raise ValueError(\'expected plan to have type cufft.Plan1d\')\n        if fft_type != plan.fft_type:\n            raise ValueError(\'cuFFT plan dtype mismatch.\')\n        if out_size != plan.nx:\n            raise ValueError(\'Target array size does not match the plan.\',\n                             out_size, plan.nx)\n        if batch != plan.batch:\n            raise ValueError(\'Batch size does not match the plan.\')\n        if config.use_multi_gpus != plan._use_multi_gpus:\n            raise ValueError(\'Unclear if multiple GPUs are to be used or not.\')\n\n    if overwrite_x and value_type == \'C2C\':\n        out = a\n    elif out is not None:\n        # verify that out has the expected shape and dtype\n        plan.check_output_array(a, out)\n    else:\n        out = plan.get_output_array(a)\n\n    plan.fft(a, out, direction)\n\n    sz = out.shape[-1]\n    if fft_type == cufft.CUFFT_R2C or fft_type == cufft.CUFFT_D2Z:\n        sz = a.shape[-1]\n    if norm is None:\n        if direction == cufft.CUFFT_INVERSE:\n            out /= sz\n    else:\n        out /= math.sqrt(sz)\n\n    if axis % a.ndim != a.ndim - 1:\n        out = out.swapaxes(axis, -1)\n\n    return out\n\n\ndef _fft_c2c(a, direction, norm, axes, overwrite_x, plan=None):\n    for axis in axes:\n        a = _exec_fft(a, direction, \'C2C\', norm, axis, overwrite_x, plan=plan)\n    return a\n\n\ndef _fft(a, s, axes, norm, direction, value_type=\'C2C\', overwrite_x=False,\n         plan=None):\n    if norm not in (None, \'ortho\'):\n        raise ValueError(\'Invalid norm value %s, should be None or ""ortho"".\'\n                         % norm)\n\n    if s is not None:\n        for n in s:\n            if (n is not None) and (n < 1):\n                raise ValueError(\n                    \'Invalid number of FFT data points (%d) specified.\' % n)\n\n    if (s is not None) and (axes is not None) and len(s) != len(axes):\n        raise ValueError(\'Shape and axes have different lengths.\')\n\n    if axes is None:\n        if s is None:\n            dim = a.ndim\n        else:\n            dim = len(s)\n        axes = [i for i in range(-dim, 0)]\n    else:\n        axes = tuple(axes)\n    if not axes:\n        if value_type == \'C2C\':\n            return a\n        else:\n            raise IndexError(\'list index out of range\')\n    a = _convert_dtype(a, value_type)\n    a = _cook_shape(a, s, axes, value_type)\n\n    if value_type == \'C2C\':\n        a = _fft_c2c(a, direction, norm, axes, overwrite_x, plan=plan)\n    elif value_type == \'R2C\':\n        a = _exec_fft(a, direction, value_type, norm, axes[-1], overwrite_x)\n        a = _fft_c2c(a, direction, norm, axes[:-1], overwrite_x)\n    else:  # C2R\n        a = _fft_c2c(a, direction, norm, axes[:-1], overwrite_x)\n        # _cook_shape tells us input shape only, and no output shape\n        out_size = _get_fftn_out_size(a.shape, s, axes[-1], value_type)\n        a = _exec_fft(a, direction, value_type, norm, axes[-1], overwrite_x,\n                      out_size)\n\n    return a\n\n\ndef _prep_fftn_axes(ndim, s=None, axes=None, value_type=\'C2C\'):\n    """"""Configure axes argument for an n-dimensional FFT.\n\n    The axes to be transformed are returned in ascending order.\n    """"""\n\n    # compatibility checks for cupy.cuda.cufft.PlanNd\n    if (s is not None) and (axes is not None) and len(s) != len(axes):\n        raise ValueError(""Shape and axes have different lengths."")\n\n    if axes is None:\n        if s is None:\n            dim = ndim\n        else:\n            dim = len(s)\n        axes = tuple([i + ndim for i in range(-dim, 0)])\n        axes_sorted = axes\n    else:\n        axes = tuple(axes)\n        if not axes:\n            return (), ()\n        if _reduce(min, axes) < -ndim or _reduce(max, axes) > ndim - 1:\n            raise ValueError(""The specified axes exceed the array dimensions."")\n        if value_type == \'C2C\':\n            axes_sorted = tuple(sorted([ax % ndim for ax in axes]))\n        else:  # C2R or R2C\n            # The last axis is special, need to isolate it and append\n            # to the rest of (sorted) axes\n            axes_sorted = sorted([ax % ndim for ax in axes[:-1]])\n            axes_sorted.append(axes[-1] % ndim)\n            axes_sorted = tuple(axes_sorted)\n\n    # unsorted axes for _cook_shape, sorted ones are otherwise used\n    return axes, axes_sorted\n\n\ndef _nd_plan_is_possible(axes_sorted, ndim):\n    # PlanNd supports 1D, 2D and 3D batch transforms over contiguous axes\n    # Axes must be contiguous and the first or last axis must be in the axes.\n    return (0 < len(axes_sorted) <= 3\n            and (0 in axes_sorted or (ndim - 1) in axes_sorted)\n            and all([\n                (axes_sorted[n + 1] - axes_sorted[n]) == 1\n                for n in range(len(axes_sorted) - 1)]))\n\n\ndef _get_cufft_plan_nd(shape, fft_type, axes=None, order=\'C\', out_size=None):\n    """"""Generate a CUDA FFT plan for transforming up to three axes.\n\n    Args:\n        shape (tuple of int): The shape of the array to transform\n        fft_type (int): The FFT type to perform. Supported values are:\n            `cufft.CUFFT_C2C`, `cufft.CUFFT_C2R`, `cufft.CUFFT_R2C`,\n            `cufft.CUFFT_Z2Z`, `cufft.CUFFT_Z2D`, and `cufft.CUFFT_D2Z`.\n        axes (None or int or tuple of int):  The axes of the array to\n            transform. Currently, these must be a set of up to three adjacent\n            axes and must include either the first or the last axis of the\n            array.  If `None`, it is assumed that all axes are transformed.\n        order ({\'C\', \'F\'}): Specify whether the data to be transformed has C or\n            Fortran ordered data layout.\n        out_size (int): The output length along the last axis for R2C/C2R FFTs.\n            For C2C FFT, this is ignored (and set to `None`).\n\n    Returns:\n        plan (cufft.PlanNd): A cuFFT Plan for the chosen `fft_type`.\n    """"""\n    ndim = len(shape)\n\n    if fft_type in (cufft.CUFFT_C2C, cufft.CUFFT_Z2Z):\n        value_type = \'C2C\'\n    elif fft_type in (cufft.CUFFT_C2R, cufft.CUFFT_Z2D):\n        value_type = \'C2R\'\n    else:  # CUFFT_R2C or CUFFT_D2Z\n        value_type = \'R2C\'\n\n    if axes is None:\n        # transform over all axes\n        fft_axes = tuple(range(ndim))\n    else:\n        _, fft_axes = _prep_fftn_axes(ndim, s=None, axes=axes,\n                                      value_type=value_type)\n\n    if not _nd_plan_is_possible(fft_axes, ndim):\n        raise ValueError(\n            ""An n-dimensional cuFFT plan could not be created. The axes must ""\n            ""be contiguous and non-repeating. Between one and three axes can ""\n            ""be transformed and either the first or last axis must be ""\n            ""included in axes."")\n\n    if order not in [\'C\', \'F\']:\n        raise ValueError(\'order must be \\\'C\\\' or \\\'F\\\'\')\n\n    """"""\n    For full details on idist, istride, iembed, etc. see:\n    http://docs.nvidia.com/cuda/cufft/index.html#advanced-data-layout\n\n    in 1D:\n    input[b * idist + x * istride]\n    output[b * odist + x * ostride]\n\n    in 2D:\n    input[b * idist + (x * inembed[1] + y) * istride]\n    output[b * odist + (x * onembed[1] + y) * ostride]\n\n    in 3D:\n    input[b * idist + ((x * inembed[1] + y) * inembed[2] + z) * istride]\n    output[b * odist + ((x * onembed[1] + y) * onembed[2] + z) * ostride]\n    """"""\n    # At this point, _default_fft_func() guarantees that for F-order arrays\n    # we only need to consider C2C, and not C2R or R2C.\n    # TODO(leofang): figure out if we really have to skip F-order?\n    in_dimensions = [shape[d] for d in fft_axes]\n    if order == \'F\':\n        in_dimensions = in_dimensions[::-1]\n    in_dimensions = tuple(in_dimensions)\n    if fft_type in (cufft.CUFFT_C2C, cufft.CUFFT_Z2Z):\n        out_dimensions = in_dimensions\n        plan_dimensions = in_dimensions\n    else:\n        out_dimensions = list(in_dimensions)\n        if out_size is not None:  # for C2R & R2C\n            out_dimensions[-1] = out_size  # only valid for C order!\n        out_dimensions = tuple(out_dimensions)\n        if fft_type in (cufft.CUFFT_R2C, cufft.CUFFT_D2Z):\n            plan_dimensions = in_dimensions\n        else:  # CUFFT_C2R or CUFFT_Z2D\n            plan_dimensions = out_dimensions\n    inembed = in_dimensions\n    onembed = out_dimensions\n\n    if fft_axes == tuple(range(ndim)):\n        # tranfsorm over all axes\n        nbatch = 1\n        idist = odist = 1  # doesn\'t matter since nbatch = 1\n        istride = ostride = 1\n    else:\n        # batch along the first or the last axis\n        if 0 not in fft_axes:\n            # don\'t FFT along the first min_axis_fft axes\n            min_axis_fft = _reduce(min, fft_axes)\n            nbatch = _prod(shape[:min_axis_fft])\n            if order == \'C\':\n                # C-ordered GPU array with batch along first dim\n                idist = _prod(in_dimensions)\n                odist = _prod(out_dimensions)\n                istride = 1\n                ostride = 1\n            elif order == \'F\':\n                # F-ordered GPU array with batch along first dim\n                idist = 1\n                odist = 1\n                istride = nbatch\n                ostride = nbatch\n        elif (ndim - 1) not in fft_axes:\n            # don\'t FFT along the last axis\n            num_axes_batch = ndim - len(fft_axes)\n            nbatch = _prod(shape[-num_axes_batch:])\n            if order == \'C\':\n                # C-ordered GPU array with batch along last dim\n                idist = 1\n                odist = 1\n                istride = nbatch\n                ostride = nbatch\n            elif order == \'F\':\n                # F-ordered GPU array with batch along last dim\n                idist = _prod(in_dimensions)\n                odist = _prod(out_dimensions)\n                istride = 1\n                ostride = 1\n        else:\n            raise ValueError(\n                \'General subsets of FFT axes not currently supported for \'\n                \'GPU case (Can only batch FFT over the first or last \'\n                \'spatial axes).\')\n\n    plan = cufft.PlanNd(shape=plan_dimensions,\n                        inembed=inembed,\n                        istride=istride,\n                        idist=idist,\n                        onembed=onembed,\n                        ostride=ostride,\n                        odist=odist,\n                        fft_type=fft_type,\n                        batch=nbatch,\n                        order=order,\n                        last_axis=fft_axes[-1],\n                        last_size=out_size)\n    return plan\n\n\ndef _get_fftn_out_size(in_shape, s, last_axis, value_type):\n    if value_type == \'C2R\':\n        if (s is None) or (s[-1] is None):\n            out_size = 2 * (in_shape[last_axis] - 1)\n        else:\n            out_size = s[-1]\n    elif value_type == \'R2C\':\n        out_size = in_shape[last_axis] // 2 + 1\n    else:  # C2C\n        out_size = None\n    return out_size\n\n\ndef _exec_fftn(a, direction, value_type, norm, axes, overwrite_x,\n               plan=None, out=None, out_size=None):\n\n    fft_type = _convert_fft_type(a.dtype, value_type)\n\n    if a.flags.c_contiguous:\n        order = \'C\'\n    elif a.flags.f_contiguous:\n        order = \'F\'\n    else:\n        raise ValueError(\'a must be contiguous\')\n\n    curr_plan = cufft.get_current_plan()\n    if curr_plan is not None:\n        plan = curr_plan\n        # don\'t check repeated usage; it\'s done in _default_fft_func()\n    if plan is None:\n        # generate a plan\n        plan = _get_cufft_plan_nd(a.shape, fft_type, axes=axes, order=order,\n                                  out_size=out_size)\n    else:\n        if not isinstance(plan, cufft.PlanNd):\n            raise ValueError(\'expected plan to have type cufft.PlanNd\')\n        if order != plan.order:\n            raise ValueError(\'array orders mismatch (plan: {}, input: {})\'\n                             .format(plan.order, order))\n        if a.flags.c_contiguous:\n            expected_shape = [a.shape[ax] for ax in axes]\n            if value_type == \'C2R\':\n                expected_shape[-1] = out_size\n        else:\n            # plan.shape will be reversed for Fortran-ordered inputs\n            expected_shape = [a.shape[ax] for ax in axes[::-1]]\n            # TODO(leofang): modify the shape for C2R\n        expected_shape = tuple(expected_shape)\n        if expected_shape != plan.shape:\n            raise ValueError(\n                \'The cuFFT plan and a.shape do not match: \'\n                \'plan.shape = {}, expected_shape={}, a.shape = {}\'.format(\n                    plan.shape, expected_shape, a.shape))\n        if fft_type != plan.fft_type:\n            raise ValueError(\'cuFFT plan dtype mismatch.\')\n        if value_type != \'C2C\':\n            if axes[-1] != plan.last_axis:\n                raise ValueError(\'The last axis for R2C/C2R mismatch\')\n            if out_size != plan.last_size:\n                raise ValueError(\'The size along the last R2C/C2R axis \'\n                                 \'mismatch\')\n\n    # TODO(leofang): support in-place transform for R2C/C2R\n    if overwrite_x and value_type == \'C2C\':\n        out = a\n    elif out is None:\n        out = plan.get_output_array(a, order=order)\n    else:\n        plan.check_output_array(a, out)\n    plan.fft(a, out, direction)\n\n    # normalize by the product of the shape along the transformed axes\n    arr = a if fft_type in (cufft.CUFFT_R2C, cufft.CUFFT_D2Z) else out\n    sz = _prod([arr.shape[ax] for ax in axes])\n    if norm is None:\n        if direction == cufft.CUFFT_INVERSE:\n            out /= sz\n    else:\n        out /= math.sqrt(sz)\n\n    return out\n\n\ndef _fftn(a, s, axes, norm, direction, value_type=\'C2C\', order=\'A\', plan=None,\n          overwrite_x=False, out=None):\n    if norm not in (None, \'ortho\'):\n        raise ValueError(\'Invalid norm value %s, should be None or ""ortho"".\'\n                         % norm)\n\n    axes, axes_sorted = _prep_fftn_axes(a.ndim, s, axes, value_type)\n    if not axes_sorted:\n        if value_type == \'C2C\':\n            return a\n        else:\n            raise IndexError(\'list index out of range\')\n    a = _convert_dtype(a, value_type)\n\n    if order == \'A\':\n        if a.flags.f_contiguous:\n            order = \'F\'\n        elif a.flags.c_contiguous:\n            order = \'C\'\n        else:\n            a = cupy.ascontiguousarray(a)\n            order = \'C\'\n    elif order not in [\'C\', \'F\']:\n        raise ValueError(\'Unsupported order: {}\'.format(order))\n\n    # Note: need to call _cook_shape prior to sorting the axes\n    a = _cook_shape(a, s, axes, value_type, order=order)\n\n    if order == \'C\' and not a.flags.c_contiguous:\n        a = cupy.ascontiguousarray(a)\n    elif order == \'F\' and not a.flags.f_contiguous:\n        a = cupy.asfortranarray(a)\n\n    # _cook_shape tells us input shape only, and not output shape\n    out_size = _get_fftn_out_size(a.shape, s, axes_sorted[-1], value_type)\n\n    a = _exec_fftn(a, direction, value_type, norm=norm, axes=axes_sorted,\n                   overwrite_x=overwrite_x, plan=plan, out=out,\n                   out_size=out_size)\n    return a\n\n\ndef _default_fft_func(a, s=None, axes=None, plan=None, value_type=\'C2C\'):\n    curr_plan = cufft.get_current_plan()\n    if curr_plan is not None:\n        if plan is None:\n            plan = curr_plan\n        else:\n            raise RuntimeError(\'Use the cuFFT plan either as a context manager\'\n                               \' or as an argument.\')\n\n    if isinstance(plan, cufft.PlanNd):  # a shortcut for using _fftn\n        return _fftn\n    elif (isinstance(plan, cufft.Plan1d) or\n          a.ndim == 1 or not config.enable_nd_planning):\n        return _fft\n\n    # cuFFT\'s N-D C2R/R2C transforms may not agree with NumPy\'s outcomes\n    if a.flags.f_contiguous and value_type != \'C2C\':\n        return _fft\n\n    _, axes_sorted = _prep_fftn_axes(a.ndim, s, axes, value_type)\n    if len(axes_sorted) > 1 and _nd_plan_is_possible(axes_sorted, a.ndim):\n        # prefer Plan1D in the 1D case\n        return _fftn\n    return _fft\n\n\ndef fft(a, n=None, axis=-1, norm=None):\n    """"""Compute the one-dimensional FFT.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        n (None or int): Length of the transformed axis of the output. If ``n``\n            is not given, the length of the input along the axis specified by\n            ``axis`` is used.\n        axis (int): Axis over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``n`` and type\n            will convert to complex if the input is other.\n\n    .. seealso:: :func:`numpy.fft.fft`\n    """"""\n    return _fft(a, (n,), (axis,), norm, cupy.cuda.cufft.CUFFT_FORWARD)\n\n\ndef ifft(a, n=None, axis=-1, norm=None):\n    """"""Compute the one-dimensional inverse FFT.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        n (None or int): Length of the transformed axis of the output. If ``n``\n            is not given, the length of the input along the axis specified by\n            ``axis`` is used.\n        axis (int): Axis over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``n`` and type\n            will convert to complex if the input is other.\n\n    .. seealso:: :func:`numpy.fft.ifft`\n    """"""\n    return _fft(a, (n,), (axis,), norm, cufft.CUFFT_INVERSE)\n\n\ndef fft2(a, s=None, axes=(-2, -1), norm=None):\n    """"""Compute the two-dimensional FFT.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        s (None or tuple of ints): Shape of the transformed axes of the\n            output. If ``s`` is not given, the lengths of the input along the\n            axes specified by ``axes`` are used.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``s`` and type\n            will convert to complex if the input is other.\n\n    .. seealso:: :func:`numpy.fft.fft2`\n    """"""\n    func = _default_fft_func(a, s, axes)\n    return func(a, s, axes, norm, cufft.CUFFT_FORWARD)\n\n\ndef ifft2(a, s=None, axes=(-2, -1), norm=None):\n    """"""Compute the two-dimensional inverse FFT.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        s (None or tuple of ints): Shape of the transformed axes of the\n            output. If ``s`` is not given, the lengths of the input along the\n            axes specified by ``axes`` are used.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``s`` and type\n            will convert to complex if the input is other.\n\n    .. seealso:: :func:`numpy.fft.ifft2`\n    """"""\n    func = _default_fft_func(a, s, axes)\n    return func(a, s, axes, norm, cufft.CUFFT_INVERSE)\n\n\ndef fftn(a, s=None, axes=None, norm=None):\n    """"""Compute the N-dimensional FFT.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        s (None or tuple of ints): Shape of the transformed axes of the\n            output. If ``s`` is not given, the lengths of the input along the\n            axes specified by ``axes`` are used.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``s`` and type\n            will convert to complex if the input is other.\n\n    .. seealso:: :func:`numpy.fft.fftn`\n    """"""\n    func = _default_fft_func(a, s, axes)\n    return func(a, s, axes, norm, cufft.CUFFT_FORWARD)\n\n\ndef ifftn(a, s=None, axes=None, norm=None):\n    """"""Compute the N-dimensional inverse FFT.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        s (None or tuple of ints): Shape of the transformed axes of the\n            output. If ``s`` is not given, the lengths of the input along the\n            axes specified by ``axes`` are used.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``s`` and type\n            will convert to complex if the input is other.\n\n    .. seealso:: :func:`numpy.fft.ifftn`\n    """"""\n    func = _default_fft_func(a, s, axes)\n    return func(a, s, axes, norm, cufft.CUFFT_INVERSE)\n\n\ndef rfft(a, n=None, axis=-1, norm=None):\n    """"""Compute the one-dimensional FFT for real input.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        n (None or int): Number of points along transformation axis in the\n            input to use. If ``n`` is not given, the length of the input along\n            the axis specified by ``axis`` is used.\n        axis (int): Axis over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``n`` and type\n            will convert to complex if the input is other. The length of the\n            transformed axis is ``n//2+1``.\n\n    .. seealso:: :func:`numpy.fft.rfft`\n    """"""\n    return _fft(a, (n,), (axis,), norm, cufft.CUFFT_FORWARD, \'R2C\')\n\n\ndef irfft(a, n=None, axis=-1, norm=None):\n    """"""Compute the one-dimensional inverse FFT for real input.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        n (None or int): Length of the transformed axis of the output. For\n            ``n`` output points, ``n//2+1`` input points are necessary. If\n            ``n`` is not given, it is determined from the length of the input\n            along the axis specified by ``axis``.\n        axis (int): Axis over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``n`` and type\n            will convert to complex if the input is other. If ``n`` is not\n            given, the length of the transformed axis is`2*(m-1)` where `m`\n            is the length of the transformed axis of the input.\n\n    .. seealso:: :func:`numpy.fft.irfft`\n    """"""\n    return _fft(a, (n,), (axis,), norm, cufft.CUFFT_INVERSE, \'C2R\')\n\n\ndef rfft2(a, s=None, axes=(-2, -1), norm=None):\n    """"""Compute the two-dimensional FFT for real input.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        s (None or tuple of ints): Shape to use from the input. If ``s`` is not\n            given, the lengths of the input along the axes specified by\n            ``axes`` are used.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``s`` and type\n            will convert to complex if the input is other. The length of the\n            last axis transformed will be ``s[-1]//2+1``.\n\n    .. seealso:: :func:`numpy.fft.rfft2`\n    """"""\n    func = _default_fft_func(a, s, axes, value_type=\'R2C\')\n    return func(a, s, axes, norm, cufft.CUFFT_FORWARD, \'R2C\')\n\n\ndef irfft2(a, s=None, axes=(-2, -1), norm=None):\n    """"""Compute the two-dimensional inverse FFT for real input.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        s (None or tuple of ints): Shape of the output. If ``s`` is not given,\n            they are determined from the lengths of the input along the axes\n            specified by ``axes``.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``s`` and type\n            will convert to complex if the input is other. If ``s`` is not\n            given, the length of final transformed axis of output will be\n            `2*(m-1)` where `m` is the length of the final transformed axis of\n            the input.\n\n    .. seealso:: :func:`numpy.fft.irfft2`\n    """"""\n    func = _default_fft_func(a, s, axes, value_type=\'C2R\')\n    return func(a, s, axes, norm, cufft.CUFFT_INVERSE, \'C2R\')\n\n\ndef rfftn(a, s=None, axes=None, norm=None):\n    """"""Compute the N-dimensional FFT for real input.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        s (None or tuple of ints): Shape to use from the input. If ``s`` is not\n            given, the lengths of the input along the axes specified by\n            ``axes`` are used.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``s`` and type\n            will convert to complex if the input is other. The length of the\n            last axis transformed will be ``s[-1]//2+1``.\n\n    .. seealso:: :func:`numpy.fft.rfftn`\n    """"""\n    func = _default_fft_func(a, s, axes, value_type=\'R2C\')\n    return func(a, s, axes, norm, cufft.CUFFT_FORWARD, \'R2C\')\n\n\ndef _size_last_transform_axis(shape, s, axes):\n    if s is not None:\n        if s[-1] is not None:\n            return s[-1]\n    elif axes is not None:\n        return shape[axes[-1]]\n    return shape[-1]\n\n\ndef irfftn(a, s=None, axes=None, norm=None):\n    """"""Compute the N-dimensional inverse FFT for real input.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        s (None or tuple of ints): Shape of the output. If ``s`` is not given,\n            they are determined from the lengths of the input along the axes\n            specified by ``axes``.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``s`` and type\n            will convert to complex if the input is other. If ``s`` is not\n            given, the length of final transformed axis of output will be\n            ``2*(m-1)`` where `m` is the length of the final transformed axis\n            of the input.\n\n    .. seealso:: :func:`numpy.fft.irfftn`\n    """"""\n    if (10020 >= cupy.cuda.runtime.runtimeGetVersion() >= 10010\n            and int(cupy.cuda.device.get_compute_capability()) < 70\n            and _size_last_transform_axis(a.shape, s, axes) == 2):\n        warnings.warn(\'Output of irfftn might not be correct due to issue \'\n                      \'of cuFFT in CUDA 10.1/10.2 on Pascal or older GPUs.\')\n\n    func = _default_fft_func(a, s, axes, value_type=\'C2R\')\n    return func(a, s, axes, norm, cufft.CUFFT_INVERSE, \'C2R\')\n\n\ndef hfft(a, n=None, axis=-1, norm=None):\n    """"""Compute the FFT of a signal that has Hermitian symmetry.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        n (None or int): Length of the transformed axis of the output. For\n            ``n`` output points, ``n//2+1`` input points are necessary. If\n            ``n`` is not given, it is determined from the length of the input\n            along the axis specified by ``axis``.\n        axis (int): Axis over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``n`` and type\n            will convert to complex if the input is other. If ``n`` is not\n            given, the length of the transformed axis is ``2*(m-1)`` where `m`\n            is the length of the transformed axis of the input.\n\n    .. seealso:: :func:`numpy.fft.hfft`\n    """"""\n    a = irfft(a.conj(), n, axis)\n    return a * (a.shape[axis] if norm is None else\n                cupy.sqrt(a.shape[axis], dtype=a.dtype))\n\n\ndef ihfft(a, n=None, axis=-1, norm=None):\n    """"""Compute the FFT of a signal that has Hermitian symmetry.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        n (None or int): Number of points along transformation axis in the\n            input to use. If ``n`` is not given, the length of the input along\n            the axis specified by ``axis`` is used.\n        axis (int): Axis over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``n`` and type\n            will convert to complex if the input is other. The length of the\n            transformed axis is ``n//2+1``.\n\n    .. seealso:: :func:`numpy.fft.ihfft`\n    """"""\n    if n is None:\n        n = a.shape[axis]\n    return rfft(a, n, axis, norm).conj() / (n if norm is None else 1)\n\n\ndef fftfreq(n, d=1.0):\n    """"""Return the FFT sample frequencies.\n\n    Args:\n        n (int): Window length.\n        d (scalar): Sample spacing.\n\n    Returns:\n        cupy.ndarray: Array of length ``n`` containing the sample frequencies.\n\n    .. seealso:: :func:`numpy.fft.fftfreq`\n    """"""\n    return cupy.hstack((cupy.arange(0, (n - 1) // 2 + 1, dtype=np.float64),\n                        cupy.arange(-(n // 2), 0, dtype=np.float64))) / n / d\n\n\ndef rfftfreq(n, d=1.0):\n    """"""Return the FFT sample frequencies for real input.\n\n    Args:\n        n (int): Window length.\n        d (scalar): Sample spacing.\n\n    Returns:\n        cupy.ndarray:\n            Array of length ``n//2+1`` containing the sample frequencies.\n\n    .. seealso:: :func:`numpy.fft.rfftfreq`\n    """"""\n    return cupy.arange(0, n // 2 + 1, dtype=np.float64) / n / d\n\n\ndef fftshift(x, axes=None):\n    """"""Shift the zero-frequency component to the center of the spectrum.\n\n    Args:\n        x (cupy.ndarray): Input array.\n        axes (int or tuple of ints): Axes over which to shift. Default is\n            ``None``, which shifts all axes.\n\n    Returns:\n        cupy.ndarray: The shifted array.\n\n    .. seealso:: :func:`numpy.fft.fftshift`\n    """"""\n    x = cupy.asarray(x)\n    if axes is None:\n        axes = list(range(x.ndim))\n    elif isinstance(axes, np.compat.integer_types):\n        axes = (axes,)\n    for axis in axes:\n        x = cupy.roll(x, x.shape[axis] // 2, axis)\n    return x\n\n\ndef ifftshift(x, axes=None):\n    """"""The inverse of :meth:`fftshift`.\n\n    Args:\n        x (cupy.ndarray): Input array.\n        axes (int or tuple of ints): Axes over which to shift. Default is\n            ``None``, which shifts all axes.\n\n    Returns:\n        cupy.ndarray: The shifted array.\n\n    .. seealso:: :func:`numpy.fft.ifftshift`\n    """"""\n    x = cupy.asarray(x)\n    if axes is None:\n        axes = list(range(x.ndim))\n    elif isinstance(axes, np.compat.integer_types):\n        axes = (axes,)\n    for axis in axes:\n        x = cupy.roll(x, -(x.shape[axis] // 2), axis)\n    return x\n'"
cupy/indexing/__init__.py,0,"b'# Functions from the following NumPy document\n# https://docs.scipy.org/doc/numpy/reference/routines.indexing.html\n\n# ""NOQA"" to suppress flake8 warning\nfrom cupy.indexing import generate  # NOQA\nfrom cupy.indexing import indexing  # NOQA\nfrom cupy.indexing import insert  # NOQA\nfrom cupy.indexing import iterate  # NOQA\n'"
cupy/indexing/generate.py,4,"b'# class s_(object):\n\nimport functools\nimport numbers\nimport operator\n\nimport numpy\n\nimport cupy\nfrom cupy import core\nfrom cupy.creation import from_data\nfrom cupy.manipulation import join\n\n\nclass AxisConcatenator(object):\n    """"""Translates slice objects to concatenation along an axis.\n\n    For detailed documentation on usage, see :func:`cupy.r_`.\n    This implementation is partially borrowed from NumPy\'s one.\n\n    """"""\n\n    def _output_obj(self, obj, ndim, ndmin, trans1d):\n        k2 = ndmin - ndim\n        if trans1d < 0:\n            trans1d += k2 + 1\n        defaxes = list(range(ndmin))\n        k1 = trans1d\n        axes = defaxes[:k1] + defaxes[k2:] + \\\n            defaxes[k1:k2]\n        return obj.transpose(axes)\n\n    def __init__(self, axis=0, matrix=False, ndmin=1, trans1d=-1):\n        self.axis = axis\n        self.trans1d = trans1d\n        self.matrix = matrix\n        self.ndmin = ndmin\n\n    def __getitem__(self, key):\n        trans1d = self.trans1d\n        ndmin = self.ndmin\n        objs = []\n        scalars = []\n        arraytypes = []\n        scalartypes = []\n        if isinstance(key, str):\n            raise NotImplementedError\n        if not isinstance(key, tuple):\n            key = (key,)\n\n        for i, k in enumerate(key):\n            scalar = False\n            if isinstance(k, slice):\n                raise NotImplementedError\n            elif isinstance(k, str):\n                if i != 0:\n                    raise ValueError(\n                        \'special directives must be the first entry.\')\n                raise NotImplementedError\n            elif type(k) in numpy.ScalarType:\n                newobj = from_data.array(k, ndmin=ndmin)\n                scalars.append(i)\n                scalar = True\n                scalartypes.append(newobj.dtype)\n            else:\n                newobj = from_data.array(k, copy=False, ndmin=ndmin)\n                if ndmin > 1:\n                    ndim = from_data.array(k, copy=False).ndim\n                    if trans1d != -1 and ndim < ndmin:\n                        newobj = self._output_obj(newobj, ndim, ndmin, trans1d)\n\n            objs.append(newobj)\n            if not scalar and isinstance(newobj, core.ndarray):\n                arraytypes.append(newobj.dtype)\n\n        final_dtype = numpy.find_common_type(arraytypes, scalartypes)\n        if final_dtype is not None:\n            for k in scalars:\n                objs[k] = objs[k].astype(final_dtype)\n\n        return join.concatenate(tuple(objs), axis=self.axis)\n\n    def __len__(self):\n        return 0\n\n\nclass CClass(AxisConcatenator):\n\n    def __init__(self):\n        super(CClass, self).__init__(-1, ndmin=2, trans1d=0)\n\n\nc_ = CClass()\n""""""Translates slice objects to concatenation along the second axis.\n\nThis is a CuPy object that corresponds to :obj:`cupy.r_`, which is\nuseful because of its common occurrence. In particular, arrays will be\nstacked along their last axis after being upgraded to at least 2-D with\n1\'s post-pended to the shape (column vectors made out of 1-D arrays).\n\nFor detailed documentation, see :obj:`r_`.\n\nThis implementation is partially borrowed from NumPy\'s one.\n\nReturns:\n    cupy.ndarray: Joined array.\n\n.. seealso:: :obj:`numpy.c_`\n\nExamples\n--------\n>>> a = cupy.array([[1, 2, 3]], dtype=np.int32)\n>>> b = cupy.array([[4, 5, 6]], dtype=np.int32)\n>>> cupy.c_[a, 0, 0, b]\narray([[1, 2, 3, 0, 0, 4, 5, 6]], dtype=int32)\n\n""""""\n\n\nclass RClass(AxisConcatenator):\n\n    def __init__(self):\n        super(RClass, self).__init__()\n\n\nr_ = RClass()\n""""""Translates slice objects to concatenation along the first axis.\n\nThis is a simple way to build up arrays quickly.\nIf the index expression contains comma separated arrays, then stack\nthem along their first axis.\n\nThis object can build up from normal CuPy arrays.\nTherefore, the other objects (e.g. writing strings like \'2,3,4\',\nor using imaginary numbers like [1,2,3j],\nor using string integers like \'-1\') are not implemented yet\ncompared with NumPy.\n\nThis implementation is partially borrowed from NumPy\'s one.\n\nReturns:\n    cupy.ndarray: Joined array.\n\n.. seealso:: :obj:`numpy.r_`\n\nExamples\n--------\n>>> a = cupy.array([1, 2, 3], dtype=np.int32)\n>>> b = cupy.array([4, 5, 6], dtype=np.int32)\n>>> cupy.r_[a, 0, 0, b]\narray([1, 2, 3, 0, 0, 4, 5, 6], dtype=int32)\n\n""""""\n\n\ndef indices(dimensions, dtype=int):\n    """"""Returns an array representing the indices of a grid.\n\n    Computes an array where the subarrays contain index values 0,1,...\n    varying only along the corresponding axis.\n\n    Args:\n        dimensions: The shape of the grid.\n        dtype: Data type specifier. It is int by default.\n\n    Returns:\n        ndarray:\n        The array of grid indices,\n        ``grid.shape = (len(dimensions),) + tuple(dimensions)``.\n\n    Examples\n    --------\n    >>> grid = cupy.indices((2, 3))\n    >>> grid.shape\n    (2, 2, 3)\n    >>> grid[0]        # row indices\n    array([[0, 0, 0],\n           [1, 1, 1]])\n    >>> grid[1]        # column indices\n    array([[0, 1, 2],\n           [0, 1, 2]])\n\n    .. seealso:: :func:`numpy.indices`\n\n    """"""\n    dimensions = tuple(dimensions)\n    N = len(dimensions)\n    shape = (1,) * N\n    res = cupy.empty((N,) + dimensions, dtype=dtype)\n    for i, dim in enumerate(dimensions):\n        res[i] = cupy.arange(dim, dtype=dtype).reshape(\n            shape[:i] + (dim,) + shape[i + 1:]\n        )\n    return res\n\n\ndef ix_(*args):\n    """"""Construct an open mesh from multiple sequences.\n\n    This function takes N 1-D sequences and returns N outputs with N\n    dimensions each, such that the shape is 1 in all but one dimension\n    and the dimension with the non-unit shape value cycles through all\n    N dimensions.\n\n    Using `ix_` one can quickly construct index arrays that will index\n    the cross product. ``a[cupy.ix_([1,3],[2,5])]`` returns the array\n    ``[[a[1,2] a[1,5]], [a[3,2] a[3,5]]]``.\n\n    Args:\n        *args: 1-D sequences\n\n    Returns:\n        tuple of ndarrays:\n        N arrays with N dimensions each, with N the number of input sequences.\n        Together these arrays form an open mesh.\n\n    Examples\n    --------\n    >>> a = cupy.arange(10).reshape(2, 5)\n    >>> a\n    array([[0, 1, 2, 3, 4],\n           [5, 6, 7, 8, 9]])\n    >>> ixgrid = cupy.ix_([0,1], [2,4])\n    >>> ixgrid\n    (array([[0],\n           [1]]), array([[2, 4]]))\n\n    .. warning::\n\n        This function may synchronize the device.\n\n    .. seealso:: :func:`numpy.ix_`\n\n    """"""\n    # TODO(niboshi): Avoid nonzero which may synchronize the device.\n    out = []\n    nd = len(args)\n    for k, new in enumerate(args):\n        new = from_data.asarray(new)\n        if new.ndim != 1:\n            raise ValueError(\'Cross index must be 1 dimensional\')\n        if new.size == 0:\n            # Explicitly type empty arrays to avoid float default\n            new = new.astype(numpy.intp)\n        if cupy.issubdtype(new.dtype, cupy.bool_):\n            new, = new.nonzero()  # may synchronize\n        new = new.reshape((1,) * k + (new.size,) + (1,) * (nd - k - 1))\n        out.append(new)\n    return tuple(out)\n\n\ndef ravel_multi_index(multi_index, dims, mode=\'wrap\', order=\'C\'):\n    """"""\n    Converts a tuple of index arrays into an array of flat indices, applying\n    boundary modes to the multi-index.\n\n    Args:\n        multi_index (tuple of cupy.ndarray) : A tuple of integer arrays, one\n            array for each dimension.\n        dims (tuple of ints): The shape of array into which the indices from\n            ``multi_index`` apply.\n        mode (\'raise\', \'wrap\' or \'clip\'), optional: Specifies how out-of-bounds\n            indices are handled.  Can specify either one mode or a tuple of\n            modes, one mode per index:\n\n            - *\'raise\'* -- raise an error\n            - *\'wrap\'* -- wrap around (default)\n            - *\'clip\'* -- clip to the range\n\n            In \'clip\' mode, a negative index which would normally wrap will\n            clip to 0 instead.\n        order (\'C\' or \'F\'), optional: Determines whether the multi-index should\n            be viewed as indexing in row-major (C-style) or column-major\n            (Fortran-style) order.\n\n    Returns:\n        raveled_indices (cupy.ndarray): An array of indices into the flattened\n            version of an array of dimensions ``dims``.\n\n    .. warning::\n\n        This function may synchronize the device when ``mode == \'raise\'``.\n\n    Notes\n    -----\n    Note that the default `mode` (``\'wrap\'``) is different than in NumPy. This\n    is done to avoid potential device synchronization.\n\n    Examples\n    --------\n    >>> cupy.ravel_multi_index(cupy.asarray([[3,6,6],[4,5,1]]), (7,6))\n    array([22, 41, 37])\n    >>> cupy.ravel_multi_index(cupy.asarray([[3,6,6],[4,5,1]]), (7,6),\n    ...                        order=\'F\')\n    array([31, 41, 13])\n    >>> cupy.ravel_multi_index(cupy.asarray([[3,6,6],[4,5,1]]), (4,6),\n    ...                        mode=\'clip\')\n    array([22, 23, 19])\n    >>> cupy.ravel_multi_index(cupy.asarray([[3,6,6],[4,5,1]]), (4,4),\n    ...                        mode=(\'clip\', \'wrap\'))\n    array([12, 13, 13])\n    >>> cupy.ravel_multi_index(cupy.asarray((3,1,4,1)), (6,7,8,9))\n    array(1621)\n\n    .. seealso:: :func:`numpy.ravel_multi_index`, :func:`unravel_index`\n    """"""\n\n    ndim = len(dims)\n    if len(multi_index) != ndim:\n        raise ValueError(\n            ""parameter multi_index must be a sequence of ""\n            ""length {}"".format(ndim))\n\n    for d in dims:\n        if not isinstance(d, numbers.Integral):\n            raise TypeError(\n                ""{} object cannot be interpreted as an integer"".format(\n                    type(d)))\n\n    if isinstance(mode, str):\n        mode = (mode, ) * ndim\n\n    if functools.reduce(operator.mul, dims) > cupy.iinfo(cupy.int64).max:\n        raise ValueError(""invalid dims: array size defined by dims is larger ""\n                         ""than the maximum possible size"")\n\n    s = 1\n    ravel_strides = [1] * ndim\n    if order is None:\n        order = ""C""\n    if order == ""C"":\n        for i in range(ndim - 2, -1, -1):\n            s = s * dims[i + 1]\n            ravel_strides[i] = s\n    elif order == ""F"":\n        for i in range(1, ndim):\n            s = s * dims[i - 1]\n            ravel_strides[i] = s\n    else:\n        raise TypeError(""order not understood"")\n\n    multi_index = cupy.broadcast_arrays(*multi_index)\n    raveled_indices = cupy.zeros(multi_index[0].shape, dtype=cupy.int64)\n    for d, stride, idx, _mode in zip(dims, ravel_strides, multi_index, mode):\n\n        if not isinstance(idx, cupy.ndarray):\n            raise TypeError(""elements of multi_index must be cupy arrays"")\n        if not cupy.can_cast(idx, cupy.int64, \'same_kind\'):\n            raise TypeError(\n                \'multi_index entries could not be cast from dtype(\\\'{}\\\') to \'\n                \'dtype(\\\'{}\\\') according to the rule \\\'same_kind\\\'\'.format(\n                    idx.dtype, cupy.int64().dtype))\n        idx = idx.astype(cupy.int64, copy=False)\n\n        if _mode == ""raise"":\n            if cupy.any(cupy.logical_or(idx >= d, idx < 0)):\n                raise ValueError(""invalid entry in coordinates array"")\n        elif _mode == ""clip"":\n            idx = cupy.clip(idx, 0, d - 1)\n        elif _mode == \'wrap\':\n            idx = idx % d\n        else:\n            raise TypeError(""Unrecognized mode: {}"".format(_mode))\n        raveled_indices += stride * idx\n    return raveled_indices\n\n\ndef unravel_index(indices, dims, order=\'C\'):\n    """"""Converts array of flat indices into a tuple of coordinate arrays.\n\n    Args:\n        indices (cupy.ndarray): An integer array whose elements are indices\n            into the flattened version of an array of dimensions :obj:`dims`.\n        dims (tuple of ints): The shape of the array to use for unraveling\n            indices.\n        order (\'C\' or \'F\'): Determines whether the indices should be viewed as\n            indexing in row-major (C-style) or column-major (Fortran-style)\n            order.\n\n    Returns:\n        tuple of ndarrays:\n        Each array in the tuple has the same shape as the indices array.\n\n    Examples\n    --------\n    >>> cupy.unravel_index(cupy.array([22, 41, 37]), (7, 6))\n    (array([3, 6, 6]), array([4, 5, 1]))\n    >>> cupy.unravel_index(cupy.array([31, 41, 13]), (7, 6), order=\'F\')\n    (array([3, 6, 6]), array([4, 5, 1]))\n\n    .. warning::\n\n        This function may synchronize the device.\n\n    .. seealso:: :func:`numpy.unravel_index`, :func:`ravel_multi_index`\n\n    """"""\n    if order is None:\n        order = \'C\'\n\n    if order == \'C\':\n        dims = reversed(dims)\n    elif order == \'F\':\n        pass\n    else:\n        raise TypeError(\'order not understood\')\n\n    if not cupy.can_cast(indices, cupy.int64, \'same_kind\'):\n        raise TypeError(\n            \'Iterator operand 0 dtype could not be cast \'\n            \'from dtype(\\\'{}\\\') to dtype(\\\'{}\\\') \'\n            \'according to the rule \\\'same_kind\\\'\'.format(\n                indices.dtype, cupy.int64().dtype))\n\n    if (indices < 0).any():  # synchronize!\n        raise ValueError(\'invalid entry in index array\')\n\n    unraveled_coords = []\n    for dim in dims:\n        unraveled_coords.append(indices % dim)\n        indices = indices // dim\n\n    if (indices > 0).any():  # synchronize!\n        raise ValueError(\'invalid entry in index array\')\n\n    if order == \'C\':\n        unraveled_coords = reversed(unraveled_coords)\n    return tuple(unraveled_coords)\n\n\n# TODO(okuta): Implement diag_indices\n\n\n# TODO(okuta): Implement diag_indices_from\n\n\n# TODO(okuta): Implement mask_indices\n\n\n# TODO(okuta): Implement tril_indices\n\n\n# TODO(okuta): Implement tril_indices_from\n\n\n# TODO(okuta): Implement triu_indices\n\n\n# TODO(okuta): Implement triu_indices_from\n'"
cupy/indexing/indexing.py,2,"b'import numpy\n\nimport cupy\n\n\ndef take(a, indices, axis=None, out=None):\n    """"""Takes elements of an array at specified indices along an axis.\n\n    This is an implementation of ""fancy indexing"" at single axis.\n\n    This function does not support ``mode`` option.\n\n    Args:\n        a (cupy.ndarray): Array to extract elements.\n        indices (int or array-like): Indices of elements that this function\n            takes.\n        axis (int): The axis along which to select indices. The flattened input\n            is used by default.\n        out (cupy.ndarray): Output array. If provided, it should be of\n            appropriate shape and dtype.\n\n    Returns:\n        cupy.ndarray: The result of fancy indexing.\n\n    .. seealso:: :func:`numpy.take`\n\n    """"""\n    # TODO(okuta): check type\n    return a.take(indices, axis, out)\n\n\ndef take_along_axis(a, indices, axis):\n    """"""Take values from the input array by matching 1d index and data slices.\n\n    Args:\n        a (cupy.ndarray): Array to extract elements.\n        indices (cupy.ndarray): Indices to take along each 1d slice of ``a``.\n        axis (int): The axis to take 1d slices along.\n\n    Returns:\n        cupy.ndarray: The indexed result.\n\n    .. seealso:: :func:`numpy.take_along_axis`\n    """"""\n\n    if indices.dtype.kind not in (\'i\', \'u\'):\n        raise IndexError(\'`indices` must be an integer array\')\n\n    if axis is None:\n        a = a.ravel()\n        axis = 0\n\n    ndim = a.ndim\n\n    if not (-ndim <= axis < ndim):\n        raise numpy.AxisError(\'Axis overrun\')\n\n    axis %= a.ndim\n\n    if ndim != indices.ndim:\n        raise ValueError(\n            \'`indices` and `a` must have the same number of dimensions\')\n\n    fancy_index = []\n    for i, n in enumerate(a.shape):\n        if i == axis:\n            fancy_index.append(indices)\n        else:\n            ind_shape = (1,) * i + (-1,) + (1,) * (ndim - i - 1)\n            fancy_index.append(cupy.arange(n).reshape(ind_shape))\n\n    return a[fancy_index]\n\n\ndef choose(a, choices, out=None, mode=\'raise\'):\n    return a.choose(choices, out, mode)\n\n\ndef compress(condition, a, axis=None, out=None):\n    """"""Returns selected slices of an array along given axis.\n\n    Args:\n        condition (1-D array of bools): Array that selects which entries to\n            return. If len(condition) is less than the size of a along the\n            given axis, then output is truncated to the length of the condition\n            array.\n        a (cupy.ndarray): Array from which to extract a part.\n        axis (int): Axis along which to take slices. If None (default), work\n            on the flattened array.\n        out (cupy.ndarray): Output array. If provided, it should be of\n            appropriate shape and dtype.\n\n    Returns:\n        cupy.ndarray: A copy of a without the slices along axis for which\n            condition is false.\n\n    .. warning::\n\n            This function may synchronize the device.\n\n\n    .. seealso:: :func:`numpy.compress`\n\n    """"""\n    return a.compress(condition, axis, out)\n\n\ndef diagonal(a, offset=0, axis1=0, axis2=1):\n    """"""Returns specified diagonals.\n\n    This function extracts the diagonals along two specified axes. The other\n    axes are not changed. This function returns a writable view of this array\n    as NumPy 1.10 will do.\n\n    Args:\n        a (cupy.ndarray): Array from which the diagonals are taken.\n        offset (int): Index of the diagonals. Zero indicates the main\n            diagonals, a positive value upper diagonals, and a negative value\n            lower diagonals.\n        axis1 (int): The first axis to take diagonals from.\n        axis2 (int): The second axis to take diagonals from.\n\n    Returns:\n        cupy.ndarray: A view of the diagonals of ``a``.\n\n    .. seealso:: :func:`numpy.diagonal`\n\n    """"""\n    # TODO(okuta): check type\n    return a.diagonal(offset, axis1, axis2)\n\n\ndef extract(condition, a):\n    """"""Return the elements of an array that satisfy some condition.\n\n    This is equivalent to ``np.compress(ravel(condition), ravel(arr))``.\n    If ``condition`` is boolean, ``np.extract`` is equivalent to\n    ``arr[condition]``.\n\n    Args:\n        condition (int or array_like): An array whose nonzero or True entries\n            indicate the elements of array to extract.\n        a (cupy.ndarray): Input array of the same size as condition.\n\n    Returns:\n        cupy.ndarray: Rank 1 array of values from arr where condition is True.\n\n    .. warning::\n\n            This function may synchronize the device.\n\n    .. seealso:: :func:`numpy.extract`\n    """"""\n\n    if not isinstance(a, cupy.ndarray):\n        raise TypeError(\'extract requires input array to be cupy.ndarray\')\n\n    if not isinstance(condition, cupy.ndarray):\n        condition = cupy.array(condition)\n\n    a = a.ravel()\n    condition = condition.ravel()\n\n    return a.take(condition.nonzero()[0])\n\n\ndef select(condlist, choicelist, default=0):\n    """"""Return an array drawn from elements in choicelist, depending on conditions.\n\n    Args:\n        condlist (list of bool arrays): The list of conditions which determine\n            from which array in `choicelist` the output elements are taken.\n            When multiple conditions are satisfied, the first one encountered\n            in `condlist` is used.\n        choicelist (list of cupy.ndarray): The list of arrays from which the\n            output elements are taken. It has to be of the same length\n            as `condlist`.\n        default (scalar) : If provided, will fill element inserted in `output`\n            when all conditions evaluate to False. default value is 0.\n\n    Returns:\n        cupy.ndarray: The output at position m is the m-th element of the\n        array in `choicelist` where the m-th element of the corresponding\n        array in `condlist` is True.\n\n    .. seealso:: :func:`numpy.select`\n    """"""\n\n    if len(condlist) != len(choicelist):\n        raise ValueError(\n            \'list of cases must be same length as list of conditions\')\n\n    if len(condlist) == 0:\n        raise ValueError(""select with an empty condition list is not possible"")\n\n    if not cupy.isscalar(default):\n        raise TypeError(""default only accepts scalar values"")\n\n    for i in range(len(choicelist)):\n        if not isinstance(choicelist[i], cupy.ndarray):\n            raise TypeError(""choicelist only accepts lists of cupy ndarrays"")\n        cond = condlist[i]\n        if cond.dtype.type is not cupy.bool_:\n            raise ValueError(\n                \'invalid entry {} in condlist: should be boolean ndarray\'\n                .format(i))\n\n    dtype = cupy.result_type(*choicelist)\n\n    condlist = cupy.broadcast_arrays(*condlist)\n    choicelist = cupy.broadcast_arrays(*choicelist, default)\n\n    if choicelist[0].ndim == 0:\n        result_shape = condlist[0].shape\n    else:\n        result_shape = cupy.broadcast_arrays(condlist[0],\n                                             choicelist[0])[0].shape\n\n    result = cupy.empty(result_shape, dtype)\n    cupy.copyto(result, default)\n\n    choicelist = choicelist[-2::-1]\n    condlist = condlist[::-1]\n    for choice, cond in zip(choicelist, condlist):\n        cupy.copyto(result, choice, where=cond)\n\n    return result\n'"
cupy/indexing/insert.py,2,"b'import numpy\n\nimport cupy\nfrom cupy import core\n\n\ndef place(arr, mask, vals):\n    """"""Change elements of an array based on conditional and input values.\n\n    This function uses the first N elements of `vals`, where N is the number\n    of true values in `mask`.\n\n    Args:\n        arr (cupy.ndarray): Array to put data into.\n        mask (array-like): Boolean mask array. Must have the same size as `a`.\n        vals (array-like): Values to put into `a`. Only the first\n            N elements are used, where N is the number of True values in\n            `mask`. If `vals` is smaller than N, it will be repeated, and if\n            elements of `a` are to be masked, this sequence must be non-empty.\n\n    Examples\n    --------\n    >>> arr = np.arange(6).reshape(2, 3)\n    >>> np.place(arr, arr>2, [44, 55])\n    >>> arr\n    array([[ 0,  1,  2],\n           [44, 55, 44]])\n\n    .. warning::\n\n        This function may synchronize the device.\n\n    .. seealso:: :func:`numpy.place`\n    """"""\n    # TODO(niboshi): Avoid nonzero which may synchronize the device.\n    mask = cupy.asarray(mask)\n    if arr.size != mask.size:\n        raise ValueError(\'Mask and data must be the same size.\')\n    vals = cupy.asarray(vals)\n\n    mask_indices = mask.ravel().nonzero()[0]  # may synchronize\n    if mask_indices.size == 0:\n        return\n    if vals.size == 0:\n        raise ValueError(\'Cannot insert from an empty array.\')\n    arr.put(mask_indices, vals, mode=\'wrap\')\n\n\ndef put(a, ind, v, mode=\'wrap\'):\n    """"""Replaces specified elements of an array with given values.\n\n    Args:\n        a (cupy.ndarray): Target array.\n        ind (array-like): Target indices, interpreted as integers.\n        v (array-like): Values to place in `a` at target indices.\n            If `v` is shorter than `ind` it will be repeated as necessary.\n        mode (str): How out-of-bounds indices will behave. Its value must be\n            either `\'raise\'`, `\'wrap\'` or `\'clip\'`. Otherwise,\n            :class:`TypeError` is raised.\n\n    .. note::\n        Default `mode` is set to `\'wrap\'` to avoid unintended performance drop.\n        If you need NumPy\'s behavior, please pass `mode=\'raise\'` manually.\n\n    .. seealso:: :func:`numpy.put`\n    """"""\n    a.put(ind, v, mode=mode)\n\n\n_putmask_kernel = core.ElementwiseKernel(\n    \'Q mask, raw S values, uint64 len_vals\', \'T out\',\n    \'\'\'\n    if (mask) out = (T) values[i % len_vals];\n    \'\'\',\n    \'putmask_kernel\'\n)\n\n\ndef putmask(a, mask, values):\n    """"""\n    Changes elements of an array inplace, based on a conditional mask and\n    input values.\n\n    Sets ``a.flat[n] = values[n]`` for each n where ``mask.flat[n]==True``.\n    If `values` is not the same size as `a` and `mask` then it will repeat.\n\n    Args:\n        a (cupy.ndarray): Target array.\n        mask (cupy.ndarray): Boolean mask array. It has to be\n            the same shape as `a`.\n        values (cupy.ndarray or scalar): Values to put into `a` where `mask`\n            is True. If `values` is smaller than `a`, then it will be\n            repeated.\n\n    Examples\n    --------\n    >>> x = cupy.arange(6).reshape(2, 3)\n    >>> cupy.putmask(x, x>2, x**2)\n    >>> x\n    array([[ 0,  1,  2],\n           [ 9, 16, 25]])\n\n    If `values` is smaller than `a` it is repeated:\n\n    >>> x = cupy.arange(6)\n    >>> cupy.putmask(x, x>2, cupy.array([-33, -44]))\n    >>> x\n    array([  0,   1,   2, -44, -33, -44])\n\n    .. seealso:: :func:`numpy.putmask`\n\n    """"""\n\n    if not isinstance(a, cupy.ndarray):\n        raise TypeError(\'`a` should be of type cupy.ndarray\')\n    if not isinstance(mask, cupy.ndarray):\n        raise TypeError(\'`mask` should be of type cupy.ndarray\')\n    if not (cupy.isscalar(values) or isinstance(values, cupy.ndarray)):\n        raise TypeError(\'`values` should be of type cupy.ndarray\')\n\n    if not a.shape == mask.shape:\n        raise ValueError(\'mask and data must be the same size\')\n\n    mask = mask.astype(numpy.bool_)\n\n    if cupy.isscalar(values):\n        a[mask] = values\n\n    elif not numpy.can_cast(values.dtype, a.dtype):\n        raise TypeError(\'Cannot cast array data from\'\n                        \' {} to {} according to the rule \\\'safe\\\'\'\n                        .format(values.dtype, a.dtype))\n\n    elif a.shape == values.shape:\n        a[mask] = values[mask]\n\n    else:\n        values = values.ravel()\n        _putmask_kernel(mask, values, len(values), a)\n\n\ndef fill_diagonal(a, val, wrap=False):\n    """"""Fills the main diagonal of the given array of any dimensionality.\n\n    For an array `a` with ``a.ndim > 2``, the diagonal is the list of\n    locations with indices ``a[i, i, ..., i]`` all identical. This function\n    modifies the input array in-place, it does not return a value.\n\n    Args:\n        a (cupy.ndarray): The array, at least 2-D.\n        val (scalar): The value to be written on the diagonal.\n            Its type must be compatible with that of the array a.\n        wrap (bool): If specified, the diagonal is ""wrapped"" after N columns.\n            This affects only tall matrices.\n\n    Examples\n    --------\n    >>> a = cupy.zeros((3, 3), int)\n    >>> cupy.fill_diagonal(a, 5)\n    >>> a\n    array([[5, 0, 0],\n           [0, 5, 0],\n           [0, 0, 5]])\n\n     .. seealso:: :func:`numpy.fill_diagonal`\n    """"""\n    # The followings are imported from the original numpy\n    if a.ndim < 2:\n        raise ValueError(\'array must be at least 2-d\')\n    end = None\n    if a.ndim == 2:\n        step = a.shape[1] + 1\n        if not wrap:\n            end = a.shape[1] * a.shape[1]\n    else:\n        if not numpy.alltrue(numpy.diff(a.shape) == 0):\n            raise ValueError(\'All dimensions of input must be of equal length\')\n        step = 1 + numpy.cumprod(a.shape[:-1]).sum()\n\n    a.flat[:end:step] = val\n\n\ndef diag_indices(n, ndim=2):\n    """""" Return the indices to access the main diagonal of an array.\n\n    Returns a tuple of indices that can be used to access the main\n    diagonal of an array with ``ndim >= 2`` dimensions and shape\n    (n, n, ..., n).\n\n    Args:\n        n (int): The size, along each dimension of the arrays for which\n            the indices are to be returned.\n        ndim (int): The number of dimensions. default `2`.\n\n    Examples\n    --------\n    Create a set of indices to access the diagonal of a (4, 4) array:\n    >>> dig = cupy.diag_indices(4)\n    >>> dig\n    (array([0, 1, 2, 3]), array([0, 1, 2, 3]))\n    >>> a = cupy.arange(16).reshape(4, 4)\n    >>> a\n    array([[ 0,  1,  2,  3],\n           [ 4,  5,  6,  7],\n           [ 8,  9, 10, 11],\n           [12, 13, 14, 15]])\n    >>> a[di] = 100\n    >>> a\n    array([[100,   1,   2,   3],\n           [  4, 100,   6,   7],\n           [  8,   9, 100,  11],\n           [ 12,  13,  14, 100]])\n\n    Create indices to manipulate a 3-D array:\n    >>> d3 = cupy.diag_indices(2, 3)\n    >>> d3\n    (array([0, 1]), array([0, 1]), array([0, 1]))\n    And use it to set the diagonal of an array of zeros to 1:\n    >>> a = cupy.zeros((2, 2, 2), dtype=int)\n    >>> a[d3] = 1\n    >>> a\n    array([[[1, 0],\n            [0, 0]],\n           [[0, 0],\n            [0, 1]]])\n\n     .. seealso:: :func:`numpy.diag_indices`\n\n    """"""\n    idx = cupy.arange(n)\n    return (idx,) * ndim\n\n\ndef diag_indices_from(arr):\n    """"""\n    Return the indices to access the main diagonal of an n-dimensional array.\n    See `diag_indices` for full details.\n\n    Args:\n        arr (cupy.ndarray): At least 2-D.\n\n     .. seealso:: :func:`numpy.diag_indices_from`\n\n    """"""\n    if not isinstance(arr, cupy.ndarray):\n        raise TypeError(""Argument must be cupy.ndarray"")\n\n    if not arr.ndim >= 2:\n        raise ValueError(""input array must be at least 2-d"")\n    # For more than d=2, the strided formula is only valid for arrays with\n    # all dimensions equal, so we check first.\n    if not cupy.all(cupy.diff(arr.shape) == 0):\n        raise ValueError(""All dimensions of input must be of equal length"")\n\n    return diag_indices(arr.shape[0], arr.ndim)\n'"
cupy/indexing/iterate.py,0,"b'import numpy\n\nimport cupy\nfrom cupy import core\nfrom cupy.core import internal\n\n\nclass flatiter:\n    """"""Flat iterator object to iterate over arrays.\n\n    A flatiter iterator is returned by ``x.flat`` for any array ``x``. It\n    allows iterating over the array as if it were a 1-D array, either in a\n    for-loop or by calling its ``next`` method.\n\n    Iteration is done in row-major, C-style order (the last index varying the\n    fastest).\n\n    Attributes:\n        base (cupy.ndarray): A reference to the array that is iterated over.\n\n    .. note::\n       Restricted support of basic slicing is currently supplied. Advanced\n       indexing is not supported yet.\n\n    .. seealso:: :func:`numpy.flatiter`\n\n    """"""\n\n    def __init__(self, a):\n        self._base = a\n        self._index = 0\n\n    def __setitem__(self, ind, value):\n        if ind is Ellipsis:\n            self[:] = value\n            return\n\n        if isinstance(ind, tuple):\n            raise IndexError(\'unsupported iterator index\')\n\n        if isinstance(ind, bool):\n            raise IndexError(\'unsupported iterator index\')\n\n        if numpy.isscalar(ind):\n            ind = int(ind)\n            base = self._base\n            size = base.size\n            indices = []\n            for s in base.shape:\n                size = size // s\n                indices.append(ind // size)\n                ind %= size\n            base[tuple(indices)] = value\n            return\n\n        if isinstance(ind, slice):\n            base = self._base\n            s = internal.complete_slice(ind, base.size)\n            s_start = s.start\n            s_step = s.step\n            size = s.stop - s.start\n            if s_step > 0:\n                size = (size - 1) // s_step + 1\n            else:\n                size = (size + 1) // s_step + 1\n            value = cupy.asarray(value, dtype=base.dtype)\n            _flatiter_setitem_slice(value, s_start, s_step, base, size=size)\n            return\n\n        raise IndexError(\'unsupported iterator index\')\n\n    def __getitem__(self, ind):\n        if ind is Ellipsis:\n            return self[:]\n\n        if isinstance(ind, tuple):\n            raise IndexError(\'unsupported iterator index\')\n\n        if isinstance(ind, bool):\n            raise IndexError(\'unsupported iterator index\')\n\n        if numpy.isscalar(ind):\n            ind = int(ind)\n            base = self._base\n            size = base.size\n            indices = []\n            for s in base.shape:\n                size = size // s\n                indices.append(ind // size)\n                ind %= size\n            return base[tuple(indices)].copy()\n\n        if isinstance(ind, slice):\n            base = self._base\n            s = internal.complete_slice(ind, base.size)\n            s_start = s.start\n            s_step = s.step\n            size = s.stop - s.start\n            if s_step > 0:\n                size = (size - 1) // s_step + 1\n            else:\n                size = (size + 1) // s_step + 1\n            return _flatiter_getitem_slice(base, s_start, s_step, size=size)\n\n        raise IndexError(\'unsupported iterator index\')\n\n    # TODO(Takagi): Implement __iter__\n\n    def __next__(self):\n        index = self._index\n        if index == len(self):\n            raise StopIteration()\n        self._index += 1\n        return self[index]\n\n    # TODO(Takagi): Implement copy\n\n    @property\n    def base(self):\n        """"""A reference to the array that is iterate over.""""""\n        return self._base\n\n    # TODO(Takagi): Implement coords\n\n    # TODO(Takagi): Implement index\n\n    # TODO(Takagi): Implement __lt__\n\n    # TODO(Takagi): Implement __le__\n\n    # TODO(Takagi): Implement __eq__\n\n    # TODO(Takagi): Implement __ne__\n\n    # TODO(Takagi): Implement __ge__\n\n    # TODO(Takagi): Implement __gt__\n\n    def __len__(self):\n        return self.base.size\n\n\n_flatiter_setitem_slice = core.ElementwiseKernel(\n    \'raw T val, int64 start, int64 step\', \'raw T a\',\n    \'a[start + i * step] = val[i % val.size()]\',\n    \'cupy_flatiter_setitem_slice\')\n\n\n_flatiter_getitem_slice = core.ElementwiseKernel(\n    \'raw T a, int64 start, int64 step\', \'T o\',\n    \'o = a[start + i * step]\',\n    \'cupy_flatiter_getitem_slice\')\n'"
cupy/io/__init__.py,0,"b'# Functions from the following NumPy document\n# https://docs.scipy.org/doc/numpy/reference/routines.io.html\n\n# ""NOQA"" to suppress flake8 warning\nfrom cupy.io import formatting  # NOQA\nfrom cupy.io import npz  # NOQA\nfrom cupy.io import text  # NOQA\n'"
cupy/io/formatting.py,0,"b'import numpy\n\nimport cupy\n\n\ndef array_repr(arr, max_line_width=None, precision=None, suppress_small=None):\n    """"""Returns the string representation of an array.\n\n    Args:\n        arr (array_like): Input array. It should be able to feed to\n            :func:`cupy.asnumpy`.\n        max_line_width (int): The maximum number of line lengths.\n        precision (int): Floating point precision. It uses the current printing\n            precision of NumPy.\n        suppress_small (bool): If ``True``, very small numbers are printed as\n            zeros\n\n    Returns:\n        str: The string representation of ``arr``.\n\n    .. seealso:: :func:`numpy.array_repr`\n\n    """"""\n    return numpy.array_repr(cupy.asnumpy(arr), max_line_width, precision,\n                            suppress_small)\n\n\ndef array_str(arr, max_line_width=None, precision=None, suppress_small=None):\n    """"""Returns the string representation of the content of an array.\n\n    Args:\n        arr (array_like): Input array. It should be able to feed to\n            :func:`cupy.asnumpy`.\n        max_line_width (int): The maximum number of line lengths.\n        precision (int): Floating point precision. It uses the current printing\n            precision of NumPy.\n        suppress_small (bool): If ``True``, very small number are printed as\n            zeros.\n\n    .. seealso:: :func:`numpy.array_str`\n\n    """"""\n    return numpy.array_str(cupy.asnumpy(arr), max_line_width, precision,\n                           suppress_small)\n'"
cupy/io/npz.py,0,"b'import warnings\n\nimport numpy\n\nimport cupy\n\n\n_support_allow_pickle = (numpy.lib.NumpyVersion(numpy.__version__) >= \'1.10.0\')\n\n\nclass NpzFile(object):\n\n    def __init__(self, npz_file):\n        self.npz_file = npz_file\n\n    def __enter__(self):\n        self.npz_file.__enter__()\n        return self\n\n    def __exit__(self, typ, val, traceback):\n        self.npz_file.__exit__(typ, val, traceback)\n\n    def __getitem__(self, key):\n        arr = self.npz_file[key]\n        return cupy.array(arr)\n\n    def close(self):\n        self.npz_file.close()\n\n\ndef load(file, mmap_mode=None, allow_pickle=None):\n    """"""Loads arrays or pickled objects from ``.npy``, ``.npz`` or pickled file.\n\n    This function just calls ``numpy.load`` and then sends the arrays to the\n    current device. NPZ file is converted to NpzFile object, which defers the\n    transfer to the time of accessing the items.\n\n    Args:\n        file (file-like object or string): The file to read.\n        mmap_mode (None, \'r+\', \'r\', \'w+\', \'c\'): If not ``None``, memory-map the\n            file to construct an intermediate :class:`numpy.ndarray` object and\n            transfer it to the current device.\n        allow_pickle (bool): Allow loading pickled object arrays stored in npy\n            files. Reasons for disallowing pickles include security, as\n            loading pickled data can execute arbitrary code. If pickles are\n            disallowed, loading object arrays will fail.\n            Please be aware that CuPy does not support arrays with dtype of\n            `object`.\n            The default is False.\n            This option is available only for NumPy 1.10 or later.\n            In NumPy 1.9, this option cannot be specified (loading pickled\n            objects is always allowed).\n\n    Returns:\n        CuPy array or NpzFile object depending on the type of the file. NpzFile\n        object is a dictionary-like object with the context manager protocol\n        (which enables us to use *with* statement on it).\n\n    .. seealso:: :func:`numpy.load`\n\n    """"""\n    if _support_allow_pickle:\n        allow_pickle = False if allow_pickle is None else allow_pickle\n        obj = numpy.load(file, mmap_mode, allow_pickle)\n    else:\n        if allow_pickle is not None:\n            warnings.warn(\'allow_pickle option is not supported in NumPy 1.9\')\n        obj = numpy.load(file, mmap_mode)\n\n    if isinstance(obj, numpy.ndarray):\n        return cupy.array(obj)\n    elif isinstance(obj, numpy.lib.npyio.NpzFile):\n        return NpzFile(obj)\n    else:\n        return obj\n\n\ndef save(file, arr, allow_pickle=None):\n    """"""Saves an array to a binary file in ``.npy`` format.\n\n    Args:\n        file (file or str): File or filename to save.\n        arr (array_like): Array to save. It should be able to feed to\n            :func:`cupy.asnumpy`.\n        allow_pickle (bool): Allow saving object arrays using Python pickles.\n            Reasons for disallowing pickles include security (loading pickled\n            data can execute arbitrary code) and portability (pickled objects\n            may not be loadable on different Python installations, for example\n            if the stored objects require libraries that are not available,\n            and not all pickled data is compatible between Python 2 and Python\n            3).\n            The default is True.\n            This option is available only for NumPy 1.10 or later.\n            In NumPy 1.9, this option cannot be specified (saving objects\n            using pickles is always allowed).\n\n    .. seealso:: :func:`numpy.save`\n\n    """"""\n    if _support_allow_pickle:\n        allow_pickle = True if allow_pickle is None else allow_pickle\n        numpy.save(file, cupy.asnumpy(arr), allow_pickle)\n    else:\n        if allow_pickle is not None:\n            warnings.warn(\'allow_pickle option is not supported in NumPy 1.9\')\n        numpy.save(file, cupy.asnumpy(arr))\n\n\ndef savez(file, *args, **kwds):\n    """"""Saves one or more arrays into a file in uncompressed ``.npz`` format.\n\n    Arguments without keys are treated as arguments with automatic keys named\n    ``arr_0``, ``arr_1``, etc. corresponding to the positions in the argument\n    list. The keys of arguments are used as keys in the ``.npz`` file, which\n    are used for accessing NpzFile object when the file is read by\n    :func:`cupy.load` function.\n\n    Args:\n        file (file or str): File or filename to save.\n        *args: Arrays with implicit keys.\n        **kwds: Arrays with explicit keys.\n\n    .. seealso:: :func:`numpy.savez`\n\n    """"""\n    args = map(cupy.asnumpy, args)\n    for key in kwds:\n        kwds[key] = cupy.asnumpy(kwds[key])\n    numpy.savez(file, *args, **kwds)\n\n\ndef savez_compressed(file, *args, **kwds):\n    """"""Saves one or more arrays into a file in compressed ``.npz`` format.\n\n    It is equivalent to :func:`cupy.savez` function except the output file is\n    compressed.\n\n    .. seealso::\n       :func:`cupy.savez` for more detail,\n       :func:`numpy.savez_compressed`\n\n    """"""\n    args = map(cupy.asnumpy, args)\n    for key in kwds:\n        kwds[key] = cupy.asnumpy(kwds[key])\n    numpy.savez_compressed(file, *args, **kwds)\n'"
cupy/io/text.py,0,"b'# flake8: NOQA\n# ""flake8: NOQA"" to suppress warning ""H104  File contains nothing but comments""\n\n\n# TODO(okuta): Implement loadtxt\n\n\n# TODO(okuta): Implement savetxt\n\n\n# TODO(okuta): Implement genfromtxt\n\n\n# TODO(okuta): Implement fromregex\n\n\n# TODO(okuta): Implement fromstring\n'"
cupy/lib/__init__.py,0,b'from cupy.lib import stride_tricks  # NOQA\n'
cupy/lib/stride_tricks.py,0,"b'import cupy\n\n\ndef as_strided(x, shape=None, strides=None):\n    """"""\n    Create a view into the array with the given shape and strides.\n\n    .. warning:: This function has to be used with extreme care, see notes.\n\n    Parameters\n    ----------\n    x : ndarray\n        Array to create a new.\n    shape : sequence of int, optional\n        The shape of the new array. Defaults to ``x.shape``.\n    strides : sequence of int, optional\n        The strides of the new array. Defaults to ``x.strides``.\n\n    Returns\n    -------\n    view : ndarray\n\n    See also\n    --------\n    numpy.lib.stride_tricks.as_strided\n    reshape : reshape an array.\n\n    Notes\n    -----\n    ``as_strided`` creates a view into the array given the exact strides\n    and shape. This means it manipulates the internal data structure of\n    ndarray and, if done incorrectly, the array elements can point to\n    invalid memory and can corrupt results or crash your program.\n    """"""\n    shape = x.shape if shape is None else tuple(shape)\n    strides = x.strides if strides is None else tuple(strides)\n\n    return cupy.ndarray(shape=shape, dtype=x.dtype,\n                        memptr=x.data, strides=strides)\n'"
cupy/linalg/__init__.py,0,"b'# Functions from the following NumPy document\n# https://docs.scipy.org/doc/numpy/reference/routines.linalg.html\n\n# ""NOQA"" to suppress flake8 warning\nfrom cupy.linalg import decomposition  # NOQA\nfrom cupy.linalg import eigenvalue  # NOQA\nfrom cupy.linalg import einsum  # NOQA\nfrom cupy.linalg import norms  # NOQA\nfrom cupy.linalg.norms import det  # NOQA\nfrom cupy.linalg.norms import matrix_rank  # NOQA\nfrom cupy.linalg.norms import norm  # NOQA\nfrom cupy.linalg.norms import slogdet  # NOQA\nfrom cupy.linalg import product  # NOQA\nfrom cupy.linalg import solve  # NOQA\n\nfrom cupy.linalg.decomposition import cholesky  # NOQA\nfrom cupy.linalg.decomposition import qr  # NOQA\nfrom cupy.linalg.decomposition import svd  # NOQA\n\nfrom cupy.linalg.eigenvalue import eigh  # NOQA\nfrom cupy.linalg.eigenvalue import eigvalsh  # NOQA\n\nfrom cupy.linalg.solve import inv  # NOQA\nfrom cupy.linalg.solve import lstsq  # NOQA\nfrom cupy.linalg.solve import pinv  # NOQA\nfrom cupy.linalg.solve import solve  # NOQA\nfrom cupy.linalg.solve import tensorinv  # NOQA\nfrom cupy.linalg.solve import tensorsolve  # NOQA\n\nfrom cupy.linalg.product import matrix_power  # NOQA\n'"
cupy/linalg/decomposition.py,2,"b'import numpy\n\nimport cupy\nfrom cupy.cuda import cublas\nfrom cupy.cuda import cusolver\nfrom cupy.cuda import device\nfrom cupy.linalg import util\n\n\ndef _lu_factor(a_t, dtype):\n    """"""Compute pivoted LU decomposition.\n\n    Decompose a given batch of square matrices. Inputs and outputs are\n    transposed.\n\n    Args:\n        a_t (cupy.ndarray): The input matrix with dimension ``(..., N, N)``.\n            The dimension condition is not checked.\n        dtype (numpy.dtype): float32, float64, complex64, or complex128.\n\n    Returns:\n        lu_t (cupy.ndarray): ``L`` without its unit diagonal and ``U`` with\n            dimension ``(..., N, N)``.\n        piv (cupy.ndarray): 1-origin pivot indices with dimension\n            ``(..., N)``.\n        dev_info (cupy.ndarray): ``getrf`` info with dimension ``(...)``.\n\n    .. seealso:: :func:`scipy.linalg.lu_factor`\n\n    """"""\n    orig_shape = a_t.shape\n    n = orig_shape[-2]\n\n    # copy is necessary to present `a` to be overwritten.\n    a_t = a_t.astype(dtype, order=\'C\').reshape(-1, n, n)\n    batch_size = a_t.shape[0]\n    ipiv = cupy.empty((batch_size, n), dtype=numpy.int32)\n    dev_info = cupy.empty((batch_size,), dtype=numpy.int32)\n\n    # Heuristic condition from some performance test.\n    # TODO(kataoka): autotune\n    use_batched = batch_size * 65536 >= n * n\n\n    if use_batched:\n        handle = device.get_cublas_handle()\n        lda = n\n        step = n * lda * a_t.itemsize\n        start = a_t.data.ptr\n        stop = start + step * batch_size\n        a_array = cupy.arange(start, stop, step, dtype=cupy.uintp)\n\n        if dtype == numpy.float32:\n            getrfBatched = cupy.cuda.cublas.sgetrfBatched\n        elif dtype == numpy.float64:\n            getrfBatched = cupy.cuda.cublas.dgetrfBatched\n        elif dtype == numpy.complex64:\n            getrfBatched = cupy.cuda.cublas.cgetrfBatched\n        elif dtype == numpy.complex128:\n            getrfBatched = cupy.cuda.cublas.zgetrfBatched\n        else:\n            assert False\n\n        getrfBatched(\n            handle, n, a_array.data.ptr, lda, ipiv.data.ptr,\n            dev_info.data.ptr, batch_size)\n\n    else:\n        handle = device.get_cusolver_handle()\n        if dtype == numpy.float32:\n            getrf_bufferSize = cusolver.sgetrf_bufferSize\n            getrf = cusolver.sgetrf\n        elif dtype == numpy.float64:\n            getrf_bufferSize = cusolver.dgetrf_bufferSize\n            getrf = cusolver.dgetrf\n        elif dtype == numpy.complex64:\n            getrfBatched = cupy.cuda.cublas.cgetrfBatched\n        elif dtype == numpy.complex128:\n            getrfBatched = cupy.cuda.cublas.zgetrfBatched\n        else:\n            assert False\n\n        for i in range(batch_size):\n            a_ptr = a_t[i].data.ptr\n            buffersize = getrf_bufferSize(handle, n, n, a_ptr, n)\n            workspace = cupy.empty(buffersize, dtype=dtype)\n            getrf(\n                handle, n, n, a_ptr, n, workspace.data.ptr,\n                ipiv[i].data.ptr, dev_info[i].data.ptr)\n\n    return (\n        a_t.reshape(orig_shape),\n        ipiv.reshape(orig_shape[:-1]),\n        dev_info.reshape(orig_shape[:-2]),\n    )\n\n\ndef cholesky(a):\n    """"""Cholesky decomposition.\n\n    Decompose a given two-dimensional square matrix into ``L * L.T``,\n    where ``L`` is a lower-triangular matrix and ``.T`` is a conjugate\n    transpose operator.\n\n    Args:\n        a (cupy.ndarray): The input matrix with dimension ``(N, N)``\n\n    Returns:\n        cupy.ndarray: The lower-triangular matrix.\n\n    .. warning::\n        This function calls one or more cuSOLVER routine(s) which may yield\n        invalid results if input conditions are not met.\n        To detect these invalid results, you can set the `linalg`\n        configuration to a value that is not `ignore` in\n        :func:`cupyx.errstate` or :func:`cupyx.seterr`.\n\n    .. seealso:: :func:`numpy.linalg.cholesky`\n    """"""\n    # TODO(Saito): Current implementation only accepts two-dimensional arrays\n    util._assert_cupy_array(a)\n    util._assert_rank2(a)\n    util._assert_nd_squareness(a)\n\n    if a.dtype.char == \'f\' or a.dtype.char == \'d\':\n        dtype = a.dtype.char\n    else:\n        dtype = numpy.promote_types(a.dtype.char, \'f\').char\n\n    x = a.astype(dtype, order=\'C\', copy=True)\n    n = len(a)\n    handle = device.get_cusolver_handle()\n    dev_info = cupy.empty(1, dtype=numpy.int32)\n\n    if dtype == \'f\':\n        potrf = cusolver.spotrf\n        potrf_bufferSize = cusolver.spotrf_bufferSize\n    elif dtype == \'d\':\n        potrf = cusolver.dpotrf\n        potrf_bufferSize = cusolver.dpotrf_bufferSize\n    elif dtype == \'F\':\n        potrf = cusolver.cpotrf\n        potrf_bufferSize = cusolver.cpotrf_bufferSize\n    else:  # dtype == \'D\':\n        potrf = cusolver.zpotrf\n        potrf_bufferSize = cusolver.zpotrf_bufferSize\n\n    buffersize = potrf_bufferSize(\n        handle, cublas.CUBLAS_FILL_MODE_UPPER, n, x.data.ptr, n)\n    workspace = cupy.empty(buffersize, dtype=dtype)\n    potrf(\n        handle, cublas.CUBLAS_FILL_MODE_UPPER, n, x.data.ptr, n,\n        workspace.data.ptr, buffersize, dev_info.data.ptr)\n    cupy.linalg.util._check_cusolver_dev_info_if_synchronization_allowed(\n        potrf, dev_info)\n\n    util._tril(x, k=0)\n    return x\n\n\ndef qr(a, mode=\'reduced\'):\n    """"""QR decomposition.\n\n    Decompose a given two-dimensional matrix into ``Q * R``, where ``Q``\n    is an orthonormal and ``R`` is an upper-triangular matrix.\n\n    Args:\n        a (cupy.ndarray): The input matrix.\n        mode (str): The mode of decomposition. Currently \'reduced\',\n            \'complete\', \'r\', and \'raw\' modes are supported. The default mode\n            is \'reduced\', in which matrix ``A = (M, N)`` is decomposed into\n            ``Q``, ``R`` with dimensions ``(M, K)``, ``(K, N)``, where\n            ``K = min(M, N)``.\n\n    Returns:\n        cupy.ndarray, or tuple of ndarray:\n            Although the type of returned object depends on the mode,\n            it returns a tuple of ``(Q, R)`` by default.\n            For details, please see the document of :func:`numpy.linalg.qr`.\n\n    .. warning::\n        This function calls one or more cuSOLVER routine(s) which may yield\n        invalid results if input conditions are not met.\n        To detect these invalid results, you can set the `linalg`\n        configuration to a value that is not `ignore` in\n        :func:`cupyx.errstate` or :func:`cupyx.seterr`.\n\n    .. seealso:: :func:`numpy.linalg.qr`\n    """"""\n    # TODO(Saito): Current implementation only accepts two-dimensional arrays\n    util._assert_cupy_array(a)\n    util._assert_rank2(a)\n\n    if mode not in (\'reduced\', \'complete\', \'r\', \'raw\'):\n        if mode in (\'f\', \'full\', \'e\', \'economic\'):\n            msg = \'The deprecated mode \\\'{}\\\' is not supported\'.format(mode)\n            raise ValueError(msg)\n        else:\n            raise ValueError(\'Unrecognized mode \\\'{}\\\'\'.format(mode))\n\n    # support float32, float64, complex64, and complex128\n    if a.dtype.char in \'fdFD\':\n        dtype = a.dtype.char\n    else:\n        dtype = numpy.promote_types(a.dtype.char, \'f\').char\n\n    m, n = a.shape\n    mn = min(m, n)\n    if mn == 0:\n        if mode == \'reduced\':\n            return cupy.empty((m, 0), dtype), cupy.empty((0, n), dtype)\n        elif mode == \'complete\':\n            return cupy.identity(m, dtype), cupy.empty((m, n), dtype)\n        elif mode == \'r\':\n            return cupy.empty((0, n), dtype)\n        else:  # mode == \'raw\'\n            # compatibility with numpy.linalg.qr\n            dtype = numpy.promote_types(dtype, \'d\')\n            return cupy.empty((n, m), dtype), cupy.empty((0,), dtype)\n\n    x = a.transpose().astype(dtype, order=\'C\', copy=True)\n    handle = device.get_cusolver_handle()\n    dev_info = cupy.empty(1, dtype=numpy.int32)\n\n    if dtype == \'f\':\n        geqrf_bufferSize = cusolver.sgeqrf_bufferSize\n        geqrf = cusolver.sgeqrf\n    elif dtype == \'d\':\n        geqrf_bufferSize = cusolver.dgeqrf_bufferSize\n        geqrf = cusolver.dgeqrf\n    elif dtype == \'F\':\n        geqrf_bufferSize = cusolver.cgeqrf_bufferSize\n        geqrf = cusolver.cgeqrf\n    elif dtype == \'D\':\n        geqrf_bufferSize = cusolver.zgeqrf_bufferSize\n        geqrf = cusolver.zgeqrf\n    else:\n        msg = (\'dtype must be float32, float64, complex64 or complex128\'\n               \' (actual: {})\'.format(a.dtype))\n        raise ValueError(msg)\n\n    # compute working space of geqrf and solve R\n    buffersize = geqrf_bufferSize(handle, m, n, x.data.ptr, n)\n    workspace = cupy.empty(buffersize, dtype=dtype)\n    tau = cupy.empty(mn, dtype=dtype)\n    geqrf(handle, m, n, x.data.ptr, m,\n          tau.data.ptr, workspace.data.ptr, buffersize, dev_info.data.ptr)\n    cupy.linalg.util._check_cusolver_dev_info_if_synchronization_allowed(\n        geqrf, dev_info)\n\n    if mode == \'r\':\n        r = x[:, :mn].transpose()\n        return util._triu(r)\n\n    if mode == \'raw\':\n        if a.dtype.char == \'f\':\n            # The original numpy.linalg.qr returns float64 in raw mode,\n            # whereas the cusolver returns float32. We agree that the\n            # following code would be inappropriate, however, in this time\n            # we explicitly convert them to float64 for compatibility.\n            return x.astype(numpy.float64), tau.astype(numpy.float64)\n        elif a.dtype.char == \'F\':\n            # The same applies to complex64\n            return x.astype(numpy.complex128), tau.astype(numpy.complex128)\n        return x, tau\n\n    if mode == \'complete\' and m > n:\n        mc = m\n        q = cupy.empty((m, m), dtype)\n    else:\n        mc = mn\n        q = cupy.empty((n, m), dtype)\n    q[:n] = x\n\n    # compute working space of orgqr and solve Q\n    if dtype == \'f\':\n        orgqr_bufferSize = cusolver.sorgqr_bufferSize\n        orgqr = cusolver.sorgqr\n    elif dtype == \'d\':\n        orgqr_bufferSize = cusolver.dorgqr_bufferSize\n        orgqr = cusolver.dorgqr\n    elif dtype == \'F\':\n        orgqr_bufferSize = cusolver.cungqr_bufferSize\n        orgqr = cusolver.cungqr\n    elif dtype == \'D\':\n        orgqr_bufferSize = cusolver.zungqr_bufferSize\n        orgqr = cusolver.zungqr\n\n    buffersize = orgqr_bufferSize(\n        handle, m, mc, mn, q.data.ptr, m, tau.data.ptr)\n    workspace = cupy.empty(buffersize, dtype=dtype)\n    orgqr(\n        handle, m, mc, mn, q.data.ptr, m, tau.data.ptr, workspace.data.ptr,\n        buffersize, dev_info.data.ptr)\n    cupy.linalg.util._check_cusolver_dev_info_if_synchronization_allowed(\n        orgqr, dev_info)\n\n    q = q[:mc].transpose()\n    r = x[:, :mc].transpose()\n    return q, util._triu(r)\n\n\ndef svd(a, full_matrices=True, compute_uv=True):\n    """"""Singular Value Decomposition.\n\n    Factorizes the matrix ``a`` as ``u * np.diag(s) * v``, where ``u`` and\n    ``v`` are unitary and ``s`` is an one-dimensional array of ``a``\'s\n    singular values.\n\n    Args:\n        a (cupy.ndarray): The input matrix with dimension ``(M, N)``.\n        full_matrices (bool): If True, it returns u and v with dimensions\n            ``(M, M)`` and ``(N, N)``. Otherwise, the dimensions of u and v\n            are respectively ``(M, K)`` and ``(K, N)``, where\n            ``K = min(M, N)``.\n        compute_uv (bool): If ``False``, it only returns singular values.\n\n    Returns:\n        tuple of :class:`cupy.ndarray`:\n            A tuple of ``(u, s, v)`` such that ``a = u * np.diag(s) * v``.\n\n    .. warning::\n        This function calls one or more cuSOLVER routine(s) which may yield\n        invalid results if input conditions are not met.\n        To detect these invalid results, you can set the `linalg`\n        configuration to a value that is not `ignore` in\n        :func:`cupyx.errstate` or :func:`cupyx.seterr`.\n\n    .. seealso:: :func:`numpy.linalg.svd`\n    """"""\n    # TODO(Saito): Current implementation only accepts two-dimensional arrays\n    util._assert_cupy_array(a)\n    util._assert_rank2(a)\n\n    # Cast to float32 or float64\n    a_dtype = numpy.promote_types(a.dtype.char, \'f\').char\n    if a_dtype == \'f\':\n        s_dtype = \'f\'\n    elif a_dtype == \'d\':\n        s_dtype = \'d\'\n    elif a_dtype == \'F\':\n        s_dtype = \'f\'\n    else:  # a_dtype == \'D\':\n        a_dtype = \'D\'\n        s_dtype = \'d\'\n\n    # Remark 1: gesvd only supports m >= n (WHAT?)\n    # Remark 2: gesvd only supports jobu = \'A\' and jobvt = \'A\'\n    # Remark 3: gesvd returns matrix U and V^H\n    # Remark 4: Remark 2 is removed since cuda 8.0 (new!)\n    n, m = a.shape\n\n    if m == 0 or n == 0:\n        s = cupy.empty((0,), s_dtype)\n        if compute_uv:\n            if full_matrices:\n                u = cupy.eye(n, dtype=a_dtype)\n                vt = cupy.eye(m, dtype=a_dtype)\n            else:\n                u = cupy.empty((n, 0), dtype=a_dtype)\n                vt = cupy.empty((0, m), dtype=a_dtype)\n            return u, s, vt\n        else:\n            return s\n\n    # `a` must be copied because xgesvd destroys the matrix\n    if m >= n:\n        x = a.astype(a_dtype, order=\'C\', copy=True)\n        trans_flag = False\n    else:\n        m, n = a.shape\n        x = a.transpose().astype(a_dtype, order=\'C\', copy=True)\n        trans_flag = True\n\n    k = n  # = min(m, n) where m >= n is ensured above\n    if compute_uv:\n        if full_matrices:\n            u = cupy.empty((m, m), dtype=a_dtype)\n            vt = x[:, :n]\n            job_u = ord(\'A\')\n            job_vt = ord(\'O\')\n        else:\n            u = x\n            vt = cupy.empty((k, n), dtype=a_dtype)\n            job_u = ord(\'O\')\n            job_vt = ord(\'S\')\n        u_ptr, vt_ptr = u.data.ptr, vt.data.ptr\n    else:\n        u_ptr, vt_ptr = 0, 0  # Use nullptr\n        job_u = ord(\'N\')\n        job_vt = ord(\'N\')\n    s = cupy.empty(k, dtype=s_dtype)\n    handle = device.get_cusolver_handle()\n    dev_info = cupy.empty(1, dtype=numpy.int32)\n\n    if a_dtype == \'f\':\n        gesvd = cusolver.sgesvd\n        gesvd_bufferSize = cusolver.sgesvd_bufferSize\n    elif a_dtype == \'d\':\n        gesvd = cusolver.dgesvd\n        gesvd_bufferSize = cusolver.dgesvd_bufferSize\n    elif a_dtype == \'F\':\n        gesvd = cusolver.cgesvd\n        gesvd_bufferSize = cusolver.cgesvd_bufferSize\n    else:  # a_dtype == \'D\':\n        gesvd = cusolver.zgesvd\n        gesvd_bufferSize = cusolver.zgesvd_bufferSize\n\n    buffersize = gesvd_bufferSize(handle, m, n)\n    workspace = cupy.empty(buffersize, dtype=a_dtype)\n    gesvd(\n        handle, job_u, job_vt, m, n, x.data.ptr, m, s.data.ptr, u_ptr, m,\n        vt_ptr, n, workspace.data.ptr, buffersize, 0, dev_info.data.ptr)\n    cupy.linalg.util._check_cusolver_dev_info_if_synchronization_allowed(\n        gesvd, dev_info)\n\n    # Note that the returned array may need to be transposed\n    # depending on the structure of an input\n    if compute_uv:\n        if trans_flag:\n            return u.transpose(), s, vt.transpose()\n        else:\n            return vt, s, u\n    else:\n        return s\n'"
cupy/linalg/eigenvalue.py,0,"b'import numpy\n\nimport cupy\nfrom cupy.cuda import cublas\nfrom cupy.cuda import cusolver\nfrom cupy.cuda import device\n\n\ndef _syevd(a, UPLO, with_eigen_vector):\n    if UPLO not in (\'L\', \'U\'):\n        raise ValueError(\'UPLO argument must be \\\'L\\\' or \\\'U\\\'\')\n\n    if a.dtype == \'f\' or a.dtype == \'e\':\n        dtype = \'f\'\n        inp_w_dtype = \'f\'\n        inp_v_dtype = \'f\'\n        ret_w_dtype = a.dtype\n        ret_v_dtype = a.dtype\n    elif a.dtype == \'d\':\n        dtype = \'d\'\n        inp_w_dtype = \'d\'\n        inp_v_dtype = \'d\'\n        ret_w_dtype = \'d\'\n        ret_v_dtype = \'d\'\n    elif a.dtype == \'F\':\n        dtype = \'F\'\n        inp_w_dtype = \'f\'\n        inp_v_dtype = \'F\'\n        ret_w_dtype = \'f\'\n        ret_v_dtype = \'F\'\n    elif a.dtype == \'D\':\n        dtype = \'D\'\n        inp_w_dtype = \'d\'\n        inp_v_dtype = \'D\'\n        ret_w_dtype = \'d\'\n        ret_v_dtype = \'D\'\n    else:\n        # NumPy uses float64 when an input is not floating point number.\n        dtype = \'d\'\n        inp_w_dtype = \'d\'\n        inp_v_dtype = \'d\'\n        ret_w_dtype = \'d\'\n        ret_v_dtype = \'d\'\n\n    # Note that cuSolver assumes fortran array\n    v = a.astype(inp_v_dtype, order=\'F\', copy=True)\n\n    m, lda = a.shape\n    w = cupy.empty(m, inp_w_dtype)\n    dev_info = cupy.empty((), numpy.int32)\n    handle = device.Device().cusolver_handle\n\n    if with_eigen_vector:\n        jobz = cusolver.CUSOLVER_EIG_MODE_VECTOR\n    else:\n        jobz = cusolver.CUSOLVER_EIG_MODE_NOVECTOR\n\n    if UPLO == \'L\':\n        uplo = cublas.CUBLAS_FILL_MODE_LOWER\n    else:  # UPLO == \'U\'\n        uplo = cublas.CUBLAS_FILL_MODE_UPPER\n\n    if dtype == \'f\':\n        buffer_size = cupy.cuda.cusolver.ssyevd_bufferSize\n        syevd = cupy.cuda.cusolver.ssyevd\n    elif dtype == \'d\':\n        buffer_size = cupy.cuda.cusolver.dsyevd_bufferSize\n        syevd = cupy.cuda.cusolver.dsyevd\n    elif dtype == \'F\':\n        buffer_size = cupy.cuda.cusolver.cheevd_bufferSize\n        syevd = cupy.cuda.cusolver.cheevd\n    elif dtype == \'D\':\n        buffer_size = cupy.cuda.cusolver.zheevd_bufferSize\n        syevd = cupy.cuda.cusolver.zheevd\n    else:\n        raise RuntimeError(\'Only float and double and cuComplex and \'\n                           + \'cuDoubleComplex are supported\')\n\n    work_size = buffer_size(\n        handle, jobz, uplo, m, v.data.ptr, lda, w.data.ptr)\n    work = cupy.empty(work_size, inp_v_dtype)\n    syevd(\n        handle, jobz, uplo, m, v.data.ptr, lda,\n        w.data.ptr, work.data.ptr, work_size, dev_info.data.ptr)\n    cupy.linalg.util._check_cusolver_dev_info_if_synchronization_allowed(\n        syevd, dev_info)\n\n    return w.astype(ret_w_dtype, copy=False), v.astype(ret_v_dtype, copy=False)\n\n\n# TODO(okuta): Implement eig\n\n\ndef eigh(a, UPLO=\'L\'):\n    """"""Eigenvalues and eigenvectors of a symmetric matrix.\n\n    This method calculates eigenvalues and eigenvectors of a given\n    symmetric matrix.\n\n    .. note::\n\n       Currently only 2-D matrix is supported.\n\n    Args:\n        a (cupy.ndarray): A symmetric 2-D square matrix.\n        UPLO (str): Select from ``\'L\'`` or ``\'U\'``. It specifies which\n            part of ``a`` is used. ``\'L\'`` uses the lower triangular part of\n            ``a``, and ``\'U\'`` uses the upper triangular part of ``a``.\n    Returns:\n        tuple of :class:`~cupy.ndarray`:\n            Returns a tuple ``(w, v)``. ``w`` contains eigenvalues and\n            ``v`` contains eigenvectors. ``v[:, i]`` is an eigenvector\n            corresponding to an eigenvalue ``w[i]``.\n\n    .. warning::\n        This function calls one or more cuSOLVER routine(s) which may yield\n        invalid results if input conditions are not met.\n        To detect these invalid results, you can set the `linalg`\n        configuration to a value that is not `ignore` in\n        :func:`cupyx.errstate` or :func:`cupyx.seterr`.\n\n    .. seealso:: :func:`numpy.linalg.eigh`\n    """"""\n    return _syevd(a, UPLO, True)\n\n\n# TODO(okuta): Implement eigvals\n\n\ndef eigvalsh(a, UPLO=\'L\'):\n    """"""Calculates eigenvalues of a symmetric matrix.\n\n    This method calculates eigenvalues a given symmetric matrix.\n    Note that :func:`cupy.linalg.eigh` calculates both eigenvalues and\n    eigenvectors.\n\n    .. note::\n\n       Currenlty only 2-D matrix is supported.\n\n    Args:\n        a (cupy.ndarray): A symmetric 2-D square matrix.\n        UPLO (str): Select from ``\'L\'`` or ``\'U\'``. It specifies which\n            part of ``a`` is used. ``\'L\'`` uses the lower triangular part of\n            ``a``, and ``\'U\'`` uses the upper triangular part of ``a``.\n    Returns:\n        cupy.ndarray:\n            Returns eigenvalues as a vector.\n\n    .. warning::\n        This function calls one or more cuSOLVER routine(s) which may yield\n        invalid results if input conditions are not met.\n        To detect these invalid results, you can set the `linalg`\n        configuration to a value that is not `ignore` in\n        :func:`cupyx.errstate` or :func:`cupyx.seterr`.\n\n    .. seealso:: :func:`numpy.linalg.eigvalsh`\n    """"""\n    return _syevd(a, UPLO, False)[0]\n'"
cupy/linalg/einsum.py,2,"b'import copy\nimport itertools\nimport string\nimport warnings\n\nimport cupy\nfrom cupy import util\nfrom cupy.linalg.einsum_opt import _greedy_path\nfrom cupy.linalg.einsum_opt import _optimal_path\nif cupy.cuda.cutensor_enabled:\n    from cupy import cutensor\n\n\noptions = {\n    \'sum_ellipsis\': False,\n    \'broadcast_diagonal\': False,\n}\n\n\neinsum_symbols = string.ascii_uppercase + string.ascii_lowercase\n\n\ndef _transpose_ex(a, axeses):\n    """"""Transpose and diagonal\n\n    Args:\n        a\n        axeses (sequence of sequences of ints)\n\n    Returns:\n        p: a with its axes permutated. A writeable view is returned whenever\n            possible.\n    """"""\n\n    shape = []\n    strides = []\n    for axes in axeses:\n        shape.append(a.shape[axes[0]] if axes else 1)\n        stride = sum(a.strides[axis] for axis in axes)\n        strides.append(stride)\n    a = a.view()\n    # TODO(niboshi): Confirm update_x_contiguity flags\n    a._set_shape_and_strides(shape, strides, True, True)\n    return a\n\n\ndef _parse_int_subscript(list_subscript):\n    str_subscript = \'\'\n    for s in list_subscript:\n        if s is Ellipsis:\n            str_subscript += \'@\'\n        elif isinstance(s, int):\n            str_subscript += einsum_symbols[s]\n        else:\n            raise ValueError(\n                \'each subscript must be either an integer or an ellipsis\'\n                \' to provide subscripts strings as lists\')\n    return str_subscript\n\n\ndef _parse_einsum_input(args):\n    """"""Parse einsum operands.\n\n    This function is based on `numpy.core.einsumfunc._parse_einsum_input`\n    function in NumPy 1.14.\n\n    Parameters\n    ----------\n    args : tuple\n        The non-keyword arguments to einsum\n\n    Returns\n    -------\n    input_strings : str\n        Parsed input strings\n    output_string : str\n        Parsed output string\n    operands : list of array_like\n        The operands to use in the contraction\n\n    Examples\n    --------\n    The operand list is simplified to reduce printing:\n\n    >>> a = np.random.rand(4, 4)\n    >>> b = np.random.rand(4, 4, 4)\n    >>> _parse_einsum_input((\'...a,...a->...\', a, b))\n    ([\'@a, @a\'], \'xz\', [a, b])\n\n    >>> _parse_einsum_input((a, [Ellipsis, 0], b, [Ellipsis, 0]))\n    ([\'@a, @a\'], \'xz\', [a, b])\n    """"""\n\n    if len(args) == 0:\n        raise ValueError(\n            \'must specify the einstein sum subscripts string and at least one \'\n            \'operand, or at least one operand and its corresponding \'\n            \'subscripts list\')\n\n    if isinstance(args[0], str):\n        subscripts = args[0]\n        operands = list(args[1:])\n\n        # Ensure all characters are valid\n        for s in subscripts:\n            if s in \'.,-> \':\n                continue\n            if s not in einsum_symbols:\n                raise ValueError(\n                    \'invalid subscript \\\'%s\\\' in einstein sum subscripts \'\n                    \'string, subscripts must be letters\' % s)\n\n        # Parse \'...\'\n        subscripts = subscripts.replace(\'...\', \'@\')\n        if \'.\' in subscripts:\n            raise ValueError(\n                \'einstein sum subscripts string contains a \\\'.\\\' that is not \'\n                \'part of an ellipsis (\\\'...\\\')\')\n\n        # Parse \'->\'\n        if (\'-\' in subscripts) or (\'>\' in subscripts):\n            # Check for proper \'->\'\n            invalid = subscripts.count(\'-\') > 1 or subscripts.count(\'>\') > 1\n            subscripts = subscripts.split(\'->\')\n            if invalid or len(subscripts) != 2:\n                raise ValueError(\n                    \'einstein sum subscript string does not contain proper \'\n                    \'\\\'->\\\' output specified\')\n            input_subscripts, output_subscript = subscripts\n            output_subscript = output_subscript.replace(\' \', \'\')\n\n        else:\n            input_subscripts = subscripts\n            output_subscript = None\n\n        input_subscripts = input_subscripts.replace(\' \', \'\').split(\',\')\n        if len(input_subscripts) != len(operands):\n            msg = \'more\' if len(operands) > len(input_subscripts) else \'fewer\'\n            raise ValueError(\n                msg + \' operands provided to einstein sum function than \'\n                \'specified in the subscripts string\')\n\n    else:\n        args = list(args)\n        operands = []\n        input_subscripts = []\n        while len(args) >= 2:\n            operands.append(args.pop(0))\n            input_subscripts.append(_parse_int_subscript(args.pop(0)))\n        if args:\n            output_subscript = _parse_int_subscript(args[0])\n        else:\n            output_subscript = None\n\n    return input_subscripts, output_subscript, operands\n\n\ndef _chr(label):\n    if label < 0:\n        return \'...[%d]\' % label\n    else:\n        return chr(label)\n\n\ndef _parse_ellipsis_subscript(subscript, idx, ndim=None, ellipsis_len=None):\n    """"""Parse a subscript that may contain ellipsis\n\n    Args:\n        subscript (str): An einsum subscript of an operand or an output. \'...\'\n            should be replaced by \'@\'.\n        idx (int or None): For error messages, give int idx for the idx-th\n            operand or None for the output.\n        ndim (int, optional): ndim of the operand\n        ellipsis_len (int, optional): number of broadcast dimensions of the\n            output.\n\n    Returns:\n        list of ints: The parsed subscript\n\n    """"""\n    subs = subscript.split(\'@\')\n    if len(subs) == 1:\n        sub, = subs\n        if ndim is not None and len(sub) != ndim:\n            if len(sub) > ndim:\n                raise ValueError(\n                    \'einstein sum subscripts string %s contains too many \'\n                    \'subscripts for operand %d\' % (sub, idx))\n            raise ValueError(\n                \'operand %d has more dimensions than subscripts string %s \'\n                \'given in einstein sum, but no \\\'...\\\' ellipsis provided to \'\n                \'broadcast the extra dimensions.\' % (idx, sub))\n        return [ord(label) for label in sub]\n    elif len(subs) == 2:\n        left_sub, right_sub = subs\n        if ndim is not None:\n            ellipsis_len = ndim - (len(left_sub) + len(right_sub))\n        if ellipsis_len < 0:\n            raise ValueError(\n                \'einstein sum subscripts string %s...%s contains too many \'\n                \'subscripts for operand %d\' % (left_sub, right_sub, idx))\n        ret = []\n        ret.extend(ord(label) for label in left_sub)\n        ret.extend(range(-ellipsis_len, 0))\n        ret.extend(ord(label) for label in right_sub)\n        return ret\n    else:\n        # >= 2 ellipses for an operand\n        raise ValueError(\n            \'einstein sum subscripts string contains a \\\'.\\\' that is not \'\n            \'part of an ellipsis (\\\'...\\\') \' +\n            (\'in the output\' if idx is None else \'for operand %d\' % idx))\n\n\ndef _einsum_diagonals(input_subscripts, operands):\n    """"""Compute diagonal for each operand\n\n    This function mutates args.\n    """"""\n    for idx in range(len(input_subscripts)):\n        sub = input_subscripts[idx]\n        arr = operands[idx]\n\n        if len(set(sub)) < len(sub):\n            axeses = {}\n            for axis, label in enumerate(sub):\n                axeses.setdefault(label, []).append(axis)\n\n            axeses = list(axeses.items())\n\n            for label, axes in axeses:\n                if options[\'broadcast_diagonal\']:\n                    axes = [axis for axis in axes if arr.shape[axis] != 1]\n                dims = {arr.shape[axis] for axis in axes}\n                if len(dims) >= 2:\n                    dim0 = dims.pop()\n                    dim1 = dims.pop()\n                    raise ValueError(\n                        \'dimensions in operand %d\'\n                        \' for collapsing index \\\'%s\\\' don\\\'t match (%d != %d)\'\n                        % (idx, _chr(label), dim0, dim1)\n                    )\n\n            sub, axeses = zip(*axeses)  # axeses is not empty\n            input_subscripts[idx] = list(sub)\n            operands[idx] = _transpose_ex(arr, axeses)\n\n\ndef _iter_path_pairs(path):\n    """"""Decompose path into binary path\n\n    Args:\n        path (sequence of tuples of ints)\n\n    Yields:\n        tuple of ints: pair (idx0, idx1) that represents the operation\n            {pop(idx0); pop(idx1); append();}\n    """"""\n\n    for indices in path:\n        assert all(idx >= 0 for idx in indices)\n        # [3, 1, 4, 9] -> [(9, 4), (-1, 3), (-1, 1)]\n        if len(indices) >= 2:\n            indices = sorted(indices, reverse=True)\n            yield indices[0], indices[1]\n            for idx in indices[2:]:\n                yield -1, idx\n\n\ndef _flatten_transpose(a, axeses):\n    """"""Transpose and flatten each\n\n    Args:\n        a\n        axeses (sequence of sequences of ints)\n\n    Returns:\n        aT: a with its axes permutated and flatten\n        shapes: flattened shapes\n    """"""\n\n    transpose_axes = []\n    shapes = []\n    for axes in axeses:\n        transpose_axes.extend(axes)\n        shapes.append([a.shape[axis] for axis in axes])\n    return (\n        a.transpose(transpose_axes).reshape(\n            tuple([cupy.core.internal.prod(shape) for shape in shapes])),\n        shapes\n    )\n\n\ndef _use_cutensor(dtype0, sub0, dtype1, sub1, batch_dims, contract_dims):\n    if not cupy.cuda.cutensor_enabled:\n        return False\n    if dtype0 != dtype1:\n        return False\n    if dtype0 not in (cupy.float32, cupy.float64,\n                      cupy.complex64, cupy.complex128):\n        return False\n    if (len(contract_dims) >= 1 and (sub0[-1] in batch_dims or\n                                     sub1[-1] in batch_dims)):\n        return False\n    return True\n\n\ndef _get_out_shape(shape0, sub0, shape1, sub1, sub_out):\n    extent = {}\n    for size, i in zip(shape0 + shape1, sub0 + sub1):\n        extent[i] = size\n    out_shape = [extent[i] for i in sub_out]\n    return out_shape\n\n\ndef _expand_dims_transpose(arr, mode, mode_out):\n    """"""Return a reshaped and transposed array.\n\n    The input array ``arr`` having ``mode`` as its modes is reshaped and\n    transposed so that modes of the output becomes ``mode_out``.\n\n    Example\n        >>> import cupy\n        >>> a = cupy.zeros((10, 20))\n        >>> mode_a = (\'A\', \'B\')\n        >>> mode_out = (\'B\', \'C\', \'A\')\n        >>> out = cupy.linalg.einsum._expand_dims_transpose(a, mode_a,\n        ...                                                 mode_out)\n        >>> out.shape\n        (20, 1, 10)\n\n    Args:\n        arr (cupy.ndarray):\n        mode (tuple or list): The modes of input array.\n        mode_out (tuple or list): The modes of output array.\n\n    Returns:\n        cupy.ndarray: The reshaped and transposed array.\n\n    """"""\n    mode = list(mode)\n    shape = list(arr.shape)\n    axes = []\n    for i in mode_out:\n        if i not in mode:\n            mode.append(i)\n            shape.append(1)\n        axes.append(mode.index(i))\n    return cupy.transpose(arr.reshape(shape), axes)\n\n\ndef reduced_binary_einsum(arr0, sub0, arr1, sub1, sub_others):\n    set0 = set(sub0)\n    set1 = set(sub1)\n    assert len(set0) == len(sub0), \'operand 0 should be reduced: diagonal\'\n    assert len(set1) == len(sub1), \'operand 1 should be reduced: diagonal\'\n\n    if len(sub0) == 0 or len(sub1) == 0:\n        return arr0 * arr1, sub0 + sub1\n\n    set_others = set(sub_others)\n    shared = set0 & set1\n    batch_dims = shared & set_others\n    contract_dims = shared - batch_dims\n\n    bs0, cs0, ts0 = _make_transpose_axes(sub0, batch_dims, contract_dims)\n    bs1, cs1, ts1 = _make_transpose_axes(sub1, batch_dims, contract_dims)\n\n    sub_b = [sub0[axis] for axis in bs0]\n    assert sub_b == [sub1[axis] for axis in bs1]\n    sub_l = [sub0[axis] for axis in ts0]\n    sub_r = [sub1[axis] for axis in ts1]\n\n    sub_out = sub_b + sub_l + sub_r\n    assert set(sub_out) <= set_others, \'operands should be reduced: unary sum\'\n\n    if len(contract_dims) == 0:\n        # Use element-wise multiply when no contraction is needed\n        if len(sub_out) == len(sub_others):\n            # to assure final output of einsum is C-contiguous\n            sub_out = sub_others\n        arr0 = _expand_dims_transpose(arr0, sub0, sub_out)\n        arr1 = _expand_dims_transpose(arr1, sub1, sub_out)\n        return arr0 * arr1, sub_out\n\n    if _use_cutensor(arr0.dtype, sub0, arr1.dtype, sub1,\n                     batch_dims, contract_dims):\n        if len(sub_out) == len(sub_others):\n            # to assure final output of einsum is C-contiguous\n            sub_out = sub_others\n        out_shape = _get_out_shape(arr0.shape, sub0, arr1.shape, sub1, sub_out)\n        arr_out = cupy.empty(out_shape, arr0.dtype)\n        arr0 = cupy.ascontiguousarray(arr0)\n        arr1 = cupy.ascontiguousarray(arr1)\n        desc_0 = cutensor.create_tensor_descriptor(arr0)\n        desc_1 = cutensor.create_tensor_descriptor(arr1)\n        desc_out = cutensor.create_tensor_descriptor(arr_out)\n        arr_out = cutensor.contraction(1.0,\n                                       arr0, desc_0, sub0,\n                                       arr1, desc_1, sub1,\n                                       0.0,\n                                       arr_out, desc_out, sub_out)\n        return arr_out, sub_out\n\n    tmp0, shapes0 = _flatten_transpose(arr0, [bs0, ts0, cs0])\n    tmp1, shapes1 = _flatten_transpose(arr1, [bs1, cs1, ts1])\n    shapes_out = shapes0[0] + shapes0[1] + shapes1[2]\n    assert shapes0[0] == shapes1[0]\n    arr_out = cupy.matmul(tmp0, tmp1).reshape(shapes_out)\n    return arr_out, sub_out\n\n\ndef _make_transpose_axes(sub, b_dims, c_dims):\n    bs = []\n    cs = []\n    ts = []\n    for axis, label in enumerate(sub):\n        if label in b_dims:\n            bs.append((label, axis))\n        elif label in c_dims:\n            cs.append((label, axis))\n        else:\n            ts.append((label, axis))\n    return (\n        _tuple_sorted_by_0(bs),\n        _tuple_sorted_by_0(cs),\n        _tuple_sorted_by_0(ts),\n    )\n\n\ndef _tuple_sorted_by_0(zs):\n    return tuple(i for _, i in sorted(zs))\n\n\ndef einsum(*operands, **kwargs):\n    """"""einsum(subscripts, *operands, dtype=False)\n\n    Evaluates the Einstein summation convention on the operands.\n    Using the Einstein summation convention, many common multi-dimensional\n    array operations can be represented in a simple fashion. This function\n    provides a way to compute such summations.\n\n    .. note::\n       Memory contiguity of calculation result is not always compatible with\n       `numpy.einsum`.\n       ``out``, ``order``, and ``casting`` options are not supported.\n\n    Args:\n        subscripts (str): Specifies the subscripts for summation.\n        operands (sequence of arrays): These are the arrays for the operation.\n\n    Returns:\n        cupy.ndarray:\n            The calculation based on the Einstein summation convention.\n\n    .. seealso:: :func:`numpy.einsum`\n\n    """"""\n\n    input_subscripts, output_subscript, operands = \\\n        _parse_einsum_input(operands)\n    assert isinstance(input_subscripts, list)\n    assert isinstance(operands, list)\n\n    dtype = kwargs.pop(\'dtype\', None)\n\n    # casting = kwargs.pop(\'casting\', \'safe\')\n    casting_kwargs = {}  # casting is not supported yet in astype\n\n    optimize = kwargs.pop(\'optimize\', False)\n    if optimize is True:\n        optimize = \'greedy\'\n    if kwargs:\n        raise TypeError(\'Did not understand the following kwargs: %s\'\n                        % list(kwargs.keys))\n\n    result_dtype = cupy.result_type(*operands) if dtype is None else dtype\n    operands = [\n        cupy.asanyarray(arr)\n        for arr in operands\n    ]\n\n    input_subscripts = [\n        _parse_ellipsis_subscript(sub, idx, ndim=arr.ndim)\n        for idx, (sub, arr) in enumerate(zip(input_subscripts, operands))\n    ]\n\n    # Get length of each unique dimension and ensure all dimensions are correct\n    dimension_dict = {}\n    for idx, sub in enumerate(input_subscripts):\n        sh = operands[idx].shape\n        for axis, label in enumerate(sub):\n            dim = sh[axis]\n            if label in dimension_dict.keys():\n                # For broadcasting cases we always want the largest dim size\n                if dimension_dict[label] == 1:\n                    dimension_dict[label] = dim\n                elif dim not in (1, dimension_dict[label]):\n                    dim_old = dimension_dict[label]\n                    raise ValueError(\n                        \'Size of label \\\'%s\\\' for operand %d (%d) \'\n                        \'does not match previous terms (%d).\'\n                        % (_chr(label), idx, dim, dim_old))\n            else:\n                dimension_dict[label] = dim\n\n    if output_subscript is None:\n        # Build output subscripts\n        tmp_subscripts = list(itertools.chain.from_iterable(input_subscripts))\n        output_subscript = [\n            label\n            for label in sorted(set(tmp_subscripts))\n            if label < 0 or tmp_subscripts.count(label) == 1\n        ]\n    else:\n        if not options[\'sum_ellipsis\']:\n            if \'@\' not in output_subscript and -1 in dimension_dict:\n                raise ValueError(\n                    \'output has more dimensions than subscripts \'\n                    \'given in einstein sum, but no \\\'...\\\' ellipsis \'\n                    \'provided to broadcast the extra dimensions.\')\n        output_subscript = _parse_ellipsis_subscript(\n            output_subscript, None,\n            ellipsis_len=sum(label < 0 for label in dimension_dict.keys())\n        )\n\n        # Make sure output subscripts are in the input\n        tmp_subscripts = set(itertools.chain.from_iterable(input_subscripts))\n        for label in output_subscript:\n            if label not in tmp_subscripts:\n                raise ValueError(\n                    \'einstein sum subscripts string included output subscript \'\n                    \'\\\'%s\\\' which never appeared in an input\' % _chr(label))\n        if len(output_subscript) != len(set(output_subscript)):\n            for label in output_subscript:\n                if output_subscript.count(label) >= 2:\n                    raise ValueError(\n                        \'einstein sum subscripts string includes output \'\n                        \'subscript \\\'%s\\\' multiple times\' % _chr(label))\n\n    _einsum_diagonals(input_subscripts, operands)\n\n    # no more raises\n\n    if len(operands) >= 2:\n        if any(arr.size == 0 for arr in operands):\n            return cupy.zeros(\n                tuple(dimension_dict[label] for label in output_subscript),\n                dtype=result_dtype\n            )\n\n        # Don\'t squeeze if unary, because this affects later (in trivial sum)\n        # whether the return is a writeable view.\n        for idx in range(len(operands)):\n            arr = operands[idx]\n            if 1 in arr.shape:\n                squeeze_indices = []\n                sub = []\n                for axis, label in enumerate(input_subscripts[idx]):\n                    if arr.shape[axis] == 1:\n                        squeeze_indices.append(axis)\n                    else:\n                        sub.append(label)\n                input_subscripts[idx] = sub\n                operands[idx] = cupy.squeeze(arr, axis=tuple(squeeze_indices))\n                assert operands[idx].ndim == len(input_subscripts[idx])\n            del arr\n\n    # unary einsum without summation should return a (writeable) view\n    returns_view = len(operands) == 1\n\n    # unary sum\n    for idx, sub in enumerate(input_subscripts):\n        other_subscripts = copy.copy(input_subscripts)\n        other_subscripts[idx] = output_subscript\n        other_subscripts = set(itertools.chain.from_iterable(other_subscripts))\n        sum_axes = tuple(\n            axis\n            for axis, label in enumerate(sub)\n            if label not in other_subscripts\n        )\n        if sum_axes:\n            returns_view = False\n            input_subscripts[idx] = [\n                label\n                for axis, label in enumerate(sub)\n                if axis not in sum_axes\n            ]\n\n            operands[idx] = operands[idx].sum(\n                axis=sum_axes, dtype=result_dtype)\n\n    if returns_view:\n        operands = [a.view() for a in operands]\n    else:\n        operands = [\n            a.astype(result_dtype, copy=False, **casting_kwargs)\n            for a in operands\n        ]\n\n    # no more casts\n\n    optimize_algorithms = {\n        \'greedy\': _greedy_path,\n        \'optimal\': _optimal_path,\n    }\n    if optimize is False:\n        path = [tuple(range(len(operands)))]\n    elif len(optimize) and (optimize[0] == \'einsum_path\'):\n        path = optimize[1:]\n    else:\n        try:\n            if len(optimize) == 2 and isinstance(optimize[1], (int, float)):\n                algo = optimize_algorithms[optimize[0]]\n                memory_limit = int(optimize[1])\n            else:\n                algo = optimize_algorithms[optimize]\n                memory_limit = 2 ** 31  # TODO(kataoka): fix?\n        except (TypeError, KeyError):  # unhashable type or not found\n            raise TypeError(\'Did not understand the path (optimize): %s\'\n                            % str(optimize))\n        input_sets = [set(sub) for sub in input_subscripts]\n        output_set = set(output_subscript)\n        path = algo(input_sets, output_set, dimension_dict, memory_limit)\n        if any(len(indices) > 2 for indices in path):\n            warnings.warn(\n                \'memory efficient einsum is not supported yet\',\n                util.PerformanceWarning)\n\n    for idx0, idx1 in _iter_path_pairs(path):\n        # ""reduced"" binary einsum\n        arr0 = operands.pop(idx0)\n        sub0 = input_subscripts.pop(idx0)\n        arr1 = operands.pop(idx1)\n        sub1 = input_subscripts.pop(idx1)\n        sub_others = list(itertools.chain(\n            output_subscript,\n            itertools.chain.from_iterable(input_subscripts)))\n        arr_out, sub_out = reduced_binary_einsum(\n            arr0, sub0, arr1, sub1, sub_others)\n        operands.append(arr_out)\n        input_subscripts.append(sub_out)\n        del arr0, arr1\n\n    # unary einsum at last\n    arr0, = operands\n    sub0, = input_subscripts\n\n    transpose_axes = []\n    for label in output_subscript:\n        if label in sub0:\n            transpose_axes.append(sub0.index(label))\n\n    arr_out = arr0.transpose(transpose_axes).reshape([\n        dimension_dict[label]\n        for label in output_subscript\n    ])\n    assert returns_view or arr_out.dtype == result_dtype\n    return arr_out\n'"
cupy/linalg/einsum_opt.py,0,"b'import itertools\n\n\ndef _flop_count(idx_contraction, inner, num_terms, size_dictionary):\n    """"""Copied from _flop_count in numpy/core/einsumfunc.py\n\n    Computes the number of FLOPS in the contraction.\n\n    Parameters\n    ----------\n    idx_contraction : iterable\n        The indices involved in the contraction\n    inner : bool\n        Does this contraction require an inner product?\n    num_terms : int\n        The number of terms in a contraction\n    size_dictionary : dict\n        The size of each of the indices in idx_contraction\n\n    Returns\n    -------\n    flop_count : int\n        The total number of FLOPS required for the contraction.\n\n    Examples\n    --------\n\n    >>> _flop_count(\'abc\', False, 1, {\'a\': 2, \'b\':3, \'c\':5})\n    90\n\n    >>> _flop_count(\'abc\', True, 2, {\'a\': 2, \'b\':3, \'c\':5})\n    270\n\n    """"""\n\n    overall_size = _compute_size_by_dict(idx_contraction, size_dictionary)\n    op_factor = max(1, num_terms - 1)\n    if inner:\n        op_factor += 1\n\n    return overall_size * op_factor\n\n\ndef _compute_size_by_dict(indices, idx_dict):\n    """"""Copied from _compute_size_by_dict in numpy/core/einsumfunc.py\n\n    Computes the product of the elements in indices based on the dictionary\n    idx_dict.\n\n    Parameters\n    ----------\n    indices : iterable\n        Indices to base the product on.\n    idx_dict : dictionary\n        Dictionary of index sizes\n\n    Returns\n    -------\n    ret : int\n        The resulting product.\n\n    Examples\n    --------\n    >>> _compute_size_by_dict(\'abbc\', {\'a\': 2, \'b\':3, \'c\':5})\n    90\n\n    """"""\n    ret = 1\n    for i in indices:\n        ret *= idx_dict[i]\n    return ret\n\n\ndef _find_contraction(positions, input_sets, output_set):\n    """"""Copied from _find_contraction in numpy/core/einsumfunc.py\n\n    Finds the contraction for a given set of input and output sets.\n\n    Parameters\n    ----------\n    positions : iterable\n        Integer positions of terms used in the contraction.\n    input_sets : list\n        List of sets that represent the lhs side of the einsum subscript\n    output_set : set\n        Set that represents the rhs side of the overall einsum subscript\n\n    Returns\n    -------\n    new_result : set\n        The indices of the resulting contraction\n    remaining : list\n        List of sets that have not been contracted, the new set is appended to\n        the end of this list\n    idx_removed : set\n        Indices removed from the entire contraction\n    idx_contraction : set\n        The indices used in the current contraction\n\n    Examples\n    --------\n\n    # A simple dot product test case\n    >>> pos = (0, 1)\n    >>> isets = [set(\'ab\'), set(\'bc\')]\n    >>> oset = set(\'ac\')\n    >>> _find_contraction(pos, isets, oset)\n    ({\'a\', \'c\'}, [{\'a\', \'c\'}], {\'b\'}, {\'a\', \'b\', \'c\'})\n\n    # A more complex case with additional terms in the contraction\n    >>> pos = (0, 2)\n    >>> isets = [set(\'abd\'), set(\'ac\'), set(\'bdc\')]\n    >>> oset = set(\'ac\')\n    >>> _find_contraction(pos, isets, oset)\n    ({\'a\', \'c\'}, [{\'a\', \'c\'}, {\'a\', \'c\'}], {\'b\', \'d\'}, {\'a\', \'b\', \'c\', \'d\'})\n    """"""\n\n    idx_contract = set()\n    idx_remain = output_set.copy()\n    remaining = []\n    for ind, value in enumerate(input_sets):\n        if ind in positions:\n            idx_contract |= value\n        else:\n            remaining.append(value)\n            idx_remain |= value\n\n    new_result = idx_remain & idx_contract\n    idx_removed = (idx_contract - new_result)\n    remaining.append(new_result)\n\n    return (new_result, remaining, idx_removed, idx_contract)\n\n\ndef _optimal_path(input_sets, output_set, idx_dict, memory_limit):\n    """"""Copied from _optimal_path in numpy/core/einsumfunc.py\n\n    Computes all possible pair contractions, sieves the results based\n    on ``memory_limit`` and returns the lowest cost path. This algorithm\n    scales factorial with respect to the elements in the list ``input_sets``.\n\n    Parameters\n    ----------\n    input_sets : list\n        List of sets that represent the lhs side of the einsum subscript\n    output_set : set\n        Set that represents the rhs side of the overall einsum subscript\n    idx_dict : dictionary\n        Dictionary of index sizes\n    memory_limit : int\n        The maximum number of elements in a temporary array\n\n    Returns\n    -------\n    path : list\n        The optimal contraction order within the memory limit constraint.\n\n    Examples\n    --------\n    >>> isets = [set(\'abd\'), set(\'ac\'), set(\'bdc\')]\n    >>> oset = set(\'\')\n    >>> idx_sizes = {\'a\': 1, \'b\':2, \'c\':3, \'d\':4}\n    >>> _optimal_path(isets, oset, idx_sizes, 5000)\n    [(0, 2), (0, 1)]\n    """"""\n\n    full_results = [(0, [], input_sets)]\n    for iteration in range(len(input_sets) - 1):\n        iter_results = []\n\n        # Compute all unique pairs\n        for curr in full_results:\n            cost, positions, remaining = curr\n            for con in itertools.combinations(range(len(input_sets) - iteration), 2):  # NOQA\n\n                # Find the contraction\n                cont = _find_contraction(con, remaining, output_set)\n                new_result, new_input_sets, idx_removed, idx_contract = cont\n\n                # Sieve the results based on memory_limit\n                new_size = _compute_size_by_dict(new_result, idx_dict)\n                if new_size > memory_limit:\n                    continue\n\n                # Build (total_cost, positions, indices_remaining)\n                total_cost = cost + \\\n                    _flop_count(idx_contract, idx_removed, len(con), idx_dict)\n                new_pos = positions + [con]\n                iter_results.append((total_cost, new_pos, new_input_sets))\n\n        # Update combinatorial list, if we did not find anything return best\n        # path + remaining contractions\n        if iter_results:\n            full_results = iter_results\n        else:\n            path = min(full_results, key=lambda x: x[0])[1]\n            path += [tuple(range(len(input_sets) - iteration))]\n            return path\n\n    # If we have not found anything return single einsum contraction\n    if len(full_results) == 0:\n        return [tuple(range(len(input_sets)))]\n\n    path = min(full_results, key=lambda x: x[0])[1]\n    return path\n\n\ndef _parse_possible_contraction(positions, input_sets, output_set, idx_dict, memory_limit, path_cost, naive_cost):  # NOQA\n    """"""Copied from _parse_possible_contraction in numpy/core/einsumfunc.py\n\n    Compute the cost (removed size + flops) and resultant indices for\n    performing the contraction specified by ``positions``.\n\n    Parameters\n    ----------\n    positions : tuple of int\n        The locations of the proposed tensors to contract.\n    input_sets : list of sets\n        The indices found on each tensors.\n    output_set : set\n        The output indices of the expression.\n    idx_dict : dict\n        Mapping of each index to its size.\n    memory_limit : int\n        The total allowed size for an intermediary tensor.\n    path_cost : int\n        The contraction cost so far.\n    naive_cost : int\n        The cost of the unoptimized expression.\n\n    Returns\n    -------\n    cost : (int, int)\n        A tuple containing the size of any indices removed, and the flop cost.\n    positions : tuple of int\n        The locations of the proposed tensors to contract.\n    new_input_sets : list of sets\n        The resulting new list of indices if this proposed contraction is performed.\n\n    """"""  # NOQA\n\n    # Find the contraction\n    contract = _find_contraction(positions, input_sets, output_set)\n    idx_result, new_input_sets, idx_removed, idx_contract = contract\n\n    # Sieve the results based on memory_limit\n    new_size = _compute_size_by_dict(idx_result, idx_dict)\n    if new_size > memory_limit:\n        return None\n\n    # Build sort tuple\n    old_sizes = (_compute_size_by_dict(\n        input_sets[p], idx_dict) for p in positions)\n    removed_size = sum(old_sizes) - new_size\n\n    # NB: removed_size used to be just the size of any removed indices i.e.:\n    #     helpers.compute_size_by_dict(idx_removed, idx_dict)\n    cost = _flop_count(idx_contract, idx_removed, len(positions), idx_dict)\n    sort = (-removed_size, cost)\n\n    # Sieve based on total cost as well\n    if (path_cost + cost) > naive_cost:\n        return None\n\n    # Add contraction to possible choices\n    return [sort, positions, new_input_sets]\n\n\ndef _update_other_results(results, best):\n    """"""Copied from _update_other_results in numpy/core/einsumfunc.py\n\n    Update the positions and provisional input_sets of ``results`` based on\n    performing the contraction result ``best``. Remove any involving the tensors\n    contracted.\n\n    Parameters\n    ----------\n    results : list\n        List of contraction results produced by ``_parse_possible_contraction``.\n    best : list\n        The best contraction of ``results`` i.e. the one that will be performed.\n\n    Returns\n    -------\n    mod_results : list\n        The list of modifed results, updated with outcome of ``best`` contraction.  # NOQA\n    """"""\n\n    best_con = best[1]\n    bx, by = best_con\n    mod_results = []\n\n    for cost, (x, y), con_sets in results:\n\n        # Ignore results involving tensors just contracted\n        if x in best_con or y in best_con:\n            continue\n\n        # Update the input_sets\n        del con_sets[by - int(by > x) - int(by > y)]\n        del con_sets[bx - int(bx > x) - int(bx > y)]\n        con_sets.insert(-1, best[2][-1])\n\n        # Update the position indices\n        mod_con = x - int(x > bx) - int(x > by), y - int(y > bx) - int(y > by)\n        mod_results.append((cost, mod_con, con_sets))\n\n    return mod_results\n\n\ndef _greedy_path(input_sets, output_set, idx_dict, memory_limit):\n    """"""Copied from _greedy_path in numpy/core/einsumfunc.py\n\n    Finds the path by contracting the best pair until the input list is\n    exhausted. The best pair is found by minimizing the tuple\n    ``(-prod(indices_removed), cost)``.  What this amounts to is prioritizing\n    matrix multiplication or inner product operations, then Hadamard like\n    operations, and finally outer operations. Outer products are limited by\n    ``memory_limit``. This algorithm scales cubically with respect to the\n    number of elements in the list ``input_sets``.\n\n    Parameters\n    ----------\n    input_sets : list\n        List of sets that represent the lhs side of the einsum subscript\n    output_set : set\n        Set that represents the rhs side of the overall einsum subscript\n    idx_dict : dictionary\n        Dictionary of index sizes\n    memory_limit_limit : int\n        The maximum number of elements in a temporary array\n\n    Returns\n    -------\n    path : list\n        The greedy contraction order within the memory limit constraint.\n\n    Examples\n    --------\n    >>> isets = [set(\'abd\'), set(\'ac\'), set(\'bdc\')]\n    >>> oset = set(\'\')\n    >>> idx_sizes = {\'a\': 1, \'b\':2, \'c\':3, \'d\':4}\n    >>> _greedy_path(isets, oset, idx_sizes, 5000)\n    [(0, 2), (0, 1)]\n    """"""\n\n    # Handle trivial cases that leaked through\n    if len(input_sets) == 1:\n        return [(0,)]\n    elif len(input_sets) == 2:\n        return [(0, 1)]\n\n    # Build up a naive cost\n    contract = _find_contraction(\n        range(len(input_sets)), input_sets, output_set)\n    idx_result, new_input_sets, idx_removed, idx_contract = contract\n    naive_cost = _flop_count(idx_contract, idx_removed,\n                             len(input_sets), idx_dict)\n\n    # Initially iterate over all pairs\n    comb_iter = itertools.combinations(range(len(input_sets)), 2)\n    known_contractions = []\n\n    path_cost = 0\n    path = []\n\n    for iteration in range(len(input_sets) - 1):\n\n        # Iterate over all pairs on first step, only previously found pairs on subsequent steps  # NOQA\n        for positions in comb_iter:\n\n            # Always initially ignore outer products\n            if input_sets[positions[0]].isdisjoint(input_sets[positions[1]]):\n                continue\n\n            result = _parse_possible_contraction(positions, input_sets, output_set, idx_dict, memory_limit, path_cost,  # NOQA\n                                                 naive_cost)\n            if result is not None:\n                known_contractions.append(result)\n\n        # If we do not have a inner contraction, rescan pairs including outer products  # NOQA\n        if len(known_contractions) == 0:\n\n            # Then check the outer products\n            for positions in itertools.combinations(range(len(input_sets)), 2):\n                result = _parse_possible_contraction(positions, input_sets, output_set, idx_dict, memory_limit,  # NOQA\n                                                     path_cost, naive_cost)\n                if result is not None:\n                    known_contractions.append(result)\n\n            # If we still did not find any remaining contractions, default back to einsum like behavior  # NOQA\n            if len(known_contractions) == 0:\n                path.append(tuple(range(len(input_sets))))\n                break\n\n        # Sort based on first index\n        best = min(known_contractions, key=lambda x: x[0])\n\n        # Now propagate as many unused contractions as possible to next iteration  # NOQA\n        known_contractions = _update_other_results(known_contractions, best)\n\n        # Next iteration only compute contractions with the new tensor\n        # All other contractions have been accounted for\n        input_sets = best[2]\n        new_tensor_pos = len(input_sets) - 1\n        comb_iter = ((i, new_tensor_pos) for i in range(new_tensor_pos))\n\n        # Update path and total cost\n        path.append(best[1])\n        path_cost += best[0][1]\n\n    return path\n'"
cupy/linalg/norms.py,0,"b'import numpy\nfrom numpy import linalg\n\nimport cupy\nfrom cupy import core\nfrom cupy.linalg import decomposition\nfrom cupy.linalg import util\n\nimport functools\n\n\ndef _multi_svd_norm(x, row_axis, col_axis, op):\n    y = cupy.moveaxis(x, (row_axis, col_axis), (-2, -1))\n    result = op(decomposition.svd(y, compute_uv=False), axis=-1)\n    return result\n\n\n_norm_ord2 = core.create_reduction_func(\n    \'_norm_ord2\',\n    (\'?->l\', \'b->l\', \'B->L\', \'h->l\', \'H->L\', \'i->l\', \'I->L\', \'l->l\', \'L->L\',\n     \'q->q\', \'Q->Q\',\n     (\'e->e\', (None, None, None, \'float\')),\n     \'f->f\', \'d->d\'),\n    (\'in0 * in0\', \'a + b\', \'out0 = sqrt(type_out0_raw(a))\', None), 0)\n_norm_ord2_complex = core.create_reduction_func(\n    \'_norm_ord2_complex\',\n    (\'F->F\', \'D->D\'),\n    (\'in0.real() * in0.real() + in0.imag() * in0.imag()\',\n     \'a + b\', \'out0 = sqrt(type_out0_raw(a))\', None), 0)\n\n\ndef norm(x, ord=None, axis=None, keepdims=False):\n    """"""Returns one of matrix norms specified by ``ord`` parameter.\n\n    See numpy.linalg.norm for more detail.\n\n    Args:\n        x (cupy.ndarray): Array to take norm. If ``axis`` is None,\n            ``x`` must be 1-D or 2-D.\n        ord (non-zero int, inf, -inf, \'fro\'): Norm type.\n        axis (int, 2-tuple of ints, None): 1-D or 2-D norm is cumputed over\n            ``axis``.\n        keepdims (bool): If this is set ``True``, the axes which are normed\n            over are left.\n\n    Returns:\n        cupy.ndarray\n\n    """"""\n    if not issubclass(x.dtype.type, numpy.inexact):\n        x = x.astype(float)\n\n    # Immediately handle some default, simple, fast, and common cases.\n    if axis is None:\n        ndim = x.ndim\n        if (ord is None or (ndim == 1 and ord == 2) or\n                (ndim == 2 and ord in (\'f\', \'fro\'))):\n            if x.dtype.kind == \'c\':\n                s = abs(x.ravel())\n                s *= s\n                ret = cupy.sqrt(s.sum())\n            else:\n                ret = cupy.sqrt((x * x).sum())\n            if keepdims:\n                ret = ret.reshape((1,) * ndim)\n            return ret\n\n    # Normalize the `axis` argument to a tuple.\n    nd = x.ndim\n    if axis is None:\n        axis = tuple(range(nd))\n    elif not isinstance(axis, tuple):\n        try:\n            axis = int(axis)\n        except Exception:\n            raise TypeError(\n                \'\\\'axis\\\' must be None, an integer or a tuple of integers\')\n        axis = (axis,)\n\n    if len(axis) == 1:\n        if ord == numpy.Inf:\n            return abs(x).max(axis=axis, keepdims=keepdims)\n        elif ord == -numpy.Inf:\n            return abs(x).min(axis=axis, keepdims=keepdims)\n        elif ord == 0:\n            # Zero norm\n            # Convert to Python float in accordance with NumPy\n            return (x != 0).astype(x.real.dtype).sum(\n                axis=axis, keepdims=keepdims)\n        elif ord == 1:\n            # special case for speedup\n            return abs(x).sum(axis=axis, keepdims=keepdims)\n        elif ord is None or ord == 2:\n            # special case for speedup\n            if x.dtype.kind == \'c\':\n                return _norm_ord2_complex(x, axis=axis, keepdims=keepdims)\n            return _norm_ord2(x, axis=axis, keepdims=keepdims)\n        else:\n            try:\n                float(ord)\n            except TypeError:\n                raise ValueError(\'Invalid norm order for vectors.\')\n\n            absx = abs(x)\n            absx **= ord\n            ret = absx.sum(axis=axis, keepdims=keepdims)\n            ret **= cupy.reciprocal(ord, dtype=ret.dtype)\n            return ret\n    elif len(axis) == 2:\n        row_axis, col_axis = axis\n        if row_axis < 0:\n            row_axis += nd\n        if col_axis < 0:\n            col_axis += nd\n        if not (0 <= row_axis < nd and 0 <= col_axis < nd):\n            raise ValueError(\'Invalid axis %r for an array with shape %r\' %\n                             (axis, x.shape))\n        if row_axis == col_axis:\n            raise ValueError(\'Duplicate axes given.\')\n        if ord == 2:\n            op_max = functools.partial(cupy.take, indices=0)\n            ret = _multi_svd_norm(x, row_axis, col_axis, op_max)\n        elif ord == -2:\n            op_min = functools.partial(cupy.take, indices=-1)\n            ret = _multi_svd_norm(x, row_axis, col_axis, op_min)\n        elif ord == 1:\n            if col_axis > row_axis:\n                col_axis -= 1\n            ret = abs(x).sum(axis=row_axis).max(axis=col_axis)\n        elif ord == numpy.Inf:\n            if row_axis > col_axis:\n                row_axis -= 1\n            ret = abs(x).sum(axis=col_axis).max(axis=row_axis)\n        elif ord == -1:\n            if col_axis > row_axis:\n                col_axis -= 1\n            ret = abs(x).sum(axis=row_axis).min(axis=col_axis)\n        elif ord == -numpy.Inf:\n            if row_axis > col_axis:\n                row_axis -= 1\n            ret = abs(x).sum(axis=col_axis).min(axis=row_axis)\n        elif ord in [None, \'fro\', \'f\']:\n            if x.dtype.kind == \'c\':\n                ret = _norm_ord2_complex(x, axis=axis)\n            else:\n                ret = _norm_ord2(x, axis=axis)\n        elif ord == \'nuc\':\n            ret = _multi_svd_norm(x, row_axis, col_axis, cupy.sum)\n        else:\n            raise ValueError(\'Invalid norm order for matrices.\')\n        if keepdims:\n            ret_shape = list(x.shape)\n            ret_shape[axis[0]] = 1\n            ret_shape[axis[1]] = 1\n            ret = ret.reshape(ret_shape)\n        return ret\n    else:\n        raise ValueError(\'Improper number of dimensions to norm.\')\n\n\n# TODO(okuta): Implement cond\n\n\ndef det(a):\n    """"""Retruns the deteminant of an array.\n\n    Args:\n        a (cupy.ndarray): The input matrix with dimension ``(..., N, N)``.\n\n    Returns:\n        cupy.ndarray: Determinant of ``a``. Its shape is ``a.shape[:-2]``.\n\n    .. seealso:: :func:`numpy.linalg.det`\n    """"""\n    sign, logdet = slogdet(a)\n    return sign * cupy.exp(logdet)\n\n\ndef matrix_rank(M, tol=None):\n    """"""Return matrix rank of array using SVD method\n\n    Args:\n        M (cupy.ndarray): Input array. Its `ndim` must be less than or equal to\n            2.\n        tol (None or float): Threshold of singular value of `M`.\n            When `tol` is `None`, and `eps` is the epsilon value for datatype\n            of `M`, then `tol` is set to `S.max() * max(M.shape) * eps`,\n            where `S` is the singular value of `M`.\n            It obeys :func:`numpy.linalg.matrix_rank`.\n\n    Returns:\n        cupy.ndarray: Rank of `M`.\n\n    .. seealso:: :func:`numpy.linalg.matrix_rank`\n    """"""\n    if M.ndim < 2:\n        return (M != 0).any().astype(int)\n    S = decomposition.svd(M, compute_uv=False)\n    if tol is None:\n        tol = (S.max(axis=-1, keepdims=True) * max(M.shape[-2:]) *\n               numpy.finfo(S.dtype).eps)\n    return (S > tol).sum(axis=-1, dtype=numpy.intp)\n\n\ndef slogdet(a):\n    """"""Returns sign and logarithm of the determinant of an array.\n\n    It calculates the natural logarithm of the determinant of a given value.\n\n    Args:\n        a (cupy.ndarray): The input matrix with dimension ``(..., N, N)``.\n\n    Returns:\n        tuple of :class:`~cupy.ndarray`:\n            It returns a tuple ``(sign, logdet)``. ``sign`` represents each\n            sign of the determinant as a real number ``0``, ``1`` or ``-1``.\n            \'logdet\' represents the natural logarithm of the absolute of the\n            determinant.\n            If the determinant is zero, ``sign`` will be ``0`` and ``logdet``\n            will be ``-inf``.\n            The shapes of both ``sign`` and ``logdet`` are equal to\n            ``a.shape[:-2]``.\n\n    .. warning::\n        This function calls one or more cuSOLVER routine(s) which may yield\n        invalid results if input conditions are not met.\n        To detect these invalid results, you can set the `linalg`\n        configuration to a value that is not `ignore` in\n        :func:`cupyx.errstate` or :func:`cupyx.seterr`.\n\n    .. warning::\n        To produce the same results as :func:`numpy.linalg.slogdet` for\n        singular inputs, set the `linalg` configuration to `raise`.\n\n    .. seealso:: :func:`numpy.linalg.slogdet`\n    """"""\n    if a.ndim < 2:\n        msg = (\'%d-dimensional array given. \'\n               \'Array must be at least two-dimensional\' % a.ndim)\n        raise linalg.LinAlgError(msg)\n    util._assert_nd_squareness(a)\n\n    dtype = numpy.promote_types(a.dtype.char, \'f\')\n    real_dtype = dtype\n\n    # TODO(kataoka): support complex types\n    if dtype not in (numpy.float32, numpy.float64):\n        msg = (\'dtype must be float32 or float64\'\n               \' (actual: {})\'.format(a.dtype))\n        raise ValueError(msg)\n\n    a_shape = a.shape\n    shape = a_shape[:-2]\n    n = a_shape[-2]\n\n    if a.size == 0:\n        # empty batch (result is empty, too) or empty matrices det([[]]) == 1\n        sign = cupy.ones(shape, dtype)\n        logdet = cupy.zeros(shape, real_dtype)\n        return sign, logdet\n\n    lu, ipiv, dev_info = decomposition._lu_factor(a, dtype)\n\n    # dev_info < 0 means illegal value (in dimensions, strides, and etc.) that\n    # should never happen even if the matrix contains nan or inf.\n    # TODO(kataoka): assert dev_info >= 0 if synchronization is allowed for\n    # debugging purposes.\n\n    diag = cupy.diagonal(lu, axis1=-2, axis2=-1)\n\n    # ipiv is 1-origin\n    non_zero = (cupy.count_nonzero(ipiv != cupy.arange(1, n + 1), axis=-1) +\n                cupy.count_nonzero(diag < 0, axis=-1))\n\n    # Note: sign == -1 ** (non_zero % 2)\n    sign = (non_zero % 2) * -2 + 1\n    logdet = cupy.log(abs(diag)).sum(axis=-1)\n\n    singular = dev_info > 0\n    return (\n        cupy.where(singular, dtype.type(0), sign.astype(dtype)).reshape(shape),\n        cupy.where(singular, real_dtype.type(\'-inf\'), logdet).reshape(shape),\n    )\n\n\ndef trace(a, offset=0, axis1=0, axis2=1, dtype=None, out=None):\n    """"""Returns the sum along the diagonals of an array.\n\n    It computes the sum along the diagonals at ``axis1`` and ``axis2``.\n\n    Args:\n        a (cupy.ndarray): Array to take trace.\n        offset (int): Index of diagonals. Zero indicates the main diagonal, a\n            positive value an upper diagonal, and a negative value a lower\n            diagonal.\n        axis1 (int): The first axis along which the trace is taken.\n        axis2 (int): The second axis along which the trace is taken.\n        dtype: Data type specifier of the output.\n        out (cupy.ndarray): Output array.\n\n    Returns:\n        cupy.ndarray: The trace of ``a`` along axes ``(axis1, axis2)``.\n\n    .. seealso:: :func:`numpy.trace`\n\n    """"""\n    # TODO(okuta): check type\n    return a.trace(offset, axis1, axis2, dtype, out)\n'"
cupy/linalg/product.py,0,"b'import collections.abc\n\nimport numpy\n\nimport cupy\nfrom cupy import core\nfrom cupy.core import internal\n\nfrom cupy.linalg.solve import inv\nfrom cupy import util\n\nmatmul = core.matmul\n\n\ndef dot(a, b, out=None):\n    """"""Returns a dot product of two arrays.\n\n    For arrays with more than one axis, it computes the dot product along the\n    last axis of ``a`` and the second-to-last axis of ``b``. This is just a\n    matrix product if the both arrays are 2-D. For 1-D arrays, it uses their\n    unique axis as an axis to take dot product over.\n\n    Args:\n        a (cupy.ndarray): The left argument.\n        b (cupy.ndarray): The right argument.\n        out (cupy.ndarray): Output array.\n\n    Returns:\n        cupy.ndarray: The dot product of ``a`` and ``b``.\n\n    .. seealso:: :func:`numpy.dot`\n\n    """"""\n    # TODO(okuta): check type\n    return a.dot(b, out)\n\n\ndef vdot(a, b):\n    """"""Returns the dot product of two vectors.\n\n    The input arrays are flattened into 1-D vectors and then it performs inner\n    product of these vectors.\n\n    Args:\n        a (cupy.ndarray): The first argument.\n        b (cupy.ndarray): The second argument.\n\n    Returns:\n        cupy.ndarray: Zero-dimensional array of the dot product result.\n\n    .. seealso:: :func:`numpy.vdot`\n\n    """"""\n    if a.size != b.size:\n        raise ValueError(\'Axis dimension mismatch\')\n    if a.dtype.kind == \'c\':\n        a = a.conj()\n\n    return core.tensordot_core(a, b, None, 1, 1, a.size, ())\n\n\ndef cross(a, b, axisa=-1, axisb=-1, axisc=-1, axis=None):\n    """"""Returns the cross product of two vectors.\n\n    The cross product of ``a`` and ``b`` in :math:`R^3` is a vector\n    perpendicular to both ``a`` and ``b``.  If ``a`` and ``b`` are arrays\n    of vectors, the vectors are defined by the last axis of ``a`` and ``b``\n    by default, and these axes can have dimensions 2 or 3.  Where the\n    dimension of either ``a`` or ``b`` is 2, the third component of the input\n    vector is assumed to be zero and the cross product calculated accordingly.\n    In cases where both input vectors have dimension 2, the z-component of\n    the cross product is returned.\n\n    Args:\n        a (cupy.ndarray): Components of the first vector(s).\n        b (cupy.ndarray): Components of the second vector(s).\n        axisa (int, optional):\n            Axis of ``a`` that defines the vector(s).\n            By default, the last axis.\n        axisb (int, optional):\n            Axis of ``b`` that defines the vector(s).\n            By default, the last axis.\n        axisc (int, optional):\n            Axis of ``c`` containing the cross product vector(s).  Ignored if\n            both input vectors have dimension 2, as the return is scalar.\n            By default, the last axis.\n        axis (int, optional):\n            If defined, the axis of ``a``, ``b`` and ``c``\n            that defines the vector(s) and cross product(s).\n            Overrides ``axisa``, ``axisb`` and ``axisc``.\n\n    Returns:\n        cupy.ndarray :\n            Vector cross product(s).\n\n    .. seealso:: :func:`numpy.cross`\n\n    """"""\n\n    if axis is not None:\n        axisa, axisb, axisc = (axis,) * 3\n    a = cupy.asarray(a)\n    b = cupy.asarray(b)\n    # Check axisa and axisb are within bounds\n    axisa = util._normalize_axis_index(axisa, a.ndim)\n    axisb = util._normalize_axis_index(axisb, b.ndim)\n\n    # Move working axis to the end of the shape\n    a = cupy.moveaxis(a, axisa, -1)\n    b = cupy.moveaxis(b, axisb, -1)\n    if a.shape[-1] not in (2, 3) or b.shape[-1] not in (2, 3):\n        msg = (\'incompatible dimensions for cross product\\n\'\n               \'(dimension must be 2 or 3)\')\n        raise ValueError(msg)\n\n    # Create the output array\n    shape = cupy.broadcast(a[..., 0], b[..., 0]).shape\n    if a.shape[-1] == 3 or b.shape[-1] == 3:\n        shape += (3,)\n        # Check axisc is within bounds\n        axisc = util._normalize_axis_index(axisc, len(shape))\n    dtype = cupy.promote_types(a.dtype, b.dtype)\n    cp = cupy.empty(shape, dtype)\n\n    # create local aliases for readability\n    a0 = a[..., 0]\n    a1 = a[..., 1]\n    if a.shape[-1] == 3:\n        a2 = a[..., 2]\n    b0 = b[..., 0]\n    b1 = b[..., 1]\n    if b.shape[-1] == 3:\n        b2 = b[..., 2]\n    if cp.ndim != 0 and cp.shape[-1] == 3:\n        cp0 = cp[..., 0]\n        cp1 = cp[..., 1]\n        cp2 = cp[..., 2]\n\n    if a.shape[-1] == 2:\n        if b.shape[-1] == 2:\n            # a0 * b1 - a1 * b0\n            cupy.multiply(a0, b1, out=cp)\n            cp -= a1 * b0\n            return cp\n        else:\n            assert b.shape[-1] == 3\n            # cp0 = a1 * b2 - 0  (a2 = 0)\n            # cp1 = 0 - a0 * b2  (a2 = 0)\n            # cp2 = a0 * b1 - a1 * b0\n            cupy.multiply(a1, b2, out=cp0)\n            cupy.multiply(a0, b2, out=cp1)\n            cupy.negative(cp1, out=cp1)\n            cupy.multiply(a0, b1, out=cp2)\n            cp2 -= a1 * b0\n    else:\n        assert a.shape[-1] == 3\n        if b.shape[-1] == 3:\n            # cp0 = a1 * b2 - a2 * b1\n            # cp1 = a2 * b0 - a0 * b2\n            # cp2 = a0 * b1 - a1 * b0\n            cupy.multiply(a1, b2, out=cp0)\n            tmp = a2 * b1\n            cp0 -= tmp\n            cupy.multiply(a2, b0, out=cp1)\n            cupy.multiply(a0, b2, out=tmp)\n            cp1 -= tmp\n            cupy.multiply(a0, b1, out=cp2)\n            cupy.multiply(a1, b0, out=tmp)\n            cp2 -= tmp\n        else:\n            assert b.shape[-1] == 2\n            # cp0 = 0 - a2 * b1  (b2 = 0)\n            # cp1 = a2 * b0 - 0  (b2 = 0)\n            # cp2 = a0 * b1 - a1 * b0\n            cupy.multiply(a2, b1, out=cp0)\n            cupy.negative(cp0, out=cp0)\n            cupy.multiply(a2, b0, out=cp1)\n            cupy.multiply(a0, b1, out=cp2)\n            cp2 -= a1 * b0\n\n    return cupy.moveaxis(cp, -1, axisc)\n\n\ndef inner(a, b):\n    """"""Returns the inner product of two arrays.\n\n    It uses the last axis of each argument to take sum product.\n\n    Args:\n        a (cupy.ndarray): The first argument.\n        b (cupy.ndarray): The second argument.\n\n    Returns:\n        cupy.ndarray: The inner product of ``a`` and ``b``.\n\n    .. seealso:: :func:`numpy.inner`\n\n    """"""\n    a_ndim = a.ndim\n    b_ndim = b.ndim\n    if a_ndim == 0 or b_ndim == 0:\n        return cupy.multiply(a, b)\n\n    a_axis = a_ndim - 1\n    b_axis = b_ndim - 1\n\n    if a.shape[-1] != b.shape[-1]:\n        raise ValueError(\'Axis dimension mismatch\')\n\n    if a_axis:\n        a = cupy.rollaxis(a, a_axis, 0)\n    if b_axis:\n        b = cupy.rollaxis(b, b_axis, 0)\n\n    ret_shape = a.shape[1:] + b.shape[1:]\n\n    k = a.shape[0]\n    n = a.size // k\n    m = b.size // k\n\n    return core.tensordot_core(a, b, None, n, m, k, ret_shape)\n\n\ndef outer(a, b, out=None):\n    """"""Returns the outer product of two vectors.\n\n    The input arrays are flattened into 1-D vectors and then it performs outer\n    product of these vectors.\n\n    Args:\n        a (cupy.ndarray): The first argument.\n        b (cupy.ndarray): The second argument.\n        out (cupy.ndarray): Output array.\n\n    Returns:\n        cupy.ndarray: 2-D array of the outer product of ``a`` and ``b``.\n\n    .. seealso:: :func:`numpy.outer`\n\n    """"""\n    n = a.size\n    m = b.size\n    ret_shape = (n, m)\n\n    if out is None:\n        return core.tensordot_core(a, b, None, n, m, 1, ret_shape)\n\n    if out.size != n * m:\n        raise ValueError(\'Output array has an invalid size\')\n    if out.flags.c_contiguous:\n        return core.tensordot_core(a, b, out, n, m, 1, ret_shape)\n    else:\n        out[:] = core.tensordot_core(a, b, None, n, m, 1, ret_shape)\n        return out\n\n\ndef tensordot(a, b, axes=2):\n    """"""Returns the tensor dot product of two arrays along specified axes.\n\n    This is equivalent to compute dot product along the specified axes which\n    are treated as one axis by reshaping.\n\n    Args:\n        a (cupy.ndarray): The first argument.\n        b (cupy.ndarray): The second argument.\n        axes:\n            - If it is an integer, then ``axes`` axes at the last of ``a`` and\n              the first of ``b`` are used.\n            - If it is a pair of sequences of integers, then these two\n              sequences specify the list of axes for ``a`` and ``b``. The\n              corresponding axes are paired for sum-product.\n\n    Returns:\n        cupy.ndarray: The tensor dot product of ``a`` and ``b`` along the\n        axes specified by ``axes``.\n\n    .. seealso:: :func:`numpy.tensordot`\n\n    """"""\n    a_ndim = a.ndim\n    b_ndim = b.ndim\n    if a_ndim == 0 or b_ndim == 0:\n        if axes != 0 and axes != ((), ()):\n            raise ValueError(\'An input is zero-dim while axes has dimensions\')\n        return cupy.multiply(a, b)\n\n    if isinstance(axes, collections.abc.Sequence):\n        if len(axes) != 2:\n            raise ValueError(\'Axes must consist of two arrays.\')\n        a_axes, b_axes = axes\n        if numpy.isscalar(a_axes):\n            a_axes = a_axes,\n        if numpy.isscalar(b_axes):\n            b_axes = b_axes,\n    else:\n        a_axes = tuple(range(a_ndim - axes, a_ndim))\n        b_axes = tuple(range(axes))\n\n    sum_ndim = len(a_axes)\n    if sum_ndim != len(b_axes):\n        raise ValueError(\'Axes length mismatch\')\n\n    for a_axis, b_axis in zip(a_axes, b_axes):\n        if a.shape[a_axis] != b.shape[b_axis]:\n            raise ValueError(\'Axis dimension mismatch\')\n\n    # Make the axes non-negative\n    a = _move_axes_to_head(a, [axis % a_ndim for axis in a_axes])\n    b = _move_axes_to_head(b, [axis % b_ndim for axis in b_axes])\n\n    ret_shape = a.shape[sum_ndim:] + b.shape[sum_ndim:]\n\n    k = internal.prod(a.shape[:sum_ndim])\n    # Avoid division by zero: core.tensordot_core returns zeros without\n    # checking n, m consistency, thus allowing 0-length dimensions to work\n    n = a.size // k if k != 0 else 0\n    m = b.size // k if k != 0 else 0\n\n    return core.tensordot_core(a, b, None, n, m, k, ret_shape)\n\n\ndef matrix_power(M, n):\n    """"""Raise a square matrix to the (integer) power `n`.\n\n    Args:\n        M (~cupy.ndarray): Matrix to raise by power n.\n        n (~int): Power to raise matrix to.\n\n    Returns:\n        ~cupy.ndarray: Output array.\n\n    .. note:: M must be of dtype `float32` or `float64`.\n\n    ..seealso:: :func:`numpy.linalg.matrix_power`\n    """"""\n    if M.ndim != 2 or M.shape[0] != M.shape[1]:\n        raise ValueError(\'input must be a square array\')\n    if not isinstance(n, int):\n        raise TypeError(\'exponent must be an integer\')\n\n    if n == 0:\n        return cupy.identity(M.shape[0], dtype=M.dtype)\n    elif n < 0:\n        M = inv(M)\n        n *= -1\n\n    # short-cuts\n    if n <= 3:\n        if n == 1:\n            return M\n        elif n == 2:\n            return cupy.matmul(M, M)\n        else:\n            return cupy.matmul(cupy.matmul(M, M), M)\n\n    # binary decomposition to reduce the number of Matrix\n    # multiplications for n > 3.\n    result, Z = None, None\n    for b in cupy.binary_repr(n)[::-1]:\n        Z = M if Z is None else cupy.matmul(Z, Z)\n        if b == \'1\':\n            result = Z if result is None else cupy.matmul(result, Z)\n\n    return result\n\n\ndef kron(a, b):\n    """"""Returns the kronecker product of two arrays.\n\n    Args:\n        a (~cupy.ndarray): The first argument.\n        b (~cupy.ndarray): The second argument.\n\n    Returns:\n        ~cupy.ndarray: Output array.\n\n    .. seealso:: :func:`numpy.kron`\n\n    """"""\n    a_ndim = a.ndim\n    b_ndim = b.ndim\n    if a_ndim == 0 or b_ndim == 0:\n        return cupy.multiply(a, b)\n\n    ndim = b_ndim\n    a_shape = a.shape\n    b_shape = b.shape\n    if a_ndim != b_ndim:\n        if b_ndim > a_ndim:\n            a_shape = (1,) * (b_ndim - a_ndim) + a_shape\n        else:\n            b_shape = (1,) * (a_ndim - b_ndim) + b_shape\n            ndim = a_ndim\n\n    axis = ndim - 1\n    out = core.tensordot_core(a, b, None, a.size, b.size, 1, a_shape + b_shape)\n    for _ in range(ndim):\n        out = core.concatenate_method(out, axis=axis)\n\n    return out\n\n\ndef _move_axes_to_head(a, axes):\n    # This function moves the axes of ``s`` to the head of the shape.\n    for idx, axis in enumerate(axes):\n        if idx != axis:\n            break\n    else:\n        return a\n\n    return a.transpose(\n        axes + [i for i in range(a.ndim) if i not in axes])\n'"
cupy/linalg/solve.py,0,"b'import numpy\nfrom numpy import linalg\n\nimport cupy\nfrom cupy.core import core\nfrom cupy.cuda import cublas\nfrom cupy.cuda import cusolver\nfrom cupy.cuda import device\nfrom cupy.linalg import decomposition\nfrom cupy.linalg import util\n\n\ndef solve(a, b):\n    """"""Solves a linear matrix equation.\n\n    It computes the exact solution of ``x`` in ``ax = b``,\n    where ``a`` is a square and full rank matrix.\n\n    Args:\n        a (cupy.ndarray): The matrix with dimension ``(..., M, M)``.\n        b (cupy.ndarray): The matrix with dimension ``(...,M)`` or\n            ``(..., M, K)``.\n\n    Returns:\n        cupy.ndarray:\n            The matrix with dimension ``(..., M)`` or ``(..., M, K)``.\n\n    .. warning::\n        This function calls one or more cuSOLVER routine(s) which may yield\n        invalid results if input conditions are not met.\n        To detect these invalid results, you can set the `linalg`\n        configuration to a value that is not `ignore` in\n        :func:`cupyx.errstate` or :func:`cupyx.seterr`.\n\n    .. seealso:: :func:`numpy.linalg.solve`\n    """"""\n    # NOTE: Since cusolver in CUDA 8.0 does not support gesv,\n    #       we manually solve a linear system with QR decomposition.\n    #       For details, please see the following:\n    #       https://docs.nvidia.com/cuda/cusolver/index.html#qr_examples\n    util._assert_cupy_array(a, b)\n    util._assert_nd_squareness(a)\n\n    if not ((a.ndim == b.ndim or a.ndim == b.ndim + 1) and\n            a.shape[:-1] == b.shape[:a.ndim - 1]):\n        raise ValueError(\n            \'a must have (..., M, M) shape and b must have (..., M) \'\n            \'or (..., M, K)\')\n\n    # Cast to float32 or float64\n    if a.dtype.char == \'f\' or a.dtype.char == \'d\':\n        dtype = a.dtype\n    else:\n        dtype = numpy.promote_types(a.dtype.char, \'f\')\n\n    cublas_handle = device.get_cublas_handle()\n    cusolver_handle = device.get_cusolver_handle()\n\n    a = a.astype(dtype)\n    b = b.astype(dtype)\n    if a.ndim == 2:\n        return _solve(a, b, cublas_handle, cusolver_handle)\n\n    x = cupy.empty_like(b)\n    shape = a.shape[:-2]\n    for i in range(numpy.prod(shape)):\n        index = numpy.unravel_index(i, shape)\n        x[index] = _solve(a[index], b[index], cublas_handle, cusolver_handle)\n    return x\n\n\ndef _solve(a, b, cublas_handle, cusolver_handle):\n    a = cupy.asfortranarray(a)\n    b = cupy.asfortranarray(b)\n    dtype = a.dtype\n    m, k = (b.size, 1) if b.ndim == 1 else b.shape\n    dev_info = cupy.empty(1, dtype=numpy.int32)\n\n    if dtype == \'f\':\n        geqrf = cusolver.sgeqrf\n        geqrf_bufferSize = cusolver.sgeqrf_bufferSize\n        ormqr = cusolver.sormqr\n        ormqr_bufferSize = cusolver.sormqr_bufferSize\n        trans = cublas.CUBLAS_OP_T\n        trsm = cublas.strsm\n    elif dtype == \'d\':\n        geqrf = cusolver.dgeqrf\n        geqrf_bufferSize = cusolver.dgeqrf_bufferSize\n        ormqr = cusolver.dormqr\n        ormqr_bufferSize = cusolver.dormqr_bufferSize\n        trans = cublas.CUBLAS_OP_T\n        trsm = cublas.dtrsm\n    elif dtype == \'F\':\n        geqrf = cusolver.cgeqrf\n        geqrf_bufferSize = cusolver.cgeqrf_bufferSize\n        ormqr = cusolver.cormqr\n        ormqr_bufferSize = cusolver.cunmqr_bufferSize\n        trans = cublas.CUBLAS_OP_C\n        trsm = cublas.ctrsm\n    elif dtype == \'D\':\n        geqrf = cusolver.zgeqrf\n        geqrf_bufferSize = cusolver.zgeqrf_bufferSize\n        ormqr = cusolver.zormqr\n        ormqr_bufferSize = cusolver.zunmqr_bufferSize\n        trans = cublas.CUBLAS_OP_C\n        trsm = cublas.ztrsm\n    else:\n        raise NotImplementedError(dtype)\n\n    # 1. QR decomposition (A = Q * R)\n    buffersize = geqrf_bufferSize(cusolver_handle, m, m, a.data.ptr, m)\n    workspace = cupy.empty(buffersize, dtype=dtype)\n    tau = cupy.empty(m, dtype=dtype)\n    geqrf(\n        cusolver_handle, m, m, a.data.ptr, m, tau.data.ptr, workspace.data.ptr,\n        buffersize, dev_info.data.ptr)\n    cupy.linalg.util._check_cusolver_dev_info_if_synchronization_allowed(\n        geqrf, dev_info)\n    # Explicitly free the space allocated by geqrf\n    del workspace\n    # 2. ormqr (Q^T * B)\n    buffersize = ormqr_bufferSize(\n        cusolver_handle, cublas.CUBLAS_SIDE_LEFT, trans, m, k, m, a.data.ptr,\n        m, tau.data.ptr, b.data.ptr, m)\n    workspace = cupy.empty(buffersize, dtype=dtype)\n    ormqr(\n        cusolver_handle, cublas.CUBLAS_SIDE_LEFT, trans, m, k, m, a.data.ptr,\n        m, tau.data.ptr, b.data.ptr, m, workspace.data.ptr, buffersize,\n        dev_info.data.ptr)\n    cupy.linalg.util._check_cusolver_dev_info_if_synchronization_allowed(\n        ormqr, dev_info)\n\n    # Explicitly free the space allocated by ormqr\n    del workspace\n    # 3. trsm (X = R^{-1} * (Q^T * B))\n    trsm(\n        cublas_handle, cublas.CUBLAS_SIDE_LEFT, cublas.CUBLAS_FILL_MODE_UPPER,\n        cublas.CUBLAS_OP_N, cublas.CUBLAS_DIAG_NON_UNIT,\n        m, k, 1, a.data.ptr, m, b.data.ptr, m)\n    return b\n\n\ndef tensorsolve(a, b, axes=None):\n    """"""Solves tensor equations denoted by ``ax = b``.\n\n    Suppose that ``b`` is equivalent to ``cupy.tensordot(a, x)``.\n    This function computes tensor ``x`` from ``a`` and ``b``.\n\n    Args:\n        a (cupy.ndarray): The tensor with ``len(shape) >= 1``\n        b (cupy.ndarray): The tensor with ``len(shape) >= 1``\n        axes (tuple of ints): Axes in ``a`` to reorder to the right\n            before inversion.\n\n    Returns:\n        cupy.ndarray:\n            The tensor with shape ``Q`` such that ``b.shape + Q == a.shape``.\n\n    .. warning::\n        This function calls one or more cuSOLVER routine(s) which may yield\n        invalid results if input conditions are not met.\n        To detect these invalid results, you can set the `linalg`\n        configuration to a value that is not `ignore` in\n        :func:`cupyx.errstate` or :func:`cupyx.seterr`.\n\n    .. seealso:: :func:`numpy.linalg.tensorsolve`\n    """"""\n    if axes is not None:\n        allaxes = list(range(a.ndim))\n        for k in axes:\n            allaxes.remove(k)\n            allaxes.insert(a.ndim, k)\n        a = a.transpose(allaxes)\n\n    oldshape = a.shape[-(a.ndim - b.ndim):]\n    prod = cupy.core.internal.prod(oldshape)\n\n    a = a.reshape(-1, prod)\n    b = b.ravel()\n    result = solve(a, b)\n    return result.reshape(oldshape)\n\n\ndef lstsq(a, b, rcond=1e-15):\n    """"""Return the least-squares solution to a linear matrix equation.\n\n    Solves the equation `a x = b` by computing a vector `x` that\n    minimizes the Euclidean 2-norm `|| b - a x ||^2`.  The equation may\n    be under-, well-, or over- determined (i.e., the number of\n    linearly independent rows of `a` can be less than, equal to, or\n    greater than its number of linearly independent columns).  If `a`\n    is square and of full rank, then `x` (but for round-off error) is\n    the ""exact"" solution of the equation.\n\n    Args:\n        a (cupy.ndarray): ""Coefficient"" matrix with dimension ``(M, N)``\n        b (cupy.ndarray): ""Dependent variable"" values with dimension ``(M,)``\n            or ``(M, K)``\n        rcond (float): Cutoff parameter for small singular values.\n            For stability it computes the largest singular value denoted by\n            ``s``, and sets all singular values smaller than ``s`` to zero.\n\n    Returns:\n        tuple:\n            A tuple of ``(x, residuals, rank, s)``. Note ``x`` is the\n            least-squares solution with shape ``(N,)`` or ``(N, K)`` depending\n            if ``b`` was two-dimensional. The sums of ``residuals`` is the\n            squared Euclidean 2-norm for each column in b - a*x. The\n            ``residuals`` is an empty array if the rank of a is < N or M <= N,\n            but  iff b is 1-dimensional, this is a (1,) shape array, Otherwise\n            the shape is (K,). The ``rank`` of matrix ``a`` is an integer. The\n            singular values of ``a`` are ``s``.\n\n    .. warning::\n        This function calls one or more cuSOLVER routine(s) which may yield\n        invalid results if input conditions are not met.\n        To detect these invalid results, you can set the `linalg`\n        configuration to a value that is not `ignore` in\n        :func:`cupyx.errstate` or :func:`cupyx.seterr`.\n\n    .. seealso:: :func:`numpy.linalg.lstsq`\n    """"""\n    util._assert_cupy_array(a, b)\n    util._assert_rank2(a)\n    if b.ndim > 2:\n        raise linalg.LinAlgError(\'{}-dimensional array given. Array must be at\'\n                                 \' most two-dimensional\'.format(b.ndim))\n    m, n = a.shape[-2:]\n    m2 = b.shape[0]\n    if m != m2:\n        raise linalg.LinAlgError(\'Incompatible dimensions\')\n\n    u, s, vt = cupy.linalg.svd(a, full_matrices=False)\n    # number of singular values and matrix rank\n    cutoff = rcond * s.max()\n    s1 = 1 / s\n    sing_vals = s <= cutoff\n    s1[sing_vals] = 0\n    rank = s.size - sing_vals.sum()\n\n    if b.ndim == 2:\n        s1 = cupy.repeat(s1.reshape(-1, 1), b.shape[1], axis=1)\n    # Solve the least-squares solution\n    z = core.dot(u.transpose(), b) * s1\n    x = core.dot(vt.transpose(), z)\n    # Calculate squared Euclidean 2-norm for each column in b - a*x\n    if rank != n or m <= n:\n        resids = cupy.array([], dtype=a.dtype)\n    elif b.ndim == 2:\n        e = b - core.dot(a, x)\n        resids = cupy.sum(cupy.square(e), axis=0)\n    else:\n        e = b - cupy.dot(a, x)\n        resids = cupy.dot(e.T, e).reshape(-1)\n    return x, resids, rank, s\n\n\ndef inv(a):\n    """"""Computes the inverse of a matrix.\n\n    This function computes matrix ``a_inv`` from n-dimensional regular matrix\n    ``a`` such that ``dot(a, a_inv) == eye(n)``.\n\n    Args:\n        a (cupy.ndarray): The regular matrix\n\n    Returns:\n        cupy.ndarray: The inverse of a matrix.\n\n    .. warning::\n        This function calls one or more cuSOLVER routine(s) which may yield\n        invalid results if input conditions are not met.\n        To detect these invalid results, you can set the `linalg`\n        configuration to a value that is not `ignore` in\n        :func:`cupyx.errstate` or :func:`cupyx.seterr`.\n\n    .. seealso:: :func:`numpy.linalg.inv`\n    """"""\n    if a.ndim >= 3:\n        return _batched_inv(a)\n\n    # to prevent `a` to be overwritten\n    a = a.copy()\n\n    util._assert_cupy_array(a)\n    util._assert_rank2(a)\n    util._assert_nd_squareness(a)\n\n    # support float32, float64, complex64, and complex128\n    if a.dtype.char in \'fdFD\':\n        dtype = a.dtype.char\n    else:\n        dtype = numpy.promote_types(a.dtype.char, \'f\')\n\n    cusolver_handle = device.get_cusolver_handle()\n    dev_info = cupy.empty(1, dtype=numpy.int32)\n\n    ipiv = cupy.empty((a.shape[0], 1), dtype=numpy.intc)\n\n    if dtype == \'f\':\n        getrf = cusolver.sgetrf\n        getrf_bufferSize = cusolver.sgetrf_bufferSize\n        getrs = cusolver.sgetrs\n    elif dtype == \'d\':\n        getrf = cusolver.dgetrf\n        getrf_bufferSize = cusolver.dgetrf_bufferSize\n        getrs = cusolver.dgetrs\n    elif dtype == \'F\':\n        getrf = cusolver.cgetrf\n        getrf_bufferSize = cusolver.cgetrf_bufferSize\n        getrs = cusolver.cgetrs\n    elif dtype == \'D\':\n        getrf = cusolver.zgetrf\n        getrf_bufferSize = cusolver.zgetrf_bufferSize\n        getrs = cusolver.zgetrs\n    else:\n        msg = (\'dtype must be float32, float64, complex64 or complex128\'\n               \' (actual: {})\'.format(a.dtype))\n        raise ValueError(msg)\n\n    m = a.shape[0]\n\n    buffersize = getrf_bufferSize(cusolver_handle, m, m, a.data.ptr, m)\n    workspace = cupy.empty(buffersize, dtype=dtype)\n\n    # LU factorization\n    getrf(\n        cusolver_handle, m, m, a.data.ptr, m, workspace.data.ptr,\n        ipiv.data.ptr, dev_info.data.ptr)\n    cupy.linalg.util._check_cusolver_dev_info_if_synchronization_allowed(\n        getrf, dev_info)\n\n    b = cupy.eye(m, dtype=dtype)\n\n    # solve for the inverse\n    getrs(\n        cusolver_handle, 0, m, m, a.data.ptr, m, ipiv.data.ptr, b.data.ptr, m,\n        dev_info.data.ptr)\n    cupy.linalg.util._check_cusolver_dev_info_if_synchronization_allowed(\n        getrs, dev_info)\n\n    return b\n\n\ndef _batched_inv(a):\n\n    assert(a.ndim >= 3)\n    util._assert_cupy_array(a)\n    util._assert_nd_squareness(a)\n\n    if a.dtype == cupy.float32:\n        getrf = cupy.cuda.cublas.sgetrfBatched\n        getri = cupy.cuda.cublas.sgetriBatched\n    elif a.dtype == cupy.float64:\n        getrf = cupy.cuda.cublas.dgetrfBatched\n        getri = cupy.cuda.cublas.dgetriBatched\n    elif a.dtype == cupy.complex64:\n        getrf = cupy.cuda.cublas.cgetrfBatched\n        getri = cupy.cuda.cublas.cgetriBatched\n    elif a.dtype == cupy.complex128:\n        getrf = cupy.cuda.cublas.zgetrfBatched\n        getri = cupy.cuda.cublas.zgetriBatched\n    else:\n        msg = (\'dtype must be float32, float64, complex64 or complex128\'\n               \' (actual: {})\'.format(a.dtype))\n        raise ValueError(msg)\n\n    if 0 in a.shape:\n        return cupy.empty_like(a)\n    a_shape = a.shape\n\n    # copy is necessary to present `a` to be overwritten.\n    a = a.copy().reshape(-1, a_shape[-2], a_shape[-1])\n\n    handle = device.get_cublas_handle()\n    batch_size = a.shape[0]\n    n = a.shape[1]\n    lda = n\n    step = n * lda * a.itemsize\n    start = a.data.ptr\n    stop = start + step * batch_size\n    a_array = cupy.arange(start, stop, step, dtype=cupy.uintp)\n    pivot_array = cupy.empty((batch_size, n), dtype=cupy.int32)\n    info_array = cupy.empty((batch_size,), dtype=cupy.int32)\n\n    getrf(handle, n, a_array.data.ptr, lda, pivot_array.data.ptr,\n          info_array.data.ptr, batch_size)\n    cupy.linalg.util._check_cublas_info_array_if_synchronization_allowed(\n        getrf, info_array)\n\n    c = cupy.empty_like(a)\n    ldc = lda\n    step = n * ldc * c.itemsize\n    start = c.data.ptr\n    stop = start + step * batch_size\n    c_array = cupy.arange(start, stop, step, dtype=cupy.uintp)\n\n    getri(handle, n, a_array.data.ptr, lda, pivot_array.data.ptr,\n          c_array.data.ptr, ldc, info_array.data.ptr, batch_size)\n    cupy.linalg.util._check_cublas_info_array_if_synchronization_allowed(\n        getri, info_array)\n\n    return c.reshape(a_shape)\n\n\ndef pinv(a, rcond=1e-15):\n    """"""Compute the Moore-Penrose pseudoinverse of a matrix.\n\n    It computes a pseudoinverse of a matrix ``a``, which is a generalization\n    of the inverse matrix with Singular Value Decomposition (SVD).\n    Note that it automatically removes small singular values for stability.\n\n    Args:\n        a (cupy.ndarray): The matrix with dimension ``(M, N)``\n        rcond (float): Cutoff parameter for small singular values.\n            For stability it computes the largest singular value denoted by\n            ``s``, and sets all singular values smaller than ``s`` to zero.\n\n    Returns:\n        cupy.ndarray: The pseudoinverse of ``a`` with dimension ``(N, M)``.\n\n    .. warning::\n        This function calls one or more cuSOLVER routine(s) which may yield\n        invalid results if input conditions are not met.\n        To detect these invalid results, you can set the `linalg`\n        configuration to a value that is not `ignore` in\n        :func:`cupyx.errstate` or :func:`cupyx.seterr`.\n\n    .. seealso:: :func:`numpy.linalg.pinv`\n    """"""\n    u, s, vt = decomposition.svd(a.conj(), full_matrices=False)\n    cutoff = rcond * s.max()\n    s1 = 1 / s\n    s1[s <= cutoff] = 0\n    return core.dot(vt.T, s1[:, None] * u.T)\n\n\ndef tensorinv(a, ind=2):\n    """"""Computes the inverse of a tensor.\n\n    This function computes tensor ``a_inv`` from tensor ``a`` such that\n    ``tensordot(a_inv, a, ind) == I``, where ``I`` denotes the identity tensor.\n\n    Args:\n        a (cupy.ndarray):\n            The tensor such that\n            ``prod(a.shape[:ind]) == prod(a.shape[ind:])``.\n        ind (int):\n            The positive number used in ``axes`` option of ``tensordot``.\n\n    Returns:\n        cupy.ndarray:\n            The inverse of a tensor whose shape is equivalent to\n            ``a.shape[ind:] + a.shape[:ind]``.\n\n    .. warning::\n        This function calls one or more cuSOLVER routine(s) which may yield\n        invalid results if input conditions are not met.\n        To detect these invalid results, you can set the `linalg`\n        configuration to a value that is not `ignore` in\n        :func:`cupyx.errstate` or :func:`cupyx.seterr`.\n\n    .. seealso:: :func:`numpy.linalg.tensorinv`\n    """"""\n    util._assert_cupy_array(a)\n\n    if ind <= 0:\n        raise ValueError(\'Invalid ind argument\')\n    oldshape = a.shape\n    invshape = oldshape[ind:] + oldshape[:ind]\n    prod = cupy.core.internal.prod(oldshape[ind:])\n    a = a.reshape(prod, -1)\n    a_inv = inv(a)\n    return a_inv.reshape(*invshape)\n'"
cupy/linalg/util.py,0,"b'from numpy import linalg\n\nimport cupy\nfrom cupy import core\nimport cupyx\n\n\ndef _assert_cupy_array(*arrays):\n    for a in arrays:\n        if not isinstance(a, cupy.core.ndarray):\n            raise linalg.LinAlgError(\n                \'cupy.linalg only supports cupy.core.ndarray\')\n\n\ndef _assert_rank2(*arrays):\n    for a in arrays:\n        if a.ndim != 2:\n            raise linalg.LinAlgError(\n                \'{}-dimensional array given. Array must be \'\n                \'two-dimensional\'.format(a.ndim))\n\n\ndef _assert_nd_squareness(*arrays):\n    for a in arrays:\n        if max(a.shape[-2:]) != min(a.shape[-2:]):\n            raise linalg.LinAlgError(\n                \'Last 2 dimensions of the array must be square\')\n\n\ndef _check_cusolver_dev_info_if_synchronization_allowed(routine, dev_info):\n    # `dev_info` contains a single integer, the status code of a cuSOLVER\n    # routine call. It is referred to as ""devInfo"" in the official cuSOLVER\n    # documentation.\n    assert isinstance(dev_info, core.ndarray)\n    assert dev_info.size == 1\n    config_linalg = cupyx._ufunc_config.get_config_linalg()\n    # Only \'ignore\' and \'raise\' are currently supported.\n    if config_linalg == \'ignore\':\n        return\n\n    assert config_linalg == \'raise\'\n    dev_info_host = dev_info.item()\n    if dev_info_host != 0:\n        raise linalg.LinAlgError(\n            \'Error reported by {} in cuSOLVER. devInfo = {}. Please refer\'\n            \' to the cuSOLVER documentation.\'.format(\n                routine.__name__, dev_info_host))\n\n\ndef _check_cublas_info_array_if_synchronization_allowed(routine, info_array):\n    # `info_array` contains integers, the status codes of a cuBLAS routine\n    # call. It is referrd to as ""infoArray"" or ""devInfoArray"" in the official\n    # cuBLAS documentation.\n    assert isinstance(info_array, core.ndarray)\n    assert info_array.ndim == 1\n\n    config_linalg = cupyx._ufunc_config.get_config_linalg()\n    # Only \'ignore\' and \'raise\' are currently supported.\n    if config_linalg == \'ignore\':\n        return\n\n    assert config_linalg == \'raise\'\n    if (info_array != 0).any():\n        raise linalg.LinAlgError(\n            \'Error reported by {} in cuBLAS. infoArray/devInfoArray = {}.\'\n            \' Please refer to the cuBLAS documentation.\'.format(\n                routine.__name__, info_array))\n\n\n_tril_kernel = core.ElementwiseKernel(\n    \'int64 k\', \'S x\',\n    \'x = (_ind.get()[1] - _ind.get()[0] <= k) ? x : 0\',\n    \'tril_kernel\',\n    reduce_dims=False\n)\n\n\ndef _tril(x, k=0):\n    _tril_kernel(k, x)\n    return x\n\n\n_triu_kernel = core.ElementwiseKernel(\n    \'int64 k\', \'S x\',\n    \'x = (_ind.get()[1] - _ind.get()[0] >= k) ? x : 0\',\n    \'triu_kernel\',\n    reduce_dims=False\n)\n\n\ndef _triu(x, k=0):\n    _triu_kernel(k, x)\n    return x\n'"
cupy/logic/__init__.py,0,"b'# Functions from the following NumPy document\n# https://docs.scipy.org/doc/numpy/reference/routines.logic.html\n\n# ""NOQA"" to suppress flake8 warning\nfrom cupy.logic import comparison  # NOQA\nfrom cupy.logic import content  # NOQA\nfrom cupy.logic import ops  # NOQA\nfrom cupy.logic import truth  # NOQA\nfrom cupy.logic import type_test  # NOQA\n'"
cupy/logic/comparison.py,0,"b'import numpy\n\nimport cupy\nfrom cupy import core\n\n\n_is_close = core.create_ufunc(\n    \'cupy_is_close\',\n    (\'eeee?->?\', \'ffff?->?\', \'dddd?->?\'),\n    \'\'\'\n    bool equal_nan = in4;\n    if (isfinite(in0) && isfinite(in1)) {\n      out0 = fabs(in0 - in1) <= in3 + in2 * fabs(in1);\n    } else if (equal_nan) {\n      out0 = (in0 == in1) || (isnan(in0) && isnan(in1));\n    } else {\n      out0 = (in0 == in1);\n    }\n    \'\'\'\n)\n\n# Note that in cupy/core/include/cupy/complex.cuh, we already got isfinite and\n# isnan working for complex numbers, so just replace fabs above by abs (from\n# thrust) and we are ready to go\n_is_close_complex = core.create_ufunc(\n    \'cupy_is_close_complex\',\n    (\'FFff?->?\', \'DDdd?->?\'),\n    \'\'\'\n    bool equal_nan = in4;\n    if (isfinite(in0) && isfinite(in1)) {\n      out0 = abs(in0 - in1) <= in3 + in2 * abs(in1);\n    } else if (equal_nan) {\n      out0 = (in0 == in1) || (isnan(in0) && isnan(in1));\n    } else {\n      out0 = (in0 == in1);\n    }\n    \'\'\'\n)\n\n\ndef array_equal(a1, a2):\n    """"""Returns ``True`` if two arrays are element-wise exactly equal.\n\n    Args:\n        a1 (cupy.ndarray): Input array to compare.\n        a2 (cupy.ndarray): Input array to compare.\n\n    Returns:\n        cupy.ndarray: A boolean 0-dim array.\n            If its value is ``True``, two arrays are element-wise equal.\n\n    .. seealso:: :func:`numpy.array_equal`\n\n    """"""\n    if a1.shape != a2.shape:\n        return cupy.array(False)\n    return (a1 == a2).all()\n\n\ndef allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n    """"""Returns True if two arrays are element-wise equal within a tolerance.\n\n    Two values in ``a`` and ``b`` are  considiered equal when the following\n    equation is satisfied.\n\n    .. math::\n\n       |a - b| \\\\le \\\\mathrm{atol} + \\\\mathrm{rtol} |b|\n\n    Args:\n        a (cupy.ndarray): Input array to compare.\n        b (cupy.ndarray): Input array to compare.\n        rtol (float): The relative tolerance.\n        atol (float): The absolute tolerance.\n        equal_nan (bool): If ``True``, NaN\'s in ``a`` will be considered equal\n            to NaN\'s in ``b``.\n\n    Returns:\n        cupy.ndarray: A boolean 0-dim array.\n            If its value is ``True``, two arrays are element-wise equal within\n            a tolerance.\n\n    .. seealso:: :func:`numpy.allclose`\n\n    """"""\n    return isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan).all()\n\n\ndef isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n    """"""Returns a boolean array where two arrays are equal within a tolerance.\n\n    Two values in ``a`` and ``b`` are  considiered equal when the following\n    equation is satisfied.\n\n    .. math::\n\n       |a - b| \\\\le \\\\mathrm{atol} + \\\\mathrm{rtol} |b|\n\n    Args:\n        a (cupy.ndarray): Input array to compare.\n        b (cupy.ndarray): Input array to compare.\n        rtol (float): The relative tolerance.\n        atol (float): The absolute tolerance.\n        equal_nan (bool): If ``True``, NaN\'s in ``a`` will be considered equal\n            to NaN\'s in ``b``.\n\n    Returns:\n        cupy.ndarray: A boolean array storing where ``a`` and ``b`` are equal.\n\n    .. seealso:: :func:`numpy.isclose`\n\n    """"""\n    a = cupy.asanyarray(a)\n    b = cupy.asanyarray(b)\n    if (a.dtype in [numpy.complex64, numpy.complex128]) or \\\n       (b.dtype in [numpy.complex64, numpy.complex128]):\n        return _is_close_complex(a, b, rtol, atol, equal_nan)\n    else:\n        return _is_close(a, b, rtol, atol, equal_nan)\n\n\n# TODO(okuta): Implement array_equal\n\n\n# TODO(okuta): Implement array_equiv\n\n\ngreater = core.greater\n\n\ngreater_equal = core.greater_equal\n\n\nless = core.less\n\n\nless_equal = core.less_equal\n\n\nequal = core.equal\n\n\nnot_equal = core.not_equal\n'"
cupy/logic/content.py,0,"b""from cupy import core\n\n\ndef _create_float_test_ufunc(name, doc):\n    return core.create_ufunc(\n        'cupy_' + name,\n        ('e->?', 'f->?', 'd->?', 'F->?', 'D->?',\n         ), 'out0 = %s(in0)' % name,\n        doc=doc)\n\n\nisfinite = _create_float_test_ufunc(\n    'isfinite',\n    '''Tests finiteness elementwise.\n\n    Each element of returned array is ``True`` only if the corresponding\n    element of the input is finite (i.e. not an infinity nor NaN).\n\n    .. seealso:: :data:`numpy.isfinite`\n\n    ''')\n\n\nisinf = _create_float_test_ufunc(\n    'isinf',\n    '''Tests if each element is the positive or negative infinity.\n\n    .. seealso:: :data:`numpy.isinf`\n\n    ''')\n\n\nisnan = _create_float_test_ufunc(\n    'isnan',\n    '''Tests if each element is a NaN.\n\n    .. seealso:: :data:`numpy.isnan`\n\n    ''')\n\n\n# TODO(okuta): Implement isneginf\n\n\n# TODO(okuta): Implement isposinf\n"""
cupy/logic/ops.py,0,"b""from cupy import core\n\nlogical_and = core.create_comparison(\n    'logical_and', '&&',\n    '''Computes the logical AND of two arrays.\n\n    .. seealso:: :data:`numpy.logical_and`\n\n    ''')\n\n\nlogical_or = core.create_comparison(\n    'logical_or', '||',\n    '''Computes the logical OR of two arrays.\n\n    .. seealso:: :data:`numpy.logical_or`\n\n    ''')\n\n\nlogical_not = core.create_ufunc(\n    'cupy_logical_not',\n    ('?->?', 'b->?', 'B->?', 'h->?', 'H->?', 'i->?', 'I->?', 'l->?', 'L->?',\n     'q->?', 'Q->?', 'e->?', 'f->?', 'd->?'),\n    'out0 = !in0',\n    doc='''Computes the logical NOT of an array.\n\n    .. seealso:: :data:`numpy.logical_not`\n\n    ''')\n\n\nlogical_xor = core.create_ufunc(\n    'cupy_logical_xor',\n    ('??->?', 'bb->?', 'BB->?', 'hh->?', 'HH->?', 'ii->?', 'II->?', 'll->?',\n     'LL->?', 'qq->?', 'QQ->?', 'ee->?', 'ff->?', 'dd->?'),\n    'out0 = !in0 != !in1',\n    doc='''Computes the logical XOR of two arrays.\n\n    .. seealso:: :data:`numpy.logical_xor`\n\n    ''')\n"""
cupy/logic/truth.py,0,"b'import cupy\nfrom cupy.core import _routines_logic as _logic\nfrom cupy.core import fusion\nfrom cupy import util\n\n\ndef all(a, axis=None, out=None, keepdims=False):\n    """"""Tests whether all array elements along a given axis evaluate to True.\n\n    Args:\n        a (cupy.ndarray): Input array.\n        axis (int or tuple of ints): Along which axis to compute all.\n            The flattened array is used by default.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the axis is remained as an axis of\n            size one.\n\n    Returns:\n        cupy.ndarray: An array reduced of the input array along the axis.\n\n    .. seealso:: :func:`numpy.all`\n\n    """"""\n    if fusion._is_fusing():\n        if keepdims:\n            raise NotImplementedError(\n                \'cupy.all does not support `keepdims` in fusion yet.\')\n        return fusion._call_reduction(\n            _logic.all, a, axis=axis, out=out)\n\n    util.check_array(a, arg_name=\'a\')\n\n    return a.all(axis=axis, out=out, keepdims=keepdims)\n\n\ndef any(a, axis=None, out=None, keepdims=False):\n    """"""Tests whether any array elements along a given axis evaluate to True.\n\n    Args:\n        a (cupy.ndarray): Input array.\n        axis (int or tuple of ints): Along which axis to compute all.\n            The flattened array is used by default.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the axis is remained as an axis of\n            size one.\n\n    Returns:\n        cupy.ndarray: An array reduced of the input array along the axis.\n\n    .. seealso:: :func:`numpy.any`\n\n    """"""\n    if fusion._is_fusing():\n        if keepdims:\n            raise NotImplementedError(\n                \'cupy.any does not support `keepdims` in fusion yet.\')\n        return fusion._call_reduction(\n            _logic.any, a, axis=axis, out=out)\n\n    util.check_array(a, arg_name=\'a\')\n\n    return a.any(axis=axis, out=out, keepdims=keepdims)\n\n\ndef in1d(ar1, ar2, assume_unique=False, invert=False):\n    """"""Tests whether each element of a 1-D array is also present in a second\n    array.\n\n    Returns a boolean array the same length as ``ar1`` that is ``True``\n    where an element of ``ar1`` is in ``ar2`` and ``False`` otherwise.\n\n    Args:\n        ar1 (cupy.ndarray): Input array.\n        ar2 (cupy.ndarray): The values against which to test each value of\n            ``ar1``.\n        assume_unique (bool, optional): Ignored\n        invert (bool, optional): If ``True``, the values in the returned array\n            are inverted (that is, ``False`` where an element of ``ar1`` is in\n            ``ar2`` and ``True`` otherwise). Default is ``False``.\n\n    Returns:\n        cupy.ndarray, bool: The values ``ar1[in1d]`` are in ``ar2``.\n\n    """"""\n    # Ravel both arrays, behavior for the first array could be different\n    ar1 = ar1.ravel()\n    ar2 = ar2.ravel()\n    if ar1.size == 0 or ar2.size == 0:\n        if invert:\n            return cupy.ones(ar1.shape, dtype=cupy.bool_)\n        else:\n            return cupy.zeros(ar1.shape, dtype=cupy.bool_)\n\n    shape = (ar1.size, ar2.size)\n    ar1_broadcast = cupy.broadcast_to(ar1[..., cupy.newaxis], shape)\n    ar2_broadcast = cupy.broadcast_to(ar2, shape)\n    count = (ar1_broadcast == ar2_broadcast).sum(axis=1)\n    if invert:\n        return count == 0\n    else:\n        return count > 0\n\n\ndef isin(element, test_elements, assume_unique=False, invert=False):\n    """"""Calculates element in ``test_elements``, broadcasting over ``element``\n    only. Returns a boolean array of the same shape as ``element`` that is\n    ``True`` where an element of ``element`` is in ``test_elements`` and\n    ``False`` otherwise.\n\n    Args:\n        element (cupy.ndarray): Input array.\n        test_elements (cupy.ndarray): The values against which to test each\n            value of ``element``. This argument is flattened if it is an\n            array or array_like.\n        assume_unique (bool, optional): Ignored\n        invert (bool, optional): If ``True``, the values in the returned array\n            are inverted, as if calculating element not in ``test_elements``.\n            Default is ``False``.\n\n    Returns:\n        cupy.ndarray, bool:\n            Has the same shape as ``element``. The values ``element[isin]``\n            are in ``test_elements``.\n    """"""\n    return in1d(element, test_elements, assume_unique=assume_unique,\n                invert=invert).reshape(element.shape)\n'"
cupy/logic/type_test.py,1,"b'import numpy\n\nimport cupy\n\n\ndef iscomplex(x):\n    """"""Returns a bool array, where True if input element is complex.\n\n    What is tested is whether the input has a non-zero imaginary part, not if\n    the input type is complex.\n\n    Args:\n        x (cupy.ndarray): Input array.\n\n    Returns:\n        cupy.ndarray: Boolean array of the same shape as ``x``.\n\n    .. seealso::\n        :func:`isreal`, :func:`iscomplexobj`\n\n    Examples\n    --------\n    >>> cupy.iscomplex(cupy.array([1+1j, 1+0j, 4.5, 3, 2, 2j]))\n    array([ True, False, False, False, False,  True])\n\n    """"""\n    if numpy.isscalar(x):\n        return numpy.iscomplex(x)\n    if not isinstance(x, cupy.ndarray):\n        return cupy.asarray(numpy.iscomplex(x))\n    if x.dtype.kind == \'c\':\n        return x.imag != 0\n    return cupy.zeros(x.shape, bool)\n\n\ndef iscomplexobj(x):\n    """"""Check for a complex type or an array of complex numbers.\n\n    The type of the input is checked, not the value. Even if the input\n    has an imaginary part equal to zero, `iscomplexobj` evaluates to True.\n\n    Args:\n        x (cupy.ndarray): Input array.\n\n    Returns:\n        bool: The return value, True if ``x`` is of a complex type or\n        has at least one complex element.\n\n    .. seealso::\n        :func:`isrealobj`, :func:`iscomplex`\n\n    Examples\n    --------\n    >>> cupy.iscomplexobj(cupy.array([3, 1+0j, True]))\n    True\n    >>> cupy.iscomplexobj(cupy.array([3, 1, True]))\n    False\n\n    """"""\n    if not isinstance(x, cupy.ndarray):\n        return numpy.iscomplexobj(x)\n    return x.dtype.kind == \'c\'\n\n\ndef isfortran(a):\n    """"""Returns True if the array is Fortran contiguous but *not* C contiguous.\n\n    If you only want to check if an array is Fortran contiguous use\n    ``a.flags.f_contiguous`` instead.\n\n    Args:\n        a (cupy.ndarray): Input array.\n\n    Returns:\n        bool: The return value, True if ``a`` is Fortran contiguous but not C\n        contiguous.\n\n    .. seealso::\n       :func:`~numpy.isfortran`\n\n    Examples\n    --------\n\n    cupy.array allows to specify whether the array is written in C-contiguous\n    order (last index varies the fastest), or FORTRAN-contiguous order in\n    memory (first index varies the fastest).\n\n    >>> a = cupy.array([[1, 2, 3], [4, 5, 6]], order=\'C\')\n    >>> a\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> cupy.isfortran(a)\n    False\n\n    >>> b = cupy.array([[1, 2, 3], [4, 5, 6]], order=\'F\')\n    >>> b\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> cupy.isfortran(b)\n    True\n\n    The transpose of a C-ordered array is a FORTRAN-ordered array.\n\n    >>> a = cupy.array([[1, 2, 3], [4, 5, 6]], order=\'C\')\n    >>> a\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> cupy.isfortran(a)\n    False\n    >>> b = a.T\n    >>> b\n    array([[1, 4],\n           [2, 5],\n           [3, 6]])\n    >>> cupy.isfortran(b)\n    True\n\n    C-ordered arrays evaluate as False even if they are also FORTRAN-ordered.\n\n    >>> cupy.isfortran(np.array([1, 2], order=\'F\'))\n    False\n\n    """"""\n    return a.flags.f_contiguous and not a.flags.c_contiguous\n\n\ndef isreal(x):\n    """"""Returns a bool array, where True if input element is real.\n\n    If element has complex type with zero complex part, the return value\n    for that element is True.\n\n    Args:\n        x (cupy.ndarray): Input array.\n\n    Returns:\n        cupy.ndarray: Boolean array of same shape as ``x``.\n\n    .. seealso::\n        :func:`iscomplex`, :func:`isrealobj`\n\n    Examples\n    --------\n    >>> cupy.isreal(cp.array([1+1j, 1+0j, 4.5, 3, 2, 2j]))\n    array([False,  True,  True,  True,  True, False])\n\n    """"""\n    if numpy.isscalar(x):\n        return numpy.isreal(x)\n    if not isinstance(x, cupy.ndarray):\n        return cupy.asarray(numpy.isreal(x))\n    if x.dtype.kind == \'c\':\n        return x.imag == 0\n    return cupy.ones(x.shape, bool)\n\n\ndef isrealobj(x):\n    """"""Return True if x is a not complex type or an array of complex numbers.\n\n    The type of the input is checked, not the value. So even if the input\n    has an imaginary part equal to zero, `isrealobj` evaluates to False\n    if the data type is complex.\n\n    Args:\n        x (cupy.ndarray): The input can be of any type and shape.\n\n    Returns:\n        bool: The return value, False if ``x`` is of a complex type.\n\n    .. seealso::\n        :func:`iscomplexobj`, :func:`isreal`\n\n    Examples\n    --------\n    >>> cupy.isrealobj(cupy.array([3, 1+0j, True]))\n    False\n    >>> cupy.isrealobj(cupy.array([3, 1, True]))\n    True\n\n    """"""\n    if not isinstance(x, cupy.ndarray):\n        return numpy.isrealobj(x)\n    return x.dtype.kind != \'c\'\n'"
cupy/manipulation/__init__.py,0,"b'# Functions from the following NumPy document\n# https://docs.scipy.org/doc/numpy/reference/routines.array-manipulation.html\n\n# ""NOQA"" to suppress flake8 warning\nfrom cupy.manipulation import add_remove  # NOQA\nfrom cupy.manipulation import basic  # NOQA\nfrom cupy.manipulation import dims  # NOQA\nfrom cupy.manipulation import join  # NOQA\nfrom cupy.manipulation import kind  # NOQA\nfrom cupy.manipulation import rearrange  # NOQA\nfrom cupy.manipulation import shape  # NOQA\nfrom cupy.manipulation import split  # NOQA\nfrom cupy.manipulation import tiling  # NOQA\nfrom cupy.manipulation import transpose  # NOQA\n'"
cupy/manipulation/add_remove.py,0,"b'import cupy\n\nfrom cupy import core\n\n# TODO(okuta): Implement delete\n\n\n# TODO(okuta): Implement insert\n\n\n# TODO(okuta): Implement append\n\n\n# TODO(okuta): Implement resize\n\n_first_nonzero_krnl = core.ReductionKernel(\n    \'T data, int64 len\',\n    \'int64 y\',\n    \'data == T(0) ? len : _j\',\n    \'min(a, b)\',\n    \'y = a\',\n    \'len\',\n    \'first_nonzero\'\n)\n\n\ndef trim_zeros(filt, trim=\'fb\'):\n    """"""Trim the leading and/or trailing zeros from a 1-D array or sequence.\n\n    Returns the trimmed array\n\n    Args:\n        filt(cupy.ndarray): Input array\n        trim(str, optional):\n            \'fb\' default option trims the array from both sides.\n            \'f\' option trim zeros from front.\n            \'b\' option trim zeros from back.\n\n    Returns:\n        cupy.ndarray: trimmed input\n\n    .. seealso:: :func:`numpy.trim_zeros`\n\n    """"""\n    if filt.ndim > 1:\n        raise ValueError(\'Multi-dimensional trim is not supported\')\n    if not filt.ndim:\n        raise TypeError(\'0-d array cannot be trimmed\')\n    start = 0\n    end = filt.size\n    trim = trim.upper()\n    if \'F\' in trim:\n        start = _first_nonzero_krnl(filt, filt.size).item()\n    if \'B\' in trim:\n        end = filt.size - _first_nonzero_krnl(filt[::-1], filt.size).item()\n    return filt[start:end]\n\n\ndef unique(ar, return_index=False, return_inverse=False,\n           return_counts=False, axis=None):\n    """"""Find the unique elements of an array.\n\n    Returns the sorted unique elements of an array. There are three optional\n    outputs in addition to the unique elements:\n\n    * the indices of the input array that give the unique values\n    * the indices of the unique array that reconstruct the input array\n    * the number of times each unique value comes up in the input array\n\n    Args:\n        ar(array_like): Input array. This will be flattened if it is not\n            already 1-D.\n        return_index(bool, optional): If True, also return the indices of `ar`\n            (along the specified axis, if provided, or in the flattened array)\n            that result in the unique array.\n        return_inverse(bool, optional): If True, also return the indices of the\n            unique array (for the specified axis, if provided) that can be used\n            to reconstruct `ar`.\n        return_counts(bool, optional): If True, also return the number of times\n            each unique item appears in `ar`.\n        axis(int or None, optional): Not supported yet.\n\n    Returns:\n        cupy.ndarray or tuple:\n            If there are no optional outputs, it returns the\n            :class:`cupy.ndarray` of the sorted unique values. Otherwise, it\n            returns the tuple which contains the sorted unique values and\n            followings.\n\n            * The indices of the first occurrences of the unique values in the\n              original array. Only provided if `return_index` is True.\n            * The indices to reconstruct the original array from the\n              unique array. Only provided if `return_inverse` is True.\n            * The number of times each of the unique values comes up in the\n              original array. Only provided if `return_counts` is True.\n\n    .. warning::\n\n        This function may synchronize the device.\n\n    .. seealso:: :func:`numpy.unique`\n    """"""\n    if axis is not None:\n        raise NotImplementedError(\'axis option is not supported yet.\')\n\n    ar = cupy.asarray(ar).flatten()\n\n    if return_index or return_inverse:\n        perm = ar.argsort()\n        aux = ar[perm]\n    else:\n        ar.sort()\n        aux = ar\n    mask = cupy.empty(aux.shape, dtype=cupy.bool_)\n    mask[0] = True\n    mask[1:] = aux[1:] != aux[:-1]\n\n    ret = aux[mask]\n    if not return_index and not return_inverse and not return_counts:\n        return ret\n\n    ret = ret,\n    if return_index:\n        ret += perm[mask],\n    if return_inverse:\n        imask = cupy.cumsum(mask) - 1\n        inv_idx = cupy.empty(mask.shape, dtype=cupy.intp)\n        inv_idx[perm] = imask\n        ret += inv_idx,\n    if return_counts:\n        nonzero = cupy.nonzero(mask)[0]  # may synchronize\n        idx = cupy.empty((nonzero.size + 1,), nonzero.dtype)\n        idx[:-1] = nonzero\n        idx[-1] = mask.size\n        ret += idx[1:] - idx[:-1],\n    return ret\n'"
cupy/manipulation/basic.py,0,"b'import numpy\n\nfrom cupy import core\nfrom cupy.core import fusion\nfrom cupy._sorting import search\n\n\ndef copyto(dst, src, casting=\'same_kind\', where=None):\n    """"""Copies values from one array to another with broadcasting.\n\n    This function can be called for arrays on different devices. In this case,\n    casting, ``where``, and broadcasting is not supported, and an exception is\n    raised if these are used.\n\n    Args:\n        dst (cupy.ndarray): Target array.\n        src (cupy.ndarray): Source array.\n        casting (str): Casting rule. See :func:`numpy.can_cast` for detail.\n        where (cupy.ndarray of bool): If specified, this array acts as a mask,\n            and an element is copied only if the corresponding element of\n            ``where`` is True.\n\n    .. seealso:: :func:`numpy.copyto`\n\n    """"""\n\n    src_type = type(src)\n    src_is_python_scalar = src_type in (\n        int, bool, float, complex, fusion._FusionVarScalar)\n    if src_is_python_scalar:\n        src_dtype = numpy.dtype(type(src))\n        can_cast = numpy.can_cast(src, dst.dtype, casting)\n    else:\n        src_dtype = src.dtype\n        can_cast = numpy.can_cast(src_dtype, dst.dtype, casting)\n\n    if not can_cast:\n        raise TypeError(\'Cannot cast %s to %s in %s casting mode\' %\n                        (src_dtype, dst.dtype, casting))\n    if fusion._is_fusing():\n        if where is None:\n            core.elementwise_copy(src, dst)\n        else:\n            fusion._call_ufunc(search._where_ufunc, where, src, dst, dst)\n        return\n\n    if dst.size == 0:\n        return\n\n    if src_is_python_scalar and where is None:\n        dst.fill(src)\n        return\n\n    if where is None:\n        if _can_memcpy(dst, src):\n            dst.data.copy_from_async(src.data, src.nbytes)\n        else:\n            device = dst.device\n            with device:\n                if src.device != device:\n                    src = src.copy()\n                core.elementwise_copy(src, dst)\n    else:\n        core.elementwise_copy_where(src, where, dst)\n\n\ndef _can_memcpy(dst, src):\n    c_contiguous = dst.flags.c_contiguous and src.flags.c_contiguous\n    f_contiguous = dst.flags.f_contiguous and src.flags.f_contiguous\n    return (c_contiguous or f_contiguous) and dst.dtype == src.dtype and \\\n        dst.size == src.size\n'"
cupy/manipulation/dims.py,0,"b'import cupy\nfrom cupy import core\nimport cupy.core._routines_manipulation as _manipulation\n\n\n# Shape map for atleast_nd functions\n# (minimum dimension, input dimension) -> (output shape)\n_atleast_nd_shape_map = {\n    (1, 0): lambda shape: (1,),\n    (2, 0): lambda shape: (1, 1),\n    (2, 1): lambda shape: (1,) + shape,\n    (3, 0): lambda shape: (1, 1, 1),\n    (3, 1): lambda shape: (1,) + shape + (1,),\n    (3, 2): lambda shape: shape + (1,),\n}\n\n\ndef _atleast_nd_helper(n, arys):\n    """"""Helper function for atleast_nd functions.""""""\n\n    res = []\n    for a in arys:\n        a = cupy.asarray(a)\n        if a.ndim < n:\n            new_shape = _atleast_nd_shape_map[(n, a.ndim)](a.shape)\n            a = a.reshape(*new_shape)\n        res.append(a)\n\n    if len(res) == 1:\n        res, = res\n    return res\n\n\ndef atleast_1d(*arys):\n    """"""Converts arrays to arrays with dimensions >= 1.\n\n    Args:\n        arys (tuple of arrays): Arrays to be converted. All arguments must be\n            :class:`cupy.ndarray` objects. Only zero-dimensional array is\n            affected.\n\n    Returns:\n        If there are only one input, then it returns its converted version.\n        Otherwise, it returns a list of converted arrays.\n\n    .. seealso:: :func:`numpy.atleast_1d`\n\n    """"""\n    return _atleast_nd_helper(1, arys)\n\n\ndef atleast_2d(*arys):\n    """"""Converts arrays to arrays with dimensions >= 2.\n\n    If an input array has dimensions less than two, then this function inserts\n    new axes at the head of dimensions to make it have two dimensions.\n\n    Args:\n        arys (tuple of arrays): Arrays to be converted. All arguments must be\n            :class:`cupy.ndarray` objects.\n\n    Returns:\n        If there are only one input, then it returns its converted version.\n        Otherwise, it returns a list of converted arrays.\n\n    .. seealso:: :func:`numpy.atleast_2d`\n\n    """"""\n    return _atleast_nd_helper(2, arys)\n\n\ndef atleast_3d(*arys):\n    """"""Converts arrays to arrays with dimensions >= 3.\n\n    If an input array has dimensions less than three, then this function\n    inserts new axes to make it have three dimensions. The place of the new\n    axes are following:\n\n    - If its shape is ``()``, then the shape of output is ``(1, 1, 1)``.\n    - If its shape is ``(N,)``, then the shape of output is ``(1, N, 1)``.\n    - If its shape is ``(M, N)``, then the shape of output is ``(M, N, 1)``.\n    - Otherwise, the output is the input array itself.\n\n    Args:\n        arys (tuple of arrays): Arrays to be converted. All arguments must be\n            :class:`cupy.ndarray` objects.\n\n    Returns:\n        If there are only one input, then it returns its converted version.\n        Otherwise, it returns a list of converted arrays.\n\n    .. seealso:: :func:`numpy.atleast_3d`\n\n    """"""\n    return _atleast_nd_helper(3, arys)\n\n\nbroadcast = core.broadcast\n\n\ndef broadcast_arrays(*args):\n    """"""Broadcasts given arrays.\n\n    Args:\n        args (tuple of arrays): Arrays to broadcast for each other.\n\n    Returns:\n        list: A list of broadcasted arrays.\n\n    .. seealso:: :func:`numpy.broadcast_arrays`\n\n    """"""\n    return list(broadcast(*args).values)\n\n\ndef broadcast_to(array, shape):\n    """"""Broadcast an array to a given shape.\n\n    Args:\n        array (cupy.ndarray): Array to broadcast.\n        shape (tuple of int): The shape of the desired array.\n\n    Returns:\n        cupy.ndarray: Broadcasted view.\n\n    .. seealso:: :func:`numpy.broadcast_to`\n\n    """"""\n    return core.broadcast_to(array, shape)\n\n\ndef expand_dims(a, axis):\n    """"""Expands given arrays.\n\n    Args:\n        a (cupy.ndarray): Array to be expanded.\n        axis (int): Position where new axis is to be inserted.\n\n    Returns:\n        cupy.ndarray: The number of dimensions is one greater than that of\n            the input array.\n\n    .. seealso:: :func:`numpy.expand_dims`\n\n    """"""\n    if type(axis) not in (tuple, list):\n        axis = axis,\n    return _manipulation._expand_dims(a, axis)\n\n\ndef squeeze(a, axis=None):\n    """"""Removes size-one axes from the shape of an array.\n\n    Args:\n        a (cupy.ndarray): Array to be reshaped.\n        axis (int or tuple of ints): Axes to be removed. This function removes\n            all size-one axes by default. If one of the specified axes is not\n            of size one, an exception is raised.\n\n    Returns:\n        cupy.ndarray: An array without (specified) size-one axes.\n\n    .. seealso:: :func:`numpy.squeeze`\n\n    """"""\n    # TODO(okuta): check type\n    return a.squeeze(axis)\n'"
cupy/manipulation/join.py,0,"b'import numpy\n\nimport cupy\nfrom cupy import core\n\n\ndef column_stack(tup):\n    """"""Stacks 1-D and 2-D arrays as columns into a 2-D array.\n\n    A 1-D array is first converted to a 2-D column array. Then, the 2-D arrays\n    are concatenated along the second axis.\n\n    Args:\n        tup (sequence of arrays): 1-D or 2-D arrays to be stacked.\n\n    Returns:\n        cupy.ndarray: A new 2-D array of stacked columns.\n\n    .. seealso:: :func:`numpy.column_stack`\n\n    """"""\n    if any(not isinstance(a, cupy.ndarray) for a in tup):\n        raise TypeError(\'Only cupy arrays can be column stacked\')\n\n    lst = list(tup)\n    for i, a in enumerate(lst):\n        if a.ndim == 1:\n            a = a[:, cupy.newaxis]\n            lst[i] = a\n        elif a.ndim != 2:\n            raise ValueError(\n                \'Only 1 or 2 dimensional arrays can be column stacked\')\n\n    return concatenate(lst, axis=1)\n\n\ndef concatenate(tup, axis=0, out=None):\n    """"""Joins arrays along an axis.\n\n    Args:\n        tup (sequence of arrays): Arrays to be joined. All of these should have\n            same dimensionalities except the specified axis.\n        axis (int or None): The axis to join arrays along.\n            If axis is None, arrays are flattened before use.\n            Default is 0.\n        out (cupy.ndarray): Output array.\n\n    Returns:\n        cupy.ndarray: Joined array.\n\n    .. seealso:: :func:`numpy.concatenate`\n\n    """"""\n    if axis is None:\n        tup = [m.ravel() for m in tup]\n        axis = 0\n    return core.concatenate_method(tup, axis, out)\n\n\ndef dstack(tup):\n    """"""Stacks arrays along the third axis.\n\n    Args:\n        tup (sequence of arrays): Arrays to be stacked. Each array is converted\n            by :func:`cupy.atleast_3d` before stacking.\n\n    Returns:\n        cupy.ndarray: Stacked array.\n\n    .. seealso:: :func:`numpy.dstack`\n\n    """"""\n    return concatenate([cupy.atleast_3d(m) for m in tup], 2)\n\n\ndef hstack(tup):\n    """"""Stacks arrays horizontally.\n\n    If an input array has one dimension, then the array is treated as a\n    horizontal vector and stacked along the first axis. Otherwise, the array is\n    stacked along the second axis.\n\n    Args:\n        tup (sequence of arrays): Arrays to be stacked.\n\n    Returns:\n        cupy.ndarray: Stacked array.\n\n    .. seealso:: :func:`numpy.hstack`\n\n    """"""\n    arrs = [cupy.atleast_1d(a) for a in tup]\n    axis = 1\n    if arrs[0].ndim == 1:\n        axis = 0\n    return concatenate(arrs, axis)\n\n\ndef vstack(tup):\n    """"""Stacks arrays vertically.\n\n    If an input array has one dimension, then the array is treated as a\n    horizontal vector and stacked along the additional axis at the head.\n    Otherwise, the array is stacked along the first axis.\n\n    Args:\n        tup (sequence of arrays): Arrays to be stacked. Each array is converted\n            by :func:`cupy.atleast_2d` before stacking.\n\n    Returns:\n        cupy.ndarray: Stacked array.\n\n    .. seealso:: :func:`numpy.dstack`\n\n    """"""\n    return concatenate([cupy.atleast_2d(m) for m in tup], 0)\n\n\ndef stack(tup, axis=0, out=None):\n    """"""Stacks arrays along a new axis.\n\n    Args:\n        tup (sequence of arrays): Arrays to be stacked.\n        axis (int): Axis along which the arrays are stacked.\n        out (cupy.ndarray): Output array.\n\n    Returns:\n        cupy.ndarray: Stacked array.\n\n    .. seealso:: :func:`numpy.stack`\n    """"""\n    # TODO(okuta) Remove this if exampd_dims is updated\n    for x in tup:\n        if not (-x.ndim - 1 <= axis <= x.ndim):\n            raise numpy.AxisError(\n                \'axis {} out of bounds [{}, {}]\'.format(\n                    axis, -x.ndim - 1, x.ndim))\n    return concatenate([cupy.expand_dims(x, axis) for x in tup], axis, out)\n\n\ndef _get_positive_axis(ndim, axis):\n    a = axis\n    if a < 0:\n        a += ndim\n    if a < 0 or a >= ndim:\n        raise numpy.AxisError(\n            \'axis {} out of bounds [0, {})\'.format(axis, ndim))\n    return a\n'"
cupy/manipulation/kind.py,0,"b'import cupy\nfrom cupy import core\n\n\n# TODO(okuta): Implement asfarray\n\n\ndef asfortranarray(a, dtype=None):\n    """"""Return an array laid out in Fortran order in memory.\n\n    Args:\n        a (~cupy.ndarray): The input array.\n        dtype (str or dtype object, optional): By default, the data-type is\n            inferred from the input data.\n\n    Returns:\n        ~cupy.ndarray: The input `a` in Fortran, or column-major, order.\n\n    .. seealso:: :func:`numpy.asfortranarray`\n\n    """"""\n    return core.asfortranarray(a, dtype)\n\n\n# TODO(okuta): Implement asarray_chkfinite\n\n\ndef require(a, dtype=None, requirements=None):\n    """"""Return an array which satisfies the requirements.\n\n    Args:\n        a (~cupy.ndarray): The input array.\n        dtype (str or dtype object, optional): The required data-type.\n            If None preserve the current dtype.\n        requirements (str or list of str): The requirements can be any\n            of the following\n\n            * \'F_CONTIGUOUS\' (\'F\', \'FORTRAN\') - ensure a Fortran-contiguous \\\n                array. \\\n\n            * \'C_CONTIGUOUS\' (\'C\', \'CONTIGUOUS\') - ensure a C-contiguous array.\n\n            * \'OWNDATA\' (\'O\')      - ensure an array that owns its own data.\n\n    Returns:\n        ~cupy.ndarray: The input array ``a`` with specified requirements and\n            type if provided.\n\n    .. seealso:: :func:`numpy.require`\n\n    """"""\n\n    possible_flags = {\'C\': \'C\', \'C_CONTIGUOUS\': \'C\', \'CONTIGUOUS\': \'C\',\n                      \'F\': \'F\', \'F_CONTIGUOUS\': \'F\', \'FORTRAN\': \'F\',\n                      \'O\': \'OWNDATA\', \'OWNDATA\': \'OWNDATA\'}\n\n    if not requirements:\n        try:\n            return cupy.asanyarray(a, dtype=dtype)\n        except TypeError:\n            raise(ValueError(""Incorrect dtype \\""{}\\"" provided"".format(dtype)))\n    else:\n        try:\n            requirements = {possible_flags[x.upper()] for x in requirements}\n        except KeyError:\n            raise(ValueError(""Incorrect flag \\""{}\\"" in requirements"".format(\n                             (set(requirements) -\n                              set(possible_flags.keys())).pop())))\n\n    order = \'A\'\n    if requirements >= {\'C\', \'F\'}:\n        raise ValueError(\'Cannot specify both ""C"" and ""F"" order\')\n    elif \'F\' in requirements:\n        order = \'F_CONTIGUOUS\'\n        requirements.remove(\'F\')\n    elif \'C\' in requirements:\n        order = \'C_CONTIGUOUS\'\n        requirements.remove(\'C\')\n\n    copy = \'OWNDATA\' in requirements\n    try:\n        arr = cupy.array(a, dtype=dtype, order=order, copy=copy, subok=False)\n    except TypeError:\n        raise(ValueError(""Incorrect dtype \\""{}\\"" provided"".format(dtype)))\n    return arr\n'"
cupy/manipulation/rearrange.py,0,"b'import itertools\n\nimport numpy\n\nimport cupy\nfrom cupy.core._reduction import _get_axis\n\n\ndef flip(a, axis):\n    """"""Reverse the order of elements in an array along the given axis.\n\n    Note that ``flip`` function has been introduced since NumPy v1.12.\n    The contents of this document is the same as the original one.\n\n    Args:\n        a (~cupy.ndarray): Input array.\n        axis (int): Axis in array, which entries are reversed.\n\n    Returns:\n        ~cupy.ndarray: Output array.\n\n    .. seealso:: :func:`numpy.flip`\n\n    """"""\n    a_ndim = a.ndim\n    if a_ndim < 1:\n        raise numpy.AxisError(\'Input must be >= 1-d\')\n\n    axis = int(axis)\n    if not -a_ndim <= axis < a_ndim:\n        raise numpy.AxisError(\n            \'axis must be >= %d and < %d\' % (-a_ndim, a_ndim))\n\n    return _flip(a, axis)\n\n\ndef fliplr(a):\n    """"""Flip array in the left/right direction.\n\n    Flip the entries in each row in the left/right direction. Columns\n    are preserved, but appear in a different order than before.\n\n    Args:\n        a (~cupy.ndarray): Input array.\n\n    Returns:\n        ~cupy.ndarray: Output array.\n\n    .. seealso:: :func:`numpy.fliplr`\n\n    """"""\n    if a.ndim < 2:\n        raise ValueError(\'Input must be >= 2-d\')\n    return a[::, ::-1]\n\n\ndef flipud(a):\n    """"""Flip array in the up/down direction.\n\n    Flip the entries in each column in the up/down direction. Rows are\n    preserved, but appear in a different order than before.\n\n    Args:\n        a (~cupy.ndarray): Input array.\n\n    Returns:\n        ~cupy.ndarray: Output array.\n\n    .. seealso:: :func:`numpy.flipud`\n\n    """"""\n    if a.ndim < 1:\n        raise ValueError(\'Input must be >= 1-d\')\n    return a[::-1]\n\n\ndef roll(a, shift, axis=None):\n    """"""Roll array elements along a given axis.\n\n    Elements that roll beyond the last position are re-introduced at the first.\n\n    Args:\n        a (~cupy.ndarray): Array to be rolled.\n        shift (int or tuple of int): The number of places by which elements are\n            shifted. If a tuple, then `axis` must be a tuple of the same size,\n            and each of the given axes is shifted by the corresponding number.\n            If an int while `axis` is a tuple of ints, then the same value is\n            used for all given axes.\n        axis (int or tuple of int or None): The axis along which elements are\n            shifted. By default, the array is flattened before shifting, after\n            which the original shape is restored.\n\n    Returns:\n        ~cupy.ndarray: Output array.\n\n    .. seealso:: :func:`numpy.roll`\n\n    """"""\n    if axis is None:\n        return roll(a.ravel(), shift, 0).reshape(a.shape)\n    else:\n        axis = _get_axis(axis, a.ndim)[0]\n\n        broadcasted = numpy.broadcast(shift, axis)\n        if broadcasted.nd > 1:\n            raise ValueError(\n                \'\\\'shift\\\' and \\\'axis\\\' should be scalars or 1D sequences\')\n        shifts = {ax: 0 for ax in range(a.ndim)}\n        for sh, ax in broadcasted:\n            shifts[ax] += sh\n\n        rolls = [((slice(None), slice(None)),)] * a.ndim\n        for ax, offset in shifts.items():\n            offset %= a.shape[ax] or 1  # If `a` is empty, nothing matters.\n            if offset:\n                # (original, result), (original, result)\n                rolls[ax] = ((slice(None, -offset), slice(offset, None)),\n                             (slice(-offset, None), slice(None, offset)))\n\n        result = cupy.empty_like(a)\n        for indices in itertools.product(*rolls):\n            arr_index, res_index = zip(*indices)\n            result[res_index] = a[arr_index]\n\n        return result\n\n\ndef rot90(a, k=1, axes=(0, 1)):\n    """"""Rotate an array by 90 degrees in the plane specified by axes.\n\n    Note that ``axes`` argument has been introduced since NumPy v1.12.\n    The contents of this document is the same as the original one.\n\n    Args:\n        a (~cupy.ndarray): Array of two or more dimensions.\n        k (int): Number of times the array is rotated by 90 degrees.\n        axes: (tuple of ints): The array is rotated in the plane defined by\n            the axes. Axes must be different.\n\n    Returns:\n        ~cupy.ndarray: Output array.\n\n    .. seealso:: :func:`numpy.rot90`\n\n    """"""\n    a_ndim = a.ndim\n    if a_ndim < 2:\n        raise ValueError(\'Input must be >= 2-d\')\n\n    axes = tuple(axes)\n    if len(axes) != 2:\n        raise ValueError(\'len(axes) must be 2\')\n    if axes[0] == axes[1] or abs(axes[0] - axes[1]) == a_ndim:\n        raise ValueError(\'axes must be different\')\n    if not (-a_ndim <= axes[0] < a_ndim and -a_ndim <= axes[1] < a_ndim):\n        raise ValueError(\'axes must be >= %d and < %d\' % (-a_ndim, a_ndim))\n\n    k = k % 4\n\n    if k == 0:\n        return a[:]\n    if k == 2:\n        return _flip(_flip(a, axes[0]), axes[1])\n\n    axes_t = list(range(0, a_ndim))\n    axes_t[axes[0]], axes_t[axes[1]] = axes_t[axes[1]], axes_t[axes[0]]\n\n    if k == 1:\n        return cupy.transpose(_flip(a, axes[1]), axes_t)\n    else:\n        return _flip(cupy.transpose(a, axes_t), axes[1])\n\n\ndef _flip(a, axis):\n    # This function flips array without checking args.\n    indexer = [slice(None)] * a.ndim\n    indexer[axis] = slice(None, None, -1)\n\n    return a[tuple(indexer)]\n'"
cupy/manipulation/shape.py,0,"b'def reshape(a, newshape, order=\'C\'):\n    """"""Returns an array with new shape and same elements.\n\n    It tries to return a view if possible, otherwise returns a copy.\n\n    Args:\n        a (cupy.ndarray): Array to be reshaped.\n        newshape (int or tuple of ints): The new shape of the array to return.\n            If it is an integer, then it is treated as a tuple of length one.\n            It should be compatible with ``a.size``. One of the elements can be\n            -1, which is automatically replaced with the appropriate value to\n            make the shape compatible with ``a.size``.\n        order ({\'C\', \'F\', \'A\'}):\n            Read the elements of ``a`` using this index order, and place the\n            elements into the reshaped array using this index order.\n            \'C\' means to read / write the elements using C-like index order,\n            with the last axis index changing fastest, back to the first axis\n            index changing slowest. \'F\' means to read / write the elements\n            using Fortran-like index order, with the first index changing\n            fastest, and the last index changing slowest. Note that the \'C\'\n            and \'F\' options take no account of the memory layout of the\n            underlying array, and only refer to the order of indexing. \'A\'\n            means to read / write the elements in Fortran-like index order if\n            a is Fortran contiguous in memory, C-like order otherwise.\n\n    Returns:\n        cupy.ndarray: A reshaped view of ``a`` if possible, otherwise a copy.\n\n    .. seealso:: :func:`numpy.reshape`\n\n    """"""\n    # TODO(okuta): check type\n    return a.reshape(newshape, order=order)\n\n\ndef ravel(a, order=\'C\'):\n    """"""Returns a flattened array.\n\n    It tries to return a view if possible, otherwise returns a copy.\n\n    This function currently does not support the ``order = \'K\'`` option.\n\n    Args:\n        a (cupy.ndarray): Array to be flattened.\n        order ({\'C\', \'F\', \'A\'}):\n            Read the elements of ``a`` using this index order, and place the\n            elements into the reshaped array using this index order.\n            \'C\' means to read / write the elements using C-like index order,\n            with the last axis index changing fastest, back to the first axis\n            index changing slowest. \'F\' means to read / write the elements\n            using Fortran-like index order, with the first index changing\n            fastest, and the last index changing slowest. Note that the \'C\'\n            and \'F\' options take no account of the memory layout of the\n            underlying array, and only refer to the order of indexing. \'A\'\n            means to read / write the elements in Fortran-like index order if\n            a is Fortran contiguous in memory, C-like order otherwise.\n\n    Returns:\n        cupy.ndarray: A flattened view of ``a`` if possible, otherwise a copy.\n\n    .. seealso:: :func:`numpy.ravel`\n\n    """"""\n    # TODO(beam2d, grlee77): Support ordering option K\n    # TODO(okuta): check type\n    return a.ravel(order)\n'"
cupy/manipulation/split.py,0,"b'import numpy\n\nfrom cupy import core\n\n\ndef array_split(ary, indices_or_sections, axis=0):\n    """"""Splits an array into multiple sub arrays along a given axis.\n\n    This function is almost equivalent to :func:`cupy.split`. The only\n    difference is that this function allows an integer sections that does not\n    evenly divide the axis.\n\n    .. seealso:: :func:`cupy.split` for more detail, :func:`numpy.array_split`\n\n    """"""\n    return core.array_split(ary, indices_or_sections, axis)\n\n\ndef dsplit(ary, indices_or_sections):\n    """"""Splits an array into multiple sub arrays along the third axis.\n\n    This is equivalent to ``split`` with ``axis=2``.\n\n    .. seealso:: :func:`cupy.split` for more detail, :func:`numpy.dsplit`\n\n    """"""\n    if ary.ndim <= 2:\n        raise ValueError(\'Cannot dsplit an array with less than 3 dimensions\')\n    return split(ary, indices_or_sections, 2)\n\n\ndef hsplit(ary, indices_or_sections):\n    """"""Splits an array into multiple sub arrays horizontally.\n\n    This is equivalent to ``split`` with ``axis=0`` if ``ary`` has one\n    dimension, and otherwise that with ``axis=1``.\n\n    .. seealso:: :func:`cupy.split` for more detail, :func:`numpy.hsplit`\n\n    """"""\n    if ary.ndim == 0:\n        raise ValueError(\'Cannot hsplit a zero-dimensional array\')\n    if ary.ndim == 1:\n        return split(ary, indices_or_sections, 0)\n    else:\n        return split(ary, indices_or_sections, 1)\n\n\ndef split(ary, indices_or_sections, axis=0):\n    """"""Splits an array into multiple sub arrays along a given axis.\n\n    Args:\n        ary (cupy.ndarray): Array to split.\n        indices_or_sections (int or sequence of ints): A value indicating how\n            to divide the axis. If it is an integer, then is treated as the\n            number of sections, and the axis is evenly divided. Otherwise,\n            the integers indicate indices to split at. Note that the sequence\n            on the device memory is not allowed.\n        axis (int): Axis along which the array is split.\n\n    Returns:\n        A list of sub arrays. Each array is a view of the corresponding input\n        array.\n\n    .. seealso:: :func:`numpy.split`\n\n    """"""\n    if ary.ndim <= axis:\n        raise IndexError(\'Axis exceeds ndim\')\n    size = ary.shape[axis]\n\n    if numpy.isscalar(indices_or_sections):\n        if size % indices_or_sections != 0:\n            raise ValueError(\n                \'indices_or_sections must divide the size along the axes.\\n\'\n                \'If you want to split the array into non-equally-sized \'\n                \'arrays, use array_split instead.\')\n    return array_split(ary, indices_or_sections, axis)\n\n\ndef vsplit(ary, indices_or_sections):\n    """"""Splits an array into multiple sub arrays along the first axis.\n\n    This is equivalent to ``split`` with ``axis=0``.\n\n    .. seealso:: :func:`cupy.split` for more detail, :func:`numpy.dsplit`\n\n    """"""\n    if ary.ndim <= 1:\n        raise ValueError(\'Cannot vsplit an array with less than 2 dimensions\')\n    return split(ary, indices_or_sections, 0)\n'"
cupy/manipulation/tiling.py,0,"b'import cupy\n\n\ndef tile(A, reps):\n    """"""Construct an array by repeating A the number of times given by reps.\n\n    Args:\n        A (cupy.ndarray): Array to transform.\n        reps (int or tuple): The number of repeats.\n\n    Returns:\n        cupy.ndarray: Transformed array with repeats.\n\n    .. seealso:: :func:`numpy.tile`\n\n    """"""\n    try:\n        tup = tuple(reps)\n    except TypeError:\n        tup = (reps,)\n    d = len(tup)\n    if tup.count(1) == len(tup) and isinstance(A, cupy.ndarray):\n        # Fixes the problem that the function does not make a copy if A is a\n        # array and the repetitions are 1 in all dimensions\n        return cupy.array(A, copy=True, ndmin=d)\n    else:\n        # Note that no copy of zero-sized arrays is made. However since they\n        # have no data there is no risk of an inadvertent overwrite.\n        c = cupy.array(A, copy=False, ndmin=d)\n    if d < c.ndim:\n        tup = (1,) * (c.ndim - d) + tup\n    shape_out = tuple(s * t for s, t in zip(c.shape, tup))\n    if c.size == 0:\n        return cupy.empty(shape_out, dtype=c.dtype)\n    c_shape = []\n    ret_shape = []\n    for dim_in, nrep in zip(c.shape, tup):\n        if nrep == 1:\n            c_shape.append(dim_in)\n            ret_shape.append(dim_in)\n        elif dim_in == 1:\n            c_shape.append(dim_in)\n            ret_shape.append(nrep)\n        else:\n            c_shape.append(1)\n            c_shape.append(dim_in)\n            ret_shape.append(nrep)\n            ret_shape.append(dim_in)\n    ret = cupy.empty(ret_shape, dtype=c.dtype)\n    if ret.size:\n        ret[...] = c.reshape(c_shape)\n    return ret.reshape(shape_out)\n\n\ndef repeat(a, repeats, axis=None):\n    """"""Repeat arrays along an axis.\n\n    Args:\n        a (cupy.ndarray): Array to transform.\n        repeats (int, list or tuple): The number of repeats.\n        axis (int): The axis to repeat.\n\n    Returns:\n        cupy.ndarray: Transformed array with repeats.\n\n    .. seealso:: :func:`numpy.repeat`\n\n    """"""\n    return a.repeat(repeats, axis)\n'"
cupy/manipulation/transpose.py,0,"b'from cupy import core\n\n\ndef rollaxis(a, axis, start=0):\n    """"""Moves the specified axis backwards to the given place.\n\n    Args:\n        a (cupy.ndarray): Array to move the axis.\n        axis (int): The axis to move.\n        start (int): The place to which the axis is moved.\n\n    Returns:\n        cupy.ndarray: A view of ``a`` that the axis is moved to ``start``.\n\n    .. seealso:: :func:`numpy.rollaxis`\n\n    """"""\n    return core.rollaxis(a, axis, start)\n\n\ndef swapaxes(a, axis1, axis2):\n    """"""Swaps the two axes.\n\n    Args:\n        a (cupy.ndarray): Array to swap the axes.\n        axis1 (int): The first axis to swap.\n        axis2 (int): The second axis to swap.\n\n    Returns:\n        cupy.ndarray: A view of ``a`` that the two axes are swapped.\n\n    .. seealso:: :func:`numpy.swapaxes`\n\n    """"""\n    # TODO(okuta): check type\n    return a.swapaxes(axis1, axis2)\n\n\ndef moveaxis(a, source, destination):\n    """"""Moves axes of an array to new positions.\n\n    Other axes remain in their original order.\n\n    Args:\n        a (cupy.ndarray): Array whose axes should be reordered.\n        source (int or sequence of int):\n            Original positions of the axes to move. These must be unique.\n        destination (int or sequence of int):\n            Destination positions for each of the original axes. These must\n            also be unique.\n\n    Returns:\n        cupy.ndarray:\n        Array with moved axes. This array is a view of the input array.\n\n    .. seealso:: :func:`numpy.moveaxis`\n\n    """"""\n    # TODO(fukatani): check type\n    return core.moveaxis(a, source, destination)\n\n\ndef transpose(a, axes=None):\n    """"""Permutes the dimensions of an array.\n\n    Args:\n        a (cupy.ndarray): Array to permute the dimensions.\n        axes (tuple of ints): Permutation of the dimensions. This function\n            reverses the shape by default.\n\n    Returns:\n        cupy.ndarray: A view of ``a`` that the dimensions are permuted.\n\n    .. seealso:: :func:`numpy.transpose`\n\n    """"""\n    # TODO(okuta): check type\n    return a.transpose(axes)\n'"
cupy/math/__init__.py,0,"b'# Functions from the following NumPy document\n# https://docs.scipy.org/doc/numpy/reference/routines.math.html\n\n# ""NOQA"" to suppress flake8 warning\nfrom cupy.math import arithmetic  # NOQA\nfrom cupy.math import explog  # NOQA\nfrom cupy.math import floating  # NOQA\nfrom cupy.math import hyperbolic  # NOQA\nfrom cupy.math import misc  # NOQA\nfrom cupy.math import rounding  # NOQA\nfrom cupy.math import special  # NOQA\nfrom cupy.math import sumprod  # NOQA\nfrom cupy.math import trigonometric  # NOQA\nfrom cupy.math import rational  # NOQA\n'"
cupy/math/arithmetic.py,0,"b""from cupy import core\nfrom cupy.core import fusion\n\n\nadd = core.add\n\n\nreciprocal = core.create_ufunc(\n    'cupy_reciprocal',\n    ('b', 'B', 'h', 'H', 'i', 'I', 'l', 'L', 'q', 'Q',\n     ('e', 'out0 = 1 / in0'),\n     ('f', 'out0 = 1 / in0'),\n     ('d', 'out0 = 1 / in0'),\n     ('F', 'out0 = in0_type(1) / in0'),\n     ('D', 'out0 = in0_type(1) / in0')),\n    'out0 = in0 == 0 ? 0 : (1 / in0)',\n    doc='''Computes ``1 / x`` elementwise.\n\n    .. seealso:: :data:`numpy.reciprocal`\n\n    ''')\n\n\nnegative = core.negative\n\n\nconjugate = core.conjugate\n\n\nangle = core.angle\n\n\ndef real(val):\n    '''Returns the real part of the elements of the array.\n\n    .. seealso:: :func:`numpy.real`\n\n    '''\n    if fusion._is_fusing():\n        return fusion._call_ufunc(core.real, val)\n    if not isinstance(val, core.ndarray):\n        val = core.array(val)\n    return val.real\n\n\ndef imag(val):\n    '''Returns the imaginary part of the elements of the array.\n\n    .. seealso:: :func:`numpy.imag`\n\n    '''\n    if fusion._is_fusing():\n        return fusion._call_ufunc(core.imag, val)\n    if not isinstance(val, core.ndarray):\n        val = core.array(val)\n    return val.imag\n\n\nmultiply = core.multiply\n\n\ndivide = core.divide\n\n\ndivmod = core.divmod\n\n\npower = core.power\n\n\nsubtract = core.subtract\n\n\ntrue_divide = core.true_divide\n\n\nfloor_divide = core.floor_divide\n\n\nfmod = core.create_ufunc(\n    'cupy_fmod',\n    ('bb->b', 'BB->B', 'hh->h', 'HH->H', 'ii->i', 'II->I', 'll->l', 'LL->L',\n     'qq->q', 'QQ->Q',\n     ('ee->e', 'out0 = fmodf(in0, in1)'),\n     ('ff->f', 'out0 = fmodf(in0, in1)'),\n     ('dd->d', 'out0 = fmod(in0, in1)')),\n    'out0 = in1 == 0 ? 0 : fmod((double)in0, (double)in1)',\n    doc='''Computes the remainder of C division elementwise.\n\n    .. seealso:: :data:`numpy.fmod`\n\n    ''')\n\n\nmodf = core.create_ufunc(\n    'cupy_modf',\n    ('e->ee', 'f->ff',\n     ('d->dd', 'double iptr; out0 = modf(in0, &iptr); out1 = iptr')),\n    'float iptr; out0 = modff(in0, &iptr); out1 = iptr',\n    doc='''Extracts the fractional and integral parts of an array elementwise.\n\n    This ufunc returns two arrays.\n\n    .. seealso:: :data:`numpy.modf`\n\n    ''')\n\n\nremainder = core.remainder\n"""
cupy/math/explog.py,0,"b""from cupy import core\nfrom cupy.math import ufunc\n\n\nexp = ufunc.create_math_ufunc(\n    'exp', 1, 'cupy_exp',\n    '''Elementwise exponential function.\n\n    .. seealso:: :data:`numpy.exp`\n\n    ''')\n\n\nexpm1 = ufunc.create_math_ufunc(\n    'expm1', 1, 'cupy_expm1',\n    '''Computes ``exp(x) - 1`` elementwise.\n\n    .. seealso:: :data:`numpy.expm1`\n\n    ''')\n\n\nexp2 = core.create_ufunc(\n    'cupy_exp2',\n    ('e->e', 'f->f', 'd->d', 'F->F', 'D->D'),\n    'out0 = pow(in0_type(2), in0)',\n    doc='''Elementwise exponentiation with base 2.\n\n    .. seealso:: :data:`numpy.exp2`\n\n    ''')\n\n\nlog = ufunc.create_math_ufunc(\n    'log', 1, 'cupy_log',\n    '''Elementwise natural logarithm function.\n\n    .. seealso:: :data:`numpy.log`\n\n    ''')\n\n\nlog10 = ufunc.create_math_ufunc(\n    'log10', 1, 'cupy_log10',\n    '''Elementwise common logarithm function.\n\n    .. seealso:: :data:`numpy.log10`\n\n    ''')\n\n\nlog2 = ufunc.create_math_ufunc(\n    'log2', 1, 'cupy_log2',\n    '''Elementwise binary logarithm function.\n\n    .. seealso:: :data:`numpy.log2`\n\n    ''')\n\n\nlog1p = ufunc.create_math_ufunc(\n    'log1p', 1, 'cupy_log1p',\n    '''Computes ``log(1 + x)`` elementwise.\n\n    .. seealso:: :data:`numpy.log1p`\n\n    ''')\n\n\nlogaddexp = core.create_ufunc(\n    'cupy_logaddexp',\n    ('ee->e', 'ff->f', 'dd->d'),\n    'out0 = fmax(in0, in1) + log1p(exp(-fabs(in0 - in1)))',\n    doc='''Computes ``log(exp(x1) + exp(x2))`` elementwise.\n\n    .. seealso:: :data:`numpy.logaddexp`\n\n    ''')\n\n\nlogaddexp2 = core.create_ufunc(\n    'cupy_logaddexp2',\n    ('ee->e', 'ff->f', 'dd->d'),\n    'out0 = fmax(in0, in1) + log2(1 + exp2(-fabs(in0 - in1)))',\n    doc='''Computes ``log2(exp2(x1) + exp2(x2))`` elementwise.\n\n    .. seealso:: :data:`numpy.logaddexp2`\n\n    ''')\n"""
cupy/math/floating.py,0,"b""from cupy import core\nfrom cupy.math import ufunc\n\n\nsignbit = core.create_ufunc(\n    'cupy_signbit',\n    ('e->?', 'f->?', 'd->?'),\n    'out0 = signbit(in0)',\n    doc='''Tests elementwise if the sign bit is set (i.e. less than zero).\n\n    .. seealso:: :data:`numpy.signbit`\n\n    ''')\n\n\ncopysign = ufunc.create_math_ufunc(\n    'copysign', 2, 'cupy_copysign',\n    '''Returns the first argument with the sign bit of the second elementwise.\n\n    .. seealso:: :data:`numpy.copysign`\n\n    ''')\n\n\nldexp = core.create_ufunc(\n    'cupy_ldexp',\n    ('ei->e', 'fi->f', 'el->e', 'fl->f', 'di->d', 'dq->d'),\n    'out0 = ldexp(in0, in1)',\n    doc='''Computes ``x1 * 2 ** x2`` elementwise.\n\n    .. seealso:: :data:`numpy.ldexp`\n\n    ''')\n\n\nfrexp = core.create_ufunc(\n    'cupy_frexp',\n    ('e->ei', 'f->fi', 'd->di'),\n    'int nptr; out0 = frexp(in0, &nptr); out1 = nptr',\n    doc='''Decomposes each element to mantissa and two's exponent.\n\n    This ufunc outputs two arrays of the input dtype and the ``int`` dtype.\n\n    .. seealso:: :data:`numpy.frexp`\n\n    ''')\n\n\nnextafter = ufunc.create_math_ufunc(\n    'nextafter', 2, 'cupy_nextafter',\n    '''Computes the nearest neighbor float values towards the second argument.\n\n    .. note::\n        For values that are close to zero (or denormal numbers),\n        results of :func:`cupy.nextafter` may be different from those of\n        :func:`numpy.nextafter`, because CuPy sets ``-ftz=true``.\n\n    .. seealso:: :data:`numpy.nextafter`\n\n    ''')\n"""
cupy/math/hyperbolic.py,0,"b""from cupy.math import ufunc\n\n\nsinh = ufunc.create_math_ufunc(\n    'sinh', 1, 'cupy_sinh',\n    '''Elementwise hyperbolic sine function.\n\n    .. seealso:: :data:`numpy.sinh`\n\n    ''')\n\n\ncosh = ufunc.create_math_ufunc(\n    'cosh', 1, 'cupy_cosh',\n    '''Elementwise hyperbolic cosine function.\n\n    .. seealso:: :data:`numpy.cosh`\n\n    ''')\n\n\ntanh = ufunc.create_math_ufunc(\n    'tanh', 1, 'cupy_tanh',\n    '''Elementwise hyperbolic tangent function.\n\n    .. seealso:: :data:`numpy.tanh`\n\n    ''')\n\n\narcsinh = ufunc.create_math_ufunc(\n    'asinh', 1, 'cupy_arcsinh',\n    '''Elementwise inverse of hyperbolic sine function.\n\n    .. seealso:: :data:`numpy.arcsinh`\n\n    ''')\n\n\narccosh = ufunc.create_math_ufunc(\n    'acosh', 1, 'cupy_arccosh',\n    '''Elementwise inverse of hyperbolic cosine function.\n\n    .. seealso:: :data:`numpy.arccosh`\n\n    ''')\n\n\narctanh = ufunc.create_math_ufunc(\n    'atanh', 1, 'cupy_arctanh',\n    '''Elementwise inverse of hyperbolic tangent function.\n\n    .. seealso:: :data:`numpy.arctanh`\n\n    ''')\n"""
cupy/math/misc.py,0,"b'from cupy import core\nfrom cupy.core import _routines_math as _math\nfrom cupy.core import fusion\n\n# TODO(okuta): Implement convolve\n\n\ndef clip(a, a_min=None, a_max=None, out=None):\n    """"""Clips the values of an array to a given interval.\n\n    This is equivalent to ``maximum(minimum(a, a_max), a_min)``, while this\n    function is more efficient.\n\n    Args:\n        a (cupy.ndarray): The source array.\n        a_min (scalar, cupy.ndarray or None): The left side of the interval.\n            When it is ``None``, it is ignored.\n        a_max (scalar, cupy.ndarray or None): The right side of the interval.\n            When it is ``None``, it is ignored.\n        out (cupy.ndarray): Output array.\n\n    Returns:\n        cupy.ndarray: Clipped array.\n\n    .. seealso:: :func:`numpy.clip`\n\n    """"""\n    if fusion._is_fusing():\n        return fusion._call_ufunc(_math.clip,\n                                  a, a_min, a_max, out=out)\n\n    # TODO(okuta): check type\n    return a.clip(a_min, a_max, out=out)\n\n\n# sqrt_fixed is deprecated.\n# numpy.sqrt is fixed in numpy 1.11.2.\nsqrt = sqrt_fixed = core.sqrt\n\n\ncbrt = core.create_ufunc(\n    \'cupy_cbrt\',\n    (\'e->e\', \'f->f\', \'d->d\'),\n    \'out0 = cbrt(in0)\',\n    doc=\'\'\'Elementwise cube root function.\n\n    .. seealso:: :data:`numpy.cbrt`\n\n    \'\'\')\n\n\nsquare = core.create_ufunc(\n    \'cupy_square\',\n    (\'b->b\', \'B->B\', \'h->h\', \'H->H\', \'i->i\', \'I->I\', \'l->l\', \'L->L\', \'q->q\',\n     \'Q->Q\', \'e->e\', \'f->f\', \'d->d\', \'F->F\', \'D->D\'),\n    \'out0 = in0 * in0\',\n    doc=\'\'\'Elementwise square function.\n\n    .. seealso:: :data:`numpy.square`\n\n    \'\'\')\n\n\nabsolute = core.absolute\n\n\n# TODO(beam2d): Implement it\n# fabs\n\n\n_unsigned_sign = \'out0 = in0 > 0\'\n_complex_sign = \'\'\'\nif (in0.real() == 0) {\n  out0 = (in0.imag() > 0) - (in0.imag() < 0);\n} else {\n  out0 = (in0.real() > 0) - (in0.real() < 0);\n}\n\'\'\'\nsign = core.create_ufunc(\n    \'cupy_sign\',\n    (\'b->b\', (\'B->B\', _unsigned_sign), \'h->h\', (\'H->H\', _unsigned_sign),\n     \'i->i\', (\'I->I\', _unsigned_sign), \'l->l\', (\'L->L\', _unsigned_sign),\n     \'q->q\', (\'Q->Q\', _unsigned_sign), \'e->e\', \'f->f\', \'d->d\',\n     (\'F->F\', _complex_sign), (\'D->D\', _complex_sign)),\n    \'out0 = (in0 > 0) - (in0 < 0)\',\n    doc=\'\'\'Elementwise sign function.\n\n    It returns -1, 0, or 1 depending on the sign of the input.\n\n    .. seealso:: :data:`numpy.sign`\n\n    \'\'\')\n\n\n_float_preamble = \'\'\'\n#ifndef NAN\n#define NAN __int_as_float(0x7fffffff)\n#endif\n\'\'\'\n_float_maximum = (\'out0 = (isnan(in0) | isnan(in1)) ? out0_type(NAN) : \'\n                  \'out0_type(max(in0, in1))\')\nmaximum = core.create_ufunc(\n    \'cupy_maximum\',\n    (\'??->?\', \'bb->b\', \'BB->B\', \'hh->h\', \'HH->H\', \'ii->i\', \'II->I\', \'ll->l\',\n     \'LL->L\', \'qq->q\', \'QQ->Q\',\n     (\'ee->e\', _float_maximum),\n     (\'ff->f\', _float_maximum),\n     (\'dd->d\', _float_maximum),\n     (\'FF->F\', _float_maximum),\n     (\'DD->D\', _float_maximum)),\n    \'out0 = max(in0, in1)\',\n    preamble=_float_preamble,\n    doc=\'\'\'Takes the maximum of two arrays elementwise.\n\n    If NaN appears, it returns the NaN.\n\n    .. seealso:: :data:`numpy.maximum`\n\n    \'\'\')\n\n\n_float_minimum = (\'out0 = (isnan(in0) | isnan(in1)) ? out0_type(NAN) : \'\n                  \'out0_type(min(in0, in1))\')\nminimum = core.create_ufunc(\n    \'cupy_minimum\',\n    (\'??->?\', \'bb->b\', \'BB->B\', \'hh->h\', \'HH->H\', \'ii->i\', \'II->I\', \'ll->l\',\n     \'LL->L\', \'qq->q\', \'QQ->Q\',\n     (\'ee->e\', _float_minimum),\n     (\'ff->f\', _float_minimum),\n     (\'dd->d\', _float_minimum),\n     (\'FF->F\', _float_minimum),\n     (\'DD->D\', _float_minimum)),\n    \'out0 = min(in0, in1)\',\n    preamble=_float_preamble,\n    doc=\'\'\'Takes the minimum of two arrays elementwise.\n\n    If NaN appears, it returns the NaN.\n\n    .. seealso:: :data:`numpy.minimum`\n\n    \'\'\')\n\n\nfmax = core.create_ufunc(\n    \'cupy_fmax\',\n    (\'??->?\', \'bb->b\', \'BB->B\', \'hh->h\', \'HH->H\', \'ii->i\', \'II->I\', \'ll->l\',\n     \'LL->L\', \'qq->q\', \'QQ->Q\',\n     (\'ee->e\', \'out0 = fmax(in0, in1)\'),\n     (\'ff->f\', \'out0 = fmax(in0, in1)\'),\n     (\'dd->d\', \'out0 = fmax(in0, in1)\'),\n     \'FF->F\', \'DD->D\'),\n    \'out0 = max(in0, in1)\',\n    doc=\'\'\'Takes the maximum of two arrays elementwise.\n\n    If NaN appears, it returns the other operand.\n\n    .. seealso:: :data:`numpy.fmax`\n\n    \'\'\')\n\n\nfmin = core.create_ufunc(\n    \'cupy_fmin\',\n    (\'??->?\', \'bb->b\', \'BB->B\', \'hh->h\', \'HH->H\', \'ii->i\', \'II->I\', \'ll->l\',\n     \'LL->L\', \'qq->q\', \'QQ->Q\',\n     (\'ee->e\', \'out0 = fmin(in0, in1)\'),\n     (\'ff->f\', \'out0 = fmin(in0, in1)\'),\n     (\'dd->d\', \'out0 = fmin(in0, in1)\'),\n     \'FF->F\', \'DD->D\'),\n    \'out0 = min(in0, in1)\',\n    doc=\'\'\'Takes the minimum of two arrays elementwise.\n\n    If NaN appears, it returns the other operand.\n\n    .. seealso:: :data:`numpy.fmin`\n\n    \'\'\')\n\n\n_nan_to_num_preamble = \'\'\'\ntemplate <class T>\n__device__ T nan_to_num(T x, T large) {\n    if (isnan(x))\n        return 0;\n    if (isinf(x))\n        return copysign(large, x);\n    return x;\n}\n\ntemplate <class T>\n__device__ complex<T> nan_to_num(complex<T> x, T large) {\n    T re = nan_to_num(x.real(), large);\n    T im = nan_to_num(x.imag(), large);\n    return complex<T>(re, im);\n}\n\'\'\'\n\n\nnan_to_num = core.create_ufunc(\n    \'cupy_nan_to_num\',\n    (\'?->?\', \'b->b\', \'B->B\', \'h->h\', \'H->H\',\n     \'i->i\', \'I->I\', \'l->l\', \'L->L\', \'q->q\', \'Q->Q\',\n     (\'e->e\',\n      \'out0 = nan_to_num(in0, float16(32 * 0x7FF))\'),\n     (\'f->f\',\n      \'out0 = nan_to_num(in0, __int_as_float(0x7F800000 - 1))\'),\n     (\'d->d\',\n      \'out0 = nan_to_num(in0, __longlong_as_double(0x7FF0000000000000 - 1))\'),\n     (\'F->F\',\n      \'out0 = nan_to_num(in0, __int_as_float(0x7F800000 - 1))\'),\n     (\'D->D\',\n      \'out0 = nan_to_num(in0, __longlong_as_double(0x7FF0000000000000 - 1))\')),\n    \'out0 = in0\',\n    preamble=_nan_to_num_preamble,\n    doc=\'\'\'Elementwise nan_to_num function.\n\n    .. seealso:: :data:`numpy.nan_to_num`\n\n    \'\'\')\n\n\n# TODO(okuta): Implement real_if_close\n\n\n# TODO(okuta): Implement interp\n'"
cupy/math/rational.py,0,"b'from cupy import core\n\n\ndef _negative_gcd_error():\n    raise TypeError(""gcd cannot be computed with boolean arrays"")\n\n\ndef _negative_lcm_error():\n    raise TypeError(""lcm cannot be computed with boolean arrays"")\n\n\n_gcd_preamble = \'\'\'\ntemplate <typename T> inline __device__ T gcd(T in0, T in1) {\n  T r;\n  while (in1 != 0) {\n    r = in0 % in1;\n    in0 = in1;\n    in1 = r;\n  }\n  if (in0 < 0)\n    return -in0;\n  return in0;\n}\n\'\'\'\n\ngcd = core.create_ufunc(\n    \'cupy_gcd\',\n    ((\'??->?\', _negative_gcd_error),\n     \'bb->b\', \'BB->B\', \'hh->h\', \'HH->H\', \'ii->i\', \'II->I\', \'ll->l\',\n     \'LL->L\', \'qq->q\', \'QQ->Q\'),\n    \'out0 = gcd(in0, in1)\',\n    preamble=_gcd_preamble,\n    doc=\'\'\'Computes gcd of ``x1`` and ``x2`` elementwise.\n\n    .. seealso:: :data:`numpy.gcd`\n\n    \'\'\')\n\n_lcm_preamble = \'\'\'\ntemplate <typename T> inline __device__ T lcm(T in0, T in1) {\n  T r = gcd(in0, in1);\n  if (r == 0)\n    return 0;\n  r = in0 / r * in1;\n  if (r < 0)\n    return -r;\n  return r;\n}\n\'\'\' + _gcd_preamble\n\nlcm = core.create_ufunc(\n    \'cupy_lcm\',\n    ((\'??->?\', _negative_lcm_error),\n     \'bb->b\', \'BB->B\', \'hh->h\', \'HH->H\', \'ii->i\', \'II->I\', \'ll->l\',\n     \'LL->L\', \'qq->q\', \'QQ->Q\'),\n    \'out0 = lcm(in0, in1)\',\n    preamble=_lcm_preamble,\n    doc=\'\'\'Computes lcm of ``x1`` and ``x2`` elementwise.\n\n    .. seealso:: :data:`numpy.lcm`\n\n    \'\'\')\n'"
cupy/math/rounding.py,0,"b'from cupy import core\nfrom cupy.core import fusion\nfrom cupy.math import ufunc\n\n\ndef around(a, decimals=0, out=None):\n    """"""Rounds to the given number of decimals.\n\n    Args:\n        a (cupy.ndarray): The source array.\n        decimals (int): Number of decimal places to round to (default: 0).\n            If decimals is negative, it specifies the number of positions to\n            the left of the decimal point.\n        out (cupy.ndarray): Output array.\n\n    Returns:\n        cupy.ndarray: Rounded array.\n\n    .. seealso:: :func:`numpy.around`\n\n    """"""\n    if fusion._is_fusing():\n        return fusion._call_ufunc(core.core._round_ufunc, a, decimals, out=out)\n    a = core.array(a, copy=False)\n    return a.round(decimals, out=out)\n\n\ndef round_(a, decimals=0, out=None):\n    return around(a, decimals, out=out)\n\n\nrint = ufunc.create_math_ufunc(\n    \'rint\', 1, \'cupy_rint\',\n    \'\'\'Rounds each element of an array to the nearest integer.\n\n    .. seealso:: :data:`numpy.rint`\n\n    \'\'\')\n\n\nfloor = ufunc.create_math_ufunc(\n    \'floor\', 1, \'cupy_floor\',\n    \'\'\'Rounds each element of an array to its floor integer.\n\n    .. seealso:: :data:`numpy.floor`\n\n    \'\'\', support_complex=False)\n\n\nceil = ufunc.create_math_ufunc(\n    \'ceil\', 1, \'cupy_ceil\',\n    \'\'\'Rounds each element of an array to its ceiling integer.\n\n    .. seealso:: :data:`numpy.ceil`\n\n    \'\'\', support_complex=False)\n\n\ntrunc = ufunc.create_math_ufunc(\n    \'trunc\', 1, \'cupy_trunc\',\n    \'\'\'Rounds each element of an array towards zero.\n\n    .. seealso:: :data:`numpy.trunc`\n\n    \'\'\', support_complex=False)\n\n\nfix = core.create_ufunc(\n    \'cupy_fix\', (\'e->e\', \'f->f\', \'d->d\'),\n    \'out0 = (in0 >= 0.0) ? floor(in0): ceil(in0)\',\n    doc=\'\'\'If given value x is positive, it return floor(x).\n    Else, it return ceil(x).\n\n    .. seealso:: :func:`numpy.fix`\n\n    \'\'\')\n'"
cupy/math/special.py,0,"b""from cupy import core\nfrom cupy.math import ufunc\n\n\ni0 = ufunc.create_math_ufunc(\n    'cyl_bessel_i0', 1, 'cupy_i0',\n    '''Modified Bessel function of the first kind, order 0.\n\n    .. seealso:: :func:`numpy.i0`\n\n    ''')\n\n\nsinc = core.create_ufunc(\n    'cupy_sinc',\n    ('e->e', 'f->f', 'd->d',\n     ('F->F', 'in0_type pi_in0 = (in0_type) M_PI * in0;'\n              'out0 = abs(in0) > 1e-9 ? sin(pi_in0) / (pi_in0) : 1'),\n     ('D->D', 'in0_type pi_in0 = (in0_type) M_PI * in0;'\n              'out0 = abs(in0) > 1e-9 ? sin(pi_in0) / (pi_in0) : 1')),\n    'out0 = abs(in0) > 1e-9 ? sinpi(in0) / (M_PI * in0) : 1',\n    doc='''Elementwise sinc function.\n\n    .. seealso:: :func:`numpy.sinc`\n\n    ''')\n"""
cupy/math/sumprod.py,0,"b'import numpy\n\nimport cupy\nfrom cupy.core import _routines_math as _math\nfrom cupy.core import fusion\nfrom cupy.util import _normalize_axis_index\n\n\ndef sum(a, axis=None, dtype=None, out=None, keepdims=False):\n    """"""Returns the sum of an array along given axes.\n\n    Args:\n        a (cupy.ndarray): Array to take sum.\n        axis (int or sequence of ints): Axes along which the sum is taken.\n        dtype: Data type specifier.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the specified axes are remained as axes\n            of length one.\n\n    Returns:\n        cupy.ndarray: The result array.\n\n    .. seealso:: :func:`numpy.sum`\n\n    """"""\n    if fusion._is_fusing():\n        if keepdims:\n            raise NotImplementedError(\n                \'cupy.sum does not support `keepdims` in fusion yet.\')\n        return fusion._call_reduction(_math.sum_auto_dtype,\n                                      a, axis=axis, dtype=dtype, out=out)\n\n    # TODO(okuta): check type\n    return a.sum(axis, dtype, out, keepdims)\n\n\ndef prod(a, axis=None, dtype=None, out=None, keepdims=False):\n    """"""Returns the product of an array along given axes.\n\n    Args:\n        a (cupy.ndarray): Array to take product.\n        axis (int or sequence of ints): Axes along which the product is taken.\n        dtype: Data type specifier.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the specified axes are remained as axes\n            of length one.\n\n    Returns:\n        cupy.ndarray: The result array.\n\n    .. seealso:: :func:`numpy.prod`\n\n    """"""\n    if fusion._is_fusing():\n        if keepdims:\n            raise NotImplementedError(\n                \'cupy.prod does not support `keepdims` in fusion yet.\')\n        return fusion._call_reduction(_math.prod_auto_dtype,\n                                      a, axis=axis, dtype=dtype, out=out)\n\n    # TODO(okuta): check type\n    return a.prod(axis, dtype, out, keepdims)\n\n\ndef nansum(a, axis=None, dtype=None, out=None, keepdims=False):\n    """"""Returns the sum of an array along given axes treating Not a Numbers\n    (NaNs) as zero.\n\n    Args:\n        a (cupy.ndarray): Array to take sum.\n        axis (int or sequence of ints): Axes along which the sum is taken.\n        dtype: Data type specifier.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the specified axes are remained as axes\n            of length one.\n\n    Returns:\n        cupy.ndarray: The result array.\n\n    .. seealso:: :func:`numpy.nansum`\n\n    """"""\n    if fusion._is_fusing():\n        if keepdims:\n            raise NotImplementedError(\n                \'cupy.nansum does not support `keepdims` in fusion yet.\')\n        return fusion._call_reduction(_math.nansum_auto_dtype,\n                                      a, axis=axis, dtype=dtype, out=out)\n\n    # TODO(okuta): check type\n    return _math._nansum(a, axis, dtype, out, keepdims)\n\n\ndef nanprod(a, axis=None, dtype=None, out=None, keepdims=False):\n    """"""Returns the product of an array along given axes treating Not a Numbers\n    (NaNs) as zero.\n\n    Args:\n        a (cupy.ndarray): Array to take product.\n        axis (int or sequence of ints): Axes along which the product is taken.\n        dtype: Data type specifier.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the specified axes are remained as axes\n            of length one.\n\n    Returns:\n        cupy.ndarray: The result array.\n\n    .. seealso:: :func:`numpy.nanprod`\n\n    """"""\n    if fusion._is_fusing():\n        if keepdims:\n            raise NotImplementedError(\n                \'cupy.nanprod does not support `keepdims` in fusion yet.\')\n        return fusion._call_reduction(_math.nanprod_auto_dtype,\n                                      a, axis=axis, dtype=dtype, out=out)\n\n    # TODO(okuta): check type\n    return _math._nanprod(a, axis, dtype, out, keepdims)\n\n\ndef cumsum(a, axis=None, dtype=None, out=None):\n    """"""Returns the cumulative sum of an array along a given axis.\n\n    Args:\n        a (cupy.ndarray): Input array.\n        axis (int): Axis along which the cumulative sum is taken. If it is not\n            specified, the input is flattened.\n        dtype: Data type specifier.\n        out (cupy.ndarray): Output array.\n\n    Returns:\n        cupy.ndarray: The result array.\n\n    .. seealso:: :func:`numpy.cumsum`\n\n    """"""\n    return _math.scan_core(a, axis, _math.scan_op.SCAN_SUM, dtype, out)\n\n\ndef cumprod(a, axis=None, dtype=None, out=None):\n    """"""Returns the cumulative product of an array along a given axis.\n\n    Args:\n        a (cupy.ndarray): Input array.\n        axis (int): Axis along which the cumulative product is taken. If it is\n            not specified, the input is flattened.\n        dtype: Data type specifier.\n        out (cupy.ndarray): Output array.\n\n    Returns:\n        cupy.ndarray: The result array.\n\n    .. seealso:: :func:`numpy.cumprod`\n\n    """"""\n    return _math.scan_core(a, axis, _math.scan_op.SCAN_PROD, dtype, out)\n\n\ndef diff(a, n=1, axis=-1, prepend=None, append=None):\n    """"""Calculate the n-th discrete difference along the given axis.\n\n    Args:\n        a (cupy.ndarray): Input array.\n        n (int): The number of times values are differenced. If zero, the input\n            is returned as-is.\n        axis (int): The axis along which the difference is taken, default is\n            the last axis.\n        prepend (int, float, cupy.ndarray): Value to prepend to ``a``.\n        append (int, float, cupy.ndarray): Value to append to ``a``.\n\n    Returns:\n        cupy.ndarray: The result array.\n\n    .. seealso:: :func:`numpy.diff`\n    """"""\n\n    if n == 0:\n        return a\n    if n < 0:\n        raise ValueError(\n            ""order must be non-negative but got "" + repr(n))\n\n    a = cupy.asanyarray(a)\n    nd = a.ndim\n    axis = _normalize_axis_index(axis, nd)\n\n    combined = []\n\n    if prepend is not None:\n        prepend = cupy.asanyarray(prepend)\n        if prepend.ndim == 0:\n            shape = list(a.shape)\n            shape[axis] = 1\n            prepend = cupy.broadcast_to(prepend, tuple(shape))\n        combined.append(prepend)\n\n    combined.append(a)\n\n    if append is not None:\n        append = cupy.asanyarray(append)\n        if append.ndim == 0:\n            shape = list(a.shape)\n            shape[axis] = 1\n            append = cupy.broadcast_to(append, tuple(shape))\n        combined.append(append)\n\n    if len(combined) > 1:\n        a = cupy.concatenate(combined, axis)\n\n    slice1 = [slice(None)] * nd\n    slice2 = [slice(None)] * nd\n    slice1[axis] = slice(1, None)\n    slice2[axis] = slice(None, -1)\n    slice1 = tuple(slice1)\n    slice2 = tuple(slice2)\n\n    op = cupy.not_equal if a.dtype == numpy.bool_ else cupy.subtract\n    for _ in range(n):\n        a = op(a[slice1], a[slice2])\n\n    return a\n\n\n# TODO(okuta): Implement ediff1d\n\n\n# TODO(okuta): Implement gradient\n\n\n# TODO(okuta): Implement cross\n\n\n# TODO(okuta): Implement trapz\n'"
cupy/math/trigonometric.py,0,"b'import numpy\n\nimport cupy\nfrom cupy import core\nfrom cupy.math import sumprod\nfrom cupy.math import ufunc\n\n\nsin = ufunc.create_math_ufunc(\n    \'sin\', 1, \'cupy_sin\',\n    \'\'\'Elementwise sine function.\n\n    .. seealso:: :data:`numpy.sin`\n\n    \'\'\')\n\n\ncos = ufunc.create_math_ufunc(\n    \'cos\', 1, \'cupy_cos\',\n    \'\'\'Elementwise cosine function.\n\n    .. seealso:: :data:`numpy.cos`\n\n    \'\'\')\n\n\ntan = ufunc.create_math_ufunc(\n    \'tan\', 1, \'cupy_tan\',\n    \'\'\'Elementwise tangent function.\n\n    .. seealso:: :data:`numpy.tan`\n\n    \'\'\')\n\n\narcsin = ufunc.create_math_ufunc(\n    \'asin\', 1, \'cupy_arcsin\',\n    \'\'\'Elementwise inverse-sine function (a.k.a. arcsine function).\n\n    .. seealso:: :data:`numpy.arcsin`\n\n    \'\'\')\n\n\narccos = ufunc.create_math_ufunc(\n    \'acos\', 1, \'cupy_arccos\',\n    \'\'\'Elementwise inverse-cosine function (a.k.a. arccosine function).\n\n    .. seealso:: :data:`numpy.arccos`\n\n    \'\'\')\n\n\narctan = ufunc.create_math_ufunc(\n    \'atan\', 1, \'cupy_arctan\',\n    \'\'\'Elementwise inverse-tangent function (a.k.a. arctangent function).\n\n    .. seealso:: :data:`numpy.arctan`\n\n    \'\'\')\n\n\nhypot = ufunc.create_math_ufunc(\n    \'hypot\', 2, \'cupy_hypot\',\n    \'\'\'Computes the hypoteneous of orthogonal vectors of given length.\n\n    This is equivalent to ``sqrt(x1 **2 + x2 ** 2)``, while this function is\n    more efficient.\n\n    .. seealso:: :data:`numpy.hypot`\n\n    \'\'\')\n\n\narctan2 = ufunc.create_math_ufunc(\n    \'atan2\', 2, \'cupy_arctan2\',\n    \'\'\'Elementwise inverse-tangent of the ratio of two arrays.\n\n    .. seealso:: :data:`numpy.arctan2`\n\n    \'\'\')\n\n\ndeg2rad = core.create_ufunc(\n    \'cupy_deg2rad\',\n    (\'e->e\', \'f->f\', \'d->d\'),\n    \'out0 = in0 * (out0_type)(M_PI / 180)\',\n    doc=\'\'\'Converts angles from degrees to radians elementwise.\n\n    .. seealso:: :data:`numpy.deg2rad`, :data:`numpy.radians`\n\n    \'\'\')\n\n\nrad2deg = core.create_ufunc(\n    \'cupy_rad2deg\',\n    (\'e->e\', \'f->f\', \'d->d\'),\n    \'out0 = in0 * (out0_type)(180 / M_PI)\',\n    doc=\'\'\'Converts angles from radians to degrees elementwise.\n\n    .. seealso:: :data:`numpy.rad2deg`, :data:`numpy.degrees`\n\n    \'\'\')\n\n\n@core.fusion.fuse()\ndef _unwrap_correct(dd, discont):\n    ddmod = cupy.mod(dd + numpy.pi, 2*numpy.pi) - numpy.pi\n    cupy.copyto(ddmod, numpy.pi, where=(ddmod == -numpy.pi) & (dd > 0))\n    ph_correct = ddmod - dd\n    cupy.copyto(ph_correct, 0., where=cupy.abs(dd) < discont)\n    return ph_correct\n\n\ndef unwrap(p, discont=numpy.pi, axis=-1):\n    """"""Unwrap by changing deltas between values to 2*pi complement.\n\n    Args:\n        p (cupy.ndarray): Input array.\n        discont (float): Maximum discontinuity between values, default is\n            ``pi``.\n        axis (int): Axis along which unwrap will operate, default is the last\n            axis.\n    Returns:\n        cupy.ndarray: The result array.\n\n    .. seealso:: :func:`numpy.unwrap`\n    """"""\n\n    p = cupy.asarray(p)\n    nd = p.ndim\n    dd = sumprod.diff(p, axis=axis)\n    slice1 = [slice(None, None)]*nd     # full slices\n    slice1[axis] = slice(1, None)\n    slice1 = tuple(slice1)\n    ph_correct = _unwrap_correct(dd, discont)\n    up = cupy.array(p, copy=True, dtype=\'d\')\n    up[slice1] = p[slice1] + cupy.cumsum(ph_correct, axis=axis)\n    return up\n\n\ndegrees = rad2deg\nradians = deg2rad\n'"
cupy/math/ufunc.py,0,"b""from cupy import core\n\n\ndef create_math_ufunc(math_name, nargs, name, doc, support_complex=True):\n    assert 1 <= nargs <= 2\n    if nargs == 1:\n        types = ('e->e', 'f->f', 'd->d')\n        if support_complex:\n            types += ('F->F', 'D->D')\n        return core.create_ufunc(\n            name, types, 'out0 = %s(in0)' % math_name, doc=doc)\n    else:\n        types = ('ee->e', 'ff->f', 'dd->d')\n        if support_complex:\n            types += ('FF->F', 'DD->D')\n        return core.create_ufunc(\n            name, types, 'out0 = %s(in0, in1)' % math_name, doc=doc)\n"""
cupy/math/window.py,0,"b'import numpy\n\nimport cupy\nfrom cupy import core\n\n_blackman_kernel = core.ElementwiseKernel(\n    ""float32 alpha"",\n    ""float64 out"",\n    """"""\n    out = 0.42 - 0.5 * cos(i * alpha) + 0.08 * cos(2 * alpha * i);\n    """""", name=""cupy_blackman"")\n\n\n_bartlett_kernel = core.ElementwiseKernel(\n    ""float32 alpha"",\n    ""T arr"",\n    """"""\n    if (i < alpha)\n        arr = i / alpha;\n    else\n        arr = 2.0 - i / alpha;\n    """""", name=""cupy_bartlett"")\n\n\ndef bartlett(M):\n    """"""Returns the Bartlett window.\n\n    The Bartlett window is defined as\n\n    .. math::\n            w(n) = \\\\frac{2}{M-1} \\\\left(\n            \\\\frac{M-1}{2} - \\\\left|n - \\\\frac{M-1}{2}\\\\right|\n            \\\\right)\n\n    Args:\n        M (int):\n            Number of points in the output window. If zero or less, an empty\n            array is returned.\n\n    Returns:\n        ~cupy.ndarray: Output ndarray.\n\n    .. seealso:: :func:`numpy.bartlett`\n    """"""\n    if M == 1:\n        return cupy.ones(1, dtype=cupy.float64)\n    if M <= 0:\n        return cupy.array([])\n    alpha = (M - 1) / 2.0\n    out = cupy.empty(M, dtype=cupy.float64)\n    return _bartlett_kernel(alpha, out)\n\n\ndef blackman(M):\n    """"""Returns the Blackman window.\n\n    The Blackman window is defined as\n\n    .. math::\n        w(n) = 0.42 - 0.5 \\\\cos\\\\left(\\\\frac{2\\\\pi{n}}{M-1}\\\\right)\n        + 0.08 \\\\cos\\\\left(\\\\frac{4\\\\pi{n}}{M-1}\\\\right)\n        \\\\qquad 0 \\\\leq n \\\\leq M-1\n\n    Args:\n        M (:class:`~int`):\n            Number of points in the output window. If zero or less, an empty\n            array is returned.\n\n    Returns:\n        ~cupy.ndarray: Output ndarray.\n\n    .. seealso:: :func:`numpy.blackman`\n    """"""\n    if M == 1:\n        return cupy.ones(1, dtype=cupy.float64)\n    if M <= 0:\n        return cupy.array([])\n    alpha = numpy.pi * 2 / (M - 1)\n    out = cupy.empty(M, dtype=cupy.float64)\n    return _blackman_kernel(alpha, out)\n\n\n_hamming_kernel = core.ElementwiseKernel(\n    ""float32 alpha"",\n    ""float64 out"",\n    """"""\n    out = 0.54 - 0.46 * cos(i * alpha);\n    """""", name=""cupy_hamming"")\n\n\ndef hamming(M):\n    """"""Returns the Hamming window.\n\n    The Hamming window is defined as\n\n    .. math::\n        w(n) = 0.54 - 0.46\\\\cos\\\\left(\\\\frac{2\\\\pi{n}}{M-1}\\\\right)\n        \\\\qquad 0 \\\\leq n \\\\leq M-1\n\n    Args:\n        M (:class:`~int`):\n            Number of points in the output window. If zero or less, an empty\n            array is returned.\n\n    Returns:\n        ~cupy.ndarray: Output ndarray.\n\n    .. seealso:: :func:`numpy.hamming`\n    """"""\n    if M == 1:\n        return cupy.ones(1, dtype=cupy.float64)\n    if M <= 0:\n        return cupy.array([])\n    alpha = numpy.pi * 2 / (M - 1)\n    out = cupy.empty(M, dtype=cupy.float64)\n    return _hamming_kernel(alpha, out)\n\n\n_hanning_kernel = core.ElementwiseKernel(\n    ""float32 alpha"",\n    ""float64 out"",\n    """"""\n    out = 0.5 - 0.5 * cos(i * alpha);\n    """""", name=""cupy_hanning"")\n\n\ndef hanning(M):\n    """"""Returns the Hanning window.\n\n    The Hanning window is defined as\n\n    .. math::\n        w(n) = 0.5 - 0.5\\\\cos\\\\left(\\\\frac{2\\\\pi{n}}{M-1}\\\\right)\n        \\\\qquad 0 \\\\leq n \\\\leq M-1\n\n    Args:\n        M (:class:`~int`):\n            Number of points in the output window. If zero or less, an empty\n            array is returned.\n\n    Returns:\n        ~cupy.ndarray: Output ndarray.\n\n    .. seealso:: :func:`numpy.hanning`\n    """"""\n    if M == 1:\n        return cupy.ones(1, dtype=cupy.float64)\n    if M <= 0:\n        return cupy.array([])\n    alpha = numpy.pi * 2 / (M - 1)\n    out = cupy.empty(M, dtype=cupy.float64)\n    return _hanning_kernel(alpha, out)\n\n\n_kaiser_kernel = core.ElementwiseKernel(\n    ""float32 beta, float32 alpha"",\n    ""T arr"",\n    """"""\n    float temp = (i - alpha) / alpha;\n    arr = cyl_bessel_i0(beta * sqrt(1 - (temp * temp)));\n    arr /= cyl_bessel_i0(beta);\n    """""", name=""cupy_kaiser"")\n\n\ndef kaiser(M, beta):\n    """"""Return the Kaiser window.\n    The Kaiser window is a taper formed by using a Bessel function.\n\n    .. math::  w(n) = I_0\\\\left( \\\\beta \\\\sqrt{1-\\\\frac{4n^2}{(M-1)^2}}\n               \\\\right)/I_0(\\\\beta)\n\n    with\n\n    .. math:: \\\\quad -\\\\frac{M-1}{2} \\\\leq n \\\\leq \\\\frac{M-1}{2}\n\n    where :math:`I_0` is the modified zeroth-order Bessel function.\n\n     Args:\n        M (int):\n            Number of points in the output window. If zero or less, an empty\n            array is returned.\n        beta (float):\n            Shape parameter for window\n\n    Returns:\n        ~cupy.ndarray:  The window, with the maximum value normalized to one\n        (the value one appears only if the number of samples is odd).\n\n    .. seealso:: :func:`numpy.kaiser`\n    """"""\n    if M == 1:\n        return cupy.array([1.])\n    if M <= 0:\n        return cupy.array([])\n    alpha = (M - 1) / 2.0\n    out = cupy.empty(M, dtype=cupy.float64)\n    return _kaiser_kernel(beta, alpha, out)\n'"
cupy/misc/__init__.py,0,b'from cupy.misc.memory_ranges import may_share_memory  # NOQA\nfrom cupy.misc.memory_ranges import shares_memory  # NOQA\nfrom cupy.misc.who import who  # NOQA\n'
cupy/misc/memory_ranges.py,0,"b""from cupy.core import _memory_range\n\n\ndef may_share_memory(a, b, max_work=None):\n    if max_work is None:\n        return _memory_range.may_share_bounds(a, b)\n\n    raise NotImplementedError('Only supported for `max_work` is `None`')\n\n\ndef shares_memory(a, b, max_work=None):\n    if max_work == 'MAY_SHARE_BOUNDS':\n        return _memory_range.may_share_bounds(a, b)\n\n    raise NotImplementedError(\n        'Only supported for `max_work` is MAY_SHARE_BOUNDS')\n"""
cupy/misc/who.py,0,"b'import sys\nimport cupy\n\n\ndef who(vardict=None):\n    """"""Print the CuPy arrays in the given dictionary.\n\n    Prints out the name, shape, bytes and type of all of the ndarrays\n    present in `vardict`.\n\n    If there is no dictionary passed in or `vardict` is None then returns\n    CuPy arrays in the globals() dictionary (all CuPy arrays in the\n    namespace).\n\n    Args:\n        vardict : (None or dict)  A dictionary possibly containing ndarrays.\n                  Default is globals() if `None` specified\n\n\n    .. admonition:: Example\n\n        >>> a = cupy.arange(10)\n        >>> b = cupy.ones(20)\n        >>> cupy.who()\n        Name            Shape            Bytes            Type\n        ===========================================================\n        <BLANKLINE>\n        a               10               80               int64\n        b               20               160              float64\n        <BLANKLINE>\n        Upper bound on total bytes  =       240\n        >>> d = {\'x\': cupy.arange(2.0),\n        ... \'y\': cupy.arange(3.0), \'txt\': \'Some str\',\n        ... \'idx\':5}\n        >>> cupy.who(d)\n        Name            Shape            Bytes            Type\n        ===========================================================\n        <BLANKLINE>\n        x               2                16               float64\n        y               3                24               float64\n        <BLANKLINE>\n        Upper bound on total bytes  =       40\n\n    """"""\n\n    # Implementation is largely copied from numpy.who()\n    if vardict is None:\n        frame = sys._getframe().f_back\n        vardict = frame.f_globals\n    sta = []\n    cache = {}\n    for name in sorted(vardict.keys()):\n        if isinstance(vardict[name], cupy.ndarray):\n            var = vardict[name]\n            idv = id(var)\n            if idv in cache.keys():\n                namestr = \'{} ({})\'.format(name, cache[idv])\n                original = 0\n            else:\n                cache[idv] = name\n                namestr = name\n                original = 1\n            shapestr = \' x \'.join(map(str, var.shape))\n            bytestr = str(var.nbytes)\n            sta.append(\n                [namestr, shapestr, bytestr, var.dtype.name, original]\n            )\n\n    maxname = 0\n    maxshape = 0\n    maxbyte = 0\n    totalbytes = 0\n    for k in range(len(sta)):\n        val = sta[k]\n        if maxname < len(val[0]):\n            maxname = len(val[0])\n        if maxshape < len(val[1]):\n            maxshape = len(val[1])\n        if maxbyte < len(val[2]):\n            maxbyte = len(val[2])\n        if val[4]:\n            totalbytes += int(val[2])\n\n    if len(sta) > 0:\n        sp1 = max(10, maxname)\n        sp2 = max(10, maxshape)\n        sp3 = max(10, maxbyte)\n        prval = \'Name {} Shape {} Bytes {} Type\'.format(\n            sp1 * \' \', sp2 * \' \', sp3 * \' \'\n        )\n        print(""{}\\n{}\\n"".format(prval, ""="" * (len(prval) + 5)))\n\n    for k in range(len(sta)):\n        val = sta[k]\n        print(\n            \'{} {} {} {} {} {} {}\'.format(\n                val[0],\n                \' \' * (sp1 - len(val[0]) + 4),\n                val[1],\n                \' \' * (sp2 - len(val[1]) + 5),\n                val[2],\n                \' \' * (sp3 - len(val[2]) + 5),\n                val[3],\n            )\n        )\n    print(\'\\nUpper bound on total bytes  =       {}\'.format(totalbytes))\n'"
cupy/padding/__init__.py,0,"b'# Functions from the following NumPy document\n# https://docs.scipy.org/doc/numpy/reference/routines.padding.html\n\n# ""NOQA"" to suppress flake8 warning\nfrom cupy.padding import pad  # NOQA\n'"
cupy/padding/pad.py,0,"b'import numbers\n\nimport numpy\n\nimport cupy\n\n\n###############################################################################\n# Private utility functions.\n\n\ndef _round_if_needed(arr, dtype):\n    """"""Rounds arr inplace if the destination dtype is an integer.\n    """"""\n    if cupy.issubdtype(dtype, cupy.integer):\n        arr.round(out=arr)  # bug in round so use rint (cupy/cupy#2330)\n\n\ndef _slice_at_axis(sl, axis):\n    """"""Constructs a tuple of slices to slice an array in the given dimension.\n\n    Args:\n      sl(slice): The slice for the given dimension.\n      axis(int): The axis to which `sl` is applied. All other dimensions are\n          left ""unsliced"".\n\n    Returns:\n      tuple of slices: A tuple with slices matching `shape` in length.\n    """"""\n    return (slice(None),) * axis + (sl,) + (Ellipsis,)\n\n\ndef _view_roi(array, original_area_slice, axis):\n    """"""Gets a view of the current region of interest during iterative padding.\n\n    When padding multiple dimensions iteratively corner values are\n    unnecessarily overwritten multiple times. This function reduces the\n    working area for the first dimensions so that corners are excluded.\n\n    Args:\n      array(cupy.ndarray): The array with the region of interest.\n      original_area_slice(tuple of slices): Denotes the area with original\n          values of the unpadded array.\n      axis(int): The currently padded dimension assuming that `axis` is padded\n          before `axis` + 1.\n\n    Returns:\n    """"""\n    axis += 1\n    sl = (slice(None),) * axis + original_area_slice[axis:]\n    return array[sl]\n\n\ndef _pad_simple(array, pad_width, fill_value=None):\n    """"""Pads an array on all sides with either a constant or undefined values.\n\n    Args:\n      array(cupy.ndarray): Array to grow.\n      pad_width(sequence of tuple[int, int]): Pad width on both sides for each\n          dimension in `arr`.\n      fill_value(scalar, optional): If provided the padded area is\n          filled with this value, otherwise the pad area left undefined.\n          (Default value = None)\n    """"""\n    # Allocate grown array\n    new_shape = tuple(\n        left + size + right\n        for size, (left, right) in zip(array.shape, pad_width)\n    )\n    order = \'F\' if array.flags.fnc else \'C\'  # Fortran and not also C-order\n    padded = cupy.empty(new_shape, dtype=array.dtype, order=order)\n\n    if fill_value is not None:\n        padded.fill(fill_value)\n\n    # Copy old array into correct space\n    original_area_slice = tuple(\n        slice(left, left + size)\n        for size, (left, right) in zip(array.shape, pad_width)\n    )\n    padded[original_area_slice] = array\n\n    return padded, original_area_slice\n\n\ndef _set_pad_area(padded, axis, width_pair, value_pair):\n    """"""Set an empty-padded area in given dimension.\n    """"""\n    left_slice = _slice_at_axis(slice(None, width_pair[0]), axis)\n    padded[left_slice] = value_pair[0]\n\n    right_slice = _slice_at_axis(\n        slice(padded.shape[axis] - width_pair[1], None), axis\n    )\n    padded[right_slice] = value_pair[1]\n\n\ndef _get_edges(padded, axis, width_pair):\n    """"""Retrieves edge values from an empty-padded array along a given axis.\n\n    Args:\n      padded(cupy.ndarray): Empty-padded array.\n      axis(int): Dimension in which the edges are considered.\n      width_pair((int, int)): Pair of widths that mark the pad area on both\n          sides in the given dimension.\n    """"""\n    left_index = width_pair[0]\n    left_slice = _slice_at_axis(slice(left_index, left_index + 1), axis)\n    left_edge = padded[left_slice]\n\n    right_index = padded.shape[axis] - width_pair[1]\n    right_slice = _slice_at_axis(slice(right_index - 1, right_index), axis)\n    right_edge = padded[right_slice]\n\n    return left_edge, right_edge\n\n\ndef _get_linear_ramps(padded, axis, width_pair, end_value_pair):\n    """"""Constructs linear ramps for an empty-padded array along a given axis.\n\n    Args:\n      padded(cupy.ndarray): Empty-padded array.\n      axis(int): Dimension in which the ramps are constructed.\n      width_pair((int, int)): Pair of widths that mark the pad area on both\n          sides in the given dimension.\n      end_value_pair((scalar, scalar)): End values for the linear ramps which\n          form the edge of the fully padded array. These values are included in\n          the linear ramps.\n    """"""\n    edge_pair = _get_edges(padded, axis, width_pair)\n\n    left_ramp = cupy.linspace(\n        start=end_value_pair[0],\n        # squeeze axis replaced by linspace\n        stop=edge_pair[0].squeeze(axis),\n        num=width_pair[0],\n        endpoint=False,\n        dtype=padded.dtype,\n        axis=axis,\n    )\n\n    right_ramp = cupy.linspace(\n        start=end_value_pair[1],\n        # squeeze axis replaced by linspace\n        stop=edge_pair[1].squeeze(axis),\n        num=width_pair[1],\n        endpoint=False,\n        dtype=padded.dtype,\n        axis=axis,\n    )\n    # Reverse linear space in appropriate dimension\n    right_ramp = right_ramp[_slice_at_axis(slice(None, None, -1), axis)]\n\n    return left_ramp, right_ramp\n\n\ndef _get_stats(padded, axis, width_pair, length_pair, stat_func):\n    """"""Calculates a statistic for an empty-padded array along a given axis.\n\n    Args:\n      padded(cupy.ndarray): Empty-padded array.\n      axis(int): Dimension in which the statistic is calculated.\n      width_pair((int, int)): Pair of widths that mark the pad area on both\n          sides in the given dimension.\n      length_pair(2-element sequence of None or int): Gives the number of\n          values in valid area from each side that is taken into account when\n          calculating the statistic. If None the entire valid area in `padded`\n          is considered.\n      stat_func(function): Function to compute statistic. The expected\n          signature is\n          ``stat_func(x: ndarray, axis: int, keepdims: bool) -> ndarray``.\n    """"""\n    # Calculate indices of the edges of the area with original values\n    left_index = width_pair[0]\n    right_index = padded.shape[axis] - width_pair[1]\n    # as well as its length\n    max_length = right_index - left_index\n\n    # Limit stat_lengths to max_length\n    left_length, right_length = length_pair\n    if left_length is None or max_length < left_length:\n        left_length = max_length\n    if right_length is None or max_length < right_length:\n        right_length = max_length\n\n    # Calculate statistic for the left side\n    left_slice = _slice_at_axis(\n        slice(left_index, left_index + left_length), axis\n    )\n    left_chunk = padded[left_slice]\n    left_stat = stat_func(left_chunk, axis=axis, keepdims=True)\n    _round_if_needed(left_stat, padded.dtype)\n\n    if left_length == right_length == max_length:\n        # return early as right_stat must be identical to left_stat\n        return left_stat, left_stat\n\n    # Calculate statistic for the right side\n    right_slice = _slice_at_axis(\n        slice(right_index - right_length, right_index), axis\n    )\n    right_chunk = padded[right_slice]\n    right_stat = stat_func(right_chunk, axis=axis, keepdims=True)\n    _round_if_needed(right_stat, padded.dtype)\n    return left_stat, right_stat\n\n\ndef _set_reflect_both(padded, axis, width_pair, method, include_edge=False):\n    """"""Pads an `axis` of `arr` using reflection.\n\n    Args:\n      padded(cupy.ndarray): Input array of arbitrary shape.\n      axis(int): Axis along which to pad `arr`.\n      width_pair((int, int)): Pair of widths that mark the pad area on both\n          sides in the given dimension.\n      method(str): Controls method of reflection; options are \'even\' or \'odd\'.\n      include_edge(bool, optional): If true, edge value is included in\n          reflection, otherwise the edge value forms the symmetric axis to the\n          reflection. (Default value = False)\n    """"""\n    left_pad, right_pad = width_pair\n    old_length = padded.shape[axis] - right_pad - left_pad\n\n    if include_edge:\n        # Edge is included, we need to offset the pad amount by 1\n        edge_offset = 1\n    else:\n        edge_offset = 0  # Edge is not included, no need to offset pad amount\n        old_length -= 1  # but must be omitted from the chunk\n\n    if left_pad > 0:\n        # Pad with reflected values on left side:\n        # First limit chunk size which can\'t be larger than pad area\n        chunk_length = min(old_length, left_pad)\n        # Slice right to left, stop on or next to edge, start relative to stop\n        stop = left_pad - edge_offset\n        start = stop + chunk_length\n        left_slice = _slice_at_axis(slice(start, stop, -1), axis)\n        left_chunk = padded[left_slice]\n\n        if method == \'odd\':\n            # Negate chunk and align with edge\n            edge_slice = _slice_at_axis(slice(left_pad, left_pad + 1), axis)\n            left_chunk = 2 * padded[edge_slice] - left_chunk\n\n        # Insert chunk into padded area\n        start = left_pad - chunk_length\n        stop = left_pad\n        pad_area = _slice_at_axis(slice(start, stop), axis)\n        padded[pad_area] = left_chunk\n        # Adjust pointer to left edge for next iteration\n        left_pad -= chunk_length\n\n    if right_pad > 0:\n        # Pad with reflected values on right side:\n        # First limit chunk size which can\'t be larger than pad area\n        chunk_length = min(old_length, right_pad)\n        # Slice right to left, start on or next to edge, stop relative to start\n        start = -right_pad + edge_offset - 2\n        stop = start - chunk_length\n        right_slice = _slice_at_axis(slice(start, stop, -1), axis)\n        right_chunk = padded[right_slice]\n\n        if method == \'odd\':\n            # Negate chunk and align with edge\n            edge_slice = _slice_at_axis(\n                slice(-right_pad - 1, -right_pad), axis\n            )\n            right_chunk = 2 * padded[edge_slice] - right_chunk\n\n        # Insert chunk into padded area\n        start = padded.shape[axis] - right_pad\n        stop = start + chunk_length\n        pad_area = _slice_at_axis(slice(start, stop), axis)\n        padded[pad_area] = right_chunk\n        # Adjust pointer to right edge for next iteration\n        right_pad -= chunk_length\n\n    return left_pad, right_pad\n\n\ndef _set_wrap_both(padded, axis, width_pair):\n    """"""Pads an `axis` of `arr` with wrapped values.\n\n    Args:\n      padded(cupy.ndarray): Input array of arbitrary shape.\n      axis(int): Axis along which to pad `arr`.\n      width_pair((int, int)): Pair of widths that mark the pad area on both\n          sides in the given dimension.\n    """"""\n    left_pad, right_pad = width_pair\n    period = padded.shape[axis] - right_pad - left_pad\n\n    # If the current dimension of `arr` doesn\'t contain enough valid values\n    # (not part of the undefined pad area) we need to pad multiple times.\n    # Each time the pad area shrinks on both sides which is communicated with\n    # these variables.\n    new_left_pad = 0\n    new_right_pad = 0\n\n    if left_pad > 0:\n        # Pad with wrapped values on left side\n        # First slice chunk from right side of the non-pad area.\n        # Use min(period, left_pad) to ensure that chunk is not larger than\n        # pad area\n        right_slice = _slice_at_axis(\n            slice(\n                -right_pad - min(period, left_pad),\n                -right_pad if right_pad != 0 else None,\n            ),\n            axis,\n        )\n        right_chunk = padded[right_slice]\n\n        if left_pad > period:\n            # Chunk is smaller than pad area\n            pad_area = _slice_at_axis(slice(left_pad - period, left_pad), axis)\n            new_left_pad = left_pad - period\n        else:\n            # Chunk matches pad area\n            pad_area = _slice_at_axis(slice(None, left_pad), axis)\n        padded[pad_area] = right_chunk\n\n    if right_pad > 0:\n        # Pad with wrapped values on right side\n        # First slice chunk from left side of the non-pad area.\n        # Use min(period, right_pad) to ensure that chunk is not larger than\n        # pad area\n        left_slice = _slice_at_axis(\n            slice(left_pad, left_pad + min(period, right_pad)), axis\n        )\n        left_chunk = padded[left_slice]\n\n        if right_pad > period:\n            # Chunk is smaller than pad area\n            pad_area = _slice_at_axis(\n                slice(-right_pad, -right_pad + period), axis\n            )\n            new_right_pad = right_pad - period\n        else:\n            # Chunk matches pad area\n            pad_area = _slice_at_axis(slice(-right_pad, None), axis)\n        padded[pad_area] = left_chunk\n\n    return new_left_pad, new_right_pad\n\n\ndef _as_pairs(x, ndim, as_index=False):\n    """"""Broadcasts `x` to an array with shape (`ndim`, 2).\n\n    A helper function for `pad` that prepares and validates arguments like\n    `pad_width` for iteration in pairs.\n\n    Args:\n      x(scalar or array-like, optional): The object to broadcast to the shape\n          (`ndim`, 2).\n      ndim(int): Number of pairs the broadcasted `x` will have.\n      as_index(bool, optional): If `x` is not None, try to round each\n          element of `x` to an integer (dtype `cupy.intp`) and ensure every\n          element is positive. (Default value = False)\n\n    Returns:\n      nested iterables, shape (`ndim`, 2): The broadcasted version of `x`.\n    """"""\n    if x is None:\n        # Pass through None as a special case, otherwise cupy.round(x) fails\n        # with an AttributeError\n        return ((None, None),) * ndim\n    elif isinstance(x, numbers.Number):\n        if as_index:\n            x = round(x)\n        return ((x, x),) * ndim\n\n    x = numpy.array(x)\n    if as_index:\n        x = numpy.asarray(numpy.round(x), dtype=numpy.intp)\n\n    if x.ndim < 3:\n        # Optimization: Possibly use faster paths for cases where `x` has\n        # only 1 or 2 elements. `numpy.broadcast_to` could handle these as well\n        # but is currently slower\n\n        if x.size == 1:\n            # x was supplied as a single value\n            x = x.ravel()  # Ensure x[0] works for x.ndim == 0, 1, 2\n            if as_index and x < 0:\n                raise ValueError(""index can\'t contain negative values"")\n            return ((x[0], x[0]),) * ndim\n\n        if x.size == 2 and x.shape != (2, 1):\n            # x was supplied with a single value for each side\n            # but except case when each dimension has a single value\n            # which should be broadcasted to a pair,\n            # e.g. [[1], [2]] -> [[1, 1], [2, 2]] not [[1, 2], [1, 2]]\n            x = x.ravel()  # Ensure x[0], x[1] works\n            if as_index and (x[0] < 0 or x[1] < 0):\n                raise ValueError(""index can\'t contain negative values"")\n            return ((x[0], x[1]),) * ndim\n\n    if as_index and x.min() < 0:\n        raise ValueError(""index can\'t contain negative values"")\n\n    # Converting the array with `tolist` seems to improve performance\n    # when iterating and indexing the result (see usage in `pad`)\n    x_view = x.view()\n    x_view.shape = (ndim, 2)\n    return x_view.tolist()\n\n# def _pad_dispatcher(array, pad_width, mode=None, **kwargs):\n#    return (array,)\n\n\n###############################################################################\n# Public functions\n\n\n# @array_function_dispatch(_pad_dispatcher, module=\'numpy\')\ndef pad(array, pad_width, mode=\'constant\', **kwargs):\n    """"""Pads an array with specified widths and values.\n\n    Args:\n      array(cupy.ndarray): The array to pad.\n      pad_width(sequence, array_like or int): Number of values padded to the\n          edges of each axis. ((before_1, after_1), ... (before_N, after_N))\n          unique pad widths for each axis. ((before, after),) yields same\n          before and after pad for each axis. (pad,) or int is a shortcut for\n          before = after = pad width for all axes. You cannot specify\n          ``cupy.ndarray``.\n      mode(str or function, optional): One of the following string values or a\n          user supplied function\n\n          \'constant\' (default)\n              Pads with a constant value.\n          \'edge\'\n              Pads with the edge values of array.\n          \'linear_ramp\'\n              Pads with the linear ramp between end_value and the array edge\n              value.\n          \'maximum\'\n              Pads with the maximum value of all or part of the vector along\n              each axis.\n          \'mean\'\n              Pads with the mean value of all or part of the vector along each\n              axis.\n          \'median\'\n              Pads with the median value of all or part of the vector along\n              each axis. (Not Implemented)\n          \'minimum\'\n              Pads with the minimum value of all or part of the vector along\n              each axis.\n          \'reflect\'\n              Pads with the reflection of the vector mirrored on the first and\n              last values of the vector along each axis.\n          \'symmetric\'\n               Pads with the reflection of the vector mirrored along the edge\n               of the array.\n          \'wrap\'\n              Pads with the wrap of the vector along the axis. The first\n              values are used to pad the end and the end values are used to\n              pad the beginning.\n          \'empty\'\n              Pads with undefined values.\n          <function>\n              Padding function, see Notes.\n      stat_length(sequence or int, optional): Used in \'maximum\', \'mean\',\n          \'median\', and \'minimum\'.  Number of values at edge of each axis used\n          to calculate the statistic value.\n          ((before_1, after_1), ... (before_N, after_N)) unique statistic\n          lengths for each axis. ((before, after),) yields same before and\n          after statistic lengths for each axis. (stat_length,) or int is a\n          shortcut for before = after = statistic length for all axes.\n          Default is ``None``, to use the entire axis. You cannot specify\n          ``cupy.ndarray``.\n      constant_values(sequence or scalar, optional): Used in \'constant\'. The\n          values to set the padded values for each axis.\n          ((before_1, after_1), ... (before_N, after_N)) unique pad constants\n          for each axis.\n          ((before, after),) yields same before and after constants for each\n          axis.\n          (constant,) or constant is a shortcut for before = after = constant\n          for all axes.\n          Default is 0. You cannot specify ``cupy.ndarray``.\n      end_values(sequence or scalar, optional): Used in \'linear_ramp\'. The\n          values used for the ending value of the linear_ramp and that will\n          form the edge of the padded array.\n          ((before_1, after_1), ... (before_N, after_N)) unique end values\n          for each axis.\n          ((before, after),) yields same before and after end\n          values for each axis.\n          (constant,) or constant is a shortcut for before = after = constant\n          for all axes.\n          Default is 0. You cannot specify ``cupy.ndarray``.\n      reflect_type({\'even\', \'odd\'}, optional): Used in \'reflect\', and\n          \'symmetric\'.  The \'even\' style is the default with an unaltered\n          reflection around the edge value.  For the \'odd\' style, the extended\n          part of the array is created by subtracting the reflected values from\n          two times the edge value.\n\n    Returns:\n      cupy.ndarray: Padded array with shape extended by ``pad_width``.\n\n    .. note::\n        For an array with rank greater than 1, some of the padding of later\n        axes is calculated from padding of previous axes.  This is easiest to\n        think about with a rank 2 array where the corners of the padded array\n        are calculated by using padded values from the first axis.\n\n        The padding function, if used, should modify a rank 1 array in-place.\n        It has the following signature:\n\n        ``padding_func(vector, iaxis_pad_width, iaxis, kwargs)``\n\n        where\n\n        vector (cupy.ndarray)\n            A rank 1 array already padded with zeros.  Padded values are\n            ``vector[:iaxis_pad_width[0]]`` and\n            ``vector[-iaxis_pad_width[1]:]``.\n        iaxis_pad_width (tuple)\n            A 2-tuple of ints, ``iaxis_pad_width[0]`` represents the number of\n            values padded at the beginning of vector where\n            ``iaxis_pad_width[1]`` represents the number of values padded at\n            the end of vector.\n        iaxis (int)\n            The axis currently being calculated.\n        kwargs (dict)\n            Any keyword arguments the function requires.\n\n    Examples\n    --------\n    >>> a = cupy.array([1, 2, 3, 4, 5])\n    >>> cupy.pad(a, (2, 3), \'constant\', constant_values=(4, 6))\n    array([4, 4, 1, ..., 6, 6, 6])\n\n    >>> cupy.pad(a, (2, 3), \'edge\')\n    array([1, 1, 1, ..., 5, 5, 5])\n\n    >>> cupy.pad(a, (2, 3), \'linear_ramp\', end_values=(5, -4))\n    array([ 5,  3,  1,  2,  3,  4,  5,  2, -1, -4])\n\n    >>> cupy.pad(a, (2,), \'maximum\')\n    array([5, 5, 1, 2, 3, 4, 5, 5, 5])\n\n    >>> cupy.pad(a, (2,), \'mean\')\n    array([3, 3, 1, 2, 3, 4, 5, 3, 3])\n\n    >>> a = cupy.array([[1, 2], [3, 4]])\n    >>> cupy.pad(a, ((3, 2), (2, 3)), \'minimum\')\n    array([[1, 1, 1, 2, 1, 1, 1],\n           [1, 1, 1, 2, 1, 1, 1],\n           [1, 1, 1, 2, 1, 1, 1],\n           [1, 1, 1, 2, 1, 1, 1],\n           [3, 3, 3, 4, 3, 3, 3],\n           [1, 1, 1, 2, 1, 1, 1],\n           [1, 1, 1, 2, 1, 1, 1]])\n\n    >>> a = cupy.array([1, 2, 3, 4, 5])\n    >>> cupy.pad(a, (2, 3), \'reflect\')\n    array([3, 2, 1, 2, 3, 4, 5, 4, 3, 2])\n\n    >>> cupy.pad(a, (2, 3), \'reflect\', reflect_type=\'odd\')\n    array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8])\n\n    >>> cupy.pad(a, (2, 3), \'symmetric\')\n    array([2, 1, 1, 2, 3, 4, 5, 5, 4, 3])\n\n    >>> cupy.pad(a, (2, 3), \'symmetric\', reflect_type=\'odd\')\n    array([0, 1, 1, 2, 3, 4, 5, 5, 6, 7])\n\n    >>> cupy.pad(a, (2, 3), \'wrap\')\n    array([4, 5, 1, 2, 3, 4, 5, 1, 2, 3])\n\n    >>> def pad_with(vector, pad_width, iaxis, kwargs):\n    ...     pad_value = kwargs.get(\'padder\', 10)\n    ...     vector[:pad_width[0]] = pad_value\n    ...     vector[-pad_width[1]:] = pad_value\n    >>> a = cupy.arange(6)\n    >>> a = a.reshape((2, 3))\n    >>> cupy.pad(a, 2, pad_with)\n    array([[10, 10, 10, 10, 10, 10, 10],\n           [10, 10, 10, 10, 10, 10, 10],\n           [10, 10,  0,  1,  2, 10, 10],\n           [10, 10,  3,  4,  5, 10, 10],\n           [10, 10, 10, 10, 10, 10, 10],\n           [10, 10, 10, 10, 10, 10, 10]])\n    >>> cupy.pad(a, 2, pad_with, padder=100)\n    array([[100, 100, 100, 100, 100, 100, 100],\n           [100, 100, 100, 100, 100, 100, 100],\n           [100, 100,   0,   1,   2, 100, 100],\n           [100, 100,   3,   4,   5, 100, 100],\n           [100, 100, 100, 100, 100, 100, 100],\n           [100, 100, 100, 100, 100, 100, 100]])\n    """"""\n    if isinstance(pad_width, numbers.Integral):\n        pad_width = ((pad_width, pad_width),) * array.ndim\n    else:\n        pad_width = numpy.asarray(pad_width)\n\n        if not pad_width.dtype.kind == \'i\':\n            raise TypeError(\'`pad_width` must be of integral type.\')\n\n        # Broadcast to shape (array.ndim, 2)\n        pad_width = _as_pairs(pad_width, array.ndim, as_index=True)\n\n    if callable(mode):\n        # Old behavior: Use user-supplied function with numpy.apply_along_axis\n        function = mode\n        # Create a new zero padded array\n        padded, _ = _pad_simple(array, pad_width, fill_value=0)\n        # And apply along each axis\n\n        for axis in range(padded.ndim):\n            # Iterate using ndindex as in apply_along_axis, but assuming that\n            # function operates inplace on the padded array.\n\n            # view with the iteration axis at the end\n            view = cupy.moveaxis(padded, axis, -1)\n\n            # compute indices for the iteration axes, and append a trailing\n            # ellipsis to prevent 0d arrays decaying to scalars (gh-8642)\n            inds = numpy.ndindex(view.shape[:-1])\n            inds = (ind + (Ellipsis,) for ind in inds)\n            for ind in inds:\n                function(view[ind], pad_width[axis], axis, kwargs)\n\n        return padded\n\n    # Make sure that no unsupported keywords were passed for the current mode\n    allowed_kwargs = {\n        \'empty\': [],\n        \'edge\': [],\n        \'wrap\': [],\n        \'constant\': [\'constant_values\'],\n        \'linear_ramp\': [\'end_values\'],\n        \'maximum\': [\'stat_length\'],\n        \'mean\': [\'stat_length\'],\n        # \'median\': [\'stat_length\'],\n        \'minimum\': [\'stat_length\'],\n        \'reflect\': [\'reflect_type\'],\n        \'symmetric\': [\'reflect_type\'],\n    }\n    try:\n        unsupported_kwargs = set(kwargs) - set(allowed_kwargs[mode])\n    except KeyError:\n        raise ValueError(""mode \'{}\' is not supported"".format(mode))\n    if unsupported_kwargs:\n        raise ValueError(\n            ""unsupported keyword arguments for mode \'{}\': {}"".format(\n                mode, unsupported_kwargs\n            )\n        )\n\n    if mode == \'constant\':\n        values = kwargs.get(\'constant_values\', 0)\n        if isinstance(values, numbers.Number) and values == 0 and (\n                array.ndim == 1 or array.size < 4e6):\n            # faster path for 1d arrays or small n-dimensional arrays\n            return _pad_simple(array, pad_width, 0)[0]\n\n    stat_functions = {\n        \'maximum\': cupy.max,\n        \'minimum\': cupy.min,\n        \'mean\': cupy.mean,\n        # \'median\': cupy.median,\n    }\n\n    # Create array with final shape and original values\n    # (padded area is undefined)\n    padded, original_area_slice = _pad_simple(array, pad_width)\n    # And prepare iteration over all dimensions\n    # (zipping may be more readable than using enumerate)\n    axes = range(padded.ndim)\n\n    if mode == \'constant\':\n        values = _as_pairs(values, padded.ndim)\n        for axis, width_pair, value_pair in zip(axes, pad_width, values):\n            roi = _view_roi(padded, original_area_slice, axis)\n            _set_pad_area(roi, axis, width_pair, value_pair)\n\n    elif mode == \'empty\':\n        pass  # Do nothing as _pad_simple already returned the correct result\n\n    elif array.size == 0:\n        # Only modes \'constant\' and \'empty\' can extend empty axes, all other\n        # modes depend on `array` not being empty\n        # -> ensure every empty axis is only \'padded with 0\'\n        for axis, width_pair in zip(axes, pad_width):\n            if array.shape[axis] == 0 and any(width_pair):\n                raise ValueError(\n                    ""can\'t extend empty axis {} using modes other than ""\n                    ""\'constant\' or \'empty\'"".format(axis)\n                )\n        # passed, don\'t need to do anything more as _pad_simple already\n        # returned the correct result\n\n    elif mode == \'edge\':\n        for axis, width_pair in zip(axes, pad_width):\n            roi = _view_roi(padded, original_area_slice, axis)\n            edge_pair = _get_edges(roi, axis, width_pair)\n            _set_pad_area(roi, axis, width_pair, edge_pair)\n\n    elif mode == \'linear_ramp\':\n        end_values = kwargs.get(\'end_values\', 0)\n        end_values = _as_pairs(end_values, padded.ndim)\n        for axis, width_pair, value_pair in zip(axes, pad_width, end_values):\n            roi = _view_roi(padded, original_area_slice, axis)\n            ramp_pair = _get_linear_ramps(roi, axis, width_pair, value_pair)\n            _set_pad_area(roi, axis, width_pair, ramp_pair)\n\n    elif mode in stat_functions:\n        func = stat_functions[mode]\n        length = kwargs.get(\'stat_length\', None)\n        length = _as_pairs(length, padded.ndim, as_index=True)\n        for axis, width_pair, length_pair in zip(axes, pad_width, length):\n            roi = _view_roi(padded, original_area_slice, axis)\n            stat_pair = _get_stats(roi, axis, width_pair, length_pair, func)\n            _set_pad_area(roi, axis, width_pair, stat_pair)\n\n    elif mode in {\'reflect\', \'symmetric\'}:\n        method = kwargs.get(\'reflect_type\', \'even\')\n        include_edge = True if mode == \'symmetric\' else False\n        for axis, (left_index, right_index) in zip(axes, pad_width):\n            if array.shape[axis] == 1 and (left_index > 0 or right_index > 0):\n                # Extending singleton dimension for \'reflect\' is legacy\n                # behavior; it really should raise an error.\n                edge_pair = _get_edges(padded, axis, (left_index, right_index))\n                _set_pad_area(\n                    padded, axis, (left_index, right_index), edge_pair\n                )\n                continue\n\n            roi = _view_roi(padded, original_area_slice, axis)\n            while left_index > 0 or right_index > 0:\n                # Iteratively pad until dimension is filled with reflected\n                # values. This is necessary if the pad area is larger than\n                # the length of the original values in the current dimension.\n                left_index, right_index = _set_reflect_both(\n                    roi, axis, (left_index, right_index), method, include_edge\n                )\n\n    elif mode == \'wrap\':\n        for axis, (left_index, right_index) in zip(axes, pad_width):\n            roi = _view_roi(padded, original_area_slice, axis)\n            while left_index > 0 or right_index > 0:\n                # Iteratively pad until dimension is filled with wrapped\n                # values. This is necessary if the pad area is larger than\n                # the length of the original values in the current dimension.\n                left_index, right_index = _set_wrap_both(\n                    roi, axis, (left_index, right_index)\n                )\n\n    return padded\n'"
cupy/prof/__init__.py,0,b'from cupy.prof.time_range import time_range  # NOQA\nfrom cupy.prof.time_range import TimeRangeDecorator  # NOQA\n'
cupy/prof/time_range.py,0,"b'import contextlib\nimport functools\n\nfrom cupy import cuda\nfrom cupy.cuda import runtime\n\n\n@contextlib.contextmanager\ndef time_range(message, color_id=None, argb_color=None, sync=False):\n    """"""A context manager to describe the enclosed block as a nested range\n\n    >>> from cupy import prof\n    >>> with cupy.prof.time_range(\'some range in green\', color_id=0):\n    ...    # do something you want to measure\n    ...    pass\n\n    Args:\n        message: Name of a range.\n        color_id: range color ID\n        argb_color: range color in ARGB (e.g. 0xFF00FF00 for green)\n        sync (bool): If ``True``, waits for completion of all outstanding\n            processing on GPU before calling :func:`cupy.cuda.nvtx.RangePush()`\n            or :func:`cupy.cuda.nvtx.RangePop()`\n\n    .. seealso:: :func:`cupy.cuda.nvtx.RangePush`\n        :func:`cupy.cuda.nvtx.RangePop`\n    """"""\n    if not cuda.nvtx_enabled:\n        raise RuntimeError(\'nvtx is not installed\')\n\n    if color_id is not None and argb_color is not None:\n        raise ValueError(\'Only either color_id or argb_color can be specified\')\n\n    if sync:\n        runtime.deviceSynchronize()\n    if argb_color is not None:\n        cuda.nvtx.RangePushC(message, argb_color)\n    else:\n        if color_id is None:\n            color_id = -1\n        cuda.nvtx.RangePush(message, color_id)\n    try:\n        yield\n    finally:\n        if sync:\n            runtime.deviceSynchronize()\n        cuda.nvtx.RangePop()\n\n\nclass TimeRangeDecorator(object):\n    """"""Decorator to mark function calls with range in NVIDIA profiler\n\n    Decorated function calls are marked as ranges in NVIDIA profiler timeline.\n\n    >>> from cupy import prof\n    >>> @cupy.prof.TimeRangeDecorator()\n    ... def function_to_profile():\n    ...     pass\n\n    Args:\n        message (str): Name of a range, default use ``func.__name__``.\n        color_id: range color ID\n        argb_color: range color in ARGB (e.g. 0xFF00FF00 for green)\n        sync (bool): If ``True``, waits for completion of all outstanding\n            processing on GPU before calling :func:`cupy.cuda.nvtx.RangePush()`\n            or :func:`cupy.cuda.nvtx.RangePop()`\n\n    .. seealso:: :func:`cupy.cuda.nvtx.RangePush`\n        :func:`cupy.cuda.nvtx.RangePop`\n    """"""\n\n    def __init__(self, message=None, color_id=None, argb_color=None,\n                 sync=False):\n        if not cuda.nvtx_enabled:\n            raise RuntimeError(\'nvtx is not installed\')\n\n        if color_id is not None and argb_color is not None:\n            raise ValueError(\n                \'Only either color_id or argb_color can be specified\'\n            )\n        self.message = message\n        self.color_id = color_id if color_id is not None else -1\n        self.argb_color = argb_color\n        self.sync = sync\n\n    def __enter__(self):\n        if self.sync:\n            runtime.deviceSynchronize()\n        if self.argb_color is not None:\n            cuda.nvtx.RangePushC(self.message, self.argb_color)\n        else:\n            cuda.nvtx.RangePush(self.message, self.color_id)\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if self.sync:\n            runtime.deviceSynchronize()\n        cuda.nvtx.RangePop()\n\n    def _recreate_cm(self, message):\n        if self.message is None:\n            self.message = message\n        return self\n\n    def __call__(self, func):\n        @functools.wraps(func)\n        def inner(*args, **kwargs):\n            with self._recreate_cm(func.__name__):\n                return func(*args, **kwargs)\n        return inner\n'"
cupy/random/__init__.py,0,"b'import numpy\n\n\ndef bytes(length):\n    """"""Returns random bytes.\n\n    .. seealso:: :meth:`numpy.random.bytes\n                 <numpy.random.mtrand.RandomState.bytes>`\n    """"""\n    return numpy.bytes(length)\n\n\nfrom cupy.random import distributions  # NOQA\nfrom cupy.random import generator  # NOQA\nfrom cupy.random import permutations  # NOQA\nfrom cupy.random import sample as sample_  # NOQA\n\n\n# import class and function\nfrom cupy.random.distributions import beta  # NOQA\nfrom cupy.random.distributions import binomial  # NOQA\nfrom cupy.random.distributions import chisquare  # NOQA\nfrom cupy.random.distributions import dirichlet  # NOQA\nfrom cupy.random.distributions import exponential  # NOQA\nfrom cupy.random.distributions import f  # NOQA\nfrom cupy.random.distributions import gamma  # NOQA\nfrom cupy.random.distributions import geometric  # NOQA\nfrom cupy.random.distributions import gumbel  # NOQA\nfrom cupy.random.distributions import hypergeometric  # NOQA\nfrom cupy.random.distributions import laplace  # NOQA\nfrom cupy.random.distributions import logistic  # NOQA\nfrom cupy.random.distributions import lognormal  # NOQA\nfrom cupy.random.distributions import logseries  # NOQA\nfrom cupy.random.distributions import multivariate_normal  # NOQA\nfrom cupy.random.distributions import negative_binomial  # NOQA\nfrom cupy.random.distributions import noncentral_chisquare  # NOQA\nfrom cupy.random.distributions import noncentral_f  # NOQA\nfrom cupy.random.distributions import normal  # NOQA\nfrom cupy.random.distributions import pareto  # NOQA\nfrom cupy.random.distributions import poisson  # NOQA\nfrom cupy.random.distributions import power  # NOQA\nfrom cupy.random.distributions import rayleigh  # NOQA\nfrom cupy.random.distributions import standard_cauchy  # NOQA\nfrom cupy.random.distributions import standard_exponential  # NOQA\nfrom cupy.random.distributions import standard_gamma  # NOQA\nfrom cupy.random.distributions import standard_normal  # NOQA\nfrom cupy.random.distributions import standard_t  # NOQA\nfrom cupy.random.distributions import triangular  # NOQA\nfrom cupy.random.distributions import uniform  # NOQA\nfrom cupy.random.distributions import vonmises  # NOQA\nfrom cupy.random.distributions import wald  # NOQA\nfrom cupy.random.distributions import weibull  # NOQA\nfrom cupy.random.distributions import zipf  # NOQA\nfrom cupy.random.generator import get_random_state  # NOQA\nfrom cupy.random.generator import RandomState  # NOQA\nfrom cupy.random.generator import reset_states  # NOQA\nfrom cupy.random.generator import seed  # NOQA\nfrom cupy.random.generator import set_random_state  # NOQA\nfrom cupy.random.permutations import permutation  # NOQA\nfrom cupy.random.permutations import shuffle  # NOQA\nfrom cupy.random.sample import choice  # NOQA\nfrom cupy.random.sample import multinomial  # NOQA\nfrom cupy.random.sample import rand  # NOQA\nfrom cupy.random.sample import randint  # NOQA\nfrom cupy.random.sample import randn  # NOQA\nfrom cupy.random.sample import random_integers  # NOQA\nfrom cupy.random.sample import random_sample  # NOQA\nfrom cupy.random.sample import random_sample as random  # NOQA\nfrom cupy.random.sample import random_sample as ranf  # NOQA\nfrom cupy.random.sample import random_sample as sample  # NOQA\n'"
cupy/random/_kernels.py,0,"b'""""""\nRandom kit 1.3\n\nCopyright (c) 2003-2005, Jean-Sebastien Roy (js@jeannot.org)\n\nThe rk_random and rk_seed functions algorithms and the original design of\nthe Mersenne Twister RNG:\n\n  Copyright (C) 1997 - 2002, Makoto Matsumoto and Takuji Nishimura,\n  All rights reserved.\n\n  Redistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions\n  are met:\n\n  1. Redistributions of source code must retain the above copyright\n  notice, this list of conditions and the following disclaimer.\n\n  2. Redistributions in binary form must reproduce the above copyright\n  notice, this list of conditions and the following disclaimer in the\n  documentation and/or other materials provided with the distribution.\n\n  3. The names of its contributors may not be used to endorse or promote\n  products derived from this software without specific prior written\n  permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n  ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n  A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nOriginal algorithm for the implementation of rk_interval function from\nRichard J. Wagner\'s implementation of the Mersenne Twister RNG, optimised by\nMagnus Jonsson.\n\nConstants used in the rk_double implementation by Isaku Wada.\n\nPermission is hereby granted, free of charge, to any person obtaining a\ncopy of this software and associated documentation files (the\n""Software""), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be included\nin all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS\nOR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n""""""  # NOQA\n\nfrom cupy import core\n\n\nrk_use_binominal = \'\'\'\n#define CUPY_USE_BINOMIAL\n\'\'\'\n\nrk_basic_definition = \'\'\'\ntypedef struct {\n    unsigned int xor128[4];\n    double gauss;\n    int has_gauss; // !=0: gauss contains a gaussian deviate\n\n#ifdef CUPY_USE_BINOMIAL\n    int has_binomial; // !=0: following parameters initialized for binomial\n    /* The rk_state structure has been extended to store the following\n     * information for the binomial generator. If the input values of n or p\n     * are different than nsave and psave, then the other parameters will be\n     * recomputed. RTK 2005-09-02 */\n    int nsave, m;\n    double psave, r, q, fm, p1, xm, xl, xr, c, laml, lamr, p2, p3, p4;\n#endif\n} rk_state;\n\n\n__device__ void rk_seed(unsigned long long s, rk_state *state) {\n    for (int i = 1; i <= 4; i++) {\n        s = 1812433253U * (s ^ (s >> 30)) + i;\n        state->xor128[i - 1] = s;\n    }\n    state->has_gauss = 0;\n#ifdef CUPY_USE_BINOMIAL\n    state->has_binomial = 0;\n#endif\n}\n\n\n__device__ unsigned long rk_random(rk_state *state) {\n    unsigned int *xor128 = state->xor128;\n    unsigned int t = xor128[0] ^ (xor128[0] << 11);\n    xor128[0] = xor128[1];\n    xor128[1] = xor128[2];\n    xor128[2] = xor128[3];\n    return xor128[3] ^= (xor128[3] >> 19) ^ t ^ (t >> 8);\n}\n\n\n__device__ double rk_double(rk_state *state) {\n    /* shifts : 67108864 = 0x4000000, 9007199254740992 = 0x20000000000000 */\n    int a = rk_random(state) >> 5, b = rk_random(state) >> 6;\n    return (a * 67108864.0 + b) / 9007199254740992.0;\n}\n\'\'\'\n\n\n# The kernels for distributions are based on\n# numpy/random/mtrand/distributions.c\n# with the following licenses:\n""""""\n/* Copyright 2005 Robert Kern (robert.kern@gmail.com)\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the\n * ""Software""), to deal in the Software without restriction, including\n * without limitation the rights to use, copy, modify, merge, publish,\n * distribute, sublicense, and/or sell copies of the Software, and to\n * permit persons to whom the Software is furnished to do so, subject to\n * the following conditions:\n *\n * The above copyright notice and this permission notice shall be included\n * in all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n */\n\n/* The implementations of rk_hypergeometric_hyp(), rk_hypergeometric_hrua(),\n * and rk_triangular() were adapted from Ivan Frohne\'s rv.py which has this\n * license:\n *\n *            Copyright 1998 by Ivan Frohne; Wasilla, Alaska, U.S.A.\n *                            All Rights Reserved\n *\n * Permission to use, copy, modify and distribute this software and its\n * documentation for any purpose, free of charge, is granted subject to the\n * following conditions:\n *   The above copyright notice and this permission notice shall be included in\n *   all copies or substantial portions of the software.\n *\n *   THE SOFTWARE AND DOCUMENTATION IS PROVIDED WITHOUT WARRANTY OF ANY KIND,\n *   EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO MERCHANTABILITY, FITNESS\n *   FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE AUTHOR\n *   OR COPYRIGHT HOLDER BE LIABLE FOR ANY CLAIM OR DAMAGES IN A CONTRACT\n *   ACTION, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n *   SOFTWARE OR ITS DOCUMENTATION.\n */\n""""""  # NOQA\n\nloggam_definition = \'\'\'\n/*\n * log-gamma function to support some of these distributions. The\n * algorithm comes from SPECFUN by Shanjie Zhang and Jianming Jin and their\n * book ""Computation of Special Functions"", 1996, John Wiley & Sons, Inc.\n */\nstatic __device__ double loggam(double x) {\n    double x0, x2, xp, gl, gl0;\n    long k, n;\n    double a[10] = {8.333333333333333e-02,-2.777777777777778e-03,\n         7.936507936507937e-04,-5.952380952380952e-04,\n         8.417508417508418e-04,-1.917526917526918e-03,\n         6.410256410256410e-03,-2.955065359477124e-02,\n         1.796443723688307e-01,-1.39243221690590e+00};\n    x0 = x;\n    n = 0;\n    if ((x == 1.0) || (x == 2.0)) {\n        return 0.0;\n    } else if (x <= 7.0) {\n        n = (long)(7 - x);\n        x0 = x + n;\n    }\n    x2 = 1.0/(x0*x0);\n    xp = 2*M_PI;\n    gl0 = a[9];\n    for (k=8; k>=0; k--) {\n        gl0 *= x2;\n        gl0 += a[k];\n    }\n    gl = gl0/x0 + 0.5*log(xp) + (x0-0.5)*log(x0) - x0;\n    if (x <= 7.0) {\n        for (k=1; k<=n; k++) {\n            gl -= log(x0-1.0);\n            x0 -= 1.0;\n        }\n    }\n    return gl;\n}\n\'\'\'\n\nrk_standard_exponential_definition = \'\'\'\n__device__ double rk_standard_exponential(rk_state *state) {\n    /* We use -log(1-U) since U is [0, 1) */\n    return -log(1.0 - rk_double(state));\n}\n\'\'\'\n\nrk_standard_gamma_definition = \'\'\'\n__device__ double rk_standard_gamma(rk_state *state, double shape) {\n    double b, c;\n    double U, V, X, Y;\n    if (shape == 1.0) {\n        return rk_standard_exponential(state);\n    } else if (shape < 1.0) {\n        for (;;) {\n            U = rk_double(state);\n            V = rk_standard_exponential(state);\n            if (U <= 1.0 - shape) {\n                X = pow(U, 1./shape);\n                if (X <= V) {\n                    return X;\n                }\n            } else {\n                Y = -log((1-U)/shape);\n                X = pow(1.0 - shape + shape*Y, 1./shape);\n                if (X <= (V + Y)) {\n                    return X;\n                }\n            }\n        }\n    } else {\n        b = shape - 1./3.;\n        c = 1./sqrt(9*b);\n        for (;;) {\n            do {\n                X = rk_gauss(state);\n                V = 1.0 + c*X;\n            } while (V <= 0.0);\n            V = V*V*V;\n            U = rk_double(state);\n            if (U < 1.0 - 0.0331*(X*X)*(X*X)) return (b*V);\n            if (log(U) < 0.5*X*X + b*(1. - V + log(V))) return (b*V);\n        }\n    }\n}\n\'\'\'\n\nrk_beta_definition = \'\'\'\n__device__ double rk_beta(rk_state *state, double a, double b) {\n    double Ga, Gb;\n    if ((a <= 1.0) && (b <= 1.0)) {\n        double U, V, X, Y;\n        /* Use Johnk\'s algorithm */\n        while (1) {\n            U = rk_double(state);\n            V = rk_double(state);\n            X = pow(U, 1.0/a);\n            Y = pow(V, 1.0/b);\n            if ((X + Y) <= 1.0) {\n                if (X +Y > 0) {\n                    return X / (X + Y);\n                } else {\n                    double logX = log(U) / a;\n                    double logY = log(V) / b;\n                    double logM = logX > logY ? logX : logY;\n                    logX -= logM;\n                    logY -= logM;\n                    return exp(logX - log(exp(logX) + exp(logY)));\n                }\n            }\n        }\n    } else {\n        Ga = rk_standard_gamma(state, a);\n        Gb = rk_standard_gamma(state, b);\n        return Ga/(Ga + Gb);\n    }\n}\n\'\'\'\n\nrk_chisquare_definition = \'\'\'\n__device__ double rk_chisquare(rk_state *state, double df) {\n    return 2.0*rk_standard_gamma(state, df/2.0);\n}\n\'\'\'\n\nrk_noncentral_chisquare_definition = \'\'\'\n__device__ double rk_noncentral_chisquare(\n    rk_state *state, double df, double nonc)\n{\n    if (nonc == 0){\n        return rk_chisquare(state, df);\n    }\n    if(1 < df)\n    {\n        const double Chi2 = rk_chisquare(state, df - 1);\n        const double N = rk_gauss(state) + sqrt(nonc);\n        return Chi2 + N*N;\n    }\n    else\n    {\n        const long i = rk_poisson(state, nonc / 2.0);\n        return rk_chisquare(state, df + 2 * i);\n    }\n}\n\'\'\'\n\nrk_f_definition = \'\'\'\n__device__ double rk_f(rk_state *state, double dfnum, double dfden) {\n    return ((rk_chisquare(state, dfnum) * dfden) /\n            (rk_chisquare(state, dfden) * dfnum));\n}\n\'\'\'\n\nrk_noncentral_f_definition = \'\'\'\n__device__ double rk_noncentral_f(\n    rk_state *state, double dfnum, double dfden, double nonc)\n{\n    double t = rk_noncentral_chisquare(state, dfnum, nonc) * dfden;\n    return t / (rk_chisquare(state, dfden) * dfnum);\n}\n\'\'\'\n\nrk_binomial_definition = \'\'\'\n__device__ long rk_binomial_btpe(rk_state *state, long n, double p) {\n    double r,q,fm,p1,xm,xl,xr,c,laml,lamr,p2,p3,p4;\n    double a,u,v,s,F,rho,t,A,nrq,x1,x2,f1,f2,z,z2,w,w2,x;\n    int m,y,k,i;\n    if (!(state->has_binomial) ||\n         (state->nsave != n) ||\n         (state->psave != p)) {\n        /* initialize */\n        state->nsave = n;\n        state->psave = p;\n        state->has_binomial = 1;\n        state->r = r = min(p, 1.0-p);\n        state->q = q = 1.0 - r;\n        state->fm = fm = n*r+r;\n        state->m = m = (long)floor(state->fm);\n        state->p1 = p1 = floor(2.195*sqrt(n*r*q)-4.6*q) + 0.5;\n        state->xm = xm = m + 0.5;\n        state->xl = xl = xm - p1;\n        state->xr = xr = xm + p1;\n        state->c = c = 0.134 + 20.5/(15.3 + m);\n        a = (fm - xl)/(fm-xl*r);\n        state->laml = laml = a*(1.0 + a/2.0);\n        a = (xr - fm)/(xr*q);\n        state->lamr = lamr = a*(1.0 + a/2.0);\n        state->p2 = p2 = p1*(1.0 + 2.0*c);\n        state->p3 = p3 = p2 + c/laml;\n        state->p4 = p4 = p3 + c/lamr;\n    } else {\n        r = state->r;\n        q = state->q;\n        fm = state->fm;\n        m = state->m;\n        p1 = state->p1;\n        xm = state->xm;\n        xl = state->xl;\n        xr = state->xr;\n        c = state->c;\n        laml = state->laml;\n        lamr = state->lamr;\n        p2 = state->p2;\n        p3 = state->p3;\n        p4 = state->p4;\n    }\n  /* sigh ... */\n  Step10:\n    nrq = n*r*q;\n    u = rk_double(state)*p4;\n    v = rk_double(state);\n    if (u > p1) goto Step20;\n    y = (long)floor(xm - p1*v + u);\n    goto Step60;\n  Step20:\n    if (u > p2) goto Step30;\n    x = xl + (u - p1)/c;\n    v = v*c + 1.0 - fabs(m - x + 0.5)/p1;\n    if (v > 1.0) goto Step10;\n    y = (long)floor(x);\n    goto Step50;\n  Step30:\n    if (u > p3) goto Step40;\n    y = (long)floor(xl + log(v)/laml);\n    if (y < 0) goto Step10;\n    v = v*(u-p2)*laml;\n    goto Step50;\n  Step40:\n    y = (long)floor(xr - log(v)/lamr);\n    if (y > n) goto Step10;\n    v = v*(u-p3)*lamr;\n  Step50:\n    k = labs(y - m);\n    if ((k > 20) && (k < ((nrq)/2.0 - 1))) goto Step52;\n    s = r/q;\n    a = s*(n+1);\n    F = 1.0;\n    if (m < y) {\n        for (i=m+1; i<=y; i++) {\n            F *= (a/i - s);\n        }\n    } else if (m > y) {\n        for (i=y+1; i<=m; i++) {\n            F /= (a/i - s);\n        }\n    }\n    if (v > F) goto Step10;\n    goto Step60;\n    Step52:\n    rho = (k/(nrq))*((k*(k/3.0 + 0.625) + 0.16666666666666666)/nrq + 0.5);\n    t = -k*k/(2*nrq);\n    A = log(v);\n    if (A < (t - rho)) goto Step60;\n    if (A > (t + rho)) goto Step10;\n    x1 = y+1;\n    f1 = m+1;\n    z = n+1-m;\n    w = n-y+1;\n    x2 = x1*x1;\n    f2 = f1*f1;\n    z2 = z*z;\n    w2 = w*w;\n    if (A > (xm*log(f1/x1)\n           + (n-m+0.5)*log(z/w)\n           + (y-m)*log(w*r/(x1*q))\n           + (13680.-(462.-(132.-(99.-140./f2)/f2)/f2)/f2)/f1/166320.\n           + (13680.-(462.-(132.-(99.-140./z2)/z2)/z2)/z2)/z/166320.\n           + (13680.-(462.-(132.-(99.-140./x2)/x2)/x2)/x2)/x1/166320.\n           + (13680.-(462.-(132.-(99.-140./w2)/w2)/w2)/w2)/w/166320.)) {\n        goto Step10;\n    }\n  Step60:\n    if (p > 0.5) {\n        y = n - y;\n    }\n    return y;\n}\n\n__device__ long rk_binomial_inversion(rk_state *state, int n, double p) {\n    double q, qn, np, px, U;\n    int X, bound;\n    if (!(state->has_binomial) ||\n         (state->nsave != n) ||\n         (state->psave != p)) {\n        state->nsave = n;\n        state->psave = p;\n        state->has_binomial = 1;\n        state->q = q = 1.0 - p;\n        state->r = qn = exp(n * log(q));\n        state->c = np = n*p;\n        state->m = bound = min((double)n, np + 10.0*sqrt(np*q + 1));\n    } else {\n        q = state->q;\n        qn = state->r;\n        np = state->c;\n        bound = state->m;\n    }\n    X = 0;\n    px = qn;\n    U = rk_double(state);\n    while (U > px) {\n        X++;\n        if (X > bound) {\n            X = 0;\n            px = qn;\n            U = rk_double(state);\n        } else {\n            U -= px;\n            px  = ((n-X+1) * p * px)/(X*q);\n        }\n    }\n    return X;\n}\n\n__device__ long rk_binomial(rk_state *state, int n, double p) {\n    double q;\n    if (p <= 0.5) {\n        if (p*n <= 30.0) {\n            return rk_binomial_inversion(state, n, p);\n        } else {\n            return rk_binomial_btpe(state, n, p);\n        }\n    } else {\n        q = 1.0-p;\n        if (q*n <= 30.0) {\n            return n - rk_binomial_inversion(state, n, q);\n        } else {\n            return n - rk_binomial_btpe(state, n, q);\n        }\n    }\n}\n\'\'\'\n\nrk_poisson_mult_definition = \'\'\'\n__device__ long rk_poisson_mult(rk_state *state, double lam) {\n    long X;\n    double prod, U, enlam;\n    enlam = exp(-lam);\n    X = 0;\n    prod = 1.0;\n    while (1) {\n        U = rk_double(state);\n        prod *= U;\n        if (prod > enlam) {\n            X += 1;\n        } else {\n            return X;\n        }\n    }\n}\n\'\'\'\n\nrk_poisson_ptrs_definition = \'\'\'\n/*\n * The transformed rejection method for generating Poisson random variables\n * W. Hoermann\n * Insurance: Mathematics and Economics 12, 39-45 (1993)\n */\n#define LS2PI 0.91893853320467267\n#define TWELFTH 0.083333333333333333333333\n__device__ long rk_poisson_ptrs(rk_state *state, double lam) {\n    long k;\n    double U, V, slam, loglam, a, b, invalpha, vr, us;\n    slam = sqrt(lam);\n    loglam = log(lam);\n    b = 0.931 + 2.53*slam;\n    a = -0.059 + 0.02483*b;\n    invalpha = 1.1239 + 1.1328/(b-3.4);\n    vr = 0.9277 - 3.6224/(b-2);\n    while (1) {\n        U = rk_double(state) - 0.5;\n        V = rk_double(state);\n        us = 0.5 - fabs(U);\n        k = (long)floor((2*a/us + b)*U + lam + 0.43);\n        if ((us >= 0.07) && (V <= vr)) {\n            return k;\n        }\n        if ((k < 0) ||\n            ((us < 0.013) && (V > us))) {\n            continue;\n        }\n        if ((log(V) + log(invalpha) - log(a/(us*us)+b)) <=\n            (-lam + k*loglam - loggam(k+1))) {\n            return k;\n        }\n    }\n}\n\'\'\'\n\nrk_poisson_definition = \'\'\'\n__device__ long rk_poisson(rk_state *state, double lam) {\n    if (lam >= 10) {\n        return rk_poisson_ptrs(state, lam);\n    } else if (lam == 0) {\n        return 0;\n    } else {\n        return rk_poisson_mult(state, lam);\n    }\n}\n\'\'\'\n\nrk_standard_t_definition = \'\'\'\n__device__ double rk_standard_t(rk_state *state, double df) {\n    return sqrt(df/2)*rk_gauss(state)/sqrt(rk_standard_gamma(state, df/2));\n}\n\'\'\'\n\nrk_vonmises_definition = \'\'\'\n__device__ double rk_vonmises(rk_state *state, double mu, double kappa)\n{\n    double s;\n    double U, V, W, Y, Z;\n    double result, mod;\n    int neg;\n\n    if (kappa < 1e-8)\n    {\n        return M_PI * (2*rk_double(state)-1);\n    }\n    else\n    {\n        /* with double precision rho is zero until 1.4e-8 */\n        if (kappa < 1e-5) {\n            /*\n             * second order taylor expansion around kappa = 0\n             * precise until relatively large kappas as second order is 0\n             */\n            s = (1./kappa + kappa);\n        }\n        else {\n            double r = 1 + sqrt(1 + 4*kappa*kappa);\n            double rho = (r - sqrt(2*r)) / (2*kappa);\n            s = (1 + rho*rho)/(2*rho);\n        }\n\n        while (1)\n        {\n        U = rk_double(state);\n            Z = cos(M_PI*U);\n            W = (1 + s*Z)/(s + Z);\n            Y = kappa * (s - W);\n            V = rk_double(state);\n            if ((Y*(2-Y) - V >= 0) || (log(Y/V)+1 - Y >= 0))\n            {\n                break;\n            }\n        }\n\n        U = rk_double(state);\n\n        result = acos(W);\n        if (U < 0.5)\n        {\n        result = -result;\n        }\n        result += mu;\n        neg = (result < 0);\n        mod = fabs(result);\n        mod = (fmod(mod+M_PI, 2*M_PI)-M_PI);\n        if (neg)\n        {\n            mod *= -1;\n        }\n\n        return mod;\n    }\n}\n\'\'\'\n\nrk_zipf_definition = \'\'\'\n__device__ long rk_zipf(rk_state *state, double a)\n{\n    double am1, b;\n\n    am1 = a - 1.0;\n    b = pow(2.0, am1);\n    while (1) {\n        double T, U, V, X;\n\n        U = 1.0 - rk_double(state);\n        V = rk_double(state);\n        X = floor(pow(U, -1.0/am1));\n\n        if (X < 1.0) {\n            continue;\n        }\n\n        T = pow(1.0 + 1.0/X, am1);\n        if (V*X*(T - 1.0)/(b - 1.0) <= T/b) {\n            return (long)X;\n        }\n    }\n}\n\'\'\'\n\nrk_geometric_definition = \'\'\'\n__device__ long rk_geometric_search(rk_state *state, double p) {\n    double U;\n    long X;\n    double sum, prod, q;\n    X = 1;\n    sum = prod = p;\n    q = 1.0 - p;\n    U = rk_double(state);\n    while (U > sum) {\n        prod *= q;\n        sum += prod;\n        X++;\n    }\n    return X;\n}\n\n__device__ long rk_geometric_inversion(rk_state *state, double p) {\n    return (long)ceil(log(1.0-rk_double(state))/log(1.0-p));\n}\n\n__device__ long rk_geometric(rk_state *state, double p) {\n    if (p >= 0.333333333333333333333333) {\n        return rk_geometric_search(state, p);\n    } else {\n        return rk_geometric_inversion(state, p);\n    }\n}\n\'\'\'\n\n# min and max for the long type are not defined in cuda90 but in cuda75.\nlong_min_max_definition = \'\'\'\n__device__ long long_min(long a, long b)\n{\n    return a < b ? a : b;\n}\n\n__device__ long long_max(long a, long b)\n{\n    return a > b ? a : b;\n}\n\'\'\'\n\nrk_hypergeometric_definition = \'\'\'\n__device__ long rk_hypergeometric_hyp(\n    rk_state *state, long good, long bad, long sample)\n{\n    long d1, K, Z;\n    double d2, U, Y;\n\n    d1 = bad + good - sample;\n    d2 = (double)long_min(bad, good);\n\n    Y = d2;\n    K = sample;\n    while (Y > 0.0)\n    {\n        U = rk_double(state);\n        Y -= (long)floor(U + Y/(d1 + K));\n        K--;\n        if (K == 0) break;\n    }\n    Z = (long)(d2 - Y);\n    if (good > bad) Z = sample - Z;\n    return Z;\n}\n\n/* D1 = 2*sqrt(2/e) */\n/* D2 = 3 - 2*sqrt(3/e) */\n#define D1 1.7155277699214135\n#define D2 0.8989161620588988\n__device__ long rk_hypergeometric_hrua(\n    rk_state *state, long good, long bad, long sample)\n{\n    long mingoodbad, maxgoodbad, popsize, m, d9;\n    double d4, d5, d6, d7, d8, d10, d11;\n    long Z;\n    double T, W, X, Y;\n\n    mingoodbad = long_min(good, bad);\n    popsize = good + bad;\n    maxgoodbad = long_max(good, bad);\n    m = long_min(sample, popsize - sample);\n    d4 = ((double)mingoodbad) / popsize;\n    d5 = 1.0 - d4;\n    d6 = m*d4 + 0.5;\n    d7 = sqrt((double)(popsize - m) * sample * d4 * d5 / (popsize - 1) + 0.5);\n    d8 = D1*d7 + D2;\n    d9 = (long)floor((double)(m + 1) * (mingoodbad + 1) / (popsize + 2));\n    d10 = (loggam(d9+1) + loggam(mingoodbad-d9+1) + loggam(m-d9+1) +\n           loggam(maxgoodbad-m+d9+1));\n    d11 = min(long_min(m, mingoodbad)+1.0, floor(d6+16*d7));\n    /* 16 for 16-decimal-digit precision in D1 and D2 */\n\n    while (1)\n    {\n        X = rk_double(state);\n        Y = rk_double(state);\n        W = d6 + d8*(Y- 0.5)/X;\n\n        /* fast rejection: */\n        if ((W < 0.0) || (W >= d11)) continue;\n\n        Z = (long)floor(W);\n        T = d10 - (loggam(Z+1) + loggam(mingoodbad-Z+1) + loggam(m-Z+1) +\n                   loggam(maxgoodbad-m+Z+1));\n\n        /* fast acceptance: */\n        if ((X*(4.0-X)-3.0) <= T) break;\n\n        /* fast rejection: */\n        if (X*(X-T) >= 1) continue;\n\n        if (2.0*log(X) <= T) break;  /* acceptance */\n    }\n\n    /* this is a correction to HRUA* by Ivan Frohne in rv.py */\n    if (good > bad) Z = m - Z;\n\n    /* another fix from rv.py to allow sample to exceed popsize/2 */\n    if (m < sample) Z = good - Z;\n\n    return Z;\n}\n#undef D1\n#undef D2\n\n__device__ long rk_hypergeometric(\n    rk_state *state, long good, long bad, long sample)\n{\n    if (sample > 10)\n    {\n        return rk_hypergeometric_hrua(state, good, bad, sample);\n    } else\n    {\n        return rk_hypergeometric_hyp(state, good, bad, sample);\n    }\n}\n\'\'\'\n\nrk_logseries_definition = \'\'\'\n__device__ long rk_logseries(rk_state *state, double p)\n{\n    double q, r, U, V;\n    long result;\n\n    r = log(1.0 - p);\n\n    while (1) {\n        V = rk_double(state);\n        if (V >= p) {\n            return 1;\n        }\n        U = rk_double(state);\n        q = 1.0 - exp(r*U);\n        if (V <= q*q) {\n            result = (long)floor(1 + log(V)/log(q));\n            if (result < 1) {\n                continue;\n            }\n            else {\n                return result;\n            }\n        }\n        if (V >= q) {\n            return 1;\n        }\n        return 2;\n    }\n}\n\'\'\'\n\nrk_gauss_definition = \'\'\'\n__device__ double rk_gauss(rk_state *state) {\n    if (state->has_gauss) {\n        const double tmp = state->gauss;\n        state->gauss = 0;\n        state->has_gauss = 0;\n        return tmp;\n    } else {\n        double f, x1, x2, r2;\n        do {\n            x1 = 2.0*rk_double(state) - 1.0;\n            x2 = 2.0*rk_double(state) - 1.0;\n            r2 = x1*x1 + x2*x2;\n        }\n        while (r2 >= 1.0 || r2 == 0.0);\n        /* Box-Muller transform */\n        f = sqrt(-2.0*log(r2)/r2);\n        /* Keep for next call */\n        state->gauss = f*x1;\n        state->has_gauss = 1;\n        return f*x2;\n    }\n}\n\'\'\'\n\nopen_uniform_definition = \'\'\'\n__device__ void open_uniform(rk_state *state, double *U) {\n    do {\n        *U = rk_double(state);\n    } while (*U <= 0.0 || *U >= 1.0);\n}\n\n__device__ void open_uniform(rk_state *state, float *U) {\n    do {\n        *U = rk_double(state);\n    } while (*U <= 0.0 || *U >= 1.0);\n}\n\'\'\'\n\ndefinitions = [\n    rk_basic_definition, rk_gauss_definition,\n    rk_standard_exponential_definition, rk_standard_gamma_definition,\n    rk_beta_definition]\nbeta_kernel = core.ElementwiseKernel(\n    \'S a, T b, uint64 seed\', \'Y y\',\n    \'\'\'\n    rk_seed(seed + i, &internal_state);\n    y = rk_beta(&internal_state, a, b);\n    \'\'\',\n    \'beta_kernel\',\n    preamble=\'\'.join(definitions),\n    loop_prep=\'rk_state internal_state;\'\n)\n\ndefinitions = [\n    rk_use_binominal, rk_basic_definition, rk_binomial_definition]\nbinomial_kernel = core.ElementwiseKernel(\n    \'S n, T p, uint64 seed\', \'Y y\',\n    \'\'\'\n    rk_seed(seed + i, &internal_state);\n    y = rk_binomial(&internal_state, n, p);\n    \'\'\',\n    \'binomial_kernel\',\n    preamble=\'\'.join(definitions),\n    loop_prep=\'rk_state internal_state;\'\n)\n\ndefinitions = \\\n    [rk_basic_definition, rk_gauss_definition,\n     rk_standard_exponential_definition, rk_standard_gamma_definition,\n     rk_standard_t_definition]\nstandard_t_kernel = core.ElementwiseKernel(\n    \'S df, uint64 seed\', \'Y y\',\n    \'\'\'\n    rk_seed(seed + i, &internal_state);\n    y = rk_standard_t(&internal_state, df);\n    \'\'\',\n    \'standard_t_kernel\',\n    preamble=\'\'.join(definitions),\n    loop_prep=\'rk_state internal_state;\'\n)\n\ndefinitions = \\\n    [rk_basic_definition, rk_gauss_definition,\n     rk_standard_exponential_definition, rk_standard_gamma_definition,\n     rk_chisquare_definition]\nchisquare_kernel = core.ElementwiseKernel(\n    \'T df, uint64 seed\', \'Y y\',\n    \'\'\'\n    rk_seed(seed + i, &internal_state);\n    y = rk_chisquare(&internal_state, df);\n    \'\'\',\n    \'chisquare_kernel\',\n    preamble=\'\'.join(definitions),\n    loop_prep=\'rk_state internal_state;\'\n)\n\ndefinitions = \\\n    [rk_basic_definition, rk_gauss_definition,\n     rk_standard_exponential_definition, rk_standard_gamma_definition,\n     rk_chisquare_definition, rk_f_definition]\nf_kernel = core.ElementwiseKernel(\n    \'S dfnum, T dfden, uint64 seed\', \'Y y\',\n    \'\'\'\n    rk_seed(seed + i, &internal_state);\n    y = rk_f(&internal_state, dfnum, dfden);\n    \'\'\',\n    \'f_kernel\',\n    preamble=\'\'.join(definitions),\n    loop_prep=\'rk_state internal_state;\'\n)\n\ndefinitions = \\\n    [rk_basic_definition, rk_geometric_definition]\ngeometric_kernel = core.ElementwiseKernel(\n    \'T p, uint64 seed\', \'Y y\',\n    \'\'\'\n    rk_seed(seed + i, &internal_state);\n    y = rk_geometric(&internal_state, p);\n    \'\'\',\n    \'geometric_kernel\',\n    preamble=\'\'.join(definitions),\n    loop_prep=\'rk_state internal_state;\'\n)\n\ndefinitions = \\\n    [rk_basic_definition, loggam_definition, long_min_max_definition,\n     rk_hypergeometric_definition]\nhypergeometric_kernel = core.ElementwiseKernel(\n    \'S good, T bad, U sample, uint64 seed\', \'Y y\',\n    \'\'\'\n    rk_seed(seed + i, &internal_state);\n    y = rk_hypergeometric(&internal_state, good, bad, sample);\n    \'\'\',\n    \'hypergeometric_kernel\',\n    preamble=\'\'.join(definitions),\n    loop_prep=\'rk_state internal_state;\'\n)\n\ndefinitions = \\\n    [rk_basic_definition, rk_logseries_definition]\nlogseries_kernel = core.ElementwiseKernel(\n    \'T p, uint64 seed\', \'Y y\',\n    \'\'\'\n    rk_seed(seed + i, &internal_state);\n    y = rk_logseries(&internal_state, p);\n    \'\'\',\n    \'logseries_kernel\',\n    preamble=\'\'.join(definitions),\n    loop_prep=\'rk_state internal_state;\'\n)\n\ndefinitions = [\n    rk_basic_definition, loggam_definition, rk_gauss_definition,\n    rk_standard_exponential_definition, rk_standard_gamma_definition,\n    rk_chisquare_definition, rk_poisson_mult_definition,\n    rk_poisson_ptrs_definition, rk_poisson_definition,\n    rk_noncentral_chisquare_definition]\nnoncentral_chisquare_kernel = core.ElementwiseKernel(\n    \'S df, T nonc, uint64 seed\', \'Y y\',\n    \'\'\'\n    rk_seed(seed + i, &internal_state);\n    y = rk_noncentral_chisquare(&internal_state, df, nonc);\n    \'\'\',\n    \'noncentral_chisquare_kernel\',\n    preamble=\'\'.join(definitions),\n    loop_prep=\'rk_state internal_state;\'\n)\n\ndefinitions = [\n    rk_basic_definition, loggam_definition, rk_gauss_definition,\n    rk_standard_exponential_definition, rk_standard_gamma_definition,\n    rk_chisquare_definition, rk_poisson_mult_definition,\n    rk_poisson_ptrs_definition, rk_poisson_definition,\n    rk_noncentral_chisquare_definition, rk_noncentral_f_definition]\nnoncentral_f_kernel = core.ElementwiseKernel(\n    \'S dfnum, T dfden, U nonc, uint64 seed\', \'Y y\',\n    \'\'\'\n    rk_seed(seed + i, &internal_state);\n    y = rk_noncentral_f(&internal_state, dfnum, dfden, nonc);\n    \'\'\',\n    \'noncentral_f_kernel\',\n    preamble=\'\'.join(definitions),\n    loop_prep=\'rk_state internal_state;\'\n)\n\ndefinitions = \\\n    [rk_basic_definition, loggam_definition,\n     rk_poisson_mult_definition, rk_poisson_ptrs_definition,\n     rk_poisson_definition]\npoisson_kernel = core.ElementwiseKernel(\n    \'T lam, uint64 seed\', \'Y y\',\n    \'\'\'\n    rk_seed(seed + i, &internal_state);\n    y = rk_poisson(&internal_state, lam);\n    \'\'\',\n    \'poisson_kernel\',\n    preamble=\'\'.join(definitions),\n    loop_prep=\'rk_state internal_state;\'\n)\n\ndefinitions = [\n    rk_basic_definition, rk_gauss_definition,\n    rk_standard_exponential_definition, rk_standard_gamma_definition]\nstandard_gamma_kernel = core.ElementwiseKernel(\n    \'T shape, uint64 seed\', \'Y y\',\n    \'\'\'\n    rk_seed(seed + i, &internal_state);\n    y = rk_standard_gamma(&internal_state, shape);\n    \'\'\',\n    \'standard_gamma_kernel\',\n    preamble=\'\'.join(definitions),\n    loop_prep=\'rk_state internal_state;\'\n)\n\ndefinitions = [\n    rk_basic_definition, rk_vonmises_definition]\nvonmises_kernel = core.ElementwiseKernel(\n    \'S mu, T kappa, uint64 seed\', \'Y y\',\n    \'\'\'\n    rk_seed(seed + i, &internal_state);\n    y = rk_vonmises(&internal_state, mu, kappa);\n    \'\'\',\n    \'vonmises_kernel\',\n    preamble=\'\'.join(definitions),\n    loop_prep=\'rk_state internal_state;\'\n)\n\ndefinitions = [\n    rk_basic_definition, rk_zipf_definition]\nzipf_kernel = core.ElementwiseKernel(\n    \'T a, uint64 seed\', \'Y y\',\n    \'\'\'\n    rk_seed(seed + i, &internal_state);\n    y = rk_zipf(&internal_state, a);\n    \'\'\',\n    \'zipf_kernel\',\n    preamble=\'\'.join(definitions),\n    loop_prep=\'rk_state internal_state;\'\n)\n\ndefinitions = [\n    rk_basic_definition, open_uniform_definition]\nopen_uniform_kernel = core.ElementwiseKernel(\n    \'uint64 seed\', \'Y y\',\n    \'\'\'\n    rk_seed(seed + i, &internal_state);\n    open_uniform(&internal_state, &y);\n    \'\'\',\n    \'open_uniform_kernel\',\n    preamble=\'\'.join(definitions),\n    loop_prep=\'rk_state internal_state;\'\n)\n'"
cupy/random/distributions.py,0,"b'import cupy\nfrom cupy.random import generator\nfrom cupy import util\n\n\n# TODO(beam2d): Implement many distributions\n\n\ndef beta(a, b, size=None, dtype=float):\n    """"""Beta distribution.\n\n    Returns an array of samples drawn from the beta distribution. Its\n    probability density function is defined as\n\n    .. math::\n       f(x) = \\\\frac{x^{\\\\alpha-1}(1-x)^{\\\\beta-1}}{B(\\\\alpha,\\\\beta)}.\n\n    Args:\n        a (float): Parameter of the beta distribution :math:`\\\\alpha`.\n        b (float): Parameter of the beta distribution :math:`\\\\beta`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the beta distribution.\n\n    .. seealso::\n        :meth:`numpy.random.beta\n        <numpy.random.mtrand.RandomState.beta>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.beta(a, b, size, dtype)\n\n\ndef binomial(n, p, size=None, dtype=int):\n    """"""Binomial distribution.\n\n    Returns an array of samples drawn from the binomial distribution. Its\n    probability mass function is defined as\n\n    .. math::\n        f(x) = \\\\binom{n}{x}p^x(1-p)^{n-x}.\n\n    Args:\n        n (int): Trial number of the binomial distribution.\n        p (float): Success probability of the binomial distribution.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.int32` and\n            :class:`numpy.int64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the binomial distribution.\n\n    .. seealso::\n        :meth:`numpy.random.binomial\n        <numpy.random.mtrand.RandomState.binomial>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.binomial(n, p, size, dtype)\n\n\ndef chisquare(df, size=None, dtype=float):\n    """"""Chi-square distribution.\n\n    Returns an array of samples drawn from the chi-square distribution. Its\n    probability density function is defined as\n\n    .. math::\n       f(x) = \\\\frac{(1/2)^{k/2}}{\\\\Gamma(k/2)}x^{k/2-1}e^{-x/2}.\n\n    Args:\n        df (int or array_like of ints): Degree of freedom :math:`k`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the chi-square distribution.\n\n    .. seealso::\n        :meth:`numpy.random.chisquare\n        <numpy.random.mtrand.RandomState.chisquare>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.chisquare(df, size, dtype)\n\n\ndef dirichlet(alpha, size=None, dtype=float):\n    """"""Dirichlet distribution.\n\n    Returns an array of samples drawn from the dirichlet distribution. Its\n    probability density function is defined as\n\n    .. math::\n        f(x) = \\\\frac{\\\\Gamma(\\\\sum_{i=1}^K\\\\alpha_i)} \\\n            {\\\\prod_{i=1}^{K}\\\\Gamma(\\\\alpha_i)} \\\n            \\\\prod_{i=1}^Kx_i^{\\\\alpha_i-1}.\n\n    Args:\n        alpha (array): Parameters of the dirichlet distribution\n            :math:`\\\\alpha`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the dirichlet distribution.\n\n    .. seealso::\n        :meth:`numpy.random.dirichlet\n        <numpy.random.mtrand.RandomState.dirichlet>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.dirichlet(alpha, size, dtype)\n\n\ndef exponential(scale, size=None, dtype=float):\n    """"""Exponential distribution.\n\n    Returns an array of samples drawn from the exponential distribution. Its\n    probability density function is defined as\n\n    .. math::\n       f(x) = \\\\frac{1}{\\\\beta}\\\\exp (-\\\\frac{x}{\\\\beta}).\n\n    Args:\n        scale (float or array_like of floats): The scale parameter\n            :math:`\\\\beta`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the exponential distribution.\n\n    .. seealso::\n        :meth:`numpy.random.exponential\n        <numpy.random.mtrand.RandomState.exponential>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.exponential(scale, size, dtype)\n\n\ndef f(dfnum, dfden, size=None, dtype=float):\n    """"""F distribution.\n\n    Returns an array of samples drawn from the f distribution. Its probability\n    density function is defined as\n\n    .. math::\n        f(x) = \\\\frac{1}{B(\\\\frac{d_1}{2},\\\\frac{d_2}{2})} \\\n            \\\\left(\\\\frac{d_1}{d_2}\\\\right)^{\\\\frac{d_1}{2}} \\\n            x^{\\\\frac{d_1}{2}-1} \\\n            \\\\left(1+\\\\frac{d_1}{d_2}x\\\\right) \\\n            ^{-\\\\frac{d_1+d_2}{2}}.\n\n    Args:\n        dfnum (float or array_like of floats): Parameter of the f distribution\n            :math:`d_1`.\n        dfden (float or array_like of floats): Parameter of the f distribution\n            :math:`d_2`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the f distribution.\n\n    .. seealso::\n        :meth:`numpy.random.f\n        <numpy.random.mtrand.RandomState.f>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.f(dfnum, dfden, size, dtype)\n\n\ndef gamma(shape, scale=1.0, size=None, dtype=float):\n    """"""Gamma distribution.\n\n    Returns an array of samples drawn from the gamma distribution. Its\n    probability density function is defined as\n\n    .. math::\n       f(x) = \\\\frac{1}{\\\\Gamma(k)\\\\theta^k}x^{k-1}e^{-x/\\\\theta}.\n\n    Args:\n        shape (array): Parameter of the gamma distribution :math:`k`.\n        scale (array): Parameter of the gamma distribution :math:`\\\\theta`\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:cupy.ndarray: Samples drawn from the gamma distribution.\n\n    .. seealso::\n        :meth:`numpy.random.gamma\n        <numpy.random.mtrand.RandomState.gamma>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.gamma(shape, scale, size, dtype)\n\n\ndef geometric(p, size=None, dtype=int):\n    """"""Geometric distribution.\n\n    Returns an array of samples drawn from the geometric distribution. Its\n    probability mass function is defined as\n\n    .. math::\n        f(x) = p(1-p)^{k-1}.\n\n    Args:\n        p (float): Success probability of the geometric distribution.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.int32` and\n            :class:`numpy.int64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the geometric distribution.\n\n    .. seealso::\n        :func:`cupy.random.RandomState.geometric`\n        :meth:`numpy.random.geometric\n        <numpy.random.mtrand.RandomState.geometric>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.geometric(p, size, dtype)\n\n\ndef gumbel(loc=0.0, scale=1.0, size=None, dtype=float):\n    """"""Returns an array of samples drawn from a Gumbel distribution.\n\n    The samples are drawn from a Gumbel distribution with location ``loc``\n    and scale ``scale``.\n    Its probability density function is defined as\n\n    .. math::\n       f(x) = \\\\frac{1}{\\\\eta} \\\n           \\\\exp\\\\left\\\\{ - \\\\frac{x - \\\\mu}{\\\\eta} \\\\right\\\\} \\\n           \\\\exp\\\\left[-\\\\exp\\\\left\\\\{-\\\\frac{x - \\\\mu}{\\\\eta} \\\n           \\\\right\\\\}\\\\right],\n\n    where :math:`\\\\mu` is ``loc`` and :math:`\\\\eta` is ``scale``.\n\n    Args:\n        loc (float): The location of the mode :math:`\\\\mu`.\n        scale (float): The scale parameter :math:`\\\\eta`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the Gumbel distribution.\n\n    .. seealso::\n        :meth:`numpy.random.gumbel\n        <numpy.random.mtrand.RandomState.gumbel>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.gumbel(loc, scale, size, dtype)\n\n\ndef hypergeometric(ngood, nbad, nsample, size=None, dtype=int):\n    """"""hypergeometric distribution.\n\n    Returns an array of samples drawn from the hypergeometric distribution. Its\n    probability mass function is defined as\n\n    .. math::\n        f(x) = \\\\frac{\\\\binom{m}{n}\\\\binom{N-m}{n-x}}{\\\\binom{N}{n}}.\n\n    Args:\n        ngood (int or array_like of ints): Parameter of the hypergeometric\n            distribution :math:`n`.\n        nbad (int or array_like of ints): Parameter of the hypergeometric\n            distribution :math:`m`.\n        nsample (int or array_like of ints): Parameter of the hypergeometric\n            distribution :math:`N`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.int32` and\n            :class:`numpy.int64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the hypergeometric distribution.\n\n    .. seealso::\n        :meth:`numpy.random.hypergeometric\n        <numpy.random.mtrand.RandomState.hypergeometric>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.hypergeometric(ngood, nbad, nsample, size, dtype)\n\n\ndef logistic(loc=0.0, scale=1.0, size=None, dtype=float):\n    """"""Logistic distribution.\n\n    Returns an array of samples drawn from the logistic distribution. Its\n    probability density function is defined as\n\n    .. math::\n       f(x) = \\\\frac{e^{-(x-\\\\mu)/s}}{s(1+e^{-(x-\\\\mu)/s})^2}.\n\n    Args:\n        loc (float): The location of the mode :math:`\\\\mu`.\n        scale (float): The scale parameter :math:`s`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the logistic distribution.\n\n    .. seealso::\n        :meth:`numpy.random.logistic\n        <numpy.random.mtrand.RandomState.logistic>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.logistic(loc, scale, size, dtype)\n\n\ndef laplace(loc=0.0, scale=1.0, size=None, dtype=float):\n    """"""Laplace distribution.\n\n    Returns an array of samples drawn from the laplace distribution. Its\n    probability density function is defined as\n\n    .. math::\n       f(x) = \\\\frac{1}{2b}\\\\exp\\\\left(-\\\\frac{|x-\\\\mu|}{b}\\\\right).\n\n    Args:\n        loc (float): The location of the mode :math:`\\\\mu`.\n        scale (float): The scale parameter :math:`b`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the laplace distribution.\n\n    .. seealso::\n        :meth:`numpy.random.laplace\n        <numpy.random.mtrand.RandomState.laplace>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.laplace(loc, scale, size, dtype)\n\n\ndef lognormal(mean=0.0, sigma=1.0, size=None, dtype=float):\n    """"""Returns an array of samples drawn from a log normal distribution.\n\n    The samples are natural log of samples drawn from a normal distribution\n    with mean ``mean`` and deviation ``sigma``.\n\n    Args:\n        mean (float): Mean of the normal distribution.\n        sigma (float): Standard deviation of the normal distribution.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the log normal distribution.\n\n    .. seealso:: :meth:`numpy.random.lognormal\n                 <numpy.random.mtrand.RandomState.lognormal>`\n\n    """"""\n    rs = generator.get_random_state()\n    return rs.lognormal(mean, sigma, size=size, dtype=dtype)\n\n\ndef logseries(p, size=None, dtype=int):\n    """"""Log series distribution.\n\n    Returns an array of samples drawn from the log series distribution. Its\n    probability mass function is defined as\n\n    .. math::\n       f(x) = \\\\frac{-p^x}{x\\\\ln(1-p)}.\n\n    Args:\n        p (float): Parameter of the log series distribution :math:`p`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.int32` and\n            :class:`numpy.int64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the log series distribution.\n\n    .. seealso:: :meth:`numpy.random.logseries\n                 <numpy.random.mtrand.RandomState.logseries>`\n\n    """"""\n    rs = generator.get_random_state()\n    return rs.logseries(p, size=size, dtype=dtype)\n\n\ndef negative_binomial(n, p, size=None, dtype=int):\n    """"""Negative binomial distribution.\n\n    Returns an array of samples drawn from the negative binomial distribution.\n    Its probability mass function is defined as\n\n    .. math::\n        f(x) = \\\\binom{x + n - 1}{n - 1}p^n(1-p)^{x}.\n\n    Args:\n        n (int): Parameter of the negative binomial distribution :math:`n`.\n        p (float): Parameter of the negative binomial distribution :math:`p`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.int32` and\n            :class:`numpy.int64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the negative binomial distribution.\n\n    .. seealso::\n        :meth:`numpy.random.negative_binomial\n        <numpy.random.mtrand.RandomState.negative_binomial>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.negative_binomial(n, p, size=size, dtype=dtype)\n\n\ndef multivariate_normal(mean, cov, size=None, check_valid=\'ignore\',\n                        tol=1e-08, method=\'cholesky\', dtype=float):\n    """"""Multivariate normal distribution.\n\n    Returns an array of samples drawn from the multivariate normal\n    distribution. Its probability density function is defined as\n\n    .. math::\n       f(x) = \\\\frac{1}{(2\\\\pi|\\\\Sigma|)^(n/2)} \\\n           \\\\exp\\\\left(-\\\\frac{1}{2} \\\n           (x-\\\\mu)^{\\\\top}\\\\Sigma^{-1}(x-\\\\mu)\\\\right).\n\n    Args:\n        mean (1-D array_like, of length N): Mean of the multivariate normal\n            distribution :math:`\\\\mu`.\n        cov (2-D array_like, of shape (N, N)): Covariance matrix\n            :math:`\\\\Sigma` of the multivariate normal distribution. It must be\n            symmetric and positive-semidefinite for proper sampling.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        check_valid (\'warn\', \'raise\', \'ignore\'): Behavior when the covariance\n            matrix is not positive semidefinite.\n        tol (float): Tolerance when checking the singular values in\n            covariance matrix.\n        method : { \'cholesky\', \'eigh\', \'svd\'}, optional\n            The cov input is used to compute a factor matrix A such that\n            ``A @ A.T = cov``. This argument is used to select the method\n            used to compute the factor matrix A. The default method \'cholesky\'\n            is the fastest, while \'svd\' is the slowest but more robust than\n            the fastest method. The method `eigh` uses eigen decomposition to\n            compute A and is faster than svd but slower than cholesky.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the multivariate normal distribution.\n\n    .. note:: Default `method` is set to fastest, \'cholesky\', unlike numpy\n        which defaults to \'svd\'. Cholesky decomposition in CuPy will fail\n        silently if the input covariance matrix is not positive definite and\n        give invalid results, unlike in numpy, where an invalid covariance\n        matrix will raise an exception. Setting `check_valid` to \'raise\' will\n        replicate numpy behavior by checking the input, but will also force\n        device synchronization. If validity of input is unknown, setting\n        `method` to \'einh\' or \'svd\' and `check_valid` to \'warn\' will use\n        cholesky decomposition for positive definite matrices, and fallback to\n        the specified `method` for other matrices (i.e., not positive\n        semi-definite), and will warn if decomposition is suspect.\n\n    .. seealso:: :meth:`numpy.random.multivariate_normal\n                 <numpy.random.mtrand.RandomState.multivariate_normal>`\n\n    """"""\n    util.experimental(\'cupy.random.multivariate_normal\')\n    rs = generator.get_random_state()\n    x = rs.multivariate_normal(mean, cov, size, check_valid, tol, method,\n                               dtype)\n    return x\n\n\ndef normal(loc=0.0, scale=1.0, size=None, dtype=float):\n    """"""Returns an array of normally distributed samples.\n\n    Args:\n        loc (float or array_like of floats): Mean of the normal distribution.\n        scale (float or array_like of floats):\n            Standard deviation of the normal distribution.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Normally distributed samples.\n\n    .. seealso:: :meth:`numpy.random.normal\n                 <numpy.random.mtrand.RandomState.normal>`\n\n    """"""\n    rs = generator.get_random_state()\n    x = rs.normal(0, 1, size, dtype)\n    cupy.multiply(x, scale, out=x)\n    cupy.add(x, loc, out=x)\n    return x\n\n\ndef pareto(a, size=None, dtype=float):\n    """"""Pareto II or Lomax distribution.\n\n    Returns an array of samples drawn from the Pareto II distribution. Its\n    probability density function is defined as\n\n    .. math::\n        f(x) = \\\\alpha(1+x)^{-(\\\\alpha+1)}.\n\n    Args:\n        a (float or array_like of floats): Parameter of the Pareto II\n            distribution :math:`\\\\alpha`.\n        size (int or tuple of ints): The shape of the array. If ``None``, this\n            function generate an array whose shape is `a.shape`.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the Pareto II distribution.\n\n    .. seealso:: :meth:`numpy.random.pareto\n                 <numpy.random.mtrand.RandomState.pareto>`\n    """"""\n    rs = generator.get_random_state()\n    x = rs.pareto(a, size, dtype)\n    return x\n\n\ndef noncentral_chisquare(df, nonc, size=None, dtype=float):\n    """"""Noncentral chisquare distribution.\n\n    Returns an array of samples drawn from the noncentral chisquare\n    distribution. Its probability density function is defined as\n\n    .. math::\n       f(x) = \\\\frac{1}{2}e^{-(x+\\\\lambda)/2} \\\\\n        \\\\left(\\\\frac{x}{\\\\lambda}\\\\right)^{k/4 - 1/2} \\\\\n        I_{k/2 - 1}(\\\\sqrt{\\\\lambda x}),\n\n    where :math:`I` is the modified Bessel function of the first kind.\n\n    Args:\n        df (float): Parameter of the noncentral chisquare distribution\n            :math:`k`.\n        nonc (float): Parameter of the noncentral chisquare distribution\n            :math:`\\\\lambda`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the noncentral chisquare distribution.\n\n    .. seealso::\n        :meth:`numpy.random.noncentral_chisquare\n        <numpy.random.mtrand.RandomState.noncentral_chisquare>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.noncentral_chisquare(df, nonc, size=size, dtype=dtype)\n\n\ndef noncentral_f(dfnum, dfden, nonc, size=None, dtype=float):\n    """"""Noncentral F distribution.\n\n    Returns an array of samples drawn from the noncentral F\n    distribution.\n\n    Reference: https://en.wikipedia.org/wiki/Noncentral_F-distribution\n\n    Args:\n        dfnum (float): Parameter of the noncentral F distribution.\n        dfden (float): Parameter of the noncentral F distribution.\n        nonc (float): Parameter of the noncentral F distribution.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the noncentral F distribution.\n\n    .. seealso::\n        :meth:`numpy.random.noncentral_f\n        <numpy.random.mtrand.RandomState.noncentral_f>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.noncentral_f(dfnum, dfden, nonc, size=size, dtype=dtype)\n\n\ndef poisson(lam=1.0, size=None, dtype=int):\n    """"""Poisson distribution.\n\n    Returns an array of samples drawn from the poisson distribution. Its\n    probability mass function is defined as\n\n    .. math::\n        f(x) = \\\\frac{\\\\lambda^xe^{-\\\\lambda}}{k!}.\n\n    Args:\n        lam (array_like of floats): Parameter of the poisson distribution\n            :math:`\\\\lambda`.\n        size (int or tuple of ints): The shape of the array. If ``None``, this\n            function generate an array whose shape is `lam.shape`.\n        dtype: Data type specifier. Only :class:`numpy.int32` and\n            :class:`numpy.int64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the poisson distribution.\n\n    .. seealso:: :meth:`numpy.random.poisson\n                 <numpy.random.mtrand.RandomState.poisson>`\n    """"""\n    rs = generator.get_random_state()\n    x = rs.poisson(lam, size, dtype)\n    return x\n\n\ndef power(a, size=None, dtype=float):\n    """"""Power distribution.\n\n    Returns an array of samples drawn from the power distribution. Its\n    probability density function is defined as\n\n    .. math::\n       f(x) = ax^{a-1}.\n\n    Args:\n        a (float): Parameter of the power distribution :math:`a`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the power distribution.\n\n    .. seealso::\n        :meth:`numpy.random.power\n        <numpy.random.mtrand.RandomState.power>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.power(a, size, dtype)\n\n\ndef rayleigh(scale=1.0, size=None, dtype=float):\n    """"""Rayleigh distribution.\n\n    Returns an array of samples drawn from the rayleigh distribution.\n    Its probability density function is defined as\n\n      .. math::\n         f(x) = \\\\frac{x}{\\\\sigma^2}e^{\\\\frac{-x^2}{2-\\\\sigma^2}}, x \\\\ge 0.\n\n    Args:\n        scale (array): Parameter of the rayleigh distribution :math:`\\\\sigma`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the rayleigh distribution.\n\n    .. seealso:: :meth:`numpy.random.rayleigh\n                 <numpy.random.mtrand.RandomState.rayleigh>`\n    """"""\n    rs = generator.get_random_state()\n    x = rs.rayleigh(scale, size, dtype)\n    return x\n\n\ndef standard_cauchy(size=None, dtype=float):\n    """"""Standard cauchy distribution.\n\n    Returns an array of samples drawn from the standard cauchy distribution.\n    Its probability density function is defined as\n\n      .. math::\n         f(x) = \\\\frac{1}{\\\\pi(1+x^2)}.\n\n    Args:\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the standard cauchy distribution.\n\n    .. seealso:: :meth:`numpy.random.standard_cauchy\n                 <numpy.random.mtrand.RandomState.standard_cauchy>`\n    """"""\n    rs = generator.get_random_state()\n    x = rs.standard_cauchy(size, dtype)\n    return x\n\n\ndef standard_exponential(size=None, dtype=float):\n    """"""Standard exponential distribution.\n\n    Returns an array of samples drawn from the standard exponential\n    distribution. Its probability density function is defined as\n\n      .. math::\n         f(x) = e^{-x}.\n\n    Args:\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the standard exponential distribution.\n\n    .. seealso:: :meth:`numpy.random.standard_exponential\n                 <numpy.random.mtrand.RandomState.standard_exponential>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.standard_exponential(size, dtype)\n\n\ndef standard_gamma(shape, size=None, dtype=float):\n    """"""Standard gamma distribution.\n\n    Returns an array of samples drawn from the standard gamma distribution. Its\n    probability density function is defined as\n\n    .. math::\n       f(x) = \\\\frac{1}{\\\\Gamma(k)}x^{k-1}e^{-x}.\n\n    Args:\n        shape (array): Parameter of the gamma distribution :math:`k`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the standard gamma distribution.\n\n    .. seealso::\n        :meth:`numpy.random.standard_gamma\n        <numpy.random.mtrand.RandomState.standard_gamma>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.standard_gamma(shape, size, dtype)\n\n\ndef standard_normal(size=None, dtype=float):\n    """"""Returns an array of samples drawn from the standard normal distribution.\n\n    This is a variant of :func:`cupy.random.randn`.\n\n    Args:\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the standard normal distribution.\n\n    .. seealso:: :meth:`numpy.random.standard_normal\n                 <numpy.random.mtrand.RandomState.standard_normal>`\n\n    """"""\n    return normal(size=size, dtype=dtype)\n\n\ndef standard_t(df, size=None, dtype=float):\n    """"""Standard Student\'s t distribution.\n\n    Returns an array of samples drawn from the standard Student\'s t\n    distribution. Its probability density function is defined as\n\n    .. math::\n        f(x) = \\\\frac{\\\\Gamma(\\\\frac{\\\\nu+1}{2})} \\\n            {\\\\sqrt{\\\\nu\\\\pi}\\\\Gamma(\\\\frac{\\\\nu}{2})} \\\n            \\\\left(1 + \\\\frac{x^2}{\\\\nu} \\\\right)^{-(\\\\frac{\\\\nu+1}{2})}.\n\n    Args:\n        df (float or array_like of floats): Degree of freedom :math:`\\\\nu`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the standard Student\'s t distribution.\n\n    .. seealso::\n        :meth:`numpy.random.standard_t\n        <numpy.random.mtrand.RandomState.standard_t>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.standard_t(df, size, dtype)\n\n\ndef triangular(left, mode, right, size=None, dtype=float):\n    """"""Triangular distribution.\n\n    Returns an array of samples drawn from the triangular distribution. Its\n    probability density function is defined as\n\n    .. math::\n       f(x) = \\\\begin{cases}\n            \\\\frac{2(x-l)}{(r-l)(m-l)} & \\\\text{for } l \\\\leq x \\\\leq m, \\\\\\\\\n            \\\\frac{2(r-x)}{(r-l)(r-m)} & \\\\text{for } m \\\\leq x \\\\leq r, \\\\\\\\\n            0 & \\\\text{otherwise}.\n          \\\\end{cases}\n\n    Args:\n        left (float): Lower limit :math:`l`.\n        mode (float): The value where the peak of the distribution occurs.\n            :math:`m`.\n        right (float): Higher Limit :math:`r`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the triangular distribution.\n\n    .. seealso::\n        :func:`cupy.random.RandomState.triangular`\n        :meth:`numpy.random.triangular\n        <numpy.random.mtrand.RandomState.triangular>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.triangular(left, mode, right, size, dtype)\n\n\ndef uniform(low=0.0, high=1.0, size=None, dtype=float):\n    """"""Returns an array of uniformly-distributed samples over an interval.\n\n    Samples are drawn from a uniform distribution over the half-open interval\n    ``[low, high)``.\n\n    Args:\n        low (float): Lower end of the interval.\n        high (float): Upper end of the interval.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the uniform distribution.\n\n    .. seealso:: :meth:`numpy.random.uniform\n                 <numpy.random.mtrand.RandomState.uniform>`\n\n    """"""\n    rs = generator.get_random_state()\n    return rs.uniform(low, high, size=size, dtype=dtype)\n\n\ndef vonmises(mu, kappa, size=None, dtype=float):\n    """"""von Mises distribution.\n\n    Returns an array of samples drawn from the von Mises distribution. Its\n    probability density function is defined as\n\n    .. math::\n       f(x) = \\\\frac{e^{\\\\kappa \\\\cos(x-\\\\mu)}}{2\\\\pi I_0(\\\\kappa)}.\n\n    Args:\n        mu (float): Parameter of the von Mises distribution :math:`\\\\mu`.\n        kappa (float): Parameter of the von Mises distribution :math:`\\\\kappa`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the von Mises distribution.\n\n    .. seealso::\n        :meth:`numpy.random.vonmises\n        <numpy.random.mtrand.RandomState.vonmises>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.vonmises(mu, kappa, size=size, dtype=dtype)\n\n\ndef wald(mean, scale, size=None, dtype=float):\n    """"""Wald distribution.\n\n    Returns an array of samples drawn from the Wald distribution. Its\n    probability density function is defined as\n\n    .. math::\n       f(x) = \\\\sqrt{\\\\frac{\\\\lambda}{2\\\\pi x^3}}\\\\\n           e^{\\\\frac{-\\\\lambda(x-\\\\mu)^2}{2\\\\mu^2x}}.\n\n    Args:\n        mean (float): Parameter of the wald distribution :math:`\\\\mu`.\n        scale (float): Parameter of the wald distribution :math:`\\\\lambda`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the wald distribution.\n\n    .. seealso::\n        :func:`cupy.random.RandomState.wald`\n        :meth:`numpy.random.wald\n        <numpy.random.mtrand.RandomState.wald>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.wald(mean, scale, size, dtype)\n\n\ndef weibull(a, size=None, dtype=float):\n    """"""weibull distribution.\n\n    Returns an array of samples drawn from the weibull distribution. Its\n    probability density function is defined as\n\n    .. math::\n       f(x) = ax^{(a-1)}e^{-x^a}.\n\n    Args:\n        a (float): Parameter of the weibull distribution :math:`a`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the weibull distribution.\n\n    .. seealso::\n        :meth:`numpy.random.weibull\n        <numpy.random.mtrand.RandomState.weibull>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.weibull(a, size=size, dtype=dtype)\n\n\ndef zipf(a, size=None, dtype=int):\n    """"""Zipf distribution.\n\n    Returns an array of samples drawn from the Zipf distribution. Its\n    probability mass function is defined as\n\n    .. math::\n        f(x) = \\\\frac{x^{-a}}{ \\\\zeta (a)},\n\n    where :math:`\\\\zeta` is the Riemann Zeta function.\n\n    Args:\n        a (float): Parameter of the beta distribution :math:`a`.\n        size (int or tuple of ints): The shape of the array. If ``None``, a\n            zero-dimensional array is generated.\n        dtype: Data type specifier. Only :class:`numpy.int32` and\n            :class:`numpy.int64` types are allowed.\n\n    Returns:\n        cupy.ndarray: Samples drawn from the Zipf distribution.\n\n    .. seealso::\n        :meth:`numpy.random.zipf\n        <numpy.random.mtrand.RandomState.zipf>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.zipf(a, size=size, dtype=dtype)\n'"
cupy/random/generator.py,0,"b'import atexit\nimport binascii\nimport functools\nimport hashlib\nimport operator\nimport os\nimport time\n\nimport numpy\nimport warnings\nfrom numpy.linalg import LinAlgError\n\nimport cupy\nfrom cupy import core\nfrom cupy import cuda\nfrom cupy.cuda import curand\nfrom cupy.cuda import device\nfrom cupy.random import _kernels\nfrom cupy import util\n\nimport cupyx\n\n\n_UINT32_MAX = 0xffffffff\n_UINT64_MAX = 0xffffffffffffffff\n\n\nclass RandomState(object):\n\n    """"""Portable container of a pseudo-random number generator.\n\n    An instance of this class holds the state of a random number generator. The\n    state is available only on the device which has been current at the\n    initialization of the instance.\n\n    Functions of :mod:`cupy.random` use global instances of this class.\n    Different instances are used for different devices. The global state for\n    the current device can be obtained by the\n    :func:`cupy.random.get_random_state` function.\n\n    Args:\n        seed (None or int): Seed of the random number generator. See the\n            :meth:`~cupy.random.RandomState.seed` method for detail.\n        method (int): Method of the random number generator. Following values\n            are available::\n\n               cupy.cuda.curand.CURAND_RNG_PSEUDO_DEFAULT\n               cupy.cuda.curand.CURAND_RNG_XORWOW\n               cupy.cuda.curand.CURAND_RNG_MRG32K3A\n               cupy.cuda.curand.CURAND_RNG_MTGP32\n               cupy.cuda.curand.CURAND_RNG_MT19937\n               cupy.cuda.curand.CURAND_RNG_PHILOX4_32_10\n\n    """"""\n\n    def __init__(self, seed=None, method=curand.CURAND_RNG_PSEUDO_DEFAULT):\n        self._generator = curand.createGenerator(method)\n        self.seed(seed)\n\n    def __del__(self, is_shutting_down=util.is_shutting_down):\n        # When createGenerator raises an error, _generator is not initialized\n        if is_shutting_down():\n            return\n        if hasattr(self, \'_generator\'):\n            curand.destroyGenerator(self._generator)\n\n    def _update_seed(self, size):\n        self._rk_seed = (self._rk_seed + size) % _UINT64_MAX\n\n    def _generate_normal(self, func, size, dtype, *args):\n        # curand functions below don\'t support odd size.\n        # * curand.generateNormal\n        # * curand.generateNormalDouble\n        # * curand.generateLogNormal\n        # * curand.generateLogNormalDouble\n        size = core.get_size(size)\n        element_size = functools.reduce(operator.mul, size, 1)\n        if element_size % 2 == 0:\n            out = cupy.empty(size, dtype=dtype)\n            func(self._generator, out.data.ptr, out.size, *args)\n            return out\n        else:\n            out = cupy.empty((element_size + 1,), dtype=dtype)\n            func(self._generator, out.data.ptr, out.size, *args)\n            return out[:element_size].reshape(size)\n\n    # NumPy compatible functions\n\n    def beta(self, a, b, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the beta distribution.\n\n        .. seealso::\n            :func:`cupy.random.beta` for full documentation,\n            :meth:`numpy.random.RandomState.beta\n            <numpy.random.mtrand.RandomState.beta>`\n        """"""\n        a, b = cupy.asarray(a), cupy.asarray(b)\n        if size is None:\n            size = cupy.broadcast(a, b).shape\n        y = cupy.empty(shape=size, dtype=dtype)\n        _kernels.beta_kernel(a, b, self._rk_seed, y)\n        self._update_seed(y.size)\n        return y\n\n    def binomial(self, n, p, size=None, dtype=int):\n        """"""Returns an array of samples drawn from the binomial distribution.\n\n        .. seealso::\n            :func:`cupy.random.binomial` for full documentation,\n            :meth:`numpy.random.RandomState.binomial\n            <numpy.random.mtrand.RandomState.binomial>`\n        """"""\n        n, p = cupy.asarray(n), cupy.asarray(p)\n        if size is None:\n            size = cupy.broadcast(n, p).shape\n        y = cupy.empty(shape=size, dtype=dtype)\n        _kernels.binomial_kernel(n, p, self._rk_seed, y)\n        self._update_seed(y.size)\n        return y\n\n    def chisquare(self, df, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the chi-square distribution.\n\n        .. seealso::\n            :func:`cupy.random.chisquare` for full documentation,\n            :meth:`numpy.random.RandomState.chisquare\n            <numpy.random.mtrand.RandomState.chisquare>`\n        """"""\n        df = cupy.asarray(df)\n        if size is None:\n            size = df.shape\n        y = cupy.empty(shape=size, dtype=dtype)\n        _kernels.chisquare_kernel(df, self._rk_seed, y)\n        self._update_seed(y.size)\n        return y\n\n    def dirichlet(self, alpha, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the dirichlet distribution.\n\n        .. seealso::\n            :func:`cupy.random.dirichlet` for full documentation,\n            :meth:`numpy.random.RandomState.dirichlet\n            <numpy.random.mtrand.RandomState.dirichlet>`\n        """"""\n        alpha = cupy.asarray(alpha)\n        if size is None:\n            size = alpha.shape\n        else:\n            size += alpha.shape\n        y = cupy.empty(shape=size, dtype=dtype)\n        _kernels.standard_gamma_kernel(alpha, self._rk_seed, y)\n        y /= y.sum(axis=-1, keepdims=True)\n        self._update_seed(y.size)\n        return y\n\n    def exponential(self, scale=1.0, size=None, dtype=float):\n        """"""Returns an array of samples drawn from a exponential distribution.\n\n        .. warning::\n\n            This function may synchronize the device.\n\n        .. seealso::\n            :func:`cupy.random.exponential` for full documentation,\n            :meth:`numpy.random.RandomState.exponential\n            <numpy.random.mtrand.RandomState.exponential>`\n        """"""\n        scale = cupy.asarray(scale, dtype)\n        if (scale < 0).any():  # synchronize!\n            raise ValueError(\'scale < 0\')\n        if size is None:\n            size = scale.shape\n        x = self.standard_exponential(size, dtype)\n        x *= scale\n        return x\n\n    def f(self, dfnum, dfden, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the f distribution.\n\n        .. seealso::\n            :func:`cupy.random.f` for full documentation,\n            :meth:`numpy.random.RandomState.f\n            <numpy.random.mtrand.RandomState.f>`\n        """"""\n        dfnum, dfden = cupy.asarray(dfnum), cupy.asarray(dfden)\n        if size is None:\n            size = cupy.broadcast(dfnum, dfden).shape\n        y = cupy.empty(shape=size, dtype=dtype)\n        _kernels.f_kernel(dfnum, dfden, self._rk_seed, y)\n        self._update_seed(y.size)\n        return y\n\n    def gamma(self, shape, scale=1.0, size=None, dtype=float):\n        """"""Returns an array of samples drawn from a gamma distribution.\n\n        .. seealso::\n            :func:`cupy.random.gamma` for full documentation,\n            :meth:`numpy.random.RandomState.gamma\n            <numpy.random.mtrand.RandomState.gamma>`\n        """"""\n        shape, scale = cupy.asarray(shape), cupy.asarray(scale)\n        if size is None:\n            size = cupy.broadcast(shape, scale).shape\n        y = cupy.empty(shape=size, dtype=dtype)\n        _kernels.standard_gamma_kernel(shape, self._rk_seed, y)\n        y *= scale\n        self._update_seed(y.size)\n        return y\n\n    def geometric(self, p, size=None, dtype=int):\n        """"""Returns an array of samples drawn from the geometric distribution.\n\n        .. seealso::\n            :func:`cupy.random.geometric` for full documentation,\n            :meth:`numpy.random.RandomState.geometric\n            <numpy.random.mtrand.RandomState.geometric>`\n        """"""\n        p = cupy.asarray(p)\n        if size is None:\n            size = p.shape\n        y = cupy.empty(shape=size, dtype=dtype)\n        _kernels.geometric_kernel(p, self._rk_seed, y)\n        self._update_seed(y.size)\n        return y\n\n    def hypergeometric(self, ngood, nbad, nsample, size=None, dtype=int):\n        """"""Returns an array of samples drawn from the hypergeometric distribution.\n\n        .. seealso::\n            :func:`cupy.random.hypergeometric` for full documentation,\n            :meth:`numpy.random.RandomState.hypergeometric\n            <numpy.random.mtrand.RandomState.hypergeometric>`\n        """"""\n        ngood, nbad, nsample = \\\n            cupy.asarray(ngood), cupy.asarray(nbad), cupy.asarray(nsample)\n        if size is None:\n            size = cupy.broadcast(ngood, nbad, nsample).shape\n        y = cupy.empty(shape=size, dtype=dtype)\n        _kernels.hypergeometric_kernel(ngood, nbad, nsample, self._rk_seed, y)\n        self._update_seed(y.size)\n        return y\n\n    _laplace_kernel = core.ElementwiseKernel(\n        \'T x, T loc, T scale\', \'T y\',\n        \'y = loc + scale * ((x <= 0.5) ? log(x + x): -log(x + x - 1.0))\',\n        \'laplace_kernel\')\n\n    def laplace(self, loc=0.0, scale=1.0, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the laplace distribution.\n\n        .. seealso::\n            :func:`cupy.random.laplace` for full documentation,\n            :meth:`numpy.random.RandomState.laplace\n            <numpy.random.mtrand.RandomState.laplace>`\n        """"""\n        loc = cupy.asarray(loc, dtype)\n        scale = cupy.asarray(scale, dtype)\n        if size is None:\n            size = cupy.broadcast(loc, scale).shape\n        x = self._random_sample_raw(size, dtype)\n        RandomState._laplace_kernel(x, loc, scale, x)\n        return x\n\n    def logistic(self, loc=0.0, scale=1.0, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the logistic distribution.\n\n        .. seealso::\n            :func:`cupy.random.logistic` for full documentation,\n            :meth:`numpy.random.RandomState.logistic\n            <numpy.random.mtrand.RandomState.logistic>`\n        """"""\n        loc, scale = cupy.asarray(loc), cupy.asarray(scale)\n        if size is None:\n            size = cupy.broadcast(loc, scale).shape\n        x = cupy.empty(shape=size, dtype=dtype)\n        _kernels.open_uniform_kernel(self._rk_seed, x)\n        self._update_seed(x.size)\n        x = (1.0 - x) / x\n        cupy.log(x, out=x)\n        cupy.multiply(x, scale, out=x)\n        cupy.add(x, loc, out=x)\n        return x\n\n    def lognormal(self, mean=0.0, sigma=1.0, size=None, dtype=float):\n        """"""Returns an array of samples drawn from a log normal distribution.\n\n        .. seealso::\n            :func:`cupy.random.lognormal` for full documentation,\n            :meth:`numpy.random.RandomState.lognormal\n            <numpy.random.mtrand.RandomState.lognormal>`\n\n        """"""\n        dtype = _check_and_get_dtype(dtype)\n        if dtype.char == \'f\':\n            func = curand.generateLogNormal\n        else:\n            func = curand.generateLogNormalDouble\n        return self._generate_normal(func, size, dtype, mean, sigma)\n\n    def logseries(self, p, size=None, dtype=int):\n        """"""Returns an array of samples drawn from a log series distribution.\n\n        .. warning::\n\n            This function may synchronize the device.\n\n        .. seealso::\n            :func:`cupy.random.logseries` for full documentation,\n            :meth:`numpy.random.RandomState.logseries\n            <numpy.random.mtrand.RandomState.logseries>`\n\n        """"""\n        p = cupy.asarray(p)\n        if cupy.any(p <= 0):  # synchronize!\n            raise ValueError(\'p <= 0.0\')\n        if cupy.any(p >= 1):  # synchronize!\n            raise ValueError(\'p >= 1.0\')\n        if size is None:\n            size = p.shape\n        y = cupy.empty(shape=size, dtype=dtype)\n        _kernels.logseries_kernel(p, self._rk_seed, y)\n        self._update_seed(y.size)\n        return y\n\n    def multivariate_normal(self, mean, cov, size=None, check_valid=\'ignore\',\n                            tol=1e-08, method=\'cholesky\', dtype=float):\n        """"""Returns an array of samples drawn from the multivariate normal\n        distribution.\n\n        .. warning::\n            This function calls one or more cuSOLVER routine(s) which may yield\n            invalid results if input conditions are not met.\n            To detect these invalid results, you can set the `linalg`\n            configuration to a value that is not `ignore` in\n            :func:`cupyx.errstate` or :func:`cupyx.seterr`.\n\n        .. seealso::\n            :func:`cupy.random.multivariate_normal` for full documentation,\n            :meth:`numpy.random.RandomState.multivariate_normal\n            <numpy.random.mtrand.RandomState.multivariate_normal>`\n        """"""\n        util.experimental(\'cupy.random.RandomState.multivariate_normal\')\n        mean = cupy.asarray(mean, dtype=dtype)\n        cov = cupy.asarray(cov, dtype=dtype)\n        if size is None:\n            shape = []\n        elif isinstance(size, (int, cupy.integer)):\n            shape = [size]\n        else:\n            shape = size\n\n        if len(mean.shape) != 1:\n            raise ValueError(\'mean must be 1 dimensional\')\n        if (len(cov.shape) != 2) or (cov.shape[0] != cov.shape[1]):\n            raise ValueError(\'cov must be 2 dimensional and square\')\n        if mean.shape[0] != cov.shape[0]:\n            raise ValueError(\'mean and cov must have same length\')\n\n        final_shape = list(shape[:])\n        final_shape.append(mean.shape[0])\n\n        if method not in {\'eigh\', \'svd\', \'cholesky\'}:\n            raise ValueError(\n                ""method must be one of {\'eigh\', \'svd\', \'cholesky\'}"")\n\n        if check_valid != \'ignore\':\n            if check_valid != \'warn\' and check_valid != \'raise\':\n                raise ValueError(\n                    ""check_valid must equal \'warn\', \'raise\', or \'ignore\'"")\n\n        if check_valid == \'warn\':\n            with cupyx.errstate(linalg=\'raise\'):\n                try:\n                    decomp = cupy.linalg.cholesky(cov)\n                except LinAlgError:\n                    with cupyx.errstate(linalg=\'ignore\'):\n                        if method != \'cholesky\':\n                            if method == \'eigh\':\n                                (s, u) = cupy.linalg.eigh(cov)\n                                psd = not cupy.any(s < -tol)\n                            if method == \'svd\':\n                                (u, s, vh) = cupy.linalg.svd(cov)\n                                psd = cupy.allclose(cupy.dot(vh.T * s, vh),\n                                                    cov, rtol=tol, atol=tol)\n                            decomp = u * cupy.sqrt(cupy.abs(s))\n                            if not psd:\n                                warnings.warn(""covariance is not positive-"" +\n                                              ""semidefinite, output may be "" +\n                                              ""invalid."", RuntimeWarning)\n\n                        else:\n                            warnings.warn(""covariance is not positive-"" +\n                                          ""semidefinite, output *is* "" +\n                                          ""invalid."", RuntimeWarning)\n                            decomp = cupy.linalg.cholesky(cov)\n\n        else:\n            with cupyx.errstate(linalg=check_valid):\n                try:\n                    if method == \'cholesky\':\n                        decomp = cupy.linalg.cholesky(cov)\n                    elif method == \'eigh\':\n                        (s, u) = cupy.linalg.eigh(cov)\n                        decomp = u * cupy.sqrt(cupy.abs(s))\n                    elif method == \'svd\':\n                        (u, s, vh) = cupy.linalg.svd(cov)\n                        decomp = u * cupy.sqrt(cupy.abs(s))\n\n                except LinAlgError:\n                    raise LinAlgError(""Matrix is not positive definite; if "" +\n                                      ""matrix is positive-semidefinite, set"" +\n                                      ""\'check_valid\' to \'warn\'"")\n\n        x = self.standard_normal(final_shape,\n                                 dtype=dtype).reshape(-1, mean.shape[0])\n        x = cupy.dot(decomp, x.T)\n        x = x.T\n        x += mean\n        x.shape = tuple(final_shape)\n        return x\n\n    def negative_binomial(self, n, p, size=None, dtype=int):\n        """"""Returns an array of samples drawn from the negative binomial distribution.\n\n        .. warning::\n\n            This function may synchronize the device.\n\n        .. seealso::\n            :func:`cupy.random.negative_binomial` for full documentation,\n            :meth:`numpy.random.RandomState.negative_binomial\n            <numpy.random.mtrand.RandomState.negative_binomial>`\n        """"""\n        n = cupy.asarray(n)\n        p = cupy.asarray(p)\n        if cupy.any(n <= 0):  # synchronize!\n            raise ValueError(\'n <= 0\')\n        if cupy.any(p < 0):  # synchronize!\n            raise ValueError(\'p < 0\')\n        if cupy.any(p > 1):  # synchronize!\n            raise ValueError(\'p > 1\')\n        y = self.gamma(n, (1-p)/p, size)\n        return self.poisson(y, dtype=dtype)\n\n    def normal(self, loc=0.0, scale=1.0, size=None, dtype=float):\n        """"""Returns an array of normally distributed samples.\n\n        .. seealso::\n            :func:`cupy.random.normal` for full documentation,\n            :meth:`numpy.random.RandomState.normal\n            <numpy.random.mtrand.RandomState.normal>`\n\n        """"""\n        dtype = _check_and_get_dtype(dtype)\n        if dtype.char == \'f\':\n            func = curand.generateNormal\n        else:\n            func = curand.generateNormalDouble\n        return self._generate_normal(func, size, dtype, loc, scale)\n\n    def pareto(self, a, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the pareto II distribution.\n\n        .. seealso::\n            :func:`cupy.random.pareto_kernel` for full documentation,\n            :meth:`numpy.random.RandomState.pareto\n            <numpy.random.mtrand.RandomState.pareto>`\n        """"""\n        a = cupy.asarray(a)\n        x = self._random_sample_raw(size, dtype)\n        cupy.log(x, out=x)\n        cupy.exp(-x/a, out=x)\n        return x - 1\n\n    def noncentral_chisquare(self, df, nonc, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the noncentral chi-square\n        distribution.\n\n        .. warning::\n\n            This function may synchronize the device.\n\n        .. seealso::\n            :func:`cupy.random.noncentral_chisquare` for full documentation,\n            :meth:`numpy.random.RandomState.noncentral_chisquare\n            <numpy.random.mtrand.RandomState.noncentral_chisquare>`\n        """"""\n        df, nonc = cupy.asarray(df), cupy.asarray(nonc)\n        if cupy.any(df <= 0):  # synchronize!\n            raise ValueError(\'df <= 0\')\n        if cupy.any(nonc < 0):  # synchronize!\n            raise ValueError(\'nonc < 0\')\n        if size is None:\n            size = cupy.broadcast(df, nonc).shape\n        y = cupy.empty(shape=size, dtype=dtype)\n        _kernels.noncentral_chisquare_kernel(df, nonc, self._rk_seed, y)\n        self._update_seed(y.size)\n        return y\n\n    def noncentral_f(self, dfnum, dfden, nonc, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the noncentral F distribution.\n\n        .. warning::\n\n            This function may synchronize the device.\n\n        .. seealso::\n            :func:`cupy.random.noncentral_f` for full documentation,\n            :meth:`numpy.random.RandomState.noncentral_f\n            <numpy.random.mtrand.RandomState.noncentral_f>`\n        """"""\n        dfnum, dfden, nonc = \\\n            cupy.asarray(dfnum), cupy.asarray(dfden), cupy.asarray(nonc)\n        if cupy.any(dfnum <= 0):  # synchronize!\n            raise ValueError(\'dfnum <= 0\')\n        if cupy.any(dfden <= 0):  # synchronize!\n            raise ValueError(\'dfden <= 0\')\n        if cupy.any(nonc < 0):  # synchronize!\n            raise ValueError(\'nonc < 0\')\n        if size is None:\n            size = cupy.broadcast(dfnum, dfden, nonc).shape\n        y = cupy.empty(shape=size, dtype=dtype)\n        _kernels.noncentral_f_kernel(dfnum, dfden, nonc, self._rk_seed, y)\n        self._update_seed(y.size)\n        return y\n\n    def poisson(self, lam=1.0, size=None, dtype=int):\n        """"""Returns an array of samples drawn from the poisson distribution.\n\n        .. seealso::\n            :func:`cupy.random.poisson` for full documentation,\n            :meth:`numpy.random.RandomState.poisson\n            <numpy.random.mtrand.RandomState.poisson>`\n        """"""\n        lam = cupy.asarray(lam)\n        if size is None:\n            size = lam.shape\n        y = cupy.empty(shape=size, dtype=dtype)\n        _kernels.poisson_kernel(lam, self._rk_seed, y)\n        self._update_seed(y.size)\n        return y\n\n    def power(self, a, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the power distribution.\n\n        .. warning::\n\n            This function may synchronize the device.\n\n        .. seealso::\n            :func:`cupy.random.power` for full documentation,\n            :meth:`numpy.random.RandomState.power\n            <numpy.random.mtrand.RandomState.power>`\n        """"""\n        a = cupy.asarray(a)\n        if cupy.any(a < 0):  # synchronize!\n            raise ValueError(\'a < 0\')\n        if size is None:\n            size = a.shape\n        x = self.standard_exponential(size=size, dtype=dtype)\n        cupy.exp(-x, out=x)\n        cupy.add(1, -x, out=x)\n        cupy.power(x, 1./a, out=x)\n        return x\n\n    def rand(self, *size, **kwarg):\n        """"""Returns uniform random values over the interval ``[0, 1)``.\n\n        .. seealso::\n            :func:`cupy.random.rand` for full documentation,\n            :meth:`numpy.random.RandomState.rand\n            <numpy.random.mtrand.RandomState.rand>`\n\n        """"""\n        dtype = kwarg.pop(\'dtype\', float)\n        if kwarg:\n            raise TypeError(\'rand() got unexpected keyword arguments %s\'\n                            % \', \'.join(kwarg.keys()))\n        return self.random_sample(size=size, dtype=dtype)\n\n    def randn(self, *size, **kwarg):\n        """"""Returns an array of standard normal random values.\n\n        .. seealso::\n            :func:`cupy.random.randn` for full documentation,\n            :meth:`numpy.random.RandomState.randn\n            <numpy.random.mtrand.RandomState.randn>`\n\n        """"""\n        dtype = kwarg.pop(\'dtype\', float)\n        if kwarg:\n            raise TypeError(\'randn() got unexpected keyword arguments %s\'\n                            % \', \'.join(kwarg.keys()))\n        return self.normal(size=size, dtype=dtype)\n\n    _mod1_kernel = core.ElementwiseKernel(\n        \'\', \'T x\', \'x = (x == (T)1) ? 0 : x\', \'cupy_random_x_mod_1\')\n\n    def _random_sample_raw(self, size, dtype):\n        dtype = _check_and_get_dtype(dtype)\n        out = cupy.empty(size, dtype=dtype)\n        if dtype.char == \'f\':\n            func = curand.generateUniform\n        else:\n            func = curand.generateUniformDouble\n        func(self._generator, out.data.ptr, out.size)\n        return out\n\n    def random_sample(self, size=None, dtype=float):\n        """"""Returns an array of random values over the interval ``[0, 1)``.\n\n        .. seealso::\n            :func:`cupy.random.random_sample` for full documentation,\n            :meth:`numpy.random.RandomState.random_sample\n            <numpy.random.mtrand.RandomState.random_sample>`\n\n        """"""\n        out = self._random_sample_raw(size, dtype)\n        RandomState._mod1_kernel(out)\n        return out\n\n    def rayleigh(self, scale=1.0, size=None, dtype=float):\n        """"""Returns an array of samples drawn from a rayleigh distribution.\n\n        .. warning::\n\n            This function may synchronize the device.\n\n        .. seealso::\n            :func:`cupy.random.rayleigh` for full documentation,\n            :meth:`numpy.random.RandomState.rayleigh\n            <numpy.random.mtrand.RandomState.rayleigh>`\n        """"""\n        scale = cupy.asarray(scale)\n        if size is None:\n            size = scale.shape\n        if cupy.any(scale < 0):  # synchronize!\n            raise ValueError(\'scale < 0\')\n        x = self._random_sample_raw(size, dtype)\n        x = cupy.log(x, out=x)\n        x = cupy.multiply(x, -2., out=x)\n        x = cupy.sqrt(x, out=x)\n        x = cupy.multiply(x, scale, out=x)\n        return x\n\n    def _interval(self, mx, size):\n        """"""Generate multiple integers independently sampled uniformly from ``[0, mx]``.\n\n        Args:\n            mx (int): Upper bound of the interval\n            size (None or int or tuple): Shape of the array or the scalar\n                returned.\n        Returns:\n            int or cupy.ndarray: If ``None``, an :class:`cupy.ndarray` with\n            shape ``()`` is returned.\n            If ``int``, 1-D array of length size is returned.\n            If ``tuple``, multi-dimensional array with shape\n            ``size`` is returned.\n            Currently, only 32 bit or 64 bit integers can be sampled.\n        """"""  # NOQA\n        if size is None:\n            size = ()\n        elif isinstance(size, int):\n            size = size,\n\n        if mx == 0:\n            return cupy.zeros(size, dtype=numpy.uint32)\n\n        if mx < 0:\n            raise ValueError(\n                \'mx must be non-negative (actual: {})\'.format(mx))\n        elif mx <= _UINT32_MAX:\n            dtype = numpy.uint32\n        elif mx <= _UINT64_MAX:\n            dtype = numpy.uint64\n        else:\n            raise ValueError(\n                \'mx must be within uint64 range (actual: {})\'.format(mx))\n\n        mask = (1 << mx.bit_length()) - 1\n        mask = cupy.array(mask, dtype=dtype)\n\n        n = functools.reduce(operator.mul, size, 1)\n\n        if n == 0:\n            return cupy.empty(size, dtype=dtype)\n\n        sample = cupy.empty((n,), dtype=dtype)\n        size32 = sample.view(dtype=numpy.uint32).size\n        n_rem = n  # The number of remaining elements to sample\n        ret = None\n        while n_rem > 0:\n            # Call 32-bit RNG to fill 32-bit or 64-bit `sample`\n            curand.generate(\n                self._generator, sample.data.ptr, size32)\n            # Drop the samples that exceed the upper limit\n            sample &= mask\n            success = sample <= mx\n\n            if ret is None:\n                # If the sampling has finished in the first iteration,\n                # just return the sample.\n                if success.all():\n                    n_rem = 0\n                    ret = sample\n                    break\n\n                # Allocate the return array.\n                ret = cupy.empty((n,), dtype=dtype)\n\n            n_succ = min(n_rem, int(success.sum()))\n            ret[n - n_rem:n - n_rem + n_succ] = sample[success][:n_succ]\n            n_rem -= n_succ\n\n        assert n_rem == 0\n\n        return ret.reshape(size)\n\n    def seed(self, seed=None):\n        """"""Resets the state of the random number generator with a seed.\n\n        .. seealso::\n            :func:`cupy.random.seed` for full documentation,\n            :meth:`numpy.random.RandomState.seed\n            <numpy.random.mtrand.RandomState.seed>`\n\n        """"""\n        if seed is None:\n            try:\n                seed_str = binascii.hexlify(os.urandom(8))\n                seed = int(seed_str, 16)\n            except NotImplementedError:\n                seed = (time.time() * 1000000) % _UINT64_MAX\n        else:\n            if isinstance(seed, numpy.ndarray):\n                seed = int(hashlib.md5(seed).hexdigest()[:16], 16)\n            else:\n                seed = int(\n                    numpy.asarray(seed).astype(numpy.uint64, casting=\'safe\'))\n\n        curand.setPseudoRandomGeneratorSeed(self._generator, seed)\n        curand.setGeneratorOffset(self._generator, 0)\n\n        self._rk_seed = seed\n\n    def standard_cauchy(self, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the standard cauchy distribution.\n\n        .. seealso::\n            :func:`cupy.random.standard_cauchy` for full documentation,\n            :meth:`numpy.random.RandomState.standard_cauchy\n            <numpy.random.mtrand.RandomState.standard_cauchy>`\n        """"""\n        x = self.uniform(size=size, dtype=dtype)\n        return cupy.tan(cupy.pi * (x - 0.5))\n\n    def standard_exponential(self, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the standard exp distribution.\n\n         .. seealso::\n            :func:`cupy.random.standard_exponential` for full documentation,\n            :meth:`numpy.random.RandomState.standard_exponential\n            <numpy.random.mtrand.RandomState.standard_exponential>`\n        """"""\n        x = self._random_sample_raw(size, dtype)\n        return -cupy.log(x, out=x)\n\n    def standard_gamma(self, shape, size=None, dtype=float):\n        """"""Returns an array of samples drawn from a standard gamma distribution.\n\n        .. seealso::\n            :func:`cupy.random.standard_gamma` for full documentation,\n            :meth:`numpy.random.RandomState.standard_gamma\n            <numpy.random.mtrand.RandomState.standard_gamma>`\n        """"""\n        shape = cupy.asarray(shape)\n        if size is None:\n            size = shape.shape\n        y = cupy.empty(shape=size, dtype=dtype)\n        _kernels.standard_gamma_kernel(shape, self._rk_seed, y)\n        self._update_seed(y.size)\n        return y\n\n    def standard_normal(self, size=None, dtype=float):\n        """"""Returns samples drawn from the standard normal distribution.\n\n        .. seealso::\n            :func:`cupy.random.standard_normal` for full documentation,\n            :meth:`numpy.random.RandomState.standard_normal\n            <numpy.random.mtrand.RandomState.standard_normal>`\n\n        """"""\n        return self.normal(size=size, dtype=dtype)\n\n    def standard_t(self, df, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the standard t distribution.\n\n        .. seealso::\n            :func:`cupy.random.standard_t` for full documentation,\n            :meth:`numpy.random.RandomState.standard_t\n            <numpy.random.mtrand.RandomState.standard_t>`\n        """"""\n        df = cupy.asarray(df)\n        if size is None:\n            size = df.shape\n        y = cupy.empty(shape=size, dtype=dtype)\n        _kernels.standard_t_kernel(df, self._rk_seed, y)\n        self._update_seed(y.size)\n        return y\n\n    def tomaxint(self, size=None):\n        """"""Draws integers between 0 and max integer inclusive.\n\n        Args:\n            size (int or tuple of ints): Output shape.\n\n        Returns:\n            cupy.ndarray: Drawn samples.\n\n        .. seealso::\n            :meth:`numpy.random.RandomState.tomaxint\n            <numpy.random.mtrand.RandomState.tomaxint>`\n\n        """"""\n        if size is None:\n            size = ()\n        sample = cupy.empty(size, dtype=cupy.int_)\n        # cupy.random only uses int32 random generator\n        size_in_int = sample.dtype.itemsize // 4\n        curand.generate(\n            self._generator, sample.data.ptr, sample.size * size_in_int)\n\n        # Disable sign bit\n        sample &= cupy.iinfo(cupy.int_).max\n        return sample\n\n    _triangular_kernel = core.ElementwiseKernel(\n        \'L left, M mode, R right\', \'T x\',\n        """"""\n        T base, leftbase, ratio, leftprod, rightprod;\n\n        base = right - left;\n        leftbase = mode - left;\n        ratio = leftbase / base;\n        leftprod = leftbase*base;\n        rightprod = (right - mode)*base;\n\n        if (x <= ratio)\n        {\n            x = left + sqrt(x*leftprod);\n        } else\n        {\n            x = right - sqrt((1.0 - x) * rightprod);\n        }\n        """""",\n        \'triangular_kernel\'\n    )\n\n    def triangular(self, left, mode, right, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the triangular distribution.\n\n        .. warning::\n\n            This function may synchronize the device.\n\n        .. seealso::\n            :func:`cupy.random.triangular` for full documentation,\n            :meth:`numpy.random.RandomState.triangular\n            <numpy.random.mtrand.RandomState.triangular>`\n        """"""\n        left, mode, right = \\\n            cupy.asarray(left), cupy.asarray(mode), cupy.asarray(right)\n        if cupy.any(left > mode):  # synchronize!\n            raise ValueError(\'left > mode\')\n        if cupy.any(mode > right):  # synchronize!\n            raise ValueError(\'mode > right\')\n        if cupy.any(left == right):  # synchronize!\n            raise ValueError(\'left == right\')\n        if size is None:\n            size = cupy.broadcast(left, mode, right).shape\n        x = self.random_sample(size=size, dtype=dtype)\n        return RandomState._triangular_kernel(left, mode, right, x)\n\n    _scale_kernel = core.ElementwiseKernel(\n        \'T low, T high\', \'T x\',\n        \'x = T(low) + x * T(high - low)\',\n        \'cupy_scale\')\n\n    def uniform(self, low=0.0, high=1.0, size=None, dtype=float):\n        """"""Returns an array of uniformly-distributed samples over an interval.\n\n        .. seealso::\n            :func:`cupy.random.uniform` for full documentation,\n            :meth:`numpy.random.RandomState.uniform\n            <numpy.random.mtrand.RandomState.uniform>`\n\n        """"""\n        dtype = numpy.dtype(dtype)\n        rand = self.random_sample(size=size, dtype=dtype)\n        if not numpy.isscalar(low):\n            low = cupy.asarray(low, dtype)\n        if not numpy.isscalar(high):\n            high = cupy.asarray(high, dtype)\n        return RandomState._scale_kernel(low, high, rand)\n\n    def vonmises(self, mu, kappa, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the von Mises distribution.\n\n        .. seealso::\n            :func:`cupy.random.vonmises` for full documentation,\n            :meth:`numpy.random.RandomState.vonmises\n            <numpy.random.mtrand.RandomState.vonmises>`\n        """"""\n        mu, kappa = cupy.asarray(mu), cupy.asarray(kappa)\n        if size is None:\n            size = cupy.broadcast(mu, kappa).shape\n        y = cupy.empty(shape=size, dtype=dtype)\n        _kernels.vonmises_kernel(mu, kappa, self._rk_seed, y)\n        self._update_seed(y.size)\n        return y\n\n    _wald_kernel = core.ElementwiseKernel(\n        \'T mean, T scale, T U\', \'T X\',\n        """"""\n            T mu_2l;\n            T Y;\n            mu_2l = mean / (2*scale);\n            Y = mean*X*X;\n            X = mean + mu_2l*(Y - sqrt(4*scale*Y + Y*Y));\n            if (U > mean/(mean+X))\n            {\n                X = mean*mean/X;\n            }\n        """""",\n        \'wald_scale\')\n\n    def wald(self, mean, scale, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the Wald distribution.\n\n         .. seealso::\n            :func:`cupy.random.wald` for full documentation,\n            :meth:`numpy.random.RandomState.wald\n            <numpy.random.mtrand.RandomState.wald>`\n        """"""\n        mean, scale = \\\n            cupy.asarray(mean, dtype=dtype), cupy.asarray(scale, dtype=dtype)\n        if size is None:\n            size = cupy.broadcast(mean, scale).shape\n        x = self.normal(size=size, dtype=dtype)\n        u = self.random_sample(size=size, dtype=dtype)\n        return RandomState._wald_kernel(mean, scale, u, x)\n\n    def weibull(self, a, size=None, dtype=float):\n        """"""Returns an array of samples drawn from the weibull distribution.\n\n        .. warning::\n\n            This function may synchronize the device.\n\n        .. seealso::\n            :func:`cupy.random.weibull` for full documentation,\n            :meth:`numpy.random.RandomState.weibull\n            <numpy.random.mtrand.RandomState.weibull>`\n        """"""\n        a = cupy.asarray(a)\n        if cupy.any(a < 0):  # synchronize!\n            raise ValueError(\'a < 0\')\n        x = self.standard_exponential(size, dtype)\n        cupy.power(x, 1./a, out=x)\n        return x\n\n    def zipf(self, a, size=None, dtype=int):\n        """"""Returns an array of samples drawn from the Zipf distribution.\n\n        .. warning::\n\n            This function may synchronize the device.\n\n        .. seealso::\n            :func:`cupy.random.zipf` for full documentation,\n            :meth:`numpy.random.RandomState.zipf\n            <numpy.random.mtrand.RandomState.zipf>`\n        """"""\n        a = cupy.asarray(a)\n        if cupy.any(a <= 1.0):  # synchronize!\n            raise ValueError(\'\\\'a\\\' must be a valid float > 1.0\')\n        if size is None:\n            size = a.shape\n        y = cupy.empty(shape=size, dtype=dtype)\n        _kernels.zipf_kernel(a, self._rk_seed, y)\n        self._update_seed(y.size)\n        return y\n\n    def choice(self, a, size=None, replace=True, p=None):\n        """"""Returns an array of random values from a given 1-D array.\n\n        .. seealso::\n            :func:`cupy.random.choice` for full document,\n            :meth:`numpy.random.choice\n            <numpy.random.mtrand.RandomState.choice>`\n\n        """"""\n        if a is None:\n            raise ValueError(\'a must be 1-dimensional or an integer\')\n        if isinstance(a, cupy.ndarray) and a.ndim == 0:\n            raise NotImplementedError\n        if isinstance(a, int):\n            a_size = a\n            if a_size <= 0:\n                raise ValueError(\'a must be greater than 0\')\n        else:\n            a = cupy.array(a, copy=False)\n            if a.ndim != 1:\n                raise ValueError(\'a must be 1-dimensional or an integer\')\n            else:\n                a_size = len(a)\n                if a_size == 0:\n                    raise ValueError(\'a must be non-empty\')\n\n        if p is not None:\n            p = cupy.array(p)\n            if p.ndim != 1:\n                raise ValueError(\'p must be 1-dimensional\')\n            if len(p) != a_size:\n                raise ValueError(\'a and p must have same size\')\n            if not (p >= 0).all():\n                raise ValueError(\'probabilities are not non-negative\')\n            p_sum = cupy.sum(p).get()\n            if not numpy.allclose(p_sum, 1):\n                raise ValueError(\'probabilities do not sum to 1\')\n\n        if size is None:\n            raise NotImplementedError\n        shape = size\n        size = numpy.prod(shape)\n\n        if not replace and p is None:\n            if a_size < size:\n                raise ValueError(\n                    \'Cannot take a larger sample than population when \'\n                    \'\\\'replace=False\\\'\')\n            if isinstance(a, int):\n                indices = cupy.arange(a, dtype=\'l\')\n            else:\n                indices = a.copy()\n            self.shuffle(indices)\n            return indices[:size].reshape(shape)\n\n        if not replace:\n            raise NotImplementedError\n\n        if p is not None:\n            p = cupy.broadcast_to(p, (size, a_size))\n            index = cupy.argmax(cupy.log(p) +\n                                self.gumbel(size=(size, a_size)),\n                                axis=1)\n            if not isinstance(shape, int):\n                index = cupy.reshape(index, shape)\n        else:\n            index = self.randint(0, a_size, size=shape)\n            # Align the dtype with NumPy\n            index = index.astype(cupy.int64, copy=False)\n\n        if isinstance(a, int):\n            return index\n\n        if index.ndim == 0:\n            return cupy.array(a[index], dtype=a.dtype)\n\n        return a[index]\n\n    def shuffle(self, a):\n        """"""Returns a shuffled array.\n\n        .. seealso::\n            :func:`cupy.random.shuffle` for full document,\n            :meth:`numpy.random.shuffle\n            <numpy.random.mtrand.RandomState.shuffle>`\n\n        """"""\n        if not isinstance(a, cupy.ndarray):\n            raise TypeError(\'The array must be cupy.ndarray\')\n\n        if a.ndim == 0:\n            raise TypeError(\'An array whose ndim is 0 is not supported\')\n\n        a[:] = a[self._permutation(len(a))]\n\n    def permutation(self, a):\n        """"""Returns a permuted range or a permutation of an array.""""""\n        if isinstance(a, int):\n            return self._permutation(a)\n        else:\n            return a[self._permutation(len(a))]\n\n    def _permutation(self, num):\n        """"""Returns a permuted range.""""""\n        sample = cupy.empty((num), dtype=numpy.int32)\n        curand.generate(self._generator, sample.data.ptr, num)\n        if 128 < num <= 32 * 1024 * 1024:\n            array = cupy.arange(num, dtype=numpy.int32)\n            # apply sort of cache blocking\n            block_size = 1 * 1024 * 1024\n            # The block size above is a value determined from the L2 cache size\n            # of GP100 (L2 cache size / size of int = 4MB / 4B = 1M). It may be\n            # better to change the value base on the L2 cache size of the GPU\n            # you use.\n            # When num > block_size, cupy kernel: _cupy_permutation is to be\n            # launched multiple times. However, it is observed that performance\n            # will be degraded if the launch count is too many. Therefore,\n            # the block size is adjusted so that launch count will not exceed\n            # twelve Note that this twelve is the value determined from\n            # measurement on GP100.\n            while num // block_size > 12:\n                block_size *= 2\n            for j_start in range(0, num, block_size):\n                j_end = j_start + block_size\n                _cupy_permutation(sample, j_start, j_end, array, size=num)\n        else:\n            # When num > 32M, argsort is used, because it is faster than\n            # custom kernel. See https://github.com/cupy/cupy/pull/603.\n            array = cupy.argsort(sample)\n        return array\n\n    _gumbel_kernel = core.ElementwiseKernel(\n        \'T x, T loc, T scale\', \'T y\',\n        \'y = T(loc) - log(-log(x)) * T(scale)\',\n        \'gumbel_kernel\')\n\n    def gumbel(self, loc=0.0, scale=1.0, size=None, dtype=float):\n        """"""Returns an array of samples drawn from a Gumbel distribution.\n\n        .. seealso::\n            :func:`cupy.random.gumbel` for full documentation,\n            :meth:`numpy.random.RandomState.gumbel\n            <numpy.random.mtrand.RandomState.gumbel>`\n        """"""\n        x = self._random_sample_raw(size=size, dtype=dtype)\n        if not numpy.isscalar(loc):\n            loc = cupy.asarray(loc, dtype)\n        if not numpy.isscalar(scale):\n            scale = cupy.asarray(scale, dtype)\n        RandomState._gumbel_kernel(x, loc, scale, x)\n        return x\n\n    def randint(self, low, high=None, size=None, dtype=\'l\'):\n        """"""Returns a scalar or an array of integer values over ``[low, high)``.\n\n        .. seealso::\n            :func:`cupy.random.randint` for full documentation,\n            :meth:`numpy.random.RandomState.randint\n            <numpy.random.mtrand.RandomState.randint>`\n        """"""\n        if high is None:\n            lo = 0\n            hi1 = int(low) - 1\n        else:\n            lo = int(low)\n            hi1 = int(high) - 1\n\n        if lo > hi1:\n            raise ValueError(\'low >= high\')\n        if lo < cupy.iinfo(dtype).min:\n            raise ValueError(\n                \'low is out of bounds for {}\'.format(cupy.dtype(dtype).name))\n        if hi1 > cupy.iinfo(dtype).max:\n            raise ValueError(\n                \'high is out of bounds for {}\'.format(cupy.dtype(dtype).name))\n\n        diff = hi1 - lo\n        x = self._interval(diff, size).astype(dtype, copy=False)\n        cupy.add(x, lo, out=x)\n        return x\n\n\n_cupy_permutation = core.ElementwiseKernel(\n    \'raw int32 sample, int32 j_start, int32 _j_end\',\n    \'raw int32 array\',\n    \'\'\'\n        const int invalid = -1;\n        const int num = _ind.size();\n        int j = (sample[i] & 0x7fffffff) % num;\n        int j_end = _j_end;\n        if (j_end > num) j_end = num;\n        if (j == i || j < j_start || j >= j_end) continue;\n\n        // If a thread fails to do data swaping once, it changes j\n        // value using j_offset below and try data swaping again.\n        // This process is repeated until data swapping is succeeded.\n        // The j_offset is determined from the initial j\n        // (random number assigned to each thread) and the initial\n        // offset between j and i (ID of each thread).\n        // If a given number sequence in sample is really random,\n        // this j-update would not be necessary. This is work-around\n        // mainly to avoid potential eternal conflict when sample has\n        // rather synthetic number sequence.\n        int j_offset = ((2*j - i + num) % (num - 1)) + 1;\n\n        // A thread gives up to do data swapping if loop count exceed\n        // a threathod determined below. This is kind of safety\n        // mechanism to escape the eternal race condition, though I\n        // believe it never happens.\n        int loops = 256;\n\n        bool do_next = true;\n        while (do_next && loops > 0) {\n            // try to swap the contents of array[i] and array[j]\n            if (i != j) {\n                int val_j = atomicExch(&array[j], invalid);\n                if (val_j != invalid) {\n                    int val_i = atomicExch(&array[i], invalid);\n                    if (val_i != invalid) {\n                        array[i] = val_j;\n                        array[j] = val_i;\n                        do_next = false;\n                        // done\n                    }\n                    else {\n                        // restore array[j]\n                        array[j] = val_j;\n                    }\n                }\n            }\n            j = (j + j_offset) % num;\n            loops--;\n        }\n    \'\'\',\n    \'cupy_permutation\',\n)\n\n\ndef seed(seed=None):\n    """"""Resets the state of the random number generator with a seed.\n\n    This function resets the state of the global random number generator for\n    the current device. Be careful that generators for other devices are not\n    affected.\n\n    Args:\n        seed (None or int): Seed for the random number generator. If ``None``,\n            it uses :func:`os.urandom` if available or :func:`time.time`\n            otherwise. Note that this function does not support seeding by\n            an integer array.\n\n    """"""\n    get_random_state().seed(seed)\n\n\n# CuPy specific functions\n\n_random_states = {}\n\n\n@atexit.register\ndef reset_states():\n    global _random_states\n    _random_states = {}\n\n\ndef get_random_state():\n    """"""Gets the state of the random number generator for the current device.\n\n    If the state for the current device is not created yet, this function\n    creates a new one, initializes it, and stores it as the state for the\n    current device.\n\n    Returns:\n        RandomState: The state of the random number generator for the\n        device.\n\n    """"""\n    dev = cuda.Device()\n    rs = _random_states.get(dev.id, None)\n    if rs is None:\n        seed = os.getenv(\'CUPY_SEED\')\n        if seed is None:\n            seed = os.getenv(\'CHAINER_SEED\')\n        if seed is not None:\n            seed = numpy.uint64(int(seed))\n        rs = RandomState(seed)\n        rs = _random_states.setdefault(dev.id, rs)\n    return rs\n\n\ndef set_random_state(rs):\n    """"""Sets the state of the random number generator for the current device.\n\n    Args:\n        state(RandomState): Random state to set for the current device.\n    """"""\n    if not isinstance(rs, RandomState):\n        raise TypeError(\n            \'Random state must be an instance of RandomState. \'\n            \'Actual: {}\'.format(type(rs)))\n    _random_states[device.get_device_id()] = rs\n\n\ndef _check_and_get_dtype(dtype):\n    dtype = numpy.dtype(dtype)\n    if dtype.char not in (\'f\', \'d\'):\n        raise TypeError(\'cupy.random only supports float32 and float64\')\n    return dtype\n'"
cupy/random/permutations.py,0,"b'from cupy.random import generator\n\n\ndef shuffle(a):\n    """"""Shuffles an array.\n\n    Args:\n        a (cupy.ndarray): The array to be shuffled.\n\n    .. seealso:: :meth:`numpy.random.shuffle\n                 <numpy.random.mtrand.RandomState.shuffle>`\n\n    """"""\n    rs = generator.get_random_state()\n    return rs.shuffle(a)\n\n\ndef permutation(a):\n    """"""Returns a permuted range or a permutation of an array.\n\n    Args:\n        a (int or cupy.ndarray): The range or the array to be shuffled.\n\n    Returns:\n        cupy.ndarray: If `a` is an integer, it is permutation range between 0\n        and `a` - 1.\n        Otherwise, it is a permutation of `a`.\n\n    .. seealso:: :meth:`numpy.random.permutation\n                 <numpy.random.mtrand.RandomState.permutation>`\n    """"""\n    rs = generator.get_random_state()\n    return rs.permutation(a)\n'"
cupy/random/sample.py,0,"b'from cupy import core\nfrom cupy.creation import basic\nfrom cupy.random import distributions\nfrom cupy.random import generator\n\n\ndef rand(*size, **kwarg):\n    """"""Returns an array of uniform random values over the interval ``[0, 1)``.\n\n    Each element of the array is uniformly distributed on the half-open\n    interval ``[0, 1)``. All elements are identically and independently\n    distributed (i.i.d.).\n\n    Args:\n        size (ints): The shape of the array.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed. The default is\n            :class:`numpy.float64`.\n\n    Returns:\n        cupy.ndarray: A random array.\n\n    .. seealso:: :meth:`numpy.random.rand\n                 <numpy.random.mtrand.RandomState.rand>`\n\n    .. admonition:: Example\n\n       .. code-block:: python\n\n          >>> cupy.random.rand(3, 2)\n          array([[0.86476479, 0.05633727],   # random\n                 [0.27283185, 0.38255354],   # random\n                 [0.16592278, 0.75150313]])  # random\n\n          >>> cupy.random.rand(3, 2, dtype=cupy.float32)\n          array([[0.9672306 , 0.9590486 ],                  # random\n                 [0.6851264 , 0.70457625],                  # random\n                 [0.22382522, 0.36055237]], dtype=float32)  # random\n\n    """"""\n    dtype = kwarg.pop(\'dtype\', float)\n    if kwarg:\n        raise TypeError(\'rand() got unexpected keyword arguments %s\'\n                        % \', \'.join(kwarg.keys()))\n    return random_sample(size=size, dtype=dtype)\n\n\ndef randn(*size, **kwarg):\n    """"""Returns an array of standard normal random values.\n\n    Each element of the array is normally distributed with zero mean and unit\n    variance. All elements are identically and independently distributed\n    (i.i.d.).\n\n    Args:\n        size (ints): The shape of the array.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n            The default is :class:`numpy.float64`.\n\n    Returns:\n        cupy.ndarray: An array of standard normal random values.\n\n    .. seealso:: :meth:`numpy.random.randn\n                 <numpy.random.mtrand.RandomState.randn>`\n\n    .. admonition:: Example\n\n       .. code-block:: python\n\n          >>> cupy.random.randn(3, 2)\n          array([[0.41193321, 1.59579542],   # random\n                 [0.47904589, 0.18566376],   # random\n                 [0.59748424, 2.32602829]])  # random\n\n          >>> cupy.random.randn(3, 2, dtype=cupy.float32)\n          array([[ 0.1373886 ,  2.403238  ],                  # random\n                 [ 0.84020025,  1.5089266 ],                  # random\n                 [-1.2268474 , -0.48219103]], dtype=float32)  # random\n\n    """"""\n    dtype = kwarg.pop(\'dtype\', float)\n    if kwarg:\n        raise TypeError(\'randn() got unexpected keyword arguments %s\'\n                        % \', \'.join(kwarg.keys()))\n    return distributions.normal(size=size, dtype=dtype)\n\n\ndef randint(low, high=None, size=None, dtype=\'l\'):\n    """"""Returns a scalar or an array of integer values over ``[low, high)``.\n\n    Each element of returned values are independently sampled from\n    uniform distribution over left-close and right-open interval\n    ``[low, high)``.\n\n    Args:\n        low (int): If ``high`` is not ``None``,\n            it is the lower bound of the interval.\n            Otherwise, it is the **upper** bound of the interval\n            and lower bound of the interval is set to ``0``.\n        high (int): Upper bound of the interval.\n        size (None or int or tuple of ints): The shape of returned value.\n        dtype: Data type specifier.\n\n    Returns:\n        int or cupy.ndarray of ints: If size is ``None``,\n        it is single integer sampled.\n        If size is integer, it is the 1D-array of length ``size`` element.\n        Otherwise, it is the array whose shape specified by ``size``.\n    """"""\n    rs = generator.get_random_state()\n    return rs.randint(low, high, size, dtype)\n\n\ndef random_integers(low, high=None, size=None):\n    """"""Return a scalar or an array of integer values over ``[low, high]``\n\n    Each element of returned values are independently sampled from\n    uniform distribution over closed interval ``[low, high]``.\n\n    Args:\n        low (int): If ``high`` is not ``None``,\n            it is the lower bound of the interval.\n            Otherwise, it is the **upper** bound of the interval\n            and the lower bound is set to ``1``.\n        high (int): Upper bound of the interval.\n        size (None or int or tuple of ints): The shape of returned value.\n\n    Returns:\n        int or cupy.ndarray of ints: If size is ``None``,\n        it is single integer sampled.\n        If size is integer, it is the 1D-array of length ``size`` element.\n        Otherwise, it is the array whose shape specified by ``size``.\n    """"""\n    if high is None:\n        high = low\n        low = 1\n    return randint(low, high + 1, size)\n\n\ndef random_sample(size=None, dtype=float):\n    """"""Returns an array of random values over the interval ``[0, 1)``.\n\n    This is a variant of :func:`cupy.random.rand`.\n\n    Args:\n        size (int or tuple of ints): The shape of the array.\n        dtype: Data type specifier. Only :class:`numpy.float32` and\n            :class:`numpy.float64` types are allowed.\n\n    Returns:\n        cupy.ndarray: An array of uniformly distributed random values.\n\n    .. seealso:: :meth:`numpy.random.random_sample\n                 <numpy.random.mtrand.RandomState.random_sample>`\n\n    """"""\n    rs = generator.get_random_state()\n    return rs.random_sample(size=size, dtype=dtype)\n\n\ndef choice(a, size=None, replace=True, p=None):\n    """"""Returns an array of random values from a given 1-D array.\n\n    Each element of the returned array is independently sampled\n    from ``a`` according to ``p`` or uniformly.\n\n    .. note::\n\n       Currently ``p`` is not supported when ``replace=False``.\n\n    Args:\n        a (1-D array-like or int):\n            If an array-like,\n            a random sample is generated from its elements.\n            If an int, the random sample is generated as if ``a`` was\n            ``cupy.arange(n)``\n        size (int or tuple of ints): The shape of the array.\n        replace (boolean): Whether the sample is with or without replacement.\n        p (1-D array-like):\n            The probabilities associated with each entry in ``a``.\n            If not given the sample assumes a uniform distribution over all\n            entries in ``a``.\n\n    Returns:\n        cupy.ndarray: An array of ``a`` values distributed according to\n                      ``p`` or uniformly.\n\n    .. seealso:: :meth:`numpy.random.choice\n                 <numpy.random.mtrand.RandomState.choice>`\n\n    """"""\n    rs = generator.get_random_state()\n    return rs.choice(a, size, replace, p)\n\n\n_multinominal_kernel = core.ElementwiseKernel(\n    \'int64 x, int32 p, int32 n\', \'raw U ys\',\n    \'atomicAdd(&ys[i / n * p + x], U(1))\',\n    \'cupy_random_multinomial\',\n    preamble=\'\'\'\n__device__ long long atomicAdd(long long *address, long long val) {\n    return atomicAdd(reinterpret_cast<unsigned long long*>(address),\n                     static_cast<unsigned long long>(val));\n}\'\'\')\n\n\ndef multinomial(n, pvals, size=None):\n    """"""Returns an array from multinomial distribution.\n\n    Args:\n        n (int): Number of trials.\n        pvals (cupy.ndarray): Probabilities of each of the ``p`` different\n            outcomes. The sum of these values must be 1.\n        size (int or tuple of ints or None): Shape of a sample in each trial.\n            For example when ``size`` is ``(a, b)``, shape of returned value is\n            ``(a, b, p)`` where ``p`` is ``len(pvals)``.\n            If ``size`` is ``None``, it is treated as ``()``. So, shape of\n            returned value is ``(p,)``.\n\n    Returns:\n        cupy.ndarray: An array drawn from multinomial distribution.\n\n    .. note::\n       It does not support ``sum(pvals) < 1`` case.\n\n    .. seealso:: :meth:`numpy.random.multinomial\n                 <numpy.random.mtrand.RandomState.multinomial>`\n    """"""\n\n    if size is None:\n        m = 1\n        size = ()\n    elif isinstance(size, int):\n        m = size\n        size = (size,)\n    else:\n        size = tuple(size)\n        m = 1\n        for x in size:\n            m *= x\n\n    p = len(pvals)\n    shape = size + (p,)\n    ys = basic.zeros(shape, \'l\')\n    if ys.size > 0:\n        xs = choice(p, p=pvals, size=n * m)\n        _multinominal_kernel(xs, p, n, ys)\n    return ys\n'"
cupy/sparse/__init__.py,0,b'from cupyx.scipy.sparse.base import issparse  # NOQA\nfrom cupyx.scipy.sparse.base import isspmatrix  # NOQA\nfrom cupyx.scipy.sparse.base import spmatrix  # NOQA\nfrom cupyx.scipy.sparse.coo import coo_matrix  # NOQA\nfrom cupyx.scipy.sparse.coo import isspmatrix_coo  # NOQA\nfrom cupyx.scipy.sparse.csc import csc_matrix  # NOQA\nfrom cupyx.scipy.sparse.csc import isspmatrix_csc  # NOQA\nfrom cupyx.scipy.sparse.csr import csr_matrix  # NOQA\nfrom cupyx.scipy.sparse.csr import isspmatrix_csr  # NOQA\nfrom cupyx.scipy.sparse.dia import dia_matrix  # NOQA\nfrom cupyx.scipy.sparse.dia import isspmatrix_dia  # NOQA\n\nfrom cupyx.scipy.sparse.construct import eye  # NOQA\nfrom cupyx.scipy.sparse.construct import identity  # NOQA\nfrom cupyx.scipy.sparse.construct import rand  # NOQA\nfrom cupyx.scipy.sparse.construct import random  # NOQA\nfrom cupyx.scipy.sparse.construct import spdiags  # NOQA\n\nfrom cupyx.scipy.sparse.construct import bmat  # NOQA\nfrom cupyx.scipy.sparse.construct import hstack  # NOQA\nfrom cupyx.scipy.sparse.construct import vstack  # NOQA\n\n# TODO(unno): implement bsr_matrix\n# TODO(unno): implement dok_matrix\n# TODO(unno): implement lil_matrix\n\n# TODO(unno): implement kron\n# TODO(unno): implement kronsum\n# TODO(unno): implement diags\n# TODO(unno): implement block_diag\n# TODO(unno): implement tril\n# TODO(unno): implement triu\n\n# TODO(unno): implement save_npz\n# TODO(unno): implement load_npz\n\n# TODO(unno): implement find\n\n# TODO(unno): implement isspmatrix_bsr(x)\n# TODO(unno): implement isspmatrix_lil(x)\n# TODO(unno): implement isspmatrix_dok(x)\n\nfrom cupyx.scipy.sparse import linalg  # NOQA\n'
cupy/statistics/__init__.py,0,"b'# Functions from the following NumPy document\n# https://docs.scipy.org/doc/numpy/reference/routines.statistics.html\n\n# ""NOQA"" to suppress flake8 warning\nfrom cupy.statistics import correlation  # NOQA\nfrom cupy.statistics import histogram  # NOQA\nfrom cupy.statistics import meanvar  # NOQA\nfrom cupy.statistics import order  # NOQA\n'"
cupy/statistics/correlation.py,0,"b'import functools\nimport warnings\n\nimport numpy\n\nimport cupy\nfrom cupy import core\n\n\ndef corrcoef(a, y=None, rowvar=True, bias=None, ddof=None):\n    """"""Returns the Pearson product-moment correlation coefficients of an array.\n\n    Args:\n        a (cupy.ndarray): Array to compute the Pearson product-moment\n            correlation coefficients.\n        y (cupy.ndarray): An additional set of variables and observations.\n        rowvar (bool): If ``True``, then each row represents a variable, with\n            observations in the columns. Otherwise, the relationship is\n            transposed.\n        bias (None): Has no effect, do not use.\n        ddof (None): Has no effect, do not use.\n\n    Returns:\n        cupy.ndarray: The Pearson product-moment correlation coefficients of\n        the input array.\n\n    .. seealso:: :func:`numpy.corrcoef`\n\n    """"""\n    if bias is not None or ddof is not None:\n        warnings.warn(\'bias and ddof have no effect and are deprecated\',\n                      DeprecationWarning)\n\n    out = cov(a, y, rowvar)\n    try:\n        d = cupy.diag(out)\n    except ValueError:\n        return out / out\n\n    stddev = cupy.sqrt(d.real)\n    out /= stddev[:, None]\n    out /= stddev[None, :]\n\n    cupy.clip(out.real, -1, 1, out=out.real)\n    if cupy.iscomplexobj(out):\n        cupy.clip(out.imag, -1, 1, out=out.imag)\n\n    return out\n\n\n# TODO(okuta): Implement correlate\n\n\ndef cov(a, y=None, rowvar=True, bias=False, ddof=None):\n    """"""Returns the covariance matrix of an array.\n\n    This function currently does not support ``fweights`` and ``aweights``\n    options.\n\n    Args:\n        a (cupy.ndarray): Array to compute covariance matrix.\n        y (cupy.ndarray): An additional set of variables and observations.\n        rowvar (bool): If ``True``, then each row represents a variable, with\n            observations in the columns. Otherwise, the relationship is\n            transposed.\n        bias (bool): If ``False``, normalization is by ``(N - 1)``, where N is\n            the number of observations given (unbiased estimate). If ``True``,\n            then normalization is by ``N``.\n        ddof (int): If not ``None`` the default value implied by bias is\n            overridden. Note that ``ddof=1`` will return the unbiased estimate\n            and ``ddof=0`` will return the simple average.\n\n    Returns:\n        cupy.ndarray: The covariance matrix of the input array.\n\n    .. seealso:: :func:`numpy.cov`\n\n    """"""\n    if ddof is not None and ddof != int(ddof):\n        raise ValueError(\'ddof must be integer\')\n\n    if a.ndim > 2:\n        raise ValueError(\'Input must be <= 2-d\')\n\n    if y is None:\n        dtype = numpy.promote_types(a.dtype, numpy.float64)\n    else:\n        if y.ndim > 2:\n            raise ValueError(\'y must be <= 2-d\')\n        dtype = functools.reduce(numpy.promote_types,\n                                 (a.dtype, y.dtype, numpy.float64))\n\n    X = cupy.array(a, ndmin=2, dtype=dtype)\n    if not rowvar and X.shape[0] != 1:\n        X = X.T\n    if X.shape[0] == 0:\n        return cupy.array([]).reshape(0, 0)\n    if y is not None:\n        y = cupy.array(y, copy=False, ndmin=2, dtype=dtype)\n        if not rowvar and y.shape[0] != 1:\n            y = y.T\n        X = core.concatenate_method((X, y), axis=0)\n\n    if ddof is None:\n        ddof = 0 if bias else 1\n\n    fact = X.shape[1] - ddof\n    if fact <= 0:\n        warnings.warn(\'Degrees of freedom <= 0 for slice\',\n                      RuntimeWarning, stacklevel=2)\n        fact = 0.0\n\n    X -= X.mean(axis=1)[:, None]\n    out = X.dot(X.T.conj()) * (1 / cupy.float64(fact))\n\n    return out.squeeze()\n'"
cupy/statistics/histogram.py,0,"b'import operator\nimport warnings\n\nimport numpy\n\nimport cupy\nfrom cupy import core\n\n\n_preamble = \'\'\'\n__device__ long long atomicAdd(long long *address, long long val) {\n    return atomicAdd(reinterpret_cast<unsigned long long*>(address),\n                     static_cast<unsigned long long>(val));\n}\'\'\'\n\n# TODO(unno): use searchsorted\n_histogram_kernel = core.ElementwiseKernel(\n    \'S x, raw T bins, int32 n_bins\',\n    \'raw U y\',\n    \'\'\'\n    if (x < bins[0] or bins[n_bins - 1] < x) {\n        return;\n    }\n    int high = n_bins - 1;\n    int low = 0;\n\n    while (high - low > 1) {\n        int mid = (high + low) / 2;\n        if (bins[mid] <= x) {\n            low = mid;\n        } else {\n            high = mid;\n        }\n    }\n    atomicAdd(&y[low], U(1));\n    \'\'\',\n    preamble=_preamble)\n\n\n_weighted_histogram_kernel = core.ElementwiseKernel(\n    \'S x, raw T bins, int32 n_bins, raw W weights\',\n    \'raw Y y\',\n    \'\'\'\n    if (x < bins[0] or bins[n_bins - 1] < x) {\n        return;\n    }\n    int high = n_bins - 1;\n    int low = 0;\n\n    while (high - low > 1) {\n        int mid = (high + low) / 2;\n        if (bins[mid] <= x) {\n            low = mid;\n        } else {\n            high = mid;\n        }\n    }\n    atomicAdd(&y[low], (Y)weights[i]);\n    \'\'\',\n    preamble=_preamble)\n\n\ndef _ravel_and_check_weights(a, weights):\n    """""" Check a and weights have matching shapes, and ravel both """"""\n\n    # Ensure that the array is a ""subtractable"" dtype\n    if a.dtype == cupy.bool_:\n        warnings.warn(""Converting input from {} to {} for compatibility.""\n                      .format(a.dtype, cupy.uint8),\n                      RuntimeWarning, stacklevel=3)\n        a = a.astype(cupy.uint8)\n\n    if weights is not None:\n        if not isinstance(weights, cupy.ndarray):\n            raise ValueError(""weights must be a cupy.ndarray"")\n        if weights.shape != a.shape:\n            raise ValueError(\n                \'weights should have the same shape as a.\')\n        weights = weights.ravel()\n    a = a.ravel()\n    return a, weights\n\n\ndef _get_outer_edges(a, range):\n    """"""\n    Determine the outer bin edges to use, from either the data or the range\n    argument\n    """"""\n    if range is not None:\n        first_edge, last_edge = range\n        if first_edge > last_edge:\n            raise ValueError(\n                \'max must be larger than min in range parameter.\')\n        if not (numpy.isfinite(first_edge) and numpy.isfinite(last_edge)):\n            raise ValueError(\n                ""supplied range of [{}, {}] is not finite"".format(\n                    first_edge, last_edge))\n    elif a.size == 0:\n        first_edge = 0.0\n        last_edge = 1.0\n    else:\n        first_edge = float(a.min())\n        last_edge = float(a.max())\n        if not (cupy.isfinite(first_edge) and cupy.isfinite(last_edge)):\n            raise ValueError(\n                ""autodetected range of [{}, {}] is not finite"".format(\n                    first_edge, last_edge))\n\n    # expand empty range to avoid divide by zero\n    if first_edge == last_edge:\n        first_edge = first_edge - 0.5\n        last_edge = last_edge + 0.5\n\n    return first_edge, last_edge\n\n\ndef _get_bin_edges(a, bins, range):\n    """"""\n    Computes the bins used internally by `histogram`.\n\n    Args:\n        a (ndarray): Ravelled data array\n        bins (int or ndarray): Forwarded argument from `histogram`.\n        range (None or tuple): Forwarded argument from `histogram`.\n\n    Returns:\n        bin_edges (ndarray): Array of bin edges\n    """"""\n    # parse the overloaded bins argument\n    n_equal_bins = None\n    bin_edges = None\n\n    if isinstance(bins, int):  # cupy.ndim(bins) == 0:\n        try:\n            n_equal_bins = operator.index(bins)\n        except TypeError:\n            raise TypeError(\n                \'`bins` must be an integer, a string, or an array\')\n        if n_equal_bins < 1:\n            raise ValueError(\'`bins` must be positive, when an integer\')\n\n        first_edge, last_edge = _get_outer_edges(a, range)\n\n    elif isinstance(bins, cupy.ndarray):\n        if bins.ndim == 1:    # cupy.ndim(bins) == 0:\n            bin_edges = cupy.asarray(bins)\n            if (bin_edges[:-1] > bin_edges[1:]).any():  # synchronize!\n                raise ValueError(\n                    \'`bins` must increase monotonically, when an array\')\n\n    elif isinstance(bins, str):\n        raise NotImplementedError(\n            ""only integer and array bins are implemented"")\n\n    if n_equal_bins is not None:\n        # numpy\'s gh-10322 means that type resolution rules are dependent on\n        # array shapes. To avoid this causing problems, we pick a type now and\n        # stick with it throughout.\n        bin_type = cupy.result_type(first_edge, last_edge, a)\n        if cupy.issubdtype(bin_type, cupy.integer):\n            bin_type = cupy.result_type(bin_type, float)\n\n        # bin edges must be computed\n        bin_edges = cupy.linspace(\n            first_edge, last_edge, n_equal_bins + 1,\n            endpoint=True, dtype=bin_type)\n    return bin_edges\n\n\ndef histogram(x, bins=10, range=None, weights=None, density=False):\n    """"""Computes the histogram of a set of data.\n\n    Args:\n        x (cupy.ndarray): Input array.\n        bins (int or cupy.ndarray): If ``bins`` is an int, it represents the\n            number of bins. If ``bins`` is an :class:`~cupy.ndarray`, it\n            represents a bin edges.\n        range (2-tuple of float, optional): The lower and upper range of the\n            bins.  If not provided, range is simply ``(x.min(), x.max())``.\n            Values outside the range are ignored. The first element of the\n            range must be less than or equal to the second. `range` affects the\n            automatic bin computation as well. While bin width is computed to\n            be optimal based on the actual data within `range`, the bin count\n            will fill the entire range including portions containing no data.\n        density (bool, optional): If False, the default, returns the number of\n            samples in each bin. If True, returns the probability *density*\n            function at the bin, ``bin_count / sample_count / bin_volume``.\n        weights (cupy.ndarray, optional): An array of weights, of the same\n            shape as `x`.  Each value in `x` only contributes its associated\n            weight towards the bin count (instead of 1).\n    Returns:\n        tuple: ``(hist, bin_edges)`` where ``hist`` is a :class:`cupy.ndarray`\n        storing the values of the histogram, and ``bin_edges`` is a\n        :class:`cupy.ndarray` storing the bin edges.\n\n    .. warning::\n\n        This function may synchronize the device.\n\n    .. seealso:: :func:`numpy.histogram`\n    """"""\n\n    if x.dtype.kind == \'c\':\n        # TODO(unno): comparison between complex numbers is not implemented\n        raise NotImplementedError(\'complex number is not supported\')\n\n    if not isinstance(x, cupy.ndarray):\n        raise ValueError(""x must be a cupy.ndarray"")\n\n    x, weights = _ravel_and_check_weights(x, weights)\n    bin_edges = _get_bin_edges(x, bins, range)\n\n    if weights is None:\n        y = cupy.zeros(bin_edges.size - 1, dtype=\'l\')\n        _histogram_kernel(x, bin_edges, bin_edges.size, y)\n    else:\n        simple_weights = (\n            cupy.can_cast(weights.dtype, cupy.float64) or\n            cupy.can_cast(weights.dtype, cupy.complex128)\n        )\n        if not simple_weights:\n            # object dtype such as Decimal are supported in NumPy, but not here\n            raise NotImplementedError(\n                ""only weights with dtype that can be cast to float or complex ""\n                ""are supported"")\n        if weights.dtype.kind == \'c\':\n            y = cupy.zeros(bin_edges.size - 1, dtype=cupy.complex128)\n            _weighted_histogram_kernel(\n                x, bin_edges, bin_edges.size, weights.real, y.real)\n            _weighted_histogram_kernel(\n                x, bin_edges, bin_edges.size, weights.imag, y.imag)\n        else:\n            if weights.dtype.kind in \'bui\':\n                y = cupy.zeros(bin_edges.size - 1, dtype=int)\n            else:\n                y = cupy.zeros(bin_edges.size - 1, dtype=cupy.float64)\n            _weighted_histogram_kernel(\n                x, bin_edges, bin_edges.size, weights, y)\n\n    if density:\n        db = cupy.array(cupy.diff(bin_edges), cupy.float64)\n        return y/db/y.sum(), bin_edges\n    return y, bin_edges\n\n# TODO(okuta): Implement histogram2d\n\n\n# TODO(okuta): Implement histogramdd\n\n_bincount_kernel = core.ElementwiseKernel(\n    \'S x\', \'raw U bin\',\n    \'atomicAdd(&bin[x], U(1))\',\n    \'bincount_kernel\',\n    preamble=_preamble)\n_bincount_with_weight_kernel = core.ElementwiseKernel(\n    \'S x, T w\', \'raw U bin\',\n    \'atomicAdd(&bin[x], w)\',\n    \'bincount_with_weight_kernel\')\n\n\ndef bincount(x, weights=None, minlength=None):\n    """"""Count number of occurrences of each value in array of non-negative ints.\n\n    Args:\n        x (cupy.ndarray): Input array.\n        weights (cupy.ndarray): Weights array which has the same shape as\n            ``x``.\n        minlength (int): A minimum number of bins for the output array.\n\n    Returns:\n        cupy.ndarray: The result of binning the input array. The length of\n            output is equal to ``max(cupy.max(x) + 1, minlength)``.\n\n    .. warning::\n\n        This function may synchronize the device.\n\n    .. seealso:: :func:`numpy.bincount`\n\n    """"""\n    if x.ndim > 1:\n        raise ValueError(\'object too deep for desired array\')\n    if x.ndim < 1:\n        raise ValueError(\'object of too small depth for desired array\')\n    if x.dtype.kind == \'f\':\n        raise TypeError(\'x must be int array\')\n    if (x < 0).any():  # synchronize!\n        raise ValueError(\'The first argument of bincount must be non-negative\')\n    if weights is not None and x.shape != weights.shape:\n        raise ValueError(\'The weights and list don\\\'t have the same length.\')\n    if minlength is not None:\n        minlength = int(minlength)\n        if minlength < 0:\n            raise ValueError(\'minlength must be non-negative\')\n\n    size = int(cupy.max(x)) + 1\n    if minlength is not None:\n        size = max(size, minlength)\n\n    if weights is None:\n        b = cupy.zeros((size,), dtype=numpy.intp)\n        _bincount_kernel(x, b)\n    else:\n        b = cupy.zeros((size,), dtype=numpy.float64)\n        _bincount_with_weight_kernel(x, weights, b)\n\n    return b\n\n\ndef digitize(x, bins, right=False):\n    """"""Finds the indices of the bins to which each value in input array belongs.\n\n    .. note::\n\n        In order to avoid device synchronization, digitize does not raise\n        an exception when the array is not monotonic\n\n    Args:\n        x (cupy.ndarray): Input array.\n        bins (cupy.ndarray): Array of bins.\n            It has to be 1-dimensional and monotonic increasing or decreasing.\n        right (bool):\n            Indicates whether the intervals include the right or the left bin\n            edge.\n\n    Returns:\n        cupy.ndarray: Output array of indices, of same shape as ``x``.\n\n    .. seealso:: :func:`numpy.digitize`\n    """"""\n    # This is for NumPy compat, although it works fine\n    if x.dtype.kind == \'c\':\n        raise TypeError(\'x may not be complex\')\n\n    if bins.ndim > 1:\n        raise ValueError(\'object too deep for desired array\')\n    if bins.ndim < 1:\n        raise ValueError(\'object of too small depth for desired array\')\n\n    # As the order of the arguments are reversed, the side must be too.\n    side = \'left\' if right else \'right\'\n    return cupy._sorting.search._searchsorted(bins, x, side, None, False)\n'"
cupy/statistics/meanvar.py,0,"b'import functools\nimport numpy\n\nimport cupy\nfrom cupy.core import _routines_statistics as _statistics\n\n\ndef median(a, axis=None, out=None, overwrite_input=False, keepdims=False):\n    """"""Compute the median along the specified axis.\n\n    Returns the median of the array elements.\n\n    Args:\n        a (cupy.ndarray): Array to compute the median.\n        axis (int): Axis along which the medians are computed. The flattened\n            array is used by default.\n        out (cupy.ndarray): Output array.\n        overwrite_input (bool): If ``True``, then allow use of memory of input\n            array a for calculations. The input array will be modified by the\n            call to median. This will save memory when you do not need to\n            preserve the contents of the input array. Treat the input as\n            undefined, but it will probably be fully or partially sorted.\n            Default is ``False``. If ``overwrite_input`` is ``True`` and ``a``\n            is not already an ndarray, an error will be raised.\n        keepdims (bool): If ``True``, the axis is remained as an axis of size\n            one.\n\n    Returns:\n        cupy.ndarray: The median of ``a``, along the axis if specified.\n\n    .. seealso:: :func:`numpy.median`\n\n    """"""\n    return _statistics._median(a, axis, out, overwrite_input, keepdims)\n\n\ndef average(a, axis=None, weights=None, returned=False):\n    """"""Returns the weighted average along an axis.\n\n    Args:\n        a (cupy.ndarray): Array to compute average.\n        axis (int): Along which axis to compute average. The flattened array\n            is used by default.\n        weights (cupy.ndarray): Array of weights where each element\n            corresponds to the value in ``a``. If ``None``, all the values\n            in ``a`` have a weight equal to one.\n        returned (bool): If ``True``, a tuple of the average and the sum\n            of weights is returned, otherwise only the average is returned.\n\n    Returns:\n        cupy.ndarray or tuple of cupy.ndarray: The average of the input array\n            along the axis and the sum of weights.\n\n    .. warning::\n\n        This function may synchronize the device if ``weight`` is given.\n\n    .. seealso:: :func:`numpy.average`\n    """"""\n    # TODO(niboshi): Avoid synchronization.\n    a = cupy.asarray(a)\n\n    if weights is None:\n        avg = a.mean(axis)\n        scl = avg.dtype.type(a.size / avg.size)\n    else:\n        wgt = cupy.asarray(weights)\n\n        if issubclass(a.dtype.type, (numpy.integer, numpy.bool_)):\n            result_dtype = functools.reduce(numpy.promote_types,\n                                            (a.dtype, wgt.dtype, \'f8\'))\n        else:\n            result_dtype = numpy.promote_types(a.dtype, wgt.dtype)\n\n        # Sanity checks\n        if a.shape != wgt.shape:\n            if axis is None:\n                raise TypeError(\n                    \'Axis must be specified when shapes of a and weights \'\n                    \'differ.\')\n            if wgt.ndim != 1:\n                raise TypeError(\n                    \'1D weights expected when shapes of a and weights differ.\')\n            if wgt.shape[0] != a.shape[axis]:\n                raise ValueError(\n                    \'Length of weights not compatible with specified axis.\')\n\n            # setup wgt to broadcast along axis\n            wgt = cupy.broadcast_to(wgt, (a.ndim - 1) * (1,) + wgt.shape)\n            wgt = wgt.swapaxes(-1, axis)\n\n        scl = wgt.sum(axis=axis, dtype=result_dtype)\n        if cupy.any(scl == 0.0):  # synchronize!\n            raise ZeroDivisionError(\n                \'Weights sum to zero, can\\\'t be normalized\')\n\n        avg = cupy.multiply(a, wgt, dtype=result_dtype).sum(axis) / scl\n\n    if returned:\n        if scl.shape != avg.shape:\n            scl = cupy.broadcast_to(cupy.array(scl), avg.shape).copy()\n        return avg, scl\n    else:\n        return avg\n\n\ndef mean(a, axis=None, dtype=None, out=None, keepdims=False):\n    """"""Returns the arithmetic mean along an axis.\n\n    Args:\n        a (cupy.ndarray): Array to compute mean.\n        axis (int): Along which axis to compute mean. The flattened array is\n            used by default.\n        dtype: Data type specifier.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the axis is remained as an axis of\n            size one.\n\n    Returns:\n        cupy.ndarray: The mean of the input array along the axis.\n\n    .. seealso:: :func:`numpy.mean`\n\n    """"""\n    # TODO(okuta): check type\n    return a.mean(axis=axis, dtype=dtype, out=out, keepdims=keepdims)\n\n\ndef var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):\n    """"""Returns the variance along an axis.\n\n    Args:\n        a (cupy.ndarray): Array to compute variance.\n        axis (int): Along which axis to compute variance. The flattened array\n            is used by default.\n        dtype: Data type specifier.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the axis is remained as an axis of\n            size one.\n\n    Returns:\n        cupy.ndarray: The variance of the input array along the axis.\n\n    .. seealso:: :func:`numpy.var`\n\n    """"""\n    # TODO(okuta): check type\n    return a.var(axis=axis, dtype=dtype, out=out, ddof=ddof,\n                 keepdims=keepdims)\n\n\ndef std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):\n    """"""Returns the standard deviation along an axis.\n\n    Args:\n        a (cupy.ndarray): Array to compute standard deviation.\n        axis (int): Along which axis to compute standard deviation. The\n            flattened array is used by default.\n        dtype: Data type specifier.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the axis is remained as an axis of\n            size one.\n\n    Returns:\n        cupy.ndarray: The standard deviation of the input array along the axis.\n\n    .. seealso:: :func:`numpy.std`\n\n    """"""\n    # TODO(okuta): check type\n    return a.std(axis=axis, dtype=dtype, out=out, ddof=ddof,\n                 keepdims=keepdims)\n\n\ndef nanmean(a, axis=None, dtype=None, out=None, keepdims=False):\n    """"""Returns the arithmetic mean along an axis ignoring NaN values.\n\n    Args:\n        a (cupy.ndarray): Array to compute mean.\n        axis (int): Along which axis to compute mean. The flattened array is\n            used by default.\n        dtype: Data type specifier.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the axis is remained as an axis of\n            size one.\n\n    Returns:\n        cupy.ndarray: The mean of the input array along the axis ignoring NaNs.\n\n    .. seealso:: :func:`numpy.nanmean`\n\n    """"""\n    if a.dtype.kind in \'biu\':\n        return a.mean(axis=axis, dtype=dtype, out=out, keepdims=keepdims)\n\n    # TODO(okuta): check type\n    return _statistics._nanmean(\n        a, axis=axis, dtype=dtype, out=out, keepdims=keepdims)\n\n\ndef nanvar(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):\n    """"""Returns the variance along an axis ignoring NaN values.\n\n    Args:\n        a (cupy.ndarray): Array to compute variance.\n        axis (int): Along which axis to compute variance. The flattened array\n            is used by default.\n        dtype: Data type specifier.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the axis is remained as an axis of\n            size one.\n\n    Returns:\n        cupy.ndarray: The variance of the input array along the axis.\n\n    .. seealso:: :func:`numpy.nanvar`\n\n    """"""\n    if a.dtype.kind in \'biu\':\n        return a.var(axis=axis, dtype=dtype, out=out, ddof=ddof,\n                     keepdims=keepdims)\n\n    # TODO(okuta): check type\n    return _statistics._nanvar(\n        a, axis=axis, dtype=dtype, out=out, ddof=ddof, keepdims=keepdims)\n\n\ndef nanstd(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):\n    """"""Returns the standard deviation along an axis ignoring NaN values.\n\n    Args:\n        a (cupy.ndarray): Array to compute standard deviation.\n        axis (int): Along which axis to compute standard deviation. The\n            flattened array is used by default.\n        dtype: Data type specifier.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the axis is remained as an axis of\n            size one.\n\n    Returns:\n        cupy.ndarray: The standard deviation of the input array along the axis.\n\n    .. seealso:: :func:`numpy.nanstd`\n\n    """"""\n    if a.dtype.kind in \'biu\':\n        return a.std(axis=axis, dtype=dtype, out=out, ddof=ddof,\n                     keepdims=keepdims)\n\n    # TODO(okuta): check type\n    return _statistics._nanstd(\n        a, axis=axis, dtype=dtype, out=out, ddof=ddof, keepdims=keepdims)\n'"
cupy/statistics/order.py,0,"b'import warnings\n\nimport cupy\nfrom cupy import core\nfrom cupy.core import _routines_statistics as _statistics\nfrom cupy.core import fusion\nfrom cupy.logic import content\n\n\ndef amin(a, axis=None, out=None, keepdims=False):\n    """"""Returns the minimum of an array or the minimum along an axis.\n\n    .. note::\n\n       When at least one element is NaN, the corresponding min value will be\n       NaN.\n\n    Args:\n        a (cupy.ndarray): Array to take the minimum.\n        axis (int): Along which axis to take the minimum. The flattened array\n            is used by default.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the axis is remained as an axis of\n            size one.\n\n    Returns:\n        cupy.ndarray: The minimum of ``a``, along the axis if specified.\n\n    .. seealso:: :func:`numpy.amin`\n\n    """"""\n    if fusion._is_fusing():\n        if keepdims:\n            raise NotImplementedError(\n                \'cupy.amin does not support `keepdims` in fusion yet.\')\n        return fusion._call_reduction(_statistics.amin,\n                                      a, axis=axis, out=out)\n\n    # TODO(okuta): check type\n    return a.min(axis=axis, out=out, keepdims=keepdims)\n\n\ndef amax(a, axis=None, out=None, keepdims=False):\n    """"""Returns the maximum of an array or the maximum along an axis.\n\n    .. note::\n\n       When at least one element is NaN, the corresponding min value will be\n       NaN.\n\n    Args:\n        a (cupy.ndarray): Array to take the maximum.\n        axis (int): Along which axis to take the maximum. The flattened array\n            is used by default.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the axis is remained as an axis of\n            size one.\n\n    Returns:\n        cupy.ndarray: The maximum of ``a``, along the axis if specified.\n\n    .. seealso:: :func:`numpy.amax`\n\n    """"""\n    if fusion._is_fusing():\n        if keepdims:\n            raise NotImplementedError(\n                \'cupy.amax does not support `keepdims` in fusion yet.\')\n        return fusion._call_reduction(_statistics.amax,\n                                      a, axis=axis, out=out)\n\n    # TODO(okuta): check type\n    return a.max(axis=axis, out=out, keepdims=keepdims)\n\n\ndef nanmin(a, axis=None, out=None, keepdims=False):\n    """"""Returns the minimum of an array along an axis ignoring NaN.\n\n    When there is a slice whose elements are all NaN, a :class:`RuntimeWarning`\n    is raised and NaN is returned.\n\n    Args:\n        a (cupy.ndarray): Array to take the minimum.\n        axis (int): Along which axis to take the minimum. The flattened array\n            is used by default.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the axis is remained as an axis of\n            size one.\n\n    Returns:\n        cupy.ndarray: The minimum of ``a``, along the axis if specified.\n\n    .. warning::\n\n        This function may synchronize the device.\n\n    .. seealso:: :func:`numpy.nanmin`\n\n    """"""\n    # TODO(niboshi): Avoid synchronization.\n    res = core.nanmin(a, axis=axis, out=out, keepdims=keepdims)\n    if content.isnan(res).any():  # synchronize!\n        warnings.warn(\'All-NaN slice encountered\', RuntimeWarning)\n    return res\n\n\ndef nanmax(a, axis=None, out=None, keepdims=False):\n    """"""Returns the maximum of an array along an axis ignoring NaN.\n\n    When there is a slice whose elements are all NaN, a :class:`RuntimeWarning`\n    is raised and NaN is returned.\n\n    Args:\n        a (cupy.ndarray): Array to take the maximum.\n        axis (int): Along which axis to take the maximum. The flattened array\n            is used by default.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the axis is remained as an axis of\n            size one.\n\n    Returns:\n        cupy.ndarray: The maximum of ``a``, along the axis if specified.\n\n    .. warning::\n\n        This function may synchronize the device.\n\n    .. seealso:: :func:`numpy.nanmax`\n\n    """"""\n    # TODO(niboshi): Avoid synchronization.\n    res = core.nanmax(a, axis=axis, out=out, keepdims=keepdims)\n    if content.isnan(res).any():  # synchronize!\n        warnings.warn(\'All-NaN slice encountered\', RuntimeWarning)\n    return res\n\n\ndef ptp(a, axis=None, out=None, keepdims=False):\n    """"""Returns the range of values (maximum - minimum) along an axis.\n\n    .. note::\n\n       The name of the function comes from the acronym for \'peak to peak\'.\n\n       When at least one element is NaN, the corresponding ptp value will be\n       NaN.\n\n    Args:\n        a (cupy.ndarray): Array over which to take the range.\n        axis (int): Axis along which to take the minimum. The flattened\n            array is used by default.\n        out (cupy.ndarray): Output array.\n        keepdims (bool): If ``True``, the axis is retained as an axis of\n            size one.\n\n    Returns:\n        cupy.ndarray: The minimum of ``a``, along the axis if specified.\n\n    .. seealso:: :func:`numpy.amin`\n\n    """"""\n    return a.ptp(axis=axis, out=out, keepdims=keepdims)\n\n\ndef percentile(a, q, axis=None, out=None, interpolation=\'linear\',\n               keepdims=False):\n    """"""Computes the q-th percentile of the data along the specified axis.\n\n    Args:\n        a (cupy.ndarray): Array for which to compute percentiles.\n        q (float, tuple of floats or cupy.ndarray): Percentiles to compute\n            in the range between 0 and 100 inclusive.\n        axis (int or tuple of ints): Along which axis or axes to compute the\n            percentiles. The flattened array is used by default.\n        out (cupy.ndarray): Output array.\n        interpolation (str): Interpolation method when a quantile lies between\n            two data points. ``linear`` interpolation is used by default.\n            Supported interpolations are``lower``, ``higher``, ``midpoint``,\n            ``nearest`` and ``linear``.\n        keepdims (bool): If ``True``, the axis is remained as an axis of\n            size one.\n\n    Returns:\n        cupy.ndarray: The percentiles of ``a``, along the axis if specified.\n\n    .. seealso:: :func:`numpy.percentile`\n\n    """"""\n    q = cupy.asarray(q, dtype=a.dtype)\n    if q.ndim == 0:\n        q = q[None]\n        zerod = True\n    else:\n        zerod = False\n    if q.ndim > 1:\n        raise ValueError(\'Expected q to have a dimension of 1.\\n\'\n                         \'Actual: {0} != 1\'.format(q.ndim))\n\n    if keepdims:\n        if axis is None:\n            keepdim = (1,) * a.ndim\n        else:\n            keepdim = list(a.shape)\n            for ax in axis:\n                keepdim[ax % a.ndim] = 1\n            keepdim = tuple(keepdim)\n\n    # Copy a since we need it sorted but without modifying the original array\n    if isinstance(axis, int):\n        axis = axis,\n    if axis is None:\n        ap = a.flatten()\n        nkeep = 0\n    else:\n        # Reduce axes from a and put them last\n        axis = tuple(ax % a.ndim for ax in axis)\n        keep = set(range(a.ndim)) - set(axis)\n        nkeep = len(keep)\n        for i, s in enumerate(sorted(keep)):\n            a = a.swapaxes(i, s)\n        ap = a.reshape(a.shape[:nkeep] + (-1,)).copy()\n\n    axis = -1\n    ap.sort(axis=axis)\n    Nx = ap.shape[axis]\n    indices = q * 0.01 * (Nx - 1.)  # percents to decimals\n\n    if interpolation == \'lower\':\n        indices = cupy.floor(indices).astype(cupy.int32)\n    elif interpolation == \'higher\':\n        indices = cupy.ceil(indices).astype(cupy.int32)\n    elif interpolation == \'midpoint\':\n        indices = 0.5 * (cupy.floor(indices) + cupy.ceil(indices))\n    elif interpolation == \'nearest\':\n        # TODO(hvy): Implement nearest using around\n        raise ValueError(\'\\\'nearest\\\' interpolation is not yet supported. \'\n                         \'Please use any other interpolation method.\')\n    elif interpolation == \'linear\':\n        pass\n    else:\n        raise ValueError(\'Unexpected interpolation method.\\n\'\n                         \'Actual: \\\'{0}\\\' not in (\\\'linear\\\', \\\'lower\\\', \'\n                         \'\\\'higher\\\', \\\'midpoint\\\')\'.format(interpolation))\n\n    if indices.dtype == cupy.int32:\n        ret = cupy.rollaxis(ap, axis)\n        ret = ret.take(indices, axis=0, out=out)\n    else:\n        if out is None:\n            ret = cupy.empty(ap.shape[:-1] + q.shape, dtype=cupy.float64)\n        else:\n            ret = cupy.rollaxis(out, 0, out.ndim)\n\n        cupy.ElementwiseKernel(\n            \'S idx, raw T a, raw int32 offset\', \'U ret\',\n            \'\'\'\n            ptrdiff_t idx_below = floor(idx);\n            U weight_above = idx - idx_below;\n\n            ptrdiff_t offset_i = _ind.get()[0] * offset;\n            ret = a[offset_i + idx_below] * (1.0 - weight_above)\n              + a[offset_i + idx_below + 1] * weight_above;\n            \'\'\',\n            \'percentile_weightnening\'\n        )(indices, ap, ap.shape[-1] if ap.ndim > 1 else 0, ret)\n        ret = cupy.rollaxis(ret, -1)  # Roll q dimension back to first axis\n\n    if zerod:\n        ret = ret.squeeze(0)\n    if keepdims:\n        if q.size > 1:\n            keepdim = (-1,) + keepdim\n        ret = ret.reshape(keepdim)\n\n    return core._internal_ascontiguousarray(ret)\n'"
cupy/testing/__init__.py,0,b'from cupy.testing import array  # NOQA\nfrom cupy.testing import attr  # NOQA\nfrom cupy.testing import helper  # NOQA\nfrom cupy.testing import parameterized  # NOQA\nfrom cupy.testing import random  # NOQA\n\nfrom cupy.testing.array import assert_allclose  # NOQA\nfrom cupy.testing.array import assert_array_almost_equal  # NOQA\nfrom cupy.testing.array import assert_array_almost_equal_nulp  # NOQA\nfrom cupy.testing.array import assert_array_equal  # NOQA\nfrom cupy.testing.array import assert_array_less  # NOQA\nfrom cupy.testing.array import assert_array_list_equal  # NOQA\nfrom cupy.testing.array import assert_array_max_ulp  # NOQA\nfrom cupy.testing.attr import gpu  # NOQA\nfrom cupy.testing.attr import multi_gpu  # NOQA\nfrom cupy.testing.attr import slow  # NOQA\nfrom cupy.testing.helper import assert_warns  # NOQA\nfrom cupy.testing.helper import empty  # NOQA\nfrom cupy.testing.helper import for_all_dtypes  # NOQA\nfrom cupy.testing.helper import for_all_dtypes_combination  # NOQA\nfrom cupy.testing.helper import for_CF_orders  # NOQA\nfrom cupy.testing.helper import for_complex_dtypes  # NOQA\nfrom cupy.testing.helper import for_dtypes  # NOQA\nfrom cupy.testing.helper import for_dtypes_combination  # NOQA\nfrom cupy.testing.helper import for_float_dtypes  # NOQA\nfrom cupy.testing.helper import for_int_dtypes  # NOQA\nfrom cupy.testing.helper import for_int_dtypes_combination  # NOQA\nfrom cupy.testing.helper import for_orders  # NOQA\nfrom cupy.testing.helper import for_signed_dtypes  # NOQA\nfrom cupy.testing.helper import for_signed_dtypes_combination  # NOQA\nfrom cupy.testing.helper import for_unsigned_dtypes  # NOQA\nfrom cupy.testing.helper import for_unsigned_dtypes_combination  # NOQA\nfrom cupy.testing.helper import numpy_cupy_allclose  # NOQA\nfrom cupy.testing.helper import numpy_cupy_array_almost_equal  # NOQA\nfrom cupy.testing.helper import numpy_cupy_array_almost_equal_nulp  # NOQA\nfrom cupy.testing.helper import numpy_cupy_array_equal  # NOQA\nfrom cupy.testing.helper import numpy_cupy_array_less  # NOQA\nfrom cupy.testing.helper import numpy_cupy_array_list_equal  # NOQA\nfrom cupy.testing.helper import numpy_cupy_array_max_ulp  # NOQA\nfrom cupy.testing.helper import numpy_cupy_equal  # NOQA\nfrom cupy.testing.helper import numpy_cupy_raises  # NOQA\nfrom cupy.testing.helper import numpy_satisfies  # NOQA\nfrom cupy.testing.helper import NumpyAliasBasicTestBase  # NOQA\nfrom cupy.testing.helper import NumpyAliasValuesTestBase  # NOQA\nfrom cupy.testing.helper import NumpyError  # NOQA\nfrom cupy.testing.helper import shaped_arange  # NOQA\nfrom cupy.testing.helper import shaped_random  # NOQA\nfrom cupy.testing.helper import shaped_reverse_arange  # NOQA\nfrom cupy.testing.helper import with_requires  # NOQA\nfrom cupy.testing.parameterized import from_pytest_parameterize  # NOQA\nfrom cupy.testing.parameterized import parameterize  # NOQA\nfrom cupy.testing.parameterized import parameterize_pytest  # NOQA\nfrom cupy.testing.parameterized import product  # NOQA\nfrom cupy.testing.parameterized import product_dict  # NOQA\nfrom cupy.testing.random import fix_random  # NOQA\nfrom cupy.testing.random import generate_seed  # NOQA\n'
cupy/testing/_bundle.py,0,"b""import collections\nimport inspect\nimport sys\n\n\n# A tuple that represents a test case.\n# For bare (non-generated) test cases, [1] and [2] are None.\n# [0] Test case class\n# [1] Module name in whicn the class is defined\n# [2] Class name\n_TestCaseTuple = collections.namedtuple(\n    '_TestCaseTuple', ('klass', 'module_name', 'class_name'))\n\n\nclass _ParameterizedTestCaseBundle(object):\n    def __init__(self, cases):\n        # cases is a list of _TestCaseTuple's\n        assert isinstance(cases, list)\n        assert all(isinstance(tup, _TestCaseTuple) for tup in cases)\n\n        self.cases = cases\n\n\ndef make_decorator(test_case_generator):\n    # `test_case_generator` is a callable that receives the source test class\n    # (typically a subclass of unittest.TestCase) and returns an iterable of\n    # generated test cases.\n    # Each element of the iterable is a 3-element tuple:\n    # [0] Generated class name\n    # [1] Dict of members\n    # [2] Method generator\n    # The method generator is also a callable that receives an original test\n    # method and returns a new test method.\n\n    def f(cases):\n        if isinstance(cases, _ParameterizedTestCaseBundle):\n            # The input is a parameterized test case.\n            cases = cases.cases\n        else:\n            # Input is a bare test case, i.e. not one generated from another\n            # parameterize.\n            cases = [_TestCaseTuple(cases, None, None)]\n\n        generated_cases = []\n        for klass, mod_name, cls_name in cases:\n            if mod_name is not None:\n                # The input is a parameterized test case.\n                # Remove it from its module.\n                delattr(sys.modules[mod_name], cls_name)\n            else:\n                # The input is a bare test case\n                mod_name = klass.__module__\n\n            # Generate parameterized test cases out of the input test case.\n            c = _generate_test_cases(mod_name, klass, test_case_generator)\n            generated_cases += c\n\n        # Return the bundle of generated cases to allow repeated application of\n        # parameterize decorators.\n        return _ParameterizedTestCaseBundle(generated_cases)\n    return f\n\n\ndef _generate_case(base, module, cls_name, mb, method_generator):\n    # Returns a _TestCaseTuple.\n\n    members = mb.copy()\n    # ismethod for Python 2 and isfunction for Python 3\n    base_methods = inspect.getmembers(\n        base, predicate=lambda m: inspect.ismethod(m) or inspect.isfunction(m))\n    for name, value in base_methods:\n        if not name.startswith('test_'):\n            continue\n        value = method_generator(value)\n        # If the return value of method_generator is None, None is assigned\n        # as the value of the test method and it is ignored by pytest.\n        members[name] = value\n\n    cls = type(cls_name, (base,), members)\n\n    # Add new test class to module\n    setattr(module, cls_name, cls)\n\n    return _TestCaseTuple(cls, module.__name__, cls_name)\n\n\ndef _generate_test_cases(module_name, base_class, test_case_generator):\n    # Returns a list of _TestCaseTuple's holding generated test cases.\n    module = sys.modules[module_name]\n\n    generated_cases = []\n    for cls_name, members, method_generator in (\n            test_case_generator(base_class)):\n        c = _generate_case(\n            base_class, module, cls_name, members, method_generator)\n        generated_cases.append(c)\n\n    return generated_cases\n"""
cupy/testing/array.py,0,"b'import numpy.testing\n\nimport cupy\n\n\n# NumPy-like assertion functions that accept both NumPy and CuPy arrays\n\ndef assert_allclose(actual, desired, rtol=1e-7, atol=0, err_msg=\'\',\n                    verbose=True):\n    """"""Raises an AssertionError if objects are not equal up to desired tolerance.\n\n    Args:\n         actual(numpy.ndarray or cupy.ndarray): The actual object to check.\n         desired(numpy.ndarray or cupy.ndarray): The desired, expected object.\n         rtol(float): Relative tolerance.\n         atol(float): Absolute tolerance.\n         err_msg(str): The error message to be printed in case of failure.\n         verbose(bool): If ``True``, the conflicting\n             values are appended to the error message.\n\n    .. seealso:: :func:`numpy.testing.assert_allclose`\n\n    """"""  # NOQA\n    numpy.testing.assert_allclose(\n        cupy.asnumpy(actual), cupy.asnumpy(desired),\n        rtol=rtol, atol=atol, err_msg=err_msg, verbose=verbose)\n\n\ndef assert_array_almost_equal(x, y, decimal=6, err_msg=\'\', verbose=True):\n    """"""Raises an AssertionError if objects are not equal up to desired precision.\n\n    Args:\n         x(numpy.ndarray or cupy.ndarray): The actual object to check.\n         y(numpy.ndarray or cupy.ndarray): The desired, expected object.\n         decimal(int): Desired precision.\n         err_msg(str): The error message to be printed in case of failure.\n         verbose(bool): If ``True``, the conflicting\n             values are appended to the error message.\n\n    .. seealso:: :func:`numpy.testing.assert_array_almost_equal`\n    """"""  # NOQA\n    numpy.testing.assert_array_almost_equal(\n        cupy.asnumpy(x), cupy.asnumpy(y), decimal=decimal,\n        err_msg=err_msg, verbose=verbose)\n\n\ndef assert_array_almost_equal_nulp(x, y, nulp=1):\n    """"""Compare two arrays relatively to their spacing.\n\n    Args:\n         x(numpy.ndarray or cupy.ndarray): The actual object to check.\n         y(numpy.ndarray or cupy.ndarray): The desired, expected object.\n         nulp(int): The maximum number of unit in the last place for tolerance.\n\n    .. seealso:: :func:`numpy.testing.assert_array_almost_equal_nulp`\n    """"""\n    numpy.testing.assert_array_almost_equal_nulp(\n        cupy.asnumpy(x), cupy.asnumpy(y), nulp=nulp)\n\n\ndef assert_array_max_ulp(a, b, maxulp=1, dtype=None):\n    """"""Check that all items of arrays differ in at most N Units in the Last Place.\n\n    Args:\n         a(numpy.ndarray or cupy.ndarray): The actual object to check.\n         b(numpy.ndarray or cupy.ndarray): The desired, expected object.\n         maxulp(int): The maximum number of units in the last place\n             that elements of ``a`` and ``b`` can differ.\n         dtype(numpy.dtype): Data-type to convert ``a`` and ``b`` to if given.\n\n    .. seealso:: :func:`numpy.testing.assert_array_max_ulp`\n    """"""  # NOQA\n    numpy.testing.assert_array_max_ulp(\n        cupy.asnumpy(a), cupy.asnumpy(b), maxulp=maxulp, dtype=dtype)\n\n\ndef assert_array_equal(x, y, err_msg=\'\', verbose=True, strides_check=False):\n    """"""Raises an AssertionError if two array_like objects are not equal.\n\n    Args:\n         x(numpy.ndarray or cupy.ndarray): The actual object to check.\n         y(numpy.ndarray or cupy.ndarray): The desired, expected object.\n         strides_check(bool): If ``True``, consistency of strides is also\n             checked.\n         err_msg(str): The error message to be printed in case of failure.\n         verbose(bool): If ``True``, the conflicting values\n             are appended to the error message.\n\n    .. seealso:: :func:`numpy.testing.assert_array_equal`\n    """"""\n    numpy.testing.assert_array_equal(\n        cupy.asnumpy(x), cupy.asnumpy(y), err_msg=err_msg,\n        verbose=verbose)\n\n    if strides_check:\n        if x.strides != y.strides:\n            msg = [\'Strides are not equal:\']\n            if err_msg:\n                msg = [msg[0] + \' \' + err_msg]\n            if verbose:\n                msg.append(\' x: {}\'.format(x.strides))\n                msg.append(\' y: {}\'.format(y.strides))\n            raise AssertionError(\'\\n\'.join(msg))\n\n\ndef assert_array_list_equal(xlist, ylist, err_msg=\'\', verbose=True):\n    """"""Compares lists of arrays pairwise with ``assert_array_equal``.\n\n    Args:\n         x(array_like): Array of the actual objects.\n         y(array_like): Array of the desired, expected objects.\n         err_msg(str): The error message to be printed in case of failure.\n         verbose(bool): If ``True``, the conflicting values\n             are appended to the error message.\n\n    Each element of ``x`` and ``y`` must be either :class:`numpy.ndarray`\n    or :class:`cupy.ndarray`. ``x`` and ``y`` must have same length.\n    Otherwise, this function raises ``AssertionError``.\n    It compares elements of ``x`` and ``y`` pairwise\n    with :func:`assert_array_equal` and raises error if at least one\n    pair is not equal.\n\n    .. seealso:: :func:`numpy.testing.assert_array_equal`\n    """"""\n    x_type = type(xlist)\n    y_type = type(ylist)\n    if x_type is not y_type:\n        raise AssertionError(\n            \'Matching types of list or tuple are expected, \'\n            \'but were different types \'\n            \'(xlist:{} ylist:{})\'.format(x_type, y_type))\n    if x_type not in (list, tuple):\n        raise AssertionError(\n            \'List or tuple is expected, but was {}\'.format(x_type))\n    if len(xlist) != len(ylist):\n        raise AssertionError(\'List size is different\')\n    for x, y in zip(xlist, ylist):\n        numpy.testing.assert_array_equal(\n            cupy.asnumpy(x), cupy.asnumpy(y), err_msg=err_msg,\n            verbose=verbose)\n\n\ndef assert_array_less(x, y, err_msg=\'\', verbose=True):\n    """"""Raises an AssertionError if array_like objects are not ordered by less than.\n\n    Args:\n         x(numpy.ndarray or cupy.ndarray): The smaller object to check.\n         y(numpy.ndarray or cupy.ndarray): The larger object to compare.\n         err_msg(str): The error message to be printed in case of failure.\n         verbose(bool): If ``True``, the conflicting values\n             are appended to the error message.\n\n    .. seealso:: :func:`numpy.testing.assert_array_less`\n    """"""  # NOQA\n    numpy.testing.assert_array_less(\n        cupy.asnumpy(x), cupy.asnumpy(y), err_msg=err_msg,\n        verbose=verbose)\n'"
cupy/testing/attr.py,0,"b'import os\nimport unittest\n\n\ntry:\n    import pytest\n    _error = None\nexcept ImportError as e:\n    _error = e\n\n\ndef is_available():\n    return _error is None\n\n\ndef check_available():\n    if _error is not None:\n        raise RuntimeError(\'\'\'\\\n{} is not available.\n\nReason: {}: {}\'\'\'.format(__name__, type(_error).__name__, _error))\n\n\ndef get_error():\n    return _error\n\n\nif _error is None:\n    _gpu_limit = int(os.getenv(\'CUPY_TEST_GPU_LIMIT\', \'-1\'))\n\n    def cudnn(*args, **kwargs):\n        return pytest.mark.cudnn(*args, **kwargs)\n\n    def slow(*args, **kwargs):\n        return pytest.mark.slow(*args, **kwargs)\n\nelse:\n    def _dummy_callable(*args, **kwargs):\n        check_available()\n        assert False  # Not reachable\n\n    cudnn = _dummy_callable\n    slow = _dummy_callable\n\n\ndef multi_gpu(gpu_num):\n    """"""Decorator to indicate number of GPUs required to run the test.\n\n    Tests can be annotated with this decorator (e.g., ``@multi_gpu(2)``) to\n    declare number of GPUs required to run. When running tests, if\n    ``CUPY_TEST_GPU_LIMIT`` environment variable is set to value greater\n    than or equals to 0, test cases that require GPUs more than the limit will\n    be skipped.\n    """"""\n\n    check_available()\n    return unittest.skipIf(\n        0 <= _gpu_limit < gpu_num,\n        reason=\'{} GPUs required\'.format(gpu_num))\n\n\ndef gpu(f):\n    """"""Decorator to indicate that GPU is required to run the test.\n\n    Tests can be annotated with this decorator (e.g., ``@gpu``) to\n    declare that one GPU is required to run.\n    """"""\n\n    check_available()\n    return multi_gpu(1)(pytest.mark.gpu(f))\n'"
cupy/testing/condition.py,0,"b'import functools\nimport os\nimport unittest\n\n\nclass QuietTestRunner(object):\n\n    def run(self, suite):\n        result = unittest.TestResult()\n        suite(result)\n        return result\n\n\ndef repeat_with_success_at_least(times, min_success):\n    """"""Decorator for multiple trial of the test case.\n\n    The decorated test case is launched multiple times.\n    The case is judged as passed at least specified number of trials.\n    If the number of successful trials exceeds `min_success`,\n    the remaining trials are skipped.\n\n    Args:\n        times(int): The number of trials.\n        min_success(int): Threshold that the decorated test\n            case is regarded as passed.\n\n    """"""\n\n    assert times >= min_success\n\n    def _repeat_with_success_at_least(f):\n        @functools.wraps(f)\n        def wrapper(*args, **kwargs):\n            assert len(args) > 0\n            instance = args[0]\n            assert isinstance(instance, unittest.TestCase)\n            success_counter = 0\n            failure_counter = 0\n            results = []\n\n            def fail():\n                msg = \'\\nFail: {0}, Success: {1}\'.format(\n                    failure_counter, success_counter)\n                if len(results) > 0:\n                    first = results[0]\n                    errs = first.failures + first.errors\n                    if len(errs) > 0:\n                        err_msg = \'\\n\'.join(fail[1] for fail in errs)\n                        msg += \'\\n\\nThe first error message:\\n\' + err_msg\n                instance.fail(msg)\n\n            for _ in range(times):\n                suite = unittest.TestSuite()\n                # Create new instance to call the setup and the teardown only\n                # once.\n                ins = type(instance)(instance._testMethodName)\n                suite.addTest(\n                    unittest.FunctionTestCase(\n                        lambda: f(ins, *args[1:], **kwargs),\n                        setUp=ins.setUp,\n                        tearDown=ins.tearDown))\n\n                result = QuietTestRunner().run(suite)\n                if len(result.skipped) == 1:\n                    # ""Skipped"" is a special case of ""Successful"".\n                    # When the test has been skipped, immedeately quit the\n                    # test regardleess of `times` and `min_success` by raising\n                    # SkipTest exception using the original reason.\n                    instance.skipTest(result.skipped[0][1])\n                elif result.wasSuccessful():\n                    success_counter += 1\n                else:\n                    results.append(result)\n                    failure_counter += 1\n                if success_counter >= min_success:\n                    instance.assertTrue(True)\n                    return\n                if failure_counter > times - min_success:\n                    fail()\n                    return\n            fail()\n        return wrapper\n    return _repeat_with_success_at_least\n\n\ndef repeat(times, intensive_times=None):\n    """"""Decorator that imposes the test to be successful in a row.\n\n    Decorated test case is launched multiple times.\n    The case is regarded as passed only if it is successful\n    specified times in a row.\n\n    .. note::\n        In current implementation, this decorator grasps the\n        failure information of each trial.\n\n    Args:\n        times(int): The number of trials in casual test.\n        intensive_times(int or None): The number of trials in more intensive\n            test. If ``None``, the same number as `times` is used.\n    """"""\n    if intensive_times is None:\n        return repeat_with_success_at_least(times, times)\n\n    casual_test = bool(int(os.environ.get(\'CUPY_TEST_CASUAL\', \'0\')))\n    times_ = times if casual_test else intensive_times\n    return repeat_with_success_at_least(times_, times_)\n\n\ndef retry(times):\n    """"""Decorator that imposes the test to be successful at least once.\n\n    Decorated test case is launched multiple times.\n    The case is regarded as passed if it is successful\n    at least once.\n\n    .. note::\n        In current implementation, this decorator grasps the\n        failure information of each trial.\n\n    Args:\n        times(int): The number of trials.\n    """"""\n    return repeat_with_success_at_least(times, 1)\n'"
cupy/testing/helper.py,0,"b'import contextlib\nimport functools\nimport inspect\nimport os\nimport random\nimport traceback\nimport unittest\nimport warnings\n\nimport numpy\n\nimport cupy\nfrom cupy.core import internal\nfrom cupy.testing import array\nfrom cupy.testing import parameterized\nimport cupyx\nimport cupyx.scipy.sparse\n\n\ndef _call_func(self, impl, args, kw):\n    try:\n        result = impl(self, *args, **kw)\n        error = None\n        tb = None\n    except Exception as e:\n        tb = e.__traceback__\n        if tb.tb_next is None:\n            # failed before impl is called, e.g. invalid kw\n            raise e\n        result = None\n        error = e\n\n    return result, error, tb\n\n\ndef _call_func_cupy(self, impl, args, kw, name, sp_name, scipy_name):\n    assert isinstance(name, str)\n    assert sp_name is None or isinstance(sp_name, str)\n    assert scipy_name is None or isinstance(scipy_name, str)\n    kw = kw.copy()\n\n    # Run cupy\n    if sp_name:\n        kw[sp_name] = cupyx.scipy.sparse\n    if scipy_name:\n        kw[scipy_name] = cupyx.scipy\n    kw[name] = cupy\n    result, error, tb = _call_func(self, impl, args, kw)\n    return result, error, tb\n\n\ndef _call_func_numpy(self, impl, args, kw, name, sp_name, scipy_name):\n    assert isinstance(name, str)\n    assert sp_name is None or isinstance(sp_name, str)\n    assert scipy_name is None or isinstance(scipy_name, str)\n    kw = kw.copy()\n\n    # Run numpy\n    kw[name] = numpy\n    if sp_name:\n        import scipy.sparse\n        kw[sp_name] = scipy.sparse\n    if scipy_name:\n        import scipy\n        kw[scipy_name] = scipy\n    result, error, tb = _call_func(self, impl, args, kw)\n    return result, error, tb\n\n\ndef _call_func_numpy_cupy(self, impl, args, kw, name, sp_name, scipy_name):\n    # Run cupy\n    cupy_result, cupy_error, cupy_tb = _call_func_cupy(\n        self, impl, args, kw, name, sp_name, scipy_name)\n\n    # Run numpy\n    numpy_result, numpy_error, numpy_tb = _call_func_numpy(\n        self, impl, args, kw, name, sp_name, scipy_name)\n\n    return (\n        cupy_result, cupy_error, cupy_tb,\n        numpy_result, numpy_error, numpy_tb)\n\n\n_numpy_errors = [\n    AttributeError, Exception, IndexError, TypeError, ValueError,\n    NotImplementedError, DeprecationWarning,\n    numpy.AxisError, numpy.linalg.LinAlgError,\n]\n\n\ndef _check_numpy_cupy_error_compatible(cupy_error, numpy_error):\n    """"""Checks if try/except blocks are equivalent up to public error classes\n    """"""\n\n    return all([isinstance(cupy_error, err) == isinstance(numpy_error, err)\n                for err in _numpy_errors])\n\n\ndef _fail_test_with_unexpected_errors(\n        testcase, msg_format, cupy_error, cupy_tb, numpy_error, numpy_tb):\n    # Fails the test due to unexpected errors raised from the test.\n    # msg_format may include format placeholders:\n    # \'{cupy_error}\' \'{cupy_tb}\' \'{numpy_error}\' \'{numpy_tb}\'\n\n    msg = msg_format.format(\n        cupy_error=\'\'.join(str(cupy_error)),\n        cupy_tb=\'\'.join(traceback.format_tb(cupy_tb)),\n        numpy_error=\'\'.join(str(numpy_error)),\n        numpy_tb=\'\'.join(traceback.format_tb(numpy_tb)))\n\n    # Fail the test with the traceback of the error (for pytest --pdb)\n    try:\n        testcase.fail(msg)\n    except AssertionError as e:\n        raise e.with_traceback(cupy_tb or numpy_tb)\n    assert False  # never reach\n\n\ndef _check_cupy_numpy_error(self, cupy_error, cupy_tb, numpy_error,\n                            numpy_tb, accept_error=False):\n    # Skip the test if both raised SkipTest.\n    if (isinstance(cupy_error, unittest.SkipTest)\n            and isinstance(numpy_error, unittest.SkipTest)):\n        if cupy_error.args != numpy_error.args:\n            raise AssertionError(\n                \'Both numpy and cupy were skipped but with different causes.\')\n        raise numpy_error  # reraise SkipTest\n\n    # For backward compatibility\n    if accept_error is True:\n        accept_error = Exception\n    elif not accept_error:\n        accept_error = ()\n    # TODO(oktua): expected_regexp like numpy.testing.assert_raises_regex\n    if cupy_error is None and numpy_error is None:\n        self.fail(\'Both cupy and numpy are expected to raise errors, but not\')\n    elif cupy_error is None:\n        _fail_test_with_unexpected_errors(\n            self,\n            \'Only numpy raises error\\n\\n{numpy_tb}{numpy_error}\',\n            None, None, numpy_error, numpy_tb)\n    elif numpy_error is None:\n        _fail_test_with_unexpected_errors(\n            self,\n            \'Only cupy raises error\\n\\n{cupy_tb}{cupy_error}\',\n            cupy_error, cupy_tb, None, None)\n\n    elif not _check_numpy_cupy_error_compatible(cupy_error, numpy_error):\n        _fail_test_with_unexpected_errors(\n            self,\n            \'\'\'Different types of errors occurred\n\ncupy\n{cupy_tb}{cupy_error}\n\nnumpy\n{numpy_tb}{numpy_error}\n\'\'\',\n            cupy_error, cupy_tb, numpy_error, numpy_tb)\n\n    elif not (isinstance(cupy_error, accept_error)\n              and isinstance(numpy_error, accept_error)):\n        _fail_test_with_unexpected_errors(\n            self,\n            \'\'\'Both cupy and numpy raise exceptions\n\ncupy\n{cupy_tb}{cupy_error}\n\nnumpy\n{numpy_tb}{numpy_error}\n\'\'\',\n            cupy_error, cupy_tb, numpy_error, numpy_tb)\n\n\ndef _make_positive_mask(self, impl, args, kw, name, sp_name, scipy_name):\n    # Returns a mask of output arrays that indicates valid elements for\n    # comparison. See the comment at the caller.\n    ks = [k for k, v in kw.items() if v in _unsigned_dtypes]\n    for k in ks:\n        kw[k] = numpy.intp\n    result, error, tb = _call_func_cupy(\n        self, impl, args, kw, name, sp_name, scipy_name)\n    assert error is None\n    return cupy.asnumpy(result) >= 0\n\n\ndef _contains_signed_and_unsigned(kw):\n    vs = set(kw.values())\n    return any(d in vs for d in _unsigned_dtypes) and \\\n        any(d in vs for d in _float_dtypes + _signed_dtypes)\n\n\ndef _make_decorator(check_func, name, type_check, accept_error, sp_name=None,\n                    scipy_name=None):\n    assert isinstance(name, str)\n    assert sp_name is None or isinstance(sp_name, str)\n    assert scipy_name is None or isinstance(scipy_name, str)\n\n    def decorator(impl):\n        @functools.wraps(impl)\n        def test_func(self, *args, **kw):\n            # Run cupy and numpy\n            (\n                cupy_result, cupy_error, cupy_tb,\n                numpy_result, numpy_error, numpy_tb) = (\n                    _call_func_numpy_cupy(\n                        self, impl, args, kw, name, sp_name, scipy_name))\n            assert cupy_result is not None or cupy_error is not None\n            assert numpy_result is not None or numpy_error is not None\n\n            # Check errors raised\n            if cupy_error or numpy_error:\n                _check_cupy_numpy_error(self, cupy_error, cupy_tb,\n                                        numpy_error, numpy_tb,\n                                        accept_error=accept_error)\n                return\n\n            # Check returned arrays\n\n            if not isinstance(cupy_result, (tuple, list)):\n                cupy_result = cupy_result,\n            if not isinstance(numpy_result, (tuple, list)):\n                numpy_result = numpy_result,\n\n            assert len(cupy_result) == len(numpy_result)\n\n            if type_check:\n                for cupy_r, numpy_r in zip(cupy_result, numpy_result):\n                    assert cupy_r.dtype == numpy_r.dtype\n\n            for cupy_r, numpy_r in zip(cupy_result, numpy_result):\n                assert cupy_r.shape == numpy_r.shape\n\n                # Behavior of assigning a negative value to an unsigned integer\n                # variable is undefined.\n                # nVidia GPUs and Intel CPUs behave differently.\n                # To avoid this difference, we need to ignore dimensions whose\n                # values are negative.\n                skip = False\n                if (_contains_signed_and_unsigned(kw)\n                        and cupy_r.dtype in _unsigned_dtypes):\n                    mask = _make_positive_mask(\n                        self, impl, args, kw, name, sp_name, scipy_name)\n                    if cupy_r.shape == ():\n                        skip = (mask == 0).all()\n                    else:\n                        cupy_r = cupy.asnumpy(cupy_r[mask])\n                        numpy_r = cupy.asnumpy(numpy_r[mask])\n\n                if not skip:\n                    check_func(cupy_r, numpy_r)\n        return test_func\n    return decorator\n\n\ndef numpy_cupy_allclose(rtol=1e-7, atol=0, err_msg=\'\', verbose=True,\n                        name=\'xp\', type_check=True, accept_error=False,\n                        sp_name=None, scipy_name=None, contiguous_check=True):\n    """"""Decorator that checks NumPy results and CuPy ones are close.\n\n    Args:\n         rtol(float): Relative tolerance.\n         atol(float): Absolute tolerance.\n         err_msg(str): The error message to be printed in case of failure.\n         verbose(bool): If ``True``, the conflicting values are\n             appended to the error message.\n         name(str): Argument name whose value is either\n             ``numpy`` or ``cupy`` module.\n         type_check(bool): If ``True``, consistency of dtype is also checked.\n         accept_error(bool, Exception or tuple of Exception): Specify\n             acceptable errors. When both NumPy test and CuPy test raises the\n             same type of errors, and the type of the errors is specified with\n             this argument, the errors are ignored and not raised.\n             If it is ``True`` all error types are acceptable.\n             If it is ``False`` no error is acceptable.\n         sp_name(str or None): Argument name whose value is either\n             ``scipy.sparse`` or ``cupyx.scipy.sparse`` module. If ``None``, no\n             argument is given for the modules.\n         scipy_name(str or None): Argument name whose value is either ``scipy``\n             or ``cupyx.scipy`` module. If ``None``, no argument is given for\n             the modules.\n         contiguous_check(bool): If ``True``, consistency of contiguity is\n             also checked.\n\n    Decorated test fixture is required to return the arrays whose values are\n    close between ``numpy`` case and ``cupy`` case.\n    For example, this test case checks ``numpy.zeros`` and ``cupy.zeros``\n    should return same value.\n\n    >>> import unittest\n    >>> from cupy import testing\n    >>> @testing.gpu\n    ... class TestFoo(unittest.TestCase):\n    ...\n    ...     @testing.numpy_cupy_allclose()\n    ...     def test_foo(self, xp):\n    ...         # ...\n    ...         # Prepare data with xp\n    ...         # ...\n    ...\n    ...         xp_result = xp.zeros(10)\n    ...         return xp_result\n\n    .. seealso:: :func:`cupy.testing.assert_allclose`\n    """"""\n    def check_func(c, n):\n        c_array = c\n        n_array = n\n        if sp_name is not None:\n            import scipy.sparse\n            if cupyx.scipy.sparse.issparse(c):\n                c_array = c.A\n            if scipy.sparse.issparse(n):\n                n_array = n.A\n        array.assert_allclose(c_array, n_array, rtol, atol, err_msg, verbose)\n        if contiguous_check and isinstance(n, numpy.ndarray):\n            if n.flags.c_contiguous and not c.flags.c_contiguous:\n                raise AssertionError(\n                    \'The state of c_contiguous flag is false. \'\n                    \'(cupy_result:{} numpy_result:{})\'.format(\n                        c.flags.c_contiguous, n.flags.c_contiguous))\n            if n.flags.f_contiguous and not c.flags.f_contiguous:\n                raise AssertionError(\n                    \'The state of f_contiguous flag is false. \'\n                    \'(cupy_result:{} numpy_result:{})\'.format(\n                        c.flags.f_contiguous, n.flags.f_contiguous))\n    return _make_decorator(check_func, name, type_check, accept_error, sp_name,\n                           scipy_name)\n\n\ndef numpy_cupy_array_almost_equal(decimal=6, err_msg=\'\', verbose=True,\n                                  name=\'xp\', type_check=True,\n                                  accept_error=False, sp_name=None,\n                                  scipy_name=None):\n    """"""Decorator that checks NumPy results and CuPy ones are almost equal.\n\n    Args:\n         decimal(int): Desired precision.\n         err_msg(str): The error message to be printed in case of failure.\n         verbose(bool): If ``True``, the conflicting values\n             are appended to the error message.\n         name(str): Argument name whose value is either\n             ``numpy`` or ``cupy`` module.\n         type_check(bool): If ``True``, consistency of dtype is also checked.\n         accept_error(bool, Exception or tuple of Exception): Specify\n             acceptable errors. When both NumPy test and CuPy test raises the\n             same type of errors, and the type of the errors is specified with\n             this argument, the errors are ignored and not raised.\n             If it is ``True`` all error types are acceptable.\n             If it is ``False`` no error is acceptable.\n         sp_name(str or None): Argument name whose value is either\n             ``scipy.sparse`` or ``cupyx.scipy.sparse`` module. If ``None``, no\n             argument is given for the modules.\n         scipy_name(str or None): Argument name whose value is either ``scipy``\n             or ``cupyx.scipy`` module. If ``None``, no argument is given for\n             the modules.\n\n    Decorated test fixture is required to return the same arrays\n    in the sense of :func:`cupy.testing.assert_array_almost_equal`\n    (except the type of array module) even if ``xp`` is ``numpy`` or ``cupy``.\n\n    .. seealso:: :func:`cupy.testing.assert_array_almost_equal`\n    """"""\n    def check_func(x, y):\n        array.assert_array_almost_equal(\n            x, y, decimal, err_msg, verbose)\n    return _make_decorator(check_func, name, type_check, accept_error, sp_name,\n                           scipy_name)\n\n\ndef numpy_cupy_array_almost_equal_nulp(nulp=1, name=\'xp\', type_check=True,\n                                       accept_error=False, sp_name=None,\n                                       scipy_name=None):\n    """"""Decorator that checks results of NumPy and CuPy are equal w.r.t. spacing.\n\n    Args:\n         nulp(int): The maximum number of unit in the last place for tolerance.\n         name(str): Argument name whose value is either\n             ``numpy`` or ``cupy`` module.\n         type_check(bool): If ``True``, consistency of dtype is also checked.\n         accept_error(bool, Exception or tuple of Exception): Specify\n             acceptable errors. When both NumPy test and CuPy test raises the\n             same type of errors, and the type of the errors is specified with\n             this argument, the errors are ignored and not raised.\n             If it is ``True``, all error types are acceptable.\n             If it is ``False``, no error is acceptable.\n         sp_name(str or None): Argument name whose value is either\n             ``scipy.sparse`` or ``cupyx.scipy.sparse`` module. If ``None``, no\n             argument is given for the modules.\n         scipy_name(str or None): Argument name whose value is either ``scipy``\n             or ``cupyx.scipy`` module. If ``None``, no argument is given for\n             the modules.\n\n    Decorated test fixture is required to return the same arrays\n    in the sense of :func:`cupy.testing.assert_array_almost_equal_nulp`\n    (except the type of array module) even if ``xp`` is ``numpy`` or ``cupy``.\n\n    .. seealso:: :func:`cupy.testing.assert_array_almost_equal_nulp`\n    """"""  # NOQA\n    def check_func(x, y):\n        array.assert_array_almost_equal_nulp(x, y, nulp)\n    return _make_decorator(check_func, name, type_check, accept_error, sp_name,\n                           scipy_name=None)\n\n\ndef numpy_cupy_array_max_ulp(maxulp=1, dtype=None, name=\'xp\', type_check=True,\n                             accept_error=False, sp_name=None,\n                             scipy_name=None):\n    """"""Decorator that checks results of NumPy and CuPy ones are equal w.r.t. ulp.\n\n    Args:\n         maxulp(int): The maximum number of units in the last place\n             that elements of resulting two arrays can differ.\n         dtype(numpy.dtype): Data-type to convert the resulting\n             two array to if given.\n         name(str): Argument name whose value is either\n             ``numpy`` or ``cupy`` module.\n         type_check(bool): If ``True``, consistency of dtype is also checked.\n         accept_error(bool, Exception or tuple of Exception): Specify\n             acceptable errors. When both NumPy test and CuPy test raises the\n             same type of errors, and the type of the errors is specified with\n             this argument, the errors are ignored and not raised.\n             If it is ``True`` all error types are acceptable.\n             If it is ``False`` no error is acceptable.\n         sp_name(str or None): Argument name whose value is either\n             ``scipy.sparse`` or ``cupyx.scipy.sparse`` module. If ``None``, no\n             argument is given for the modules.\n         scipy_name(str or None): Argument name whose value is either ``scipy``\n             or ``cupyx.scipy`` module. If ``None``, no argument is given for\n             the modules.\n\n    Decorated test fixture is required to return the same arrays\n    in the sense of :func:`assert_array_max_ulp`\n    (except the type of array module) even if ``xp`` is ``numpy`` or ``cupy``.\n\n    .. seealso:: :func:`cupy.testing.assert_array_max_ulp`\n\n    """"""  # NOQA\n    def check_func(x, y):\n        array.assert_array_max_ulp(x, y, maxulp, dtype)\n    return _make_decorator(check_func, name, type_check, accept_error, sp_name,\n                           scipy_name)\n\n\ndef numpy_cupy_array_equal(err_msg=\'\', verbose=True, name=\'xp\',\n                           type_check=True, accept_error=False, sp_name=None,\n                           scipy_name=None, strides_check=False):\n    """"""Decorator that checks NumPy results and CuPy ones are equal.\n\n    Args:\n         err_msg(str): The error message to be printed in case of failure.\n         verbose(bool): If ``True``, the conflicting values are\n             appended to the error message.\n         name(str): Argument name whose value is either\n             ``numpy`` or ``cupy`` module.\n         type_check(bool): If ``True``, consistency of dtype is also checked.\n         accept_error(bool, Exception or tuple of Exception): Specify\n             acceptable errors. When both NumPy test and CuPy test raises the\n             same type of errors, and the type of the errors is specified with\n             this argument, the errors are ignored and not raised.\n             If it is ``True`` all error types are acceptable.\n             If it is ``False`` no error is acceptable.\n         sp_name(str or None): Argument name whose value is either\n             ``scipy.sparse`` or ``cupyx.scipy.sparse`` module. If ``None``, no\n             argument is given for the modules.\n         scipy_name(str or None): Argument name whose value is either ``scipy``\n             or ``cupyx.scipy`` module. If ``None``, no argument is given for\n             the modules.\n         strides_check(bool): If ``True``, consistency of strides is also\n             checked.\n\n    Decorated test fixture is required to return the same arrays\n    in the sense of :func:`numpy_cupy_array_equal`\n    (except the type of array module) even if ``xp`` is ``numpy`` or ``cupy``.\n\n    .. seealso:: :func:`cupy.testing.assert_array_equal`\n    """"""\n    def check_func(x, y):\n        if sp_name is not None:\n            import scipy.sparse\n            if cupyx.scipy.sparse.issparse(x):\n                x = x.A\n            if scipy.sparse.issparse(y):\n                y = y.A\n\n        array.assert_array_equal(x, y, err_msg, verbose, strides_check)\n\n    return _make_decorator(check_func, name, type_check, accept_error, sp_name,\n                           scipy_name)\n\n\ndef numpy_cupy_array_list_equal(\n        err_msg=\'\', verbose=True, name=\'xp\', sp_name=None, scipy_name=None):\n    """"""Decorator that checks the resulting lists of NumPy and CuPy\'s one are equal.\n\n    Args:\n         err_msg(str): The error message to be printed in case of failure.\n         verbose(bool): If ``True``, the conflicting values are appended\n             to the error message.\n         name(str): Argument name whose value is either\n             ``numpy`` or ``cupy`` module.\n         sp_name(str or None): Argument name whose value is either\n             ``scipy.sparse`` or ``cupyx.scipy.sparse`` module. If ``None``, no\n             argument is given for the modules.\n         scipy_name(str or None): Argument name whose value is either ``scipy``\n             or ``cupyx.scipy`` module. If ``None``, no argument is given for\n             the modules.\n\n    Decorated test fixture is required to return the same list of arrays\n    (except the type of array module) even if ``xp`` is ``numpy`` or ``cupy``.\n\n    .. seealso:: :func:`cupy.testing.assert_array_list_equal`\n    """"""  # NOQA\n    def check_func(x, y):\n        array.assert_array_equal(x, y, err_msg, verbose)\n    return _make_decorator(check_func, name, False, False, sp_name, scipy_name)\n\n\ndef numpy_cupy_array_less(err_msg=\'\', verbose=True, name=\'xp\',\n                          type_check=True, accept_error=False, sp_name=None,\n                          scipy_name=None):\n    """"""Decorator that checks the CuPy result is less than NumPy result.\n\n    Args:\n         err_msg(str): The error message to be printed in case of failure.\n         verbose(bool): If ``True``, the conflicting values are\n             appended to the error message.\n         name(str): Argument name whose value is either\n             ``numpy`` or ``cupy`` module.\n         type_check(bool): If ``True``, consistency of dtype is also checked.\n         accept_error(bool, Exception or tuple of Exception): Specify\n             acceptable errors. When both NumPy test and CuPy test raises the\n             same type of errors, and the type of the errors is specified with\n             this argument, the errors are ignored and not raised.\n             If it is ``True`` all error types are acceptable.\n             If it is ``False`` no error is acceptable.\n         sp_name(str or None): Argument name whose value is either\n             ``scipy.sparse`` or ``cupyx.scipy.sparse`` module. If ``None``, no\n             argument is given for the modules.\n         scipy_name(str or None): Argument name whose value is either ``scipy``\n             or ``cupyx.scipy`` module. If ``None``, no argument is given for\n             the modules.\n\n    Decorated test fixture is required to return the smaller array\n    when ``xp`` is ``cupy`` than the one when ``xp`` is ``numpy``.\n\n    .. seealso:: :func:`cupy.testing.assert_array_less`\n    """"""\n    def check_func(x, y):\n        array.assert_array_less(x, y, err_msg, verbose)\n    return _make_decorator(check_func, name, type_check, accept_error, sp_name,\n                           scipy_name)\n\n\ndef numpy_cupy_equal(name=\'xp\', sp_name=None, scipy_name=None):\n    """"""Decorator that checks NumPy results are equal to CuPy ones.\n\n    Args:\n         name(str): Argument name whose value is either\n             ``numpy`` or ``cupy`` module.\n         sp_name(str or None): Argument name whose value is either\n             ``scipy.sparse`` or ``cupyx.sciyp.sparse`` module. If ``None``, no\n             argument is given for the modules.\n         scipy_name(str or None): Argument name whose value is either ``scipy``\n             or ``cupyx.scipy`` module. If ``None``, no argument is given for\n             the modules.\n\n    Decorated test fixture is required to return the same results\n    even if ``xp`` is ``numpy`` or ``cupy``.\n    """"""\n    def decorator(impl):\n        @functools.wraps(impl)\n        def test_func(self, *args, **kw):\n            # Run cupy and numpy\n            (\n                cupy_result, cupy_error, cupy_tb,\n                numpy_result, numpy_error, numpy_tb) = (\n                    _call_func_numpy_cupy(\n                        self, impl, args, kw, name, sp_name, scipy_name))\n\n            if cupy_result != numpy_result:\n                message = \'\'\'Results are not equal:\ncupy: %s\nnumpy: %s\'\'\' % (str(cupy_result), str(numpy_result))\n                raise AssertionError(message)\n        return test_func\n    return decorator\n\n\ndef numpy_cupy_raises(name=\'xp\', sp_name=None, scipy_name=None,\n                      accept_error=Exception):\n    """"""Decorator that checks the NumPy and CuPy throw same errors.\n\n    Args:\n         name(str): Argument name whose value is either\n             ``numpy`` or ``cupy`` module.\n         sp_name(str or None): Argument name whose value is either\n             ``scipy.sparse`` or ``cupyx.scipy.sparse`` module. If ``None``, no\n             argument is given for the modules.\n         scipy_name(str or None): Argument name whose value is either ``scipy``\n             or ``cupyx.scipy`` module. If ``None``, no argument is given for\n             the modules.\n         accept_error(bool, Exception or tuple of Exception): Specify\n             acceptable errors. When both NumPy test and CuPy test raises the\n             same type of errors, and the type of the errors is specified with\n             this argument, the errors are ignored and not raised.\n             If it is ``True`` all error types are acceptable.\n             If it is ``False`` no error is acceptable.\n\n    Decorated test fixture is required throw same errors\n    even if ``xp`` is ``numpy`` or ``cupy``.\n    """"""\n    def decorator(impl):\n        @functools.wraps(impl)\n        def test_func(self, *args, **kw):\n            # Run cupy and numpy\n            (\n                cupy_result, cupy_error, cupy_tb,\n                numpy_result, numpy_error, numpy_tb) = (\n                    _call_func_numpy_cupy(\n                        self, impl, args, kw, name, sp_name, scipy_name))\n\n            _check_cupy_numpy_error(self, cupy_error, cupy_tb,\n                                    numpy_error, numpy_tb,\n                                    accept_error=accept_error)\n        return test_func\n    return decorator\n\n\ndef for_dtypes(dtypes, name=\'dtype\'):\n    """"""Decorator for parameterized dtype test.\n\n    Args:\n         dtypes(list of dtypes): dtypes to be tested.\n         name(str): Argument name to which specified dtypes are passed.\n\n    This decorator adds a keyword argument specified by ``name``\n    to the test fixture. Then, it runs the fixtures in parallel\n    by passing the each element of ``dtypes`` to the named\n    argument.\n    """"""\n    def decorator(impl):\n        @functools.wraps(impl)\n        def test_func(self, *args, **kw):\n            for dtype in dtypes:\n                try:\n                    kw[name] = numpy.dtype(dtype).type\n                    impl(self, *args, **kw)\n                except unittest.SkipTest as e:\n                    print(\'skipped: {} = {} ({})\'.format(name, dtype, e))\n                except Exception:\n                    print(name, \'is\', dtype)\n                    raise\n\n        return test_func\n    return decorator\n\n\n_complex_dtypes = (numpy.complex64, numpy.complex128)\n_regular_float_dtypes = (numpy.float64, numpy.float32)\n_float_dtypes = _regular_float_dtypes + (numpy.float16,)\n_signed_dtypes = tuple(numpy.dtype(i).type for i in \'bhilq\')\n_unsigned_dtypes = tuple(numpy.dtype(i).type for i in \'BHILQ\')\n_int_dtypes = _signed_dtypes + _unsigned_dtypes\n_int_bool_dtypes = _int_dtypes + (numpy.bool_,)\n_regular_dtypes = _regular_float_dtypes + _int_bool_dtypes\n_dtypes = _float_dtypes + _int_bool_dtypes\n\n\ndef _make_all_dtypes(no_float16, no_bool, no_complex):\n    if no_float16:\n        dtypes = _regular_float_dtypes\n    else:\n        dtypes = _float_dtypes\n\n    if no_bool:\n        dtypes += _int_dtypes\n    else:\n        dtypes += _int_bool_dtypes\n\n    if not no_complex:\n        dtypes += _complex_dtypes\n\n    return dtypes\n\n\ndef for_all_dtypes(name=\'dtype\', no_float16=False, no_bool=False,\n                   no_complex=False):\n    """"""Decorator that checks the fixture with all dtypes.\n\n    Args:\n         name(str): Argument name to which specified dtypes are passed.\n         no_float16(bool): If ``True``, ``numpy.float16`` is\n             omitted from candidate dtypes.\n         no_bool(bool): If ``True``, ``numpy.bool_`` is\n             omitted from candidate dtypes.\n         no_complex(bool): If ``True``, ``numpy.complex64`` and\n             ``numpy.complex128`` are omitted from candidate dtypes.\n\n    dtypes to be tested: ``numpy.complex64`` (optional),\n    ``numpy.complex128`` (optional),\n    ``numpy.float16`` (optional), ``numpy.float32``,\n    ``numpy.float64``, ``numpy.dtype(\'b\')``, ``numpy.dtype(\'h\')``,\n    ``numpy.dtype(\'i\')``, ``numpy.dtype(\'l\')``, ``numpy.dtype(\'q\')``,\n    ``numpy.dtype(\'B\')``, ``numpy.dtype(\'H\')``, ``numpy.dtype(\'I\')``,\n    ``numpy.dtype(\'L\')``, ``numpy.dtype(\'Q\')``, and ``numpy.bool_`` (optional).\n\n    The usage is as follows.\n    This test fixture checks if ``cPickle`` successfully reconstructs\n    :class:`cupy.ndarray` for various dtypes.\n    ``dtype`` is an argument inserted by the decorator.\n\n    >>> import unittest\n    >>> from cupy import testing\n    >>> @testing.gpu\n    ... class TestNpz(unittest.TestCase):\n    ...\n    ...     @testing.for_all_dtypes()\n    ...     def test_pickle(self, dtype):\n    ...         a = testing.shaped_arange((2, 3, 4), dtype=dtype)\n    ...         s = pickle.dumps(a)\n    ...         b = pickle.loads(s)\n    ...         testing.assert_array_equal(a, b)\n\n    Typically, we use this decorator in combination with\n    decorators that check consistency between NumPy and CuPy like\n    :func:`cupy.testing.numpy_cupy_allclose`.\n    The following is such an example.\n\n    >>> import unittest\n    >>> from cupy import testing\n    >>> @testing.gpu\n    ... class TestMean(unittest.TestCase):\n    ...\n    ...     @testing.for_all_dtypes()\n    ...     @testing.numpy_cupy_allclose()\n    ...     def test_mean_all(self, xp, dtype):\n    ...         a = testing.shaped_arange((2, 3), xp, dtype)\n    ...         return a.mean()\n\n    .. seealso:: :func:`cupy.testing.for_dtypes`\n    """"""\n    return for_dtypes(_make_all_dtypes(no_float16, no_bool, no_complex),\n                      name=name)\n\n\ndef for_float_dtypes(name=\'dtype\', no_float16=False):\n    """"""Decorator that checks the fixture with float dtypes.\n\n    Args:\n         name(str): Argument name to which specified dtypes are passed.\n         no_float16(bool): If ``True``, ``numpy.float16`` is\n             omitted from candidate dtypes.\n\n    dtypes to be tested are ``numpy.float16`` (optional), ``numpy.float32``,\n    and ``numpy.float64``.\n\n    .. seealso:: :func:`cupy.testing.for_dtypes`,\n        :func:`cupy.testing.for_all_dtypes`\n    """"""\n    if no_float16:\n        return for_dtypes(_regular_float_dtypes, name=name)\n    else:\n        return for_dtypes(_float_dtypes, name=name)\n\n\ndef for_signed_dtypes(name=\'dtype\'):\n    """"""Decorator that checks the fixture with signed dtypes.\n\n    Args:\n         name(str): Argument name to which specified dtypes are passed.\n\n    dtypes to be tested are ``numpy.dtype(\'b\')``, ``numpy.dtype(\'h\')``,\n    ``numpy.dtype(\'i\')``, ``numpy.dtype(\'l\')``, and ``numpy.dtype(\'q\')``.\n\n    .. seealso:: :func:`cupy.testing.for_dtypes`,\n        :func:`cupy.testing.for_all_dtypes`\n    """"""\n    return for_dtypes(_signed_dtypes, name=name)\n\n\ndef for_unsigned_dtypes(name=\'dtype\'):\n    """"""Decorator that checks the fixture with unsinged dtypes.\n\n    Args:\n         name(str): Argument name to which specified dtypes are passed.\n\n    dtypes to be tested are ``numpy.dtype(\'B\')``, ``numpy.dtype(\'H\')``,\n\n     ``numpy.dtype(\'I\')``, ``numpy.dtype(\'L\')``, and ``numpy.dtype(\'Q\')``.\n\n    .. seealso:: :func:`cupy.testing.for_dtypes`,\n        :func:`cupy.testing.for_all_dtypes`\n    """"""\n    return for_dtypes(_unsigned_dtypes, name=name)\n\n\ndef for_int_dtypes(name=\'dtype\', no_bool=False):\n    """"""Decorator that checks the fixture with integer and optionally bool dtypes.\n\n    Args:\n         name(str): Argument name to which specified dtypes are passed.\n         no_bool(bool): If ``True``, ``numpy.bool_`` is\n             omitted from candidate dtypes.\n\n    dtypes to be tested are ``numpy.dtype(\'b\')``, ``numpy.dtype(\'h\')``,\n    ``numpy.dtype(\'i\')``, ``numpy.dtype(\'l\')``, ``numpy.dtype(\'q\')``,\n    ``numpy.dtype(\'B\')``, ``numpy.dtype(\'H\')``, ``numpy.dtype(\'I\')``,\n    ``numpy.dtype(\'L\')``, ``numpy.dtype(\'Q\')``, and ``numpy.bool_`` (optional).\n\n    .. seealso:: :func:`cupy.testing.for_dtypes`,\n        :func:`cupy.testing.for_all_dtypes`\n    """"""  # NOQA\n    if no_bool:\n        return for_dtypes(_int_dtypes, name=name)\n    else:\n        return for_dtypes(_int_bool_dtypes, name=name)\n\n\ndef for_complex_dtypes(name=\'dtype\'):\n    """"""Decorator that checks the fixture with complex dtypes.\n\n    Args:\n         name(str): Argument name to which specified dtypes are passed.\n\n    dtypes to be tested are ``numpy.complex64`` and ``numpy.complex128``.\n\n    .. seealso:: :func:`cupy.testing.for_dtypes`,\n        :func:`cupy.testing.for_all_dtypes`\n    """"""\n    return for_dtypes(_complex_dtypes, name=name)\n\n\ndef for_dtypes_combination(types, names=(\'dtype\',), full=None):\n    """"""Decorator that checks the fixture with a product set of dtypes.\n\n    Args:\n         types(list of dtypes): dtypes to be tested.\n         names(list of str): Argument names to which dtypes are passed.\n         full(bool): If ``True``, then all combinations\n             of dtypes will be tested.\n             Otherwise, the subset of combinations will be tested\n             (see the description below).\n\n    Decorator adds the keyword arguments specified by ``names``\n    to the test fixture. Then, it runs the fixtures in parallel\n    with passing (possibly a subset of) the product set of dtypes.\n    The range of dtypes is specified by ``types``.\n\n    The combination of dtypes to be tested changes depending\n    on the option ``full``. If ``full`` is ``True``,\n    all combinations of ``types`` are tested.\n    Sometimes, such an exhaustive test can be costly.\n    So, if ``full`` is ``False``, only a subset of possible combinations\n    is randomly sampled. If ``full`` is ``None``, the behavior is\n    determined by an environment variable ``CUPY_TEST_FULL_COMBINATION``.\n    If the value is set to ``\'1\'``, it behaves as if ``full=True``, and\n    otherwise ``full=False``.\n    """"""\n\n    types = list(types)\n\n    if len(types) == 1:\n        name, = names\n        return for_dtypes(types, name)\n\n    if full is None:\n        full = int(os.environ.get(\'CUPY_TEST_FULL_COMBINATION\', \'0\')) != 0\n\n    if full:\n        combination = parameterized.product({name: types for name in names})\n    else:\n        ts = []\n        for _ in range(len(names)):\n            # Make shuffled list of types for each name\n            shuffled_types = types[:]\n            random.shuffle(shuffled_types)\n            ts.append(types + shuffled_types)\n\n        combination = [tuple(zip(names, typs)) for typs in zip(*ts)]\n        # Remove duplicate entries\n        combination = [dict(assoc_list) for assoc_list in set(combination)]\n\n    def decorator(impl):\n        @functools.wraps(impl)\n        def test_func(self, *args, **kw):\n            for dtypes in combination:\n                kw_copy = kw.copy()\n                kw_copy.update(dtypes)\n\n                try:\n                    impl(self, *args, **kw_copy)\n                except Exception:\n                    print(dtypes)\n                    raise\n\n        return test_func\n    return decorator\n\n\ndef for_all_dtypes_combination(names=(\'dtyes\',),\n                               no_float16=False, no_bool=False, full=None,\n                               no_complex=False):\n    """"""Decorator that checks the fixture with a product set of all dtypes.\n\n    Args:\n         names(list of str): Argument names to which dtypes are passed.\n         no_float16(bool): If ``True``, ``numpy.float16`` is\n             omitted from candidate dtypes.\n         no_bool(bool): If ``True``, ``numpy.bool_`` is\n             omitted from candidate dtypes.\n         full(bool): If ``True``, then all combinations of dtypes\n             will be tested.\n             Otherwise, the subset of combinations will be tested\n             (see description in :func:`cupy.testing.for_dtypes_combination`).\n         no_complex(bool): If, True, ``numpy.complex64`` and\n             ``numpy.complex128`` are omitted from candidate dtypes.\n\n    .. seealso:: :func:`cupy.testing.for_dtypes_combination`\n    """"""\n    types = _make_all_dtypes(no_float16, no_bool, no_complex)\n    return for_dtypes_combination(types, names, full)\n\n\ndef for_signed_dtypes_combination(names=(\'dtype\',), full=None):\n    """"""Decorator for parameterized test w.r.t. the product set of signed dtypes.\n\n    Args:\n         names(list of str): Argument names to which dtypes are passed.\n         full(bool): If ``True``, then all combinations of dtypes\n             will be tested.\n             Otherwise, the subset of combinations will be tested\n             (see description in :func:`cupy.testing.for_dtypes_combination`).\n\n    .. seealso:: :func:`cupy.testing.for_dtypes_combination`\n    """"""  # NOQA\n    return for_dtypes_combination(_signed_dtypes, names=names, full=full)\n\n\ndef for_unsigned_dtypes_combination(names=(\'dtype\',), full=None):\n    """"""Decorator for parameterized test w.r.t. the product set of unsigned dtypes.\n\n    Args:\n         names(list of str): Argument names to which dtypes are passed.\n         full(bool): If ``True``, then all combinations of dtypes\n             will be tested.\n             Otherwise, the subset of combinations will be tested\n             (see description in :func:`cupy.testing.for_dtypes_combination`).\n\n    .. seealso:: :func:`cupy.testing.for_dtypes_combination`\n    """"""  # NOQA\n    return for_dtypes_combination(_unsigned_dtypes, names=names, full=full)\n\n\ndef for_int_dtypes_combination(names=(\'dtype\',), no_bool=False, full=None):\n    """"""Decorator for parameterized test w.r.t. the product set of int and boolean.\n\n    Args:\n         names(list of str): Argument names to which dtypes are passed.\n         no_bool(bool): If ``True``, ``numpy.bool_`` is\n             omitted from candidate dtypes.\n         full(bool): If ``True``, then all combinations of dtypes\n             will be tested.\n             Otherwise, the subset of combinations will be tested\n             (see description in :func:`cupy.testing.for_dtypes_combination`).\n\n    .. seealso:: :func:`cupy.testing.for_dtypes_combination`\n    """"""  # NOQA\n    if no_bool:\n        types = _int_dtypes\n    else:\n        types = _int_bool_dtypes\n    return for_dtypes_combination(types, names, full)\n\n\ndef for_orders(orders, name=\'order\'):\n    """"""Decorator to parameterize tests with order.\n\n    Args:\n         orders(list of order): orders to be tested.\n         name(str): Argument name to which the specified order is passed.\n\n    This decorator adds a keyword argument specified by ``name``\n    to the test fixtures. Then, the fixtures run by passing each element of\n    ``orders`` to the named argument.\n\n    """"""\n    def decorator(impl):\n        @functools.wraps(impl)\n        def test_func(self, *args, **kw):\n            for order in orders:\n                try:\n                    kw[name] = order\n                    impl(self, *args, **kw)\n                except Exception:\n                    print(name, \'is\', order)\n                    raise\n\n        return test_func\n    return decorator\n\n\ndef for_CF_orders(name=\'order\'):\n    """"""Decorator that checks the fixture with orders \'C\' and \'F\'.\n\n    Args:\n         name(str): Argument name to which the specified order is passed.\n\n    .. seealso:: :func:`cupy.testing.for_all_dtypes`\n\n    """"""\n    return for_orders([None, \'C\', \'F\', \'c\', \'f\'], name)\n\n\ndef with_requires(*requirements):\n    """"""Run a test case only when given requirements are satisfied.\n\n    .. admonition:: Example\n\n       This test case runs only when `numpy>=1.18` is installed.\n\n       >>> from cupy import testing\n       ... class Test(unittest.TestCase):\n       ...     @testing.with_requires(\'numpy>=1.18\')\n       ...     def test_for_numpy_1_18(self):\n       ...         pass\n\n    Args:\n        requirements: A list of string representing requirement condition to\n            run a given test case.\n\n    """"""\n    # Delay import of pkg_resources because it is excruciatingly slow.\n    # See https://github.com/pypa/setuptools/issues/510\n    import pkg_resources\n\n    ws = pkg_resources.WorkingSet()\n    try:\n        ws.require(*requirements)\n        skip = False\n    except pkg_resources.ResolutionError:\n        skip = True\n\n    msg = \'requires: {}\'.format(\',\'.join(requirements))\n    return unittest.skipIf(skip, msg)\n\n\ndef numpy_satisfies(version_range):\n    """"""Returns True if numpy version satisfies the specified criteria.\n\n    Args:\n        version_range: A version specifier (e.g., `>=1.13.0`).\n    """"""\n    # Delay import of pkg_resources because it is excruciatingly slow.\n    # See https://github.com/pypa/setuptools/issues/510\n    import pkg_resources\n\n    spec = \'numpy{}\'.format(version_range)\n    try:\n        pkg_resources.require(spec)\n    except pkg_resources.VersionConflict:\n        return False\n    return True\n\n\ndef shaped_arange(shape, xp=cupy, dtype=numpy.float32, order=\'C\'):\n    """"""Returns an array with given shape, array module, and dtype.\n\n    Args:\n         shape(tuple of int): Shape of returned ndarray.\n         xp(numpy or cupy): Array module to use.\n         dtype(dtype): Dtype of returned ndarray.\n         order({\'C\', \'F\'}): Order of returned ndarray.\n\n    Returns:\n         numpy.ndarray or cupy.ndarray:\n         The array filled with :math:`1, \\\\cdots, N` with specified dtype\n         with given shape, array module. Here, :math:`N` is\n         the size of the returned array.\n         If ``dtype`` is ``numpy.bool_``, evens (resp. odds) are converted to\n         ``True`` (resp. ``False``).\n\n    """"""\n    dtype = numpy.dtype(dtype)\n    a = numpy.arange(1, internal.prod(shape) + 1, 1)\n    if dtype == \'?\':\n        a = a % 2 == 0\n    elif dtype.kind == \'c\':\n        a = a + a * 1j\n    return xp.array(a.astype(dtype).reshape(shape), order=order)\n\n\ndef shaped_reverse_arange(shape, xp=cupy, dtype=numpy.float32):\n    """"""Returns an array filled with decreasing numbers.\n\n    Args:\n         shape(tuple of int): Shape of returned ndarray.\n         xp(numpy or cupy): Array module to use.\n         dtype(dtype): Dtype of returned ndarray.\n\n    Returns:\n         numpy.ndarray or cupy.ndarray:\n         The array filled with :math:`N, \\\\cdots, 1` with specified dtype\n         with given shape, array module.\n         Here, :math:`N` is the size of the returned array.\n         If ``dtype`` is ``numpy.bool_``, evens (resp. odds) are converted to\n         ``True`` (resp. ``False``).\n    """"""\n    dtype = numpy.dtype(dtype)\n    size = internal.prod(shape)\n    a = numpy.arange(size, 0, -1)\n    if dtype == \'?\':\n        a = a % 2 == 0\n    elif dtype.kind == \'c\':\n        a = a + a * 1j\n    return xp.array(a.astype(dtype).reshape(shape))\n\n\ndef shaped_random(shape, xp=cupy, dtype=numpy.float32, scale=10, seed=0):\n    """"""Returns an array filled with random values.\n\n    Args:\n         shape(tuple): Shape of returned ndarray.\n         xp(numpy or cupy): Array module to use.\n         dtype(dtype): Dtype of returned ndarray.\n         scale(float): Scaling factor of elements.\n         seed(int): Random seed.\n\n    Returns:\n         numpy.ndarray or cupy.ndarray: The array with\n             given shape, array module,\n\n    If ``dtype`` is ``numpy.bool_``, the elements are\n    independently drawn from ``True`` and ``False``\n    with same probabilities.\n    Otherwise, the array is filled with samples\n    independently and identically drawn\n    from uniform distribution over :math:`[0, scale)`\n    with specified dtype.\n    """"""\n    numpy.random.seed(seed)\n    dtype = numpy.dtype(dtype)\n    if dtype == \'?\':\n        return xp.asarray(numpy.random.randint(2, size=shape), dtype=dtype)\n    elif dtype.kind == \'c\':\n        a = numpy.random.rand(*shape) + 1j * numpy.random.rand(*shape)\n        return xp.asarray(a * scale, dtype=dtype)\n    else:\n        return xp.asarray(numpy.random.rand(*shape) * scale, dtype=dtype)\n\n\ndef empty(xp=cupy, dtype=numpy.float32):\n    return xp.zeros((0,))\n\n\nclass NumpyError(object):\n\n    def __init__(self, **kw):\n        self.kw = kw\n\n    def __enter__(self):\n        self.err = numpy.geterr()\n        numpy.seterr(**self.kw)\n\n    def __exit__(self, *_):\n        numpy.seterr(**self.err)\n\n\n@contextlib.contextmanager\ndef assert_warns(expected):\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\'always\')\n        yield\n\n    if any(isinstance(m.message, expected) for m in w):\n        return\n\n    try:\n        exc_name = expected.__name__\n    except AttributeError:\n        exc_name = str(expected)\n\n    raise AssertionError(\'%s not triggerred\' % exc_name)\n\n\nclass NumpyAliasTestBase(unittest.TestCase):\n\n    @property\n    def func(self):\n        raise NotImplementedError()\n\n    @property\n    def cupy_func(self):\n        return getattr(cupy, self.func)\n\n    @property\n    def numpy_func(self):\n        return getattr(numpy, self.func)\n\n\nclass NumpyAliasBasicTestBase(NumpyAliasTestBase):\n\n    def test_argspec(self):\n        f = inspect.signature\n        assert f(self.cupy_func) == f(self.numpy_func)\n\n    def test_docstring(self):\n        cupy_func = self.cupy_func\n        numpy_func = self.numpy_func\n        assert hasattr(cupy_func, \'__doc__\')\n        assert cupy_func.__doc__ is not None\n        assert cupy_func.__doc__ != \'\'\n        assert cupy_func.__doc__ is not numpy_func.__doc__\n\n\nclass NumpyAliasValuesTestBase(NumpyAliasTestBase):\n\n    def test_values(self):\n        assert self.cupy_func(*self.args) == self.numpy_func(*self.args)\n'"
cupy/testing/hypothesis.py,0,"b'import numpy\n\n\ndef chi_square_test(observed, expected, alpha=0.05, df=None):\n    """"""Testing Goodness-of-fit Test with Pearson\'s Chi-squared Test.\n\n    Args:\n        observed (list of ints): List of # of counts each element is observed.\n        expected (list of floats): List of # of counts each element is expected\n            to be observed.\n        alpha (float): Significance level. Currently,\n            only 0.05 and 0.01 are acceptable.\n        df (int): Degree of freedom. If ``None``,\n            it is set to the length of ``observed`` minus 1.\n\n    Returns:\n        bool: ``True`` if null hypothesis is **NOT** reject.\n        Otherwise, ``False``.\n    """"""\n    if df is None:\n        df = observed.size - 1\n\n    if alpha == 0.01:\n        alpha_idx = 0\n    elif alpha == 0.05:\n        alpha_idx = 1\n    else:\n        raise ValueError(\'support only alpha == 0.05 or 0.01\')\n\n    chi_square = numpy.sum((observed - expected) ** 2 / expected)\n    return chi_square < chi_square_table[alpha_idx][df]\n\n\n# https://www.medcalc.org/manual/chi-square-table.php\nchi_square_table = [\n    [None,\n     6.635, 9.210, 11.345, 13.277, 15.086,\n     16.812, 18.475, 20.090, 21.666, 23.209,\n     24.725, 26.217, 27.688, 29.141, 30.578,\n     32.000, 33.409, 34.805, 36.191, 37.566,\n     38.932, 40.289, 41.638, 42.980, 44.314,\n     45.642, 46.963, 48.278, 49.588, 50.892,\n     52.191, 53.486, 54.776, 56.061, 57.342,\n     58.619, 59.893, 61.162, 62.428, 63.691,\n     64.950, 66.206, 67.459, 68.710, 69.957,\n     71.201, 72.443, 73.683, 74.919, 76.154,\n     77.386, 78.616, 79.843, 81.069, 82.292,\n     83.513, 84.733, 85.950, 87.166, 88.379,\n     89.591, 90.802, 92.010, 93.217, 94.422,\n     95.626, 96.828, 98.028, 99.228, 100.425,\n     101.621, 102.816, 104.010, 105.202, 106.393,\n     107.583, 108.771, 109.958, 111.144, 112.329,\n     113.512, 114.695, 115.876, 117.057, 118.236,\n     119.414, 120.591, 121.767, 122.942, 124.116,\n     125.289, 126.462, 127.633, 128.803, 129.973,\n     131.141, 132.309, 133.476, 134.642, 135.807,\n     136.971, 138.134, 139.297, 140.459, 141.620,\n     142.780, 143.940, 145.099, 146.257, 147.414,\n     148.571, 149.727, 150.882, 152.037, 153.191,\n     154.344, 155.496, 156.648, 157.800, 158.950,\n     160.100, 161.250, 162.398, 163.546, 164.694,\n     165.841, 166.987, 168.133, 169.278, 170.423,\n     171.567, 172.711, 173.854, 174.996, 176.138,\n     177.280, 178.421, 179.561, 180.701, 181.840,\n     182.979, 184.118, 185.256, 186.393, 187.530,\n     188.666, 189.802, 190.938, 192.073, 193.208,\n     194.342, 195.476, 196.609, 197.742, 198.874,\n     200.006, 201.138, 202.269, 203.400, 204.530,\n     205.660, 206.790, 207.919, 209.047, 210.176,\n     211.304, 212.431, 213.558, 214.685, 215.812,\n     216.938, 218.063, 219.189, 220.314, 221.438,\n     222.563, 223.687, 224.810, 225.933, 227.056,\n     228.179, 229.301, 230.423, 231.544, 232.665,\n     233.786, 234.907, 236.027, 237.147, 238.266,\n     239.386, 240.505, 241.623, 242.742, 243.860,\n     244.977, 246.095, 247.212, 248.329, 249.445,\n     250.561, 251.677, 252.793, 253.908, 255.023,\n     256.138, 257.253, 258.367, 259.481, 260.595,\n     261.708, 262.821, 263.934, 265.047, 266.159,\n     267.271, 268.383, 269.495, 270.606, 271.717,\n     272.828, 273.939, 275.049, 276.159, 277.269,\n     278.379, 279.488, 280.597, 281.706, 282.814,\n     283.923, 285.031, 286.139, 287.247, 288.354,\n     289.461, 290.568, 291.675, 292.782, 293.888,\n     294.994, 296.100, 297.206, 298.311, 299.417,\n     300.522, 301.626, 302.731, 303.835, 304.940],\n    [None,\n     3.841, 5.991, 7.815, 9.488, 11.070,\n     12.592, 14.067, 15.507, 16.919, 18.307,\n     19.675, 21.026, 22.362, 23.685, 24.996,\n     26.296, 27.587, 28.869, 30.144, 31.410,\n     32.671, 33.924, 35.172, 36.415, 37.652,\n     38.885, 40.113, 41.337, 42.557, 43.773,\n     44.985, 46.194, 47.400, 48.602, 49.802,\n     50.998, 52.192, 53.384, 54.572, 55.758,\n     56.942, 58.124, 59.304, 60.481, 61.656,\n     62.830, 64.001, 65.171, 66.339, 67.505,\n     68.669, 69.832, 70.993, 72.153, 73.311,\n     74.468, 75.624, 76.778, 77.931, 79.082,\n     80.232, 81.381, 82.529, 83.675, 84.821,\n     85.965, 87.108, 88.250, 89.391, 90.531,\n     91.670, 92.808, 93.945, 95.081, 96.217,\n     97.351, 98.484, 99.617, 100.749, 101.879,\n     103.010, 104.139, 105.267, 106.395, 107.522,\n     108.648, 109.773, 110.898, 112.022, 113.145,\n     114.268, 115.390, 116.511, 117.632, 118.752,\n     119.871, 120.990, 122.108, 123.225, 124.342,\n     125.458, 126.574, 127.689, 128.804, 129.918,\n     131.031, 132.144, 133.257, 134.369, 135.480,\n     136.591, 137.701, 138.811, 139.921, 141.030,\n     142.138, 143.246, 144.354, 145.461, 146.567,\n     147.674, 148.779, 149.885, 150.989, 152.094,\n     153.198, 154.302, 155.405, 156.508, 157.610,\n     158.712, 159.814, 160.915, 162.016, 163.116,\n     164.216, 165.316, 166.415, 167.514, 168.613,\n     169.711, 170.809, 171.907, 173.004, 174.101,\n     175.198, 176.294, 177.390, 178.485, 179.581,\n     180.676, 181.770, 182.865, 183.959, 185.052,\n     186.146, 187.239, 188.332, 189.424, 190.516,\n     191.608, 192.700, 193.791, 194.883, 195.973,\n     197.064, 198.154, 199.244, 200.334, 201.423,\n     202.513, 203.602, 204.690, 205.779, 206.867,\n     207.955, 209.042, 210.130, 211.217, 212.304,\n     213.391, 214.477, 215.563, 216.649, 217.735,\n     218.820, 219.906, 220.991, 222.076, 223.160,\n     224.245, 225.329, 226.413, 227.496, 228.580,\n     229.663, 230.746, 231.829, 232.912, 233.994,\n     235.077, 236.159, 237.240, 238.322, 239.403,\n     240.485, 241.566, 242.647, 243.727, 244.808,\n     245.888, 246.968, 248.048, 249.128, 250.207,\n     251.286, 252.365, 253.444, 254.523, 255.602,\n     256.680, 257.758, 258.837, 259.914, 260.992,\n     262.070, 263.147, 264.224, 265.301, 266.378,\n     267.455, 268.531, 269.608, 270.684, 271.760,\n     272.836, 273.911, 274.987, 276.062, 277.138,\n     278.213, 279.288, 280.362, 281.437, 282.511,\n     283.586, 284.660, 285.734, 286.808, 287.882]]\n'"
cupy/testing/parameterized.py,0,"b""import functools\nimport itertools\nimport io\nimport types\nimport typing as tp  # NOQA\nimport unittest\n\nfrom cupy.testing import _bundle\n\n\ndef _param_to_str(obj):\n    if isinstance(obj, type):\n        return obj.__name__\n    elif hasattr(obj, '__name__') and isinstance(obj.__name__, str):\n        # print __name__ attribute for classes, functions and modules\n        return obj.__name__\n    return repr(obj)\n\n\ndef _shorten(s, maxlen):\n    # Shortens the string down to maxlen, by replacing the middle part with\n    # a 3-dots string '...'.\n    ellipsis = '...'\n    if len(s) <= maxlen:\n        return s\n    n1 = (maxlen - len(ellipsis)) // 2\n    n2 = maxlen - len(ellipsis) - n1\n    s = s[:n1] + ellipsis + s[-n2:]\n    assert len(s) == maxlen\n    return s\n\n\ndef _make_class_name(base_class_name, i_param, param):\n    # Creates a class name for a single combination of parameters.\n    SINGLE_PARAM_MAXLEN = 100  # Length limit of a single parameter value\n    PARAMS_MAXLEN = 5000  # Length limit of the whole parameters part\n    param_strs = [\n        '{}={}'.format(k, _shorten(_param_to_str(v), SINGLE_PARAM_MAXLEN))\n        for k, v in sorted(param.items())]\n    param_strs = _shorten(', '.join(param_strs), PARAMS_MAXLEN)\n    cls_name = '{}_param_{}_{{{}}}'.format(\n        base_class_name, i_param, param_strs)\n    return cls_name\n\n\ndef _parameterize_test_case_generator(base, params):\n    # Defines the logic to generate parameterized test case classes.\n\n    for i, param in enumerate(params):\n        yield _parameterize_test_case(base, i, param)\n\n\ndef _parameterize_test_case(base, i, param):\n    cls_name = _make_class_name(base.__name__, i, param)\n\n    def __str__(self):\n        name = base.__str__(self)\n        return '%s  parameter: %s' % (name, param)\n\n    mb = {'__str__': __str__}\n    for k, v in sorted(param.items()):\n        if isinstance(v, types.FunctionType):\n\n            def create_new_v():\n                f = v\n\n                def new_v(self, *args, **kwargs):\n                    return f(*args, **kwargs)\n                return new_v\n\n            mb[k] = create_new_v()\n        else:\n            mb[k] = v\n\n    def method_generator(base_method):\n        # Generates a wrapped test method\n\n        @functools.wraps(base_method)\n        def new_method(self, *args, **kwargs):\n            try:\n                return base_method(self, *args, **kwargs)\n            except unittest.SkipTest:\n                raise\n            except Exception as e:\n                s = io.StringIO()\n                s.write('Parameterized test failed.\\n\\n')\n                s.write('Base test method: {}.{}\\n'.format(\n                    base.__name__, base_method.__name__))\n                s.write('Test parameters:\\n')\n                for k, v in sorted(param.items()):\n                    s.write('  {}: {}\\n'.format(k, v))\n                raise e.__class__(s.getvalue()).with_traceback(e.__traceback__)\n        return new_method\n\n    return (cls_name, mb, method_generator)\n\n\ndef parameterize(*params):\n    # TODO(niboshi): Add documentation\n    return _bundle.make_decorator(\n        lambda base: _parameterize_test_case_generator(base, params))\n\n\ndef _values_to_dicts(names, values):\n    assert isinstance(names, str)\n    assert isinstance(values, (tuple, list))\n\n    def safe_zip(ns, vs):\n        if len(ns) == 1:\n            return [(ns[0], vs)]\n        assert isinstance(vs, (tuple, list)) and len(ns) == len(vs)\n        return zip(ns, vs)\n\n    names = names.split(',')\n    params = [dict(safe_zip(names, value_list)) for value_list in values]\n    return params\n\n\ndef from_pytest_parameterize(names, values):\n    # Pytest-style parameterization.\n    # TODO(niboshi): Add documentation\n    return _values_to_dicts(names, values)\n\n\ndef parameterize_pytest(names, values):\n    # Pytest-style parameterization.\n    # TODO(niboshi): Add documentation\n    return parameterize(*from_pytest_parameterize(names, values))\n\n\ndef product(parameter):\n    # TODO(niboshi): Add documentation\n    if isinstance(parameter, dict):\n        return product_dict(*[\n            _values_to_dicts(names, values)\n            for names, values in sorted(parameter.items())])\n\n    elif isinstance(parameter, list):\n        # list of lists of dicts\n        if not all(isinstance(_, list) for _ in parameter):\n            raise TypeError('parameter must be list of lists of dicts')\n        if not all(isinstance(_, dict) for l in parameter for _ in l):\n            raise TypeError('parameter must be list of lists of dicts')\n        return product_dict(*parameter)\n\n    else:\n        raise TypeError(\n            'parameter must be either dict or list. Actual: {}'.format(\n                type(parameter)))\n\n\ndef product_dict(*parameters):\n    # TODO(niboshi): Add documentation\n    return [\n        {k: v for dic in dicts for k, v in dic.items()}\n        for dicts in itertools.product(*parameters)]\n"""
cupy/testing/random.py,0,"b'import atexit\nimport functools\nimport numpy\nimport os\nimport random\nimport types\nimport unittest\n\nimport cupy\n\n\n_old_python_random_state = None\n_old_numpy_random_state = None\n_old_cupy_random_states = None\n\n\ndef do_setup(deterministic=True):\n    global _old_python_random_state\n    global _old_numpy_random_state\n    global _old_cupy_random_states\n    _old_python_random_state = random.getstate()\n    _old_numpy_random_state = numpy.random.get_state()\n    _old_cupy_random_states = cupy.random.generator._random_states\n    cupy.random.reset_states()\n    # Check that _random_state has been recreated in\n    # cupy.random.reset_states(). Otherwise the contents of\n    # _old_cupy_random_states would be overwritten.\n    assert (cupy.random.generator._random_states is not\n            _old_cupy_random_states)\n\n    if not deterministic:\n        random.seed()\n        numpy.random.seed()\n        cupy.random.seed()\n    else:\n        random.seed(99)\n        numpy.random.seed(100)\n        cupy.random.seed(101)\n\n\ndef do_teardown():\n    global _old_python_random_state\n    global _old_numpy_random_state\n    global _old_cupy_random_states\n    random.setstate(_old_python_random_state)\n    numpy.random.set_state(_old_numpy_random_state)\n    cupy.random.generator._random_states = _old_cupy_random_states\n    _old_python_random_state = None\n    _old_numpy_random_state = None\n    _old_cupy_random_states = None\n\n\n# In some tests (which utilize condition.repeat or condition.retry),\n# setUp/tearDown is nested. _setup_random() and _teardown_random() do their\n# work only in the outermost setUp/tearDown pair.\n_nest_count = 0\n\n\n@atexit.register\ndef _check_teardown():\n    assert _nest_count == 0, (\'_setup_random() and _teardown_random() \'\n                              \'must be called in pairs.\')\n\n\ndef _setup_random():\n    """"""Sets up the deterministic random states of ``numpy`` and ``cupy``.\n\n    """"""\n    global _nest_count\n    if _nest_count == 0:\n        nondeterministic = bool(int(os.environ.get(\n            \'CUPY_TEST_RANDOM_NONDETERMINISTIC\', \'0\')))\n        do_setup(not nondeterministic)\n    _nest_count += 1\n\n\ndef _teardown_random():\n    """"""Tears down the deterministic random states set up by ``_setup_random``.\n\n    """"""\n    global _nest_count\n    assert _nest_count > 0, \'_setup_random has not been called\'\n    _nest_count -= 1\n    if _nest_count == 0:\n        do_teardown()\n\n\ndef generate_seed():\n    assert _nest_count > 0, \'random is not set up\'\n    return numpy.random.randint(0x7fffffff)\n\n\ndef fix_random():\n    """"""Decorator that fixes random numbers in a test.\n\n    This decorator can be applied to either a test case class or a test method.\n    It should not be applied within ``condition.retry`` or\n    ``condition.repeat``.\n    """"""\n\n    # TODO(niboshi): Prevent this decorator from being applied within\n    #    condition.repeat or condition.retry decorators. That would repeat\n    #    tests with the same random seeds. It\'s okay to apply this outside\n    #    these decorators.\n\n    def decorator(impl):\n        if (isinstance(impl, types.FunctionType) and\n                impl.__name__.startswith(\'test_\')):\n            # Applied to test method\n            @functools.wraps(impl)\n            def test_func(self, *args, **kw):\n                _setup_random()\n                try:\n                    impl(self, *args, **kw)\n                finally:\n                    _teardown_random()\n            return test_func\n        elif isinstance(impl, type) and issubclass(impl, unittest.TestCase):\n            # Applied to test case class\n            klass = impl\n\n            def wrap_setUp(f):\n                def func(self):\n                    _setup_random()\n                    f(self)\n                return func\n\n            def wrap_tearDown(f):\n                def func(self):\n                    try:\n                        f(self)\n                    finally:\n                        _teardown_random()\n                return func\n\n            klass.setUp = wrap_setUp(klass.setUp)\n            klass.tearDown = wrap_tearDown(klass.tearDown)\n            return klass\n        else:\n            raise ValueError(\'Can\\\'t apply fix_random to {}\'.format(impl))\n\n    return decorator\n'"
cupyx/fallback_mode/__init__.py,0,"b'from cupy import util as _util\n\n# Attributes and Methods for fallback_mode\n# Auto-execute numpy method when corresponding cupy method is not found\n\n# ""NOQA"" to suppress flake8 warning\nfrom cupyx.fallback_mode.fallback import numpy  # NOQA\n\n\n_util.experimental(\'cupyx.fallback_mode.numpy\')\n'"
cupyx/fallback_mode/fallback.py,15,"b'""""""\n`fallback_mode` for cupy. Whenever a method is not yet implemented in CuPy,\nit will fallback to corresponding NumPy method.\n""""""\nimport types\n\nimport numpy as np\n\nimport cupy as cp\n\n\nfrom cupyx.fallback_mode import notification\n\n\nclass _RecursiveAttr(object):\n    """"""\n    RecursiveAttr class to catch all attributes corresponding to numpy,\n    when user calls fallback_mode. numpy is an instance of this class.\n    """"""\n\n    def __init__(self, numpy_object, cupy_object, array=None):\n        """"""\n        _RecursiveAttr initializer.\n\n        Args:\n            numpy_object (method): NumPy method.\n            cupy_method (method): Corresponding CuPy method.\n            array (ndarray): Acts as flag to know if _RecursiveAttr object\n                is called from ``ndarray`` class. Also, acts as container for\n                modifying args in case it is called from ``ndarray``.\n                None otherwise.\n        """"""\n\n        self._numpy_object = numpy_object\n        self._cupy_object = cupy_object\n        self._fallback_array = array\n\n    def __instancecheck__(self, instance):\n        """"""\n        Enable support for isinstance(instance, _RecursiveAttr instance)\n        by redirecting it to appropriate isinstance method.\n        """"""\n\n        if self._cupy_object is not None:\n            return isinstance(instance, self._cupy_object)\n\n        return isinstance(instance, self._numpy_object)\n\n    def __getattr__(self, attr):\n        """"""\n        Catches attributes corresponding to numpy.\n\n        Runs recursively till attribute gets called.\n        Or numpy ScalarType is retrieved.\n\n        Args:\n            attr (str): Attribute of _RecursiveAttr class object.\n\n        Returns:\n            (_RecursiveAttr object, NumPy scalar):\n                Returns_RecursiveAttr object with new numpy_object,\n                cupy_object. OR\n                Returns objects in cupy which is an alias\n                of numpy object. OR\n                Returns wrapper objects, `ndarray`, `vectorize`.\n        """"""\n\n        numpy_object = getattr(self._numpy_object, attr)\n        cupy_object = getattr(self._cupy_object, attr, None)\n\n        if numpy_object is np.ndarray:\n            return ndarray\n\n        if numpy_object is np.vectorize:\n            return vectorize\n\n        if numpy_object is cupy_object:\n            return numpy_object\n\n        return _RecursiveAttr(numpy_object, cupy_object)\n\n    def __repr__(self):\n\n        if isinstance(self._numpy_object, types.ModuleType):\n            return ""<numpy = module {}, cupy = module {}>"".format(\n                self._numpy_object.__name__,\n                getattr(self._cupy_object, \'__name__\', None))\n\n        return ""<numpy = {}, cupy = {}>"".format(\n            self._numpy_object, self._cupy_object)\n\n    @property\n    def __doc__(self):\n        return self._numpy_object.__doc__\n\n    @staticmethod\n    def _is_cupy_compatible(arg):\n        """"""\n        Returns False if CuPy\'s functions never accept the arguments as\n        parameters due to the following reasons.\n        - The inputs include an object of a NumPy\'s specific class other than\n          `np.ndarray`.\n        - The inputs include a dtype which is not supported in CuPy.\n        """"""\n\n        if isinstance(arg, ndarray):\n            if not arg._supports_cupy:\n                return False\n\n        if isinstance(arg, (tuple, list)):\n            return all([_RecursiveAttr._is_cupy_compatible(i) for i in arg])\n\n        if isinstance(arg, dict):\n            bools = [_RecursiveAttr._is_cupy_compatible(arg[i]) for i in arg]\n            return all(bools)\n\n        return True\n\n    def __call__(self, *args, **kwargs):\n        """"""\n        Gets invoked when last attribute of _RecursiveAttr class gets called.\n        Calls _cupy_object if not None else call _numpy_object.\n\n        Args:\n            args (tuple): Arguments.\n            kwargs (dict): Keyword arguments.\n\n        Returns:\n            (res, ndarray): Returns of methods call_cupy or call_numpy\n        """"""\n\n        if not callable(self._numpy_object):\n            raise TypeError(""\'{}\' object is not callable"".format(\n                type(self._numpy_object).__name__))\n\n        # _RecursiveAttr gets called from ndarray\n        if self._fallback_array is not None:\n            args = ((self._fallback_array,) + args)\n\n        if self._cupy_object is not None and \\\n           _RecursiveAttr._is_cupy_compatible((args, kwargs)):\n            try:\n                return _call_cupy(self._cupy_object, args, kwargs)\n            except Exception:\n                return _call_numpy(self._numpy_object, args, kwargs)\n\n        notification._dispatch_notification(self._numpy_object)\n        return _call_numpy(self._numpy_object, args, kwargs)\n\n\nnumpy = _RecursiveAttr(np, cp)\n\n\n# -----------------------------------------------------------------------------\n# proxying of ndarray magic methods and wrappers\n# -----------------------------------------------------------------------------\n\n\nclass ndarray(object):\n    """"""\n    Wrapper around cupy.ndarray\n    Supports cupy.ndarray.__init__ as well as,\n    gets initialized with a cupy ndarray.\n    """"""\n\n    def __new__(cls, *args, **kwargs):\n        """"""\n        If `_initial_array` and `_supports_cupy` are arguments,\n        initialize cls(ndarray).\n        Else get cupy.ndarray from provided arguments,\n        then initialize cls(ndarray).\n        """"""\n        _initial_array = kwargs.get(\'_initial_array\', None)\n        if _initial_array is not None:\n            return object.__new__(cls)\n\n        cupy_ndarray_init = cp.ndarray(*args, **kwargs)\n        return cls(_initial_array=cupy_ndarray_init, _supports_cupy=True)\n\n    def __init__(self, *args, **kwargs):\n        """"""\n        Args:\n            _initial_array (None, cp.ndarray/np.ndarray(including variants)):\n                If _initial_array is None, object is not initialized.\n                Otherwise, _initial_array (ndarray) would be set to\n                _cupy_array and/or _numpy_array depending upon _supports_cupy.\n            _supports_cupy (bool): If _supports_cupy is True, _initial_array\n                is set as _cupy_array and _numpy_array.\n                Otherwise, _initial_array is set as only _numpy_array.\n\n        Attributes:\n            _cupy_array (None or cp.ndarray): ndarray fully compatible with\n                CuPy. This will be always set to a ndarray in GPU.\n            _numpy_array (None or np.ndarray(including variants)): ndarray not\n                supported by CuPy. Such as np.ndarray (where dtype is not in\n                \'?bhilqBHILQefdFD\') and it\'s variants. This will be always set\n                to a ndarray in CPU.\n            _supports_cupy (bool): If _supports_cupy is True, data of array\n                will contain in _cupy_array and _numpy_array.\n                Else only _numpy_array will have the data.\n        """"""\n\n        _supports_cupy = kwargs.pop(\'_supports_cupy\', None)\n        _initial_array = kwargs.pop(\'_initial_array\', None)\n        if _initial_array is None:\n            return\n\n        self._cupy_array = None\n        self._numpy_array = None\n        self.base = None\n        self._supports_cupy = _supports_cupy\n\n        assert isinstance(_initial_array, (cp.ndarray, np.ndarray))\n        if _supports_cupy:\n            if type(_initial_array) is cp.ndarray:\n                # _initial_array is in GPU memory\n                # called by _store_array_from_cupy\n                self._cupy_array = _initial_array\n                self._remember_numpy = False\n            else:\n                # _initial_array is in CPU memory\n                # called by _store_array_from_numpy\n                self._numpy_array = _initial_array\n                self._remember_numpy = True\n        else:\n            self._numpy_array = _initial_array\n\n    @classmethod\n    def _store_array_from_cupy(cls, array):\n        return cls(_initial_array=array, _supports_cupy=True)\n\n    @classmethod\n    def _store_array_from_numpy(cls, array):\n        if type(array) is np.ndarray and \\\n           array.dtype.kind in \'?bhilqBHILQefdFD\':\n            return cls(_initial_array=array, _supports_cupy=True)\n\n        return cls(_initial_array=array, _supports_cupy=False)\n\n    @property\n    def dtype(self):\n        if self._supports_cupy and not self._remember_numpy:\n            return self._cupy_array.dtype\n        return self._numpy_array.dtype\n\n    def __getattr__(self, attr):\n        """"""\n        Catches attributes corresponding to ndarray.\n\n        Args:\n            attr (str): Attribute of ndarray class.\n\n        Returns:\n            (_RecursiveAttr object, self._array.attr):\n            Returns_RecursiveAttr object with numpy_object, cupy_object.\n            Returns self._array.attr if attr is not callable.\n        """"""\n\n        if self._supports_cupy:\n            cupy_object = getattr(cp.ndarray, attr, None)\n            numpy_object = getattr(np.ndarray, attr)\n        else:\n            cupy_object = None\n            numpy_object = getattr(self._numpy_array.__class__, attr)\n\n        if not callable(numpy_object):\n            if self._supports_cupy:\n                if self._remember_numpy:\n                    self._update_cupy_array()\n                return getattr(self._cupy_array, attr)\n            return getattr(self._numpy_array, attr)\n\n        return _RecursiveAttr(numpy_object, cupy_object, self)\n\n    def _get_cupy_array(self):\n        """"""\n        Returns _cupy_array (cupy.ndarray) of ndarray object. And marks\n        self(ndarray) and it\'s base (if exist) as numpy not up-to-date.\n        """"""\n        base = self.base\n        if base is not None:\n            base._remember_numpy = False\n        self._remember_numpy = False\n        return self._cupy_array\n\n    def _get_numpy_array(self):\n        """"""\n        Returns _numpy_array (ex: np.ndarray, numpy.ma.MaskedArray,\n        numpy.chararray etc.) of ndarray object. And marks self(ndarray)\n        and it\'s base (if exist) as numpy up-to-date.\n        """"""\n        base = self.base\n        if base is not None and base._supports_cupy:\n            base._remember_numpy = True\n        if self._supports_cupy:\n            self._remember_numpy = True\n        return self._numpy_array\n\n    def _update_numpy_array(self):\n        """"""\n        Updates _numpy_array from _cupy_array.\n        To be executed before calling numpy function.\n        """"""\n        base = self.base\n        _type = np.ndarray if self._supports_cupy \\\n            else self._numpy_array.__class__\n\n        if self._supports_cupy:\n            # cupy-compatible\n            if base is None:\n                if not self._remember_numpy:\n                    if self._numpy_array is None:\n                        self._numpy_array = cp.asnumpy(self._cupy_array)\n                    else:\n                        self._cupy_array.get(out=self._numpy_array)\n            else:\n                if not base._remember_numpy:\n                    base._update_numpy_array()\n                    if self._numpy_array is None:\n                        self._numpy_array = base._numpy_array.view(type=_type)\n                        self._numpy_array.shape = self._cupy_array.shape\n                        self._numpy_array.strides = self._cupy_array.strides\n        else:\n            # not cupy-compatible\n            if base is not None:\n                assert base._supports_cupy\n                if not base._remember_numpy:\n                    base._update_numpy_array()\n\n    def _update_cupy_array(self):\n        """"""\n        Updates _cupy_array from _numpy_array.\n        To be executed before calling cupy function.\n        """"""\n        base = self.base\n\n        if base is None:\n            if self._remember_numpy:\n                if self._cupy_array is None:\n                    self._cupy_array = cp.array(self._numpy_array)\n                else:\n                    self._cupy_array[:] = self._numpy_array\n        else:\n            if base._remember_numpy:\n                base._update_cupy_array()\n\n\ndef _create_magic_methods():\n    """"""\n    Set magic methods of cupy.ndarray as methods of fallback.ndarray.\n    """"""\n\n    # Decorator for ndarray magic methods\n    def make_method(name):\n        def method(self, *args, **kwargs):\n            CLASS = cp.ndarray if self._supports_cupy \\\n                else self._numpy_array.__class__\n            _method = getattr(CLASS, name)\n            args = ((self,) + args)\n            if self._supports_cupy:\n                return _call_cupy(_method, args, kwargs)\n            return _call_numpy(_method, args, kwargs)\n        return method\n\n    for method in (\n        # Comparison operators:\n        \'__eq__\', \'__ne__\', \'__lt__\', \'__gt__\', \'__le__\', \'__ge__\',\n\n        # Unary operations:\n        \'__neg__\', \'__pos__\', \'__abs__\', \'__invert__\',\n\n        # Arithmetic:\n        \'__add__\', \'__sub__\', \'__mul__\', \'__truediv__\', \'__floordiv__\',\n        \'__mod__\', \'__divmod__\', \'__pow__\', \'__lshift__\', \'__rshift__\',\n        \'__and__\', \'__or__\', \'__xor__\',\n\n        # Arithmetic, in-place:\n        \'__iadd__\', \'__isub__\', \'__imul__\', \'__itruediv__\', \'__ifloordiv__\',\n        \'__imod__\', \'__ipow__\', \'__ilshift__\', \'__irshift__\',\n        \'__iand__\', \'__ior__\', \'__ixor__\',\n        \'__matmul__\',\n\n        # reflected-methods:\n        \'__radd__\', \'__rsub__\', \'__rmul__\', \'__rtruediv__\', \'__rfloordiv__\',\n        \'__rmod__\', \'__rdivmod__\', \'__rpow__\', \'__rlshift__\', \'__rrshift__\',\n        \'__rand__\', \'__ror__\', \'__rxor__\',\n        \'__rmatmul__\',\n\n        # For standard library functions:\n        \'__copy__\', \'__deepcopy__\', \'__reduce__\',\n\n        # Container customization:\n        \'__iter__\', \'__len__\', \'__getitem__\', \'__setitem__\',\n\n        # Conversion:\n        \'__bool__\', \'__int__\', \'__float__\', \'__complex__\',\n\n        # String representations:\n        \'__repr__\', \'__str__\'\n    ):\n        setattr(ndarray, method, make_method(method))\n\n\n_create_magic_methods()\n\n\nclass vectorize(object):\n\n    def __init__(self, *args, **kwargs):\n        # NumPy will raise error if pyfunc is a cupy method\n        self.__dict__[\'_is_numpy_pyfunc\'] = False\n        self.__dict__[\'_cupy_support\'] = False\n        if isinstance(args[0], _RecursiveAttr):\n            self.__dict__[\'_is_numpy_pyfunc\'] = True\n            if args[0]._cupy_object:\n                self.__dict__[\'_cupy_support\'] = True\n            args = (args[0]._numpy_object,) + args[1:]\n        notification._dispatch_notification(np.vectorize)\n        self.__dict__[\'vec_obj\'] = np.vectorize(*args, **kwargs)\n\n    def __getattr__(self, attr):\n        return getattr(self.__dict__[\'vec_obj\'], attr)\n\n    def __setattr__(self, name, value):\n        return setattr(self.vec_obj, name, value)\n\n    @property\n    def __doc__(self):\n        return self.vec_obj.__doc__\n\n    def __call__(self, *args, **kwargs):\n        if self._is_numpy_pyfunc:\n            notification._dispatch_notification(\n                self.vec_obj.pyfunc, self._cupy_support)\n        return _call_numpy(self.vec_obj, args, kwargs)\n\n\n# -----------------------------------------------------------------------------\n# Data Transfer methods\n# -----------------------------------------------------------------------------\n\n\ndef _get_xp_args(ndarray_instance, to_xp, arg):\n    """"""\n    Converts ndarray_instance type object to target object using to_xp.\n\n    Args:\n        ndarray_instance (numpy.ndarray, cupy.ndarray or fallback.ndarray):\n            Objects of type `ndarray_instance` will be converted using `to_xp`.\n        to_xp (FunctionType): Method to convert ndarray_instance type objects.\n        arg (object): `ndarray_instance`, `tuple`, `list` and `dict` type\n            objects will be returned by either converting the object or it\'s\n            elements, if object is iterable. Objects of other types is\n            returned as it is.\n\n    Returns:\n        Return data structure will be same as before after converting ndarrays.\n    """"""\n\n    if isinstance(arg, ndarray_instance):\n        return to_xp(arg)\n\n    if isinstance(arg, tuple):\n        return tuple([_get_xp_args(ndarray_instance, to_xp, x) for x in arg])\n\n    if isinstance(arg, dict):\n        return {x_name: _get_xp_args(ndarray_instance, to_xp, x)\n                for x_name, x in arg.items()}\n\n    if isinstance(arg, list):\n        return [_get_xp_args(ndarray_instance, to_xp, x) for x in arg]\n\n    return arg\n\n\ndef _convert_numpy_to_fallback(numpy_res):\n    return _get_xp_args(np.ndarray, ndarray._store_array_from_numpy, numpy_res)\n\n\ndef _convert_fallback_to_numpy(args, kwargs):\n    return _get_xp_args(ndarray, ndarray._get_numpy_array, (args, kwargs))\n\n\ndef _convert_fallback_to_cupy(args, kwargs):\n    return _get_xp_args(ndarray, ndarray._get_cupy_array, (args, kwargs))\n\n\ndef _convert_cupy_to_fallback(cupy_res):\n    return _get_xp_args(cp.ndarray, ndarray._store_array_from_cupy, cupy_res)\n\n\ndef _update_numpy_args(args, kwargs):\n    return _get_xp_args(ndarray, ndarray._update_numpy_array, (args, kwargs))\n\n\ndef _update_cupy_args(args, kwargs):\n    return _get_xp_args(ndarray, ndarray._update_cupy_array, (args, kwargs))\n\n\n# -----------------------------------------------------------------------------\n# utils\n# -----------------------------------------------------------------------------\n\n\ndef _call_cupy(func, args, kwargs):\n    """"""\n    Calls cupy function with *args and **kwargs and\n    does necessary data transfers.\n\n    Args:\n        func: A cupy function that needs to be called.\n        args (tuple): Arguments.\n        kwargs (dict): Keyword arguments.\n\n    Returns:\n        Result after calling func and performing data transfers.\n    """"""\n\n    _update_cupy_args(args, kwargs)\n    cupy_args, cupy_kwargs = _convert_fallback_to_cupy(args, kwargs)\n    cupy_res = func(*cupy_args, **cupy_kwargs)\n\n    # If existing argument is being returned\n    ext_res = _get_same_reference(\n        cupy_res, cupy_args, cupy_kwargs, args, kwargs)\n    if ext_res is not None:\n        return ext_res\n\n    if isinstance(cupy_res, cp.ndarray):\n        if cupy_res.base is None:\n            # Don\'t share memory\n            fallback_res = _convert_cupy_to_fallback(cupy_res)\n        else:\n            # Share memory with one of the arguments\n            base_arg = _get_same_reference(\n                cupy_res.base, cupy_args, cupy_kwargs, args, kwargs)\n            fallback_res = _convert_cupy_to_fallback(cupy_res)\n            fallback_res.base = base_arg\n        return fallback_res\n    return cupy_res\n\n\ndef _call_numpy(func, args, kwargs):\n    """"""\n    Calls numpy function with *args and **kwargs and\n    does necessary data transfers.\n\n    Args:\n        func: A numpy function that needs to be called.\n        args (tuple): Arguments.\n        kwargs (dict): Keyword arguments.\n\n    Returns:\n        Result after calling func and performing data transfers.\n    """"""\n\n    _update_numpy_args(args, kwargs)\n    numpy_args, numpy_kwargs = _convert_fallback_to_numpy(args, kwargs)\n    numpy_res = func(*numpy_args, **numpy_kwargs)\n\n    # If existing argument is being returned\n    ext_res = _get_same_reference(\n        numpy_res, numpy_args, numpy_kwargs, args, kwargs)\n    if ext_res is not None:\n        return ext_res\n\n    if isinstance(numpy_res, np.ndarray):\n        if numpy_res.base is None:\n            # Don\'t share memory\n            fallback_res = _convert_numpy_to_fallback(numpy_res)\n        else:\n            # Share memory with one of the arguments\n            base_arg = _get_same_reference(\n                numpy_res.base, numpy_args, numpy_kwargs, args, kwargs)\n            fallback_res = _convert_numpy_to_fallback(numpy_res)\n            fallback_res.base = base_arg\n        return fallback_res\n    return numpy_res\n\n\ndef _get_same_reference(res, args, kwargs, ret_args, ret_kwargs):\n    """"""\n    Returns object corresponding to res in (args, kwargs)\n    from (ret_args, ret_kwargs)\n    """"""\n    for i in range(len(args)):\n        if res is args[i]:\n            return ret_args[i]\n\n    for key in kwargs:\n        if res is kwargs[key]:\n            return ret_kwargs[key]\n\n    return\n'"
cupyx/fallback_mode/notification.py,0,"b'""""""\nMethods related to notifications.\n""""""\n\nimport warnings\n\nfrom cupyx import _ufunc_config\n\n\ndef _init_warnings():\n    FallbackWarning = type(\'FallbackWarning\', (Warning,), {})\n    warnings.simplefilter(action=\'always\', category=FallbackWarning)\n    return FallbackWarning\n\n\ndef _dispatch_notification(func, cupy_support=False):\n    """"""\n    Dispatch notifications using appropriate dispatch type.\n    """"""\n\n    dispatch_type = _ufunc_config.get_config_fallback_mode()\n\n    _module = getattr(func, \'__module__\', None)\n    _name = getattr(func, \'__name__\', None)\n\n    if not cupy_support:\n        if _name and _module:\n            msg = ""\'{}\' method not in cupy, falling back to \'{}.{}\'"".format(\n                _name, _module, _name)\n        elif _name:\n            msg = ""\'{}\' method not in cupy, "".format(_name)\n            msg += ""falling back to its numpy implementation""\n        else:\n            msg = ""This method is not available in cupy, ""\n            msg += ""falling back to numpy""\n\n        if _name:\n            raise_msg = ""\'{}\' method not found in cupy"".format(_name)\n        else:\n            raise_msg = ""This method is not available in cupy""\n    else:\n        if _name and _module:\n            msg = ""\'{}\' method is available in cupy but "".format(_name)\n            msg += ""cannot be used, falling back to \'{}.{}\'"".format(\n                _module, _name)\n        elif _name:\n            msg = ""\'{}\' method is available in cupy but "".format(_name)\n            msg += ""cannot be used, falling back to its numpy implementation""\n        else:\n            msg = ""This method is available in cupy, but cannot be used""\n            msg += ""falling back to numpy""\n\n        if _name:\n            raise_msg = ""\'{}\' method is available in cupy "".format(_name)\n            raise_msg += ""but cannot be used""\n        else:\n            raise_msg = ""This method is available in cupy but cannot be used""\n\n    if dispatch_type == \'print\':\n        print(""Warning: {}"".format(msg))\n\n    elif dispatch_type == \'warn\':\n        warnings.warn(msg, FallbackWarning, stacklevel=3)\n\n    elif dispatch_type == \'ignore\':\n        pass\n\n    elif dispatch_type == \'raise\':\n        raise AttributeError(raise_msg)\n\n    else:\n        assert False\n\n\nFallbackWarning = _init_warnings()\n'"
cupyx/linalg/__init__.py,0,"b'# ""NOQA"" to suppress flake8 warning\nfrom cupyx.linalg import sparse  # NOQA\nfrom cupyx.linalg.solve import invh  # NOQA\n'"
cupyx/linalg/solve.py,0,"b'import numpy\n\nimport cupy\nfrom cupy.cuda import cublas\nfrom cupy.cuda import cusolver\nfrom cupy.cuda import device\nfrom cupy.linalg import util\n\n\ndef invh(a):\n    """"""Compute the inverse of a Hermitian matrix.\n\n    This function computes a inverse of a real symmetric or complex hermitian\n    positive-definite matrix using Cholesky factorization. If matrix ``a`` is\n    not positive definite, Cholesky factorization fails and it raises an error.\n\n    Args:\n        a (cupy.ndarray): Real symmetric or complex hermitian maxtix.\n\n    Returns:\n        cupy.ndarray: The inverse of matrix ``a``.\n    """"""\n\n    # to prevent `a` from being overwritten\n    a = a.copy()\n\n    util._assert_cupy_array(a)\n    util._assert_rank2(a)\n    util._assert_nd_squareness(a)\n\n    # support float32, float64, complex64, and complex128\n    if a.dtype.char in \'fdFD\':\n        dtype = a.dtype.char\n    else:\n        dtype = numpy.promote_types(a.dtype.char, \'f\').char\n\n    cusolver_handle = device.get_cusolver_handle()\n    dev_info = cupy.empty(1, dtype=numpy.int32)\n\n    if dtype == \'f\':\n        potrf = cusolver.spotrf\n        potrf_bufferSize = cusolver.spotrf_bufferSize\n        potrs = cusolver.spotrs\n    elif dtype == \'d\':\n        potrf = cusolver.dpotrf\n        potrf_bufferSize = cusolver.dpotrf_bufferSize\n        potrs = cusolver.dpotrs\n    elif dtype == \'F\':\n        potrf = cusolver.cpotrf\n        potrf_bufferSize = cusolver.cpotrf_bufferSize\n        potrs = cusolver.cpotrs\n    elif dtype == \'D\':\n        potrf = cusolver.zpotrf\n        potrf_bufferSize = cusolver.zpotrf_bufferSize\n        potrs = cusolver.zpotrs\n    else:\n        msg = (\'dtype must be float32, float64, complex64 or complex128\'\n               \' (actual: {})\'.format(a.dtype))\n        raise ValueError(msg)\n\n    m = a.shape[0]\n    uplo = cublas.CUBLAS_FILL_MODE_LOWER\n\n    worksize = potrf_bufferSize(cusolver_handle, uplo, m, a.data.ptr, m)\n    workspace = cupy.empty(worksize, dtype=dtype)\n\n    # Cholesky factorization\n    potrf(cusolver_handle, uplo, m, a.data.ptr, m, workspace.data.ptr,\n          worksize, dev_info.data.ptr)\n\n    info = dev_info[0]\n    if info != 0:\n        if info < 0:\n            msg = \'\\tThe {}-th parameter is wrong\'.format(-info)\n        else:\n            msg = (\'\\tThe leading minor of order {} is not positive definite\'\n                   .format(info))\n        raise RuntimeError(\'matrix inversion failed at potrf.\\n\' + msg)\n\n    b = cupy.eye(m, dtype=dtype)\n\n    # Solve: A * X = B\n    potrs(cusolver_handle, uplo, m, m, a.data.ptr, m, b.data.ptr, m,\n          dev_info.data.ptr)\n\n    info = dev_info[0]\n    if info > 0:\n        assert False, (\'Unexpected output returned by potrs (actual: {})\'\n                       .format(info))\n    elif info < 0:\n        raise RuntimeError(\'matrix inversion failed at potrs.\\n\'\n                           \'\\tThe {}-th parameter is wrong\'.format(-info))\n\n    return b\n'"
cupyx/optimizing/__init__.py,0,b'from cupyx.optimizing._optimize import optimize  # NOQA\n'
cupyx/optimizing/_optimize.py,0,"b'import contextlib\nimport math\n\nimport optuna\n\nfrom cupy.core import _optimize_config\nfrom cupyx import time\n\n\ndef _optimize(\n        optimize_config, target_func, suggest_func,\n        default_best, ignore_error=()):\n    assert isinstance(optimize_config, _optimize_config._OptimizationConfig)\n    assert callable(target_func)\n    assert callable(suggest_func)\n\n    def objective(trial):\n        args = suggest_func(trial)\n        max_total_time = optimize_config.max_total_time_per_trial\n        try:\n            perf = time.repeat(target_func, args, max_duration=max_total_time)\n            return perf.gpu_times.mean()\n        except Exception as e:\n            if isinstance(e, ignore_error):\n                return math.inf\n            else:\n                raise e\n\n    study = optuna.create_study()\n    study.enqueue_trial(default_best)\n    study.optimize(\n        objective,\n        n_trials=optimize_config.max_trials,\n        timeout=optimize_config.timeout)\n    return study.best_trial\n\n\n@contextlib.contextmanager\ndef optimize(*, key=None, **config_dict):\n    old_context = _optimize_config.get_current_context()\n    context = _optimize_config.get_new_context(key, _optimize, config_dict)\n    _optimize_config.set_current_context(context)\n\n    try:\n        yield context\n    finally:\n        _optimize_config.set_current_context(old_context)\n'"
cupyx/scipy/__init__.py,0,"b'import sys\n\nfrom cupy.core import ndarray\nfrom cupyx.scipy.sparse.base import spmatrix\n\n\ntry:\n    import scipy\n    _scipy_available = True\nexcept ImportError:\n    _scipy_available = False\n\n\n_cupyx_scipy = sys.modules[__name__]\n\n\ndef get_array_module(*args):\n    """"""Returns the array module for arguments.\n\n    This function is used to implement CPU/GPU generic code. If at least one of\n    the arguments is a :class:`cupy.ndarray` object, the :mod:`cupyx.scipy`\n    module is returned.\n\n    Args:\n        args: Values to determine whether NumPy or CuPy should be used.\n\n    Returns:\n        module: :mod:`cupyx.scipy` or :mod:`scipy` is returned based on the\n        types of the arguments.\n\n    """"""\n    for arg in args:\n        if isinstance(arg, (ndarray, spmatrix)):\n            return _cupyx_scipy\n    return scipy\n'"
docs/source/_comparison_generator.py,0,"b""import importlib\n\n\ndef _get_functions(obj):\n    return set([\n        n for n in dir(obj)\n        if (n not in ['test']  # not in blacklist\n            and callable(getattr(obj, n))  # callable\n            and not isinstance(getattr(obj, n), type)  # not class\n            and n[0].islower()  # starts with lower char\n            and not n.startswith('__')  # not special methods\n            )\n    ])\n\n\ndef _import(mod, klass):\n    obj = importlib.import_module(mod)\n    if klass:\n        obj = getattr(obj, klass)\n        return obj, ':meth:`{}.{}.{{}}`'.format(mod, klass)\n    else:\n        # ufunc is not a function\n        return obj, ':obj:`{}.{{}}`'.format(mod)\n\n\ndef _generate_comparison_rst(\n        base_mod, cupy_mod, base_type, klass, exclude_mod):\n    base_obj, base_fmt = _import(base_mod, klass)\n    base_funcs = _get_functions(base_obj)\n    cp_obj, cp_fmt = _import(cupy_mod, klass)\n    cp_funcs = _get_functions(cp_obj)\n\n    if exclude_mod:\n        exclude_obj, _ = _import(exclude_mod, klass)\n        exclude_funcs = _get_functions(exclude_obj)\n        base_funcs -= exclude_funcs\n        cp_funcs -= exclude_funcs\n\n    buf = []\n    buf += [\n        '.. csv-table::',\n        '   :header: {}, CuPy'.format(base_type),\n        '',\n    ]\n    for f in sorted(base_funcs):\n        base_cell = base_fmt.format(f)\n        cp_cell = r'\\-'\n        if f in cp_funcs:\n            cp_cell = cp_fmt.format(f)\n            if getattr(base_obj, f) is getattr(cp_obj, f):\n                cp_cell = '{} (*alias of* {})'.format(cp_cell, base_cell)\n        line = '   {}, {}'.format(base_cell, cp_cell)\n        buf.append(line)\n\n    buf += [\n        '',\n        '.. Summary:',\n        '   Number of NumPy functions: {}'.format(len(base_funcs)),\n        '   Number of functions covered by CuPy: {}'.format(\n            len(cp_funcs & base_funcs)),\n        '   CuPy specific functions:',\n    ] + [\n        '   - {}'.format(f) for f in (cp_funcs - base_funcs)\n    ]\n    return buf\n\n\ndef _section(\n        header, base_mod, cupy_mod,\n        base_type='NumPy', klass=None, exclude=None):\n    return [\n        header,\n        '~' * len(header),\n        '',\n    ] + _generate_comparison_rst(\n        base_mod, cupy_mod, base_type, klass, exclude\n    ) + [\n        '',\n    ]\n\n\ndef generate():\n    buf = []\n\n    buf += [\n        'NumPy / CuPy APIs',\n        '-----------------',\n        '',\n    ]\n    buf += _section(\n        'Module-Level',\n        'numpy', 'cupy')\n    buf += _section(\n        'Multi-Dimensional Array',\n        'numpy', 'cupy', klass='ndarray')\n    buf += _section(\n        'Linear Algebra',\n        'numpy.linalg', 'cupy.linalg')\n    buf += _section(\n        'Discrete Fourier Transform',\n        'numpy.fft', 'cupy.fft')\n    buf += _section(\n        'Random Sampling',\n        'numpy.random', 'cupy.random')\n\n    buf += [\n        'SciPy / CuPy APIs',\n        '-----------------',\n        '',\n    ]\n    buf += _section(\n        'Discrete Fourier Transform',\n        'scipy.fftpack', 'cupyx.scipy.fftpack', 'SciPy')\n    buf += _section(\n        'Sparse Matrices',\n        'scipy.sparse', 'cupyx.scipy.sparse', 'SciPy')\n    buf += _section(\n        'Sparse Linear Algebra',\n        'scipy.sparse.linalg', 'cupyx.scipy.sparse.linalg', 'SciPy')\n    buf += _section(\n        'Advanced Linear Algebra',\n        'scipy.linalg', 'cupyx.scipy.linalg', 'SciPy', exclude='numpy.linalg')\n    buf += _section(\n        'Multidimensional Image Processing',\n        'scipy.ndimage', 'cupyx.scipy.ndimage', 'SciPy')\n    buf += _section(\n        'Special Functions',\n        'scipy.special', 'cupyx.scipy.special', 'SciPy')\n\n    return '\\n'.join(buf)\n"""
docs/source/conf.py,1,"b'# -*- coding: utf-8 -*-\n#\n# CuPy documentation build configuration file, created by\n# sphinx-quickstart on Sun May 10 12:22:10 2015.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport inspect\nimport os\nimport pkg_resources\nimport sys\n\n\nsys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))\nimport _comparison_generator\n\n\n__version__ = pkg_resources.get_distribution(\'cupy\').version\n\non_rtd = os.environ.get(\'READTHEDOCS\', None) == \'True\'\n\nrtd_version = os.environ.get(\'READTHEDOCS_VERSION\')\nif rtd_version == \'latest\':\n    tag = \'master\'\nelse:\n    tag = \'v{}\'.format(__version__)\nextlinks = {\n    \'blob\': (\'https://github.com/cupy/cupy/blob/{}/%s\'.format(tag), \'\'),\n    \'tree\': (\'https://github.com/cupy/cupy/tree/{}/%s\'.format(tag), \'\'),\n}\n\n\n# Generate comparison table.\nwith open(\'reference/comparison_table.rst.inc\', \'w\') as f:\n    f.write(_comparison_generator.generate())\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#sys.path.insert(0, os.path.abspath(\'.\'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\',\n              \'sphinx.ext.autosummary\',\n              \'sphinx.ext.doctest\',\n              \'sphinx.ext.extlinks\',\n              \'sphinx.ext.intersphinx\',\n              \'sphinx.ext.mathjax\',\n              \'sphinx.ext.napoleon\',\n              \'sphinx.ext.linkcode\']\n\ntry:\n    import sphinxcontrib.spelling  # noqa\n    extensions.append(\'sphinxcontrib.spelling\')\nexcept ImportError:\n    pass\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'CuPy\'\ncopyright = u\'2015, Preferred Networks, inc. and Preferred Infrastructure, inc.\'\nauthor = u\'Preferred Networks, inc. and Preferred Infrastructure, inc.\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = __version__\n# The full version, including alpha/beta/rc tags.\nrelease = __version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = []\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n# Suppress a warning that multiple targets are found for a cross-reference.\n# See #3250\nsuppress_warnings = [\'ref.python\']\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n# Napoleon settings\nnapoleon_use_ivar = True\nnapoleon_include_special_with_doc = True\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nif not on_rtd:\n    html_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\nhtml_style = \'css/modified_theme.css\'\n\nif on_rtd:\n    html_context = {\n        \'css_files\': [\n            \'https://media.readthedocs.org/css/sphinx_rtd_theme.css\',\n            \'https://media.readthedocs.org/css/readthedocs-doc-embed.css\',\n            \'_static/css/modified_theme.css\',\n        ],\n    }\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\nhtml_show_sourcelink = False\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'hu\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'ru\', \'sv\', \'tr\'\n#html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# Now only \'ja\' uses this config value\n#html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'CuPydoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'CuPy.tex\', u\'CuPy Documentation\',\n     u\'Preferred Networks, inc. and Preferred Infrastructure, inc.\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'cupy\', u\'CuPy Documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'CuPy\', u\'CuPy Documentation\',\n     author, \'CuPy\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n\nautosummary_generate = True\n\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/3/\', None),\n    \'numpy\': (\'https://docs.scipy.org/doc/numpy/\', None),\n    \'scipy\': (\'https://docs.scipy.org/doc/scipy/reference/\', None),\n    \'chainer\': (\'https://docs.chainer.org/en/latest/\', None),\n}\n\ndoctest_global_setup = \'\'\'\nimport numpy as np\nimport cupy # TODO(okuta) : Remove this line\nimport cupyx\nimport cupy as cp\nnp.random.seed(0)\n\'\'\'\n\nspelling_lang = \'en_US\'\nspelling_word_list_filename = \'spelling_wordlist.txt\'\n\n\ndef _import_object_from_name(module_name, fullname):\n    obj = sys.modules.get(module_name)\n    if obj is None:\n        return None\n    for comp in fullname.split(\'.\'):\n        obj = getattr(obj, comp)\n    return obj\n\n\ndef _is_egg_directory(path):\n    return (path.endswith(\'.egg\') and\n            os.path.isdir(os.path.join(path, \'EGG-INFO\')))\n\n\ndef _is_git_root(path):\n    return os.path.isdir(os.path.join(path, \'.git\'))\n\n\n_source_root = None\n\n\ndef _find_source_root(source_abs_path):\n    # Note that READTHEDOCS* environment variable cannot be used, because they\n    # are not set under docker environment.\n    global _source_root\n    if _source_root is None:\n        dir = os.path.dirname(source_abs_path)\n        while True:\n            if _is_egg_directory(dir) or _is_git_root(dir):\n                # Reached the root directory\n                _source_root = dir\n                break\n\n            dir_ = os.path.dirname(dir)\n            if len(dir_) == len(dir):\n                raise RuntimeError(\'Couldn\\\'t parse root directory from \'\n                                   \'source file: {}\'.format(source_abs_path))\n            dir = dir_\n    return _source_root\n\n\ndef _get_source_relative_path(source_abs_path):\n    return os.path.relpath(source_abs_path, _find_source_root(source_abs_path))\n\n\ndef linkcode_resolve(domain, info):\n    if domain != \'py\' or not info[\'module\']:\n        return None\n\n    # Import the object from module path\n    obj = _import_object_from_name(info[\'module\'], info[\'fullname\'])\n\n    # If it\'s not defined in the internal module, return None.\n    mod = inspect.getmodule(obj)\n    if mod is None:\n        return None\n    if not (mod.__name__ == \'cupy\' or mod.__name__.startswith(\'cupy.\')):\n        return None\n\n    # Get the source file name and line number at which obj is defined.\n    try:\n        filename = inspect.getsourcefile(obj)\n    except TypeError:\n        # obj is not a module, class, function, ..etc.\n        return None\n\n    # inspect can return None for cython objects\n    if filename is None:\n        return None\n\n    # Get the source line number\n    _, linenum = inspect.getsourcelines(obj)\n    assert isinstance(linenum, int)\n\n    filename = os.path.realpath(filename)\n    relpath = _get_source_relative_path(filename)\n\n    return \'https://github.com/cupy/cupy/blob/{}/{}#L{}\'.format(\n        tag, relpath, linenum)\n'"
examples/cg/cg.py,7,"b'import argparse\nimport contextlib\nimport time\n\nimport numpy as np\n\nimport cupy\n\n\n@contextlib.contextmanager\ndef timer(message):\n    cupy.cuda.Stream.null.synchronize()\n    start = time.time()\n    yield\n    cupy.cuda.Stream.null.synchronize()\n    end = time.time()\n    print(\'%s:  %f sec\' % (message, end - start))\n\n\ndef fit(A, b, tol, max_iter):\n    # Note that this function works even tensors \'A\' and \'b\' are NumPy or CuPy\n    # arrays.\n    xp = cupy.get_array_module(A)\n    x = xp.zeros_like(b, dtype=np.float64)\n    r0 = b - xp.dot(A, x)\n    p = r0\n    for i in range(max_iter):\n        a = xp.inner(r0, r0) / xp.inner(p, xp.dot(A, p))\n        x += a * p\n        r1 = r0 - a * xp.dot(A, p)\n        if xp.linalg.norm(r1) < tol:\n            return x\n        b = xp.inner(r1, r1) / xp.inner(r0, r0)\n        p = r1 + b * p\n        r0 = r1\n    print(\'Failed to converge. Increase max-iter or tol.\')\n    return x\n\n\ndef run(gpu_id, tol, max_iter):\n    """"""CuPy Conjugate gradient example\n\n    Solve simultaneous linear equations, Ax = b.\n    \'A\' and \'x\' are created randomly and \'b\' is computed by \'Ax\' at first.\n    Then, \'x\' is computed from \'A\' and \'b\' in two ways, namely with CPU and\n    GPU. To evaluate the accuracy of computation, the Euclidean distances\n    between the answer \'x\' and the reconstructed \'x\' are computed.\n\n    """"""\n    for repeat in range(3):\n        print(\'Trial: %d\' % repeat)\n        # Create the large symmetric matrix \'A\'.\n        N = 2000\n        A = np.random.randint(-50, 50, size=(N, N))\n        A = (A + A.T).astype(np.float64)\n        x_ans = np.random.randint(-50, 50, size=N).astype(np.float64)\n        b = np.dot(A, x_ans)\n\n        print(\'Running CPU...\')\n        with timer(\' CPU \'):\n            x_cpu = fit(A, b, tol, max_iter)\n        print(np.linalg.norm(x_cpu - x_ans))\n\n        with cupy.cuda.Device(gpu_id):\n            A_gpu = cupy.asarray(A)\n            b_gpu = cupy.asarray(b)\n            print(\'Running GPU...\')\n            with timer(\' GPU \'):\n                x_gpu = fit(A_gpu, b_gpu, tol, max_iter)\n            print(np.linalg.norm(cupy.asnumpy(x_gpu) - x_ans))\n\n        print()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu-id\', \'-g\', default=0, type=int,\n                        help=\'ID of GPU.\')\n    parser.add_argument(\'--tol\', \'-t\', default=0.1, type=float,\n                        help=\'tolerance to stop iteration\')\n    parser.add_argument(\'--max-iter\', \'-m\', default=5000, type=int,\n                        help=\'number of iterations\')\n    args = parser.parse_args()\n    run(args.gpu_id, args.tol, args.max_iter)\n'"
examples/cutensor/contraction.py,0,"b""#\n# C_{m,u,n,v} = alpha * A_{m,h,k,n} * B_{u,k,v,h} + beta * C_{m,u,n,v}\n#\nimport numpy\nimport cupy\nfrom cupy import cutensor\nfrom cupy.cuda import stream\n\ndtype = numpy.float32\n\nmode_a = ('m', 'h', 'k', 'n')\nmode_b = ('u', 'k', 'v', 'h')\nmode_c = ('m', 'u', 'n', 'v')\n\nextent = {'m': 96, 'n': 96, 'u': 96, 'v': 64, 'h': 64, 'k': 64}\n\na = cupy.random.random([extent[i] for i in mode_a])\nb = cupy.random.random([extent[i] for i in mode_b])\nc = cupy.random.random([extent[i] for i in mode_c])\na = a.astype(dtype)\nb = b.astype(dtype)\nc = c.astype(dtype)\n\ndesc_a = cutensor.create_tensor_descriptor(a)\ndesc_b = cutensor.create_tensor_descriptor(b)\ndesc_c = cutensor.create_tensor_descriptor(c)\n\nalpha = 1.1\nbeta = 1.0\n\n# rehearsal\nc = cutensor.contraction(alpha, a, desc_a, mode_a, b, desc_b, mode_b,\n                         beta, c, desc_c, mode_c)\n\nev_start = stream.Event()\nev_end = stream.Event()\nst = stream.Stream()\nwith st:\n    # measurement\n    ev_start.record()\n    c = cutensor.contraction(alpha, a, desc_a, mode_a, b, desc_b, mode_b,\n                             beta, c, desc_c, mode_c)\n    ev_end.record()\nst.synchronize()\n\nelapsed_ms = stream.get_elapsed_time(ev_start, ev_end)\ntotal_flops = 2 * numpy.prod(numpy.array(list(extent.values())))\n\nprint('dtype: {}'.format(numpy.dtype(dtype).name))\nprint('time (ms): {}'.format(elapsed_ms))\nprint('GFLOPS: {}'.format(total_flops / elapsed_ms / 1e6))\n"""
examples/cutensor/elementwise_binary.py,0,"b""#\n# D_{x,y,z} = alpha * A_{z,y,x} + gamma * C_{x,y,z}\n#\nimport numpy\nimport cupy\nfrom cupy import cutensor\nfrom cupy.cuda import stream\n\ndtype = numpy.float32\n\nmode_a = ('z', 'y', 'x')\nmode_c = ('x', 'y', 'z')\n\nextent = {'x': 400, 'y': 200, 'z': 300}\n\na = cupy.random.random([extent[i] for i in mode_a])\nc = cupy.random.random([extent[i] for i in mode_c])\na = a.astype(dtype)\nc = c.astype(dtype)\n\ndesc_a = cutensor.create_tensor_descriptor(a)\ndesc_c = cutensor.create_tensor_descriptor(c)\n\nalpha = 1.1\ngamma = 1.3\n\n# rehearsal\nd = cutensor.elementwise_binary(alpha, a, desc_a, mode_a,\n                                gamma, c, desc_c, mode_c)\n\nev_start = stream.Event()\nev_end = stream.Event()\nst = stream.Stream()\nwith st:\n    # measurement\n    ev_start.record()\n    d = cutensor.elementwise_binary(alpha, a, desc_a, mode_a,\n                                    gamma, c, desc_c, mode_c)\n    ev_end.record()\nst.synchronize()\n\nelapsed_ms = stream.get_elapsed_time(ev_start, ev_end)\ntransfer_byte = d.size * d.itemsize\nif alpha != 0.0:\n    transfer_byte += a.size * a.itemsize\nif gamma != 0.0:\n    transfer_byte += c.size * c.itemsize\ngbs = transfer_byte / elapsed_ms / 1e6\n\nprint('dtype: {}'.format(numpy.dtype(dtype).name))\nprint('time (ms): {}'.format(elapsed_ms))\nprint('effective memory bandwidth (GB/s): {}'.format(gbs))\n"""
examples/cutensor/elementwise_trinary.py,0,"b""#\n# D_{x,y,z} = alpha * A_{z,y,x} + beta * B_{y,z,x} + gamma * C_{x,y,z}\n#\nimport numpy\nimport cupy\nfrom cupy import cutensor\nfrom cupy.cuda import stream\n\ndtype = numpy.float32\n\nmode_a = ('z', 'y', 'x')\nmode_b = ('y', 'z', 'x')\nmode_c = ('x', 'y', 'z')\n\nextent = {'x': 400, 'y': 200, 'z': 300}\n\na = cupy.random.random([extent[i] for i in mode_a])\nb = cupy.random.random([extent[i] for i in mode_b])\nc = cupy.random.random([extent[i] for i in mode_c])\na = a.astype(dtype)\nb = b.astype(dtype)\nc = c.astype(dtype)\n\ndesc_a = cutensor.create_tensor_descriptor(a)\ndesc_b = cutensor.create_tensor_descriptor(b)\ndesc_c = cutensor.create_tensor_descriptor(c)\n\nalpha = 1.1\nbeta = 1.2\ngamma = 1.3\n\n# rehearsal\nd = cutensor.elementwise_trinary(alpha, a, desc_a, mode_a,\n                                 beta,  b, desc_b, mode_b,\n                                 gamma, c, desc_c, mode_c)\n\nev_start = stream.Event()\nev_end = stream.Event()\nst = stream.Stream()\nwith st:\n    # measurement\n    ev_start.record()\n    d = cutensor.elementwise_trinary(alpha, a, desc_a, mode_a,\n                                     beta,  b, desc_b, mode_b,\n                                     gamma, c, desc_c, mode_c)\n    ev_end.record()\nst.synchronize()\n\nelapsed_ms = stream.get_elapsed_time(ev_start, ev_end)\ntransfer_byte = d.size * d.itemsize\nif alpha != 0.0:\n    transfer_byte += a.size * a.itemsize\nif beta != 0.0:\n    transfer_byte += b.size * b.itemsize\nif gamma != 0.0:\n    transfer_byte += c.size * c.itemsize\ngbs = transfer_byte / elapsed_ms / 1e6\n\nprint('dtype: {}'.format(numpy.dtype(dtype).name))\nprint('time (ms): {}'.format(elapsed_ms))\nprint('effective memory bandwidth (GB/s): {}'.format(gbs))\n"""
examples/cutensor/reduction.py,0,"b""#\n# C_{m,v} = alpha * A_{m,h,k,v} + beta * C_{m,v}\n#\nimport numpy\nimport cupy\nfrom cupy import cutensor\nfrom cupy.cuda import stream\n\ndtype = numpy.float32\n\nmode_a = ('m', 'h', 'k', 'v')\nmode_c = ('m', 'v')\n\nextent = {'m': 196, 'h': 256, 'k': 64, 'v': 64}\n\na = cupy.random.random([extent[i] for i in mode_a])\nc = cupy.random.random([extent[i] for i in mode_c])\na = a.astype(dtype)\nc = c.astype(dtype)\n\ndesc_a = cutensor.create_tensor_descriptor(a)\ndesc_c = cutensor.create_tensor_descriptor(c)\n\nalpha = 1.0\nbeta = 0.1\n\n# rehearsal\nc = cutensor.reduction(alpha, a, desc_a, mode_a,\n                       beta, c, desc_c, mode_c)\n\nev_start = stream.Event()\nev_end = stream.Event()\nst = stream.Stream()\nwith st:\n    # measurement\n    ev_start.record()\n    c = cutensor.reduction(alpha, a, desc_a, mode_a,\n                           beta, c, desc_c, mode_c)\n    ev_end.record()\nst.synchronize()\n\nelapsed_ms = stream.get_elapsed_time(ev_start, ev_end)\ntransfer_byte = a.size * a.itemsize + c.size * c.itemsize\nif beta != 0.0:\n    transfer_byte += c.size * c.itemsize\ngbs = transfer_byte / elapsed_ms / 1e6\n\nprint('dtype: {}'.format(numpy.dtype(dtype).name))\nprint('time (ms): {}'.format(elapsed_ms))\nprint('effective memory bandwidth (GB/s): {}'.format(gbs))\n"""
examples/finance/black_scholes.py,0,"b""import argparse\nimport contextlib\nimport time\n\nimport cupy\nimport numpy\n\n# This sample computes call and put prices for European options with\n# Black-Scholes equation. It was based on a sample of the financial package\n# in CUDA toolkit. For details, please see the corresponding whitepaper.\n#\n# The following code shows that CuPy enables us to write algorithms for GPUs\n# without significantly modifying the existing NumPy's code.\n# It also briefly describes how to create your own kernel with CuPy.\n# If you want to speed up the existing code, please define the kernel\n# with cupy.ElementwiseKernel.\n\n\n# Naive implementation of the pricing of options with NumPy and CuPy.\ndef black_scholes(xp, s, x, t, r, v):\n    sqrt_t = xp.sqrt(t)\n    d1 = (xp.log(s / x) + (r + v * v / 2) * t) / (v * sqrt_t)\n    d2 = d1 - v * sqrt_t\n\n    def get_cumulative_normal_distribution(x):\n        A1 = 0.31938153\n        A2 = -0.356563782\n        A3 = 1.781477937\n        A4 = -1.821255978\n        A5 = 1.330274429\n        RSQRT2PI = 0.39894228040143267793994605993438\n        W = 0.2316419\n\n        k = 1 / (1 + W * xp.abs(x))\n        cnd = RSQRT2PI * xp.exp(-x * x / 2) * \\\n            (k * (A1 + k * (A2 + k * (A3 + k * (A4 + k * A5)))))\n        cnd = xp.where(x > 0, 1 - cnd, cnd)\n        return cnd\n\n    cnd_d1 = get_cumulative_normal_distribution(d1)\n    cnd_d2 = get_cumulative_normal_distribution(d2)\n\n    exp_rt = xp.exp(- r * t)\n    call = s * cnd_d1 - x * exp_rt * cnd_d2\n    put = x * exp_rt * (1 - cnd_d2) - s * (1 - cnd_d1)\n    return call, put\n\n\n# An example of calling the kernel via cupy.ElementwiseKernel.\n# When executing __call__ method of the instance, it automatically compiles\n# the code depending on the types of the given arrays, and calls the kernel.\n# Other functions used inside the kernel can be defined by 'preamble' option.\nblack_scholes_kernel = cupy.ElementwiseKernel(\n    'T s, T x, T t, T r, T v',  # Inputs\n    'T call, T put',  # Outputs\n    '''\n    const T sqrt_t = sqrt(t);\n    const T d1 = (log(s / x) + (r + v * v / 2) * t) / (v * sqrt_t);\n    const T d2 = d1 - v * sqrt_t;\n\n    const T cnd_d1 = get_cumulative_normal_distribution(d1);\n    const T cnd_d2 = get_cumulative_normal_distribution(d2);\n\n    const T exp_rt = exp(- r * t);\n    call = s * cnd_d1 - x * exp_rt * cnd_d2;\n    put = x * exp_rt * (1 - cnd_d2) - s * (1 - cnd_d1);\n    ''',\n    'black_scholes_kernel',\n    preamble='''\n    __device__\n    inline T get_cumulative_normal_distribution(T x) {\n        const T A1 = 0.31938153;\n        const T A2 = -0.356563782;\n        const T A3 = 1.781477937;\n        const T A4 = -1.821255978;\n        const T A5 = 1.330274429;\n        const T RSQRT2PI = 0.39894228040143267793994605993438;\n        const T W = 0.2316419;\n\n        const T k = 1 / (1 + W * abs(x));\n        T cnd = RSQRT2PI * exp(- x * x / 2) *\n            (k * (A1 + k * (A2 + k * (A3 + k * (A4 + k * A5)))));\n        if (x > 0) {\n            cnd = 1 - cnd;\n        }\n        return cnd;\n    }\n    ''',\n)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu-id', '-g', default=0, type=int, help='GPU ID')\n    parser.add_argument('--n-options', '-n', default=10000000, type=int)\n    args = parser.parse_args()\n\n    cupy.cuda.Device(args.gpu_id).use()\n\n    def rand_range(m, M):\n        samples = cupy.random.rand(args.n_options)\n        return (m + (M - m) * samples).astype(numpy.float64)\n\n    print('initializing...')\n    stock_price_gpu = rand_range(5, 30)\n    option_strike_gpu = rand_range(1, 100)\n    option_years_gpu = rand_range(0.25, 10)\n\n    stock_price_cpu = stock_price_gpu.get()\n    option_strike_cpu = option_strike_gpu.get()\n    option_years_cpu = option_years_gpu.get()\n\n    @contextlib.contextmanager\n    def timer(message):\n        cupy.cuda.Stream.null.synchronize()\n        start = time.time()\n        yield\n        cupy.cuda.Stream.null.synchronize()\n        end = time.time()\n        print('%s:\\t%f sec' % (message, end - start))\n\n    print('start computation')\n    risk_free = 0.02\n    volatility = 0.3\n    with timer(' CPU (NumPy, Naive implementation)'):\n        call_cpu, put_cpu = black_scholes(\n            numpy, stock_price_cpu, option_strike_cpu, option_years_cpu,\n            risk_free, volatility)\n\n    with timer(' GPU (CuPy, Naive implementation)'):\n        call_gpu1, put_gpu1 = black_scholes(\n            cupy, stock_price_gpu, option_strike_gpu, option_years_gpu,\n            risk_free, volatility)\n\n    with timer(' GPU (CuPy, Elementwise kernel)'):\n        call_gpu2, put_gpu2 = black_scholes_kernel(\n            stock_price_gpu, option_strike_gpu, option_years_gpu,\n            risk_free, volatility)\n\n    # Check whether all elements in gpu arrays are equal to those of cpus\n    cupy.testing.assert_allclose(call_cpu, call_gpu1)\n    cupy.testing.assert_allclose(call_cpu, call_gpu2)\n    cupy.testing.assert_allclose(put_cpu, put_gpu1)\n    cupy.testing.assert_allclose(put_cpu, put_gpu2)\n"""
examples/finance/monte_carlo.py,0,"b""import argparse\nimport contextlib\nimport sys\nimport time\n\nimport cupy\nimport numpy\n\nfrom black_scholes import black_scholes_kernel\n\n# This sample computes call prices for European options with\n# Monte-Carlo simulation. It was based on a sample of the financial package\n# in CUDA toolkit. For details, please see the corresponding whitepaper.\n#\n# The present price of an option can also be represented as a discounted\n# expectation of the option price under a risk-neutral measure.\n# Since it is assumed that the stock price follows a lognormal distribution,\n# the call price for European option can be evaluated by approximating the\n# risk-neutral expectation at the time of exercise with the Monte-Carlo method.\n# Note that as current ElementwiseKernel does not support 'curand'\n# due to nvrtc, this sample manually implements a pseudorandom function.\n\n\nmonte_carlo_kernel = cupy.ElementwiseKernel(\n    'T s, T x, T t, T r, T v, int32 n_samples, int32 seed', 'T call',\n    '''\n    // We can use special variables i and _ind to get the index of the thread.\n    // In this case, we used an index as a seed of random sequence.\n    uint64_t rand_state[2];\n    init_state(rand_state, i, seed);\n\n    T call_sum = 0;\n    const T v_by_sqrt_t = v * sqrt(t);\n    const T mu_by_t = (r - v * v / 2) * t;\n\n    // compute the price of the call option with Monte Carlo method\n    for (int i = 0; i < n_samples; ++i) {\n        const T p = sample_normal(rand_state);\n        call_sum += get_call_value(s, x, p, mu_by_t, v_by_sqrt_t);\n    }\n    // convert the future value of the call option to the present value\n    const T discount_factor = exp(- r * t);\n    call = discount_factor * call_sum / n_samples;\n    ''',\n    preamble='''\n    typedef unsigned long long uint64_t;\n\n    __device__\n    inline T get_call_value(T s, T x, T p, T mu_by_t, T v_by_sqrt_t) {\n        const T call_value = s * exp(mu_by_t + v_by_sqrt_t * p) - x;\n        return (call_value > 0) ? call_value : 0;\n    }\n\n    // Initialize state\n    __device__ inline void init_state(uint64_t* a, int i, int seed) {\n        a[0] = i + 1;\n        a[1] = 0x5c721fd808f616b6 + seed;\n    }\n\n    __device__ inline uint64_t xorshift128plus(uint64_t* x) {\n        uint64_t s1 = x[0];\n        uint64_t s0 = x[1];\n        x[0] = s0;\n        s1 = s1 ^ (s1 << 23);\n        s1 = s1 ^ (s1 >> 17);\n        s1 = s1 ^ s0;\n        s1 = s1 ^ (s0 >> 26);\n        x[1] = s1;\n        return s0 + s1;\n    }\n\n    // Draw a sample from an uniform distribution in a range of [0, 1]\n    __device__ inline T sample_uniform(uint64_t* state) {\n        const uint64_t x = xorshift128plus(state);\n        // 18446744073709551615 = 2^64 - 1\n        return T(x) / T(18446744073709551615);\n    }\n\n    // Draw a sample from a normal distribution with N(0, 1)\n    __device__ inline T sample_normal(uint64_t* state) {\n        T x = sample_uniform(state);\n        T s = T(-1.4142135623730950488016887242097);  // = -sqrt(2)\n        if (x > 0.5) {\n            x = 1 - x;\n            s = -s;\n        }\n        T p = x + T(0.5);\n        return s * erfcinv(2 * p);\n    }\n    ''',\n)\n\n\ndef compute_option_prices(\n        stock_price, option_strike, option_years, risk_free, volatility,\n        n_threads_per_option, n_samples_per_thread, seed=0):\n\n    n_options = len(stock_price)\n    call_prices = cupy.empty(\n        (n_options, n_threads_per_option), dtype=numpy.float64)\n    # Because of the broadcasting rule, in this case this kernel\n    # launches n_options * n_threads_per_options threads\n    # each of which corresponds to the element of 'call_prices'.\n    monte_carlo_kernel(\n        stock_price[:, None], option_strike[:, None], option_years[:, None],\n        risk_free, volatility, n_samples_per_thread, seed, call_prices)\n    return call_prices.mean(axis=1)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu-id', '-g', default=0, type=int, help='GPU ID')\n    parser.add_argument('--n-options', default=1000, type=int)\n    parser.add_argument('--n-samples-per-thread', default=1000, type=int)\n    parser.add_argument('--n-threads-per-option', default=100000, type=int)\n    args = parser.parse_args()\n\n    cupy.cuda.Device(args.gpu_id).use()\n\n    def rand_range(m, M):\n        samples = cupy.random.rand(args.n_options)\n        return (m + (M - m) * samples).astype(numpy.float64)\n\n    print('initializing...')\n    stock_price = rand_range(5, 30)\n    option_strike = rand_range(1, 100)\n    option_years = rand_range(0.25, 10)\n    risk_free = 0.02\n    volatility = 0.3\n\n    @contextlib.contextmanager\n    def timer(message):\n        cupy.cuda.Stream.null.synchronize()\n        start = time.time()\n        yield\n        cupy.cuda.Stream.null.synchronize()\n        end = time.time()\n        print('%s:\\t%f sec' % (message, end - start))\n\n    print('start computation')\n    print('    # of options: {}'.format(args.n_options))\n    print('    # of samples per option: {}'.format(\n        args.n_samples_per_thread * args.n_threads_per_option))\n    with timer('GPU (CuPy, Monte Carlo method)'):\n        call_mc = compute_option_prices(\n            stock_price, option_strike, option_years, risk_free, volatility,\n            args.n_threads_per_option, args.n_samples_per_thread)\n\n    # Compute the error between the value of the exact solution\n    # and that of the Monte-Carlo simulation\n    call_bs, _ = black_scholes_kernel(\n        stock_price, option_strike, option_years, risk_free, volatility)\n    error = cupy.std(call_mc - call_bs)\n    print('Error: %f' % error)\n    return 0\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n"""
examples/finance/monte_carlo_multigpu.py,0,"b""import argparse\nimport contextlib\nimport sys\nimport time\n\nimport cupy\nimport numpy\n\nfrom black_scholes import black_scholes_kernel\nfrom monte_carlo import monte_carlo_kernel\n\n# CuPy also implements a feature to call kernels in different GPUs.\n# Through this sample, we will explain how to allocate arrays\n# in different devices, and call kernels in parallel.\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpus', type=int, nargs='*',\n                        default=[0], help='GPU IDs')\n    parser.add_argument('--n-options', default=1000, type=int)\n    parser.add_argument('--n-samples-per-thread', default=1000, type=int)\n    parser.add_argument('--n-threads-per-option', default=10000, type=int)\n    args = parser.parse_args()\n\n    if len(args.gpus) == 0:\n        print('At least one GPU is required.')\n        sys.exit(1)\n\n    def rand_range(m, M):\n        samples = numpy.random.rand(args.n_options)\n        return (m + (M - m) * samples).astype(numpy.float64)\n\n    print('initializing...')\n    stock_price_cpu = rand_range(5, 30)\n    option_strike_cpu = rand_range(1, 100)\n    option_years_cpu = rand_range(0.25, 10)\n    risk_free = 0.02\n    volatility = 0.3\n\n    stock_price_gpus = []\n    option_strike_gpus = []\n    option_years_gpus = []\n    call_prices_gpus = []\n    print('start computation')\n    print('    # of gpus: {}'.format(len(args.gpus)))\n    print('    # of options: {}'.format(args.n_options))\n    print('    # of samples per option: {}'.format(\n        len(args.gpus) * args.n_samples_per_thread * args.n_threads_per_option)\n    )\n    # Allocate arrays in different devices\n    for gpu_id in args.gpus:\n        with cupy.cuda.Device(gpu_id):\n            stock_price_gpus.append(cupy.array(stock_price_cpu))\n            option_strike_gpus.append(cupy.array(option_strike_cpu))\n            option_years_gpus.append(cupy.array(option_years_cpu))\n            call_prices_gpus.append(cupy.empty(\n                (args.n_options, args.n_threads_per_option),\n                dtype=numpy.float64))\n\n    @contextlib.contextmanager\n    def timer(message):\n        cupy.cuda.Stream.null.synchronize()\n        start = time.time()\n        yield\n        cupy.cuda.Stream.null.synchronize()\n        end = time.time()\n        print('%s:\\t%f sec' % (message, end - start))\n\n    with timer('GPU (CuPy, Monte Carlo method)'):\n        for i, gpu_id in enumerate(args.gpus):\n            # Performs Monte-Carlo simulations in parallel\n            with cupy.cuda.Device(gpu_id):\n                monte_carlo_kernel(\n                    stock_price_gpus[i][:, None],\n                    option_strike_gpus[i][:, None],\n                    option_years_gpus[i][:, None],\n                    risk_free, volatility, args.n_samples_per_thread, i,\n                    call_prices_gpus[i])\n\n    # Transfer the result from the GPUs\n    call_prices = [c.get() for c in call_prices_gpus]\n    call_mc = numpy.concatenate(call_prices).reshape(\n        len(args.gpus), args.n_options, args.n_threads_per_option)\n    call_mc = call_mc.mean(axis=(0, 2))\n    # Compute the error between the value of the exact solution\n    # and that of the Monte-Carlo simulation\n    with cupy.cuda.Device(args.gpus[0]):\n        call_bs = black_scholes_kernel(\n            stock_price_gpus[0], option_strike_gpus[0], option_years_gpus[0],\n            risk_free, volatility)[0].get()\n    error = cupy.std(call_mc - call_bs)\n    print('Error: %f' % error)\n"""
examples/gemm/sgemm.py,5,"b""import argparse\nimport math\nimport os\n\nimport cupy as cp\nimport numpy as np\n\nfrom utils import benchmark\nfrom utils import read_code\n\n\nsgemm_file = os.path.join(os.path.dirname(__file__), 'sgemm.cu')\n\n\ndef sgemm(A, B,\n          dim_x=16, dim_y=16, blk_m=64, blk_n=64, blk_k=4,\n          dim_xa=64, dim_ya=4, dim_xb=4, dim_yb=64):\n    assert A.dtype == cp.float32\n    assert B.dtype == cp.float32\n    assert(dim_x * dim_y == dim_xa * dim_ya == dim_xb * dim_yb)\n\n    m, k = A.shape\n    k, n = B.shape\n\n    # Inputs matrices need to be in Fortran order.\n    A = cp.asfortranarray(A)\n    B = cp.asfortranarray(B)\n\n    C = cp.empty((m, n), dtype=cp.float32, order='F')\n\n    config = {'DIM_X': dim_x, 'DIM_Y': dim_y,\n              'BLK_M': blk_m, 'BLK_N': blk_n, 'BLK_K': blk_k,\n              'DIM_XA': dim_xa, 'DIM_YA': dim_ya,\n              'DIM_XB': dim_xb, 'DIM_YB': dim_yb,\n              'THR_M': blk_m // dim_x, 'THR_N': blk_n // dim_y}\n    code = read_code(sgemm_file, params=config)\n    kern = cp.RawKernel(code, 'sgemm')\n\n    grid = (int(math.ceil(m / blk_m)), int(math.ceil(n / blk_n)), 1)\n    block = (dim_x, dim_y, 1)\n    args = (m, n, k, A, B, C)\n    shared_mem = blk_k * (blk_m + 1) * 4 + blk_n * (blk_k + 1) * 4\n    kern(grid, block, args=args, shared_mem=shared_mem)\n    return C\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='SGEMM kernel call from CuPy')\n    parser.add_argument('--gpu', '-g', default=0, type=int,\n                        help='ID of GPU.')\n    parser.add_argument(\n        '--m', type=int, default=np.random.randint(1000, 1500))\n    parser.add_argument(\n        '--n', type=int, default=np.random.randint(1000, 1500))\n    parser.add_argument(\n        '--k', type=int, default=np.random.randint(500, 3000))\n    args = parser.parse_args()\n\n    print('m={} n={} k={}'.format(args.m, args.n, args.k))\n    print('start benchmarking')\n    print('')\n\n    with cp.cuda.Device(args.gpu):\n        A = cp.random.uniform(\n            low=-1., high=1., size=(args.m, args.k)).astype(cp.float32)\n        B = cp.random.uniform(\n            low=-1., high=1., size=(args.k, args.n)).astype(cp.float32)\n\n        # check correctness\n        cp.testing.assert_array_almost_equal(\n            sgemm(A, B), cp.dot(A, B), decimal=3)\n\n        # dry run\n        for _ in range(3):\n            sgemm(A, B)\n        kernel_times = benchmark(sgemm, (A, B), n_run=5)\n\n        for _ in range(3):\n            cp.dot(A, B)\n        cublas_times = benchmark(cp.dot, (A, B), n_run=5)\n\n    print('=============================Result===============================')\n    print('hand written kernel time {} ms'.format(np.mean(kernel_times)))\n    print('cuBLAS              time {} ms'.format(np.mean(cublas_times)))\n\n\nif __name__ == '__main__':\n    main()\n"""
examples/gemm/utils.py,0,"b""import cupy as cp\n\n\ndef read_code(code_filename, params):\n    with open(code_filename, 'r') as f:\n        code = f.read()\n    for k, v in params.items():\n        code = '#define ' + k + ' ' + str(v) + '\\n' + code\n    return code\n\n\ndef benchmark(func, args, n_run):\n    times = []\n    for _ in range(n_run):\n        start = cp.cuda.Event()\n        end = cp.cuda.Event()\n        start.record()\n        func(*args)\n        end.record()\n        end.synchronize()\n        times.append(cp.cuda.get_elapsed_time(start, end))  # milliseconds\n    return times\n"""
examples/gmm/gmm.py,21,"b'import argparse\nimport contextlib\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\n\nimport cupy\n\n\n@contextlib.contextmanager\ndef timer(message):\n    cupy.cuda.Stream.null.synchronize()\n    start = time.time()\n    yield\n    cupy.cuda.Stream.null.synchronize()\n    end = time.time()\n    print(\'%s:  %f sec\' % (message, end - start))\n\n\ndef estimate_log_prob(X, inv_cov, means):\n    xp = cupy.get_array_module(X)\n    n_features = X.shape[1]\n    log_det = xp.sum(xp.log(inv_cov), axis=1)\n    precisions = inv_cov ** 2\n    log_prob = xp.sum((means ** 2 * precisions), 1) - \\\n        2 * xp.dot(X, (means * precisions).T) + xp.dot(X ** 2, precisions.T)\n    return -0.5 * (n_features * xp.log(2 * np.pi) + log_prob) + log_det\n\n\ndef m_step(X, resp):\n    xp = cupy.get_array_module(X)\n    nk = xp.sum(resp, axis=0)\n    means = xp.dot(resp.T, X) / nk[:, None]\n    X2 = xp.dot(resp.T, X * X) / nk[:, None]\n    covariances = X2 - means ** 2\n    return nk / len(X), means, covariances\n\n\ndef e_step(X, inv_cov, means, weights):\n    xp = cupy.get_array_module(X)\n    weighted_log_prob = estimate_log_prob(X, inv_cov, means) + \\\n        xp.log(weights)\n    log_prob_norm = xp.log(xp.sum(xp.exp(weighted_log_prob), axis=1))\n    log_resp = weighted_log_prob - log_prob_norm[:, None]\n    return xp.mean(log_prob_norm), log_resp\n\n\ndef train_gmm(X, max_iter, tol, means, covariances):\n    xp = cupy.get_array_module(X)\n    lower_bound = -np.infty\n    converged = False\n    weights = xp.array([0.5, 0.5], dtype=np.float32)\n    inv_cov = 1 / xp.sqrt(covariances)\n\n    for n_iter in range(max_iter):\n        prev_lower_bound = lower_bound\n        log_prob_norm, log_resp = e_step(X, inv_cov, means, weights)\n        weights, means, covariances = m_step(X, xp.exp(log_resp))\n        inv_cov = 1 / xp.sqrt(covariances)\n        lower_bound = log_prob_norm\n        change = lower_bound - prev_lower_bound\n        if abs(change) < tol:\n            converged = True\n            break\n\n    if not converged:\n        print(\'Failed to converge. Increase max-iter or tol.\')\n\n    return inv_cov, means, weights, covariances\n\n\ndef predict(X, inv_cov, means, weights):\n    xp = cupy.get_array_module(X)\n    log_prob = estimate_log_prob(X, inv_cov, means)\n    return (log_prob + xp.log(weights)).argmax(axis=1)\n\n\ndef calc_acc(X_train, y_train, X_test, y_test, max_iter, tol, means,\n             covariances):\n    xp = cupy.get_array_module(X_train)\n    inv_cov, means, weights, cov = \\\n        train_gmm(X_train, max_iter, tol, means, covariances)\n    y_train_pred = predict(X_train, inv_cov, means, weights)\n    train_accuracy = xp.mean(y_train_pred == y_train) * 100\n    y_test_pred = predict(X_test, inv_cov, means, weights)\n    test_accuracy = xp.mean(y_test_pred == y_test) * 100\n    print(\'train_accuracy : %f\' % train_accuracy)\n    print(\'test_accuracy : %f\' % test_accuracy)\n    return y_test_pred, means, cov\n\n\ndef draw(X, pred, means, covariances, output):\n    xp = cupy.get_array_module(X)\n    for i in range(2):\n        labels = X[pred == i]\n        if xp is cupy:\n            labels = labels.get()\n        plt.scatter(labels[:, 0], labels[:, 1], c=np.random.rand(1, 3))\n    if xp is cupy:\n        means = means.get()\n        covariances = covariances.get()\n    plt.scatter(means[:, 0], means[:, 1], s=120, marker=\'s\', facecolors=\'y\',\n                edgecolors=\'k\')\n    x = np.linspace(-5, 5, 1000)\n    y = np.linspace(-5, 5, 1000)\n    X, Y = np.meshgrid(x, y)\n    for i in range(2):\n        dist = stats.multivariate_normal(means[i], covariances[i])\n        Z = dist.pdf(np.stack([X, Y], axis=-1))\n        plt.contour(X, Y, Z)\n    plt.savefig(output)\n\n\ndef run(gpuid, num, dim, max_iter, tol, output):\n    """"""CuPy Gaussian Mixture Model example\n\n    Compute GMM parameters, weights, means and covariance matrix, depending on\n    sampled data. There are two main components, e_step and m_step.\n    In e_step, compute burden rate, which is expressed `resp`, by latest\n    weights, means and covariance matrix.\n    In m_step, compute weights, means and covariance matrix by latest `resp`.\n\n    """"""\n    scale = np.ones(dim)\n    train1 = np.random.normal(1, scale, size=(num, dim)).astype(np.float32)\n    train2 = np.random.normal(-1, scale, size=(num, dim)).astype(np.float32)\n    X_train = np.r_[train1, train2]\n    test1 = np.random.normal(1, scale, size=(100, dim)).astype(np.float32)\n    test2 = np.random.normal(-1, scale, size=(100, dim)).astype(np.float32)\n    X_test = np.r_[test1, test2]\n    y_train = np.r_[np.zeros(num), np.ones(num)].astype(np.int32)\n    y_test = np.r_[np.zeros(100), np.ones(100)].astype(np.int32)\n\n    mean1 = np.random.normal(1, scale, size=dim)\n    mean2 = np.random.normal(-1, scale, size=dim)\n    means = np.stack([mean1, mean2])\n    covariances = np.random.rand(2, dim)\n    print(\'Running CPU...\')\n    with timer(\' CPU \'):\n        y_test_pred, means, cov = \\\n            calc_acc(X_train, y_train, X_test, y_test, max_iter, tol,\n                     means, covariances)\n\n    with cupy.cuda.Device(gpuid):\n        X_train_gpu = cupy.array(X_train)\n        y_train_gpu = cupy.array(y_train)\n        y_test_gpu = cupy.array(y_test)\n        X_test_gpu = cupy.array(X_test)\n        means = cupy.array(means)\n        covariances = cupy.array(covariances)\n        print(\'Running GPU...\')\n        with timer(\' GPU \'):\n            y_test_pred, means, cov = \\\n                calc_acc(X_train_gpu, y_train_gpu, X_test_gpu, y_test_gpu,\n                         max_iter, tol, means, covariances)\n        if output is not None:\n            draw(X_test_gpu, y_test_pred, means, cov, output)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu-id\', \'-g\', default=0, type=int,\n                        help=\'ID of GPU.\')\n    parser.add_argument(\'--num\', \'-n\', default=500000, type=int,\n                        help=\'number of train data\')\n    parser.add_argument(\'--dim\', \'-d\', default=2, type=int,\n                        help=\'dimension of each data\')\n    parser.add_argument(\'--max-iter\', \'-m\', default=30, type=int,\n                        help=\'number of iterations\')\n    parser.add_argument(\'--tol\', \'-t\', default=1e-3, type=float,\n                        help=\'error tolerance to stop iterations\')\n    parser.add_argument(\'--output-image\', \'-o\', default=None, type=str,\n                        dest=\'output\', help=\'output image file name\')\n    args = parser.parse_args()\n    run(args.gpu_id, args.num, args.dim, args.max_iter, args.tol, args.output)\n'"
examples/interoperability/mpi4py_multiple_devices.py,0,"b'# Run this script with the following command:\n#\n#   mpiexec -n 2 python multple_devices.py\n#\n# This script executes simple communication and computation with 2 MPI\n# processes, each of which uses a different GPU\n\nimport cupy\nfrom mpi4py import MPI\n\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\nsize = comm.Get_size()\nif size != 2:\n    raise RuntimeError(""run this script with 2 processes: mpiexec -n 2 ..."")\ndevice_count = cupy.cuda.runtime.getDeviceCount()\nif device_count < 2:\n    raise RuntimeError(""this script requires 2 GPUs"")\n\n# Select device based on local MPI rank.\n# Caveat: for simplicity we assume local_rank == rank here, which may or may\n# not be the case depending how MPI processes are lauched and how your code\n# is written. For more robust usage, you may need to consult the user manual\n# for your MPI library. For example:\n# local_rank = int(os.getenv(""OMPI_COMM_WORLD_LOCAL_RANK""))  # Open MPI\n# local_rank = int(os.getenv(""MV2_COMM_WORLD_LOCAL_RANK""))   # MVAPICH2\nlocal_rank = rank\ncupy.cuda.Device(local_rank).use()\n\n# send-recv\nif rank == 0:\n    arr = cupy.empty(100, dtype=cupy.int64)\n    comm.Recv(arr, source=1, tag=87)\n    assert (arr == cupy.arange(100).astype(cupy.int64)).all()\nelse:\n    arr = cupy.arange(100).astype(cupy.int64)\n    comm.Send(arr, dest=0, tag=87)\n\n# allreduce\narr1 = cupy.empty(1000)\narr2 = cupy.random.random(1000)\narr_total = arr2.copy()\ncomm.Allreduce(MPI.IN_PLACE, arr_total)  # in-place reduction\nif rank == 0:\n    comm.Recv(arr1, source=1, tag=88)\n    comm.Send(arr2, dest=1, tag=89)\nelse:\n    comm.Send(arr2, dest=0, tag=88)\n    comm.Recv(arr1, source=0, tag=89)\nassert (arr1 + arr2 == arr_total).all()\n\nprint(""process {}: finished"".format(rank))\n'"
examples/kmeans/kmeans.py,0,"b""import argparse\nimport contextlib\nimport time\n\nimport cupy\nimport matplotlib.pyplot as plt\nimport numpy\n\n\n@contextlib.contextmanager\ndef timer(message):\n    cupy.cuda.Stream.null.synchronize()\n    start = time.time()\n    yield\n    cupy.cuda.Stream.null.synchronize()\n    end = time.time()\n    print('%s:  %f sec' % (message, end - start))\n\n\nvar_kernel = cupy.ElementwiseKernel(\n    'T x0, T x1, T c0, T c1', 'T out',\n    'out = (x0 - c0) * (x0 - c0) + (x1 - c1) * (x1 - c1)',\n    'var_kernel'\n)\nsum_kernel = cupy.ReductionKernel(\n    'T x, S mask', 'T out',\n    'mask ? x : 0',\n    'a + b', 'out = a', '0',\n    'sum_kernel'\n)\ncount_kernel = cupy.ReductionKernel(\n    'T mask', 'float32 out',\n    'mask ? 1.0 : 0.0',\n    'a + b', 'out = a', '0.0',\n    'count_kernel'\n)\n\n\ndef fit_xp(X, n_clusters, max_iter):\n    assert X.ndim == 2\n\n    # Get NumPy or CuPy module from the supplied array.\n    xp = cupy.get_array_module(X)\n\n    n_samples = len(X)\n\n    # Make an array to store the labels indicating which cluster each sample is\n    # contained.\n    pred = xp.zeros(n_samples)\n\n    # Choose the initial centroid for each cluster.\n    initial_indexes = xp.random.choice(n_samples, n_clusters, replace=False)\n    centers = X[initial_indexes]\n\n    for _ in range(max_iter):\n        # Compute the new label for each sample.\n        distances = xp.linalg.norm(X[:, None, :] - centers[None, :, :], axis=2)\n        new_pred = xp.argmin(distances, axis=1)\n\n        # If the label is not changed for each sample, we suppose the\n        # algorithm has converged and exit from the loop.\n        if xp.all(new_pred == pred):\n            break\n        pred = new_pred\n\n        # Compute the new centroid for each cluster.\n        i = xp.arange(n_clusters)\n        mask = pred == i[:, None]\n        sums = xp.where(mask[:, :, None], X, 0).sum(axis=1)\n        counts = xp.count_nonzero(mask, axis=1).reshape((n_clusters, 1))\n        centers = sums / counts\n\n    return centers, pred\n\n\ndef fit_custom(X, n_clusters, max_iter):\n    assert X.ndim == 2\n\n    n_samples = len(X)\n\n    pred = cupy.zeros(n_samples)\n\n    initial_indexes = cupy.random.choice(n_samples, n_clusters, replace=False)\n    centers = X[initial_indexes]\n\n    for _ in range(max_iter):\n        distances = var_kernel(X[:, None, 0], X[:, None, 1],\n                               centers[None, :, 1], centers[None, :, 0])\n        new_pred = cupy.argmin(distances, axis=1)\n        if cupy.all(new_pred == pred):\n            break\n        pred = new_pred\n\n        i = cupy.arange(n_clusters)\n        mask = pred == i[:, None]\n        sums = sum_kernel(X, mask[:, :, None], axis=1)\n        counts = count_kernel(mask, axis=1).reshape((n_clusters, 1))\n        centers = sums / counts\n\n    return centers, pred\n\n\ndef draw(X, n_clusters, centers, pred, output):\n    # Plot the samples and centroids of the fitted clusters into an image file.\n    for i in range(n_clusters):\n        labels = X[pred == i]\n        plt.scatter(labels[:, 0], labels[:, 1], c=numpy.random.rand(3))\n    plt.scatter(\n        centers[:, 0], centers[:, 1], s=120, marker='s', facecolors='y',\n        edgecolors='k')\n    plt.savefig(output)\n\n\ndef run(gpuid, n_clusters, num, max_iter, use_custom_kernel, output):\n    samples = numpy.random.randn(num, 2)\n    X_train = numpy.r_[samples + 1, samples - 1]\n\n    with timer(' CPU '):\n        centers, pred = fit_xp(X_train, n_clusters, max_iter)\n\n    with cupy.cuda.Device(gpuid):\n        X_train = cupy.asarray(X_train)\n\n        with timer(' GPU '):\n            if use_custom_kernel:\n                centers, pred = fit_custom(X_train, n_clusters, max_iter)\n            else:\n                centers, pred = fit_xp(X_train, n_clusters, max_iter)\n\n        if output is not None:\n            index = numpy.random.choice(10000000, 300, replace=False)\n            draw(X_train[index].get(), n_clusters, centers.get(),\n                 pred[index].get(), output)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu-id', '-g', default=0, type=int,\n                        help='ID of GPU.')\n    parser.add_argument('--n-clusters', '-n', default=2, type=int,\n                        help='number of clusters')\n    parser.add_argument('--num', default=5000000, type=int,\n                        help='number of samples')\n    parser.add_argument('--max-iter', '-m', default=10, type=int,\n                        help='number of iterations')\n    parser.add_argument('--use-custom-kernel', action='store_true',\n                        default=False, help='use Elementwise kernel')\n    parser.add_argument('--output-image', '-o', default=None, type=str,\n                        help='output image file name')\n    args = parser.parse_args()\n    run(args.gpu_id, args.n_clusters, args.num, args.max_iter,\n        args.use_custom_kernel, args.output_image)\n"""
examples/stream/cublas.py,0,"b'# nvprof --print-gpu-trace python examples/stream/cublas.py\nimport cupy\n\nx = cupy.array([1, 2, 3])\ny = cupy.array([[1], [2], [3]])\nexpected = cupy.matmul(x, y)\ncupy.cuda.Device().synchronize()\n\nstream = cupy.cuda.stream.Stream()\nwith stream:\n    z = cupy.matmul(x, y)\nstream.synchronize()\ncupy.testing.assert_array_equal(z, expected)\n\nstream = cupy.cuda.stream.Stream()\nstream.use()\nz = cupy.matmul(x, y)\nstream.synchronize()\ncupy.testing.assert_array_equal(z, expected)\n'"
examples/stream/cudnn.py,0,"b'# nvprof --print-gpu-trace python examples/stream/cudnn.py\nimport cupy\nimport cupy.cudnn\n\nx = cupy.array([1.0, 2.0, 3.0])\nmode = cupy.cuda.cudnn.CUDNN_ACTIVATION_RELU\nexpected = cupy.cudnn.activation_forward(x, mode)\ncupy.cuda.Device().synchronize()\n\nstream = cupy.cuda.stream.Stream()\nwith stream:\n    y = cupy.cudnn.activation_forward(x, mode)\nstream.synchronize()\ncupy.testing.assert_array_equal(y, expected)\n\nstream = cupy.cuda.stream.Stream()\nstream.use()\ny = cupy.cudnn.activation_forward(x, mode)\nstream.synchronize()\ncupy.testing.assert_array_equal(y, expected)\n'"
examples/stream/cufft.py,0,"b'# nvprof --print-gpu-trace python examples/stream/cufft.py\nimport cupy\n\nx = cupy.array([1, 0, 3, 0, 5, 0, 7, 0, 9], dtype=float)\nexpected_f = cupy.fft.fft(x)\ncupy.cuda.Device().synchronize()\n\nstream = cupy.cuda.stream.Stream()\nwith stream:\n    f = cupy.fft.fft(x)\nstream.synchronize()\ncupy.testing.assert_array_equal(f, expected_f)\n\nstream = cupy.cuda.stream.Stream()\nstream.use()\nf = cupy.fft.fft(x)\nstream.synchronize()\ncupy.testing.assert_array_equal(f, expected_f)\n'"
examples/stream/cupy_event.py,0,"b'# nvprof --print-gpu-trace python examples/stream/cupy_event.py\nimport cupy\n\nx = cupy.array([1, 2, 3])\n\nstart_event = cupy.cuda.stream.Event()\nstop_event = cupy.cuda.stream.Event()\n\n\ndef _norm_with_elapsed_time(x):\n    start_event.record()\n    y = cupy.linalg.norm(x)\n    stop_event.record()\n    stop_event.synchronize()\n    print(cupy.cuda.get_elapsed_time(start_event, stop_event))\n    return y\n\n\nexpected = _norm_with_elapsed_time(x)\ncupy.cuda.Device().synchronize()\n\nstream = cupy.cuda.stream.Stream()\nwith stream:\n    y = _norm_with_elapsed_time(x)\nstream.synchronize()\ncupy.testing.assert_array_equal(y, expected)\n\nstream = cupy.cuda.stream.Stream()\nstream.use()\ny = _norm_with_elapsed_time(x)\nstream.synchronize()\ncupy.testing.assert_array_equal(y, expected)\n'"
examples/stream/cupy_kernel.py,0,"b'# nvprof --print-gpu-trace python examples/stream/cupy_kernel.py\nimport cupy\n\nx = cupy.array([1, 2, 3])\nexpected = cupy.linalg.norm(x)\ncupy.cuda.Device().synchronize()\n\nstream = cupy.cuda.stream.Stream()\nwith stream:\n    y = cupy.linalg.norm(x)\nstream.synchronize()\ncupy.testing.assert_array_equal(y, expected)\n\nstream = cupy.cuda.stream.Stream()\nstream.use()\ny = cupy.linalg.norm(x)\nstream.synchronize()\ncupy.testing.assert_array_equal(y, expected)\n'"
examples/stream/cupy_memcpy.py,0,"b""# nvprof --print-gpu-trace python examples/stream/cupy_memcpy.py\nimport cupy\nimport numpy\n\npinned_memory_pool = cupy.cuda.PinnedMemoryPool()\ncupy.cuda.set_pinned_memory_allocator(pinned_memory_pool.malloc)\n\n\ndef _pin_memory(array):\n    mem = cupy.cuda.alloc_pinned_memory(array.nbytes)\n    ret = numpy.frombuffer(mem, array.dtype, array.size).reshape(array.shape)\n    ret[...] = array\n    return ret\n\n\nSIZE = 1024 * 1024\nx_cpu_src = numpy.arange(SIZE, dtype=numpy.float32)\nx_gpu_src = cupy.arange(SIZE, dtype=numpy.float32)\n\n\n# synchronous\nstream = cupy.cuda.Stream.null\nstart = stream.record()\nx_gpu_dst = cupy.empty(x_cpu_src.shape, x_cpu_src.dtype)\nx_gpu_dst.set(x_cpu_src)\nx_cpu_dst = x_gpu_src.get()\nend = stream.record()\n\nprint('Synchronous Device to Host / Host to Device (ms)')\nprint(cupy.cuda.get_elapsed_time(start, end))\n\n\n# asynchronous\nx_gpu_dst = cupy.empty(x_cpu_src.shape, x_cpu_src.dtype)\nx_cpu_dst = numpy.empty(x_gpu_src.shape, x_gpu_src.dtype)\n\nx_pinned_cpu_src = _pin_memory(x_cpu_src)\nx_pinned_cpu_dst = _pin_memory(x_cpu_dst)\n\nwith cupy.cuda.stream.Stream() as stream_htod:\n    start = stream_htod.record()\n    x_gpu_dst.set(x_pinned_cpu_src)\n    with cupy.cuda.stream.Stream() as stream_dtoh:\n        x_gpu_src.get(out=x_pinned_cpu_dst)\n        stream_dtoh.synchronize()\n    stream_htod.synchronize()\n    end = stream_htod.record()\n\nprint('Asynchronous Device to Host / Host to Device (ms)')\nprint(cupy.cuda.get_elapsed_time(start, end))\n"""
examples/stream/curand.py,0,"b'# nvprof --print-gpu-trace python examples/stream/curand.py\nimport cupy\n\nrand = cupy.random.generator.RandomState()\n\nstream = cupy.cuda.stream.Stream()\nwith stream:\n    y = rand.lognormal(size=(1, 3))\n\nstream = cupy.cuda.stream.Stream()\nstream.use()\ny = rand.lognormal(size=(1, 3))\n'"
examples/stream/cusolver.py,0,"b""# nvprof --print-gpu-trace python examples/stream/cusolver.py\nimport cupy\n\nx = cupy.array([[1, 0, 3], [0, 5, 0], [7, 0, 9]], float)\nexpected_w, expected_v = cupy.linalg.eigh(x, UPLO='U')\ncupy.cuda.Device().synchronize()\n\nstream = cupy.cuda.stream.Stream()\nwith stream:\n    w, v = cupy.linalg.eigh(x, UPLO='U')\nstream.synchronize()\ncupy.testing.assert_array_equal(w, expected_w)\ncupy.testing.assert_array_equal(v, expected_v)\n\nstream = cupy.cuda.stream.Stream()\nstream.use()\nw, v = cupy.linalg.eigh(x, UPLO='U')\nstream.synchronize()\ncupy.testing.assert_array_equal(w, expected_w)\ncupy.testing.assert_array_equal(v, expected_v)\n"""
examples/stream/cusparse.py,0,"b""# nvprof --print-gpu-trace python examples/stream/cusparse.py\nimport cupy\n\n\ndef _make(xp, sp, dtype):\n    data = xp.array([0, 1, 3, 2], dtype)\n    indices = xp.array([0, 0, 2, 1], 'i')\n    indptr = xp.array([0, 1, 2, 3, 4], 'i')\n    # 0, 1, 0, 0\n    # 0, 0, 0, 2\n    # 0, 0, 3, 0\n    return sp.csc_matrix((data, indices, indptr), shape=(3, 4))\n\n\nx = _make(cupy, cupy.sparse, float)\nexpected = cupy.cusparse.cscsort(x)\ncupy.cuda.Device().synchronize()\n\nstream = cupy.cuda.stream.Stream()\nwith stream:\n    y = cupy.cusparse.cscsort(x)\nstream.synchronize()\ncupy.testing.assert_array_equal(y, expected)\n\nstream = cupy.cuda.stream.Stream()\nstream.use()\ny = cupy.cusparse.cscsort(x)\nstream.synchronize()\ncupy.testing.assert_array_equal(y, expected)\n"""
examples/stream/map_reduce.py,0,"b""import cupy\nimport time\n\ndevice = cupy.cuda.Device()\nmemory_pool = cupy.cuda.MemoryPool()\ncupy.cuda.set_allocator(memory_pool.malloc)\nrand = cupy.random.generator.RandomState(seed=1)\n\nn = 10\nzs = []\nmap_streams = []\nstop_events = []\nreduce_stream = cupy.cuda.stream.Stream()\nfor i in range(n):\n    map_streams.append(cupy.cuda.stream.Stream())\n\nstart_time = time.time()\n\n# Map\nfor stream in map_streams:\n    with stream:\n        x = rand.normal(size=(1, 1024**2))\n        y = rand.normal(size=(1024**2, 1))\n        z = cupy.matmul(x, y)\n        zs.append(z)\n    stop_event = stream.record()\n    stop_events.append(stop_event)\n\n# Block the `reduce_stream` until all events occur. This does not block host.\n# This is not required when reduction is performed in the default (Stream.null)\n# stream unless streams are created with `non_blocking=True` flag.\nfor i in range(n):\n    reduce_stream.wait_event(stop_events[i])\n\n# Reduce\nwith reduce_stream:\n    z = sum(zs)\n\ndevice.synchronize()\nelapsed_time = time.time() - start_time\nprint('elapsed time', elapsed_time)\nprint('total bytes', memory_pool.total_bytes())\n\n# Free all blocks in the memory pool of streams\nfor stream in map_streams:\n    memory_pool.free_all_blocks(stream=stream)\nprint('total bytes', memory_pool.total_bytes())\n"""
examples/stream/thrust.py,0,"b'# nvprof --print-gpu-trace python examples/stream/thrust.py\nimport cupy\n\nx = cupy.array([1, 3, 2])\nexpected = x.sort()\ncupy.cuda.Device().synchronize()\n\nstream = cupy.cuda.stream.Stream()\nwith stream:\n    y = x.sort()\nstream.synchronize()\ncupy.testing.assert_array_equal(y, expected)\n\nstream = cupy.cuda.stream.Stream()\nstream.use()\ny = x.sort()\nstream.synchronize()\ncupy.testing.assert_array_equal(y, expected)\n'"
tests/cupy_tests/__init__.py,0,b''
tests/cupy_tests/test_cudnn.py,0,"b""import unittest\n\nimport numpy\n\nimport cupy\n\ntry:\n    import cupy.cuda.cudnn as libcudnn\n    cudnn_enabled = True\n    modes = [\n        libcudnn.CUDNN_ACTIVATION_SIGMOID,\n        libcudnn.CUDNN_ACTIVATION_RELU,\n        libcudnn.CUDNN_ACTIVATION_TANH,\n    ]\n    coef_modes = [\n        libcudnn.CUDNN_ACTIVATION_CLIPPED_RELU,\n    ]\n    layouts = [\n        libcudnn.CUDNN_TENSOR_NCHW,\n        libcudnn.CUDNN_TENSOR_NHWC,\n    ]\n    cudnn_version = libcudnn.getVersion()\n    if cudnn_version >= 6000:\n        coef_modes.append(libcudnn.CUDNN_ACTIVATION_ELU)\n\n    from cupy import cudnn\nexcept ImportError:\n    cudnn_enabled = False\n    cudnn_version = -1\n    modes = []\n    coef_modes = []\n    layouts = []\n\nfrom cupy import testing\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64],\n    'mode': modes,\n}))\n@unittest.skipUnless(cudnn_enabled, 'cuDNN is not available')\nclass TestCudnnActivation(unittest.TestCase):\n\n    def setUp(self):\n        self.x = testing.shaped_arange((3, 4), cupy, self.dtype)\n        self.y = testing.shaped_arange((3, 4), cupy, self.dtype)\n        self.g = testing.shaped_arange((3, 4), cupy, self.dtype)\n\n    def test_activation_forward(self):\n        cudnn.activation_forward(self.x, self.mode)\n\n    def test_activation_backward(self):\n        cudnn.activation_backward(self.x, self.y, self.g, self.mode)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64],\n    'mode': coef_modes,\n}))\n@unittest.skipUnless(cudnn_enabled, 'cuDNN is not available')\nclass TestCudnnActivationCoef(unittest.TestCase):\n\n    def setUp(self):\n        self.x = testing.shaped_arange((3, 4), cupy, self.dtype)\n        self.y = testing.shaped_arange((3, 4), cupy, self.dtype)\n        self.g = testing.shaped_arange((3, 4), cupy, self.dtype)\n        self.coef = self.dtype(0.75)\n\n    def test_activation_forward(self):\n        cudnn.activation_forward(self.x, self.mode, self.coef)\n\n    def test_activation_backward(self):\n        cudnn.activation_backward(self.x, self.y, self.g, self.mode,\n                                  self.coef)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64],\n    'ratio': [0.0, 0.1, 0.2, 0.5],\n    'seed': [0, 100]\n}))\n@unittest.skipUnless(cudnn_enabled, 'cuDNN is not available')\nclass TestCudnnDropout(unittest.TestCase):\n\n    def setUp(self):\n        self.x = testing.shaped_arange((3, 4), cupy, self.dtype)\n        self.gy = testing.shaped_arange((3, 4), cupy, self.dtype)\n        self.states = cudnn.DropoutStates(None, self.seed)\n\n    def test_dropout_forward(self):\n        _, y = self.states.forward(None, self.x, self.ratio)\n        if self.ratio == 0:\n            self.assertTrue(cupy.all(self.x == y))\n        else:\n            self.assertTrue(cupy.all(self.x != y))\n\n    def test_dropout_backward(self):\n        rspace, y = self.states.forward(None, self.x, self.ratio)\n        gx = self.states.backward(\n            None, self.gy, self.ratio, rspace)\n\n        forward_mask = y / self.x\n        backward_mask = gx / self.gy\n\n        # backward_mask must be the same as forward_mask\n        self.assertTrue(cupy.all(forward_mask == backward_mask))\n\n    def test_dropout_seed(self):\n        # initialize Dropoutstates with the same seed\n        states2 = cudnn.DropoutStates(None, self.seed)\n\n        rspace, y = self.states.forward(None, self.x, self.ratio)\n        rspace2, y2 = states2.forward(None, self.x, self.ratio)\n        # forward results must be the same\n        self.assertTrue(cupy.all(y == y2))\n\n        gx = self.states.backward(None, self.gy, self.ratio, rspace)\n        gx2 = states2.backward(None, self.gy, self.ratio, rspace2)\n        # backward results must be the same\n        self.assertTrue(cupy.all(gx == gx2))\n\n\n@testing.parameterize(*(testing.product({\n    'tensor_core': ['always', 'auto', 'never'],\n    'dtype': [numpy.float16, numpy.float32, numpy.float64],\n    'dilate': [1, 2],\n    'groups': [1, 2],\n    'ndim': [2],\n    'max_workspace_size': [0, 2 ** 22],\n    'auto_tune': [True, False],\n    'bias': [True, False],\n    'layout': layouts,\n})))\n@unittest.skipUnless(cudnn_enabled, 'cuDNN is not available')\nclass TestConvolutionForward(unittest.TestCase):\n\n    def setUp(self):\n        ndim = self.ndim\n        dtype = self.dtype\n        batches = 2\n        if self.layout == libcudnn.CUDNN_TENSOR_NHWC:\n            # channel size must be multiple of 4\n            in_channels_a_group = 4\n            out_channels_a_group = 4\n        else:\n            in_channels_a_group = 3\n            out_channels_a_group = 2\n        in_channels = in_channels_a_group * self.groups\n        out_channels = out_channels_a_group * self.groups\n        # TODO(anaruse): increase test cases.\n        ksize = 3\n        stride = 2\n        pad = ksize // stride * self.dilate\n        self.strides = (stride,) * ndim\n        self.pads = (pad,) * ndim\n        self.dilations = (self.dilate,) * ndim\n        if self.layout == libcudnn.CUDNN_TENSOR_NHWC:\n            self.x = cupy.zeros(\n                (batches,) + (ksize,) * ndim + (in_channels,), dtype)\n            self.W = cupy.zeros(\n                (out_channels,) + (ksize,) * ndim + (in_channels_a_group,),\n                dtype)\n            self.y = cupy.ones(\n                (batches,) + (2,) * ndim + (out_channels,), dtype)\n        else:\n            self.x = cupy.zeros(\n                (batches, in_channels) + (ksize,) * ndim, dtype)\n            self.W = cupy.zeros(\n                (out_channels, in_channels_a_group) + (ksize,) * ndim, dtype)\n            self.y = cupy.ones((batches, out_channels) + (2,) * ndim, dtype)\n        self.b = None\n        if self.bias:\n            self.b = cupy.zeros((out_channels,), dtype)\n\n        version = libcudnn.getVersion()\n        self.err = None\n        if ((self.dilate > 1 and version < 6000) or\n                (self.groups > 1 and version < 7000)):\n            self.err = ValueError\n        elif ndim > 2 and self.dilate > 1:\n            self.err = libcudnn.CuDNNError\n        self._workspace_size = cudnn.get_max_workspace_size()\n        cudnn.set_max_workspace_size(self.max_workspace_size)\n\n    def tearDown(self):\n        cudnn.set_max_workspace_size(self._workspace_size)\n\n    def call(self):\n        cudnn.convolution_forward(\n            self.x, self.W, self.b, self.y,\n            self.pads, self.strides, self.dilations, self.groups,\n            auto_tune=self.auto_tune, tensor_core=self.tensor_core,\n            d_layout=self.layout, w_layout=self.layout)\n\n    def test_call(self):\n        if self.layout == libcudnn.CUDNN_TENSOR_NHWC:\n            version = libcudnn.getVersion()\n            if self.groups > 1:\n                return unittest.SkipTest()\n            if self.dilate > 1 and version < 7300:\n                return unittest.SkipTest()\n            if self.dtype is numpy.float64 and version < 7100:\n                return unittest.SkipTest()\n        if self.err is None:\n            self.call()\n            self.assertTrue((self.y == 0).all())\n        else:\n            with self.assertRaises(self.err):\n                self.call()\n\n\n@testing.parameterize(*(testing.product({\n    'tensor_core': ['always', 'auto', 'never'],\n    'dtype': [numpy.float16, numpy.float32, numpy.float64],\n    'dilate': [1, 2],\n    'groups': [1, 2],\n    'ndim': [2, 3],\n    'max_workspace_size': [0, 2 ** 22],\n    'auto_tune': [True, False],\n    'deterministic': [True, False],\n})))\n@unittest.skipUnless(cudnn_enabled, 'cuDNN is not available')\nclass TestConvolutionBackwardFilter(unittest.TestCase):\n\n    def setUp(self):\n        ndim = self.ndim\n        dtype = self.dtype\n        batches = 2\n        in_channels_a_group = 3\n        out_channels_a_group = 2\n        in_channels = in_channels_a_group * self.groups\n        out_channels = out_channels_a_group * self.groups\n        # TODO(anaruse): increase test cases.\n        ksize = 3\n        stride = 2\n        pad = ksize // stride * self.dilate\n        self.strides = (stride,) * ndim\n        self.pads = (pad,) * ndim\n        self.dilations = (self.dilate,) * ndim\n        self.x = cupy.zeros(\n            (batches, in_channels) + (ksize,) * ndim, dtype)\n        self.gy = cupy.zeros((batches, out_channels) + (2,) * ndim, dtype)\n\n        self.gW = cupy.ones(\n            (out_channels, in_channels_a_group) + (ksize,) * ndim, dtype)\n\n        version = libcudnn.getVersion()\n        deterministic = self.deterministic\n        self.err = None\n        if ((self.dilate > 1 and version < 6000) or\n                (self.groups > 1 and version < 7000)):\n            self.err = ValueError\n        elif deterministic and (\n                (self.dilate > 1 and version < 7000) or\n                (ndim > 2 and version < 6000) or\n                (ndim > 2 and self.dtype == numpy.float64)):\n            self.err = libcudnn.CuDNNError\n        self._workspace_size = cudnn.get_max_workspace_size()\n        cudnn.set_max_workspace_size(self.max_workspace_size)\n\n    def tearDown(self):\n        cudnn.set_max_workspace_size(self._workspace_size)\n\n    def call(self):\n        cudnn.convolution_backward_filter(\n            self.x, self.gy, self.gW,\n            self.pads, self.strides, self.dilations, self.groups,\n            deterministic=self.deterministic,\n            auto_tune=self.auto_tune,\n            tensor_core=self.tensor_core)\n\n    def test_call(self):\n        if self.deterministic and self.max_workspace_size == 0:\n            # This test case is very unstable\n            return\n        if self.err is None:\n            self.call()\n            self.assertTrue((self.gW == 0).all())\n        else:\n            with self.assertRaises(self.err):\n                self.call()\n\n\n@testing.parameterize(*(testing.product({\n    'tensor_core': ['always', 'auto', 'never'],\n    'dtype': [numpy.float16, numpy.float32, numpy.float64],\n    'dilate': [1, 2],\n    'groups': [1, 2],\n    'ndim': [2, 3],\n    'max_workspace_size': [0, 2 ** 22],\n    'auto_tune': [True, False],\n    'deterministic': [True, False],\n    'bias': [True, False],\n})))\n@unittest.skipUnless(cudnn_enabled, 'cuDNN is not available')\nclass TestConvolutionBackwardData(unittest.TestCase):\n\n    def setUp(self):\n        ndim = self.ndim\n        dtype = self.dtype\n        batches = 2\n        in_channels_a_group = 3\n        out_channels_a_group = 2\n        in_channels = in_channels_a_group * self.groups\n        out_channels = out_channels_a_group * self.groups\n        # TODO(anaruse): increase test cases.\n        ksize = 3\n        stride = 2\n        pad = ksize // stride * self.dilate\n        self.strides = (stride,) * ndim\n        self.pads = (pad,) * ndim\n        self.dilations = (self.dilate,) * ndim\n        self.W = cupy.zeros(\n            (out_channels, in_channels_a_group) + (ksize,) * ndim, dtype)\n        self.gy = cupy.zeros((batches, out_channels) + (2,) * ndim, dtype)\n        self.b = None\n        if self.bias:\n            self.b = cupy.zeros((in_channels,), dtype)\n\n        self.gx = cupy.ones(\n            (batches, in_channels) + (ksize,) * ndim, dtype)\n\n        version = libcudnn.getVersion()\n        deterministic = self.deterministic\n        self.err = None\n        if ((self.dilate > 1 and version < 6000) or\n                (self.groups > 1 and version < 7000)):\n            self.err = ValueError\n        elif deterministic and (\n                (self.dilate > 1 and (ndim != 2 or version < 7300)) or\n                (ndim > 2 and version < 6000) or\n                (ndim > 2 and self.dtype == numpy.float64)):\n            self.err = libcudnn.CuDNNError\n        self._workspace_size = cudnn.get_max_workspace_size()\n        cudnn.set_max_workspace_size(self.max_workspace_size)\n\n    def tearDown(self):\n        cudnn.set_max_workspace_size(self._workspace_size)\n\n    def call(self):\n        cudnn.convolution_backward_data(\n            self.W, self.gy, self.b, self.gx,\n            self.pads, self.strides, self.dilations, self.groups,\n            deterministic=self.deterministic,\n            auto_tune=self.auto_tune,\n            tensor_core=self.tensor_core)\n\n    def test_call(self):\n        if self.deterministic and self.max_workspace_size == 0:\n            # This test case is very unstable\n            return\n        if self.err is None:\n            self.call()\n            self.assertTrue((self.gx == 0).all())\n        else:\n            with self.assertRaises(self.err):\n                self.call()\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64],\n    'ksize': [1, 3, 5],\n    'stride': [2, 4],\n    'auto_tune': [True, False],\n}))\n@unittest.skipIf(not cudnn_enabled or cudnn_version < 7500,\n                 'cuDNN 7.5.0 or later is required')\nclass TestConvolutionNoAvailableAlgorithm(unittest.TestCase):\n    '''Checks if an expected error is raised.\n\n    This checks if an expected error is raised when no available algorithm\n    is found by cuDNN for a configuration. This (no available algorithm found)\n    can occur when convolution_backward_data or convolution_backward_filter is\n    performed with NHWC layout.\n\n    Please notice that conditions that cause the error may change depending on\n    cuDNN version. The conditions below are set based on cuDNN 7.5.0 and 7.6.0.\n    '''\n\n    def setUp(self):\n        self.layout = libcudnn.CUDNN_TENSOR_NHWC\n        n = 16\n        x_c, y_c = 64, 64\n        x_h, x_w = 32, 32\n        y_h, y_w = x_h // self.stride, x_w // self.stride\n        self.pad = (self.ksize - 1) // 2\n        if self.layout == libcudnn.CUDNN_TENSOR_NHWC:\n            x_shape = (n, x_h, x_w, x_c)\n            y_shape = (n, y_h, y_w, y_c)\n            W_shape = (y_c, self.ksize, self.ksize, x_c)\n        else:\n            x_shape = (n, x_c, x_h, x_w)\n            y_shape = (n, y_c, y_h, y_w)\n            W_shape = (y_c, x_c, self.ksize, self.ksize)\n        self.x = cupy.ones(x_shape, dtype=self.dtype)\n        self.W = cupy.ones(W_shape, dtype=self.dtype)\n        self.y = cupy.empty(y_shape, dtype=self.dtype)\n        self.gx = cupy.empty(x_shape, dtype=self.dtype)\n        self.gW = cupy.empty(W_shape, dtype=self.dtype)\n        self.gy = cupy.ones(y_shape, dtype=self.dtype)\n        self._workspace_size = cudnn.get_max_workspace_size()\n        cudnn.set_max_workspace_size(0)\n\n    def tearDown(self):\n        cudnn.set_max_workspace_size(self._workspace_size)\n\n    def test_backward_filter(self):\n        if not (self.layout == libcudnn.CUDNN_TENSOR_NHWC and\n                self.dtype == numpy.float64):\n            return unittest.SkipTest()\n        with self.assertRaises(RuntimeError):\n            cudnn.convolution_backward_filter(\n                self.x, self.gy, self.gW,\n                pad=(self.pad, self.pad), stride=(self.stride, self.stride),\n                dilation=(1, 1), groups=1, deterministic=False,\n                auto_tune=self.auto_tune, tensor_core='always',\n                d_layout=self.layout, w_layout=self.layout)\n\n    def test_backward_data(self):\n        if self.layout != libcudnn.CUDNN_TENSOR_NHWC:\n            return unittest.SkipTest()\n        with self.assertRaises(RuntimeError):\n            cudnn.convolution_backward_data(\n                self.W, self.gy, None, self.gx,\n                pad=(self.pad, self.pad), stride=(self.stride, self.stride),\n                dilation=(1, 1), groups=1, deterministic=0,\n                auto_tune=self.auto_tune, tensor_core='always',\n                d_layout=self.layout, w_layout=self.layout)\n\n    def _get_error_type(self):\n        if self.auto_tune:\n            return RuntimeError\n        else:\n            return libcudnn.CuDNNError\n"""
tests/cupy_tests/test_cusolver.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import cusolver\nfrom cupy import testing\nfrom cupy.testing import attr\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n    'shape': [\n        # gesvdj tests\n        (5, 3), (4, 4), (3, 5),\n        # gesvdjBatched tests\n        (2, 5, 3), (2, 4, 4), (2, 3, 5),\n    ],\n    'order': ['C', 'F'],\n    'full_matrices': [True, False],\n    'overwrite_a': [True, False],\n}))\n@attr.gpu\nclass TestGesvdj(unittest.TestCase):\n\n    def setUp(self):\n        if not cusolver.check_availability('gesvdj'):\n            pytest.skip('gesvdj is not available')\n        shape = self.shape\n        if self.dtype == numpy.complex64:\n            a_real = numpy.random.random(shape).astype(numpy.float32)\n            a_imag = numpy.random.random(shape).astype(numpy.float32)\n            self.a = a_real + 1.j * a_imag\n        elif self.dtype == numpy.complex128:\n            a_real = numpy.random.random(shape).astype(numpy.float64)\n            a_imag = numpy.random.random(shape).astype(numpy.float64)\n            self.a = a_real + 1.j * a_imag\n        else:\n            self.a = numpy.random.random(shape).astype(self.dtype)\n\n    def test_gesvdj(self):\n        a = cupy.array(self.a, order=self.order)\n        u, s, v = cusolver.gesvdj(a, full_matrices=self.full_matrices,\n                                  overwrite_a=self.overwrite_a)\n\n        # sigma = diag(s)\n        shape = self.shape\n        mn = min(shape[-2:])\n        if self.full_matrices:\n            sigma_shape = shape\n        else:\n            sigma_shape = shape[:-2] + (mn, mn)\n        sigma = cupy.zeros(sigma_shape, self.dtype)\n        ix = numpy.arange(mn)\n        sigma[..., ix, ix] = s\n\n        vh = v.swapaxes(-2, -1).conjugate()\n        aa = cupy.matmul(cupy.matmul(u, sigma), vh)\n        if self.dtype in (numpy.float32, numpy.complex64):\n            decimal = 5\n        else:\n            decimal = 10\n        testing.assert_array_almost_equal(aa, self.a, decimal=decimal)\n\n    def test_gesvdj_no_uv(self):\n        a = cupy.array(self.a, order=self.order)\n        s = cusolver.gesvdj(a, full_matrices=self.full_matrices,\n                            compute_uv=False, overwrite_a=self.overwrite_a)\n        expect = numpy.linalg.svd(self.a, full_matrices=self.full_matrices,\n                                  compute_uv=False)\n        if self.dtype in (numpy.float32, numpy.complex64):\n            decimal = 5\n        else:\n            decimal = 10\n        testing.assert_array_almost_equal(s, expect, decimal=decimal)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n    'shape': [(5, 4), (1, 4, 3), (4, 3, 2)],\n}))\n@attr.gpu\nclass TestGesvda(unittest.TestCase):\n\n    def setUp(self):\n        if not cusolver.check_availability('gesvda'):\n            pytest.skip('gesvda is not available')\n        if self.dtype == numpy.complex64:\n            a_real = numpy.random.random(self.shape).astype(numpy.float32)\n            a_imag = numpy.random.random(self.shape).astype(numpy.float32)\n            self.a = a_real + 1.j * a_imag\n        elif self.dtype == numpy.complex128:\n            a_real = numpy.random.random(self.shape).astype(numpy.float64)\n            a_imag = numpy.random.random(self.shape).astype(numpy.float64)\n            self.a = a_real + 1.j * a_imag\n        else:\n            self.a = numpy.random.random(self.shape).astype(self.dtype)\n\n    def test_gesvda(self):\n        a = cupy.array(self.a)\n        u, s, v = cusolver.gesvda(a)\n        if a.ndim == 2:\n            batch_size = 1\n            a = a.reshape((1,) + a.shape)\n            u = u.reshape((1,) + u.shape)\n            s = s.reshape((1,) + s.shape)\n            v = v.reshape((1,) + v.shape)\n        else:\n            batch_size = a.shape[0]\n        for i in range(batch_size):\n            sigma = cupy.diag(s[i])\n            vh = v[i].T.conjugate()\n            aa = cupy.matmul(cupy.matmul(u[i], sigma), vh)\n            if self.dtype in (numpy.float32, numpy.complex64):\n                decimal = 5\n            else:\n                decimal = 10\n            testing.assert_array_almost_equal(aa, a[i], decimal=decimal)\n\n    def test_gesvda_no_uv(self):\n        a = cupy.array(self.a)\n        s = cusolver.gesvda(a, compute_uv=False)\n        expect = numpy.linalg.svd(self.a, compute_uv=False)\n        if self.dtype in (numpy.float32, numpy.complex64):\n            decimal = 5\n        else:\n            decimal = 10\n        testing.assert_array_almost_equal(s, expect, decimal=decimal)\n"""
tests/cupy_tests/test_cusparse.py,0,"b""import pickle\nimport unittest\n\nimport numpy\nimport pytest\ntry:\n    import scipy.sparse\nexcept ImportError:\n    pass\n\nimport cupy\nfrom cupy import testing\nfrom cupy import cusparse\nfrom cupyx.scipy import sparse\n\n\nclass TestMatDescriptor(unittest.TestCase):\n\n    def test_create(self):\n        md = cusparse.MatDescriptor.create()\n        assert isinstance(md.descriptor, int)\n\n    def test_pickle(self):\n        md = cusparse.MatDescriptor.create()\n        md2 = pickle.loads(pickle.dumps(md))\n        assert isinstance(md2.descriptor, int)\n        assert md.descriptor != md2.descriptor\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64],\n    'transa': [True, False],\n}))\n@testing.with_requires('scipy')\nclass TestCsrmm(unittest.TestCase):\n\n    alpha = 0.5\n    beta = 0.25\n\n    def setUp(self):\n        self.op_a = scipy.sparse.random(2, 3, density=0.5, dtype=self.dtype)\n        if self.transa:\n            self.a = self.op_a.T\n        else:\n            self.a = self.op_a\n        self.b = numpy.random.uniform(-1, 1, (3, 4)).astype(self.dtype)\n        self.c = numpy.random.uniform(-1, 1, (2, 4)).astype(self.dtype)\n\n    def test_csrmm(self):\n        a = sparse.csr_matrix(self.a)\n        b = cupy.array(self.b, order='f')\n        y = cupy.cusparse.csrmm(a, b, alpha=self.alpha, transa=self.transa)\n        expect = self.alpha * self.op_a.dot(self.b)\n        testing.assert_array_almost_equal(y, expect)\n\n    def test_csrmm_with_c(self):\n        a = sparse.csr_matrix(self.a)\n        b = cupy.array(self.b, order='f')\n        c = cupy.array(self.c, order='f')\n        y = cupy.cusparse.csrmm(\n            a, b, c=c, alpha=self.alpha, beta=self.beta, transa=self.transa)\n        expect = self.alpha * self.op_a.dot(self.b) + self.beta * self.c\n        self.assertIs(y, c)\n        testing.assert_array_almost_equal(y, expect)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64],\n    'trans': [(False, False), (True, False), (False, True)],\n}))\n@testing.with_requires('scipy')\nclass TestCsrmm2(unittest.TestCase):\n\n    alpha = 0.5\n    beta = 0.25\n\n    def setUp(self):\n        self.transa, self.transb = self.trans\n        self.op_a = scipy.sparse.random(2, 3, density=0.5, dtype=self.dtype)\n        if self.transa:\n            self.a = self.op_a.T\n        else:\n            self.a = self.op_a\n        self.op_b = numpy.random.uniform(-1, 1, (3, 4)).astype(self.dtype)\n        if self.transb:\n            self.b = self.op_b.T\n        else:\n            self.b = self.op_b\n        self.c = numpy.random.uniform(-1, 1, (2, 4)).astype(self.dtype)\n\n    def test_csrmm2(self):\n        a = sparse.csr_matrix(self.a)\n        b = cupy.array(self.b, order='f')\n        y = cupy.cusparse.csrmm2(\n            a, b, alpha=self.alpha, transa=self.transa, transb=self.transb)\n        expect = self.alpha * self.op_a.dot(self.op_b)\n        testing.assert_array_almost_equal(y, expect)\n\n    def test_csrmm2_with_c(self):\n        a = sparse.csr_matrix(self.a)\n        b = cupy.array(self.b, order='f')\n        c = cupy.array(self.c, order='f')\n        y = cupy.cusparse.csrmm2(\n            a, b, c=c, alpha=self.alpha, beta=self.beta,\n            transa=self.transa, transb=self.transb)\n        expect = self.alpha * self.op_a.dot(self.op_b) + self.beta * self.c\n        self.assertIs(y, c)\n        testing.assert_array_almost_equal(y, expect)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n    'shape': [(3, 4), (4, 3)]\n}))\n@testing.with_requires('scipy>=1.2.0')\nclass TestCsrgeam(unittest.TestCase):\n\n    alpha = 0.5\n    beta = 0.25\n\n    def setUp(self):\n        m, n = self.shape\n        self.a = scipy.sparse.random(m, n, density=0.3, dtype=self.dtype)\n        self.b = scipy.sparse.random(m, n, density=0.3, dtype=self.dtype)\n\n    def test_csrgeam(self):\n        if not cupy.cusparse.check_availability('csrgeam'):\n            pytest.skip('csrgeam is not available')\n        a = sparse.csr_matrix(self.a)\n        b = sparse.csr_matrix(self.b)\n        c = cupy.cusparse.csrgeam(a, b, alpha=self.alpha, beta=self.beta)\n        expect = self.alpha * self.a + self.beta * self.b\n        testing.assert_array_almost_equal(c.toarray(), expect.toarray())\n\n    def test_csrgeam2(self):\n        if not cupy.cusparse.check_availability('csrgeam2'):\n            pytest.skip('csrgeam2 is not available')\n        a = sparse.csr_matrix(self.a)\n        b = sparse.csr_matrix(self.b)\n        c = cupy.cusparse.csrgeam2(a, b, alpha=self.alpha, beta=self.beta)\n        expect = self.alpha * self.a + self.beta * self.b\n        testing.assert_array_almost_equal(c.toarray(), expect.toarray())\n\n\n@testing.with_requires('scipy')\nclass TestCsrgeamInvalidCases(unittest.TestCase):\n\n    dtype = numpy.float32\n    shape = (4, 3)\n\n    def setUp(self):\n        m, n = self.shape\n        self.a = scipy.sparse.random(m, n, density=0.3, dtype=self.dtype)\n        self.b = scipy.sparse.random(m, n, density=0.3, dtype=self.dtype)\n\n    def test_csrgeam_invalid_format(self):\n        if not cupy.cusparse.check_availability('csrgeam'):\n            pytest.skip('csrgeam is not available')\n        a = sparse.csc_matrix(self.a)\n        b = sparse.csr_matrix(self.b)\n        with self.assertRaises(TypeError):\n            cupy.cusparse.csrgeam(a, b)\n        with self.assertRaises(TypeError):\n            cupy.cusparse.csrgeam(b, a)\n\n    def test_csrgeam_invalid_shape(self):\n        if not cupy.cusparse.check_availability('csrgeam'):\n            pytest.skip('csrgeam is not available')\n        a = sparse.csr_matrix(self.a.T)\n        b = sparse.csr_matrix(self.b)\n        with self.assertRaises(ValueError):\n            cupy.cusparse.csrgeam(a, b)\n\n    def test_csrgeam_availability(self):\n        if not cupy.cusparse.check_availability('csrgeam'):\n            a = sparse.csr_matrix(self.a)\n            b = sparse.csr_matrix(self.b)\n            with self.assertRaises(RuntimeError):\n                cupy.cusparse.csrgeam(a, b)\n\n    def test_csrgeam2_invalid_format(self):\n        if not cupy.cusparse.check_availability('csrgeam2'):\n            pytest.skip('csrgeam2 is not available')\n        a = sparse.csc_matrix(self.a)\n        b = sparse.csr_matrix(self.b)\n        with self.assertRaises(TypeError):\n            cupy.cusparse.csrgeam2(a, b)\n        with self.assertRaises(TypeError):\n            cupy.cusparse.csrgeam2(b, a)\n\n    def test_csrgeam2_invalid_shape(self):\n        if not cupy.cusparse.check_availability('csrgeam2'):\n            pytest.skip('csrgeam2 is not available')\n        a = sparse.csr_matrix(self.a.T)\n        b = sparse.csr_matrix(self.b)\n        with self.assertRaises(ValueError):\n            cupy.cusparse.csrgeam2(a, b)\n\n    def test_csrgeam2_availability(self):\n        if not cupy.cusparse.check_availability('csrgeam2'):\n            a = sparse.csr_matrix(self.a)\n            b = sparse.csr_matrix(self.b)\n            with self.assertRaises(RuntimeError):\n                cupy.cusparse.csrgeam2(a, b)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64],\n    'transa': [False, True],\n    'transb': [False, True],\n}))\n@testing.with_requires('scipy')\nclass TestCsrgemm(unittest.TestCase):\n\n    def setUp(self):\n        self.op_a = scipy.sparse.random(2, 3, density=0.5, dtype=self.dtype)\n        if self.transa:\n            self.a = self.op_a.T\n        else:\n            self.a = self.op_a\n        self.op_b = scipy.sparse.random(3, 4, density=0.5, dtype=self.dtype)\n        if self.transb:\n            self.b = self.op_b.T\n        else:\n            self.b = self.op_b\n\n    def test_csrgemm(self):\n        if not cupy.cusparse.check_availability('csrgemm'):\n            pytest.skip('csrgemm is not available.')\n\n        a = sparse.csr_matrix(self.a)\n        b = sparse.csr_matrix(self.b)\n        y = cupy.cusparse.csrgemm(a, b, transa=self.transa, transb=self.transb)\n        expect = self.op_a.dot(self.op_b)\n        testing.assert_array_almost_equal(y.toarray(), expect.toarray())\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n    'shape': [(2, 3, 4), (4, 3, 2)]\n}))\n@testing.with_requires('scipy>=1.2.0')\nclass TestCsrgemm2(unittest.TestCase):\n\n    alpha = 0.5\n    beta = 0.25\n\n    def setUp(self):\n        m, n, k = self.shape\n        self.a = scipy.sparse.random(m, k, density=0.5, dtype=self.dtype)\n        self.b = scipy.sparse.random(k, n, density=0.5, dtype=self.dtype)\n        self.d = scipy.sparse.random(m, n, density=0.5, dtype=self.dtype)\n\n    def test_csrgemm2_ab(self):\n        if not cupy.cusparse.check_availability('csrgemm2'):\n            pytest.skip('csrgemm2 is not available.')\n\n        a = sparse.csr_matrix(self.a)\n        b = sparse.csr_matrix(self.b)\n        c = cupy.cusparse.csrgemm2(a, b, alpha=self.alpha)\n        expect = self.alpha * self.a.dot(self.b)\n        testing.assert_array_almost_equal(c.toarray(), expect.toarray())\n\n    def test_csrgemm2_abpd(self):\n        if not cupy.cusparse.check_availability('csrgemm2'):\n            pytest.skip('csrgemm2 is not available.')\n\n        a = sparse.csr_matrix(self.a)\n        b = sparse.csr_matrix(self.b)\n        d = sparse.csr_matrix(self.d)\n        c = cupy.cusparse.csrgemm2(a, b, d=d, alpha=self.alpha, beta=self.beta)\n        expect = self.alpha * self.a.dot(self.b) + self.beta * self.d\n        testing.assert_array_almost_equal(c.toarray(), expect.toarray())\n\n\n@testing.with_requires('scipy')\nclass TestCsrgemm2InvalidCases(unittest.TestCase):\n\n    dtype = numpy.float32\n    shape = (2, 3, 4)\n\n    def setUp(self):\n        m, n, k = self.shape\n        self.a = scipy.sparse.random(m, k, density=0.5, dtype=self.dtype)\n        self.b = scipy.sparse.random(k, n, density=0.5, dtype=self.dtype)\n        self.d = scipy.sparse.random(m, n, density=0.5, dtype=self.dtype)\n\n    def test_csrgemm2_invalid_format(self):\n        if not cupy.cusparse.check_availability('csrgemm2'):\n            pytest.skip('csrgemm2 is not available.')\n        a = sparse.csc_matrix(self.a)\n        b = sparse.csr_matrix(self.b)\n        with self.assertRaises(TypeError):\n            cupy.cusparse.csrgemm2(a, b)\n        a = sparse.csr_matrix(self.a)\n        b = sparse.csc_matrix(self.b)\n        with self.assertRaises(TypeError):\n            cupy.cusparse.csrgemm2(a, b)\n        a = sparse.csr_matrix(self.a)\n        b = sparse.csr_matrix(self.b)\n        d = sparse.csc_matrix(self.d)\n        with self.assertRaises(TypeError):\n            cupy.cusparse.csrgemm2(a, b, d=d)\n\n    def test_csrgemm2_invalid_shape(self):\n        if not cupy.cusparse.check_availability('csrgemm2'):\n            pytest.skip('csrgemm2 is not available.')\n        a = sparse.csc_matrix(self.a).T\n        b = sparse.csr_matrix(self.b)\n        with self.assertRaises(ValueError):\n            cupy.cusparse.csrgemm2(a, b)\n        a = sparse.csr_matrix(self.a)\n        b = sparse.csc_matrix(self.b).T\n        with self.assertRaises(ValueError):\n            cupy.cusparse.csrgemm2(a, b)\n        a = sparse.csr_matrix(self.a)\n        b = sparse.csr_matrix(self.b)\n        d = sparse.csc_matrix(self.d).T\n        with self.assertRaises(ValueError):\n            cupy.cusparse.csrgemm2(a, b, d=d)\n\n    def test_csrgemm2_availability(self):\n        if not cupy.cusparse.check_availability('csrgemm2'):\n            a = sparse.csr_matrix(self.a)\n            b = sparse.csr_matrix(self.b)\n            with self.assertRaises(RuntimeError):\n                cupy.cusparse.csrgemm2(a, b)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64],\n    'transa': [False, True],\n}))\n@testing.with_requires('scipy')\nclass TestCsrmv(unittest.TestCase):\n\n    alpha = 0.5\n    beta = 0.25\n\n    def setUp(self):\n        self.op_a = scipy.sparse.random(2, 3, density=0.5, dtype=self.dtype)\n        if self.transa:\n            self.a = self.op_a.T\n        else:\n            self.a = self.op_a\n        self.x = numpy.random.uniform(-1, 1, 3).astype(self.dtype)\n        self.y = numpy.random.uniform(-1, 1, 2).astype(self.dtype)\n\n    def test_csrmv(self):\n        a = sparse.csr_matrix(self.a)\n        x = cupy.array(self.x, order='f')\n        y = cupy.cusparse.csrmv(\n            a, x, alpha=self.alpha, transa=self.transa)\n        expect = self.alpha * self.op_a.dot(self.x)\n        testing.assert_array_almost_equal(y, expect)\n\n    def test_csrmv_with_y(self):\n        a = sparse.csr_matrix(self.a)\n        x = cupy.array(self.x, order='f')\n        y = cupy.array(self.y, order='f')\n        z = cupy.cusparse.csrmv(\n            a, x, y=y, alpha=self.alpha, beta=self.beta, transa=self.transa)\n        expect = self.alpha * self.op_a.dot(self.x) + self.beta * self.y\n        self.assertIs(y, z)\n        testing.assert_array_almost_equal(y, expect)\n\n    def test_csrmvEx_aligned(self):\n        a = sparse.csr_matrix(self.a)\n        x = cupy.array(self.x, order='f')\n\n        self.assertTrue(cupy.cusparse.csrmvExIsAligned(a, x))\n\n    def test_csrmvEx_not_aligned(self):\n        a = sparse.csr_matrix(self.a)\n        tmp = cupy.array(numpy.hstack([self.x, self.y]), order='f')\n        x = tmp[0:len(self.x)]\n        y = tmp[len(self.x):]\n        self.assertFalse(cupy.cusparse.csrmvExIsAligned(a, x, y))\n\n    def test_csrmvEx(self):\n        if self.transa:\n            # no support for transa\n            return\n\n        a = sparse.csr_matrix(self.a)\n        x = cupy.array(self.x, order='f')\n        y = cupy.cusparse.csrmvEx(a, x, alpha=self.alpha)\n        expect = self.alpha * self.op_a.dot(self.x)\n        testing.assert_array_almost_equal(y, expect)\n\n    def test_csrmvEx_with_y(self):\n        if self.transa:\n            # no support for transa\n            return\n        a = sparse.csr_matrix(self.a)\n        x = cupy.array(self.x, order='f')\n        y = cupy.array(self.y, order='f')\n        z = cupy.cusparse.csrmvEx(\n            a, x, y=y, alpha=self.alpha, beta=self.beta)\n        expect = self.alpha * self.op_a.dot(self.x) + self.beta * self.y\n        self.assertIs(y, z)\n        testing.assert_array_almost_equal(y, expect)\n\n\n@testing.with_requires('scipy')\nclass TestCoosort(unittest.TestCase):\n\n    def setUp(self):\n        self.a = scipy.sparse.random(\n            100, 100, density=0.9, dtype=numpy.float32, format='coo')\n\n    def test_coosort(self):\n        a = sparse.coo_matrix(self.a)\n        cupy.cusparse.coosort(a)\n        # lexsort by row first and col second\n        argsort = numpy.lexsort((self.a.col, self.a.row))\n        testing.assert_array_equal(self.a.row[argsort], a.row)\n        testing.assert_array_equal(self.a.col[argsort], a.col)\n        testing.assert_array_almost_equal(self.a.data[argsort], a.data)\n\n\n@testing.with_requires('scipy')\nclass TestCsrsort(unittest.TestCase):\n\n    def setUp(self):\n        self.a = scipy.sparse.random(\n            1, 1000, density=0.9, dtype=numpy.float32, format='csr')\n        numpy.random.shuffle(self.a.indices)\n        self.a.has_sorted_indices = False\n\n    def test_csrsort(self):\n        a = sparse.csr_matrix(self.a)\n        cupy.cusparse.csrsort(a)\n\n        self.a.sort_indices()\n        testing.assert_array_equal(self.a.indptr, a.indptr)\n        testing.assert_array_equal(self.a.indices, a.indices)\n        testing.assert_array_almost_equal(self.a.data, a.data)\n\n\n@testing.with_requires('scipy')\nclass TestCscsort(unittest.TestCase):\n\n    def setUp(self):\n        self.a = scipy.sparse.random(\n            1000, 1, density=0.9, dtype=numpy.float32, format='csc')\n        numpy.random.shuffle(self.a.indices)\n        self.a.has_sorted_indices = False\n\n    def test_csrsort(self):\n        a = sparse.csc_matrix(self.a)\n        cupy.cusparse.cscsort(a)\n\n        self.a.sort_indices()\n        testing.assert_array_equal(self.a.indptr, a.indptr)\n        testing.assert_array_equal(self.a.indices, a.indices)\n        testing.assert_array_almost_equal(self.a.data, a.data)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n    'transa': [False, True],\n    'shape': [(3, 2), (4, 3)],\n    'format': ['csr', 'csc', 'coo'],\n}))\n@testing.with_requires('scipy>=1.2.0')\nclass TestSpmv(unittest.TestCase):\n\n    alpha = 0.5\n    beta = 0.25\n\n    def setUp(self):\n        m, n = self.shape\n        self.op_a = scipy.sparse.random(m, n, density=0.5, format=self.format,\n                                        dtype=self.dtype)\n        if self.transa:\n            self.a = self.op_a.T\n        else:\n            self.a = self.op_a\n        self.x = numpy.random.uniform(-1, 1, n).astype(self.dtype)\n        self.y = numpy.random.uniform(-1, 1, m).astype(self.dtype)\n        if self.format == 'csr':\n            self.sparse_matrix = sparse.csr_matrix\n        elif self.format == 'csc':\n            self.sparse_matrix = sparse.csc_matrix\n        elif self.format == 'coo':\n            self.sparse_matrix = sparse.coo_matrix\n\n    def test_spmv(self):\n        if not cupy.cusparse.check_availability('spmv'):\n            pytest.skip('spmv is not available')\n        a = self.sparse_matrix(self.a)\n        if not a.has_canonical_format:\n            a.sum_duplicates()\n        x = cupy.array(self.x)\n        y = cupy.cusparse.spmv(a, x, alpha=self.alpha, transa=self.transa)\n        expect = self.alpha * self.op_a.dot(self.x)\n        testing.assert_array_almost_equal(y, expect)\n\n    def test_spmv_with_y(self):\n        if not cupy.cusparse.check_availability('spmv'):\n            pytest.skip('spmv is not available')\n        a = self.sparse_matrix(self.a)\n        if not a.has_canonical_format:\n            a.sum_duplicates()\n        x = cupy.array(self.x)\n        y = cupy.array(self.y)\n        z = cupy.cusparse.spmv(a, x, y=y, alpha=self.alpha, beta=self.beta,\n                               transa=self.transa)\n        expect = self.alpha * self.op_a.dot(self.x) + self.beta * self.y\n        self.assertIs(y, z)\n        testing.assert_array_almost_equal(y, expect)\n\n\n@testing.with_requires('scipy')\nclass TestErrorSpmv(unittest.TestCase):\n\n    dtype = numpy.float32\n\n    def setUp(self):\n        m, n = 2, 3\n        self.a = scipy.sparse.random(m, n, density=0.5,\n                                     dtype=self.dtype)\n        self.x = numpy.random.uniform(-1, 1, n).astype(self.dtype)\n        self.y = numpy.random.uniform(-1, 1, m).astype(self.dtype)\n\n    def test_error_shape(self):\n        if not cupy.cusparse.check_availability('spmv'):\n            pytest.skip('spmv is not available')\n\n        a = sparse.csr_matrix(self.a.T)\n        x = cupy.array(self.x)\n        with self.assertRaises(ValueError):\n            cupy.cusparse.spmv(a, x)\n\n        a = sparse.csr_matrix(self.a)\n        x = cupy.array(self.x)\n        with self.assertRaises(ValueError):\n            cupy.cusparse.spmv(a, x, transa=True)\n\n        a = sparse.csr_matrix(self.a)\n        x = cupy.array(self.y)\n        with self.assertRaises(ValueError):\n            cupy.cusparse.spmv(a, x)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n    'transa': [False, True],\n    'transb': [False, True],\n    'dims': [(2, 3, 4), (3, 4, 2)],\n    'format': ['csr', 'csc', 'coo'],\n}))\n@testing.with_requires('scipy>=1.2.0')\nclass TestSpmm(unittest.TestCase):\n\n    alpha = 0.5\n    beta = 0.25\n\n    def setUp(self):\n        m, n, k = self.dims\n        self.op_a = scipy.sparse.random(m, k, density=0.5, format=self.format,\n                                        dtype=self.dtype)\n        if self.transa:\n            self.a = self.op_a.T\n        else:\n            self.a = self.op_a\n        self.op_b = numpy.random.uniform(-1, 1, (k, n)).astype(self.dtype)\n        if self.transb:\n            self.b = self.op_b.T\n        else:\n            self.b = self.op_b\n        self.c = numpy.random.uniform(-1, 1, (m, n)).astype(self.dtype)\n        if self.format == 'csr':\n            self.sparse_matrix = sparse.csr_matrix\n        elif self.format == 'csc':\n            self.sparse_matrix = sparse.csc_matrix\n        elif self.format == 'coo':\n            self.sparse_matrix = sparse.coo_matrix\n\n    def test_spmm(self):\n        if not cupy.cusparse.check_availability('spmm'):\n            pytest.skip('spmm is not available')\n        a = self.sparse_matrix(self.a)\n        if not a.has_canonical_format:\n            a.sum_duplicates()\n        b = cupy.array(self.b, order='f')\n        c = cupy.cusparse.spmm(\n            a, b, alpha=self.alpha, transa=self.transa, transb=self.transb)\n        expect = self.alpha * self.op_a.dot(self.op_b)\n        testing.assert_array_almost_equal(c, expect)\n\n    def test_spmm_with_c(self):\n        if not cupy.cusparse.check_availability('spmm'):\n            pytest.skip('spmm is not available')\n        a = self.sparse_matrix(self.a)\n        if not a.has_canonical_format:\n            a.sum_duplicates()\n        b = cupy.array(self.b, order='f')\n        c = cupy.array(self.c, order='f')\n        y = cupy.cusparse.spmm(\n            a, b, c=c, alpha=self.alpha, beta=self.beta,\n            transa=self.transa, transb=self.transb)\n        expect = self.alpha * self.op_a.dot(self.op_b) + self.beta * self.c\n        self.assertIs(y, c)\n        testing.assert_array_almost_equal(y, expect)\n\n\n@testing.with_requires('scipy')\nclass TestErrorSpmm(unittest.TestCase):\n\n    dtype = numpy.float32\n\n    def setUp(self):\n        m, n, k = 2, 3, 4\n        self.a = scipy.sparse.random(m, k, density=0.5,\n                                     dtype=self.dtype)\n        self.b = numpy.random.uniform(-1, 1, (k, n)).astype(self.dtype)\n        self.c = numpy.random.uniform(-1, 1, (m, n)).astype(self.dtype)\n\n    def test_error_shape(self):\n        if not cupy.cusparse.check_availability('spmm'):\n            pytest.skip('spmm is not available')\n\n        a = sparse.csr_matrix(self.a.T)\n        b = cupy.array(self.b, order='f')\n        with self.assertRaises(ValueError):\n            cupy.cusparse.spmm(a, b)\n\n        a = sparse.csr_matrix(self.a)\n        b = cupy.array(self.b, order='f')\n        with self.assertRaises(AssertionError):\n            cupy.cusparse.spmm(a, b.T)\n\n        a = sparse.csr_matrix(self.a)\n        b = cupy.array(self.b)\n        with self.assertRaises(AssertionError):\n            cupy.cusparse.spmm(a, b)\n\n        a = sparse.csr_matrix(self.a)\n        b = cupy.array(self.c, order='f')\n        with self.assertRaises(ValueError):\n            cupy.cusparse.spmm(a, b)\n\n        a = sparse.csr_matrix(self.a)\n        b = cupy.array(self.b, order='f')\n        c = cupy.array(self.b, order='f')\n        with self.assertRaises(ValueError):\n            cupy.cusparse.spmm(a, b, c=c)\n"""
tests/cupy_tests/test_cutensor.py,0,"b""import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\n\nif cupy.cuda.cutensor_enabled:\n    from cupy import cutensor\n    from cupy.cuda import cutensor as ct\n\n\n@testing.parameterize(\n    {'dtype': numpy.float16, 'tol': 3e-3},\n    {'dtype': numpy.float32, 'tol': 1e-6},\n    {'dtype': numpy.float64, 'tol': 1e-12},\n    {'dtype': numpy.complex64, 'tol': 1e-6},\n    {'dtype': numpy.complex128, 'tol': 1e-12},\n)\n@unittest.skipUnless(cupy.cuda.cutensor_enabled, 'cuTensor is unavailable')\nclass TestCuTensor(unittest.TestCase):\n\n    def setUp(self):\n        self.a = testing.shaped_random(\n            (20, 40, 30), cupy, self.dtype, seed=0)\n        self.b = testing.shaped_random(\n            (40, 30, 20), cupy, self.dtype, seed=1)\n        self.c = testing.shaped_random(\n            (30, 20, 40), cupy, self.dtype, seed=2)\n        self.mode_a = ('y', 'z', 'x')\n        self.mode_b = ('z', 'x', 'y')\n        self.mode_c = ('x', 'y', 'z')\n        self.alpha = 1.1\n        self.beta = 1.2\n        self.gamma = 1.3\n        self.a_transposed = self.a.transpose(2, 0, 1).copy()\n        self.b_transposed = self.b.transpose(1, 2, 0).copy()\n        self.c_transposed = self.c.copy()\n\n    def test_elementwise_trinary(self):\n        desc_a = cutensor.create_tensor_descriptor(self.a)\n        desc_b = cutensor.create_tensor_descriptor(self.b)\n        desc_c = cutensor.create_tensor_descriptor(self.c)\n\n        d = cutensor.elementwise_trinary(\n            self.alpha, self.a, desc_a, self.mode_a,\n            self.beta, self.b, desc_b, self.mode_b,\n            self.gamma, self.c, desc_c, self.mode_c\n        )\n\n        assert d.dtype == self.dtype\n\n        testing.assert_allclose(\n            self.alpha * self.a_transposed +\n            self.beta * self.b_transposed +\n            self.gamma * self.c_transposed,\n            d,\n            rtol=self.tol, atol=self.tol\n        )\n\n    def test_elementwise_trinary_out(self):\n        out = testing.shaped_random(\n            (30, 20, 40), cupy, self.dtype, seed=3)\n\n        desc_a = cutensor.create_tensor_descriptor(self.a)\n        desc_b = cutensor.create_tensor_descriptor(self.b)\n        desc_c = cutensor.create_tensor_descriptor(self.c)\n\n        d = cutensor.elementwise_trinary(\n            self.alpha, self.a, desc_a, self.mode_a,\n            self.beta, self.b, desc_b, self.mode_b,\n            self.gamma, self.c, desc_c, self.mode_c, out=out\n        )\n\n        assert d is out\n        testing.assert_allclose(\n            self.alpha * self.a_transposed +\n            self.beta * self.b_transposed +\n            self.gamma * self.c,\n            d,\n            rtol=self.tol, atol=self.tol\n        )\n\n    def test_elementwise_binary(self):\n        desc_a = cutensor.create_tensor_descriptor(self.a)\n        desc_c = cutensor.create_tensor_descriptor(self.c)\n\n        d = cutensor.elementwise_binary(\n            self.alpha, self.a, desc_a, self.mode_a,\n            self.gamma, self.c, desc_c, self.mode_c\n        )\n\n        assert d.dtype == self.dtype\n\n        testing.assert_allclose(\n            self.alpha * self.a_transposed +\n            self.gamma * self.c_transposed,\n            d,\n            rtol=self.tol, atol=self.tol\n        )\n\n    def test_elementwise_binary_out(self):\n        out = testing.shaped_random(\n            (30, 20, 40), cupy, self.dtype, seed=3)\n        desc_a = cutensor.create_tensor_descriptor(self.a)\n        desc_c = cutensor.create_tensor_descriptor(self.c)\n\n        d = cutensor.elementwise_binary(\n            self.alpha, self.a, desc_a, self.mode_a,\n            self.gamma, self.c, desc_c, self.mode_c, out=out\n        )\n\n        assert d is out\n        testing.assert_allclose(\n            self.alpha * self.a_transposed +\n            self.gamma * self.c_transposed,\n            d,\n            rtol=self.tol, atol=self.tol\n        )\n\n    def test_contraction(self):\n        desc_a = cutensor.create_tensor_descriptor(self.a)\n        desc_b = cutensor.create_tensor_descriptor(self.b)\n        desc_c = cutensor.create_tensor_descriptor(self.c)\n\n        d = cutensor.contraction(\n            self.alpha, self.a, desc_a, self.mode_a,\n            self.b, desc_b, self.mode_b,\n            self.beta, self.c, desc_c, self.mode_c\n        )\n\n        assert self.c is d\n        testing.assert_allclose(\n            self.alpha * self.a_transposed * self.b_transposed +\n            self.beta * self.c_transposed,\n            d,\n            rtol=self.tol, atol=self.tol\n        )\n\n    def test_reduction(self):\n        if self.dtype == numpy.float16:\n            self.skipTest('Not supported.')\n\n        c = testing.shaped_random((30,), cupy, self.dtype, seed=2)\n        c_orig = c.copy()\n\n        desc_a = cutensor.create_tensor_descriptor(self.a)\n        desc_c = cutensor.create_tensor_descriptor(c)\n\n        d = cutensor.reduction(\n            self.alpha, self.a, desc_a, self.mode_a,\n            self.beta, c, desc_c, ('x',)\n        )\n\n        assert c is d\n        testing.assert_allclose(\n            self.alpha * self.a_transposed.sum(axis=(1, 2)) +\n            self.beta * c_orig,\n            d,\n            rtol=self.tol, atol=self.tol\n        )\n\n\n@unittest.skipUnless(cupy.cuda.cutensor_enabled, 'cuTensor is unavailable')\nclass TestCuTensorDescriptor(unittest.TestCase):\n\n    def setUp(self):\n        self.a = testing.shaped_random(\n            (20, 40, 30), cupy, numpy.float32, seed=0)\n        self.b = testing.shaped_random(\n            (40, 30, 20), cupy, numpy.float32, seed=1)\n        self.c = testing.shaped_random(\n            (30, 20, 40), cupy, numpy.float32, seed=2)\n        self.mode_a = ('y', 'z', 'x')\n        self.mode_b = ('z', 'x', 'y')\n        self.mode_c = ('x', 'y', 'z')\n        self.alpha = 1.1\n        self.beta = 1.2\n        self.gamma = 1.3\n        self.a_transposed = self.a.transpose(2, 0, 1).copy()\n        self.b_transposed = self.b.transpose(1, 2, 0).copy()\n        self.c_transposed = self.c.copy()\n\n    def test_elementwise_trinary(self):\n        desc_a = cutensor.create_tensor_descriptor(self.a, ct.OP_SQRT)\n        desc_b = cutensor.create_tensor_descriptor(self.b, ct.OP_TANH)\n        desc_c = cutensor.create_tensor_descriptor(self.c, ct.OP_COS)\n\n        d = cutensor.elementwise_trinary(\n            self.alpha, self.a, desc_a, self.mode_a,\n            self.beta, self.b, desc_b, self.mode_b,\n            self.gamma, self.c, desc_c, self.mode_c,\n            op_AB=ct.OP_ADD, op_ABC=ct.OP_MUL\n        )\n\n        testing.assert_allclose(\n            (self.alpha * cupy.sqrt(self.a_transposed) +\n             self.beta * cupy.tanh(self.b_transposed)) *\n            self.gamma * cupy.cos(self.c),\n            d,\n            rtol=1e-6, atol=1e-6\n        )\n\n    def test_elementwise_binary(self):\n        desc_a = cutensor.create_tensor_descriptor(self.a, ct.OP_SIGMOID)\n        desc_c = cutensor.create_tensor_descriptor(self.c, ct.OP_ABS)\n\n        d = cutensor.elementwise_binary(\n            self.alpha, self.a, desc_a, self.mode_a,\n            self.gamma, self.c, desc_c, self.mode_c,\n            op_AC=ct.OP_MUL\n        )\n\n        testing.assert_allclose(\n            self.alpha * (1 / (1 + cupy.exp(-self.a_transposed))) *\n            self.gamma * cupy.abs(self.c),\n            d,\n            rtol=1e-6, atol=1e-6\n        )\n\n    def test_reduction(self):\n        c = testing.shaped_random((30,), cupy, numpy.float32, seed=2)\n        c_orig = c.copy()\n\n        desc_a = cutensor.create_tensor_descriptor(self.a, ct.OP_COS)\n        desc_c = cutensor.create_tensor_descriptor(c, ct.OP_TANH)\n\n        d = cutensor.reduction(\n            self.alpha, self.a, desc_a, self.mode_a,\n            self.beta, c, desc_c, ('x',),\n            reduce_op=ct.OP_MAX\n        )\n\n        assert c is d\n        testing.assert_allclose(\n            self.alpha * cupy.cos(self.a_transposed).max(axis=(1, 2)) +\n            self.beta * cupy.tanh(c_orig),\n            d,\n            rtol=1e-6, atol=1e-6\n        )\n"""
tests/cupy_tests/test_init.py,0,"b""import os\nimport shutil\nimport subprocess\nimport sys\nimport tempfile\nimport unittest\n\nimport mock\nimport numpy\n\nimport cupy\nfrom cupy import testing\nimport cupyx\n\n\ndef _run_script(code):\n    # subprocess is required not to interfere with cupy module imported in top\n    # of this file\n    temp_dir = tempfile.mkdtemp()\n    try:\n        script_path = os.path.join(temp_dir, 'script.py')\n        with open(script_path, 'w') as f:\n            f.write(code)\n        proc = subprocess.Popen(\n            [sys.executable, script_path],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE)\n        stdoutdata, stderrdata = proc.communicate()\n    finally:\n        shutil.rmtree(temp_dir, ignore_errors=True)\n    return proc.returncode, stdoutdata, stderrdata\n\n\ndef _test_cupy_available(self):\n    returncode, stdoutdata, stderrdata = _run_script('''\nimport cupy\nprint(cupy.is_available())''')\n    self.assertEqual(returncode, 0, 'stderr: {!r}'.format(stderrdata))\n    self.assertIn(stdoutdata,\n                  (b'True\\n', b'True\\r\\n', b'False\\n', b'False\\r\\n'))\n    return stdoutdata == b'True\\n' or stdoutdata == b'True\\r\\n'\n\n\nclass TestImportError(unittest.TestCase):\n\n    def test_import_error(self):\n        returncode, stdoutdata, stderrdata = _run_script('''\ntry:\n    import cupy\nexcept Exception as e:\n    print(type(e).__name__)\n''')\n        self.assertEqual(returncode, 0, 'stderr: {!r}'.format(stderrdata))\n        self.assertIn(stdoutdata, (b'', b'RuntimeError\\n'))\n\n\nclass TestAvailable(unittest.TestCase):\n\n    @testing.gpu\n    def test_available(self):\n        available = _test_cupy_available(self)\n        self.assertTrue(available)\n\n\nclass TestNotAvailable(unittest.TestCase):\n\n    def setUp(self):\n        self.old = os.environ.get('CUDA_VISIBLE_DEVICES')\n\n    def tearDown(self):\n        if self.old is None:\n            os.environ.pop('CUDA_VISIBLE_DEVICES')\n        else:\n            os.environ['CUDA_VISIBLE_DEVICES'] = self.old\n\n    def test_no_device_1(self):\n        os.environ['CUDA_VISIBLE_DEVICES'] = ' '\n        available = _test_cupy_available(self)\n        self.assertFalse(available)\n\n    def test_no_device_2(self):\n        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n        available = _test_cupy_available(self)\n        self.assertFalse(available)\n\n\nclass TestMemoryPool(unittest.TestCase):\n\n    def test_get_default_memory_pool(self):\n        p = cupy.get_default_memory_pool()\n        self.assertIsInstance(p, cupy.cuda.memory.MemoryPool)\n\n    def test_get_default_pinned_memory_pool(self):\n        p = cupy.get_default_pinned_memory_pool()\n        self.assertIsInstance(p, cupy.cuda.pinned_memory.PinnedMemoryPool)\n\n\nclass TestShowConfig(unittest.TestCase):\n\n    def test_show_config(self):\n        with mock.patch('sys.stdout.write') as write_func:\n            cupy.show_config()\n        write_func.assert_called_once_with(str(cupyx.get_runtime_info()))\n\n\nclass TestAliases(unittest.TestCase):\n\n    def test_abs_is_absolute(self):\n        for xp in (numpy, cupy):\n            assert xp.abs is xp.absolute\n\n    def test_conj_is_conjugate(self):\n        for xp in (numpy, cupy):\n            assert xp.conj is xp.conjugate\n\n    def test_bitwise_not_is_invert(self):\n        for xp in (numpy, cupy):\n            assert xp.bitwise_not is xp.invert\n\n\n# This is copied from chainer/testing/__init__.py, so should be replaced in\n# some way.\nif __name__ == '__main__':\n    import pytest\n    pytest.main([__file__, '-vvs', '-x', '--pdb'])\n"""
tests/cupy_tests/test_numpy_interop.py,0,"b'import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\n\n\ntry:\n    import scipy.sparse\n    scipy_available = True\nexcept ImportError:\n    scipy_available = False\n\n\n@testing.gpu\nclass TestGetArrayModule(unittest.TestCase):\n\n    def test_get_array_module_1(self):\n        n1 = numpy.array([2], numpy.float32)\n        c1 = cupy.array([2], numpy.float32)\n        csr1 = cupy.sparse.csr_matrix((5, 3), dtype=numpy.float32)\n\n        self.assertIs(numpy, cupy.get_array_module())\n        self.assertIs(numpy, cupy.get_array_module(n1))\n        self.assertIs(cupy, cupy.get_array_module(c1))\n        self.assertIs(cupy, cupy.get_array_module(csr1))\n\n        self.assertIs(numpy, cupy.get_array_module(n1, n1))\n        self.assertIs(cupy, cupy.get_array_module(c1, c1))\n        self.assertIs(cupy, cupy.get_array_module(csr1, csr1))\n\n        self.assertIs(cupy, cupy.get_array_module(n1, csr1))\n        self.assertIs(cupy, cupy.get_array_module(csr1, n1))\n        self.assertIs(cupy, cupy.get_array_module(c1, n1))\n        self.assertIs(cupy, cupy.get_array_module(n1, c1))\n        self.assertIs(cupy, cupy.get_array_module(c1, csr1))\n        self.assertIs(cupy, cupy.get_array_module(csr1, c1))\n\n        if scipy_available:\n            csrn1 = scipy.sparse.csr_matrix((5, 3), dtype=numpy.float32)\n\n            self.assertIs(numpy, cupy.get_array_module(csrn1))\n            self.assertIs(cupy, cupy.get_array_module(csrn1, csr1))\n            self.assertIs(cupy, cupy.get_array_module(csr1, csrn1))\n            self.assertIs(cupy, cupy.get_array_module(c1, csrn1))\n            self.assertIs(cupy, cupy.get_array_module(csrn1, c1))\n            self.assertIs(numpy, cupy.get_array_module(n1, csrn1))\n            self.assertIs(numpy, cupy.get_array_module(csrn1, n1))\n'"
tests/cupy_tests/test_type_routines.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\ndef _generate_type_routines_input(xp, dtype, obj_type):\n    dtype = numpy.dtype(dtype)\n    if obj_type == 'dtype':\n        return dtype\n    if obj_type == 'specifier':\n        return str(dtype)\n    if obj_type == 'scalar':\n        return dtype.type(3)\n    if obj_type == 'array':\n        return xp.zeros(3, dtype=dtype)\n    if obj_type == 'primitive':\n        return type(dtype.type(3).tolist())\n    assert False\n\n\n@testing.parameterize(\n    *testing.product({\n        'obj_type': ['dtype', 'specifier', 'scalar', 'array', 'primitive'],\n    })\n)\nclass TestCanCast(unittest.TestCase):\n\n    @testing.for_all_dtypes_combination(names=('from_dtype', 'to_dtype'))\n    @testing.numpy_cupy_equal()\n    def test_can_cast(self, xp, from_dtype, to_dtype):\n        from_obj = _generate_type_routines_input(xp, from_dtype, self.obj_type)\n        ret = xp.can_cast(from_obj, to_dtype)\n        assert isinstance(ret, bool)\n        return ret\n\n\nclass TestCommonType(unittest.TestCase):\n\n    @testing.numpy_cupy_equal()\n    def test_common_type_empty(self, xp):\n        ret = xp.common_type()\n        assert type(ret) == type\n        return ret\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_equal()\n    def test_common_type_single_argument(self, xp, dtype):\n        array = _generate_type_routines_input(xp, dtype, 'array')\n        ret = xp.common_type(array)\n        assert type(ret) == type\n        return ret\n\n    @testing.for_all_dtypes_combination(\n        names=('dtype1', 'dtype2'), no_bool=True)\n    @testing.numpy_cupy_equal()\n    def test_common_type_two_arguments(self, xp, dtype1, dtype2):\n        array1 = _generate_type_routines_input(xp, dtype1, 'array')\n        array2 = _generate_type_routines_input(xp, dtype2, 'array')\n        ret = xp.common_type(array1, array2)\n        assert type(ret) == type\n        return ret\n\n    @testing.for_all_dtypes()\n    def test_common_type_bool(self, dtype):\n        for xp in (numpy, cupy):\n            array1 = _generate_type_routines_input(xp, dtype, 'array')\n            array2 = _generate_type_routines_input(xp, 'bool_', 'array')\n            with pytest.raises(TypeError):\n                xp.common_type(array1, array2)\n\n\n@testing.parameterize(\n    *testing.product({\n        'obj_type1': ['dtype', 'specifier', 'scalar', 'array', 'primitive'],\n        'obj_type2': ['dtype', 'specifier', 'scalar', 'array', 'primitive'],\n    })\n)\nclass TestResultType(unittest.TestCase):\n\n    @testing.for_all_dtypes_combination(names=('dtype1', 'dtype2'))\n    @testing.numpy_cupy_equal()\n    def test_result_type(self, xp, dtype1, dtype2):\n        input1 = _generate_type_routines_input(xp, dtype1, self.obj_type1)\n        input2 = _generate_type_routines_input(xp, dtype2, self.obj_type2)\n        ret = xp.result_type(input1, input2)\n        assert isinstance(ret, numpy.dtype)\n        return ret\n"""
tests/cupyx_tests/__init__.py,0,b''
tests/cupyx_tests/test_cupyx.py,0,"b""import unittest\n\nimport cupyx\nfrom cupy import testing\n\n\n@testing.parameterize(*testing.product({\n    'divide': [None],\n}))\nclass TestErrState(unittest.TestCase):\n\n    def test_errstate(self):\n        with cupyx.errstate(divide=self.divide):\n            state = cupyx.geterr()\n            assert state['divide'] == self.divide\n\n    def test_seterr(self):\n        pass\n\n\n# TODO(hvy): Implement TestErrStateDivide\n\n# TODO(hvy): Implement TestErrStateOver\n\n# TODO(hvy): Implement TestErrStateUnder\n\n# TODO(hvy): Implement TestErrStateInvalid\n"""
tests/cupyx_tests/test_optimize.py,0,"b""import mock\nimport pytest\nimport tempfile\nimport unittest\n\nimport cupy\nfrom cupy import testing\n\n\ntry:\n    import warnings\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=DeprecationWarning)\n        import cupyx.optimizing\n        import cupyx.optimizing._optimize\n        import cupy.core._optimize_config\nexcept ImportError:\n    pass\n\n\n@testing.gpu\n@testing.with_requires('optuna')\nclass TestOptimize(unittest.TestCase):\n\n    def setUp(self):\n        cupy.core._optimize_config._clear_all_contexts_cache()\n\n    def test_optimize_reduction_kernel(self):\n        my_sum = cupy.ReductionKernel(\n            'T x', 'T out', 'x', 'a + b', 'out = a', '0', 'my_sum')\n        x = testing.shaped_arange((3, 4), cupy)\n        y1 = my_sum(x, axis=1)\n        with cupyx.optimizing.optimize():\n            y2 = my_sum(x, axis=1)\n        testing.assert_array_equal(y1, y2)\n\n    def test_optimize_cache(self):\n        target = cupyx.optimizing._optimize._optimize\n        target_full_name = '{}.{}'.format(target.__module__, target.__name__)\n\n        with mock.patch(target_full_name) as optimize_impl:\n            my_sum = cupy.ReductionKernel(\n                'T x', 'T out', 'x', 'a + b', 'out = a', '0', 'my_sum')\n            my_sum_ = cupy.ReductionKernel(\n                'T x', 'T out', 'x', 'a + b', 'out = a', '0', 'my_sum_')\n            x = testing.shaped_arange((3, 4), cupy)\n            x_ = testing.shaped_arange((3, 4), cupy)\n            y = testing.shaped_arange((4, 4), cupy)\n            z = testing.shaped_arange((3, 4), cupy)[::-1]\n            assert x.strides == y.strides\n            assert x.shape == z.shape\n\n            with cupyx.optimizing.optimize():\n                my_sum(x, axis=1)\n                assert optimize_impl.call_count == 1\n                my_sum(x, axis=1)\n                assert optimize_impl.call_count == 1\n                my_sum(x, axis=0)\n                assert optimize_impl.call_count == 2\n                my_sum(x_, axis=1)\n                assert optimize_impl.call_count == 2\n                my_sum(y, axis=1)\n                assert optimize_impl.call_count == 3\n                my_sum(z, axis=1)\n                assert optimize_impl.call_count == 4\n                my_sum_(x, axis=1)\n                assert optimize_impl.call_count == 5\n\n            with cupyx.optimizing.optimize(key='new_key'):\n                my_sum(x, axis=1)\n                assert optimize_impl.call_count == 6\n\n            with cupyx.optimizing.optimize(key=None):\n                my_sum(x, axis=1)\n                assert optimize_impl.call_count == 6\n                my_sum(x)\n                assert optimize_impl.call_count == 7\n\n    @testing.multi_gpu(2)\n    def test_optimize_cache_multi_gpus(self):\n        target = cupyx.optimizing._optimize._optimize\n        target_full_name = '{}.{}'.format(target.__module__, target.__name__)\n\n        with mock.patch(target_full_name) as optimize_impl:\n            my_sum = cupy.ReductionKernel(\n                'T x', 'T out', 'x', 'a + b', 'out = a', '0', 'my_sum')\n\n            with cupyx.optimizing.optimize():\n                with cupy.cuda.Device(0):\n                    x = testing.shaped_arange((3, 4), cupy)\n                    my_sum(x, axis=1)\n                    assert optimize_impl.call_count == 1\n\n                with cupy.cuda.Device(1):\n                    x = testing.shaped_arange((3, 4), cupy)\n                    my_sum(x, axis=1)\n                    assert optimize_impl.call_count == 2\n\n    def test_optimize_pickle(self):\n        my_sum = cupy.ReductionKernel(\n            'T x', 'T out', 'x', 'a + b', 'out = a', '0', 'my_sum')\n        x = testing.shaped_arange((3, 4), cupy)\n\n        with tempfile.TemporaryDirectory() as directory:\n            filepath = directory + '/optimize_params'\n\n            with cupyx.optimizing.optimize() as context:\n                my_sum(x, axis=1)\n                params_map = context._params_map\n                context.save(filepath)\n\n            cupy.core._optimize_config._clear_all_contexts_cache()\n\n            with cupyx.optimizing.optimize() as context:\n                assert params_map.keys() != context._params_map.keys()\n                context.load(filepath)\n                assert params_map.keys() == context._params_map.keys()\n\n            with cupyx.optimizing.optimize(key='other_key') as context:\n                with pytest.raises(ValueError):\n                    context.load(filepath)\n"""
tests/cupyx_tests/test_rsqrt.py,0,"b'import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\nimport cupyx\n\n\n@testing.gpu\nclass TestRsqrt(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    def test_rsqrt(self, dtype):\n        # Adding 1.0 to avoid division by zero.\n        a = testing.shaped_arange((2, 3), numpy, dtype) + 1.0\n        out = cupyx.rsqrt(cupy.array(a))\n        # numpy.sqrt is broken in numpy<1.11.2\n        testing.assert_allclose(out, 1.0 / numpy.sqrt(a))\n'"
tests/cupyx_tests/test_runtime.py,0,"b""import unittest\n\nimport mock\n\nimport cupy\nimport cupyx\n\n\ntry:\n    import cupy.cuda.cudnn as cudnn\nexcept ImportError:\n    cudnn = None\n\n\ndef _get_error_func(error, *args, **kwargs):\n    def raise_error(*_args, **_kwargs):\n        raise error(*args, **kwargs)\n    return raise_error\n\n\nclass TestRuntime(unittest.TestCase):\n    def test_runtime(self):\n        runtime = cupyx.get_runtime_info()\n        assert cupy.__version__ == runtime.cupy_version\n        assert cupy.__version__ in str(runtime)\n\n    @unittest.skipUnless(cudnn is not None, 'cuDNN is required')\n    def test_error(self):\n        runtime = cupyx.get_runtime_info()\n        assert 'Error' not in str(runtime)\n\n        with mock.patch(\n                'cupy.cuda.runtime.driverGetVersion',\n                side_effect=_get_error_func(\n                    cupy.cuda.runtime.CUDARuntimeError, 0)):\n            runtime = cupyx.get_runtime_info()\n            assert 'CUDARuntimeError' in str(runtime)\n\n        with mock.patch(\n                'cupy.cuda.runtime.runtimeGetVersion',\n                side_effect=_get_error_func(\n                    cupy.cuda.runtime.CUDARuntimeError, 0)):\n            runtime = cupyx.get_runtime_info()\n            assert 'CUDARuntimeError' in str(runtime)\n\n        with mock.patch(\n                'cupy.cuda.cudnn.getVersion',\n                side_effect=_get_error_func(cudnn.CuDNNError, 0)):\n            runtime = cupyx.get_runtime_info()\n            assert 'CuDNNError' in str(runtime)\n"""
tests/cupyx_tests/test_scatter.py,0,"b'import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\nimport cupyx\n\n\n@testing.gpu\nclass TestScatter(unittest.TestCase):\n\n    def test_scatter_add(self):\n        a = cupy.zeros((3,), dtype=numpy.float32)\n        i = cupy.array([1, 1], numpy.int32)\n        v = cupy.array([2., 1.], dtype=numpy.float32)\n        cupyx.scatter_add(a, i, v)\n        testing.assert_array_equal(a, cupy.array([0, 3, 0]))\n\n    def test_scatter_max(self):\n        a = cupy.zeros((4,), dtype=numpy.float32)\n        i = cupy.array([0, 1, 0, 1, 2, 2], numpy.int32)\n        v = cupy.array([-1, 1, 1, 3, 2, 4], dtype=numpy.float32)\n        cupyx.scatter_max(a, i, v)\n        testing.assert_array_equal(a, cupy.array([1, 3, 4, 0]))\n\n    def test_scatter_min(self):\n        a = cupy.zeros((4,), dtype=numpy.float32)\n        i = cupy.array([0, 1, 0, 1, 2, 2], numpy.int32)\n        v = cupy.array([1, -1, -1, -3, -2, -4], dtype=numpy.float32)\n        cupyx.scatter_min(a, i, v)\n        testing.assert_array_equal(a, cupy.array([-1, -3, -4, 0]))\n'"
tests/cupyx_tests/test_time.py,0,"b""import mock\nimport unittest\n\nimport numpy\n\nimport cupy\nimport cupyx\n\n\nclass TestRepeat(unittest.TestCase):\n\n    def test_cpu_routine(self):\n        with mock.patch('time.perf_counter',\n                        mock.Mock(side_effect=[2.4, 3.8, 3.8] * 10)):\n            with mock.patch('cupy.cuda.get_elapsed_time',\n                            mock.Mock(return_value=2500)):\n                mock_func = mock.Mock()\n                mock_func.__name__ = 'test_name_xxx'\n                x = cupy.testing.shaped_random((2, 3), cupy, 'int32')\n                y = cupy.testing.shaped_random((2, 3), cupy, 'int32')\n                assert mock_func.call_count == 0\n\n                perf = cupyx.time.repeat(\n                    mock_func, (x, y), n_repeat=10, n_warmup=3)\n\n                assert perf.name == 'test_name_xxx'\n                assert mock_func.call_count == 13\n                assert perf.cpu_times.shape == (10,)\n                assert perf.gpu_times.shape == (10,)\n                assert (perf.cpu_times == 1.4).all()\n                assert (perf.gpu_times == 2.5).all()\n\n    def test_repeat_max_duration(self):\n        with mock.patch('time.perf_counter',\n                        mock.Mock(side_effect=[1., 2., 2.] * 6)):\n            with mock.patch('cupy.cuda.get_elapsed_time',\n                            mock.Mock(return_value=2500)):\n                mock_func = mock.Mock()\n                mock_func.__name__ = 'test_name_xxx'\n                x = cupy.testing.shaped_random((2, 3), cupy, 'int32')\n                y = cupy.testing.shaped_random((2, 3), cupy, 'int32')\n                assert mock_func.call_count == 0\n\n                perf = cupyx.time.repeat(\n                    mock_func, (x, y), n_warmup=3, max_duration=2.5)\n\n                assert perf.name == 'test_name_xxx'\n                assert mock_func.call_count == 6\n                assert perf.cpu_times.shape == (3,)\n                assert perf.gpu_times.shape == (3,)\n                assert (perf.cpu_times == 1.).all()\n                assert (perf.gpu_times == 2.5).all()\n\n    def test_repeat_kwargs(self):\n        x = cupy.random.rand(5)\n        cupyx.time.repeat(\n            cupy.nonzero, kwargs={'a': x}, n_repeat=1, n_warmup=1)\n\n\nclass TestPerfCaseResult(unittest.TestCase):\n    def test_show_gpu(self):\n        times = numpy.array([\n            [5.4, 7.1, 6.0, 5.4, 4.2],\n            [6.4, 4.3, 8.9, 9.6, 3.8],\n        ]) * 1e-6\n        perf = cupyx.time._PerfCaseResult('test_name_xxx', times)\n        expected = (\n            'test_name_xxx       :'\n            '    CPU:    5.620 us   +/- 0.943 '\n            '(min:    4.200 / max:    7.100) us '\n            '    GPU:    6.600 us   +/- 2.344 '\n            '(min:    3.800 / max:    9.600) us'\n        )\n        assert str(perf) == expected\n\n    def test_no_show_gpu(self):\n        times = numpy.array([\n            [5.4, 7.1, 6.0, 5.4, 4.2],\n            [6.4, 4.3, 8.9, 9.6, 3.8],\n        ]) * 1e-6\n        perf = cupyx.time._PerfCaseResult('test_name_xxx', times)\n        expected = (\n            'test_name_xxx       :'\n            '    CPU:    5.620 us   +/- 0.943 '\n            '(min:    4.200 / max:    7.100) us'\n        )\n        assert perf.to_str() == expected\n        # Checks if the result does not change.\n        assert perf.to_str() == expected\n\n    def test_single_show_gpu(self):\n        times = numpy.array([[5.4], [6.4]]) * 1e-6\n        perf = cupyx.time._PerfCaseResult('test_name_xxx', times)\n        assert str(perf) == ('test_name_xxx       :    CPU:    5.400 us '\n                             '    GPU:    6.400 us')\n\n    def test_single_no_show_gpu(self):\n        times = numpy.array([[5.4], [6.4]]) * 1e-6\n        perf = cupyx.time._PerfCaseResult('test_name_xxx', times)\n        assert perf.to_str() == 'test_name_xxx       :    CPU:    5.400 us'\n"""
tests/example_tests/__init__.py,0,b''
tests/example_tests/example_test.py,0,"b""import os\nimport subprocess\nimport sys\n\n\ndef run_example(path, *args):\n    examples_path = os.path.join(\n        os.path.dirname(__file__), '..', '..', 'examples')\n    fullpath = os.path.join(examples_path, path)\n\n    try:\n        return subprocess.check_output(\n            (sys.executable, fullpath) + args,\n            stderr=subprocess.STDOUT)\n    except subprocess.CalledProcessError as e:\n        print('Original error message:')\n        print(e.output.decode('utf-8'))\n        raise\n"""
tests/example_tests/test_finance.py,0,"b""import os\nimport re\nimport unittest\n\nfrom example_tests import example_test\n\nfrom cupy import testing\n\n\ndef _normalize_regexp_eol(pattern):\n    return pattern.replace(r'\\n', re.escape(os.linesep))\n\n\nclass TestBlackScholes(unittest.TestCase):\n\n    def test_black_scholes(self):\n        output = example_test.run_example(\n            'finance/black_scholes.py', '--n-options', '10')\n        pattern = _normalize_regexp_eol(\n            r'initializing...\\n' +\n            r'start computation\\n' +\n            r' CPU \\(NumPy, Naive implementation\\):\\t[0-9\\.]+ sec\\n' +\n            r' GPU \\(CuPy, Naive implementation\\):\\t[0-9\\.]+ sec\\n' +\n            r' GPU \\(CuPy, Elementwise kernel\\):\\t[0-9\\.]+ sec')\n        self.assertRegex(output.decode('utf-8'), pattern)\n\n\nclass TestMonteCarlo(unittest.TestCase):\n\n    def test_monte_carlo(self):\n        output = example_test.run_example(\n            'finance/monte_carlo.py', '--n-options', '10',\n            '--n-samples-per-thread', '10',\n            '--n-threads-per-option', '10')\n        pattern = _normalize_regexp_eol(\n            r'initializing...\\n' +\n            r'start computation\\n' +\n            r'    # of options: 10\\n' +\n            r'    # of samples per option: 100\\n' +\n            r'GPU \\(CuPy, Monte Carlo method\\):\\t[0-9\\.]+ sec\\n' +\n            r'Error: [0-9\\.]+')\n        self.assertRegex(output.decode('utf-8'), pattern)\n\n\nclass TestMonteCarloWithMultiGPU(unittest.TestCase):\n\n    @testing.multi_gpu(2)\n    def test_monte_carlo_multigpu(self):\n        output = example_test.run_example(\n            'finance/monte_carlo_multigpu.py', '--gpus', '0', '1',\n            '--n-options', '10',\n            '--n-samples-per-thread', '10',\n            '--n-threads-per-option', '10')\n        pattern = _normalize_regexp_eol(\n            r'initializing...\\n' +\n            r'start computation\\n' +\n            r'    # of gpus: 2\\n' +\n            r'    # of options: 10\\n' +\n            r'    # of samples per option: 200\\n' +\n            r'GPU \\(CuPy, Monte Carlo method\\):\\t[0-9\\.]+ sec\\n' +\n            r'Error: [0-9\\.]+')\n        self.assertRegex(output.decode('utf-8'), pattern)\n"""
tests/example_tests/test_gemm.py,0,"b""import unittest\n\nfrom example_tests import example_test\n\n\nclass TestGEMM(unittest.TestCase):\n\n    def test_sgemm(self):\n        example_test.run_example('gemm/sgemm.py')\n"""
tests/example_tests/test_gmm.py,0,"b""import os\nimport shutil\nimport tempfile\nimport unittest\n\nfrom cupy import testing\n\nfrom example_tests import example_test\n\n\nos.environ['MPLBACKEND'] = 'Agg'\n\n\n@testing.with_requires('matplotlib')\n@testing.with_requires('scipy')\nclass TestGMM(unittest.TestCase):\n\n    def test_gmm(self):\n        output = example_test.run_example('gmm/gmm.py', '--num', '10')\n        self.assertRegex(\n            output.decode('utf-8'),\n            r'Running CPU\\.\\.\\.\\s+train_accuracy : [0-9\\.]+\\s+' +\n            r'test_accuracy : [0-9\\.]+\\s+CPU :  [0-9\\.]+ sec\\s+' +\n            r'Running GPU\\.\\.\\.\\s+train_accuracy : [0-9\\.]+\\s+' +\n            r'test_accuracy : [0-9\\.]+\\s+GPU :  [0-9\\.]+ sec')\n\n    def test_output_image(self):\n        dir_path = tempfile.mkdtemp()\n        try:\n            image_path = os.path.join(dir_path, 'gmm.png')\n            example_test.run_example(\n                'gmm/gmm.py', '--num', '10', '-o', image_path)\n            self.assertTrue(os.path.exists(image_path))\n        finally:\n            shutil.rmtree(dir_path, ignore_errors=True)\n"""
tests/example_tests/test_kmeans.py,0,"b""import os\nimport shutil\nimport tempfile\nimport unittest\n\nfrom cupy import testing\n\nfrom example_tests import example_test\n\n\nos.environ['MPLBACKEND'] = 'Agg'\n\n\n@testing.with_requires('matplotlib')\nclass TestKmeans(unittest.TestCase):\n\n    def test_default(self):\n        output = example_test.run_example(\n            'kmeans/kmeans.py', '-m', '1', '--num', '10')\n        self.assertRegex(\n            output.decode('utf-8'),\n            r' CPU :  [0-9\\.]+ sec\\s+GPU :  [0-9\\.]+ sec')\n\n    def test_custom_kernel(self):\n        output = example_test.run_example(\n            'kmeans/kmeans.py', '-m', '1', '--num', '10',\n            '--use-custom-kernel')\n        self.assertRegex(\n            output.decode('utf-8'),\n            r' CPU :  [0-9\\.]+ sec\\s+GPU :  [0-9\\.]+ sec')\n\n    def test_result_image(self):\n        dir_path = tempfile.mkdtemp()\n        try:\n            image_path = os.path.join(dir_path, 'kmeans.png')\n            example_test.run_example(\n                'kmeans/kmeans.py', '-m', '1', '--num', '10', '-o', image_path)\n            self.assertTrue(os.path.exists(image_path))\n        finally:\n            shutil.rmtree(dir_path, ignore_errors=True)\n"""
tests/install_tests/__init__.py,0,b''
tests/install_tests/test_build.py,0,"b'from distutils import ccompiler\nfrom distutils import sysconfig\nimport unittest\n\nimport pytest\n\nfrom install import build\n\n\nclass TestCheckVersion(unittest.TestCase):\n\n    def setUp(self):\n        self.compiler = ccompiler.new_compiler()\n        sysconfig.customize_compiler(self.compiler)\n        self.settings = build.get_compiler_setting(False)\n\n    @pytest.mark.gpu\n    def test_check_cuda_version(self):\n        with self.assertRaises(RuntimeError):\n            build.get_cuda_version()\n        self.assertTrue(build.check_cuda_version(\n            self.compiler, self.settings))\n        self.assertIsInstance(build.get_cuda_version(), int)\n        self.assertIsInstance(build.get_cuda_version(True), str)\n\n    @pytest.mark.gpu\n    @pytest.mark.cudnn\n    def test_check_cudnn_version(self):\n        with self.assertRaises(RuntimeError):\n            build.get_cudnn_version()\n        self.assertTrue(build.check_cudnn_version(\n            self.compiler, self.settings))\n        self.assertIsInstance(build.get_cudnn_version(), int)\n        self.assertIsInstance(build.get_cudnn_version(True), str)\n'"
tests/install_tests/test_utils.py,0,"b""import unittest\n\nfrom install import utils\n\n\nclass TestPrintWarning(unittest.TestCase):\n\n    def test_print_warning(self):\n        utils.print_warning('This is a test.')\n\n\nclass TestSearchOnPath(unittest.TestCase):\n\n    def test_exec_not_found(self):\n        self.assertIsNone(utils.search_on_path(['no_such_exec']))\n"""
cupy/cuda/memory_hooks/__init__.py,0,b'from cupy.cuda.memory_hooks import debug_print  # NOQA\nfrom cupy.cuda.memory_hooks import line_profile  # NOQA\n\n# import class and function\nfrom cupy.cuda.memory_hooks.debug_print import DebugPrintHook  # NOQA\nfrom cupy.cuda.memory_hooks.line_profile import LineProfileHook  # NOQA\n'
cupy/cuda/memory_hooks/debug_print.py,0,"b'import sys\n\nfrom cupy.cuda import memory_hook\n\n\nclass DebugPrintHook(memory_hook.MemoryHook):\n    """"""Memory hook that prints debug information.\n\n    This memory hook outputs the debug information of input arguments of\n    ``malloc`` and ``free`` methods involved in the hooked functions\n    at postprocessing time (that is, just after each method is called).\n\n    Example:\n        The basic usage is to use it with ``with`` statement.\n\n        Code example::\n\n            >>> import cupy\n            >>> from cupy.cuda import memory_hooks\n            >>>\n            >>> cupy.cuda.set_allocator(cupy.cuda.MemoryPool().malloc)\n            >>> with memory_hooks.DebugPrintHook():\n            ...     x = cupy.array([1, 2, 3])\n            ...     del x  # doctest:+SKIP\n\n        Output example::\n\n            {""hook"":""alloc"",""device_id"":0,""mem_size"":512,""mem_ptr"":150496608256}\n            {""hook"":""malloc"",""device_id"":0,""size"":24,""mem_size"":512,""mem_ptr"":150496608256,""pmem_id"":""0x7f39200c5278""}\n            {""hook"":""free"",""device_id"":0,""mem_size"":512,""mem_ptr"":150496608256,""pmem_id"":""0x7f39200c5278""}\n\n        where the output format is JSONL (JSON Lines) and\n        ``hook`` is the name of hook point, and\n        ``device_id`` is the CUDA Device ID, and\n        ``size`` is the requested memory size to allocate, and\n        ``mem_size`` is the rounded memory size to be allocated, and\n        ``mem_ptr`` is the memory pointer, and\n        ``pmem_id`` is the pooled memory object ID.\n\n    Attributes:\n        file: Output file_like object that redirect to.\n        flush: If ``True``, this hook forcibly flushes the text stream\n            at the end of print. The default is ``True``.\n\n    """"""\n\n    name = \'DebugPrintHook\'\n\n    def __init__(self, file=sys.stdout, flush=True):\n        self.file = file\n        self.flush = flush\n\n    def _print(self, msg):\n        self.file.write(msg)\n        self.file.write(\'\\n\')\n        if self.flush:\n            self.file.flush()\n\n    def alloc_postprocess(self, **kwargs):\n        msg = \'{""hook"":""%s"",""device_id"":%d,\' \\\n              \'""mem_size"":%d,""mem_ptr"":%d}\'\n        msg %= (\'alloc\', kwargs[\'device_id\'],\n                kwargs[\'mem_size\'], kwargs[\'mem_ptr\'])\n        self._print(msg)\n\n    def malloc_postprocess(self, **kwargs):\n        msg = \'{""hook"":""%s"",""device_id"":%d,""size"":%d,\' \\\n              \'""mem_size"":%d,""mem_ptr"":%d,""pmem_id"":""%s""}\'\n        msg %= (\'malloc\', kwargs[\'device_id\'], kwargs[\'size\'],\n                kwargs[\'mem_size\'], kwargs[\'mem_ptr\'], hex(kwargs[\'pmem_id\']))\n        self._print(msg)\n\n    def free_postprocess(self, **kwargs):\n        msg = \'{""hook"":""%s"",""device_id"":%d,\' \\\n              \'""mem_size"":%d,""mem_ptr"":%d,""pmem_id"":""%s""}\'\n        msg %= (\'free\', kwargs[\'device_id\'],\n                kwargs[\'mem_size\'], kwargs[\'mem_ptr\'], hex(kwargs[\'pmem_id\']))\n        self._print(msg)\n'"
cupy/cuda/memory_hooks/line_profile.py,0,"b'from os import path\nimport sys\nimport traceback\n\nfrom cupy.cuda import memory_hook\n\n\nclass LineProfileHook(memory_hook.MemoryHook):\n    """"""Code line CuPy memory profiler.\n\n    This profiler shows line-by-line GPU memory consumption using traceback\n    module. But, note that it can trace only CPython level, no Cython level.\n    ref. https://github.com/cython/cython/issues/1755\n\n    Example:\n        Code example::\n\n            from cupy.cuda import memory_hooks\n            hook = memory_hooks.LineProfileHook()\n            with hook:\n                # some CuPy codes\n            hook.print_report()\n\n        Output example::\n\n            _root (4.00KB, 4.00KB)\n              lib/python3.6/unittest/__main__.py:18:<module> (4.00KB, 4.00KB)\n                lib/python3.6/unittest/main.py:255:runTests (4.00KB, 4.00KB)\n                  tests/cupy_tests/test.py:37:test (1.00KB, 1.00KB)\n                  tests/cupy_tests/test.py:38:test (1.00KB, 1.00KB)\n                  tests/cupy_tests/test.py:39:test (2.00KB, 2.00KB)\n\n        Each line shows::\n\n            {filename}:{lineno}:{func_name} ({used_bytes}, {acquired_bytes})\n\n        where *used_bytes* is the memory bytes used from CuPy memory pool, and\n        *acquired_bytes* is the actual memory bytes the CuPy memory pool\n        acquired from GPU device.\n        *_root* is a root node of the stack trace to show total memory usage.\n\n    Args:\n        max_depth (int): maximum depth to follow stack traces.\n            Default is 0 (no limit).\n    """"""\n\n    name = \'LineProfileHook\'\n\n    def __init__(self, max_depth=0):\n        self._memory_frames = {}\n        self._root = MemoryFrame(None, None)\n        self._filename = path.abspath(__file__)\n        self._max_depth = max_depth\n\n    # callback\n    def malloc_preprocess(self, device_id, size, mem_size):\n        self._cretate_frame_tree(used_bytes=mem_size)\n\n    # callback\n    def alloc_preprocess(self, device_id, mem_size):\n        self._cretate_frame_tree(acquired_bytes=mem_size)\n\n    def _cretate_frame_tree(self, used_bytes=0, acquired_bytes=0):\n        self._root.used_bytes += used_bytes\n        self._root.acquired_bytes += acquired_bytes\n        parent = self._root\n        for depth, stackframe in enumerate(self._extract_stackframes()):\n            if 0 < self._max_depth <= depth + 1:\n                break\n            memory_frame = self._add_frame(parent, stackframe)\n            memory_frame.used_bytes += used_bytes\n            memory_frame.acquired_bytes += acquired_bytes\n            parent = memory_frame\n\n    def _extract_stackframes(self):\n        stackframes = traceback.extract_stack()\n        stackframes = [StackFrame(st) for st in stackframes]\n        stackframes = [\n            st for st in stackframes if st.filename != self._filename]\n        return stackframes\n\n    def _key_frame(self, parent, stackframe):\n        return (parent,\n                stackframe.filename,\n                stackframe.lineno,\n                stackframe.name)\n\n    def _add_frame(self, parent, stackframe):\n        key = self._key_frame(parent, stackframe)\n        if key in self._memory_frames:\n            memory_frame = self._memory_frames[key]\n        else:\n            memory_frame = MemoryFrame(parent, stackframe)\n            self._memory_frames[key] = memory_frame\n        return memory_frame\n\n    def print_report(self, file=sys.stdout):\n        """"""Prints a report of line memory profiling.""""""\n        line = \'_root (%s, %s)\\n\' % self._root.humanized_bytes()\n        file.write(line)\n        for child in self._root.children:\n            self._print_frame(child, depth=1, file=file)\n        file.flush()\n\n    def _print_frame(self, memory_frame, depth=0, file=sys.stdout):\n        indent = \' \' * (depth * 2)\n        st = memory_frame.stackframe\n        used_bytes, acquired_bytes = memory_frame.humanized_bytes()\n        line = \'%s%s:%s:%s (%s, %s)\\n\' % (\n            indent, st.filename, st.lineno, st.name,\n            used_bytes, acquired_bytes)\n        file.write(line)\n        for child in memory_frame.children:\n            self._print_frame(child, depth=depth + 1, file=file)\n\n\nclass StackFrame(object):\n    """"""Compatibility layer for outputs of traceback.extract_stack().\n\n    Attributes:\n        filename (string): filename\n        lineno (int): line number\n        name (string): function name\n    """"""\n\n    def __init__(self, obj):\n        if isinstance(obj, tuple):  # < 3.5\n            self.filename = obj[0]\n            self.lineno = obj[1]\n            self.name = obj[2]\n        else:  # >= 3.5 FrameSummary\n            self.filename = obj.filename\n            self.lineno = obj.lineno\n            self.name = obj.name\n\n\nclass MemoryFrame(object):\n    """"""A single stack frame along with sum of memory usage at the frame.\n\n    Attributes:\n        stackframe (FrameSummary): stackframe from traceback.extract_stack().\n        parent (MemoryFrame): parent frame, that is, caller.\n        children (list of MemoryFrame): child frames, that is, callees.\n        used_bytes (int): memory bytes that users used from CuPy memory pool.\n        acquired_bytes (int): memory bytes that CuPy memory pool acquired\n            from GPU device.\n    """"""\n\n    def __init__(self, parent, stackframe):\n        self.stackframe = stackframe\n        self.children = []\n        self._set_parent(parent)\n        self.used_bytes = 0\n        self.acquired_bytes = 0\n\n    def humanized_bytes(self):\n        used_bytes = self._humanized_size(self.used_bytes)\n        acquired_bytes = self._humanized_size(self.acquired_bytes)\n        return (used_bytes, acquired_bytes)\n\n    def _set_parent(self, parent):\n        if parent and parent not in parent.children:\n            self.parent = parent\n            parent.children.append(self)\n\n    def _humanized_size(self, size):\n        for unit in [\'\', \'K\', \'M\', \'G\', \'T\', \'P\', \'E\']:\n            if size < 1024.0:\n                return \'%3.2f%sB\' % (size, unit)\n            size /= 1024.0\n        return \'%.2f%sB\' % (size, \'Z\')\n'"
cupy/sparse/linalg/__init__.py,0,"b'# Functions from the following SciPy document\n# https://docs.scipy.org/doc/scipy/reference/sparse.linalg.html\n\n# ""NOQA"" to suppress flake8 warning\nfrom cupyx.scipy.sparse.linalg.solve import lsqr  # NOQA\n'"
cupyx/linalg/sparse/__init__.py,0,"b'\n# ""NOQA"" to suppress flake8 warning\nfrom cupyx.linalg.sparse.solve import lschol  # NOQA\n'"
cupyx/linalg/sparse/solve.py,0,"b'import numpy\n\nimport cupy\nfrom cupy.cuda import cusolver\nfrom cupy.cuda import device\nfrom cupy.linalg import util\nimport cupy.sparse\n\n\ndef lschol(A, b):\n    """"""Solves linear system with cholesky decomposition.\n\n    Find the solution to a large, sparse, linear system of equations.\n    The function solves ``Ax = b``. Given two-dimensional matrix ``A`` is\n    decomposed into ``L * L^*``.\n\n    Args:\n        A (cupy.ndarray or cupy.sparse.csr_matrix): The input matrix with\n            dimension ``(N, N)``. Must be positive-definite input matrix.\n            Only symmetric real matrix is supported currently.\n        b (cupy.ndarray): Right-hand side vector.\n\n    Returns:\n        ret (cupy.ndarray): The solution vector ``x``.\n\n    """"""\n\n    if not cupy.sparse.isspmatrix_csr(A):\n        A = cupy.sparse.csr_matrix(A)\n    util._assert_nd_squareness(A)\n    util._assert_cupy_array(b)\n    m = A.shape[0]\n    if b.ndim != 1 or len(b) != m:\n        raise ValueError(\'b must be 1-d array whose size is same as A\')\n\n    # Cast to float32 or float64\n    if A.dtype == \'f\' or A.dtype == \'d\':\n        dtype = A.dtype\n    else:\n        dtype = numpy.promote_types(A.dtype, \'f\')\n\n    handle = device.get_cusolver_sp_handle()\n    nnz = A.nnz\n    tol = 1.0\n    reorder = 1\n    x = cupy.empty(m, dtype=dtype)\n    singularity = numpy.empty(1, numpy.int32)\n\n    if dtype == \'f\':\n        csrlsvchol = cusolver.scsrlsvchol\n    else:\n        csrlsvchol = cusolver.dcsrlsvchol\n    csrlsvchol(\n        handle, m, nnz, A._descr.descriptor, A.data.data.ptr,\n        A.indptr.data.ptr, A.indices.data.ptr, b.data.ptr, tol, reorder,\n        x.data.ptr, singularity.ctypes.data)\n\n    # The return type of SciPy is always float64.\n    x = x.astype(numpy.float64)\n\n    return x\n'"
cupyx/scipy/fft/__init__.py,0,"b'# flake8: NOQA\nfrom cupyx.scipy.fft.fft import *\nfrom cupyx.scipy.fft.fft import (\n    __all__, __ua_domain__, __ua_convert__, __ua_function__)\n'"
cupyx/scipy/fft/fft.py,2,"b'from numbers import Number\nimport warnings\n\nimport numpy as np\n\nimport cupy\nfrom cupy.cuda import cufft\nfrom cupy.fft.fft import (_fft, _default_fft_func, hfft as _hfft,\n                          ihfft as _ihfft, _size_last_transform_axis)\nfrom cupy.fft.fft import fftshift, ifftshift, fftfreq, rfftfreq\n\nfrom cupyx.scipy.fftpack import get_fft_plan\n\n__all__ = [\'fft\', \'ifft\', \'fft2\', \'ifft2\', \'fftn\', \'ifftn\',\n           \'rfft\', \'irfft\', \'rfft2\', \'irfft2\', \'rfftn\', \'irfftn\',\n           \'hfft\', \'ihfft\',\n           \'fftshift\', \'ifftshift\', \'fftfreq\', \'rfftfreq\',\n           \'get_fft_plan\']\n\n_scipy_150 = False\ntry:\n    import scipy\n    import scipy.fft as _scipy_fft\nexcept ImportError:\n    class _DummyModule:\n        def __getattr__(self, name):\n            return None\n\n    _scipy_fft = _DummyModule()\nelse:\n    from numpy.lib import NumpyVersion as Version\n    _scipy_150 = Version(scipy.__version__) >= Version(\'1.5.0\')\n    del Version\n    del scipy\n\n# Backend support for scipy.fft\n\n__ua_domain__ = \'numpy.scipy.fft\'\n_implemented = {}\n\n\ndef __ua_convert__(dispatchables, coerce):\n    if coerce:\n        try:\n            replaced = [\n                cupy.asarray(d.value) if d.coercible and d.type is np.ndarray\n                else d.value for d in dispatchables]\n        except TypeError:\n            return NotImplemented\n    else:\n        replaced = [d.value for d in dispatchables]\n\n    if not all(d.type is not np.ndarray or isinstance(r, cupy.ndarray)\n               for r, d in zip(replaced, dispatchables)):\n        return NotImplemented\n\n    return replaced\n\n\ndef __ua_function__(method, args, kwargs):\n    fn = _implemented.get(method, None)\n    if fn is None:\n        return NotImplemented\n    if \'plan\' in kwargs and not _scipy_150:\n        warnings.warn(\'The \\\'plan\\\' argument is supported in SciPy v1.5.0+\')\n    return fn(*args, **kwargs)\n\n\ndef _implements(scipy_func):\n    """"""Decorator adds function to the dictionary of implemented functions""""""\n    def inner(func):\n        _implemented[scipy_func] = func\n        return func\n\n    return inner\n\n\ndef _assequence(x):\n    """"""Convert scalars to a sequence, otherwise pass through ``x`` unchanged""""""\n    if isinstance(x, Number):\n        return (x,)\n    return x\n\n\n@_implements(_scipy_fft.fft)\ndef fft(x, n=None, axis=-1, norm=None, overwrite_x=False, *, plan=None):\n    """"""Compute the one-dimensional FFT.\n\n    Args:\n        x (cupy.ndarray): Array to be transformed.\n        n (None or int): Length of the transformed axis of the output. If ``n``\n            is not given, the length of the input along the axis specified by\n            ``axis`` is used.\n        axis (int): Axis over which to compute the FFT.\n        norm (None or ``\'ortho\'``): Normalization mode.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.Plan1d` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axis``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, n, axis)\n\n            Note that ``plan`` is defaulted to ``None``, meaning CuPy will use\n            an auto-generated plan behind the scene.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``n`` and type\n            will convert to complex if that of the input is another.\n\n    .. seealso:: :func:`scipy.fft.fft`\n    """"""\n    return _fft(x, (n,), (axis,), norm, cufft.CUFFT_FORWARD,\n                overwrite_x=overwrite_x, plan=plan)\n\n\n@_implements(_scipy_fft.ifft)\ndef ifft(x, n=None, axis=-1, norm=None, overwrite_x=False, *, plan=None):\n    """"""Compute the one-dimensional inverse FFT.\n\n    Args:\n        x (cupy.ndarray): Array to be transformed.\n        n (None or int): Length of the transformed axis of the output. If ``n``\n            is not given, the length of the input along the axis specified by\n            ``axis`` is used.\n        axis (int): Axis over which to compute the FFT.\n        norm (None or ``\'ortho\'``): Normalization mode.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.Plan1d` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axis``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, n, axis)\n\n            Note that ``plan`` is defaulted to ``None``, meaning CuPy will use\n            an auto-generated plan behind the scene.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``n`` and type\n            will convert to complex if that of the input is another.\n\n    .. seealso:: :func:`scipy.fft.ifft`\n    """"""\n    return _fft(x, (n,), (axis,), norm, cufft.CUFFT_INVERSE,\n                overwrite_x=overwrite_x, plan=plan)\n\n\n@_implements(_scipy_fft.fft2)\ndef fft2(x, s=None, axes=(-2, -1), norm=None, overwrite_x=False, *, plan=None):\n    """"""Compute the two-dimensional FFT.\n\n    Args:\n        x (cupy.ndarray): Array to be transformed.\n        s (None or tuple of ints): Shape of the transformed axes of the\n            output. If ``s`` is not given, the lengths of the input along\n            the axes specified by ``axes`` are used.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        norm (None or ``\'ortho\'``): Normalization mode.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.PlanNd` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axes``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, s, axes)\n\n            Note that ``plan`` is defaulted to ``None``, meaning CuPy will use\n            an auto-generated plan behind the scene.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``s`` and\n            type will convert to complex if that of the input is another.\n\n    .. seealso:: :func:`scipy.fft.fft2`\n    """"""\n    return fftn(x, s, axes, norm, overwrite_x, plan=plan)\n\n\n@_implements(_scipy_fft.ifft2)\ndef ifft2(x, s=None, axes=(-2, -1), norm=None, overwrite_x=False, *,\n          plan=None):\n    """"""Compute the two-dimensional inverse FFT.\n\n    Args:\n        x (cupy.ndarray): Array to be transformed.\n        s (None or tuple of ints): Shape of the transformed axes of the\n            output. If ``s`` is not given, the lengths of the input along\n            the axes specified by ``axes`` are used.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        norm (None or ``\'ortho\'``): Normalization mode.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.PlanNd` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axes``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, s, axes)\n\n            Note that ``plan`` is defaulted to ``None``, meaning CuPy will use\n            an auto-generated plan behind the scene.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``s`` and\n            type will convert to complex if that of the input is another.\n\n    .. seealso:: :func:`scipy.fft.ifft2`\n    """"""\n    return ifftn(x, s, axes, norm, overwrite_x, plan=plan)\n\n\n@_implements(_scipy_fft.fftn)\ndef fftn(x, s=None, axes=None, norm=None, overwrite_x=False, *, plan=None):\n    """"""Compute the N-dimensional FFT.\n\n    Args:\n        x (cupy.ndarray): Array to be transformed.\n        s (None or tuple of ints): Shape of the transformed axes of the\n            output. If ``s`` is not given, the lengths of the input along\n            the axes specified by ``axes`` are used.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        norm (None or ``\'ortho\'``): Normalization mode.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.PlanNd` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axes``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, s, axes)\n\n            Note that ``plan`` is defaulted to ``None``, meaning CuPy will use\n            an auto-generated plan behind the scene.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``s`` and\n            type will convert to complex if that of the input is another.\n\n    .. seealso:: :func:`scipy.fft.fftn`\n    """"""\n    s = _assequence(s)\n    axes = _assequence(axes)\n    func = _default_fft_func(x, s, axes)\n    return func(x, s, axes, norm, cufft.CUFFT_FORWARD, overwrite_x=overwrite_x,\n                plan=plan)\n\n\n@_implements(_scipy_fft.ifftn)\ndef ifftn(x, s=None, axes=None, norm=None, overwrite_x=False, *, plan=None):\n    """"""Compute the N-dimensional inverse FFT.\n\n    Args:\n        x (cupy.ndarray): Array to be transformed.\n        s (None or tuple of ints): Shape of the transformed axes of the\n            output. If ``s`` is not given, the lengths of the input along\n            the axes specified by ``axes`` are used.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        norm (None or ``\'ortho\'``): Normalization mode.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.PlanNd` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axes``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, s, axes)\n\n            Note that ``plan`` is defaulted to ``None``, meaning CuPy will use\n            an auto-generated plan behind the scene.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``s`` and\n            type will convert to complex if that of the input is another.\n\n    .. seealso:: :func:`scipy.fft.ifftn`\n    """"""\n    s = _assequence(s)\n    axes = _assequence(axes)\n    func = _default_fft_func(x, s, axes)\n    return func(x, s, axes, norm, cufft.CUFFT_INVERSE, overwrite_x=overwrite_x,\n                plan=plan)\n\n\n@_implements(_scipy_fft.rfft)\ndef rfft(x, n=None, axis=-1, norm=None, overwrite_x=False, *, plan=None):\n    """"""Compute the one-dimensional FFT for real input.\n\n    The returned array contains the positive frequency components of the\n    corresponding :func:`fft`, up to and including the Nyquist frequency.\n\n    Args:\n        x (cupy.ndarray): Array to be transformed.\n        n (None or int): Length of the transformed axis of the output. If ``n``\n            is not given, the length of the input along the axis specified by\n            ``axis`` is used.\n        axis (int): Axis over which to compute the FFT.\n        norm (None or ``\'ortho\'``): Normalization mode.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.Plan1d` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axis``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, n, axis,\n                                                        value_type=\'R2C\')\n\n            Note that ``plan`` is defaulted to ``None``, meaning CuPy will use\n            an auto-generated plan behind the scene.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array.\n\n    .. seealso:: :func:`scipy.fft.rfft`\n\n    """"""\n    return _fft(x, (n,), (axis,), norm, cufft.CUFFT_FORWARD, \'R2C\',\n                overwrite_x=overwrite_x, plan=plan)\n\n\n@_implements(_scipy_fft.irfft)\ndef irfft(x, n=None, axis=-1, norm=None, overwrite_x=False, *, plan=None):\n    """"""Compute the one-dimensional inverse FFT for real input.\n\n    Args:\n        x (cupy.ndarray): Array to be transformed.\n        n (None or int): Length of the transformed axis of the output. If ``n``\n            is not given, the length of the input along the axis specified by\n            ``axis`` is used.\n        axis (int): Axis over which to compute the FFT.\n        norm (None or ``\'ortho\'``): Normalization mode.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.Plan1d` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axis``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, n, axis,\n                                                        value_type=\'C2R\')\n\n            Note that ``plan`` is defaulted to ``None``, meaning CuPy will use\n            an auto-generated plan behind the scene.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array.\n\n    .. seealso:: :func:`scipy.fft.irfft`\n    """"""\n    return _fft(x, (n,), (axis,), norm, cufft.CUFFT_INVERSE, \'C2R\',\n                overwrite_x=overwrite_x, plan=plan)\n\n\n@_implements(_scipy_fft.rfft2)\ndef rfft2(x, s=None, axes=(-2, -1), norm=None, overwrite_x=False, *,\n          plan=None):\n    """"""Compute the two-dimensional FFT for real input.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        s (None or tuple of ints): Shape to use from the input. If ``s`` is not\n            given, the lengths of the input along the axes specified by\n            ``axes`` are used.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.PlanNd` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axes``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, s, axes,\n                                                        value_type=\'R2C\')\n\n            Note that ``plan`` is defaulted to ``None``, meaning CuPy will use\n            an auto-generated plan behind the scene.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``s`` and type\n            will convert to complex if the input is other. The length of the\n            last axis transformed will be ``s[-1]//2+1``.\n\n    .. seealso:: :func:`scipy.fft.rfft2`\n    """"""\n    return rfftn(x, s, axes, norm, overwrite_x, plan=plan)\n\n\n@_implements(_scipy_fft.irfft2)\ndef irfft2(x, s=None, axes=(-2, -1), norm=None, overwrite_x=False, *,\n           plan=None):\n    """"""Compute the two-dimensional inverse FFT for real input.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        s (None or tuple of ints): Shape of the output. If ``s`` is not given,\n            they are determined from the lengths of the input along the axes\n            specified by ``axes``.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.PlanNd` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axes``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, s, axes,\n                                                        value_type=\'C2R\')\n\n            Note that ``plan`` is defaulted to ``None``, meaning CuPy will use\n            an auto-generated plan behind the scene.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``s`` and type\n            will convert to complex if the input is other. If ``s`` is not\n            given, the length of final transformed axis of output will be\n            `2*(m-1)` where `m` is the length of the final transformed axis of\n            the input.\n\n    .. seealso:: :func:`scipy.fft.irfft2`\n    """"""\n    return irfftn(x, s, axes, norm, overwrite_x, plan=plan)\n\n\n@_implements(_scipy_fft.rfftn)\ndef rfftn(x, s=None, axes=None, norm=None, overwrite_x=False, *, plan=None):\n    """"""Compute the N-dimensional FFT for real input.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        s (None or tuple of ints): Shape to use from the input. If ``s`` is not\n            given, the lengths of the input along the axes specified by\n            ``axes`` are used.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.PlanNd` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axes``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, s, axes,\n                                                        value_type=\'R2C\')\n\n            Note that ``plan`` is defaulted to ``None``, meaning CuPy will use\n            an auto-generated plan behind the scene.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``s`` and type\n            will convert to complex if the input is other. The length of the\n            last axis transformed will be ``s[-1]//2+1``.\n\n    .. seealso:: :func:`scipy.fft.rfftn`\n    """"""\n    s = _assequence(s)\n    axes = _assequence(axes)\n    func = _default_fft_func(x, s, axes, value_type=\'R2C\')\n    return func(x, s, axes, norm, cufft.CUFFT_FORWARD, \'R2C\',\n                overwrite_x=overwrite_x, plan=plan)\n\n\n@_implements(_scipy_fft.irfftn)\ndef irfftn(x, s=None, axes=None, norm=None, overwrite_x=False, *, plan=None):\n    """"""Compute the N-dimensional inverse FFT for real input.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        s (None or tuple of ints): Shape of the output. If ``s`` is not given,\n            they are determined from the lengths of the input along the axes\n            specified by ``axes``.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.PlanNd` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axes``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, s, axes,\n                                                        value_type=\'C2R\')\n\n            Note that ``plan`` is defaulted to ``None``, meaning CuPy will use\n            an auto-generated plan behind the scene.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``s`` and type\n            will convert to complex if the input is other. If ``s`` is not\n            given, the length of final transformed axis of output will be\n            ``2*(m-1)`` where `m` is the length of the final transformed axis\n            of the input.\n\n    .. seealso:: :func:`scipy.fft.irfftn`\n    """"""\n    s = _assequence(s)\n    axes = _assequence(axes)\n    if (10020 >= cupy.cuda.runtime.runtimeGetVersion() >= 10010\n            and int(cupy.cuda.device.get_compute_capability()) < 70\n            and _size_last_transform_axis(x.shape, s, axes) == 2):\n        warnings.warn(\'Output of irfftn might not be correct due to issue \'\n                      \'of cuFFT in CUDA 10.1/10.2 on Pascal or older GPUs.\')\n    func = _default_fft_func(x, s, axes, value_type=\'C2R\')\n    return func(x, s, axes, norm, cufft.CUFFT_INVERSE, \'C2R\',\n                overwrite_x=overwrite_x, plan=plan)\n\n\n@_implements(_scipy_fft.hfft)\ndef hfft(x, n=None, axis=-1, norm=None, overwrite_x=False, *, plan=None):\n    """"""Compute the FFT of a signal that has Hermitian symmetry.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        n (None or int): Length of the transformed axis of the output. For\n            ``n`` output points, ``n//2+1`` input points are necessary. If\n            ``n`` is not given, it is determined from the length of the input\n            along the axis specified by ``axis``.\n        axis (int): Axis over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (None): This argument is currently not supported.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``n`` and type\n            will convert to complex if the input is other. If ``n`` is not\n            given, the length of the transformed axis is ``2*(m-1)`` where `m`\n            is the length of the transformed axis of the input.\n\n    .. seealso:: :func:`scipy.fft.hfft`\n    """"""\n    # TODO(leofang): support R2C & C2R plans\n    if plan is not None:\n        raise NotImplementedError(\'hfft plan is currently not yet supported\')\n    return _hfft(x, n, axis, norm)\n\n\n@_implements(_scipy_fft.ihfft)\ndef ihfft(x, n=None, axis=-1, norm=None, overwrite_x=False, *, plan=None):\n    """"""Compute the FFT of a signal that has Hermitian symmetry.\n\n    Args:\n        a (cupy.ndarray): Array to be transform.\n        n (None or int): Number of points along transformation axis in the\n            input to use. If ``n`` is not given, the length of the input along\n            the axis specified by ``axis`` is used.\n        axis (int): Axis over which to compute the FFT.\n        norm (None or ``""ortho""``): Keyword to specify the normalization mode.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (None): This argument is currently not supported.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``n`` and type\n            will convert to complex if the input is other. The length of the\n            transformed axis is ``n//2+1``.\n\n    .. seealso:: :func:`scipy.fft.ihfft`\n    """"""\n    # TODO(leofang): support R2C & C2R plans\n    if plan is not None:\n        raise NotImplementedError(\'ihfft plan is currently not yet supported\')\n    return _ihfft(x, n, axis, norm)\n'"
cupyx/scipy/fftpack/__init__.py,0,b'from cupyx.scipy.fftpack.fft import fft  # NOQA\nfrom cupyx.scipy.fftpack.fft import fft2  # NOQA\nfrom cupyx.scipy.fftpack.fft import fftn  # NOQA\nfrom cupyx.scipy.fftpack.fft import ifft  # NOQA\nfrom cupyx.scipy.fftpack.fft import ifft2  # NOQA\nfrom cupyx.scipy.fftpack.fft import ifftn  # NOQA\nfrom cupyx.scipy.fftpack.fft import irfft  # NOQA\nfrom cupyx.scipy.fftpack.fft import rfft  # NOQA\nfrom cupyx.scipy.fftpack.fft import get_fft_plan  # NOQA\n'
cupyx/scipy/fftpack/fft.py,0,"b'from numpy import prod\n\nimport cupy\nfrom cupy.cuda import cufft\nfrom cupy.fft import config\nfrom cupy.fft.fft import (_convert_fft_type, _default_fft_func, _fft,\n                          _get_cufft_plan_nd, _get_fftn_out_size,\n                          _output_dtype)\n\n\ndef get_fft_plan(a, shape=None, axes=None, value_type=\'C2C\'):\n    """""" Generate a CUDA FFT plan for transforming up to three axes.\n\n    Args:\n        a (cupy.ndarray): Array to be transform, assumed to be either C- or\n            F- contiguous.\n        shape (None or tuple of ints): Shape of the transformed axes of the\n            output. If ``shape`` is not given, the lengths of the input along\n            the axes specified by ``axes`` are used.\n        axes (None or int or tuple of int):  The axes of the array to\n            transform. If `None`, it is assumed that all axes are transformed.\n\n            Currently, for performing N-D transform these must be a set of up\n            to three adjacent axes, and must include either the first or the\n            last axis of the array.\n        value_type (str): The FFT type to perform. Acceptable values are:\n\n            * \'C2C\': complex-to-complex transform (default)\n            * \'R2C\': real-to-complex transform\n            * \'C2R\': complex-to-real transform\n\n    Returns:\n        a cuFFT plan for either 1D transform (``cupy.cuda.cufft.Plan1d``) or\n        N-D transform (``cupy.cuda.cufft.PlanNd``).\n\n    .. note::\n        The returned plan can not only be passed as one of the arguments of\n        the functions in ``cupyx.scipy.fftpack``, but also be used as a\n        context manager for both ``cupy.fft`` and ``cupyx.scipy.fftpack``\n        functions:\n\n        .. code-block:: python\n\n            x = cupy.random.random(16).reshape(4, 4).astype(cupy.complex)\n            plan = cupyx.scipy.fftpack.get_fft_plan(x)\n            with plan:\n                y = cupy.fft.fftn(x)\n                # alternatively:\n                y = cupyx.scipy.fftpack.fftn(x)  # no explicit plan is given!\n            # alternatively:\n            y = cupyx.scipy.fftpack.fftn(x, plan=plan)  # pass plan explicitly\n\n        In the first case, no cuFFT plan will be generated automatically,\n        even if ``cupy.fft.config.enable_nd_planning = True`` is set.\n\n    .. warning::\n        This API is a deviation from SciPy\'s, is currently experimental, and\n        may be changed in the future version.\n    """"""\n    # check input array\n    if a.flags.c_contiguous:\n        order = \'C\'\n    elif a.flags.f_contiguous:\n        order = \'F\'\n    else:\n        raise ValueError(\'Input array a must be contiguous\')\n\n    if isinstance(shape, int):\n        shape = (shape,)\n    if isinstance(axes, int):\n        axes = (axes,)\n    if (shape is not None) and (axes is not None) and len(shape) != len(axes):\n        raise ValueError(\'Shape and axes have different lengths.\')\n\n    # check axes\n    # n=1: 1d (need axis1D); n>1: Nd\n    if axes is None:\n        n = a.ndim if shape is None else len(shape)\n        axes = tuple(i for i in range(-n, 0))\n        if n == 1:\n            axis1D = 0\n    else:  # axes is a tuple\n        n = len(axes)\n        if n == 1:\n            axis1D = axes[0]\n            if axis1D >= a.ndim or axis1D < -a.ndim:\n                err = \'The chosen axis ({0}) exceeds the number of \'\\\n                      \'dimensions of a ({1})\'.format(axis1D, a.ndim)\n                raise ValueError(err)\n        elif n > 3:\n            raise ValueError(\'Only up to three axes is supported\')\n\n    # Note that ""shape"" here refers to the shape along trasformed axes, not\n    # the shape of the output array, and we need to convert it to the latter.\n    # The result is as if ""a=_cook_shape(a); return a.shape"" is called.\n    # Because of this, we need to use (possibly unsorted) axes.\n    transformed_shape = shape\n    shape = list(a.shape)\n    if transformed_shape is not None:\n        for s, axis in zip(transformed_shape, axes):\n            if s is not None:\n                if axis == axes[-1] and value_type == \'C2R\':\n                    s = s // 2 + 1\n                shape[axis] = s\n    shape = tuple(shape)\n\n    # check value_type\n    out_dtype = _output_dtype(a.dtype, value_type)\n    fft_type = _convert_fft_type(out_dtype, value_type)\n    # TODO(leofang): figure out if we really have to skip F-order?\n    if n > 1 and value_type != \'C2C\' and a.flags.f_contiguous:\n        raise ValueError(\'C2R/R2C PlanNd for F-order arrays is not supported\')\n\n    # generate plan\n    if n > 1:  # ND transform\n        out_size = _get_fftn_out_size(\n            shape, transformed_shape, axes[-1], value_type)\n        plan = _get_cufft_plan_nd(\n            shape, fft_type, axes=axes, order=order, out_size=out_size)\n    else:  # 1D transform\n        if value_type != \'C2R\':\n            out_size = shape[axis1D]\n        else:\n            out_size = _get_fftn_out_size(\n                shape, transformed_shape, axis1D, value_type)\n        batch = prod(shape) // shape[axis1D]\n        devices = None if not config.use_multi_gpus else config._devices\n        plan = cufft.Plan1d(out_size, fft_type, batch, devices=devices)\n\n    return plan\n\n\ndef fft(x, n=None, axis=-1, overwrite_x=False, plan=None):\n    """"""Compute the one-dimensional FFT.\n\n    Args:\n        x (cupy.ndarray): Array to be transformed.\n        n (None or int): Length of the transformed axis of the output. If ``n``\n            is not given, the length of the input along the axis specified by\n            ``axis`` is used.\n        axis (int): Axis over which to compute the FFT.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.Plan1d` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axis``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, axis)\n\n            Note that `plan` is defaulted to None, meaning CuPy will use an\n            auto-generated plan behind the scene.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``n`` and type\n            will convert to complex if that of the input is another.\n\n    .. note::\n       The argument `plan` is currently experimental and the interface may be\n       changed in the future version.\n\n    .. seealso:: :func:`scipy.fftpack.fft`\n    """"""\n    return _fft(x, (n,), (axis,), None, cufft.CUFFT_FORWARD,\n                overwrite_x=overwrite_x, plan=plan)\n\n\ndef ifft(x, n=None, axis=-1, overwrite_x=False, plan=None):\n    """"""Compute the one-dimensional inverse FFT.\n\n    Args:\n        x (cupy.ndarray): Array to be transformed.\n        n (None or int): Length of the transformed axis of the output. If ``n``\n            is not given, the length of the input along the axis specified by\n            ``axis`` is used.\n        axis (int): Axis over which to compute the FFT.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.Plan1d` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axis``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, axis)\n\n            Note that `plan` is defaulted to None, meaning CuPy will use an\n            auto-generated plan behind the scene.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``n`` and type\n            will convert to complex if that of the input is another.\n\n    .. note::\n       The argument `plan` is currently experimental and the interface may be\n       changed in the future version.\n\n    .. seealso:: :func:`scipy.fftpack.ifft`\n    """"""\n    return _fft(x, (n,), (axis,), None, cufft.CUFFT_INVERSE,\n                overwrite_x=overwrite_x, plan=plan)\n\n\ndef fft2(x, shape=None, axes=(-2, -1), overwrite_x=False, plan=None):\n    """"""Compute the two-dimensional FFT.\n\n    Args:\n        x (cupy.ndarray): Array to be transformed.\n        shape (None or tuple of ints): Shape of the transformed axes of the\n            output. If ``shape`` is not given, the lengths of the input along\n            the axes specified by ``axes`` are used.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.PlanNd` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axes``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, axes)\n\n            Note that `plan` is defaulted to None, meaning CuPy will either\n            use an auto-generated plan behind the scene if cupy.fft.config.\n            enable_nd_planning = True, or use no cuFFT plan if it is set to\n            False.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``shape`` and\n            type will convert to complex if that of the input is another.\n\n    .. seealso:: :func:`scipy.fftpack.fft2`\n\n    .. note::\n       The argument `plan` is currently experimental and the interface may be\n       changed in the future version.\n    """"""\n    func = _default_fft_func(x, shape, axes, plan)\n    return func(x, shape, axes, None, cufft.CUFFT_FORWARD,\n                overwrite_x=overwrite_x, plan=plan)\n\n\ndef ifft2(x, shape=None, axes=(-2, -1), overwrite_x=False, plan=None):\n    """"""Compute the two-dimensional inverse FFT.\n\n    Args:\n        x (cupy.ndarray): Array to be transformed.\n        shape (None or tuple of ints): Shape of the transformed axes of the\n            output. If ``shape`` is not given, the lengths of the input along\n            the axes specified by ``axes`` are used.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.PlanNd` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axes``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, axes)\n\n            Note that `plan` is defaulted to None, meaning CuPy will either\n            use an auto-generated plan behind the scene if cupy.fft.config.\n            enable_nd_planning = True, or use no cuFFT plan if it is set to\n            False.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``shape`` and\n            type will convert to complex if that of the input is another.\n\n    .. seealso:: :func:`scipy.fftpack.ifft2`\n\n    .. note::\n       The argument `plan` is currently experimental and the interface may be\n       changed in the future version.\n    """"""\n    func = _default_fft_func(x, shape, axes, plan)\n    return func(x, shape, axes, None, cufft.CUFFT_INVERSE,\n                overwrite_x=overwrite_x, plan=plan)\n\n\ndef fftn(x, shape=None, axes=None, overwrite_x=False, plan=None):\n    """"""Compute the N-dimensional FFT.\n\n    Args:\n        x (cupy.ndarray): Array to be transformed.\n        shape (None or tuple of ints): Shape of the transformed axes of the\n            output. If ``shape`` is not given, the lengths of the input along\n            the axes specified by ``axes`` are used.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.PlanNd` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axes``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, axes)\n\n            Note that `plan` is defaulted to None, meaning CuPy will either\n            use an auto-generated plan behind the scene if cupy.fft.config.\n            enable_nd_planning = True, or use no cuFFT plan if it is set to\n            False.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``shape`` and\n            type will convert to complex if that of the input is another.\n\n    .. seealso:: :func:`scipy.fftpack.fftn`\n\n    .. note::\n       The argument `plan` is currently experimental and the interface may be\n       changed in the future version.\n    """"""\n    func = _default_fft_func(x, shape, axes, plan)\n    return func(x, shape, axes, None, cufft.CUFFT_FORWARD,\n                overwrite_x=overwrite_x, plan=plan)\n\n\ndef ifftn(x, shape=None, axes=None, overwrite_x=False, plan=None):\n    """"""Compute the N-dimensional inverse FFT.\n\n    Args:\n        x (cupy.ndarray): Array to be transformed.\n        shape (None or tuple of ints): Shape of the transformed axes of the\n            output. If ``shape`` is not given, the lengths of the input along\n            the axes specified by ``axes`` are used.\n        axes (tuple of ints): Axes over which to compute the FFT.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.PlanNd` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axes``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(x, axes)\n\n            Note that `plan` is defaulted to None, meaning CuPy will either\n            use an auto-generated plan behind the scene if cupy.fft.config.\n            enable_nd_planning = True, or use no cuFFT plan if it is set to\n            False.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array which shape is specified by ``shape`` and\n            type will convert to complex if that of the input is another.\n\n    .. seealso:: :func:`scipy.fftpack.ifftn`\n\n    .. note::\n       The argument `plan` is currently experimental and the interface may be\n       changed in the future version.\n    """"""\n    func = _default_fft_func(x, shape, axes, plan)\n    return func(x, shape, axes, None, cufft.CUFFT_INVERSE,\n                overwrite_x=overwrite_x, plan=plan)\n\n\ndef rfft(x, n=None, axis=-1, overwrite_x=False, plan=None):\n    """"""Compute the one-dimensional FFT for real input.\n\n    The returned real array contains\n\n    .. code-block:: python\n\n        [y(0),Re(y(1)),Im(y(1)),...,Re(y(n/2))]  # if n is even\n        [y(0),Re(y(1)),Im(y(1)),...,Re(y(n/2)),Im(y(n/2))]  # if n is odd\n\n    Args:\n        x (cupy.ndarray): Array to be transformed.\n        n (None or int): Length of the transformed axis of the output. If ``n``\n            is not given, the length of the input along the axis specified by\n            ``axis`` is used.\n        axis (int): Axis over which to compute the FFT.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n        plan (:class:`cupy.cuda.cufft.Plan1d` or ``None``): a cuFFT plan for\n            transforming ``x`` over ``axis``, which can be obtained using::\n\n                plan = cupyx.scipy.fftpack.get_fft_plan(\n                    x, axes, value_type=\'R2C\')\n\n            Note that `plan` is defaulted to None, meaning CuPy will either\n            use an auto-generated plan behind the scene if cupy.fft.config.\n            enable_nd_planning = True, or use no cuFFT plan if it is set to\n            False.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array.\n\n    .. seealso:: :func:`scipy.fftpack.rfft`\n\n    .. note::\n       The argument `plan` is currently experimental and the interface may be\n       changed in the future version.\n    """"""\n    if n is None:\n        n = x.shape[axis]\n\n    shape = list(x.shape)\n    shape[axis] = n\n    f = _fft(x, (n,), (axis,), None, cufft.CUFFT_FORWARD, \'R2C\',\n             overwrite_x=overwrite_x, plan=plan)\n    z = cupy.empty(shape, f.real.dtype)\n\n    slice_z = [slice(None)] * x.ndim\n    slice_f = [slice(None)] * x.ndim\n\n    slice_z[axis] = slice(1)\n    slice_f[axis] = slice(1)\n    z[slice_z] = f[slice_f].real\n\n    slice_z[axis] = slice(1, None, 2)\n    slice_f[axis] = slice(1, None)\n    z[slice_z] = f[slice_f].real\n\n    slice_z[axis] = slice(2, None, 2)\n    slice_f[axis] = slice(1, n - f.shape[axis] + 1)\n    z[slice_z] = f[slice_f].imag\n\n    return z\n\n\ndef irfft(x, n=None, axis=-1, overwrite_x=False):\n    """"""Compute the one-dimensional inverse FFT for real input.\n\n    Args:\n        x (cupy.ndarray): Array to be transformed.\n        n (None or int): Length of the transformed axis of the output. If ``n``\n            is not given, the length of the input along the axis specified by\n            ``axis`` is used.\n        axis (int): Axis over which to compute the FFT.\n        overwrite_x (bool): If True, the contents of ``x`` can be destroyed.\n\n    Returns:\n        cupy.ndarray:\n            The transformed array.\n\n    .. seealso:: :func:`scipy.fftpack.irfft`\n\n    .. note::\n       This function does not support a precomputed `plan`. If you need this\n       capability, please consider using :func:`cupy.fft.irfft` or :func:`\n       cupyx.scipy.fft.irfft`.\n    """"""\n    if n is None:\n        n = x.shape[axis]\n    m = min(n, x.shape[axis])\n\n    shape = list(x.shape)\n    shape[axis] = n // 2 + 1\n    if x.dtype in (cupy.float16, cupy.float32):\n        z = cupy.zeros(shape, dtype=cupy.complex64)\n    else:\n        z = cupy.zeros(shape, dtype=cupy.complex128)\n\n    slice_x = [slice(None)] * x.ndim\n    slice_z = [slice(None)] * x.ndim\n\n    slice_x[axis] = slice(1)\n    slice_z[axis] = slice(1)\n    z[slice_z].real = x[slice_x]\n\n    slice_x[axis] = slice(1, m, 2)\n    slice_z[axis] = slice(1, m // 2 + 1)\n    z[slice_z].real = x[slice_x]\n\n    slice_x[axis] = slice(2, m, 2)\n    slice_z[axis] = slice(1, (m + 1) // 2)\n    z[slice_z].imag = x[slice_x]\n\n    return _fft(z, (n,), (axis,), None, cufft.CUFFT_INVERSE, \'C2R\',\n                overwrite_x=overwrite_x)\n'"
cupyx/scipy/linalg/__init__.py,0,"b'from cupyx.scipy.linalg.solve_triangular import solve_triangular  # NOQA\nfrom cupyx.scipy.linalg.decomp_lu import lu_factor, lu_solve  # NOQA\n'"
cupyx/scipy/linalg/decomp_lu.py,2,"b'from warnings import warn\n\nimport numpy\n\nimport cupy\nfrom cupy.cuda import cublas\nfrom cupy.cuda import cusolver\nfrom cupy.cuda import device\nfrom cupy.linalg import util\n\n\ndef lu_factor(a, overwrite_a=False, check_finite=True):\n    """"""LU decomposition.\n\n    Decompose a given two-dimensional square matrix into ``P * L * U``,\n    where ``P`` is a permutation matrix,  ``L`` lower-triangular with\n    unit diagonal elements, and ``U`` upper-triangular matrix.\n    Note that in the current implementation ``a`` must be\n    a real matrix, and only :class:`numpy.float32` and :class:`numpy.float64`\n    are supported.\n\n    Args:\n        a (cupy.ndarray): The input matrix with dimension ``(M, N)``\n        overwrite_a (bool): Allow overwriting data in ``a`` (may enhance\n            performance)\n        check_finite (bool): Whether to check that the input matrices contain\n            only finite numbers. Disabling may give a performance gain, but may\n            result in problems (crashes, non-termination) if the inputs do\n            contain infinities or NaNs.\n\n    Returns:\n        tuple:\n            ``(lu, piv)`` where ``lu`` is a :class:`cupy.ndarray`\n            storing ``U`` in its upper triangle, and ``L`` without\n            unit diagonal elements in its lower triangle, and ``piv`` is\n            a :class:`cupy.ndarray` storing pivot indices representing\n            permutation matrix ``P``. For ``0 <= i < min(M,N)``, row\n            ``i`` of the matrix was interchanged with row ``piv[i]``\n\n    .. seealso:: :func:`scipy.linalg.lu_factor`\n\n    .. note::\n\n        Current implementation returns result different from SciPy when the\n        matrix singular. SciPy returns an array containing ``0.`` while the\n        current implementation returns an array containing ``nan``.\n\n        >>> import numpy as np\n        >>> import scipy.linalg\n        >>> scipy.linalg.lu_factor(np.array([[0, 1], [0, 0]], \\\ndtype=np.float32))\n        (array([[0., 1.],\n               [0., 0.]], dtype=float32), array([0, 1], dtype=int32))\n\n        >>> import cupy as cp\n        >>> import cupyx.scipy.linalg\n        >>> cupyx.scipy.linalg.lu_factor(cp.array([[0, 1], [0, 0]], \\\ndtype=cp.float32))\n        (array([[ 0.,  1.],\n               [nan, nan]], dtype=float32), array([0, 1], dtype=int32))\n    """"""\n\n    a = cupy.asarray(a)\n    util._assert_rank2(a)\n\n    dtype = a.dtype\n\n    if dtype.char == \'f\':\n        getrf = cusolver.sgetrf\n        getrf_bufferSize = cusolver.sgetrf_bufferSize\n    elif dtype.char == \'d\':\n        getrf = cusolver.dgetrf\n        getrf_bufferSize = cusolver.dgetrf_bufferSize\n    else:\n        raise NotImplementedError(\'Only float32 and float64 are supported.\')\n\n    a = a.astype(dtype, order=\'F\', copy=(not overwrite_a))\n\n    if check_finite:\n        if a.dtype.kind == \'f\' and not cupy.isfinite(a).all():\n            raise ValueError(\n                \'array must not contain infs or NaNs\')\n\n    cusolver_handle = device.get_cusolver_handle()\n    dev_info = cupy.empty(1, dtype=numpy.int32)\n\n    m, n = a.shape\n\n    ipiv = cupy.empty((min(m, n),), dtype=numpy.intc)\n\n    buffersize = getrf_bufferSize(cusolver_handle, m, n, a.data.ptr, m)\n    workspace = cupy.empty(buffersize, dtype=dtype)\n\n    # LU factorization\n    getrf(cusolver_handle, m, n, a.data.ptr, m, workspace.data.ptr,\n          ipiv.data.ptr, dev_info.data.ptr)\n\n    if dev_info[0] < 0:\n        raise ValueError(\'illegal value in %d-th argument of \'\n                         \'internal getrf (lu_factor)\' % -dev_info[0])\n    elif dev_info[0] > 0:\n        warn(\'Diagonal number %d is exactly zero. Singular matrix.\'\n             % dev_info[0], RuntimeWarning, stacklevel=2)\n\n    # cuSolver uses 1-origin while SciPy uses 0-origin\n    ipiv -= 1\n\n    return (a, ipiv)\n\n\ndef lu_solve(lu_and_piv, b, trans=0, overwrite_b=False, check_finite=True):\n    """"""Solve an equation system, ``a * x = b``, given the LU factorization of ``a``\n\n    Args:\n        lu_and_piv (tuple): LU factorization of matrix ``a`` (``(M, M)``)\n            together with pivot indices.\n        b (cupy.ndarray): The matrix with dimension ``(M,)`` or\n            ``(M, N)``.\n        trans ({0, 1, 2}): Type of system to solve:\n\n            ========  =========\n            trans     system\n            ========  =========\n            0         a x  = b\n            1         a^T x = b\n            2         a^H x = b\n            ========  =========\n        overwrite_b (bool): Allow overwriting data in b (may enhance\n            performance)\n        check_finite (bool): Whether to check that the input matrices contain\n            only finite numbers. Disabling may give a performance gain, but may\n            result in problems (crashes, non-termination) if the inputs do\n            contain infinities or NaNs.\n\n    Returns:\n        cupy.ndarray:\n            The matrix with dimension ``(M,)`` or ``(M, N)``.\n\n    .. seealso:: :func:`scipy.linalg.lu_solve`\n    """"""\n\n    (lu, ipiv) = lu_and_piv\n\n    util._assert_cupy_array(lu)\n    util._assert_rank2(lu)\n    util._assert_nd_squareness(lu)\n\n    m = lu.shape[0]\n    if m != b.shape[0]:\n        raise ValueError(\'incompatible dimensions.\')\n\n    dtype = lu.dtype\n    if dtype.char == \'f\':\n        getrs = cusolver.sgetrs\n    elif dtype.char == \'d\':\n        getrs = cusolver.dgetrs\n    else:\n        raise NotImplementedError(\'Only float32 and float64 are supported.\')\n\n    if trans == 0:\n        trans = cublas.CUBLAS_OP_N\n    elif trans == 1:\n        trans = cublas.CUBLAS_OP_T\n    elif trans == 2:\n        trans = cublas.CUBLAS_OP_C\n    else:\n        raise ValueError(\'unknown trans\')\n\n    lu = lu.astype(dtype, order=\'F\', copy=False)\n    ipiv = ipiv.astype(ipiv.dtype, order=\'F\', copy=True)\n    # cuSolver uses 1-origin while SciPy uses 0-origin\n    ipiv += 1\n    b = b.astype(dtype, order=\'F\', copy=(not overwrite_b))\n\n    if check_finite:\n        if lu.dtype.kind == \'f\' and not cupy.isfinite(lu).all():\n            raise ValueError(\n                \'array must not contain infs or NaNs.\\n\'\n                \'Note that when a singular matrix is given, unlike \'\n                \'scipy.linalg.lu_factor, cupyx.scipy.linalg.lu_factor \'\n                \'returns an array containing NaN.\')\n        if b.dtype.kind == \'f\' and not cupy.isfinite(b).all():\n            raise ValueError(\n                \'array must not contain infs or NaNs\')\n\n    n = 1 if b.ndim == 1 else b.shape[1]\n    cusolver_handle = device.get_cusolver_handle()\n    dev_info = cupy.empty(1, dtype=numpy.int32)\n\n    # solve for the inverse\n    getrs(cusolver_handle,\n          trans,\n          m, n, lu.data.ptr, m, ipiv.data.ptr, b.data.ptr,\n          m, dev_info.data.ptr)\n\n    if dev_info[0] < 0:\n        raise ValueError(\'illegal value in %d-th argument of \'\n                         \'internal getrs (lu_solve)\' % -dev_info[0])\n\n    return b\n'"
cupyx/scipy/linalg/solve_triangular.py,0,"b'import numpy\n\nimport cupy\nfrom cupy.cuda import cublas\nfrom cupy.cuda import device\nfrom cupy.linalg import util\n\n\ndef solve_triangular(a, b, trans=0, lower=False, unit_diagonal=False,\n                     overwrite_b=False, check_finite=False):\n    """"""Solve the equation a x = b for x, assuming a is a triangular matrix.\n\n    Args:\n        a (cupy.ndarray): The matrix with dimension ``(M, M)``.\n        b (cupy.ndarray): The matrix with dimension ``(M,)`` or\n            ``(M, N)``.\n        lower (bool): Use only data contained in the lower triangle of `a`.\n            Default is to use upper triangle.\n        trans ({0, 1, 2, \'N\', \'T\', \'C\'}): Type of system to solve:\n            ========  =========\n            trans     system\n            ========  =========\n            0 or \'N\'  a x  = b\n            1 or \'T\'  a^T x = b\n            2 or \'C\'  a^H x = b\n            ========  =========\n        unit_diagonal (bool): If True, diagonal elements of `a` are assumed to\n            be 1 and will not be referenced.\n        overwrite_b (bool): Allow overwriting data in b (may enhance\n            performance)\n        check_finite (bool): Whether to check that the input matrices contain\n            only finite numbers. Disabling may give a performance gain, but may\n            result in problems (crashes, non-termination) if the inputs do\n            contain infinities or NaNs.\n\n    Returns:\n        cupy.ndarray:\n            The matrix with dimension ``(M,)`` or ``(M, N)``.\n\n    .. seealso:: :func:`scipy.linalg.solve_triangular`\n    """"""\n\n    util._assert_cupy_array(a, b)\n\n    if len(a.shape) != 2 or a.shape[0] != a.shape[1]:\n        raise ValueError(\'expected square matrix\')\n    if len(a) != len(b):\n        raise ValueError(\'incompatible dimensions\')\n\n    # Cast to float32 or float64\n    if a.dtype.char in \'fd\':\n        dtype = a.dtype\n    else:\n        dtype = numpy.promote_types(a.dtype.char, \'f\')\n\n    a = cupy.array(a, dtype=dtype, order=\'F\', copy=False)\n    b = cupy.array(b, dtype=dtype, order=\'F\', copy=(not overwrite_b))\n\n    if check_finite:\n        if a.dtype.kind == \'f\' and not cupy.isfinite(a).all():\n            raise ValueError(\n                \'array must not contain infs or NaNs\')\n        if b.dtype.kind == \'f\' and not cupy.isfinite(b).all():\n            raise ValueError(\n                \'array must not contain infs or NaNs\')\n\n    m, n = (b.size, 1) if b.ndim == 1 else b.shape\n    cublas_handle = device.get_cublas_handle()\n\n    if dtype == \'f\':\n        trsm = cublas.strsm\n    else:  # dtype == \'d\'\n        trsm = cublas.dtrsm\n\n    if lower:\n        uplo = cublas.CUBLAS_FILL_MODE_LOWER\n    else:\n        uplo = cublas.CUBLAS_FILL_MODE_UPPER\n\n    if trans == \'N\':\n        trans = cublas.CUBLAS_OP_N\n    elif trans == \'T\':\n        trans = cublas.CUBLAS_OP_T\n    elif trans == \'C\':\n        trans = cublas.CUBLAS_OP_C\n\n    if unit_diagonal:\n        diag = cublas.CUBLAS_DIAG_UNIT\n    else:\n        diag = cublas.CUBLAS_DIAG_NON_UNIT\n\n    trsm(\n        cublas_handle, cublas.CUBLAS_SIDE_LEFT, uplo,\n        trans, diag,\n        m, n, 1, a.data.ptr, m, b.data.ptr, m)\n    return b\n'"
cupyx/scipy/ndimage/__init__.py,0,b'from cupyx.scipy.ndimage.filters import correlate  # NOQA\nfrom cupyx.scipy.ndimage.filters import convolve  # NOQA\nfrom cupyx.scipy.ndimage.filters import minimum_filter  # NOQA\nfrom cupyx.scipy.ndimage.filters import maximum_filter  # NOQA\n\nfrom cupyx.scipy.ndimage.interpolation import affine_transform  # NOQA\nfrom cupyx.scipy.ndimage.interpolation import map_coordinates  # NOQA\nfrom cupyx.scipy.ndimage.interpolation import rotate  # NOQA\nfrom cupyx.scipy.ndimage.interpolation import shift  # NOQA\nfrom cupyx.scipy.ndimage.interpolation import zoom  # NOQA\n\nfrom cupyx.scipy.ndimage.measurements import label  # NOQA\nfrom cupyx.scipy.ndimage.measurements import sum  # NOQA\nfrom cupyx.scipy.ndimage.measurements import mean  # NOQA\nfrom cupyx.scipy.ndimage.measurements import variance  # NOQA\nfrom cupyx.scipy.ndimage.measurements import standard_deviation  # NOQA\n\nfrom cupyx.scipy.ndimage.morphology import grey_erosion  # NOQA\nfrom cupyx.scipy.ndimage.morphology import grey_dilation  # NOQA\nfrom cupyx.scipy.ndimage.morphology import grey_closing  # NOQA\nfrom cupyx.scipy.ndimage.morphology import grey_opening  # NOQA\n'
cupyx/scipy/ndimage/_interp_kernels.py,0,"b'import cupy\nimport cupy.core.internal\n\nfrom cupyx.scipy.ndimage import filters\n\n\ndef _get_coord_map(ndim):\n    """"""Extract target coordinate from coords array (for map_coordinates).\n\n    Notes\n    -----\n    Assumes the following variables have been initialized on the device::\n\n        coords (ndarray): array of shape (ncoords, ndim) containing the target\n            coordinates.\n        c_j: variables to hold the target coordinates\n\n    computes::\n\n        c_j = coords[i + j * ncoords];\n\n    ncoords is determined by the size of the output array, y.\n    y will be indexed by the CIndexer, _ind.\n    Thus ncoords = _ind.size();\n\n    """"""\n    ops = []\n    ops.append(\'ptrdiff_t ncoords = _ind.size();\')\n    for j in range(ndim):\n        ops.append(\n            """"""\n    W c_{j} = coords[i + {j} * ncoords];"""""".format(j=j))\n    return ops\n\n\ndef _get_coord_zoom_and_shift(ndim):\n    """"""Compute target coordinate based on a shift followed by a zoom.\n\n    Notes\n    -----\n    Assumes the following variables have been initialized on the device::\n\n        in_coord[ndim]: array containing the source coordinate\n        zoom[ndim]: array containing the zoom for each axis\n        shift[ndim]: array containing the zoom for each axis\n\n    computes::\n\n        c_j = zoom[j] * (in_coord[j] - shift[j])\n\n    """"""\n    ops = []\n    for j in range(ndim):\n        ops.append(\n            """"""\n    W c_{j} = zoom[{j}] * ((W)in_coord[{j}] - shift[{j}]);"""""".format(j=j))\n    return ops\n\n\ndef _get_coord_zoom(ndim):\n    """"""Compute target coordinate based on a zoom.\n\n    Notes\n    -----\n    Assumes the following variables have been initialized on the device::\n\n        in_coord[ndim]: array containing the source coordinate\n        zoom[ndim]: array containing the zoom for each axis\n\n    computes::\n\n        c_j = zoom[j] * in_coord[j]\n\n    """"""\n    ops = []\n    for j in range(ndim):\n        ops.append(\n            """"""\n    W c_{j} = zoom[{j}] * (W)in_coord[{j}];"""""".format(j=j))\n    return ops\n\n\ndef _get_coord_shift(ndim):\n    """"""Compute target coordinate based on a shift.\n\n    Notes\n    -----\n    Assumes the following variables have been initialized on the device::\n\n        in_coord[ndim]: array containing the source coordinate\n        shift[ndim]: array containing the zoom for each axis\n\n    computes::\n\n        c_j = in_coord[j] - shift[j]\n\n    """"""\n    ops = []\n    for j in range(ndim):\n        ops.append(\n            """"""\n    W c_{j} = (W)in_coord[{j}] - shift[{j}];"""""".format(j=j))\n    return ops\n\n\ndef _get_coord_affine(ndim):\n    """"""Compute target coordinate based on a homogeneous transformation matrix.\n\n    The homogeneous matrix has shape (ndim, ndim + 1). It corresponds to\n    affine matrix where the last row of the affine is assumed to be:\n    ``[0] * ndim + [1]``.\n\n    Notes\n    -----\n    Assumes the following variables have been initialized on the device::\n\n        mat(array): array containing the (ndim, ndim + 1) transform matrix.\n        in_coords(array): coordinates of the input\n\n    For example, in 2D:\n\n        c_0 = mat[0] * in_coords[0] + mat[1] * in_coords[1] + aff[2];\n        c_1 = mat[3] * in_coords[0] + mat[4] * in_coords[1] + aff[5];\n\n    """"""\n    ops = []\n    ncol = ndim + 1\n    for j in range(ndim):\n        ops.append(""""""\n            W c_{j} = (W)0.0;\n            """""".format(j=j))\n        for k in range(ndim):\n            m_index = ncol * j + k\n            ops.append(\n                """"""\n            c_{j} += mat[{m_index}] * (W)in_coord[{k}];"""""".format(\n                    j=j, k=k, m_index=m_index))\n        ops.append(\n            """"""\n            c_{j} += mat[{m_index}];"""""".format(\n                j=j, m_index=ncol * j + ndim))\n    return ops\n\n\ndef _unravel_loop_index(shape, uint_t=\'unsigned int\'):\n    """"""\n    declare a multi-index array in_coord and unravel the 1D index, i into it.\n    This code assumes that the array is a C-ordered array.\n    """"""\n    ndim = len(shape)\n    code = [\n        """"""\n        {uint_t} in_coord[{ndim}];\n        {uint_t} s, t, idx = i;"""""".format(uint_t=uint_t, ndim=ndim)]\n    for j in range(ndim - 1, 0, -1):\n        code.append(""""""\n        s = {size};\n        t = idx / s;\n        in_coord[{j}] = idx - t * s;\n        idx = t;"""""".format(j=j, size=shape[j]))\n    code.append(""""""\n        in_coord[0] = idx;"""""")\n    return \'\\n\'.join(code)\n\n\ndef _generate_interp_custom(coord_func, ndim, large_int, yshape, mode, cval,\n                            order, name=\'\', integer_output=False):\n    """"""\n    Args:\n        coord_func (function): generates code to do the coordinate\n            transformation. See for example, `_get_coord_shift`.\n        ndim (int): The number of dimensions.\n        large_int (bool): If true use Py_ssize_t instead of int for indexing.\n        yshape (tuple): Shape of the output array.\n        mode (str): Signal extension mode to use at the array boundaries\n        cval (float): constant value used when `mode == \'constant\'`.\n        name (str): base name for the interpolation kernel\n        integer_output (bool): boolean indicating whether the output has an\n            integer type.\n\n    Returns:\n        operation (str): code body for the ElementwiseKernel\n        name (str): name for the ElementwiseKernel\n    """"""\n\n    ops = []\n    ops.append(\'double out = 0.0;\')\n\n    if large_int:\n        uint_t = \'size_t\'\n        int_t = \'ptrdiff_t\'\n    else:\n        uint_t = \'unsigned int\'\n        int_t = \'int\'\n\n    # determine strides for x along each axis\n    for j in range(ndim):\n        ops.append(\n            \'const {int_t} xsize_{j} = x.shape()[{j}];\'.format(\n                int_t=int_t, j=j)\n        )\n    ops.append(\'const {uint_t} sx_{j} = 1;\'.format(uint_t=uint_t, j=ndim - 1))\n    for j in range(ndim - 1, 0, -1):\n        ops.append(\n            \'const {uint_t} sx_{jm} = sx_{j} * xsize_{j};\'.format(\n                uint_t=uint_t, jm=j - 1, j=j,\n            )\n        )\n\n    # create in_coords array to store the unraveled indices\n    ops.append(_unravel_loop_index(yshape, uint_t))\n\n    # compute the transformed (target) coordinates, c_j\n    ops = ops + coord_func(ndim)\n\n    if mode == \'constant\':\n        # use cval if coordinate is outside the bounds of x\n        _cond = \' || \'.join(\n            [\'(c_{j} < 0) || (c_{j} > xsize_{j} - 1)\'.format(j=j)\n             for j in range(ndim)])\n        ops.append(""""""\n        if ({cond})\n        {{\n            out = (double){cval};\n        }}\n        else\n        {{"""""".format(cond=_cond, cval=cval))\n\n    if order == 0:\n        for j in range(ndim):\n            # determine nearest neighbor\n            ops.append(""""""\n            {int_t} cf_{j} = ({int_t})lrint((double)c_{j});\n            """""".format(int_t=int_t, j=j))\n\n            # handle boundary\n            if mode != \'constant\':\n                ixvar = \'cf_{j}\'.format(j=j)\n                ops.append(\n                    filters._generate_boundary_condition_ops(\n                        mode, ixvar, \'xsize_{}\'.format(j)))\n\n            # sum over ic_j will give the raveled coordinate in the input\n            ops.append(""""""\n            {int_t} ic_{j} = cf_{j} * sx_{j};\n            """""".format(int_t=int_t, j=j))\n        _coord_idx = \' + \'.join([\'ic_{}\'.format(j) for j in range(ndim)])\n        ops.append(""""""\n            out = x[{coord_idx}];"""""".format(coord_idx=_coord_idx))\n\n    elif order == 1:\n        for j in range(ndim):\n            # get coordinates for linear interpolation along axis j\n            ops.append(""""""\n            {int_t} cf_{j} = ({int_t})floor((double)c_{j});\n            {int_t} cc_{j} = cf_{j} + 1;\n            {int_t} n_{j} = (c_{j} == cf_{j}) ? 1 : 2;  // points needed\n            """""".format(int_t=int_t, j=j))\n\n            # handle boundaries for extension modes.\n            ops.append(""""""\n            {int_t} cf_bounded_{j} = cf_{j};\n            {int_t} cc_bounded_{j} = cc_{j};\n            """""".format(int_t=int_t, j=j))\n            if mode != \'constant\':\n                ixvar = \'cf_bounded_{j}\'.format(j=j)\n                ops.append(\n                    filters._generate_boundary_condition_ops(\n                        mode, ixvar, \'xsize_{}\'.format(j)))\n                ixvar = \'cc_bounded_{j}\'.format(j=j)\n                ops.append(\n                    filters._generate_boundary_condition_ops(\n                        mode, ixvar, \'xsize_{}\'.format(j)))\n\n            ops.append(""""""\n            for (int s_{j} = 0; s_{j} < n_{j}; s_{j}++)\n                {{\n                    W w_{j};\n                    {int_t} ic_{j};\n                    if (s_{j} == 0)\n                    {{\n                        w_{j} = (W)cc_{j} - c_{j};\n                        ic_{j} = cf_bounded_{j} * sx_{j};\n                    }} else\n                    {{\n                        w_{j} = c_{j} - (W)cf_{j};\n                        ic_{j} = cc_bounded_{j} * sx_{j};\n                    }}"""""".format(int_t=int_t, j=j))\n\n        _weight = \' * \'.join([\'w_{j}\'.format(j=j) for j in range(ndim)])\n        _coord_idx = \' + \'.join([\'ic_{j}\'.format(j=j) for j in range(ndim)])\n        ops.append(""""""\n        X val = x[{coord_idx}];\n        out += val * ({weight});"""""".format(\n            coord_idx=_coord_idx, weight=_weight))\n        ops.append(\'}\' * ndim)\n\n    if mode == \'constant\':\n        ops.append(\'}\')\n\n    if integer_output:\n        ops.append(\'y = (Y)rint((double)out);\')\n    else:\n        ops.append(\'y = (Y)out;\')\n    operation = \'\\n\'.join(ops)\n\n    name = \'interpolate_{}_order{}_{}_{}d_y{}\'.format(\n        name, order, mode, ndim, ""_"".join([""{}"".format(j) for j in yshape]),\n    )\n    if uint_t == \'size_t\':\n        name += \'_i64\'\n    return operation, name\n\n\n@cupy.util.memoize(for_each_device=True)\ndef _get_map_kernel(ndim, large_int, yshape, mode, cval=0.0, order=1,\n                    integer_output=False):\n    in_params = \'raw X x, raw W coords\'\n    out_params = \'Y y\'\n    operation, name = _generate_interp_custom(\n        coord_func=_get_coord_map,\n        ndim=ndim,\n        large_int=large_int,\n        yshape=yshape,\n        mode=mode,\n        cval=cval,\n        order=order,\n        name=\'shift\',\n        integer_output=integer_output,\n    )\n    return cupy.ElementwiseKernel(in_params, out_params, operation, name)\n\n\n@cupy.util.memoize(for_each_device=True)\ndef _get_shift_kernel(ndim, large_int, yshape, mode, cval=0.0, order=1,\n                      integer_output=False):\n    in_params = \'raw X x, raw W shift\'\n    out_params = \'Y y\'\n    operation, name = _generate_interp_custom(\n        coord_func=_get_coord_shift,\n        ndim=ndim,\n        large_int=large_int,\n        yshape=yshape,\n        mode=mode,\n        cval=cval,\n        order=order,\n        name=\'shift\',\n        integer_output=integer_output,\n    )\n    return cupy.ElementwiseKernel(in_params, out_params, operation, name)\n\n\n@cupy.util.memoize(for_each_device=True)\ndef _get_zoom_shift_kernel(ndim, large_int, yshape, mode, cval=0.0, order=1,\n                           integer_output=False):\n    in_params = \'raw X x, raw W shift, raw W zoom\'\n    out_params = \'Y y\'\n    operation, name = _generate_interp_custom(\n        coord_func=_get_coord_zoom_and_shift,\n        ndim=ndim,\n        large_int=large_int,\n        yshape=yshape,\n        mode=mode,\n        cval=cval,\n        order=order,\n        name=\'zoom_shift\',\n        integer_output=integer_output,\n    )\n    return cupy.ElementwiseKernel(in_params, out_params, operation, name)\n\n\n@cupy.util.memoize(for_each_device=True)\ndef _get_zoom_kernel(ndim, large_int, yshape, mode, cval=0.0, order=1,\n                     integer_output=False):\n    in_params = \'raw X x, raw W zoom\'\n    out_params = \'Y y\'\n    operation, name = _generate_interp_custom(\n        coord_func=_get_coord_zoom,\n        ndim=ndim,\n        large_int=large_int,\n        yshape=yshape,\n        mode=mode,\n        cval=cval,\n        order=order,\n        name=\'zoom\',\n        integer_output=integer_output,\n    )\n    return cupy.ElementwiseKernel(in_params, out_params, operation, name)\n\n\n@cupy.util.memoize(for_each_device=True)\ndef _get_affine_kernel(ndim, large_int, yshape, mode, cval=0.0, order=1,\n                       integer_output=False):\n    in_params = \'raw X x, raw W mat\'\n    out_params = \'Y y\'\n    operation, name = _generate_interp_custom(\n        coord_func=_get_coord_affine,\n        ndim=ndim,\n        large_int=large_int,\n        yshape=yshape,\n        mode=mode,\n        cval=cval,\n        order=order,\n        name=\'affine\',\n        integer_output=integer_output,\n    )\n    return cupy.ElementwiseKernel(in_params, out_params, operation, name)\n'"
cupyx/scipy/ndimage/filters.py,0,"b'import cupy\nfrom cupy import util\n\nimport warnings\n\n\ndef correlate(input, weights, output=None, mode=\'reflect\', cval=0.0, origin=0):\n    """"""Multi-dimensional correlate.\n\n    The array is correlated with the given kernel.\n\n    Args:\n        input (cupy.ndarray): The input array.\n        weights (cupy.ndarray): Array of weights, same number of dimensions as\n            input\n        output (cupy.ndarray, dtype or None): The array in which to place the\n            output.\n        mode (str): The array borders are handled according to the given mode\n            (``\'reflect\'``, ``\'constant\'``, ``\'nearest\'``, ``\'mirror\'``,\n            ``\'wrap\'``). Default is ``\'reflect\'``.\n        cval (scalar): Value to fill past edges of input if mode is\n            ``constant``. Default is ``0.0``.\n        origin (scalar or tuple of scalar): The origin parameter controls the\n            placement of the filter, relative to the center of the current\n            element of the input. Default of 0 is equivalent to\n            ``(0,)*input.ndim``.\n\n    Returns:\n        cupy.ndarray: The result of correlate.\n\n    .. seealso:: :func:`scipy.ndimage.correlate`\n    """"""\n    return _correlate_or_convolve(input, weights, output, mode, cval, origin,\n                                  False)\n\n\ndef convolve(input, weights, output=None, mode=\'reflect\', cval=0.0, origin=0):\n    """"""Multi-dimensional convolution.\n\n    The array is convolved with the given kernel.\n\n    Args:\n        input (cupy.ndarray): The input array.\n        weights (cupy.ndarray): Array of weights, same number of dimensions as\n            input\n        output (cupy.ndarray, dtype or None): The array in which to place the\n            output.\n        mode (str): The array borders are handled according to the given mode\n            (``\'reflect\'``, ``\'constant\'``, ``\'nearest\'``, ``\'mirror\'``,\n            ``\'wrap\'``). Default is ``\'reflect\'``.\n        cval (scalar): Value to fill past edges of input if mode is\n            ``constant``. Default is ``0.0``.\n        origin (scalar or tuple of scalar): The origin parameter controls the\n            placement of the filter, relative to the center of the current\n            element of the input. Default of 0 is equivalent to\n            ``(0,)*input.ndim``.\n\n    Returns:\n        cupy.ndarray: The result of convolution.\n\n    .. seealso:: :func:`scipy.ndimage.convolve`\n    """"""\n    return _correlate_or_convolve(input, weights, output, mode, cval, origin,\n                                  True)\n\n\ndef _get_output(output, input, shape=None):\n    if shape is None:\n        shape = input.shape\n    if isinstance(output, cupy.ndarray):\n        if output.shape != tuple(shape):\n            raise ValueError(\'output shape is not correct\')\n    else:\n        dtype = output\n        if dtype is None:\n            dtype = input.dtype\n        output = cupy.zeros(shape, dtype)\n    return output\n\n\ndef _correlate_or_convolve(input, weights, output, mode, cval, origin,\n                           convolution):\n    if input.dtype.kind == \'c\':\n        raise TypeError(\'Complex type not supported.\')\n    if not hasattr(origin, \'__getitem__\'):\n        origin = [origin, ] * input.ndim\n    else:\n        origin = list(origin)\n    wshape = [ii for ii in weights.shape if ii > 0]\n    if len(wshape) != input.ndim:\n        raise RuntimeError(\'filter weights array has incorrect shape.\')\n    if convolution:\n        weights = weights[tuple([slice(None, None, -1)] * weights.ndim)]\n        for ii in range(len(origin)):\n            origin[ii] = -origin[ii]\n            if weights.shape[ii] % 2 == 0:\n                origin[ii] -= 1\n    for _origin, lenw in zip(origin, wshape):\n        if (lenw // 2 + _origin < 0) or (lenw // 2 + _origin >= lenw):\n            raise ValueError(\'invalid origin\')\n    if mode not in (\'reflect\', \'constant\', \'nearest\', \'mirror\', \'wrap\'):\n        msg = \'boundary mode not supported (actual: {}).\'.format(mode)\n        raise RuntimeError(msg)\n\n    output = _get_output(output, input)\n    if weights.size == 0:\n        return output\n    input = cupy.ascontiguousarray(input)\n    weights = cupy.ascontiguousarray(weights, cupy.float64)\n    return _get_correlete_kernel(\n        input.ndim, mode, cval, input.shape, tuple(wshape), tuple(origin))(\n        input, weights, output)\n\n\ndef _generate_boundary_condition_ops(mode, ix, xsize):\n    if mode == \'reflect\':\n        ops = \'\'\'\n        if ({ix} < 0) {{\n            {ix} = - 1 - {ix};\n        }}\n        {ix} %= {xsize} * 2;\n        {ix} = min({ix}, 2 * {xsize} - 1 - {ix});\'\'\'.format(ix=ix, xsize=xsize)\n    elif mode == \'mirror\':\n        ops = \'\'\'\n        if ({ix} < 0) {{\n            {ix} = - {ix};\n        }}\n        if ({xsize} == 1) {{\n            {ix} = 0;\n        }} else {{\n            {ix} = 1 + ({ix} - 1) % (({xsize} - 1) * 2);\n            {ix} = min({ix}, 2 * {xsize} - 2 - {ix});\n        }}\'\'\'.format(ix=ix, xsize=xsize)\n    elif mode == \'nearest\':\n        ops = \'\'\'\n        {ix} = min(max({ix}, 0), {xsize} - 1);\'\'\'.format(ix=ix, xsize=xsize)\n    elif mode == \'wrap\':\n        ops = \'\'\'\n        if ({ix} < 0) {{\n            {ix} += (1 - ({ix} / {xsize})) * {xsize};\n        }}\n        {ix} %= {xsize};\'\'\'.format(ix=ix, xsize=xsize)\n    elif mode == \'constant\':\n        ops = \'\'\'\n        if ({ix} >= {xsize}) {{\n            {ix} = -1;\n        }}\'\'\'.format(ix=ix, xsize=xsize)\n    return ops\n\n\ndef _generate_correlete_kernel(ndim, mode, cval, xshape, wshape, origin):\n    in_params = \'raw X x, raw W w\'\n    out_params = \'Y y\'\n\n    ops = []\n    ops.append(\'X* x_data = (X*)&(x[0]);\')\n    ops.append(\'W* w_data = (W*)&(w[0]);\')\n    ops.append(\'const int sx_{} = 1;\'.format(ndim-1))\n    for j in range(ndim-1, 0, -1):\n        ops.append(\'int sx_{jm} = sx_{j} * {xsize_j};\'.\n                   format(jm=j-1, j=j, xsize_j=xshape[j]))\n    ops.append(\'int _i = i;\')\n    for j in range(ndim-1, -1, -1):\n        ops.append(\'int cx_{j} = _i % {xsize} - ({wsize} / 2) - ({origin});\'\n                   .format(j=j, xsize=xshape[j], wsize=wshape[j],\n                           origin=origin[j]))\n        if (j > 0):\n            ops.append(\'_i /= {xsize};\'.format(xsize=xshape[j]))\n    ops.append(\'W sum = (W)0;\')\n    ops.append(\'int iw = 0;\')\n\n    for j in range(ndim):\n        ops.append(\'\'\'\n    for (int iw_{j} = 0; iw_{j} < {wsize}; iw_{j}++)\n    {{\n        int ix_{j} = cx_{j} + iw_{j};\'\'\'.format(j=j, wsize=wshape[j]))\n        ixvar = \'ix_{}\'.format(j)\n        ops.append(_generate_boundary_condition_ops(mode, ixvar, xshape[j]))\n        ops.append(\'        ix_{j} *= sx_{j};\'.format(j=j))\n\n    ops.append(\'\'\'\n        W wval = w_data[iw];\n        if (wval == (W)0) {{\n            iw += 1;\n            continue;\n        }}\'\'\')\n    _cond = \' || \'.join([\'(ix_{0} < 0)\'.format(j) for j in range(ndim)])\n    _expr = \' + \'.join([\'ix_{0}\'.format(j) for j in range(ndim)])\n    ops.append(\'\'\'\n        if ({cond}) {{\n            sum += (W){cval} * wval;\n        }} else {{\n            int ix = {expr};\n            sum += (W)x_data[ix] * wval;\n        }}\n        iw += 1;\'\'\'.format(cond=_cond, expr=_expr, cval=cval))\n\n    ops.append(\'} \' * ndim)\n    ops.append(\'y = (Y)sum;\')\n    operation = \'\\n\'.join(ops)\n\n    name = \'cupy_ndimage_correlate_{}d_{}_x{}_w{}\'.format(\n        ndim, mode, \'_\'.join([\'{}\'.format(j) for j in xshape]),\n        \'_\'.join([\'{}\'.format(j) for j in wshape]))\n    return in_params, out_params, operation, name\n\n\n@util.memoize()\ndef _get_correlete_kernel(ndim, mode, cval, xshape, wshape, origin):\n    # weights is always casted to float64 in order to get an output compatible\n    # with SciPy, thought float32 might be sufficient when input dtype is low\n    # precision.\n    in_params, out_params, operation, name = _generate_correlete_kernel(\n        ndim, mode, cval, xshape, wshape, origin)\n    return cupy.ElementwiseKernel(in_params, out_params, operation, name)\n\n\ndef _normalize_sequence(x, ndim):\n    if not hasattr(x, \'__getitem__\') or isinstance(x, str):\n        return [x] * ndim\n    else:\n        return list(x)\n\n\ndef _min_or_max_filter(input, size, footprint, structure, output, mode,\n                       cval, origin, minimum):\n    if input.dtype.kind == \'c\':\n        raise TypeError(\'Complex type not supported.\')\n    if (size is not None) and (footprint is not None):\n        warnings.warn(""ignoring size because footprint is set"", UserWarning,\n                      stacklevel=3)\n    if structure is None:\n        if footprint is None:\n            if size is None:\n                raise RuntimeError(""no footprint provided"")\n            else:\n                footprint = cupy.ones(_normalize_sequence(size, input.ndim),\n                                      bool)\n        else:\n            footprint = cupy.array(footprint, dtype=bool, order=\'C\')\n            if not footprint.any():\n                raise ValueError(""All-zero footprint is not supported"")\n    else:\n        structure = cupy.array(structure, dtype=cupy.float64, order=\'C\')\n        if footprint is None:\n            footprint = cupy.ones(structure.shape, bool)\n        else:\n            footprint = cupy.array(footprint, dtype=bool, order=\'C\')\n    modes = _normalize_sequence(mode, input.ndim)\n    for mode in modes:\n        if mode not in (\'reflect\', \'constant\', \'nearest\', \'mirror\', \'wrap\'):\n            msg = \'boundary mode not supported (actual: {}).\'.format(mode)\n            raise RuntimeError(msg)\n    origins = _normalize_sequence(origin, input.ndim)\n    for origin, lenfp in zip(origins, footprint.shape):\n        if (lenfp // 2 + origin < 0) or (lenfp // 2 + origin >= lenfp):\n            raise ValueError(\'invalid origin\')\n\n    kernel = _get_min_or_max_kernel(\n        input.ndim, tuple(footprint.shape), structure is not None,\n        tuple(modes), cval, tuple(origins), minimum)\n\n    input = cupy.ascontiguousarray(input)\n    input_shape = cupy.array(input.shape, dtype=cupy.int32)\n    if structure is None:\n        structure = cupy.empty(0, dtype=cupy.float64)\n    output = _get_output(output, input)\n    return kernel(input, input_shape, footprint, structure, output)\n\n\ndef _generate_min_or_max_kernel(ndim, fp_shape, use_structure, modes, cval,\n                                origins, minimum):\n    in_params = \'raw X x, raw int32 x_shape, raw bool fp, raw S st\'\n    out_params = \'Y y\'\n\n    ops = []\n    ops.append(\'const int sx_{} = 1;\'.format(ndim-1))\n    for j in range(ndim-1, 0, -1):\n        ops.append(\'int sx_{jm} = sx_{j} * x_shape[{j}];\'.\n                   format(jm=j-1, j=j))\n\n    ops.append(\'int remain = i;\')\n    for j in range(ndim-1, -1, -1):\n        ops.append(\'int cx_{j} = remain % x_shape[{j}] - ({fpsize} / 2)\'\n                   \' - ({origin});\'\n                   .format(j=j, fpsize=fp_shape[j], origin=origins[j]))\n        if (j > 0):\n            ops.append(\'remain /= x_shape[{j}];\'.format(j=j))\n    ops.append(\'S ret = (S)0;\')\n    ops.append(\'int first = 1;\')\n    ops.append(\'int ifp = 0;\')\n\n    for j in range(ndim):\n        ops.append(\'\'\'\n    for (int ifp_{j} = 0; ifp_{j} < {fpsize}; ifp_{j}++)\n    {{\n        int ix_{j} = cx_{j} + ifp_{j};\'\'\'.format(j=j,\n                                                 fpsize=fp_shape[j]))\n        mode = modes[j]\n        ixvar = \'ix_{}\'.format(j)\n        xsize = \'x_shape[{}]\'.format(j)\n        ops.append(_generate_boundary_condition_ops(mode, ixvar, xsize))\n        ops.append(\'        ix_{j} *= sx_{j};\'.format(j=j))\n\n    ops.append(\'if (fp[ifp]) {\')\n    cond = \' || \'.join([\'(ix_{} < 0)\'.format(j) for j in range(ndim)])\n    ix = \' + \'.join([\'ix_{}\'.format(j) for j in range(ndim)])\n    ops.append(\'\'\'\n        S val;\n        if ({cond}) {{\n            val = (S){cval};\n        }} else {{\n            val = (S)x[{ix}];\n        }}\'\'\'.format(cond=cond, ix=ix, cval=cval))\n    if use_structure:\n        if minimum:\n            ops.append(\'        val -= st[ifp];\')\n        else:\n            ops.append(\'        val += st[ifp];\')\n    if minimum:\n        filter_type = \'min\'\n    else:\n        filter_type = \'max\'\n    ops.append(\'\'\'\n        if (first) {{\n            first = 0;\n            ret = val;\n        }} else {{\n            ret = {filter_type}(ret, val);\n        }}\'\'\'.format(filter_type=filter_type))\n    ops.append(\'}\')\n    ops.append(\'ifp++;\')\n\n    ops.append(\'} \' * ndim)\n    ops.append(\'y = (Y)ret;\')\n    operation = \'\\n\'.join(ops)\n\n    name = \'cupyx_nd_{}_{}d_fp_{}_md_{}\'.format(\n        filter_type, ndim,\n        \'_\'.join([\'{}\'.format(fp_shape[j]) for j in range(ndim)]),\n        \'_\'.join([\'{}\'.format(modes[j]) for j in range(ndim)])\n    )\n    if use_structure:\n        name += \'_wt_structure\'\n    else:\n        name += \'_wo_structure\'\n\n    return in_params, out_params, operation, name\n\n\n@util.memoize()\ndef _get_min_or_max_kernel(ndim, fp_shape, use_structure, modes, cval,\n                           origins, minimum):\n    in_params, out_params, operation, name = _generate_min_or_max_kernel(\n        ndim, fp_shape, use_structure, modes, cval, origins, minimum)\n    return cupy.ElementwiseKernel(in_params, out_params, operation, name)\n\n\ndef minimum_filter(input, size=None, footprint=None, output=None,\n                   mode=\'reflect\', cval=0.0, origin=0):\n    """"""Calculates a multi-dimensional minimum filter.\n\n    Args:\n        input (cupy.ndarray): The input array.\n        size (tuple of ints): ```size``` specifies the shape that is taken from\n            the input array, at every element position, to define the input to\n            the filter function.\n        footprint (array of ints): ```footprint``` specifies the shape, but\n            also which elements within the shape will get passed to the filter\n             function.\n        output (cupy.ndarray, dtype or None): The array in which to place the\n            output.\n        mode (str): The array borders are handled according to the given mode\n            (``\'reflect\'``, ``\'constant\'``, ``\'nearest\'``, ``\'mirror\'``,\n            ``\'wrap\'``). Default is ``\'reflect\'``.\n        cval (scalar): Value to fill past edges of input if mode is\n            ``constant``. Default is ``0.0``.\n        origin (scalar or tuple of scalar): The origin parameter controls the\n            placement of the filter, relative to the center of the current\n            element of the input. Default of 0 is equivalent to\n            ``(0,)*input.ndim``.\n\n    Returns:\n        cupy.ndarray: The result of minimum filter.\n\n    .. seealso:: :func:`scipy.ndimage.minimum_filter`\n    """"""\n    return _min_or_max_filter(input, size, footprint, None, output, mode, cval,\n                              origin, True)\n\n\ndef maximum_filter(input, size=None, footprint=None, output=None,\n                   mode=\'reflect\', cval=0.0, origin=0):\n    """"""Calculates a multi-dimensional maximum filter.\n\n    Args:\n        input (cupy.ndarray): The input array.\n        size (tuple of ints): ```size``` specifies the shape that is taken from\n            the input array, at every element position, to define the input to\n            the filter function.\n        footprint (array of ints): ```footprint``` specifies the shape, but\n            also which elements within the shape will get passed to the filter\n             function.\n        output (cupy.ndarray, dtype or None): The array in which to place the\n            output.\n        mode (str): The array borders are handled according to the given mode\n            (``\'reflect\'``, ``\'constant\'``, ``\'nearest\'``, ``\'mirror\'``,\n            ``\'wrap\'``). Default is ``\'reflect\'``.\n        cval (scalar): Value to fill past edges of input if mode is\n            ``constant``. Default is ``0.0``.\n        origin (scalar or tuple of scalar): The origin parameter controls the\n            placement of the filter, relative to the center of the current\n            element of the input. Default of 0 is equivalent to\n            ``(0,)*input.ndim``.\n\n    Returns:\n        cupy.ndarray: The result of maximum filter.\n\n    .. seealso:: :func:`scipy.ndimage.maximum_filter`\n    """"""\n    return _min_or_max_filter(input, size, footprint, None, output, mode, cval,\n                              origin, False)\n'"
cupyx/scipy/ndimage/interpolation.py,0,"b'import math\nimport warnings\n\nimport cupy\nimport numpy\n\nfrom cupyx.scipy.ndimage import _interp_kernels\n\n_prod = cupy.core.internal.prod\n\n\ndef _get_output(output, input, shape=None):\n    if shape is None:\n        shape = input.shape\n    if isinstance(output, cupy.ndarray):\n        if output.shape != tuple(shape):\n            raise ValueError(\'output shape is not correct\')\n    else:\n        dtype = output\n        if dtype is None:\n            dtype = input.dtype\n        output = cupy.zeros(shape, dtype)\n    return output\n\n\ndef _check_parameter(func_name, order, mode):\n    if order is None:\n        warnings.warn(\'In the current feature the default order of {} is 1. \'\n                      \'It is different from scipy.ndimage and can change in \'\n                      \'the future.\'.format(func_name))\n    elif order < 0 or 5 < order:\n        raise ValueError(\'spline order is not supported\')\n    elif 1 < order:\n        # SciPy supports order 0-5, but CuPy supports only order 0 and 1. Other\n        # orders will be implemented, therefore it raises NotImplementedError\n        # instead of ValueError.\n        raise NotImplementedError(\'spline order is not supported\')\n\n    if mode in (\'reflect\', \'wrap\'):\n        raise NotImplementedError(\'\\\'{}\\\' mode is not supported. See \'\n                                  \'https://github.com/scipy/scipy/issues/8465\'\n                                  .format(mode))\n    elif mode not in (\'constant\', \'nearest\', \'mirror\', \'opencv\',\n                      \'_opencv_edge\'):\n        raise ValueError(\'boundary mode is not supported\')\n\n\ndef map_coordinates(input, coordinates, output=None, order=None,\n                    mode=\'constant\', cval=0.0, prefilter=True):\n    """"""Map the input array to new coordinates by interpolation.\n\n    The array of coordinates is used to find, for each point in the output, the\n    corresponding coordinates in the input. The value of the input at those\n    coordinates is determined by spline interpolation of the requested order.\n\n    The shape of the output is derived from that of the coordinate array by\n    dropping the first axis. The values of the array along the first axis are\n    the coordinates in the input array at which the output value is found.\n\n    Args:\n        input (cupy.ndarray): The input array.\n        coordinates (array_like): The coordinates at which ``input`` is\n            evaluated.\n        output (cupy.ndarray or ~cupy.dtype): The array in which to place the\n            output, or the dtype of the returned array.\n        order (int): The order of the spline interpolation. If it is not given,\n            order 1 is used. It is different from :mod:`scipy.ndimage` and can\n            change in the future. Currently it supports only order 0 and 1.\n        mode (str): Points outside the boundaries of the input are filled\n            according to the given mode (``\'constant\'``, ``\'nearest\'``,\n            ``\'mirror\'`` or ``\'opencv\'``). Default is ``\'constant\'``.\n        cval (scalar): Value used for points outside the boundaries of\n            the input if ``mode=\'constant\'`` or ``mode=\'opencv\'``. Default is\n            0.0\n        prefilter (bool): It is not used yet. It just exists for compatibility\n            with :mod:`scipy.ndimage`.\n\n    Returns:\n        cupy.ndarray:\n            The result of transforming the input. The shape of the output is\n            derived from that of ``coordinates`` by dropping the first axis.\n\n    .. seealso:: :func:`scipy.ndimage.map_coordinates`\n    """"""\n\n    _check_parameter(\'map_coordinates\', order, mode)\n\n    if mode == \'opencv\' or mode == \'_opencv_edge\':\n        input = cupy.pad(input, [(1, 1)] * input.ndim, \'constant\',\n                         constant_values=cval)\n        coordinates = cupy.add(coordinates, 1)\n        mode = \'constant\'\n\n    ret = _get_output(output, input, coordinates.shape[1:])\n    integer_output = ret.dtype.kind in \'iu\'\n\n    if input.dtype.kind in \'iu\':\n        input = input.astype(cupy.float32)\n\n    large_int = max(_prod(input.shape), coordinates.shape[0]) > 1 << 31\n    kern = _interp_kernels._get_map_kernel(\n        input.ndim, large_int, yshape=coordinates.shape, mode=mode, cval=cval,\n        order=order, integer_output=integer_output)\n    kern(input, coordinates, ret)\n    return ret\n\n\ndef affine_transform(input, matrix, offset=0.0, output_shape=None, output=None,\n                     order=None, mode=\'constant\', cval=0.0, prefilter=True):\n    """"""Apply an affine transformation.\n\n    Given an output image pixel index vector ``o``, the pixel value is\n    determined from the input image at position\n    ``cupy.dot(matrix, o) + offset``.\n\n    Args:\n        input (cupy.ndarray): The input array.\n        matrix (cupy.ndarray): The inverse coordinate transformation matrix,\n            mapping output coordinates to input coordinates. If ``ndim`` is the\n            number of dimensions of ``input``, the given matrix must have one\n            of the following shapes:\n\n                - ``(ndim, ndim)``: the linear transformation matrix for each\n                  output coordinate.\n                - ``(ndim,)``: assume that the 2D transformation matrix is\n                  diagonal, with the diagonal specified by the given value.\n                - ``(ndim + 1, ndim + 1)``: assume that the transformation is\n                  specified using homogeneous coordinates. In this case, any\n                  value passed to ``offset`` is ignored.\n                - ``(ndim, ndim + 1)``: as above, but the bottom row of a\n                  homogeneous transformation matrix is always\n                  ``[0, 0, ..., 1]``, and may be omitted.\n\n        offset (float or sequence): The offset into the array where the\n            transform is applied. If a float, ``offset`` is the same for each\n            axis. If a sequence, ``offset`` should contain one value for each\n            axis.\n        output_shape (tuple of ints): Shape tuple.\n        output (cupy.ndarray or ~cupy.dtype): The array in which to place the\n            output, or the dtype of the returned array.\n        order (int): The order of the spline interpolation. If it is not given,\n            order 1 is used. It is different from :mod:`scipy.ndimage` and can\n            change in the future. Currently it supports only order 0 and 1.\n        mode (str): Points outside the boundaries of the input are filled\n            according to the given mode (``\'constant\'``, ``\'nearest\'``,\n            ``\'mirror\'`` or ``\'opencv\'``). Default is ``\'constant\'``.\n        cval (scalar): Value used for points outside the boundaries of\n            the input if ``mode=\'constant\'`` or ``mode=\'opencv\'``. Default is\n            0.0\n        prefilter (bool): It is not used yet. It just exists for compatibility\n            with :mod:`scipy.ndimage`.\n\n    Returns:\n        cupy.ndarray or None:\n            The transformed input. If ``output`` is given as a parameter,\n            ``None`` is returned.\n\n    .. seealso:: :func:`scipy.ndimage.affine_transform`\n    """"""\n\n    _check_parameter(\'affine_transform\', order, mode)\n\n    if not hasattr(offset, \'__iter__\') and type(offset) is not cupy.ndarray:\n        offset = [offset] * input.ndim\n\n    if matrix.ndim not in [1, 2] or matrix.shape[0] < 1:\n        raise RuntimeError(\'no proper affine matrix provided\')\n    if matrix.ndim == 2:\n        if matrix.shape[0] == matrix.shape[1] - 1:\n            offset = matrix[:, -1]\n            matrix = matrix[:, :-1]\n        elif matrix.shape[0] == input.ndim + 1:\n            offset = matrix[:-1, -1]\n            matrix = matrix[:-1, :-1]\n        if matrix.shape != (input.ndim, input.ndim):\n            raise RuntimeError(""improper affine shape"")\n\n    if mode == \'opencv\':\n        m = cupy.zeros((input.ndim + 1, input.ndim + 1))\n        m[:-1, :-1] = matrix\n        m[:-1, -1] = offset\n        m[-1, -1] = 1\n        m = cupy.linalg.inv(m)\n        m[:2] = cupy.roll(m[:2], 1, axis=0)\n        m[:2, :2] = cupy.roll(m[:2, :2], 1, axis=1)\n        matrix = m[:-1, :-1]\n        offset = m[:-1, -1]\n\n    if output_shape is None:\n        output_shape = input.shape\n\n    matrix = matrix.astype(cupy.float64, copy=False)\n    if order is None:\n        order = 1\n    ndim = input.ndim\n    output = _get_output(output, input, shape=output_shape)\n    if input.dtype.kind in \'iu\':\n        input = input.astype(cupy.float32)\n\n    integer_output = output.dtype.kind in \'iu\'\n    large_int = max(_prod(input.shape), _prod(output_shape)) > 1 << 31\n    if matrix.ndim == 1:\n        offset = cupy.asarray(offset, dtype=cupy.float64)\n        offset = -offset / matrix\n        kern = _interp_kernels._get_zoom_shift_kernel(\n            ndim, large_int, output_shape, mode, cval=cval, order=order,\n            integer_output=integer_output)\n        kern(input, offset, matrix, output)\n    else:\n        kern = _interp_kernels._get_affine_kernel(\n            ndim, large_int, output_shape, mode, cval=cval, order=order,\n            integer_output=integer_output)\n        m = cupy.zeros((ndim, ndim + 1), dtype=cupy.float64)\n        m[:, :-1] = matrix\n        m[:, -1] = cupy.asarray(offset, dtype=cupy.float64)\n        kern(input, m, output)\n    return output\n\n\ndef _minmax(coor, minc, maxc):\n    if coor[0] < minc[0]:\n        minc[0] = coor[0]\n    if coor[0] > maxc[0]:\n        maxc[0] = coor[0]\n    if coor[1] < minc[1]:\n        minc[1] = coor[1]\n    if coor[1] > maxc[1]:\n        maxc[1] = coor[1]\n    return minc, maxc\n\n\ndef rotate(input, angle, axes=(1, 0), reshape=True, output=None, order=None,\n           mode=\'constant\', cval=0.0, prefilter=True):\n    """"""Rotate an array.\n\n    The array is rotated in the plane defined by the two axes given by the\n    ``axes`` parameter using spline interpolation of the requested order.\n\n    Args:\n        input (cupy.ndarray): The input array.\n        angle (float): The rotation angle in degrees.\n        axes (tuple of 2 ints): The two axes that define the plane of rotation.\n            Default is the first two axes.\n        reshape (bool): If ``reshape`` is True, the output shape is adapted so\n            that the input array is contained completely in the output. Default\n            is True.\n        output (cupy.ndarray or ~cupy.dtype): The array in which to place the\n            output, or the dtype of the returned array.\n        order (int): The order of the spline interpolation. If it is not given,\n            order 1 is used. It is different from :mod:`scipy.ndimage` and can\n            change in the future. Currently it supports only order 0 and 1.\n        mode (str): Points outside the boundaries of the input are filled\n            according to the given mode (``\'constant\'``, ``\'nearest\'``,\n            ``\'mirror\'`` or ``\'opencv\'``). Default is ``\'constant\'``.\n        cval (scalar): Value used for points outside the boundaries of\n            the input if ``mode=\'constant\'`` or ``mode=\'opencv\'``. Default is\n            0.0\n        prefilter (bool): It is not used yet. It just exists for compatibility\n            with :mod:`scipy.ndimage`.\n\n    Returns:\n        cupy.ndarray or None:\n            The rotated input.\n\n    .. seealso:: :func:`scipy.ndimage.rotate`\n    """"""\n\n    _check_parameter(\'rotate\', order, mode)\n\n    if mode == \'opencv\':\n        mode = \'_opencv_edge\'\n\n    input_arr = input\n    axes = list(axes)\n    if axes[0] < 0:\n        axes[0] += input_arr.ndim\n    if axes[1] < 0:\n        axes[1] += input_arr.ndim\n    if axes[0] > axes[1]:\n        axes = [axes[1], axes[0]]\n    if axes[0] < 0 or input_arr.ndim <= axes[1]:\n        raise ValueError(\'invalid rotation plane specified\')\n\n    ndim = input_arr.ndim\n    rad = numpy.deg2rad(angle)\n    sin = math.sin(rad)\n    cos = math.cos(rad)\n\n    # determine offsets and output shape as in scipy.ndimage.rotate\n    rot_matrix = numpy.array([[cos, sin],\n                              [-sin, cos]])\n\n    img_shape = numpy.asarray(input_arr.shape)\n    in_plane_shape = img_shape[axes]\n    if reshape:\n        # Compute transformed input bounds\n        iy, ix = in_plane_shape\n        out_bounds = rot_matrix @ [[0, 0, iy, iy],\n                                   [0, ix, 0, ix]]\n        # Compute the shape of the transformed input plane\n        out_plane_shape = (out_bounds.ptp(axis=1) + 0.5).astype(cupy.int64)\n    else:\n        out_plane_shape = img_shape[axes]\n\n    out_center = rot_matrix @ ((out_plane_shape - 1) / 2)\n    in_center = (in_plane_shape - 1) / 2\n\n    output_shape = img_shape\n    output_shape[axes] = out_plane_shape\n    output_shape = tuple(output_shape)\n\n    matrix = numpy.identity(ndim)\n    matrix[axes[0], axes[0]] = cos\n    matrix[axes[0], axes[1]] = sin\n    matrix[axes[1], axes[0]] = -sin\n    matrix[axes[1], axes[1]] = cos\n\n    iy = input.shape[axes[0]]\n    offset = numpy.zeros(ndim, dtype=cupy.float64)\n    offset[axes] = in_center - out_center\n\n    matrix = cupy.asarray(matrix)\n    offset = cupy.asarray(offset)\n\n    return affine_transform(input, matrix, offset, output_shape, output, order,\n                            mode, cval, prefilter)\n\n\ndef shift(input, shift, output=None, order=None, mode=\'constant\', cval=0.0,\n          prefilter=True):\n    """"""Shift an array.\n\n    The array is shifted using spline interpolation of the requested order.\n    Points outside the boundaries of the input are filled according to the\n    given mode.\n\n    Args:\n        input (cupy.ndarray): The input array.\n        shift (float or sequence): The shift along the axes. If a float,\n            ``shift`` is the same for each axis. If a sequence, ``shift``\n            should contain one value for each axis.\n        output (cupy.ndarray or ~cupy.dtype): The array in which to place the\n            output, or the dtype of the returned array.\n        order (int): The order of the spline interpolation. If it is not given,\n            order 1 is used. It is different from :mod:`scipy.ndimage` and can\n            change in the future. Currently it supports only order 0 and 1.\n        mode (str): Points outside the boundaries of the input are filled\n            according to the given mode (``\'constant\'``, ``\'nearest\'``,\n            ``\'mirror\'`` or ``\'opencv\'``). Default is ``\'constant\'``.\n        cval (scalar): Value used for points outside the boundaries of\n            the input if ``mode=\'constant\'`` or ``mode=\'opencv\'``. Default is\n            0.0\n        prefilter (bool): It is not used yet. It just exists for compatibility\n            with :mod:`scipy.ndimage`.\n\n    Returns:\n        cupy.ndarray or None:\n            The shifted input.\n\n    .. seealso:: :func:`scipy.ndimage.shift`\n    """"""\n\n    _check_parameter(\'shift\', order, mode)\n\n    if not hasattr(shift, \'__iter__\') and type(shift) is not cupy.ndarray:\n        shift = [shift] * input.ndim\n\n    if mode == \'opencv\':\n        mode = \'_opencv_edge\'\n\n        output = affine_transform(\n            input,\n            cupy.ones(input.ndim, input.dtype),\n            cupy.negative(cupy.asarray(shift)),\n            None,\n            output,\n            order,\n            mode,\n            cval,\n            prefilter,\n        )\n    else:\n        if order is None:\n            order = 1\n        output = _get_output(output, input)\n        if input.dtype.kind in \'iu\':\n            input = input.astype(cupy.float32)\n        integer_output = output.dtype.kind in \'iu\'\n        large_int = _prod(input.shape) > 1 << 31\n        kern = _interp_kernels._get_shift_kernel(\n            input.ndim, large_int, input.shape, mode, cval=cval, order=order,\n            integer_output=integer_output)\n        shift = cupy.asarray(shift, dtype=cupy.float64)\n        kern(input, shift, output)\n    return output\n\n\ndef zoom(input, zoom, output=None, order=None, mode=\'constant\', cval=0.0,\n         prefilter=True):\n    """"""Zoom an array.\n\n    The array is zoomed using spline interpolation of the requested order.\n\n    Args:\n        input (cupy.ndarray): The input array.\n        zoom (float or sequence): The zoom factor along the axes. If a float,\n            ``zoom`` is the same for each axis. If a sequence, ``zoom`` should\n            contain one value for each axis.\n        output (cupy.ndarray or ~cupy.dtype): The array in which to place the\n            output, or the dtype of the returned array.\n        order (int): The order of the spline interpolation. If it is not given,\n            order 1 is used. It is different from :mod:`scipy.ndimage` and can\n            change in the future. Currently it supports only order 0 and 1.\n        mode (str): Points outside the boundaries of the input are filled\n            according to the given mode (``\'constant\'``, ``\'nearest\'``,\n            ``\'mirror\'`` or ``\'opencv\'``). Default is ``\'constant\'``.\n        cval (scalar): Value used for points outside the boundaries of\n            the input if ``mode=\'constant\'`` or ``mode=\'opencv\'``. Default is\n            0.0\n        prefilter (bool): It is not used yet. It just exists for compatibility\n            with :mod:`scipy.ndimage`.\n\n    Returns:\n        cupy.ndarray or None:\n            The zoomed input.\n\n    .. seealso:: :func:`scipy.ndimage.zoom`\n    """"""\n\n    _check_parameter(\'zoom\', order, mode)\n\n    if not hasattr(zoom, \'__iter__\') and type(zoom) is not cupy.ndarray:\n        zoom = [zoom] * input.ndim\n    output_shape = []\n    for s, z in zip(input.shape, zoom):\n        output_shape.append(int(round(s * z)))\n    output_shape = tuple(output_shape)\n\n    if mode == \'opencv\':\n        zoom = []\n        offset = []\n        for in_size, out_size in zip(input.shape, output_shape):\n            if out_size > 1:\n                zoom.append(float(in_size) / out_size)\n                offset.append((zoom[-1] - 1) / 2.0)\n            else:\n                zoom.append(0)\n                offset.append(0)\n        mode = \'nearest\'\n\n        output = affine_transform(\n            input,\n            cupy.asarray(zoom),\n            offset,\n            output_shape,\n            output,\n            order,\n            mode,\n            cval,\n            prefilter,\n        )\n    else:\n        if order is None:\n            order = 1\n\n        zoom = []\n        for in_size, out_size in zip(input.shape, output_shape):\n            if out_size > 1:\n                zoom.append(float(in_size - 1) / (out_size - 1))\n            else:\n                zoom.append(0)\n\n        output = _get_output(output, input, shape=output_shape)\n        if input.dtype.kind in \'iu\':\n            input = input.astype(cupy.float32)\n        integer_output = output.dtype.kind in \'iu\'\n        large_int = max(_prod(input.shape), _prod(output_shape)) > 1 << 31\n        kern = _interp_kernels._get_zoom_kernel(\n            input.ndim, large_int, output_shape, mode, order=order,\n            integer_output=integer_output)\n        zoom = cupy.asarray(zoom, dtype=cupy.float64)\n        kern(input, zoom, output)\n    return output\n'"
cupyx/scipy/ndimage/measurements.py,0,"b'import warnings\n\nimport numpy\n\nimport cupy\nfrom cupy import util\n\n\ndef label(input, structure=None, output=None):\n    """"""Labels features in an array.\n\n    Args:\n        input (cupy.ndarray): The input array.\n        structure (array_like or None): A structuring element that defines\n            feature connections. ```structure``` must be centersymmetric. If\n            None, structure is automatically generated with a squared\n            connectivity equal to one.\n        output (cupy.ndarray, dtype or None): The array in which to place the\n            output.\n    Returns:\n        label (cupy.ndarray): An integer array where each unique feature in\n        ```input``` has a unique label in the array.\n\n        num_features (int): Number of features found.\n\n    .. warning::\n\n        This function may synchronize the device.\n\n    .. seealso:: :func:`scipy.ndimage.label`\n    """"""\n    if not isinstance(input, cupy.ndarray):\n        raise TypeError(\'input must be cupy.ndarray\')\n    if input.dtype.char in \'FD\':\n        raise TypeError(\'Complex type not supported\')\n    if structure is None:\n        structure = _generate_binary_structure(input.ndim, 1)\n    elif isinstance(structure, cupy.ndarray):\n        structure = cupy.asnumpy(structure)\n    structure = numpy.array(structure, dtype=bool)\n    if structure.ndim != input.ndim:\n        raise RuntimeError(\'structure and input must have equal rank\')\n    for i in structure.shape:\n        if i != 3:\n            raise ValueError(\'structure dimensions must be equal to 3\')\n\n    if isinstance(output, cupy.ndarray):\n        if output.shape != input.shape:\n            raise ValueError(""output shape not correct"")\n        caller_provided_output = True\n    else:\n        caller_provided_output = False\n        if output is None:\n            output = cupy.empty(input.shape, numpy.int32)\n        else:\n            output = cupy.empty(input.shape, output)\n\n    if input.size == 0:\n        # empty\n        maxlabel = 0\n    elif input.ndim == 0:\n        # 0-dim array\n        maxlabel = 0 if input.item() == 0 else 1\n        output[...] = maxlabel\n    else:\n        if output.dtype != numpy.int32:\n            y = cupy.empty(input.shape, numpy.int32)\n        else:\n            y = output\n        maxlabel = _label(input, structure, y)\n        if output.dtype != numpy.int32:\n            output[...] = y[...]\n\n    if caller_provided_output:\n        return maxlabel\n    else:\n        return output, maxlabel\n\n\ndef _generate_binary_structure(rank, connectivity):\n    if connectivity < 1:\n        connectivity = 1\n    if rank < 1:\n        return numpy.array(True, dtype=bool)\n    output = numpy.fabs(numpy.indices([3] * rank) - 1)\n    output = numpy.add.reduce(output, 0)\n    return output <= connectivity\n\n\ndef _label(x, structure, y):\n    elems = numpy.where(structure != 0)\n    vecs = [elems[dm] - 1 for dm in range(x.ndim)]\n    offset = vecs[0]\n    for dm in range(1, x.ndim):\n        offset = offset * 3 + vecs[dm]\n    indxs = numpy.where(offset < 0)[0]\n    dirs = [[vecs[dm][dr] for dm in range(x.ndim)] for dr in indxs]\n    dirs = cupy.array(dirs, dtype=numpy.int32)\n    ndirs = indxs.shape[0]\n    y_shape = cupy.array(y.shape, dtype=numpy.int32)\n    count = cupy.zeros(2, dtype=numpy.int32)\n    _kernel_init()(x, y)\n    _kernel_connect()(y_shape, dirs, ndirs, x.ndim, y, size=y.size)\n    _kernel_count()(y, count, size=y.size)\n    maxlabel = int(count[0])\n    labels = cupy.empty(maxlabel, dtype=numpy.int32)\n    _kernel_labels()(y, count, labels, size=y.size)\n    _kernel_finalize()(maxlabel, cupy.sort(labels), y, size=y.size)\n    return maxlabel\n\n\ndef _kernel_init():\n    return cupy.ElementwiseKernel(\n        \'X x\', \'Y y\', \'if (x == 0) { y = -1; } else { y = i; }\',\n        \'cupyx_nd_label_init\')\n\n\ndef _kernel_connect():\n    return cupy.ElementwiseKernel(\n        \'raw int32 shape, raw int32 dirs, int32 ndirs, int32 ndim\',\n        \'raw Y y\',\n        \'\'\'\n        if (y[i] < 0) continue;\n        for (int dr = 0; dr < ndirs; dr++) {\n            int j = i;\n            int rest = j;\n            int stride = 1;\n            int k = 0;\n            for (int dm = ndim-1; dm >= 0; dm--) {\n                int pos = rest % shape[dm] + dirs[dm + dr * ndim];\n                if (pos < 0 || pos >= shape[dm]) {\n                    k = -1;\n                    break;\n                }\n                k += pos * stride;\n                rest /= shape[dm];\n                stride *= shape[dm];\n            }\n            if (k < 0) continue;\n            if (y[k] < 0) continue;\n            while (1) {\n                while (j != y[j]) { j = y[j]; }\n                while (k != y[k]) { k = y[k]; }\n                if (j == k) break;\n                if (j < k) {\n                    int old = atomicCAS( &y[k], k, j );\n                    if (old == k) break;\n                    k = old;\n                }\n                else {\n                    int old = atomicCAS( &y[j], j, k );\n                    if (old == j) break;\n                    j = old;\n                }\n            }\n        }\n        \'\'\',\n        \'cupyx_nd_label_connect\')\n\n\ndef _kernel_count():\n    return cupy.ElementwiseKernel(\n        \'\', \'raw Y y, raw int32 count\',\n        \'\'\'\n        if (y[i] < 0) continue;\n        int j = i;\n        while (j != y[j]) { j = y[j]; }\n        if (j != i) y[i] = j;\n        else atomicAdd(&count[0], 1);\n        \'\'\',\n        \'cupyx_nd_label_count\')\n\n\ndef _kernel_labels():\n    return cupy.ElementwiseKernel(\n        \'\', \'raw Y y, raw int32 count, raw int32 labels\',\n        \'\'\'\n        if (y[i] != i) continue;\n        int j = atomicAdd(&count[1], 1);\n        labels[j] = i;\n        \'\'\',\n        \'cupyx_nd_label_labels\')\n\n\ndef _kernel_finalize():\n    return cupy.ElementwiseKernel(\n        \'int32 maxlabel\', \'raw int32 labels, raw Y y\',\n        \'\'\'\n        if (y[i] < 0) {\n            y[i] = 0;\n            continue;\n        }\n        int yi = y[i];\n        int j_min = 0;\n        int j_max = maxlabel - 1;\n        int j = (j_min + j_max) / 2;\n        while (j_min < j_max) {\n            if (yi == labels[j]) break;\n            if (yi < labels[j]) j_max = j - 1;\n            else j_min = j + 1;\n            j = (j_min + j_max) / 2;\n        }\n        y[i] = j + 1;\n        \'\'\',\n        \'cupyx_nd_label_finalize\')\n\n\n_ndimage_variance_kernel = cupy.ElementwiseKernel(\n    \'T input, R labels, raw X index, uint64 size, raw float64 mean\',\n    \'raw float64 out\',\n    """"""\n    for (ptrdiff_t j = 0; j < size; j++) {\n      if (labels == index[j]) {\n        atomicAdd(&out[j], (input - mean[j]) * (input - mean[j]));\n        break;\n      }\n    }\n    """""")\n\n\n_ndimage_sum_kernel = cupy.ElementwiseKernel(\n    \'T input, R labels, raw X index, uint64 size\',\n    \'raw float64 out\',\n    """"""\n    for (ptrdiff_t j = 0; j < size; j++) {\n      if (labels == index[j]) {\n        atomicAdd(&out[j], input);\n        break;\n      }\n    }\n    """""")\n\n\ndef _ndimage_sum_kernel_2(input, labels, index, sum_val, batch_size=4):\n    for i in range(0, index.size, batch_size):\n        matched = labels == index[i:i + batch_size].reshape(\n            (-1,) + (1,) * input.ndim)\n        sum_axes = tuple(range(1, 1 + input.ndim))\n        sum_val[i:i + batch_size] = cupy.where(matched, input, 0).sum(\n            axis=sum_axes)\n    return sum_val\n\n\n_ndimage_mean_kernel = cupy.ElementwiseKernel(\n    \'T input, R labels, raw X index, uint64 size\',\n    \'raw float64 out, raw uint64 count\',\n    """"""\n    for (ptrdiff_t j = 0; j < size; j++) {\n      if (labels == index[j]) {\n        atomicAdd(&out[j], input);\n        atomicAdd(&count[j], 1);\n        break;\n      }\n    }\n    """""")\n\n\ndef _ndimage_mean_kernel_2(input, labels, index, batch_size=4,\n                           return_count=False):\n    sum_val = cupy.empty_like(index, dtype=cupy.float64)\n    count = cupy.empty_like(index, dtype=cupy.uint64)\n    for i in range(0, index.size, batch_size):\n        matched = labels == index[i:i + batch_size].reshape(\n            (-1,) + (1,) * input.ndim)\n        mean_axes = tuple(range(1, 1 + input.ndim))\n        count[i:i + batch_size] = matched.sum(axis=mean_axes)\n        sum_val[i:i + batch_size] = cupy.where(matched, input, 0).sum(\n            axis=mean_axes)\n    if return_count:\n        return sum_val / count, count\n    return sum_val / count\n\n\ndef _mean_driver(input, labels, index, return_count=False, use_kern=False):\n    if use_kern:\n        return _ndimage_mean_kernel_2(input, labels, index,\n                                      return_count=return_count)\n\n    out = cupy.zeros_like(index, cupy.float64)\n    count = cupy.zeros_like(index, dtype=cupy.uint64)\n    sum, count = _ndimage_mean_kernel(input,\n                                      labels, index, index.size, out, count)\n    if return_count:\n        return sum / count, count\n    return sum / count\n\n\ndef variance(input, labels=None, index=None):\n    """"""Calculates the variance of the values of an n-D image array, optionally\n    at specified sub-regions.\n\n    Args:\n        input (cupy.ndarray): Nd-image data to process.\n        labels (cupy.ndarray or None): Labels defining sub-regions in `input`.\n            If not None, must be same shape as `input`.\n        index (cupy.ndarray or None): `labels` to include in output. If None\n            (default), all values where `labels` is non-zero are used.\n\n    Returns:\n        variance (cupy.ndarray): Values of variance, for each sub-region if\n            `labels` and `index` are specified.\n\n    .. seealso:: :func:`scipy.ndimage.variance`\n    """"""\n    if not isinstance(input, cupy.ndarray):\n        raise TypeError(\'input must be cupy.ndarray\')\n\n    if input.dtype in (cupy.bool_, cupy.complex64, cupy.complex128):\n        raise TypeError(""cupyx.scipy.ndimage.variance doesn\'t support %{}""\n                        """".format(input.dtype.type))\n\n    use_kern = False\n    # There is constraints on types because of atomicAdd() in CUDA.\n    if input.dtype not in [cupy.int32, cupy.float16, cupy.float32,\n                           cupy.float64, cupy.uint32, cupy.uint64,\n                           cupy.ulonglong]:\n        warnings.warn(\n            \'Using the slower implmentation as \'\n            \'cupyx.scipy.ndimage.sum supports int32, float16, \'\n            \'float32, float64, uint32, uint64 as data types\'\n            \'for the fast implmentation\', util.PerformanceWarning)\n        use_kern = True\n\n    def calc_var_with_intermediate_float(input):\n        vals_c = input - input.mean()\n        count = vals_c.size\n        # Does not use `ndarray.mean()` here to return the same results as\n        # SciPy does, especially in case `input`\'s dtype is float16.\n        return cupy.square(vals_c).sum() / cupy.asanyarray(count).astype(float)\n\n    if labels is None:\n        return calc_var_with_intermediate_float(input)\n\n    if not isinstance(labels, cupy.ndarray):\n        raise TypeError(\'label must be cupy.ndarray\')\n\n    if index is None:\n        return calc_var_with_intermediate_float(input[labels > 0])\n\n    if cupy.isscalar(index):\n        return calc_var_with_intermediate_float(input[labels == index])\n\n    input, labels = cupy.broadcast_arrays(input, labels)\n\n    if not isinstance(index, cupy.ndarray):\n        if not isinstance(index, int):\n            raise TypeError(\'index must be cupy.ndarray or a scalar int\')\n        else:\n            return (input[labels == index]).var().astype(cupy.float64,\n                                                         copy=False)\n\n    mean_val, count = _mean_driver(input, labels, index, True, use_kern)\n    if use_kern:\n        new_axis = (..., *(cupy.newaxis for _ in range(input.ndim)))\n        return cupy.where(labels[None, ...] == index[new_axis],\n                          cupy.square(input - mean_val[new_axis]),\n                          0).sum(tuple(range(1, input.ndim + 1))) / count\n    out = cupy.zeros_like(index, dtype=cupy.float64)\n    return _ndimage_variance_kernel(input, labels, index, index.size, mean_val,\n                                    out) / count\n\n\ndef sum(input, labels=None, index=None):\n    """"""Calculates the sum of the values of an n-D image array, optionally\n       at specified sub-regions.\n\n    Args:\n        input (cupy.ndarray): Nd-image data to process.\n        labels (cupy.ndarray or None): Labels defining sub-regions in `input`.\n            If not None, must be same shape as `input`.\n        index (cupy.ndarray or None): `labels` to include in output. If None\n            (default), all values where `labels` is non-zero are used.\n\n    Returns:\n       sum (cupy.ndarray): sum of values, for each sub-region if\n       `labels` and `index` are specified.\n\n    .. seealso:: :func:`scipy.ndimage.sum`\n    """"""\n    if not isinstance(input, cupy.ndarray):\n        raise TypeError(\'input must be cupy.ndarray\')\n\n    if input.dtype in (cupy.bool_, cupy.complex64, cupy.complex128):\n        raise TypeError(""cupyx.scipy.ndimage.sum doesnt support %{}"".format(\n            input.dtype.type))\n\n    use_kern = False\n    # There is constraints on types because of atomicAdd() in CUDA.\n    if input.dtype not in [cupy.int32, cupy.float16, cupy.float32,\n                           cupy.float64, cupy.uint32, cupy.uint64,\n                           cupy.ulonglong]:\n        warnings.warn(\n            \'Using the slower implmentation as \'\n            \'cupyx.scipy.ndimage.sum supports int32, float16, \'\n            \'float32, float64, uint32, uint64 as data types\'\n            \'for the fast implmentation\', util.PerformanceWarning)\n        use_kern = True\n\n    if labels is None:\n        return input.sum()\n    if len(labels) == 0:\n        return cupy.array([], dtype=cupy.int64)\n\n    if not isinstance(labels, cupy.ndarray):\n        raise TypeError(\'label must be cupy.ndarray\')\n\n    if index is None:\n        return input[labels != 0].sum()\n\n    input, labels = cupy.broadcast_arrays(input, labels)\n\n    if not isinstance(index, cupy.ndarray):\n        if not isinstance(index, int):\n            raise TypeError(\'index must be cupy.ndarray or a scalar int\')\n        else:\n            return (input[labels == index]).sum()\n\n    out = cupy.zeros_like(index, dtype=cupy.float64)\n\n    # The following parameters for sum where determined using a Tesla P100.\n    if (input.size >= 262144 and index.size <= 4) or use_kern:\n        return _ndimage_sum_kernel_2(input, labels, index, out)\n    return _ndimage_sum_kernel(input, labels, index, index.size, out)\n\n\ndef mean(input, labels=None, index=None):\n    """"""Calculates the mean of the values of an n-D image array, optionally\n       at specified sub-regions.\n\n    Args:\n        input (cupy.ndarray): Nd-image data to process.\n        labels (cupy.ndarray or None): Labels defining sub-regions in `input`.\n            If not None, must be same shape as `input`.\n        index (cupy.ndarray or None): `labels` to include in output. If None\n            (default), all values where `labels` is non-zero are used.\n\n    Returns:\n        mean (cupy.ndarray): mean of values, for each sub-region if\n        `labels` and `index` are specified.\n\n\n    .. seealso:: :func:`scipy.ndimage.mean`\n    """"""\n    if not isinstance(input, cupy.ndarray):\n        raise TypeError(\'input must be cupy.ndarray\')\n\n    if input.dtype in (cupy.bool_, cupy.complex64, cupy.complex128):\n        raise TypeError(""cupyx.scipy.ndimage.mean doesnt support %{}"".format(\n            input.dtype.type))\n\n    use_kern = False\n    # There is constraints on types because of atomicAdd() in CUDA.\n    if input.dtype not in [cupy.int32, cupy.float16, cupy.float32,\n                           cupy.float64, cupy.uint32, cupy.uint64,\n                           cupy.ulonglong]:\n        warnings.warn(\n            \'Using the slower implmentation as \'\n            \'cupyx.scipy.ndimage.mean supports int32, float16, \'\n            \'float32, float64, uint32, uint64 as data types \'\n            \'for the fast implmentation\', util.PerformanceWarning)\n        use_kern = True\n\n    def calc_mean_with_intermediate_float(input):\n        sum = input.sum()\n        count = input.size\n        # Does not use `ndarray.mean()` here to return the same results as\n        # SciPy does, especially in case `input`\'s dtype is float16.\n        return sum / cupy.asanyarray(count).astype(float)\n\n    if labels is None:\n        return calc_mean_with_intermediate_float(input)\n\n    if not isinstance(labels, cupy.ndarray):\n        raise TypeError(\'label must be cupy.ndarray\')\n\n    if index is None:\n        return calc_mean_with_intermediate_float(input[labels > 0])\n\n    if cupy.isscalar(index):\n        return calc_mean_with_intermediate_float(input[labels == index])\n\n    input, labels = cupy.broadcast_arrays(input, labels)\n\n    if not isinstance(index, cupy.ndarray):\n        if not isinstance(index, int):\n            raise TypeError(\'index must be cupy.ndarray or a scalar int\')\n        else:\n            return (input[labels == index]).mean(dtype=cupy.float64)\n\n    return _mean_driver(input, labels, index, use_kern=use_kern)\n\n\ndef standard_deviation(input, labels=None, index=None):\n    """"""Calculates the standard deviation of the values of an n-D image array,\n    optionally at specified sub-regions.\n\n    Args:\n        input (cupy.ndarray): Nd-image data to process.\n        labels (cupy.ndarray or None): Labels defining sub-regions in `input`.\n            If not None, must be same shape as `input`.\n        index (cupy.ndarray or None): `labels` to include in output. If None\n            (default), all values where `labels` is non-zero are used.\n\n    Returns:\n        standard_deviation (cupy.ndarray): standard deviation of values, for\n        each sub-region if `labels` and `index` are specified.\n\n    .. seealso:: :func:`scipy.ndimage.standard_deviation`\n    """"""\n    return cupy.sqrt(variance(input, labels, index))\n'"
cupyx/scipy/ndimage/morphology.py,0,"b'import numpy\n\nimport cupy\n\nimport warnings\n\nfrom . import filters\n\n\ndef grey_erosion(input, size=None, footprint=None, structure=None, output=None,\n                 mode=\'reflect\', cval=0.0, origin=0):\n    """"""Calculates a greyscale erosion.\n\n    Args:\n        input (cupy.ndarray): The input array.\n        size (tuple of ints): Shape of a flat and full structuring element used\n            for the greyscale erosion. Optional if ```footprint``` or\n            ```structure``` is provided.\n        footprint (array of ints): Positions of non-infinite elements of a flat\n            structuring element used for greyscale erosion. Non-zero values\n            give the set of neighbors of the center over which minimum is\n            chosen.\n        structure (array of ints): Structuring element used for the greyscale\n            erosion. ```structure``` may be a non-flat structuring element.\n        output (cupy.ndarray, dtype or None): The array in which to place the\n            output.\n        mode (str): The array borders are handled according to the given mode\n            (``\'reflect\'``, ``\'constant\'``, ``\'nearest\'``, ``\'mirror\'``,\n            ``\'wrap\'``). Default is ``\'reflect\'``.\n        cval (scalar): Value to fill past edges of input if mode is\n            ``constant``. Default is ``0.0``.\n        origin (scalar or tuple of scalar): The origin parameter controls the\n            placement of the filter, relative to the center of the current\n            element of the input. Default of 0 is equivalent to\n            ``(0,)*input.ndim``.\n\n    Returns:\n        cupy.ndarray: The result of greyscale erosion.\n\n    .. seealso:: :func:`scipy.ndimage.grey_erosion`\n    """"""\n\n    if size is None and footprint is None and structure is None:\n        raise ValueError(\'size, footprint or structure must be specified\')\n\n    return filters._min_or_max_filter(input, size, footprint, structure,\n                                      output, mode, cval, origin, True)\n\n\ndef grey_dilation(input, size=None, footprint=None, structure=None,\n                  output=None, mode=\'reflect\', cval=0.0, origin=0):\n    """"""Calculates a greyscale dilation.\n\n    Args:\n        input (cupy.ndarray): The input array.\n        size (tuple of ints): Shape of a flat and full structuring element used\n            for the greyscale dilation. Optional if ```footprint``` or\n            ```structure``` is provided.\n        footprint (array of ints): Positions of non-infinite elements of a flat\n            structuring element used for greyscale dilation. Non-zero values\n            give the set of neighbors of the center over which maximum is\n            chosen.\n        structure (array of ints): Structuring element used for the greyscale\n            dilation. ```structure``` may be a non-flat structuring element.\n        output (cupy.ndarray, dtype or None): The array in which to place the\n            output.\n        mode (str): The array borders are handled according to the given mode\n            (``\'reflect\'``, ``\'constant\'``, ``\'nearest\'``, ``\'mirror\'``,\n            ``\'wrap\'``). Default is ``\'reflect\'``.\n        cval (scalar): Value to fill past edges of input if mode is\n            ``constant``. Default is ``0.0``.\n        origin (scalar or tuple of scalar): The origin parameter controls the\n            placement of the filter, relative to the center of the current\n            element of the input. Default of 0 is equivalent to\n            ``(0,)*input.ndim``.\n\n    Returns:\n        cupy.ndarray: The result of greyscale dilation.\n\n    .. seealso:: :func:`scipy.ndimage.grey_dilation`\n    """"""\n\n    if size is None and footprint is None and structure is None:\n        raise ValueError(\'size, footprint or structure must be specified\')\n    if structure is not None:\n        structure = cupy.array(structure)\n        structure = structure[tuple([slice(None, None, -1)] * structure.ndim)]\n    if footprint is not None:\n        footprint = cupy.array(footprint)\n        footprint = footprint[tuple([slice(None, None, -1)] * footprint.ndim)]\n\n    origin = filters._normalize_sequence(origin, input.ndim)\n    for i in range(len(origin)):\n        origin[i] = -origin[i]\n        if footprint is not None:\n            sz = footprint.shape[i]\n        elif structure is not None:\n            sz = structure.shape[i]\n        elif numpy.isscalar(size):\n            sz = size\n        else:\n            sz = size[i]\n        if sz % 2 == 0:\n            origin[i] -= 1\n\n    return filters._min_or_max_filter(input, size, footprint, structure,\n                                      output, mode, cval, origin, False)\n\n\ndef grey_closing(input, size=None, footprint=None, structure=None,\n                 output=None, mode=\'reflect\', cval=0.0, origin=0):\n    """"""Calculates a multi-dimensional greyscale closing.\n\n    Args:\n        input (cupy.ndarray): The input array.\n        size (tuple of ints): Shape of a flat and full structuring element used\n            for the greyscale closing. Optional if ```footprint``` or\n            ```structure``` is provided.\n        footprint (array of ints): Positions of non-infinite elements of a flat\n            structuring element used for greyscale closing. Non-zero values\n            give the set of neighbors of the center over which closing is\n            chosen.\n        structure (array of ints): Structuring element used for the greyscale\n            closing. ```structure``` may be a non-flat structuring element.\n        output (cupy.ndarray, dtype or None): The array in which to place the\n            output.\n        mode (str): The array borders are handled according to the given mode\n            (``\'reflect\'``, ``\'constant\'``, ``\'nearest\'``, ``\'mirror\'``,\n            ``\'wrap\'``). Default is ``\'reflect\'``.\n        cval (scalar): Value to fill past edges of input if mode is\n            ``constant``. Default is ``0.0``.\n        origin (scalar or tuple of scalar): The origin parameter controls the\n            placement of the filter, relative to the center of the current\n            element of the input. Default of 0 is equivalent to\n            ``(0,)*input.ndim``.\n\n    Returns:\n        cupy.ndarray: The result of greyscale closing.\n\n    .. seealso:: :func:`scipy.ndimage.grey_closing`\n    """"""\n    if (size is not None) and (footprint is not None):\n        warnings.warn(""ignoring size because footprint is set"", UserWarning,\n                      stacklevel=2)\n    tmp = grey_dilation(input, size, footprint, structure, None, mode, cval,\n                        origin)\n    return grey_erosion(tmp, size, footprint, structure, output, mode, cval,\n                        origin)\n\n\ndef grey_opening(input, size=None, footprint=None, structure=None,\n                 output=None, mode=\'reflect\', cval=0.0, origin=0):\n    """"""Calculates a multi-dimensional greyscale opening.\n\n    Args:\n        input (cupy.ndarray): The input array.\n        size (tuple of ints): Shape of a flat and full structuring element used\n            for the greyscale opening. Optional if ```footprint``` or\n            ```structure``` is provided.\n        footprint (array of ints): Positions of non-infinite elements of a flat\n            structuring element used for greyscale opening. Non-zero values\n            give the set of neighbors of the center over which opening is\n            chosen.\n        structure (array of ints): Structuring element used for the greyscale\n            opening. ```structure``` may be a non-flat structuring element.\n        output (cupy.ndarray, dtype or None): The array in which to place the\n            output.\n        mode (str): The array borders are handled according to the given mode\n            (``\'reflect\'``, ``\'constant\'``, ``\'nearest\'``, ``\'mirror\'``,\n            ``\'wrap\'``). Default is ``\'reflect\'``.\n        cval (scalar): Value to fill past edges of input if mode is\n            ``constant``. Default is ``0.0``.\n        origin (scalar or tuple of scalar): The origin parameter controls the\n            placement of the filter, relative to the center of the current\n            element of the input. Default of 0 is equivalent to\n            ``(0,)*input.ndim``.\n\n    Returns:\n        cupy.ndarray: The result of greyscale opening.\n\n    .. seealso:: :func:`scipy.ndimage.grey_opening`\n    """"""\n    if (size is not None) and (footprint is not None):\n        warnings.warn(""ignoring size because footprint is set"", UserWarning,\n                      stacklevel=2)\n    tmp = grey_erosion(input, size, footprint, structure, None, mode, cval,\n                       origin)\n    return grey_dilation(tmp, size, footprint, structure, output, mode, cval,\n                         origin)\n'"
cupyx/scipy/sparse/__init__.py,0,b'from cupyx.scipy.sparse.base import issparse  # NOQA\nfrom cupyx.scipy.sparse.base import isspmatrix  # NOQA\nfrom cupyx.scipy.sparse.base import spmatrix  # NOQA\nfrom cupyx.scipy.sparse.coo import coo_matrix  # NOQA\nfrom cupyx.scipy.sparse.coo import isspmatrix_coo  # NOQA\nfrom cupyx.scipy.sparse.csc import csc_matrix  # NOQA\nfrom cupyx.scipy.sparse.csc import isspmatrix_csc  # NOQA\nfrom cupyx.scipy.sparse.csr import csr_matrix  # NOQA\nfrom cupyx.scipy.sparse.csr import isspmatrix_csr  # NOQA\nfrom cupyx.scipy.sparse.dia import dia_matrix  # NOQA\nfrom cupyx.scipy.sparse.dia import isspmatrix_dia  # NOQA\n\nfrom cupyx.scipy.sparse.construct import eye  # NOQA\nfrom cupyx.scipy.sparse.construct import identity  # NOQA\nfrom cupyx.scipy.sparse.construct import rand  # NOQA\nfrom cupyx.scipy.sparse.construct import random  # NOQA\nfrom cupyx.scipy.sparse.construct import spdiags  # NOQA\nfrom cupyx.scipy.sparse.construct import diags  # NOQA\n\nfrom cupyx.scipy.sparse.construct import bmat  # NOQA\nfrom cupyx.scipy.sparse.construct import hstack  # NOQA\nfrom cupyx.scipy.sparse.construct import vstack  # NOQA\n\n# TODO(unno): implement bsr_matrix\n# TODO(unno): implement dok_matrix\n# TODO(unno): implement lil_matrix\n\n# TODO(unno): implement kron\n# TODO(unno): implement kronsum\n# TODO(unno): implement diags\n# TODO(unno): implement block_diag\n# TODO(unno): implement tril\n# TODO(unno): implement triu\n\n# TODO(unno): implement save_npz\n# TODO(unno): implement load_npz\n\n# TODO(unno): implement find\n\n# TODO(unno): implement isspmatrix_bsr(x)\n# TODO(unno): implement isspmatrix_lil(x)\n# TODO(unno): implement isspmatrix_dok(x)\n\nfrom cupyx.scipy.sparse import linalg  # NOQA\n'
cupyx/scipy/sparse/base.py,0,"b'import numpy\n\nimport cupy\nfrom cupyx.scipy.sparse import util\n\n\nclass spmatrix(object):\n\n    """"""Base class of all sparse matrixes.\n\n    See :class:`scipy.sparse.spmatrix`\n    """"""\n\n    __array_priority__ = 101\n\n    def __init__(self, maxprint=50):\n        if self.__class__ == spmatrix:\n            raise ValueError(\n                \'This class is not intended to be instantiated directly.\')\n        self.maxprint = maxprint\n\n    @property\n    def device(self):\n        """"""CUDA device on which this array resides.""""""\n        raise NotImplementedError\n\n    def get(self, stream=None):\n        """"""Return a copy of the array on host memory.\n\n        Args:\n            stream (cupy.cuda.Stream): CUDA stream object. If it is given, the\n                copy runs asynchronously. Otherwise, the copy is synchronous.\n\n        Returns:\n            scipy.sparse.spmatrix: An array on host memory.\n\n        """"""\n        raise NotImplementedError\n\n    def __len__(self):\n        raise TypeError(\'sparse matrix length is ambiguous; \'\n                        \'use getnnz() or shape[0]\')\n\n    def __str__(self):\n        # TODO(unno): Do not use get method which is only available when scipy\n        # is installed.\n        return str(self.get())\n\n    def __iter__(self):\n        for r in range(self.shape[0]):\n            yield self[r, :]\n\n    def __bool__(self):\n        if self.shape == (1, 1):\n            return self.nnz != 0\n        else:\n            raise ValueError(\'The truth value of an array with more than one \'\n                             \'element is ambiguous. Use a.any() or a.all().\')\n\n    __nonzero__ = __bool__\n\n    def __eq__(self, other):\n        return self.tocsr().__eq__(other)\n\n    def __ne__(self, other):\n        return self.tocsr().__ne__(other)\n\n    def __lt__(self, other):\n        return self.tocsr().__lt__(other)\n\n    def __gt__(self, other):\n        return self.tocsr().__gt__(other)\n\n    def __le__(self, other):\n        return self.tocsr().__le__(other)\n\n    def __ge__(self, other):\n        return self.tocsr().__ge__(other)\n\n    def __abs__(self):\n        return self.tocsr().__abs__()\n\n    def __add__(self, other):\n        return self.tocsr().__add__(other)\n\n    def __radd__(self, other):\n        return self.tocsr().__radd__(other)\n\n    def __sub__(self, other):\n        return self.tocsr().__sub__(other)\n\n    def __rsub__(self, other):\n        return self.tocsr().__rsub__(other)\n\n    def __mul__(self, other):\n        return self.tocsr().__mul__(other)\n\n    def __rmul__(self, other):\n        if cupy.isscalar(other) or isdense(other) and other.ndim == 0:\n            return self * other\n        else:\n            try:\n                tr = other.T\n            except AttributeError:\n                return NotImplemented\n            return (self.T * tr).T\n\n    def __div__(self, other):\n        return self.tocsr().__div__(other)\n\n    def __rdiv__(self, other):\n        return self.tocsr().__rdiv__(other)\n\n    def __truediv__(self, other):\n        return self.tocsr().__truediv__(other)\n\n    def __rtruediv__(self, other):\n        return self.tocsr().__rdtrueiv__(other)\n\n    def __neg__(self):\n        return -self.tocsr()\n\n    def __iadd__(self, other):\n        return NotImplemented\n\n    def __isub__(self, other):\n        return NotImplemented\n\n    def __imul__(self, other):\n        return NotImplemented\n\n    def __idiv__(self, other):\n        return self.__itruediv__(other)\n\n    def __itruediv__(self, other):\n        return NotImplemented\n\n    def __pow__(self, other):\n        """"""Calculates n-th power of the matrix.\n\n        This method calculates n-th power of a given matrix. The matrix must\n        be a squared matrix, and a given exponent must be an integer.\n\n        Args:\n            other (int): Exponent.\n\n        Returns:\n            cupyx.scipy.sparse.spmatrix: A sparse matrix representing n-th\n                power of this matrix.\n\n        """"""\n        m, n = self.shape\n        if m != n:\n            raise TypeError(\'matrix is not square\')\n\n        if util.isintlike(other):\n            other = int(other)\n            if other < 0:\n                raise ValueError(\'exponent must be >= 0\')\n\n            if other == 0:\n                import cupyx.scipy.sparse\n                return cupyx.scipy.sparse.identity(\n                    m, dtype=self.dtype, format=\'csr\')\n            elif other == 1:\n                return self.copy()\n            else:\n                tmp = self.__pow__(other // 2)\n                if other % 2:\n                    return self * tmp * tmp\n                else:\n                    return tmp * tmp\n        elif util.isscalarlike(other):\n            raise ValueError(\'exponent must be an integer\')\n        else:\n            return NotImplemented\n\n    @property\n    def A(self):\n        """"""Dense ndarray representation of this matrix.\n\n        This property is equivalent to\n        :meth:`~cupyx.scipy.sparse.spmatrix.toarray` method.\n\n        """"""\n        return self.toarray()\n\n    @property\n    def T(self):\n        return self.transpose()\n\n    @property\n    def H(self):\n        return self.getH()\n\n    @property\n    def ndim(self):\n        return 2\n\n    @property\n    def size(self):\n        return self.getnnz()\n\n    @property\n    def nnz(self):\n        return self.getnnz()\n\n    @property\n    def shape(self):\n        return self.get_shape()\n\n    @shape.setter\n    def shape(self, value):\n        self.set_shape(value)\n\n    def asformat(self, format):\n        """"""Return this matrix in a given sparse format.\n\n        Args:\n            format (str or None): Format you need.\n        """"""\n        if format is None or format == self.format:\n            return self\n        else:\n            return getattr(self, \'to\' + format)()\n\n    def asfptype(self):\n        """"""Upcasts matrix to a floating point format.\n\n        When the matrix has floating point type, the method returns itself.\n        Otherwise it makes a copy with floating point type and the same format.\n\n        Returns:\n            cupyx.scipy.sparse.spmatrix: A matrix with float type.\n\n        """"""\n        if self.dtype.kind == \'f\':\n            return self\n        else:\n            typ = numpy.promote_types(self.dtype, \'f\')\n            return self.astype(typ)\n\n    def astype(self, t):\n        """"""Casts the array to given data type.\n\n        Args:\n            t: Type specifier.\n\n        Returns:\n            cupyx.scipy.sparse.spmatrix:\n                A copy of the array with the given type and the same format.\n\n        """"""\n        return self.tocsr().astype(t).asformat(self.format)\n\n    def conj(self, copy=True):\n        """"""Element-wise complex conjugation.\n\n        If the matrix is of non-complex data type and `copy` is False,\n        this method does nothing and the data is not copied.\n\n        Args:\n            copy (bool):\n                If True, the result is guaranteed to not share data with self.\n\n        Returns:\n            cupyx.scipy.sparse.spmatrix : The element-wise complex conjugate.\n\n        """"""\n        if self.dtype.kind == \'c\':\n            return self.tocsr(copy=copy).conj(copy=False)\n        elif copy:\n            return self.copy()\n        else:\n            return self\n\n    def conjugate(self, copy=True):\n        return self.conj(copy=copy)\n\n    conjugate.__doc__ = conj.__doc__\n\n    def copy(self):\n        """"""Returns a copy of this matrix.\n\n        No data/indices will be shared between the returned value and current\n        matrix.\n        """"""\n        return self.__class__(self, copy=True)\n\n    def count_nonzero(self):\n        """"""Number of non-zero entries, equivalent to""""""\n        raise NotImplementedError\n\n    def diagonal(self, k=0):\n        """"""Returns the k-th diagonal of the matrix.\n\n        Args:\n            k (int, optional): Which diagonal to get, corresponding to elements\n            a[i, i+k]. Default: 0 (the main diagonal).\n\n        Returns:\n            cupy.ndarray : The k-th diagonal.\n        """"""\n        return self.tocsr().diagonal(k=k)\n\n    def dot(self, other):\n        """"""Ordinary dot product""""""\n        return self * other\n\n    def getH(self):\n        return self.transpose().conj()\n\n    def get_shape(self):\n        raise NotImplementedError\n\n    # TODO(unno): Implement getcol\n\n    def getformat(self):\n        return self.format\n\n    def getmaxprint(self):\n        return self.maxprint\n\n    def getnnz(self, axis=None):\n        """"""Number of stored values, including explicit zeros.""""""\n        raise NotImplementedError\n\n    # TODO(unno): Implement getrow\n\n    def maximum(self, other):\n        return self.tocsr().maximum(other)\n\n    # TODO(unno): Implement mean\n\n    def minimum(self, other):\n        return self.tocsr().minimum(other)\n\n    def multiply(self, other):\n        """"""Point-wise multiplication by another matrix""""""\n        return self.tocsr().multiply(other)\n\n    # TODO(unno): Implement nonzero\n\n    def power(self, n, dtype=None):\n        return self.tocsr().power(n, dtype=dtype)\n\n    def reshape(self, shape, order=\'C\'):\n        """"""Gives a new shape to a sparse matrix without changing its data.""""""\n        raise NotImplementedError\n\n    def set_shape(self, shape):\n        self.reshape(shape)\n\n    # TODO(unno): Implement setdiag\n\n    def sum(self, axis=None, dtype=None, out=None):\n        """"""Sums the matrix elements over a given axis.\n\n        Args:\n            axis (int or ``None``): Axis along which the sum is comuted.\n                If it is ``None``, it computes the sum of all the elements.\n                Select from ``{None, 0, 1, -2, -1}``.\n            dtype: The type of returned matrix. If it is not specified, type\n                of the array is used.\n            out (cupy.ndarray): Output matrix.\n\n        Returns:\n            cupy.ndarray: Summed array.\n\n        .. seealso::\n           :meth:`scipy.sparse.spmatrix.sum`\n\n        """"""\n        util.validateaxis(axis)\n\n        # This implementation uses multiplication, though it is not efficient\n        # for some matrix types. These should override this function.\n\n        m, n = self.shape\n\n        if axis is None:\n            return self.dot(cupy.ones(n, dtype=self.dtype)).sum(\n                dtype=dtype, out=out)\n\n        if axis < 0:\n            axis += 2\n\n        if axis == 0:\n            ret = self.T.dot(cupy.ones(m, dtype=self.dtype)).reshape(1, n)\n        else:  # axis == 1\n            ret = self.dot(cupy.ones(n, dtype=self.dtype)).reshape(m, 1)\n\n        if out is not None:\n            if out.shape != ret.shape:\n                raise ValueError(\'dimensions do not match\')\n            out[:] = ret\n            return out\n        elif dtype is not None:\n            return ret.astype(dtype, copy=False)\n        else:\n            return ret\n\n    def toarray(self, order=None, out=None):\n        """"""Return a dense ndarray representation of this matrix.""""""\n        return self.tocsr().toarray(order=order, out=out)\n\n    def tobsr(self, blocksize=None, copy=False):\n        """"""Convert this matrix to Block Sparse Row format.""""""\n        return self.tocsr(copy=copy).tobsr(copy=False)\n\n    def tocoo(self, copy=False):\n        """"""Convert this matrix to COOrdinate format.""""""\n        return self.tocsr(copy=copy).tocoo(copy=False)\n\n    def tocsc(self, copy=False):\n        """"""Convert this matrix to Compressed Sparse Column format.""""""\n        return self.tocsr(copy=copy).tocsc(copy=False)\n\n    def tocsr(self, copy=False):\n        """"""Convert this matrix to Compressed Sparse Row format.""""""\n        raise NotImplementedError\n\n    def todense(self, order=None, out=None):\n        """"""Return a dense matrix representation of this matrix.""""""\n        return self.toarray(order=order, out=out)\n\n    def todia(self, copy=False):\n        """"""Convert this matrix to sparse DIAgonal format.""""""\n        return self.tocsr(copy=copy).todia(copy=False)\n\n    def todok(self, copy=False):\n        """"""Convert this matrix to Dictionary Of Keys format.""""""\n        return self.tocsr(copy=copy).todok(copy=False)\n\n    def tolil(self, copy=False):\n        """"""Convert this matrix to LInked List format.""""""\n        return self.tocsr(copy=copy).tolil(copy=False)\n\n    def transpose(self, axes=None, copy=False):\n        """"""Reverses the dimensions of the sparse matrix.""""""\n        return self.tocsr(copy=copy).transpose(axes=axes, copy=False)\n\n\ndef issparse(x):\n    """"""Checks if a given matrix is a sparse matrix.\n\n    Returns:\n        bool: Returns if ``x`` is :class:`cupyx.scipy.sparse.spmatrix` that is\n            a base class of all sparse matrix classes.\n\n    """"""\n    return isinstance(x, spmatrix)\n\n\nisdense = util.isdense\nisspmatrix = issparse\n'"
cupyx/scipy/sparse/compressed.py,0,"b'import numpy\ntry:\n    import scipy.sparse\n    scipy_available = True\nexcept ImportError:\n    scipy_available = False\n\nimport cupy\nfrom cupy import core\nfrom cupy.creation import basic\nfrom cupy import cusparse\nfrom cupyx.scipy.sparse import base\nfrom cupyx.scipy.sparse import data as sparse_data\nfrom cupyx.scipy.sparse import util\n\n\nclass _compressed_sparse_matrix(sparse_data._data_matrix,\n                                sparse_data._minmax_mixin):\n\n    _compress_getitem_kern = core.ElementwiseKernel(\n        \'T d, S ind, int32 minor\', \'raw T answer\',\n        \'if (ind == minor) atomicAdd(&answer[0], d);\',\n        \'compress_getitem\')\n\n    _compress_getitem_complex_kern = core.ElementwiseKernel(\n        \'T real, T imag, S ind, int32 minor\',\n        \'raw T answer_real, raw T answer_imag\',\n        \'\'\'\n        if (ind == minor) {\n          atomicAdd(&answer_real[0], real);\n          atomicAdd(&answer_imag[0], imag);\n        }\n        \'\'\',\n        \'compress_getitem_complex\')\n\n    _max_reduction_kern = core.RawKernel(r\'\'\'\n        extern ""C"" __global__\n        void max_reduction(double* data, int* x, int* y, int length,\n                           double* z) {\n            // Get the index of the block\n            int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n            // Calculate the block length\n            int block_length = y[tid] - x[tid];\n\n            // Select initial value based on the block density\n            double running_value = 0;\n            if (block_length == length){\n                running_value = data[x[tid]];\n            } else {\n                running_value = 0;\n            }\n\n            // Iterate over the block and update\n            for (int entry = x[tid]; entry < y[tid]; entry++){\n                if (data[entry] != data[entry]){\n                    // Check for NaN\n                    running_value = nan("""");\n                    break;\n                } else {\n                    // Check for a value update\n                    if (data[entry] > running_value){\n                        running_value = data[entry];\n                    }\n                }\n            }\n\n            // Store in the return function\n            z[tid] = running_value;\n        }\n        \'\'\', \'max_reduction\')\n\n    _max_nonzero_reduction_kern = core.RawKernel(r\'\'\'\n        extern ""C"" __global__\n        void max_nonzero_reduction(double* data, int* x, int* y, int length,\n                                   double* z) {\n            // Get the index of the block\n            int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n            // Calculate the block length\n            int block_length = y[tid] - x[tid];\n\n            // Select initial value based on the block density\n            double running_value = 0;\n            if (block_length > 0){\n                running_value = data[x[tid]];\n            } else {\n                running_value = 0;\n            }\n\n            // Iterate over the section of the sparse matrix\n            for (int entry = x[tid]; entry < y[tid]; entry++){\n                if (data[entry] != data[entry]){\n                    // Check for NaN\n                    running_value = nan("""");\n                    break;\n                } else {\n                    // Check for a value update\n                    if (running_value < data[entry]){\n                        running_value = data[entry];\n                    }\n                }\n            }\n\n            // Store in the return function\n            z[tid] = running_value;\n        }\n        \'\'\', \'max_nonzero_reduction\')\n\n    _min_reduction_kern = core.RawKernel(r\'\'\'\n        extern ""C"" __global__\n        void min_reduction(double* data, int* x, int* y, int length,\n                           double* z) {\n            // Get the index of the block\n            int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n            // Calculate the block length\n            int block_length = y[tid] - x[tid];\n\n            // Select initial value based on the block density\n            double running_value = 0;\n            if (block_length == length){\n                running_value = data[x[tid]];\n            } else {\n                running_value = 0;\n            }\n\n            // Iterate over the block to update the initial value\n            for (int entry = x[tid]; entry < y[tid]; entry++){\n                if (data[entry] != data[entry]){\n                    // Check for NaN\n                    running_value = nan("""");\n                    break;\n                } else {\n                    // Check for a value update\n                    if (data[entry] < running_value){\n                        running_value = data[entry];\n                    }\n                }\n            }\n\n            // Store in the return function\n            z[tid] = running_value;\n        }\n        \'\'\', \'min_reduction\')\n\n    _min_nonzero_reduction_kern = core.RawKernel(r\'\'\'\n        extern ""C"" __global__\n        void min_nonzero_reduction(double* data, int* x, int* y, int length,\n                                   double* z) {\n            // Get the index of hte block\n            int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n            // Calculate the block length\n            int block_length = y[tid] - x[tid];\n\n            // Select initial value based on the block density\n            double running_value = 0;\n            if (block_length > 0){\n                running_value = data[x[tid]];\n            } else {\n                running_value = 0;\n            }\n\n            // Iterate over the section of the sparse matrix\n            for (int entry = x[tid]; entry < y[tid]; entry++){\n                if (data[entry] != data[entry]){\n                    // Check for NaN\n                    running_value = nan("""");\n                    break;\n                } else {\n                    // Check for a value update\n                    if (running_value > data[entry]){\n                        running_value = data[entry];\n                    }\n                }\n            }\n\n            // Store in the return function\n            z[tid] = running_value;\n        }\n        \'\'\', \'min_nonzero_reduction\')\n\n    _max_arg_reduction_kern = core.RawKernel(r\'\'\'\n        extern ""C"" __global__\n        void max_arg_reduction(double* data, int* indices, int* x, int* y,\n                               int length, long long* z) {\n            // Get the index of the block\n            int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n            // Calculate the block length\n            int block_length = y[tid] - x[tid];\n\n            // Select initial value based on the block density\n            int data_index = 0;\n            double data_value = 0;\n\n            if (block_length == length){\n                // Block is dense. Fill the first value\n                data_value = data[x[tid]];\n                data_index = indices[x[tid]];\n            } else if (block_length > 0)  {\n                // Block has at least one zero. Assign first occurrence as the\n                // starting reference\n                data_value = 0;\n                for (data_index = 0; data_index < length; data_index++){\n                    if (data_index != indices[x[tid] + data_index] ||\n                        x[tid] + data_index >= y[tid]){\n                        break;\n                    }\n                }\n            } else {\n                 // Zero valued array\n                data_value = 0;\n                data_index = 0;\n            }\n\n            // Iterate over the section of the sparse matrix\n            for (int entry = x[tid]; entry < y[tid]; entry++){\n                if (data[entry] != data[entry]){\n                    // Check for NaN\n                    data_value = nan("""");\n                    data_index = 0;\n                    break;\n                } else {\n                    // Check for a value update\n                    if (data[entry] > data_value){\n                        data_index = indices[entry];\n                        data_value = data[entry];\n                    }\n                }\n            }\n\n            // Store in the return function\n            z[tid] = data_index;\n        }\n        \'\'\', \'max_arg_reduction\')\n\n    _min_arg_reduction_kern = core.RawKernel(r\'\'\'\n        extern ""C"" __global__\n        void min_arg_reduction(double* data, int* indices, int* x, int* y,\n                               int length, long long* z) {\n            // Get the index of hte block\n            int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n            // Calculate the block length\n            int block_length = y[tid] - x[tid];\n\n            // Select initial value based on the block density\n            int data_index = 0;\n            double data_value = 0;\n\n            if (block_length == length){\n                // Block is dense. Fill the first value\n                data_value = data[x[tid]];\n                data_index = indices[x[tid]];\n            } else if (block_length > 0)  {\n                // Block has at least one zero. Assign first occurrence as the\n                // starting reference\n                data_value = 0;\n                for (data_index = 0; data_index < length; data_index++){\n                    if (data_index != indices[x[tid] + data_index] ||\n                        x[tid] + data_index >= y[tid]){\n                        break;\n                    }\n                }\n            } else {\n                // Zero valued array\n                data_value = 0;\n                data_index = 0;\n            }\n\n            // Iterate over the section of the sparse matrix\n            for (int entry = x[tid]; entry < y[tid]; entry++){\n                if (data[entry] != data[entry]){\n                    // Check for NaN\n                    data_value = nan("""");\n                    data_index = 0;\n                    break;\n                } else {\n                    // Check for a value update\n                    if (data[entry] < data_value){\n                        data_index = indices[entry];\n                        data_value = data[entry];\n                    }\n                }\n            }\n\n            // Store in the return function\n            z[tid] = data_index;\n\n        }\n        \'\'\', \'min_arg_reduction\')\n\n    def __init__(self, arg1, shape=None, dtype=None, copy=False):\n        if shape is not None:\n            if not util.isshape(shape):\n                raise ValueError(\'invalid shape (must be a 2-tuple of int)\')\n            shape = int(shape[0]), int(shape[1])\n\n        if base.issparse(arg1):\n            x = arg1.asformat(self.format)\n            data = x.data\n            indices = x.indices\n            indptr = x.indptr\n\n            if arg1.format != self.format:\n                # When formats are differnent, all arrays are already copied\n                copy = False\n\n            if shape is None:\n                shape = arg1.shape\n\n            has_canonical_format = x.has_canonical_format\n        elif util.isshape(arg1):\n            m, n = arg1\n            m, n = int(m), int(n)\n            data = basic.zeros(0, dtype if dtype else \'d\')\n            indices = basic.zeros(0, \'i\')\n            indptr = basic.zeros(self._swap(m, n)[0] + 1, dtype=\'i\')\n            # shape and copy argument is ignored\n            shape = (m, n)\n            copy = False\n            has_canonical_format = True\n\n        elif scipy_available and scipy.sparse.issparse(arg1):\n            # Convert scipy.sparse to cupyx.scipy.sparse\n            x = arg1.asformat(self.format)\n            data = cupy.array(x.data)\n            indices = cupy.array(x.indices, dtype=\'i\')\n            indptr = cupy.array(x.indptr, dtype=\'i\')\n            copy = False\n\n            if shape is None:\n                shape = arg1.shape\n            has_canonical_format = x.has_canonical_format\n\n        elif isinstance(arg1, tuple) and len(arg1) == 3:\n            data, indices, indptr = arg1\n            if not (base.isdense(data) and data.ndim == 1 and\n                    base.isdense(indices) and indices.ndim == 1 and\n                    base.isdense(indptr) and indptr.ndim == 1):\n                raise ValueError(\n                    \'data, indices, and indptr should be 1-D\')\n\n            if len(data) != len(indices):\n                raise ValueError(\'indices and data should have the same size\')\n\n            has_canonical_format = False\n\n        elif base.isdense(arg1):\n            if arg1.ndim > 2:\n                raise TypeError(\'expected dimension <= 2 array or matrix\')\n            elif arg1.ndim == 1:\n                arg1 = arg1[None]\n            elif arg1.ndim == 0:\n                arg1 = arg1[None, None]\n            data, indices, indptr = self._convert_dense(arg1)\n            copy = False\n            if shape is None:\n                shape = arg1.shape\n\n            has_canonical_format = True\n\n        else:\n            raise ValueError(\n                \'Unsupported initializer format\')\n\n        if dtype is None:\n            dtype = data.dtype\n        else:\n            dtype = numpy.dtype(dtype)\n\n        if dtype != \'f\' and dtype != \'d\' and dtype != \'F\' and dtype != \'D\':\n            raise ValueError(\n                \'Only float32, float64, complex64 and complex128 \'\n                \'are supported\')\n\n        data = data.astype(dtype, copy=copy)\n        sparse_data._data_matrix.__init__(self, data)\n\n        self.indices = indices.astype(\'i\', copy=copy)\n        self.indptr = indptr.astype(\'i\', copy=copy)\n\n        if shape is None:\n            shape = self._swap(len(indptr) - 1, int(indices.max()) + 1)\n\n        major, minor = self._swap(*shape)\n        if len(indptr) != major + 1:\n            raise ValueError(\'index pointer size (%d) should be (%d)\'\n                             % (len(indptr), major + 1))\n\n        self._descr = cusparse.MatDescriptor.create()\n        self._shape = shape\n        self._has_canonical_format = has_canonical_format\n\n    def _with_data(self, data, copy=True):\n        if copy:\n            return self.__class__(\n                (data, self.indices.copy(), self.indptr.copy()),\n                shape=self.shape,\n                dtype=data.dtype)\n        else:\n            return self.__class__(\n                (data, self.indices, self.indptr),\n                shape=self.shape,\n                dtype=data.dtype)\n\n    def _convert_dense(self, x):\n        raise NotImplementedError\n\n    def _swap(self, x, y):\n        raise NotImplementedError\n\n    def _add_sparse(self, other, alpha, beta):\n        raise NotImplementedError\n\n    def _add(self, other, lhs_negative, rhs_negative):\n        if cupy.isscalar(other):\n            if other == 0:\n                if lhs_negative:\n                    return -self\n                else:\n                    return self.copy()\n            else:\n                raise NotImplementedError(\n                    \'adding a nonzero scalar to a sparse matrix is not \'\n                    \'supported\')\n        elif base.isspmatrix(other):\n            alpha = -1 if lhs_negative else 1\n            beta = -1 if rhs_negative else 1\n            return self._add_sparse(other, alpha, beta)\n        elif base.isdense(other):\n            if lhs_negative:\n                if rhs_negative:\n                    return -self.todense() - other\n                else:\n                    return other - self.todense()\n            else:\n                if rhs_negative:\n                    return self.todense() - other\n                else:\n                    return self.todense() + other\n        else:\n            return NotImplemented\n\n    def __add__(self, other):\n        return self._add(other, False, False)\n\n    def __radd__(self, other):\n        return self._add(other, False, False)\n\n    def __sub__(self, other):\n        return self._add(other, False, True)\n\n    def __rsub__(self, other):\n        return self._add(other, True, False)\n\n    def __getitem__(self, slices):\n        if isinstance(slices, tuple):\n            slices = list(slices)\n        elif isinstance(slices, list):\n            slices = list(slices)\n            if all([isinstance(s, int) for s in slices]):\n                slices = [slices]\n        else:\n            slices = [slices]\n\n        ellipsis = -1\n        n_ellipsis = 0\n        for i, s in enumerate(slices):\n            if s is None:\n                raise IndexError(\'newaxis is not supported\')\n            elif s is Ellipsis:\n                ellipsis = i\n                n_ellipsis += 1\n        if n_ellipsis > 0:\n            ellipsis_size = self.ndim - (len(slices) - 1)\n            slices[ellipsis:ellipsis + 1] = [slice(None)] * ellipsis_size\n\n        if len(slices) == 2:\n            row, col = slices\n        elif len(slices) == 1:\n            row, col = slices[0], slice(None)\n        else:\n            raise IndexError(\'invalid number of indices\')\n\n        major, minor = self._swap(row, col)\n        major_size, minor_size = self._swap(*self._shape)\n        if numpy.isscalar(major):\n            i = int(major)\n            if i < 0:\n                i += major_size\n            if not (0 <= i < major_size):\n                raise IndexError(\'index out of bounds\')\n            if numpy.isscalar(minor):\n                j = int(minor)\n                if j < 0:\n                    j += minor_size\n                if not (0 <= j < minor_size):\n                    raise IndexError(\'index out of bounds\')\n                return self._get_single(i, j)\n            elif minor == slice(None):\n                return self._get_major_slice(slice(i, i + 1))\n        elif isinstance(major, slice):\n            if minor == slice(None):\n                return self._get_major_slice(major)\n\n        raise ValueError(\'unsupported indexing\')\n\n    def _get_single(self, major, minor):\n        start = self.indptr[major]\n        end = self.indptr[major + 1]\n        answer = cupy.zeros((), self.dtype)\n        data = self.data[start:end]\n        indices = self.indices[start:end]\n        if self.dtype.kind == \'c\':\n            self._compress_getitem_complex_kern(\n                data.real, data.imag, indices, minor, answer.real, answer.imag)\n        else:\n            self._compress_getitem_kern(\n                data, indices, minor, answer)\n        return answer[()]\n\n    def _get_major_slice(self, major):\n        major_size, minor_size = self._swap(*self._shape)\n        # major.indices cannot be used because scipy.sparse behaves differently\n        major_start = major.start\n        major_stop = major.stop\n        major_step = major.step\n        if major_start is None:\n            major_start = 0\n        if major_stop is None:\n            major_stop = major_size\n        if major_step is None:\n            major_step = 1\n        if major_start < 0:\n            major_start += major_size\n        if major_stop < 0:\n            major_stop += major_size\n        major_start = max(min(major_start, major_size), 0)\n        major_stop = max(min(major_stop, major_size), 0)\n\n        if major_step != 1:\n            raise ValueError(\'slicing with step != 1 not supported\')\n\n        if not (major_start <= major_stop):\n            # will give an empty slice, but preserve shape on the other axis\n            major_start = major_stop\n\n        start = self.indptr[major_start]\n        stop = self.indptr[major_stop]\n        data = self.data[start:stop]\n        indptr = self.indptr[major_start:major_stop + 1] - start\n        indices = self.indices[start:stop]\n\n        shape = self._swap(len(indptr) - 1, minor_size)\n        return self.__class__(\n            (data, indices, indptr), shape=shape, dtype=self.dtype, copy=False)\n\n    @property\n    def has_canonical_format(self):\n        return self._has_canonical_format\n\n    def get_shape(self):\n        """"""Returns the shape of the matrix.\n\n        Returns:\n            tuple: Shape of the matrix.\n\n        """"""\n        return self._shape\n\n    def getnnz(self, axis=None):\n        """"""Returns the number of stored values, including explicit zeros.\n\n        Args:\n            axis: Not supported yet.\n\n        Returns:\n            int: The number of stored values.\n\n        """"""\n        if axis is None:\n            return self.data.size\n        else:\n            raise ValueError\n\n    # TODO(unno): Implement sorted_indices\n\n    def sum_duplicates(self):\n        if self._has_canonical_format:\n            return\n        if self.data.size == 0:\n            self._has_canonical_format = True\n            return\n        coo = self.tocoo()\n        coo.sum_duplicates()\n        self.__init__(coo.asformat(self.format))\n        self._has_canonical_format = True\n\n    #####################\n    # Reduce operations #\n    #####################\n\n    def _minor_reduce(self, ufunc, axis, nonzero):\n        """"""Reduce nonzeros with a ufunc over the minor axis when non-empty\n\n        Can be applied to a function of self.data by supplying data parameter.\n        Warning: this does not call sum_duplicates()\n\n        Args:\n            ufunc (object): Function handle giving the operation to be\n                conducted.\n            axis (int): Matrix over which the reduction should be\n                conducted.\n\n        Returns:\n            (cupy.ndarray): Reduce result for nonzeros in each\n                major_index.\n\n        """"""\n\n        # Call to the appropriate kernel function\n        if axis == 1:\n            # Create the vector to hold output\n            value = cupy.zeros(self.shape[0]).astype(cupy.float64)\n\n            if nonzero:\n                # Perform the calculation\n                if ufunc == cupy.amax:\n                    self._max_nonzero_reduction_kern(\n                        (self.shape[0],), (1,),\n                        (self.data.astype(cupy.float64),\n                         self.indptr[:len(self.indptr) - 1],\n                         self.indptr[1:], cupy.int64(self.shape[1]),\n                         value))\n                if ufunc == cupy.amin:\n                    self._min_nonzero_reduction_kern(\n                        (self.shape[0],), (1,),\n                        (self.data.astype(cupy.float64),\n                         self.indptr[:len(self.indptr) - 1],\n                         self.indptr[1:], cupy.int64(self.shape[1]),\n                         value))\n\n            else:\n                # Perform the calculation\n                if ufunc == cupy.amax:\n                    self._max_reduction_kern(\n                        (self.shape[0],), (1,),\n                        (self.data.astype(cupy.float64),\n                         self.indptr[:len(self.indptr) - 1],\n                         self.indptr[1:], cupy.int64(self.shape[1]),\n                         value))\n                if ufunc == cupy.amin:\n                    self._min_reduction_kern(\n                        (self.shape[0],), (1,),\n                        (self.data.astype(cupy.float64),\n                         self.indptr[:len(self.indptr) - 1],\n                         self.indptr[1:], cupy.int64(self.shape[1]),\n                         value))\n\n        if axis == 0:\n            # Create the vector to hold output\n            value = cupy.zeros(self.shape[1]).astype(cupy.float64)\n\n            if nonzero:\n                # Perform the calculation\n                if ufunc == cupy.amax:\n                    self._max_nonzero_reduction_kern(\n                        (self.shape[1],), (1,),\n                        (self.data.astype(cupy.float64),\n                         self.indptr[:len(self.indptr) - 1],\n                         self.indptr[1:], cupy.int64(self.shape[0]),\n                         value))\n                if ufunc == cupy.amin:\n                    self._min_nonzero_reduction_kern(\n                        (self.shape[1],), (1,),\n                        (self.data.astype(cupy.float64),\n                         self.indptr[:len(self.indptr) - 1],\n                         self.indptr[1:], cupy.int64(self.shape[0]),\n                         value))\n            else:\n                # Perform the calculation\n                if ufunc == cupy.amax:\n                    self._max_reduction_kern(\n                        (self.shape[1],), (1,),\n                        (self.data.astype(cupy.float64),\n                         self.indptr[:len(self.indptr) - 1],\n                         self.indptr[1:], cupy.int64(self.shape[0]),\n                         value))\n                if ufunc == cupy.amin:\n                    self._min_reduction_kern(\n                        (self.shape[1],), (1,),\n                        (self.data.astype(cupy.float64),\n                         self.indptr[:len(self.indptr) - 1],\n                         self.indptr[1:], cupy.int64(self.shape[0]),\n                         value))\n\n        return value\n\n    def _arg_minor_reduce(self, ufunc, axis):\n        """"""Reduce nonzeros with a ufunc over the minor axis when non-empty\n\n        Can be applied to a function of self.data by supplying data parameter.\n        Warning: this does not call sum_duplicates()\n\n        Args:\n            ufunc (object): Function handle giving the operation to be\n                conducted.\n            axis (int): Maxtrix over which the reduction should be conducted\n\n        Returns:\n            (cupy.ndarray): Reduce result for nonzeros in each\n                major_index\n\n        """"""\n\n        # Call to the appropriate kernel function\n        if axis == 1:\n            # Create the vector to hold output\n            value = cupy.zeros(self.shape[0]).astype(cupy.int64)\n\n            # Perform the calculation\n            if ufunc == cupy.argmax:\n                self._max_arg_reduction_kern(\n                    (self.shape[0],), (1,),\n                    (self.data.astype(cupy.float64), self.indices,\n                     self.indptr[:len(self.indptr) - 1],\n                     self.indptr[1:], cupy.int64(self.shape[1]),\n                     value))\n            if ufunc == cupy.argmin:\n                self._min_arg_reduction_kern(\n                    (self.shape[0],), (1,),\n                    (self.data.astype(cupy.float64), self.indices,\n                     self.indptr[:len(self.indptr) - 1],\n                     self.indptr[1:], cupy.int64(self.shape[1]),\n                     value))\n\n        if axis == 0:\n            # Create the vector to hold output\n            value = cupy.zeros(self.shape[1]).astype(cupy.int64)\n\n            # Perform the calculation\n            if ufunc == cupy.argmax:\n                self._max_arg_reduction_kern(\n                    (self.shape[1],), (1,),\n                    (self.data.astype(cupy.float64), self.indices,\n                     self.indptr[:len(self.indptr) - 1],\n                     self.indptr[1:], cupy.int64(self.shape[0]),\n                     value))\n            if ufunc == cupy.argmin:\n                self._min_arg_reduction_kern(\n                    (self.shape[1],), (1,),\n                    (self.data.astype(cupy.float64), self.indices,\n                     self.indptr[:len(self.indptr) - 1],\n                     self.indptr[1:],\n                     cupy.int64(self.shape[0]), value))\n\n        return value\n'"
cupyx/scipy/sparse/construct.py,0,"b'import numpy\nimport cupy\nfrom cupyx.scipy.sparse import coo\nfrom cupyx.scipy.sparse import csc\nfrom cupyx.scipy.sparse import csr\nfrom cupyx.scipy.sparse import dia\nfrom cupyx.scipy.sparse import sputils\n\n\ndef eye(m, n=None, k=0, dtype=\'d\', format=None):\n    """"""Creates a sparse matrix with ones on diagonal.\n\n    Args:\n        m (int): Number of rows.\n        n (int or None): Number of columns. If it is ``None``,\n            it makes a square matrix.\n        k (int): Diagonal to place ones on.\n        dtype: Type of a matrix to create.\n        format (str or None): Format of the result, e.g. ``format=""csr""``.\n\n    Returns:\n        cupyx.scipy.sparse.spmatrix: Created sparse matrix.\n\n    .. seealso:: :func:`scipy.sparse.eye`\n\n    """"""\n    if n is None:\n        n = m\n    m, n = int(m), int(n)\n\n    if m == n and k == 0:\n        if format in [\'csr\', \'csc\']:\n            indptr = cupy.arange(n + 1, dtype=\'i\')\n            indices = cupy.arange(n, dtype=\'i\')\n            data = cupy.ones(n, dtype=dtype)\n            if format == \'csr\':\n                cls = csr.csr_matrix\n            else:\n                cls = csc.csc_matrix\n            return cls((data, indices, indptr), (n, n))\n\n        elif format == \'coo\':\n            row = cupy.arange(n, dtype=\'i\')\n            col = cupy.arange(n, dtype=\'i\')\n            data = cupy.ones(n, dtype=dtype)\n            return coo.coo_matrix((data, (row, col)), (n, n))\n\n    diags = cupy.ones((1, max(0, min(m + k, n))), dtype=dtype)\n    return spdiags(diags, k, m, n).asformat(format)\n\n\ndef identity(n, dtype=\'d\', format=None):\n    """"""Creates an identity matrix in sparse format.\n\n    .. note::\n       Currently it only supports csr, csc and coo formats.\n\n    Args:\n        n (int): Number of rows and columns.\n        dtype: Type of a matrix to create.\n        format (str or None): Format of the result, e.g. ``format=""csr""``.\n\n    Returns:\n        cupyx.scipy.sparse.spmatrix: Created identity matrix.\n\n    .. seealso:: :func:`scipy.sparse.identity`\n\n    """"""\n    return eye(n, n, dtype=dtype, format=format)\n\n\ndef spdiags(data, diags, m, n, format=None):\n    """"""Creates a sparse matrix from diagonals.\n\n    Args:\n        data (cupy.ndarray): Matrix diagonals stored row-wise.\n        diags (cupy.ndarray): Diagonals to set.\n        m (int): Number of rows.\n        n (int): Number of cols.\n        format (str or None): Sparse format, e.g. ``format=""csr""``.\n\n    Returns:\n        cupyx.scipy.sparse.spmatrix: Created sparse matrix.\n\n    .. seealso:: :func:`scipy.sparse.spdiags`\n\n    """"""\n    return dia.dia_matrix((data, diags), shape=(m, n)).asformat(format)\n\n\ndef _compressed_sparse_stack(blocks, axis):\n    """"""Fast path for stacking CSR/CSC matrices\n    (i) vstack for CSR, (ii) hstack for CSC.\n    """"""\n    other_axis = 1 if axis == 0 else 0\n    data = cupy.concatenate([b.data for b in blocks])\n    constant_dim = blocks[0].shape[other_axis]\n    idx_dtype = sputils.get_index_dtype(arrays=[b.indptr for b in blocks],\n                                        maxval=max(data.size, constant_dim))\n    indices = cupy.empty(data.size, dtype=idx_dtype)\n    indptr = cupy.empty(sum(b.shape[axis]\n                            for b in blocks) + 1, dtype=idx_dtype)\n    last_indptr = idx_dtype(0)\n    sum_dim = 0\n    sum_indices = 0\n    for b in blocks:\n        if b.shape[other_axis] != constant_dim:\n            raise ValueError(\n                \'incompatible dimensions for axis %d\' % other_axis)\n        indices[sum_indices:sum_indices+b.indices.size] = b.indices\n        sum_indices += b.indices.size\n        idxs = slice(sum_dim, sum_dim + b.shape[axis])\n        indptr[idxs] = b.indptr[:-1]\n        indptr[idxs] += last_indptr\n        sum_dim += b.shape[axis]\n        last_indptr += b.indptr[-1]\n    indptr[-1] = last_indptr\n    if axis == 0:\n        return csr.csr_matrix((data, indices, indptr),\n                              shape=(sum_dim, constant_dim))\n    else:\n        return csc.csc_matrix((data, indices, indptr),\n                              shape=(constant_dim, sum_dim))\n\n\ndef hstack(blocks, format=None, dtype=None):\n    """"""Stacks sparse matrices horizontally (column wise)\n\n    Args:\n        blocks (sequence of cupyx.scipy.sparse.spmatrix):\n            sparse matrices to stack\n\n        format (str):\n            sparse format of the result (e.g. ""csr"")\n            by default an appropriate sparse matrix format is returned.\n            This choice is subject to change.\n        dtype (dtype, optional):\n            The data-type of the output matrix.  If not given, the dtype is\n            determined from that of ``blocks``.\n\n    Returns:\n        cupyx.scipy.sparse.spmatrix: the stacked sparse matrix\n\n    .. seealso:: :func:`scipy.sparse.hstack`\n\n    Examples:\n        >>> from cupy import array\n        >>> from cupy.sparse import csr_matrix, hstack\n        >>> A = csr_matrix(array([[1., 2.], [3., 4.]]))\n        >>> B = csr_matrix(array([[5.], [6.]]))\n        >>> hstack([A, B]).toarray()\n        array([[1., 2., 5.],\n               [3., 4., 6.]])\n    """"""\n    return bmat([blocks], format=format, dtype=dtype)\n\n\ndef vstack(blocks, format=None, dtype=None):\n    """"""Stacks sparse matrices vertically (row wise)\n\n    Args:\n        blocks (sequence of cupyx.scipy.sparse.spmatrix)\n            sparse matrices to stack\n        format (str, optional):\n            sparse format of the result (e.g. ""csr"")\n            by default an appropriate sparse matrix format is returned.\n            This choice is subject to change.\n        dtype (dtype, optional):\n            The data-type of the output matrix.  If not given, the dtype is\n            determined from that of `blocks`.\n\n    Returns:\n        cupyx.scipy.sparse.spmatrix: the stacked sparse matrix\n\n    .. seealso:: :func:`scipy.sparse.vstack`\n\n    Examples:\n        >>> from cupy import array\n        >>> from cupy.sparse import csr_matrix, vstack\n        >>> A = csr_matrix(array([[1., 2.], [3., 4.]]))\n        >>> B = csr_matrix(array([[5., 6.]]))\n        >>> vstack([A, B]).toarray()\n        array([[1., 2.],\n               [3., 4.],\n               [5., 6.]])\n    """"""\n    return bmat([[b] for b in blocks], format=format, dtype=dtype)\n\n\ndef bmat(blocks, format=None, dtype=None):\n    """"""Builds a sparse matrix from sparse sub-blocks\n\n    Args:\n        blocks (array_like):\n            Grid of sparse matrices with compatible shapes.\n            An entry of None implies an all-zero matrix.\n        format ({\'bsr\', \'coo\', \'csc\', \'csr\', \'dia\', \'dok\', \'lil\'}, optional):\n            The sparse format of the result (e.g. ""csr"").  By default an\n            appropriate sparse matrix format is returned.\n            This choice is subject to change.\n        dtype (dtype, optional):\n            The data-type of the output matrix.  If not given, the dtype is\n            determined from that of `blocks`.\n    Returns:\n        bmat (sparse matrix)\n\n    .. seealso:: :func:`scipy.sparse.bmat`\n\n    Examples:\n        >>> from cupy import array\n        >>> from cupy.sparse import csr_matrix, bmat\n        >>> A = csr_matrix(array([[1., 2.], [3., 4.]]))\n        >>> B = csr_matrix(array([[5.], [6.]]))\n        >>> C = csr_matrix(array([[7.]]))\n        >>> bmat([[A, B], [None, C]]).toarray()\n        array([[1., 2., 5.],\n               [3., 4., 6.],\n               [0., 0., 7.]])\n        >>> bmat([[A, None], [None, C]]).toarray()\n        array([[1., 2., 0.],\n               [3., 4., 0.],\n               [0., 0., 7.]])\n\n    """"""\n\n    # We assume here that blocks will be 2-D so we need to look, at most,\n    # 2 layers deep for the shape\n    # TODO(Corey J. Nolet): Check this assumption and raise ValueError\n\n    # NOTE: We can\'t follow scipy exactly here\n    # since we don\'t have an `object` datatype\n    M = len(blocks)\n    N = len(blocks[0])\n\n    blocks_flat = []\n    for m in range(M):\n        for n in range(N):\n            if blocks[m][n] is not None:\n                blocks_flat.append(blocks[m][n])\n\n    if len(blocks_flat) == 0:\n        return coo.coo_matrix((0, 0), dtype=dtype)\n\n    # check for fast path cases\n    if (N == 1 and format in (None, \'csr\') and\n            all(isinstance(b, csr.csr_matrix)\n                for b in blocks_flat)):\n        A = _compressed_sparse_stack(blocks_flat, 0)\n        if dtype is not None:\n            A = A.astype(dtype)\n        return A\n    elif (M == 1 and format in (None, \'csc\')\n          and all(isinstance(b, csc.csc_matrix) for b in blocks_flat)):\n        A = _compressed_sparse_stack(blocks_flat, 1)\n        if dtype is not None:\n            A = A.astype(dtype)\n        return A\n\n    block_mask = numpy.zeros((M, N), dtype=bool)\n    brow_lengths = numpy.zeros(M+1, dtype=numpy.int64)\n    bcol_lengths = numpy.zeros(N+1, dtype=numpy.int64)\n\n    # convert everything to COO format\n    for i in range(M):\n        for j in range(N):\n            if blocks[i][j] is not None:\n                A = coo.coo_matrix(blocks[i][j])\n                blocks[i][j] = A\n                block_mask[i][j] = True\n\n                if brow_lengths[i+1] == 0:\n                    brow_lengths[i+1] = A.shape[0]\n                elif brow_lengths[i+1] != A.shape[0]:\n                    msg = (\'blocks[{i},:] has incompatible row dimensions. \'\n                           \'Got blocks[{i},{j}].shape[0] == {got}, \'\n                           \'expected {exp}.\'.format(i=i, j=j,\n                                                    exp=brow_lengths[i+1],\n                                                    got=A.shape[0]))\n                    raise ValueError(msg)\n\n                if bcol_lengths[j+1] == 0:\n                    bcol_lengths[j+1] = A.shape[1]\n                elif bcol_lengths[j+1] != A.shape[1]:\n                    msg = (\'blocks[:,{j}] has incompatible row dimensions. \'\n                           \'Got blocks[{i},{j}].shape[1] == {got}, \'\n                           \'expected {exp}.\'.format(i=i, j=j,\n                                                    exp=bcol_lengths[j+1],\n                                                    got=A.shape[1]))\n                    raise ValueError(msg)\n\n    nnz = sum(block.nnz for block in blocks_flat)\n    if dtype is None:\n        all_dtypes = [blk.dtype for blk in blocks_flat]\n        dtype = sputils.upcast(*all_dtypes) if all_dtypes else None\n\n    row_offsets = numpy.cumsum(brow_lengths)\n    col_offsets = numpy.cumsum(bcol_lengths)\n\n    shape = (row_offsets[-1], col_offsets[-1])\n\n    data = cupy.empty(nnz, dtype=dtype)\n    idx_dtype = sputils.get_index_dtype(maxval=max(shape))\n    row = cupy.empty(nnz, dtype=idx_dtype)\n    col = cupy.empty(nnz, dtype=idx_dtype)\n\n    nnz = 0\n    ii, jj = numpy.nonzero(block_mask)\n    for i, j in zip(ii, jj):\n        B = blocks[int(i)][int(j)]\n        idx = slice(nnz, nnz + B.nnz)\n        data[idx] = B.data\n        row[idx] = B.row + row_offsets[i]\n        col[idx] = B.col + col_offsets[j]\n        nnz += B.nnz\n\n    return coo.coo_matrix((data, (row, col)), shape=shape).asformat(format)\n\n\ndef random(m, n, density=0.01, format=\'coo\', dtype=None,\n           random_state=None, data_rvs=None):\n    """"""Generates a random sparse matrix.\n\n    This function generates a random sparse matrix. First it selects non-zero\n    elements with given density ``density`` from ``(m, n)`` elements.\n    So the number of non-zero elements ``k`` is ``k = m * n * density``.\n    Value of each element is selected with ``data_rvs`` function.\n\n    Args:\n        m (int): Number of rows.\n        n (int): Number of cols.\n        density (float): Ratio of non-zero entries.\n        format (str): Matrix format.\n        dtype (~cupy.dtype): Type of the returned matrix values.\n        random_state (cupy.random.RandomState or int):\n            State of random number generator.\n            If an integer is given, the method makes a new state for random\n            number generator and uses it.\n            If it is not given, the default state is used.\n            This state is used to generate random indexes for nonzero entries.\n        data_rvs (callable): A function to generate data for a random matrix.\n            If it is not given, `random_state.rand` is used.\n\n    Returns:\n        cupyx.scipy.sparse.spmatrix: Generated matrix.\n\n    .. seealso:: :func:`scipy.sparse.random`\n\n    """"""\n    if density < 0 or density > 1:\n        raise ValueError(\'density expected to be 0 <= density <= 1\')\n    dtype = cupy.dtype(dtype)\n    if dtype.char not in \'fd\':\n        raise NotImplementedError(\'type %s not supported\' % dtype)\n\n    mn = m * n\n\n    k = int(density * m * n)\n\n    if random_state is None:\n        random_state = cupy.random\n    elif isinstance(random_state, (int, cupy.integer)):\n        random_state = cupy.random.RandomState(random_state)\n\n    if data_rvs is None:\n        data_rvs = random_state.rand\n\n    ind = random_state.choice(mn, size=k, replace=False)\n    j = cupy.floor(ind * (1. / m)).astype(\'i\')\n    i = ind - j * m\n    vals = data_rvs(k).astype(dtype)\n    return coo.coo_matrix(\n        (vals, (i, j)), shape=(m, n)).asformat(format)\n\n\ndef rand(m, n, density=0.01, format=\'coo\', dtype=None, random_state=None):\n    """"""Generates a random sparse matrix.\n\n    See :func:`cupyx.scipy.sparse.random` for detail.\n\n    Args:\n        m (int): Number of rows.\n        n (int): Number of cols.\n        density (float): Ratio of non-zero entries.\n        format (str): Matrix format.\n        dtype (~cupy.dtype): Type of the returned matrix values.\n        random_state (cupy.random.RandomState or int):\n            State of random number generator.\n            If an integer is given, the method makes a new state for random\n            number generator and uses it.\n            If it is not given, the default state is used.\n            This state is used to generate random indexes for nonzero entries.\n\n    Returns:\n        cupyx.scipy.sparse.spmatrix: Generated matrix.\n\n    .. seealso:: :func:`scipy.sparse.rand`\n    .. seealso:: :func:`cupyx.scipy.sparse.random`\n\n    """"""\n    return random(m, n, density, format, dtype, random_state)\n\n\ndef diags(diagonals, offsets=0, shape=None, format=None, dtype=None):\n    """"""Construct a sparse matrix from diagonals.\n\n    Args:\n        diagonals (sequence of array_like):\n            Sequence of arrays containing the matrix diagonals, corresponding\n            to `offsets`.\n        offsets (sequence of int or an int):\n            Diagonals to set:\n                - k = 0  the main diagonal (default)\n                - k > 0  the k-th upper diagonal\n                - k < 0  the k-th lower diagonal\n        shape (tuple of int):\n            Shape of the result. If omitted, a square matrix large enough\n            to contain the diagonals is returned.\n        format ({""dia"", ""csr"", ""csc"", ""lil"", ...}):\n            Matrix format of the result.  By default (format=None) an\n            appropriate sparse matrix format is returned.  This choice is\n            subject to change.\n        dtype (dtype): Data type of the matrix.\n\n    Returns:\n        cupyx.scipy.sparse.spmatrix: Generated matrix.\n\n    Notes:\n        This function differs from `spdiags` in the way it handles\n        off-diagonals.\n\n        The result from `diags` is the sparse equivalent of::\n\n            cupy.diag(diagonals[0], offsets[0])\n            + ...\n            + cupy.diag(diagonals[k], offsets[k])\n\n        Repeated diagonal offsets are disallowed.\n    """"""\n    # if offsets is not a sequence, assume that there\'s only one diagonal\n    if sputils.isscalarlike(offsets):\n        # now check that there\'s actually only one diagonal\n        if len(diagonals) == 0 or sputils.isscalarlike(diagonals[0]):\n            diagonals = [cupy.atleast_1d(diagonals)]\n        else:\n            raise ValueError(\'Different number of diagonals and offsets.\')\n    else:\n        diagonals = list(map(cupy.atleast_1d, diagonals))\n\n    if isinstance(offsets, cupy.ndarray):\n        offsets = offsets.get()\n    offsets = numpy.atleast_1d(offsets)\n\n    # Basic check\n    if len(diagonals) != len(offsets):\n        raise ValueError(\'Different number of diagonals and offsets.\')\n\n    # Determine shape, if omitted\n    if shape is None:\n        m = len(diagonals[0]) + abs(int(offsets[0]))\n        shape = (m, m)\n\n    # Determine data type, if omitted\n    if dtype is None:\n        dtype = cupy.common_type(*diagonals)\n\n    # Construct data array\n    m, n = shape\n\n    M = max([min(m + offset, n - offset) + max(0, offset)\n             for offset in offsets])\n    M = max(0, M)\n    data_arr = cupy.zeros((len(offsets), M), dtype=dtype)\n\n    K = min(m, n)\n\n    for j, diagonal in enumerate(diagonals):\n        offset = offsets[j]\n        k = max(0, offset)\n        length = min(m + offset, n - offset, K)\n        if length < 0:\n            raise ValueError(\n                \'Offset %d (index %d) out of bounds\' % (offset, j))\n        try:\n            data_arr[j, k:k+length] = diagonal[..., :length]\n        except ValueError:\n            if len(diagonal) != length and len(diagonal) != 1:\n                raise ValueError(\n                    \'Diagonal length (index %d: %d at offset %d) does not \'\n                    \'agree with matrix size (%d, %d).\' % (\n                        j, len(diagonal), offset, m, n))\n            raise\n\n    return dia.dia_matrix((data_arr, offsets), shape=(m, n)).asformat(format)\n'"
cupyx/scipy/sparse/coo.py,0,"b'import numpy\ntry:\n    import scipy.sparse\n    _scipy_available = True\nexcept ImportError:\n    _scipy_available = False\n\nimport cupy\nfrom cupy import cusparse\nfrom cupyx.scipy.sparse import base\nfrom cupyx.scipy.sparse import csr\nfrom cupyx.scipy.sparse import data as sparse_data\nfrom cupyx.scipy.sparse import util\n\n\nclass coo_matrix(sparse_data._data_matrix):\n\n    """"""COOrdinate format sparse matrix.\n\n    Now it has only one initializer format below:\n\n    ``coo_matrix(S)``\n        ``S`` is another sparse matrix. It is equivalent to ``S.tocoo()``.\n\n    ``coo_matrix((M, N), [dtype])``\n        It constructs an empty matrix whose shape is ``(M, N)``. Default dtype\n        is float64.\n\n    ``coo_matrix((data, (row, col))``\n        All ``data``, ``row`` and ``col`` are one-dimenaional\n        :class:`cupy.ndarray`.\n\n    Args:\n        arg1: Arguments for the initializer.\n        shape (tuple): Shape of a matrix. Its length must be two.\n        dtype: Data type. It must be an argument of :class:`numpy.dtype`.\n        copy (bool): If ``True``, copies of given data are always used.\n\n    .. seealso::\n       :class:`scipy.sparse.coo_matrix`\n\n    """"""\n\n    format = \'coo\'\n\n    def __init__(self, arg1, shape=None, dtype=None, copy=False):\n        if shape is not None and len(shape) != 2:\n            raise ValueError(\n                \'Only two-dimensional sparse arrays are supported.\')\n\n        if base.issparse(arg1):\n            x = arg1.asformat(self.format)\n            data = x.data\n            row = x.row\n            col = x.col\n\n            if arg1.format != self.format:\n                # When formats are differnent, all arrays are already copied\n                copy = False\n\n            if shape is None:\n                shape = arg1.shape\n\n            has_canonical_format = x.has_canonical_format\n\n        elif util.isshape(arg1):\n            m, n = arg1\n            m, n = int(m), int(n)\n            data = cupy.zeros(0, dtype if dtype else \'d\')\n            row = cupy.zeros(0, dtype=\'i\')\n            col = cupy.zeros(0, dtype=\'i\')\n            # shape and copy argument is ignored\n            shape = (m, n)\n            copy = False\n            has_canonical_format = True\n\n        elif _scipy_available and scipy.sparse.issparse(arg1):\n            # Convert scipy.sparse to cupyx.scipy.sparse\n            x = arg1.tocoo()\n            data = cupy.array(x.data)\n            row = cupy.array(x.row, dtype=\'i\')\n            col = cupy.array(x.col, dtype=\'i\')\n            copy = False\n\n            if shape is None:\n                shape = arg1.shape\n            has_canonical_format = x.has_canonical_format\n\n        elif isinstance(arg1, tuple) and len(arg1) == 2:\n            try:\n                data, (row, col) = arg1\n            except (TypeError, ValueError):\n                raise TypeError(\'invalid input format\')\n\n            if not (base.isdense(data) and data.ndim == 1 and\n                    base.isdense(row) and row.ndim == 1 and\n                    base.isdense(col) and col.ndim == 1):\n                raise ValueError(\'row, column, and data arrays must be 1-D\')\n            if not (len(data) == len(row) == len(col)):\n                raise ValueError(\n                    \'row, column, and data array must all be the same length\')\n\n            has_canonical_format = False\n\n        else:\n            raise TypeError(\'invalid input format\')\n\n        if dtype is None:\n            dtype = data.dtype\n        else:\n            dtype = numpy.dtype(dtype)\n\n        if dtype != \'f\' and dtype != \'d\' and dtype != \'F\' and dtype != \'D\':\n            raise ValueError(\n                \'Only float32, float64, complex64 and complex128\'\n                \' are supported\')\n\n        data = data.astype(dtype, copy=copy)\n        row = row.astype(\'i\', copy=copy)\n        col = col.astype(\'i\', copy=copy)\n\n        if shape is None:\n            if len(row) == 0 or len(col) == 0:\n                raise ValueError(\n                    \'cannot infer dimensions from zero sized index arrays\')\n            shape = (int(row.max()) + 1, int(col.max()) + 1)\n\n        if len(data) > 0:\n            if row.max() >= shape[0]:\n                raise ValueError(\'row index exceeds matrix dimensions\')\n            if col.max() >= shape[1]:\n                raise ValueError(\'column index exceeds matrix dimensions\')\n            if row.min() < 0:\n                raise ValueError(\'negative row index found\')\n            if col.min() < 0:\n                raise ValueError(\'negative column index found\')\n\n        sparse_data._data_matrix.__init__(self, data)\n        self.row = row\n        self.col = col\n        if not util.isshape(shape):\n            raise ValueError(\'invalid shape (must be a 2-tuple of int)\')\n        self._shape = int(shape[0]), int(shape[1])\n        self._has_canonical_format = has_canonical_format\n\n    def _with_data(self, data, copy=True):\n        """"""Returns a matrix with the same sparsity structure as self,\n        but with different data.  By default the index arrays\n        (i.e. .row and .col) are copied.\n        """"""\n        if copy:\n            return coo_matrix(\n                (data, (self.row.copy(), self.col.copy())),\n                shape=self.shape, dtype=data.dtype)\n        else:\n            return coo_matrix(\n                (data, (self.row, self.col)), shape=self.shape,\n                dtype=data.dtype)\n\n    def eliminate_zeros(self):\n        """"""Removes zero entories in place.""""""\n        ind = self.data != 0\n        self.data = self.data[ind]\n        self.row = self.row[ind]\n        self.col = self.col[ind]\n\n    @property\n    def has_canonical_format(self):\n        return self._has_canonical_format\n\n    def get_shape(self):\n        """"""Returns the shape of the matrix.\n\n        Returns:\n            tuple: Shape of the matrix.\n        """"""\n        return self._shape\n\n    def getnnz(self, axis=None):\n        """"""Returns the number of stored values, including explicit zeros.""""""\n        if axis is None:\n            return self.data.size\n        else:\n            raise ValueError\n\n    def get(self, stream=None):\n        """"""Returns a copy of the array on host memory.\n\n        Args:\n            stream (cupy.cuda.Stream): CUDA stream object. If it is given, the\n                copy runs asynchronously. Otherwise, the copy is synchronous.\n\n        Returns:\n            scipy.sparse.coo_matrix: Copy of the array on host memory.\n\n        """"""\n        if not _scipy_available:\n            raise RuntimeError(\'scipy is not available\')\n\n        data = self.data.get(stream)\n        row = self.row.get(stream)\n        col = self.col.get(stream)\n        return scipy.sparse.coo_matrix(\n            (data, (row, col)), shape=self.shape)\n\n    def sum_duplicates(self):\n        """"""Eliminate duplicate matrix entries by adding them together.\n\n        .. seealso::\n           :meth:`scipy.sparse.coo_matrix.sum_duplicates`\n\n        """"""\n        if self._has_canonical_format:\n            return\n        if self.data.size == 0:\n            self._has_canonical_format = True\n            return\n        keys = cupy.stack([self.col, self.row])\n        order = cupy.lexsort(keys)\n        src_data = self.data[order]\n        src_row = self.row[order]\n        src_col = self.col[order]\n        diff = cupy.ElementwiseKernel(\n            \'raw int32 row, raw int32 col\',\n            \'int32 diff\',\n            \'\'\'\n            int index;\n            if (i == 0 || row[i - 1] == row[i] && col[i - 1] == col[i]) {\n              diff = 0;\n            } else {\n              diff = 1;\n            }\n            \'\'\',\n            \'sum_duplicates_diff\'\n        )(src_row, src_col, size=self.row.size)\n\n        if diff[1:].all():\n            # All elements have different indices.\n            data = src_data\n            row = src_row\n            col = src_col\n        else:\n            index = cupy.cumsum(diff, dtype=\'i\')\n            size = int(index[-1]) + 1\n            data = cupy.zeros(size, dtype=self.data.dtype)\n            row = cupy.empty(size, dtype=\'i\')\n            col = cupy.empty(size, dtype=\'i\')\n            if self.data.dtype.kind == \'f\':\n                cupy.ElementwiseKernel(\n                    \'T src_data, int32 src_row, int32 src_col, int32 index\',\n                    \'raw T data, raw int32 row, raw int32 col\',\n                    \'\'\'\n                    atomicAdd(&data[index], src_data);\n                    row[index] = src_row;\n                    col[index] = src_col;\n                    \'\'\',\n                    \'sum_duplicates_assign\'\n                )(src_data, src_row, src_col, index, data, row, col)\n            elif self.data.dtype.kind == \'c\':\n                cupy.ElementwiseKernel(\n                    \'T src_real, T src_imag, int32 src_row, int32 src_col, \'\n                    \'int32 index\',\n                    \'raw T real, raw T imag, raw int32 row, raw int32 col\',\n                    \'\'\'\n                    atomicAdd(&real[index], src_real);\n                    atomicAdd(&imag[index], src_imag);\n                    row[index] = src_row;\n                    col[index] = src_col;\n                    \'\'\',\n                    \'sum_duplicates_assign_complex\'\n                )(src_data.real, src_data.imag, src_row, src_col, index,\n                  data.real, data.imag, row, col)\n\n        self.data = data\n        self.row = row\n        self.col = col\n        self._has_canonical_format = True\n\n    def toarray(self, order=None, out=None):\n        """"""Returns a dense matrix representing the same value.\n\n        Args:\n            order (str): Not supported.\n            out: Not supported.\n\n        Returns:\n            cupy.ndarray: Dense array representing the same value.\n\n        .. seealso:: :meth:`scipy.sparse.coo_matrix.toarray`\n\n        """"""\n        return self.tocsr().toarray(order=order, out=out)\n\n    def tocoo(self, copy=False):\n        """"""Converts the matrix to COOdinate format.\n\n        Args:\n            copy (bool): If ``False``, it shares data arrays as much as\n                possible.\n\n        Returns:\n            cupyx.scipy.sparse.coo_matrix: Converted matrix.\n\n        """"""\n        if copy:\n            return self.copy()\n        else:\n            return self\n\n    def tocsc(self, copy=False):\n        """"""Converts the matrix to Compressed Sparse Column format.\n\n        Args:\n            copy (bool): If ``False``, it shares data arrays as much as\n                possible. Actually this option is ignored because all\n                arrays in a matrix cannot be shared in coo to csc conversion.\n\n        Returns:\n            cupyx.scipy.sparse.csc_matrix: Converted matrix.\n\n        """"""\n        return self.T.tocsr().T\n\n    def tocsr(self, copy=False):\n        """"""Converts the matrix to Compressed Sparse Row format.\n\n        Args:\n            copy (bool): If ``False``, it shares data arrays as much as\n                possible. Actually this option is ignored because all\n                arrays in a matrix cannot be shared in coo to csr conversion.\n\n        Returns:\n            cupyx.scipy.sparse.csr_matrix: Converted matrix.\n\n        """"""\n        if self.nnz == 0:\n            return csr.csr_matrix(self.shape, dtype=self.dtype)\n        self.sum_duplicates()\n        # copy is ignored because coosort method breaks an original.\n        x = self.copy()\n        cusparse.coosort(x)\n        return cusparse.coo2csr(x)\n\n    def transpose(self, axes=None, copy=False):\n        """"""Returns a transpose matrix.\n\n        Args:\n            axes: This option is not supported.\n            copy (bool): If ``True``, a returned matrix shares no data.\n                Otherwise, it shared data arrays as much as possible.\n\n        Returns:\n            cupyx.scipy.sparse.spmatrix: Transpose matrix.\n\n        """"""\n        if axes is not None:\n            raise ValueError(\n                \'Sparse matrices do not support an \\\'axes\\\' parameter because \'\n                \'swapping dimensions is the only logical permutation.\')\n        shape = self.shape[1], self.shape[0]\n        return coo_matrix(\n            (self.data, (self.col, self.row)), shape=shape, copy=copy)\n\n\ndef isspmatrix_coo(x):\n    """"""Checks if a given matrix is of COO format.\n\n    Returns:\n        bool: Returns if ``x`` is :class:`cupyx.scipy.sparse.coo_matrix`.\n\n    """"""\n    return isinstance(x, coo_matrix)\n'"
cupyx/scipy/sparse/csc.py,0,"b'try:\n    import scipy.sparse\n    _scipy_available = True\nexcept ImportError:\n    _scipy_available = False\n\nimport cupy\nfrom cupy import cusparse\nimport cupyx.scipy.sparse\nfrom cupyx.scipy.sparse import base\nfrom cupyx.scipy.sparse import compressed\n\n\nclass csc_matrix(compressed._compressed_sparse_matrix):\n\n    """"""Compressed Sparse Column matrix.\n\n    Now it has only part of initializer formats:\n\n    ``csc_matrix(D)``\n        ``D`` is a rank-2 :class:`cupy.ndarray`.\n    ``csc_matrix(S)``\n        ``S`` is another sparse matrix. It is equivalent to ``S.tocsc()``.\n    ``csc_matrix((M, N), [dtype])``\n        It constructs an empty matrix whose shape is ``(M, N)``. Default dtype\n        is float64.\n    ``csc_matrix((data, indices, indptr))``\n        All ``data``, ``indices`` and ``indptr`` are one-dimenaional\n        :class:`cupy.ndarray`.\n\n    Args:\n        arg1: Arguments for the initializer.\n        shape (tuple): Shape of a matrix. Its length must be two.\n        dtype: Data type. It must be an argument of :class:`numpy.dtype`.\n        copy (bool): If ``True``, copies of given arrays are always used.\n\n    .. seealso::\n       :class:`scipy.sparse.csc_matrix`\n\n    """"""\n\n    format = \'csc\'\n\n    def get(self, stream=None):\n        """"""Returns a copy of the array on host memory.\n\n        .. warning::\n           You need to install SciPy to use this method.\n\n        Args:\n            stream (cupy.cuda.Stream): CUDA stream object. If it is given, the\n                copy runs asynchronously. Otherwise, the copy is synchronous.\n\n        Returns:\n            scipy.sparse.csc_matrix: Copy of the array on host memory.\n\n        """"""\n        if not _scipy_available:\n            raise RuntimeError(\'scipy is not available\')\n        data = self.data.get(stream)\n        indices = self.indices.get(stream)\n        indptr = self.indptr.get(stream)\n        return scipy.sparse.csc_matrix(\n            (data, indices, indptr), shape=self._shape)\n\n    def _convert_dense(self, x):\n        m = cusparse.dense2csc(x)\n        return m.data, m.indices, m.indptr\n\n    def _swap(self, x, y):\n        return (y, x)\n\n    # TODO(unno): Implement __getitem__\n\n    def __mul__(self, other):\n        if cupy.isscalar(other):\n            self.sum_duplicates()\n            return self._with_data(self.data * other)\n        elif cupyx.scipy.sparse.isspmatrix_csr(other):\n            self.sum_duplicates()\n            other.sum_duplicates()\n            return cusparse.csrgemm(self.T, other, transa=True)\n        elif isspmatrix_csc(other):\n            self.sum_duplicates()\n            other.sum_duplicates()\n            return cusparse.csrgemm(self.T, other.T, transa=True, transb=True)\n        elif cupyx.scipy.sparse.isspmatrix(other):\n            return self * other.tocsr()\n        elif base.isdense(other):\n            if other.ndim == 0:\n                self.sum_duplicates()\n                return self._with_data(self.data * other)\n            elif other.ndim == 1:\n                self.sum_duplicates()\n                return cusparse.csrmv(\n                    self.T, cupy.asfortranarray(other), transa=True)\n            elif other.ndim == 2:\n                self.sum_duplicates()\n                return cusparse.csrmm2(\n                    self.T, cupy.asfortranarray(other), transa=True)\n            else:\n                raise ValueError(\'could not interpret dimensions\')\n        else:\n            return NotImplemented\n\n    # TODO(unno): Implement check_format\n    # TODO(unno): Implement diagonal\n\n    def eliminate_zeros(self):\n        """"""Removes zero entories in place.""""""\n        t = self.T\n        t.eliminate_zeros()\n        compress = t.T\n        self.data = compress.data\n        self.indices = compress.indices\n        self.indptr = compress.indptr\n\n    # TODO(unno): Implement maximum\n    # TODO(unno): Implement minimum\n    # TODO(unno): Implement multiply\n    # TODO(unno): Implement prune\n    # TODO(unno): Implement reshape\n\n    def sort_indices(self):\n        """"""Sorts the indices of the matrix in place.""""""\n        cusparse.cscsort(self)\n\n    def toarray(self, order=None, out=None):\n        """"""Returns a dense matrix representing the same value.\n\n        Args:\n            order ({\'C\', \'F\', None}): Whether to store data in C (row-major)\n                order or F (column-major) order. Default is C-order.\n            out: Not supported.\n\n        Returns:\n            cupy.ndarray: Dense array representing the same matrix.\n\n        .. seealso:: :meth:`scipy.sparse.csc_matrix.toarray`\n\n        """"""\n        if order is None:\n            order = \'C\'\n\n        if self.nnz == 0:\n            return cupy.zeros(shape=self.shape, dtype=self.dtype, order=order)\n\n        self.sum_duplicates()\n        # csc2dense and csr2dense returns F-contiguous array.\n        if order == \'C\':\n            # To return C-contiguous array, it uses transpose.\n            return cusparse.csr2dense(self.T).T\n        elif order == \'F\':\n            return cusparse.csc2dense(self)\n        else:\n            raise TypeError(\'order not understood\')\n\n    def _add_sparse(self, other, alpha, beta):\n        self.sum_duplicates()\n        other = other.tocsc().T\n        other.sum_duplicates()\n        return cusparse.csrgeam(self.T, other, alpha, beta).T\n\n    # TODO(unno): Implement tobsr\n\n    def tocoo(self, copy=False):\n        """"""Converts the matrix to COOdinate format.\n\n        Args:\n            copy (bool): If ``False``, it shares data arrays as much as\n                possible.\n\n        Returns:\n            cupyx.scipy.sparse.coo_matrix: Converted matrix.\n\n        """"""\n        return self.T.tocoo(copy).T\n\n    def tocsc(self, copy=None):\n        """"""Converts the matrix to Compressed Sparse Column format.\n\n        Args:\n            copy (bool): If ``False``, the method returns itself.\n                Otherwise it makes a copy of the matrix.\n\n        Returns:\n            cupyx.scipy.sparse.csc_matrix: Converted matrix.\n\n        """"""\n        if copy:\n            return self.copy()\n        else:\n            return self\n\n    def tocsr(self, copy=False):\n        """"""Converts the matrix to Compressed Sparse Row format.\n\n        Args:\n            copy (bool): If ``False``, it shares data arrays as much as\n                possible. Actually this option is ignored because all\n                arrays in a matrix cannot be shared in csr to csc conversion.\n\n        Returns:\n            cupyx.scipy.sparse.csr_matrix: Converted matrix.\n\n        """"""\n        return self.T.tocsc(copy=False).T\n\n    # TODO(unno): Implement todia\n    # TODO(unno): Implement todok\n    # TODO(unno): Implement tolil\n\n    def transpose(self, axes=None, copy=False):\n        """"""Returns a transpose matrix.\n\n        Args:\n            axes: This option is not supported.\n            copy (bool): If ``True``, a returned matrix shares no data.\n                Otherwise, it shared data arrays as much as possible.\n\n        Returns:\n            cupyx.scipy.sparse.spmatrix: Transpose matrix.\n\n        """"""\n        if axes is not None:\n            raise ValueError(\n                \'Sparse matrices do not support an \\\'axes\\\' parameter because \'\n                \'swapping dimensions is the only logical permutation.\')\n\n        shape = self.shape[1], self.shape[0]\n        trans = cupyx.scipy.sparse.csr.csr_matrix(\n            (self.data, self.indices, self.indptr), shape=shape, copy=copy)\n        trans._has_canonical_format = self._has_canonical_format\n        return trans\n\n\ndef isspmatrix_csc(x):\n    """"""Checks if a given matrix is of CSC format.\n\n    Returns:\n        bool: Returns if ``x`` is :class:`cupyx.scipy.sparse.csc_matrix`.\n\n    """"""\n    return isinstance(x, csc_matrix)\n'"
cupyx/scipy/sparse/csr.py,0,"b'try:\n    import scipy.sparse\n    _scipy_available = True\nexcept ImportError:\n    _scipy_available = False\n\nimport cupy\nfrom cupy import cusparse\nfrom cupyx.scipy.sparse import base\nfrom cupyx.scipy.sparse import compressed\nfrom cupyx.scipy.sparse import csc\nif cupy.cuda.cub_enabled:\n    from cupy.cuda.cub import device_csrmv\n\n\nclass csr_matrix(compressed._compressed_sparse_matrix):\n\n    """"""Compressed Sparse Row matrix.\n\n    Now it has only part of initializer formats:\n\n    ``csr_matrix(D)``\n        ``D`` is a rank-2 :class:`cupy.ndarray`.\n    ``csr_matrix(S)``\n        ``S`` is another sparse matrix. It is equivalent to ``S.tocsr()``.\n    ``csr_matrix((M, N), [dtype])``\n        It constructs an empty matrix whose shape is ``(M, N)``. Default dtype\n        is float64.\n    ``csr_matrix((data, indices, indptr))``\n        All ``data``, ``indices`` and ``indptr`` are one-dimenaional\n        :class:`cupy.ndarray`.\n\n    Args:\n        arg1: Arguments for the initializer.\n        shape (tuple): Shape of a matrix. Its length must be two.\n        dtype: Data type. It must be an argument of :class:`numpy.dtype`.\n        copy (bool): If ``True``, copies of given arrays are always used.\n\n    .. seealso::\n       :class:`scipy.sparse.csr_matrix`\n\n    """"""\n\n    format = \'csr\'\n\n    # TODO(unno): Implement has_sorted_indices\n\n    def get(self, stream=None):\n        """"""Returns a copy of the array on host memory.\n\n        Args:\n            stream (cupy.cuda.Stream): CUDA stream object. If it is given, the\n                copy runs asynchronously. Otherwise, the copy is synchronous.\n\n        Returns:\n            scipy.sparse.csr_matrix: Copy of the array on host memory.\n\n        """"""\n        if not _scipy_available:\n            raise RuntimeError(\'scipy is not available\')\n        data = self.data.get(stream)\n        indices = self.indices.get(stream)\n        indptr = self.indptr.get(stream)\n        return scipy.sparse.csr_matrix(\n            (data, indices, indptr), shape=self._shape)\n\n    def _convert_dense(self, x):\n        m = cusparse.dense2csr(x)\n        return m.data, m.indices, m.indptr\n\n    def _swap(self, x, y):\n        return (x, y)\n\n    # TODO(unno): Implement __getitem__\n\n    def _add_sparse(self, other, alpha, beta):\n        self.sum_duplicates()\n        other = other.tocsr()\n        other.sum_duplicates()\n        return cusparse.csrgeam(self, other, alpha, beta)\n\n    def __eq__(self, other):\n        raise NotImplementedError\n\n    def __ne__(self, other):\n        raise NotImplementedError\n\n    def __lt__(self, other):\n        raise NotImplementedError\n\n    def __gt__(self, other):\n        raise NotImplementedError\n\n    def __le__(self, other):\n        raise NotImplementedError\n\n    def __ge__(self, other):\n        raise NotImplementedError\n\n    def __mul__(self, other):\n        if cupy.isscalar(other):\n            self.sum_duplicates()\n            return self._with_data(self.data * other)\n        elif isspmatrix_csr(other):\n            self.sum_duplicates()\n            other.sum_duplicates()\n            return cusparse.csrgemm(self, other)\n        elif csc.isspmatrix_csc(other):\n            self.sum_duplicates()\n            other.sum_duplicates()\n            return cusparse.csrgemm(self, other.T, transb=True)\n        elif base.isspmatrix(other):\n            return self * other.tocsr()\n        elif base.isdense(other):\n            if other.ndim == 0:\n                self.sum_duplicates()\n                return self._with_data(self.data * other)\n            elif other.ndim == 1:\n                self.sum_duplicates()\n                other = cupy.asfortranarray(other)\n                # csrmvEx does not work if nnz == 0\n                if self.nnz > 0 and cusparse.csrmvExIsAligned(self, other):\n                    if cupy.cuda.cub_enabled and other.flags.c_contiguous:\n                        return device_csrmv(\n                            self.shape[0], self.shape[1], self.nnz, self.data,\n                            self.indptr, self.indices, other)\n                    else:\n                        return cusparse.csrmvEx(self, other)\n                else:\n                    return cusparse.csrmv(self, other)\n            elif other.ndim == 2:\n                self.sum_duplicates()\n                return cusparse.csrmm2(self, cupy.asfortranarray(other))\n            else:\n                raise ValueError(\'could not interpret dimensions\')\n        else:\n            return NotImplemented\n\n    def __div__(self, other):\n        raise NotImplementedError\n\n    def __rdiv__(self, other):\n        raise NotImplementedError\n\n    def __truediv__(self, other):\n        raise NotImplementedError\n\n    def __rtruediv__(self, other):\n        raise NotImplementedError\n\n    # TODO(unno): Implement check_format\n\n    def diagonal(self, k=0):\n        # TODO(unno): Implement diagonal\n        raise NotImplementedError\n\n    def eliminate_zeros(self):\n        """"""Removes zero entories in place.""""""\n        compress = cusparse.csr2csr_compress(self, 0)\n        self.data = compress.data\n        self.indices = compress.indices\n        self.indptr = compress.indptr\n\n    def maximum(self, other):\n        # TODO(unno): Implement maximum\n        raise NotImplementedError\n\n    def minimum(self, other):\n        # TODO(unno): Implement minimum\n        raise NotImplementedError\n\n    def multiply(self, other):\n        # TODO(unno): Implement multiply\n        raise NotImplementedError\n\n    # TODO(unno): Implement prune\n    # TODO(unno): Implement reshape\n\n    def sort_indices(self):\n        """"""Sorts the indices of the matrix in place.""""""\n        cusparse.csrsort(self)\n\n    def toarray(self, order=None, out=None):\n        """"""Returns a dense matrix representing the same value.\n\n        Args:\n            order ({\'C\', \'F\', None}): Whether to store data in C (row-major)\n                order or F (column-major) order. Default is C-order.\n            out: Not supported.\n\n        Returns:\n            cupy.ndarray: Dense array representing the same matrix.\n\n        .. seealso:: :meth:`scipy.sparse.csr_matrix.toarray`\n\n        """"""\n        if order is None:\n            order = \'C\'\n\n        if self.nnz == 0:\n            return cupy.zeros(shape=self.shape, dtype=self.dtype, order=order)\n\n        self.sum_duplicates()\n        # csr2dense returns F-contiguous array.\n        if order == \'C\':\n            # To return C-contiguous array, it uses transpose.\n            return cusparse.csc2dense(self.T).T\n        elif order == \'F\':\n            return cusparse.csr2dense(self)\n        else:\n            raise TypeError(\'order not understood\')\n\n    def tobsr(self, blocksize=None, copy=False):\n        # TODO(unno): Implement tobsr\n        raise NotImplementedError\n\n    def tocoo(self, copy=False):\n        """"""Converts the matrix to COOdinate format.\n\n        Args:\n            copy (bool): If ``False``, it shares data arrays as much as\n                possible.\n\n        Returns:\n            cupyx.scipy.sparse.coo_matrix: Converted matrix.\n\n        """"""\n        if copy:\n            data = self.data.copy()\n            indices = self.indices.copy()\n        else:\n            data = self.data\n            indices = self.indices\n\n        return cusparse.csr2coo(self, data, indices)\n\n    def tocsc(self, copy=False):\n        """"""Converts the matrix to Compressed Sparse Column format.\n\n        Args:\n            copy (bool): If ``False``, it shares data arrays as much as\n                possible. Actually this option is ignored because all\n                arrays in a matrix cannot be shared in csr to csc conversion.\n\n        Returns:\n            cupyx.scipy.sparse.csc_matrix: Converted matrix.\n\n        """"""\n        # copy is ignored\n        return cusparse.csr2csc(self)\n\n    def tocsr(self, copy=False):\n        """"""Converts the matrix to Compressed Sparse Row format.\n\n        Args:\n            copy (bool): If ``False``, the method returns itself.\n                Otherwise it makes a copy of the matrix.\n\n        Returns:\n            cupyx.scipy.sparse.csr_matrix: Converted matrix.\n\n        """"""\n        if copy:\n            return self.copy()\n        else:\n            return self\n\n    def todia(self, copy=False):\n        # TODO(unno): Implement todia\n        raise NotImplementedError\n\n    def todok(self, copy=False):\n        # TODO(unno): Implement todok\n        raise NotImplementedError\n\n    def tolil(self, copy=False):\n        # TODO(unno): Implement tolil\n        raise NotImplementedError\n\n    def transpose(self, axes=None, copy=False):\n        """"""Returns a transpose matrix.\n\n        Args:\n            axes: This option is not supported.\n            copy (bool): If ``True``, a returned matrix shares no data.\n                Otherwise, it shared data arrays as much as possible.\n\n        Returns:\n            cupyx.scipy.sparse.spmatrix: Transpose matrix.\n\n        """"""\n        if axes is not None:\n            raise ValueError(\n                \'Sparse matrices do not support an \\\'axes\\\' parameter because \'\n                \'swapping dimensions is the only logical permutation.\')\n\n        shape = self.shape[1], self.shape[0]\n        return csc.csc_matrix(\n            (self.data, self.indices, self.indptr), shape=shape, copy=copy)\n\n\ndef isspmatrix_csr(x):\n    """"""Checks if a given matrix is of CSR format.\n\n    Returns:\n        bool: Returns if ``x`` is :class:`cupyx.scipy.sparse.csr_matrix`.\n\n    """"""\n    return isinstance(x, csr_matrix)\n'"
cupyx/scipy/sparse/data.py,0,"b'import cupy\nfrom cupyx.scipy.sparse import base\nfrom .util import validateaxis\n\n\n_ufuncs = [\n    \'arcsin\', \'arcsinh\', \'arctan\', \'arctanh\', \'ceil\', \'deg2rad\', \'expm1\',\n    \'floor\', \'log1p\', \'rad2deg\', \'rint\', \'sign\', \'sin\', \'sinh\', \'sqrt\', \'tan\',\n    \'tanh\', \'trunc\',\n]\n\n\nclass _data_matrix(base.spmatrix):\n\n    def __init__(self, data):\n        self.data = data\n\n    @property\n    def dtype(self):\n        """"""Data type of the matrix.""""""\n        return self.data.dtype\n\n    def _with_data(self, data, copy=True):\n        raise NotImplementedError\n\n    def __abs__(self):\n        """"""Elementwise abosulte.""""""\n        return self._with_data(abs(self.data))\n\n    def __neg__(self):\n        """"""Elementwise negative.""""""\n        return self._with_data(-self.data)\n\n    def astype(self, t):\n        """"""Casts the array to given data type.\n\n        Args:\n            dtype: Type specifier.\n\n        Returns:\n            A copy of the array with a given type.\n\n        """"""\n        return self._with_data(self.data.astype(t))\n\n    def conj(self, copy=True):\n        if cupy.issubdtype(self.dtype, cupy.complexfloating):\n            return self._with_data(self.data.conj(), copy=copy)\n        elif copy:\n            return self.copy()\n        else:\n            return self\n\n    conj.__doc__ = base.spmatrix.conj.__doc__\n\n    def copy(self):\n        return self._with_data(self.data.copy(), copy=True)\n\n    copy.__doc__ = base.spmatrix.copy.__doc__\n\n    def count_nonzero(self):\n        """"""Returns number of non-zero entries.\n\n        .. note::\n           This method counts the actual number of non-zero entories, which\n           does not include explicit zero entries.\n           Instead ``nnz`` returns the number of entries including explicit\n           zeros.\n\n        Returns:\n            Number of non-zero entries.\n\n        """"""\n        return cupy.count_nonzero(self.data)\n\n    def mean(self, axis=None, dtype=None, out=None):\n        """"""Compute the arithmetic mean along the specified axis.\n\n        Args:\n            axis (int or ``None``): Axis along which the sum is computed.\n                If it is ``None``, it computes the average of all the elements.\n                Select from ``{None, 0, 1, -2, -1}``.\n\n        Returns:\n            cupy.ndarray: Summed array.\n\n        .. seealso::\n           :meth:`scipy.sparse.spmatrix.mean`\n\n        """"""\n        validateaxis(axis)\n        nRow, nCol = self.shape\n        data = self.data.copy()\n\n        if axis is None:\n            n = nRow * nCol\n        elif axis in (0, -2):\n            n = nRow\n        else:\n            n = nCol\n\n        return self._with_data(data / n).sum(axis, dtype, out)\n\n    def power(self, n, dtype=None):\n        """"""Elementwise power function.\n\n        Args:\n            n: Exponent.\n            dtype: Type specifier.\n\n        """"""\n        if dtype is None:\n            data = self.data.copy()\n        else:\n            data = self.data.astype(dtype, copy=True)\n        data **= n\n        return self._with_data(data)\n\n\ndef _find_missing_index(ind, n):\n    for k, a in enumerate(ind):\n        if k != a:\n            return k\n\n    k += 1\n    if k < n:\n        return k\n    else:\n        return -1\n\n\nclass _minmax_mixin(object):\n    """"""Mixin for min and max methods.\n    These are not implemented for dia_matrix, hence the separate class.\n\n    """"""\n\n    def _min_or_max_axis(self, axis, min_or_max, sum_duplicates, nonzero):\n        N = self.shape[axis]\n        if N == 0:\n            raise ValueError(""zero-size array to reduction operation"")\n\n        mat = self.tocsc() if axis == 0 else self.tocsr()\n        if sum_duplicates:\n            mat.sum_duplicates()\n\n        # Do the reudction\n        value = mat._minor_reduce(min_or_max, axis, nonzero)\n\n        return value\n\n    def _min_or_max(self, axis, out, min_or_max, sum_duplicates, non_zero):\n        if out is not None:\n            raise ValueError((""Sparse matrices do not support ""\n                              ""an \'out\' parameter.""))\n\n        validateaxis(axis)\n\n        if axis == 0 or axis == 1:\n            return self._min_or_max_axis(axis, min_or_max, sum_duplicates,\n                                         non_zero)\n        else:\n            raise ValueError(""axis out of range"")\n\n    def _arg_min_or_max_axis(self, axis, op, sum_duplicates):\n        if self.shape[axis] == 0:\n            raise ValueError(""Can\'t apply the operation along a zero-sized ""\n                             ""dimension."")\n\n        mat = self.tocsc() if axis == 0 else self.tocsr()\n\n        if sum_duplicates:\n            mat.sum_duplicates()\n\n        # Do the reudction\n        value = mat._arg_minor_reduce(op, axis)\n\n        return value\n\n    def _arg_min_or_max(self, axis, out, op, compare, sum_duplicates):\n        if out is not None:\n            raise ValueError(""Sparse matrices do not support ""\n                             ""an \'out\' parameter."")\n\n        validateaxis(axis)\n\n        if axis is None:\n            if 0 in self.shape:\n                raise ValueError(""Can\'t apply the operation to ""\n                                 ""an empty matrix."")\n\n            if self.nnz == 0:\n                return 0\n            else:\n                zero = self.dtype.type(0)\n                mat = self.tocoo()\n\n                if sum_duplicates:\n                    mat.sum_duplicates()\n\n                am = op(mat.data)\n                m = mat.data[am]\n\n                if compare(m, zero):\n                    return mat.row[am] * mat.shape[1] + mat.col[am]\n                else:\n                    size = cupy.prod(mat.shape)\n                    if size == mat.nnz:\n                        return am\n                    else:\n                        ind = mat.row * mat.shape[1] + mat.col\n                        zero_ind = _find_missing_index(ind, size)\n                        if m == zero:\n                            return min(zero_ind, am)\n                        else:\n                            return zero_ind\n\n        return self._arg_min_or_max_axis(axis, op, sum_duplicates)\n\n    def max(self, axis=None, out=None, sum_duplicates=False, nonzero=False):\n        """"""Returns the maximum of the matrix or maximum along an axis.\n\n        Args:\n            axis (int): {-2, -1, 0, 1, ``None``} (optional)\n                Axis along which the sum is computed. The default is to\n                compute the maximum over all the matrix elements, returning\n                a scalar (i.e. ``axis`` = ``None``).\n            out (None): (optional)\n                This argument is in the signature *solely* for NumPy\n                compatibility reasons. Do not pass in anything except\n                for the default value, as this argument is not used.\n            sum_duplicates (bool): Flag to indicate that duplicate elements\n                should be combined prior to the operation\n            nonzero (bool): Return the maximum nonzero value and ignore all\n                zero entries. If the dimension has no nonzero values, a zero is\n                then returned to indicate that it is the only available value.\n\n        Returns:\n            (cupy.ndarray or float): Maximum of ``a``. If ``axis`` is\n                ``None``, the result is a scalar value. If ``axis`` is given,\n                the result is an array of dimension ``a.ndim - 1``. This\n                differs from numpy for computational efficiency.\n\n        .. seealso:: min : The minimum value of a sparse matrix along a given\n          axis.\n        .. seealso:: numpy.matrix.max : NumPy\'s implementation of ``max`` for\n          matrices\n\n        """"""\n\n        return self._min_or_max(axis, out, cupy.max, sum_duplicates, nonzero)\n\n    def min(self, axis=None, out=None, sum_duplicates=False, nonzero=False):\n        """"""Returns the minimum of the matrix or maximum along an axis.\n\n        Args:\n            axis (int): {-2, -1, 0, 1, ``None``} (optional)\n                Axis along which the sum is computed. The default is to\n                compute the minimum over all the matrix elements, returning\n                a scalar (i.e. ``axis`` = ``None``).\n            out (None): (optional)\n                This argument is in the signature *solely* for NumPy\n                compatibility reasons. Do not pass in anything except for\n                the default value, as this argument is not used.\n            sum_duplicates (bool): Flag to indicate that duplicate elements\n                should be combined prior to the operation\n            nonzero (bool): Return the minimum nonzero value and ignore all\n                zero entries. If the dimension has no nonzero values, a zero is\n                then returned to indicate that it is the only available value.\n\n        Returns:\n            (cupy.ndarray or float): Minimum of ``a``. If ``axis`` is\n                None, the result is a scalar value. If ``axis`` is given, the\n                result is an array of dimension ``a.ndim - 1``. This differs\n                from numpy for computational efficiency.\n\n        .. seealso:: max : The maximum value of a sparse matrix along a given\n          axis.\n        .. seealso:: numpy.matrix.min : NumPy\'s implementation of \'min\' for\n          matrices\n\n        """"""\n\n        return self._min_or_max(axis, out, cupy.min, sum_duplicates, nonzero)\n\n    def argmax(self, axis=None, out=None, sum_duplicates=False):\n        """"""Returns indices of maximum elements along an axis.\n\n        Implicit zero elements are taken into account. If there are several\n        maximum values, the index of the first occurrence is returned. If\n        ``NaN`` values occur in the matrix, the output defaults to a zero entry\n        for the row/column in which the NaN occurs.\n\n        Args:\n            axis (int): {-2, -1, 0, 1, ``None``} (optional)\n                Axis along which the argmax is computed. If ``None`` (default),\n                index of the maximum element in the flatten data is returned.\n            out (None): (optional)\n                This argument is in the signature *solely* for NumPy\n                compatibility reasons. Do not pass in anything except for\n                the default value, as this argument is not used.\n            sum_duplicates (bool): Flag to indicate that duplicate elements\n                should be combined prior to the operation\n\n        Returns:\n            (cupy.narray or int): Indices of maximum elements. If array,\n                its size along ``axis`` is 1.\n\n        """"""\n\n        return self._arg_min_or_max(axis, out, cupy.argmax, cupy.greater,\n                                    sum_duplicates)\n\n    def argmin(self, axis=None, out=None, sum_duplicates=False):\n        """"""\n        Returns indices of minimum elements along an axis.\n\n        Implicit zero elements are taken into account. If there are several\n        minimum values, the index of the first occurrence is returned. If\n        ``NaN`` values occur in the matrix, the output defaults to a zero entry\n        for the row/column in which the NaN occurs.\n\n        Args:\n            axis (int): {-2, -1, 0, 1, ``None``} (optional)\n                Axis along which the argmin is computed. If ``None`` (default),\n                index of the minimum element in the flatten data is returned.\n            out (None): (optional)\n                This argument is in the signature *solely* for NumPy\n                compatibility reasons. Do not pass in anything except for\n                the default value, as this argument is not used.\n            sum_duplicates (bool): Flag to indicate that duplicate elements\n                should be combined prior to the operation\n\n        Returns:\n            (cupy.narray or int): Indices of minimum elements. If matrix,\n                its size along ``axis`` is 1.\n\n        """"""\n\n        return self._arg_min_or_max(axis, out, cupy.argmin, cupy.less,\n                                    sum_duplicates)\n\n\ndef _install_ufunc(func_name):\n\n    def f(self):\n        ufunc = getattr(cupy, func_name)\n        result = ufunc(self.data)\n        return self._with_data(result)\n\n    f.__doc__ = \'Elementwise %s.\' % func_name\n    f.__name__ = func_name\n\n    setattr(_data_matrix, func_name, f)\n\n\ndef _install_ufuncs():\n    for func_name in _ufuncs:\n        _install_ufunc(func_name)\n\n\n_install_ufuncs()\n'"
cupyx/scipy/sparse/dia.py,0,"b'try:\n    import scipy.sparse\n    _scipy_available = True\nexcept ImportError:\n    _scipy_available = False\n\nimport cupy\nfrom cupy import core\nfrom cupyx.scipy.sparse import csc\nfrom cupyx.scipy.sparse import data\nfrom cupyx.scipy.sparse import util\n\n\nclass dia_matrix(data._data_matrix):\n\n    """"""Sparse matrix with DIAgonal storage.\n\n    Now it has only one initializer format below:\n\n    ``dia_matrix((data, offsets))``\n\n    Args:\n        arg1: Arguments for the initializer.\n        shape (tuple): Shape of a matrix. Its length must be two.\n        dtype: Data type. It must be an argument of :class:`numpy.dtype`.\n        copy (bool): If ``True``, copies of given arrays are always used.\n\n    .. seealso::\n       :class:`scipy.sparse.dia_matrix`\n\n    """"""\n\n    format = \'dia\'\n\n    def __init__(self, arg1, shape=None, dtype=None, copy=False):\n        if _scipy_available and scipy.sparse.issparse(arg1):\n            x = arg1.todia()\n            data = x.data\n            offsets = x.offsets\n            shape = x.shape\n            dtype = x.dtype\n            copy = False\n        elif isinstance(arg1, tuple):\n            data, offsets = arg1\n            if shape is None:\n                raise ValueError(\'expected a shape argument\')\n\n        else:\n            raise ValueError(\n                \'unrecognized form for dia_matrix constructor\')\n\n        data = cupy.array(data, dtype=dtype, copy=copy)\n        data = cupy.atleast_2d(data)\n        offsets = cupy.array(offsets, dtype=\'i\', copy=copy)\n        offsets = cupy.atleast_1d(offsets)\n\n        if offsets.ndim != 1:\n            raise ValueError(\'offsets array must have rank 1\')\n\n        if data.ndim != 2:\n            raise ValueError(\'data array must have rank 2\')\n\n        if data.shape[0] != len(offsets):\n            raise ValueError(\n                \'number of diagonals (%d) does not match the number of \'\n                \'offsets (%d)\'\n                % (data.shape[0], len(offsets)))\n\n        sorted_offsets = cupy.sort(offsets)\n        if (sorted_offsets[:-1] == sorted_offsets[1:]).any():\n            raise ValueError(\'offset array contains duplicate values\')\n\n        self.data = data\n        self.offsets = offsets\n        if not util.isshape(shape):\n            raise ValueError(\'invalid shape (must be a 2-tuple of int)\')\n        self._shape = int(shape[0]), int(shape[1])\n\n    def _with_data(self, data, copy=True):\n        """"""Returns a matrix with the same sparsity structure as self,\n        but with different data.  By default the structure arrays are copied.\n        """"""\n        if copy:\n            return dia_matrix((data, self.offsets.copy()), shape=self.shape)\n        else:\n            return dia_matrix((data, self.offsets), shape=self.shape)\n\n    def get(self, stream=None):\n        """"""Returns a copy of the array on host memory.\n\n        Args:\n            stream (cupy.cuda.Stream): CUDA stream object. If it is given, the\n                copy runs asynchronously. Otherwise, the copy is synchronous.\n\n        Returns:\n            scipy.sparse.dia_matrix: Copy of the array on host memory.\n\n        """"""\n        if not _scipy_available:\n            raise RuntimeError(\'scipy is not available\')\n        data = self.data.get(stream)\n        offsets = self.offsets.get(stream)\n        return scipy.sparse.dia_matrix((data, offsets), shape=self._shape)\n\n    def get_shape(self):\n        """"""Returns the shape of the matrix.\n\n        Returns:\n            tuple: Shape of the matrix.\n        """"""\n        return self._shape\n\n    def getnnz(self, axis=None):\n        """"""Returns the number of stored values, including explicit zeros.\n\n        Args:\n            axis: Not supported yet.\n\n        Returns:\n            int: The number of stored values.\n\n        """"""\n        if axis is not None:\n            raise NotImplementedError(\n                \'getnnz over an axis is not implemented for DIA format\')\n\n        m, n = self.shape\n        nnz = core.ReductionKernel(\n            \'int32 offsets, int32 m, int32 n\', \'int32 nnz\',\n            \'offsets > 0 ? min(m, n - offsets) : min(m + offsets, n)\',\n            \'a + b\', \'nnz = a\', \'0\', \'dia_nnz\')(self.offsets, m, n)\n        return int(nnz)\n\n    def toarray(self, order=None, out=None):\n        """"""Returns a dense matrix representing the same value.""""""\n        return self.tocsc().toarray(order=order, out=out)\n\n    def tocsc(self, copy=False):\n        """"""Converts the matrix to Compressed Sparse Column format.\n\n        Args:\n            copy (bool): If ``False``, it shares data arrays as much as\n                possible. Actually this option is ignored because all\n                arrays in a matrix cannot be shared in dia to csc conversion.\n\n        Returns:\n            cupyx.scipy.sparse.csc_matrix: Converted matrix.\n\n        """"""\n        if self.data.size == 0:\n            return csc.csc_matrix(self.shape, dtype=self.dtype)\n\n        num_rows, num_cols = self.shape\n        num_offsets, offset_len = self.data.shape\n\n        row, mask = core.ElementwiseKernel(\n            \'int32 offset_len, int32 offsets, int32 num_rows, \'\n            \'int32 num_cols, T data\',\n            \'int32 row, bool mask\',\n            \'\'\'\n            int offset_inds = i % offset_len;\n            row = offset_inds - offsets;\n            mask = (row >= 0 && row < num_rows && offset_inds < num_cols\n                    && data != T(0));\n            \'\'\',\n            \'dia_tocsc\')(offset_len, self.offsets[:, None], num_rows,\n                         num_cols, self.data)\n        indptr = cupy.zeros(num_cols + 1, dtype=\'i\')\n        indptr[1: offset_len + 1] = cupy.cumsum(mask.sum(axis=0))\n        indptr[offset_len + 1:] = indptr[offset_len]\n        indices = row.T[mask.T].astype(\'i\', copy=False)\n        data = self.data.T[mask.T]\n        return csc.csc_matrix(\n            (data, indices, indptr), shape=self.shape, dtype=self.dtype)\n\n    def tocsr(self, copy=False):\n        """"""Converts the matrix to Compressed Sparse Row format.\n\n        Args:\n            copy (bool): If ``False``, it shares data arrays as much as\n                possible. Actually this option is ignored because all\n                arrays in a matrix cannot be shared in dia to csr conversion.\n\n        Returns:\n            cupyx.scipy.sparse.csc_matrix: Converted matrix.\n\n        """"""\n        return self.tocsc().tocsr()\n\n    def diagonal(self, k=0):\n        """"""Returns the k-th diagonal of the matrix.\n\n        Args:\n            k (int, optional): Which diagonal to get, corresponding to elements\n            a[i, i+k]. Default: 0 (the main diagonal).\n\n        Returns:\n            cupy.ndarray : The k-th diagonal.\n        """"""\n        rows, cols = self.shape\n        if k <= -rows or k >= cols:\n            raise ValueError(""k exceeds matrix dimensions"")\n        idx, = cupy.nonzero(self.offsets == k)\n        first_col, last_col = max(0, k), min(rows + k, cols)\n        if idx.size == 0:\n            return cupy.zeros(last_col - first_col, dtype=self.data.dtype)\n        return self.data[idx[0], first_col:last_col]\n\n\ndef isspmatrix_dia(x):\n    """"""Checks if a given matrix is of DIA format.\n\n    Returns:\n        bool: Returns if ``x`` is :class:`cupyx.scipy.sparse.dia_matrix`.\n\n    """"""\n    return isinstance(x, dia_matrix)\n'"
cupyx/scipy/sparse/sputils.py,0,"b'import cupy\n\nfrom cupy.core._dtype import get_dtype\n\nsupported_dtypes = [get_dtype(x) for x in\n                    (\'single\', \'double\', \'csingle\', \'cdouble\')]\n\n_upcast_memo = {}\n\n\ndef isdense(x):\n    return isinstance(x, cupy.ndarray)\n\n\ndef isscalarlike(x):\n    """"""Is x either a scalar, an array scalar, or a 0-dim array?""""""\n    return cupy.isscalar(x) or (isdense(x) and x.ndim == 0)\n\n\ndef get_index_dtype(arrays=(), maxval=None, check_contents=False):\n    """"""Based on input (integer) arrays ``a``, determines a suitable index data\n    type that can hold the data in the arrays.\n\n    Args:\n        arrays (tuple of array_like):\n            Input arrays whose types/contents to check\n        maxval (float, optional):\n            Maximum value needed\n        check_contents (bool, optional):\n            Whether to check the values in the arrays and not just their types.\n            Default: False (check only the types)\n\n    Returns:\n        dtype: Suitable index data type (int32 or int64)\n    """"""\n\n    int32min = cupy.iinfo(cupy.int32).min\n    int32max = cupy.iinfo(cupy.int32).max\n\n    dtype = cupy.intc\n    if maxval is not None:\n        if maxval > int32max:\n            dtype = cupy.int64\n\n    if isinstance(arrays, cupy.ndarray):\n        arrays = (arrays,)\n\n    for arr in arrays:\n        arr = cupy.asarray(arr)\n        if not cupy.can_cast(arr.dtype, cupy.int32):\n            if check_contents:\n                if arr.size == 0:\n                    # a bigger type not needed\n                    continue\n                elif cupy.issubdtype(arr.dtype, cupy.integer):\n                    maxval = arr.max()\n                    minval = arr.min()\n                    if minval >= int32min and maxval <= int32max:\n                        # a bigger type not needed\n                        continue\n\n            dtype = cupy.int64\n            break\n\n    return dtype\n\n\ndef upcast(*args):\n    """"""Returns the nearest supported sparse dtype for the\n    combination of one or more types.\n\n    upcast(t0, t1, ..., tn) -> T  where T is a supported dtype\n\n    Examples:\n        >>> upcast(\'int32\')\n        <type \'numpy.int32\'>\n        >>> upcast(\'int32\',\'float32\')\n        <type \'numpy.float64\'>\n        >>> upcast(\'bool\',float)\n        <type \'numpy.complex128\'>\n    """"""\n\n    t = _upcast_memo.get(args)\n    if t is not None:\n        return t\n\n    upcast = cupy.find_common_type(args, [])\n\n    for t in supported_dtypes:\n        if cupy.can_cast(upcast, t):\n            _upcast_memo[args] = t\n            return t\n\n    raise TypeError(\'no supported conversion for types: %r\' % (args,))\n'"
cupyx/scipy/sparse/util.py,0,"b""import cupy\nfrom cupy.core import core\n\n\ndef isdense(x):\n    return isinstance(x, core.ndarray)\n\n\ndef isintlike(x):\n    try:\n        return bool(int(x) == x)\n    except (TypeError, ValueError):\n        return False\n\n\ndef isscalarlike(x):\n    return cupy.isscalar(x) or (isdense(x) and x.ndim == 0)\n\n\ndef isshape(x):\n    if not isinstance(x, tuple) or len(x) != 2:\n        return False\n    m, n = x\n    return isintlike(m) and isintlike(n)\n\n\ndef validateaxis(axis):\n    if axis is not None:\n        axis_type = type(axis)\n\n        if axis_type == tuple:\n            raise TypeError(\n                'Tuples are not accepted for the \\'axis\\' '\n                'parameter. Please pass in one of the '\n                'following: {-2, -1, 0, 1, None}.')\n\n        if not cupy.issubdtype(cupy.dtype(axis_type), cupy.integer):\n            raise TypeError('axis must be an integer, not {name}'\n                            .format(name=axis_type.__name__))\n\n        if not (-2 <= axis <= 1):\n            raise ValueError('axis out of range')\n"""
cupyx/scipy/special/__init__.py,0,b'from cupyx.scipy.special.bessel import i0  # NOQA\nfrom cupyx.scipy.special.bessel import i1  # NOQA\nfrom cupyx.scipy.special.bessel import j0  # NOQA\nfrom cupyx.scipy.special.bessel import j1  # NOQA\nfrom cupyx.scipy.special.bessel import y0  # NOQA\nfrom cupyx.scipy.special.bessel import y1  # NOQA\n\nfrom cupyx.scipy.special.polygamma import polygamma  # NOQA\n\nfrom cupyx.scipy.special.digamma import digamma  # NOQA\nfrom cupyx.scipy.special.gamma import gamma  # NOQA\nfrom cupyx.scipy.special.gammaln import gammaln  # NOQA\nfrom cupyx.scipy.special.zeta import zeta  # NOQA\n\nfrom cupyx.scipy.special.statistics import ndtr  # NOQA\n\nfrom cupyx.scipy.special.erf import erf  # NOQA\nfrom cupyx.scipy.special.erf import erfc  # NOQA\nfrom cupyx.scipy.special.erf import erfcx  # NOQA\nfrom cupyx.scipy.special.erf import erfinv  # NOQA\nfrom cupyx.scipy.special.erf import erfcinv  # NOQA\n\nfrom cupyx.scipy.special.convex_analysis import entr  # NOQA\nfrom cupyx.scipy.special.convex_analysis import huber  # NOQA\nfrom cupyx.scipy.special.convex_analysis import kl_div  # NOQA\nfrom cupyx.scipy.special.convex_analysis import pseudo_huber  # NOQA\nfrom cupyx.scipy.special.convex_analysis import rel_entr  # NOQA\n'
cupyx/scipy/special/bessel.py,0,"b""from cupy import core\n\n\nj0 = core.create_ufunc(\n    'cupyx_scipy_j0', ('f->f', 'd->d'),\n    'out0 = j0(in0)',\n    doc='''Bessel function of the first kind of order 0.\n\n    .. seealso:: :meth:`scipy.special.j0`\n\n    ''')\n\n\nj1 = core.create_ufunc(\n    'cupyx_scipy_j1', ('f->f', 'd->d'),\n    'out0 = j1(in0)',\n    doc='''Bessel function of the first kind of order 1.\n\n    .. seealso:: :meth:`scipy.special.j1`\n\n    ''')\n\n\ny0 = core.create_ufunc(\n    'cupyx_scipy_y0', ('f->f', 'd->d'),\n    'out0 = y0(in0)',\n    doc='''Bessel function of the second kind of order 0.\n\n    .. seealso:: :meth:`scipy.special.y0`\n\n    ''')\n\n\ny1 = core.create_ufunc(\n    'cupyx_scipy_y1', ('f->f', 'd->d'),\n    'out0 = y1(in0)',\n    doc='''Bessel function of the second kind of order 1.\n\n    .. seealso:: :meth:`scipy.special.y1`\n\n    ''')\n\n\ni0 = core.create_ufunc(\n    'cupyx_scipy_i0', ('f->f', 'd->d'),\n    'out0 = cyl_bessel_i0(in0)',\n    doc='''Modified Bessel function of order 0.\n\n    .. seealso:: :meth:`scipy.special.i0`\n\n    ''')\n\n\ni1 = core.create_ufunc(\n    'cupyx_scipy_i1', ('f->f', 'd->d'),\n    'out0 = cyl_bessel_i1(in0)',\n    doc='''Modified Bessel function of order 1.\n\n    .. seealso:: :meth:`scipy.special.i1`\n\n    ''')\n"""
cupyx/scipy/special/convex_analysis.py,0,"b""from cupy import core\n\n\n_float_preamble = '''\n#include <math_constants.h>\n\ndouble __device__ entr(double x) {\n    if (isnan(x)) {\n        return CUDART_NAN;\n    } else if (x > 0){\n        return -x * log(x);\n    } else if (x == 0){\n        return 0;\n    } else {\n        return -CUDART_INF;\n    }\n}\n\ndouble __device__ kl_div(double x, double y) {\n    if (isnan(x) || isnan(y)) {\n        return CUDART_NAN;\n    } else if (x > 0 && y > 0) {\n        return x * log(x / y) - x + y;\n    } else if (x == 0 && y >= 0) {\n        return y;\n    } else {\n        return CUDART_INF;\n    }\n}\n\ndouble __device__ rel_entr(double x, double y) {\n    if (isnan(x) || isnan(y)) {\n        return CUDART_NAN;\n    } else if (x > 0 && y > 0) {\n        return x * log(x / y);\n    } else if (x == 0 && y >= 0) {\n        return 0;\n    } else {\n        return CUDART_INF;\n    }\n}\n\ndouble __device__ huber(double delta, double r) {\n    if (delta < 0) {\n        return CUDART_INF;\n    } else if (abs(r) <= delta) {\n        return 0.5 * r * r;\n    } else {\n        return delta * (abs(r) - 0.5 * delta);\n    }\n}\n\ndouble __device__ pseudo_huber(double delta, double r) {\n    if (delta < 0) {\n        return CUDART_INF;\n    } else if (delta == 0 || r == 0) {\n        return 0;\n    } else {\n        double u = delta;\n        double v = r / delta;\n        return u * u * (sqrt(1 + v * v) - 1);\n    }\n}\n\n'''\n\n\nentr = core.create_ufunc(\n    'cupyx_scipy_entr', ('f->f', 'd->d'),\n    'out0 = out0_type(entr(in0));',\n    preamble=_float_preamble,\n    doc='''Elementwise function for computing entropy.\n\n    .. seealso:: :meth:`scipy.special.entr`\n\n    ''')\n\n\nkl_div = core.create_ufunc(\n    'cupyx_scipy_kl_div', ('ff->f', 'dd->d'),\n    'out0 = out0_type(kl_div(in0, in1));',\n    preamble=_float_preamble,\n    doc='''Elementwise function for computing Kullback-Leibler divergence.\n\n    .. seealso:: :meth:`scipy.special.kl_div`\n\n    ''')\n\n\nrel_entr = core.create_ufunc(\n    'cupyx_scipy_rel_entr', ('ff->f', 'dd->d'),\n    'out0 = out0_type(rel_entr(in0, in1));',\n    preamble=_float_preamble,\n    doc='''Elementwise function for computing relative entropy.\n\n    .. seealso:: :meth:`scipy.special.rel_entr`\n\n    ''')\n\n\nhuber = core.create_ufunc(\n    'cupyx_scipy_huber', ('ff->f', 'dd->d'),\n    'out0 = out0_type(huber(in0, in1));',\n    preamble=_float_preamble,\n    doc='''Elementwise function for computing the Huber loss.\n\n    .. seealso:: :meth:`scipy.special.huber`\n\n    ''')\n\n\npseudo_huber = core.create_ufunc(\n    'cupyx_scipy_pseudo_huber', ('ff->f', 'dd->d'),\n    'out0 = out0_type(pseudo_huber(in0, in1));',\n    preamble=_float_preamble,\n    doc='''Elementwise function for computing the Pseudo-Huber loss.\n\n    .. seealso:: :meth:`scipy.special.pseudo_huber`\n\n    ''')\n"""
cupyx/scipy/special/digamma.py,0,"b'# This source code contains SciPy\'s code.\n# https://github.com/scipy/scipy/blob/master/scipy/special/cephes/psi.c\n#\n#\n# Cephes Math Library Release 2.8:  June, 2000\n# Copyright 1984, 1987, 1992, 2000 by Stephen L. Moshier\n#\n#\n# Code for the rational approximation on [1, 2] is:\n#\n# (C) Copyright John Maddock 2006.\n# Use, modification and distribution are subject to the\n# Boost Software License, Version 1.0. (See accompanying file\n# LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)\n\nfrom cupy import core\n\n\npolevl_definition = \'\'\'\ntemplate<int N> static __device__ double polevl(double x, double coef[])\n{\n    double ans;\n    double *p;\n\n    p = coef;\n    ans = *p++;\n\n    for (int i = 0; i < N; ++i){\n        ans = ans * x + *p++;\n    }\n\n    return ans;\n}\n\'\'\'\n\n\npsi_definition = \'\'\'\n__constant__ double A[] = {\n    8.33333333333333333333E-2,\n    -2.10927960927960927961E-2,\n    7.57575757575757575758E-3,\n    -4.16666666666666666667E-3,\n    3.96825396825396825397E-3,\n    -8.33333333333333333333E-3,\n    8.33333333333333333333E-2\n};\n\n__constant__ double PI = 3.141592653589793;\n__constant__ double EULER = 0.5772156649015329;\n\n__constant__ float Y = 0.99558162689208984f;\n\n__constant__ double root1 = 1569415565.0 / 1073741824.0;\n__constant__ double root2 = (381566830.0 / 1073741824.0) / 1073741824.0;\n__constant__ double root3 = 0.9016312093258695918615325266959189453125e-19;\n\n__constant__ double P[] = {\n    -0.0020713321167745952,\n    -0.045251321448739056,\n    -0.28919126444774784,\n    -0.65031853770896507,\n    -0.32555031186804491,\n    0.25479851061131551\n};\n__constant__ double Q[] = {\n    -0.55789841321675513e-6,\n    0.0021284987017821144,\n    0.054151797245674225,\n    0.43593529692665969,\n    1.4606242909763515,\n    2.0767117023730469,\n    1.0\n};\n\nstatic __device__ double digamma_imp_1_2(double x)\n{\n    /*\n     * Rational approximation on [1, 2] taken from Boost.\n     *\n     * Now for the approximation, we use the form:\n     *\n     * digamma(x) = (x - root) * (Y + R(x-1))\n     *\n     * Where root is the location of the positive root of digamma,\n     * Y is a constant, and R is optimised for low absolute error\n     * compared to Y.\n     *\n     * Maximum Deviation Found:               1.466e-18\n     * At double precision, max error found:  2.452e-17\n     */\n    double r, g;\n    g = x - root1 - root2 - root3;\n    r = polevl<5>(x - 1.0, P) / polevl<6>(x - 1.0, Q);\n\n    return g * Y + g * r;\n}\n\n\nstatic __device__ double psi_asy(double x)\n{\n    double y, z;\n\n    if (x < 1.0e17) {\n        z = 1.0 / (x * x);\n        y = z * polevl<6>(z, A);\n    }\n    else {\n        y = 0.0;\n    }\n\n    return log(x) - (0.5 / x) - y;\n}\n\n\ndouble __device__ psi(double x)\n{\n    double y = 0.0;\n    double q, r;\n    int i, n;\n\n    if (isnan(x)) {\n        return x;\n    }\n    else if (isinf(x)){\n        if(x > 0){\n            return x;\n        }else{\n            return nan("""");\n        }\n    }\n    else if (x == 0) {\n        return -1.0/0.0;\n    }\n    else if (x < 0.0) {\n        /* argument reduction before evaluating tan(pi * x) */\n        r = modf(x, &q);\n        if (r == 0.0) {\n            return nan("""");\n        }\n        y = -PI / tan(PI * r);\n        x = 1.0 - x;\n    }\n\n    /* check for positive integer up to 10 */\n    if ((x <= 10.0) && (x == floor(x))) {\n        n = (int)x;\n        for (i = 1; i < n; i++) {\n            y += 1.0 / i;\n        }\n        y -= EULER;\n        return y;\n    }\n\n    /* use the recurrence relation to move x into [1, 2] */\n    if (x < 1.0) {\n        y -= 1.0 / x;\n        x += 1.0;\n    }\n    else if (x < 10.0) {\n        while (x > 2.0) {\n            x -= 1.0;\n            y += 1.0 / x;\n        }\n    }\n    if ((1.0 <= x) && (x <= 2.0)) {\n        y += digamma_imp_1_2(x);\n        return y;\n    }\n\n    /* x is large, use the asymptotic series */\n    y += psi_asy(x);\n    return y;\n}\n\'\'\'\n\n\ndigamma = core.create_ufunc(\n    \'cupyx_scipy_digamma\', (\'f->f\', \'d->d\'),\n    \'out0 = psi(in0)\',\n    preamble=polevl_definition+psi_definition,\n    doc=""""""The digamma function.\n\n    Args:\n        x (cupy.ndarray): The input of digamma function.\n\n    Returns:\n        cupy.ndarray: Computed value of digamma function.\n\n    .. seealso:: :data:`scipy.special.digamma`\n\n    """""")\n'"
cupyx/scipy/special/erf.py,0,"b""from cupy import core\n\n\nerf = core.create_ufunc(\n    'cupyx_scipy_erf', ('f->f', 'd->d'),\n    'out0 = erf(in0)',\n    doc='''Error function.\n\n    .. seealso:: :meth:`scipy.special.erf`\n\n    ''')\n\n\nerfc = core.create_ufunc(\n    'cupyx_scipy_erfc', ('f->f', 'd->d'),\n    'out0 = erfc(in0)',\n    doc='''Complementary error function.\n\n    .. seealso:: :meth:`scipy.special.erfc`\n\n    ''')\n\n\nerfcx = core.create_ufunc(\n    'cupyx_scipy_erfcx', ('f->f', 'd->d'),\n    'out0 = erfcx(in0)',\n    doc='''Scaled complementary error function.\n\n    .. seealso:: :meth:`scipy.special.erfcx`\n\n    ''')\n\n\nerfinv = core.create_ufunc(\n    'cupyx_scipy_erfinv', ('f->f', 'd->d'),\n    'out0 = erfinv(in0);',\n    doc='''Inverse function of error function.\n\n    .. seealso:: :meth:`scipy.special.erfinv`\n\n    .. note::\n        The behavior close to (and outside) the domain follows that of\n        SciPy v1.4.0+.\n\n    ''')\n\n\nerfcinv = core.create_ufunc(\n    'cupyx_scipy_erfcinv', ('f->f', 'd->d'),\n    'out0 = erfcinv(in0);',\n    doc='''Inverse function of complementary error function.\n\n    .. seealso:: :meth:`scipy.special.erfcinv`\n\n    .. note::\n        The behavior close to (and outside) the domain follows that of\n        SciPy v1.4.0+.\n\n    ''')\n"""
cupyx/scipy/special/gamma.py,0,"b'from cupy import core\n\n\ngamma = core.create_ufunc(\n    \'cupyx_scipy_gamma\', (\'f->f\', \'d->d\'),\n    \'\'\'\n    if (isinf(in0) && in0 < 0) {\n        out0 = -1.0 / 0.0;\n    } else if (in0 < 0. && in0 == floor(in0)) {\n        out0 = 1.0 / 0.0;\n    } else {\n        out0 = tgamma(in0);\n    }\n    \'\'\',\n    doc=""""""Gamma function.\n\n    Args:\n        z (cupy.ndarray): The input of gamma function.\n\n    Returns:\n        cupy.ndarray: Computed value of gamma function.\n\n    .. seealso:: :data:`scipy.special.gamma`\n\n    """""")\n'"
cupyx/scipy/special/gammaln.py,0,"b'from cupy import core\n\n\ngammaln = core.create_ufunc(\n    \'cupyx_scipy_gammaln\', (\'f->f\', \'d->d\'),\n    \'\'\'\n    if (isinf(in0) && in0 < 0) {\n        out0 = -1.0 / 0.0;\n    } else {\n        out0 = lgamma(in0);\n    }\n    \'\'\',\n    doc=""""""Logarithm of the absolute value of the Gamma function.\n\n    Args:\n        x (cupy.ndarray): Values on the real line at which to compute\n        ``gammaln``.\n\n    Returns:\n        cupy.ndarray: Values of ``gammaln`` at x.\n\n    .. seealso:: :data:`scipy.special.gammaln`\n\n    """""")\n'"
cupyx/scipy/special/polygamma.py,0,"b'import cupy\nfrom cupyx.scipy.special import digamma\nfrom cupyx.scipy.special import gamma\nfrom cupyx.scipy.special import zeta\n\n\ndef polygamma(n, x):\n    """"""Polygamma function n.\n\n    Args:\n        n (cupy.ndarray): The order of the derivative of `psi`.\n        x (cupy.ndarray): Where to evaluate the polygamma function.\n\n    Returns:\n        cupy.ndarray: The result.\n\n    .. seealso:: :data:`scipy.special.polygamma`\n\n    """"""\n    n, x = cupy.broadcast_arrays(n, x)\n    fac2 = (-1.0)**(n+1) * gamma.gamma(n+1.0) * zeta.zeta(n+1.0, x)\n    return cupy.where(n == 0, digamma.digamma(x), fac2)\n'"
cupyx/scipy/special/statistics.py,0,"b""from cupy import core\n\n\nndtr = core.create_ufunc(\n    'cupyx_scipy_ndtr', ('f->f', 'd->d'),\n    'out0 = normcdf(in0)',\n    doc='''Cumulative distribution function of normal distribution.\n\n    .. seealso:: :meth:`scipy.special.ndtr`\n\n    ''')\n"""
cupyx/scipy/special/zeta.py,0,"b'# This source code contains SciPy\'s code.\n# https://github.com/scipy/scipy/blob/master/scipy/special/cephes/zeta.c\n#\n#\n# Cephes Math Library Release 2.0:  April, 1987\n# Copyright 1984, 1987 by Stephen L. Moshier\n# Direct inquiries to 30 Frost Street, Cambridge, MA 02140\n\n# TODO(YoshikawaMasashi): float implementation of zeta function\n\nfrom cupy import core\n\n\nzeta_definition = \'\'\'\n/* Expansion coefficients\n * for Euler-Maclaurin summation formula\n * (2k)! / B2k\n * where B2k are Bernoulli numbers\n */\n__constant__ double A[] = {\n    12.0,\n    -720.0,\n    30240.0,\n    -1209600.0,\n    47900160.0,\n    -1.8924375803183791606e9,\t/*1.307674368e12/691 */\n    7.47242496e10,\n    -2.950130727918164224e12,\t/*1.067062284288e16/3617 */\n    1.1646782814350067249e14,\t/*5.109094217170944e18/43867 */\n    -4.5979787224074726105e15,\t/*8.028576626982912e20/174611 */\n    1.8152105401943546773e17,\t/*1.5511210043330985984e23/854513 */\n    -7.1661652561756670113e18\t/*1.6938241367317436694528e27/236364091 */\n};\n\n__constant__ double MACHEP = 1.11022302462515654042E-16;\n\n/* 30 Nov 86 -- error in third coefficient fixed */\n\n\ndouble __device__ zeta(double x, double q)\n{\n    int i;\n    double a, b, k, s, t, w;\n\n    if (x == 1.0){\n        return 1.0/0.0;\n    }\n\n    if (x < 1.0) {\n        return nan("""");\n    }\n\n    if (q <= 0.0) {\n        if (q == floor(q)) {\n            return 1.0/0.0;\n        }\n        if (x != floor(x)){\n            return nan("""");\t/* because q^-x not defined */\n        }\n    }\n\n    /* Asymptotic expansion\n     * http://dlmf.nist.gov/25.11#E43\n     */\n    if (q > 1e8) {\n        return (1/(x - 1) + 1/(2*q)) * pow(q, 1 - x);\n    }\n\n    /* Euler-Maclaurin summation formula */\n\n    /* Permit negative q but continue sum until n+q > +9 .\n     * This case should be handled by a reflection formula.\n     * If q<0 and x is an integer, there is a relation to\n     * the polyGamma function.\n     */\n    s = pow(q, -x);\n    a = q;\n    i = 0;\n    b = 0.0;\n    while ((i < 9) || (a <= 9.0)) {\n        i += 1;\n        a += 1.0;\n        b = pow(a, -x);\n        s += b;\n        if (fabs(b / s) < MACHEP){\n            return s;\n        }\n    }\n\n    w = a;\n    s += b * w / (x - 1.0);\n    s -= 0.5 * b;\n    a = 1.0;\n    k = 0.0;\n    for (i = 0; i < 12; i++) {\n        a *= x + k;\n        b /= w;\n        t = a * b / A[i];\n        s = s + t;\n        t = fabs(t / s);\n        if (t < MACHEP){\n            return s;\n        }\n        k += 1.0;\n        a *= x + k;\n        b /= w;\n        k += 1.0;\n    }\n    return s;\n}\n\'\'\'\n\n\nzeta = core.create_ufunc(\n    \'cupyx_scipy_zeta\', (\'ff->f\', \'dd->d\'),\n    \'out0 = zeta(in0, in1)\',\n    preamble=zeta_definition,\n    doc=""""""Hurwitz zeta function.\n\n    Args:\n        x (cupy.ndarray): Input data, must be real.\n        q (cupy.ndarray): Input data, must be real.\n\n    Returns:\n        cupy.ndarray: Values of zeta(x, q).\n\n    .. seealso:: :data:`scipy.special.zeta`\n\n    """""")\n'"
tests/cupy_tests/binary_tests/__init__.py,0,b''
tests/cupy_tests/binary_tests/test_elementwise.py,0,"b""import unittest\n\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestElementwise(unittest.TestCase):\n\n    @testing.for_int_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def check_unary_int(self, name, xp, dtype):\n        a = xp.array([-3, -2, -1, 0, 1, 2, 3], dtype=dtype)\n        return getattr(xp, name)(a)\n\n    @testing.for_int_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def check_binary_int(self, name, xp, dtype):\n        a = xp.array([-3, -2, -1, 0, 1, 2, 3], dtype=dtype)\n        b = xp.array([0, 1, 2, 3, 4, 5, 6], dtype=dtype)\n        return getattr(xp, name)(a, b)\n\n    def test_bitwise_and(self):\n        self.check_binary_int('bitwise_and')\n\n    def test_bitwise_or(self):\n        self.check_binary_int('bitwise_or')\n\n    def test_bitwise_xor(self):\n        self.check_binary_int('bitwise_xor')\n\n    def test_invert(self):\n        self.check_unary_int('invert')\n\n    def test_left_shift(self):\n        self.check_binary_int('left_shift')\n\n    def test_right_shift(self):\n        self.check_binary_int('right_shift')\n"""
tests/cupy_tests/binary_tests/test_packing.py,0,"b'import numpy\nimport unittest\n\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestPacking(unittest.TestCase):\n\n    @testing.for_int_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def check_packbits(self, data, xp, dtype):\n        # Note numpy <= 1.9 raises an Exception when an input array is bool.\n        # See https://github.com/numpy/numpy/issues/5377\n        a = xp.array(data, dtype=dtype)\n        return xp.packbits(a)\n\n    @testing.numpy_cupy_array_equal()\n    def check_unpackbits(self, data, xp):\n        a = xp.array(data, dtype=xp.uint8)\n        return xp.unpackbits(a)\n\n    def test_packbits(self):\n        self.check_packbits([0])\n        self.check_packbits([1])\n        self.check_packbits([0, 1])\n        self.check_packbits([1, 0, 1, 1, 0, 1, 1, 1])\n        self.check_packbits([1, 0, 1, 1, 0, 1, 1, 1, 1])\n        self.check_packbits(numpy.arange(24).reshape((2, 3, 4)) % 2)\n\n    def test_packbits_empty(self):\n        # Note packbits of numpy <= 1.11 has a bug against empty arrays.\n        # See https://github.com/numpy/numpy/issues/8324\n        self.check_packbits([])\n\n    def test_unpackbits(self):\n        self.check_unpackbits([])\n        self.check_unpackbits([0])\n        self.check_unpackbits([1])\n        self.check_unpackbits([255])\n        self.check_unpackbits([100, 200, 123, 213])\n'"
tests/cupy_tests/core_tests/__init__.py,0,b''
tests/cupy_tests/core_tests/test_array_function.py,0,"b""import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestArrayFunction(unittest.TestCase):\n\n    @testing.with_requires('numpy>=1.17.0')\n    def test_array_function(self):\n        a = numpy.random.randn(100, 100)\n        a_cpu = numpy.asarray(a)\n        a_gpu = cupy.asarray(a)\n\n        # The numpy call for both CPU and GPU arrays is intentional to test the\n        # __array_function__ protocol\n        qr_cpu = numpy.linalg.qr(a_cpu)\n        qr_gpu = numpy.linalg.qr(a_gpu)\n\n        if isinstance(qr_cpu, tuple):\n            for b_cpu, b_gpu in zip(qr_cpu, qr_gpu):\n                self.assertEqual(b_cpu.dtype, b_gpu.dtype)\n                cupy.testing.assert_allclose(b_cpu, b_gpu, atol=1e-4)\n        else:\n            self.assertEqual(qr_cpu.dtype, qr_gpu.dtype)\n            cupy.testing.assert_allclose(qr_cpu, qr_gpu, atol=1e-4)\n\n    @testing.with_requires('numpy>=1.17.0')\n    @testing.numpy_cupy_equal()\n    def test_array_function_can_cast(self, xp):\n        return numpy.can_cast(xp.arange(2), 'f4')\n\n    @testing.with_requires('numpy>=1.17.0')\n    @testing.numpy_cupy_equal()\n    def test_array_function_common_type(self, xp):\n        return numpy.common_type(xp.arange(2, dtype='f8'),\n                                 xp.arange(2, dtype='f4'))\n\n    @testing.with_requires('numpy>=1.17.0')\n    @testing.numpy_cupy_equal()\n    def test_array_function_result_type(self, xp):\n        return numpy.result_type(3, xp.arange(2, dtype='f8'))\n"""
tests/cupy_tests/core_tests/test_carray.py,0,"b""import unittest\n\nimport cupy\nfrom cupy import testing\n\n\nclass TestCArray(unittest.TestCase):\n\n    def test_size(self):\n        x = cupy.arange(3).astype('i')\n        y = cupy.ElementwiseKernel(\n            'raw int32 x', 'int32 y', 'y = x.size()', 'test_carray_size',\n        )(x, size=1)\n        self.assertEqual(int(y[0]), 3)\n\n    def test_shape(self):\n        x = cupy.arange(6).reshape((2, 3)).astype('i')\n        y = cupy.ElementwiseKernel(\n            'raw int32 x', 'int32 y', 'y = x.shape()[i]', 'test_carray_shape',\n        )(x, size=2)\n        testing.assert_array_equal(y, (2, 3))\n\n    def test_strides(self):\n        x = cupy.arange(6).reshape((2, 3)).astype('i')\n        y = cupy.ElementwiseKernel(\n            'raw int32 x', 'int32 y', 'y = x.strides()[i]',\n            'test_carray_strides',\n        )(x, size=2)\n        testing.assert_array_equal(y, (12, 4))\n\n    def test_getitem_int(self):\n        x = cupy.arange(24).reshape((2, 3, 4)).astype('i')\n        y = cupy.empty_like(x)\n        y = cupy.ElementwiseKernel(\n            'raw T x', 'int32 y', 'y = x[i]', 'test_carray_getitem_int',\n        )(x, y)\n        testing.assert_array_equal(y, x)\n\n    def test_getitem_idx(self):\n        x = cupy.arange(24).reshape((2, 3, 4)).astype('i')\n        y = cupy.empty_like(x)\n        y = cupy.ElementwiseKernel(\n            'raw T x', 'int32 y',\n            'ptrdiff_t idx[] = {i / 12, i / 4 % 3, i % 4}; y = x[idx]',\n            'test_carray_getitem_idx',\n        )(x, y)\n        testing.assert_array_equal(y, x)\n\n\n@testing.parameterize(\n    {'size': 2 ** 31 - 1024},\n    {'size': 2 ** 31},\n    {'size': 2 ** 31 + 1024},\n    {'size': 2 ** 32 - 1024},\n    {'size': 2 ** 32},\n    {'size': 2 ** 32 + 1024},\n)\n@testing.slow\nclass TestCArray32BitBoundary(unittest.TestCase):\n    # This test case is intended to confirm CArray indexing work correctly\n    # with input/output arrays whose size is so large that it crosses the\n    # 32-bit boundary (in terms of both number of elements and size in bytes).\n    # This test requires approx. 8 GiB GPU memory to run.\n    # See https://github.com/cupy/cupy/pull/882 for detailed discussions.\n\n    def tearDown(self):\n        # Free huge memory for slow test\n        cupy.get_default_memory_pool().free_all_blocks()\n\n    def test(self):\n        # Elementwise\n        a = cupy.full((1, self.size), 7, dtype=cupy.int8)\n        # Reduction\n        result = a.sum(axis=0, dtype=cupy.int8)\n        assert result.sum() == self.size * 7\n"""
tests/cupy_tests/core_tests/test_core.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy.core import core\nfrom cupy import testing\n\n\nclass TestSize(unittest.TestCase):\n\n    def tearDown(self):\n        # Free huge memory for slow test\n        cupy.get_default_memory_pool().free_all_blocks()\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_size(self, xp, dtype):\n        a = xp.ndarray((2, 3), dtype=dtype)\n        return xp.size(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_size_axis(self, xp, dtype):\n        a = xp.ndarray((2, 3), dtype=dtype)\n        return xp.size(a, axis=1)\n\n    @testing.for_all_dtypes()\n    def test_size_axis_error(self, dtype):\n        for xp in (numpy, cupy):\n            a = xp.ndarray((2, 3), dtype=dtype)\n            with pytest.raises(IndexError):\n                return xp.size(a, axis=3)\n\n    @testing.numpy_cupy_equal()\n    @testing.slow\n    def test_size_huge(self, xp):\n        a = xp.ndarray(2 ** 32, 'b')  # 4 GiB\n        return xp.size(a)\n\n\n_orders = {\n    order_arg: order_expect\n    for order_expect, order_args in [\n        ('C', ['C', 'c', 'CONTIGUOUS', '', None]),\n        ('F', ['F', 'f', 'FORTRAN']),\n    ]\n    for order_arg in order_args\n}\n\n\nclass TestOrder(unittest.TestCase):\n\n    @testing.for_orders(_orders.keys())\n    def test_ndarray(self, order):\n        order_expect = _orders[order]\n        a = core.ndarray((2, 3), order=order)\n        expect_c = order_expect == 'C'\n        expect_f = order_expect == 'F'\n        assert a.flags.c_contiguous == expect_c\n        assert a.flags.f_contiguous == expect_f\n"""
tests/cupy_tests/core_tests/test_dlpack.py,0,"b""import unittest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [\n        cupy.uint8, cupy.uint16, cupy.uint32, cupy.uint64,\n        cupy.int8, cupy.int16, cupy.int32, cupy.int64,\n        cupy.float16, cupy.float32, cupy.float64],\n}))\nclass TestDLPackConversion(unittest.TestCase):\n\n    def setUp(self):\n        if cupy.issubdtype(self.dtype, cupy.unsignedinteger):\n            self.array = cupy.random.randint(\n                0, 10, size=(2, 3)).astype(self.dtype)\n        elif cupy.issubdtype(self.dtype, cupy.integer):\n            self.array = cupy.random.randint(\n                -10, 10, size=(2, 3)).astype(self.dtype)\n        elif cupy.issubdtype(self.dtype, cupy.floating):\n            self.array = cupy.random.rand(\n                2, 3).astype(self.dtype)\n\n    def test_conversion(self):\n        tensor = self.array.toDlpack()\n        array = cupy.fromDlpack(tensor)\n        testing.assert_array_equal(self.array, array)\n        testing.assert_array_equal(self.array.data.ptr, array.data.ptr)\n\n\nclass TestDLTensorMemory(unittest.TestCase):\n\n    def setUp(self):\n        self.old_pool = cupy.get_default_memory_pool()\n        self.pool = cupy.cuda.MemoryPool()\n        cupy.cuda.set_allocator(self.pool.malloc)\n\n    def tearDown(self):\n        cupy.cuda.set_allocator(self.old_pool.malloc)\n\n    def test_deleter(self):\n        array = cupy.empty(10)\n        tensor = array.toDlpack()\n        assert self.pool.n_free_blocks() == 0\n        del array\n        assert self.pool.n_free_blocks() == 0\n        del tensor\n        assert self.pool.n_free_blocks() == 1\n"""
tests/cupy_tests/core_tests/test_elementwise.py,0,"b""import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import core\nfrom cupy import cuda\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestElementwise(unittest.TestCase):\n\n    def check_copy(self, dtype, src_id, dst_id):\n        with cuda.Device(src_id):\n            src = testing.shaped_arange((2, 3, 4), dtype=dtype)\n        with cuda.Device(dst_id):\n            dst = cupy.empty((2, 3, 4), dtype=dtype)\n        core.elementwise_copy(src, dst)\n        testing.assert_allclose(src, dst)\n\n    @testing.for_all_dtypes()\n    def test_copy(self, dtype):\n        device_id = cuda.Device().id\n        self.check_copy(dtype, device_id, device_id)\n\n    @testing.multi_gpu(2)\n    @testing.for_all_dtypes()\n    def test_copy_multigpu(self, dtype):\n        with self.assertRaises(ValueError):\n            self.check_copy(dtype, 0, 1)\n\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_copy_zero_sized_array1(self, xp, dtype, order):\n        src = xp.empty((0,), dtype=dtype)\n        res = xp.copy(src, order=order)\n        self.assertIsNot(src, res)\n        return res\n\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_copy_zero_sized_array2(self, xp, dtype, order):\n        src = xp.empty((1, 0, 2), dtype=dtype)\n        res = xp.copy(src, order=order)\n        self.assertIsNot(src, res)\n        return res\n\n    @testing.for_orders('CFAK')\n    def test_copy_orders(self, order):\n        a = cupy.empty((2, 3, 4))\n        b = cupy.copy(a, order)\n\n        a_cpu = numpy.empty((2, 3, 4))\n        b_cpu = numpy.copy(a_cpu, order)\n\n        self.assertEqual(b.strides, b_cpu.strides)\n\n\n@testing.gpu\nclass TestElementwiseInvalidShape(unittest.TestCase):\n\n    def test_invalid_shape(self):\n        with self.assertRaisesRegex(ValueError, 'Out shape is mismatched'):\n            f = cupy.ElementwiseKernel('T x', 'T y', 'y += x')\n            x = cupy.arange(12).reshape(3, 4)\n            y = cupy.arange(4)\n            f(x, y)\n\n\n@testing.gpu\nclass TestElementwiseInvalidArgument(unittest.TestCase):\n\n    def test_invalid_kernel_name(self):\n        with self.assertRaisesRegex(ValueError, 'Invalid kernel name'):\n            cupy.ElementwiseKernel('T x', '', '', '1')\n\n\n@testing.gpu\nclass TestElementwiseType(unittest.TestCase):\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_large_int_upper_1(self, xp, dtype):\n        a = xp.array([0], dtype=xp.int8)\n        b = xp.iinfo(dtype).max\n        return a + b\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_large_int_upper_2(self, xp, dtype):\n        a = xp.array([1], dtype=xp.int8)\n        b = xp.iinfo(dtype).max - 1\n        return a + b\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_large_int_upper_3(self, xp, dtype):\n        a = xp.array([xp.iinfo(dtype).max], dtype=dtype)\n        b = xp.int8(0)\n        return a + b\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_large_int_upper_4(self, xp, dtype):\n        a = xp.array([xp.iinfo(dtype).max - 1], dtype=dtype)\n        b = xp.int8(1)\n        return a + b\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_large_int_lower_1(self, xp, dtype):\n        a = xp.array([0], dtype=xp.int8)\n        b = xp.iinfo(dtype).min\n        return a + b\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_large_int_lower_2(self, xp, dtype):\n        a = xp.array([-1], dtype=xp.int8)\n        b = xp.iinfo(dtype).min + 1\n        return a + b\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_large_int_lower_3(self, xp, dtype):\n        a = xp.array([xp.iinfo(dtype).min], dtype=dtype)\n        b = xp.int8(0)\n        return a + b\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_large_int_lower_4(self, xp, dtype):\n        a = xp.array([xp.iinfo(dtype).min + 1], dtype=dtype)\n        b = xp.int8(-1)\n        return a + b\n"""
tests/cupy_tests/core_tests/test_flags.py,0,"b""import unittest\n\nfrom cupy.core import flags\nfrom cupy import testing\n\n\nclass TestFlags(unittest.TestCase):\n\n    def setUp(self):\n        self.flags = flags.Flags(1, 2, 3)\n\n    def test_c_contiguous(self):\n        self.assertEqual(1, self.flags['C_CONTIGUOUS'])\n\n    def test_f_contiguous(self):\n        self.assertEqual(2, self.flags['F_CONTIGUOUS'])\n\n    def test_owndata(self):\n        self.assertEqual(3, self.flags['OWNDATA'])\n\n    def test_key_error(self):\n        with self.assertRaises(KeyError):\n            self.flags['unknown key']\n\n    def test_repr(self):\n        self.assertEqual('''  C_CONTIGUOUS : 1\n  F_CONTIGUOUS : 2\n  OWNDATA : 3''', repr(self.flags))\n\n\n@testing.parameterize(\n    *testing.product({\n        'order': ['C', 'F', 'non-contiguous'],\n        'shape': [(8, ), (4, 8)],\n    })\n)\nclass TestContiguityFlags(unittest.TestCase):\n\n    def setUp(self):\n        self.flags = None\n\n    def init_flags(self, xp):\n        if self.order == 'non-contiguous':\n            a = xp.empty(self.shape)[::2]\n        else:\n            a = xp.empty(self.shape, order=self.order)\n        self.flags = a.flags\n\n    @testing.numpy_cupy_equal()\n    def test_fnc(self, xp):\n        self.init_flags(xp)\n        return self.flags.fnc\n\n    @testing.numpy_cupy_equal()\n    def test_forc(self, xp):\n        self.init_flags(xp)\n        return self.flags.forc\n\n    @testing.numpy_cupy_equal()\n    def test_f_contiguous(self, xp):\n        self.init_flags(xp)\n        return self.flags.f_contiguous\n\n    @testing.numpy_cupy_equal()\n    def test_c_contiguous(self, xp):\n        self.init_flags(xp)\n        return self.flags.c_contiguous\n"""
tests/cupy_tests/core_tests/test_function.py,0,"b'import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy.cuda import compiler\nfrom cupy import testing\n\n\ndef _compile_func(kernel_name, code):\n    mod = compiler.compile_with_cache(code)\n    return mod.get_function(kernel_name)\n\n\n@testing.gpu\nclass TestFunction(unittest.TestCase):\n\n    def test_python_scalar(self):\n        code = \'\'\'\nextern ""C"" __global__ void test_kernel(const double* a, double b, double* x) {\n  int i = threadIdx.x;\n  x[i] = a[i] + b;\n}\n\'\'\'\n\n        a_cpu = numpy.arange(24, dtype=numpy.float64).reshape((4, 6))\n        a = cupy.array(a_cpu)\n        b = float(2)\n        x = cupy.empty_like(a)\n\n        func = _compile_func(\'test_kernel\', code)\n\n        func.linear_launch(a.size, (a, b, x))\n\n        expected = a_cpu + b\n        testing.assert_array_equal(x, expected)\n\n    def test_numpy_scalar(self):\n        code = \'\'\'\nextern ""C"" __global__ void test_kernel(const double* a, double b, double* x) {\n  int i = threadIdx.x;\n  x[i] = a[i] + b;\n}\n\'\'\'\n\n        a_cpu = numpy.arange(24, dtype=numpy.float64).reshape((4, 6))\n        a = cupy.array(a_cpu)\n        b = numpy.float64(2)\n        x = cupy.empty_like(a)\n\n        func = _compile_func(\'test_kernel\', code)\n\n        func.linear_launch(a.size, (a, b, x))\n\n        expected = a_cpu + b\n        testing.assert_array_equal(x, expected)\n'"
tests/cupy_tests/core_tests/test_internal.py,0,"b""import math\n\nimport unittest\n\nfrom cupy.core import internal\nfrom cupy import testing\n\n\nclass TestProd(unittest.TestCase):\n\n    def test_empty(self):\n        self.assertEqual(internal.prod([]), 1)\n\n    def test_one(self):\n        self.assertEqual(internal.prod([2]), 2)\n\n    def test_two(self):\n        self.assertEqual(internal.prod([2, 3]), 6)\n\n\nclass TestProdSequence(unittest.TestCase):\n\n    def test_empty(self):\n        self.assertEqual(internal.prod_sequence(()), 1)\n\n    def test_one(self):\n        self.assertEqual(internal.prod_sequence((2,)), 2)\n\n    def test_two(self):\n        self.assertEqual(internal.prod_sequence((2, 3)), 6)\n\n\nclass TestGetSize(unittest.TestCase):\n\n    def test_none(self):\n        self.assertEqual(internal.get_size(None), ())\n\n    def check_collection(self, a):\n        self.assertEqual(internal.get_size(a), tuple(a))\n\n    def test_list(self):\n        self.check_collection([1, 2, 3])\n\n    def test_tuple(self):\n        self.check_collection((1, 2, 3))\n\n    def test_int(self):\n        self.assertEqual(internal.get_size(1), (1,))\n\n    def test_invalid(self):\n        with self.assertRaises(ValueError):\n            internal.get_size(1.0)\n\n\nclass TestVectorEqual(unittest.TestCase):\n\n    def test_empty(self):\n        self.assertEqual(internal.vector_equal([], []), True)\n\n    def test_not_equal(self):\n        self.assertEqual(internal.vector_equal([1, 2, 3], [1, 2, 0]), False)\n\n    def test_equal(self):\n        self.assertEqual(internal.vector_equal([-1, 0, 1], [-1, 0, 1]), True)\n\n    def test_different_size(self):\n        self.assertEqual(internal.vector_equal([1, 2, 3], [1, 2]), False)\n\n\nclass TestGetCContiguity(unittest.TestCase):\n\n    def test_zero_in_shape(self):\n        self.assertTrue(internal.get_c_contiguity((1, 0, 1), (1, 1, 1), 3))\n\n    def test_all_one_shape(self):\n        self.assertTrue(internal.get_c_contiguity((1, 1, 1), (1, 1, 1), 3))\n\n    def test_normal1(self):\n        self.assertTrue(internal.get_c_contiguity((3, 4, 3), (24, 6, 2), 2))\n\n    def test_normal2(self):\n        self.assertTrue(internal.get_c_contiguity((3, 1, 3), (6, 100, 2), 2))\n\n    def test_normal3(self):\n        self.assertTrue(internal.get_c_contiguity((3,), (4, ), 4))\n\n    def test_normal4(self):\n        self.assertTrue(internal.get_c_contiguity((), (), 4))\n\n    def test_normal5(self):\n        self.assertTrue(internal.get_c_contiguity((3, 1), (4, 8), 4))\n\n    def test_no_contiguous1(self):\n        self.assertFalse(internal.get_c_contiguity((3, 4, 3), (30, 6, 2), 2))\n\n    def test_no_contiguous2(self):\n        self.assertFalse(internal.get_c_contiguity((3, 1, 3), (24, 6, 2), 2))\n\n    def test_no_contiguous3(self):\n        self.assertFalse(internal.get_c_contiguity((3, 1, 3), (6, 6, 4), 2))\n\n\nclass TestInferUnknownDimension(unittest.TestCase):\n\n    def test_known_all(self):\n        self.assertEqual(internal.infer_unknown_dimension((1, 2, 3), 6),\n                         [1, 2, 3])\n\n    def test_multiple_unknown(self):\n        with self.assertRaises(ValueError):\n            internal.infer_unknown_dimension((-1, 1, -1), 10)\n\n    def test_infer(self):\n        self.assertEqual(internal.infer_unknown_dimension((-1, 2, 3), 12),\n                         [2, 2, 3])\n\n\n@testing.parameterize(\n    {'slice': (2, 8, 1),    'expect': (2, 8, 1)},\n    {'slice': (2, None, 1), 'expect': (2, 10, 1)},\n    {'slice': (2, 1, 1),    'expect': (2, 2, 1)},\n    {'slice': (2, -1, 1),   'expect': (2, 9, 1)},\n\n    {'slice': (None, 8, 1),  'expect': (0, 8, 1)},\n    {'slice': (-3, 8, 1),    'expect': (7, 8, 1)},\n    {'slice': (11, 8, 1),    'expect': (10, 10, 1)},\n    {'slice': (11, 11, 1),   'expect': (10, 10, 1)},\n    {'slice': (-11, 8, 1),   'expect': (0, 8, 1)},\n    {'slice': (-11, -11, 1), 'expect': (0, 0, 1)},\n\n    {'slice': (8, 2, -1),    'expect': (8, 2, -1)},\n    {'slice': (8, None, -1), 'expect': (8, -1, -1)},\n    {'slice': (8, 9, -1),    'expect': (8, 8, -1)},\n    {'slice': (8, -3, -1),   'expect': (8, 7, -1)},\n\n    {'slice': (None, 8, -1),  'expect': (9, 8, -1)},\n    {'slice': (-3, 6, -1),    'expect': (7, 6, -1)},\n\n    {'slice': (10, 10, -1),   'expect': (9, 9, -1)},\n    {'slice': (10, 8, -1),    'expect': (9, 8, -1)},\n    {'slice': (9, 10, -1),    'expect': (9, 9, -1)},\n    {'slice': (9, 9, -1),     'expect': (9, 9, -1)},\n    {'slice': (9, 8, -1),     'expect': (9, 8, -1)},\n    {'slice': (8, 8, -1),     'expect': (8, 8, -1)},\n    {'slice': (-9, -8, -1),   'expect': (1, 1, -1)},\n    {'slice': (-9, -9, -1),   'expect': (1, 1, -1)},\n    {'slice': (-9, -10, -1),  'expect': (1, 0, -1)},\n    {'slice': (-9, -11, -1),  'expect': (1, -1, -1)},\n    {'slice': (-9, -12, -1),  'expect': (1, -1, -1)},\n    {'slice': (-10, -9, -1),  'expect': (0, 0, -1)},\n    {'slice': (-10, -10, -1), 'expect': (0, 0, -1)},\n    {'slice': (-10, -11, -1), 'expect': (0, -1, -1)},\n    {'slice': (-10, -12, -1), 'expect': (0, -1, -1)},\n    {'slice': (-11, 8, -1),   'expect': (-1, -1, -1)},\n    {'slice': (-11, -9, -1),  'expect': (-1, -1, -1)},\n    {'slice': (-11, -10, -1), 'expect': (-1, -1, -1)},\n    {'slice': (-11, -11, -1), 'expect': (-1, -1, -1)},\n    {'slice': (-11, -12, -1), 'expect': (-1, -1, -1)},\n)\nclass TestCompleteSlice(unittest.TestCase):\n\n    def test_complete_slice(self):\n        self.assertEqual(\n            internal.complete_slice(slice(*self.slice), 10),\n            slice(*self.expect))\n\n\nclass TestCompleteSliceError(unittest.TestCase):\n\n    def test_invalid_step_value(self):\n        with self.assertRaises(ValueError):\n            internal.complete_slice(slice(1, 1, 0), 1)\n\n    def test_invalid_step_type(self):\n        with self.assertRaises(TypeError):\n            internal.complete_slice(slice(1, 1, (1, 2)), 1)\n\n    def test_invalid_start_type(self):\n        with self.assertRaises(TypeError):\n            internal.complete_slice(slice((1, 2), 1, 1), 1)\n        with self.assertRaises(TypeError):\n            internal.complete_slice(slice((1, 2), 1, -1), 1)\n\n    def test_invalid_stop_type(self):\n        with self.assertRaises(TypeError):\n            internal.complete_slice(slice((1, 2), 1, 1), 1)\n        with self.assertRaises(TypeError):\n            internal.complete_slice(slice((1, 2), 1, -1), 1)\n\n\n@testing.parameterize(\n    {'x': 0, 'expect': 0},\n    {'x': 1, 'expect': 1},\n    {'x': 2, 'expect': 2},\n    {'x': 3, 'expect': 4},\n    {'x': 2 ** 10,     'expect': 2 ** 10},\n    {'x': 2 ** 10 - 1, 'expect': 2 ** 10},\n    {'x': 2 ** 10 + 1, 'expect': 2 ** 11},\n    {'x': 2 ** 40,     'expect': 2 ** 40},\n    {'x': 2 ** 40 - 1, 'expect': 2 ** 40},\n    {'x': 2 ** 40 + 1, 'expect': 2 ** 41},\n)\nclass TestClp2(unittest.TestCase):\n\n    def test_clp2(self):\n        assert internal.clp2(self.x) == self.expect\n\n\n@testing.parameterize(*testing.product({\n    'value': [0.0, 1.0, -1.0,\n              0.25, -0.25,\n              11.0, -11.0,\n              2 ** -15, -(2 ** -15),  # Denormalized Number\n              float('inf'), float('-inf')],\n}))\nclass TestConvertFloat16(unittest.TestCase):\n\n    def test_conversion(self):\n        half = internal.to_float16(self.value)\n        assert internal.from_float16(half) == self.value\n\n\nclass TestConvertFloat16Nan(unittest.TestCase):\n\n    def test_conversion(self):\n        half = internal.to_float16(float('nan'))\n        assert math.isnan(internal.from_float16(half))\n"""
tests/cupy_tests/core_tests/test_iter.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\n@testing.parameterize(*testing.product(\n    {'shape': [(3,), (2, 3, 4), (0,), (0, 2), (3, 0)]},\n))\nclass TestIter(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test_list(self, xp, dtype):\n        x = testing.shaped_arange(self.shape, xp, dtype)\n        return list(x)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_len(self, xp, dtype):\n        x = testing.shaped_arange(self.shape, xp, dtype)\n        return len(x)\n\n\n@testing.gpu\nclass TestIterInvalid(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_iter(self, dtype):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((), xp, dtype)\n            with pytest.raises(TypeError):\n                iter(x)\n\n    @testing.for_all_dtypes()\n    def test_len(self, dtype):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((), xp, dtype)\n            with pytest.raises(TypeError):\n                len(x)\n"""
tests/cupy_tests/core_tests/test_ndarray.py,0,"b""import copy\nimport unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import core\nfrom cupy import cuda\nfrom cupy import get_array_module\nfrom cupy import testing\n\n\nclass TestGetSize(unittest.TestCase):\n\n    def test_none(self):\n        assert core.get_size(None) == ()\n\n    def check_collection(self, a):\n        assert core.get_size(a) == tuple(a)\n\n    def test_list(self):\n        self.check_collection([1, 2, 3])\n\n    def test_tuple(self):\n        self.check_collection((1, 2, 3))\n\n    def test_int(self):\n        assert core.get_size(1) == (1,)\n\n    def test_float(self):\n        with pytest.raises(ValueError):\n            core.get_size(1.0)\n\n\ndef wrap_take(array, *args, **kwargs):\n    if get_array_module(array) == numpy:\n        kwargs['mode'] = 'wrap'\n\n    return array.take(*args, **kwargs)\n\n\n@testing.gpu\nclass TestNdarrayInit(unittest.TestCase):\n\n    def test_shape_none(self):\n        a = cupy.ndarray(None)\n        assert a.shape == ()\n\n    def test_shape_int(self):\n        a = cupy.ndarray(3)\n        assert a.shape == (3,)\n\n    def test_shape_int_with_strides(self):\n        dummy = cupy.ndarray(3)\n        a = cupy.ndarray(3, strides=(0,), memptr=dummy.data)\n        assert a.shape == (3,)\n        assert a.strides == (0,)\n\n    def test_memptr(self):\n        a = cupy.arange(6).astype(numpy.float32).reshape((2, 3))\n        memptr = a.data\n\n        b = cupy.ndarray((2, 3), numpy.float32, memptr)\n        testing.assert_array_equal(a, b)\n\n        b += 1\n        testing.assert_array_equal(a, b)\n\n    def test_memptr_with_strides(self):\n        buf = cupy.ndarray(20, numpy.uint8)\n        memptr = buf.data\n\n        # self-overlapping strides\n        a = cupy.ndarray((2, 3), numpy.float32, memptr, strides=(8, 4))\n        assert a.strides == (8, 4)\n\n        a[:] = 1\n        a[0, 2] = 4\n        assert float(a[1, 0]) == 4\n\n    def test_strides_without_memptr(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.ndarray((2, 3), numpy.float32, strides=(20, 4))\n\n    def test_strides_is_given_and_order_is_ignored(self):\n        buf = cupy.ndarray(20, numpy.uint8)\n        a = cupy.ndarray(\n            (2, 3), numpy.float32, buf.data, strides=(8, 4), order='C')\n        assert a.strides == (8, 4)\n\n    def test_strides_is_given_but_order_is_invalid(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(TypeError):\n                xp.ndarray((2, 3), numpy.float32, strides=(8, 4), order='!')\n\n    def test_order(self):\n        shape = (2, 3, 4)\n        a = core.ndarray(shape, order='F')\n        a_cpu = numpy.ndarray(shape, order='F')\n        assert a.strides == a_cpu.strides\n        assert a.flags.f_contiguous\n        assert not a.flags.c_contiguous\n\n    def test_order_none(self):\n        shape = (2, 3, 4)\n        a = core.ndarray(shape, order=None)\n        a_cpu = numpy.ndarray(shape, order=None)\n        assert a.flags.c_contiguous == a_cpu.flags.c_contiguous\n        assert a.flags.f_contiguous == a_cpu.flags.f_contiguous\n        assert a.strides == a_cpu.strides\n\n\n@testing.parameterize(\n    *testing.product({\n        'shape': [(), (1,), (1, 2), (1, 2, 3)],\n        'order': ['C', 'F'],\n        'dtype': [\n            numpy.uint8,  # itemsize=1\n            numpy.uint16,  # itemsize=2\n        ],\n    }))\n@testing.gpu\nclass TestNdarrayInitStrides(unittest.TestCase):\n\n    # Check the strides given shape, itemsize and order.\n\n    @testing.numpy_cupy_equal()\n    def test_strides(self, xp):\n        arr = xp.ndarray(self.shape, dtype=self.dtype, order=self.order)\n        return (\n            arr.strides,\n            arr.flags.c_contiguous,\n            arr.flags.f_contiguous)\n\n\n@testing.gpu\nclass TestNdarrayInitRaise(unittest.TestCase):\n\n    def test_unsupported_type(self):\n        arr = numpy.ndarray((2, 3), dtype=object)\n        with pytest.raises(ValueError):\n            core.array(arr)\n\n\n@testing.parameterize(\n    *testing.product({\n        'shape': [(), (0,), (1,), (0, 0, 2), (2, 3)],\n    })\n)\n@testing.gpu\nclass TestNdarrayDeepCopy(unittest.TestCase):\n\n    def _check_deepcopy(self, arr, arr2):\n        assert arr.data is not arr2.data\n        assert arr.shape == arr2.shape\n        assert arr.size == arr2.size\n        assert arr.dtype == arr2.dtype\n        assert arr.strides == arr2.strides\n        testing.assert_array_equal(arr, arr2)\n\n    def test_deepcopy(self):\n        arr = core.ndarray(self.shape)\n        arr2 = copy.deepcopy(arr)\n        self._check_deepcopy(arr, arr2)\n\n    @testing.multi_gpu(2)\n    def test_deepcopy_multi_device(self):\n        arr = core.ndarray(self.shape)\n        with cuda.Device(1):\n            arr2 = copy.deepcopy(arr)\n        self._check_deepcopy(arr, arr2)\n        assert arr2.device == arr.device\n\n\n@testing.gpu\nclass TestNdarrayCopy(unittest.TestCase):\n\n    @testing.multi_gpu(2)\n    @testing.for_orders('CFA')\n    def test_copy_multi_device_non_contiguous(self, order):\n        arr = core.ndarray((20,))[::2]\n        dev1 = cuda.Device(1)\n        with dev1:\n            arr2 = arr.copy(order)\n        assert arr2.device == dev1\n        testing.assert_array_equal(arr, arr2)\n\n    @testing.multi_gpu(2)\n    def test_copy_multi_device_non_contiguous_K(self):\n        arr = core.ndarray((20,))[::2]\n        with cuda.Device(1):\n            with self.assertRaises(NotImplementedError):\n                arr.copy('K')\n\n\n@testing.gpu\nclass TestNdarrayShape(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_shape_set(self, xp):\n        arr = xp.ndarray((2, 3))\n        arr.shape = (3, 2)\n        return xp.array(arr.shape)\n\n    @testing.numpy_cupy_array_equal()\n    def test_shape_set_infer(self, xp):\n        arr = xp.ndarray((2, 3))\n        arr.shape = (3, -1)\n        return xp.array(arr.shape)\n\n    @testing.numpy_cupy_array_equal()\n    def test_shape_set_int(self, xp):\n        arr = xp.ndarray((2, 3))\n        arr.shape = 6\n        return xp.array(arr.shape)\n\n\nclass TestNdarrayCudaInterface(unittest.TestCase):\n\n    def test_cuda_array_interface(self):\n        arr = cupy.zeros(shape=(2, 3), dtype=cupy.float64)\n        iface = arr.__cuda_array_interface__\n        assert (set(iface.keys()) ==\n                set(['shape', 'typestr', 'data', 'version', 'descr',\n                     'strides']))\n        assert iface['shape'] == (2, 3)\n        assert iface['typestr'] == '<f8'\n        assert isinstance(iface['data'], tuple)\n        assert len(iface['data']) == 2\n        assert iface['data'][0] == arr.data.ptr\n        assert not iface['data'][1]\n        assert iface['version'] == 2\n        assert iface['descr'] == [('', '<f8')]\n        assert iface['strides'] is None\n\n    def test_cuda_array_interface_view(self):\n        arr = cupy.zeros(shape=(10, 20), dtype=cupy.float64)\n        view = arr[::2, ::5]\n        iface = view.__cuda_array_interface__\n        assert (set(iface.keys()) ==\n                set(['shape', 'typestr', 'data', 'version',\n                     'strides', 'descr']))\n        assert iface['shape'] == (5, 4)\n        assert iface['typestr'] == '<f8'\n        assert isinstance(iface['data'], tuple)\n        assert len(iface['data']) == 2\n        assert iface['data'][0] == arr.data.ptr\n        assert not iface['data'][1]\n        assert iface['version'] == 2\n        assert iface['strides'] == (320, 40)\n        assert iface['descr'] == [('', '<f8')]\n\n    def test_cuda_array_interface_zero_size(self):\n        arr = cupy.zeros(shape=(10,), dtype=cupy.float64)\n        view = arr[0:3:-1]\n        iface = view.__cuda_array_interface__\n        assert (set(iface.keys()) ==\n                set(['shape', 'typestr', 'data', 'version',\n                     'strides', 'descr']))\n        assert iface['shape'] == (0,)\n        assert iface['typestr'] == '<f8'\n        assert isinstance(iface['data'], tuple)\n        assert len(iface['data']) == 2\n        assert iface['data'][0] == 0\n        assert not iface['data'][1]\n        assert iface['version'] == 2\n        assert iface['strides'] is None\n        assert iface['descr'] == [('', '<f8')]\n\n\n@testing.parameterize(\n    *testing.product({\n        'indices_shape': [(2,), (2, 3)],\n        'axis': [None, 0, 1, 2, -1, -2],\n    })\n)\n@testing.gpu\nclass TestNdarrayTake(unittest.TestCase):\n\n    shape = (3, 4, 5)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_take(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        if self.axis is None:\n            m = a.size\n        else:\n            m = a.shape[self.axis]\n        i = testing.shaped_arange(self.indices_shape, xp, numpy.int32) % m\n        return wrap_take(a, i, self.axis)\n\n\n@testing.parameterize(\n    *testing.product({\n        'indices': [2, [0, 1], -1, [-1, -2]],\n        'axis': [None, 0, 1, -1, -2],\n    })\n)\n@testing.gpu\nclass TestNdarrayTakeWithInt(unittest.TestCase):\n\n    shape = (3, 4, 5)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_take(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        return wrap_take(a, self.indices, self.axis)\n\n\n@testing.parameterize(\n    *testing.product({\n        'indices': [2, [0, 1], -1, [-1, -2]],\n        'axis': [None, 0, 1, -1, -2],\n    })\n)\n@testing.gpu\nclass TestNdarrayTakeWithIntWithOutParam(unittest.TestCase):\n\n    shape = (3, 4, 5)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_take(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        r1 = wrap_take(a, self.indices, self.axis)\n        r2 = xp.zeros_like(r1)\n        wrap_take(a, self.indices, self.axis, out=r2)\n        testing.assert_array_equal(r1, r2)\n        return r2\n\n\n@testing.parameterize(\n    *testing.product({\n        'indices': [0, -1, [0], [0, -1]],\n        'axis': [None, 0, -1],\n    })\n)\n@testing.gpu\nclass TestScalaNdarrayTakeWithIntWithOutParam(unittest.TestCase):\n\n    shape = ()\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_take(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        r1 = wrap_take(a, self.indices, self.axis)\n        r2 = xp.zeros_like(r1)\n        wrap_take(a, self.indices, self.axis, out=r2)\n        testing.assert_array_equal(r1, r2)\n        return r2\n\n\n@testing.parameterize(\n    {'shape': (3, 4, 5), 'indices': (2,), 'axis': 3},\n    {'shape': (), 'indices': (0,), 'axis': 2}\n)\n@testing.gpu\nclass TestNdarrayTakeErrorAxisOverRun(unittest.TestCase):\n\n    def test_axis_overrun1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange(self.shape, xp)\n            with pytest.raises(numpy.AxisError):\n                wrap_take(a, self.indices, axis=self.axis)\n\n    def test_axis_overrun2(self):\n        a = testing.shaped_arange(self.shape, cupy)\n        with pytest.raises(numpy.AxisError):\n            wrap_take(a, self.indices, axis=self.axis)\n\n\n@testing.parameterize(\n    {'shape': (3, 4, 5), 'indices': (2, 3), 'out_shape': (2, 4)},\n    {'shape': (), 'indices': (), 'out_shape': (1,)}\n)\n@testing.gpu\nclass TestNdarrayTakeErrorShapeMismatch(unittest.TestCase):\n\n    def test_shape_mismatch(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange(self.shape, xp)\n            i = testing.shaped_arange(self.indices, xp, numpy.int32) % 3\n            o = testing.shaped_arange(self.out_shape, xp)\n            with pytest.raises(ValueError):\n                wrap_take(a, i, out=o)\n\n\n@testing.parameterize(\n    {'shape': (3, 4, 5), 'indices': (2, 3), 'out_shape': (2, 3)},\n    {'shape': (), 'indices': (), 'out_shape': ()}\n)\n@testing.gpu\nclass TestNdarrayTakeErrorTypeMismatch(unittest.TestCase):\n\n    def test_output_type_mismatch(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange(self.shape, xp, numpy.int32)\n            i = testing.shaped_arange(self.indices, xp, numpy.int32) % 3\n            o = testing.shaped_arange(self.out_shape, xp, numpy.float32)\n            with pytest.raises(TypeError):\n                wrap_take(a, i, out=o)\n\n\n@testing.parameterize(\n    {'shape': (0,), 'indices': (0,)},\n    {'shape': (0,), 'indices': (0, 1)},\n)\n@testing.gpu\nclass TestZeroSizedNdarrayTake(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_output_type_mismatch(self, xp):\n        a = testing.shaped_arange(self.shape, xp, numpy.int32)\n        i = testing.shaped_arange(self.indices, xp, numpy.int32)\n        return wrap_take(a, i)\n\n\n@testing.parameterize(\n    {'shape': (0,), 'indices': (1,)},\n    {'shape': (0,), 'indices': (1, 1)},\n)\n@testing.gpu\nclass TestZeroSizedNdarrayTakeIndexError(unittest.TestCase):\n\n    def test_output_type_mismatch(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange(self.shape, xp, numpy.int32)\n            i = testing.shaped_arange(self.indices, xp, numpy.int32)\n            with pytest.raises(IndexError):\n                wrap_take(a, i)\n\n\n@testing.gpu\nclass TestSize(unittest.TestCase):\n\n    @testing.numpy_cupy_equal()\n    def test_size_without_axis(self, xp):\n        x = testing.shaped_arange((3, 4, 5), xp, numpy.int32)\n        return xp.size(x)\n\n    @testing.numpy_cupy_equal()\n    def test_size_with_axis(self, xp):\n        x = testing.shaped_arange((3, 4, 5), xp, numpy.int32)\n        return xp.size(x, 0)\n\n    @testing.numpy_cupy_equal()\n    def test_size_with_negative_axis(self, xp):\n        x = testing.shaped_arange((3, 4, 5), xp, numpy.int32)\n        return xp.size(x, -1)\n\n    @testing.numpy_cupy_equal()\n    def test_size_zero_dim_array(self, xp):\n        x = testing.shaped_arange((), xp, numpy.int32)\n        return xp.size(x)\n\n    def test_size_axis_too_large(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((3, 4, 5), xp, numpy.int32)\n            with pytest.raises(IndexError):\n                xp.size(x, 3)\n\n    def test_size_axis_too_small(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((3, 4, 5), xp, numpy.int32)\n            with pytest.raises(IndexError):\n                xp.size(x, -4)\n\n    def test_size_zero_dim_array_with_axis(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((), xp, numpy.int32)\n            with pytest.raises(IndexError):\n                xp.size(x, 0)\n\n\n@testing.gpu\nclass TestPythonInterface(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_bytes_tobytes(self, xp, dtype):\n        x = testing.shaped_arange((3, 4, 5), xp, dtype)\n        return bytes(x)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_bytes_tobytes_empty(self, xp, dtype):\n        x = xp.empty((3, 4, 5), dtype)\n        return bytes(x)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_bytes_tobytes_empty2(self, xp, dtype):\n        x = xp.empty((), dtype)\n        return bytes(x)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_bytes_tobytes_scalar(self, xp, dtype):\n        x = xp.array([3], dtype).item()\n        return bytes(x)\n"""
tests/cupy_tests/core_tests/test_ndarray_adv_indexing.py,0,"b""import unittest\n\nimport itertools\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\ndef perm(iterable):\n    return list(itertools.permutations(iterable))\n\n\n@testing.parameterize(\n    *testing.product({\n        'shape': [(4, 4, 4)],\n        'indexes': (\n            perm(([1, 0], slice(None))) +\n            perm(([1, 0], Ellipsis)) +\n            perm(([1, 2], None, slice(None))) +\n            perm(([1, 0], 1, slice(None))) +\n            perm(([1, 2], slice(0, 2), slice(None))) +\n            perm((1, [1, 2], 1)) +\n            perm(([[1, -1], [0, 3]], slice(None), slice(None))) +\n            perm(([1, 0], [3, 2], slice(None))) +\n            perm((slice(0, 3, 2), [1, 2], [1, 0])) +\n            perm(([1, 0], [2, 1], [3, 1])) +\n            perm(([1, 0], 1, [3, 1])) +\n            perm(([1, 2], [[1, 0], [0, 1], [-1, 1]], slice(None))) +\n            perm((None, [1, 2], [1, 0])) +\n            perm((numpy.array(0), numpy.array(-1))) +\n            perm((numpy.array(0), None)) +\n            perm((1, numpy.array(2), slice(None)))\n        )\n    })\n)\n@testing.gpu\nclass TestArrayAdvancedIndexingGetitemPerm(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_adv_getitem(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        return a[self.indexes]\n\n\n@testing.parameterize(\n    {'shape': (2, 3, 4), 'indexes': numpy.array(-1)},\n    {'shape': (2, 3, 4), 'indexes': (None, [1, 0], [0, 2], slice(None))},\n    {'shape': (2, 3, 4), 'indexes': (None, [0, 1], None, [2, 1], slice(None))},\n    {'shape': (2, 3, 4), 'indexes': numpy.array([1, 0])},\n    {'shape': (2, 3, 4), 'indexes': [1, -1]},\n    {'shape': (2, 3, 4), 'indexes': ([0, 1], slice(None), [[2, 1], [3, 1]])},\n    # mask\n    {'shape': (10,), 'indexes': (numpy.random.choice([False, True], (10,)),)},\n    {'shape': (2, 3, 4),\n     'indexes': (numpy.random.choice([False, True], (2, 3, 4)),)},\n    {'shape': (2, 3, 4),\n     'indexes': (slice(None), numpy.array([True, False, True]))},\n    {'shape': (2, 3, 4),\n     'indexes': (slice(None), slice(None),\n                 numpy.array([True, False, False, True]))},\n    {'shape': (2, 3, 4),\n     'indexes': (slice(None), numpy.random.choice([False, True], (3, 4)))},\n    {'shape': (2, 3, 4),\n     'indexes': numpy.random.choice([False, True], (2, 3))},\n    # empty arrays\n    {'shape': (2, 3, 4), 'indexes': []},\n    {'shape': (2, 3, 4), 'indexes': numpy.array([], dtype=numpy.int32)},\n    {'shape': (2, 3, 4), 'indexes': [[]]},\n    {'shape': (2, 3, 4), 'indexes': numpy.array([[]], dtype=numpy.int32)},\n    {'shape': (2, 3, 4), 'indexes': [[[]]]},\n    {'shape': (2, 3, 4), 'indexes': [[[[]]]]},\n    {'shape': (2, 3, 4, 5), 'indexes': [[[[]]]]},\n    {'shape': (2, 3, 4, 5), 'indexes': [[[[[]]]]]},\n    {'shape': (2, 3, 4), 'indexes': (slice(None), [])},\n    {'shape': (2, 3, 4), 'indexes': ([], [])},\n    {'shape': (2, 3, 4), 'indexes': ([[]],)},\n    {'shape': (2, 3, 4), 'indexes': numpy.array([], dtype=numpy.bool)},\n    {'shape': (2, 3, 4),\n     'indexes': (slice(None), numpy.array([], dtype=numpy.bool))},\n    {'shape': (2, 3, 4), 'indexes': numpy.array([[], []], dtype=numpy.bool)},\n    # list indexes\n    {'shape': (2, 3, 4), 'indexes': [1]},\n    {'shape': (2, 3, 4), 'indexes': [1, 1]},\n    {'shape': (2, 3, 4), 'indexes': [[1]]},\n    {'shape': (2, 3, 4), 'indexes': [[1, 1]]},\n    {'shape': (2, 3, 4), 'indexes': [[1], [1]]},\n    {'shape': (2, 3, 4), 'indexes': [[1, 1], 1]},\n    {'shape': (2, 3, 4), 'indexes': [[1], slice(1, 2)]},\n    {'shape': (2, 3, 4), 'indexes': [[[1]], slice(1, 2)]},\n    # TODO(okuta): pass the following commented out tests\n    # {'shape': (2, 3, 4), 'indexes': (True, [True, False])},\n    # {'shape': (2, 3, 4), 'indexes': (False, [True, False])},\n    # {'shape': (2, 3, 4), 'indexes': (True, [[1]], slice(1, 2))},\n    # {'shape': (2, 3, 4), 'indexes': (False, [[1]], slice(1, 2))},\n    # {'shape': (2, 3, 4), 'indexes': (True, [[1]], slice(1, 2), True)},\n    # {'shape': (2, 3, 4), 'indexes': (True, [[1]], slice(1, 2), False)},\n    # zero-dim and zero-sized arrays\n    {'shape': (), 'indexes': Ellipsis},\n    {'shape': (), 'indexes': ()},\n    {'shape': (), 'indexes': None},\n    {'shape': (), 'indexes': True},\n    {'shape': (), 'indexes': (True,)},\n    # TODO(niboshi): pass the following commented out tests\n    # {'shape': (), 'indexes': (False, True, True)},\n    {'shape': (), 'indexes': numpy.ones((), dtype=numpy.bool_)},\n    {'shape': (), 'indexes': numpy.zeros((), dtype=numpy.bool_)},\n    {'shape': (0,), 'indexes': None},\n    {'shape': (0,), 'indexes': ()},\n    # TODO(niboshi): pass the following commented out tests\n    # {'shape': (0,), 'indexes': (False, True, True)},\n)\n@testing.gpu\nclass TestArrayAdvancedIndexingGetitemParametrized(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_adv_getitem(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        return a[self.indexes]\n\n\n@testing.parameterize(\n    {'shape': (0,), 'indexes': True},\n    {'shape': (0,), 'indexes': (True,)},\n    {'shape': (0,), 'indexes': numpy.ones((), dtype=numpy.bool_)},\n    {'shape': (0,), 'indexes': numpy.zeros((), dtype=numpy.bool_)},\n)\n@testing.gpu\nclass TestArrayAdvancedIndexingGetitemParametrized2(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_adv_getitem(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        return a[self.indexes]\n\n\n@testing.parameterize(\n    {'shape': (2, 3, 4), 'transpose': (1, 2, 0),\n     'indexes': (slice(None), [1, 0])},\n    {'shape': (2, 3, 4), 'transpose': (1, 0, 2),\n     'indexes': (None, [1, 2], [0, -1])},\n)\n@testing.gpu\nclass TestArrayAdvancedIndexingGetitemParametrizedTransp(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_adv_getitem(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        if self.transpose:\n            a = a.transpose(self.transpose)\n        return a[self.indexes]\n\n\n@testing.gpu\nclass TestArrayAdvancedIndexingGetitemCupyIndices(unittest.TestCase):\n\n    shape = (2, 3, 4)\n\n    def test_adv_getitem_cupy_indices1(self):\n        a = cupy.zeros(self.shape)\n        index = cupy.array([1, 0])\n        original_index = index.copy()\n        b = a[index]\n        b_cpu = a.get()[index.get()]\n        testing.assert_array_equal(b, b_cpu)\n        testing.assert_array_equal(original_index, index)\n\n    def test_adv_getitem_cupy_indices2(self):\n        a = cupy.zeros(self.shape)\n        index = cupy.array([1, 0])\n        original_index = index.copy()\n        b = a[(slice(None), index)]\n        b_cpu = a.get()[(slice(None), index.get())]\n        testing.assert_array_equal(b, b_cpu)\n        testing.assert_array_equal(original_index, index)\n\n    def test_adv_getitem_cupy_indices3(self):\n        a = cupy.zeros(self.shape)\n        index = cupy.array([True, False])\n        original_index = index.copy()\n        b = a[index]\n        b_cpu = a.get()[index.get()]\n        testing.assert_array_equal(b, b_cpu)\n        testing.assert_array_equal(original_index, index)\n\n    def test_adv_getitem_cupy_indices4(self):\n        a = cupy.zeros(self.shape)\n        index = cupy.array([4, -5])\n        original_index = index.copy()\n        b = a[index]\n        b_cpu = a.get()[index.get() % self.shape[1]]\n        testing.assert_array_equal(b, b_cpu)\n        testing.assert_array_equal(original_index, index)\n\n    def test_adv_getitem_cupy_indices5(self):\n        a = cupy.zeros(self.shape)\n        index = cupy.array([4, -5])\n        original_index = index.copy()\n        b = a[[1, 0], index]\n        b_cpu = a.get()[[1, 0], index.get() % self.shape[1]]\n        testing.assert_array_equal(b, b_cpu)\n        testing.assert_array_equal(original_index, index)\n\n\n@testing.parameterize(\n    {'shape': (2**3 + 1, 2**4), 'indexes': (\n        numpy.array([2**3], dtype=numpy.int8),\n        numpy.array([1], dtype=numpy.int8))},\n    {'shape': (2**4 + 1, 2**4), 'indexes': (\n        numpy.array([2**4], dtype=numpy.uint8),\n        numpy.array([1], dtype=numpy.uint8))},\n    {'shape': (2**7 + 1, 2**8), 'indexes': (\n        numpy.array([2**7], dtype=numpy.int16),\n        numpy.array([1], dtype=numpy.int16))},\n    {'shape': (2**8 + 1, 2**8), 'indexes': (\n        numpy.array([2**8], dtype=numpy.uint16),\n        numpy.array([1], dtype=numpy.uint16))},\n    {'shape': (2**7 + 1, 2**8), 'indexes': (\n        numpy.array([2**7], dtype=numpy.int16),\n        numpy.array([1], dtype=numpy.int32))},\n    {'shape': (2**7 + 1, 2**8), 'indexes': (\n        numpy.array([2**7], dtype=numpy.int16),\n        numpy.array([1], dtype=numpy.int8))},\n    # Three-dimensional case\n    {'shape': (2**3 + 1, 3, 2**4), 'indexes': (\n        numpy.array([2**3], dtype=numpy.int8),\n        slice(None),\n        numpy.array([1], dtype=numpy.int8))},\n)\n@testing.gpu\nclass TestArrayAdvancedIndexingOverflow(unittest.TestCase):\n\n    def test_getitem(self):\n        a = cupy.arange(numpy.prod(self.shape)).reshape(self.shape)\n        indexes_gpu = []\n        for s in self.indexes:\n            if isinstance(s, numpy.ndarray):\n                s = cupy.array(s)\n            indexes_gpu.append(s)\n        indexes_gpu = tuple(indexes_gpu)\n        b = a[indexes_gpu]\n        b_cpu = a.get()[self.indexes]\n        testing.assert_array_equal(b, b_cpu)\n\n    def test_setitem(self):\n        a_cpu = numpy.arange(numpy.prod(self.shape)).reshape(self.shape)\n        a = cupy.array(a_cpu)\n        indexes_gpu = []\n        for s in self.indexes:\n            if isinstance(s, numpy.ndarray):\n                s = cupy.array(s)\n            indexes_gpu.append(s)\n        indexes_gpu = tuple(indexes_gpu)\n        a[indexes_gpu] = -1\n        a_cpu[self.indexes] = -1\n        testing.assert_array_equal(a, a_cpu)\n\n\n@testing.parameterize(\n    {'shape': (), 'indexes': (-1,)},\n    {'shape': (), 'indexes': (0,)},\n    {'shape': (), 'indexes': (1,)},\n    {'shape': (), 'indexes': ([0],)},\n    {'shape': (), 'indexes': (numpy.array([0]),)},\n    {'shape': (), 'indexes': (numpy.array(0),)},\n    {'shape': (), 'indexes': numpy.array([True])},\n    {'shape': (), 'indexes': numpy.array([False, True, True])},\n    {'shape': (), 'indexes': ([False],)},\n    {'shape': (0,), 'indexes': (-1,)},\n    {'shape': (0,), 'indexes': (0,)},\n    {'shape': (0,), 'indexes': (1,)},\n    {'shape': (0,), 'indexes': ([0],)},\n    {'shape': (0,), 'indexes': (numpy.array([0]),)},\n    {'shape': (0,), 'indexes': (numpy.array(0),)},\n    {'shape': (0,), 'indexes': numpy.array([True])},\n    {'shape': (0,), 'indexes': numpy.array([False, True, True])},\n    {'shape': (0, 1), 'indexes': (0, Ellipsis)},\n    {'shape': (2, 3), 'indexes': (slice(None), [1, 2], slice(None))},\n    {'shape': (2, 3), 'indexes': numpy.array([], dtype=numpy.float)},\n)\n@testing.gpu\nclass TestArrayInvalidIndexAdvGetitem(unittest.TestCase):\n\n    def test_invalid_adv_getitem(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange(self.shape, xp)\n            with pytest.raises(IndexError):\n                a[self.indexes]\n\n\n@testing.parameterize(\n    {'shape': (0,), 'indexes': ([False],)},\n    {'shape': (2, 3, 4),\n     'indexes': (slice(None), numpy.random.choice([False, True], (3, 1)))},\n    {'shape': (2, 3, 4),\n     'indexes': numpy.random.choice([False, True], (1, 3))},\n)\n@testing.gpu\nclass TestArrayInvalidIndexAdvGetitem2(unittest.TestCase):\n\n    def test_invalid_adv_getitem(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange(self.shape, xp)\n            with pytest.raises(IndexError):\n                a[self.indexes]\n\n\n@testing.parameterize(\n    {'shape': (2, 3, 4), 'indexes': [1, [1, [1]]]},\n)\n@testing.gpu\n@testing.with_requires('numpy>=1.16')\nclass TestArrayInvalidValueAdvGetitem(unittest.TestCase):\n\n    def test_invalid_adv_getitem(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange(self.shape, xp)\n            with pytest.raises(IndexError):\n                a[self.indexes]\n\n\n@testing.parameterize(\n    # array only\n    {'shape': (2, 3, 4), 'indexes': numpy.array(-1), 'value': 1},\n    {'shape': (2, 3, 4), 'indexes': numpy.array([1, 0]), 'value': 1},\n    {'shape': (2, 3, 4), 'indexes': [1, -1], 'value': 1},\n    {'shape': (2, 3, 4), 'indexes': (slice(None), [1, 2]), 'value': 1},\n    {'shape': (2, 3, 4),\n     'indexes': (slice(None), [[1, 2], [0, -1]],), 'value': 1},\n    {'shape': (2, 3, 4),\n     'indexes': (slice(None), slice(None), [[1, 2], [0, -1]]), 'value': 1},\n    # slice and array\n    {'shape': (2, 3, 4),\n     'indexes': (slice(None), slice(1, 2), [[1, 2], [0, -1]]), 'value': 1},\n    # None and array\n    {'shape': (2, 3, 4),\n     'indexes': (None, [1, -1]), 'value': 1},\n    {'shape': (2, 3, 4),\n     'indexes': (None, [1, -1], None), 'value': 1},\n    {'shape': (2, 3, 4),\n     'indexes': (None, None, None, [1, -1]), 'value': 1},\n    # None, slice and array\n    {'shape': (2, 3, 4),\n     'indexes': (slice(0, 1), None, [1, -1]), 'value': 1},\n    {'shape': (2, 3, 4),\n     'indexes': (slice(0, 1), slice(1, 2), [1, -1]), 'value': 1},\n    {'shape': (2, 3, 4),\n     'indexes': (slice(0, 1), None, slice(1, 2), [1, -1]), 'value': 1},\n    # mask\n    {'shape': (2, 3, 4),\n     'indexes': numpy.array([True, False]), 'value': 1},\n    {'shape': (2, 3, 4),\n     'indexes': (slice(None), numpy.array([True, False, True])), 'value': 1},\n    {'shape': (2, 3, 4),\n     'indexes': (slice(None), slice(None),\n                 numpy.random.choice([False, True], (4,))),\n     'value': 1},\n    {'shape': (2, 3, 4),\n     'indexes': (numpy.random.choice([False, True], (2, 3)),), 'value': 1},\n    {'shape': (2, 3, 4),\n     'indexes': (slice(None), numpy.random.choice([False, True], (3, 4)),),\n     'value': 1},\n    {'shape': (2, 3, 4),\n     'indexes': (numpy.random.choice([False, True], (2, 3, 4)),), 'value': 1},\n    # multiple arrays\n    {'shape': (2, 3, 4), 'indexes': ([0, -1], [1, -1]), 'value': 1},\n    {'shape': (2, 3, 4),\n     'indexes': ([0, -1], [1, -1], [2, 1]), 'value': 1},\n    {'shape': (2, 3, 4), 'indexes': ([0, -1], 1), 'value': 1},\n    {'shape': (2, 3, 4), 'indexes': ([0, -1], slice(None), [1, -1]),\n     'value': 1},\n    {'shape': (2, 3, 4), 'indexes': ([0, -1], 1, 2), 'value': 1},\n    {'shape': (2, 3, 4), 'indexes': ([1, 0], slice(None), [[2, 0], [3, 1]]),\n     'value': 1},\n    # multiple arrays and basic indexing\n    {'shape': (2, 3, 4), 'indexes': ([0, -1], None, [1, 0]), 'value': 1},\n    {'shape': (2, 3, 4), 'indexes': ([0, -1], slice(0, 2), [1, 0]),\n     'value': 1},\n    {'shape': (2, 3, 4), 'indexes': ([0, -1], None, slice(0, 2), [1, 0]),\n     'value': 1},\n    {'shape': (1, 1, 2, 3, 4),\n     'indexes': (None, slice(None), slice(None), [1, 0], [2, -1], 1),\n     'value': 1},\n    {'shape': (1, 1, 2, 3, 4),\n     'indexes': (None, slice(None), 0, [1, 0], slice(0, 2, 2), [2, -1]),\n     'value': 1},\n    {'shape': (2, 3, 4),\n     'indexes': (slice(None), [0, -1], [[1, 0], [0, 1], [-1, 1]]), 'value': 1},\n    # empty arrays\n    {'shape': (2, 3, 4), 'indexes': [], 'value': 1},\n    {'shape': (2, 3, 4), 'indexes': [],\n     'value': numpy.array([1, 1, 1, 1])},\n    {'shape': (2, 3, 4), 'indexes': [],\n     'value': numpy.random.uniform(size=(3, 4))},\n    {'shape': (2, 3, 4), 'indexes': numpy.array([], dtype=numpy.int32),\n     'value': 1},\n    {'shape': (2, 3, 4), 'indexes': [[]],\n     'value': 1},\n    {'shape': (2, 3, 4), 'indexes': numpy.array([[]], dtype=numpy.int32),\n     'value': numpy.random.uniform(size=(3, 4))},\n    {'shape': (2, 3, 4), 'indexes': [[[]]],\n     'value': 1},\n    {'shape': (2, 3, 4), 'indexes': [[[[]]]],\n     'value': 1},\n    {'shape': (2, 3, 4, 5), 'indexes': [[[[]]]],\n     'value': 1},\n    {'shape': (2, 3, 4, 5), 'indexes': [[[[[]]]]],\n     'value': 1},\n    {'shape': (2, 3, 4), 'indexes': (slice(None), []),\n     'value': 1},\n    {'shape': (2, 3, 4), 'indexes': ([], []),\n     'value': 1},\n    {'shape': (2, 3, 4), 'indexes': numpy.array([], dtype=numpy.bool),\n     'value': 1},\n    {'shape': (2, 3, 4),\n     'indexes': (slice(None), numpy.array([], dtype=numpy.bool)),\n     'value': 1},\n    {'shape': (2, 3, 4), 'indexes': numpy.array([[], []], dtype=numpy.bool),\n     'value': numpy.random.uniform(size=(4,))},\n    # list indexes\n    {'shape': (2, 3, 4), 'indexes': [1, 0], 'value': 1},\n    {'shape': (2, 3, 4), 'indexes': [[1]], 'value': 1},\n    {'shape': (2, 3, 4), 'indexes': [[1, 0]], 'value': 1},\n    {'shape': (2, 3, 4), 'indexes': [[1], [0]], 'value': 1},\n    {'shape': (2, 3, 4), 'indexes': [[1, 0], 2], 'value': 1},\n    {'shape': (2, 3, 4), 'indexes': [[1], slice(1, 2)], 'value': 1},\n    {'shape': (2, 3, 4), 'indexes': [[[1]], slice(1, 2)], 'value': 1},\n    # zero-dim and zero-sized arrays\n    {'shape': (), 'indexes': Ellipsis, 'value': 1},\n    {'shape': (), 'indexes': (), 'value': 1},\n    {'shape': (), 'indexes': None, 'value': 1},\n    {'shape': (), 'indexes': True, 'value': 1},\n    {'shape': (), 'indexes': (True,), 'value': 1},\n    # TODO(niboshi): pass the following commented out tests\n    # {'shape': (), 'indexes': (False, True, True), 'value': 1},\n    {'shape': (), 'indexes': numpy.ones((), dtype=numpy.bool_), 'value': 1},\n    {'shape': (), 'indexes': numpy.zeros((), dtype=numpy.bool_), 'value': 1},\n    {'shape': (0,), 'indexes': None, 'value': 1},\n    {'shape': (0,), 'indexes': (), 'value': 1},\n    # TODO(niboshi): pass the following commented out tests\n    # {'shape': (0,), 'indexes': (False, True, True), 'value': 1},\n)\n@testing.gpu\nclass TestArrayAdvancedIndexingSetitemScalarValue(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_adv_setitem(self, xp, dtype):\n        a = xp.zeros(self.shape, dtype=dtype)\n        a[self.indexes] = self.value\n        return a\n\n\n@testing.parameterize(\n    {'shape': (0,), 'indexes': True, 'value': 1},\n    {'shape': (0,), 'indexes': (True,), 'value': 1},\n    {'shape': (0,), 'indexes': numpy.ones((), dtype=numpy.bool_), 'value': 1},\n    {'shape': (0,), 'indexes': numpy.zeros((), dtype=numpy.bool_), 'value': 1},\n)\n@testing.gpu\nclass TestArrayAdvancedIndexingSetitemScalarValue2(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_adv_setitem(self, xp, dtype):\n        a = xp.zeros(self.shape, dtype=dtype)\n        a[self.indexes] = self.value\n        return a\n\n\n@testing.parameterize(\n    # zero-dim and zero-sized arrays\n    {'shape': (), 'indexes': numpy.array([True]), 'value': 1},\n    {'shape': (), 'indexes': numpy.array([False, True, True]), 'value': 1},\n    {'shape': (0,), 'indexes': numpy.array([True]), 'value': 1},\n    {'shape': (0,), 'indexes': numpy.array([False, True, True]), 'value': 1},\n)\n@testing.gpu\nclass TestArrayAdvancedIndexingSetitemScalarValueIndexError(unittest.TestCase):\n\n    def test_adv_setitem(self):\n        for xp in (numpy, cupy):\n            a = xp.zeros(self.shape)\n            with pytest.raises(IndexError):\n                a[self.indexes] = self.value\n\n\n@testing.parameterize(\n    {'shape': (2, 3, 4), 'indexes': numpy.array(1),\n     'value': numpy.array([1])},\n    {'shape': (2, 3, 4), 'indexes': numpy.array(1),\n     'value': numpy.array([1, 2, 3, 4])},\n    {'shape': (2, 3, 4), 'indexes': (slice(None), [0, -1]),\n     'value': numpy.arange(2 * 2 * 4).reshape(2, 2, 4)},\n    {'shape': (2, 5, 4), 'indexes': (slice(None), [[0, 2], [1, -1]]),\n     'value': numpy.arange(2 * 2 * 2 * 4).reshape(2, 2, 2, 4)},\n    # mask\n    {'shape': (2, 3, 4), 'indexes': numpy.random.choice([False, True], (2, 3)),\n     'value': numpy.arange(4)},\n    {'shape': (2, 3, 4),\n     'indexes': (slice(None), numpy.array([True, False, True])),\n     'value': numpy.arange(2 * 2 * 4).reshape(2, 2, 4)},\n    {'shape': (2, 3, 4),\n     'indexes': (numpy.array([[True, False, False], [False, True, True]]),),\n     'value': numpy.arange(3 * 4).reshape(3, 4)},\n    {'shape': (2, 2, 2),\n     'indexes': (slice(None), numpy.array([[True, False], [False, True]]),),\n     'value': numpy.arange(2 * 2).reshape(2, 2)},\n    {'shape': (2, 2, 2),\n     'indexes': (numpy.array(\n         [[[True, False], [True, False]], [[True, True], [False, False]]]),),\n     'value': numpy.arange(4)},\n    {'shape': (5,),\n     'indexes': numpy.array([True, False, False, True, True]),\n     'value': numpy.arange(3)},\n    # multiple arrays\n    {'shape': (2, 3, 4), 'indexes': ([1, 0], [2, 1]),\n     'value': numpy.arange(2 * 4).reshape(2, 4)},\n    {'shape': (2, 3, 4), 'indexes': ([1, 0], slice(None), [2, 1]),\n     'value': numpy.arange(2 * 3).reshape(2, 3)},\n    {'shape': (2, 3, 4), 'indexes': ([1, 0], slice(None), [[2, 0], [3, 1]]),\n     'value': numpy.arange(2 * 2 * 3).reshape(2, 2, 3)},\n    {'shape': (2, 3, 4),\n     'indexes': ([[1, 0], [1, 0]], slice(None), [[2, 0], [3, 1]]),\n     'value': numpy.arange(2 * 2 * 3).reshape(2, 2, 3)},\n    {'shape': (2, 3, 4),\n     'indexes': (1, slice(None), [[2, 0], [3, 1]]),\n     'value': numpy.arange(2 * 2 * 3).reshape(2, 2, 3)},\n    # list indexes\n    {'shape': (2, 3, 4), 'indexes': [1],\n     'value': numpy.arange(3 * 4).reshape(3, 4)},\n)\n@testing.gpu\nclass TestArrayAdvancedIndexingVectorValue(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_adv_setitem(self, xp, dtype):\n        a = xp.zeros(self.shape, dtype=dtype)\n        a[self.indexes] = self.value.astype(a.dtype)\n        return a\n\n\n@testing.gpu\nclass TestArrayAdvancedIndexingSetitemCupyIndices(unittest.TestCase):\n\n    shape = (2, 3)\n\n    def test_cupy_indices_integer_array_1(self):\n        a = cupy.zeros(self.shape)\n        index = cupy.array([0, 1])\n        original_index = index.copy()\n        a[:, index] = cupy.array(1.)\n        testing.assert_array_equal(\n            a, cupy.array([[1., 1., 0.], [1., 1., 0.]]))\n        testing.assert_array_equal(index, original_index)\n\n    def test_cupy_indices_integer_array_2(self):\n        a = cupy.zeros(self.shape)\n        index = cupy.array([3, -5])\n        original_index = index.copy()\n        a[:, index] = cupy.array(1.)\n        testing.assert_array_equal(\n            a, cupy.array([[1., 1., 0.], [1., 1., 0.]]))\n        testing.assert_array_equal(index, original_index)\n\n    def test_cupy_indices_integer_array_3(self):\n        a = cupy.zeros(self.shape)\n        index = cupy.array([3, -5])\n        original_index = index.copy()\n        a[[1, 1], index] = cupy.array(1.)\n        testing.assert_array_equal(\n            a, cupy.array([[0., 0., 0.], [1., 1., 0.]]))\n        testing.assert_array_equal(index, original_index)\n\n    def test_cupy_indices_boolean_array(self):\n        a = cupy.zeros(self.shape)\n        index = cupy.array([True, False])\n        original_index = index.copy()\n        a[index] = cupy.array(1.)\n        testing.assert_array_equal(\n            a, cupy.array([[1., 1., 1.], [0., 0., 0.]]))\n        testing.assert_array_almost_equal(original_index, index)\n\n\n@testing.gpu\nclass TestArrayAdvancedIndexingSetitemDifferetnDtypes(unittest.TestCase):\n\n    @testing.for_all_dtypes_combination(names=['src_dtype', 'dst_dtype'],\n                                        no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_differnt_dtypes(self, xp, src_dtype, dst_dtype):\n        shape = (2, 3)\n        a = xp.zeros(shape, dtype=src_dtype)\n        indexes = xp.array([0, 1])\n        a[:, indexes] = xp.array(1, dtype=dst_dtype)\n        return a\n\n    @testing.for_all_dtypes_combination(names=['src_dtype', 'dst_dtype'],\n                                        no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_differnt_dtypes_mask(self, xp, src_dtype, dst_dtype):\n        shape = (2, 3)\n        a = xp.zeros(shape, dtype=src_dtype)\n        indexes = xp.array([True, False])\n        a[indexes] = xp.array(1, dtype=dst_dtype)\n        return a\n\n\n@testing.gpu\nclass TestArrayAdvancedIndexingSetitemTranspose(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_adv_setitem_transp(self, xp):\n        shape = (2, 3, 4)\n        a = xp.zeros(shape).transpose(0, 2, 1)\n        slices = (numpy.array([1, 0]), slice(None), numpy.array([2, 1]))\n        a[slices] = 1\n        return a\n"""
tests/cupy_tests/core_tests/test_ndarray_complex_ops.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestConj(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_almost_equal()\n    def test_conj(self, xp, dtype):\n        x = testing.shaped_arange((2, 3), xp, dtype)\n        return x.conj()\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_almost_equal()\n    def test_conj_pass(self, xp, dtype):\n        x = testing.shaped_arange((2, 3), xp, dtype)\n        y = x.conj()\n        self.assertIs(x, y)\n        return y\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_almost_equal()\n    def test_conjugate(self, xp, dtype):\n        x = testing.shaped_arange((2, 3), xp, dtype)\n        return x.conjugate()\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_almost_equal()\n    def test_conjugate_pass(self, xp, dtype):\n        x = testing.shaped_arange((2, 3), xp, dtype)\n        y = x.conjugate()\n        self.assertIs(x, y)\n        return y\n\n\n@testing.gpu\nclass TestAngle(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_almost_equal()\n    def test_angle(self, xp, dtype):\n        x = testing.shaped_arange((2, 3), xp, dtype)\n        return xp.angle(x)\n\n\n@testing.gpu\nclass TestRealImag(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_almost_equal(accept_error=False)\n    def test_real(self, xp, dtype):\n        x = testing.shaped_arange((2, 3), xp, dtype)\n        return x.real\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_almost_equal(accept_error=False)\n    def test_real_zero_dim(self, xp, dtype):\n        x = xp.array(1, dtype=dtype)\n        return x.real\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_almost_equal(accept_error=False)\n    def test_real_non_contiguous(self, xp, dtype):\n        x = testing.shaped_arange((2, 3, 2), xp, dtype).transpose(0, 2, 1)\n        return x.real\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_almost_equal(accept_error=False)\n    def test_imag(self, xp, dtype):\n        x = testing.shaped_arange((2, 3), xp, dtype)\n        return x.imag\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_almost_equal(accept_error=False)\n    def test_imag_zero_dim(self, xp, dtype):\n        x = xp.array(1, dtype=dtype)\n        return x.imag\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_almost_equal(accept_error=False)\n    def test_imag_non_contiguous(self, xp, dtype):\n        x = testing.shaped_arange((2, 3, 2), xp, dtype).transpose(0, 2, 1)\n        return x.imag\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_almost_equal(accept_error=False)\n    def test_real_setter(self, xp, dtype):\n        x = testing.shaped_arange((2, 3), xp, dtype)\n        x.real = testing.shaped_reverse_arange((2, 3), xp, dtype).real\n        return x\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_almost_equal(accept_error=False)\n    def test_real_setter_zero_dim(self, xp, dtype):\n        x = xp.array(1, dtype=dtype)\n        x.real = 2\n        return x\n\n    @testing.for_dtypes('FD')\n    @testing.numpy_cupy_array_almost_equal(accept_error=False)\n    def test_real_setter_non_contiguous(self, xp, dtype):\n        x = testing.shaped_arange((2, 3, 2), xp, dtype).transpose(0, 2, 1)\n        x.real = testing.shaped_reverse_arange((2, 2, 3), xp, dtype).real\n        return x\n\n    @testing.for_dtypes('FD')\n    @testing.numpy_cupy_array_almost_equal(accept_error=False)\n    def test_imag_setter(self, xp, dtype):\n        x = testing.shaped_arange((2, 3), xp, dtype)\n        x.imag = testing.shaped_reverse_arange((2, 3), xp, dtype).real\n        return x\n\n    @testing.for_dtypes('FD')\n    @testing.numpy_cupy_array_almost_equal(accept_error=False)\n    def test_imag_setter_zero_dim(self, xp, dtype):\n        x = xp.array(1, dtype=dtype)\n        x.imag = 2\n        return x\n\n    @testing.for_dtypes('FD')\n    @testing.numpy_cupy_array_almost_equal(accept_error=False)\n    def test_imag_setter_non_contiguous(self, xp, dtype):\n        x = testing.shaped_arange((2, 3, 2), xp, dtype).transpose(0, 2, 1)\n        x.imag = testing.shaped_reverse_arange((2, 2, 3), xp, dtype).real\n        return x\n\n    @testing.for_all_dtypes(no_complex=True)\n    def test_imag_setter_raise(self, dtype):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((2, 3), xp, dtype)\n            with pytest.raises(TypeError):\n                x.imag = testing.shaped_reverse_arange((2, 3), xp, dtype)\n\n    @testing.for_all_dtypes()\n    def test_real_inplace(self, dtype):\n        x = cupy.zeros((2, 3), dtype=dtype)\n        x.real[:] = 1\n        expected = cupy.ones((2, 3), dtype=dtype)\n        assert cupy.all(x == expected)\n\n    @testing.for_all_dtypes()\n    def test_imag_inplace(self, dtype):\n        x = cupy.zeros((2, 3), dtype=dtype)\n\n        # TODO(kmaehashi) The following line should raise error for real\n        # dtypes, but currently ignored silently.\n        x.imag[:] = 1\n\n        expected = cupy.zeros((2, 3), dtype=dtype) + (\n            1j if x.dtype.kind == 'c' else 0)\n        assert cupy.all(x == expected)\n\n\n@testing.gpu\nclass TestScalarConversion(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_scalar_conversion(self, dtype):\n        scalar = 1 + 1j if numpy.dtype(dtype).kind == 'c' else 1\n        x_1d = cupy.array([scalar]).astype(dtype)\n        self.assertEqual(complex(x_1d), scalar)\n\n        x_0d = x_1d.reshape(())\n        self.assertEqual(complex(x_0d), scalar)\n"""
tests/cupy_tests/core_tests/test_ndarray_contiguity.py,0,"b'import unittest\n\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestArrayContiguity(unittest.TestCase):\n\n    def test_is_contiguous(self):\n        a = testing.shaped_arange((2, 3, 4))\n        self.assertTrue(a.flags.c_contiguous)\n        b = a.transpose(2, 0, 1)\n        self.assertFalse(b.flags.c_contiguous)\n        c = a[::-1]\n        self.assertFalse(c.flags.c_contiguous)\n        d = a[:, :, ::2]\n        self.assertFalse(d.flags.c_contiguous)\n'"
tests/cupy_tests/core_tests/test_ndarray_conversion.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.parameterize(\n    {'shape': ()},\n    {'shape': (1,)},\n    {'shape': (1, 1, 1)},\n)\nclass TestNdarrayItem(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_item(self, xp, dtype):\n        a = xp.full(self.shape, 3, dtype)\n        return a.item()\n\n\n@testing.parameterize(\n    {'shape': (0,)},\n    {'shape': (2, 3)},\n    {'shape': (1, 0, 1)},\n)\nclass TestNdarrayItemRaise(unittest.TestCase):\n\n    def test_item(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange(self.shape, xp, xp.float32)\n            with pytest.raises(ValueError):\n                a.item()\n\n\n@testing.parameterize(\n    {'shape': ()},\n    {'shape': (1,)},\n    {'shape': (2, 3)},\n    {'shape': (2, 3), 'order': 'C'},\n    {'shape': (2, 3), 'order': 'F'},\n)\nclass TestNdarrayToBytes(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_item(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        if hasattr(self, 'order'):\n            return a.tobytes(self.order)\n        else:\n            return a.tobytes()\n"""
tests/cupy_tests/core_tests/test_ndarray_copy_and_view.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\nfrom cupy import util\n\n\ndef astype_without_warning(x, dtype, *args, **kwargs):\n    dtype = numpy.dtype(dtype)\n    if x.dtype.kind == 'c' and dtype.kind not in ['b', 'c']:\n        with testing.assert_warns(numpy.ComplexWarning):\n            return x.astype(dtype, *args, **kwargs)\n    else:\n        return x.astype(dtype, *args, **kwargs)\n\n\n@testing.gpu\nclass TestArrayCopyAndView(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_view(self, xp):\n        a = testing.shaped_arange((4,), xp, dtype=numpy.float32)\n        b = a.view(dtype=numpy.int32)\n        b[:] = 0\n        return a\n\n    @testing.for_dtypes([numpy.int16, numpy.int64])\n    @testing.numpy_cupy_array_equal()\n    def test_view_itemsize(self, xp, dtype):\n        a = testing.shaped_arange((4,), xp, dtype=numpy.int32)\n        b = a.view(dtype=dtype)\n        return b\n\n    @testing.numpy_cupy_array_equal()\n    def test_view_0d(self, xp):\n        a = xp.array(1.5, dtype=numpy.float32)\n        return a.view(dtype=numpy.int32)\n\n    @testing.for_dtypes([numpy.int16, numpy.int64])\n    def test_view_0d_raise(self, dtype):\n        for xp in (numpy, cupy):\n            a = xp.array(3, dtype=numpy.int32)\n            with pytest.raises(ValueError):\n                a.view(dtype=dtype)\n\n    @testing.for_dtypes([numpy.int16, numpy.int64])\n    def test_view_non_contiguous_raise(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 2, 2), xp, dtype=numpy.int32)\n            a = a.transpose(0, 2, 1)\n            with pytest.raises(ValueError):\n                a.view(dtype=dtype)\n\n    @testing.numpy_cupy_array_equal()\n    def test_flatten(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return a.flatten()\n\n    @testing.numpy_cupy_array_equal()\n    def test_flatten_copied(self, xp):\n        a = testing.shaped_arange((4,), xp)\n        b = a.flatten()\n        a[:] = 1\n        return b\n\n    @testing.numpy_cupy_array_equal()\n    def test_transposed_flatten(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp).transpose(2, 0, 1)\n        return a.flatten()\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_fill(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        a.fill(1)\n        return a\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_fill_with_numpy_scalar_ndarray(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        a.fill(numpy.ones((), dtype=dtype))\n        return a\n\n    @testing.for_all_dtypes()\n    def test_fill_with_numpy_nonscalar_ndarray(self, dtype):\n        a = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        with self.assertRaises(ValueError):\n            a.fill(numpy.ones((1,), dtype=dtype))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_transposed_fill(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = a.transpose(2, 0, 1)\n        b.fill(1)\n        return b\n\n    @testing.for_orders(['C', 'F', 'A', 'K', None])\n    @testing.for_all_dtypes_combination(('src_dtype', 'dst_dtype'))\n    @testing.numpy_cupy_array_equal()\n    def test_astype(self, xp, src_dtype, dst_dtype, order):\n        a = testing.shaped_arange((2, 3, 4), xp, src_dtype)\n        return astype_without_warning(a, dst_dtype, order=order)\n\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes_combination(('src_dtype', 'dst_dtype'))\n    def test_astype_type(self, src_dtype, dst_dtype, order):\n        a = testing.shaped_arange((2, 3, 4), cupy, src_dtype)\n        b = astype_without_warning(a, dst_dtype, order=order)\n        a_cpu = testing.shaped_arange((2, 3, 4), numpy, src_dtype)\n        b_cpu = astype_without_warning(a_cpu, dst_dtype, order=order)\n        self.assertEqual(b.dtype.type, b_cpu.dtype.type)\n\n    @testing.for_orders('CAK')\n    @testing.for_all_dtypes()\n    def test_astype_type_c_contiguous_no_copy(self, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        b = a.astype(dtype, order=order, copy=False)\n        self.assertTrue(b is a)\n\n    @testing.for_orders('FAK')\n    @testing.for_all_dtypes()\n    def test_astype_type_f_contiguous_no_copy(self, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        a = cupy.asfortranarray(a)\n        b = a.astype(dtype, order=order, copy=False)\n        self.assertTrue(b is a)\n\n    @testing.for_all_dtypes_combination(('src_dtype', 'dst_dtype'))\n    @testing.numpy_cupy_array_equal()\n    def test_astype_strides(self, xp, src_dtype, dst_dtype):\n        src = xp.empty((1, 2, 3), dtype=src_dtype)\n        return numpy.array(\n            astype_without_warning(src, dst_dtype, order='K').strides)\n\n    @testing.for_all_dtypes_combination(('src_dtype', 'dst_dtype'))\n    @testing.numpy_cupy_array_equal()\n    def test_astype_strides_negative(self, xp, src_dtype, dst_dtype):\n        src = xp.empty((2, 3), dtype=src_dtype)[::-1, :]\n        return numpy.array(\n            astype_without_warning(src, dst_dtype, order='K').strides)\n\n    @testing.for_all_dtypes_combination(('src_dtype', 'dst_dtype'))\n    @testing.numpy_cupy_array_equal()\n    def test_astype_strides_swapped(self, xp, src_dtype, dst_dtype):\n        src = xp.swapaxes(xp.empty((2, 3, 4), dtype=src_dtype), 1, 0)\n        return numpy.array(\n            astype_without_warning(src, dst_dtype, order='K').strides)\n\n    @testing.for_all_dtypes_combination(('src_dtype', 'dst_dtype'))\n    @testing.numpy_cupy_array_equal()\n    def test_astype_strides_broadcast(self, xp, src_dtype, dst_dtype):\n        src, _ = xp.broadcast_arrays(xp.empty((2,), dtype=src_dtype),\n                                     xp.empty((2, 3, 2), dtype=src_dtype))\n        return numpy.array(\n            astype_without_warning(src, dst_dtype, order='K').strides)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_diagonal1(self, xp, dtype):\n        a = testing.shaped_arange((3, 4, 5), xp, dtype)\n        return a.diagonal(1, 2, 0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_diagonal2(self, xp, dtype):\n        a = testing.shaped_arange((3, 4, 5), xp, dtype)\n        return a.diagonal(-1, 2, 0)\n\n    @unittest.skipUnless(util.ENABLE_SLICE_COPY, 'Special copy disabled')\n    @testing.for_orders('CF')\n    @testing.for_dtypes([numpy.int16, numpy.int64,\n                         numpy.float16, numpy.float64])\n    @testing.numpy_cupy_array_equal()\n    def test_isinstance_numpy_copy(self, xp, dtype, order):\n        a = numpy.arange(100, dtype=dtype).reshape(10, 10, order=order)\n        b = xp.empty(a.shape, dtype=dtype, order=order)\n        b[:] = a\n        return b\n\n    @unittest.skipUnless(util.ENABLE_SLICE_COPY, 'Special copy disabled')\n    def test_isinstance_numpy_copy_wrong_dtype(self):\n        for xp in (numpy, cupy):\n            a = numpy.arange(100, dtype=numpy.float64).reshape(10, 10)\n            b = cupy.empty(a.shape, dtype=numpy.int32)\n            with pytest.raises(ValueError):\n                b[:] = a\n\n    @unittest.skipUnless(util.ENABLE_SLICE_COPY, 'Special copy disabled')\n    def test_isinstance_numpy_copy_wrong_shape(self):\n        for xp in (numpy, cupy):\n            a = numpy.arange(100, dtype=numpy.float64).reshape(10, 10)\n            b = cupy.empty(100, dtype=a.dtype)\n            with pytest.raises(ValueError):\n                b[:] = a\n\n    @unittest.skipUnless(util.ENABLE_SLICE_COPY, 'Special copy disabled')\n    @testing.numpy_cupy_array_equal()\n    def test_isinstance_numpy_copy_not_slice(self, xp):\n        a = xp.arange(5, dtype=numpy.float64)\n        a[a < 3] = 0\n        return a\n\n\n@testing.parameterize(\n    {'src_order': 'C'},\n    {'src_order': 'F'},\n)\n@testing.gpu\nclass TestNumPyArrayCopyView(unittest.TestCase):\n    @unittest.skipUnless(util.ENABLE_SLICE_COPY, 'Special copy disabled')\n    @testing.for_orders('CF')\n    @testing.for_dtypes([numpy.int16, numpy.int64,\n                         numpy.float16, numpy.float64])\n    @testing.numpy_cupy_array_equal()\n    def test_isinstance_numpy_view_copy_f(self, xp, dtype, order):\n        a = numpy.arange(100, dtype=dtype).reshape(\n            10, 10, order=self.src_order)\n        a = a[2:5, 1:8]\n        b = xp.empty(a.shape, dtype=dtype, order=order)\n        b[:] = a\n        return b\n"""
tests/cupy_tests/core_tests/test_ndarray_cuda_array_interface.py,0,"b""import unittest\n\nimport cupy\nfrom cupy import core\nfrom cupy import testing\n\n\nclass DummyObjectWithCudaArrayInterface(object):\n\n    def __init__(self, a):\n        self.a = a\n\n    @property\n    def __cuda_array_interface__(self):\n        desc = {\n            'shape': self.a.shape,\n            'strides': self.a.strides,\n            'typestr': self.a.dtype.str,\n            'descr': self.a.dtype.descr,\n            'data': (self.a.data.ptr, False),\n            'version': 2,\n        }\n        return desc\n\n\n@testing.gpu\nclass TestArrayUfunc(unittest.TestCase):\n\n    @testing.for_all_dtypes_combination(names=['x_type', 'y_type'])\n    @testing.numpy_cupy_allclose(rtol=1e-6, accept_error=TypeError,\n                                 contiguous_check=False)\n    def check_array_scalar_op(self, op, xp, x_type, y_type, trans=False):\n        a = xp.array([[1, 2, 3], [4, 5, 6]], x_type)\n        if trans:\n            a = a.T\n\n        if xp is cupy:\n            a = DummyObjectWithCudaArrayInterface(a)\n        return getattr(xp, op)(a, y_type(3))\n\n    def test_add_scalar(self):\n        self.check_array_scalar_op('add')\n\n    def test_add_scalar_with_strides(self):\n        self.check_array_scalar_op('add', trans=True)\n\n\n@testing.gpu\nclass TestElementwiseKernel(unittest.TestCase):\n\n    @testing.for_all_dtypes_combination()\n    @testing.numpy_cupy_allclose(rtol=1e-6, accept_error=TypeError,\n                                 contiguous_check=False)\n    def check_array_scalar_op(self, op, xp, dtyes, trans=False):\n        a = xp.array([[1, 2, 3], [4, 5, 6]], dtyes)\n        if trans:\n            a = a.T\n\n        if xp is cupy:\n            a = DummyObjectWithCudaArrayInterface(a)\n            f = cupy.ElementwiseKernel('T x, T y', 'T z', 'z = x + y')\n            return f(a, dtyes(3))\n        else:\n            return a + dtyes(3)\n\n    def test_add_scalar(self):\n        self.check_array_scalar_op('add')\n\n    def test_add_scalar_with_strides(self):\n        self.check_array_scalar_op('add', trans=True)\n\n\n@testing.gpu\nclass SimpleReductionFunction(unittest.TestCase):\n\n    def setUp(self):\n        self.my_int8_sum = core.create_reduction_func(\n            'my_sum', ('b->b',), ('in0', 'a + b', 'out0 = a', None))\n\n    @testing.numpy_cupy_allclose()\n    def check_int8_sum(self, shape, xp, axis=None, keepdims=False,\n                       trans=False):\n        a = testing.shaped_random(shape, xp, 'b')\n        if trans:\n            a = a.T\n\n        if xp == cupy:\n            a = DummyObjectWithCudaArrayInterface(a)\n            return self.my_int8_sum(\n                a, axis=axis, keepdims=keepdims)\n        else:\n            return a.sum(axis=axis, keepdims=keepdims, dtype='b')\n\n    def test_shape(self):\n        self.check_int8_sum((2 ** 10,))\n\n    def test_shape_with_strides(self):\n        self.check_int8_sum((2 ** 10, 16), trans=True)\n\n\n@testing.gpu\nclass TestReductionKernel(unittest.TestCase):\n\n    def setUp(self):\n        self.my_sum = core.ReductionKernel(\n            'T x', 'T out', 'x', 'a + b', 'out = a', '0', 'my_sum')\n\n    @testing.numpy_cupy_allclose()\n    def check_int8_sum(self, shape, xp, axis=None, keepdims=False,\n                       trans=False):\n        a = testing.shaped_random(shape, xp, 'b')\n        if trans:\n            a = a.T\n\n        if xp == cupy:\n            a = DummyObjectWithCudaArrayInterface(a)\n            return self.my_sum(\n                a, axis=axis, keepdims=keepdims)\n        else:\n            return a.sum(axis=axis, keepdims=keepdims, dtype='b')\n\n    def test_shape(self):\n        self.check_int8_sum((2 ** 10,))\n\n    def test_shape_with_strides(self):\n        self.check_int8_sum((2 ** 10, 16), trans=True)\n\n\n@testing.parameterize(\n    {'shape': (10,), 'slices': (slice(0, None),)},\n    {'shape': (10,), 'slices': (slice(2, None),)},\n    {'shape': (10, 10), 'slices': (slice(0, None), slice(0, None))},\n    {'shape': (10, 10), 'slices': (slice(0, None), slice(2, None))},\n    {'shape': (10, 10), 'slices': (slice(2, None), slice(0, None))},\n    {'shape': (10, 10), 'slices': (slice(2, None), slice(2, None))},\n    {'shape': (10, 10), 'slices': (slice(2, None), slice(4, None))},\n)\n@testing.gpu\nclass TestSlicingMemoryPointer(unittest.TestCase):\n\n    @testing.for_all_dtypes_combination(names=['dtype'])\n    @testing.for_orders('CF')\n    def test_shape_with_strides(self, dtype, order):\n        x = cupy.zeros(self.shape, dtype=dtype, order=order)\n\n        start = [s.start for s in self.slices]\n        itemsize = cupy.dtype(dtype).itemsize\n        dimsize = [s * itemsize for s in start]\n        if len(self.shape) == 1:\n            offset = start[0] * itemsize\n        else:\n            if order == 'C':\n                offset = self.shape[0] * dimsize[0] + dimsize[1]\n            else:\n                offset = self.shape[0] * dimsize[1] + dimsize[0]\n\n        cai_ptr, _ = x.__cuda_array_interface__['data']\n        slice_cai_ptr, _ = x[self.slices].__cuda_array_interface__['data']\n        cupy_data_ptr = x.data.ptr\n        sliced_cupy_data_ptr = x[self.slices].data.ptr\n\n        assert cai_ptr == cupy_data_ptr\n        assert slice_cai_ptr == sliced_cupy_data_ptr\n        assert slice_cai_ptr == cai_ptr+offset\n\n\n@testing.parameterize(\n    {'shape': (10,), 'slices': (slice(0, None),)},\n    {'shape': (10,), 'slices': (slice(2, None),)},\n    {'shape': (10, 10), 'slices': (slice(0, None), slice(0, None))},\n    {'shape': (10, 10), 'slices': (slice(0, None), slice(2, None))},\n    {'shape': (10, 10), 'slices': (slice(2, None), slice(0, None))},\n    {'shape': (10, 10), 'slices': (slice(2, None), slice(2, None))},\n    {'shape': (10, 10), 'slices': (slice(2, None), slice(4, None))},\n)\n@testing.gpu\nclass TestCUDAArrayInterfaceCompliance(unittest.TestCase):\n\n    @testing.for_all_dtypes_combination(names=['dtype'])\n    @testing.for_orders('CF')\n    def test_value_type(self, dtype, order):\n        x = cupy.zeros(self.shape, dtype=dtype, order=order)\n        y = x[self.slices]\n\n        # mandatory entries\n        shape = y.__cuda_array_interface__['shape']\n        typestr = y.__cuda_array_interface__['typestr']\n        ptr, readonly = y.__cuda_array_interface__['data']\n        version = y.__cuda_array_interface__['version']\n        strides = y.__cuda_array_interface__['strides']\n\n        # optional entries\n        if 'descr' in y.__cuda_array_interface__:\n            descr = y.__cuda_array_interface__['descr']\n        else:\n            descr = None\n\n        # Don't validate correctness of data here, just their types\n        assert isinstance(shape, tuple)\n        assert isinstance(typestr, str)\n        assert isinstance(ptr, int)\n        assert isinstance(readonly, bool)\n        assert version == 2  # update this when the standard is updated!\n        assert (strides is None) or isinstance(strides, tuple)\n        assert (descr is None) or isinstance(descr, list)\n        if isinstance(descr, list):\n            for item in descr:\n                assert isinstance(item, tuple)\n"""
tests/cupy_tests/core_tests/test_ndarray_elementwise_op.py,0,"b'import operator\nimport unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestArrayElementwiseOp(unittest.TestCase):\n\n    @testing.for_all_dtypes_combination(names=[\'x_type\', \'y_type\'])\n    @testing.numpy_cupy_allclose(rtol=1e-6, accept_error=TypeError)\n    def check_array_scalar_op(self, op, xp, x_type, y_type, swap=False,\n                              no_bool=False, no_complex=False):\n        x_dtype = numpy.dtype(x_type)\n        y_dtype = numpy.dtype(y_type)\n        if no_bool and x_dtype == \'?\' and y_dtype == \'?\':\n            return xp.array(True)\n        if no_complex and (x_dtype.kind == \'c\' or y_dtype.kind == \'c\'):\n            return xp.array(True)\n        a = xp.array([[1, 2, 3], [4, 5, 6]], x_type)\n        if swap:\n            return op(y_type(3), a)\n        else:\n            return op(a, y_type(3))\n\n    def test_add_scalar(self):\n        self.check_array_scalar_op(operator.add)\n\n    def test_radd_scalar(self):\n        self.check_array_scalar_op(operator.add, swap=True)\n\n    def test_iadd_scalar(self):\n        self.check_array_scalar_op(operator.iadd)\n\n    def test_sub_scalar(self):\n        self.check_array_scalar_op(operator.sub, no_bool=True)\n\n    def test_rsub_scalar(self):\n        self.check_array_scalar_op(operator.sub, swap=True, no_bool=True)\n\n    def test_isub_scalar(self):\n        self.check_array_scalar_op(operator.isub, no_bool=True)\n\n    def test_mul_scalar(self):\n        self.check_array_scalar_op(operator.mul)\n\n    def test_rmul_scalar(self):\n        self.check_array_scalar_op(operator.mul, swap=True)\n\n    def test_imul_scalar(self):\n        self.check_array_scalar_op(operator.imul)\n\n    def test_truediv_scalar(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_scalar_op(operator.truediv)\n\n    def test_rtruediv_scalar(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_scalar_op(operator.truediv, swap=True)\n\n    def test_itruediv_scalar(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_scalar_op(operator.itruediv)\n\n    def test_floordiv_scalar(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_scalar_op(operator.floordiv, no_complex=True)\n\n    def test_rfloordiv_scalar(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_scalar_op(operator.floordiv, swap=True,\n                                       no_complex=True)\n\n    def test_ifloordiv_scalar(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_scalar_op(operator.ifloordiv, no_complex=True)\n\n    def test_pow_scalar(self):\n        self.check_array_scalar_op(operator.pow)\n\n    def test_rpow_scalar(self):\n        self.check_array_scalar_op(operator.pow, swap=True)\n\n    @testing.for_all_dtypes_combination(names=[\'x_type\', \'y_type\'])\n    @testing.numpy_cupy_allclose(atol=1.0, accept_error=TypeError)\n    def check_ipow_scalar(self, xp, x_type, y_type):\n        a = xp.array([[1, 2, 3], [4, 5, 6]], x_type)\n        return operator.ipow(a, y_type(3))\n\n    def test_ipow_scalar(self):\n        self.check_ipow_scalar()\n\n    def test_divmod0_scalar(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_scalar_op(lambda x, y: divmod(x, y)[0],\n                                       no_complex=True)\n\n    def test_divmod1_scalar(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_scalar_op(lambda x, y: divmod(x, y)[1],\n                                       no_complex=True)\n\n    def test_rdivmod0_scalar(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_scalar_op(lambda x, y: divmod(x, y)[0], swap=True,\n                                       no_complex=True)\n\n    def test_rdivmod1_scalar(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_scalar_op(lambda x, y: divmod(x, y)[1], swap=True,\n                                       no_complex=True)\n\n    def test_lt_scalar(self):\n        self.check_array_scalar_op(operator.lt, no_complex=False)\n\n    def test_le_scalar(self):\n        self.check_array_scalar_op(operator.le, no_complex=False)\n\n    def test_gt_scalar(self):\n        self.check_array_scalar_op(operator.gt, no_complex=False)\n\n    def test_ge_scalar(self):\n        self.check_array_scalar_op(operator.ge, no_complex=False)\n\n    def test_eq_scalar(self):\n        self.check_array_scalar_op(operator.eq)\n\n    def test_ne_scalar(self):\n        self.check_array_scalar_op(operator.ne)\n\n    @testing.for_all_dtypes_combination(names=[\'x_type\', \'y_type\'])\n    @testing.numpy_cupy_allclose(accept_error=TypeError)\n    def check_array_array_op(self, op, xp, x_type, y_type,\n                             no_bool=False, no_complex=False):\n        x_dtype = numpy.dtype(x_type)\n        y_dtype = numpy.dtype(y_type)\n        if no_bool and x_dtype == \'?\' and y_dtype == \'?\':\n            return xp.array(True)\n        if no_complex and (x_dtype.kind == \'c\' or y_dtype.kind == \'c\'):\n            return xp.array(True)\n        a = xp.array([[1, 2, 3], [4, 5, 6]], x_type)\n        b = xp.array([[6, 5, 4], [3, 2, 1]], y_type)\n        return op(a, b)\n\n    def test_add_array(self):\n        self.check_array_array_op(operator.add)\n\n    def test_iadd_array(self):\n        self.check_array_array_op(operator.iadd)\n\n    def test_sub_array(self):\n        self.check_array_array_op(operator.sub, no_bool=True)\n\n    def test_isub_array(self):\n        self.check_array_array_op(operator.isub, no_bool=True)\n\n    def test_mul_array(self):\n        self.check_array_array_op(operator.mul)\n\n    def test_imul_array(self):\n        self.check_array_array_op(operator.imul)\n\n    def test_truediv_array(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_array_op(operator.truediv)\n\n    def test_itruediv_array(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_array_op(operator.itruediv)\n\n    def test_floordiv_array(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_array_op(operator.floordiv, no_complex=True)\n\n    def test_ifloordiv_array(self):\n        if \'1.16.1\' <= numpy.lib.NumpyVersion(numpy.__version__) < \'1.18.0\':\n            self.skipTest(""NumPy Issue #12927"")\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_array_op(operator.ifloordiv, no_complex=True)\n\n    def test_pow_array(self):\n        self.check_array_array_op(operator.pow)\n\n    @testing.for_all_dtypes_combination(names=[\'x_type\', \'y_type\'])\n    @testing.numpy_cupy_allclose(atol=1.0, accept_error=TypeError)\n    def check_ipow_array(self, xp, x_type, y_type):\n        a = xp.array([[1, 2, 3], [4, 5, 6]], x_type)\n        b = xp.array([[6, 5, 4], [3, 2, 1]], y_type)\n        return operator.ipow(a, b)\n\n    def test_ipow_array(self):\n        self.check_ipow_array()\n\n    def test_divmod0_array(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_array_op(lambda x, y: divmod(x, y)[0])\n\n    def test_divmod1_array(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_array_op(lambda x, y: divmod(x, y)[1])\n\n    def test_lt_array(self):\n        self.check_array_array_op(operator.lt, no_complex=True)\n\n    def test_le_array(self):\n        self.check_array_array_op(operator.le, no_complex=True)\n\n    def test_gt_array(self):\n        self.check_array_array_op(operator.gt, no_complex=True)\n\n    def test_ge_array(self):\n        self.check_array_array_op(operator.ge, no_complex=True)\n\n    def test_eq_array(self):\n        self.check_array_array_op(operator.eq)\n\n    def test_ne_array(self):\n        self.check_array_array_op(operator.ne)\n\n    @testing.for_all_dtypes_combination(names=[\'x_type\', \'y_type\'])\n    @testing.numpy_cupy_allclose(accept_error=TypeError)\n    def check_array_broadcasted_op(self, op, xp, x_type, y_type,\n                                   no_bool=False, no_complex=False):\n        x_dtype = numpy.dtype(x_type)\n        y_dtype = numpy.dtype(y_type)\n        if no_bool and x_dtype == \'?\' and y_dtype == \'?\':\n            return xp.array(True)\n        if no_complex and (x_dtype.kind == \'c\' or y_dtype.kind == \'c\'):\n            return xp.array(True)\n        a = xp.array([[1, 2, 3], [4, 5, 6]], x_type)\n        b = xp.array([[1], [2]], y_type)\n        return op(a, b)\n\n    def test_broadcasted_add(self):\n        self.check_array_broadcasted_op(operator.add)\n\n    def test_broadcasted_iadd(self):\n        self.check_array_broadcasted_op(operator.iadd)\n\n    def test_broadcasted_sub(self):\n        # TODO(unno): sub for boolean array is deprecated in numpy>=1.13\n        self.check_array_broadcasted_op(operator.sub, no_bool=True)\n\n    def test_broadcasted_isub(self):\n        # TODO(unno): sub for boolean array is deprecated in numpy>=1.13\n        self.check_array_broadcasted_op(operator.isub, no_bool=True)\n\n    def test_broadcasted_mul(self):\n        self.check_array_broadcasted_op(operator.mul)\n\n    def test_broadcasted_imul(self):\n        self.check_array_broadcasted_op(operator.imul)\n\n    def test_broadcasted_truediv(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_broadcasted_op(operator.truediv)\n\n    def test_broadcasted_itruediv(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_broadcasted_op(operator.itruediv)\n\n    def test_broadcasted_floordiv(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_broadcasted_op(operator.floordiv, no_complex=True)\n\n    def test_broadcasted_ifloordiv(self):\n        if \'1.16.1\' <= numpy.lib.NumpyVersion(numpy.__version__) < \'1.18.0\':\n            self.skipTest(""NumPy Issue #12927"")\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_broadcasted_op(operator.ifloordiv,\n                                            no_complex=True)\n\n    def test_broadcasted_pow(self):\n        self.check_array_broadcasted_op(operator.pow)\n\n    @testing.for_all_dtypes_combination(names=[\'x_type\', \'y_type\'])\n    @testing.numpy_cupy_allclose(atol=1.0, accept_error=TypeError)\n    def check_broadcasted_ipow(self, xp, x_type, y_type):\n        a = xp.array([[1, 2, 3], [4, 5, 6]], x_type)\n        b = xp.array([[1], [2]], y_type)\n        return operator.ipow(a, b)\n\n    def test_broadcasted_ipow(self):\n        self.check_broadcasted_ipow()\n\n    def test_broadcasted_divmod0(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_broadcasted_op(lambda x, y: divmod(x, y)[0],\n                                            no_complex=True)\n\n    def test_broadcasted_divmod1(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_broadcasted_op(lambda x, y: divmod(x, y)[1],\n                                            no_complex=True)\n\n    def test_broadcasted_lt(self):\n        self.check_array_broadcasted_op(operator.lt, no_complex=True)\n\n    def test_broadcasted_le(self):\n        self.check_array_broadcasted_op(operator.le, no_complex=True)\n\n    def test_broadcasted_gt(self):\n        self.check_array_broadcasted_op(operator.gt, no_complex=True)\n\n    def test_broadcasted_ge(self):\n        self.check_array_broadcasted_op(operator.ge, no_complex=True)\n\n    def test_broadcasted_eq(self):\n        self.check_array_broadcasted_op(operator.eq)\n\n    def test_broadcasted_ne(self):\n        self.check_array_broadcasted_op(operator.ne)\n\n    @testing.for_all_dtypes_combination(names=[\'x_type\', \'y_type\'])\n    @testing.numpy_cupy_allclose()\n    def check_array_doubly_broadcasted_op(self, op, xp, x_type, y_type,\n                                          no_bool=False, no_complex=False):\n        x_dtype = numpy.dtype(x_type)\n        y_dtype = numpy.dtype(y_type)\n        if no_bool and x_dtype == \'?\' and y_dtype == \'?\':\n            return xp.array(True)\n        if no_complex and (x_dtype.kind == \'c\' or y_dtype.kind == \'c\'):\n            return xp.array(True)\n        a = xp.array([[[1, 2, 3]], [[4, 5, 6]]], x_type)\n        b = xp.array([[1], [2], [3]], y_type)\n        return op(a, b)\n\n    def test_doubly_broadcasted_add(self):\n        self.check_array_doubly_broadcasted_op(operator.add)\n\n    def test_doubly_broadcasted_sub(self):\n        self.check_array_doubly_broadcasted_op(operator.sub, no_bool=True)\n\n    def test_doubly_broadcasted_mul(self):\n        self.check_array_doubly_broadcasted_op(operator.mul)\n\n    def test_doubly_broadcasted_truediv(self):\n        with testing.NumpyError(divide=\'ignore\', invalid=\'ignore\'):\n            self.check_array_doubly_broadcasted_op(operator.truediv)\n\n    def test_doubly_broadcasted_floordiv(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_doubly_broadcasted_op(operator.floordiv,\n                                                   no_complex=True)\n\n    def test_doubly_broadcasted_pow(self):\n        self.check_array_doubly_broadcasted_op(operator.pow)\n\n    def test_doubly_broadcasted_divmod0(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_doubly_broadcasted_op(\n                lambda x, y: divmod(x, y)[0],\n                no_complex=True)\n\n    def test_doubly_broadcasted_divmod1(self):\n        with testing.NumpyError(divide=\'ignore\'):\n            self.check_array_doubly_broadcasted_op(\n                lambda x, y: divmod(x, y)[1],\n                no_complex=True)\n\n    def test_doubly_broadcasted_lt(self):\n        self.check_array_doubly_broadcasted_op(operator.lt, no_complex=True)\n\n    def test_doubly_broadcasted_le(self):\n        self.check_array_doubly_broadcasted_op(operator.le, no_complex=True)\n\n    def test_doubly_broadcasted_gt(self):\n        self.check_array_doubly_broadcasted_op(operator.gt, no_complex=True)\n\n    def test_doubly_broadcasted_ge(self):\n        self.check_array_doubly_broadcasted_op(operator.ge, no_complex=True)\n\n    def test_doubly_broadcasted_eq(self):\n        self.check_array_doubly_broadcasted_op(operator.eq)\n\n    def test_doubly_broadcasted_ne(self):\n        self.check_array_doubly_broadcasted_op(operator.ne)\n\n    @testing.for_all_dtypes_combination(names=[\'x_type\', \'y_type\'])\n    @testing.numpy_cupy_allclose()\n    def check_array_reversed_op(self, op, xp, x_type, y_type, no_bool=False):\n        if no_bool and x_type == numpy.bool_ and y_type == numpy.bool_:\n            return xp.array(True)\n        a = xp.array([1, 2, 3, 4, 5], x_type)\n        b = xp.array([1, 2, 3, 4, 5], y_type)\n        return op(a, b[::-1])\n\n    def test_array_reversed_add(self):\n        self.check_array_reversed_op(operator.add)\n\n    def test_array_reversed_sub(self):\n        self.check_array_reversed_op(operator.sub, no_bool=True)\n\n    def test_array_reversed_mul(self):\n        self.check_array_reversed_op(operator.mul)\n\n    @testing.for_all_dtypes(no_bool=True)\n    def check_typecast(self, val, dtype):\n        operators = [\n            operator.add, operator.sub, operator.mul, operator.truediv]\n\n        for op in operators:\n            with testing.NumpyError(divide=\'ignore\', invalid=\'ignore\'):\n                a = op(val, (testing.shaped_arange((5,), numpy, dtype) - 2))\n            b = op(val, (testing.shaped_arange((5,), cupy, dtype) - 2))\n            self.assertEqual(a.dtype, b.dtype)\n\n    def test_typecast_bool1(self):\n        self.check_typecast(True)\n\n    def test_typecast_bool2(self):\n        self.check_typecast(False)\n\n    def test_typecast_int1(self):\n        self.check_typecast(0)\n\n    def test_typecast_int2(self):\n        self.check_typecast(-127)\n\n    def test_typecast_int3(self):\n        self.check_typecast(255)\n\n    def test_typecast_int4(self):\n        self.check_typecast(-32768)\n\n    def test_typecast_int5(self):\n        self.check_typecast(65535)\n\n    def test_typecast_int6(self):\n        self.check_typecast(-2147483648)\n\n    def test_typecast_int7(self):\n        self.check_typecast(4294967295)\n\n    def test_typecast_float1(self):\n        self.check_typecast(0.0)\n\n    def test_typecast_float2(self):\n        self.check_typecast(100000.0)\n\n\n@testing.gpu\nclass TestArrayIntElementwiseOp(unittest.TestCase):\n\n    @testing.for_all_dtypes_combination(names=[\'x_type\', \'y_type\'])\n    @testing.numpy_cupy_allclose(accept_error=TypeError)\n    def check_array_scalar_op(self, op, xp, x_type, y_type, swap=False):\n        a = xp.array([[0, 1, 2], [1, 0, 2]], dtype=x_type)\n        if swap:\n            return op(y_type(2), a)\n        else:\n            return op(a, y_type(2))\n\n    def test_lshift_scalar(self):\n        self.check_array_scalar_op(operator.lshift)\n\n    def test_rlshift_scalar(self):\n        self.check_array_scalar_op(operator.lshift, swap=True)\n\n    def test_rshift_scalar(self):\n        self.check_array_scalar_op(operator.rshift)\n\n    def test_rrshift_scalar(self):\n        self.check_array_scalar_op(operator.rshift, swap=True)\n\n    def test_and_scalar(self):\n        self.check_array_scalar_op(operator.and_)\n\n    def test_rand_scalar(self):\n        self.check_array_scalar_op(operator.and_, swap=True)\n\n    def test_or_scalar(self):\n        self.check_array_scalar_op(operator.or_)\n\n    def test_ror_scalar(self):\n        self.check_array_scalar_op(operator.or_, swap=True)\n\n    def test_xor_scalar(self):\n        self.check_array_scalar_op(operator.xor)\n\n    def test_rxor_scalar(self):\n        self.check_array_scalar_op(operator.xor, swap=True)\n\n    def test_mod_scalar(self):\n        with testing.NumpyError(divide=\'ignore\', invalid=\'ignore\'):\n            self.check_array_scalar_op(operator.mod)\n\n    def test_rmod_scalar(self):\n        with testing.NumpyError(divide=\'ignore\', invalid=\'ignore\'):\n            self.check_array_scalar_op(operator.mod, swap=True)\n\n    @testing.for_all_dtypes_combination(names=[\'x_type\', \'y_type\'])\n    @testing.numpy_cupy_allclose(accept_error=TypeError)\n    def check_array_scalarzero_op(self, op, xp, x_type, y_type, swap=False):\n        a = xp.array([[0, 1, 2], [1, 0, 2]], dtype=x_type)\n        if swap:\n            return op(y_type(0), a)\n        else:\n            return op(a, y_type(0))\n\n    def test_lshift_scalarzero(self):\n        self.check_array_scalarzero_op(operator.lshift)\n\n    def test_rlshift_scalarzero(self):\n        self.check_array_scalarzero_op(operator.lshift, swap=True)\n\n    def test_rshift_scalarzero(self):\n        self.check_array_scalarzero_op(operator.rshift)\n\n    def test_rrshift_scalarzero(self):\n        self.check_array_scalarzero_op(operator.rshift, swap=True)\n\n    def test_and_scalarzero(self):\n        self.check_array_scalarzero_op(operator.and_)\n\n    def test_rand_scalarzero(self):\n        self.check_array_scalarzero_op(operator.and_, swap=True)\n\n    def test_or_scalarzero(self):\n        self.check_array_scalarzero_op(operator.or_)\n\n    def test_ror_scalarzero(self):\n        self.check_array_scalarzero_op(operator.or_, swap=True)\n\n    def test_xor_scalarzero(self):\n        self.check_array_scalarzero_op(operator.xor)\n\n    def test_rxor_scalarzero(self):\n        self.check_array_scalarzero_op(operator.xor, swap=True)\n\n    def test_mod_scalarzero(self):\n        with testing.NumpyError(divide=\'ignore\', invalid=\'ignore\'):\n            self.check_array_scalarzero_op(operator.mod)\n\n    def test_rmod_scalarzero(self):\n        with testing.NumpyError(divide=\'ignore\', invalid=\'ignore\'):\n            self.check_array_scalarzero_op(operator.mod, swap=True)\n\n    @testing.for_all_dtypes_combination(names=[\'x_type\', \'y_type\'])\n    @testing.numpy_cupy_allclose(accept_error=TypeError)\n    def check_array_array_op(self, op, xp, x_type, y_type):\n        a = xp.array([[0, 1, 2], [1, 0, 2]], dtype=x_type)\n        b = xp.array([[0, 0, 1], [0, 1, 2]], dtype=y_type)\n        return op(a, b)\n\n    def test_lshift_array(self):\n        self.check_array_array_op(operator.lshift)\n\n    def test_ilshift_array(self):\n        self.check_array_array_op(operator.ilshift)\n\n    def test_rshift_array(self):\n        self.check_array_array_op(operator.rshift)\n\n    def test_irshift_array(self):\n        self.check_array_array_op(operator.irshift)\n\n    def test_and_array(self):\n        self.check_array_array_op(operator.and_)\n\n    def test_iand_array(self):\n        self.check_array_array_op(operator.iand)\n\n    def test_or_array(self):\n        self.check_array_array_op(operator.or_)\n\n    def test_ior_array(self):\n        self.check_array_array_op(operator.ior)\n\n    def test_xor_array(self):\n        self.check_array_array_op(operator.xor)\n\n    def test_ixor_array(self):\n        self.check_array_array_op(operator.ixor)\n\n    def test_mod_array(self):\n        with testing.NumpyError(divide=\'ignore\', invalid=\'ignore\'):\n            self.check_array_array_op(operator.mod)\n\n    def test_imod_array(self):\n        with testing.NumpyError(divide=\'ignore\', invalid=\'ignore\'):\n            self.check_array_array_op(operator.imod)\n\n    @testing.for_all_dtypes_combination(names=[\'x_type\', \'y_type\'])\n    @testing.numpy_cupy_allclose(accept_error=TypeError)\n    def check_array_broadcasted_op(self, op, xp, x_type, y_type):\n        a = xp.array([[0, 1, 2], [1, 0, 2], [2, 1, 0]], dtype=x_type)\n        b = xp.array([[0, 0, 1]], dtype=y_type)\n        return op(a, b)\n\n    def test_broadcasted_lshift(self):\n        self.check_array_broadcasted_op(operator.lshift)\n\n    def test_broadcasted_ilshift(self):\n        self.check_array_broadcasted_op(operator.ilshift)\n\n    def test_broadcasted_rshift(self):\n        self.check_array_broadcasted_op(operator.rshift)\n\n    def test_broadcasted_irshift(self):\n        self.check_array_broadcasted_op(operator.irshift)\n\n    def test_broadcasted_and(self):\n        self.check_array_broadcasted_op(operator.and_)\n\n    def test_broadcasted_iand(self):\n        self.check_array_broadcasted_op(operator.iand)\n\n    def test_broadcasted_or(self):\n        self.check_array_broadcasted_op(operator.or_)\n\n    def test_broadcasted_ior(self):\n        self.check_array_broadcasted_op(operator.ior)\n\n    def test_broadcasted_xor(self):\n        self.check_array_broadcasted_op(operator.xor)\n\n    def test_broadcasted_ixor(self):\n        self.check_array_broadcasted_op(operator.ixor)\n\n    def test_broadcasted_mod(self):\n        with testing.NumpyError(divide=\'ignore\', invalid=\'ignore\'):\n            self.check_array_broadcasted_op(operator.mod)\n\n    def test_broadcasted_imod(self):\n        with testing.NumpyError(divide=\'ignore\', invalid=\'ignore\'):\n            self.check_array_broadcasted_op(operator.imod)\n\n    @testing.for_all_dtypes_combination(names=[\'x_type\', \'y_type\'])\n    @testing.numpy_cupy_allclose(accept_error=TypeError)\n    def check_array_doubly_broadcasted_op(self, op, xp, x_type, y_type):\n        a = xp.array([[[0, 1, 2]], [[1, 0, 2]]], dtype=x_type)\n        b = xp.array([[0], [0], [1]], dtype=y_type)\n        return op(a, b)\n\n    def test_doubly_broadcasted_lshift(self):\n        self.check_array_doubly_broadcasted_op(operator.lshift)\n\n    def test_doubly_broadcasted_rshift(self):\n        self.check_array_doubly_broadcasted_op(operator.rshift)\n\n    def test_doubly_broadcasted_and(self):\n        self.check_array_doubly_broadcasted_op(operator.and_)\n\n    def test_doubly_broadcasted_or(self):\n        self.check_array_doubly_broadcasted_op(operator.or_)\n\n    def test_doubly_broadcasted_xor(self):\n        self.check_array_doubly_broadcasted_op(operator.xor)\n\n    def test_doubly_broadcasted_mod(self):\n        with testing.NumpyError(divide=\'ignore\', invalid=\'ignore\'):\n            self.check_array_doubly_broadcasted_op(operator.mod)\n'"
tests/cupy_tests/core_tests/test_ndarray_get.py,0,"b""import unittest\n\nimport cupy\nfrom cupy import cuda\nfrom cupy import testing\nimport numpy\nfrom numpy import testing as np_testing\n\n\n@testing.gpu\nclass TestArrayGet(unittest.TestCase):\n\n    def setUp(self):\n        self.stream = cuda.Stream.null\n\n    def check_get(self, f, stream, order='C'):\n        a_gpu = f(cupy)\n        a_cpu = a_gpu.get(stream, order=order)\n        if stream:\n            stream.synchronize()\n        b_cpu = f(numpy)\n        np_testing.assert_array_equal(a_cpu, b_cpu)\n        if order == 'F' or (order == 'A' and a_gpu.flags.f_contiguous):\n            assert a_cpu.flags.f_contiguous\n        else:\n            assert a_cpu.flags.c_contiguous\n\n    @testing.for_orders('CFA')\n    @testing.for_all_dtypes()\n    def test_contiguous_array(self, dtype, order):\n        def contiguous_array(xp):\n            return testing.shaped_arange((3,), xp, dtype, order)\n        self.check_get(contiguous_array, None, order)\n\n    @testing.for_orders('CFA')\n    @testing.for_all_dtypes()\n    def test_non_contiguous_array(self, dtype, order):\n        def non_contiguous_array(xp):\n            return testing.shaped_arange((3, 3), xp, dtype, order)[0::2, 0::2]\n        self.check_get(non_contiguous_array, None, order)\n\n    @testing.for_orders('CFA')\n    @testing.for_all_dtypes()\n    def test_contiguous_array_stream(self, dtype, order):\n        def contiguous_array(xp):\n            return testing.shaped_arange((3,), xp, dtype, order)\n        self.check_get(contiguous_array, self.stream, order)\n\n    @testing.for_orders('CFA')\n    @testing.for_all_dtypes()\n    def test_non_contiguous_array_stream(self, dtype, order):\n        def non_contiguous_array(xp):\n            return testing.shaped_arange((3, 3), xp, dtype, order)[0::2, 0::2]\n        self.check_get(non_contiguous_array, self.stream)\n\n    @testing.multi_gpu(2)\n    @testing.for_orders('CFA')\n    @testing.for_all_dtypes()\n    def test_get_multigpu(self, dtype, order):\n        with cuda.Device(1):\n            src = testing.shaped_arange((2, 3), cupy, dtype, order)\n            src = cupy.asfortranarray(src)\n        with cuda.Device(0):\n            dst = src.get()\n        expected = testing.shaped_arange((2, 3), numpy, dtype, order)\n        np_testing.assert_array_equal(dst, expected)\n\n\n@testing.gpu\nclass TestArrayGetWithOut(unittest.TestCase):\n\n    def setUp(self):\n        self.stream = cuda.Stream.null\n\n    def check_get(self, f, out, stream):\n        a_gpu = f(cupy)\n        a_cpu = a_gpu.get(stream, out=out)\n        if stream:\n            stream.synchronize()\n        b_cpu = f(numpy)\n        assert a_cpu is out\n        np_testing.assert_array_equal(a_cpu, b_cpu)\n\n    @testing.for_orders('CF')\n    @testing.for_all_dtypes()\n    def test_contiguous_array(self, dtype, order):\n        def contiguous_array(xp):\n            return testing.shaped_arange((3,), xp, dtype, order)\n        out = numpy.empty((3,), dtype, order)\n        self.check_get(contiguous_array, out, None)\n\n    @testing.for_orders('CF')\n    @testing.for_all_dtypes()\n    def test_contiguous_array_cross(self, dtype, order):\n        def contiguous_array(xp):\n            return testing.shaped_arange((3,), xp, dtype, order)\n        out_order = 'C' if order == 'F' else 'F'\n        out = numpy.empty((3,), dtype, out_order)\n        self.check_get(contiguous_array, out, None)\n\n    @testing.for_orders('CF')\n    @testing.for_all_dtypes()\n    def test_contiguous_array_with_error(self, dtype, order):\n        out = numpy.empty((3, 3), dtype)[0:2, 0:2]\n        with self.assertRaises(RuntimeError):\n            a_gpu = testing.shaped_arange((3, 3), cupy, dtype, order)[0:2, 0:2]\n            a_gpu.get(out=out)\n\n    @testing.for_orders('CF')\n    @testing.for_all_dtypes()\n    def test_non_contiguous_array(self, dtype, order):\n        def non_contiguous_array(xp):\n            return testing.shaped_arange((3, 3), xp, dtype, order)[0::2, 0::2]\n        out = numpy.empty((2, 2), dtype, order)\n        self.check_get(non_contiguous_array, out, None)\n\n    @testing.for_orders('CF')\n    @testing.for_all_dtypes()\n    def test_contiguous_array_stream(self, dtype, order):\n        def contiguous_array(xp):\n            return testing.shaped_arange((3,), xp, dtype, order)\n        out = numpy.empty((3,), dtype, order)\n        self.check_get(contiguous_array, out, self.stream)\n\n    @testing.for_orders('CF')\n    @testing.for_all_dtypes()\n    def test_non_contiguous_array_stream(self, dtype, order):\n        def non_contiguous_array(xp):\n            return testing.shaped_arange((3, 3), xp, dtype, order)[0::2, 0::2]\n        out = numpy.empty((2, 2), dtype, order)\n        self.check_get(non_contiguous_array, out, self.stream)\n\n    @testing.multi_gpu(2)\n    @testing.for_orders('CF')\n    @testing.for_all_dtypes()\n    def test_get_multigpu(self, dtype, order):\n        with cuda.Device(1):\n            src = testing.shaped_arange((2, 3), cupy, dtype, order)\n            src = cupy.asfortranarray(src)\n        with cuda.Device(0):\n            dst = numpy.empty((2, 3), dtype, order)\n            src.get(out=dst)\n        expected = testing.shaped_arange((2, 3), numpy, dtype, order)\n        np_testing.assert_array_equal(dst, expected)\n"""
tests/cupy_tests/core_tests/test_ndarray_indexing.py,0,"b""import unittest\nimport warnings\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.parameterize(\n    {'shape': (2, 3, 4), 'transpose': None, 'indexes': (1, 0, 2)},\n    {'shape': (2, 3, 4), 'transpose': None, 'indexes': (-1, 0, -2)},\n    {'shape': (2, 3, 4), 'transpose': (2, 0, 1), 'indexes': (1, 0, 2)},\n    {'shape': (2, 3, 4), 'transpose': (2, 0, 1), 'indexes': (-1, 0, -2)},\n    {'shape': (2, 3, 4), 'transpose': None,\n     'indexes': (slice(None), slice(None, 1), slice(2))},\n    {'shape': (2, 3, 4), 'transpose': None,\n     'indexes': (slice(None), slice(None, -1), slice(-2))},\n    {'shape': (2, 3, 4), 'transpose': (2, 0, 1),\n     'indexes': (slice(None), slice(None, 1), slice(2))},\n    {'shape': (2, 3, 5), 'transpose': None,\n     'indexes': (slice(None, None, -1), slice(1, None, -1), slice(4, 1, -2))},\n    {'shape': (2, 3, 5), 'transpose': (2, 0, 1),\n     'indexes': (slice(4, 1, -2), slice(None, None, -1), slice(1, None, -1))},\n    {'shape': (2, 3, 4), 'transpose': None, 'indexes': (Ellipsis, 2)},\n    {'shape': (2, 3, 4), 'transpose': None, 'indexes': (1, Ellipsis)},\n    {'shape': (2, 3, 4, 5), 'transpose': None, 'indexes': (1, Ellipsis, 3)},\n    {'shape': (2, 3, 4), 'transpose': None,\n     'indexes': (1, None, slice(2), None, 2)},\n    {'shape': (2, 3), 'transpose': None, 'indexes': (None,)},\n    {'shape': (2,), 'transpose': None, 'indexes': (slice(None,), None)},\n    {'shape': (), 'transpose': None, 'indexes': (None,)},\n    {'shape': (), 'transpose': None, 'indexes': (None, None)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(10, -9, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-9, -10, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-1, -10, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-1, -11, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-11, -11, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(10, -9, -3),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-1, -11, -3),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(1, -5, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(0, -5, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-1, -5, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-4, -5, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-5, -5, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-6, -5, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-10, -5, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-11, -5, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-12, -5, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-5, 1, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-5, 0, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-5, -1, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-5, -4, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-5, -5, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-5, -6, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-5, -10, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-5, -11, -1),)},\n    {'shape': (10,), 'transpose': None, 'indexes': (slice(-5, -12, -1),)},\n    # reversing indexing on empty dimension\n    {'shape': (0,), 'transpose': None, 'indexes': (slice(None, None, -1),)},\n    {'shape': (0, 0), 'transpose': None,\n     'indexes': (slice(None, None, -1), slice(None, None, -1))},\n    {'shape': (0, 0), 'transpose': None,\n     'indexes': (None, slice(None, None, -1))},\n    {'shape': (0, 0), 'transpose': None,\n     'indexes': (slice(None, None, -1), None)},\n    {'shape': (0, 1), 'transpose': None,\n     'indexes': (slice(None, None, -1), None)},\n    {'shape': (1, 0), 'transpose': None,\n     'indexes': (None, slice(None, None, -1))},\n    {'shape': (1, 0, 1), 'transpose': None,\n     'indexes': (None, slice(None, None, -1), None)},\n    #\n    {'shape': (2, 0), 'transpose': None,\n     'indexes': (1, slice(None, None, None))},\n)\n@testing.gpu\nclass TestArrayIndexingParameterized(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_getitem(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        if self.transpose:\n            a = a.transpose(self.transpose)\n        return a[self.indexes]\n\n\n@testing.parameterize(\n    {'shape': (), 'transpose': None, 'indexes': 0},\n    {'shape': (), 'transpose': None, 'indexes': (slice(0, 1, 0),)},\n    {'shape': (2, 3), 'transpose': None, 'indexes': (0, 0, 0)},\n    {'shape': (2, 3, 4), 'transpose': None, 'indexes': -3},\n    {'shape': (2, 3, 4), 'transpose': (2, 0, 1), 'indexes': -5},\n    {'shape': (2, 3, 4), 'transpose': None, 'indexes': 3},\n    {'shape': (2, 3, 4), 'transpose': (2, 0, 1), 'indexes': 5},\n    {'shape': (2, 3, 4), 'transpose': None,\n     'indexes': (slice(0, 1, 0), )},\n    {'shape': (2, 3, 4), 'transpose': None,\n     'indexes': (slice((0, 0), None, None), )},\n    {'shape': (2, 3, 4), 'transpose': None,\n     'indexes': (slice(None, (0, 0), None), )},\n    {'shape': (2, 3, 4), 'transpose': None,\n     'indexes': (slice(None, None, (0, 0)), )},\n)\n@testing.gpu\nclass TestArrayInvalidIndex(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_invalid_getitem(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange(self.shape, xp, dtype)\n            if self.transpose:\n                a = a.transpose(self.transpose)\n            with pytest.raises((ValueError, IndexError, TypeError)):\n                a[self.indexes]\n\n\n@testing.gpu\nclass TestArrayIndex(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_setitem_constant(self, xp, dtype):\n        a = xp.zeros((2, 3, 4), dtype=dtype)\n        a[:] = 1\n        return a\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_setitem_partial_constant(self, xp, dtype):\n        a = xp.zeros((2, 3, 4), dtype=dtype)\n        a[1, 1:3] = 1\n        return a\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_setitem_copy(self, xp, dtype):\n        a = xp.zeros((2, 3, 4), dtype=dtype)\n        b = testing.shaped_arange((2, 3, 4), xp, dtype)\n        a[:] = b\n        return a\n\n    @testing.for_all_dtypes_combination(('src_type', 'dst_type'))\n    @testing.numpy_cupy_array_equal()\n    def test_setitem_different_type(self, xp, src_type, dst_type):\n        a = xp.zeros((2, 3, 4), dtype=dst_type)\n        b = testing.shaped_arange((2, 3, 4), xp, src_type)\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore', numpy.ComplexWarning)\n            a[:] = b\n        return a\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_setitem_partial_copy(self, xp, dtype):\n        a = xp.zeros((2, 3, 4), dtype=dtype)\n        b = testing.shaped_arange((3, 2), xp, dtype)\n        a[1, ::-1, 1:4:2] = b\n        return a\n\n    @testing.numpy_cupy_array_equal()\n    def test_T(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return a.T\n\n    @testing.numpy_cupy_array_equal()\n    def test_T_vector(self, xp):\n        a = testing.shaped_arange((4,), xp)\n        return a.T\n"""
tests/cupy_tests/core_tests/test_ndarray_math.py,0,"b""import unittest\n\nimport numpy\n\nfrom cupy import testing\n\n\n@testing.parameterize(*testing.product({\n    'decimals': [-2, -1, 0, 1, 2],\n}))\nclass TestRound(unittest.TestCase):\n\n    shape = (20,)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_round(self, xp, dtype):\n        if dtype == numpy.bool_:\n            # avoid cast problem\n            a = testing.shaped_random(self.shape, xp, scale=10, dtype=dtype)\n            return a.round(0)\n        if dtype == numpy.float16:\n            # avoid accuracy problem\n            a = testing.shaped_random(self.shape, xp, scale=10, dtype=dtype)\n            return a.round(0)\n        a = testing.shaped_random(self.shape, xp, scale=100, dtype=dtype)\n        return a.round(self.decimals)\n\n    @testing.numpy_cupy_array_equal()\n    def test_round_out(self, xp):\n        a = testing.shaped_random(self.shape, xp, scale=100, dtype='d')\n        out = xp.empty_like(a)\n        a.round(self.decimals, out)\n        return out\n\n\n@testing.parameterize(*testing.product({\n    # limit to:\n    # * <=0: values like 0.35 and 0.035 cannot be expressed exactly in IEEE 754\n    # * >-4: to avoid float16 overflow\n    'decimals': [-3, -2, -1, 0],\n}))\nclass TestRoundHalfway(unittest.TestCase):\n\n    shape = (20,)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_round_halfway_float(self, xp, dtype):\n        # generate [..., -1.5, -0.5, 0.5, 1.5, ...] * 10^{-decimals}\n        a = testing.shaped_arange(self.shape, xp, dtype=dtype)\n        a *= 2\n        a -= a.size + 1\n        scale = 10**abs(self.decimals)\n        if self.decimals < 0:\n            a *= scale\n        else:\n            a /= scale\n        a /= 2\n\n        return a.round(self.decimals)\n\n    @testing.for_signed_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_round_halfway_int(self, xp, dtype):\n        # generate [..., -1.5, -0.5, 0.5, 1.5, ...] * 10^{-decimals}\n        a = testing.shaped_arange(self.shape, xp, dtype=dtype)\n        a *= 2\n        a -= a.size + 1\n        scale = 10**abs(self.decimals)\n        if self.decimals < 0:\n            a *= xp.array(scale, dtype=dtype)\n        a >>= 1\n\n        return a.round(self.decimals)\n\n    @testing.for_unsigned_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_round_halfway_uint(self, xp, dtype):\n        # generate [0.5, 1.5, ...] * 10^{-decimals}\n        a = testing.shaped_arange(self.shape, xp, dtype=dtype)\n        a *= 2\n        a -= 1\n        scale = 10**abs(self.decimals)\n        if self.decimals < 0:\n            a *= xp.array(scale, dtype=dtype)\n        a >>= 1\n\n        return a.round(self.decimals)\n\n\n@testing.parameterize(*testing.product({\n    'decimals': [-5, -4, -3, -2, -1, 0]\n}))\nclass TestRoundMinMax(unittest.TestCase):\n\n    @unittest.skip('Known incompatibility: see core.pyx')\n    @testing.numpy_cupy_array_equal()\n    def _test_round_int64(self, xp):\n        a = xp.array([-2**62, 2**62], dtype=xp.int64)\n        return a.round(self.decimals)\n\n    @unittest.skip('Known incompatibility: see core.pyx')\n    @testing.numpy_cupy_array_equal()\n    def test_round_uint64(self, xp):\n        a = xp.array([2**63], dtype=xp.uint64)\n        return a.round(self.decimals)\n\n    @unittest.skip('Known incompatibility: see core.pyx')\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_round_minmax(self, xp, dtype):\n        a = xp.array([xp.iinfo(dtype).min, xp.iinfo(dtype).max], dtype=dtype)\n        return a.round(self.decimals)\n"""
tests/cupy_tests/core_tests/test_ndarray_owndata.py,0,b'import unittest\n\nfrom cupy import core\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestArrayOwndata(unittest.TestCase):\n\n    def setUp(self):\n        self.a = core.ndarray(())\n\n    def test_original_array(self):\n        self.assertTrue(self.a.flags.owndata)\n\n    def test_view_array(self):\n        v = self.a.view()\n        self.assertFalse(v.flags.owndata)\n\n    def test_reshaped_array(self):\n        r = self.a.reshape(())\n        self.assertFalse(r.flags.owndata)\n'
tests/cupy_tests/core_tests/test_ndarray_reduction.py,0,"b""import unittest\n\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestArrayReduction(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_max_all(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return a.max()\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_max_all_keepdims(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return a.max(keepdims=True)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_max_axis_large(self, xp, dtype):\n        a = testing.shaped_random((3, 1000), xp, dtype)\n        return a.max(axis=0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_max_axis0(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.max(axis=0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_max_axis1(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.max(axis=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_max_axis2(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.max(axis=2)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_max_multiple_axes(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.max(axis=(1, 2))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_max_multiple_axes_keepdims(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.max(axis=(1, 2), keepdims=True)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_max_nan(self, xp, dtype):\n        a = xp.array([float('nan'), 1, -1], dtype)\n        return a.max()\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_max_nan_real(self, xp, dtype):\n        a = xp.array([float('nan'), 1, -1], dtype)\n        return a.max()\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_max_nan_imag(self, xp, dtype):\n        a = xp.array([float('nan')*1.j, 1.j, -1.j], dtype)\n        return a.max()\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_min_all(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return a.min()\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_min_all_keepdims(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return a.min(keepdims=True)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_min_axis_large(self, xp, dtype):\n        a = testing.shaped_random((3, 1000), xp, dtype)\n        return a.min(axis=0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_min_axis0(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.min(axis=0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_min_axis1(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.min(axis=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_min_axis2(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.min(axis=2)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_min_multiple_axes(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.min(axis=(1, 2))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_min_multiple_axes_keepdims(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.min(axis=(1, 2), keepdims=True)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_min_nan(self, xp, dtype):\n        a = xp.array([float('nan'), 1, -1], dtype)\n        return a.min()\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_min_nan_real(self, xp, dtype):\n        a = xp.array([float('nan'), 1, -1], dtype)\n        return a.min()\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_min_nan_imag(self, xp, dtype):\n        a = xp.array([float('nan')*1.j, 1.j, -1.j], dtype)\n        return a.min()\n\n    # skip bool: numpy's ptp raises a TypeError on bool inputs\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_ptp_all(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return a.ptp()\n\n    @testing.with_requires('numpy>=1.15')\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_ptp_all_keepdims(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return a.ptp(keepdims=True)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_ptp_axis_large(self, xp, dtype):\n        a = testing.shaped_random((3, 1000), xp, dtype)\n        return a.ptp(axis=0)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_ptp_axis0(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.ptp(axis=0)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_ptp_axis1(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.ptp(axis=1)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_ptp_axis2(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.ptp(axis=2)\n\n    @testing.with_requires('numpy>=1.15')\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_ptp_multiple_axes(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.ptp(axis=(1, 2))\n\n    @testing.with_requires('numpy>=1.15')\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_ptp_multiple_axes_keepdims(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.ptp(axis=(1, 2), keepdims=True)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_ptp_nan(self, xp, dtype):\n        a = xp.array([float('nan'), 1, -1], dtype)\n        return a.ptp()\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_ptp_nan_real(self, xp, dtype):\n        a = xp.array([float('nan'), 1, -1], dtype)\n        return a.ptp()\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_ptp_nan_imag(self, xp, dtype):\n        a = xp.array([float('nan')*1.j, 1.j, -1.j], dtype)\n        return a.ptp()\n"""
tests/cupy_tests/core_tests/test_ndarray_scatter.py,0,"b""import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.parameterize(\n    # array only\n    {'shape': (2, 3, 4), 'slices': numpy.array(-1), 'value': 1},\n    {'shape': (2, 3, 4), 'slices': numpy.array([1, 0]), 'value': 1},\n    {'shape': (2, 3, 4), 'slices': (slice(None), [1, 2]), 'value': 1},\n    {'shape': (3, 4, 5),\n     'slices': (slice(None), [[1, 2], [0, -1]],), 'value': 1},\n    {'shape': (3, 4, 5),\n     'slices': (slice(None), slice(None), [[1, 2], [0, 3]]), 'value': 1},\n    # array with duplicate indices\n    {'shape': (2, 3), 'slices': ([1, 1], slice(None)), 'value': 1},\n    {'shape': (2, 3), 'slices': ([1, 0, 1], slice(None)), 'value': 1},\n    {'shape': (2, 3), 'slices': (slice(1, 2), [1, 0, 1]), 'value': 1},\n    # slice and array\n    {'shape': (3, 4, 5),\n     'slices': (slice(None), slice(1, 2), [[1, 3], [0, 2]]), 'value': 1},\n    # None and array\n    {'shape': (3, 4, 5),\n     'slices': (None, [1, -1]), 'value': 1},\n    {'shape': (3, 4, 5),\n     'slices': (None, [1, -1], None), 'value': 1},\n    {'shape': (3, 4, 5),\n     'slices': (None, None, None, [1, -1]), 'value': 1},\n    # None, slice and array\n    {'shape': (3, 4, 5),\n     'slices': (slice(0, 1), None, [1, -1]), 'value': 1},\n    {'shape': (3, 4, 5),\n     'slices': (slice(0, 1), slice(1, 2), [1, -1]), 'value': 1},\n    {'shape': (3, 4, 5),\n     'slices': (slice(0, 1), None, slice(1, 2), [1, -1]), 'value': 1},\n    # broadcasting\n    {'shape': (3, 4, 5), 'slices': (slice(None), [[1, 2], [0, -1]],),\n     'value': numpy.arange(3 * 2 * 2 * 5).reshape(3, 2, 2, 5)},\n    # multiple integer arrays\n    {'shape': (2, 3, 4), 'slices': ([1, 0], [2, 1]),\n     'value': numpy.arange(2 * 4).reshape(2, 4)},\n    {'shape': (2, 3, 4), 'slices': ([1, 0], slice(None), [2, 1]),\n     'value': numpy.arange(2 * 3).reshape(2, 3)},\n    {'shape': (2, 3, 4), 'slices': ([1, 0], slice(None), [[2, 0], [3, 1]]),\n     'value': numpy.arange(2 * 2 * 3).reshape(2, 2, 3)},\n    {'shape': (1, 1, 2, 3, 4),\n     'slices': (None, slice(None), 0, [1, 0], slice(0, 2, 2), [2, -1]),\n     'value': 1},\n    # multiple integer arrays duplicate\n    {'shape': (2, 3, 4), 'slices': ([1, 1], [1, 1]),\n     'value': numpy.arange(2 * 4).reshape(2, 4)},\n    {'shape': (2, 3, 4), 'slices': ([1, 1], slice(None), [[2, 2], [3, 1]]),\n     'value': numpy.arange(2 * 2 * 3).reshape(2, 2, 3)},\n    {'shape': (2, 3, 4), 'slices': ([1, 1], 1, [[2, 2], [3, 1]]),\n     'value': numpy.arange(2 * 2).reshape(2, 2)},\n    # mask\n    {'shape': (3, 4, 5),\n     'slices': (numpy.random.choice([False, True], (3, 4, 5)),),\n     'value': 1},\n    {'shape': (3, 4, 5),\n     'slices': (numpy.random.choice([False, True], (3,)),),\n     'value': numpy.arange(4 * 5).reshape(4, 5)},\n    {'shape': (3, 4, 5),\n     'slices': (slice(None), numpy.array([True, False, False, True]),),\n     'value': numpy.arange(3 * 2 * 5).reshape(3, 2, 5)},\n    # empty arrays\n    {'shape': (2, 3, 4), 'slices': [], 'value': 1},\n    {'shape': (2, 3, 4), 'slices': [],\n     'value': numpy.array([1, 1, 1, 1])},\n    {'shape': (2, 3, 4), 'slices': [],\n     'value': numpy.random.uniform(size=(3, 4))},\n    {'shape': (2, 3, 4), 'slices': numpy.array([], dtype=numpy.int32),\n     'value': 1},\n    {'shape': (2, 3, 4), 'slices': [[]],\n     'value': 1},\n    {'shape': (2, 3, 4), 'slices': numpy.array([[]], dtype=numpy.int32),\n     'value': numpy.random.uniform(size=(3, 4))},\n    {'shape': (2, 3, 4), 'slices': [[[]]],\n     'value': 1},\n    {'shape': (2, 3, 4), 'slices': [[[[]]]],\n     'value': 1},\n    {'shape': (2, 3, 4, 5), 'slices': [[[[]]]],\n     'value': 1},\n    {'shape': (2, 3, 4, 5), 'slices': [[[[[]]]]],\n     'value': 1},\n    {'shape': (2, 3, 4), 'slices': (slice(None), []),\n     'value': 1},\n    {'shape': (2, 3, 4), 'slices': ([], []),\n     'value': 1},\n    {'shape': (2, 3, 4), 'slices': numpy.array([], dtype=numpy.bool),\n     'value': 1},\n    {'shape': (2, 3, 4),\n     'slices': (slice(None), numpy.array([], dtype=numpy.bool)),\n     'value': 1},\n    {'shape': (2, 3, 4), 'slices': numpy.array([[], []], dtype=numpy.bool),\n     'value': numpy.random.uniform(size=(4,))},\n    # list indexes\n    {'shape': (2, 3, 4), 'slices': [1], 'value': 1},\n    {'shape': (2, 3, 4), 'slices': [1, 1],\n     'value': numpy.arange(2 * 3 * 4).reshape(2, 3, 4)},\n    {'shape': (2, 3, 4), 'slices': [[1]], 'value': 1},\n    {'shape': (2, 3, 4), 'slices': [[1, 1]], 'value': 1},\n    {'shape': (2, 3, 4), 'slices': [[1], [1]], 'value': 1},\n    {'shape': (2, 3, 4), 'slices': [[1, 1], 1], 'value': 1},\n    {'shape': (2, 3, 4), 'slices': [[1], slice(1, 2)], 'value': 1},\n    {'shape': (2, 3, 4), 'slices': [[[1]], slice(1, 2)], 'value': 1},\n)\n@testing.gpu\nclass TestScatterParametrized(unittest.TestCase):\n\n    @testing.for_dtypes([numpy.float32, numpy.int32, numpy.uint32,\n                         numpy.uint64, numpy.ulonglong, numpy.float16,\n                         numpy.float64])\n    @testing.numpy_cupy_array_equal()\n    def test_scatter_add(self, xp, dtype):\n        a = xp.zeros(self.shape, dtype)\n        if xp is cupy:\n            a.scatter_add(self.slices, self.value)\n        else:\n            numpy.add.at(a, self.slices, self.value)\n        return a\n\n    @testing.for_dtypes([numpy.float32, numpy.int32, numpy.uint32,\n                         numpy.uint64, numpy.ulonglong, numpy.float64])\n    @testing.numpy_cupy_array_equal()\n    def test_scatter_max(self, xp, dtype):\n        a = xp.zeros(self.shape, dtype)\n        if xp is cupy:\n            a.scatter_max(self.slices, self.value)\n        else:\n            numpy.maximum.at(a, self.slices, self.value)\n        return a\n\n    @testing.for_dtypes([numpy.float32, numpy.int32, numpy.uint32,\n                         numpy.uint64, numpy.ulonglong, numpy.float64])\n    @testing.numpy_cupy_array_equal()\n    def test_scatter_min(self, xp, dtype):\n        a = xp.zeros(self.shape, dtype)\n        if xp is cupy:\n            a.scatter_min(self.slices, self.value)\n        else:\n            numpy.minimum.at(a, self.slices, self.value)\n        return a\n\n\n@testing.gpu\nclass TestScatterAdd(unittest.TestCase):\n\n    @testing.for_dtypes([numpy.float32, numpy.int32, numpy.uint32,\n                         numpy.uint64, numpy.ulonglong, numpy.float16,\n                         numpy.float64])\n    def test_scatter_add_cupy_arguments(self, dtype):\n        shape = (2, 3)\n        a = cupy.zeros(shape, dtype)\n        slices = (cupy.array([1, 1]), slice(None))\n        a.scatter_add(slices, cupy.array(1.))\n        testing.assert_array_equal(\n            a, cupy.array([[0., 0., 0.], [2., 2., 2.]], dtype))\n\n    @testing.for_dtypes([numpy.float32, numpy.int32, numpy.uint32,\n                         numpy.uint64, numpy.ulonglong, numpy.float16,\n                         numpy.float64])\n    def test_scatter_add_cupy_arguments_mask(self, dtype):\n        shape = (2, 3)\n        a = cupy.zeros(shape, dtype)\n        slices = (cupy.array([True, False]), slice(None))\n        a.scatter_add(slices, cupy.array(1.))\n        testing.assert_array_equal(\n            a, cupy.array([[1., 1., 1.], [0., 0., 0.]], dtype))\n\n    @testing.for_dtypes_combination(\n        [numpy.float32, numpy.int32, numpy.uint32, numpy.uint64,\n         numpy.ulonglong, numpy.float16, numpy.float64],\n        names=['src_dtype', 'dst_dtype'])\n    def test_scatter_add_differnt_dtypes(self, src_dtype, dst_dtype):\n        shape = (2, 3)\n        a = cupy.zeros(shape, dtype=src_dtype)\n        value = cupy.array(1, dtype=dst_dtype)\n        slices = ([1, 1], slice(None))\n        a.scatter_add(slices, value)\n\n        numpy.testing.assert_almost_equal(\n            a.get(),\n            numpy.array([[0, 0, 0], [2, 2, 2]], dtype=src_dtype))\n\n    @testing.for_dtypes_combination(\n        [numpy.float32, numpy.int32, numpy.uint32, numpy.uint64,\n         numpy.ulonglong, numpy.float16, numpy.float64],\n        names=['src_dtype', 'dst_dtype'])\n    def test_scatter_add_differnt_dtypes_mask(self, src_dtype, dst_dtype):\n        shape = (2, 3)\n        a = cupy.zeros(shape, dtype=src_dtype)\n        value = cupy.array(1, dtype=dst_dtype)\n        slices = (numpy.array([[True, False, False], [False, True, True]]))\n        a.scatter_add(slices, value)\n\n        numpy.testing.assert_almost_equal(\n            a.get(),\n            numpy.array([[1, 0, 0], [0, 1, 1]], dtype=src_dtype))\n\n\nclass TestScatterMinMax(unittest.TestCase):\n\n    @testing.for_dtypes([numpy.float32, numpy.int32, numpy.uint32,\n                         numpy.uint64, numpy.ulonglong, numpy.float64])\n    def test_scatter_minmax_cupy_arguments(self, dtype):\n        shape = (2, 3)\n        a = cupy.zeros(shape, dtype)\n        slices = (cupy.array([1, 1]), slice(None))\n        a.scatter_max(slices, cupy.array(1.))\n        testing.assert_array_equal(\n            a, cupy.array([[0., 0., 0.], [1., 1., 1.]], dtype))\n\n        a = cupy.ones(shape, dtype)\n        a.scatter_min(slices, cupy.array(0.))\n        testing.assert_array_equal(\n            a, cupy.array([[1., 1., 1.], [0., 0., 0.]], dtype))\n\n    @testing.for_dtypes([numpy.float32, numpy.int32, numpy.uint32,\n                         numpy.uint64, numpy.ulonglong, numpy.float64])\n    def test_scatter_minmax_cupy_arguments_mask(self, dtype):\n        shape = (2, 3)\n        a = cupy.zeros(shape, dtype)\n        slices = (cupy.array([True, False]), slice(None))\n        a.scatter_max(slices, cupy.array(1.))\n        testing.assert_array_equal(\n            a, cupy.array([[1., 1., 1.], [0., 0., 0.]], dtype))\n\n        a = cupy.ones(shape, dtype)\n        a.scatter_min(slices, cupy.array(0.))\n        testing.assert_array_equal(\n            a, cupy.array([[0., 0., 0.], [1., 1., 1.]], dtype))\n\n    @testing.for_dtypes_combination(\n        [numpy.float32, numpy.int32, numpy.uint32, numpy.uint64,\n         numpy.ulonglong, numpy.float64],\n        names=['src_dtype', 'dst_dtype'])\n    def test_scatter_minmax_differnt_dtypes(self, src_dtype, dst_dtype):\n        shape = (2, 3)\n        a = cupy.zeros(shape, dtype=src_dtype)\n        value = cupy.array(1, dtype=dst_dtype)\n        slices = ([1, 1], slice(None))\n        a.scatter_max(slices, value)\n        numpy.testing.assert_almost_equal(\n            a.get(),\n            numpy.array([[0, 0, 0], [1, 1, 1]], dtype=src_dtype))\n\n        a = cupy.ones(shape, dtype=src_dtype)\n        value = cupy.array(0, dtype=dst_dtype)\n        a.scatter_min(slices, value)\n        numpy.testing.assert_almost_equal(\n            a.get(),\n            numpy.array([[1, 1, 1], [0, 0, 0]], dtype=src_dtype))\n\n    @testing.for_dtypes_combination(\n        [numpy.float32, numpy.int32, numpy.uint32, numpy.uint64,\n         numpy.ulonglong, numpy.float16, numpy.float64],\n        names=['src_dtype', 'dst_dtype'])\n    def test_scatter_minmax_differnt_dtypes_mask(self, src_dtype, dst_dtype):\n        shape = (2, 3)\n        a = cupy.zeros(shape, dtype=src_dtype)\n        value = cupy.array(1, dtype=dst_dtype)\n        slices = (numpy.array([[True, False, False], [False, True, True]]))\n        a.scatter_max(slices, value)\n        numpy.testing.assert_almost_equal(\n            a.get(),\n            numpy.array([[1, 0, 0], [0, 1, 1]], dtype=src_dtype))\n\n        a = cupy.ones(shape, dtype=src_dtype)\n        value = cupy.array(0, dtype=dst_dtype)\n        a.scatter_min(slices, value)\n        numpy.testing.assert_almost_equal(\n            a.get(),\n            numpy.array([[0, 1, 1], [1, 0, 0]], dtype=src_dtype))\n"""
tests/cupy_tests/core_tests/test_ndarray_ufunc.py,28,"b""import unittest\n\nimport numpy as np\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestArrayUfunc(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_unary_op(self, dtype):\n        a = cupy.array(np.array([0, 1, 2]), dtype=dtype)\n        outa = np.sin(a)\n        # numpy operation produced a cupy array\n        self.assertTrue(isinstance(outa, cupy.ndarray))\n        b = a.get()\n        outb = np.sin(b)\n        self.assertTrue(np.allclose(outa.get(), outb))\n\n    @testing.for_all_dtypes()\n    def test_unary_op_out(self, dtype):\n        a = cupy.array(np.array([0, 1, 2]), dtype=dtype)\n        b = a.get()\n        outb = np.sin(b)\n        # pre-make output with same type as input\n        outa = cupy.array(np.array([0, 1, 2]), dtype=outb.dtype)\n        np.sin(a, out=outa)\n        self.assertTrue(np.allclose(outa.get(), outb))\n\n    @testing.for_all_dtypes()\n    def test_binary_op(self, dtype):\n        a1 = cupy.array(np.array([0, 1, 2]), dtype=dtype)\n        a2 = cupy.array(np.array([0, 1, 2]), dtype=dtype)\n        outa = np.add(a1, a2)\n        # numpy operation produced a cupy array\n        self.assertTrue(isinstance(outa, cupy.ndarray))\n        b1 = a1.get()\n        b2 = a2.get()\n        outb = np.add(b1, b2)\n        self.assertTrue(np.allclose(outa.get(), outb))\n\n    @testing.for_all_dtypes()\n    def test_binary_op_out(self, dtype):\n        a1 = cupy.array(np.array([0, 1, 2]), dtype=dtype)\n        a2 = cupy.array(np.array([0, 1, 2]), dtype=dtype)\n        outa = cupy.array(np.array([0, 1, 2]), dtype=dtype)\n        np.add(a1, a2, out=outa)\n        b1 = a1.get()\n        b2 = a2.get()\n        outb = np.add(b1, b2)\n        self.assertTrue(np.allclose(outa.get(), outb))\n\n    @testing.for_all_dtypes()\n    def test_binary_mixed_op(self, dtype):\n        a1 = cupy.array(np.array([0, 1, 2]), dtype=dtype)\n        a2 = cupy.array(np.array([0, 1, 2]), dtype=dtype).get()\n        with self.assertRaises(TypeError):\n            # attempt to add cupy and numpy arrays\n            np.add(a1, a2)\n        with self.assertRaises(TypeError):\n            # check reverse order\n            np.add(a2, a1)\n        with self.assertRaises(TypeError):\n            # reject numpy output from cupy\n            np.add(a1, a1, out=a2)\n        with self.assertRaises(TypeError):\n            # reject cupy output from numpy\n            np.add(a2, a2, out=a1)\n        with self.assertRaises(ValueError):\n            # bad form for out=\n            # this is also an error with numpy array\n            np.sin(a1, out=())\n        with self.assertRaises(ValueError):\n            # bad form for out=\n            # this is also an error with numpy array\n            np.sin(a1, out=(a1, a1))\n\n    @testing.numpy_cupy_array_equal()\n    def test_indexing(self, xp):\n        a = cupy.testing.shaped_arange((3, 1), xp)[:, :, None]\n        b = cupy.testing.shaped_arange((3, 2), xp)[:, None, :]\n        return a * b\n\n    @testing.numpy_cupy_array_equal()\n    def test_shares_memory(self, xp):\n        a = cupy.testing.shaped_arange((1000, 1000), xp, 'int64')\n        b = xp.transpose(a)\n        a += b\n        return a\n"""
tests/cupy_tests/core_tests/test_ndarray_unary_op.py,0,"b""import operator\nimport unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestArrayBoolOp(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_bool_empty(self, dtype):\n        self.assertFalse(bool(cupy.array((), dtype=dtype)))\n\n    def test_bool_scalar_bool(self):\n        self.assertTrue(bool(cupy.array(True, dtype=numpy.bool)))\n        self.assertFalse(bool(cupy.array(False, dtype=numpy.bool)))\n\n    @testing.for_all_dtypes()\n    def test_bool_scalar(self, dtype):\n        self.assertTrue(bool(cupy.array(1, dtype=dtype)))\n        self.assertFalse(bool(cupy.array(0, dtype=dtype)))\n\n    def test_bool_one_element_bool(self):\n        self.assertTrue(bool(cupy.array([True], dtype=numpy.bool)))\n        self.assertFalse(bool(cupy.array([False], dtype=numpy.bool)))\n\n    @testing.for_all_dtypes()\n    def test_bool_one_element(self, dtype):\n        self.assertTrue(bool(cupy.array([1], dtype=dtype)))\n        self.assertFalse(bool(cupy.array([0], dtype=dtype)))\n\n    @testing.for_all_dtypes()\n    def test_bool_two_elements(self, dtype):\n        with self.assertRaises(ValueError):\n            bool(cupy.array([1, 2], dtype=dtype))\n\n\n@testing.gpu\nclass TestArrayUnaryOp(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def check_array_op(self, op, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return op(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def check_array_op_full(self, op, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return op(a)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_neg_array(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return operator.neg(a)\n\n    def test_pos_array(self):\n        self.check_array_op(operator.pos)\n\n    @testing.with_requires('numpy<1.16')\n    def test_pos_array_full(self):\n        self.check_array_op_full(operator.pos)\n\n    def test_abs_array(self):\n        self.check_array_op_full(operator.abs)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def check_zerodim_op(self, op, xp, dtype):\n        a = xp.array(-2, dtype)\n        return op(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def check_zerodim_op_full(self, op, xp, dtype):\n        a = xp.array(-2, dtype)\n        return op(a)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_neg_zerodim(self, xp, dtype):\n        a = xp.array(-2, dtype)\n        return operator.neg(a)\n\n    def test_pos_zerodim(self):\n        self.check_zerodim_op(operator.pos)\n\n    def test_abs_zerodim(self):\n        self.check_zerodim_op_full(operator.abs)\n\n    def test_abs_zerodim_full(self):\n        self.check_zerodim_op_full(operator.abs)\n\n\n@testing.gpu\nclass TestArrayIntUnaryOp(unittest.TestCase):\n\n    @testing.for_int_dtypes()\n    @testing.numpy_cupy_allclose()\n    def check_array_op(self, op, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return op(a)\n\n    def test_invert_array(self):\n        self.check_array_op(operator.invert)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(accept_error=TypeError)\n    def check_zerodim_op(self, op, xp, dtype):\n        a = xp.array(-2, dtype)\n        return op(a)\n\n    def test_invert_zerodim(self):\n        self.check_zerodim_op(operator.invert)\n\n\n@testing.parameterize(*testing.product({\n    'xp': [numpy, cupy],\n    'shape': [(3, 2), (), (3, 0, 2)]\n}))\n@testing.gpu\nclass TestBoolNeg(unittest.TestCase):\n\n    def test_bool_neg(self):\n        xp = self.xp\n        if xp is numpy and not testing.numpy_satisfies('>=1.13.0'):\n            raise unittest.SkipTest('NumPy<1.13.0')\n        shape = self.shape\n        x = testing.shaped_random(shape, xp, dtype=numpy.bool_)\n        with pytest.raises(TypeError):\n            -x\n"""
tests/cupy_tests/core_tests/test_raw.py,0,"b'import os\nimport pytest\nimport shutil\nimport tempfile\nimport unittest\n\nimport cupy\nfrom cupy import testing\nfrom cupy.cuda import compiler\n\n\n_test_source1 = r\'\'\'\nextern ""C"" __global__\nvoid test_sum(const float* x1, const float* x2, float* y, unsigned int N) {\n    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N)\n        y[tid] = x1[tid] + x2[tid];\n}\n\'\'\'\n\n# test compiling and invoking multiple kernels in one single .cubin\n_test_source2 = r\'\'\'\nextern ""C""{\n\n__global__ void test_sum(const float* x1, const float* x2, float* y, \\\n                         unsigned int N)\n{\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N)\n    {\n        y[tid] = x1[tid] + x2[tid];\n    }\n}\n\n__global__ void test_multiply(const float* x1, const float* x2, float* y, \\\n                              unsigned int N)\n{\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N)\n    {\n        y[tid] = x1[tid] * x2[tid];\n    }\n}\n\n}\n\'\'\'\n\n# test C macros\n_test_source3 = r\'\'\'\n#ifndef PRECISION\n    #define PRECISION 2\n#endif\n\n#if PRECISION == 2\n    #define TYPE double\n#elif PRECISION == 1\n    #define TYPE float\n#else\n    #error precision not supported\n#endif\n\nextern ""C""{\n\n__global__ void test_sum(const TYPE* x1, const TYPE* x2, TYPE* y, \\\n                         unsigned int N)\n{\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N)\n    {\n        y[tid] = x1[tid] + x2[tid];\n    }\n}\n\n__global__ void test_multiply(const TYPE* x1, const TYPE* x2, TYPE* y, \\\n                              unsigned int N)\n{\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N)\n    {\n        y[tid] = x1[tid] * x2[tid];\n    }\n}\n\n}\n\'\'\'\n\n# dynamic parallelism\n_test_source4 = r\'\'\'\nextern ""C""{\n\n__global__ void test_kernel_inner(float *arr, int N)\n{\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n    if (tid < N)\n        arr[tid] = 1.0;\n}\n\n__global__ void test_kernel(float *arr, int N, int inner_blk)\n{\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n    if (tid < N/inner_blk)\n        test_kernel_inner<<<1, inner_blk>>>(arr+tid*inner_blk, inner_blk);\n}\n\n}\n\'\'\'\n\n# to generate cubin/ptx\n_test_source5 = r\'\'\'\nextern ""C"" __global__\nvoid test_div(const float* x1, const float* x2, float* y, unsigned int N) {\n    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N)\n        y[tid] = x1[tid] / (x2[tid] + 1.0);\n}\n\'\'\'\n\n_test_cuComplex = r\'\'\'\n#include <cuComplex.h>\n#define N 100\n\nextern ""C""{\n/* ------------------- double complex ------------------- */\n\n__global__ void test_add(cuDoubleComplex* arr1, cuDoubleComplex* arr2,\n                         cuDoubleComplex* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuCadd(arr1[tid], arr2[tid]);\n    }\n}\n\n__global__ void test_sub(cuDoubleComplex* arr1, cuDoubleComplex* arr2,\n                         cuDoubleComplex* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuCsub(arr1[tid], arr2[tid]);\n    }\n}\n\n__global__ void test_mul(cuDoubleComplex* arr1, cuDoubleComplex* arr2,\n                         cuDoubleComplex* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuCmul(arr1[tid], arr2[tid]);\n    }\n}\n\n__global__ void test_div(cuDoubleComplex* arr1, cuDoubleComplex* arr2,\n                         cuDoubleComplex* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuCdiv(arr1[tid], arr2[tid]);\n    }\n}\n\n__global__ void test_conj(cuDoubleComplex* arr, cuDoubleComplex* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuConj(arr[tid]);\n    }\n}\n\n__global__ void test_abs(cuDoubleComplex* arr, double* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuCabs(arr[tid]);\n    }\n}\n\n__global__ void test_fma(cuDoubleComplex* A, cuDoubleComplex* B,\n                         cuDoubleComplex* C, cuDoubleComplex* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuCfma(A[tid], B[tid], C[tid]);\n    }\n}\n\n__global__ void test_make(cuDoubleComplex* arr) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    cuDoubleComplex out = make_cuDoubleComplex(1.8, 2.9);\n    if (tid < N) {\n        arr[tid] = make_cuDoubleComplex(cuCreal(out), -3.*cuCimag(out));\n    }\n}\n\n__global__ void test_downcast(cuDoubleComplex* arr, cuComplex* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuComplexDoubleToFloat(arr[tid]);\n    }\n}\n\n__global__ void test_add_scalar(cuDoubleComplex* arr, cuDoubleComplex scalar,\n                                cuDoubleComplex* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuCadd(arr[tid], scalar);\n    }\n}\n\n/* ------------------- single complex ------------------- */\n\n__global__ void test_addf(cuComplex* arr1, cuComplex* arr2,\n                          cuComplex* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuCaddf(arr1[tid], arr2[tid]);\n    }\n}\n\n__global__ void test_subf(cuComplex* arr1, cuComplex* arr2,\n                          cuComplex* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuCsubf(arr1[tid], arr2[tid]);\n    }\n}\n\n__global__ void test_mulf(cuComplex* arr1, cuComplex* arr2,\n                          cuComplex* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuCmulf(arr1[tid], arr2[tid]);\n    }\n}\n\n__global__ void test_divf(cuComplex* arr1, cuComplex* arr2,\n                          cuComplex* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuCdivf(arr1[tid], arr2[tid]);\n    }\n}\n\n__global__ void test_conjf(cuComplex* arr, cuComplex* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuConjf(arr[tid]);\n    }\n}\n\n__global__ void test_absf(cuFloatComplex* arr, float* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuCabsf(arr[tid]);\n    }\n}\n\n__global__ void test_fmaf(cuFloatComplex* A, cuFloatComplex* B,\n                          cuFloatComplex* C, cuFloatComplex* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuCfmaf(A[tid], B[tid], C[tid]);\n    }\n}\n\n__global__ void test_makef(cuComplex* arr) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    cuComplex out = make_cuFloatComplex(1.8, 2.9);\n    if (tid < N) {\n        arr[tid] = make_cuFloatComplex(cuCrealf(out), -3.*cuCimagf(out));\n    }\n}\n\n__global__ void test_upcast(cuComplex* arr, cuDoubleComplex* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuComplexFloatToDouble(arr[tid]);\n    }\n}\n\n__global__ void test_addf_scalar(cuComplex* arr, cuComplex scalar,\n                                 cuComplex* out) {\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        out[tid] = cuCadd(arr[tid], scalar);\n    }\n}\n\n}\n\'\'\'\n\ntest_const_mem = r\'\'\'\nextern ""C""{\n__constant__ float some_array[100];\n\n__global__ void multiply_by_const(float* x, int N) {\n    int id = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (id < N) {\n        x[id] *= some_array[id];\n    }\n}\n}\n\'\'\'\n\ntest_cxx_template = r\'\'\'\n#include <cupy/complex.cuh>\n\ntemplate<typename T>\n__global__ void my_sqrt(T* input, int N) {\n  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;\n  if (x < N) {\n    input[x] *= input[x];\n  }\n}\n\n__global__ void my_func(double* input, int N) {\n  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;\n  if (x < N) {\n    input[x] *= input[x];\n  }\n}\n\'\'\'\n\nif \'CUPY_CACHE_DIR\' in os.environ:\n    _old_cache_dir = os.environ[\'CUPY_CACHE_DIR\']\n    _is_cache_env_var_set = True\nelse:\n    _old_cache_dir = os.path.expanduser(\'~/.cupy/kernel_cache\')\n    _is_cache_env_var_set = False\n_test_cache_dir = None\n\n\n@testing.parameterize(*testing.product({\n    \'backend\': (\'nvrtc\', \'nvcc\'),\n}))\nclass TestRaw(unittest.TestCase):\n\n    def setUp(self):\n        self.dev = cupy.cuda.runtime.getDevice()\n        assert self.dev != 1\n\n        global _test_cache_dir\n        _test_cache_dir = tempfile.mkdtemp()\n        os.environ[\'CUPY_CACHE_DIR\'] = _test_cache_dir\n\n        self.kern = cupy.RawKernel(\n            _test_source1, \'test_sum\',\n            backend=self.backend)\n        self.mod2 = cupy.RawModule(\n            code=_test_source2,\n            backend=self.backend)\n        self.mod3 = cupy.RawModule(\n            code=_test_source3,\n            options=(\'-DPRECISION=2\',),\n            backend=self.backend)\n\n    def tearDown(self):\n        # To avoid cache interference, we remove cached files after every test,\n        # and restore users\' old setting\n        global _test_cache_dir\n        shutil.rmtree(_test_cache_dir)\n        if _is_cache_env_var_set:\n            os.environ[\'CUPY_CACHE_DIR\'] = _old_cache_dir\n        else:\n            os.environ.pop(\'CUPY_CACHE_DIR\')\n        compiler._empty_file_preprocess_cache = {}\n\n    def _helper(self, kernel, dtype):\n        N = 10\n        x1 = cupy.arange(N**2, dtype=dtype).reshape(N, N)\n        x2 = cupy.ones((N, N), dtype=dtype)\n        y = cupy.zeros((N, N), dtype=dtype)\n        kernel((N,), (N,), (x1, x2, y, N**2))\n        return x1, x2, y\n\n    def test_basic(self):\n        x1, x2, y = self._helper(self.kern, cupy.float32)\n        assert cupy.allclose(y, x1 + x2)\n\n    def test_kernel_attributes(self):\n        attrs = self.kern.attributes\n        for attribute in [\'binary_version\',\n                          \'cache_mode_ca\',\n                          \'const_size_bytes\',\n                          \'local_size_bytes\',\n                          \'max_dynamic_shared_size_bytes\',\n                          \'max_threads_per_block\',\n                          \'num_regs\',\n                          \'preferred_shared_memory_carveout\',\n                          \'ptx_version\',\n                          \'shared_size_bytes\']:\n            assert attribute in attrs\n        assert self.kern.num_regs > 0\n        assert self.kern.max_threads_per_block > 0\n        assert self.kern.shared_size_bytes == 0\n\n    def test_module(self):\n        module = self.mod2\n        ker_sum = module.get_function(\'test_sum\')\n        ker_times = module.get_function(\'test_multiply\')\n\n        x1, x2, y = self._helper(ker_sum, cupy.float32)\n        assert cupy.allclose(y, x1 + x2)\n\n        x1, x2, y = self._helper(ker_times, cupy.float32)\n        assert cupy.allclose(y, x1 * x2)\n\n    def test_compiler_flag(self):\n        module = self.mod3\n        ker_sum = module.get_function(\'test_sum\')\n        ker_times = module.get_function(\'test_multiply\')\n\n        x1, x2, y = self._helper(ker_sum, cupy.float64)\n        assert cupy.allclose(y, x1 + x2)\n\n        x1, x2, y = self._helper(ker_times, cupy.float64)\n        assert cupy.allclose(y, x1 * x2)\n\n    def test_invalid_compiler_flag(self):\n        with pytest.raises(cupy.cuda.compiler.CompileException) as ex:\n            cupy.RawModule(\n                code=_test_source3,\n                options=(\'-DPRECISION=3\',),\n                backend=self.backend)\n        assert \'precision not supported\' in str(ex.value)\n\n    def _generate_file(self, ext: str):\n        # generate cubin/ptx by calling nvcc\n        global _test_cache_dir\n\n        nvcc = cupy.cuda.get_nvcc_path()\n        # split() is needed because nvcc could come from the env var NVCC\n        cmd = nvcc.split()\n        arch = \'-gencode=arch=compute_{cc},code=sm_{cc}\'.format(\n            cc=compiler._get_arch())\n        source = \'{}/test_load_cubin.cu\'.format(_test_cache_dir)\n        file_path = _test_cache_dir + \'test_load_cubin\'\n        with open(source, \'w\') as f:\n            f.write(_test_source5)\n        if ext == \'cubin\':\n            file_path += \'.cubin\'\n            flag = \'-cubin\'\n        elif ext == \'ptx\':\n            file_path += \'.ptx\'\n            flag = \'-ptx\'\n        else:\n            raise ValueError\n        cmd += [arch, flag, source, \'-o\', file_path]\n        compiler._run_nvcc(cmd, _test_cache_dir)\n\n        return file_path\n\n    def test_load_cubin(self):\n        # generate cubin in the temp dir\n        file_path = self._generate_file(\'cubin\')\n\n        # load cubin and test the kernel\n        mod = cupy.RawModule(path=file_path, backend=self.backend)\n        ker = mod.get_function(\'test_div\')\n        x1, x2, y = self._helper(ker, cupy.float32)\n        assert cupy.allclose(y, x1 / (x2 + 1.0))\n\n    def test_load_ptx(self):\n        # generate ptx in the temp dir\n        file_path = self._generate_file(\'ptx\')\n\n        # load ptx and test the kernel\n        mod = cupy.RawModule(path=file_path, backend=self.backend)\n        ker = mod.get_function(\'test_div\')\n        x1, x2, y = self._helper(ker, cupy.float32)\n        assert cupy.allclose(y, x1 / (x2 + 1.0))\n\n    def test_module_load_failure(self):\n        # in principle this test is better done in test_driver.py, but\n        # this error is more likely to appear when using RawModule, so\n        # let us do it here\n        with pytest.raises(cupy.cuda.driver.CUDADriverError) as ex:\n            cupy.RawModule(\n                path=os.path.expanduser(\'~/this_does_not_exist.cubin\'),\n                backend=self.backend)\n        assert \'CUDA_ERROR_FILE_NOT_FOUND\' in str(ex.value)\n\n    def test_module_neither_code_nor_path(self):\n        with pytest.raises(TypeError):\n            cupy.RawModule()\n\n    def test_module_both_code_and_path(self):\n        with pytest.raises(TypeError):\n            cupy.RawModule(\n                code=_test_source1,\n                path=\'test.cubin\')\n\n    def test_get_function_failure(self):\n        # in principle this test is better done in test_driver.py, but\n        # this error is more likely to appear when using RawModule, so\n        # let us do it here\n        with pytest.raises(cupy.cuda.driver.CUDADriverError) as ex:\n            self.mod2.get_function(\'no_such_kernel\')\n        assert \'CUDA_ERROR_NOT_FOUND\' in str(ex.value)\n\n    def test_dynamical_parallelism(self):\n        ker = cupy.RawKernel(_test_source4, \'test_kernel\', options=(\'-dc\',),\n                             backend=self.backend)\n        N = 169\n        inner_chunk = 13\n        x = cupy.zeros((N,), dtype=cupy.float32)\n        ker((1,), (N//inner_chunk,), (x, N, inner_chunk))\n        assert (x == 1.0).all()\n\n    def test_dynamical_parallelism_compile_failure(self):\n        # no option for separate compilation is given should cause an error\n        ker = cupy.RawKernel(_test_source4, \'test_kernel\',\n                             backend=self.backend)\n        N = 10\n        inner_chunk = 2\n        x = cupy.zeros((N,), dtype=cupy.float32)\n        if self.backend == \'nvrtc\':\n            # raised when calling ls.complete()\n            with pytest.raises(cupy.cuda.driver.CUDADriverError):\n                ker((1,), (N//inner_chunk,), (x, N, inner_chunk))\n        else:  # nvcc\n            with pytest.raises(cupy.cuda.compiler.CompileException):\n                ker((1,), (N//inner_chunk,), (x, N, inner_chunk))\n\n    def test_cuFloatComplex(self):\n        N = 100\n        block = 32\n        grid = (N + block - 1) // block\n        dtype = cupy.complex64\n\n        mod = cupy.RawModule(\n            code=_test_cuComplex,\n            translate_cucomplex=True)\n        a = cupy.random.random((N,)) + 1j*cupy.random.random((N,))\n        a = a.astype(dtype)\n        b = cupy.random.random((N,)) + 1j*cupy.random.random((N,))\n        b = b.astype(dtype)\n        c = cupy.random.random((N,)) + 1j*cupy.random.random((N,))\n        c = c.astype(dtype)\n        out = cupy.zeros((N,), dtype=dtype)\n        out_float = cupy.zeros((N,), dtype=cupy.float32)\n        out_up = cupy.zeros((N,), dtype=cupy.complex128)\n\n        ker = mod.get_function(\'test_addf\')\n        ker((grid,), (block,), (a, b, out))\n        assert (out == a + b).all()\n\n        ker = mod.get_function(\'test_subf\')\n        ker((grid,), (block,), (a, b, out))\n        assert (out == a - b).all()\n\n        ker = mod.get_function(\'test_mulf\')\n        ker((grid,), (block,), (a, b, out))\n        assert (out == a * b).all()\n\n        ker = mod.get_function(\'test_divf\')\n        ker((grid,), (block,), (a, b, out))\n        assert (out == a / b).all()\n\n        ker = mod.get_function(\'test_conjf\')\n        ker((grid,), (block,), (a, out))\n        assert (out == cupy.conj(a)).all()\n\n        ker = mod.get_function(\'test_absf\')\n        ker((grid,), (block,), (a, out_float))\n        assert (out_float == cupy.abs(a)).all()\n\n        ker = mod.get_function(\'test_fmaf\')\n        ker((grid,), (block,), (a, b, c, out))\n        assert (out == a * b + c).all()\n\n        ker = mod.get_function(\'test_makef\')\n        ker((grid,), (block,), (out,))\n        # because of precision issue, the (A==B).all() semantics would fail\n        assert cupy.allclose(out, 1.8 - 1j * 8.7)\n\n        ker = mod.get_function(\'test_upcast\')\n        ker((grid,), (block,), (a, out_up))\n        assert (out_up == a.astype(cupy.complex128)).all()\n\n        # NumPy scalars.\n        b = cupy.complex64(2 + 3j)\n        ker = mod.get_function(\'test_addf_scalar\')\n        ker((grid,), (block,), (a, b, out))\n        assert (out == a + b).all()\n\n    def test_cuDoubleComplex(self):\n        N = 100\n        block = 32\n        grid = (N + block - 1) // block\n        dtype = cupy.complex128\n\n        mod = cupy.RawModule(\n            code=_test_cuComplex,\n            translate_cucomplex=True)\n        a = cupy.random.random((N,)) + 1j*cupy.random.random((N,))\n        a = a.astype(dtype)\n        b = cupy.random.random((N,)) + 1j*cupy.random.random((N,))\n        b = b.astype(dtype)\n        c = cupy.random.random((N,)) + 1j*cupy.random.random((N,))\n        c = c.astype(dtype)\n        out = cupy.zeros((N,), dtype=dtype)\n        out_float = cupy.zeros((N,), dtype=cupy.float64)\n        out_down = cupy.zeros((N,), dtype=cupy.complex64)\n\n        ker = mod.get_function(\'test_add\')\n        ker((grid,), (block,), (a, b, out))\n        assert (out == a + b).all()\n\n        ker = mod.get_function(\'test_sub\')\n        ker((grid,), (block,), (a, b, out))\n        assert (out == a - b).all()\n\n        ker = mod.get_function(\'test_mul\')\n        ker((grid,), (block,), (a, b, out))\n        assert (out == a * b).all()\n\n        ker = mod.get_function(\'test_div\')\n        ker((grid,), (block,), (a, b, out))\n        assert (out == a / b).all()\n\n        ker = mod.get_function(\'test_conj\')\n        ker((grid,), (block,), (a, out))\n        assert (out == cupy.conj(a)).all()\n\n        ker = mod.get_function(\'test_abs\')\n        ker((grid,), (block,), (a, out_float))\n        assert (out_float == cupy.abs(a)).all()\n\n        ker = mod.get_function(\'test_fma\')\n        ker((grid,), (block,), (a, b, c, out))\n        assert (out == a * b + c).all()\n\n        ker = mod.get_function(\'test_make\')\n        ker((grid,), (block,), (out,))\n        assert (out == 1.8 - 1j * 8.7).all()\n\n        ker = mod.get_function(\'test_downcast\')\n        ker((grid,), (block,), (a, out_down))\n        assert (out_down == a.astype(cupy.complex64)).all()\n\n        # NumPy scalars.\n        b = cupy.complex128(2 + 3j)\n        ker = mod.get_function(\'test_add_scalar\')\n        ker((grid,), (block,), (a, b, out))\n        assert (out == a + b).all()\n\n        # Python scalars.\n        b = 2 + 3j\n        ker = mod.get_function(\'test_add_scalar\')\n        ker((grid,), (block,), (a, b, out))\n        assert (out == a + b).all()\n\n    def test_const_memory(self):\n        mod = cupy.RawModule(code=test_const_mem, backend=self.backend)\n        ker = mod.get_function(\'multiply_by_const\')\n        mem_ptr = mod.get_global(\'some_array\')\n        const_arr = cupy.ndarray((100,), cupy.float32, mem_ptr)\n        data = cupy.arange(100, dtype=cupy.float32)\n        const_arr[...] = data\n        output_arr = cupy.ones(100, dtype=cupy.float32)\n        ker((1,), (100,), (output_arr, cupy.int32(100)))\n        assert (data == output_arr).all()\n\n    def test_template_specialization(self):\n        if self.backend == \'nvcc\':\n            self.skipTest(\'nvcc does not support template specialization\')\n\n        # compile code\n        name_expressions = [\'my_sqrt<int>\', \'my_sqrt<float>\',\n                            \'my_sqrt<complex<double>>\', \'my_func\']\n        mod = cupy.RawModule(code=test_cxx_template, options=(\'--std=c++11\',),\n                             name_expressions=name_expressions)\n\n        dtypes = (cupy.int32, cupy.float32, cupy.complex128, cupy.float64)\n        for ker_T, dtype in zip(name_expressions, dtypes):\n            # get specialized kernels\n            ker = mod.get_function(ker_T)\n\n            # prepare inputs & expected outputs\n            in_arr = cupy.testing.shaped_random((10,), dtype=dtype)\n            out_arr = in_arr**2\n\n            # run\n            ker((1,), (10,), (in_arr, 10))\n\n            # check results\n            assert cupy.allclose(in_arr, out_arr)\n\n    def test_template_failure(self):\n        name_expressions = [\'my_sqrt<int>\']\n\n        # 1. nvcc is disabled for this feature\n        if self.backend == \'nvcc\':\n            with pytest.raises(ValueError) as e:\n                cupy.RawModule(code=test_cxx_template, backend=self.backend,\n                               options=(\'--std=c++11\',),\n                               name_expressions=name_expressions)\n            assert \'nvrtc\' in str(e.value)\n            return  # the rest of tests do not apply to nvcc\n\n        # 2. compile code without specializations\n        mod = cupy.RawModule(code=test_cxx_template, options=(\'--std=c++11\',))\n        # ...try to get a specialized kernel\n        with pytest.raises(cupy.cuda.driver.CUDADriverError,\n                           match=\'named symbol not found\'):\n            mod.get_function(\'my_sqrt<int>\')\n\n        # 3. compile code without specifying C++ standard\n        with pytest.raises(ValueError):\n            cupy.RawModule(code=test_cxx_template,\n                           name_expressions=name_expressions)\n\n        # 4. try to fetch something we didn\'t specialize for\n        mod = cupy.RawModule(code=test_cxx_template, options=(\'--std=c++11\',),\n                             name_expressions=name_expressions)\n        with pytest.raises(cupy.cuda.driver.CUDADriverError,\n                           match=\'named symbol not found\'):\n            mod.get_function(\'my_sqrt<double>\')\n\n    @testing.multi_gpu(2)\n    def test_context_switch_RawKernel(self):\n        # run test_basic() on another device\n\n        # For RawKernel, we need to launch it once to force compiling\n        x1, x2, y = self._helper(self.kern, cupy.float32)\n\n        with cupy.cuda.Device(1):\n            x1, x2, y = self._helper(self.kern, cupy.float32)\n            assert cupy.allclose(y, x1 + x2)\n\n    @testing.multi_gpu(2)\n    def test_context_switch_RawModule1(self):\n        # run test_module() on another device\n        # in this test, re-compiling happens at get_function()\n        with cupy.cuda.Device(1):\n            module = self.mod2\n            ker_sum = module.get_function(\'test_sum\')\n            x1, x2, y = self._helper(ker_sum, cupy.float32)\n            assert cupy.allclose(y, x1 + x2)\n\n    @testing.multi_gpu(2)\n    def test_context_switch_RawModule2(self):\n        # run test_module() on another device\n        # in this test, re-compiling happens at kernel launch\n        module = self.mod2\n        ker_sum = module.get_function(\'test_sum\')\n\n        with cupy.cuda.Device(1):\n            x1, x2, y = self._helper(ker_sum, cupy.float32)\n            assert cupy.allclose(y, x1 + x2)\n\n    @testing.multi_gpu(2)\n    def test_context_switch_RawModule3(self):\n        # run test_load_cubin() on another device\n        # generate cubin in the temp dir and load it on device 0\n        file_path = self._generate_file(\'cubin\')\n        mod = cupy.RawModule(path=file_path, backend=self.backend)\n\n        # in this test, reloading happens at get_function()\n        with cupy.cuda.Device(1):\n            ker = mod.get_function(\'test_div\')\n            x1, x2, y = self._helper(ker, cupy.float32)\n            assert cupy.allclose(y, x1 / (x2 + 1.0))\n\n    @testing.multi_gpu(2)\n    def test_context_switch_RawModule4(self):\n        # run test_load_cubin() on another device\n        # generate cubin in the temp dir and load it on device 0\n        file_path = self._generate_file(\'cubin\')\n        mod = cupy.RawModule(path=file_path, backend=self.backend)\n        ker = mod.get_function(\'test_div\')\n\n        # in this test, reloading happens at kernel launch\n        with cupy.cuda.Device(1):\n            x1, x2, y = self._helper(ker, cupy.float32)\n            assert cupy.allclose(y, x1 / (x2 + 1.0))\n\n    @testing.multi_gpu(2)\n    def test_context_switch_RawModule5(self):\n        # run test_template_specialization() on another device\n        # in this test, re-compiling happens at get_function()\n        if self.backend == \'nvcc\':\n            self.skipTest(\'nvcc does not support template specialization\')\n\n        # compile code\n        name_expressions = [\'my_sqrt<unsigned int>\']\n        mod = cupy.RawModule(code=test_cxx_template, options=(\'--std=c++11\',),\n                             name_expressions=name_expressions)\n\n        # switch device\n        with cupy.cuda.Device(1):\n            # get specialized kernels\n            name = name_expressions[0]\n            ker = mod.get_function(name)\n\n            # prepare inputs & expected outputs\n            in_arr = cupy.testing.shaped_random((10,), dtype=cupy.uint32)\n            out_arr = in_arr**2\n\n            # run\n            ker((1,), (10,), (in_arr, 10))\n\n            # check results\n            assert cupy.allclose(in_arr, out_arr)\n\n    @testing.multi_gpu(2)\n    def test_context_switch_RawModule6(self):\n        # run test_template_specialization() on another device\n        # in this test, re-compiling happens at kernel launch\n        if self.backend == \'nvcc\':\n            self.skipTest(\'nvcc does not support template specialization\')\n\n        # compile code\n        name_expressions = [\'my_sqrt<unsigned int>\']\n        mod = cupy.RawModule(code=test_cxx_template, options=(\'--std=c++11\',),\n                             name_expressions=name_expressions)\n\n        # get specialized kernels\n        name = name_expressions[0]\n        ker = mod.get_function(name)\n\n        # switch device\n        with cupy.cuda.Device(1):\n            # prepare inputs & expected outputs\n            in_arr = cupy.testing.shaped_random((10,), dtype=cupy.uint32)\n            out_arr = in_arr**2\n\n            # run\n            ker((1,), (10,), (in_arr, 10))\n\n            # check results\n            assert cupy.allclose(in_arr, out_arr)\n\n\n_test_grid_sync = r\'\'\'\n#include <cooperative_groups.h>\n\nextern ""C"" __global__\nvoid test_grid_sync(const float* x1, const float* x2, float* y) {\n    namespace cg = cooperative_groups;\n    cg::grid_group grid = cg::this_grid();\n    int size = gridDim.x * blockDim.x;\n    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    y[tid] = x1[tid];\n    cg::sync(grid);\n    y[size - tid - 1] += x2[size - tid - 1];\n}\n\'\'\'\n\n\n@testing.parameterize(*testing.product({\n    \'n\': [10, 100, 256]\n}))\n@unittest.skipUnless(\n    9000 <= cupy.cuda.runtime.runtimeGetVersion(),\n    \'Requires CUDA 9.x or later\')\n@unittest.skipUnless(\n    60 <= int(cupy.cuda.device.get_compute_capability()),\n    \'Requires compute capability 6.0 or later\')\nclass TestRawGridSync(unittest.TestCase):\n\n    def setUp(self):\n        global _test_cache_dir\n        _test_cache_dir = tempfile.mkdtemp()\n        os.environ[\'CUPY_CACHE_DIR\'] = _test_cache_dir\n\n        self.kern_grid_sync = cupy.RawKernel(\n            _test_grid_sync, \'test_grid_sync\', backend=\'nvcc\',\n            enable_cooperative_groups=True)\n        self.mod_grid_sync = cupy.RawModule(\n            code=_test_grid_sync, backend=\'nvcc\',\n            enable_cooperative_groups=True)\n\n    def tearDown(self):\n        # To avoid cache interference, we remove cached files after every test,\n        # and restore users\' old setting\n        global _test_cache_dir\n        shutil.rmtree(_test_cache_dir)\n        if _is_cache_env_var_set:\n            os.environ[\'CUPY_CACHE_DIR\'] = _old_cache_dir\n        else:\n            os.environ.pop(\'CUPY_CACHE_DIR\')\n        compiler._empty_file_preprocess_cache = {}\n\n    def test_grid_sync_rawkernel(self):\n        n = self.n\n        x1 = cupy.arange(n ** 2, dtype=\'float32\').reshape(n, n)\n        x2 = cupy.ones((n, n), dtype=\'float32\')\n        y = cupy.zeros((n, n), dtype=\'float32\')\n        self.kern_grid_sync((n,), (n,), (x1, x2, y, n ** 2))\n        assert cupy.allclose(y, x1 + x2)\n\n    def test_grid_sync_rawmodule(self):\n        n = self.n\n        x1 = cupy.arange(n ** 2, dtype=\'float32\').reshape(n, n)\n        x2 = cupy.ones((n, n), dtype=\'float32\')\n        y = cupy.zeros((n, n), dtype=\'float32\')\n        kern = self.mod_grid_sync.get_function(\'test_grid_sync\')\n        kern((n,), (n,), (x1, x2, y, n ** 2))\n        assert cupy.allclose(y, x1 + x2)\n'"
tests/cupy_tests/core_tests/test_reduction.py,0,"b""import unittest\n\nimport cupy\nfrom cupy import core\nfrom cupy import testing\n\n\n_noncontiguous_params = [\n    # reduce at head axes\n    {'shape': (2, 4, 3), 'trans': (2, 1, 0), 'axis': (0, 1)},\n    # reduce at middle axes\n    {'shape': (2, 4, 5, 3), 'trans': (3, 2, 1, 0), 'axis': (1, 2)},\n    # reduce at tail axes\n    {'shape': (2, 4, 3), 'trans': (2, 1, 0), 'axis': (1, 2)},\n    # out_axis = (0,)\n    {'shape': (0, 4, 3), 'trans': (2, 1, 0), 'axis': (0, 1)},\n    # out_axis = ()\n    {'shape': (2, 4, 3), 'trans': (2, 1, 0), 'axis': (0, 1, 2)},\n]\n\n\nclass AbstractReductionTestBase:\n\n    def get_sum_func(self):\n        raise NotImplementedError()\n\n    @testing.numpy_cupy_allclose(contiguous_check=False)\n    def check_int8_sum(self, shape, xp, axis=None, keepdims=False, trans=None):\n        a = testing.shaped_random(shape, xp, 'b')\n        if trans:\n            a = a.transpose(*trans)\n        sum_func = self.get_sum_func()\n        if xp == cupy:\n            return sum_func(\n                a, axis=axis, keepdims=keepdims)\n        else:\n            return a.sum(axis=axis, keepdims=keepdims, dtype='b')\n\n\nclass SimpleReductionFunctionTestBase(AbstractReductionTestBase):\n\n    def get_sum_func(self):\n        return core.create_reduction_func(\n            'my_sum', ('b->b',), ('in0', 'a + b', 'out0 = a', None), 0)\n\n\n@testing.gpu\nclass SimpleReductionFunctionTest(\n        unittest.TestCase, SimpleReductionFunctionTestBase):\n    def test_shape1(self):\n        for i in range(1, 10):\n            self.check_int8_sum((2 ** i,))\n            self.check_int8_sum((2 ** i - 1,))\n            self.check_int8_sum((2 ** i + 1,))\n\n    def test_shape2(self):\n        for i in range(1, 10):\n            self.check_int8_sum((2 ** i, 1000), axis=0)\n            self.check_int8_sum((2 ** i - 1, 1000), axis=0)\n            self.check_int8_sum((2 ** i + 1, 1000), axis=0)\n\n    def test_shape3(self):\n        for i in range(1, 10):\n            self.check_int8_sum((2 ** i, 1000), axis=1)\n            self.check_int8_sum((2 ** i - 1, 1000), axis=1)\n            self.check_int8_sum((2 ** i + 1, 1000), axis=1)\n\n    def test_shape4(self):\n        self.check_int8_sum((512, 256 * 256), axis=0)\n        self.check_int8_sum((512, 256 * 256), axis=1)\n\n        self.check_int8_sum((512 + 1, 256 * 256 + 1), axis=0)\n        self.check_int8_sum((512 + 1, 256 * 256 + 1), axis=1)\n\n    def test_shape5(self):\n        block_size = 512\n        size = ((2 << 32) // block_size)\n        self.check_int8_sum((size, 1), axis=1)\n        self.check_int8_sum((size, 1), axis=0)\n\n\n@testing.gpu\n@testing.parameterize(*_noncontiguous_params)\nclass TestSimpleReductionFunctionNonContiguous(\n        SimpleReductionFunctionTestBase, unittest.TestCase):\n\n    def test_noncontiguous(self):\n        self.check_int8_sum(self.shape, trans=self.trans, axis=self.axis)\n\n\nclass ReductionKernelTestBase(AbstractReductionTestBase):\n\n    def get_sum_func(self):\n        return cupy.ReductionKernel(\n            'T x', 'T out', 'x', 'a + b', 'out = a', '0', 'my_sum')\n\n\n@testing.gpu\nclass TestReductionKernel(ReductionKernelTestBase, unittest.TestCase):\n\n    def test_shape1(self):\n        for i in range(1, 10):\n            self.check_int8_sum((2 ** i,))\n            self.check_int8_sum((2 ** i - 1,))\n            self.check_int8_sum((2 ** i + 1,))\n\n    def test_shape2(self):\n        for i in range(1, 10):\n            self.check_int8_sum((2 ** i, 1000), axis=0)\n            self.check_int8_sum((2 ** i - 1, 1000), axis=0)\n            self.check_int8_sum((2 ** i + 1, 1000), axis=0)\n\n    def test_shape3(self):\n        for i in range(1, 10):\n            self.check_int8_sum((2 ** i, 1000), axis=1)\n            self.check_int8_sum((2 ** i - 1, 1000), axis=1)\n            self.check_int8_sum((2 ** i + 1, 1000), axis=1)\n\n    def test_shape4(self):\n        self.check_int8_sum((512, 256 * 256), axis=0)\n        self.check_int8_sum((512, 256 * 256), axis=1)\n        self.check_int8_sum((512 + 1, 256 * 256 + 1), axis=0)\n        self.check_int8_sum((512 + 1, 256 * 256 + 1), axis=1)\n\n\n@testing.gpu\n@testing.parameterize(*_noncontiguous_params)\nclass TestReductionKernelNonContiguous(\n        ReductionKernelTestBase, unittest.TestCase):\n\n    def test_noncontiguous(self):\n        self.check_int8_sum(self.shape, trans=self.trans, axis=self.axis)\n\n\n@testing.gpu\nclass TestReductionKernelInvalidArgument(unittest.TestCase):\n\n    def test_invalid_kernel_name(self):\n        with self.assertRaisesRegex(ValueError, 'Invalid kernel name'):\n            cupy.ReductionKernel(\n                'T x', 'T y', 'x', 'a + b', 'y = a', '0', name='1')\n\n\n@testing.gpu\nclass TestLargeMultiDimReduction(\n        ReductionKernelTestBase, unittest.TestCase):\n\n    def test_large_dims_keep_kernels(self):\n        # This test creates a CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES\n        # if the output array dims are not reduced\n        shape = (4, 3, 2, 4, 3, 2, 2)\n        axis = (1, 4, 3, 6)\n        self.check_int8_sum(shape, axis=axis, keepdims=True)\n"""
tests/cupy_tests/core_tests/test_scan.py,0,"b'# coding: utf-8\n\nimport unittest\n\nimport cupy\nfrom cupy import cuda\nfrom cupy.core._routines_math import _scan_for_test as scan\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestScan(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_scan(self, dtype):\n        element_num = 10000\n\n        if dtype in {cupy.int8, cupy.uint8, cupy.float16}:\n            element_num = 100\n\n        a = cupy.ones((element_num,), dtype=dtype)\n        prefix_sum = scan(a)\n        expect = cupy.arange(start=1, stop=element_num + 1).astype(dtype)\n\n        testing.assert_array_equal(prefix_sum, expect)\n\n    def test_check_1d_array(self):\n        with self.assertRaises(TypeError):\n            a = cupy.zeros((2, 2))\n            scan(a)\n\n    @testing.multi_gpu(2)\n    def test_multi_gpu(self):\n        with cuda.Device(0):\n            a = cupy.zeros((10,))\n            scan(a)\n        with cuda.Device(1):\n            a = cupy.zeros((10,))\n            scan(a)\n\n    @testing.for_all_dtypes()\n    def test_scan_out(self, dtype):\n        element_num = 10000\n\n        if dtype in {cupy.int8, cupy.uint8, cupy.float16}:\n            element_num = 100\n\n        a = cupy.ones((element_num,), dtype=dtype)\n        b = cupy.zeros_like(a)\n        scan(a, b)\n        expect = cupy.arange(start=1, stop=element_num + 1).astype(dtype)\n\n        testing.assert_array_equal(b, expect)\n\n        scan(a, a)\n        testing.assert_array_equal(a, expect)\n'"
tests/cupy_tests/core_tests/test_syncdetect.py,0,"b'import unittest\n\nimport pytest\n\nimport cupy\nimport cupyx\n\n\nclass TestSyncDetect(unittest.TestCase):\n\n    def test_disallowed(self):\n        a = cupy.array([2, 3])\n        with cupyx.allow_synchronize(False):\n            with pytest.raises(cupyx.DeviceSynchronized):\n                a.get()\n\n    def test_allowed(self):\n        a = cupy.array([2, 3])\n        with cupyx.allow_synchronize(True):\n            a.get()\n\n    def test_nested_disallowed(self):\n        a = cupy.array([2, 3])\n        with cupyx.allow_synchronize(True):\n            with cupyx.allow_synchronize(False):\n                with pytest.raises(cupyx.DeviceSynchronized):\n                    a.get()\n\n    def test_nested_allowed(self):\n        a = cupy.array([2, 3])\n        with cupyx.allow_synchronize(False):\n            with cupyx.allow_synchronize(True):\n                a.get()\n'"
tests/cupy_tests/core_tests/test_userkernel.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\nclass TestUserkernel(unittest.TestCase):\n\n    def test_manual_indexing(self, n=100):\n        in1 = cupy.random.uniform(-1, 1, n).astype(cupy.float32)\n        in2 = cupy.random.uniform(-1, 1, n).astype(cupy.float32)\n        uesr_kernel_1 = cupy.ElementwiseKernel(\n            'T x, T y',\n            'T z',\n            '''\n                z = x + y;\n            ''',\n            'uesr_kernel_1')\n        out1 = uesr_kernel_1(in1, in2)\n\n        uesr_kernel_2 = cupy.ElementwiseKernel(\n            'raw T x, raw T y',\n            'raw T z',\n            '''\n                z[i] = x[i] + y[i];\n            ''',\n            'uesr_kernel_2')\n        out2 = uesr_kernel_2(in1, in2, size=n)\n\n        testing.assert_array_equal(out1, out2)\n\n    def test_python_scalar(self):\n        for typ in (int, float, bool):\n            dtype = numpy.dtype(typ).type\n            in1_cpu = numpy.random.randint(0, 1, (4, 5)).astype(dtype)\n            in1 = cupy.array(in1_cpu)\n            scalar_value = typ(2)\n            uesr_kernel_1 = cupy.ElementwiseKernel(\n                'T x, T y',\n                'T z',\n                '''\n                    z = x + y;\n                ''',\n                'uesr_kernel_1')\n            out1 = uesr_kernel_1(in1, scalar_value)\n\n            expected = in1_cpu + dtype(2)\n            testing.assert_array_equal(out1, expected)\n\n    @testing.for_all_dtypes()\n    def test_numpy_scalar(self, dtype):\n        in1_cpu = numpy.random.randint(0, 1, (4, 5)).astype(dtype)\n        in1 = cupy.array(in1_cpu)\n        scalar_value = dtype(2)\n        uesr_kernel_1 = cupy.ElementwiseKernel(\n            'T x, T y',\n            'T z',\n            '''\n                z = x + y;\n            ''',\n            'uesr_kernel_1')\n        out1 = uesr_kernel_1(in1, scalar_value)\n\n        expected = in1_cpu + dtype(2)\n        testing.assert_array_equal(out1, expected)\n\n\nclass TestElementwiseKernelSize(unittest.TestCase):\n    # Tests to check whether size argument raises ValueError correctly\n    # depending on the raw specifiers of a user kernel.\n\n    def setUp(self):\n        self.arr1 = cupy.array([1, 2], dtype='float32')\n        self.arr2 = cupy.array([3, 4], dtype='float32')\n\n    def raises_size_not_allowed(self):\n        return pytest.raises(ValueError, match=r'^Specified \\'size\\' can')\n\n    def raises_size_required(self):\n        return pytest.raises(ValueError, match=r'^Loop size is undecided\\.')\n\n    def create_kernel(self, input_raw, output_raw):\n        # Creates a no-op kernel with given parameter specification.\n        # input_raw and output_raw are tuples of True/False whose\n        # corresponding parameter will be designated as 'raw' if True.\n        input_types = (\n            ', '.join([\n                '{}float32 x{}'.format(\n                    ('raw ' if raw else ''), i)\n                for i, raw in enumerate(input_raw)]))\n        output_types = (\n            ', '.join([\n                '{}float32 y{}'.format(\n                    ('raw ' if raw else ''), i)\n                for i, raw in enumerate(output_raw)]))\n        return cupy.ElementwiseKernel(input_types, output_types, '', 'kernel')\n\n    def test_all_raws(self):\n        # Input arrays are all raw -> size required\n        kernel1 = self.create_kernel((True, True), (False,))\n        kernel1(self.arr1, self.arr2, size=2)\n        with self.raises_size_required():\n            kernel1(self.arr1, self.arr2)\n        kernel2 = self.create_kernel((True, True), (True,))\n        kernel2(self.arr1, self.arr2, size=2)\n        with self.raises_size_required():\n            kernel2(self.arr1, self.arr2)\n\n    def test_all_nonraws(self):\n        # All arrays are not raw -> size not allowed\n        kernel1 = self.create_kernel((False, False), (False,))\n        with self.raises_size_not_allowed():\n            kernel1(self.arr1, self.arr2, size=2)\n        kernel2 = self.create_kernel((False, False), (True,))\n        with self.raises_size_not_allowed():\n            kernel2(self.arr1, self.arr2, size=2)\n\n    def test_some_nonraws(self):\n        # Some arrays are not raw -> size not allowed\n        kernel1 = self.create_kernel((True, False), (False,))\n        with self.raises_size_not_allowed():\n            kernel1(self.arr1, self.arr2, size=2)\n        kernel2 = self.create_kernel((False, True), (False,))\n        with self.raises_size_not_allowed():\n            kernel2(self.arr1, self.arr2, size=2)\n        kernel3 = self.create_kernel((True, False), (True,))\n        with self.raises_size_not_allowed():\n            kernel3(self.arr1, self.arr2, size=2)\n        kernel4 = self.create_kernel((False, True), (True,))\n        with self.raises_size_not_allowed():\n            kernel4(self.arr1, self.arr2, size=2)\n\n    def test_scalars_and_nonraws(self):\n        # Combination of scalars and non-raw arrays -> size not allowed\n        kernel1 = self.create_kernel((False, False), (False,))\n        with self.raises_size_not_allowed():\n            kernel1(self.arr1, 7, size=2)\n        kernel2 = self.create_kernel((False, False), (False,))\n        with self.raises_size_not_allowed():\n            kernel2(7, self.arr1, size=2)\n        kernel3 = self.create_kernel((False, False), (True,))\n        with self.raises_size_not_allowed():\n            kernel3(self.arr1, 7, size=2)\n        kernel4 = self.create_kernel((False, False), (True,))\n        with self.raises_size_not_allowed():\n            kernel4(7, self.arr1, size=2)\n\n    def test_scalars_and_raws_and_nonraws(self):\n        # Combination of scalars and raw arrays and non-raw arrays\n        #                                                   -> size not allowed\n        kernel1 = self.create_kernel((False, False, True), (False,))\n        with self.raises_size_not_allowed():\n            kernel1(self.arr1, 7, self.arr2, size=2)\n        kernel2 = self.create_kernel((False, False, True), (True,))\n        with self.raises_size_not_allowed():\n            kernel2(self.arr1, 7, self.arr2, size=2)\n\n    def test_scalars_and_raws(self):\n        # Combination of scalars and raw arrays -> size required\n        kernel1 = self.create_kernel((True, False), (False,))\n        kernel1(self.arr1, 7, size=2)\n        with self.raises_size_required():\n            kernel1(self.arr1, 7)\n        kernel2 = self.create_kernel((False, True), (False,))\n        kernel2(7, self.arr1, size=2)\n        with self.raises_size_required():\n            kernel2(7, self.arr1)\n        kernel3 = self.create_kernel((True, False), (True,))\n        kernel3(self.arr1, 7, size=2)\n        with self.raises_size_required():\n            kernel3(self.arr1, 7)\n        kernel4 = self.create_kernel((False, True), (True,))\n        kernel4(7, self.arr1, size=2)\n        with self.raises_size_required():\n            kernel4(7, self.arr1)\n\n    def test_size_determined_by_output(self):\n        # All the input args are unsized, but the size can be determined by the\n        # output arg. size argument is not allowed.\n\n        # Raw input\n        kernel1 = self.create_kernel((True,), (False,))\n        kernel1(self.arr1, self.arr2)\n        with self.raises_size_not_allowed():\n            kernel1(self.arr1, self.arr2, size=2)\n\n        # Scalar input\n        kernel2 = self.create_kernel((False,), (False,))\n        kernel2(self.arr1, self.arr2)\n        with self.raises_size_not_allowed():\n            kernel2(7, self.arr2, size=2)\n\n        # No input\n        kernel3 = self.create_kernel((), (False,))\n        kernel3(self.arr1)\n        with self.raises_size_not_allowed():\n            kernel3(self.arr1, size=2)\n\n    def test_no_input_and_raw_output(self):\n        # No input and the given output is raw -> size required\n        kernel1 = self.create_kernel((), (True,))\n        kernel1(self.arr1, size=2)\n        with self.raises_size_required():\n            kernel1(self.arr1)\n\n\n@testing.parameterize(*testing.product({\n    'value': [-1, 2 ** 32, 2 ** 63 - 1, -(2 ** 63)],\n}))\nclass TestUserkernelScalar(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_scalar(self, xp, dtype):\n        x = testing.shaped_arange((2, 3, 4), xp, dtype)\n        if xp is numpy:\n            y = numpy.array(self.value).astype(dtype)\n            return x + y\n        else:\n            kernel = cupy.ElementwiseKernel('T x, T y', 'T z', 'z = x + y')\n            return kernel(x, self.value)\n\n\nclass TestUserkernelManualBlockSize(unittest.TestCase):\n\n    def test_invalid_block_size(self):\n        x = testing.shaped_arange((2, 3, 4), cupy, cupy.float32)\n        kernel = cupy.ElementwiseKernel('T x, T y', 'T z', 'z = x + y')\n        with pytest.raises(ValueError):\n            kernel(x, 1, block_size=0)\n\n    def test_block_size(self):\n        x = testing.shaped_arange((2, 3, 4), cupy, cupy.float32)\n        kernel = cupy.ElementwiseKernel('T x, T y', 'T z', 'z = x + y')\n        y = kernel(x, 1, block_size=1)\n        testing.assert_array_equal(y, x + 1)\n"""
tests/cupy_tests/creation_tests/__init__.py,0,b''
tests/cupy_tests/creation_tests/test_basic.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestBasic(unittest.TestCase):\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_empty(self, xp, dtype, order):\n        a = xp.empty((2, 3, 4), dtype=dtype, order=order)\n        a.fill(0)\n        return a\n\n    @testing.slow\n    def test_empty_huge_size(self):\n        a = cupy.empty((1024, 2048, 1024), dtype='b')\n        a.fill(123)\n        self.assertTrue((a == 123).all())\n        # Free huge memory for slow test\n        del a\n        cupy.get_default_memory_pool().free_all_blocks()\n\n    @testing.slow\n    def test_empty_huge_size_fill0(self):\n        a = cupy.empty((1024, 2048, 1024), dtype='b')\n        a.fill(0)\n        self.assertTrue((a == 0).all())\n        # Free huge memory for slow test\n        del a\n        cupy.get_default_memory_pool().free_all_blocks()\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_empty_scalar(self, xp, dtype, order):\n        a = xp.empty(None, dtype=dtype, order=order)\n        a.fill(0)\n        return a\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_empty_int(self, xp, dtype, order):\n        a = xp.empty(3, dtype=dtype, order=order)\n        a.fill(0)\n        return a\n\n    @testing.slow\n    def test_empty_int_huge_size(self):\n        a = cupy.empty(2 ** 31, dtype='b')\n        a.fill(123)\n        self.assertTrue((a == 123).all())\n        # Free huge memory for slow test\n        del a\n        cupy.get_default_memory_pool().free_all_blocks()\n\n    @testing.slow\n    def test_empty_int_huge_size_fill0(self):\n        a = cupy.empty(2 ** 31, dtype='b')\n        a.fill(0)\n        self.assertTrue((a == 0).all())\n        # Free huge memory for slow test\n        del a\n        cupy.get_default_memory_pool().free_all_blocks()\n\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_empty_like(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = xp.empty_like(a, order=order)\n        b.fill(0)\n        return b\n\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_empty_like_contiguity(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = xp.empty_like(a, order=order)\n        b.fill(0)\n        if order in ['f', 'F']:\n            self.assertTrue(b.flags.f_contiguous)\n        else:\n            self.assertTrue(b.flags.c_contiguous)\n        return b\n\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_empty_like_contiguity2(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        a = xp.asfortranarray(a)\n        b = xp.empty_like(a, order=order)\n        b.fill(0)\n        if order in ['c', 'C']:\n            self.assertTrue(b.flags.c_contiguous)\n        else:\n            self.assertTrue(b.flags.f_contiguous)\n        return b\n\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_empty_like_contiguity3(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        # test strides that are both non-contiguous and non-descending\n        a = a[:, ::2, :].swapaxes(0, 1)\n        b = xp.empty_like(a, order=order)\n        b.fill(0)\n        if order in ['k', 'K', None]:\n            self.assertFalse(b.flags.c_contiguous)\n            self.assertFalse(b.flags.f_contiguous)\n        elif order in ['f', 'F']:\n            self.assertFalse(b.flags.c_contiguous)\n            self.assertTrue(b.flags.f_contiguous)\n        else:\n            self.assertTrue(b.flags.c_contiguous)\n            self.assertFalse(b.flags.f_contiguous)\n        return b\n\n    @testing.for_all_dtypes()\n    def test_empty_like_K_strides(self, dtype):\n        # test strides that are both non-contiguous and non-descending\n        a = testing.shaped_arange((2, 3, 4), numpy, dtype)\n        a = a[:, ::2, :].swapaxes(0, 1)\n        b = numpy.empty_like(a, order='K')\n        b.fill(0)\n\n        # GPU case\n        ag = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        ag = ag[:, ::2, :].swapaxes(0, 1)\n        bg = cupy.empty_like(ag, order='K')\n        bg.fill(0)\n\n        # make sure NumPy and CuPy strides agree\n        self.assertEqual(b.strides, bg.strides)\n        return\n\n    @testing.for_all_dtypes()\n    def test_empty_like_invalid_order(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp, dtype)\n            with pytest.raises(TypeError):\n                xp.empty_like(a, order='Q')\n\n    def test_empty_like_subok(self):\n        a = testing.shaped_arange((2, 3, 4), cupy)\n        with pytest.raises(TypeError):\n            cupy.empty_like(a, subok=True)\n\n    @testing.for_CF_orders()\n    def test_empty_zero_sized_array_strides(self, order):\n        a = numpy.empty((1, 0, 2), dtype='d', order=order)\n        b = cupy.empty((1, 0, 2), dtype='d', order=order)\n        self.assertEqual(b.strides, a.strides)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_eye(self, xp, dtype):\n        return xp.eye(5, 4, 1, dtype)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_identity(self, xp, dtype):\n        return xp.identity(4, dtype)\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_zeros(self, xp, dtype, order):\n        return xp.zeros((2, 3, 4), dtype=dtype, order=order)\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_zeros_scalar(self, xp, dtype, order):\n        return xp.zeros(None, dtype=dtype, order=order)\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_zeros_int(self, xp, dtype, order):\n        return xp.zeros(3, dtype=dtype, order=order)\n\n    @testing.for_CF_orders()\n    def test_zeros_strides(self, order):\n        a = numpy.zeros((2, 3), dtype='d', order=order)\n        b = cupy.zeros((2, 3), dtype='d', order=order)\n        self.assertEqual(b.strides, a.strides)\n\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_zeros_like(self, xp, dtype, order):\n        a = xp.ndarray((2, 3, 4), dtype=dtype)\n        return xp.zeros_like(a, order=order)\n\n    def test_zeros_like_subok(self):\n        a = cupy.ndarray((2, 3, 4))\n        with pytest.raises(TypeError):\n            cupy.zeros_like(a, subok=True)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_ones(self, xp, dtype):\n        return xp.ones((2, 3, 4), dtype=dtype)\n\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_ones_like(self, xp, dtype, order):\n        a = xp.ndarray((2, 3, 4), dtype=dtype)\n        return xp.ones_like(a, order=order)\n\n    def test_ones_like_subok(self):\n        a = cupy.ndarray((2, 3, 4))\n        with pytest.raises(TypeError):\n            cupy.ones_like(a, subok=True)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_full(self, xp, dtype):\n        return xp.full((2, 3, 4), 1, dtype=dtype)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_full_default_dtype(self, xp, dtype):\n        return xp.full((2, 3, 4), xp.array(1, dtype=dtype))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_full_default_dtype_cpu_input(self, xp, dtype):\n        return xp.full((2, 3, 4), numpy.array(1, dtype=dtype))\n\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_full_like(self, xp, dtype, order):\n        a = xp.ndarray((2, 3, 4), dtype=dtype)\n        return xp.full_like(a, 1, order=order)\n\n    def test_full_like_subok(self):\n        a = cupy.ndarray((2, 3, 4))\n        with pytest.raises(TypeError):\n            cupy.full_like(a, 1, subok=True)\n\n\n@testing.parameterize(\n    *testing.product({\n        'shape': [4, (4, ), (4, 2), (4, 2, 3), (5, 4, 2, 3)],\n    })\n)\n@testing.gpu\nclass TestBasicReshape(unittest.TestCase):\n\n    @testing.with_requires('numpy>=1.17.0')\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_empty_like_reshape(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = xp.empty_like(a, order=order, shape=self.shape)\n        b.fill(0)\n        return b\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    def test_empty_like_reshape_cupy_only(self, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        b = cupy.empty_like(a, shape=self.shape)\n        b.fill(0)\n        c = cupy.empty(self.shape, order=order, dtype=dtype)\n        c.fill(0)\n\n        testing.assert_array_equal(b, c)\n\n    @testing.with_requires('numpy>=1.17.0')\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_empty_like_reshape_contiguity(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = xp.empty_like(a, order=order, shape=self.shape)\n        b.fill(0)\n        if order in ['f', 'F']:\n            self.assertTrue(b.flags.f_contiguous)\n        else:\n            self.assertTrue(b.flags.c_contiguous)\n        return b\n\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    def test_empty_like_reshape_contiguity_cupy_only(self, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        b = cupy.empty_like(a, order=order, shape=self.shape)\n        b.fill(0)\n        c = cupy.empty(self.shape)\n        c.fill(0)\n        if order in ['f', 'F']:\n            self.assertTrue(b.flags.f_contiguous)\n        else:\n            self.assertTrue(b.flags.c_contiguous)\n        testing.assert_array_equal(b, c)\n\n    @testing.with_requires('numpy>=1.17.0')\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_empty_like_reshape_contiguity2(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        a = xp.asfortranarray(a)\n        b = xp.empty_like(a, order=order, shape=self.shape)\n        b.fill(0)\n        shape = self.shape if not numpy.isscalar(self.shape) else (self.shape,)\n        if (order in ['c', 'C'] or\n                (order in ['k', 'K', None] and len(shape) != a.ndim)):\n            self.assertTrue(b.flags.c_contiguous)\n        else:\n            self.assertTrue(b.flags.f_contiguous)\n        return b\n\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    def test_empty_like_reshape_contiguity2_cupy_only(self, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        a = cupy.asfortranarray(a)\n        b = cupy.empty_like(a, order=order, shape=self.shape)\n        b.fill(0)\n        c = cupy.empty(self.shape)\n        c.fill(0)\n        shape = self.shape if not numpy.isscalar(self.shape) else (self.shape,)\n        if (order in ['c', 'C'] or\n                (order in ['k', 'K', None] and len(shape) != a.ndim)):\n            self.assertTrue(b.flags.c_contiguous)\n        else:\n            self.assertTrue(b.flags.f_contiguous)\n        testing.assert_array_equal(b, c)\n\n    @testing.with_requires('numpy>=1.17.0')\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_empty_like_reshape_contiguity3(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        # test strides that are both non-contiguous and non-descending\n        a = a[:, ::2, :].swapaxes(0, 1)\n        b = xp.empty_like(a, order=order, shape=self.shape)\n        b.fill(0)\n        shape = self.shape if not numpy.isscalar(self.shape) else (self.shape,)\n        if len(shape) == 1:\n            self.assertTrue(b.flags.c_contiguous)\n            self.assertTrue(b.flags.f_contiguous)\n        elif order in ['k', 'K', None] and len(shape) == a.ndim:\n            self.assertFalse(b.flags.c_contiguous)\n            self.assertFalse(b.flags.f_contiguous)\n        elif order in ['f', 'F']:\n            self.assertFalse(b.flags.c_contiguous)\n            self.assertTrue(b.flags.f_contiguous)\n        else:\n            self.assertTrue(b.flags.c_contiguous)\n            self.assertFalse(b.flags.f_contiguous)\n        return b\n\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    def test_empty_like_reshape_contiguity3_cupy_only(self, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        # test strides that are both non-contiguous and non-descending\n        a = a[:, ::2, :].swapaxes(0, 1)\n        b = cupy.empty_like(a, order=order, shape=self.shape)\n        b.fill(0)\n        shape = self.shape if not numpy.isscalar(self.shape) else (self.shape,)\n        if len(shape) == 1:\n            self.assertTrue(b.flags.c_contiguous)\n            self.assertTrue(b.flags.f_contiguous)\n        elif order in ['k', 'K', None] and len(shape) == a.ndim:\n            self.assertFalse(b.flags.c_contiguous)\n            self.assertFalse(b.flags.f_contiguous)\n        elif order in ['f', 'F']:\n            self.assertFalse(b.flags.c_contiguous)\n            self.assertTrue(b.flags.f_contiguous)\n        else:\n            self.assertTrue(b.flags.c_contiguous)\n            self.assertFalse(b.flags.f_contiguous)\n\n        c = cupy.zeros(self.shape)\n        c.fill(0)\n        testing.assert_array_equal(b, c)\n\n    @testing.with_requires('numpy>=1.17.0')\n    @testing.for_all_dtypes()\n    def test_empty_like_K_strides_reshape(self, dtype):\n        # test strides that are both non-contiguous and non-descending\n        a = testing.shaped_arange((2, 3, 4), numpy, dtype)\n        a = a[:, ::2, :].swapaxes(0, 1)\n        b = numpy.empty_like(a, order='K', shape=self.shape)\n        b.fill(0)\n\n        # GPU case\n        ag = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        ag = ag[:, ::2, :].swapaxes(0, 1)\n        bg = cupy.empty_like(ag, order='K', shape=self.shape)\n        bg.fill(0)\n\n        # make sure NumPy and CuPy strides agree\n        self.assertEqual(b.strides, bg.strides)\n        return\n\n    @testing.with_requires('numpy>=1.17.0')\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_zeros_like_reshape(self, xp, dtype, order):\n        a = xp.ndarray((2, 3, 4), dtype=dtype)\n        return xp.zeros_like(a, order=order, shape=self.shape)\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    def test_zeros_like_reshape_cupy_only(self, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        b = cupy.zeros_like(a, shape=self.shape)\n        c = cupy.zeros(self.shape, order=order, dtype=dtype)\n\n        testing.assert_array_equal(b, c)\n\n    @testing.with_requires('numpy>=1.17.0')\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_ones_like_reshape(self, xp, dtype, order):\n        a = xp.ndarray((2, 3, 4), dtype=dtype)\n        return xp.ones_like(a, order=order, shape=self.shape)\n\n    @testing.for_all_dtypes()\n    def test_ones_like_reshape_cupy_only(self, dtype):\n        a = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        b = cupy.ones_like(a, shape=self.shape)\n        c = cupy.ones(self.shape, dtype=dtype)\n\n        testing.assert_array_equal(b, c)\n\n    @testing.with_requires('numpy>=1.17.0')\n    @testing.for_orders('CFAK')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_full_like_reshape(self, xp, dtype, order):\n        a = xp.ndarray((2, 3, 4), dtype=dtype)\n        return xp.full_like(a, 1, order=order, shape=self.shape)\n\n    @testing.for_all_dtypes()\n    def test_full_like_reshape_cupy_only(self, dtype):\n        a = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        b = cupy.full_like(a, 1, shape=self.shape)\n        c = cupy.full(self.shape, 1, dtype=dtype)\n\n        testing.assert_array_equal(b, c)\n"""
tests/cupy_tests/creation_tests/test_from_data.py,0,"b'import tempfile\nimport unittest\n\nimport pytest\n\nimport cupy\nfrom cupy import cuda\nfrom cupy import testing\nimport numpy\n\n\n@testing.gpu\nclass TestFromData(unittest.TestCase):\n\n    @testing.for_orders(\'CFAK\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_array(self, xp, dtype, order):\n        return xp.array([[1, 2, 3], [2, 3, 4]], dtype=dtype, order=order)\n\n    @testing.for_orders(\'CFAK\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_array_from_empty_list(self, xp, dtype, order):\n        return xp.array([], dtype=dtype, order=order)\n\n    @testing.for_orders(\'CFAK\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_array_from_nested_empty_list(self, xp, dtype, order):\n        return xp.array([[], []], dtype=dtype, order=order)\n\n    @testing.for_orders(\'CFAK\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_array_from_numpy(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), numpy, dtype)\n        return xp.array(a, order=order)\n\n    @testing.for_orders(\'CFAK\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_array_from_numpy_scalar(self, xp, dtype, order):\n        a = numpy.array(2, dtype=dtype)\n        return xp.array(a, order=order)\n\n    @testing.for_orders(\'CFAK\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_array_from_numpy_broad_cast(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 1, 4), numpy, dtype)\n        a = numpy.broadcast_to(a, (2, 3, 4))\n        return xp.array(a, order=order)\n\n    @testing.for_orders(\'CFAK\', name=\'src_order\')\n    @testing.for_orders(\'CFAK\', name=\'dst_order\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal(strides_check=True)\n    def test_array_from_list_of_numpy(self, xp, dtype, src_order, dst_order):\n        # compares numpy.array(<list of numpy.ndarray>) with\n        # cupy.array(<list of numpy.ndarray>)\n        a = [\n            testing.shaped_arange((3, 4), numpy, dtype, src_order) + (12 * i)\n            for i in range(2)]\n        return xp.array(a, order=dst_order)\n\n    @testing.for_orders(\'CFAK\', name=\'src_order\')\n    @testing.for_orders(\'CFAK\', name=\'dst_order\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal(strides_check=True)\n    def test_array_from_list_of_numpy_view(self, xp, dtype, src_order,\n                                           dst_order):\n        # compares numpy.array(<list of numpy.ndarray>) with\n        # cupy.array(<list of numpy.ndarray>)\n\n        # create a list of view of ndarrays\n        a = [\n            (testing.shaped_arange((3, 8), numpy,\n                                   dtype, src_order) + (24 * i))[:, ::2]\n            for i in range(2)]\n        return xp.array(a, order=dst_order)\n\n    @testing.for_orders(\'CFAK\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal(strides_check=True)\n    def test_array_from_list_of_numpy_scalar(self, xp, dtype, order):\n        # compares numpy.array(<list of numpy.ndarray>) with\n        # cupy.array(<list of numpy.ndarray>)\n        a = [numpy.array(i, dtype=dtype) for i in range(2)]\n        return xp.array(a, order=order)\n\n    @testing.for_orders(\'CFAK\', name=\'src_order\')\n    @testing.for_orders(\'CFAK\', name=\'dst_order\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal(strides_check=True)\n    def test_array_from_nested_list_of_numpy(self, xp, dtype, src_order,\n                                             dst_order):\n        # compares numpy.array(<list of numpy.ndarray>) with\n        # cupy.array(<list of numpy.ndarray>)\n        a = [\n            [testing.shaped_arange(\n                (3, 4), numpy, dtype, src_order) + (12 * i)]\n            for i in range(2)]\n        return xp.array(a, order=dst_order)\n\n    @testing.for_orders(\'CFAK\', name=\'src_order\')\n    @testing.for_orders(\'CFAK\', name=\'dst_order\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal(strides_check=True)\n    def test_array_from_list_of_cupy(self, xp, dtype, src_order, dst_order):\n        # compares numpy.array(<list of numpy.ndarray>) with\n        # cupy.array(<list of cupy.ndarray>)\n        a = [\n            testing.shaped_arange((3, 4), xp, dtype, src_order) + (12 * i)\n            for i in range(2)]\n        return xp.array(a, order=dst_order)\n\n    @testing.for_orders(\'CFAK\', name=\'src_order\')\n    @testing.for_orders(\'CFAK\', name=\'dst_order\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal(strides_check=True)\n    def test_array_from_list_of_cupy_view(self, xp, dtype, src_order,\n                                          dst_order):\n        # compares numpy.array(<list of numpy.ndarray>) with\n        # cupy.array(<list of cupy.ndarray>)\n\n        # create a list of view of ndarrays\n        a = [\n            (testing.shaped_arange((3, 8), xp,\n                                   dtype, src_order) + (24 * i))[:, ::2]\n            for i in range(2)]\n        return xp.array(a, order=dst_order)\n\n    @testing.for_orders(\'CFAK\', name=\'src_order\')\n    @testing.for_orders(\'CFAK\', name=\'dst_order\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal(strides_check=True)\n    def test_array_from_nested_list_of_cupy(self, xp, dtype, src_order,\n                                            dst_order):\n        # compares numpy.array(<list of numpy.ndarray>) with\n        # cupy.array(<list of cupy.ndarray>)\n        a = [\n            [testing.shaped_arange((3, 4), xp, dtype, src_order) + (12 * i)]\n            for i in range(2)]\n        return xp.array(a, order=dst_order)\n\n    @testing.for_orders(\'CFAK\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal(strides_check=True)\n    def test_array_from_list_of_cupy_scalar(self, xp, dtype, order):\n        # compares numpy.array(<list of numpy.ndarray>) with\n        # cupy.array(<list of cupy.ndarray>)\n        a = [xp.array(i, dtype=dtype) for i in range(2)]\n        return xp.array(a, order=order)\n\n    @testing.for_orders(\'CFAK\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_array_copy(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.array(a, order=order)\n\n    @testing.for_orders(\'CFAK\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_array_copy_is_copied(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = xp.array(a, order=order)\n        a.fill(0)\n        return b\n\n    @testing.for_orders(\'CFAK\')\n    @testing.for_all_dtypes(name=\'dtype1\', no_complex=True)\n    @testing.for_all_dtypes(name=\'dtype2\')\n    @testing.numpy_cupy_array_equal()\n    def test_array_copy_with_dtype(self, xp, dtype1, dtype2, order):\n        # complex to real makes no sense\n        a = testing.shaped_arange((2, 3, 4), xp, dtype1)\n        return xp.array(a, dtype=dtype2, order=order)\n\n    @testing.for_orders(\'CFAK\')\n    @testing.for_all_dtypes(name=\'dtype1\', no_complex=True)\n    @testing.for_all_dtypes(name=\'dtype2\')\n    @testing.numpy_cupy_array_equal()\n    def test_array_copy_with_dtype_char(self, xp, dtype1, dtype2, order):\n        # complex to real makes no sense\n        a = testing.shaped_arange((2, 3, 4), xp, dtype1)\n        return xp.array(a, dtype=numpy.dtype(dtype2).char, order=order)\n\n    @testing.for_orders(\'CFAK\')\n    @testing.numpy_cupy_array_equal()\n    def test_array_copy_with_dtype_being_none(self, xp, order):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return xp.array(a, dtype=None, order=order)\n\n    @testing.for_orders(\'CFAK\', name=\'src_order\')\n    @testing.for_orders(\'CFAK\', name=\'dst_order\')\n    @testing.for_all_dtypes(name=\'dtype1\', no_complex=True)\n    @testing.for_all_dtypes(name=\'dtype2\')\n    @testing.numpy_cupy_array_equal(strides_check=True)\n    def test_array_copy_list_of_numpy_with_dtype(self, xp, dtype1, dtype2,\n                                                 src_order, dst_order):\n        # compares numpy.array(<list of numpy.ndarray>) with\n        # cupy.array(<list of numpy.ndarray>)\n        a = [\n            testing.shaped_arange((3, 4), numpy, dtype1, src_order) + (12 * i)\n            for i in range(2)]\n        return xp.array(a, dtype=dtype2, order=dst_order)\n\n    @testing.for_orders(\'CFAK\', name=\'src_order\')\n    @testing.for_orders(\'CFAK\', name=\'dst_order\')\n    @testing.for_all_dtypes(name=\'dtype1\', no_complex=True)\n    @testing.for_all_dtypes(name=\'dtype2\')\n    @testing.numpy_cupy_array_equal(strides_check=True)\n    def test_array_copy_list_of_numpy_with_dtype_char(self, xp, dtype1,\n                                                      dtype2, src_order,\n                                                      dst_order):\n        # compares numpy.array(<list of numpy.ndarray>) with\n        # cupy.array(<list of numpy.ndarray>)\n        a = [\n            testing.shaped_arange((3, 4), numpy, dtype1, src_order) + (12 * i)\n            for i in range(2)]\n        return xp.array(a, dtype=numpy.dtype(dtype2).char, order=dst_order)\n\n    @testing.for_orders(\'CFAK\', name=\'src_order\')\n    @testing.for_orders(\'CFAK\', name=\'dst_order\')\n    @testing.for_all_dtypes(name=\'dtype1\', no_complex=True)\n    @testing.for_all_dtypes(name=\'dtype2\')\n    @testing.numpy_cupy_array_equal(strides_check=True)\n    def test_array_copy_list_of_cupy_with_dtype(self, xp, dtype1, dtype2,\n                                                src_order, dst_order):\n        # compares numpy.array(<list of numpy.ndarray>) with\n        # cupy.array(<list of cupy.ndarray>)\n        a = [\n            testing.shaped_arange((3, 4), xp, dtype1, src_order) + (12 * i)\n            for i in range(2)]\n        return xp.array(a, dtype=dtype2, order=dst_order)\n\n    @testing.for_orders(\'CFAK\', name=\'src_order\')\n    @testing.for_orders(\'CFAK\', name=\'dst_order\')\n    @testing.for_all_dtypes(name=\'dtype1\', no_complex=True)\n    @testing.for_all_dtypes(name=\'dtype2\')\n    @testing.numpy_cupy_array_equal(strides_check=True)\n    def test_array_copy_list_of_cupy_with_dtype_char(self, xp, dtype1, dtype2,\n                                                     src_order, dst_order):\n        # compares numpy.array(<list of numpy.ndarray>) with\n        # cupy.array(<list of cupy.ndarray>)\n        a = [\n            testing.shaped_arange((3, 4), xp, dtype1, src_order) + (12 * i)\n            for i in range(2)]\n        return xp.array(a, dtype=numpy.dtype(dtype2).char, order=dst_order)\n\n    @testing.for_orders(\'CFAK\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_array_no_copy(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = xp.array(a, copy=False, order=order)\n        a.fill(0)\n        return b\n\n    @testing.for_orders(\'CFAK\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_array_f_contiguous_input(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype, order=\'F\')\n        b = xp.array(a, copy=False, order=order)\n        return b\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_array_f_contiguous_output(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = xp.array(a, copy=False, order=\'F\')\n        assert b.flags.f_contiguous\n        return b\n\n    @testing.multi_gpu(2)\n    def test_array_multi_device(self):\n        with cuda.Device(0):\n            x = testing.shaped_arange((2, 3, 4), cupy, dtype=\'f\')\n        with cuda.Device(1):\n            y = cupy.array(x)\n        assert isinstance(y, cupy.ndarray)\n        assert x is not y  # Do copy\n        assert int(x.device) == 0\n        assert int(y.device) == 1\n        testing.assert_array_equal(x, y)\n\n    @testing.multi_gpu(2)\n    def test_array_multi_device_zero_size(self):\n        with cuda.Device(0):\n            x = testing.shaped_arange((0,), cupy, dtype=\'f\')\n        with cuda.Device(1):\n            y = cupy.array(x)\n        assert isinstance(y, cupy.ndarray)\n        assert x is not y  # Do copy\n        assert x.device.id == 0\n        assert y.device.id == 1\n        testing.assert_array_equal(x, y)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_array_no_copy_ndmin(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = xp.array(a, copy=False, ndmin=5)\n        assert a.shape == (2, 3, 4)\n        a.fill(0)\n        return b\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_asarray(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.asarray(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_asarray_is_not_copied(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = xp.asarray(a)\n        a.fill(0)\n        return b\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_asarray_with_order(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = xp.asarray(a, order=order)\n        if order in [\'F\', \'f\']:\n            assert b.flags.f_contiguous\n        else:\n            assert b.flags.c_contiguous\n        return b\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_asarray_preserves_numpy_array_order(self, xp, dtype, order):\n        a_numpy = testing.shaped_arange((2, 3, 4), numpy, dtype, order)\n        b = xp.asarray(a_numpy)\n        assert b.flags.f_contiguous == a_numpy.flags.f_contiguous\n        assert b.flags.c_contiguous == a_numpy.flags.c_contiguous\n        return b\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_asanyarray_with_order(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = xp.asanyarray(a, order=order)\n        if order in [\'F\', \'f\']:\n            assert b.flags.f_contiguous\n        else:\n            assert b.flags.c_contiguous\n        return b\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_asarray_from_numpy(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), numpy, dtype)\n        b = xp.asarray(a, order=order)\n        if order in [\'F\', \'f\']:\n            assert b.flags.f_contiguous\n        else:\n            assert b.flags.c_contiguous\n        return b\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_asarray_with_order_copy_behavior(self, xp, dtype, order):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = xp.asarray(a, order=order)\n        a.fill(0)\n        return b\n\n    def test_ascontiguousarray_on_noncontiguous_array(self):\n        a = testing.shaped_arange((2, 3, 4))\n        b = a.transpose(2, 0, 1)\n        c = cupy.ascontiguousarray(b)\n        assert c.flags.c_contiguous\n        testing.assert_array_equal(b, c)\n\n    def test_ascontiguousarray_on_contiguous_array(self):\n        a = testing.shaped_arange((2, 3, 4))\n        b = cupy.ascontiguousarray(a)\n        assert a is b\n\n    @testing.numpy_cupy_array_equal()\n    def test_asarray_cuda_array_zero_dim(self, xp):\n        a = xp.ones(())\n        return xp.ascontiguousarray(a)\n\n    @testing.numpy_cupy_array_equal()\n    def test_asarray_cuda_array_zero_dim_dtype(self, xp):\n        a = xp.ones((), dtype=numpy.float64)\n        return xp.ascontiguousarray(a, dtype=numpy.int64)\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_copy(self, xp, dtype, order):\n        a = xp.zeros((2, 3, 4), dtype=dtype)\n        b = xp.copy(a, order=order)\n        a[1] = 1\n        return b\n\n    @testing.multi_gpu(2)\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    def test_copy_multigpu(self, dtype, order):\n        with cuda.Device(0):\n            src = cupy.random.uniform(-1, 1, (2, 3)).astype(dtype)\n        with cuda.Device(1):\n            dst = cupy.copy(src, order)\n        testing.assert_allclose(src, dst, rtol=0, atol=0)\n\n    @testing.for_CF_orders()\n    @testing.numpy_cupy_equal()\n    def test_copy_order(self, xp, order):\n        a = xp.zeros((2, 3, 4), order=order)\n        b = xp.copy(a)\n        return (b.flags.c_contiguous, b.flags.f_contiguous)\n\n    @testing.numpy_cupy_array_equal()\n    def test_asfortranarray_cuda_array_zero_dim(self, xp):\n        a = xp.ones(())\n        return xp.asfortranarray(a)\n\n    @testing.for_all_dtypes_combination([\'dtype_a\', \'dtype_b\'],\n                                        no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_asfortranarray_cuda_array_zero_dim_dtype(\n            self, xp, dtype_a, dtype_b):\n        a = xp.ones((), dtype=dtype_a)\n        return xp.asfortranarray(a, dtype=dtype_b)\n\n    @testing.numpy_cupy_array_equal()\n    def test_fromfile(self, xp):\n        with tempfile.TemporaryFile() as fh:\n            fh.write(b""\\x00\\x01\\x02\\x03\\x04"")\n            fh.flush()\n            fh.seek(0)\n            return xp.fromfile(fh, dtype=""u1"")\n\n\nmax_cuda_array_interface_version = 2\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    \'ver\': tuple(range(max_cuda_array_interface_version+1)),\n    \'strides\': (False, None, True),\n}))\nclass TestCudaArrayInterface(unittest.TestCase):\n    @testing.for_all_dtypes()\n    def test_base(self, dtype):\n        a = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        b = cupy.asarray(\n            DummyObjectWithCudaArrayInterface(a, self.ver, self.strides))\n        testing.assert_array_equal(a, b)\n\n    @testing.for_all_dtypes()\n    def test_not_copied(self, dtype):\n        a = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        b = cupy.asarray(\n            DummyObjectWithCudaArrayInterface(a, self.ver, self.strides))\n        a.fill(0)\n        testing.assert_array_equal(a, b)\n\n    @testing.for_all_dtypes()\n    def test_order(self, dtype):\n        a = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        b = cupy.asarray(\n            DummyObjectWithCudaArrayInterface(a, self.ver, self.strides),\n            order=\'F\')\n        assert b.flags.f_contiguous\n        testing.assert_array_equal(a, b)\n\n    @testing.for_all_dtypes()\n    def test_with_strides(self, dtype):\n        a = testing.shaped_arange((2, 3, 4), cupy, dtype).T\n        b = cupy.asarray(\n            DummyObjectWithCudaArrayInterface(a, self.ver, self.strides))\n        assert a.strides == b.strides\n        assert a.nbytes == b.data.mem.size\n\n    @testing.for_all_dtypes()\n    def test_with_zero_size_array(self, dtype):\n        a = testing.shaped_arange((0,), cupy, dtype)\n        b = cupy.asarray(\n            DummyObjectWithCudaArrayInterface(a, self.ver, self.strides))\n        assert a.strides == b.strides\n        assert a.nbytes == b.data.mem.size\n        assert a.data.ptr == 0\n        assert a.size == 0\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    \'ver\': tuple(range(1, max_cuda_array_interface_version+1)),\n    \'strides\': (False, None, True),\n}))\nclass TestCudaArrayInterfaceMaskedArray(unittest.TestCase):\n    # TODO(leofang): update this test when masked array is supported\n    @testing.for_all_dtypes()\n    def test_masked_array(self, dtype):\n        a = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        mask = testing.shaped_arange((2, 3, 4), cupy, dtype)\n        a = DummyObjectWithCudaArrayInterface(a, self.ver, self.strides, mask)\n        with pytest.raises(ValueError) as ex:\n            b = cupy.asarray(a)  # noqa\n        assert \'does not support\' in str(ex.value)\n\n\n@testing.slow\n@testing.gpu\nclass TestCudaArrayInterfaceBigArray(unittest.TestCase):\n    def test_with_over_size_array(self):\n        # real example from #3009\n        size = 5 * 10**8\n        try:\n            a = testing.shaped_random((size,), cupy, cupy.float64)\n            b = cupy.asarray(DummyObjectWithCudaArrayInterface(a, 2, None))\n            testing.assert_array_equal(a, b)\n        except cupy.cuda.memory.OutOfMemoryError:\n            pass\n        else:\n            del b, a\n        finally:\n            cupy.get_default_memory_pool().free_all_blocks()\n\n\nclass DummyObjectWithCudaArrayInterface(object):\n    def __init__(self, a, ver, include_strides=False, mask=None):\n        assert ver in tuple(range(max_cuda_array_interface_version+1))\n        self.a = a\n        self.ver = ver\n        self.include_strides = include_strides\n        self.mask = mask\n\n    @property\n    def __cuda_array_interface__(self):\n        desc = {\n            \'shape\': self.a.shape,\n            \'typestr\': self.a.dtype.str,\n            \'descr\': self.a.dtype.descr,\n            \'data\': (self.a.data.ptr, False),\n            \'version\': self.ver,\n        }\n        if self.a.flags.c_contiguous:\n            if self.include_strides is True:\n                desc[\'strides\'] = self.a.strides\n            elif self.include_strides is None:\n                desc[\'strides\'] = None\n            else:  # self.include_strides is False\n                pass\n        else:  # F contiguous or neither\n            desc[\'strides\'] = self.a.strides\n        if self.mask is not None:\n            desc[\'mask\'] = self.mask\n        return desc\n\n\n@testing.parameterize(\n    *testing.product({\n        \'ndmin\': [0, 1, 2, 3],\n        \'copy\': [True, False],\n        \'xp\': [numpy, cupy]\n    })\n)\nclass TestArrayPreservationOfShape(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_cupy_array(self, dtype):\n        shape = 2, 3\n        a = testing.shaped_arange(shape, self.xp, dtype)\n        cupy.array(a, copy=self.copy, ndmin=self.ndmin)\n\n        # Check if cupy.ndarray does not alter\n        # the shape of the original array.\n        assert a.shape == shape\n\n\n@testing.parameterize(\n    *testing.product({\n        \'ndmin\': [0, 1, 2, 3],\n        \'copy\': [True, False],\n        \'xp\': [numpy, cupy]\n    })\n)\nclass TestArrayCopy(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_cupy_array(self, dtype):\n        a = testing.shaped_arange((2, 3), self.xp, dtype)\n        actual = cupy.array(a, copy=self.copy, ndmin=self.ndmin)\n\n        should_copy = (self.xp is numpy) or self.copy\n        # TODO(Kenta Oono): Better determination of copy.\n        is_copied = not ((actual is a) or (actual.base is a) or\n                         (actual.base is a.base and a.base is not None))\n        assert should_copy == is_copied\n\n\nclass TestArrayInvalidObject(unittest.TestCase):\n\n    def test_invalid_type(self):\n        a = numpy.array([1, 2, 3], dtype=object)\n        with self.assertRaises(ValueError):\n            cupy.array(a)\n'"
tests/cupy_tests/creation_tests/test_matrix.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestMatrix(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_diag1(self, xp):\n        a = testing.shaped_arange((3, 3), xp)\n        return xp.diag(a)\n\n    @testing.numpy_cupy_array_equal()\n    def test_diag2(self, xp):\n        a = testing.shaped_arange((3, 3), xp)\n        return xp.diag(a, 1)\n\n    @testing.numpy_cupy_array_equal()\n    def test_diag3(self, xp):\n        a = testing.shaped_arange((3, 3), xp)\n        return xp.diag(a, -2)\n\n    @testing.numpy_cupy_array_equal()\n    def test_diag_extraction_from_nested_list(self, xp):\n        a = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n        r = xp.diag(a, 1)\n        self.assertIsInstance(r, xp.ndarray)\n        return r\n\n    @testing.numpy_cupy_array_equal()\n    def test_diag_extraction_from_nested_tuple(self, xp):\n        a = ((1, 2, 3), (4, 5, 6), (7, 8, 9))\n        r = xp.diag(a, -1)\n        self.assertIsInstance(r, xp.ndarray)\n        return r\n\n    @testing.numpy_cupy_array_equal()\n    def test_diag_construction(self, xp):\n        a = testing.shaped_arange((3,), xp)\n        r = xp.diag(a)\n        self.assertIsInstance(r, xp.ndarray)\n        return r\n\n    @testing.numpy_cupy_array_equal()\n    def test_diag_construction_from_list(self, xp):\n        a = [1, 2, 3]\n        r = xp.diag(a)\n        self.assertIsInstance(r, xp.ndarray)\n        return r\n\n    @testing.numpy_cupy_array_equal()\n    def test_diag_construction_from_tuple(self, xp):\n        a = (1, 2, 3)\n        r = xp.diag(a)\n        self.assertIsInstance(r, xp.ndarray)\n        return r\n\n    def test_diag_scaler(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.diag(1)\n\n    def test_diag_0dim(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.diag(xp.zeros(()))\n\n    def test_diag_3dim(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.diag(xp.zeros((2, 2, 2)))\n\n    @testing.numpy_cupy_array_equal()\n    def test_diagflat1(self, xp):\n        a = testing.shaped_arange((3, 3), xp)\n        return xp.diagflat(a)\n\n    @testing.numpy_cupy_array_equal()\n    def test_diagflat2(self, xp):\n        a = testing.shaped_arange((3, 3), xp)\n        return xp.diagflat(a, 1)\n\n    @testing.numpy_cupy_array_equal()\n    def test_diagflat3(self, xp):\n        a = testing.shaped_arange((3, 3), xp)\n        return xp.diagflat(a, -2)\n\n    @testing.numpy_cupy_array_equal()\n    def test_diagflat_from_scalar(self, xp):\n        return xp.diagflat(3)\n\n    @testing.numpy_cupy_array_equal()\n    def test_diagflat_from_scalar_with_k0(self, xp):\n        return xp.diagflat(3, 0)\n\n    @testing.numpy_cupy_array_equal()\n    def test_diagflat_from_scalar_with_k1(self, xp):\n        return xp.diagflat(3, 1)\n\n\n@testing.parameterize(\n    {'shape': (2,)},\n    {'shape': (3, 3)},\n    {'shape': (4, 3)},\n)\n@testing.gpu\nclass TestTri(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_tri(self, xp, dtype):\n        return xp.tri(*self.shape, k=0, dtype=dtype)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_tri_nega(self, xp, dtype):\n        return xp.tri(*self.shape, k=-1, dtype=dtype)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_tri_posi(self, xp, dtype):\n        return xp.tri(*self.shape, k=1, dtype=dtype)\n\n\n@testing.parameterize(\n    {'shape': (2,)},\n    {'shape': (3, 3)},\n    {'shape': (4, 3)},\n    {'shape': (2, 3, 4)},\n)\n@testing.gpu\nclass TestTriLowerAndUpper(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_tril(self, xp, dtype):\n        m = testing.shaped_arange(self.shape, xp, dtype)\n        return xp.tril(m)\n\n    @testing.numpy_cupy_array_equal()\n    def test_tril_array_like(self, xp):\n        return xp.tril([[1, 2], [3, 4]])\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_tril_nega(self, xp, dtype):\n        m = testing.shaped_arange(self.shape, xp, dtype)\n        return xp.tril(m, -1)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_tril_posi(self, xp, dtype):\n        m = testing.shaped_arange(self.shape, xp, dtype)\n        return xp.tril(m, 1)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_triu(self, xp, dtype):\n        m = testing.shaped_arange(self.shape, xp, dtype)\n        return xp.triu(m)\n\n    @testing.numpy_cupy_array_equal()\n    def test_triu_array_like(self, xp):\n        return xp.triu([[1, 2], [3, 4]])\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_triu_nega(self, xp, dtype):\n        m = testing.shaped_arange(self.shape, xp, dtype)\n        return xp.triu(m, -1)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_triu_posi(self, xp, dtype):\n        m = testing.shaped_arange(self.shape, xp, dtype)\n        return xp.triu(m, 1)\n"""
tests/cupy_tests/creation_tests/test_ranges.py,0,"b""import math\nimport sys\nimport unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestRanges(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_arange(self, xp, dtype):\n        return xp.arange(10, dtype=dtype)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_arange2(self, xp, dtype):\n        return xp.arange(5, 10, dtype=dtype)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_arange3(self, xp, dtype):\n        return xp.arange(1, 11, 2, dtype=dtype)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_arange4(self, xp, dtype):\n        return xp.arange(20, 2, -3, dtype=dtype)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_arange5(self, xp, dtype):\n        return xp.arange(0, 100, None, dtype=dtype)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_arange6(self, xp, dtype):\n        return xp.arange(0, 2, dtype=dtype)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_arange7(self, xp, dtype):\n        return xp.arange(10, 11, dtype=dtype)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_arange8(self, xp, dtype):\n        return xp.arange(10, 8, -1, dtype=dtype)\n\n    def test_arange9(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.arange(10, dtype=xp.bool_)\n\n    @testing.numpy_cupy_array_equal()\n    def test_arange_no_dtype_int(self, xp):\n        return xp.arange(1, 11, 2)\n\n    @testing.numpy_cupy_array_equal()\n    def test_arange_no_dtype_float(self, xp):\n        return xp.arange(1.0, 11.0, 2.0)\n\n    @testing.numpy_cupy_array_equal()\n    def test_arange_negative_size(self, xp):\n        return xp.arange(3, 1)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_linspace(self, xp, dtype):\n        return xp.linspace(0, 10, 5, dtype=dtype)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_linspace2(self, xp, dtype):\n        return xp.linspace(10, 0, 5, dtype=dtype)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_linspace_zero_num(self, xp, dtype):\n        return xp.linspace(0, 10, 0, dtype=dtype)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_linspace_zero_num_no_endopoint_with_retstep(self, xp, dtype):\n        x, step = xp.linspace(0, 10, 0, dtype=dtype, endpoint=False,\n                              retstep=True)\n        self.assertTrue(math.isnan(step))\n        return x\n\n    @testing.with_requires('numpy>=1.18')\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_linspace_one_num_no_endopoint_with_retstep(self, xp, dtype):\n        start, stop = 3, 7\n        x, step = xp.linspace(start, stop, 1, dtype=dtype, endpoint=False,\n                              retstep=True)\n        self.assertEqual(step, stop - start)\n        return x\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_linspace_one_num(self, xp, dtype):\n        return xp.linspace(0, 2, 1, dtype=dtype)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_linspace_no_endpoint(self, xp, dtype):\n        return xp.linspace(0, 10, 5, dtype=dtype, endpoint=False)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_linspace_with_retstep(self, xp, dtype):\n        x, step = xp.linspace(0, 10, 5, dtype=dtype, retstep=True)\n        self.assertEqual(step, 2.5)\n        return x\n\n    @testing.numpy_cupy_allclose()\n    def test_linspace_no_dtype_int(self, xp):\n        return xp.linspace(0, 10)\n\n    @testing.numpy_cupy_allclose()\n    def test_linspace_no_dtype_float(self, xp):\n        return xp.linspace(0.0, 10.0)\n\n    @testing.numpy_cupy_allclose()\n    def test_linspace_float_args_with_int_dtype(self, xp):\n        return xp.linspace(0.1, 9.1, 11, dtype=int)\n\n    def test_linspace_neg_num(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.linspace(0, 10, -1)\n\n    @testing.numpy_cupy_allclose()\n    def test_linspace_float_overflow(self, xp):\n        return xp.linspace(0., sys.float_info.max / 5, 10, dtype=float)\n\n    @testing.numpy_cupy_array_equal()\n    def test_linspace_float_underflow(self, xp):\n        # find minimum subnormal number\n        x = sys.float_info.min\n        while x / 2 > 0:\n            x /= 2\n        return xp.linspace(0., x, 10, dtype=float)\n\n    @testing.with_requires('numpy>=1.16')\n    @testing.for_all_dtypes_combination(names=('dtype_range', 'dtype_out'),\n                                        no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_linspace_array_start_stop(self, xp, dtype_range, dtype_out):\n        start = xp.array([0, 120], dtype=dtype_range)\n        stop = xp.array([100, 0], dtype=dtype_range)\n        return xp.linspace(start, stop, num=50, dtype=dtype_out)\n\n    @testing.with_requires('numpy>=1.16')\n    @testing.for_all_dtypes_combination(names=('dtype_range', 'dtype_out'),\n                                        no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_linspace_mixed_start_stop(self, xp, dtype_range, dtype_out):\n        start = 0.0\n        if xp.dtype(dtype_range).kind in 'u':\n            stop = xp.array([100, 16], dtype=dtype_range)\n        else:\n            stop = xp.array([100, -100], dtype=dtype_range)\n        return xp.linspace(start, stop, num=50, dtype=dtype_out)\n\n    @testing.with_requires('numpy>=1.16')\n    @testing.for_all_dtypes_combination(names=('dtype_range', 'dtype_out'),\n                                        no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_linspace_mixed_start_stop2(self, xp, dtype_range, dtype_out):\n        if xp.dtype(dtype_range).kind in 'u':\n            start = xp.array([160, 120], dtype=dtype_range)\n        else:\n            start = xp.array([-120, 120], dtype=dtype_range)\n        stop = 0\n        return xp.linspace(start, stop, num=50, dtype=dtype_out)\n\n    @testing.with_requires('numpy>=1.16')\n    @testing.for_all_dtypes_combination(names=('dtype_range', 'dtype_out'),\n                                        no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_linspace_array_start_stop_axis1(self, xp, dtype_range, dtype_out):\n        start = xp.array([0, 120], dtype=dtype_range)\n        stop = xp.array([100, 0], dtype=dtype_range)\n        return xp.linspace(start, stop, num=50, dtype=dtype_out, axis=1)\n\n    @testing.with_requires('numpy>=1.16')\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_linspace_complex_start_stop(self, xp, dtype):\n        start = xp.array([0, 120], dtype=dtype)\n        stop = xp.array([100, 0], dtype=dtype)\n        return xp.linspace(start, stop, num=50, dtype=dtype)\n\n    @testing.with_requires('numpy>=1.16')\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_linspace_start_stop_list(self, xp, dtype):\n        start = [0, 0]\n        stop = [100, 16]\n        return xp.linspace(start, stop, num=50, dtype=dtype)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_logspace(self, xp, dtype):\n        return xp.logspace(0, 2, 5, dtype=dtype)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_logspace2(self, xp, dtype):\n        return xp.logspace(2, 0, 5, dtype=dtype)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_logspace_zero_num(self, xp, dtype):\n        return xp.logspace(0, 2, 0, dtype=dtype)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_logspace_one_num(self, xp, dtype):\n        return xp.logspace(0, 2, 1, dtype=dtype)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_logspace_no_endpoint(self, xp, dtype):\n        return xp.logspace(0, 2, 5, dtype=dtype, endpoint=False)\n\n    @testing.numpy_cupy_allclose()\n    def test_logspace_no_dtype_int(self, xp):\n        return xp.logspace(0, 2)\n\n    @testing.numpy_cupy_allclose()\n    def test_logspace_no_dtype_float(self, xp):\n        return xp.logspace(0.0, 2.0)\n\n    @testing.numpy_cupy_allclose()\n    def test_logspace_float_args_with_int_dtype(self, xp):\n        return xp.logspace(0.1, 2.1, 11, dtype=int)\n\n    def test_logspace_neg_num(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.logspace(0, 10, -1)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_logspace_base(self, xp, dtype):\n        return xp.logspace(0, 2, 5, base=2.0, dtype=dtype)\n\n\n@testing.parameterize(\n    *testing.product({\n        'indexing': ['xy', 'ij'],\n        'sparse': [False, True],\n        'copy': [False, True],\n    })\n)\n@testing.gpu\nclass TestMeshgrid(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_meshgrid0(self, dtype):\n        out = cupy.meshgrid(indexing=self.indexing, sparse=self.sparse,\n                            copy=self.copy)\n        assert(out == [])\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test_meshgrid1(self, xp, dtype):\n        x = xp.arange(2).astype(dtype)\n        return xp.meshgrid(x, indexing=self.indexing, sparse=self.sparse,\n                           copy=self.copy)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test_meshgrid2(self, xp, dtype):\n        x = xp.arange(2).astype(dtype)\n        y = xp.arange(3).astype(dtype)\n        return xp.meshgrid(x, y, indexing=self.indexing, sparse=self.sparse,\n                           copy=self.copy)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test_meshgrid3(self, xp, dtype):\n        x = xp.arange(2).astype(dtype)\n        y = xp.arange(3).astype(dtype)\n        z = xp.arange(4).astype(dtype)\n        return xp.meshgrid(x, y, z, indexing=self.indexing, sparse=self.sparse,\n                           copy=self.copy)\n\n\n@testing.gpu\nclass TestMgrid(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_mgrid0(self, xp):\n        return xp.mgrid[0:]\n\n    @testing.numpy_cupy_array_equal()\n    def test_mgrid1(self, xp):\n        return xp.mgrid[-10:10]\n\n    @testing.numpy_cupy_array_equal()\n    def test_mgrid2(self, xp):\n        return xp.mgrid[-10:10:10j]\n\n    @testing.numpy_cupy_array_equal()\n    def test_mgrid3(self, xp):\n        x = xp.zeros(10)[:, None]\n        y = xp.ones(10)[:, None]\n        return xp.mgrid[x:y:10j]\n\n    @testing.numpy_cupy_array_equal()\n    def test_mgrid4(self, xp):\n        # check len(keys) > 1\n        return xp.mgrid[-10:10:10j, -10:10:10j]\n\n    @testing.numpy_cupy_array_equal()\n    def test_mgrid5(self, xp):\n        # check len(keys) > 1\n        x = xp.zeros(10)[:, None]\n        y = xp.ones(10)[:, None]\n        return xp.mgrid[x:y:10j, x:y:10j]\n\n\n@testing.gpu\nclass TestOgrid(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_ogrid0(self, xp):\n        return xp.ogrid[0:]\n\n    @testing.numpy_cupy_array_equal()\n    def test_ogrid1(self, xp):\n        return xp.ogrid[-10:10]\n\n    @testing.numpy_cupy_array_equal()\n    def test_ogrid2(self, xp):\n        return xp.ogrid[-10:10:10j]\n\n    @testing.numpy_cupy_array_equal()\n    def test_ogrid3(self, xp):\n        x = xp.zeros(10)[:, None]\n        y = xp.ones(10)[:, None]\n        return xp.ogrid[x:y:10j]\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_ogrid4(self, xp):\n        # check len(keys) > 1\n        return xp.ogrid[-10:10:10j, -10:10:10j]\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_ogrid5(self, xp):\n        # check len(keys) > 1\n        x = xp.zeros(10)[:, None]\n        y = xp.ones(10)[:, None]\n        return xp.ogrid[x:y:10j, x:y:10j]\n"""
tests/cupy_tests/cuda_tests/__init__.py,0,b''
tests/cupy_tests/cuda_tests/test_compiler.py,0,"b""import pickle\nimport unittest\n\nimport mock\n\nimport cupy\nfrom cupy.cuda import compiler\nfrom cupy import testing\n\n\ndef cuda_version():\n    return cupy.cuda.runtime.runtimeGetVersion()\n\n\n@testing.gpu\nclass TestNvrtcArch(unittest.TestCase):\n    def setUp(self):\n        cupy.util.clear_memo()  # _get_arch result is cached\n\n    def _check_get_arch(self, device_cc, expected_arch):\n        with mock.patch('cupy.cuda.device.Device') as device_class:\n            device_class.return_value.compute_capability = device_cc\n            assert compiler._get_arch() == expected_arch\n        cupy.util.clear_memo()  # _get_arch result is cached\n\n    @unittest.skipUnless(cuda_version() < 9000, 'Requires CUDA 8.x or earlier')\n    def test_get_arch_cuda8(self):\n        self._check_get_arch('37', '37')\n        self._check_get_arch('50', '50')\n        self._check_get_arch('52', '52')\n        self._check_get_arch('53', '53')  # Tegra\n\n    @unittest.skipUnless(9000 <= cuda_version(), 'Requires CUDA 9.x or later')\n    def test_get_arch_cuda9(self):\n        self._check_get_arch('62', '62')  # Tegra\n        self._check_get_arch('70', '70')\n        self._check_get_arch('72', '72')  # Tegra\n\n    @unittest.skipUnless(10010 <= cuda_version(),\n                         'Requires CUDA 10.1 or later')\n    def test_get_arch_cuda101(self):\n        self._check_get_arch('75', '75')\n\n    def _compile(self, arch):\n        compiler.compile_using_nvrtc('', arch=arch)\n\n    @unittest.skipUnless(cuda_version() < 9000, 'Requires CUDA 8.x or earlier')\n    def test_compile_cuda8(self):\n        # This test is intended to detect specification change in NVRTC API.\n\n        # It should not fail.\n        # (Do not test `compute_53` as it is for Tegra.)\n        self._compile('52')\n\n        # It should fail.\n        # (`compute_60` and `compute_61` are not supported by NVRTC in CUDA 8\n        #  but it does not raise error when used.)\n        self.assertRaises(\n            compiler.CompileException, self._compile, '54')\n        self.assertRaises(\n            compiler.CompileException, self._compile, '70')\n\n    @unittest.skipUnless(9000 <= cuda_version(), 'Requires CUDA 9.0 or later')\n    def test_compile_cuda9(self):\n        # This test is intended to detect specification change in NVRTC API.\n\n        # It should not fail.\n        # (Do not test `compute_72` as it is for Tegra.)\n        self._compile('70')\n\n        # It should fail.\n        self.assertRaises(\n            compiler.CompileException, self._compile, '73')\n\n    @unittest.skipUnless(10010 <= cuda_version() < 11000,\n                         'Requires CUDA 10.1 or 10.2')\n    def test_compile_cuda101(self):\n        # This test is intended to detect specification change in NVRTC API.\n\n        # It should not fail.\n        # (Do not test `compute_72` as it is for Tegra.)\n        self._compile('75')\n\n        # It should fail. (compute_80 is not supported until CUDA 11)\n        self.assertRaises(\n            compiler.CompileException, self._compile, '80')\n\n\n@testing.gpu\nclass TestNvrtcStderr(unittest.TestCase):\n\n    def test(self):\n        # An error message contains the file name `kern.cu`\n        with self.assertRaisesRegex(compiler.CompileException, 'kern.cu'):\n            compiler.compile_using_nvrtc('a')\n\n\nclass TestIsValidKernelName(unittest.TestCase):\n\n    def test_valid(self):\n        self.assertTrue(compiler.is_valid_kernel_name('valid_name_1'))\n\n    def test_empty(self):\n        self.assertFalse(compiler.is_valid_kernel_name(''))\n\n    def test_start_with_digit(self):\n        self.assertFalse(compiler.is_valid_kernel_name('0_invalid'))\n\n    def test_new_line(self):\n        self.assertFalse(compiler.is_valid_kernel_name('invalid\\nname'))\n\n    def test_symbol(self):\n        self.assertFalse(compiler.is_valid_kernel_name('invalid$name'))\n\n    def test_space(self):\n        self.assertFalse(compiler.is_valid_kernel_name('invalid name'))\n\n\nclass TestExceptionPicklable(unittest.TestCase):\n\n    def test(self):\n        e1 = compiler.CompileException('msg', 'fn.cu', 'fn', ('-ftz=true',))\n        e2 = pickle.loads(pickle.dumps(e1))\n        assert e1.args == e2.args\n        assert str(e1) == str(e2)\n"""
tests/cupy_tests/cuda_tests/test_cublas.py,0,b'import pickle\nimport unittest\n\nfrom cupy.cuda import cublas\n\n\nclass TestExceptionPicklable(unittest.TestCase):\n\n    def test(self):\n        e1 = cublas.CUBLASError(1)\n        e2 = pickle.loads(pickle.dumps(e1))\n        assert e1.args == e2.args\n        assert str(e1) == str(e2)\n'
tests/cupy_tests/cuda_tests/test_cudnn.py,0,"b""import pickle\nimport unittest\n\ntry:\n    from cupy.cuda import cudnn\n    cudnn_available = True\nexcept Exception:\n    cudnn_available = False\n\n\n@unittest.skipUnless(cudnn_available, 'cuDNN is unavailable')\nclass TestExceptionPicklable(unittest.TestCase):\n\n    def test(self):\n        e1 = cudnn.CuDNNError(1)\n        e2 = pickle.loads(pickle.dumps(e1))\n        assert e1.args == e2.args\n        assert str(e1) == str(e2)\n"""
tests/cupy_tests/cuda_tests/test_cufft.py,0,b'import pickle\nimport unittest\n\nfrom cupy.cuda import cufft\n\n\nclass TestExceptionPicklable(unittest.TestCase):\n\n    def test(self):\n        e1 = cufft.CuFFTError(1)\n        e2 = pickle.loads(pickle.dumps(e1))\n        assert e1.args == e2.args\n        assert str(e1) == str(e2)\n'
tests/cupy_tests/cuda_tests/test_curand.py,0,"b'import pickle\nimport unittest\n\nimport numpy\n\nimport cupy\nfrom cupy.cuda import curand\n\n\nclass TestGenerateNormal(unittest.TestCase):\n\n    def setUp(self):\n        self.generator = curand.createGenerator(\n            curand.CURAND_RNG_PSEUDO_DEFAULT)\n\n    def test_invalid_argument_normal_float(self):\n        out = cupy.empty((1,), dtype=numpy.float32)\n        with self.assertRaises(ValueError):\n            curand.generateNormal(\n                self.generator, out.data.ptr, 1, 0.0, 1.0)\n\n    def test_invalid_argument_normal_double(self):\n        out = cupy.empty((1,), dtype=numpy.float64)\n        with self.assertRaises(ValueError):\n            curand.generateNormalDouble(\n                self.generator, out.data.ptr, 1, 0.0, 1.0)\n\n    def test_invalid_argument_log_normal_float(self):\n        out = cupy.empty((1,), dtype=numpy.float32)\n        with self.assertRaises(ValueError):\n            curand.generateLogNormal(\n                self.generator, out.data.ptr, 1, 0.0, 1.0)\n\n    def test_invalid_argument_log_normal_double(self):\n        out = cupy.empty((1,), dtype=numpy.float64)\n        with self.assertRaises(ValueError):\n            curand.generateLogNormalDouble(\n                self.generator, out.data.ptr, 1, 0.0, 1.0)\n\n\nclass TestExceptionPicklable(unittest.TestCase):\n\n    def test(self):\n        e1 = curand.CURANDError(100)\n        e2 = pickle.loads(pickle.dumps(e1))\n        assert e1.args == e2.args\n        assert str(e1) == str(e2)\n'"
tests/cupy_tests/cuda_tests/test_cusolver.py,0,"b'import pickle\nimport unittest\n\nfrom cupy import cuda\n\n\nclass TestCusolver(unittest.TestCase):\n\n    def test_cusolver_enabled(self):\n        self.assertEqual(cuda.runtime.runtimeGetVersion() >= 8000,\n                         cuda.cusolver_enabled)\n\n\nclass TestExceptionPicklable(unittest.TestCase):\n\n    def test(self):\n        e1 = cuda.cusolver.CUSOLVERError(1)\n        e2 = pickle.loads(pickle.dumps(e1))\n        assert e1.args == e2.args\n        assert str(e1) == str(e2)\n'"
tests/cupy_tests/cuda_tests/test_cusparse.py,0,b'import pickle\nimport unittest\n\nfrom cupy.cuda import cusparse\n\n\nclass TestExceptionPicklable(unittest.TestCase):\n\n    def test(self):\n        e1 = cusparse.CuSparseError(1)\n        e2 = pickle.loads(pickle.dumps(e1))\n        assert e1.args == e2.args\n        assert str(e1) == str(e2)\n'
tests/cupy_tests/cuda_tests/test_cutensor.py,0,"b""import pickle\nimport unittest\n\ntry:\n    from cupy.cuda import cutensor\n    cutensor_available = True\nexcept Exception:\n    cutensor_available = False\n\n\n@unittest.skipUnless(cutensor_available, 'cuTensor is unavailable')\nclass TestExceptionPicklable(unittest.TestCase):\n\n    def test(self):\n        e1 = cutensor.CuTensorError(1)\n        e2 = pickle.loads(pickle.dumps(e1))\n        assert e1.args == e2.args\n        assert str(e1) == str(e2)\n"""
tests/cupy_tests/cuda_tests/test_device.py,0,"b""import re\nimport threading\nimport unittest\n\nimport pytest\n\nfrom cupy import cuda\nfrom cupy import testing\n\n\nclass TestDeviceComparison(unittest.TestCase):\n\n    def check_eq(self, result, obj1, obj2):\n        if result:\n            assert obj1 == obj2\n            assert obj2 == obj1\n            assert not (obj1 != obj2)\n            assert not (obj2 != obj1)\n        else:\n            assert obj1 != obj2\n            assert obj2 != obj1\n            assert not (obj1 == obj2)\n            assert not (obj2 == obj1)\n\n    def test_equality(self):\n        self.check_eq(True, cuda.Device(0), cuda.Device(0))\n        self.check_eq(True, cuda.Device(1), cuda.Device(1))\n        self.check_eq(False, cuda.Device(0), cuda.Device(1))\n        self.check_eq(False, cuda.Device(0), 0)\n        self.check_eq(False, cuda.Device(0), None)\n        self.check_eq(False, cuda.Device(0), object())\n\n    def test_lt_device(self):\n        assert cuda.Device(0) < cuda.Device(1)\n        assert not (cuda.Device(0) < cuda.Device(0))\n        assert not (cuda.Device(1) < cuda.Device(0))\n\n    def test_le_device(self):\n        assert cuda.Device(0) <= cuda.Device(1)\n        assert cuda.Device(0) <= cuda.Device(0)\n        assert not (cuda.Device(1) <= cuda.Device(0))\n\n    def test_gt_device(self):\n        assert not (cuda.Device(0) > cuda.Device(0))\n        assert not (cuda.Device(0) > cuda.Device(0))\n        assert cuda.Device(1) > cuda.Device(0)\n\n    def test_ge_device(self):\n        assert not (cuda.Device(0) >= cuda.Device(1))\n        assert cuda.Device(0) >= cuda.Device(0)\n        assert cuda.Device(1) >= cuda.Device(0)\n\n    def check_comparison_other_type(self, obj1, obj2):\n        with pytest.raises(TypeError):\n            obj1 < obj2\n        with pytest.raises(TypeError):\n            obj1 <= obj2\n        with pytest.raises(TypeError):\n            obj1 > obj2\n        with pytest.raises(TypeError):\n            obj1 >= obj2\n        with pytest.raises(TypeError):\n            obj2 < obj1\n        with pytest.raises(TypeError):\n            obj2 <= obj1\n        with pytest.raises(TypeError):\n            obj2 > obj1\n        with pytest.raises(TypeError):\n            obj2 >= obj1\n\n    def test_comparison_other_type(self):\n        self.check_comparison_other_type(cuda.Device(0), 0)\n        self.check_comparison_other_type(cuda.Device(0), 1)\n        self.check_comparison_other_type(cuda.Device(1), 0)\n        self.check_comparison_other_type(cuda.Device(1), None)\n        self.check_comparison_other_type(cuda.Device(1), object())\n\n\n@testing.gpu\nclass TestDeviceAttributes(unittest.TestCase):\n\n    def test_device_attributes(self):\n        d = cuda.Device()\n        attributes = d.attributes\n        assert isinstance(attributes, dict)\n        assert all(isinstance(a, int) for a in attributes.values())\n        # test a specific attribute that would be present on any supported GPU\n        assert 'MaxThreadsPerBlock' in attributes\n\n    def test_device_attributes_error(self):\n        with pytest.raises(cuda.runtime.CUDARuntimeError):\n            # try to retrieve attributes from a non-existent device\n            cuda.device.Device(cuda.runtime.getDeviceCount()).attributes\n\n\n@testing.gpu\nclass TestDevicePCIBusId(unittest.TestCase):\n    def test_device_get_pci_bus_id(self):\n        d = cuda.Device()\n        pci_bus_id = d.pci_bus_id\n        assert re.match(\n            '^[a-fA-F0-9]{4}:[a-fA-F0-9]{2}:[a-fA-F0-9]{2}.[a-fA-F0-9]',\n            pci_bus_id\n        )\n\n    def test_device_by_pci_bus_id(self):\n        d1 = cuda.Device()\n        d2 = cuda.Device.from_pci_bus_id(d1.pci_bus_id)\n        assert d1 == d2\n        d3 = cuda.Device(d2)\n        assert d2 == d3\n\n        with pytest.raises(cuda.runtime.CUDARuntimeError) as excinfo:\n            cuda.Device.from_pci_bus_id('fake:id')\n            assert excinfo == 'cudaErrorInvalidValue: invalid argument'\n\n        with pytest.raises(cuda.runtime.CUDARuntimeError) as excinfo:\n            cuda.Device.from_pci_bus_id('FFFF:FF:FF.F')\n            assert excinfo == 'cudaErrorInvalidDevice: invalid device ordinal'\n\n\n@testing.gpu\nclass TestDeviceHandles(unittest.TestCase):\n    def _check_handle(self, func):\n        handles = [func(), None, None]\n\n        def _subthread():\n            handles[1] = func()\n            handles[2] = func()\n\n        t = threading.Thread(target=_subthread)\n        t.start()\n        t.join()\n        assert handles[0] is not None\n        assert handles[0] != handles[1]\n        assert handles[1] == handles[2]\n\n    def test_cublas_handle(self):\n        self._check_handle(cuda.get_cublas_handle)\n\n    def test_cusolver_handle(self):\n        self._check_handle(cuda.device.get_cusolver_handle)\n\n    def test_cusolver_sp_handle(self):\n        self._check_handle(cuda.device.get_cublas_handle)\n\n    def test_cusparse_handle(self):\n        self._check_handle(cuda.device.get_cusparse_handle)\n"""
tests/cupy_tests/cuda_tests/test_driver.py,0,"b'import pickle\nimport threading\nimport unittest\n\nimport cupy\nfrom cupy.cuda import driver\n\n\nclass TestDriver(unittest.TestCase):\n    def test_ctxGetCurrent(self):\n        # Make sure to create context.\n        cupy.arange(1)\n        self.assertNotEqual(0, driver.ctxGetCurrent())\n\n    def test_ctxGetCurrent_thread(self):\n        # Make sure to create context in main thread.\n        cupy.arange(1)\n\n        def f(self):\n            self._result0 = driver.ctxGetCurrent()\n            cupy.arange(1)\n            self._result1 = driver.ctxGetCurrent()\n\n        self._result0 = None\n        self._result1 = None\n        t = threading.Thread(target=f, args=(self,))\n        t.daemon = True\n        t.start()\n        t.join()\n\n        # The returned context pointer must be NULL on sub thread\n        # without valid context.\n        self.assertEqual(0, self._result0)\n\n        # After the context is created, it should return the valid\n        # context pointer.\n        self.assertNotEqual(0, self._result1)\n\n\nclass TestExceptionPicklable(unittest.TestCase):\n\n    def test(self):\n        e1 = driver.CUDADriverError(1)\n        e2 = pickle.loads(pickle.dumps(e1))\n        assert e1.args == e2.args\n        assert str(e1) == str(e2)\n'"
tests/cupy_tests/cuda_tests/test_memory.py,0,"b""import ctypes\nimport gc\nimport pickle\nimport threading\nimport unittest\n\nimport fastrlock\n\nimport cupy.cuda\nfrom cupy.cuda import device\nfrom cupy.cuda import memory\nfrom cupy.cuda import stream as stream_module\nfrom cupy import testing\n\n\nclass MockMemory(memory.Memory):\n    cur_ptr = 1\n\n    def __init__(self, size):\n        self.ptr = MockMemory.cur_ptr\n        MockMemory.cur_ptr += size\n        self.size = size\n        self.device_id = 0\n\n    def __del__(self):\n        self.ptr = 0\n        pass\n\n\ndef mock_alloc(size):\n    mem = MockMemory(size)\n    return memory.MemoryPointer(mem, 0)\n\n\nclass TestUnownedMemoryClass(unittest.TestCase):\n\n    def test_inherits_base_memory(self):\n        assert issubclass(memory.UnownedMemory, memory.BaseMemory)\n\n\n@testing.parameterize(*testing.product({\n    'allocator': [memory._malloc, memory.malloc_managed],\n    'specify_device_id': [True, False],\n}))\n@testing.gpu\nclass TestUnownedMemory(unittest.TestCase):\n\n    def check(self, device_id):\n        size = 24\n        shape = (2, 3)\n        dtype = cupy.float32\n        with device.Device(device_id):\n            src_mem_ptr = self.allocator(size)\n        src_ptr = src_mem_ptr.ptr\n\n        args = (src_ptr, size, src_mem_ptr)\n        kwargs = {}\n        if self.specify_device_id:\n            kwargs = {'device_id': device_id}\n\n        unowned_mem = memory.UnownedMemory(*args, **kwargs)\n        assert unowned_mem.size == size\n        assert unowned_mem.ptr == src_ptr\n        assert unowned_mem.device_id == device_id\n\n        arr = cupy.ndarray(shape, dtype, memory.MemoryPointer(unowned_mem, 0))\n\n        # Delete the source object\n        del src_mem_ptr\n\n        with device.Device(device_id):\n            arr[:] = 2\n            assert (arr == 2).all()\n\n    def test_device0(self):\n        self.check(0)\n\n    @testing.multi_gpu(2)\n    def test_device1(self):\n        self.check(1)\n\n\n@testing.gpu\nclass TestMemoryPointer(unittest.TestCase):\n\n    def test_int(self):\n        pval = MockMemory.cur_ptr\n        memptr = mock_alloc(1)\n        self.assertEqual(pval, int(memptr))\n\n    def test_add(self):\n        pval = MockMemory.cur_ptr\n        memptr = mock_alloc(8)\n\n        memptr2 = memptr + 4\n        self.assertIsInstance(memptr2, memory.MemoryPointer)\n        self.assertEqual(pval + 4, int(memptr2))\n\n        memptr3 = 4 + memptr\n        self.assertIsInstance(memptr3, memory.MemoryPointer)\n        self.assertEqual(pval + 4, int(memptr3))\n\n        memptr += 4\n        self.assertIsInstance(memptr, memory.MemoryPointer)\n        self.assertEqual(pval + 4, int(memptr))\n\n    def test_sub(self):\n        pval = MockMemory.cur_ptr\n        memptr = mock_alloc(8) + 4\n\n        memptr2 = memptr - 4\n        self.assertIsInstance(memptr2, memory.MemoryPointer)\n        self.assertEqual(pval, int(memptr2))\n\n        memptr -= 4\n        self.assertIsInstance(memptr, memory.MemoryPointer)\n        self.assertEqual(pval, int(memptr))\n\n    def test_copy_to_and_from_host(self):\n        a_gpu = memory.alloc(4)\n        a_cpu = ctypes.c_int(100)\n        a_gpu.copy_from(ctypes.cast(ctypes.byref(a_cpu), ctypes.c_void_p), 4)\n        b_cpu = ctypes.c_int()\n        a_gpu.copy_to_host(\n            ctypes.cast(ctypes.byref(b_cpu), ctypes.c_void_p), 4)\n        self.assertEqual(b_cpu.value, a_cpu.value)\n\n    def test_copy_from_device(self):\n        a_gpu = memory.alloc(4)\n        a_cpu = ctypes.c_int(100)\n        a_gpu.copy_from(ctypes.cast(ctypes.byref(a_cpu), ctypes.c_void_p), 4)\n\n        b_gpu = memory.alloc(4)\n        b_gpu.copy_from(a_gpu, 4)\n        b_cpu = ctypes.c_int()\n        b_gpu.copy_to_host(\n            ctypes.cast(ctypes.byref(b_cpu), ctypes.c_void_p), 4)\n        self.assertEqual(b_cpu.value, a_cpu.value)\n\n    def test_memset(self):\n        a_gpu = memory.alloc(4)\n        a_gpu.memset(1, 4)\n        a_cpu = ctypes.c_ubyte()\n        for i in range(4):\n            a_gpu.copy_to_host(\n                ctypes.cast(ctypes.byref(a_cpu), ctypes.c_void_p), 1)\n            self.assertEqual(a_cpu.value, 1)\n            a_gpu += 1\n\n\n# -----------------------------------------------------------------------------\n# Memory pool\n\n@testing.gpu\nclass TestSingleDeviceMemoryPool(unittest.TestCase):\n\n    def setUp(self):\n        self.pool = memory.SingleDeviceMemoryPool(allocator=mock_alloc)\n        self.unit = memory._allocation_unit_size\n        self.stream = stream_module.Stream()\n        self.stream_ptr = self.stream.ptr\n\n    def test_round_size(self):\n        self.assertEqual(memory._round_size(self.unit - 1), self.unit)\n        self.assertEqual(memory._round_size(self.unit), self.unit)\n        self.assertEqual(memory._round_size(self.unit + 1), self.unit * 2)\n\n    def test_bin_index_from_size(self):\n        self.assertEqual(memory._bin_index_from_size(self.unit - 1), 0)\n        self.assertEqual(memory._bin_index_from_size(self.unit), 0)\n        self.assertEqual(memory._bin_index_from_size(self.unit + 1), 1)\n\n    def test_split(self):\n        mem = MockMemory(self.unit * 4)\n        chunk = memory._Chunk(mem, 0, mem.size, self.stream_ptr)\n        tail = chunk.split(self.unit * 2)\n        self.assertEqual(chunk.ptr(), mem.ptr)\n        self.assertEqual(chunk.offset, 0)\n        self.assertEqual(chunk.size, self.unit * 2)\n        self.assertEqual(chunk.prev, None)\n        self.assertEqual(chunk.next.ptr(), tail.ptr())\n        self.assertEqual(chunk.stream_ptr, self.stream_ptr)\n        self.assertEqual(tail.ptr(), mem.ptr + self.unit * 2)\n        self.assertEqual(tail.offset, self.unit * 2)\n        self.assertEqual(tail.size, self.unit * 2)\n        self.assertEqual(tail.prev.ptr(), chunk.ptr())\n        self.assertEqual(tail.next, None)\n        self.assertEqual(tail.stream_ptr, self.stream_ptr)\n\n        tail_of_head = chunk.split(self.unit)\n        self.assertEqual(chunk.ptr(), mem.ptr)\n        self.assertEqual(chunk.offset, 0)\n        self.assertEqual(chunk.size, self.unit)\n        self.assertEqual(chunk.prev, None)\n        self.assertEqual(chunk.next.ptr(), tail_of_head.ptr())\n        self.assertEqual(chunk.stream_ptr, self.stream_ptr)\n        self.assertEqual(tail_of_head.ptr(), mem.ptr + self.unit)\n        self.assertEqual(tail_of_head.offset, self.unit)\n        self.assertEqual(tail_of_head.size, self.unit)\n        self.assertEqual(tail_of_head.prev.ptr(), chunk.ptr())\n        self.assertEqual(tail_of_head.next.ptr(), tail.ptr())\n        self.assertEqual(tail_of_head.stream_ptr, self.stream_ptr)\n\n        tail_of_tail = tail.split(self.unit)\n        self.assertEqual(tail.ptr(), chunk.ptr() + self.unit * 2)\n        self.assertEqual(tail.offset, self.unit * 2)\n        self.assertEqual(tail.size, self.unit)\n        self.assertEqual(tail.prev.ptr(), tail_of_head.ptr())\n        self.assertEqual(tail.next.ptr(), tail_of_tail.ptr())\n        self.assertEqual(tail.stream_ptr, self.stream_ptr)\n        self.assertEqual(tail_of_tail.ptr(), mem.ptr + self.unit * 3)\n        self.assertEqual(tail_of_tail.offset, self.unit * 3)\n        self.assertEqual(tail_of_tail.size, self.unit)\n        self.assertEqual(tail_of_tail.prev.ptr(), tail.ptr())\n        self.assertEqual(tail_of_tail.next, None)\n        self.assertEqual(tail_of_tail.stream_ptr, self.stream_ptr)\n\n    def test_merge(self):\n        mem = MockMemory(self.unit * 4)\n        chunk = memory._Chunk(mem, 0, mem.size, self.stream_ptr)\n        chunk_ptr = chunk.ptr()\n        chunk_offset = chunk.offset\n        chunk_size = chunk.size\n\n        tail = chunk.split(self.unit * 2)\n        head = chunk\n        head_ptr = head.ptr()\n        head_offset = head.offset\n        head_size = head.size\n        tail_ptr = tail.ptr()\n        tail_offset = tail.offset\n        tail_size = tail.size\n\n        tail_of_head = head.split(self.unit)\n        tail_of_tail = tail.split(self.unit)\n\n        head.merge(tail_of_head)\n        self.assertEqual(head.ptr(), head_ptr)\n        self.assertEqual(head.offset, head_offset)\n        self.assertEqual(head.size, head_size)\n        self.assertEqual(head.prev, None)\n        self.assertEqual(head.next.ptr(), tail_ptr)\n        self.assertEqual(head.stream_ptr, self.stream_ptr)\n\n        tail.merge(tail_of_tail)\n        self.assertEqual(tail.ptr(), tail_ptr)\n        self.assertEqual(tail.offset, tail_offset)\n        self.assertEqual(tail.size, tail_size)\n        self.assertEqual(tail.prev.ptr(), head_ptr)\n        self.assertEqual(tail.next, None)\n        self.assertEqual(tail.stream_ptr, self.stream_ptr)\n\n        head.merge(tail)\n        self.assertEqual(head.ptr(), chunk_ptr)\n        self.assertEqual(head.offset, chunk_offset)\n        self.assertEqual(head.size, chunk_size)\n        self.assertEqual(head.prev, None)\n        self.assertEqual(head.next, None)\n        self.assertEqual(head.stream_ptr, self.stream_ptr)\n\n    def test_alloc(self):\n        p1 = self.pool.malloc(self.unit * 4)\n        p2 = self.pool.malloc(self.unit * 4)\n        p3 = self.pool.malloc(self.unit * 8)\n        self.assertNotEqual(p1.ptr, p2.ptr)\n        self.assertNotEqual(p1.ptr, p3.ptr)\n        self.assertNotEqual(p2.ptr, p3.ptr)\n\n    def test_alloc_split(self):\n        p = self.pool.malloc(self.unit * 4)\n        ptr = p.ptr\n        del p\n        head = self.pool.malloc(self.unit * 2)\n        tail = self.pool.malloc(self.unit * 2)\n        self.assertEqual(ptr, head.ptr)\n        self.assertEqual(ptr + self.unit * 2, tail.ptr)\n\n    def test_alloc_limit(self):\n        self.pool.set_limit(size=(self.unit * 6))\n\n        p1 = self.pool.malloc(self.unit * 5)\n        p2 = self.pool.malloc(self.unit * 1)\n        with self.assertRaises(memory.OutOfMemoryError):\n            self.pool.malloc(self.unit)\n\n        self.pool.set_limit(size=(self.unit * 7))\n        p3 = self.pool.malloc(self.unit)\n        del p1, p2, p3\n\n    def test_free(self):\n        p1 = self.pool.malloc(self.unit * 4)\n        ptr1 = p1.ptr\n        del p1\n        p2 = self.pool.malloc(self.unit * 4)\n        self.assertEqual(ptr1, p2.ptr)\n\n    def test_free_stream(self):\n        p1 = self.pool.malloc(self.unit * 4)\n        ptr1 = p1.ptr\n        del p1\n        with self.stream:\n            p2 = self.pool.malloc(self.unit * 4)\n        self.assertNotEqual(ptr1, p2.ptr)\n\n    def test_free_merge(self):\n        p = self.pool.malloc(self.unit * 4)\n        ptr = p.ptr\n        del p\n\n        # merge head into tail\n        head = self.pool.malloc(self.unit * 2)\n        tail = self.pool.malloc(self.unit * 2)\n        self.assertEqual(ptr, head.ptr)\n        del tail\n        del head\n        p = self.pool.malloc(self.unit * 4)\n        self.assertEqual(ptr, p.ptr)\n        del p\n\n        # merge tail into head\n        head = self.pool.malloc(self.unit * 2)\n        tail = self.pool.malloc(self.unit * 2)\n        self.assertEqual(ptr, head.ptr)\n        del head\n        del tail\n        p = self.pool.malloc(self.unit * 4)\n        self.assertEqual(ptr, p.ptr)\n        del p\n\n    def test_free_different_size(self):\n        p1 = self.pool.malloc(self.unit * 4)\n        ptr1 = p1.ptr\n        del p1\n        p2 = self.pool.malloc(self.unit * 8)\n        self.assertNotEqual(ptr1, p2.ptr)\n\n    def test_free_all_blocks(self):\n        p1 = self.pool.malloc(self.unit * 4)\n        ptr1 = p1.ptr\n        del p1\n        self.pool.free_all_blocks()\n        p2 = self.pool.malloc(self.unit * 4)\n        self.assertNotEqual(ptr1, p2.ptr)\n        del p2\n\n    def test_free_all_blocks_split(self):\n        # do not free splitted blocks\n        p = self.pool.malloc(self.unit * 4)\n        del p\n        head = self.pool.malloc(self.unit * 2)\n        tail = self.pool.malloc(self.unit * 2)\n        tailptr = tail.ptr\n        del tail\n        self.pool.free_all_blocks()\n        p = self.pool.malloc(self.unit * 2)\n        self.assertEqual(tailptr, p.ptr)\n        del head\n\n    def test_free_all_blocks_stream(self):\n        p1 = self.pool.malloc(self.unit * 4)\n        ptr1 = p1.ptr\n        del p1\n        with self.stream:\n            p2 = self.pool.malloc(self.unit * 4)\n            ptr2 = p2.ptr\n            del p2\n        self.pool.free_all_blocks(stream=stream_module.Stream.null)\n        p3 = self.pool.malloc(self.unit * 4)\n        self.assertNotEqual(ptr1, p3.ptr)\n        self.assertNotEqual(ptr2, p3.ptr)\n        with self.stream:\n            p4 = self.pool.malloc(self.unit * 4)\n            self.assertNotEqual(ptr1, p4.ptr)\n            self.assertEqual(ptr2, p4.ptr)\n\n    def test_free_all_blocks_all_streams(self):\n        p1 = self.pool.malloc(self.unit * 4)\n        ptr1 = p1.ptr\n        del p1\n        with self.stream:\n            p2 = self.pool.malloc(self.unit * 4)\n            ptr2 = p2.ptr\n            del p2\n        self.pool.free_all_blocks()\n        p3 = self.pool.malloc(self.unit * 4)\n        self.assertNotEqual(ptr1, p3.ptr)\n        self.assertNotEqual(ptr2, p3.ptr)\n        with self.stream:\n            p4 = self.pool.malloc(self.unit * 4)\n            self.assertNotEqual(ptr1, p4.ptr)\n            self.assertNotEqual(ptr2, p4.ptr)\n\n    def test_free_all_free(self):\n        p1 = self.pool.malloc(self.unit * 4)\n        ptr1 = p1.ptr\n        del p1\n        with testing.assert_warns(DeprecationWarning):\n            self.pool.free_all_free()\n        p2 = self.pool.malloc(self.unit * 4)\n        self.assertNotEqual(ptr1, p2.ptr)\n\n    def test_used_bytes(self):\n        p1 = self.pool.malloc(self.unit * 2)\n        self.assertEqual(self.unit * 2, self.pool.used_bytes())\n        p2 = self.pool.malloc(self.unit * 4)\n        self.assertEqual(self.unit * 6, self.pool.used_bytes())\n        del p2\n        self.assertEqual(self.unit * 2, self.pool.used_bytes())\n        del p1\n        self.assertEqual(self.unit * 0, self.pool.used_bytes())\n        p3 = self.pool.malloc(self.unit * 1)\n        self.assertEqual(self.unit * 1, self.pool.used_bytes())\n        del p3\n\n    def test_used_bytes_stream(self):\n        p1 = self.pool.malloc(self.unit * 4)\n        del p1\n        with self.stream:\n            p2 = self.pool.malloc(self.unit * 2)\n        self.assertEqual(self.unit * 2, self.pool.used_bytes())\n        del p2\n\n    def test_free_bytes(self):\n        p1 = self.pool.malloc(self.unit * 2)\n        self.assertEqual(self.unit * 0, self.pool.free_bytes())\n        p2 = self.pool.malloc(self.unit * 4)\n        self.assertEqual(self.unit * 0, self.pool.free_bytes())\n        del p2\n        self.assertEqual(self.unit * 4, self.pool.free_bytes())\n        del p1\n        self.assertEqual(self.unit * 6, self.pool.free_bytes())\n        p3 = self.pool.malloc(self.unit * 1)\n        self.assertEqual(self.unit * 5, self.pool.free_bytes())\n        del p3\n\n    def test_free_bytes_stream(self):\n        p1 = self.pool.malloc(self.unit * 4)\n        del p1\n        with self.stream:\n            p2 = self.pool.malloc(self.unit * 2)\n        self.assertEqual(self.unit * 4, self.pool.free_bytes())\n        del p2\n\n    def test_total_bytes(self):\n        p1 = self.pool.malloc(self.unit * 2)\n        self.assertEqual(self.unit * 2, self.pool.total_bytes())\n        p2 = self.pool.malloc(self.unit * 4)\n        self.assertEqual(self.unit * 6, self.pool.total_bytes())\n        del p1\n        self.assertEqual(self.unit * 6, self.pool.total_bytes())\n        del p2\n        self.assertEqual(self.unit * 6, self.pool.total_bytes())\n        p3 = self.pool.malloc(self.unit * 1)\n        self.assertEqual(self.unit * 6, self.pool.total_bytes())\n\n        self.assertEqual(\n            self.pool.used_bytes() + self.pool.free_bytes(),\n            self.pool.total_bytes())\n\n        del p3\n\n        self.pool.free_all_blocks()\n        self.assertEqual(0, self.pool.total_bytes())\n\n    def test_total_bytes_stream(self):\n        p1 = self.pool.malloc(self.unit * 4)\n        del p1\n        with self.stream:\n            p2 = self.pool.malloc(self.unit * 2)\n        self.assertEqual(self.unit * 6, self.pool.total_bytes())\n        del p2\n\n    def test_get_limit(self):\n        # limit is disabled by default\n        self.assertEqual(0, self.pool.get_limit())\n\n    def test_set_limit_size(self):\n        self.pool.set_limit(size=1024)\n        self.assertEqual(1024, self.pool.get_limit())\n\n        self.pool.set_limit(size=2**33)\n        self.assertEqual(2**33, self.pool.get_limit())\n\n        self.pool.set_limit(size=0)\n        self.assertEqual(0, self.pool.get_limit())\n\n        with self.assertRaises(ValueError):\n            self.pool.set_limit(size=-1)\n\n    def test_set_limit_fraction(self):\n        _, total = cupy.cuda.runtime.memGetInfo()\n\n        self.pool.set_limit(fraction=0)\n        self.assertEqual(0, self.pool.get_limit())\n\n        self.pool.set_limit(fraction=0.5)\n        self.assertEqual(total * 0.5, self.pool.get_limit())\n\n        self.pool.set_limit(fraction=1.0)\n        self.assertEqual(total, self.pool.get_limit())\n\n        with self.assertRaises(ValueError):\n            self.pool.set_limit(fraction=-1)\n\n        with self.assertRaises(ValueError):\n            self.pool.set_limit(fraction=1.1)\n\n    def test_parse_limit_string(self):\n        parse_limit_string = self.pool._parse_limit_string\n\n        # size\n        param = parse_limit_string('0')\n        self.assertEqual(0, param['size'])\n        self.assertEqual(None, param['fraction'])\n\n        param = parse_limit_string('1073741824')\n        self.assertEqual(1073741824, param['size'])\n        self.assertEqual(None, param['fraction'])\n\n        # fraction\n        param = parse_limit_string('0%')\n        self.assertEqual(None, param['size'])\n        self.assertEqual(0.0, param['fraction'])\n\n        param = parse_limit_string('40%')\n        self.assertEqual(None, param['size'])\n        self.assertEqual(0.4, param['fraction'])\n\n        param = parse_limit_string('70.5%')\n        self.assertEqual(None, param['size'])\n        self.assertEqual(0.705, param['fraction'])\n\n        param = parse_limit_string('100%')\n        self.assertEqual(None, param['size'])\n        self.assertEqual(1.0, param['fraction'])\n\n\n@testing.parameterize(*testing.product({\n    'allocator': [memory._malloc, memory.malloc_managed],\n}))\n@testing.gpu\nclass TestMemoryPool(unittest.TestCase):\n\n    def setUp(self):\n        self.pool = memory.MemoryPool(self.allocator)\n\n    def test_zero_size_alloc(self):\n        with cupy.cuda.Device(0):\n            mem = self.pool.malloc(0).mem\n            self.assertIsInstance(mem, memory.Memory)\n            self.assertNotIsInstance(mem, memory.PooledMemory)\n\n    def test_double_free(self):\n        with cupy.cuda.Device(0):\n            mem = self.pool.malloc(1).mem\n            mem.free()\n            mem.free()\n\n    def test_free_all_blocks(self):\n        with cupy.cuda.Device(0):\n            mem = self.pool.malloc(1).mem\n            self.assertIsInstance(mem, memory.BaseMemory)\n            self.assertIsInstance(mem, memory.PooledMemory)\n            self.assertEqual(self.pool.n_free_blocks(), 0)\n            mem.free()\n            self.assertEqual(self.pool.n_free_blocks(), 1)\n            self.pool.free_all_blocks()\n            self.assertEqual(self.pool.n_free_blocks(), 0)\n\n    def test_free_all_blocks_without_malloc(self):\n        with cupy.cuda.Device(0):\n            # call directly without malloc.\n            self.pool.free_all_blocks()\n            self.assertEqual(self.pool.n_free_blocks(), 0)\n\n    def test_free_all_free(self):\n        with cupy.cuda.Device(0):\n            mem = self.pool.malloc(1).mem\n            self.assertIsInstance(mem, memory.BaseMemory)\n            self.assertIsInstance(mem, memory.PooledMemory)\n            self.assertEqual(self.pool.n_free_blocks(), 0)\n            mem.free()\n            self.assertEqual(self.pool.n_free_blocks(), 1)\n            with testing.assert_warns(DeprecationWarning):\n                self.pool.free_all_free()\n            self.assertEqual(self.pool.n_free_blocks(), 0)\n\n    def test_free_all_free_without_malloc(self):\n        with cupy.cuda.Device(0):\n            # call directly without malloc.\n            with testing.assert_warns(DeprecationWarning):\n                self.pool.free_all_free()\n            self.assertEqual(self.pool.n_free_blocks(), 0)\n\n    def test_n_free_blocks_without_malloc(self):\n        with cupy.cuda.Device(0):\n            # call directly without malloc/free_all_free.\n            self.assertEqual(self.pool.n_free_blocks(), 0)\n\n    def test_used_bytes(self):\n        with cupy.cuda.Device(0):\n            self.assertEqual(0, self.pool.used_bytes())\n\n    def test_free_bytes(self):\n        with cupy.cuda.Device(0):\n            self.assertEqual(0, self.pool.free_bytes())\n\n    def test_total_bytes(self):\n        with cupy.cuda.Device(0):\n            self.assertEqual(0, self.pool.total_bytes())\n\n\n@testing.gpu\nclass TestAllocator(unittest.TestCase):\n\n    def setUp(self):\n        self.old_pool = cupy.get_default_memory_pool()\n        self.pool = memory.MemoryPool()\n        memory.set_allocator(self.pool.malloc)\n\n    def tearDown(self):\n        self.pool.free_all_blocks()\n        memory.set_allocator(self.old_pool.malloc)\n\n    def test_set_allocator(self):\n        with cupy.cuda.Device(0):\n            self.assertEqual(0, self.pool.used_bytes())\n            arr = cupy.arange(128, dtype=cupy.int64)\n            self.assertEqual(1024, arr.data.mem.size)\n            self.assertEqual(1024, self.pool.used_bytes())\n\n    def test_get_allocator(self):\n        assert memory.get_allocator() == self.pool.malloc\n\n    def test_allocator_context_manager(self):\n        new_pool = memory.MemoryPool()\n        with cupy.cuda.using_allocator(new_pool.malloc):\n            assert memory.get_allocator() == new_pool.malloc\n        assert memory.get_allocator() == self.pool.malloc\n\n    def test_set_allocator_cm(self):\n        new_pool = memory.MemoryPool()\n        new_pool2 = memory.MemoryPool()\n        with cupy.cuda.using_allocator(new_pool.malloc):\n            with self.assertRaises(ValueError):\n                memory.set_allocator(new_pool2.malloc)\n\n    def test_allocator_nested_context_manager(self):\n        new_pool = memory.MemoryPool()\n        with cupy.cuda.using_allocator(new_pool.malloc):\n            new_pool2 = memory.MemoryPool()\n            assert memory.get_allocator() == new_pool.malloc\n            with cupy.cuda.using_allocator(new_pool2.malloc):\n                assert memory.get_allocator() == new_pool2.malloc\n            assert memory.get_allocator() == new_pool.malloc\n        assert memory.get_allocator() == self.pool.malloc\n\n    def test_allocator_thread_local(self):\n        def thread_body(self):\n            new_pool = memory.MemoryPool()\n            with cupy.cuda.using_allocator(new_pool.malloc):\n                assert memory.get_allocator() == new_pool.malloc\n                threading.Barrier(2)\n                arr = cupy.zeros(128, dtype=cupy.int64)\n                threading.Barrier(2)\n                self.assertEqual(arr.data.mem.size, new_pool.used_bytes())\n                threading.Barrier(2)\n            assert memory.get_allocator() == self.pool.malloc\n\n        with cupy.cuda.Device(0):\n            t = threading.Thread(target=thread_body, args=(self,))\n            t.daemon = True\n            t.start()\n            threading.Barrier(2)\n            assert memory.get_allocator() == self.pool.malloc\n            arr = cupy.ones(256, dtype=cupy.int64)\n            threading.Barrier(2)\n            self.assertEqual(arr.data.mem.size, self.pool.used_bytes())\n            threading.Barrier(2)\n            t.join()\n\n    def test_thread_local_valid(self):\n        new_pool = memory.MemoryPool()\n        arr = None\n        with cupy.cuda.using_allocator(new_pool.malloc):\n            arr = cupy.zeros(128, dtype=cupy.int64)\n            arr += 1\n        # Check that arr and the pool have not ben released\n        self.assertEqual(arr.data.mem.size, new_pool.used_bytes())\n        assert arr.sum() == 128\n\n    def test_reuse_between_thread(self):\n        def job(self):\n            cupy.arange(16)\n            self._error = False\n\n        # Run in main thread.\n        self._error = True\n        job(self)\n        self.assertFalse(self._error)\n\n        # Run in sub thread.\n        self._error = True\n        with cupy.cuda.Device(0):\n            t = threading.Thread(target=job, args=(self,))\n            t.daemon = True\n            t.start()\n            t.join()\n        self.assertFalse(self._error)\n\n\n@testing.gpu\nclass TestAllocatorDisabled(unittest.TestCase):\n\n    def setUp(self):\n        self.pool = cupy.get_default_memory_pool()\n\n    def tearDown(self):\n        memory.set_allocator(self.pool.malloc)\n\n    def _check_pool_not_used(self):\n        used_bytes = self.pool.used_bytes()\n        with cupy.cuda.Device(0):\n            arr = cupy.arange(128, dtype=cupy.int64)\n            self.assertEqual(0, self.pool.used_bytes() - used_bytes)\n            del arr\n\n    def test(self):\n        memory.set_allocator()\n        self._check_pool_not_used()\n\n    def test_none(self):\n        memory.set_allocator(None)\n        self._check_pool_not_used()\n\n\n@testing.gpu\nclass TestMemInfo(unittest.TestCase):\n\n    def test_mem_info(self):\n        d = cupy.cuda.Device()\n        mem_info = d.mem_info\n        assert isinstance(mem_info, tuple)\n        assert len(mem_info) == 2\n        assert all(isinstance(m, int) for m in mem_info)\n        assert all(m > 0 for m in mem_info)\n\n\n@testing.gpu\nclass TestLockAndNoGc(unittest.TestCase):\n\n    def test(self):\n        lock = fastrlock.rlock.FastRLock()\n        ctx = memory.LockAndNoGc(lock)\n\n        assert gc.isenabled()\n        self.assertRaises(Exception, lock.release)\n        with ctx:\n            assert not gc.isenabled()\n            lock.release()\n            lock.acquire()\n        assert gc.isenabled()\n        self.assertRaises(Exception, lock.release)\n\n\nclass TestExceptionPicklable(unittest.TestCase):\n\n    def test(self):\n        e1 = memory.OutOfMemoryError(124, 1024, 1024)\n        e2 = pickle.loads(pickle.dumps(e1))\n        assert e1.args == e2.args\n        assert str(e1) == str(e2)\n"""
tests/cupy_tests/cuda_tests/test_memory_hook.py,0,"b""import unittest\n\nimport cupy.cuda\nfrom cupy.cuda import memory\nfrom cupy.cuda import memory_hook\nfrom cupy import testing\n\n\nclass SimpleMemoryHook(memory_hook.MemoryHook):\n    name = 'SimpleMemoryHook'\n\n    def __init__(self):\n        self.alloc_preprocess_history = []\n        self.alloc_postprocess_history = []\n        self.malloc_preprocess_history = []\n        self.malloc_postprocess_history = []\n        self.free_preprocess_history = []\n        self.free_postprocess_history = []\n\n    def alloc_preprocess(self, **kwargs):\n        self.alloc_preprocess_history.append(kwargs)\n\n    def alloc_postprocess(self, **kwargs):\n        self.alloc_postprocess_history.append(kwargs)\n\n    def malloc_preprocess(self, **kwargs):\n        self.malloc_preprocess_history.append(kwargs)\n\n    def malloc_postprocess(self, **kwargs):\n        self.malloc_postprocess_history.append(kwargs)\n\n    def free_preprocess(self, **kwargs):\n        self.free_preprocess_history.append(kwargs)\n\n    def free_postprocess(self, **kwargs):\n        self.free_postprocess_history.append(kwargs)\n\n\n@testing.gpu\nclass TestMemoryHook(unittest.TestCase):\n\n    def setUp(self):\n        self.pool = memory.MemoryPool()\n        self.unit = 512\n\n    def test_hook(self):\n        hook = SimpleMemoryHook()\n        with cupy.cuda.Device(0):\n            with hook:\n                mem = self.pool.malloc(1)\n                ptr1, pmem1 = (mem.ptr, id(mem.mem))\n                del mem\n                mem = self.pool.malloc(1)\n                ptr2, pmem2 = (mem.ptr, id(mem.mem))\n                del mem\n        self.assertEqual(1, len(hook.alloc_preprocess_history))\n        self.assertEqual(1, len(hook.alloc_postprocess_history))\n        self.assertEqual(2, len(hook.malloc_preprocess_history))\n        self.assertEqual(2, len(hook.malloc_postprocess_history))\n        self.assertEqual(2, len(hook.free_preprocess_history))\n        self.assertEqual(2, len(hook.free_postprocess_history))\n        self.assertEqual({'device_id': 0, 'mem_size': self.unit},\n                         hook.alloc_preprocess_history[0])\n        self.assertEqual({'device_id': 0, 'mem_size': self.unit,\n                          'mem_ptr': ptr1},\n                         hook.alloc_postprocess_history[0])\n        self.assertEqual({'device_id': 0, 'size': 1, 'mem_size': self.unit},\n                         hook.malloc_preprocess_history[0])\n        self.assertEqual({'device_id': 0, 'size': 1, 'mem_size': self.unit,\n                          'mem_ptr': ptr1, 'pmem_id': pmem1},\n                         hook.malloc_postprocess_history[0])\n        self.assertEqual({'device_id': 0, 'size': 1, 'mem_size': self.unit},\n                         hook.malloc_preprocess_history[1])\n        self.assertEqual({'device_id': 0, 'size': 1, 'mem_size': self.unit,\n                          'mem_ptr': ptr2, 'pmem_id': pmem2},\n                         hook.malloc_postprocess_history[1])\n        self.assertEqual({'device_id': 0, 'mem_size': self.unit,\n                          'mem_ptr': ptr1, 'pmem_id': pmem1},\n                         hook.free_preprocess_history[0])\n        self.assertEqual({'device_id': 0, 'mem_size': self.unit,\n                          'mem_ptr': ptr1, 'pmem_id': pmem1},\n                         hook.free_postprocess_history[0])\n        self.assertEqual({'device_id': 0, 'mem_size': self.unit,\n                          'mem_ptr': ptr2, 'pmem_id': pmem2},\n                         hook.free_preprocess_history[1])\n        self.assertEqual({'device_id': 0, 'mem_size': self.unit,\n                          'mem_ptr': ptr2, 'pmem_id': pmem2},\n                         hook.free_postprocess_history[1])\n"""
tests/cupy_tests/cuda_tests/test_nccl.py,0,"b""import pickle\nimport unittest\n\nimport cupy\nfrom cupy import cuda\nfrom cupy.testing import attr\n\n\n@unittest.skipUnless(cuda.nccl_enabled, 'nccl is not installed')\nclass TestNCCL(unittest.TestCase):\n\n    @attr.gpu\n    def test_single_proc_ring(self):\n        id = cuda.nccl.get_unique_id()\n        comm = cuda.nccl.NcclCommunicator(1, id, 0)\n        assert 0 == comm.rank_id()\n        comm.destroy()\n\n    @attr.gpu\n    @unittest.skipUnless(cuda.nccl_enabled and\n                         cuda.nccl.get_version() >= 2400, 'Using old NCCL')\n    def test_abort(self):\n        id = cuda.nccl.get_unique_id()\n        comm = cuda.nccl.NcclCommunicator(1, id, 0)\n        comm.abort()\n\n    @attr.gpu\n    @unittest.skipUnless(cuda.nccl_enabled and\n                         cuda.nccl.get_version() >= 2400, 'Using old NCCL')\n    def test_check_async_error(self):\n        id = cuda.nccl.get_unique_id()\n        comm = cuda.nccl.NcclCommunicator(1, id, 0)\n        comm.check_async_error()\n        comm.destroy()\n\n    @attr.gpu\n    def test_init_all(self):\n        comms = cuda.nccl.NcclCommunicator.initAll(1)\n        for i, comm in enumerate(comms):\n            assert i == comms[i].rank_id()\n        for i, comm in enumerate(comms):\n            comms[i].destroy()\n\n    @attr.gpu\n    @unittest.skipUnless(cuda.nccl_enabled and\n                         cuda.nccl.get_version() >= 2000, 'Using old NCCL')\n    def test_single_proc_single_dev(self):\n        comms = cuda.nccl.NcclCommunicator.initAll(1)\n        cuda.nccl.groupStart()\n        for comm in comms:\n            cuda.Device(comm.device_id()).use()\n            sendbuf = cupy.arange(10)\n            recvbuf = cupy.zeros_like(sendbuf)\n            comm.allReduce(sendbuf.data.ptr, recvbuf.data.ptr, 10,\n                           cuda.nccl.NCCL_INT64, cuda.nccl.NCCL_SUM,\n                           cuda.Stream.null.ptr)\n        cuda.nccl.groupEnd()\n        assert cupy.allclose(sendbuf, recvbuf)\n\n    @attr.gpu\n    def test_comm_size(self):\n        id = cuda.nccl.get_unique_id()\n        comm = cuda.nccl.NcclCommunicator(1, id, 0)\n        assert 1 == comm.size()\n\n\n@unittest.skipUnless(cuda.nccl_enabled, 'nccl is not installed')\nclass TestExceptionPicklable(unittest.TestCase):\n\n    def test(self):\n        e1 = cuda.nccl.NcclError(1)\n        e2 = pickle.loads(pickle.dumps(e1))\n        assert e1.args == e2.args\n        assert str(e1) == str(e2)\n"""
tests/cupy_tests/cuda_tests/test_nvrtc.py,0,b'import pickle\nimport unittest\n\nfrom cupy.cuda import nvrtc\n\n\nclass TestExceptionPicklable(unittest.TestCase):\n\n    def test(self):\n        e1 = nvrtc.NVRTCError(1)\n        e2 = pickle.loads(pickle.dumps(e1))\n        assert e1.args == e2.args\n        assert str(e1) == str(e2)\n'
tests/cupy_tests/cuda_tests/test_nvtx.py,0,"b""import unittest\n\nfrom cupy import cuda\nfrom cupy.testing import attr\n\n\n@unittest.skipUnless(cuda.nvtx_enabled, 'nvtx is not installed')\nclass TestNVTX(unittest.TestCase):\n\n    @attr.gpu\n    def test_Mark(self):\n        cuda.nvtx.Mark('test:Mark', 0)\n\n    @attr.gpu\n    def test_MarkC(self):\n        cuda.nvtx.MarkC('test:MarkC', 0xFF000000)\n\n    @attr.gpu\n    def test_RangePush(self):\n        cuda.nvtx.RangePush('test:RangePush', 1)\n        cuda.nvtx.RangePop()\n\n    @attr.gpu\n    def test_RangePushC(self):\n        cuda.nvtx.RangePushC('test:RangePushC', 0xFF000000)\n        cuda.nvtx.RangePop()\n"""
tests/cupy_tests/cuda_tests/test_pinned_memory.py,0,"b'import unittest\n\nfrom cupy.cuda import pinned_memory\nfrom cupy import testing\n\n\nclass MockMemory(pinned_memory.PinnedMemory):\n    cur_ptr = 1\n\n    def __init__(self, size):\n        self.ptr = MockMemory.cur_ptr\n        MockMemory.cur_ptr += size\n        self.size = size\n\n    def __del__(self):\n        self.ptr = 0\n        pass\n\n\ndef mock_alloc(size):\n    mem = MockMemory(size)\n    return pinned_memory.PinnedMemoryPointer(mem, 0)\n\n\n# -----------------------------------------------------------------------------\n# Memory pointer\n\n@testing.gpu\nclass TestMemoryPointer(unittest.TestCase):\n\n    def test_int(self):\n        pval = MockMemory.cur_ptr\n        memptr = mock_alloc(1)\n        self.assertEqual(pval, int(memptr))\n\n    def test_add(self):\n        pval = MockMemory.cur_ptr\n        memptr = mock_alloc(8)\n\n        memptr2 = memptr + 4\n        self.assertIsInstance(memptr2, pinned_memory.PinnedMemoryPointer)\n        self.assertEqual(pval + 4, int(memptr2))\n\n        memptr3 = 4 + memptr\n        self.assertIsInstance(memptr3, pinned_memory.PinnedMemoryPointer)\n        self.assertEqual(pval + 4, int(memptr3))\n\n        memptr += 4\n        self.assertIsInstance(memptr, pinned_memory.PinnedMemoryPointer)\n        self.assertEqual(pval + 4, int(memptr))\n\n    def test_sub(self):\n        pval = MockMemory.cur_ptr\n        memptr = mock_alloc(8) + 4\n\n        memptr2 = memptr - 4\n        self.assertIsInstance(memptr2, pinned_memory.PinnedMemoryPointer)\n        self.assertEqual(pval, int(memptr2))\n\n        memptr -= 4\n        self.assertIsInstance(memptr, pinned_memory.PinnedMemoryPointer)\n        self.assertEqual(pval, int(memptr))\n\n\n# -----------------------------------------------------------------------------\n# Memory pool\n\n\n@testing.gpu\nclass TestSingleDeviceMemoryPool(unittest.TestCase):\n\n    def setUp(self):\n        self.pool = pinned_memory.PinnedMemoryPool(allocator=mock_alloc)\n\n    def test_alloc(self):\n        p1 = self.pool.malloc(1000)\n        p2 = self.pool.malloc(1000)\n        p3 = self.pool.malloc(2000)\n        self.assertNotEqual(p1.ptr, p2.ptr)\n        self.assertNotEqual(p1.ptr, p3.ptr)\n        self.assertNotEqual(p2.ptr, p3.ptr)\n\n    def test_free(self):\n        p1 = self.pool.malloc(1000)\n        ptr1 = p1.ptr\n        del p1\n        p2 = self.pool.malloc(1000)\n        self.assertEqual(ptr1, p2.ptr)\n\n    def test_free_different_size(self):\n        p1 = self.pool.malloc(1000)\n        ptr1 = p1.ptr\n        del p1\n        p2 = self.pool.malloc(2000)\n        self.assertNotEqual(ptr1, p2.ptr)\n\n    def test_free_all_blocks(self):\n        p1 = self.pool.malloc(1000)\n        ptr1 = p1.ptr\n        del p1\n        self.pool.free_all_blocks()\n        p2 = self.pool.malloc(1000)\n        self.assertNotEqual(ptr1, p2.ptr)\n\n    def test_free_all_blocks2(self):\n        mem = self.pool.malloc(1).mem\n        self.assertIsInstance(mem, pinned_memory.PinnedMemory)\n        self.assertIsInstance(mem, pinned_memory.PooledPinnedMemory)\n        self.assertEqual(self.pool.n_free_blocks(), 0)\n        mem.free()\n        self.assertEqual(self.pool.n_free_blocks(), 1)\n        self.pool.free_all_blocks()\n        self.assertEqual(self.pool.n_free_blocks(), 0)\n\n    def test_zero_size_alloc(self):\n        mem = self.pool.malloc(0).mem\n        self.assertIsInstance(mem, pinned_memory.PinnedMemory)\n        self.assertNotIsInstance(mem, pinned_memory.PooledPinnedMemory)\n\n    def test_double_free(self):\n        mem = self.pool.malloc(1).mem\n        mem.free()\n        mem.free()\n\n    def test_free_all_blocks_without_malloc(self):\n        # call directly without malloc.\n        self.pool.free_all_blocks()\n        self.assertEqual(self.pool.n_free_blocks(), 0)\n\n    def test_n_free_blocks_without_malloc(self):\n        # call directly without malloc/free_all_blocks.\n        self.assertEqual(self.pool.n_free_blocks(), 0)\n'"
tests/cupy_tests/cuda_tests/test_profile.py,0,"b""import unittest\n\nimport mock\n\nfrom cupy import cuda\n\n\nclass TestProfile(unittest.TestCase):\n\n    def test_profile(self):\n        start_patch = mock.patch('cupy.cuda.profiler.start')\n        stop_patch = mock.patch('cupy.cuda.profiler.stop')\n        with start_patch as start, stop_patch as stop:\n            with cuda.profile():\n                pass\n            start.assert_called_once_with()\n            stop.assert_called_once_with()\n\n    def test_err_case(self):\n        start_patch = mock.patch('cupy.cuda.profiler.start')\n        stop_patch = mock.patch('cupy.cuda.profiler.stop')\n        with start_patch as start, stop_patch as stop:\n            try:\n                with cuda.profile():\n                    raise Exception()\n            except Exception:\n                # ignore\n                pass\n            start.assert_called_once_with()\n            stop.assert_called_once_with()\n"""
tests/cupy_tests/cuda_tests/test_stream.py,0,"b'import unittest\n\nfrom cupy.creation import from_data\nfrom cupy import cuda\nfrom cupy import testing\nfrom cupy.testing import attr\n\n\nclass TestStream(unittest.TestCase):\n\n    @attr.gpu\n    def test_eq(self):\n        null0 = cuda.Stream.null\n        null1 = cuda.Stream(True)\n        null2 = cuda.Stream(True)\n        null3 = cuda.Stream()\n\n        self.assertEqual(null0, null1)\n        self.assertEqual(null1, null2)\n        self.assertNotEqual(null2, null3)\n\n    @attr.gpu\n    def test_del(self):\n        stream = cuda.Stream().use()\n        stream_ptr = stream.ptr\n        x = from_data.array([1, 2, 3])\n        del stream\n        self.assertEqual(cuda.Stream.null, cuda.get_current_stream())\n        # Want to test cudaStreamDestory is issued, but\n        # runtime.streamQuery(stream_ptr) causes SEGV. We cannot test...\n        del stream_ptr\n        del x\n\n    @attr.gpu\n    def test_get_and_add_callback(self):\n        N = 100\n        cupy_arrays = [testing.shaped_random((2, 3)) for _ in range(N)]\n\n        stream = cuda.Stream.null\n        out = []\n        for i in range(N):\n            numpy_array = cupy_arrays[i].get(stream=stream)\n            stream.add_callback(\n                lambda _, __, t: out.append(t[0]),\n                (i, numpy_array))\n\n        stream.synchronize()\n        self.assertEqual(out, list(range(N)))\n\n    @attr.gpu\n    def test_with_statement(self):\n        stream1 = cuda.Stream()\n        stream2 = cuda.Stream()\n        self.assertEqual(cuda.Stream.null, cuda.get_current_stream())\n        with stream1:\n            self.assertEqual(stream1, cuda.get_current_stream())\n            with stream2:\n                self.assertEqual(stream2, cuda.get_current_stream())\n            self.assertEqual(stream1, cuda.get_current_stream())\n        self.assertEqual(cuda.Stream.null, cuda.get_current_stream())\n\n    @attr.gpu\n    def test_use(self):\n        stream1 = cuda.Stream().use()\n        self.assertEqual(stream1, cuda.get_current_stream())\n        cuda.Stream.null.use()\n        self.assertEqual(cuda.Stream.null, cuda.get_current_stream())\n\n\nclass TestExternalStream(unittest.TestCase):\n\n    def setUp(self):\n        self.stream_ptr = cuda.runtime.streamCreate()\n        self.stream = cuda.ExternalStream(self.stream_ptr)\n\n    def tearDown(self):\n        cuda.runtime.streamDestroy(self.stream_ptr)\n\n    @attr.gpu\n    def test_get_and_add_callback(self):\n        N = 100\n        cupy_arrays = [testing.shaped_random((2, 3)) for _ in range(N)]\n\n        stream = self.stream\n        out = []\n        for i in range(N):\n            numpy_array = cupy_arrays[i].get(stream=stream)\n            stream.add_callback(\n                lambda _, __, t: out.append(t[0]),\n                (i, numpy_array))\n\n        stream.synchronize()\n        self.assertEqual(out, list(range(N)))\n'"
tests/cupy_tests/cuda_tests/test_texture.py,0,"b'import pytest\nimport unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\nfrom cupy.cuda import runtime\nfrom cupy.cuda.texture import (ChannelFormatDescriptor, CUDAarray,\n                               ResourceDescriptor, TextureDescriptor,\n                               TextureObject, TextureReference,\n                               SurfaceObject)\n\n\ndev = cupy.cuda.Device(runtime.getDevice())\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    \'xp\': (\'numpy\', \'cupy\'),\n    \'stream\': (True, False),\n    \'dimensions\': ((67, 0, 0), (67, 19, 0), (67, 19, 31)),\n    \'n_channels\': (1, 2, 4),\n    \'dtype\': (numpy.float16, numpy.float32, numpy.int8, numpy.int16,\n              numpy.int32, numpy.uint8, numpy.uint16, numpy.uint32),\n}))\nclass TestCUDAarray(unittest.TestCase):\n    def test_array_gen_cpy(self):\n        xp = numpy if self.xp == \'numpy\' else cupy\n        stream = None if not self.stream else cupy.cuda.Stream()\n        width, height, depth = self.dimensions\n        n_channel = self.n_channels\n\n        dim = 3 if depth != 0 else 2 if height != 0 else 1\n        shape = (depth, height, n_channel*width) if dim == 3 else \\\n                (height, n_channel*width) if dim == 2 else \\\n                (n_channel*width,)\n\n        # generate input data and allocate output buffer\n        if self.dtype in (numpy.float16, numpy.float32):\n            arr = xp.random.random(shape).astype(self.dtype)\n            kind = runtime.cudaChannelFormatKindFloat\n        else:  # int\n            # randint() in NumPy <= 1.10 does not have the dtype argument...\n            arr = xp.random.randint(100, size=shape).astype(self.dtype)\n            if self.dtype in (numpy.int8, numpy.int16, numpy.int32):\n                kind = runtime.cudaChannelFormatKindSigned\n            else:\n                kind = runtime.cudaChannelFormatKindUnsigned\n        arr2 = xp.zeros_like(arr)\n\n        assert arr.flags[\'C_CONTIGUOUS\']\n        assert arr2.flags[\'C_CONTIGUOUS\']\n\n        # create a CUDA array\n        ch_bits = [0, 0, 0, 0]\n        for i in range(n_channel):\n            ch_bits[i] = arr.dtype.itemsize*8\n        # unpacking arguments using *ch_bits is not supported before PY35...\n        ch = ChannelFormatDescriptor(ch_bits[0], ch_bits[1], ch_bits[2],\n                                     ch_bits[3], kind)\n        cu_arr = CUDAarray(ch, width, height, depth)\n\n        # copy from input to CUDA array, and back to output\n        cu_arr.copy_from(arr, stream)\n        cu_arr.copy_to(arr2, stream)\n\n        # check input and output are identical\n        if stream is not None:\n            dev.synchronize()\n        assert (arr == arr2).all()\n\n\nsource_texobj = r\'\'\'\nextern ""C""{\n__global__ void copyKernel1Dfetch(float* output,\n                                  cudaTextureObject_t texObj,\n                                  int width)\n{\n    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Read from texture and write to global memory\n    if (x < width)\n        output[x] = tex1Dfetch<float>(texObj, x);\n}\n\n__global__ void copyKernel1D(float* output,\n                             cudaTextureObject_t texObj,\n                             int width)\n{\n    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Read from texture and write to global memory\n    float u = x;\n    if (x < width)\n        output[x] = tex1D<float>(texObj, u);\n}\n\n__global__ void copyKernel2D(float* output,\n                             cudaTextureObject_t texObj,\n                             int width, int height)\n{\n    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n    // Read from texture and write to global memory\n    float u = x;\n    float v = y;\n    if (x < width && y < height)\n        output[y * width + x] = tex2D<float>(texObj, u, v);\n}\n\n__global__ void copyKernel3D(float* output,\n                             cudaTextureObject_t texObj,\n                             int width, int height, int depth)\n{\n    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;\n    unsigned int z = blockIdx.z * blockDim.z + threadIdx.z;\n\n    // Read from texture and write to global memory\n    float u = x;\n    float v = y;\n    float w = z;\n    if (x < width && y < height && z < depth)\n        output[z*width*height+y*width+x] = tex3D<float>(texObj, u, v, w);\n}\n\n__global__ void copyKernel3D_4ch(float* output_x,\n                                 float* output_y,\n                                 float* output_z,\n                                 float* output_w,\n                                 cudaTextureObject_t texObj,\n                                 int width, int height, int depth)\n{\n    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;\n    unsigned int z = blockIdx.z * blockDim.z + threadIdx.z;\n    float4 data;\n\n    // Read from texture, separate channels, and write to global memory\n    float u = x;\n    float v = y;\n    float w = z;\n    if (x < width && y < height && z < depth) {\n        data = tex3D<float4>(texObj, u, v, w);\n        output_x[z*width*height+y*width+x] = data.x;\n        output_y[z*width*height+y*width+x] = data.y;\n        output_z[z*width*height+y*width+x] = data.z;\n        output_w[z*width*height+y*width+x] = data.w;\n    }\n}\n}\n\'\'\'\n\n\nsource_texref = r\'\'\'\nextern ""C""{\ntexture<float, cudaTextureType1D, cudaReadModeElementType> texref1D;\ntexture<float, cudaTextureType2D, cudaReadModeElementType> texref2D;\ntexture<float, cudaTextureType3D, cudaReadModeElementType> texref3D;\ntexture<float4, cudaTextureType3D, cudaReadModeElementType> texref3Df4;\n\n__global__ void copyKernel1Dfetch(float* output,\n                                  int width)\n{\n    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Read from texture and write to global memory\n    if (x < width)\n        output[x] = tex1Dfetch(texref1D, x);\n}\n\n__global__ void copyKernel1D(float* output,\n                             int width)\n{\n    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Read from texture and write to global memory\n    float u = x;\n    if (x < width)\n        output[x] = tex1D(texref1D, u);\n}\n\n__global__ void copyKernel2D(float* output,\n                             int width, int height)\n{\n    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n    // Read from texture and write to global memory\n    float u = x;\n    float v = y;\n    if (x < width && y < height)\n        output[y * width + x] = tex2D(texref2D, u, v);\n}\n\n__global__ void copyKernel3D(float* output,\n                             int width, int height, int depth)\n{\n    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;\n    unsigned int z = blockIdx.z * blockDim.z + threadIdx.z;\n\n    // Read from texture and write to global memory\n    float u = x;\n    float v = y;\n    float w = z;\n    if (x < width && y < height && z < depth)\n        output[z*width*height+y*width+x] = tex3D(texref3D, u, v, w);\n}\n\n__global__ void copyKernel3D_4ch(float* output_x,\n                                 float* output_y,\n                                 float* output_z,\n                                 float* output_w,\n                                 int width, int height, int depth)\n{\n    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;\n    unsigned int z = blockIdx.z * blockDim.z + threadIdx.z;\n    float4 data;\n\n    // Read from texture, separate channels, and write to global memory\n    float u = x;\n    float v = y;\n    float w = z;\n    if (x < width && y < height && z < depth) {\n        data = tex3D(texref3Df4, u, v, w);\n        output_x[z*width*height+y*width+x] = data.x;\n        output_y[z*width*height+y*width+x] = data.y;\n        output_z[z*width*height+y*width+x] = data.z;\n        output_w[z*width*height+y*width+x] = data.w;\n    }\n}\n}\n\'\'\'\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    \'dimensions\': ((64, 0, 0), (64, 32, 0), (64, 32, 19)),\n    \'mem_type\': (\'CUDAarray\', \'linear\', \'pitch2D\'),\n    \'target\': (\'object\', \'reference\'),\n}))\nclass TestTexture(unittest.TestCase):\n    def test_fetch_float_texture(self):\n        width, height, depth = self.dimensions\n        dim = 3 if depth != 0 else 2 if height != 0 else 1\n\n        if (self.mem_type == \'linear\' and dim != 1) or \\\n           (self.mem_type == \'pitch2D\' and dim != 2):\n            pytest.skip(\'The test case {0} is inapplicable for {1} and thus \'\n                        \'skipped.\'.format(self.dimensions, self.mem_type))\n\n        # generate input data and allocate output buffer\n        shape = (depth, height, width) if dim == 3 else \\\n                (height, width) if dim == 2 else \\\n                (width,)\n\n        # prepare input, output, and texture memory\n        tex_data = cupy.random.random(shape, dtype=cupy.float32)\n        real_output = cupy.zeros_like(tex_data)\n        ch = ChannelFormatDescriptor(32, 0, 0, 0,\n                                     runtime.cudaChannelFormatKindFloat)\n        assert tex_data.flags[\'C_CONTIGUOUS\']\n        assert real_output.flags[\'C_CONTIGUOUS\']\n        if self.mem_type == \'CUDAarray\':\n            arr = CUDAarray(ch, width, height, depth)\n            expected_output = cupy.zeros_like(tex_data)\n            assert expected_output.flags[\'C_CONTIGUOUS\']\n            # test bidirectional copy\n            arr.copy_from(tex_data)\n            arr.copy_to(expected_output)\n        else:  # linear are pitch2D are backed by ndarray\n            arr = tex_data\n            expected_output = tex_data\n\n        # create resource and texture descriptors\n        if self.mem_type == \'CUDAarray\':\n            res = ResourceDescriptor(runtime.cudaResourceTypeArray, cuArr=arr)\n        elif self.mem_type == \'linear\':\n            res = ResourceDescriptor(runtime.cudaResourceTypeLinear,\n                                     arr=arr,\n                                     chDesc=ch,\n                                     sizeInBytes=arr.size*arr.dtype.itemsize)\n        else:  # pitch2D\n            # In this case, we rely on the fact that the hand-picked array\n            # shape meets the alignment requirement. This is CUDA\'s limitation,\n            # see CUDA Runtime API reference guide. ""TexturePitchAlignment"" is\n            # assumed to be 32, which should be applicable for most devices.\n            res = ResourceDescriptor(runtime.cudaResourceTypePitch2D,\n                                     arr=arr,\n                                     chDesc=ch,\n                                     width=width,\n                                     height=height,\n                                     pitchInBytes=width*arr.dtype.itemsize)\n        address_mode = (runtime.cudaAddressModeClamp,\n                        runtime.cudaAddressModeClamp)\n        tex = TextureDescriptor(address_mode, runtime.cudaFilterModePoint,\n                                runtime.cudaReadModeElementType)\n\n        if self.target == \'object\':\n            # create a texture object\n            texobj = TextureObject(res, tex)\n            mod = cupy.RawModule(code=source_texobj)\n        else:  # self.target == \'reference\'\n            mod = cupy.RawModule(code=source_texref)\n            texref_name = \'texref\'\n            texref_name += \'3D\' if dim == 3 else \'2D\' if dim == 2 else \'1D\'\n            texrefPtr = mod.get_texref(texref_name)\n            # bind texture ref to resource\n            texref = TextureReference(texrefPtr, res, tex)  # noqa\n\n        # get and launch the kernel\n        ker_name = \'copyKernel\'\n        ker_name += \'3D\' if dim == 3 else \'2D\' if dim == 2 else \'1D\'\n        ker_name += \'fetch\' if self.mem_type == \'linear\' else \'\'\n        ker = mod.get_function(ker_name)\n        block = (4, 4, 2) if dim == 3 else (4, 4) if dim == 2 else (4,)\n        grid = ()\n        args = (real_output,)\n        if self.target == \'object\':\n            args = args + (texobj,)\n        if dim >= 1:\n            grid_x = (width + block[0] - 1)//block[0]\n            grid = grid + (grid_x,)\n            args = args + (width,)\n        if dim >= 2:\n            grid_y = (height + block[1] - 1)//block[1]\n            grid = grid + (grid_y,)\n            args = args + (height,)\n        if dim == 3:\n            grid_z = (depth + block[2] - 1)//block[2]\n            grid = grid + (grid_z,)\n            args = args + (depth,)\n        ker(grid, block, args)\n\n        # validate result\n        assert (real_output == expected_output).all()\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    \'target\': (\'object\', \'reference\'),\n}))\nclass TestTextureVectorType(unittest.TestCase):\n    def test_fetch_float4_texture(self):\n        width = 47\n        height = 39\n        depth = 11\n        n_channel = 4\n\n        # generate input data and allocate output buffer\n        in_shape = (depth, height, n_channel*width)\n        out_shape = (depth, height, width)\n\n        # prepare input, output, and texture memory\n        tex_data = cupy.random.random(in_shape, dtype=cupy.float32)\n        real_output_x = cupy.zeros(out_shape, dtype=cupy.float32)\n        real_output_y = cupy.zeros(out_shape, dtype=cupy.float32)\n        real_output_z = cupy.zeros(out_shape, dtype=cupy.float32)\n        real_output_w = cupy.zeros(out_shape, dtype=cupy.float32)\n        ch = ChannelFormatDescriptor(32, 32, 32, 32,\n                                     runtime.cudaChannelFormatKindFloat)\n        arr = CUDAarray(ch, width, height, depth)\n        arr.copy_from(tex_data)\n\n        # create resource and texture descriptors\n        res = ResourceDescriptor(runtime.cudaResourceTypeArray, cuArr=arr)\n        address_mode = (runtime.cudaAddressModeClamp,\n                        runtime.cudaAddressModeClamp)\n        tex = TextureDescriptor(address_mode, runtime.cudaFilterModePoint,\n                                runtime.cudaReadModeElementType)\n\n        if self.target == \'object\':\n            # create a texture object\n            texobj = TextureObject(res, tex)\n            mod = cupy.RawModule(code=source_texobj)\n        else:  # self.target == \'reference\'\n            mod = cupy.RawModule(code=source_texref)\n            texrefPtr = mod.get_texref(\'texref3Df4\')\n            # bind texture ref to resource\n            texref = TextureReference(texrefPtr, res, tex)  # noqa\n\n        # get and launch the kernel\n        ker_name = \'copyKernel3D_4ch\'\n        ker = mod.get_function(ker_name)\n        block = (4, 4, 2)\n        grid = ((width + block[0] - 1)//block[0],\n                (height + block[1] - 1)//block[1],\n                (depth + block[2] - 1)//block[2])\n        args = (real_output_x, real_output_y, real_output_z, real_output_w)\n        if self.target == \'object\':\n            args = args + (texobj,)\n        args = args + (width, height, depth)\n        ker(grid, block, args)\n\n        # validate result\n        assert (real_output_x == tex_data[..., 0::4]).all()\n        assert (real_output_y == tex_data[..., 1::4]).all()\n        assert (real_output_z == tex_data[..., 2::4]).all()\n        assert (real_output_w == tex_data[..., 3::4]).all()\n\n\nsource_surfobj = r""""""\nextern ""C"" {\n__global__ void writeKernel1D(cudaSurfaceObject_t surf,\n                              int width)\n{\n    unsigned int w = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (w < width)\n    {\n        float value = w;\n        value *= 3.0;\n        surf1Dwrite(value, surf, w * 4);\n    }\n}\n\n__global__ void writeKernel2D(cudaSurfaceObject_t surf,\n                              int width, int height)\n{\n    unsigned int w = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int h = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (w < width && h < height)\n    {\n        float value = h * width + w;\n        value *= 3.0;\n        surf2Dwrite(value, surf, w * 4, h);\n    }\n}\n\n__global__ void writeKernel3D(cudaSurfaceObject_t surf,\n                              int width, int height, int depth)\n{\n    unsigned int w = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int h = blockIdx.y * blockDim.y + threadIdx.y;\n    unsigned int d = blockIdx.z * blockDim.z + threadIdx.z;\n\n    if (w < width && h < height && d < depth)\n    {\n        float value = d * width * height + h * width + w;\n        value *= 3.0;\n        surf3Dwrite(value, surf, w * 4, h, d);\n    }\n}\n}\n""""""\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    \'dimensions\': ((64, 0, 0), (64, 32, 0), (64, 32, 32)),\n}))\nclass TestSurface(unittest.TestCase):\n    def test_write_float_surface(self):\n        width, height, depth = self.dimensions\n        dim = 3 if depth != 0 else 2 if height != 0 else 1\n\n        # generate input data and allocate output buffer\n        shape = (depth, height, width) if dim == 3 else \\\n                (height, width) if dim == 2 else \\\n                (width,)\n\n        # prepare input, output, and surface memory\n        real_output = cupy.zeros(shape, dtype=cupy.float32)\n        assert real_output.flags[\'C_CONTIGUOUS\']\n        ch = ChannelFormatDescriptor(32, 0, 0, 0,\n                                     runtime.cudaChannelFormatKindFloat)\n        expected_output = cupy.arange(numpy.prod(shape), dtype=cupy.float32)\n        expected_output = expected_output.reshape(shape) * 3.0\n        assert expected_output.flags[\'C_CONTIGUOUS\']\n\n        # create resource descriptor\n        # note that surface memory only support CUDA array\n        arr = CUDAarray(ch, width, height, depth,\n                        runtime.cudaArraySurfaceLoadStore)\n        arr.copy_from(real_output)  # init to zero\n        res = ResourceDescriptor(runtime.cudaResourceTypeArray, cuArr=arr)\n\n        # create a surface object; currently we don\'t support surface reference\n        surfobj = SurfaceObject(res)\n        mod = cupy.RawModule(code=source_surfobj)\n\n        # get and launch the kernel\n        ker_name = \'writeKernel\'\n        ker_name += \'3D\' if dim == 3 else \'2D\' if dim == 2 else \'1D\'\n        ker = mod.get_function(ker_name)\n        block = (4, 4, 2) if dim == 3 else (4, 4) if dim == 2 else (4,)\n        grid = ()\n        args = (surfobj,)\n        if dim >= 1:\n            grid_x = (width + block[0] - 1)//block[0]\n            grid = grid + (grid_x,)\n            args = args + (width,)\n        if dim >= 2:\n            grid_y = (height + block[1] - 1)//block[1]\n            grid = grid + (grid_y,)\n            args = args + (height,)\n        if dim == 3:\n            grid_z = (depth + block[2] - 1)//block[2]\n            grid = grid + (grid_z,)\n            args = args + (depth,)\n        ker(grid, block, args)\n\n        # validate result\n        arr.copy_to(real_output)\n        assert (real_output == expected_output).all()\n'"
tests/cupy_tests/fft_tests/__init__.py,0,b''
tests/cupy_tests/fft_tests/test_fft.py,64,"b'import functools\nimport unittest\nimport pytest\n\nimport numpy as np\n\nimport cupy\nfrom cupy import testing\nfrom cupy.fft import config\nfrom cupy.fft.fft import (_default_fft_func, _fft, _fftn,\n                          _size_last_transform_axis)\n\n\ndef nd_planning_states(states=[True, False], name=\'enable_nd\'):\n    """"""Decorator for parameterized tests with and wihout nd planning\n\n    Tests are repeated with config.enable_nd_planning set to True and False\n\n    Args:\n         states(list of bool): The boolean cases to test.\n         name(str): Argument name to which specified dtypes are passed.\n\n    This decorator adds a keyword argument specified by ``name``\n    to the test fixture. Then, it runs the fixtures in parallel\n    by passing the each element of ``dtypes`` to the named\n    argument.\n    """"""\n    def decorator(impl):\n        @functools.wraps(impl)\n        def test_func(self, *args, **kw):\n            # get original global planning state\n            planning_state = config.enable_nd_planning\n            try:\n                for nd_planning in states:\n                    try:\n                        # enable or disable nd planning\n                        config.enable_nd_planning = nd_planning\n\n                        kw[name] = nd_planning\n                        impl(self, *args, **kw)\n                    except Exception:\n                        print(name, \'is\', nd_planning)\n                        raise\n            finally:\n                # restore original global planning state\n                config.enable_nd_planning = planning_state\n\n        return test_func\n    return decorator\n\n\ndef multi_gpu_config(gpu_configs=None):\n    """"""Decorator for parameterized tests with different GPU configurations.\n\n    Args:\n        gpu_configs(list of list): The GPUs to test.\n\n    .. notes:\n        The decorated tests are skipped if no or only one GPU is available.\n    """"""\n    def decorator(impl):\n        @functools.wraps(impl)\n        def test_func(self, *args, **kw):\n            use_multi_gpus = config.use_multi_gpus\n            _devices = config._devices\n\n            try:\n                for gpus in gpu_configs:\n                    try:\n                        nGPUs = len(gpus)\n                        assert nGPUs >= 2, \'Must use at least two gpus\'\n                        config.use_multi_gpus = True\n                        config.set_cufft_gpus(gpus)\n\n                        impl(self, *args, **kw)\n                    except Exception:\n                        print(\'GPU config is:\', gpus)\n                        raise\n            finally:\n                config.use_multi_gpus = use_multi_gpus\n                config._devices = _devices\n\n        return test_func\n    return decorator\n\n\n@testing.parameterize(*testing.product({\n    \'n\': [None, 0, 5, 10, 15],\n    \'shape\': [(10,), (10, 10)],\n    \'norm\': [None, \'ortho\', \'\'],\n}))\n@testing.gpu\nclass TestFft(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        out = xp.fft.fft(a, n=self.n, norm=self.norm)\n\n        # np.fft.fft alway returns np.complex128\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.complex64)\n\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    # NumPy 1.17.0 and 1.17.1 raises ZeroDivisonError due to a bug\n    @testing.with_requires(\'numpy!=1.17.0\')\n    @testing.with_requires(\'numpy!=1.17.1\')\n    def test_ifft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        out = xp.fft.ifft(a, n=self.n, norm=self.norm)\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.complex64)\n\n        return out\n\n\n@testing.parameterize(*testing.product({\n    \'shape\': [(10, 10), (10, 5, 10)],\n    \'data_order\': [\'F\', \'C\'],\n    \'axis\': [0, 1, -1],\n}))\n@testing.gpu\nclass TestFftOrder(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if self.data_order == \'F\':\n            a = xp.asfortranarray(a)\n        out = xp.fft.fft(a, axis=self.axis)\n\n        # np.fft.fft alway returns np.complex128\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.complex64)\n\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if self.data_order == \'F\':\n            a = xp.asfortranarray(a)\n        out = xp.fft.ifft(a, axis=self.axis)\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.complex64)\n\n        return out\n\n\n# Almost identical to the TestFft class, except that\n# 1. multi-GPU cuFFT is used\n# 2. the tested parameter combinations are adjusted to meet the requirements\n@testing.parameterize(*testing.product({\n    \'n\': [None, 0, 64],\n    \'shape\': [(64,), (4, 64)],\n    \'norm\': [None, \'ortho\', \'\'],\n}))\n@testing.multi_gpu(2)\nclass TestMultiGpuFft(unittest.TestCase):\n    @multi_gpu_config(gpu_configs=[[0, 1], [1, 0]])\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        out = xp.fft.fft(a, n=self.n, norm=self.norm)\n\n        # np.fft.fft alway returns np.complex128\n        if xp is np and dtype is np.complex64:\n            out = out.astype(dtype)\n\n        return out\n\n    @multi_gpu_config(gpu_configs=[[0, 1], [1, 0]])\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    # NumPy 1.17.0 and 1.17.1 raises ZeroDivisonError due to a bug\n    @testing.with_requires(\'numpy!=1.17.0\')\n    @testing.with_requires(\'numpy!=1.17.1\')\n    def test_ifft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        out = xp.fft.ifft(a, n=self.n, norm=self.norm)\n\n        # np.fft.fft alway returns np.complex128\n        if xp is np and dtype is np.complex64:\n            out = out.astype(dtype)\n\n        return out\n\n\n# Almost identical to the TestFftOrder class, except that\n# 1. multi-GPU cuFFT is used\n# 2. the tested parameter combinations are adjusted to meet the requirements\n@testing.parameterize(*testing.product({\n    \'shape\': [(10, 10), (10, 5, 10)],\n    \'data_order\': [\'F\', \'C\'],\n    \'axis\': [0, 1, -1],\n}))\n@testing.multi_gpu(2)\nclass TestMultiGpuFftOrder(unittest.TestCase):\n    @multi_gpu_config(gpu_configs=[[0, 1], [1, 0]])\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if self.data_order == \'F\':\n            a = xp.asfortranarray(a)\n        out = xp.fft.fft(a, axis=self.axis)\n\n        # np.fft.fft alway returns np.complex128\n        if xp is np and dtype is np.complex64:\n            out = out.astype(dtype)\n\n        return out\n\n    @multi_gpu_config(gpu_configs=[[0, 1], [1, 0]])\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if self.data_order == \'F\':\n            a = xp.asfortranarray(a)\n        out = xp.fft.ifft(a, axis=self.axis)\n\n        # np.fft.fft alway returns np.complex128\n        if xp is np and dtype is np.complex64:\n            out = out.astype(dtype)\n\n        return out\n\n\n@testing.gpu\nclass TestDefaultPlanType(unittest.TestCase):\n\n    @nd_planning_states()\n    def test_default_fft_func(self, enable_nd):\n        # test cases where nd cuFFT plan is possible\n        ca = cupy.ones((16, 16, 16))\n        for axes in [(0, 1), (1, 2), None, (0, 1, 2)]:\n            fft_func = _default_fft_func(ca, axes=axes)\n            if enable_nd:\n                assert fft_func is _fftn\n            else:\n                assert fft_func is _fft\n\n        # only a single axis is transformed -> 1d plan preferred\n        for axes in [(0, ), (1, ), (2, )]:\n            assert _default_fft_func(ca, axes=axes) is _fft\n\n        # non-contiguous axes -> nd plan not possible\n        assert _default_fft_func(ca, axes=(0, 2)) is _fft\n\n        # >3 axes transformed -> nd plan not possible\n        ca = cupy.ones((2, 4, 6, 8))\n        assert _default_fft_func(ca) is _fft\n\n        # first or last axis not included -> nd plan not possible\n        assert _default_fft_func(ca, axes=(1, )) is _fft\n\n        # for rfftn\n        ca = cupy.random.random((4, 2, 6))\n        for s, axes in zip([(3, 4), None, (8, 7, 5)],\n                           [(-2, -1), (0, 1), None]):\n            fft_func = _default_fft_func(ca, s=s, axes=axes, value_type=\'R2C\')\n            if enable_nd:\n                assert fft_func is _fftn\n            else:\n                assert fft_func is _fft\n\n        # nd plan not possible if last axis is not 0 or ndim-1\n        assert _default_fft_func(ca, axes=(2, 1), value_type=\'R2C\') is _fft\n\n        # for irfftn\n        ca = cupy.random.random((4, 2, 6)).astype(cupy.complex128)\n        for s, axes in zip([(3, 4), None, (8, 7, 5)],\n                           [(-2, -1), (0, 1), None]):\n            fft_func = _default_fft_func(ca, s=s, axes=axes, value_type=\'C2R\')\n            if enable_nd:\n                assert fft_func is _fftn\n            else:\n                assert fft_func is _fft\n\n        # nd plan not possible if last axis is not 0 or ndim-1\n        assert _default_fft_func(ca, axes=(2, 1), value_type=\'C2R\') is _fft\n\n\n@testing.gpu\n@testing.slow\nclass TestFftAllocate(unittest.TestCase):\n\n    def test_fft_allocate(self):\n        # Check CuFFTError is not raised when the GPU memory is enough.\n        # See https://github.com/cupy/cupy/issues/1063\n        # TODO(mizuno): Simplify ""a"" after memory compaction is implemented.\n        a = []\n        for i in range(10):\n            a.append(cupy.empty(100000000))\n        del a\n        b = cupy.empty(100000007, dtype=cupy.float32)\n        cupy.fft.fft(b)\n        # Free huge memory for slow test\n        del b\n        cupy.get_default_memory_pool().free_all_blocks()\n\n\n@testing.parameterize(\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': (1, None), \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': (1, 5), \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-2, -1), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-1, -2), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (0,), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, None), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, 10), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-3, -2, -1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-1, -2, -3), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (0, 1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (2, 3), \'axes\': (0, 1, 2), \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4, 5), \'s\': None, \'axes\': None, \'norm\': None},\n)\n@testing.gpu\nclass TestFft2(unittest.TestCase):\n\n    @nd_planning_states()\n    @testing.for_orders(\'CF\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft2(self, xp, dtype, order, enable_nd):\n        assert config.enable_nd_planning == enable_nd\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if order == \'F\':\n            a = xp.asfortranarray(a)\n        out = xp.fft.fft2(a, s=self.s, axes=self.axes, norm=self.norm)\n\n        if self.axes is not None and not self.axes:\n            assert out is a\n            return out\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.complex64)\n\n        return out\n\n    @nd_planning_states()\n    @testing.for_orders(\'CF\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifft2(self, xp, dtype, order, enable_nd):\n        assert config.enable_nd_planning == enable_nd\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if order == \'F\':\n            a = xp.asfortranarray(a)\n        out = xp.fft.ifft2(a, s=self.s, axes=self.axes, norm=self.norm)\n\n        if self.axes is not None and not self.axes:\n            assert out is a\n            return out\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.complex64)\n\n        return out\n\n\n@testing.parameterize(\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': (1, None), \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': (1, 5), \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-2, -1), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-1, -2), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': [-1, -2], \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (0,), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, None), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, 10), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-3, -2, -1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-1, -2, -3), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-1, -3), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (0, 1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (), \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': (2, 3), \'axes\': (0, 1, 2), \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': (4, 3, 2), \'axes\': (2, 0, 1), \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4, 5), \'s\': None, \'axes\': None, \'norm\': None},\n)\n@testing.gpu\nclass TestFftn(unittest.TestCase):\n\n    @nd_planning_states()\n    @testing.for_orders(\'CF\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fftn(self, xp, dtype, order, enable_nd):\n        assert config.enable_nd_planning == enable_nd\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if order == \'F\':\n            a = xp.asfortranarray(a)\n        out = xp.fft.fftn(a, s=self.s, axes=self.axes, norm=self.norm)\n\n        if self.axes is not None and not self.axes:\n            assert out is a\n            return out\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.complex64)\n\n        return out\n\n    @nd_planning_states()\n    @testing.for_orders(\'CF\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifftn(self, xp, dtype, order, enable_nd):\n        assert config.enable_nd_planning == enable_nd\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if order == \'F\':\n            a = xp.asfortranarray(a)\n        out = xp.fft.ifftn(a, s=self.s, axes=self.axes, norm=self.norm)\n\n        if self.axes is not None and not self.axes:\n            assert out is a\n            return out\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.complex64)\n\n        return out\n\n\n@testing.parameterize(\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': (1, 5), \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-2, -1), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-1, -2), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (0,), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, 10), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-3, -2, -1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-1, -2, -3), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (0, 1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': (2, 3), \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': (2, 3), \'axes\': (0, 1, 2), \'norm\': \'ortho\'},\n)\n@testing.gpu\nclass TestPlanCtxManagerFftn(unittest.TestCase):\n\n    @nd_planning_states()\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fftn(self, xp, dtype, enable_nd):\n        assert config.enable_nd_planning == enable_nd\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if xp is cupy:\n            from cupyx.scipy.fftpack import get_fft_plan\n            plan = get_fft_plan(a, self.s, self.axes)\n            with plan:\n                out = xp.fft.fftn(a, s=self.s, axes=self.axes, norm=self.norm)\n        else:\n            out = xp.fft.fftn(a, s=self.s, axes=self.axes, norm=self.norm)\n\n        if xp is np and dtype is np.complex64:\n            out = out.astype(np.complex64)\n\n        return out\n\n    @nd_planning_states()\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifftn(self, xp, dtype, enable_nd):\n        assert config.enable_nd_planning == enable_nd\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if xp is cupy:\n            from cupyx.scipy.fftpack import get_fft_plan\n            plan = get_fft_plan(a, self.s, self.axes)\n            with plan:\n                out = xp.fft.ifftn(a, s=self.s, axes=self.axes, norm=self.norm)\n        else:\n            out = xp.fft.ifftn(a, s=self.s, axes=self.axes, norm=self.norm)\n\n        if xp is np and dtype is np.complex64:\n            out = out.astype(np.complex64)\n\n        return out\n\n    @nd_planning_states()\n    @testing.for_complex_dtypes()\n    def test_fftn_error_on_wrong_plan(self, dtype, enable_nd):\n        # This test ensures the context manager plan is picked up\n\n        from cupyx.scipy.fftpack import get_fft_plan\n        from cupy.fft import fftn\n        assert config.enable_nd_planning == enable_nd\n\n        # can\'t get a plan, so skip\n        if self.axes is not None:\n            if self.s is not None:\n                if len(self.s) != len(self.axes):\n                    return\n            elif len(self.shape) != len(self.axes):\n                return\n\n        a = testing.shaped_random(self.shape, cupy, dtype)\n        bad_in_shape = tuple(2*i for i in self.shape)\n        if self.s is None:\n            bad_out_shape = bad_in_shape\n        else:\n            bad_out_shape = tuple(2*i for i in self.s)\n        b = testing.shaped_random(bad_in_shape, cupy, dtype)\n        plan_wrong = get_fft_plan(b, bad_out_shape, self.axes)\n\n        with pytest.raises(ValueError) as ex, plan_wrong:\n            fftn(a, s=self.s, axes=self.axes, norm=self.norm)\n        # targeting a particular error\n        assert \'The cuFFT plan and a.shape do not match\' in str(ex.value)\n\n\n@testing.parameterize(*testing.product({\n    \'n\': [None, 5, 10, 15],\n    \'shape\': [(10,), ],\n    \'norm\': [None, \'ortho\'],\n}))\n@testing.gpu\nclass TestPlanCtxManagerFft(unittest.TestCase):\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if xp is cupy:\n            from cupyx.scipy.fftpack import get_fft_plan\n            shape = (self.n,) if self.n is not None else None\n            plan = get_fft_plan(a, shape=shape)\n            assert isinstance(plan, cupy.cuda.cufft.Plan1d)\n            with plan:\n                out = xp.fft.fft(a, n=self.n, norm=self.norm)\n        else:\n            out = xp.fft.fft(a, n=self.n, norm=self.norm)\n\n        # np.fft.fft alway returns np.complex128\n        if xp is np and dtype is np.complex64:\n            out = out.astype(np.complex64)\n\n        return out\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if xp is cupy:\n            from cupyx.scipy.fftpack import get_fft_plan\n            shape = (self.n,) if self.n is not None else None\n            plan = get_fft_plan(a, shape=shape)\n            assert isinstance(plan, cupy.cuda.cufft.Plan1d)\n            with plan:\n                out = xp.fft.ifft(a, n=self.n, norm=self.norm)\n        else:\n            out = xp.fft.ifft(a, n=self.n, norm=self.norm)\n\n        if xp is np and dtype is np.complex64:\n            out = out.astype(np.complex64)\n\n        return out\n\n    @testing.for_complex_dtypes()\n    def test_fft_error_on_wrong_plan(self, dtype):\n        # This test ensures the context manager plan is picked up\n\n        from cupyx.scipy.fftpack import get_fft_plan\n        from cupy.fft import fft\n\n        a = testing.shaped_random(self.shape, cupy, dtype)\n        bad_shape = tuple(5*i for i in self.shape)\n        b = testing.shaped_random(bad_shape, cupy, dtype)\n        plan_wrong = get_fft_plan(b)\n        assert isinstance(plan_wrong, cupy.cuda.cufft.Plan1d)\n\n        with pytest.raises(ValueError) as ex, plan_wrong:\n            fft(a, n=self.n, norm=self.norm)\n        # targeting a particular error\n        assert \'Target array size does not match the plan.\' in str(ex.value)\n\n\n# Almost identical to the TestPlanCtxManagerFft class, except that\n# 1. multi-GPU cuFFT is used\n# 2. the tested parameter combinations are adjusted to meet the requirements\n@testing.parameterize(*testing.product({\n    \'n\': [None, 64],\n    \'shape\': [(64,), (128,)],\n    \'norm\': [None, \'ortho\'],\n}))\n@testing.multi_gpu(2)\nclass TestMultiGpuPlanCtxManagerFft(unittest.TestCase):\n    @multi_gpu_config(gpu_configs=[[0, 1], [1, 0]])\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if xp is cupy:\n            from cupyx.scipy.fftpack import get_fft_plan\n            shape = (self.n,) if self.n is not None else None\n            plan = get_fft_plan(a, shape=shape)\n            assert isinstance(plan, cupy.cuda.cufft.Plan1d)\n            with plan:\n                out = xp.fft.fft(a, n=self.n, norm=self.norm)\n        else:\n            out = xp.fft.fft(a, n=self.n, norm=self.norm)\n\n        # np.fft.fft alway returns np.complex128\n        if xp is np and dtype is np.complex64:\n            out = out.astype(np.complex64)\n\n        return out\n\n    @multi_gpu_config(gpu_configs=[[0, 1], [1, 0]])\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if xp is cupy:\n            from cupyx.scipy.fftpack import get_fft_plan\n            shape = (self.n,) if self.n is not None else None\n            plan = get_fft_plan(a, shape=shape)\n            assert isinstance(plan, cupy.cuda.cufft.Plan1d)\n            with plan:\n                out = xp.fft.ifft(a, n=self.n, norm=self.norm)\n        else:\n            out = xp.fft.ifft(a, n=self.n, norm=self.norm)\n\n        if xp is np and dtype is np.complex64:\n            out = out.astype(np.complex64)\n\n        return out\n\n    @multi_gpu_config(gpu_configs=[[0, 1], [1, 0]])\n    @testing.for_complex_dtypes()\n    def test_fft_error_on_wrong_plan(self, dtype):\n        # This test ensures the context manager plan is picked up\n\n        from cupyx.scipy.fftpack import get_fft_plan\n        from cupy.fft import fft\n\n        a = testing.shaped_random(self.shape, cupy, dtype)\n        bad_shape = tuple(4*i for i in self.shape)\n        b = testing.shaped_random(bad_shape, cupy, dtype)\n        plan_wrong = get_fft_plan(b)\n        assert isinstance(plan_wrong, cupy.cuda.cufft.Plan1d)\n\n        with pytest.raises(ValueError) as ex, plan_wrong:\n            fft(a, n=self.n, norm=self.norm)\n        # targeting a particular error\n        assert \'Target array size does not match the plan.\' in str(ex.value)\n\n\n@testing.parameterize(\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-2, -1), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-1, -2), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (0,), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, None), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, 10), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-3, -2, -1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-1, -2, -3), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (0, 1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4, 5), \'s\': None, \'axes\': (-3, -2, -1), \'norm\': None},\n)\n@testing.gpu\nclass TestFftnContiguity(unittest.TestCase):\n\n    @nd_planning_states([True])\n    @testing.for_all_dtypes()\n    def test_fftn_orders(self, dtype, enable_nd):\n        for order in [\'C\', \'F\']:\n            a = testing.shaped_random(self.shape, cupy, dtype)\n            if order == \'F\':\n                a = cupy.asfortranarray(a)\n            out = cupy.fft.fftn(a, s=self.s, axes=self.axes)\n\n            fft_func = _default_fft_func(a, s=self.s, axes=self.axes)\n            if fft_func is _fftn:\n                # nd plans have output with contiguity matching the input\n                self.assertEqual(out.flags.c_contiguous, a.flags.c_contiguous)\n                self.assertEqual(out.flags.f_contiguous, a.flags.f_contiguous)\n            else:\n                # 1d planning case doesn\'t guarantee preserved contiguity\n                pass\n\n    @nd_planning_states([True])\n    @testing.for_all_dtypes()\n    def test_ifftn_orders(self, dtype, enable_nd):\n        for order in [\'C\', \'F\']:\n\n            a = testing.shaped_random(self.shape, cupy, dtype)\n            if order == \'F\':\n                a = cupy.asfortranarray(a)\n            out = cupy.fft.ifftn(a, s=self.s, axes=self.axes)\n\n            fft_func = _default_fft_func(a, s=self.s, axes=self.axes)\n            if fft_func is _fftn:\n                # nd plans have output with contiguity matching the input\n                self.assertEqual(out.flags.c_contiguous, a.flags.c_contiguous)\n                self.assertEqual(out.flags.f_contiguous, a.flags.f_contiguous)\n            else:\n                # 1d planning case doesn\'t guarantee preserved contiguity\n                pass\n\n\n@testing.parameterize(*testing.product({\n    \'n\': [None, 5, 10, 15],\n    \'shape\': [(10,), (10, 10)],\n    \'norm\': [None, \'ortho\'],\n}))\n@testing.gpu\nclass TestRfft(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, contiguous_check=False)\n    def test_rfft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        out = xp.fft.rfft(a, n=self.n, norm=self.norm)\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.complex64)\n\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, contiguous_check=False)\n    def test_irfft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        out = xp.fft.irfft(a, n=self.n, norm=self.norm)\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.float32)\n\n        return out\n\n\n@testing.parameterize(*testing.product({\n    \'n\': [None, 5, 10, 15],\n    \'shape\': [(10,)],\n    \'norm\': [None, \'ortho\'],\n}))\n@testing.gpu\nclass TestPlanCtxManagerRfft(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if xp is cupy:\n            from cupyx.scipy.fftpack import get_fft_plan\n            shape = (self.n,) if self.n is not None else None\n            plan = get_fft_plan(a, shape=shape, value_type=\'R2C\')\n            assert isinstance(plan, cupy.cuda.cufft.Plan1d)\n            with plan:\n                out = xp.fft.rfft(a, n=self.n, norm=self.norm)\n        else:\n            out = xp.fft.rfft(a, n=self.n, norm=self.norm)\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.complex64)\n\n        return out\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if xp is cupy:\n            from cupyx.scipy.fftpack import get_fft_plan\n            shape = (self.n,) if self.n is not None else None\n            plan = get_fft_plan(a, shape=shape, value_type=\'C2R\')\n            assert isinstance(plan, cupy.cuda.cufft.Plan1d)\n            with plan:\n                out = xp.fft.irfft(a, n=self.n, norm=self.norm)\n        else:\n            out = xp.fft.irfft(a, n=self.n, norm=self.norm)\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.float32)\n\n        return out\n\n    @testing.for_all_dtypes(no_complex=True)\n    def test_rfft_error_on_wrong_plan(self, dtype):\n        # This test ensures the context manager plan is picked up\n\n        from cupyx.scipy.fftpack import get_fft_plan\n        from cupy.fft import rfft\n\n        a = testing.shaped_random(self.shape, cupy, dtype)\n        bad_shape = tuple(5*i for i in self.shape)\n        b = testing.shaped_random(bad_shape, cupy, dtype)\n        plan_wrong = get_fft_plan(b, value_type=\'R2C\')\n        assert isinstance(plan_wrong, cupy.cuda.cufft.Plan1d)\n\n        with pytest.raises(ValueError) as ex, plan_wrong:\n            rfft(a, n=self.n, norm=self.norm)\n        # targeting a particular error\n        assert \'Target array size does not match the plan.\' in str(ex.value)\n\n\n@testing.parameterize(\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': (1, None), \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': (1, 5), \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-2, -1), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-1, -2), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (0,), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, None), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, 10), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-3, -2, -1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-1, -2, -3), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (0, 1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': (2, 3), \'axes\': (0, 1, 2), \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4, 5), \'s\': None, \'axes\': None, \'norm\': None},\n)\n@testing.gpu\nclass TestRfft2(unittest.TestCase):\n\n    @nd_planning_states()\n    @testing.for_orders(\'CF\')\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfft2(self, xp, dtype, order, enable_nd):\n        assert config.enable_nd_planning == enable_nd\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if order == \'F\':\n            a = xp.asfortranarray(a)\n        out = xp.fft.rfft2(a, s=self.s, axes=self.axes, norm=self.norm)\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.complex64)\n        return out\n\n    @nd_planning_states()\n    @testing.for_orders(\'CF\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfft2(self, xp, dtype, order, enable_nd):\n        assert config.enable_nd_planning == enable_nd\n        if (10020 >= cupy.cuda.runtime.runtimeGetVersion() >= 10010\n                and int(cupy.cuda.device.get_compute_capability()) < 70\n                and _size_last_transform_axis(\n                    self.shape, self.s, self.axes) == 2):\n            raise unittest.SkipTest(\'work-around for cuFFT issue\')\n\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if order == \'F\':\n            a = xp.asfortranarray(a)\n        out = xp.fft.irfft2(a, s=self.s, axes=self.axes, norm=self.norm)\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.float32)\n        return out\n\n\n@testing.parameterize(\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (), \'norm\': None},\n)\n@testing.gpu\nclass TestRfft2EmptyAxes(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    def test_rfft2(self, dtype):\n        for xp in (np, cupy):\n            a = testing.shaped_random(self.shape, xp, dtype)\n            with pytest.raises(IndexError):\n                xp.fft.rfft2(a, s=self.s, axes=self.axes, norm=self.norm)\n\n    @testing.for_all_dtypes()\n    def test_irfft2(self, dtype):\n        for xp in (np, cupy):\n            a = testing.shaped_random(self.shape, xp, dtype)\n            with pytest.raises(IndexError):\n                xp.fft.irfft2(a, s=self.s, axes=self.axes, norm=self.norm)\n\n\n@testing.parameterize(\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': (1, None), \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': (1, 5), \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-2, -1), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-1, -2), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (0,), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, None), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, 10), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-3, -2, -1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-1, -2, -3), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (0, 1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': (2, 3), \'axes\': (0, 1, 2), \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4, 5), \'s\': None, \'axes\': None, \'norm\': None},\n)\n@testing.gpu\nclass TestRfftn(unittest.TestCase):\n\n    @nd_planning_states()\n    @testing.for_orders(\'CF\')\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfftn(self, xp, dtype, order, enable_nd):\n        assert config.enable_nd_planning == enable_nd\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if order == \'F\':\n            a = xp.asfortranarray(a)\n        out = xp.fft.rfftn(a, s=self.s, axes=self.axes, norm=self.norm)\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.complex64)\n\n        return out\n\n    @nd_planning_states()\n    @testing.for_orders(\'CF\')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfftn(self, xp, dtype, order, enable_nd):\n        assert config.enable_nd_planning == enable_nd\n        if (10020 >= cupy.cuda.runtime.runtimeGetVersion() >= 10010\n                and int(cupy.cuda.device.get_compute_capability()) < 70\n                and _size_last_transform_axis(\n                    self.shape, self.s, self.axes) == 2):\n            raise unittest.SkipTest(\'work-around for cuFFT issue\')\n\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if order == \'F\':\n            a = xp.asfortranarray(a)\n        out = xp.fft.irfftn(a, s=self.s, axes=self.axes, norm=self.norm)\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.float32)\n\n        return out\n\n\n# Only those tests in which a legit plan can be obtained are kept\n@testing.parameterize(\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': (1, None), \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': (1, 5), \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-2, -1), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (0,), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, None), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, 10), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-3, -2, -1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (0, 1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': (2, 3), \'axes\': (0, 1, 2), \'norm\': \'ortho\'},\n)\n@testing.gpu\nclass TestPlanCtxManagerRfftn(unittest.TestCase):\n\n    @nd_planning_states()\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfftn(self, xp, dtype, enable_nd):\n        assert config.enable_nd_planning == enable_nd\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if xp is cupy:\n            from cupyx.scipy.fftpack import get_fft_plan\n            plan = get_fft_plan(a, self.s, self.axes, value_type=\'R2C\')\n            with plan:\n                out = xp.fft.rfftn(a, s=self.s, axes=self.axes, norm=self.norm)\n        else:\n            out = xp.fft.rfftn(a, s=self.s, axes=self.axes, norm=self.norm)\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.complex64)\n\n        return out\n\n    @nd_planning_states()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfftn(self, xp, dtype, enable_nd):\n        assert config.enable_nd_planning == enable_nd\n        a = testing.shaped_random(self.shape, xp, dtype)\n        if xp is cupy:\n            from cupyx.scipy.fftpack import get_fft_plan\n            plan = get_fft_plan(a, self.s, self.axes, value_type=\'C2R\')\n            with plan:\n                out = xp.fft.irfftn(\n                    a, s=self.s, axes=self.axes, norm=self.norm)\n        else:\n            out = xp.fft.irfftn(a, s=self.s, axes=self.axes, norm=self.norm)\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.float32)\n\n        return out\n\n    # TODO(leofang): write test_rfftn_error_on_wrong_plan()?\n\n\n@testing.parameterize(\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-2, -1), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-1, -2), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (0,), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, None), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, 10), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-3, -2, -1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-1, -2, -3), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (0, 1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4, 5), \'s\': None, \'axes\': None, \'norm\': None},\n)\n@testing.gpu\nclass TestRfftnContiguity(unittest.TestCase):\n\n    @nd_planning_states([True])\n    @testing.for_float_dtypes()\n    def test_rfftn_orders(self, dtype, enable_nd):\n        for order in [\'C\', \'F\']:\n            a = testing.shaped_random(self.shape, cupy, dtype)\n            if order == \'F\':\n                a = cupy.asfortranarray(a)\n            out = cupy.fft.rfftn(a, s=self.s, axes=self.axes)\n\n            fft_func = _default_fft_func(a, s=self.s, axes=self.axes,\n                                         value_type=\'R2C\')\n            if fft_func is _fftn:\n                # nd plans have output with contiguity matching the input\n                self.assertEqual(out.flags.c_contiguous, a.flags.c_contiguous)\n                self.assertEqual(out.flags.f_contiguous, a.flags.f_contiguous)\n            else:\n                # 1d planning case doesn\'t guarantee preserved contiguity\n                pass\n\n    @nd_planning_states([True])\n    @testing.for_all_dtypes()\n    def test_ifftn_orders(self, dtype, enable_nd):\n        for order in [\'C\', \'F\']:\n\n            a = testing.shaped_random(self.shape, cupy, dtype)\n            if order == \'F\':\n                a = cupy.asfortranarray(a)\n            out = cupy.fft.irfftn(a, s=self.s, axes=self.axes)\n\n            fft_func = _default_fft_func(a, s=self.s, axes=self.axes,\n                                         value_type=\'C2R\')\n            if fft_func is _fftn:\n                # nd plans have output with contiguity matching the input\n                self.assertEqual(out.flags.c_contiguous, a.flags.c_contiguous)\n                self.assertEqual(out.flags.f_contiguous, a.flags.f_contiguous)\n            else:\n                # 1d planning case doesn\'t guarantee preserved contiguity\n                pass\n\n\n@testing.parameterize(\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (), \'norm\': None},\n)\n@testing.gpu\nclass TestRfftnEmptyAxes(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    def test_rfftn(self, dtype):\n        for xp in (np, cupy):\n            a = testing.shaped_random(self.shape, xp, dtype)\n            with pytest.raises(IndexError):\n                xp.fft.rfftn(a, s=self.s, axes=self.axes, norm=self.norm)\n\n    @testing.for_all_dtypes()\n    def test_irfftn(self, dtype):\n        for xp in (np, cupy):\n            a = testing.shaped_random(self.shape, xp, dtype)\n            with pytest.raises(IndexError):\n                xp.fft.irfftn(a, s=self.s, axes=self.axes, norm=self.norm)\n\n\n@testing.parameterize(*testing.product({\n    \'n\': [None, 5, 10, 15],\n    \'shape\': [(10,), (10, 10)],\n    \'norm\': [None, \'ortho\'],\n}))\n@testing.gpu\nclass TestHfft(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, contiguous_check=False)\n    def test_hfft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        out = xp.fft.hfft(a, n=self.n, norm=self.norm)\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.float32)\n\n        return out\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, contiguous_check=False)\n    def test_ihfft(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        out = xp.fft.ihfft(a, n=self.n, norm=self.norm)\n\n        if xp is np and dtype in [np.float16, np.float32, np.complex64]:\n            out = out.astype(np.complex64)\n\n        return out\n\n\n@testing.parameterize(\n    {\'n\': 1, \'d\': 1},\n    {\'n\': 10, \'d\': 0.5},\n    {\'n\': 100, \'d\': 2},\n)\n@testing.gpu\nclass TestFftfreq(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, contiguous_check=False)\n    def test_fftfreq(self, xp, dtype):\n        out = xp.fft.fftfreq(self.n, self.d)\n\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, contiguous_check=False)\n    def test_rfftfreq(self, xp, dtype):\n        out = xp.fft.rfftfreq(self.n, self.d)\n\n        return out\n\n\n@testing.parameterize(\n    {\'shape\': (5,), \'axes\': None},\n    {\'shape\': (5,), \'axes\': 0},\n    {\'shape\': (10,), \'axes\': None},\n    {\'shape\': (10,), \'axes\': 0},\n    {\'shape\': (10, 10), \'axes\': None},\n    {\'shape\': (10, 10), \'axes\': 0},\n    {\'shape\': (10, 10), \'axes\': (0, 1)},\n)\n@testing.gpu\nclass TestFftshift(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, contiguous_check=False)\n    def test_fftshift(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        out = xp.fft.fftshift(x, self.axes)\n\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, contiguous_check=False)\n    def test_ifftshift(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        out = xp.fft.ifftshift(x, self.axes)\n\n        return out\n\n\nclass TestThreading(unittest.TestCase):\n\n    def test_threading1(self):\n        import threading\n        from cupy.cuda.cufft import get_current_plan\n\n        def thread_get_curr_plan():\n            return get_current_plan()\n\n        new_thread = threading.Thread(target=thread_get_curr_plan)\n        new_thread.start()\n\n    def test_threading2(self):\n        import threading\n\n        a = cupy.arange(100, dtype=cupy.complex64).reshape(10, 10)\n\n        def thread_do_fft():\n            b = cupy.fft.fftn(a)\n            return b\n\n        new_thread = threading.Thread(target=thread_do_fft)\n        new_thread.start()\n'"
tests/cupy_tests/indexing_tests/__init__.py,0,b''
tests/cupy_tests/indexing_tests/test_generate.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy.indexing import generate\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestIndices(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_indices_list0(self, xp, dtype):\n        return xp.indices((0,), dtype)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_indices_list1(self, xp, dtype):\n        return xp.indices((1, 2), dtype)\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_indices_list2(self, xp, dtype):\n        return xp.indices((1, 2, 3, 4), dtype)\n\n    def test_indices_list3(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.indices((1, 2, 3, 4), dtype=xp.bool_)\n\n\n@testing.gpu\nclass TestIX_(unittest.TestCase):\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_ix_list(self, xp):\n        return xp.ix_([0, 1], [2, 4])\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test_ix_ndarray(self, xp, dtype):\n        return xp.ix_(xp.array([0, 1], dtype), xp.array([2, 3], dtype))\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_ix_empty_ndarray(self, xp):\n        return xp.ix_(xp.array([]))\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_ix_bool_ndarray(self, xp):\n        return xp.ix_(xp.array([True, False] * 2))\n\n\n@testing.gpu\nclass TestR_(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_r_1(self, xp, dtype):\n        a = testing.shaped_arange((3, 4), xp, dtype)\n        b = testing.shaped_reverse_arange((2, 4), xp, dtype)\n        return xp.r_[a, b]\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_r_8(self, xp, dtype):\n        a = testing.shaped_arange((3, 4), xp, dtype)\n        b = testing.shaped_reverse_arange((2, 4), xp, dtype)\n        c = testing.shaped_reverse_arange((1, 4), xp, dtype)\n        return xp.r_[a, b, c]\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_r_2(self, xp, dtype):\n        a = xp.array([1, 2, 3], dtype)\n        return xp.r_[a, 0, 0, a]\n\n    def test_r_3(self):\n        with self.assertRaises(NotImplementedError):\n            cupy.r_[-1:1:6j, [0] * 3, 5, 6]\n\n    @testing.for_all_dtypes()\n    def test_r_4(self, dtype):\n        a = testing.shaped_arange((1, 3), cupy, dtype)\n        with self.assertRaises(NotImplementedError):\n            cupy.r_['-1', a, a]\n\n    def test_r_5(self):\n        with self.assertRaises(NotImplementedError):\n            cupy.r_['0,2', [1, 2, 3], [4, 5, 6]]\n\n    def test_r_6(self):\n        with self.assertRaises(NotImplementedError):\n            cupy.r_['0,2,0', [1, 2, 3], [4, 5, 6]]\n\n    def test_r_7(self):\n        with self.assertRaises(NotImplementedError):\n            cupy.r_['r', [1, 2, 3], [4, 5, 6]]\n\n    @testing.for_all_dtypes()\n    def test_r_9(self, dtype):\n        a = testing.shaped_arange((3, 4), cupy, dtype)\n        b = testing.shaped_reverse_arange((2, 5), cupy, dtype)\n        with self.assertRaises(ValueError):\n            cupy.r_[a, b]\n\n\n@testing.gpu\nclass TestC_(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_c_1(self, xp, dtype):\n        a = testing.shaped_arange((4, 2), xp, dtype)\n        b = testing.shaped_reverse_arange((4, 3), xp, dtype)\n        return xp.c_[a, b]\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_c_2(self, xp, dtype):\n        a = testing.shaped_arange((4, 2), xp, dtype)\n        b = testing.shaped_reverse_arange((4, 3), xp, dtype)\n        c = testing.shaped_reverse_arange((4, 1), xp, dtype)\n        return xp.c_[a, b, c]\n\n    @testing.for_all_dtypes()\n    def test_c_3(self, dtype):\n        a = testing.shaped_arange((3, 4), cupy, dtype)\n        b = testing.shaped_reverse_arange((2, 5), cupy, dtype)\n        with self.assertRaises(ValueError):\n            cupy.c_[a, b]\n\n\n@testing.gpu\nclass TestAxisConcatenator(unittest.TestCase):\n\n    def test_AxisConcatenator_init1(self):\n        with self.assertRaises(TypeError):\n            cupy.indexing.generate.AxisConcatenator.__init__()\n\n    def test_len(self):\n        a = generate.AxisConcatenator()\n        self.assertEqual(len(a), 0)\n\n\n@testing.gpu\nclass TestUnravelIndex(unittest.TestCase):\n\n    @testing.for_orders(['C', 'F', None])\n    @testing.for_int_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test(self, xp, order, dtype):\n        a = testing.shaped_arange((4, 3, 2), xp, dtype)\n        a = xp.minimum(a, 6 * 4 - 1)\n        return xp.unravel_index(a, (6, 4), order=order)\n\n    @testing.for_int_dtypes()\n    def test_invalid_order(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((4, 3, 2), xp, dtype)\n            with pytest.raises(TypeError):\n                xp.unravel_index(a, (6, 4), order='V')\n\n    @testing.for_orders(['C', 'F', None])\n    @testing.for_int_dtypes(no_bool=True)\n    def test_invalid_index(self, order, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((4, 3, 2), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.unravel_index(a, (6, 4), order=order)\n\n    @testing.for_orders(['C', 'F', None])\n    @testing.for_float_dtypes()\n    def test_invalid_dtype(self, order, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((4, 3, 2), xp, dtype)\n            with pytest.raises(TypeError):\n                xp.unravel_index(a, (6, 4), order=order)\n\n\n@testing.gpu\nclass TestRavelMultiIndex(unittest.TestCase):\n\n    @testing.for_orders(['C', 'F', None])\n    @testing.for_int_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test_basic(self, xp, order, dtype):\n        dims = (8, 4)\n        a = [xp.ones(5, dtype=dtype)] * len(dims)\n        return xp.ravel_multi_index(a, dims, order=order)\n\n    @testing.for_orders(['C', 'F', None])\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_multi_index_broadcasting(self, xp, order, dtype):\n        dims = (3, 5)\n        x, y = xp.meshgrid(*[xp.arange(s, dtype=dtype) for s in dims],\n                           sparse=True)\n        return xp.ravel_multi_index((x, y), dims, order=order)\n\n    @testing.for_orders(['C', 'F', None])\n    @testing.for_int_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test_basic_nd_coords(self, xp, order, dtype):\n        dims = (8, 4)\n        a = [xp.ones((3, 3, 3), dtype=dtype)] * len(dims)\n        return xp.ravel_multi_index(a, dims, order=order)\n\n    @testing.for_orders(['C', 'F', None])\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_basic_clip(self, xp, order, dtype):\n        dims = (8, 4, 2)\n        a = [xp.arange(max(dims), dtype=dtype)] * len(dims)\n        return xp.ravel_multi_index(a, dims, order=order, mode='clip')\n\n    @testing.for_orders(['C', 'F', None])\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_basic_wrap(self, xp, order, dtype):\n        dims = (8, 4, 2)\n        a = [xp.arange(max(dims), dtype=dtype)] * len(dims)\n        return xp.ravel_multi_index(a, dims, order=order, mode='wrap')\n\n    @testing.for_orders(['C', 'F', None])\n    @testing.for_int_dtypes(no_bool=True)\n    def test_basic_raise(self, order, dtype):\n        for xp in (numpy, cupy):\n            dims = (8, 4, 2)\n            a = [xp.arange(max(dims), dtype=dtype)] * len(dims)\n            with pytest.raises(ValueError):\n                return xp.ravel_multi_index(a, dims, order=order, mode='raise')\n\n    @testing.for_int_dtypes()\n    def test_invalid_float_dims(self, dtype):\n        for xp in (numpy, cupy):\n            a = xp.ones((3, 5), dtype=dtype)\n            with pytest.raises(TypeError):\n                xp.ravel_multi_index(a, (2., 4, 8.))\n\n    @testing.for_float_dtypes()\n    def test_invalid_multi_index_dtype(self, dtype):\n        for xp in (numpy, cupy):\n            a = xp.ones((3, 5), dtype=dtype)\n            with pytest.raises(TypeError):\n                xp.ravel_multi_index(a, (2, 4, 8))\n\n    @testing.for_orders(['C', 'F', None])\n    @testing.for_int_dtypes(no_bool=True)\n    def test_invalid_multi_index_shape(self, order, dtype):\n        for xp in (numpy, cupy):\n            # a.shape[0] != len(dims)\n            dims = (2, 4)\n            a = xp.ones((len(dims) + 1, 5), dtype=dtype)\n            with pytest.raises(ValueError):\n                xp.ravel_multi_index(a, dims, order=order)\n\n    @testing.for_int_dtypes(no_bool=True)\n    def test_invalid_order(self, dtype):\n        for xp in (numpy, cupy):\n            dims = (8, 4)\n            a = tuple([xp.arange(min(dims), dtype=dtype) for d in dims])\n            with pytest.raises(TypeError):\n                xp.ravel_multi_index(a, dims, order='V')\n\n    @testing.for_orders(['C', 'F', None])\n    @testing.for_int_dtypes(no_bool=True)\n    def test_dims_overflow(self, order, dtype):\n        for xp in (numpy, cupy):\n            dims = (8, 4)\n            a = tuple([xp.arange(min(dims), dtype=dtype) for d in dims])\n            with pytest.raises(ValueError):\n                xp.ravel_multi_index(\n                    a, (xp.iinfo(xp.int64).max, 4), order=order)\n\n    @testing.for_int_dtypes(no_bool=True)\n    def test_invalid_mode(self, dtype):\n        for xp in (numpy, cupy):\n            dims = (8, 4)\n            a = tuple([xp.arange(min(dims), dtype=dtype) for d in dims])\n            with pytest.raises(TypeError):\n                xp.ravel_multi_index(a, dims, mode='invalid')\n"""
tests/cupy_tests/indexing_tests/test_indexing.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestIndexing(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_take_by_scalar(self, xp):\n        a = testing.shaped_arange((2, 4, 3), xp)\n        return a.take(2, axis=1)\n\n    @testing.numpy_cupy_array_equal()\n    def test_external_take_by_scalar(self, xp):\n        a = testing.shaped_arange((2, 4, 3), xp)\n        return xp.take(a, 2, axis=1)\n\n    @testing.numpy_cupy_array_equal()\n    def test_take_by_array(self, xp):\n        a = testing.shaped_arange((2, 4, 3), xp)\n        b = xp.array([[1, 3], [2, 0]])\n        return a.take(b, axis=1)\n\n    @testing.numpy_cupy_array_equal()\n    def test_take_no_axis(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        b = xp.array([[10, 5], [3, 20]])\n        return a.take(b)\n\n    # see cupy#3017\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_take_index_range_overflow(self, xp, dtype):\n        # Skip for too large dimensions\n        if numpy.dtype(dtype) in (numpy.int64, numpy.uint64):\n            return xp.array([])\n        # Skip because NumPy actually allocates a contiguous array in the\n        # `take` below to require much time.\n        if dtype in (numpy.int32, numpy.uint32):\n            return xp.array([])\n        iinfo = numpy.iinfo(dtype)\n        a = xp.broadcast_to(xp.ones(1), (iinfo.max + 1,))\n        b = xp.array([0], dtype=dtype)\n        return a.take(b)\n\n    @testing.numpy_cupy_array_equal()\n    def test_take_along_axis(self, xp):\n        a = testing.shaped_random((2, 4, 3), xp, dtype='float32')\n        b = testing.shaped_random((2, 6, 3), xp, dtype='int64', scale=4)\n        return xp.take_along_axis(a, b, axis=-2)\n\n    @testing.numpy_cupy_array_equal()\n    def test_take_along_axis_none_axis(self, xp):\n        a = testing.shaped_random((2, 4, 3), xp, dtype='float32')\n        b = testing.shaped_random((30,), xp, dtype='int64', scale=24)\n        return xp.take_along_axis(a, b, axis=None)\n\n    @testing.numpy_cupy_array_equal()\n    def test_compress(self, xp):\n        a = testing.shaped_arange((3, 4, 5), xp)\n        b = xp.array([True, False, True])\n        return xp.compress(b, a, axis=1)\n\n    @testing.numpy_cupy_array_equal()\n    def test_compress_no_axis(self, xp):\n        a = testing.shaped_arange((3, 4, 5), xp)\n        b = xp.array([True, False, True])\n        return xp.compress(b, a)\n\n    @testing.for_int_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_compress_no_bool(self, xp, dtype):\n        a = testing.shaped_arange((3, 4, 5), xp)\n        b = testing.shaped_arange((3,), xp, dtype)\n        return xp.compress(b, a, axis=1)\n\n    @testing.numpy_cupy_array_equal()\n    def test_compress_empty_1dim(self, xp):\n        a = testing.shaped_arange((3, 4, 5), xp)\n        b = xp.array([])\n        return xp.compress(b, a, axis=1)\n\n    @testing.numpy_cupy_array_equal()\n    def test_compress_empty_1dim_no_axis(self, xp):\n        a = testing.shaped_arange((3, 4, 5), xp)\n        b = xp.array([])\n        return xp.compress(b, a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_diagonal(self, xp, dtype):\n        a = testing.shaped_arange((3, 4, 5), xp, dtype)\n        return a.diagonal(1, 2, 0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_external_diagonal(self, xp, dtype):\n        a = testing.shaped_arange((3, 4, 5), xp, dtype)\n        return xp.diagonal(a, 1, 2, 0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_diagonal_negative1(self, xp, dtype):\n        a = testing.shaped_arange((3, 4, 5), xp, dtype)\n        return a.diagonal(-1, 2, 0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_diagonal_negative2(self, xp, dtype):\n        a = testing.shaped_arange((3, 3, 3), xp, dtype)\n        return a.diagonal(0, -1, -2)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_diagonal_negative3(self, xp, dtype):\n        a = testing.shaped_arange((3, 3, 3), xp, dtype)\n        return a.diagonal(0, -1, 1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_diagonal_negative4(self, xp, dtype):\n        a = testing.shaped_arange((3, 3, 3), xp, dtype)\n        return a.diagonal(0, -3, -1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_diagonal_negative5(self, xp, dtype):\n        a = testing.shaped_arange((3, 3, 3), xp, dtype)\n        return a.diagonal(0, -1, -3)\n\n    def test_diagonal_invalid1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((3, 3, 3), xp)\n            with pytest.raises(IndexError):\n                a.diagonal(0, 1, 3)\n\n    def test_diagonal_invalid2(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((3, 3, 3), xp)\n            with pytest.raises(IndexError):\n                a.diagonal(0, 2, -4)\n\n    @testing.numpy_cupy_array_equal()\n    def test_extract(self, xp):\n        a = testing.shaped_arange((3, 3), xp)\n        b = xp.array([[True, False, True],\n                      [False, True, False],\n                      [True, False, True]])\n        return xp.extract(b, a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_extract_no_bool(self, xp, dtype):\n        a = testing.shaped_arange((3, 3), xp)\n        b = xp.array([[1, 0, 1], [0, 1, 0], [1, 0, 1]], dtype=dtype)\n        return xp.extract(b, a)\n\n    @testing.numpy_cupy_array_equal()\n    def test_extract_shape_mismatch(self, xp):\n        a = testing.shaped_arange((2, 3), xp)\n        b = xp.array([[True, False],\n                      [True, False],\n                      [True, False]])\n        return xp.extract(b, a)\n\n    @testing.numpy_cupy_array_equal()\n    def test_extract_size_mismatch(self, xp):\n        a = testing.shaped_arange((3, 3), xp)\n        b = xp.array([[True, False, True],\n                      [False, True, False]])\n        return xp.extract(b, a)\n\n    @testing.numpy_cupy_array_equal()\n    def test_extract_size_mismatch2(self, xp):\n        a = testing.shaped_arange((3, 3), xp)\n        b = xp.array([[True, False, True, False],\n                      [False, True, False, True]])\n        return xp.extract(b, a)\n\n    @testing.numpy_cupy_array_equal()\n    def test_extract_empty_1dim(self, xp):\n        a = testing.shaped_arange((3, 3), xp)\n        b = xp.array([])\n        return xp.extract(b, a)\n\n\n@testing.gpu\nclass TestChoose(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_choose(self, xp, dtype):\n        a = xp.array([0, 2, 1, 2])\n        c = testing.shaped_arange((3, 4), xp, dtype)\n        return a.choose(c)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_choose_broadcast(self, xp, dtype):\n        a = xp.array([[1, 0, 1], [0, 1, 0], [1, 0, 1]])\n        c = xp.array([-10, 10], dtype=dtype)\n        return a.choose(c)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_choose_broadcast2(self, xp, dtype):\n        a = xp.array([0, 1])\n        c = testing.shaped_arange((3, 5, 2), xp, dtype)\n        return a.choose(c)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_choose_wrap(self, xp, dtype):\n        a = xp.array([0, 3, -1, 5])\n        c = testing.shaped_arange((3, 4), xp, dtype)\n        return a.choose(c, mode='wrap')\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_choose_clip(self, xp, dtype):\n        a = xp.array([0, 3, -1, 5])\n        c = testing.shaped_arange((3, 4), xp, dtype)\n        return a.choose(c, mode='clip')\n\n    def test_unknown_clip(self):\n        a = cupy.array([0, 3, -1, 5])\n        c = testing.shaped_arange((3, 4), cupy, cupy.float32)\n        with self.assertRaises(TypeError):\n            a.choose(c, mode='unknow')\n\n    def test_raise(self):\n        a = cupy.array([2])\n        c = cupy.array([[0, 1]])\n        with self.assertRaises(ValueError):\n            a.choose(c)\n\n    @testing.for_all_dtypes()\n    def test_choose_broadcast_fail(self, dtype):\n        for xp in (numpy, cupy):\n            a = xp.array([0, 1])\n            c = testing.shaped_arange((3, 5, 4), xp, dtype)\n            with pytest.raises(ValueError):\n                return a.choose(c)\n\n\n@testing.gpu\nclass TestSelect(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_select(self, xp, dtype):\n        a = xp.arange(10, dtype=dtype)\n        condlist = [a > 3, a < 5]\n        choicelist = [a, a**2]\n        return xp.select(condlist, choicelist)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_array_almost_equal()\n    def test_select_complex(self, xp, dtype):\n        a = xp.arange(10, dtype=dtype)\n        condlist = [a > 3, a < 5]\n        choicelist = [a, a**2]\n        return xp.select(condlist, choicelist)\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_select_default(self, xp, dtype):\n        a = xp.arange(10, dtype=dtype)\n        condlist = [a > 3, a < 5]\n        choicelist = [a, a**2]\n        default = 3\n        return xp.select(condlist, choicelist, default)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_array_almost_equal()\n    def test_select_default_complex(self, xp, dtype):\n        a = xp.arange(10, dtype=dtype)\n        condlist = [a > 3, a < 5]\n        choicelist = [a, a**2]\n        default = 3\n        return xp.select(condlist, choicelist, default)\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_select_odd_shaped_broadcastable(self, xp, dtype):\n        a = xp.arange(10, dtype=dtype)\n        b = xp.arange(30, dtype=dtype).reshape(3, 10)\n        condlist = [a < 3, b > 8]\n        choicelist = [a, b]\n        return xp.select(condlist, choicelist)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_array_almost_equal()\n    def test_select_odd_shaped_broadcastable_complex(self, xp, dtype):\n        a = cupy.arange(10, dtype=dtype)\n        b = cupy.arange(20, dtype=dtype).reshape(2, 10)\n        condlist = [a < 3, b > 8]\n        choicelist = [a, b**2]\n        return cupy.select(condlist, choicelist)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_select_1D_choicelist(self, xp, dtype):\n        a = cupy.array(1)\n        b = cupy.array(3)\n        condlist = [a < 3, b > 8]\n        choicelist = [a, b]\n        return cupy.select(condlist, choicelist)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_select_choicelist_condlist_broadcast(self, xp, dtype):\n        a = cupy.arange(10, dtype=dtype)\n        b = cupy.arange(20, dtype=dtype).reshape(2, 10)\n        condlist = [a < 4, b > 8]\n        choicelist = [cupy.repeat(a, 2).reshape(2, 10), b]\n        return cupy.select(condlist, choicelist)\n\n    @testing.for_all_dtypes(no_bool=True)\n    def test_select_length_error(self, dtype):\n        a = cupy.arange(10, dtype=dtype)\n        condlist = [a > 3]\n        choicelist = [a, a**2]\n        with pytest.raises(ValueError):\n            cupy.select(condlist, choicelist)\n\n    @testing.for_all_dtypes(no_bool=True)\n    def test_select_type_error_condlist(self, dtype):\n        a = cupy.arange(10, dtype=dtype)\n        condlist = [[3] * 10, [2] * 10]\n        choicelist = [a, a**2]\n        with pytest.raises(AttributeError):\n            cupy.select(condlist, choicelist)\n\n    @testing.for_all_dtypes(no_bool=True)\n    def test_select_type_error_choicelist(self, dtype):\n        a, b = list(range(10)), list(range(-10, 0))\n        condlist = [0] * 10\n        choicelist = [a, b]\n        with pytest.raises(ValueError):\n            cupy.select(condlist, choicelist)\n\n    def test_select_empty_lists(self):\n        condlist = []\n        choicelist = []\n        with pytest.raises(ValueError):\n            cupy.select(condlist, choicelist)\n\n    @testing.for_all_dtypes(no_bool=True)\n    def test_select_odd_shaped_non_broadcastable(self, dtype):\n        a = cupy.arange(10, dtype=dtype)\n        b = cupy.arange(20, dtype=dtype)\n        condlist = [a < 3, b > 8]\n        choicelist = [a, b]\n        with pytest.raises(ValueError):\n            cupy.select(condlist, choicelist)\n\n    @testing.for_all_dtypes(no_bool=True)\n    def test_select_default_scalar(self, dtype):\n        a = cupy.arange(10)\n        b = cupy.arange(20)\n        condlist = [a < 3, b > 8]\n        choicelist = [a, b]\n        with pytest.raises(TypeError):\n            cupy.select(condlist, choicelist, [dtype(2)])\n"""
tests/cupy_tests/indexing_tests/test_insert.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(7,), (2, 3), (4, 3, 2)],\n    'n_vals': [0, 1, 3, 15],\n}))\n@testing.gpu\nclass TestPlace(unittest.TestCase):\n\n    # NumPy 1.9 don't wraps values.\n    # https://github.com/numpy/numpy/pull/5821\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_place(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        if self.n_vals == 0:\n            mask = xp.zeros(self.shape, dtype=numpy.bool_)\n        else:\n            mask = testing.shaped_random(self.shape, xp, numpy.bool_)\n        vals = testing.shaped_random((self.n_vals,), xp, dtype)\n        xp.place(a, mask, vals)\n        return a\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(7,), (2, 3), (4, 3, 2)],\n}))\n@testing.gpu\nclass TestPlaceRaises(unittest.TestCase):\n\n    # NumPy 1.9 performs illegal memory access.\n    # https://github.com/numpy/numpy/pull/5821\n    @testing.with_requires('numpy>=1.10')\n    @testing.for_all_dtypes()\n    def test_place_empty_value_error(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange(self.shape, xp, dtype)\n            mask = testing.shaped_arange(self.shape, xp, numpy.int) % 2 == 0\n            vals = testing.shaped_random((0,), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.place(a, mask, vals)\n\n    # Before NumPy 1.12 it was TypeError.\n    # https://github.com/numpy/numpy/pull/7003\n    @testing.for_all_dtypes()\n    def test_place_shape_unmatch_error(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange(self.shape, xp, dtype)\n            mask = testing.shaped_random((3, 4), xp, numpy.bool_)\n            vals = testing.shaped_random((1,), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.place(a, mask, vals)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(7,), (2, 3), (4, 3, 2)],\n    'mode': ['raise', 'wrap', 'clip'],\n    'n_vals': [0, 1, 3, 4, 5],\n}))\n@testing.gpu\nclass TestPut(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_put(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        # Take care so that actual indices don't overlap.\n        if self.mode == 'raise':\n            inds = xp.array([2, -1, 3, 0])\n        else:\n            inds = xp.array([2, -8, 3, 7])\n        vals = testing.shaped_random((self.n_vals,), xp, dtype)\n        xp.put(a, inds, vals, self.mode)\n        return a\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(7,), (2, 3), (4, 3, 2)],\n}))\n@testing.gpu\nclass TestPutScalars(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_put_index_scalar(self, xp):\n        dtype = cupy.float32\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        inds = 4\n        vals = testing.shaped_random((4,), xp, dtype)\n        xp.put(a, inds, vals)\n        return a\n\n    @testing.numpy_cupy_array_equal()\n    def test_put_values_scalar(self, xp):\n        dtype = cupy.float32\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        # Take care so that actual indices don't overlap.\n        inds = xp.array([2, 3, 5])\n        vals = 3.0\n        xp.put(a, inds, vals)\n        return a\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(7,), (2, 3)],\n}))\n@testing.gpu\nclass TestPutRaises(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_put_inds_underflow_error(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange(self.shape, xp, dtype)\n            inds = xp.array([2, -8, 3, 0])\n            vals = testing.shaped_random((4,), xp, dtype)\n            with pytest.raises(IndexError):\n                xp.put(a, inds, vals, mode='raise')\n\n    @testing.for_all_dtypes()\n    def test_put_inds_overflow_error(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange(self.shape, xp, dtype)\n            inds = xp.array([2, -1, 3, 7])\n            vals = testing.shaped_random((4,), xp, dtype)\n            with pytest.raises(IndexError):\n                xp.put(a, inds, vals, mode='raise')\n\n    @testing.for_all_dtypes()\n    def test_put_mode_error(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange(self.shape, xp, dtype)\n            inds = xp.array([2, -1, 3, 0])\n            vals = testing.shaped_random((4,), xp, dtype)\n            with pytest.raises(TypeError):\n                xp.put(a, inds, vals, mode='unknown')\n\n\n@testing.parameterize(\n    *testing.product(\n        {'shape': [(0,), (1,), (2, 3), (2, 3, 4)]}))\n@testing.gpu\nclass TestPutmaskSameShape(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_putmask(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype=dtype, seed=0)\n        mask = testing.shaped_random(self.shape, xp, dtype=numpy.bool_, seed=1)\n        values = testing.shaped_random(self.shape, xp, dtype=dtype, seed=2)\n        ret = xp.putmask(a, mask, values)\n        assert ret is None\n        return a\n\n\n@testing.parameterize(\n    *testing.product(\n        {'shape': [(0,), (1,), (2, 3), (2, 3, 4)],\n         'values_shape': [(2,), (3, 1), (5,)]}))\n@testing.gpu\nclass TestPutmaskDifferentShapes(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_putmask(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype=dtype, seed=3)\n        mask = testing.shaped_random(self.shape, xp, dtype=numpy.bool_, seed=4)\n        values = testing.shaped_random(self.values_shape,\n                                       xp, dtype=dtype, seed=5)\n        ret = xp.putmask(a, mask, values)\n        assert ret is None\n        return a\n\n\n@testing.gpu\nclass TestPutmask(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_putmask_scalar_values(self, xp):\n        shape = (2, 3)\n        a = testing.shaped_arange(shape, xp)\n        xp.putmask(a, a > 1, 30)\n        return a\n\n    def test_putmask_non_equal_shape_raises(self):\n        for xp in (numpy, cupy):\n            a = xp.array([1, 2, 3])\n            mask = xp.array([True, False])\n            with pytest.raises(ValueError):\n                xp.putmask(a, mask, a**2)\n\n    @testing.numpy_cupy_array_equal()\n    def test_putmask_int_mask_scalar_values(self, xp):\n        a = xp.array([1, 2, 3, 3])\n        mask = xp.array([0, 1, 0, 2])\n        xp.putmask(a, mask, 0)\n        return a\n\n\nclass TestPutmaskDifferentDtypes(unittest.TestCase):\n\n    @testing.for_all_dtypes_combination(names=['a_dtype', 'val_dtype'])\n    def test_putmask_differnt_dtypes_raises(self, a_dtype, val_dtype):\n        shape = (2, 3)\n        for xp in (numpy, cupy):\n            a = testing.shaped_random(shape, xp, dtype=a_dtype)\n            mask = testing.shaped_random(shape, xp, dtype=numpy.bool_)\n            values = testing.shaped_random((3,), xp, dtype=val_dtype)\n            if not numpy.can_cast(val_dtype, a_dtype):\n                with pytest.raises(TypeError):\n                    xp.putmask(a, mask, values)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_putmask_differnt_dtypes_mask(self, xp, dtype):\n        shape = (2, 3)\n        a = testing.shaped_random(shape, xp, dtype=numpy.int64)\n        mask = testing.shaped_random(shape, xp, dtype=dtype)\n        values = testing.shaped_random((3,), xp, dtype=numpy.int64)\n        xp.putmask(a, mask, values)\n        return a\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(3, 3), (2, 2, 2), (3, 5), (5, 3)],\n    'val': [1, 0, (2,), (2, 2)],\n    'wrap': [True, False],\n}))\n@testing.gpu\nclass TestFillDiagonal(unittest.TestCase):\n\n    def _compute_val(self, xp):\n        if type(self.val) is int:\n            return self.val\n        else:\n            return xp.arange(numpy.prod(self.val)).reshape(self.val)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_fill_diagonal(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        val = self._compute_val(xp)\n        xp.fill_diagonal(a, val=val, wrap=self.wrap)\n        return a\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_columnar_slice(self, xp, dtype):  # see cupy#2970\n        if self.shape == (2, 2, 2):\n            pytest.skip(\n                'The length of each dimension must be the same after slicing')\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        val = self._compute_val(xp)\n        xp.fill_diagonal(a[:, 1:], val=val, wrap=self.wrap)\n        return a\n\n    @testing.for_all_dtypes()\n    def test_1darray(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((5,), xp, dtype)\n            val = self._compute_val(xp)\n            with pytest.raises(ValueError):\n                xp.fill_diagonal(a, val=val, wrap=self.wrap)\n\n\n@testing.parameterize(*testing.product({\n    'n': [2, 4, -3, 0],\n    'ndim': [2, 3, 1, 0, -2],\n}))\n@testing.gpu\nclass TestDiagIndices(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_diag_indices(self, xp):\n        return xp.diag_indices(self.n, self.ndim)\n\n\n@testing.parameterize(*testing.product({\n    'n': [-3, 0],\n    'ndim': [1, 0, -2],\n}))\n@testing.gpu\nclass TestDiagIndicesInvalidValues(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_diag_indices(self, xp):\n        return xp.diag_indices(self.n, self.ndim)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(3, 3), (0, 0), (2, 2, 2)],\n}))\n@testing.gpu\nclass TestDiagIndicesFrom(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_diag_indices_from(self, xp):\n        arr = testing.shaped_arange(self.shape, xp)\n        return xp.diag_indices_from(arr)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(3, 5), (3, 3, 4), (5,), (0,), (-1,)],\n}))\n@testing.gpu\nclass TestDiagIndicesFromRaises(unittest.TestCase):\n\n    def test_non_equal_dims(self):\n        for xp in (numpy, cupy):\n            arr = testing.shaped_arange(self.shape, xp)\n            with pytest.raises(ValueError):\n                xp.diag_indices_from(arr)\n"""
tests/cupy_tests/indexing_tests/test_iterate.py,0,"b""import unittest\nimport warnings\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestFlatiter(unittest.TestCase):\n\n    def test_base(self):\n        a = cupy.zeros((2, 3, 4))\n        assert a.flat.base is a\n\n    def test_next(self):\n        a = testing.shaped_arange((2, 3, 4), cupy)\n        e = a.flatten()\n        for ai, ei in zip(a.flat, e):\n            assert(ai == ei)\n\n    def test_len(self):\n        a = cupy.zeros((2, 3, 4))\n        assert(len(a.flat) == 24)\n        assert(len(a[::2].flat) == 12)\n\n\n@testing.parameterize(\n    {'shape': (2, 3, 4), 'index': Ellipsis},\n    {'shape': (2, 3, 4), 'index': 0},\n    {'shape': (2, 3, 4), 'index': 10},\n    {'shape': (2, 3, 4), 'index': slice(None)},\n    {'shape': (2, 3, 4), 'index': slice(None, 10)},\n    {'shape': (2, 3, 4), 'index': slice(None, None, 2)},\n    {'shape': (2, 3, 4), 'index': slice(None, None, -1)},\n    {'shape': (2, 3, 4), 'index': slice(10, None, -1)},\n    {'shape': (2, 3, 4), 'index': slice(10, None, -2)},\n    {'shape': (), 'index': slice(None)},\n    {'shape': (10,), 'index': slice(None)},\n)\n@testing.gpu\nclass TestFlatiterSubscript(unittest.TestCase):\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_getitem(self, xp, dtype, order):\n        a = testing.shaped_arange(self.shape, xp, dtype, order)\n        return a.flat[self.index]\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_setitem_scalar(self, xp, dtype, order):\n        a = xp.zeros(self.shape, dtype=dtype, order=order)\n        a.flat[self.index] = 1\n        return a\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_setitem_ndarray_1d(self, xp, dtype, order):\n        if numpy.isscalar(self.index):\n            return xp.array([])  # skip\n        a = xp.zeros(self.shape, dtype=dtype, order=order)\n        v = testing.shaped_arange((3,), xp, dtype, order)\n        a.flat[self.index] = v\n        return a\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_setitem_ndarray_nd(self, xp, dtype, order):\n        if numpy.isscalar(self.index):\n            return xp.array([])  # skip\n        a = xp.zeros(self.shape, dtype=dtype, order=order)\n        v = testing.shaped_arange((2, 3), xp, dtype, order)\n        a.flat[self.index] = v\n        return a\n\n    @testing.for_CF_orders()\n    @testing.for_all_dtypes_combination(('a_dtype', 'v_dtype'))\n    @testing.numpy_cupy_array_equal()\n    def test_setitem_ndarray_different_types(\n            self, xp, a_dtype, v_dtype, order):\n        if numpy.isscalar(self.index):\n            return xp.array([])  # skip\n        a = xp.zeros(self.shape, dtype=a_dtype, order=order)\n        v = testing.shaped_arange((3,), xp, v_dtype, order)\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore', numpy.ComplexWarning)\n            a.flat[self.index] = v\n        return a\n\n\n@testing.parameterize(\n    {'shape': (2, 3, 4), 'index': None},\n    {'shape': (2, 3, 4), 'index': (0,)},\n    {'shape': (2, 3, 4), 'index': True},\n    {'shape': (2, 3, 4), 'index': cupy.array([0])},\n    {'shape': (2, 3, 4), 'index': [0]},\n)\n@testing.gpu\nclass TestFlatiterSubscriptIndexError(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_getitem(self, dtype):\n        a = testing.shaped_arange(self.shape, cupy, dtype)\n        with pytest.raises(IndexError):\n            a.flat[self.index]\n\n    @testing.for_all_dtypes()\n    def test_setitem(self, dtype):\n        a = testing.shaped_arange(self.shape, cupy, dtype)\n        v = testing.shaped_arange((1,), cupy, dtype)\n        with pytest.raises(IndexError):\n            a.flat[self.index] = v\n"""
tests/cupy_tests/io_tests/__init__.py,0,b''
tests/cupy_tests/io_tests/test_base_n.py,0,"b""from cupy import testing\n\n\nclass TestBinaryRepr(testing.NumpyAliasBasicTestBase):\n\n    func = 'binary_repr'\n\n\n@testing.parameterize(\n    *testing.product({\n        'args': [\n            (0,), (3,), (-3,),\n            (0, 0),\n            (3, 5),\n            (-3, 5),\n            # TODO(unno): Insuffisicent width is deprecated in numpy>=1.13.\n            # We need to check if it cause a warning, and maybe it causes an\n            # error in the future.\n            # (3, 0),\n            # (-3, 0),\n        ]}))\nclass TestBinaryReprValues(testing.NumpyAliasValuesTestBase):\n\n    func = 'binary_repr'\n\n\nclass TestBaseRepr(testing.NumpyAliasBasicTestBase):\n\n    func = 'base_repr'\n\n\n@testing.parameterize(\n    *testing.product({\n        'args': [\n            (0,), (5,), (-5,),\n            (0, 2), (0, 10), (0, 36),\n            (5, 2), (5, 10), (5, 36),\n            (-5, 2), (-5, 10), (-5, 36),\n            (-5, 2, 0),\n            (-5, 2, 2),\n            (-5, 2, 10),\n            (5, 2, 0),\n            (5, 2, 2),\n            (5, 2, 10),\n        ]}))\nclass TestBaseReprValues(testing.NumpyAliasValuesTestBase):\n\n    func = 'base_repr'\n"""
tests/cupy_tests/io_tests/test_formatting.py,0,"b'import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestFormatting(unittest.TestCase):\n\n    def test_array_repr(self):\n        a = testing.shaped_arange((2, 3, 4), cupy)\n        b = testing.shaped_arange((2, 3, 4), numpy)\n        self.assertEqual(cupy.array_repr(a), numpy.array_repr(b))\n\n    def test_array_str(self):\n        a = testing.shaped_arange((2, 3, 4), cupy)\n        b = testing.shaped_arange((2, 3, 4), numpy)\n        self.assertEqual(cupy.array_str(a), numpy.array_str(b))\n'"
tests/cupy_tests/io_tests/test_npz.py,0,"b""import io\nimport pickle\nimport unittest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestNpz(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_save_load(self, dtype):\n        a = testing.shaped_arange((2, 3, 4), dtype=dtype)\n        sio = io.BytesIO()\n        cupy.save(sio, a)\n        s = sio.getvalue()\n        sio.close()\n\n        sio = io.BytesIO(s)\n        b = cupy.load(sio)\n        sio.close()\n\n        testing.assert_array_equal(a, b)\n\n    def test_save_pickle(self):\n        data = object()\n\n        sio = io.BytesIO()\n        with self.assertRaises(ValueError):\n            cupy.save(sio, data, allow_pickle=False)\n        sio.close()\n\n        sio = io.BytesIO()\n        cupy.save(sio, data, allow_pickle=True)\n        sio.close()\n\n    def test_load_pickle(self):\n        a = testing.shaped_arange((2, 3, 4), dtype=cupy.float32)\n\n        sio = io.BytesIO()\n        a.dump(sio)\n        s = sio.getvalue()\n        sio.close()\n\n        sio = io.BytesIO(s)\n        b = cupy.load(sio, allow_pickle=True)\n        testing.assert_array_equal(a, b)\n        sio.close()\n\n        sio = io.BytesIO(s)\n        with self.assertRaises(ValueError):\n            cupy.load(sio, allow_pickle=False)\n        sio.close()\n\n    @testing.for_all_dtypes()\n    def check_savez(self, savez, dtype):\n        a1 = testing.shaped_arange((2, 3, 4), dtype=dtype)\n        a2 = testing.shaped_arange((3, 4, 5), dtype=dtype)\n\n        sio = io.BytesIO()\n        savez(sio, a1, a2)\n        s = sio.getvalue()\n        sio.close()\n\n        sio = io.BytesIO(s)\n        with cupy.load(sio) as d:\n            b1 = d['arr_0']\n            b2 = d['arr_1']\n        sio.close()\n\n        testing.assert_array_equal(a1, b1)\n        testing.assert_array_equal(a2, b2)\n\n    def test_savez(self):\n        self.check_savez(cupy.savez)\n\n    def test_savez_compressed(self):\n        self.check_savez(cupy.savez_compressed)\n\n    @testing.for_all_dtypes()\n    def test_pickle(self, dtype):\n        a = testing.shaped_arange((2, 3, 4), dtype=dtype)\n        s = pickle.dumps(a)\n        b = pickle.loads(s)\n        testing.assert_array_equal(a, b)\n\n    @testing.for_all_dtypes()\n    def test_dump(self, dtype):\n        a = testing.shaped_arange((2, 3, 4), dtype=dtype)\n\n        sio = io.BytesIO()\n        a.dump(sio)\n        s = sio.getvalue()\n        sio.close()\n\n        sio = io.BytesIO(s)\n        b = cupy.load(sio, allow_pickle=True)\n        sio.close()\n\n        testing.assert_array_equal(a, b)\n"""
tests/cupy_tests/io_tests/test_text.py,0,b'import unittest\n\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestText(unittest.TestCase):\n\n    pass\n'
tests/cupy_tests/lib_tests/__init__.py,0,b''
tests/cupy_tests/lib_tests/test_strided_tricks.py,0,"b'import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\nfrom cupy.lib import stride_tricks\n\n\n@testing.gpu\nclass TestAsStrided(unittest.TestCase):\n    def test_as_strided(self):\n        a = cupy.array([1, 2, 3, 4])\n        a_view = stride_tricks.as_strided(\n            a, shape=(2,), strides=(2 * a.itemsize,))\n        expected = cupy.array([1, 3])\n        testing.assert_array_equal(a_view, expected)\n\n        a = cupy.array([1, 2, 3, 4])\n        a_view = stride_tricks.as_strided(\n            a, shape=(3, 4), strides=(0, 1 * a.itemsize))\n        expected = cupy.array([[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]])\n        testing.assert_array_equal(a_view, expected)\n\n    @testing.numpy_cupy_array_equal()\n    def test_rolling_window(self, xp):\n        a = testing.shaped_arange((3, 4), xp)\n        a_rolling = rolling_window(a, 2, 0)\n\n        return a_rolling\n\n\ndef rolling_window(a, window, axis=-1):\n    """"""\n    Make an ndarray with a rolling window along axis.\n    This function is taken from https://github.com/numpy/numpy/pull/31\n    but slightly modified to accept axis option.\n    """"""\n    a = numpy.swapaxes(a, axis, -1)\n    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n    strides = a.strides + (a.strides[-1],)\n    if isinstance(a, numpy.ndarray):\n        rolling = numpy.lib.stride_tricks.as_strided(\n            a, shape=shape, strides=strides)\n    elif isinstance(a, cupy.ndarray):\n        rolling = stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return rolling.swapaxes(-2, axis)\n'"
tests/cupy_tests/linalg_tests/__init__.py,0,b''
tests/cupy_tests/linalg_tests/test_decomposition.py,1,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\nfrom cupy.testing import condition\nimport cupyx\n\n\ndef random_matrix(shape, dtype, scale, sym=False):\n    m, n = shape[-2:]\n    dtype = numpy.dtype(dtype)\n    assert dtype.kind in 'iufc'\n    low_s, high_s = scale\n    bias = None\n    if dtype.kind in 'iu':\n        # For an m \\times n matrix M whose element is in [-0.5, 0.5], it holds\n        # (singular value of M) <= \\sqrt{mn} / 2\n        err = numpy.sqrt(m * n) / 2.\n        low_s += err\n        high_s -= err\n        if dtype.kind in 'u':\n            assert sym, (\n                'generating nonsymmetric matrix with uint cells is not'\n                ' supported')\n            # (singular value of numpy.ones((m, n))) <= \\sqrt{mn}\n            high_s = bias = high_s / (1 + numpy.sqrt(m * n))\n    assert low_s <= high_s\n    a = numpy.random.standard_normal(shape)\n    if dtype.kind == 'c':\n        a = a + 1j * numpy.random.standard_normal(shape)\n    u, s, vh = numpy.linalg.svd(a)\n    if sym:\n        assert m == n\n        vh = u.conj().swapaxes(-1, -2)\n    new_s = numpy.random.uniform(low_s, high_s, s.shape)\n    new_a = numpy.einsum('...ij,...j,...jk->...ik', u, new_s, vh)\n    if bias is not None:\n        new_a += bias\n    if dtype.kind in 'iu':\n        new_a = numpy.rint(new_a)\n    return new_a.astype(dtype)\n\n\n@testing.gpu\nclass TestCholeskyDecomposition(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(atol=1e-3)\n    def check_L(self, array, xp):\n        a = xp.asarray(array)\n        return xp.linalg.cholesky(a)\n\n    @testing.for_dtypes([\n        numpy.int32, numpy.int64, numpy.uint32, numpy.uint64,\n        numpy.float32, numpy.float64, numpy.complex64, numpy.complex128])\n    def test_decomposition(self, dtype):\n        # A positive definite matrix\n        A = random_matrix((5, 5), dtype, scale=(10, 10000), sym=True)\n        self.check_L(A)\n        # np.linalg.cholesky only uses a lower triangle of an array\n        self.check_L(numpy.array([[1, 2], [1, 9]], dtype))\n\n\n@testing.gpu\nclass TestCholeskyInvalid(unittest.TestCase):\n\n    def check_L(self, array):\n        for xp in (numpy, cupy):\n            a = xp.asarray(array)\n            with cupyx.errstate(linalg='raise'):\n                with pytest.raises(numpy.linalg.LinAlgError):\n                    xp.linalg.cholesky(a)\n\n    @testing.for_dtypes([\n        numpy.int32, numpy.int64, numpy.uint32, numpy.uint64,\n        numpy.float32, numpy.float64])\n    def test_decomposition(self, dtype):\n        A = numpy.array([[1, -2], [-2, 1]]).astype(dtype)\n        self.check_L(A)\n\n\n@testing.parameterize(*testing.product({\n    'mode': ['r', 'raw', 'complete', 'reduced'],\n}))\n@testing.gpu\nclass TestQRDecomposition(unittest.TestCase):\n\n    @testing.for_dtypes('fdFD')\n    def check_mode(self, array, mode, dtype):\n        a_cpu = numpy.asarray(array, dtype=dtype)\n        a_gpu = cupy.asarray(array, dtype=dtype)\n        result_cpu = numpy.linalg.qr(a_cpu, mode=mode)\n        result_gpu = cupy.linalg.qr(a_gpu, mode=mode)\n        if isinstance(result_cpu, tuple):\n            for b_cpu, b_gpu in zip(result_cpu, result_gpu):\n                self.assertEqual(b_cpu.dtype, b_gpu.dtype)\n                cupy.testing.assert_allclose(b_cpu, b_gpu, atol=1e-4)\n        else:\n            self.assertEqual(result_cpu.dtype, result_gpu.dtype)\n            cupy.testing.assert_allclose(result_cpu, result_gpu, atol=1e-4)\n\n    @testing.fix_random()\n    @condition.repeat(3, 10)\n    def test_mode(self):\n        self.check_mode(numpy.random.randn(2, 4), mode=self.mode)\n        self.check_mode(numpy.random.randn(3, 3), mode=self.mode)\n        self.check_mode(numpy.random.randn(5, 4), mode=self.mode)\n\n    @testing.with_requires('numpy>=1.16')\n    def test_empty_array(self):\n        self.check_mode(numpy.empty((0, 3)), mode=self.mode)\n        self.check_mode(numpy.empty((3, 0)), mode=self.mode)\n\n\n@testing.parameterize(*testing.product({\n    'full_matrices': [True, False],\n}))\n@testing.fix_random()\n@testing.gpu\nclass TestSVD(unittest.TestCase):\n\n    def setUp(self):\n        self.seed = testing.generate_seed()\n\n    @testing.for_dtypes([\n        numpy.int32, numpy.int64, numpy.uint32, numpy.uint64,\n        numpy.float32, numpy.float64, numpy.complex64, numpy.complex128,\n    ])\n    def check_usv(self, shape, dtype):\n        array = testing.shaped_random(\n            shape, numpy, dtype=dtype, seed=self.seed)\n        a_cpu = numpy.asarray(array, dtype=dtype)\n        a_gpu = cupy.asarray(array, dtype=dtype)\n        result_cpu = numpy.linalg.svd(a_cpu, full_matrices=self.full_matrices)\n        result_gpu = cupy.linalg.svd(a_gpu, full_matrices=self.full_matrices)\n        # Check if the input matrix is not broken\n        cupy.testing.assert_allclose(a_gpu, a_cpu)\n\n        assert len(result_gpu) == 3\n        for i in range(3):\n            assert result_gpu[i].shape == result_cpu[i].shape\n            assert result_gpu[i].dtype == result_cpu[i].dtype\n        u_cpu, s_cpu, vh_cpu = result_cpu\n        u_gpu, s_gpu, vh_gpu = result_gpu\n        cupy.testing.assert_allclose(s_gpu, s_cpu, atol=1e-4)\n\n        k, = s_cpu.shape\n        for j in range(k):\n            # assert corresponding vectors are equal up to rotation (`sign`)\n            uj_cpu = u_cpu[:, j]\n            vj_cpu = vh_cpu[j, :].conj()\n            uj_gpu = cupy.asnumpy(u_gpu[:, j])\n            vj_gpu = cupy.asnumpy(vh_gpu[j, :]).conj()\n            # Use least-squares estimation to compute rotation from cpu result\n            # to gpu result. We know norms of uj_cpu, vj_cpu are 1.\n            u_sign = numpy.vdot(uj_cpu, uj_gpu)\n            v_sign = numpy.vdot(vj_cpu, vj_gpu)\n            numpy.testing.assert_allclose(uj_gpu, u_sign * uj_cpu, atol=1e-4)\n            numpy.testing.assert_allclose(vj_gpu, v_sign * vj_cpu, atol=1e-4)\n            numpy.testing.assert_allclose(abs(u_sign), 1, atol=1e-4)\n            numpy.testing.assert_allclose(abs(v_sign), 1, atol=1e-4)\n            numpy.testing.assert_allclose(u_sign, v_sign, atol=1e-4)\n\n        # assert unitary\n        cupy.testing.assert_allclose(\n            cupy.matmul(u_gpu.T.conj(), u_gpu),\n            numpy.eye(u_gpu.shape[1]),\n            atol=1e-4)\n        cupy.testing.assert_allclose(\n            cupy.matmul(vh_gpu, vh_gpu.T.conj()),\n            numpy.eye(vh_gpu.shape[0]),\n            atol=1e-4)\n\n    @testing.for_dtypes([\n        numpy.int32, numpy.int64, numpy.uint32, numpy.uint64,\n        numpy.float32, numpy.float64, numpy.complex64, numpy.complex128,\n    ])\n    @testing.numpy_cupy_allclose(atol=1e-4)\n    def check_singular(self, shape, xp, dtype):\n        array = testing.shaped_random(shape, xp, dtype=dtype, seed=self.seed)\n        a = xp.asarray(array, dtype=dtype)\n        a_copy = a.copy()\n        result = xp.linalg.svd(\n            a, full_matrices=self.full_matrices, compute_uv=False)\n        # Check if the input matrix is not broken\n        assert (a == a_copy).all()\n        return result\n\n    def check_rank2(self, array):\n        with pytest.raises(numpy.linalg.LinAlgError):\n            cupy.linalg.svd(array, full_matrices=self.full_matrices)\n\n    @condition.repeat(3, 10)\n    def test_svd(self):\n        self.check_usv((3, 7))\n        self.check_usv((2, 2))\n        self.check_usv((7, 3))\n\n    @condition.repeat(3, 10)\n    def test_svd_no_uv(self):\n        self.check_singular((3, 7))\n        self.check_singular((2, 2))\n        self.check_singular((7, 3))\n\n    @condition.repeat(3, 10)\n    def test_rank2(self):\n        self.check_rank2(cupy.random.randn(2, 3, 4).astype(numpy.float32))\n        self.check_rank2(cupy.random.randn(1, 2, 3, 4).astype(numpy.float64))\n\n    @testing.with_requires('numpy>=1.16')\n    def test_empty_array(self):\n        self.check_usv((0, 3))\n        self.check_usv((3, 0))\n        self.check_usv((1, 0))\n\n    @testing.with_requires('numpy>=1.16')\n    @testing.numpy_cupy_array_equal()\n    def test_empty_array_compute_uv_false(self, xp):\n        array = xp.empty((3, 0))\n        return xp.linalg.svd(\n            array, full_matrices=self.full_matrices, compute_uv=False)\n"""
tests/cupy_tests/linalg_tests/test_eigenvalue.py,0,"b""import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.parameterize(*testing.product({\n    'UPLO': ['U', 'L'],\n}))\n@testing.gpu\nclass TestEigenvalue(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_float16=True, no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-4)\n    def test_eigh(self, xp, dtype):\n        a = xp.array([[1, 0, 3], [0, 5, 0], [7, 0, 9]], dtype)\n        w, v = xp.linalg.eigh(a, UPLO=self.UPLO)\n\n        # Order of eigen values is not defined.\n        # They must be sorted to compare them.\n        inds = xp.argsort(w)\n        w = w[inds]\n        v = v[inds]\n        return w, v\n\n    def test_eigh_float16(self):\n        # NumPy's eigh deos not support float16\n        a = cupy.array([[1, 0, 3], [0, 5, 0], [7, 0, 9]], 'e')\n        w, v = cupy.linalg.eigh(a, UPLO=self.UPLO)\n\n        self.assertEqual(w.dtype, numpy.float16)\n        self.assertEqual(v.dtype, numpy.float16)\n\n        na = numpy.array([[1, 0, 3], [0, 5, 0], [7, 0, 9]], 'f')\n        nw, nv = numpy.linalg.eigh(na, UPLO=self.UPLO)\n\n        testing.assert_allclose(w, nw, rtol=1e-3, atol=1e-4)\n        testing.assert_allclose(v, nv, rtol=1e-3, atol=1e-4)\n\n    @testing.for_dtypes('FD')\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-4)\n    def test_eigh_complex(self, xp, dtype):\n        a = xp.array([[1, 2j, 3], [4j, 5, 6j], [7, 8j, 9]], dtype)\n        w, v = xp.linalg.eigh(a, UPLO=self.UPLO)\n\n        # Order of eigen values is not defined.\n        # They must be sorted to compare them.\n        inds = xp.argsort(w)\n        w = w[inds]\n        v = v[inds]\n        return w, v\n\n    @testing.for_all_dtypes(no_float16=True, no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-4)\n    def test_eigvalsh(self, xp, dtype):\n        a = xp.array([[1, 0, 3], [0, 5, 0], [7, 0, 9]], dtype)\n        w = xp.linalg.eigvalsh(a, UPLO=self.UPLO)\n\n        # Order of eigen values is not defined.\n        # They must be sorted to compare them.\n        inds = xp.argsort(w)\n        w = w[inds]\n        return w\n"""
tests/cupy_tests/linalg_tests/test_einsum.py,1,"b'import unittest\nimport warnings\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\ndef _dec_shape(shape, dec):\n    # Test smaller shape\n    return tuple(1 if s == 1 else max(0, s - dec) for s in shape)\n\n\ndef _rand1_shape(shape, prob):\n    # Test broadcast\n    # If diagonals are ""broadcasted"" we can simply:\n    # return tuple(1 if numpy.random.rand() < prob else s for s in shape)\n    table = {}\n    new_shape = []\n    for s in shape:\n        if s not in table:\n            table[s] = 1 if numpy.random.rand() < prob else s\n        new_shape.append(table[s])\n    return tuple(new_shape)\n\n\ndef augument_einsum_testcases(*params):\n    """"""Modify shapes in einsum tests\n\n    Shape parameter should be starts with \'shape_\'.\n    The original parameter is stored as \'_raw_params\'.\n\n    Args:\n        params (sequence of dicts)\n\n    Yields:\n        dict: parameter with modified shapes.\n\n    """"""\n    for dec in range(3):\n        for drop in [False, True]:\n            for param in params:\n                param_new = param.copy()\n                for k in param.keys():\n                    if k.startswith(\'shape_\'):\n                        new_shape = _dec_shape(param[k], dec)\n                        if drop:\n                            prob = numpy.random.rand()\n                            new_shape = _rand1_shape(new_shape, prob)\n                        param_new[k] = new_shape\n                param_new[\'_raw_params\'] = {\n                    \'orig\': param,\n                    \'dec\': dec,\n                    \'drop\': drop,\n                }\n                yield param_new\n\n\nclass TestEinSumError(unittest.TestCase):\n\n    def test_irregular_ellipsis1(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'..\', xp.zeros((2, 2, 2)))\n\n    def test_irregular_ellipsis2(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'...i...\', xp.zeros((2, 2, 2)))\n\n    def test_irregular_ellipsis3(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'i...->...i...\', xp.zeros((2, 2, 2)))\n\n    def test_irregular_ellipsis4(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'...->\', xp.zeros((2, 2, 2)))\n\n    def test_no_arguments(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum()\n\n    def test_one_argument(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'\')\n\n    def test_not_string_subject(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(TypeError):\n                xp.einsum(0, 0)\n\n    def test_bad_argument(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(TypeError):\n                xp.einsum(\'\', 0, bad_arg=0)\n\n    def test_too_many_operands1(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'\', 0, 0)\n\n    def test_too_many_operands2(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\n                    \'i,j\',\n                    xp.array([0, 0]),\n                    xp.array([0, 0]),\n                    xp.array([0, 0]))\n\n    def test_too_few_operands1(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\',\', 0)\n\n    def test_too_many_dimension1(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'i\', 0)\n\n    def test_too_many_dimension2(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'ij\', xp.array([0, 0]))\n\n    def test_too_many_dimension3(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'ijk...->...\', xp.arange(6).reshape(2, 3))\n\n    def test_too_few_dimension(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'i->i\', xp.arange(6).reshape(2, 3))\n\n    def test_invalid_char1(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'i%\', xp.array([0, 0]))\n\n    def test_invalid_char2(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'j$\', xp.array([0, 0]))\n\n    def test_invalid_char3(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'i->&\', xp.array([0, 0]))\n\n    # output subscripts must appear in inumpy.t\n    def test_invalid_output_subscripts1(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'i->ij\', xp.array([0, 0]))\n\n    # output subscripts may only be specified once\n    def test_invalid_output_subscripts2(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'ij->jij\', xp.array([[0, 0], [0, 0]]))\n\n    # output subscripts must not incrudes comma\n    def test_invalid_output_subscripts3(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'ij->i,j\', xp.array([[0, 0], [0, 0]]))\n\n    # dimensions much match when being collapsed\n    def test_invalid_diagonal1(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'ii\', xp.arange(6).reshape(2, 3))\n\n    def test_invalid_diagonal2(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'ii->\', xp.arange(6).reshape(2, 3))\n\n    def test_invalid_diagonal3(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'ii\', xp.arange(3).reshape(1, 3))\n\n    def test_dim_mismatch_char1(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'i,i\', xp.arange(2), xp.arange(3))\n\n    def test_dim_mismatch_ellipsis1(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'...,...\', xp.arange(2), xp.arange(3))\n\n    def test_dim_mismatch_ellipsis2(self):\n        for xp in (numpy, cupy):\n            a = xp.arange(12).reshape(2, 3, 2)\n            with pytest.raises(ValueError):\n                xp.einsum(\'i...,...i\', a, a)\n\n    def test_dim_mismatch_ellipsis3(self):\n        for xp in (numpy, cupy):\n            a = xp.arange(12).reshape(2, 3, 2)\n            with pytest.raises(ValueError):\n                xp.einsum(\'...,...\', a, a[:, :2])\n\n    # invalid -> operator\n    def test_invalid_arrow1(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'i-i\', xp.array([0, 0]))\n\n    def test_invalid_arrow2(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'i>i\', xp.array([0, 0]))\n\n    def test_invalid_arrow3(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'i->->i\', xp.array([0, 0]))\n\n    def test_invalid_arrow4(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(\'i-\', xp.array([0, 0]))\n\n\nclass TestListArgEinSumError(unittest.TestCase):\n\n    def test_invalid_sub1(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(xp.arange(2), [None])\n\n    def test_invalid_sub2(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(xp.arange(2), [0], [1])\n\n    def test_invalid_sub3(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(xp.arange(2), [Ellipsis, 0, Ellipsis])\n\n    def test_dim_mismatch1(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(xp.arange(2), [0], xp.arange(3), [0])\n\n    def test_dim_mismatch2(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(xp.arange(2), [0], xp.arange(3), [0], [0])\n\n    def test_dim_mismatch3(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(xp.arange(6).reshape(2, 3), [0, 0])\n\n    def test_too_many_dims1(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(3, [0])\n\n    def test_too_many_dims2(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(xp.arange(2), [0, 1])\n\n    def test_too_many_dims3(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                xp.einsum(xp.arange(6).reshape(2, 3), [Ellipsis, 0, 1, 2])\n\n\n@testing.parameterize(*augument_einsum_testcases(\n    {\'shape_a\': (2, 3), \'subscripts\': \'ij\'},  # do nothing\n    {\'shape_a\': (2, 3), \'subscripts\': \'...\'},  # do nothing\n    {\'shape_a\': (2, 3), \'subscripts\': \'ji\'},  # transpose\n    {\'shape_a\': (3, 3), \'subscripts\': \'ii->i\'},  # diagonal 2d\n    {\'shape_a\': (3, 3, 3), \'subscripts\': \'jii->ij\'},  # partial diagonal 3d\n    {\'shape_a\': (3, 3, 3), \'subscripts\': \'iji->ij\'},  # partial diagonal 3d\n    {\'shape_a\': (3, 3, 3), \'subscripts\': \'...ii->...i\'},  # partial diagonal 3d\n    {\'shape_a\': (3, 3, 3), \'subscripts\': \'iii->i\'},  # diagonal 3d\n    {\'shape_a\': (2, 3, 4), \'subscripts\': \'ijk->jik\'},  # swap axes\n    {\'shape_a\': (2, 3, 4), \'subscripts\': \'ijk->kij\'},  # swap axes\n    {\'shape_a\': (2, 3, 4), \'subscripts\': \'ijk->ikj\'},  # swap axes\n    {\'shape_a\': (2, 3, 4), \'subscripts\': \'kji->ikj\'},  # swap axes\n    {\'shape_a\': (2, 3, 4), \'subscripts\': \'j...i->i...j\'},  # swap axes\n    {\'shape_a\': (3,), \'subscripts\': \'i->\'},  # sum\n    {\'shape_a\': (3, 3), \'subscripts\': \'ii\'},  # trace\n    {\'shape_a\': (2, 2, 2, 2), \'subscripts\': \'ijkj->kij\'},\n    {\'shape_a\': (2, 2, 2, 2), \'subscripts\': \'ijij->ij\'},\n    {\'shape_a\': (2, 2, 2, 2), \'subscripts\': \'jiji->ij\'},\n    {\'shape_a\': (2, 2, 2, 2), \'subscripts\': \'ii...->...\'},  # trace\n    {\'shape_a\': (2, 2, 2, 2), \'subscripts\': \'i...i->...\'},  # trace\n    {\'shape_a\': (2, 2, 2, 2), \'subscripts\': \'...ii->...\'},  # trace\n    {\'shape_a\': (2, 2, 2, 2), \'subscripts\': \'j...i->...\'},  # sum\n\n    {\'shape_a\': (2, 3), \'subscripts\': \'ij->ij...\'},  # do nothing\n    {\'shape_a\': (2, 3), \'subscripts\': \'ij->i...j\'},  # do nothing\n    {\'shape_a\': (2, 3), \'subscripts\': \'ij->...ij\'},  # do nothing\n    {\'shape_a\': (2, 3), \'subscripts\': \'ij...->ij\'},  # do nothing\n    {\'shape_a\': (2, 3), \'subscripts\': \'i...j->ij\'},  # do nothing\n\n    {\'shape_a\': (), \'subscripts\': \'\'},  # do nothing\n    {\'shape_a\': (), \'subscripts\': \'->\'},  # do nothing\n))\nclass TestEinSumUnaryOperation(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_bool=False)\n    @testing.numpy_cupy_allclose(contiguous_check=False)\n    def test_einsum_unary(self, xp, dtype):\n        a = testing.shaped_arange(self.shape_a, xp, dtype)\n        out = xp.einsum(self.subscripts, a)\n        if xp is not numpy:\n            optimized_out = xp.einsum(self.subscripts, a, optimize=True)\n            testing.assert_allclose(optimized_out, out)\n        return out\n\n    @testing.for_all_dtypes(no_bool=False)\n    @testing.numpy_cupy_equal()\n    def test_einsum_unary_views(self, xp, dtype):\n        a = testing.shaped_arange(self.shape_a, xp, dtype)\n        b = xp.einsum(self.subscripts, a)\n\n        return b.ndim == 0 or b.base is a\n\n    @testing.for_all_dtypes_combination(\n        [\'dtype_a\', \'dtype_out\'],\n        no_bool=False,\n        no_complex=True)  # avoid ComplexWarning\n    @testing.numpy_cupy_allclose(contiguous_check=False)\n    def test_einsum_unary_dtype(self, xp, dtype_a, dtype_out):\n        if not numpy.can_cast(dtype_a, dtype_out):\n            # skip this combination\n            return xp.array([])\n\n        a = testing.shaped_arange(self.shape_a, xp, dtype_a)\n        return xp.einsum(self.subscripts, a, dtype=dtype_out)\n\n\nclass TestEinSumUnaryOperationWithScalar(unittest.TestCase):\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_scalar_int(self, xp, dtype):\n        return xp.asarray(xp.einsum(\'->\', 2, dtype=dtype))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_scalar_float(self, xp, dtype):\n        return xp.asarray(xp.einsum(\'\', 2.0, dtype=dtype))\n\n\n@testing.parameterize(*augument_einsum_testcases(\n    # dot vecvec\n    {\'shape_a\': (3,), \'shape_b\': (3,),\n     \'subscripts\': \'i,i\'},\n    # outer\n    {\'shape_a\': (2,), \'shape_b\': (3,),\n     \'subscripts\': \'i,j\'},\n    # dot matvec\n    {\'shape_a\': (2, 3), \'shape_b\': (3,),\n     \'subscripts\': \'ij,j\'},\n    {\'shape_a\': (2, 3), \'shape_b\': (2,),\n     \'subscripts\': \'ij,i\'},\n    # dot matmat\n    {\'shape_a\': (2, 3), \'shape_b\': (3, 4),\n     \'subscripts\': \'ij,jk\'},\n    # tensordot\n    {\'shape_a\': (3, 4, 2), \'shape_b\': (4, 3, 2),\n     \'subscripts\': \'ijk, jil -> kl\'},\n    {\'shape_a\': (3, 4, 2), \'shape_b\': (4, 2, 3),\n     \'subscripts\': \'i...,...k->ki...\'},\n    {\'shape_a\': (3, 4, 2), \'shape_b\': (4, 3, 2),\n     \'subscripts\': \'ij...,ji...->i...\'},\n    # trace and tensordot and diagonal\n    {\'shape_a\': (2, 3, 2, 4), \'shape_b\': (3, 2, 2),\n     \'subscripts\': \'ijil,jkk->kj\'},\n    {\'shape_a\': (2, 4, 2, 3), \'shape_b\': (3, 2, 4),\n     \'subscripts\': \'i...ij,ji...->...j\'},\n    # broadcast\n    {\'shape_a\': (2, 3, 4), \'shape_b\': (3,),\n     \'subscripts\': \'ij...,j...->ij...\'},\n    {\'shape_a\': (2, 3, 4), \'shape_b\': (3,),\n     \'subscripts\': \'ij...,...j->ij...\'},\n    {\'shape_a\': (2, 3, 4), \'shape_b\': (3,),\n     \'subscripts\': \'ij...,j->ij...\'},\n    {\'shape_a\': (4, 3), \'shape_b\': (3, 2),\n     \'subscripts\': \'ik...,k...->i...\'},\n    {\'shape_a\': (4, 3), \'shape_b\': (3, 2),\n     \'subscripts\': \'ik...,...kj->i...j\'},\n    {\'shape_a\': (4, 3), \'shape_b\': (3, 2),\n     \'subscripts\': \'...k,kj\'},\n    {\'shape_a\': (4, 3), \'shape_b\': (3, 2),\n     \'subscripts\': \'ik,k...->i...\'},\n    {\'shape_a\': (2, 3, 4, 5), \'shape_b\': (4,),\n     \'subscripts\': \'ijkl,k\'},\n    {\'shape_a\': (2, 3, 4, 5), \'shape_b\': (4,),\n     \'subscripts\': \'...kl,k\'},\n    {\'shape_a\': (2, 3, 4, 5), \'shape_b\': (4,),\n     \'subscripts\': \'...kl,k...\'},\n    {\'shape_a\': (1, 1, 1, 2, 3, 2), \'shape_b\': (2, 3, 2, 2),\n     \'subscripts\': \'...lmn,lmno->...o\'},\n))\nclass TestEinSumBinaryOperation(unittest.TestCase):\n    @testing.for_all_dtypes_combination(\n        [\'dtype_a\', \'dtype_b\'],\n        no_bool=False,\n        no_float16=False)\n    @testing.numpy_cupy_allclose(contiguous_check=False)\n    def test_einsum_binary(self, xp, dtype_a, dtype_b):\n        a = testing.shaped_arange(self.shape_a, xp, dtype_a)\n        b = testing.shaped_arange(self.shape_b, xp, dtype_b)\n        return xp.einsum(self.subscripts, a, b)\n\n\nclass TestEinSumBinaryOperationWithScalar(unittest.TestCase):\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(contiguous_check=False)\n    def test_scalar_1(self, xp, dtype):\n        shape_a = (2,)\n        a = testing.shaped_arange(shape_a, xp, dtype)\n        return xp.asarray(xp.einsum(\',i->\', 3, a))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(contiguous_check=False)\n    def test_scalar_2(self, xp, dtype):\n        shape_a = (2,)\n        a = testing.shaped_arange(shape_a, xp, dtype)\n        return xp.asarray(xp.einsum(\'i,->\', a, 4))\n\n\n@testing.parameterize(*augument_einsum_testcases(\n    {\'shape_a\': (2, 3), \'shape_b\': (3, 4), \'shape_c\': (4, 5),\n     \'subscripts\': \'ij,jk,kl\'},\n    {\'shape_a\': (2, 4), \'shape_b\': (2, 3), \'shape_c\': (2,),\n     \'subscripts\': \'ij,ik,i->ijk\'},\n    {\'shape_a\': (2, 4), \'shape_b\': (3, 2), \'shape_c\': (2,),\n     \'subscripts\': \'ij,ki,i->jk\'},\n    {\'shape_a\': (2, 3, 4), \'shape_b\': (2,), \'shape_c\': (3, 4, 2),\n     \'subscripts\': \'i...,i,...i->...i\'},\n    {\'shape_a\': (2, 3, 4), \'shape_b\': (4, 3), \'shape_c\': (3, 3, 4),\n     \'subscripts\': \'a...,...b,c...->abc...\'},\n    {\'shape_a\': (2, 3, 4), \'shape_b\': (3, 4), \'shape_c\': (3, 3, 4),\n     \'subscripts\': \'a...,...,c...->ac...\'},\n    {\'shape_a\': (3, 3, 4), \'shape_b\': (4, 3), \'shape_c\': (2, 3, 4),\n     \'subscripts\': \'a...,...b,c...->abc...\'},\n    {\'shape_a\': (3, 3, 4), \'shape_b\': (3, 4), \'shape_c\': (2, 3, 4),\n     \'subscripts\': \'a...,...,c...->ac...\'},\n))\nclass TestEinSumTernaryOperation(unittest.TestCase):\n    @testing.for_all_dtypes_combination(\n        [\'dtype_a\', \'dtype_b\', \'dtype_c\'],\n        no_bool=False,\n        no_float16=False)\n    @testing.numpy_cupy_allclose(contiguous_check=False)\n    def test_einsum_ternary(self, xp, dtype_a, dtype_b, dtype_c):\n        a = testing.shaped_arange(self.shape_a, xp, dtype_a)\n        b = testing.shaped_arange(self.shape_b, xp, dtype_b)\n        c = testing.shaped_arange(self.shape_c, xp, dtype_c)\n\n        try:\n            out = xp.einsum(self.subscripts, a, b, c, optimize=False)\n        except TypeError:\n            self.assertIs(xp, numpy)\n            out = xp.einsum(self.subscripts, a, b, c)\n\n        if xp is not numpy:  # Avoid numpy issues #11059, #11060\n            for optimize in [\n                    True,  # \'greedy\'\n                    \'optimal\',\n                    [\'einsum_path\', (0, 1), (0, 1)],\n                    [\'einsum_path\', (0, 2), (0, 1)],\n                    [\'einsum_path\', (1, 2), (0, 1)],\n            ]:\n                optimized_out = xp.einsum(\n                    self.subscripts, a, b, c, optimize=optimize)\n                testing.assert_allclose(optimized_out, out)\n        return out\n\n\n@testing.parameterize(*([\n    # memory constraint\n    {\'subscript\': \'a,b,c->abc\', \'opt\': (\'greedy\', 0)},\n    {\'subscript\': \'acdf,jbje,gihb,hfac\', \'opt\': (\'greedy\', 0)},\n] + testing.product({\'subscript\': [\n    # long paths\n    \'acdf,jbje,gihb,hfac,gfac,gifabc,hfac\',\n    \'chd,bde,agbc,hiad,bdi,cgh,agdb\',\n    # edge cases\n    \'eb,cb,fb->cef\',\n    \'dd,fb,be,cdb->cef\',\n    \'bca,cdb,dbf,afc->\',\n    \'dcc,fce,ea,dbf->ab\',\n    \'a,ac,ab,ad,cd,bd,bc->\',\n], \'opt\': [\'greedy\', \'optimal\'],\n})))\nclass TestEinSumLarge(unittest.TestCase):\n\n    def setUp(self):\n        chars = \'abcdefghij\'\n        sizes = numpy.array([2, 3, 4, 5, 4, 3, 2, 6, 5, 4, 3])\n        size_dict = {}\n        for size, char in zip(sizes, chars):\n            size_dict[char] = size\n\n        # Builds views based off initial operands\n        string = self.subscript\n        operands = [string]\n        terms = string.split(\'->\')[0].split(\',\')\n        for term in terms:\n            dims = [size_dict[x] for x in term]\n            operands.append(numpy.random.rand(*dims))\n\n        self.operands = operands\n\n    @testing.numpy_cupy_allclose(contiguous_check=False)\n    def test_einsum(self, xp):\n        # TODO(kataoka): support memory efficient cupy.einsum\n        with warnings.catch_warnings(record=True) as ws:\n            # I hope there\'s no problem with np.einsum for these cases...\n            out = xp.einsum(*self.operands, optimize=self.opt)\n            if xp is not numpy and \\\n                    isinstance(self.opt, tuple):  # with memory limit\n                for w in ws:\n                    self.assertIn(\'memory\', str(w.message))\n            else:\n                self.assertEqual(len(ws), 0)\n        return out\n'"
tests/cupy_tests/linalg_tests/test_norms.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\nimport cupyx\n\n\n@testing.gpu\nclass TestTrace(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_trace(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4, 5), xp, dtype)\n        return a.trace(1, 3, 2)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_external_trace(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4, 5), xp, dtype)\n        return xp.trace(a, 1, 3, 2)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(1,), (2,)],\n    'ord': [-numpy.Inf, -2, -1, 0, 1, 2, 3, numpy.Inf],\n    'axis': [0, None],\n    'keepdims': [True, False],\n}) + testing.product({\n    'shape': [(1, 2), (2, 2)],\n    'ord': [-numpy.Inf, -2, -1, 1, 2, numpy.Inf, 'fro', 'nuc'],\n    'axis': [(0, 1), None],\n    'keepdims': [True, False],\n}) + testing.product({\n    'shape': [(2, 2, 2)],\n    'ord': [-numpy.Inf, -2, -1, 0, 1, 2, 3, numpy.Inf],\n    'axis': [0, 1, 2],\n    'keepdims': [True, False],\n}) + testing.product({\n    'shape': [(2, 2, 2)],\n    'ord': [-numpy.Inf, -1, 1, numpy.Inf, 'fro'],\n    'axis': [(0, 1), (0, 2), (1, 2)],\n    'keepdims': [True, False],\n})\n)\n@testing.gpu\nclass TestNorm(unittest.TestCase):\n\n    # TODO(kmaehashi) Currently dtypes returned from CuPy is not compatible\n    # with NumPy. We should remove `type_check=False` once NumPy is fixed.\n    # See https://github.com/cupy/cupy/pull/875 for details.\n    @testing.for_all_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-4, type_check=False)\n    def test_norm(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        return xp.linalg.norm(a, self.ord, self.axis, self.keepdims)\n\n\n@testing.parameterize(*testing.product({\n    'array': [\n        [[1, 2], [3, 4]],\n        [[1, 2], [1, 2]],\n        [[0, 0], [0, 0]],\n        [1, 2],\n        [0, 1],\n        [0, 0],\n    ],\n    'tol': [None, 1]\n}))\n@testing.gpu\nclass TestMatrixRank(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_float16=True, no_complex=True)\n    @testing.numpy_cupy_array_equal(type_check=True)\n    def test_matrix_rank(self, xp, dtype):\n        a = xp.array(self.array, dtype=dtype)\n        y = xp.linalg.matrix_rank(a, tol=self.tol)\n        if xp is cupy:\n            self.assertIsInstance(y, cupy.ndarray)\n            self.assertEqual(y.shape, ())\n        else:\n            # Note numpy returns numpy scalar or python int\n            y = xp.array(y)\n        return y\n\n\n@testing.gpu\nclass TestDet(unittest.TestCase):\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-4)\n    def test_det(self, xp, dtype):\n        a = testing.shaped_arange((2, 2), xp, dtype) + 1\n        return xp.linalg.det(a)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-4)\n    def test_det_3(self, xp, dtype):\n        a = testing.shaped_arange((2, 2, 2), xp, dtype) + 1\n        return xp.linalg.det(a)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-4)\n    def test_det_4(self, xp, dtype):\n        a = testing.shaped_arange((2, 2, 2, 2), xp, dtype) + 1\n        return xp.linalg.det(a)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-4)\n    def test_det_empty_batch(self, xp, dtype):\n        a = xp.empty((2, 0, 3, 3), dtype)\n        return xp.linalg.det(a)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-4)\n    def test_det_empty_matrix(self, xp, dtype):\n        a = xp.empty((0, 0), dtype)\n        return xp.linalg.det(a)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-4)\n    def test_det_empty_matrices(self, xp, dtype):\n        a = xp.empty((2, 3, 0, 0), dtype)\n        return xp.linalg.det(a)\n\n    @testing.for_float_dtypes(no_float16=True)\n    def test_det_different_last_two_dims(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 2), xp, dtype)\n            with pytest.raises(numpy.linalg.LinAlgError):\n                xp.linalg.det(a)\n\n    @testing.for_float_dtypes(no_float16=True)\n    def test_det_different_last_two_dims_empty_batch(self, dtype):\n        for xp in (numpy, cupy):\n            a = xp.empty((0, 3, 2), dtype)\n            with pytest.raises(numpy.linalg.LinAlgError):\n                xp.linalg.det(a)\n\n    @testing.for_float_dtypes(no_float16=True)\n    def test_det_one_dim(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2,), xp, dtype)\n            with pytest.raises(numpy.linalg.LinAlgError):\n                xp.linalg.det(a)\n\n    @testing.for_float_dtypes(no_float16=True)\n    def test_det_zero_dim(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((), xp, dtype)\n            with pytest.raises(numpy.linalg.LinAlgError):\n                xp.linalg.det(a)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-4)\n    def test_det_singular(self, xp, dtype):\n        a = xp.zeros((2, 3, 3), dtype)\n        return xp.linalg.det(a)\n\n\n@testing.gpu\nclass TestSlogdet(unittest.TestCase):\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-4)\n    def test_slogdet(self, xp, dtype):\n        a = testing.shaped_arange((2, 2), xp, dtype) + 1\n        sign, logdet = xp.linalg.slogdet(a)\n        return xp.array([sign, logdet], dtype)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-4)\n    def test_slogdet_3(self, xp, dtype):\n        a = testing.shaped_arange((2, 2, 2), xp, dtype) + 1\n        sign, logdet = xp.linalg.slogdet(a)\n        return xp.array([sign, logdet], dtype)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-4)\n    def test_slogdet_4(self, xp, dtype):\n        a = testing.shaped_arange((2, 2, 2, 2), xp, dtype) + 1\n        sign, logdet = xp.linalg.slogdet(a)\n        return xp.array([sign, logdet], dtype)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-4)\n    def test_slogdet_singular(self, xp, dtype):\n        a = xp.zeros((3, 3), dtype)\n        sign, logdet = xp.linalg.slogdet(a)\n        return xp.array([sign, logdet], dtype)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-4)\n    def test_slogdet_singular_errstate(self, xp, dtype):\n        a = xp.zeros((3, 3), dtype)\n        with cupyx.errstate(linalg='raise'):\n            # `cupy.linalg.slogdet` internally catches `dev_info < 0` from\n            # cuSOLVER, which should not affect `dev_info > 0` cases.\n            sign, logdet = xp.linalg.slogdet(a)\n        return xp.array([sign, logdet], dtype)\n\n    @testing.for_float_dtypes(no_float16=True)\n    def test_slogdet_one_dim(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2,), xp, dtype)\n            with pytest.raises(numpy.linalg.LinAlgError):\n                xp.linalg.slogdet(a)\n"""
tests/cupy_tests/linalg_tests/test_product.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.parameterize(*testing.product({\n    'shape': [\n        ((2, 3, 4), (3, 4, 2)),\n        ((1, 1), (1, 1)),\n        ((1, 1), (1, 2)),\n        ((1, 2), (2, 1)),\n        ((2, 1), (1, 1)),\n        ((1, 2), (2, 3)),\n        ((2, 1), (1, 3)),\n        ((2, 3), (3, 1)),\n        ((2, 3), (3, 4)),\n        ((0, 3), (3, 4)),\n        ((2, 3), (3, 0)),\n        ((0, 3), (3, 0)),\n        ((3, 0), (0, 4)),\n        ((2, 3, 0), (3, 0, 2)),\n        ((0, 0), (0, 0)),\n        ((3,), (3,)),\n        ((2,), (2, 4)),\n        ((4, 2), (2,)),\n    ],\n    'trans_a': [True, False],\n    'trans_b': [True, False],\n}))\n@testing.gpu\nclass TestDot(unittest.TestCase):\n\n    @testing.for_all_dtypes_combination(['dtype_a', 'dtype_b'])\n    @testing.numpy_cupy_allclose()\n    def test_dot(self, xp, dtype_a, dtype_b):\n        shape_a, shape_b = self.shape\n        if self.trans_a:\n            a = testing.shaped_arange(shape_a[::-1], xp, dtype_a).T\n        else:\n            a = testing.shaped_arange(shape_a, xp, dtype_a)\n        if self.trans_b:\n            b = testing.shaped_arange(shape_b[::-1], xp, dtype_b).T\n        else:\n            b = testing.shaped_arange(shape_b, xp, dtype_b)\n        return xp.dot(a, b)\n\n    @testing.for_float_dtypes(name='dtype_a')\n    @testing.for_float_dtypes(name='dtype_b')\n    @testing.for_float_dtypes(name='dtype_c')\n    @testing.numpy_cupy_allclose(accept_error=ValueError)\n    def test_dot_with_out(self, xp, dtype_a, dtype_b, dtype_c):\n        shape_a, shape_b = self.shape\n        if self.trans_a:\n            a = testing.shaped_arange(shape_a[::-1], xp, dtype_a).T\n        else:\n            a = testing.shaped_arange(shape_a, xp, dtype_a)\n        if self.trans_b:\n            b = testing.shaped_arange(shape_b[::-1], xp, dtype_b).T\n        else:\n            b = testing.shaped_arange(shape_b, xp, dtype_b)\n        if a.ndim == 0 or b.ndim == 0:\n            shape_c = shape_a + shape_b\n        else:\n            shape_c = shape_a[:-1] + shape_b[:-2] + shape_b[-1:]\n        c = xp.empty(shape_c, dtype=dtype_c)\n        out = xp.dot(a, b, out=c)\n        self.assertIs(out, c)\n        return c\n\n\n@testing.parameterize(*testing.product({\n    'params': [\n        #  Test for 0 dimension\n        ((3, ), (3, ), -1, -1, -1),\n        #  Test for basic cases\n        ((1, 2), (1, 2), -1, -1, 1),\n        ((1, 3), (1, 3), 1, -1, -1),\n        ((1, 2), (1, 3), -1, -1, 1),\n        ((2, 2), (1, 3), -1, -1, 0),\n        ((3, 3), (1, 2), 0, -1, -1),\n        ((0, 3), (0, 3), -1, -1, -1),\n        #  Test for higher dimensions\n        ((2, 0, 3), (2, 0, 3), 0, 0, 0),\n        ((2, 4, 5, 3), (2, 4, 5, 3), -1, -1, 0),\n        ((2, 4, 5, 2), (2, 4, 5, 2), 0, 0, -1),\n    ],\n}))\n@testing.gpu\nclass TestCrossProduct(unittest.TestCase):\n\n    @testing.for_all_dtypes_combination(['dtype_a', 'dtype_b'])\n    @testing.numpy_cupy_allclose()\n    def test_cross(self, xp, dtype_a, dtype_b):\n        if dtype_a == dtype_b == numpy.bool_:\n            # cross does not support bool-bool inputs.\n            return xp.array(True)\n        shape_a, shape_b, axisa, axisb, axisc = self.params\n        a = testing.shaped_arange(shape_a, xp, dtype_a)\n        b = testing.shaped_arange(shape_b, xp, dtype_b)\n        return xp.cross(a, b, axisa, axisb, axisc)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [\n        ((), ()),\n        ((), (2, 4)),\n        ((4, 2), ()),\n    ],\n    'trans_a': [True, False],\n    'trans_b': [True, False],\n}))\n@testing.gpu\nclass TestDotFor0Dim(unittest.TestCase):\n\n    @testing.for_all_dtypes_combination(['dtype_a', 'dtype_b'])\n    @testing.numpy_cupy_allclose(contiguous_check=False)\n    def test_dot(self, xp, dtype_a, dtype_b):\n        shape_a, shape_b = self.shape\n        if self.trans_a:\n            a = testing.shaped_arange(shape_a[::-1], xp, dtype_a).T\n        else:\n            a = testing.shaped_arange(shape_a, xp, dtype_a)\n        if self.trans_b:\n            b = testing.shaped_arange(shape_b[::-1], xp, dtype_b).T\n        else:\n            b = testing.shaped_arange(shape_b, xp, dtype_b)\n        return xp.dot(a, b)\n\n\n@testing.gpu\nclass TestProduct(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_dot_vec1(self, xp, dtype):\n        a = testing.shaped_arange((2,), xp, dtype)\n        b = testing.shaped_arange((2,), xp, dtype)\n        return xp.dot(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_dot_vec2(self, xp, dtype):\n        a = testing.shaped_arange((2,), xp, dtype)\n        b = testing.shaped_arange((2, 1), xp, dtype)\n        return xp.dot(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_dot_vec3(self, xp, dtype):\n        a = testing.shaped_arange((1, 2), xp, dtype)\n        b = testing.shaped_arange((2,), xp, dtype)\n        return xp.dot(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_transposed_dot(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype).transpose(1, 0, 2)\n        b = testing.shaped_arange((2, 3, 4), xp, dtype).transpose(0, 2, 1)\n        return xp.dot(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_transposed_dot_with_out(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype).transpose(1, 0, 2)\n        b = testing.shaped_arange((4, 2, 3), xp, dtype).transpose(2, 0, 1)\n        c = xp.ndarray((3, 2, 3, 2), dtype=dtype)\n        xp.dot(a, b, out=c)\n        return c\n\n    @testing.for_all_dtypes()\n    def test_transposed_dot_with_out_f_contiguous(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp, dtype).transpose(1, 0, 2)\n            b = testing.shaped_arange((4, 2, 3), xp, dtype).transpose(2, 0, 1)\n            c = xp.ndarray((3, 2, 3, 2), dtype=dtype, order='F')\n            with pytest.raises(ValueError):\n                # Only C-contiguous array is acceptable\n                xp.dot(a, b, out=c)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_dot_with_single_elem_array1(self, xp, dtype):\n        a = testing.shaped_arange((3, 1), xp, dtype)\n        b = xp.array([[2]], dtype=dtype)\n        return xp.dot(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_dot_with_single_elem_array2(self, xp, dtype):\n        a = xp.array([[2]], dtype=dtype)\n        b = testing.shaped_arange((1, 3), xp, dtype)\n        return xp.dot(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_vdot(self, xp, dtype):\n        a = testing.shaped_arange((5,), xp, dtype)\n        b = testing.shaped_reverse_arange((5,), xp, dtype)\n        return xp.vdot(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_reversed_vdot(self, xp, dtype):\n        a = testing.shaped_arange((5,), xp, dtype)[::-1]\n        b = testing.shaped_reverse_arange((5,), xp, dtype)[::-1]\n        return xp.vdot(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_multidim_vdot(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = testing.shaped_arange((2, 2, 2, 3), xp, dtype)\n        return xp.vdot(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_transposed_multidim_vdot(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype).transpose(2, 0, 1)\n        b = testing.shaped_arange(\n            (2, 2, 2, 3), xp, dtype).transpose(1, 3, 0, 2)\n        return xp.vdot(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_inner(self, xp, dtype):\n        a = testing.shaped_arange((5,), xp, dtype)\n        b = testing.shaped_reverse_arange((5,), xp, dtype)\n        return xp.inner(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_reversed_inner(self, xp, dtype):\n        a = testing.shaped_arange((5,), xp, dtype)[::-1]\n        b = testing.shaped_reverse_arange((5,), xp, dtype)[::-1]\n        return xp.inner(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_multidim_inner(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = testing.shaped_arange((3, 2, 4), xp, dtype)\n        return xp.inner(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_transposed_higher_order_inner(self, xp, dtype):\n        a = testing.shaped_arange((2, 4, 3), xp, dtype).transpose(2, 0, 1)\n        b = testing.shaped_arange((4, 2, 3), xp, dtype).transpose(1, 2, 0)\n        return xp.inner(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_outer(self, xp, dtype):\n        a = testing.shaped_arange((5,), xp, dtype)\n        b = testing.shaped_arange((4,), xp, dtype)\n        return xp.outer(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_reversed_outer(self, xp, dtype):\n        a = testing.shaped_arange((5,), xp, dtype)\n        b = testing.shaped_arange((4,), xp, dtype)\n        return xp.outer(a[::-1], b[::-1])\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_multidim_outer(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        b = testing.shaped_arange((4, 5), xp, dtype)\n        return xp.outer(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_tensordot(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = testing.shaped_arange((3, 4, 5), xp, dtype)\n        return xp.tensordot(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_transposed_tensordot(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype).transpose(1, 0, 2)\n        b = testing.shaped_arange((4, 3, 2), xp, dtype).transpose(2, 0, 1)\n        return xp.tensordot(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_tensordot_with_int_axes(self, xp, dtype):\n        if dtype in (numpy.uint8, numpy.int8, numpy.uint16, numpy.int16):\n            a = testing.shaped_arange((1, 2, 3), xp, dtype)\n            b = testing.shaped_arange((2, 3, 1), xp, dtype)\n            return xp.tensordot(a, b, axes=2)\n        else:\n            a = testing.shaped_arange((2, 3, 4, 5), xp, dtype)\n            b = testing.shaped_arange((3, 4, 5, 2), xp, dtype)\n            return xp.tensordot(a, b, axes=3)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_transposed_tensordot_with_int_axes(self, xp, dtype):\n        if dtype in (numpy.uint8, numpy.int8, numpy.uint16, numpy.int16):\n            # Avoid overflow\n            a = testing.shaped_arange(\n                (1, 2, 3), xp, dtype).transpose(2, 0, 1)\n            b = testing.shaped_arange(\n                (3, 2, 1), xp, dtype).transpose(2, 1, 0)\n            return xp.tensordot(a, b, axes=2)\n        else:\n            a = testing.shaped_arange(\n                (2, 3, 4, 5), xp, dtype).transpose(2, 0, 3, 1)\n            b = testing.shaped_arange(\n                (5, 4, 3, 2), xp, dtype).transpose(3, 0, 2, 1)\n            return xp.tensordot(a, b, axes=3)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_tensordot_with_list_axes(self, xp, dtype):\n        if dtype in (numpy.uint8, numpy.int8, numpy.uint16, numpy.int16):\n            # Avoid overflow\n            a = testing.shaped_arange((1, 2, 3), xp, dtype)\n            b = testing.shaped_arange((3, 1, 2), xp, dtype)\n            return xp.tensordot(a, b, axes=([2, 1], [0, 2]))\n        else:\n            a = testing.shaped_arange((2, 3, 4, 5), xp, dtype)\n            b = testing.shaped_arange((3, 5, 4, 2), xp, dtype)\n            return xp.tensordot(a, b, axes=([3, 2, 1], [1, 2, 0]))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_transposed_tensordot_with_list_axes(self, xp, dtype):\n        if dtype in (numpy.uint8, numpy.int8, numpy.uint16, numpy.int16):\n            # Avoid overflow\n            a = testing.shaped_arange(\n                (1, 2, 3), xp, dtype).transpose(2, 0, 1)\n            b = testing.shaped_arange(\n                (2, 3, 1), xp, dtype).transpose(0, 2, 1)\n            return xp.tensordot(a, b, axes=([2, 0], [0, 2]))\n        else:\n            a = testing.shaped_arange(\n                (2, 3, 4, 5), xp, dtype).transpose(2, 0, 3, 1)\n            b = testing.shaped_arange(\n                (3, 5, 4, 2), xp, dtype).transpose(3, 0, 2, 1)\n            return xp.tensordot(a, b, axes=([2, 0, 3], [3, 2, 1]))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_tensordot_zero_dim(self, xp, dtype):\n        a = xp.array(2, dtype=dtype)\n        b = testing.shaped_arange((3, 4, 2), xp, dtype)\n        return xp.tensordot(a, b, axes=0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_kron(self, xp, dtype):\n        a = testing.shaped_arange((4,), xp, dtype)\n        b = testing.shaped_arange((5,), xp, dtype)\n        return xp.kron(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_reversed_kron(self, xp, dtype):\n        a = testing.shaped_arange((4,), xp, dtype)\n        b = testing.shaped_arange((5,), xp, dtype)\n        return xp.kron(a[::-1], b[::-1])\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_multidim_kron(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = testing.shaped_arange((4, 2, 3), xp, dtype)\n        return xp.kron(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_zerodim_kron(self, xp, dtype):\n        a = xp.array(2, dtype=dtype)\n        b = testing.shaped_arange((4, 5), xp, dtype)\n        return xp.kron(a, b)\n\n\n@testing.parameterize(*testing.product({\n    'params': [\n        ((0, 0), 2),\n        ((0, 0), (1, 0)),\n        ((0, 0, 0), 2),\n        ((0, 0, 0), 3),\n        ((0, 0, 0), ([2, 1], [0, 2])),\n        ((0, 0, 0), ([0, 2, 1], [1, 2, 0])),\n    ],\n}))\n@testing.gpu\nclass TestProductZeroLength(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_tensordot_zero_length(self, xp, dtype):\n        shape, axes = self.params\n        a = testing.shaped_arange(shape, xp, dtype)\n        return xp.tensordot(a, a, axes=axes)\n\n\nclass TestMatrixPower(unittest.TestCase):\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_matrix_power_0(self, xp, dtype):\n        a = testing.shaped_arange((3, 3), xp, dtype)\n        return xp.linalg.matrix_power(a, 0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_matrix_power_1(self, xp, dtype):\n        a = testing.shaped_arange((3, 3), xp, dtype)\n        return xp.linalg.matrix_power(a, 1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_matrix_power_2(self, xp, dtype):\n        a = testing.shaped_arange((3, 3), xp, dtype)\n        return xp.linalg.matrix_power(a, 2)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_matrix_power_3(self, xp, dtype):\n        a = testing.shaped_arange((3, 3), xp, dtype)\n        return xp.linalg.matrix_power(a, 3)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-5)\n    def test_matrix_power_inv1(self, xp, dtype):\n        a = testing.shaped_arange((3, 3), xp, dtype)\n        a = a * a % 30\n        return xp.linalg.matrix_power(a, -1)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-5)\n    def test_matrix_power_inv2(self, xp, dtype):\n        a = testing.shaped_arange((3, 3), xp, dtype)\n        a = a * a % 30\n        return xp.linalg.matrix_power(a, -2)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4)\n    def test_matrix_power_inv3(self, xp, dtype):\n        a = testing.shaped_arange((3, 3), xp, dtype)\n        a = a * a % 30\n        return xp.linalg.matrix_power(a, -3)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_matrix_power_of_two(self, xp, dtype):\n        a = xp.eye(23, k=17, dtype=dtype) + xp.eye(23, k=-6, dtype=dtype)\n        return xp.linalg.matrix_power(a, 1 << 50)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_matrix_power_large(self, xp, dtype):\n        a = xp.eye(23, k=17, dtype=dtype) + xp.eye(23, k=-6, dtype=dtype)\n        return xp.linalg.matrix_power(a, 123456789123456789)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose()\n    def test_matrix_power_invlarge(self, xp, dtype):\n        a = xp.eye(23, k=17, dtype=dtype) + xp.eye(23, k=-6, dtype=dtype)\n        return xp.linalg.matrix_power(a, -987654321987654321)\n"""
tests/cupy_tests/linalg_tests/test_solve.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\nfrom cupy.testing import condition\nimport cupyx\n\n\n@testing.gpu\n@testing.fix_random()\nclass TestSolve(unittest.TestCase):\n\n    @testing.for_dtypes('fdFD')\n    # TODO(kataoka): Fix contiguity\n    @testing.numpy_cupy_allclose(atol=1e-3, contiguous_check=False)\n    def check_x(self, a_shape, b_shape, xp, dtype):\n        a = testing.shaped_random(a_shape, xp, dtype=dtype, seed=0)\n        b = testing.shaped_random(b_shape, xp, dtype=dtype, seed=1)\n        a_copy = a.copy()\n        b_copy = b.copy()\n        result = xp.linalg.solve(a, b)\n        cupy.testing.assert_array_equal(a_copy, a)\n        cupy.testing.assert_array_equal(b_copy, b)\n        return result\n\n    def test_solve(self):\n        self.check_x((4, 4), (4,))\n        self.check_x((5, 5), (5, 2))\n        self.check_x((2, 4, 4), (2, 4,))\n        self.check_x((2, 5, 5), (2, 5, 2))\n        self.check_x((2, 3, 2, 2), (2, 3, 2,))\n        self.check_x((2, 3, 3, 3), (2, 3, 3, 2))\n\n    def check_shape(self, a_shape, b_shape, error_type):\n        for xp in (numpy, cupy):\n            a = xp.random.rand(*a_shape)\n            b = xp.random.rand(*b_shape)\n            with pytest.raises(error_type):\n                xp.linalg.solve(a, b)\n\n    def test_invalid_shape(self):\n        self.check_shape((2, 3), (4,), numpy.linalg.LinAlgError)\n        self.check_shape((3, 3), (2,), ValueError)\n        self.check_shape((3, 3), (2, 2), ValueError)\n        self.check_shape((3, 3, 4), (3,), numpy.linalg.LinAlgError)\n        self.check_shape((2, 3, 3), (3,), ValueError)\n\n\n@testing.parameterize(*testing.product({\n    'a_shape': [(2, 3, 6), (3, 4, 4, 3)],\n    'dtype': [numpy.float32, numpy.float64],\n    'axes': [None, (0, 2)],\n}))\n@testing.fix_random()\n@testing.gpu\nclass TestTensorSolve(unittest.TestCase):\n\n    def setUp(self):\n        self.a = numpy.random.randint(\n            0, 10, size=self.a_shape).astype(self.dtype)\n        self.b = numpy.random.randint(\n            0, 10, size=self.a_shape[:2]).astype(self.dtype)\n\n    @testing.numpy_cupy_allclose(atol=0.02)\n    def test_tensorsolve(self, xp):\n        a = xp.array(self.a)\n        b = xp.array(self.b)\n        return xp.linalg.tensorsolve(a, b, axes=self.axes)\n\n\n@testing.gpu\nclass TestInv(unittest.TestCase):\n\n    @testing.for_dtypes('fdFD')\n    @condition.retry(10)\n    def check_x(self, a_shape, dtype):\n        a_cpu = numpy.random.randint(0, 10, size=a_shape).astype(dtype)\n        a_gpu = cupy.asarray(a_cpu)\n        a_gpu_copy = a_gpu.copy()\n        result_cpu = numpy.linalg.inv(a_cpu)\n        result_gpu = cupy.linalg.inv(a_gpu)\n        self.assertEqual(result_cpu.dtype, result_gpu.dtype)\n        cupy.testing.assert_allclose(result_cpu, result_gpu, atol=1e-3)\n        cupy.testing.assert_array_equal(a_gpu_copy, a_gpu)\n\n    def check_shape(self, a_shape):\n        a = cupy.random.rand(*a_shape)\n        with self.assertRaises(numpy.linalg.LinAlgError):\n            cupy.linalg.inv(a)\n\n    def test_inv(self):\n        self.check_x((3, 3))\n        self.check_x((4, 4))\n        self.check_x((5, 5))\n        self.check_x((2, 5, 5))\n        self.check_x((3, 4, 4))\n        self.check_x((4, 2, 3, 3))\n\n    def test_invalid_shape(self):\n        self.check_shape((2, 3))\n        self.check_shape((4, 1))\n        self.check_shape((4, 3, 2))\n        self.check_shape((2, 4, 3))\n\n\n@testing.gpu\nclass TestInvInvalid(unittest.TestCase):\n\n    @testing.for_float_dtypes(no_float16=True)\n    def test_inv(self, dtype):\n        for xp in (numpy, cupy):\n            a = xp.array([[1, 2], [2, 4]]).astype(dtype)\n            with cupyx.errstate(linalg='raise'):\n                with pytest.raises(numpy.linalg.LinAlgError):\n                    xp.linalg.inv(a)\n\n    @testing.for_float_dtypes(no_float16=True)\n    def test_batched_inv(self, dtype):\n        for xp in (numpy, cupy):\n            a = xp.array([[[1, 2], [2, 4]]]).astype(dtype)\n            assert a.ndim >= 3  # CuPy internally uses a batched function.\n            with cupyx.errstate(linalg='raise'):\n                with pytest.raises(numpy.linalg.LinAlgError):\n                    xp.linalg.inv(a)\n\n\n@testing.gpu\nclass TestPinv(unittest.TestCase):\n\n    @testing.for_dtypes('fdFD')\n    @condition.retry(10)\n    def check_x(self, a_shape, rcond, dtype):\n        a_gpu = testing.shaped_random(a_shape, dtype=dtype)\n        a_cpu = cupy.asnumpy(a_gpu)\n        a_gpu_copy = a_gpu.copy()\n        result_cpu = numpy.linalg.pinv(a_cpu, rcond=rcond)\n        result_gpu = cupy.linalg.pinv(a_gpu, rcond=rcond)\n\n        self.assertEqual(result_cpu.dtype, result_gpu.dtype)\n        cupy.testing.assert_allclose(result_cpu, result_gpu, atol=1e-3)\n        cupy.testing.assert_array_equal(a_gpu_copy, a_gpu)\n\n    def check_shape(self, a_shape, rcond):\n        a = cupy.random.rand(*a_shape)\n        with self.assertRaises(numpy.linalg.LinAlgError):\n            cupy.linalg.pinv(a)\n\n    def test_pinv(self):\n        self.check_x((3, 3), rcond=1e-15)\n        self.check_x((2, 4), rcond=1e-15)\n        self.check_x((3, 2), rcond=1e-15)\n\n        self.check_x((4, 4), rcond=0.3)\n        self.check_x((2, 5), rcond=0.5)\n        self.check_x((5, 3), rcond=0.6)\n\n    def test_invalid_shape(self):\n        self.check_shape((2, 3, 4), rcond=1e-15)\n        self.check_shape((2, 3, 4), rcond=0.5)\n        self.check_shape((4, 3, 2, 1), rcond=1e-14)\n        self.check_shape((4, 3, 2, 1), rcond=0.1)\n\n\n@testing.gpu\nclass TestLstsq(unittest.TestCase):\n\n    @testing.for_float_dtypes(no_float16=True)\n    def check_lstsq_solution(self, a_shape, b_shape, seed, rcond, dtype,\n                             singular=False):\n        numpy.random.seed(seed)\n        a_cpu = numpy.random.randint(0, 10, size=a_shape).astype(dtype)\n        if singular:\n            # make one row a linear combination of the others\n            a_cpu[-1] = numpy.sum(a_cpu[0:-1], axis=0)\n        b_cpu = numpy.random.randint(0, 10, size=b_shape).astype(dtype)\n        a_gpu = cupy.asarray(a_cpu)\n        b_gpu = cupy.asarray(b_cpu)\n        a_gpu_copy = a_gpu.copy()\n        b_gpu_copy = b_gpu.copy()\n        x_cpu, resids_cpu, rank_cpu, s_cpu = numpy.linalg.lstsq(a_cpu,\n                                                                b_cpu,\n                                                                rcond=rcond)\n        x_gpu, resids_gpu, rank_gpu, s_gpu = cupy.linalg.lstsq(a_gpu,\n                                                               b_gpu,\n                                                               rcond=rcond)\n        self.assertEqual(x_cpu.dtype, x_gpu.dtype)\n        # check the least squares solutions are close\n        # if a is singular, no guarantee that x_cpu will be close to x_gpu\n        if not singular:\n            cupy.testing.assert_allclose(x_cpu, x_gpu, atol=1e-3)\n        cupy.testing.assert_allclose(resids_cpu, resids_gpu, atol=1e-3)\n        self.assertEqual(rank_cpu, rank_gpu)\n        cupy.testing.assert_allclose(s_cpu, s_gpu, atol=1e-3)\n        # check that lstsq did not modify arrays\n        cupy.testing.assert_array_equal(a_gpu_copy, a_gpu)\n        cupy.testing.assert_array_equal(b_gpu_copy, b_gpu)\n\n    def check_invalid_shapes(self, a_shape, b_shape):\n        a = cupy.random.rand(*a_shape)\n        b = cupy.random.rand(*b_shape)\n        with self.assertRaises(numpy.linalg.LinAlgError):\n            cupy.linalg.lstsq(a, b)\n\n    def test_lstsq_solutions(self):\n        # Comapres numpy.linalg.lstsq and cupy.linalg.lstsq solutions for:\n        #   a shapes range from (3, 3) to (5, 3) and (3, 5)\n        #   b shapes range from (i, 3) to (i, )\n        #   sets a random seed for deterministic testing\n        for i in range(3, 6):\n            for j in range(3, 6):\n                for k in range(2, 4):\n                    seed = i + j + k\n                    # check when b has shape (i, k)\n                    self.check_lstsq_solution((i, j), (i, k), seed,\n                                              rcond=1e-15)\n                    self.check_lstsq_solution((i, j), (i, k), seed,\n                                              rcond=0.5)\n                    self.check_lstsq_solution((i, j), (i, k), seed,\n                                              rcond=1e-7, singular=True)\n                # check when b has shape (i, )\n                self.check_lstsq_solution((i, j), (i, ), seed+1, rcond=1e-15)\n                self.check_lstsq_solution((i, j), (i, ), seed+1, rcond=0.5)\n                self.check_lstsq_solution((i, j), (i, ), seed+1, rcond=1e-7,\n                                          singular=True)\n\n    def test_invalid_shapes(self):\n        self.check_invalid_shapes((4, 3), (3, ))\n        self.check_invalid_shapes((3, 3, 3), (2, 2))\n        self.check_invalid_shapes((3, 3, 3), (3, 3))\n        self.check_invalid_shapes((3, 3), (3, 3, 3))\n        self.check_invalid_shapes((2, 2), (10, ))\n        self.check_invalid_shapes((3, 3), (2, 2))\n        self.check_invalid_shapes((4, 3), (10, 3, 3))\n\n\n@testing.gpu\nclass TestTensorInv(unittest.TestCase):\n\n    @testing.for_float_dtypes(no_float16=True)\n    @condition.retry(10)\n    def check_x(self, a_shape, ind, dtype):\n        a_cpu = numpy.random.randint(0, 10, size=a_shape).astype(dtype)\n        a_gpu = cupy.asarray(a_cpu)\n        a_gpu_copy = a_gpu.copy()\n        result_cpu = numpy.linalg.tensorinv(a_cpu, ind=ind)\n        result_gpu = cupy.linalg.tensorinv(a_gpu, ind=ind)\n        self.assertEqual(result_cpu.dtype, result_gpu.dtype)\n        cupy.testing.assert_allclose(result_cpu, result_gpu, atol=1e-3)\n        cupy.testing.assert_array_equal(a_gpu_copy, a_gpu)\n\n    def check_shape(self, a_shape, ind):\n        a = cupy.random.rand(*a_shape)\n        with self.assertRaises(numpy.linalg.LinAlgError):\n            cupy.linalg.tensorinv(a, ind=ind)\n\n    def check_ind(self, a_shape, ind):\n        a = cupy.random.rand(*a_shape)\n        with self.assertRaises(ValueError):\n            cupy.linalg.tensorinv(a, ind=ind)\n\n    def test_tensorinv(self):\n        self.check_x((12, 3, 4), ind=1)\n        self.check_x((3, 8, 24), ind=2)\n        self.check_x((18, 3, 3, 2), ind=1)\n        self.check_x((1, 4, 2, 2), ind=2)\n        self.check_x((2, 3, 5, 30), ind=3)\n        self.check_x((24, 2, 2, 3, 2), ind=1)\n        self.check_x((3, 4, 2, 3, 2), ind=2)\n        self.check_x((1, 2, 3, 2, 3), ind=3)\n        self.check_x((3, 2, 1, 2, 12), ind=4)\n\n    def test_invalid_shape(self):\n        self.check_shape((2, 3, 4), ind=1)\n        self.check_shape((1, 2, 3, 4), ind=3)\n\n    def test_invalid_index(self):\n        self.check_ind((12, 3, 4), ind=-1)\n        self.check_ind((18, 3, 3, 2), ind=0)\n"""
tests/cupy_tests/logic_tests/__init__.py,0,b''
tests/cupy_tests/logic_tests/test_comparison.py,0,"b""import operator\nimport unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestComparison(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_binary(self, name, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        b = testing.shaped_reverse_arange((2, 3), xp, dtype)\n        return getattr(xp, name)(a, b)\n\n    def test_greater(self):\n        self.check_binary('greater')\n\n    def test_greater_equal(self):\n        self.check_binary('greater_equal')\n\n    def test_less(self):\n        self.check_binary('less')\n\n    def test_less_equal(self):\n        self.check_binary('less_equal')\n\n    def test_not_equal(self):\n        self.check_binary('not_equal')\n\n    def test_equal(self):\n        self.check_binary('equal')\n\n\n@testing.gpu\nclass TestComparisonOperator(unittest.TestCase):\n\n    operators = [\n        operator.lt, operator.le,\n        operator.eq, operator.ne,\n        operator.gt, operator.ge,\n    ]\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_binary_npscalar_array(self, xp, dtype):\n        a = numpy.int16(3)\n        b = testing.shaped_arange((2, 3), xp, dtype)\n        return [op(a, b) for op in self.operators]\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_binary_pyscalar_array(self, xp, dtype):\n        a = 3.0\n        b = testing.shaped_arange((2, 3), xp, dtype)\n        return [op(a, b) for op in self.operators]\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_binary_array_npscalar(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        b = numpy.float32(3.0)\n        return [op(a, b) for op in self.operators]\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_binary_array_pyscalar(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        b = 3\n        return [op(a, b) for op in self.operators]\n\n\nclass TestArrayEqual(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_array_equal_not_equal(self, xp, dtype):\n        a = xp.array([1, 2, 3, 4], dtype=dtype)\n        b = xp.array([1, 2, 4, 5], dtype=dtype)\n        return xp.array_equal(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_array_equal_is_equal(self, xp, dtype):\n        a = xp.array([1, 2, 3, 4], dtype=dtype)\n        b = xp.array([1, 2, 3, 4], dtype=dtype)\n        return xp.array_equal(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_array_equal_diff_length(self, xp, dtype):\n        a = xp.array([1, 2, 3, 4], dtype=dtype)\n        b = xp.array([1, 2, 3], dtype=dtype)\n        return xp.array_equal(a, b)\n\n    @testing.numpy_cupy_equal()\n    def test_array_equal_diff_dtypes_not_equal(self, xp):\n        a = xp.array([0.9e-5, 1.1e-5, 100.5, 10.5])\n        b = xp.array([0, 0, 1000, 1000])\n        return xp.array_equal(a, b)\n\n    @testing.numpy_cupy_equal()\n    def test_array_equal_diff_dtypes_is_equal(self, xp):\n        a = xp.array([0.0, 1.0, 100.0, 10.0])\n        b = xp.array([0, 1, 100, 10])\n        return xp.array_equal(a, b)\n\n    @testing.numpy_cupy_equal()\n    def test_array_equal_broadcast_not_allowed(self, xp):\n        a = xp.array([1, 1, 1, 1])\n        b = xp.array([1])\n        return xp.array_equal(a, b)\n\n\nclass TestAllclose(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_allclose_finite(self, xp, dtype):\n        a = xp.array([0.9e-5, 1.1e-5, 1000 + 1e-4, 1000 - 1e-4], dtype=dtype)\n        b = xp.array([0, 0, 1000, 1000], dtype=dtype)\n        return xp.allclose(a, b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_allclose_min_int(self, xp, dtype):\n        a = xp.array([0], dtype=dtype)\n        b = xp.array([numpy.iinfo('i').min], dtype=dtype)\n        return xp.allclose(a, b)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_allclose_infinite(self, xp, dtype):\n        nan = float('nan')\n        inf = float('inf')\n        ninf = float('-inf')\n        a = xp.array([0, nan, nan, 0, inf, ninf], dtype=dtype)\n        b = xp.array([0, nan, 0, nan, inf, ninf], dtype=dtype)\n        return xp.allclose(a, b)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_allclose_infinite_equal_nan(self, xp, dtype):\n        nan = float('nan')\n        inf = float('inf')\n        ninf = float('-inf')\n        a = xp.array([0, nan, inf, ninf], dtype=dtype)\n        b = xp.array([0, nan, inf, ninf], dtype=dtype)\n        return xp.allclose(a, b, equal_nan=True)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_allclose_array_scalar(self, xp, dtype):\n        a = xp.array([0.9e-5, 1.1e-5], dtype=dtype)\n        b = xp.dtype(xp.dtype).type(0)\n        return xp.allclose(a, b)\n\n\nclass TestIsclose(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_is_close_finite(self, xp, dtype):\n        # In numpy<1.10 this test fails when dtype is bool\n        a = xp.array([0.9e-5, 1.1e-5, 1000 + 1e-4, 1000 - 1e-4], dtype=dtype)\n        b = xp.array([0, 0, 1000, 1000], dtype=dtype)\n        return xp.isclose(a, b)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_is_close_min_int(self, xp, dtype):\n        # In numpy<1.10 this test fails when dtype is bool\n        a = xp.array([0], dtype=dtype)\n        b = xp.array([numpy.iinfo('i').min], dtype=dtype)\n        return xp.isclose(a, b)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_is_close_infinite(self, xp, dtype):\n        nan = float('nan')\n        inf = float('inf')\n        ninf = float('-inf')\n        a = xp.array([0, nan, nan, 0, inf, ninf], dtype=dtype)\n        b = xp.array([0, nan, 0, nan, inf, ninf], dtype=dtype)\n        return xp.isclose(a, b)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_is_close_infinite_equal_nan(self, xp, dtype):\n        nan = float('nan')\n        inf = float('inf')\n        ninf = float('-inf')\n        a = xp.array([0, nan, inf, ninf], dtype=dtype)\n        b = xp.array([0, nan, inf, ninf], dtype=dtype)\n        return xp.isclose(a, b, equal_nan=True)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_is_close_array_scalar(self, xp, dtype):\n        a = xp.array([0.9e-5, 1.1e-5], dtype=dtype)\n        b = xp.dtype(xp.dtype).type(0)\n        return xp.isclose(a, b)\n\n    @testing.for_all_dtypes(no_complex=True)\n    def test_is_close_scalar_scalar(self, dtype):\n        # cupy.isclose always returns ndarray\n        a = cupy.dtype(cupy.dtype).type(0)\n        b = cupy.dtype(cupy.dtype).type(0)\n        cond = cupy.isclose(a, b)\n        assert cond.shape == ()\n        assert bool(cond)\n"""
tests/cupy_tests/logic_tests/test_content.py,0,"b""import unittest\n\nimport numpy\n\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestContent(unittest.TestCase):\n\n    @testing.for_dtypes('efFdD')\n    @testing.numpy_cupy_array_equal()\n    def check_unary_inf(self, name, xp, dtype):\n        a = xp.array([-3, numpy.inf, -1, -numpy.inf, 0, 1, 2],\n                     dtype=dtype)\n        return getattr(xp, name)(a)\n\n    @testing.for_dtypes('efFdD')\n    @testing.numpy_cupy_array_equal()\n    def check_unary_nan(self, name, xp, dtype):\n        a = xp.array(\n            [-3, numpy.NAN, -1, numpy.NAN, 0, numpy.NAN, numpy.inf],\n            dtype=dtype)\n        return getattr(xp, name)(a)\n\n    def test_isfinite(self):\n        self.check_unary_inf('isfinite')\n\n    def test_isinf(self):\n        self.check_unary_inf('isinf')\n\n    def test_isnan(self):\n        self.check_unary_nan('isnan')\n"""
tests/cupy_tests/logic_tests/test_ops.py,0,"b""import unittest\n\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestOps(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_unary(self, name, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return getattr(xp, name)(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_binary(self, name, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        b = testing.shaped_reverse_arange((2, 3), xp, dtype)\n        return getattr(xp, name)(a, b)\n\n    def test_logical_and(self):\n        self.check_binary('logical_and')\n\n    def test_logical_or(self):\n        self.check_binary('logical_or')\n\n    def test_logical_xor(self):\n        self.check_binary('logical_xor')\n\n    def test_logical_not(self):\n        self.check_unary('logical_not')\n"""
tests/cupy_tests/logic_tests/test_truth.py,0,"b""import unittest\n\nimport numpy\n\nfrom cupy import testing\n\n\ndef _calc_out_shape(shape, axis, keepdims):\n    if axis is None:\n        axis = list(range(len(shape)))\n    elif isinstance(axis, int):\n        axis = [axis]\n    else:\n        axis = list(axis)\n\n    shape = numpy.array(shape)\n\n    if keepdims:\n        shape[axis] = 1\n    else:\n        shape[axis] = -1\n        shape = filter(lambda x: x != -1, shape)\n    return tuple(shape)\n\n\n@testing.parameterize(\n    *testing.product(\n        {'f': ['all', 'any'],\n         'x': [numpy.arange(24).reshape(2, 3, 4) - 10,\n               numpy.zeros((2, 3, 4)),\n               numpy.ones((2, 3, 4)),\n               numpy.zeros((0, 3, 4)),\n               numpy.ones((0, 3, 4))],\n         'axis': [None, (0, 1, 2), 0, 1, 2, (0, 1)],\n         'keepdims': [False, True]}))\n@testing.gpu\nclass TestAllAny(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_without_out(self, xp, dtype):\n        x = xp.asarray(self.x).astype(dtype)\n        return getattr(xp, self.f)(x, self.axis, None, self.keepdims)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_with_out(self, xp, dtype):\n        x = xp.asarray(self.x).astype(dtype)\n        out_shape = _calc_out_shape(x.shape, self.axis, self.keepdims)\n        out = xp.empty(out_shape, dtype=x.dtype)\n        getattr(xp, self.f)(x, self.axis, out, self.keepdims)\n        return out\n\n\n@testing.parameterize(\n    *testing.product(\n        {'f': ['all', 'any'],\n         'x': [numpy.array([[[numpy.nan]]]),\n               numpy.array([[[numpy.nan, 0]]]),\n               numpy.array([[[numpy.nan, 1]]]),\n               numpy.array([[[numpy.nan, 0, 1]]])],\n         'axis': [None, (0, 1, 2), 0, 1, 2, (0, 1)],\n         'keepdims': [False, True]}))\n@testing.gpu\nclass TestAllAnyWithNaN(unittest.TestCase):\n\n    @testing.for_dtypes(\n        (numpy.float64, numpy.float32, numpy.float16, numpy.bool_))\n    @testing.numpy_cupy_array_equal()\n    def test_without_out(self, xp, dtype):\n        x = xp.asarray(self.x).astype(dtype)\n        return getattr(xp, self.f)(x, self.axis, None, self.keepdims)\n\n    @testing.for_dtypes(\n        (numpy.float64, numpy.float32, numpy.float16, numpy.bool_))\n    @testing.numpy_cupy_array_equal()\n    def test_with_out(self, xp, dtype):\n        x = xp.asarray(self.x).astype(dtype)\n        out_shape = _calc_out_shape(x.shape, self.axis, self.keepdims)\n        out = xp.empty(out_shape, dtype=x.dtype)\n        getattr(xp, self.f)(x, self.axis, out, self.keepdims)\n        return out\n\n\n@testing.parameterize(\n    *testing.product(\n        {'f': ['in1d', 'isin'],\n         'shape_x': [\n             (0, ),\n             (3, ),\n             (2, 3),\n             (2, 1, 3),\n             (2, 0, 1),\n             (2, 0, 1, 1)\n        ],\n            'shape_y': [\n             (0, ),\n             (3, ),\n             (2, 3),\n             (2, 1, 3),\n             (2, 0, 1),\n             (2, 0, 1, 1)\n        ],\n            'assume_unique': [False, True],\n            'invert': [False, True]}))\n@testing.gpu\nclass TestIn1DIsIn(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test(self, xp, dtype):\n        x = testing.shaped_arange(self.shape_x, xp, dtype)\n        y = testing.shaped_arange(self.shape_y, xp, dtype)\n        if xp is numpy and self.f == 'isin':\n            return xp.in1d(x, y, self.assume_unique, self.invert)\\\n                .reshape(x.shape)\n        return getattr(xp, self.f)(x, y, self.assume_unique, self.invert)\n"""
tests/cupy_tests/logic_tests/test_type_test.py,0,"b""import unittest\n\nimport numpy\n\nfrom cupy import testing\n\n\nclass TestIsScalar(testing.NumpyAliasBasicTestBase):\n\n    func = 'isscalar'\n\n    @testing.with_requires('numpy>=1.18')\n    def test_argspec(self):\n        super().test_argspec()\n\n\n@testing.parameterize(\n    *testing.product({\n        'value': [\n            0, 0.0, True,\n            numpy.int32(1), numpy.array([1, 2], numpy.int32),\n            numpy.complex(1), numpy.complex(1j), numpy.complex(1 + 1j),\n            None, object(), 'abc', '', int, numpy.int32]}))\nclass TestIsScalarValues(testing.NumpyAliasValuesTestBase):\n\n    func = 'isscalar'\n\n    def setUp(self):\n        self.args = (self.value,)\n\n\n@testing.parameterize(\n    *testing.product({\n        'value': [\n            # C and F\n            numpy.ones(24, order='C'),\n            # C and not F\n            numpy.ones((4, 6), order='C'),\n            # not C and F\n            numpy.ones((4, 6), order='F'),\n            # not C and not F\n            numpy.ones((4, 6), order='C')[1:3][1:3],\n        ]\n    })\n)\nclass TestIsFortran(unittest.TestCase):\n\n    @testing.numpy_cupy_equal()\n    def test(self, xp):\n        return xp.isfortran(xp.asarray(self.value))\n\n\n@testing.parameterize(\n    {'func': 'iscomplex'},\n    {'func': 'isreal'},\n)\nclass TestTypeTestingFunctions(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test(self, xp, dtype):\n        return getattr(xp, self.func)(xp.ones(5, dtype=dtype))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_scalar(self, xp, dtype):\n        return getattr(xp, self.func)(dtype(3))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_list(self, xp, dtype):\n        return getattr(xp, self.func)(\n            testing.shaped_arange((2, 3), xp, dtype).tolist())\n\n\n@testing.parameterize(\n    {'func': 'iscomplexobj'},\n    {'func': 'isrealobj'},\n)\nclass TestTypeTestingObjFunctions(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test(self, xp, dtype):\n        return getattr(xp, self.func)(xp.ones(5, dtype=dtype))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_scalar(self, xp, dtype):\n        return getattr(xp, self.func)(dtype(3))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_list(self, xp, dtype):\n        return getattr(xp, self.func)(\n            testing.shaped_arange((2, 3), xp, dtype).tolist())\n"""
tests/cupy_tests/manipulation_tests/__init__.py,0,b''
tests/cupy_tests/manipulation_tests/test_add_remove.py,0,"b""import unittest\n\nimport pytest\n\nimport numpy\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestUnique(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_float16=True, no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_unique(self, xp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        return xp.unique(a)\n\n    @testing.for_all_dtypes(no_float16=True, no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_unique_index(self, xp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        return xp.unique(a, return_index=True)[1]\n\n    @testing.for_all_dtypes(no_float16=True, no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_unique_inverse(self, xp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        return xp.unique(a, return_inverse=True)[1]\n\n    @testing.for_all_dtypes(no_float16=True, no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_unique_counts(self, xp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        return xp.unique(a, return_counts=True)[1]\n\n\n@testing.parameterize(*testing.product({\n    'trim': ['fb', 'f', 'b']\n}))\n@testing.gpu\nclass TestTrim_zeros(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_trim_non_zeros(self, xp, dtype):\n        a = xp.array([-1, 2, -3, 7], dtype=dtype)\n        return xp.trim_zeros(a, trim=self.trim)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_trim_trimmed(self, xp, dtype):\n        a = xp.array([1, 0, 2, 3, 0, 5], dtype=dtype)\n        return xp.trim_zeros(a, trim=self.trim)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_trim_all_zeros(self, xp, dtype):\n        a = xp.zeros(shape=(1000,), dtype=dtype)\n        return xp.trim_zeros(a, trim=self.trim)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_trim_front_zeros(self, xp, dtype):\n        a = xp.array([0, 0, 4, 1, 0, 2, 3, 0, 5], dtype=dtype)\n        return xp.trim_zeros(a, trim=self.trim)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_trim_back_zeros(self, xp, dtype):\n        a = xp.array([1, 0, 2, 3, 0, 5, 0, 0, 0], dtype=dtype)\n        return xp.trim_zeros(a, trim=self.trim)\n\n    @testing.for_all_dtypes()\n    def test_trim_zero_dim(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((), xp, dtype)\n            with pytest.raises(TypeError):\n                xp.trim_zeros(a, trim=self.trim)\n\n    @testing.for_all_dtypes()\n    def test_trim_ndim(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3), xp, dtype=dtype)\n            with pytest.raises(ValueError):\n                xp.trim_zeros(a, trim=self.trim)\n"""
tests/cupy_tests/manipulation_tests/test_basic.py,0,"b""import itertools\nimport unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import cuda\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestBasic(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_copyto(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = xp.empty((2, 3, 4), dtype=dtype)\n        xp.copyto(b, a)\n        return b\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_copyto_dtype(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype='?')\n        b = xp.empty((2, 3, 4), dtype=dtype)\n        xp.copyto(b, a)\n        return b\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_copyto_broadcast(self, xp, dtype):\n        a = testing.shaped_arange((3, 1), xp, dtype)\n        b = xp.empty((2, 3, 4), dtype=dtype)\n        xp.copyto(b, a)\n        return b\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_copyto_where(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = testing.shaped_reverse_arange((2, 3, 4), xp, dtype)\n        c = testing.shaped_arange((2, 3, 4), xp, '?')\n        xp.copyto(a, b, where=c)\n        return a\n\n    def _check_copyto_where_multigpu_raises(self, dtype, ngpus):\n        def get_numpy():\n            a = testing.shaped_arange((2, 3, 4), numpy, dtype)\n            b = testing.shaped_reverse_arange((2, 3, 4), numpy, dtype)\n            c = testing.shaped_arange((2, 3, 4), numpy, '?')\n            numpy.copyto(a, b, where=c)\n            return a\n\n        for dev1, dev2, dev3, dev4 in itertools.product(*[range(ngpus)] * 4):\n            if dev1 == dev2 == dev3 == dev4:\n                continue\n            if not dev1 <= dev2 <= dev3 <= dev4:\n                continue\n\n            with cuda.Device(dev1):\n                a = testing.shaped_arange((2, 3, 4), cupy, dtype)\n            with cuda.Device(dev2):\n                b = testing.shaped_reverse_arange((2, 3, 4), cupy, dtype)\n            with cuda.Device(dev3):\n                c = testing.shaped_arange((2, 3, 4), cupy, '?')\n            with cuda.Device(dev4):\n                with self.assertRaisesRegex(\n                        ValueError,\n                        '^Array device must be same as the current device'):\n                    cupy.copyto(a, b, where=c)\n\n    @testing.multi_gpu(2)\n    @testing.for_all_dtypes()\n    def test_copyto_where_multigpu_raises(self, dtype):\n        self._check_copyto_where_multigpu_raises(dtype, 2)\n\n    @testing.multi_gpu(2)\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_copyto_multigpu(self, xp, dtype):\n        with cuda.Device(0):\n            a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        with cuda.Device(1):\n            b = xp.empty((2, 3, 4), dtype=dtype)\n        xp.copyto(b, a)\n        return b\n\n    @testing.multi_gpu(2)\n    @testing.for_all_dtypes()\n    def test_copyto_multigpu_noncontinguous(self, dtype):\n        with cuda.Device(0):\n            src = testing.shaped_arange((2, 3, 4), cupy, dtype)\n            src = src.swapaxes(0, 1)\n        with cuda.Device(1):\n            dst = cupy.empty_like(src)\n            cupy.copyto(dst, src)\n\n        expected = testing.shaped_arange((2, 3, 4), numpy, dtype)\n        expected = expected.swapaxes(0, 1)\n\n        testing.assert_array_equal(expected, src.get())\n        testing.assert_array_equal(expected, dst.get())\n\n\n@testing.parameterize(\n    *testing.product(\n        {'src': [float(3.2), int(0), int(4), int(-4), True, False, 1 + 1j],\n         'dst_shape': [(), (0,), (1,), (1, 1), (2, 2)]}))\n@testing.gpu\nclass TestCopytoFromScalar(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(accept_error=TypeError)\n    def test_copyto(self, xp, dtype):\n        dst = xp.ones(self.dst_shape, dtype=dtype)\n        xp.copyto(dst, self.src)\n        return dst\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(accept_error=TypeError)\n    def test_copyto_where(self, xp, dtype):\n        dst = xp.ones(self.dst_shape, dtype=dtype)\n        mask = (testing.shaped_arange(\n            self.dst_shape, xp, dtype) % 2).astype(xp.bool_)\n        xp.copyto(dst, self.src, where=mask)\n        return dst\n"""
tests/cupy_tests/manipulation_tests/test_dims.py,3,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestDims(unittest.TestCase):\n\n    def check_atleast(self, func, xp):\n        a = testing.shaped_arange((), xp)\n        b = testing.shaped_arange((2,), xp)\n        c = testing.shaped_arange((2, 2), xp)\n        d = testing.shaped_arange((4, 3, 2), xp)\n        e = 1\n        f = numpy.float32(1)\n        return func(a, b, c, d, e, f)\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_atleast_1d1(self, xp):\n        return self.check_atleast(xp.atleast_1d, xp)\n\n    @testing.numpy_cupy_array_equal()\n    def test_atleast_1d2(self, xp):\n        a = testing.shaped_arange((1, 3, 2), xp)\n        return xp.atleast_1d(a)\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_atleast_2d1(self, xp):\n        return self.check_atleast(xp.atleast_2d, xp)\n\n    @testing.numpy_cupy_array_equal()\n    def test_atleast_2d2(self, xp):\n        a = testing.shaped_arange((1, 3, 2), xp)\n        return xp.atleast_2d(a)\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_atleast_3d1(self, xp):\n        return self.check_atleast(xp.atleast_3d, xp)\n\n    @testing.numpy_cupy_array_equal()\n    def test_atleast_3d2(self, xp):\n        a = testing.shaped_arange((1, 3, 2), xp)\n        return xp.atleast_3d(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_broadcast_to(self, xp, dtype):\n        # Note that broadcast_to is only supported on numpy>=1.10\n        a = testing.shaped_arange((3, 1, 4), xp, dtype)\n        b = xp.broadcast_to(a, (2, 3, 3, 4))\n        return b\n\n    @testing.for_all_dtypes()\n    def test_broadcast_to_fail(self, dtype):\n        for xp in (numpy, cupy):\n            # Note that broadcast_to is only supported on numpy>=1.10\n            a = testing.shaped_arange((3, 1, 4), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.broadcast_to(a, (1, 3, 4))\n\n    @testing.for_all_dtypes()\n    def test_broadcast_to_short_shape(self, dtype):\n        for xp in (numpy, cupy):\n            # Note that broadcast_to is only supported on numpy>=1.10\n            a = testing.shaped_arange((1, 3, 4), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.broadcast_to(a, (3, 4))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_broadcast_to_numpy19(self, xp, dtype):\n        # Note that broadcast_to is only supported on numpy>=1.10\n        a = testing.shaped_arange((3, 1, 4), xp, dtype)\n        if xp is cupy:\n            b = xp.broadcast_to(a, (2, 3, 3, 4))\n        else:\n            dummy = xp.empty((2, 3, 3, 4))\n            b, _ = xp.broadcast_arrays(a, dummy)\n        return b\n\n    @testing.for_all_dtypes()\n    def test_broadcast_to_fail_numpy19(self, dtype):\n        for xp in (numpy, cupy):\n            # Note that broadcast_to is only supported on numpy>=1.10\n            a = testing.shaped_arange((3, 1, 4), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.broadcast_to(a, (1, 3, 4))\n\n    @testing.for_all_dtypes()\n    def test_broadcast_to_short_shape_numpy19(self, dtype):\n        for xp in (numpy, cupy):\n            # Note that broadcast_to is only supported on numpy>=1.10\n            a = testing.shaped_arange((1, 3, 4), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.broadcast_to(a, (3, 4))\n\n    @testing.numpy_cupy_array_equal()\n    def test_expand_dims0(self, xp):\n        a = testing.shaped_arange((2, 3), xp)\n        return xp.expand_dims(a, 0)\n\n    @testing.numpy_cupy_array_equal()\n    def test_expand_dims1(self, xp):\n        a = testing.shaped_arange((2, 3), xp)\n        return xp.expand_dims(a, 1)\n\n    @testing.numpy_cupy_array_equal()\n    def test_expand_dims2(self, xp):\n        a = testing.shaped_arange((2, 3), xp)\n        return xp.expand_dims(a, 2)\n\n    @testing.numpy_cupy_array_equal()\n    def test_expand_dims_negative1(self, xp):\n        a = testing.shaped_arange((2, 3), xp)\n        return xp.expand_dims(a, -2)\n\n    @testing.with_requires('numpy>=1.18')\n    def test_expand_dims_negative2(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3), xp)\n            with pytest.raises(numpy.AxisError):\n                xp.expand_dims(a, -4)\n\n    @testing.with_requires('numpy>=1.18')\n    @testing.numpy_cupy_array_list_equal()\n    def test_expand_dims_tuple_axis(self, xp):\n        a = testing.shaped_arange((2, 2, 2), xp)\n        return [xp.expand_dims(a, axis) for axis in [\n            (0, 1, 2),\n            (0, -1, -2),\n            (0, 3, 5),\n            (0, -3, -5),\n            (),\n            (1,),\n        ]]\n\n    @testing.with_requires('numpy>=1.18')\n    def test_expand_dims_out_of_range(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 2, 2), xp)\n            for axis in [(1, -6), (1, 5)]:\n                with pytest.raises(numpy.AxisError):\n                    xp.expand_dims(a, axis)\n\n    @testing.with_requires('numpy>=1.18')\n    def test_expand_dims_repeated_axis(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 2, 2), xp)\n            with pytest.raises(ValueError):\n                xp.expand_dims(a, (1, 1))\n\n    @testing.numpy_cupy_array_equal()\n    def test_squeeze1(self, xp):\n        a = testing.shaped_arange((1, 2, 1, 3, 1, 1, 4, 1), xp)\n        return a.squeeze()\n\n    @testing.numpy_cupy_array_equal()\n    def test_squeeze2(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return a.squeeze()\n\n    @testing.numpy_cupy_array_equal()\n    def test_squeeze_int_axis1(self, xp):\n        a = testing.shaped_arange((1, 2, 1, 3, 1, 1, 4, 1), xp)\n        return a.squeeze(axis=2)\n\n    @testing.numpy_cupy_array_equal()\n    def test_squeeze_int_axis2(self, xp):\n        a = testing.shaped_arange((1, 2, 1, 3, 1, 1, 4, 1), xp)\n        return a.squeeze(axis=-3)\n\n    def test_squeeze_int_axis_failure1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((1, 2, 1, 3, 1, 1, 4, 1), xp)\n            with pytest.raises(ValueError):\n                a.squeeze(axis=-9)\n\n    def test_squeeze_int_axis_failure2(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((1, 2, 1, 3, 1, 1, 4, 1), xp)\n            with pytest.raises(numpy.AxisError):\n                a.squeeze(axis=-9)\n\n    @testing.numpy_cupy_array_equal()\n    def test_squeeze_tuple_axis1(self, xp):\n        a = testing.shaped_arange((1, 2, 1, 3, 1, 1, 4, 1), xp)\n        return a.squeeze(axis=(2, 4))\n\n    @testing.numpy_cupy_array_equal()\n    def test_squeeze_tuple_axis2(self, xp):\n        a = testing.shaped_arange((1, 2, 1, 3, 1, 1, 4, 1), xp)\n        return a.squeeze(axis=(-4, -3))\n\n    @testing.numpy_cupy_array_equal()\n    def test_squeeze_tuple_axis3(self, xp):\n        a = testing.shaped_arange((1, 2, 1, 3, 1, 1, 4, 1), xp)\n        return a.squeeze(axis=(4, 2))\n\n    @testing.numpy_cupy_array_equal()\n    def test_squeeze_tuple_axis4(self, xp):\n        a = testing.shaped_arange((1, 2, 1, 3, 1, 1, 4, 1), xp)\n        return a.squeeze(axis=())\n\n    def test_squeeze_tuple_axis_failure1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((1, 2, 1, 3, 1, 1, 4, 1), xp)\n            with pytest.raises(ValueError):\n                a.squeeze(axis=(-9,))\n\n    def test_squeeze_tuple_axis_failure2(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((1, 2, 1, 3, 1, 1, 4, 1), xp)\n            with pytest.raises(ValueError):\n                a.squeeze(axis=(2, 2))\n\n    def test_squeeze_tuple_axis_failure3(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((1, 2, 1, 3, 1, 1, 4, 1), xp)\n            with pytest.raises(numpy.AxisError):\n                a.squeeze(axis=(-9,))\n\n    @testing.numpy_cupy_array_equal()\n    def test_squeeze_scalar1(self, xp):\n        a = testing.shaped_arange((), xp)\n        return a.squeeze(axis=0)\n\n    @testing.numpy_cupy_array_equal()\n    def test_squeeze_scalar2(self, xp):\n        a = testing.shaped_arange((), xp)\n        return a.squeeze(axis=-1)\n\n    def test_squeeze_scalar_failure1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((), xp)\n            with pytest.raises(ValueError):\n                a.squeeze(axis=-2)\n\n    def test_squeeze_scalar_failure2(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((), xp)\n            with pytest.raises(ValueError):\n                a.squeeze(axis=1)\n\n    def test_squeeze_scalar_failure3(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((), xp)\n            with pytest.raises(numpy.AxisError):\n                a.squeeze(axis=-2)\n\n    def test_squeeze_scalar_failure4(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((), cupy)\n            with pytest.raises(numpy.AxisError):\n                a.squeeze(axis=1)\n\n    def test_squeeze_failure(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 1, 3, 4), xp)\n            with pytest.raises(ValueError):\n                a.squeeze(axis=2)\n\n    @testing.numpy_cupy_array_equal()\n    def test_external_squeeze(self, xp):\n        a = testing.shaped_arange((1, 2, 1, 3, 1, 1, 4, 1), xp)\n        return xp.squeeze(a)\n\n\n@testing.parameterize(\n    {'shapes': [(), ()]},\n    {'shapes': [(0,), (0,)]},\n    {'shapes': [(1,), (1,)]},\n    {'shapes': [(2,), (2,)]},\n    {'shapes': [(0,), (1,)]},\n    {'shapes': [(2, 3), (1, 3)]},\n    {'shapes': [(2, 1, 3, 4), (3, 1, 4)]},\n    {'shapes': [(4, 3, 2, 3), (2, 3)]},\n    {'shapes': [(2, 0, 1, 1, 3), (2, 1, 0, 0, 3)]},\n    {'shapes': [(0, 1, 1, 3), (2, 1, 0, 0, 3)]},\n    {'shapes': [(0, 1, 1, 0, 3), (5, 2, 0, 1, 0, 0, 3), (2, 1, 0, 0, 0, 3)]},\n)\n@testing.gpu\nclass TestBroadcast(unittest.TestCase):\n\n    def _broadcast(self, xp, dtype, shapes):\n        arrays = [\n            testing.shaped_arange(s, xp, dtype) for s in shapes]\n        return xp.broadcast(*arrays)\n\n    @testing.for_all_dtypes()\n    def test_broadcast(self, dtype):\n        broadcast_np = self._broadcast(numpy, dtype, self.shapes)\n        broadcast_cp = self._broadcast(cupy, dtype, self.shapes)\n        self.assertEqual(broadcast_np.shape, broadcast_cp.shape)\n        self.assertEqual(broadcast_np.size, broadcast_cp.size)\n        self.assertEqual(broadcast_np.nd, broadcast_cp.nd)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test_broadcast_arrays(self, xp, dtype):\n        arrays = [\n            testing.shaped_arange(s, xp, dtype) for s in self.shapes]\n        return xp.broadcast_arrays(*arrays)\n\n\n@testing.parameterize(\n    {'shapes': [(3,), (2,)]},\n    {'shapes': [(3, 2), (2, 3,)]},\n    {'shapes': [(3, 2), (3, 4,)]},\n    {'shapes': [(0,), (2,)]},\n)\n@testing.gpu\nclass TestInvalidBroadcast(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_invalid_broadcast(self, dtype):\n        for xp in (numpy, cupy):\n            arrays = [testing.shaped_arange(s, xp, dtype) for s in self.shapes]\n            with pytest.raises(ValueError):\n                xp.broadcast(*arrays)\n\n    @testing.for_all_dtypes()\n    def test_invalid_broadcast_arrays(self, dtype):\n        for xp in (numpy, cupy):\n            arrays = [testing.shaped_arange(s, xp, dtype) for s in self.shapes]\n            with pytest.raises(ValueError):\n                xp.broadcast_arrays(*arrays)\n"""
tests/cupy_tests/manipulation_tests/test_join.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestJoin(unittest.TestCase):\n\n    @testing.for_all_dtypes(name='dtype1')\n    @testing.for_all_dtypes(name='dtype2')\n    @testing.numpy_cupy_array_equal()\n    def test_column_stack(self, xp, dtype1, dtype2):\n        a = testing.shaped_arange((4, 3), xp, dtype1)\n        b = testing.shaped_arange((4,), xp, dtype2)\n        c = testing.shaped_arange((4, 2), xp, dtype1)\n        return xp.column_stack((a, b, c))\n\n    def test_column_stack_wrong_ndim1(self):\n        a = cupy.zeros(())\n        b = cupy.zeros((3,))\n        with self.assertRaises(ValueError):\n            cupy.column_stack((a, b))\n\n    def test_column_stack_wrong_ndim2(self):\n        a = cupy.zeros((3, 2, 3))\n        b = cupy.zeros((3, 2))\n        with self.assertRaises(ValueError):\n            cupy.column_stack((a, b))\n\n    def test_column_stack_wrong_shape(self):\n        a = cupy.zeros((3, 2))\n        b = cupy.zeros((4, 3))\n        with self.assertRaises(ValueError):\n            cupy.column_stack((a, b))\n\n    @testing.for_all_dtypes(name='dtype')\n    @testing.numpy_cupy_array_equal()\n    def test_concatenate1(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = testing.shaped_reverse_arange((2, 3, 2), xp, dtype)\n        c = testing.shaped_arange((2, 3, 3), xp, dtype)\n        return xp.concatenate((a, b, c), axis=2)\n\n    @testing.for_all_dtypes(name='dtype')\n    @testing.numpy_cupy_array_equal()\n    def test_concatenate2(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = testing.shaped_reverse_arange((2, 3, 2), xp, dtype)\n        c = testing.shaped_arange((2, 3, 3), xp, dtype)\n        return xp.concatenate((a, b, c), axis=-1)\n\n    @testing.for_all_dtypes(name='dtype')\n    @testing.numpy_cupy_array_equal()\n    def test_concatenate_axis_none(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        b = testing.shaped_reverse_arange((3, 5, 2), xp, dtype)\n        c = testing.shaped_arange((7, ), xp, dtype)\n        return xp.concatenate((a, b, c), axis=None)\n\n    @testing.for_all_dtypes(name='dtype')\n    @testing.numpy_cupy_array_equal()\n    def test_concatenate_large_2(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = testing.shaped_reverse_arange((2, 3, 2), xp, dtype)\n        c = testing.shaped_arange((2, 3, 3), xp, dtype)\n        d = testing.shaped_arange((2, 3, 5), xp, dtype)\n        e = testing.shaped_arange((2, 3, 2), xp, dtype)\n        return xp.concatenate((a, b, c, d, e) * 2, axis=-1)\n\n    @testing.for_all_dtypes(name='dtype')\n    @testing.numpy_cupy_array_equal()\n    def test_concatenate_large_3(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 1), xp, dtype)\n        b = testing.shaped_reverse_arange((2, 3, 1), xp, dtype)\n        return xp.concatenate((a, b) * 10, axis=-1)\n\n    @testing.for_all_dtypes(name='dtype')\n    @testing.numpy_cupy_array_equal()\n    def test_concatenate_large_4(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = testing.shaped_reverse_arange((2, 3, 4), xp, dtype)\n        return xp.concatenate((a, b) * 10, axis=-1)\n\n    @testing.for_all_dtypes(name='dtype')\n    @testing.numpy_cupy_array_equal()\n    def test_concatenate_large_5(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = testing.shaped_reverse_arange((2, 3, 4), xp, 'i')\n        return xp.concatenate((a, b) * 10, axis=-1)\n\n    @testing.for_all_dtypes(name='dtype')\n    @testing.numpy_cupy_array_equal()\n    def test_concatenate_f_contiguous(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = testing.shaped_arange((2, 3, 2), xp, dtype).T\n        c = testing.shaped_arange((2, 3, 3), xp, dtype)\n        return xp.concatenate((a, b, c), axis=-1)\n\n    @testing.for_all_dtypes(name='dtype')\n    @testing.numpy_cupy_array_equal()\n    def test_concatenate_large_f_contiguous(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = testing.shaped_arange((2, 3, 2), xp, dtype).T\n        c = testing.shaped_arange((2, 3, 3), xp, dtype)\n        d = testing.shaped_arange((2, 3, 2), xp, dtype).T\n        e = testing.shaped_arange((2, 3, 2), xp, dtype)\n        return xp.concatenate((a, b, c, d, e) * 2, axis=-1)\n\n    @testing.numpy_cupy_array_equal()\n    def test_concatenate_many_multi_dptye(self, xp):\n        a = testing.shaped_arange((2, 1), xp, 'i')\n        b = testing.shaped_arange((2, 1), xp, 'f')\n        return xp.concatenate((a, b) * 1024, axis=1)\n\n    @testing.slow\n    def test_concatenate_32bit_boundary(self):\n        a = cupy.zeros((2 ** 30,), dtype=cupy.int8)\n        b = cupy.zeros((2 ** 30,), dtype=cupy.int8)\n        ret = cupy.concatenate([a, b])\n        del a\n        del b\n        del ret\n        # Free huge memory for slow test\n        cupy.get_default_memory_pool().free_all_blocks()\n\n    def test_concatenate_wrong_ndim(self):\n        a = cupy.empty((2, 3))\n        b = cupy.empty((2,))\n        with self.assertRaises(ValueError):\n            cupy.concatenate((a, b))\n\n    def test_concatenate_wrong_shape(self):\n        a = cupy.empty((2, 3, 4))\n        b = cupy.empty((3, 3, 4))\n        c = cupy.empty((4, 4, 4))\n        with self.assertRaises(ValueError):\n            cupy.concatenate((a, b, c))\n\n    @testing.for_all_dtypes(name='dtype')\n    @testing.numpy_cupy_array_equal()\n    def test_concatenate_out(self, xp, dtype):\n        a = testing.shaped_arange((3, 4), xp, dtype)\n        b = testing.shaped_reverse_arange((3, 4), xp, dtype)\n        c = testing.shaped_arange((3, 4), xp, dtype)\n        out = xp.zeros((3, 12), dtype=dtype)\n        xp.concatenate((a, b, c), axis=1, out=out)\n        return out\n\n    @testing.numpy_cupy_array_equal()\n    def test_concatenate_out_same_kind(self, xp):\n        a = testing.shaped_arange((3, 4), xp, xp.float64)\n        b = testing.shaped_reverse_arange((3, 4), xp, xp.float64)\n        c = testing.shaped_arange((3, 4), xp, xp.float64)\n        out = xp.zeros((3, 12), dtype=xp.float32)\n        xp.concatenate((a, b, c), axis=1, out=out)\n        return out\n\n    def test_concatenate_out_invalid_shape(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((3, 4), xp, xp.float64)\n            b = testing.shaped_reverse_arange((3, 4), xp, xp.float64)\n            c = testing.shaped_arange((3, 4), xp, xp.float64)\n            out = xp.zeros((4, 10), dtype=xp.float64)\n            with pytest.raises(ValueError):\n                xp.concatenate((a, b, c), axis=1, out=out)\n\n    def test_concatenate_out_invalid_shape_2(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((3, 4), xp, xp.float64)\n            b = testing.shaped_reverse_arange((3, 4), xp, xp.float64)\n            c = testing.shaped_arange((3, 4), xp, xp.float64)\n            out = xp.zeros((2, 2, 10), dtype=xp.float64)\n            with pytest.raises(ValueError):\n                xp.concatenate((a, b, c), axis=1, out=out)\n\n    def test_concatenate_out_invalid_dtype(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((3, 4), xp, xp.float64)\n            b = testing.shaped_reverse_arange((3, 4), xp, xp.float64)\n            c = testing.shaped_arange((3, 4), xp, xp.float64)\n            out = xp.zeros((3, 12), dtype=xp.int64)\n            with pytest.raises(TypeError):\n                xp.concatenate((a, b, c), axis=1, out=out)\n\n    @testing.numpy_cupy_array_equal()\n    def test_dstack(self, xp):\n        a = testing.shaped_arange((1, 3, 2), xp)\n        b = testing.shaped_arange((3,), xp)\n        c = testing.shaped_arange((1, 3), xp)\n        return xp.dstack((a, b, c))\n\n    @testing.numpy_cupy_array_equal()\n    def test_dstack_single_element(self, xp):\n        a = testing.shaped_arange((1, 2, 3), xp)\n        return xp.dstack((a,))\n\n    @testing.numpy_cupy_array_equal()\n    def test_dstack_single_element_2(self, xp):\n        a = testing.shaped_arange((1, 2), xp)\n        return xp.dstack((a,))\n\n    @testing.numpy_cupy_array_equal()\n    def test_dstack_single_element_3(self, xp):\n        a = testing.shaped_arange((1,), xp)\n        return xp.dstack((a,))\n\n    @testing.numpy_cupy_array_equal()\n    def test_hstack_vectors(self, xp):\n        a = xp.arange(3)\n        b = xp.arange(2, -1, -1)\n        return xp.hstack((a, b))\n\n    @testing.numpy_cupy_array_equal()\n    def test_hstack_scalars(self, xp):\n        a = testing.shaped_arange((), xp)\n        b = testing.shaped_arange((), xp)\n        c = testing.shaped_arange((), xp)\n        return xp.hstack((a, b, c))\n\n    @testing.numpy_cupy_array_equal()\n    def test_hstack(self, xp):\n        a = testing.shaped_arange((2, 1), xp)\n        b = testing.shaped_arange((2, 2), xp)\n        c = testing.shaped_arange((2, 3), xp)\n        return xp.hstack((a, b, c))\n\n    @testing.numpy_cupy_array_equal()\n    def test_vstack_vectors(self, xp):\n        a = xp.arange(3)\n        b = xp.arange(2, -1, -1)\n        return xp.vstack((a, b))\n\n    @testing.numpy_cupy_array_equal()\n    def test_vstack_single_element(self, xp):\n        a = xp.arange(3)\n        return xp.vstack((a,))\n\n    def test_vstack_wrong_ndim(self):\n        a = cupy.empty((3,))\n        b = cupy.empty((3, 1))\n        with self.assertRaises(ValueError):\n            cupy.vstack((a, b))\n\n    @testing.numpy_cupy_array_equal()\n    def test_stack(self, xp):\n        a = testing.shaped_arange((2, 3), xp)\n        b = testing.shaped_arange((2, 3), xp)\n        c = testing.shaped_arange((2, 3), xp)\n        return xp.stack((a, b, c))\n\n    def test_stack_value(self):\n        a = testing.shaped_arange((2, 3), cupy)\n        b = testing.shaped_arange((2, 3), cupy)\n        c = testing.shaped_arange((2, 3), cupy)\n        s = cupy.stack((a, b, c))\n        self.assertEqual(s.shape, (3, 2, 3))\n        cupy.testing.assert_array_equal(s[0], a)\n        cupy.testing.assert_array_equal(s[1], b)\n        cupy.testing.assert_array_equal(s[2], c)\n\n    @testing.numpy_cupy_array_equal()\n    def test_stack_with_axis1(self, xp):\n        a = testing.shaped_arange((2, 3), xp)\n        return xp.stack((a, a), axis=1)\n\n    @testing.numpy_cupy_array_equal()\n    def test_stack_with_axis2(self, xp):\n        a = testing.shaped_arange((2, 3), xp)\n        return xp.stack((a, a), axis=2)\n\n    def test_stack_with_axis_over(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3), xp)\n            with pytest.raises(ValueError):\n                xp.stack((a, a), axis=3)\n\n    def test_stack_with_axis_value(self):\n        a = testing.shaped_arange((2, 3), cupy)\n        s = cupy.stack((a, a), axis=1)\n\n        self.assertEqual(s.shape, (2, 2, 3))\n        cupy.testing.assert_array_equal(s[:, 0, :], a)\n        cupy.testing.assert_array_equal(s[:, 1, :], a)\n\n    @testing.numpy_cupy_array_equal()\n    def test_stack_with_negative_axis(self, xp):\n        a = testing.shaped_arange((2, 3), xp)\n        return xp.stack((a, a), axis=-1)\n\n    def test_stack_with_negative_axis_value(self):\n        a = testing.shaped_arange((2, 3), cupy)\n        s = cupy.stack((a, a), axis=-1)\n\n        self.assertEqual(s.shape, (2, 3, 2))\n        cupy.testing.assert_array_equal(s[:, :, 0], a)\n        cupy.testing.assert_array_equal(s[:, :, 1], a)\n\n    def test_stack_different_shape(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3), xp)\n            b = testing.shaped_arange((2, 4), xp)\n            with pytest.raises(ValueError):\n                xp.stack([a, b])\n\n    def test_stack_out_of_bounds1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3), xp)\n            with pytest.raises(ValueError):\n                xp.stack([a, a], axis=3)\n\n    def test_stack_out_of_bounds2(self):\n        a = testing.shaped_arange((2, 3), cupy)\n        with self.assertRaises(numpy.AxisError):\n            return cupy.stack([a, a], axis=3)\n\n    @testing.for_all_dtypes(name='dtype')\n    @testing.numpy_cupy_array_equal()\n    def test_stack_out(self, xp, dtype):\n        a = testing.shaped_arange((3, 4), xp, dtype)\n        b = testing.shaped_reverse_arange((3, 4), xp, dtype)\n        c = testing.shaped_arange((3, 4), xp, dtype)\n        out = xp.zeros((3, 3, 4), dtype=dtype)\n        xp.stack((a, b, c), axis=1, out=out)\n        return out\n\n    @testing.numpy_cupy_array_equal()\n    def test_stack_out_same_kind(self, xp):\n        a = testing.shaped_arange((3, 4), xp, xp.float64)\n        b = testing.shaped_reverse_arange((3, 4), xp, xp.float64)\n        c = testing.shaped_arange((3, 4), xp, xp.float64)\n        out = xp.zeros((3, 3, 4), dtype=xp.float32)\n        xp.stack((a, b, c), axis=1, out=out)\n        return out\n\n    def test_stack_out_invalid_shape(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((3, 4), xp, xp.float64)\n            b = testing.shaped_reverse_arange((3, 4), xp, xp.float64)\n            c = testing.shaped_arange((3, 4), xp, xp.float64)\n            out = xp.zeros((3, 3, 10), dtype=xp.float64)\n            with pytest.raises(ValueError):\n                xp.stack((a, b, c), axis=1, out=out)\n\n    def test_stack_out_invalid_shape_2(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((3, 4), xp, xp.float64)\n            b = testing.shaped_reverse_arange((3, 4), xp, xp.float64)\n            c = testing.shaped_arange((3, 4), xp, xp.float64)\n            out = xp.zeros((3, 3, 3, 10), dtype=xp.float64)\n            with pytest.raises(ValueError):\n                xp.stack((a, b, c), axis=1, out=out)\n\n    def test_stack_out_invalid_dtype(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((3, 4), xp, xp.float64)\n            b = testing.shaped_reverse_arange((3, 4), xp, xp.float64)\n            c = testing.shaped_arange((3, 4), xp, xp.float64)\n            out = xp.zeros((3, 3, 4), dtype=xp.int64)\n            with pytest.raises(TypeError):\n                xp.stack((a, b, c), axis=1, out=out)\n"""
tests/cupy_tests/manipulation_tests/test_kind.py,0,"b""import unittest\nimport pytest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestKind(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_asfortranarray1(self, dtype):\n        def func(xp):\n            x = xp.zeros((2, 3), dtype)\n            ret = xp.asfortranarray(x)\n            self.assertTrue(x.flags.c_contiguous)\n            self.assertTrue(ret.flags.f_contiguous)\n            return ret.strides\n        self.assertEqual(func(numpy), func(cupy))\n\n    @testing.for_all_dtypes()\n    def test_asfortranarray2(self, dtype):\n        def func(xp):\n            x = xp.zeros((2, 3, 4), dtype)\n            ret = xp.asfortranarray(x)\n            self.assertTrue(x.flags.c_contiguous)\n            self.assertTrue(ret.flags.f_contiguous)\n            return ret.strides\n        self.assertEqual(func(numpy), func(cupy))\n\n    @testing.for_all_dtypes()\n    def test_asfortranarray3(self, dtype):\n        def func(xp):\n            x = xp.zeros((2, 3, 4), dtype)\n            ret = xp.asfortranarray(xp.asfortranarray(x))\n            self.assertTrue(x.flags.c_contiguous)\n            self.assertTrue(ret.flags.f_contiguous)\n            return ret.strides\n        self.assertEqual(func(numpy), func(cupy))\n\n    @testing.for_all_dtypes()\n    def test_asfortranarray4(self, dtype):\n        def func(xp):\n            x = xp.zeros((2, 3), dtype)\n            x = xp.transpose(x, (1, 0))\n            ret = xp.asfortranarray(x)\n            self.assertTrue(ret.flags.f_contiguous)\n            return ret.strides\n        self.assertEqual(func(numpy), func(cupy))\n\n    @testing.for_all_dtypes()\n    def test_asfortranarray5(self, dtype):\n        def func(xp):\n            x = testing.shaped_arange((2, 3), xp, dtype)\n            ret = xp.asfortranarray(x)\n            self.assertTrue(x.flags.c_contiguous)\n            self.assertTrue(ret.flags.f_contiguous)\n            return ret.strides\n        self.assertEqual(func(numpy), func(cupy))\n\n    @testing.for_all_dtypes()\n    def test_require_flag_check(self, dtype):\n        possible_flags = [['C_CONTIGUOUS'], ['F_CONTIGUOUS']]\n        x = cupy.zeros((2, 3, 4), dtype)\n        for flags in possible_flags:\n            arr = cupy.require(x, dtype, flags)\n            for parameter in flags:\n                assert arr.flags[parameter]\n                assert arr.dtype == dtype\n\n    @testing.for_all_dtypes()\n    def test_require_owndata(self, dtype):\n        x = cupy.zeros((2, 3, 4), dtype)\n        arr = x.view()\n        arr = cupy.require(arr, dtype, ['O'])\n        assert arr.flags['OWNDATA']\n\n    @testing.for_all_dtypes()\n    def test_require_C_and_F_flags(self, dtype):\n        x = cupy.zeros((2, 3, 4), dtype)\n        with pytest.raises(ValueError):\n            cupy.require(x, dtype, ['C', 'F'])\n\n    @testing.for_all_dtypes()\n    def test_require_incorrect_requirments(self, dtype):\n        x = cupy.zeros((2, 3, 4), dtype)\n        with pytest.raises(ValueError):\n            cupy.require(x, dtype, ['W'])\n\n    @testing.for_all_dtypes()\n    def test_require_incorrect_dtype(self, dtype):\n        x = cupy.zeros((2, 3, 4), dtype)\n        with pytest.raises(ValueError):\n            cupy.require(x, 'random', 'C')\n\n    @testing.for_all_dtypes()\n    def test_require_empty_requirements(self, dtype):\n        x = cupy.zeros((2, 3, 4), dtype)\n        x = cupy.require(x, dtype, [])\n        assert x.flags['C_CONTIGUOUS']\n"""
tests/cupy_tests/manipulation_tests/test_rearrange.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestRoll(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal(accept_error=TypeError)\n    def test_roll(self, xp, dtype):\n        x = xp.arange(10, dtype)\n        return xp.roll(x, 2)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_roll2(self, xp, dtype):\n        x = testing.shaped_arange((5, 2), xp, dtype)\n        return xp.roll(x, 1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_roll_negative(self, xp, dtype):\n        x = testing.shaped_arange((5, 2), xp, dtype)\n        return xp.roll(x, -2)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_roll_with_axis(self, xp, dtype):\n        x = testing.shaped_arange((5, 2), xp, dtype)\n        return xp.roll(x, 1, axis=0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_roll_with_negative_axis(self, xp, dtype):\n        x = testing.shaped_arange((5, 2), xp, dtype)\n        return xp.roll(x, 1, axis=-1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_roll_double_shift(self, xp, dtype):\n        x = testing.shaped_arange((10,), xp, dtype)\n        return xp.roll(x, 35)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_roll_double_shift_with_axis(self, xp, dtype):\n        x = testing.shaped_arange((5, 2), xp, dtype)\n        return xp.roll(x, 11, axis=0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_roll_zero_array(self, xp, dtype):\n        x = testing.shaped_arange((), xp, dtype)\n        return xp.roll(x, 5)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_roll_scalar_shift_multi_axis(self, xp, dtype):\n        x = testing.shaped_arange((5, 2), xp, dtype)\n        return xp.roll(x, 1, axis=(0, 1))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_roll_scalar_shift_duplicate_axis(self, xp, dtype):\n        x = testing.shaped_arange((5, 2), xp, dtype)\n        return xp.roll(x, 1, axis=(0, 0))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_roll_large_shift(self, xp, dtype):\n        x = testing.shaped_arange((5, 2), xp, dtype)\n        return xp.roll(x, 50, axis=0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_roll_multi_shift_multi_axis(self, xp, dtype):\n        x = testing.shaped_arange((5, 2), xp, dtype)\n        return xp.roll(x, (2, 1), axis=(0, 1))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_roll_multi_shift_multi_axis2(self, xp, dtype):\n        x = testing.shaped_arange((5, 2), xp, dtype)\n        return xp.roll(x, (2, 1), axis=(0, -1))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_roll_multi_shift_multi_axis3(self, xp, dtype):\n        x = testing.shaped_arange((5, 2), xp, dtype)\n        return xp.roll(x, (2, 1), axis=(1, -1))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_roll_multi_shift_scalar_axis(self, xp, dtype):\n        x = testing.shaped_arange((5, 2), xp, dtype)\n        return xp.roll(x, (2, 1, 3), axis=0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_roll_multi_shift_axis_none(self, xp, dtype):\n        x = testing.shaped_arange((5, 2), xp, dtype)\n        return xp.roll(x, (2, 1, 3), axis=None)\n\n    def test_roll_invalid_shift(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((5, 2), xp)\n            with pytest.raises(TypeError):\n                xp.roll(x, '0', axis=0)\n\n    def test_roll_shape_mismatch(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((5, 2, 3), xp)\n            with pytest.raises(ValueError):\n                xp.roll(x, (2, 2, 2), axis=(0, 1))\n\n    def test_roll_invalid_axis1(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((5, 2), xp)\n            with pytest.raises(ValueError):\n                xp.roll(x, 1, axis=2)\n\n    def test_roll_invalid_axis2(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((5, 2), xp)\n            with pytest.raises(ValueError):\n                xp.roll(x, 1, axis=-3)\n\n    def test_roll_invalid_axis_length(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((5, 2, 2), xp)\n            with pytest.raises(ValueError):\n                cupy.roll(x, shift=(1, 0), axis=(0, 1, 2))\n\n    def test_roll_invalid_axis_type(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((5, 2), xp)\n            with pytest.raises(TypeError):\n                xp.roll(x, 2, axis='0')\n\n    def test_roll_invalid_negative_axis1(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((5, 2), xp)\n            with pytest.raises(ValueError):\n                xp.roll(x, 1, axis=-3)\n\n    def test_roll_invalid_negative_axis2(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((5, 2), xp)\n            with pytest.raises(ValueError):\n                xp.roll(x, 1, axis=(1, -3))\n\n\n@testing.gpu\nclass TestFliplr(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_fliplr_2(self, xp, dtype):\n        x = testing.shaped_arange((3, 4), xp, dtype)\n        return xp.fliplr(x)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_fliplr_3(self, xp, dtype):\n        x = testing.shaped_arange((3, 4, 2), xp, dtype)\n        return xp.fliplr(x)\n\n    @testing.for_all_dtypes()\n    def test_fliplr_insufficient_ndim(self, dtype):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((3,), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.fliplr(x)\n\n\n@testing.gpu\nclass TestFlipud(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_flipud_1(self, xp, dtype):\n        x = testing.shaped_arange((3,), xp, dtype)\n        return xp.flipud(x)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_flipud_2(self, xp, dtype):\n        x = testing.shaped_arange((3, 4), xp, dtype)\n        return xp.flipud(x)\n\n    @testing.for_all_dtypes()\n    def test_flipud_insufficient_ndim(self, dtype):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.flipud(x)\n\n\n@testing.gpu\nclass TestFlip(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_flip_1(self, xp, dtype):\n        x = testing.shaped_arange((3,), xp, dtype)\n        return xp.flip(x, 0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_flip_2(self, xp, dtype):\n        x = testing.shaped_arange((3, 4), xp, dtype)\n        return xp.flip(x, 1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_flip_with_negative_axis(self, xp, dtype):\n        x = testing.shaped_arange((3, 4, 2), xp, dtype)\n        return xp.flip(x, -1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_flip_empty_dim_1(self, xp, dtype):\n        x = xp.array([], dtype).reshape((0,))\n        return xp.flip(x, 0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_flip_empty_dim_2(self, xp, dtype):\n        x = xp.array([], dtype).reshape((0, 0))\n        return xp.flip(x, 1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_flip_empty_dim_3(self, xp, dtype):\n        x = xp.array([], dtype).reshape((1, 0, 1))\n        return xp.flip(x, 1)\n\n    @testing.for_all_dtypes()\n    def test_flip_insufficient_ndim(self, dtype):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.flip(x, 0)\n\n    @testing.for_all_dtypes()\n    def test_flip_invalid_axis(self, dtype):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((3, 4), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.flip(x, 2)\n\n    @testing.for_all_dtypes()\n    def test_flip_invalid_negative_axis(self, dtype):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((3, 4), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.flip(x, -3)\n\n\n@testing.gpu\nclass TestRot90(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_rot90_none(self, xp, dtype):\n        x = testing.shaped_arange((3, 4), xp, dtype)\n        return xp.rot90(x, 0)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_rot90_twice(self, xp, dtype):\n        x = testing.shaped_arange((3, 4, 2), xp, dtype)\n        return xp.rot90(x, 2)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_rot90_negative(self, xp, dtype):\n        x = testing.shaped_arange((3, 4, 2), xp, dtype)\n        return xp.rot90(x, -1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_rot90_with_axes(self, xp, dtype):\n        x = testing.shaped_arange((3, 4, 2), xp, dtype)\n        return xp.rot90(x, 1, axes=(1, 2))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_rot90_with_negative_axes(self, xp, dtype):\n        x = testing.shaped_arange((3, 4, 2), xp, dtype)\n        return xp.rot90(x, 1, axes=(1, -1))\n\n    @testing.for_all_dtypes()\n    def test_rot90_insufficient_ndim(self, dtype):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((3,), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.rot90(x)\n\n    @testing.for_all_dtypes()\n    def test_rot90_too_much_axes(self, dtype):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((3, 4, 2), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.rot90(x, 1, axes=(0, 1, 2))\n\n    @testing.for_all_dtypes()\n    def test_rot90_invalid_axes(self, dtype):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((3, 4, 2), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.rot90(x, 1, axes=(1, 3))\n\n    @testing.for_all_dtypes()\n    def test_rot90_invalid_negative_axes(self, dtype):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((3, 4, 2), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.rot90(x, 1, axes=(1, -2))\n"""
tests/cupy_tests/manipulation_tests/test_shape.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestShape(unittest.TestCase):\n\n    def test_reshape_strides(self):\n        def func(xp):\n            a = testing.shaped_arange((1, 1, 1, 2, 2), xp)\n            return a.strides\n        self.assertEqual(func(numpy), func(cupy))\n\n    def test_reshape2(self):\n        def func(xp):\n            a = xp.zeros((8,), dtype=xp.float32)\n            return a.reshape((1, 1, 1, 4, 1, 2)).strides\n        self.assertEqual(func(numpy), func(cupy))\n\n    @testing.for_orders('CFA')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_nocopy_reshape(self, xp, dtype, order):\n        a = xp.zeros((2, 3, 4), dtype=dtype)\n        b = a.reshape(4, 3, 2, order=order)\n        b[1] = 1\n        return a\n\n    @testing.for_orders('CFA')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_nocopy_reshape_with_order(self, xp, dtype, order):\n        a = xp.zeros((2, 3, 4), dtype=dtype)\n        b = a.reshape(4, 3, 2, order=order)\n        b[1] = 1\n        return a\n\n    @testing.for_orders('CFA')\n    @testing.numpy_cupy_array_equal()\n    def test_transposed_reshape2(self, xp, order):\n        a = testing.shaped_arange((2, 3, 4), xp).transpose(2, 0, 1)\n        return a.reshape(2, 3, 4, order=order)\n\n    @testing.for_orders('CFA')\n    @testing.numpy_cupy_array_equal()\n    def test_reshape_with_unknown_dimension(self, xp, order):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return a.reshape(3, -1, order=order)\n\n    def test_reshape_with_multiple_unknown_dimensions(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(ValueError):\n                a.reshape(3, -1, -1)\n\n    def test_reshape_with_changed_arraysize(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(ValueError):\n                a.reshape(2, 4, 4)\n\n    def test_reshape_invalid_order(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(ValueError):\n                a.reshape(2, 4, 4, order='K')\n\n    def test_reshape_empty_invalid(self):\n        for xp in (numpy, cupy):\n            a = testing.empty(xp)\n            with pytest.raises(ValueError):\n                a.reshape(())\n\n    @testing.numpy_cupy_array_equal()\n    def test_reshape_empty(self, xp):\n        a = testing.empty(xp)\n        return a.reshape((0,))\n\n    @testing.for_orders('CFA')\n    @testing.numpy_cupy_array_equal()\n    def test_external_reshape(self, xp, order):\n        a = xp.zeros((8,), dtype=xp.float32)\n        return xp.reshape(a, (1, 1, 1, 4, 1, 2), order=order)\n\n    @testing.for_orders('CFA')\n    @testing.numpy_cupy_array_equal()\n    def test_ravel(self, xp, order):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        a = a.transpose(2, 0, 1)\n        return a.ravel(order)\n\n    @testing.for_orders('CFA')\n    @testing.numpy_cupy_array_equal()\n    def test_ravel2(self, xp, order):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return a.ravel(order)\n\n    @testing.for_orders('CFA')\n    @testing.numpy_cupy_array_equal()\n    def test_ravel3(self, xp, order):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        a = xp.asfortranarray(a)\n        return a.ravel(order)\n\n    @testing.numpy_cupy_array_equal()\n    def test_external_ravel(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        a = a.transpose(2, 0, 1)\n        return xp.ravel(a)\n\n\n@testing.parameterize(*testing.product({\n    'order_init': ['C', 'F'],\n    'order_reshape': ['C', 'F', 'A', 'c', 'f', 'a'],\n    'shape_in_out': [((2, 3), (1, 6, 1)),  # (shape_init, shape_final)\n                     ((6,), (2, 3)),\n                     ((3, 3, 3), (9, 3))],\n}))\n@testing.gpu\nclass TestReshapeOrder(unittest.TestCase):\n\n    def test_reshape_contiguity(self):\n        shape_init, shape_final = self.shape_in_out\n\n        a_cupy = testing.shaped_arange(shape_init, xp=cupy)\n        a_cupy = cupy.asarray(a_cupy, order=self.order_init)\n        b_cupy = a_cupy.reshape(shape_final, order=self.order_reshape)\n\n        a_numpy = testing.shaped_arange(shape_init, xp=numpy)\n        a_numpy = numpy.asarray(a_numpy, order=self.order_init)\n        b_numpy = a_numpy.reshape(shape_final, order=self.order_reshape)\n\n        assert b_cupy.flags.f_contiguous == b_numpy.flags.f_contiguous\n        assert b_cupy.flags.c_contiguous == b_numpy.flags.c_contiguous\n\n        testing.assert_array_equal(b_cupy.strides, b_numpy.strides)\n        testing.assert_array_equal(b_cupy, b_numpy)\n"""
tests/cupy_tests/manipulation_tests/test_split.py,0,"b'import unittest\n\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestSplit(unittest.TestCase):\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_array_split1(self, xp):\n        a = testing.shaped_arange((3, 11), xp)\n        return xp.array_split(a, 4, 1)\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_array_split2(self, xp):\n        a = testing.shaped_arange((3, 11), xp)\n        return xp.array_split(a, 4, -1)\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_array_split_empty_array(self, xp):\n        a = testing.shaped_arange((5, 0), xp)\n        return xp.array_split(a, [2, 4], 0)\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_array_split_empty_sections(self, xp):\n        a = testing.shaped_arange((3, 11), xp)\n        return xp.array_split(a, [])\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_array_split_out_of_bound1(self, xp):\n        a = testing.shaped_arange((2, 3), xp)\n        return xp.array_split(a, [3])\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_array_split_out_of_bound2(self, xp):\n        a = testing.shaped_arange((0,), xp)\n        return xp.array_split(a, [1])\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_array_split_unordered_sections(self, xp):\n        a = testing.shaped_arange((5,), xp)\n        return xp.array_split(a, [4, 2])\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_array_split_non_divisible(self, xp):\n        a = testing.shaped_arange((5, 3), xp)\n        return xp.array_split(a, 4)\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_dsplit(self, xp):\n        a = testing.shaped_arange((3, 3, 12), xp)\n        return xp.dsplit(a, 4)\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_hsplit_vectors(self, xp):\n        a = testing.shaped_arange((12,), xp)\n        return xp.hsplit(a, 4)\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_hsplit(self, xp):\n        a = testing.shaped_arange((3, 12), xp)\n        return xp.hsplit(a, 4)\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_split_by_sections1(self, xp):\n        a = testing.shaped_arange((3, 11), xp)\n        return xp.split(a, (2, 4, 9), 1)\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_split_by_sections2(self, xp):\n        a = testing.shaped_arange((3, 11), xp)\n        return xp.split(a, (2, 4, 9), -1)\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_split_by_sections3(self, xp):\n        a = testing.shaped_arange((3, 11), xp)\n        return xp.split(a, (-9, 4, -2), 1)\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_split_out_of_bound1(self, xp):\n        a = testing.shaped_arange((2, 3), xp)\n        return xp.split(a, [3])\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_split_out_of_bound2(self, xp):\n        a = testing.shaped_arange((0,), xp)\n        return xp.split(a, [1])\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_split_unordered_sections(self, xp):\n        a = testing.shaped_arange((5,), xp)\n        return xp.split(a, [4, 2])\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_vsplit(self, xp):\n        a = testing.shaped_arange((12, 3), xp)\n        return xp.vsplit(a, 4)\n'"
tests/cupy_tests/manipulation_tests/test_tiling.py,0,"b'import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.parameterize(\n    {\'repeats\': 0, \'axis\': None},\n    {\'repeats\': 2, \'axis\': None},\n    {\'repeats\': 2, \'axis\': 1},\n    {\'repeats\': 2, \'axis\': -1},\n    {\'repeats\': [0, 0, 0], \'axis\': 1},\n    {\'repeats\': [1, 2, 3], \'axis\': 1},\n    {\'repeats\': [1, 2, 3], \'axis\': -2},\n)\n@testing.gpu\nclass TestRepeat(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_array_repeat(self, xp):\n        x = testing.shaped_arange((2, 3, 4), xp)\n        return xp.repeat(x, self.repeats, self.axis)\n\n\nclass TestRepeatRepeatsNdarray(unittest.TestCase):\n\n    def test_func(self):\n        a = testing.shaped_arange((2, 3, 4), cupy)\n        repeats = cupy.array([2, 3], dtype=cupy.int32)\n        with pytest.raises(ValueError, match=r\'repeats\'):\n            cupy.repeat(a, repeats)\n\n    def test_method(self):\n        a = testing.shaped_arange((2, 3, 4), cupy)\n        repeats = cupy.array([2, 3], dtype=cupy.int32)\n        with pytest.raises(ValueError, match=r\'repeats\'):\n            a.repeat(repeats)\n\n\n@testing.parameterize(\n    {\'repeats\': [2], \'axis\': None},\n    {\'repeats\': [2], \'axis\': 1},\n)\n@testing.gpu\nclass TestRepeatListBroadcast(unittest.TestCase):\n\n    """"""Test for `repeats` argument using single element list.\n\n    This feature is only supported in NumPy 1.10 or later.\n    """"""\n\n    @testing.numpy_cupy_array_equal()\n    def test_array_repeat(self, xp):\n        x = testing.shaped_arange((2, 3, 4), xp)\n        return xp.repeat(x, self.repeats, self.axis)\n\n\n@testing.parameterize(\n    {\'repeats\': 0, \'axis\': None},\n    {\'repeats\': 2, \'axis\': None},\n    {\'repeats\': 2, \'axis\': 0},\n    {\'repeats\': [1, 2, 3, 4], \'axis\': None},\n    {\'repeats\': [1, 2, 3, 4], \'axis\': 0},\n)\n@testing.gpu\nclass TestRepeat1D(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_array_repeat(self, xp):\n        x = testing.shaped_arange((4,), xp)\n        return xp.repeat(x, self.repeats, self.axis)\n\n\n@testing.parameterize(\n    {\'repeats\': [2], \'axis\': None},\n    {\'repeats\': [2], \'axis\': 0},\n)\n@testing.gpu\nclass TestRepeat1DListBroadcast(unittest.TestCase):\n\n    """"""See comment in TestRepeatListBroadcast class.""""""\n\n    @testing.numpy_cupy_array_equal()\n    def test_array_repeat(self, xp):\n        x = testing.shaped_arange((4,), xp)\n        return xp.repeat(x, self.repeats, self.axis)\n\n\n@testing.parameterize(\n    {\'repeats\': -3, \'axis\': None},\n    {\'repeats\': [-3, -3], \'axis\': 0},\n    {\'repeats\': [1, 2, 3], \'axis\': None},\n    {\'repeats\': [1, 2], \'axis\': 1},\n    {\'repeats\': 2, \'axis\': -4},\n    {\'repeats\': 2, \'axis\': 3},\n)\n@testing.gpu\nclass TestRepeatFailure(unittest.TestCase):\n\n    def test_repeat_failure(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(ValueError):\n                xp.repeat(x, self.repeats, self.axis)\n\n\n@testing.parameterize(\n    {\'reps\': 0},\n    {\'reps\': 1},\n    {\'reps\': 2},\n    {\'reps\': (0, 1)},\n    {\'reps\': (2, 3)},\n    {\'reps\': (2, 3, 4, 5)},\n)\n@testing.gpu\nclass TestTile(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_array_tile(self, xp):\n        x = testing.shaped_arange((2, 3, 4), xp)\n        return xp.tile(x, self.reps)\n\n\n@testing.parameterize(\n    {\'reps\': -1},\n    {\'reps\': (-1, -2)},\n)\n@testing.gpu\nclass TestTileFailure(unittest.TestCase):\n\n    def test_tile_failure(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(ValueError):\n                xp.tile(x, -3)\n'"
tests/cupy_tests/manipulation_tests/test_transpose.py,0,"b'import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestTranspose(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_moveaxis1(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return xp.moveaxis(a, [0, 1], [1, 2])\n\n    @testing.numpy_cupy_array_equal()\n    def test_moveaxis2(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return xp.moveaxis(a, 1, -1)\n\n    @testing.numpy_cupy_array_equal()\n    def test_moveaxis3(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return xp.moveaxis(a, [0, 2], [1, 0])\n\n    @testing.numpy_cupy_array_equal()\n    def test_moveaxis4(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return xp.moveaxis(a, [2, 0], [1, 0])\n\n    @testing.numpy_cupy_array_equal()\n    def test_moveaxis5(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return xp.moveaxis(a, [2, 0], [0, 1])\n\n    @testing.numpy_cupy_array_equal()\n    def test_moveaxis6(self, xp):\n        a = testing.shaped_arange((2, 3, 4, 5, 6), xp)\n        return xp.moveaxis(a, [0, 2, 1], [3, 4, 0])\n\n    # dim is too large\n    def test_moveaxis_invalid1_1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(numpy.AxisError):\n                xp.moveaxis(a, [0, 1], [1, 3])\n\n    def test_moveaxis_invalid1_2(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(numpy.AxisError):\n                xp.moveaxis(a, [0, 1], [1, 3])\n\n    # dim is too small\n    def test_moveaxis_invalid2_1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(numpy.AxisError):\n                xp.moveaxis(a, [0, -4], [1, 2])\n\n    def test_moveaxis_invalid2_2(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(numpy.AxisError):\n                xp.moveaxis(a, [0, -4], [1, 2])\n\n    # len(source) != len(destination)\n    def test_moveaxis_invalid3(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(ValueError):\n                xp.moveaxis(a, [0, 1, 2], [1, 2])\n\n    # len(source) != len(destination)\n    def test_moveaxis_invalid4(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(ValueError):\n                xp.moveaxis(a, [0, 1], [1, 2, 0])\n\n    # Use the same axis twice\n    def test_moveaxis_invalid5_1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(numpy.AxisError):\n                xp.moveaxis(a, [1, -1], [1, 3])\n\n    def test_moveaxis_invalid5_2(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(ValueError):\n                xp.moveaxis(a, [0, 1], [-1, 2])\n\n    def test_moveaxis_invalid5_3(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(ValueError):\n                xp.moveaxis(a, [0, 1], [1, 1])\n\n    @testing.numpy_cupy_array_equal()\n    def test_rollaxis(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return xp.rollaxis(a, 2)\n\n    def test_rollaxis_failure(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(ValueError):\n                xp.rollaxis(a, 3)\n\n    @testing.numpy_cupy_array_equal()\n    def test_swapaxes(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return xp.swapaxes(a, 2, 0)\n\n    def test_swapaxes_failure(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(ValueError):\n                xp.swapaxes(a, 3, 0)\n\n    @testing.numpy_cupy_array_equal()\n    def test_transpose(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return a.transpose(-1, 0, 1)\n\n    @testing.numpy_cupy_array_equal()\n    def test_transpose_empty(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return a.transpose()\n\n    @testing.numpy_cupy_array_equal()\n    def test_transpose_none(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return a.transpose(None)\n\n    @testing.numpy_cupy_array_equal()\n    def test_external_transpose(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return xp.transpose(a, (-1, 0, 1))\n\n    @testing.numpy_cupy_array_equal()\n    def test_external_transpose_all(self, xp):\n        a = testing.shaped_arange((2, 3, 4), xp)\n        return xp.transpose(a)\n'"
tests/cupy_tests/math_tests/__init__.py,0,b''
tests/cupy_tests/math_tests/test_arithmetic.py,0,"b""import itertools\nimport unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\nfloat_types = [numpy.float16, numpy.float32, numpy.float64]\ncomplex_types = [numpy.complex, numpy.complex64, numpy.complex128]\nsigned_int_types = [numpy.int8, numpy.int16, numpy.int32, numpy.int64]\nunsigned_int_types = [numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64]\nint_types = signed_int_types + unsigned_int_types\nall_types = [numpy.bool] + float_types + int_types + complex_types\nnegative_types = (\n    [numpy.bool] + float_types + signed_int_types + complex_types)\nnegative_no_complex_types = [numpy.bool] + float_types + signed_int_types\nno_complex_types = [numpy.bool] + float_types + int_types\n\n\n@testing.gpu\n@testing.parameterize(*(\n    testing.product({\n        'nargs': [1],\n        'name': ['reciprocal', 'conj', 'conjugate', 'angle'],\n    }) + testing.product({\n        'nargs': [2],\n        'name': [\n            'add', 'multiply', 'divide', 'power', 'subtract', 'true_divide',\n            'floor_divide', 'fmod', 'remainder'],\n    })\n))\nclass TestArithmeticRaisesWithNumpyInput(unittest.TestCase):\n\n    def test_raises_with_numpy_input(self):\n        nargs = self.nargs\n        name = self.name\n\n        # Check TypeError is raised if numpy.ndarray is given as input\n        func = getattr(cupy, name)\n        for input_xp_list in itertools.product(*[[numpy, cupy]] * nargs):\n            if all(xp is cupy for xp in input_xp_list):\n                # We don't test all-cupy-array inputs here\n                continue\n            arys = [xp.array([2, -3]) for xp in input_xp_list]\n            with self.assertRaises(TypeError):\n                func(*arys)\n\n\n@testing.gpu\n@testing.parameterize(*(\n    testing.product({\n        'arg1': ([testing.shaped_arange((2, 3), numpy, dtype=d)\n                  for d in all_types\n                  ] + [0, 0.0j, 0j, 2, 2.0, 2j, True, False]),\n        'name': ['conj', 'conjugate', 'angle', 'real', 'imag'],\n    }) + testing.product({\n        'arg1': ([numpy.array([-3, -2, -1, 1, 2, 3], dtype=d)\n                  for d in negative_types\n                  ] + [0, 0.0j, 0j, 2, 2.0, 2j, -2, -2.0, -2j, True, False]),\n        'name': ['angle'],\n    }) + testing.product({\n        'arg1': ([testing.shaped_arange((2, 3), numpy, dtype=d) + 1\n                  for d in all_types\n                  ] + [2, 2.0, 2j, True]),\n        'name': ['reciprocal'],\n    })\n))\nclass TestArithmeticUnary(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_unary(self, xp):\n        arg1 = self.arg1\n        if isinstance(arg1, numpy.ndarray):\n            arg1 = xp.asarray(arg1)\n        y = getattr(xp, self.name)(arg1)\n\n        if self.name in ('real', 'imag'):\n            # Some NumPy functions return Python scalars for Python scalar\n            # inputs.\n            # We need to convert them to arrays to compare with CuPy outputs.\n            if xp is numpy and isinstance(arg1, (bool, int, float, complex)):\n                y = xp.asarray(y)\n\n            # TODO(niboshi): Fix this\n            # numpy.real and numpy.imag return Python int if the input is\n            # Python bool. CuPy should return an array of dtype.int32 or\n            # dtype.int64 (depending on the platform) in such cases, instead\n            # of an array of dtype.bool.\n            if xp is cupy and isinstance(arg1, bool):\n                y = y.astype(int)\n\n        return y\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(3, 2), (), (3, 0, 2)],\n}))\n@testing.gpu\nclass TestComplex(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_real_ndarray_nocomplex(self, xp, dtype):\n        x = testing.shaped_arange(self.shape, xp, dtype=dtype)\n        real = x.real\n        assert real is x  # real returns self\n        return real\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_real_nocomplex(self, xp, dtype):\n        x = testing.shaped_arange(self.shape, xp, dtype=dtype)\n        real = xp.real(x)\n        assert real is x  # real returns self\n        return real\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_imag_ndarray_nocomplex(self, xp, dtype):\n        x = testing.shaped_arange(self.shape, xp, dtype=dtype)\n        imag = x.imag\n        return imag\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_imag_nocomplex(self, xp, dtype):\n        x = testing.shaped_arange(self.shape, xp, dtype=dtype)\n        imag = xp.imag(x)\n        return imag\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_real_ndarray_complex(self, xp, dtype):\n        x = testing.shaped_arange(self.shape, xp, dtype=dtype)\n        x_ = x.copy()\n        real = x_.real\n        # real returns a view\n        assert real.base is x_\n        x_ += 1 + 1j\n        testing.assert_array_equal(real, x.real + 1)\n        return real\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_real_complex(self, xp, dtype):\n        x = testing.shaped_arange(self.shape, xp, dtype=dtype)\n        x_ = x.copy()\n        real = xp.real(x_)\n        # real returns a view\n        assert real.base is x_\n        x_ += 1 + 1j\n        testing.assert_array_equal(real, x.real + 1)\n        return real\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_imag_ndarray_complex(self, xp, dtype):\n        x = testing.shaped_arange(self.shape, xp, dtype=dtype)\n        x_ = x.copy()\n        imag = x_.imag\n        # imag returns a view\n        assert imag.base is x_\n        x_ += 1 + 1j\n        testing.assert_array_equal(imag, x.imag + 1)\n        return imag\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_imag_complex(self, xp, dtype):\n        x = testing.shaped_arange(self.shape, xp, dtype=dtype)\n        x_ = x.copy()\n        imag = xp.imag(x_)\n        # imag returns a view\n        assert imag.base is x_\n        x_ += 1 + 1j\n        testing.assert_array_equal(imag, x.imag + 1)\n        return imag\n\n\nclass ArithmeticBinaryBase:\n\n    @testing.numpy_cupy_allclose(atol=1e-4)\n    def check_binary(self, xp):\n        arg1 = self.arg1\n        arg2 = self.arg2\n        np1 = numpy.asarray(arg1)\n        np2 = numpy.asarray(arg2)\n        dtype1 = np1.dtype\n        dtype2 = np2.dtype\n\n        if self.name == 'power':\n            # TODO(niboshi): Fix this: power(0, 1j)\n            #     numpy => 1+0j\n            #     cupy => 0j\n            if dtype2 in complex_types and (np1 == 0).any():\n                return xp.array(True)\n\n            # TODO(niboshi): Fix this: xp.power(0j, 0)\n            #     numpy => 1+0j\n            #     cupy => 0j\n            c_arg1 = dtype1 in complex_types\n            if c_arg1 and (np1 == 0j).any() and (np2 == 0).any():\n                return xp.array(True)\n\n        # TODO(niboshi): Fix this: xp.add(0j, xp.array([2.], 'f')).dtype\n        #     numpy => complex64\n        #     cupy => complex128\n        if isinstance(arg1, complex):\n            if dtype2 in (numpy.float16, numpy.float32):\n                return xp.array(True)\n\n        if isinstance(arg1, numpy.ndarray):\n            arg1 = xp.asarray(arg1)\n        if isinstance(arg2, numpy.ndarray):\n            arg2 = xp.asarray(arg2)\n\n        # Subtraction between booleans is not allowed.\n        if (self.name == 'subtract'\n                and dtype1 == numpy.bool_\n                and dtype2 == numpy.bool_):\n            return xp.array(True)\n\n        func = getattr(xp, self.name)\n        with testing.NumpyError(divide='ignore'):\n            with numpy.warnings.catch_warnings():\n                numpy.warnings.filterwarnings('ignore')\n                if self.use_dtype:\n                    y = func(arg1, arg2, dtype=self.dtype)\n                else:\n                    y = func(arg1, arg2)\n\n        # TODO(niboshi): Fix this. If rhs is a Python complex,\n        #    numpy returns complex64\n        #    cupy returns complex128\n        if xp is cupy and isinstance(arg2, complex):\n            if dtype1 in (numpy.float16, numpy.float32):\n                y = y.astype(numpy.complex64)\n\n        # NumPy returns different values (nan/inf) on division by zero\n        # depending on the architecture.\n        # As it is not possible for CuPy to replicate this behavior, we ignore\n        # the difference here.\n        if self.name in ('floor_divide', 'remainder'):\n            if y.dtype in (float_types + complex_types) and (np2 == 0).any():\n                y = xp.asarray(y)\n                y[y == numpy.inf] = numpy.nan\n                y[y == -numpy.inf] = numpy.nan\n\n        return y\n\n\n@testing.gpu\n@testing.parameterize(*(\n    testing.product({\n        # TODO(unno): boolean subtract causes DeprecationWarning in numpy>=1.13\n        'arg1': [testing.shaped_arange((2, 3), numpy, dtype=d)\n                 for d in all_types\n                 ] + [0, 0.0, 0j, 2, 2.0, 2j, True, False],\n        'arg2': [testing.shaped_reverse_arange((2, 3), numpy, dtype=d)\n                 for d in all_types\n                 ] + [0, 0.0, 0j, 2, 2.0, 2j, True, False],\n        'name': ['add', 'multiply', 'power', 'subtract'],\n    }) + testing.product({\n        'arg1': [numpy.array([-3, -2, -1, 1, 2, 3], dtype=d)\n                 for d in negative_types\n                 ] + [0, 0.0, 0j, 2, 2.0, 2j, -2, -2.0, -2j, True, False],\n        'arg2': [numpy.array([-3, -2, -1, 1, 2, 3], dtype=d)\n                 for d in negative_types\n                 ] + [0, 0.0, 0j, 2, 2.0, 2j, -2, -2.0, -2j, True, False],\n        'name': ['divide', 'true_divide', 'subtract'],\n    })\n))\nclass TestArithmeticBinary(ArithmeticBinaryBase, unittest.TestCase):\n\n    def test_binary(self):\n        self.use_dtype = False\n        self.check_binary()\n\n\n@testing.gpu\n@testing.parameterize(*(\n    testing.product({\n        'arg1': [numpy.array([-3, -2, -1, 1, 2, 3], dtype=d)\n                 for d in int_types\n                 ] + [0, 0.0, 2, 2.0, -2, -2.0, True, False],\n        'arg2': [numpy.array([-3, -2, -1, 1, 2, 3], dtype=d)\n                 for d in int_types\n                 ] + [0, 0.0, 2, 2.0, -2, -2.0, True, False],\n        'name': ['true_divide'],\n        'dtype': [numpy.float64],\n        'use_dtype': [True, False],\n    }) + testing.product({\n        'arg1': [numpy.array([-3, -2, -1, 1, 2, 3], dtype=d)\n                 for d in float_types] + [0.0, 2.0, -2.0],\n        'arg2': [numpy.array([-3, -2, -1, 1, 2, 3], dtype=d)\n                 for d in float_types] + [0.0, 2.0, -2.0],\n        'name': ['power', 'true_divide', 'subtract'],\n        'dtype': [numpy.float64],\n        'use_dtype': [True, False],\n    }) + testing.product({\n        'arg1': [testing.shaped_arange((2, 3), numpy, dtype=d)\n                 for d in no_complex_types\n                 ] + [0, 0.0, 2, 2.0, -2, -2.0, True, False],\n        'arg2': [testing.shaped_reverse_arange((2, 3), numpy, dtype=d)\n                 for d in no_complex_types\n                 ] + [0, 0.0, 2, 2.0, -2, -2.0, True, False],\n        'name': ['floor_divide', 'fmod', 'remainder'],\n        'dtype': [numpy.float64],\n        'use_dtype': [True, False],\n    }) + testing.product({\n        'arg1': [numpy.array([-3, -2, -1, 1, 2, 3], dtype=d)\n                 for d in negative_no_complex_types\n                 ] + [0, 0.0, 2, 2.0, -2, -2.0, True, False],\n        'arg2': [numpy.array([-3, -2, -1, 1, 2, 3], dtype=d)\n                 for d in negative_no_complex_types\n                 ] + [0, 0.0, 2, 2.0, -2, -2.0, True, False],\n        'name': ['floor_divide', 'fmod', 'remainder'],\n        'dtype': [numpy.float64],\n        'use_dtype': [True, False],\n    })\n))\nclass TestArithmeticBinary2(ArithmeticBinaryBase, unittest.TestCase):\n\n    def test_binary(self):\n        if (self.use_dtype and\n                numpy.lib.NumpyVersion(numpy.__version__) < '1.10.0'):\n            raise unittest.SkipTest('Test for numpy>=1.10')\n        self.check_binary()\n\n\nclass TestArithmeticModf(unittest.TestCase):\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_modf(self, xp, dtype):\n        a = xp.array([-2.5, -1.5, -0.5, 0, 0.5, 1.5, 2.5], dtype=dtype)\n        b, c = xp.modf(a)\n        d = xp.empty((2, 7), dtype=dtype)\n        d[0] = b\n        d[1] = c\n        return d\n\n\n@testing.parameterize(*testing.product({\n    'xp': [numpy, cupy],\n    'shape': [(3, 2), (), (3, 0, 2)]\n}))\n@testing.gpu\nclass TestBoolSubtract(unittest.TestCase):\n\n    def test_bool_subtract(self):\n        xp = self.xp\n        if xp is numpy and not testing.numpy_satisfies('>=1.14.0'):\n            raise unittest.SkipTest('NumPy<1.14.0')\n        shape = self.shape\n        x = testing.shaped_random(shape, xp, dtype=numpy.bool_)\n        y = testing.shaped_random(shape, xp, dtype=numpy.bool_)\n        with pytest.raises(TypeError):\n            xp.subtract(x, y)\n"""
tests/cupy_tests/math_tests/test_explog.py,0,"b""import unittest\n\nimport numpy\n\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestExplog(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_unary(self, name, xp, dtype, no_complex=False):\n        if no_complex:\n            if numpy.dtype(dtype).kind == 'c':\n                return xp.array(True)\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return getattr(xp, name)(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_binary(self, name, xp, dtype, no_complex=False):\n        if no_complex:\n            if numpy.dtype(dtype).kind == 'c':\n                return xp.array(True)\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        b = testing.shaped_reverse_arange((2, 3), xp, dtype)\n        return getattr(xp, name)(a, b)\n\n    def test_exp(self):\n        self.check_unary('exp')\n\n    def test_expm1(self):\n        self.check_unary('expm1')\n\n    def test_exp2(self):\n        self.check_unary('exp2')\n\n    def test_log(self):\n        with testing.NumpyError(divide='ignore'):\n            self.check_unary('log')\n\n    def test_log10(self):\n        with testing.NumpyError(divide='ignore'):\n            self.check_unary('log10')\n\n    def test_log2(self):\n        with testing.NumpyError(divide='ignore'):\n            self.check_unary('log2')\n\n    def test_log1p(self):\n        self.check_unary('log1p')\n\n    def test_logaddexp(self):\n        self.check_binary('logaddexp', no_complex=True)\n\n    def test_logaddexp2(self):\n        self.check_binary('logaddexp2', no_complex=True)\n"""
tests/cupy_tests/math_tests/test_floating.py,0,"b""import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestFloating(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_signbit(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return xp.signbit(a)\n\n    @testing.for_all_dtypes_combination(\n        ('dtype_a', 'dtype_b'), no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_copysign_combination(self, xp, dtype_a, dtype_b):\n        a = testing.shaped_arange((2, 3), xp, dtype_a)\n        b = testing.shaped_reverse_arange((2, 3), xp, dtype_b)\n        return xp.copysign(a, b)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_copysign_float(self, xp, dtype):\n        a = xp.array([-xp.inf, -3, -0.0, 0, 3, xp.inf], dtype=dtype)[:, None]\n        b = xp.array([-xp.inf, -3, -0.0, 0, 3, xp.inf], dtype=dtype)[None, :]\n        return xp.copysign(a, b)\n\n    @testing.for_float_dtypes(name='ftype')\n    @testing.for_dtypes(['i', 'l'], name='itype')\n    @testing.numpy_cupy_array_equal()\n    def test_ldexp(self, xp, ftype, itype):\n        a = xp.array([-3, -2, -1, 0, 1, 2, 3], dtype=ftype)\n        b = xp.array([-3, -2, -1, 0, 1, 2, 3], dtype=itype)\n        return xp.ldexp(a, b)\n\n    @testing.for_float_dtypes()\n    def test_frexp(self, dtype):\n        numpy_a = numpy.array([-300, -20, -10, -1, 0, 1, 10, 20, 300],\n                              dtype=dtype)\n        numpy_b, numpy_c = numpy.frexp(numpy_a)\n\n        cupy_a = cupy.array(numpy_a)\n        cupy_b, cupy_c = cupy.frexp(cupy_a)\n\n        testing.assert_array_equal(cupy_b, numpy_b)\n        testing.assert_array_equal(cupy_c, numpy_c)\n\n    @testing.for_all_dtypes_combination(\n        ('dtype_a', 'dtype_b'), no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_nextafter_combination(self, xp, dtype_a, dtype_b):\n        a = testing.shaped_arange((2, 3), xp, dtype_a)\n        # skip 0 because cupy (may) handle denormals differently (-ftz=true)\n        a[a == 0] = 1\n        b = testing.shaped_reverse_arange((2, 3), xp, dtype_b)\n        return xp.nextafter(a, b)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_nextafter_float(self, xp, dtype):\n        a = xp.array([-5, -3, 3, 5], dtype=dtype)[:, None]\n        b = xp.array([-xp.inf, -4, 0, 4, xp.inf], dtype=dtype)[None, :]\n        return xp.nextafter(a, b)\n"""
tests/cupy_tests/math_tests/test_hyperbolic.py,0,"b""import unittest\n\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestHyperbolic(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_unary(self, name, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return getattr(xp, name)(a)\n\n    @testing.for_dtypes(['e', 'f', 'd'])\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_unary_unit(self, name, xp, dtype):\n        a = xp.array([0.2, 0.4, 0.6, 0.8], dtype=dtype)\n        return getattr(xp, name)(a)\n\n    def test_sinh(self):\n        self.check_unary('sinh')\n\n    def test_cosh(self):\n        self.check_unary('cosh')\n\n    def test_tanh(self):\n        self.check_unary('tanh')\n\n    def test_arcsinh(self):\n        self.check_unary('arcsinh')\n\n    @testing.for_dtypes(['e', 'f', 'd'])\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_arccosh(self, xp, dtype):\n        a = xp.array([1, 2, 3], dtype=dtype)\n        return xp.arccosh(a)\n\n    def test_arctanh(self):\n        self.check_unary_unit('arctanh')\n"""
tests/cupy_tests/math_tests/test_matmul.py,0,"b""import operator\nimport unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.parameterize(\n    *testing.product({\n        'shape_pair': [\n            # dot test\n            ((3, 2), (2, 4)),\n            ((3, 0), (0, 4)),\n            ((0, 2), (2, 4)),\n            ((3, 2), (2, 0)),\n            ((2,), (2, 4)),\n            ((0,), (0, 4)),\n            ((3, 2), (2,)),\n            ((3, 0), (0,)),\n            ((2,), (2,)),\n            ((0,), (0,)),\n            # matmul test\n            ((5, 3, 2), (5, 2, 4)),\n            ((0, 3, 2), (0, 2, 4)),\n            ((5, 3, 2), (2, 4)),\n            ((0, 3, 2), (2, 4)),\n            ((3, 2), (5, 2, 4)),\n            ((3, 2), (0, 2, 4)),\n            ((5, 3, 2), (1, 2, 4)),\n            ((0, 3, 2), (1, 2, 4)),\n            ((1, 3, 2), (5, 2, 4)),\n            ((1, 3, 2), (0, 2, 4)),\n            ((5, 3, 2), (2,)),\n            ((5, 3, 0), (0,)),\n            ((2,), (5, 2, 4)),\n            ((0,), (5, 0, 4)),\n            ((2, 2, 3, 2), (2, 2, 2, 4)),\n            ((5, 0, 3, 2), (5, 0, 2, 4)),\n            ((6, 5, 3, 2), (2, 4)),\n            ((5, 0, 3, 2), (2, 4)),\n            ((3, 2), (6, 5, 2, 4)),\n            ((3, 2), (5, 0, 2, 4)),\n            ((1, 5, 3, 2), (6, 1, 2, 4)),\n            ((1, 0, 3, 2), (6, 1, 2, 4)),\n            ((6, 1, 3, 2), (1, 5, 2, 4)),\n            ((6, 1, 3, 2), (1, 0, 2, 4)),\n            ((6, 5, 3, 2), (2,)),\n            ((6, 5, 3, 0), (0,)),\n            ((2,), (6, 5, 2, 4)),\n            ((0,), (6, 5, 0, 4)),\n            ((1, 3, 3), (10, 1, 3, 1)),\n        ],\n    }))\n@testing.gpu\nclass TestMatmul(unittest.TestCase):\n\n    @testing.for_all_dtypes(name='dtype1')\n    @testing.for_all_dtypes(name='dtype2')\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-3)  # required for uint8\n    def test_operator_matmul(self, xp, dtype1, dtype2):\n        x1 = testing.shaped_arange(self.shape_pair[0], xp, dtype1)\n        x2 = testing.shaped_arange(self.shape_pair[1], xp, dtype2)\n        return operator.matmul(x1, x2)\n\n    @testing.for_all_dtypes(name='dtype1')\n    @testing.for_all_dtypes(name='dtype2')\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-3)  # required for uint8\n    def test_cupy_matmul(self, xp, dtype1, dtype2):\n        x1 = testing.shaped_arange(self.shape_pair[0], xp, dtype1)\n        x2 = testing.shaped_arange(self.shape_pair[1], xp, dtype2)\n        return xp.matmul(x1, x2)\n\n\n@testing.parameterize(\n    *testing.product({\n        'shape_pair': [\n            ((6, 5, 3, 2), (6, 5, 2, 4)),\n            ((6, 5, 3, 2), (6, 1, 2, 4)),\n            ((6, 5, 3, 2), (1, 5, 2, 4)),\n            ((6, 5, 3, 2), (1, 1, 2, 4)),\n            ((6, 1, 3, 2), (6, 5, 2, 4)),\n            ((1, 5, 3, 2), (6, 5, 2, 4)),\n            ((1, 1, 3, 2), (6, 5, 2, 4)),\n            ((3, 2), (6, 5, 2, 4)),\n            ((6, 5, 3, 2), (2, 4)),\n            ((2,), (6, 5, 2, 4)),\n            ((6, 5, 3, 2), (2,)),\n        ],\n    }))\n@testing.gpu\nclass TestMatmulLarge(unittest.TestCase):\n\n    # Avoid overflow\n    skip_dtypes = {\n        (numpy.int8, numpy.uint8),\n        (numpy.int8, numpy.int16),\n        (numpy.int8, numpy.float16),\n        (numpy.uint8, numpy.uint8),\n        (numpy.uint8, numpy.int16),\n        (numpy.uint8, numpy.uint16),\n        (numpy.int16, numpy.int16),\n        (numpy.uint16, numpy.uint16),\n    }\n\n    @testing.for_all_dtypes(name='dtype1')\n    @testing.for_all_dtypes(name='dtype2')\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-3)  # required for uint8\n    def test_operator_matmul(self, xp, dtype1, dtype2):\n        if ((dtype1, dtype2) in self.skip_dtypes or\n                (dtype2, dtype1) in self.skip_dtypes):\n            return xp.array([])\n        x1 = testing.shaped_random(self.shape_pair[0], xp, dtype1)\n        x2 = testing.shaped_random(self.shape_pair[1], xp, dtype2)\n        return operator.matmul(x1, x2)\n\n    @testing.for_all_dtypes(name='dtype1')\n    @testing.for_all_dtypes(name='dtype2')\n    @testing.numpy_cupy_allclose(rtol=1e-3, atol=1e-3)  # required for uint8\n    def test_cupy_matmul(self, xp, dtype1, dtype2):\n        if ((dtype1, dtype2) in self.skip_dtypes or\n                (dtype2, dtype1) in self.skip_dtypes):\n            return xp.array([])\n        shape1, shape2 = self.shape_pair\n        x1 = testing.shaped_random(shape1, xp, dtype1)\n        x2 = testing.shaped_random(shape2, xp, dtype2)\n        return xp.matmul(x1, x2)\n\n\n@testing.parameterize(\n    *testing.product({\n        'shape_pair': [\n            ((5, 3, 1), (3, 1, 4)),\n            ((3, 2, 3), (3, 2, 4)),\n            ((3, 2), ()),\n            ((), (3, 2)),\n            ((), ()),\n            ((3, 2), (1,)),\n            ((0, 2), (3, 0)),\n            ((0, 1, 1), (2, 1, 1)),\n        ],\n    }))\n@testing.gpu\nclass TestMatmulInvalidShape(unittest.TestCase):\n\n    def test_invalid_shape(self):\n        for xp in (numpy, cupy):\n            shape1, shape2 = self.shape_pair\n            x1 = testing.shaped_arange(shape1, xp, numpy.float32)\n            x2 = testing.shaped_arange(shape2, xp, numpy.float32)\n            with pytest.raises(ValueError):\n                xp.matmul(x1, x2)\n"""
tests/cupy_tests/math_tests/test_misc.py,0,"b""import sys\nimport unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestMisc(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_unary(self, name, xp, dtype, no_bool=False):\n        if no_bool and numpy.dtype(dtype).char == '?':\n            return numpy.int_(0)\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return getattr(xp, name)(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_binary(self, name, xp, dtype, no_bool=False):\n        if no_bool and numpy.dtype(dtype).char == '?':\n            return numpy.int_(0)\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        b = testing.shaped_reverse_arange((2, 3), xp, dtype)\n        return getattr(xp, name)(a, b)\n\n    @testing.for_dtypes(['?', 'b', 'h', 'i', 'q', 'e', 'f', 'd', 'F', 'D'])\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_unary_negative(self, name, xp, dtype, no_bool=False):\n        if no_bool and numpy.dtype(dtype).char == '?':\n            return numpy.int_(0)\n        a = xp.array([-3, -2, -1, 1, 2, 3], dtype=dtype)\n        if numpy.dtype(dtype).kind == 'c':\n            a += (a * 1j).astype(dtype)\n        return getattr(xp, name)(a)\n\n    @testing.for_dtypes(['e', 'f', 'd', 'F', 'D'])\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_unary_inf(self, name, xp, dtype):\n        inf = numpy.inf\n        if numpy.dtype(dtype).kind != 'c':\n            a = xp.array([0, -1, 1, -inf, inf], dtype=dtype)\n        else:\n            a = xp.array([complex(x, y)\n                          for x in [0, -1, 1, -inf, inf]\n                          for y in [0, -1, 1, -inf, inf]],\n                         dtype=dtype)\n        return getattr(xp, name)(a)\n\n    @testing.for_dtypes(['e', 'f', 'd', 'F', 'D'])\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_unary_nan(self, name, xp, dtype):\n        nan = numpy.nan\n        if numpy.dtype(dtype).kind != 'c':\n            a = xp.array([0, -1, 1, -nan, nan], dtype=dtype)\n        else:\n            a = xp.array([complex(x, y)\n                          for x in [0, -1, 1, -nan, nan]\n                          for y in [0, -1, 1, -nan, nan]],\n                         dtype=dtype)\n        return getattr(xp, name)(a)\n\n    @testing.for_dtypes(['e', 'f', 'd', 'F', 'D'])\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_unary_inf_nan(self, name, xp, dtype):\n        inf = numpy.inf\n        nan = numpy.nan\n        if numpy.dtype(dtype).kind != 'c':\n            a = xp.array([0, -1, 1, -inf, inf, -nan, nan], dtype=dtype)\n        else:\n            a = xp.array([complex(x, y)\n                          for x in [0, -1, 1, -inf, inf, -nan, nan]\n                          for y in [0, -1, 1, -inf, inf, -nan, nan]],\n                         dtype=dtype)\n        return getattr(xp, name)(a)\n\n    @testing.for_dtypes(['e', 'f', 'd', 'F', 'D'])\n    @testing.numpy_cupy_array_equal()\n    def check_binary_nan(self, name, xp, dtype):\n        a = xp.array([-3, numpy.NAN, -1, numpy.NAN, 0, numpy.NAN, 2],\n                     dtype=dtype)\n        b = xp.array([numpy.NAN, numpy.NAN, 1, 0, numpy.NAN, -1, -2],\n                     dtype=dtype)\n        return getattr(xp, name)(a, b)\n\n    @unittest.skipIf(\n        sys.platform == 'win32', 'dtype problem on Windows')\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_clip1(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return a.clip(3, 13)\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_clip3(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return a.clip(3, 13)\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_clip_min_none(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return a.clip(None, 3)\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_clip_max_none(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return a.clip(3, None)\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    def test_clip_min_max_none(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp, dtype)\n            with pytest.raises(ValueError):\n                a.clip(None, None)\n\n    @unittest.skipIf(\n        sys.platform == 'win32', 'dtype problem on Windows')\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_external_clip1(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.clip(a, 3, 13)\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_external_clip2(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.clip(a, 3, 13)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_clip2(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        a_min = xp.array([3, 4, 5, 6], dtype=dtype)\n        a_max = xp.array([[10], [9], [8]], dtype=dtype)\n        return a.clip(a_min, a_max)\n\n    def test_sqrt(self):\n        self.check_unary('sqrt')\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_cbrt(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.cbrt(a)\n\n    def test_square(self):\n        self.check_unary('square')\n\n    def test_absolute(self):\n        self.check_unary('absolute')\n\n    def test_absolute_negative(self):\n        self.check_unary_negative('absolute')\n\n    def test_sign(self):\n        self.check_unary('sign', no_bool=True)\n\n    def test_sign_negative(self):\n        self.check_unary_negative('sign', no_bool=True)\n\n    def test_maximum(self):\n        self.check_binary('maximum')\n\n    def test_maximum_nan(self):\n        self.check_binary_nan('maximum')\n\n    def test_minimum(self):\n        self.check_binary('minimum')\n\n    def test_minimum_nan(self):\n        self.check_binary_nan('minimum')\n\n    def test_fmax(self):\n        self.check_binary('fmax')\n\n    def test_fmax_nan(self):\n        self.check_binary_nan('fmax')\n\n    def test_fmin(self):\n        self.check_binary('fmin')\n\n    def test_fmin_nan(self):\n        self.check_binary_nan('fmin')\n\n    def test_nan_to_num(self):\n        self.check_unary('nan_to_num')\n\n    def test_nan_to_num_negative(self):\n        self.check_unary_negative('nan_to_num')\n\n    def test_nan_to_num_for_old_numpy(self):\n        self.check_unary('nan_to_num', no_bool=True)\n\n    def test_nan_to_num_negative_for_old_numpy(self):\n        self.check_unary_negative('nan_to_num', no_bool=True)\n\n    def test_nan_to_num_inf(self):\n        self.check_unary_inf('nan_to_num')\n\n    def test_nan_to_num_nan(self):\n        self.check_unary_nan('nan_to_num')\n\n    def test_nan_to_num_inf_nan(self):\n        self.check_unary_inf_nan('nan_to_num')\n"""
tests/cupy_tests/math_tests/test_rational.py,0,"b""import unittest\n\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestRational(unittest.TestCase):\n\n    @testing.for_dtypes(['?', 'e', 'f', 'd', 'F', 'D'])\n    def test_gcd_dtype_check(self, dtype):\n        a = cupy.random.randint(-10, 10, size=(10, 10)).astype(dtype)\n        b = cupy.random.randint(-10, 10, size=(10, 10)).astype(dtype)\n        with pytest.raises(TypeError):\n            cupy.gcd(a, b)\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_gcd_check_boundary_cases(self, xp, dtype):\n        a = xp.array([0, -10, -5, 10, 410, 1, 6, 33])\n        b = xp.array([0, 5, -10, -5, 20, 51, 6, 42])\n        return xp.gcd(a, b)\n\n    @testing.for_dtypes(['?', 'e', 'f', 'd', 'F', 'D'])\n    def test_lcm_dtype_check(self, dtype):\n        a = cupy.random.randint(-10, 10, size=(10, 10)).astype(dtype)\n        b = cupy.random.randint(-10, 10, size=(10, 10)).astype(dtype)\n        with pytest.raises(TypeError):\n            cupy.lcm(a, b)\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_lcm_check_boundary_cases(self, xp, dtype):\n        a = xp.array([0, -10, -5, 10, 410, 1, 6, 33])\n        b = xp.array([0, 5, -10, -5, 20, 51, 6, 42])\n        return xp.lcm(a, b)\n"""
tests/cupy_tests/math_tests/test_rounding.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestRounding(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_unary(self, name, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return getattr(xp, name)(a)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_unary_complex(self, name, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return getattr(xp, name)(a)\n\n    @testing.for_complex_dtypes()\n    def check_unary_complex_unsupported(self, name, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3), xp, dtype)\n            with pytest.raises(TypeError):\n                getattr(xp, name)(a)\n\n    @testing.for_dtypes(['?', 'b', 'h', 'i', 'q', 'e', 'f', 'd'])\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_unary_negative(self, name, xp, dtype):\n        a = xp.array([-3, -2, -1, 1, 2, 3], dtype=dtype)\n        return getattr(xp, name)(a)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_unary_negative_complex(self, name, xp, dtype):\n        a = xp.array([-3-3j, -2-2j, -1-1j, 1+1j, 2+2j, 3+3j], dtype=dtype)\n        return getattr(xp, name)(a)\n\n    def test_rint(self):\n        self.check_unary('rint')\n        self.check_unary_complex('rint')\n\n    def test_rint_negative(self):\n        self.check_unary_negative('rint')\n        self.check_unary_negative_complex('rint')\n\n    def test_floor(self):\n        self.check_unary('floor')\n        self.check_unary_complex_unsupported('floor')\n\n    def test_ceil(self):\n        self.check_unary('ceil')\n        self.check_unary_complex_unsupported('ceil')\n\n    def test_trunc(self):\n        self.check_unary('trunc')\n        self.check_unary_complex_unsupported('trunc')\n\n    def test_fix(self):\n        self.check_unary('fix')\n        self.check_unary_complex_unsupported('fix')\n\n    def test_around(self):\n        self.check_unary('around')\n        self.check_unary_complex('around')\n\n    def test_round_(self):\n        self.check_unary('round_')\n        self.check_unary_complex('around')\n\n\n@testing.parameterize(*testing.product({\n    'decimals': [-2, -1, 0, 1, 2],\n}))\nclass TestRound(unittest.TestCase):\n\n    shape = (20,)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_round(self, xp, dtype):\n        if dtype == numpy.bool_:\n            # avoid cast problem\n            a = testing.shaped_random(self.shape, xp, scale=10, dtype=dtype)\n            return xp.around(a, 0)\n        if dtype == numpy.float16:\n            # avoid accuracy problem\n            a = testing.shaped_random(self.shape, xp, scale=10, dtype=dtype)\n            return xp.around(a, 0)\n        a = testing.shaped_random(self.shape, xp, scale=100, dtype=dtype)\n        return xp.around(a, self.decimals)\n\n    @testing.numpy_cupy_array_equal()\n    def test_round_out(self, xp):\n        a = testing.shaped_random(self.shape, xp, scale=100, dtype='d')\n        out = xp.empty_like(a)\n        xp.around(a, self.decimals, out)\n        return out\n\n\n@testing.parameterize(*testing.product({\n    'decimals': [-100, -99, -90, 0, 90, 99, 100],\n}))\nclass TestRoundExtreme(unittest.TestCase):\n\n    shape = (20,)\n\n    @testing.for_dtypes([numpy.float64, numpy.complex128])\n    @testing.numpy_cupy_allclose()\n    def test_round_large(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, scale=1e100, dtype=dtype)\n        return xp.around(a, self.decimals)\n\n    @testing.for_dtypes([numpy.float64, numpy.complex128])\n    @testing.numpy_cupy_allclose()\n    def test_round_small(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, scale=1e-100, dtype=dtype)\n        return xp.around(a, self.decimals)\n\n\n@testing.parameterize(*testing.product({\n    'value': [\n        (14, -1),\n        (15, -1),\n        (16, -1),\n        (14.0, -1),\n        (15.0, -1),\n        (16.0, -1),\n        (1.4, 0),\n        (1.5, 0),\n        (1.6, 0),\n    ]\n}))\nclass TestRoundBorder(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_around_positive1(self, xp):\n        a, decimals = self.value\n        return xp.around(a, decimals)\n\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_around_positive2(self, xp):\n        a, decimals = self.value\n        a = xp.asarray(a)\n        return xp.around(a, decimals)\n\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_around_negative1(self, xp):\n        a, decimals = self.value\n        return xp.around(-a, decimals)\n\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_around_negative2(self, xp):\n        a, decimals = self.value\n        a = xp.asarray(a)\n        return xp.around(-a, decimals)\n"""
tests/cupy_tests/math_tests/test_special.py,0,"b""import unittest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestSpecial(unittest.TestCase):\n\n    @testing.for_dtypes(['e', 'f', 'd'])\n    @testing.numpy_cupy_allclose(rtol=1e-3)\n    def test_i0(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return xp.i0(a)\n\n    @testing.for_dtypes(['e', 'f', 'd', 'F', 'D'])\n    @testing.numpy_cupy_allclose(atol=1e-3)\n    def test_sinc(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype, scale=1)\n        return xp.sinc(a)\n\n    @testing.for_dtypes(['e', 'f', 'd', 'F', 'D'])\n    def test_sinc_zero(self, dtype):\n        a = cupy.sinc(cupy.zeros(1, dtype=dtype))\n        testing.assert_array_equal(a, cupy.ones(1, dtype=dtype))\n"""
tests/cupy_tests/math_tests/test_sumprod.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestSumprod(unittest.TestCase):\n\n    def tearDown(self):\n        # Free huge memory for slow test\n        cupy.get_default_memory_pool().free_all_blocks()\n        cupy.get_default_pinned_memory_pool().free_all_blocks()\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_sum_all(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return a.sum()\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_sum_all_keepdims(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return a.sum(keepdims=True)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_external_sum_all(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.sum(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_sum_all2(self, xp, dtype):\n        a = testing.shaped_arange((20, 30, 40), xp, dtype)\n        return a.sum()\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_sum_all_transposed(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype).transpose(2, 0, 1)\n        return a.sum()\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_sum_all_transposed2(self, xp, dtype):\n        a = testing.shaped_arange((20, 30, 40), xp, dtype).transpose(2, 0, 1)\n        return a.sum()\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_sum_axis(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return a.sum(axis=1)\n\n    @testing.slow\n    @testing.numpy_cupy_allclose()\n    def test_sum_axis_huge(self, xp):\n        a = testing.shaped_random((2048, 1, 1024), xp, 'b')\n        a = xp.broadcast_to(a, (2048, 1024, 1024))\n        return a.sum(axis=2)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_external_sum_axis(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.sum(a, axis=1)\n\n    # float16 is omitted, since NumPy's sum on float16 arrays has more error\n    # than CuPy's.\n    @testing.for_all_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose()\n    def test_sum_axis2(self, xp, dtype):\n        a = testing.shaped_arange((20, 30, 40), xp, dtype)\n        return a.sum(axis=1)\n\n    def test_sum_axis2_float16(self):\n        # Note that the above test example overflows in float16. We use a\n        # smaller array instead.\n        a = testing.shaped_arange((2, 30, 4), dtype='e')\n        sa = a.sum(axis=1)\n        b = testing.shaped_arange((2, 30, 4), numpy, dtype='f')\n        sb = b.sum(axis=1)\n        testing.assert_allclose(sa, sb.astype('e'))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(contiguous_check=False)\n    def test_sum_axis_transposed(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype).transpose(2, 0, 1)\n        return a.sum(axis=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(contiguous_check=False)\n    def test_sum_axis_transposed2(self, xp, dtype):\n        a = testing.shaped_arange((20, 30, 40), xp, dtype).transpose(2, 0, 1)\n        return a.sum(axis=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_sum_axes(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4, 5), xp, dtype)\n        return a.sum(axis=(1, 3))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4)\n    def test_sum_axes2(self, xp, dtype):\n        a = testing.shaped_arange((20, 30, 40, 50), xp, dtype)\n        return a.sum(axis=(1, 3))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_sum_axes3(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4, 5), xp, dtype)\n        return a.sum(axis=(0, 2, 3))\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_sum_axes4(self, xp, dtype):\n        a = testing.shaped_arange((20, 30, 40, 50), xp, dtype)\n        return a.sum(axis=(0, 2, 3))\n\n    @testing.for_all_dtypes_combination(names=['src_dtype', 'dst_dtype'])\n    @testing.numpy_cupy_allclose()\n    def test_sum_dtype(self, xp, src_dtype, dst_dtype):\n        if not xp.can_cast(src_dtype, dst_dtype):\n            return xp.array([])  # skip\n        a = testing.shaped_arange((2, 3, 4), xp, src_dtype)\n        return a.sum(dtype=dst_dtype)\n\n    @testing.for_all_dtypes_combination(names=['src_dtype', 'dst_dtype'])\n    @testing.numpy_cupy_allclose()\n    def test_sum_keepdims_and_dtype(self, xp, src_dtype, dst_dtype):\n        if not xp.can_cast(src_dtype, dst_dtype):\n            return xp.array([])  # skip\n        a = testing.shaped_arange((2, 3, 4), xp, src_dtype)\n        return a.sum(axis=2, dtype=dst_dtype, keepdims=True)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_sum_keepdims_multiple_axes(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return a.sum(axis=(1, 2), keepdims=True)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_sum_out(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        b = xp.empty((2, 4), dtype=dtype)\n        a.sum(axis=1, out=b)\n        return b\n\n    def test_sum_out_wrong_shape(self):\n        a = testing.shaped_arange((2, 3, 4))\n        b = cupy.empty((2, 3))\n        with self.assertRaises(ValueError):\n            a.sum(axis=1, out=b)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_prod_all(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return a.prod()\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_external_prod_all(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return xp.prod(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_prod_axis(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return a.prod(axis=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_external_prod_axis(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.prod(a, axis=1)\n\n    @testing.for_all_dtypes_combination(names=['src_dtype', 'dst_dtype'])\n    @testing.numpy_cupy_allclose()\n    def test_prod_dtype(self, xp, src_dtype, dst_dtype):\n        if not xp.can_cast(src_dtype, dst_dtype):\n            return xp.array([])  # skip\n        a = testing.shaped_arange((2, 3), xp, src_dtype)\n        return a.prod(dtype=dst_dtype)\n\n\n@testing.parameterize(\n    *testing.product({\n        'shape': [(2, 3, 4), (20, 30, 40)],\n        'axis': [0, 1],\n        'transpose_axes': [True, False],\n        'keepdims': [True, False],\n        'func': ['nansum', 'nanprod']\n    })\n)\n@testing.gpu\nclass TestNansumNanprodLong(unittest.TestCase):\n\n    def _do_transposed_axis_test(self):\n        return not self.transpose_axes and self.axis != 1\n\n    def _numpy_nanprod_implemented(self):\n        return (self.func == 'nanprod' and\n                numpy.__version__ >= numpy.lib.NumpyVersion('1.10.0'))\n\n    def _test(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        if self.transpose_axes:\n            a = a.transpose(2, 0, 1)\n        if not issubclass(dtype, xp.integer):\n            a[:, 1] = xp.nan\n        func = getattr(xp, self.func)\n        return func(a, axis=self.axis, keepdims=self.keepdims)\n\n    @testing.for_all_dtypes(no_bool=True, no_float16=True)\n    @testing.numpy_cupy_allclose()\n    def test_nansum_all(self, xp, dtype):\n        if (not self._numpy_nanprod_implemented() or\n                not self._do_transposed_axis_test()):\n            return xp.array(())\n        return self._test(xp, dtype)\n\n    @testing.for_all_dtypes(no_bool=True, no_float16=True)\n    @testing.numpy_cupy_allclose(contiguous_check=False)\n    def test_nansum_axis_transposed(self, xp, dtype):\n        if (not self._numpy_nanprod_implemented() or\n                not self._do_transposed_axis_test()):\n            return xp.array(())\n        return self._test(xp, dtype)\n\n\n@testing.parameterize(\n    *testing.product({\n        'shape': [(2, 3, 4), (20, 30, 40)],\n    })\n)\n@testing.gpu\nclass TestNansumNanprodExtra(unittest.TestCase):\n\n    def test_nansum_axis_float16(self):\n        # Note that the above test example overflows in float16. We use a\n        # smaller array instead, return True if array is too large.\n        if (numpy.prod(self.shape) > 24):\n            return True\n        a = testing.shaped_arange(self.shape, dtype='e')\n        a[:, 1] = cupy.nan\n        sa = cupy.nansum(a, axis=1)\n        b = testing.shaped_arange(self.shape, numpy, dtype='f')\n        b[:, 1] = numpy.nan\n        sb = numpy.nansum(b, axis=1)\n        testing.assert_allclose(sa, sb.astype('e'))\n\n    @testing.for_all_dtypes(no_bool=True, no_float16=True)\n    @testing.numpy_cupy_allclose()\n    def test_nansum_out(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        if not issubclass(dtype, xp.integer):\n            a[:, 1] = xp.nan\n        b = xp.empty((self.shape[0], self.shape[2]), dtype=dtype)\n        xp.nansum(a, axis=1, out=b)\n        return b\n\n    def test_nansum_out_wrong_shape(self):\n        a = testing.shaped_arange(self.shape)\n        a[:, 1] = cupy.nan\n        b = cupy.empty((2, 3))\n        with self.assertRaises(ValueError):\n            cupy.nansum(a, axis=1, out=b)\n\n\n@testing.parameterize(\n    *testing.product({\n        'shape': [(2, 3, 4, 5), (20, 30, 40, 50)],\n        'axis': [(1, 3), (0, 2, 3)],\n    })\n)\n@testing.gpu\nclass TestNansumNanprodAxes(unittest.TestCase):\n    @testing.for_all_dtypes(no_bool=True, no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_nansum_axes(self, xp, dtype):\n        a = testing.shaped_arange(self.shape, xp, dtype)\n        if not issubclass(dtype, xp.integer):\n            a[:, 1] = xp.nan\n        return xp.nansum(a, axis=self.axis)\n\n\n@testing.gpu\nclass TestNansumNanprodHuge(unittest.TestCase):\n    def _test(self, xp, nan_slice):\n        a = testing.shaped_random((2048, 1, 1024), xp, 'f')\n        a[nan_slice] = xp.nan\n        a = xp.broadcast_to(a, (2048, 1024, 1024))\n        return xp.nansum(a, axis=2)\n\n    @testing.slow\n    @testing.numpy_cupy_allclose(atol=1e-1)\n    def test_nansum_axis_huge(self, xp):\n        return self._test(\n            xp, (slice(None, None), slice(None, None), slice(1, 2)))\n\n    @testing.slow\n    @testing.numpy_cupy_allclose(atol=1e-2)\n    def test_nansum_axis_huge_halfnan(self, xp):\n        return self._test(\n            xp, (slice(None, None), slice(None, None), slice(0, 512)))\n\n\naxes = [0, 1, 2]\n\n\n@testing.parameterize(*testing.product({'axis': axes}))\n@testing.gpu\nclass TestCumsum(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_cumsum(self, xp, dtype):\n        a = testing.shaped_arange((5,), xp, dtype)\n        return xp.cumsum(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_cumsum_out(self, xp, dtype):\n        a = testing.shaped_arange((5,), xp, dtype)\n        out = xp.zeros((5,), dtype=dtype)\n        xp.cumsum(a, out=out)\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_cumsum_out_noncontiguous(self, xp, dtype):\n        a = testing.shaped_arange((5,), xp, dtype)\n        out = xp.zeros((10,), dtype=dtype)[::2]  # Non contiguous view\n        xp.cumsum(a, out=out)\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_cumsum_2dim(self, xp, dtype):\n        a = testing.shaped_arange((4, 5), xp, dtype)\n        return xp.cumsum(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(contiguous_check=False)\n    def test_cumsum_axis(self, xp, dtype):\n        n = len(axes)\n        a = testing.shaped_arange(tuple(range(4, 4 + n)), xp, dtype)\n        return xp.cumsum(a, axis=self.axis)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_cumsum_axis_out(self, xp, dtype):\n        n = len(axes)\n        shape = tuple(range(4, 4 + n))\n        a = testing.shaped_arange(shape, xp, dtype)\n        out = xp.zeros(shape, dtype=dtype)\n        xp.cumsum(a, axis=self.axis, out=out)\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_cumsum_axis_out_noncontiguous(self, xp, dtype):\n        n = len(axes)\n        shape = tuple(range(4, 4 + n))\n        a = testing.shaped_arange(shape, xp, dtype)\n        out = xp.zeros((8,)+shape[1:], dtype=dtype)[::2]  # Non contiguous view\n        xp.cumsum(a, axis=self.axis, out=out)\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(contiguous_check=False)\n    def test_ndarray_cumsum_axis(self, xp, dtype):\n        n = len(axes)\n        a = testing.shaped_arange(tuple(range(4, 4 + n)), xp, dtype)\n        return a.cumsum(axis=self.axis)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_cumsum_axis_empty(self, xp, dtype):\n        n = len(axes)\n        a = testing.shaped_arange(tuple(range(0, n)), xp, dtype)\n        return xp.cumsum(a, axis=self.axis)\n\n    @testing.for_all_dtypes()\n    def test_invalid_axis_lower1(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((4, 5), xp, dtype)\n            with pytest.raises(numpy.AxisError):\n                xp.cumsum(a, axis=-a.ndim - 1)\n\n    @testing.for_all_dtypes()\n    def test_invalid_axis_lower2(self, dtype):\n        a = testing.shaped_arange((4, 5), cupy, dtype)\n        with self.assertRaises(numpy.AxisError):\n            return cupy.cumsum(a, axis=-a.ndim - 1)\n\n    @testing.for_all_dtypes()\n    def test_invalid_axis_upper1(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((4, 5), xp, dtype)\n            with pytest.raises(numpy.AxisError):\n                xp.cumsum(a, axis=a.ndim + 1)\n\n    @testing.for_all_dtypes()\n    def test_invalid_axis_upper2(self, dtype):\n        a = testing.shaped_arange((4, 5), cupy, dtype)\n        with self.assertRaises(numpy.AxisError):\n            return cupy.cumsum(a, axis=a.ndim + 1)\n\n    def test_cumsum_arraylike(self):\n        with self.assertRaises(TypeError):\n            return cupy.cumsum((1, 2, 3))\n\n    @testing.for_float_dtypes()\n    def test_cumsum_numpy_array(self, dtype):\n        a_numpy = numpy.arange(8, dtype=dtype)\n        with self.assertRaises(TypeError):\n            return cupy.cumsum(a_numpy)\n\n\n@testing.gpu\nclass TestCumprod(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_cumprod_1dim(self, xp, dtype):\n        a = testing.shaped_arange((5,), xp, dtype)\n        return xp.cumprod(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_cumprod_out(self, xp, dtype):\n        a = testing.shaped_arange((5,), xp, dtype)\n        out = xp.zeros((5,), dtype=dtype)\n        xp.cumprod(a, out=out)\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_cumprod_out_noncontiguous(self, xp, dtype):\n        a = testing.shaped_arange((5,), xp, dtype)\n        out = xp.zeros((10,), dtype=dtype)[::2]  # Non contiguous view\n        xp.cumprod(a, out=out)\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_cumprod_2dim_without_axis(self, xp, dtype):\n        a = testing.shaped_arange((4, 5), xp, dtype)\n        return xp.cumprod(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_cumprod_2dim_with_axis(self, xp, dtype):\n        a = testing.shaped_arange((4, 5), xp, dtype)\n        return xp.cumprod(a, axis=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_ndarray_cumprod_2dim_with_axis(self, xp, dtype):\n        a = testing.shaped_arange((4, 5), xp, dtype)\n        return a.cumprod(axis=1)\n\n    @testing.slow\n    def test_cumprod_huge_array(self):\n        size = 2 ** 32\n        # Free huge memory for slow test\n        cupy.get_default_memory_pool().free_all_blocks()\n        a = cupy.ones(size, 'b')\n        result = cupy.cumprod(a, dtype='b')\n        del a\n        self.assertTrue((result == 1).all())\n        # Free huge memory for slow test\n        del result\n        cupy.get_default_memory_pool().free_all_blocks()\n\n    @testing.for_all_dtypes()\n    def test_invalid_axis_lower1(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((4, 5), xp, dtype)\n            with pytest.raises(numpy.AxisError):\n                xp.cumprod(a, axis=-a.ndim - 1)\n\n    @testing.for_all_dtypes()\n    def test_invalid_axis_lower2(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((4, 5), xp, dtype)\n            with pytest.raises(numpy.AxisError):\n                xp.cumprod(a, axis=-a.ndim - 1)\n\n    @testing.for_all_dtypes()\n    def test_invalid_axis_upper1(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((4, 5), xp, dtype)\n            with pytest.raises(numpy.AxisError):\n                return xp.cumprod(a, axis=a.ndim)\n\n    @testing.for_all_dtypes()\n    def test_invalid_axis_upper2(self, dtype):\n        a = testing.shaped_arange((4, 5), cupy, dtype)\n        with self.assertRaises(numpy.AxisError):\n            return cupy.cumprod(a, axis=a.ndim)\n\n    def test_cumprod_arraylike(self):\n        with self.assertRaises(TypeError):\n            return cupy.cumprod((1, 2, 3))\n\n    @testing.for_float_dtypes()\n    def test_cumprod_numpy_array(self, dtype):\n        a_numpy = numpy.arange(1, 6, dtype=dtype)\n        with self.assertRaises(TypeError):\n            return cupy.cumprod(a_numpy)\n\n\n@testing.gpu\nclass TestDiff(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_diff_1dim(self, xp, dtype):\n        a = testing.shaped_arange((5,), xp, dtype)\n        return xp.diff(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_diff_1dim_with_n(self, xp, dtype):\n        a = testing.shaped_arange((5,), xp, dtype)\n        return xp.diff(a, n=3)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_diff_2dim_without_axis(self, xp, dtype):\n        a = testing.shaped_arange((4, 5), xp, dtype)\n        return xp.diff(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_diff_2dim_with_axis(self, xp, dtype):\n        a = testing.shaped_arange((4, 5), xp, dtype)\n        return xp.diff(a, axis=-2)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_diff_2dim_with_n_and_axis(self, xp, dtype):\n        a = testing.shaped_arange((4, 5), xp, dtype)\n        return xp.diff(a, 2, 1)\n\n    @testing.with_requires('numpy>=1.16')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_diff_2dim_with_prepend(self, xp, dtype):\n        a = testing.shaped_arange((4, 5), xp, dtype)\n        b = testing.shaped_arange((4, 1), xp, dtype)\n        return xp.diff(a, axis=-1, prepend=b)\n\n    @testing.with_requires('numpy>=1.16')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_diff_2dim_with_append(self, xp, dtype):\n        a = testing.shaped_arange((4, 5), xp, dtype)\n        b = testing.shaped_arange((1, 5), xp, dtype)\n        return xp.diff(a, axis=0, append=b, n=2)\n\n    @testing.with_requires('numpy>=1.16')\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_diff_2dim_with_scalar_append(self, xp, dtype):\n        a = testing.shaped_arange((4, 5), xp, dtype)\n        return xp.diff(a, prepend=1, append=0)\n\n    @testing.with_requires('numpy>=1.16')\n    def test_diff_invalid_axis(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_arange((2, 3, 4), xp)\n            with pytest.raises(numpy.AxisError):\n                xp.diff(a, axis=3)\n            with pytest.raises(numpy.AxisError):\n                xp.diff(a, axis=-4)\n"""
tests/cupy_tests/math_tests/test_trigonometric.py,0,"b""import unittest\n\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestTrigonometric(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_unary(self, name, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return getattr(xp, name)(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_binary(self, name, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        b = testing.shaped_reverse_arange((2, 3), xp, dtype)\n        return getattr(xp, name)(a, b)\n\n    @testing.for_dtypes(['e', 'f', 'd'])\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def check_unary_unit(self, name, xp, dtype):\n        a = xp.array([0.2, 0.4, 0.6, 0.8], dtype=dtype)\n        return getattr(xp, name)(a)\n\n    def test_sin(self):\n        self.check_unary('sin')\n\n    def test_cos(self):\n        self.check_unary('cos')\n\n    def test_tan(self):\n        self.check_unary('tan')\n\n    def test_arcsin(self):\n        self.check_unary_unit('arcsin')\n\n    def test_arccos(self):\n        self.check_unary_unit('arccos')\n\n    def test_arctan(self):\n        self.check_unary('arctan')\n\n    def test_arctan2(self):\n        self.check_binary('arctan2')\n\n    def test_hypot(self):\n        self.check_binary('hypot')\n\n    def test_deg2rad(self):\n        self.check_unary('deg2rad')\n\n    def test_rad2deg(self):\n        self.check_unary('rad2deg')\n\n\n@testing.gpu\nclass TestUnwrap(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_unwrap_1dim(self, xp, dtype):\n        a = testing.shaped_random((5,), xp, dtype)\n        return xp.unwrap(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_unwrap_1dim_with_discont(self, xp, dtype):\n        a = testing.shaped_random((5,), xp, dtype)\n        return xp.unwrap(a, discont=1.0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_unwrap_2dim_without_axis(self, xp, dtype):\n        a = testing.shaped_random((4, 5), xp, dtype)\n        return xp.unwrap(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_unwrap_2dim_with_axis(self, xp, dtype):\n        a = testing.shaped_random((4, 5), xp, dtype)\n        return xp.unwrap(a, axis=1)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_unwrap_2dim_with_discont(self, xp, dtype):\n        a = testing.shaped_random((4, 5), xp, dtype)\n        return xp.unwrap(a, discont=5.0, axis=1)\n"""
tests/cupy_tests/math_tests/test_window.py,0,"b""import unittest\n\nfrom cupy import testing\n\n\n@testing.parameterize(\n    *testing.product({\n        'm': [0, 1, -1, 1024],\n        'name': ['bartlett', 'blackman', 'hamming', 'hanning'],\n    })\n)\nclass TestWindow(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_window(self, xp):\n        return getattr(xp, self.name)(self.m)\n\n\n@testing.parameterize(\n    *testing.product({\n        'm': [10, 30, 1024],\n        'beta': [-3.4, 0, 5, 6, 8.6],\n        'name': ['kaiser'],\n    })\n)\nclass TestKaiser(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_kaiser_parametric(self, xp):\n        return getattr(xp, self.name)(self.m, self.beta)\n\n\n@testing.parameterize(*testing.product({'m': [-1, 0, 1]}))\nclass TestKaiserBoundary(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_kaiser(self, xp):\n        return xp.kaiser(self.m, 1.5)\n"""
tests/cupy_tests/misc_tests/__init__.py,0,b''
tests/cupy_tests/misc_tests/test_memory_ranges.py,1,"b""import numpy\nimport unittest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestMayShareMemory(unittest.TestCase):\n\n    @testing.numpy_cupy_equal()\n    def test_different_arrays(self, xp):\n        a = xp.array([1, 2, 3])\n        b = xp.array([1, 2, 3])\n        assert xp.may_share_memory(a, b) is False\n\n    @testing.numpy_cupy_equal()\n    def test_same_array(self, xp):\n        a = xp.array([1, 2, 3])\n        assert xp.may_share_memory(a, a) is True\n\n    @testing.numpy_cupy_equal()\n    def test_zero_size(self, xp):\n        a = xp.array([])\n        assert xp.may_share_memory(a, a) is False\n\n    @testing.numpy_cupy_equal()\n    def test_shares_memory(self, xp):\n        x = xp.arange(12)\n        a = x[0:7]\n        b = x[6:12]\n        assert xp.may_share_memory(a, b) is True\n\n    @testing.numpy_cupy_equal()\n    def test_cover(self, xp):\n        x = xp.arange(12)\n        a = x[1:10]\n        b = x[4:6]\n        assert xp.may_share_memory(a, b) is True\n\n    @testing.numpy_cupy_equal()\n    def test_away(self, xp):\n        x = xp.arange(12)\n        a = x[1:6]\n        b = x[8:11]\n        assert xp.may_share_memory(a, b) is False\n\n    @testing.numpy_cupy_equal()\n    def test_touch_edge_true(self, xp):\n        x = xp.arange(12)\n        a = x[1:10]\n        b = x[7:10]\n        assert xp.may_share_memory(a, b) is True\n\n    @testing.numpy_cupy_equal()\n    def test_touch_edge_false(self, xp):\n        x = xp.arange(12)\n        a = x[1:7]\n        b = x[7:10]\n        assert xp.may_share_memory(a, b) is False\n\n    def _get_slices(self, size):\n        slices = []\n        for start in range(0, size + 1):\n            for end in range(start, size + 1):\n                for step in range(-2, 2):\n                    if step != 0:\n                        slices.append(slice(start, end, step))\n        return slices\n\n    def test_combination(self):\n        size = 4\n        slices = self._get_slices(size)\n        memory_np = numpy.empty(size * size)\n        memory_cp = cupy.empty(size * size)\n\n        arrays = []\n\n        array_1d_np = memory_np[5:5+size]\n        array_1d_cp = memory_cp[5:5+size]\n        for s in slices:\n            arrays.append((array_1d_np[s], array_1d_cp[s], s))\n\n        array_2d_np = memory_np.reshape(size, size)\n        array_2d_cp = memory_cp.reshape(size, size)\n        for s1 in slices:\n            for s2 in slices:\n                arrays.append((\n                    array_2d_np[s1, s2], array_2d_cp[s1, s2], (s1, s2)))\n\n        for array1_np, array1_cp, sl1 in arrays:\n            for array2_np, array2_cp, sl2 in arrays:\n                ret_np = numpy.may_share_memory(array1_np, array2_np)\n                ret_cp = cupy.may_share_memory(array1_cp, array2_cp)\n                assert ret_np == ret_cp, \\\n                    'Failed in case of {} and {}'.format(sl1, sl2)\n"""
tests/cupy_tests/misc_tests/test_who.py,0,"b""import cupy\n\n\nclass TestWho:\n    def test_who_empty(self, capsys):\n        cupy.who()\n        out, err = capsys.readouterr()\n        lines = out.split('\\n')\n        assert len(lines) == 3\n        assert lines[1] == 'Upper bound on total bytes  =       0'\n\n    def test_who_local_var(self, capsys):\n        # Variables declared inside an object function are not visible\n        # this is true also for numpy\n        x = cupy.ones(10)  # NOQA\n        cupy.who()\n        out, err = capsys.readouterr()\n        lines = out.split('\\n')\n        assert len(lines) == 3\n        assert lines[1] == 'Upper bound on total bytes  =       0'\n\n    def test_who_global(self, capsys):\n        global x\n        x = cupy.ones(10)  # NOQA\n        cupy.who()\n        out, err = capsys.readouterr()\n        lines = out.split('\\n')\n        assert lines[-4].split() == ['x', '10', '80', 'float64']\n        assert lines[-2] == 'Upper bound on total bytes  =       80'\n\n    def test_who_global_multi(self, capsys):\n        global x\n        global y\n        x = cupy.ones(10)  # NOQA\n        y = cupy.ones(20, dtype=cupy.int32)  # NOQA\n        cupy.who()\n        out, err = capsys.readouterr()\n        lines = out.split('\\n')\n        # depending on the env, the order in which vars are print\n        # might be different\n        var_1 = lines[-5].split()\n        var_2 = lines[-4].split()\n        if var_1[0] == 'x':\n            assert var_1 == ['x', '10', '80', 'float64']\n            assert var_2 == ['y', '20', '80', 'int32']\n        else:\n            assert var_2 == ['x', '10', '80', 'float64']\n            assert var_1 == ['y', '20', '80', 'int32']\n        assert lines[-2] == 'Upper bound on total bytes  =       160'\n\n    def test_who_dict_arrays(self, capsys):\n        var_dict = {'x': cupy.ones(10)}\n        cupy.who(var_dict)\n        out, err = capsys.readouterr()\n        lines = out.split('\\n')\n        assert lines[-4].split() == ['x', '10', '80', 'float64']\n        assert lines[-2] == 'Upper bound on total bytes  =       80'\n\n    def test_who_dict_empty(self, capsys):\n        global x\n        x = cupy.ones(10)  # NOQA\n        cupy.who({})\n        out, err = capsys.readouterr()\n        lines = out.split('\\n')\n        assert lines[-2] == 'Upper bound on total bytes  =       0'\n"""
tests/cupy_tests/padding_tests/__init__.py,0,b''
tests/cupy_tests/padding_tests/test_pad.py,0,"b'import unittest\nimport warnings\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.parameterize(\n    *testing.product({\n        \'array\': [numpy.arange(6).reshape([2, 3])],\n        \'pad_width\': [1, [1, 2], [[1, 2], [3, 4]]],\n        # mode \'mean\' is non-exact, so it is tested in a separate class\n        \'mode\': [\'constant\', \'edge\', \'linear_ramp\', \'maximum\',\n                 \'minimum\', \'reflect\', \'symmetric\', \'wrap\'],\n    })\n)\n@testing.gpu\nclass TestPadDefault(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_pad_default(self, xp, dtype):\n        array = xp.array(self.array, dtype=dtype)\n\n        if (xp.dtype(dtype).kind in [\'i\', \'u\'] and\n                self.mode == \'linear_ramp\'):\n            # TODO: can remove this skip once cupy/cupy/#2330 is merged\n            return array\n\n        # Older version of NumPy(<1.12) can emit ComplexWarning\n        def f():\n            return xp.pad(array, self.pad_width, mode=self.mode)\n\n        if xp is numpy:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\'ignore\', numpy.ComplexWarning)\n                return f()\n        else:\n            return f()\n\n\n@testing.parameterize(\n    *testing.product({\n        \'array\': [numpy.arange(6).reshape([2, 3])],\n        \'pad_width\': [1, [1, 2], [[1, 2], [3, 4]]],\n    })\n)\n@testing.gpu\nclass TestPadDefaultMean(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_almost_equal(decimal=5)\n    def test_pad_default(self, xp, dtype):\n        array = xp.array(self.array, dtype=dtype)\n\n        if xp.dtype(dtype).kind in [\'i\', \'u\']:\n            # TODO: can remove this skip once cupy/cupy/#2330 is merged\n            return array\n\n        # Older version of NumPy(<1.12) can emit ComplexWarning\n        def f():\n            return xp.pad(array, self.pad_width, mode=\'mean\')\n\n        if xp is numpy:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\'ignore\', numpy.ComplexWarning)\n                return f()\n        else:\n            return f()\n\n\n@testing.parameterize(\n    # mode=\'constant\'\n    {\'array\': numpy.arange(6).reshape([2, 3]), \'pad_width\': 1,\n     \'mode\': \'constant\', \'constant_values\': 3},\n    {\'array\': numpy.arange(6).reshape([2, 3]),\n     \'pad_width\': [1, 2], \'mode\': \'constant\',\n     \'constant_values\': [3, 4]},\n    {\'array\': numpy.arange(6).reshape([2, 3]),\n     \'pad_width\': [[1, 2], [3, 4]], \'mode\': \'constant\',\n     \'constant_values\': [[3, 4], [5, 6]]},\n    # mode=\'reflect\'\n    {\'array\': numpy.arange(6).reshape([2, 3]), \'pad_width\': 1,\n     \'mode\': \'reflect\', \'reflect_type\': \'odd\'},\n    {\'array\': numpy.arange(6).reshape([2, 3]),\n     \'pad_width\': [1, 2], \'mode\': \'reflect\', \'reflect_type\': \'odd\'},\n    {\'array\': numpy.arange(6).reshape([2, 3]),\n     \'pad_width\': [[1, 2], [3, 4]], \'mode\': \'reflect\',\n     \'reflect_type\': \'odd\'},\n    # mode=\'symmetric\'\n    {\'array\': numpy.arange(6).reshape([2, 3]), \'pad_width\': 1,\n     \'mode\': \'symmetric\', \'reflect_type\': \'odd\'},\n    {\'array\': numpy.arange(6).reshape([2, 3]),\n     \'pad_width\': [1, 2], \'mode\': \'symmetric\', \'reflect_type\': \'odd\'},\n    {\'array\': numpy.arange(6).reshape([2, 3]),\n     \'pad_width\': [[1, 2], [3, 4]], \'mode\': \'symmetric\',\n     \'reflect_type\': \'odd\'},\n    # mode=\'minimum\'\n    {\'array\': numpy.arange(60).reshape([5, 12]), \'pad_width\': 1,\n     \'mode\': \'minimum\', \'stat_length\': 2},\n    {\'array\': numpy.arange(60).reshape([5, 12]),\n     \'pad_width\': [1, 2], \'mode\': \'minimum\', \'stat_length\': (2, 4)},\n    {\'array\': numpy.arange(60).reshape([5, 12]),\n     \'pad_width\': [[1, 2], [3, 4]], \'mode\': \'minimum\',\n     \'stat_length\': ((2, 4), (3, 5))},\n    {\'array\': numpy.arange(60).reshape([5, 12]),\n     \'pad_width\': [[1, 2], [3, 4]], \'mode\': \'minimum\',\n     \'stat_length\': None},\n    # mode=\'maximum\'\n    {\'array\': numpy.arange(60).reshape([5, 12]), \'pad_width\': 1,\n     \'mode\': \'maximum\', \'stat_length\': 2},\n    {\'array\': numpy.arange(60).reshape([5, 12]),\n     \'pad_width\': [1, 2], \'mode\': \'maximum\', \'stat_length\': (2, 4)},\n    {\'array\': numpy.arange(60).reshape([5, 12]),\n     \'pad_width\': [[1, 2], [3, 4]], \'mode\': \'maximum\',\n     \'stat_length\': ((2, 4), (3, 5))},\n    {\'array\': numpy.arange(60).reshape([5, 12]),\n     \'pad_width\': [[1, 2], [3, 4]], \'mode\': \'maximum\',\n     \'stat_length\': None},\n)\n@testing.gpu\n# Old numpy does not work with multi-dimensional constant_values\nclass TestPad(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_pad(self, xp, dtype):\n        array = xp.array(self.array, dtype=dtype)\n\n        # Older version of NumPy(<1.12) can emit ComplexWarning\n        def f():\n            if self.mode == \'constant\':\n                return xp.pad(array, self.pad_width, mode=self.mode,\n                              constant_values=self.constant_values)\n            elif self.mode in [\'minimum\', \'maximum\']:\n                return xp.pad(array, self.pad_width, mode=self.mode,\n                              stat_length=self.stat_length)\n            elif self.mode in [\'reflect\', \'symmetric\']:\n                return xp.pad(array, self.pad_width, mode=self.mode,\n                              reflect_type=self.reflect_type)\n\n        if xp is numpy:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\'ignore\', numpy.ComplexWarning)\n                return f()\n        else:\n            return f()\n\n\n@testing.parameterize(\n    # mode=\'mean\'\n    {\'array\': numpy.arange(60).reshape([5, 12]), \'pad_width\': 1,\n     \'mode\': \'mean\', \'stat_length\': 2},\n    {\'array\': numpy.arange(60).reshape([5, 12]),\n     \'pad_width\': [1, 2], \'mode\': \'mean\', \'stat_length\': (2, 4)},\n    {\'array\': numpy.arange(60).reshape([5, 12]),\n     \'pad_width\': [[1, 2], [3, 4]], \'mode\': \'mean\',\n     \'stat_length\': ((2, 4), (3, 5))},\n    {\'array\': numpy.arange(60).reshape([5, 12]),\n     \'pad_width\': [[1, 2], [3, 4]], \'mode\': \'mean\',\n     \'stat_length\': None},\n)\n@testing.gpu\n# Old numpy does not work with multi-dimensional constant_values\nclass TestPadMean(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_almost_equal(decimal=5)\n    def test_pad(self, xp, dtype):\n        array = xp.array(self.array, dtype=dtype)\n\n        if xp.dtype(dtype).kind in [\'i\', \'u\']:\n            # TODO: can remove this skip once cupy/cupy/#2330 is merged\n            return array\n\n        # Older version of NumPy(<1.12) can emit ComplexWarning\n        def f():\n            return xp.pad(array, self.pad_width, mode=self.mode,\n                          stat_length=self.stat_length)\n\n        if xp is numpy:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\'ignore\', numpy.ComplexWarning)\n                return f()\n        else:\n            return f()\n\n\n@testing.gpu\nclass TestPadNumpybug(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_equal()\n    def test_pad_highdim_default(self, xp, dtype):\n        array = xp.arange(6, dtype=dtype).reshape([2, 3])\n        pad_width = [[1, 2], [3, 4]]\n        constant_values = [[1, 2], [3, 4]]\n        a = xp.pad(array, pad_width, mode=\'constant\',\n                   constant_values=constant_values)\n        return a\n\n\n@testing.gpu\nclass TestPadEmpty(unittest.TestCase):\n\n    @testing.with_requires(\'numpy>=1.17\')\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_pad_empty(self, xp, dtype):\n        array = xp.arange(6, dtype=dtype).reshape([2, 3])\n        pad_width = 2\n        a = xp.pad(array, pad_width=pad_width, mode=\'empty\')\n        # omit uninitialized ""empty"" boundary from the comparison\n        return a[pad_width:-pad_width, pad_width:-pad_width]\n\n\n@testing.gpu\nclass TestPadCustomFunction(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_pad_via_func(self, xp, dtype):\n        def _padwithtens(vector, pad_width, iaxis, kwargs):\n            vector[:pad_width[0]] = 10\n            vector[-pad_width[1]:] = 10\n        a = xp.arange(6, dtype=dtype).reshape(2, 3)\n        a = xp.pad(a, 2, _padwithtens)\n        return a\n\n\n@testing.parameterize(\n    # mode=\'constant\'\n    {\'array\': [], \'pad_width\': 1, \'mode\': \'constant\', \'constant_values\': 3},\n    {\'array\': 1, \'pad_width\': 1, \'mode\': \'constant\', \'constant_values\': 3},\n    {\'array\': [0, 1, 2, 3], \'pad_width\': 1, \'mode\': \'constant\',\n     \'constant_values\': 3},\n    {\'array\': [0, 1, 2, 3], \'pad_width\': [1, 2], \'mode\': \'constant\',\n     \'constant_values\': 3},\n    # mode=\'edge\'\n    {\'array\': 1, \'pad_width\': 1, \'mode\': \'edge\'},\n    {\'array\': [0, 1, 2, 3], \'pad_width\': 1, \'mode\': \'edge\'},\n    {\'array\': [0, 1, 2, 3], \'pad_width\': [1, 2], \'mode\': \'edge\'},\n    # mode=\'reflect\'\n    {\'array\': 1, \'pad_width\': 1, \'mode\': \'reflect\'},\n    {\'array\': [0, 1, 2, 3], \'pad_width\': 1, \'mode\': \'reflect\'},\n    {\'array\': [0, 1, 2, 3], \'pad_width\': [1, 2], \'mode\': \'reflect\'},\n)\n@testing.gpu\nclass TestPadSpecial(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal()\n    def test_pad_special(self, xp):\n        array = xp.array(self.array)\n\n        if self.mode == \'constant\':\n            a = xp.pad(array, self.pad_width, mode=self.mode,\n                       constant_values=self.constant_values)\n        elif self.mode in [\'edge\', \'reflect\']:\n            a = xp.pad(array, self.pad_width, mode=self.mode)\n        return a\n\n\n@testing.parameterize(\n    {\'array\': [0, 1, 2, 3], \'pad_width\': [-1, 1], \'mode\': \'constant\',\n     \'kwargs\': {\'constant_values\': 3}},\n    {\'array\': [0, 1, 2, 3], \'pad_width\': [[3, 4], [5, 6]], \'mode\': \'constant\',\n     \'kwargs\': {\'constant_values\': 3}},\n    {\'array\': [0, 1, 2, 3], \'pad_width\': [1], \'mode\': \'constant\',\n     \'kwargs\': {\'notallowedkeyword\': 3}},\n    # edge\n    {\'array\': [], \'pad_width\': 1, \'mode\': \'edge\',\n     \'kwargs\': {}},\n    {\'array\': [0, 1, 2, 3], \'pad_width\': [-1, 1], \'mode\': \'edge\',\n     \'kwargs\': {}},\n    {\'array\': [0, 1, 2, 3], \'pad_width\': [[3, 4], [5, 6]], \'mode\': \'edge\',\n     \'kwargs\': {}},\n    {\'array\': [0, 1, 2, 3], \'pad_width\': [1], \'mode\': \'edge\',\n     \'kwargs\': {\'notallowedkeyword\': 3}},\n    # mode=\'reflect\'\n    {\'array\': [], \'pad_width\': 1, \'mode\': \'reflect\',\n     \'kwargs\': {}},\n    {\'array\': [0, 1, 2, 3], \'pad_width\': [-1, 1], \'mode\': \'reflect\',\n     \'kwargs\': {}},\n    {\'array\': [0, 1, 2, 3], \'pad_width\': [[3, 4], [5, 6]], \'mode\': \'reflect\',\n     \'kwargs\': {}},\n    {\'array\': [0, 1, 2, 3], \'pad_width\': [1], \'mode\': \'reflect\',\n     \'kwargs\': {\'notallowedkeyword\': 3}},\n)\n@testing.gpu\n@testing.with_requires(\'numpy>=1.17\')\nclass TestPadValueError(unittest.TestCase):\n\n    def test_pad_failure(self):\n        for xp in (numpy, cupy):\n            array = xp.array(self.array)\n            with pytest.raises(ValueError):\n                xp.pad(array, self.pad_width, self.mode, **self.kwargs)\n\n\n@testing.parameterize(\n    {\'array\': [0, 1, 2, 3], \'pad_width\': [], \'mode\': \'constant\',\n     \'kwargs\': {\'constant_values\': 3}},\n    # edge\n    {\'array\': [0, 1, 2, 3], \'pad_width\': [], \'mode\': \'edge\',\n     \'kwargs\': {}},\n    # mode=\'reflect\'\n    {\'array\': [0, 1, 2, 3], \'pad_width\': [], \'mode\': \'reflect\',\n     \'kwargs\': {}},\n)\n@testing.gpu\nclass TestPadTypeError(unittest.TestCase):\n\n    def test_pad_failure(self):\n        for xp in (numpy, cupy):\n            array = xp.array(self.array)\n            with pytest.raises(TypeError):\n                xp.pad(array, self.pad_width, self.mode, **self.kwargs)\n'"
tests/cupy_tests/prof_tests/__init__.py,0,b''
tests/cupy_tests/prof_tests/test_range.py,0,"b""import unittest\n\nimport mock\n\nfrom cupy import cuda\nfrom cupy import prof\n\n\n@unittest.skipUnless(cuda.nvtx_enabled, 'nvtx is required for time_range')\nclass TestTimeRange(unittest.TestCase):\n\n    def test_time_range(self):\n        push_patch = mock.patch('cupy.cuda.nvtx.RangePush')\n        pop_patch = mock.patch('cupy.cuda.nvtx.RangePop')\n        with push_patch as push, pop_patch as pop:\n            with prof.time_range('test:time_range', color_id=-1):\n                pass\n            push.assert_called_once_with('test:time_range', -1)\n            pop.assert_called_once_with()\n\n    def test_time_range_with_ARGB(self):\n        push_patch = mock.patch('cupy.cuda.nvtx.RangePushC')\n        pop_patch = mock.patch('cupy.cuda.nvtx.RangePop')\n        with push_patch as push, pop_patch as pop:\n            with prof.time_range('test:time_range_with_ARGB',\n                                 argb_color=0xFF00FF00):\n                pass\n            push.assert_called_once_with(\n                'test:time_range_with_ARGB', 0xFF00FF00)\n            pop.assert_called_once_with()\n\n    def test_time_range_err(self):\n        push_patch = mock.patch('cupy.cuda.nvtx.RangePush')\n        pop_patch = mock.patch('cupy.cuda.nvtx.RangePop')\n        with push_patch as push, pop_patch as pop:\n            try:\n                with prof.time_range('test:time_range_error', -1):\n                    raise Exception()\n            except Exception:\n                pass\n            push.assert_called_once_with('test:time_range_error', -1)\n            pop.assert_called_once_with()\n\n    def test_TimeRangeDecorator(self):\n        push_patch = mock.patch('cupy.cuda.nvtx.RangePush')\n        pop_patch = mock.patch('cupy.cuda.nvtx.RangePop')\n        with push_patch as push, pop_patch as pop:\n            @prof.TimeRangeDecorator()\n            def f():\n                pass\n            f()\n            # Default value of color id is -1\n            push.assert_called_once_with('f', -1)\n            pop.assert_called_once_with()\n\n    def test_TimeRangeDecorator_with_ARGB(self):\n        push_patch = mock.patch('cupy.cuda.nvtx.RangePushC')\n        pop_patch = mock.patch('cupy.cuda.nvtx.RangePop')\n        with push_patch as push, pop_patch as pop:\n            @prof.TimeRangeDecorator(argb_color=0xFFFF0000)\n            def f():\n                pass\n            f()\n            push.assert_called_once_with('f', 0xFFFF0000)\n            pop.assert_called_once_with()\n\n    def test_TimeRangeDecorator_err(self):\n        push_patch = mock.patch('cupy.cuda.nvtx.RangePush')\n        pop_patch = mock.patch('cupy.cuda.nvtx.RangePop')\n        with push_patch as push, pop_patch as pop:\n            @prof.TimeRangeDecorator()\n            def f():\n                raise Exception()\n            try:\n                f()\n            except Exception:\n                pass\n            # Default value of color id is -1\n            push.assert_called_once_with('f', -1)\n            pop.assert_called_once_with()\n\n\nclass TestTimeRangeNVTXUnavailable(unittest.TestCase):\n\n    def setUp(self):\n        self.nvtx_enabled = cuda.nvtx_enabled\n        cuda.nvtx_enabled = False\n\n    def tearDown(self):\n        cuda.nvtx_enabled = self.nvtx_enabled\n\n    def test_time_range(self):\n        with self.assertRaises(RuntimeError):\n            with prof.time_range(''):\n                pass\n\n    def test_time_range_decorator(self):\n        with self.assertRaises(RuntimeError):\n            prof.TimeRangeDecorator()\n"""
tests/cupy_tests/random_tests/__init__.py,0,b''
tests/cupy_tests/random_tests/test_distributions.py,0,"b""import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy.random import distributions\nfrom cupy import testing\n\n\n_regular_float_dtypes = (numpy.float64, numpy.float32)\n_float_dtypes = _regular_float_dtypes + (numpy.float16,)\n_signed_dtypes = tuple(numpy.dtype(i).type for i in 'bhilq')\n_unsigned_dtypes = tuple(numpy.dtype(i).type for i in 'BHILQ')\n_int_dtypes = _signed_dtypes + _unsigned_dtypes\n\n\nclass RandomDistributionsTestCase(unittest.TestCase):\n    def check_distribution(self, dist_name, params, dtype):\n        cp_params = {k: cupy.asarray(params[k]) for k in params}\n        np_out = numpy.asarray(\n            getattr(numpy.random, dist_name)(size=self.shape, **params),\n            dtype)\n        cp_out = getattr(distributions, dist_name)(\n            size=self.shape, dtype=dtype, **cp_params)\n        self.assertEqual(cp_out.shape, np_out.shape)\n        self.assertEqual(cp_out.dtype, np_out.dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'a_shape': [(), (3, 2)],\n    'b_shape': [(), (3, 2)],\n    'dtype': _float_dtypes,  # to escape timeout\n})\n)\n@testing.gpu\nclass TestDistributionsBeta(RandomDistributionsTestCase):\n\n    @cupy.testing.for_dtypes_combination(\n        _float_dtypes, names=['a_dtype', 'b_dtype'])\n    def test_beta(self, a_dtype, b_dtype):\n        a = numpy.full(self.a_shape, 3, dtype=a_dtype)\n        b = numpy.full(self.b_shape, 3, dtype=b_dtype)\n        self.check_distribution('beta',\n                                {'a': a, 'b': b}, self.dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'n_shape': [(), (3, 2)],\n    'p_shape': [(), (3, 2)],\n    'dtype': _int_dtypes,  # to escape timeout\n})\n)\n@testing.gpu\nclass TestDistributionsBinomial(RandomDistributionsTestCase):\n\n    @cupy.testing.for_signed_dtypes('n_dtype')\n    @cupy.testing.for_float_dtypes('p_dtype')\n    def test_binomial(self, n_dtype, p_dtype):\n        if numpy.dtype('l') == numpy.int32 and n_dtype == numpy.int64:\n            self.skipTest('n must be able to cast to long')\n        n = numpy.full(self.n_shape, 5, dtype=n_dtype)\n        p = numpy.full(self.p_shape, 0.5, dtype=p_dtype)\n        self.check_distribution('binomial',\n                                {'n': n, 'p': p}, self.dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'df_shape': [(), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsChisquare(unittest.TestCase):\n\n    def check_distribution(self, dist_func, df_dtype, dtype):\n        df = cupy.full(self.df_shape, 5, dtype=df_dtype)\n        out = dist_func(df, self.shape, dtype)\n        self.assertEqual(self.shape, out.shape)\n        self.assertEqual(out.dtype, dtype)\n\n    @cupy.testing.for_float_dtypes('df_dtype')\n    @cupy.testing.for_float_dtypes('dtype')\n    def test_chisquare(self, df_dtype, dtype):\n        self.check_distribution(distributions.chisquare, df_dtype, dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2, 3), (3, 2, 3)],\n    'alpha_shape': [(3,)],\n})\n)\n@testing.gpu\nclass TestDistributionsDirichlet(RandomDistributionsTestCase):\n\n    @cupy.testing.for_dtypes_combination(\n        _float_dtypes, names=['alpha_dtype', 'dtype'])\n    def test_dirichlet(self, alpha_dtype, dtype):\n        alpha = numpy.ones(self.alpha_shape, dtype=alpha_dtype)\n        self.check_distribution('dirichlet',\n                                {'alpha': alpha}, dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2), None],\n    'scale_shape': [(), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsExponential(RandomDistributionsTestCase):\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_float_dtypes('scale_dtype')\n    def test_exponential(self, scale_dtype, dtype):\n        scale = numpy.ones(self.scale_shape, dtype=scale_dtype)\n        self.check_distribution('exponential',\n                                {'scale': scale}, dtype)\n\n\n@testing.gpu\nclass TestDistributionsExponentialError(RandomDistributionsTestCase):\n\n    def test_negative_scale(self):\n        scale = cupy.array([2, -1, 3], dtype=numpy.float32)\n        with self.assertRaises(ValueError):\n            cupy.random.exponential(scale)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'dfnum_shape': [(), (3, 2)],\n    'dfden_shape': [(), (3, 2)],\n    'dtype': _float_dtypes,  # to escape timeout\n})\n)\n@testing.gpu\nclass TestDistributionsF(unittest.TestCase):\n\n    def check_distribution(self, dist_func, dfnum_dtype, dfden_dtype, dtype):\n        dfnum = cupy.ones(self.dfnum_shape, dtype=dfnum_dtype)\n        dfden = cupy.ones(self.dfden_shape, dtype=dfden_dtype)\n        out = dist_func(dfnum, dfden, self.shape, dtype)\n        self.assertEqual(self.shape, out.shape)\n        self.assertEqual(out.dtype, dtype)\n\n    @cupy.testing.for_float_dtypes('dfnum_dtype')\n    @cupy.testing.for_float_dtypes('dfden_dtype')\n    def test_f(self, dfnum_dtype, dfden_dtype):\n        self.check_distribution(distributions.f,\n                                dfnum_dtype, dfden_dtype, self.dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'shape_shape': [(), (3, 2)],\n    'scale_shape': [(), (3, 2)],\n    'dtype': _float_dtypes,  # to escape timeout\n})\n)\n@testing.gpu\nclass TestDistributionsGamma(unittest.TestCase):\n\n    def check_distribution(self, dist_func, shape_dtype, scale_dtype, dtype):\n        shape = cupy.ones(self.shape_shape, dtype=shape_dtype)\n        scale = cupy.ones(self.scale_shape, dtype=scale_dtype)\n        out = dist_func(shape, scale, self.shape, dtype)\n        self.assertEqual(self.shape, out.shape)\n        self.assertEqual(out.dtype, dtype)\n\n    @cupy.testing.for_dtypes_combination(\n        _float_dtypes, names=['shape_dtype', 'scale_dtype'])\n    def test_gamma(self, shape_dtype, scale_dtype):\n        self.check_distribution(distributions.gamma,\n                                shape_dtype, scale_dtype, self.dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'p_shape': [(), (3, 2)],\n    'dtype': _int_dtypes,  # to escape timeout\n})\n)\n@testing.gpu\nclass TestDistributionsGeometric(unittest.TestCase):\n\n    def check_distribution(self, dist_func, p_dtype, dtype):\n        p = 0.5 * cupy.ones(self.p_shape, dtype=p_dtype)\n        out = dist_func(p, self.shape, dtype)\n        self.assertEqual(self.shape, out.shape)\n        self.assertEqual(out.dtype, dtype)\n\n    @cupy.testing.for_float_dtypes('p_dtype')\n    def test_geometric(self, p_dtype):\n        self.check_distribution(distributions.geometric,\n                                p_dtype, self.dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'loc_shape': [(), (3, 2)],\n    'scale_shape': [(), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsGumbel(RandomDistributionsTestCase):\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_dtypes_combination(\n        _float_dtypes, names=['loc_dtype', 'scale_dtype'])\n    def test_gumbel(self, loc_dtype, scale_dtype, dtype):\n        loc = numpy.ones(self.loc_shape, dtype=loc_dtype)\n        scale = numpy.ones(self.scale_shape, dtype=scale_dtype)\n        self.check_distribution('gumbel',\n                                {'loc': loc, 'scale': scale}, dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'ngood_shape': [(), (3, 2)],\n    'nbad_shape': [(), (3, 2)],\n    'nsample_shape': [(), (3, 2)],\n    'nsample_dtype': [numpy.int32, numpy.int64],  # to escape timeout\n    'dtype': [numpy.int32, numpy.int64],  # to escape timeout\n})\n)\n@testing.gpu\nclass TestDistributionsHyperGeometric(unittest.TestCase):\n\n    def check_distribution(self, dist_func, ngood_dtype, nbad_dtype,\n                           nsample_dtype, dtype):\n        ngood = cupy.ones(self.ngood_shape, dtype=ngood_dtype)\n        nbad = cupy.ones(self.nbad_shape, dtype=nbad_dtype)\n        nsample = cupy.ones(self.nsample_shape, dtype=nsample_dtype)\n        out = dist_func(ngood, nbad, nsample, self.shape, dtype)\n        self.assertEqual(self.shape, out.shape)\n        self.assertEqual(out.dtype, dtype)\n\n    @cupy.testing.for_dtypes_combination(\n        [numpy.int32, numpy.int64], names=['ngood_dtype', 'nbad_dtype'])\n    def test_hypergeometric(self, ngood_dtype, nbad_dtype):\n        self.check_distribution(distributions.hypergeometric, ngood_dtype,\n                                nbad_dtype, self.nsample_dtype, self.dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'loc_shape': [(), (3, 2)],\n    'scale_shape': [(), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsLaplace(RandomDistributionsTestCase):\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_dtypes_combination(\n        _float_dtypes, names=['loc_dtype', 'scale_dtype'])\n    def test_laplace(self, loc_dtype, scale_dtype, dtype):\n        loc = numpy.ones(self.loc_shape, dtype=loc_dtype)\n        scale = numpy.ones(self.scale_shape, dtype=scale_dtype)\n        self.check_distribution('laplace',\n                                {'loc': loc, 'scale': scale}, dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'loc_shape': [(), (3, 2)],\n    'scale_shape': [(), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsLogistic(RandomDistributionsTestCase):\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_dtypes_combination(\n        _float_dtypes, names=['loc_dtype', 'scale_dtype'])\n    def test_logistic(self, loc_dtype, scale_dtype, dtype):\n        loc = numpy.ones(self.loc_shape, dtype=loc_dtype)\n        scale = numpy.ones(self.scale_shape, dtype=scale_dtype)\n        self.check_distribution('logistic',\n                                {'loc': loc, 'scale': scale}, dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'mean_shape': [()],\n    'sigma_shape': [()],\n})\n)\n@testing.gpu\nclass TestDistributionsLognormal(RandomDistributionsTestCase):\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_dtypes_combination(\n        _float_dtypes, names=['mean_dtype', 'sigma_dtype'])\n    def test_lognormal(self, mean_dtype, sigma_dtype, dtype):\n        mean = numpy.ones(self.mean_shape, dtype=mean_dtype)\n        sigma = numpy.ones(self.sigma_shape, dtype=sigma_dtype)\n        self.check_distribution('lognormal',\n                                {'mean': mean, 'sigma': sigma}, dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'p_shape': [()],\n})\n)\n@testing.gpu\nclass TestDistributionsLogseries(RandomDistributionsTestCase):\n\n    @cupy.testing.for_dtypes([numpy.int64, numpy.int32], 'dtype')\n    @cupy.testing.for_float_dtypes('p_dtype', no_float16=True)\n    def test_logseries(self, p_dtype, dtype):\n        p = numpy.full(self.p_shape, 0.5, dtype=p_dtype)\n        self.check_distribution('logseries',\n                                {'p': p}, dtype)\n\n    @cupy.testing.for_dtypes([numpy.int64, numpy.int32], 'dtype')\n    @cupy.testing.for_float_dtypes('p_dtype', no_float16=True)\n    def test_logseries_for_invalid_p(self, p_dtype, dtype):\n        with self.assertRaises(ValueError):\n            cp_params = {'p': cupy.zeros(self.p_shape, dtype=p_dtype)}\n            distributions.logseries(size=self.shape, dtype=dtype, **cp_params)\n        with self.assertRaises(ValueError):\n            cp_params = {'p': cupy.ones(self.p_shape, dtype=p_dtype)}\n            distributions.logseries(size=self.shape, dtype=dtype, **cp_params)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'd': [2, 4],\n})\n)\n@testing.gpu\nclass TestDistributionsMultivariateNormal(unittest.TestCase):\n\n    def check_distribution(self, dist_func, mean_dtype, cov_dtype, dtype):\n        mean = cupy.zeros(self.d, dtype=mean_dtype)\n        cov = cupy.random.normal(size=(self.d, self.d), dtype=cov_dtype)\n        cov = cov.T.dot(cov)\n        out = dist_func(mean, cov, self.shape, dtype=dtype)\n        self.assertEqual(self.shape+(self.d,), out.shape)\n        self.assertEqual(out.dtype, dtype)\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_float_dtypes('mean_dtype', no_float16=True)\n    @cupy.testing.for_float_dtypes('cov_dtype', no_float16=True)\n    def test_normal(self, mean_dtype, cov_dtype, dtype):\n        self.check_distribution(distributions.multivariate_normal,\n                                mean_dtype, cov_dtype, dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'n_shape': [(), (3, 2)],\n    'p_shape': [(), (3, 2)],\n    'dtype': _int_dtypes,  # to escape timeout\n})\n)\n@testing.gpu\nclass TestDistributionsNegativeBinomial(RandomDistributionsTestCase):\n\n    @cupy.testing.for_float_dtypes('n_dtype')\n    @cupy.testing.for_float_dtypes('p_dtype')\n    def test_negative_binomial(self, n_dtype, p_dtype):\n        n = numpy.full(self.n_shape, 5, dtype=n_dtype)\n        p = numpy.full(self.p_shape, 0.5, dtype=p_dtype)\n        self.check_distribution('negative_binomial',\n                                {'n': n, 'p': p}, self.dtype)\n\n    @cupy.testing.for_float_dtypes('n_dtype')\n    @cupy.testing.for_float_dtypes('p_dtype')\n    def test_negative_binomial_for_noninteger_n(self, n_dtype, p_dtype):\n        n = numpy.full(self.n_shape, 5.5, dtype=n_dtype)\n        p = numpy.full(self.p_shape, 0.5, dtype=p_dtype)\n        self.check_distribution('negative_binomial',\n                                {'n': n, 'p': p}, self.dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'df_shape': [(), (3, 2)],\n    'nonc_shape': [(), (3, 2)],\n    'dtype': _int_dtypes,  # to escape timeout\n})\n)\n@testing.gpu\nclass TestDistributionsNoncentralChisquare(RandomDistributionsTestCase):\n\n    @cupy.testing.for_dtypes_combination(\n        _regular_float_dtypes, names=['df_dtype', 'nonc_dtype'])\n    def test_noncentral_chisquare(self, df_dtype, nonc_dtype):\n        df = numpy.full(self.df_shape, 1, dtype=df_dtype)\n        nonc = numpy.full(self.nonc_shape, 1, dtype=nonc_dtype)\n        self.check_distribution('noncentral_chisquare',\n                                {'df': df, 'nonc': nonc}, self.dtype)\n\n    @cupy.testing.for_float_dtypes('param_dtype', no_float16=True)\n    def test_noncentral_chisquare_for_invalid_params(self, param_dtype):\n        df = cupy.full(self.df_shape, -1, dtype=param_dtype)\n        nonc = cupy.full(self.nonc_shape, 1, dtype=param_dtype)\n        with self.assertRaises(ValueError):\n            distributions.noncentral_chisquare(\n                df, nonc, size=self.shape, dtype=self.dtype)\n\n        df = cupy.full(self.df_shape, 1, dtype=param_dtype)\n        nonc = cupy.full(self.nonc_shape, -1, dtype=param_dtype)\n        with self.assertRaises(ValueError):\n            distributions.noncentral_chisquare(\n                df, nonc, size=self.shape, dtype=self.dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'dfnum_shape': [(), (3, 2)],\n    'dfden_shape': [(), (3, 2)],\n    'nonc_shape': [(), (3, 2)],\n    'dtype': _int_dtypes,  # to escape timeout\n})\n)\n@testing.gpu\nclass TestDistributionsNoncentralF(RandomDistributionsTestCase):\n\n    @cupy.testing.for_dtypes_combination(\n        _regular_float_dtypes,\n        names=['dfnum_dtype', 'dfden_dtype', 'nonc_dtype'])\n    def test_noncentral_f(self, dfnum_dtype, dfden_dtype, nonc_dtype):\n        dfnum = numpy.full(self.dfnum_shape, 1, dtype=dfnum_dtype)\n        dfden = numpy.full(self.dfden_shape, 1, dtype=dfden_dtype)\n        nonc = numpy.full(self.nonc_shape, 1, dtype=nonc_dtype)\n        self.check_distribution('noncentral_f',\n                                {'dfnum': dfnum, 'dfden': dfden, 'nonc': nonc},\n                                self.dtype)\n\n    @cupy.testing.for_float_dtypes('param_dtype', no_float16=True)\n    def test_noncentral_f_for_invalid_params(self, param_dtype):\n        dfnum = numpy.full(self.dfnum_shape, -1, dtype=param_dtype)\n        dfden = numpy.full(self.dfden_shape, 1, dtype=param_dtype)\n        nonc = numpy.full(self.nonc_shape, 1, dtype=param_dtype)\n        with self.assertRaises(ValueError):\n            distributions.noncentral_f(\n                dfnum, dfden, nonc, size=self.shape, dtype=self.dtype)\n\n        dfnum = numpy.full(self.dfnum_shape, 1, dtype=param_dtype)\n        dfden = numpy.full(self.dfden_shape, -1, dtype=param_dtype)\n        nonc = numpy.full(self.nonc_shape, 1, dtype=param_dtype)\n        with self.assertRaises(ValueError):\n            distributions.noncentral_f(\n                dfnum, dfden, nonc, size=self.shape, dtype=self.dtype)\n\n        dfnum = numpy.full(self.dfnum_shape, 1, dtype=param_dtype)\n        dfden = numpy.full(self.dfden_shape, 1, dtype=param_dtype)\n        nonc = numpy.full(self.nonc_shape, -1, dtype=param_dtype)\n        with self.assertRaises(ValueError):\n            distributions.noncentral_f(\n                dfnum, dfden, nonc, size=self.shape, dtype=self.dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'loc_shape': [(), (3, 2)],\n    'scale_shape': [(), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsNormal(RandomDistributionsTestCase):\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_dtypes_combination(\n        _float_dtypes, names=['loc_dtype', 'scale_dtype'])\n    def test_normal(self, loc_dtype, scale_dtype, dtype):\n        loc = numpy.ones(self.loc_shape, dtype=loc_dtype)\n        scale = numpy.ones(self.scale_shape, dtype=scale_dtype)\n        self.check_distribution('normal',\n                                {'loc': loc, 'scale': scale}, dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'a_shape': [(), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsPareto(unittest.TestCase):\n\n    def check_distribution(self, dist_func, a_dtype, dtype):\n        a = cupy.ones(self.a_shape, dtype=a_dtype)\n        out = dist_func(a, self.shape, dtype)\n        self.assertEqual(self.shape, out.shape)\n        self.assertEqual(out.dtype, dtype)\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_float_dtypes('a_dtype')\n    def test_pareto(self, a_dtype, dtype):\n        self.check_distribution(distributions.pareto,\n                                a_dtype, dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'lam_shape': [(), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsPoisson(unittest.TestCase):\n\n    def check_distribution(self, dist_func, lam_dtype, dtype):\n        lam = cupy.full(self.lam_shape, 5, dtype=lam_dtype)\n        out = dist_func(lam, self.shape, dtype)\n        self.assertEqual(self.shape, out.shape)\n        self.assertEqual(out.dtype, dtype)\n\n    @cupy.testing.for_int_dtypes('dtype')\n    @cupy.testing.for_float_dtypes('lam_dtype')\n    def test_poisson(self, lam_dtype, dtype):\n        self.check_distribution(distributions.poisson, lam_dtype, dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'a_shape': [()],\n})\n)\n@testing.gpu\nclass TestDistributionsPower(RandomDistributionsTestCase):\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_float_dtypes('a_dtype')\n    def test_power(self, a_dtype, dtype):\n        a = numpy.full(self.a_shape, 0.5, dtype=a_dtype)\n        self.check_distribution('power', {'a': a}, dtype)\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_float_dtypes('a_dtype')\n    def test_power_for_negative_a(self, a_dtype, dtype):\n        a = numpy.full(self.a_shape, -0.5, dtype=a_dtype)\n        with self.assertRaises(ValueError):\n            cp_params = {'a': cupy.asarray(a)}\n            getattr(distributions, 'power')(\n                size=self.shape, dtype=dtype, **cp_params)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'scale_shape': [(), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsRayleigh(RandomDistributionsTestCase):\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_float_dtypes('scale_dtype')\n    def test_rayleigh(self, scale_dtype, dtype):\n        scale = numpy.full(self.scale_shape, 3, dtype=scale_dtype)\n        self.check_distribution('rayleigh',\n                                {'scale': scale}, dtype)\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_float_dtypes('scale_dtype')\n    def test_rayleigh_for_zero_scale(self, scale_dtype, dtype):\n        scale = numpy.zeros(self.scale_shape, dtype=scale_dtype)\n        self.check_distribution('rayleigh',\n                                {'scale': scale}, dtype)\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_float_dtypes('scale_dtype')\n    def test_rayleigh_for_negative_scale(self, scale_dtype, dtype):\n        scale = numpy.full(self.scale_shape, -0.5, dtype=scale_dtype)\n        with self.assertRaises(ValueError):\n            cp_params = {'scale': cupy.asarray(scale)}\n            distributions.rayleigh(size=self.shape, dtype=dtype, **cp_params)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsStandardCauchy(RandomDistributionsTestCase):\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    def test_standard_cauchy(self, dtype):\n        self.check_distribution('standard_cauchy', {}, dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsStandardExponential(RandomDistributionsTestCase):\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    def test_standard_exponential(self, dtype):\n        self.check_distribution('standard_exponential', {}, dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'shape_shape': [(), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsStandardGamma(RandomDistributionsTestCase):\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_float_dtypes('shape_dtype')\n    def test_standard_gamma(self, shape_dtype, dtype):\n        shape = numpy.ones(self.shape_shape, dtype=shape_dtype)\n        self.check_distribution('standard_gamma',\n                                {'shape': shape}, dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsStandardNormal(RandomDistributionsTestCase):\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    def test_standard_normal(self, dtype):\n        self.check_distribution('standard_normal', {}, dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'df_shape': [(), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsStandardT(unittest.TestCase):\n\n    def check_distribution(self, dist_func, df_dtype, dtype):\n        df = cupy.ones(self.df_shape, dtype=df_dtype)\n        out = dist_func(df, self.shape, dtype)\n        self.assertEqual(self.shape, out.shape)\n        self.assertEqual(out.dtype, dtype)\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_float_dtypes('df_dtype')\n    def test_standard_t(self, df_dtype, dtype):\n        self.check_distribution(distributions.standard_t, df_dtype, dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'left_shape': [(), (3, 2)],\n    'mode_shape': [(), (3, 2)],\n    'right_shape': [(), (3, 2)],\n    'dtype': _regular_float_dtypes,  # to escape timeout\n})\n)\n@testing.gpu\nclass TestDistributionsTriangular(RandomDistributionsTestCase):\n\n    @cupy.testing.for_dtypes_combination(\n        _regular_float_dtypes,\n        names=['left_dtype', 'mode_dtype', 'right_dtype'])\n    def test_triangular(self, left_dtype, mode_dtype, right_dtype):\n        left = numpy.full(self.left_shape, -1, dtype=left_dtype)\n        mode = numpy.full(self.mode_shape, 0, dtype=mode_dtype)\n        right = numpy.full(self.right_shape, 2, dtype=right_dtype)\n        self.check_distribution('triangular',\n                                {'left': left, 'mode': mode, 'right': right},\n                                self.dtype)\n\n    @cupy.testing.for_float_dtypes('param_dtype', no_float16=True)\n    def test_triangular_for_invalid_params(self, param_dtype):\n        left = cupy.full(self.left_shape, 1, dtype=param_dtype)\n        mode = cupy.full(self.mode_shape, 0, dtype=param_dtype)\n        right = cupy.full(self.right_shape, 2, dtype=param_dtype)\n        with self.assertRaises(ValueError):\n            distributions.triangular(\n                left, mode, right, size=self.shape, dtype=self.dtype)\n\n        left = cupy.full(self.left_shape, -2, dtype=param_dtype)\n        mode = cupy.full(self.mode_shape, 0, dtype=param_dtype)\n        right = cupy.full(self.right_shape, -1, dtype=param_dtype)\n        with self.assertRaises(ValueError):\n            distributions.triangular(\n                left, mode, right, size=self.shape, dtype=self.dtype)\n\n        left = cupy.full(self.left_shape, 0, dtype=param_dtype)\n        mode = cupy.full(self.mode_shape, 0, dtype=param_dtype)\n        right = cupy.full(self.right_shape, 0, dtype=param_dtype)\n        with self.assertRaises(ValueError):\n            distributions.triangular(\n                left, mode, right, size=self.shape, dtype=self.dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'low_shape': [(), (3, 2)],\n    'high_shape': [(), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsUniform(RandomDistributionsTestCase):\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_dtypes_combination(\n        _float_dtypes, names=['low_dtype', 'high_dtype'])\n    def test_uniform(self, low_dtype, high_dtype, dtype):\n        low = numpy.ones(self.low_shape, dtype=low_dtype)\n        high = numpy.ones(self.high_shape, dtype=high_dtype) * 2.\n        self.check_distribution('uniform',\n                                {'low': low, 'high': high}, dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'mu_shape': [(), (3, 2)],\n    'kappa_shape': [(), (3, 2)],\n    'dtype': _float_dtypes,  # to escape timeout\n})\n)\n@testing.gpu\nclass TestDistributionsVonmises(unittest.TestCase):\n\n    def check_distribution(self, dist_func, mu_dtype, kappa_dtype, dtype):\n        mu = cupy.ones(self.mu_shape, dtype=mu_dtype)\n        kappa = cupy.ones(self.kappa_shape, dtype=kappa_dtype)\n        out = dist_func(mu, kappa, self.shape, dtype)\n        self.assertEqual(self.shape, out.shape)\n        self.assertEqual(out.dtype, dtype)\n\n    @cupy.testing.for_dtypes_combination(\n        _float_dtypes, names=['mu_dtype', 'kappa_dtype'])\n    def test_vonmises(self, mu_dtype, kappa_dtype):\n        self.check_distribution(distributions.vonmises,\n                                mu_dtype, kappa_dtype, self.dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'mean_shape': [(), (3, 2)],\n    'scale_shape': [(), (3, 2)],\n    'dtype': _regular_float_dtypes,  # to escape timeout\n})\n)\n@testing.gpu\nclass TestDistributionsWald(RandomDistributionsTestCase):\n\n    @cupy.testing.for_dtypes_combination(\n        _float_dtypes, names=['mean_dtype', 'scale_dtype'])\n    def test_wald(self, mean_dtype, scale_dtype):\n        mean = numpy.full(self.mean_shape, 3, dtype=mean_dtype)\n        scale = numpy.full(self.scale_shape, 3, dtype=scale_dtype)\n        self.check_distribution('wald',\n                                {'mean': mean, 'scale': scale}, self.dtype)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'a_shape': [(), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsWeibull(RandomDistributionsTestCase):\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_float_dtypes('a_dtype')\n    def test_weibull(self, a_dtype, dtype):\n        a = numpy.ones(self.a_shape, dtype=a_dtype)\n        self.check_distribution('weibull',\n                                {'a': a}, dtype)\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_float_dtypes('a_dtype')\n    def test_weibull_for_inf_a(self, a_dtype, dtype):\n        a = numpy.full(self.a_shape, numpy.inf, dtype=a_dtype)\n        self.check_distribution('weibull', {'a': a}, dtype)\n\n    @cupy.testing.for_float_dtypes('dtype', no_float16=True)\n    @cupy.testing.for_float_dtypes('a_dtype')\n    def test_weibull_for_negative_a(self, a_dtype, dtype):\n        a = numpy.full(self.a_shape, -0.5, dtype=a_dtype)\n        with self.assertRaises(ValueError):\n            cp_params = {'a': cupy.asarray(a)}\n            getattr(distributions, 'weibull')(\n                size=self.shape, dtype=dtype, **cp_params)\n\n\n@testing.parameterize(*testing.product({\n    'shape': [(4, 3, 2), (3, 2)],\n    'a_shape': [(), (3, 2)],\n})\n)\n@testing.gpu\nclass TestDistributionsZipf(RandomDistributionsTestCase):\n\n    @cupy.testing.for_dtypes([numpy.int32, numpy.int64], 'dtype')\n    @cupy.testing.for_float_dtypes('a_dtype')\n    def test_zipf(self, a_dtype, dtype):\n        a = numpy.full(self.a_shape, 2, dtype=a_dtype)\n        self.check_distribution('zipf', {'a': a}, dtype)\n"""
tests/cupy_tests/random_tests/test_generator.py,0,"b'import functools\nimport os\nimport threading\nimport unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import core\nfrom cupy import cuda\nfrom cupy.random import generator\nfrom cupy import testing\nfrom cupy.testing import attr\nfrom cupy.testing import condition\nfrom cupy.testing import hypothesis\n\n\ndef numpy_cupy_equal_continuous_distribution(significance_level, name=\'xp\'):\n    """"""Decorator that tests the distributions of NumPy samples and CuPy ones.\n\n    Args:\n        significance_level (float): The test fails if p-value is lower than\n            this argument.\n        name(str): Argument name whose value is either\n            ``numpy`` or ``cupy`` module.\n\n    Decorated test fixture is required to return samples from the same\n    distribution even if ``xp`` is ``numpy`` or ``cupy``.\n\n    """"""\n    def decorator(impl):\n        @functools.wraps(impl)\n        def test_func(self, *args, **kw):\n            kw[name] = cupy\n            cupy_result = impl(self, *args, **kw)\n\n            kw[name] = numpy\n            numpy_result = impl(self, *args, **kw)\n\n            self.assertIsNotNone(cupy_result)\n            self.assertIsNotNone(numpy_result)\n            d_plus, d_minus, p_value = \\\n                two_sample_Kolmogorov_Smirnov_test(\n                    cupy.asnumpy(cupy_result), numpy_result)\n            if p_value < significance_level:\n                message = \'\'\'Rejected null hypothesis:\np: %f\nD+ (cupy < numpy): %f\nD- (cupy > numpy): %f\'\'\' % (p_value, d_plus, d_minus)\n                raise AssertionError(message)\n        return test_func\n    return decorator\n\n\ndef two_sample_Kolmogorov_Smirnov_test(observed1, observed2):\n    """"""Computes the Kolmogorov-Smirnov statistic on 2 samples\n\n    Unlike `scipy.stats.ks_2samp`, the returned p-value is not accurate\n    for large p.\n    """"""\n    assert observed1.dtype == observed2.dtype\n    n1, = observed1.shape\n    n2, = observed2.shape\n    assert n1 >= 100 and n2 >= 100\n    observed = numpy.concatenate([observed1, observed2])\n    indices = numpy.argsort(observed)\n    observed = observed[indices]  # sort\n    ds = numpy.cumsum(numpy.where(indices < n1, -n2, n1).astype(numpy.int64))\n    assert ds[-1] == 0\n    check = numpy.concatenate([observed[:-1] < observed[1:], [True]])\n    ds = ds[check]\n    d_plus = float(ds.max()) / (n1 * n2)\n    d_minus = -float(ds.min()) / (n1 * n2)\n    d = max(d_plus, d_minus)\n    # Approximate p = special.kolmogorov(d * numpy.sqrt(n1 * n2 / (n1 + n2)))\n    p = min(1.0, 2.0 * numpy.exp(-2.0 * d**2 * n1 * n2 / (n1 + n2)))\n    return d_plus, d_minus, p\n\n\nclass RandomGeneratorTestCase(unittest.TestCase):\n\n    target_method = None\n\n    def setUp(self):\n        self.__seed = testing.generate_seed()\n        self.rs = generator.RandomState(seed=self.__seed)\n\n    def _get_generator_func(self, *args, **kwargs):\n        assert isinstance(self.target_method, str), (\n            \'generate_method must be overridden\')\n        f = getattr(self.rs, self.target_method)\n        return lambda: f(*args, **kwargs)\n\n    def _generate_check_repro(self, func, seed):\n        # Sample a random array while checking reproducibility\n        self.rs.seed(seed)\n        x = func()\n        self.rs.seed(seed)\n        y = func()\n        testing.assert_array_equal(\n            x, y,\n            \'Randomly generated arrays with the same seed did not match\')\n        return x\n\n    def generate(self, *args, **kwargs):\n        # Pick one sample from generator.\n        # Reproducibility is checked by repeating seed-and-sample cycle twice.\n        func = self._get_generator_func(*args, **kwargs)\n        return self._generate_check_repro(func, self.__seed)\n\n    def generate_many(self, *args, **kwargs):\n        # Pick many samples from generator.\n        # Reproducibility is checked only for the first sample,\n        # because it\'s very slow to set seed every time.\n        _count = kwargs.pop(\'_count\', None)\n        assert _count is not None, \'_count is required\'\n        func = self._get_generator_func(*args, **kwargs)\n\n        if _count == 0:\n            return []\n\n        vals = [self._generate_check_repro(func, self.__seed)]\n        for _ in range(1, _count):\n            vals.append(func())\n        return vals\n\n    def check_ks(self, significance_level, cupy_len=100, numpy_len=1000):\n        return functools.partial(\n            self._check_ks, significance_level, cupy_len, numpy_len)\n\n    def _check_ks(\n            self, significance_level, cupy_len, numpy_len,\n            *args, **kwargs):\n        assert \'size\' in kwargs\n\n        # cupy\n        func = self._get_generator_func(*args, **kwargs)\n        vals_cupy = func()\n        assert vals_cupy.size > 0\n        count = 1 + (cupy_len - 1) // vals_cupy.size\n        vals_cupy = [vals_cupy]\n        for _ in range(1, count):\n            vals_cupy.append(func())\n        vals_cupy = cupy.stack(vals_cupy).ravel()\n\n        # numpy\n        kwargs[\'size\'] = numpy_len\n        dtype = kwargs.pop(\'dtype\', None)\n        numpy_rs = numpy.random.RandomState(self.__seed)\n        vals_numpy = getattr(numpy_rs, self.target_method)(*args, **kwargs)\n        if dtype is not None:\n            vals_numpy = vals_numpy.astype(dtype, copy=False)\n\n        # test\n        d_plus, d_minus, p_value = \\\n            two_sample_Kolmogorov_Smirnov_test(\n                cupy.asnumpy(vals_cupy), vals_numpy)\n        if p_value < significance_level:\n            message = \'\'\'Rejected null hypothesis:\np: %f\nD+ (cupy < numpy): %f\nD- (cupy > numpy): %f\'\'\' % (p_value, d_plus, d_minus)\n            raise AssertionError(message)\n\n\ndef _xp_random(xp, method_name):\n    method = getattr(xp.random.RandomState(), method_name)\n    if xp == cupy:\n        return method\n\n    def f(*args, **kwargs):\n        dtype = kwargs.pop(\'dtype\', None)\n        ret = method(*args, **kwargs)\n        if dtype is not None:\n            ret = ret.astype(dtype, copy=False)\n        return ret\n\n    return f\n\n\n@testing.fix_random()\n@testing.gpu\nclass TestRandomState(unittest.TestCase):\n\n    def setUp(self):\n        self.rs = generator.RandomState(seed=testing.generate_seed())\n\n    def check_seed(self, seed):\n        rs = self.rs\n\n        rs.seed(seed)\n        xs1 = [rs.uniform() for _ in range(100)]\n\n        rs.seed(seed)\n        xs2 = [rs.uniform() for _ in range(100)]\n\n        rs.seed(seed)\n        rs.seed(None)\n        xs3 = [rs.uniform() for _ in range(100)]\n\n        # Random state must be reproducible\n        assert xs1 == xs2\n        # Random state must be initialized randomly with seed=None\n        assert xs1 != xs3\n\n    @testing.for_int_dtypes()\n    def test_seed_not_none(self, dtype):\n        self.check_seed(dtype(0))\n\n    @testing.for_dtypes([numpy.complex_])\n    def test_seed_invalid_type_complex(self, dtype):\n        with self.assertRaises(TypeError):\n            self.rs.seed(dtype(0))\n\n    @testing.for_float_dtypes()\n    def test_seed_invalid_type_float(self, dtype):\n        with self.assertRaises(TypeError):\n            self.rs.seed(dtype(0))\n\n    def test_array_seed(self):\n        self.check_seed(numpy.random.randint(0, 2**31, size=40))\n\n\n@testing.parameterize(\n    {\'a\': 1.0, \'b\': 3.0},\n    {\'a\': 3.0, \'b\': 3.0},\n    {\'a\': 3.0, \'b\': 1.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestBeta(RandomGeneratorTestCase):\n\n    target_method = \'beta\'\n\n    def test_beta(self):\n        self.generate(a=self.a, b=self.b, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_beta_ks(self, dtype):\n        self.check_ks(0.05)(\n            a=self.a, b=self.b, size=2000, dtype=dtype)\n\n\n@testing.parameterize(\n    {\'n\': 5, \'p\': 0.5},\n    {\'n\': 5, \'p\': 0.0},\n    {\'n\': 5, \'p\': 1.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestBinomial(RandomGeneratorTestCase):\n    # TODO(niboshi):\n    #   Test soundness of distribution.\n    #   Currently only reprocibility is checked.\n\n    target_method = \'binomial\'\n\n    def test_binomial(self):\n        self.generate(n=self.n, p=self.p, size=(3, 2))\n\n\n@testing.parameterize(\n    {\'df\': 1.0},\n    {\'df\': 3.0},\n    {\'df\': 10.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestChisquare(RandomGeneratorTestCase):\n\n    target_method = \'chisquare\'\n\n    def test_chisquare(self):\n        self.generate(df=self.df, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_chisquare_ks(self, dtype):\n        self.check_ks(0.05)(\n            df=self.df, size=2000, dtype=dtype)\n\n\n@testing.gpu\n@testing.parameterize(\n    {\'alpha\': cupy.array([1.0, 1.0, 1.0])},\n    {\'alpha\': cupy.array([1.0, 3.0, 5.0])},\n)\n@testing.fix_random()\nclass TestDirichlet(RandomGeneratorTestCase):\n\n    target_method = \'dirichlet\'\n\n    def test_dirichlet(self):\n        self.generate(alpha=self.alpha, size=(3, 2, 3))\n\n    # TODO(kataoka): add distribution test\n\n\n@testing.parameterize(\n    {\'scale\': 1.0},\n    {\'scale\': 3.0},\n    {\'scale\': 10.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestExponential(RandomGeneratorTestCase):\n\n    target_method = \'exponential\'\n\n    def test_exponential(self):\n        self.generate(scale=self.scale, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_exponential_ks(self, dtype):\n        self.check_ks(0.05)(\n            self.scale, size=2000, dtype=dtype)\n\n\n@testing.parameterize(\n    {\'dfnum\': 1.0, \'dfden\': 3.0},\n    {\'dfnum\': 3.0, \'dfden\': 3.0},\n    {\'dfnum\': 3.0, \'dfden\': 1.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestF(RandomGeneratorTestCase):\n\n    target_method = \'f\'\n\n    def test_f(self):\n        self.generate(dfnum=self.dfnum, dfden=self.dfden, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_f_ks(self, dtype):\n        self.check_ks(0.05)(\n            self.dfnum, self.dfden, size=2000, dtype=dtype)\n\n\n@testing.parameterize(\n    {\'shape\': 0.5, \'scale\': 0.5},\n    {\'shape\': 1.0, \'scale\': 0.5},\n    {\'shape\': 3.0, \'scale\': 0.5},\n    {\'shape\': 0.5, \'scale\': 1.0},\n    {\'shape\': 1.0, \'scale\': 1.0},\n    {\'shape\': 3.0, \'scale\': 1.0},\n    {\'shape\': 0.5, \'scale\': 3.0},\n    {\'shape\': 1.0, \'scale\': 3.0},\n    {\'shape\': 3.0, \'scale\': 3.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestGamma(RandomGeneratorTestCase):\n\n    target_method = \'gamma\'\n\n    def test_gamma_1(self):\n        self.generate(shape=self.shape, scale=self.scale, size=(3, 2))\n\n    def test_gamma_2(self):\n        self.generate(shape=self.shape, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_gamma_ks(self, dtype):\n        self.check_ks(0.05)(\n            self.shape, self.scale, size=2000, dtype=dtype)\n\n\n@testing.parameterize(\n    {\'p\': 0.5},\n    {\'p\': 0.1},\n    {\'p\': 1.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestGeometric(RandomGeneratorTestCase):\n\n    target_method = \'geometric\'\n\n    def test_geometric(self):\n        self.generate(p=self.p, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_geometric_ks(self, dtype):\n        self.check_ks(0.05)(\n            p=self.p, size=2000, dtype=dtype)\n\n\n@testing.parameterize(\n    {\'ngood\': 1, \'nbad\': 1, \'nsample\': 1},\n    {\'ngood\': 1, \'nbad\': 1, \'nsample\': 2},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestHypergeometric(RandomGeneratorTestCase):\n\n    target_method = \'hypergeometric\'\n\n    def test_hypergeometric(self):\n        self.generate(ngood=self.ngood, nbad=self.nbad, nsample=self.nsample,\n                      size=(3, 2))\n\n    # TODO(kataoka): add distribution test\n\n\n@testing.gpu\n@testing.fix_random()\nclass TestLaplace(RandomGeneratorTestCase):\n\n    target_method = \'laplace\'\n\n    def test_laplace_1(self):\n        self.generate()\n\n    def test_laplace_2(self):\n        self.generate(0.0, 1.0, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_laplace_ks_1(self, dtype):\n        self.check_ks(0.05)(\n            size=2000, dtype=dtype)\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_laplace_ks_2(self, dtype):\n        self.check_ks(0.05)(\n            2.3, 4.5, size=2000, dtype=dtype)\n\n\n@testing.gpu\n@testing.fix_random()\nclass TestLogistic(RandomGeneratorTestCase):\n\n    target_method = \'logistic\'\n\n    def test_logistic_1(self):\n        self.generate()\n\n    def test_logistic_2(self):\n        self.generate(0.0, 1.0, size=(3, 2))\n\n    @attr.slow\n    @condition.repeat(10)\n    def test_standard_logistic_isfinite(self):\n        x = self.generate(size=10**7)\n        self.assertTrue(cupy.isfinite(x).all())\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_logistic_ks_1(self, dtype):\n        self.check_ks(0.05)(\n            size=2000, dtype=dtype)\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_logistic_ks_2(self, dtype):\n        self.check_ks(0.05)(\n            2.3, 4.5, size=2000, dtype=dtype)\n\n\n@testing.gpu\n@testing.parameterize(*[\n    {\'args\': (0.0, 1.0), \'size\': None},\n    {\'args\': (10.0, 20.0), \'size\': None},\n    {\'args\': (0.0, 1.0), \'size\': 10},\n    {\'args\': (0.0, 1.0), \'size\': (1, 2, 3)},\n    {\'args\': (0.0, 1.0), \'size\': 3},\n    {\'args\': (0.0, 1.0), \'size\': (3, 3)},\n    {\'args\': (0.0, 1.0), \'size\': ()},\n])\n@testing.fix_random()\nclass TestLogNormal(RandomGeneratorTestCase):\n\n    target_method = \'lognormal\'\n\n    def check_lognormal(self, dtype):\n        vals = self.generate_many(\n            self.args[0], self.args[1], self.size, dtype, _count=10)\n\n        shape = core.get_size(self.size)\n        for val in vals:\n            assert isinstance(val, cupy.ndarray)\n            assert val.dtype == dtype\n            assert val.shape == shape\n            assert (0 <= val).all()\n\n    def test_lognormal_float(self):\n        self.check_lognormal(float)\n\n    def test_lognormal_float32(self):\n        self.check_lognormal(numpy.float32)\n\n    def test_lognormal_float64(self):\n        self.check_lognormal(numpy.float64)\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_lognormal_ks(self, dtype):\n        self.check_ks(0.05)(\n            *self.args, size=self.size, dtype=dtype)\n\n\n@testing.parameterize(\n    {\'p\': 0.5},\n    {\'p\': 0.1},\n    {\'p\': 0.9},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestLogseries(RandomGeneratorTestCase):\n\n    target_method = \'logseries\'\n\n    def test_logseries(self):\n        self.generate(p=self.p, size=(3, 2))\n\n    # TODO(kataoka): add distribution test\n\n\n@testing.gpu\n@testing.parameterize(*[\n    {\'args\': ([0., 0.], [[1., 0.], [0., 1.]]), \'size\': None, \'tol\': 1e-6},\n    {\'args\': ([10., 10.], [[20., 10.], [10., 20.]]),\n     \'size\': None, \'tol\': 1e-6},\n    {\'args\': ([0., 0.], [[1., 0.], [0., 1.]]), \'size\': 10, \'tol\': 1e-6},\n    {\'args\': ([0., 0.], [[1., 0.], [0., 1.]]), \'size\': (1, 2, 3), \'tol\': 1e-6},\n    {\'args\': ([0., 0.], [[1., 0.], [0., 1.]]), \'size\': 3, \'tol\': 1e-6},\n    {\'args\': ([0., 0.], [[1., 0.], [0., 1.]]), \'size\': (3, 3), \'tol\': 1e-6},\n    {\'args\': ([0., 0.], [[1., 0.], [0., 1.]]), \'size\': (), \'tol\': 1e-6},\n])\n@testing.fix_random()\nclass TestMultivariateNormal(RandomGeneratorTestCase):\n\n    target_method = \'multivariate_normal\'\n\n    def check_multivariate_normal(self, dtype):\n        vals = self.generate_many(\n            mean=self.args[0], cov=self.args[1], size=self.size, tol=self.tol,\n            dtype=dtype, _count=10)\n\n        shape = core.get_size(self.size)\n        for val in vals:\n            assert isinstance(val, cupy.ndarray)\n            assert val.dtype == dtype\n            assert val.shape == shape + (2,)\n\n    def test_multivariate_normal_float32(self):\n        self.check_multivariate_normal(numpy.float32)\n\n    def test_multivariate_normal_float64(self):\n        self.check_multivariate_normal(numpy.float64)\n\n    # TODO(kataoka): add distribution test\n\n\n@testing.parameterize(\n    {\'n\': 5, \'p\': 0.5},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestNegativeBinomial(RandomGeneratorTestCase):\n    target_method = \'negative_binomial\'\n\n    def test_negative_binomial(self):\n        self.generate(n=self.n, p=self.p, size=(3, 2))\n\n    # TODO(kataoka): add distribution test\n\n\n@testing.parameterize(\n    {\'df\': 1.5, \'nonc\': 2.0},\n    {\'df\': 2.0, \'nonc\': 0.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestNoncentralChisquare(RandomGeneratorTestCase):\n\n    target_method = \'noncentral_chisquare\'\n\n    def test_noncentral_chisquare(self):\n        self.generate(df=self.df, nonc=self.nonc, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_noncentral_chisquare_ks(self, dtype):\n        self.check_ks(0.05)(\n            self.df, self.nonc, size=2000, dtype=dtype)\n\n\n@testing.parameterize(\n    {\'dfnum\': 2.0, \'dfden\': 3.0, \'nonc\': 4.0},\n    {\'dfnum\': 2.5, \'dfden\': 1.5, \'nonc\': 0.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestNoncentralF(RandomGeneratorTestCase):\n\n    target_method = \'noncentral_f\'\n\n    def test_noncentral_f(self):\n        self.generate(\n            dfnum=self.dfnum, dfden=self.dfden, nonc=self.nonc, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_noncentral_f_ks(self, dtype):\n        self.check_ks(0.05)(\n            self.dfnum, self.dfden, self.nonc, size=2000, dtype=dtype)\n\n\n@testing.gpu\n@testing.parameterize(*[\n    {\'args\': (0.0, 1.0), \'size\': None},\n    {\'args\': (10.0, 20.0), \'size\': None},\n    {\'args\': (0.0, 1.0), \'size\': 10},\n    {\'args\': (0.0, 1.0), \'size\': (1, 2, 3)},\n    {\'args\': (0.0, 1.0), \'size\': 3},\n    {\'args\': (0.0, 1.0), \'size\': (3, 3)},\n    {\'args\': (0.0, 1.0), \'size\': ()},\n])\n@testing.fix_random()\nclass TestNormal(RandomGeneratorTestCase):\n\n    target_method = \'normal\'\n\n    def check_normal(self, dtype):\n        vals = self.generate_many(\n            self.args[0], self.args[1], self.size, dtype, _count=10)\n\n        shape = core.get_size(self.size)\n        for val in vals:\n            assert isinstance(val, cupy.ndarray)\n            assert val.dtype == dtype\n            assert val.shape == shape\n\n    def test_normal_float32(self):\n        self.check_normal(numpy.float32)\n\n    def test_normal_float64(self):\n        self.check_normal(numpy.float64)\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_normal_ks(self, dtype):\n        self.check_ks(0.05)(\n            *self.args, size=self.size, dtype=dtype)\n\n\n@testing.parameterize(\n    {\'a\': 1.0},\n    {\'a\': 3.0},\n    {\'a\': 10.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestPareto(RandomGeneratorTestCase):\n\n    target_method = \'pareto\'\n\n    def test_pareto(self):\n        self.generate(a=self.a, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_pareto_ks(self, dtype):\n        self.check_ks(0.05)(\n            a=self.a, size=2000, dtype=dtype)\n\n\n@testing.parameterize(\n    {\'lam\': 1.0},\n    {\'lam\': 3.0},\n    {\'lam\': 10.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestPoisson(RandomGeneratorTestCase):\n\n    target_method = \'poisson\'\n\n    def test_poisson(self):\n        self.generate(lam=self.lam, size=(3, 2))\n\n    # TODO(kataoka): add distribution test\n\n\n@testing.parameterize(\n    {\'df\': 1.0},\n    {\'df\': 3.0},\n    {\'df\': 10.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestStandardT(RandomGeneratorTestCase):\n\n    target_method = \'standard_t\'\n\n    def test_standard_t(self):\n        self.generate(df=self.df, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_standard_t_ks(self, dtype):\n        self.check_ks(0.05)(\n            df=self.df, size=2000, dtype=dtype)\n\n\n@testing.gpu\n@testing.parameterize(*[\n    {\'size\': None},\n    {\'size\': 10},\n    {\'size\': (1, 2, 3)},\n    {\'size\': 3},\n    {\'size\': ()},\n])\n@testing.fix_random()\nclass TestRandomSample(unittest.TestCase):\n\n    def setUp(self):\n        self.rs = generator.RandomState(seed=testing.generate_seed())\n\n    def check_random_sample(self, dtype):\n        vals = [self.rs.random_sample(self.size, dtype) for _ in range(10)]\n\n        shape = core.get_size(self.size)\n        for val in vals:\n            assert isinstance(val, cupy.ndarray)\n            assert val.dtype == dtype\n            assert val.shape == shape\n            assert (0 <= val).all()\n            assert (val < 1).all()\n\n    def test_random_sample_float32(self):\n        self.check_random_sample(numpy.float32)\n\n    def test_random_sample_float64(self):\n        self.check_random_sample(numpy.float64)\n\n\n@testing.fix_random()\nclass TestRandomSampleDistrib(unittest.TestCase):\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    @numpy_cupy_equal_continuous_distribution(0.05)\n    def test_random_sample_ks(self, xp, dtype):\n        return _xp_random(xp, \'random_sample\')(size=2000, dtype=dtype)\n\n\n@testing.fix_random()\n@testing.gpu\nclass TestRandAndRandN(unittest.TestCase):\n\n    def setUp(self):\n        self.rs = generator.RandomState(seed=testing.generate_seed())\n\n    def test_rand_invalid_argument(self):\n        with self.assertRaises(TypeError):\n            self.rs.rand(1, 2, 3, unnecessary=\'unnecessary_argument\')\n\n    def test_randn_invalid_argument(self):\n        with self.assertRaises(TypeError):\n            self.rs.randn(1, 2, 3, unnecessary=\'unnecessary_argument\')\n\n\n@testing.parameterize(\n    {\'a\': 0.5},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestPower(RandomGeneratorTestCase):\n\n    target_method = \'power\'\n\n    def test_power(self):\n        self.generate(a=self.a, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_power_ks(self, dtype):\n        self.check_ks(0.05)(\n            a=self.a, size=2000, dtype=dtype)\n\n\n@testing.parameterize(\n    {\'scale\': 1.0},\n    {\'scale\': 3.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestRayleigh(RandomGeneratorTestCase):\n\n    target_method = \'rayleigh\'\n\n    def test_rayleigh(self):\n        self.generate(scale=self.scale, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_rayleigh_ks(self, dtype):\n        self.check_ks(0.05)(\n            scale=self.scale, size=2000, dtype=dtype)\n\n\n@testing.gpu\n@testing.fix_random()\nclass TestStandardCauchy(RandomGeneratorTestCase):\n\n    target_method = \'standard_cauchy\'\n\n    def test_standard_cauchy(self):\n        self.generate(size=(3, 2))\n\n    @attr.slow\n    @condition.repeat(10)\n    def test_standard_cauchy_isfinite(self):\n        x = self.generate(size=10**7)\n        self.assertTrue(cupy.isfinite(x).all())\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_standard_cauchy_ks(self, dtype):\n        self.check_ks(0.05)(\n            size=2000, dtype=dtype)\n\n\n@testing.parameterize(\n    {\'shape\': 0.5},\n    {\'shape\': 1.0},\n    {\'shape\': 3.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestStandardGamma(RandomGeneratorTestCase):\n\n    target_method = \'standard_gamma\'\n\n    def test_standard_gamma(self):\n        self.generate(shape=self.shape, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_standard_gamma_ks(self, dtype):\n        self.check_ks(0.05)(\n            shape=self.shape, size=2000, dtype=dtype)\n\n\n@testing.fix_random()\n@testing.gpu\nclass TestInterval(RandomGeneratorTestCase):\n\n    target_method = \'_interval\'\n\n    def test_zero(self):\n        shape = (2, 3)\n        vals = self.generate_many(0, shape, _count=10)\n        for val in vals:\n            assert isinstance(val, cupy.ndarray)\n            assert val.dtype.kind in \'iu\'\n            assert val.shape == shape\n            assert (val == 0).all()\n\n    def test_shape_zero(self):\n        mx = 10\n        vals = self.generate_many(mx, None, _count=10)\n        for val in vals:\n            assert isinstance(val, cupy.ndarray)\n            assert val.dtype.kind in \'iu\'\n            assert val.shape == ()\n            assert (0 <= val).all()\n            assert (val <= mx).all()\n        # TODO(niboshi): Distribution test\n\n    def test_shape_one_dim(self):\n        mx = 10\n        size = 20\n        vals = self.generate_many(mx, size, _count=10)\n        for val in vals:\n            assert isinstance(val, cupy.ndarray)\n            assert val.dtype.kind in \'iu\'\n            assert val.shape == (size,)\n            assert (0 <= val).all()\n            assert (val <= mx).all()\n        # TODO(niboshi): Distribution test\n\n    def test_shape_multi_dim(self):\n        mx = 10\n        shape = (1, 2)\n        vals = self.generate_many(mx, shape, _count=10)\n        for val in vals:\n            assert isinstance(val, cupy.ndarray)\n            assert val.dtype.kind in \'iu\'\n            assert val.shape == shape\n            assert (0 <= val).all()\n            assert (val <= mx).all()\n        # TODO(niboshi): Distribution test\n\n    def test_bound_1(self):\n        vals = self.generate_many(10, (2, 3), _count=10)\n        for val in vals:\n            assert isinstance(val, cupy.ndarray)\n            assert val.dtype.kind in \'iu\'\n            assert val.shape == (2, 3)\n            assert (0 <= val).all()\n            assert (val <= 10).all()\n\n    def test_bound_2(self):\n        vals = self.generate_many(2, None, _count=20)\n        for val in vals:\n            assert isinstance(val, cupy.ndarray)\n            assert val.dtype.kind in \'iu\'\n            assert val.shape == ()\n            assert (0 <= val).all()\n            assert (val <= 2).all()\n\n    @condition.repeat(3, 10)\n    def test_goodness_of_fit(self):\n        mx = 5\n        trial = 100\n        vals = self.generate_many(mx, None, _count=trial)\n        vals = [val.get() for val in vals]\n        counts = numpy.histogram(vals, bins=numpy.arange(mx + 2))[0]\n        expected = numpy.array([float(trial) / (mx + 1)] * (mx + 1))\n        self.assertTrue(hypothesis.chi_square_test(counts, expected))\n\n    @condition.repeat(3)\n    def test_goodness_of_fit_2(self):\n        mx = 5\n        vals = self.generate(mx, (5, 5)).get()\n        counts = numpy.histogram(vals, bins=numpy.arange(mx + 2))[0]\n        expected = numpy.array([float(vals.size) / (mx + 1)] * (mx + 1))\n        self.assertTrue(hypothesis.chi_square_test(counts, expected))\n\n\n@testing.fix_random()\n@testing.gpu\nclass TestTomaxint(RandomGeneratorTestCase):\n\n    target_method = \'tomaxint\'\n\n    def test_tomaxint_none(self):\n        x = self.generate()\n        self.assertEqual(x.shape, ())\n        self.assertTrue((0 <= x).all())\n        self.assertTrue((x <= cupy.iinfo(cupy.int_).max).all())\n\n    def test_tomaxint_int(self):\n        x = self.generate(3)\n        self.assertEqual(x.shape, (3,))\n        self.assertTrue((0 <= x).all())\n        self.assertTrue((x <= cupy.iinfo(cupy.int_).max).all())\n\n    def test_tomaxint_tuple(self):\n        x = self.generate((2, 3))\n        self.assertEqual(x.shape, (2, 3))\n        self.assertTrue((0 <= x).all())\n        self.assertTrue((x <= cupy.iinfo(cupy.int_).max).all())\n\n\n@testing.parameterize(\n    {\'a\': 3, \'size\': 2, \'p\': None},\n    {\'a\': 3, \'size\': 2, \'p\': [0.3, 0.3, 0.4]},\n    {\'a\': 3, \'size\': (5, 5), \'p\': [0.3, 0.3, 0.4]},\n    {\'a\': 3, \'size\': (5, 5), \'p\': numpy.array([0.3, 0.3, 0.4])},\n    {\'a\': 3, \'size\': (), \'p\': None},\n    {\'a\': numpy.array([0.0, 1.0, 2.0]), \'size\': 2, \'p\': [0.3, 0.3, 0.4]},\n)\n@testing.fix_random()\n@testing.gpu\nclass TestChoice1(RandomGeneratorTestCase):\n\n    target_method = \'choice\'\n\n    def test_dtype_shape(self):\n        v = self.generate(a=self.a, size=self.size, p=self.p)\n        if isinstance(self.size, int):\n            expected_shape = (self.size,)\n        else:\n            expected_shape = self.size\n        if isinstance(self.a, numpy.ndarray):\n            expected_dtype = \'float\'\n        else:\n            expected_dtype = \'int64\'\n        self.assertEqual(v.dtype, expected_dtype)\n        self.assertEqual(v.shape, expected_shape)\n\n    @condition.repeat(3, 10)\n    def test_bound(self):\n        vals = self.generate_many(\n            a=self.a, size=self.size, p=self.p, _count=20)\n        vals = [val.get() for val in vals]\n        size_ = self.size if isinstance(self.size, tuple) else (self.size,)\n        for val in vals:\n            self.assertEqual(val.shape, size_)\n        self.assertEqual(min(val.min() for val in vals), 0)\n        self.assertEqual(max(val.max() for val in vals), 2)\n\n\n@testing.parameterize(\n    {\'a\': [0, 1, 2], \'size\': 2, \'p\': [0.3, 0.3, 0.4]},\n)\n@testing.fix_random()\n@testing.gpu\nclass TestChoice2(RandomGeneratorTestCase):\n\n    target_method = \'choice\'\n\n    def test_dtype_shape(self):\n        v = self.generate(a=self.a, size=self.size, p=self.p)\n        if isinstance(self.size, int):\n            expected_shape = (self.size,)\n        else:\n            expected_shape = self.size\n        if isinstance(self.a, numpy.ndarray):\n            expected_dtype = \'float\'\n        else:\n            expected_dtype = \'int\'\n        self.assertEqual(v.dtype, expected_dtype)\n        self.assertEqual(v.shape, expected_shape)\n\n    @condition.repeat(3, 10)\n    def test_bound(self):\n        vals = self.generate_many(\n            a=self.a, size=self.size, p=self.p, _count=20)\n        vals = [val.get() for val in vals]\n        size_ = self.size if isinstance(self.size, tuple) else (self.size,)\n        for val in vals:\n            self.assertEqual(val.shape, size_)\n        self.assertEqual(min(val.min() for val in vals), 0)\n        self.assertEqual(max(val.max() for val in vals), 2)\n\n\n@testing.fix_random()\n@testing.gpu\nclass TestChoiceChi(RandomGeneratorTestCase):\n\n    target_method = \'choice\'\n\n    @condition.repeat(3, 10)\n    def test_goodness_of_fit(self):\n        trial = 100\n        vals = self.generate_many(3, 1, True, [0.3, 0.3, 0.4], _count=trial)\n        vals = [val.get() for val in vals]\n        counts = numpy.histogram(vals, bins=numpy.arange(4))[0]\n        expected = numpy.array([30, 30, 40])\n        self.assertTrue(hypothesis.chi_square_test(counts, expected))\n\n    @condition.repeat(3, 10)\n    def test_goodness_of_fit_2(self):\n        vals = self.generate(3, (5, 20), True, [0.3, 0.3, 0.4]).get()\n        counts = numpy.histogram(vals, bins=numpy.arange(4))[0]\n        expected = numpy.array([30, 30, 40])\n        self.assertTrue(hypothesis.chi_square_test(counts, expected))\n\n\n@testing.fix_random()\n@testing.gpu\nclass TestChoiceMultinomial(unittest.TestCase):\n\n    @condition.repeat(3, 10)\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose(atol=0.02)\n    def test_choice_multinomial(self, xp, dtype):\n        p = xp.array([0.5, 0.25, 0.125, 0.125], dtype)\n        trial = 10000\n        x = xp.random.choice(len(p), trial, p=p)\n        y = xp.bincount(x).astype(\'f\') / trial\n        return y\n\n\n@testing.parameterize(\n    {\'a\': 3.1, \'size\': 1, \'p\': [0.1, 0.1, 0.8]},\n    {\'a\': None, \'size\': 1, \'p\': [0.1, 0.1, 0.8]},\n    {\'a\': -3, \'size\': 1, \'p\': [0.1, 0.1, 0.8]},\n    {\'a\': [[0, 1], [2]], \'size\': 1, \'p\': [0.1, 0.1, 0.8]},\n    {\'a\': [], \'size\': 1, \'p\': [0.1, 0.1, 0.8]},\n    {\'a\': 3, \'size\': 1, \'p\': [[0.1, 0.1], [0.8]]},\n    {\'a\': 2, \'size\': 1, \'p\': [0.1, 0.1, 0.8]},\n    {\'a\': 3, \'size\': 1, \'p\': [-0.1, 0.3, 0.8]},\n    {\'a\': 3, \'size\': 1, \'p\': [0.1, 0.1, 0.7]},\n)\n@testing.fix_random()\n@testing.gpu\nclass TestChoiceFailure(unittest.TestCase):\n\n    def setUp(self):\n        self.rs = generator.RandomState(seed=testing.generate_seed())\n\n    def test_choice_invalid_value(self):\n        with self.assertRaises(ValueError):\n            self.rs.choice(a=self.a, size=self.size, p=self.p)\n\n\n@testing.parameterize(\n    {\'a\': 5, \'size\': 2},\n    {\'a\': 5, \'size\': (2, 2)},\n    {\'a\': 5, \'size\': ()},\n    {\'a\': numpy.array([0.0, 2.0, 4.0]), \'size\': 2},\n)\n@testing.fix_random()\n@testing.gpu\nclass TestChoiceReplaceFalse(RandomGeneratorTestCase):\n\n    target_method = \'choice\'\n\n    def test_dtype_shape(self):\n        v = self.generate(a=self.a, size=self.size, replace=False)\n        if isinstance(self.size, int):\n            expected_shape = (self.size,)\n        else:\n            expected_shape = self.size\n        if isinstance(self.a, numpy.ndarray):\n            expected_dtype = \'float\'\n        else:\n            expected_dtype = \'int\'\n        self.assertEqual(v.dtype, expected_dtype)\n        self.assertEqual(v.shape, expected_shape)\n\n    @condition.repeat(3, 10)\n    def test_bound(self):\n        val = self.generate(a=self.a, size=self.size, replace=False).get()\n        size = self.size if isinstance(self.size, tuple) else (self.size,)\n        self.assertEqual(val.shape, size)\n        self.assertTrue((0 <= val).all())\n        self.assertTrue((val < 5).all())\n        val = numpy.asarray(val)\n        self.assertEqual(numpy.unique(val).size, val.size)\n\n\n@testing.gpu\n@testing.fix_random()\nclass TestGumbel(RandomGeneratorTestCase):\n\n    target_method = \'gumbel\'\n\n    def test_gumbel_1(self):\n        self.generate()\n\n    def test_gumbel_2(self):\n        self.generate(0.0, 1.0, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_gumbel_ks_1(self, dtype):\n        self.check_ks(0.05)(\n            size=2000, dtype=dtype)\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_gumbel_ks_2(self, dtype):\n        self.check_ks(0.05)(\n            2.3, 4.5, size=2000, dtype=dtype)\n\n\n@testing.gpu\n@testing.fix_random()\nclass TestRandint(RandomGeneratorTestCase):\n    # TODO(niboshi):\n    #   Test soundness of distribution.\n    #   Currently only reprocibility is checked.\n\n    target_method = \'randint\'\n\n    def test_randint_1(self):\n        self.generate(3)\n\n    def test_randint_2(self):\n        self.generate(3, 4, size=(3, 2))\n\n    def test_randint_empty1(self):\n        self.generate(3, 10, size=0)\n\n    def test_randint_empty2(self):\n        self.generate(3, size=(4, 0, 5))\n\n    def test_randint_overflow(self):\n        self.generate(numpy.int8(-100), numpy.int8(100))\n\n    def test_randint_float1(self):\n        self.generate(-1.2, 3.4, 5)\n\n    def test_randint_float2(self):\n        self.generate(6.7, size=(2, 3))\n\n    def test_randint_int64_1(self):\n        self.generate(2**34, 2**40, 3)\n\n\n@testing.gpu\n@testing.fix_random()\nclass TestUniform(RandomGeneratorTestCase):\n\n    target_method = \'uniform\'\n\n    def test_uniform_1(self):\n        self.generate()\n\n    def test_uniform_2(self):\n        self.generate(-4.2, 2.4, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_uniform_ks_1(self, dtype):\n        self.check_ks(0.05)(\n            size=2000, dtype=dtype)\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_uniform_ks_2(self, dtype):\n        self.check_ks(0.05)(\n            -4.2, 2.4, size=2000, dtype=dtype)\n\n\n@testing.parameterize(\n    {\'mu\': 0.0, \'kappa\': 1.0},\n    {\'mu\': 3.0, \'kappa\': 3.0},\n    {\'mu\': 3.0, \'kappa\': 1.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestVonmises(RandomGeneratorTestCase):\n\n    target_method = \'vonmises\'\n\n    def test_vonmises(self):\n        self.generate(mu=self.mu, kappa=self.kappa, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_vonmises_ks(self, dtype):\n        self.check_ks(0.05)(\n            self.mu, self.kappa, size=2000, dtype=dtype)\n\n\n@testing.parameterize(\n    {\'mean\': 1.0, \'scale\': 3.0},\n    {\'mean\': 3.0, \'scale\': 3.0},\n    {\'mean\': 3.0, \'scale\': 1.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestWald(RandomGeneratorTestCase):\n\n    target_method = \'wald\'\n\n    def test_wald(self):\n        self.generate(mean=self.mean, scale=self.scale, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_wald_ks(self, dtype):\n        self.check_ks(0.05)(\n            self.mean, self.scale, size=2000, dtype=dtype)\n\n\n@testing.parameterize(\n    {\'a\': 0.5},\n    {\'a\': 1.0},\n    {\'a\': 3.0},\n    {\'a\': numpy.inf},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestWeibull(RandomGeneratorTestCase):\n\n    target_method = \'weibull\'\n\n    def test_weibull(self):\n        self.generate(a=self.a, size=(3, 2))\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_weibull_ks(self, dtype):\n        self.check_ks(0.05)(\n            a=self.a, size=2000, dtype=dtype)\n\n\n@testing.parameterize(\n    {\'a\': 2.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestZipf(RandomGeneratorTestCase):\n\n    target_method = \'zipf\'\n\n    def test_zipf(self):\n        self.generate(a=self.a, size=(3, 2))\n\n    # TODO(kataoka): add distribution test\n\n\n@testing.parameterize(\n    {\'a\': 3, \'size\': 5},\n    {\'a\': [1, 2, 3], \'size\': 5},\n)\n@testing.fix_random()\n@testing.gpu\nclass TestChoiceReplaceFalseFailure(unittest.TestCase):\n\n    def test_choice_invalid_value(self):\n        for xp in (numpy, cupy):\n            rs = xp.random.RandomState(seed=testing.generate_seed())\n            with pytest.raises(ValueError):\n                rs.choice(a=self.a, size=self.size, replace=False)\n\n\nclass TestResetStates(unittest.TestCase):\n\n    def test_reset_states(self):\n        generator._random_states = \'dummy\'\n        generator.reset_states()\n        self.assertEqual({}, generator._random_states)\n\n\n@testing.gpu\nclass TestGetRandomState(unittest.TestCase):\n\n    def setUp(self):\n        self.device_id = cuda.Device().id\n        self.rs_tmp = generator._random_states\n\n    def tearDown(self, *args):\n        generator._random_states = self.rs_tmp\n\n    def test_get_random_state_initialize(self):\n        generator._random_states = {}\n        rs = generator.get_random_state()\n        self.assertEqual(generator._random_states[self.device_id], rs)\n\n    def test_get_random_state_memoized(self):\n        generator._random_states = {self.device_id: \'expected\',\n                                    self.device_id + 1: \'dummy\'}\n        rs = generator.get_random_state()\n        self.assertEqual(\'expected\', generator._random_states[self.device_id])\n        self.assertEqual(\'dummy\', generator._random_states[self.device_id + 1])\n        self.assertEqual(\'expected\', rs)\n\n\n@testing.gpu\nclass TestSetRandomState(unittest.TestCase):\n\n    def setUp(self):\n        self.rs_tmp = generator._random_states\n\n    def tearDown(self, *args):\n        generator._random_states = self.rs_tmp\n\n    def test_set_random_state(self):\n        rs = generator.RandomState()\n        generator.set_random_state(rs)\n        assert generator.get_random_state() is rs\n\n    def test_set_random_state_call_multiple_times(self):\n        generator.set_random_state(generator.RandomState())\n        rs = generator.RandomState()\n        generator.set_random_state(rs)\n        assert generator.get_random_state() is rs\n\n\n@testing.gpu\n@testing.fix_random()\nclass TestStandardExponential(RandomGeneratorTestCase):\n\n    target_method = \'standard_exponential\'\n\n    def test_standard_exponential(self):\n        self.generate(size=(3, 2))\n\n    @attr.slow\n    @condition.repeat(10)\n    def test_standard_exponential_isfinite(self):\n        x = self.generate(size=10**7)\n        self.assertTrue(cupy.isfinite(x).all())\n\n    @testing.for_dtypes(\'fd\')\n    @condition.repeat_with_success_at_least(10, 3)\n    def test_standard_exponential_ks(self, dtype):\n        self.check_ks(0.05)(\n            size=2000, dtype=dtype)\n\n\n@testing.parameterize(\n    {\'left\': -1.0, \'mode\': 0.0, \'right\': 2.0},\n)\n@testing.gpu\n@testing.fix_random()\nclass TestTriangular(RandomGeneratorTestCase):\n\n    target_method = \'triangular\'\n\n    def test_triangular(self):\n        self.generate(\n            left=self.left, mode=self.mode, right=self.right, size=(3, 2))\n\n\n@testing.gpu\nclass TestRandomStateThreadSafe(unittest.TestCase):\n\n    def setUp(self):\n        cupy.random.reset_states()\n\n    def test_get_random_state_thread_safe(self):\n        seed = 10\n        threads = [\n            threading.Thread(target=lambda: cupy.random.seed(seed)),\n            threading.Thread(target=lambda: cupy.random.get_random_state()),\n            threading.Thread(target=lambda: cupy.random.get_random_state()),\n            threading.Thread(target=lambda: cupy.random.get_random_state()),\n            threading.Thread(target=lambda: cupy.random.get_random_state()),\n            threading.Thread(target=lambda: cupy.random.get_random_state()),\n            threading.Thread(target=lambda: cupy.random.get_random_state()),\n        ]\n\n        for t in threads:\n            t.start()\n        for t in threads:\n            t.join()\n\n        actual = cupy.random.uniform()\n        cupy.random.seed(seed)\n        expected = cupy.random.uniform()\n        self.assertEqual(actual, expected)\n\n    def test_set_random_state_thread_safe(self):\n        rs = cupy.random.RandomState()\n        threads = [\n            threading.Thread(target=lambda: cupy.random.set_random_state(rs)),\n            threading.Thread(target=lambda: cupy.random.set_random_state(rs)),\n        ]\n\n        for t in threads:\n            t.start()\n        for t in threads:\n            t.join()\n\n        assert cupy.random.get_random_state() is rs\n\n\n@testing.gpu\nclass TestGetRandomState2(unittest.TestCase):\n\n    def setUp(self):\n        self.rs_dict = generator._random_states\n        generator._random_states = {}\n        self.cupy_seed = os.getenv(\'CUPY_SEED\')\n        self.chainer_seed = os.getenv(\'CHAINER_SEED\')\n\n    def tearDown(self, *args):\n        generator._random_states = self.rs_dict\n        if self.cupy_seed is None:\n            os.environ.pop(\'CUPY_SEED\', None)\n        else:\n            os.environ[\'CUPY_SEED\'] = self.cupy_seed\n        if self.chainer_seed is None:\n            os.environ.pop(\'CHAINER_SEED\', None)\n        else:\n            os.environ[\'CHAINER_SEED\'] = self.chainer_seed\n\n    def test_get_random_state_no_cupy_no_chainer_seed(self):\n        os.environ.pop(\'CUPY_SEED\', None)\n        os.environ.pop(\'CHAINER_SEED\', None)\n        rvs0 = self._get_rvs_reset()\n        rvs1 = self._get_rvs_reset()\n\n        self._check_different(rvs0, rvs1)\n\n    def test_get_random_state_no_cupy_with_chainer_seed(self):\n        rvs0 = self._get_rvs(generator.RandomState(5))\n\n        os.environ.pop(\'CUPY_SEED\', None)\n        os.environ[\'CHAINER_SEED\'] = \'5\'\n        rvs1 = self._get_rvs_reset()\n\n        self._check_same(rvs0, rvs1)\n\n    def test_get_random_state_with_cupy_no_chainer_seed(self):\n        rvs0 = self._get_rvs(generator.RandomState(6))\n\n        os.environ[\'CUPY_SEED\'] = \'6\'\n        os.environ.pop(\'CHAINER_SEED\', None)\n        rvs1 = self._get_rvs_reset()\n\n        self._check_same(rvs0, rvs1)\n\n    def test_get_random_state_with_cupy_with_chainer_seed(self):\n        rvs0 = self._get_rvs(generator.RandomState(7))\n\n        os.environ[\'CUPY_SEED\'] = \'7\'\n        os.environ[\'CHAINER_SEED\'] = \'8\'\n        rvs1 = self._get_rvs_reset()\n\n        self._check_same(rvs0, rvs1)\n\n    def _get_rvs(self, rs):\n        rvu = rs.rand(4)\n        rvn = rs.randn(4)\n        return rvu, rvn\n\n    def _get_rvs_reset(self):\n        generator.reset_states()\n        return self._get_rvs(generator.get_random_state())\n\n    def _check_same(self, rvs0, rvs1):\n        for rv0, rv1 in zip(rvs0, rvs1):\n            testing.assert_array_equal(rv0, rv1)\n\n    def _check_different(self, rvs0, rvs1):\n        for rv0, rv1 in zip(rvs0, rvs1):\n            for r0, r1 in zip(rv0, rv1):\n                self.assertNotEqual(r0, r1)\n\n\nclass TestCheckAndGetDtype(unittest.TestCase):\n\n    @testing.for_float_dtypes(no_float16=True)\n    def test_float32_64_type(self, dtype):\n        self.assertEqual(generator._check_and_get_dtype(dtype),\n                         numpy.dtype(dtype))\n\n    def test_float16(self):\n        with self.assertRaises(TypeError):\n            generator._check_and_get_dtype(numpy.float16)\n\n    @testing.for_int_dtypes()\n    def test_int_type(self, dtype):\n        with self.assertRaises(TypeError):\n            generator._check_and_get_dtype(dtype)\n'"
tests/cupy_tests/random_tests/test_permutations.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\nfrom cupy.testing import condition\n\n\n@testing.parameterize(\n    {'seed': None},\n    {'seed': 0},\n)\n@testing.gpu\nclass TestPermutations(unittest.TestCase):\n\n    def _xp_random(self, xp):\n        if self.seed is None:\n            return xp.random\n        else:\n            return xp.random.RandomState(seed=self.seed)\n\n    # Test ranks\n\n    # TODO(niboshi): Fix xfail\n    @pytest.mark.xfail(reason='Explicit error types required')\n    def test_permutation_zero_dim(self):\n        for xp in (numpy, cupy):\n            xp_random = self._xp_random(xp)\n            a = testing.shaped_random((), xp)\n            with pytest.raises(IndexError):\n                xp_random.permutation(a)\n\n    # Test same values\n\n    @testing.for_all_dtypes(no_float16=True, no_bool=True, no_complex=True)\n    def test_permutation_sort_1dim(self, dtype):\n        cupy_random = self._xp_random(cupy)\n        a = cupy.arange(10, dtype=dtype)\n        b = cupy.copy(a)\n        c = cupy_random.permutation(a)\n        testing.assert_allclose(a, b)\n        testing.assert_allclose(b, cupy.sort(c))\n\n    @testing.for_all_dtypes(no_float16=True, no_bool=True, no_complex=True)\n    def test_permutation_sort_ndim(self, dtype):\n        cupy_random = self._xp_random(cupy)\n        a = cupy.arange(15, dtype=dtype).reshape(5, 3)\n        b = cupy.copy(a)\n        c = cupy_random.permutation(a)\n        testing.assert_allclose(a, b)\n        testing.assert_allclose(b, cupy.sort(c, axis=0))\n\n    # Test seed\n\n    @testing.for_all_dtypes()\n    def test_permutation_seed1(self, dtype):\n        a = testing.shaped_random((10,), cupy, dtype)\n        b = cupy.copy(a)\n\n        cupy_random = self._xp_random(cupy)\n        if self.seed is None:\n            cupy_random.seed(0)\n        pa = cupy_random.permutation(a)\n        cupy_random = self._xp_random(cupy)\n        if self.seed is None:\n            cupy_random.seed(0)\n        pb = cupy_random.permutation(b)\n\n        testing.assert_allclose(pa, pb)\n\n\n@testing.gpu\nclass TestShuffle(unittest.TestCase):\n\n    # Test ranks\n\n    def test_shuffle_zero_dim(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((), xp)\n            with pytest.raises(TypeError):\n                xp.random.shuffle(a)\n\n    # Test same values\n\n    @testing.for_all_dtypes(no_float16=True, no_bool=True, no_complex=True)\n    def test_shuffle_sort_1dim(self, dtype):\n        a = cupy.arange(10, dtype=dtype)\n        b = cupy.copy(a)\n        cupy.random.shuffle(a)\n        testing.assert_allclose(cupy.sort(a), b)\n\n    @testing.for_all_dtypes(no_float16=True, no_bool=True, no_complex=True)\n    def test_shuffle_sort_ndim(self, dtype):\n        a = cupy.arange(15, dtype=dtype).reshape(5, 3)\n        b = cupy.copy(a)\n        cupy.random.shuffle(a)\n        testing.assert_allclose(cupy.sort(a, axis=0), b)\n\n    # Test seed\n\n    @testing.for_all_dtypes()\n    def test_shuffle_seed1(self, dtype):\n        a = testing.shaped_random((10,), cupy, dtype)\n        b = cupy.copy(a)\n        cupy.random.seed(0)\n        cupy.random.shuffle(a)\n        cupy.random.seed(0)\n        cupy.random.shuffle(b)\n        testing.assert_allclose(a, b)\n\n\n@testing.parameterize(*(testing.product({\n    'num': [0, 1, 100, 1000, 10000, 100000],\n})))\n@testing.gpu\nclass TestPermutationSoundness(unittest.TestCase):\n\n    def setUp(self):\n        a = cupy.random.permutation(self.num)\n        self.a = a.get()\n\n    # Test soundness\n\n    @condition.repeat(3)\n    def test_permutation_soundness(self):\n        assert(numpy.sort(self.a) == numpy.arange(self.num)).all()\n\n\n@testing.parameterize(*(testing.product({\n    'offset': [0, 17, 34, 51],\n    'gap': [1, 2, 3, 5, 7],\n    'mask': [1, 2, 4, 8, 16, 32, 64, 128],\n})))\n@testing.gpu\nclass TestPermutationRandomness(unittest.TestCase):\n\n    num = 256\n\n    def setUp(self):\n        a = cupy.random.permutation(self.num)\n        self.a = a.get()\n        self.num_half = int(self.num / 2)\n\n    # Simple bit proportion test\n\n    # This test is to check kind of randomness of permutation.\n    # An intuition behind this test is that, when you make a sub-array\n    # by regularly extracting half elements from the permuted array,\n    # the sub-array should also hold randomeness and accordingly\n    # frequency of appearance of 0 and 1 at each bit position of\n    # whole elements in the sub-array should become similar\n    # when elements count of original array is 2^N.\n    # Note that this is not an establishd method to check randomness.\n    # TODO(anaruse): implement randomness check using some established methods.\n    @condition.repeat_with_success_at_least(5, 3)\n    def test_permutation_randomness(self):\n        if self.mask > self.num_half:\n            return\n        index = numpy.arange(self.num_half)\n        index = (index * self.gap + self.offset) % self.num\n        samples = self.a[index]\n        ret = (samples & self.mask > 0)\n        count = numpy.count_nonzero(ret)  # expectation: self.num_half / 2\n        if count > self.num_half - count:\n            count = self.num_half - count\n        prob_le_count = self._calc_probability(count)\n        if prob_le_count < 0.001:\n            raise\n\n    def _calc_probability(self, count):\n        comb_all = self._comb(self.num, self.num_half)\n        comb_le_count = 0\n        for i in range(count + 1):\n            tmp = self._comb(self.num_half, i)\n            comb_i = tmp * tmp\n            comb_le_count += comb_i\n        prob = comb_le_count / comb_all\n        return prob\n\n    def _comb(self, N, k):\n        val = numpy.float64(1)\n        for i in range(k):\n            val *= (N - i) / (k - i)\n        return val\n"""
tests/cupy_tests/random_tests/test_random.py,0,"b'import unittest\n\nfrom cupy import random\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestResetSeed(unittest.TestCase):\n\n    @testing.for_float_dtypes(no_float16=True)\n    def test_reset_seed(self, dtype):\n        rs = random.get_random_state()\n        rs.seed(0)\n        l1 = rs.rand(10, dtype=dtype)\n\n        rs = random.get_random_state()\n        rs.seed(0)\n        l2 = rs.rand(10, dtype=dtype)\n\n        testing.assert_array_equal(l1, l2)\n'"
tests/cupy_tests/random_tests/test_sample.py,0,"b""import mock\nimport unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import cuda\nfrom cupy import random\nfrom cupy import testing\nfrom cupy.testing import condition\nfrom cupy.testing import hypothesis\n\n\n@testing.gpu\nclass TestRandint(unittest.TestCase):\n\n    def test_lo_hi_reversed(self):\n        with self.assertRaises(ValueError):\n            random.randint(100, 1)\n\n    def test_lo_hi_equal(self):\n        with self.assertRaises(ValueError):\n            random.randint(3, 3, size=0)\n\n        with self.assertRaises(ValueError):\n            # int(-0.2) is not less than int(0.3)\n            random.randint(-0.2, 0.3)\n\n    def test_lo_hi_nonrandom(self):\n        a = random.randint(-0.9, 1.1, size=3)\n        testing.assert_array_equal(a, cupy.full((3,), 0))\n\n        a = random.randint(-1.1, -0.9, size=(2, 2))\n        testing.assert_array_equal(a, cupy.full((2, 2), -1))\n\n    def test_zero_sizes(self):\n        a = random.randint(10, size=(0,))\n        testing.assert_array_equal(a, cupy.array(()))\n\n        a = random.randint(10, size=0)\n        testing.assert_array_equal(a, cupy.array(()))\n\n\n@testing.fix_random()\n@testing.gpu\nclass TestRandint2(unittest.TestCase):\n\n    @condition.repeat(3, 10)\n    def test_bound_1(self):\n        vals = [random.randint(0, 10, (2, 3)).get() for _ in range(10)]\n        for val in vals:\n            self.assertEqual(val.shape, (2, 3))\n        self.assertEqual(min(_.min() for _ in vals), 0)\n        self.assertEqual(max(_.max() for _ in vals), 9)\n\n    @condition.repeat(3, 10)\n    def test_bound_2(self):\n        vals = [random.randint(0, 2).get() for _ in range(20)]\n        for val in vals:\n            self.assertEqual(val.shape, ())\n        self.assertEqual(min(vals), 0)\n        self.assertEqual(max(vals), 1)\n\n    @condition.repeat(3, 10)\n    def test_bound_overflow(self):\n        # 100 - (-100) exceeds the range of int8\n        val = random.randint(numpy.int8(-100), numpy.int8(100), size=20).get()\n        self.assertEqual(val.shape, (20,))\n        self.assertGreaterEqual(val.min(), -100)\n        self.assertLess(val.max(), 100)\n\n    @condition.repeat(3, 10)\n    def test_bound_float1(self):\n        # generate floats s.t. int(low) < int(high)\n        low, high = sorted(numpy.random.uniform(-5, 5, size=2))\n        low -= 1\n        high += 1\n        vals = [random.randint(low, high, (2, 3)).get() for _ in range(10)]\n        for val in vals:\n            self.assertEqual(val.shape, (2, 3))\n        self.assertEqual(min(_.min() for _ in vals), int(low))\n        self.assertEqual(max(_.max() for _ in vals), int(high) - 1)\n\n    def test_bound_float2(self):\n        vals = [random.randint(-1.0, 1.0, (2, 3)).get() for _ in range(10)]\n        for val in vals:\n            self.assertEqual(val.shape, (2, 3))\n        self.assertEqual(min(_.min() for _ in vals), -1)\n        self.assertEqual(max(_.max() for _ in vals), 0)\n\n    @condition.repeat(3, 10)\n    def test_goodness_of_fit(self):\n        mx = 5\n        trial = 100\n        vals = [random.randint(mx).get() for _ in range(trial)]\n        counts = numpy.histogram(vals, bins=numpy.arange(mx + 1))[0]\n        expected = numpy.array([float(trial) / mx] * mx)\n        self.assertTrue(hypothesis.chi_square_test(counts, expected))\n\n    @condition.repeat(3, 10)\n    def test_goodness_of_fit_2(self):\n        mx = 5\n        vals = random.randint(mx, size=(5, 20)).get()\n        counts = numpy.histogram(vals, bins=numpy.arange(mx + 1))[0]\n        expected = numpy.array([float(vals.size) / mx] * mx)\n        self.assertTrue(hypothesis.chi_square_test(counts, expected))\n\n\n@testing.gpu\nclass TestRandintDtype(unittest.TestCase):\n\n    @testing.for_dtypes([\n        numpy.int8, numpy.uint8, numpy.int16, numpy.uint16, numpy.int32])\n    def test_dtype(self, dtype):\n        size = (1000,)\n        low = numpy.iinfo(dtype).min\n        high = numpy.iinfo(dtype).max + 1\n        x = random.randint(low, high, size, dtype).get()\n        self.assertLessEqual(low, min(x))\n        self.assertLessEqual(max(x), high)\n\n    @testing.for_int_dtypes(no_bool=True)\n    def test_dtype2(self, dtype):\n        dtype = numpy.dtype(dtype)\n\n        # randint does not support 64 bit integers\n        if dtype in (numpy.int64, numpy.uint64):\n            return\n\n        iinfo = numpy.iinfo(dtype)\n        size = (10000,)\n\n        x = random.randint(iinfo.min, iinfo.max + 1, size, dtype).get()\n        self.assertEqual(x.dtype, dtype)\n        self.assertLessEqual(iinfo.min, min(x))\n        self.assertLessEqual(max(x), iinfo.max)\n\n        # Lower bound check\n        with self.assertRaises(ValueError):\n            random.randint(iinfo.min - 1, iinfo.min + 10, size, dtype)\n\n        # Upper bound check\n        with self.assertRaises(ValueError):\n            random.randint(iinfo.max - 10, iinfo.max + 2, size, dtype)\n\n\n@testing.gpu\nclass TestRandomIntegers(unittest.TestCase):\n\n    def test_normal(self):\n        with mock.patch('cupy.random.sample_.randint') as m:\n            random.random_integers(3, 5)\n        m.assert_called_with(3, 6, None)\n\n    def test_high_is_none(self):\n        with mock.patch('cupy.random.sample_.randint') as m:\n            random.random_integers(3, None)\n        m.assert_called_with(1, 4, None)\n\n    def test_size_is_not_none(self):\n        with mock.patch('cupy.random.sample_.randint') as m:\n            random.random_integers(3, 5, (1, 2, 3))\n        m.assert_called_with(3, 6, (1, 2, 3))\n\n\n@testing.fix_random()\n@testing.gpu\nclass TestRandomIntegers2(unittest.TestCase):\n\n    @condition.repeat(3, 10)\n    def test_bound_1(self):\n        vals = [random.random_integers(0, 10, (2, 3)).get() for _ in range(10)]\n        for val in vals:\n            self.assertEqual(val.shape, (2, 3))\n        self.assertEqual(min(_.min() for _ in vals), 0)\n        self.assertEqual(max(_.max() for _ in vals), 10)\n\n    @condition.repeat(3, 10)\n    def test_bound_2(self):\n        vals = [random.random_integers(0, 2).get() for _ in range(20)]\n        for val in vals:\n            self.assertEqual(val.shape, ())\n        self.assertEqual(min(vals), 0)\n        self.assertEqual(max(vals), 2)\n\n    @condition.repeat(3, 10)\n    def test_goodness_of_fit(self):\n        mx = 5\n        trial = 100\n        vals = [random.randint(0, mx).get() for _ in range(trial)]\n        counts = numpy.histogram(vals, bins=numpy.arange(mx + 1))[0]\n        expected = numpy.array([float(trial) / mx] * mx)\n        self.assertTrue(hypothesis.chi_square_test(counts, expected))\n\n    @condition.repeat(3, 10)\n    def test_goodness_of_fit_2(self):\n        mx = 5\n        vals = random.randint(0, mx, (5, 20)).get()\n        counts = numpy.histogram(vals, bins=numpy.arange(mx + 1))[0]\n        expected = numpy.array([float(vals.size) / mx] * mx)\n        self.assertTrue(hypothesis.chi_square_test(counts, expected))\n\n\n@testing.gpu\nclass TestChoice(unittest.TestCase):\n\n    def setUp(self):\n        self.rs_tmp = random.generator._random_states\n        device_id = cuda.Device().id\n        self.m = mock.Mock()\n        self.m.choice.return_value = 0\n        random.generator._random_states = {device_id: self.m}\n\n    def tearDown(self):\n        random.generator._random_states = self.rs_tmp\n\n    def test_size_and_replace_and_p_are_none(self):\n        random.choice(3)\n        self.m.choice.assert_called_with(3, None, True, None)\n\n    def test_size_and_replace_are_none(self):\n        random.choice(3, None, None, [0.1, 0.1, 0.8])\n        self.m.choice.assert_called_with(3, None, None, [0.1, 0.1, 0.8])\n\n    def test_size_and_p_are_none(self):\n        random.choice(3, None, True)\n        self.m.choice.assert_called_with(3, None, True, None)\n\n    def test_replace_and_p_are_none(self):\n        random.choice(3, 1)\n        self.m.choice.assert_called_with(3, 1, True, None)\n\n    def test_size_is_none(self):\n        random.choice(3, None, True, [0.1, 0.1, 0.8])\n        self.m.choice.assert_called_with(3, None, True, [0.1, 0.1, 0.8])\n\n    def test_replace_is_none(self):\n        random.choice(3, 1, None, [0.1, 0.1, 0.8])\n        self.m.choice.assert_called_with(3, 1, None, [0.1, 0.1, 0.8])\n\n    def test_p_is_none(self):\n        random.choice(3, 1, True)\n        self.m.choice.assert_called_with(3, 1, True, None)\n\n    def test_no_none(self):\n        random.choice(3, 1, True, [0.1, 0.1, 0.8])\n        self.m.choice.assert_called_with(3, 1, True, [0.1, 0.1, 0.8])\n\n\n@testing.gpu\nclass TestRandomSample(unittest.TestCase):\n\n    def test_rand(self):\n        with mock.patch('cupy.random.sample_.random_sample') as m:\n            random.rand(1, 2, 3, dtype=numpy.float32)\n        m.assert_called_once_with(\n            size=(1, 2, 3), dtype=numpy.float32)\n\n    def test_rand_default_dtype(self):\n        with mock.patch('cupy.random.sample_.random_sample') as m:\n            random.rand(1, 2, 3)\n        m.assert_called_once_with(\n            size=(1, 2, 3), dtype=float)\n\n    def test_rand_invalid_argument(self):\n        with self.assertRaises(TypeError):\n            random.rand(1, 2, 3, unnecessary='unnecessary_argument')\n\n    def test_randn(self):\n        with mock.patch('cupy.random.distributions.normal') as m:\n            random.randn(1, 2, 3, dtype=numpy.float32)\n        m.assert_called_once_with(\n            size=(1, 2, 3), dtype=numpy.float32)\n\n    def test_randn_default_dtype(self):\n        with mock.patch('cupy.random.distributions.normal') as m:\n            random.randn(1, 2, 3)\n        m.assert_called_once_with(\n            size=(1, 2, 3), dtype=float)\n\n    def test_randn_invalid_argument(self):\n        with self.assertRaises(TypeError):\n            random.randn(1, 2, 3, unnecessary='unnecessary_argument')\n\n\n@testing.parameterize(\n    {'size': None},\n    {'size': ()},\n    {'size': 4},\n    {'size': (0,)},\n    {'size': (1, 0)},\n)\n@testing.fix_random()\n@testing.gpu\nclass TestMultinomial(unittest.TestCase):\n\n    @condition.repeat(3, 10)\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose(rtol=0.05)\n    def test_multinomial(self, xp, dtype):\n        pvals = xp.array([0.2, 0.3, 0.5], dtype)\n        x = xp.random.multinomial(100000, pvals, self.size)\n        self.assertEqual(x.dtype, 'l')\n        return x / 100000\n"""
tests/cupy_tests/sorting_tests/__init__.py,0,b''
tests/cupy_tests/sorting_tests/test_count.py,0,"b""import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestCount(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_count_nonzero(self, dtype):\n        def func(xp):\n            m = testing.shaped_random((2, 3), xp, xp.bool_)\n            a = testing.shaped_random((2, 3), xp, dtype) * m\n            c = xp.count_nonzero(a)\n            if xp is cupy:\n                # CuPy returns zero-dimensional array instead of\n                # returning a scalar value\n                self.assertIsInstance(c, xp.ndarray)\n                self.assertEqual(c.dtype, 'l')\n                self.assertEqual(c.shape, ())\n            return int(c)\n        self.assertEqual(func(numpy), func(cupy))\n\n    @testing.for_all_dtypes()\n    def test_count_nonzero_zero_dim(self, dtype):\n        def func(xp):\n            a = xp.array(1.0, dtype=dtype)\n            c = xp.count_nonzero(a)\n            if xp is cupy:\n                # CuPy returns zero-dimensional array instead of\n                # returning a scalar value\n                self.assertIsInstance(c, xp.ndarray)\n                self.assertEqual(c.dtype, 'l')\n                self.assertEqual(c.shape, ())\n            return int(c)\n        self.assertEqual(func(numpy), func(cupy))\n\n    @testing.for_all_dtypes()\n    def test_count_nonzero_int_axis(self, dtype):\n        for ax in range(3):\n            def func(xp):\n                m = testing.shaped_random((2, 3, 4), xp, xp.bool_)\n                a = testing.shaped_random((2, 3, 4), xp, dtype) * m\n                return xp.count_nonzero(a, axis=ax)\n            testing.assert_allclose(func(numpy), func(cupy))\n\n    @testing.for_all_dtypes()\n    def test_count_nonzero_tuple_axis(self, dtype):\n        for ax in range(3):\n            for ay in range(3):\n                if ax == ay:\n                    continue\n\n                def func(xp):\n                    m = testing.shaped_random((2, 3, 4), xp, xp.bool_)\n                    a = testing.shaped_random((2, 3, 4), xp, dtype) * m\n                    return xp.count_nonzero(a, axis=(ax, ay))\n                testing.assert_allclose(func(numpy), func(cupy))\n"""
tests/cupy_tests/sorting_tests/test_search.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestSearch(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_argmax_all(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return a.argmax()\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_external_argmax_all(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return xp.argmax(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(accept_error=ValueError)\n    def test_argmax_nan(self, xp, dtype):\n        a = xp.array([float('nan'), -1, 1], dtype)\n        return a.argmax()\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_argmax_axis_large(self, xp, dtype):\n        a = testing.shaped_random((3, 1000), xp, dtype)\n        return a.argmax(axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_external_argmax_axis_large(self, xp, dtype):\n        a = testing.shaped_random((3, 1000), xp, dtype)\n        return xp.argmax(a, axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_argmax_axis0(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.argmax(axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_argmax_axis1(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.argmax(axis=1)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_argmax_axis2(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.argmax(axis=2)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_argmax_tie(self, xp, dtype):\n        a = xp.array([0, 5, 2, 3, 4, 5], dtype)\n        return a.argmax()\n\n    @testing.for_all_dtypes(no_complex=True)\n    def test_argmax_zero_size(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((0, 1), xp, dtype)\n            with pytest.raises(ValueError):\n                a.argmax()\n\n    @testing.for_all_dtypes(no_complex=True)\n    def test_argmax_zero_size_axis0(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((0, 1), xp, dtype)\n            with pytest.raises(ValueError):\n                a.argmax(axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_argmax_zero_size_axis1(self, xp, dtype):\n        a = testing.shaped_random((0, 1), xp, dtype)\n        return a.argmax(axis=1)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_argmin_all(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return a.argmin()\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(accept_error=ValueError)\n    def test_argmin_nan(self, xp, dtype):\n        a = xp.array([float('nan'), -1, 1], dtype)\n        return a.argmin()\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_external_argmin_all(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return xp.argmin(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_argmin_axis_large(self, xp, dtype):\n        a = testing.shaped_random((3, 1000), xp, dtype)\n        return a.argmin(axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_external_argmin_axis_large(self, xp, dtype):\n        a = testing.shaped_random((3, 1000), xp, dtype)\n        return xp.argmin(a, axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_argmin_axis0(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.argmin(axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_argmin_axis1(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.argmin(axis=1)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_argmin_axis2(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return a.argmin(axis=2)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_argmin_tie(self, xp, dtype):\n        a = xp.array([0, 1, 2, 3, 0, 5], dtype)\n        return a.argmin()\n\n    @testing.for_all_dtypes(no_complex=True)\n    def test_argmin_zero_size(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((0, 1), xp, dtype)\n            with pytest.raises(ValueError):\n                return a.argmin()\n\n    @testing.for_all_dtypes(no_complex=True)\n    def test_argmin_zero_size_axis0(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((0, 1), xp, dtype)\n            with pytest.raises(ValueError):\n                a.argmin(axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_argmin_zero_size_axis1(self, xp, dtype):\n        a = testing.shaped_random((0, 1), xp, dtype)\n        return a.argmin(axis=1)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': ['argmin', 'argmax'],\n    'is_module': [True, False],\n    'shape': [(3, 4), ()],\n}))\nclass TestArgMinMaxDtype(unittest.TestCase):\n\n    @testing.for_dtypes(\n        dtypes=[numpy.int8, numpy.int16, numpy.int32, numpy.int64],\n        name='result_dtype')\n    @testing.for_all_dtypes(name='in_dtype')\n    def test_argminmax_dtype(self, in_dtype, result_dtype):\n        a = testing.shaped_random(self.shape, cupy, in_dtype)\n        if self.is_module:\n            func = getattr(cupy, self.func)\n            y = func(a, dtype=result_dtype)\n        else:\n            func = getattr(a, self.func)\n            y = func(dtype=result_dtype)\n        assert y.shape == ()\n        assert y.dtype == result_dtype\n\n\n@testing.parameterize(\n    {'cond_shape': (2, 3, 4), 'x_shape': (2, 3, 4), 'y_shape': (2, 3, 4)},\n    {'cond_shape': (4,),      'x_shape': (2, 3, 4), 'y_shape': (2, 3, 4)},\n    {'cond_shape': (2, 3, 4), 'x_shape': (2, 3, 4), 'y_shape': (3, 4)},\n    {'cond_shape': (3, 4),    'x_shape': (2, 3, 4), 'y_shape': (4,)},\n)\n@testing.gpu\nclass TestWhereTwoArrays(unittest.TestCase):\n\n    @testing.for_all_dtypes_combination(\n        names=['cond_type', 'x_type', 'y_type'])\n    @testing.numpy_cupy_allclose()\n    def test_where_two_arrays(self, xp, cond_type, x_type, y_type):\n        m = testing.shaped_random(self.cond_shape, xp, xp.bool_)\n        # Almost all values of a matrix `shaped_random` makes are not zero.\n        # To make a sparse matrix, we need multiply `m`.\n        cond = testing.shaped_random(self.cond_shape, xp, cond_type) * m\n        x = testing.shaped_random(self.x_shape, xp, x_type, seed=0)\n        y = testing.shaped_random(self.y_shape, xp, y_type, seed=1)\n        return xp.where(cond, x, y)\n\n\n@testing.parameterize(\n    {'cond_shape': (2, 3, 4)},\n    {'cond_shape': (4,)},\n    {'cond_shape': (2, 3, 4)},\n    {'cond_shape': (3, 4)},\n)\n@testing.gpu\nclass TestWhereCond(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test_where_cond(self, xp, dtype):\n        m = testing.shaped_random(self.cond_shape, xp, xp.bool_)\n        cond = testing.shaped_random(self.cond_shape, xp, dtype) * m\n        return xp.where(cond)\n\n\n@testing.gpu\nclass TestWhereError(unittest.TestCase):\n\n    def test_one_argument(self):\n        for xp in (numpy, cupy):\n            cond = testing.shaped_random((3, 4), xp, dtype=xp.bool_)\n            x = testing.shaped_random((2, 3, 4), xp, xp.int32)\n            with pytest.raises(ValueError):\n                xp.where(cond, x)\n\n\n@testing.parameterize(\n    {'array': numpy.random.randint(0, 2, (20,))},\n    {'array': numpy.random.randn(3, 2, 4)},\n    {'array': numpy.empty((0,))},\n    {'array': numpy.empty((0, 2))},\n    {'array': numpy.empty((0, 2, 0))},\n)\n@testing.gpu\nclass TestNonzero(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test_nonzero(self, xp, dtype):\n        array = xp.array(self.array, dtype=dtype)\n        return xp.nonzero(array)\n\n\n@testing.parameterize(\n    {'array': numpy.array(0)},\n    {'array': numpy.array(1)},\n)\n@testing.gpu\n@testing.with_requires('numpy>=1.17.0')\nclass TestNonzeroZeroDimension(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_nonzero(self, dtype):\n        for xp in (numpy, cupy):\n            array = xp.array(self.array, dtype=dtype)\n            with pytest.raises(DeprecationWarning):\n                xp.nonzero(array)\n\n\n@testing.parameterize(\n    {'array': numpy.random.randint(0, 2, (20,))},\n    {'array': numpy.random.randn(3, 2, 4)},\n    {'array': numpy.array(0)},\n    {'array': numpy.array(1)},\n    {'array': numpy.empty((0,))},\n    {'array': numpy.empty((0, 2))},\n    {'array': numpy.empty((0, 2, 0))},\n)\n@testing.gpu\nclass TestFlatNonzero(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_flatnonzero(self, xp, dtype):\n        array = xp.array(self.array, dtype=dtype)\n        return xp.flatnonzero(array)\n\n\n@testing.parameterize(\n    {'array': numpy.random.randint(0, 2, (20,))},\n    {'array': numpy.random.randn(3, 2, 4)},\n    {'array': numpy.empty((0,))},\n    {'array': numpy.empty((0, 2))},\n    {'array': numpy.empty((0, 2, 0))},\n)\n@testing.gpu\nclass TestArgwhere(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_argwhere(self, xp, dtype):\n        array = xp.array(self.array, dtype=dtype)\n        return xp.argwhere(array)\n\n\n@testing.parameterize(\n    {'array': cupy.array(1)},\n)\n@testing.gpu\nclass TestArgwhereZeroDimension(unittest.TestCase):\n\n    def test_argwhere(self):\n        with testing.assert_warns(DeprecationWarning):\n            return cupy.nonzero(self.array)\n\n\n@testing.gpu\nclass TestNanArgMin(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanargmin_all(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return xp.nanargmin(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(accept_error=ValueError)\n    def test_nanargmin_nan(self, xp, dtype):\n        a = xp.array([float('nan'), -1, 1], dtype)\n        return xp.nanargmin(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(accept_error=ValueError)\n    def test_nanargmin_nan2(self, xp, dtype):\n        a = xp.array([float('nan'), float('nan'), -1, 1], dtype)\n        return xp.nanargmin(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(accept_error=ValueError)\n    def test_nanargmin_nan3(self, xp, dtype):\n        a = xp.array([float('nan'), float('nan'), -1, 1, 1.0, -2.0], dtype)\n        return xp.nanargmin(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(accept_error=ValueError)\n    def test_nanargmin_nan4(self, xp, dtype):\n        a = xp.array([-1, 1, 1.0, -2.0, float('nan'), float('nan')],\n                     dtype)\n        return xp.nanargmin(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(accept_error=ValueError)\n    def test_nanargmin_nan5(self, xp, dtype):\n        a = xp.array([-1, 1, 1.0, -2.0, float('nan'), float('nan'), -1, 1],\n                     dtype)\n        return xp.nanargmin(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanargmin_axis_large(self, xp, dtype):\n        a = testing.shaped_random((3, 1000), xp, dtype)\n        return xp.nanargmin(a, axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanargmin_axis0(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return xp.nanargmin(a, axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanargmin_axis1(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return xp.nanargmin(a, axis=1)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanargmin_axis2(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return xp.nanargmin(a, axis=2)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanargmin_tie(self, xp, dtype):\n        a = xp.array([0, 5, 2, 3, 4, 5], dtype)\n        return xp.nanargmin(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    def test_nanargmin_zero_size(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((0, 1), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.nanargmin(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    def test_nanargmin_zero_size_axis0(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((0, 1), xp, dtype)\n            with pytest.raises(ValueError):\n                return xp.nanargmin(a, axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanargmin_zero_size_axis1(self, xp, dtype):\n        a = testing.shaped_random((0, 1), xp, dtype)\n        return xp.nanargmin(a, axis=1)\n\n\n@testing.gpu\nclass TestNanArgMax(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanargmax_all(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return xp.nanargmax(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(accept_error=ValueError)\n    def test_nanargmax_nan(self, xp, dtype):\n        a = xp.array([float('nan'), -1, 1], dtype)\n        return xp.nanargmax(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(accept_error=ValueError)\n    def test_nanargmax_nan2(self, xp, dtype):\n        a = xp.array([float('nan'), float('nan'), -1, 1], dtype)\n        return xp.nanargmax(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(accept_error=ValueError)\n    def test_nanargmax_nan3(self, xp, dtype):\n        a = xp.array([float('nan'), float('nan'), -1, 1, 1.0, -2.0], dtype)\n        return xp.nanargmax(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(accept_error=ValueError)\n    def test_nanargmax_nan4(self, xp, dtype):\n        a = xp.array([-1, 1, 1.0, -2.0, float('nan'), float('nan')],\n                     dtype)\n        return xp.nanargmax(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(accept_error=ValueError)\n    def test_nanargmax_nan5(self, xp, dtype):\n        a = xp.array([-1, 1, 1.0, -2.0, float('nan'), float('nan'), -1, 1],\n                     dtype)\n        return xp.nanargmax(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanargmax_axis_large(self, xp, dtype):\n        a = testing.shaped_random((3, 1000), xp, dtype)\n        return xp.nanargmax(a, axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanargmax_axis0(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return xp.nanargmax(a, axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanargmax_axis1(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return xp.nanargmax(a, axis=1)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanargmax_axis2(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return xp.nanargmax(a, axis=2)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanargmax_tie(self, xp, dtype):\n        a = xp.array([0, 5, 2, 3, 4, 5], dtype)\n        return xp.nanargmax(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    def test_nanargmax_zero_size(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((0, 1), xp, dtype)\n            with pytest.raises(ValueError):\n                xp.nanargmax(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    def test_nanargmax_zero_size_axis0(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((0, 1), xp, dtype)\n            with pytest.raises(ValueError):\n                return xp.nanargmax(a, axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanargmax_zero_size_axis1(self, xp, dtype):\n        a = testing.shaped_random((0, 1), xp, dtype)\n        return xp.nanargmax(a, axis=1)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product(\n    {'bins': [\n        [],\n        [0, 1, 2, 4, 10],\n        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n        [0.0, 1.0, 2.5, 4.0, 10.0],\n        [-1.0, 1.0, 2.5, 4.0, 20.0],\n        [1.5, 2.5, 4.0, 6.0],\n        [float('-inf'), 1.5, 2.5, 4.0, 6.0],\n        [1.5, 2.5, 4.0, 6.0, float('inf')],\n        [float('-inf'), 1.5, 2.5, 4.0, 6.0, float('inf')],\n        [0.0, 1.0, 1.0, 4.0, 4.0, 10.0],\n        [0.0, 1.0, 1.0, 4.0, 4.0, 4.0, 4.0, 10.0],\n    ],\n        'side': ['left', 'right'],\n        'shape': [(), (10,), (6, 3, 3)]})\n)\nclass TestSearchSorted(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_searchsorted(self, xp, dtype):\n        x = testing.shaped_arange(self.shape, xp, dtype)\n        bins = xp.array(self.bins)\n        y = xp.searchsorted(bins, x, side=self.side)\n        return y,\n\n\n@testing.gpu\n@testing.parameterize(\n    {'side': 'left'},\n    {'side': 'right'})\nclass TestSearchSortedNanInf(unittest.TestCase):\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_searchsorted_nanbins(self, xp):\n        x = testing.shaped_arange((10,), xp, xp.float64)\n        bins = xp.array([0, 1, 2, 4, 10, float('nan')])\n        y = xp.searchsorted(bins, x, side=self.side)\n        return y,\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_searchsorted_nan(self, xp):\n        x = testing.shaped_arange((10,), xp, xp.float64)\n        x[5] = float('nan')\n        bins = xp.array([0, 1, 2, 4, 10])\n        y = xp.searchsorted(bins, x, side=self.side)\n        return y,\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_searchsorted_nan_last(self, xp):\n        x = testing.shaped_arange((10,), xp, xp.float64)\n        x[-1] = float('nan')\n        bins = xp.array([0, 1, 2, 4, float('nan')])\n        y = xp.searchsorted(bins, x, side=self.side)\n        return y,\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_searchsorted_nan_last_repeat(self, xp):\n        x = testing.shaped_arange((10,), xp, xp.float64)\n        x[-1] = float('nan')\n        bins = xp.array([0, 1, 2, float('nan'), float('nan')])\n        y = xp.searchsorted(bins, x, side=self.side)\n        return y,\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_searchsorted_all_nans(self, xp):\n        x = testing.shaped_arange((10,), xp, xp.float64)\n        x[-1] = float('nan')\n        bins = xp.array([float('nan'), float('nan'), float('nan'),\n                         float('nan'), float('nan')])\n        y = xp.searchsorted(bins, x, side=self.side)\n        return y,\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_searchsorted_inf(self, xp):\n        x = testing.shaped_arange((10,), xp, xp.float64)\n        x[5] = float('inf')\n        bins = xp.array([0, 1, 2, 4, 10])\n        y = xp.searchsorted(bins, x, side=self.side)\n        return y,\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_searchsorted_minf(self, xp):\n        x = testing.shaped_arange((10,), xp, xp.float64)\n        x[5] = float('-inf')\n        bins = xp.array([0, 1, 2, 4, 10])\n        y = xp.searchsorted(bins, x, side=self.side)\n        return y,\n\n\n@testing.gpu\nclass TestSearchSortedInvalid(unittest.TestCase):\n\n    # Cant test unordered bins due to numpy undefined\n    # behavior for searchsorted\n\n    def test_searchsorted_ndbins(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((10,), xp, xp.float64)\n            bins = xp.array([[10, 4], [2, 1], [7, 8]])\n            with pytest.raises(ValueError):\n                xp.searchsorted(bins, x)\n\n\n@testing.gpu\nclass TestSearchSortedWithSorter(unittest.TestCase):\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_sorter(self, xp):\n        x = testing.shaped_arange((12,), xp, xp.float64)\n        bins = xp.array([10, 4, 2, 1, 8])\n        sorter = xp.array([3, 2, 1, 4, 0])\n        y = xp.searchsorted(bins, x, sorter=sorter)\n        return y,\n\n    def test_invalid_sorter(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((12,), xp, xp.float64)\n            bins = xp.array([10, 4, 2, 1, 8])\n            sorter = xp.array([0])\n            with pytest.raises(ValueError):\n                xp.searchsorted(bins, x, sorter=sorter)\n\n    def test_nonint_sorter(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((12,), xp, xp.float64)\n            bins = xp.array([10, 4, 2, 1, 8])\n            sorter = xp.array([], dtype=xp.float64)\n            with pytest.raises(TypeError):\n                xp.searchsorted(bins, x, sorter=sorter)\n"""
tests/cupy_tests/sorting_tests/test_sort.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestSort(unittest.TestCase):\n\n    # Test ranks\n\n    def test_sort_zero_dim(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((), xp)\n            with pytest.raises(numpy.AxisError):\n                a.sort()\n\n    def test_external_sort_zero_dim(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((), xp)\n            with pytest.raises(numpy.AxisError):\n                xp.sort(a)\n\n    @testing.numpy_cupy_array_equal()\n    def test_sort_two_or_more_dim(self, xp):\n        a = testing.shaped_random((2, 3, 3), xp)\n        a.sort()\n        return a\n\n    @testing.numpy_cupy_array_equal()\n    def test_external_sort_two_or_more_dim(self, xp):\n        a = testing.shaped_random((2, 3, 3), xp)\n        return xp.sort(a)\n\n    # Test dtypes\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_sort_dtype(self, xp, dtype):\n        a = testing.shaped_random((10,), xp, dtype)\n        a.sort()\n        return a\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_external_sort_dtype(self, xp, dtype):\n        a = testing.shaped_random((10,), xp, dtype)\n        return xp.sort(a)\n\n    # Test contiguous arrays\n\n    @testing.numpy_cupy_array_equal()\n    def test_sort_contiguous(self, xp):\n        a = testing.shaped_random((10,), xp)  # C contiguous view\n        a.sort()\n        return a\n\n    def test_sort_non_contiguous(self):\n        a = testing.shaped_random((10,), cupy)[::2]  # Non contiguous view\n        with self.assertRaises(NotImplementedError):\n            a.sort()\n\n    @testing.numpy_cupy_array_equal()\n    def test_external_sort_contiguous(self, xp):\n        a = testing.shaped_random((10,), xp)  # C contiguous view\n        return xp.sort(a)\n\n    @testing.numpy_cupy_array_equal()\n    def test_external_sort_non_contiguous(self, xp):\n        a = testing.shaped_random((10,), xp)[::2]  # Non contiguous view\n        return xp.sort(a)\n\n    # Test axis\n\n    @testing.numpy_cupy_array_equal()\n    def test_sort_axis1(self, xp):\n        a = testing.shaped_random((2, 3, 4), xp)\n        a.sort(axis=0)\n        return a\n\n    @testing.numpy_cupy_array_equal()\n    def test_sort_axis2(self, xp):\n        a = testing.shaped_random((2, 3, 4), xp)\n        a.sort(axis=1)\n        return a\n\n    @testing.numpy_cupy_array_equal()\n    def test_sort_axis3(self, xp):\n        a = testing.shaped_random((2, 3, 4), xp)\n        a.sort(axis=2)\n        return a\n\n    @testing.numpy_cupy_array_equal()\n    def test_external_sort_axis(self, xp):\n        a = testing.shaped_random((2, 3, 3), xp)\n        return xp.sort(a, axis=0)\n\n    @testing.numpy_cupy_array_equal()\n    def test_sort_negative_axis(self, xp):\n        a = testing.shaped_random((2, 3, 3), xp)\n        a.sort(axis=-2)\n        return a\n\n    @testing.numpy_cupy_array_equal()\n    def test_external_sort_negative_axis(self, xp):\n        a = testing.shaped_random((2, 3, 3), xp)\n        return xp.sort(a, axis=-2)\n\n    @testing.numpy_cupy_array_equal()\n    def test_external_sort_none_axis(self, xp):\n        a = testing.shaped_random((2, 3, 3), xp)\n        return xp.sort(a, axis=None)\n\n    def test_sort_invalid_axis1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((2, 3, 3), xp)\n            with pytest.raises(numpy.AxisError):\n                a.sort(axis=3)\n\n    def test_sort_invalid_axis2(self):\n        a = testing.shaped_random((2, 3, 3), cupy)\n        with self.assertRaises(numpy.AxisError):\n            a.sort(axis=3)\n\n    def test_external_sort_invalid_axis1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((2, 3, 3), xp)\n            with pytest.raises(numpy.AxisError):\n                xp.sort(a, axis=3)\n\n    def test_external_sort_invalid_axis2(self):\n        a = testing.shaped_random((2, 3, 3), cupy)\n        with self.assertRaises(numpy.AxisError):\n            cupy.sort(a, axis=3)\n\n    def test_sort_invalid_negative_axis1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((2, 3, 3), xp)\n            with pytest.raises(numpy.AxisError):\n                a.sort(axis=-4)\n\n    def test_sort_invalid_negative_axis2(self):\n        a = testing.shaped_random((2, 3, 3), cupy)\n        with self.assertRaises(numpy.AxisError):\n            a.sort(axis=-4)\n\n    def test_external_sort_invalid_negative_axis1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((2, 3, 3), xp)\n            with pytest.raises(numpy.AxisError):\n                xp.sort(a, axis=-4)\n\n    def test_external_sort_invalid_negative_axis2(self):\n        a = testing.shaped_random((2, 3, 3), cupy)\n        with self.assertRaises(numpy.AxisError):\n            cupy.sort(a, axis=-4)\n\n    # Test NaN ordering\n\n    @testing.for_dtypes('efdFD')\n    @testing.numpy_cupy_array_equal()\n    def test_nan1(self, xp, dtype):\n        a = testing.shaped_random((10,), xp, dtype)\n        a[2] = a[6] = xp.nan\n        out = xp.sort(a)\n        return out\n\n    @testing.for_dtypes('efdFD')\n    @testing.numpy_cupy_array_equal()\n    def test_nan2(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        a[0, 2, 1] = a[1, 0, 3] = xp.nan\n        out = xp.sort(a, axis=0)\n        return out\n\n    @testing.for_dtypes('efdFD')\n    @testing.numpy_cupy_array_equal()\n    def test_nan3(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        a[0, 2, 1] = a[1, 0, 3] = xp.nan\n        out = xp.sort(a, axis=1)\n        return out\n\n    @testing.for_dtypes('efdFD')\n    @testing.numpy_cupy_array_equal()\n    def test_nan4(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        a[0, 2, 1] = a[1, 0, 3] = xp.nan\n        out = xp.sort(a, axis=2)\n        return out\n\n\n@testing.gpu\nclass TestLexsort(unittest.TestCase):\n\n    # Test ranks\n\n    # TODO(niboshi): Fix xfail\n    @pytest.mark.xfail(reason='Explicit error types required')\n    def test_lexsort_zero_dim(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((), xp)\n            with pytest.raises(numpy.AxisError):\n                return xp.lexsort(a)\n\n    @testing.numpy_cupy_array_equal\n    def test_lexsort_one_dim(self, xp):\n        a = testing.shaped_random((2,), xp)\n        return xp.lexsort(a)\n\n    @testing.numpy_cupy_array_equal\n    def test_lexsort_two_dim(self, xp):\n        a = xp.array([[9, 4, 0, 4, 0, 2, 1],\n                      [1, 5, 1, 4, 3, 4, 4]])  # from numpy.lexsort example\n        return xp.lexsort(a)\n\n    def test_lexsort_three_or_more_dim(self):\n        a = testing.shaped_random((2, 10, 10), cupy)\n        with self.assertRaises(NotImplementedError):\n            return cupy.lexsort(a)\n\n    # Test dtypes\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_lexsort_dtype(self, xp, dtype):\n        a = testing.shaped_random((2, 10), xp, dtype)\n        return xp.lexsort(a)\n\n    # Test NaN ordering\n\n    @testing.for_dtypes('efdFD')\n    @testing.numpy_cupy_array_equal()\n    def test_nan1(self, xp, dtype):\n        a = testing.shaped_random((2, 10), xp, dtype)\n        a[0, 2] = a[0, 6] = xp.nan\n        return xp.lexsort(a)\n\n    @testing.for_dtypes('efdFD')\n    @testing.numpy_cupy_array_equal()\n    def test_nan2(self, xp, dtype):\n        a = testing.shaped_random((2, 10), xp, dtype)\n        a[1, 2] = a[0, 6] = xp.nan\n        return xp.lexsort(a)\n\n    @testing.for_dtypes('efdFD')\n    @testing.numpy_cupy_array_equal()\n    def test_nan3(self, xp, dtype):\n        a = testing.shaped_random((2, 10), xp, dtype)\n        a[1, 2] = a[1, 6] = xp.nan\n        return xp.lexsort(a)\n\n    # Test non C-contiguous input\n\n    @testing.numpy_cupy_array_equal()\n    def test_view(self, xp):\n        # from #3232\n        a = testing.shaped_random((4, 8), xp, dtype=xp.float64)\n        a = a.T[::-1]\n        return xp.lexsort(a)\n\n    @testing.numpy_cupy_array_equal()\n    def test_F_order(self, xp):\n        a = testing.shaped_random((4, 8), xp, dtype=xp.float64)\n        a = xp.asfortranarray(a)\n        assert a.flags.f_contiguous\n        assert not a.flags.c_contiguous\n        return xp.lexsort(a)\n\n\n@testing.parameterize(*testing.product({\n    'external': [False, True],\n}))\n@testing.gpu\nclass TestArgsort(unittest.TestCase):\n\n    def argsort(self, a, axis=-1):\n        if self.external:\n            xp = cupy.get_array_module(a)\n            return xp.argsort(a, axis=axis)\n        else:\n            return a.argsort(axis=axis)\n\n    # Test base cases\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_argsort_zero_dim(self, xp, dtype):\n        a = testing.shaped_random((), xp, dtype)\n        return self.argsort(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_argsort_one_dim(self, xp, dtype):\n        a = testing.shaped_random((10,), xp, dtype)\n        return self.argsort(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_argsort_multi_dim(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 3), xp, dtype)\n        return self.argsort(a)\n\n    @testing.numpy_cupy_array_equal()\n    def test_argsort_non_contiguous(self, xp):\n        a = xp.array([1, 0, 2, 3])[::2]\n        return self.argsort(a)\n\n    # Test axis\n\n    @testing.numpy_cupy_array_equal()\n    def test_argsort_axis(self, xp):\n        a = testing.shaped_random((2, 3, 3), xp)\n        return self.argsort(a, axis=0)\n\n    @testing.numpy_cupy_array_equal()\n    def test_argsort_negative_axis(self, xp):\n        a = testing.shaped_random((2, 3, 3), xp)\n        return self.argsort(a, axis=2)\n\n    @testing.numpy_cupy_array_equal()\n    def test_argsort_none_axis(self, xp):\n        a = testing.shaped_random((2, 3, 3), xp)\n        return self.argsort(a, axis=None)\n\n    def test_argsort_invalid_axis1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((2, 3, 3), xp)\n            with pytest.raises(numpy.AxisError):\n                self.argsort(a, axis=3)\n\n    def test_argsort_invalid_axis2(self):\n        a = testing.shaped_random((2, 3, 3), cupy)\n        with self.assertRaises(numpy.AxisError):\n            return self.argsort(a, axis=3)\n\n    @testing.numpy_cupy_array_equal()\n    def test_argsort_zero_dim_axis(self, xp):\n        a = testing.shaped_random((), xp)\n        return self.argsort(a, axis=0)\n\n    def test_argsort_zero_dim_invalid_axis(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((), xp)\n            with pytest.raises(numpy.AxisError):\n                self.argsort(a, axis=1)\n\n    def test_argsort_invalid_negative_axis1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((2, 3, 3), xp)\n            with pytest.raises(numpy.AxisError):\n                self.argsort(a, axis=-4)\n\n    def test_argsort_invalid_negative_axis2(self):\n        a = testing.shaped_random((2, 3, 3), cupy)\n        with self.assertRaises(numpy.AxisError):\n            return self.argsort(a, axis=-4)\n\n    # Misc tests\n\n    def test_argsort_original_array_not_modified_one_dim(self):\n        a = testing.shaped_random((10,), cupy)\n        b = cupy.array(a)\n        self.argsort(a)\n        testing.assert_allclose(a, b)\n\n    def test_argsort_original_array_not_modified_multi_dim(self):\n        a = testing.shaped_random((2, 3, 3), cupy)\n        b = cupy.array(a)\n        self.argsort(a)\n        testing.assert_allclose(a, b)\n\n    # Test NaN ordering\n\n    @testing.for_dtypes('efdFD')\n    @testing.numpy_cupy_array_equal()\n    def test_nan1(self, xp, dtype):\n        a = testing.shaped_random((10,), xp, dtype)\n        a[2] = a[6] = xp.nan\n        return self.argsort(a)\n\n    @testing.for_dtypes('efdFD')\n    @testing.numpy_cupy_array_equal()\n    def test_nan2(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        a[0, 2, 1] = a[1, 1, 3] = xp.nan\n        return self.argsort(a)\n\n\n@testing.gpu\nclass TestMsort(unittest.TestCase):\n\n    # Test base cases\n\n    # TODO(niboshi): Fix xfail\n    @pytest.mark.xfail(reason='Explicit error types required')\n    def test_msort_zero_dim(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((), xp)\n            with pytest.raises(numpy.AxisError):\n                xp.msort(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_msort_one_dim(self, xp, dtype):\n        a = testing.shaped_random((10,), xp, dtype)\n        return xp.msort(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_msort_multi_dim(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return xp.msort(a)\n\n\n@testing.gpu\nclass TestSort_complex(unittest.TestCase):\n\n    def test_sort_complex_zero_dim(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((), xp)\n            with pytest.raises(numpy.AxisError):\n                xp.sort_complex(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test_sort_complex_1dim(self, xp, dtype):\n        a = testing.shaped_random((100,), xp, dtype)\n        return a, xp.sort_complex(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test_sort_complex_ndim(self, xp, dtype):\n        a = testing.shaped_random((2, 5, 3), xp, dtype)\n        return a, xp.sort_complex(a)\n\n    @testing.for_dtypes('efdFD')\n    @testing.numpy_cupy_array_list_equal()\n    def test_sort_complex_nan(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 5), xp, dtype)\n        a[0, 2, 1] = a[1, 0, 3] = xp.nan\n        return a, xp.sort_complex(a)\n\n\n@testing.parameterize(*testing.product({\n    'external': [False, True],\n    'length': [10, 20000],\n}))\n@testing.gpu\nclass TestPartition(unittest.TestCase):\n\n    def partition(self, a, kth, axis=-1):\n        if self.external:\n            xp = cupy.get_array_module(a)\n            return xp.partition(a, kth, axis=axis)\n        else:\n            a.partition(kth, axis=axis)\n            return a\n\n    # Test base cases\n\n    def test_partition_zero_dim(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((), xp)\n            kth = 2\n            with pytest.raises(numpy.AxisError):\n                self.partition(a, kth)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_partition_one_dim(self, xp, dtype):\n        a = testing.shaped_random((self.length,), xp, dtype)\n        kth = 2\n        x = self.partition(a, kth)\n        self.assertTrue(xp.all(x[0:kth] <= x[kth:kth + 1]))\n        self.assertTrue(xp.all(x[kth:kth + 1] <= x[kth + 1:]))\n        return x[kth]\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_partition_multi_dim(self, xp, dtype):\n        a = testing.shaped_random((10, 10, self.length), xp, dtype)\n        kth = 2\n        x = self.partition(a, kth)\n        self.assertTrue(xp.all(x[:, :, 0:kth] <= x[:, :, kth:kth + 1]))\n        self.assertTrue(xp.all(x[:, :, kth:kth + 1] <= x[:, :, kth + 1:]))\n        return x[:, :, kth:kth + 1]\n\n    # Test non-contiguous array\n\n    @testing.numpy_cupy_equal()\n    def test_partition_non_contiguous(self, xp):\n        a = testing.shaped_random((self.length,), xp)[::-1]\n        kth = 2\n        if not self.external:\n            if xp is cupy:\n                with self.assertRaises(NotImplementedError):\n                    return self.partition(a, kth)\n            return 0  # dummy\n        else:\n            x = self.partition(a, kth)\n            self.assertTrue(xp.all(x[0:kth] <= x[kth:kth + 1]))\n            self.assertTrue(xp.all(x[kth:kth + 1] <= x[kth + 1:]))\n            return x[kth]\n\n    # Test kth\n\n    @testing.numpy_cupy_equal()\n    def test_partition_sequence_kth(self, xp):\n        a = testing.shaped_random((self.length,), xp)\n        kth = (2, 4)\n        x = self.partition(a, kth)\n        return x[kth[0]], x[kth[1]]\n\n    @testing.numpy_cupy_equal()\n    def test_partition_negative_kth(self, xp):\n        a = testing.shaped_random((self.length,), xp)\n        kth = -3\n        x = self.partition(a, kth)\n        return x[kth]\n\n    def test_partition_invalid_kth(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((self.length,), xp)\n            kth = self.length\n            with pytest.raises(ValueError):\n                self.partition(a, kth)\n\n    def test_partition_invalid_negative_kth(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((self.length,), xp)\n            kth = -self.length - 1\n            with pytest.raises(ValueError):\n                self.partition(a, kth)\n\n    # Test axis\n\n    @testing.numpy_cupy_array_equal()\n    def test_partition_axis(self, xp):\n        a = testing.shaped_random((self.length, 10, 10), xp)\n        kth = 2\n        axis = 0\n        x = self.partition(a, kth, axis=axis)\n        return x[kth, :, :]\n\n    @testing.numpy_cupy_array_equal()\n    def test_partition_negative_axis(self, xp):\n        a = testing.shaped_random((10, 10, self.length), xp)\n        kth = 2\n        axis = -1\n        x = self.partition(a, kth, axis=axis)\n        return x[:, :, kth]\n\n    @testing.numpy_cupy_equal()\n    def test_partition_none_axis(self, xp):\n        if self.external:\n            a = testing.shaped_random((2, self.length), xp)\n            kth = 2\n            axis = None\n            x = self.partition(a, kth, axis=axis)\n            return x[kth]\n        else:\n            return None\n\n    def test_partition_invalid_axis1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((2, 2, self.length), xp)\n            kth = 2\n            axis = 3\n            with pytest.raises(numpy.AxisError):\n                self.partition(a, kth, axis=axis)\n\n    def test_partition_invalid_axis2(self):\n        a = testing.shaped_random((2, 2, self.length), cupy)\n        with self.assertRaises(numpy.AxisError):\n            kth = 2\n            axis = 3\n            return self.partition(a, kth, axis=axis)\n\n    def test_partition_invalid_negative_axis1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((2, 2, self.length), xp)\n            kth = 2\n            axis = -4\n            with pytest.raises(numpy.AxisError):\n                self.partition(a, kth, axis=axis)\n\n    def test_partition_invalid_negative_axis2(self):\n        a = testing.shaped_random((2, 2, self.length), cupy)\n        with self.assertRaises(numpy.AxisError):\n            kth = 2\n            axis = -4\n            return self.partition(a, kth, axis=axis)\n\n\n@testing.parameterize(*testing.product({\n    'external': [False, True],\n}))\n@testing.gpu\nclass TestArgpartition(unittest.TestCase):\n\n    def argpartition(self, a, kth, axis=-1):\n        if self.external:\n            xp = cupy.get_array_module(a)\n            return xp.argpartition(a, kth, axis=axis)\n        else:\n            return a.argpartition(kth, axis=axis)\n\n    # Test base cases\n\n    def test_argpartition_zero_dim(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((), xp)\n            kth = 2\n            with pytest.raises(ValueError):\n                self.argpartition(a, kth)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_equal()\n    def test_argpartition_one_dim(self, xp, dtype):\n        a = testing.shaped_random((10,), xp, dtype, 100)\n        kth = 2\n        idx = self.argpartition(a, kth)\n        self.assertTrue((a[idx[:kth]] < a[idx[kth]]).all())\n        self.assertTrue((a[idx[kth]] < a[idx[kth + 1:]]).all())\n        return idx[kth]\n\n    # TODO(leofang): test all dtypes -- this workaround needs to be kept,\n    # likely due to #3287? Need investigation.\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_argpartition_multi_dim(self, xp, dtype):\n        a = testing.shaped_random((3, 3, 10), xp, dtype, 100)\n        kth = 2\n        idx = self.argpartition(a, kth)\n        rows = [[[0]], [[1]], [[2]]]\n        cols = [[[0], [1], [2]]]\n        self.assertTrue((a[rows, cols, idx[:, :, :kth]] <\n                         a[rows, cols, idx[:, :, kth:kth + 1]]).all())\n        self.assertTrue((a[rows, cols, idx[:, :, kth:kth + 1]] <\n                         a[rows, cols, idx[:, :, kth + 1:]]).all())\n        return idx[:, :, kth:kth + 1]\n\n    # Test non-contiguous array\n\n    @testing.numpy_cupy_equal()\n    def test_argpartition_non_contiguous(self, xp):\n        a = testing.shaped_random((10,), xp, 'i', 100)[::2]\n        kth = 2\n        idx = self.argpartition(a, kth)\n        self.assertTrue((a[idx[:kth]] < a[idx[kth]]).all())\n        self.assertTrue((a[idx[kth]] < a[idx[kth + 1:]]).all())\n        return idx[kth]\n\n    # Test kth\n\n    @testing.numpy_cupy_equal()\n    def test_argpartition_sequence_kth(self, xp):\n        a = testing.shaped_random((10,), xp, scale=100)\n        kth = (2, 4)\n        idx = self.argpartition(a, kth)\n        for _kth in kth:\n            self.assertTrue((a[idx[:_kth]] < a[idx[_kth]]).all())\n            self.assertTrue((a[idx[_kth]] < a[idx[_kth + 1:]]).all())\n        return (idx[2], idx[4])\n\n    @testing.numpy_cupy_equal()\n    def test_argpartition_negative_kth(self, xp):\n        a = testing.shaped_random((10,), xp, scale=100)\n        kth = -3\n        idx = self.argpartition(a, kth)\n        self.assertTrue((a[idx[:kth]] < a[idx[kth]]).all())\n        self.assertTrue((a[idx[kth]] < a[idx[kth + 1:]]).all())\n        return idx[kth]\n\n    def test_argpartition_invalid_kth(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((10,), xp, scale=100)\n            kth = 10\n            with pytest.raises(ValueError):\n                self.argpartition(a, kth)\n\n    def test_argpartition_invalid_negative_kth(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((10,), xp, scale=100)\n            kth = -11\n            with pytest.raises(ValueError):\n                self.argpartition(a, kth)\n\n    # Test axis\n\n    @testing.numpy_cupy_array_equal()\n    def test_argpartition_axis(self, xp):\n        a = testing.shaped_random((10, 3, 3), xp, scale=100)\n        kth = 2\n        axis = 0\n        idx = self.argpartition(a, kth, axis=axis)\n        rows = [[[0], [1], [2]]]\n        cols = [[[0, 1, 2]]]\n        self.assertTrue((a[idx[:kth, :, :], rows, cols] <\n                         a[idx[kth:kth + 1, :, :], rows, cols]).all())\n        self.assertTrue((a[idx[kth:kth + 1, :, :], rows, cols] <\n                         a[idx[kth + 1:, :, :], rows, cols]).all())\n        return idx[kth:kth + 1, :, :]\n\n    @testing.numpy_cupy_array_equal()\n    def test_argpartition_negative_axis(self, xp):\n        a = testing.shaped_random((3, 3, 10), xp, scale=100)\n        kth = 2\n        axis = -1\n        idx = self.argpartition(a, kth, axis=axis)\n        rows = [[[0]], [[1]], [[2]]]\n        cols = [[[0], [1], [2]]]\n        self.assertTrue((a[rows, cols, idx[:, :, :kth]] <\n                         a[rows, cols, idx[:, :, kth:kth + 1]]).all())\n        self.assertTrue((a[rows, cols, idx[:, :, kth:kth + 1]] <\n                         a[rows, cols, idx[:, :, kth + 1:]]).all())\n        return idx[:, :, kth:kth + 1]\n\n    @testing.numpy_cupy_equal()\n    def test_argpartition_none_axis(self, xp):\n        a = testing.shaped_random((2, 2), xp, scale=100)\n        kth = 2\n        axis = None\n        idx = self.argpartition(a, kth, axis=axis)\n        a1 = a.flatten()\n        self.assertTrue((a1[idx[:kth]] < a1[idx[kth]]).all())\n        self.assertTrue((a1[idx[kth]] < a1[idx[kth + 1:]]).all())\n        return idx[kth]\n\n    def test_argpartition_invalid_axis1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((2, 2, 2), xp, scale=100)\n            kth = 1\n            axis = 3\n            with pytest.raises(numpy.AxisError):\n                self.argpartition(a, kth, axis=axis)\n\n    def test_argpartition_invalid_axis2(self):\n        a = testing.shaped_random((2, 2, 2), cupy, scale=100)\n        kth = 1\n        axis = 3\n        with self.assertRaises(numpy.AxisError):\n            self.argpartition(a, kth, axis=axis)\n\n    def test_argpartition_invalid_negative_axis1(self):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((2, 2, 2), xp, scale=100)\n            kth = 1\n            axis = -4\n            with pytest.raises(numpy.AxisError):\n                self.argpartition(a, kth, axis=axis)\n\n    def test_argpartition_invalid_negative_axis2(self):\n        a = testing.shaped_random((2, 2, 2), cupy, scale=100)\n        kth = 1\n        axis = -4\n        with self.assertRaises(numpy.AxisError):\n            self.argpartition(a, kth, axis=axis)\n"""
tests/cupy_tests/statics_tests/__init__.py,0,b''
tests/cupy_tests/statics_tests/test_correlation.py,0,"b'import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestCorrcoef(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_corrcoef(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return xp.corrcoef(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_corrcoef_diag_exception(self, xp, dtype):\n        a = testing.shaped_arange((1, 3), xp, dtype)\n        return xp.corrcoef(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_corrcoef_y(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        y = testing.shaped_arange((2, 3), xp, dtype)\n        return xp.corrcoef(a, y=y)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_corrcoef_rowvar(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        y = testing.shaped_arange((2, 3), xp, dtype)\n        return xp.corrcoef(a, y=y, rowvar=False)\n\n\n@testing.gpu\nclass TestCov(unittest.TestCase):\n\n    def generate_input(self, a_shape, y_shape, xp, dtype):\n        a = testing.shaped_arange(a_shape, xp, dtype)\n        y = None\n        if y_shape is not None:\n            y = testing.shaped_arange(y_shape, xp, dtype)\n        return a, y\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def check(self, a_shape, y_shape=None, rowvar=True, bias=False,\n              ddof=None, xp=None, dtype=None):\n        a, y = self.generate_input(a_shape, y_shape, xp, dtype)\n        return xp.cov(a, y, rowvar, bias, ddof)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def check_warns(self, a_shape, y_shape=None, rowvar=True, bias=False,\n                    ddof=None, xp=None, dtype=None):\n        with testing.assert_warns(RuntimeWarning):\n            a, y = self.generate_input(a_shape, y_shape, xp, dtype)\n            return xp.cov(a, y, rowvar, bias, ddof)\n\n    @testing.for_all_dtypes()\n    def check_raises(self, a_shape, y_shape=None, rowvar=True, bias=False,\n                     ddof=None, dtype=None):\n        for xp in (numpy, cupy):\n            a, y = self.generate_input(a_shape, y_shape, xp, dtype)\n            with pytest.raises(ValueError):\n                xp.cov(a, y, rowvar, bias, ddof)\n\n    def test_cov(self):\n        self.check((2, 3))\n        self.check((2,), (2,))\n        self.check((1, 3), (1, 3), rowvar=False)\n        self.check((2, 3), (2, 3), rowvar=False)\n        self.check((2, 3), bias=True)\n        self.check((2, 3), ddof=2)\n\n    def test_cov_warns(self):\n        self.check_warns((2, 3), ddof=3)\n        self.check_warns((2, 3), ddof=4)\n\n    def test_cov_raises(self):\n        self.check_raises((2, 3), ddof=1.2)\n        self.check_raises((3, 4, 2))\n        self.check_raises((2, 3), (3, 4, 2))\n\n    def test_cov_empty(self):\n        self.check((0, 1))\n'"
tests/cupy_tests/statics_tests/test_histogram.py,0,"b""import sys\nimport unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n# Note that numpy.bincount does not support uint64 on 64-bit environment\n# as it casts an input array to intp.\n# And it does not support uint32, int64 and uint64 on 32-bit environment.\n_all_types = (\n    numpy.float16, numpy.float32, numpy.float64,\n    numpy.int8, numpy.int16, numpy.int32,\n    numpy.uint8, numpy.uint16,\n    numpy.bool_)\n_signed_types = (\n    numpy.int8, numpy.int16, numpy.int32,\n    numpy.bool_)\n\nif sys.maxsize > 2 ** 32:\n    _all_types = _all_types + (numpy.int64, numpy.uint32)\n    _signed_types = _signed_types + (numpy.int64,)\n\n\ndef for_all_dtypes_bincount(name='dtype'):\n    return testing.for_dtypes(_all_types, name=name)\n\n\ndef for_signed_dtypes_bincount(name='dtype'):\n    return testing.for_dtypes(_signed_types, name=name)\n\n\ndef for_all_dtypes_combination_bincount(names):\n    return testing.helper.for_dtypes_combination(_all_types, names=names)\n\n\n@testing.gpu\nclass TestHistogram(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_histogram(self, xp, dtype):\n        x = testing.shaped_arange((10,), xp, dtype)\n        y, bin_edges = xp.histogram(x)\n        return y, bin_edges\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_histogram_same_value(self, xp, dtype):\n        x = xp.zeros(10, dtype)\n        y, bin_edges = xp.histogram(x, 3)\n        return y, bin_edges\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_histogram_density(self, xp, dtype):\n        x = testing.shaped_arange((10,), xp, dtype)\n        y, bin_edges = xp.histogram(x, density=True)\n        # check normalization\n        area = xp.sum(y * xp.diff(bin_edges))\n        testing.assert_allclose(area, 1)\n        return y, bin_edges\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test_histogram_range_lower_outliers(self, xp, dtype):\n        # Check that lower outliers are not tallied\n        a = xp.arange(10, dtype=dtype) + .5\n        h, b = xp.histogram(a, range=[0, 9])\n        assert int(h.sum()) == 9\n        return h, b\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test_histogram_range_upper_outliers(self, xp, dtype):\n        # Check that upper outliers are not tallied\n        a = xp.arange(10, dtype=dtype) + .5\n        h, b = xp.histogram(a, range=[1, 10])\n        assert int(h.sum()) == 9\n        return h, b\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_histogram_range_with_density(self, xp, dtype):\n        a = xp.arange(10, dtype=dtype) + .5\n        h, b = xp.histogram(a, range=[1, 9], density=True)\n        # check normalization\n        testing.assert_allclose(float((h * xp.diff(b)).sum()), 1)\n        return h\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_histogram_range_with_weights_and_density(self, xp, dtype):\n        a = xp.arange(10, dtype=dtype) + .5\n        w = xp.arange(10, dtype=dtype) + .5\n        h, b = xp.histogram(a, range=[1, 9], weights=w, density=True)\n        testing.assert_allclose(float((h * xp.diff(b)).sum()), 1)\n        return h\n\n    def test_histogram_invalid_range(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(ValueError):\n                # range must be None or have two elements\n                xp.histogram(xp.arange(10), range=[1, 9, 15])\n\n    def test_histogram_invalid_range2(self):\n        for xp in (numpy, cupy):\n            with pytest.raises(TypeError):\n                xp.histogram(xp.arange(10), range=10)\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    def test_histogram_weights_mismatch(self, dtype):\n        for xp in (numpy, cupy):\n            a = xp.arange(10, dtype=dtype) + .5\n            w = xp.arange(11, dtype=dtype) + .5\n            with pytest.raises(ValueError):\n                xp.histogram(a, range=[1, 9], weights=w, density=True)\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_histogram_int_weights_dtype(self, xp, dtype):\n        # Check the type of the returned histogram\n        a = xp.arange(10, dtype=dtype)\n        h, b = xp.histogram(a, weights=xp.ones(10, int))\n        assert xp.issubdtype(h.dtype, xp.integer)\n        return h\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_histogram_float_weights_dtype(self, xp, dtype):\n        # Check the type of the returned histogram\n        a = xp.arange(10, dtype=dtype)\n        h, b = xp.histogram(a, weights=xp.ones(10, float))\n        assert xp.issubdtype(h.dtype, xp.floating)\n        return h\n\n    def test_histogram_weights_basic(self):\n        v = cupy.random.rand(100)\n        w = cupy.ones(100) * 5\n        a, b = cupy.histogram(v)\n        na, nb = cupy.histogram(v, density=True)\n        wa, wb = cupy.histogram(v, weights=w)\n        nwa, nwb = cupy.histogram(v, weights=w, density=True)\n        testing.assert_array_almost_equal(a * 5, wa)\n        testing.assert_array_almost_equal(na, nwa)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_histogram_float_weights(self, xp, dtype):\n        # Check weights are properly applied.\n        v = xp.linspace(0, 10, 10, dtype=dtype)\n        w = xp.concatenate((xp.zeros(5, dtype=dtype), xp.ones(5, dtype=dtype)))\n        wa, wb = xp.histogram(v, bins=xp.arange(11), weights=w)\n        testing.assert_array_almost_equal(wa, w)\n        return wb\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_histogram_int_weights(self, xp, dtype):\n        # Check with integer weights\n        v = xp.asarray([1, 2, 2, 4], dtype=dtype)\n        w = xp.asarray([4, 3, 2, 1], dtype=dtype)\n        wa, wb = xp.histogram(v, bins=4, weights=w)\n        testing.assert_array_equal(wa, [4, 5, 0, 1])\n        return wa, wb\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_histogram_int_weights_normalized(self, xp, dtype):\n        v = xp.asarray([1, 2, 2, 4], dtype=dtype)\n        w = xp.asarray([4, 3, 2, 1], dtype=dtype)\n        wa, wb = xp.histogram(v, bins=4, weights=w, density=True)\n        testing.assert_array_almost_equal(\n            wa, xp.asarray([4, 5, 0, 1]) / 10. / 3. * 4)\n        return wb\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_histogram_int_weights_nonuniform_bins(self, xp, dtype):\n        # Check weights with non-uniform bin widths\n        a, b = xp.histogram(\n            xp.arange(9, dtype=dtype),\n            xp.asarray([0, 1, 3, 6, 10], dtype=dtype),\n            weights=xp.asarray([2, 1, 1, 1, 1, 1, 1, 1, 1], dtype=dtype),\n            density=True)\n        testing.assert_array_almost_equal(a, [.2, .1, .1, .075])\n        return a, b\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test_histogram_complex_weights(self, xp, dtype):\n        values = xp.asarray([1.3, 2.5, 2.3])\n        weights = xp.asarray([1, -1, 2]) + 1j * xp.asarray([2, 1, 2])\n        weights = weights.astype(dtype)\n        a, b = xp.histogram(\n            values, bins=2, weights=weights)\n        return a, b\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_array_list_equal()\n    def test_histogram_complex_weights_uneven_bins(self, xp, dtype):\n        values = xp.asarray([1.3, 2.5, 2.3])\n        weights = xp.asarray([1, -1, 2]) + 1j * xp.asarray([2, 1, 2])\n        weights = weights.astype(dtype)\n        a, b = xp.histogram(\n            values, bins=xp.asarray([0, 2, 3]), weights=weights)\n        return a, b\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_histogram_empty(self, xp, dtype):\n        x = xp.array([], dtype)\n        y, bin_edges = xp.histogram(x)\n        return y, bin_edges\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_histogram_int_bins(self, xp, dtype):\n        x = testing.shaped_arange((10,), xp, dtype)\n        y, bin_edges = xp.histogram(x, 4)\n        return y, bin_edges\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_histogram_array_bins(self, xp, dtype):\n        x = testing.shaped_arange((10,), xp, dtype)\n        bins = testing.shaped_arange((3,), xp, dtype)\n        y, bin_edges = xp.histogram(x, bins)\n        return y, bin_edges\n\n    # numpy 1.13.1 does not check this error correctly with unsigned int.\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    def test_histogram_bins_not_ordered(self, dtype):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((10,), xp, dtype)\n            bins = xp.array([1, 3, 2], dtype)\n            with pytest.raises(ValueError):\n                xp.histogram(x, bins)\n\n    @for_all_dtypes_bincount()\n    @testing.numpy_cupy_allclose(accept_error=TypeError)\n    def test_bincount(self, xp, dtype):\n        x = testing.shaped_arange((3,), xp, dtype)\n        return xp.bincount(x)\n\n    @for_all_dtypes_bincount()\n    @testing.numpy_cupy_allclose(accept_error=TypeError)\n    def test_bincount_duplicated_value(self, xp, dtype):\n        x = xp.array([1, 2, 2, 1, 2, 4], dtype)\n        return xp.bincount(x)\n\n    @for_all_dtypes_combination_bincount(names=['x_type', 'w_type'])\n    @testing.numpy_cupy_allclose(accept_error=TypeError)\n    def test_bincount_with_weight(self, xp, x_type, w_type):\n        x = testing.shaped_arange((3,), xp, x_type)\n        w = testing.shaped_arange((3,), xp, w_type)\n        return xp.bincount(x, weights=w)\n\n    @for_all_dtypes_bincount()\n    @testing.numpy_cupy_allclose(accept_error=TypeError)\n    def test_bincount_with_minlength(self, xp, dtype):\n        x = testing.shaped_arange((3,), xp, dtype)\n        return xp.bincount(x, minlength=5)\n\n    @for_all_dtypes_combination_bincount(names=['x_type', 'w_type'])\n    def test_bincount_invalid_weight_length(self, x_type, w_type):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((1,), xp, x_type)\n            w = testing.shaped_arange((2,), xp, w_type)\n            # TODO(imanishi): Split this test into a test for ValueError and\n            # a test for TypeError.\n            with pytest.raises((ValueError, TypeError)):\n                xp.bincount(x, weights=w)\n\n    @for_signed_dtypes_bincount()\n    def test_bincount_negative(self, dtype):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((3,), xp, dtype) - 2\n            with pytest.raises(ValueError):\n                xp.bincount(x)\n\n    @for_all_dtypes_bincount()\n    def test_bincount_too_deep(self, dtype):\n        for xp in (numpy, cupy):\n            x = xp.array([[1]], dtype)\n            with pytest.raises(ValueError):\n                xp.bincount(x)\n\n    @for_all_dtypes_bincount()\n    def test_bincount_too_small(self, dtype):\n        for xp in (numpy, cupy):\n            x = xp.zeros((), dtype)\n            with pytest.raises(ValueError):\n                xp.bincount(x)\n\n    @for_all_dtypes_bincount()\n    @testing.numpy_cupy_allclose(accept_error=TypeError)\n    def test_bincount_zero(self, xp, dtype):\n        x = testing.shaped_arange((3,), xp, dtype)\n        return xp.bincount(x, minlength=0)\n\n    @for_all_dtypes_bincount()\n    def test_bincount_too_small_minlength(self, dtype):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((3,), xp, dtype)\n            # TODO(imanishi): Split this test into a test for ValueError and\n            # a test for TypeError.\n            with pytest.raises((ValueError, TypeError)):\n                xp.bincount(x, minlength=-1)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product(\n    {'bins': [\n        # Test monotonically increasing with in-bounds values\n        [1.5, 2.5, 4.0, 6.0],\n        # Explicit out-of-bounds for x values\n        [-1.0, 1.0, 2.5, 4.0, 20.0],\n        # Repeated values should yield right-most or left-most indexes\n        [0.0, 1.0, 1.0, 4.0, 4.0, 10.0],\n    ],\n        'increasing': [True, False],\n        'right': [True, False],\n        'shape': [(), (10,), (6, 3, 3)]})\n)\nclass TestDigitize(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_list_equal()\n    def test_digitize(self, xp, dtype):\n        x = testing.shaped_arange(self.shape, xp, dtype)\n        bins = self.bins\n        if not self.increasing:\n            bins = bins[::-1]\n        bins = xp.array(bins)\n        y = xp.digitize(x, bins, right=self.right)\n        return y,\n\n\n@testing.gpu\n@testing.parameterize(\n    {'right': True},\n    {'right': False})\nclass TestDigitizeNanInf(unittest.TestCase):\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_digitize_nan(self, xp):\n        x = testing.shaped_arange((14,), xp, xp.float32)\n        x[5] = float('nan')\n        bins = xp.array([1.0, 3.0, 5.0, 8.0, 12.0], xp.float32)\n        y = xp.digitize(x, bins, right=self.right)\n        return y,\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_digitize_nan_bins(self, xp):\n        x = testing.shaped_arange((14,), xp, xp.float32)\n        bins = xp.array([1.0, 3.0, 5.0, 8.0, float('nan')], xp.float32)\n        y = xp.digitize(x, bins, right=self.right)\n        return y,\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_digitize_nan_bins_repeated(self, xp):\n        x = testing.shaped_arange((14,), xp, xp.float32)\n        x[5] = float('nan')\n        bins = [1.0, 3.0, 5.0, 8.0, float('nan'), float('nan')]\n        bins = xp.array(bins, xp.float32)\n        y = xp.digitize(x, bins, right=self.right)\n        return y,\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_digitize_nan_bins_decreasing(self, xp):\n        x = testing.shaped_arange((14,), xp, xp.float32)\n        x[5] = float('nan')\n        bins = [float('nan'), 8.0, 5.0, 3.0, 1.0]\n        bins = xp.array(bins, xp.float32)\n        y = xp.digitize(x, bins, right=self.right)\n        return y,\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_digitize_nan_bins_decreasing_repeated(self, xp):\n        x = testing.shaped_arange((14,), xp, xp.float32)\n        x[5] = float('nan')\n        bins = [float('nan'), float('nan'), float('nan'), 5.0, 3.0, 1.0]\n        bins = xp.array(bins, xp.float32)\n        y = xp.digitize(x, bins, right=self.right)\n        return y,\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_digitize_all_nan_bins(self, xp):\n        x = testing.shaped_arange((14,), xp, xp.float32)\n        x[5] = float('nan')\n        bins = [float('nan'), float('nan'), float('nan'), float('nan')]\n        bins = xp.array(bins, xp.float32)\n        y = xp.digitize(x, bins, right=self.right)\n        return y,\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_searchsorted_inf(self, xp):\n        x = testing.shaped_arange((14,), xp, xp.float64)\n        x[5] = float('inf')\n        bins = xp.array([0, 1, 2, 4, 10])\n        y = xp.digitize(x, bins, right=self.right)\n        return y,\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_searchsorted_minf(self, xp):\n        x = testing.shaped_arange((14,), xp, xp.float64)\n        x[5] = float('-inf')\n        bins = xp.array([0, 1, 2, 4, 10])\n        y = xp.digitize(x, bins, right=self.right)\n        return y,\n\n\n@testing.gpu\nclass TestDigitizeInvalid(unittest.TestCase):\n\n    def test_digitize_complex(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((14,), xp, xp.complex)\n            bins = xp.array([1.0, 3.0, 5.0, 8.0, 12.0], xp.complex)\n            with pytest.raises(TypeError):\n                xp.digitize(x, bins)\n\n    def test_digitize_nd_bins(self):\n        for xp in (numpy, cupy):\n            x = testing.shaped_arange((14,), xp, xp.float64)\n            bins = xp.array([[1], [2]])\n            with pytest.raises(ValueError):\n                xp.digitize(x, bins)\n"""
tests/cupy_tests/statics_tests/test_meanvar.py,0,"b'import unittest\nimport pytest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\n\nignore_runtime_warnings = pytest.mark.filterwarnings(\n    ""ignore"", category=RuntimeWarning)\n\n\n@testing.gpu\nclass TestMedian(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_median_noaxis(self, xp, dtype):\n        a = testing.shaped_random((3, 4, 5), xp, dtype)\n        return xp.median(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_median_axis1(self, xp, dtype):\n        a = testing.shaped_random((3, 4, 5), xp, dtype)\n        return xp.median(a, axis=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_median_axis2(self, xp, dtype):\n        a = testing.shaped_random((3, 4, 5), xp, dtype)\n        return xp.median(a, axis=2)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_median_overwrite_input(self, xp, dtype):\n        a = testing.shaped_random((3, 4, 5), xp, dtype)\n        return xp.median(a, overwrite_input=True)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_median_keepdims_axis1(self, xp, dtype):\n        a = testing.shaped_random((3, 4, 5), xp, dtype)\n        return xp.median(a, axis=1, keepdims=True)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_median_keepdims_noaxis(self, xp, dtype):\n        a = testing.shaped_random((3, 4, 5), xp, dtype)\n        return xp.median(a, keepdims=True)\n\n    def test_median_invalid_axis(self):\n        for xp in [numpy, cupy]:\n            a = testing.shaped_random((3, 4, 5), xp)\n            with pytest.raises(numpy.AxisError):\n                return xp.median(a, -a.ndim - 1, keepdims=False)\n\n            with pytest.raises(numpy.AxisError):\n                return xp.median(a, a.ndim, keepdims=False)\n\n            with pytest.raises(numpy.AxisError):\n                return xp.median(a, (-a.ndim - 1, 1), keepdims=False)\n\n            with pytest.raises(numpy.AxisError):\n                return xp.median(a, (0, a.ndim,), keepdims=False)\n\n\n@testing.parameterize(\n    *testing.product({\n        \'shape\': [(3, 4, 5)],\n        \'axis\': [(0, 1), (0, -1), (1, 2), (1,)],\n        \'keepdims\': [True, False]\n    })\n)\n@testing.gpu\nclass TestMedianAxis(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_median_axis_sequence(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        return xp.median(a, self.axis, keepdims=self.keepdims)\n\n\n@testing.gpu\nclass TestAverage(unittest.TestCase):\n\n    _multiprocess_can_split_ = True\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_average_all(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return xp.average(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_average_axis(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.average(a, axis=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_average_weights(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        w = testing.shaped_arange((2, 3), xp, dtype)\n        return xp.average(a, weights=w)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_average_axis_weights(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        w = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.average(a, axis=2, weights=w)\n\n    def check_returned(self, a, axis, weights):\n        average_cpu, sum_weights_cpu = numpy.average(\n            a, axis, weights, returned=True)\n        result = cupy.average(\n            cupy.asarray(a), axis, weights, returned=True)\n        self.assertTrue(isinstance(result, tuple))\n        self.assertEqual(len(result), 2)\n        average_gpu, sum_weights_gpu = result\n        testing.assert_allclose(average_cpu, average_gpu)\n        testing.assert_allclose(sum_weights_cpu, sum_weights_gpu)\n\n    @testing.for_all_dtypes()\n    def test_returned(self, dtype):\n        a = testing.shaped_arange((2, 3), numpy, dtype)\n        w = testing.shaped_arange((2, 3), numpy, dtype)\n        self.check_returned(a, axis=1, weights=None)\n        self.check_returned(a, axis=None, weights=w)\n        self.check_returned(a, axis=1, weights=w)\n\n\n@testing.gpu\nclass TestMeanVar(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_mean_all(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return a.mean()\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_external_mean_all(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return xp.mean(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_mean_axis(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return a.mean(axis=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_external_mean_axis(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.mean(a, axis=1)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_mean_all_float64_dtype(self, xp, dtype):\n        a = xp.full((2, 3, 4), 123456789, dtype=dtype)\n        return xp.mean(a, dtype=numpy.float64)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_mean_all_int64_dtype(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.mean(a, dtype=numpy.int64)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_mean_all_complex_dtype(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.mean(a, dtype=numpy.complex64)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_var_all(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return a.var()\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_external_var_all(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return xp.var(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_var_all_ddof(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return a.var(ddof=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_external_var_all_ddof(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return xp.var(a, ddof=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_var_axis(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return a.var(axis=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_external_var_axis(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.var(a, axis=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_var_axis_ddof(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return a.var(axis=1, ddof=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_external_var_axis_ddof(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.var(a, axis=1, ddof=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_std_all(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return a.std()\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_external_std_all(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return xp.std(a)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_std_all_ddof(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return a.std(ddof=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_external_std_all_ddof(self, xp, dtype):\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return xp.std(a, ddof=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_std_axis(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return a.std(axis=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_external_std_axis(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.std(a, axis=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_std_axis_ddof(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return a.std(axis=1, ddof=1)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_external_std_axis_ddof(self, xp, dtype):\n        a = testing.shaped_arange((2, 3, 4), xp, dtype)\n        return xp.std(a, axis=1, ddof=1)\n\n\n@testing.parameterize(\n    *testing.product({\n        \'shape\': [(3, 4), (30, 40, 50)],\n        \'axis\': [None, 0, 1],\n        \'keepdims\': [True, False]\n    })\n)\n@testing.gpu\nclass TestNanMean(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_nanmean_without_nan(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n        return xp.nanmean(a, axis=self.axis, keepdims=self.keepdims)\n\n    @ignore_runtime_warnings\n    @testing.for_all_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_nanmean_with_nan_float(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype)\n\n        if a.dtype.kind not in \'biu\':\n            a[1, :] = xp.nan\n            a[:, 3] = xp.nan\n\n        return xp.nanmean(a, axis=self.axis, keepdims=self.keepdims)\n\n\n@testing.gpu\nclass TestNanMeanAdditional(unittest.TestCase):\n\n    @ignore_runtime_warnings\n    @testing.for_all_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_nanmean_out(self, xp, dtype):\n        a = testing.shaped_random((10, 20, 30), xp, dtype)\n        z = xp.zeros((20, 30), dtype=dtype)\n\n        if a.dtype.kind not in \'biu\':\n            a[1, :] = xp.nan\n            a[:, 3] = xp.nan\n\n        xp.nanmean(a, axis=0, out=z)\n        return z\n\n    @testing.slow\n    @testing.for_all_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_nanmean_huge(self, xp, dtype):\n        a = testing.shaped_random((1024, 512), xp, dtype)\n\n        if a.dtype.kind not in \'biu\':\n            a[:512, :256] = xp.nan\n\n        return xp.nanmean(a, axis=1)\n\n    @testing.numpy_cupy_allclose(rtol=1e-4)\n    def test_nanmean_float16(self, xp):\n        a = testing.shaped_arange((2, 3), xp, numpy.float16)\n        a[0][0] = xp.nan\n        return xp.nanmean(a)\n\n    @ignore_runtime_warnings\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_nanmean_all_nan(self, xp):\n        a = xp.zeros((3, 4))\n        a[:] = xp.nan\n        return xp.nanmean(a)\n\n\n@testing.parameterize(\n    *testing.product({\n        \'shape\': [(3, 4), (4, 3, 5)],\n        \'axis\': [None, 0, 1],\n        \'keepdims\': [True, False],\n        \'ddof\': [0, 1]\n    }))\n@testing.gpu\nclass TestNanVarStd(unittest.TestCase):\n\n    @ignore_runtime_warnings\n    @testing.for_all_dtypes(no_float16=True, no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_nanvar(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype=dtype)\n        if a.dtype.kind not in \'biu\':\n            a[0, :] = xp.nan\n        return xp.nanvar(\n            a, axis=self.axis, ddof=self.ddof, keepdims=self.keepdims)\n\n    @ignore_runtime_warnings\n    @testing.for_all_dtypes(no_float16=True, no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_nanstd(self, xp, dtype):\n        a = testing.shaped_random(self.shape, xp, dtype=dtype)\n        if a.dtype.kind not in \'biu\':\n            a[0, :] = xp.nan\n        return xp.nanstd(\n            a, axis=self.axis, ddof=self.ddof, keepdims=self.keepdims)\n\n\n@testing.gpu\nclass TestNanVarStdAdditional(unittest.TestCase):\n\n    @ignore_runtime_warnings\n    @testing.for_all_dtypes(no_float16=True, no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_nanvar_out(self, xp, dtype):\n        a = testing.shaped_random((10, 20, 30), xp, dtype)\n        z = xp.zeros((20, 30))\n\n        if a.dtype.kind not in \'biu\':\n            a[1, :] = xp.nan\n            a[:, 3] = xp.nan\n\n        xp.nanvar(a, axis=0, out=z)\n        return z\n\n    @testing.slow\n    @testing.for_all_dtypes(no_float16=True, no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_nanvar_huge(self, xp, dtype):\n        a = testing.shaped_random((1024, 512), xp, dtype)\n\n        if a.dtype.kind not in \'biu\':\n            a[:512, :256] = xp.nan\n\n        return xp.nanvar(a, axis=1)\n\n    @testing.numpy_cupy_allclose(rtol=1e-4)\n    def test_nanvar_float16(self, xp):\n        a = testing.shaped_arange((4, 5), xp, numpy.float16)\n        a[0][0] = xp.nan\n        return xp.nanvar(a, axis=0)\n\n    @ignore_runtime_warnings\n    @testing.for_all_dtypes(no_float16=True, no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_nanstd_out(self, xp, dtype):\n        a = testing.shaped_random((10, 20, 30), xp, dtype)\n        z = xp.zeros((20, 30))\n\n        if a.dtype.kind not in \'biu\':\n            a[1, :] = xp.nan\n            a[:, 3] = xp.nan\n\n        xp.nanstd(a, axis=0, out=z)\n        return z\n\n    @testing.slow\n    @testing.for_all_dtypes(no_float16=True, no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_nanstd_huge(self, xp, dtype):\n        a = testing.shaped_random((1024, 512), xp, dtype)\n\n        if a.dtype.kind not in \'biu\':\n            a[:512, :256] = xp.nan\n\n        return xp.nanstd(a, axis=1)\n\n    @testing.numpy_cupy_allclose(rtol=1e-4)\n    def test_nanstd_float16(self, xp):\n        a = testing.shaped_arange((4, 5), xp, numpy.float16)\n        a[0][0] = xp.nan\n        return xp.nanstd(a, axis=1)\n\n\n@testing.parameterize(*testing.product({\n    \'params\': [\n        ((), None),\n        ((0,), None),\n        ((0, 0), None),\n        ((0, 0), 1),\n        ((0, 0, 0), None),\n        ((0, 0, 0), (0, 2)),\n    ],\n    \'func\': [\'mean\', \'std\', \'var\'],\n}))\n@testing.gpu\nclass TestProductZeroLength(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_external_mean_zero_len(self, xp, dtype):\n        shape, axis = self.params\n        a = testing.shaped_arange(shape, xp, dtype)\n        f = getattr(xp, self.func)\n        return f(a, axis=axis)\n'"
tests/cupy_tests/statics_tests/test_order.py,0,"b""import unittest\nimport warnings\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\n\n\n_all_interpolations = (\n    'lower',\n    'higher',\n    'midpoint',\n    # 'nearest', # TODO(hvy): Not implemented\n    'linear')\n\n\ndef for_all_interpolations(name='interpolation'):\n    return testing.for_orders(_all_interpolations, name=name)\n\n\n@testing.gpu\nclass TestOrder(unittest.TestCase):\n\n    @for_all_interpolations()\n    @testing.for_all_dtypes(no_float16=True, no_bool=True, no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_percentile_defaults(self, xp, dtype, interpolation):\n        a = testing.shaped_random((2, 3, 8), xp, dtype)\n        q = testing.shaped_random((3,), xp, dtype=dtype, scale=100)\n        return xp.percentile(a, q, interpolation=interpolation)\n\n    @for_all_interpolations()\n    @testing.for_all_dtypes(no_float16=True, no_bool=True, no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_percentile_no_axis(self, xp, dtype, interpolation):\n        a = testing.shaped_random((10, 2, 4, 8), xp, dtype)\n        q = testing.shaped_random((5,), xp, dtype=dtype, scale=100)\n        return xp.percentile(a, q, axis=None, interpolation=interpolation)\n\n    @for_all_interpolations()\n    @testing.for_all_dtypes(no_float16=True, no_bool=True, no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_percentile_neg_axis(self, xp, dtype, interpolation):\n        a = testing.shaped_random((4, 3, 10, 2, 8), xp, dtype)\n        q = testing.shaped_random((5,), xp, dtype=dtype, scale=100)\n        return xp.percentile(a, q, axis=-1, interpolation=interpolation)\n\n    @for_all_interpolations()\n    @testing.for_all_dtypes(no_float16=True, no_bool=True, no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_percentile_tuple_axis(self, xp, dtype, interpolation):\n        a = testing.shaped_random((1, 6, 3, 2), xp, dtype)\n        q = testing.shaped_random((5,), xp, dtype=dtype, scale=100)\n        return xp.percentile(a, q, axis=(0, 1, 2), interpolation=interpolation)\n\n    @for_all_interpolations()\n    @testing.for_all_dtypes(no_float16=True, no_bool=True, no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_percentile_scalar_q(self, xp, dtype, interpolation):\n        a = testing.shaped_random((2, 3, 8), xp, dtype)\n        q = 13.37\n        return xp.percentile(a, q, interpolation=interpolation)\n\n    @for_all_interpolations()\n    @testing.for_all_dtypes(no_float16=True, no_bool=True, no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-5)\n    def test_percentile_keepdims(self, xp, dtype, interpolation):\n        a = testing.shaped_random((7, 2, 9, 2), xp, dtype)\n        q = testing.shaped_random((5,), xp, dtype=dtype, scale=100)\n        return xp.percentile(\n            a, q, axis=None, keepdims=True, interpolation=interpolation)\n\n    @for_all_interpolations()\n    @testing.for_float_dtypes(no_float16=True)  # NumPy raises error on int8\n    @testing.numpy_cupy_allclose(rtol=1e-6)\n    def test_percentile_out(self, xp, dtype, interpolation):\n        a = testing.shaped_random((10, 2, 3, 2), xp, dtype)\n        q = testing.shaped_random((5,), xp, dtype=dtype, scale=100)\n        out = testing.shaped_random((5, 10, 2, 3), xp, dtype)\n        return xp.percentile(\n            a, q, axis=-1, interpolation=interpolation, out=out)\n\n    @for_all_interpolations()\n    @testing.for_all_dtypes(no_float16=True, no_bool=True, no_complex=True)\n    def test_percentile_bad_q(self, dtype, interpolation):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((4, 2, 3, 2), xp, dtype)\n            q = testing.shaped_random((1, 2, 3), xp, dtype=dtype, scale=100)\n            with pytest.raises(ValueError):\n                xp.percentile(a, q, axis=-1, interpolation=interpolation)\n\n    @testing.for_all_dtypes(no_float16=True, no_bool=True, no_complex=True)\n    def test_percentile_uxpected_interpolation(self, dtype):\n        for xp in (numpy, cupy):\n            a = testing.shaped_random((4, 2, 3, 2), xp, dtype)\n            q = testing.shaped_random((5,), xp, dtype=dtype, scale=100)\n            with pytest.raises(ValueError):\n                xp.percentile(a, q, axis=-1, interpolation='deadbeef')\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanmax_all(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return xp.nanmax(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanmax_axis_large(self, xp, dtype):\n        a = testing.shaped_random((3, 1000), xp, dtype)\n        return xp.nanmax(a, axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanmax_axis0(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return xp.nanmax(a, axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanmax_axis1(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return xp.nanmax(a, axis=1)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanmax_axis2(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return xp.nanmax(a, axis=2)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_nanmax_nan(self, xp, dtype):\n        a = xp.array([float('nan'), 1, -1], dtype)\n        with warnings.catch_warnings():\n            return xp.nanmax(a)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_nanmax_all_nan(self, xp, dtype):\n        a = xp.array([float('nan'), float('nan')], dtype)\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter('always')\n            m = xp.nanmax(a)\n        self.assertEqual(len(w), 1)\n        self.assertIs(w[0].category, RuntimeWarning)\n        return m\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanmin_all(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return xp.nanmin(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanmin_axis_large(self, xp, dtype):\n        a = testing.shaped_random((3, 1000), xp, dtype)\n        return xp.nanmin(a, axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanmin_axis0(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return xp.nanmin(a, axis=0)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanmin_axis1(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return xp.nanmin(a, axis=1)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose()\n    def test_nanmin_axis2(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return xp.nanmin(a, axis=2)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_nanmin_nan(self, xp, dtype):\n        a = xp.array([float('nan'), 1, -1], dtype)\n        with warnings.catch_warnings():\n            return xp.nanmin(a)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_nanmin_all_nan(self, xp, dtype):\n        a = xp.array([float('nan'), float('nan')], dtype)\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter('always')\n            m = xp.nanmin(a)\n        self.assertEqual(len(w), 1)\n        self.assertIs(w[0].category, RuntimeWarning)\n        return m\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_ptp_all(self, xp, dtype):\n        a = testing.shaped_random((2, 3), xp, dtype)\n        return xp.ptp(a)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_ptp_axis_large(self, xp, dtype):\n        a = testing.shaped_random((3, 1000), xp, dtype)\n        return xp.ptp(a, axis=0)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_ptp_axis0(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return xp.ptp(a, axis=0)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_ptp_axis1(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return xp.ptp(a, axis=1)\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose()\n    def test_ptp_axis2(self, xp, dtype):\n        a = testing.shaped_random((2, 3, 4), xp, dtype)\n        return xp.ptp(a, axis=2)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_ptp_nan(self, xp, dtype):\n        a = xp.array([float('nan'), 1, -1], dtype)\n        return xp.ptp(a)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose()\n    def test_ptp_all_nan(self, xp, dtype):\n        a = xp.array([float('nan'), float('nan')], dtype)\n        return xp.ptp(a)\n"""
tests/cupy_tests/testing_tests/__init__.py,0,b''
tests/cupy_tests/testing_tests/test_array.py,0,"b""import copy\nimport unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\n\n\n@testing.parameterize(\n    *testing.product({\n        'assertion': ['assert_allclose', 'assert_array_almost_equal',\n                      'assert_array_almost_equal_nulp',\n                      'assert_array_max_ulp', 'assert_array_equal'],\n        'array_module_x': [numpy, cupy],\n        'array_module_y': [numpy, cupy]\n    })\n)\n@testing.gpu\nclass TestEqualityAssertion(unittest.TestCase):\n\n    def setUp(self):\n        self.assertion = getattr(testing, self.assertion)\n        val = numpy.random.uniform(-1, 1, (2, 3))\n        self.x = self.array_module_x.array(val, val.dtype, copy=True)\n        self.y = self.array_module_y.array(val, val.dtype, copy=True)\n\n    def test_equality(self):\n        self.assertion(self.x, self.y)\n\n    def test_inequality(self):\n        self.y += 1\n        with self.assertRaises(AssertionError):\n            self.assertion(self.x, self.y)\n\n\ndef _convert_array(xs, array_module):\n    if array_module == 'all_numpy':\n        return xs\n    elif array_module == 'all_cupy':\n        return [\n            cupy.asarray(x)\n            for x in xs\n        ]\n    else:\n        return [\n            cupy.asarray(x) if numpy.random.randint(0, 2) else x\n            for x in xs\n        ]\n\n\n@testing.parameterize(\n    *testing.product({\n        'array_module_x': ['all_numpy', 'all_cupy', 'random'],\n        'array_module_y': ['all_numpy', 'all_cupy', 'random']\n    })\n)\n@testing.gpu\nclass TestListEqualityAssertion(unittest.TestCase):\n\n    def setUp(self):\n        xs = [numpy.random.uniform(-1, 1, (2, 3)) for _ in range(10)]\n        ys = copy.deepcopy(xs)\n        self.xs = _convert_array(xs, self.array_module_x)\n        self.ys = _convert_array(ys, self.array_module_y)\n\n    def test_equality_numpy(self):\n        testing.assert_array_list_equal(self.xs, self.ys)\n\n    def test_inequality_numpy(self):\n        self.xs[0] += 1\n        with self.assertRaisesRegex(\n                AssertionError, '^\\nArrays are not equal'):\n            testing.assert_array_list_equal(self.xs, self.ys)\n\n\n@testing.parameterize(\n    *testing.product({\n        'array_module_x': [numpy, cupy],\n        'array_module_y': [numpy, cupy]\n    })\n)\n@testing.gpu\nclass TestStridesEqualityAssertion(unittest.TestCase):\n\n    def setUp(self):\n        val = numpy.random.uniform(-1, 1, (2, 3))\n        self.x = self.array_module_x.array(val, val.dtype, copy=True)\n        self.y = self.array_module_y.array(val, val.dtype, copy=True)\n\n    def test_equality_numpy(self):\n        testing.assert_array_equal(self.x, self.y, strides_check=True)\n\n    def test_inequality_numpy(self):\n        self.y = self.array_module_y.asfortranarray(self.y)\n        with self.assertRaises(AssertionError):\n            testing.assert_array_equal(self.x, self.y, strides_check=True)\n\n\n@testing.parameterize(\n    *testing.product({\n        'array_module_x': [numpy, cupy],\n        'array_module_y': [numpy, cupy]\n    })\n)\n@testing.gpu\nclass TestLessAssertion(unittest.TestCase):\n\n    def setUp(self):\n        val = numpy.random.uniform(-1, 1, (2, 3))\n        self.x = self.array_module_x.array(val, val.dtype, copy=True)\n        self.y = self.array_module_y.array(val + 1, val.dtype, copy=True)\n\n    def test_equality_numpy(self):\n        testing.assert_array_less(self.x, self.y)\n\n    def test_inequality_numpy(self):\n        self.x[0] += 100\n        with self.assertRaises(AssertionError):\n            testing.assert_array_less(self.x, self.y)\n"""
tests/cupy_tests/testing_tests/test_condition.py,0,"b""import unittest\n\nfrom cupy.testing import condition\n\n\nSKIP_REASON = 'test skip reason'\n\n\n# The test fixtures of this TestCase is used to be decorated by\n# decorator in test. So we do not run them alone.\nclass MockUnitTest(unittest.TestCase):\n\n    failure_case_counter = 0\n    success_case_counter = 0\n    skip_case_counter = 0\n    probabilistic_case_counter = 0\n    probabilistic_case_success_counter = 0\n    probabilistic_case_failure_counter = 0\n\n    @staticmethod\n    def clear_counter():\n        MockUnitTest.failure_case_counter = 0\n        MockUnitTest.success_case_counter = 0\n        MockUnitTest.skip_case_counter = 0\n        MockUnitTest.probabilistic_case_counter = 0\n        MockUnitTest.probabilistic_case_success_counter = 0\n        MockUnitTest.probabilistic_case_failure_counter = 0\n\n    def failure_case(self):\n        MockUnitTest.failure_case_counter += 1\n        self.fail()\n\n    def success_case(self):\n        MockUnitTest.success_case_counter += 1\n        self.assertTrue(True)\n\n    def skip_case(self):\n        MockUnitTest.skip_case_counter += 1\n        self.skipTest(SKIP_REASON)\n\n    def error_case(self):\n        raise Exception()\n\n    def probabilistic_case(self):\n        MockUnitTest.probabilistic_case_counter += 1\n        if MockUnitTest.probabilistic_case_counter % 2 == 0:\n            MockUnitTest.probabilistic_case_success_counter += 1\n            self.assertTrue(True)\n        else:\n            MockUnitTest.probabilistic_case_failure_counter += 1\n            self.fail()\n\n    def runTest(self):\n        pass\n\n\ndef _should_fail(self, f):\n    try:\n        f(self.unit_test)\n        self.fail(\n            'AssertionError is expected to be raised, but none is raised')\n    except AssertionError as e:\n        # check if the detail is included in the error object\n        self.assertIn('first error message:', str(e))\n\n\ndef _should_pass(self, f):\n    f(self.unit_test)\n\n\ndef _should_skip(self, f):\n    try:\n        f(self.unit_test)\n        self.fail(\n            'SkipTest is expected to be raised, but none is raised')\n    except unittest.SkipTest as e:\n        self.assertIn(SKIP_REASON, str(e))\n\n\nclass TestRepeatWithSuccessAtLeast(unittest.TestCase):\n\n    def _decorate(self, f, times, min_success):\n        return condition.repeat_with_success_at_least(\n            times, min_success)(f)\n\n    def setUp(self):\n        self.unit_test = MockUnitTest()\n        MockUnitTest.clear_counter()\n\n    def test_all_trials_fail(self):\n        f = self._decorate(MockUnitTest.failure_case, 10, 1)\n        _should_fail(self, f)\n        self.assertEqual(self.unit_test.failure_case_counter, 10)\n\n    def test_all_trials_fail2(self):\n        f = self._decorate(MockUnitTest.failure_case, 10, 0)\n        _should_pass(self, f)\n        self.assertLessEqual(self.unit_test.failure_case_counter, 10)\n\n    def test_all_trials_error(self):\n        f = self._decorate(MockUnitTest.error_case, 10, 1)\n        _should_fail(self, f)\n\n    def test_all_trials_succeed(self):\n        f = self._decorate(MockUnitTest.success_case, 10, 10)\n        _should_pass(self, f)\n        self.assertEqual(self.unit_test.success_case_counter, 10)\n\n    def test_all_trials_succeed2(self):\n        self.assertRaises(AssertionError,\n                          condition.repeat_with_success_at_least,\n                          10, 11)\n\n    def test_half_of_trials_succeed(self):\n        f = self._decorate(MockUnitTest.probabilistic_case, 10, 5)\n        _should_pass(self, f)\n        self.assertLessEqual(self.unit_test.probabilistic_case_counter, 10)\n        self.assertGreaterEqual(\n            self.unit_test.probabilistic_case_success_counter, 5)\n        self.assertLessEqual(\n            self.unit_test.probabilistic_case_failure_counter, 5)\n\n    def test_half_of_trials_succeed2(self):\n        f = self._decorate(MockUnitTest.probabilistic_case, 10, 6)\n        _should_fail(self, f)\n        self.assertLessEqual(self.unit_test.probabilistic_case_counter, 10)\n        self.assertLess(\n            self.unit_test.probabilistic_case_success_counter, 6)\n        self.assertGreaterEqual(\n            self.unit_test.probabilistic_case_failure_counter, 5)\n\n\nclass TestRepeat(unittest.TestCase):\n\n    def _decorate(self, f, times):\n        return condition.repeat(times)(f)\n\n    def setUp(self):\n        self.unit_test = MockUnitTest()\n        MockUnitTest.clear_counter()\n\n    def test_failure_case(self):\n        f = self._decorate(MockUnitTest.failure_case, 10)\n        _should_fail(self, f)\n        self.assertLessEqual(self.unit_test.failure_case_counter, 10)\n\n    def test_success_case(self):\n        f = self._decorate(MockUnitTest.success_case, 10)\n        _should_pass(self, f)\n        self.assertEqual(self.unit_test.success_case_counter, 10)\n\n    def test_skip_case(self):\n        f = self._decorate(MockUnitTest.skip_case, 10)\n        _should_skip(self, f)\n        self.assertEqual(self.unit_test.skip_case_counter, 1)\n\n    def test_probabilistic_case(self):\n        f = self._decorate(MockUnitTest.probabilistic_case, 10)\n        _should_fail(self, f)\n        self.assertLessEqual(self.unit_test.probabilistic_case_counter, 10)\n        self.assertLess(self.unit_test.probabilistic_case_success_counter, 10)\n        self.assertGreater(\n            self.unit_test.probabilistic_case_failure_counter, 0)\n\n\nclass TestRetry(unittest.TestCase):\n\n    def _decorate(self, f, times):\n        return condition.retry(times)(f)\n\n    def setUp(self):\n        self.unit_test = MockUnitTest()\n        MockUnitTest.clear_counter()\n\n    def test_failure_case(self):\n        f = self._decorate(MockUnitTest.failure_case, 10)\n        _should_fail(self, f)\n        self.assertEqual(self.unit_test.failure_case_counter, 10)\n\n    def test_success_case(self):\n        f = self._decorate(MockUnitTest.success_case, 10)\n        _should_pass(self, f)\n        self.assertLessEqual(self.unit_test.success_case_counter, 10)\n\n    def test_skip_case(self):\n        f = self._decorate(MockUnitTest.skip_case, 10)\n        _should_skip(self, f)\n        self.assertEqual(self.unit_test.skip_case_counter, 1)\n\n    def test_probabilistic_case(self):\n        f = self._decorate(MockUnitTest.probabilistic_case, 10)\n        _should_pass(self, f)\n        self.assertLessEqual(\n            self.unit_test.probabilistic_case_counter, 10)\n        self.assertGreater(\n            self.unit_test.probabilistic_case_success_counter, 0)\n        self.assertLess(self.unit_test.probabilistic_case_failure_counter, 10)\n"""
tests/cupy_tests/testing_tests/test_helper.py,0,"b""import re\nimport unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\nfrom cupy.testing import helper\n\n\nclass _Exception1(Exception):\n    pass\n\n\nclass _Exception2(Exception):\n    pass\n\n\nclass TestContainsSignedAndUnsigned(unittest.TestCase):\n\n    def test_include(self):\n        kw = {'x': numpy.int32, 'y': numpy.uint32}\n        self.assertTrue(helper._contains_signed_and_unsigned(kw))\n\n        kw = {'x': numpy.float32, 'y': numpy.uint32}\n        self.assertTrue(helper._contains_signed_and_unsigned(kw))\n\n    def test_signed_only(self):\n        kw = {'x': numpy.int32}\n        self.assertFalse(helper._contains_signed_and_unsigned(kw))\n\n        kw = {'x': numpy.float}\n        self.assertFalse(helper._contains_signed_and_unsigned(kw))\n\n    def test_unsigned_only(self):\n        kw = {'x': numpy.uint32}\n        self.assertFalse(helper._contains_signed_and_unsigned(kw))\n\n\nclass TestCheckCupyNumpyError(unittest.TestCase):\n\n    tbs = {\n        cupy: 'xxxx',\n        numpy: 'yyyy'\n    }\n\n    def test_both_success(self):\n        @testing.helper.numpy_cupy_raises()\n        def dummy_both_success(self, xp):\n            pass\n\n        with self.assertRaises(AssertionError):\n            dummy_both_success(self)\n\n    def test_cupy_error(self):\n        @testing.helper.numpy_cupy_raises()\n        def dummy_cupy_error(self, xp):\n            if xp is cupy:\n                raise Exception(self.tbs.get(cupy))\n\n        with self.assertRaisesRegex(AssertionError, self.tbs.get(cupy)):\n            dummy_cupy_error(self)\n\n    def test_numpy_error(self):\n        @testing.helper.numpy_cupy_raises()\n        def dummy_numpy_error(self, xp):\n            if xp is numpy:\n                raise Exception(self.tbs.get(numpy))\n\n        with self.assertRaisesRegex(AssertionError, self.tbs.get(numpy)):\n            dummy_numpy_error(self)\n\n    def test_cupy_numpy_different_error(self):\n        @testing.helper.numpy_cupy_raises()\n        def dummy_cupy_numpy_different_error(self, xp):\n            if xp is cupy:\n                raise TypeError(self.tbs.get(cupy))\n            elif xp is numpy:\n                raise ValueError(self.tbs.get(numpy))\n\n        # Use re.S mode to ignore new line characters\n        pattern = re.compile(\n            self.tbs.get(cupy) + '.*' + self.tbs.get(numpy), re.S)\n        with self.assertRaisesRegex(AssertionError, pattern):\n            dummy_cupy_numpy_different_error(self)\n\n    def test_cupy_derived_error(self):\n        @testing.helper.numpy_cupy_raises()\n        def dummy_cupy_derived_error(self, xp):\n            if xp is cupy:\n                raise _Exception1(self.tbs.get(cupy))\n            elif xp is numpy:\n                raise _Exception2(self.tbs.get(numpy))\n\n        dummy_cupy_derived_error(self)  # Assert no exceptions\n\n    def test_numpy_derived_error(self):\n        @testing.helper.numpy_cupy_raises()\n        def dummy_numpy_derived_error(self, xp):\n            if xp is cupy:\n                raise Exception(self.tbs.get(cupy))\n            elif xp is numpy:\n                raise IndexError(self.tbs.get(numpy))\n\n        # NumPy errors may not derive from CuPy errors, i.e. CuPy errors should\n        # be at least as explicit as the NumPy error\n        pattern = re.compile(\n            self.tbs.get(cupy) + '.*' + self.tbs.get(numpy), re.S)\n        with self.assertRaisesRegex(AssertionError, pattern):\n            dummy_numpy_derived_error(self)\n\n    def test_same_error(self):\n        @testing.helper.numpy_cupy_raises(accept_error=Exception)\n        def dummy_same_error(self, xp):\n            raise Exception(self.tbs.get(xp))\n\n        dummy_same_error(self)\n\n    def test_cupy_derived_unaccept_error(self):\n        @testing.helper.numpy_cupy_raises(accept_error=ValueError)\n        def dummy_cupy_derived_unaccept_error(self, xp):\n            if xp is cupy:\n                raise IndexError(self.tbs.get(cupy))\n            elif xp is numpy:\n                raise Exception(self.tbs.get(numpy))\n\n        # Neither `IndexError` nor `Exception` is derived from `ValueError`,\n        # therefore expect an error\n        pattern = re.compile(\n            self.tbs.get(cupy) + '.*' + self.tbs.get(numpy), re.S)\n        with self.assertRaisesRegex(AssertionError, pattern):\n            dummy_cupy_derived_unaccept_error(self)\n\n    def test_numpy_derived_unaccept_error(self):\n        @testing.helper.numpy_cupy_raises(accept_error=ValueError)\n        def dummy_numpy_derived_unaccept_error(self, xp):\n            if xp is cupy:\n                raise Exception(self.tbs.get(cupy))\n            elif xp is numpy:\n                raise ValueError(self.tbs.get(numpy))\n\n        # `Exception` is not derived from `ValueError`, therefore expect an\n        # error\n        pattern = re.compile(\n            self.tbs.get(cupy) + '.*' + self.tbs.get(numpy), re.S)\n        with self.assertRaisesRegex(AssertionError, pattern):\n            dummy_numpy_derived_unaccept_error(self)\n\n    def test_forbidden_error(self):\n        @testing.helper.numpy_cupy_raises(accept_error=False)\n        def dummy_forbidden_error(self, xp):\n            raise Exception(self.tbs.get(xp))\n\n        pattern = re.compile(\n            self.tbs.get(cupy) + '.*' + self.tbs.get(numpy), re.S)\n        with self.assertRaisesRegex(AssertionError, pattern):\n            dummy_forbidden_error(self)\n\n    def test_axis_error_different_type(self):\n        @testing.helper.numpy_cupy_raises()\n        def dummy_axis_error(self, xp):\n            if xp is cupy:\n                raise numpy.AxisError(self.tbs.get(cupy))\n            elif xp is numpy:\n                raise TypeError(self.tbs.get(numpy))\n\n        pattern = re.compile(\n            self.tbs.get(cupy) + '.*' + self.tbs.get(numpy), re.S)\n        with self.assertRaisesRegex(AssertionError, pattern):\n            dummy_axis_error(self)\n\n    def test_axis_error_value_different_type(self):\n        @testing.helper.numpy_cupy_raises()\n        def dummy_axis_error(self, xp):\n            if xp is cupy:\n                raise numpy.AxisError(self.tbs.get(cupy))\n            elif xp is numpy:\n                raise ValueError(self.tbs.get(numpy))\n\n        pattern = re.compile(\n            self.tbs.get(cupy) + '.*' + self.tbs.get(numpy), re.S)\n        with self.assertRaisesRegex(AssertionError, pattern):\n            dummy_axis_error(self)\n\n    def test_axis_error_index_different_type(self):\n        @testing.helper.numpy_cupy_raises()\n        def dummy_axis_error(self, xp):\n            if xp is cupy:\n                raise numpy.AxisError(self.tbs.get(cupy))\n            elif xp is numpy:\n                raise IndexError(self.tbs.get(numpy))\n\n        pattern = re.compile(\n            self.tbs.get(cupy) + '.*' + self.tbs.get(numpy), re.S)\n        with self.assertRaisesRegex(AssertionError, pattern):\n            dummy_axis_error(self)\n\n\nclass NumPyCuPyDecoratorBase(object):\n\n    def test_valid(self):\n        decorator = getattr(testing, self.decorator)()\n        decorated_func = decorator(type(self).valid_func)\n        decorated_func(self)\n\n    def test_invalid(self):\n        decorator = getattr(testing, self.decorator)()\n        decorated_func = decorator(type(self).invalid_func)\n        with self.assertRaises(AssertionError):\n            decorated_func(self)\n\n    def test_name(self):\n        decorator = getattr(testing, self.decorator)(name='foo')\n        decorated_func = decorator(type(self).strange_kw_func)\n        decorated_func(self)\n\n\ndef numpy_error(_, xp):\n    if xp == numpy:\n        raise ValueError()\n    elif xp == cupy:\n        return cupy.array(1)\n\n\ndef cupy_error(_, xp):\n    if xp == numpy:\n        return numpy.array(1)\n    elif xp == cupy:\n        raise ValueError()\n\n\n@testing.gpu\nclass NumPyCuPyDecoratorBase2(object):\n\n    def test_accept_error_numpy(self):\n        decorator = getattr(testing, self.decorator)(accept_error=False)\n        decorated_func = decorator(numpy_error)\n        with self.assertRaises(AssertionError):\n            decorated_func(self)\n\n    def test_accept_error_cupy(self):\n        decorator = getattr(testing, self.decorator)(accept_error=False)\n        decorated_func = decorator(cupy_error)\n        with self.assertRaises(AssertionError):\n            decorated_func(self)\n\n\ndef make_result(xp, np_result, cp_result):\n    if xp == numpy:\n        return np_result\n    elif xp == cupy:\n        return cp_result\n\n\n@testing.parameterize(\n    {'decorator': 'numpy_cupy_allclose'},\n    {'decorator': 'numpy_cupy_array_almost_equal'},\n    {'decorator': 'numpy_cupy_array_almost_equal_nulp'},\n    {'decorator': 'numpy_cupy_array_max_ulp'},\n    {'decorator': 'numpy_cupy_array_equal'}\n)\nclass TestNumPyCuPyEqual(unittest.TestCase, NumPyCuPyDecoratorBase,\n                         NumPyCuPyDecoratorBase2):\n\n    def valid_func(self, xp):\n        return make_result(xp, numpy.array(1), cupy.array(1))\n\n    def invalid_func(self, xp):\n        return make_result(xp, numpy.array(1), cupy.array(2))\n\n    def strange_kw_func(self, foo):\n        return make_result(foo, numpy.array(1), cupy.array(1))\n\n\n@testing.parameterize(\n    {'decorator': 'numpy_cupy_array_list_equal'}\n)\n@testing.gpu\nclass TestNumPyCuPyListEqual(unittest.TestCase, NumPyCuPyDecoratorBase):\n\n    def valid_func(self, xp):\n        return make_result(xp, [numpy.array(1)], [cupy.array(1)])\n\n    def invalid_func(self, xp):\n        return make_result(xp, [numpy.array(1)], [cupy.array(2)])\n\n    def strange_kw_func(self, foo):\n        return make_result(foo, [numpy.array(1)], [cupy.array(1)])\n\n\n@testing.parameterize(\n    {'decorator': 'numpy_cupy_array_less'}\n)\nclass TestNumPyCuPyLess(unittest.TestCase, NumPyCuPyDecoratorBase,\n                        NumPyCuPyDecoratorBase2):\n\n    def valid_func(self, xp):\n        return make_result(xp, numpy.array(2), cupy.array(1))\n\n    def invalid_func(self, xp):\n        return make_result(xp, numpy.array(1), cupy.array(2))\n\n    def strange_kw_func(self, foo):\n        return make_result(foo, numpy.array(2), cupy.array(1))\n\n\n@testing.parameterize(\n    {'decorator': 'numpy_cupy_raises'}\n)\nclass TestNumPyCuPyRaise(unittest.TestCase, NumPyCuPyDecoratorBase):\n\n    def valid_func(self, xp):\n        raise ValueError()\n\n    def invalid_func(self, xp):\n        return make_result(xp, numpy.array(1), cupy.array(1))\n\n    def strange_kw_func(self, foo):\n        raise ValueError()\n\n    def test_accept_error_numpy(self):\n        decorator = getattr(testing, self.decorator)()\n        decorated_func = decorator(numpy_error)\n        with self.assertRaises(AssertionError):\n            decorated_func(self)\n\n    def test_accept_error_cupy(self):\n        decorator = getattr(testing, self.decorator)()\n        decorated_func = decorator(cupy_error)\n        with self.assertRaises(AssertionError):\n            decorated_func(self)\n\n\nclass TestIgnoreOfNegativeValueDifferenceOnCpuAndGpu(unittest.TestCase):\n\n    @helper.for_unsigned_dtypes('dtype1')\n    @helper.for_signed_dtypes('dtype2')\n    @helper.numpy_cupy_allclose()\n    def correct_failure(self, xp, dtype1, dtype2):\n        if xp == numpy:\n            return xp.array(-1, dtype=numpy.float32)\n        else:\n            return xp.array(-2, dtype=numpy.float32)\n\n    def test_correct_failure(self):\n        with pytest.raises(AssertionError):\n            self.correct_failure()\n\n    @helper.for_unsigned_dtypes('dtype1')\n    @helper.for_signed_dtypes('dtype2')\n    @helper.numpy_cupy_allclose()\n    def test_correct_success(self, xp, dtype1, dtype2):\n        # Behavior of assigning a negative value to an unsigned integer\n        # variable is undefined.\n        # nVidia GPUs and Intel CPUs behave differently.\n        # To avoid this difference, we need to ignore dimensions whose\n        # values are negative.\n        if xp == numpy:\n            return xp.array(-1, dtype=dtype1)\n        else:\n            return xp.array(-2, dtype=dtype1)\n\n\n@testing.parameterize(*testing.product({\n    'xp': [numpy, cupy],\n    'shape': [(3, 2), (), (3, 0, 2)],\n}))\n@testing.gpu\nclass TestShapedRandom(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    def test_shape_and_dtype(self, dtype):\n        a = testing.shaped_random(self.shape, self.xp, dtype)\n        self.assertIsInstance(a, self.xp.ndarray)\n        self.assertTrue(a.shape == self.shape)\n        self.assertTrue(a.dtype == dtype)\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    def test_value_range(self, dtype):\n        a = testing.shaped_random(self.shape, self.xp, dtype)\n        self.assertTrue(self.xp.all(0 <= a))\n        self.assertTrue(self.xp.all(a < 10))\n\n    @testing.for_complex_dtypes()\n    def test_complex(self, dtype):\n        a = testing.shaped_random(self.shape, self.xp, dtype)\n        self.assertTrue(self.xp.all(0 <= a.real))\n        self.assertTrue(self.xp.all(a.real < 10))\n        self.assertTrue(self.xp.all(0 <= a.imag))\n        self.assertTrue(self.xp.all(a.imag < 10))\n        if 0 not in self.shape:\n            self.assertTrue(self.xp.any(a.imag))\n\n\n@testing.parameterize(*testing.product({\n    'xp': [numpy, cupy],\n}))\n@testing.gpu\nclass TestShapedRandomBool(unittest.TestCase):\n\n    def test_bool(self):\n        a = testing.shaped_random(10000, self.xp, numpy.bool_)\n        self.assertTrue(4000 < self.xp.sum(a) < 6000)\n\n\nclass TestSkip(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose()\n    def test_allclose(self, xp):\n        raise unittest.SkipTest('Test for skip with @numpy_cupy_allclose')\n        assert False\n\n    @testing.numpy_cupy_array_almost_equal()\n    def test_array_almost_equal(self, xp):\n        raise unittest.SkipTest(\n            'Test for skip with @numpy_cupy_array_almost_equal')\n        assert False\n\n    @testing.numpy_cupy_array_almost_equal_nulp()\n    def test_array_almost_equal_nulp(self, xp):\n        raise unittest.SkipTest(\n            'Test for skip with @numpy_cupy_array_almost_equal_nulp')\n        assert False\n\n    @testing.numpy_cupy_array_max_ulp()\n    def test_array_max_ulp(self, xp):\n        raise unittest.SkipTest('Test for skip with @numpy_cupy_array_max_ulp')\n        assert False\n\n    @testing.numpy_cupy_array_equal()\n    def test_array_equal(self, xp):\n        raise unittest.SkipTest('Test for skip with @numpy_cupy_array_equal')\n        assert False\n\n    @testing.numpy_cupy_array_list_equal()\n    def test_array_list_equal(self, xp):\n        raise unittest.SkipTest(\n            'Test for skip with @numpy_cupy_array_list_equal')\n        assert False\n\n    @testing.numpy_cupy_array_less()\n    def test_less(self, xp):\n        raise unittest.SkipTest('Test for skip with @numpy_cupy_array_less')\n        assert False\n\n    @testing.numpy_cupy_equal()\n    def test_equal(self, xp):\n        raise unittest.SkipTest('Test for skip with @numpy_cupy_equal')\n        assert False\n\n    @testing.numpy_cupy_raises()\n    def test_raises(self, xp):\n        raise unittest.SkipTest('Test for skip with @numpy_cupy_raises')\n        assert False\n\n\nclass TestSkipFail(unittest.TestCase):\n\n    @pytest.mark.xfail(strict=True)\n    @testing.numpy_cupy_allclose()\n    def test_different_reason(self, xp):\n        if xp is numpy:\n            raise unittest.SkipTest('skip1')\n        else:\n            raise unittest.SkipTest('skip2')\n\n    @pytest.mark.xfail(strict=True)\n    @testing.numpy_cupy_allclose()\n    def test_only_numpy(self, xp):\n        if xp is numpy:\n            raise unittest.SkipTest('skip')\n        else:\n            return xp.array(True)\n\n    @pytest.mark.xfail(strict=True)\n    @testing.numpy_cupy_allclose()\n    def test_only_cupy(self, xp):\n        if xp is numpy:\n            return xp.array(True)\n        else:\n            raise unittest.SkipTest('skip')\n"""
tests/cupy_tests/testing_tests/test_parameterized.py,0,"b""import unittest\n\nfrom cupy import testing\n\n\n@testing.parameterize(\n    {'actual': {'a': [1, 2], 'b': [3, 4, 5]},\n     'expect': [{'a': 1, 'b': 3}, {'a': 1, 'b': 4}, {'a': 1, 'b': 5},\n                {'a': 2, 'b': 3}, {'a': 2, 'b': 4}, {'a': 2, 'b': 5}]},\n    {'actual': {'a': [1, 2]}, 'expect': [{'a': 1}, {'a': 2}]},\n    {'actual': {'a': [1, 2], 'b': []}, 'expect': []},\n    {'actual': {'a': []}, 'expect': []},\n    {'actual': {}, 'expect': [{}]})\nclass ProductTest(unittest.TestCase):\n\n    def test_product(self):\n        self.assertListEqual(testing.product(self.actual), self.expect)\n\n\n@testing.parameterize(\n    {'actual': [[{'a': 1, 'b': 3}, {'a': 2, 'b': 4}], [{'c': 5}, {'c': 6}]],\n     'expect': [{'a': 1, 'b': 3, 'c': 5}, {'a': 1, 'b': 3, 'c': 6},\n                {'a': 2, 'b': 4, 'c': 5}, {'a': 2, 'b': 4, 'c': 6}]},\n    {'actual': [[{'a': 1}, {'a': 2}], [{'b': 3}, {'b': 4}, {'b': 5}]],\n     'expect': [{'a': 1, 'b': 3}, {'a': 1, 'b': 4}, {'a': 1, 'b': 5},\n                {'a': 2, 'b': 3}, {'a': 2, 'b': 4}, {'a': 2, 'b': 5}]},\n    {'actual': [[{'a': 1}, {'a': 2}]], 'expect': [{'a': 1}, {'a': 2}]},\n    {'actual': [[{'a': 1}, {'a': 2}], []], 'expect': []},\n    {'actual': [[]], 'expect': []},\n    {'actual': [], 'expect': [{}]})\nclass ProductDictTest(unittest.TestCase):\n\n    def test_product_dict(self):\n        self.assertListEqual(testing.product_dict(*self.actual), self.expect)\n\n\ndef f(x):\n    return x\n\n\nclass C(object):\n\n    def __call__(self, x):\n        return x\n\n    def method(self, x):\n        return x\n\n\n@testing.parameterize(\n    {'callable': f},\n    {'callable': lambda x: x},\n    {'callable': C()},\n    {'callable': C().method}\n)\nclass TestParameterize(unittest.TestCase):\n\n    def test_callable(self):\n        y = self.callable(1)\n        self.assertEqual(y, 1)\n\n    def test_skip(self):\n        # Skipping the test case should not report error.\n        self.skipTest('skip')\n\n\n@testing.parameterize(\n    {'param1': 1},\n    {'param1': 2})\n@testing.parameterize(\n    {'param2': 3},\n    {'param2': 4})\nclass TestParameterizeTwice(unittest.TestCase):\n    # This test only checks if each of the parameterized combinations is a\n    # member of the expected combinations. This test does not check if each\n    # of the expected combinations is actually visited by the parameterization,\n    # as there are no way to test that in a robust manner.\n\n    def test_twice(self):\n        assert hasattr(self, 'param1')\n        assert hasattr(self, 'param2')\n        assert (self.param1, self.param2) in (\n            (1, 3),\n            (1, 4),\n            (2, 3),\n            (2, 4))\n"""
tests/cupyx_tests/fallback_mode_tests/__init__.py,0,b''
tests/cupyx_tests/fallback_mode_tests/test_fallback.py,0,"b'import pytest\nimport unittest\nimport functools\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\nfrom cupyx import fallback_mode\nfrom cupyx.fallback_mode import fallback\nfrom cupyx.fallback_mode.notification import FallbackWarning\n\n\nignore_fallback_warnings = pytest.mark.filterwarnings(\n    ""ignore"", category=FallbackWarning)\n\n\ndef numpy_fallback_equal(name=\'xp\'):\n    """"""\n    Decorator that checks fallback_mode results are equal to NumPy ones.\n    Checks results that are non-ndarray.\n\n    Args:\n        name(str): Argument name whose value is either\n        ``numpy`` or ``cupy`` module.\n    """"""\n    def decorator(impl):\n        @functools.wraps(impl)\n        def test_func(self, *args, **kwargs):\n\n            kwargs[name] = fallback_mode.numpy\n            fallback_result = impl(self, *args, **kwargs)\n\n            kwargs[name] = numpy\n            numpy_result = impl(self, *args, **kwargs)\n\n            assert numpy_result == fallback_result\n\n        return test_func\n    return decorator\n\n\ndef numpy_fallback_array_equal(name=\'xp\'):\n    """"""\n    Decorator that checks fallback_mode results are equal to NumPy ones.\n    Checks ndarrays.\n\n    Args:\n        name(str): Argument name whose value is either\n        ``numpy`` or ``cupy`` module.\n    """"""\n    def decorator(impl):\n        @functools.wraps(impl)\n        def test_func(self, *args, **kwargs):\n\n            kwargs[name] = fallback_mode.numpy\n            fallback_result = impl(self, *args, **kwargs)\n\n            kwargs[name] = numpy\n            numpy_result = impl(self, *args, **kwargs)\n\n            if isinstance(numpy_result, numpy.ndarray):\n                # if numpy returns ndarray, cupy must return ndarray\n                assert isinstance(fallback_result, fallback.ndarray)\n\n                fallback_mode.numpy.testing.assert_array_equal(\n                    numpy_result, fallback_result)\n\n                assert fallback_result.dtype == numpy_result.dtype\n\n            elif isinstance(numpy_result, numpy.ScalarType):\n                # if numpy returns scalar\n                # cupy may return 0-dim array\n                assert numpy_result == fallback_result._cupy_array.item() or \\\n                    (numpy_result == fallback_result._numpy_array).all()\n\n            else:\n                assert False\n\n        return test_func\n    return decorator\n\n\ndef numpy_fallback_array_allclose(name=\'xp\', rtol=1e-07):\n    """"""\n    Decorator that checks fallback_mode results are almost equal to NumPy ones.\n    Checks ndarrays.\n\n    Args:\n        name(str): Argument name whose value is either\n        ``numpy`` or ``cupy`` module.\n    """"""\n    def decorator(impl):\n        @functools.wraps(impl)\n        def test_func(self, *args, **kwargs):\n\n            kwargs[name] = fallback_mode.numpy\n            fallback_result = impl(self, *args, **kwargs)\n\n            kwargs[name] = numpy\n            numpy_result = impl(self, *args, **kwargs)\n\n            assert isinstance(fallback_result, fallback.ndarray)\n\n            fallback_mode.numpy.testing.assert_allclose(\n                numpy_result, fallback_result, rtol=rtol)\n\n            assert fallback_result.dtype == numpy_result.dtype\n\n        return test_func\n    return decorator\n\n\ndef enable_slice_copy(func):\n    """"""\n    Decorator that enables CUPY_EXPERIMENTAL_SLICE_COPY.\n    And then restores it to previous state.\n    """"""\n    def decorator(*args, **kwargs):\n        old = cupy.util.ENABLE_SLICE_COPY\n        cupy.util.ENABLE_SLICE_COPY = True\n        func(*args, **kwargs)\n        cupy.util.ENABLE_SLICE_COPY = old\n\n    return decorator\n\n\ndef get_numpy_version():\n    return tuple(map(int, numpy.__version__.split(\'.\')))\n\n\n@ignore_fallback_warnings\n@testing.gpu\nclass TestFallbackMode(unittest.TestCase):\n\n    def test_module_not_callable(self):\n\n        pytest.raises(TypeError, fallback_mode.numpy)\n\n        pytest.raises(TypeError, fallback_mode.numpy.linalg)\n\n    def test_numpy_scalars(self):\n\n        assert fallback_mode.numpy.inf is numpy.inf\n\n        assert fallback_mode.numpy.pi is numpy.pi\n\n        # True, because \'is\' checks for reference\n        # fallback_mode.numpy.nan and numpy.nan both have same reference\n        assert fallback_mode.numpy.nan is numpy.nan\n\n    def test_cupy_specific_func(self):\n\n        with pytest.raises(AttributeError):\n            func = fallback_mode.numpy.ElementwiseKernel  # NOQA\n\n    def test_func_not_in_numpy(self):\n\n        with pytest.raises(AttributeError):\n            func = fallback_mode.numpy.dummy  # NOQA\n\n    def test_same_reference(self):\n\n        assert fallback_mode.numpy.int64 is numpy.int64\n\n        assert fallback_mode.numpy.float32 is numpy.float32\n\n    def test_isinstance(self):\n\n        a = fallback_mode.numpy.float64(3)\n        assert isinstance(a, fallback_mode.numpy.float64)\n\n        abs = fallback_mode.numpy.vectorize(fallback_mode.numpy.abs)\n        assert isinstance(abs, fallback_mode.numpy.vectorize)\n\n        date = fallback_mode.numpy.datetime64(\'2019-07-18\')\n        assert isinstance(date, fallback_mode.numpy.datetime64)\n\n\n@testing.parameterize(\n    {\'func\': \'min\', \'shape\': (3, 4), \'args\': (), \'kwargs\': {\'axis\': 0}},\n    {\'func\': \'argmin\', \'shape\': (3, 4), \'args\': (), \'kwargs\': {}},\n    {\'func\': \'roots\', \'shape\': (3,), \'args\': (), \'kwargs\': {}},\n    {\'func\': \'resize\', \'shape\': (2, 6), \'args\': ((6, 2),), \'kwargs\': {}},\n    {\'func\': \'resize\', \'shape\': (3, 4), \'args\': ((4, 9),), \'kwargs\': {}},\n    {\'func\': \'delete\', \'shape\': (5, 4), \'args\': (1, 0), \'kwargs\': {}},\n    {\'func\': \'append\', \'shape\': (2, 3), \'args\': ([[7, 8, 9]],),\n     \'kwargs\': {\'axis\': 0}},\n    {\'func\': \'asarray_chkfinite\', \'shape\': (2, 4), \'args\': (),\n     \'kwargs\': {\'dtype\': numpy.float64}}\n)\n@ignore_fallback_warnings\n@testing.gpu\nclass TestFallbackMethodsArrayExternal(unittest.TestCase):\n\n    @numpy_fallback_array_equal()\n    def test_fallback_methods_array_external(self, xp):\n\n        a = testing.shaped_random(self.shape, xp=xp, dtype=numpy.int64)\n        return getattr(xp, self.func)(a, *self.args, **self.kwargs)\n\n\n@testing.parameterize(\n    {\'func\': \'min\', \'shape\': (3, 4), \'args\': (), \'kwargs\': {\'axis\': 0},\n     \'numpy_version\': None},\n    {\'func\': \'argmin\', \'shape\': (3, 4), \'args\': (), \'kwargs\': {},\n     \'numpy_version\': (1, 10, 0)},\n    {\'func\': \'arccos\', \'shape\': (2, 3), \'args\': (), \'kwargs\': {},\n     \'numpy_version\': None},\n    {\'func\': \'fabs\', \'shape\': (2, 3), \'args\': (), \'kwargs\': {},\n     \'numpy_version\': None},\n    {\'func\': \'nancumsum\', \'shape\': (5, 3), \'args\': (), \'kwargs\': {\'axis\': 1},\n     \'numpy_version\': (1, 12, 0)},\n    {\'func\': \'nanpercentile\', \'shape\': (3, 4), \'args\': (50,),\n     \'kwargs\': {\'axis\': 0}, \'numpy_version\': None}\n)\n@ignore_fallback_warnings\n@testing.gpu\nclass TestFallbackMethodsArrayExternalOut(unittest.TestCase):\n\n    @numpy_fallback_array_equal()\n    def test_fallback_methods_array_external_out(self, xp):\n        if self.numpy_version and get_numpy_version() < self.numpy_version:\n            self.skipTest(\'Test not supported for this version of numpy\')\n\n        a = testing.shaped_random(self.shape, xp=xp)\n        kwargs = self.kwargs.copy()\n        res = getattr(xp, self.func)(a, *self.args, **kwargs)\n\n        # to get the shape of out\n        out = xp.zeros(res.shape, dtype=res.dtype)\n        kwargs[\'out\'] = out\n        getattr(xp, self.func)(a, *self.args, **kwargs)\n        return out\n\n\n@testing.parameterize(\n    {\'object\': fallback_mode.numpy.ndarray},\n    {\'object\': fallback_mode.numpy.ndarray.__add__},\n    {\'object\': fallback_mode.numpy.vectorize},\n    {\'object\': fallback_mode.numpy.linalg.eig},\n)\n@testing.gpu\nclass TestDocs(unittest.TestCase):\n\n    @numpy_fallback_equal()\n    def test_docs(self, xp):\n        return getattr(self.object, \'__doc__\')\n\n\n@testing.gpu\nclass FallbackArray(unittest.TestCase):\n\n    def test_ndarray_creation_compatible(self):\n\n        a = fallback_mode.numpy.array([[1, 2], [3, 4]])\n        assert isinstance(a, fallback.ndarray)\n        assert a._supports_cupy\n\n        b = fallback_mode.numpy.arange(9)\n        assert isinstance(b, fallback.ndarray)\n        assert a._supports_cupy\n\n    def test_ndarray_creation_not_compatible(self):\n\n        a = fallback_mode.numpy.array([1, 2, 3], dtype=object)\n        assert isinstance(a, fallback.ndarray)\n        assert not a._supports_cupy\n\n        b = fallback_mode.numpy.array([\'a\', \'b\', \'c\', \'d\'], dtype=\'|S1\')\n        assert isinstance(b, fallback.ndarray)\n        assert not b._supports_cupy\n\n        # Structured array will automatically be _numpy_array\n        c = fallback_mode.numpy.array(\n            [(\'Rex\', 9, 81.0), (\'Fido\', 3, 27.0)],\n            dtype=[(\'name\', \'U10\'), (\'age\', \'i4\'), (\'weight\', \'f4\')])\n\n        assert isinstance(c, fallback.ndarray)\n        assert not c._supports_cupy\n\n    def test_getitem(self):\n\n        x = fallback_mode.numpy.array([1, 2, 3])\n\n        # single element\n        assert int(x[2]) == 3\n\n        # slicing\n        res = cupy.array([1, 2, 3])\n        testing.assert_array_equal(x[:2]._cupy_array, res[:2])\n\n    def test_setitem(self):\n\n        x = fallback_mode.numpy.array([1, 2, 3])\n\n        # single element\n        x[2] = 99\n        res = cupy.array([1, 2, 99])\n        testing.assert_array_equal(x._cupy_array, res)\n\n        # slicing\n        y = fallback_mode.numpy.array([11, 22])\n        x[:2] = y\n        res = cupy.array([11, 22, 99])\n        testing.assert_array_equal(x._cupy_array, res)\n\n    @numpy_fallback_equal()\n    def test_ndarray_shape(self, xp):\n\n        x = xp.arange(20)\n        x = x.reshape(4, 5)\n\n        return x.shape\n\n    @numpy_fallback_equal()\n    def test_ndarray_init(self, xp):\n        a = xp.ndarray((3, 4))\n        return a.shape\n\n    @numpy_fallback_equal()\n    def test_ndarray_shape_creation(self, xp):\n        a = xp.ndarray((4, 5))\n        return a.shape\n\n    def test_instancecheck_ndarray(self):\n\n        a = fallback_mode.numpy.array([1, 2, 3])\n        assert isinstance(a, fallback_mode.numpy.ndarray)\n\n        b = fallback_mode.numpy.ndarray((2, 3))\n        assert isinstance(b, fallback_mode.numpy.ndarray)\n\n    def test_instancecheck_type(self):\n        a = fallback_mode.numpy.arange(3)\n        assert isinstance(a, type(a))\n\n    @numpy_fallback_equal()\n    def test_type_call(self, xp):\n        a = xp.array([1])\n        b = type(a)((2, 3))\n        return b.shape\n\n    @numpy_fallback_equal()\n    def test_type_assert(self, xp):\n        a = xp.array([1, 2, 3])\n        return type(a) == xp.ndarray\n\n    @numpy_fallback_equal()\n    def test_base(self, xp):\n        a = xp.arange(7)\n        b = a[2:]\n        return b.base is a\n\n\n@testing.parameterize(\n    {\'func\': \'min\', \'shape\': (5,), \'args\': (), \'kwargs\': {}},\n    {\'func\': \'argmax\', \'shape\': (5, 3), \'args\': (), \'kwargs\': {\'axis\': 0}},\n    {\'func\': \'ptp\', \'shape\': (3, 3), \'args\': (), \'kwargs\': {\'axis\': 1}},\n    {\'func\': \'compress\', \'shape\': (3, 2), \'args\': ([False, True],),\n     \'kwargs\': {\'axis\': 0}}\n)\n@testing.gpu\nclass TestFallbackArrayMethodsInternal(unittest.TestCase):\n\n    @numpy_fallback_array_equal()\n    def test_fallback_array_methods_internal(self, xp):\n\n        a = testing.shaped_random(self.shape, xp=xp)\n        return getattr(a, self.func)(*self.args, **self.kwargs)\n\n    @numpy_fallback_array_equal()\n    def test_fallback_array_methods_internal_out(self, xp):\n\n        a = testing.shaped_random(self.shape, xp=xp)\n        kwargs = self.kwargs.copy()\n        res = getattr(a, self.func)(*self.args, **kwargs)\n\n        # to get the shape of out\n        out = xp.zeros(res.shape, dtype=res.dtype)\n        kwargs[\'out\'] = out\n        getattr(a, self.func)(*self.args, **kwargs)\n        return out\n\n\n@testing.parameterize(\n    {\'func\': \'__eq__\', \'shape\': (3, 4)},\n    {\'func\': \'__ne__\', \'shape\': (3, 1)},\n    {\'func\': \'__gt__\', \'shape\': (4,)},\n    {\'func\': \'__lt__\', \'shape\': (1, 1)},\n    {\'func\': \'__ge__\', \'shape\': (1, 2)},\n    {\'func\': \'__le__\', \'shape\': (1,)}\n)\n@testing.gpu\nclass TestArrayComparison(unittest.TestCase):\n\n    @numpy_fallback_array_equal()\n    def test_ndarray_comparison(self, xp):\n\n        a = testing.shaped_random(self.shape, xp=xp)\n        b = testing.shaped_random(self.shape, xp=xp, seed=3)\n\n        return getattr(a, self.func)(b)\n\n\n@testing.parameterize(\n    {\'func\': \'__str__\', \'shape\': (5, 6)},\n    {\'func\': \'__repr__\', \'shape\': (3, 4)},\n    {\'func\': \'__int__\', \'shape\': (1,)},\n    {\'func\': \'__float__\', \'shape\': (1, 1)},\n    {\'func\': \'__len__\', \'shape\': (3, 3)},\n    {\'func\': \'__bool__\', \'shape\': (1,)},\n)\n@testing.gpu\nclass TestArrayUnaryMethods(unittest.TestCase):\n\n    @numpy_fallback_equal()\n    def test_unary_methods(self, xp):\n        a = testing.shaped_random(self.shape, xp=xp)\n        return getattr(a, self.func)()\n\n\n@testing.parameterize(\n    {\'func\': \'__abs__\', \'shape\': (5, 6), \'dtype\': numpy.float32},\n    {\'func\': \'__copy__\', \'shape\': (3, 4), \'dtype\': numpy.float32},\n    {\'func\': \'__neg__\', \'shape\': (3, 3), \'dtype\': numpy.float32},\n    {\'func\': \'__invert__\', \'shape\': (2, 4), \'dtype\': numpy.int32}\n)\n@testing.gpu\nclass TestArrayUnaryMethodsArray(unittest.TestCase):\n\n    @numpy_fallback_array_equal()\n    def test_unary_methods_array(self, xp):\n\n        a = testing.shaped_random(self.shape, xp=xp, dtype=self.dtype)\n\n        return getattr(a, self.func)()\n\n\n@testing.parameterize(\n    {\'func\': \'__add__\', \'shape\': (3, 4), \'dtype\': numpy.float32},\n    {\'func\': \'__sub__\', \'shape\': (2, 2), \'dtype\': numpy.float32},\n    {\'func\': \'__mul__\', \'shape\': (5, 6), \'dtype\': numpy.float32},\n    {\'func\': \'__mod__\', \'shape\': (3, 4), \'dtype\': numpy.float32},\n    {\'func\': \'__iadd__\', \'shape\': (1,), \'dtype\': numpy.float32},\n    {\'func\': \'__imul__\', \'shape\': (1, 1), \'dtype\': numpy.float32},\n    {\'func\': \'__and__\', \'shape\': (3, 3), \'dtype\': numpy.int32},\n    {\'func\': \'__ipow__\', \'shape\': (4, 5), \'dtype\': numpy.int32},\n    {\'func\': \'__xor__\', \'shape\': (4, 4), \'dtype\': numpy.int32},\n    {\'func\': \'__lshift__\', \'shape\': (2,), \'dtype\': numpy.int32},\n    {\'func\': \'__irshift__\', \'shape\': (3, 2), \'dtype\': numpy.int32},\n)\n@testing.gpu\nclass TestArrayArithmeticMethods(unittest.TestCase):\n\n    @numpy_fallback_array_equal()\n    def test_arithmetic_methods(self, xp):\n        a = testing.shaped_random(self.shape, xp=xp, dtype=self.dtype)\n        b = testing.shaped_random(self.shape, xp=xp, dtype=self.dtype, seed=5)\n        return getattr(a, self.func)(b)\n\n\n@testing.gpu\nclass TestArrayMatmul(unittest.TestCase):\n\n    @testing.with_requires(\'numpy>=1.16\')\n    @numpy_fallback_array_allclose(rtol=1e-05)\n    def test_mm_matmul(self, xp):\n        a = testing.shaped_random((4, 5), xp)\n        b = testing.shaped_random((5, 3), xp, seed=5)\n\n        return a.__matmul__(b)\n\n\n@testing.gpu\nclass TestVectorizeWrapper(unittest.TestCase):\n\n    @numpy_fallback_array_equal()\n    def test_pyfunc_custom_list(self, xp):\n\n        def function(a, b):\n            if a > b:\n                return a - b\n            return a + b\n\n        return xp.vectorize(function)([1, 2, 3, 4], 2)\n\n    @numpy_fallback_array_equal()\n    def test_pyfunc_builtin(self, xp):\n        a = testing.shaped_random((4, 5), xp)\n        vabs = xp.vectorize(abs)\n        return vabs(a)\n\n    @numpy_fallback_array_equal()\n    def test_pyfunc_numpy(self, xp):\n        a = testing.shaped_random((4, 5), xp)\n        vabs = xp.vectorize(numpy.abs)\n        return vabs(a)\n\n    @numpy_fallback_equal()\n    def test_getattr(self, xp):\n        vabs = xp.vectorize(numpy.abs)\n        return vabs.pyfunc\n\n    @numpy_fallback_array_equal()\n    def test_setattr(self, xp):\n        a = xp.array([-1, 2, -3])\n        vabs = xp.vectorize(abs)\n        vabs.otypes = [\'float\']\n        return vabs(a)\n\n    @numpy_fallback_equal()\n    def test_doc(self, xp):\n        vabs = xp.vectorize(abs)\n        return vabs.__doc__\n\n\n@ignore_fallback_warnings\n@testing.gpu\nclass TestInplaceSpecialMethods(unittest.TestCase):\n\n    @numpy_fallback_array_equal()\n    def test_resize_internal(self, xp):\n        a = testing.shaped_random((3, 4), xp)\n        a.resize(4, 5, refcheck=False)\n        return a\n\n    @numpy_fallback_array_equal()\n    def test_ndarray_byteswap(self, xp):\n        a = testing.shaped_random((4,), xp, dtype=xp.int16)\n        return a.byteswap()\n\n    @unittest.skipIf(get_numpy_version() < (1, 13, 0),\n                     \'inplace kwarg for byteswap was added in numpy v1.13.0\')\n    @numpy_fallback_array_equal()\n    def test_ndarray_byteswap_inplace(self, xp):\n        a = testing.shaped_random((4,), xp, dtype=xp.int16)\n        a.byteswap(inplace=True)\n        return a\n\n    @numpy_fallback_array_equal()\n    def test_putmask(self, xp):\n        a = testing.shaped_random((3, 4), xp, dtype=xp.int8)\n        xp.putmask(a, a > 2, a**2)\n        return a\n\n    @unittest.skipIf(get_numpy_version() < (1, 15, 0),\n                     \'put_along_axis introduced in numpy v1.15.0\')\n    @numpy_fallback_array_equal()\n    def test_put_along_axis(self, xp):\n        a = xp.array([[10, 30, 20], [60, 40, 50]])\n        ai = xp.expand_dims(xp.argmax(a, axis=1), axis=1)\n        xp.put_along_axis(a, ai, 99, axis=1)\n        return a\n\n    @unittest.skipIf(get_numpy_version() < (1, 12, 0),\n                     \'nancumsum introduced in numpy v1.12.0\')\n    @numpy_fallback_array_equal()\n    def test_out_is_returned_when_fallbacked(self, xp):\n        a = testing.shaped_random((3, 4), xp)\n        z = xp.zeros((3, 4))\n        res = xp.nancumsum(a, axis=0, out=z)\n        assert res is z\n        return res\n\n    @numpy_fallback_array_allclose()\n    def test_out_is_returned_when_not_fallbacked(self, xp):\n        a = testing.shaped_random((3, 4), xp, dtype=xp.float64)\n        z = xp.zeros((4,))\n        res = xp.var(a, axis=0, out=z)\n        assert res is z\n        return res\n\n\n@ignore_fallback_warnings\n@testing.gpu\nclass TestArrayVariants(unittest.TestCase):\n\n    @numpy_fallback_array_equal()\n    def test_creation_masked(self, xp):\n        mx = xp.ma.array([1, 2, 3, 4], mask=[1, 0, 1, 0])\n        return mx\n\n    @numpy_fallback_equal()\n    def test_method_internal(self, xp):\n        mx = xp.ma.array([1, 2, 3, 4], mask=[1, 0, 1, 0])\n        return mx.min()\n\n    @numpy_fallback_equal()\n    def test_method_internal_not_callable(self, xp):\n        mx = xp.ma.array([1, 2, 3, 4], mask=[1, 0, 1, 0])\n        return mx.shape\n\n    @numpy_fallback_equal()\n    def test_method_external_masked(self, xp):\n        mx = xp.ma.array([1, 2, 3, 4], mask=[1, 0, 1, 0])\n        return xp.mean(mx)\n\n    @numpy_fallback_array_equal()\n    def test_magic_method_masked(self, xp):\n        mx = xp.ma.array([1, 2, 3, 4], mask=[1, 0, 1, 0])\n        my = xp.ma.array([4, 2, 3, 1], mask=[1, 0, 1, 0])\n        return mx >= my\n\n    @numpy_fallback_array_equal()\n    def test_creation_char(self, xp):\n        cx = xp.char.array([\'a\', \'b\', \'c\'], itemsize=3)\n        return cx\n\n    @numpy_fallback_array_equal()\n    def test_method_external_char(self, xp):\n        cx = xp.char.array([\'a\', \'b\', \'c\'], itemsize=3)\n        cy = xp.char.array([\'a\', \'b\', \'c\'], itemsize=3)\n        return xp.char.add(cx, cy)\n\n    @numpy_fallback_array_equal()\n    def test_magic_method_char(self, xp):\n        cx = xp.char.array([\'a\', \'b\', \'c\'], itemsize=3)\n        cy = xp.char.array([\'a\', \'b\', \'c\'], itemsize=3)\n        return cx == cy\n\n    @numpy_fallback_array_equal()\n    def test_inplace(self, xp):\n        x = xp.arange(12).reshape((3, 4))\n        mask = xp.zeros_like(x)\n        mask[0, :] = 1\n        mx = xp.ma.array(x, mask=mask)\n        z = xp.ma.zeros((4,))\n        xp.nanmean(mx, axis=0, out=z)\n        return z\n\n    @numpy_fallback_array_equal()\n    def test_matrix_returned(self, xp):\n        x = testing.shaped_random((2, 3), xp=xp)\n        y = xp.asmatrix(x)\n\n        if xp is fallback_mode.numpy:\n            assert x._supports_cupy\n            assert isinstance(y, fallback.ndarray)\n            assert not y._supports_cupy\n            assert y._numpy_array.__class__ is numpy.matrix\n\n        return y\n\n    @numpy_fallback_array_equal()\n    def test_record_array(self, xp):\n        ra = xp.rec.array([1, 2, 3])\n        return ra\n\n    # changes in MaskedArray should be reflected in base ndarray\n    @numpy_fallback_array_equal()\n    def test_ma_func(self, xp):\n        x = xp.array([1, 2, 3, 4])\n        x += x\n        mx = xp.ma.array(x, mask=[1, 0, 1, 0])\n        assert mx.base is x\n        mx += mx\n        return x\n\n    # changes in base ndarray should be reflected in MaskedArray\n    @enable_slice_copy\n    @numpy_fallback_array_equal()\n    def test_ma_func_inverse(self, xp):\n        x = xp.array([1, 2, 3, 4])\n        mx = xp.ma.array(x, mask=[1, 0, 1, 0])\n        assert mx.base is x\n        mx += mx\n        x += x\n        return mx\n'"
tests/cupyx_tests/fallback_mode_tests/test_notifications.py,0,"b'import contextlib\nimport io\nimport pytest\nimport unittest\n\nimport numpy\n\nfrom cupy import testing\nfrom cupyx import fallback_mode\nfrom cupyx import _ufunc_config\nfrom cupyx_tests.fallback_mode_tests import test_fallback as test_utils\n\n\n@testing.gpu\nclass TestNotifications(unittest.TestCase):\n\n    def test_seterr_geterr(self):\n\n        default = _ufunc_config.geterr()\n        assert default[\'fallback_mode\'] == \'warn\'\n\n        old = _ufunc_config.seterr(fallback_mode=\'ignore\')\n        current = _ufunc_config.geterr()\n        assert old[\'fallback_mode\'] == \'warn\'\n        assert current[\'fallback_mode\'] == \'ignore\'\n        _ufunc_config.seterr(**old)\n\n    def test_errstate(self):\n\n        old = _ufunc_config.seterr(fallback_mode=\'print\')\n        before = _ufunc_config.geterr()\n\n        with _ufunc_config.errstate(fallback_mode=\'raise\'):\n            inside = _ufunc_config.geterr()\n            assert inside[\'fallback_mode\'] == \'raise\'\n\n        after = _ufunc_config.geterr()\n        assert before[\'fallback_mode\'] == after[\'fallback_mode\']\n        _ufunc_config.seterr(**old)\n\n\n@testing.parameterize(\n    {\'func\': fallback_mode.numpy.array_equiv, \'shape\': (3, 4)},\n    {\'func\': fallback_mode.numpy.polyadd, \'shape\': (2, 3)},\n    {\'func\': fallback_mode.numpy.convolve, \'shape\': (5,)}\n)\n@testing.gpu\nclass TestNotificationModes(unittest.TestCase):\n\n    def test_notification_ignore(self):\n\n        old = _ufunc_config.seterr(fallback_mode=\'ignore\')\n        saved_stdout = io.StringIO()\n\n        with contextlib.redirect_stdout(saved_stdout):\n            a = testing.shaped_random(self.shape, fallback_mode.numpy)\n            b = testing.shaped_random(self.shape, fallback_mode.numpy)\n            self.func(a, b)\n\n        _ufunc_config.seterr(**old)\n        output = saved_stdout.getvalue().strip()\n        assert output == """"\n\n    def test_notification_print(self):\n\n        old = _ufunc_config.seterr(fallback_mode=\'print\')\n        saved_stdout = io.StringIO()\n\n        with contextlib.redirect_stdout(saved_stdout):\n            a = testing.shaped_random(self.shape, fallback_mode.numpy)\n            b = testing.shaped_random(self.shape, fallback_mode.numpy)\n            self.func(a, b)\n\n        _ufunc_config.seterr(**old)\n        nf = self.func._numpy_object\n        output = saved_stdout.getvalue().strip()\n        msg1 = ""\'{}\' method not in cupy, "".format(nf.__name__)\n        msg2 = ""falling back to \'{}.{}\'"".format(nf.__module__, nf.__name__)\n        assert output == (""Warning: "" + msg1 + msg2)\n\n    def test_notification_warn(self):\n\n        _ufunc_config.seterr(fallback_mode=\'warn\')\n\n        with pytest.warns(fallback_mode.notification.FallbackWarning):\n            a = testing.shaped_random(self.shape, fallback_mode.numpy)\n            b = testing.shaped_random(self.shape, fallback_mode.numpy)\n            self.func(a, b)\n\n    def test_notification_raise(self):\n\n        old = _ufunc_config.seterr(fallback_mode=\'raise\')\n\n        with pytest.raises(AttributeError):\n            a = testing.shaped_random(self.shape, fallback_mode.numpy)\n            b = testing.shaped_random(self.shape, fallback_mode.numpy)\n            self.func(a, b)\n\n        _ufunc_config.seterr(**old)\n\n\n@testing.gpu\nclass TestNotificationVectorize(unittest.TestCase):\n\n    @test_utils.enable_slice_copy\n    def test_custom_or_builtin_pyfunc(self):\n\n        old = _ufunc_config.seterr(fallback_mode=\'print\')\n        saved_stdout = io.StringIO()\n\n        with contextlib.redirect_stdout(saved_stdout):\n            def custom_abs(x):\n                if x >= 0:\n                    return x\n                return -x\n\n            a = testing.shaped_random((3, 4), fallback_mode.numpy)\n            vec_abs = fallback_mode.numpy.vectorize(custom_abs)\n            vec_abs(a)\n\n            # built-in\n            vec_abs = fallback_mode.numpy.vectorize(abs)\n            vec_abs(a)\n\n        _ufunc_config.seterr(**old)\n        output = saved_stdout.getvalue().strip()\n        msg = ""\'vectorize\' method not in cupy, ""\n        msg += ""falling back to \'""\n        msg += numpy.vectorize.__module__ + "".vectorize\'""\n        assert output == (""Warning: "" + msg + ""\\nWarning: "" + msg)\n\n    @test_utils.enable_slice_copy\n    def test_cupy_supported_pyfunc(self):\n\n        old = _ufunc_config.seterr(fallback_mode=\'print\')\n        saved_stdout = io.StringIO()\n\n        with contextlib.redirect_stdout(saved_stdout):\n            a = testing.shaped_random((3, 4), fallback_mode.numpy)\n            vec_abs = fallback_mode.numpy.vectorize(fallback_mode.numpy.abs)\n            vec_abs(a)\n\n        _ufunc_config.seterr(**old)\n        output = saved_stdout.getvalue().strip()\n        msg1 = ""\'vectorize\' method not in cupy, ""\n        msg1 += ""falling back to \'""\n        msg1 += numpy.vectorize.__module__ + "".vectorize\'""\n        msg2 = ""\'absolute\' method is available in cupy but cannot be used, ""\n        msg2 += ""falling back to its numpy implementation""\n        assert output == (""Warning: "" + msg1 + ""\\nWarning: "" + msg2)\n\n    @test_utils.enable_slice_copy\n    def test_numpy_only_pyfunc(self):\n\n        old = _ufunc_config.seterr(fallback_mode=\'print\')\n        saved_stdout = io.StringIO()\n\n        with contextlib.redirect_stdout(saved_stdout):\n            a = testing.shaped_random((3, 4), fallback_mode.numpy)\n            vec_abs = fallback_mode.numpy.vectorize(fallback_mode.numpy.fabs)\n            vec_abs(a)\n\n        _ufunc_config.seterr(**old)\n        output = saved_stdout.getvalue().strip()\n        msg1 = ""\'vectorize\' method not in cupy, ""\n        msg1 += ""falling back to \'""\n        msg1 += numpy.vectorize.__module__ + "".vectorize\'""\n        msg2 = ""\'fabs\' method not in cupy, ""\n        msg2 += ""falling back to its numpy implementation""\n        assert output == (""Warning: "" + msg1 + ""\\nWarning: "" + msg2)\n'"
tests/cupyx_tests/linalg_tests/__init__.py,0,b''
tests/cupyx_tests/linalg_tests/test_solve.py,0,"b""import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\nimport cupyx\n\n\n@testing.parameterize(*testing.product({\n    'size': [5, 9, 17, 33],\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@testing.gpu\nclass TestInvh(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_invh(self, xp):\n        a = self._create_symmetric_matrix(xp, self.size, self.dtype)\n        if xp == cupy:\n            return cupyx.linalg.invh(a)\n        else:\n            return numpy.linalg.inv(a)\n\n    def _create_symmetric_matrix(self, xp, n, dtype):\n        if dtype == numpy.complex128:\n            f_dtype = numpy.float64\n        elif dtype == numpy.complex64:\n            f_dtype = numpy.float32\n        else:\n            f_dtype = dtype\n        a = testing.shaped_random((n, n), xp, f_dtype, scale=1)\n        a = a + a.T + xp.eye(n, dtype=f_dtype) * n\n        if dtype in (numpy.complex64, numpy.complex128):\n            b = testing.shaped_random((n, n), xp, f_dtype, scale=1)\n            b = b - b.T\n            a = a + 1j * b\n        return a\n\n\n@testing.parameterize(*testing.product({\n    'size': [8],\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@testing.gpu\nclass TestErrorInvh(unittest.TestCase):\n\n    def test_invh(self):\n        a = self._create_symmetric_matrix(self.size, self.dtype)\n        with self.assertRaises(RuntimeError):\n            cupyx.linalg.invh(a)\n\n    def _create_symmetric_matrix(self, n, dtype):\n        a = testing.shaped_random((n, n), cupy, dtype, scale=1)\n        a = a + a.T - cupy.eye(n, dtype=dtype)\n        return a\n"""
tests/cupyx_tests/scipy_tests/__init__.py,0,b''
tests/cupyx_tests/scipy_tests/test_get_array_module.py,0,"b""import unittest\n\nfrom cupy import testing\nimport cupyx.scipy.special\n\n\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestSpecial(unittest.TestCase):\n\n    @testing.for_dtypes(['f', 'd'])\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_get_array_module(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        module = cupyx.scipy.get_array_module(a)\n        assert module is scp\n        return module.special.j0(a)\n\n    @testing.for_dtypes(['f', 'd'])\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_get_array_module_multiple_parameters(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        module = cupyx.scipy.get_array_module(a, a)\n        assert module is scp\n        return module.special.j1(a)\n"""
cupyx/scipy/sparse/linalg/__init__.py,0,"b'# Functions from the following SciPy document\n# https://docs.scipy.org/doc/scipy/reference/sparse.linalg.html\n\n# ""NOQA"" to suppress flake8 warning\nfrom cupyx.scipy.sparse.linalg.solve import lsqr  # NOQA\n'"
cupyx/scipy/sparse/linalg/solve.py,0,"b'import numpy\n\nimport cupy\nfrom cupy.cuda import cusolver\nfrom cupy.cuda import device\nfrom cupy.linalg import util\nimport cupyx.scipy.sparse\n\n\ndef lsqr(A, b):\n    """"""Solves linear system with QR decomposition.\n\n    Find the solution to a large, sparse, linear system of equations.\n    The function solves ``Ax = b``. Given two-dimensional matrix ``A`` is\n    decomposed into ``Q * R``.\n\n    Args:\n        A (cupy.ndarray or cupyx.scipy.sparse.csr_matrix): The input matrix\n            with dimension ``(N, N)``\n        b (cupy.ndarray): Right-hand side vector.\n\n    Returns:\n        tuple:\n            Its length must be ten. It has same type elements\n            as SciPy. Only the first element, the solution vector ``x``, is\n            available and other elements are expressed as ``None`` because\n            the implementation of cuSOLVER is different from the one of SciPy.\n            You can easily calculate the fourth element by ``norm(b - Ax)``\n            and the ninth element by ``norm(x)``.\n\n    .. seealso:: :func:`scipy.sparse.linalg.lsqr`\n    """"""\n\n    if not cupyx.scipy.sparse.isspmatrix_csr(A):\n        A = cupyx.scipy.sparse.csr_matrix(A)\n    util._assert_nd_squareness(A)\n    util._assert_cupy_array(b)\n    m = A.shape[0]\n    if b.ndim != 1 or len(b) != m:\n        raise ValueError(\'b must be 1-d array whose size is same as A\')\n\n    # Cast to float32 or float64\n    if A.dtype == \'f\' or A.dtype == \'d\':\n        dtype = A.dtype\n    else:\n        dtype = numpy.promote_types(A.dtype, \'f\')\n\n    handle = device.get_cusolver_sp_handle()\n    nnz = A.nnz\n    tol = 1.0\n    reorder = 1\n    x = cupy.empty(m, dtype=dtype)\n    singularity = numpy.empty(1, numpy.int32)\n\n    if dtype == \'f\':\n        csrlsvqr = cusolver.scsrlsvqr\n    else:\n        csrlsvqr = cusolver.dcsrlsvqr\n    csrlsvqr(\n        handle, m, nnz, A._descr.descriptor, A.data.data.ptr,\n        A.indptr.data.ptr, A.indices.data.ptr, b.data.ptr, tol, reorder,\n        x.data.ptr, singularity.ctypes.data)\n\n    # The return type of SciPy is always float64. Therefore, x must be casted.\n    x = x.astype(numpy.float64)\n    ret = (x, None, None, None, None, None, None, None, None, None)\n    return ret\n'"
tests/cupy_tests/core_tests/fusion_tests/__init__.py,0,b''
tests/cupy_tests/core_tests/fusion_tests/fusion_utils.py,0,"b'import numpy\n\nimport cupy\nfrom cupy import testing\n\n\nscalar_types = (numpy.generic, int, float, complex)\n\n\ndef check_fusion(\n        generate_inputs_name=\'generate_inputs\',\n        generate_inputs_args=None,\n        check_array=None,\n        check_array_kwargs=None,\n        accept_error=()):\n    """"""Decorator for tests for ``cupy.fuse``.\n\n    This decorator checks the results of the original function is equals to\n    that of the fused function.\n\n    Args:\n        generate_input_args(tuple): argument tuple passed to\n            ``generate_input``. Defaults to ``()``.\n        check_array(function): testing function which compares\n            ``{numpy/cupy}.ndarray`` objects.\n            Defaults to ``testing.assert_allclose``.\n        check_array_kwargs(dict): keyword arguments passed to\n            ``check_array``. Defaults to ``{\'rtol\': 3e-3, \'atol\': 3e-3}``.\n        accept_error(Exception or tuple of Exception):\n            Specify acceptable errors.\n    """"""\n    if generate_inputs_args is None:\n        generate_inputs_args = ()\n    if check_array is None:\n        check_array = testing.assert_allclose\n    if check_array_kwargs is None:\n        # TODO(imanishi): Relax tolerances only when comparing float16 arrays.\n        check_array_kwargs = {\'rtol\': 3e-3, \'atol\': 3e-3}\n    if not isinstance(accept_error, (tuple, list)):\n        accept_error = (accept_error,)\n\n    def check(xp, actual, expected):\n        if expected is None:\n            assert actual is None\n\n        elif isinstance(expected, scalar_types + (numpy.ndarray,)):\n            assert isinstance(actual, scalar_types + (xp.ndarray,))\n            check_array(actual, expected, **check_array_kwargs)\n\n        elif isinstance(expected, (list, tuple)):\n            assert type(actual) == type(expected)\n            for item_actual, item_expected in zip(actual, expected):\n                check(xp, item_actual, item_expected)\n\n        elif isinstance(expected, dict):\n            assert isinstance(actual, dict)\n            for key, expected_value in expected:\n                actual_value = actual.pop(key)\n                check_array(xp, actual_value, expected_value)\n            assert len(actual) == 0\n\n        else:\n            assert False\n\n    # Calls `func` with the arguments `args` and `kwargs`, and returns\n    # the tuple of the return value and the exception object raised by the\n    # function.\n    def call(func, args, kwargs):\n        try:\n            ret = func(*args, **kwargs)\n            err = None\n        except Exception as e:\n            if not isinstance(e, accept_error):\n                raise\n            ret = None\n            err = e\n        return ret, err\n\n    def check_result(xp, actual, expected):\n        ret_a, err_a = actual\n        ret_e, err_e = expected\n\n        if err_e is None:\n            # Exception not raised\n            assert err_a is None\n            check(xp, ret_a, ret_e)\n\n    def deco(func):\n        def wrapper(self, **generate_inputs_kwargs):\n            generate_inputs = getattr(self, generate_inputs_name)\n\n            impl_np = func(self, numpy, **generate_inputs_kwargs)\n            impl_cp = func(self, cupy, **generate_inputs_kwargs)\n\n            # TODO(imanishi): Fix these workaround after `cupy.fuse`\n            # supports lambda function.\n            # If `cupy.fuse` supports lambda function, these lines can be\n            # written more simply (as `impl_fuse_np = cupy.fuse(impl_np)`).\n            @cupy.fuse()\n            def impl_fuse_np(*args, **kwargs):\n                return impl_np(*args, **kwargs)\n\n            @cupy.fuse()\n            def impl_fuse_cp(*args, **kwargs):\n                return impl_cp(*args, **kwargs)\n\n            args_np, kwargs_np = generate_inputs(\n                numpy, *generate_inputs_args, **generate_inputs_kwargs)\n            args_cp, kwargs_cp = generate_inputs(\n                cupy, *generate_inputs_args, **generate_inputs_kwargs)\n            args_fuse_np, kwargs_fuse_np = generate_inputs(\n                numpy, *generate_inputs_args, **generate_inputs_kwargs)\n            args_fuse_cp, kwargs_fuse_cp = generate_inputs(\n                cupy, *generate_inputs_args, **generate_inputs_kwargs)\n\n            result_np = call(impl_np, args_np, kwargs_np)\n            result_cp = call(impl_cp, args_cp, kwargs_cp)\n            result_fuse_np = call(impl_fuse_np, args_fuse_np, kwargs_fuse_np)\n            result_fuse_cp = call(impl_fuse_cp, args_fuse_cp, kwargs_fuse_cp)\n\n            check_result(cupy, result_cp, result_np)\n            check_result(numpy, result_fuse_np, result_np)\n            check_result(cupy, result_fuse_cp, result_np)\n            check(cupy, args_cp, args_np)\n            check(numpy, args_fuse_np, args_np)\n            check(cupy, args_fuse_cp, args_np)\n\n        return wrapper\n    return deco\n'"
tests/cupy_tests/core_tests/fusion_tests/test_array.py,0,"b""import unittest\n\nimport numpy\n\nfrom cupy import testing\nfrom cupy_tests.core_tests.fusion_tests import fusion_utils\n\n\nclass FusionArrayTestBase(unittest.TestCase):\n\n    def _get_argument(self, xp, dtype, seed, value_type):\n        dtype = numpy.dtype(dtype)\n        if value_type == 'array':\n            x = testing.shaped_random((3, 4), xp, dtype, scale=5, seed=seed)\n            # Avoid zero-division\n            # TODO(imanishi): Doing this only in division tests.\n            x[x == 0] = 1\n            return x\n        if value_type == 'scalar':\n            return dtype.type(3)\n        if value_type == 'primitive':\n            return dtype.type(3).tolist()\n        assert False\n\n    def generate_inputs(self, xp, dtype1, dtype2):\n        x = self._get_argument(xp, dtype1, 0, self.left_value)\n        y = self._get_argument(xp, dtype2, 1, self.right_value)\n        return (x, y), {}\n\n\n@testing.gpu\n@testing.parameterize(*testing.parameterized.product_dict(\n    [\n        {'name': 'neg', 'func': lambda x, y: -x},\n        {'name': 'add', 'func': lambda x, y: x + y},\n        {'name': 'sub', 'func': lambda x, y: x - y},\n        {'name': 'mul', 'func': lambda x, y: x * y},\n        {'name': 'div', 'func': lambda x, y: x / y},\n        {'name': 'pow', 'func': lambda x, y: x ** y},\n        {'name': 'eq', 'func': lambda x, y: x == y},\n        {'name': 'ne', 'func': lambda x, y: x != y},\n        {'name': 'lt', 'func': lambda x, y: x < y},\n        {'name': 'le', 'func': lambda x, y: x <= y},\n        {'name': 'gt', 'func': lambda x, y: x > y},\n        {'name': 'ge', 'func': lambda x, y: x >= y},\n    ],\n    [\n        {'left_value': 'array', 'right_value': 'array'},\n        {'left_value': 'array', 'right_value': 'scalar'},\n        {'left_value': 'array', 'right_value': 'primitive'},\n        {'left_value': 'scalar', 'right_value': 'array'},\n        {'left_value': 'primitive', 'right_value': 'array'},\n    ]\n))\nclass TestFusionArrayOperator(FusionArrayTestBase):\n\n    @testing.for_all_dtypes_combination(\n        names=('dtype1', 'dtype2'), no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_operator(self, xp, dtype1, dtype2):\n        return self.func\n\n\n@testing.gpu\n@testing.parameterize(*testing.parameterized.product_dict(\n    [\n        {'name': 'lshift', 'func': lambda x, y: x << y},\n        {'name': 'rshift', 'func': lambda x, y: x >> y},\n        {'name': 'and', 'func': lambda x, y: x & y},\n        {'name': 'or', 'func': lambda x, y: x | y},\n        {'name': 'xor', 'func': lambda x, y: x ^ y},\n        {'name': 'invert', 'func': lambda x, y: ~x},\n    ],\n    [\n        {'left_value': 'array', 'right_value': 'array'},\n        {'left_value': 'array', 'right_value': 'scalar'},\n        {'left_value': 'array', 'right_value': 'primitive'},\n        {'left_value': 'scalar', 'right_value': 'array'},\n        {'left_value': 'primitive', 'right_value': 'array'},\n    ]\n))\nclass TestFusionArrayBitwiseOperator(FusionArrayTestBase):\n\n    def _is_uint64(self, x):\n        return not isinstance(x, int) and x.dtype == 'uint64'\n\n    def _is_signed_int(self, x):\n        return isinstance(x, int) or x.dtype.kind == 'i'\n\n    @testing.for_int_dtypes_combination(\n        names=('dtype1', 'dtype2'), no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_operator(self, xp, dtype1, dtype2):\n        def func(x, y):\n            if ((self._is_uint64(x) and self._is_signed_int(y))\n                    or (self._is_uint64(y) and self._is_signed_int(x))):\n                # Skip TypeError case.\n                return\n            return self.func(x, y)\n\n        return func\n\n\n@testing.gpu\n@testing.parameterize(\n    {'left_value': 'array', 'right_value': 'array'},\n    {'left_value': 'array', 'right_value': 'scalar'},\n    {'left_value': 'array', 'right_value': 'primitive'},\n    {'left_value': 'scalar', 'right_value': 'array'},\n    {'left_value': 'primitive', 'right_value': 'array'},\n)\nclass TestFusionArrayFloorDivide(FusionArrayTestBase):\n\n    @testing.for_all_dtypes_combination(\n        names=('dtype1', 'dtype2'), no_bool=True, no_complex=True)\n    @fusion_utils.check_fusion()\n    def test_floor_divide(self, xp, dtype1, dtype2):\n        return lambda x, y: x // y\n\n\n# TODO(imanishi): Fix TypeError in use of dtypes_combination test.\n@testing.gpu\n@testing.parameterize(*testing.parameterized.product_dict(\n    [\n        {'left_value': 'array', 'right_value': 'array'},\n        {'left_value': 'array', 'right_value': 'scalar'},\n        {'left_value': 'array', 'right_value': 'primitive'},\n    ]\n))\nclass TestFusionArrayInplaceOperator(FusionArrayTestBase):\n\n    def generate_inputs(self, xp, dtype):\n        x = self._get_argument(xp, dtype, 0, self.left_value)\n        y = self._get_argument(xp, dtype, 1, self.right_value)\n        return (x, y), {}\n\n    @testing.for_all_dtypes(no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_iadd(self, xp, dtype):\n        def func(x, y):\n            x += y\n\n        return func\n\n    @testing.for_all_dtypes(no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_isub(self, xp, dtype):\n        def func(x, y):\n            x -= y\n\n        return func\n\n    @testing.for_all_dtypes(no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_imul(self, xp, dtype):\n        def func(x, y):\n            x *= y\n\n        return func\n\n    @testing.for_float_dtypes()\n    @fusion_utils.check_fusion()\n    def test_itruediv_py3(self, xp, dtype):\n        def func(x, y):\n            x /= y\n\n        return func\n\n    @testing.for_int_dtypes(no_bool=True)\n    @fusion_utils.check_fusion(accept_error=(TypeError,))\n    def test_int_itruediv_py3_raises(self, xp, dtype):\n        def func(x, y):\n            x /= y\n\n        return func\n\n    @testing.for_int_dtypes(no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_imod(self, xp, dtype):\n        def func(x, y):\n            x %= y\n\n        return func\n\n    @testing.for_all_dtypes(no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_ipow(self, xp, dtype):\n        def func(x, y):\n            x **= y\n\n        return func\n\n    @testing.for_int_dtypes(no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_ilshift(self, xp, dtype):\n        def func(x, y):\n            x <<= y\n\n        return func\n\n    @testing.for_int_dtypes(no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_irshift(self, xp, dtype):\n        def func(x, y):\n            x >>= y\n\n        return func\n\n    @testing.for_int_dtypes(no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_iand(self, xp, dtype):\n        def func(x, y):\n            x &= y\n\n        return func\n\n    @testing.for_int_dtypes(no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_ior(self, xp, dtype):\n        def func(x, y):\n            x |= y\n\n        return func\n\n    @testing.for_int_dtypes(no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_ixor(self, xp, dtype):\n        def func(x, y):\n            x ^= y\n\n        return func\n\n\n@testing.gpu\nclass TestFusionArraySetItem(unittest.TestCase):\n\n    def generate_inputs(self, xp):\n        x = testing.shaped_random((3, 4), xp, 'int32', scale=10, seed=0)\n        y = testing.shaped_random((3, 4), xp, 'int32', scale=10, seed=1)\n        return (x, y), {}\n\n    # TODO(imanishi): Fix TypeError in use of dtypes_combination test.\n    @fusion_utils.check_fusion(accept_error=TypeError)\n    def test_setitem_ellipsis(self, xp):\n        def func(x, y):\n            y[...] = x\n            return y\n\n        return func\n\n    # TODO(imanishi): Fix TypeError in use of dtypes_combination test.\n    @fusion_utils.check_fusion(accept_error=TypeError)\n    def test_setitem_non_slice(self, xp):\n        def func(x, y):\n            y[:] = x\n            return y\n\n        return func\n\n\n@testing.gpu\nclass TestFusionArrayCopy(unittest.TestCase):\n\n    def generate_inputs(self, xp, dtype):\n        x = testing.shaped_random((3, 4), xp, dtype, scale=10, seed=0)\n        return (x,), {}\n\n    @testing.for_all_dtypes()\n    @fusion_utils.check_fusion()\n    def test_copy(self, xp, dtype):\n        return lambda x: x.copy()\n\n\n@testing.gpu\nclass TestFusionArrayAsType(unittest.TestCase):\n\n    def generate_inputs(self, xp, dtype1, dtype2):\n        x = testing.shaped_random((3, 4), xp, dtype1, scale=10, seed=0)\n        return (x,), {}\n\n    @testing.for_all_dtypes(name='dtype1', no_complex=True)\n    @testing.for_all_dtypes(name='dtype2')\n    @fusion_utils.check_fusion()\n    def test_astype(self, xp, dtype1, dtype2):\n        return lambda x: x.astype(dtype2)\n"""
tests/cupy_tests/core_tests/fusion_tests/test_function.py,0,"b'import threading\nimport unittest\n\nimport mock\n\nimport cupy\nfrom cupy import testing\nfrom cupy_tests.core_tests.fusion_tests import fusion_utils\n\n\nclass FusionTestBase(unittest.TestCase):\n    def generate_inputs(self, xp, nargs, dtype):\n        inputs = [\n            testing.shaped_random((3, 4), xp, dtype, scale=10, seed=seed)\n            for seed in range(nargs)\n        ]\n        return inputs, {}\n\n    def dtype_combination(self, xp, dtype1, dtype2):\n        x = testing.shaped_random((3, 4), xp, dtype1, scale=10, seed=0)\n        y = testing.shaped_random((3, 4), xp, dtype2, scale=10, seed=1)\n        return (x, y), {}\n\n\n@testing.gpu\nclass TestFusionOutArg(unittest.TestCase):\n\n    def generate_inputs(self, xp, dtype1, dtype2):\n        x = testing.shaped_random((3, 4), xp, dtype1, scale=10, seed=0)\n        y = testing.shaped_random((3, 4), xp, dtype1, scale=10, seed=1)\n        z = testing.shaped_random((3, 4), xp, dtype2, scale=10, seed=2)\n        return (x, y, z), {}\n\n    @testing.for_all_dtypes_combination(\n        names=(\'dtype1\', \'dtype2\'), full=True, no_complex=True)\n    @fusion_utils.check_fusion(accept_error=TypeError)\n    def test_outarg_add(self, xp, dtype1, dtype2):\n        def func(x, y, z):\n            xp.add(x, y, out=z)\n            return z\n\n        return func\n\n\n@testing.gpu\nclass TestFusionInplaceUpdate(FusionTestBase):\n\n    @testing.for_all_dtypes(no_bool=True)\n    @fusion_utils.check_fusion(generate_inputs_args=(5,))\n    def test_outarg_mixed(self, xp, dtype):\n        def func(x, y, z, u, v):\n            xp.add(x, y, out=z)\n            xp.subtract(z, x, out=u)\n            xp.multiply(z, x, out=v)\n            return u\n\n        return func\n\n    @testing.for_all_dtypes(no_bool=True)\n    @fusion_utils.check_fusion(generate_inputs_args=(2,))\n    def test_iadd_multiple_times(self, xp, dtype):\n        def func(x, y):\n            x += y\n            x += y\n            x += y\n            return x\n\n        return func\n\n\n@testing.gpu\nclass TestFusionTuple(FusionTestBase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @fusion_utils.check_fusion(generate_inputs_args=(3,))\n    def test_tuple(self, xp, dtype):\n        def func(x, y, z):\n            w = x * y + z\n            (x, w) = (w, x)\n            return z * w + y + x\n\n        return func\n\n    @testing.for_all_dtypes(no_complex=True)\n    @fusion_utils.check_fusion(generate_inputs_args=(3,))\n    def test_return_tuple(self, xp, dtype):\n        def func(x, y, z):\n            return x + y, y + z, z * x\n\n        return func\n\n    @testing.for_all_dtypes(no_complex=True)\n    @fusion_utils.check_fusion(generate_inputs_args=(3,))\n    def test_multiple_outputdifferent_type_same_ufunc(self, xp, dtype):\n        def func(x, y, z):\n            x = x.astype(\'int32\')\n            y = x.astype(\'float32\')\n            return x + y, y + z, z + x\n\n        return func\n\n    @testing.for_all_dtypes(no_complex=True)\n    @fusion_utils.check_fusion(generate_inputs_args=(1,))\n    def test_return_empty_tuple(self, xp, dtype):\n        def func(x):\n            return ()\n\n        return func\n\n    @testing.for_all_dtypes(no_complex=True)\n    @fusion_utils.check_fusion(generate_inputs_args=(1,))\n    def test_return_singleton_tuple(self, xp, dtype):\n        def func(x):\n            return (x,)\n\n        return func\n\n\n@testing.gpu\nclass TestFusionReduction(FusionTestBase):\n\n    @testing.for_all_dtypes(no_bool=True)\n    @fusion_utils.check_fusion(generate_inputs_args=(1,))\n    def test_reduction_premap(self, xp, dtype):\n        def func(x):\n            return xp.sum(xp.sqrt(x) + 10)\n\n        return func\n\n    @testing.for_all_dtypes(no_bool=True)\n    @fusion_utils.check_fusion(generate_inputs_args=(1,))\n    def test_reduction_postmap(self, xp, dtype):\n        def func(x):\n            return xp.sqrt(xp.sum(x)) + 10\n\n        return func\n\n    @testing.for_all_dtypes_combination(\n        names=(\'dtype1\', \'dtype2\'), no_bool=True)\n    @fusion_utils.check_fusion(generate_inputs_name=\'dtype_combination\')\n    def test_reduction_pairwise_premap(self, xp, dtype1, dtype2):\n        def func(x, y):\n            return xp.sum(xp.sqrt(xp.abs(x - y)))\n\n        return func\n\n    @testing.for_all_dtypes(no_bool=True)\n    @fusion_utils.check_fusion(generate_inputs_args=(1,))\n    def test_sum_axis_0(self, xp, dtype):\n        def func(x):\n            return xp.sum(x, axis=0)\n\n        return func\n\n    @testing.for_all_dtypes(no_bool=True)\n    @fusion_utils.check_fusion(generate_inputs_args=(1,))\n    def test_sum_axis_1(self, xp, dtype):\n        def func(x):\n            return xp.sum(x, axis=1)\n\n        return func\n\n\n@testing.gpu\nclass TestFusionDecorator(unittest.TestCase):\n    def test_without_paren(self):\n        @cupy.fuse\n        def func_wo_paren(x):\n            """"""Fuse without parentheses""""""\n            return x + x\n\n        self.assertEqual(func_wo_paren.__name__, \'func_wo_paren\')\n        self.assertEqual(func_wo_paren.__doc__, \'Fuse without parentheses\')\n\n    def test_with_paren(self):\n        @cupy.fuse()\n        def func_w_paren(x):\n            """"""Fuse with parentheses""""""\n            return x + x\n\n        self.assertEqual(func_w_paren.__name__, \'func_w_paren\')\n        self.assertEqual(func_w_paren.__doc__, \'Fuse with parentheses\')\n\n\n@testing.gpu\nclass TestFusionKernelName(unittest.TestCase):\n\n    def check(self, xp, func, expected_name, is_elementwise):\n        a = xp.arange(0, 12, dtype=\'d\').reshape(3, 4)\n        b = xp.arange(5, 17, dtype=\'d\').reshape(3, 4)\n        c = xp.arange(13, 25, dtype=\'d\').reshape(3, 4)\n\n        # Test kernel name (with mock)\n        if xp is cupy:\n            target = (\n                cupy.ElementwiseKernel if is_elementwise\n                else cupy.ReductionKernel)\n            target_full_name = \'{}.{}\'.format(\n                target.__module__, target.__name__)\n\n            with mock.patch(target_full_name) as kernel:\n                func(a, b, c)\n                kernel.assert_called_once()\n                self.assertEqual(kernel.call_args[1][\'name\'], expected_name)\n\n        # Test there\'s no error in computation (without mock)\n        return func(a, b, c)\n\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_elementwise(self, xp):\n        def func(a, b, c):\n            @cupy.fuse()\n            def func_a1(x, y, z):\n                return (x + y) * z\n\n            return func_a1(a, b, c)\n\n        return self.check(xp, func, \'func_a1\', True)\n\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_elementwise_with_name(self, xp):\n        def func(a, b, c):\n            @cupy.fuse(kernel_name=\'abc\')\n            def func_a1(x, y, z):\n                return (x + y) * z\n\n            return func_a1(a, b, c)\n\n        return self.check(xp, func, \'abc\', True)\n\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_reduction_premap(self, xp):\n        def func(a, b, c):\n            @cupy.fuse()\n            def func_a1(x, y, z):\n                return xp.sum((x + y) * z)\n\n            return func_a1(a, b, c)\n\n        return self.check(xp, func, \'func_a1\', False)\n\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_reduction_postmap(self, xp):\n        def func(a, b, c):\n            @cupy.fuse()\n            def func_a1(x):\n                return xp.sqrt(xp.sum(x) + 10)\n\n            return func_a1(a)\n\n        return self.check(xp, func, \'func_a1\', False)\n\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_reduction_01(self, xp):\n        def func(a, b, c):\n            @cupy.fuse()\n            def func_a1(x, y, z):\n                return xp.sqrt(xp.prod(x + y * z, axis=1) + 10)\n\n            return func_a1(a, b, c)\n\n        return self.check(xp, func, \'func_a1\', False)\n\n    @testing.numpy_cupy_allclose(atol=1e-5)\n    def test_reduction_with_name(self, xp):\n        def func(a, b, c):\n            @cupy.fuse(kernel_name=\'abc\')\n            def func_a1(x, y, z):\n                return xp.sum((x + y) * z)\n\n            return func_a1(a, b, c)\n\n        return self.check(xp, func, \'abc\', False)\n\n\n@testing.gpu\nclass TestFusionScalar(FusionTestBase):\n\n    def generate_inputs(self, xp, dtype1, dtype2):\n        array = testing.shaped_random((3, 4), xp, dtype1, scale=10, seed=0)\n        return (array,), {}\n\n    @testing.for_all_dtypes_combination(names=(\'dtype1\', \'dtype2\'))\n    @fusion_utils.check_fusion()\n    def test_python_scalar(self, xp, dtype1, dtype2):\n        def func(array):\n            py_scalar = dtype2(1).item()\n            return array + py_scalar\n\n        return func\n\n    @testing.for_all_dtypes_combination(names=(\'dtype1\', \'dtype2\'))\n    @fusion_utils.check_fusion()\n    def test_numpy_scalar(self, xp, dtype1, dtype2):\n        def func(array):\n            np_scalar = dtype2(1)\n            return array + np_scalar\n\n        return func\n\n    def python_scalar_param(self, xp, dtype1, dtype2):\n        array = testing.shaped_random((3, 4), xp, dtype1, scale=10, seed=0)\n        py_scalar = dtype2(1).item()\n        return (array, py_scalar), {}\n\n    @testing.for_all_dtypes_combination(names=(\'dtype1\', \'dtype2\'))\n    @fusion_utils.check_fusion(generate_inputs_name=\'python_scalar_param\')\n    def test_python_scalar_param(self, xp, dtype1, dtype2):\n        def func(array, py_scalar):\n            return array + py_scalar\n\n        return func\n\n    def numpy_scalar_param(self, xp, dtype1, dtype2):\n        array = testing.shaped_random((3, 4), xp, dtype1, scale=10, seed=0)\n        np_scalar = dtype2(1)\n        return (array, np_scalar), {}\n\n    @testing.for_all_dtypes_combination(names=(\'dtype1\', \'dtype2\'))\n    @fusion_utils.check_fusion(generate_inputs_name=\'numpy_scalar_param\')\n    def test_numpy_scalar_param(self, xp, dtype1, dtype2):\n        def func(array, np_scalar):\n            return array + np_scalar\n\n        return func\n\n    def numpy_scalar_params_binop(self, xp, dtype1, dtype2):\n        scalar1 = dtype1(1)\n        scalar2 = dtype2(1)\n        array = testing.shaped_random((3, 4), xp, \'int64\', scale=10, seed=0)\n        return (scalar1, scalar2, array), {}\n\n    @testing.for_all_dtypes_combination(names=(\'dtype1\', \'dtype2\'))\n    @fusion_utils.check_fusion(\n        generate_inputs_name=\'numpy_scalar_params_binop\')\n    def test_numpy_scalar_params_binop(self, xp, dtype1, dtype2):\n        def func(scalar1, scalar2, array):\n            dtype = (scalar1 + scalar2).dtype\n            return array.astype(dtype)\n\n        return func\n\n    @testing.for_all_dtypes_combination(names=(\'dtype1\', \'dtype2\'))\n    @fusion_utils.check_fusion(\n        generate_inputs_name=\'numpy_scalar_params_binop\')\n    def test_scalar_inplace_update(self, xp, dtype1, dtype2):\n        def func(scalar1, scalar2, array):\n            scalar1_copy = scalar1\n            scalar1 += scalar2\n            return array + scalar1 + scalar1_copy\n\n        return func\n\n    @testing.for_all_dtypes_combination(names=(\'dtype1\', \'dtype2\'))\n    @fusion_utils.check_fusion(generate_inputs_name=\'numpy_scalar_param\')\n    def test_scalar_inplace_update_with_array(self, xp, dtype1, dtype2):\n        def func(array, scalar):\n            scalar += array\n            return scalar\n\n        return func\n\n\n@testing.gpu\nclass TestFusionBroadcast(unittest.TestCase):\n\n    def generate_inputs(self, xp, dtype):\n        x = testing.shaped_random((3, 4), xp, dtype, scale=10, seed=0)\n        y = testing.shaped_random((4,), xp, dtype, scale=10, seed=1)\n        return (x, y), {}\n\n    @testing.for_all_dtypes()\n    @fusion_utils.check_fusion()\n    def test_broadcast(self, xp, dtype):\n        def func(x, y):\n            x += y\n            return x\n\n        return func\n\n    @testing.for_all_dtypes()\n    @fusion_utils.check_fusion(accept_error=ValueError)\n    def test_broadcast_error(self, xp, dtype):\n        def func(x, y):\n            y += x\n            return y\n\n        return func\n\n\n@testing.gpu\nclass TestFusionNoneParams(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_array_equal()\n    def test_python_none_parameter(self, xp, dtype):\n        @cupy.fuse()\n        def f(x, y, z):\n            if y is None:\n                return x * z\n            return x + y + z\n\n        x = testing.shaped_arange((10,), xp, dtype)\n        y = testing.shaped_arange((10,), xp, dtype)\n        z = testing.shaped_arange((10,), xp, dtype)\n        return f(x, None, z) + f(x, y, z)\n\n\n@testing.gpu\nclass TestFusionReturnsConstantValue(FusionTestBase):\n\n    @testing.for_all_dtypes()\n    @fusion_utils.check_fusion(generate_inputs_args=(1,))\n    def test_pass(self, xp, dtype):\n        def func(x):\n            pass\n\n        return func\n\n    @testing.for_all_dtypes(no_bool=True)\n    @fusion_utils.check_fusion(generate_inputs_args=(1,))\n    def test_no_return_value(self, xp, dtype):\n        def func(x):\n            x += 1\n\n        return func\n\n\nclass TestFusionComposition(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_composition(self, xp, dtype):\n        @cupy.fuse()\n        def f(x, y):\n            return x - y * 2, x + y\n\n        @cupy.fuse()\n        def g(x, y, z):\n            a, b = f(x + z, z - x * 3)\n            c, d = f(x - y, y - z)\n            return a + b * c - d\n\n        @cupy.fuse()\n        def h(x, y):\n            a, b = f(x + y * 2, y * 3)\n            return a - b * g(x - 2, x - 3, -y)\n\n        x = testing.shaped_arange((3, 3), xp, dtype)\n        y = testing.shaped_arange((3, 3), xp, dtype)\n        return h(x, y)\n\n\nclass TestFusionCompile(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_bool=True)\n    @testing.numpy_cupy_array_equal()\n    def test_clear_cache(self, xp, dtype):\n        @cupy.fuse()\n        def f(x, y):\n            return x - y * 2\n\n        x = testing.shaped_arange((3, 3), xp, dtype)\n        y = testing.shaped_arange((3, 3), xp, dtype)\n        f.clear_cache()\n        return f(x, y)\n\n\n@testing.gpu\nclass TestFusionGetArrayModule(FusionTestBase):\n\n    @testing.for_all_dtypes()\n    @fusion_utils.check_fusion(generate_inputs_args=(1,))\n    def test_get_array_module(self, xp, dtype):\n        def func(x):\n            assert xp == cupy.get_array_module(x)\n            return x\n\n        return func\n\n\nclass TestFusionThread(unittest.TestCase):\n\n    def test_thread(self):\n        x = testing.shaped_arange((3, 3), cupy, cupy.int64)\n        y = testing.shaped_arange((3, 3), cupy, cupy.int64)\n        out = [None]\n\n        @cupy.fuse()\n        def f(x, y):\n            return x + y * 2\n\n        def _target(x, y):\n            cupy.cuda.Device(0).use()\n            out[0] = f(x, y)\n\n        t = threading.Thread(target=_target, args=(x, y))\n        t.daemon = True\n        t.start()\n        t.join()\n        assert (out[0] == f(x, y)).all()\n\n    @testing.numpy_cupy_array_equal()\n    def test_thread_multiple_dtypes(self, xp):\n        x1 = testing.shaped_arange((3, 3), xp, xp.int64)\n        y1 = testing.shaped_arange((3, 3), xp, xp.int64)\n        x2 = x1.astype(xp.float64)\n        y2 = y1.astype(xp.float64)\n        threads = [None] * 100\n        out = [None] * 100\n\n        @cupy.fuse()\n        def f(x, y):\n            return x + y * 2\n\n        def _target(tid, x, y):\n            if xp is cupy:\n                xp.cuda.Device(0).use()\n            out[tid] = f(x, y).astype(xp.int64)\n\n        def run_thread(tid):\n            x, y = (x1, y1) if tid % 2 == 0 else (x2, y2)\n            t = threading.Thread(target=_target, args=(tid, x, y))\n            threads[tid] = t\n            t.daemon = True\n            t.start()\n\n        for tid in range(0, 50):\n            run_thread(tid)\n\n        for tid in range(0, 50):\n            threads[tid].join()\n\n        for tid in range(50, 100):\n            run_thread(tid)\n\n        for tid in range(50, 100):\n            threads[tid].join()\n\n        return xp.concatenate(out)\n'"
tests/cupy_tests/core_tests/fusion_tests/test_ufunc.py,0,"b""import unittest\n\nimport numpy\n\nfrom cupy import testing\nfrom cupy_tests.core_tests.fusion_tests import fusion_utils\n\n\nclass FusionUnaryUfuncTestBase(unittest.TestCase):\n\n    def generate_inputs(self, xp, dtype):\n        x = testing.shaped_random((3, 4), xp, dtype, scale=10, seed=0)\n        return (x,), {}\n\n\nclass FusionBinaryUfuncTestBase(unittest.TestCase):\n\n    def generate_inputs(self, xp, dtype1, dtype2):\n        x = testing.shaped_random((3, 4), xp, dtype1, scale=10, seed=0)\n        y = testing.shaped_random((3, 4), xp, dtype2, scale=10, seed=1)\n        return (x, y), {}\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': [\n        'bitwise_and', 'bitwise_or', 'bitwise_xor', 'left_shift', 'right_shift'\n    ]\n}))\nclass TestFusionBitwiseBinary(FusionBinaryUfuncTestBase):\n\n    @testing.for_int_dtypes_combination(names=('dtype1', 'dtype2'))\n    @fusion_utils.check_fusion()\n    def test_bitwise(self, xp, dtype1, dtype2):\n        def impl(x, y):\n            if ((x.dtype == 'uint64' and y.dtype.kind == 'i')\n                    or (y.dtype == 'uint64' and x.dtype.kind == 'i')):\n                # Skip TypeError case.\n                return\n            return getattr(xp, self.func)(x, y)\n        return impl\n\n\nclass TestFusionBitwiseUnary(FusionUnaryUfuncTestBase):\n\n    @testing.for_int_dtypes()\n    @fusion_utils.check_fusion()\n    def test_invert(self, xp, dtype):\n        return lambda x: xp.invert(x)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': [\n        'greater', 'greater_equal', 'less', 'less_equal', 'equal', 'not_equal',\n        'logical_and', 'logical_or', 'logical_xor',\n        'maximum', 'minimum', 'fmax', 'fmin',\n    ]\n}))\nclass TestFusionComparisonBinary(FusionBinaryUfuncTestBase):\n\n    @testing.for_all_dtypes_combination(\n        no_complex=True, names=('dtype1', 'dtype2'))\n    @fusion_utils.check_fusion()\n    def test_comparison(self, xp, dtype1, dtype2):\n        return lambda x, y: getattr(xp, self.func)(x, y)\n\n\n@testing.gpu\nclass TestFusionComparisonUnary(FusionUnaryUfuncTestBase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @fusion_utils.check_fusion()\n    def test_comparison(self, xp, dtype):\n        return lambda x: xp.logical_not(x)\n\n\n@testing.gpu\nclass TestFusionArrayContents(FusionUnaryUfuncTestBase):\n\n    def generate_inputs(self, xp, has_nan, dtype):\n        if numpy.dtype(dtype).kind not in ('f', 'c'):\n            return super(TestFusionArrayContents, self).generate_inputs(\n                xp, dtype)\n\n        nan = numpy.nan\n        inf = dtype(float('inf'))\n\n        if has_nan:\n            x = xp.array([-3, nan, -1, nan, 0, nan, inf], dtype=dtype)\n        else:\n            x = xp.array([-3, inf, -1, -inf, 0, 1, 2], dtype=dtype)\n        return (x,), {}\n\n    @testing.for_all_dtypes()\n    @fusion_utils.check_fusion(generate_inputs_args=(False,))\n    def test_isfinite(self, xp, dtype):\n        return lambda x: xp.isfinite(x)\n\n    @testing.for_all_dtypes()\n    @fusion_utils.check_fusion(generate_inputs_args=(False,))\n    def test_isinf(self, xp, dtype):\n        return lambda x: xp.isinf(x)\n\n    @testing.for_all_dtypes()\n    @fusion_utils.check_fusion(generate_inputs_args=(True,))\n    def test_isnan(self, xp, dtype):\n        return lambda x: xp.isnan(x)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': [\n        'sin', 'cos', 'tan', 'arcsin', 'arccos', 'arctan',\n        'sinh', 'cosh', 'tanh', 'arcsinh', 'arccosh', 'arctanh',\n    ],\n}))\nclass TestFusionTrigonometricUnary(unittest.TestCase):\n\n    def generate_inputs(self, xp, dtype):\n        if numpy.dtype(dtype).kind not in ('f', 'c'):\n            x = xp.array([0, 1])\n        else:\n            x = testing.shaped_random((3, 4), xp, dtype, scale=1, seed=0)\n        return (x,), {}\n\n    @testing.for_all_dtypes()\n    @fusion_utils.check_fusion()\n    def test_trigonometric(self, xp, dtype):\n        def impl(x):\n            with testing.NumpyError(divide='ignore', invalid='ignore'):\n                return getattr(xp, self.func)(x)\n        return impl\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': ['arctan2', 'hypot']\n}))\nclass TestFusionTrigonometricBinary(FusionBinaryUfuncTestBase):\n\n    @testing.for_all_dtypes_combination(\n        no_complex=True, names=('dtype1', 'dtype2'))\n    @fusion_utils.check_fusion()\n    def test_trigonometric(self, xp, dtype1, dtype2):\n        return lambda x, y: getattr(xp, self.func)(x, y)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': ['deg2rad', 'rad2deg', 'degrees', 'radians']\n}))\nclass TestFusionDegRad(FusionUnaryUfuncTestBase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @fusion_utils.check_fusion()\n    def test_trigonometric(self, xp, dtype):\n        return lambda x: getattr(xp, self.func)(x)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': ['around', 'round_', 'rint', 'floor', 'ceil', 'trunc', 'fix']\n}))\nclass TestFusionRounding(FusionUnaryUfuncTestBase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @fusion_utils.check_fusion()\n    def test_rounding(self, xp, dtype):\n        return lambda x: getattr(xp, self.func)(x)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': ['exp', 'expm1', 'exp2', 'log', 'log10', 'log2', 'log1p']\n}))\nclass TestFusionExpLogUnary(unittest.TestCase):\n\n    def generate_inputs(self, xp, dtype):\n        x = testing.shaped_random((3, 4), xp, dtype, scale=10, seed=0) + 1\n        return (x,), {}\n\n    @testing.for_all_dtypes()\n    @fusion_utils.check_fusion()\n    def test_explog(self, xp, dtype):\n        return lambda x: getattr(xp, self.func)(x)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': ['logaddexp', 'logaddexp2']\n}))\nclass TestFusionExpLogBinary(FusionBinaryUfuncTestBase):\n\n    @testing.for_all_dtypes_combination(\n        no_complex=True, names=('dtype1', 'dtype2'))\n    @fusion_utils.check_fusion()\n    def test_explog(self, xp, dtype1, dtype2):\n        return lambda x, y: getattr(xp, self.func)(x, y)\n\n\n@testing.gpu\nclass TestFusionLdexp(FusionBinaryUfuncTestBase):\n\n    @testing.for_float_dtypes(name='dtype1')\n    @testing.for_dtypes(['i', 'l'], name='dtype2')\n    @fusion_utils.check_fusion()\n    def test_explog(self, xp, dtype1, dtype2):\n        return lambda x, y: xp.ldexp(x, y)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': ['signbit', 'frexp']\n}))\nclass TestFusionFloatingUnary(FusionUnaryUfuncTestBase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @fusion_utils.check_fusion()\n    def test_floating_point_routine(self, xp, dtype):\n        return lambda x: getattr(xp, self.func)(x)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': ['copysign', 'nextafter']\n}))\nclass TestFusionFloatingBinary(FusionBinaryUfuncTestBase):\n\n    @testing.for_all_dtypes_combination(\n        names=('dtype1', 'dtype2'), no_complex=True)\n    @fusion_utils.check_fusion()\n    def test_floating_point_routine(self, xp, dtype1, dtype2):\n        return lambda x, y: getattr(xp, self.func)(x, y)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': ['reciprocal', 'negative', 'angle', 'conj', 'real', 'imag']\n}))\nclass TestArithmeticUnary(FusionUnaryUfuncTestBase):\n\n    def generate_inputs(self, xp, dtype):\n        x = testing.shaped_random((3, 4), xp, dtype, scale=10, seed=0)\n        x[x == 0] = 1\n        return (x,), {}\n\n    @testing.for_all_dtypes(no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_arithmetic(self, xp, dtype):\n        return lambda x: getattr(xp, self.func)(x)\n\n\n@testing.gpu\nclass TestModf(FusionUnaryUfuncTestBase):\n\n    def generate_inputs(self, xp, dtype):\n        x = testing.shaped_random((3, 4), xp, dtype, scale=10, seed=0)\n        return (x,), {}\n\n    @testing.for_all_dtypes(no_complex=True)\n    @fusion_utils.check_fusion()\n    def test_arithmetic(self, xp, dtype):\n        return lambda x: xp.modf(x)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': ['add', 'subtract', 'multiply', 'power']\n}))\nclass TestArithmeticBinary(FusionBinaryUfuncTestBase):\n\n    def generate_inputs(self, xp, dtype1, dtype2):\n        x = testing.shaped_random((3, 4), xp, dtype1, scale=5, seed=0)\n        y = testing.shaped_random((3, 4), xp, dtype2, scale=5, seed=0)\n        return (x, y), {}\n\n    @testing.for_all_dtypes_combination(\n        names=('dtype1', 'dtype2'), no_complex=True, no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_arithmetic(self, xp, dtype1, dtype2):\n        # TODO(unno): boolean subtract causes DeprecationWarning in numpy>=1.13\n        return lambda x, y: getattr(xp, self.func)(x, y)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': ['divide', 'true_divide', 'floor_divide', 'fmod', 'remainder']\n}))\nclass TestDivide(unittest.TestCase):\n\n    def generate_inputs(self, xp, dtype1, dtype2):\n        x = testing.shaped_random((3, 4), xp, dtype1, scale=10, seed=0)\n        y = testing.shaped_random((3, 4), xp, dtype2, scale=10, seed=1)\n        y[y == 0] = 1\n        return (x, y), {}\n\n    @testing.for_all_dtypes_combination(\n        names=('dtype1', 'dtype2'), no_complex=True)\n    @fusion_utils.check_fusion()\n    def test_divide(self, xp, dtype1, dtype2):\n        return lambda x, y: getattr(xp, self.func)(x, y)\n\n\n@testing.gpu\nclass TestDivmod(unittest.TestCase):\n\n    def generate_inputs(self, xp, dtype1, dtype2):\n        x = testing.shaped_random((3, 4), xp, dtype1, scale=10, seed=0)\n        y = testing.shaped_random((3, 4), xp, dtype2, scale=10, seed=1)\n        y[y == 0] = 1\n        return (x, y), {}\n\n    @testing.for_all_dtypes_combination(\n        names=('dtype1', 'dtype2'), no_complex=True)\n    @fusion_utils.check_fusion()\n    def test_divmod(self, xp, dtype1, dtype2):\n        return lambda x, y: xp.divmod(x, y)\n\n\n@testing.gpu\nclass TestFusionMisc(FusionUnaryUfuncTestBase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @fusion_utils.check_fusion()\n    def test_sqrt(self, xp, dtype):\n        return lambda x: xp.sqrt(x)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @fusion_utils.check_fusion()\n    def test_cbrt(self, xp, dtype):\n        return lambda x: xp.cbrt(x)\n\n    @testing.for_all_dtypes()\n    @fusion_utils.check_fusion()\n    def test_square(self, xp, dtype):\n        return lambda x: xp.square(x)\n\n    @testing.for_all_dtypes(no_complex=True, no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_absolute(self, xp, dtype):\n        return lambda x: xp.absolute(x)\n\n    @testing.for_all_dtypes(no_complex=True, no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_abs(self, xp, dtype):\n        return lambda x: xp.abs(x)\n\n    @testing.for_all_dtypes(no_complex=True, no_bool=True)\n    @fusion_utils.check_fusion()\n    def test_sign(self, xp, dtype):\n        return lambda x: xp.sign(x)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @fusion_utils.check_fusion()\n    def test_clip(self, xp, dtype):\n        return lambda x: xp.clip(x, dtype(2), dtype(4))\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': ['i0', 'sinc']\n}))\nclass TestFusionSpecialMath(FusionUnaryUfuncTestBase):\n\n    # TODO(imanishi): Fix for integer tests\n    @testing.for_float_dtypes()\n    @fusion_utils.check_fusion()\n    def test_special_math(self, xp, dtype):\n        return lambda x: getattr(xp, self.func)(x)\n\n\nclass TestFusionManipulation(unittest.TestCase):\n\n    def generate_inputs(self, xp, dtype1, dtype2):\n        cond = testing.shaped_random((3, 4), xp, 'bool_', seed=0)\n        x = testing.shaped_random((3, 4), xp, dtype1, scale=10, seed=1)\n        y = testing.shaped_random((3, 4), xp, dtype2, scale=10, seed=2)\n        return (cond, x, y), {}\n\n    @testing.for_all_dtypes_combination(names=('dtype1', 'dtype2'))\n    @fusion_utils.check_fusion()\n    def test_where(self, xp, dtype1, dtype2):\n        return lambda cond, x, y: xp.where(cond, x, y)\n\n    # TODO(imanishi): Supoort complex dtypes\n    @testing.for_all_dtypes_combination(\n        names=('dtype1', 'dtype2'), no_complex=True)\n    @fusion_utils.check_fusion(accept_error=(TypeError,))\n    def test_copyto(self, xp, dtype1, dtype2):\n        return lambda cond, x, y: xp.copyto(x, y)\n\n    # TODO(imanishi): Supoort complex dtypes\n    @testing.for_all_dtypes_combination(\n        names=('dtype1', 'dtype2'), no_complex=True)\n    @fusion_utils.check_fusion(accept_error=(TypeError,))\n    def test_copyto_where(self, xp, dtype1, dtype2):\n        return lambda cond, x, y: xp.where(x, y, where=cond)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': ['sum', 'prod', 'amax', 'amin', 'max', 'min']\n}))\nclass TestFusionNumericalReduction(FusionUnaryUfuncTestBase):\n\n    @testing.for_all_dtypes()\n    @fusion_utils.check_fusion()\n    def test_reduction(self, xp, dtype):\n        return lambda x: getattr(xp, self.func)(x)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'func': ['all', 'any']\n}))\nclass TestFusionLogicalReduction(FusionUnaryUfuncTestBase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @fusion_utils.check_fusion()\n    def test_reduction(self, xp, dtype):\n        return lambda x: getattr(xp, self.func)(x)\n"""
tests/cupy_tests/cuda_tests/memory_hooks_tests/__init__.py,0,b''
tests/cupy_tests/cuda_tests/memory_hooks_tests/test_debug_print.py,0,"b""import io\nimport json\nimport unittest\n\nimport cupy.cuda\nfrom cupy.cuda import memory\nfrom cupy.cuda import memory_hooks\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestDebugPrintHook(unittest.TestCase):\n\n    def setUp(self):\n        self.io = io.StringIO()\n        self.hook = memory_hooks.DebugPrintHook(file=self.io)\n        self.pool = memory.MemoryPool()\n\n    def test_print(self):\n        device_id = 0\n        size = 1\n        unit = 512\n        with cupy.cuda.Device(device_id):\n            with self.hook:\n                mem = self.pool.malloc(size)\n                ptr1, pmem1 = mem.ptr, id(mem.mem)\n                del mem\n                mem = self.pool.malloc(size)\n                ptr2, pmem2 = mem.ptr, id(mem.mem)\n                del mem\n        actual_lines = self.io.getvalue().splitlines()\n\n        expect = {'hook': 'alloc', 'device_id': device_id,\n                  'mem_size': unit, 'mem_ptr': ptr1}\n        self.assertEqual(expect, json.loads(actual_lines[0]))\n\n        expect = {'hook': 'malloc', 'device_id': device_id, 'size': size,\n                  'mem_size': unit, 'mem_ptr': ptr1, 'pmem_id': hex(pmem1)}\n        self.assertEqual(expect, json.loads(actual_lines[1]))\n\n        expect = {'hook': 'free', 'device_id': device_id,\n                  'mem_size': unit, 'mem_ptr': ptr1, 'pmem_id': hex(pmem1)}\n        self.assertEqual(expect, json.loads(actual_lines[2]))\n\n        expect = {'hook': 'malloc', 'device_id': device_id, 'size': size,\n                  'mem_size': unit, 'mem_ptr': ptr2, 'pmem_id': hex(pmem2)}\n        self.assertEqual(expect, json.loads(actual_lines[3]))\n\n        expect = {'hook': 'free', 'device_id': device_id,\n                  'mem_size': unit, 'mem_ptr': ptr2, 'pmem_id': hex(pmem2)}\n        self.assertEqual(expect, json.loads(actual_lines[4]))\n"""
tests/cupy_tests/cuda_tests/memory_hooks_tests/test_line_profile.py,0,"b""import io\nimport unittest\n\nfrom cupy.cuda import memory\nfrom cupy.cuda import memory_hooks\nfrom cupy import testing\n\n\n@testing.gpu\nclass TestLineProfileHook(unittest.TestCase):\n\n    def setUp(self):\n        self.pool = memory.MemoryPool()\n\n    def test_print_report(self):\n        hook = memory_hooks.LineProfileHook()\n        p = self.pool.malloc(1000)\n        del p\n        with hook:\n            p1 = self.pool.malloc(1000)\n            p2 = self.pool.malloc(2000)\n        del p1\n        del p2\n        f = io.StringIO()\n        hook.print_report(file=f)\n        actual = f.getvalue()\n        expect = r'\\A_root \\(3\\.00KB, 2\\.00KB\\)'\n        self.assertRegex(actual, expect)\n        expect = r'.*\\.py:[0-9]+:test_print_report \\(1\\.00KB, 0\\.00B\\)'\n        self.assertRegex(actual, expect)\n        expect = r'.*\\.py:[0-9]+:test_print_report \\(2\\.00KB, 2\\.00KB\\)'\n        self.assertRegex(actual, expect)\n\n    def test_print_report_max_depth(self):\n        hook = memory_hooks.LineProfileHook(max_depth=1)\n        with hook:\n            p = self.pool.malloc(1000)\n        del p\n        f = io.StringIO()\n        hook.print_report(file=f)\n        actual = f.getvalue()\n        self.assertEqual(2, len(actual.split('\\n')))\n\n        hook = memory_hooks.LineProfileHook(max_depth=2)\n        with hook:\n            p = self.pool.malloc(1000)\n        del p\n        f = io.StringIO()\n        hook.print_report(file=f)\n        actual = f.getvalue()\n        self.assertEqual(3, len(actual.split('\\n')))\n"""
tests/cupyx_tests/linalg_tests/sparse_tests/__init__.py,0,b''
tests/cupyx_tests/linalg_tests/sparse_tests/test_solve.py,0,"b""import unittest\n\nimport numpy\nimport pytest\ntry:\n    import scipy.linalg\n    import scipy.sparse\n    import scipy.stats\n\n    scipy_available = True\nexcept ImportError:\n    scipy_available = False\n\nimport cupy as cp\nimport cupy.sparse as sp\nfrom cupy import testing\nfrom cupy.testing import condition\nimport cupyx\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64],\n}))\n@unittest.skipUnless(scipy_available, 'requires scipy')\nclass TestLschol(unittest.TestCase):\n\n    def setUp(self):\n        rvs = scipy.stats.randint(0, 15).rvs\n        self.A = scipy.sparse.random(\n            50, 50, density=0.2, data_rvs=rvs, dtype=self.dtype)\n        self.b = numpy.random.randint(5, size=50)\n        # symmetric and positive definite\n        self.A = self.A.T*self.A + 10*scipy.sparse.eye(50)\n        self.b = self.A.T*self.b\n        # initial scipy results by dense cholesky method.\n        L = scipy.linalg.cho_factor(self.A.todense())\n        self.x = scipy.linalg.cho_solve(L, self.b)\n        if self.dtype == numpy.float64:\n            self.decimal = 8\n        else:\n            self.decimal = 3\n\n    def test_size(self):\n        A = sp.csr_matrix(self.A, dtype=self.dtype)\n        b = cp.array(numpy.append(self.b, [1]), dtype=self.dtype)\n        with pytest.raises(ValueError):\n            cupyx.linalg.sparse.lschol(A, b)\n\n    def test_shape(self):\n        A = sp.csr_matrix(self.A, dtype=self.dtype)\n        b = cp.array(numpy.tile(self.b, (2, 1)), dtype=self.dtype)\n        with pytest.raises(ValueError):\n            cupyx.linalg.sparse.lschol(A, b)\n\n    @condition.retry(10)\n    def test_csrmatrix(self):\n        A = sp.csr_matrix(self.A, dtype=self.dtype)\n        b = cp.array(self.b, dtype=self.dtype)\n        x = cupyx.linalg.sparse.lschol(A, b)\n        testing.assert_array_almost_equal(x, self.x, decimal=self.decimal)\n\n    @condition.retry(10)\n    def test_ndarray(self):\n        A = cp.array(self.A.A, dtype=self.dtype)\n        b = cp.array(self.b, dtype=self.dtype)\n        x = cupyx.linalg.sparse.lschol(A, b)\n        testing.assert_array_almost_equal(x, self.x, decimal=self.decimal)\n"""
tests/cupyx_tests/scipy_tests/fft_tests/__init__.py,0,b''
tests/cupyx_tests/scipy_tests/fft_tests/test_fft.py,4,"b'import unittest\n\nfrom cupy import testing\nfrom cupy.fft.fft import _default_fft_func, _fftn\nimport cupyx.scipy.fft as cp_fft\nimport numpy as np\nimport cupy as cp\nimport pytest\n\n\ndef _fft_module(xp):\n    # Test cupyx.scipy against numpy since scipy.fft is not yet released\n    if xp != np:\n        return cp_fft\n    else:\n        return np.fft\n\n\ndef _correct_np_dtype(xp, dtype, out):\n    # NumPy always transforms in double precision, cast output to correct type\n    if xp == np:\n        if dtype in [np.float16, np.float32, np.complex64]:\n            if out.dtype.kind == \'f\':\n                return out.astype(np.float32)\n            else:\n                return out.astype(np.complex64)\n    return out\n\n\n@testing.parameterize(*testing.product({\n    \'n\': [None, 0, 5, 10, 15],\n    \'shape\': [(9,), (10,), (10, 9), (10, 10)],\n    \'axis\': [-1, 0],\n    \'norm\': [None, \'ortho\']\n}))\n@testing.gpu\nclass TestFft(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = _fft_module(xp).fft(x, n=self.n, axis=self.axis, norm=self.norm)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft_overwrite(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        overwrite_kw = {} if xp == np else {\'overwrite_x\': True}\n        out = _fft_module(xp).fft(x, n=self.n, axis=self.axis, norm=self.norm,\n                                  **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when the output array is of size 0\n        # because cuFFT and numpy raise different kinds of exceptions\n        if self.n == 0:\n            return x\n        x_orig = x.copy()\n        if xp is cp:\n            overwrite_kw = {\'plan\': _fft_module(xp).get_fft_plan(\n                x, shape=self.n, axes=self.axis)}\n        else:\n            overwrite_kw = {}\n        out = _fft_module(xp).fft(x, n=self.n, axis=self.axis, norm=self.norm,\n                                  **overwrite_kw)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft_overwrite_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when the output array is of size 0\n        # because cuFFT and numpy raise different kinds of exceptions\n        if self.n == 0:\n            return x\n        if xp is cp:\n            overwrite_kw = {\'plan\': _fft_module(xp).get_fft_plan(\n                x, shape=self.n, axes=self.axis), \'overwrite_x\': True}\n        else:\n            overwrite_kw = {}\n        out = _fft_module(xp).fft(x, n=self.n, axis=self.axis, norm=self.norm,\n                                  **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft_plan_manager(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when the output array is of size 0\n        # because cuFFT and numpy raise different kinds of exceptions\n        if self.n == 0:\n            return x\n        x_orig = x.copy()\n        if xp is cp:\n            from cupy.cuda.cufft import get_current_plan\n            plan = _fft_module(xp).get_fft_plan(x, shape=self.n,\n                                                axes=self.axis)\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = _fft_module(xp).fft(x, n=self.n, axis=self.axis)\n            assert get_current_plan() is None\n        else:\n            out = _fft_module(xp).fft(x, n=self.n, axis=self.axis)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifft(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = _fft_module(xp).ifft(x, n=self.n, axis=self.axis, norm=self.norm)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifft_overwrite(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        overwrite_kw = {} if xp == np else {\'overwrite_x\': True}\n        out = _fft_module(xp).ifft(x, n=self.n, axis=self.axis, norm=self.norm,\n                                   **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifft_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when the output array is of size 0\n        # because cuFFT and numpy raise different kinds of exceptions\n        if self.n == 0:\n            return x\n        x_orig = x.copy()\n        if xp is cp:\n            overwrite_kw = {\'plan\': _fft_module(xp).get_fft_plan(\n                x, shape=self.n, axes=self.axis)}\n        else:\n            overwrite_kw = {}\n        out = _fft_module(xp).ifft(x, n=self.n, axis=self.axis, norm=self.norm,\n                                   **overwrite_kw)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifft_overwrite_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when the output array is of size 0\n        # because cuFFT and numpy raise different kinds of exceptions\n        if self.n == 0:\n            return x\n        if xp is cp:\n            overwrite_kw = {\'plan\': _fft_module(xp).get_fft_plan(\n                x, shape=self.n, axes=self.axis), \'overwrite_x\': True}\n        else:\n            overwrite_kw = {}\n        out = _fft_module(xp).ifft(x, n=self.n, axis=self.axis, norm=self.norm,\n                                   **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifft_plan_manager(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when the output array is of size 0\n        # because cuFFT and numpy raise different kinds of exceptions\n        if self.n == 0:\n            return x\n        x_orig = x.copy()\n        if xp is cp:\n            from cupy.cuda.cufft import get_current_plan\n            plan = _fft_module(xp).get_fft_plan(x, shape=self.n,\n                                                axes=self.axis)\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = _fft_module(xp).ifft(x, n=self.n, axis=self.axis)\n            assert get_current_plan() is None\n        else:\n            out = _fft_module(xp).ifft(x, n=self.n, axis=self.axis)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n\n@testing.parameterize(*(\n    testing.product({\n        \'shape\': [(3, 4)],\n        \'s\': [None, (1, 5)],\n        \'axes\': [None, (-2, -1), (-1, -2), (0,)],\n        \'norm\': [None, \'ortho\']\n    })\n    + testing.product({\n        \'shape\': [(2, 3, 4)],\n        \'s\': [None, (1, 5), (1, 4, 10)],\n        \'axes\': [None, (-2, -1), (-1, -2, -3)],\n        \'norm\': [None, \'ortho\']\n    })))\n@testing.gpu\nclass TestFft2(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft2(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = _fft_module(xp).fft2(x, s=self.s, axes=self.axes, norm=self.norm)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft2_overwrite(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        overwrite_kw = {} if xp == np else {\'overwrite_x\': True}\n        out = _fft_module(xp).fft2(x, s=self.s, axes=self.axes,\n                                   norm=self.norm, **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft2_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        x_orig = x.copy()\n        if xp is cp:\n            overwrite_kw = {\'plan\': _fft_module(xp).get_fft_plan(\n                x, shape=self.s, axes=self.axes)}\n        else:\n            overwrite_kw = {}\n        out = _fft_module(xp).fft2(x, s=self.s, axes=self.axes, norm=self.norm,\n                                   **overwrite_kw)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft2_overwrite_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        if xp is cp:\n            overwrite_kw = {\'plan\': _fft_module(xp).get_fft_plan(\n                x, shape=self.s, axes=self.axes), \'overwrite_x\': True}\n        else:\n            overwrite_kw = {}\n        out = _fft_module(xp).fft2(x, s=self.s, axes=self.axes, norm=self.norm,\n                                   **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fft2_plan_manager(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        x_orig = x.copy()\n        if xp is cp:\n            from cupy.cuda.cufft import get_current_plan\n            plan = _fft_module(xp).get_fft_plan(x, shape=self.s,\n                                                axes=self.axes)\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = _fft_module(xp).fft2(x, s=self.s, axes=self.axes)\n            assert get_current_plan() is None\n        else:\n            out = _fft_module(xp).fft2(x, s=self.s, axes=self.axes)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifft2(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = _fft_module(xp).ifft2(\n            x, s=self.s, axes=self.axes, norm=self.norm)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifft2_overwrite(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        overwrite_kw = {} if xp == np else {\'overwrite_x\': True}\n        out = _fft_module(xp).ifft2(x, s=self.s, axes=self.axes,\n                                    norm=self.norm, **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifft2_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        x_orig = x.copy()\n        if xp is cp:\n            overwrite_kw = {\'plan\': _fft_module(xp).get_fft_plan(\n                x, shape=self.s, axes=self.axes)}\n        else:\n            overwrite_kw = {}\n        out = _fft_module(xp).ifft2(x, s=self.s, axes=self.axes,\n                                    norm=self.norm, **overwrite_kw)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifft2_overwrite_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        if xp is cp:\n            overwrite_kw = {\'plan\': _fft_module(xp).get_fft_plan(\n                x, shape=self.s, axes=self.axes), \'overwrite_x\': True}\n        else:\n            overwrite_kw = {}\n        out = _fft_module(xp).ifft2(x, s=self.s, axes=self.axes,\n                                    norm=self.norm, **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifft2_plan_manager(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        x_orig = x.copy()\n        if xp is cp:\n            from cupy.cuda.cufft import get_current_plan\n            plan = _fft_module(xp).get_fft_plan(x, shape=self.s,\n                                                axes=self.axes)\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = _fft_module(xp).ifft2(x, s=self.s, axes=self.axes)\n            assert get_current_plan() is None\n        else:\n            out = _fft_module(xp).ifft2(x, s=self.s, axes=self.axes)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n\n@testing.parameterize(*(\n    testing.product({\n        \'shape\': [(3, 4)],\n        \'s\': [None, (1, 5)],\n        \'axes\': [None, (-2, -1), (-1, -2), (0,)],\n        \'norm\': [None, \'ortho\']\n    })\n    + testing.product({\n        \'shape\': [(2, 3, 4)],\n        \'s\': [None, (1, 5), (1, 4, 10)],\n        \'axes\': [None, (-2, -1), (-1, -2, -3)],\n        \'norm\': [None, \'ortho\']\n    })\n    + testing.product({\n        \'shape\': [(2, 3, 4, 5)],\n        \'s\': [None],\n        \'axes\': [None, (0, 1, 2, 3)],\n        \'norm\': [None, \'ortho\']\n    })))\n@testing.gpu\nclass TestFftn(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fftn(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = _fft_module(xp).fftn(x, s=self.s, axes=self.axes,\n                                   norm=self.norm)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fftn_overwrite(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        overwrite_kw = {} if xp == np else {\'overwrite_x\': True}\n        out = _fft_module(xp).fftn(x, s=self.s, axes=self.axes,\n                                   norm=self.norm, **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fftn_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        x_orig = x.copy()\n        if xp is cp:\n            overwrite_kw = {\'plan\': _fft_module(xp).get_fft_plan(\n                x, shape=self.s, axes=self.axes)}\n        else:\n            overwrite_kw = {}\n        out = _fft_module(xp).fftn(x, s=self.s, axes=self.axes, norm=self.norm,\n                                   **overwrite_kw)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fftn_overwrite_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        if xp is cp:\n            overwrite_kw = {\'plan\': _fft_module(xp).get_fft_plan(\n                x, shape=self.s, axes=self.axes), \'overwrite_x\': True}\n        else:\n            overwrite_kw = {}\n        out = _fft_module(xp).fftn(x, s=self.s, axes=self.axes, norm=self.norm,\n                                   **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_fftn_plan_manager(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        x_orig = x.copy()\n        if xp is cp:\n            from cupy.cuda.cufft import get_current_plan\n            plan = _fft_module(xp).get_fft_plan(x, shape=self.s,\n                                                axes=self.axes)\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = _fft_module(xp).fftn(x, s=self.s, axes=self.axes)\n            assert get_current_plan() is None\n        else:\n            out = _fft_module(xp).fftn(x, s=self.s, axes=self.axes)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifftn(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = _fft_module(xp).ifftn(x, s=self.s, axes=self.axes,\n                                    norm=self.norm)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifftn_overwrite(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        overwrite_kw = {} if xp == np else {\'overwrite_x\': True}\n        out = _fft_module(xp).ifftn(x, s=self.s, axes=self.axes,\n                                    norm=self.norm, **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifftn_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        x_orig = x.copy()\n        if xp is cp:\n            overwrite_kw = {\'plan\': _fft_module(xp).get_fft_plan(\n                x, shape=self.s, axes=self.axes)}\n        else:\n            overwrite_kw = {}\n        out = _fft_module(xp).ifftn(x, s=self.s, axes=self.axes,\n                                    norm=self.norm, **overwrite_kw)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifftn_overwrite_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        if xp is cp:\n            overwrite_kw = {\'plan\': _fft_module(xp).get_fft_plan(\n                x, shape=self.s, axes=self.axes), \'overwrite_x\': True}\n        else:\n            overwrite_kw = {}\n        out = _fft_module(xp).ifftn(x, s=self.s, axes=self.axes,\n                                    norm=self.norm, **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ifftn_plan_manager(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        x_orig = x.copy()\n        if xp is cp:\n            from cupy.cuda.cufft import get_current_plan\n            plan = _fft_module(xp).get_fft_plan(x, shape=self.s,\n                                                axes=self.axes)\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = _fft_module(xp).ifftn(x, s=self.s, axes=self.axes)\n            assert get_current_plan() is None\n        else:\n            out = _fft_module(xp).ifftn(x, s=self.s, axes=self.axes)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n\n@testing.parameterize(*testing.product({\n    \'n\': [None, 5, 10, 15],\n    \'shape\': [(9,), (10,), (10, 9), (10, 10)],\n    \'axis\': [-1, 0],\n    \'norm\': [None, \'ortho\']\n}))\n@testing.gpu\nclass TestRfft(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-6, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfft(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = _fft_module(xp).rfft(x, n=self.n, axis=self.axis, norm=self.norm)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-6, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfft_overwrite(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        overwrite_kw = {} if xp == np else {\'overwrite_x\': True}\n        out = _fft_module(xp).rfft(x, n=self.n, axis=self.axis,\n                                   norm=self.norm, **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-6, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfft_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        if xp is cp:\n            kw = {\'plan\': _fft_module(xp).get_fft_plan(\n                x, shape=self.n, axes=self.axis, value_type=\'R2C\')}\n        else:\n            kw = {}\n        out = _fft_module(xp).rfft(x, n=self.n, axis=self.axis, norm=self.norm,\n                                   **kw)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-6, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfft_overwrite_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        if xp is cp:\n            kw = {\'plan\': _fft_module(xp).get_fft_plan(\n                x, shape=self.n, axes=self.axis, value_type=\'R2C\'),\n                \'overwrite_x\': True}\n        else:\n            kw = {}\n        out = _fft_module(xp).rfft(x, n=self.n, axis=self.axis, norm=self.norm,\n                                   **kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-6, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfft_plan_manager(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        if xp is cp:\n            from cupy.cuda.cufft import get_current_plan\n            plan = _fft_module(xp).get_fft_plan(\n                x, shape=self.n, axes=self.axis, value_type=\'R2C\')\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = _fft_module(xp).rfft(x, n=self.n, axis=self.axis)\n            assert get_current_plan() is None\n        else:\n            out = _fft_module(xp).rfft(x, n=self.n, axis=self.axis)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-6, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfft(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = _fft_module(xp).irfft(x, n=self.n, axis=self.axis,\n                                    norm=self.norm)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-6, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfft_overwrite(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        overwrite_kw = {} if xp == np else {\'overwrite_x\': True}\n        out = _fft_module(xp).irfft(x, n=self.n, axis=self.axis,\n                                    norm=self.norm, **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-6, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfft_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        if xp is cp:\n            kw = {\'plan\': _fft_module(xp).get_fft_plan(\n                x, shape=self.n, axes=self.axis, value_type=\'C2R\')}\n        else:\n            kw = {}\n        out = _fft_module(xp).irfft(\n            x, n=self.n, axis=self.axis, norm=self.norm, **kw)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-6, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfft_overwrite_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        if xp is cp:\n            kw = {\'plan\': _fft_module(xp).get_fft_plan(\n                x, shape=self.n, axes=self.axis, value_type=\'C2R\'),\n                \'overwrite_x\': True}\n        else:\n            kw = {}\n        out = _fft_module(xp).irfft(\n            x, n=self.n, axis=self.axis, norm=self.norm, **kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-6, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfft_plan_manager(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        if xp is cp:\n            from cupy.cuda.cufft import get_current_plan\n            plan = _fft_module(xp).get_fft_plan(\n                x, shape=self.n, axes=self.axis, value_type=\'C2R\')\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = _fft_module(xp).irfft(x, n=self.n, axis=self.axis)\n            assert get_current_plan() is None\n        else:\n            out = _fft_module(xp).irfft(x, n=self.n, axis=self.axis)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n\n@testing.parameterize(\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': (1, None), \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': (1, 5), \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-2, -1), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-1, -2), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (0,), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, None), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, 10), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-3, -2, -1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-1, -2, -3), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (0, 1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': (2, 3), \'axes\': (0, 1, 2), \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4, 5), \'s\': None, \'axes\': None, \'norm\': None},\n)\n@testing.gpu\nclass TestRfft2(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfft2(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = _fft_module(xp).rfft2(x, s=self.s, axes=self.axes,\n                                    norm=self.norm)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfft2_overwrite(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        overwrite_kw = {} if xp == np else {\'overwrite_x\': True}\n        out = _fft_module(xp).rfft2(x, s=self.s, axes=self.axes,\n                                    norm=self.norm, **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfft2_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n\n        # hack: skip testing if getting a cuFFT plan is impossible\n        try:\n            plan = _fft_module(cp).get_fft_plan(\n                x, shape=self.s, axes=self.axes, value_type=\'R2C\')\n        except ValueError:\n            return x\n\n        if xp is cp:\n            kw = {\'plan\': plan}\n        else:\n            kw = {}\n        out = _fft_module(xp).rfft2(\n            x, s=self.s, axes=self.axes, norm=self.norm, **kw)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfft2_overwrite_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n\n        # hack: skip testing if getting a cuFFT plan is impossible\n        try:\n            plan = _fft_module(cp).get_fft_plan(\n                x, shape=self.s, axes=self.axes, value_type=\'R2C\')\n        except ValueError:\n            return x\n\n        if xp is cp:\n            kw = {\'plan\': plan, \'overwrite_x\': True}\n        else:\n            kw = {}\n        out = _fft_module(xp).rfft2(\n            x, s=self.s, axes=self.axes, norm=self.norm, **kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfft2_plan_manager(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n\n        # hack: skip testing if getting a cuFFT plan is impossible\n        try:\n            plan = _fft_module(cp).get_fft_plan(\n                x, shape=self.s, axes=self.axes, value_type=\'R2C\')\n        except ValueError:\n            return x\n\n        if xp is cp:\n            from cupy.cuda.cufft import get_current_plan\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = _fft_module(xp).rfft2(x, s=self.s, axes=self.axes)\n            assert get_current_plan() is None\n        else:\n            out = _fft_module(xp).rfft2(x, s=self.s, axes=self.axes)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @pytest.mark.skipif(int(cp.cuda.device.get_compute_capability()) < 70,\n                        reason=""Known to fail with Pascal or older"")\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfft2(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = _fft_module(xp).irfft2(x, s=self.s, axes=self.axes,\n                                     norm=self.norm)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @pytest.mark.skipif(int(cp.cuda.device.get_compute_capability()) < 70,\n                        reason=""Known to fail with Pascal or older"")\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfft2_overwrite(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        overwrite_kw = {} if xp == np else {\'overwrite_x\': True}\n        out = _fft_module(xp).irfft2(x, s=self.s, axes=self.axes,\n                                     norm=self.norm, **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @pytest.mark.skipif(int(cp.cuda.device.get_compute_capability()) < 70,\n                        reason=""Known to fail with Pascal or older"")\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfft2_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n\n        # hack: skip testing if getting a cuFFT plan is impossible\n        try:\n            plan = _fft_module(cp).get_fft_plan(\n                x, shape=self.s, axes=self.axes, value_type=\'C2R\')\n        except ValueError:\n            return x\n\n        if xp is cp:\n            kw = {\'plan\': plan}\n        else:\n            kw = {}\n        out = _fft_module(xp).irfft2(\n            x, s=self.s, axes=self.axes, norm=self.norm, **kw)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @pytest.mark.skipif(int(cp.cuda.device.get_compute_capability()) < 70,\n                        reason=""Known to fail with Pascal or older"")\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfft2_overwrite_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n\n        # hack: skip testing if getting a cuFFT plan is impossible\n        try:\n            plan = _fft_module(cp).get_fft_plan(\n                x, shape=self.s, axes=self.axes, value_type=\'C2R\')\n        except ValueError:\n            return x\n\n        if xp is cp:\n            kw = {\'plan\': plan, \'overwrite_x\': True}\n        else:\n            kw = {}\n        out = _fft_module(xp).irfft2(\n            x, s=self.s, axes=self.axes, norm=self.norm, **kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @pytest.mark.skipif(int(cp.cuda.device.get_compute_capability()) < 70,\n                        reason=""Known to fail with Pascal or older"")\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfft2_plan_manager(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n\n        # hack: skip testing if getting a cuFFT plan is impossible\n        try:\n            plan = _fft_module(cp).get_fft_plan(\n                x, shape=self.s, axes=self.axes, value_type=\'C2R\')\n        except ValueError:\n            return x\n\n        if xp is cp:\n            from cupy.cuda.cufft import get_current_plan\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = _fft_module(xp).irfft2(x, s=self.s, axes=self.axes)\n            assert get_current_plan() is None\n        else:\n            out = _fft_module(xp).irfft2(x, s=self.s, axes=self.axes)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n\n@testing.parameterize(\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': (1, None), \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': (1, 5), \'axes\': None, \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-2, -1), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (-1, -2), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': (0,), \'norm\': None},\n    {\'shape\': (3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, None), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': (1, 4, 10), \'axes\': None, \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-3, -2, -1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (-1, -2, -3), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': (0, 1), \'norm\': None},\n    {\'shape\': (2, 3, 4), \'s\': None, \'axes\': None, \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4), \'s\': (2, 3), \'axes\': (0, 1, 2), \'norm\': \'ortho\'},\n    {\'shape\': (2, 3, 4, 5), \'s\': None, \'axes\': None, \'norm\': None},\n)\n@testing.gpu\nclass TestRfftn(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfftn(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = _fft_module(xp).rfftn(x, s=self.s, axes=self.axes,\n                                    norm=self.norm)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfftn_overwrite(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        overwrite_kw = {} if xp == np else {\'overwrite_x\': True}\n        out = _fft_module(xp).rfftn(x, s=self.s, axes=self.axes,\n                                    norm=self.norm, **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfftn_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n\n        # hack: skip testing if getting a cuFFT plan is impossible\n        try:\n            plan = _fft_module(cp).get_fft_plan(\n                x, shape=self.s, axes=self.axes, value_type=\'R2C\')\n        except ValueError:\n            return x\n\n        if xp is cp:\n            kw = {\'plan\': plan}\n        else:\n            kw = {}\n        out = _fft_module(xp).rfftn(\n            x, s=self.s, axes=self.axes, norm=self.norm, **kw)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfftn_overwrite_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n\n        # hack: skip testing if getting a cuFFT plan is impossible\n        try:\n            plan = _fft_module(cp).get_fft_plan(\n                x, shape=self.s, axes=self.axes, value_type=\'R2C\')\n        except ValueError:\n            return x\n\n        if xp is cp:\n            kw = {\'plan\': plan, \'overwrite_x\': True}\n        else:\n            kw = {}\n        out = _fft_module(xp).rfftn(\n            x, s=self.s, axes=self.axes, norm=self.norm, **kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_rfftn_plan_manager(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n\n        # hack: skip testing if getting a cuFFT plan is impossible\n        try:\n            plan = _fft_module(cp).get_fft_plan(\n                x, shape=self.s, axes=self.axes, value_type=\'R2C\')\n        except ValueError:\n            return x\n\n        if xp is cp:\n            from cupy.cuda.cufft import get_current_plan\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = _fft_module(xp).rfftn(x, s=self.s, axes=self.axes)\n            assert get_current_plan() is None\n        else:\n            out = _fft_module(xp).rfftn(x, s=self.s, axes=self.axes)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @pytest.mark.skipif(int(cp.cuda.device.get_compute_capability()) < 70,\n                        reason=""Known to fail with Pascal or older"")\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfftn(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = _fft_module(xp).irfftn(x, s=self.s, axes=self.axes,\n                                     norm=self.norm)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @pytest.mark.skipif(int(cp.cuda.device.get_compute_capability()) < 70,\n                        reason=""Known to fail with Pascal or older"")\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfftn_overwrite(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        overwrite_kw = {} if xp == np else {\'overwrite_x\': True}\n        out = _fft_module(xp).irfftn(x, s=self.s, axes=self.axes,\n                                     norm=self.norm, **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @pytest.mark.skipif(int(cp.cuda.device.get_compute_capability()) < 70,\n                        reason=""Known to fail with Pascal or older"")\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfftn_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n\n        # hack: skip testing if getting a cuFFT plan is impossible\n        try:\n            plan = _fft_module(cp).get_fft_plan(\n                x, shape=self.s, axes=self.axes, value_type=\'C2R\')\n        except ValueError:\n            return x\n\n        if xp is cp:\n            kw = {\'plan\': plan}\n        else:\n            kw = {}\n        out = _fft_module(xp).irfftn(\n            x, s=self.s, axes=self.axes, norm=self.norm, **kw)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @pytest.mark.skipif(int(cp.cuda.device.get_compute_capability()) < 70,\n                        reason=""Known to fail with Pascal or older"")\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfftn_overwrite_plan(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n\n        # hack: skip testing if getting a cuFFT plan is impossible\n        try:\n            plan = _fft_module(cp).get_fft_plan(\n                x, shape=self.s, axes=self.axes, value_type=\'C2R\')\n        except ValueError:\n            return x\n\n        if xp is cp:\n            kw = {\'plan\': plan, \'overwrite_x\': True}\n        else:\n            kw = {}\n        out = _fft_module(xp).irfftn(\n            x, s=self.s, axes=self.axes, norm=self.norm, **kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @pytest.mark.skipif(int(cp.cuda.device.get_compute_capability()) < 70,\n                        reason=""Known to fail with Pascal or older"")\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_irfftn_plan_manager(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n\n        # hack: skip testing if getting a cuFFT plan is impossible\n        try:\n            plan = _fft_module(cp).get_fft_plan(\n                x, shape=self.s, axes=self.axes, value_type=\'C2R\')\n        except ValueError:\n            return x\n\n        if xp is cp:\n            from cupy.cuda.cufft import get_current_plan\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = _fft_module(xp).irfftn(x, s=self.s, axes=self.axes)\n            assert get_current_plan() is None\n        else:\n            out = _fft_module(xp).irfftn(x, s=self.s, axes=self.axes)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n\n@testing.parameterize(*testing.product({\n    \'n\': [None, 5, 10, 15],\n    \'shape\': [(10,), (10, 10)],\n    \'axis\': [0, -1],\n    \'norm\': [None, \'ortho\'],\n}))\n@testing.gpu\nclass TestHfft(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_hfft(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = _fft_module(xp).hfft(x, n=self.n, axis=self.axis, norm=self.norm)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_hfft_overwrite(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        overwrite_kw = {} if xp == np else {\'overwrite_x\': True}\n        out = _fft_module(xp).hfft(x, n=self.n, axis=self.axis, norm=self.norm,\n                                   **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    # TODO(leofang): rewrite this test when we support R2C/C2R cuFFT plans\n    @testing.for_all_dtypes()\n    def test_hfft_plan(self, dtype):\n        x = testing.shaped_random(self.shape, cp, dtype)\n        with pytest.raises(NotImplementedError, match=\'not yet supported\'):\n            _fft_module(cp).hfft(x, n=self.n, axis=self.axis,\n                                 norm=self.norm, plan=\'abc\')\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ihfft(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = _fft_module(xp).ihfft(x, n=self.n, norm=self.norm)\n        testing.assert_array_equal(x, x_orig)\n        return _correct_np_dtype(xp, dtype, out)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False)\n    def test_ihfft_overwrite(self, xp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        overwrite_kw = {} if xp == np else {\'overwrite_x\': True}\n        out = _fft_module(xp).ihfft(x, n=self.n, norm=self.norm,\n                                    **overwrite_kw)\n        return _correct_np_dtype(xp, dtype, out)\n\n    # TODO(leofang): rewrite this test when we support R2C/C2R cuFFT plans\n    @testing.for_all_dtypes(no_complex=True)\n    def test_ihfft_plan(self, dtype):\n        x = testing.shaped_random(self.shape, cp, dtype)\n        with pytest.raises(NotImplementedError, match=\'not yet supported\'):\n            _fft_module(cp).ihfft(x, n=self.n, axis=self.axis,\n                                  norm=self.norm, plan=\'abc\')\n\n\n@testing.gpu\n@pytest.mark.parametrize(\'func\', [\n    cp_fft.fft2, cp_fft.ifft2, cp_fft.rfft2, cp_fft.irfft2,\n    cp_fft.fftn, cp_fft.ifftn, cp_fft.rfftn, cp_fft.irfftn])\ndef test_scalar_shape_axes(func):\n    x = testing.shaped_random((10, 10), cp)\n    y_scalar = func(x, s=5, axes=-1)\n    y_normal = func(x, s=(5,), axes=(-1,))\n    testing.assert_allclose(y_scalar, y_normal)\n'"
tests/cupyx_tests/scipy_tests/fftpack_tests/__init__.py,0,b''
tests/cupyx_tests/scipy_tests/fftpack_tests/test_fftpack.py,0,"b""import unittest\nimport pytest\n\nimport cupy\nfrom cupy import testing\nimport cupyx.scipy.fftpack  # NOQA\nfrom cupy.fft.fft import _default_fft_func, _fftn\n\nif cupyx.scipy._scipy_available:\n    import scipy.fftpack  # NOQA\n\n\n@testing.parameterize(*testing.product({\n    'n': [None, 0, 5, 10, 15],\n    'shape': [(9,), (10,), (10, 9), (10, 10)],\n    'axis': [-1, 0],\n}))\n@testing.gpu\n@testing.with_requires('scipy>=0.19.0')\nclass TestFft(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_fft(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = scp.fftpack.fft(x, n=self.n, axis=self.axis)\n        testing.assert_array_equal(x, x_orig)\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_fft_overwrite(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        return scp.fftpack.fft(x, n=self.n, axis=self.axis,\n                               overwrite_x=True)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_fft_plan(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when the output array is of size 0\n        # because cuFFT and numpy raise different kinds of exceptions\n        if self.n == 0:\n            return x\n        x_orig = x.copy()\n        if scp is cupyx.scipy:\n            plan = scp.fftpack.get_fft_plan(x, shape=self.n, axes=self.axis)\n            out = scp.fftpack.fft(x, n=self.n, axis=self.axis, plan=plan)\n        else:  # scipy\n            out = scp.fftpack.fft(x, n=self.n, axis=self.axis)\n        testing.assert_array_equal(x, x_orig)\n        return out\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_fft_overwrite_plan(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when the output array is of size 0\n        # because cuFFT and numpy raise different kinds of exceptions\n        if self.n == 0:\n            return x\n        if scp is cupyx.scipy:\n            plan = scp.fftpack.get_fft_plan(x, shape=self.n, axes=self.axis)\n            x = scp.fftpack.fft(x, n=self.n, axis=self.axis,\n                                overwrite_x=True, plan=plan)\n        else:  # scipy\n            x = scp.fftpack.fft(x, n=self.n, axis=self.axis,\n                                overwrite_x=True)\n        return x\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_fft_plan_manager(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when the output array is of size 0\n        # because cuFFT and numpy raise different kinds of exceptions\n        if self.n == 0:\n            return x\n        x_orig = x.copy()\n        if scp is cupyx.scipy:\n            from cupy.cuda.cufft import get_current_plan\n            plan = scp.fftpack.get_fft_plan(x, shape=self.n, axes=self.axis)\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = scp.fftpack.fft(x, n=self.n, axis=self.axis)\n            assert get_current_plan() is None\n        else:  # scipy\n            out = scp.fftpack.fft(x, n=self.n, axis=self.axis)\n        testing.assert_array_equal(x, x_orig)\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_ifft(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = scp.fftpack.ifft(x, n=self.n, axis=self.axis)\n        testing.assert_array_equal(x, x_orig)\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_ifft_overwrite(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        return scp.fftpack.ifft(x, n=self.n, axis=self.axis,\n                                overwrite_x=True)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_ifft_plan(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when the output array is of size 0\n        # because cuFFT and numpy raise different kinds of exceptions\n        if self.n == 0:\n            return x\n        x_orig = x.copy()\n        if scp is cupyx.scipy:\n            plan = scp.fftpack.get_fft_plan(x, shape=self.n, axes=self.axis)\n            out = scp.fftpack.ifft(x, n=self.n, axis=self.axis, plan=plan)\n        else:  # scipy\n            out = scp.fftpack.ifft(x, n=self.n, axis=self.axis)\n        testing.assert_array_equal(x, x_orig)\n        return out\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_ifft_overwrite_plan(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when the output array is of size 0\n        # because cuFFT and numpy raise different kinds of exceptions\n        if self.n == 0:\n            return x\n        if scp is cupyx.scipy:\n            plan = scp.fftpack.get_fft_plan(x, shape=self.n, axes=self.axis)\n            x = scp.fftpack.ifft(x, n=self.n, axis=self.axis,\n                                 overwrite_x=True, plan=plan)\n        else:  # scipy\n            x = scp.fftpack.ifft(x, n=self.n, axis=self.axis,\n                                 overwrite_x=True)\n        return x\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_ifft_plan_manager(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when the output array is of size 0\n        # because cuFFT and numpy raise different kinds of exceptions\n        if self.n == 0:\n            return x\n        x_orig = x.copy()\n        if scp is cupyx.scipy:\n            from cupy.cuda.cufft import get_current_plan\n            plan = scp.fftpack.get_fft_plan(x, shape=self.n, axes=self.axis)\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = scp.fftpack.ifft(x, n=self.n, axis=self.axis)\n            assert get_current_plan() is None\n        else:  # scipy\n            out = scp.fftpack.ifft(x, n=self.n, axis=self.axis)\n        testing.assert_array_equal(x, x_orig)\n        return out\n\n    @testing.for_complex_dtypes()\n    def test_fft_multiple_plan_error(self, dtype):\n        # hack: avoid testing the cases when the output array is of size 0\n        # because cuFFT and numpy raise different kinds of exceptions\n        if self.n == 0:\n            return\n        import cupy\n        import cupyx.scipy.fftpack as fftpack\n        x = testing.shaped_random(self.shape, cupy, dtype)\n        plan = fftpack.get_fft_plan(x, shape=self.n, axes=self.axis)\n        with pytest.raises(RuntimeError) as ex, plan:\n            fftpack.fft(x, n=self.n, axis=self.axis, plan=plan)\n        assert 'Use the cuFFT plan either as' in str(ex.value)\n\n\n@testing.parameterize(\n    {'shape': (3, 4), 's': None, 'axes': None},\n    {'shape': (3, 4), 's': (1, 5), 'axes': None},\n    {'shape': (3, 4), 's': None, 'axes': (-2, -1)},\n    {'shape': (3, 4), 's': None, 'axes': (-1, -2)},\n    {'shape': (3, 4), 's': None, 'axes': (0,)},\n    {'shape': (2, 3, 4), 's': None, 'axes': None},\n    {'shape': (2, 3, 4), 's': (1, 4, 10), 'axes': None},\n    {'shape': (2, 3, 4), 's': None, 'axes': (-3, -2, -1)},\n    {'shape': (2, 3, 4), 's': None, 'axes': (-1, -2, -3)},\n    {'shape': (2, 3, 4), 's': None, 'axes': (0, 1)},\n    {'shape': (2, 3, 4, 5), 's': None, 'axes': None},\n)\n@testing.gpu\n@testing.with_requires('scipy>=0.19.0')\nclass TestFft2(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_fft2(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = scp.fftpack.fft2(x, shape=self.s, axes=self.axes)\n        testing.assert_array_equal(x, x_orig)\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_fft2_overwrite(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        return scp.fftpack.fft2(x, shape=self.s, axes=self.axes,\n                                overwrite_x=True)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_fft2_plan(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        if scp is cupyx.scipy:\n            import cupy.fft.config as config\n            config.enable_nd_planning = False  # use explicit plan\n            plan = scp.fftpack.get_fft_plan(x, shape=self.s, axes=self.axes)\n            out = scp.fftpack.fft2(x, shape=self.s, axes=self.axes, plan=plan)\n            config.enable_nd_planning = True  # default\n        else:  # scipy\n            out = scp.fftpack.fft2(x, shape=self.s, axes=self.axes)\n        return out\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_fft2_overwrite_plan(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        if scp is cupyx.scipy:\n            import cupy.fft.config as config\n            config.enable_nd_planning = False  # use explicit plan\n            plan = scp.fftpack.get_fft_plan(x, shape=self.s, axes=self.axes)\n            x = scp.fftpack.fft2(x, shape=self.s, axes=self.axes,\n                                 overwrite_x=True, plan=plan)\n            config.enable_nd_planning = True  # default\n        else:  # scipy\n            x = scp.fftpack.fft2(x, shape=self.s, axes=self.axes,\n                                 overwrite_x=True)\n        return x\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_fft2_plan_manager(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        if scp is cupyx.scipy:\n            from cupy.cuda.cufft import get_current_plan\n            plan = scp.fftpack.get_fft_plan(x, shape=self.s, axes=self.axes)\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = scp.fftpack.fft2(x, shape=self.s, axes=self.axes)\n            assert get_current_plan() is None\n        else:  # scipy\n            out = scp.fftpack.fft2(x, shape=self.s, axes=self.axes)\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_ifft2(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = scp.fftpack.ifft2(x, shape=self.s, axes=self.axes)\n        testing.assert_array_equal(x, x_orig)\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_ifft2_overwrite(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        return scp.fftpack.ifft2(x, shape=self.s, axes=self.axes,\n                                 overwrite_x=True)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_ifft2_plan(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        if scp is cupyx.scipy:\n            import cupy.fft.config as config\n            config.enable_nd_planning = False  # use explicit plan\n            plan = scp.fftpack.get_fft_plan(x, shape=self.s, axes=self.axes)\n            out = scp.fftpack.ifft2(x, shape=self.s, axes=self.axes, plan=plan)\n            config.enable_nd_planning = True  # default\n        else:  # scipy\n            out = scp.fftpack.ifft2(x, shape=self.s, axes=self.axes)\n        return out\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_ifft2_overwrite_plan(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        if scp is cupyx.scipy:\n            import cupy.fft.config as config\n            config.enable_nd_planning = False  # use explicit plan\n            plan = scp.fftpack.get_fft_plan(x, shape=self.s, axes=self.axes)\n            x = scp.fftpack.ifft2(x, shape=self.s, axes=self.axes,\n                                  overwrite_x=True, plan=plan)\n            config.enable_nd_planning = True  # default\n        else:  # scipy\n            x = scp.fftpack.ifft2(x, shape=self.s, axes=self.axes,\n                                  overwrite_x=True)\n        return x\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_ifft2_plan_manager(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        if scp is cupyx.scipy:\n            from cupy.cuda.cufft import get_current_plan\n            plan = scp.fftpack.get_fft_plan(x, shape=self.s, axes=self.axes)\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = scp.fftpack.ifft2(x, shape=self.s, axes=self.axes)\n            assert get_current_plan() is None\n        else:  # scipy\n            out = scp.fftpack.ifft2(x, shape=self.s, axes=self.axes)\n        return out\n\n\n@testing.parameterize(\n    {'shape': (3, 4), 's': None, 'axes': None},\n    {'shape': (3, 4), 's': (1, 5), 'axes': None},\n    {'shape': (3, 4), 's': None, 'axes': (-2, -1)},\n    {'shape': (3, 4), 's': None, 'axes': (-1, -2)},\n    {'shape': (3, 4), 's': None, 'axes': (0,)},\n    {'shape': (2, 3, 4), 's': None, 'axes': None},\n    {'shape': (2, 3, 4), 's': (1, 4, 10), 'axes': None},\n    {'shape': (2, 3, 4), 's': None, 'axes': (-3, -2, -1)},\n    {'shape': (2, 3, 4), 's': None, 'axes': (-1, -2, -3)},\n    {'shape': (2, 3, 4), 's': None, 'axes': (0, 1)},\n    {'shape': (2, 3, 4, 5), 's': None, 'axes': None},\n)\n@testing.gpu\n@testing.with_requires('scipy>=0.19.0')\nclass TestFftn(unittest.TestCase):\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_fftn(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = scp.fftpack.fftn(x, shape=self.s, axes=self.axes)\n        testing.assert_array_equal(x, x_orig)\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_fftn_overwrite(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        return scp.fftpack.fftn(x, shape=self.s, axes=self.axes,\n                                overwrite_x=True)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_fftn_plan(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        if scp is cupyx.scipy:\n            import cupy.fft.config as config\n            config.enable_nd_planning = False  # use explicit plan\n            plan = scp.fftpack.get_fft_plan(x, shape=self.s, axes=self.axes)\n            out = scp.fftpack.fftn(x, shape=self.s, axes=self.axes, plan=plan)\n            config.enable_nd_planning = True  # default\n        else:  # scipy\n            out = scp.fftpack.fftn(x, shape=self.s, axes=self.axes)\n        return out\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_fftn_overwrite_plan(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        if scp is cupyx.scipy:\n            import cupy.fft.config as config\n            config.enable_nd_planning = False  # use explicit plan\n            plan = scp.fftpack.get_fft_plan(x, shape=self.s, axes=self.axes)\n            x = scp.fftpack.fftn(x, shape=self.s, axes=self.axes,\n                                 overwrite_x=True, plan=plan)\n            config.enable_nd_planning = True  # default\n        else:  # scipy\n            x = scp.fftpack.fftn(x, shape=self.s, axes=self.axes,\n                                 overwrite_x=True)\n        return x\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_fftn_plan_manager(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        if scp is cupyx.scipy:\n            from cupy.cuda.cufft import get_current_plan\n            plan = scp.fftpack.get_fft_plan(x, shape=self.s, axes=self.axes)\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = scp.fftpack.fftn(x, shape=self.s, axes=self.axes)\n            assert get_current_plan() is None\n        else:  # scipy\n            out = scp.fftpack.fftn(x, shape=self.s, axes=self.axes)\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_ifftn(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = scp.fftpack.ifftn(x, shape=self.s, axes=self.axes)\n        testing.assert_array_equal(x, x_orig)\n        return out\n\n    @testing.for_all_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_ifftn_overwrite(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        return scp.fftpack.ifftn(x, shape=self.s, axes=self.axes,\n                                 overwrite_x=True)\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_ifftn_plan(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        if scp is cupyx.scipy:\n            import cupy.fft.config as config\n            config.enable_nd_planning = False  # use explicit plan\n            plan = scp.fftpack.get_fft_plan(x, shape=self.s, axes=self.axes)\n            out = scp.fftpack.ifftn(x, shape=self.s, axes=self.axes, plan=plan)\n            config.enable_nd_planning = True  # default\n        else:  # scipy\n            out = scp.fftpack.ifftn(x, shape=self.s, axes=self.axes)\n        return out\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_ifftn_overwrite_plan(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        if scp is cupyx.scipy:\n            import cupy.fft.config as config\n            config.enable_nd_planning = False  # use explicit plan\n            plan = scp.fftpack.get_fft_plan(x, shape=self.s, axes=self.axes)\n            x = scp.fftpack.ifftn(x, shape=self.s, axes=self.axes,\n                                  overwrite_x=True, plan=plan)\n            config.enable_nd_planning = True  # default\n        else:  # scipy\n            x = scp.fftpack.ifftn(x, shape=self.s, axes=self.axes,\n                                  overwrite_x=True)\n        return x\n\n    @testing.for_complex_dtypes()\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_ifftn_plan_manager(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return x\n        if scp is cupyx.scipy:\n            from cupy.cuda.cufft import get_current_plan\n            plan = scp.fftpack.get_fft_plan(x, shape=self.s, axes=self.axes)\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = scp.fftpack.ifftn(x, shape=self.s, axes=self.axes)\n            assert get_current_plan() is None\n        else:  # scipy\n            out = scp.fftpack.ifftn(x, shape=self.s, axes=self.axes)\n        return out\n\n    @testing.for_complex_dtypes()\n    def test_fftn_multiple_plan_error(self, dtype):\n        import cupy\n        import cupyx.scipy.fftpack as fftpack\n        x = testing.shaped_random(self.shape, cupy, dtype)\n        # hack: avoid testing the cases when getting a cuFFT plan is impossible\n        if _default_fft_func(x, s=self.s, axes=self.axes) is not _fftn:\n            return\n        plan = fftpack.get_fft_plan(x, shape=self.s, axes=self.axes)\n        with pytest.raises(RuntimeError) as ex, plan:\n            fftpack.fftn(x, shape=self.s, axes=self.axes, plan=plan)\n        assert 'Use the cuFFT plan either as' in str(ex.value)\n\n\n@testing.parameterize(*testing.product({\n    'n': [None, 5, 10, 15],\n    'shape': [(9,), (10,), (10, 9), (10, 10)],\n    'axis': [-1, 0],\n}))\n@testing.gpu\n@testing.with_requires('scipy>=0.19.0')\nclass TestRfft(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-6, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_rfft(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = scp.fftpack.rfft(x, n=self.n, axis=self.axis)\n        testing.assert_array_equal(x, x_orig)\n        return out\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-6, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_rfft_overwrite(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        return scp.fftpack.rfft(x, n=self.n, axis=self.axis,\n                                overwrite_x=True)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-6, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_rfft_plan(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        if scp is cupyx.scipy:\n            plan = scp.fftpack.get_fft_plan(x, shape=self.n, axes=self.axis,\n                                            value_type='R2C')\n            out = scp.fftpack.rfft(x, n=self.n, axis=self.axis, plan=plan)\n        else:  # scipy\n            out = scp.fftpack.rfft(x, n=self.n, axis=self.axis)\n        testing.assert_array_equal(x, x_orig)\n        return out\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-6, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_rfft_overwrite_plan(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        if scp is cupyx.scipy:\n            plan = scp.fftpack.get_fft_plan(x, shape=self.n, axes=self.axis,\n                                            value_type='R2C')\n            x = scp.fftpack.rfft(x, n=self.n, axis=self.axis,\n                                 overwrite_x=True, plan=plan)\n        else:  # scipy\n            x = scp.fftpack.rfft(x, n=self.n, axis=self.axis,\n                                 overwrite_x=True)\n        return x\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-6, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_rfft_plan_manager(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        if scp is cupyx.scipy:\n            from cupy.cuda.cufft import get_current_plan\n            plan = scp.fftpack.get_fft_plan(x, shape=self.n, axes=self.axis,\n                                            value_type='R2C')\n            with plan:\n                assert id(plan) == id(get_current_plan())\n                out = scp.fftpack.rfft(x, n=self.n, axis=self.axis)\n            assert get_current_plan() is None\n        else:  # scipy\n            out = scp.fftpack.rfft(x, n=self.n, axis=self.axis)\n        testing.assert_array_equal(x, x_orig)\n        return out\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_irfft(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        x_orig = x.copy()\n        out = scp.fftpack.irfft(x, n=self.n, axis=self.axis)\n        testing.assert_array_equal(x, x_orig)\n        return out\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(rtol=1e-4, atol=1e-7, accept_error=ValueError,\n                                 contiguous_check=False, scipy_name='scp')\n    def test_irfft_overwrite(self, xp, scp, dtype):\n        x = testing.shaped_random(self.shape, xp, dtype)\n        return scp.fftpack.irfft(x, n=self.n, axis=self.axis,\n                                 overwrite_x=True)\n\n\n@testing.parameterize(\n    {'shape': (32, 16, 4), 'data_order': 'F'},\n    {'shape': (4, 32, 16), 'data_order': 'C'},\n)\nclass TestFftnView(unittest.TestCase):\n\n    @testing.for_complex_dtypes()\n    def test_contiguous_view(self, dtype):\n        # Fortran-ordered case tests: https://github.com/cupy/cupy/issues/3079\n        a = testing.shaped_random(self.shape, cupy, dtype)\n        if self.data_order == 'F':\n            a = cupy.asfortranarray(a)\n            sl = [Ellipsis, 0]\n        else:\n            sl = [0, Ellipsis]\n\n        # transform a contiguous view without pre-planning\n        view = a[sl]\n        expected = cupyx.scipy.fftpack.fftn(view)\n\n        # create plan and then apply it to a contiguous view\n        plan = cupyx.scipy.fftpack.get_fft_plan(view)\n        with plan:\n            out = cupyx.scipy.fftpack.fftn(view)\n        testing.assert_allclose(expected, out)\n\n    @testing.for_complex_dtypes()\n    def test_noncontiguous_view(self, dtype):\n        a = testing.shaped_random(self.shape, cupy, dtype)\n        if self.data_order == 'F':\n            a = cupy.asfortranarray(a)\n            sl = [Ellipsis, slice(None, None, 2)]\n        else:\n            sl = [slice(None, None, 2), Ellipsis]\n\n        # transform a non-contiguous view without pre-planning\n        view = a[sl]\n        expected = cupyx.scipy.fftpack.fftn(view)\n\n        # create plan and then apply it to a non-contiguous view\n        plan = cupyx.scipy.fftpack.get_fft_plan(view.copy())\n        with plan:\n            out = cupyx.scipy.fftpack.fftn(view)\n        testing.assert_allclose(expected, out)\n\n    @testing.for_complex_dtypes()\n    def test_overwrite_x_with_contiguous_view(self, dtype):\n        # Test case for: https://github.com/cupy/cupy/issues/3079\n        a = testing.shaped_random(self.shape, cupy, dtype)\n        if self.data_order == 'C':\n            # C-contiguous view\n            b = a[:a.shape[0] // 2, ...]\n        else:\n            # F-contiguous view\n            a = cupy.asfortranarray(a)\n            b = a[..., :a.shape[-1] // 2]\n        b_ptr = b.data.ptr\n        out = cupyx.scipy.fftpack.fftn(b, overwrite_x=True)\n        assert out.data.ptr == b_ptr\n"""
tests/cupyx_tests/scipy_tests/linalg_tests/__init__.py,0,b''
tests/cupyx_tests/scipy_tests/linalg_tests/test_decomp_lu.py,0,"b""import unittest\n\nimport numpy\nimport cupy\nfrom cupy import testing\nimport cupyx.scipy.linalg\nif cupyx.scipy._scipy_available:\n    import scipy.linalg\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'shape': [(1, 1), (2, 2), (3, 3), (5, 5), (1, 5), (5, 1), (2, 5), (5, 2)],\n}))\n@testing.fix_random()\n@testing.with_requires('scipy')\nclass TestLUFactor(unittest.TestCase):\n\n    @testing.for_float_dtypes(no_float16=True)\n    def test_lu_factor(self, dtype):\n        if self.shape[0] != self.shape[1]:\n            # skip non-square tests since scipy.lu_factor requires square\n            return unittest.SkipTest()\n        array = numpy.random.randn(*self.shape)\n        a_cpu = numpy.asarray(array, dtype=dtype)\n        a_gpu = cupy.asarray(array, dtype=dtype)\n        result_cpu = scipy.linalg.lu_factor(a_cpu)\n        result_gpu = cupyx.scipy.linalg.lu_factor(a_gpu)\n        self.assertEqual(len(result_cpu), len(result_gpu))\n        self.assertEqual(result_cpu[0].dtype, result_gpu[0].dtype)\n        self.assertEqual(result_cpu[1].dtype, result_gpu[1].dtype)\n        cupy.testing.assert_allclose(result_cpu[0], result_gpu[0], atol=1e-5)\n        cupy.testing.assert_array_equal(result_cpu[1], result_gpu[1])\n\n    @testing.for_float_dtypes(no_float16=True)\n    def test_lu_factor_reconstruction(self, dtype):\n        m, n = self.shape\n        A = cupy.random.randn(m, n, dtype=dtype)\n        lu, piv = cupyx.scipy.linalg.lu_factor(A)\n        # extract ``L`` and ``U`` from ``lu``\n        L = cupy.tril(lu, k=-1)\n        cupy.fill_diagonal(L, 1.)\n        L = L[:, :m]\n        U = cupy.triu(lu)\n        U = U[:n, :]\n        # check output shapes\n        assert lu.shape == (m, n)\n        assert L.shape == (m, min(m, n))\n        assert U.shape == (min(m, n), n)\n        assert piv.shape == (min(m, n),)\n        # apply pivot (on CPU since slaswp is not available in cupy)\n        piv = cupy.asnumpy(piv)\n        rows = numpy.arange(m)\n        for i, row in enumerate(piv):\n            if i != row:\n                rows[i], rows[row] = rows[row], rows[i]\n        PA = A[rows]\n        # check that reconstruction is close to original\n        LU = L.dot(U)\n        cupy.testing.assert_allclose(LU, PA, atol=1e-5)\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'trans': [0, 1, 2],\n    'shapes': [((4, 4), (4,)), ((5, 5), (5, 2))],\n}))\n@testing.fix_random()\n@testing.with_requires('scipy')\nclass TestLUSolve(unittest.TestCase):\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_lu_solve(self, xp, scp, dtype):\n        a_shape, b_shape = self.shapes\n        A = testing.shaped_random(a_shape, xp, dtype=dtype)\n        b = testing.shaped_random(b_shape, xp, dtype=dtype)\n        lu = scp.linalg.lu_factor(A)\n        return scp.linalg.lu_solve(lu, b, trans=self.trans)\n"""
tests/cupyx_tests/scipy_tests/linalg_tests/test_solve_triangular.py,0,"b""import unittest\n\nimport cupy\nfrom cupy import testing\nimport cupyx.scipy.linalg\n\nimport numpy\nimport pytest\n\ntry:\n    import scipy.linalg\n    _scipy_available = True\nexcept ImportError:\n    _scipy_available = False\n\n\n@testing.parameterize(*testing.product({\n    'trans': [0, 1, 2, 'N', 'T', 'C'],\n    'lower': [True, False],\n    'unit_diagonal': [True, False],\n    'overwrite_b': [True, False],\n    'check_finite': [True, False],\n}))\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestSolveTriangular(unittest.TestCase):\n\n    @testing.for_float_dtypes(no_float16=True)\n    def check_x(self, a_shape, b_shape, dtype):\n        a_cpu = numpy.random.randint(1, 10, size=a_shape).astype(dtype)\n        b_cpu = numpy.random.randint(1, 10, size=b_shape).astype(dtype)\n        a_cpu = numpy.tril(a_cpu)\n\n        if self.lower is False:\n            a_cpu = a_cpu.T\n        if self.unit_diagonal is True:\n            numpy.fill_diagonal(a_cpu, 1)\n\n        a_gpu = cupy.asarray(a_cpu)\n        b_gpu = cupy.asarray(b_cpu)\n        a_gpu_copy = a_gpu.copy()\n        b_gpu_copy = b_gpu.copy()\n        result_cpu = scipy.linalg.solve_triangular(\n            a_cpu, b_cpu, trans=self.trans, lower=self.lower,\n            unit_diagonal=self.unit_diagonal, overwrite_b=self.overwrite_b,\n            check_finite=self.check_finite)\n        result_gpu = cupyx.scipy.linalg.solve_triangular(\n            a_gpu, b_gpu, trans=self.trans, lower=self.lower,\n            unit_diagonal=self.unit_diagonal, overwrite_b=self.overwrite_b,\n            check_finite=self.check_finite)\n        self.assertEqual(result_cpu.dtype, result_gpu.dtype)\n        cupy.testing.assert_allclose(result_cpu, result_gpu, atol=1e-3)\n        cupy.testing.assert_array_equal(a_gpu_copy, a_gpu)\n        if not self.overwrite_b:\n            cupy.testing.assert_array_equal(b_gpu_copy, b_gpu)\n\n    def test_solve(self):\n        self.check_x((4, 4), (4,))\n        self.check_x((5, 5), (5, 2))\n        self.check_x((5, 5), (5, 5))\n\n    def check_shape(self, a_shape, b_shape):\n        for xp, sp in ((numpy, scipy), (cupy, cupyx.scipy)):\n            a = xp.random.rand(*a_shape)\n            b = xp.random.rand(*b_shape)\n            with pytest.raises(ValueError):\n                sp.linalg.solve_triangular(\n                    a, b, trans=self.trans, lower=self.lower,\n                    unit_diagonal=self.unit_diagonal,\n                    overwrite_b=self.overwrite_b,\n                    check_finite=self.check_finite)\n\n    def test_invalid_shape(self):\n        self.check_shape((2, 3), (4,))\n        self.check_shape((3, 3), (2,))\n        self.check_shape((3, 3), (2, 2))\n        self.check_shape((3, 3, 4), (3,))\n\n    def check_infinite(self, a_shape, b_shape):\n        for xp, sp in ((numpy, scipy), (cupy, cupyx.scipy)):\n            a = xp.random.rand(*a_shape)\n            b = xp.random.rand(*b_shape)\n            a[(0,) * a.ndim] = numpy.inf\n            b[(0,) * b.ndim] = numpy.inf\n            with pytest.raises(ValueError):\n                sp.linalg.solve_triangular(\n                    a, b, trans=self.trans, lower=self.lower,\n                    unit_diagonal=self.unit_diagonal,\n                    overwrite_b=self.overwrite_b,\n                    check_finite=self.check_finite)\n\n    def test_infinite(self):\n        if self.check_finite:\n            self.check_infinite((4, 4), (4,))\n            self.check_infinite((5, 5), (5, 2))\n            self.check_infinite((5, 5), (5, 5))\n"""
tests/cupyx_tests/scipy_tests/ndimage_tests/__init__.py,0,b''
tests/cupyx_tests/scipy_tests/ndimage_tests/test_filters.py,0,"b""import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\nimport cupyx.scipy.ndimage  # NOQA\n\ntry:\n    import scipy.ndimage  # NOQA\nexcept ImportError:\n    pass\n\n\n@testing.parameterize(*(\n    testing.product({\n        'shape': [(3, 4), (2, 3, 4), (1, 2, 3, 4)],\n        'ksize': [3, 4],\n        'mode': ['reflect'],\n        'cval': [0.0],\n        'origin': [0, 1, None],\n        'adtype': [numpy.int8, numpy.int16, numpy.int32,\n                   numpy.float32, numpy.float64],\n        'wdtype': [None, numpy.int32, numpy.float64],\n        'output': [None, numpy.int32, numpy.float64],\n        'filter': ['convolve', 'correlate']\n    }) + testing.product({\n        'shape': [(3, 4), (2, 3, 4), (1, 2, 3, 4)],\n        'ksize': [3, 4],\n        'mode': ['constant'],\n        'cval': [-1.0, 0.0, 1.0],\n        'origin': [0],\n        'adtype': [numpy.int32, numpy.float64],\n        'wdtype': [None],\n        'output': [None],\n        'filter': ['convolve', 'correlate']\n    }) + testing.product({\n        'shape': [(3, 4), (2, 3, 4), (1, 2, 3, 4)],\n        'ksize': [3, 4],\n        'mode': ['nearest', 'mirror', 'wrap'],\n        'cval': [0.0],\n        'origin': [0],\n        'adtype': [numpy.int32, numpy.float64],\n        'wdtype': [None],\n        'output': [None],\n        'filter': ['convolve', 'correlate']\n    })\n))\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestConvolveAndCorrelate(unittest.TestCase):\n\n    def _filter(self, xp, scp, a, w):\n        filter = getattr(scp.ndimage, self.filter)\n        if self.origin is None:\n            origin = (-1, 1, -1, 1)[:a.ndim]\n        else:\n            origin = self.origin\n        return filter(a, w, output=self.output, mode=self.mode,\n                      cval=self.cval, origin=origin)\n\n    @testing.numpy_cupy_allclose(atol=1e-5, rtol=1e-5, scipy_name='scp')\n    def test_convolve_and_correlate(self, xp, scp):\n        if self.adtype == self.wdtype or self.adtype == self.output:\n            return xp.array(True)\n        a = testing.shaped_random(self.shape, xp, self.adtype)\n        if self.wdtype is None:\n            wdtype = self.adtype\n        else:\n            wdtype = self.wdtype\n        w = testing.shaped_random((self.ksize,) * a.ndim, xp, wdtype)\n        return self._filter(xp, scp, a, w)\n\n\n@testing.parameterize(*testing.product({\n    'ndim': [2, 3],\n    'dtype': [numpy.int32, numpy.float64],\n    'filter': ['convolve', 'correlate']\n}))\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestConvolveAndCorrelateSpecialCases(unittest.TestCase):\n\n    def _filter(self, scp, a, w, mode='reflect', origin=0):\n        filter = getattr(scp.ndimage, self.filter)\n        return filter(a, w, mode=mode, origin=origin)\n\n    @testing.numpy_cupy_allclose(atol=1e-5, rtol=1e-5, scipy_name='scp')\n    def test_weights_with_size_zero_dim(self, xp, scp):\n        a = testing.shaped_random((3, ) * self.ndim, xp, self.dtype)\n        w = testing.shaped_random((0, ) + (3, ) * self.ndim, xp, self.dtype)\n        return self._filter(scp, a, w)\n\n    def test_invalid_shape_weights(self):\n        a = testing.shaped_random((3, ) * self.ndim, cupy, self.dtype)\n        w = testing.shaped_random((3, ) * (self.ndim - 1), cupy, self.dtype)\n        with self.assertRaises(RuntimeError):\n            self._filter(cupyx.scipy, a, w)\n        w = testing.shaped_random((0, ) + (3, ) * (self.ndim - 1), cupy,\n                                  self.dtype)\n        with self.assertRaises(RuntimeError):\n            self._filter(cupyx.scipy, a, w)\n\n    def test_invalid_mode(self):\n        a = testing.shaped_random((3, ) * self.ndim, cupy, self.dtype)\n        w = testing.shaped_random((3, ) * self.ndim, cupy, self.dtype)\n        with self.assertRaises(RuntimeError):\n            self._filter(cupyx.scipy, a, w, mode='unknown')\n\n    # SciPy behavior fixed in 1.2.0: https://github.com/scipy/scipy/issues/822\n    @testing.with_requires('scipy>=1.2.0')\n    def test_invalid_origin(self):\n        a = testing.shaped_random((3, ) * self.ndim, cupy, self.dtype)\n        for lenw in [3, 4]:\n            w = testing.shaped_random((lenw, ) * self.ndim, cupy, self.dtype)\n            for origin in range(-3, 4):\n                if (lenw // 2 + origin < 0) or (lenw // 2 + origin >= lenw):\n                    with self.assertRaises(ValueError):\n                        self._filter(cupyx.scipy, a, w, origin=origin)\n                else:\n                    self._filter(cupyx.scipy, a, w, origin=origin)\n\n\n@testing.parameterize(*testing.product({\n    'size': [3, 4],\n    'footprint': [None, 'random'],\n    'mode': ['reflect', 'constant', 'nearest', 'mirror', 'wrap'],\n    'origin': [0, None],\n    'x_dtype': [numpy.int32, numpy.float32],\n    'output': [None, numpy.float64],\n    'filter': ['minimum_filter', 'maximum_filter']\n}))\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestMinimumMaximumFilter(unittest.TestCase):\n\n    shape = (4, 5)\n    cval = 0.0\n\n    def _filter(self, xp, scp, x):\n        filter = getattr(scp.ndimage, self.filter)\n        if self.origin is None:\n            origin = (-1, 1, -1, 1)[:x.ndim]\n        else:\n            origin = self.origin\n        if self.footprint is None:\n            footprint = None\n        else:\n            shape = (self.size, ) * x.ndim\n            r = testing.shaped_random(shape, xp, scale=1)\n            footprint = xp.where(r < .5, 1, 0)\n            if not footprint.any():\n                footprint = xp.ones(shape)\n        return filter(x, size=self.size, footprint=footprint,\n                      output=self.output, mode=self.mode, cval=self.cval,\n                      origin=origin)\n\n    @testing.numpy_cupy_allclose(atol=1e-5, rtol=1e-5, scipy_name='scp')\n    def test_minimum_and_maximum_filter(self, xp, scp):\n        x = testing.shaped_random(self.shape, xp, self.x_dtype)\n        return self._filter(xp, scp, x)\n"""
tests/cupyx_tests/scipy_tests/ndimage_tests/test_interpolation.py,0,"b""import unittest\n\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\nimport cupyx.scipy.ndimage\n\ntry:\n    import scipy.ndimage\nexcept ImportError:\n    pass\n\ntry:\n    import cv2\nexcept ImportError:\n    pass\n\n\n@testing.parameterize(*testing.product({\n    'output': [None, numpy.float64, 'f', float, 'empty'],\n    'order': [0, 1],\n    'mode': ['constant', 'nearest', 'mirror'],\n    'cval': [1.0],\n    'prefilter': [True],\n}))\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestMapCoordinates(unittest.TestCase):\n\n    _multiprocess_can_split = True\n\n    def _map_coordinates(self, xp, scp, a, coordinates):\n        map_coordinates = scp.ndimage.map_coordinates\n        if self.output == 'empty':\n            output = xp.empty(coordinates.shape[1:], dtype=a.dtype)\n            return_value = map_coordinates(a, coordinates, output, self.order,\n                                           self.mode, self.cval,\n                                           self.prefilter)\n            self.assertTrue(return_value is None or return_value is output)\n            return output\n        else:\n            return map_coordinates(a, coordinates, self.output, self.order,\n                                   self.mode, self.cval, self.prefilter)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_map_coordinates_float(self, xp, scp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        coordinates = testing.shaped_random((a.ndim, 100), xp, dtype)\n        return self._map_coordinates(xp, scp, a, coordinates)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_map_coordinates_fortran_order(self, xp, scp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        coordinates = testing.shaped_random((a.ndim, 100), xp, dtype)\n        a = xp.asfortranarray(a)\n        coordinates = xp.asfortranarray(coordinates)\n        return self._map_coordinates(xp, scp, a, coordinates)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_map_coordinates_float_nd_coords(self, xp, scp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        coordinates = testing.shaped_random((a.ndim, 10, 10), xp, dtype,\n                                            scale=99.0)\n        return self._map_coordinates(xp, scp, a, coordinates)\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_map_coordinates_int(self, xp, scp, dtype):\n        if numpy.lib.NumpyVersion(scipy.__version__) < '1.0.0':\n            if dtype in (numpy.dtype('l'), numpy.dtype('q')):\n                dtype = numpy.int64\n            elif dtype in (numpy.dtype('L'), numpy.dtype('Q')):\n                dtype = numpy.uint64\n\n        a = testing.shaped_random((100, 100), xp, dtype)\n        coordinates = testing.shaped_random((a.ndim, 100), xp, dtype)\n        out = self._map_coordinates(xp, scp, a, coordinates)\n        float_out = self._map_coordinates(xp, scp, a.astype(xp.float64),\n                                          coordinates) % 1\n        half = xp.full_like(float_out, 0.5)\n        out[xp.isclose(float_out, half, atol=1e-5)] = 0\n        return out\n\n\n@testing.parameterize(*testing.product({\n    'matrix_shape': [(2,), (2, 2), (2, 3), (3, 3)],\n    'offset': [0.3, [-1.3, 1.3]],\n    'output_shape': [None],\n    'output': [None, numpy.float64, 'empty'],\n    'order': [0, 1],\n    'mode': ['constant', 'nearest', 'mirror'],\n    'cval': [1.0],\n    'prefilter': [True],\n}))\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestAffineTransform(unittest.TestCase):\n\n    _multiprocess_can_split = True\n\n    def _affine_transform(self, xp, scp, a, matrix):\n        ver = numpy.lib.NumpyVersion(scipy.__version__)\n        if ver < '1.0.0' and matrix.ndim == 2 and matrix.shape[1] == 3:\n            return xp.empty(0)\n\n        if matrix.shape == (3, 3):\n            matrix[-1, 0:-1] = 0\n            matrix[-1, -1] = 1\n        affine_transform = scp.ndimage.affine_transform\n        if self.output == 'empty':\n            output = xp.empty_like(a)\n            return_value = affine_transform(a, matrix, self.offset,\n                                            self.output_shape, output,\n                                            self.order, self.mode, self.cval,\n                                            self.prefilter)\n            self.assertTrue(return_value is None or return_value is output)\n            return output\n        else:\n            return affine_transform(a, matrix, self.offset, self.output_shape,\n                                    self.output, self.order, self.mode,\n                                    self.cval, self.prefilter)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_affine_transform_float(self, xp, scp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        matrix = testing.shaped_random(self.matrix_shape, xp, dtype)\n        return self._affine_transform(xp, scp, a, matrix)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_affine_transform_fortran_order(self, xp, scp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        a = xp.asfortranarray(a)\n        matrix = testing.shaped_random(self.matrix_shape, xp, dtype)\n        matrix = xp.asfortranarray(matrix)\n        return self._affine_transform(xp, scp, a, matrix)\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_affine_transform_int(self, xp, scp, dtype):\n        if numpy.lib.NumpyVersion(scipy.__version__) < '1.0.0':\n            if dtype in (numpy.dtype('l'), numpy.dtype('q')):\n                dtype = numpy.int64\n            elif dtype in (numpy.dtype('L'), numpy.dtype('Q')):\n                dtype = numpy.uint64\n\n        a = testing.shaped_random((100, 100), xp, dtype)\n        matrix = testing.shaped_random(self.matrix_shape, xp, dtype)\n        out = self._affine_transform(xp, scp, a, matrix)\n        float_out = self._affine_transform(xp, scp, a.astype(xp.float64),\n                                           matrix) % 1\n        half = xp.full_like(float_out, 0.5)\n        out[xp.isclose(float_out, half, atol=1e-5)] = 0\n        return out\n\n\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestAffineExceptions(unittest.TestCase):\n\n    def test_invalid_affine_ndim(self):\n        ndimage_modules = (scipy.ndimage, cupyx.scipy.ndimage)\n        for (xp, ndi) in zip((numpy, cupy), ndimage_modules):\n            x = xp.ones((8, 8, 8))\n            with pytest.raises(RuntimeError):\n                ndi.affine_transform(x, xp.ones((3, 3, 3)))\n            with pytest.raises(RuntimeError):\n                ndi.affine_transform(x, xp.ones(()))\n\n    def test_invalid_affine_shape(self):\n        ndimage_modules = (scipy.ndimage, cupyx.scipy.ndimage)\n        for (xp, ndi) in zip((numpy, cupy), ndimage_modules):\n            x = xp.ones((8, 8, 8))\n            with pytest.raises(RuntimeError):\n                ndi.affine_transform(x, xp.ones((0, 3)))\n            with pytest.raises(RuntimeError):\n                ndi.affine_transform(x, xp.eye(x.ndim - 1))\n            with pytest.raises(RuntimeError):\n                ndi.affine_transform(x, xp.eye(x.ndim + 2))\n            with pytest.raises(RuntimeError):\n                ndi.affine_transform(x, xp.eye(x.ndim)[:, :-1])\n\n\n@testing.gpu\n@testing.with_requires('opencv-python')\nclass TestAffineTransformOpenCV(unittest.TestCase):\n\n    _multiprocess_can_split = True\n\n    @testing.for_float_dtypes(no_float16=True)\n    # The precision of cv2.warpAffine is not good because it uses fixed-point\n    # arithmetic.\n    @testing.numpy_cupy_allclose(atol=0.2)\n    def test_affine_transform_opencv(self, xp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        matrix = testing.shaped_random((2, 3), xp, dtype)\n        if xp == cupy:\n            return cupyx.scipy.ndimage.affine_transform(a, matrix, order=1,\n                                                        mode='opencv')\n        else:\n            return cv2.warpAffine(a, matrix, (a.shape[1], a.shape[0]))\n\n\n@testing.parameterize(*testing.product({\n    'angle': [-10, 1000],\n    'axes': [(1, 0)],\n    'reshape': [False, True],\n    'output': [None, numpy.float64, 'empty'],\n    'order': [0, 1],\n    'mode': ['constant', 'nearest', 'mirror'],\n    'cval': [1.0],\n    'prefilter': [True],\n}))\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestRotate(unittest.TestCase):\n\n    _multiprocess_can_split = True\n\n    def _rotate(self, xp, scp, a):\n        rotate = scp.ndimage.rotate\n        if self.output == 'empty':\n            output = rotate(a, self.angle, self.axes,\n                            self.reshape, None, self.order,\n                            self.mode, self.cval, self.prefilter)\n            return_value = rotate(a, self.angle, self.axes,\n                                  self.reshape, output, self.order,\n                                  self.mode, self.cval, self.prefilter)\n            self.assertTrue(return_value is None or return_value is output)\n            return output\n        else:\n            return rotate(a, self.angle, self.axes,\n                          self.reshape, self.output, self.order,\n                          self.mode, self.cval, self.prefilter)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_rotate_float(self, xp, scp, dtype):\n        a = testing.shaped_random((10, 10), xp, dtype)\n        return self._rotate(xp, scp, a)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_rotate_fortran_order(self, xp, scp, dtype):\n        a = testing.shaped_random((10, 10), xp, dtype)\n        a = xp.asfortranarray(a)\n        return self._rotate(xp, scp, a)\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_rotate_int(self, xp, scp, dtype):\n        if numpy.lib.NumpyVersion(scipy.__version__) < '1.0.0':\n            if dtype in (numpy.dtype('l'), numpy.dtype('q')):\n                dtype = numpy.int64\n            elif dtype in (numpy.dtype('L'), numpy.dtype('Q')):\n                dtype = numpy.uint64\n\n        a = testing.shaped_random((10, 10), xp, dtype)\n        out = self._rotate(xp, scp, a)\n        float_out = self._rotate(xp, scp, a.astype(xp.float64)) % 1\n        half = xp.full_like(float_out, 0.5)\n        out[xp.isclose(float_out, half, atol=1e-5)] = 0\n        return out\n\n\n@testing.gpu\n# Scipy older than 1.3.0 raises IndexError instead of ValueError\n@testing.with_requires('scipy>=1.3.0')\nclass TestRotateExceptions(unittest.TestCase):\n\n    def test_rotate_invalid_plane(self):\n        ndimage_modules = (scipy.ndimage, cupyx.scipy.ndimage)\n        for (xp, ndi) in zip((numpy, cupy), ndimage_modules):\n            x = xp.ones((8, 8, 8))\n            angle = 15\n            with pytest.raises(ValueError):\n                ndi.rotate(x, angle, [0, x.ndim])\n            with pytest.raises(ValueError):\n                ndi.rotate(x, angle, [-(x.ndim + 1), 1])\n\n\n@testing.parameterize(\n    {'axes': (-1, -2)},\n    {'axes': (0, 1)},\n    {'axes': (2, 0)},\n    {'axes': (-2, 2)},\n)\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestRotateAxes(unittest.TestCase):\n\n    _multiprocess_can_split = True\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_rotate_axes(self, xp, scp, dtype):\n        a = testing.shaped_random((10, 10, 10), xp, dtype)\n        rotate = scp.ndimage.rotate\n        return rotate(a, 1, self.axes, order=1)\n\n\n@testing.gpu\n@testing.with_requires('opencv-python')\nclass TestRotateOpenCV(unittest.TestCase):\n\n    _multiprocess_can_split = True\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(atol=0.3)\n    def test_rotate_opencv(self, xp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        if xp == cupy:\n            return cupyx.scipy.ndimage.rotate(a, 10, reshape=False,\n                                              order=1, mode='opencv')\n        else:\n            matrix = cv2.getRotationMatrix2D((49.5, 49.5), 10, 1)\n            return cv2.warpAffine(a, matrix, (a.shape[1], a.shape[0]))\n\n\n@testing.parameterize(*testing.product({\n    'shift': [0.1, -10, (5, -5)],\n    'output': [None, numpy.float64, 'empty'],\n    'order': [0, 1],\n    'mode': ['constant', 'nearest', 'mirror'],\n    'cval': [1.0],\n    'prefilter': [True],\n}))\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestShift(unittest.TestCase):\n\n    _multiprocess_can_split = True\n\n    def _shift(self, xp, scp, a):\n        shift = scp.ndimage.shift\n        if self.output == 'empty':\n            output = xp.empty_like(a)\n            return_value = shift(a, self.shift, output, self.order,\n                                 self.mode, self.cval, self.prefilter)\n            self.assertTrue(return_value is None or return_value is output)\n            return output\n        else:\n            return shift(a, self.shift, self.output, self.order,\n                         self.mode, self.cval, self.prefilter)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_shift_float(self, xp, scp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        return self._shift(xp, scp, a)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_shift_fortran_order(self, xp, scp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        a = xp.asfortranarray(a)\n        return self._shift(xp, scp, a)\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_shift_int(self, xp, scp, dtype):\n        if numpy.lib.NumpyVersion(scipy.__version__) < '1.0.0':\n            if dtype in (numpy.dtype('l'), numpy.dtype('q')):\n                dtype = numpy.int64\n            elif dtype in (numpy.dtype('L'), numpy.dtype('Q')):\n                dtype = numpy.uint64\n\n        a = testing.shaped_random((100, 100), xp, dtype)\n        out = self._shift(xp, scp, a)\n        float_out = self._shift(xp, scp, a.astype(xp.float64)) % 1\n        half = xp.full_like(float_out, 0.5)\n        out[xp.isclose(float_out, half, atol=1e-5)] = 0\n        return out\n\n\n@testing.gpu\n@testing.with_requires('opencv-python')\nclass TestShiftOpenCV(unittest.TestCase):\n\n    _multiprocess_can_split = True\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(atol=0.2)\n    def test_shift_opencv(self, xp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        shift = testing.shaped_random((2,), xp, dtype)\n        if xp == cupy:\n            return cupyx.scipy.ndimage.shift(a, shift, order=1,\n                                             mode='opencv')\n        else:\n            matrix = numpy.array([[1, 0, shift[1]], [0, 1, shift[0]]])\n            return cv2.warpAffine(a, matrix, (a.shape[1], a.shape[0]))\n\n\n@testing.parameterize(*testing.product({\n    'zoom': [0.1, 10, (0.1, 10)],\n    'output': [None, numpy.float64, 'empty'],\n    'order': [0, 1],\n    'mode': ['constant', 'nearest', 'mirror'],\n    'cval': [1.0],\n    'prefilter': [True],\n}))\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestZoom(unittest.TestCase):\n\n    _multiprocess_can_split = True\n\n    def _zoom(self, xp, scp, a):\n        zoom = scp.ndimage.zoom\n        if self.output == 'empty':\n            output = zoom(a, self.zoom, None, self.order,\n                          self.mode, self.cval, self.prefilter)\n            return_value = zoom(a, self.zoom, output, self.order,\n                                self.mode, self.cval, self.prefilter)\n            self.assertTrue(return_value is None or return_value is output)\n            return output\n        else:\n            return zoom(a, self.zoom, self.output, self.order,\n                        self.mode, self.cval, self.prefilter)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_zoom_float(self, xp, scp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        return self._zoom(xp, scp, a)\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_zoom_fortran_order(self, xp, scp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        a = xp.asfortranarray(a)\n        return self._zoom(xp, scp, a)\n\n    @testing.for_int_dtypes(no_bool=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_zoom_int(self, xp, scp, dtype):\n        if numpy.lib.NumpyVersion(scipy.__version__) < '1.0.0':\n            if dtype in (numpy.dtype('l'), numpy.dtype('q')):\n                dtype = numpy.int64\n            elif dtype in (numpy.dtype('L'), numpy.dtype('Q')):\n                dtype = numpy.uint64\n\n        a = testing.shaped_random((100, 100), xp, dtype)\n        out = self._zoom(xp, scp, a)\n        float_out = self._zoom(xp, scp, a.astype(xp.float64)) % 1\n        half = xp.full_like(float_out, 0.5)\n        out[xp.isclose(float_out, half, atol=1e-5)] = 0\n        return out\n\n\n@testing.parameterize(\n    {'zoom': 3},\n    {'zoom': 0.3},\n)\n@testing.gpu\n@testing.with_requires('opencv-python')\nclass TestZoomOpenCV(unittest.TestCase):\n\n    _multiprocess_can_split = True\n\n    @testing.for_float_dtypes(no_float16=True)\n    @testing.numpy_cupy_allclose(atol=1e-4)\n    def test_zoom_opencv(self, xp, dtype):\n        a = testing.shaped_random((100, 100), xp, dtype)\n        if xp == cupy:\n            return cupyx.scipy.ndimage.zoom(a, self.zoom, order=1,\n                                            mode='opencv')\n        else:\n            output_shape = numpy.rint(numpy.multiply(a.shape, self.zoom))\n            return cv2.resize(a, tuple(output_shape.astype(int)))\n"""
tests/cupyx_tests/scipy_tests/ndimage_tests/test_measurements.py,0,"b""import unittest\nimport pytest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\nimport cupyx.scipy.ndimage  # NOQA\n\ntry:\n    import scipy.ndimage  # NOQA\nexcept ImportError:\n    pass\n\n\ndef _generate_binary_structure(rank, connectivity):\n    if connectivity < 1:\n        connectivity = 1\n    if rank < 1:\n        return numpy.array(True, dtype=bool)\n    output = numpy.fabs(numpy.indices([3] * rank) - 1)\n    output = numpy.add.reduce(output, 0)\n    return output <= connectivity\n\n\n@testing.parameterize(*testing.product({\n    'ndim': [1, 2, 3, 4],\n    'size': [50, 100],\n    'density': [0.2, 0.3, 0.4],\n    'connectivity': [None, 2, 3],\n    'x_dtype': [bool, numpy.int8, numpy.int32, numpy.int64,\n                numpy.float32, numpy.float64],\n    'output': [None, numpy.int32, numpy.int64],\n    'o_type': [None, 'ndarray']\n}))\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestLabel(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal(scipy_name='scp')\n    def test_label(self, xp, scp):\n        size = int(pow(self.size, 1 / self.ndim))\n        x_shape = range(size, size + self.ndim)\n        x = xp.zeros(x_shape, dtype=self.x_dtype)\n        x[testing.shaped_random(x_shape, xp) < self.density] = 1\n        if self.connectivity is None:\n            structure = None\n        else:\n            structure = _generate_binary_structure(self.ndim,\n                                                   self.connectivity)\n        if self.o_type == 'ndarray' and self.output is not None:\n            output = xp.empty(x_shape, dtype=self.output)\n            num_features = scp.ndimage.label(x, structure=structure,\n                                             output=output)\n            return output\n        labels, num_features = scp.ndimage.label(x, structure=structure,\n                                                 output=self.output)\n        return labels\n\n\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestLabelSpecialCases(unittest.TestCase):\n\n    @testing.numpy_cupy_array_equal(scipy_name='scp')\n    def test_label_empty(self, xp, scp):\n        x = xp.empty(0)\n        labels, num_features = scp.ndimage.label(x)\n        return labels\n\n    @testing.numpy_cupy_array_equal(scipy_name='scp')\n    def test_label_0d_zero(self, xp, scp):\n        x = xp.zeros([])\n        labels, num_features = scp.ndimage.label(x)\n        return labels\n\n    @testing.numpy_cupy_array_equal(scipy_name='scp')\n    def test_label_0d_one(self, xp, scp):\n        x = xp.ones([])\n        labels, num_features = scp.ndimage.label(x)\n        return labels\n\n    @testing.numpy_cupy_array_equal(scipy_name='scp')\n    def test_label_swirl(self, xp, scp):\n        x = [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n             [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1],\n             [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n             [1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1],\n             [1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1],\n             [1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1],\n             [1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1],\n             [1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1],\n             [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n             [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n             [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n        x = xp.array(x)\n        labels, num_features = scp.ndimage.label(x)\n        return labels\n\n\n@testing.gpu\n@testing.parameterize(*testing.product({\n    'op': ['sum', 'mean', 'variance', 'standard_deviation'],\n}))\n@testing.with_requires('scipy')\nclass TestNdimage(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_almost_equal(scipy_name='scp')\n    def test_ndimage_single_dim(self, xp, scp, dtype):\n        image = xp.arange(100, dtype=dtype)\n        label = testing.shaped_random((100,), xp, dtype=xp.int32, scale=3)\n        index = xp.array([0, 1, 2])\n        return getattr(scp.ndimage, self.op)(image, label, index)\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_array_almost_equal(scipy_name='scp')\n    def test_ndimage_multi_dim(self, xp, scp, dtype):\n        image = xp.arange(512, dtype=dtype).reshape(8, 8, 8)\n        label = testing.shaped_random((8, 8, 8), xp, dtype=xp.int32, scale=3)\n        index = xp.array([0, 1, 2])\n        return getattr(scp.ndimage, self.op)(image, label, index)\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_allclose(scipy_name='scp')\n    def test_ndimage_only_input(self, xp, scp, dtype):\n        image = xp.arange(100, dtype=dtype)\n        return getattr(scp.ndimage, self.op)(image)\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_allclose(scipy_name='scp')\n    def test_ndimage_no_index(self, xp, scp, dtype):\n        image = xp.arange(100, dtype=dtype)\n        label = testing.shaped_random((100,), xp, dtype=xp.int32, scale=3)\n        return getattr(scp.ndimage, self.op)(image, label)\n\n    @testing.for_all_dtypes(no_bool=True, no_complex=True)\n    @testing.numpy_cupy_allclose(scipy_name='scp')\n    def test_ndimage_scalar_index(self, xp, scp, dtype):\n        image = xp.arange(100, dtype=dtype)\n        label = testing.shaped_random((100,), xp, dtype=xp.int32, scale=3)\n        return getattr(scp.ndimage, self.op)(image, label, 1)\n\n    @testing.for_dtypes([cupy.bool_, cupy.complex64, cupy.complex128])\n    def test_ndimage_wrong_dtype(self, dtype):\n        image = cupy.arange(100).astype(dtype)\n        label = cupy.random.randint(1, 4, dtype=cupy.int32)\n        index = cupy.array([1, 2, 3])\n        with pytest.raises(TypeError):\n            getattr(cupyx.scipy.ndimage, self.op)(image, label, index)\n\n    def test_ndimage_wrong_label_shape(self):\n        image = cupy.arange(100, dtype=cupy.int32)\n        label = cupy.random.randint(1, 3, dtype=cupy.int32, size=50)\n        index = cupy.array([1, 2, 3])\n        with pytest.raises(ValueError):\n            getattr(cupyx.scipy.ndimage, self.op)(image, label, index)\n\n    def test_ndimage_wrong_image_type(self):\n        image = list(range(100))\n        label = cupy.random.randint(1, 3, dtype=cupy.int32, size=100)\n        index = cupy.array([1, 2, 3])\n        with pytest.raises(TypeError):\n            getattr(cupyx.scipy.ndimage, self.op)(image, label, index)\n\n    def test_ndimage_wrong_label_type(self):\n        image = cupy.arange(100, dtype=cupy.int32)\n        label = numpy.random.randint(1, 3, dtype=numpy.int32, size=100)\n        index = cupy.array([1, 2, 3])\n        with pytest.raises(TypeError):\n            getattr(cupyx.scipy.ndimage, self.op)(image, label, index)\n\n    def test_ndimage_wrong_index_type(self):\n        image = cupy.arange(100, dtype=cupy.int32)\n        label = cupy.random.randint(1, 3, dtype=cupy.int32, size=100)\n        index = [1, 2, 3]\n        with pytest.raises(TypeError):\n            getattr(cupyx.scipy.ndimage, self.op)(image, label, index)\n\n    @testing.numpy_cupy_array_almost_equal(scipy_name='scp')\n    def test_ndimage_zero_values(self, xp, scp):\n        image = xp.array([])\n        label = xp.array([])\n        index = xp.array([])\n        return getattr(scp.ndimage, self.op)(image, label, index)\n"""
tests/cupyx_tests/scipy_tests/ndimage_tests/test_morphology.py,0,"b""import unittest\n\nimport numpy\n\nfrom cupy import testing\nimport cupyx.scipy.ndimage  # NOQA\n\nimport pytest\n\ntry:\n    import scipy.ndimage  # NOQA\nexcept ImportError:\n    pass\n\n\n@testing.parameterize(*(\n    testing.product({\n        'shape': [(3, 4), (2, 3, 4), (1, 2, 3, 4)],\n        'size': [3, 4],\n        'footprint': [None, 'random'],\n        'structure': [None, 'random'],\n        'mode': ['reflect'],\n        'cval': [0.0],\n        'origin': [0, 1, None],\n        'x_dtype': [numpy.int8, numpy.int16, numpy.int32,\n                    numpy.float32, numpy.float64],\n        'output': [None, numpy.int32, numpy.float64],\n        'filter': ['grey_erosion', 'grey_dilation']\n    }) + testing.product({\n        'shape': [(3, 4), (2, 3, 4), (1, 2, 3, 4)],\n        'size': [3, 4],\n        'footprint': [None, 'random'],\n        'structure': [None, 'random'],\n        'mode': ['constant'],\n        'cval': [-1.0, 0.0, 1.0],\n        'origin': [0],\n        'x_dtype': [numpy.int32, numpy.float64],\n        'output': [None],\n        'filter': ['grey_erosion', 'grey_dilation']\n    }) + testing.product({\n        'shape': [(3, 4), (2, 3, 4), (1, 2, 3, 4)],\n        'size': [3, 4],\n        'footprint': [None, 'random'],\n        'structure': [None, 'random'],\n        'mode': ['nearest', 'mirror', 'wrap'],\n        'cval': [0.0],\n        'origin': [0],\n        'x_dtype': [numpy.int32, numpy.float64],\n        'output': [None],\n        'filter': ['grey_erosion', 'grey_dilation']\n    })\n))\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestGreyErosionAndDilation(unittest.TestCase):\n\n    def _filter(self, xp, scp, x):\n        filter = getattr(scp.ndimage, self.filter)\n        if self.origin is None:\n            origin = (-1, 1, -1, 1)[:x.ndim]\n        else:\n            origin = self.origin\n        if self.footprint is None:\n            footprint = None\n        else:\n            shape = (self.size, ) * x.ndim\n            r = testing.shaped_random(shape, xp, scale=1)\n            footprint = xp.where(r < .5, 1, 0)\n            if not footprint.any():\n                footprint = xp.ones(shape)\n        if self.structure is None:\n            structure = None\n        else:\n            shape = (self.size, ) * x.ndim\n            structure = testing.shaped_random(shape, xp, dtype=xp.int32)\n        return filter(x, size=self.size, footprint=footprint,\n                      structure=structure, output=self.output,\n                      mode=self.mode, cval=self.cval, origin=origin)\n\n    @testing.numpy_cupy_allclose(atol=1e-5, rtol=1e-5, scipy_name='scp')\n    def test_grey_erosion_and_dilation(self, xp, scp):\n        if self.mode == 'mirror' and 1 in self.shape:\n            pytest.skip()\n        if self.x_dtype == self.output:\n            pytest.skip()\n        x = testing.shaped_random(self.shape, xp, self.x_dtype)\n        return self._filter(xp, scp, x)\n\n\n@testing.parameterize(*testing.product({\n    'size': [3, 4],\n    'structure': [None, 'random'],\n    'mode': ['reflect', 'constant', 'nearest', 'mirror', 'wrap'],\n    'origin': [0, None],\n    'x_dtype': [numpy.int32, numpy.float32],\n    'output': [None, numpy.float64],\n    'filter': ['grey_closing', 'grey_opening']\n}))\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestGreyClosingAndOpening(unittest.TestCase):\n\n    shape = (4, 5)\n    footprint = None\n    cval = 0.0\n\n    def _filter(self, xp, scp, x):\n        filter = getattr(scp.ndimage, self.filter)\n        if self.origin is None:\n            origin = (-1, 1, -1, 1)[:x.ndim]\n        else:\n            origin = self.origin\n        if self.structure is None:\n            structure = None\n        else:\n            shape = (self.size, ) * x.ndim\n            structure = testing.shaped_random(shape, xp, dtype=xp.int32)\n        return filter(x, size=self.size, footprint=self.footprint,\n                      structure=structure, output=self.output,\n                      mode=self.mode, cval=self.cval, origin=origin)\n\n    @testing.numpy_cupy_allclose(atol=1e-5, rtol=1e-5, scipy_name='scp')\n    def test_grey_closing_and_opening(self, xp, scp):\n        x = testing.shaped_random(self.shape, xp, self.x_dtype)\n        return self._filter(xp, scp, x)\n"""
tests/cupyx_tests/scipy_tests/sparse_tests/__init__.py,0,b''
tests/cupyx_tests/scipy_tests/sparse_tests/test_base.py,0,"b""import unittest\n\nimport pytest\ntry:\n    import scipy.sparse\n    scipy_available = True\nexcept ImportError:\n    scipy_available = False\n\nfrom cupy import testing\nfrom cupyx.scipy import sparse\n\n\nif scipy_available:\n    class DummySparseCPU(scipy.sparse.spmatrix):\n\n        def __init__(self, maxprint=50, shape=None, nnz=0):\n            super(DummySparseCPU, self).__init__(maxprint)\n            self._shape = shape\n            self._nnz = nnz\n\n        def getnnz(self):\n            return self._nnz\n\n\nclass DummySparseGPU(sparse.spmatrix):\n\n    def __init__(self, maxprint=50, shape=None, nnz=0):\n        super(DummySparseGPU, self).__init__(maxprint)\n        self._shape = shape\n        self._nnz = nnz\n\n    def get_shape(self):\n        return self._shape\n\n    def getnnz(self):\n        return self._nnz\n\n\n@testing.with_requires('scipy')\nclass TestSpmatrix(unittest.TestCase):\n\n    def dummy_class(self, sp):\n        if sp is sparse:\n            return DummySparseGPU\n        else:\n            return DummySparseCPU\n\n    def test_instantiation(self):\n        for sp in (scipy.sparse, sparse):\n            with pytest.raises(ValueError):\n                sp.spmatrix()\n\n    def test_len(self):\n        for sp in (scipy.sparse, sparse):\n            s = self.dummy_class(sp)()\n            with pytest.raises(TypeError):\n                len(s)\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_bool_true(self, xp, sp):\n        s = self.dummy_class(sp)(shape=(1, 1), nnz=1)\n        return bool(s)\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_bool_false(self, xp, sp):\n        s = self.dummy_class(sp)(shape=(1, 1), nnz=0)\n        return bool(s)\n\n    def test_bool_invalid(self):\n        for sp in (scipy.sparse, sparse):\n            s = self.dummy_class(sp)(shape=(2, 1))\n            with pytest.raises(ValueError):\n                bool(s)\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_asformat_none(self, xp, sp):\n        s = self.dummy_class(sp)()\n        self.assertIs(s.asformat(None), s)\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_maxprint(self, xp, sp):\n        s = self.dummy_class(sp)(maxprint=30)\n        return s.getmaxprint()\n"""
tests/cupyx_tests/scipy_tests/sparse_tests/test_construct.py,0,"b'import re\nimport unittest\n\nimport mock\nimport numpy\nimport pytest\ntry:\n    import scipy.sparse\n    scipy_available = True\nexcept ImportError:\n    scipy_available = False\n\nimport cupy\nfrom cupy import testing\nfrom cupyx.scipy import sparse\nfrom cupyx.scipy.sparse import construct\n\n\n@testing.parameterize(*testing.product({\n    \'dtype\': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n    \'format\': [\'csr\', \'csc\', \'coo\'],\n    \'m\': [3],\n    \'n\': [None, 3, 2],\n    \'k\': [0, 1],\n}))\n@testing.with_requires(\'scipy\')\nclass TestEye(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name=\'sp\')\n    def test_eye(self, xp, sp):\n        x = sp.eye(\n            self.m, n=self.n, k=self.k, dtype=self.dtype, format=self.format)\n        self.assertIsInstance(x, sp.spmatrix)\n        self.assertEqual(x.format, self.format)\n        return x\n\n\n@testing.parameterize(*testing.product({\n    \'dtype\': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n    \'format\': [\'csr\', \'csc\', \'coo\'],\n}))\n@testing.with_requires(\'scipy\')\nclass TestIdentity(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name=\'sp\')\n    def test_eye(self, xp, sp):\n        x = sp.identity(3, dtype=self.dtype, format=self.format)\n        self.assertIsInstance(x, sp.spmatrix)\n        self.assertEqual(x.format, self.format)\n        return x\n\n\n@testing.parameterize(*testing.product({\n    \'dtype\': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@testing.with_requires(\'scipy\')\nclass TestSpdiags(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name=\'sp\')\n    def test_spdiags(self, xp, sp):\n        data = xp.arange(12, dtype=self.dtype).reshape(3, 4)\n        diags = xp.array([0, -1, 2], dtype=\'i\')\n        x = sp.spdiags(data, diags, 3, 4)\n        return x\n\n\n@testing.parameterize(*testing.product({\n    \'dtype\': [numpy.float32, numpy.float64]\n}))\nclass TestVstack(unittest.TestCase):\n\n    def data(self):\n\n        A = sparse.coo_matrix((cupy.asarray([1.0, 2.0, 3.0, 4.0]),\n                               (cupy.asarray([0, 0, 1, 1]),\n                                cupy.asarray([0, 1, 0, 1]))))\n        B = sparse.coo_matrix((cupy.asarray([5.0, 6.0]),\n                               (cupy.asarray([0, 0]),\n                                cupy.asarray([0, 1]))))\n\n        return A, B\n\n    def expected(self):\n\n        return cupy.asarray([[1, 2],\n                             [3, 4],\n                             [5, 6]], self.dtype)\n\n    def test_basic_vstack(self):\n\n        A, B = self.data()\n\n        actual = construct.vstack([A, B]).todense()\n        testing.assert_array_equal(actual, self.expected())\n\n    def test_dtype(self):\n\n        A, B = self.data()\n\n        actual = construct.vstack([A, B], dtype=self.dtype)\n        self.assertEqual(actual.dtype, self.dtype)\n\n    def test_csr(self):\n\n        A, B = self.data()\n\n        actual = construct.vstack([A.tocsr(), B.tocsr()]).todense()\n        testing.assert_array_equal(actual, self.expected())\n\n    def test_csr_with_dtype(self):\n\n        A, B = self.data()\n\n        actual = construct.vstack([A.tocsr(), B.tocsr()],\n                                  dtype=self.dtype)\n        self.assertEqual(actual.dtype, self.dtype)\n        self.assertEqual(actual.indices.dtype, cupy.int32)\n        self.assertEqual(actual.indptr.dtype, cupy.int32)\n\n\n@testing.parameterize(*testing.product({\n    \'dtype\': [numpy.float32, numpy.float64]\n}))\nclass TestHstack(unittest.TestCase):\n\n    def data(self):\n\n        A = sparse.coo_matrix((cupy.asarray([1.0, 2.0, 3.0, 4.0]),\n                               (cupy.asarray([0, 0, 1, 1]),\n                                cupy.asarray([0, 1, 0, 1]))))\n        B = sparse.coo_matrix((cupy.asarray([5.0, 6.0]),\n                               (cupy.asarray([0, 1]),\n                                cupy.asarray([0, 0]))))\n\n        return A, B\n\n    def expected(self):\n\n        return cupy.asarray([[1, 2, 5],\n                             [3, 4, 6]])\n\n    def test_basic_hstack(self):\n\n        A, B = self.data()\n        actual = construct.hstack([A, B], dtype=self.dtype).todense()\n        testing.assert_array_equal(actual, self.expected())\n        self.assertEqual(actual.dtype, self.dtype)\n\n    def test_csc(self):\n        A, B = self.data()\n        actual = construct.hstack([A.tocsc(), B.tocsc()],\n                                  dtype=self.dtype).todense()\n        testing.assert_array_equal(actual, self.expected())\n        self.assertEqual(actual.dtype, self.dtype)\n\n    def test_csc_with_dtype(self):\n\n        A, B = self.data()\n\n        actual = construct.hstack([A.tocsc(), B.tocsc()],\n                                  dtype=self.dtype)\n        self.assertEqual(actual.indices.dtype, cupy.int32)\n        self.assertEqual(actual.indptr.dtype, cupy.int32)\n\n\n@testing.parameterize(*testing.product({\n    \'dtype\': [numpy.float32, numpy.float64]\n}))\nclass TestBmat(unittest.TestCase):\n\n    def data(self):\n        A = sparse.csr_matrix(cupy.asarray([[1, 2], [3, 4]],\n                                           self.dtype)).tocoo()\n        B = sparse.csr_matrix(cupy.asarray([[5], [6]],\n                                           self.dtype)).tocoo()\n        C = sparse.csr_matrix(cupy.asarray([[7]],\n                                           self.dtype)).tocoo()\n        D = sparse.coo_matrix((0, 0), dtype=self.dtype)\n\n        return A, B, C, D\n\n    def test_basic_inputs(self):\n\n        A, B, C, D = self.data()\n\n        expected = cupy.asarray([[1, 2, 5],\n                                 [3, 4, 6],\n                                 [0, 0, 7]], dtype=self.dtype)\n\n        testing.assert_array_equal(\n            construct.bmat([[A, B], [None, C]]).todense(), expected\n        )\n\n        expected = cupy.asarray([[1, 2, 0],\n                                 [3, 4, 0],\n                                 [0, 0, 7]])\n        testing.assert_array_equal(\n            construct.bmat([[A, None], [None, C]]).todense(), expected\n        )\n\n        expected = cupy.asarray([[0, 5],\n                                 [0, 6],\n                                 [7, 0]])\n\n        testing.assert_array_equal(\n            construct.bmat([[None, B], [C, None]]).todense(), expected\n        )\n\n    def test_empty(self):\n\n        A, B, C, D = self.data()\n\n        expected = cupy.empty((0, 0), dtype=self.dtype)\n        testing.assert_array_equal(construct.bmat([[None, None]]).todense(),\n                                   expected)\n        testing.assert_array_equal(construct.bmat([[None, D], [D, None]])\n                                   .todense(), expected)\n\n    def test_edge_cases(self):\n        """"""Catch-all for small edge cases""""""\n\n        A, B, C, D = self.data()\n\n        expected = cupy.asarray([[7]], dtype=self.dtype)\n        testing.assert_array_equal(construct.bmat([[None, D], [C, None]])\n                                   .todense(), expected)\n\n    def test_failure_cases(self):\n\n        A, B, C, D = self.data()\n\n        match = r\'.*Got blocks\\[{}\\]\\.shape\\[{}\\] == 1, expected 2\'\n\n        # test failure cases\n        message1 = re.compile(match.format(\'1,0\', \'1\'))\n        with pytest.raises(ValueError, match=message1):\n            construct.bmat([[A], [B]], dtype=self.dtype)\n\n        message2 = re.compile(match.format(\'0,1\', \'0\'))\n        with pytest.raises(ValueError, match=message2):\n            construct.bmat([[A, C]], dtype=self.dtype)\n\n\n@testing.parameterize(*testing.product({\n    \'random_method\': [\'random\', \'rand\'],\n    \'dtype\': [numpy.float32, numpy.float64],\n    \'format\': [\'csr\', \'csc\', \'coo\'],\n}))\nclass TestRandom(unittest.TestCase):\n\n    def test_random(self):\n        x = getattr(sparse, self.random_method)(\n            3, 4, density=0.1,\n            format=self.format, dtype=self.dtype)\n        self.assertEqual(x.shape, (3, 4))\n        self.assertEqual(x.dtype, self.dtype)\n        self.assertEqual(x.format, self.format)\n\n    def test_random_with_seed(self):\n        x = getattr(sparse, self.random_method)(\n            3, 4, density=0.1,\n            format=self.format, dtype=self.dtype,\n            random_state=1)\n        self.assertEqual(x.shape, (3, 4))\n        self.assertEqual(x.dtype, self.dtype)\n        self.assertEqual(x.format, self.format)\n\n        y = getattr(sparse, self.random_method)(\n            3, 4, density=0.1,\n            format=self.format, dtype=self.dtype,\n            random_state=1)\n\n        self.assertTrue((x.toarray() == y.toarray()).all())\n\n    def test_random_with_state(self):\n        state1 = cupy.random.RandomState(1)\n        x = getattr(sparse, self.random_method)(\n            3, 4, density=0.1,\n            format=self.format, dtype=self.dtype,\n            random_state=state1)\n        self.assertEqual(x.shape, (3, 4))\n        self.assertEqual(x.dtype, self.dtype)\n        self.assertEqual(x.format, self.format)\n\n        state2 = cupy.random.RandomState(1)\n        y = getattr(sparse, self.random_method)(\n            3, 4, density=0.1,\n            format=self.format, dtype=self.dtype,\n            random_state=state2)\n\n        self.assertTrue((x.toarray() == y.toarray()).all())\n\n    def test_random_with_data_rvs(self):\n        if self.random_method == \'rand\':\n            pytest.skip(\'cupyx.scipy.sparse.rand does not support data_rvs\')\n        data_rvs = mock.MagicMock(side_effect=cupy.zeros)\n        x = getattr(sparse, self.random_method)(\n            3, 4, density=0.1, data_rvs=data_rvs,\n            format=self.format, dtype=self.dtype)\n        self.assertEqual(x.shape, (3, 4))\n        self.assertEqual(x.dtype, self.dtype)\n        self.assertEqual(x.format, self.format)\n\n        self.assertEqual(data_rvs.call_count, 1)\n        # Note that its value is generated randomly\n        self.assertIsInstance(data_rvs.call_args[0][0], int)\n\n\n@testing.with_requires(\'scipy\')\nclass TestRandomInvalidArgument(unittest.TestCase):\n\n    def test_too_small_density(self):\n        for sp in (scipy.sparse, sparse):\n            with pytest.raises(ValueError):\n                sp.random(3, 4, density=-0.1)\n\n    def test_too_large_density(self):\n        for sp in (scipy.sparse, sparse):\n            with pytest.raises(ValueError):\n                sp.random(3, 4, density=1.1)\n\n    def test_invalid_dtype(self):\n        # Note: SciPy 1.12+ accepts integer.\n        with self.assertRaises(NotImplementedError):\n            sparse.random(3, 4, dtype=\'i\')\n\n\n@testing.parameterize(*testing.product({\n    \'dtype\': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n    \'format\': [\'dia\', \'csr\', \'csc\', \'coo\'],\n}))\n@testing.with_requires(\'scipy\')\nclass TestDiags(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name=\'sp\')\n    def test_diags_scalar_offset(self, xp, sp):\n        x = sp.diags(\n            xp.arange(16), offsets=0, dtype=self.dtype, format=self.format)\n        self.assertIsInstance(x, sp.spmatrix)\n        self.assertEqual(x.format, self.format)\n        return x\n\n    @testing.numpy_cupy_allclose(sp_name=\'sp\')\n    def test_diags_single_element_lists(self, xp, sp):\n        x = sp.diags(\n            [xp.arange(16)], offsets=[0], dtype=self.dtype, format=self.format)\n        self.assertIsInstance(x, sp.spmatrix)\n        self.assertEqual(x.format, self.format)\n        return x\n\n    @testing.numpy_cupy_allclose(sp_name=\'sp\')\n    def test_diags_multiple(self, xp, sp):\n        x = sp.diags(\n            [xp.arange(15), xp.arange(16), xp.arange(15), xp.arange(13)],\n            offsets=[-1, 0, 1, 3],\n            dtype=self.dtype, format=self.format)\n        self.assertIsInstance(x, sp.spmatrix)\n        self.assertEqual(x.format, self.format)\n        return x\n\n    @testing.numpy_cupy_allclose(sp_name=\'sp\')\n    def test_diags_offsets_as_array(self, xp, sp):\n        x = sp.diags(\n            [xp.arange(15), xp.arange(16), xp.arange(15), xp.arange(13)],\n            offsets=xp.array([-1, 0, 1, 3]),\n            dtype=self.dtype, format=self.format)\n        self.assertIsInstance(x, sp.spmatrix)\n        self.assertEqual(x.format, self.format)\n        return x\n\n    @testing.numpy_cupy_allclose(sp_name=\'sp\')\n    def test_diags_non_square(self, xp, sp):\n        x = sp.diags(\n            [xp.arange(5), xp.arange(3)],\n            offsets=[0, -2], shape=(5, 10),\n            dtype=self.dtype, format=self.format)\n        self.assertIsInstance(x, sp.spmatrix)\n        self.assertEqual(x.format, self.format)\n        return x\n'"
tests/cupyx_tests/scipy_tests/sparse_tests/test_coo.py,0,"b""import pickle\nimport unittest\n\nimport numpy\nimport pytest\ntry:\n    import scipy.sparse\n    scipy_available = True\nexcept ImportError:\n    scipy_available = False\n\nimport cupy\nfrom cupy import testing\nfrom cupyx.scipy import sparse\n\n\ndef _make(xp, sp, dtype):\n    data = xp.array([0, 1, 2, 3], dtype)\n    row = xp.array([0, 0, 1, 2], 'i')\n    col = xp.array([0, 1, 3, 2], 'i')\n    # 0, 1, 0, 0\n    # 0, 0, 0, 2\n    # 0, 0, 3, 0\n    return sp.coo_matrix((data, (row, col)), shape=(3, 4))\n\n\ndef _make_complex(xp, sp, dtype):\n    data = xp.array([0, 1, 2, 3], dtype)\n    if dtype in [numpy.complex64, numpy.complex128]:\n        data = data - 1j\n    row = xp.array([0, 0, 1, 2], 'i')\n    col = xp.array([0, 1, 3, 2], 'i')\n    # 0, 1 - 1j, 0, 0\n    # 0, 0, 0, 2 - 1j\n    # 0, 0, 3 - 1j, 0\n    return sp.coo_matrix((data, (row, col)), shape=(3, 4))\n\n\ndef _make2(xp, sp, dtype):\n    data = xp.array([1, 2, 3, 4], dtype)\n    row = xp.array([0, 1, 1, 2], 'i')\n    col = xp.array([2, 1, 2, 2], 'i')\n    # 0, 0, 1, 0\n    # 0, 2, 3, 0\n    # 0, 0, 4, 0\n    return sp.coo_matrix((data, (row, col)), shape=(3, 4))\n\n\ndef _make3(xp, sp, dtype):\n    data = xp.array([1, 2, 3, 4, 5], dtype)\n    row = xp.array([0, 1, 1, 3, 3], 'i')\n    col = xp.array([0, 2, 1, 0, 2], 'i')\n    # 1, 0, 0\n    # 0, 3, 2\n    # 0, 0, 0\n    # 4, 0, 5\n    return sp.coo_matrix((data, (row, col)), shape=(4, 3))\n\n\ndef _make_unordered(xp, sp, dtype):\n    data = xp.array([1, 4, 3, 2], dtype)\n    row = xp.array([0, 2, 1, 0], 'i')\n    col = xp.array([0, 2, 3, 1], 'i')\n    # 1, 2, 0, 0\n    # 0, 0, 0, 3\n    # 0, 0, 4, 0\n    return sp.coo_matrix((data, (row, col)), shape=(3, 4))\n\n\ndef _make_duplicate(xp, sp, dtype):\n    data = xp.array([0, 1, 2, 3, 4, 5], dtype)\n    row = xp.array([1, 1, 1, 1, 0, 1], 'i')\n    col = xp.array([0, 0, 2, 0, 0, 2], 'i')\n    # 4, 0, 0, 0\n    # 4, 0, 7, 0\n    # 0, 0, 0, 0\n    return sp.coo_matrix((data, (row, col)), shape=(3, 4))\n\n\ndef _make_empty(xp, sp, dtype):\n    data = xp.array([], dtype)\n    row = xp.array([], 'i')\n    col = xp.array([], 'i')\n    return sp.coo_matrix((data, (row, col)), shape=(3, 4))\n\n\ndef _make_square(xp, sp, dtype):\n    data = xp.array([0, 1, 2, 3], dtype)\n    row = xp.array([0, 0, 1, 2], 'i')\n    col = xp.array([0, 2, 0, 2], 'i')\n    # 0, 1, 0\n    # 2, 0, 0\n    # 0, 0, 3\n    return sp.coo_matrix((data, (row, col)), shape=(3, 3))\n\n\ndef _make_shape(xp, sp, dtype):\n    return sp.coo_matrix((3, 4))\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\nclass TestCooMatrix(unittest.TestCase):\n\n    def setUp(self):\n        self.m = _make(cupy, sparse, self.dtype)\n\n    def test_dtype(self):\n        self.assertEqual(self.m.dtype, self.dtype)\n\n    def test_data(self):\n        self.assertEqual(self.m.data.dtype, self.dtype)\n        testing.assert_array_equal(\n            self.m.data, cupy.array([0, 1, 2, 3], self.dtype))\n\n    def test_row(self):\n        self.assertEqual(self.m.row.dtype, numpy.int32)\n        testing.assert_array_equal(\n            self.m.row, cupy.array([0, 0, 1, 2], self.dtype))\n\n    def test_col(self):\n        self.assertEqual(self.m.col.dtype, numpy.int32)\n        testing.assert_array_equal(\n            self.m.col, cupy.array([0, 1, 3, 2], self.dtype))\n\n    def test_init_copy(self):\n        n = sparse.coo_matrix(self.m)\n        self.assertIsNot(n, self.m)\n        cupy.testing.assert_array_equal(n.toarray(), self.m.toarray())\n\n    def test_init_copy_other_sparse(self):\n        n = sparse.coo_matrix(self.m.tocsr())\n        cupy.testing.assert_array_equal(n.toarray(), self.m.toarray())\n\n    @unittest.skipUnless(scipy_available, 'requires scipy')\n    def test_init_copy_scipy_sparse(self):\n        m = _make(numpy, scipy.sparse, self.dtype)\n        n = sparse.coo_matrix(m)\n        self.assertIsInstance(n.data, cupy.ndarray)\n        self.assertIsInstance(n.row, cupy.ndarray)\n        self.assertIsInstance(n.col, cupy.ndarray)\n        cupy.testing.assert_array_equal(n.data, m.data)\n        cupy.testing.assert_array_equal(n.row, m.row)\n        cupy.testing.assert_array_equal(n.col, m.col)\n        self.assertEqual(n.shape, m.shape)\n\n    @unittest.skipUnless(scipy_available, 'requires scipy')\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_init_copy_other_scipy_sparse(self, xp, sp):\n        m = _make(numpy, scipy.sparse, self.dtype)\n        n = sp.coo_matrix(m.tocsc())\n        assert len(n.data) == len(m.data)\n        assert len(n.row) == len(m.row)\n        assert len(n.col) == len(m.col)\n        assert n.shape == m.shape\n        return n\n\n    def test_pickle_roundtrip(self):\n        s = _make(cupy, sparse, self.dtype)\n        s2 = pickle.loads(pickle.dumps(s))\n        assert s.shape == s2.shape\n        assert s.dtype == s2.dtype\n        if scipy_available:\n            assert (s.get() != s2.get()).count_nonzero() == 0\n\n    def test_shape(self):\n        self.assertEqual(self.m.shape, (3, 4))\n\n    def test_ndim(self):\n        self.assertEqual(self.m.ndim, 2)\n\n    def test_nnz(self):\n        self.assertEqual(self.m.nnz, 4)\n\n    def test_conj(self):\n        n = _make_complex(cupy, sparse, self.dtype)\n        cupy.testing.assert_array_equal(n.conj().data, n.data.conj())\n\n    def test_has_canonical_format(self):\n        self.assertFalse(self.m.has_canonical_format)\n\n    @unittest.skipUnless(scipy_available, 'requires scipy')\n    def test_get(self):\n        m = self.m.get()\n        self.assertIsInstance(m, scipy.sparse.coo_matrix)\n        expect = [\n            [0, 1, 0, 0],\n            [0, 0, 0, 2],\n            [0, 0, 3, 0]\n        ]\n        numpy.testing.assert_allclose(m.toarray(), expect)\n\n    @unittest.skipUnless(scipy_available, 'requires scipy')\n    def test_str(self):\n        if numpy.dtype(self.dtype).kind == 'f':\n            expect = '''  (0, 0)\\t0.0\n  (0, 1)\\t1.0\n  (1, 3)\\t2.0\n  (2, 2)\\t3.0'''\n        elif numpy.dtype(self.dtype).kind == 'c':\n            expect = '''  (0, 0)\\t0j\n  (0, 1)\\t(1+0j)\n  (1, 3)\\t(2+0j)\n  (2, 2)\\t(3+0j)'''\n        self.assertEqual(str(self.m), expect)\n\n    def test_toarray(self):\n        m = self.m.toarray()\n        expect = [\n            [0, 1, 0, 0],\n            [0, 0, 0, 2],\n            [0, 0, 3, 0]\n        ]\n        cupy.testing.assert_allclose(m, expect)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@unittest.skipUnless(scipy_available, 'requires scipy')\nclass TestCooMatrixInit(unittest.TestCase):\n\n    def setUp(self):\n        self.shape = (3, 4)\n\n    def data(self, xp):\n        return xp.array([0, 1, 2, 3], self.dtype)\n\n    def row(self, xp):\n        return xp.array([0, 0, 1, 2], 'i')\n\n    def col(self, xp):\n        return xp.array([0, 1, 3, 2], 'i')\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_shape_none(self, xp, sp):\n        x = sp.coo_matrix(\n            (self.data(xp), (self.row(xp), self.col(xp))), shape=None)\n        self.assertEqual(x.shape, (3, 4))\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_dtype(self, xp, sp):\n        data = self.data(xp).real.astype('i')\n        x = sp.coo_matrix(\n            (data, (self.row(xp), self.col(xp))), dtype=self.dtype)\n        self.assertEqual(x.dtype, self.dtype)\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_copy_true(self, xp, sp):\n        data = self.data(xp)\n        row = self.row(xp)\n        col = self.col(xp)\n        x = sp.coo_matrix((data, (row, col)), copy=True)\n\n        self.assertIsNot(data, x.data)\n        self.assertIsNot(row, x.row)\n        self.assertIsNot(col, x.col)\n\n    def test_invalid_format(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(TypeError):\n                sp.coo_matrix(\n                    (self.data(xp), self.row(xp)), shape=self.shape)\n\n    @testing.numpy_cupy_allclose(sp_name='sp', atol=1e-5)\n    def test_intlike_shape(self, xp, sp):\n        s = sp.coo_matrix((self.data(xp), (self.row(xp), self.col(xp))),\n                          shape=(xp.array(self.shape[0]),\n                                 xp.int32(self.shape[1])))\n        assert isinstance(s.shape[0], int)\n        assert isinstance(s.shape[1], int)\n        return s\n\n    def test_shape_invalid(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.coo_matrix(\n                    (self.data(xp), (self.row(xp), self.col(xp))),\n                    shape=(2,))\n\n    def test_data_invalid(self):\n        with self.assertRaises(ValueError):\n            sparse.coo_matrix(\n                ('invalid', (self.row(cupy), self.col(cupy))),\n                shape=self.shape)\n\n    def test_data_invalid_ndim(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.coo_matrix(\n                    (self.data(xp)[None], (self.row(xp), self.col(xp))),\n                    shape=self.shape)\n\n    def test_row_invalid(self):\n        with self.assertRaises(ValueError):\n            sparse.coo_matrix(\n                (self.data(cupy), ('invalid', self.col(cupy))),\n                shape=self.shape)\n\n    def test_row_invalid_ndim(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.coo_matrix(\n                    (self.data(xp), (self.row(xp)[None], self.col(xp))),\n                    shape=self.shape)\n\n    def test_col_invalid(self):\n        with self.assertRaises(ValueError):\n            sparse.coo_matrix(\n                (self.data(cupy), (self.row(cupy), 'invalid')),\n                shape=self.shape)\n\n    def test_col_invalid_ndim(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.coo_matrix(\n                    (self.data(xp), (self.row(xp), self.col(xp)[None])),\n                    shape=self.shape)\n\n    def test_data_different_length(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            data = xp.arange(5, dtype=self.dtype)\n            with pytest.raises(TypeError):\n                sp.coo_matrix(\n                    (data(xp), (self.row(xp), self.col(xp))),\n                    shape=self.shape)\n\n    def test_row_different_length(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            row = xp.arange(5, dtype=self.dtype)\n            with pytest.raises(TypeError):\n                sp.coo_matrix(\n                    (self.data(xp), (row(xp), self.col(xp))),\n                    shape=self.shape)\n\n    def test_col_different_length(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            col = xp.arange(5, dtype=self.dtype)\n            with pytest.raises(TypeError):\n                sp.coo_matrix(\n                    (self.data(xp), (self.row(xp), col(xp))),\n                    shape=self.shape)\n\n    def test_fail_to_infer_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            data = xp.array([], dtype=self.dtype)\n            row = xp.array([], dtype='i')\n            col = xp.array([], dtype='i')\n            with pytest.raises(ValueError):\n                sp.coo_matrix((data, (row, col)), shape=None)\n\n    def test_row_too_large(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            row = xp.array([0, 0, 1, 3], 'i')\n            with pytest.raises(ValueError):\n                sp.coo_matrix(\n                    (self.data(xp), (row, self.col(xp))),\n                    shape=self.shape)\n\n    def test_row_too_small(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            row = xp.array([0, -1, 1, 2], 'i')\n            with pytest.raises(ValueError):\n                sp.coo_matrix(\n                    (self.data(xp), (row, self.col(xp))),\n                    shape=self.shape)\n\n    def test_col_too_large(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            col = xp.array([0, 1, 4, 2], 'i')\n            with pytest.raises(ValueError):\n                sp.coo_matrix(\n                    (self.data(xp), (self.row(xp), col)),\n                    shape=self.shape)\n\n    def test_col_too_small(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            col = xp.array([0, -1, 3, 2], 'i')\n            with pytest.raises(ValueError):\n                sp.coo_matrix(\n                    (self.data(xp), (self.row(xp), col)),\n                    shape=self.shape)\n\n    def test_unsupported_dtype(self):\n        with self.assertRaises(ValueError):\n            sparse.coo_matrix(\n                (self.data(cupy), (self.row(cupy), self.col(cupy))),\n                shape=self.shape, dtype='i')\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_conj(self, xp, sp):\n        n = _make_complex(xp, sp, self.dtype)\n        cupy.testing.assert_array_equal(n.conj().data, n.data.conj())\n\n\n@testing.parameterize(*testing.product({\n    'make_method': [\n        '_make', '_make_unordered', '_make_empty', '_make_duplicate',\n        '_make_shape'],\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@unittest.skipUnless(scipy_available, 'requires scipy')\nclass TestCooMatrixScipyComparison(unittest.TestCase):\n\n    @property\n    def make(self):\n        return globals()[self.make_method]\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_dtype(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.dtype\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_nnz(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.getnnz()\n\n    @testing.numpy_cupy_array_equal(sp_name='sp')\n    def test_asfptype(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m.asfptype()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_toarray(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.toarray()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_A(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.A\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocoo(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m.tocoo()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocoo_copy(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        n = m.tocoo(copy=True)\n        self.assertIsNot(m.data, n.data)\n        self.assertIsNot(m.row, n.row)\n        self.assertIsNot(m.col, n.col)\n        return n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocsc(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.tocsc()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocsc_copy(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        n = m.tocsc(copy=True)\n        self.assertIsNot(m.data, n.data)\n        return n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocsr(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.tocsr()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocsr_copy(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        n = m.tocsr(copy=True)\n        self.assertIsNot(m.data, n.data)\n        return n\n\n    # dot\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_scalar(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m.dot(2.0)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_numpy_scalar(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m.dot(numpy.dtype(self.dtype).type(2.0))\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_csr(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype)\n        return m.dot(x)\n\n    def test_dot_csr_invalid_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            x = sp.csr_matrix((5, 3), dtype=self.dtype)\n            with pytest.raises(ValueError):\n                m.dot(x)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_csc(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocsc()\n        return m.dot(x)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_sparse(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocoo()\n        return m.dot(x)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_zero_dim(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = xp.array(2, dtype=self.dtype)\n        return m.dot(x)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_dense_vector(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = xp.arange(4).astype(self.dtype)\n        return m.dot(x)\n\n    def test_dot_dense_vector_invalid_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            x = xp.arange(5).astype(self.dtype)\n            with pytest.raises(ValueError):\n                m.dot(x)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_dense_matrix(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = xp.arange(8).reshape(4, 2).astype(self.dtype)\n        return m.dot(x)\n\n    def test_dot_dense_matrix_invalid_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            x = xp.arange(10).reshape(5, 2).astype(self.dtype)\n            with pytest.raises(ValueError):\n                m.dot(x)\n\n    def test_dot_dense_ndim3(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            x = xp.arange(24).reshape(4, 2, 3).astype(self.dtype)\n            with pytest.raises(ValueError):\n                m.dot(x)\n\n    def test_dot_unsupported(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m.dot(None)\n\n    # __add__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_add_zero(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m + 0\n\n    def test_add_scalar(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            with pytest.raises(NotImplementedError):\n                m + 1\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_add_csr(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        n = _make2(xp, sp, self.dtype)\n        return m + n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_add_coo(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        n = _make2(xp, sp, self.dtype).tocoo()\n        return m + n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_add_dense(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        n = xp.arange(12).reshape(3, 4)\n        return m + n\n\n    # __radd__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_radd_zero(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return 0 + m\n\n    def test_radd_scalar(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            with pytest.raises(NotImplementedError):\n                1 + m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_radd_dense(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        n = xp.arange(12).reshape(3, 4)\n        return n + m\n\n    # __sub__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sub_zero(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m - 0\n\n    def test_sub_scalar(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            with pytest.raises(NotImplementedError):\n                m - 1\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sub_csr(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        n = _make2(xp, sp, self.dtype)\n        return m - n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sub_coo(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        n = _make2(xp, sp, self.dtype).tocoo()\n        return m - n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sub_dense(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        n = xp.arange(12).reshape(3, 4)\n        return m - n\n\n    # __rsub__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rsub_zero(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return 0 - m\n\n    def test_rsub_scalar(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            with pytest.raises(NotImplementedError):\n                1 - m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rsub_dense(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        n = xp.arange(12).reshape(3, 4)\n        return n - m\n\n    # __mul__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_scalar(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m * 2.0\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_numpy_scalar(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m * numpy.dtype(self.dtype).type(2.0)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_csr(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype)\n        return m * x\n\n    def test_mul_csr_invalid_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            x = sp.csr_matrix((5, 3), dtype=self.dtype)\n            with pytest.raises(ValueError):\n                m * x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_csc(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocsc()\n        return m * x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_sparse(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocoo()\n        return m * x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_zero_dim(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = xp.array(2, dtype=self.dtype)\n        return m * x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_dense_vector(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = xp.arange(4).astype(self.dtype)\n        return m * x\n\n    def test_mul_dense_vector_invalid_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            x = xp.arange(5).astype(self.dtype)\n            with pytest.raises(ValueError):\n                m * x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_dense_matrix(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = xp.arange(8).reshape(4, 2).astype(self.dtype)\n        return m * x\n\n    def test_mul_dense_matrix_invalid_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            x = xp.arange(10).reshape(5, 2).astype(self.dtype)\n            with pytest.raises(ValueError):\n                m * x\n\n    def test_mul_dense_ndim3(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            x = xp.arange(24).reshape(4, 2, 3).astype(self.dtype)\n            with pytest.raises(ValueError):\n                m * x\n\n    def test_mul_unsupported(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m * None\n\n    # __rmul__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_scalar(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return 2.0 * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_numpy_scalar(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return numpy.dtype(self.dtype).type(2.0) * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_csr(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype)\n        return x * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_csc(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocsc()\n        return x * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_sparse(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocoo()\n        return x * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_zero_dim(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = xp.array(2, dtype=self.dtype)\n        return x * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_dense_matrix(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        x = xp.arange(12).reshape(4, 3).astype(self.dtype)\n        return x * m\n\n    def test_rmul_dense_ndim3(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            x = xp.arange(24).reshape(4, 2, 3).astype(self.dtype)\n            with pytest.raises(ValueError):\n                x * m\n\n    def test_rmul_unsupported(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                None * m\n\n    # __pow__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_pow_0(self, xp, sp):\n        m = _make_square(xp, sp, self.dtype)\n        return m ** 0\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_pow_1(self, xp, sp):\n        m = _make_square(xp, sp, self.dtype)\n        return m ** 1\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_pow_2(self, xp, sp):\n        m = _make_square(xp, sp, self.dtype)\n        return m ** 2\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_pow_3(self, xp, sp):\n        m = _make_square(xp, sp, self.dtype)\n        return m ** 3\n\n    def test_pow_neg(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make_square(xp, sp, self.dtype)\n            with pytest.raises(ValueError):\n                m ** -1\n\n    def test_sum_tuple_axis(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m.sum(axis=(0, 1))\n\n    def test_sum_float_axis(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m.sum(axis=0.0)\n\n    def test_sum_too_large_axis(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            with pytest.raises(ValueError):\n                m.sum(axis=3)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_transpose(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.transpose()\n\n    def test_transpose_axes_int(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            with pytest.raises(ValueError):\n                m.transpose(axes=0)\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_eliminate_zeros(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        m.eliminate_zeros()\n        return m.nnz\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64],\n    'ret_dtype': [None, numpy.float32, numpy.float64],\n    'axis': [None, 0, 1, -1, -2],\n}))\n@unittest.skipUnless(scipy_available, 'requires scipy')\nclass TestCooMatrixSum(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sum(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m.sum(axis=self.axis, dtype=self.ret_dtype)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sum_with_out(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        if self.axis is None:\n            shape = ()\n        else:\n            shape = list(m.shape)\n            shape[self.axis] = 1\n            shape = tuple(shape)\n        out = xp.empty(shape, dtype=self.ret_dtype)\n        if xp is numpy:\n            # TODO(unno): numpy.matrix is used for scipy.sparse though\n            # cupy.ndarray is used for cupyx.scipy.sparse.\n            out = xp.asmatrix(out)\n        return m.sum(axis=self.axis, dtype=self.ret_dtype, out=out)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@unittest.skipUnless(scipy_available, 'requires scipy')\nclass TestCooMatrixSumDuplicates(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sum_duplicates(self, xp, sp):\n        m = _make_duplicate(xp, sp, self.dtype)\n        self.assertFalse(m.has_canonical_format)\n        m.sum_duplicates()\n        self.assertTrue(m.has_canonical_format)\n        self.assertEqual(m.nnz, 3)\n\n        m.sum_duplicates()\n        self.assertTrue(m.has_canonical_format)\n        return m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sum_duplicates_canonical(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        self.assertFalse(m.has_canonical_format)\n        m.sum_duplicates()\n        self.assertTrue(m.has_canonical_format)\n        self.assertEqual(m.nnz, 4)\n        return m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sum_duplicates_empty(self, xp, sp):\n        m = _make_empty(xp, sp, self.dtype)\n        self.assertFalse(m.has_canonical_format)\n        m.sum_duplicates()\n        self.assertTrue(m.has_canonical_format)\n        self.assertEqual(m.nnz, 0)\n        return m\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n    'ufunc': [\n        'arcsin', 'arcsinh', 'arctan', 'arctanh', 'ceil', 'deg2rad', 'expm1',\n        'floor', 'log1p', 'rad2deg', 'rint', 'sign', 'sin', 'sinh', 'sqrt',\n        'tan', 'tanh', 'trunc',\n    ],\n}))\n@unittest.skipUnless(scipy_available, 'requires scipy')\nclass TestUfunc(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name='sp', atol=1e-5)\n    def test_ufun(self, xp, sp):\n        x = _make(xp, sp, self.dtype)\n        x.data *= 0.1\n        func = getattr(x, self.ufunc)\n        complex_unsupported = {'ceil', 'deg2rad', 'floor', 'rad2deg', 'trunc'}\n        if (numpy.dtype(self.dtype).kind == 'c' and\n                self.ufunc in complex_unsupported):\n            with self.assertRaises(TypeError):\n                func()\n            return numpy.array(0)\n        else:\n            return func()\n\n\nclass TestIsspmatrixCoo(unittest.TestCase):\n\n    def test_coo(self):\n        x = sparse.coo_matrix(\n            (cupy.array([0], 'f'),\n             (cupy.array([0], 'i'), cupy.array([0], 'i'))),\n            shape=(1, 1), dtype='f')\n        self.assertTrue(sparse.isspmatrix_coo(x))\n\n    def test_csr(self):\n        x = sparse.csr_matrix(\n            (cupy.array([], 'f'),\n             cupy.array([], 'i'),\n             cupy.array([0], 'i')),\n            shape=(0, 0), dtype='f')\n        self.assertFalse(sparse.isspmatrix_coo(x))\n"""
tests/cupyx_tests/scipy_tests/sparse_tests/test_csc.py,0,"b""import pickle\nimport unittest\n\nimport numpy\nimport pytest\ntry:\n    import scipy.sparse\n    scipy_available = True\nexcept ImportError:\n    scipy_available = False\n\nimport cupy\nfrom cupy import testing\nfrom cupyx.scipy import sparse\n\n\ndef _make(xp, sp, dtype):\n    data = xp.array([0, 1, 3, 2], dtype)\n    indices = xp.array([0, 0, 2, 1], 'i')\n    indptr = xp.array([0, 1, 2, 3, 4], 'i')\n    # 0, 1, 0, 0\n    # 0, 0, 0, 2\n    # 0, 0, 3, 0\n    return sp.csc_matrix((data, indices, indptr), shape=(3, 4))\n\n\ndef _make_complex(xp, sp, dtype):\n    data = xp.array([0, 1, 2, 3], dtype)\n    if dtype in [numpy.complex64, numpy.complex128]:\n        data = data - 1j\n    indices = xp.array([0, 1, 3, 2], 'i')\n    indptr = xp.array([0, 2, 3, 4], 'i')\n    # 0, 1 - 1j, 0, 0\n    # 0, 0, 0, 2 - 1j\n    # 0, 0, 3 - 1j, 0\n    return sp.csr_matrix((data, indices, indptr), shape=(3, 4))\n\n\ndef _make2(xp, sp, dtype):\n    data = xp.array([2, 1, 3, 4], dtype)\n    indices = xp.array([1, 0, 1, 2], 'i')\n    indptr = xp.array([0, 0, 1, 4, 4], 'i')\n    # 0, 0, 1, 0\n    # 0, 2, 3, 0\n    # 0, 0, 4, 0\n    return sp.csc_matrix((data, indices, indptr), shape=(3, 4))\n\n\ndef _make3(xp, sp, dtype):\n    data = xp.array([1, 4, 3, 2, 5], dtype)\n    indices = xp.array([0, 3, 1, 1, 3], 'i')\n    indptr = xp.array([0, 2, 3, 5], 'i')\n    # 1, 0, 0\n    # 0, 3, 2\n    # 0, 0, 0\n    # 4, 0, 5\n    return sp.csc_matrix((data, indices, indptr), shape=(4, 3))\n\n\ndef _make_unordered(xp, sp, dtype):\n    data = xp.array([1, 2, 3, 4], dtype)\n    indices = xp.array([1, 0, 1, 2], 'i')\n    indptr = xp.array([0, 0, 0, 2, 4], 'i')\n    return sp.csc_matrix((data, indices, indptr), shape=(3, 4))\n\n\ndef _make_duplicate(xp, sp, dtype):\n    data = xp.array([1, 4, 3, 0, 2, 5], dtype)\n    indices = xp.array([0, 1, 0, 2, 1, 1], 'i')\n    indptr = xp.array([0, 3, 4, 6, 6], 'i')\n    # 4, 0, 0, 0\n    # 4, 0, 7, 0\n    # 0, 0, 0, 0\n    return sp.csc_matrix((data, indices, indptr), shape=(3, 4))\n\n\ndef _make_empty(xp, sp, dtype):\n    data = xp.array([], dtype)\n    indices = xp.array([], 'i')\n    indptr = xp.array([0, 0, 0, 0, 0], 'i')\n    return sp.csc_matrix((data, indices, indptr), shape=(3, 4))\n\n\ndef _make_shape(xp, sp, dtype):\n    return sp.csc_matrix((3, 4))\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\nclass TestCscMatrix(unittest.TestCase):\n\n    def setUp(self):\n        self.m = _make(cupy, sparse, self.dtype)\n\n    def test_dtype(self):\n        self.assertEqual(self.m.dtype, self.dtype)\n\n    def test_data(self):\n        self.assertEqual(self.m.data.dtype, self.dtype)\n        testing.assert_array_equal(\n            self.m.data, cupy.array([0, 1, 3, 2], self.dtype))\n\n    def test_indices(self):\n        self.assertEqual(self.m.indices.dtype, numpy.int32)\n        testing.assert_array_equal(\n            self.m.indices, cupy.array([0, 0, 2, 1], self.dtype))\n\n    def test_indptr(self):\n        self.assertEqual(self.m.indptr.dtype, numpy.int32)\n        testing.assert_array_equal(\n            self.m.indptr, cupy.array([0, 1, 2, 3, 4], self.dtype))\n\n    def test_init_copy(self):\n        n = sparse.csc_matrix(self.m)\n        self.assertIsNot(n, self.m)\n        cupy.testing.assert_array_equal(n.data, self.m.data)\n        cupy.testing.assert_array_equal(n.indices, self.m.indices)\n        cupy.testing.assert_array_equal(n.indptr, self.m.indptr)\n        self.assertEqual(n.shape, self.m.shape)\n\n    def test_init_copy_other_sparse(self):\n        n = sparse.csc_matrix(self.m.tocsr())\n        cupy.testing.assert_array_equal(n.data, self.m.data)\n        cupy.testing.assert_array_equal(n.indices, self.m.indices)\n        cupy.testing.assert_array_equal(n.indptr, self.m.indptr)\n        self.assertEqual(n.shape, self.m.shape)\n\n    @testing.with_requires('scipy')\n    def test_init_copy_scipy_sparse(self):\n        m = _make(numpy, scipy.sparse, self.dtype)\n        n = sparse.csc_matrix(m)\n        self.assertIsInstance(n.data, cupy.ndarray)\n        self.assertIsInstance(n.indices, cupy.ndarray)\n        self.assertIsInstance(n.indptr, cupy.ndarray)\n        cupy.testing.assert_array_equal(n.data, m.data)\n        cupy.testing.assert_array_equal(n.indices, m.indices)\n        cupy.testing.assert_array_equal(n.indptr, m.indptr)\n        self.assertEqual(n.shape, m.shape)\n\n    @testing.with_requires('scipy')\n    def test_init_copy_other_scipy_sparse(self):\n        m = _make(numpy, scipy.sparse, self.dtype)\n        n = sparse.csc_matrix(m.tocsr())\n        self.assertIsInstance(n.data, cupy.ndarray)\n        self.assertIsInstance(n.indices, cupy.ndarray)\n        self.assertIsInstance(n.indptr, cupy.ndarray)\n        cupy.testing.assert_array_equal(n.data, m.data)\n        cupy.testing.assert_array_equal(n.indices, m.indices)\n        cupy.testing.assert_array_equal(n.indptr, m.indptr)\n        self.assertEqual(n.shape, m.shape)\n\n    def test_init_dense(self):\n        m = cupy.array([[0, 1, 0, 2],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 3]], dtype=self.dtype)\n        n = sparse.csc_matrix(m)\n        self.assertEqual(n.nnz, 3)\n        self.assertEqual(n.shape, (3, 4))\n        cupy.testing.assert_array_equal(n.data, [1, 2, 3])\n        cupy.testing.assert_array_equal(n.indices, [0, 0, 2])\n        cupy.testing.assert_array_equal(n.indptr, [0, 0, 1, 1, 3])\n\n    def test_init_dense_empty(self):\n        m = cupy.array([[0, 0, 0, 0],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0]], dtype=self.dtype)\n        n = sparse.csc_matrix(m)\n        self.assertEqual(n.nnz, 0)\n        self.assertEqual(n.shape, (3, 4))\n        cupy.testing.assert_array_equal(n.data, [])\n        cupy.testing.assert_array_equal(n.indices, [])\n        cupy.testing.assert_array_equal(n.indptr, [0, 0, 0, 0, 0])\n\n    def test_init_dense_one_dim(self):\n        m = cupy.array([0, 1, 0, 2], dtype=self.dtype)\n        n = sparse.csc_matrix(m)\n        self.assertEqual(n.nnz, 2)\n        self.assertEqual(n.shape, (1, 4))\n        cupy.testing.assert_array_equal(n.data, [1, 2])\n        cupy.testing.assert_array_equal(n.indices, [0, 0])\n        cupy.testing.assert_array_equal(n.indptr, [0, 0, 1, 1, 2])\n\n    def test_init_dense_zero_dim(self):\n        m = cupy.array(1, dtype=self.dtype)\n        n = sparse.csc_matrix(m)\n        self.assertEqual(n.nnz, 1)\n        self.assertEqual(n.shape, (1, 1))\n        cupy.testing.assert_array_equal(n.data, [1])\n        cupy.testing.assert_array_equal(n.indices, [0])\n        cupy.testing.assert_array_equal(n.indptr, [0, 1])\n\n    @testing.with_requires('scipy')\n    def test_init_dense_invalid_ndim(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(TypeError):\n                m = xp.zeros((1, 1, 1), dtype=self.dtype)\n                sp.csc_matrix(m)\n\n    def test_copy(self):\n        n = self.m.copy()\n        self.assertIsInstance(n, sparse.csc_matrix)\n        self.assertIsNot(n, self.m)\n        self.assertIsNot(n.data, self.m.data)\n        self.assertIsNot(n.indices, self.m.indices)\n        self.assertIsNot(n.indptr, self.m.indptr)\n        cupy.testing.assert_array_equal(n.data, self.m.data)\n        cupy.testing.assert_array_equal(n.indices, self.m.indices)\n        cupy.testing.assert_array_equal(n.indptr, self.m.indptr)\n        self.assertEqual(n.shape, self.m.shape)\n\n    def test_shape(self):\n        self.assertEqual(self.m.shape, (3, 4))\n\n    def test_ndim(self):\n        self.assertEqual(self.m.ndim, 2)\n\n    def test_nnz(self):\n        self.assertEqual(self.m.nnz, 4)\n\n    def test_conj(self):\n        n = _make_complex(cupy, sparse, self.dtype)\n        cupy.testing.assert_array_equal(n.conj().data, n.data.conj())\n\n    @testing.with_requires('scipy')\n    def test_get(self):\n        m = self.m.get()\n        self.assertIsInstance(m, scipy.sparse.csc_matrix)\n        expect = [\n            [0, 1, 0, 0],\n            [0, 0, 0, 2],\n            [0, 0, 3, 0]\n        ]\n        numpy.testing.assert_allclose(m.toarray(), expect)\n\n    @testing.with_requires('scipy')\n    def test_str(self):\n        if numpy.dtype(self.dtype).kind == 'f':\n            expect = '''  (0, 0)\\t0.0\n  (0, 1)\\t1.0\n  (2, 2)\\t3.0\n  (1, 3)\\t2.0'''\n        elif numpy.dtype(self.dtype).kind == 'c':\n            expect = '''  (0, 0)\\t0j\n  (0, 1)\\t(1+0j)\n  (2, 2)\\t(3+0j)\n  (1, 3)\\t(2+0j)'''\n\n        self.assertEqual(str(self.m), expect)\n\n    def test_toarray(self):\n        m = self.m.toarray()\n        expect = [\n            [0, 1, 0, 0],\n            [0, 0, 0, 2],\n            [0, 0, 3, 0]\n        ]\n        self.assertTrue(m.flags.c_contiguous)\n        cupy.testing.assert_allclose(m, expect)\n\n    def test_pickle_roundtrip(self):\n        s = _make(cupy, sparse, self.dtype)\n        s2 = pickle.loads(pickle.dumps(s))\n        assert s._descr.descriptor != s2._descr.descriptor\n        assert s.shape == s2.shape\n        assert s.dtype == s2.dtype\n        if scipy_available:\n            assert (s.get() != s2.get()).count_nonzero() == 0\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@testing.with_requires('scipy')\nclass TestCscMatrixInit(unittest.TestCase):\n\n    def setUp(self):\n        self.shape = (3, 4)\n\n    def data(self, xp):\n        return xp.array([1, 2, 3, 4], self.dtype)\n\n    def indices(self, xp):\n        return xp.array([0, 0, 2, 1], 'i')\n\n    def indptr(self, xp):\n        return xp.array([0, 1, 2, 3, 4], 'i')\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_shape_none(self, xp, sp):\n        x = sp.csc_matrix(\n            (self.data(xp), self.indices(xp), self.indptr(xp)), shape=None)\n        self.assertEqual(x.shape, (3, 4))\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_dtype(self, xp, sp):\n        data = self.data(xp).real.astype('i')\n        x = sp.csc_matrix(\n            (data, self.indices(xp), self.indptr(xp)), dtype=self.dtype)\n        self.assertEqual(x.dtype, self.dtype)\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_copy_true(self, xp, sp):\n        data = self.data(xp)\n        indices = self.indices(xp)\n        indptr = self.indptr(xp)\n        x = sp.csc_matrix((data, indices, indptr), copy=True)\n\n        self.assertIsNot(data, x.data)\n        self.assertIsNot(indices, x.indices)\n        self.assertIsNot(indptr, x.indptr)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_init_with_shape(self, xp, sp):\n        s = sp.csc_matrix(self.shape)\n        self.assertEqual(s.shape, self.shape)\n        self.assertEqual(s.dtype, 'd')\n        self.assertEqual(s.size, 0)\n        return s\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_init_with_shape_and_dtype(self, xp, sp):\n        s = sp.csc_matrix(self.shape, dtype=self.dtype)\n        self.assertEqual(s.shape, self.shape)\n        self.assertEqual(s.dtype, self.dtype)\n        self.assertEqual(s.size, 0)\n        return s\n\n    @testing.numpy_cupy_allclose(sp_name='sp', atol=1e-5)\n    def test_intlike_shape(self, xp, sp):\n        s = sp.csc_matrix((self.data(xp), self.indices(xp), self.indptr(xp)),\n                          shape=(xp.array(self.shape[0]),\n                                 xp.int32(self.shape[1])))\n        assert isinstance(s.shape[0], int)\n        assert isinstance(s.shape[1], int)\n        return s\n\n    def test_shape_invalid(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.csc_matrix(\n                    (self.data(xp), self.indices(xp), self.indptr(xp)),\n                    shape=(2,))\n\n    def test_data_invalid(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.csc_matrix(\n                    ('invalid', self.indices(xp), self.indptr(xp)),\n                    shape=self.shape)\n\n    def test_data_invalid_ndim(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.csc_matrix(\n                    (self.data(xp)[None], self.indices(xp),\n                     self.indptr(xp)),\n                    shape=self.shape)\n\n    def test_indices_invalid(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.csc_matrix(\n                    (self.data(xp), 'invalid', self.indptr(xp)),\n                    shape=self.shape)\n\n    def test_indices_invalid_ndim(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.csc_matrix(\n                    (self.data(xp), self.indices(xp)[None], self.indptr(xp)),\n                    shape=self.shape)\n\n    def test_indptr_invalid(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.csc_matrix(\n                    (self.data(xp), self.indices(xp), 'invalid'),\n                    shape=self.shape)\n\n    def test_indptr_invalid_ndim(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.csc_matrix(\n                    (self.data(xp), self.indices(xp), self.indptr(xp)[None]),\n                    shape=self.shape)\n\n    def test_data_indices_different_length(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            data = xp.arange(5, dtype=self.dtype)\n            with pytest.raises(ValueError):\n                sp.csc_matrix(\n                    (data, self.indices(xp), self.indptr(xp)),\n                    shape=self.shape)\n\n    def test_indptr_invalid_length(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            indptr = xp.array([0, 1], 'i')\n            with pytest.raises(ValueError):\n                sp.csc_matrix(\n                    (self.data(xp), self.indices(xp), indptr),\n                    shape=self.shape)\n\n    def test_unsupported_dtype(self):\n        with self.assertRaises(ValueError):\n            sparse.csc_matrix(\n                (self.data(cupy), self.indices(cupy), self.indptr(cupy)),\n                shape=self.shape, dtype='i')\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_conj(self, xp, sp):\n        n = _make_complex(xp, sp, self.dtype)\n        cupy.testing.assert_array_equal(n.conj().data, n.data.conj())\n\n\n@testing.parameterize(*testing.product({\n    'make_method': [\n        '_make', '_make_unordered', '_make_empty', '_make_duplicate',\n        '_make_shape'],\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@testing.with_requires('scipy')\nclass TestCscMatrixScipyComparison(unittest.TestCase):\n\n    @property\n    def make(self):\n        return globals()[self.make_method]\n\n    def test_len(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                len(m)\n\n    @testing.numpy_cupy_array_equal(sp_name='sp')\n    def test_asfptype(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.asfptype()\n\n    @testing.numpy_cupy_allclose(sp_name='sp', contiguous_check=False)\n    def test_toarray(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        a = m.toarray()\n        if sp is sparse:\n            self.assertTrue(a.flags.c_contiguous)\n        return a\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_toarray_c_order(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        a = m.toarray(order='C')\n        self.assertTrue(a.flags.c_contiguous)\n        return a\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_toarray_f_order(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        a = m.toarray(order='F')\n        self.assertTrue(a.flags.f_contiguous)\n        return a\n\n    def test_toarray_unknown_order(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m.toarray(order='#')\n\n    @testing.numpy_cupy_allclose(sp_name='sp', contiguous_check=False)\n    def test_A(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.A\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocoo(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.tocoo()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocoo_copy(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = m.tocoo(copy=True)\n        self.assertIsNot(m.data, n.data)\n        return n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocsc(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.tocsc()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocsc_copy(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = m.tocsc(copy=True)\n        self.assertIsNot(m.data, n.data)\n        self.assertIsNot(m.indices, n.indices)\n        self.assertIsNot(m.indptr, n.indptr)\n        return n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocsr(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.tocsr()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocsr_copy(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = m.tocsr(copy=True)\n        self.assertIsNot(m.data, n.data)\n        self.assertIsNot(m.indices, n.indices)\n        self.assertIsNot(m.indptr, n.indptr)\n        return n\n\n    # dot\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_scalar(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.dot(2.0)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_numpy_scalar(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.dot(numpy.dtype(self.dtype).type(2.0))\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_csr(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype)\n        return m.dot(x)\n\n    def test_dot_csr_invalid_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            x = sp.csr_matrix((5, 3), dtype=self.dtype)\n            with pytest.raises(ValueError):\n                m.dot(x)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_csc(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocsc()\n        return m.dot(x)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_sparse(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocoo()\n        return m.dot(x)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_zero_dim(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = xp.array(2, dtype=self.dtype)\n        return m.dot(x)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_dense_vector(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = xp.arange(4).astype(self.dtype)\n        return m.dot(x)\n\n    def test_dot_dense_vector_invalid_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            x = xp.arange(5).astype(self.dtype)\n            with pytest.raises(ValueError):\n                m.dot(x)\n\n    @testing.numpy_cupy_allclose(sp_name='sp', contiguous_check=False)\n    def test_dot_dense_matrix(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = xp.arange(8).reshape(4, 2).astype(self.dtype)\n        return m.dot(x)\n\n    def test_dot_dense_matrix_invalid_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            x = xp.arange(10).reshape(5, 2).astype(self.dtype)\n            with pytest.raises(ValueError):\n                m.dot(x)\n\n    def test_dot_dense_ndim3(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            x = xp.arange(24).reshape(4, 2, 3).astype(self.dtype)\n            with pytest.raises(ValueError):\n                m.dot(x)\n\n    def test_dot_unsupported(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m.dot(None)\n\n    # __add__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_add_zero(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m + 0\n\n    def test_add_scalar(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(NotImplementedError):\n                m + 1\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_add_csr(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = _make2(xp, sp, self.dtype)\n        return m + n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_add_coo(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = _make2(xp, sp, self.dtype).tocoo()\n        return m + n\n\n    @testing.numpy_cupy_allclose(sp_name='sp', contiguous_check=False)\n    def test_add_dense(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = xp.arange(12).reshape(3, 4)\n        return m + n\n\n    # __radd__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_radd_zero(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return 0 + m\n\n    def test_radd_scalar(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(NotImplementedError):\n                1 + m\n\n    @testing.numpy_cupy_allclose(sp_name='sp', contiguous_check=False)\n    def test_radd_dense(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = xp.arange(12).reshape(3, 4)\n        return n + m\n\n    # __sub__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sub_zero(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m - 0\n\n    def test_sub_scalar(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(NotImplementedError):\n                m - 1\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sub_csr(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = _make2(xp, sp, self.dtype)\n        return m - n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sub_coo(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = _make2(xp, sp, self.dtype).tocoo()\n        return m - n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sub_dense(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = xp.arange(12).reshape(3, 4)\n        return m - n\n\n    # __rsub__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rsub_zero(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return 0 - m\n\n    def test_rsub_scalar(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(NotImplementedError):\n                1 - m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rsub_dense(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = xp.arange(12).reshape(3, 4)\n        return n - m\n\n    # __mul__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_scalar(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m * 2.0\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_numpy_scalar(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m * numpy.dtype(self.dtype).type(2.0)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_csr(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype)\n        return m * x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_csc(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocsc()\n        return m * x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_sparse(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocoo()\n        return m * x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_zero_dim(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = xp.array(2, dtype=self.dtype)\n        return m * x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_dense_vector(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = xp.arange(4).astype(self.dtype)\n        return m * x\n\n    @testing.numpy_cupy_allclose(sp_name='sp', contiguous_check=False)\n    def test_mul_dense_matrix(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = xp.arange(8).reshape(4, 2).astype(self.dtype)\n        return m * x\n\n    def test_mul_dense_ndim3(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            x = xp.arange(24).reshape(4, 2, 3).astype(self.dtype)\n            with pytest.raises(ValueError):\n                m * x\n\n    def test_mul_unsupported(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m * None\n\n    # __rmul__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_scalar(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return 2.0 * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_numpy_scalar(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return numpy.dtype(self.dtype).type(2.0) * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_csr(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype)\n        return x * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_csc(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocsc()\n        return x * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_sparse(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocoo()\n        return x * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_zero_dim(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = xp.array(2, dtype=self.dtype)\n        return x * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp', contiguous_check=False)\n    def test_rmul_dense_matrix(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = xp.arange(12).reshape(4, 3).astype(self.dtype)\n        return x * m\n\n    def test_rmul_dense_ndim3(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            x = xp.arange(24).reshape(4, 2, 3).astype(self.dtype)\n            with pytest.raises(ValueError):\n                x * m\n\n    def test_rmul_unsupported(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            # TODO(unno): When a sparse matrix has no element, scipy.sparse\n            # does not raise an error.\n            if m.nnz == 0:\n                continue\n            with pytest.raises(TypeError):\n                None * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sort_indices(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        m.sort_indices()\n        return m\n\n    def test_sum_tuple_axis(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m.sum(axis=(0, 1))\n\n    def test_sum_too_large_axis(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(ValueError):\n                m.sum(axis=3)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sum_duplicates(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        m.sum_duplicates()\n        self.assertTrue(m.has_canonical_format)\n        return m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_transpose(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.transpose()\n\n    def test_transpose_axes_int(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(ValueError):\n                m.transpose(axes=0)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_eliminate_zeros(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        m.eliminate_zeros()\n        return m\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    @unittest.skipIf(\n        cupy.cuda.runtime.runtimeGetVersion() < 8000,\n        'CUDA <8 cannot keep number of non-zero entries ')\n    def test_eliminate_zeros_nnz(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        m.eliminate_zeros()\n        return m.nnz\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64],\n    'ret_dtype': [None, numpy.float32, numpy.float64],\n    'axis': [None, 0, 1, -1, -2],\n}))\n@testing.with_requires('scipy')\nclass TestCscMatrixSum(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sum(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m.sum(axis=self.axis, dtype=self.ret_dtype)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sum_with_out(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        if self.axis is None:\n            shape = ()\n        else:\n            shape = list(m.shape)\n            shape[self.axis] = 1\n            shape = tuple(shape)\n        out = xp.empty(shape, dtype=self.ret_dtype)\n        if xp is numpy:\n            # TODO(unno): numpy.matrix is used for scipy.sparse though\n            # cupy.ndarray is used for cupyx.scipy.sparse.\n            out = xp.asmatrix(out)\n        return m.sum(axis=self.axis, dtype=self.ret_dtype, out=out)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@testing.with_requires('scipy')\nclass TestCscMatrixScipyCompressed(unittest.TestCase):\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_get_shape(self, xp, sp):\n        return _make(xp, sp, self.dtype).get_shape()\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_getnnz(self, xp, sp):\n        return _make(xp, sp, self.dtype).getnnz()\n\n\n@testing.with_requires('scipy>=0.19.0')\nclass TestCsrMatrixScipyCompressedMinMax(unittest.TestCase):\n\n    def test_min_sparse_axis_0(self):\n        dm_data = numpy.random.random((10, 20))\n        dm_data[dm_data < 0.95] = 0\n\n        dm_data = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.min(axis=0))\n        da_scipy_values = numpy.array(dm_data.min(axis=0).todense().ravel())\n        da_scipy_values = da_scipy_values[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_min_dense_axis_0(self):\n        dm_data = numpy.random.random((10, 20))\n\n        dm_data = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.min(axis=0))\n        da_scipy_values = numpy.array(dm_data.min(axis=0).todense().ravel())\n        da_scipy_values = da_scipy_values[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_min_axis_0_nonzero(self):\n        dm_data = numpy.arange(0, 100, 1).reshape((10, 10)).astype(float)\n\n        dm_sparse = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_sparse.data),\n                                       cupy.array(dm_sparse.indices),\n                                       cupy.array(dm_sparse.indptr)),\n                                      shape=(10, 10))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.min(axis=0, nonzero=True))\n        da_numpy_values = numpy.array([10, 1, 2, 3, 4,\n                                       5, 6, 7, 8, 9]).astype(float)\n        assert numpy.array_equal(da_cupy_values, da_numpy_values)\n\n    def test_min_sparse_axis_1(self):\n        dm_data = numpy.random.random((10, 20))\n        dm_data[dm_data < 0.95] = 0\n\n        dm_data = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.min(axis=1))\n        da_scipy_values = numpy.array(dm_data.min(axis=1).todense().ravel())\n        da_scipy_values = da_scipy_values[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_min_dense_axis_1(self):\n        dm_data = numpy.random.random((10, 20))\n\n        dm_data = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.min(axis=1))\n        da_scipy_values = numpy.array(dm_data.min(axis=1).todense().ravel())\n        da_scipy_values = da_scipy_values[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_min_axis_1_nonzero(self):\n        dm_data = numpy.arange(0, 100, 1).reshape((10, 10)).astype(float)\n\n        dm_sparse = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_sparse.data),\n                                       cupy.array(dm_sparse.indices),\n                                       cupy.array(dm_sparse.indptr)),\n                                      shape=(10, 10))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.min(axis=1, nonzero=True))\n        da_numpy_values = numpy.array([1, 10, 20, 30, 40,\n                                       50, 60, 70, 80, 90]).astype(float)\n        assert numpy.array_equal(da_cupy_values, da_numpy_values)\n\n    def test_max_sparse_axis_0(self):\n        dm_data = numpy.random.random((10, 20))\n        dm_data[dm_data < 0.95] = 0\n\n        dm_data = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.max(axis=0))\n        da_scipy_values = numpy.array(dm_data.max(axis=0).todense().ravel())\n        da_scipy_values = da_scipy_values[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_max_dense_axis_0(self):\n        dm_data = numpy.random.random((10, 20))\n\n        dm_data = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.max(axis=0))\n        da_scipy_values = numpy.array(dm_data.max(axis=0).todense().ravel())\n        da_scipy_values = da_scipy_values[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_max_axis_0_nonzero(self):\n        dm_data = numpy.arange(0, 100, 1).reshape((10, 10)).astype(float)\n\n        dm_sparse = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_sparse.data),\n                                       cupy.array(dm_sparse.indices),\n                                       cupy.array(dm_sparse.indptr)),\n                                      shape=(10, 10))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.max(axis=0, nonzero=True))\n        da_numpy_values = numpy.array([90, 91, 92, 93, 94,\n                                       95, 96, 97, 98, 99]).astype(float)\n        assert numpy.array_equal(da_cupy_values, da_numpy_values)\n\n    def test_max_sparse_axis_1(self):\n        dm_data = numpy.random.random((10, 20))\n        dm_data[dm_data < 0.95] = 0\n\n        dm_data = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.max(axis=1))\n        da_scipy_values = numpy.array(dm_data.max(axis=1).todense().ravel())\n        da_scipy_values = da_scipy_values[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_max_dense_axis_1(self):\n        dm_data = numpy.random.random((10, 20))\n\n        dm_data = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.max(axis=1))\n        da_scipy_values = numpy.array(dm_data.max(axis=1).todense().ravel())\n        da_scipy_values = da_scipy_values[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_max_axis_1_nonzero(self):\n        dm_data = numpy.arange(0, 100, 1).reshape((10, 10)).astype(float)\n\n        dm_sparse = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_sparse.data),\n                                       cupy.array(dm_sparse.indices),\n                                       cupy.array(dm_sparse.indptr)),\n                                      shape=(10, 10))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.max(axis=1, nonzero=True))\n        da_numpy_values = numpy.array([9, 19, 29, 39, 49,\n                                       59, 69, 79, 89, 99]).astype(float)\n        assert numpy.array_equal(da_cupy_values, da_numpy_values)\n\n    def test_argmin_sparse_axis_0(self):\n        dm_data = numpy.random.random((10, 20))\n        dm_data[dm_data < 0.95] = 0\n\n        dm_data = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.argmin(axis=0))\n        da_scipy_values = numpy.array(dm_data.argmin(axis=0))[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_argmin_dense_axis_0(self):\n        dm_data = numpy.random.random((10, 20))\n\n        dm_data = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.argmin(axis=0))\n        da_scipy_values = numpy.array(dm_data.argmin(axis=0))[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_argmin_sparse_axis_1(self):\n        dm_data = numpy.random.random((10, 20))\n        dm_data[dm_data < 0.95] = 0\n\n        dm_data = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.argmin(axis=1))\n        da_scipy_values = numpy.array(dm_data.argmin(axis=1))[:, 0]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_argmin_dense_axis_1(self):\n        dm_data = numpy.random.random((10, 20))\n\n        dm_data = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.argmin(axis=1))\n        da_scipy_values = numpy.array(dm_data.argmin(axis=1))[:, 0]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_argmax_sparse_axis_0(self):\n        dm_data = numpy.random.random((10, 20))\n        dm_data[dm_data < 0.95] = 0\n\n        dm_data = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.argmax(axis=0))\n        da_scipy_values = numpy.array(dm_data.argmax(axis=0))[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_argmax_dense_axis_0(self):\n        dm_data = numpy.random.random((10, 20))\n\n        dm_data = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.argmax(axis=0))\n        da_scipy_values = numpy.array(dm_data.argmax(axis=0))[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_argmax_sparse_axis_1(self):\n        dm_data = numpy.random.random((10, 20))\n        dm_data[dm_data < 0.95] = 0\n\n        dm_data = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.argmax(axis=1))\n        da_scipy_values = numpy.array(dm_data.argmax(axis=1))[:, 0]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_argmax_dense_axis_1(self):\n        dm_data = numpy.random.random((10, 20))\n\n        dm_data = scipy.sparse.csc_matrix(dm_data)\n        cp_matrix = sparse.csc_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.argmax(axis=1))\n        da_scipy_values = numpy.array(dm_data.argmax(axis=1))[:, 0]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@testing.with_requires('scipy')\nclass TestCscMatrixData(unittest.TestCase):\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_dtype(self, xp, sp):\n        return _make(xp, sp, self.dtype).dtype\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_abs(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return abs(m)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_neg(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return (-m)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_astype(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        if numpy.dtype(self.dtype).kind == 'c':\n            t = 'D'\n        else:\n            t = 'd'\n        return m.astype(t)\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_count_nonzero(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m.count_nonzero()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_power(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m.power(2)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_power_with_dtype(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        if numpy.dtype(self.dtype).kind == 'c':\n            t = 'D'\n        else:\n            t = 'd'\n        return m.power(2, t)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n    'ufunc': [\n        'arcsin', 'arcsinh', 'arctan', 'arctanh', 'ceil', 'deg2rad', 'expm1',\n        'floor', 'log1p', 'rad2deg', 'rint', 'sign', 'sin', 'sinh', 'sqrt',\n        'tan', 'tanh', 'trunc',\n    ],\n}))\n@testing.with_requires('scipy')\nclass TestUfunc(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name='sp', atol=1e-5)\n    def test_ufun(self, xp, sp):\n        x = _make(xp, sp, self.dtype)\n        x.data *= 0.1\n        func = getattr(x, self.ufunc)\n        complex_unsupported = {'ceil', 'deg2rad', 'floor', 'rad2deg', 'trunc'}\n        if (numpy.dtype(self.dtype).kind == 'c' and\n                self.ufunc in complex_unsupported):\n            with self.assertRaises(TypeError):\n                func()\n            return numpy.array(0)\n        else:\n            return func()\n\n\nclass TestIsspmatrixCsc(unittest.TestCase):\n\n    def test_csr(self):\n        x = sparse.csr_matrix(\n            (cupy.array([], 'f'),\n             cupy.array([], 'i'),\n             cupy.array([0], 'i')),\n            shape=(0, 0), dtype='f')\n        self.assertFalse(sparse.isspmatrix_csc(x))\n\n    def test_csc(self):\n        x = sparse.csc_matrix(\n            (cupy.array([], 'f'),\n             cupy.array([], 'i'),\n             cupy.array([0], 'i')),\n            shape=(0, 0), dtype='f')\n        self.assertTrue(sparse.isspmatrix_csc(x))\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@testing.with_requires('scipy')\nclass TestCsrMatrixGetitem(unittest.TestCase):\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_getitem_int_int(self, xp, sp):\n        self.assertEqual(_make(xp, sp, self.dtype)[0, 1], 1)\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_getitem_int_int_not_found(self, xp, sp):\n        self.assertEqual(_make(xp, sp, self.dtype)[1, 1], 0)\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_getitem_int_int_negative(self, xp, sp):\n        self.assertEqual(_make(xp, sp, self.dtype)[-1, -2], 3)\n\n    def test_getitem_int_int_too_small_row(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(IndexError):\n                _make(xp, sp, self.dtype)[-4, 0]\n\n    def test_getitem_int_int_too_large_row(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(IndexError):\n                _make(xp, sp, self.dtype)[3, 0]\n\n    def test_getitem_int_int_too_small_col(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(IndexError):\n                _make(xp, sp, self.dtype)[0, -5]\n\n    def test_getitem_int_int_too_large_col(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(IndexError):\n                _make(xp, sp, self.dtype)[0, 4]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_int(self, xp, sp):\n        return _make(xp, sp, self.dtype)[:, 1]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_negative_int(self, xp, sp):\n        return _make(xp, sp, self.dtype)[:, -1]\n\n    def test_getitem_int_too_small(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(IndexError):\n                _make(xp, sp, self.dtype)[:, -5]\n\n    def test_getitem_int_too_large(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(IndexError):\n                _make(xp, sp, self.dtype)[:, 4]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_slice(self, xp, sp):\n        return _make(xp, sp, self.dtype)[:, 1:3]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_slice_negative(self, xp, sp):\n        return _make(xp, sp, self.dtype)[:, -2:-1]\n\n    # SciPy prior to 1.4 has bugs where either an IndexError is raised or a\n    # segfault occurs instead of returning an empty slice.\n    @testing.with_requires('scipy>=1.4')\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_slice_start_larger_than_stop(self, xp, sp):\n        return _make(xp, sp, self.dtype)[:, 3:2]\n\n    def test_getitem_slice_step_2(self):\n        with self.assertRaises(ValueError):\n            _make(cupy, sparse, self.dtype)[:, 0::2]\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@testing.with_requires('scipy>=1.0.0')\nclass TestCsrMatrixGetitem2(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_slice_start_too_small(self, xp, sp):\n        return _make(xp, sp, self.dtype)[:, -5:None]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_slice_start_too_large(self, xp, sp):\n        return _make(xp, sp, self.dtype)[:, 5:None]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_slice_stop_too_small(self, xp, sp):\n        return _make(xp, sp, self.dtype)[:, None:-5]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_slice_stop_too_large(self, xp, sp):\n        return _make(xp, sp, self.dtype)[:, None:5]\n"""
tests/cupyx_tests/scipy_tests/sparse_tests/test_csr.py,0,"b""import pickle\nimport unittest\n\nimport numpy\nimport pytest\ntry:\n    import scipy.sparse\n    scipy_available = True\nexcept ImportError:\n    scipy_available = False\n\nimport cupy\nfrom cupy import testing\nfrom cupyx.scipy import sparse\n\n\ndef _make(xp, sp, dtype):\n    data = xp.array([0, 1, 2, 3], dtype)\n    indices = xp.array([0, 1, 3, 2], 'i')\n    indptr = xp.array([0, 2, 3, 4], 'i')\n    # 0, 1, 0, 0\n    # 0, 0, 0, 2\n    # 0, 0, 3, 0\n    return sp.csr_matrix((data, indices, indptr), shape=(3, 4))\n\n\ndef _make_complex(xp, sp, dtype):\n    data = xp.array([0, 1, 2, 3], dtype)\n    if dtype in [numpy.complex64, numpy.complex128]:\n        data = data - 1j\n    indices = xp.array([0, 1, 3, 2], 'i')\n    indptr = xp.array([0, 2, 3, 4], 'i')\n    # 0, 1 - 1j, 0, 0\n    # 0, 0, 0, 2 - 1j\n    # 0, 0, 3 - 1j, 0\n    return sp.csr_matrix((data, indices, indptr), shape=(3, 4))\n\n\ndef _make2(xp, sp, dtype):\n    data = xp.array([1, 2, 3, 4], dtype)\n    indices = xp.array([2, 1, 2, 2], 'i')\n    indptr = xp.array([0, 1, 3, 4], 'i')\n    # 0, 0, 1, 0\n    # 0, 2, 3, 0\n    # 0, 0, 4, 0\n    return sp.csr_matrix((data, indices, indptr), shape=(3, 4))\n\n\ndef _make3(xp, sp, dtype):\n    data = xp.array([1, 2, 3, 4, 5], dtype)\n    indices = xp.array([0, 2, 1, 0, 2], 'i')\n    indptr = xp.array([0, 1, 3, 3, 5], 'i')\n    # 1, 0, 0\n    # 0, 3, 2\n    # 0, 0, 0\n    # 4, 0, 5\n    return sp.csr_matrix((data, indices, indptr), shape=(4, 3))\n\n\ndef _make_unordered(xp, sp, dtype):\n    data = xp.array([1, 2, 3, 4], dtype)\n    indices = xp.array([1, 0, 1, 2], 'i')\n    indptr = xp.array([0, 2, 3, 4], 'i')\n    # 2, 1, 0, 0\n    # 0, 3, 0, 0\n    # 0, 0, 4, 0\n    return sp.csr_matrix((data, indices, indptr), shape=(3, 4))\n\n\ndef _make_duplicate(xp, sp, dtype):\n    data = xp.array([0, 1, 3, 2, 4, 5], dtype)\n    indices = xp.array([0, 0, 0, 2, 0, 2], 'i')\n    indptr = xp.array([0, 3, 6, 6], 'i')\n    # 4, 0, 0, 0\n    # 4, 0, 7, 0\n    # 0, 0, 0, 0\n    return sp.csr_matrix((data, indices, indptr), shape=(3, 4))\n\n\ndef _make_empty(xp, sp, dtype):\n    data = xp.array([], dtype)\n    indices = xp.array([], 'i')\n    indptr = xp.array([0, 0, 0, 0], 'i')\n    return sp.csr_matrix((data, indices, indptr), shape=(3, 4))\n\n\ndef _make_square(xp, sp, dtype):\n    data = xp.array([0, 1, 2, 3], dtype)\n    indices = xp.array([0, 1, 0, 2], 'i')\n    indptr = xp.array([0, 2, 3, 4], 'i')\n    # 0, 1, 0\n    # 2, 0, 0\n    # 0, 0, 3\n    return sp.csr_matrix((data, indices, indptr), shape=(3, 3))\n\n\ndef _make_shape(xp, sp, dtype):\n    return sp.csr_matrix((3, 4))\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\nclass TestCsrMatrix(unittest.TestCase):\n\n    def setUp(self):\n        self.m = _make(cupy, sparse, self.dtype)\n\n    def test_dtype(self):\n        self.assertEqual(self.m.dtype, self.dtype)\n\n    def test_data(self):\n        self.assertEqual(self.m.data.dtype, self.dtype)\n        testing.assert_array_equal(\n            self.m.data, cupy.array([0, 1, 2, 3], self.dtype))\n\n    def test_indices(self):\n        self.assertEqual(self.m.indices.dtype, numpy.int32)\n        testing.assert_array_equal(\n            self.m.indices, cupy.array([0, 1, 3, 2], self.dtype))\n\n    def test_indptr(self):\n        self.assertEqual(self.m.indptr.dtype, numpy.int32)\n        testing.assert_array_equal(\n            self.m.indptr, cupy.array([0, 2, 3, 4], self.dtype))\n\n    def test_init_copy(self):\n        n = sparse.csr_matrix(self.m)\n        self.assertIsNot(n, self.m)\n        cupy.testing.assert_array_equal(n.data, self.m.data)\n        cupy.testing.assert_array_equal(n.indices, self.m.indices)\n        cupy.testing.assert_array_equal(n.indptr, self.m.indptr)\n        self.assertEqual(n.shape, self.m.shape)\n\n    def test_init_copy_other_sparse(self):\n        n = sparse.csr_matrix(self.m.tocsc())\n        cupy.testing.assert_array_equal(n.data, self.m.data)\n        cupy.testing.assert_array_equal(n.indices, self.m.indices)\n        cupy.testing.assert_array_equal(n.indptr, self.m.indptr)\n        self.assertEqual(n.shape, self.m.shape)\n\n    @testing.with_requires('scipy')\n    def test_init_copy_scipy_sparse(self):\n        m = _make(numpy, scipy.sparse, self.dtype)\n        n = sparse.csr_matrix(m)\n        self.assertIsInstance(n.data, cupy.ndarray)\n        self.assertIsInstance(n.indices, cupy.ndarray)\n        self.assertIsInstance(n.indptr, cupy.ndarray)\n        cupy.testing.assert_array_equal(n.data, m.data)\n        cupy.testing.assert_array_equal(n.indices, m.indices)\n        cupy.testing.assert_array_equal(n.indptr, m.indptr)\n        self.assertEqual(n.shape, m.shape)\n\n    @testing.with_requires('scipy')\n    def test_init_copy_other_scipy_sparse(self):\n        m = _make(numpy, scipy.sparse, self.dtype)\n        n = sparse.csr_matrix(m.tocsc())\n        self.assertIsInstance(n.data, cupy.ndarray)\n        self.assertIsInstance(n.indices, cupy.ndarray)\n        self.assertIsInstance(n.indptr, cupy.ndarray)\n        cupy.testing.assert_array_equal(n.data, m.data)\n        cupy.testing.assert_array_equal(n.indices, m.indices)\n        cupy.testing.assert_array_equal(n.indptr, m.indptr)\n        self.assertEqual(n.shape, m.shape)\n\n    def test_init_dense(self):\n        m = cupy.array([[0, 1, 0, 2],\n                        [0, 0, 0, 0],\n                        [0, 0, 3, 0]], dtype=self.dtype)\n        n = sparse.csr_matrix(m)\n        self.assertEqual(n.nnz, 3)\n        self.assertEqual(n.shape, (3, 4))\n        cupy.testing.assert_array_equal(n.data, [1, 2, 3])\n        cupy.testing.assert_array_equal(n.indices, [1, 3, 2])\n        cupy.testing.assert_array_equal(n.indptr, [0, 2, 2, 3])\n\n    def test_init_dense_empty(self):\n        m = cupy.array([[0, 0, 0, 0],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0]], dtype=self.dtype)\n        n = sparse.csr_matrix(m)\n        self.assertEqual(n.nnz, 0)\n        self.assertEqual(n.shape, (3, 4))\n        cupy.testing.assert_array_equal(n.data, [])\n        cupy.testing.assert_array_equal(n.indices, [])\n        cupy.testing.assert_array_equal(n.indptr, [0, 0, 0, 0])\n\n    def test_init_dense_one_dim(self):\n        m = cupy.array([0, 1, 0, 2], dtype=self.dtype)\n        n = sparse.csr_matrix(m)\n        self.assertEqual(n.nnz, 2)\n        self.assertEqual(n.shape, (1, 4))\n        cupy.testing.assert_array_equal(n.data, [1, 2])\n        cupy.testing.assert_array_equal(n.indices, [1, 3])\n        cupy.testing.assert_array_equal(n.indptr, [0, 2])\n\n    def test_init_dense_zero_dim(self):\n        m = cupy.array(1, dtype=self.dtype)\n        n = sparse.csr_matrix(m)\n        self.assertEqual(n.nnz, 1)\n        self.assertEqual(n.shape, (1, 1))\n        cupy.testing.assert_array_equal(n.data, [1])\n        cupy.testing.assert_array_equal(n.indices, [0])\n        cupy.testing.assert_array_equal(n.indptr, [0, 1])\n\n    @testing.with_requires('scipy')\n    def test_init_dense_invalid_ndim(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = xp.zeros((1, 1, 1), dtype=self.dtype)\n            with pytest.raises(TypeError):\n                sp.csr_matrix(m)\n\n    def test_copy(self):\n        n = self.m.copy()\n        self.assertIsInstance(n, sparse.csr_matrix)\n        self.assertIsNot(n, self.m)\n        self.assertIsNot(n.data, self.m.data)\n        self.assertIsNot(n.indices, self.m.indices)\n        self.assertIsNot(n.indptr, self.m.indptr)\n        cupy.testing.assert_array_equal(n.data, self.m.data)\n        cupy.testing.assert_array_equal(n.indices, self.m.indices)\n        cupy.testing.assert_array_equal(n.indptr, self.m.indptr)\n        self.assertEqual(n.shape, self.m.shape)\n\n    def test_shape(self):\n        self.assertEqual(self.m.shape, (3, 4))\n\n    def test_ndim(self):\n        self.assertEqual(self.m.ndim, 2)\n\n    def test_nnz(self):\n        self.assertEqual(self.m.nnz, 4)\n\n    def test_conj(self):\n        n = _make_complex(cupy, sparse, self.dtype)\n        cupy.testing.assert_array_equal(n.conj().data, n.data.conj())\n\n    @testing.with_requires('scipy')\n    def test_get(self):\n        m = self.m.get()\n        self.assertIsInstance(m, scipy.sparse.csr_matrix)\n        expect = [\n            [0, 1, 0, 0],\n            [0, 0, 0, 2],\n            [0, 0, 3, 0]\n        ]\n        numpy.testing.assert_allclose(m.toarray(), expect)\n\n    @testing.with_requires('scipy')\n    def test_str(self):\n        if numpy.dtype(self.dtype).kind == 'f':\n            expect = '''  (0, 0)\\t0.0\n  (0, 1)\\t1.0\n  (1, 3)\\t2.0\n  (2, 2)\\t3.0'''\n        elif numpy.dtype(self.dtype).kind == 'c':\n            expect = '''  (0, 0)\\t0j\n  (0, 1)\\t(1+0j)\n  (1, 3)\\t(2+0j)\n  (2, 2)\\t(3+0j)'''\n\n        self.assertEqual(str(self.m), expect)\n\n    def test_toarray(self):\n        m = self.m.toarray()\n        expect = [\n            [0, 1, 0, 0],\n            [0, 0, 0, 2],\n            [0, 0, 3, 0]\n        ]\n        self.assertTrue(m.flags.c_contiguous)\n        cupy.testing.assert_allclose(m, expect)\n\n    def test_pickle_roundtrip(self):\n        s = _make(cupy, sparse, self.dtype)\n        s2 = pickle.loads(pickle.dumps(s))\n        assert s._descr.descriptor != s2._descr.descriptor\n        assert s.shape == s2.shape\n        assert s.dtype == s2.dtype\n        if scipy_available:\n            assert (s.get() != s2.get()).count_nonzero() == 0\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@testing.with_requires('scipy')\nclass TestCsrMatrixInit(unittest.TestCase):\n\n    def setUp(self):\n        self.shape = (3, 4)\n\n    def data(self, xp):\n        return xp.array([1, 2, 3, 4], self.dtype)\n\n    def indices(self, xp):\n        return xp.array([0, 1, 3, 2], 'i')\n\n    def indptr(self, xp):\n        return xp.array([0, 2, 3, 4], 'i')\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_shape_none(self, xp, sp):\n        x = sp.csr_matrix(\n            (self.data(xp), self.indices(xp), self.indptr(xp)), shape=None)\n        self.assertEqual(x.shape, (3, 4))\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_dtype(self, xp, sp):\n        data = self.data(xp).real.astype('i')\n        x = sp.csr_matrix(\n            (data, self.indices(xp), self.indptr(xp)), dtype=self.dtype)\n        self.assertEqual(x.dtype, self.dtype)\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_copy_true(self, xp, sp):\n        data = self.data(xp)\n        indices = self.indices(xp)\n        indptr = self.indptr(xp)\n        x = sp.csr_matrix((data, indices, indptr), copy=True)\n\n        self.assertIsNot(data, x.data)\n        self.assertIsNot(indices, x.indices)\n        self.assertIsNot(indptr, x.indptr)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_init_with_shape(self, xp, sp):\n        s = sp.csr_matrix(self.shape)\n        self.assertEqual(s.shape, self.shape)\n        self.assertEqual(s.dtype, 'd')\n        self.assertEqual(s.size, 0)\n        return s\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_init_with_shape_and_dtype(self, xp, sp):\n        s = sp.csr_matrix(self.shape, dtype=self.dtype)\n        self.assertEqual(s.shape, self.shape)\n        self.assertEqual(s.dtype, self.dtype)\n        self.assertEqual(s.size, 0)\n        return s\n\n    @testing.numpy_cupy_allclose(sp_name='sp', atol=1e-5)\n    def test_intlike_shape(self, xp, sp):\n        s = sp.csr_matrix((self.data(xp), self.indices(xp), self.indptr(xp)),\n                          shape=(xp.array(self.shape[0]),\n                                 xp.int32(self.shape[1])))\n        assert isinstance(s.shape[0], int)\n        assert isinstance(s.shape[1], int)\n        return s\n\n    def test_shape_invalid(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.csr_matrix(\n                    (self.data(xp), self.indices(xp), self.indptr(xp)),\n                    shape=(2,))\n\n    def test_data_invalid(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.csr_matrix(\n                    ('invalid', self.indices(xp), self.indptr(xp)),\n                    shape=self.shape)\n\n    def test_data_invalid_ndim(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.csr_matrix(\n                    (self.data(xp)[None], self.indices(xp), self.indptr(xp)),\n                    shape=self.shape)\n\n    def test_indices_invalid(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.csr_matrix(\n                    (self.data(xp), 'invalid', self.indptr(xp)),\n                    shape=self.shape)\n\n    def test_indices_invalid_ndim(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.csr_matrix(\n                    (self.data(xp), self.indices(xp)[None], self.indptr(xp)),\n                    shape=self.shape)\n\n    def test_indptr_invalid(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.csr_matrix(\n                    (self.data(xp), self.indices(xp), 'invalid'),\n                    shape=self.shape)\n\n    def test_indptr_invalid_ndim(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.csr_matrix(\n                    (self.data(xp), self.indices(xp), self.indptr(xp)[None]),\n                    shape=self.shape)\n\n    def test_data_indices_different_length(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            data = xp.arange(5, dtype=self.dtype)\n            with pytest.raises(ValueError):\n                sp.csr_matrix(\n                    (data, self.indices(xp), self.indptr(xp)),\n                    shape=self.shape)\n\n    def test_indptr_invalid_length(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            indptr = xp.array([0, 1], 'i')\n            with pytest.raises(ValueError):\n                sp.csr_matrix(\n                    (self.data(xp), self.indices(xp), indptr),\n                    shape=self.shape)\n\n    def test_unsupported_dtype(self):\n        with self.assertRaises(ValueError):\n            sparse.csr_matrix(\n                (self.data(cupy), self.indices(cupy), self.indptr(cupy)),\n                shape=self.shape, dtype='i')\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_conj(self, xp, sp):\n        n = _make_complex(xp, sp, self.dtype)\n        cupy.testing.assert_array_equal(n.conj().data, n.data.conj())\n\n\n@testing.parameterize(*testing.product({\n    'make_method': [\n        '_make', '_make_unordered', '_make_empty', '_make_duplicate',\n        '_make_shape'],\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@testing.with_requires('scipy')\nclass TestCsrMatrixScipyComparison(unittest.TestCase):\n\n    @property\n    def make(self):\n        return globals()[self.make_method]\n\n    def test_len(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                len(m)\n\n    @testing.numpy_cupy_array_equal(sp_name='sp')\n    def test_iter(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        rows = []\n        for r in m:\n            rows.append(r)\n            self.assertIsInstance(r, sp.spmatrix)\n        self.assertEqual(len(rows), 3)\n        return xp.concatenate([r.toarray() for r in rows])\n\n    @testing.numpy_cupy_array_equal(sp_name='sp')\n    def test_asfptype(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.asfptype().toarray()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_toarray(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        a = m.toarray()\n        self.assertTrue(a.flags.c_contiguous)\n        return a\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_toarray_c_order(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        a = m.toarray(order='C')\n        self.assertTrue(a.flags.c_contiguous)\n        return a\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_toarray_f_order(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        a = m.toarray(order='F')\n        self.assertTrue(a.flags.f_contiguous)\n        return a\n\n    def test_toarray_unknown_order(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m.toarray(order='#')\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_A(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.A\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocoo(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.tocoo()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocoo_copy(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = m.tocoo(copy=True)\n        self.assertIsNot(m.data, n.data)\n        return n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocsc(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.tocsc()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocsc_copy(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = m.tocsc(copy=True)\n        self.assertIsNot(m.data, n.data)\n        self.assertIsNot(m.indices, n.indices)\n        self.assertIsNot(m.indptr, n.indptr)\n        return n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocsr(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.tocsr()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocsr_copy(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = m.tocsr(copy=True)\n        self.assertIsNot(m.data, n.data)\n        self.assertIsNot(m.indices, n.indices)\n        self.assertIsNot(m.indptr, n.indptr)\n        return n\n\n    # dot\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_scalar(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.dot(2.0)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_numpy_scalar(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.dot(numpy.dtype(self.dtype).type(2.0)).toarray()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_csr(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype)\n        return m.dot(x)\n\n    def test_dot_csr_invalid_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            x = sp.csr_matrix((5, 3), dtype=self.dtype)\n            with pytest.raises(ValueError):\n                m.dot(x)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_csc(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocsc()\n        return m.dot(x)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_sparse(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocoo()\n        return m.dot(x)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_zero_dim(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = xp.array(2, dtype=self.dtype)\n        return m.dot(x)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_dot_dense_vector(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = xp.arange(4).astype(self.dtype)\n        return m.dot(x)\n\n    def test_dot_dense_vector_invalid_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            x = xp.arange(5).astype(self.dtype)\n            with pytest.raises(ValueError):\n                m.dot(x)\n\n    @testing.numpy_cupy_allclose(sp_name='sp', contiguous_check=False)\n    def test_dot_dense_matrix(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = xp.arange(8).reshape(4, 2).astype(self.dtype)\n        return m.dot(x)\n\n    def test_dot_dense_matrix_invalid_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            x = xp.arange(10).reshape(5, 2).astype(self.dtype)\n            with pytest.raises(ValueError):\n                m.dot(x)\n\n    def test_dot_dense_ndim3(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            x = xp.arange(24).reshape(4, 2, 3).astype(self.dtype)\n            with pytest.raises(ValueError):\n                m.dot(x)\n\n    def test_dot_unsupported(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m.dot(None)\n\n    # __add__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_add_zero(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m + 0\n\n    def test_add_scalar(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(NotImplementedError):\n                m + 1\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_add_csr(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = _make2(xp, sp, self.dtype)\n        return m + n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_add_coo(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = _make2(xp, sp, self.dtype).tocoo()\n        return m + n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_add_dense(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = xp.arange(12).reshape(3, 4)\n        return m + n\n\n    # __radd__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_radd_zero(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return (0 + m).toarray()\n\n    def test_radd_scalar(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(NotImplementedError):\n                1 + m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_radd_dense(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = xp.arange(12).reshape(3, 4)\n        return n + m\n\n    # __sub__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sub_zero(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return (m - 0).toarray()\n\n    def test_sub_scalar(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(NotImplementedError):\n                m - 1\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sub_csr(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = _make2(xp, sp, self.dtype)\n        return (m - n).toarray()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sub_coo(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = _make2(xp, sp, self.dtype).tocoo()\n        return m - n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sub_dense(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = xp.arange(12).reshape(3, 4)\n        return m - n\n\n    # __rsub__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rsub_zero(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return 0 - m\n\n    def test_rsub_scalar(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(NotImplementedError):\n                1 - m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rsub_dense(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = xp.arange(12).reshape(3, 4)\n        return n - m\n\n    # __mul__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_scalar(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m * 2.0\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_numpy_scalar(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m * numpy.dtype(self.dtype).type(2.0)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_csr(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype)\n        return m * x\n\n    def test_mul_csr_invalid_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            x = sp.csr_matrix((5, 3), dtype=self.dtype)\n            with pytest.raises(ValueError):\n                m * x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_csc(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocsc()\n        return m * x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_sparse(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocoo()\n        return m * x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_zero_dim(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = xp.array(2, dtype=self.dtype)\n        return m * x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mul_dense_vector(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = xp.arange(4).astype(self.dtype)\n        return m * x\n\n    def test_mul_dense_vector_invalid_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            x = xp.arange(5).astype(self.dtype)\n            with pytest.raises(ValueError):\n                m * x\n\n    @testing.numpy_cupy_allclose(sp_name='sp', contiguous_check=False)\n    def test_mul_dense_matrix(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = xp.arange(8).reshape(4, 2).astype(self.dtype)\n        return m * x\n\n    def test_mul_dense_matrix_invalid_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            x = xp.arange(10).reshape(5, 2).astype(self.dtype)\n            with pytest.raises(ValueError):\n                m * x\n\n    def test_mul_dense_ndim3(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            x = xp.arange(24).reshape(4, 2, 3).astype(self.dtype)\n            with pytest.raises(ValueError):\n                m * x\n\n    def test_mul_unsupported(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m * None\n\n    # __rmul__\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_scalar(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return 2.0 * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_numpy_scalar(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return numpy.dtype(self.dtype).type(2.0) * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_csr(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype)\n        return x * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_csc(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocsc()\n        return x * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_sparse(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = _make3(xp, sp, self.dtype).tocoo()\n        return x * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_rmul_zero_dim(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = xp.array(2, dtype=self.dtype)\n        return x * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp', contiguous_check=False)\n    def test_rmul_dense_matrix(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        x = xp.arange(12).reshape(4, 3).astype(self.dtype)\n        return x * m\n\n    def test_rmul_dense_ndim3(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            x = xp.arange(24).reshape(4, 2, 3).astype(self.dtype)\n            with pytest.raises(ValueError):\n                x * m\n\n    def test_rmul_unsupported(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            if m.nnz == 0:\n                # When there is no element, a SciPy's sparse matrix does\n                # not raise an error when it is multiplied with None.\n                continue\n            with pytest.raises(TypeError):\n                None * m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sort_indices(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        m.sort_indices()\n        return m\n\n    def test_sum_tuple_axis(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m.sum(axis=(0, 1))\n\n    def test_sum_str_axis(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m.sum(axis='test')\n\n    def test_sum_too_large_axis(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(ValueError):\n                m.sum(axis=3)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sum_duplicates(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        m.sum_duplicates()\n        self.assertTrue(m.has_canonical_format)\n        return m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_transpose(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.transpose()\n\n    def test_transpose_axes_int(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(ValueError):\n                m.transpose(axes=0)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_eliminate_zeros(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        m.eliminate_zeros()\n        return m\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    @unittest.skipIf(\n        cupy.cuda.runtime.runtimeGetVersion() < 8000,\n        'CUDA <8 cannot keep number of non-zero entries ')\n    def test_eliminate_zeros_nnz(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        m.eliminate_zeros()\n        return m.nnz\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@testing.with_requires('scipy')\nclass TestCsrMatrixPowScipyComparison(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_pow_0(self, xp, sp):\n        m = _make_square(xp, sp, self.dtype)\n        return m ** 0\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_pow_1(self, xp, sp):\n        m = _make_square(xp, sp, self.dtype)\n        return m ** 1\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_pow_2(self, xp, sp):\n        m = _make_square(xp, sp, self.dtype)\n        return m ** 2\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_pow_3(self, xp, sp):\n        m = _make_square(xp, sp, self.dtype)\n        return m ** 3\n        return m ** 3\n\n    def test_pow_neg(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make_square(xp, sp, self.dtype)\n            with pytest.raises(ValueError):\n                m ** -1\n\n    def test_pow_not_square(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m ** 2\n\n    def test_pow_float(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make_square(xp, sp, self.dtype)\n            with pytest.raises(ValueError):\n                m ** 1.5\n\n    def test_pow_list(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make_square(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m ** []\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64],\n    'ret_dtype': [None, numpy.float32, numpy.float64],\n    'axis': [None, 0, 1, -1, -2],\n}))\n@testing.with_requires('scipy')\nclass TestCsrMatrixSum(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sum(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m.sum(axis=self.axis, dtype=self.ret_dtype)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sum_with_out(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        if self.axis is None:\n            shape = ()\n        else:\n            shape = list(m.shape)\n            shape[self.axis] = 1\n            shape = tuple(shape)\n        out = xp.empty(shape, dtype=self.ret_dtype)\n        if xp is numpy:\n            # TODO(unno): numpy.matrix is used for scipy.sparse though\n            # cupy.ndarray is used for cupyx.scipy.sparse.\n            out = xp.asmatrix(out)\n        return m.sum(axis=self.axis, dtype=self.ret_dtype, out=out)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mean(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m.mean(axis=self.axis, dtype=self.ret_dtype)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_mean_with_out(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        if self.axis is None:\n            shape = ()\n        else:\n            shape = list(m.shape)\n            shape[self.axis] = 1\n            shape = tuple(shape)\n        out = xp.empty(shape, dtype=self.ret_dtype)\n        if xp is numpy:\n            # TODO(unno): numpy.matrix is used for scipy.sparse though\n            # cupy.ndarray is used for cupyx.scipy.sparse.\n            out = xp.asmatrix(out)\n        return m.mean(axis=self.axis, dtype=self.ret_dtype, out=out)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@testing.with_requires('scipy')\nclass TestCsrMatrixScipyCompressed(unittest.TestCase):\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_get_shape(self, xp, sp):\n        return _make(xp, sp, self.dtype).get_shape()\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_getnnz(self, xp, sp):\n        return _make(xp, sp, self.dtype).getnnz()\n\n\n@testing.with_requires('scipy>=0.19.0')\nclass TestCsrMatrixScipyCompressedMinMax(unittest.TestCase):\n\n    def test_min_sparse_axis_0(self):\n        dm_data = numpy.random.random((10, 20))\n        dm_data[dm_data < 0.95] = 0\n\n        dm_data = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.min(axis=0))\n        da_scipy_values = numpy.array(dm_data.min(axis=0).todense().ravel())\n        da_scipy_values = da_scipy_values[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_min_dense_axis_0(self):\n        dm_data = numpy.random.random((10, 20))\n\n        dm_data = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.min(axis=0))\n        da_scipy_values = numpy.array(dm_data.min(axis=0).todense().ravel())\n        da_scipy_values = da_scipy_values[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_min_axis_0_nonzero(self):\n        dm_data = numpy.arange(0, 100, 1).reshape((10, 10)).astype(float)\n\n        dm_sparse = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_sparse.data),\n                                       cupy.array(dm_sparse.indices),\n                                       cupy.array(dm_sparse.indptr)),\n                                      shape=(10, 10))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.min(axis=0, nonzero=True))\n        da_numpy_values = numpy.array([10, 1, 2, 3, 4,\n                                       5, 6, 7, 8, 9]).astype(float)\n        assert numpy.array_equal(da_cupy_values, da_numpy_values)\n\n    def test_min_sparse_axis_1(self):\n        dm_data = numpy.random.random((10, 20))\n        dm_data[dm_data < 0.95] = 0\n\n        dm_data = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.min(axis=1))\n        da_scipy_values = numpy.array(dm_data.min(axis=1).todense().ravel())\n        da_scipy_values = da_scipy_values[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_min_dense_axis_1(self):\n        dm_data = numpy.random.random((10, 20))\n\n        dm_data = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.min(axis=1))\n        da_scipy_values = numpy.array(dm_data.min(axis=1).todense().ravel())\n        da_scipy_values = da_scipy_values[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_min_axis_1_nonzero(self):\n        dm_data = numpy.arange(0, 100, 1).reshape((10, 10)).astype(float)\n\n        dm_sparse = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_sparse.data),\n                                       cupy.array(dm_sparse.indices),\n                                       cupy.array(dm_sparse.indptr)),\n                                      shape=(10, 10))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.min(axis=1, nonzero=True))\n        da_numpy_values = numpy.array([1, 10, 20, 30, 40,\n                                       50, 60, 70, 80, 90]).astype(float)\n        assert numpy.array_equal(da_cupy_values, da_numpy_values)\n\n    def test_max_sparse_axis_0(self):\n        dm_data = numpy.random.random((10, 20))\n        dm_data[dm_data < 0.95] = 0\n\n        dm_data = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.max(axis=0))\n        da_scipy_values = numpy.array(dm_data.max(axis=0).todense().ravel())\n        da_scipy_values = da_scipy_values[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_max_dense_axis_0(self):\n        dm_data = numpy.random.random((10, 20))\n\n        dm_data = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.max(axis=0))\n        da_scipy_values = numpy.array(dm_data.max(axis=0).todense().ravel())\n        da_scipy_values = da_scipy_values[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_max_axis_0_nonzero(self):\n        dm_data = numpy.arange(0, 100, 1).reshape((10, 10)).astype(float)\n\n        dm_sparse = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_sparse.data),\n                                       cupy.array(dm_sparse.indices),\n                                       cupy.array(dm_sparse.indptr)),\n                                      shape=(10, 10))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.max(axis=0, nonzero=True))\n        da_numpy_values = numpy.array([90, 91, 92, 93, 94,\n                                       95, 96, 97, 98, 99]).astype(float)\n        assert numpy.array_equal(da_cupy_values, da_numpy_values)\n\n    def test_max_sparse_axis_1(self):\n        dm_data = numpy.random.random((10, 20))\n        dm_data[dm_data < 0.95] = 0\n\n        dm_data = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.max(axis=1))\n        da_scipy_values = numpy.array(dm_data.max(axis=1).todense().ravel())\n        da_scipy_values = da_scipy_values[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_max_dense_axis_1(self):\n        dm_data = numpy.random.random((10, 20))\n\n        dm_data = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.max(axis=1))\n        da_scipy_values = numpy.array(dm_data.max(axis=1).todense().ravel())\n        da_scipy_values = da_scipy_values[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_max_axis_1_nonzero(self):\n        dm_data = numpy.arange(0, 100, 1).reshape((10, 10)).astype(float)\n\n        dm_sparse = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_sparse.data),\n                                       cupy.array(dm_sparse.indices),\n                                       cupy.array(dm_sparse.indptr)),\n                                      shape=(10, 10))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.max(axis=1, nonzero=True))\n        da_numpy_values = numpy.array([9, 19, 29, 39, 49,\n                                       59, 69, 79, 89, 99]).astype(float)\n        assert numpy.array_equal(da_cupy_values, da_numpy_values)\n\n    def test_argmin_sparse_axis_0(self):\n        dm_data = numpy.random.random((10, 20))\n        dm_data[dm_data < 0.95] = 0\n\n        dm_data = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.argmin(axis=0))\n        da_scipy_values = numpy.array(dm_data.argmin(axis=0))[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_argmin_dense_axis_0(self):\n        dm_data = numpy.random.random((10, 20))\n\n        dm_data = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.argmin(axis=0))\n        da_scipy_values = numpy.array(dm_data.argmin(axis=0))[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_argmin_sparse_axis_1(self):\n        dm_data = numpy.random.random((10, 20))\n        dm_data[dm_data < 0.95] = 0\n\n        dm_data = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.argmin(axis=1))\n        da_scipy_values = numpy.array(dm_data.argmin(axis=1))[:, 0]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_argmin_dense_axis_1(self):\n        dm_data = numpy.random.random((10, 20))\n\n        dm_data = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.argmin(axis=1))\n        da_scipy_values = numpy.array(dm_data.argmin(axis=1))[:, 0]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_argmax_sparse_axis_0(self):\n        dm_data = numpy.random.random((10, 20))\n        dm_data[dm_data < 0.95] = 0\n\n        dm_data = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.argmax(axis=0))\n        da_scipy_values = numpy.array(dm_data.argmax(axis=0))[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_argmax_dense_axis_0(self):\n        dm_data = numpy.random.random((10, 20))\n\n        dm_data = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.argmax(axis=0))\n        da_scipy_values = numpy.array(dm_data.argmax(axis=0))[0, :]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_argmax_sparse_axis_1(self):\n        dm_data = numpy.random.random((10, 20))\n        dm_data[dm_data < 0.95] = 0\n\n        dm_data = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.argmax(axis=1))\n        da_scipy_values = numpy.array(dm_data.argmax(axis=1))[:, 0]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n    def test_argmax_dense_axis_1(self):\n        dm_data = numpy.random.random((10, 20))\n\n        dm_data = scipy.sparse.csr_matrix(dm_data)\n        cp_matrix = sparse.csr_matrix((cupy.array(dm_data.data),\n                                       cupy.array(dm_data.indices),\n                                       cupy.array(dm_data.indptr)),\n                                      shape=(10, 20))\n\n        da_cupy_values = cupy.asnumpy(cp_matrix.argmax(axis=1))\n        da_scipy_values = numpy.array(dm_data.argmax(axis=1))[:, 0]\n        assert numpy.array_equal(da_cupy_values, da_scipy_values)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@testing.with_requires('scipy')\nclass TestCsrMatrixData(unittest.TestCase):\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_dtype(self, xp, sp):\n        return _make(xp, sp, self.dtype).dtype\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_abs(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return abs(m)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_neg(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return -m\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_astype(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        if numpy.dtype(self.dtype).kind == 'c':\n            t = 'D'\n        else:\n            t = 'd'\n        return m.astype(t)\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_count_nonzero(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m.count_nonzero()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_power(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m.power(2)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_power_with_dtype(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        if numpy.dtype(self.dtype).kind == 'c':\n            t = 'D'\n        else:\n            t = 'd'\n        return m.power(2, t)\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64],\n    'ufunc': [\n        'arcsin', 'arcsinh', 'arctan', 'arctanh', 'ceil', 'deg2rad', 'expm1',\n        'floor', 'log1p', 'rad2deg', 'rint', 'sign', 'sin', 'sinh', 'sqrt',\n        'tan', 'tanh', 'trunc',\n    ],\n}))\n@testing.with_requires('scipy')\nclass TestUfunc(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name='sp', atol=1e-5)\n    def test_ufun(self, xp, sp):\n        x = _make(xp, sp, self.dtype)\n        x.data *= 0.1\n        func = getattr(x, self.ufunc)\n        complex_unsupported = {'ceil', 'deg2rad', 'floor', 'rad2deg', 'trunc'}\n        if (numpy.dtype(self.dtype).kind == 'c' and\n                self.ufunc in complex_unsupported):\n            with self.assertRaises(TypeError):\n                func()\n            return numpy.array(0)\n        else:\n            return func()\n\n\nclass TestIsspmatrixCsr(unittest.TestCase):\n\n    def test_csr(self):\n        x = sparse.csr_matrix(\n            (cupy.array([], 'f'),\n             cupy.array([], 'i'),\n             cupy.array([0], 'i')),\n            shape=(0, 0), dtype='f')\n        self.assertTrue(sparse.isspmatrix_csr(x))\n\n    def test_csc(self):\n        x = sparse.csr_matrix(\n            (cupy.array([], 'f'),\n             cupy.array([], 'i'),\n             cupy.array([0], 'i')),\n            shape=(0, 0), dtype='f')\n        self.assertFalse(sparse.isspmatrix_csc(x))\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, cupy.complex64, cupy.complex128],\n}))\n@testing.with_requires('scipy')\nclass TestCsrMatrixGetitem(unittest.TestCase):\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_getitem_int_int(self, xp, sp):\n        self.assertEqual(_make(xp, sp, self.dtype)[0, 1], 1)\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_getitem_int_int_not_found(self, xp, sp):\n        self.assertEqual(_make(xp, sp, self.dtype)[1, 1], 0)\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_getitem_int_int_negative(self, xp, sp):\n        self.assertEqual(_make(xp, sp, self.dtype)[-1, -2], 3)\n\n    def test_getitem_int_int_too_small_row(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(IndexError):\n                _make(xp, sp, self.dtype)[-4, 0]\n\n    def test_getitem_int_int_too_large_row(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(IndexError):\n                _make(xp, sp, self.dtype)[3, 0]\n\n    def test_getitem_int_int_too_small_col(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(IndexError):\n                _make(xp, sp, self.dtype)[0, -5]\n\n    def test_getitem_int_int_too_large_col(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(IndexError):\n                _make(xp, sp, self.dtype)[0, 4]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_int(self, xp, sp):\n        return _make(xp, sp, self.dtype)[1]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_int_negative(self, xp, sp):\n        return _make(xp, sp, self.dtype)[-1]\n\n    def test_getitem_int_to_small(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(IndexError):\n                _make(xp, sp, self.dtype)[-4]\n\n    def test_getitem_int_to_large(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(IndexError):\n                _make(xp, sp, self.dtype)[3]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_int_none_slice(self, xp, sp):\n        return _make(xp, sp, self.dtype)[1, :]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_negative_int_none_slice(self, xp, sp):\n        return _make(xp, sp, self.dtype)[-1, :]\n\n    def test_getitem_int_too_small_none_slice(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(IndexError):\n                _make(xp, sp, self.dtype)[-4, :]\n\n    def test_getitem_int_too_large_none_slice(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(IndexError):\n                _make(xp, sp, self.dtype)[3, :]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_slice(self, xp, sp):\n        return _make(xp, sp, self.dtype)[1:3]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_slice_negative(self, xp, sp):\n        return _make(xp, sp, self.dtype)[-2:-1]\n\n    # SciPy prior to 1.4 has bugs where either an IndexError is raised or a\n    # segfault occurs instead of returning an empty slice.\n    @testing.with_requires('scipy>=1.4')\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_slice_start_larger_than_stop(self, xp, sp):\n        return _make(xp, sp, self.dtype)[3:2]\n\n    def test_getitem_slice_step_2(self):\n        with self.assertRaises(ValueError):\n            _make(cupy, sparse, self.dtype)[0::2]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_ellipsis(self, xp, sp):\n        return _make(xp, sp, self.dtype)[...]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_int_ellipsis(self, xp, sp):\n        return _make(xp, sp, self.dtype)[1, ...]\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, cupy.complex64, cupy.complex128],\n}))\n@testing.with_requires('scipy>=1.0.0')\nclass TestCsrMatrixGetitem2(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_slice_start_too_small(self, xp, sp):\n        return _make(xp, sp, self.dtype)[-4:None]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_slice_start_too_large(self, xp, sp):\n        return _make(xp, sp, self.dtype)[4:None]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_slice_stop_too_small(self, xp, sp):\n        return _make(xp, sp, self.dtype)[None:-4]\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_getitem_slice_stop_too_large(self, xp, sp):\n        return _make(xp, sp, self.dtype)[None:4]\n"""
tests/cupyx_tests/scipy_tests/sparse_tests/test_dia.py,0,"b""import pickle\nimport unittest\n\n\nimport numpy\nimport pytest\ntry:\n    import scipy.sparse  # NOQA\n    scipy_available = True\nexcept ImportError:\n    scipy_available = False\n\nimport cupy\nfrom cupy import testing\nfrom cupyx.scipy import sparse\n\n\ndef _make(xp, sp, dtype):\n    data = xp.array([[0, 1, 2], [3, 4, 5]], dtype)\n    offsets = xp.array([0, -1], 'i')\n    # 0, 0, 0, 0\n    # 3, 1, 0, 0\n    # 0, 4, 2, 0\n    return sp.dia_matrix((data, offsets), shape=(3, 4))\n\n\ndef _make_complex(xp, sp, dtype):\n    data = xp.array([[0, 1, 2], [3, 4, 5]], dtype)\n    if dtype in [numpy.complex64, numpy.complex128]:\n        data = data - 1j\n    offsets = xp.array([0, -1], 'i')\n    # 0, 0, 0, 0\n    # 3 - 1j, 1 - 1j, 0, 0\n    # 0, 4 - 1j, 2 - 1j, 0\n    return sp.dia_matrix((data, offsets), shape=(3, 4))\n\n\ndef _make_empty(xp, sp, dtype):\n    data = xp.array([[]], 'f')\n    offsets = xp.array([0], 'i')\n    return sp.dia_matrix((data, offsets), shape=(3, 4))\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\nclass TestDiaMatrix(unittest.TestCase):\n\n    def setUp(self):\n        self.m = _make(cupy, sparse, self.dtype)\n\n    def test_dtype(self):\n        self.assertEqual(self.m.dtype, self.dtype)\n\n    def test_data(self):\n        self.assertEqual(self.m.data.dtype, self.dtype)\n        testing.assert_array_equal(\n            self.m.data, cupy.array([[0, 1, 2], [3, 4, 5]], self.dtype))\n\n    def test_offsets(self):\n        self.assertEqual(self.m.offsets.dtype, numpy.int32)\n        testing.assert_array_equal(\n            self.m.offsets, cupy.array([0, -1], self.dtype))\n\n    def test_shape(self):\n        self.assertEqual(self.m.shape, (3, 4))\n\n    def test_ndim(self):\n        self.assertEqual(self.m.ndim, 2)\n\n    def test_nnz(self):\n        self.assertEqual(self.m.nnz, 5)\n\n    def test_conj(self):\n        n = _make_complex(cupy, sparse, self.dtype)\n        cupy.testing.assert_array_equal(n.conj().data, n.data.conj())\n\n    def test_conjugate(self):\n        n = _make_complex(cupy, sparse, self.dtype)\n        cupy.testing.assert_array_equal(n.conjugate().data, n.data.conj())\n\n    @unittest.skipUnless(scipy_available, 'requires scipy')\n    def test_str(self):\n        if numpy.dtype(self.dtype).kind == 'f':\n            expect = '''  (1, 1)\\t1.0\n  (2, 2)\\t2.0\n  (1, 0)\\t3.0\n  (2, 1)\\t4.0'''\n        else:\n            expect = '''  (1, 1)\\t(1+0j)\n  (2, 2)\\t(2+0j)\n  (1, 0)\\t(3+0j)\n  (2, 1)\\t(4+0j)'''\n        self.assertEqual(str(self.m), expect)\n\n    def test_toarray(self):\n        m = self.m.toarray()\n        expect = [\n            [0, 0, 0, 0],\n            [3, 1, 0, 0],\n            [0, 4, 2, 0]\n        ]\n        self.assertTrue(m.flags.c_contiguous)\n        cupy.testing.assert_allclose(m, expect)\n\n    def test_pickle_roundtrip(self):\n        s = _make(cupy, sparse, self.dtype)\n        s2 = pickle.loads(pickle.dumps(s))\n        assert s.shape == s2.shape\n        assert s.dtype == s2.dtype\n        if scipy_available:\n            assert (s.get() != s2.get()).count_nonzero() == 0\n\n    def test_diagonal(self):\n        testing.assert_array_equal(\n            self.m.diagonal(-2), cupy.array([0], self.dtype))\n        testing.assert_array_equal(\n            self.m.diagonal(-1), cupy.array([3, 4], self.dtype))\n        testing.assert_array_equal(\n            self.m.diagonal(), cupy.array([0, 1, 2], self.dtype))\n        testing.assert_array_equal(\n            self.m.diagonal(1), cupy.array([0, 0, 0], self.dtype))\n        testing.assert_array_equal(\n            self.m.diagonal(2), cupy.array([0, 0], self.dtype))\n        testing.assert_array_equal(\n            self.m.diagonal(3), cupy.array([0], self.dtype))\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@unittest.skipUnless(scipy_available, 'requires scipy')\nclass TestDiaMatrixInit(unittest.TestCase):\n\n    def setUp(self):\n        self.shape = (3, 4)\n\n    def data(self, xp):\n        return xp.array([[1, 2, 3], [4, 5, 6]], self.dtype)\n\n    def offsets(self, xp):\n        return xp.array([0, -1], 'i')\n\n    def test_shape_none(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.dia_matrix(\n                    (self.data(xp), self.offsets(xp)), shape=None)\n\n    def test_scipy_sparse(self):\n        s_h = scipy.sparse.dia_matrix((self.data(numpy), self.offsets(numpy)),\n                                      shape=self.shape)\n        s_d = sparse.dia_matrix(s_h)\n        s_h2 = s_d.get()\n        assert s_h.shape == s_d.shape\n        assert s_h.dtype == s_d.dtype\n        assert s_h.shape == s_h2.shape\n        assert s_h.dtype == s_h2.dtype\n        assert (s_h.data == s_h2.data).all()\n        assert (s_h.offsets == s_h2.offsets).all()\n\n    @testing.numpy_cupy_allclose(sp_name='sp', atol=1e-5)\n    def test_intlike_shape(self, xp, sp):\n        s = sp.dia_matrix((self.data(xp), self.offsets(xp)),\n                          shape=(xp.array(self.shape[0]),\n                                 xp.int32(self.shape[1])))\n        assert isinstance(s.shape[0], int)\n        assert isinstance(s.shape[1], int)\n        return s\n\n    def test_large_rank_offset(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.dia_matrix(\n                    (self.data(xp), self.offsets(xp)[None]), shape=self.shape)\n\n    def test_large_rank_data(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            with pytest.raises(ValueError):\n                sp.dia_matrix(\n                    (self.data(xp)[None], self.offsets(xp)), shape=self.shape)\n\n    def test_data_offsets_different_size(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            offsets = xp.array([0, -1, 1], 'i')\n            with pytest.raises(ValueError):\n                sp.dia_matrix(\n                    (self.data(xp), offsets), shape=self.shape)\n\n    def test_duplicated_offsets(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            offsets = xp.array([1, 1], 'i')\n            with pytest.raises(ValueError):\n                sp.dia_matrix(\n                    (self.data(xp), offsets), shape=self.shape)\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_conj(self, xp, sp):\n        n = _make_complex(xp, sp, self.dtype)\n        cupy.testing.assert_array_equal(n.conj().data, n.data.conj())\n\n\n@testing.parameterize(*testing.product({\n    'make_method': ['_make', '_make_empty'],\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@unittest.skipUnless(scipy_available, 'requires scipy')\nclass TestDiaMatrixScipyComparison(unittest.TestCase):\n\n    @property\n    def make(self):\n        return globals()[self.make_method]\n\n    @testing.numpy_cupy_equal(sp_name='sp')\n    def test_nnz_axis(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.nnz\n\n    def test_nnz_axis_not_none(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = self.make(xp, sp, self.dtype)\n            with pytest.raises(NotImplementedError):\n                m.getnnz(axis=0)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_toarray(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.toarray()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_A(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.A\n\n    def test_sum_tuple_axis(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m.sum(axis=(0, 1))\n\n    def test_sum_float_axis(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            with pytest.raises(TypeError):\n                m.sum(axis=0.0)\n\n    def test_sum_too_large_axis(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            with pytest.raises(ValueError):\n                m.sum(axis=3)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocoo(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.tocoo()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocoo_copy(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = m.tocoo(copy=True)\n        self.assertIsNot(m.data, n.data)\n        return n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocsc(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m.tocsc()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocsc_copy(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = m.tocsc(copy=True)\n        self.assertIsNot(m.data, n.data)\n        return n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocsr(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.tocsr()\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_tocsr_copy(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        n = m.tocsr(copy=True)\n        self.assertIsNot(m.data, n.data)\n        return n\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_transpose(self, xp, sp):\n        m = self.make(xp, sp, self.dtype)\n        return m.transpose()\n\n    @testing.with_requires('scipy>=1.0')\n    def test_diagonal_error(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            m = _make(xp, sp, self.dtype)\n            with pytest.raises(ValueError):\n                m.diagonal(k=10)  # out of range\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64],\n    'ret_dtype': [None, numpy.float32, numpy.float64],\n    'axis': [None, 0, 1, -1, -2],\n}))\n@unittest.skipUnless(scipy_available, 'requires scipy')\nclass TestDiaMatrixSum(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sum(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        return m.sum(axis=self.axis, dtype=self.ret_dtype)\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_sum_with_out(self, xp, sp):\n        m = _make(xp, sp, self.dtype)\n        if self.axis is None:\n            shape = ()\n        else:\n            shape = list(m.shape)\n            shape[self.axis] = 1\n            shape = tuple(shape)\n        out = xp.empty(shape, dtype=self.ret_dtype)\n        if xp is numpy:\n            # TODO(unno): numpy.matrix is used for scipy.sparse though\n            # cupy.ndarray is used for cupyx.scipy.sparse.\n            out = xp.asmatrix(out)\n        return m.sum(axis=self.axis, dtype=self.ret_dtype, out=out)\n\n\nclass TestIsspmatrixDia(unittest.TestCase):\n\n    def test_dia(self):\n        x = sparse.dia_matrix(\n            (cupy.array([], 'f'),\n             cupy.array([0], 'i')),\n            shape=(0, 0), dtype='f')\n        self.assertTrue(sparse.isspmatrix_dia(x))\n\n    def test_csr(self):\n        x = sparse.csr_matrix(\n            (cupy.array([], 'f'),\n             cupy.array([], 'i'),\n             cupy.array([0], 'i')),\n            shape=(0, 0), dtype='f')\n        self.assertFalse(sparse.isspmatrix_dia(x))\n"""
tests/cupyx_tests/scipy_tests/sparse_tests/test_linalg.py,0,"b""import unittest\n\nimport numpy\nimport pytest\ntry:\n    import scipy.sparse\n    import scipy.stats\n    scipy_available = True\nexcept ImportError:\n    scipy_available = False\n\nimport cupy\nfrom cupy import testing\nfrom cupy.testing import condition\nfrom cupyx.scipy import sparse\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64],\n}))\n@unittest.skipUnless(scipy_available, 'requires scipy')\nclass TestLsqr(unittest.TestCase):\n\n    def setUp(self):\n        rvs = scipy.stats.randint(0, 15).rvs\n        self.A = scipy.sparse.random(50, 50, density=0.2, data_rvs=rvs)\n        self.b = numpy.random.randint(15, size=50)\n\n    def test_size(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            A = sp.csr_matrix(self.A, dtype=self.dtype)\n            b = xp.array(numpy.append(self.b, [1]), dtype=self.dtype)\n            with pytest.raises(ValueError):\n                sp.linalg.lsqr(A, b)\n\n    def test_shape(self):\n        for xp, sp in ((numpy, scipy.sparse), (cupy, sparse)):\n            A = sp.csr_matrix(self.A, dtype=self.dtype)\n            b = xp.array(numpy.tile(self.b, (2, 1)), dtype=self.dtype)\n            with pytest.raises(ValueError):\n                sp.linalg.lsqr(A, b)\n\n    @condition.retry(10)\n    @testing.numpy_cupy_allclose(atol=1e-1, sp_name='sp')\n    def test_csrmatrix(self, xp, sp):\n        A = sp.csr_matrix(self.A, dtype=self.dtype)\n        b = xp.array(self.b, dtype=self.dtype)\n        x = sp.linalg.lsqr(A, b)\n        return x[0]\n\n    @condition.retry(10)\n    @testing.numpy_cupy_allclose(atol=1e-1, sp_name='sp')\n    def test_ndarray(self, xp, sp):\n        A = xp.array(self.A.A, dtype=self.dtype)\n        b = xp.array(self.b, dtype=self.dtype)\n        x = sp.linalg.lsqr(A, b)\n        return x[0]\n"""
tests/cupyx_tests/scipy_tests/special_tests/__init__.py,0,b''
tests/cupyx_tests/scipy_tests/special_tests/test_bessel.py,0,"b""import unittest\n\nimport cupy\nfrom cupy import testing\nimport cupyx.scipy.special  # NOQA\n\n\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestSpecial(unittest.TestCase):\n\n    @testing.for_dtypes(['e', 'f', 'd'])\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def check_unary(self, name, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return getattr(scp.special, name)(a)\n\n    def test_j0(self):\n        self.check_unary('j0')\n\n    def test_j1(self):\n        self.check_unary('j1')\n\n    def test_y0(self):\n        self.check_unary('y0')\n\n    def test_y1(self):\n        self.check_unary('y1')\n\n    def test_i0(self):\n        self.check_unary('i0')\n\n    def test_i1(self):\n        self.check_unary('i1')\n\n\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestFusionSpecial(unittest.TestCase):\n\n    @testing.for_dtypes(['e', 'f', 'd'])\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def check_unary(self, name, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = testing.shaped_arange((2, 3), xp, dtype)\n\n        @cupy.fuse()\n        def f(x):\n            return getattr(scp.special, name)(x)\n\n        return f(a)\n\n    def test_j0(self):\n        self.check_unary('j0')\n\n    def test_j1(self):\n        self.check_unary('j1')\n\n    def test_y0(self):\n        self.check_unary('y0')\n\n    def test_y1(self):\n        self.check_unary('y1')\n\n    def test_i0(self):\n        self.check_unary('i0')\n\n    def test_i1(self):\n        self.check_unary('i1')\n"""
tests/cupyx_tests/scipy_tests/special_tests/test_convex_analysis.py,0,"b""import itertools\nimport unittest\n\nimport cupy\nimport numpy\n\nfrom cupy import testing\nimport cupyx.scipy.special\n\n\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestSpecialConvex(unittest.TestCase):\n\n    def test_huber_basic(self):\n        assert cupyx.scipy.special.huber(-1, 1.5) == cupy.inf\n        testing.assert_allclose(cupyx.scipy.special.huber(2, 1.5),\n                                0.5 * 1.5**2)\n        testing.assert_allclose(cupyx.scipy.special.huber(2, 2.5),\n                                2 * (2.5 - 0.5 * 2))\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose(scipy_name='scp')\n    def test_huber(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n        z = testing.shaped_random((10, 2), xp=xp, dtype=dtype)\n        return scp.special.huber(z[:, 0], z[:, 1])\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose(scipy_name='scp')\n    def test_entr(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n        values = (0, 0.5, 1.0, cupy.inf)\n        signs = [-1, 1]\n        arr = []\n        for sgn, v in itertools.product(signs, values):\n            arr.append(sgn * v)\n        z = xp.asarray(arr, dtype=dtype)\n        return scp.special.entr(z)\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose(scipy_name='scp')\n    def test_rel_entr(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n        values = (0, 0.5, 1.0)\n        signs = [-1, 1]\n        arr = []\n        for sgna, va, sgnb, vb in itertools.product(signs, values, signs,\n                                                    values):\n            arr.append((sgna*va, sgnb*vb))\n        z = xp.asarray(numpy.array(arr, dtype=dtype))\n        return scp.special.kl_div(z[:, 0], z[:, 1])\n\n    @testing.for_float_dtypes()\n    @testing.numpy_cupy_allclose(scipy_name='scp')\n    def test_pseudo_huber(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n        z = testing.shaped_random((10, 2), xp=numpy, dtype=dtype).tolist()\n        z = xp.asarray(z + [[0, 0.5], [0.5, 0]], dtype=dtype)\n        return scp.special.pseudo_huber(z[:, 0], z[:, 1])\n"""
tests/cupyx_tests/scipy_tests/special_tests/test_digamma.py,0,"b""import unittest\n\nfrom cupy import testing\nimport cupyx.scipy.special  # NOQA\nimport numpy\n\n\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestDigamma(unittest.TestCase):\n\n    @testing.with_requires('scipy>=1.1.0')\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_arange(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return scp.special.digamma(a)\n\n    @testing.with_requires('scipy>=1.1.0')\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_linspace_positive(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = numpy.linspace(0, 30, 1000, dtype=dtype)\n        a = xp.asarray(a)\n        return scp.special.digamma(a)\n\n    @testing.with_requires('scipy>=1.1.0')\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-2, rtol=1e-3, scipy_name='scp')\n    def test_linspace_negative(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = numpy.linspace(-30, 0, 1000, dtype=dtype)\n        a = xp.asarray(a)\n        return scp.special.digamma(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-2, rtol=1e-3, scipy_name='scp')\n    def test_scalar(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        return scp.special.digamma(dtype(1.5))\n\n    @testing.with_requires('scipy>=1.1.0')\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-2, rtol=1e-3, scipy_name='scp')\n    def test_inf_and_nan(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = numpy.array([-numpy.inf, numpy.nan, numpy.inf]).astype(dtype)\n        a = xp.asarray(a)\n        return scp.special.digamma(a)\n"""
tests/cupyx_tests/scipy_tests/special_tests/test_erf.py,0,"b""import unittest\n\nimport numpy\n\nimport cupy\nfrom cupy import testing\nimport cupyx.scipy.special  # NOQA\n\n\ndef _boundary_inputs(boundary, rtol, atol):\n    left = boundary * (1 - numpy.copysign(rtol, boundary)) - atol\n    right = boundary * (1 + numpy.copysign(rtol, boundary)) + atol\n    return [left, boundary, right]\n\n\nclass _TestBase(object):\n\n    def test_erf(self):\n        self.check_unary('erf')\n\n    def test_erfc(self):\n        self.check_unary('erfc')\n\n    def test_erfcx(self):\n        self.check_unary('erfcx')\n\n    @testing.with_requires('scipy>=1.4.0')\n    def test_erfinv(self):\n        self.check_unary('erfinv')\n        self.check_unary_random('erfinv', scale=2, offset=-1)\n        self.check_unary_boundary('erfinv', boundary=-1)\n        self.check_unary_boundary('erfinv', boundary=1)\n\n    @testing.with_requires('scipy>=1.4.0')\n    def test_erfcinv(self):\n        self.check_unary('erfcinv')\n        self.check_unary_random('erfcinv', scale=2, offset=0)\n        self.check_unary_boundary('erfcinv', boundary=0)\n        self.check_unary_boundary('erfcinv', boundary=2)\n\n\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestSpecial(unittest.TestCase, _TestBase):\n\n    @testing.for_dtypes(['e', 'f', 'd'])\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def check_unary(self, name, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return getattr(scp.special, name)(a)\n\n    @testing.for_dtypes(['f', 'd'])\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def check_unary_random(self, name, xp, scp, dtype, scale, offset):\n        import scipy.special  # NOQA\n\n        a = testing.shaped_random((2, 3), xp, dtype, scale=scale) + offset\n        return getattr(scp.special, name)(a)\n\n    @testing.for_dtypes(['f', 'd'])\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def check_unary_boundary(self, name, xp, scp, dtype, boundary):\n        import scipy.special  # NOQA\n\n        a = _boundary_inputs(boundary, 1.0 / 1024, 1.0 / 1024)\n        a = xp.array(a, dtype=dtype)\n        return getattr(scp.special, name)(a)\n\n    @testing.with_requires('scipy>=1.4.0')\n    @testing.for_dtypes(['f', 'd'])\n    def test_erfinv_behavior(self, dtype):\n        a = cupy.empty((1,), dtype=dtype)\n\n        a[:] = 1.0 + 1E-6\n        a = cupyx.scipy.special.erfinv(a)\n        assert cupy.isnan(a)\n        a[:] = -1.0 - 1E-6\n        a = cupyx.scipy.special.erfinv(a)\n        assert cupy.isnan(a)\n        a[:] = 1.0\n        a = cupyx.scipy.special.erfinv(a)\n        assert numpy.isposinf(cupy.asnumpy(a))\n        a[:] = -1.0\n        a = cupyx.scipy.special.erfinv(a)\n        assert numpy.isneginf(cupy.asnumpy(a))\n\n    @testing.with_requires('scipy>=1.4.0')\n    @testing.for_dtypes(['f', 'd'])\n    def test_erfcinv_behavior(self, dtype):\n        a = cupy.empty((1,), dtype=dtype)\n\n        a[:] = 2.0 + 1E-6\n        a = cupyx.scipy.special.erfcinv(a)\n        assert cupy.isnan(a)\n        a[:] = 0.0 - 1E-6\n        a = cupyx.scipy.special.erfcinv(a)\n        assert cupy.isnan(a)\n        a[:] = 0.0\n        a = cupyx.scipy.special.erfcinv(a)\n        assert numpy.isposinf(cupy.asnumpy(a))\n        a[:] = 2.0\n        a = cupyx.scipy.special.erfcinv(a)\n        assert numpy.isneginf(cupy.asnumpy(a))\n\n\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestFusionSpecial(unittest.TestCase, _TestBase):\n\n    @testing.for_dtypes(['e', 'f', 'd'])\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def check_unary(self, name, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = testing.shaped_arange((2, 3), xp, dtype)\n\n        @cupy.fuse()\n        def f(x):\n            return getattr(scp.special, name)(x)\n\n        return f(a)\n\n    @testing.for_dtypes(['f', 'd'])\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def check_unary_random(self, name, xp, scp, dtype, scale, offset):\n        import scipy.special  # NOQA\n\n        a = testing.shaped_random((2, 3), xp, dtype, scale=scale) + offset\n\n        @cupy.fuse()\n        def f(x):\n            return getattr(scp.special, name)(x)\n\n        return f(a)\n\n    @testing.for_dtypes(['f', 'd'])\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def check_unary_boundary(self, name, xp, scp, dtype, boundary):\n        import scipy.special  # NOQA\n\n        a = _boundary_inputs(boundary, 1.0 / 1024, 1.0 / 1024)\n        a = xp.array(a, dtype=dtype)\n\n        @cupy.fuse()\n        def f(x):\n            return getattr(scp.special, name)(x)\n\n        return f(a)\n"""
tests/cupyx_tests/scipy_tests/special_tests/test_gamma.py,0,"b""import unittest\n\nfrom cupy import testing\nimport cupyx.scipy.special  # NOQA\nimport numpy\n\n\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestGamma(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_arange(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return scp.special.gamma(a)\n\n    @testing.for_all_dtypes(no_complex=True, no_bool=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, rtol=1e-5, scipy_name='scp')\n    def test_linspace(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = numpy.linspace(-30, 30, 1000, dtype=dtype)\n        a = xp.asarray(a)\n        return scp.special.gamma(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-2, rtol=1e-3, scipy_name='scp')\n    def test_scalar(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        return scp.special.gamma(dtype(1.5))\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-2, rtol=1e-3, scipy_name='scp')\n    def test_inf_and_nan(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = numpy.array([-numpy.inf, numpy.nan, numpy.inf]).astype(dtype)\n        a = xp.asarray(a)\n        return scp.special.gamma(a)\n"""
tests/cupyx_tests/scipy_tests/special_tests/test_gammaln.py,0,"b""import unittest\n\nfrom cupy import testing\nimport cupyx.scipy.special  # NOQA\nimport numpy\n\n\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestGammaln(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_arange(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return scp.special.gammaln(a)\n\n    @testing.for_all_dtypes(no_complex=True, no_bool=True)\n    @testing.numpy_cupy_allclose(atol=1e-4, rtol=1e-5, scipy_name='scp')\n    def test_linspace(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = numpy.linspace(-30, 30, 1000, dtype=dtype)\n        a = xp.asarray(a)\n        return scp.special.gammaln(a)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-2, rtol=1e-3, scipy_name='scp')\n    def test_scalar(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        return scp.special.gammaln(dtype(1.5))\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-2, rtol=1e-3, scipy_name='scp')\n    def test_inf_and_nan(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = numpy.array([-numpy.inf, numpy.nan, numpy.inf]).astype(dtype)\n        a = xp.asarray(a)\n        return scp.special.gammaln(a)\n"""
tests/cupyx_tests/scipy_tests/special_tests/test_polygamma.py,0,"b""import unittest\n\nfrom cupy import testing\nimport cupyx.scipy.special  # NOQA\nimport numpy\n\nimport warnings\n\n\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestPolygamma(unittest.TestCase):\n\n    @testing.with_requires('scipy>=1.1.0')\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_arange(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        b = testing.shaped_arange((2, 3), xp, dtype)\n        return scp.special.polygamma(a, b)\n\n    @testing.with_requires('scipy>=1.1.0')\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-3, rtol=1e-3, scipy_name='scp')\n    def test_linspace(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = numpy.tile(numpy.arange(5), 200).astype(dtype)\n        b = numpy.linspace(-30, 30, 1000, dtype=dtype)\n        a = xp.asarray(a)\n        b = xp.asarray(b)\n        return scp.special.polygamma(a, b)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-2, rtol=1e-3, scipy_name='scp')\n    def test_scalar(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        # polygamma in scipy returns numpy.float64 value when inputs scalar.\n        # whatever type input is.\n        return scp.special.polygamma(\n            dtype(2.), dtype(1.5)).astype(numpy.float32)\n\n    @testing.with_requires('scipy>=1.1.0')\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-2, rtol=1e-3, scipy_name='scp')\n    def test_inf_and_nan(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        x = numpy.array([-numpy.inf, numpy.nan, numpy.inf]).astype(dtype)\n        a = numpy.tile(x, 3)\n        b = numpy.repeat(x, 3)\n        a = xp.asarray(a)\n        b = xp.asarray(b)\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            return scp.special.polygamma(a, b)\n"""
tests/cupyx_tests/scipy_tests/special_tests/test_statistics.py,0,"b""import unittest\n\nimport cupy\nfrom cupy import testing\nimport cupyx.scipy.special  # NOQA\n\n\nclass _TestBase(object):\n\n    def test_ndtr(self):\n        self.check_unary('ndtr')\n\n\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestSpecial(unittest.TestCase, _TestBase):\n\n    @testing.for_dtypes(['e', 'f', 'd'])\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def check_unary(self, name, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        return getattr(scp.special, name)(a)\n\n\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestFusionSpecial(unittest.TestCase, _TestBase):\n\n    @testing.for_dtypes(['e', 'f', 'd'])\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def check_unary(self, name, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = testing.shaped_arange((2, 3), xp, dtype)\n\n        @cupy.fuse()\n        def f(x):\n            return getattr(scp.special, name)(x)\n\n        return f(a)\n"""
tests/cupyx_tests/scipy_tests/special_tests/test_zeta.py,0,"b""import unittest\n\nfrom cupy import testing\nimport cupyx.scipy.special  # NOQA\nimport numpy\n\n\n@testing.gpu\n@testing.with_requires('scipy')\nclass TestZeta(unittest.TestCase):\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, scipy_name='scp')\n    def test_arange(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = testing.shaped_arange((2, 3), xp, dtype)\n        b = testing.shaped_arange((2, 3), xp, dtype)\n        return scp.special.zeta(a, b)\n\n    @testing.for_all_dtypes(no_complex=True, no_bool=True)\n    @testing.numpy_cupy_allclose(atol=1e-5, rtol=1e-6, scipy_name='scp')\n    def test_linspace(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        a = numpy.linspace(-30, 30, 1000, dtype=dtype)\n        b = numpy.linspace(-30, 30, 1000, dtype=dtype)\n        a = xp.asarray(a)\n        b = xp.asarray(b)\n        return scp.special.zeta(a, b)\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-2, rtol=1e-3, scipy_name='scp')\n    def test_scalar(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        return scp.special.zeta(dtype(2.), dtype(1.5))\n\n    @testing.for_all_dtypes(no_complex=True)\n    @testing.numpy_cupy_allclose(atol=1e-2, rtol=1e-3, scipy_name='scp')\n    def test_inf_and_nan(self, xp, scp, dtype):\n        import scipy.special  # NOQA\n\n        x = numpy.array([-numpy.inf, numpy.nan, numpy.inf]).astype(dtype)\n        a = numpy.tile(x, 3)\n        b = numpy.repeat(x, 3)\n        a = xp.asarray(a)\n        b = xp.asarray(b)\n        return scp.special.zeta(a, b)\n"""
