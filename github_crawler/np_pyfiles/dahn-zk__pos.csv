file_path,api_count,code
correctness.py,0,"b'import sys, os.path\r\n\r\ndef main():\r\n    if len(sys.argv) < 3:\r\n        print(""Usage: python "" + os.path.basename(__file__) +\r\n                "" <tagger output> <reference file>"")\r\n        exit(1)\r\n\r\n    with open(sys.argv[1], ""r"") as f:\r\n        user_sentences = f.readlines()\r\n\r\n    with open(sys.argv[2], ""r"") as f:\r\n        correct_sentences = f.readlines()\r\n\r\n    num_correct = 0\r\n    total = 0\r\n\r\n    for user_sent, correct_sent in zip(user_sentences, correct_sentences):\r\n        user_tok = user_sent.split()\r\n        correct_tok = correct_sent.split()\r\n\r\n        # TODO: compare corresponding sentences\r\n        if len(user_tok) != len(correct_tok):\r\n            continue\r\n\r\n        for u, c in zip(user_tok, correct_tok):\r\n            if u == c:\r\n                num_correct += 1\r\n            total += 1\r\n\r\n    score = float(num_correct) / total * 100\r\n\r\n    print(""{:d}% of tags are correct"".format(int(score)))\r\n\r\n\r\nif __name__ == ""__main__"": main()\r\n'"
get-random-wikipedia-sentence.py,0,"b'import wikipedia as wi, nltk, main, pickle\nimport sys\n\n\n# TODO: consider using https://en.wikipedia.org/wiki/Special:Random\n# https://en.wikipedia.org/wiki/Special:RandomInCategory/Mathematics\n\n# FIXME: this selects only uncategorized articles :<\n"" titles ""\nn = 5\nif len(sys.argv) > 1:\n    n = int(sys.argv[1])\nt = wi.random(pages=n)\nif type(t) is str: t = [t]\n\nprint(\'Random wikipedia articles: ""\' + \'"", ""\'.join(t) + \'""\')\n\nz = nltk.data.load(\'tokenizers/punkt/english.pickle\')\n\ndef c(s):\n    return 10 < len(s) < 17\n\n"""""" model parameters """"""\np = {}\nfor n in \'e_values known_words q_values tagset\'.split():\n    with open(\'parameters/\' + n + \'.pkl\', \'rb\') as f:\n        p[n] = pickle.load(f)\n\n"""""" sentences """"""\ns = [] \n\nfor t in t:\n\n    print(\'Downloading and processing summary of ""\' + t + \'"" ...\')\n\n    """""" list of summary word-tokenized sentences """"""\n    ss = [nltk.word_tokenize(s) for s in z.tokenize(wi.summary(t))]\n    \n    s.extend(list(filter(c, ss)))\n\nprint(""Number of selected sentences: {}"".format(len(s)))\n\nfor s0 in ["" "".join(l) for l in s]:\n    print(s0)\n\nfor tagg in [main.tag_viterbi(sen,\n    p[\'tagset\'],\n    p[\'known_words\'],\n    p[\'q_values\'],\n    p[\'e_values\']) for sen in s]:\n    print("" "".join(map(lambda z: ""/"".join(map(str,z)),tagg)))\n'"
main.py,0,"b'""""""""""\nPart of speech tagging\nChernivtsi National University\nDanylo Dubinin\n""""""""""\n\n""""""\n1. Imports\n-------------------------------------------------------------------------------\n""""""\n\nimport nltk\n# Natural Language Toolkit\n# http://www.nltk.org/\n\nfrom math import log\nfrom time import clock\n\nimport numpy\n# Scientific computing\n# http://www.numpy.org/\n\nimport re\n# Regular Expressions\n\nfrom collections import Counter\n\nimport pickle\n# A liquid or marinade :) uhmm... I mean...\n# Binary protocols for serializing and de-serializing object structures\n# https://docs.python.org/3/library/pickle.html\n\nimport os\n# Crossplatform filepath related operations.\n\n""""""\n2. Global parameters and constants\n-------------------------------------------------------------------------------\n""""""\n\n"""""" Start and stop symbols embraces sentences. """"""\nSTAR = \'*\'\nSTOP = \'STOP\'\n\n"""""" for smoothing """"""\nRARE = \'_RARE_\'\n\n"""""" Maximum number of occurances a word must have to be considered rare """"""\n# TODO: parameter identification\nRARE_MAX_FREQ = 5\n\n# When probability is zero, its logarithm is undefined, instead we use big\n# negative number to denote log(0)\n# NOTE: float(\'-inf\') maybe?\n#       http://stackoverflow.com/questions/1628026/python-infinity-any-caveats\nLOG_OF_ZERO = -1000\n\n"""""" Current directory """"""\nPATH = os.path.dirname(os.path.realpath(__file__))\n\n""""""\n3. Function definitions\n-------------------------------------------------------------------------------\n""""""\n\ndef split_wordtags(sentences):\n    """"""\n    Receives a list of tagged sentences and processes each sentence to generate\n    a list of words and a list of tags.\n    Start and stop symbols are included in returned lists, as defined by the\n    constants STAR and STOP respectively.\n\n    Parameters\n    ----------\n    sentences : list of list of str\n        Each sentence is a string of space-separated ""WORD/TAG"" tokens, with a\n        newline character in the end.\n\n    Returns\n    -------\n    words, tags : 2d lists of str\n        Lists where every element is a list of the words/tags of a particular\n        sentence\n\n    """"""\n    words = []\n    tags  = []\n    for sentence in sentences:\n        sentence_split = sentence.split()\n        word_tag = [re.split(r\'(^.+)/([A-Z.]+$)\', pair)[1:3] \\\n                for pair in sentence_split]\n        word_tag = [[STAR]*2]*2 + word_tag + [[STOP]*2]\n        word_tag_transposed = numpy.array(word_tag).transpose()\n        words.append(list(word_tag_transposed[0]))\n        tags .append(list(word_tag_transposed[1]))\n    return words, tags\n\ndef calculate_q(tags):\n    """"""\n    Takes tags from the training data and calculates tag trigram probabilities.\n\n    Parameters\n    ----------\n    tags : list of list of str\n        Each element of tags list is list of tags of particular sentence.\n\n    Returns\n    -------\n    q_values : dict of tuple:float\n        The keys are tuples of str that represent the tag trigram.\n        The values are the float log probability of that trigram.\n\n    """"""\n    trigrams = [trigram for sentence in tags \\\n            for trigram in nltk.trigrams(sentence)]\n    bigrams  = [bigram  for sentence in tags \\\n            for bigram  in nltk.bigrams(sentence)]\n    trigrams_c = Counter(trigrams)\n    bigram_c   = Counter(bigrams )\n    q_values = {trigram: log(count,2)-log(bigram_c[trigram[:-1]],2) \\\n            for trigram,count in trigrams_c.items()}\n    return q_values\n\ndef calculate_known(words):\n    """"""\n    Takes the words from the training data and returns a set of all of the\n    words that occur more than RARE_MAX_FREQ\n\n    Parameters\n    ----------\n    words : list of list of str\n        Each element of sentence_words is a list with str words of a particular\n        sentence enclosed with STAR and STOP symbols.\n\n    Returns\n    -------\n    known_words : set of str\n        Set of known words.\n\n    """"""\n    words_count = Counter([word for sentence in words for word in sentence])\n    known_words = set([word for word,count in words_count.items() \\\n            if count > RARE_MAX_FREQ])\n    return known_words\n\ndef replace_rare(sentences, known_words):\n    """"""\n    Replaces rare words with RARE symbol.\n\n    Parameters\n    ----------\n    sentences : list of list of str\n        Each sentence is a string of space-separated ""WORD/TAG"" tokens, with a\n        newline character in the end.\n\n    Returns\n    -------\n    replaced : list of list of str\n        List with the same structure as sentences but with words that is not in\n        known_words replaced by RARE.\n\n    """"""\n    replaced = [[word in known_words and word or RARE for word in sentence]\n            for sentence in sentences]\n    return replaced\n\n# \n# The first return value is a python dictionary where each key is a tuple in\n# which the first element is a word\n# and the second is a tag, and the value is the log probability of the emission\n# of the word given the tag\n# The second return value is a set of all possible tags for this data set\n# FIXME: total nonesence. extremely bad design to return e_values and taglist\ndef calculate_e(toks, tags):\n    """"""\n    Calculates emission probabilities and creates a set of all possible tags.\n\n    Parameters\n    ----------\n    toks : list of list of str\n        Each sentence is a string of space-separated ""WORD/TAG"" tokens, with a\n        newline character in the end.\n    tags : list of list of str \n        Each element of tags list is list of tags of particular sentence.\n\n    Returns\n    -------\n\n    """"""\n    tags_flat  = [tag  for sentence in tags for tag  in sentence]\n    words_flat = [word for sentence in toks for word in sentence]\n    tags_c = Counter(tags_flat)\n    word_tag = zip(words_flat, tags_flat)\n    word_tag_c = Counter(word_tag)\n    e_values = {k: log(float(c),2)-log(float(tags_c[k[1]]),2) \\\n            for k,c in word_tag_c.items()}\n    taglist = set(tags_flat)\n    return e_values, taglist\n\n""""""\n3.a Viterbi algorithm\n-------------------------------------------------------------------------------\n""""""\n\ndef tag_viterbi(tokens, tagset, known_words, Q, E):\n    """"""\n    Tags tokens of a sentence.\n\n    Parameters\n    ----------\n    tokens : list of str\n    tagset : set of str\n    known_words : list of str\n        List of known words, those that occur more than RARE_MAX_FREQ times in\n        corpus.\n    Q : dict of tuple:float \n    E : dict of tuple:float \n\n    Returns\n    -------\n    tagged : list of tuples of str\n        List of sentence tokens together with its tags in tuples.\n\n    """"""\n    tags = tagset.difference({STAR,STOP})\n    def S(n):\n        if n < 2:\n            return [STAR]\n        elif n == T+2:\n            return [STOP]\n        else:\n            return tags\n    T = len(tokens)\n    pi = [{STAR: {STAR: 0.0}}]\n    bp = [None]\n    for k in range(2,T+2):\n        pi.append({})\n        bp.append({})\n        for u in S(k-1):\n            pi[k-1][u] = {}\n            bp[k-1][u] = {}\n            for v in S(k):\n                pi_max = float(\'-inf\')\n                w_max = None\n                for w in pi[k-2]:\n                    #if not w in pi[k-2] or not u in pi[k-2][w]:\n                    #    continue\n                    q = Q.get((w,u,v),LOG_OF_ZERO)\n                    if q == LOG_OF_ZERO:\n                        s = q\n                    else:\n                        p = pi[k-2][w][u]\n                        e_word = tokens[k-2]\n                        if not e_word in known_words:\n                            e_word = RARE\n                        if not (e_word,v) in E:\n                            s = LOG_OF_ZERO\n                        else:\n                            e = E[e_word,v]\n                            s = p + q + e\n                    if s > pi_max:\n                        pi_max = s\n                        w_max = w\n                if not w_max:\n                    continue\n                pi[k-1][u][v], bp[k-1][u][v] = pi_max, w_max\n    uv = None\n    pi_max = float(\'-inf\')\n    for u in S(T):\n        for v in S(T+1):\n            q = Q.get((u,v,STOP),LOG_OF_ZERO)\n            if q == LOG_OF_ZERO:\n                s = q\n            else:\n                s = pi[T][u][v] + q\n            if s > pi_max:\n                pi_max = s\n                uv = (u,v)\n    y = [\'X\']*(T+2)\n    y[T] = uv[0]\n    y[T+1] = uv[1]\n    for k in reversed(range(T)):\n        y[k] = bp[k+1][y[k+1]][y[k+2]]\n    return zip(tokens,y[2:])\n\n""""""\n3.b NLTK native tagging\n-------------------------------------------------------------------------------\n""""""\n\n# This function uses nltk to create the taggers described in question 6\n# brown_words and brown_tags is the data to be used in training\n# brown_dev_words is the data that should be tagged\n# The return value is a list of tagged sentences in the format ""WORD/TAG"",\n# separated by spaces. Each sentence is a string with a\n# terminal newline, not a list of tokens.\n# FIXME: this one is broken in python 3\ndef nltk_tagger(brown_words, brown_tags, brown_dev_words):\n    assert(len(brown_words)==len(brown_tags))\n    training = [zip(brown_words[i], brown_tags[i]) \\\n            for i in range(len(brown_words))]\n    print([list(z) for z in training])\n    patterns = [(r\'.*(ing|ed|es)$\', \'VERB\'),(r\'.*(est|ous)$\', \'ADJ\')]\n    default_tagger = nltk.DefaultTagger(\'NOUN\')\n    regexp_tagger = nltk.RegexpTagger(patterns, backoff=default_tagger)\n    bigram_tagger = nltk.BigramTagger(training, backoff=regexp_tagger)\n    trigram_tagger = nltk.TrigramTagger(training, backoff=bigram_tagger)\n    tagged = [trigram_tagger.tag(tokens) for tokens in brown_dev_words]\n    tagged = ["" "".join([""{0}/{1}"".format(*pair) for pair in tokens]) + \' \\n\' \\\n            for tokens in tagged]\n    return tagged\n\n\n""""""\n4. Input/Output\n-------------------------------------------------------------------------------\n""""""\n\n""""""\n4.a Parameters\n\n""""""\n\n"""""" Delimiter """"""\nDEL = "" ""\n\nDATA_PATH = os.path.join(PATH, \'data\')\nOUTPUT_PATH = os.path.join(PATH, \'output\')\nPARAMETERS_PATH = os.path.join(PATH, \'parameters\')\n\n""""""\n4.b Function definitions\n\n""""""\n\n# This function takes output from calculate_q() and outputs it in the proper\n# format\n# TODO: pickle it!?\n# TODO: docstringificate\ndef output_q(q_values, filename):\n    outfile = open(filename, ""w"")\n    trigrams = list(q_values.keys())\n    trigrams.sort()\n    for trigram in trigrams:\n        line = DEL.join(list(trigram) + [str(q_values[trigram])])\n        outfile.write(line + \'\\n\')\n    outfile.close()\n\ndef output_e(e_values, filename):\n    outfile = open(filename, ""w"")\n    emissions = list(e_values.keys())\n    emissions.sort()\n    for item in emissions:\n        output = DEL.join([item[0], item[1], str(e_values[item])])\n        outfile.write(output + \'\\n\')\n    outfile.close()\n\ndef output_tagged(tagged, filename):\n    outfile = open(filename, \'w\')\n    for sentence in tagged:\n        outfile.write(sentence + \'\\n\')\n    outfile.close()\n\ndef load_data(name):\n    infile = open(DATA_PATH + name + "".txt"", ""r"")\n    data = infile.readlines()\n    infile.close()\n    return data\n\ndef save_object(obj, filename):\n    with open(filename, \'wb\') as output:\n        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n\ndef load_viterbi_parameters():\n    """"""\n    Loads parameters to viterbi algorithm.\n\n    Returns\n    -------\n    tagset, known_words, q_values, e_values\n\n    """"""\n    names = \'tagset known_words q_values e_values\'.split()\n    objects = {}\n    for name in names:\n        with open(PARAMETERS_PATH + name + \'.pkl\', \'rb\') as object_file:\n            objects[name] = pickle.load(object_file)\n\n    return tuple([objects[name] for name in names])\n\n""""""\n5. Main\n-------------------------------------------------------------------------------\n""""""\n\ndef main():\n    # FIXME: clock() returns seconds elapsed since the FIRST CALL to this\n    # function, as a floating point number, so if one want to run main()\n    # mutliple times, final timing output will be not correct.\n    clock()\n\n    train = load_data(\'Brown_tagged_train\')\n    words, tags = split_wordtags(train)\n    q_values = calculate_q(tags)\n    save_object(q_values, PARAMETERS_PATH + \'q_values.pkl\')\n    output_q(q_values, OUTPUT_PATH + \'q_values.txt\')\n\n    known_words = calculate_known(words)\n    save_object(known_words, PARAMETERS_PATH + \'known_words.pkl\')\n    words_rare = replace_rare(words, known_words)\n\n    e_values, tagset = calculate_e(words_rare, tags)\n    save_object(e_values, PARAMETERS_PATH + \'e_values.pkl\')\n    save_object(tagset, PARAMETERS_PATH + \'tagset.pkl\')\n    output_e(e_values, OUTPUT_PATH + ""e_values.txt"")\n\n    del train\n    del words_rare\n\n    dev = load_data(\'Brown_dev\')\n    dev_words = []\n    for sentence in dev:\n        dev_words.append(sentence.split("" "")[:-1])\n    viterbi_tagged = ( \\\n        "" "".join([""{0}/{1}"".format(*x) for x in \\\n            tag_viterbi(tokens, tagset, known_words, q_values, e_values)]) \\\n        for tokens in dev_words)\n    output_tagged(viterbi_tagged, OUTPUT_PATH + \'Brown_tagged_dev.txt\')\n\n    #nltk_tagged = nltk_tagger(words, tags, dev_words)\n    #output_nltk_tagged(nltk_tagged, OUTPUT_PATH + \'B6.txt\')\n\n    print(""Ellapsed time: "" + str(clock()) + \' sec\')\n\nif __name__ == ""__main__"": main()\n'"
perplexity.py,0,"b'import sys\r\n\r\ndef main():\r\n    if len(sys.argv) < 2:\r\n        print ""Usage: python perplexity.py <file of scores> <file of sentences that were scored>""\r\n        exit(1)\r\n    \r\n    infile = open(sys.argv[1], ""r"")\r\n    scores = infile.readlines()\r\n    infile.close()\r\n    infile = open(sys.argv[2], \'r\')\r\n    sentences = infile.readlines()\r\n    infile.close()\r\n    \r\n    M = 0\r\n\r\n    for sentence in sentences:\r\n        words = sentence.split()\r\n        M += len(words) + 1\r\n\r\n    perplexity = 0\r\n    for score in scores:\r\n       perplexity += float(score.split()[0])  # assume log probability\r\n\r\n    perplexity /= M\r\n    perplexity = 2 ** (-1 * perplexity)\r\n\r\n    print ""The perplexity is"", perplexity    \r\n\t \r\nif __name__ == ""__main__"": main()\r\n'"
postag-interactive.py,0,"b'import pickle\nimport main\nimport nltk\n\nnames = \'tagset known_words q_values e_values\'.split()\nobjects = {}\nfor name in names:\n    with open(\'parameters/\' + name + \'.pkl\', \'rb\') as object_file:\n        objects[name] = pickle.load(object_file)\n\nprompt = \'Sentence > \'\n\ninput_string = None\nwhile not input_string in [\'q\',\'quit\',\'exit\']:\n    if not input_string:\n        input_string = \'Enter an English sentence to tag its tokens with the respective parts of speech, -- this is an example.\'\n        print(prompt + input_string)\n    else:\n        input_string = input(prompt)\n    sentence = nltk.word_tokenize(input_string)\n    tagged = main.tag_viterbi(sentence, objects[\'tagset\'], objects[\'known_words\'], objects[\'q_values\'], objects[\'e_values\'])\n    print(\'Tagged : \' + "" "".join([""{0}/{1}"".format(*x) for x in tagged]))\n'"
