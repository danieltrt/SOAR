file_path,api_count,code
preprocessor_utils.py,9,"b'import re\n\nclass TypeReplacer(object):\n    def __init__(self, macro_name, templated_type, base_class, internal_property, deref):\n        self.pattern = re.compile(macro_name + r""\\((?P<var>.+?)\\)"")\n        self.templated_type = templated_type\n        self.deref = deref\n        self.base_class = base_class\n        self.internal_property = internal_property\n\n    def rephrase(self, type_name):\n        def wrapped(match):\n            var = match.group(""var"")\n            if self.deref:\n                return ""(<%s[%s]*>((<%s>(%s)).%s))[0]"" % (self.templated_type, type_name, self.base_class, var, self.internal_property)\n            else:\n                return ""(<%s[%s]*>((<%s>(%s)).%s))"" % (self.templated_type, type_name, self.base_class, var, self.internal_property)\n        return wrapped\n\n    def __call__(self, *args, **kwargs):\n        return self.replace(*args, **kwargs)\n\n    def replace(self, type_name, text):\n        return self.pattern.sub(self.rephrase(type_name), text)\n\nclass WrapperReplacer(object):\n    def __init__(self, pattern, wrapper_function):\n        self.pattern = pattern\n        self.wrapper_function = wrapper_function\n\n    def __call__(self, *args, **kwargs):\n        return self.replace(*args, **kwargs)\n\n    def replace(self, type_name, text):\n        return text.replace(self.pattern, self.wrapper_function % (type_name,))\n\nclass LambdaReplacer(object):\n    def __init__(self, macro_name, lambdaf):\n        self.pattern = re.compile(macro_name + r""\\((?P<var>.+?)\\)"")\n        self.lambdaf = lambdaf\n\n    def __call__(self, *args, **kwargs):\n        return self.replace(*args, **kwargs)\n\n    def replace(self, type_name, text):\n        return self.pattern.sub(self.lambdaf(type_name), text)\n\nclass TypedName(LambdaReplacer):\n    def __init__(self):\n        def replacer(type_name):\n            def wrapped(match):\n                var = match.group(""var"")\n                return \'%s_%s\' % (var, type_name)\n            return wrapped\n\n        super(TypedName, self).__init__(\'TYPED\', replacer)\n\n\nREPLACERS = [\n    TypeReplacer(""DEREF_MAT"", ""CMat"", ""Mat"", ""matinternal"", deref=True),\n    TypeReplacer(""PTR_MAT"", ""CMat"",  ""Mat"", ""matinternal"", deref=False),\n    WrapperReplacer(""WRAP_MAT"", \'WrapMat_%s\'),\n\n    TypeReplacer(""DEREF_LAYER"", ""CLayer"", ""Layer"", ""layerinternal"", deref=True),\n    TypeReplacer(""PTR_LAYER"", ""CLayer"",  ""Layer"", ""layerinternal"", deref=False),\n    WrapperReplacer(""WRAP_LAYER"", \'WrapLayer_%s\'),\n\n    TypeReplacer(""DEREF_RNN"", ""CRNN"", ""RNN"", ""layerinternal"", deref=True),\n    TypeReplacer(""PTR_RNN"", ""CRNN"",  ""RNN"", ""layerinternal"", deref=False),\n    WrapperReplacer(""WRAP_RNN"", \'WrapRNN_%s\'),\n\n    TypeReplacer(""DEREF_GRU"", ""CGRU"", ""GRU"", ""layerinternal"", deref=True),\n    TypeReplacer(""PTR_GRU"", ""CGRU"",  ""GRU"", ""layerinternal"", deref=False),\n    WrapperReplacer(""WRAP_GRU"", \'WrapGRU_%s\'),\n\n    TypeReplacer(""DEREF_STACKEDLAYER"", ""CStackedInputLayer"", ""StackedInputLayer"", ""layerinternal"", deref=True),\n    TypeReplacer(""PTR_STACKEDLAYER"", ""CStackedInputLayer"", ""StackedInputLayer"", ""layerinternal"", deref=False),\n    WrapperReplacer(""WRAP_STACKEDLAYER"", \'WrapStackedLayer_%s\'),\n\n    TypeReplacer(""DEREF_LSTMSTATE"", ""CLSTMState"", ""LSTMState"", ""lstmstateinternal"", deref=True),\n    TypeReplacer(""PTR_LSTMSTATE"", ""CLSTMState"", ""LSTMState"", ""lstmstateinternal"", deref=False),\n    WrapperReplacer(""WRAP_LSTMSTATE"", \'WrapLSTMState_%s\'),\n\n    TypeReplacer(""DEREF_LSTM"", ""CLSTM"", ""LSTM"", ""layerinternal"", deref=True),\n    TypeReplacer(""PTR_LSTM"", ""CLSTM"", ""LSTM"", ""layerinternal"", deref=False),\n    WrapperReplacer(""WRAP_LSTM"", \'WrapLSTM_%s\'),\n\n    TypeReplacer(""DEREF_STACKEDLSTM"", ""CStackedLSTM"", ""StackedLSTM"", ""layerinternal"", deref=True),\n    TypeReplacer(""PTR_STACKEDLSTM"", ""CStackedLSTM"", ""StackedLSTM"", ""layerinternal"", deref=False),\n    WrapperReplacer(""WRAP_STACKEDLSTM"", \'WrapStackedLSTM_%s\'),\n\n    TypedName()\n]\n\nfor solver in [""SGD"", ""AdaGrad"", ""RMSProp"", ""AdaDelta"", ""Adam""]:\n    REPLACERS.append(\n        TypeReplacer(""DEREF_"" + solver.upper(), ""C"" + solver, solver, ""solverinternal"", deref=True)\n    )\n    REPLACERS.append(\n        TypeReplacer(""PTR_"" + solver.upper(), ""C"" + solver, solver, ""solverinternal"", deref=False)\n    )\n\nTYPE_NPYINTERNAL_DICT = {\n    \'int\':    \'np.NPY_INT32\',\n    \'float\':  \'np.NPY_FLOAT32\',\n    \'double\': \'np.NPY_FLOAT64\',\n}\n\nTYPE_NUMPY_PRETTY = {\n    \'int\':    \'np.int32\',\n    \'float\':  \'np.float32\',\n    \'double\': \'np.float64\',\n}\n\ndef modify_snippet(pyp, code, type_name):\n    modified = code\n    modified = modified.replace(\'TYPE_NAME\',       type_name)\n    modified = modified.replace(\'TYPE_NPYINTERNAL\', TYPE_NPYINTERNAL_DICT.get(type_name))\n    modified = modified.replace(\'TYPE_NPYPRETTY\', TYPE_NUMPY_PRETTY.get(type_name))\n\n    for replacer in REPLACERS:\n        modified = replacer(type_name, modified)\n\n    pyp.indent(modified)\n\n\n\ndef type_repeat_with_types(pyp, types, code):\n    for typ in types:\n        modify_snippet(pyp, code, typ)\n\ndef type_repeat(pyp, code):\n    type_repeat_with_types(pyp, [""int"", ""float"", ""double""], code)\n\ndef type_frepeat(pyp, code):\n    type_repeat_with_types(pyp, [""float"", ""double""], code)\n\ndef typed_expression_args_with_types(pyp, types, args, code):\n    if type(args) == tuple:\n        args_class\n    assert len(args) > 0\n    if len(args) > 1:\n        check_str = []\n        for arg1, arg2 in zip(args[:-1], args[1:]):\n            check_str.append(\'(%s).dtypeinternal != (%s).dtypeinternal\' % (arg1, arg2))\n        check_str = \'if \' + \' or \'.join(check_str) + \':\'\n        pyp.indent(check_str)\n        pyp.indent(\'   raise ValueError(""All arguments must be of the same type"")\')\n\n    first_run = True\n    for typ in types:\n        if_str = \'if\' if first_run else \'elif\'\n        first_run = False\n        pyp.indent(if_str + \' (%s).dtypeinternal == %s:\' % (args[0], TYPE_NPYINTERNAL_DICT[typ]))\n        modify_snippet(pyp, code, typ)\n    pyp.indent(\'else:\')\n    types_str = \', \'.join([TYPE_NUMPY_PRETTY[typ] for typ in types])\n    pyp.indent(\'    raise ValueError(""Invalid dtype:"" + str(\' + args[0] + \'.dtype) + "" (should be one of \' + types_str+ \')"")\')\n\ndef typed_expression_args(pyp, args, code):\n    typed_expression_args_with_types(pyp, [""int"", ""float"", ""double""], args, code)\n\ndef typed_fexpression_args(pyp, args, code):\n    typed_expression_args_with_types(pyp, [""float"", ""double""], args, code)\n\ndef typed_expressions_with_types(pyp, lst, cast_to, types, code):\n    assert len(lst) > 0\n    pyp.indent(\'if len(%s) == 0:\' % (lst,))\n    pyp.indent(""    raise ValueError(\'list cannot be empty\')"")\n    pyp.indent(\'common_dtype = (<%s>(%s[0])).dtypeinternal\' % (cast_to, lst,))\n    pyp.indent(\'for el in %s:\' % (lst,))\n    pyp.indent(\'    if (<%s>el).dtypeinternal != common_dtype:\' % (cast_to,))\n    pyp.indent(\'        common_dtype = -1\')\n    pyp.indent(\'        break\')\n    pyp.indent(\'if common_dtype == -1:\')\n    pyp.indent(\'    raise ValueError(""All the arguments must be of the same type"")\')\n\n    first_run = True\n    for typ in types:\n        if_str = \'if\' if first_run else \'elif\'\n        first_run = False\n        pyp.indent(if_str + \' common_dtype == %s:\' % (TYPE_NPYINTERNAL_DICT[typ],))\n        modify_snippet(pyp, code, typ)\n    pyp.indent(\'else:\')\n    types_str = \', \'.join([TYPE_NUMPY_PRETTY[typ] for typ in types])\n    pyp.indent(\'    raise ValueError(""Invalid dtype:"" + str(\' + lst + \'[0].dtype) + "" (should be one of \' + types_str+ \')"")\')\n\n\ndef typed_expression_list(pyp, lst, cast_to, code):\n    typed_expressions_with_types(pyp, lst, cast_to, [""int"", ""float"", ""double""], code)\n\ndef typed_fexpression_list(pyp, lst, cast_to, code):\n    typed_expressions_with_types(pyp, lst, cast_to, [""float"", ""double""], code)\n\ndef typed_expression(pyp, code):\n    return typed_expression_args(pyp, [""self""], code)\n\ndef typed_fexpression(pyp, code):\n    return typed_fexpression_args(pyp, [""self""], code)\n\ndef rich_typed_expression(pyp, replacable_type, code):\n    def modify_snippet(type_name):\n        modified = code\n        modified = modified.replace(\'TYPE_NAME\', type_name)\n        modified = modified.replace(\'TEMPLATED_TYPE\', \'%s[%s]\' % (replacable_type, type_name))\n        modified = modified.replace(\'TEMPLATED_CAST\', \'<%s[%s]>\' % (replacable_type, type_name))\n        pyp.indent(modified)\n\n    pyp.indent(\'if self.dtypeinternal == np.NPY_INT32:\')\n    modify_snippet(\'int\')\n    pyp.indent(\'elif self.dtypeinternal == np.NPY_FLOAT32:\')\n    modify_snippet(\'float\')\n    pyp.indent(\'elif self.dtypeinternal == np.NPY_FLOAT64:\')\n    modify_snippet(\'double\')\n    pyp.indent(\'else:\')\n    pyp.indent(\'    raise ValueError(""Invalid dtype:"" + str(self.dtype) + "" (should be one of int32, float32, float64)"")\')\n\nWITHOUT_INT = [""float"", ""double""]\n'"
setup.py,1,"b'import distutils.ccompiler\nimport distutils.sysconfig\nimport numpy as np\nimport preprocessor\nimport subprocess\n\nfrom Cython.Distutils.extension import Extension\nfrom Cython.Distutils           import build_ext\nfrom distutils.core             import setup\nfrom distutils.command          import build as build_module, clean as clean_module\nfrom distutils.spawn            import find_executable\nfrom os.path                    import join, dirname, realpath, exists, getmtime, relpath\nfrom os                         import environ, walk, makedirs\nfrom sys                        import platform, exit\n\n\nfrom tempfile import TemporaryDirectory\n\nSCRIPT_DIR = dirname(realpath(__file__))\nDALI_CORE_DIR    = join(SCRIPT_DIR, ""cython"", ""dali"", ""core"")\nDALI_CORE_MODULE = ""dali.core""\n\n################################################################################\n##                               TOOLS                                        ##\n################################################################################\n\ndef find_extension_files(path, extension):\n    """"""Recursively find files with specific extension in a directory""""""\n    for relative_path, dirs, files in walk(path):\n        for fname in files:\n            if fname.endswith(extension):\n                yield join(path, relative_path, fname)\n\ndef execute_bash(command, *args, **kwargs):\n    """"""Executes bash command, prints output and throws an exception on failure.""""""\n    #print(subprocess.check_output(command.split(\' \'), shell=True))\n    process = subprocess.Popen(command,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.STDOUT,\n                               universal_newlines=True,\n                               *args, **kwargs)\n    process.wait()\n    return str(process.stdout.read()), process.returncode\n\n################################################################################\n##                 STEALING LINKING ARGS FROM CMAKE                           ##\n################################################################################\n\ndef cmake_robbery(varnames, fake_executable=""dummy""):\n    """"""Capture Cmake environment variables by running `find_package(dali)`""""""\n    varstealers = []\n    magic_command = ""CYTHON_DALI_BEGIN_VARIABLE_STEALING""\n    varstealers.append(""message(STATUS \\""%s\\"")"" % (magic_command,))\n    for varname in varnames:\n        varstealers.append(""message(STATUS  \\""CYTHON_DALI_%s: ${%s}\\"")"" % (varname, varname,))\n    varstealers = ""\\n"".join(varstealers) + ""\\n""\n\n    with TemporaryDirectory() as temp_dir:\n        with open(join(temp_dir, ""source.cpp""), ""wt"") as source_cpp:\n            source_cpp.write(""int main() {};\\n"")\n        with open(join(temp_dir, ""CMakeLists.txt""), ""wt"") as cmake_conf:\n            cmake_conf.write(""""""\n                cmake_minimum_required(VERSION 2.8 FATAL_ERROR)\n                project(""dali-cython"")\n                find_package(Dali REQUIRED) # find Dali.\n                add_executable(%s source.cpp)\n                target_link_libraries(%s ${DALI_AND_DEPS_LIBRARIES})\n            """""" % (fake_executable, fake_executable,) + varstealers)\n\n        cmake_subdirectory = fake_executable + "".dir""\n        cmake_stdout, cmake_status = execute_bash([""cmake"", "".""], cwd=temp_dir)\n        if cmake_status != 0:\n            print(""HORRIBLE CMAKE ERROR."")\n            print(\'*\' * 79)\n            print(cmake_stdout)\n            print(\'*\' * 79)\n            exit(1)\n        # capture the link arguments\n        with open(join(temp_dir, ""CMakeFiles"", cmake_subdirectory, ""link.txt""), ""rt"") as f:\n            linking_command = f.read()\n\n    linking_command = linking_command.replace(""-o %s"" % (fake_executable,), "" "")\n    linking_args = linking_command.split("" "", 1)[1].strip().split()\n    linking_args = [arg for arg in linking_args if cmake_subdirectory not in arg]\n    outvars = {}\n    outvars[""LINK_ARGS""] = linking_args\n\n    # slice output after the magic command and retrieve these variables\n    # from the CMake environment\n    idx = cmake_stdout.find(magic_command) + len(magic_command) + 1\n    lines = cmake_stdout[idx:].split(""\\n"")[:len(varnames)]\n\n    for varname, line in zip(varnames, lines):\n        assert(varname in line)\n        _, value = line.split("":"", 1)\n        outvars[varname] = value.strip().split("";"")\n    return outvars\n\n# cmake environment variables\nrobbed = cmake_robbery([""DALI_AND_DEPS_INCLUDE_DIRS""])\n\n################################################################################\n##                 AUTODETECTING COMPILER VERSION                             ##\n################################################################################\n\nclass Version(tuple):\n    @staticmethod\n    def from_string(version_str):\n        return Version([int(n) for n in version_str.split(\'.\')])\n\n    def __str__(self):\n        return \'.\'.join([str(n) for n in self])\n\ndef detect_compiler(possible_commands, version_extractor, min_version):\n    good_executable = None\n    good_version    = None\n    for command in possible_commands:\n        executable = find_executable(command)\n        if executable is None: continue\n        version = version_extractor(executable)\n        if version is None: continue\n        if version >= min_version:\n            good_executable = executable\n            good_version    = version\n            break\n    return good_executable, good_version\n\ndef obtain_gxx_version(gcc_executable):\n    try:\n        gcc_version, status = execute_bash([gcc_executable, \'-dumpversion\'])\n        assert status == 0\n        return Version.from_string(gcc_version)\n    except Exception:\n        return None\n\nGXX_VERSION_ERROR = \\\n""""""Minimum required version of gcc/g++ must is %s.\n\nWe strive to cover all the cases for automatic compiler detection,\nhowever if we failed to detect yours please kindly report it on github.\n\nYou can explicitly specify an executables by running:\n\n    CC=/path/to/my/gcc CXX=/path/to/my/g++ python3 setup.py ...\n\n""""""\n\n# set the compiler unless explicitly specified.\nif platform == \'linux\':\n    for env_var, possible_commands, min_version in [\n                (\'CC\',  [\'gcc\', \'gcc4.9\', \'gcc-4.9\'], Version((4, 9))),\n                (\'CXX\', [\'g++\', \'g++4.9\', \'g++-4.9\'], Version((4, 9))),\n            ]:\n        if env_var not in environ:\n            gxx_executable, gxx_version = detect_compiler(possible_commands, obtain_gxx_version, min_version)\n            if gxx_executable is None:\n                print(GXX_VERSION_ERROR % (str(min_version),))\n                exit(2)\n            else:\n                print(\'Autodetected %s executable %s, version: %s\' % (env_var, gxx_executable, str(gxx_version)))\n                environ[env_var] = gxx_executable\nelse:\n    if ""CC"" not in environ:\n        environ[""CC""]  = ""clang""\n    if ""CXX"" not in environ:\n        environ[""CXX""] = ""clang++""\n\n################################################################################\n##                      TAKING OUT THE TRASH                                  ##\n################################################################################\n\n\n# Make a `cleanall` rule to get rid of intermediate and library files\nclass clean(clean_module.clean):\n    def run(self):\n        print(""Cleaning up cython files..."")\n        # Just in case the build directory was created by accident,\n        # note that shell=True should be OK here because the command is constant.\n        for place in [""build"", ""cython/dali/core.c"", ""cython/dali/core.cpp"", ""dali/*.so"", ""MANIFEST.in""]:\n            subprocess.Popen(""rm -rf %s"" % (place,), shell=True, executable=""/bin/bash"", cwd=SCRIPT_DIR)\n\ncompiler = distutils.ccompiler.new_compiler()\ndistutils.sysconfig.customize_compiler(compiler)\nBLACKLISTED_COMPILER_SO = [\'-Wp,-D_FORTIFY_SOURCE=2\']\nbuild_ext.compiler = compiler\n\next_modules = [Extension(\n    name=DALI_CORE_MODULE,\n    sources=[join(SCRIPT_DIR, ""cython"", ""dali"", ""core.pyx"")] + list(find_extension_files(DALI_CORE_DIR, "".cpp"")),\n    library_dirs=[],\n    language=\'c++\',\n    extra_compile_args=[\'-std=c++11\'],\n    extra_link_args=robbed[""LINK_ARGS""],\n    libraries=[],\n    extra_objects=[],\n    include_dirs=[np.get_include()] + robbed[""DALI_AND_DEPS_INCLUDE_DIRS""]\n)]\n\n\n################################################################################\n##       PREPROCSSOR - HOW TO SHRINK DALI CYTHON CODE THREEFOLD               ##\n################################################################################\n\n\ndef run_preprocessor():\n    """"""\n    Generate python files using a file prepocessor (essentially macros\n    that generate multiple versions of the code for each dtype supported\n    by a Dali operation)\n    """"""\n    EXTENSION = "".pre""\n    for py_processor_file in find_extension_files(SCRIPT_DIR, EXTENSION):\n        output_file = py_processor_file[:-len(EXTENSION)]\n\n        if not exists(output_file) or \\\n                getmtime(py_processor_file) > getmtime(output_file) or \\\n                getmtime(join(SCRIPT_DIR, ""preprocessor_utils.py"")) > getmtime(output_file):\n            print(\'Preprocessing %s\' % (py_processor_file,))\n            with open(output_file, ""wt"") as f:\n                f.write(preprocessor.process_file(py_processor_file, prefix=\'pyp\', suffix=\'ypy\'))\n\n\n\n################################################################################\n##                 POSSIBLY NO LONGER NEEDED                                  ##\n################################################################################\n\n\n# We need to remove some compiler flags, to make sure\n# the code can compile on Fedora (to be honest it seems\n# to be a bug in Fedora\'s distrubtion of Clang).\n# Nevertheless this little madness below is to change\n# default compiler flags used by Cython.\n# If you know a better way call me immediately day\n# or night at 4getszymo4. Thank you!\nclass nonbroken_build_ext(build_ext):\n    def build_extensions(self, *args, **kwargs):\n        run_preprocessor()\n        new_compiler_so = []\n        for arg in self.compiler.compiler_so:\n            if arg not in BLACKLISTED_COMPILER_SO:\n                new_compiler_so.append(arg)\n        self.compiler.compiler_so = new_compiler_so\n        super(nonbroken_build_ext, self).build_extensions(*args, **kwargs)\n\n\n################################################################################\n##                 FIND ALL THE FILES AND CONFIGURE SETUP                     ##\n################################################################################\n\n# generate manifest.in\npre_files = list(find_extension_files(SCRIPT_DIR, "".pre""))\npyx_files = list(find_extension_files(SCRIPT_DIR, "".pyx""))\n# check that this file was not auto-generated\npyx_files = [fname for fname in pyx_files if fname + "".pre"" not in pre_files]\ncpp_files = list(find_extension_files(DALI_CORE_DIR, "".cpp""))\nheader_files = list(find_extension_files(DALI_CORE_DIR, "".h""))\npxd_files = (\n    list(find_extension_files(join(SCRIPT_DIR, ""libcpp11""), "".pxd"")) +\n    list(find_extension_files(join(SCRIPT_DIR, ""modern_numpy""), "".pxd""))\n)\n\nwith open(join(SCRIPT_DIR, ""MANIFEST.in""), ""wt"") as manifest_in:\n    for fname in pre_files + pyx_files + cpp_files + header_files + pxd_files + [join(SCRIPT_DIR, ""preprocessor_utils.py"")]:\n        manifest_in.write(""include %s\\n"" % (relpath(fname, SCRIPT_DIR)))\n\nsetup(\n  name=""dali"",\n  version=\'1.0.9\',\n  cmdclass={""build_ext"": nonbroken_build_ext, \'clean\': clean},\n  ext_modules=ext_modules,\n  description=""Buttery smooth automatic differentiation using Dali."",\n  author=""Jonathan Raiman, Szymon Sidor"",\n  author_email=""jonathanraiman at gmail dot com"",\n  install_requires=[\n    \'preprocessor\',\n    \'numpy\',\n    \'dill\'\n  ],\n  packages=[\n    ""dali"",\n    ""dali.utils"",\n    ""dali.data"",\n    ""dali.models"",\n  ]\n)\n'"
dali/__init__.py,0,b'import dali.utils\nimport dali.data\n\nfrom .beam_search import *\n'
dali/activation.py,0,"b'""""""\nThe point of this file is to make those\nfunctions pickle friendly\n""""""\nfrom dali.core import MatOps as ops\n\nclass tanh_object(object):\n    def __call__(self, *args, **kwargs):\n        return ops.tanh(*args, **kwargs)\n\nclass relu_object(object):\n    def __call__(self, *args, **kwargs):\n        return ops.relu(*args, **kwargs)\n\nclass sigmoid_object(object):\n    def __call__(self, *args, **kwargs):\n        return ops.sigmoid(*args, **kwargs)\n\nclass identity_object(object):\n    def __call__(self, *args, **kwargs):\n        assert len(args) == 1\n        assert len(kwargs) == 0\n        return args[0]\n\ntanh     = tanh_object()\nrelu     = relu_object()\nsigmoid  = sigmoid_object()\nidentity = identity_object()\n\n__all__ = [""tanh"", ""relu"", ""sigmoid"", ""identity""]\n'"
dali/beam_search.py,0,"b'from collections import namedtuple\nfrom types import FunctionType\nimport dali.core as D\n\nBeam = namedtuple(""Beam"", [""solution"", ""score"", ""state""])\n\ndef beam_search(initial_state,\n                candidate_scores,\n                make_choice,\n                beam_width=5,\n                eos_symbol = None,\n                max_sequence_length=None,\n                blacklist=[]):\n\n    if beam_width <= 0:\n        raise ValueError(""Beam width must be positive, received: "" + str(beam_width))\n\n    iterations = 0\n    results = [\n        Beam([], D.Mat(1,1), initial_state)\n    ]\n\n    def lazy_beam(prev_beam, candidate, new_score):\n        def generate():\n            choice = make_choice(prev_beam.state, candidate)\n            if type(choice) == tuple:\n                choice_repr, new_state  = choice\n            else:\n                choice_repr, new_state = candidate, choice\n            return Beam(\n                prev_beam.solution + [choice_repr],\n                new_score,\n                new_state,\n            )\n        return generate\n\n    def lazy_identity(beam):\n        def generate():\n            return beam\n        return generate\n\n    if eos_symbol is None:\n        check_eos = lambda beam: False\n    elif type(eos_symbol) != FunctionType:\n        check_eos = lambda beam: len(beam.solution) > 0 and beam.solution[-1] == eos_symbol\n    else:\n        check_eos = eos_symbol\n\n    while max_sequence_length is None or iterations < max_sequence_length:\n        proposals = []\n        for beam in results:\n            if check_eos(beam):\n                proposals.append((beam.score, lazy_identity(beam)))\n            else:\n                scores = candidate_scores(beam.state)\n                sorted_candidates = D.MatOps.argsort(scores)\n\n                sorted_candidates = sorted_candidates[::-1]\n                candidates_remaining = beam_width\n                for candidate_idx in sorted_candidates:\n                    if candidate_idx in blacklist:\n                        continue\n                    new_score = beam.score + scores.T()[candidate_idx]\n                    proposals.append((new_score, lazy_beam(beam, candidate_idx, new_score)))\n                    candidates_remaining -= 1\n                    if candidates_remaining <= 0:\n                        break\n        proposals.sort(reverse=True, key=lambda x: x[0].w[0])\n        results = [ eval_beam() for _, eval_beam in proposals[:beam_width]]\n\n        iterations += 1\n\n    return results\n\n__all__ = [""beam_search"", ""Beam""]\n'"
examples/mlbasics_learn_to_add.py,0,"b'from test_dali import Mat, random, MatOps, Graph\n\nnum_examples = 100\nexample_size = 3\niterations   = 150\nlr           = 0.01\n\nX = random.uniform(\n    0.0,\n    1.0 / example_size,\n    size=(num_examples, example_size)\n)\nones = Mat.ones((X.shape[1], 1))\nY = X.dot(ones)\n\nX = MatOps.consider_constant(X)\nY = MatOps.consider_constant(Y)\n\nW = random.uniform(-1.0, 1.0, (example_size, 1))\nprint(repr(W))\nfor i in range(iterations):\n    predY = X.dot(W)\n    error = ((predY - Y) ** 2).sum()\n    print(repr(error))\n    # line below can be replaced by simply error.grad()\n    error.dw += 1\n    Graph.backward()\n    # there are much nicer solvers in Dali,\n    # but here we write out gradient descent\n    # explicitly\n    W.w -= W.dw * lr\n    W.dw = 0\nprint(repr(W))\n'"
examples/mlbasics_rnn_binary_addition.py,0,"b'import sys\nfrom os.path import dirname, realpath\nsys.path.append(dirname(dirname(realpath(__file__))))\n\nimport random\nfrom test_dali import Mat, MatOps, Graph, SGD, RNN, Layer\n\ndef as_bytes(num, final_size):\n    res = []\n    for _ in range(final_size):\n        res.append(num % 2)\n        num //= 2\n    return res\n\ndef generate_example(num_bits):\n    a = random.randint(0, 2**(num_bits - 1) - 1)\n    b = random.randint(0, 2**(num_bits - 1) - 1)\n    res = a + b\n    return (as_bytes(a,  num_bits),\n            as_bytes(b,  num_bits),\n            as_bytes(res,num_bits))\n\nITERATIONS_PER_EPOCH = 30\nNUM_BITS             = 30\nINPUT_SIZE           = 2\nOUTPUT_SIZE          = 1\nMEMORY_SIZE          = 5\nMAX_EPOCHS           = 5000\n\nrnn                  = RNN(INPUT_SIZE, MEMORY_SIZE)\nclassifier           = Layer(MEMORY_SIZE, OUTPUT_SIZE)\nrnn_initial          = Mat(1, MEMORY_SIZE)\n\nsolver               = SGD()\nsolver.step_size     = 0.001\nparams               = rnn.parameters() + classifier.parameters() + [rnn_initial]\n\nfor epoch in range(MAX_EPOCHS):\n    for _ in range(ITERATIONS_PER_EPOCH):\n        a, b, res = generate_example(NUM_BITS)\n        error = Mat.zeros((1,1))\n        prev_hidden = rnn_initial\n        for bit_idx in range(NUM_BITS):\n            input_i = Mat([a[bit_idx], b[bit_idx]], dtype=rnn.dtype)\n            prev_hidden = rnn.activate(input_i, prev_hidden).tanh()\n            #prev_hidden = (rnn.Wx.dot(input_i) + rnn.Wh.dot(prev_hidden) + rnn.b).tanh()\n            output_i    = classifier.activate(prev_hidden).sigmoid()\n            # print(repr(output_i))\n            error = error + MatOps.binary_cross_entropy(output_i, res[bit_idx])\n        error.grad()\n        Graph.backward()\n    if epoch % 20 == 0:\n        print(""epoch %d, error = %.3f"" % (epoch, error.w[0,0]))\n    solver.step(params)\n'"
tests/__init__.py,0,b''
tests/tests_beam_search.py,1,"b'import math\nimport unittest\n\nimport numpy as np\nimport dali.core as D\n\nfrom dali import beam_search, Beam\n\nclass BeamSearchTests(unittest.TestCase):\n    def test_letters(self):\n        MAX_LENGTH = 2\n        choices = {\n            #initial_choices\n            ""a"": 0.6,\n            ""b"": 0.4,\n            #after chosing a\n            ""aa"": 0.55,  # (total worth 0.33)\n            ""ab"": 0.45,  # (total worth 0.18)\n            #after choosing b\n            ""ba"": 0.99,  # (total worth 0.495)\n            ""bb"": 0.11,  # (total worth 0.044)\n        };\n\n        # Above example is designed to demonstrate greedy solution,\n        # as well as better optimal solution:\n        # GREEDY:    (beam_width == 1) => ""aa"" worth 0.33\n        # OPTIMAL:   (beam_width == 2) => ""ba"" worth 0.495\n        res_aa = Beam([0,0], D.Mat([math.log(0.6 * 0.55)]), ""aa"")\n        res_ab = Beam([0,1], D.Mat([math.log(0.6 * 0.45)]), ""ab"")\n        res_ba = Beam([1,0], D.Mat([math.log(0.4 * 0.99)]), ""ba"")\n        res_bb = Beam([1,1], D.Mat([math.log(0.4 * 0.11)]), ""bb"")\n\n        initial_state = """";\n        def candidate_scores(state):\n            ret = D.Mat(1,2)\n            ret.w[0,0] = math.log(choices[state + ""a""])\n            ret.w[0,1] = math.log(choices[state + ""b""])\n            return ret\n        def make_choice(prev_state, choice):\n            return prev_state + (""a"" if choice == 0 else ""b"")\n\n        def my_beam_search(beam_width):\n            return beam_search(initial_state=initial_state,\n                               candidate_scores=candidate_scores,\n                               make_choice=make_choice,\n                               beam_width=beam_width,\n                               max_sequence_length = MAX_LENGTH)\n\n        def beams_equal(b1, b2):\n            return (b1.solution == b2.solution and\n                    np.allclose(b1.score.w, b2.score.w) and\n                    b1.state == b2.state)\n\n        def results_equal(a,b):\n            return len(a) == len(b) and all(beams_equal(b1,b2) for b1,b2 in zip(a,b))\n\n        with self.assertRaises(ValueError):\n            my_beam_search(0)\n\n        self.assertTrue(results_equal(my_beam_search(1), [res_aa]))\n        self.assertTrue(results_equal(my_beam_search(2), [res_ba, res_aa]))\n        self.assertTrue(results_equal(my_beam_search(4), [res_ba, res_aa, res_ab, res_bb]))\n        self.assertTrue(results_equal(my_beam_search(10),[res_ba, res_aa, res_ab, res_bb]))\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/tests_data.py,0,"b'import dill as pickle\nimport mock\nimport unittest\n\nfrom dali.data import DiscoverFiles, Lines, BatchBenefactor\n\nfrom os.path import join, dirname, realpath\nSCRIPT_DIR = dirname(realpath(__file__))\n\nclass DataTests(unittest.TestCase):\n    @mock.patch(\'os.walk\')\n    def test_discover_files(self, patched_os_walk):\n        patched_os_walk.return_value = [\n            (\'test_dir\', [\'hi\'], [\'lol.py\', \'lol2.py\']),\n            (\'test_dir/hi\', [], [\'hello.py\', \'yikes.txt\']),\n        ]\n\n        x = DiscoverFiles(\'/what/ever\', \'.py\')\n\n        self.assertEqual(next(x), ""test_dir/lol.py"")\n        x_pickle = pickle.loads(pickle.dumps(x))\n\n        self.assertEqual(next(x), ""test_dir/lol2.py"")\n        self.assertEqual(next(x_pickle), ""test_dir/lol2.py"")\n\n        self.assertEqual(next(x), ""test_dir/hi/hello.py"")\n        self.assertEqual(next(x_pickle), ""test_dir/hi/hello.py"")\n\n        with self.assertRaises(StopIteration):\n            next(x)\n        with self.assertRaises(StopIteration):\n            next(x_pickle)\n\n    def test_lines(self):\n        r = Lines() \\\n            .lower()                                 \\\n            .split_spaces()                          \\\n            .bound_length(2,4)\n\n        r.set_file(join(SCRIPT_DIR, ""test.txt""))\n\n        self.assertEqual(next(r), [""ala"", ""ma"", ""kota""])\n\n        r2 = pickle.loads(pickle.dumps(r))\n\n        self.assertEqual(next(r), [""gdzie"", ""jest"",""ala"", ""?""])\n        self.assertEqual(next(r2), [""gdzie"", ""jest"",""ala"", ""?""])\n\n        with self.assertRaises(StopIteration):\n            next(r)\n        with self.assertRaises(StopIteration):\n            next(r2)\n\n\n    @mock.patch(\'random.shuffle\')\n    def test_batch_benefactor(self, patched_random_shuffle):\n        # random shuffle no longer shuffles ;-)\n\n        d = BatchBenefactor(2, lambda x: \' \'.join(x), 4)\n        d.add(""where"")\n        with self.assertRaises(StopIteration):\n            next(d)\n        d.add(""is"")\n        with self.assertRaises(StopIteration):\n            next(d)\n        d.add(""the"")\n        with self.assertRaises(StopIteration):\n            next(d)\n        d.add(""love"")\n        self.assertEqual(list(d), [""is the"", ""love where""])\n\n        d = pickle.loads(pickle.dumps(d))\n        d.add(""where"")\n        with self.assertRaises(StopIteration):\n            next(d)\n        d.add(""is"")\n        with self.assertRaises(StopIteration):\n            next(d)\n        d.add(""the"")\n        with self.assertRaises(StopIteration):\n            next(d)\n        d.add(""love"")\n        self.assertEqual(list(d), [""is the"", ""love where""])\n\n        d = pickle.loads(pickle.dumps(d))\n        d.update_minibatch_size(4)\n        d.add(""siema"")\n        with self.assertRaises(StopIteration):\n            next(d)\n        d.add(""is"")\n        with self.assertRaises(StopIteration):\n            next(d)\n        d.add(""polish"")\n        with self.assertRaises(StopIteration):\n            next(d)\n        d.add(""greeting"")\n        self.assertEqual(list(d), [""is siema polish greeting""])\n        with self.assertRaises(StopIteration):\n            next(d)\n'"
tests/tests_vocab.py,0,"b""import unittest\n\nfrom dali.utils import Vocab, VocabEncoded\n\nclass VocabTests(unittest.TestCase):\n    @classmethod\n    def setUp(self):\n        self.vocab = Vocab()\n        self.vocab.add([[{\n            'interesting_words': ['awesome', 'cat', 'lol'],\n            'daniel' : 'daniel',\n            'wtf':[[[[[[[[[[[['there']]]]]]]]]]]]\n        }]])\n\n        self.example =  {1:{1:{1:[[[[[ 'awesome', 'but','staph', 'daniel' ]]]]]}}}\n        self.example_unks = {1: {1: {1: [[[[['awesome', '**UNK**', '**UNK**', 'daniel']]]]]}}}\n\n\n    def test_addition(self):\n        self.assertEqual(set(self.vocab.words()),\n                         set(['awesome',\n                              'there',\n                              'daniel',\n                              '**UNK**',\n                              'cat',\n                              '**EOS**',\n                              'lol']))\n\n\n    def test_encode(self):\n        encoded  = self.vocab.encode(self.example, encode_type=VocabEncoded)\n        decoded  = self.vocab.decode(encoded, decode_type=VocabEncoded)\n        self.assertEqual(self.example_unks, decoded)\n\n    def test_encode_eos(self):\n        encoded  = self.vocab.encode(self.example, add_eos=True, encode_type=VocabEncoded)\n        decoded  = self.vocab.decode(encoded, strip_eos=True, decode_type=VocabEncoded)\n        assert self.example_unks == decoded\n"""
dali/data/__init__.py,0,b'from .process import *\nfrom . import batch\nfrom . import translation\n'
dali/data/batch.py,2,"b'import numpy as np\n\nimport dali.core as D\n\nclass Batch(object):\n    def __init__(self):\n        self.timesteps = 0\n        self.examples  = 0\n    def inputs(timestep):\n        return None\n    def targets(timestep):\n        return None\n    def __repr__(self):\n        return \'Batch(timesteps=%d, examples=%d)\' % (self.timesteps, self.examples)\n\nSTART_TOKEN = \'**START**\'\n\ndef create_lines_batch(lines, vocab, add_start_token=False, fill_eos=False, add_eos=False, align_right=False):\n    encoded_lines = []\n    for l in lines:\n        if type(l) == str:\n            l = l.split("" "")\n        if add_start_token:\n            l = [START_TOKEN] + l\n        encoded_lines.append(vocab.encode(l, add_eos=add_eos))\n\n\n    seq_length = max(map(len, encoded_lines))\n    # we add one index to account for start of sequence token\n    data = np.empty((seq_length, len(lines)), dtype=np.int32)\n    if fill_eos:\n        data.fill(vocab.eos)\n    else:\n        data.fill(0)\n\n    for line_idx, encoded_line in enumerate(encoded_lines):\n        if align_right:\n            data[-len(encoded_line):, line_idx] = encoded_line\n        else:\n            data[:len(encoded_line), line_idx] = encoded_line\n    data = D.Mat(data, borrow=True, dtype=np.int32)\n    return data\n\nclass LMBatch(object):\n    @staticmethod\n    def given_vocab(vocab, **kwargs):\n        def wrapper(sentences):\n            return LMBatch(sentences, vocab, **kwargs)\n        return wrapper\n\n    def __init__(self, sentences, vocab, store_originals=False, fill_eos=True, add_eos=True, add_start_token=True):\n        if store_originals:\n            self.sentences = sentences\n        self.sentence_lengths = [len(s) for s in sentences]\n        self.data = create_lines_batch(\n            sentences,\n            vocab,\n            add_start_token=add_start_token,\n            fill_eos=fill_eos,\n            add_eos=add_eos\n        )\n        self.timesteps = self.data.shape[0] - 1\n        self.examples  = self.data.shape[1]\n\n    def inputs(self, timestep):\n        return self.data[timestep]\n\n    def targets(self, timestep):\n         # predictions are offset by 1 to inputs, so\n        return self.data[timestep + 1]\n\n\nclass TranslationBatch(object):\n    @staticmethod\n    def given_vocabs(vocabs, **kwargs):\n        def wrapper(sentence_pairs):\n            return TranslationBatch(sentence_pairs, vocabs, **kwargs)\n        return wrapper\n\n\n    def __init__(self, sentence_pairs, vocabs, store_originals=False, input_add_eos=False, output_add_eos=True, add_start_token=False, reverse_input=True):\n        if store_originals:\n            self.sentence_pairs = sentence_pairs\n        from_sentences = [sentence_pair[0] for sentence_pair in sentence_pairs]\n        to_sentences   = [sentence_pair[1] for sentence_pair in sentence_pairs]\n\n        if reverse_input:\n            from_sentences = [list(reversed(s)) for s in from_sentences]\n\n\n        from_vocab, to_vocab = vocabs\n\n        from_eos_correction = (1 if input_add_eos else 0)\n        to_eos_correction   = (1 if output_add_eos else 0)\n\n        self.from_data = create_lines_batch(\n            from_sentences,\n            from_vocab,\n            add_start_token=add_start_token,\n            fill_eos=False,\n            add_eos=input_add_eos,\n            align_right=True\n        )\n\n        self.to_data = create_lines_batch(\n            to_sentences,\n            to_vocab,\n            add_start_token=add_start_token,\n            fill_eos=True,\n            add_eos=output_add_eos\n        )\n        self.from_tokens  = sum(map(len, from_sentences)) + from_eos_correction * len(from_sentences)\n        self.to_tokens    = sum(map(len, to_sentences))   + to_eos_correction  * len(to_sentences)\n\n        self.target_mask = D.Mat(*self.to_data.shape)\n        for example_idx, sentence in enumerate(to_sentences):\n            for ts in range(len(sentence) + to_eos_correction):\n                self.target_mask.w[ts, example_idx] = 1\n                self.target_mask.constant = True\n\n        self.from_timesteps = self.from_data.shape[0]\n        self.timesteps = self.from_data.shape[0] + self.to_data.shape[0]\n        self.examples  = len(sentence_pairs)\n\n    def inputs(self, timestep):\n        if timestep < self.from_timesteps:\n            return self.from_data[timestep]\n        else:\n            return None\n\n    def targets(self, timestep):\n        if timestep >= self.from_timesteps:\n            return self.to_data[timestep - self.from_timesteps]\n        else:\n            return None\n\n    def masks(self, timestep):\n        if timestep >= self.from_timesteps:\n            return self.target_mask[timestep - self.from_timesteps]\n        else:\n            return None\n'"
dali/data/process.py,0,"b'import os\nimport random\n\nfrom queue import Queue\n\nfrom dali.data.utils import split_punctuation as split_punctuation_f\n\nclass Process(object):\n    def __init__(self, files, mapper, reducer):\n        if files == str:\n            files = (make_me_iterator for make_me_iterator in [files])\n        self.files   = files\n        self.mapper  = mapper\n        self.reducer = reducer\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        while True:\n            try:\n                return next(self.reducer)\n            except StopIteration:\n                pass\n            # if we got here it means reducer run out of elements\n            # and we need to add more from mapper.\n            try:\n                self.reducer.add(next(self.mapper))\n                continue\n            except StopIteration:\n                pass\n            # if we got here it means that mapper run out of elements and we need to give\n            # if another file\n            next_file = next(self.files)\n            self.mapper.set_file(next_file)\n\nclass DiscoverFiles(object):\n    def __init__(self, root_path, extension=None):\n        self.files = []\n        self.next_file = 0\n        for path, dirs, files in os.walk(root_path):\n            for file in files:\n                if extension is None or file.endswith(extension):\n                    self.files.append(os.path.join(path, file))\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.next_file >= len(self.files):\n            raise StopIteration()\n        else:\n            self.next_file += 1\n            return self.files[self.next_file - 1]\n\n\nclass FileMapper(object):\n    FILTER      = 1\n    TRANSFORMER = 2\n\n    def __init__(self):\n        self._transformations = []\n        self.file_name = None\n        self.file_handle = None\n\n    def set_file(self, file_name):\n        if self.file_handle is not None:\n            self.file_handle.close()\n            self.file_handle = None\n        self.file_name = file_name\n        self.file_handle = None\n\n\n    def get_file(self, fargs=""rt""):\n        if self.file_handle is None:\n            self.file_handle = open(self.file_name, fargs)\n        return self.file_handle\n\n    def __del__(self):\n        if self.file_handle is not None:\n            self.file_handle.close()\n            self.file_handle = None\n\n    def __next__(self):\n        if self.file_name is None:\n            raise StopIteration()\n        while True:\n            item = self.next_item_no_filter()\n            if item is not None:\n                return item\n\n    def next_item_no_filter(self):\n        """"""By default __next__ does not even return an item\n           if it does not pass a filer. This function, returns\n           None even if the filters are not passing. This\n           is useful for example for translation where,\n           we want to ensure that two different file streams\n           are always aligned\n        """"""\n        if self.file_name is None:\n            raise StopIteration()\n\n        item = self.next_item()\n        item = self.transform_item(item)\n\n        return item\n\n    def transform_item(self, item):\n        for transform_f in self._transformations:\n            item = transform_f(item)\n            if item is None:\n                break\n        return item\n\n    def next_item(self):\n        raise StopIteration()\n\n\n    def add_filter(self, filter_f):\n        def wrapper(element):\n            if filter_f(element):\n                return element\n            return None\n        self.add_transform(wrapper)\n        return self\n\n    def add_transform(self, transform_f):\n        self._transformations.append(transform_f)\n        return self\n\n    def __iter__(self):\n        return self\n\n    def __getstate__(self):\n        # Copy the object\'s state from self.__dict__ which contains\n        # all our instance attributes. Always use the dict.copy()\n        # method to avoid modifying the original state.\n        state = self.__dict__.copy()\n        # Remove the unpicklable entries.\n        del state[\'file_handle\']\n        if self.file_handle is not None:\n            state[\'__file_position\'] = self.file_handle.tell()\n        return state\n\n    def __setstate__(self, state):\n        # Restore instance attributes (i.e., filename and lineno).\n        self.__dict__.update(state)\n        # Restore the previously opened file\'s state. To do so, we need to\n        # reopen it and read from it until the line count is restored.\n        self.file_handle = open(self.file_name)\n        if \'__file_position\' in state:\n            self.file_handle.seek(state[\'__file_position\'])\n\n\nclass Lines(FileMapper):\n    def next_item(self):\n        res = self.get_file().readline()\n        if len(res) == 0:\n            raise StopIteration()\n        if len(res) > 0 and res[-1] == \'\\n\':\n            return res[:-1]\n        else:\n            return res\n\n    def lower(self):\n        return self.add_transform(lambda x: x.lower())\n\n    def bound_length(self, lower_bound=None, upper_bound=None):\n        if lower_bound:\n            self.add_filter(lambda x: lower_bound <= len(x))\n        if upper_bound:\n            self.add_filter(lambda x: len(x) <= upper_bound)\n        return self\n\n    def split_spaces(self):\n        return self.add_transform(lambda x: x.split(\' \'))\n\n    def reverse(self):\n        return self.add_transform(lambda x: list(reversed(x)))\n\n    def split_punctuation(self):\n        return self.add_transform(split_punctuation_f)\n\n\nclass Multiplexer(object):\n    def __init__(self, *mappers):\n        self.mappers = mappers\n\n    def set_file(self, args):\n        assert len(args) == len(self.mappers)\n\n        for mapper, arg in zip(self.mappers, args):\n            mapper.set_file(arg)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        while True:\n            ret = tuple([m.next_item_no_filter() for m in self.mappers])\n            if all(ret_i is not None for ret_i in ret):\n                return ret\n\n\n\nclass BatchBenefactor(object):\n    def __init__(self, minibatch_size,\n                 minibatch_class=None,\n                 examples_until_minibatches=None,\n                 sorting_key=None):\n        self.minibatch_size = minibatch_size\n        self.minibatch_class = minibatch_class\n        self.examples_until_minibatches = examples_until_minibatches or minibatch_size\n        self.sorting_key = sorting_key\n\n        self.collected = []\n        self.batches = []\n        self.next_batch = 0\n\n    def add(self, element):\n        self.collected.append(element)\n\n        if len(self.collected) >= self.examples_until_minibatches:\n            self.batches = []\n            self.next_batch = 0\n\n            sorting_key = self.sorting_key or (lambda x: len(x))\n            self.collected.sort(key=sorting_key)\n\n\n            batch_start_idxes = list(range(0, len(self.collected), self.minibatch_size))\n            random.shuffle(batch_start_idxes)\n            for i in batch_start_idxes:\n                if i + self.minibatch_size <= len(self.collected):\n                    self.batches.append(self.collected[i:(i + self.minibatch_size)])\n            self.collected = []\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if len(self.batches) == 0 or self.next_batch == len(self.batches):\n            self.batches = []\n            self.next_batch = 0\n            raise StopIteration()\n        else:\n            assert self.next_batch < len(self.batches)\n            self.next_batch += 1\n            minibatch_class = self.minibatch_class or (lambda x:x)\n            return minibatch_class(self.batches[self.next_batch - 1])\n\n\n\n    def update_minibatch_size(self, minibatch_size):\n        self.minibatch_size = minibatch_size\n\n\nclass IdentityReducer(object):\n    def __init__(self):\n        self.q = Queue()\n\n    def add(self, element):\n        self.q.put(element)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.q.empty():\n            raise StopIteration()\n        else:\n            return self.q.get()\n'"
dali/data/translation.py,0,"b'from collections import defaultdict\n\nfrom .batch import TranslationBatch\nfrom .process import Process, Multiplexer, Lines, DiscoverFiles, IdentityReducer, BatchBenefactor\nfrom dali.utils import Vocab\n\n\nclass TranslationFiles(object):\n    def __init__(self, root_path, from_lang, to_lang):\n        files_from = set(DiscoverFiles(root_path, ""."" + from_lang))\n        files_to   = set(DiscoverFiles(root_path, ""."" + to_lang))\n\n        self.pairs = []\n        for file_name in files_from:\n            pref = file_name[:-(len(from_lang) + 1)]\n            hypothetical_to_file = pref + \'.\' + to_lang\n            if hypothetical_to_file in files_to:\n                self.pairs.append((file_name, hypothetical_to_file))\n\n        self.next_pair = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.next_pair >= len(self.pairs):\n            raise StopIteration()\n        else:\n            self.next_pair += 1\n            return self.pairs[self.next_pair - 1]\n\n\ndef TranslationMapper(sentence_bounds=(None, None)):\n    def translation_lines():\n        lines =  Lines()                         \\\n                 .split_punctuation()            \\\n                 .split_spaces()                 \\\n                 .bound_length(*sentence_bounds)\n        return lines\n\n\n    return Multiplexer(translation_lines(), translation_lines())\n\n\ndef build_vocabs(path, from_lang, to_lang, from_max_size=None, to_max_size=None):\n    from_occurence = defaultdict(lambda: 0)\n    to_occurence   = defaultdict(lambda: 0)\n    try:\n        for from_sentence, to_sentence in Process(files=TranslationFiles(path, from_lang, to_lang),\n                                                  mapper=TranslationMapper(sentence_bounds=(None, None)),\n                                                  reducer=IdentityReducer()):\n            for word in from_sentence:\n                from_occurence[word] += 1\n\n            for word in to_sentence:\n                to_occurence[word] += 1\n    except KeyboardInterrupt:\n        print(\'Impatient User Detected, file processing halted, proceeding to build vocab.\')\n\n\n    from_occurence = list(from_occurence.items())\n    to_occurence   = list(to_occurence.items())\n\n    # highest occurrence first\n    from_occurence.sort(key=lambda x: x[1], reverse=True)\n    to_occurence  .sort(key=lambda x: x[1], reverse=True)\n\n    # remove occurences, keep sorted words\n    from_occurence = [x[0] for x in from_occurence]\n    to_occurence =   [x[0] for x in to_occurence]\n\n    from_vocab = Vocab(from_occurence[:from_max_size])\n    to_vocab   = Vocab(to_occurence[:to_max_size])\n\n    return from_vocab, to_vocab\n\ndef iterate_examples(root_path, from_lang, to_lang, vocabs, minibatch_size, reverse_input=True, sentences_until_minibatch=None, sentence_length_bounds=(None, None)):\n    sentences_until_minibatch = sentences_until_minibatch or 10000 * minibatch_size\n    files   = TranslationFiles(root_path, from_lang, to_lang)\n    mapper = TranslationMapper(sentence_bounds=sentence_length_bounds)\n    sorting_key = lambda sentence_pair: (len(sentence_pair[0]), len(sentence_pair[1])) # sort by length of the input sentence first and then by the length of the output sentence\n\n    reducer = BatchBenefactor(minibatch_size,\n                              TranslationBatch.given_vocabs(vocabs, store_originals=True, reverse_input=reverse_input),\n                              sentences_until_minibatch,\n                              sorting_key=sorting_key)\n    return Process(files=files, mapper=mapper, reducer=reducer)\n'"
dali/data/utils.py,0,"b'PUNCTUATION_CHARS = set(list(\'.,?!-""\\\'()[]{}:;\'))\n\ndef split_punctuation(sentence, punctuation=PUNCTUATION_CHARS):\n    res = []\n    for i, char in enumerate(list(sentence)):\n        if char in punctuation:\n            if i - 1 >= 0 and sentence[i-1] != \' \':\n                res.append(\' \')\n            res.append(char)\n            if i + 1 < len(sentence) and sentence[i + 1] != \' \':\n                res.append(\' \')\n        else:\n            res.append(char)\n    return \'\'.join(res)\n'"
dali/models/__init__.py,0,b'from .mlp import *\n'
dali/models/mlp.py,0,"b'import dali.core as D\n\nclass MLP(object):\n    def __init__(self, input_sizes, hiddens, nonlinearities):\n        self.input_sizes = input_sizes\n        self.hiddens = hiddens\n        self.input_nonlinearity, self.layer_nonlinearities = nonlinearities[0], nonlinearities[1:]\n\n        self.input_layer = D.StackedInputLayer(input_sizes, hiddens[0])\n        self.layers = [D.Layer(h_from, h_to) for h_from, h_to in zip(hiddens[:-1], hiddens[1:])]\n\n    def activate(self, inputs):\n        assert len(self.layers) == len(self.layer_nonlinearities)\n        hidden = self.input_nonlinearity(self.input_layer.activate(inputs))\n        for l, nonlinearity in zip(self.layers, self.layer_nonlinearities):\n            hidden = nonlinearity(l.activate(hidden))\n        return hidden\n\n    def parameters(self):\n        ret = self.input_layer.parameters()\n        for l in self.layers:\n            ret.extend(l.parameters())\n        return ret\n\n    def name_parameters(self, prefix):\n        self.input_layer.name_parameters(prefix + ""_input_layer"")\n        for layer_idx, layer in enumerate(self.layers):\n            layer.name_parameters(prefix + \'_layer%d\' % (layer_idx,))\n'"
dali/utils/__init__.py,0,b'from .misc import *\nfrom .throttled import *\nfrom .vocab import *\nfrom .solver import *\nfrom .capture import *\n'
dali/utils/capture.py,0,"b'from collections import defaultdict\n\nclass Capture(object):\n    instances = set()\n\n    def __init__(self):\n        self.state = defaultdict(lambda: [])\n\n    @classmethod\n    def add(cls, name, value):\n        for instance in cls.instances:\n            instance.state[name].append(value)\n\n    def __enter__(self):\n        Capture.instances.add(self)\n\n    def __exit__(self, *args, **kwargs):\n        Capture.instances.remove(self)\n'"
dali/utils/misc.py,1,"b'import dill as pickle\nimport inspect\nimport numpy as np\nimport types\n\nfrom os import makedirs, listdir\nfrom os.path import join, exists\n\nimport dali.core as D\n\nclass RunningAverage(object):\n    def __init__(self, alpha=0.95):\n        self.alpha = alpha\n        self.value = None\n\n    def update(self, measurement):\n        if self.value is None:\n            self.value = measurement\n        else:\n            self.value = (self.alpha * self.value +\n                         (1.0 - self.alpha) * measurement)\n\n    def __float__(self):\n        return float(self.value)\n\n\ndef apply_recursively_on_type(x, f, target_type, list_callback=None):\n    if type(x) == target_type:\n        return f(x)\n    elif type(x) == list or isinstance(x, types.GeneratorType):\n        ret = [ apply_recursively_on_type(el, f, target_type, list_callback) for el in x]\n        if list_callback and all(type(el) == target_type for el in x):\n            ret = list_callback(ret)\n        return ret\n    elif type(x) == dict:\n        res = {}\n        for k,v in x.items():\n            res[k] = apply_recursively_on_type(v, f, target_type, list_callback)\n        return res\n    else:\n        return x\n\ndef integer_ceil(a, b):\n    return (a + b - 1) // b\n\ndef subsample(seq, maximum_length):\n    if seq == []:\n        return seq\n    return seq[::integer_ceil(len(seq), maximum_length)]\n\ndef median_smoothing(signal, window=10):\n    res = []\n    for i in range(window, len(signal)):\n        actual_window = signal[i-window:i]\n        res.append(np.median(actual_window))\n    return res\n\ndef pickle_from_scope(directory, variables, caller_globals=None, caller_locals=None):\n    if not exists(directory):\n        makedirs(directory)\n\n    if caller_globals is None or caller_locals is None:\n        stack = inspect.stack()\n        if caller_globals is None:\n            caller_globals = stack[1][0].f_globals\n        if caller_locals is None:\n            caller_locals  = stack[1][0].f_locals\n        del stack\n\n    for var in variables:\n        with open(join(directory, var + "".pkz""), ""wb"") as f:\n            value = caller_locals.get(var) or caller_globals.get(var)\n            assert value is not None\n            pickle.dump(value, f)\n\ndef unpickle_as_dict(directory, whitelist=None, extension=\'.pkz\'):\n    assert exists(directory)\n\n    res = {}\n\n    for file_name in listdir(directory):\n        if file_name.endswith(extension):\n            var_name = file_name[:-len(extension)]\n            if whitelist is None or var_name in whitelist:\n                with open(join(directory, file_name), ""rb"") as f:\n                    res[var_name] = pickle.load(f)\n\n    return res\n\ndef add_device_args(parser):\n    parser.add_argument(""--device"",    type=str, default=\'gpu\', choices=[\'gpu\',\'cpu\'], help=""Whether model should run on GPU or CPU."")\n    parser.add_argument(""--gpu_id"",    type=int, default=0, help=""Which GPU to use (zero-indexed just like in CUDA APIs)"")\n\ndef set_device_from_args(args, verbose=False):\n    D.config.default_device = args.device\n    if args.device == \'gpu\':\n        D.config.default_gpu = args.gpu_id\n        if verbose:\n            print(""Using %s"" % (D.config.gpu_id_to_name(args.gpu_id)))\n\n__all__ = [\n    ""apply_recursively_on_type"",\n    ""integer_ceil"",\n    ""subsample"",\n    ""median_smoothing"",\n    ""pickle_from_scope"",\n    ""unpickle_as_dict"",\n    ""RunningAverage"",\n    ""add_device_args"",\n    ""set_device_from_args""\n]\n'"
dali/utils/scoring.py,2,"b'import numpy as np\nimport os\nimport subprocess\nimport stat\nimport tempfile\n\nfrom os.path import join, exists\nfrom urllib.request import urlretrieve\n\nfrom dali.core import Mat\nfrom .misc import subsample, median_smoothing\n\nMULTIBLEU_SCRIPT = \'multi-bleu.perl\'\nMULTIBLEU_URL = ""https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/generic/multi-bleu.perl""\n\nclass ErrorTracker(object):\n    def __init__(self):\n        self.epoch_error = []\n        self.error_evolution = []\n\n    def append(self, error):\n        if type(error) == Mat:\n            self.epoch_error.append(error.w[0,0])\n        elif type(error) == np.ndarray:\n            self.epoch_error.append(error[0,0])\n        else:\n            self.epoch_error.append(error)\n\n    def finalize_epoch(self):\n        if len(self.epoch_error) > 0:\n            self.epoch_error = subsample(self.epoch_error, maximum_length=1000)\n            self.error_evolution.append(self.epoch_error)\n            self.epoch_error = []\n\n    def raw(self):\n        x = []\n        y = []\n        for epoch_no, error_epoch in enumerate(self.error_evolution):\n            x.extend(float(epoch_no) + float(t) / len(error_epoch) for t in range(len(error_epoch)))\n            y.extend(error_epoch)\n\n        if len(y) > 100:\n            y = median_smoothing(y, 30)\n            x = x[:len(y)]\n        return x,y\n\n    def num_epochs(self):\n        return len(self.error_evolution)\n\n    def recent(self, tsteps=1):\n        if len(self.epoch_error) == 0:\n            return np.nan\n        else:\n            recent = self.epoch_error[-tsteps:]\n            return sum(recent)/len(recent)\n\ndef bleu(reference, hypotheses, script_location=None):\n    if script_location is None:\n        script_location = join(tempfile.gettempdir(), MULTIBLEU_SCRIPT)\n        if not exists(script_location):\n            urlretrieve(MULTIBLEU_URL, script_location)\n    else:\n        assert(exists(script_location))\n\n\n    def process_input(val):\n        if type(val) == str:\n            assert exists(val)\n            return open(val)\n        elif type(val) == list:\n            ret = tempfile.NamedTemporaryFile(""wt"")\n            for example in val:\n                if len(example) > 0 and example[-1] == \'\\n\':\n                    ret.write(example)\n                else:\n                    ret.write(example + \'\\n\')\n            ret.seek(0)\n            return ret\n\n    try:\n        reference  = process_input(reference)\n        hypotheses = process_input(hypotheses)\n        prefix = ""BLEU = ""\n\n        try:\n            res_str = subprocess.check_output([script_location, reference.name], stdin=hypotheses, universal_newlines=True)\n        except PermissionError:\n            current_permissions = os.stat(script_location).st_mode\n            os.chmod(script_location, current_permissions | stat.S_IEXEC)\n            res_str = subprocess.check_output([script_location, reference.name], stdin=hypotheses, universal_newlines=True)\n\n        assert(res_str.startswith(prefix))\n        res_str = res_str[len(prefix):]\n        res_str = res_str.split(\',\')[0]\n        return float(res_str)\n    finally:\n        if hasattr(reference, \'close\'):\n            reference.close()\n        if hasattr(hypotheses, \'close\'):\n            hypotheses.close()\n\n__all__ = [\n    ""ErrorTracker"",\n    ""bleu""\n]\n\n\n\n\n'"
dali/utils/solver.py,0,"b'import copy\nfrom dali.core import Mat, MatOps\nfrom dali.utils.throttled import Throttled\n\nclass SolverBase(object):\n    t = Throttled(1)\n    known_solvers = [\n        \'sgd\',\n        \'adagrad\',\n        \'rmsprop\',\n        \'rmsprop_momentum\',\n        \'adadelta\',\n        \'adam\',\n    ]\n    known_params = [\n        \'learning_rate\',\n        \'clipval\',\n        \'regc\',\n        \'smooth_eps\',\n        \'rho\',\n        \'b1\',\n        \'b2\',\n        \'decay_rate\',\n        \'gradient_normalization\',\n        \'debug\',\n    ]\n\n    def __init__(self, solver_type, **kwargs):\n        self.solver_type = solver_type\n        if solver_type not in SolverBase.known_solvers:\n            raise AttributeError(""Unknown solver "" + str(solver_type))\n        self.kwargs = kwargs\n        for key in kwargs:\n            if key not in SolverBase.known_params:\n                raise AttributeError(""Unknown keyword argument "" + key)\n\n    def get_arg(self, override, name, default_val):\n        if name in override:\n            return override[name]\n        if name in self.kwargs:\n            return self.kwargs[name]\n        return default_val\n\n    def param_to_cache(self, param):\n        if \'solver_cache\' not in param.extra_state:\n            param.extra_state[\'solver_cache\'] = {}\n        return param.extra_state[\'solver_cache\']\n\n    def step(self, params, param_caches=None, **kwargs_override):\n        if type(params) != list:\n            params = [params]\n            if param_caches is not None:\n                param_caches = [param_caches]\n\n        if param_caches is None:\n            param_caches = [None for _ in range(params)]\n        assert len(params) == len(param_caches)\n\n        for key in kwargs_override:\n            if not key in SolverBase.known_params:\n                raise AttributeError(""Unknown keyword argument "" + key)\n\n        debug = self.get_arg(kwargs_override, \'debug\', [\'nans\'])\n        clip_val = self.get_arg(kwargs_override, \'clipval\', 5.0)\n        regc     = self.get_arg(kwargs_override, \'regc\',     0.0)\n\n        ########## GRADIENT NORMALIZATION ###########\n\n        gradient_normalization = self.get_arg(kwargs_override, \'gradient_normalization\', \'norm\')\n        if gradient_normalization == \'norm\':\n            for param in params:\n                MatOps.clip_and_regularize(param, 0.0, clip_val, regc)\n        elif gradient_normalization == \'clipping\':\n            for param in params:\n                MatOps.clip_and_regularize(param, clip_val, 0.0, regc)\n        elif gradient_normalization == \'discard\':\n            params_exceeding = []\n            for param in params:\n                if MatOps.grad_norm(param).w[0,0] > clip_val:\n                    params_exceeding.append(param.name if param.name != \'\' else \'(unnamed parameter)\')\n            if len(params_exceeding) > 0:\n                if \'discards\' in debug:\n                    print(\'Discarding gradient update due to exceeded norm for the following parameters: %s\' % (params_exceeding,))\n                for param in params:\n                    param.clear_grad()\n                return\n        elif gradient_normalization == \'none\':\n            if regc > 0.0:\n                MatOps.regularize(param, regc)\n        else:\n            raise AttributeError(""Unknown gradient_normalization mode : "" + gradient_normalization)\n\n\n        ########## SOLVING ###########\n\n        for param, param_cache in zip(params, param_caches):\n            if param_cache is None:\n                param_cache = self.param_to_cache(param)\n\n            learning_rate = self.get_arg(kwargs_override, ""learning_rate"", 0.01)\n\n            lr_multiplier = param.extra_state.get(\'lr_multiplier\', 1.0)\n            learning_rate *= lr_multiplier\n            if MatOps.is_grad_nan(param):\n                if SolverBase.t.should_i_run() and \'nans\' in debug:\n                    name_str = \' (unnamed parameter)\'\n                    if param.name is not None:\n                        name_str = \' (name: %s)\' % (param.name,)\n                    print(""Warning ignoring grad update due to NaNs%s."" % (name_str,))\n            else:\n                if self.solver_type == \'sgd\':\n                    MatOps.sgd_update(param, learning_rate)\n                elif self.solver_type == \'adagrad\':\n                    smooth_eps = self.get_arg(kwargs_override, ""smooth_eps"", 1e-6)\n                    cache = self.get_cache(param, param_cache, \'adagrad_cache\')\n                    MatOps.adagrad_update(param, cache, learning_rate, smooth_eps)\n                elif self.solver_type == \'rmsprop\':\n                    smooth_eps = self.get_arg(kwargs_override, ""smooth_eps"", 1e-6)\n                    decay_rate = self.get_arg(kwargs_override, ""decay_rate"", 0.95)\n                    cache = self.get_cache(param, param_cache, \'rmsprop_cache\')\n                    MatOps.rmsprop_update(param, cache, decay_rate, learning_rate, smooth_eps)\n                elif self.solver_type == \'rmsprop_momentum\':\n                    decay_rate = self.get_arg(kwargs_override,    ""decay_rate"", 0.95)\n                    momentum = self.get_arg(kwargs_override,      ""momentum"",   0.9)\n                    smooth_eps = self.get_arg(kwargs_override,    ""smooth_eps"", 1e-4)\n                    n_cache = self.get_cache(param, param_cache, \'rmsprop_momentum_n_cache\')\n                    g_cache = self.get_cache(param, param_cache, \'rmsprop_momentum_g_cache\')\n                    momentum_cache = self.get_cache(param, param_cache, \'rmsprop_momentum_momentum_cache\')\n                    MatOps.rmsprop_momentum_update(param, n_cache, g_cache, momentum_cache, decay_rate, momentum, learning_rate, smooth_eps)\n                elif self.solver_type == \'adadelta\':\n                    smooth_eps = self.get_arg(kwargs_override, ""smooth_eps"", 1e-4)\n                    rho        = self.get_arg(kwargs_override, ""rho"",        0.95)\n                    gsum = self.get_cache(param, param_cache, \'adadelta_gsum\')\n                    xsum = self.get_cache(param, param_cache, \'adadelta_xsum\')\n                    MatOps.adadelta_update(param, gsum, xsum, rho, smooth_eps)\n                elif self.solver_type == \'adam\':\n                    smooth_eps = self.get_arg(kwargs_override, ""smooth_eps"", 1e-4)\n                    b1         = self.get_arg(kwargs_override, ""b1"",        0.5)\n                    b2         = self.get_arg(kwargs_override, ""b2"",        1e-6)\n                    m  = self.get_cache(param, param_cache, \'adam_m\')\n                    v  = self.get_cache(param, param_cache, \'adam_v\')\n                    epoch = param.extra_state.get(\'adam_epoch\', 1)\n\n                    MatOps.adam_update(param, m, v, b1, b2, smooth_eps, learning_rate, epoch)\n\n                    param_cache[\'adam_epoch\'] = epoch + 1\n                else:\n                    assert False\n            param.clear_grad()\n\n    def set_lr_multiplier(self, param, lr_multiplier):\n        param.extra_state[""lr_multiplier""] = lr_multiplier\n\n    def get_cache(self, param, cache_state, cache_name):\n        if cache_name not in cache_state:\n            cache_state[cache_name] = Mat.zeros(param.shape, dtype=param.dtype)\n        ret = cache_state[cache_name]\n        assert ret.shape == param.shape, \\\n                ""Wrong parameter passed to solver (cache shape does not match parameter\'s shape)""\n        return ret\n\n    def reset_caches(self, param, param_caches=None):\n        if type(param) == list:\n            if param_caches is not None:\n                assert len(param) == len(param_caches)\n                for p,c  in zip(param, param_caches):\n                    self.reset_caches(p, c)\n            else:\n                for p in param:\n                    self.reset_caches(p)\n        elif type(param) == Mat:\n            # get caches\n            if param_caches is None:\n                param_caches = self.param_to_cache(param)\n            # reset\n            if self.solver_type == \'adagrad\':\n                self.get_cache(param, param_caches, \'adagrad_cache\').clear()\n            elif self.solver_type == \'rmsprop\':\n                self.get_cache(param, param_caches, \'rmsprop_cache\').clear()\n            elif self.solver_type == \'adadelta\':\n                self.get_cache(param, param_caches, \'adadelta_gsum\').clear()\n                self.get_cache(param, param_caches, \'adadelta_xsum\').clear()\n            elif self.solver_type == \'adam\':\n                self.get_cache(param, param_caches, \'adam_m\').clear()\n                self.get_cache(param, param_caches, \'adam_v\').clear()\n                if \'adam_epoch\' in param:\n                    del param_caches[""adam_epoch""]\n            else:\n                assert False\n\nclass Solver(object):\n    def __init__(self, parameters, *args, **kwargs):\n        """"""Solver\n\n        It is pickleable.\n        """"""\n        self.base = SolverBase(*args, **kwargs)\n\n        if type(parameters) == list:\n            self._parameters = parameters\n        else:\n            self._parameters = [parameters]\n        self.caches =  [{} for _ in range(len(self._parameters))]\n\n        self.lr_multipliers = [None for _ in range(len(self._parameters))]\n\n    @property\n    def parameters(self):\n        return self._parameters\n\n    @parameters.setter\n    def parameters(self, val):\n        assert self._parameters is None or \\\n                len(self._parameters) == len(val), \\\n                ""Number of parameters must remain unchanged""\n        self._parameters = val\n        for lr_multiplier, param in zip(self.lr_multipliers, self._parameters):\n            if lr_multiplier is not None:\n                self.base.set_lr_multiplier(param, lr_multiplier)\n\n    def set_lr_multiplier(self, where, val):\n        indices = []\n        if type(where) == str:\n            for i, param in enumerate(self.parameters):\n                if param.name == where:\n                    indices.append(i)\n            assert len(indices) > 0, \\\n                    ""Could not find parameters %s"" % (where,)\n        elif type(where) == int:\n            indices.append(where)\n        else:\n            raise ValueError(""where must be int or str"")\n        for i in indices:\n            self.lr_multipliers[i] = val\n            self.base.set_lr_multiplier(self.parameters[i], val)\n\n    def step(self):\n        assert self.parameters is not None, \\\n                ""Remeber to use set parameters after unpickling.""\n        self.base.step(self.parameters, self.caches)\n\n    def reset_caches(self):\n        assert self.parameters is not None, \\\n                ""Remeber to use set parameters after unpickling.""\n        self.base.reset_caches(self.parameters, self.caches)\n\n    @property\n    def solver_type(self):\n        return self.base.solver_type\n\n    def __setstate__(self, state):\n        self.base          = state[\'solver\']\n        self.caches       = state[\'caches\']\n        self.lr_multipliers = state[\'lr_multipliers\']\n        self._parameters = None\n\n    def __getstate__(self):\n        return {\n            \'solver\'      : self.base,\n            \'caches\'      : self.caches,\n            \'lr_multipliers\': self.lr_multipliers\n        }\n\nclass CombinedSolver(object):\n    def __init__(self, solvers):\n        self.solvers = solvers\n\n    def step(self):\n        for solver in self.solvers:\n            solver.step()\n\n    def reset_caches(self):\n        for solver in self.solvers:\n            solver.reset_caches()\n\n__all__ = [\n    ""CombinedSolver"",\n    ""SolverBase"",\n    ""Solver"",\n]\n'"
dali/utils/throttled.py,0,"b'import time\n\n\n\n\nclass Throttled(object):\n    def __init__(self, min_time_since_last_run_s=5):\n        """"""Used for simple throttled execution.\n\n        Here\'s a simple example:\n\n            @Throttled(1)\n            def lol(i):\n                print(\'epoch %d\' % (i,), flush=True)\n\n            for i in range(100000000):\n                lol(i)\n\n        Above code will report the epoch every second.\n\n        Here\'s another way:\n\n            throttled = Throttled(1)\n\n\n            for i in range(100000000000):\n                if throttled.should_i_run():\n                    print(\'epoch %d\' % (i,), flush=True)\n        """"""\n        self.last_time = None\n        self.min_time_since_last_run_s = min_time_since_last_run_s\n\n\n    def should_i_run(self, min_time_since_last_run_s=None):\n        min_time_since_last_run_s = min_time_since_last_run_s or self.min_time_since_last_run_s\n        now = time.time()\n        if self.last_time is None or (now - self.last_time) > min_time_since_last_run_s:\n            self.last_time = now\n            return True\n        else:\n            return False\n\n    def maybe_run(self, f, min_time_since_last_run_s=None):\n        if self.should_i_run(min_time_since_last_run_s):\n            return f()\n        else:\n            return None\n\n    def __call__(self, f):\n        def wrapper(*args, **kwargs):\n            return self.maybe_run(lambda: f(*args, **kwargs))\n        return wrapper\n'"
dali/utils/vocab.py,0,"b'from dali.utils import apply_recursively_on_type\n\nclass VocabEncoded(int):\n    pass\n\nclass Vocab(object):\n    UNK = \'**UNK**\'\n    EOS = \'**EOS**\'\n\n    def __init__(self, words=None, add_eos=True, add_unk=True):\n        self.index2word = []\n        self.word2index = {}\n        self.eos = None\n        self.unk = None\n        if add_unk:\n            self.add(Vocab.UNK)\n        if add_eos:\n            self.add(Vocab.EOS)\n\n        if words:\n            self.add(words)\n\n\n    def __contains__(self, key):\n        if isinstance(key, int):\n            return key in range(len(self.index2word))\n        elif isinstance(key, str):\n            return key in self.word2index\n        else:\n            raise ValueError(""expected(index or string)"")\n\n    def add(self, obj):\n        def add_f(word):\n            idx = self.word2index.get(word)\n            if idx is None:\n                idx = len(self.index2word)\n                self.index2word.append(word)\n                self.word2index[word] = idx\n                if word is Vocab.UNK:\n                    self.unk = idx\n                if word is Vocab.EOS:\n                    self.eos = idx\n            return word\n        apply_recursively_on_type(obj, add_f, str)\n\n    def words(self):\n        return self.word2index.keys()\n\n    def __len__(self):\n        return len(self.index2word)\n\n    def __getitem__(self, index):\n        if isinstance(index, int):\n            return self.index2word[index]\n        elif isinstance(index, str):\n            if self.unk is not None:\n                return VocabEncoded(self.word2index.get(index) or self.unk)\n            else:\n                return VocabEncoded(self.word2index[index])\n        else:\n            raise ValueError(""expected(index or string)"")\n\n    def decode(self, obj, strip_eos=False, decode_type=int):\n        def decode_f(word_idx):\n            return self.index2word[word_idx]\n        def decode_list_f(lst):\n            if strip_eos:\n                assert self.eos is not None\n                return [el for el in lst if el != Vocab.EOS]\n            else:\n                return lst\n        return apply_recursively_on_type(obj, decode_f, decode_type, list_callback=decode_list_f)\n\n    def encode(self, obj, add_eos=False, encode_type=int):\n        def encode_f(word):\n            if self.unk is not None:\n                return encode_type(self.word2index.get(word) or self.unk)\n            else:\n                return encode_type(self.word2index[word])\n        def encode_list_f(lst):\n            lst = [encode_f(word) for word in lst]\n            if add_eos:\n                assert self.eos is not None\n                lst.append(VocabEncoded(self.eos))\n            return lst\n        return apply_recursively_on_type(obj, lambda x:x, str, list_callback=encode_list_f)\n\n__all__ = [\n    ""VocabEncoded"",""Vocab""\n]\n'"
examples/translation/predict.py,0,"b'import argparse\nimport random\nimport math\n\nfrom dali.utils import (\n    set_device_from_args,\n    add_device_args,\n    unpickle_as_dict,\n)\nfrom dali.data.utils import split_punctuation\n\nfrom translation import TranslationModel\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    add_device_args(parser)\n    parser.add_argument(""--path"",              type=str,  required=\'True\',  help=""Path to saved model"")\n    parser.add_argument(""--beam_width"",        type=int,  default=5,        help=""Beam width used when prediction"")\n    parser.add_argument(""--max_output_length"", type=int,  default=40,       help=""Maximum number of words in the translation"")\n    parser.add_argument(""--show_beams"",        action=\'store_true\', default=False,\n                                               help=""If true shows all the beams and probabilities"")\n\n    return parser.parse_args()\n\ndef show_reconstructions(model, example_pair, vocabs, max_sentence_length):\n    from_words, to_words = example_pair\n    from_vocab, to_vocab = vocabs\n    from_with_unk = \' \'.join(from_vocab.decode(from_vocab.encode(from_words)))\n    to_with_unk   = \' \'.join(to_vocab.decode(to_vocab.encode(to_words)))\n    print(\'TRANSLATING: %s\' % from_with_unk)\n    print(\'REFERENCE:   %s\' % to_with_unk)\n    print(\'\')\n\n\ndef main(args):\n    set_device_from_args(args)\n\n    RELEVANT_VARIABLES = [""model"", ""vocabs""]\n    loaded = unpickle_as_dict(args.path, RELEVANT_VARIABLES)\n    model = loaded[""model""]\n    from_vocab, to_vocab = loaded[""vocabs""]\n\n    while True:\n        from_sentence = split_punctuation(input()).split(\' \')\n        encoded       = from_vocab.encode(list(reversed(from_sentence)), add_eos=False)\n\n        beams = model.predict(encoded,\n                              eos_symbol=to_vocab.eos,\n                              max_sequence_length=args.max_output_length + 1,\n                              beam_width=args.beam_width)\n\n        if args.show_beams:\n            for solution, score, _ in beams:\n                score = math.exp(score.w[0])\n                # reveal the unks\n                solution = \' \'.join(to_vocab.decode(solution, strip_eos=True))\n                print(\'%f => %s\' % (score, to_vocab.decode(solution, True)))\n        else:\n            print(\' \'.join(to_vocab.decode(beams[0].solution, strip_eos=True)))\n\n\n\nif __name__ == \'__main__\':\n    main(parse_args())\n'"
examples/translation/train.py,0,"b'import argparse\nimport dali.core as D\nfrom dali.data import Lines, Process, DiscoverFiles, BatchBenefactor, IdentityReducer\nfrom dali.data.batch import TranslationBatch\nfrom dali.data.translation import TranslationFiles, TranslationMapper, build_vocabs, iterate_examples\nfrom dali.utils.scoring import bleu, ErrorTracker\nfrom dali.utils import (\n    Vocab,\n    Solver,\n    median_smoothing,\n    subsample,\n    Throttled,\n    pickle_from_scope,\n    unpickle_as_dict,\n    set_device_from_args,\n    add_device_args,\n)\nimport math\nimport os\nimport sys\nimport time\nimport random\n\n\nfrom translation import TranslationModel\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    # device\n    add_device_args(parser)\n\n    # paths and data\n    parser.add_argument(""--train"",     type=str, required=True)\n    parser.add_argument(""--validate"",  type=str, required=True)\n    parser.add_argument(""--save"",      type=str, default=None)\n    parser.add_argument(""--from_lang"", type=str, required=True)\n    parser.add_argument(""--to_lang"",   type=str, required=True)\n    parser.add_argument(""--max_from_vocab"", type=int, default=20000)\n    parser.add_argument(""--max_to_vocab"",   type=int, default=20000)\n\n    # training\n    parser.add_argument(""--minibatch"",           type=int, default=64)\n    parser.add_argument(""--max_sentence_length"", type=int, default=40)\n\n    # model\n    parser.add_argument(""--input_size"",   type=int, default=512)\n    parser.add_argument(""--hidden_sizes"", type=int, nargs=\'+\', default=[512,512,512,512])\n\n    # solver\n    parser.add_argument(""--solver_type"",   type=str,   default=""sgd"")\n    parser.add_argument(""--learning_rate"", type=float, default=0.003)\n\n    return parser.parse_args()\n\ndef load_raw_validation(path, from_lang, to_lang, max_sentence_length):\n    """"""List of validation sentences as strings.\n\n    Used for reconstructions and BLEU.\n    """"""\n    p = Process(files=TranslationFiles(path, from_lang, to_lang),\n            mapper=TranslationMapper(sentence_bounds=(0, max_sentence_length)),\n            reducer=IdentityReducer())\n    return list(p)\n\ndef show_reconstructions(model, example_pair, vocabs, max_sentence_length):\n    from_words, to_words = example_pair\n    from_vocab, to_vocab = vocabs\n    from_with_unk = \' \'.join(from_vocab.decode(from_vocab.encode(from_words)))\n    to_with_unk   = \' \'.join(to_vocab.decode(to_vocab.encode(to_words)))\n    print(\'TRANSLATING: %s\' % from_with_unk)\n    print(\'REFERENCE:   %s\' % to_with_unk)\n    print(\'\')\n    for solution, score, _ in model.predict(from_vocab.encode(list(reversed(from_words)), add_eos=False),\n                                           eos_symbol=to_vocab.eos,\n                                           max_sequence_length=max_sentence_length + 1):\n        score = math.exp(score.w[0])\n        # reveal the unks\n        solution = \' \'.join(to_vocab.decode(solution, False))\n        print(\'    %f => %s\' % (score, to_vocab.decode(solution, True)))\n\ndef main(args):\n    set_device_from_args(args, verbose=True)\n\n    ############### MODEL/DATA LOADING ####################\n\n    RELEVANT_VARIABLES = [""model"", ""vocabs"", ""solver"", ""data"", ""train_error"", ""validate_error""]\n\n    if args.save is not None and os.path.exists(args.save):\n        print(""Resuming saved experiment at %s."" % (args.save,))\n        loaded = unpickle_as_dict(args.save)\n        model, vocabs, solver, data, train_error, validate_error = [loaded[x] for x in RELEVANT_VARIABLES]\n        solver.parameters = model.parameters()\n    else:\n        print(""Loading vocabs - for monstrous datasets hit ctrl+C after you feel like probably enough words have been sampled."")\n        vocabs = build_vocabs(args.train, args.from_lang, args.to_lang,\n                              from_max_size=args.max_from_vocab,\n                              to_max_size=args.max_to_vocab)\n        print(""Creating model"")\n        model  = TranslationModel(args.input_size,\n                                  args.hidden_sizes,\n                                  len(vocabs[0]),\n                                  len(vocabs[1]))\n        model.name_parameters(""model"")\n        solver = Solver(model.parameters(), args.solver_type, learning_rate=args.learning_rate)\n\n        solver.set_lr_multiplier(""model.encoder_embedding"", 2)\n        solver.set_lr_multiplier(""model.decoder_embedding"", 2)\n\n        data             = []\n        train_error      = ErrorTracker()\n        validate_error   = ErrorTracker()\n\n    from_vocab, to_vocab = vocabs\n\n    print(""Input size:   "",        args.input_size)\n    print(""Hidden sizes: "",        args.hidden_sizes)\n    print(""max sentence length: "", args.max_sentence_length)\n\n    print (args.from_lang + "" vocabulary containts"", len(from_vocab), ""words"")\n    print (args.to_lang   + "" vocabulary containts"", len(to_vocab),   ""words"")\n\n\n    def create_dataset_iterator(dataset, sentences_until_minibatch):\n        return iterate_examples(dataset, args.from_lang, args.to_lang, vocabs,\n                                minibatch_size=args.minibatch,\n                                sentence_length_bounds=(0, args.max_sentence_length),\n                                sentences_until_minibatch=sentences_until_minibatch)\n\n\n    validation_pairs_text = load_raw_validation(args.validate, args.from_lang, args.to_lang, args.max_sentence_length)\n    validation_batches    = list(create_dataset_iterator(args.validate, args.minibatch))\n\n\n    t = Throttled(10)\n\n    while True:\n        total_time  = 0.0\n        num_words, num_batches = 0, 0\n\n        if solver.solver_type == \'adagrad\':\n            solver.reset_caches(params)\n\n        for batch in data:\n            batch_start_time = time.time()\n            error = model.error(batch)\n            (error / batch.examples).grad()\n            D.Graph.backward()\n\n            solver.step()\n            batch_end_time = time.time()\n\n            train_error.append(error / batch.to_tokens)\n\n            total_time += batch_end_time - batch_start_time\n            num_words   += batch.from_tokens + batch.to_tokens\n            num_batches += 1\n\n            if num_batches % 10 == 0:\n                val_batch = random.choice(validation_batches)\n                with D.NoBackprop():\n                    validate_error.append(model.error(val_batch) / val_batch.to_tokens)\n\n            if t.should_i_run() and num_batches > 0 and abs(total_time) > 1e-6:\n                print(\'Epochs completed:  \', train_error.num_epochs())\n                print(\'Error:             \', train_error.recent(10))\n                print(\'Time per batch:    \', total_time  / num_batches)\n                print(\'Words per second:  \', num_words   / total_time )\n                print(\'Batches processed: \', num_batches)\n                if hasattr(solver, \'step_size\'):\n                    print(\'Solver step size:  \', solver.step_size)\n                show_reconstructions(model, random.choice(validation_pairs_text), vocabs, args.max_sentence_length)\n                sys.stdout.flush()\n\n            # free memory as soon as possible\n            del batch\n\n        train_error.finalize_epoch()\n        validate_error.finalize_epoch()\n        if train_error.num_epochs() > 0 and args.save is not None:\n            print(""Saving model to %s."" % (args.save,))\n            pickle_from_scope(args.save, RELEVANT_VARIABLES)\n\n        data = create_dataset_iterator(args.train, 1000 * args.minibatch)\n\nif __name__ == \'__main__\':\n    main(parse_args())\n'"
examples/translation/translation.py,1,"b'import numpy as np\n\nimport dali.core as D\nimport dali\n\nfrom dali import beam_search\n\nclass TranslationModel(object):\n    def __init__(self, input_size, hiddens,\n                       encoder_vocab_size, decoder_vocab_size,\n                       softmax_input_size=None, dtype=np.float32):\n        self.input_size = input_size\n        self.hiddens    = hiddens\n        self.encoder_vocab_size = encoder_vocab_size\n        self.decoder_vocab_size = decoder_vocab_size\n        self.softmax_input_size = softmax_input_size\n        self.dtype = dtype\n\n        self.encoder_embedding = D.random.uniform(-0.05, 0.05, (encoder_vocab_size, input_size), dtype=dtype)\n        self.decoder_embedding = D.random.uniform(-0.05, 0.05, (decoder_vocab_size, input_size), dtype=dtype)\n\n        self.encoder_lstm    = D.StackedLSTM(input_size, hiddens, memory_feeds_gates=True, dtype=dtype)\n        self.decoder_lstm    = D.StackedLSTM(input_size, hiddens, memory_feeds_gates=True, dtype=dtype)\n\n        if self.softmax_input_size is not None:\n            self.predecoder = D.StackedInputLayer(self.hiddens, self.softmax_input_size)\n            self.decoder = D.Layer(self.softmax_input_size, decoder_vocab_size, dtype=dtype)\n        else:\n            self.decoder = D.Layer(hiddens[-1], decoder_vocab_size, dtype=dtype)\n\n    def decode_state(self, state):\n        if self.softmax_input_size is not None:\n            decoder_input = self.predecoder.activate([s.hidden for s in state])\n        else:\n            decoder_input = state[-1].hidden\n        return self.decoder.activate(decoder_input)\n\n    def error(self, batch):\n        error = D.Mat(1,1)\n        state = self.encoder_lstm.initial_states()\n        for ts in range(batch.timesteps):\n            inputs  = batch.inputs(ts)\n            targets = batch.targets(ts)\n            if ts < batch.from_timesteps:\n                assert targets is None\n                encoded = self.encoder_embedding[inputs]\n                state = self.encoder_lstm.activate(encoded, state)\n            else:\n                assert inputs is None\n                decoded = self.decode_state(state)\n                # mask the error - only for the relevant sentences\n                tstep_error = batch.masks(ts).T() * D.MatOps.softmax_cross_entropy(decoded, targets)\n                #tstep_error = D.MatOps.softmax_cross_entropy(decoded, targets)\n                error = error + tstep_error.sum()\n                # feedback the predictions\n                if ts + 1 != batch.timesteps:\n                    # for the last timestep encoding is not necessary\n                    encoded = self.decoder_embedding[targets]\n                    state = self.decoder_lstm.activate(encoded, state)\n\n        return error\n\n    def predict(self, input_sentence, **kwargs):\n        with D.NoBackprop():\n            state = self.encoder_lstm.initial_states()\n            for word_idx in input_sentence:\n                encoded = self.encoder_embedding[word_idx]\n                state = self.encoder_lstm.activate(encoded, state)\n            def candidate_scores(state):\n                decoded = self.decode_state(state)\n                return D.MatOps.softmax(decoded).log()\n            def make_choice(state, candidate_idx):\n                encoded = self.decoder_embedding[candidate_idx]\n                return self.decoder_lstm.activate(encoded, state)\n\n            return beam_search(state,\n                               candidate_scores,\n                               make_choice,\n                               **kwargs)\n\n    def parameters(self):\n        ret = ([self.encoder_embedding,\n               self.decoder_embedding]\n            + self.encoder_lstm.parameters()\n            + self.decoder_lstm.parameters()\n            + self.decoder.parameters())\n        if self.softmax_input_size is not None:\n            ret.extend(self.predecoder.parameters())\n        return ret\n\n    def name_parameters(self, prefix):\n        self.encoder_embedding.name = prefix + "".encoder_embedding""\n        self.decoder_embedding.name = prefix + "".decoder_embedding""\n        self.encoder_lstm.name_parameters(prefix + "".encoder_lstm"")\n        self.decoder_lstm.name_parameters(prefix + "".decoder_lstm"")\n        self.decoder.name_parameters(prefix + "".decoder"")\n        if self.softmax_input_size is not None:\n            self.predecoder.name_parameters(prefix + "".predecoder"")\n'"
