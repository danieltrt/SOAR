file_path,api_count,code
chained_operations.py,28,"b""import numpy as np\n\n\ndef run(return_values, feed_dict=None, backwards=True):\n    feed_dict = feed_dict or {}\n    for operation, value in feed_dict.items():\n        operation.run(value)\n\n    if isinstance(return_values, (list, tuple)):\n        ret = []\n        for operation in return_values:\n            if isinstance(operation, ChainedOperation):\n                output = operation.get_output()\n            else:\n                output = operation\n            ret.append(output)\n            if backwards:\n                operation.backwards()\n        return tuple(ret)\n    elif isinstance(return_values, dict):\n        ret = {}\n        for key, operation in return_values.items():\n            if isinstance(operation, ChainedOperation):\n                output = operation.get_output()\n            else:\n                output = operation\n            ret[key] = output\n            if backwards:\n                operation.backwards()\n        return ret\n\n    operation = return_values\n    if isinstance(operation, ChainedOperation):\n        output = operation.get_output()\n    else:\n        output = operation\n    if backwards:\n        operation.backwards()\n    return output\n\n\nclass ChainedOperation(object):\n    def __init__(self, inputs=None):\n        self.input_objects = inputs or []\n        self.output_objects = []\n        self.inputs_ready = {}\n        self.outputs_ready = {}\n        self.inputs = []\n        self.output = None\n        self.grad = 0\n\n        for i in self.input_objects:\n            if isinstance(i, ChainedOperation):\n                if not isinstance(i, Variable):\n                    self.inputs_ready[i] = False\n                i.add_output(self)\n\n    def add_output(self, output_object):\n        self.output_objects.append(output_object)\n        self.outputs_ready[output_object] = False\n\n    def forwards(self, input_object=None):\n        self_forward = input_object is None\n        if not self_forward:\n            self.inputs_ready[input_object] = True\n\n        inputs_ready = self.all_inputs_ready()\n        if inputs_ready and not self_forward:\n            self.inputs.clear()\n            for i in self.input_objects:\n                if isinstance(i, ChainedOperation):\n                    self.inputs.append(i.get_output())\n                else:\n                    self.inputs.append(i)\n            self.output = self.calc_forwards(self.inputs)\n\n        if inputs_ready or self_forward:\n            for o in self.output_objects:\n                o.forwards(self)\n\n            # Clear variables\n            self.grad = 0\n            for key in self.outputs_ready.keys():\n                self.outputs_ready[key] = False\n            for key in self.inputs_ready.keys():\n                self.inputs_ready[key] = False\n\n    def backwards(self, output_object=None):\n        if len(self.inputs) != len(self.input_objects):\n            raise ValueError('cannot preform backwards pass before full forwards pass,'\n                             'have you missed placeholder values in the feed_dict?')\n\n        if output_object is None:\n            if not np.any(self.grad):\n                self.grad = np.ones_like(self.output)\n        else:\n            self.outputs_ready[output_object] = True\n            self.grad += output_object.get_grad(self)\n\n        if self.all_outputs_ready():\n            for input_object in self.input_objects:\n                if isinstance(input_object, ChainedOperation):\n                    input_object.backwards(self)\n\n    def get_output(self):\n        return self.output\n\n    def get_grad(self, input_object=None):\n        if input_object is None:\n            return self.grad\n        return np.multiply(self.grad, self.calc_backwards(input_object))\n\n    def all_inputs_ready(self):\n        return self.all_ready(self.inputs_ready)\n\n    def all_outputs_ready(self):\n        return self.all_ready(self.outputs_ready)\n\n    @staticmethod\n    def all_ready(l):\n        for state in l.values():\n            if state is False:\n                return False\n        return True\n\n    def calc_forwards(self, inputs):\n        raise NotImplementedError('Abstract class')\n\n    def calc_backwards(self, input_object):\n        raise NotImplementedError('Abstract class')\n\n\nclass UnaryChainedOperation(ChainedOperation):\n    def __init__(self, x):\n        super(UnaryChainedOperation, self).__init__([x])\n\n    def calc_forwards(self, inputs):\n        return self.calc_forwards_single(inputs[0])\n\n    def calc_backwards(self, input_object):\n        return self.calc_backwards_single(self.inputs[0])\n\n    def calc_forwards_single(self, x):\n        raise NotImplementedError('Abstract class')\n\n    def calc_backwards_single(self, x):\n        raise NotImplementedError('Abstract class')\n\n\nclass BinaryChainedOperation(ChainedOperation):\n    def __init__(self, a, b):\n        super(BinaryChainedOperation, self).__init__([a, b])\n\n    def calc_forwards(self, inputs):\n        return self.calc_forwards_binary(inputs[0], inputs[1])\n\n    def calc_backwards(self, input_object):\n        return self.calc_backwards_binary(input_object, self.inputs[0], self.inputs[1])\n\n    def calc_forwards_binary(self, a, b):\n        raise NotImplementedError('Abstract class')\n\n    def calc_backwards_binary(self, input_object, a, b):\n        raise NotImplementedError('Abstract class')\n\n\nclass Placeholder(ChainedOperation):\n\n    def __init__(self):\n        super(Placeholder, self).__init__()\n\n    def calc_forwards(self, _):\n        return 1\n\n    def calc_backwards(self, _):\n        return 1\n\n    def run(self, value):\n        self.output = np.asmatrix(value)\n        self.forwards()\n\n\ndef random_weight(*args):\n    return (np.random.rand(*args) - 0.5) * 2\n\n\nclass Variable(Placeholder):\n    def __init__(self, shape, random_func=random_weight):\n        super(Variable, self).__init__()\n        self.shape = shape\n        self.random_func = random_func\n        self.value = None\n        self.initialize()\n\n    def get_output(self):\n        return self.value\n\n    def update(self, delta, direction=1):\n        self.value += direction * delta\n\n    def initialize(self):\n        self.value = self.random_func(*self.shape)\n\n    def reset_grad(self):\n        self.grad = 0\n\n\nclass Log(UnaryChainedOperation):\n    def calc_forwards_single(self, x):\n        return np.log(x)\n\n    def calc_backwards_single(self, x):\n        return 1 / x\n\n\nclass Sum(UnaryChainedOperation):\n    def __init__(self, x, axis=None):\n        super(Sum, self).__init__(x)\n        self.axis = axis\n\n    def calc_forwards_single(self, x):\n        return np.sum(x, self.axis)\n\n    def get_grad(self, input_object):\n        if self.axis is not None:\n            reps = np.ones_like(np.shape(self.inputs[0]))\n            reps[self.axis] = np.shape(self.inputs[0])[self.axis]\n            return np.tile(self.grad, reps)\n        return super(Sum, self).get_grad(input_object)\n\n    def calc_backwards_single(self, x):\n        return np.ones_like(x)\n\n\nclass Reciprocal(UnaryChainedOperation):\n    def calc_forwards_single(self, x):\n        return 1 / x\n\n    def calc_backwards_single(self, x):\n        return - 1 / np.square(x)\n\n\nclass Exp(UnaryChainedOperation):\n    def __init__(self, x, axis=None):\n        super(Exp, self).__init__(x)\n        self.axis = axis\n\n    def calc_forwards_single(self, x):\n        if self.axis is not None:\n            return np.exp(x - np.max(x, axis=self.axis))\n\n        return np.exp(x)\n\n    def calc_backwards_single(self, x):\n        if self.axis is not None:\n            return np.exp(x - np.max(x, axis=self.axis))\n\n        return np.exp(x)\n\n\nclass Add(BinaryChainedOperation):\n    def __init__(self, a, b, axis=0):\n        super(Add, self).__init__(a, b)\n        self.axis = axis\n\n    def calc_forwards_binary(self, a, b):\n        return np.add(a, b)\n\n    def get_grad(self, input_object=None):\n        if input_object is None:\n            return self.grad\n        grad = np.multiply(self.grad, self.calc_backwards(input_object))\n        if np.shape(grad) != np.shape(self.inputs[self.input_objects.index(input_object)]):\n            grad = np.sum(grad, self.axis)\n        return grad\n\n    def calc_backwards_binary(self, input_object, a, b):\n        if self.input_objects.index(input_object) == 0:\n            return np.ones_like(a)\n        return np.ones_like(b)\n\n\nclass Mul(BinaryChainedOperation):\n    def calc_forwards_binary(self, a, b):\n        return np.multiply(a, b)\n\n    def calc_backwards_binary(self, input_object, a, b):\n        if self.input_objects.index(input_object) == 0:\n            return b\n        return a\n\n\nclass Dot(BinaryChainedOperation):\n    def get_grad(self, input_object=None):\n        if input_object is None:\n            return self.grad\n        if self.input_objects.index(input_object) == 0:\n            return np.dot(self.grad, self.calc_backwards(input_object))\n        return np.dot(self.calc_backwards(input_object), self.grad)\n\n    def calc_forwards_binary(self, a, b):\n        return np.dot(a, b)\n\n    def calc_backwards_binary(self, input_object, a, b):\n        if self.input_objects.index(input_object) == 0:\n            return np.transpose(b)\n        return np.transpose(a)\n"""
chained_optimizers.py,2,"b""import chained_operations as op\nimport neural_layers as nn\nimport numpy as np\n\n\nclass ChainedOptimizer(object):\n    def __init__(self, loss, minimize=True):\n        self.loss = loss\n        self.direction = -1 if minimize else 1\n\n        self.variable_layers = []\n        self.variables = self.find_variables(loss)\n        if not self.variables:\n            raise ValueError('no variables found to optimize')\n\n    def step(self, feed_dict=None):\n        feed_dict = feed_dict or {}\n        self.reset_grads()\n        for layer in self.variable_layers:\n            layer.forwards()\n        loss = op.run(self.loss, feed_dict)\n\n        for variable in self.variables:\n            variable.update(self.update(variable, variable.get_grad()), self.direction)\n        return loss\n\n    def reset_grads(self):\n        for variable in self.variables:\n            variable.reset_grad()\n\n    def update(self, variable, gradient):\n        raise NotImplementedError()\n\n    def find_variables(self, chained_operation):\n        if isinstance(chained_operation, op.ChainedOperation):\n            if isinstance(chained_operation, nn.VariableLayer):\n                self.variable_layers.append(chained_operation)\n            if isinstance(chained_operation, op.Variable):\n                return [chained_operation]\n\n            variables = []\n            if isinstance(chained_operation, nn.Layer):\n                variables.extend(chained_operation.get_variables())\n            for input_object in chained_operation.input_objects:\n                variables.extend(self.find_variables(input_object))\n            return variables\n        return []\n\n\nclass SGD(ChainedOptimizer):\n    def __init__(self, loss, learning_rate=0.01):\n        super(SGD, self).__init__(loss)\n        self.learning_rate = learning_rate\n\n    def update(self, _, gradient):\n        return self.learning_rate * gradient\n\n\nclass Momentum(SGD):\n    def __init__(self, loss, learning_rate=0.01, momentum=0.9):\n        super(Momentum, self).__init__(loss, learning_rate)\n        self.momentum = momentum\n        self.last_change = {variable: 0 for variable in self.variables}\n\n    def update(self, variable, gradient):\n        change = self.momentum * self.last_change[variable] + self.learning_rate * gradient\n        self.last_change[variable] = change\n        return self.last_change[variable]\n\n\nclass Adagrad(SGD):\n    def __init__(self, loss, learning_rate=0.01):\n        super(Adagrad, self).__init__(loss, learning_rate)\n        self.squares_sum = {variable: 0 for variable in self.variables}\n\n    def update(self, variable, gradient):\n        self.squares_sum[variable] += np.square(gradient)\n        return np.multiply(self.learning_rate / np.sqrt(self.squares_sum[variable] + 1e-8), gradient)\n"""
data_utils.py,4,"b""import sys\nimport os\nimport numpy as np\nfrom PIL import Image\n\nif sys.version_info[0] == 2:\n    # noinspection PyUnresolvedReferences,PyPep8Naming\n    import cPickle as c_pickle\nelse:\n    import _pickle as c_pickle\n\n\ndef batches(data, batch_size):\n    for i in range(0, len(data), batch_size):\n        yield data[i:i + batch_size]\n\n\nclass Cifar10(object):\n    @staticmethod\n    def load_labels():\n        with open(os.path.join('cifar-10-batches-py', 'batches.meta'), 'rb') as f:\n            label_names = c_pickle.load(f)['label_names']\n        return label_names\n\n    @staticmethod\n    def get_batch(n, raw=False):\n        path = os.path.join('cifar-10-batches-py', 'data_batch_%d' % n)\n        with open(path, 'rb') as f:\n            dictionary = c_pickle.load(f, encoding='bytes')\n        if not raw:\n            dictionary[b'data'] = dictionary[b'data'].astype(np.float64)\n            dictionary[b'data'] /= 255\n            dictionary[b'data'] -= 0.5\n            labels = np.zeros((len(dictionary[b'labels']), 10))\n            for i in range(len(dictionary[b'labels'])):\n                labels[i, dictionary[b'labels'][i]] = 1\n            dictionary[b'labels'] = labels\n        return dictionary[b'data'], dictionary[b'labels']\n\n    @staticmethod\n    def show_img_array(flat, normalized=True):\n        img_array = np.zeros((32, 32, 3))\n        for channel in range(3):\n            for row in range(32):\n                for collum in range(32):\n                    img_array[row, collum, channel] = flat[channel * 1024 + row * 32 + collum]\n        if normalized:\n            img_array -= img_array.min()\n            img_array /= img_array.max()\n            img_array *= 255\n        with Image.fromarray(img_array.astype(np.uint8)) as img:\n            img.show()\n"""
linear_classifier.py,3,"b""import numpy as np\nimport itertools as it\n\nimport chained_operations as op\nimport neural_layers as nn\nimport data_utils\nimport plotting\nimport chained_optimizers as optimizers\n\nprint('get data')\n\nlabel_names = data_utils.Cifar10.load_labels()\n\ndata, labels = data_utils.Cifar10.get_batch(1, raw=False)\n\nprint('build model')\n\nx = nn.InputLayer(1024 * 3)\n\ny = nn.Dense(x, 10)\n\ny_ = op.Placeholder()\nloss = nn.Softmax(y, y_)\n\nprint('train model')\n\nbatch_size = 100\n\ngraph = plotting.Graph('loss')\noptimizer = optimizers.Adagrad(loss)\n\nfor count, mini_batch_x, mini_batch_y_ in zip(\n        it.count(),\n        data_utils.batches(data, batch_size),\n        data_utils.batches(labels, batch_size)):\n\n    batch_loss = optimizer.step({x: mini_batch_x, y_: mini_batch_y_})\n    batch_loss = np.mean(batch_loss)\n\n    print('%d loss - %f' % (count, batch_loss))\n    graph.maybe_add(count, count, batch_loss, True)\n\n    if batch_loss < 4:\n        break\n\n\nprint('deep dream')\n\nimages = nn.VariableLayer(1, 1024 * 3)\ny.set_inputs(images)\ny.lock()\n\ny.set_inputs(x)\n\ntruths = np.zeros((1, 10))\ntruths[0][1] = 1\n\ngraph.clear()\noptimizer = optimizers.Adagrad(loss)\n\nfor i in range(15):\n    loss = np.mean(optimizer.step({y_: truths}))\n    print('%d loss - %f' % (i, loss))\n    graph.maybe_add(i, i, loss, plot=True)\n\nprint('done')\n"""
neural_layers.py,1,"b""import numpy as np\n\nimport chained_operations as op\n\n\nclass Layer(op.ChainedOperation):\n    def __init__(self, inputs=None):\n        inputs = inputs or []\n        super(Layer, self).__init__(inputs)\n        self.is_locked = False\n\n        self.firsts = [op.Placeholder() for _ in inputs]\n        self.input_shapes = []\n        for i in inputs:\n            if isinstance(i, Layer):\n                self.input_shapes.append(i.get_output_len())\n            else:\n                self.input_shapes.append(None)\n\n        self.last, self.variables = self.build_layer(self.firsts)\n        self.variables = self.variables or []\n        if not isinstance(self.variables, (list, tuple)):\n            self.variables = [self.variables]\n\n    def backwards(self, output_object=None):\n        if self.last is not None:\n            if output_object is not None:\n                self.last.grad = output_object.get_grad(self)\n            if self.last != self:\n                self.last.backwards()\n        super(Layer, self).backwards(output_object)\n\n    def calc_forwards(self, inputs):\n        for i, input_value in enumerate(inputs):\n            self.firsts[i].run(input_value)\n        if self.last is not None:\n            return self.last.output\n\n    def get_grad(self, input_object=None):\n        if input_object is None:\n            return self.grad\n        index = self.input_objects.index(input_object)\n        return self.firsts[index].grad\n\n    def calc_backwards(self, input_object):\n        pass\n\n    def get_variables(self):\n        if self.is_locked:\n            return []\n        return self.variables\n\n    def lock(self):\n        self.is_locked = True\n\n    def unlock(self):\n        self.is_locked = False\n\n    def set_inputs(self, new_inputs=None):\n        new_inputs = new_inputs or []\n        if not len(new_inputs) == len(self.input_objects):\n            raise ValueError('cannot change inputs number')\n\n        self.input_objects = new_inputs or []\n        self.inputs_ready = {}\n        for i in self.input_objects:\n            if isinstance(i, op.ChainedOperation):\n                if not isinstance(i, op.Variable):\n                    self.inputs_ready[i] = False\n                i.add_output(self)\n\n    def get_output_len(self):\n        raise NotImplementedError()\n\n    def build_layer(self, inputs):\n        raise NotImplementedError()\n\n\nclass InputLayer(op.Placeholder, Layer):\n    def __init__(self, size=0, axis=1):\n        self.size = size\n        self.axis = axis\n        super(InputLayer, self).__init__()\n\n    def build_layer(self, inputs):\n        return None, None\n\n    def get_output_len(self):\n        return self.size\n\n    def run(self, value):\n        shape = np.shape(value)\n        if len(shape) < self.axis + 1 or shape[self.axis] != self.size:\n            raise ValueError('value shape not matching size %d in axis %d' % (self.size, self.axis))\n        super(InputLayer, self).run(value)\n\n\nclass VariableLayer(op.Variable, Layer):\n    def __init__(self, depth=0, size=0):\n        self.size = size\n        super(VariableLayer, self).__init__((depth, size))\n\n    def get_output_len(self):\n        return self.size\n\n    def build_layer(self, inputs):\n        return self, self\n\n\nclass UnaryLayer(Layer):\n    def __init__(self, x):\n        super(UnaryLayer, self).__init__([x])\n\n    def build_layer(self, inputs):\n        return self.build_unary_layer(inputs[0])\n\n    def build_unary_layer(self, x):\n        raise NotImplementedError()\n\n    def get_output_len(self):\n        return self.get_unary_len(self.input_shapes[0])\n\n    def get_unary_len(self, shape):\n        raise NotImplementedError()\n\n    def set_inputs(self, new_input=None):\n        super(UnaryLayer, self).set_inputs([new_input])\n\n\nclass BinaryLayer(Layer):\n    def __init__(self, a, b):\n        super(BinaryLayer, self).__init__([a, b])\n\n    def build_layer(self, inputs):\n        return self.build_binary_layer(inputs[0], inputs[1])\n\n    def build_binary_layer(self, a, b):\n        raise NotImplementedError()\n\n    def get_output_len(self):\n        return self.get_binary_len(self.input_shapes[0], self.input_shapes[1])\n\n    def get_binary_len(self, shape_a, shape_b):\n        raise NotImplementedError()\n\n\nclass Softmax(BinaryLayer):\n    def __init__(self, y, y_):\n        super(Softmax, self).__init__(y, y_)\n\n    def build_binary_layer(self, y, y_):\n        exp = op.Exp(y, axis=1)\n        top = op.Sum(op.Mul(y_, exp), axis=1)\n        reciprocal = op.Reciprocal(op.Sum(exp, axis=1))\n        naive = op.Mul(top, reciprocal)\n        return op.Mul(-1, op.Log(naive)), None\n\n    def get_binary_len(self, shape_a, shape_b):\n        return 1\n\n\nclass Dense(UnaryLayer):\n    def __init__(self, x=None, neurons=1):\n        self.neurons = neurons\n        super(Dense, self).__init__(x)\n\n    def build_unary_layer(self, x):\n        if self.input_shapes[0] is None:\n            raise ValueError('input has no defined shape')\n        w = op.Variable((self.input_shapes[0], self.neurons))\n        b = op.Variable((1, self.neurons))\n        y = op.Add(op.Dot(x, w), b, axis=0)\n        return y, [w, b]\n\n    def get_unary_len(self, shape):\n        return self.neurons\n"""
operations.py,6,"b'""""""\nNaive operations with forwards and backwards numpy functions\n""""""\n\n\nimport numpy as np\n\n\nclass Operation(object):\n    def forwards(self, *args):\n        yield NotImplementedError(\'Abstract class\')\n\n    def backwards(self, *args):\n        yield NotImplementedError(\'Abstract class\')\n\n\nclass Sum(Operation):\n    # noinspection PyAttributeOutsideInit\n    def forwards(self, x):\n        self.x = x\n        return np.sum(x)\n\n    def backwards(self):\n        return np.ones_like(self.x)\n\n\nclass Dot(Operation):\n    # noinspection PyAttributeOutsideInit\n    def forwards(self, x, a):\n        self.a = a\n        return np.dot(a, x)\n\n    def backwards(self):\n        return self.a\n\n\nclass Reciprocal(Operation):\n    # noinspection PyAttributeOutsideInit\n    def forwards(self, x):\n        self.x = x\n        return 1 / x\n\n    def backwards(self):\n        return - 1 / np.square(self.x)\n\n\nclass Exp(Operation):\n    # noinspection PyAttributeOutsideInit\n    def forwards(self, x):\n        self.x = x\n        return np.exp(x)\n\n    def backwards(self):\n        return np.exp(self.x)\n'"
optimizers.py,10,"b'""""""\nNaive optimizers with numeric gradient\n""""""\n\n\nimport math\nimport numpy as np\n\n\ndef numeric_gradient(x, f):\n    x = np.asmatrix(x, np.float64)\n    d_x = np.zeros_like(x)\n    fx = f(x)\n    r = [None]\n    if np.shape(fx) != () and np.shape(fx) != np.shape(d_x):\n        for rank in range(len(np.shape(fx))):\n            if np.shape(fx)[rank] != np.shape(d_x)[rank]:\n                r[0] = rank\n                break\n\n    it = np.nditer(d_x, flags=[\'multi_index\'])\n    while not it.finished:\n        like_x = np.copy(x)\n        dx = like_x[it.multi_index] * 1e-8\n        like_x[it.multi_index] += dx\n\n        derivative = (f(like_x) - fx) / dx\n        if np.shape(derivative) == ():\n            d_x[it.multi_index] = derivative\n        elif np.shape(derivative) == np.shape(d_x):\n            d_x[it.multi_index] = derivative[it.multi_index]\n        elif r[0] is not None:\n            for i in range(np.shape(derivative)[r[0]]):\n                location = list(it.multi_index)\n                location[r[0]] = i\n                d_x[it.multi_index] += derivative[tuple(location)]\n\n        it.iternext()\n    return d_x\n\n\nclass BaseOptimizer(object):\n    # noinspection PyMethodMayBeStatic\n    def optimize(self, params):\n        yield NotImplementedError()\n\n\nclass SGD(BaseOptimizer):\n    def __init__(self, function, learning_rate=0.01, gradient_func=numeric_gradient):\n        self.function = function\n        self.learning_rate = learning_rate\n        self.gradient_func = gradient_func\n\n    def optimize(self, params):\n        d_params = self.gradient_func(params, self.function)\n        for i in range(len(params)):\n            params[i] -= self.learning_rate * d_params[i]\n\n\nclass Momentum(BaseOptimizer):\n    def __init__(self, function, learning_rate=0.01, gradient_func=numeric_gradient, momentum_strength=0.9):\n        self.function = function\n        self.learning_rate = learning_rate\n        self.gradient_func = gradient_func\n        self.momentum_strength = momentum_strength\n        self.last_v = 0\n\n    def optimize(self, params):\n        d_params = self.gradient_func(params, self.function)\n        for i in range(len(params)):\n            v = self.momentum_strength*self.last_v + self.learning_rate*d_params[i]\n            params[i] -= v\n            self.last_v = v\n\n\nclass NEG(BaseOptimizer):\n    """"""\n        Nestrov Accelerated Gradient\n    """"""\n    def __init__(self, function, learning_rate=0.01, gradient_func=numeric_gradient, momentum_strength=0.9):\n        self.function = function\n        self.learning_rate = learning_rate\n        self.gradient_func = gradient_func\n        self.momentum_strength = momentum_strength\n        self.last_v = 0\n\n    def optimize(self, params):\n        d_params = self.gradient_func([param - self.momentum_strength*self.last_v for param in params],\n                                      self.function)\n        for i in range(len(params)):\n            v = self.momentum_strength*self.last_v + self.learning_rate*d_params[i]\n            params[i] -= v\n            self.last_v = v\n\n\nclass Adagrad(BaseOptimizer):\n    def __init__(self, function, learning_rate=0.01, gradient_func=numeric_gradient):\n        self.function = function\n        self.learning_rate = learning_rate\n        self.gradient_func = gradient_func\n        self.G = []\n\n    def optimize(self, params):\n        if not len(self.G) == len(params):\n            self.G = [0 for _ in range(len(params))]\n        d_params = self.gradient_func(params,\n                                      self.function)\n        for i in range(len(params)):\n            g = d_params[i]\n            self.G[i] += g ** 2\n            params[i] -= (self.learning_rate / math.sqrt(self.G[i] + 1e-8)) * g\n\n\nclass Adadelta(BaseOptimizer):\n    def __init__(self, function, decay=0.9, gradient_func=numeric_gradient):\n        self.function = function\n        self.decay = decay\n        self.gradient_func = gradient_func\n        self.decaying_g = []\n        self.decaying_d = []\n\n    def optimize(self, params):\n        if not len(self.decaying_g) == len(params):\n            self.decaying_g = [0 for _ in range(len(params))]\n        if not len(self.decaying_d) == len(params):\n            self.decaying_d = [0 for _ in range(len(params))]\n\n        d_params = self.gradient_func(params,\n                                      self.function)\n        for i in range(len(params)):\n            g = d_params[i]\n            self.decay_update(self.decaying_g, i, g)\n            d = -(math.sqrt(self.decaying_d[i] + 1e-8) / math.sqrt(self.decaying_g[i] + 1e-8)) * g\n            params[i] += d\n            self.decay_update(self.decaying_d, i, d)\n\n    def decay_update(self, l, i, value):\n        l[i] = self.decay*l[i] + (1 - self.decay) * (value ** 2)\n\n\nclass RMSProp(BaseOptimizer):\n    def __init__(self, function, learning_rate=0.001, decay=0.9, gradient_func=numeric_gradient):\n        self.function = function\n        self.learning_rate = learning_rate\n        self.decay = decay\n        self.gradient_func = gradient_func\n        self.decaying_g = []\n\n    def optimize(self, params):\n        if not len(self.decaying_g) == len(params):\n            self.decaying_g = [0 for _ in range(len(params))]\n\n        d_params = self.gradient_func(params,\n                                      self.function)\n        for i in range(len(params)):\n            g = d_params[i]\n            self.decay_update(self.decaying_g, i, g)\n            d = -(self.learning_rate / math.sqrt(self.decaying_g[i] + 1e-8)) * g\n            params[i] += d\n\n    def decay_update(self, l, i, value):\n        l[i] = self.decay*l[i] + (1 - self.decay) * (value ** 2)\n\n\nclass Adam(BaseOptimizer):\n    def __init__(self, function, learning_rate=0.001, decay1=0.9, decay2=0.999, gradient_func=numeric_gradient):\n        self.function = function\n        self.learning_rate = learning_rate\n        self.decay1 = decay1\n        self.decay2 = decay2\n        self.gradient_func = gradient_func\n        self.decaying_v = []\n        self.decaying_m = []\n\n    def optimize(self, params):\n        if not len(self.decaying_v) == len(params):\n            self.decaying_v = [0 for _ in range(len(params))]\n        if not len(self.decaying_m) == len(params):\n            self.decaying_m = [0 for _ in range(len(params))]\n\n        d_params = self.gradient_func(params,\n                                      self.function)\n        for i in range(len(params)):\n            self.decay_updates(i, d_params[i])\n            m_hat = self.decaying_m[i] / (1 - self.decay1)\n            v_hat = self.decaying_v[i] / (1 - self.decay2)\n            d = -(self.learning_rate / (math.sqrt(v_hat) + 1e-8)) * m_hat\n            params[i] += d\n\n    def decay_updates(self, i, value):\n        self.decaying_m[i] = self.decay1 * self.decaying_m[i] + (1 - self.decay1) * value\n        self.decaying_v[i] = self.decay2 * self.decaying_v[i] + (1 - self.decay2) * (value ** 2)\n'"
playground.py,3,"b""import chained_operations as op\nimport optimizers\nimport plotting\nimport numpy as np\nimport itertools as it\n\ny_ = np.array([1, 0])\n\nx = op.Placeholder()\ng = op.Gradient(x)\n\ny = op.Mul(-1, op.Log(op.Mul(op.Sum(op.Mul(y_, op.Exp(x))), op.Reciprocal(op.Sum(op.Exp(x))))))\ny2 = op.Mul(-1, op.Log(op.Mul(op.Dot(y_, op.Exp(x)), op.Reciprocal(op.Sum(op.Exp(x))))))\n#y = op.Sum(op.Mul(-3, x))\n\n\ndef f(inp):\n    return op.run(y2, {x: inp})\n\n\ndef grad(inp, _):\n    op.run(y2, {x: inp})\n    return op.run(g)\n\ndata1 = np.random.rand(*y_.shape)\ndata2 = np.copy(data1)\ndata2[1] += 1\n\ngraph1 = plotting.Graph('numeric')\ngraph2 = plotting.Graph('backprop')\n\nopt1 = optimizers.NEG(f)\nopt2 = optimizers.NEG(f, gradient_func=grad)\n\nfor i in it.count():\n    evl = f(data1)\n    graph1.maybe_add(i, i, evl)\n    opt1.optimize(data1)\n\n    evl = f(data2)\n    graph2.maybe_add(i, i, evl)\n    opt2.optimize(data2)\n\n    plotting.replot()\n"""
plotting.py,0,"b""import matplotlib.pyplot as plt\nimport random\n\ngraphs = []\n\n\ndef set_ylabel(label):\n    plt.ylabel(label)\n\n\ndef replot():\n    plt.clf()\n    for graph in graphs:\n        args, kwargs = graph.get_plot_args()\n        plt.plot(*args, **kwargs)\n    plt.legend()\n    plt.pause(0.01)\n\n\nclass Graph(object):\n    def __init__(self, label='', points=1000):\n        self.label = label\n        self.points = points\n        graphs.append(self)\n        self.ys = []\n        self.xs = []\n\n    def maybe_add(self, i, x, y, plot=False):\n        if len(self.xs) == self.points:\n            r = random.randint(0 + 1, i)\n            if r < self.points:\n                self.pop(r)\n                self.add(x, y)\n                self.maybe_plot(plot)\n        else:\n            self.add(x, y)\n            self.maybe_plot(plot)\n\n    def pop(self, i):\n        self.xs.pop(i)\n        self.ys.pop(i)\n\n    def add(self, x, y):\n        self.xs.append(x)\n        self.ys.append(y)\n\n    def get_plot_args(self):\n        return (self.xs, self.ys), {'label': self.label}\n\n    def clear(self, plot=False):\n        self.xs.clear()\n        self.ys.clear()\n        self.maybe_plot(plot)\n\n    @staticmethod\n    def maybe_plot(plot):\n        if plot:\n            replot()\n\nplt.ion()\n"""
