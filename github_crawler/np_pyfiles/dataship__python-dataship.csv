file_path,api_count,code
setup.py,0,"b""from distutils.core import setup\nsetup(\n  name = 'dataship',\n  packages = ['dataship', 'dataship.beam'],\n  install_requires=['numpy', 'pandas'],\n  version = '0.7.0',\n  description = 'Lightweight tools for reading, writing and storing data, locally and over the internet.',\n  author = 'Waylon Flinn',\n  author_email = 'waylonflinn@gmail.com',\n  url = 'https://github.com/dataship/python-dataship', # use the URL to the github repo\n  keywords = ['numpy', 'pandas', 'data-science', 'javascript', 'interoperability', 'columnar', 'data', 'compressed'], # arbitrary keywords\n  classifiers = [],\n)\n"""
dataship/__init__.py,0,b''
dataship/beam/__init__.py,0,"b""from .beam import load\nfrom .beam import read\nfrom .beam import write\nfrom .beam import to_dataframe\nfrom .beam import from_dataframe\nfrom .beam import write_column\nfrom .beam import read_column\n__all__ = ['load', 'read', 'write', 'to_dataframe', 'from_dataframe', 'write_column', 'read_column']\n"""
dataship/beam/beam.py,5,"b'import os\nfrom functools import reduce\nimport json\nimport numpy as np\nimport pandas as pd\nimport lz4framed # multithread compatible, threads must be implemented by user\n\nEXTENSION_MAP = {\n    "".i8"" : ""int8"",\n    "".u8"" : ""uint8"",\n    "".i16"" : ""int16"",\n    "".u16"" : ""uint16"",\n    "".i32"" : ""int32"",\n    "".u32"" : ""uint32"",\n    "".f32"" : ""float32"",\n    "".f64"" : ""float64""\n}\n\nCOMPRESS_EXTENSION = "".lz4""\n\nKEYED_EXTENSION_MAP = {\n    "".k8"" : ""uint8"",\n    "".k16"" : ""uint16"",\n    "".k32"" : ""uint32""\n}\n\nreverse_extension_map = {value : key for key, value in EXTENSION_MAP.items()}\n\nreverse_keyed_extension_map = {value : key for key, value in KEYED_EXTENSION_MAP.items()}\n\ndef load(root_dir, index):\n    """"""Load data columns located at the given path using the given index.\n\n    Args:\n        root_dir: the path to the directory containing the data.\n        index: a dictionary mapping column names to file names.\n\n    Returns:\n        a dictionary mapping column names to arrays of data\n    """"""\n    if(not root_dir.endswith(\'/\')):\n        root_dir += ""/""\n\n    columns = {}\n    keys = None\n\n    # iterate through items in the index dictionary\n    for column_name, file_name in index.items():\n\n        column, key = read_column(root_dir, file_name)\n\n        if(key is not None):\n            if(keys is None):\n                keys = {}\n            keys[column_name] = key\n\n        columns[column_name] = column\n\n    return (columns, keys)\n\n\ndef read_column(root_dir, file_name):\n    basepath, ext = os.path.splitext(file_name)\n\n    compress = (ext == COMPRESS_EXTENSION)\n    if(compress):\n        basepath, ext = os.path.splitext(file_name[:-len(COMPRESS_EXTENSION)])\n\n    shape = None\n    if(\'.\' in basepath):\n        tensor_extensions = basepath[basepath.find(\'.\'):]\n        shape = parse_tensor_extensions(tensor_extensions)\n\n    column = None\n    key = None\n\n    if(ext == "".json""):\n        with open(root_dir + file_name, ""rt"") as json_file:\n            column = json.loads(json_file.read())\n    elif(ext in EXTENSION_MAP):\n        dtype = EXTENSION_MAP[ext]\n        with open(root_dir + file_name, \'rb\') as binary_file:\n            if(compress):\n                column = np.frombuffer(lz4framed.decompress(binary_file.read()), dtype=dtype)\n            else:\n                column = np.frombuffer(binary_file.read(), dtype=dtype)\n\n            if(shape is not None):\n                column.shape = get_final_shape(column, shape)\n\n    elif(ext in KEYED_EXTENSION_MAP):\n        dtype = KEYED_EXTENSION_MAP[ext]\n        with open(root_dir + file_name, \'rb\') as binary_file:\n            if(compress):\n                column = np.frombuffer(lz4framed.decompress(binary_file.read()), dtype=dtype)\n            else:\n                column = np.frombuffer(binary_file.read(), dtype=dtype)\n        with open(root_dir + file_name + "".key"", \'rt\') as key_file:\n            key = json.loads(key_file.read())\n\n    return column, key\n\ndef read(input_path):\n    """"""Look for an index in the specified directory and load it\'s data columns.\n    Args:\n        input_path: the path to the directory containing the data columns and\n            index, or the path to the index.\n\n    Returns:\n        a dictionary mapping column names to arrays of data\n    """"""\n\n    if(not input_path.endswith(""index.json"")):\n        if(not input_path.endswith(\'/\')):\n            input_path += ""/""\n        root_dir = input_path\n        input_path += ""index.json""\n    else:\n        root_dir = os.path.dirname(input_path) + ""/""\n\n    with open(input_path, ""rt"") as f:\n        index = json.loads(f.read())\n        return load(root_dir, index)\n\ndef write(root_dir, columns, keys=None, compact=True):\n    """"""Write data columns to the given directory.\n    Args:\n        root_dir: the path to the directory to write the columns to\n        columns: a dictionary mapping column names to column data\n\n    column data should be a list of strings, or a numpy array\n    """"""\n    if(not root_dir[-1] == ""/""): root_dir = root_dir + ""/""\n\n    index = {}\n\n    for column_name, column_data in columns.items():\n        filename = None\n        has_key = keys is not None and column_name in keys\n\n        if(has_key):\n            key_data = keys[column_name]\n        else:\n            key_data = None\n\n        filename = write_column(root_dir, column_name, column_data, key_data, compact)\n\n        index[column_name] = filename\n\n\n    with open(root_dir + ""index.json"", \'wt\') as f:\n        if(compact):\n            f.write(json.dumps(index))\n        else:\n            f.write(json.dumps(index, indent=1))\n\n\ndef write_column(root_dir , column_name, column_data, key_data=None, compact=True, compress=False, compress_level=3):\n\n        has_key = (key_data is not None)\n\n        if(type(column_data) == list and (type(column_data[0]) == str or type(column_data[0] == int))):\n            filename = column_name + "".json""\n            with open(root_dir + filename, ""wt"") as f:\n                if(compact):\n                    f.write(json.dumps(column_data))\n                else:\n                    f.write(json.dumps(column_data, indent=1))\n        elif(type(column_data) == np.ndarray):\n            dtype = column_data.dtype.name\n            if(has_key and dtype in reverse_keyed_extension_map):\n                ext = reverse_keyed_extension_map[dtype]\n            elif(dtype in reverse_extension_map):\n                ext = reverse_extension_map[dtype]\n            else:\n                raise Exception(""No mapping for dtype \'"" + dtype + ""\'"")\n\n            if(len(column_data.shape) > 1):\n                filename = column_name + get_tensor_extensions(column_data.shape) + ext\n            else:\n                filename = column_name + ext\n\n\n            if(compress):\n                filename += COMPRESS_EXTENSION\n            with open(root_dir + filename, \'wb\') as f:\n                if(compress):\n                    f.write(lz4framed.compress(column_data.tostring(), level=compress_level))\n                else:\n                    f.write(column_data.tostring())\n        else:\n            raise Exception(""Unknown type for column \'"" + column_name +""\': "" + str(type(column_data)))\n\n\n        # write key file, if present\n        if has_key:\n            with open(root_dir + filename + "".key"", \'wt\') as f:\n                if(compact):\n                    f.write(json.dumps(key_data))\n                else:\n                    f.write(json.dumps(key_data, indent=1))\n\n        return filename\n\ndef get_tensor_extensions(shape):\n\n    return \'.\' + \'.\'.join(\'t{}\'.format(size) for size in shape[1:])\n\ndef parse_tensor_extensions(tensor_extensions):\n    extension_list = tensor_extensions.split(\'.t\')\n\n    if(extension_list[0] != \'\'):\n        raise ValueError(""Tensor extensions malformed. ({})"".format(tensor_extensions))\n\n    extension_list = extension_list[1:]\n\n    # can we parse extensions as integers?\n    try:\n        shape = tuple(int(extension) for extension in extension_list)\n    except:\n        raise ValueError(""Tensor extensions malformed. ({})"".format(tensor_extensions))\n\n    return shape\n\ndef get_final_shape(column, shape):\n\n    object_size = reduce(lambda total, x : total * x, shape)\n\n    if(len(column) % object_size != 0):\n        raise ValueError(""Tensor dimensions ({}) not consistent with array size ({})."".format(object_size, len(column)))\n    object_count = len(column) // object_size\n\n    shape = (object_count, ) + shape\n\n    return shape\n\n\ndef to_dataframe(columns, keys=None):\n    """"""Turn a dictionary of columns into a Pandas dataframe.\n    """"""\n\n    column_dict = {}\n    for column_name, data in columns.items():\n        column_dict[column_name] = pd.Series(data)\n\n    return pd.DataFrame(column_dict)\n\ndef from_dataframe(df):\n    """"""Turn a dataframe into a dictionary of columns, suitable writing.""""""\n    index = {}\n\n    for column in df.columns:\n        values = df[column].values\n        if(values.dtype == ""object""):\n            index[column] = values.tolist()\n        else:\n            index[column] = values\n\n    return index\n'"
dataship/test/test_beam.py,20,"b'\nimport sys\nimport unittest\nimport numpy as np\nimport os\nimport shutil\n\nsys.path.append(\'../../\')\nfrom dataship import beam\n\nBASE_DIR = ""./data/""\nDATA_DIR = BASE_DIR + ""0001/"";\n\nINDEX = {\n\t""doric"" : ""doric.f32"",\n\t""tuscan"" : ""tuscan.u8"",\n\t""composite"" : ""composite.i32""\n};\n\n# close comparison constants\nRTOL = 1e-05\nATOL = 1e-07\n\n\nclass TestBeam(unittest.TestCase):\n\n\tdef test_number_of_columns_and_keys(self):\n\n\t\t# TODO add test for key columns\n\t\t(columns, _) = beam.load(DATA_DIR, INDEX)\n\n\t\texpected = 3\n\t\tactual = len(columns.keys())\n\t\tself.assertEqual(actual, expected)\n\n\tdef test_column_names(self):\n\n\t\t(columns, _) = beam.load(DATA_DIR, INDEX)\n\t\tself.assertTrue(""doric"" in columns)\n\t\tself.assertTrue(""tuscan"" in columns)\n\t\tself.assertTrue(""composite"" in columns)\n\t\t#self.assertTrue(""ionic"" in columns)\n\t\t#self.assertTrue(""corinthian"" in columns)\n\n\n# tape(""correctly named keys"", function(t){\n# \tt.plan(2);\n#\n# \tbeam.load(dir, index, function(err, results){\n#\n# \t\tt.assert(""ionic"" in results.keys, ""key present"");\n# \t\tt.assert(""corinthian"" in results.keys, ""key present"");\n# \t});\n# });\n#\n\n\tdef test_type_float32(self):\n\n\t\t(columns, _) = beam.load(DATA_DIR, INDEX)\n\t\tdoric = columns[""doric""]\n\t\tself.assertEqual(type(doric), np.ndarray)\n\t\tself.assertEqual(doric.dtype, np.float32)\n\n\tdef test_values_float32(self):\n\n\t\t(columns, _) = beam.load(DATA_DIR, INDEX)\n\t\tdoric = columns[""doric""]\n\t\tself.assertTrue(np.isclose(doric[0], 3.14159, RTOL, ATOL))\n\t\tself.assertTrue(np.isclose(doric[23], 1.0101010, RTOL, ATOL))\n\t\tself.assertTrue(np.isclose(doric[78], 2.7182818, RTOL, ATOL))\n\n\tdef test_type_uint8(self):\n\n\t\t(columns, _) = beam.load(DATA_DIR, INDEX)\n\t\ttuscan = columns[""tuscan""]\n\t\tself.assertEqual(type(tuscan), np.ndarray)\n\t\tself.assertEqual(tuscan.dtype, np.uint8)\n\n\tdef test_values_uint8(self):\n\n\t\t(columns, _) = beam.load(DATA_DIR, INDEX)\n\t\ttuscan = columns[""tuscan""]\n\t\tself.assertTrue(np.isclose(tuscan[0], 7, RTOL, ATOL))\n\t\tself.assertTrue(np.isclose(tuscan[13], 37, RTOL, ATOL))\n\t\tself.assertTrue(np.isclose(tuscan[34], 89, RTOL, ATOL))\n\n\tdef test_type_int32(self):\n\n\t\t(columns, _) = beam.load(DATA_DIR, INDEX)\n\t\tcomposite = columns[""composite""]\n\t\tself.assertEqual(type(composite), np.ndarray)\n\t\tself.assertEqual(composite.dtype, np.int32)\n\n\tdef test_values_int32(self):\n\n\t\t(columns, _) = beam.load(DATA_DIR, INDEX)\n\t\tcomposite = columns[""composite""]\n\t\tself.assertTrue(np.isclose(composite[0], 8956, RTOL, ATOL))\n\t\tself.assertTrue(np.isclose(composite[37], 13, RTOL, ATOL))\n\t\tself.assertTrue(np.isclose(composite[78], 78, RTOL, ATOL))\n\n\tdef test_write_and_read(self):\n\n\t\ttest_columns = {\n\t\t\t""test_a"" : np.zeros(100, dtype=""uint8""),\n\t\t\t""test_b"" : np.zeros(100, dtype=""float32""),\n\t\t\t""test_c"" : np.zeros(100, dtype=""int32""),\n\t\t\t""test_d"" : list(range(0, 100)),\n\t\t\t""test_e"" : [""""] * 100\n\t\t}\n\n\t\ttest_columns[""test_a""][11] = 53\n\t\ttest_columns[""test_b""][67] = 3.1415\n\t\ttest_columns[""test_c""][2] = 66532\n\t\ttest_columns[""test_e""][45] = ""hello""\n\n\t\twrite_dir = BASE_DIR  + ""9999/""\n\t\tif os.path.exists(write_dir):\n\t\t\tshutil.rmtree(write_dir)\n\n\t\tos.makedirs(write_dir)\n\n\n\t\tbeam.write(write_dir, test_columns)\n\n\t\t(columns, keys) = beam.read(write_dir)\n\n\t\tfor column_name in test_columns.keys():\n\n\t\t\tself.assertTrue(column_name in columns)\n\t\t\tif(type(test_columns[column_name]) == np.ndarray):\n\t\t\t\tself.assertTrue(np.allclose(test_columns[column_name], columns[column_name]))\n\t\t\telif(type(test_columns[column_name]) == list):\n\t\t\t\tself.assertTrue(test_columns[column_name] == columns[column_name])\n\n#\n# tape(""keys have correct values"", function(t){\n# \tt.plan(4);\n#\n# \tbeam.load(dir, index, function(err, results){\n#\n# \t\tvar ionic_k = results.keys[""ionic""];\n# \t\tt.equal(ionic_k[0], ""volute"", ""key values correct"");\n# \t\tt.equal(ionic_k[1], ""abacus"", ""key values correct"");\n# \t\tt.equal(ionic_k[2], ""shaft"", ""key values correct"");\n# \t\tt.equal(ionic_k[3], ""base"", ""key values correct"");\n# \t});\n# });\n\ndef main():\n\tunittest.main()\n\nif __name__ == \'__main__\':\n\tmain()\n'"
dataship/test/data/generate.py,3,"b'#! /usr/bin/env python3\nimport os\nimport numpy as np\n\nDATA_DIR = ""0001/"";\n\ndoric = np.zeros(100, dtype=np.float32)\ntuscan = np.zeros(100, dtype=np.uint8)\ncomposite = np.zeros(100, dtype=np.int32)\n\n\ndoric[0] = 3.141592;\ndoric[23] = 1.010101010;\ndoric[78] = 2.7182818;\n\ntuscan[0] = 7;\ntuscan[13] = 37;\ntuscan[34] = 89;\n\ncomposite[0] = 8956\ncomposite[37] = 13\ncomposite[78] = 78\n\nos.makedirs(DATA_DIR, exist_ok=True)\n\nwith open(DATA_DIR+""doric.f32"", ""wb"") as f:\n\tf.write(doric.tostring())\n\nwith open(DATA_DIR+""tuscan.u8"", ""wb"") as f:\n\tf.write(tuscan.tostring())\n\nwith open(DATA_DIR+""composite.i32"", ""wb"") as f:\n\tf.write(composite.tostring())\n'"
