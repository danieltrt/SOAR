file_path,api_count,code
figures.py,5,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# figures.py\n""""""\nSimple script to generate the figures in the README.md\n\nCopyright (c) 2020, David Hoffman\n""""""\n\nimport time\nimport warnings\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom skimage.external import tifffile as tif\n\nfrom pyotf.otf import HanserPSF, SheppardPSF, apply_named_aberration\nfrom pyotf.zernike import zernike, cart2pol, noll2name, noll2degrees, name2noll\nfrom pyotf.phaseretrieval import retrieve_phase\nfrom pyotf.utils import prep_data_for_PR\n\nOTF_MODEL = dict(\n    wl=525,  # units in nm\n    na=1.27,\n    ni=1.33,\n    res=90,\n    size=256,\n    zres=190,\n    zsize=128,\n    vec_corr=""none"",\n    condition=""none"",\n)\n\nSAVE = dict(dpi=150, transparent=True, bbox_inches=""tight"")\n\n\ndef otf_plots(model_kwargs):\n    """"""NOTE: the results are _very_ close on a qualitative scale, but they do not match exactly as\n    theory says they should (they\'re mathematically identical to one another)\n    """"""\n    # generate a comparison\n\n    psfs = HanserPSF(**model_kwargs), SheppardPSF(**model_kwargs)\n\n    fig, axs = plt.subplots(2, 2, figsize=(9, 6), gridspec_kw=dict(width_ratios=(1, 2)))\n\n    for psf, ax_sub in zip(psfs, axs):\n        # make coordinates\n        ax_yx, ax_zx = ax_sub\n        # get magnitude\n        otf = abs(psf.OTFi)\n        # normalize\n        otf /= otf.max()\n        otf /= otf.mean()\n        otf = np.log(otf + np.finfo(float).eps)\n\n        # plot\n        ax_yx.matshow(otf[otf.shape[0] // 2], vmin=-3, vmax=5, cmap=""inferno"")\n        ax_yx.set_title(""{} $k_y k_x$ plane"".format(psf.__class__.__name__))\n        ax_zx.matshow(otf[..., otf.shape[1] // 2], vmin=-3, vmax=5, cmap=""inferno"")\n        ax_zx.set_title(""{} $k_z k_x$ plane"".format(psf.__class__.__name__))\n\n        for ax in ax_sub:\n            ax.xaxis.set_major_locator(plt.NullLocator())\n            ax.yaxis.set_major_locator(plt.NullLocator())\n    fig.tight_layout()\n    fig.savefig(""fixtures/otf.png"", **SAVE)\n\n\ndef aberration_plots(model_kwargs):\n    model_kwargs = model_kwargs.copy()\n    model_kwargs[""zrange""] = [0]\n    model_kwargs[""vec_corr""] = ""total""\n    model_kwargs[""condition""] = ""sine""\n    model = HanserPSF(**model_kwargs)\n\n    mag = model.na / model.wl * model.res * 2 * np.pi\n\n    fig, axs = plt.subplots(3, 5, figsize=(12, 8))\n    # fill out plot\n    for ax, name in zip(axs.ravel(), name2noll.keys()):\n        model2 = apply_named_aberration(model, name, mag * 2)\n        ax.matshow(model2.PSFi.squeeze()[104:-104, 104:-104], cmap=""inferno"")\n        ax.set_xlabel(name.replace("" "", ""\\n"", 1).title())\n        ax.xaxis.set_major_locator(plt.NullLocator())\n        ax.yaxis.set_major_locator(plt.NullLocator())\n\n    fig.savefig(""fixtures/aberrations.png"", **SAVE)\n\n\ndef zernike_plots():\n    # make coordinates\n    x = np.linspace(-1, 1, 1025)\n    xx, yy = np.meshgrid(x, x)  # xy indexing is default\n    r, theta = cart2pol(yy, xx)\n    # set up plot\n    fig, axs = plt.subplots(3, 5, figsize=(20, 12))\n    # fill out plot\n    for ax, (k, v) in zip(axs.ravel(), noll2name.items()):\n        zern = zernike(r, theta, k, norm=False)\n        ax.matshow(np.ma.array(zern, mask=r > 1), vmin=-1, vmax=1, cmap=""coolwarm"")\n        ax.set_title(v + r"", $Z_{{{}}}^{{{}}}$"".format(*noll2degrees(k)))\n        ax.axis(""off"")\n    fig.tight_layout()\n\n    fig.savefig(""fixtures/zernike.png"", **SAVE)\n\n\ndef pr_plots():\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(""ignore"")\n        data = tif.imread(""fixtures/psf_wl520nm_z300nm_x130nm_na0.85_n1.0.tif"")\n\n    # prep data\n    data_prepped = prep_data_for_PR(data, 512)\n\n    # set up model params\n    params = dict(wl=520, na=0.85, ni=1.0, res=130, zres=300)\n\n    pr_result = retrieve_phase(data_prepped, params)\n\n    # plot\n    fig, axs = pr_result.plot()\n    fig.savefig(""fixtures/PR Result.png"", **SAVE)\n    fig, axs = pr_result.plot_convergence()\n    fig.savefig(""fixtures/PR Convergence.png"", **SAVE)\n\n    # fit to zernikes\n    pr_result.fit_to_zernikes(120)\n\n    # plot\n    fig, axs = pr_result.zd_result.plot_named_coefs()\n    fig.savefig(""fixtures/Named Coefs.png"", **SAVE)\n    pr_result.zd_result.plot_coefs()\n\n    fig, axs = pr_result.zd_result.plot()\n    fig.savefig(""fixtures/PR Result ZD.png"", **SAVE)\n\n\nif __name__ == ""__main__"":\n    otf_plots(OTF_MODEL)\n    zernike_plots()\n    aberration_plots(OTF_MODEL)\n    pr_plots()\n'"
setup.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# setup.py\n""""""\nSetup files\n\nCopyright (c) 2020, David Hoffman\n""""""\n\nimport setuptools\nimport versioneer\n\n# read in long description\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\n# get requirements\nwith open(""requirements.txt"", ""r"") as fh:\n    requirements = [line.strip() for line in fh]\n\nsetuptools.setup(\n    name=""pyotf"",\n    version=versioneer.get_version(),\n    cmdclass=versioneer.get_cmdclass(),\n    author=""David Hoffman"",\n    author_email=""dave.p.hoffman@gmail.com"",\n    url=""https://github.com/david-hoffman/pyOTF"",\n    description=""A python library for simulating and analyzing microscope point spread functions (PSFs)"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    license=""Apache V2.0"",\n    packages=setuptools.find_packages(),\n    classifiers=[\n        ""Development Status :: 3 - Alpha"",\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Natural Language :: English"",\n        ""Operating System :: OS Independent"",\n        ""Topic :: Scientific/Engineering"",\n    ],\n    python_requires="">=3"",\n    install_requires=requirements,\n)\n'"
versioneer.py,0,"b'# Version: 0.18\n\n""""""The Versioneer - like a rocketeer, but for versions.\n\nThe Versioneer\n==============\n\n* like a rocketeer, but for versions!\n* https://github.com/warner/python-versioneer\n* Brian Warner\n* License: Public Domain\n* Compatible With: python2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, and pypy\n* [![Latest Version]\n(https://pypip.in/version/versioneer/badge.svg?style=flat)\n](https://pypi.python.org/pypi/versioneer/)\n* [![Build Status]\n(https://travis-ci.org/warner/python-versioneer.png?branch=master)\n](https://travis-ci.org/warner/python-versioneer)\n\nThis is a tool for managing a recorded version number in distutils-based\npython projects. The goal is to remove the tedious and error-prone ""update\nthe embedded version string"" step from your release process. Making a new\nrelease should be as easy as recording a new tag in your version-control\nsystem, and maybe making new tarballs.\n\n\n## Quick Install\n\n* `pip install versioneer` to somewhere to your $PATH\n* add a `[versioneer]` section to your setup.cfg (see below)\n* run `versioneer install` in your source tree, commit the results\n\n## Version Identifiers\n\nSource trees come from a variety of places:\n\n* a version-control system checkout (mostly used by developers)\n* a nightly tarball, produced by build automation\n* a snapshot tarball, produced by a web-based VCS browser, like github\'s\n  ""tarball from tag"" feature\n* a release tarball, produced by ""setup.py sdist"", distributed through PyPI\n\nWithin each source tree, the version identifier (either a string or a number,\nthis tool is format-agnostic) can come from a variety of places:\n\n* ask the VCS tool itself, e.g. ""git describe"" (for checkouts), which knows\n  about recent ""tags"" and an absolute revision-id\n* the name of the directory into which the tarball was unpacked\n* an expanded VCS keyword ($Id$, etc)\n* a `_version.py` created by some earlier build step\n\nFor released software, the version identifier is closely related to a VCS\ntag. Some projects use tag names that include more than just the version\nstring (e.g. ""myproject-1.2"" instead of just ""1.2""), in which case the tool\nneeds to strip the tag prefix to extract the version identifier. For\nunreleased software (between tags), the version identifier should provide\nenough information to help developers recreate the same tree, while also\ngiving them an idea of roughly how old the tree is (after version 1.2, before\nversion 1.3). Many VCS systems can report a description that captures this,\nfor example `git describe --tags --dirty --always` reports things like\n""0.7-1-g574ab98-dirty"" to indicate that the checkout is one revision past the\n0.7 tag, has a unique revision id of ""574ab98"", and is ""dirty"" (it has\nuncommitted changes.\n\nThe version identifier is used for multiple purposes:\n\n* to allow the module to self-identify its version: `myproject.__version__`\n* to choose a name and prefix for a \'setup.py sdist\' tarball\n\n## Theory of Operation\n\nVersioneer works by adding a special `_version.py` file into your source\ntree, where your `__init__.py` can import it. This `_version.py` knows how to\ndynamically ask the VCS tool for version information at import time.\n\n`_version.py` also contains `$Revision$` markers, and the installation\nprocess marks `_version.py` to have this marker rewritten with a tag name\nduring the `git archive` command. As a result, generated tarballs will\ncontain enough information to get the proper version.\n\nTo allow `setup.py` to compute a version too, a `versioneer.py` is added to\nthe top level of your source tree, next to `setup.py` and the `setup.cfg`\nthat configures it. This overrides several distutils/setuptools commands to\ncompute the version when invoked, and changes `setup.py build` and `setup.py\nsdist` to replace `_version.py` with a small static file that contains just\nthe generated version data.\n\n## Installation\n\nSee [INSTALL.md](./INSTALL.md) for detailed installation instructions.\n\n## Version-String Flavors\n\nCode which uses Versioneer can learn about its version string at runtime by\nimporting `_version` from your main `__init__.py` file and running the\n`get_versions()` function. From the ""outside"" (e.g. in `setup.py`), you can\nimport the top-level `versioneer.py` and run `get_versions()`.\n\nBoth functions return a dictionary with different flavors of version\ninformation:\n\n* `[\'version\']`: A condensed version string, rendered using the selected\n  style. This is the most commonly used value for the project\'s version\n  string. The default ""pep440"" style yields strings like `0.11`,\n  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the ""Styles"" section\n  below for alternative styles.\n\n* `[\'full-revisionid\']`: detailed revision identifier. For Git, this is the\n  full SHA1 commit id, e.g. ""1076c978a8d3cfc70f408fe5974aa6c092c949ac"".\n\n* `[\'date\']`: Date and time of the latest `HEAD` commit. For Git, it is the\n  commit date in ISO 8601 format. This will be None if the date is not\n  available.\n\n* `[\'dirty\']`: a boolean, True if the tree has uncommitted changes. Note that\n  this is only accurate if run in a VCS checkout, otherwise it is likely to\n  be False or None\n\n* `[\'error\']`: if the version string could not be computed, this will be set\n  to a string describing the problem, otherwise it will be None. It may be\n  useful to throw an exception in setup.py if this is set, to avoid e.g.\n  creating tarballs with a version string of ""unknown"".\n\nSome variants are more useful than others. Including `full-revisionid` in a\nbug report should allow developers to reconstruct the exact code being tested\n(or indicate the presence of local changes that should be shared with the\ndevelopers). `version` is suitable for display in an ""about"" box or a CLI\n`--version` output: it can be easily compared against release notes and lists\nof bugs fixed in various releases.\n\nThe installer adds the following text to your `__init__.py` to place a basic\nversion in `YOURPROJECT.__version__`:\n\n    from ._version import get_versions\n    __version__ = get_versions()[\'version\']\n    del get_versions\n\n## Styles\n\nThe setup.cfg `style=` configuration controls how the VCS information is\nrendered into a version string.\n\nThe default style, ""pep440"", produces a PEP440-compliant string, equal to the\nun-prefixed tag name for actual releases, and containing an additional ""local\nversion"" section with more detail for in-between builds. For Git, this is\nTAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n--dirty --always`. For example ""0.11+2.g1076c97.dirty"" indicates that the\ntree is like the ""1076c97"" commit but has uncommitted changes ("".dirty""), and\nthat this commit is two revisions (""+2"") beyond the ""0.11"" tag. For released\nsoftware (exactly equal to a known tag), the identifier will only contain the\nstripped tag, e.g. ""0.11"".\n\nOther styles are available. See [details.md](details.md) in the Versioneer\nsource tree for descriptions.\n\n## Debugging\n\nVersioneer tries to avoid fatal errors: if something goes wrong, it will tend\nto return a version of ""0+unknown"". To investigate the problem, run `setup.py\nversion`, which will run the version-lookup code in a verbose mode, and will\ndisplay the full contents of `get_versions()` (including the `error` string,\nwhich may help identify what went wrong).\n\n## Known Limitations\n\nSome situations are known to cause problems for Versioneer. This details the\nmost significant ones. More can be found on Github\n[issues page](https://github.com/warner/python-versioneer/issues).\n\n### Subprojects\n\nVersioneer has limited support for source trees in which `setup.py` is not in\nthe root directory (e.g. `setup.py` and `.git/` are *not* siblings). The are\ntwo common reasons why `setup.py` might not be in the root:\n\n* Source trees which contain multiple subprojects, such as\n  [Buildbot](https://github.com/buildbot/buildbot), which contains both\n  ""master"" and ""slave"" subprojects, each with their own `setup.py`,\n  `setup.cfg`, and `tox.ini`. Projects like these produce multiple PyPI\n  distributions (and upload multiple independently-installable tarballs).\n* Source trees whose main purpose is to contain a C library, but which also\n  provide bindings to Python (and perhaps other langauges) in subdirectories.\n\nVersioneer will look for `.git` in parent directories, and most operations\nshould get the right version string. However `pip` and `setuptools` have bugs\nand implementation details which frequently cause `pip install .` from a\nsubproject directory to fail to find a correct version string (so it usually\ndefaults to `0+unknown`).\n\n`pip install --editable .` should work correctly. `setup.py install` might\nwork too.\n\nPip-8.1.1 is known to have this problem, but hopefully it will get fixed in\nsome later version.\n\n[Bug #38](https://github.com/warner/python-versioneer/issues/38) is tracking\nthis issue. The discussion in\n[PR #61](https://github.com/warner/python-versioneer/pull/61) describes the\nissue from the Versioneer side in more detail.\n[pip PR#3176](https://github.com/pypa/pip/pull/3176) and\n[pip PR#3615](https://github.com/pypa/pip/pull/3615) contain work to improve\npip to let Versioneer work correctly.\n\nVersioneer-0.16 and earlier only looked for a `.git` directory next to the\n`setup.cfg`, so subprojects were completely unsupported with those releases.\n\n### Editable installs with setuptools <= 18.5\n\n`setup.py develop` and `pip install --editable .` allow you to install a\nproject into a virtualenv once, then continue editing the source code (and\ntest) without re-installing after every change.\n\n""Entry-point scripts"" (`setup(entry_points={""console_scripts"": ..})`) are a\nconvenient way to specify executable scripts that should be installed along\nwith the python package.\n\nThese both work as expected when using modern setuptools. When using\nsetuptools-18.5 or earlier, however, certain operations will cause\n`pkg_resources.DistributionNotFound` errors when running the entrypoint\nscript, which must be resolved by re-installing the package. This happens\nwhen the install happens with one version, then the egg_info data is\nregenerated while a different version is checked out. Many setup.py commands\ncause egg_info to be rebuilt (including `sdist`, `wheel`, and installing into\na different virtualenv), so this can be surprising.\n\n[Bug #83](https://github.com/warner/python-versioneer/issues/83) describes\nthis one, but upgrading to a newer version of setuptools should probably\nresolve it.\n\n### Unicode version strings\n\nWhile Versioneer works (and is continually tested) with both Python 2 and\nPython 3, it is not entirely consistent with bytes-vs-unicode distinctions.\nNewer releases probably generate unicode version strings on py2. It\'s not\nclear that this is wrong, but it may be surprising for applications when then\nwrite these strings to a network connection or include them in bytes-oriented\nAPIs like cryptographic checksums.\n\n[Bug #71](https://github.com/warner/python-versioneer/issues/71) investigates\nthis question.\n\n\n## Updating Versioneer\n\nTo upgrade your project to a new release of Versioneer, do the following:\n\n* install the new Versioneer (`pip install -U versioneer` or equivalent)\n* edit `setup.cfg`, if necessary, to include any new configuration settings\n  indicated by the release notes. See [UPGRADING](./UPGRADING.md) for details.\n* re-run `versioneer install` in your source tree, to replace\n  `SRC/_version.py`\n* commit any changed files\n\n## Future Directions\n\nThis tool is designed to make it easily extended to other version-control\nsystems: all VCS-specific components are in separate directories like\nsrc/git/ . The top-level `versioneer.py` script is assembled from these\ncomponents by running make-versioneer.py . In the future, make-versioneer.py\nwill take a VCS name as an argument, and will construct a version of\n`versioneer.py` that is specific to the given VCS. It might also take the\nconfiguration arguments that are currently provided manually during\ninstallation by editing setup.py . Alternatively, it might go the other\ndirection and include code from all supported VCS systems, reducing the\nnumber of intermediate scripts.\n\n\n## License\n\nTo make Versioneer easier to embed, all its code is dedicated to the public\ndomain. The `_version.py` that it creates is also in the public domain.\nSpecifically, both are released under the Creative Commons ""Public Domain\nDedication"" license (CC0-1.0), as described in\nhttps://creativecommons.org/publicdomain/zero/1.0/ .\n\n""""""\n\nfrom __future__ import print_function\n\ntry:\n    import configparser\nexcept ImportError:\n    import ConfigParser as configparser\nimport errno\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_root():\n    """"""Get the project root directory.\n\n    We require that all commands are run from the project root, i.e. the\n    directory that contains setup.py, setup.cfg, and versioneer.py .\n    """"""\n    root = os.path.realpath(os.path.abspath(os.getcwd()))\n    setup_py = os.path.join(root, ""setup.py"")\n    versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        # allow \'python path/to/setup.py COMMAND\'\n        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n        setup_py = os.path.join(root, ""setup.py"")\n        versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        err = (\n            ""Versioneer was unable to run the project root directory. ""\n            ""Versioneer requires setup.py to be executed from ""\n            ""its immediate directory (like \'python setup.py COMMAND\'), ""\n            ""or in a way that lets it use sys.argv[0] to find the root ""\n            ""(like \'python path/to/setup.py COMMAND\').""\n        )\n        raise VersioneerBadRootError(err)\n    try:\n        # Certain runtime workflows (setup.py install/develop in a setuptools\n        # tree) execute all dependencies in a single python process, so\n        # ""versioneer"" may be imported multiple times, and python\'s shared\n        # module-import table will cache the first one. So we can\'t use\n        # os.path.dirname(__file__), as that will find whichever\n        # versioneer.py was first imported, even in later projects.\n        me = os.path.realpath(os.path.abspath(__file__))\n        me_dir = os.path.normcase(os.path.splitext(me)[0])\n        vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n        if me_dir != vsr_dir:\n            print(\n                ""Warning: build in %s is using versioneer.py from %s""\n                % (os.path.dirname(me), versioneer_py)\n            )\n    except NameError:\n        pass\n    return root\n\n\ndef get_config_from_root(root):\n    """"""Read the project setup.cfg file to determine Versioneer config.""""""\n    # This might raise EnvironmentError (if setup.cfg is missing), or\n    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n    # configparser.NoOptionError (if it lacks ""VCS=""). See the docstring at\n    # the top of versioneer.py for instructions on writing your setup.cfg .\n    setup_cfg = os.path.join(root, ""setup.cfg"")\n    parser = configparser.SafeConfigParser()\n    with open(setup_cfg, ""r"") as f:\n        parser.readfp(f)\n    VCS = parser.get(""versioneer"", ""VCS"")  # mandatory\n\n    def get(parser, name):\n        if parser.has_option(""versioneer"", name):\n            return parser.get(""versioneer"", name)\n        return None\n\n    cfg = VersioneerConfig()\n    cfg.VCS = VCS\n    cfg.style = get(parser, ""style"") or """"\n    cfg.versionfile_source = get(parser, ""versionfile_source"")\n    cfg.versionfile_build = get(parser, ""versionfile_build"")\n    cfg.tag_prefix = get(parser, ""tag_prefix"")\n    if cfg.tag_prefix in (""\'\'"", \'""""\'):\n        cfg.tag_prefix = """"\n    cfg.parentdir_prefix = get(parser, ""parentdir_prefix"")\n    cfg.verbose = get(parser, ""verbose"")\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\n# these dictionaries contain VCS-specific tools\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen(\n                [c] + args,\n                cwd=cwd,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=(subprocess.PIPE if hide_stderr else None),\n            )\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %s"" % (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n            print(""stdout was %s"" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\nLONG_VERSION_PY[\n    ""git""\n] = \'\'\'\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n""""""Git implementation of _version.py.""""""\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    """"""Get the keywords needed to look up the version information.""""""\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""%(DOLLAR)sFormat:%%d%(DOLLAR)s""\n    git_full = ""%(DOLLAR)sFormat:%%H%(DOLLAR)s""\n    git_date = ""%(DOLLAR)sFormat:%%ci%(DOLLAR)s""\n    keywords = {""refnames"": git_refnames, ""full"": git_full, ""date"": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_config():\n    """"""Create, populate and return the VersioneerConfig() object.""""""\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""%(STYLE)s""\n    cfg.tag_prefix = ""%(TAG_PREFIX)s""\n    cfg.parentdir_prefix = ""%(PARENTDIR_PREFIX)s""\n    cfg.versionfile_source = ""%(VERSIONFILE_SOURCE)s""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %%s"" %% dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %%s"" %% (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %%s (error)"" %% dispcmd)\n            print(""stdout was %%s"" %% stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {""version"": dirname[len(parentdir_prefix):],\n                    ""full-revisionid"": None,\n                    ""dirty"": False, ""error"": None, ""date"": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %%s but none started with prefix %%s"" %%\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %%d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%%s\', no digits"" %% "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %%s"" %% "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %%s"" %% r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None,\n                    ""date"": date}\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags"", ""date"": None}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %%s not under git control"" %% root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                          ""--always"", ""--long"",\n                                          ""--match"", ""%%s*"" %% tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%%s\'""\n                               %% describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                print(fmt %% (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                               %% (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                    cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%%ci"", ""HEAD""],\n                       cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%%d.g%%s"" %% (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%%d.g%%s"" %% (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%%d"" %% pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%%d"" %% pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%%s"" %% pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%%s"" %% pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""],\n                ""date"": None}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%%s\'"" %% style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None,\n            ""date"": pieces.get(""date"")}\n\n\ndef get_versions():\n    """"""Get version information or return default if unable to do so.""""""\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(\'/\'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {""version"": ""0+unknown"", ""full-revisionid"": None,\n                ""dirty"": None,\n                ""error"": ""unable to find root of source tree"",\n                ""date"": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to compute version"", ""date"": None}\n\'\'\'\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG) :] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r""\\d"", r)])\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            if verbose:\n                print(""picking %s"" % r)\n            return {\n                ""version"": r,\n                ""full-revisionid"": keywords[""full""].strip(),\n                ""dirty"": False,\n                ""error"": None,\n                ""date"": date,\n            }\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {\n        ""version"": ""0+unknown"",\n        ""full-revisionid"": keywords[""full""].strip(),\n        ""dirty"": False,\n        ""error"": ""no suitable tags"",\n        ""date"": None,\n    }\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root, hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %s not under git control"" % root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(\n        GITS,\n        [""describe"", ""--tags"", ""--dirty"", ""--always"", ""--long"", ""--match"", ""%s*"" % tag_prefix],\n        cwd=root,\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r""^(.+)-(\\d+)-g([0-9a-f]+)$"", git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = ""unable to parse git-describe output: \'%s\'"" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = ""tag \'%s\' doesn\'t start with prefix \'%s\'"" % (full_tag, tag_prefix)\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""], cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%ci"", ""HEAD""], cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef do_vcs_install(manifest_in, versionfile_source, ipy):\n    """"""Git-specific installation logic for Versioneer.\n\n    For Git, this means creating/changing .gitattributes to mark _version.py\n    for export-subst keyword substitution.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n    files = [manifest_in, versionfile_source]\n    if ipy:\n        files.append(ipy)\n    try:\n        me = __file__\n        if me.endswith("".pyc"") or me.endswith("".pyo""):\n            me = os.path.splitext(me)[0] + "".py""\n        versioneer_file = os.path.relpath(me)\n    except NameError:\n        versioneer_file = ""versioneer.py""\n    files.append(versioneer_file)\n    present = False\n    try:\n        f = open("".gitattributes"", ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(versionfile_source):\n                if ""export-subst"" in line.strip().split()[1:]:\n                    present = True\n        f.close()\n    except EnvironmentError:\n        pass\n    if not present:\n        f = open("".gitattributes"", ""a+"")\n        f.write(""%s export-subst\\n"" % versionfile_source)\n        f.close()\n        files.append("".gitattributes"")\n    run_command(GITS, [""add"", ""--""] + files)\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\n                ""version"": dirname[len(parentdir_prefix) :],\n                ""full-revisionid"": None,\n                ""dirty"": False,\n                ""error"": None,\n                ""date"": None,\n            }\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\n            ""Tried directories %s but none started with prefix %s""\n            % (str(rootdirs), parentdir_prefix)\n        )\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\nSHORT_VERSION_PY = """"""\n# This file was generated by \'versioneer.py\' (0.18) from\n# revision-control system data, or from the parent directory name of an\n# unpacked source archive. Distribution tarballs contain a pre-generated copy\n# of this file.\n\nimport json\n\nversion_json = \'\'\'\n%s\n\'\'\'  # END VERSION_JSON\n\n\ndef get_versions():\n    return json.loads(version_json)\n""""""\n\n\ndef versions_from_file(filename):\n    """"""Try to determine the version from _version.py if present.""""""\n    try:\n        with open(filename) as f:\n            contents = f.read()\n    except EnvironmentError:\n        raise NotThisMethod(""unable to read _version.py"")\n    mo = re.search(r""version_json = \'\'\'\\n(.*)\'\'\'  # END VERSION_JSON"", contents, re.M | re.S)\n    if not mo:\n        mo = re.search(r""version_json = \'\'\'\\r\\n(.*)\'\'\'  # END VERSION_JSON"", contents, re.M | re.S)\n    if not mo:\n        raise NotThisMethod(""no version_json in _version.py"")\n    return json.loads(mo.group(1))\n\n\ndef write_to_version_file(filename, versions):\n    """"""Write the given version number to the given _version.py file.""""""\n    os.unlink(filename)\n    contents = json.dumps(versions, sort_keys=True, indent=1, separators=("","", "": ""))\n    with open(filename, ""w"") as f:\n        f.write(SHORT_VERSION_PY % contents)\n\n    print(""set %s to \'%s\'"" % (filename, versions[""version""]))\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""], pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {\n            ""version"": ""unknown"",\n            ""full-revisionid"": pieces.get(""long""),\n            ""dirty"": None,\n            ""error"": pieces[""error""],\n            ""date"": None,\n        }\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {\n        ""version"": rendered,\n        ""full-revisionid"": pieces[""long""],\n        ""dirty"": pieces[""dirty""],\n        ""error"": None,\n        ""date"": pieces.get(""date""),\n    }\n\n\nclass VersioneerBadRootError(Exception):\n    """"""The project root directory is unknown or missing key files.""""""\n\n\ndef get_versions(verbose=False):\n    """"""Get the project version from whatever source is available.\n\n    Returns dict with two keys: \'version\' and \'full\'.\n    """"""\n    if ""versioneer"" in sys.modules:\n        # see the discussion in cmdclass.py:get_cmdclass()\n        del sys.modules[""versioneer""]\n\n    root = get_root()\n    cfg = get_config_from_root(root)\n\n    assert cfg.VCS is not None, ""please set [versioneer]VCS= in setup.cfg""\n    handlers = HANDLERS.get(cfg.VCS)\n    assert handlers, ""unrecognized VCS \'%s\'"" % cfg.VCS\n    verbose = verbose or cfg.verbose\n    assert cfg.versionfile_source is not None, ""please set versioneer.versionfile_source""\n    assert cfg.tag_prefix is not None, ""please set versioneer.tag_prefix""\n\n    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n    # extract version from first of: _version.py, VCS command (e.g. \'git\n    # describe\'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by \'setup.py sdist\',\n    # and for users of a tarball/zipball created by \'git archive\' or github\'s\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    get_keywords_f = handlers.get(""get_keywords"")\n    from_keywords_f = handlers.get(""keywords"")\n    if get_keywords_f and from_keywords_f:\n        try:\n            keywords = get_keywords_f(versionfile_abs)\n            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n            if verbose:\n                print(""got version from expanded keyword %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        ver = versions_from_file(versionfile_abs)\n        if verbose:\n            print(""got version from file %s %s"" % (versionfile_abs, ver))\n        return ver\n    except NotThisMethod:\n        pass\n\n    from_vcs_f = handlers.get(""pieces_from_vcs"")\n    if from_vcs_f:\n        try:\n            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n            ver = render(pieces, cfg.style)\n            if verbose:\n                print(""got version from VCS %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        if cfg.parentdir_prefix:\n            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n            if verbose:\n                print(""got version from parentdir %s"" % ver)\n            return ver\n    except NotThisMethod:\n        pass\n\n    if verbose:\n        print(""unable to compute version"")\n\n    return {\n        ""version"": ""0+unknown"",\n        ""full-revisionid"": None,\n        ""dirty"": None,\n        ""error"": ""unable to compute version"",\n        ""date"": None,\n    }\n\n\ndef get_version():\n    """"""Get the short version string for this project.""""""\n    return get_versions()[""version""]\n\n\ndef get_cmdclass():\n    """"""Get the custom setuptools/distutils subclasses used by Versioneer.""""""\n    if ""versioneer"" in sys.modules:\n        del sys.modules[""versioneer""]\n        # this fixes the ""python setup.py develop"" case (also \'install\' and\n        # \'easy_install .\'), in which subdependencies of the main project are\n        # built (using setup.py bdist_egg) in the same python process. Assume\n        # a main project A and a dependency B, which use different versions\n        # of Versioneer. A\'s setup.py imports A\'s Versioneer, leaving it in\n        # sys.modules by the time B\'s setup.py is executed, causing B to run\n        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n        # sandbox that restores sys.modules to it\'s pre-build state, so the\n        # parent is protected against the child\'s ""import versioneer"". By\n        # removing ourselves from sys.modules here, before the child build\n        # happens, we protect the child from the parent\'s versioneer too.\n        # Also see https://github.com/warner/python-versioneer/issues/52\n\n    cmds = {}\n\n    # we add ""version"" to both distutils and setuptools\n    from distutils.core import Command\n\n    class cmd_version(Command):\n        description = ""report generated version string""\n        user_options = []\n        boolean_options = []\n\n        def initialize_options(self):\n            pass\n\n        def finalize_options(self):\n            pass\n\n        def run(self):\n            vers = get_versions(verbose=True)\n            print(""Version: %s"" % vers[""version""])\n            print("" full-revisionid: %s"" % vers.get(""full-revisionid""))\n            print("" dirty: %s"" % vers.get(""dirty""))\n            print("" date: %s"" % vers.get(""date""))\n            if vers[""error""]:\n                print("" error: %s"" % vers[""error""])\n\n    cmds[""version""] = cmd_version\n\n    # we override ""build_py"" in both distutils and setuptools\n    #\n    # most invocation pathways end up running build_py:\n    #  distutils/build -> build_py\n    #  distutils/install -> distutils/build ->..\n    #  setuptools/bdist_wheel -> distutils/install ->..\n    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n    #  setuptools/install -> bdist_egg ->..\n    #  setuptools/develop -> ?\n    #  pip install:\n    #   copies source tree to a tempdir before running egg_info/etc\n    #   if .git isn\'t copied too, \'git describe\' will fail\n    #   then does setup.py bdist_wheel, or sometimes setup.py install\n    #  setup.py egg_info -> ?\n\n    # we override different ""build_py"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.build_py import build_py as _build_py\n    else:\n        from distutils.command.build_py import build_py as _build_py\n\n    class cmd_build_py(_build_py):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_py.run(self)\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if cfg.versionfile_build:\n                target_versionfile = os.path.join(self.build_lib, cfg.versionfile_build)\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n    cmds[""build_py""] = cmd_build_py\n\n    if ""cx_Freeze"" in sys.modules:  # cx_freeze enabled?\n        from cx_Freeze.dist import build_exe as _build_exe\n\n        # nczeczulin reports that py2exe won\'t like the pep440-style string\n        # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.\n        # setup(console=[{\n        #   ""version"": versioneer.get_version().split(""+"", 1)[0], # FILEVERSION\n        #   ""product_version"": versioneer.get_version(),\n        #   ...\n\n        class cmd_build_exe(_build_exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _build_exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(\n                        LONG\n                        % {\n                            ""DOLLAR"": ""$"",\n                            ""STYLE"": cfg.style,\n                            ""TAG_PREFIX"": cfg.tag_prefix,\n                            ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                            ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                        }\n                    )\n\n        cmds[""build_exe""] = cmd_build_exe\n        del cmds[""build_py""]\n\n    if ""py2exe"" in sys.modules:  # py2exe enabled?\n        try:\n            from py2exe.distutils_buildexe import py2exe as _py2exe  # py3\n        except ImportError:\n            from py2exe.build_exe import py2exe as _py2exe  # py2\n\n        class cmd_py2exe(_py2exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _py2exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(\n                        LONG\n                        % {\n                            ""DOLLAR"": ""$"",\n                            ""STYLE"": cfg.style,\n                            ""TAG_PREFIX"": cfg.tag_prefix,\n                            ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                            ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                        }\n                    )\n\n        cmds[""py2exe""] = cmd_py2exe\n\n    # we override different ""sdist"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.sdist import sdist as _sdist\n    else:\n        from distutils.command.sdist import sdist as _sdist\n\n    class cmd_sdist(_sdist):\n        def run(self):\n            versions = get_versions()\n            self._versioneer_generated_versions = versions\n            # unless we update this, the command will keep using the old\n            # version\n            self.distribution.metadata.version = versions[""version""]\n            return _sdist.run(self)\n\n        def make_release_tree(self, base_dir, files):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            _sdist.make_release_tree(self, base_dir, files)\n            # now locate _version.py in the new base_dir directory\n            # (remembering that it may be a hardlink) and replace it with an\n            # updated value\n            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n            print(""UPDATING %s"" % target_versionfile)\n            write_to_version_file(target_versionfile, self._versioneer_generated_versions)\n\n    cmds[""sdist""] = cmd_sdist\n\n    return cmds\n\n\nCONFIG_ERROR = """"""\nsetup.cfg is missing the necessary Versioneer configuration. You need\na section like:\n\n [versioneer]\n VCS = git\n style = pep440\n versionfile_source = src/myproject/_version.py\n versionfile_build = myproject/_version.py\n tag_prefix =\n parentdir_prefix = myproject-\n\nYou will also need to edit your setup.py to use the results:\n\n import versioneer\n setup(version=versioneer.get_version(),\n       cmdclass=versioneer.get_cmdclass(), ...)\n\nPlease read the docstring in ./versioneer.py for configuration instructions,\nedit setup.cfg, and re-run the installer or \'python versioneer.py setup\'.\n""""""\n\nSAMPLE_CONFIG = """"""\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run \'versioneer.py setup\' after changing this section, and commit the\n# resulting files.\n\n[versioneer]\n#VCS = git\n#style = pep440\n#versionfile_source =\n#versionfile_build =\n#tag_prefix =\n#parentdir_prefix =\n\n""""""\n\nINIT_PY_SNIPPET = """"""\nfrom ._version import get_versions\n__version__ = get_versions()[\'version\']\ndel get_versions\n""""""\n\n\ndef do_setup():\n    """"""Main VCS-independent setup function for installing Versioneer.""""""\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (EnvironmentError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):\n            print(""Adding sample versioneer config to setup.cfg"", file=sys.stderr)\n            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return 1\n\n    print("" creating %s"" % cfg.versionfile_source)\n    with open(cfg.versionfile_source, ""w"") as f:\n        LONG = LONG_VERSION_PY[cfg.VCS]\n        f.write(\n            LONG\n            % {\n                ""DOLLAR"": ""$"",\n                ""STYLE"": cfg.style,\n                ""TAG_PREFIX"": cfg.tag_prefix,\n                ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n            }\n        )\n\n    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), ""__init__.py"")\n    if os.path.exists(ipy):\n        try:\n            with open(ipy, ""r"") as f:\n                old = f.read()\n        except EnvironmentError:\n            old = """"\n        if INIT_PY_SNIPPET not in old:\n            print("" appending to %s"" % ipy)\n            with open(ipy, ""a"") as f:\n                f.write(INIT_PY_SNIPPET)\n        else:\n            print("" %s unmodified"" % ipy)\n    else:\n        print("" %s doesn\'t exist, ok"" % ipy)\n        ipy = None\n\n    # Make sure both the top-level ""versioneer.py"" and versionfile_source\n    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n    # they\'ll be copied into source distributions. Pip won\'t be able to\n    # install the package without this.\n    manifest_in = os.path.join(root, ""MANIFEST.in"")\n    simple_includes = set()\n    try:\n        with open(manifest_in, ""r"") as f:\n            for line in f:\n                if line.startswith(""include ""):\n                    for include in line.split()[1:]:\n                        simple_includes.add(include)\n    except EnvironmentError:\n        pass\n    # That doesn\'t cover everything MANIFEST.in can do\n    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n    # it might give some false negatives. Appending redundant \'include\'\n    # lines is safe, though.\n    if ""versioneer.py"" not in simple_includes:\n        print("" appending \'versioneer.py\' to MANIFEST.in"")\n        with open(manifest_in, ""a"") as f:\n            f.write(""include versioneer.py\\n"")\n    else:\n        print("" \'versioneer.py\' already in MANIFEST.in"")\n    if cfg.versionfile_source not in simple_includes:\n        print("" appending versionfile_source (\'%s\') to MANIFEST.in"" % cfg.versionfile_source)\n        with open(manifest_in, ""a"") as f:\n            f.write(""include %s\\n"" % cfg.versionfile_source)\n    else:\n        print("" versionfile_source already in MANIFEST.in"")\n\n    # Make VCS-specific changes. For git, this means creating/changing\n    # .gitattributes to mark _version.py for export-subst keyword\n    # substitution.\n    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)\n    return 0\n\n\ndef scan_setup_py():\n    """"""Validate the contents of setup.py against Versioneer\'s expectations.""""""\n    found = set()\n    setters = False\n    errors = 0\n    with open(""setup.py"", ""r"") as f:\n        for line in f.readlines():\n            if ""import versioneer"" in line:\n                found.add(""import"")\n            if ""versioneer.get_cmdclass()"" in line:\n                found.add(""cmdclass"")\n            if ""versioneer.get_version()"" in line:\n                found.add(""get_version"")\n            if ""versioneer.VCS"" in line:\n                setters = True\n            if ""versioneer.versionfile_source"" in line:\n                setters = True\n    if len(found) != 3:\n        print("""")\n        print(""Your setup.py appears to be missing some important items"")\n        print(""(but I might be wrong). Please make sure it has something"")\n        print(""roughly like the following:"")\n        print("""")\n        print("" import versioneer"")\n        print("" setup( version=versioneer.get_version(),"")\n        print(""        cmdclass=versioneer.get_cmdclass(),  ...)"")\n        print("""")\n        errors += 1\n    if setters:\n        print(""You should remove lines like \'versioneer.VCS = \' and"")\n        print(""\'versioneer.versionfile_source = \' . This configuration"")\n        print(""now lives in setup.cfg, and should be removed from setup.py"")\n        print("""")\n        errors += 1\n    return errors\n\n\nif __name__ == ""__main__"":\n    cmd = sys.argv[1]\n    if cmd == ""setup"":\n        errors = do_setup()\n        errors += scan_setup_py()\n        if errors:\n            sys.exit(1)\n'"
pyotf/__init__.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# __init__.py\n""""""\nA package to simulate optical transfer functions and point spread functions\nand perform phase retrieval on experimental data.\n\nhttps://en.wikipedia.org/wiki/Optical_transfer_function\nhttps://en.wikipedia.org/wiki/Point_spread_function\nhttps://en.wikipedia.org/wiki/Phase_retrieval\n\nCopyright (c) 2016, David Hoffman\n""""""\n\nfrom ._version import get_versions\n\n__version__ = get_versions()[""version""]\ndel get_versions\n'"
pyotf/_version.py,0,"b'# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n""""""Git implementation of _version.py.""""""\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    """"""Get the keywords needed to look up the version information.""""""\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""$Format:%d$""\n    git_full = ""$Format:%H$""\n    git_date = ""$Format:%ci$""\n    keywords = {""refnames"": git_refnames, ""full"": git_full, ""date"": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_config():\n    """"""Create, populate and return the VersioneerConfig() object.""""""\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""pep440""\n    cfg.tag_prefix = """"\n    cfg.parentdir_prefix = """"\n    cfg.versionfile_source = ""pyotf/_version.py""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen(\n                [c] + args,\n                cwd=cwd,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=(subprocess.PIPE if hide_stderr else None),\n            )\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %s"" % (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n            print(""stdout was %s"" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\n                ""version"": dirname[len(parentdir_prefix) :],\n                ""full-revisionid"": None,\n                ""dirty"": False,\n                ""error"": None,\n                ""date"": None,\n            }\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\n            ""Tried directories %s but none started with prefix %s""\n            % (str(rootdirs), parentdir_prefix)\n        )\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG) :] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r""\\d"", r)])\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            if verbose:\n                print(""picking %s"" % r)\n            return {\n                ""version"": r,\n                ""full-revisionid"": keywords[""full""].strip(),\n                ""dirty"": False,\n                ""error"": None,\n                ""date"": date,\n            }\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {\n        ""version"": ""0+unknown"",\n        ""full-revisionid"": keywords[""full""].strip(),\n        ""dirty"": False,\n        ""error"": ""no suitable tags"",\n        ""date"": None,\n    }\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root, hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %s not under git control"" % root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(\n        GITS,\n        [""describe"", ""--tags"", ""--dirty"", ""--always"", ""--long"", ""--match"", ""%s*"" % tag_prefix],\n        cwd=root,\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r""^(.+)-(\\d+)-g([0-9a-f]+)$"", git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = ""unable to parse git-describe output: \'%s\'"" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = ""tag \'%s\' doesn\'t start with prefix \'%s\'"" % (full_tag, tag_prefix)\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""], cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%ci"", ""HEAD""], cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""], pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {\n            ""version"": ""unknown"",\n            ""full-revisionid"": pieces.get(""long""),\n            ""dirty"": None,\n            ""error"": pieces[""error""],\n            ""date"": None,\n        }\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {\n        ""version"": rendered,\n        ""full-revisionid"": pieces[""long""],\n        ""dirty"": pieces[""dirty""],\n        ""error"": None,\n        ""date"": pieces.get(""date""),\n    }\n\n\ndef get_versions():\n    """"""Get version information or return default if unable to do so.""""""\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(""/""):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\n            ""version"": ""0+unknown"",\n            ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to find root of source tree"",\n            ""date"": None,\n        }\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\n        ""version"": ""0+unknown"",\n        ""full-revisionid"": None,\n        ""dirty"": None,\n        ""error"": ""unable to compute version"",\n        ""date"": None,\n    }\n'"
pyotf/labview.py,2,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# labview.py\n""""""\nThin wrapper so that labview can perform phase retrieval.\n\nCopyright (c) 2018, David Hoffman\n""""""\n\nfrom pyotf.phaseretrieval import retrieve_phase\nfrom pyotf.utils import prep_data_for_PR\nimport numpy as np\n\n\ndef labview(\n    data, wl, na, ni, res, zres, max_iters=200, pupil_tol=1e-8, mse_tol=1e-8, phase_only=False\n):\n    """"""Generate a PSF object\n\n    Parameters\n    ----------\n    data : ndarray (3 dim)\n        The experimentally measured PSF of a subdiffractive source\n    wl : numeric\n        Emission wavelength of the simulation\n    na : numeric\n        Numerical aperature of the simulation\n    ni : numeric\n        index of refraction for the media\n    res : numeric\n        x/y resolution of the simulation, must have same units as wl\n    zres : numeric\n        z resolution of simuation, must have same units a wl\n    max_iters : int\n        The maximum number of iterations to run, default is 200\n    pupil_tol : float\n        the tolerance in percent change in change in pupil, default is 1e-8\n    mse_tol : float\n        the tolerance in percent change for the mean squared error between\n        data and simulated data, default is 1e-8\n    phase_only : bool\n        True means only the phase of the back pupil is retrieved while the\n        amplitude is not.\n    """"""\n\n    # Convert from LabVIEW data types to Python types\n    data = np.asarray(data)  # LabView converts arrays to lists.  This will convert back to arrays.\n    params = dict(\n        wl=wl, na=na, ni=ni, res=res, zres=zres\n    )  # LabView clusters appear as tuples.  This creates the dict.\n\n    # Preprocess data (background removal, filtering, etc...)\n    data_prepped = prep_data_for_PR(data)\n\n    # Phase retrieval\n    pr_result = retrieve_phase(data_prepped, params, max_iters, pupil_tol, mse_tol, phase_only)\n\n    return (\n        pr_result.phase.tolist()\n    )  # LabVIEW wants lists instead of arrays.  This will convert back to lists.\n\n\nif __name__ == ""__main__"":\n    # phase retrieve a pupil\n    import os\n    import time\n    import logging\n    from skimage.external import tifffile as tif\n    from matplotlib import pyplot as plt\n\n    logging.basicConfig()\n    logging.getLogger().setLevel(logging.INFO)\n    # read in data from fixtures\n    data = tif.imread(\n        os.path.split(__file__)[0] + ""/fixtures/psf_wl520nm_z300nm_x130nm_na0.85_n1.0.tif""\n    )\n    # set up model params\n    params = dict(wl=520, na=0.85, ni=1.0, res=130, zres=300)\n    # retrieve the phase\n    pr_start = time.time()\n    print(""Starting phase retrieval with data of size {}"".format(data.shape))\n    phase = labview(data[6:-5], **params, pupil_tol=1e-6)\n    phase = np.asarray(phase)\n    print(""It took {} seconds to retrieve the pupil function"".format(time.time() - pr_start))\n    # plot\n    max_val = abs(phase).max()\n    plt.matshow(phase, cmap=""seismic"", vmin=-max_val, vmax=max_val)\n    plt.colorbar()\n    # show\n    plt.show()\n'"
pyotf/microscope.py,18,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# microscope.py\n""""""\nA module to simulate optical transfer functions and point spread functions\nfor specific types of microscopes\n\nCurrently the available microscopes are:\n- Widefield Epi\n- Confocal\n- Apotome\n\nSee notebooks/Microscope Imaging Models for more details\n\nCopyright (c) 2020, David Hoffman\n""""""\n\nimport copy\n\nimport numpy as np\nfrom scipy.signal import fftconvolve\n\nfrom .otf import HanserPSF, SheppardPSF\nfrom .utils import (\n    cached_property,\n    easy_fft,\n    easy_ifft,\n    bin_ndarray,\n    NumericProperty,\n    radial_profile,\n)\n\nMODELS = {\n    ""hanser"": HanserPSF,\n    ""sheppard"": SheppardPSF,\n}\n\n\ndef choose_model(model):\n    try:\n        return MODELS[model.lower()]\n    except KeyError:\n        raise ValueError(\n            f""Model {model:} doesn\'t exist please choose one of: "" + "", "".join(MODELS.keys())\n        )\n\n\nclass WidefieldMicroscope(object):\n    """"""A base class for microscope models""""""\n\n    oversample_factor = NumericProperty(\n        attr=""_oversample_factor"",\n        vartype=int,\n        doc=""By how much do you want to oversample the simulation"",\n    )\n\n    def __init__(self, *, model, na, ni, wl, size, pixel_size, oversample_factor, **kwargs):\n        # set zsize and zres here because we don\'t want to oversample there.\n\n        self.oversample_factor = oversample_factor\n\n        self.psf_params = dict(na=na, ni=ni, wl=wl, zres=pixel_size, zsize=size)\n        self.psf_params.update(kwargs)\n\n        assert self.oversample_factor % 2 == 1, ""oversample_factor must be odd""\n\n        if oversample_factor == 1:\n            self.psf_params[""size""] = size\n            self.psf_params[""res""] = pixel_size\n        elif oversample_factor > 1:\n            self.psf_params[""size""] = size * oversample_factor\n            self.psf_params[""res""] = pixel_size / oversample_factor\n        else:\n            raise ValueError(""oversample_factor must be positive"")\n\n        self.oversample_factor = oversample_factor\n\n        self.model = choose_model(model)(**self.psf_params)\n\n    def _attribute_changed(self):\n        """"""What to do if an attribute has changed.""""""\n        # try removing the PSF\n        try:\n            del self.PSF\n        except AttributeError:\n            pass\n        # try removing the PSF\n        try:\n            del self.OTF\n        except AttributeError:\n            pass\n\n    @property\n    def model_psf(self):\n        return self.model.PSFi\n\n    @cached_property\n    def PSF(self):\n        """"""The point spread function of the microscope""""""\n        psf = self.model_psf\n        if self.oversample_factor > 1:\n            if self.psf_params[""size""] % 2 == 0:\n                # if we\'re even then we\'ll be in the upper left hand corner of the super pixel\n                # and we\'ll need to shift to the bottom and right by oversample_factor // 2\n                shift = self.oversample_factor // 2\n                psf = np.roll(psf, (shift, shift), axis=(1, 2))\n\n            # only bin in the lateral direction\n            psf = bin_ndarray(psf, bin_size=(1, self.oversample_factor, self.oversample_factor))\n\n        # normalize psf\n        return psf / psf.sum()\n\n    @cached_property\n    def OTF(self):\n        return easy_fft(self.PSF)\n\n\ndef _disk_kernel(radius):\n    """"""Model of the pinhole transmission function""""""\n    full_size = int(np.ceil(radius * 2))\n    if full_size % 2 == 0:\n        full_size += 1\n    coords = np.indices((full_size, full_size)) - (full_size - 1) // 2\n    r = np.sqrt((coords ** 2).sum(0))\n    kernel = r < radius\n    return kernel / kernel.sum()\n\n\nclass ConfocalMicroscope(WidefieldMicroscope):\n    """"""A class representing a confocal microscope""""""\n\n    pinhole_size = NumericProperty(\n        attr=""_pinhole_size"",\n        vartype=(float, int),\n        doc=""Size of the pinhole (in airy units relative to emission wavelength"",\n    )\n\n    wl_exc = NumericProperty(attr=""_wl_exc"", vartype=float, doc=""The excitation wavelength"")\n\n    def __init__(self, *, wl_exc, pinhole_size, **kwargs):\n        super().__init__(**kwargs)\n        self.pinhole_size = pinhole_size\n\n        # make the emission PSF\n        self.model_exc = copy.deepcopy(self.model)\n        self.model_exc.wl = wl_exc\n\n    @property\n    def model_psf(self):\n        """"""The oversampled confocal PSF""""""\n        # Calculate the AU in pixels\n        airy_unit = 1.22 * self.model.wl / self.model.na / self.model.res\n        # Calculate the pinhole radius in pixels\n        pixel_pinhole_radius = self.pinhole_size * airy_unit / 2\n        #\n        if pixel_pinhole_radius > 1.5:\n            kernel = _disk_kernel(pixel_pinhole_radius)\n            psf_det_au = fftconvolve(self.model.PSFi, kernel[None], ""same"", axes=(1, 2))\n        else:\n            psf_det_au = self.model.PSFi\n        psf_con_au = psf_det_au * self.model_exc.PSFi\n        return psf_con_au\n\n\nclass ApotomeMicroscope(WidefieldMicroscope):\n    """"""A class representing the approximate PSF/OTF for the apotome microscope\n\n    This is a poor approximation (see notebooks) and thus has limited functionality.\n\n    The grid pattern is set at half NA\n    \n    https://www.zeiss.com/microscopy/us/products/imaging-systems/apotome-2-for-biology.html\n    https://www.osapublishing.org/abstract.cfm?URI=ol-22-24-1905\n    http://www.sciencedirect.com/science/article/pii/S0030401898002107\n    """"""\n\n    @property\n    def model_psf(self):\n        """"""The oversampled apotome PSF""""""\n        # make the hybrid OTF\n        hybrid_otf = easy_fft(self.model.PSFi, axes=(1, 2))\n        # get the radial average\n        rotf = np.array([radial_profile(o)[0] for o in hybrid_otf])\n        rotf /= rotf.max()\n        rotf = abs(rotf)\n\n        # define the Abbe diffraction limit in frequency space pixels.\n        nyquist_sampling = self.psf_params[""wl""] / self.psf_params[""na""] / 4\n        abbe_limit = int(\n            np.rint(self.psf_params[""size""] * self.psf_params[""res""] / nyquist_sampling / 2)\n        )\n\n        # define the approximate axial response of the system\n        axial_profile = rotf[:, abbe_limit // 2]\n\n        psf_apotome = axial_profile[:, None, None] * self.model.PSFi\n        return psf_apotome\n\n\nclass BaseSIMMicroscope(WidefieldMicroscope):\n    """"""A base class for SIM and SIM like microscopes""""""\n\n    na_exc = NumericProperty(attr=""_na_exc"", vartype=float, doc=""The excitation NA"")\n    wl_exc = NumericProperty(attr=""_wl_exc"", vartype=float, doc=""The excitation wavelength"")\n    coherent = NumericProperty(\n        attr=""_coherent"", vartype=bool, doc=""Treat the orientations coherently?""\n    )\n    dc = NumericProperty(attr=""_dc"", vartype=bool, doc=""Include the DC component"")\n    dc_suppress = NumericProperty(\n        attr=""_dc_suppress"", vartype=bool, doc=""Suppress the DC component""\n    )\n\n    def __init__(\n        self, *, na_exc, wl_exc, wiener, coherent, dc, dc_suppress, orientations, **kwargs\n    ):\n        """"""orientations : sequence\n            The different orentation angles of the excitation\n        wiener : float or None\n            The value of the wiener paramter, if None is passed then no deconvolution is performed\n        """"""\n        super().__init__(**kwargs)\n\n        if na_exc is None:\n            na_exc = self.psf_params[""na""]\n        self.na_exc = na_exc\n\n        if wl_exc is None:\n            wl_exc = self.psf_params[""wl""]\n        self.wl_exc = wl_exc\n\n        self.coherent = coherent\n        self.dc = dc\n        self.dc_suppress = dc_suppress\n\n        self.orientations = orientations\n\n        self.wiener = wiener\n        if self.wiener < 0:\n            raise ValueError(f""self.wiener is {self.wiener:} which should be greater than zero"")\n\n    @property\n    def model_psf(self):\n        """"""The oversampled SIM PSF\n\n        This is by no means the most general implementation, but it seems to serve\n        all my use cases\n        """"""\n        # centered coordinate system.\n        x = (\n            np.arange(self.psf_params[""size""]) - (self.psf_params[""size""] + 1) // 2\n        ) * self.psf_params[""res""]\n\n        # open grid\n        zz, yy, xx = x[:, None, None], x[None, :, None], x[None, None, :]\n\n        # magnitude of the excitation k vector (spatial frequency of a plane wave with\n        # wavelength self.exc_wl)\n        freq = 2 * np.pi * self.psf_params[""ni""] / self.wl_exc\n\n        # the angle in frequency space that the exciation waves make with the kz axis\n        alpha = np.arcsin(self.na_exc / self.psf_params[""ni""])\n\n        # are we applying a wiener filter?\n        if self.wiener is None:\n            psf = self.model.PSFi\n        elif self.wiener >= 0:\n            # https://en.wikipedia.org/wiki/Wiener_deconvolution\n            otf = abs(self.model.OTFi) ** 2\n            if self.wiener == 0:\n                # everything within OTF support is 1\n                wiener_otf = otf > 1e-16\n            else:\n                w = otf.max() / self.wiener ** 2\n                wiener_otf = otf / (otf + w)\n            # The PSF is real, discard the imaginary part\n            # we don\'t take the absolute value because\n            # wiener deconvolution doesn\'t prescribe it\n            psf = easy_ifft(wiener_otf).real\n        else:\n            raise ValueError(f""self.wiener is {self.wiener:} which should be greater than zero"")\n\n        # Are we including a DC beam?\n        if self.dc:\n            base_pattern = np.exp(1j * zz * freq) * np.ones(psf.shape[:2], dtype=complex)\n        else:\n            base_pattern = np.zeros(psf.shape, dtype=complex)\n\n        # If we\'re not coherent, initialize the PSF\n        if not self.coherent:\n            sim_psf = np.zeros_like(psf)\n\n        # loop through orientations\n        for orientation in self.orientations:\n            # if we\'re not coherent reinitialize for the new orientation.\n            if not self.coherent:\n                exc_pattern = base_pattern.copy()\n            else:\n                exc_pattern = base_pattern\n\n            # Calculate lateral rotated coordinate system\n            rr = xx * np.cos(orientation) + yy * np.sin(orientation)\n\n            # Again, not the most general, but covers most cases\n            # There\'s symmetry to be exploited here for computational\n            # gains.\n\n            # build up the excitation pattern\n            for theta in (-alpha, alpha):\n                exc_pattern += np.exp(1j * ((rr * np.sin(theta) + zz * np.cos(theta)) * freq))\n\n            # if we\'re not coherent sum the effective PSFs for each orientation\n            if not self.coherent:\n                sim_psf += psf * (abs(exc_pattern) ** 2)\n\n        # if we are coherent then just calculate the total PSF\n        if self.coherent:\n            sim_psf = psf * (abs(exc_pattern) ** 2)\n        else:\n            # If we want to suppress the DC component do it here.\n            if self.dc_suppress:\n                sim_psf -= (2 * len(self.orientations) - 1) * psf\n\n        return sim_psf\n\n\nclass SIM2DMicroscope(BaseSIMMicroscope):\n    """"""A class for 2D-SIM, including optical sectioning SIM""""""\n\n    def __init__(self, **kwargs):\n        super().__init__(coherent=False, dc=False, **kwargs)\n\n\nclass SIM3DMicroscope(BaseSIMMicroscope):\n    """"""A class for 3D-SIM using multiple pattern orientations""""""\n\n    def __init__(self, **kwargs):\n        super().__init__(coherent=False, dc=True, **kwargs)\n\n\nclass LatticeSIMMicroscope(BaseSIMMicroscope):\n    """"""A class for Zeiss Lattice SIM\n\n    https://www.zeiss.com/microscopy/us/products/elyra-7-with-lattice-sim-for-fast-and-gentle-3d-superresolution-microscopy.html\n    """"""\n\n    def __init__(self, **kwargs):\n        super().__init__(coherent=True, dc=True, orientations=(0, np.pi / 2), **kwargs)\n\n\nif __name__ == ""__main__"":\n    import matplotlib as mpl\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.axes_grid1 import ImageGrid\n\n    base_psf_params = {\n        ""model"": ""sheppard"",\n        ""oversample_factor"": 1,\n        ""pixel_size"": 0.05,\n        ""na"": 1.27,\n        ""ni"": 1.33,\n        ""wl"": 0.585,\n        ""size"": 128,\n        ""vec_corr"": ""none"",\n    }\n\n    sim_psf_params = {""na_exc"": None, ""wl_exc"": 0.561, ""wiener"": 10, ""dc_suppress"": True}\n\n    sim_psf_params.update(base_psf_params)\n\n    orientations = (0, 2 * np.pi / 3, 4 * np.pi / 3)\n\n    psfs = (\n        WidefieldMicroscope(**base_psf_params),\n        ConfocalMicroscope(**base_psf_params, pinhole_size=1.5, wl_exc=0.561),\n        ConfocalMicroscope(**base_psf_params, pinhole_size=0, wl_exc=0.561),\n        SIM2DMicroscope(\n            orientations=orientations, **{**sim_psf_params, ""na_exc"": sim_psf_params[""na""] / 2}\n        ),\n        SIM2DMicroscope(orientations=orientations, **sim_psf_params),\n        SIM3DMicroscope(orientations=orientations, **sim_psf_params),\n        LatticeSIMMicroscope(**sim_psf_params),\n    )\n\n    labels = (""Epi"", ""Confocal 1.5 AU"", ""AiryScan"", ""OS-SIM"", ""2D-SIM"", ""3D-SIM"", ""Lattice SIM"")\n\n    ncols = len(psfs)\n    gam = 0.5\n    interpolation = ""bicubic""\n    vmin = 1e-3\n    res = base_psf_params[""pixel_size""]\n\n    assert ncols == len(labels), ""Lengths mismatched""\n    assert ncols < 10\n\n    plot_size = 1.5\n\n    fig = plt.figure(\n        None,\n        (plot_size * ncols, plot_size * 4),\n        subplotpars=mpl.figure.SubplotParams(bottom=0.015, left=0.025, right=0.975, top=0.965,),\n    )\n    grid = ImageGrid(fig, 111, nrows_ncols=(4, ncols), axes_pad=0.1)\n\n    fig2, axp = plt.subplots(\n        dpi=150,\n        figsize=(plot_size * ncols, 4),\n        subplotpars=mpl.figure.SubplotParams(bottom=0.1, left=0.025, right=0.975, top=0.925,),\n    )\n\n    plt.set_cmap(""inferno"")\n\n    for (i, p), l, col in zip(enumerate(psfs), labels, grid.axes_column):\n        p = p.PSF\n        p /= p.max()\n        col[0].imshow(p.max(1), norm=mpl.colors.PowerNorm(gam), interpolation=interpolation)\n        col[1].imshow(p.max(0), norm=mpl.colors.PowerNorm(gam), interpolation=interpolation)\n\n        col[0].set_title(l)\n\n        otf = abs(easy_fft(p))\n        otf /= otf.max()\n        otf = np.fmax(otf, vmin)\n        c = (len(otf) + 1) // 2\n\n        col[2].matshow(otf[:, c], norm=mpl.colors.LogNorm(), interpolation=interpolation)\n        col[3].matshow(otf[c], norm=mpl.colors.LogNorm(), interpolation=interpolation)\n\n        pp = p.sum((1, 2))\n        axp.plot((np.arange(len(pp)) - (len(pp) + 1) // 2) * res, pp / pp.max(), label=l)\n\n    for ax in grid:\n        ax.xaxis.set_major_locator(plt.NullLocator())\n        ax.yaxis.set_major_locator(plt.NullLocator())\n\n    ylabels = ""XZ"", ""XY""\n    ylabels += tuple(map(lambda x: r""$k_{{{}}}$"".format(x), ylabels))\n    for ax, l in zip(grid.axes_column[0], ylabels):\n        ax.set_ylabel(l)\n\n    axp.yaxis.set_major_locator(plt.NullLocator())\n    axp.set_xlabel(""Axial Position (\xc2\xb5m)"")\n    axp.set_title(""On Axis Intensity"")\n    axp.legend()\n\n    plt.show()\n'"
pyotf/otf.py,34,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# __init__.py\n""""""\nA module to simulate optical transfer functions and point spread functions\n\nIf this file is run as a script (python -m pyotf.otf) it will compare\nthe HanserPSF to the SheppardPSF in a plot.\n\nhttps://en.wikipedia.org/wiki/Optical_transfer_function\nhttps://en.wikipedia.org/wiki/Point_spread_function\n\nCopyright (c) 2020, David Hoffman\n""""""\n\nimport copy\n\nimport numpy as np\nfrom numpy.linalg import norm\nfrom numpy.fft import fftshift, fftfreq, ifftn\n\nfrom .utils import NumericProperty, easy_fft, easy_ifft, cart2pol, psqrt\nfrom .zernike import zernike, name2noll\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass BasePSF(object):\n    """"""A base class for objects that can calculate OTF\'s and PSF\'s.\n    It is not intended to be used alone\n\n    To fully describe a PSF or OTF of an objective lens, assuming no\n    abberation, we generally need a few parameters:\n    - The wavelength of operation (assume monochromatic light)\n    - the numerical aperature of the objective\n    - the index of refraction of the medium\n\n    For numerical calculations we\'ll also want to know the x/y resolution\n    and number of points. Note that it is assumed that z is the optical\n    axis of the objective lens""""""\n\n    # Define all the numeric properties of the base class\n    wl = NumericProperty(attr=""_wl"", vartype=(float, int), doc=""Wavelength of emission, in nm"")\n    na = NumericProperty(attr=""_na"", vartype=(float, int), doc=""Numerical Aperature"")\n    ni = NumericProperty(attr=""_ni"", vartype=(float, int), doc=""Refractive index"")\n    size = NumericProperty(attr=""_size"", vartype=int, doc=""x/y size"")\n    zsize = NumericProperty(attr=""_zsize"", vartype=int, doc=""z size"")\n\n    def __init__(\n        self, wl, na, ni, res, size, zres=None, zsize=None, vec_corr=""none"", condition=""sine""\n    ):\n        """"""Generate a PSF object\n\n        Parameters\n        ----------\n        wl : numeric\n            Emission wavelength of the simulation\n        na : numeric\n            Numerical aperature of the simulation\n        ni : numeric\n            index of refraction for the media\n        res : numeric\n            x/y resolution of the simulation, must have same units as wl\n        size : int\n            x/y size of the simulation\n\n        Optional Parameters\n        -------------------\n        zres : numeric\n            z resolution of simuation, must have same units a wl\n        zsize : int\n            z size of simulation\n        vec_corr : str\n            keyword to indicate whether to include vectorial effects\n                Valid options are: ""none"", ""x"", ""y"", ""z"", ""total""\n                Default is: ""none""\n        condition : str\n            keyword to indicate whether to model the sine or herschel conditions\n            **Herschel\'s Condition** invariance of axial magnification\n            **Abbe\'s Sine Condition** invariance of lateral magnification\n            conditions\n                Valid options are: ""none"", ""sine"", ""herschel""\n                Default is: ""sine""\n                Note: ""none"" is not a physical solution\n        """"""\n        self.wl = wl\n        self.na = na\n        self.ni = ni\n        self.res = res\n        self.size = size\n        # if zres is not passed, set it to res\n        if zres is None:\n            zres = res\n        self.zres = zres\n        # if zsize isn\'t passed set it to size\n        if zsize is None:\n            zsize = size\n        self.zsize = zsize\n        self.vec_corr = vec_corr\n        self.condition = condition\n\n    def _attribute_changed(self):\n        """"""Called whenever key attributes are changed\n        Sets internal state variables to None so that when the\n        user asks for them they are recalculated""""""\n        self._PSFi = None\n        self._PSFa = None\n        self._OTFi = None\n        self._OTFa = None\n\n    @property\n    def zres(self):\n        """"""z resolution (nm)""""""\n        return self._zres\n\n    @zres.setter\n    def zres(self, value):\n        # make sure z is positive\n        if not value > 0:\n            raise ValueError(""zres must be positive"")\n        self._zres = value\n        self._attribute_changed()\n\n    @property\n    def res(self):\n        """"""x/y resolution (nm)""""""\n        return self._res\n\n    @res.setter\n    def res(self, value):\n        # max_val is the nyquist limit, for an accurate simulation\n        # the pixel size must be smaller than this number\n        # thinking in terms of the convolution that is implicitly\n        # performed when generating the OTFi we also don\'t want\n        # any wrapping effects.\n        max_val = 1 / (2 * self.na / self.wl) / 2\n        if value >= max_val:\n            raise ValueError(\n                (\n                    ""{!r} is larger than the Nyquist Limit,"" "" try a number smaller than {!r}""\n                ).format(value, max_val)\n            )\n        self._res = value\n        self._attribute_changed()\n\n    @property\n    def vec_corr(self):\n        """"""Whether to apply a correction to take into account the vectorial nature of\n        light. Valid values are: ""none"", ""x"", ""y"", ""z"", ""total""\n        """"""\n        return self._vec_corr\n\n    @vec_corr.setter\n    def vec_corr(self, value):\n        valid_values = {""none"", ""x"", ""y"", ""z"", ""total""}\n        if value not in valid_values:\n            raise ValueError(""Vector correction must be one of {}"".format("", "".join(valid_values)))\n        self._vec_corr = value\n        self._attribute_changed()\n\n    @property\n    def condition(self):\n        """"""Which imaging condition to simulate?""""""\n        return self._condition\n\n    @condition.setter\n    def condition(self, value):\n        valid_values = {""none"", ""sine"", ""herschel""}\n        if value not in valid_values:\n            raise ValueError((""Condition must be one of {}"").format("", "".join(valid_values)))\n        self._condition = value\n        self._attribute_changed()\n\n    @property\n    def OTFa(self):\n        """"""Amplitude OTF (coherent transfer function), complex array""""""\n        raise NotImplementedError\n\n    @property\n    def PSFa(self):\n        """"""Amplitude PSF, complex array""""""\n        raise NotImplementedError\n\n    @property\n    def PSFi(self):\n        """"""Intensity PSF, real array""""""\n        if self._PSFi is None:\n            # the intensity PSFs are the absolute value of the coherent PSF\n            # because our imaging is _incoherent_ the result is simply the sum\n            # of the intensities for each vectorial component.\n            self._PSFi = (abs(self.PSFa) ** 2).sum(axis=0)\n        return self._PSFi\n\n    @property\n    def OTFi(self):\n        """"""Intensity OTF, complex array""""""\n        if self._OTFi is None:\n            self._OTFi = easy_fft(self.PSFi)\n        return self._OTFi\n\n\nclass HanserPSF(BasePSF):\n    """"""A class defining the pupil function and its closely related methods.\n\n    Based on the following work\n\n    [(1) Hanser, B. M.; Gustafsson, M. G. L.; Agard, D. A.; Sedat, J. W.\n    Phase-Retrieved Pupil Functions in Wide-Field Fluorescence Microscopy.\n    Journal of Microscopy 2004, 216 (1), 32\xe2\x80\x9348.](dx.doi.org/10.1111/j.0022-2720.2004.01393.x)\n    [(2) Hanser, B. M.; Gustafsson, M. G. L.; Agard, D. A.; Sedat, J. W.\n    Phase Retrieval for High-Numerical-Aperture Optical Systems.\n    Optics Letters 2003, 28 (10), 801.](dx.doi.org/10.1364/OL.28.000801)\n    """"""\n\n    def __init__(self, *args, zrange=None, **kwargs):\n        """"""zrange : array-like\n            An alternate way to specify the z range for the calculation\n            must be expressed in the same units as wavelength\n        """"""\n        super().__init__(*args, **kwargs)\n        if zrange is None:\n            self._gen_zrange()\n        else:\n            self.zrange = zrange\n\n    # include parent documentation\n    __init__.__doc__ = BasePSF.__init__.__doc__ + __init__.__doc__\n\n    def _gen_zrange(self):\n        """"""Internal utility to generate the zrange from zsize and zres""""""\n        self.zrange = (np.arange(self.zsize) - (self.zsize + 1) // 2) * self.zres\n\n    @BasePSF.zsize.setter\n    def zsize(self, value):\n        # we need override this setter so that the zrange is recalculated\n        BasePSF.zsize.fset(self, value)\n        # try and except is necessary for initialization\n        try:\n            self._gen_zrange()\n        except AttributeError:\n            pass\n\n    @BasePSF.zres.setter\n    def zres(self, value):\n        # same as for zsize\n        BasePSF.zres.fset(self, value)\n        try:\n            self._gen_zrange()\n        except AttributeError:\n            pass\n\n    @property\n    def zrange(self):\n        """"""The range overwhich to calculate the psf""""""\n        return self._zrange\n\n    @zrange.setter\n    def zrange(self, value):\n        self._zrange = np.asarray(value)\n        # check if passed value is scalar\n        if not self._zrange.shape:\n            # convert to array for later multiplications\n            self._zrange.shape = (1,)\n        self._attribute_changed()\n\n    def _gen_kr(self):\n        """"""Internal utiltiy to generate coordinate system and other internal\n        parameters""""""\n        k = fftfreq(self.size, self.res)\n        kxx, kyy = np.meshgrid(k, k)\n        self._kr, self._phi = cart2pol(kyy, kxx)\n        # kmag is the radius of the spherical shell of the OTF\n        self._kmag = self.ni / self.wl\n        # because the OTF only exists on a spherical shell we can calculate\n        # a kz value for any pair of kx and ky values\n        self._kz = psqrt(self._kmag ** 2 - self._kr ** 2)\n\n    def _gen_pupil(self):\n        """"""Generate an ideal pupil""""""\n        kr = self._kr\n        # define the diffraction limit\n        # remember we""re working with _coherent_ data _not_ intensity,\n        # so drop the factor of 2\n        diff_limit = self._na / self._wl\n        # return a circle of intensity 1 over the ideal passband of the\n        # objective make sure data is complex\n        return (kr < diff_limit).astype(complex)\n\n    def _calc_defocus(self):\n        """"""Calculate the defocus to apply to the base pupil""""""\n        kz = self._kz\n        return np.exp(2 * np.pi * 1j * kz * self.zrange[:, np.newaxis, np.newaxis])\n\n    def _gen_psf(self, pupil_base=None):\n        """"""An internal utility that generates the PSF\n        Kwargs\n        ------\n        pupil_base : ndarray\n            provided so that phase retrieval algorithms can hook into this\n            method.\n\n        NOTE: that the internal state is created with fftfreq, which creates\n        _unshifted_ frequences""""""\n        # clear internal state\n        self._attribute_changed()\n        # generate internal coordinates\n        self._gen_kr()\n        # generate the pupil\n        if pupil_base is None:\n            pupil_base = self._gen_pupil()\n        else:\n            assert pupil_base.ndim == 2, ""`pupil_base` is wrong shape""\n            # Maybe we should do ifftshift here so user doesn\'t have too\n        # pull relevant internal state variables\n        kr = self._kr\n        phi = self._phi\n        kmag = self._kmag\n        # apply the defocus to the base_pupil\n        pupil = pupil_base * self._calc_defocus()\n        # calculate theta, this is possible because we know that the\n        # OTF is only non-zero on a spherical shell\n        theta = np.arcsin((kr < kmag) * kr / kmag)\n        # The authors claim that the following code is unecessary as the\n        # sine condition is already taken into account in the definition\n        # of the pupil, but I call bullshit\n        if self.condition == ""sine"":\n            a = 1.0 / np.sqrt(np.cos(theta))\n        elif self.condition == ""herschel"":\n            a = 1.0 / np.cos(theta)\n        elif self.condition == ""none"":\n            a = 1.0\n        else:\n            raise RuntimeError(""You should never see this"")\n        pupil *= a\n        # apply the vectorial corrections, if requested\n        if self.vec_corr != ""none"":\n            plist = []\n            if self.vec_corr == ""z"" or self.vec_corr == ""total"":\n                plist.append(np.sin(theta) * np.cos(phi))  # Pzx\n                plist.append(np.sin(theta) * np.sin(phi))  # Pzy\n            if self.vec_corr == ""y"" or self.vec_corr == ""total"":\n                plist.append((np.cos(theta) - 1) * np.sin(phi) * np.cos(phi))  # Pyx\n                plist.append(np.cos(theta) * np.sin(phi) ** 2 + np.cos(phi) ** 2)  # Pyy\n            if self.vec_corr == ""x"" or self.vec_corr == ""total"":\n                plist.append(np.cos(theta) * np.cos(phi) ** 2 + np.sin(phi) ** 2)  # Pxx\n                plist.append((np.cos(theta) - 1) * np.sin(phi) * np.cos(phi))  # Pxy\n            # apply the corrections to the base pupil\n            pupils = pupil * np.array(plist)[:, np.newaxis]\n        else:\n            # if no correction we still need one more axis for the following\n            # code to work generally\n            pupils = pupil[np.newaxis]\n        # save the pupil for inspection, not necessary\n        # self._pupils = pupils\n        # because the internal state is created with fftfreq, no initial shift\n        # is necessary.\n        PSFa = fftshift(ifftn(pupils, axes=(2, 3)), axes=(2, 3))\n        # save the PSF internally\n        self._PSFa = PSFa\n\n    # Because the _attribute_changed() method sets all the internal OTFs and\n    # PSFs None we can recalculate them only when needed\n    @property\n    def OTFa(self):\n        if self._OTFa is None:\n            self._OTFa = easy_fft(self.PSFa, axes=(1, 2, 3))\n        return self._OTFa\n\n    @property\n    def PSFa(self):\n        if self._PSFa is None:\n            self._gen_psf()\n        return self._PSFa\n\n\nclass SheppardPSF(BasePSF):\n    """"""Based on the following work\n\n    [(1) Arnison, M. R.; Sheppard, C. J. R. A 3D Vectorial Optical Transfer\n    Function Suitable for Arbitrary Pupil Functions. Optics Communications\n    2002, 211 (1\xe2\x80\x936), 53\xe2\x80\x9363.](dx.doi.org/10.1016/S0030-4018(02)01857-6)\n    """"""\n\n    dual = NumericProperty(attr=""_dual"", vartype=bool, doc=""Simulate dual objectives"")\n\n    def __init__(self, *args, dual=False, **kwargs):\n        """"""dual : bool\n            Simulate dual objectives\n        """"""\n        super().__init__(*args, **kwargs)\n        self.dual = dual\n\n    # include parent documentation\n    __init__.__doc__ = BasePSF.__init__.__doc__ + __init__.__doc__\n\n    @property\n    def dual(self):\n        """"""Simulate opposing objectives?""""""\n        return self._dual\n\n    @dual.setter\n    def dual(self, value):\n        if not isinstance(value, bool):\n            raise TypeError(""`dual` must be a boolean"")\n        self._dual = value\n        self._attribute_changed()\n\n    @BasePSF.zres.setter\n    def zres(self, value):\n        # this checks the nyquist limit for z\n        # remember that because we create a spherical shell for\n        # The amplitude OTF not nyquist for the final intensity OTF ...\n        max_val = 1 / (2 * self.ni / self.wl)\n        if value >= max_val:\n            # this will cause a fftconvolution error when calculating the\n            # intensity OTF\n            raise ValueError(\n                ""{!r} is too large try a number smaller than {!r}"".format(value, max_val)\n            )\n        BasePSF.zres.fset(self, value)\n\n    def _gen_kr(self):\n        """"""Internal utility function to generate internal state""""""\n        # generate internal kspace coordinates\n        k = fftfreq(self.size, self.res)\n        kz = fftfreq(self.zsize, self.zres)\n        k_tot = np.meshgrid(kz, k, k, indexing=""ij"")\n        # calculate r\n        kr = norm(k_tot, axis=0)\n        # calculate the radius of the spherical shell in k-space\n        self.kmag = kmag = self.ni / self.wl\n        # determine k-space pixel size\n        dk, dkz = k[1] - k[0], kz[1] - kz[0]\n        # save output for user\n        self.dk, self.dkz = dk, dkz\n        # determine the min value for kz given the NA and wavelength\n        kz_min = np.sqrt(kmag ** 2 - (self.na / self.wl) ** 2)\n        # make sure we\'re not crazy\n        assert kz_min >= 0, ""Something went horribly wrong""\n        # if the user gave us different z and x/y res we need to calculate\n        # the positional ""error"" in k-space to draw the spherical shell\n        if dk != dkz:\n            with np.errstate(invalid=""ignore""):\n                dd = np.array((dkz, dk, dk)).reshape(3, 1, 1, 1)\n                dkr = norm(np.array(k_tot) * dd, axis=0) / kr\n            # we know the origin is zero so replace it\n            dkr[0, 0, 0] = 0.0\n        else:\n            dkr = dk\n        if self.dual:\n            # if we want dual objectives we need two spherical shells\n            kzz = abs(k_tot[0])\n        else:\n            kzz = k_tot[0]\n        # calculate the points on the spherical shell, save them and the\n        # corresponding kz, ky and kx coordinates\n        self.valid_points = np.logical_and(abs(kr - kmag) < dkr, kzz > kz_min + dkr)\n        self.kzz, self.kyy, self.kxx = [k[self.valid_points] for k in k_tot]\n\n    def _gen_radsym_otf(self):\n        """"""Generate a radially symmetric OTF first and then interpolate to\n        requested size""""""\n        raise NotImplementedError\n\n    def _gen_otf(self):\n        """"""Internal utility function to generate the OTFs""""""\n        # generate coordinate space\n        self._gen_kr()\n        kxx, kyy, kzz = self.kxx, self.kyy, self.kzz\n        # generate direction cosines\n        m, n, s = np.array((kxx, kyy, kzz)) / norm((kxx, kyy, kzz), axis=0)\n        # apply a given imaging condition\n        if self.condition == ""sine"":\n            a = 1.0 / np.sqrt(s)\n        elif self.condition == ""herschel"":\n            a = 1.0 / s\n        elif self.condition == ""none"":\n            a = 1.0\n        else:\n            raise RuntimeError(""You should never see this"")\n        # apply the vectorial corrections if requested\n        if self.vec_corr != ""none"":\n            plist = []\n            if self.vec_corr == ""z"" or self.vec_corr == ""total"":\n                plist.append(-m)  # Pzx\n                plist.append(-n)  # Pzy\n            if self.vec_corr == ""y"" or self.vec_corr == ""total"":\n                plist.append(-n * m / (1 + s))  # Pyx\n                plist.append(1 - n ** 2 / (1 + s))  # Pyy\n            if self.vec_corr == ""x"" or self.vec_corr == ""total"":\n                plist.append(1 - m ** 2 / (1 + s))  # Pxx\n                plist.append(-m * n / (1 + s))  # Pxy\n            # generate empty otf\n            otf = np.zeros((len(plist), self.zsize, self.size, self.size), dtype=""D"")\n            # fill in the valid poins\n            for o, p in zip(otf, plist):\n                o[self.valid_points] = p * a\n        else:\n            # TODO: we can actually do a LOT better here.\n            # if the vectorial correction is None then we can\n            # calculate a 2D (kz, kr) OTF and interpolate it out to\n            # the full 3D size.\n            # otf_sub = self._gen_radsym_otf()\n            # otf = otf_sub[np.newaxis]\n            otf_sub = np.zeros((self.zsize, self.size, self.size), dtype=""D"")\n            otf_sub[self.valid_points] = 1.0\n            otf = otf_sub[np.newaxis]\n        # we\'re already calculating the OTF, so we just need to shift it into\n        # the right place.\n        self._OTFa = fftshift(otf, axes=(1, 2, 3))\n\n    @property\n    def OTFa(self):\n        if self._OTFa is None:\n            self._gen_otf()\n        return self._OTFa\n\n    @property\n    def PSFa(self):\n        if self._PSFa is None:\n            self._PSFa = easy_ifft(self.OTFa, axes=(1, 2, 3))\n        return self._PSFa\n\n\ndef apply_aberration(model, mcoefs, pcoefs):\n    """"""Applies a set of abberations to a model PSF\n\n    Parameters\n    ----------\n    model : HanserPSF\n        The model PSF to which to apply the aberrations\n    mcoefs : ndarray (n, )\n        The magnitude coefficiencts\n    pcoefs : ndarray (n, )\n        The phase coefficients\n    \n    Note: this function assumes the mcoefs and pcoefs are Noll ordered""""""\n\n    # sanity checks\n    assert isinstance(model, HanserPSF), ""Model must be a HanserPSF""\n\n    model = copy.copy(model)\n\n    if mcoefs is None and pcoefs is None:\n        warnings.warn(""No abberation applied"")\n        return model\n\n    if mcoefs is None:\n        mcoefs = np.zeros_like(pcoefs)\n\n    if pcoefs is None:\n        pcoefs = np.zeros_like(mcoefs)\n\n    assert len(mcoefs) == len(pcoefs), ""Coefficient lengths don\'t match""\n\n    # extract kr\n    model._gen_kr()\n    kr = model._kr\n    theta = model._phi\n    # make zernikes (need to convert kr to r where r = 1 when kr is at\n    # diffraction limit)\n    r = kr * model.wl / model.na\n    zerns = zernike(r, theta, np.arange(len(mcoefs)) + 1)\n\n    pupil_phase = (zerns * pcoefs[:, None, None]).sum(0)\n    pupil_mag = (zerns * mcoefs[:, None, None]).sum(0)\n\n    # apply aberrations to unaberrated pupil (N.B. the unaberrated phase is 0)\n    pupil_mag += abs(model._gen_pupil())\n\n    # generate the PSF\n    model._gen_psf(pupil_mag * np.exp(1j * pupil_phase))\n\n    return model\n\n\ndef apply_named_aberration(model, aberration, magnitude):\n    """"""A convenience function to apply a specific named aberration to the PSF. This will only effect the phase""""""\n    # get the Noll number and build pcoefs\n    noll = name2noll[aberration]\n    pcoefs = np.zeros(noll)\n    pcoefs[-1] = magnitude\n    return apply_aberration(model, None, pcoefs)\n\n\nif __name__ == ""__main__"":\n    # import plotting\n    from matplotlib import pyplot as plt\n\n    # generate a comparison\n    kwargs = dict(\n        wl=520,\n        na=1.27,\n        ni=1.33,\n        res=90,\n        size=256,\n        zres=190,\n        zsize=128,\n        vec_corr=""none"",\n        condition=""none"",\n    )\n    psfs = HanserPSF(**kwargs), SheppardPSF(**kwargs)\n\n    with plt.style.context(""dark_background""):\n\n        fig, axs = plt.subplots(2, 2, figsize=(9, 6), gridspec_kw=dict(width_ratios=(1, 2)))\n\n        for psf, ax_sub in zip(psfs, axs):\n            # make coordinates\n            ax_yx, ax_zx = ax_sub\n            # get magnitude\n            otf = abs(psf.OTFi)\n            # normalize\n            otf /= otf.max()\n            otf /= otf.mean()\n            otf = np.log(otf + np.finfo(float).eps)\n\n            # plot\n            style = dict(vmin=-3, vmax=5, cmap=""inferno"", interpolation=""bicubic"")\n            ax_yx.matshow(otf[otf.shape[0] // 2], **style)\n            ax_yx.set_title(""{} $k_y k_x$ plane"".format(psf.__class__.__name__))\n            ax_zx.matshow(otf[..., otf.shape[1] // 2], **style)\n            ax_zx.set_title(""{} $k_z k_x$ plane"".format(psf.__class__.__name__))\n\n            for ax in ax_sub:\n                ax.xaxis.set_major_locator(plt.NullLocator())\n                ax.yaxis.set_major_locator(plt.NullLocator())\n        fig.tight_layout()\n\n    # NOTE: the results are _very_ close on a qualitative scale, but they do not match exactly\n    # as theory says they should (they\'re mathematically identical to one another)\n\n    model_kwargs = dict(\n        wl=525, na=1.27, ni=1.33, res=70, size=256, zrange=[0], vec_corr=""none"", condition=""none"",\n    )\n    model = HanserPSF(**model_kwargs)\n\n    mag = model.na / model.wl * model.res * 2 * np.pi\n\n    with plt.style.context(""dark_background""):\n        fig, axs = plt.subplots(3, 5, figsize=(12, 8))\n        # fill out plot\n        for ax, name in zip(axs.ravel(), name2noll.keys()):\n            model2 = apply_named_aberration(model, name, mag * 2)\n            ax.imshow(\n                model2.PSFi.squeeze()[104:-104, 104:-104], cmap=""inferno"", interpolation=""bicubic""\n            )\n            ax.set_xlabel(name.replace("" "", ""\\n"", 1).title())\n            ax.xaxis.set_major_locator(plt.NullLocator())\n            ax.yaxis.set_major_locator(plt.NullLocator())\n\n        # fig.tight_layout()\n    plt.show()\n'"
pyotf/phaseretrieval.py,17,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# phaseretrieval.py\n""""""\nBack focal plane (pupil) phase retrieval algorithm base on:\n[(1) Hanser, B. M.; Gustafsson, M. G. L.; Agard, D. A.; Sedat, J. W.\nPhase Retrieval for High-Numerical-Aperture Optical Systems.\nOptics Letters 2003, 28 (10), 801.](dx.doi.org/10.1364/OL.28.000801)\n\nCopyright (c) 2016, David Hoffman\n""""""\n\nimport copy\n\nimport numpy as np\nfrom numpy.linalg import lstsq\nfrom numpy.fft import fftshift, ifftshift, fftn\n\nfrom .utils import psqrt\nfrom .otf import HanserPSF\nfrom .zernike import zernike, noll2name\nfrom skimage.restoration import unwrap_phase\n\nfrom matplotlib import pyplot as plt\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\ndef retrieve_phase(data, params, max_iters=200, pupil_tol=1e-8, mse_tol=1e-8, phase_only=False):\n    """"""Retrieve the phase across the objective\'s back pupil from an\n    experimentally measured PSF.\n\n    Follows: [Hanser, B. M.; Gustafsson, M. G. L.; Agard, D. A.;\n    Sedat, J. W. Phase Retrieval for High-Numerical-Aperture Optical Systems.\n    Optics Letters 2003, 28 (10), 801.](dx.doi.org/10.1364/OL.28.000801)\n\n    Parameters\n    ----------\n    data : ndarray (3 dim)\n        The experimentally measured PSF of a subdiffractive source\n    params : dict\n        Parameters to pass to HanserPSF, size and zsize will be automatically\n        updated from data.shape\n    max_iters : int\n        The maximum number of iterations to run, default is 200\n    pupil_tol : float\n        the tolerance in percent change in change in pupil, default is 1e-8\n    mse_tol : float\n        the tolerance in percent change for the mean squared error between\n        data and simulated data, default is 1e-8\n    phase_only : bool\n        True means only the phase of the back pupil is retrieved while the\n        amplitude is not.\n\n    Returns\n    -------\n    PR_result : PhaseRetrievalResult\n        An object that contains the phase retrieval result\n    """"""\n    # make sure data is square\n    assert data.shape[1] == data.shape[2], ""Data is not square in x/y""\n    assert data.ndim == 3, ""Data doesn\'t have enough dims""\n    # make sure the user hasn\'t screwed up the params\n    params.update(\n        dict(vec_corr=""none"", condition=""none"", zsize=data.shape[0], size=data.shape[-1])\n    )\n    # assume that data prep has been handled outside function\n    # The field magnitude is the square root of the intensity\n    mag = psqrt(data)\n    # generate a model from parameters\n    model = HanserPSF(**params)\n    # generate coordinates\n    model._gen_kr()\n    # start a list for iteration\n    mse = np.zeros(max_iters)\n    mse_diff = np.zeros(max_iters)\n    pupil_diff = np.zeros(max_iters)\n    # generate a pupil to start with\n    new_pupil = model._gen_pupil()\n    # save it as a mask\n    mask = new_pupil.real\n    # iterate\n    for i in range(max_iters):\n        # generate new mse and add it to the list\n        model._gen_psf(new_pupil)\n        new_mse = _calc_mse(data, model.PSFi)\n        mse[i] = new_mse\n        if i > 0:\n            # calculate the difference in mse to test for convergence\n            mse_diff[i] = abs(old_mse - new_mse) / old_mse\n            # calculate the difference in pupil\n            pupil_diff[i] = (abs(old_pupil - new_pupil) ** 2).mean() / (abs(old_pupil) ** 2).mean()\n        else:\n            mse_diff[i] = np.nan\n            pupil_diff[i] = np.nan\n        # check tolerances, how much has the pupil changed, how much has the mse changed\n        # and what\'s the absolute mse\n        logger.info(\n            ""Iteration {}, mse_diff = {:.2g}, pupil_diff = {:.2g}"".format(\n                i, mse_diff[i], pupil_diff[i]\n            )\n        )\n        if pupil_diff[i] < pupil_tol or mse_diff[i] < mse_tol or mse[i] < mse_tol:\n            break\n        # update old_mse\n        old_mse = new_mse\n        # retrieve new pupil\n        old_pupil = new_pupil\n        # keep phase\n        phase = np.angle(model.PSFa.squeeze())\n        # replace magnitude with experimentally measured mag\n        new_psf = mag * np.exp(1j * phase)\n        # generate the new pupils\n        new_pupils = fftn(ifftshift(new_psf, axes=(1, 2)), axes=(1, 2))\n        # undo defocus and take the mean\n        new_pupils /= model._calc_defocus()\n        new_pupil = new_pupils.mean(0) * mask\n        # if phase only discard magnitude info\n        if phase_only:\n            new_pupil = np.exp(1j * np.angle(new_pupil)) * mask\n    else:\n        logger.warning(""Reach max iterations without convergence"")\n    mse = mse[: i + 1]\n    mse_diff = mse_diff[: i + 1]\n    pupil_diff = pupil_diff[: i + 1]\n    # shift mask\n    mask = fftshift(mask)\n    # shift phase then unwrap and mask\n    phase = unwrap_phase(fftshift(np.angle(new_pupil))) * mask\n    # shift magnitude\n    magnitude = fftshift(abs(new_pupil)) * mask\n    return PhaseRetrievalResult(magnitude, phase, mse, pupil_diff, mse_diff, model)\n\n\nclass PhaseRetrievalResult(object):\n    """"""An object for holding the result of phase retrieval""""""\n\n    def __init__(self, mag, phase, mse, pupil_diff, mse_diff, model):\n        """"""The results of retrieving a pupil function\'s phase and magnitude\n\n        Paramters\n        ---------\n        mag : ndarray (n, n)\n            Coefficients for the zernike decomposition of the magnitude\n        phase : ndarray (n, n)\n            Coefficients for the zernike decomposition of the phase\n        mse : ndarray (m, )\n            Mean squared error as a function of the number of iterations (m)\n            performed\n        pupil_diff : ndarray (m, )\n            The relative change in the retrieved pupil function as a function\n            of the number of iterations (m) performed\n        mse_diff : ndarray (m, )\n            The relative change in the mean squared error as a function of the\n            number of iterations (m) performed\n        model : HanserPSF object\n            the model used to retrieve the pupil function\n        """"""\n        # update internals\n        self.mag = mag\n        self.phase = phase\n        self.mse = mse\n        self.pupil_diff = pupil_diff\n        self.mse_diff = mse_diff\n        self.model = model\n        # calculate coordinate system\n        model._gen_kr()\n        r, theta = model._kr, model._phi\n        self.r, self.theta = fftshift(r), fftshift(theta)\n        # pull specific model parameters\n        self.na, self.wl = model.na, model.wl\n\n    def fit_to_zernikes(self, num_zerns):\n        """"""Fits the data to a number of zernikes""""""\n        # normalize r so that 1 = diffraction limit\n        r, theta = self.r, self.theta\n        r = r / (self.na / self.wl)\n        # generate zernikes\n        zerns = zernike(r, theta, np.arange(1, num_zerns + 1))\n        mag_coefs = _fit_to_zerns(self.mag, zerns, r)\n        phase_coefs = _fit_to_zerns(self.phase, zerns, r)\n        self.zd_result = ZernikeDecomposition(mag_coefs, phase_coefs, zerns)\n        return self.zd_result\n\n    def _generate_psf(self, complex_pupil, size=None, zsize=None, zrange=None):\n        """"""Make a perfect PSF""""""\n        # make a copy of the internal model\n        model = copy.copy(self.model)\n        # update zsize or zrange\n        if zsize is not None:\n            model.zsize = zsize\n        if zrange is not None:\n            model.zrange = zrange\n        # generate the PSF from the reconstructed phase\n        model._gen_psf(ifftshift(complex_pupil))\n        # reshpae PSF if needed in x/y dimensions\n        psf = model.PSFi\n        nz, ny, nx = psf.shape\n        assert ny == nx, ""Something is very wrong""\n        if size is not None:\n            if nx < size:\n                # if size is too small, pad it out.\n                psf = fft_pad(psf, (nz, size, size), mode=""constant"")\n            elif nx > size:\n                # if size is too big, crop it\n                lb = size // 2\n                hb = size - lb\n                myslice = slice(nx // 2 - lb, nx // 2 + hb)\n                psf = psf[:, myslice, myslice]\n        # return data\n        return psf\n\n    def generate_zd_psf(self, sphase=slice(4, None, None), size=None, zsize=None, zrange=None):\n        """"""Generate a PSF from the zernike decomposition (if available)""""""\n        return self._generate_psf(self.zd_result.complex_pupil(sphase=sphase), size, zsize, zrange)\n\n    def generate_psf(self, size=None, zsize=None, zrange=None):\n        """"""Generate a PSF from the retrieved phase""""""\n        return self._generate_psf(self.complex_pupil, size, zsize, zrange)\n\n    def plot(self, axs=None):\n        """"""Plot the retrieved results""""""\n        return _plot_complex_pupil(self.mag, self.phase, axs)\n\n    def plot_convergence(self):\n        """"""Diagnostic plots of the convergence criteria""""""\n        with np.errstate(invalid=""ignore""):\n            fig, axs = plt.subplots(3, 1, figsize=(6, 6), sharex=True)\n\n            for ax, data in zip(axs, (self.mse, self.mse_diff, self.pupil_diff)):\n                ax.semilogy(data)\n\n            for ax, t in zip(\n                axs, (""Mean Squared Error"", ""Relative Change in MSE"", ""Relative Change in Pupil"")\n            ):\n                ax.set_title(t)\n\n            fig.tight_layout()\n\n        return fig, axs\n\n    @property\n    def complex_pupil(self):\n        """"""Return the complex pupil function""""""\n        return self.mag * np.exp(1j * self.phase)\n\n\nclass ZernikeDecomposition(object):\n    """"""An object for holding the results of a zernike decomposition""""""\n\n    def __init__(self, mag_coefs, phase_coefs, zerns):\n        """"""The results of decomposing a pupil function\'s phase and magnitude\n        into zernike modes\n\n        Paramters\n        ---------\n        mag_coefs : ndarray (m, )\n            Coefficients for the zernike decomposition of the magnitude\n        phase_coefs : ndarray (m, )\n            Coefficients for the zernike decomposition of the phase\n        zerns : ndarray (m, n, n)\n            Actual zernike modes used in the decomposition\n\n        """"""\n        # verify inputs make sense\n        assert mag_coefs.size == phase_coefs.size == zerns.shape[0]\n        self.mcoefs = mag_coefs\n        self.pcoefs = phase_coefs\n        self.zerns = zerns\n\n    def plot_named_coefs(self):\n        """"""Plot the first 15 zernike mode coefficients\n\n        These coefficients correspond to the classical abberations\n        """"""\n        # set up the subplot\n        fig, ax = plt.subplots(1, 1, sharex=True, figsize=(6, 6))\n        # get the ordered names\n        ordered_names = [noll2name[i + 1] for i in range(len(noll2name))]\n        # make an x range for the bar plot\n        x = np.arange(len(ordered_names)) + 1\n        # pull the data\n        data = self.pcoefs[: len(ordered_names)]\n        # make the bar plot\n        ax.bar(x, data, align=""center"", tick_label=ordered_names)\n        # set up axes\n        ax.axis(""tight"")\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n        ax.set_ylabel(""Phase Coefficient"")\n        fig.tight_layout()\n        # return figure handles\n        return fig, ax\n\n    def plot_coefs(self):\n        """"""Same as `plot_named_coefs` but for all coefs""""""\n        fig, axs = plt.subplots(2, 1, sharex=True)\n        for ax, data in zip(axs, (self.mcoefs, self.pcoefs)):\n            ax.bar(np.arange(data.size) + 1, data)\n            ax.axis(""tight"")\n        for ax, t in zip(axs, (""Magnitude Coefficients"", ""Phase Coefficients"")):\n            ax.set_title(t)\n        ax.set_xlabel(""Noll\'s Number"")\n        fig.tight_layout()\n        return fig, axs\n\n    def _recon(self, coefs, s=Ellipsis):\n        """"""reconstruct mag or phase, base function for dispatch""""""\n        return _recon_from_zerns(coefs[s], self.zerns[s])\n\n    def phase(self, *args, **kwargs):\n        """"""Reconstruct the phase from the specified slice""""""\n        return self._recon(self.pcoefs, *args, **kwargs)\n\n    def mag(self, *args, **kwargs):\n        """"""Reconstruct the magnitude from the specified slice""""""\n        return self._recon(self.mcoefs, *args, **kwargs)\n\n    def complex_pupil(self, smag=Ellipsis, sphase=Ellipsis, *args, **kwargs):\n        """"""Reconstruct the complex pupil from the specified slice""""""\n        mag = self.mag(*args, s=smag, **kwargs)\n        phase = self.phase(*args, s=sphase, **kwargs)\n        return mag * np.exp(1j * phase)\n\n    def plot(self, smag=Ellipsis, sphase=Ellipsis, axs=None, *args, **kwargs):\n        mag = self.mag(*args, s=smag, **kwargs)\n        phase = self.phase(*args, s=sphase, **kwargs)\n        return _plot_complex_pupil(mag, phase, axs)\n\n\ndef _plot_complex_pupil(mag, phase, axs=None):\n    """"""Plot the retrieved results""""""\n    if axs is None:\n        fig, (ax_phase, ax_mag) = plt.subplots(1, 2, figsize=(12, 5))\n    else:\n        (ax_phase, ax_mag) = axs\n        fig = ax_phase.get_figure()\n\n    phase_img = ax_phase.matshow(\n        np.ma.array(phase, mask=mag == 0), cmap=""coolwarm"", vmin=-np.pi, vmax=np.pi\n    )\n    plt.colorbar(phase_img, ax=ax_phase)\n\n    mag_img = ax_mag.matshow(mag, cmap=""inferno"")\n    plt.colorbar(mag_img, ax=ax_mag)\n\n    ax_phase.set_title(""Pupil Phase"", pad=0)\n    ax_mag.set_title(""Pupil Magnitude"", pad=0)\n\n    for ax in (ax_phase, ax_mag):\n        ax.xaxis.set_major_locator(plt.NullLocator())\n        ax.yaxis.set_major_locator(plt.NullLocator())\n\n    fig.tight_layout()\n\n    return fig, (ax_phase, ax_mag)\n\n\ndef _calc_mse(data1, data2):\n    """"""utility to calculate mean square error""""""\n    return ((data1 - data2) ** 2).mean()\n\n\ndef _fit_to_zerns(data, zerns, r, **kwargs):\n    """"""sub function that does the reshaping and the least squares\n\n    Parameters\n    ----------\n    data : ndarray (n, n)\n        phase or magnitude data to fit to zernikes\n    zerns : ndarray (m, n, n)\n        precalculated zernikes\n    r : ndarray (n, n)\n        radial coordinate in terms of diffraction limit\n        where r = 1 is the diffraction limit\n\n    Returns\n    -------\n    coefs : ndarray (m, )\n        least squares coefficients of the fit of the zernikes to\n        data\n    """"""\n    # find the points to fit\n    valid_points = r <= 1\n    data2fit = data[valid_points]\n    zerns2fit = zerns[:, valid_points].T\n    # fit the points\n    coefs, _, _, _ = lstsq(zerns2fit, data2fit, rcond=None, **kwargs)\n    # return the coefficients\n    return coefs\n\n\ndef _recon_from_zerns(coefs, zerns):\n    """"""Utility to reconstruct from coefs""""""\n    return (coefs[:, np.newaxis, np.newaxis] * zerns).sum(0)\n\n\nif __name__ == ""__main__"":\n    # phase retrieve a pupil\n    from pathlib import Path\n    import time\n    import warnings\n    from skimage.external import tifffile as tif\n    from .utils import prep_data_for_PR\n\n    # read in data from fixtures\n    with warnings.catch_warnings():\n        warnings.simplefilter(""ignore"")\n        data = tif.imread(\n            str(\n                Path(__file__).parent.parent / ""fixtures/psf_wl520nm_z300nm_x130nm_na0.85_n1.0.tif""\n            )\n        )\n        # prep data\n    data_prepped = prep_data_for_PR(data, 512)\n\n    # set up model params\n    params = dict(wl=520, na=0.85, ni=1.0, res=130, zres=300)\n\n    # retrieve the phase\n    pr_start = time.time()\n    print(""Starting phase retrieval ... "", end="""", flush=True)\n    pr_result = retrieve_phase(data_prepped, params)\n    pr_time = time.time() - pr_start\n    print(f""{pr_time:.1f} seconds were required to retrieve the pupil function"")\n\n    # plot\n    pr_result.plot()\n    pr_result.plot_convergence()\n\n    # fit to zernikes\n    zd_start = time.time()\n    print(""Starting zernike decomposition ... "", end="""", flush=True)\n    pr_result.fit_to_zernikes(120)\n    zd_time = time.time() - zd_start\n    print(f""{zd_time:.1f} seconds were required to fit 120 Zernikes"")\n\n    # plot\n    pr_result.zd_result.plot_named_coefs()\n    pr_result.zd_result.plot_coefs()\n\n    # show\n    plt.show()\n'"
pyotf/utils.py,33,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# utils.py\n""""""\nUtility functions for the pyotf module\nCopyright (c) 2016, David Hoffman\n""""""\n\nimport numpy as np\nfrom numpy.fft import fftshift, ifftshift, fftn, ifftn\n\n\ndef _calc_crop(s1, s2):\n    """"""Calc the cropping from the padding""""""\n    a1 = abs(s1) if s1 < 0 else None\n    a2 = s2 if s2 < 0 else None\n    return slice(a1, a2, None)\n\n\ndef _calc_pad(oldnum, newnum):\n    """""" Calculate the proper padding for fft_pad\n\n    We have three cases:\n    old number even new number even\n    >>> _calc_pad(10, 16)\n    (3, 3)\n\n    old number odd new number even\n    >>> _calc_pad(11, 16)\n    (3, 2)\n\n    old number odd new number odd\n    >>> _calc_pad(11, 17)\n    (3, 3)\n\n    old number even new number odd\n    >>> _calc_pad(10, 17)\n    (4, 3)\n\n    same numbers\n    >>> _calc_pad(17, 17)\n    (0, 0)\n\n    from larger to smaller.\n    >>> _calc_pad(17, 10)\n    (-4, -3)\n    """"""\n    # how much do we need to add?\n    width = newnum - oldnum\n    # calculate one side, smaller\n    pad_s = width // 2\n    # calculate the other, bigger\n    pad_b = width - pad_s\n    pad1, pad2 = pad_b, pad_s\n    return pad1, pad2\n\n\ndef _padding_slices(oldshape, newshape):\n    """"""This function takes the old shape and the new shape and calculates\n    the required padding or cropping.newshape\n\n    Can be used to generate the slices needed to undo fft_pad above""""""\n    # generate pad widths from new shape\n    padding = tuple(\n        _calc_pad(o, n) if n is not None else _calc_pad(o, o) for o, n in zip(oldshape, newshape)\n    )\n    # Make a crop list, if any of the padding is negative\n    slices = tuple(_calc_crop(s1, s2) for s1, s2 in padding)\n    # leave 0 pad width where it was cropped\n    padding = [(max(s1, 0), max(s2, 0)) for s1, s2 in padding]\n    return padding, slices\n\n\ndef fft_pad(array, newshape=None, mode=""median"", **kwargs):\n    """"""Pad an array to prep it for fft""""""\n    # pull the old shape\n    oldshape = array.shape\n    if newshape is None:\n        # update each dimension to a 5-smooth hamming number\n        newshape = tuple(sig.fftpack.helper.next_fast_len(n) for n in oldshape)\n    else:\n        if isinstance(newshape, int):\n            newshape = tuple(newshape for n in oldshape)\n        else:\n            newshape = tuple(newshape)\n    # generate padding and slices\n    padding, slices = _padding_slices(oldshape, newshape)\n    return np.pad(array[slices], padding, mode=mode, **kwargs)\n\n\ndef slice_maker(xs, ws):\n    """"""\n    A utility function to generate slices for later use.\n\n    Parameters\n    ----------\n    y0 : int\n        center y position of the slice\n    x0 : int\n        center x position of the slice\n    width : int\n        Width of the slice\n\n    Returns\n    -------\n    slices : list\n        A list of slice objects, the first one is for the y dimension and\n        and the second is for the x dimension.\n\n    Notes\n    -----\n    The method will automatically coerce slices into acceptable bounds.\n\n    Examples\n    --------\n    >>> slice_maker((30,20),10)\n    [slice(25, 35, None), slice(15, 25, None)]\n    >>> slice_maker((30,20),25)\n    [slice(18, 43, None), slice(8, 33, None)]\n    """"""\n    # normalize inputs\n    xs = np.asarray(xs)\n    ws = np.asarray(_normalize_sequence(ws, len(xs)))\n    if not np.isrealobj((xs, ws)):\n        raise TypeError(""`slice_maker` only accepts real input"")\n    if np.any(ws < 0):\n        raise ValueError(""width cannot be negative, width = {}"".format(ws))\n    # ensure integers\n    xs = np.rint(xs).astype(int)\n    ws = np.rint(ws).astype(int)\n    # use _calc_pad\n    toreturn = []\n    for x, w in zip(xs, ws):\n        half2, half1 = _calc_pad(0, w)\n        xstart = x - half1\n        xend = x + half2\n        assert xstart <= xend, ""xstart > xend""\n        if xend <= 0:\n            xstart, xend = 0, 0\n        # the max calls are to make slice_maker play nice with edges.\n        toreturn.append(slice(max(0, xstart), xend))\n    # return a list of slices\n    return tuple(toreturn)\n\n\ndef easy_fft(data, axes=None):\n    """"""utility method that includes fft shifting""""""\n    return fftshift(fftn(ifftshift(data, axes=axes), axes=axes), axes=axes)\n\n\ndef easy_ifft(data, axes=None):\n    """"""utility method that includes fft shifting""""""\n    return ifftshift(ifftn(fftshift(data, axes=axes), axes=axes), axes=axes)\n\n\ndef cart2pol(y, x):\n    """"""utility function for converting from cartesian to polar""""""\n    theta = np.arctan2(y, x)\n    rho = np.hypot(y, x)\n    return rho, theta\n\n\nclass NumericProperty(property):\n    """"""Define a property that must be numeric""""""\n\n    def __init__(self, fget=None, fset=None, fdel=None, doc=None, attr=None, vartype=None):\n        """"""A property that must be numeric.\n\n        Parameters\n        ----------\n        fget : callable or None\n            Callable function to get the parameter\n            Must have signature fget(self)\n        fset : callable or None\n            Callable function to set the parameter\n            Must have signature fset(self, value)\n        fdel : callable or None\n            Callable function to delete the parameter\n            Must have signature fdel(self)\n        doc : str\n            Docstring for the parameter\n        attr : str\n            The name of the backing attribute.\n        vartype : type\n            The type to validate, defaults to `int`\n        """"""\n        if attr is not None and vartype is not None:\n            self.attr = attr\n            self.vartype = vartype\n\n            def fget(obj):\n                return getattr(obj, self.attr)\n\n            def fset(obj, value):\n                if not isinstance(value, self.vartype):\n                    raise TypeError(\n                        ""{} must be an {}, var = {!r}"".format(self.attr, self.vartype, value)\n                    )\n                if value < 0:\n                    raise ValueError(""{} must be larger than 0"".format(self.attr))\n                if getattr(obj, self.attr, None) != value:\n                    setattr(obj, self.attr, value)\n                    # call update code\n                    obj._attribute_changed()\n\n        super().__init__(fget, fset, fdel, doc)\n\n\nclass cached_property(object):\n    """""" A property that is only computed once per instance and then replaces\n    itself with an ordinary attribute. Deleting the attribute resets the\n    property.\n\n    Source: https://github.com/bottlepy/bottle/commit/fa7733e075da0d790d809aa3d2f53071897e6f76\n    \n    This will be part of the standard library starting in 3.8\n    """"""\n\n    def __init__(self, func):\n        self.__doc__ = getattr(func, ""__doc__"")\n        self.func = func\n\n    def __get__(self, obj, cls):\n        if obj is None:\n            return self\n        value = obj.__dict__[self.func.__name__] = self.func(obj)\n        return value\n\n\ndef center_data(data):\n    """"""Utility to center the data\n\n    Parameters\n    ----------\n    data : ndarray\n        Array of data points\n\n    Returns\n    -------\n    centered_data : ndarray same shape as data\n        data with max value at the central location of the array\n    """"""\n    # copy data\n    centered_data = data.copy()\n    # extract shape and max location\n    data_shape = data.shape\n    max_loc = np.unravel_index(data.argmax(), data_shape)\n    # iterate through dimensions and roll data to the right place\n    for i, (x0, nx) in enumerate(zip(max_loc, data_shape)):\n        centered_data = np.roll(centered_data, nx // 2 - x0, i)\n    return centered_data\n\n\ndef remove_bg(data, multiplier=1.5):\n    """"""Utility that measures mode of data and subtracts a multiplier of it""""""\n    # should add bit for floats, that will find the mode using the hist\n    # function bincounts with num bins specified\n    mode = np.bincount(data.ravel()).argmax()\n    return data - multiplier * mode\n\n\ndef psqrt(data):\n    """"""Take the positive square root, negative values will be set to zero.""""""\n    # make zero array\n    sdata = np.zeros_like(data, float)\n    # fill only sqrt of positive values\n    sdata[data > 0] = np.sqrt(data[data > 0])\n    return sdata\n\n\ndef prep_data_for_PR(data, xysize=None, multiplier=1.5):\n    """"""A utility to prepare data for phase retrieval\n\n    Will pad or crop to xysize and remove mode times multiplier\n\n    Parameters\n    ----------\n    data : ndarray\n        The PSF data to prepare for phase retrieval\n    xysize : int\n        Size to pad or crop `data` to along the y, x dimensions\n    multiplier : float\n        The amount to by which to multiply the mode before subtracting\n\n    Returns\n    -------\n    prepped_data : ndarray\n        The data that has been prepped for phase retrieval.\n    """"""\n    # pull shape\n    nz, ny, nx = data.shape\n    # remove background\n    data_without_bg = remove_bg(data, multiplier)\n    # figure out padding or cropping\n    if xysize is None:\n        xysize = max(ny, nx)\n    if xysize == ny == nx:\n        pad_data = data_without_bg\n    elif xysize >= max(ny, nx):\n        pad_data = fft_pad(data_without_bg, (nz, xysize, xysize), mode=""constant"")\n    else:\n        # if need to crop, crop and center and return\n        my_slice = slice_maker(((ny + 1) // 2, (nx + 1) // 2), xysize)\n        return center_data(data_without_bg)[[Ellipsis] + my_slice]\n    # return centered data\n    return center_data(pad_data)\n\n\ndef bin_ndarray(ndarray, new_shape=None, bin_size=None, operation=""sum""):\n    """"""\n    Bins an ndarray in all axes based on the target shape, by summing or\n        averaging.\n\n    Number of output dimensions must match number of input dimensions and\n        new axes must divide old ones.\n\n    Parameters\n    ----------\n    ndarray : array like object (can be dask array)\n    new_shape : iterable (optional)\n        The new size to bin the data to\n    bin_size : scalar or iterable (optional)\n        The size of the new bins\n\n    Returns\n    -------\n    binned array.\n    """"""\n    if new_shape is None:\n        # if new shape isn\'t passed then calculate it\n        if bin_size is None:\n            # if bin_size isn\'t passed then raise error\n            raise ValueError(""Either new shape or bin_size must be passed"")\n        # pull old shape\n        old_shape = np.array(ndarray.shape)\n        # calculate new shape, integer division!\n        new_shape = old_shape // bin_size\n        # calculate the crop window\n        crop = tuple(slice(None, -r) if r else slice(None) for r in old_shape % bin_size)\n        # crop the input array\n        ndarray = ndarray[crop]\n    # proceed as before\n    operation = operation.lower()\n    if operation not in {""sum"", ""mean""}:\n        raise ValueError(""Operation not supported."")\n    if ndarray.ndim != len(new_shape):\n        raise ValueError(""Shape mismatch: {} -> {}"".format(ndarray.shape, new_shape))\n\n    compression_pairs = [(d, c // d) for d, c in zip(new_shape, ndarray.shape)]\n\n    flattened = [l for p in compression_pairs for l in p]\n\n    ndarray = ndarray.reshape(flattened)\n\n    for i in range(len(new_shape)):\n        op = getattr(ndarray, operation)\n        ndarray = op(-1 * (i + 1))\n    return ndarray\n\n\ndef radial_profile(data, center=None, binsize=1.0):\n    """"""Take the radial average of a 2D data array\n\n    Adapted from http://stackoverflow.com/a/21242776/5030014\n\n    Parameters\n    ----------\n    data : ndarray (2D)\n        the 2D array for which you want to calculate the radial average\n    center : sequence\n        the center about which you want to calculate the radial average\n    binsize : sequence\n        Size of radial bins, numbers less than one have questionable utility\n\n    Returns\n    -------\n    radial_mean : ndarray\n        a 1D radial average of data\n    radial_std : ndarray\n        a 1D radial standard deviation of data\n\n    Examples\n    --------\n    >>> radial_profile(np.ones((11, 11)))\n    (array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]))\n    """"""\n    # test if the data is complex\n    if np.iscomplexobj(data):\n        # if it is complex, call this function on the real and\n        # imaginary parts and return the complex sum.\n        real_prof, real_std = radial_profile(np.real(data), center, binsize)\n        imag_prof, imag_std = radial_profile(np.imag(data), center, binsize)\n        return real_prof + imag_prof * 1j, np.sqrt(real_std ** 2 + imag_std ** 2)\n        # or do mag and phase\n        # mag_prof, mag_std = radial_profile(np.abs(data), center, binsize)\n        # phase_prof, phase_std = radial_profile(np.angle(data), center, binsize)\n        # return mag_prof * np.exp(phase_prof * 1j), mag_std * np.exp(phase_std * 1j)\n    # pull the data shape\n    idx = np.indices((data.shape))\n    if center is None:\n        # find the center\n        center = np.array(data.shape) // 2\n    else:\n        # make sure center is an array.\n        center = np.asarray(center)\n    # calculate the radius from center\n    idx2 = idx - center[(Ellipsis,) + (np.newaxis,) * (data.ndim)]\n    r = np.sqrt(np.sum([i ** 2 for i in idx2], 0))\n    # convert to int\n    r = np.round(r / binsize).astype(np.int)\n    # sum the values at equal r\n    tbin = np.bincount(r.ravel(), data.ravel())\n    # sum the squares at equal r\n    tbin2 = np.bincount(r.ravel(), (data ** 2).ravel())\n    # find how many equal r\'s there are\n    nr = np.bincount(r.ravel())\n    # calculate the radial mean\n    # NOTE: because nr could be zero (for missing bins) the results will\n    # have NaN for binsize != 1\n    radial_mean = tbin / nr\n    # calculate the radial std\n    radial_std = np.sqrt(tbin2 / nr - radial_mean ** 2)\n    # return them\n    return radial_mean, radial_std\n'"
pyotf/zernike.py,21,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# zernike.py\n""""""\nA module defining the zernike polynomials and associated functions to convert\nbetween radial and azimuthal degree pairs and Noll\'s indices.\n\nRunning this file as a script will output a graph of the first 15 zernike\npolynomials on the unit disk.\n\nhttps://en.wikipedia.org/wiki/Zernike_polynomials\nhttp://mathworld.wolfram.com/ZernikePolynomial.html\n\nCopyright (c) 2016, David Hoffman\n""""""\n\nimport numpy as np\nfrom scipy.special import eval_jacobi\nfrom .utils import cart2pol\n\n# forward mapping of Noll indices https://oeis.org/A176988\nnoll_mapping = np.array(\n    [\n        1,\n        3,\n        2,\n        5,\n        4,\n        6,\n        9,\n        7,\n        8,\n        10,\n        15,\n        13,\n        11,\n        12,\n        14,\n        21,\n        19,\n        17,\n        16,\n        18,\n        20,\n        27,\n        25,\n        23,\n        22,\n        24,\n        26,\n        28,\n        35,\n        33,\n        31,\n        29,\n        30,\n        32,\n        34,\n        36,\n        45,\n        43,\n        41,\n        39,\n        37,\n        38,\n        40,\n        42,\n        44,\n        55,\n        53,\n        51,\n        49,\n        47,\n        46,\n        48,\n        50,\n        52,\n        54,\n        65,\n        63,\n        61,\n        59,\n        57,\n        56,\n        58,\n        60,\n        62,\n        64,\n        66,\n        77,\n        75,\n        73,\n        71,\n        69,\n        67,\n        68,\n        70,\n        72,\n        74,\n        76,\n        78,\n        91,\n        89,\n        87,\n        85,\n        83,\n        81,\n        79,\n        80,\n        82,\n        84,\n        86,\n        88,\n        90,\n        105,\n        103,\n        101,\n        99,\n        97,\n        95,\n        93,\n        92,\n        94,\n        96,\n        98,\n        100,\n        102,\n        104,\n        119,\n        117,\n        115,\n        113,\n        111,\n        109,\n        107,\n        106,\n        108,\n        110,\n        112,\n        114,\n        116,\n        118,\n        120,\n    ]\n)\n\n# reverse mapping of noll indices\nnoll_inverse = noll_mapping.argsort()\n\n# classical names for the Noll indices\n# https://en.wikipedia.org/wiki/Zernike_polynomials\nnoll2name = {\n    1: ""Piston"",\n    2: ""Tip"",\n    3: ""Tilt"",\n    4: ""Defocus"",\n    5: ""Oblique astigmatism"",\n    6: ""Vertical astigmatism"",\n    7: ""Vertical coma"",\n    8: ""Horizontal coma"",\n    9: ""Vertical trefoil"",\n    10: ""Oblique trefoil"",\n    11: ""Primary spherical"",\n    12: ""Vertical secondary astigmatism"",\n    13: ""Oblique secondary astigmatism"",\n    14: ""Vertical quadrafoil"",\n    15: ""Oblique quadrafoil"",\n}\n\nname2noll = {v: k for k, v in noll2name.items()}\n\n\ndef noll2degrees(noll):\n    """"""Convert from Noll\'s indices to radial degree and azimuthal degree""""""\n    noll = np.asarray(noll)\n    if not np.issubdtype(noll.dtype, np.signedinteger):\n        raise ValueError(""input is not integer, input = {}"".format(noll))\n    if not (noll > 0).all():\n        raise ValueError(""Noll indices must be greater than 0, input = {}"".format(noll))\n    # need to subtract 1 from the Noll\'s indices because they start at 1.\n    p = noll_inverse[noll - 1]\n    n = np.ceil((-3 + np.sqrt(9 + 8 * p)) / 2)\n    m = 2 * p - n * (n + 2)\n    return n.astype(int), m.astype(int)\n\n\ndef degrees2noll(n, m):\n    """"""Convert from radial and azimuthal degrees to Noll\'s index""""""\n    n, m = np.asarray(n), np.asarray(m)\n    # check inputs\n    if not np.issubdtype(n.dtype, np.signedinteger):\n        raise ValueError(""Radial degree is not integer, input = {}"".format(n))\n    if not np.issubdtype(m.dtype, np.signedinteger):\n        raise ValueError(""Azimuthal degree is not integer, input = {}"".format(m))\n    if ((n - m) % 2).any():\n        raise ValueError(""The difference between radial and azimuthal degree isn\'t mod 2"")\n    # do the mapping\n    p = (m + n * (n + 2)) / 2\n    noll = noll_mapping[p.astype(int)]\n    return noll\n\n\ndef zernike(r, theta, *args, **kwargs):\n    """"""Calculates the Zernike polynomial on the unit disk for the requested\n    orders\n\n    Parameters\n    ----------\n    r : ndarray\n    theta : ndarray\n\n    Args\n    ----\n    Noll : numeric or numeric sequence\n        Noll\'s Indices to generate\n    (n, m) : tuple of numerics or numeric sequences\n        Radial and azimuthal degrees\n    n : see above\n    m : see above\n\n    Kwargs\n    ------\n    norm : bool (default False)\n        Do you want the output normed?\n\n    Returns\n    -------\n    zernike : ndarray\n        The zernike polynomials corresponding to Noll or (n, m) whichever are\n        provided\n\n    Example\n    -------\n    >>> x = np.linspace(-1, 1, 512)\n    >>> xx, yy = np.meshgrid(x, x)\n    >>> r, theta = cart2pol(yy, xx)\n    >>> zern = zernike(r, theta, 4)  # generates the defocus zernike polynomial\n    """"""\n    if len(args) == 1:\n        args = np.asarray(args[0])\n        if args.ndim < 2:\n            n, m = noll2degrees(args)\n        elif args.ndim == 2:\n            if args.shape[0] == 2:\n                n, m = args\n            else:\n                raise RuntimeError(""This shouldn\'t happen"")\n        else:\n            raise ValueError(""{} is the wrong shape"".format(args.shape))\n    elif len(args) == 2:\n        n, m = np.asarray(args)\n        if n.ndim > 1:\n            raise ValueError(""Radial degree has the wrong shape"")\n        if m.ndim > 1:\n            raise ValueError(""Azimuthal degree has the wrong shape"")\n        if n.shape != m.shape:\n            raise ValueError(""Radial and Azimuthal degrees have different shapes"")\n    else:\n        raise ValueError(""{} is an invalid number of arguments"".format(len(args)))\n\n    # make sure r and theta are arrays\n    r = np.asarray(r, dtype=float)\n    theta = np.asarray(theta, dtype=float)\n\n    # make sure that r is always greater than 0\n    if not (r >= 0).all():\n        raise ValueError(""r must always be greater or equal to 0"")\n    if r.ndim > 2:\n        raise ValueError(""Input rho and theta cannot have more than two dimensions"")\n\n    # make sure that n and m are iterable\n    n, m = n.ravel(), m.ravel()\n\n    # make sure that n is always greater or equal to m\n    if not (n >= abs(m)).all():\n        raise ValueError(""n must always be greater or equal to m"")\n\n    # return column of zernike polynomials\n    return np.array([_zernike(r, theta, nn, mm, **kwargs) for nn, mm in zip(n, m)]).squeeze()\n\n\ndef _radial_zernike(r, n, m):\n    """"""The radial part of the zernike polynomial\n\n    Formula from http://mathworld.wolfram.com/ZernikePolynomial.html""""""\n    rad_zern = np.zeros_like(r)\n    # zernike polynomials are only valid for r <= 1\n    valid_points = r <= 1.0\n    if m == 0 and n == 0:\n        rad_zern[valid_points] = 1\n        return rad_zern\n    rprime = r[valid_points]\n    # for the radial part m is always positive\n    m = abs(m)\n    # calculate the coefs\n    coef1 = (n + m) // 2\n    coef2 = (n - m) // 2\n    jacobi = eval_jacobi(coef2, m, 0, 1 - 2 * rprime ** 2)\n    rad_zern[valid_points] = (-1) ** coef1 * rprime ** m * jacobi\n    return rad_zern\n\n\ndef _zernike(r, theta, n, m, norm=False):\n    """"""The actual function that calculates the full zernike polynomial""""""\n    # remember if m is negative\n    mneg = m < 0\n    # going forward m is positive (Radial zernikes are only defined for\n    # positive m)\n    m = abs(m)\n    # if m and n aren\'t seperated by multiple of two then return zeros\n    if (n - m) % 2:\n        return np.zeros_like(r)\n    zern = _radial_zernike(r, n, m)\n    if mneg:\n        # odd zernike\n        zern *= np.sin(m * theta)\n    else:\n        # even zernike\n        zern *= np.cos(m * theta)\n\n    # calculate the normalization factor\n    if norm:\n        raise NotImplementedError\n    return zern\n\n\nif __name__ == ""__main__"":\n    from matplotlib import pyplot as plt\n\n    # make coordinates\n    x = np.linspace(-1, 1, 257)\n    xx, yy = np.meshgrid(x, x)  # xy indexing is default\n    r, theta = cart2pol(yy, xx)\n    # set up plot\n    fig, axs = plt.subplots(3, 5, figsize=(20, 12))\n    # fill out plot\n    for ax, (k, v) in zip(axs.ravel(), noll2name.items()):\n        zern = zernike(r, theta, k, norm=False)\n        ax.imshow(\n            np.ma.array(zern, mask=r > 1),\n            vmin=-1,\n            vmax=1,\n            cmap=""coolwarm"",\n            interpolation=""bicubic"",\n        )\n        ax.set_title(v + r"", $Z_{{{}}}^{{{}}}$"".format(*noll2degrees(k)))\n        ax.axis(""off"")\n    fig.tight_layout()\n    plt.show()\n'"
tests/integration_tests.py,13,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# integration_tests.py\n""""""\nSmall test suite\n\nCopyright David Hoffman, 2018\n""""""\n\nfrom nose.tools import *\nimport unittest\nfrom pyotf.otf import *\nfrom pyotf.phaseretrieval import *\nimport numpy as np\n\n\nclass TestHanserPhaseRetrieval(unittest.TestCase):\n    """"""Test for self consistency, generate a pupil with random zernike\n    coefficients generate a psf and phase retrieve it.""""""\n\n    def setUp(self):\n        """"""Set up the test""""""\n        # random but no\n        np.random.seed(12345)\n        # model kwargs\n        self.model_kwargs = dict(\n            wl=525,\n            na=1.27,\n            ni=1.33,\n            res=100,\n            size=128,\n            zrange=[-1000, -500, 0, 250, 1000, 3000],\n            vec_corr=""none"",\n            condition=""none"",\n        )\n        # make the model\n        model = HanserPSF(**self.model_kwargs)\n        # extract kr\n        model._gen_kr()\n        kr = model._kr\n        theta = model._phi\n        # make zernikes (need to convert kr to r where r = 1 when kr is at\n        # diffraction limit)\n        r = kr * model.wl / model.na\n        self.mask = r <= 1\n        zerns = zernike(r, theta, np.arange(5, 16))\n        # make fake phase and magnitude coefs\n        self.pcoefs = np.random.rand(zerns.shape[0])\n        self.mcoefs = np.random.rand(zerns.shape[0])\n        self.pupil_phase = (zerns * self.pcoefs[:, np.newaxis, np.newaxis]).sum(0)\n        self.pupil_mag = (zerns * self.mcoefs[:, np.newaxis, np.newaxis]).sum(0)\n        self.pupil_mag = self.pupil_mag + model._gen_pupil() * (2.0 - self.pupil_mag.min())\n        # phase only test\n        model._gen_psf(self.pupil_mag * np.exp(1j * self.pupil_phase) * model._gen_pupil())\n        self.PSFi = model.PSFi\n        # we have to converge really close for this to work.\n        self.PR_result = retrieve_phase(\n            self.PSFi, self.model_kwargs, max_iters=200, pupil_tol=0, mse_tol=0, phase_only=False\n        )\n\n    def test_mag(self):\n        """"""Make sure phase retrieval returns same magnitude""""""\n        np.testing.assert_allclose(\n            fftshift(self.pupil_mag), self.PR_result.mag, err_msg=""Mag failed""\n        )\n\n    def test_phase(self):\n        """"""Make sure phase retrieval returns same phase""""""\n        # from the unwrap_phase docs:\n        # >>> np.std(image_unwrapped - image) < 1e-6   # A constant offset is normal\n        np.testing.assert_allclose(\n            (fftshift(self.pupil_phase) - self.PR_result.phase) * self.mask,\n            0,\n            err_msg=""Phase failed"",\n        )\n\n    def test_zernike_modes(self):\n        """"""Make sure the fitted zernike modes agree""""""\n        self.PR_result.fit_to_zernikes(15)\n        np.testing.assert_allclose(\n            self.PR_result.zd_result.pcoefs[4:], self.pcoefs, err_msg=""Phase coefs failed""\n        )\n        np.testing.assert_allclose(\n            self.PR_result.zd_result.mcoefs[4:], self.mcoefs, err_msg=""Mag coefs failed""\n        )\n\n    def test_psf_mse(self):\n        """"""Does the phase retrieved PSF converge to the fake PSF""""""\n        np.testing.assert_allclose(self.PR_result.model.PSFi, self.PSFi)\n'"
tests/otf_test.py,8,"b'import numpy as np\nfrom nose.tools import *\nimport unittest\nfrom pyotf.otf import *\n\n\nclass BasePSFCase(object):\n    """"""A parent class to take care of all psf base class testing\n\n    There\'s no Test in the name because I don\'t want it picked up by testing""""""\n\n    def test_dtypes(self):\n        """"""Make sure dtypes make sense""""""\n        model = self.model\n        assert np.issubdtype(\n            model.PSFi.dtype, np.floating\n        ), ""PSFi should be a float but is a {}"".format(model.PSFi.dtype)\n        assert np.issubdtype(\n            model.PSFa.dtype, np.complexfloating\n        ), ""PSFa should be complex but is a {}"".format(model.PSFa.dtype)\n        assert np.issubdtype(\n            model.OTFi.dtype, np.complexfloating\n        ), ""OTFi should be complex but is a {}"".format(model.OTFi.dtype)\n        assert np.issubdtype(\n            model.OTFa.dtype, np.complexfloating\n        ), ""OTFa should be complex but is a {}"".format(model.OTFa.dtype)\n\n    def test_PSFi_positive(self):\n        """"""The intensity PSF should always be positive""""""\n        assert_true((self.model.PSFi >= 0).all())\n\n\nclass TestHanserPSF(unittest.TestCase, BasePSFCase):\n    """"""Test HanserPSF""""""\n\n    def setUp(self):\n        self.model = HanserPSF(525, 0.85, 1.0, 140, 64)\n\n    def test_size(self):\n        """"""Make sure when size is changed the output changes accordingly""""""\n        model = self.model\n        # make one size\n        model.size = 128\n        model.zrange = [-1000, 0, 1000]\n        assert_tuple_equal(model.PSFi.shape, (3, 128, 128))\n        # make sure changing sizes is reflected in result\n        model.size = 256\n        model.zrange = 0\n        assert_tuple_equal(model.PSFi.shape, (1, 256, 256))\n\n\nclass TestSheppardPSF(unittest.TestCase, BasePSFCase):\n    """"""Test sheppard PSF""""""\n\n    def setUp(self):\n        self.model = SheppardPSF(500, 0.85, 1.0, 140, 64)\n\n    def test_zres(self):\n        """"""Make sure zres is set properly""""""\n        self.model.zres = 100\n'"
tests/phaseretrieval_tests.py,0,"b'""""""\nPhase Retrieval tests\n""""""\n\nfrom nose.tools import *\nimport unittest\nfrom pyotf.otf import *\nfrom pyotf.utils import *\nfrom pyotf.zernike import *\nfrom pyotf.phaseretrieval import *\n'"
tests/utils_test.py,16,"b'""""""\nTests for utility functions\n""""""\n\nimport numpy as np\nfrom nose.tools import *\nfrom pyotf.utils import *\n\n\ndef test_remove_bg_unsigned():\n    """"""Make sure that remove background doesn\'t fuck up unsigned ints""""""\n    test_data = np.array((1, 2, 3, 3, 3, 4, 5), dtype=np.uint16)\n    assert_true(np.allclose(remove_bg(test_data, 1.0), test_data - 3.0))\n\n\ndef test_center_data():\n    """"""Make sure center data works as advertised""""""\n    ndims = np.random.randint(2, 3)\n    shape = np.random.randint(1, 512, ndims)\n    print(np.prod(shape))\n    data = np.zeros(shape)\n    random_index = tuple((np.random.randint(i),) for i in shape)\n    data[random_index] = 1\n    data_centered = center_data(data)\n    assert_true(np.fft.ifftshift(data_centered)[((0,),) * ndims])\n\n\ndef test_psqrt():\n    """"""test psqrt""""""\n    data = np.random.randint(-1000, 1000, size=20)\n    ps_data = psqrt(data)\n    less_than_zero = data < 0\n    assert_true((ps_data[less_than_zero] == 0).all())\n    more_than_zero = np.logical_not(less_than_zero)\n    print(ps_data[more_than_zero])\n    print(np.sqrt(data[more_than_zero]))\n    assert_true(np.allclose(ps_data[more_than_zero], np.sqrt(data[more_than_zero])))\n\n\ndef test_cart2pol():\n    """"""Make sure cart2pol is good""""""\n    z = np.random.randn(10) + np.random.randn(10) * 1j\n    theta = np.angle(z)\n    r = abs(z)\n    test_r, test_theta = cart2pol(z.imag, z.real)\n    assert_true(np.allclose(test_theta, theta), ""theta failed"")\n    assert_true(np.allclose(test_r, r), ""r failed"")\n'"
tests/zernike_tests.py,19,"b'# zernike_tests.py\n""""""\nTest suite for zernike.py\n""""""\n\nfrom nose.tools import *\nfrom pyotf.zernike import *\nimport numpy as np\n\n\ndef test_degrees_input():\n    """"""Make sure an error is returned if n and m aren\'t seperated by two""""""\n    assert_raises(ValueError, degrees2noll, 1, 2)\n\n\ndef test_noll_input():\n    """"""Make sure an error is raised if noll isn\'t a positive integer""""""\n    assert_raises(ValueError, noll2degrees, 0)\n    assert_raises(ValueError, noll2degrees, -1)\n\n\ndef test_integer_input():\n    """"""make sure degrees2noll and noll2degrees only accept integer inputs""""""\n    assert_raises(ValueError, noll2degrees, 2.5)\n    assert_raises(ValueError, noll2degrees, 1.0)\n    assert_raises(ValueError, degrees2noll, 1.0, 3.0)\n    assert_raises(ValueError, degrees2noll, 1.5, 3.5)\n\n\ndef test_indices():\n    """"""Make sure that noll2degrees and degrees2noll are opposites of each\n    other""""""\n    test_noll = np.random.randint(1, 36, 10)\n    test_n, test_m = noll2degrees(test_noll)\n    test_noll2 = degrees2noll(test_n, test_m)\n    assert_true((test_noll == test_noll2).all(), ""{} != {}"".format(test_noll, test_noll2))\n\n\ndef test_n_lt_m():\n    """"""n must always be greater than or equal to m""""""\n    assert_raises(ValueError, zernike, 0.5, 0.0, 4, 5)\n\n\ndef test_forward_mapping():\n    """"""Make sure that the mapping from degrees to Noll\'s indices is correct""""""\n    # from https://en.wikipedia.org/wiki/Zernike_polynomials\n    degrees = np.array(\n        ((0, 0), (1, 1), (1, -1), (2, 0), (2, -2), (2, 2), (3, -1), (3, 1), (3, -3), (3, 3))\n    )\n    j = np.array((1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n    n, m = degrees.T\n    assert_true((degrees2noll(n, m) == j).all())\n\n\ndef test_reverse_mapping():\n    """"""Make sure that the mapping from Noll\'s indices to degrees is correct""""""\n    # from https://en.wikipedia.org/wiki/Zernike_polynomials\n    degrees = np.array(\n        ((0, 0), (1, 1), (1, -1), (2, 0), (2, -2), (2, 2), (3, -1), (3, 1), (3, -3), (3, 3))\n    )\n    j = np.array((1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n    n, m = degrees.T\n    n_test, m_test = noll2degrees(j)\n    assert_true((m_test == m).all(), ""{} != {}"".format(m_test, m))\n    assert_true((n_test == n).all(), ""{} != {}"".format(n_test, n))\n\n\ndef test_r_theta_dims():\n    """"""Make sure that a ValueError is raised if the dims are greater than 2""""""\n    r = np.ones((3, 3, 3))\n    assert_raises(ValueError, zernike, r, r, 10)\n\n\ndef test_zernike_return_shape():\n    """"""Make sure that the return shape matches input shape""""""\n    x = np.linspace(-1, 1, 512)\n    xx, yy = np.meshgrid(x, x)\n    r, theta = cart2pol(yy, xx)\n    zern = zernike(r, theta, 10)\n    assert_equal(zern.shape, r.shape)\n\n\ndef test_zernike_errors():\n    """"""Make sure zernike doesn\'t accept bad input.""""""\n    noll = np.ones((2, 2, 2))\n    # check noll dims\n    assert_raises(ValueError, zernike, 0, 0, noll)\n    # check that n and m must have dimension of 1\n    assert_raises(ValueError, zernike, 0, 0, noll, noll)\n    # check that r can\'t be negative\n    assert_raises(ValueError, zernike, -1, 0, 0, 1)\n    # check that r only has 2 dims\n    assert_raises(ValueError, zernike, np.ones((10, 10, 2)), 0, 0, 1)\n\n\ndef test_zernike_zero():\n    """"""Make sure same result is obtained for integer and float""""""\n    n, m = choose_random_nm()\n    r = 0.5\n    theta = np.random.rand() * 2 * np.pi - np.pi\n    assert_true(\n        np.isfinite(zernike(r, theta, n, m)).all(),\n        ""r, theta, n, m = {}, {}, {}, {}"".format(r, theta, n, m),\n    )\n\n\ndef test_zernike_edges():\n    """"""Make sure same result is obtained at 0 and 0.0 and 1 and 1.0""""""\n    n, m = choose_random_nm()\n    theta = np.random.rand() * 2 * np.pi - np.pi\n    assert_equal(\n        zernike(1.0, theta, n, m),\n        zernike(1, theta, n, m),\n        ""theta, n, m = {}, {}, {}"".format(theta, n, m),\n    )\n    assert_equal(\n        zernike(0.0, theta, n, m),\n        zernike(0, theta, n, m),\n        ""theta, n, m = {}, {}, {}"".format(theta, n, m),\n    )\n\n\ndef test_odd_nm():\n    """"""Make sure that n and m seperated by odd numbers gives zeros""""""\n    n, m = choose_random_nm(True)\n    theta = np.random.rand(100) * 2 * np.pi - np.pi\n    # we\'ll check outside the normal range too, when r\n    r = np.random.rand(100) * 2\n    assert_true(\n        (zernike(r, theta, n, m) == 0).all(), ""theta, n, m = {}, {}, {}"".format(theta, n, m)\n    )\n\n\ndef choose_random_nm(odd=False):\n    """"""Small utility function to choose random n and m, optional argument specifies\n    whether n and m are seperated by a factor of 2 or not""""""\n    m = np.nan\n    n = np.nan\n    # make sure m and n are seperated by a factor of 2 otherwise\n    # we\'ll get all zeros\n    while (m - n + odd) % 2:\n        # choose random positive n\n        n = np.random.randint(100)\n        if n:\n            # if n is greater than zero choose random m such that\n            # n >= m\n            m = np.random.randint(-n, n + 1)\n        else:\n            m = 0\n    assert n >= abs(m), ""Somethings very wrong {} not >= {}"".format(n, m)\n    assert not (m - n + odd) % 2, ""m = {}, n = {}"".format(m, n)\n    return n, m\n'"
notebooks/Microscope Imaging Models/easy_plot.py,2,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# easy_plot.py\n""""""\nAn easy plotting function\n\nCopyright (c) 2020, David Hoffman\n""""""\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\n\nimport numpy as np\n\nfrom pyotf.utils import easy_fft, easy_ifft\nfrom dphutils import bin_ndarray\n\n# plot function \xf0\x9f\x98\xac\ndef easy_plot(\n    psfs, labels, oversample_factor=1, res=1, gam=0.3, vmin=1e-3, interpolation=""bicubic""\n):\n    ncols = len(psfs)\n\n    assert ncols == len(labels), ""Lengths mismatched""\n    assert ncols < 10\n\n    plot_size = 2.0\n\n    fig = plt.figure(None, (plot_size * ncols, plot_size * 4), dpi=150)\n    grid = ImageGrid(fig, 111, nrows_ncols=(4, ncols), axes_pad=0.1)\n\n    fig2, axp = plt.subplots(dpi=150, figsize=(plot_size * ncols, 4))\n\n    for (i, p), l, col in zip(enumerate(psfs), labels, grid.axes_column):\n        p = bin_ndarray(p, bin_size=oversample_factor)\n        p /= p.max()\n        col[0].imshow(p.max(1), norm=mpl.colors.PowerNorm(gam), interpolation=interpolation)\n        col[1].imshow(p.max(0), norm=mpl.colors.PowerNorm(gam), interpolation=interpolation)\n\n        col[0].set_title(l)\n\n        otf = abs(easy_fft(p))\n        otf /= otf.max()\n        otf = np.fmax(otf, vmin)\n        c = (len(otf) + 1) // 2\n\n        col[2].matshow(otf[:, c], norm=mpl.colors.LogNorm(), interpolation=interpolation)\n        col[3].matshow(otf[c], norm=mpl.colors.LogNorm(), interpolation=interpolation)\n\n        pp = p[:, c, c]\n        axp.plot((np.arange(len(pp)) - (len(pp) + 1) // 2) * res, pp / pp.max(), label=l)\n\n    for ax in grid:\n        ax.xaxis.set_major_locator(plt.NullLocator())\n        ax.yaxis.set_major_locator(plt.NullLocator())\n\n    ylabels = ""XZ"", ""XY""\n    ylabels += tuple(map(lambda x: r""$k_{{{}}}$"".format(x), ylabels))\n    for ax, l in zip(grid.axes_column[0], ylabels):\n        ax.set_ylabel(l)\n\n    axp.yaxis.set_major_locator(plt.NullLocator())\n    axp.set_xlabel(""Axial Position (\xc2\xb5m)"")\n    axp.set_title(""On Axis Intensity"")\n    axp.legend()\n'"
