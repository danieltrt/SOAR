file_path,api_count,code
405.py,0,"b""import pandas\nfrom matplotlib import pyplot\n\nurls = {\n 2015: 'http://dot.ca.gov/trafficops/census/docs/2015aadt.xls',\n 2014: 'http://dot.ca.gov/trafficops/census/docs/2014aadt.xlsx',\n 2013: 'http://dot.ca.gov/trafficops/census/docs/2013aadt.xlsx',\n 2012: 'http://dot.ca.gov/trafficops/census/docs/2012aadt.xlsx',\n 2011: 'http://dot.ca.gov/trafficops/census/docs/2011aadt.xlsx'\n}\n\ncolumns = ['District', 'Route Number', 'x1', 'County', 'Milepost Prefix', 'Milepost', 'x2', 'Description', 'Back Peak Hour', 'Back Peak Month', 'Back AADT', 'Ahead Peak Hour', 'Ahead Peak Month', 'Ahead AADT']\n\naadt = pandas.DataFrame(columns = columns)\nfor year, url in urls.items():\n  df = pandas.read_excel(url)\n  df.columns = columns\n  df['Year'] = year\n  aadt = pandas.merge(aadt, df, how='outer')\n\naadt = aadt.convert_objects(convert_numeric=True) # some numbers (e.g. 405) are sometimes strings and sometimes numbers\n\nmileposts = {\n  30.856: 'Santa Monica Bl.',\n  31.542: 'Wilshire',\n  32.502: 'Waterford/Montana',\n  32.996: 'Sunset',\n  33.290: 'Moraga',\n  34.764: 'Getty Center Dr.',\n  37.026: 'Mulholland',\n  39.432: '101',\n  40.285: 'Burbank Bl.'\n}\n\naadt['Milepost'] = aadt['Milepost'].round(3) # some floats get imported funky\nplotdf = aadt[(aadt['Route Number'] == 405) & (aadt['Milepost'].isin(mileposts.keys()))][['Milepost', 'Year', 'Back AADT']] # select column to plot\nplotdf.sort_values(['Milepost', 'Year'], inplace=True)\nplotdf.set_index('Year', inplace=True)\nfig = pyplot.figure()\nfor i, milepost in enumerate(mileposts.keys()):\n  ax = fig.add_subplot(3,3,1 + i)\n  plotdf[plotdf['Milepost'] == milepost].plot.bar(ax=ax, legend=False)\n  # ax.set_ylim(plotdf[plotdf['Milepost'] == milepost]['Back AADT'].min() - 2000, plotdf[plotdf['Milepost'] == milepost]['Back AADT'].max() + 2000) # uncomment to distort axis and zoom in\n  ax.set_title(mileposts[milepost])\n  ax.set_ylabel('Vehicles/Hour')\n\npyplot.style.use('ggplot')\npyplot.tight_layout()\nfig.savefig('back_aadt_zoom.png') # filename\npyplot.show()\n"""
TransportationModel.py,0,"b'import pandas\nimport geopandas\nimport requests\n\ndef TripGeneration():\n  originDestinationFile = \'/Users/david/Desktop/maps/ca_od_2013/ca_od_main_JT0.csv\'\n  originDestination = pandas.DataFrame.from_csv(originDestinationFile)\n  originDestination.reset_index(inplace=True)\n  return originDestination\n\ndef TripDistribution(originDestination):\n  censusTractsFile = \'/Users/david/Desktop/maps/tl_2010_06_tract10/tl_2010_06_tract10.shp\'\n  censusTracts = geopandas.GeoDataFrame.from_file(censusTractsFile)\n  censusTracts.set_index(\'GEOID10\', inplace=True)\n  # censusTracts2GEOMETRY = lambda x: censusTracts.loc[\'0\' + str(x)[:10]][\'geometry\']\n  censusTracts2LatLonH = lambda x: [float(censusTracts.loc[\'0\' + str(x)[:10]][\'INTPTLAT10\']), float(censusTracts.loc[\'0\' + str(x)[:10]][\'INTPTLON10\'])]\n  censusTracts2LatLonW = lambda x: [float(censusTracts.loc[str(x)[:11]][\'INTPTLAT10\']), float(censusTracts.loc[str(x)[:11]][\'INTPTLON10\'])]\n  originDestination[\'hLatLon\'] = originDestination[\'h_geocode\'].apply(censusTracts2LatLonH)\n  originDestination[\'wLatLon\'] = originDestination[\'w_geocode\'].apply(censusTracts2LatLonW)\n  return originDestination\n\ndef modeChooser(row):\n  alpha = 900\n  beta = 600\n  gamma = 900\n  if (row[\'foot\'][\'routes\'][0][\'duration\'] - gamma < row[\'bicycle\'][\'routes\'][0][\'duration\']) & (row[\'foot\'][\'routes\'][0][\'duration\'] - alpha < row[\'car\'][\'routes\'][0][\'duration\']):\n    return \'foot\'\n  elif row[\'bicycle\'][\'routes\'][0][\'duration\'] - beta < row[\'car\'][\'routes\'][0][\'duration\']:\n    return \'bicycle\'\n  else: \n    return \'car\'\n\ndef ModeChoice(originDestination):\n  foot = lambda x: requests.get(\'http://localhost:5000/route/v1/foot/\' + str(x[\'hLatLon\'][1]) + \',\' + str(x[\'hLatLon\'][0]) + \';\' + str(x[\'wLatLon\'][1]) + \',\' + str(x[\'wLatLon\'][0])).json()\n  bicycle = lambda x: requests.get(\'http://localhost:5001/route/v1/bike/\' + str(x[\'hLatLon\'][1]) + \',\' + str(x[\'hLatLon\'][0]) + \';\' + str(x[\'wLatLon\'][1]) + \',\' + str(x[\'wLatLon\'][0])).json()\n  car = lambda x: requests.get(\'http://localhost:5002/route/v1/car/\' + str(x[\'hLatLon\'][1]) + \',\' + str(x[\'hLatLon\'][0]) + \';\' + str(x[\'wLatLon\'][1]) + \',\' + str(x[\'wLatLon\'][0])).json()\n  originDestination[\'foot\'] = originDestination.apply(foot, axis=1)\n  originDestination[\'bicycle\'] = originDestination.apply(bicycle, axis=1)\n  originDestination[\'car\'] = originDestination.apply(car, axis=1)\n  originDestination[\'mode\'] = originDestination.apply(modeChooser, axis=1)\n  return originDestination\n\ndef RouteAssignment(originDestination):\n  travelTime = lambda x: x[x[\'mode\']][\'routes\'][0][\'duration\']\n  originDestination[\'travelTime\'] = originDestination.apply(travelTime, axis=1)\n  return originDestination\n\nif __name__ == ""__main__"":\n  originDestination = TripGeneration()\n  originDestination = TripDistribution(originDestination)\n  originDestination = ModeChoice(originDestination)\n  originDestination = RouteAssignment(originDestination)\n  # sum up the number of people on each route\n  modeShare = originDestination[\'mode\'].value_counts()\n  averageTravelTime = originDestination[\'travelTime\'].sum() / len(a) \n'"
ad.py,0,"b'import ldap\nimport pandas\nimport datetime.datetime\nfrom ldap_paged_search import LdapPagedSearch\n\nhost = \'ldaps://example.com:636\'\nusername = \'domain\\\\username\'\npassword = \'password\'\n\nbaseDN = \'DC=example,DC=com\'\nfilter = ""(&(objectCategory=computer))""\n#attributes = [\'dn\']\nattributes = [\'*\']\n\n#ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_NEVER)\nl = LdapPagedSearch(host, username, password, maxPages=0, pageSize=1000)\nresults = l.search(baseDN, filter, attributes = attributes)\n\ncomputers = []\nfor computer in results:\n    dn = computer[0]\n    try:operatingSystem = computer[1][\'operatingSystem\'][0]\n    except: operatingSystem = False\n    try: operatingSystemServicePack = computer[1][\'operatingSystemServicePack\'][0]\n    except: operatingSystemServicePack = False\n    hostname = computer[1][\'cn\'][0]\n    try: fqdn = computer[1][\'dNSHostName\'][0]\n    except: fqdn = False\n    whenCreated = computer[1][\'whenCreated\'][0]\n    try: lastLogonTimestamp = datetime.datetime.utcfromtimestamp((int(computer[1][\'lastLogonTimestamp\'][0]) - 116444736000000000) / 10000000)\n    except: lastLogonTimestamp = False\n    try: description = computer[1][\'description\'][0]\n    except: description = False\n    GUID = computer[1][\'objectGUID\'][0]\n    computers.append((dn,hostname,fqdn,operatingSystem,operatingSystemServicePack,whenCreated,lastLogonTimestamp,description,GUID))\n\ncomp = pandas.DataFrame(computers)\ncomp.columns = [\'dn\',\'hostname\',\'fqdn\',\'operatingSystem\',\'operatingSystemServicePack\',\'whenCreated\',\'lastLogonTimestamp\',\'description\',\'GUID\']\nwindows = comp[comp[\'operatingSystem\'] != ""Mac OS X""]\n'"
basemap.py,2,"b'from mpl_toolkits.basemap import Basemap\nfrom matplotlib import pyplot\nimport numpy as np\n\'\'\'\nimport CoreLocation\n\nmanager = CoreLocation.CLLocationManager.alloc().init()\nmanager.delegate()\nmanager.startUpdatingLocation()\nloc = manager.location()\nif loc is None:\n lat, lon = 0,0\nelse:\n coord = loc.coordinate()\n lat, lon = coord.latitude, coord.longitude\n\'\'\'\n#def convert_to_decimal(degrees, arcminutes, arcseconds):\n# return float(degrees + arcminutes/60. + arcseconds/3600.)\nlat, lon = 0,0\n\nm = Basemap(projection=\'merc\',llcrnrlat=-80,urcrnrlat=80,llcrnrlon=-180,urcrnrlon=180,lat_ts=20,resolution=\'c\')\n\nm.drawcoastlines()\nm.drawstates()\nm.bluemarble()\nm.drawparallels(np.arange(-90.,91.,30.))\nm.drawmeridians(np.arange(-180.,181.,60.))\nm.drawmapboundary(fill_color=\'aqua\')\nm.scatter(lon,lat,100,latlon=1,color=\'m\',marker=\'.\')\n\npyplot.title(""Mercator Projection | Latitude: "" + str(lat) + "", Longitude: "" + str(lon))\npyplot.show()\n'"
county-map.py,0,"b'import overpy\nimport numpy\nfrom matplotlib import pyplot\nfrom shapely.geometry import LineString\nfrom shapely.geometry import shape\nfrom shapely.geometry import box\nfrom shapely.ops import polygonize_full\nfrom descartes import PolygonPatch\nfrom fiona import collection\n\nfeatures = collection(""~/Desktop/gshhg-shp-2/GSHHS_shp/f/GSHHS_f_L1.shp"")\nnorthAmericaPolygon = shape(features[3][\'geometry\'])\n\nlaBox = box(-119.3500,33.5277,-117.1115,34.9895)\ncoastBox = northAmericaPolygon.intersection(laBox)\n\napi = overpy.Overpass()\n\ncountyRelation = api.query(""rel(396479);(._;>;);out;"")\nprimaryHighwaysInBoundingBox = api.query(""way(33.5277,-119.3500,34.9895,-117.1115)[highway=primary];(._;>);out;"")\nsecondaryHighwaysInBoundingBox = api.query(""way(33.5277,-119.3500,34.9895,-117.1115)[highway=secondary];(._;>);out;"")\n\ncountyWaysAboveIslands = []\nfor way in countyRelation.ways:\n above = 1\n for node in way.nodes:\n  if (node.lat < 33.6078):\n   above = 0\n if (above):\n  countyWaysAboveIslands.append(way)\n\ncountyLineStrings = []\nfor way in countyWaysAboveIslands:\n lineString = []\n for node in way.nodes:\n  lineString.append((node.lon,node.lat))\n countyLineStrings.append(LineString(lineString))\n\npolygons, dangles, cuts, invalids = polygonize_full(countyLineStrings)\ncountyPolygonWithOcean = polygons.geoms[0]\n\ncountyPolygon = countyPolygonWithOcean.intersection(coastBox)\n\nhighwayLineStrings = []\nfor way in primaryHighwaysInBoundingBox.ways:\n line = []\n for node in way.nodes:\n  line.append((node.lon,node.lat))\n wayLineString = LineString(line)\n if countyPolygon.contains(wayLineString): highwayLineStrings.append(wayLineString)\n\nfor way in secondaryHighwaysInBoundingBox.ways:\n line = []\n for node in way.nodes:\n  line.append((node.lon,node.lat))\n wayLineString = LineString(line)\n if countyPolygon.contains(wayLineString): highwayLineStrings.append(wayLineString)\n\nfig = pyplot.figure(figsize=(100,100))\nax = fig.add_subplot(111)\n\nfor line in highwayLineStrings:\n x, y = line.xy\n ax.plot(x, y, color=\'#FFFFFF\', linewidth=1, zorder=2)\n\npatch = PolygonPatch(countyPolygon, fc=\'#FFD700\', ec=\'#FFD700\', alpha=0.5, zorder=1)\nax.add_patch(patch)\n\nfig.savefig(\'test2.png\')\n\n## Pickle\n>>> import pickle\n>>> output = open(\'data.pkl\', \'wb\')\n>>> pickle.dump(countyPolygon,output)\n>>> pickle.dump(highwayLineStrings,output)\n>>> output.close()\n\n## debug info\nfrom geopandas import GeoSeries\nfrom geopandas import GeoDataFrame\n\nfrom shapely.geometry import MultiLineString\ntest = MultiLineString(highwayLineStrings)\npatch = PolygonPatch(countyPolygon, fc=\'#6699cc\', ec=\'#6699cc\', alpha=0.5, zorder=2)\nax.add_patch(patch)\n\nobject.__dict__\nresult.nodes\nresult.nodes.[0].lat\nresult.ways\nresult.ways[0]._node_ids[0]\nresult.relations\nresult.relations[0].members[0].ref\n\nlen(streetsInBB.ways[0].get_nodes(resolve_missing=True))\n\nfor way in aboveIslands:\n gray = gray + .025\n for node in way.nodes:\n  ax.scatter(node.lon, node.lat, color=[gray,gray,gray], s=100, zorder=1)\n\nstates = [shapely.geometry.shape(f[\'geometry\']) for f in features]\n\nhttp://overpass.osm.rambler.ru/cgi/interpreter?data=%5Bout:json%5D;relation(396479);out;\n'"
csv 2 json or html table.py,0,"b'import pandas\nimport json\n\ndf = pandas.read_csv(\'file.csv\', encoding=\'utf-8-sig\')\n\nheaders = map(lambda x: x, df.columns)\noutput = {\'headers\': headers}\noutput.update({\'values\': json.loads(df.to_json(orient=\'records\'))})\n#print json.dumps(output)\n\n\n### /\\ JSON\n### \\/ HTML\n\nprint ""<html>""\nprint """"""\n<style>\ntable, th , td  {\n  border: 1px solid grey;\n  border-collapse: collapse;\n  padding: 5px;\n}\ntable tr:nth-child(odd) {\n  background-color: #f1f1f1;\n}\ntable tr:nth-child(even) {\n  background-color: #ffffff;\n}\n</style>\n""""""\n\nprint ""<table><tr>""\nfor x in headers:\n  print ""<td>"", x, ""</td>""\n \nprint ""</tr>""\nfor (i, x) in df.iterrows():\n  print ""<tr>""\n  for y in x:\n    print ""<td>"", y, ""</td>""\n  print ""</tr>""\n\nprint ""</table></html>""\n'"
datasetRegressionExample.py,0,"b""import pandas\nfrom scipy import optimize\nfrom sklearn import linear_model\nfrom math import exp, factorial\nfrom scipy.stats import poisson\nfrom statsmodels.api import OLS\nfrom statsmodels.discrete.discrete_model import Poisson\nfrom statsmodels.tools import add_constant\n\ndf = pandas.read_csv('datasetRegressionExample.csv')\nexposure = lambda row: row['AADT'] * row['L'] * 365\ncrash_rate = lambda row: row['Crashes'] / row['exposure'] * 1000000\n\nyes_shoulder = df[df['shoulder'] == 1].sum()\nno_shoulder = df[df['shoulder'] == 0].sum()\n\n# df = df.append(yes_shoulder, ignore_index=True)\n# df = df.append(no_shoulder, ignore_index=True)\n\ndf['exposure'] = df.apply(exposure, axis=1)\ndf['crash_rate'] = df.apply(crash_rate, axis=1)\n\ndef estimator(x, row_in='Crashes'):\n  estimated = lambda row: x[0] + x[1] * row['AADT'] + x[2] * row['L']\n  df['estimated'] = df.apply(estimated, axis=1)\n  difference = lambda row: row[row_in] - row['estimated']\n  df['difference'] = df.apply(difference, axis=1)\n  square = lambda row: row**2\n  sum_of_squares = df['difference'].apply(square).sum()\n  return(sum_of_squares)\n\nx0 = [-20, .0008, 1.1]\nestimator(x0)\noptimize.minimize(estimator, x0, method='nelder-mead', options={'xtol': 1e-8, 'disp': True})\n\nclf = linear_model.LinearRegression()\nx = df[['AADT', 'L']].as_matrix()\ny = df['Crashes']\nclf.fit(x, y)\nclf.coef_\nclf.intercept_\n\nmodel = OLS(y, add_constant(x))\nmodel_fit = model.fit()\nmodel_fit.summary()\n\ndef estimator(x, row_in='Crashes'):\n  estimated = lambda row: exp(x[0] + x[1] * row['AADT'] + x[2] * row['L'])\n  df['estimated'] = df.apply(estimated, axis=1)\n  #probability = lambda row: (row['estimated']**row[row_in] * exp(-row['estimated'])) / factorial(row[row_in])\n  probability = lambda row: poisson.pmf(row[row_in], row['estimated'])\n  df['probability'] = df.apply(probability, axis=1)\n  product = df['probability'].product()\n  return(-product)\n\nx0 = [1.6, .0000026, .032]\nestimator(x0)\noptimize.minimize(estimator, x0, method='nelder-mead', options={'xtol': 1e-8, 'disp': True})\n\nmodel = Poisson(y.as_matrix().transpose(), add_constant(x))\nmodel_fit = model.fit(start_params=x0)\nmodel_fit.summary()\n"""
delicious.py,0,"b'import re\n\ntitle_pattern = \'"" PRIVATE=""0"" TAGS="""">(.*)</A>\'\nlink_pattern = \'<DT><A HREF=""(.*)"" ADD_DATE.*\'\ndate_pattern = \'ADD_DATE=""(.*)"" PRIVATE=""0""\'\n\nwith open(\'Desktop/delicious.html\') as f:\n  for line in f:\n    title = re.search(title_pattern,line).group(1)\n    link = re.search(link_pattern,line).group(1)\n    date = re.search(date_pattern,line).group(1)\n    print(""\\"""" + title + ""\\"",\\"""" + link + ""\\"","" + date)\n'"
finance.py,4,"b'import math\nimport csv\nimport numpy\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport scipy.fftpack\nimport datetime\nimport pandas as pd\nfrom scipy import stats\n\ndef pv(fv,r):\n return (fv/(1+r))\n\ndef pvSum(c,r): //PV as a sum of cash flows c w/ interest rate \n sum=0\n for n, ck in enumerate(c):\n  print n,ck,r\n  sum += ck/((1+r)**n)\n return sum\n\ndef annuity(A,r): //Annuity pays A forever\n return A/r\n\ndef annuityEnd(A,r,n)Annuity that pays A until n\n return annuity(A,r)*(1-1/((1+r)**n)) \n\ndef discountRate(spotRate,t): \n return 1/((1+spotRate)**t)\n\ndef fv(pv,r):\n return (pv*(1+r))\n\ndef p(f,i,n):\n return (f/((1+i)**n))\nvec_p = np.vectorize(p)\nptest = np.array([np.array([ 1.,  2.,  3.]), np.array(10), np.array(10)])\nvec_p(ptest[0],ptest[1],ptest[2])\n\ndef f(p,i,n):\n return (p*((1+i)**n))\n\ndef fe(p,i,n):\n return (p*(math.e**(i*n)))\n\ndef pe(f,i,n):\n return (f/(math.e**(i*n)))\n\ncpiv = []\ncpid = []\nwith open(\'CPIAUCSL.csv\', \'rb\') as cpifile:\n cpi = csv.reader(cpifile)\n for row in cpi:\n  cpiv.append(row[1])\n  cpid.append(row[0])\n\ncpiv.pop(0)\ncpid.pop(0)\n\ncpidd = []\nfor item in cpid:\n cpidd.append(mpl.dates.date2num(datetime.datetime.strptime(item, ""%Y-%m-%d"")))\n\nplt.plot_date(x=cpidd,y=cpiv)\nplt.show()\n\nc = []\nb = 0\nfor a in cpiv:\n  if b: c.append((float(a)-b)/float(a))\n  b = float(a)\n\nnumpy.mean(c)*12\n\nx = np.poly1d([1,0])\n\ncpif = scipy.fftpack.fft(cpim)\n\ndef ERi(Rf,Bi,ERm): //CAPM\n return (Rf+Bi*(ERm-Rf))\n\ndf = pd.read_csv(\'CPIAUCSL.csv\')\n\nx = [5.05, 6.75, 3.21, 2.66]\ny = [1.65, 26.5, -5.93, 7.96]\ngradient, intercept, r_value, p_value, std_err = stats.linregress(x,y)\nt = np.arange([0.0, 10.0, 0.1])\nz = gradient*t+intercept\nplt.plot(t,z)\nplt.scatter(x,y)\nplt.show()\n'"
flowPairCheck.py,0,"b""import pandas as pd\nimport numpy as np\n\nclientHosts = ['0.0.0.0','1.1.1.1']\nserverHosts = ['2.2.2.2','3.3.3.3']\nnetflowDF = pd.read_csv('netflow.csv', sep=',')\n\nget = lambda  Num, clientHost, serverHost : '' if netflowDF[(netflowDF['Client Host'] == clientHost ) & (netflowDF['Server Host'] == serverHost)].empty else str(Num) + ','\nhosts = []\n\nfor clientHost in clientHosts:\n match = ''\n for index, serverHost in enumerate(serverHosts):\n  match += get(index, clientHost, serverHost)\n hosts.append((serverHost,match))\n\npd.DataFrame(hosts).to_csv('out.csv')\n"""
fourStepModel.py,0,"b'import requests\nimport pandas\nimport geopandas\nimport json\nimport math\nfrom haversine import haversine\nfrom ipfn import ipfn\nimport networkx\nfrom matplotlib import pyplot\nfrom matplotlib import patheffects\n\nurl = \'https://tigerweb.geo.census.gov/arcgis/rest/services/TIGERweb/State_County/MapServer/37/query?where=state%3D06&f=geojson\'\nr = requests.get(url)\nzones = geopandas.GeoDataFrame.from_features(r.json()[\'features\'])\ncentroidFunction = lambda row: (row[\'geometry\'].centroid.y, row[\'geometry\'].centroid.x)\nzones[\'centroid\'] = zones.apply(centroidFunction, axis=1)\n\nurl = \'http://api.census.gov/data/2015/acs5/profile?get=NAME,DP03_0018E&for=county&in=state:06\'\nr = requests.get(url)\nProduction = pandas.DataFrame(r.json()[1:], columns = r.json()[0], dtype=\'int\')\nnameSplit = lambda x: x.split(\',\')[0]\nProduction[\'NAME\'] = Production[\'NAME\'].apply(nameSplit)\nzones = pandas.merge(zones, Production)\nzones[\'Production\'] = zones[\'DP03_0018E\']\n\ndef getEmployment(state, county):\n  prefix = \'EN\'\n  seasonal_adjustment = \'U\'\n  area = format(state, ""02d"") + format(county, ""03d"")\n  data_type = \'1\'\n  size = \'0\'\n  ownership = \'0\'\n  industry = \'10\'\n  seriesid = prefix + seasonal_adjustment + area + data_type + size + ownership + industry\n  headers = {\'Content-type\': \'application/json\'}\n  data = json.dumps({""seriesid"": [seriesid],""startyear"":""2015"", ""endyear"":""2015"", ""registrationKey"": """"})\n  p = requests.post(\'https://api.bls.gov/publicAPI/v2/timeseries/data/\', data=data, headers=headers)\n  employment = p.json()[\'Results\'][\'series\'][0][\'data\'][0][\'value\']\n  return(employment)\n\nemployment = lambda row: int(getEmployment(row[\'state\'], row[\'county\']))\nzones[\'Attraction\'] = zones.transpose().apply(employment)\nzones[\'Production\'] = zones[\'Production\'] * zones.sum()[\'Attraction\'] / zones.sum()[\'Production\']\nzones.index = zones.NAME\nzones.sort_index(inplace=True)\n\ndef costFunction(zones, zone1, zone2, beta):\n  cost = math.exp(-beta * haversine(zones[zone1][\'centroid\'], zones[zone2][\'centroid\']))\n  return(cost)\n\ndef costMatrixGenerator(zones, costFunction, beta):\n  originList = []\n  for originZone in zones:\n    destinationList = []\n    for destinationZone in zones:\n        destinationList.append(costFunction(zones, originZone, destinationZone, beta))\n    originList.append(destinationList)\n  return(pandas.DataFrame(originList, index=zones.columns, columns=zones.columns))\n\ndef tripDistribution(tripGeneration, costMatrix):\n  costMatrix[\'ozone\'] = costMatrix.columns\n  costMatrix = costMatrix.melt(id_vars=[\'ozone\'])\n  costMatrix.columns = [\'ozone\', \'dzone\', \'total\']\n  production = tripGeneration[\'Production\']\n  production.index.name = \'ozone\'\n  attraction = tripGeneration[\'Attraction\']\n  attraction.index.name = \'dzone\'\n  aggregates = [production, attraction]\n  dimensions = [[\'ozone\'], [\'dzone\']]\n  IPF = ipfn.ipfn(costMatrix, aggregates, dimensions)\n  trips = IPF.iteration()\n  return(trips.pivot(index=\'ozone\', columns=\'dzone\', values=\'total\'))\n\nbeta = 0.01\ncostMatrix = costMatrixGenerator(zones.transpose(), costFunction, beta)\ntrips = tripDistribution(zones, costMatrix)\n\ndef modeChoiceFunction(zones, zone1, zone2, modes):\n  distance = haversine(zones[zone1][\'centroid\'], zones[zone2][\'centroid\'])\n  probability = {}\n  total = 0.0\n  for mode in modes:\n    total = total + math.exp(modes[mode] * distance)\n  for mode in modes:\n    probability[mode] = math.exp(modes[mode] * distance) / total\n  return(probability)\n\ndef probabilityMatrixGenerator (zones, modeChoiceFunction, modes):\n  probabilityMatrix = {}\n  for mode in modes:\n    originList = []\n    for originZone in zones:\n      destinationList = []\n      for destinationZone in zones:\n\t  destinationList.append(modeChoiceFunction(zones, originZone, destinationZone, modes)[mode])\n      originList.append(destinationList)\n    probabilityMatrix[mode] = pandas.DataFrame(originList, index=zones.columns, columns=zones.columns)\n  return(probabilityMatrix)\n\nmodes = {\'walking\': .05, \'cycling\': .05, \'driving\': .05}\nprobabilityMatrix = probabilityMatrixGenerator(zones.transpose(), modeChoiceFunction, modes)\ndrivingTrips = trips * probabilityMatrix[\'driving\']\n\ndef routeAssignment(zones, trips):\n  G = networkx.Graph()\n  G.add_nodes_from(zones.columns)\n  for zone1 in zones:\n    for zone2 in zones:\n      if zones[zone1][\'geometry\'].touches(zones[zone2][\'geometry\']):\n        G.add_edge(zone1, zone2, distance = haversine(zones[zone1][\'centroid\'], zones[zone2][\'centroid\']), volume=0.0)\n  for origin in trips:\n    for destination in trips:\n      path = networkx.shortest_path(G, origin, destination)\n      for i in range(len(path) - 1):\n        G[path[i]][path[i + 1]][\'volume\'] = G[path[i]][path[i + 1]][\'volume\'] + trips[zone1][zone2]\n  return(G)\n\ndef visualize(G, zones):\n  fig = pyplot.figure(1, figsize=(10, 10), dpi=90)\n  ax = fig.add_subplot(111)\n  zonesT = zones.transpose()\n  zonesT.plot(ax = ax)\n  for i, row in zones.transpose().iterrows():\n    text = pyplot.annotate(s=row[\'NAME\'], xy=((row[\'centroid\'][1], row[\'centroid\'][0])), horizontalalignment=\'center\', fontsize=6)\n    text.set_path_effects([patheffects.Stroke(linewidth=3, foreground=\'white\'), patheffects.Normal()])\n  for zone1 in G.edge:\n    for zone2 in G.edge[zone1]:\n      volume = G.edge[zone1][zone2][\'volume\']\n      x = [zones[zone1][\'centroid\'][1], zones[zone2][\'centroid\'][1]]\n      y = [zones[zone1][\'centroid\'][0], zones[zone2][\'centroid\'][0]]\n      ax.plot(x, y, color=\'#444444\', linewidth=volume/10000, solid_capstyle=\'round\', zorder=1)\n  pyplot.show(block=False)\n\nG = routeAssignment(zones.transpose(), drivingTrips)\nvisualize(G, zones.transpose())\n'"
freq.py,0,"b""import pandas as pd\nimport matplotlib.pyplot as plt\nfreq = pd.read_csv('frequency.csv', sep=',')\nfreq['count'].plot()\nplt.show()\n"""
getFlows.py,0,"b'from lxml import etree\nfrom cStringIO import StringIO\nimport pycurl\nimport pandas as pd\nimport datetime\n\nstart = datetime.datetime(2014,9,19)\nfinish = datetime.datetime(2014,11,2)\ndelta = datetime.timedelta(hours=1)\ntimes = []\nwhile start <= finish:\n times.append(start)\n start += delta\n\nfor start in times:\n print start.strftime(""%Y-%m-%dT%H:%M:%SZ"")\n end = start + delta\n request1 = \'<?xml version=""1.0"" encoding=""UTF-8""?><soapenc:Envelope xmlns:soapenc=""http://schemas.xmlsoap.org/soap/envelope/""><soapenc:Body><getFlows><flow-filter max-rows=""100000"" domain-id=""133"" include-interface-data=""false"">\'\n request2 = \'<date-selection><time-range-selection start=""\' + start.strftime(""%Y-%m-%dT%H:%M:%SZ"") + \'"" end=""\' + end.strftime(""%Y-%m-%dT%H:%M:%SZ"") + \'"" /></date-selection>\'\n request3 = \'<applications>175,53</applications>\'\n request4 = \'</flow-filter></getFlows></soapenc:Body></soapenc:Envelope>\'\n request = request1 + request2 + request3 + request4\n buffer = StringIO()\n co = pycurl.Curl()\n co.setopt(co.UNRESTRICTED_AUTH,1)\n co.setopt(co.URL,""https://lancope.example.com/smc/swsService/flows"")\n co.setopt(co.POST, 1)\n co.setopt(co.INFILESIZE,len(request) + 1) \n co.setopt(co.WRITEFUNCTION, buffer.write)\n co.setopt(co.POSTFIELDS, request)\n co.setopt(co.SSL_VERIFYPEER, 0L) \n co.setopt(co.SSL_VERIFYHOST, 0L) \n co.setopt(co.USERPWD,""username:password"")\n try:\n  co.perform()\n except:\n  print ""POST failed""\n  exit(1)\n\n co.close()\n out = buffer.getvalue()\n buffer.close()\n doc = etree.fromstring(out)\n netflows = [(\'client\', \'clientHostName\', \'clientPort\', \'clientPackets\', \'clientBytes\', \'server\', \'serverHostName\', \'serverPort\', \'serverPackets\', \'serverBytes\', \'startTime\', \'lastTime\', \'activeDuration\')]\n for elem in doc.getiterator(\'{http://www.lancope.com/sws/sws-service}flow\'):\n  startTime = elem.get(\'start-time\')\n  lastTime = elem.get(\'last-time\')\n  activeDuration = elem.get(\'active-duration\')\n  client = elem[0].get(\'ip-address\')\n  clientHostName = elem[0].get(\'host-name\')\n  clientPort = elem[0].get(\'port\')\n  clientPackets = elem[0].get(\'packets\')\n  clientBytes = elem[0].get(\'bytes\')\n  server = elem[1].get(\'ip-address\')\n  serverHostName = elem[1].get(\'host-name\')\n  serverPort = elem[1].get(\'port\')\n  serverPackets = elem[1].get(\'packets\')\n  serverBytes = elem[1].get(\'bytes\')\n  netflows.append((client, clientHostName, clientPort, clientPackets, clientBytes, server, serverHostName, serverPort, serverPackets, serverBytes, startTime, lastTime, activeDuration))\n\n pd.DataFrame(netflows).to_csv(start.strftime(""%Y-%m-%dT%H%M%SZ"") + end.strftime(""%Y-%m-%dT%H%M%SZ"") + \'.csv\')\n'"
getS3Files.py,0,"b'import s3\n\nbucket_name = \'your-bucket\'\nlocal_directory = \'/Users/user/Desktop/directory/\'\n\nconnection = s3.S3Connection(access_key_id=\'\', secret_access_key=\'\', region=\'us-west-2\', endpoint=\'s3-us-west-2.amazonaws.com\', default_bucket=bucket_name)\nstorage = s3.Storage(connection)\n\nfor bucket in storage.bucket_list():\n  print bucket.name, bucket.creation_date\n\nfor key in storage.bucket_list_keys(bucket_name):\n  remote_name = s3.S3Name(key.key, bucket=bucket_name)\n  try: storage.read(remote_name, local_directory + key.key)\n  except: print ""error: "" + key.key\n'"
gis.py,0,"b'import shapely.geometry\nimport matplotlib.pyplot as plt\nimport fiona.collection\nimport descartes\n\nplaces = {\'Los Angeles\': (34.0204989,-118.4117325), \'Phoenix\': (33.6054149,-112.125051), \'Albuquerque\': (35.0824099,-106.6764794)}\n\npath = [(x, y) for y, x in places.values()]\nls = shapely.geometry.LineString(path)\n\nwith fiona.collection(""tl_2014_us_state/tl_2014_us_state.shp"") as features:\n    states = [shapely.geometry.shape(f[\'geometry\']) for f in features]\n\nfig = plt.figure(figsize=(8,5), dpi=180)\nax = fig.add_subplot(111)\nax.axis([-125, -65, 25, 50])\nax.axis(\'off\')\n\nax.plot(*ls.xy, color=\'#FFFFFF\')\n\nfor state in states:\n if state.geom_type ==\'Polygon\':\n  state = [state]\n for poly in state:\n  poly_patch = descartes.PolygonPatch(poly, fc=\'#6699cc\', ec=\'#000000\')\n  ax.add_patch(poly_patch)\n\nfor x, y in path:\n buffered = shapely.geometry.Point(x, y).buffer(1)\n ax.add_patch(descartes.PolygonPatch(buffered, fc=\'#EEEEEE\', ec=\'#000000\'))\n\n\nfig.show()\n'"
gpx.py,0,"b'import gpxpy\n\ngpx_file = open(""file"", \'r\')\ngpx = gpxpy.parse(gpx_file)\n\npoints = []\nfor point in gpx.tracks[0].segments[0].points:\n points.append((point.latitude,point.longitude,point.elevation,point.time))\n'"
highwayDesign.py,0,"b""from sympy import *\n\nRa, Rrlf, Rrlr, Ff, Fr, W, Thetag, m, a, Rho, CD, Af, V, hpRa, frl, Rrl, hpRrl, Rg, G, Wf, Wr, h, lf, lr, L, F, Fmax, Mu = symbols('Ra Rrlf Rrlr Ff Fr W Thetag m a Rho CD Af V hpRa frl Rrl hpRrl Rg G Wf Wr h lf lr L')\n\n# Equation 2.1\nsolve(-Ff -Fr +m*a +Ra +Rrlf +Rrlr +Rg, variable)\n\n# Equation 2.2\nsolve(-F +m*a + Ra +Rrl +Rg, variable)\n\n# Equation 2.3\nsolve(-Ra + Rational(1,2)*Rho*CD*Af*V**2, variable)\n\n# Equation 2.4\nsolve(-hpRa + Rational(1,1100)*Rho*CD*Af*V**2, variable)\n\n# Equation 2.5\nsolve(-frl + .01*(1+V/147), variable)\n\n#Equation 2.6\nsolve(-Rrl + frl*W, variable)\n\n#Equation 2.7\nsolve(-hpRrl + Rational(1,550)*frl*W*V, variable)\n\n#Equation 2.8\nsolve(-Rg + W*sin(Thetag), variable)\n\n#Equation 2.9\nsolve(-Rg + W*G, variable)\n\n#Equation 2.10\nsolve(-Wr + (Ra*h + W*lf*cos(Thetag)+m*a*h+W*h*sin(Thetag))/L, variable)\n\n#Equation 2.11\nsolve(-Wr +lf*W/L + h*(F-Rrl)/L, variable)\n\n#Equation 2.12\nsolve(-Fmax + Mu*Wr, variable)\n"""
iphorse.py,0,"b""import requests\n\nr = requests.get('http://iphorse.com/json.php')\nr.json()['remote_addr']\n\nimport yaml\nprint yaml.safe_dump(r.json(), default_flow_style=False)\n"""
map-state.py,0,"b'import overpy\nimport numpy\nimport pickle\nfrom matplotlib import pyplot\nfrom shapely.geometry import LineString\nfrom shapely.geometry import shape\nfrom shapely.geometry import box\nfrom shapely.ops import polygonize_full\nfrom descartes import PolygonPatch\nfrom fiona import collection\n\nfeatures = collection(""~/Desktop/gshhg-shp-2/GSHHS_shp/f/GSHHS_f_L1.shp"")\nnorthAmericaPolygon = shape(features[3][\'geometry\'])\n\napi = overpy.Overpass()\n\nstateRelation = api.query(""rel(165475);(._;>;);out;"")\nfnwrRelation = api.query(""rel(3947664);(._;>;);out;"")\nmotorwaysInBoundingBox = api.query(""way(32.120,-125.222,42.212,-113.928)[highway=motorway];(._;>);out;"")\ntrunkInBoundingBox = api.query(""way(32.120,-125.222,42.212,-113.928)[highway=trunk];(._;>);out;"")\n \nstateLineStrings = []\nfor way in stateRelation.ways:\n lineString = []\n for node in way.nodes:\n  lineString.append((node.lon,node.lat))\n stateLineStrings.append(LineString(lineString))\n\npolygons, dangles, cuts, invalids = polygonize_full(stateLineStrings)\nstatePolygonWithOcean = polygons.geoms[3]\n\nstatePolygon = statePolygonWithOcean.intersection(northAmericaPolygon)\n\nfnwrLineStrings = []\nfor way in fnwrRelation.ways:\n lineString = []\n for node in way.nodes:\n  lineString.append((node.lon,node.lat))\n fnwrLineStrings.append(LineString(lineString))\n\nFpolygons, dangles, cuts, invalids = polygonize_full(fnwrLineStrings)\n\nmotorwayLineStrings = []\nfor way in motorwaysInBoundingBox.ways:\n line = []\n for node in way.nodes:\n  line.append((node.lon,node.lat))\n wayLineString = LineString(line)\n if statePolygonWithOcean.contains(wayLineString): motorwayLineStrings.append(wayLineString)\n\nfor way in trunkInBoundingBox.ways:\n line = []\n for node in way.nodes:\n  line.append((node.lon,node.lat))\n wayLineString = LineString(line)\n if statePolygonWithOcean.contains(wayLineString): motorwayLineStrings.append(wayLineString)\n\nislandPolygons = []\nfor i in range(0,len(polygons.geoms)):\n if (i != 3) and (i != 4):\n  islandPolygons.append(polygons.geoms[i])\n\nfor geom in Fpolygons.geoms:\n islandPolygons.append(geom)\n\nfig = pyplot.figure(figsize=(100,100))\nax = fig.add_subplot(111)\n\nfor line in motorwayLineStrings:\n x, y = line.xy\n ax.plot(x, y, color=\'#000000\', linewidth=1, zorder=2)\n\npatch = PolygonPatch(statePolygon, fc=\'#FFFFFF\', ec=\'#000000\', zorder=1)\nax.add_patch(patch)\n\nfor polygon in islandPolygons:\n patch = PolygonPatch(polygon, fc=\'#FFFFFF\', ec=\'#000000\', zorder=1)\n ax.add_patch(patch)\n\nfig.savefig(\'test2.png\')\n\n##if we want more roads:\nhgvInBoundingBox = api.query(""way(32.120,-125.222,42.212,-113.928)[hgv=designated];(._;>);out;"")\nmotorwayLineStrings = []\nfor way in hgvInBoundingBox.ways:\n line = []\n for node in way.nodes:\n  line.append((node.lon,node.lat))\n wayLineString = LineString(line)\n if statePolygonWithOcean.contains(wayLineString): motorwayLineStrings.append(wayLineString)\n\n\n##pickle\noutput = open(\'data.pkl\', \'wb\')\npickle.dump(islandPolygons,output)\npickle.dump(statePolygon,output)\npickle.dump(motorwayLineStrings,output)\noutput.close()\n\npkl_file = open(\'data.pkl\', \'rb\')\nislandPolygons = pickle.load(pkl_file)\nstatePolygon = pickle.load(pkl_file)\nmotorwayLineStrings = pickle.load(pkl_file)\npkl_file.close()\n\n\n##multiprocessing\nfrom multiprocessing import Pool\n\ndef buildLineString(way):\n lineString = []\n for node in way.nodes:\n  lineString.append((node.lon,node.lat))\n return lineString \n\np = Pool(8)\n\nfor way in stateRelation.ways:\n stateLineStrings = p.map(buildLineString,stateRelation.ways)\n\nfeatures = collection(""~/Desktop/gshhg-shp-2/GSHHS_shp/f/GSHHS_f_L1.shp"")\nnorthAmericaPolygon = shape(features[3][\'geometry\'])\n\napi = overpy.Overpass()\n\nstateRelation = api.query(""rel(165475);(._;>;);out;"")\nfnwrRelation = api.query(""rel(3947664);(._;>;);out;"")\nmotorwaysInBoundingBox = api.query(""way(32.120,-125.222,42.212,-113.928)[highway=motorway];(._;>);out;"")\n\nstateLineStrings = []\nfor way in stateRelation.ways:\n lineString = []\n for node in way.nodes:\n  lineString.append((node.lon,node.lat))\n stateLineStrings.append(LineString(lineString))\n\n## debug info\nfrom geopandas import GeoSeries\nfrom geopandas import GeoDataFrame\n\nfor i in range(0,len(islandPolygons)):\n ax.text(islandPolygons[i].centroid.x,islandPolygons[i].centroid.y,i,color = \'k\', weight = \'bold\')\n\nax.plot([-124.482003,-114.1307816],[32.5295236,42.009499])\n\nfrom shapely.geometry import MultiLineString\ntest = MultiLineString(highwayLineStrings)\npatch = PolygonPatch(countyPolygon, fc=\'#6699cc\', ec=\'#6699cc\', alpha=0.5, zorder=2)\nax.add_patch(patch)\n\nobject.__dict__\nresult.nodes\nresult.nodes.[0].lat\nresult.ways\nresult.ways[0]._node_ids[0]\nresult.relations\nresult.relations[0].members[0].ref\n\nlen(streetsInBB.ways[0].get_nodes(resolve_missing=True))\n\nfor way in aboveIslands:\n gray = gray + .025\n for node in way.nodes:\n  ax.scatter(node.lon, node.lat, color=[gray,gray,gray], s=100, zorder=1)\n\nstates = [shapely.geometry.shape(f[\'geometry\']) for f in features]\n\nhttp://overpass.osm.rambler.ru/cgi/interpreter?data=%5Bout:json%5D;relation(396479);out;\n\nFarallon National Wildlife Refuge (3947664)\nSanta Catalina Island (237602)\nSan Clemente Island (237603)\nAnacapa Island (3635899)\nSanta Cruz Island (237600)\nSanta Rosa Island (237601)\nWay: Santa Barbara Island (40501085)\nWay: Sutil Island (40501070)\nWay: San Nicolas Island (40500976)\nWay: San Miguel Island (40500912)\n\nislandRelationNumbers = [3947664,237602,237603,3635899,237600,237601]\nislandWayNumbers = [40501085,40501070,40500976,40500912]\n\nislandRelations = []\nfor relation in islandRelationNumbers:\n response = api.query(""rel("" + relation + "");(._;>;);out;"")\n islandRelations.append(response)\n\nislandWays = []\nfor ways in islandWayNumbers:\n response = api.query(""way("" + way + "");(._;>;);out;"")\n islandWays.append(response)\n\nislandPolygons = []\nfor relation in islandRelations:\n islandLineStrings = []\n for way in relation.ways:\n  lineString = []\n  for node in way.nodes:\n   lineString.append((node.lon,node.lat))\n  islandLineStrings.append(LineString(lineString))\n polygons, dangles, cuts, invalids = polygonize_full(stateLineStrings)\n islandPolygons.append(polygons.geoms[0])\n\nfor way in islandWays:\n lineString = []\n for node in way.nodes:\n  lineString.append((node.lon,node.lat))\n islandLineStrings.append(LineString(lineString))\n polygons, dangles, cuts, invalids = polygonize_full(stateLineStrings)\n islandPolygons.append(polygons.geoms[0])\n'"
movies.py,0,"b""import pandas\nimport requests\n\nmovies = pandas.read_csv('https://raw.githubusercontent.com/davidbailey/Notes/master/Movies.csv')\n\nfor movie in movies.Title:\n  r = requests.get('https://www.omdbapi.com/?t=' + movie)\n  print(r.json()['Title'], r.json()['Year'])\n"""
nessus_report.py,0,"b'import os\nimport sqlite3\nconn = sqlite3.connect(\'nessus.db\')\nc = conn.cursor()\nos.mkdir(\'nessus_report_output\')\n\ndepartments = []\nc.execute(\'SELECT DISTINCT Department FROM dates ORDER BY Department\')\nfor row in c.fetchall():\n departments.append(row[0])\n\nvulnerabilities = [\'MySQL Default Account Credentials\',\'SNMP Agent Default Community Name (public)\',\'Microsoft SQL Server sa Account Default Blank Password\',\'Microsoft Windows SMB Registry : Autologon Enabled\',\'Microsoft Windows Guest Account Belongs to a Group\',\'Microsoft Windows SMB Shares Unprivileged Access\',\'Microsoft Windows 2000 Unsupported Installation Detection\',\'Microsoft Windows Administrator Default Password Detection (W32/Deloder Worm Susceptibility)\',\'NFS Share User Mountable\']\nnonmsapplicationsall = [\'%Adobe Reader%\',\'%Flash Player%\',\'%Adobe AIR%\',\'%Shockwave Player%\',\'%Oracle Java%\',\'%Sun Java%\',\'%iTunes%\',\'%Adobe Acrobat%\',\'%Google Chrome%\',\'%Quicktime%\',\'%Safari%\',\'%RealPlayer%\',\'%Opera%\',\'%Firefox%\',\'%Foxit Reader%\',\'%VLC%\',\'%Google Picasa%\',\'%Adobe Photoshop%\',\'%Netscape Browser%\',\'%Winamp%\',\'%HP System Management%\',\'%Adobe Illustrator%\',\'%IrfanView%\',\'%PHP%\',\'%Wireshark%\',\'%VMware vCenter%\',\'%AIX 6.1 TL%\',\'%Oracle Database%\',\'%Apache%\',\'%SAP Sybase Adaptive Server Enterprise%\',\'%DB2%\',\'%IBM WebSphere Application%\']\n\ne = open(\'nessus_report_output/combined.txt\', \'w\')\nfor department in departments:\n mspatchingpresent = 0 \n nonmspatchingpresent = 0\n vulnerabilitypresent = 0\n f = open(\'nessus_report_output/\' + department + \'.tex\', \'w\')\n f.write(r""""""\n\\documentclass[12pt]{article}\n\\usepackage{longtable}\n\\usepackage{rotating}\n\\usepackage{color}\n\\usepackage{lscape}\n"""""")\n f.write(\'\\\\title{Vulnerability Report for \' + department.replace(""_"","" "") + \'}\')\n f.write(r""""""\n\\author{Author}\n\\date{\\today} \n\\begin{document}\n\\maketitle\n\\newpage\n\\tableofcontents\n\\newpage\n\\section{Executive Summary}\n"""""")\n count = c.execute(\'SELECT COUNT(DISTINCT Host) FROM nessus WHERE (Department LIKE ?)\', (department,)).fetchone()[0]\n f.write(r""""""Number of hosts scanned: """""" + str(count) + r""""""\\\\"""""")\n date = c.execute(\'SELECT Date FROM dates WHERE (Department LIKE ?)\', (department,)).fetchone()[0]\n f.write(r""""""Scan date: """""" + str(date) + r""""""\\\\"""""")\n f.write(r""""""\n\\newpage\n\\section{Microsoft Patching Report}\n\\begin{center}\n\\begin{longtable}{ | l | l | }\n\\hline\nHost & Number of Missing Microsoft Patches \\endhead \\hline\\hline\n"""""")\n microsoftTable = []\n for row in c.execute(\'SELECT DISTINCT(Host), ""Plugin Output"" FROM nessus WHERE (""Plugin ID"" LIKE ""38153"" and Department LIKE ?)\', (department,)):\n  count = row[1].count(\'\\n\')-2\n  microsoftTable.append([row[0],count])\n  mspatchingpresent = 1\n for host, count in sorted(microsoftTable,key=lambda row: -row[1]):\n  if (count > 25): f.write(host + \' & \\\\textcolor{red}{\' + str(count) + \'} \\\\\\\\ \\\\hline\\n\')\n  else: f.write(host + \' & \' + str(count) + \' \\\\\\\\ \\\\hline\\n\')\n f.write(r""""""\\end{longtable}\\end{center}"""""")\n if mspatchingpresent: f.write(\'All other hosts are up-to-date on Microsoft Patches.\')\n else: f.write(\'All hosts up-to-date on Microsoft Patches, or no Microsoft Patching information in scan results.\')\n f.write(r""""""\\newpage"""""")\n f.write(r""""""\\section{Non-Microsoft Patching Report}"""""")\n nonmsapplications = []\n for application in nonmsapplicationsall:\n  count = c.execute(\'SELECT COUNT(DISTINCT ""Plugin ID"") FROM nessus WHERE (Risk LIKE ""High"" OR Risk LIKE ""Critical"") and Name LIKE ? and Department LIKE ?\', (application,department,)).fetchone()[0]\n  if count: nonmsapplications.append(application)\n f.write(r""""""\nThe following table lists the number of missing patches for each application on each host:\n\\begin{longtable}{ | l"""""")\n for application in nonmsapplications: f.write(\' | l \')\n f.write(r"""""" | }\n\\hline\nHost"""""")\n for application in nonmsapplications:\n  f.write(\' & \\\\begin{sideways}\' + application[1:len(str(application))-1] + \' \\\\end{sideways} \')\n f.write(\'\\\\endhead \\\\hline\\\\hline\\n\')\n hosts = []\n nonmsapplicationscollapsed = \'"" OR Name LIKE ""\'.join(nonmsapplicationsall)\n c.execute(\'SELECT DISTINCT Host FROM nessus WHERE (Risk LIKE ""High"" OR Risk LIKE ""Critical"") AND Department like ? AND (Name LIKE ""\' + nonmsapplicationscollapsed + \'"")\',(department,))\n for row in c.fetchall():\n  hosts.append(row[0])\n for host in hosts:\n  f.write(host[:20])\n  for application in nonmsapplications:\n   count = c.execute(\'SELECT COUNT(DISTINCT ""Plugin ID"") FROM nessus WHERE (Risk LIKE ""High"" OR Risk LIKE ""Critical"") and Host LIKE ? and Name LIKE ? and Department LIKE ?\', (host,application,department,)).fetchone()[0]\n   if (count > 10): f.write(\' & \\\\textcolor{red}{\' + str(count) + \'}\')\n   elif (count > 0): f.write(\' & \' + str(count))\n   else: f.write(\' & \')\n   nonmspatchingpresent = 1\n  f.write(\'\\\\\\\\ \\\\hline\\n\')\n f.write(r""""""\n\\end{longtable}\n"""""")\n if nonmspatchingpresent: f.write(\'All other hosts are up-to-date on Non-Microsoft Patches.\')\n else: f.write(\'All hosts up-to-date on Non-Microsoft Patches, or No Non-Microsoft Patching information in scan results.\')\n f.write(r""""""\n\\newpage \n\\section{Other Vulnerabilities}\n\\begin{center}\n\\begin{longtable}{ | l | l | }\n\\hline\nIssue & Host \\\\ \\hline\n"""""")\n for vulnerability in vulnerabilities: \n  for row in c.execute(\'SELECT DISTINCT Host FROM nessus WHERE (Department LIKE ? and Name LIKE ?)\', (department, vulnerability,)):    \n    f.write(vulnerability + \' & \' + row[0] + \' \\\\\\\\ \\\\hline\\n\')\n    e.write(department + \',\' + vulnerability + \',\' + row[0] + \'\\n\')\n    vulnerabilitypresent = 1\n f.write(r""""""\n\\end{longtable}\n\\end{center}\n"""""")\n if not vulnerabilitypresent: f.write(\'No Other Vulnerabilities in scan results.\')\n f.write(r""""""\n\\end{document}\n"""""")\n f.close()\n os.system(\'texi2pdf -q -o nessus_report_output/\' + department + \'.pdf nessus_report_output/\' + department + \'.tex\')\ne.close()\n'"
oktaAppSAML.py,0,"b'import requests\nimport json\n\n# everything returns a 2xx on success and a 4xx on errors\n\noktaAPIToken = """"\noktaOrg = ""org.oktapreview.com"" # org.okta.com or org.oktapreview.com\n\nheaders = {\'Accept\': \'application/json\', \'Content-Type\': \'application/json\', \'Authorization\': \'SSWS \' + oktaAPIToken}\n\ndata = {\n  ""name"": ""template_app"",\n  ""label"": ""test-app"",\n  ""signOnMode"": ""SAML_2_0"",\n  ""settings"": {\n    ""app"": {\n      ""audienceRestriction"": ""http://localhost"",\n      ""forceAuthn"": False,\n      ""postBackURL"": ""http://localhost"",\n      ""authnContextClassRef"": ""urn:oasis:names:tc:SAML:2.0:ac:classes:PasswordProtectedTransport"",\n      ""requestCompressed"": ""COMPRESSED"",\n      ""recipient"": ""http://localhost"",\n      ""signAssertion"": ""SIGNED"",\n      ""destination"": ""http://localhost"",\n      ""signResponse"": ""SIGNED"",\n      ""nameIDFormat"": ""urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress"",\n      ""groupName"": """",\n      ""groupFilter"": """",\n      ""defaultRelayState"": """",\n      ""configuredIssuer"": """",\n      ""attributeStatements"": """"\n      }\n    },\n  ""visibility"": {\n      ""hide"": {\n        ""iOS"": True,\n        ""web"": True\n    }\n  }\n}\n\nr = requests.post(""https://"" + oktaOrg + ""/api/v1/apps"", headers = headers, data = json.dumps(data)) # create an app\nappid = r.json()[""id""]\n\nappid = """"\nr = requests.get(""https://"" + oktaOrg + ""/api/v1/apps/"" + appid , headers = headers) # view an app\n\ngroupid = """"\nr = requests.put(""https://"" + oktaOrg + ""/api/v1/apps/"" + appid + ""/groups/"" + groupid, headers = headers, data = {}) # assign a group to an app\n\nr = requests.post(""https://"" + oktaOrg + ""/api/v1/apps/"" + appid + ""/lifecycle/deactivate"", headers = headers) # deactivate an app (before deletion)\n\nr = requests.delete(""https://"" + oktaOrg + ""/api/v1/apps/"" + appid , headers = headers) # delete an app\n'"
oktaDiffCSV.py,0,"b'from os.path import expanduser\nimport json\nimport requests\nimport pandas\n\noktaAPIToken = """"\noktaGroup = """"\noktaOrg = """"\n\nappUsers = []\ndf = pandas.read_csv(expanduser(\'~/Desktop/users.csv\'))\nfor email in df.index:\n  appUsers.append(email)\n\noktaUsers = []\nr = requests.get(""https://"" + oktaOrg + "".okta.com/api/v1/groups/"" + oktaGroup + ""/users"", headers={\'Authorization\': \'SSWS \' + oktaAPIToken})\noktaUsersJson =  r.json()\nfor user in oktaUsersJson:\n  oktaUsers.append(user[\'id\'])\n\nactiveOktaUsers = []\nfor user in oktaUsers:\n  r = requests.get(""https://"" + oktaOrg + "".okta.com/api/v1/users/"" + user, headers={\'Authorization\': \'SSWS \' + oktaAPIToken})\n  oktaUserJson =  r.json()\n  if oktaUserJson[\'status\'] == u\'ACTIVE\':\n    activeOktaUsers.append(oktaUserJson[\'profile\'][\'email\'])\n\ninAppNotInOkta = set(appUsers) - set(activeOktaUsers)\ninOktaNotInApp = set(activeOktaUsers) - set(appUsers)\n\nprint ""These people are in App and not in Okta""\nfor user in inAppNotInOkta:\n  print user\n\nprint ""These people are in Okta and not in App""\nfor user in inOktaNotInApp :\n  print user\n'"
oktaDiffDropbox.py,0,"b'import json\nimport requests\n\ndropboxAPIToken = """"\noktaAPIToken = """"\noktaGroup = """"\noktaOrg = """"\n\ndropboxUsers = []\nr = requests.post(""https://api.dropboxapi.com/2/team/members/list"", headers={\'Authorization\': \'Bearer \' + dropboxAPIToken, \'Content-Type\': \'application/json\'}, data=""{\\""limit\\"": 700}"" )\ndropboxUsersJson =  r.json()\nfor user in dropboxUsersJson[\'members\']:\n  dropboxUsers.append(user[\'profile\'][\'email\'])\n\noktaUsers = []\nr = requests.get(""https://"" + oktaOrg + "".okta.com/api/v1/groups/"" + oktaGroup + ""/users"", headers={\'Authorization\': \'SSWS \' + oktaAPIToken})\noktaUsersJson =  r.json()\nfor user in oktaUsersJson:\n  oktaUsers.append(user[\'profile\'][\'login\'])\n\ninDropboxNotInOkta = set(dropboxUsers) - set(oktaUsers)\ninOktaNotInDropbox = set(oktaUsers) - set(dropboxUsers)\n\nprint ""These people are in Dropbox and not in Okta""\nfor user in inDropboxNotInOkta:\n  print user\n\nprint ""These people are in Okta and not in Dropbox""\nfor user in inOktaNotInDropbox :\n  print user\n'"
oktaDiffSalesforce.py,0,"b'from os.path import expanduser\nfrom suds.client import Client\nimport pandas\nimport re\n\nsfurl = ""https://org.my.salesforce.com/services/Soap/c/36.0/XXXXXXXXXXXXXXX""\nusername = """"\npassword = """"\nsecurity_token = """"\n\nclient = Client(expanduser(\'~/Desktop/enterprise.wsdl.xml\'))\nlogin_request = client.service.login(username, password+security_token)\nclient = Client(url=expanduser(\'~/Desktop/enterprise.wsdl.xml\'), location=sfurl)\nsession_header = client.factory.create(\'SessionHeader\')\nsession_header.sessionId = login_request[\'sessionId\']\nclient.set_options(\n    soapheaders = {\n        \'SessionHeader\': session_header,\n        })\n\nsoslQuery = ""SELECT Id, IsActive, FirstName, LastName, Email, UserRoleId, ProfileId, UserPermissionsMarketingUser, UserPermissionsOfflineUser, UserPermissionsMobileUser, UserPermissionsSupportUser, UserPermissionsLiveAgentUser, UserPermissionsSFContentUser, UserPermissionsJigsawProspectingUser, UserPermissionsWorkDotComUserFeature, UserPermissionsKnowledgeUser, UserPermissionsInteractionUser FROM User"" # I believe UserPermissionsSupportUser == Service Cloud User\nUser = client.service.query(soslQuery)\n\nusers = []\nfor user in User.records:\n  if user.IsActive:\n    try: users.append((user.Id, user.FirstName, user.LastName, user.Email, user.UserRoleId, user.ProfileId, user.UserPermissionsMarketingUser, user.UserPermissionsOfflineUser, user.UserPermissionsMobileUser, user.UserPermissionsSupportUser, user.UserPermissionsLiveAgentUser, user.UserPermissionsSFContentUser, user.UserPermissionsJigsawProspectingUser, user.UserPermissionsWorkDotComUserFeature, user.UserPermissionsKnowledgeUser, user.UserPermissionsInteractionUser))\n    except: users.append((user.Id, user.FirstName, user.LastName, user.Email, ""False"", user.ProfileId, user.UserPermissionsMarketingUser, user.UserPermissionsOfflineUser, user.UserPermissionsMobileUser, user.UserPermissionsSupportUser, user.UserPermissionsLiveAgentUser, user.UserPermissionsSFContentUser, user.UserPermissionsJigsawProspectingUser, user.UserPermissionsWorkDotComUserFeature, user.UserPermissionsKnowledgeUser, user.UserPermissionsInteractionUser))\n\nusersDF = pandas.DataFrame(users,columns=(\'userid\', \'FirstName\', \'LastName\', \'Email\', \'UserRoleId\', \'ProfileId\', \'UserPermissionsMarketingUser\', \'UserPermissionsOfflineUser\', \'UserPermissionsMobileUser\', \'UserPermissionsSupportUser\', \'UserPermissionsLiveAgentUser\', \'UserPermissionsSFContentUser\', \'UserPermissionsJigsawProspectingUser\', \'UserPermissionsWorkDotComUserFeature\', \'UserPermissionsKnowledgeUser\', \'UserPermissionsInteractionUser\'))\n\nsoslQuery = ""SELECT Id, Name FROM UserRole""\nUserRole = client.service.query(soslQuery)\n\nroles = []\nfor role in UserRole.records:\n  roles.append((role.Id, role.Name))\n\nrolesDF = pandas.DataFrame(roles,columns=(\'UserRoleId\',\'rolename\'))\n\nsoslQuery = ""SELECT Id, Name FROM Profile""\nProfile = client.service.query(soslQuery)\n\nprofiles = []\nfor profile in Profile.records:\n  profiles.append((profile.Id, profile.Name))\n\nprofilesDF = pandas.DataFrame(profiles,columns=(\'ProfileId\',\'profilename\'))\n\nsoslQuery = ""SELECT Id, Assignee.Id, PermissionSet.Name FROM PermissionSetAssignment""\nPermissionSetAssignment = client.service.query(soslQuery)\n\nsoslQuery = ""SELECT Id, Name FROM PermissionSet""\n\npermissionsetList = []\nPermissionSets = client.service.query(soslQuery)\nfor permissionset in PermissionSets.records:\n  soslQuery = ""SELECT Id, Assignee.Id FROM PermissionSetAssignment WHERE PermissionSet.Id = \'"" + permissionset[\'Id\'] + ""\'""\n  PermissionSetAssignments = client.service.query(soslQuery)\n  if PermissionSetAssignments.size > 0:\n    permissionsetassignments = []\n    for assignement in PermissionSetAssignments.records:\n      permissionsetassignments.append(assignement.Assignee.Id)\n    permissionsetList.append({permissionset[\'Name\']: permissionsetassignments})\n\npattern = ""X00.*""\npermissionsetassignmentList = []\nfor user in usersDF.userid:\n  userList = []\n  for permissionset in permissionsetList:\n    if user in permissionset.values()[0]:\n      if not re.search(pattern, permissionset.keys()[0]):\n        userList.append(permissionset.keys()[0])\n  permissionsetassignmentList.append((user,userList))\n\npermissionsetassignmentListDF = pandas.DataFrame(permissionsetassignmentList,columns=(\'userid\',\'Permission Set List\'))\n\nsoslQuery = ""SELECT Id, GroupId, UserOrGroupId FROM GroupMember""\nGroupMember = client.service.query(soslQuery)\n\ngroupmembers = []\nfor groupmember in GroupMember.records:\n    groupmembers.append((groupmember.GroupId, groupmember.UserOrGroupId))\n\ngroupmembersDF = pandas.DataFrame(groupmembers,columns=(\'groupid\',\'userorgroupid\'))\n\nsoslQuery = ""SELECT Id, Name, Type FROM Group""\nGroup = client.service.query(soslQuery)\n\ngroups = []\nfor group in Group.records:\n  if group.Type == ""Regular"":\n    groups.append((group.Id, group.Name))\n\ngroupsDF = pandas.DataFrame(groups,columns=(\'groupid\',\'name\'))\n\ngroupsAndgroupmembersDF = pandas.merge(groupsDF,groupmembersDF,how=\'left\')\n\ngroupsList = []\nfor user in usersDF.userid:\n  groupsList.append((user,groupsAndgroupmembersDF[groupsAndgroupmembersDF[\'userorgroupid\'] == user][\'name\'].tolist()))\n\ngroupsListDF = pandas.DataFrame(groupsList,columns=(\'userid\',\'Group List\'))\n\nusersAndRolesDF = pandas.merge(usersDF,rolesDF,how=\'left\')\nusersAndRolesAndProfilesDF = pandas.merge(usersAndRolesDF,profilesDF,how=\'left\')\nusersAndRolesAndProfilesAndpermissionsetassignmentListDF = pandas.merge(usersAndRolesAndProfilesDF,permissionsetassignmentListDF,how=\'left\')\nusersAndRolesAndProfilesAndpermissionsetassignmentListAndgroupsListDF = pandas.merge(usersAndRolesAndProfilesAndpermissionsetassignmentListDF,groupsListDF,how=\'left\')\nusersAndRolesAndProfilesAndpermissionsetassignmentListAndgroupsListDF.to_csv(expanduser(\'~/Desktop/salesforce_users.csv\'))\ndf = usersAndRolesAndProfilesAndpermissionsetassignmentListAndgroupsListDF\n\nimport json\nimport requests\n\noktaAPIToken = """"\noktaOrg = """"\nurl = ""https://"" + oktaOrg + "".okta.com/api/v1/users?limit=200""\noktaUsers = []\n\nwhile url:\n  r = requests.get(url, headers={\'Authorization\': \'SSWS \' + oktaAPIToken})\n  oktaUsersJson =  r.json()\n  for user in oktaUsersJson:\n    try: oktaUsers.append([user[\'profile\'][\'login\'], user[\'profile\'][\'title\']])\n    except: print user[\'profile\'][\'login\']\n  try: url = r.links[\'next\'][\'url\']\n  except: url = False\n\noktaDF = pandas.DataFrame(oktaUsers,columns=(\'Email\',\'orgID\',\'title\'))\nokta_sf_DF = pandas.merge(df,oktaDF,how=\'right\')\nokta_sf_DF.to_csv(expanduser(\'~/Desktop/okta_salesforce_users.csv\'))\n'"
oktaDiffSlack.py,0,"b'import json\nimport requests\n\nslackAPIToken = """"\noktaAPIToken = """"\noktaGroup = """"\noktaOrg = """"\n\nslackDeleted = []\nslackBots = []\nslackRestricted = []\nslackUsers = []\nr = requests.get(""https://slack.com/api/users.list?token="" + slackAPIToken)\nslackUsersJson =  r.json()\nfor user in slackUsersJson[\'members\']:\n  if (user[\'deleted\'] == True): slackDeleted.append(user[\'profile\'][\'real_name\'])\n  elif (user[\'is_bot\'] == True): slackBots.append(user[\'profile\'][\'real_name\'])\n  elif (user[\'is_restricted\'] == True): slackRestricted.append(user[\'profile\'][\'real_name\'])\n  else: slackUsers.append(user[\'profile\'][\'email\'])\n\noktaUsers = []\nr = requests.get(""https://"" + oktaOrg + "".okta.com/api/v1/groups/"" + oktaGroup + ""/users"", headers={\'Authorization\': \'SSWS \' + oktaAPIToken})\noktaUsersJson =  r.json()\nfor user in oktaUsersJson:\n  oktaUsers.append(user[\'profile\'][\'login\'])\n\ninSlackNotInOkta = set(slackUsers) - set(oktaUsers)\ninOktaNotInSlack = set(oktaUsers) - set(slackUsers)\n\nprint ""These people are in Slack and not in Okta""\nfor user in inSlackNotInOkta:\n  print user\n\nprint ""These people are in Okta and not in Slack""\nfor user in inOktaNotInSlack :\n  print user\n'"
oktaDiffTxt.py,0,"b'import json\nimport requests\n\noktaAPIToken = """"\noktaGroup = """"\noktaOrg = """"\n\nappUsers = []\nwith open(\'appUsers.txt\', \'r\') as appFile:\n  for line in appFile:\n    appUsers.append(line.strip().lower())\n\noktaUsers = []\nr = requests.get(""https://"" + oktaOrg + "".okta.com/api/v1/groups/"" + oktaGroup + ""/users"", headers={\'Authorization\': \'SSWS \' + oktaAPIToken})\noktaUsersJson =  r.json()\nfor user in oktaUsersJson:\n  oktaUsers.append(user[\'profile\'][\'login\'])\n\nr = requests.get(""https://"" + oktaOrg + "".okta.com/api/v1/groups/"" + oktaGroup2 + ""/users"", headers={\'Authorization\': \'SSWS \' + oktaAPIToken})\noktaUsersJson =  r.json()\nfor user in oktaUsersJson:\n  oktaUsers.append(user[\'profile\'][\'login\'])\n\ninAppNotInOkta = set(appUsers) - set(oktaUsers)\ninOktaNotInApp = set(oktaUsers) - set(appUsers)\n\nprint ""These people are in App and not in Okta""\nfor user in inAppNotInOkta:\n  print user\n\nprint ""These people are in Okta and not in App""\nfor user in inOktaNotInApp :\n  print user\n'"
pandas-stacked-bar.py,0,"b""import pandas\nfrom matplotlib import pyplot\ndf = pandas.DataFrame([431-106,106])\ndf = df.transpose()\ndf.columns=['complete','incomplete']\ndf.plot(kind='bar', stacked=True, legend=False)\npyplot.show()\n"""
pie.py,0,"b'# Pinout Diagram: https://pinout.xyz\n# GPIO - Digital Output from MQ4 (Combustable Gas), MQ7 (CO), HR202 (Humidity) - Connect 3.3V, GND, GPIO\nimport RPi.GPIO as GPIO\nGPIO.setmode(GPIO.BOARD) # or GPIO.BCM\nmode = GPIO.getmode()\n\nchannel = 8\nGPIO.setup(channel, GPIO.IN) # GPIO.setup(channel, GPIO.IN, pull_up_down=GPIO.PUD_DOWN)\nGPIO.input(channel)\n\nGPIO.cleanup()\n\n# Serial UART TTL (via CP2102) - EM4100 (125kHz RFID) - Connect 5V, GND, Tx (via CP2102) / Rx (GPIO)\nimport serial\n# s = serial.Serial(port=\'/dev/ttyUSB0\', baudrate=9600) # CP2102\ns = serial.Serial(port=\'/dev/ttyAMA0\', baudrate=9600) # https://sites.google.com/site/semilleroadt/raspberry-pi-tutorials/gpio\ns.isOpen()\n\nid = []\nwhile True:\n  id.append(s.read())\n  if id == [\'\\x00\',\'J\',\'v\',\'O\']: break\n\ns.close()\n\n# I2C - BMP180 (Barometer), ADS1115 (4 Channel, 16bit ADC) - Connect 3.3V, GND, SDA, SCL\n# sudo apt-get install python-smbus i2c-tools\n# echo ""\n# dtparam=i2c1=on\n# dtparam=i2c_arm=on\n# "" >> /boot/config.txt\n# echo ""\n# i2c-bcm2708 \n# i2c-dev\n# "" >> /etc/modules\n# i2cdetect -y 1\n# Based on https://github.com/dhhagan/Adafruit-Raspberry-Pi-Python-Code/blob/master/ADS1115/__init__.py\n# pip install adafruit-python\nimport smbus\nfrom time import sleep\nbus = smbus.SMBus(1)\na2d_address = 0x48\nbmp085_address = 0x77\n\nbus.write_i2c_block_data(a2d_address, 0x01, [197,131])\nsleep(.01)\na0 = bus.read_i2c_block_data(a2d_address, 0x00, 2)\nbus.write_i2c_block_data(a2d_address, 0x01, [213,131])\nsleep(.01)\na1 = bus.read_i2c_block_data(a2d_address, 0x00, 2)\nbus.write_i2c_block_data(a2d_address, 0x01, [229,131])\nsleep(.01)\na2 = bus.read_i2c_block_data(a2d_address, 0x00, 2)\nbus.write_i2c_block_data(a2d_address, 0x01, [245,131])\nsleep(.01)\na3 = bus.read_i2c_block_data(a2d_address, 0x00, 2)\n\nimport Adafruit_BMP.BMP085 as BMP085\nbmp085 = BMP085.BMP085()\n\ntemperature = bmp085.read_temperature()\npressure = bmp085.read_pressure()\naltitude = bmp085.read_altitude()\nsealevel_pressure = bmp085.read_sealevel_pressure()\n\nout = [time(), 256 * a0[0] + a0[1], 256 * a1[0] + a1[1], 256 * a2[0] + a2[1], 256 * a3[0] + a3[1], temperature, pressure, altitude, sealevel_pressure]\n# echo ""a0, a1, a2, a3"" > out.csv\nf = open(\'out.csv\', \'a\')\nf.write(str(out)[1:-1] + ""\\n"")\nf.close()\n\n\n# Sharp GP2Y1010AU0F (Optical Dust Sensor) - Connect ...\n# http://www.howmuchsnow.com/arduino/airquality/\n# https://github.com/PaulZC/GP2Y1010AU0F_Dust_Sensor\nimport RPi.GPIO as GPIO\nGPIO.setmode(GPIO.BOARD)\nchannel = 11\nGPIO.setup(channel, GPIO.OUT)\n\nimport smbus\nfrom time import sleep\nbus = smbus.SMBus(1)\naddress = 0x48\n\nwhile(True):\n  GPIO.output(channel, GPIO.LOW)\n  bus.write_i2c_block_data(address, 0x01, [229,131])\n  sleep(0.00028)\n  a2 = bus.read_i2c_block_data(address, 0x00, 2)\n  sleep(0.00004)\n  GPIO.output(channel, GPIO.HIGH)\n  sleep(0.0968)\n  print(a2[0]*256 + a2[1])\n\n\n# SPI - RC522 (13.56MHz RIFD NFC) - Connect ...\n# https://github.com/mxgxw/MFRC522-python\n# ...\n# echo spi-bcm2708 >> /etc/modules\n# echo dtparam=spi=on >> /boot/config.txt\n# apt-get install python-pip\n# pip install spi \nimport spi\ns = spi.SPI(""/dev/spidev0.0"")\n\ns.read(1000)\n\n# SPI - nRF24L01 (2.4GHz Tranceiver) - http://tmrh20.github.io/RF24/\n\n'"
pieSensors.py,0,"b'import RPi.GPIO as GPIO\nGPIO.setmode(GPIO.BOARD)\nchannel = 11\nGPIO.setup(channel, GPIO.OUT)\n\nimport smbus\nfrom time import sleep\nbus = smbus.SMBus(1)\naddress = 0x48\n\nimport Adafruit_BMP.BMP085 as BMP085\nbmp085 = BMP085.BMP085()\n\nfrom time import time\n\nwhile(True):\n  bus.write_i2c_block_data(address, 0x01, [229,131])\n  sleep(.01)\n  GPIO.output(channel, GPIO.LOW)\n  sleep(0.00028)\n  a2 = bus.read_i2c_block_data(address, 0x00, 2)\n  sleep(0.00004)\n  GPIO.output(channel, GPIO.HIGH)\n  sleep(0.0968)\n  bus.write_i2c_block_data(address, 0x01, [197,131])\n  sleep(.01)\n  a0 = bus.read_i2c_block_data(address, 0x00, 2)\n  bus.write_i2c_block_data(address, 0x01, [213,131])\n  sleep(.01)\n  a1 = bus.read_i2c_block_data(address, 0x00, 2)\n  bus.write_i2c_block_data(address, 0x01, [245,131])\n  sleep(.01)\n  a3 = bus.read_i2c_block_data(address, 0x00, 2)\n  temperature = bmp085.read_temperature()\n  pressure = bmp085.read_pressure()\n  altitude = bmp085.read_altitude()\n  sealevel_pressure = bmp085.read_sealevel_pressure()\n  out = [time(), 256 * a0[0] + a0[1], 256 * a1[0] + a1[1], 256 * a2[0] + a2[1], 256 * a3[0] + a3[1], temperature, pressure, altitude, sealevel_pressure]\n  f = open(\'out.csv\', \'a\')\n  f.write(str(out)[1:-1] + ""\\n"")\n  f.close()\n  sleep(59)\n\nimport pandas\nfrom matplotlib import pyplot\n\ndf = pandas.read_csv(\'out.csv\', names=[\'time\', \'a0 (MQ4 CH4)\', \'a1 (MQ7 CO)\', \'a2 (Dust)\', \'a3\', \'temperature\', \'pressure\', \'altitude\', \'sealevel_pressure\'], index_col=0)\ndf.plot(style=\'.\')\npyplot.show()\n'"
podcast.py,0,"b'import pandas\nimport pafy\nimport requests\nfrom cgi import escape\nfrom urllib import urlencode\nfrom email.Utils import formatdate\nfrom bottle import route, run, template\n\nmusic = pandas.read_csv(\'https://raw.githubusercontent.com/davidbailey/Notes/master/Music.csv\')\n#urls = [""https://www.youtube.com/watch?v=c2hkO5olZAg"", ""https://www.youtube.com/watch?v=KnddGSCu_WQ""]\nurls = music[music[\'YouTube Link\'].notnull()]\n\ndef getURL(url):\n  try:\n    video = pafy.new(url)\n    video_url = video.getbest().url\n    filesize = video.getbest().get_filesize()\n    r = requests.get(""http://tinyurl.com/api-create.php?"" + urlencode({\'url\':video_url}))\n    short_url = r.text\n    return (short_url, filesize)\n  except:\n    return (\'nourl\', 0)\n  \nurls[\'urlfilesize\'] = urls[\'YouTube Link\'].apply(getURL)\n#urls.to_csv()\nrss = \'<rss xmlns:atom=""http://www.w3.org/2005/Atom"" xmlns:media=""http://search.yahoo.com/mrss/"" xmlns:itunes=""http://www.itunes.com/dtds/podcast-1.0.dtd"" xmlns:sy=""http://purl.org/rss/1.0/modules/syndication/"" version=""2.0""><channel><title>Localhost</title><language>en-US</language>\'\ntimestamp = formatdate(0)\nfor index, row in urls.iterrows():\n  if row[\'urlfilesize\'][0] != \'nourl\':\n      rss += ""<item>""\n      rss += ""<title>"" + escape(row[\'Song\']) + ""</title>""\n      rss += ""<pubDate>"" + timestamp + ""</pubDate>""\n      rss += ""<enclosure url=\\"""" + str(row[\'urlfilesize\'][0]) + ""\\"" length=\\"""" + str(row[\'urlfilesize\'][1]) + ""\\"" type=\\""video/mp4\\"" />""\n      rss += ""</item>""\n\nrss += ""</channel></rss>""\n\n@route(\'/podcast.xml\')\ndef index():\n  return rss\n\nrun(host=\'10.0.1.4\', port=8080)\n'"
railRadius.py,0,"b'import geopandas\nimport overpass\nimport math\n\napi = overpass.API()\n\nname = \'Pacific Coast Highway\'\nresponse = api.Get(\'way[""name""=""\' + name + \'""]\')\nwith open(\'temp.tmp\', \'w\') as f:\n  f.write(str(response))\n\ngdf = geopandas.read_file(\'temp.tmp\')\n\ndef points2radius(pt1, pt2, pt3):\n  a = math.sqrt((pt1[0] - pt2[0])**2 + (pt1[1] - pt2[1])**2)\n  b = math.sqrt((pt2[0] - pt3[0])**2 + (pt2[1] - pt3[1])**2)\n  c = math.sqrt((pt1[0] - pt3[0])**2 + (pt1[1] - pt3[1])**2)\n  angleA = math.degrees(math.acos((b**2 + c**2 - a**2) / (2 * b * c)))\n  angleB = math.degrees(math.acos((c**2 + a**2 - b**2) / (2 * c * a)))\n  return max(angleA,angleB)\n\ndef geometry2maxradius(geometry):\n  maxradius = 0\n  for i in range(len(geometry.coords) - 2):\n    if points2radius(geometry.coords[i], geometry.coords[i+1], geometry.coords[i+2]) > maxradius:\n      maxradius = points2radius(geometry.coords[i], geometry.coords[i+1], geometry.coords[i+2])\n  return maxradius\n\nmaxradius = lambda x: geometry2maxradius(x)\nlength = lambda x: math.sqrt((x.coords[0][0] - x.coords[-1][0])**2 + (x.coords[0][1] - x.coords[-1][1])**2)\nsegments = lambda x: len(x.coords)\ngdf[\'maxradius\'] = gdf[\'geometry\'].map(maxradius)\ngdf[\'length\'] = gdf[\'geometry\'].map(length)\ngdf[\'segments\'] = gdf[\'geometry\'].map(segments)\n\nla = gdf[gdf[\'tiger:county\'] == \'Los Angeles, CA\'][[\'maxradius\', \'length\', \'segments\']]\nla.sort_values(\'length\')\n'"
risk.py,6,"b""import numpy as np\n#from pylab import *\nimport matplotlib.pyplot as plot\nimport math\n\ntop = np.array([[0,1,2],[2,2,1]])\nmid = np.array([[0,1,2],[2,1,0]])\nbot = np.array([[0,1,2],[2,0,0]])\n\nplot.plot(top[0],top[1])\nplot.plot(mid[0],mid[1])\nplot.plot(bot[0],bot[1])\n\nt = np.arange(0.0, 10.0, 0.1)\ntop2 = 10-np.arange(0.0, 1.0, 0.01)\ntop = 10-(.3*t)**2\nmid = -t + 10\nbot = 10-np.sqrt(t)\nbot2 = 1/(t)\n\nplot.plot(t,top2)\nplot.plot(t,top)\nplot.plot(t,mid)\nplot.plot(t,bot)\nplot.plot(t,bot2)\n\nplot.scatter(2,6)\nplot.scatter(2,5)\nplot.scatter(9,1)\n\nplot.xlabel('Cost')\nplot.ylabel('Risk')\nplot.title('Risk vs. Cost')\nplot.grid(True)\nplot.show()\n\n\n\n# Risk = Money * Time\n# Time = constant * Money\n# Outsource Time = constant1 * Time + constant2 * Money\n"""
salesforceDisableUsers.py,0,"b'from os.path import expanduser\nfrom suds.client import Client\nimport pandas\n\nstg = pandas.read_csv(expanduser(\'~/Desktop/salesforce-stg.csv\'))\nprod = pandas.read_csv(expanduser(\'~/Desktop/salesforce-prod.csv\'))\n\nremoveBits = lambda x: x.split(""="")[0].split(""@"")[0]\n\nstg[\'Prefix\'] = stg[\'Email\'].map(removeBits)\nprod[\'Prefix\'] = prod[\'Email\'].map(removeBits)\n\nusersToDisablePrefixes = pandas.DataFrame(list(set(stg[\'Prefix\']) - set(prod[\'Prefix\'])), columns=[\'Prefix\'])\nusersToDisableDF = pandas.merge(stg, usersToDisablePrefixes)\nusersToDisable = list(usersToDisableDF[\'userid\'])\n\nsfurl = ""https://org.my.salesforce.com/services/Soap/c/36.0/XXXXXXXXXXXXXXX""\nusername = \'\'\npassword = \'\'\nsecurity_token = \'\'\n\nclient = Client(expanduser(\'~/Desktop/stg.enterprise.wsdl.xml\'))\nlogin_request = client.service.login(username, password+security_token)\nclient = Client(url=expanduser(\'~/Desktop/stg.enterprise.wsdl.xml\'), location=sfurl)\nsession_header = client.factory.create(\'SessionHeader\')\nsession_header.sessionId = login_request[\'sessionId\']\nclient.set_options(\n    soapheaders = {\n        \'SessionHeader\': session_header,\n        })  \n\nfor userid in usersToDisable:\n  soslQuery = ""SELECT Id, IsActive FROM User WHERE Id=\'"" + userid + ""\'""\n  User = client.service.query(soslQuery)\n  user = User.records[0]\n  user.IsActive = False\n  client.service.update(user)\n'"
speeds.py,0,"b""import requests\nimport io\nimport zipfile\nimport pandas\nimport numpy\n#import gtfstk\nfrom shapely.geometry import Point\nfrom shapely.geometry import LineString\nimport matplotlib.pyplot as plt\nfrom datetime import timedelta\nfrom os.path import expanduser\nimport geopandas\nfrom bottle import route, run, template\n\ndef combine(listA,listB):\n  if not listA:\n    return listB\n  elif not listB:\n    return listA\n  elif listA[0] == listB[0]:\n    newList = [listA[0]]\n    newList.extend(combine(listA[1:],listB[1:]))\n    return newList\n  elif listA[0] in listB:\n    return combine(listB,listA)\n  else:\n    newList = [listA[0]]\n    newList.extend(combine(listA[1:],listB))\n    return newList\n\nclass getFeed:\n  def __init__(self,url):\n    get = requests.get(url)\n    file = io.BytesIO(get._content)\n    zipFile = zipfile.ZipFile(file)\n    for name in zipFile.namelist():\n      file = io.BytesIO(zipFile.open(name).read())\n      name = name.rstrip('.txt')\n      setattr(self, name, pandas.read_csv(file))\n  def __getitem__(self, name):\n    return getattr(self, name)\n\n#  amtrak = gtfstk.feed.Feed(expanduser('~/Desktop/la-metro_20101211_0848.zip'))  #http://www.gtfs-data-exchange.com/agency/la-metro/\namtrak = getFeed('http://gtfs.s3.amazonaws.com/amtrak_20140723_0354.zip')\namtrak.stops.set_index('stop_id', inplace=True)\namtrak.stops.sort_index(inplace=True)\n#amtrak = getFeed('http://gtfs.s3.amazonaws.com/la-metro_20101211_0848.zip')\n#amtrak.stop_times.set_index('stop_headsign', inplace=True)\n#amtrak.stop_times.sort_index(inplace=True)\n#amtrak.trips.set_index('route_id', inplace=True)\n#amtrak.trips.sort_index(inplace=True)\n\ndef getJSONs(route,direction):\n  trip_ids = amtrak.trips[(amtrak.trips['route_id'].astype(str) == str(route).replace('%20', ' ')) & (amtrak.trips['direction_id'] == int(direction))]['trip_id']\n  stops = pandas.merge(pandas.DataFrame(trip_ids),amtrak.stop_times)\n#  trip_ids = amtrak.trips.loc[str(route).replace('%20', ' ')]['trip_id']\n#  stops = pandas.merge(pandas.DataFrame(trip_ids),amtrak.stop_times.loc[str(direction).replace('%20',' ')])\n  trains = stops.groupby('trip_id')\n  stop_ids = []\n  for index, train in trains:\n    train = train.sort('stop_sequence')\n    stop_ids = combine(stop_ids,list(train['stop_id']))\n  previousStop = False\n  distances = []\n  stopLocations = []\n  lineStrings = []\n  for stop in stop_ids:\n    if previousStop:\n      currentStop = Point(float(amtrak.stops.loc[stop]['stop_lon']),float(amtrak.stops.loc[stop]['stop_lat']))\n      distanceToPreviousStop = 65.93*currentStop.distance(previousStop)\n      distances.append(distanceToPreviousStop)\n      lineStrings.append(LineString([list(currentStop.coords)[0],list(previousStop.coords)[0]]))\n    previousStop = Point(float(amtrak.stops.loc[stop]['stop_lon']),float(amtrak.stops.loc[stop]['stop_lat']))\n    stopLocations.append(previousStop)\n  times = []\n  for index, train in trains:\n    time = []\n    departureTime = False\n    for stop in stop_ids:\n      if stop in list(train['stop_id']):\n\tif departureTime:\n\t  h, m, s = map(int,list(train[train['stop_id'] == stop]['arrival_time'])[0].split(':'))\n\t  arrivalTime = timedelta(hours=h, minutes=m, seconds=s)\n\t  delta = arrivalTime - departureTime\n\t  time.append(delta.total_seconds()/60/60)\n\th, m, s = map(int,list(train[train['stop_id'] == stop]['departure_time'])[0].split(':'))\n\tdepartureTime = timedelta(hours=h, minutes=m, seconds=s)\n      else:\n\ttime.append(0)\n    times.append(time)\n  stopsGDF = geopandas.GeoDataFrame(stopLocations,stop_ids)\n  stopsGDF.columns = ['geometry']\n  distancesS = pandas.Series(distances)\n  timesDF = pandas.DataFrame(times)\n  timesDF = timesDF.replace(0.0, numpy.nan)\n#  speedS = distancesS / timesDF.max()\n#  speedS = distancesS / timesDF.min()\n  speedS = distancesS / timesDF.mean()\n  speedS = speedS.round(2)\n  speedGDF = geopandas.GeoDataFrame(lineStrings,speedS)\n  speedGDF.columns = ['geometry']\n  return [stopsGDF.to_json(), speedGDF.to_json()]\n\n@route('/<route>/<direction>')\ndef index(route,direction):\n  stops, speeds = getJSONs(route,direction)\n  f = open(expanduser('~/Desktop/py/speeds.tpl'), 'r')\n  return template(f.read(), speeds = speeds, stops = stops)\n  f.close()\n\nrun(host='localhost', port=8080)\n\n#speedDFlables = stop_ids\n#speedDFlables.pop()\n#speedDF = distancesS / timesDF\n#speedDF.columns = speedDFlables \n#speedDFT = speedDF.transpose()\n\n#speedDFT.plot(legend=False)\n#ax = plt.axes()\n#ax.yaxis.grid()\n#plt.show()\n"""
speeds2.py,0,"b""import gpxpy\nfrom os.path import expanduser\nfrom math import sqrt\nfrom shapely.geometry import Point\nimport geopandas\nimport numpy\nfrom matplotlib import pyplot\n\nwith open(expanduser('~/Desktop/2016-06-14 19_13_54.gpx'), 'r') as gpx_file:\n  gpx = gpxpy.parse(gpx_file)\n\ngpx.tracks[0].segments[0].points[0].time\ngpx.tracks[0].segments[0].points[0].latitude\ngpx.tracks[0].segments[0].points[0].longitude\n\nspeeds = []\nfor i in range(0, len(gpx.tracks[0].segments[0].points) - 1):\n  deltaLatitude = gpx.tracks[0].segments[0].points[i].latitude - gpx.tracks[0].segments[0].points[i - 1].latitude\n  deltaLongitude = gpx.tracks[0].segments[0].points[i].longitude - gpx.tracks[0].segments[0].points[i - 1].longitude\n  speed = sqrt(deltaLatitude**2 + deltaLongitude**2) * 69.04799998422561 * 3600 # not sure if this is the right conversion\n  midLatitude = (gpx.tracks[0].segments[0].points[i].latitude + gpx.tracks[0].segments[0].points[i - 1].latitude) / 2 \n  midLongitude = (gpx.tracks[0].segments[0].points[i].longitude + gpx.tracks[0].segments[0].points[i - 1].longitude) /2\n  speeds.append((Point(midLongitude, midLatitude), midLatitude, midLongitude, speed))\n\nspeedsDF = geopandas.GeoDataFrame(speeds, columns=('geometry', 'latitude', 'longitude', 'speed'))\n\n# from http://stackoverflow.com/questions/25526682/functions-to-smooth-a-time-series-with-known-dips\nfiltered = speedsDF.speed.copy()\ndm = speedsDF.speed.rolling(window=20,center=True).median()\ndf = sorted(numpy.abs(speedsDF.speed - dm).dropna(), reverse=True)\ncutoff = df[len(df) // 20] \nfiltered[numpy.abs(speedsDF.speed - dm) > cutoff] = numpy.nan\nfiltered[0] = numpy.nan\nspeedsDF['filtered'] = filtered\nspeedsDF.dropna(subset=['filtered'], inplace=True)\n\nf, ax = pyplot.subplots(2, sharex=True)\nspeedsDF.plot(ax=ax[0], column='filtered', cmap='OrRd', markersize=10, marker='.')\nax[1].plot(speedsDF.longitude, speedsDF.filtered)\npyplot.show()\n"""
splunk.py,0,"b'import socket\nimport splunklib.client as client\nimport splunklib.results as results\nimport re\nimport numpy as np\nimport pandas as pd\nfrom geoip import geolite2\nfrom ipwhois import IPWhois\nimport ipwhois\n\nheaders = [(\'address\',\'user\',\'user2\',\'logontype\',\'time\',\'raw\')]\ndata = []\n\nget_address = lambda x: re.findall(\'(Source Network Address:\\t)(.*?)(\\n)\',x)[0][1]\nget_user = lambda x: re.findall(\'(Security ID:\\t\\t)(.*?)(\\n)\',x)[0][1]\nget_user2 = lambda x: re.findall(\'(Security ID:\\t\\t)(.*?)(\\n)\',x)[1][1]\nget_logontype = lambda x: re.findall(\'(Logon Type:\\t\\t\\t)(.*?)(\\n)\',x)[0][1]\nsearch = ""search user EventCode=4624 | head 10""\n\nservice = client.connect(\n    host=""splunk.example.com"",\n    port=port,\n    username=""username"",\n    password=""password"")\n\nsocket.setdefaulttimeout(None)\n\nresponse = service.jobs.oneshot(search)\nreader = results.ResultsReader(response)\n\nfor result in reader:\n data.append((get_address(result[\'_raw\']),get_user(result[\'_raw\']),get_user2(result[\'_raw\']),get_logontype(result[\'_raw\']),result[\'_time\'],result[\'_raw\']))\n\ndf = pd.DataFrame(data)\n\nfor addr in df[0]:\n try:\n  print socket.gethostbyaddr(addr)\n except socket.herror:\n  print None, None, None\n try: \n  match = geolite2.lookup(addr)\n  print match.country\n except AttributeError:\n  print None\n try:\n  print IPWhois(addr).lookup()\n except ipwhois.ipwhois.IPDefinedError:\n  print None\n\n'"
splunkfile.py,0,"b'import pandas as pd\nimport re\n\t//checked = input[(input[\'_raw\'].str.contains(name, case=False))]\n\nnames = [\'name1\',\'name2\']\n\nfor name in names:\n\tinfile = name + \'.csv\'\n\toutfile = \'success-\' + name + \'.csv\'\n\tinput = pd.read_csv(infile, sep=\',\')\n\tchecked = input\n\tif not checked.empty:\n\t\tsuccess = checked[(checked[\'_raw\'].str.contains(\'EventCode=4624\'))]\n\t\tsuccess.index = range(0,len(success))\n\t\tfailure = checked[(checked[\'_raw\'].str.contains(\'EventCode=4625\'))]\n\t\tfailure.index = range(0,len(failure))\n\t\tget_address = lambda x: re.findall(\'(Source Network Address:\\\\t)(.*?)(\\\\r)\',x)[0][1]\n\t\tsuccess[\'ip\'] = success._raw.apply(get_address)\n\t\tfailure[\'ip\'] = failure._raw.apply(get_address)\n\t\tget_user = lambda x: re.findall(\'(Security ID:\\\\t)(.*?)(\\\\r)\',x)[0][1]\n\t\tsuccess[\'user\'] = success._raw.apply(get_user)\n\t\tget_user2 = lambda x: re.findall(\'(Security ID:\\\\t)(.*?)(\\\\r)\',x)[1][1]\n\t\tsuccess[\'user2\'] = success._raw.apply(get_user2)\n\t\tget_logontype = lambda x: re.findall(\'(Logon Type:\\\\t)(.*?)(\\\\r)\',x)[0][1]\n\t\tsuccess[\'logontype\'] = success._raw.apply(get_logontype)\n\t\tfailure[\'user\'] = failure._raw.apply(get_user)\n\t\tsuccess_csv = pd.DataFrame([success._time,success.ip,success.user,success.user2,success.logontype]).transpose()\n\t\tfailure_csv = pd.DataFrame([failure._time,failure.ip])\n\t\tsuccess_csv.to_csv(outfile)\n\t\tfailure_csv.to_csv(""failure.csv"")\n'"
stocks.py,4,"b'import datetime\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy import stats\nfrom pandas.io.data import DataReader\n\nstock = DataReader(""IVV"", ""yahoo"", start=datetime.datetime(1950, 1, 1))   \n\nregression = sm.OLS(stock[\'Adj Close\'], sm.add_constant(range(len(stock.index)), prepend=True)).fit()\nstock[\'regression\'] = regression.fittedvalues\nm = regression.params.x1\nb = regression.params.const\n\ndef y(x,m,b):\n return m*x+b\n\nyv = np.vectorize(y)\n\ndef v(x):\n return (stock[\'Adj Close\'][x]-stock[\'Adj Close\'][x-1])/stock[\'Adj Close\'][x-1]\n\ndelta = []\nfor x in range(1, len(stock.index)): delta.append(v(x))\n\nplt.plot(stock.index, stock[\'Adj Close\'], \'b-\', stock.index, stock[\'regression\'], \'r-\', stock.index, yv(range(len(stock.index)),np.array(delta).mean()*100,65.54), \'g-\')\n\nplt.show()\n\n\nnp.random.normal(size=200)\n\nvalue=65.54\nvalues=[]\nfor x in range(0,2000):\n value = value + np.array(delta).mean() * 100 * np.random.normal() + np.random.normal()\n values.append(value)\n\nplt.plot(range(0,2000),values)\nplt.show()\n\nplt.plot(stats.binom.cdf(range(0,20),50,.2))\n\nx = numpy.arange(-10, 10, .01)\nfrom scipy.stats import norm\nrv1 = norm(loc = 0., scale = 2.0)\nplt.plot(x,rv1.pdf(x))\nplt.plot(numpy.fft.fft(rv1.pdf(x)))\na = [0,1,1,0]\nplt.plot(numpy.convolve(a,rv1.pdf(x)))\n\nimport math\ndef g(t): return math.exp(-t**2/2)\n\n\n'"
trafficSim.py,0,"b'from math import sqrt\n\nPeople = []\nclass Person:\n def __init__(self,route):\n  self.arrived = False\n  self.travelTime = 0\n  self.route = route\n  self.currentRouteSegment = self.route.pop()\n  self.positionOnCurrentRouteSegment = 0\n  self.width = 2\n  self.length = 2\n def Walk(self):\n  self.travelTime += 1\n  if isinstance(self.currentRouteSegment,FourWayStopSignIntersection):\n   pass\n  elif isinstance(self.currentRouteSegment,FourWaySignalIntersection):\n   pass\n  elif isinstance(self.currentRouteSegment,RoadSegment):\n   self.positionOnCurrentRouteSegment += 3.7\n   if self.positionOnCurrentRouteSegment >= self.currentRouteSegment.length: # we reached the end of the segment and are at a signal\n    if self.currentRouteSegment.signal.color == \'green\': # if green we go\n     if self.route: # more routeSegments in our route\n      self.routePosition = self.routePosition - self.routeSegment.length\n      self.routeSegment = self.route.pop()\n     else:\n      self.arrived = True\n    else: # red/yellow we stop\n     self.routePosition = self.routeSegment.length\n  elif isinstance(self.currentRouteSegment,Crosswalk):\n   pass\n\nclass Vehicle:\n def __init__(self,driver):\n  self.driver = driver\n  self.speed = 0.0\n  self.acceleration = 0.0\n  self.deacceleraton = 0.0\n def Accelerate():\n  self.speed += self.acceleration\n def Deaccelerate():\n  self.speed += self.deacceleraton\n\nclass Bicycle(Vehicle):\n def __init__(self,driver):\n  Vehicle.__init__(self,driver)\n  self.width = 3\n  self.lenght = 6\n\nclass Car(Vehicle):\n def __init__(self,driver):\n  Vehicle.__init__(self,driver)\n\nclass Carpool(Car):\n def __init__(self,driver):\n  Car.__init__(self,driver)\n\nclass Bus(Vehicle):\n pass\n\nclass Train:\n pass\n\nclass Ferry:\n pass\n\nclass Point:\n def __init__(self, x, y):\n  self.x = x\n  self.y = y\n\nSignals = []\nclass Signal: # red = stop, yellow = stop if able, green = go\n def __init__(self,color,greenTime,redTime,yellowTime):\n  Signals.append(self)\n  self.color = color\n  self.greenTime = greenTime\n  self.redTime = redTime\n  self.yellowTime = yellowTime\n def countdown(self):\n  if self.color == \'red\':\n   if self.redTime:\n    self.redTime -= 1\n   else: self.color = \'green\'\n  elif self.color == \'green\':\n   if self.greenTime:\n    self.greenTime -= 1\n   else: self.color = \'yellow\'\n  elif self.color == \'yellow\':\n   if self.yellowTime:\n    self.yellowTime -= 1\n   else: self.color = \'red\'\n\nclass FourWayStopSign: # this direction stop, then yeild, then go\n def __init__(self):\n  pass\n\nclass FourWayStopSignIntersection:\n def __init__(self,nRoadSegment,sRoadSegment,eRoadSegment,wRoadSegment):\n  self.stopSign = FourWayStopSign()\n  self.nRoadSegment = nRoadSegment\n  self.sRoadSegment = sRoadSegment\n  self.eRoadSegment = eRoadSegment\n  self.wRoadSegment = wRoadSegment\n  nQueque = False\n  sQueque = False\n  eQueque = False\n  wQueque = False\n\nclass TwoWayStopSign: # this direction stop, then yeild, then go\n def __init__(self):\n  pass\n\nclass YeildSign: # yeild, thens go\n def __init__(self):\n  pass\n\nclass = Crosswalk: #\n def __init__(self):\n  pedestrianPresent = False\n\nclass FourWaySignal:\n def __init__(self,majorTime,minorTime,yellowTime):\n  self.major = Signal(\'green\',majorTime,minorTime+yellowTime,yellowTime)\n  self.minor = Signal(\'red\',minorTime,majorTime+yellowTime,yellowTime)\n\nclass FourWaySignalIntersection:\n def __init__(self)\n  self.pedestrianSignal1 = FourWaySignal(13,3,12)\n  self.trafficSignal = FourWaySignal(20,10,5)\n  self.pedestrianSignal2 = FourWaySignal(13,3,12)\n\nclass RoadSegment:\n def __init__(self, origin, destination, signal, type):\n  self.origin = origin\n  self.destination = destination\n  self.length = sqrt((self.origin.x+self.destination.x)**2+(self.origin.y+self.destination.y)**2)\n  self.signal = signal\n  self.type = type\n\nclass HalfTwoLaneRoad:\n def __init__(self,end1,end2):\n  self.sidewalk1 = RoadSegment(end1,end2,\'person\')\n  self.sidewalk2 = RoadSegment(end2,end1,\'person\')\n  self.lane = RoadSegment(end1,end2,\'vehicle\')\n\nclass TwoLaneRoad:\n def __init__(self,end1,end2):\n  self.half1 = HalfTwoLaneRoad(end1,end2)\n  self.half2 = HalfTwoLaneRoad(end2,end1)\n\np1 = Point(0,0)\np2 = Point(10,10)\np3 = Point(20,20)\n\nrs1 = RoadSegment(p1,p2,s1)\nrs2 = RoadSegment(p2,p3,s2)\n\nrs3 = RoadSegment(p3,p2,s1)\nrs4 = RoadSegment(p2,p1,s2)\n\nP1 = Person([rs2,rs1])\nP2 = Person([rs4,rs3])\n\nwhile P1.arrived == False:\n for S in Signals:\n  print S.color\n  S.countdown()\n for P in People:\n  print str(P.arrived) + "" "" + str(P.travelTime) + "" "" + str(P.routeSegment) + "" "" + str(P.routePosition) + ""\\n""\n  P.Walk()\n'"
transportationDataBook.py,1,"b'import pandas\nimport psycopg2\nimport sqlite3\nfrom matplotlib import pyplot\nfrom scipy.stats.kde import gaussian_kde\nimport rpy2.robjects as robjects\nimport numpy\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom math import sqrt\nimport datetime\n\nfrom os import system\nsystem(\'ls\')\nsystem(\'pwd\')\n\ndf = pandas.read_csv(""http://web.cecs.pdx.edu/~monserec/t.data/resources/data/route19march07.csv"")\n\npyplot.scatter(df.stop_time,df.est_load)\npyplot.scatter(df.index,df.est_load)\ndf.service_day.value_counts(sort=True,ascending=True).plot(kind=\'bar\')\npyplot.scatter(df.x_coord,df.y_coord)\n\npyplot.show()\n\ndf = pandas.read_csv(""http://web.cecs.pdx.edu/~monserec/t.data/resources/data/wimaug10_14.csv"")\ndf = pandas.read_csv(""http://web.cecs.pdx.edu/~monserec/t.data/resources/data/wimaug8.csv"")\n\ndf[(df[\'type\'] == 11) & (df[\'timestamp\'] >= \'2009-08-08\') & (df[\'timestamp\'] <= \'2009-08-09\')]\ndataToExplore = df[(df[\'type\'] == 11) & (df[\'timestamp\'] >= \'2009-08-08\') & (df[\'timestamp\'] <= \'2009-08-09\')][\'spc2\']\ndataToExplore.mean()\ndataToExplore.median()\ndataToExplore.mode()\ndataToExplore.std()\ndataToExplore.var()\ndataToExplore.skew()\ndataToExplore.kurtosis()\ndataToExplore.value_counts()\ndataToExplore.describe()\npyplot.boxplot(dataToExplore)\n\ndf = pandas.read_csv(""http://web.cecs.pdx.edu/~monserec/t.data/resources/data/incidents.csv"")\npyplot.scatter(df[\'Unnamed: 0\'],df[\'incidenttypeid\'])\ndf[\'incidenttypeid\'].value_counts\n\nfor i in df:\n try:\n  pyplot.close()\n  pyplot.scatter(df[\'Unnamed: 0\'],df[i])\n  pyplot.savefig(\'figure-\' + i + \'.png\')\n except:\n  pass\n\ndf = pandas.read_csv(""http://web.cecs.pdx.edu/~monserec/t.data/resources/data/wimaug8.csv"")\n\ndf[\'axl1\'].value_counts()\n\nx = numpy.linspace(0,20,1000)\npyplot.plot(x,gaussian_kde(df[\'axl1\'])(x))\npyplot.hist(df[\'axl1\'],normed=1,alpha=.3)\n\nrvector = robjects.IntVector(np.array(df[\'axl1\']))\nrhist = robjects.r[\'hist\']\nr.X11()\nrhist = robjects.r[\'hist\']\n\nx = numpy.arange(-10, 10, .01)\nrv1 = stats.norm(loc = 0., scale = 2.0)\nrv2 = stats.norm(loc = 1., scale = 2.0)\nrv3 = stats.norm(loc = 1., scale = 1.0)\nrv4 = stats.norm(loc = 0., scale = 1.0)\npyplot.plot(x,rv1.pdf(x))\npyplot.plot(x,rv2.pdf(x))\npyplot.plot(x,rv3.pdf(x))\npyplot.plot(x,rv4.pdf(x))\npyplot.plot(x,rv1.cdf(x))\n\nqqdata = numpy.random.normal(0,1, 1000)\nsm.qqplot(qqdata, line=\'s\')\n\nb = pandas.read_csv(""http://web.cecs.pdx.edu/~monserec/t.data/resources/data/bicycle.csv"")\nset1 = pandas.Series(b[(b[\'grade\'] == 0)][\'avgspeed\'].tolist())\nset2 = pandas.Series(b[(b[\'grade\'] == 1)][\'avgspeed\'].tolist())\n\nsummary = []\nfor set in [set1,set2]:\n summary.append((set.mean(),set.var(),set.std(),set.skew(),set.kurtosis()))\n\nsummaryDF = pandas.DataFrame(summary,index=[\'Grade\',\'No Grade\'],columns=[\'mean\',\'var\',\'std\',\'skew\',\'kurtosis\'])\n\nx = numpy.linspace(0,10,1000)\nfor (set, color) in zip([set1,set2],[\'red\',\'blue\']):\n pyplot.plot(x,stats.gaussian_kde(set)(x),color=color)\n pyplot.axvline(x=set.mean(),dashes=[10,10],color=color)\n\nfor (set, color) in zip([set1,set2],[\'red\',\'blue\']):\n ecdf = sm.distributions.ECDF(set)\n pyplot.plot(ecdf.x,ecdf.y,color=color)\n\npyplot.boxplot([set1,set2])\n\nsm.qqplot(set1, line=\'s\')\nsm.qqplot(set2, line=\'s\')\n\nstat = stats.norm.ppf(.8)\nerror1 = stat*set1.std()/sqrt(len(set1))\nerror2 = stat*set2.std()/sqrt(len(set2))\n\npyplot.scatter(1,set1.mean(),color=\'blue\')\npyplot.scatter(1,set1.mean()+error1,marker=\'2\',color=\'blue\',s=100)\npyplot.scatter(1,set1.mean()-error1,marker=\'1\',color=\'blue\',s=100)\npyplot.axhline(y=set1.mean()-error1,dashes=[10,10],color=\'blue\')\npyplot.scatter(2,set2.mean(),color=\'red\')\npyplot.scatter(2,set2.mean()+error2,marker=\'2\',color=\'red\',s=100)\npyplot.scatter(2,set2.mean()-error2,marker=\'1\',color=\'red\',s=100)\npyplot.axhline(y=set2.mean()+error2,dashes=[10,10],color=\'red\')\n\nstats.ttest_ind(set1,set2)\nstats.ttest_ind(set1,set2,equal_var=False)\n\ndf = pandas.read_csv(""http://web.cecs.pdx.edu/~monserec/t.data/resources/data/incidents.csv"")\ndf[\'dur\'] = df[\'duration\'].map(lambda x: datetime.datetime.strptime(x, \'%H:%M:%S\')-datetime.datetime(1900,1,1))\n\nnolane = df[(df[\'numlanesaffected\'] == 0)]\nonelane = df[(df[\'numlanesaffected\'] == 1)]\ntwolane = df[(df[\'numlanesaffected\'] == 2)]\nmorelane = df[(df[\'numlanesaffected\'] >= 3)]\n\npyplot.boxplot([(nolane[\'dur\']  / numpy.timedelta64(1, \'s\')).tolist(),(onelane[\'dur\']  / numpy.timedelta64(1, \'s\')).tolist(),(twolane[\'dur\']  / numpy.timedelta64(1, \'s\')).tolist(),(morelane[\'dur\']  / numpy.timedelta64(1, \'s\')).tolist()])\n\nstats.ttest_ind(nolane[\'dur\']  / numpy.timedelta64(1, \'s\'),onelane[\'dur\']  / numpy.timedelta64(1, \'s\'))\nstats.ttest_ind(nolane[\'dur\']  / numpy.timedelta64(1, \'s\'),twolane[\'dur\']  / numpy.timedelta64(1, \'s\'))\nstats.ttest_ind(nolane[\'dur\']  / numpy.timedelta64(1, \'s\'),morelane[\'dur\']  / numpy.timedelta64(1, \'s\'))\nstats.ttest_ind(onelane[\'dur\']  / numpy.timedelta64(1, \'s\'),twolane[\'dur\']  / numpy.timedelta64(1, \'s\'))\nstats.ttest_ind(onelane[\'dur\']  / numpy.timedelta64(1, \'s\'),morelane[\'dur\']  / numpy.timedelta64(1, \'s\'))\nstats.ttest_ind(twolane[\'dur\']  / numpy.timedelta64(1, \'s\'),morelane[\'dur\']  / numpy.timedelta64(1, \'s\'))\n\nstats.f_oneway(nolane[\'dur\']  / numpy.timedelta64(1, \'s\'),onelane[\'dur\']  / numpy.timedelta64(1, \'s\'))\nstats.f_oneway(nolane[\'dur\']  / numpy.timedelta64(1, \'s\'),twolane[\'dur\']  / numpy.timedelta64(1, \'s\'))\nstats.f_oneway(nolane[\'dur\']  / numpy.timedelta64(1, \'s\'),morelane[\'dur\']  / numpy.timedelta64(1, \'s\'))\nstats.f_oneway(onelane[\'dur\']  / numpy.timedelta64(1, \'s\'),twolane[\'dur\']  / numpy.timedelta64(1, \'s\'))\nstats.f_oneway(onelane[\'dur\']  / numpy.timedelta64(1, \'s\'),morelane[\'dur\']  / numpy.timedelta64(1, \'s\'))\nstats.f_oneway(twolane[\'dur\']  / numpy.timedelta64(1, \'s\'),morelane[\'dur\']  / numpy.timedelta64(1, \'s\'))\n\n# big difference between 0 and 1 or 2 lanes affected. not a big differnece between 1,2,3+ lanes affected\n'"
usc.py,0,"b'from os.path import expanduser\nimport pandas\nimport matplotlib.pyplot as plt\nimport numpy\n\ndf = pandas.DataFrame.from_csv(expanduser(\'~/Desktop/usc.csv\'), index_col=False)\n\ndf.Country.value_counts() # total by country\n\nfor place in [\'Gold\', \'Silver\', \'Bronze\']: # g,s,b by sport\n  print \'\\n\\n\', place, \'\\n\'\n  df[df[\'Place\'] == place][\'Sport\'].value_counts()\n\ndf.Year.value_counts() # total per year\n\n#df.Year.value_counts().sort_index().plot()\n#plt.show()\n\ndef getColor(country):\n  if country == ""United States"":\n    return numpy.array((153, 0, 0)) / 255.0\n  if country == ""Great Britain"":\n    country = \'United Kingdom\'\n  if country == \'West Germany\':\n    country = \'Germany\'\n  if country in df.Country.value_counts():\n    return numpy.array((255, 204, 0)) / 255.0\n  else:\n    return numpy.array((255,255,255)) / 255.0\n\nfrom cartopy import crs\nfrom cartopy.io import shapereader\ncountries = shapereader.natural_earth(resolution=\'110m\', category=\'cultural\', name=\'admin_0_countries\')\nax = plt.axes(projection=crs.PlateCarree())\n\nfor country in shapereader.Reader(countries).records():\n  ax.add_geometries(country.geometry, crs.PlateCarree(), facecolor=getColor(country.attributes[\'name_long\']))\n\nplt.show()\n'"
walkbike2015.py,0,"b'import requests\nimport pandas\nimport geopandas\n\nmodes = {\'Total\': \'B08006_001E\', \'Car, truck, or van\': \'B08006_002E\', \'Drove alone\': \'B08006_003E\', \'Carpooled\': \'B08006_004E\', \'In 2-person carpool\': \'B08006_005E\', \'In 3-person carpool\': \'B08006_006E\', \'In 4-o    r-more-person carpool\': \'B08006_007E\', \'Public transportation (excluding taxicab)\': \'B08006_008E\', \'Bus or trolley bus\': \'B08006_009E\', \'Streetcar or trolley car (carro publico in Puerto Rico)\': \'B08006_010E\', \'    Subway or elevated\': \'B08006_011E\', \'Railroad\': \'B08006_012E\', \'Ferryboat\': \'B08006_013E\', \'Bicycle\': \'B08006_014E\', \'Walked\': \'B08006_015E\', \'Taxicab, motorcycle, or other means\': \'B08006_016E\', \'Worked at home    \': \'B08006_017E\'}\nurl = \'http://api.census.gov/data/2015/acs5?get=B00001_001E,\' + \',\'.join(modes.values()) + \'&for=zip+code+tabulation+area:*\'\nr = requests.get(url)\ndf = pandas.DataFrame(r.json())\ndf.columns = [\'B00001_001E\'] + list(modes) + [\'zip code tabulation area\']\ndf = df.drop(0)\n\ngdf = geopandas.GeoDataFrame.from_file(\'zcta5.geo.json\') # from https://github.com/jgoodall/us-maps\ncdf = pandas.merge(gdf, df, left_on=\'ZCTA5CE10\', right_on=\'zip code tabulation area\')\ncdf = cdf.convert_objects(convert_numeric=True)\n\ncdf[\'mode\'] = cdf[\'Walked\'] * 100 / cdf[\'Total\']\nbdf = cdf[[\'mode\',\'geometry\']]\nbdf = bdf[bdf[\'mode\'] > 5]\nsimplify = bdf[\'geometry\'].simplify(100)\nbdf[\'geometry\'] = simplify\n\nbdf.to_file(\'walked.js\', driver=\'GeoJSON\') # after this, add ""var walked = "" to the beginning of this file.\n'"
walkingcycling.py,0,"b'import pandas\nimport re\nimport os\nimport geopandas\nimport requests\nfrom matplotlib import pyplot\nimport matplotlib\nfrom biokit.viz import corrplot\nfrom ftplib import FTP\nimport zipfile\nimport io\nimport pandas\nfrom dbfread import DBF\n\n# get mode share census data for one year from https://www.census.gov/data/developers/data-sets/acs-5year.html\nmodes = {\'Total\': \'B08006_001E\', \'Car, truck, or van\': \'B08006_002E\', \'Drove alone\': \'B08006_003E\', \'Carpooled\': \'B08006_004E\', \'In 2-person carpool\': \'B08006_005E\', \'In 3-person carpool\': \'B08006_006E\', \'In 4-or-more-person carpool\': \'B08006_007E\', \'Public transportation (excluding taxicab)\': \'B08006_008E\', \'Bus or trolley bus\': \'B08006_009E\', \'Streetcar or trolley car (carro publico in Puerto Rico)\': \'B08006_010E\', \'Subway or elevated\': \'B08006_011E\', \'Railroad\': \'B08006_012E\', \'Ferryboat\': \'B08006_013E\', \'Bicycle\': \'B08006_014E\', \'Walked\': \'B08006_015E\', \'Taxicab, motorcycle, or other means\': \'B08006_016E\', \'Worked at home\': \'B08006_017E\'}\nurl = \'http://api.census.gov/data/2015/acs5?get=NAME,B01001_001E,\' + \',\'.join(modes.values()) + \'&for=county:*&in=state:06\'\nr = requests.get(url)\ndf = pandas.DataFrame(r.json())\ndf.columns = [\'NAME\',\'Population\'] + list(modes) + [\'state\', \'county\'] # or df.iloc[0]\ndf = df.drop(0)\nfor mode in modes.keys():\n  df[mode + \' Rate\'] = df[mode].apply(int) / df[\'Total\'].apply(int) * 100\n\n# get collision information from chp swirts\nco = pandas.read_csv(\'CollisionRecords.txt\')\ninjuries = {\'NUMBER_KILLED\': \'Total\', \'COUNT_PED_KILLED\': \'Walked\', \'COUNT_PED_INJURED\': \'Walked\', \'COUNT_BICYCLIST_KILLED\': \'Bicycle\', \'COUNT_BICYCLIST_INJURED\': \'Bicycle\', \'COUNT_MC_KILLED\': \'Taxicab, motorcycle, or other means\', \'COUNT_MC_INJURED\': \'Taxicab, motorcycle, or other means\'}\nfor injury in list(injuries):\n  df[injury] = df.index.map(lambda county: co[(int(county) * 100 <= co[\'CNTY_CITY_LOC\']) & (co[\'CNTY_CITY_LOC\'] < int(county) * 100 + 100)][injury].sum())\n\nfor injury, mode in injuries.items():\n  df[injury + \' Rate\'] = df[injury].apply(int) / df[mode].apply(int) * 100\n\ndf[[\'NAME\',\'Bicycle\',\'Bicycle Rate\',\'COUNT_BICYCLIST_KILLED\',\'COUNT_BICYCLIST_KILLED Rate\']].sort_values([\'Bicycle Rate\'], ascending=False)\n\ndft = df.convert_objects(convert_numeric=True)\n# cor = list(set(injuries.values())) + list(injuries)\ncor = list(modes) + list(injuries)\nb = []\nfor a in cor:\n  b.append(a + \' Rate\') \nc = corrplot.Corrplot(dft[b])\nmatplotlib.rcParams.update({\'font.size\': 8})\nc.plot()\npyplot.savefig(\'/Users/david/Desktop/fig.svg\')\n# pyplot.show()\n\n# df.to_csv(\'modes.csv\')\n\npyplot.scatter(df[\'Bicycle Rate\'],df[\'COUNT_BICYCLIST_KILLED Rate\'])\npyplot.show()\n\nfor county in df.index:\n  print(df[\'NAME\'][county])\n  for injury in injuries:\n    print(injury)\n    co[(int(county) * 100 <= co[\'CNTY_CITY_LOC\']) & (co[\'CNTY_CITY_LOC\'] < int(county) * 100 + 100)][injury].sum()\n\ngdf = geopandas.GeoDataFrame.from_file(\'/Users/david/Desktop/maps/tl_2010_06_county10/tl_2010_06_county10.shp\')\ndf[\'NAMELSAD10\'] = df.NAME.map(lambda county: county.split(\',\')[0])\ndft = df.convert_objects(convert_numeric=True)\ngdf = geopandas.GeoDataFrame(pandas.merge(dft,gdf))\ngdf[\'COUNT_BICYCLIST_KILLED Rate\'][1] = 25\ngdf.plot(column=\'COUNT_BICYCLIST_KILLED Rate\', cmap=\'OrRd\')\ngdf.plot(column=\'Bicycle Rate\', cmap=\'OrRd\')\npyplot.show()\n\n# get mode share census data for one year from the past few years from https://www.census.gov/data/developers/data-sets/acs-5year.html\nmodes = {\'Total\': \'B08006_001E\', \'Car, truck, or van\': \'B08006_002E\', \'Drove alone\': \'B08006_003E\', \'Carpooled\': \'B08006_004E\', \'In 2-person carpool\': \'B08006_005E\', \'In 3-person carpool\': \'B08006_006E\', \'In 4-or-more-person carpool\': \'B08006_007E\', \'Public transportation (excluding taxicab)\': \'B08006_008E\', \'Bus or trolley bus\': \'B08006_009E\', \'Streetcar or trolley car (carro publico in Puerto Rico)\': \'B08006_010E\', \'Subway or elevated\': \'B08006_011E\', \'Railroad\': \'B08006_012E\', \'Ferryboat\': \'B08006_013E\', \'Bicycle\': \'B08006_014E\', \'Walked\': \'B08006_015E\', \'Taxicab, motorcycle, or other means\': \'B08006_016E\', \'Worked at home\': \'B08006_017E\'}\nyears = range(2010,2016)\ndfs = pandas.DataFrame(columns = [\'NAME\',\'Population\'] + list(modes) + [\'state\', \'county\', \'Year\'])\nfor year in years:\n  url = \'http://api.census.gov/data/\' + str(year) + \'/acs5?get=NAME,B01001_001E,\' + \',\'.join(modes.values()) + \'&for=county:*&in=state:06\'\n  r = requests.get(url)\n  df = pandas.DataFrame(r.json())\n  df.columns = [\'NAME\',\'Population\'] + list(modes) + [\'state\', \'county\'] # or df.iloc[0]\n  df = df.drop(0)\n  for mode in modes.keys():\n    df[mode + \' Rate\'] = df[mode].apply(int) / df[\'Total\'].apply(int) * 100\n  df[\'Year\'] = year\n  dfs = pandas.merge(dfs, df, how=\'outer\')\n\ndf = dfs\n\n# get collision data from chp swirts\nco = pandas.read_csv(\'CollisionRecords.txt\')\ninjuries = {\'NUMBER_KILLED\': \'Total\', \'COUNT_PED_KILLED\': \'Walked\', \'COUNT_PED_INJURED\': \'Walked\', \'COUNT_BICYCLIST_KILLED\': \'Bicycle\', \'COUNT_BICYCLIST_INJURED\': \'Bicycle\', \'COUNT_MC_KILLED\': \'Taxicab, motorcycle, or other means\', \'COUNT_MC_INJURED\': \'Taxicab, motorcycle, or other means\'}\nfor injury in list(injuries):\n  df[injury] = df.apply(lambda row: co[((int(row[\'county\']) + 1) / 2* 100 <= co[\'CNTY_CITY_LOC\']) & (co[\'CNTY_CITY_LOC\'] < (int(row[\'county\']) + 1) / 2 * 100 + 100) & (row[\'Year\'] == co[\'ACCIDENT_YEAR\'])][injury].sum(), axis=1)\n\nfor injury, mode in injuries.items():\n  df[injury + \' Rate\'] = df[injury].apply(int) / df[mode].apply(int) * 100\n\ndf[[\'NAME\',\'Year\',\'Bicycle\',\'Bicycle Rate\',\'COUNT_BICYCLIST_KILLED\',\'COUNT_BICYCLIST_KILLED Rate\']].sort_values([\'Bicycle Rate\'], ascending=False)\n\ndft = df.convert_objects(convert_numeric=True)\n# cor = list(set(injuries.values())) + list(injuries)\ncor = list(modes) + list(injuries)\nb = []\nfor a in cor:\n  b.append(a + \' Rate\') \nc = corrplot.Corrplot(dft[b])\nmatplotlib.rcParams.update({\'font.size\': 8})\nc.plot()\npyplot.savefig(\'fig.svg\')\n# pyplot.show()\n\n# df.to_csv(\'modes.csv\')\n\n\npyplot.scatter(df[df[\'Year\'] == 2015][\'Bicycle Rate\'], df[df[\'Year\'] == 2015][\'COUNT_BICYCLIST_KILLED Rate\'].fillna(100), color=\'g\')\npyplot.scatter(df[df[\'Year\'] == 2015][\'Walked Rate\'], df[df[\'Year\'] == 2015][\'COUNT_PED_KILLED Rate\'].fillna(100), color=\'b\')\n#pyplot.scatter(df[\'Bicycle Rate\'], df[\'COUNT_BICYCLIST_KILLED Rate\'].fillna(100), color=\'g\')\n#pyplot.scatter(df[\'Walked Rate\'], df[\'COUNT_PED_KILLED Rate\'].fillna(100), color=\'b\')\nax = pyplot.gca()\nax.set_title(\'California Bicycle and Pedestrian Fatality Rate\\nby Commute Rate per County(2015)\')\nax.set_ylabel(\'Rate of Bicycle and Pedestrian Fatalities\')\nax.set_xlabel(\'Rate of Bicycle and Pedestrian Commuters\')\n#pyplot.show(block=False)\n#pyplot.tight_layout()\npyplot.savefig(\'plot.png\')\n\nfor county in df.index:\n  print(df[\'NAME\'][county])\n  for injury in injuries:\n    print(injury)\n    co[(int(county) * 100 <= co[\'CNTY_CITY_LOC\']) & (co[\'CNTY_CITY_LOC\'] < int(county) * 100 + 100)][injury].sum()\n\n# plot on a map of california\ngdf = geopandas.GeoDataFrame.from_file(\'tl_2010_06_county10/tl_2010_06_county10.shp\')\ndf[\'NAMELSAD10\'] = df.NAME.map(lambda county: county.split(\',\')[0])\ndft = df.convert_objects(convert_numeric=True)\ngdf = geopandas.GeoDataFrame(pandas.merge(dft,gdf))\ngdf[\'COUNT_BICYCLIST_KILLED Rate\'][1] = 25\ngdf.plot(column=\'COUNT_BICYCLIST_KILLED Rate\', cmap=\'OrRd\')\ngdf.plot(column=\'Bicycle Rate\', cmap=\'OrRd\')\npyplot.show()\n\n\'\'\' fatalities based on collision data\nfatality_count = co.groupby(\'ACCIDENT_YEAR\').sum()[[\'COUNT_BICYCLIST_KILLED\',\'COUNT_PED_KILLED\',\'COUNT_MC_KILLED\',\'NUMBER_KILLED\']]\nmotorist_killed = lambda x: x[\'NUMBER_KILLED\'] - x[\'COUNT_BICYCLIST_KILLED\'] - x[\'COUNT_PED_KILLED\'] - x[\'COUNT_MC_KILLED\']\nfatality_count[\'MOTORIST_KILLED\'] = fatality_count.apply(motorist_killed, axis=1)\n\'\'\'\n\nvi = pandas.read_csv(\'VictimRecords.txt\')\nvico = pandas.merge(vi, co, how=\'left\')\nvi = vi.convert_objects(convert_numeric=True)\nfatalsevere = vico[(vico[\'VICTIM_DEGREE_OF_INJURY\'] == 1) | (vico[\'VICTIM_DEGREE_OF_INJURY\'] == 2)].groupby([\'ACCIDENT_YEAR\', \'VICTIM_ROLE\']).count()[\'CASE_ID\']\nfatalsevere.index.levels = [[2010, 2011, 2012, 2013, 2014, 2015], [\'Driver\', \'Passenger\', \'Pedestrian\', \'Bicyclist\', \'Other\']]\nfatalsevere.index.names = [\'Year\', \'Victims by Mode \']\n\nmodeshare = dft.groupby(\'Year\').sum()[[\'Total\', \'Car, truck, or van\', \'Walked\', \'Bicycle\']]\nmodeshare[\'Other\'] = modeshare[\'Total\'] - modeshare[\'Walked\'] - modeshare[\'Bicycle\'] - modeshare[\'Car, truck, or van\'] \nmodeshare.drop(\'Total\', axis=1, inplace=True)\nmodeshare.columns = [\'Car, truck, or van\', \'Walk\', \'Bicycle\', \'Other\']\nmodeshare = modeshare.stack()\nmodeshare.index.names = [\'Year\', \'Commuters by Mode\']\n\ncolors = [\'#AC7BE8\', \'#5CF24C\', \'#DB5531\', \'#4BA9F2\', \'#E8D342\']\nmodeshare.unstack().plot.area(color = [colors[i] for i in [0,2,3,4]])\nax = pyplot.gca()\nax.get_xaxis().get_major_formatter().set_useOffset(False)\nax.get_yaxis().get_major_formatter().set_scientific(False)\nax.set_title(\'California Commuters by Mode (2010-2015)\')\nax.set_ylabel(\'People\')\nfatalsevere.unstack().plot.area(color = colors)\nax = pyplot.gca()\nax.get_xaxis().get_major_formatter().set_useOffset(False)\nax.set_title(\'California Traffic Victims by Mode (2010-2015)\')\nax.set_ylabel(\'People\')\npyplot.show(block=False)\n\nfatalsevere.loc[2015]/fatalsevere.loc[2015].sum()*100\nmodeshare.loc[2015]/modeshare.loc[2015].sum()*100\n\ndf = dfs\ninjuries = {3.0: \'Walked\', 4.0: \'Bicycle\'}\nfor injury in list(injuries):\n  df[injury] = df.apply(lambda row: vico[((int(row[\'county\']) + 1) / 2 * 100 <= vico[\'CNTY_CITY_LOC\']) & (vico[\'CNTY_CITY_LOC\'] < (int(row[\'county\']) + 1) / 2 * 100 + 100) & (row[\'Year\'] == vico[\'ACCIDENT_YEAR\']) & ((vico[\'VICTIM_DEGREE_OF_INJURY\'] == 1) | (vico[\'VICTIM_DEGREE_OF_INJURY\'] == 2)) & (vico[\'VICTIM_ROLE\'] == injury)][\'CASE_ID\'].count(), axis=1)\n\nfor injury, mode in injuries.items():\n  df[str(injury) + \' Rate\'] = df[injury].apply(int) / df[mode].apply(int) * 100\n\ndf[df[\'Year\'] == 2015][[\'NAME\',\'Year\',\'Walked\',\'Walked Rate\',3.0,\'3.0 Rate\']].sort_values([\'3.0 Rate\'], ascending=False)\ndf[df[\'Year\'] == 2015][[\'NAME\',\'Year\',\'Bicycle\',\'Bicycle Rate\',4.0,\'4.0 Rate\']].sort_values([\'4.0 Rate\'], ascending=False)\n\npyplot.scatter(df[df[\'Year\'] == 2015][\'Walked Rate\'], df[df[\'Year\'] == 2015][\'3.0 Rate\'].fillna(100), color=\'b\')\npyplot.scatter(df[df[\'Year\'] == 2015][\'Bicycle Rate\'], df[df[\'Year\'] == 2015][\'4.0 Rate\'].fillna(100), color=\'g\')\nax = pyplot.gca()\nax.set_title(\'California Bicycle and Pedestrian Fatality and Severe Injury Rate\\nby Commute Rate per County(2015)\')\nax.set_ylabel(\'Rate of Bicycle and Pedestrian Fatalities and Severe Injuries\')\nax.set_xlabel(\'Rate of Bicycle and Pedestrian Commuters\')\npyplot.show(block=False)\n#pyplot.tight_layout()\n#pyplot.savefig(\'plot.png\')\n\nfrom ftplib import FTP\nimport zipfile\nimport io\nimport pandas\nfrom dbfread import DBF\n\n# this bit doesn\'t work. need na update to the dbf package\ndef ftp2dataframe(url):\nserver = url.split(\'/\')[2]\nfile = \'/\'.join(url.split(\'/\')[3:])\ndata = io.BytesIO()\nwith FTP(server) as ftp:\n  ftp.login()\n  ftp.retrbinary(\'RETR \' + file, data.write)\nzipFile = zipfile.ZipFile(data)\ndfs = dict(zip(zipFile.namelist(),list(range(len(zipFile.namelist())))))\nfor name in zipFile.namelist():\n  dbf_file = io.BytesIO(zipFile.open(name).read())\n  dfs[name] = dbf2dataframe(dbf_file)\nreturn dfs\n\ndef dbf2dataframe(dbf_file):\n  dbf_list = []\n  columns = []\n  dbf_dbf = DBF(dbf_file)\n  for row in dbf_dbf:\n    columns = row.keys()\n    dbf_list.append(row.values())\n  return pandas.DataFrame(dbf_list, columns = columns)\n\nurl = \'ftp://ftp.nhtsa.dot.gov/fars/2015/National/FARS2015NationalDBF.zip\'\ndata = ftp2dataframe(url)\ndata = dbf2dataframe(\'person.dbf\')\n\nstates = {\n1: ""Alabama"",\n2: ""Alaska"",\n4: ""Arizona"",\n5: ""Arkansas"",\n6: ""California"",\n8: ""Colorado"",\n9: ""Connecticut"",\n10: ""Delaware"",\n11: ""District of Columbia"",\n12: ""Florida"",\n13: ""Georgia"",\n15: ""Hawaii"",\n16: ""Idaho"",\n17: ""Illinois"",\n18: ""Indiana"",\n19: ""Iowa"",\n20: ""Kansas"",\n21: ""Kentucky"",\n22: ""Louisiana"",\n23: ""Maine"",\n24: ""Maryland"",\n25: ""Massachusetts"",\n26: ""Michigan"",\n27: ""Minnesota"",\n28: ""Mississippi"",\n29: ""Missouri"",\n30: ""Montana"",\n31: ""Nebraska"",\n32: ""Nevada"",\n33: ""New Hampshire"",\n34: ""New Jersey"",\n35: ""New Mexico"",\n36: ""New York"",\n37: ""North Carolina"",\n38: ""North Dakota"",\n39: ""Ohio"",\n40: ""Oklahoma"",\n41: ""Oregon"",\n42: ""Pennsylvania"",\n43: ""Puerto Rico"",\n44: ""Rhode Island"",\n45: ""South Carolina"",\n46: ""South Dakota"",\n47: ""Tennessee"",\n48: ""Texas"",\n49: ""Utah"",\n50: ""Vermont"",\n52: ""Virgin Islands"",\n51: ""Virginia"",\n53: ""Washington"",\n54: ""West Virginia"",\n55: ""Wisconsin"",\n56: ""Wyoming""\n}\n\nperson_types = { 1: \'Driver\', 2: \'Passenger\', 3: \'Passenger\', 4: \'Other\', 5: \'Pedestrian\', 6: \'Bicyclist\', 7: \'Bicyclist\', 8: \'Other\', 9: \'Passenger\', 10: \'Other\' }\n\ndf = dfs[dfs[\'Year\'] == 2015]\n\ninjuries = {\'Bicyclist\': \'Bicycle\', \'Pedestrian\': \'Walked\'}\n#p = data[\'person.dbf\'].replace({\'STATE\': states, \'PER_TYP\': person_types})\np = data.replace({\'STATE\': states, \'PER_TYP\': person_types})\np_people = p.groupby([\'STATE\', \'PER_TYP\']).count()[\'PER_NO\'].unstack()\np_people[\'NAME\'] = p_people.index\npdf = pandas.merge(p_people, df)\npdf.fillna(0, inplace=True)\n\nfor injury, mode in injuries.items():\n  pdf[injury + \' Rate\'] = pdf[injury].apply(int) / pdf[mode].apply(int) * 100\n\npdf[[\'Bicycle Rate\',\'Bicyclist Rate\',\'NAME\']].sort(\'Bicyclist Rate\')\npdf[[\'Walked Rate\',\'Pedestrian Rate\',\'NAME\']].sort(\'Pedestrian Rate\')\n\npyplot.scatter(list(pdf[\'Walked Rate\']), list(pdf[\'Pedestrian Rate\']), color=\'b\')\npyplot.scatter(list(pdf[\'Bicycle Rate\']), list(pdf[\'Bicyclist Rate\']), color=\'g\')\nax = pyplot.gca()\nax.set_title(\'Bicycle and Pedestrian Fatalities \\nby Commute Rate per State (2015)\')\nax.set_ylabel(\'Rate of Bicycle and Pedestrian Fatalities\')\nax.set_xlabel(\'Rate of Bicycle and Pedestrian Commuters\')\npyplot.savefig(\'usrates.png\')\npyplot.show(block=False)\n\nmodes = {\'Total\': \'B08006_001E\', \'Car, truck, or van\': \'B08006_002E\', \'Drove alone\': \'B08006_003E\', \'Carpooled\': \'B08006_004E\', \'In 2-person carpool\': \'B08006_005E\', \'In 3-person carpool\': \'B08006_006E\', \'In 4-or-more-person carpool\': \'B08006_007E\', \'Public transportation (excluding taxicab)\': \'B08006_008E\', \'Bus or trolley bus\': \'B08006_009E\', \'Streetcar or trolley car (carro publico in Puerto Rico)\': \'B08006_010E\', \'Subway or elevated\': \'B08006_011E\', \'Railroad\': \'B08006_012E\', \'Ferryboat\': \'B08006_013E\', \'Bicycle\': \'B08006_014E\', \'Walked\': \'B08006_015E\', \'Taxicab, motorcycle, or other means\': \'B08006_016E\', \'Worked at home\': \'B08006_017E\'}\nyears = range(2015,2016)\ndfs = pandas.DataFrame(columns = [\'NAME\',\'Population\'] + list(modes) + [\'state\', \'Year\'])\nfor year in years:\n  url = \'http://api.census.gov/data/\' + str(year) + \'/acs5?get=NAME,B01001_001E,\' + \',\'.join(modes.values()) + \'&for=state:*\'\n  r = requests.get(url)\n  df = pandas.DataFrame(r.json())\n  df.columns = [\'NAME\',\'Population\'] + list(modes) + [\'state\'] # or df.iloc[0]\n  df = df.drop(0)\n  for mode in modes.keys():\n    df[mode + \' Rate\'] = df[mode].apply(int) / df[\'Total\'].apply(int) * 100\n  df[\'Year\'] = year\n  dfs = pandas.merge(dfs, df, how=\'outer\')\n\ndf = dfs\n'"
