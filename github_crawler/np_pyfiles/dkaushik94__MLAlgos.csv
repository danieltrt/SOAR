file_path,api_count,code
LinRegGradDesc/gradientDescent.py,11,"b'\n""""""Regression using Linear Gradient Descent using two variables.""""""\n\n# Author: Debojit Kaushik(8th May 2017)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n\n\n\'\'\'\n    Cost function calculator. Parameters : slope and constant of the regression line \n    ie. the parameters for the hypotheses.\n    min(J(x)) = 1/(m*2) (Summation from i =0 to class size) d/d(theta)(h(x)-y).\n\'\'\'\ndef costFunction(theta0, theta1, mode, x, y, setSize):\n    temp = np.float64(0)\n    if mode and mode is True:\n        try:\n            for it, item in enumerate(x):\n                temp += (theta0 + theta1*x[it] - y[it])\n            return temp\n        except Exception as e:\n            print(e)\n            raise Exception(""Cost function failed."")\n    elif not mode and mode is False:\n        try:\n            for it, item in enumerate(x):\n                temp += (theta0 + theta1*x[it] - y[it])*x[it]\n            return temp\n        except Exception:\n            print(e)\n            raise Exception(""Cost function failed."")\n    else:\n        raise ValueError(""No Valid \'mode\' parameter select.Eg: mode = True or False"")\n\n\n\n\'\'\'\n    Convergence condition:\n    If current slop - previous slope is < 0.000001, then state convergence is true, and stop iterations.\n\'\'\'\ndef convergence(prevTheta1, currTheta1):\n    \'\'\'\n        Method to check for convergence condition. \n    \'\'\'\n    prevTheta1 = abs(prevTheta1)\n    currTheta1 = abs(currTheta1)\n    conv = max(prevTheta1,currTheta1) - min(prevTheta1, currTheta1)\n\n    if conv<0.000001:\n        return True\n    else:\n        return False\n\n\'\'\'\n    Line plotting before and after.\n\'\'\'\ndef regressionLine(slope, intercept, plot, i, color=None):\n    x = np.arange(i)\n    y = []\n    for point in range(i):\n        temp = slope*x[point] + intercept\n        y.append(temp)\n    if color is None:\n        plot.plot(x,y)\n    else:\n        plot.plot(x,y,color=color)\n\n\n\n\'\'\'\n    Error function for hypotheses.\n\'\'\'\ndef errorValue(x, y, theta0, theta1, setSize, it, plot):\n    Error = 0\n    try:\n        for i in range(setSize):\n            Error += (y[i] - (theta1*x[i] + theta0)) ** 2\n        err = Error / setSize\n        plot.plot(err, it, \'go\')\n    except Exception as e:\n        print(e)\n\n\n\n\n\'\'\'\n    Gradient descent for local minima.\n    Iterate and compute theta0, theta1 until convergence condition is satisfied.\n\'\'\'\ndef gradientDescent(price, area, plot):\n    try:\n        regressionParameters = []\n        theta0 = np.float64(0)\n        theta1 = np.float64(0)\n        m = len(price)\n        alpha = 0.0001\n        temp0 = np.float64(0)\n        temp1 = np.float64(0)\n        J0 = np.float64(0)\n        J1 = np.float64(0)\n        converge = False\n        error = np.empty([])\n        i = 0\n        while converge is False:\n            J0 = costFunction(theta0, theta1, True, price, area, m)\n            J1 = costFunction(theta0, theta1, False, price, area, m)\n            print(""Iteration number: %s"" %i)\n            \n            #Calculate error from cost function. Should be decreasing WRT to iterations.\n            errorValue(price, area, theta0, theta1, m, i, plot)\n            \n            temp0 = theta0 - (alpha*J0/m)\n            temp1 = theta0 - (alpha*J1/m)\n            \n            #convergence test. If slop is changing very minutely then stop.\n            converge = convergence(theta1, temp1)\n            \n            theta0 = temp0\n            theta1 = temp1\n            i += 1\n\n            if math.isnan(theta0) or math.isnan(theta1) is True:\n                break\n            else:\n                pass\n        return theta0,theta1, i\n    except Exception as e:\n        print(e)\n        raise Exception(""Please check parameters."")\n        \n\n\n\n\nif __name__ == \'__main__\':\n    print(""Linear Regression Prediction using Linear Gradient Descent\\n"")\n    try:\n        dataSet = np.genfromtxt(\'/home/debojit/ML/MLAlgos/kc_house_data.csv\', delimiter = \',\', names = True, dtype = \'float64\')\n        price, area = np.zeros([len(dataSet),1]),np.zeros([len(dataSet),1])        \n        for it, l in enumerate(dataSet):\n            price[it] = l[2]\n            area[it] = l[5]\n        \n        \n        \'\'\' Feature Scaling. \n            (\n                Every entry divded my highest number of the set. \n                Another option is to subtract mean of set from eveery element and divide it by range of values for that column.\n            )\n        \'\'\'\n        price = price/max(price)\n        area = area/max(area)\n        \n        theta0, theta1, i = gradientDescent(price, area, plt)\n        print(""Theta0: %s Theta1: %s"" %(theta0, theta1))\n        regressionLine(theta1,theta0, plt, i, color = \'red\')\n        plt.show()\n    except Exception:\n        raise Exception(""Something is wrong."")\n'"
