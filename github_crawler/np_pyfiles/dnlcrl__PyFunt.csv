file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n# coding: utf-8\n\nimport os\nimport sys\nimport subprocess\n\n\'\'\'\nOriginal Source: https://github.com/scipy/scipy/blob/master/setup.py\n\'\'\'\n\nif sys.version_info[:2] < (2, 6) or (3, 0) <= sys.version_info[0:2] < (3, 2):\n    raise RuntimeError(""Python version 2.6, 2.7 (TODO: >= 3.2) required."")\n\nif sys.version_info[0] < 3:\n    import __builtin__ as builtins\nelse:\n    import builtins\n\nMAJOR = 0\nMINOR = 1\nMICRO = 0\nISRELEASED = False\n\nVERSION = \'%d.%d.%d\' % (MAJOR, MINOR, MICRO)\n\nwith open(\'./requirements.txt\') as f:\n    required = f.read().splitlines()\n\n# BEFORE importing distutils, remove MANIFEST. distutils doesn\'t properly\n# update it when the contents of directories change.\nif os.path.exists(\'MANIFEST\'):\n    os.remove(\'MANIFEST\')\n\n\n# Return the git revision as a string\ndef git_version():\n    def _minimal_ext_cmd(cmd):\n        # construct minimal environment\n        env = {}\n        for k in [\'SYSTEMROOT\', \'PATH\']:\n            v = os.environ.get(k)\n            if v is not None:\n                env[k] = v\n        # LANGUAGE is used on win32\n        env[\'LANGUAGE\'] = \'C\'\n        env[\'LANG\'] = \'C\'\n        env[\'LC_ALL\'] = \'C\'\n        out = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, env=env).communicate()[0]\n        return out\n\n    try:\n        out = _minimal_ext_cmd([\'git\', \'rev-parse\', \'HEAD\'])\n        GIT_REVISION = out.strip().decode(\'ascii\')\n    except OSError:\n        GIT_REVISION = ""Unknown""\n\n    return GIT_REVISION\n\n\n# This is a bit hackish: we are setting a global variable so that the main\n# pyfunt __init__ can detect if it is being loaded by the setup routine, to\n# avoid attempting to load components that aren\'t built yet.  While ugly, it\'s\n# a lot more robust than what was previously being used.\nbuiltins.__PUFUNT_SETUP__ = True\n\n\ndef get_version_info():\n    # Adding the git rev number needs to be done inside\n    # write_version_py(), otherwise the import of pyfunt.version messes\n    # up the build under Python 3.\n    FULLVERSION = VERSION\n    if os.path.exists(\'.git\'):\n        GIT_REVISION = git_version()\n    elif os.path.exists(\'pyfunt/version.py\'):\n        # must be a source distribution, use existing version file\n        # load it as a separate module to not load pyfunt/__init__.py\n        import imp\n        version = imp.load_source(\'pyfunt.version\', \'pyfunt/version.py\')\n        GIT_REVISION = version.git_revision\n    else:\n        GIT_REVISION = ""Unknown""\n\n    if not ISRELEASED:\n        FULLVERSION += \'.dev0+\' + GIT_REVISION[:7]\n\n    return FULLVERSION, GIT_REVISION\n\n\ndef write_version_py(filename=\'pyfunt/version.py\'):\n    cnt = """"""\\\n        # THIS FILE IS GENERATED FROM PYFUNT SETUP.PY\\\n        short_version = \'%(version)s\'\\\n        version = \'%(version)s\'\\\n        full_version = \'%(full_version)s\'\\\n        git_revision = \'%(git_revision)s\'\\\n        release = %(isrelease)s\\\n        if not release:\\\n            version = full_version\\\n    """"""\n    FULLVERSION, GIT_REVISION = get_version_info()\n\n    a = open(filename, \'w\')\n    try:\n        a.write(cnt % {\'version\': VERSION,\n                       \'full_version\': FULLVERSION,\n                       \'git_revision\': GIT_REVISION,\n                       \'isrelease\': str(ISRELEASED)})\n    finally:\n        a.close()\n\n\ndef generate_cython():\n    cwd = os.path.abspath(os.path.dirname(__file__))\n    print(""Cythonizing sources"")\n    p = subprocess.call([sys.executable,\n                         os.path.join(cwd, \'tools\', \'cythonize.py\'),\n                         \'pyfunt\'],\n                        cwd=cwd)\n    if p != 0:\n        raise RuntimeError(""Running cythonize failed!"")\n\n\ndef configuration(parent_package=\'\', top_path=None):\n    from numpy.distutils.misc_util import Configuration\n    config = Configuration(None, parent_package, top_path)\n    config.set_options(ignore_setup_xxx_py=True,\n                       assume_default_configuration=True,\n                       delegate_options_to_subpackages=True,\n                       quiet=True)\n\n    config.add_subpackage(\'pyfunt\')\n    config.add_data_files((\'pyfunt\', \'*.txt\'))\n\n    config.get_version(\'pyfunt/version.py\')\n\n    return config\n\n\ndef setup_package():\n\n    # Rewrite the version file every time\n\n    write_version_py()\n    cmdclass = {}\n\n    # Figure out whether to add ``*_requires = [\'numpy\']``.\n    # We don\'t want to do that unconditionally, because we risk updating\n    # an installed numpy which fails too often.  Just if it\'s not installed, we\n    # may give it a try.  See gh-3379.\n    build_requires = []\n    try:\n        import numpy\n        if (len(sys.argv) >= 2 and sys.argv[1] == \'bdist_wheel\' and\n                sys.platform == \'darwin\'):\n            # We\'re ony building wheels for platforms where we know there\'s\n            # also a Numpy wheel, so do this unconditionally.  See gh-5184.\n            build_requires = [\'numpy>=1.7.1\']\n    except:\n        build_requires = [\'numpy>=1.7.1\']\n\n    metadata = dict(\n        name=""pyfunt"",\n        author=""Daniele Ettore Ciriello"",\n        author_email=""ciriello.daniele@gmail.com"",\n        version=""1.1.0"",\n        license=""MIT"",\n        url=""https://github.com/dnlcrl/PyFunt"",\n        download_url=""https://github.com/dnlcrl/PyFunt"",\n        description=""Pythonic Deep Learning Framework"",\n        packages=[\'pyfunt\', \'pyfunt/examples\', \'pyfunt/utils\', \'pyfunt/examples/residual_networks\', ],\n        cmdclass=cmdclass,  # {\'build_ext\': build_ext},\n        platforms=[""Windows"", ""Linux"", ""Solaris"", ""Mac OS-X"", ""Unix""],\n        setup_requires=build_requires,\n        install_requires=required,\n        # ext_modules=extensions,\n        keywords=\'pyfunt deep learning artificial neural network convolution\',\n    )\n\n    if len(sys.argv) >= 2 and (\'--help\' in sys.argv[1:] or\n                               sys.argv[1] in (\'--help-commands\', \'egg_info\', \'--version\',\n                                               \'clean\')):\n        # For these actions, NumPy is not required.\n        #\n        # They are required to succeed without Numpy for example when\n        # pip is used to install Scipy when Numpy is not yet present in\n        # the system.\n        try:\n            from setuptools import setup\n        except ImportError:\n            from distutils.core import setup\n    else:\n        if (len(sys.argv) >= 2 and sys.argv[1] in (\'bdist_wheel\', \'bdist_egg\')) or (\n                \'develop\' in sys.argv):\n            # bdist_wheel/bdist_egg needs setuptools\n            import setuptools\n\n        from numpy.distutils.core import setup\n\n        cwd = os.path.abspath(os.path.dirname(__file__))\n        if not os.path.exists(os.path.join(cwd, \'PKG-INFO\')):\n            # Generate Cython sources, unless building from source release\n            generate_cython()\n\n        metadata[\'configuration\'] = configuration\n\n    print \'setup complete\'\n    setup(**metadata)\n\nif __name__ == \'__main__\':\n    setup_package()\n'"
pyfunt/__init__.py,0,b'from affine import Affine\nfrom batch_normalization import BatchNormalization\nfrom c_add_table import CAddTable\nfrom class_nll_criterion import ClassNLLCriterion\nfrom concat_table import ConcatTable\nfrom container import Container\nfrom criterion import Criterion\nfrom dropout import Dropout\nfrom identity import Identity\nfrom linear import Linear\nfrom log_soft_max import LogSoftMax\nfrom module import Module\nfrom mul_constant import MulConstant\nfrom padding import Padding\nfrom parallel import Parallel\nfrom relu import ReLU\nfrom reshape import Reshape\nfrom sequential import Sequential\nfrom sigmoid import Sigmoid\nfrom soft_max import SoftMax\nfrom solver import Solver\nfrom spatial_average_pooling import SpatialAveragePooling\nfrom spatial_batch_normalitazion import SpatialBatchNormalization\nfrom spatial_convolution import SpatialConvolution\nfrom spatial_full_convolution import SpatialFullConvolution\nfrom spatial_replication_padding import SpatialReplicationPadding\nfrom spatial_reflection_padding import SpatialReflectionPadding\nfrom spatial_max_pooling import SpatialMaxPooling\nfrom spatial_up_sampling_nearest import SpatialUpSamplingNearest\nfrom tanh import Tanh\nfrom threshold import Threshold\nfrom view import View\n\nfrom . import *\n'
pyfunt/affine.py,9,"b'#!/usr/bin/env python\n# coding: utf-8\n\nfrom module import Module\nimport numpy as np\n\n\nclass Affine(Module):\n\n    def __init__(self, input_size, output_size, bias=False):\n        super(Affine, self).__init__()\n        self.weight = np.ndarray(output_size, input_size)\n        self.grad_weight = np.ndarray(output_size, input_size)\n        if bias:\n            self.bias = np.ndarray(output_size)\n            self.grad_bias = np.ndarray(output_size)\n        self.reset()\n\n    def no_bias(self):\n        self.bias = None\n        self.grad_bias = None\n        return self\n\n    def reset(self, stdv=None):\n        if not stdv:\n            stdv = 1./np.sqrt(self.weight.shape[2])\n        self.weight = np.uniform(-stdv, stdv, self.weight.shape)\n        self.bias = np.uniform(-stdv, stdv, self.weight.shape[0])\n\n    def update_output(self, x):\n        w = self.weight\n        b = self.bias or np.zeros(self.weight.shape[0])\n        self.out = x.reshape(x.shape[0], -1).dot(w) + b\n        self.x = x\n        return self.output\n\n    def update_grad_input(self, input, grad_output):\n        x, w = self.x, self.weight\n        self.grad_input = grad_output.dot(w.T).reshape(x.shape)\n        return self.grad_input\n\n    def acc_grad_parameters(self, x, grad_output, scale=1):\n        x = self.x\n        self.grad_weight = x.reshape(x.shape[0], -1).T.dot(grad_output)\n        if self.bias:\n            self.grad_bias = np.sum(grad_output, axis=0)\n\n    def clear_state(self):\n        pass\n\n    def __str__(self):\n        pass\n'"
pyfunt/batch_normalization.py,19,"b""#!/usr/bin/env python\n# coding: utf-8\n\nfrom module import Module\nimport numpy as np\n\n\nclass BatchNormalization(Module):\n\n    def __init__(self, n_output, eps=1e-5, momentum=0.1, affine=True):\n        super(BatchNormalization, self).__init__()\n        self.eps = eps\n        self.momentum = momentum\n        self.train = True\n        self.running_mean = np.zeros(n_output)\n        self.running_var = np.zeros(n_output)\n        n_dim = 2\n        if affine:\n            self.weight = np.ndarray(n_output)\n            self.bias = np.ndarray(n_output)\n            self.grad_weight = np.ndarray(n_output)\n            self.grad_bias = np.ndarray(n_output)\n        else:\n            self.weight = None\n            self.bias = None\n            self.grad_weight = None\n            self.grad_bias = None\n        self.reset()\n\n    def reset(self):\n        if self.weight is not None:\n            self.weight[:] = np.random.uniform(size=len(self.weight))[:]\n        if self.bias is not None:\n            self.bias[:] = np.zeros(len(self.bias))[:]\n        self.running_mean = np.zeros(len(self.running_mean))\n        self.running_var = np.ones(len(self.running_var))\n\n    def check_input_dim(self, x):\n        i_dim = len(x.shape)\n        if i_dim != self.n_dim or (i_dim != self.n_dim - 1 and self.train is not False):\n            raise Exception('TODO ERROR :(')\n        # feast_dim = (i_dim == self.n_dim -1) and 1 or 2\n        #      local featDim = (iDim == self.nDim - 1) and 1 or 2\n        # assert(input:size(featDim) == self.running_mean:nElement(), string.format(\n        #    'got %d-feature tensor, expected %d',\n   #    input:size(featDim), self.running_mean:nElement()))\n\n    def make_contigous(self, x, grad_output):\n        #TODO\n        pass\n\n\n    def update_output(self, x):\n\n        eps = self.eps\n        momentum = self.momentum\n        N, D = x.shape\n        running_mean = self.running_mean\n        running_var = self.running_var\n\n        if self.train:\n            mean = 1. / N * np.sum(x, axis=0)\n\n            xmu = x - mean\n\n            carre = xmu*xmu\n\n            var = 1. / N * np.sum(carre, axis=0)\n\n            sqrtvar = np.sqrt(var + eps)\n\n            invstd = 1. / sqrtvar\n\n            running_mean = momentum * mean + (1. - momentum) * running_mean\n\n            unbiased_var = np.sum(carre, axis=0)/(N - 1.)\n\n            running_var = momentum * unbiased_var + \\\n                (1. - momentum) * running_var\n\n            self.xmu = xmu\n            self.invstd = invstd\n\n        else:\n            mean = running_mean\n            invstd = 1. / np.sqrt(running_var + eps)\n\n        out = ((x - mean) * invstd)\n        if self.weight is not None:\n            out *= self.weight\n        if self.bias is not None:\n            out += self.bias\n        #out = ((x - mean) * invstd) * self.weight + self.bias\n        # Store the updated running means back into bn_param\n        self.running_mean = np.array(running_mean, copy=True)\n        self.running_var = np.array(running_var, copy=True)\n        self.output = out\n\n        return self.output\n\n    def update_grad_input(self, x, grad_output, scale=1):\n\n        xmu, invstd = self.xmu, self.invstd\n\n        N, D = grad_output.shape\n\n        _sum = np.sum(grad_output, axis=0)\n        dotp = np.sum((xmu * grad_output), axis=0)\n\n        k = 1. / N * dotp * invstd * invstd\n        dx = xmu * k\n\n        dmean = 1. / N * _sum\n        dx = (grad_output - dmean - dx) * invstd * self.weight\n\n        self.grad_weight[:] = dotp * invstd\n\n        self.grad_bias[:] = _sum\n        self.grad_input = dx\n\n        return self.grad_input\n\n    # def backward(self, x, grad_output, scale=1):\n    #     return self.update_grad_input(x, grad_output, scale)\n\n    def acc_grad_input(self, x, grad_output, scale):\n        return self.backward(x, grad_output, scale, None, self.grad_weight, self.grad_bias)\n\n    def clear_state(self):\n        pass\n"""
pyfunt/c_add_table.py,3,"b'from module import Module\nimport numpy as np\n\n\nclass CAddTable(Module):\n\n    def __init__(self):\n        super(CAddTable, self).__init__()\n        self.grad_input = None\n\n    def update_output(self, x):\n        self.output = np.sum(x, axis=0)\n        return self.output\n\n    def update_grad_input(self, x, grad_output):\n        self.grad_input = np.zeros_like(x)\n        for i in xrange(len(x)):\n            self.grad_input[i] = np.copy(grad_output)\n        return self.grad_input\n\n    def reset(self):\n        pass\n'"
pyfunt/class_nll_criterion.py,11,"b'from criterion import Criterion\nimport numpy as np\n\n\nclass ClassNLLCriterion(Criterion):\n\n    """"""docstring for ClassNLLCriterion""""""\n\n    def __init__(self, weights=None, size_average=None):\n        super(ClassNLLCriterion, self).__init__()\n        if size_average:\n            self.size_average = size_average\n        else:\n            self.size_average = True\n\n        if weights:\n            # assert(weights:dim() == 1, ""weights input should be 1-D Tensor"")\n            self.weights = weights\n        self.output_tensor = np.zeros(1)\n        self.total_weight_tensor = np.ones(1)\n        self.target = np.zeros(1)  # , dtype=np.long)\n\n    def __len__(self):\n        if self.weights:\n            return len(self.weights)\n        else:\n            return 0\n\n    def update_output(self, x, target):\n\n        # probs=np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        # probs /= np.sum(probs, axis=1, keepdims=True)\n        # return probs\n        # # N = x.shape[0]\n        # # loss = -np.mean(self.logp[np.arange(N), target])\n        # # self.output = -x\n        # # return loss\n\n        # N = x.shape[0]\n        # xdev = x - x.max(1, keepdims=True)\n        # self.logp = xdev - np.log(np.sum(np.exp(xdev), axis=1, keepdims=True))\n        # loss = -np.mean(self.logp[np.arange(N), target])\n        # self.output = loss\n        # import pdb; pdb.set_trace()\n        # return self.output\n\n        self.output = - np.mean(x[np.arange(x.shape[0]), target])\n        return self.output\n\n    def update_grad_input(self, x, target):\n        N = x.shape[0]\n        dx = np.exp(x)\n        dx[np.arange(N), target] -= 1\n        dx /= N\n        self.grad_input = dx\n        #import pdb; pdb.set_trace()\n        return self.grad_input\n'"
pyfunt/concat_table.py,1,"b'from container import Container\nimport numpy as np\n\n\nclass ConcatTable(Container):\n\n    """"""docstring for ConcatTable""""""\n\n    def __init__(self):\n        super(ConcatTable, self).__init__()\n        self.modules = []\n\n    def update_output(self, x):\n        self.output = []\n        for i in xrange(len(self.modules)):\n            current_output = self.rethrow_errors(\n                self.modules[i], i, \'update_output\', x)\n            self.output.append(current_output)\n            # if i == 0:\n            #     self.output = current_output\n            # else:\n            #     np.concatenate((self.output, current_output), axis=0)\n            # self.output.append(self.rethrow_errors(\n            # self.modules[i], i, \'update_output\', x))\n        return self.output\n\n    def _backward(self, method, x, grad_output, scale=1):\n        for i, module in enumerate(self.modules):\n            if method == \'update_grad_input\':\n                args = self.modules[i], i, method, x, grad_output[i]\n            else:\n                args = self.modules[i], i, method, x, grad_output[i], scale\n            current_grad_input = self.rethrow_errors(*args)\n            if i == 0:\n                self.grad_input = current_grad_input\n            else:\n                self.grad_input += current_grad_input\n        return self.grad_input\n\n    def update_grad_input(self, x, grad_output):\n        return self._backward(\'update_grad_input\', x, grad_output)\n\n    def backward(self, x, grad_output, scale=1):\n        return self._backward(\'backward\', x, grad_output, scale)\n\n    def acc_grad_parameters(self, x, grad_output, scale=1):\n        for i, module in enumerate(self.modules):\n            self.rethrow_errors(\n                self.modules[i], i, \'acc_grad_parameters\', x, grad_output[i], scale)\n'"
pyfunt/container.py,0,"b'from __future__ import print_function\nfrom module import Module\nfrom types import DictType\nimport sys\nimport traceback\nimport numpy as np\nimport abc\n\n\nclass Container(Module):\n    """"""docstring for Container""""""\n    def __init__(self):\n        super(Container, self).__init__()\n        self.modules = []\n\n    def add(self, module):\n        self.modules.append(module)\n        return self\n\n    def get(self, index):\n        return self.modules[index]\n\n    def size(self):\n        return len(self.modules)\n\n    def rethrow_errors(self, module, module_index, func_name, *args):\n        def handle_error(err):\n            # TODO\n            return err\n        func = getattr(module, func_name)\n        try:\n            result = func(*args)\n        except Exception as e:\n            print(\'In %d module (%s) of %s:\' % (module_index, type(module).__name__, type(self).__name__))\n            traceback.print_exc()\n            raise e\n\n        return result\n\n    def apply_to_modules(self, func):\n        for module in self.modules:\n            func(module)\n\n    def zero_grad_parameters(self):\n        self.apply_to_modules(lambda module: module.zero_grad_parameters())\n\n    def update_parameters(self, lr):\n        self.apply_to_modules(lambda module: module.update_parameters(lr))\n\n    def training(self):\n        super(Container, self).training()\n        self.apply_to_modules(lambda module: module.training())\n\n    def evaluate(self):\n        super(Container, self).evaluate()\n        self.apply_to_modules(lambda module: module.evaluate())\n\n    def share(self, mlp, args):\n        pass\n\n    def reset(self, stdv):\n        self.apply_to_modules(lambda module: module.reset(stdv))\n\n    def parameters(self):\n        def tinsert(to, _from):\n            if isinstance(_from, list):\n                for i in xrange(len(_from)):\n                    tinsert(to, _from[i])\n            else:\n                to.append(_from)\n\n        w = []\n        gw = []\n        for i in xrange(len(self.modules)):\n\n            res = self.modules[i].parameters()\n            if res:\n                mw, mgw = res\n                tinsert(w, mw)\n                tinsert(gw, mgw)\n        return w, gw\n\n    def clear_state(self):\n        pass\n'"
pyfunt/criterion.py,0,"b'import numpy as np\nimport abc\n\n\nclass Criterion(object):\n    __metaclass__ = abc.ABCMeta\n    """"""docstring for Criterion""""""\n    def __init__(self):\n        super(Criterion, self).__init__()\n        self.output = 0\n\n    @abc.abstractmethod\n    def update_output(self, x, target):\n        pass\n\n    def forward(self, x, target):\n        return self.update_output(x, target)\n\n    def backward(self, x, target):\n        return self.update_grad_input(x, target)\n\n    @abc.abstractmethod\n    def update_grad_input(self, x, target):\n        pass\n\n    def clone(self):\n        pass\n\n    def __call__(self, x, target):\n        self.output = self.forward(x, target)\n        self.grad_input = self.backward(x, target)\n        return self.output, self.grad_input\n'"
pyfunt/dropout.py,1,"b""from module import Module\nimport numpy as np\n\n\nclass Dropout(Module):\n\n    def __init__(self, p=0.5, v1=False, stochastic_inference=False):\n        super(Dropout, self).__init__()\n        self.p = p\n        self.train = True\n        self.stochastic_inference = stochastic_inference\n        # version 2 scales output during training instead of evaluation\n        self.v2 = not v1\n        if self.p >= 1 or self.p < 0:\n            raise('<Dropout> illegal percentage, must be 0 <= p < 1')\n        self.noise = None\n\n    def update_output(self, x):\n        self.output = x.copy()\n        if self.p > 0:\n            if self.train or self.stochastic_inference:\n                self.noise = np.random.binomial(1, p=1-self.p, size=x.shape)  # bernoulli\n                if self.v2:\n                    self.noise /= 1-self.p\n                self.output *= self.noise\n            elif not self.v2:\n                self.output *= 1-self.p\n        return self.output\n\n    def update_grad_input(self, x, grad_output):\n        self.grad_input = grad_output.copy()\n        if self.train:\n            if self.p > 0:\n                self.grad_input *= self.noise\n        else:\n            if not self.v2 and self.p > 0:\n                self.grad_input *= 1-self.p\n        return self.grad_input\n\n    def __str__(self):\n        return '%s(%f)' % (type(self), self.p)\n\n    def reset(self):\n        pass\n"""
pyfunt/identity.py,0,"b'from module import Module\n\n\nclass Identity(Module):\n    """"""docstring for Identity""""""\n    def __init__(self):\n        super(Identity, self).__init__()\n\n    def update_output(self, x):\n        self.output = x.copy()\n        return self.output\n\n    def update_grad_input(self, x, grad_output):\n        self.grad_input = grad_output.copy()\n        return self.grad_input\n\n    def clear_state(self):\n        pass\n\n    def reset(self):\n        pass\n'"
pyfunt/im2col.py,10,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport numpy as np\n\n\ndef get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n    # First figure out what the size of the output should be\n    N, C, H, W = x_shape\n    assert (H + 2 * padding - field_height) % stride == 0\n    assert (W + 2 * padding - field_height) % stride == 0\n    out_height = (H + 2 * padding - field_height) / stride + 1\n    out_width = (W + 2 * padding - field_width) / stride + 1\n\n    i0 = np.repeat(np.arange(field_height), field_width)\n    i0 = np.tile(i0, C)\n    i1 = stride * np.repeat(np.arange(out_height), out_width)\n    j0 = np.tile(np.arange(field_width), field_height * C)\n    j1 = stride * np.tile(np.arange(out_width), out_height)\n    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n\n    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n\n    return (k, i, j)\n\n\ndef im2col_indices(x, field_height, field_width, padding=1, stride=1):\n    ''' An implementation of im2col based on some fancy indexing '''\n    # Zero-pad the input\n    p = padding\n    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n\n    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding,\n                                 stride)\n\n    cols = x_padded[:, k, i, j]\n    C = x.shape[1]\n    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n    return cols\n\n\ndef col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1,\n                   stride=1):\n    ''' An implementation of col2im based on fancy indexing and np.add.at '''\n    N, C, H, W = x_shape\n    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding,\n                                 stride)\n    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n    if padding == 0:\n        return x_padded\n    return x_padded[:, :, padding:-padding, padding:-padding]\n\n"""
pyfunt/linear.py,10,"b'from module import Module\nimport numpy as np\n\n\nclass Linear(Module):\n\n    def __init__(self, input_size, output_size, bias=True):\n        super(Linear, self).__init__()\n        self.weight = np.ndarray((input_size, output_size))\n        self.grad_weight = np.ndarray((input_size, output_size))\n        if bias:\n            self.bias = np.ndarray(output_size)\n            self.grad_bias = np.ndarray(output_size)\n        else:\n            self.bias = None\n            self.grad_bias = None\n        self.reset()\n\n    def no_bias(self):\n        self.bias = None\n        self.grad_bias = None\n\n    def reset(self, stdv=None):\n        if stdv:\n            stdv = stdv * np.sqrt(3)\n        else:\n            std = 1./np.sqrt(self.weight.shape[0])\n            # stdv = 1./np.sqrt(self.weight.shape[1])\n        self.weight = np.random.uniform(-std, std, self.weight.shape)\n        if self.bias is not None:\n            self.bias = np.random.uniform(-std, std, self.bias.shape)\n\n    def update_output(self, x):\n        out = x.reshape(x.shape[0], -1)\n        out = out.dot(self.weight)\n        if self.bias is not None:\n            out += self.bias\n        self.output = out\n        return self.output\n\n    def update_grad_input(self, x, grad_output):\n        dx = grad_output.dot(self.weight.T).reshape(x.shape)\n        self.grad_weight[:] = x.reshape(x.shape[0], -1).T.dot(grad_output)[:]\n        if self.bias is not None:\n            self.grad_bias[:] = np.sum(grad_output, axis=0)[:]\n        self.grad_input = dx\n        return dx\n\n    def acc_grad_parameters(self, x, grad_output, scale=None):\n        pass\n'"
pyfunt/log_soft_max.py,6,"b'from module import Module\nimport numpy as np\n\n\nclass LogSoftMax(Module):\n    """"""docstring for LogSoftMax""""""\n    def __init__(self):\n        super(LogSoftMax, self).__init__()\n\n    def update_output(self, x):\n        max_input = x.max(1, keepdims=True)\n        log_sum = np.sum(np.exp(x - max_input), axis=1, keepdims=True)\n        log_sum = max_input + np.log(log_sum)\n        self.output = x - log_sum\n        return self.output\n\n    def update_grad_input(self, x, grad_output):\n        _sum = np.sum(grad_output, axis=1, keepdims=True)\n\n        max_input = x.max(1, keepdims=True)\n        log_sum = np.sum(np.exp(x - max_input), axis=1, keepdims=True)\n        log_sum = max_input + np.log(log_sum)\n        self.output = x - log_sum\n\n        self.grad_input = grad_output - np.exp(self.output)*_sum\n        return self.grad_input\n\n    def reset(self):\n        pass\n'"
pyfunt/module.py,5,"b'#!/usr/bin/env python\n# coding: utf-8\n\nimport abc\nimport numpy as np\nfrom copy import deepcopy\nfrom types import DictType\n\n\nclass Module(object):\n    __metaclass__ = abc.ABCMeta\n\n    def __init__(self):\n        self.grad_input = None  # np.ndarray()\n        self.output = None  # np.ndarray()\n        self._type = np.float\n\n    def parameters(self):\n        if hasattr(self, \'weight\'):\n            if self.weight is not None and self.bias is not None:\n                return [self.weight, self.bias], [self.grad_weight, self.grad_bias]\n            if self.weight is not None:\n                return [self.weight], [self.grad_weight]\n            if self.bias is not None:\n                return [self.bias], [self.grad_bias]\n\n    @abc.abstractmethod\n    def update_output(self, _input=None):\n        # return self.output\n        raise NotImplementedError()\n\n    def forward(self, x):\n        return self.update_output(x)\n\n    def backward(self, _input, grad_output, scale=1):\n        self.grad_input = self.update_grad_input(_input, grad_output)\n        self.acc_grad_parameters(_input, grad_output, scale)\n        return self.grad_input\n\n    def backward_update(self, _input, grad_output, lr):\n        grad_weight = self.grad_weight\n        grad_bias = self.grad_bias\n        self.grad_weight = self.weight\n        self.grad_bias = self.bias\n        self.acc_grad_parameters(_input, grad_output, -lr)\n        self.grad_weight = grad_weight\n        self.grad_bias = grad_bias\n\n    @abc.abstractmethod\n    def update_grad_input(self, _input, grad_output):\n        # return self.grad_input\n        raise NotImplementedError()\n\n    def acc_grad_parameters(self, _input, grad_output, scale):\n        pass\n\n    def acc_update_grad_parameters(self, _input, grad_output, lr):\n        grad_weight = self.grad_weight\n        grad_bias = self.grad_bias\n        self.grad_weight = self.weight\n        self.grad_bias = self.bias\n        self.acc_grad_parameters(_input, grad_output, -lr)\n        self.grad_weight = grad_weight\n        self.grad_bias = grad_bias\n\n    def shared_acc_update_grad_parameters(self, _input, grad_output, lr):\n        if self.parameters():\n            self.zero_grad_parameters()\n            self.acc_grad_parameters(_input, grad_output, 1)\n            self.update_parameters(lr)\n\n    def zero_grad_parameters(self):\n        _, grad_params = self.parameters()\n        if grad_params:\n            for g in grad_params:\n                g.zero()\n\n    def update_parameters(self, lr):\n        res = self.parameters()\n        if res:\n            params, grad_params = res\n            for i, p in enumerate(params):\n                p -= lr*grad_params[i]\n\n    def training(self):\n        self.train = True\n\n    def evaluate(self):\n        self.train = False\n\n    def share(self, mlp, p_names):\n        for i, v in enumerate(p_names):\n            if self[v] is not None:\n                self[v].set(mlp[v])\n                self.acc_update_grad_parameters = self.shared_acc_update_grad_parameters\n                mlp.acc_update_grad_parameters = self.acc_update_grad_parameters\n        return self\n\n    def clone(self, p_names=None):\n        clone = deepcopy(self)\n        if p_names:\n            clone.share(self, p_names)\n        return clone\n\n    # def type(self, type=None, cache=None):\n    #     if type is not None:\n    #         return self._type\n    #     cache = cache or {}\n    #     # find all tensors and convert them\n    #     for key, param in pairs(self):\n    #         self[key] = utils.recursive_type(param, type, cache)\n\n    #     self._type = type\n    #     return self\n\n\n# function Module:float(...)\n#    return self:type(\'torch.FloatTensor\',...)\n# end\n\n# function Module:double(...)\n#    return self:type(\'torch.DoubleTensor\',...)\n# end\n\n# function Module:cuda(...)\n#    return self:type(\'torch.CudaTensor\',...)\n# end\n\n    def reset(self):\n        raise NotImplementedError()\n\n    def write(self, file):\n        np.save(file, self)\n\n    def read(self, file):\n        obj = np.load(file)[0]\n        for k, v in enumerate(obj):\n            self[k] = v\n\n\n# -- This function is not easy to understand. It works as follows:\n# --\n# -- - gather all parameter tensors for this module (and children);\n# --   count all parameter values (floats)\n# -- - create one ginormous memory area (Storage object) with room for all\n# --   parameters\n# -- - remap each parameter tensor to point to an area within the ginormous\n# --   Storage, and copy it there\n# --\n# -- It has the effect of making all parameters point to the same memory area,\n# -- which is then returned.\n# --\n# -- The purpose is to allow operations over all parameters (such as momentum\n# -- updates and serialization), but it assumes that all parameters are of\n# -- the same type (and, in the case of CUDA, on the same device), which\n# -- is not always true. Use for_each() to iterate over this module and\n# -- children instead.\n# --\n# -- Module._flattenTensorBuffer can be used by other packages (e.g. cunn)\n# -- to specify the type of temporary buffers. For example, the temporary\n# -- buffers for CudaTensor could be FloatTensor, to avoid GPU memory usage.\n# --\n# -- TODO: This logically belongs to torch.Tensor, not nn.\n# Module._flattenTensorBuffer = {}\n\n\n# function Module.flatten(parameters)\n\n#    -- returns true if tensor occupies a contiguous region of memory (no holes)\n#    local function isCompact(tensor)\n#       local sortedStride, perm = torch.sort(\n#             torch.LongTensor(tensor:nDimension()):set(tensor:stride()), 1, true)\n#       local sortedSize = torch.LongTensor(tensor:nDimension()):set(\n#             tensor:size()):index(1, perm)\n#       local nRealDim = torch.clamp(sortedStride, 0, 1):sum()\n#       sortedStride = sortedStride:narrow(1, 1, nRealDim):clone()\n#       sortedSize   = sortedSize:narrow(1, 1, nRealDim):clone()\n#       local t = tensor.new():set(tensor:storage(), 1,\n#                                  sortedSize:storage(),\n#                                  sortedStride:storage())\n#       return t:isContiguous()\n#    end\n\n#    if not parameters or #parameters == 0 then\n#       return torch.Tensor()\n#    end\n#    local Tensor = parameters[1].new\n# local TmpTensor = Module._flattenTensorBuffer[torch.type(parameters[1])]\n# or Tensor\n\n#    -- 1. construct the set of all unique storages referenced by parameter tensors\n#    local storages = {}\n#    local nParameters = 0\n#    local parameterMeta = {}\n#    for k = 1,#parameters do\n#       local param = parameters[k]\n#       local storage = parameters[k]:storage()\n#       local storageKey = torch.pointer(storage)\n\n#       if not storages[storageKey] then\n#          storages[storageKey] = {storage, nParameters}\n#          nParameters = nParameters + storage:size()\n#       end\n\n#       parameterMeta[k] = {storageOffset = param:storageOffset() +\n#                                           storages[storageKey][2],\n#                           size          = param:size(),\n#                           stride        = param:stride()}\n#    end\n\n#    -- 2. construct a single tensor that will hold all the parameters\n#    local flatParameters = TmpTensor(nParameters):zero()\n\n#    -- 3. determine if there are elements in the storage that none of the\n#    --    parameter tensors reference (\'holes\')\n#    local tensorsCompact = true\n#    for k = 1,#parameters do\n#       local meta = parameterMeta[k]\n#       local tmp = TmpTensor():set(\n#          flatParameters:storage(), meta.storageOffset, meta.size, meta.stride)\n#       tmp:fill(1)\n#       tensorsCompact = tensorsCompact and isCompact(tmp)\n#    end\n\n#    local maskParameters  = flatParameters:byte():clone()\n#    local compactOffsets  = flatParameters:long():cumsum(1)\n#    local nUsedParameters = compactOffsets[-1]\n\n#    -- 4. copy storages into the flattened parameter tensor\n#    for _, storageAndOffset in pairs(storages) do\n#       local storage, offset = table.unpack(storageAndOffset)\n#       flatParameters[{{offset+1,offset+storage:size()}}]:copy(Tensor():set(storage))\n#    end\n\n#    -- 5. allow garbage collection\n#    storages = nil\n#    for k = 1,#parameters do\n#        parameters[k]:set(Tensor())\n#    end\n\n#    -- 6. compact the flattened parameters if there were holes\n#    if nUsedParameters ~= nParameters then\n#       assert(tensorsCompact,\n#          ""Cannot gather tensors that are not compact"")\n\n#       flatParameters = TmpTensor(nUsedParameters):copy(\n#             flatParameters:maskedSelect(maskParameters))\n#       for k = 1,#parameters do\n#         parameterMeta[k].storageOffset =\n#               compactOffsets[parameterMeta[k].storageOffset]\n#       end\n#    end\n\n#    if TmpTensor ~= Tensor then\n#       flatParameters = Tensor(flatParameters:nElement()):copy(flatParameters)\n#    end\n\n#    -- 7. fix up the parameter tensors to point at the flattened parameters\n#    for k = 1,#parameters do\n#       parameters[k]:set(flatParameters:storage(),\n#           parameterMeta[k].storageOffset,\n#           parameterMeta[k].size,\n#           parameterMeta[k].stride)\n#    end\n\n#    return flatParameters\n# end\n\n    def get_parameters(self):\n        parameters, grad_parameters = self.parameters()\n        #p, g = Module.flatten(parameters), Module.flatten(grad_parameters)\n        #if not p.n_element() == g.n_element():\n        #    raise Exception(\'check that you are sharing parameters and gradParameters\')\n        return parameters, grad_parameters\n\n    def __call__(self, _input=None, grad_output=None):\n        self.forward(_input)\n        if self.grad_output:\n            self.backward(_input, grad_output)\n            return self.output, self.grad_input\n        else:\n            return self.output\n\n    # Run a callback (called with the module as an argument) in preorder over this\n    # module and its children.\n    def apply(self, callback):\n        callback(self)\n        if self.modules:\n            for module in self.modules:\n                module.apply(callback)\n\n    def find_modules(self, type_c, container):\n        container = container or self\n        nodes = {}\n        containers = {}\n        mod_type = type(self)\n        if mod_type == type_c:\n            nodes[len(nodes)+1] = self\n            containers[len(containers)] = container\n        # Recurse on nodes with \'modules\'\n        if self.modules is not None:\n            if type(self.modules) is DictType:\n                for i in xrange(len(self.modules)):\n                    child = self.modules[i]\n                    cur_nodes, cur_containers = child.find_modules(\n                        type_c, self)\n\n                    # This shouldn\'t happen\n                    if not len(cur_nodes) == len(cur_containers):\n                        raise Exception(\'Internal error: incorrect return length\')\n\n                    # add the list items from our child to our list (ie return a\n                    # flattened table of the return nodes).\n                    for j in xrange(len(cur_nodes)):\n                        nodes[len(cur_nodes)+1] = cur_nodes[j]\n                        containers[len(containers)+1] = cur_containers[j]\n\n        return nodes, containers\n\n    def list_modules(self):\n        def tinsert(to, _from):\n            if type(_from) == DictType:\n                for i in xrange(len(_from)):\n                    tinsert(to, _from[i])\n            else:\n                to.update(_from)\n\n        modules = self\n        if self.modules:\n            for i in xrange(len(self.modules)):\n                modulas = self.modules[i].list_modules()\n                if modulas:\n                    tinsert(modules, modulas)\n        return modules\n\n    def clear_state(self):\n        return  # clear utils clear(self, \'output\', \'gradInput\')\n\n    # similar to apply, recursively goes over network and calls\n    # a callback function which returns a new module replacing the old one\n\n    def replace(self, callback):\n        callback(self)\n        if self.modules:\n            for i, m in enumerate(self.modules):\n                self.modules[i] = Module.replace(callback)\n'"
pyfunt/mul_constant.py,1,"b""from module import Module\nimport numpy as np\n\n\nclass MulConstant(Module):\n\n    def __init__(self, constant_scalar):\n        super(MulConstant, self).__init__()\n        if not np.isscalar(constant_scalar):\n            raise Exception('Constant is not a scalar: ' + constant_scalar)\n        self.constant_scalar = constant_scalar\n\n    def update_output(self, x):\n        self.output = x * self.constant_scalar\n        return self.output\n\n    def update_grad_input(self, x, grad_output):\n        self.grad_input = grad_output * self.constant_scalar\n        return self.grad_input\n\n    def validate_parameters(self):\n        if self.inplace:\n            if self.val > self.th:\n                raise Exception('in-place processing requires value not exceed threshold')\n\n    def reset(self):\n        pass\n"""
pyfunt/optim.py,7,"b""import numpy as np\n\n'''\nThis file implements various first-order update rules that are commonly used for\ntraining neural networks. Each update rule accepts current weights and the\ngradient of the loss with respect to those weights and produces the next set of\nweights. Each update rule has the same interface:\n\ndef update(w, dw, config=None):\n\nInputs:\n  - w: A numpy array giving the current weights.\n  - dw: A numpy array of the same shape as w giving the gradient of the\n    loss with respect to w.\n  - config: A dictionary containing hyperparameter values such as learning rate,\n    momentum, etc. If the update rule requires caching values over many\n    iterations, then config will also hold these cached values.\n\nReturns:\n  - next_w: The next point after the update.\n  - config: The config dictionary to be passed to the next iteration of the\n    update rule.\n\nNOTE: For most update rules, the default learning rate will probably not perform\nwell; however the default values of the other hyperparameters should work well\nfor a variety of different problems.\n\nFor efficiency, update rules may perform in-place updates, mutating w and\nsetting next_w equal to w.\n'''\n\n\ndef sgd_th(w, dw, config=None):\n    '''\n    Performs stochastic gradient descent with nesterov momentum,\n    like Torch's optim.sgd:\n    https://github.com/torch/optim/blob/master/sgd.lua\n\n    config format:\n    - learning_rate: Scalar learning rate.\n    - momentum: Scalar between 0 and 1 giving the momentum value.\n      Setting momentum = 0 reduces to sgd.\n    - nesterov: Boolean to indicate if nesterov momentum should be applied\n    - dampening: default equal to momentum.\n    - weight_decay: apply weight_decay in place.\n    - state_dw: stored gradients for the next update.\n    '''\n    if config is None:\n        config = {}\n\n    learning_rate = config.get('learning_rate', 1e-2)\n    momentum = config.get('momentum', 0)\n    nesterov = config.get('nesterov', False)\n    dampening = config.get('dampening', 0)\n    weight_decay = config.get('weight_decay', 0)\n    state_dw = config.get('state_dw', None)\n    assert (not nesterov or (momentum > 0 and dampening == 0)\n            ), 'Nesterov momentum requires a momentum and zero dampening'\n    dampening = dampening or momentum\n    dw = dw.copy()\n    if weight_decay:\n        dw += weight_decay * w\n\n    if momentum:\n        if state_dw is None:\n            state_dw = dw\n        else:\n            state_dw *= momentum\n            state_dw += (1 - dampening) * dw\n        if nesterov:\n            dw = dw + momentum * state_dw\n        else:\n            dw = state_dw\n\n    next_w = w - learning_rate * dw\n\n    config['state_dw'] = state_dw\n\n    return next_w, config\n\n\ndef nesterov(w, dw, config=None):\n    '''\n    Performs stochastic gradient descent with nesterov momentum.\n\n    config format:\n    - learning_rate: Scalar learning rate.\n    - momentum: Scalar between 0 and 1 giving the momentum value.\n      Setting momentum = 0 reduces to sgd.\n    - velocity: A numpy array of the same shape as w and dw used to store a moving\n      average of the gradients.\n    '''\n    if config is None:\n        config = {}\n    config.setdefault('learning_rate', 1e-2)\n    config.setdefault('momentum', 0.9)\n    v = config.get('velocity', np.zeros_like(w, dtype=np.float64))\n\n    next_w = None\n    prev_v = v\n    v = config['momentum'] * v - config['learning_rate'] * dw\n    next_w = w - config['momentum'] * prev_v + (1 + config['momentum']) * v\n    config['velocity'] = v\n\n    return next_w, config\n\n\ndef sgd(w, dw, config=None, p=-1):\n    '''\n    Performs vanilla stochastic gradient descent.\n\n    config format:\n    - learning_rate: Scalar learning rate.\n    '''\n    if config is None:\n        config = {}\n    config.setdefault('learning_rate', 1e-2)\n\n    w -= config['learning_rate'] * dw\n    return w, config\n\n\ndef sgd_momentum(w, dw, config=None):\n    '''\n    Performs stochastic gradient descent with momentum.\n\n    config format:\n    - learning_rate: Scalar learning rate.\n    - momentum: Scalar between 0 and 1 giving the momentum value.\n      Setting momentum = 0 reduces to sgd.\n    - velocity: A numpy array of the same shape as w and dw used to store a moving\n      average of the gradients.\n    '''\n    if config is None:\n        config = {}\n    config.setdefault('learning_rate', 1e-2)\n    config.setdefault('momentum', 0.9)\n    v = config.get('velocity', np.zeros_like(w))\n\n    next_w = None\n    v = config['momentum'] * v + config['learning_rate'] * dw\n    next_w = w - v\n    config['velocity'] = v\n\n    return next_w, config\n\n\n\ndef rmsprop(x, dx, config=None):\n    '''\n    Uses the RMSProp update rule, which uses a moving average of squared gradient\n    values to set adaptive per-parameter learning rates.\n\n    config format:\n    - learning_rate: Scalar learning rate.\n    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared\n      gradient cache.\n    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n    - cache: Moving average of second moments of gradients.\n    '''\n    if config is None:\n        config = {}\n    config.setdefault('learning_rate', 1e-2)\n    config.setdefault('decay_rate', 0.99)\n    config.setdefault('epsilon', 1e-8)\n    config.setdefault('cache', np.zeros_like(x))\n\n    next_x = None\n    cache = config['cache']\n    decay_rate = config['decay_rate']\n    learning_rate = config['learning_rate']\n    cache = decay_rate * cache + (1 - decay_rate) * dx**2\n    x += - learning_rate * dx / (np.sqrt(cache) + 1e-8)\n\n    config['cache'] = cache\n    next_x = x\n\n    return next_x, config\n\n\ndef adam(x, dx, config=None):\n    '''\n    Uses the Adam update rule, which incorporates moving averages of both the\n    gradient and its square and a bias correction term.\n\n    config format:\n    - learning_rate: Scalar learning rate.\n    - beta1: Decay rate for moving average of first moment of gradient.\n    - beta2: Decay rate for moving average of second moment of gradient.\n    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n    - m: Moving average of gradient.\n    - v: Moving average of squared gradient.\n    - t: Iteration number.\n    '''\n    if config is None:\n        config = {}\n    config.setdefault('learning_rate', 1e-3)\n    config.setdefault('beta1', 0.9)\n    config.setdefault('beta2', 0.999)\n    config.setdefault('epsilon', 1e-8)\n    config.setdefault('m', np.zeros_like(x))\n    config.setdefault('v', np.zeros_like(x))\n    config.setdefault('t', 1)\n\n    next_x = None\n    m = config['m']\n    v = config['v']\n    t = config['t']\n    beta1 = config['beta1']\n    beta2 = config['beta2']\n\n    # update parameters\n    learning_rate = config['learning_rate']\n    epsilon = config['epsilon']\n    m = beta1*m + (1-beta1)*dx\n    v = beta2*v + (1-beta2)*dx**2\n    t = t + 1\n    next_x = x - learning_rate*m/(np.sqrt(v) + epsilon)\n\n    # Writing back in config\n    config['m'] = m\n    config['v'] = v\n    config['t'] = t\n\n    return next_x, config\n"""
pyfunt/padding.py,1,"b""from module import Module\nimport numpy as np\n\n\nclass Padding(Module):\n\n    def __init__(self, dim, pad, n_input_dim, value=None, index=None):\n        super(Padding, self).__init__()\n        self.value = value or 0\n        self.index = index or 1\n        self.dim = [dim] if type(dim) == int else dim\n        self.pad = pad if pad > 0 else -pad\n        self.n_input_dim = n_input_dim\n\n    def update_output(self, x):\n        pads = []\n        for axis in range(x.ndim):\n            if axis in self.dim:\n                pads += [(self.pad, self.pad)]\n            else:\n                pads += [(0, 0)]\n        pads = tuple(pads)\n        self.output = np.pad(x, pads, mode='constant')\n        return self.output\n\n    def update_grad_input(self, x, grad_output):\n        slc = [slice(None)] * x.ndim\n        self.grad_input = grad_output\n        for axis in range(x.ndim):\n            if axis in self.dim:\n                slc[axis] = slice(self.pad, -self.pad)\n        self.grad_input = grad_output[slc]\n        return self.grad_input\n\n    def reset(self):\n        pass\n"""
pyfunt/parallel.py,0,"b'from container import Container\n\n\nclass Parallel(Container):\n    """"""docstring for Parallel""""""\n    def __init__(self):\n        super(Parallel, self).__init__()\n\n    def len(self):\n        return len(self.modules)\n\n    def add(self,  module):\n        pass\n\n    def insert(self, modules, module):\n        pass\n\n    def remove(self, index):\n        pass\n\n    def update_output(self, x):\n        pass\n\n    def update_grad_input(self, grad_output):\n        pass\n\n    def acc_grad_parameters(self, grad_output, scale):\n        pass\n\n    def backward(self, grad_output, scale):\n        pass\n\n    def __str__(self):\n        pass\n'"
pyfunt/relu.py,0,"b'from threshold import Threshold\n\n\nclass ReLU(Threshold):\n    def __init__(self, ip=False):\n        super(ReLU, self).__init__(0, 0, ip)\n'"
pyfunt/reshape.py,0,"b'from module import Module\n\n\nclass Reshape(Module):\n\n    def __init__(self, shape):\n        super(Reshape, self).__init__()\n        if type(shape) is not tuple:\n            shape = (shape,)\n        self.shape = shape\n\n    def update_output(self, x):\n        self.output = x.reshape((x.shape[0],) + self.shape)\n        return self.output\n\n    def update_grad_input(self, x, grad_output):\n        self.grad_input = grad_output.reshape(x.shape)\n        return self.grad_input\n\n    def reset(self):\n        pass\n'"
pyfunt/sequential.py,2,"b'from container import Container\nimport numpy as np\n\n\nclass Sequential(Container):\n\n    """"""docstring for Sequential""""""\n\n    def __init__(self):\n        super(Sequential, self).__init__()\n\n    def len(self):\n        return len(self.modules)\n\n    def add(self,  module):\n        if len(self.modules) == 0:\n            self.grad_input = module.grad_input\n        self.modules.append(module)\n        self.output = module.output\n        return self\n\n    def insert(self, module, index=None):\n        index = index or len(self.modules) + 1\n        if index > len(self.modules) + 1 or index < 1:\n            raise Exception(\'index should be contiguous to existing modules\')\n        self.modules.insert(module, index)\n        self.output = self.modules[len(self.modules)].output\n        self.grad_input = self.modules[0].grad_input  # 1??\n\n    def remove(self, index):\n        if index > len(self.modules) or index < 1:\n            raise Exception(\'index out of range\')\n        self.modules.remove(index)\n        if len(self.modules) > 0:\n            self.output = self.modules[-1].output\n            self.grad_input = self.modules[0].grad_input\n        else:\n            self.output = np.ndarray()\n            self.grad_input = np.ndarray()\n\n    def update_output(self, x):\n        current_output = x\n        for i in xrange(len(self.modules)):\n            current_output = self.rethrow_errors(self.modules[i], i, \'update_output\', current_output)\n        self.output = current_output\n        return self.output\n\n    def update_grad_input(self, x, grad_output):\n        current_grad_output = grad_output\n        current_module = self.modules[-1]\n        for i in range(len(self.modules)-2, -1, -1):\n            previous_module = self.modules[i]\n            current_grad_output = self.rethrow_errors(current_module, i, \'update_grad_input\', previous_module.output, current_grad_output)\n            current_module = previous_module\n        current_grad_output = self.rethrow_errors(current_module, 0, \'update_grad_input\', x, current_grad_output)\n        self.grad_input = current_grad_output\n        return current_grad_output\n\n    def acc_grad_parameters(self, x, grad_output, scale=1):\n        current_grad_output = grad_output\n        current_module = self.modules[-1]\n        for i in range(len(self.modules)-2, -1, -1):\n            previous_module = self.modules[i]\n            self.rethrow_errors(current_module, i, \'acc_grad_parameters\', previous_module.output, current_grad_output, scale)\n            current_grad_output = current_module.grad_input\n            current_module = previous_module\n        self.rethrow_errors(current_module, 0, \'acc_grad_parameters\', x, current_grad_output, scale)\n\n    def backward(self, x, grad_output, scale=1):\n        current_grad_output = grad_output\n        current_module = self.modules[-1]\n        for i in range(len(self.modules)-2, -1, -1):\n            previous_module = self.modules[i]\n            current_grad_output = self.rethrow_errors(current_module, i, \'backward\', previous_module.output, current_grad_output, scale)\n            current_module.grad_input[:] = current_grad_output[:]\n            current_module = previous_module\n\n        current_grad_output = self.rethrow_errors(current_module, 0, \'backward\', x, current_grad_output, scale)\n        self.grad_input = current_grad_output\n        return current_grad_output\n\n    def acc_update_grad_parameters(self, x, grad_output, lr):\n        current_grad_output = grad_output\n        current_module = self.modules[-1]\n        for i in range(len(self.modules)-2, -1, -1):\n            previous_module = self.modules[i]\n            self.rethrow_errors(current_module, i, \'acc_update_grad_parameters\', previous_module.output, current_grad_output, lr)\n            current_grad_output = current_module.grad_input\n            current_module = previous_module\n        self.rethrow_errors(current_module, 1, \'acc_update_grad_parameters\', x, current_grad_output, lr)\n\n    def __str__(self):\n        return \'temporary string for Sequential class\'\n'"
pyfunt/setup.py,0,"b""from __future__ import division, print_function, absolute_import\nfrom distutils.core import setup\n\n\ndef configuration(parent_package='', top_path=None):\n    from numpy.distutils.misc_util import Configuration, get_numpy_include_dirs\n\n    config = Configuration('pyfunt', parent_package, top_path)\n    config.add_subpackage('examples')\n    config.add_extension('im2col_cyt',\n                         sources=[('im2col_cyt.c')],\n                         include_dirs=[get_numpy_include_dirs()])\n\n    return config\n\nif __name__ == '__main__':\n    setup(**configuration(top_path='').todict())\n"""
pyfunt/sigmoid.py,1,"b'from module import Module\nimport numpy as np\n\n\nclass Sigmoid(Module):\n\n    def __init__(self):\n        super(Sigmoid, self).__init__()\n\n    def update_output(self, x):\n        self.output = 1 / (1 + np.exp(-x))\n        return self.output\n\n    def update_grad_input(self, x, grad_output):\n        self.grad_input = grad_output * (1.0 - grad_output)\n        return self.grad_input\n'"
pyfunt/soft_max.py,7,"b'from module import Module\nimport numpy as np\n\n\nclass SoftMax(Module):\n    """"""docstring for LogSoftMax""""""\n    def __init__(self):\n        super(SoftMax, self).__init__()\n\n    def update_output(self, x):\n        max_input = x.max(1, keepdims=True)\n        z = np.exp(x - max_input)\n        log_sum = np.sum(z, axis=1, keepdims=True)\n        # log_sum = max_input + np.log(log_sum)\n        self.output = z * 1/log_sum\n        return self.output\n\n    def update_grad_input(self, x, grad_output):\n        _sum = np.sum(grad_output*self.ouput, axis=1, keepdims=True)\n        self.grad_input = self.output * (self.grad_output - _sum)\n\n        # max_input = x.max(1, keepdims=True)\n        # log_sum = np.sum(np.exp(x - max_input), axis=1, keepdims=True)\n        # log_sum = max_input + np.log(log_sum)\n        # self.output = x - log_sum\n\n        # self.grad_input = grad_output - np.exp(self.output)*_sum\n        return self.grad_input\n\n    def reset(self):\n        pass\n'"
pyfunt/solver.py,32,"b'from __future__ import print_function\nimport numpy as np\nfrom datetime import datetime\nimport optim\nimport os\nimport multiprocessing as mp\nimport signal\nfrom copy_reg import pickle\nfrom types import MethodType\nimport sys\nfrom tqdm import tqdm\n\ndef rel_error(x, y):\n    """""" returns relative error """"""\n    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n\n\ndef _pickle_method(method):\n    \'\'\'\n    Helper for multiprocessing ops, for more infos, check answer and comments\n    here:\n    http://stackoverflow.com/a/1816969/1142814\n    \'\'\'\n    func_name = method.im_func.__name__\n    obj = method.im_self\n    cls = method.im_class\n    return _unpickle_method, (func_name, obj, cls)\n\n\ndef _unpickle_method(func_name, obj, cls):\n    \'\'\'\n    Helper for multiprocessing ops, for more infos, check answer and comments\n    here:\n    http://stackoverflow.com/a/1816969/1142814\n    \'\'\'\n    for cls in cls.mro():\n        try:\n            func = cls.__dict__[func_name]\n        except KeyError:\n            pass\n        else:\n            break\n    return func.__get__(obj, cls)\n\n\ndef init_worker():\n    \'\'\'\n    Permit to interrupt all processes trough ^C.\n    \'\'\'\n    signal.signal(signal.SIGINT, signal.SIG_IGN)\n\n\ndef loss_helper(args):\n    model, criterion, x, y = args\n    preds = model.forward(x)\n    loss = criterion.forward(preds, y)\n    dout = criterion.backward(preds, y)\n    _ = model.backward(x, dout)\n    _, grads = model.get_parameters()\n    return loss, grads\n\n\nclass Solver(object):\n\n    \'\'\'\n    A Solver encapsulates all the logic necessary for training classification\n    models. The Solver performs stochastic gradient descent using different\n    update rules defined in optim.py.\n\n    The solver accepts both training and validataion data and labels so it can\n    periodically check classification accuracy on both training and validation\n    data to watch out for overfitting.\n\n    To train a model, you will first construct a Solver instance, passing the\n    model, dataset, and various optoins (learning rate, batch size, etc) to the\n    constructor. You will then call the train() method to run the optimization\n    procedure and train the model.\n\n    After the train() method returns, model.params will contain the parameters\n    that performed best on the validation set over the course of training.\n    In addition, the instance variable solver.loss_history will contain a list\n    of all losses encountered during training and the instance variables\n    solver.train_acc_history and solver.val_acc_history will be lists containing\n    the accuracies of the model on the training and validation set at each epoch.\n\n    Example usage might look something like this:\n\n    data = {\n      \'X_train\': # training data\n      \'y_train\': # training labels\n      \'X_val\': # validation data\n      \'X_train\': # validation labels\n    }\n    model = MyAwesomeModel(hidden_size=100, reg=10)\n    solver = Solver(model, data,\n                    update_rule=\'sgd\',\n                    optim_config={\n                      \'learning_rate\': 1e-3,\n                    },\n                    lr_decay=0.95,\n                    num_epochs=10, batch_size=100,\n                    print_every=100)\n    solver.train()\n\n\n    A Solver works on a model object that must conform to the following API:\n\n    - model.params must be a dictionary mapping string parameter names to numpy\n      arrays containing parameter values.\n\n    - model.loss(X, y) must be a function that computes training-time loss and\n      gradients, and test-time classification scores, with the following inputs\n      and outputs:\n\n      Inputs:\n      - X: Array giving a minibatch of input data of shape (N, d_1, ..., d_k)\n      - y: Array of labels, of shape (N,) giving labels for X where y[i] is the\n        label for X[i].\n\n      Returns:\n      If y is None, run a test-time forward pass and return:\n      - scores: Array of shape (N, C) giving classification scores for X where\n        scores[i, c] gives the score of class c for X[i].\n\n      If y is not None, run a training time forward and backward pass and return\n      a tuple of:\n      - loss: Scalar giving the loss\n      - grads: Dictionary with the same keys as self.params mapping parameter\n        names to gradients of the loss with respect to those parameters.\n    \'\'\'\n\n    def __init__(self, model, data=None, load_dir=None, **kwargs):\n        \'\'\'\n        Construct a new Solver instance.\n\n        Required arguments:\n        - model: A model object conforming to the API described above\n        - data: A dictionary of training and validation data with the following:\n          \'X_train\': Array of shape (N_train, d_1, ..., d_k) giving training images\n          \'X_val\': Array of shape (N_val, d_1, ..., d_k) giving validation images\n          \'y_train\': Array of shape (N_train,) giving labels for training images\n          \'y_val\': Array of shape (N_val,) giving labels for validation images\n\n        Optional arguments: Arguments you also find in the Stanford\'s\n        cs231n assignments\' Solver\n        - update_rule: A string giving the name of an update rule in optim.py.\n          Default is \'sgd_th\'.\n        - optim_config: A dictionary containing hyperparameters that will be\n          passed to the chosen update rule. Each update rule requires different\n          hyperparameters (see optim.py) but all update rules require a\n          \'learning_rate\' parameter so that should always be present.\n        - lr_decay: A scalar for learning rate decay; after each epoch the learning\n          rate is multiplied by this value.\n        - batch_size: Size of minibatches used to compute loss and gradient during\n          training.\n        - num_epochs: The number of epochs to run for during training.\n        Custom arguments:\n        - load_dir: root directory for the checkpoints folder, if is not False,\n          the instance tries to load the most recent checkpoint found in load_dir.\n        - path_checkpoints: root directory where the checkpoints folder resides.\n        - check_point_every: save a checkpoint every check_point_every epochs.\n        - custom_update_ld: optional function to update the learning rate decay\n          parameter, if not False the instruction\n          self.lr_decay = custom_update_ld(self.epoch) is executed at the and\n          of each epoch.\n        - acc_check_train_pre_process: optional function to pre-process the\n          training subset for checking accuracy on training data.\n          If not False acc_check_train_pre_process is called before each\n          accuracy check.\n        - acc_check_val_pre_process: optional function to pre-process the\n          validation data.\n          If not False acc_check_val_pre_process is called on the validation\n          before each accuracy check.\n        - batch_augment_func: optional function to augment the batch data.\n          If not False X_batch = batch_augment_func(X_batch) is called before\n          each training step.\n        - num_processes: optional number of parallel processes for each\n          training step. If not 1, at each training/accuracy_check step, each\n          batch is divided by the number of processes and losses (and grads)\n          are computed in parallel when all processes finish we compute the\n          mean for the loss (and grads) and continue as usual.\n        \'\'\'\n        self.model = model\n        if data:\n            self.X_train = data[\'X_train\']\n            self.y_train = data[\'y_train\']\n            self.X_val = data[\'X_val\']\n            self.y_val = data[\'y_val\']\n\n        # Unpack keyword arguments\n        self.criterion = kwargs.pop(\'criterion\', None)\n        if self.criterion is None:\n            raise(Exception(\'Criterion cannot be None\'))\n\n        self.update_rule = kwargs.pop(\'update_rule\', \'sgd\')\n        self.optim_config = kwargs.pop(\'optim_config\', {})\n        self.learning_rate = self.optim_config[\'learning_rate\']\n        self.lr_decay = kwargs.pop(\'lr_decay\', 1.0)\n        self.batch_size = kwargs.pop(\'batch_size\', 100)\n        self.num_epochs = kwargs.pop(\'num_epochs\', 10)\n\n        # Personal Edits\n        self.path_checkpoints = kwargs.pop(\'path_checkpoints\', \'checkpoints\')\n        self.checkpoint_every = kwargs.pop(\'checkpoint_every\', 0)\n        self.check_and_swap_every = kwargs.pop(\'check_and_swap_every\', 0)\n        self.silent_train = kwargs.pop(\'silent_train\', False)\n        self.custom_update_ld = kwargs.pop(\'custom_update_ld\', False)\n        self.acc_check_train_pre_process = kwargs.pop(\n            \'acc_check_train_pre_process\', False)\n        self.acc_check_val_pre_process = kwargs.pop(\n            \'acc_check_val_pre_process\', False)\n        self.batch_augment_func = kwargs.pop(\'batch_augment_func\', False)\n        self.num_processes = kwargs.pop(\'num_processes\', 1)\n\n        # Throw an error if there are extra keyword arguments\n        if len(kwargs) > 0:\n            extra = \', \'.join(\'""%s""\' % k for k in kwargs.keys())\n            raise ValueError(\'Unrecognized arguments %s\' % extra)\n\n        # Make sure the update rule exists, then replace the string\n        # name with the actual function\n        if not hasattr(optim, self.update_rule):\n            raise ValueError(\'Invalid update_rule ""%s""\' % self.update_rule)\n        self.update_rule = getattr(optim, self.update_rule)\n        self._reset()\n        if load_dir:\n            self.load_dir = load_dir\n            self.load_current_checkpoint()\n\n    def __str__(self):\n        return """"""\n        Number of processes: %d;\n        Update Rule: %s;\n        Optim Config: %s;\n        Learning Rate Decay: %d;\n        Batch Size: %d;\n        Number of Epochs: %d;\n        """""" % (\n               self.num_processes,\n               self.update_rule.__name__,\n               str(self.optim_config),\n               self.lr_decay,\n               self.batch_size,\n               self.num_epochs\n        )\n\n    def _reset(self):\n        \'\'\'\n        Set up some book-keeping variables for optimization. Don\'t call this\n        manually.\n        \'\'\'\n        # Set up some variables for book-keeping\n        self.epoch = 0\n        self.best_val_acc = 0\n        self.best_params = {}\n        self.loss_history = []\n        self.val_acc_history = []\n        self.train_acc_history = []\n        self.pbar = None\n\n        # Make a deep copy of the optim_config for each parameter\n        self.optim_configs = {}\n        self.params, self.grad_params = self.model.get_parameters()\n        # self.weights, _ = self.model.get_parameters()\n        for p in range(len(self.params)):\n            d = {k: v for k, v in self.optim_config.iteritems()}\n            self.optim_configs[p] = d\n\n        self.multiprocessing = bool(self.num_processes-1)\n        if self.multiprocessing:\n            self.pool = mp.Pool(self.num_processes, init_worker)\n\n    def load_current_checkpoint(self):\n        \'\'\'\n        Return the current checkpoint\n        \'\'\'\n        checkpoints = [f for f in os.listdir(\n            self.load_dir) if not f.startswith(\'.\')]\n\n        try:\n            num = max([int(f.split(\'_\')[1]) for f in checkpoints])\n            name = \'check_\' + str(num)\n            try:\n                cp = np.load(\n                    os.path.join(self.path_checkpoints, name, name + \'.pkl\'))\n            except:\n                print(\'sorry, I haven\\\'t fixed this line, but it should be easy to fix, if you want you can try now and make a pull request\')\n                raise()\n            # Set up some variables for book-keeping\n\n            self.epoch = cp[\'epoch\']\n            self.best_val_acc = cp[\'best_val_acc\']\n            self.best_params = cp[\'best_params\']\n            self.loss_history = cp[\'loss_history\']\n            self.val_acc_history = cp[\'val_acc_history\']\n            self.train_acc_history = cp[\'train_acc_history\']\n            self.model = cp[\'model\']\n\n        except Exception, e:\n            raise e\n\n    def make_check_point(self):\n        \'\'\'\n        Save the solver\'s current status\n        \'\'\'\n        checkpoints = {\n            \'model\': self.model,\n            \'epoch\': self.epoch,\n            \'best_params\': self.best_params,\n            \'best_val_acc\': self.best_val_acc,\n            \'loss_history\': self.loss_history,\n            \'val_acc_history\': self.val_acc_history,\n            \'train_acc_history\': self.train_acc_history}\n\n        name = \'check_\' + str(self.epoch)\n        directory = os.path.join(self.path_checkpoints, name)\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n        try:\n            np.save(checkpoints, os.path.join(\n                directory, name + \'.pkl\'))\n        except:\n            print(\'sorry, I haven\\\'t fixed this line, but it should be easy to fix, if you want you can try now and make a pull request\')\n            raise()\n\n    def export_model(self, path):\n        if not os.path.exists(path):\n            os.makedirs(path)\n        np.save(\'%smodel\' % path, self.best_params)\n\n    def export_histories(self, path):\n        if not os.path.exists(path):\n            os.makedirs(path)\n        i = np.arange(len(self.loss_history)) + 1\n        z = np.array(zip(i, i*self.batch_size, self.loss_history))\n        np.savetxt(path + \'loss_history.csv\', z, delimiter=\',\', fmt=[\n                   \'%d\', \'%d\', \'%f\'], header=\'iteration, n_images, loss\')\n\n        i = np.arange(len(self.train_acc_history), dtype=np.int)\n\n        z = np.array(zip(i, self.train_acc_history))\n        np.savetxt(path + \'train_acc_history.csv\', z, delimiter=\',\', fmt=[\n            \'%d\', \'%f\'], header=\'epoch, train_acc\')\n\n        z = np.array(zip(i, self.val_acc_history))\n        np.savetxt(path + \'val_acc_history.csv\', z, delimiter=\',\', fmt=[\n            \'%d\', \'%f\'], header=\'epoch, val_acc\')\n        np.save(path + \'loss\', self.loss_history)\n        np.save(path + \'train_acc_history\', self.train_acc_history)\n        np.save(path + \'val_acc_history\', self.val_acc_history)\n\n    def _loss_helper(self, args):\n        x, y = args\n        preds = self.model.forward(x)\n        loss = self.criterion.forward(preds, y)\n        dout = self.criterion.backward(preds, y)\n        self.model.backward(x, dout)\n        return loss, self.grad_params\n\n    def _step(self):\n        \'\'\'\n        Make a single gradient update. This is called by train() and should not\n        be called manually.\n        \'\'\'\n        # Make a minibatch of training data\n        num_train = self.X_train.shape[0]\n        batch_mask = np.random.choice(num_train, self.batch_size)\n        X_batch = self.X_train[batch_mask]\n        y_batch = self.y_train[batch_mask]\n\n        if not self.multiprocessing:\n            # pred = model.forward(X_batch)\n            # loss = self.criterion.forward(pred, y_batch)\n            loss, grads = self._loss_helper((X_batch, y_batch))\n        else:\n            n = self.num_processes\n            pool = self.pool\n\n            X_batches = np.split(X_batch, n)\n            # sub_weights = np.array([len(x)\n            #                         for x in X_batches], dtype=np.float32)\n            # sub_weights /= sub_weights.sum()\n\n            y_batches = np.split(y_batch, n)\n            try:\n                job_args = [(self.model, self.criterion, X_batches[i], y_batches[i]) for i in range(n)]\n                results = pool.map_async(loss_helper, job_args).get()\n                losses = np.zeros(len(results))\n                gradses = []\n                i = 0\n                for i, r in enumerate(results):\n                    l, g = r\n                    losses[i] = l\n                    gradses.append(g)\n                    i += 1\n            except Exception, e:\n                self.pool.terminate()\n                self.pool.join()\n                raise e\n            loss = np.mean(losses)\n            grads = []\n            for p, w in enumerate(gradses[0]):\n                grad = np.mean([grad[p] for grad in gradses], axis=0)\n                grads.append(grad)\n                self.grad_params[p][:] = grad\n\n        self.loss_history.append(loss)\n        return loss, grads\n\n    def eval_model(self, X, y, num_samples=None, batch_size=100, return_preds=False):\n        \'\'\'\n        Check accuracy of the model on the provided data.\n\n        Inputs:\n        - X: Array of data, of shape (N, d_1, ..., d_k)\n        - y: Array of labels, of shape (N,)\n        - num_samples: If not None, subsample the data and only test the model\n          on num_samples datapoints. TODO\n        - batch_size: Split X and y into batches of this size to avoid using too\n          much memory. TODO\n        - return_preds: if True returns predictions probabilities\n\n        Returns:\n        - acc: Scalar giving the fraction of instances that were correctly\n          classified by the model.\n        \'\'\'\n        N = X.shape[0]\n        batch_size = self.batch_size\n        num_batches = N / batch_size\n        if N % batch_size != 0:\n            num_batches += 1\n        y_pred1 = []\n        y_pred5 = []\n        self.pbar = tqdm(total=N, desc=\'Accuracy Check\', unit=\'im\')\n        for i in xrange(num_batches):\n            start = i * batch_size\n            end = (i + 1) * batch_size\n\n            if not self.multiprocessing:\n                scores = self.model.forward(X[start:end])\n                y_pred1.append(np.argmax(scores, axis=1))\n                y_pred5.append(scores.argsort()[-5:][::-1])\n            else:\n                n = self.num_processes\n                pool = self.pool\n                X_batches = np.split(X[start:end], n)\n                try:\n                    results = pool.map_async(self.model.forward, X_batches).get()\n                    scores = np.vstack(results)\n                    y_pred1.append(np.argmax(scores, axis=1))\n                    y_pred5.append(scores.argsort()[-5:][::-1])\n\n                except Exception, e:\n                    self.pool.terminate()\n                    self.pool.join()\n                    raise e\n\n            self.pbar.update(end - start)\n        print()\n        y_pred1 = np.hstack(y_pred1)\n        if return_preds:\n            return y_pred1\n        acc1 = np.mean(y_pred1 == y)\n        acc5 = np.mean(np.any(y_pred5 == y))\n        return acc1, acc5\n\n    def _check_and_swap(self, it=0):\n        \'\'\'\n        Check accuracy for both X_train[:1000] and X_val.\n        \'\'\'\n        if self.acc_check_train_pre_process:\n            X_tr_check = self.acc_check_train_pre_process(self.X_train[:1000])\n        else:\n            X_tr_check = self.X_train[:1000]\n        if self.acc_check_val_pre_process:\n            X_val_check = self.acc_check_val_pre_process(self.X_val)\n        else:\n            X_val_check = self.X_val\n\n        train_acc, val_acc = 0, 0\n\n        train_acc, _ = self.eval_model(\n            X_tr_check, self.y_train[:1000])\n        val_acc, _ = self.eval_model(X_val_check, self.y_val)\n\n        self.train_acc_history.append(train_acc)\n        self.val_acc_history.append(val_acc)\n\n        self.emit_sound()\n        # Keep track of the best model\n        if val_acc > self.best_val_acc:\n            self.best_val_acc = val_acc\n            # self.best_params = {}\n            for p, w in enumerate(self.params):\n                self.best_params[p] = w.copy()\n            # for k, v in self.model.params.iteritems():\n                # self.best_params[k] = v.copy()\n\n        loss = \'%.4f\' % self.loss_history[it-1] if it > 0 else \'-\'\n        print(\'%s - iteration %d: loss:%s, train_acc:%.4f, val_acc: %.4f, best_val_acc: %.4f;\\n\' % (\n            # print(\'%s - iteration %d: loss:%s, train_acc: %.4f, val_acc: %.4f, best_val_acc: %.4f;\\n\' % ()\n            # str(datetime.now()), it, loss, val_acc, self.best_val_acc)\n            str(datetime.now()), it, loss, train_acc, val_acc, self.best_val_acc))\n\n    def _new_training_bar(self, total):\n        \'\'\'\n        Create a new loading bar.\n        \'\'\'\n        if not self.silent_train:\n            d = \'Epoch %d / %d\' % (\n                self.epoch + 1, self.num_epochs)\n            self.pbar = tqdm(total=total, desc=d, unit=\'s.\')\n\n    def _update_bar(self, amount):\n        if not self.silent_train:\n            self.pbar.update(amount)\n\n    def train(self):\n        \'\'\'\n        Run optimization to train the model.\n        \'\'\'\n        num_train = self.X_train.shape[0]\n        iterations_per_epoch = int(np.ceil(num_train / float(self.batch_size)))\n        images_per_epochs = iterations_per_epoch * self.batch_size\n        num_iterations = self.num_epochs * iterations_per_epoch\n\n        print(\'Training for %d epochs (%d iterations).\\n\' %\n              (self.num_epochs, num_iterations))\n        epoch_end = True\n        lr_decay_updated = False\n        self._check_and_swap()\n        self._new_training_bar(images_per_epochs)\n        # self.params, self.grad_params = self.model.get_parameters()\n        self.best_params = np.copy(self.params)\n        for it in xrange(num_iterations):\n\n            loss, _ = self._step()\n\n            # self.loss_history.append(loss)\n\n            # Perform a parameter update\n            # self.model.params.iteritems():\n            for p, w in enumerate(self.params):\n                dw = self.grad_params[p]\n                config = self.optim_configs[p]\n                next_w, next_config = self.update_rule(w, dw, config)\n                self.params[p][:] = next_w[:]\n                self.optim_configs[p] = next_config\n\n            self.pbar.update(self.batch_size)\n\n            epoch_end = (it + 1) % iterations_per_epoch == 0\n\n            if epoch_end:\n                print()\n                self.epoch += 1\n\n                if self.custom_update_ld:\n                    self.lr_decay = self.custom_update_ld(self.epoch)\n                    lr_decay_updated = self.lr_decay != 1\n\n                for k in self.optim_configs:\n                    self.optim_configs[k][\'learning_rate\'] *= self.lr_decay\n\n                if self.checkpoint_every and (self.epoch % self.checkpoint_every == 0):\n                    self.make_check_point()\n\n                if not self.check_and_swap_every or (self.epoch % self.check_and_swap_every == 0):\n                    self._check_and_swap(it)\n\n                finish = it == num_iterations - 1\n                if not finish:\n                    if lr_decay_updated:\n                        print(\'learning_rate updated: \', next(\n                            self.optim_configs.itervalues())[\'learning_rate\'])\n                        lr_decay_updated = False\n                    print()\n                    self._new_training_bar(images_per_epochs)\n\n        # At the end of training swap the best params into the model\n        self.params[:] = self.best_params[:]\n        if self.multiprocessing:\n            try:\n                self.pool.terminate()\n                self.pool.join()\n            except:\n                pass\n\n    def emit_sound(self):\n        \'\'\'\n        Emit sound when epoch end.\n        \'\'\'\n        sys.stdout.write(\'\\a\')\n\n\n# again, check http://stackoverflow.com/a/1816969/1142814 and comments\npickle(MethodType, _pickle_method, _unpickle_method)\n'"
pyfunt/spatial_average_pooling.py,5,"b'from module import Module\nimport numpy as np\n\ntry:\n    from im2col_cyt import im2col_cython, col2im_cython\nexcept ImportError:\n    print(\'Installation broken, please reinstall PyFunt\')\n\n\nclass SpatialAveragePooling(Module):\n\n    """"""docstring for SpatialAveragePooling""""""\n\n    def __init__(self, kW, kH, dW=1, dH=1, padW=0, padH=0):\n        super(SpatialAveragePooling, self).__init__()\n        self.kW = kW\n        self.kH = kH\n        self.dW = dW\n        self.dH = dH\n        self.padW = padW\n        self.padH = padH\n        self.ceil_mode = False\n        self.count_include_pad = True\n        self.divide = True\n\n    def reset(self):\n        #TODO\n        pass\n\n    def ceil(self):\n        self.ceil_mode = True\n\n    def floor(self):\n        self.ceil_mode = False\n\n    def set_count_include_pad(self):\n        self.count_include_pad = True\n\n    def set_count_exclude_pad(self):\n        self.count_include_pad = False\n\n    def update_output(self, x):\n        N, C, H, W = x.shape\n        pool_height, pool_width = self.kW, self.kH\n        stride = self.dW\n\n        assert (\n            H - pool_height) % stride == 0 or H == pool_height, \'Invalid height\'\n        assert (\n            W - pool_width) % stride == 0 or W == pool_width, \'Invalid width\'\n\n        out_height = int(np.floor((H - pool_height) / stride + 1))\n        out_width = int(np.floor((W - pool_width) / stride + 1))\n\n        x_split = x.reshape(N * C, 1, H, W)\n        x_cols = im2col_cython(\n            x_split, pool_height, pool_width, padding=0, stride=stride)\n        x_cols_avg = np.mean(x_cols, axis=0)\n        out = x_cols_avg.reshape(\n            out_height, out_width, N, C).transpose(2, 3, 0, 1)\n\n        self.x_shape = x.shape\n        self.x_cols = x_cols\n        self.output = out\n        return self.output\n\n    def update_grad_input(self, x, grad_output, scale=1):\n        x_cols = self.x_cols\n        dout = grad_output\n        N, C, H, W = self.x_shape\n        pool_height, pool_width = self.kW, self.kH\n        stride = self.dW\n        pool_dim = pool_height * pool_width\n\n        dout_reshaped = dout.transpose(2, 3, 0, 1).flatten()\n        dx_cols = np.zeros_like(x_cols)\n        dx_cols[:, np.arange(dx_cols.shape[1])] = 1. / pool_dim * dout_reshaped\n        dx = col2im_cython(dx_cols, N * C, 1, H, W, pool_height, pool_width,\n                           padding=0, stride=stride)\n\n        self.grad_input = dx.reshape(self.x_shape)\n\n        return self.grad_input\n\n    def __str__(self):\n        pass\n'"
pyfunt/spatial_batch_normalitazion.py,2,"b'#!/usr/bin/env python\n# coding: utf-8\nimport numpy as np\nfrom batch_normalization import BatchNormalization\n\n\nclass SpatialBatchNormalization(BatchNormalization):\n    n_dim = 4\n\n    def __init__(self, *args):\n        super(SpatialBatchNormalization, self).__init__(*args)\n\n    def update_output(self, x):\n        N, C, H, W = x.shape\n        x_flat = x.transpose(0, 2, 3, 1).reshape(-1, C)\n        x_flat = np.ascontiguousarray(x_flat, dtype=x.dtype)\n        super(SpatialBatchNormalization, self).update_output(x_flat)\n        self.output = self.output.reshape(N, H, W, C).transpose(0, 3, 1, 2)\n        return self.output\n\n    def update_grad_input(self, x, grad_output, scale=1):\n        N, C, H, W = grad_output.shape\n        dout_flat = grad_output.transpose(0, 2, 3, 1).reshape(-1, C)\n        dout_flat = np.ascontiguousarray(dout_flat, dtype=dout_flat.dtype)\n        super(SpatialBatchNormalization, self).update_grad_input(x, dout_flat, scale)\n        self.grad_input = self.grad_input.reshape(N, H, W, C).transpose(0, 3, 1, 2)\n        return self.grad_input\n\n    def backward(self, x, grad_output, scale=1):\n        return self.update_grad_input(x, grad_output, scale)\n'"
pyfunt/spatial_convolution.py,13,"b""#!/usr/bin/env python\n# coding: utf-8\n\nfrom module import Module\nimport numpy as np\ntry:\n    from im2col_cyt import col2im_cython\nexcept ImportError:\n    print('Installation broken, please reinstall PyFunt')\n\n\nclass SpatialConvolution(Module):\n\n    n_dim = 2\n\n    def __init__(self, n_input_plane, n_output_plane, kW, kH, dW=1, dH=1, padW=0, padH=0):\n        super(SpatialConvolution, self).__init__()\n\n        self.n_input_plane = n_input_plane\n        self.n_output_plane = n_output_plane\n        self.kW = kW\n        self.kH = kH\n\n        self.dW = dW\n        self.dH = dH\n        self.padW = padW\n        self.padH = padH or self.padW\n\n        self.weight = np.ndarray((n_output_plane, n_input_plane, kH, kW))\n        self.bias = np.ndarray(n_output_plane)\n        self.grad_weight = np.ndarray((n_output_plane, n_input_plane, kH, kW))\n        self.grad_bias = np.ndarray(n_output_plane)\n\n        self.reset()\n\n    def no_bias(self):\n        self.bias = None\n        self.grad_bias = None\n\n    def reset(self, stdv=None):\n        if not stdv:\n            stdv = 1/np.sqrt(self.kW*self.kH*self.n_input_plane)\n        self.weight = np.random.normal(\n            0, stdv, (self.n_output_plane, self.n_input_plane, self.kH, self.kW))\n        self.bias = np.zeros(self.n_output_plane)\n\n    def check_input_dim(self, x):\n        pass\n\n    def make_contigous(self, input, grad_output):\n        pass\n\n    def update_output(self, x):\n        w, b = self.weight, self.bias\n        # input = make_contigous (input)N, C, H, W = x.shape\n        self.x_shape = N, C, H, W = x.shape\n\n        F, _, HH, WW = w.shape\n        stride, pad = self.dW, self.padW\n        #assert (W + 2 * pad - WW) % stride == 0, 'width does not work'\n        #assert (H + 2 * pad - HH) % stride == 0, 'height does not work'\n\n        p = pad\n        x_padded = np.pad(\n            x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n\n        self.tiles_w = (W + (2 * pad) - WW) % stride\n        self.tiles_h = (H + (2 * pad) - HH) % stride\n        if not self.tiles_w == 0:\n            x_padded = x_padded[:, :, :, :-self.tiles_w]\n        if not self.tiles_h == 0:\n            x_padded = x_padded[:, :, :-self.tiles_h, :]\n\n        N, C, H, W = x_padded.shape\n        if (W + (2 * pad) - WW) % stride != 0:\n            raise Exception('width does not work')\n\n        # H += 2 * pad\n        # W += 2 * pad\n        out_h = (H - HH) / stride + 1\n        out_w = (W - WW) / stride + 1\n\n        # Perform an im2col operation by picking clever strides\n        shape = (C, HH, WW, N, out_h, out_w)\n        strides = (H * W, W, 1, C * H * W, stride * W, stride)\n        strides = x.itemsize * np.array(strides)\n        x_stride = np.lib.stride_tricks.as_strided(x_padded,\n                                                   shape=shape, strides=strides)\n        x_cols = np.ascontiguousarray(x_stride)\n        x_cols.shape = (C * HH * WW, N * out_h * out_w)\n\n        # Now all our convolutions are a big matrix multiply\n        res = w.reshape(F, -1).dot(x_cols) + b.reshape(-1, 1)\n\n        res.shape = (F, N, out_h, out_w)\n        out = res.transpose(1, 0, 2, 3)\n\n        self.output = np.ascontiguousarray(out)\n\n        self.x_cols = x_cols\n        return self.output\n\n    def update_grad_input(self, input, grad_output, scale=1):\n        x_shape, x_cols = self.x_shape, self.x_cols\n        w = self.weight\n\n        stride, pad = self.dW, self.padW\n\n        N, C, H, W = x_shape\n        F, _, HH, WW = w.shape\n        _, _, out_h, out_w = grad_output.shape\n\n        self.grad_bias[:] = np.sum(grad_output, axis=(0, 2, 3))[:]\n\n        dout_reshaped = grad_output.transpose(1, 0, 2, 3).reshape(F, -1)\n        self.grad_weight[:] = dout_reshaped.dot(x_cols.T).reshape(w.shape)[:]\n\n        dx_cols = w.reshape(F, -1).T.dot(dout_reshaped)\n        #dx_cols.shape = (C, HH, WW, N, out_h, out_w)\n        # dx = col2im_6d_cython(dx_cols, N, C, H, W, HH, WW, pad, stride)\n        dx = col2im_cython(dx_cols, N, C, H, W, HH, WW, pad, stride)\n        self.grad_input = dx\n        return dx\n\n    def type(self, type, cache):\n        pass\n\n    def __str__(self):\n        pass\n\n    def clear_state(self):\n        pass\n"""
pyfunt/spatial_full_convolution.py,14,"b'#!/usr/bin/env python\n# coding: utf-8\n\nfrom module import Module\nimport numpy as np\ntry:\n    from im2col_cyt import col2im_cython\nexcept ImportError:\n    print(\'Installation broken, please reinstall PyFunt\')\n\n\nclass SpatialFullConvolution(Module):\n\n    \'\'\'implementation of layer described in https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf""\'\'\'\n    n_dim = 2\n\n    def __init__(self, n_input_plane, n_output_plane, kW, kH, dW=1, dH=1, padW=0, padH=0, adjW=0, adjH=0):\n        super(SpatialFullConvolution, self).__init__()\n\n        self.n_input_plane = n_input_plane\n        self.n_output_plane = n_output_plane\n        self.kW = kW\n        self.kH = kH\n\n        self.dW = dW\n        self.dH = dH\n        if padH != padW or dH != dW:\n            raise Exception(\'padH != padW or dH != dW, behaviout not implemented \')\n        self.padW = padW\n        self.padH = padH or self.padW\n        self.adjW = adjW\n        self.adjH = adjH\n\n        if self.adjW > self.dW - 1 or self.adjH > self.dH - 1:\n            raise Exception(\n                \'adjW and adjH must be smaller than self.dW - 1 and self.dH - 1 respectively\')\n\n        self.weight = np.ndarray((n_input_plane, n_output_plane, kH, kW))\n        self.bias = np.ndarray(n_output_plane)\n        self.grad_weight = np.ndarray((n_input_plane, n_output_plane, kH, kW))\n        self.grad_bias = np.ndarray(n_output_plane)\n\n        self.reset()\n\n    def no_bias(self):\n        self.bias = None\n        self.grad_bias = None\n\n    def reset(self, stdv=None):\n        if not stdv:\n            stdv = 1/np.sqrt(self.kW*self.kH*self.n_input_plane)\n        self.weight = np.random.normal(\n            0, stdv, (self.n_output_plane, self.n_input_plane, self.kH, self.kW))\n        self.bias = np.zeros(self.n_output_plane)\n\n    def check_input_dim(self, x):\n        pass\n\n    def make_contigous(self, input, grad_output):\n        pass\n\n    def calcula_adj(self, target_size, ker, pad, stride):\n        return (target_size + 2 * pad - ker) % stride\n\n    def update_output(self, x):\n\n        w = self.weight\n        F, FF, HH, WW = w.shape\n\n        stride, pad = self.dW, self.padW\n        N, in_C, inH, inW = x.shape\n        C = self.n_output_plane\n        W = (inW - 1) * self.dW - 2*self.padW + WW  # x_shape\n        H = (inH - 1) * self.dH - 2*self.padH + HH  # x_shape\n        _, _, in_h, in_w = x.shape\n        #assert (H + 2 * pad - HH) % stride == 0, \'height does not work\'\n        x_reshaped = x.transpose(1, 0, 2, 3).reshape(F, -1)\n        out_cols = w.reshape(F, -1).T.dot(x_reshaped)\n        # out_cols.shape = (C, HH, WW, N, in_h, in_w)\n        self.output = col2im_cython(out_cols, N, C, H, W, HH, WW, pad, stride)\n        self.output += self.bias.reshape(1, -1, 1, 1)\n        if self.adjH:\n            self.output = np.pad(\n                self.output, ((0, 0), (0, 0), (0, self.adjH), (0, 0)), mode=\'constant\')\n        if self.adjW:\n            self.output = np.pad(\n                self.output, ((0, 0), (0, 0), (0, 0), (0, self.adjW)), mode=\'constant\')\n        return self.output\n\n\n\n        # w, b = self.weight, self.bias\n        # # input = make_contigous (input)N, C, H, W = x.shape\n        # N, C, H, W = x.shape\n        # outW = (W - 1) * self.dW - 2*self.padW + self.kW + self.adjW\n\n        # F, FF, HH, WW = w.shape\n        # stride, pad = self.dW, self.padW\n\n        # p = pad\n        # x = np.pad(\n        #     x, ((0, 0), (0, 0), (p, p), (p, p)), mode=\'constant\')\n\n        # self.tiles_w = (W + 2 * pad - WW) % stride\n        # self.tiles_h = (H + 2 * pad - HH) % stride\n\n        # stride, pad = self.dW, self.padW\n\n        # out_w  = (W - 1) * self.dW - 2*self.padW + WW;\n        # out_h = (H - 1) * self.dH - 2*self.padH + HH;\n        # _, _, out_h, out_w = x.shape\n        # import pdb; pdb.set_trace()\n        # x_reshaped = x.transpose(1, 0, 2, 3).reshape(F, -1)\n        # out_cols = w.reshape(F, -1).T.dot(x_reshaped)# + b.reshape(-1, 1)\n        # out_cols.shape = (self.n_output_plane, -1)\n        # b_reshaped = b.reshape(self.n_output_plane, -1)\n        # out_cols += b_reshaped\n        # out_cols.shape = (self.n_output_plane, HH, WW, N, out_h, out_w)\n        # #out_cols.shape = (C, HH, WW, N, out_h, out_w)\n        # self.output = col2im_6d_cython(out_cols, N, self.n_output_plane, H, W, HH, WW, pad, stride)\n\n        # if self.output.shape[1] != self.n_output:\n        #     import pdb; pdb.set_trace()\n\n    def update_grad_input(self, input, grad_output, scale=1):\n        raise NotImplementedError\n        # TODO THIS IS BROKEN FIXME PLEASE :()\n        w = self.bias\n        F, _, HH, WW = w.shape\n        stride = self.stride\n\n        if not self.adjH == 0:\n            grad_output = grad_output[:, :, :-self.adjH, :]\n        if not self.adjW == 0:\n            grad_output = grad_output[:, :, :, :-self.adjW]\n\n        N, C, H, W = grad_output.shape\n\n        # H += 2 * pad\n        # W += 2 * pad\n        out_h = (H - HH) / stride + 1\n        out_w = (W - WW) / stride + 1\n\n        # Perform an im2col operation by picking clever strides\n        shape = (C, HH, WW, N, out_h, out_w)\n        strides = (H * W, W, 1, C * H * W, stride * W, stride)\n        strides = grad_output.itemsize * np.array(strides)\n        dout_stride = np.lib.stride_tricks.as_strided(\n            grad_output, shape=shape, strides=strides)\n        dout_cols = np.ascontiguousarray(dout_stride)\n        dout_cols.shape = (C * HH * WW, N * out_h * out_w)\n\n        # Now all our convolutions are a big matrix multiply\n        res = w.reshape(F, -1).dot(dout_cols)\n\n        res.shape = (F, N, out_h, out_w)\n        out = res.transpose(1, 0, 2, 3)\n\n        self.grad_input = np.ascontiguousarray(out)\n        return self.grad_input\n\n    def type(self, type, cache):\n        pass\n\n    def __str__(self):\n        pass\n\n    def clear_state(self):\n        pass\n'"
pyfunt/spatial_max_pooling.py,4,"b'from module import Module\nimport numpy as np\ntry:\n    from im2col_cyt import im2col_cython, col2im_cython\nexcept ImportError:\n    print(\'Installation broken, please reinstall PyFunt\')\n\n\nclass SpatialMaxPooling(Module):\n\n    """"""docstring for SpatialMaxPooling""""""\n\n    def __init__(self, kW, kH, dW=1, dH=1, padW=0, padH=0):\n        super(SpatialMaxPooling, self).__init__()\n        self.kW = kW\n        self.kH = kH\n        self.dW = dW\n        self.dH = dH\n        self.padW = padW\n        self.padH = padH\n        self.ceil_mode = False\n        self.count_include_pad = True\n        self.divide = True\n\n    def ceil(self):\n        # TODO:\n        self.ceil_mode = True\n\n    def floor(self):\n        # TODO:\n        self.ceil_mode = False\n\n    def set_count_include_pad(self):\n        # TODO:\n        self.count_include_pad = True\n\n    def set_count_exclude_pad(self):\n        # TODO:\n        self.count_include_pad = False\n\n    def update_output(self, x):\n        N, C, H, W = x.shape\n        pool_height, pool_width = self.kW, self.kH\n        stride = self.dW\n\n        assert (H - pool_height) % stride == 0, \'Invalid height\'\n        assert (W - pool_width) % stride == 0, \'Invalid width\'\n\n        out_height = (H - pool_height) / stride + 1\n        out_width = (W - pool_width) / stride + 1\n\n        x_split = x.reshape(N * C, 1, H, W)\n        x_cols = im2col_cython(\n            x_split, pool_height, pool_width, padding=0, stride=stride)\n        x_cols_argmax = np.argmax(x_cols, axis=0)\n        x_cols_max = x_cols[x_cols_argmax, np.arange(x_cols.shape[1])]\n        out = x_cols_max.reshape(\n            out_height, out_width, N, C).transpose(2, 3, 0, 1)\n\n        self.x_shape = x.shape\n        self.x_cols = x_cols\n        self.x_cols_argmax = x_cols_argmax\n        self.output = out\n        return self.output\n\n    def update_grad_input(self, x, grad_output, scale=1):\n        x_cols = self.x_cols\n        x_cols_argmax = self.x_cols_argmax\n        dout = grad_output\n        N, C, H, W = x.shape\n        pool_height, pool_width = self.kW, self.kH\n        stride = self.dW\n\n        dout_reshaped = dout.transpose(2, 3, 0, 1).flatten()\n        dx_cols = np.zeros_like(x_cols)\n        dx_cols[x_cols_argmax, np.arange(dx_cols.shape[1])] = dout_reshaped\n        dx = col2im_cython(dx_cols, N * C, 1, H, W, pool_height, pool_width,\n                           padding=0, stride=stride)\n        dx = dx.reshape(self.x_shape)\n        self.grad_input = dx\n        return self.grad_input\n\n    def reset(self):\n        pass\n\n    def __str__(self):\n        pass\n'"
pyfunt/spatial_reflection_padding.py,2,"b""from module import Module\nimport numpy as np\n\n\nclass SpatialReflectionPadding(Module):\n\n    def __init__(self, pad_l, pad_r=None, pad_t=None, pad_b=None):\n        super(SpatialReflectionPadding, self).__init__()\n        self.pad_l = pad_l\n        self.pad_r = pad_r or self.pad_l\n        self.pad_t = pad_t or self.pad_l\n        self.pad_b = pad_b or self.pad_l\n\n    def update_output(self, x):\n        if x.ndim == 3:\n            self.output = np.pad(\n                x, ((0, 0), (self.pad_t, self.pad_b), (self.pad_l, self.pad_r)), 'reflect')\n        elif x.ndim == 4:\n            self.output = np.pad(\n                x, ((0, 0), (0, 0), (self.pad_t, self.pad_b), (self.pad_l, self.pad_r)), 'reflect')\n\n        else:\n            raise Exception('input must be 3 or 4-dimensional')\n        return self.output\n\n    def update_grad_input(self, x, grad_output):\n        if x.ndim == grad_output.ndim == 3:\n            if not (x.shape[0] == grad_output.shape[0] and\n                    x.shape[1] + self.pad_t + self.pad_b == grad_output.shape[1] and\n                    x.shape[2] + self.pad_l + self.pad_r == grad_output.shape[2]):\n                raise Exception('input and gradOutput must be compatible in size')\n            self.grad_input = grad_output[:, self.pad_t:self.pad_b, self.pad_l:self.pad_r]\n        elif x.ndim == grad_output.ndim == 4:\n            if not (x.shape[0] == grad_output.shape[0] and\n                    x.shape[1] == grad_output.shape[1] and\n                    x.shape[2] + self.pad_t + self.pad_b == grad_output.shape[2] and\n                    x.shape[3] + self.pad_l + self.pad_r == grad_output.shape[3]):\n                raise Exception('input and gradOutput must be compatible in size')\n            self.grad_input = grad_output[:, :, self.pad_t:self.pad_b, self.pad_l:self.pad_r]\n        else:\n            raise Exception(\n                'input and gradOutput must be 3 or 4-dimensional and have equal number of dimensions')\n        return self.grad_input\n\n    def __str__(self):\n        return str(type(self)) + '(l=%d, r=%d, t=%d, b=%d)' % (self.pad_l, self.pad_r, self.pad_t, self.pad_b)\n"""
pyfunt/spatial_replication_padding.py,2,"b""from module import Module\nimport numpy as np\n\n\nclass SpatialReplicationPadding(Module):\n\n    def __init__(self, pad_l, pad_r=None, pad_t=None, pad_b=None):\n        super(SpatialReplicationPadding, self).__init__()\n        self.pad_l = pad_l\n        self.pad_r = pad_r or self.pad_l\n        self.pad_t = pad_t or self.pad_l\n        self.pad_b = pad_b or self.pad_l\n\n    def update_output(self, x):\n        if x.ndim == 3:\n            self.output = np.pad(\n                x, ((0, 0), (self.pad_t, self.pad_b), (self.pad_l, self.pad_r)), 'edge')\n        elif x.ndim == 4:\n            self.output = np.pad(\n                x, ((0, 0), (0, 0), (self.pad_t, self.pad_b), (self.pad_l, self.pad_r)), 'edge')\n\n        else:\n            raise Exception('input must be 3 or 4-dimensional')\n        return self.output\n\n    def update_grad_input(self, x, grad_output):\n        if x.ndim == grad_output.ndim == 3:\n            if not (x.shape[0] == grad_output.shape[0] and\n                    x.shape[1] + self.pad_t + self.pad_b == grad_output.shape[1] and\n                    x.shape[2] + self.pad_l + self.pad_r == grad_output.shape[2]):\n                raise Exception('input and gradOutput must be compatible in size')\n            self.grad_input = grad_output[:, self.pad_t:self.pad_b, self.pad_l:self.pad_r]\n        elif x.ndim == grad_output.ndim == 4:\n            if not (x.shape[0] == grad_output.shape[0] and\n                    x.shape[1] == grad_output.shape[1] and\n                    x.shape[2] + self.pad_t + self.pad_b == grad_output.shape[2] and\n                    x.shape[3] + self.pad_l + self.pad_r == grad_output.shape[3]):\n                raise Exception('input and gradOutput must be compatible in size')\n            self.grad_input = grad_output[:, :, self.pad_t:self.pad_b, self.pad_l:self.pad_r]\n        else:\n            raise Exception(\n                'input and gradOutput must be 3 or 4-dimensional and have equal number of dimensions')\n        return self.grad_input\n\n    def __str__(self):\n        return str(type(self)) + '(l=%d, r=%d, t=%d, b=%d)' % (self.pad_l, self.pad_r, self.pad_t, self.pad_b)\n"""
pyfunt/spatial_up_sampling_nearest.py,4,"b""#!/usr/bin/env python\n# coding: utf-8\nfrom module import Module\nimport numpy as np\ntry:\n    from im2col_cyt import im2col_cython, col2im_cython\nexcept ImportError:\n    print('Installation broken, please reinstall PyFunt')\n\nfrom numpy.lib.stride_tricks import as_strided\n\n\ndef tile_array(a, b1, b2):\n    r, c = a.shape\n    rs, cs = a.strides\n    x = as_strided(a, (r, b1, c, b2), (rs, 0, cs, 0))\n    return x.reshape(r*b1, c*b2)\n\n\nclass SpatialUpSamplingNearest(Module):\n\n    def __init__(self, scale):\n        super(SpatialUpSamplingNearest, self).__init__()\n        self.scale_factor = scale\n        if self.scale_factor < 1:\n            raise Exception('scale_factor must be greater than 1')\n        if np.floor(self.scale_factor) != self.scale_factor:\n            raise Exception('scale_factor must be integer')\n\n    def update_output(self, x):\n        out_size = x.shape\n        out_size[x.ndim - 1] *= self.scale_factor\n        out_size[x.ndim - 2] *= self.scale_factor\n        N, C, H, W = out_size\n\n        stride = self.scale_factor\n        pool_height = pool_width = stride\n\n        x_reshaped = x.transpose(2, 3, 0, 1).flatten()\n        out_cols = np.zeros(out_size)\n        out_cols[:, np.arange(out_cols.shape[1])] = x_reshaped\n        out = col2im_cython(out_cols, N * C, 1, H, W, pool_height, pool_width,\n                            padding=0, stride=stride)\n        out = out.reshape(out_size)\n        return self.grad_input\n\n        return self.output\n\n    def update_grad_input(self, x, grad_output, scale=1):\n\n        N, C, H, W = grad_output.shape\n        pool_height = pool_width = self.scale_factor\n        stride = self.scale_factor\n\n        out_height = (H - pool_height) / stride + 1\n        out_width = (W - pool_width) / stride + 1\n\n        grad_output_split = grad_output.reshape(N * C, 1, H, W)\n        grad_output_cols = im2col_cython(\n            grad_output_split, pool_height, pool_width, padding=0, stride=stride)\n        grad_intput_cols = grad_output_cols[0, np.arange(grad_output_cols.shape[1])]\n        grad_input = grad_intput_cols.reshape(\n            out_height, out_width, N, C).transpose(2, 3, 0, 1)\n\n        self.output = grad_input\n\n\n\n"""
pyfunt/tanh.py,2,"b'from module import Module\nimport numpy as np\n\n\nclass Tanh(Module):\n\n    def __init__(self, th=1e-6, v=0, ip=False):\n        super(Tanh, self).__init__()\n        self.th = th\n        self.val = v\n        self.inplace = ip\n\n    def update_output(self, x):\n        self.output = np.tanh(x)\n        return self.output\n\n    def update_grad_input(self, x, grad_output):\n        self.grad_input = grad_output * (1 - np.power(self.output, 2))\n        return self.grad_input\n'"
pyfunt/threshold.py,2,"b""from module import Module\nimport numpy as np\n\n\nclass Threshold(Module):\n\n    def __init__(self, th=1e-6, v=0, ip=False):\n        super(Threshold, self).__init__()\n        self.th = th\n        self.val = v\n        self.inplace = ip\n\n    def update_output(self, x):\n        self.output = np.maximum(self.th, x)\n        return self.output\n\n    def update_grad_input(self, x, grad_output):\n        dx = np.array(grad_output, copy=True)\n        dx[x <= 0] = 0\n        self.grad_input = dx\n        return self.grad_input\n\n    def validate_parameters(self):\n        if self.inplace:\n            if self.val > self.th:\n                raise Exception('in-place processing requires value not exceed threshold')\n\n    def reset(self):\n        pass\n"""
pyfunt/view.py,2,"b""from module import Module\n\n\nclass View(Module):\n\n    def __init__(self, shape):\n        super(View, self).__init__()\n        if type(shape) is not tuple:\n            shape = (shape,)\n        self.shape = shape\n\n    def update_output(self, x):\n        self.output = x.view().reshape((x.shape[0],) + self.shape)\n        return self.output\n\n    def update_grad_input(self, x, grad_output):\n        self.grad_input = grad_output.view().reshape(x.shape)\n        return self.grad_input\n\n    def reset(self):\n        pass\n\n\n# class View(Module):\n\n#     def __init__(self, args):\n#         super(View, self).__init__()\n#         self.reset_size(args)\n#         self.num_input.ndim = None\n\n#     def reset_size(self, args):\n#         if len(args) == 1 and type(args[0]) == 'float64':\n#             self.size = args[0]\n#         else:\n#             self.size = None\n#         self.num_elements = 1\n#         inferdim = False\n#         for i in xrange(self.size):\n#             szi = self.size[i]\n#             if szi >= 0:\n#                 self.num_elements *= self.size[i]\n#             else:\n#                 if szi != -1:\n#                     raise Exception('size should be positive or -1')\n#                 if inferdim:\n#                     raise Exception('only one dimension can be at -1')\n#                 inferdim = True\n\n#     def update_output(self, x):\n#         self.output = self.output or np.zeros_like(x)\n#         batch_size = None\n#         if batch_size:\n#             self.output = x.view(batch_size, *self.size)\n#         else:\n#             self.output = x.view(self.size)\n#         return self.output\n\n#     def update_grad_input(self, x, grad_output):\n#         self.grad_input = self.grad_input or np.zeros_like(grad_output)\n#         self.grad_input = grad_output.view(x.size)\n#         return self.grad_input\n\n#     def __str__(self):\n#         return '%s(%s)' % (type(self), self.size)\n\n#     def reset(self):\n#         pass\n"""
tools/cythonize.py,0,"b'#!/usr/bin/env python\n"""""" cythonize\n\nSOURCE: https://github.com/scipy/scipy/blob/master/setup.py\n\nCythonize pyx files into C files as needed.\n\nUsage: cythonize [root_dir]\n\nDefault [root_dir] is \'pyfunt\'.\n\nChecks pyx files to see if they have been changed relative to their\ncorresponding C files.  If they have, then runs cython on these files to\nrecreate the C files.\n\nThe script thinks that the pyx files have changed relative to the C files\nby comparing hashes stored in a database file.\n\nSimple script to invoke Cython (and Tempita) on all .pyx (.pyx.in)\nfiles; while waiting for a proper build system. Uses file hashes to\nfigure out if rebuild is needed.\n\nFor now, this script should be run by developers when changing Cython files\nonly, and the resulting C files checked in, so that end-users (and Python-only\ndevelopers) do not get the Cython/Tempita dependencies.\n\nOriginally written by Dag Sverre Seljebotn, and copied here from:\n\nhttps://raw.github.com/dagss/private-scipy-refactor/cythonize/cythonize.py\n\nNote: this script does not check any of the dependent C libraries; it only\noperates on the Cython .pyx files.\n""""""\n\nfrom __future__ import division, print_function, absolute_import\n\nimport os\nimport re\nimport sys\nimport hashlib\nimport subprocess\n\nHASH_FILE = \'cythonize.dat\'\nDEFAULT_ROOT = \'pyfunt\'\n\n# WindowsError is not defined on unix systems\ntry:\n    WindowsError\nexcept NameError:\n    WindowsError = None\n\n#\n# Rules\n#\n\n\ndef process_pyx(fromfile, tofile):\n    try:\n        from Cython.Compiler.Version import version as cython_version\n        from distutils.version import LooseVersion\n        if LooseVersion(cython_version) < LooseVersion(\'0.22\'):\n            raise Exception(\'Building PyFunt requires Cython >= 0.22\')\n\n    except ImportError:\n        pass\n\n    flags = [\'--fast-fail\']\n    if tofile.endswith(\'.cxx\'):\n        flags += [\'--cplus\']\n\n    try:\n        try:\n            # if fromfile == \'im2col_cython.pyx\':\n            #     print(\'compiling im2col_cython\')\n            #     r = subprocess.call(\n            #         [\'python\', \'pyfunt/layers/setup.py\', \'build_ext\', \'--inplace\'])\n            # else:\n            r = subprocess.call(\n                    [\'cython\'] + flags + [""-o"", tofile, fromfile])\n            if r != 0:\n                raise Exception(\'Cython failed\')\n\n        except OSError:\n            # There are ways of installing Cython that don\'t result in a cython\n            # executable on the path, see gh-2397.\n            r = subprocess.call([sys.executable, \'-c\',\n                                 \'import sys; from Cython.Compiler.Main import \'\n                                 \'setuptools_main as main; sys.exit(main())\'] + flags +\n                                [""-o"", tofile, fromfile])\n            if r != 0:\n                raise Exception(""Cython either isn\'t installed or it failed."")\n    except OSError:\n        raise OSError(\'Cython needs to be installed\')\n\n\ndef process_tempita_pyx(fromfile, tofile):\n    try:\n        try:\n            from Cython import Tempita as tempita\n        except ImportError:\n            import tempita\n    except ImportError:\n        raise Exception(\'Building PyFunt requires Tempita: \'\n                        \'pip install --user Tempita\')\n    from_filename = tempita.Template.from_filename\n    template = from_filename(fromfile, encoding=sys.getdefaultencoding())\n    pyxcontent = template.substitute()\n    assert fromfile.endswith(\'.pyx.in\')\n    pyxfile = fromfile[:-len(\'.pyx.in\')] + \'.pyx\'\n    with open(pyxfile, ""w"") as f:\n        f.write(pyxcontent)\n    process_pyx(pyxfile, tofile)\n\nrules = {\n    # fromext : function\n    \'.pyx\': process_pyx,\n    \'.pyx.in\': process_tempita_pyx\n}\n#\n# Hash db\n#\n\n\ndef load_hashes(filename):\n    # Return { filename : (sha1 of input, sha1 of output) }\n    if os.path.isfile(filename):\n        hashes = {}\n        with open(filename, \'r\') as f:\n            for line in f:\n                filename, inhash, outhash = line.split()\n                hashes[filename] = (inhash, outhash)\n    else:\n        hashes = {}\n    return hashes\n\n\ndef save_hashes(hash_db, filename):\n    with open(filename, \'w\') as f:\n        for key, value in sorted(hash_db.items()):\n            f.write(""%s %s %s\\n"" % (key, value[0], value[1]))\n\n\ndef sha1_of_file(filename):\n    h = hashlib.sha1()\n    with open(filename, ""rb"") as f:\n        h.update(f.read())\n    return h.hexdigest()\n\n#\n# Main program\n#\n\n\ndef normpath(path):\n    path = path.replace(os.sep, \'/\')\n    if path.startswith(\'./\'):\n        path = path[2:]\n    return path\n\n\ndef get_hash(frompath, topath):\n    from_hash = sha1_of_file(frompath)\n    to_hash = sha1_of_file(topath) if os.path.exists(topath) else None\n    return (from_hash, to_hash)\n\n\ndef process(path, fromfile, tofile, processor_function, hash_db):\n    fullfrompath = os.path.join(path, fromfile)\n    fulltopath = os.path.join(path, tofile)\n    current_hash = get_hash(fullfrompath, fulltopath)\n    if current_hash == hash_db.get(normpath(fullfrompath), None):\n        print(\'%s has not changed\' % fullfrompath)\n        return\n\n    orig_cwd = os.getcwd()\n    try:\n        os.chdir(path)\n        print(\'Processing %s to %s\' % (fullfrompath, fulltopath))\n        processor_function(fromfile, tofile)\n    finally:\n        os.chdir(orig_cwd)\n    # changed target file, recompute hash\n    current_hash = get_hash(fullfrompath, fulltopath)\n    # store hash in db\n    hash_db[normpath(fullfrompath)] = current_hash\n\n\ndef find_process_files(root_dir):\n    hash_db = load_hashes(HASH_FILE)\n    for cur_dir, dirs, files in os.walk(root_dir):\n        for filename in files:\n            in_file = os.path.join(cur_dir, filename + "".in"")\n            if filename.endswith(\'.pyx\') and os.path.isfile(in_file):\n                continue\n            for fromext, function in rules.items():\n                if filename.endswith(fromext):\n                    toext = "".c""\n                    with open(os.path.join(cur_dir, filename), \'rb\') as f:\n                        data = f.read()\n                        m = re.search(\n                            br""^\\s*#\\s*distutils:\\s*language\\s*=\\s*c\\+\\+\\s*$"", data, re.I | re.M)\n                        if m:\n                            toext = "".cxx""\n                    fromfile = filename\n                    tofile = filename[:-len(fromext)] + toext\n                    process(cur_dir, fromfile, tofile, function, hash_db)\n                    save_hashes(hash_db, HASH_FILE)\n\n\ndef main():\n    try:\n        root_dir = sys.argv[1]\n    except IndexError:\n        root_dir = DEFAULT_ROOT\n    find_process_files(root_dir)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pyfunt/examples/__init__.py,0,b'from . import *\n'
pyfunt/utils/__init__.py,0,"b'from load_torch_model import (load_t7model, load_t7checkpoint, load_parser_init, load_parser_vals)\nfrom gradient_check import eval_numerical_gradient_array\nfrom . import *\n'"
pyfunt/utils/gradient_check.py,10,"b""import numpy as np\nfrom random import randrange\n\n\ndef eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n    '''\n    a naive implementation of numerical gradient of f at x\n    - f should be a function that takes a single argument\n    - x is the point (numpy array) to evaluate the gradient at\n    '''\n    grad = np.zeros_like(x)\n    # iterate over all indexes in x\n    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n    while not it.finished:\n\n        # evaluate function at x+h\n        ix = it.multi_index\n        oldval = x[ix]\n        x[ix] = oldval + h  # increment by h\n        fxph = f(x)  # evalute f(x + h)\n        x[ix] = oldval - h\n        fxmh = f(x)  # evaluate f(x - h)\n        x[ix] = oldval  # restore\n\n        # compute the partial derivative with centered formula\n        grad[ix] = (fxph - fxmh) / (2 * h)  # the slope\n        if verbose:\n            print(x), grad[ix]\n        it.iternext()  # step to next dimension\n\n    return grad\n\n\ndef eval_numerical_gradient_array(f, x, df, h=1e-5):\n    '''\n    Evaluate a numeric gradient for a function that accepts a numpy\n    array and returns a numpy array.\n    '''\n    grad = np.zeros_like(x)\n    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n    while not it.finished:\n        ix = it.multi_index\n\n        oldval = x[ix]\n        x[ix] = oldval + h\n        pos = f(x).copy()\n        x[ix] = oldval - h\n        neg = f(x).copy()\n        x[ix] = oldval\n\n        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n        it.iternext()\n    return grad\n\n\ndef eval_numerical_gradient_blobs(f, inputs, output, h=1e-5):\n    '''\n    Compute numeric gradients for a function that operates on input\n    and output blobs.\n\n    We assume that f accepts several input blobs as arguments, followed by a blob\n    into which outputs will be written. For example, f might be called like this:\n\n    f(x, w, out)\n\n    where x and w are input Blobs, and the result of f will be written to out.\n\n    Inputs:\n    - f: function\n    - inputs: tuple of input blobs\n    - output: output blob\n    - h: step size\n    '''\n    numeric_diffs = []\n    for input_blob in inputs:\n        diff = np.zeros_like(input_blob.diffs)\n        it = np.nditer(input_blob.vals, flags=['multi_index'],\n                       op_flags=['readwrite'])\n        while not it.finished:\n            idx = it.multi_index\n            orig = input_blob.vals[idx]\n\n            input_blob.vals[idx] = orig + h\n            f(*(inputs + (output,)))\n            pos = np.copy(output.vals)\n            input_blob.vals[idx] = orig - h\n            f(*(inputs + (output,)))\n            neg = np.copy(output.vals)\n            input_blob.vals[idx] = orig\n\n            diff[idx] = np.sum((pos - neg) * output.diffs) / (2.0 * h)\n\n            it.iternext()\n        numeric_diffs.append(diff)\n    return numeric_diffs\n\n\ndef eval_numerical_gradient_net(net, inputs, output, h=1e-5):\n    return eval_numerical_gradient_blobs(lambda *args: net.forward(),\n                                         inputs, output, h=h)\n\n\ndef grad_check_sparse(f, x, analytic_grad, num_checks):\n    '''\n    sample a few random elements and only return numerical\n    in this dimensions.\n    '''\n    h = 1e-5\n\n    x.shape\n    for i in xrange(num_checks):\n        ix = tuple([randrange(m) for m in x.shape])\n\n        oldval = x[ix]\n        x[ix] = oldval + h  # increment by h\n        fxph = f(x)  # evaluate f(x + h)\n        x[ix] = oldval - h  # increment by h\n        fxmh = f(x)  # evaluate f(x - h)\n        x[ix] = oldval  # reset\n\n        grad_numerical = (fxph - fxmh) / (2 * h)\n        grad_analytic = analytic_grad[ix]\n        rel_error = abs(grad_numerical - grad_analytic) / \\\n            (abs(grad_numerical) + abs(grad_analytic))\n        print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))\n"""
pyfunt/utils/load_torch_model.py,0,"b""import torchfile\nimport pyfunt\nimport pdb\nimport re\n\nplease_contribute = 'If you want you can fix it and make a pull request ;)'\n\n\n'''\n<Layer>_init (module) takes a dict for the torch layer and returns a tuple\ncontaining the values for the pyfunt layer initialization funciton.\nOnce you wrote the function, add the reation in the load_parser_init dict.\nThe same mechanism goes for the layer values using load_parser_vals dictt\n(gard input, output, weight, bias already get added).\n'''\n\n\ndef conv_init(m):\n    return m['nInputPlane'], m['nOutputPlane'], m['kW'], m['kH'], m['dW'], m['dH'], m['padW'], m['padH']\n\n\ndef dropout_init(m):\n    return m['p'], not m['v2']\n\n\ndef linear_init(m):\n    return m['weight'].shape[1], m['weight'].shape[0], len(m['bias']) != 0\n\n\ndef mul_constant_init(m):\n    return (m['constant_scalar'],)\n\n\ndef relu_init(m):\n    return (m['inplace'],)\n\n\ndef spatial_max_pooling_init(m):\n    return m['kW'], m['kH'], m['dW'], m['dH'], m['padW'], m['padH']\n\n\ndef spatial_batch_normalization_init(m):\n    return len(m['running_mean']), m['eps'], m['momentum'], len(m['weight']) > 0\n\n\ndef spatial_average_pooling_init(m):\n    return m['kW'], m['kH'], m['dW'], m['dH'], m['padW'], m['padH']\n\n\ndef spatial_full_convolution_init(m):\n    return m['nInputPlane'], m['nOutputPlane'], m['kW'], m['kH'], m['dW'], m['dH'], m['padW'], m['padH'], m['adjW'], m['adjH']\n\n\ndef spatial_padding_init(m):\n    return m['pad_l'], m['pad_r'], m['pad_t'], m['pad_b']\n\n\ndef view_init(m):\n    return (m['size'],)\n\n\nload_parser_init = {\n    'Dropout': dropout_init,\n    'Linear': linear_init,\n    'MulConstant': mul_constant_init,\n    'ReLU': relu_init,\n    'SpatialConvolution': conv_init,\n    'SpatialMaxPooling': spatial_max_pooling_init,\n    'SpatialAvergaePooling': spatial_average_pooling_init,\n    'SpatialBatchNormalization': spatial_batch_normalization_init,\n    'SpatialFullConvolution': spatial_full_convolution_init,\n    'SpatialReflectionPadding': spatial_padding_init,\n    'SpatialReplicationPadding': spatial_padding_init,\n    'View': view_init\n}\n\n\n# def add_possible_values(module, tmodule):\n#     print(len(dir(tmodule)))\n#     for k in dir(tmodule):\n#         if any(x.isupper() for x in k):\n#             ourk = re.sub('([A-Z]+)', r'_\\1', k).lower()\n#             add_value(module, tmodule, ourk, k)\n#         else:\n#             add_value(module, tmodule, k)\n\n\ndef dropout_vals(module, tmodule):\n    add_value(module, tmodule, 'noise')\n\n\ndef spatial_batch_normalization_vals(module, tmodule):\n    add_value(module, tmodule, 'running_mean')\n    add_value(module, tmodule, 'running_var')\n\n\nload_parser_vals = {\n    'Droput': dropout_vals,\n    'SpatialBatchNormalization': spatial_batch_normalization_vals\n}\n\n\ndef load_t7model(path=None, obj=None, model=None, custom_layers=None):\n    if not (path is None or obj is None):\n        raise Exception('you must pass a path or a TorchObject')\n    if path:\n        o = torchfile.load(path)\n    else:\n        o = obj\n\n    #\xc2\xa0import pdb; pdb.set_trace()\n    if type(o) is torchfile.TorchObject:\n        class_name = o._typename.split('.')[-1]\n        tmodule = o._obj\n\n        if not hasattr(pyfunt, class_name):\n            print('class %s not found' % class_name)\n            print(please_contribute)\n            raise NotImplementedError\n\n        Module = getattr(pyfunt, class_name)\n        if not is_container(Module):\n            raise Exception('model is a torchobj but not a container')\n        model = Module()\n        add_inout(model, tmodule)\n\n        m = load_t7model(obj=tmodule, model=model, custom_layers=custom_layers)\n        if not model:\n            model = m\n    else:\n\n        for i, tmodule in enumerate(o.modules):\n            if type(tmodule) is torchfile.TorchObject:\n                class_name = tmodule._typename.split('.')[-1]\n                tmodule_o = tmodule._obj\n\n                if hasattr(pyfunt, class_name):\n                    Module = getattr(pyfunt, class_name)\n                elif custom_layers and hasattr(custom_layers, class_name):\n                    Module = getattr(custom_layers, class_name)\n                else:\n                    print('class %s not found' % class_name)\n                    print(please_contribute)\n                    raise NotImplementedError\n\n                if i == 0 and model is None:\n                    if not is_container(Module):\n                        model = pyfunt.Sequential()\n                #     else:\n                #         model = Module()\n                #         model = load_t7model(obj=tmodule, model=model)\n                # else:\n                if is_container(Module):\n                    model.add(\n                        load_t7model(obj=tmodule, model=model, custom_layers=custom_layers))\n                else:\n                    if class_name in load_parser_init:\n                        args = load_parser_init[class_name](tmodule_o)\n                        module = Module(*args)\n                    else:\n                        try:\n                            module = Module()\n                        except:\n                            print('parser for %s not found' % class_name)\n                            print('%s cannot be initialized with no args' %\n                                  class_name)\n                            print(please_contribute)\n                            raise NotImplementedError\n\n                    #add_possible_values(module, tmodule)\n                    add_inout(module, tmodule_o)\n                    add_w(module, tmodule_o)\n                    if class_name in load_parser_vals:\n                        load_parser_vals[class_name](module, tmodule_o)\n                    model.add(module)\n            else:\n                print('oops!')\n                print(please_contribute)\n                pdb.set_trace()\n                raise NotImplementedError\n    return model\n\n\ndef is_container(tm):\n    return pyfunt.container.Container in tm.__bases__\n\n\ndef add_value(module, tmodule, pname, tpname=None):\n    tpname = tpname or pname\n    if hasattr(module, pname):\n        if tpname in tmodule:\n            setattr(module, pname, tmodule[tpname])\n\n\ndef add_inout(module, tmodule):\n    add_value(module, tmodule, 'output')\n    add_value(module, tmodule, 'grad_input', 'gradInput')\n\n\ndef add_w(module, tmodule):\n    add_value(module, tmodule, 'weight')\n    add_value(module, tmodule, 'bias')\n    add_value(module, tmodule, 'grad_weight', 'gradWeight')\n    add_value(module, tmodule, 'grad_bias', 'gradBias')\n\n\ndef load_t7checkpoint(path, models_keys=['model'], custom_layers=None):\n    # model_keys iterable that contains for example the word 'model'\n    # the model to load in pyfunt\n    cp = torchfile.load(path)\n    for model in models_keys:\n        cp[model] = load_t7model(obj=cp[model], custom_layers=custom_layers)\n    return cp\n"""
pyfunt/utils/vis_utils.py,7,"b""from math import sqrt, ceil\nimport numpy as np\n\n\ndef visualize_grid(Xs, ubound=255.0, padding=1, grid_size=None):\n    '''\n    Reshape a 4D tensor of image data to a grid for easy visualization.\n\n    Inputs:\n    - Xs: Data of shape (N, H, W, C)\n    - ubound: Output grid will have values scaled to the range [0, ubound]\n    - padding: The number of blank pixels between elements of the grid\n    '''\n    (N, H, W, C) = Xs.shape\n\n    if grid_size is None:\n        grid_size_y = grid_size_x = int(ceil(sqrt(N)))\n    elif type(grid_size) == tuple:\n        grid_size_x = grid_size[0]\n        grid_size_y = grid_size[1]\n    else:\n        grid_size_y = grid_size_x = grid_size\n\n    grid_height = H * grid_size_y + padding * (grid_size_y - 1)\n    grid_width = W * grid_size_x + padding * (grid_size_x - 1)\n    grid = np.zeros((grid_height, grid_width, C))\n    next_idx = 0\n    y0, y1 = 0, H\n    for y in xrange(grid_size_y):\n        x0, x1 = 0, W\n        for x in xrange(grid_size_x):\n            if next_idx < N:\n                img = Xs[next_idx]\n                low, high = np.min(img), np.max(img)\n                grid[y0:y1, x0:x1] = ubound * (img - low) / (high - low)\n                # grid[y0:y1, x0:x1] = Xs[next_idx]\n                next_idx += 1\n            x0 += W + padding\n            x1 += W + padding\n        y0 += H + padding\n        y1 += H + padding\n    # grid_max = np.max(grid)\n    # grid_min = np.min(grid)\n    # grid = ubound * (grid - grid_min) / (grid_max - grid_min)\n    return grid\n\n\ndef vis_grid(Xs):\n    ''' visualize a grid of images '''\n    (N, H, W, C) = Xs.shape\n    A = int(ceil(sqrt(N)))\n    G = np.ones((A*H+A, A*W+A, C), Xs.dtype)\n    G *= np.min(Xs)\n    n = 0\n    for y in range(A):\n        for x in range(A):\n            if n < N:\n                G[y*H+y:(y+1)*H+y, x*W+x:(x+1)*W+x, :] = Xs[n, :, :, :]\n                n += 1\n    # normalize to [0,1]\n    maxg = G.max()\n    ming = G.min()\n    G = (G - ming)/(maxg-ming)\n    return G\n\n\ndef vis_nn(rows):\n    ''' visualize array of arrays of images '''\n    N = len(rows)\n    D = len(rows[0])\n    H, W, C = rows[0][0].shape\n    Xs = rows[0][0]\n    G = np.ones((N*H+N, D*W+D, C), Xs.dtype)\n    for y in range(N):\n        for x in range(D):\n            G[y*H+y:(y+1)*H+y, x*W+x:(x+1)*W+x, :] = rows[y][x]\n    # normalize to [0,1]\n    maxg = G.max()\n    ming = G.min()\n    G = (G - ming)/(maxg-ming)\n    return G\n"""
pyfunt/examples/model_testing/test_model.py,4,"b'import numpy as np\nfrom pyfunt import (SpatialConvolution, SpatialBatchNormalization,\n                    SpatialAveragePooling, Sequential, ReLU, Linear,\n                    Reshape, LogSoftMax)\nfrom pyfunt.utils import eval_numerical_gradient_array\n\ndef rel_error(x, y):\n    """""" returns relative error """"""\n    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n\nx = np.random.randn(3, 4, 8, 8)\n# x = np.random.randn(3, 2, 8, 8)\ndout = np.random.randn(3, 10)\npool_param = {\'pool_height\': 2, \'pool_width\': 2, \'stride\': 2}\n\ns = Sequential()\ns.add(SpatialConvolution(4, 2, 1, 1, 1, 1))\ns.add(SpatialAveragePooling(2, 2, 2, 2, 0, 0))\ns.add(SpatialBatchNormalization(2))\ns.add(ReLU())\ns.add(Reshape(2*4*4))\ns.add(Linear(2*4*4, 10))\ns.add(LogSoftMax())\n\ndx_num = eval_numerical_gradient_array(lambda x: s.update_output(x), x, dout)\n\nout = s.update_output(x)\ndx = s.update_grad_input(x, dout)\n# Your error should be around 1e-8\nprint(\'Testing net backward function:\')\nprint(\'dx error: \', rel_error(dx, dx_num))\n# import pdb; pdb.set_trace()\n'"
pyfunt/examples/residual_networks/__init__.py,0,b'from . import *\n'
pyfunt/examples/residual_networks/resnet.py,0,"b'from pyfunt import (SpatialConvolution, SpatialBatchNormalization,\n                    SpatialAveragePooling, Sequential, ReLU, Linear,\n                    Reshape, LogSoftMax, Padding, Identity, ConcatTable,\n                    CAddTable)\n\n\ndef residual_layer(n_channels, n_out_channels=None, stride=None):\n    n_out_channels = n_out_channels or n_channels\n    stride = stride or 1\n\n    convs = Sequential()\n    add = convs.add\n    add(SpatialConvolution(\n        n_channels, n_out_channels, 3, 3, stride, stride, 1, 1))\n    add(SpatialBatchNormalization(n_out_channels))\n    add(SpatialConvolution(n_out_channels, n_out_channels, 3, 3, 1, 1, 1, 1))\n    add(SpatialBatchNormalization(n_out_channels))\n\n    if stride > 1:\n        shortcut = Sequential()\n        shortcut.add(SpatialAveragePooling(2, 2, stride, stride))\n        shortcut.add(Padding(1, (n_out_channels - n_channels)/2, 3))\n    else:\n        shortcut = Identity()\n\n    res = Sequential()\n    res.add(ConcatTable().add(convs).add(shortcut)).add(CAddTable())\n    # https://github.com/szagoruyko/wide-residual-networks/blob/master/models/resnet-pre-act.lua\n\n    res.add(ReLU(True))\n\n    return res\n\n\ndef resnet(n_size, num_starting_filters, reg):\n    \'\'\'\n    Implementation of [""Deep Residual Learning for Image Recognition"",Kaiming \\\n    He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - http://arxiv.org/abs/1512.03385\n\n    Inspired by https://github.com/gcr/torch-residual-networks\n\n    This network should model a similiar behaviour of gcr\'s implementation.\n    Check https://github.com/gcr/torch-residual-networks for more infos about \\\n    the structure.\n\n    The network operates on minibatches of data that have shape (N, C, H, W)\n    consisting of N images, each with height H and width W and with C input\n    channels.\n\n    The network has, like in the reference paper (except for the final optional\n    affine layers), (6*n)+2 layers, composed as below:\n\n                                                (image_dim: 3, 32, 32; F=16)\n                                                (input_dim: N, *image_dim)\n         INPUT\n            |\n            v\n       +-------------------+\n       |conv[F, *image_dim]|                    (out_shape: N, 16, 32, 32)\n       +-------------------+\n            |\n            v\n       +-------------------------+\n       |n * res_block[F, F, 3, 3]|              (out_shape: N, 16, 32, 32)\n       +-------------------------+\n            |\n            v\n       +-------------------------+\n       |res_block[2*F, F, 3, 3]  |              (out_shape: N, 32, 16, 16)\n       +-------------------------+\n            |\n            v\n       +---------------------------------+\n       |(n-1) * res_block[2*F, 2*F, 3, 3]|      (out_shape: N, 32, 16, 16)\n       +---------------------------------+\n            |\n            v\n       +-------------------------+\n       |res_block[4*F, 2*F, 3, 3]|              (out_shape: N, 64, 8, 8)\n       +-------------------------+\n            |\n            v\n       +---------------------------------+\n       |(n-1) * res_block[4*F, 4*F, 3, 3]|      (out_shape: N, 64, 8, 8)\n       +---------------------------------+\n            |\n            v\n       +-------------+\n       |pool[1, 8, 8]|                          (out_shape: N, 64, 1, 1)\n       +-------------+\n            |\n            v\n       +- - - - - - - - -+\n       |(opt) m * affine |                      (out_shape: N, 64, 1, 1)\n       +- - - - - - - - -+\n            |\n            v\n       +-------+\n       |softmax|                                (out_shape: N, num_classes)\n       +-------+\n            |\n            v\n         OUTPUT\n\n    Every convolution layer has a pad=1 and stride=1, except for the  dimension\n    enhancning layers which has a stride of 2 to mantain the computational\n    complexity.\n    Optionally, there is the possibility of setting m affine layers immediatley\n    before the softmax layer by setting the hidden_dims parameter, which should\n    be a list of integers representing the numbe of neurons for each affine\n    layer.\n\n    Each residual block is composed as below:\n\n                  Input\n                     |\n             ,-------+-----.\n       Downsampling      3x3 convolution+dimensionality reduction\n            |               |\n            v               v\n       Zero-padding      3x3 convolution\n            |               |\n            `-----( Add )---\'\n                     |\n                  Output\n\n    After every layer, a batch normalization with momentum .1 is applied.\n\n    Weight initialization (check also layers/init.py and layers/README.md):\n    - Inizialize the weights and biases for the affine layers in the same\n     way of torch\'s default mode by calling _init_affine_wb that returns a\n     tuple (w, b).\n    - Inizialize the weights for the conv layers in the same\n     way of torch\'s default mode by calling init_conv_w.\n    - Inizialize the weights for the conv layers in the same\n     way of kaiming\'s mode by calling init_conv_w_kaiming\n     (http://arxiv.org/abs/1502.01852 and\n      http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-\\\n      initialization)\n    - Initialize batch normalization layer\'s weights like torch\'s default by\n    calling init_bn_w\n    - Initialize batch normalization layer\'s weights like cgr\'s first resblock\\\n    \'s bn (https://github.com/gcr/torch-residual-networks/blob/master/residual\\\n           -layers.lua#L57-L59) by calling init_bn_w_gcr.\n\n    num_filters=[16, 16, 32, 32, 64, 64],\n        Initialize a new network.\n\n        Inputs:\n        - input_dim: Tuple (C, H, W) giving size of input data.\n        - num_starting_filters: Number of filters for the first convolution\n        layer.\n        - n_size: nSize for the residual network like in the reference paper\n        - hidden_dims: Optional list number of units to use in the\n        fully-connected hidden layers between the fianl pool and the sofmatx\n        layer.\n        - num_classes: Number of scores to produce from the final affine layer.\n        - reg: Scalar giving L2 regularization strength\n        - dtype: numpy datatype to use for computation.\n    \'\'\'\n\n    nfs = num_starting_filters\n    model = Sequential()\n    add = model.add\n    add(SpatialConvolution(3, nfs, 3, 3, 1, 1, 1, 1))\n    add(SpatialBatchNormalization(nfs))\n    add(ReLU())\n\n    for i in xrange(1, n_size):\n        add(residual_layer(nfs))\n    add(residual_layer(nfs, 2*nfs, 2))\n\n    for i in xrange(1, n_size-1):\n        add(residual_layer(2*nfs))\n    add(residual_layer(2*nfs, 4*nfs, 2))\n\n    for i in xrange(1, n_size-1):\n        add(residual_layer(4*nfs))\n\n    add(SpatialAveragePooling(8, 8))\n    add(Reshape(nfs*4))\n    add(Linear(nfs*4, 10))\n    add(LogSoftMax())\n    return model\n'"
pyfunt/examples/residual_networks/train-cifar.py,2,"b'# !/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport uuid\nimport numpy as np\n# import matplotlib.pyplot as plt\nfrom pydatset.cifar10 import get_CIFAR10_data\nfrom pydatset.data_augmentation import (random_flips,\n                                        random_crops)\nfrom resnet import resnet\nfrom pyfunt.solver import Solver as Solver\n\nimport inspect\nimport argparse\n\nfrom pyfunt.class_nll_criterion import ClassNLLCriterion\n\nnp.seterr(all=\'raise\')\n\nnp.random.seed(0)\n\nDATA_PATH = \'../CIFAR_DATASET_PATH\'\n\npath_set = False\nwhile not path_set:\n    try:\n        with open(DATA_PATH) as f:\n            DATASET_PATH = f.read()\n        path_set = True\n    except:\n        data_path = raw_input(\'Enter the path for the CIFAR10 dataset: \')\n        with open(DATA_PATH, ""w"") as f:\n            f.write(data_path)\n\n\nEXPERIMENT_PATH = \'../Experiments/\' + str(uuid.uuid4())[-10:]\n\n# residual network constants\nNSIZE = 3\nN_STARTING_FILTERS = 16\n\n# solver constants\nNUM_PROCESSES = 4\n\nNUM_TRAIN = 50000\nNUM_TEST = 10000\n\nWEIGHT_DEACY = 1e-4\nREGULARIZATION = 0\nLEARNING_RATE = .1\nMOMENTUM = .99\nNUM_EPOCHS = 160\nBATCH_SIZE = 64\nCHECKPOINT_EVERY = 20\n\nXH, XW = 32, 32\n\nargs = argparse.Namespace()\n\n\ndef parse_args():\n    """"""\n    Parse the options for running the Residual Network on CIFAR-10.\n    """"""\n    desc = \'Train a Residual Network on CIFAR-10.\'\n    parser = argparse.ArgumentParser(description=desc,\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    add = parser.add_argument\n    add(\'--dataset_path\',\n        metavar=\'DIRECOTRY\',\n        default=DATASET_PATH,\n        type=str,\n        help=\'directory where results will be saved\')\n    add(\'--experiment_path\',\n        metavar=\'DIRECOTRY\',\n        default=EXPERIMENT_PATH,\n        type=str,\n        help=\'directory where results will be saved\')\n    add(\'-load\', \'--load_checkpoint\',\n        metavar=\'DIRECOTRY\',\n        default=\'\',\n        type=str,\n        help=\'load checkpoint from load_checkpoint\')\n    add(\'--n_size\',\n        metavar=\'INT\',\n        default=NSIZE,\n        type=int,\n        help=\'Network will have (6*n)+2 conv layers\')\n    add(\'--n_starting_filters\',\n        metavar=\'INT\',\n        default=N_STARTING_FILTERS,\n        type=int,\n        help=\'Network will starts with those number of filters\')\n    add(\'--n_processes\', \'-np\',\n        metavar=\'INT\',\n        default=NUM_PROCESSES,\n        type=int,\n        help=\'Number of processes for each step\')\n    add(\'--n_train\',\n        metavar=\'INT\',\n        default=NUM_TRAIN,\n        type=int,\n        help=\'Number of total images to select for training\')\n    add(\'--n_test\',\n        metavar=\'INT\',\n        default=NUM_TEST,\n        type=int,\n        help=\'Number of total images to select for validation\')\n    add(\'-wd\', \'--weight_decay\',\n        metavar=\'FLOAT\',\n        default=WEIGHT_DEACY,\n        type=float,\n        help=\'Weight decay for sgd_th\')\n    add(\'-reg\', \'--network_regularization\',\n        metavar=\'FLOAT\',\n        default=REGULARIZATION,\n        type=float,\n        help=\'L2 regularization term for the network\')\n    add(\'-lr\', \'--learning_rate\',\n        metavar=\'FLOAT\',\n        default=LEARNING_RATE,\n        type=float,\n        help=\'Learning rate to use with sgd_th\')\n    add(\'-mom\', \'--momentum\',\n        metavar=\'FLOAT\',\n        default=MOMENTUM,\n        type=float,\n        help=\'Nesterov momentum use with sgd_th\')\n    add(\'--n_epochs\', \'-nep\',\n        metavar=\'INT\',\n        default=NUM_EPOCHS,\n        type=int,\n        help=\'Number of epochs for training\')\n    add(\'--batch_size\', \'-bs\',\n        metavar=\'INT\',\n        default=BATCH_SIZE,\n        type=int,\n        help=\'Number of images for each iteration\')\n    add(\'--checkpoint_every\', \'-cp\',\n        metavar=\'INT\',\n        default=CHECKPOINT_EVERY,\n        type=int,\n        help=\'Number of epochs between each checkpoint\')\n    parser.parse_args(namespace=args)\n    assert not (args.network_regularization and args.weight_decay)\n\n\ndef data_augm(batch):\n    p = 2\n    h, w = XH, XW\n\n    # batch = random_tint(batch)\n    # batch = random_contrast(batch)\n    batch = random_flips(batch)\n    # batch = random_rotate(batch, 10)\n    batch = random_crops(batch, (h, w), pad=p)\n    return batch\n\n\ndef custom_update_decay(epoch):\n    if epoch in (80, 120):\n        return 0.1\n    return 1\n\n\ndef print_infos(solver):\n    print(\'Model: \\n%s\' % solver.model)\n\n    print(\'Solver: \\n%s\' % solver)\n\n    print(\'Data Augmentation Function: \\n\')\n    print(\'\'.join([\'\\t\' + i for i in inspect.getsourcelines(data_augm)[0]]))\n    print(\'Custom Weight Decay Update Rule: \\n\')\n    print(\'\'.join([\'\\t\' + i for i in inspect.getsourcelines(custom_update_decay)[0]]))\n\n\ndef main():\n    parse_args()\n\n    data = get_CIFAR10_data(args.dataset_path,\n                            num_training=args.n_train, num_validation=0, num_test=args.n_test)\n\n    data = {\n        \'X_train\': data[\'X_train\'],\n        \'y_train\': data[\'y_train\'],\n        \'X_val\': data[\'X_test\'],\n        \'y_val\': data[\'y_test\'],\n    }\n\n    exp_path = args.experiment_path\n    nf = args.n_starting_filters\n    reg = args.network_regularization\n\n    model = resnet(n_size=args.n_size,\n                   num_starting_filters=nf,\n                   reg=reg)\n\n    wd = args.weight_decay\n    lr = args.learning_rate\n    mom = args.momentum\n\n    optim_config = {\'learning_rate\': lr, \'nesterov\': True,\n                    \'momentum\': mom, \'weight_decay\': wd}\n\n    epochs = args.n_epochs\n    bs = args.batch_size\n    num_p = args.n_processes\n    cp = args.checkpoint_every\n    criterion = ClassNLLCriterion()\n    solver = Solver(model, data, args.load_checkpoint,\n                    criterion=criterion,\n                    num_epochs=epochs, batch_size=bs,  # 20\n                    update_rule=\'sgd_th\',\n                    optim_config=optim_config,\n                    custom_update_ld=custom_update_decay,\n                    batch_augment_func=data_augm,\n                    checkpoint_every=cp,\n                    num_processes=num_p)\n\n    print_infos(solver)\n    solver.train()\n\n    solver.export_model(exp_path)\n    solver.export_histories(exp_path)\n\n    print(\'finish\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
