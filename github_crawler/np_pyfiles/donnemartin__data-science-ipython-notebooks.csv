file_path,api_count,code
__init__.py,0,b''
analyses/__init__.py,0,b''
analyses/churn_measurements.py,16,"b'from __future__ import division\nimport numpy as np\n\n__author__ = ""Eric Chiang""\n__email__  = ""eric[at]yhathq.com""\n\n""""""\n\nMeasurements inspired by Philip Tetlock\'s ""Expert Political Judgment""\n\nEquations take from Yaniv, Yates, & Smith (1991):\n  ""Measures of Descrimination Skill in Probabilistic Judgement""\n\n""""""\n\n\ndef calibration(prob,outcome,n_bins=10):\n    """"""Calibration measurement for a set of predictions.\n\n    When predicting events at a given probability, how far is frequency\n    of positive outcomes from that probability?\n    NOTE: Lower scores are better\n\n    prob: array_like, float\n        Probability estimates for a set of events\n\n    outcome: array_like, bool\n        If event predicted occurred\n\n    n_bins: int\n        Number of judgement categories to prefrom calculation over.\n        Prediction are binned based on probability, since ""descrete"" \n        probabilities aren\'t required. \n\n    """"""\n    prob = np.array(prob)\n    outcome = np.array(outcome)\n\n    c = 0.0\n    # Construct bins\n    judgement_bins = np.arange(n_bins + 1) / n_bins\n    # Which bin is each prediction in?\n    bin_num = np.digitize(prob,judgement_bins)\n    for j_bin in np.unique(bin_num):\n        # Is event in bin\n        in_bin = bin_num == j_bin\n        # Predicted probability taken as average of preds in bin\n        predicted_prob = np.mean(prob[in_bin])\n        # How often did events in this bin actually happen?\n        true_bin_prob = np.mean(outcome[in_bin])\n        # Squared distance between predicted and true times num of obs\n        c += np.sum(in_bin) * ((predicted_prob - true_bin_prob) ** 2)\n    return c / len(prob)\n\ndef discrimination(prob,outcome,n_bins=10):\n    """"""Discrimination measurement for a set of predictions.\n\n    For each judgement category, how far from the base probability\n    is the true frequency of that bin?\n    NOTE: High scores are better\n\n    prob: array_like, float\n        Probability estimates for a set of events\n\n    outcome: array_like, bool\n        If event predicted occurred\n\n    n_bins: int\n        Number of judgement categories to prefrom calculation over.\n        Prediction are binned based on probability, since ""descrete"" \n        probabilities aren\'t required. \n\n    """"""\n    prob = np.array(prob)\n    outcome = np.array(outcome)\n\n    d = 0.0\n    # Base frequency of outcomes\n    base_prob = np.mean(outcome)\n    # Construct bins\n    judgement_bins = np.arange(n_bins + 1) / n_bins\n    # Which bin is each prediction in?\n    bin_num = np.digitize(prob,judgement_bins)\n    for j_bin in np.unique(bin_num):\n        in_bin = bin_num == j_bin\n        true_bin_prob = np.mean(outcome[in_bin])\n        # Squared distance between true and base times num of obs\n        d += np.sum(in_bin) * ((true_bin_prob - base_prob) ** 2)\n    return d / len(prob)\n'"
aws/__init__.py,0,b''
commands/__init__.py,0,b''
kaggle/__init__.py,0,b''
mapreduce/__init__.py,0,b''
mapreduce/mr_s3_log_parser.py,0,"b'\nimport time\nfrom mrjob.job import MRJob\nfrom mrjob.protocol import RawValueProtocol, ReprProtocol\nimport re\n\n\nclass MrS3LogParser(MRJob):\n    """"""Parses the logs from S3 based on the S3 logging format:\n    http://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html\n    \n    Aggregates a user\'s daily requests by user agent and operation\n    \n    Outputs date_time, requester, user_agent, operation, count\n    """"""\n\n    LOGPATS  = r\'(\\S+) (\\S+) \\[(.*?)\\] (\\S+) (\\S+) \' \\\n               r\'(\\S+) (\\S+) (\\S+) (""([^""]+)""|-) \' \\\n               r\'(\\S+) (\\S+) (\\S+) (\\S+) (\\S+) (\\S+) \' \\\n               r\'(""([^""]+)""|-) (""([^""]+)""|-)\'\n    NUM_ENTRIES_PER_LINE = 17\n    logpat = re.compile(LOGPATS)\n\n    (S3_LOG_BUCKET_OWNER, \n     S3_LOG_BUCKET, \n     S3_LOG_DATE_TIME,\n     S3_LOG_IP, \n     S3_LOG_REQUESTER_ID, \n     S3_LOG_REQUEST_ID,\n     S3_LOG_OPERATION, \n     S3_LOG_KEY, \n     S3_LOG_HTTP_METHOD,\n     S3_LOG_HTTP_STATUS, \n     S3_LOG_S3_ERROR, \n     S3_LOG_BYTES_SENT,\n     S3_LOG_OBJECT_SIZE, \n     S3_LOG_TOTAL_TIME, \n     S3_LOG_TURN_AROUND_TIME,\n     S3_LOG_REFERER, \n     S3_LOG_USER_AGENT) = range(NUM_ENTRIES_PER_LINE)\n\n    DELIMITER = \'\\t\'\n\n    # We use RawValueProtocol for input to be format agnostic\n    # and avoid any type of parsing errors\n    INPUT_PROTOCOL = RawValueProtocol\n\n    # We use RawValueProtocol for output so we can output raw lines\n    # instead of (k, v) pairs\n    OUTPUT_PROTOCOL = RawValueProtocol\n\n    # Encode the intermediate records using repr() instead of JSON, so the\n    # record doesn\'t get Unicode-encoded\n    INTERNAL_PROTOCOL = ReprProtocol\n\n    def clean_date_time_zone(self, raw_date_time_zone):\n        """"""Converts entry 22/Jul/2013:21:04:17 +0000 to the format\n        \'YYYY-MM-DD HH:MM:SS\' which is more suitable for loading into\n        a database such as Redshift or RDS\n\n        Note: requires the chars ""[ ]"" to be stripped prior to input\n        Returns the converted datetime annd timezone\n        or None for both values if failed\n\n        TODO: Needs to combine timezone with date as one field\n        """"""\n        date_time = None\n        time_zone_parsed = None\n\n        # TODO: Probably cleaner to parse this with a regex\n        date_parsed = raw_date_time_zone[:raw_date_time_zone.find("":"")]\n        time_parsed = raw_date_time_zone[raw_date_time_zone.find("":"") + 1:\n                                         raw_date_time_zone.find(""+"") - 1]\n        time_zone_parsed = raw_date_time_zone[raw_date_time_zone.find(""+""):]\n\n        try:\n            date_struct = time.strptime(date_parsed, ""%d/%b/%Y"")\n            converted_date = time.strftime(""%Y-%m-%d"", date_struct)\n            date_time = converted_date + "" "" + time_parsed\n\n        # Throws a ValueError exception if the operation fails that is\n        # caught by the calling function and is handled appropriately\n        except ValueError as error:\n            raise ValueError(error)\n        else:\n            return converted_date, date_time, time_zone_parsed\n\n    def mapper(self, _, line):\n        line = line.strip()\n        match = self.logpat.search(line)\n\n        date_time = None\n        requester = None\n        user_agent = None\n        operation = None\n\n        try:\n            for n in range(self.NUM_ENTRIES_PER_LINE):\n                group = match.group(1 + n)\n\n                if n == self.S3_LOG_DATE_TIME:\n                    date, date_time, time_zone_parsed = \\\n                        self.clean_date_time_zone(group)\n                    # Leave the following line of code if \n                    # you want to aggregate by date\n                    date_time = date + "" 00:00:00""\n                elif n == self.S3_LOG_REQUESTER_ID:\n                    requester = group\n                elif n == self.S3_LOG_USER_AGENT:\n                    user_agent = group\n                elif n == self.S3_LOG_OPERATION:\n                    operation = group\n                else:\n                    pass\n\n        except Exception:\n            yield ((""Error while parsing line: %s"", line), 1)\n        else:\n            yield ((date_time, requester, user_agent, operation), 1)\n\n    def reducer(self, key, values):\n        output = list(key)\n        output = self.DELIMITER.join(output) + \\\n                 self.DELIMITER + \\\n                 str(sum(values))\n\n        yield None, output\n\n    def steps(self):\n        return [\n            self.mr(mapper=self.mapper,\n                    reducer=self.reducer)\n        ]\n\n\nif __name__ == \'__main__\':\n    MrS3LogParser.run()'"
mapreduce/test_mr_s3_log_parser.py,0,"b'\nfrom StringIO import StringIO\nimport unittest2 as unittest\nfrom mr_s3_log_parser import MrS3LogParser\n\n\nclass MrTestsUtil:\n\n    def run_mr_sandbox(self, mr_job, stdin):\n        # inline runs the job in the same process so small jobs tend to\n        # run faster and stack traces are simpler\n        # --no-conf prevents options from local mrjob.conf from polluting\n        # the testing environment\n        # ""-"" reads from standard in\n        mr_job.sandbox(stdin=stdin)\n\n        # make_runner ensures job cleanup is performed regardless of\n        # success or failure\n        with mr_job.make_runner() as runner:\n            runner.run()\n            for line in runner.stream_output():\n                key, value = mr_job.parse_output_line(line)\n                yield value\n\n                \nclass TestMrS3LogParser(unittest.TestCase):\n\n    mr_job = None\n    mr_tests_util = None\n\n    RAW_LOG_LINE_INVALID = \\\n        \'00000fe9688b6e57f75bd2b7f7c1610689e8f01000000\' \\\n        \'00000388225bcc00000 \' \\\n        \'s3-storage [22/Jul/2013:21:03:27 +0000] \' \\\n        \'00.111.222.33 \' \\\n\n    RAW_LOG_LINE_VALID = \\\n        \'00000fe9688b6e57f75bd2b7f7c1610689e8f01000000\' \\\n        \'00000388225bcc00000 \' \\\n        \'s3-storage [22/Jul/2013:21:03:27 +0000] \' \\\n        \'00.111.222.33 \' \\\n        \'arn:aws:sts::000005646931:federated-user/user 00000AB825500000 \' \\\n        \'REST.HEAD.OBJECT user/file.pdf \' \\\n        \'""HEAD /user/file.pdf?versionId=00000XMHZJp6DjM9x500000\' \\\n        \'00000SDZk \' \\\n        \'HTTP/1.1"" 200 - - 4000272 18 - ""-"" \' \\\n        \'""Boto/2.5.1 (darwin) USER-AGENT/1.0.14.0"" \' \\\n        \'00000XMHZJp6DjM9x5JVEAMo8MG00000\'\n\n    DATE_TIME_ZONE_INVALID = ""AB/Jul/2013:21:04:17 +0000""\n    DATE_TIME_ZONE_VALID = ""22/Jul/2013:21:04:17 +0000""\n    DATE_VALID = ""2013-07-22""\n    DATE_TIME_VALID = ""2013-07-22 21:04:17""\n    TIME_ZONE_VALID = ""+0000""\n\n    def __init__(self, *args, **kwargs):\n        super(TestMrS3LogParser, self).__init__(*args, **kwargs)\n        self.mr_job = MrS3LogParser([\'-r\', \'inline\', \'--no-conf\', \'-\'])\n        self.mr_tests_util = MrTestsUtil()\n\n    def test_invalid_log_lines(self):\n        stdin = StringIO(self.RAW_LOG_LINE_INVALID)\n\n        for result in self.mr_tests_util.run_mr_sandbox(self.mr_job, stdin):\n            self.assertEqual(result.find(""Error""), 0)\n\n    def test_valid_log_lines(self):\n        stdin = StringIO(self.RAW_LOG_LINE_VALID)\n\n        for result in self.mr_tests_util.run_mr_sandbox(self.mr_job, stdin):\n            self.assertEqual(result.find(""Error""), -1)\n\n    def test_clean_date_time_zone(self):\n        date, date_time, time_zone_parsed = \\\n            self.mr_job.clean_date_time_zone(self.DATE_TIME_ZONE_VALID)\n        self.assertEqual(date, self.DATE_VALID)\n        self.assertEqual(date_time, self.DATE_TIME_VALID)\n        self.assertEqual(time_zone_parsed, self.TIME_ZONE_VALID)\n\n        # Use a lambda to delay the calling of clean_date_time_zone so that\n        # assertRaises has enough time to handle it properly\n        self.assertRaises(ValueError,\n            lambda: self.mr_job.clean_date_time_zone(\n                self.DATE_TIME_ZONE_INVALID))\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
matplotlib/__init__.py,0,b''
numpy/__init__.py,0,b''
pandas/__init__.py,0,b''
python-data/__init__.py,0,b''
python-data/transform_util.py,0,"b'import re\n\n\nclass TransformUtil:\n\n    @classmethod\n    def remove_punctuation(cls, value):\n        """"""Removes !, #, and ?.\n        """"""        \n        return re.sub(\'[!#?]\', \'\', value) \n\n    @classmethod\n    def clean_strings(cls, strings, ops): \n        """"""General purpose method to clean strings.\n\n        Pass in a sequence of strings and the operations to perform.\n        """"""        \n        result = [] \n        for value in strings: \n            for function in ops: \n                value = function(value) \n            result.append(value) \n        return result'"
python-data/type_util.py,0,"b'class TypeUtil:\n\n    @classmethod\n    def is_iterable(cls, obj):\n        """"""Determines if obj is iterable.\n\n        Useful when writing functions that can accept multiple types of\n        input (list, tuple, ndarray, iterator).  Pairs well with\n        convert_to_list.\n        """"""\n        try:\n            iter(obj)\n            return True\n        except TypeError:\n            return False\n\n    @classmethod\n    def convert_to_list(cls, obj):\n        """"""Converts obj to a list if it is not a list and it is iterable, \n        else returns the original obj.\n        """"""\n        if not isinstance(obj, list) and cls.is_iterable(obj):\n            obj = list(obj)\n        return obj'"
scikit-learn/__init__.py,0,b''
scipy/__init__.py,0,b''
scipy/first.py,1,"b'""""""This file contains code used in ""Think Stats"",\nby Allen B. Downey, available from greenteapress.com\n\nCopyright 2014 Allen B. Downey\nLicense: GNU GPLv3 http://www.gnu.org/licenses/gpl.html\n""""""\n\nfrom __future__ import print_function\n\nimport math\nimport numpy as np\n\nimport nsfg\nimport thinkstats2\nimport thinkplot\n\n\ndef MakeFrames():\n    """"""Reads pregnancy data and partitions first babies and others.\n\n    returns: DataFrames (all live births, first babies, others)\n    """"""\n    preg = nsfg.ReadFemPreg()\n\n    live = preg[preg.outcome == 1]\n    firsts = live[live.birthord == 1]\n    others = live[live.birthord != 1]\n\n    assert len(live) == 9148\n    assert len(firsts) == 4413\n    assert len(others) == 4735\n\n    return live, firsts, others\n\n\ndef Summarize(live, firsts, others):\n    """"""Print various summary statistics.""""""\n\n    mean = live.prglngth.mean()\n    var = live.prglngth.var()\n    std = live.prglngth.std()\n\n    print(\'Live mean\', mean)\n    print(\'Live variance\', var)\n    print(\'Live std\', std)\n\n    mean1 = firsts.prglngth.mean()\n    mean2 = others.prglngth.mean()\n\n    var1 = firsts.prglngth.var()\n    var2 = others.prglngth.var()\n\n    print(\'Mean\')\n    print(\'First babies\', mean1)\n    print(\'Others\', mean2)\n\n    print(\'Variance\')\n    print(\'First babies\', var1)\n    print(\'Others\', var2)\n\n    print(\'Difference in weeks\', mean1 - mean2)\n    print(\'Difference in hours\', (mean1 - mean2) * 7 * 24)\n\n    print(\'Difference relative to 39 weeks\', (mean1 - mean2) / 39 * 100)\n\n    d = thinkstats2.CohenEffectSize(firsts.prglngth, others.prglngth)\n    print(\'Cohen d\', d)\n\n\ndef PrintExtremes(live):\n    """"""Plots the histogram of pregnancy lengths and prints the extremes.\n\n    live: DataFrame of live births\n    """"""\n    hist = thinkstats2.Hist(live.prglngth)\n    thinkplot.Hist(hist, label=\'live births\')\n\n    thinkplot.Save(root=\'first_nsfg_hist_live\', \n                   title=\'Histogram\',\n                   xlabel=\'weeks\',\n                   ylabel=\'frequency\')\n\n    print(\'Shortest lengths:\')\n    for weeks, freq in hist.Smallest(10):\n        print(weeks, freq)\n\n    print(\'Longest lengths:\')\n    for weeks, freq in hist.Largest(10):\n        print(weeks, freq)\n    \n\ndef MakeHists(live):\n    """"""Plot Hists for live births\n\n    live: DataFrame\n    others: DataFrame\n    """"""\n    hist = thinkstats2.Hist(live.birthwgt_lb, label=\'birthwgt_lb\')\n    thinkplot.Hist(hist)\n    thinkplot.Save(root=\'first_wgt_lb_hist\', \n                   xlabel=\'pounds\',\n                   ylabel=\'frequency\',\n                   axis=[-1, 14, 0, 3200])\n\n    hist = thinkstats2.Hist(live.birthwgt_oz, label=\'birthwgt_oz\')\n    thinkplot.Hist(hist)\n    thinkplot.Save(root=\'first_wgt_oz_hist\', \n                   xlabel=\'ounces\',\n                   ylabel=\'frequency\',\n                   axis=[-1, 16, 0, 1200])\n\n    hist = thinkstats2.Hist(np.floor(live.agepreg), label=\'agepreg\')\n    thinkplot.Hist(hist)\n    thinkplot.Save(root=\'first_agepreg_hist\', \n                   xlabel=\'years\',\n                   ylabel=\'frequency\')\n\n    hist = thinkstats2.Hist(live.prglngth, label=\'prglngth\')\n    thinkplot.Hist(hist)\n    thinkplot.Save(root=\'first_prglngth_hist\', \n                   xlabel=\'weeks\',\n                   ylabel=\'frequency\',\n                   axis=[-1, 53, 0, 5000])\n\n\ndef MakeComparison(firsts, others):\n    """"""Plots histograms of pregnancy length for first babies and others.\n\n    firsts: DataFrame\n    others: DataFrame\n    """"""\n    first_hist = thinkstats2.Hist(firsts.prglngth, label=\'first\')\n    other_hist = thinkstats2.Hist(others.prglngth, label=\'other\')\n\n    width = 0.45\n    thinkplot.PrePlot(2)\n    thinkplot.Hist(first_hist, align=\'right\', width=width)\n    thinkplot.Hist(other_hist, align=\'left\', width=width)\n\n    thinkplot.Save(root=\'first_nsfg_hist\', \n                   title=\'Histogram\',\n                   xlabel=\'weeks\',\n                   ylabel=\'frequency\',\n                   axis=[27, 46, 0, 2700])\n\n\ndef main(script):\n    live, firsts, others = MakeFrames()\n\n    MakeHists(live)\n    PrintExtremes(live)\n    MakeComparison(firsts, others)\n    Summarize(live, firsts, others)\n\n\nif __name__ == \'__main__\':\n    import sys\n    main(*sys.argv)\n\n\n'"
scipy/nsfg.py,7,"b'""""""This file contains code for use with ""Think Stats"",\nby Allen B. Downey, available from greenteapress.com\n\nCopyright 2010 Allen B. Downey\nLicense: GNU GPLv3 http://www.gnu.org/licenses/gpl.html\n""""""\n\nfrom __future__ import print_function\n\nfrom collections import defaultdict\nimport numpy as np\nimport sys\n\nimport thinkstats2\n\n\ndef ReadFemPreg(dct_file=\'2002FemPreg.dct\',\n                dat_file=\'2002FemPreg.dat.gz\'):\n    """"""Reads the NSFG pregnancy data.\n\n    dct_file: string file name\n    dat_file: string file name\n\n    returns: DataFrame\n    """"""\n    dct = thinkstats2.ReadStataDct(dct_file)\n    df = dct.ReadFixedWidth(dat_file, compression=\'gzip\')\n    CleanFemPreg(df)\n    return df\n\n\ndef CleanFemPreg(df):\n    """"""Recodes variables from the pregnancy frame.\n\n    df: DataFrame\n    """"""\n    # mother\'s age is encoded in centiyears; convert to years\n    df.agepreg /= 100.0\n\n    # birthwgt_lb contains at least one bogus value (51 lbs)\n    # replace with NaN\n    df.birthwgt_lb[df.birthwgt_lb > 20] = np.nan\n    \n    # replace \'not ascertained\', \'refused\', \'don\'t know\' with NaN\n    na_vals = [97, 98, 99]\n    df.birthwgt_lb.replace(na_vals, np.nan, inplace=True)\n    df.birthwgt_oz.replace(na_vals, np.nan, inplace=True)\n    df.hpagelb.replace(na_vals, np.nan, inplace=True)\n\n    df.babysex.replace([7, 9], np.nan, inplace=True)\n    df.nbrnaliv.replace([9], np.nan, inplace=True)\n\n    # birthweight is stored in two columns, lbs and oz.\n    # convert to a single column in lb\n    # NOTE: creating a new column requires dictionary syntax,\n    # not attribute assignment (like df.totalwgt_lb)\n    df[\'totalwgt_lb\'] = df.birthwgt_lb + df.birthwgt_oz / 16.0    \n\n    # due to a bug in ReadStataDct, the last variable gets clipped;\n    # so for now set it to NaN\n    df.cmintvw = np.nan\n\n\ndef MakePregMap(df):\n    """"""Make a map from caseid to list of preg indices.\n\n    df: DataFrame\n\n    returns: dict that maps from caseid to list of indices into preg df\n    """"""\n    d = defaultdict(list)\n    for index, caseid in df.caseid.iteritems():\n        d[caseid].append(index)\n    return d\n\n\ndef main(script):\n    """"""Tests the functions in this module.\n\n    script: string script name\n    """"""\n    df = ReadFemPreg()\n    print(df.shape)\n\n    assert len(df) == 13593\n\n    assert df.caseid[13592] == 12571\n    assert df.pregordr.value_counts()[1] == 5033\n    assert df.nbrnaliv.value_counts()[1] == 8981\n    assert df.babysex.value_counts()[1] == 4641\n    assert df.birthwgt_lb.value_counts()[7] == 3049\n    assert df.birthwgt_oz.value_counts()[0] == 1037\n    assert df.prglngth.value_counts()[39] == 4744\n    assert df.outcome.value_counts()[1] == 9148\n    assert df.birthord.value_counts()[1] == 4413\n    assert df.agepreg.value_counts()[22.75] == 100\n    assert df.totalwgt_lb.value_counts()[7.5] == 302\n\n    weights = df.finalwgt.value_counts()\n    key = max(weights.keys())\n    assert df.finalwgt.value_counts()[key] == 6\n\n    print(\'%s: All tests passed.\' % script)\n\nif __name__ == \'__main__\':\n    main(*sys.argv)\n'"
scipy/thinkplot.py,13,"b'""""""This file contains code for use with ""Think Stats"",\nby Allen B. Downey, available from greenteapress.com\n\nCopyright 2014 Allen B. Downey\nLicense: GNU GPLv3 http://www.gnu.org/licenses/gpl.html\n""""""\n\nfrom __future__ import print_function\n\nimport math\nimport matplotlib\nimport matplotlib.pyplot as pyplot\nimport numpy as np\nimport pandas\n\nimport warnings\n\n# customize some matplotlib attributes\n#matplotlib.rc(\'figure\', figsize=(4, 3))\n\n#matplotlib.rc(\'font\', size=14.0)\n#matplotlib.rc(\'axes\', labelsize=22.0, titlesize=22.0)\n#matplotlib.rc(\'legend\', fontsize=20.0)\n\n#matplotlib.rc(\'xtick.major\', size=6.0)\n#matplotlib.rc(\'xtick.minor\', size=3.0)\n\n#matplotlib.rc(\'ytick.major\', size=6.0)\n#matplotlib.rc(\'ytick.minor\', size=3.0)\n\n\nclass _Brewer(object):\n    """"""Encapsulates a nice sequence of colors.\n\n    Shades of blue that look good in color and can be distinguished\n    in grayscale (up to a point).\n    \n    Borrowed from http://colorbrewer2.org/\n    """"""\n    color_iter = None\n\n    colors = [\'#081D58\',\n              \'#253494\',\n              \'#225EA8\',\n              \'#1D91C0\',\n              \'#41B6C4\',\n              \'#7FCDBB\',\n              \'#C7E9B4\',\n              \'#EDF8B1\',\n              \'#FFFFD9\']\n\n    # lists that indicate which colors to use depending on how many are used\n    which_colors = [[],\n                    [1],\n                    [1, 3],\n                    [0, 2, 4],\n                    [0, 2, 4, 6],\n                    [0, 2, 3, 5, 6],\n                    [0, 2, 3, 4, 5, 6],\n                    [0, 1, 2, 3, 4, 5, 6],\n                    ]\n\n    @classmethod\n    def Colors(cls):\n        """"""Returns the list of colors.\n        """"""\n        return cls.colors\n\n    @classmethod\n    def ColorGenerator(cls, n):\n        """"""Returns an iterator of color strings.\n\n        n: how many colors will be used\n        """"""\n        for i in cls.which_colors[n]:\n            yield cls.colors[i]\n        raise StopIteration(\'Ran out of colors in _Brewer.ColorGenerator\')\n\n    @classmethod\n    def InitializeIter(cls, num):\n        """"""Initializes the color iterator with the given number of colors.""""""\n        cls.color_iter = cls.ColorGenerator(num)\n\n    @classmethod\n    def ClearIter(cls):\n        """"""Sets the color iterator to None.""""""\n        cls.color_iter = None\n\n    @classmethod\n    def GetIter(cls):\n        """"""Gets the color iterator.""""""\n        if cls.color_iter is None:\n            cls.InitializeIter(7)\n\n        return cls.color_iter\n\n\ndef PrePlot(num=None, rows=None, cols=None):\n    """"""Takes hints about what\'s coming.\n\n    num: number of lines that will be plotted\n    rows: number of rows of subplots\n    cols: number of columns of subplots\n    """"""\n    if num:\n        _Brewer.InitializeIter(num)\n\n    if rows is None and cols is None:\n        return\n\n    if rows is not None and cols is None:\n        cols = 1\n\n    if cols is not None and rows is None:\n        rows = 1\n\n    # resize the image, depending on the number of rows and cols\n    size_map = {(1, 1): (8, 6),\n                (1, 2): (14, 6),\n                (1, 3): (14, 6),\n                (2, 2): (10, 10),\n                (2, 3): (16, 10),\n                (3, 1): (8, 10),\n                }\n\n    if (rows, cols) in size_map:\n        fig = pyplot.gcf()\n        fig.set_size_inches(*size_map[rows, cols])\n\n    # create the first subplot\n    if rows > 1 or cols > 1:\n        pyplot.subplot(rows, cols, 1)\n        global SUBPLOT_ROWS, SUBPLOT_COLS\n        SUBPLOT_ROWS = rows\n        SUBPLOT_COLS = cols\n\n\ndef SubPlot(plot_number, rows=None, cols=None):\n    """"""Configures the number of subplots and changes the current plot.\n\n    rows: int\n    cols: int\n    plot_number: int\n    """"""\n    rows = rows or SUBPLOT_ROWS\n    cols = cols or SUBPLOT_COLS\n    pyplot.subplot(rows, cols, plot_number)\n\n\ndef _Underride(d, **options):\n    """"""Add key-value pairs to d only if key is not in d.\n\n    If d is None, create a new dictionary.\n\n    d: dictionary\n    options: keyword args to add to d\n    """"""\n    if d is None:\n        d = {}\n\n    for key, val in options.items():\n        d.setdefault(key, val)\n\n    return d\n\n\ndef Clf():\n    """"""Clears the figure and any hints that have been set.""""""\n    global LOC\n    LOC = None\n    _Brewer.ClearIter()\n    pyplot.clf()\n    fig = pyplot.gcf()\n    fig.set_size_inches(8, 6)\n\n\ndef Figure(**options):\n    """"""Sets options for the current figure.""""""\n    _Underride(options, figsize=(6, 8))\n    pyplot.figure(**options)\n\n\ndef _UnderrideColor(options):\n    if \'color\' in options:\n        return options\n\n    color_iter = _Brewer.GetIter()\n\n    if color_iter:\n        try:\n            options[\'color\'] = next(color_iter)\n        except StopIteration:\n            # TODO: reconsider whether this should warn\n            # warnings.warn(\'Warning: Brewer ran out of colors.\')\n            _Brewer.ClearIter()\n    return options\n\n\ndef Plot(obj, ys=None, style=\'\', **options):\n    """"""Plots a line.\n\n    Args:\n      obj: sequence of x values, or Series, or anything with Render()\n      ys: sequence of y values\n      style: style string passed along to pyplot.plot\n      options: keyword args passed to pyplot.plot\n    """"""\n    options = _UnderrideColor(options)\n    label = getattr(obj, \'label\', \'_nolegend_\')\n    options = _Underride(options, linewidth=3, alpha=0.8, label=label)\n\n    xs = obj\n    if ys is None:\n        if hasattr(obj, \'Render\'):\n            xs, ys = obj.Render()\n        if isinstance(obj, pandas.Series):\n            ys = obj.values\n            xs = obj.index\n\n    if ys is None:\n        pyplot.plot(xs, style, **options)\n    else:\n        pyplot.plot(xs, ys, style, **options)\n\n\ndef FillBetween(xs, y1, y2=None, where=None, **options):\n    """"""Plots a line.\n\n    Args:\n      xs: sequence of x values\n      y1: sequence of y values\n      y2: sequence of y values\n      where: sequence of boolean\n      options: keyword args passed to pyplot.fill_between\n    """"""\n    options = _UnderrideColor(options)\n    options = _Underride(options, linewidth=0, alpha=0.5)\n    pyplot.fill_between(xs, y1, y2, where, **options)\n\n\ndef Bar(xs, ys, **options):\n    """"""Plots a line.\n\n    Args:\n      xs: sequence of x values\n      ys: sequence of y values\n      options: keyword args passed to pyplot.bar\n    """"""\n    options = _UnderrideColor(options)\n    options = _Underride(options, linewidth=0, alpha=0.6)\n    pyplot.bar(xs, ys, **options)\n\n\ndef Scatter(xs, ys=None, **options):\n    """"""Makes a scatter plot.\n\n    xs: x values\n    ys: y values\n    options: options passed to pyplot.scatter\n    """"""\n    options = _Underride(options, color=\'blue\', alpha=0.2, \n                        s=30, edgecolors=\'none\')\n\n    if ys is None and isinstance(xs, pandas.Series):\n        ys = xs.values\n        xs = xs.index\n\n    pyplot.scatter(xs, ys, **options)\n\n\ndef HexBin(xs, ys, **options):\n    """"""Makes a scatter plot.\n\n    xs: x values\n    ys: y values\n    options: options passed to pyplot.scatter\n    """"""\n    options = _Underride(options, cmap=matplotlib.cm.Blues)\n    pyplot.hexbin(xs, ys, **options)\n\n\ndef Pdf(pdf, **options):\n    """"""Plots a Pdf, Pmf, or Hist as a line.\n\n    Args:\n      pdf: Pdf, Pmf, or Hist object\n      options: keyword args passed to pyplot.plot\n    """"""\n    low, high = options.pop(\'low\', None), options.pop(\'high\', None)\n    n = options.pop(\'n\', 101)\n    xs, ps = pdf.Render(low=low, high=high, n=n)\n    options = _Underride(options, label=pdf.label)\n    Plot(xs, ps, **options)\n\n\ndef Pdfs(pdfs, **options):\n    """"""Plots a sequence of PDFs.\n\n    Options are passed along for all PDFs.  If you want different\n    options for each pdf, make multiple calls to Pdf.\n    \n    Args:\n      pdfs: sequence of PDF objects\n      options: keyword args passed to pyplot.plot\n    """"""\n    for pdf in pdfs:\n        Pdf(pdf, **options)\n\n\ndef Hist(hist, **options):\n    """"""Plots a Pmf or Hist with a bar plot.\n\n    The default width of the bars is based on the minimum difference\n    between values in the Hist.  If that\'s too small, you can override\n    it by providing a width keyword argument, in the same units\n    as the values.\n\n    Args:\n      hist: Hist or Pmf object\n      options: keyword args passed to pyplot.bar\n    """"""\n    # find the minimum distance between adjacent values\n    xs, ys = hist.Render()\n\n    if \'width\' not in options:\n        try:\n            options[\'width\'] = 0.9 * np.diff(xs).min()\n        except TypeError:\n            warnings.warn(""Hist: Can\'t compute bar width automatically.""\n                            ""Check for non-numeric types in Hist.""\n                            ""Or try providing width option.""\n                            )\n\n    options = _Underride(options, label=hist.label)\n    options = _Underride(options, align=\'center\')\n    if options[\'align\'] == \'left\':\n        options[\'align\'] = \'edge\'\n    elif options[\'align\'] == \'right\':\n        options[\'align\'] = \'edge\'\n        options[\'width\'] *= -1\n\n    Bar(xs, ys, **options)\n\n\ndef Hists(hists, **options):\n    """"""Plots two histograms as interleaved bar plots.\n\n    Options are passed along for all PMFs.  If you want different\n    options for each pmf, make multiple calls to Pmf.\n\n    Args:\n      hists: list of two Hist or Pmf objects\n      options: keyword args passed to pyplot.plot\n    """"""\n    for hist in hists:\n        Hist(hist, **options)\n\n\ndef Pmf(pmf, **options):\n    """"""Plots a Pmf or Hist as a line.\n\n    Args:\n      pmf: Hist or Pmf object\n      options: keyword args passed to pyplot.plot\n    """"""\n    xs, ys = pmf.Render()\n    low, high = min(xs), max(xs)\n\n    width = options.pop(\'width\', None)\n    if width is None:\n        try:\n            width = np.diff(xs).min()\n        except TypeError:\n            warnings.warn(""Pmf: Can\'t compute bar width automatically.""\n                          ""Check for non-numeric types in Pmf.""\n                          ""Or try providing width option."")\n    points = []\n\n    lastx = np.nan\n    lasty = 0\n    for x, y in zip(xs, ys):\n        if (x - lastx) > 1e-5:\n            points.append((lastx, 0))\n            points.append((x, 0))\n\n        points.append((x, lasty))\n        points.append((x, y))\n        points.append((x+width, y))\n\n        lastx = x + width\n        lasty = y\n    points.append((lastx, 0))\n    pxs, pys = zip(*points)\n\n    align = options.pop(\'align\', \'center\')\n    if align == \'center\':\n        pxs = np.array(pxs) - width/2.0\n    if align == \'right\':\n        pxs = np.array(pxs) - width\n\n    options = _Underride(options, label=pmf.label)\n    Plot(pxs, pys, **options)\n\n\ndef Pmfs(pmfs, **options):\n    """"""Plots a sequence of PMFs.\n\n    Options are passed along for all PMFs.  If you want different\n    options for each pmf, make multiple calls to Pmf.\n    \n    Args:\n      pmfs: sequence of PMF objects\n      options: keyword args passed to pyplot.plot\n    """"""\n    for pmf in pmfs:\n        Pmf(pmf, **options)\n\n\ndef Diff(t):\n    """"""Compute the differences between adjacent elements in a sequence.\n\n    Args:\n        t: sequence of number\n\n    Returns:\n        sequence of differences (length one less than t)\n    """"""\n    diffs = [t[i+1] - t[i] for i in range(len(t)-1)]\n    return diffs\n\n\ndef Cdf(cdf, complement=False, transform=None, **options):\n    """"""Plots a CDF as a line.\n\n    Args:\n      cdf: Cdf object\n      complement: boolean, whether to plot the complementary CDF\n      transform: string, one of \'exponential\', \'pareto\', \'weibull\', \'gumbel\'\n      options: keyword args passed to pyplot.plot\n\n    Returns:\n      dictionary with the scale options that should be passed to\n      Config, Show or Save.\n    """"""\n    xs, ps = cdf.Render()\n    xs = np.asarray(xs)\n    ps = np.asarray(ps)\n\n    scale = dict(xscale=\'linear\', yscale=\'linear\')\n\n    for s in [\'xscale\', \'yscale\']: \n        if s in options:\n            scale[s] = options.pop(s)\n\n    if transform == \'exponential\':\n        complement = True\n        scale[\'yscale\'] = \'log\'\n\n    if transform == \'pareto\':\n        complement = True\n        scale[\'yscale\'] = \'log\'\n        scale[\'xscale\'] = \'log\'\n\n    if complement:\n        ps = [1.0-p for p in ps]\n\n    if transform == \'weibull\':\n        xs = np.delete(xs, -1)\n        ps = np.delete(ps, -1)\n        ps = [-math.log(1.0-p) for p in ps]\n        scale[\'xscale\'] = \'log\'\n        scale[\'yscale\'] = \'log\'\n\n    if transform == \'gumbel\':\n        xs = xp.delete(xs, 0)\n        ps = np.delete(ps, 0)\n        ps = [-math.log(p) for p in ps]\n        scale[\'yscale\'] = \'log\'\n\n    options = _Underride(options, label=cdf.label)\n    Plot(xs, ps, **options)\n    return scale\n\n\ndef Cdfs(cdfs, complement=False, transform=None, **options):\n    """"""Plots a sequence of CDFs.\n    \n    cdfs: sequence of CDF objects\n    complement: boolean, whether to plot the complementary CDF\n    transform: string, one of \'exponential\', \'pareto\', \'weibull\', \'gumbel\'\n    options: keyword args passed to pyplot.plot\n    """"""\n    for cdf in cdfs:\n        Cdf(cdf, complement, transform, **options)\n\n\ndef Contour(obj, pcolor=False, contour=True, imshow=False, **options):\n    """"""Makes a contour plot.\n    \n    d: map from (x, y) to z, or object that provides GetDict\n    pcolor: boolean, whether to make a pseudocolor plot\n    contour: boolean, whether to make a contour plot\n    imshow: boolean, whether to use pyplot.imshow\n    options: keyword args passed to pyplot.pcolor and/or pyplot.contour\n    """"""\n    try:\n        d = obj.GetDict()\n    except AttributeError:\n        d = obj\n\n    _Underride(options, linewidth=3, cmap=matplotlib.cm.Blues)\n\n    xs, ys = zip(*d.keys())\n    xs = sorted(set(xs))\n    ys = sorted(set(ys))\n\n    X, Y = np.meshgrid(xs, ys)\n    func = lambda x, y: d.get((x, y), 0)\n    func = np.vectorize(func)\n    Z = func(X, Y)\n\n    x_formatter = matplotlib.ticker.ScalarFormatter(useOffset=False)\n    axes = pyplot.gca()\n    axes.xaxis.set_major_formatter(x_formatter)\n\n    if pcolor:\n        pyplot.pcolormesh(X, Y, Z, **options)\n    if contour:\n        cs = pyplot.contour(X, Y, Z, **options)\n        pyplot.clabel(cs, inline=1, fontsize=10)\n    if imshow:\n        extent = xs[0], xs[-1], ys[0], ys[-1]\n        pyplot.imshow(Z, extent=extent, **options)\n        \n\ndef Pcolor(xs, ys, zs, pcolor=True, contour=False, **options):\n    """"""Makes a pseudocolor plot.\n    \n    xs:\n    ys:\n    zs:\n    pcolor: boolean, whether to make a pseudocolor plot\n    contour: boolean, whether to make a contour plot\n    options: keyword args passed to pyplot.pcolor and/or pyplot.contour\n    """"""\n    _Underride(options, linewidth=3, cmap=matplotlib.cm.Blues)\n\n    X, Y = np.meshgrid(xs, ys)\n    Z = zs\n\n    x_formatter = matplotlib.ticker.ScalarFormatter(useOffset=False)\n    axes = pyplot.gca()\n    axes.xaxis.set_major_formatter(x_formatter)\n\n    if pcolor:\n        pyplot.pcolormesh(X, Y, Z, **options)\n\n    if contour:\n        cs = pyplot.contour(X, Y, Z, **options)\n        pyplot.clabel(cs, inline=1, fontsize=10)\n        \n\ndef Text(x, y, s, **options):\n    """"""Puts text in a figure.\n\n    x: number\n    y: number\n    s: string\n    options: keyword args passed to pyplot.text\n    """"""\n    options = _Underride(options,\n                         fontsize=16,\n                         verticalalignment=\'top\',\n                         horizontalalignment=\'left\')\n    pyplot.text(x, y, s, **options)\n\n\nLEGEND = True\nLOC = None\n\ndef Config(**options):\n    """"""Configures the plot.\n\n    Pulls options out of the option dictionary and passes them to\n    the corresponding pyplot functions.\n    """"""\n    names = [\'title\', \'xlabel\', \'ylabel\', \'xscale\', \'yscale\',\n             \'xticks\', \'yticks\', \'axis\', \'xlim\', \'ylim\']\n\n    for name in names:\n        if name in options:\n            getattr(pyplot, name)(options[name])\n\n    # looks like this is not necessary: matplotlib understands text loc specs\n    loc_dict = {\'upper right\': 1,\n                \'upper left\': 2,\n                \'lower left\': 3,\n                \'lower right\': 4,\n                \'right\': 5,\n                \'center left\': 6,\n                \'center right\': 7,\n                \'lower center\': 8,\n                \'upper center\': 9,\n                \'center\': 10,\n                }\n\n    global LEGEND\n    LEGEND = options.get(\'legend\', LEGEND)\n\n    if LEGEND:\n        global LOC\n        LOC = options.get(\'loc\', LOC)\n        pyplot.legend(loc=LOC)\n\n\ndef Show(**options):\n    """"""Shows the plot.\n\n    For options, see Config.\n\n    options: keyword args used to invoke various pyplot functions\n    """"""\n    clf = options.pop(\'clf\', True)\n    Config(**options)\n    pyplot.show()\n    if clf:\n        Clf()\n\n\ndef Plotly(**options):\n    """"""Shows the plot.\n\n    For options, see Config.\n\n    options: keyword args used to invoke various pyplot functions\n    """"""\n    clf = options.pop(\'clf\', True)\n    Config(**options)\n    import plotly.plotly as plotly\n    url = plotly.plot_mpl(pyplot.gcf())\n    if clf:\n        Clf()\n    return url\n\n\ndef Save(root=None, formats=None, **options):\n    """"""Saves the plot in the given formats and clears the figure.\n\n    For options, see Config.\n\n    Args:\n      root: string filename root\n      formats: list of string formats\n      options: keyword args used to invoke various pyplot functions\n    """"""\n    clf = options.pop(\'clf\', True)\n    Config(**options)\n\n    if formats is None:\n        formats = [\'pdf\', \'eps\']\n\n    try:\n        formats.remove(\'plotly\')\n        Plotly(clf=False)\n    except ValueError:\n        pass\n\n    if root:\n        for fmt in formats:\n            SaveFormat(root, fmt)\n    if clf:\n        Clf()\n\n\ndef SaveFormat(root, fmt=\'eps\'):\n    """"""Writes the current figure to a file in the given format.\n\n    Args:\n      root: string filename root\n      fmt: string format\n    """"""\n    filename = \'%s.%s\' % (root, fmt)\n    print(\'Writing\', filename)\n    pyplot.savefig(filename, format=fmt, dpi=300)\n\n\n# provide aliases for calling functons with lower-case names\npreplot = PrePlot\nsubplot = SubPlot\nclf = Clf\nfigure = Figure\nplot = Plot\ntext = Text\nscatter = Scatter\npmf = Pmf\npmfs = Pmfs\nhist = Hist\nhists = Hists\ndiff = Diff\ncdf = Cdf\ncdfs = Cdfs\ncontour = Contour\npcolor = Pcolor\nconfig = Config\nshow = Show\nsave = Save\n\n\ndef main():\n    color_iter = _Brewer.ColorGenerator(7)\n    for color in color_iter:\n        print(color)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
scipy/thinkstats2.py,59,"b'""""""This file contains code for use with ""Think Stats"" and\n""Think Bayes"", both by Allen B. Downey, available from greenteapress.com\n\nCopyright 2014 Allen B. Downey\nLicense: GNU GPLv3 http://www.gnu.org/licenses/gpl.html\n""""""\n\nfrom __future__ import print_function, division\n\n""""""This file contains class definitions for:\n\nHist: represents a histogram (map from values to integer frequencies).\n\nPmf: represents a probability mass function (map from values to probs).\n\n_DictWrapper: private parent class for Hist and Pmf.\n\nCdf: represents a discrete cumulative distribution function\n\nPdf: represents a continuous probability density function\n\n""""""\n\nimport bisect\nimport copy\nimport logging\nimport math\nimport random\nimport re\n\nfrom collections import Counter\nfrom operator import itemgetter\n\nimport thinkplot\n\nimport numpy as np\nimport pandas\n\nimport scipy\nfrom scipy import stats\nfrom scipy import special\nfrom scipy import ndimage\n\nfrom io import open\n\nROOT2 = math.sqrt(2)\n\ndef RandomSeed(x):\n    """"""Initialize the random and np.random generators.\n\n    x: int seed\n    """"""\n    random.seed(x)\n    np.random.seed(x)\n    \n\ndef Odds(p):\n    """"""Computes odds for a given probability.\n\n    Example: p=0.75 means 75 for and 25 against, or 3:1 odds in favor.\n\n    Note: when p=1, the formula for odds divides by zero, which is\n    normally undefined.  But I think it is reasonable to define Odds(1)\n    to be infinity, so that\'s what this function does.\n\n    p: float 0-1\n\n    Returns: float odds\n    """"""\n    if p == 1:\n        return float(\'inf\')\n    return p / (1 - p)\n\n\ndef Probability(o):\n    """"""Computes the probability corresponding to given odds.\n\n    Example: o=2 means 2:1 odds in favor, or 2/3 probability\n\n    o: float odds, strictly positive\n\n    Returns: float probability\n    """"""\n    return o / (o + 1)\n\n\ndef Probability2(yes, no):\n    """"""Computes the probability corresponding to given odds.\n\n    Example: yes=2, no=1 means 2:1 odds in favor, or 2/3 probability.\n    \n    yes, no: int or float odds in favor\n    """"""\n    return yes / (yes + no)\n\n\nclass Interpolator(object):\n    """"""Represents a mapping between sorted sequences; performs linear interp.\n\n    Attributes:\n        xs: sorted list\n        ys: sorted list\n    """"""\n\n    def __init__(self, xs, ys):\n        self.xs = xs\n        self.ys = ys\n\n    def Lookup(self, x):\n        """"""Looks up x and returns the corresponding value of y.""""""\n        return self._Bisect(x, self.xs, self.ys)\n\n    def Reverse(self, y):\n        """"""Looks up y and returns the corresponding value of x.""""""\n        return self._Bisect(y, self.ys, self.xs)\n\n    def _Bisect(self, x, xs, ys):\n        """"""Helper function.""""""\n        if x <= xs[0]:\n            return ys[0]\n        if x >= xs[-1]:\n            return ys[-1]\n        i = bisect.bisect(xs, x)\n        frac = 1.0 * (x - xs[i - 1]) / (xs[i] - xs[i - 1])\n        y = ys[i - 1] + frac * 1.0 * (ys[i] - ys[i - 1])\n        return y\n\n\nclass _DictWrapper(object):\n    """"""An object that contains a dictionary.""""""\n\n    def __init__(self, obj=None, label=None):\n        """"""Initializes the distribution.\n\n        obj: Hist, Pmf, Cdf, Pdf, dict, pandas Series, list of pairs\n        label: string label\n        """"""\n        self.label = label if label is not None else \'_nolegend_\'\n        self.d = {}\n\n        # flag whether the distribution is under a log transform\n        self.log = False\n\n        if obj is None:\n            return\n\n        if isinstance(obj, (_DictWrapper, Cdf, Pdf)):\n            self.label = label if label is not None else obj.label\n\n        if isinstance(obj, dict):\n            self.d.update(obj.items())\n        elif isinstance(obj, (_DictWrapper, Cdf, Pdf)):\n            self.d.update(obj.Items())\n        elif isinstance(obj, pandas.Series):\n            self.d.update(obj.value_counts().iteritems())\n        else:\n            # finally, treat it like a list\n            self.d.update(Counter(obj))\n\n        if len(self) > 0 and isinstance(self, Pmf):\n            self.Normalize()\n\n    def __hash__(self):\n        return id(self)\n\n    def __str__(self):\n        cls = self.__class__.__name__\n        return \'%s(%s)\' % (cls, str(self.d))\n\n    __repr__ = __str__\n\n    def __eq__(self, other):\n        return self.d == other.d\n\n    def __len__(self):\n        return len(self.d)\n\n    def __iter__(self):\n        return iter(self.d)\n\n    def iterkeys(self):\n        """"""Returns an iterator over keys.""""""\n        return iter(self.d)\n\n    def __contains__(self, value):\n        return value in self.d\n\n    def __getitem__(self, value):\n        return self.d.get(value, 0)\n\n    def __setitem__(self, value, prob):\n        self.d[value] = prob\n\n    def __delitem__(self, value):\n        del self.d[value]\n\n    def Copy(self, label=None):\n        """"""Returns a copy.\n\n        Make a shallow copy of d.  If you want a deep copy of d,\n        use copy.deepcopy on the whole object.\n\n        label: string label for the new Hist\n\n        returns: new _DictWrapper with the same type\n        """"""\n        new = copy.copy(self)\n        new.d = copy.copy(self.d)\n        new.label = label if label is not None else self.label\n        return new\n\n    def Scale(self, factor):\n        """"""Multiplies the values by a factor.\n\n        factor: what to multiply by\n\n        Returns: new object\n        """"""\n        new = self.Copy()\n        new.d.clear()\n\n        for val, prob in self.Items():\n            new.Set(val * factor, prob)\n        return new\n\n    def Log(self, m=None):\n        """"""Log transforms the probabilities.\n        \n        Removes values with probability 0.\n\n        Normalizes so that the largest logprob is 0.\n        """"""\n        if self.log:\n            raise ValueError(""Pmf/Hist already under a log transform"")\n        self.log = True\n\n        if m is None:\n            m = self.MaxLike()\n\n        for x, p in self.d.items():\n            if p:\n                self.Set(x, math.log(p / m))\n            else:\n                self.Remove(x)\n\n    def Exp(self, m=None):\n        """"""Exponentiates the probabilities.\n\n        m: how much to shift the ps before exponentiating\n\n        If m is None, normalizes so that the largest prob is 1.\n        """"""\n        if not self.log:\n            raise ValueError(""Pmf/Hist not under a log transform"")\n        self.log = False\n\n        if m is None:\n            m = self.MaxLike()\n\n        for x, p in self.d.items():\n            self.Set(x, math.exp(p - m))\n\n    def GetDict(self):\n        """"""Gets the dictionary.""""""\n        return self.d\n\n    def SetDict(self, d):\n        """"""Sets the dictionary.""""""\n        self.d = d\n\n    def Values(self):\n        """"""Gets an unsorted sequence of values.\n\n        Note: one source of confusion is that the keys of this\n        dictionary are the values of the Hist/Pmf, and the\n        values of the dictionary are frequencies/probabilities.\n        """"""\n        return self.d.keys()\n\n    def Items(self):\n        """"""Gets an unsorted sequence of (value, freq/prob) pairs.""""""\n        return self.d.items()\n\n    def Render(self, **options):\n        """"""Generates a sequence of points suitable for plotting.\n\n        Note: options are ignored\n\n        Returns:\n            tuple of (sorted value sequence, freq/prob sequence)\n        """"""\n        if min(self.d.keys()) is np.nan:\n            logging.warning(\'Hist: contains NaN, may not render correctly.\')\n\n        return zip(*sorted(self.Items()))\n\n    def MakeCdf(self, label=None):\n        """"""Makes a Cdf.""""""\n        label = label if label is not None else self.label\n        return Cdf(self, label=label)\n\n    def Print(self):\n        """"""Prints the values and freqs/probs in ascending order.""""""\n        for val, prob in sorted(self.d.items()):\n            print(val, prob)\n\n    def Set(self, x, y=0):\n        """"""Sets the freq/prob associated with the value x.\n\n        Args:\n            x: number value\n            y: number freq or prob\n        """"""\n        self.d[x] = y\n\n    def Incr(self, x, term=1):\n        """"""Increments the freq/prob associated with the value x.\n\n        Args:\n            x: number value\n            term: how much to increment by\n        """"""\n        self.d[x] = self.d.get(x, 0) + term\n\n    def Mult(self, x, factor):\n        """"""Scales the freq/prob associated with the value x.\n\n        Args:\n            x: number value\n            factor: how much to multiply by\n        """"""\n        self.d[x] = self.d.get(x, 0) * factor\n\n    def Remove(self, x):\n        """"""Removes a value.\n\n        Throws an exception if the value is not there.\n\n        Args:\n            x: value to remove\n        """"""\n        del self.d[x]\n\n    def Total(self):\n        """"""Returns the total of the frequencies/probabilities in the map.""""""\n        total = sum(self.d.values())\n        return total\n\n    def MaxLike(self):\n        """"""Returns the largest frequency/probability in the map.""""""\n        return max(self.d.values())\n\n    def Largest(self, n=10):\n        """"""Returns the largest n values, with frequency/probability.\n\n        n: number of items to return\n        """"""\n        return sorted(self.d.items(), reverse=True)[:n]\n\n    def Smallest(self, n=10):\n        """"""Returns the smallest n values, with frequency/probability.\n\n        n: number of items to return\n        """"""\n        return sorted(self.d.items(), reverse=False)[:n]\n\n\nclass Hist(_DictWrapper):\n    """"""Represents a histogram, which is a map from values to frequencies.\n\n    Values can be any hashable type; frequencies are integer counters.\n    """"""\n    def Freq(self, x):\n        """"""Gets the frequency associated with the value x.\n\n        Args:\n            x: number value\n\n        Returns:\n            int frequency\n        """"""\n        return self.d.get(x, 0)\n\n    def Freqs(self, xs):\n        """"""Gets frequencies for a sequence of values.""""""\n        return [self.Freq(x) for x in xs]\n\n    def IsSubset(self, other):\n        """"""Checks whether the values in this histogram are a subset of\n        the values in the given histogram.""""""\n        for val, freq in self.Items():\n            if freq > other.Freq(val):\n                return False\n        return True\n\n    def Subtract(self, other):\n        """"""Subtracts the values in the given histogram from this histogram.""""""\n        for val, freq in other.Items():\n            self.Incr(val, -freq)\n\n\nclass Pmf(_DictWrapper):\n    """"""Represents a probability mass function.\n    \n    Values can be any hashable type; probabilities are floating-point.\n    Pmfs are not necessarily normalized.\n    """"""\n\n    def Prob(self, x, default=0):\n        """"""Gets the probability associated with the value x.\n\n        Args:\n            x: number value\n            default: value to return if the key is not there\n\n        Returns:\n            float probability\n        """"""\n        return self.d.get(x, default)\n\n    def Probs(self, xs):\n        """"""Gets probabilities for a sequence of values.""""""\n        return [self.Prob(x) for x in xs]\n\n    def Percentile(self, percentage):\n        """"""Computes a percentile of a given Pmf.\n\n        Note: this is not super efficient.  If you are planning\n        to compute more than a few percentiles, compute the Cdf.\n\n        percentage: float 0-100\n\n        returns: value from the Pmf\n        """"""\n        p = percentage / 100.0\n        total = 0\n        for val, prob in sorted(self.Items()):\n            total += prob\n            if total >= p:\n                return val\n\n    def ProbGreater(self, x):\n        """"""Probability that a sample from this Pmf exceeds x.\n\n        x: number\n\n        returns: float probability\n        """"""\n        if isinstance(x, _DictWrapper):\n            return PmfProbGreater(self, x)\n        else:\n            t = [prob for (val, prob) in self.d.items() if val > x]\n            return sum(t)\n\n    def ProbLess(self, x):\n        """"""Probability that a sample from this Pmf is less than x.\n\n        x: number\n\n        returns: float probability\n        """"""\n        if isinstance(x, _DictWrapper):\n            return PmfProbLess(self, x)\n        else:\n            t = [prob for (val, prob) in self.d.items() if val < x]\n            return sum(t)\n\n    def __lt__(self, obj):\n        """"""Less than.\n\n        obj: number or _DictWrapper\n\n        returns: float probability\n        """"""\n        return self.ProbLess(obj)\n\n    def __gt__(self, obj):\n        """"""Greater than.\n\n        obj: number or _DictWrapper\n\n        returns: float probability\n        """"""\n        return self.ProbGreater(obj)\n\n    def __ge__(self, obj):\n        """"""Greater than or equal.\n\n        obj: number or _DictWrapper\n\n        returns: float probability\n        """"""\n        return 1 - (self < obj)\n\n    def __le__(self, obj):\n        """"""Less than or equal.\n\n        obj: number or _DictWrapper\n\n        returns: float probability\n        """"""\n        return 1 - (self > obj)\n\n    def Normalize(self, fraction=1.0):\n        """"""Normalizes this PMF so the sum of all probs is fraction.\n\n        Args:\n            fraction: what the total should be after normalization\n\n        Returns: the total probability before normalizing\n        """"""\n        if self.log:\n            raise ValueError(""Normalize: Pmf is under a log transform"")\n\n        total = self.Total()\n        if total == 0.0:\n            raise ValueError(\'Normalize: total probability is zero.\')\n            #logging.warning(\'Normalize: total probability is zero.\')\n            #return total\n\n        factor = fraction / total\n        for x in self.d:\n            self.d[x] *= factor\n\n        return total\n\n    def Random(self):\n        """"""Chooses a random element from this PMF.\n\n        Note: this is not very efficient.  If you plan to call\n        this more than a few times, consider converting to a CDF.\n\n        Returns:\n            float value from the Pmf\n        """"""\n        target = random.random()\n        total = 0.0\n        for x, p in self.d.items():\n            total += p\n            if total >= target:\n                return x\n\n        # we shouldn\'t get here\n        raise ValueError(\'Random: Pmf might not be normalized.\')\n\n    def Mean(self):\n        """"""Computes the mean of a PMF.\n\n        Returns:\n            float mean\n        """"""\n        mean = 0.0\n        for x, p in self.d.items():\n            mean += p * x\n        return mean\n\n    def Var(self, mu=None):\n        """"""Computes the variance of a PMF.\n\n        mu: the point around which the variance is computed;\n                if omitted, computes the mean\n\n        returns: float variance\n        """"""\n        if mu is None:\n            mu = self.Mean()\n\n        var = 0.0\n        for x, p in self.d.items():\n            var += p * (x - mu) ** 2\n        return var\n\n    def Std(self, mu=None):\n        """"""Computes the standard deviation of a PMF.\n\n        mu: the point around which the variance is computed;\n                if omitted, computes the mean\n\n        returns: float standard deviation\n        """"""\n        var = self.Var(mu)\n        return math.sqrt(var)\n\n    def MaximumLikelihood(self):\n        """"""Returns the value with the highest probability.\n\n        Returns: float probability\n        """"""\n        _, val = max((prob, val) for val, prob in self.Items())\n        return val\n\n    def CredibleInterval(self, percentage=90):\n        """"""Computes the central credible interval.\n\n        If percentage=90, computes the 90% CI.\n\n        Args:\n            percentage: float between 0 and 100\n\n        Returns:\n            sequence of two floats, low and high\n        """"""\n        cdf = self.MakeCdf()\n        return cdf.CredibleInterval(percentage)\n\n    def __add__(self, other):\n        """"""Computes the Pmf of the sum of values drawn from self and other.\n\n        other: another Pmf or a scalar\n\n        returns: new Pmf\n        """"""\n        try:\n            return self.AddPmf(other)\n        except AttributeError:\n            return self.AddConstant(other)\n\n    def AddPmf(self, other):\n        """"""Computes the Pmf of the sum of values drawn from self and other.\n\n        other: another Pmf\n\n        returns: new Pmf\n        """"""\n        pmf = Pmf()\n        for v1, p1 in self.Items():\n            for v2, p2 in other.Items():\n                pmf.Incr(v1 + v2, p1 * p2)\n        return pmf\n\n    def AddConstant(self, other):\n        """"""Computes the Pmf of the sum a constant and values from self.\n\n        other: a number\n\n        returns: new Pmf\n        """"""\n        pmf = Pmf()\n        for v1, p1 in self.Items():\n            pmf.Set(v1 + other, p1)\n        return pmf\n\n    def __sub__(self, other):\n        """"""Computes the Pmf of the diff of values drawn from self and other.\n\n        other: another Pmf\n\n        returns: new Pmf\n        """"""\n        try:\n            return self.SubPmf(other)\n        except AttributeError:\n            return self.AddConstant(-other)\n\n    def SubPmf(self, other):\n        """"""Computes the Pmf of the diff of values drawn from self and other.\n\n        other: another Pmf\n\n        returns: new Pmf\n        """"""\n        pmf = Pmf()\n        for v1, p1 in self.Items():\n            for v2, p2 in other.Items():\n                pmf.Incr(v1 - v2, p1 * p2)\n        return pmf\n\n    def __mul__(self, other):\n        """"""Computes the Pmf of the product of values drawn from self and other.\n\n        other: another Pmf\n\n        returns: new Pmf\n        """"""\n        try:\n            return self.MulPmf(other)\n        except AttributeError:\n            return self.MulConstant(other)\n\n    def MulPmf(self, other):\n        """"""Computes the Pmf of the diff of values drawn from self and other.\n\n        other: another Pmf\n\n        returns: new Pmf\n        """"""\n        pmf = Pmf()\n        for v1, p1 in self.Items():\n            for v2, p2 in other.Items():\n                pmf.Incr(v1 * v2, p1 * p2)\n        return pmf\n\n    def MulConstant(self, other):\n        """"""Computes the Pmf of the product of a constant and values from self.\n\n        other: a number\n\n        returns: new Pmf\n        """"""\n        pmf = Pmf()\n        for v1, p1 in self.Items():\n            pmf.Set(v1 * other, p1)\n        return pmf\n\n    def __div__(self, other):\n        """"""Computes the Pmf of the ratio of values drawn from self and other.\n\n        other: another Pmf\n\n        returns: new Pmf\n        """"""\n        try:\n            return self.DivPmf(other)\n        except AttributeError:\n            return self.MulConstant(1/other)\n\n    __truediv__ = __div__\n\n    def DivPmf(self, other):\n        """"""Computes the Pmf of the ratio of values drawn from self and other.\n\n        other: another Pmf\n\n        returns: new Pmf\n        """"""\n        pmf = Pmf()\n        for v1, p1 in self.Items():\n            for v2, p2 in other.Items():\n                pmf.Incr(v1 / v2, p1 * p2)\n        return pmf\n\n    def Max(self, k):\n        """"""Computes the CDF of the maximum of k selections from this dist.\n\n        k: int\n\n        returns: new Cdf\n        """"""\n        cdf = self.MakeCdf()\n        return cdf.Max(k)\n\n\nclass Joint(Pmf):\n    """"""Represents a joint distribution.\n\n    The values are sequences (usually tuples)\n    """"""\n\n    def Marginal(self, i, label=None):\n        """"""Gets the marginal distribution of the indicated variable.\n\n        i: index of the variable we want\n\n        Returns: Pmf\n        """"""\n        pmf = Pmf(label=label)\n        for vs, prob in self.Items():\n            pmf.Incr(vs[i], prob)\n        return pmf\n\n    def Conditional(self, i, j, val, label=None):\n        """"""Gets the conditional distribution of the indicated variable.\n\n        Distribution of vs[i], conditioned on vs[j] = val.\n\n        i: index of the variable we want\n        j: which variable is conditioned on\n        val: the value the jth variable has to have\n\n        Returns: Pmf\n        """"""\n        pmf = Pmf(label=label)\n        for vs, prob in self.Items():\n            if vs[j] != val:\n                continue\n            pmf.Incr(vs[i], prob)\n\n        pmf.Normalize()\n        return pmf\n\n    def MaxLikeInterval(self, percentage=90):\n        """"""Returns the maximum-likelihood credible interval.\n\n        If percentage=90, computes a 90% CI containing the values\n        with the highest likelihoods.\n\n        percentage: float between 0 and 100\n\n        Returns: list of values from the suite\n        """"""\n        interval = []\n        total = 0\n\n        t = [(prob, val) for val, prob in self.Items()]\n        t.sort(reverse=True)\n\n        for prob, val in t:\n            interval.append(val)\n            total += prob\n            if total >= percentage / 100.0:\n                break\n\n        return interval\n\n\ndef MakeJoint(pmf1, pmf2):\n    """"""Joint distribution of values from pmf1 and pmf2.\n\n    Assumes that the PMFs represent independent random variables.\n\n    Args:\n        pmf1: Pmf object\n        pmf2: Pmf object\n\n    Returns:\n        Joint pmf of value pairs\n    """"""\n    joint = Joint()\n    for v1, p1 in pmf1.Items():\n        for v2, p2 in pmf2.Items():\n            joint.Set((v1, v2), p1 * p2)\n    return joint\n\n\ndef MakeHistFromList(t, label=None):\n    """"""Makes a histogram from an unsorted sequence of values.\n\n    Args:\n        t: sequence of numbers\n        label: string label for this histogram\n\n    Returns:\n        Hist object\n    """"""\n    return Hist(t, label=label)\n\n\ndef MakeHistFromDict(d, label=None):\n    """"""Makes a histogram from a map from values to frequencies.\n\n    Args:\n        d: dictionary that maps values to frequencies\n        label: string label for this histogram\n\n    Returns:\n        Hist object\n    """"""\n    return Hist(d, label)\n\n\ndef MakePmfFromList(t, label=None):\n    """"""Makes a PMF from an unsorted sequence of values.\n\n    Args:\n        t: sequence of numbers\n        label: string label for this PMF\n\n    Returns:\n        Pmf object\n    """"""\n    return Pmf(t, label=label)\n\n\ndef MakePmfFromDict(d, label=None):\n    """"""Makes a PMF from a map from values to probabilities.\n\n    Args:\n        d: dictionary that maps values to probabilities\n        label: string label for this PMF\n\n    Returns:\n        Pmf object\n    """"""\n    return Pmf(d, label=label)\n\n\ndef MakePmfFromItems(t, label=None):\n    """"""Makes a PMF from a sequence of value-probability pairs\n\n    Args:\n        t: sequence of value-probability pairs\n        label: string label for this PMF\n\n    Returns:\n        Pmf object\n    """"""\n    return Pmf(dict(t), label=label)\n\n\ndef MakePmfFromHist(hist, label=None):\n    """"""Makes a normalized PMF from a Hist object.\n\n    Args:\n        hist: Hist object\n        label: string label\n\n    Returns:\n        Pmf object\n    """"""\n    if label is None:\n        label = hist.label\n\n    return Pmf(hist, label=label)\n\n\ndef MakeMixture(metapmf, label=\'mix\'):\n    """"""Make a mixture distribution.\n\n    Args:\n      metapmf: Pmf that maps from Pmfs to probs.\n      label: string label for the new Pmf.\n\n    Returns: Pmf object.\n    """"""\n    mix = Pmf(label=label)\n    for pmf, p1 in metapmf.Items():\n        for x, p2 in pmf.Items():\n            mix.Incr(x, p1 * p2)\n    return mix\n\n\ndef MakeUniformPmf(low, high, n):\n    """"""Make a uniform Pmf.\n\n    low: lowest value (inclusive)\n    high: highest value (inclusize)\n    n: number of values\n    """"""\n    pmf = Pmf()\n    for x in np.linspace(low, high, n):\n        pmf.Set(x, 1)\n    pmf.Normalize()\n    return pmf\n\n\nclass Cdf(object):\n    """"""Represents a cumulative distribution function.\n\n    Attributes:\n        xs: sequence of values\n        ps: sequence of probabilities\n        label: string used as a graph label.\n    """"""\n    def __init__(self, obj=None, ps=None, label=None):\n        """"""Initializes.\n        \n        If ps is provided, obj must be the corresponding list of values.\n\n        obj: Hist, Pmf, Cdf, Pdf, dict, pandas Series, list of pairs\n        ps: list of cumulative probabilities\n        label: string label\n        """"""\n        self.label = label if label is not None else \'_nolegend_\'\n\n        if isinstance(obj, (_DictWrapper, Cdf, Pdf)):\n            if not label:\n                self.label = label if label is not None else obj.label\n\n        if obj is None:\n            # caller does not provide obj, make an empty Cdf\n            self.xs = np.asarray([])\n            self.ps = np.asarray([])\n            if ps is not None:\n                logging.warning(""Cdf: can\'t pass ps without also passing xs."")\n            return\n        else:\n            # if the caller provides xs and ps, just store them          \n            if ps is not None:\n                if isinstance(ps, str):\n                    logging.warning(""Cdf: ps can\'t be a string"")\n\n                self.xs = np.asarray(obj)\n                self.ps = np.asarray(ps)\n                return\n\n        # caller has provided just obj, not ps\n        if isinstance(obj, Cdf):\n            self.xs = copy.copy(obj.xs)\n            self.ps = copy.copy(obj.ps)\n            return\n\n        if isinstance(obj, _DictWrapper):\n            dw = obj\n        else:\n            dw = Hist(obj)\n\n        if len(dw) == 0:\n            self.xs = np.asarray([])\n            self.ps = np.asarray([])\n            return\n\n        xs, freqs = zip(*sorted(dw.Items()))\n        self.xs = np.asarray(xs)\n        self.ps = np.cumsum(freqs, dtype=np.float)\n        self.ps /= self.ps[-1]\n\n    def __str__(self):\n        return \'Cdf(%s, %s)\' % (str(self.xs), str(self.ps))\n\n    __repr__ = __str__\n\n    def __len__(self):\n        return len(self.xs)\n\n    def __getitem__(self, x):\n        return self.Prob(x)\n\n    def __setitem__(self):\n        raise UnimplementedMethodException()\n\n    def __delitem__(self):\n        raise UnimplementedMethodException()\n\n    def __eq__(self, other):\n        return np.all(self.xs == other.xs) and np.all(self.ps == other.ps)\n\n    def Copy(self, label=None):\n        """"""Returns a copy of this Cdf.\n\n        label: string label for the new Cdf\n        """"""\n        if label is None:\n            label = self.label\n        return Cdf(list(self.xs), list(self.ps), label=label)\n\n    def MakePmf(self, label=None):\n        """"""Makes a Pmf.""""""\n        if label is None:\n            label = self.label\n        return Pmf(self, label=label)\n\n    def Values(self):\n        """"""Returns a sorted list of values.\n        """"""\n        return self.xs\n\n    def Items(self):\n        """"""Returns a sorted sequence of (value, probability) pairs.\n\n        Note: in Python3, returns an iterator.\n        """"""\n        a = self.ps\n        b = np.roll(a, 1)\n        b[0] = 0\n        return zip(self.xs, a-b)\n\n    def Shift(self, term):\n        """"""Adds a term to the xs.\n\n        term: how much to add\n        """"""\n        new = self.Copy()\n        # don\'t use +=, or else an int array + float yields int array\n        new.xs = new.xs + term\n        return new\n\n    def Scale(self, factor):\n        """"""Multiplies the xs by a factor.\n\n        factor: what to multiply by\n        """"""\n        new = self.Copy()\n        # don\'t use *=, or else an int array * float yields int array\n        new.xs = new.xs * factor\n        return new\n\n    def Prob(self, x):\n        """"""Returns CDF(x), the probability that corresponds to value x.\n\n        Args:\n            x: number\n\n        Returns:\n            float probability\n        """"""\n        if x < self.xs[0]:\n            return 0.0\n        index = bisect.bisect(self.xs, x)\n        p = self.ps[index-1]\n        return p\n\n    def Probs(self, xs):\n        """"""Gets probabilities for a sequence of values.\n\n        xs: any sequence that can be converted to NumPy array\n\n        returns: NumPy array of cumulative probabilities\n        """"""\n        xs = np.asarray(xs)\n        index = np.searchsorted(self.xs, xs, side=\'right\')\n        ps = self.ps[index-1]\n        ps[xs < self.xs[0]] = 0.0\n        return ps\n\n    ProbArray = Probs\n\n    def Value(self, p):\n        """"""Returns InverseCDF(p), the value that corresponds to probability p.\n\n        Args:\n            p: number in the range [0, 1]\n\n        Returns:\n            number value\n        """"""\n        if p < 0 or p > 1:\n            raise ValueError(\'Probability p must be in range [0, 1]\')\n\n        index = bisect.bisect_left(self.ps, p)\n        return self.xs[index]\n\n    def ValueArray(self, ps):\n        """"""Returns InverseCDF(p), the value that corresponds to probability p.\n\n        Args:\n            ps: NumPy array of numbers in the range [0, 1]\n\n        Returns:\n            NumPy array of values\n        """"""\n        ps = np.asarray(ps)\n        if np.any(ps < 0) or np.any(ps > 1):\n            raise ValueError(\'Probability p must be in range [0, 1]\')\n\n        index = np.searchsorted(self.ps, ps, side=\'left\')\n        return self.xs[index]\n\n    def Percentile(self, p):\n        """"""Returns the value that corresponds to percentile p.\n\n        Args:\n            p: number in the range [0, 100]\n\n        Returns:\n            number value\n        """"""\n        return self.Value(p / 100.0)\n\n    def PercentileRank(self, x):\n        """"""Returns the percentile rank of the value x.\n\n        x: potential value in the CDF\n\n        returns: percentile rank in the range 0 to 100\n        """"""\n        return self.Prob(x) * 100.0\n\n    def Random(self):\n        """"""Chooses a random value from this distribution.""""""\n        return self.Value(random.random())\n\n    def Sample(self, n):\n        """"""Generates a random sample from this distribution.\n        \n        n: int length of the sample\n        returns: NumPy array\n        """"""\n        ps = np.random.random(n)\n        return self.ValueArray(ps)\n\n    def Mean(self):\n        """"""Computes the mean of a CDF.\n\n        Returns:\n            float mean\n        """"""\n        old_p = 0\n        total = 0.0\n        for x, new_p in zip(self.xs, self.ps):\n            p = new_p - old_p\n            total += p * x\n            old_p = new_p\n        return total\n\n    def CredibleInterval(self, percentage=90):\n        """"""Computes the central credible interval.\n\n        If percentage=90, computes the 90% CI.\n\n        Args:\n            percentage: float between 0 and 100\n\n        Returns:\n            sequence of two floats, low and high\n        """"""\n        prob = (1 - percentage / 100.0) / 2\n        interval = self.Value(prob), self.Value(1 - prob)\n        return interval\n\n    ConfidenceInterval = CredibleInterval\n\n    def _Round(self, multiplier=1000.0):\n        """"""\n        An entry is added to the cdf only if the percentile differs\n        from the previous value in a significant digit, where the number\n        of significant digits is determined by multiplier.  The\n        default is 1000, which keeps log10(1000) = 3 significant digits.\n        """"""\n        # TODO(write this method)\n        raise UnimplementedMethodException()\n\n    def Render(self, **options):\n        """"""Generates a sequence of points suitable for plotting.\n\n        An empirical CDF is a step function; linear interpolation\n        can be misleading.\n\n        Note: options are ignored\n\n        Returns:\n            tuple of (xs, ps)\n        """"""\n        def interleave(a, b):\n            c = np.empty(a.shape[0] + b.shape[0])\n            c[::2] = a\n            c[1::2] = b\n            return c\n\n        a = np.array(self.xs)\n        xs = interleave(a, a)\n        shift_ps = np.roll(self.ps, 1)\n        shift_ps[0] = 0\n        ps = interleave(shift_ps, self.ps)\n        return xs, ps\n\n    def Max(self, k):\n        """"""Computes the CDF of the maximum of k selections from this dist.\n\n        k: int\n\n        returns: new Cdf\n        """"""\n        cdf = self.Copy()\n        cdf.ps **= k\n        return cdf\n\n\ndef MakeCdfFromItems(items, label=None):\n    """"""Makes a cdf from an unsorted sequence of (value, frequency) pairs.\n\n    Args:\n        items: unsorted sequence of (value, frequency) pairs\n        label: string label for this CDF\n\n    Returns:\n        cdf: list of (value, fraction) pairs\n    """"""\n    return Cdf(dict(items), label=label)\n\n\ndef MakeCdfFromDict(d, label=None):\n    """"""Makes a CDF from a dictionary that maps values to frequencies.\n\n    Args:\n       d: dictionary that maps values to frequencies.\n       label: string label for the data.\n\n    Returns:\n        Cdf object\n    """"""\n    return Cdf(d, label=label)\n\n\ndef MakeCdfFromList(seq, label=None):\n    """"""Creates a CDF from an unsorted sequence.\n\n    Args:\n        seq: unsorted sequence of sortable values\n        label: string label for the cdf\n\n    Returns:\n       Cdf object\n    """"""\n    return Cdf(seq, label=label)\n\n\ndef MakeCdfFromHist(hist, label=None):\n    """"""Makes a CDF from a Hist object.\n\n    Args:\n       hist: Pmf.Hist object\n       label: string label for the data.\n\n    Returns:\n        Cdf object\n    """"""\n    if label is None:\n        label = hist.label\n\n    return Cdf(hist, label=label)\n\n\ndef MakeCdfFromPmf(pmf, label=None):\n    """"""Makes a CDF from a Pmf object.\n\n    Args:\n       pmf: Pmf.Pmf object\n       label: string label for the data.\n\n    Returns:\n        Cdf object\n    """"""\n    if label is None:\n        label = pmf.label\n\n    return Cdf(pmf, label=label)\n\n\nclass UnimplementedMethodException(Exception):\n    """"""Exception if someone calls a method that should be overridden.""""""\n\n\nclass Suite(Pmf):\n    """"""Represents a suite of hypotheses and their probabilities.""""""\n\n    def Update(self, data):\n        """"""Updates each hypothesis based on the data.\n\n        data: any representation of the data\n\n        returns: the normalizing constant\n        """"""\n        for hypo in self.Values():\n            like = self.Likelihood(data, hypo)\n            self.Mult(hypo, like)\n        return self.Normalize()\n\n    def LogUpdate(self, data):\n        """"""Updates a suite of hypotheses based on new data.\n\n        Modifies the suite directly; if you want to keep the original, make\n        a copy.\n\n        Note: unlike Update, LogUpdate does not normalize.\n\n        Args:\n            data: any representation of the data\n        """"""\n        for hypo in self.Values():\n            like = self.LogLikelihood(data, hypo)\n            self.Incr(hypo, like)\n\n    def UpdateSet(self, dataset):\n        """"""Updates each hypothesis based on the dataset.\n\n        This is more efficient than calling Update repeatedly because\n        it waits until the end to Normalize.\n\n        Modifies the suite directly; if you want to keep the original, make\n        a copy.\n\n        dataset: a sequence of data\n\n        returns: the normalizing constant\n        """"""\n        for data in dataset:\n            for hypo in self.Values():\n                like = self.Likelihood(data, hypo)\n                self.Mult(hypo, like)\n        return self.Normalize()\n\n    def LogUpdateSet(self, dataset):\n        """"""Updates each hypothesis based on the dataset.\n\n        Modifies the suite directly; if you want to keep the original, make\n        a copy.\n\n        dataset: a sequence of data\n\n        returns: None\n        """"""\n        for data in dataset:\n            self.LogUpdate(data)\n\n    def Likelihood(self, data, hypo):\n        """"""Computes the likelihood of the data under the hypothesis.\n\n        hypo: some representation of the hypothesis\n        data: some representation of the data\n        """"""\n        raise UnimplementedMethodException()\n\n    def LogLikelihood(self, data, hypo):\n        """"""Computes the log likelihood of the data under the hypothesis.\n\n        hypo: some representation of the hypothesis\n        data: some representation of the data\n        """"""\n        raise UnimplementedMethodException()\n\n    def Print(self):\n        """"""Prints the hypotheses and their probabilities.""""""\n        for hypo, prob in sorted(self.Items()):\n            print(hypo, prob)\n\n    def MakeOdds(self):\n        """"""Transforms from probabilities to odds.\n\n        Values with prob=0 are removed.\n        """"""\n        for hypo, prob in self.Items():\n            if prob:\n                self.Set(hypo, Odds(prob))\n            else:\n                self.Remove(hypo)\n\n    def MakeProbs(self):\n        """"""Transforms from odds to probabilities.""""""\n        for hypo, odds in self.Items():\n            self.Set(hypo, Probability(odds))\n\n\ndef MakeSuiteFromList(t, label=None):\n    """"""Makes a suite from an unsorted sequence of values.\n\n    Args:\n        t: sequence of numbers\n        label: string label for this suite\n\n    Returns:\n        Suite object\n    """"""\n    hist = MakeHistFromList(t, label=label)\n    d = hist.GetDict()\n    return MakeSuiteFromDict(d)\n\n\ndef MakeSuiteFromHist(hist, label=None):\n    """"""Makes a normalized suite from a Hist object.\n\n    Args:\n        hist: Hist object\n        label: string label\n\n    Returns:\n        Suite object\n    """"""\n    if label is None:\n        label = hist.label\n\n    # make a copy of the dictionary\n    d = dict(hist.GetDict())\n    return MakeSuiteFromDict(d, label)\n\n\ndef MakeSuiteFromDict(d, label=None):\n    """"""Makes a suite from a map from values to probabilities.\n\n    Args:\n        d: dictionary that maps values to probabilities\n        label: string label for this suite\n\n    Returns:\n        Suite object\n    """"""\n    suite = Suite(label=label)\n    suite.SetDict(d)\n    suite.Normalize()\n    return suite\n\n\nclass Pdf(object):\n    """"""Represents a probability density function (PDF).""""""\n\n    def Density(self, x):\n        """"""Evaluates this Pdf at x.\n\n        Returns: float or NumPy array of probability density\n        """"""\n        raise UnimplementedMethodException()\n\n    def GetLinspace(self):\n        """"""Get a linspace for plotting.\n\n        Not all subclasses of Pdf implement this.\n\n        Returns: numpy array\n        """"""\n        raise UnimplementedMethodException()\n\n    def MakePmf(self, **options):\n        """"""Makes a discrete version of this Pdf.\n\n        options can include\n        label: string\n        low: low end of range\n        high: high end of range\n        n: number of places to evaluate\n\n        Returns: new Pmf\n        """"""\n        label = options.pop(\'label\', \'\')\n        xs, ds = self.Render(**options)\n        return Pmf(dict(zip(xs, ds)), label=label)\n\n    def Render(self, **options):\n        """"""Generates a sequence of points suitable for plotting.\n\n        If options includes low and high, it must also include n;\n        in that case the density is evaluated an n locations between\n        low and high, including both.\n\n        If options includes xs, the density is evaluate at those location.\n\n        Otherwise, self.GetLinspace is invoked to provide the locations.\n\n        Returns:\n            tuple of (xs, densities)\n        """"""\n        low, high = options.pop(\'low\', None), options.pop(\'high\', None)\n        if low is not None and high is not None:\n            n = options.pop(\'n\', 101)\n            xs = np.linspace(low, high, n)\n        else:\n            xs = options.pop(\'xs\', None)\n            if xs is None:\n                xs = self.GetLinspace()\n            \n        ds = self.Density(xs)\n        return xs, ds\n\n    def Items(self):\n        """"""Generates a sequence of (value, probability) pairs.\n        """"""\n        return zip(*self.Render())\n\n\nclass NormalPdf(Pdf):\n    """"""Represents the PDF of a Normal distribution.""""""\n\n    def __init__(self, mu=0, sigma=1, label=None):\n        """"""Constructs a Normal Pdf with given mu and sigma.\n\n        mu: mean\n        sigma: standard deviation\n        label: string\n        """"""\n        self.mu = mu\n        self.sigma = sigma\n        self.label = label if label is not None else \'_nolegend_\'\n\n    def __str__(self):\n        return \'NormalPdf(%f, %f)\' % (self.mu, self.sigma)\n\n    def GetLinspace(self):\n        """"""Get a linspace for plotting.\n\n        Returns: numpy array\n        """"""\n        low, high = self.mu-3*self.sigma, self.mu+3*self.sigma\n        return np.linspace(low, high, 101)\n\n    def Density(self, xs):\n        """"""Evaluates this Pdf at xs.\n\n        xs: scalar or sequence of floats\n\n        returns: float or NumPy array of probability density\n        """"""\n        return stats.norm.pdf(xs, self.mu, self.sigma)\n\n\nclass ExponentialPdf(Pdf):\n    """"""Represents the PDF of an exponential distribution.""""""\n\n    def __init__(self, lam=1, label=None):\n        """"""Constructs an exponential Pdf with given parameter.\n\n        lam: rate parameter\n        label: string\n        """"""\n        self.lam = lam\n        self.label = label if label is not None else \'_nolegend_\'\n\n    def __str__(self):\n        return \'ExponentialPdf(%f)\' % (self.lam)\n\n    def GetLinspace(self):\n        """"""Get a linspace for plotting.\n\n        Returns: numpy array\n        """"""\n        low, high = 0, 5.0/self.lam\n        return np.linspace(low, high, 101)\n\n    def Density(self, xs):\n        """"""Evaluates this Pdf at xs.\n\n        xs: scalar or sequence of floats\n\n        returns: float or NumPy array of probability density\n        """"""\n        return stats.expon.pdf(xs, scale=1.0/self.lam)\n\n\nclass EstimatedPdf(Pdf):\n    """"""Represents a PDF estimated by KDE.""""""\n\n    def __init__(self, sample, label=None):\n        """"""Estimates the density function based on a sample.\n\n        sample: sequence of data\n        label: string\n        """"""\n        self.label = label if label is not None else \'_nolegend_\'\n        self.kde = stats.gaussian_kde(sample)\n        low = min(sample)\n        high = max(sample)\n        self.linspace = np.linspace(low, high, 101)\n\n    def __str__(self):\n        return \'EstimatedPdf(label=%s)\' % str(self.label)\n\n    def GetLinspace(self):\n        """"""Get a linspace for plotting.\n\n        Returns: numpy array\n        """"""\n        return self.linspace\n\n    def Density(self, xs):\n        """"""Evaluates this Pdf at xs.\n\n        returns: float or NumPy array of probability density\n        """"""\n        return self.kde.evaluate(xs)\n\n\ndef CredibleInterval(pmf, percentage=90):\n    """"""Computes a credible interval for a given distribution.\n\n    If percentage=90, computes the 90% CI.\n\n    Args:\n        pmf: Pmf object representing a posterior distribution\n        percentage: float between 0 and 100\n\n    Returns:\n        sequence of two floats, low and high\n    """"""\n    cdf = pmf.MakeCdf()\n    prob = (1 - percentage / 100.0) / 2\n    interval = cdf.Value(prob), cdf.Value(1 - prob)\n    return interval\n\n\ndef PmfProbLess(pmf1, pmf2):\n    """"""Probability that a value from pmf1 is less than a value from pmf2.\n\n    Args:\n        pmf1: Pmf object\n        pmf2: Pmf object\n\n    Returns:\n        float probability\n    """"""\n    total = 0.0\n    for v1, p1 in pmf1.Items():\n        for v2, p2 in pmf2.Items():\n            if v1 < v2:\n                total += p1 * p2\n    return total\n\n\ndef PmfProbGreater(pmf1, pmf2):\n    """"""Probability that a value from pmf1 is less than a value from pmf2.\n\n    Args:\n        pmf1: Pmf object\n        pmf2: Pmf object\n\n    Returns:\n        float probability\n    """"""\n    total = 0.0\n    for v1, p1 in pmf1.Items():\n        for v2, p2 in pmf2.Items():\n            if v1 > v2:\n                total += p1 * p2\n    return total\n\n\ndef PmfProbEqual(pmf1, pmf2):\n    """"""Probability that a value from pmf1 equals a value from pmf2.\n\n    Args:\n        pmf1: Pmf object\n        pmf2: Pmf object\n\n    Returns:\n        float probability\n    """"""\n    total = 0.0\n    for v1, p1 in pmf1.Items():\n        for v2, p2 in pmf2.Items():\n            if v1 == v2:\n                total += p1 * p2\n    return total\n\n\ndef RandomSum(dists):\n    """"""Chooses a random value from each dist and returns the sum.\n\n    dists: sequence of Pmf or Cdf objects\n\n    returns: numerical sum\n    """"""\n    total = sum(dist.Random() for dist in dists)\n    return total\n\n\ndef SampleSum(dists, n):\n    """"""Draws a sample of sums from a list of distributions.\n\n    dists: sequence of Pmf or Cdf objects\n    n: sample size\n\n    returns: new Pmf of sums\n    """"""\n    pmf = Pmf(RandomSum(dists) for i in range(n))\n    return pmf\n\n\ndef EvalNormalPdf(x, mu, sigma):\n    """"""Computes the unnormalized PDF of the normal distribution.\n\n    x: value\n    mu: mean\n    sigma: standard deviation\n    \n    returns: float probability density\n    """"""\n    return stats.norm.pdf(x, mu, sigma)\n\n\ndef MakeNormalPmf(mu, sigma, num_sigmas, n=201):\n    """"""Makes a PMF discrete approx to a Normal distribution.\n    \n    mu: float mean\n    sigma: float standard deviation\n    num_sigmas: how many sigmas to extend in each direction\n    n: number of values in the Pmf\n\n    returns: normalized Pmf\n    """"""\n    pmf = Pmf()\n    low = mu - num_sigmas * sigma\n    high = mu + num_sigmas * sigma\n\n    for x in np.linspace(low, high, n):\n        p = EvalNormalPdf(x, mu, sigma)\n        pmf.Set(x, p)\n    pmf.Normalize()\n    return pmf\n\n\ndef EvalBinomialPmf(k, n, p):\n    """"""Evaluates the binomial PMF.\n\n    Returns the probabily of k successes in n trials with probability p.\n    """"""\n    return stats.binom.pmf(k, n, p)\n    \n\ndef EvalHypergeomPmf(k, N, K, n):\n    """"""Evaluates the hypergeometric PMF.\n\n    Returns the probabily of k successes in n trials from a population\n    N with K successes in it.\n    """"""\n    return stats.hypergeom.pmf(k, N, K, n)\n    \n\ndef EvalPoissonPmf(k, lam):\n    """"""Computes the Poisson PMF.\n\n    k: number of events\n    lam: parameter lambda in events per unit time\n\n    returns: float probability\n    """"""\n    # don\'t use the scipy function (yet).  for lam=0 it returns NaN;\n    # should be 0.0\n    # return stats.poisson.pmf(k, lam)\n    return lam ** k * math.exp(-lam) / special.gamma(k+1)\n\n\ndef MakePoissonPmf(lam, high, step=1):\n    """"""Makes a PMF discrete approx to a Poisson distribution.\n\n    lam: parameter lambda in events per unit time\n    high: upper bound of the Pmf\n\n    returns: normalized Pmf\n    """"""\n    pmf = Pmf()\n    for k in range(0, high + 1, step):\n        p = EvalPoissonPmf(k, lam)\n        pmf.Set(k, p)\n    pmf.Normalize()\n    return pmf\n\n\ndef EvalExponentialPdf(x, lam):\n    """"""Computes the exponential PDF.\n\n    x: value\n    lam: parameter lambda in events per unit time\n\n    returns: float probability density\n    """"""\n    return lam * math.exp(-lam * x)\n\n\ndef EvalExponentialCdf(x, lam):\n    """"""Evaluates CDF of the exponential distribution with parameter lam.""""""\n    return 1 - math.exp(-lam * x)\n\n\ndef MakeExponentialPmf(lam, high, n=200):\n    """"""Makes a PMF discrete approx to an exponential distribution.\n\n    lam: parameter lambda in events per unit time\n    high: upper bound\n    n: number of values in the Pmf\n\n    returns: normalized Pmf\n    """"""\n    pmf = Pmf()\n    for x in np.linspace(0, high, n):\n        p = EvalExponentialPdf(x, lam)\n        pmf.Set(x, p)\n    pmf.Normalize()\n    return pmf\n\n\ndef StandardNormalCdf(x):\n    """"""Evaluates the CDF of the standard Normal distribution.\n    \n    See http://en.wikipedia.org/wiki/Normal_distribution\n    #Cumulative_distribution_function\n\n    Args:\n        x: float\n                \n    Returns:\n        float\n    """"""\n    return (math.erf(x / ROOT2) + 1) / 2\n\n\ndef EvalNormalCdf(x, mu=0, sigma=1):\n    """"""Evaluates the CDF of the normal distribution.\n    \n    Args:\n        x: float\n\n        mu: mean parameter\n        \n        sigma: standard deviation parameter\n                \n    Returns:\n        float\n    """"""\n    return stats.norm.cdf(x, loc=mu, scale=sigma)\n\n\ndef EvalNormalCdfInverse(p, mu=0, sigma=1):\n    """"""Evaluates the inverse CDF of the normal distribution.\n\n    See http://en.wikipedia.org/wiki/Normal_distribution#Quantile_function  \n\n    Args:\n        p: float\n\n        mu: mean parameter\n        \n        sigma: standard deviation parameter\n                \n    Returns:\n        float\n    """"""\n    return stats.norm.ppf(p, loc=mu, scale=sigma)\n\n\ndef EvalLognormalCdf(x, mu=0, sigma=1):\n    """"""Evaluates the CDF of the lognormal distribution.\n    \n    x: float or sequence\n    mu: mean parameter\n    sigma: standard deviation parameter\n                \n    Returns: float or sequence\n    """"""\n    return stats.lognorm.cdf(x, loc=mu, scale=sigma)\n\n\ndef RenderExpoCdf(lam, low, high, n=101):\n    """"""Generates sequences of xs and ps for an exponential CDF.\n\n    lam: parameter\n    low: float\n    high: float\n    n: number of points to render\n\n    returns: numpy arrays (xs, ps)\n    """"""\n    xs = np.linspace(low, high, n)\n    ps = 1 - np.exp(-lam * xs)\n    #ps = stats.expon.cdf(xs, scale=1.0/lam)\n    return xs, ps\n\n\ndef RenderNormalCdf(mu, sigma, low, high, n=101):\n    """"""Generates sequences of xs and ps for a Normal CDF.\n\n    mu: parameter\n    sigma: parameter\n    low: float\n    high: float\n    n: number of points to render\n\n    returns: numpy arrays (xs, ps)\n    """"""\n    xs = np.linspace(low, high, n)\n    ps = stats.norm.cdf(xs, mu, sigma)\n    return xs, ps\n\n\ndef RenderParetoCdf(xmin, alpha, low, high, n=50):\n    """"""Generates sequences of xs and ps for a Pareto CDF.\n\n    xmin: parameter\n    alpha: parameter\n    low: float\n    high: float\n    n: number of points to render\n\n    returns: numpy arrays (xs, ps)\n    """"""\n    if low < xmin:\n        low = xmin\n    xs = np.linspace(low, high, n)\n    ps = 1 - (xs / xmin) ** -alpha\n    #ps = stats.pareto.cdf(xs, scale=xmin, b=alpha)\n    return xs, ps\n\n\nclass Beta(object):\n    """"""Represents a Beta distribution.\n\n    See http://en.wikipedia.org/wiki/Beta_distribution\n    """"""\n    def __init__(self, alpha=1, beta=1, label=None):\n        """"""Initializes a Beta distribution.""""""\n        self.alpha = alpha\n        self.beta = beta\n        self.label = label if label is not None else \'_nolegend_\'\n\n    def Update(self, data):\n        """"""Updates a Beta distribution.\n\n        data: pair of int (heads, tails)\n        """"""\n        heads, tails = data\n        self.alpha += heads\n        self.beta += tails\n\n    def Mean(self):\n        """"""Computes the mean of this distribution.""""""\n        return self.alpha / (self.alpha + self.beta)\n\n    def Random(self):\n        """"""Generates a random variate from this distribution.""""""\n        return random.betavariate(self.alpha, self.beta)\n\n    def Sample(self, n):\n        """"""Generates a random sample from this distribution.\n\n        n: int sample size\n        """"""\n        size = n,\n        return np.random.beta(self.alpha, self.beta, size)\n\n    def EvalPdf(self, x):\n        """"""Evaluates the PDF at x.""""""\n        return x ** (self.alpha - 1) * (1 - x) ** (self.beta - 1)\n\n    def MakePmf(self, steps=101, label=None):\n        """"""Returns a Pmf of this distribution.\n\n        Note: Normally, we just evaluate the PDF at a sequence\n        of points and treat the probability density as a probability\n        mass.\n\n        But if alpha or beta is less than one, we have to be\n        more careful because the PDF goes to infinity at x=0\n        and x=1.  In that case we evaluate the CDF and compute\n        differences.\n        """"""\n        if self.alpha < 1 or self.beta < 1:\n            cdf = self.MakeCdf()\n            pmf = cdf.MakePmf()\n            return pmf\n\n        xs = [i / (steps - 1.0) for i in range(steps)]\n        probs = [self.EvalPdf(x) for x in xs]\n        pmf = Pmf(dict(zip(xs, probs)), label=label)\n        return pmf\n\n    def MakeCdf(self, steps=101):\n        """"""Returns the CDF of this distribution.""""""\n        xs = [i / (steps - 1.0) for i in range(steps)]\n        ps = [special.betainc(self.alpha, self.beta, x) for x in xs]\n        cdf = Cdf(xs, ps)\n        return cdf\n\n\nclass Dirichlet(object):\n    """"""Represents a Dirichlet distribution.\n\n    See http://en.wikipedia.org/wiki/Dirichlet_distribution\n    """"""\n\n    def __init__(self, n, conc=1, label=None):\n        """"""Initializes a Dirichlet distribution.\n\n        n: number of dimensions\n        conc: concentration parameter (smaller yields more concentration)\n        label: string label\n        """"""\n        if n < 2:\n            raise ValueError(\'A Dirichlet distribution with \'\n                             \'n<2 makes no sense\')\n\n        self.n = n\n        self.params = np.ones(n, dtype=np.float) * conc\n        self.label = label if label is not None else \'_nolegend_\'\n\n    def Update(self, data):\n        """"""Updates a Dirichlet distribution.\n\n        data: sequence of observations, in order corresponding to params\n        """"""\n        m = len(data)\n        self.params[:m] += data\n\n    def Random(self):\n        """"""Generates a random variate from this distribution.\n\n        Returns: normalized vector of fractions\n        """"""\n        p = np.random.gamma(self.params)\n        return p / p.sum()\n\n    def Likelihood(self, data):\n        """"""Computes the likelihood of the data.\n\n        Selects a random vector of probabilities from this distribution.\n\n        Returns: float probability\n        """"""\n        m = len(data)\n        if self.n < m:\n            return 0\n\n        x = data\n        p = self.Random()\n        q = p[:m] ** x\n        return q.prod()\n\n    def LogLikelihood(self, data):\n        """"""Computes the log likelihood of the data.\n\n        Selects a random vector of probabilities from this distribution.\n\n        Returns: float log probability\n        """"""\n        m = len(data)\n        if self.n < m:\n            return float(\'-inf\')\n\n        x = self.Random()\n        y = np.log(x[:m]) * data\n        return y.sum()\n\n    def MarginalBeta(self, i):\n        """"""Computes the marginal distribution of the ith element.\n\n        See http://en.wikipedia.org/wiki/Dirichlet_distribution\n        #Marginal_distributions\n\n        i: int\n\n        Returns: Beta object\n        """"""\n        alpha0 = self.params.sum()\n        alpha = self.params[i]\n        return Beta(alpha, alpha0 - alpha)\n\n    def PredictivePmf(self, xs, label=None):\n        """"""Makes a predictive distribution.\n\n        xs: values to go into the Pmf\n\n        Returns: Pmf that maps from x to the mean prevalence of x\n        """"""\n        alpha0 = self.params.sum()\n        ps = self.params / alpha0\n        return Pmf(zip(xs, ps), label=label)\n\n\ndef BinomialCoef(n, k):\n    """"""Compute the binomial coefficient ""n choose k"".\n\n    n: number of trials\n    k: number of successes\n\n    Returns: float\n    """"""\n    return scipy.misc.comb(n, k)\n\n\ndef LogBinomialCoef(n, k):\n    """"""Computes the log of the binomial coefficient.\n\n    http://math.stackexchange.com/questions/64716/\n    approximating-the-logarithm-of-the-binomial-coefficient\n\n    n: number of trials\n    k: number of successes\n\n    Returns: float\n    """"""\n    return n * math.log(n) - k * math.log(k) - (n - k) * math.log(n - k)\n\n\ndef NormalProbability(ys, jitter=0.0):\n    """"""Generates data for a normal probability plot.\n\n    ys: sequence of values\n    jitter: float magnitude of jitter added to the ys \n\n    returns: numpy arrays xs, ys\n    """"""\n    n = len(ys)\n    xs = np.random.normal(0, 1, n)\n    xs.sort()\n    \n    if jitter:\n        ys = Jitter(ys, jitter)\n    else:\n        ys = np.array(ys)\n    ys.sort()\n\n    return xs, ys\n\n\ndef Jitter(values, jitter=0.5):\n    """"""Jitters the values by adding a uniform variate in (-jitter, jitter).\n\n    values: sequence\n    jitter: scalar magnitude of jitter\n    \n    returns: new numpy array\n    """"""\n    n = len(values)\n    return np.random.uniform(-jitter, +jitter, n) + values\n\n\ndef NormalProbabilityPlot(sample, fit_color=\'0.8\', **options):\n    """"""Makes a normal probability plot with a fitted line.\n\n    sample: sequence of numbers\n    fit_color: color string for the fitted line\n    options: passed along to Plot\n    """"""\n    xs, ys = NormalProbability(sample)\n    mean, var = MeanVar(sample)\n    std = math.sqrt(var)\n\n    fit = FitLine(xs, mean, std)\n    thinkplot.Plot(*fit, color=fit_color, label=\'model\')\n\n    xs, ys = NormalProbability(sample)\n    thinkplot.Plot(xs, ys, **options)\n\n \ndef Mean(xs):\n    """"""Computes mean.\n\n    xs: sequence of values\n\n    returns: float mean\n    """"""\n    return np.mean(xs)\n\n\ndef Var(xs, mu=None, ddof=0):\n    """"""Computes variance.\n\n    xs: sequence of values\n    mu: option known mean\n    ddof: delta degrees of freedom\n\n    returns: float\n    """"""\n    xs = np.asarray(xs)\n\n    if mu is None:\n        mu = xs.mean()\n\n    ds = xs - mu\n    return np.dot(ds, ds) / (len(xs) - ddof)\n\n\ndef Std(xs, mu=None, ddof=0):\n    """"""Computes standard deviation.\n\n    xs: sequence of values\n    mu: option known mean\n    ddof: delta degrees of freedom\n\n    returns: float\n    """"""\n    var = Var(xs, mu, ddof)\n    return math.sqrt(var)\n\n\ndef MeanVar(xs, ddof=0):\n    """"""Computes mean and variance.\n\n    Based on http://stackoverflow.com/questions/19391149/\n    numpy-mean-and-variance-from-single-function\n\n    xs: sequence of values\n    ddof: delta degrees of freedom\n    \n    returns: pair of float, mean and var\n    """"""\n    xs = np.asarray(xs)\n    mean = xs.mean()\n    s2 = Var(xs, mean, ddof)\n    return mean, s2\n\n\ndef Trim(t, p=0.01):\n    """"""Trims the largest and smallest elements of t.\n\n    Args:\n        t: sequence of numbers\n        p: fraction of values to trim off each end\n\n    Returns:\n        sequence of values\n    """"""\n    n = int(p * len(t))\n    t = sorted(t)[n:-n]\n    return t\n\n\ndef TrimmedMean(t, p=0.01):\n    """"""Computes the trimmed mean of a sequence of numbers.\n\n    Args:\n        t: sequence of numbers\n        p: fraction of values to trim off each end\n\n    Returns:\n        float\n    """"""\n    t = Trim(t, p)\n    return Mean(t)\n\n\ndef TrimmedMeanVar(t, p=0.01):\n    """"""Computes the trimmed mean and variance of a sequence of numbers.\n\n    Side effect: sorts the list.\n\n    Args:\n        t: sequence of numbers\n        p: fraction of values to trim off each end\n\n    Returns:\n        float\n    """"""\n    t = Trim(t, p)\n    mu, var = MeanVar(t)\n    return mu, var\n\n\ndef CohenEffectSize(group1, group2):\n    """"""Compute Cohen\'s d.\n\n    group1: Series or NumPy array\n    group2: Series or NumPy array\n\n    returns: float\n    """"""\n    diff = group1.mean() - group2.mean()\n\n    n1, n2 = len(group1), len(group2)\n    var1 = group1.var()\n    var2 = group2.var()\n\n    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n    d = diff / math.sqrt(pooled_var)\n    return d\n\n\ndef Cov(xs, ys, meanx=None, meany=None):\n    """"""Computes Cov(X, Y).\n\n    Args:\n        xs: sequence of values\n        ys: sequence of values\n        meanx: optional float mean of xs\n        meany: optional float mean of ys\n\n    Returns:\n        Cov(X, Y)\n    """"""\n    xs = np.asarray(xs)\n    ys = np.asarray(ys)\n\n    if meanx is None:\n        meanx = np.mean(xs)\n    if meany is None:\n        meany = np.mean(ys)\n\n    cov = np.dot(xs-meanx, ys-meany) / len(xs)\n    return cov\n\n\ndef Corr(xs, ys):\n    """"""Computes Corr(X, Y).\n\n    Args:\n        xs: sequence of values\n        ys: sequence of values\n\n    Returns:\n        Corr(X, Y)\n    """"""\n    xs = np.asarray(xs)\n    ys = np.asarray(ys)\n\n    meanx, varx = MeanVar(xs)\n    meany, vary = MeanVar(ys)\n\n    corr = Cov(xs, ys, meanx, meany) / math.sqrt(varx * vary)\n\n    return corr\n\n\ndef SerialCorr(series, lag=1):\n    """"""Computes the serial correlation of a series.\n\n    series: Series\n    lag: integer number of intervals to shift\n\n    returns: float correlation\n    """"""\n    xs = series[lag:]\n    ys = series.shift(lag)[lag:]\n    corr = Corr(xs, ys)\n    return corr\n\n\ndef SpearmanCorr(xs, ys):\n    """"""Computes Spearman\'s rank correlation.\n\n    Args:\n        xs: sequence of values\n        ys: sequence of values\n\n    Returns:\n        float Spearman\'s correlation\n    """"""\n    xranks = pandas.Series(xs).rank()\n    yranks = pandas.Series(ys).rank()\n    return Corr(xranks, yranks)\n\n\ndef MapToRanks(t):\n    """"""Returns a list of ranks corresponding to the elements in t.\n\n    Args:\n        t: sequence of numbers\n    \n    Returns:\n        list of integer ranks, starting at 1\n    """"""\n    # pair up each value with its index\n    pairs = enumerate(t)\n    \n    # sort by value\n    sorted_pairs = sorted(pairs, key=itemgetter(1))\n\n    # pair up each pair with its rank\n    ranked = enumerate(sorted_pairs)\n\n    # sort by index\n    resorted = sorted(ranked, key=lambda trip: trip[1][0])\n\n    # extract the ranks\n    ranks = [trip[0]+1 for trip in resorted]\n    return ranks\n\n\ndef LeastSquares(xs, ys):\n    """"""Computes a linear least squares fit for ys as a function of xs.\n\n    Args:\n        xs: sequence of values\n        ys: sequence of values\n\n    Returns:\n        tuple of (intercept, slope)\n    """"""\n    meanx, varx = MeanVar(xs)\n    meany = Mean(ys)\n\n    slope = Cov(xs, ys, meanx, meany) / varx\n    inter = meany - slope * meanx\n\n    return inter, slope\n\n\ndef FitLine(xs, inter, slope):\n    """"""Fits a line to the given data.\n\n    xs: sequence of x\n\n    returns: tuple of numpy arrays (sorted xs, fit ys)\n    """"""\n    fit_xs = np.sort(xs)\n    fit_ys = inter + slope * fit_xs\n    return fit_xs, fit_ys\n\n\ndef Residuals(xs, ys, inter, slope):\n    """"""Computes residuals for a linear fit with parameters inter and slope.\n\n    Args:\n        xs: independent variable\n        ys: dependent variable\n        inter: float intercept\n        slope: float slope\n\n    Returns:\n        list of residuals\n    """"""\n    xs = np.asarray(xs)\n    ys = np.asarray(ys)\n    res = ys - (inter + slope * xs)\n    return res\n\n\ndef CoefDetermination(ys, res):\n    """"""Computes the coefficient of determination (R^2) for given residuals.\n\n    Args:\n        ys: dependent variable\n        res: residuals\n        \n    Returns:\n        float coefficient of determination\n    """"""\n    return 1 - Var(res) / Var(ys)\n\n\ndef CorrelatedGenerator(rho):\n    """"""Generates standard normal variates with serial correlation.\n\n    rho: target coefficient of correlation\n\n    Returns: iterable\n    """"""\n    x = random.gauss(0, 1)\n    yield x\n\n    sigma = math.sqrt(1 - rho**2)\n    while True:\n        x = random.gauss(x * rho, sigma)\n        yield x\n\n\ndef CorrelatedNormalGenerator(mu, sigma, rho):\n    """"""Generates normal variates with serial correlation.\n\n    mu: mean of variate\n    sigma: standard deviation of variate\n    rho: target coefficient of correlation\n\n    Returns: iterable\n    """"""\n    for x in CorrelatedGenerator(rho):\n        yield x * sigma + mu\n\n\ndef RawMoment(xs, k):\n    """"""Computes the kth raw moment of xs.\n    """"""\n    return sum(x**k for x in xs) / len(xs)\n\n\ndef CentralMoment(xs, k):\n    """"""Computes the kth central moment of xs.\n    """"""\n    mean = RawMoment(xs, 1)\n    return sum((x - mean)**k for x in xs) / len(xs)\n\n\ndef StandardizedMoment(xs, k):\n    """"""Computes the kth standardized moment of xs.\n    """"""\n    var = CentralMoment(xs, 2)\n    std = math.sqrt(var)\n    return CentralMoment(xs, k) / std**k\n\n\ndef Skewness(xs):\n    """"""Computes skewness.\n    """"""\n    return StandardizedMoment(xs, 3)\n\n\ndef Median(xs):\n    """"""Computes the median (50th percentile) of a sequence.\n\n    xs: sequence or anything else that can initialize a Cdf\n\n    returns: float\n    """"""\n    cdf = Cdf(xs)\n    return cdf.Value(0.5)\n\n\ndef IQR(xs):\n    """"""Computes the interquartile of a sequence.\n\n    xs: sequence or anything else that can initialize a Cdf\n\n    returns: pair of floats\n    """"""\n    cdf = Cdf(xs)\n    return cdf.Value(0.25), cdf.Value(0.75)\n\n\ndef PearsonMedianSkewness(xs):\n    """"""Computes the Pearson median skewness.\n    """"""\n    median = Median(xs)\n    mean = RawMoment(xs, 1)\n    var = CentralMoment(xs, 2)\n    std = math.sqrt(var)\n    gp = 3 * (mean - median) / std\n    return gp\n\n\nclass FixedWidthVariables(object):\n    """"""Represents a set of variables in a fixed width file.""""""\n\n    def __init__(self, variables, index_base=0):\n        """"""Initializes.\n\n        variables: DataFrame\n        index_base: are the indices 0 or 1 based?\n\n        Attributes:\n        colspecs: list of (start, end) index tuples\n        names: list of string variable names\n        """"""\n        self.variables = variables\n\n        # note: by default, subtract 1 from colspecs\n        self.colspecs = variables[[\'start\', \'end\']] - index_base\n\n        # convert colspecs to a list of pair of int\n        self.colspecs = self.colspecs.astype(np.int).values.tolist()\n        self.names = variables[\'name\']\n\n    def ReadFixedWidth(self, filename, **options):\n        """"""Reads a fixed width ASCII file.\n\n        filename: string filename\n\n        returns: DataFrame\n        """"""\n        df = pandas.read_fwf(filename,\n                             colspecs=self.colspecs, \n                             names=self.names,\n                             **options)\n        return df\n\n\ndef ReadStataDct(dct_file, **options):\n    """"""Reads a Stata dictionary file.\n\n    dct_file: string filename\n    options: dict of options passed to open()\n\n    returns: FixedWidthVariables object\n    """"""\n    type_map = dict(byte=int, int=int, long=int, float=float, double=float)\n\n    var_info = []\n    for line in open(dct_file, **options):\n        match = re.search( r\'_column\\(([^)]*)\\)\', line)\n        if match:\n            start = int(match.group(1))\n            t = line.split()\n            vtype, name, fstring = t[1:4]\n            name = name.lower()\n            if vtype.startswith(\'str\'):\n                vtype = str\n            else:\n                vtype = type_map[vtype]\n            long_desc = \' \'.join(t[4:]).strip(\'""\')\n            var_info.append((start, vtype, name, fstring, long_desc))\n            \n    columns = [\'start\', \'type\', \'name\', \'fstring\', \'desc\']\n    variables = pandas.DataFrame(var_info, columns=columns)\n\n    # fill in the end column by shifting the start column\n    variables[\'end\'] = variables.start.shift(-1)\n    variables.loc[len(variables)-1, \'end\'] = 0\n\n    dct = FixedWidthVariables(variables, index_base=1)\n    return dct\n\n\ndef Resample(xs, n=None):\n    """"""Draw a sample from xs with the same length as xs.\n\n    xs: sequence\n    n: sample size (default: len(xs))\n\n    returns: NumPy array\n    """"""\n    if n is None:\n        n = len(xs)\n    return np.random.choice(xs, n, replace=True)\n\n\ndef SampleRows(df, nrows, replace=False):\n    """"""Choose a sample of rows from a DataFrame.\n\n    df: DataFrame\n    nrows: number of rows\n    replace: whether to sample with replacement\n\n    returns: DataDf\n    """"""\n    indices = np.random.choice(df.index, nrows, replace=replace)\n    sample = df.loc[indices]\n    return sample\n\n\ndef ResampleRows(df):\n    """"""Resamples rows from a DataFrame.\n\n    df: DataFrame\n\n    returns: DataFrame\n    """"""\n    return SampleRows(df, len(df), replace=True)\n\n\ndef ResampleRowsWeighted(df, column=\'finalwgt\'):\n    """"""Resamples a DataFrame using probabilities proportional to given column.\n\n    df: DataFrame\n    column: string column name to use as weights\n\n    returns: DataFrame\n    """"""\n    weights = df[column]\n    cdf = Cdf(dict(weights))\n    indices = cdf.Sample(len(weights))\n    sample = df.loc[indices]\n    return sample\n\n\ndef PercentileRow(array, p):\n    """"""Selects the row from a sorted array that maps to percentile p.\n\n    p: float 0--100\n\n    returns: NumPy array (one row)\n    """"""\n    rows, cols = array.shape\n    index = int(rows * p / 100)\n    return array[index,]\n\n\ndef PercentileRows(ys_seq, percents):\n    """"""Given a collection of lines, selects percentiles along vertical axis.\n\n    For example, if ys_seq contains simulation results like ys as a\n    function of time, and percents contains (5, 95), the result would\n    be a 90% CI for each vertical slice of the simulation results.\n\n    ys_seq: sequence of lines (y values)\n    percents: list of percentiles (0-100) to select\n\n    returns: list of NumPy arrays, one for each percentile\n    """"""\n    nrows = len(ys_seq)\n    ncols = len(ys_seq[0])\n    array = np.zeros((nrows, ncols))\n\n    for i, ys in enumerate(ys_seq):\n        array[i,] = ys\n\n    array = np.sort(array, axis=0)\n\n    rows = [PercentileRow(array, p) for p in percents]\n    return rows\n\n\ndef Smooth(xs, sigma=2, **options):\n    """"""Smooths a NumPy array with a Gaussian filter.\n\n    xs: sequence\n    sigma: standard deviation of the filter\n    """"""\n    return ndimage.filters.gaussian_filter1d(xs, sigma, **options)\n\n\nclass HypothesisTest(object):\n    """"""Represents a hypothesis test.""""""\n\n    def __init__(self, data):\n        """"""Initializes.\n\n        data: data in whatever form is relevant\n        """"""\n        self.data = data\n        self.MakeModel()\n        self.actual = self.TestStatistic(data)\n        self.test_stats = None\n        self.test_cdf = None\n\n    def PValue(self, iters=1000):\n        """"""Computes the distribution of the test statistic and p-value.\n\n        iters: number of iterations\n\n        returns: float p-value\n        """"""\n        self.test_stats = [self.TestStatistic(self.RunModel()) \n                           for _ in range(iters)]\n        self.test_cdf = Cdf(self.test_stats)\n\n        count = sum(1 for x in self.test_stats if x >= self.actual)\n        return count / iters\n\n    def MaxTestStat(self):\n        """"""Returns the largest test statistic seen during simulations.\n        """"""\n        return max(self.test_stats)\n\n    def PlotCdf(self, label=None):\n        """"""Draws a Cdf with vertical lines at the observed test stat.\n        """"""\n        def VertLine(x):\n            """"""Draws a vertical line at x.""""""\n            thinkplot.Plot([x, x], [0, 1], color=\'0.8\')\n\n        VertLine(self.actual)\n        thinkplot.Cdf(self.test_cdf, label=label)\n\n    def TestStatistic(self, data):\n        """"""Computes the test statistic.\n\n        data: data in whatever form is relevant        \n        """"""\n        raise UnimplementedMethodException()\n\n    def MakeModel(self):\n        """"""Build a model of the null hypothesis.\n        """"""\n        pass\n\n    def RunModel(self):\n        """"""Run the model of the null hypothesis.\n\n        returns: simulated data\n        """"""\n        raise UnimplementedMethodException()\n\n\ndef main():\n    pass\n    \n\nif __name__ == \'__main__\':\n    main()\n'"
spark/__init__.py,0,b''
data/titanic/genderclassmodel.py,12,"b'"""""" Now that the user can read in a file this creates a model which uses the price, class and gender\nAuthor : AstroDave\nDate : 18th September 2012\nRevised : 28 March 2014\n\n""""""\n\n\nimport csv as csv\nimport numpy as np\n\ncsv_file_object = csv.reader(open(\'train.csv\', \'rb\'))       # Load in the csv file\nheader = csv_file_object.next()                             # Skip the fist line as it is a header\ndata=[]                                                     # Create a variable to hold the data\n\nfor row in csv_file_object:                 # Skip through each row in the csv file\n    data.append(row)                        # adding each row to the data variable\ndata = np.array(data)                       # Then convert from a list to an array\n\n# In order to analyse the price column I need to bin up that data\n# here are my binning parameters, the problem we face is some of the fares are very large\n# So we can either have a lot of bins with nothing in them or we can just lose some\n# information by just considering that anythng over 39 is simply in the last bin.\n# So we add a ceiling\nfare_ceiling = 40\n# then modify the data in the Fare column to = 39, if it is greater or equal to the ceiling\ndata[ data[0::,9].astype(np.float) >= fare_ceiling, 9 ] = fare_ceiling - 1.0\n\nfare_bracket_size = 10\nnumber_of_price_brackets = fare_ceiling / fare_bracket_size\nnumber_of_classes = 3                             # I know there were 1st, 2nd and 3rd classes on board.\nnumber_of_classes = len(np.unique(data[0::,2]))   # But it\'s better practice to calculate this from the Pclass directly:\n                                                  # just take the length of an array of UNIQUE values in column index 2\n\n\n# This reference matrix will show the proportion of survivors as a sorted table of\n# gender, class and ticket fare.\n# First initialize it with all zeros\nsurvival_table = np.zeros([2,number_of_classes,number_of_price_brackets],float)\n\n# I can now find the stats of all the women and men on board\nfor i in xrange(number_of_classes):\n    for j in xrange(number_of_price_brackets):\n\n        women_only_stats = data[ (data[0::,4] == ""female"") \\\n                                 & (data[0::,2].astype(np.float) == i+1) \\\n                                 & (data[0:,9].astype(np.float) >= j*fare_bracket_size) \\\n                                 & (data[0:,9].astype(np.float) < (j+1)*fare_bracket_size), 1]\n\n        men_only_stats = data[ (data[0::,4] != ""female"") \\\n                                 & (data[0::,2].astype(np.float) == i+1) \\\n                                 & (data[0:,9].astype(np.float) >= j*fare_bracket_size) \\\n                                 & (data[0:,9].astype(np.float) < (j+1)*fare_bracket_size), 1]\n\n                                 #if i == 0 and j == 3:\n\n        survival_table[0,i,j] = np.mean(women_only_stats.astype(np.float))  # Female stats\n        survival_table[1,i,j] = np.mean(men_only_stats.astype(np.float))    # Male stats\n\n# Since in python if it tries to find the mean of an array with nothing in it\n# (such that the denominator is 0), then it returns nan, we can convert these to 0\n# by just saying where does the array not equal the array, and set these to 0.\nsurvival_table[ survival_table != survival_table ] = 0.\n\n# Now I have my proportion of survivors, simply round them such that if <0.5\n# I predict they dont surivive, and if >= 0.5 they do\nsurvival_table[ survival_table < 0.5 ] = 0\nsurvival_table[ survival_table >= 0.5 ] = 1\n\n# Now I have my indicator I can read in the test file and write out\n# if a women then survived(1) if a man then did not survived (0)\n# First read in test\ntest_file = open(\'test.csv\', \'rb\')\ntest_file_object = csv.reader(test_file)\nheader = test_file_object.next()\n\n# Also open the a new file so I can write to it. \npredictions_file = open(""genderclassmodel.csv"", ""wb"")\npredictions_file_object = csv.writer(predictions_file)\npredictions_file_object.writerow([""PassengerId"", ""Survived""])\n\n# First thing to do is bin up the price file\nfor row in test_file_object:\n    for j in xrange(number_of_price_brackets):\n        # If there is no fare then place the price of the ticket according to class\n        try:\n            row[8] = float(row[8])    # No fare recorded will come up as a string so\n                                      # try to make it a float\n        except:                       # If fails then just bin the fare according to the class\n            bin_fare = 3 - float(row[1])\n            break                     # Break from the loop and move to the next row\n        if row[8] > fare_ceiling:     # Otherwise now test to see if it is higher\n                                      # than the fare ceiling we set earlier\n            bin_fare = number_of_price_brackets - 1\n            break                     # And then break to the next row\n\n        if row[8] >= j*fare_bracket_size\\\n            and row[8] < (j+1)*fare_bracket_size:     # If passed these tests then loop through\n                                                      # each bin until you find the right one\n                                                      # append it to the bin_fare\n                                                      # and move to the next loop\n            bin_fare = j\n            break\n        # Now I have the binned fare, passenger class, and whether female or male, we can\n        # just cross ref their details with our survival table\n    if row[3] == \'female\':\n        predictions_file_object.writerow([row[0], ""%d"" % int(survival_table[ 0, float(row[1]) - 1, bin_fare ])])\n    else:\n        predictions_file_object.writerow([row[0], ""%d"" % int(survival_table[ 1, float(row[1]) - 1, bin_fare])])\n\n# Close out the files\ntest_file.close()\npredictions_file.close()'"
data/titanic/gendermodel.py,8,"b'"""""" This simple code is desinged to teach a basic user to read in the files in python, simply find what proportion of males and females survived and make a predictive model based on this\nAuthor : AstroDave\nDate : 18 September 2012\nRevised: 28 March 2014\n\n""""""\n\n\nimport csv as csv\nimport numpy as np\n\ncsv_file_object = csv.reader(open(\'train.csv\', \'rb\')) \t# Load in the csv file\nheader = csv_file_object.next() \t\t\t\t\t\t# Skip the fist line as it is a header\ndata=[] \t\t\t\t\t\t\t\t\t\t\t\t# Create a variable to hold the data\n\nfor row in csv_file_object: \t\t\t\t\t\t\t# Skip through each row in the csv file,\n    data.append(row[0:]) \t\t\t\t\t\t\t\t# adding each row to the data variable\ndata = np.array(data) \t\t\t\t\t\t\t\t\t# Then convert from a list to an array.\n\n# Now I have an array of 12 columns and 891 rows\n# I can access any element I want, so the entire first column would\n# be data[0::,0].astype(np.float) -- This means all of the rows (from start to end), in column 0\n# I have to add the .astype() command, because\n# when appending the rows, python thought it was a string - so needed to convert\n\n# Set some variables\nnumber_passengers = np.size(data[0::,1].astype(np.float))\nnumber_survived = np.sum(data[0::,1].astype(np.float))\nproportion_survivors = number_survived / number_passengers \n\n# I can now find the stats of all the women on board,\n# by making an array that lists True/False whether each row is female\nwomen_only_stats = data[0::,4] == ""female"" \t# This finds where all the women are\nmen_only_stats = data[0::,4] != ""female"" \t# This finds where all the men are (note != means \'not equal\')\n\n# I can now filter the whole data, to find statistics for just women, by just placing\n# women_only_stats as a ""mask"" on my full data -- Use it in place of the \'0::\' part of the array index. \n# You can test it by placing it there, and requesting column index [4], and the output should all read \'female\'\n# e.g. try typing this:   data[women_only_stats,4]\nwomen_onboard = data[women_only_stats,1].astype(np.float)\nmen_onboard = data[men_only_stats,1].astype(np.float)\n\n# and derive some statistics about them\nproportion_women_survived = np.sum(women_onboard) / np.size(women_onboard)\nproportion_men_survived = np.sum(men_onboard) / np.size(men_onboard)\n\nprint \'Proportion of women who survived is %s\' % proportion_women_survived\nprint \'Proportion of men who survived is %s\' % proportion_men_survived\n\n# Now that I have my indicator that women were much more likely to survive,\n# I am done with the training set.\n# Now I will read in the test file and write out my simplistic prediction:\n# if female, then model that she survived (1) \n# if male, then model that he did not survive (0)\n\n# First, read in test.csv\ntest_file = open(\'test.csv\', \'rb\')\ntest_file_object = csv.reader(test_file)\nheader = test_file_object.next()\n\n# Also open the a new file so I can write to it. Call it something descriptive\n# Finally, loop through each row in the train file, and look in column index [3] (which is \'Sex\')\n# Write out the PassengerId, and my prediction.\n\npredictions_file = open(""gendermodel.csv"", ""wb"")\npredictions_file_object = csv.writer(predictions_file)\npredictions_file_object.writerow([""PassengerId"", ""Survived""])\t# write the column headers\nfor row in test_file_object:\t\t\t\t\t\t\t\t\t# For each row in test file,\n    if row[3] == \'female\':\t\t\t\t\t\t\t\t\t\t# is it a female, if yes then\n        predictions_file_object.writerow([row[0], ""1""])\t\t\t# write the PassengerId, and predict 1\n    else:\t\t\t\t\t\t\t\t\t\t\t\t\t\t# or else if male,\n        predictions_file_object.writerow([row[0], ""0""])\t\t\t# write the PassengerId, and predict 0.\ntest_file.close()\t\t\t\t\t\t\t\t\t\t\t\t# Close out the files.\npredictions_file.close()\n\n'"
data/titanic/myfirstforest.py,2,"b'"""""" Writing my first randomforest code.\nAuthor : AstroDave\nDate : 23rd September 2012\nRevised: 15 April 2014\nplease see packages.python.org/milk/randomforests.html for more\n\n"""""" \nimport pandas as pd\nimport numpy as np\nimport csv as csv\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Data cleanup\n# TRAIN DATA\ntrain_df = pd.read_csv(\'train.csv\', header=0)        # Load the train file into a dataframe\n\n# I need to convert all strings to integer classifiers.\n# I need to fill in the missing values of the data and make it complete.\n\n# female = 0, Male = 1\ntrain_df[\'Gender\'] = train_df[\'Sex\'].map( {\'female\': 0, \'male\': 1} ).astype(int)\n\n# Embarked from \'C\', \'Q\', \'S\'\n# Note this is not ideal: in translating categories to numbers, Port ""2"" is not 2 times greater than Port ""1"", etc.\n\n# All missing Embarked -> just make them embark from most common place\nif len(train_df.Embarked[ train_df.Embarked.isnull() ]) > 0:\n    train_df.Embarked[ train_df.Embarked.isnull() ] = train_df.Embarked.dropna().mode().values\n\nPorts = list(enumerate(np.unique(train_df[\'Embarked\'])))    # determine all values of Embarked,\nPorts_dict = { name : i for i, name in Ports }              # set up a dictionary in the form  Ports : index\ntrain_df.Embarked = train_df.Embarked.map( lambda x: Ports_dict[x]).astype(int)     # Convert all Embark strings to int\n\n# All the ages with no data -> make the median of all Ages\nmedian_age = train_df[\'Age\'].dropna().median()\nif len(train_df.Age[ train_df.Age.isnull() ]) > 0:\n    train_df.loc[ (train_df.Age.isnull()), \'Age\'] = median_age\n\n# Remove the Name column, Cabin, Ticket, and Sex (since I copied and filled it to Gender)\ntrain_df = train_df.drop([\'Name\', \'Sex\', \'Ticket\', \'Cabin\', \'PassengerId\'], axis=1) \n\n\n# TEST DATA\ntest_df = pd.read_csv(\'test.csv\', header=0)        # Load the test file into a dataframe\n\n# I need to do the same with the test data now, so that the columns are the same as the training data\n# I need to convert all strings to integer classifiers:\n# female = 0, Male = 1\ntest_df[\'Gender\'] = test_df[\'Sex\'].map( {\'female\': 0, \'male\': 1} ).astype(int)\n\n# Embarked from \'C\', \'Q\', \'S\'\n# All missing Embarked -> just make them embark from most common place\nif len(test_df.Embarked[ test_df.Embarked.isnull() ]) > 0:\n    test_df.Embarked[ test_df.Embarked.isnull() ] = test_df.Embarked.dropna().mode().values\n# Again convert all Embarked strings to int\ntest_df.Embarked = test_df.Embarked.map( lambda x: Ports_dict[x]).astype(int)\n\n\n# All the ages with no data -> make the median of all Ages\nmedian_age = test_df[\'Age\'].dropna().median()\nif len(test_df.Age[ test_df.Age.isnull() ]) > 0:\n    test_df.loc[ (test_df.Age.isnull()), \'Age\'] = median_age\n\n# All the missing Fares -> assume median of their respective class\nif len(test_df.Fare[ test_df.Fare.isnull() ]) > 0:\n    median_fare = np.zeros(3)\n    for f in range(0,3):                                              # loop 0 to 2\n        median_fare[f] = test_df[ test_df.Pclass == f+1 ][\'Fare\'].dropna().median()\n    for f in range(0,3):                                              # loop 0 to 2\n        test_df.loc[ (test_df.Fare.isnull()) & (test_df.Pclass == f+1 ), \'Fare\'] = median_fare[f]\n\n# Collect the test data\'s PassengerIds before dropping it\nids = test_df[\'PassengerId\'].values\n# Remove the Name column, Cabin, Ticket, and Sex (since I copied and filled it to Gender)\ntest_df = test_df.drop([\'Name\', \'Sex\', \'Ticket\', \'Cabin\', \'PassengerId\'], axis=1) \n\n\n# The data is now ready to go. So lets fit to the train, then predict to the test!\n# Convert back to a numpy array\ntrain_data = train_df.values\ntest_data = test_df.values\n\n\nprint \'Training...\'\nforest = RandomForestClassifier(n_estimators=100)\nforest = forest.fit( train_data[0::,1::], train_data[0::,0] )\n\nprint \'Predicting...\'\noutput = forest.predict(test_data).astype(int)\n\n\npredictions_file = open(""myfirstforest.csv"", ""wb"")\nopen_file_object = csv.writer(predictions_file)\nopen_file_object.writerow([""PassengerId"",""Survived""])\nopen_file_object.writerows(zip(ids, output))\npredictions_file.close()\nprint \'Done.\'\n'"
deep-learning/keras-tutorial/data_helpers.py,5,"b'import numpy as np\nimport re\nimport itertools\nfrom collections import Counter\n""""""\nOriginal taken from https://github.com/dennybritz/cnn-text-classification-tf\n""""""\n\ndef clean_str(string):\n    """"""\n    Tokenization/string cleaning for all datasets except for SST.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    """"""\n    string = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\n    string = re.sub(r""\\\'s"", "" \\\'s"", string)\n    string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n    string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n    string = re.sub(r""\\\'re"", "" \\\'re"", string)\n    string = re.sub(r""\\\'d"", "" \\\'d"", string)\n    string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n    string = re.sub(r"","", "" , "", string)\n    string = re.sub(r""!"", "" ! "", string)\n    string = re.sub(r""\\("", "" \\( "", string)\n    string = re.sub(r""\\)"", "" \\) "", string)\n    string = re.sub(r""\\?"", "" \\? "", string)\n    string = re.sub(r""\\s{2,}"", "" "", string)\n    return string.strip().lower()\n\n\ndef load_data_and_labels():\n    """"""\n    Loads MR polarity data from files, splits the data into words and generates labels.\n    Returns split sentences and labels.\n    """"""\n    # Load data from files\n    positive_examples = list(open(""./data/rt-polarity.pos"", encoding=\'ISO-8859-1\').readlines())\n    positive_examples = [s.strip() for s in positive_examples]\n    negative_examples = list(open(""./data/rt-polarity.neg"", encoding=\'ISO-8859-1\').readlines())\n    negative_examples = [s.strip() for s in negative_examples]\n    # Split by words\n    x_text = positive_examples + negative_examples\n    x_text = [clean_str(sent) for sent in x_text]\n    x_text = [s.split("" "") for s in x_text]\n    # Generate labels\n    positive_labels = [[0, 1] for _ in positive_examples]\n    negative_labels = [[1, 0] for _ in negative_examples]\n    y = np.concatenate([positive_labels, negative_labels], 0)\n    return [x_text, y]\n\n\ndef pad_sentences(sentences, padding_word=""<PAD/>""):\n    """"""\n    Pads all sentences to the same length. The length is defined by the longest sentence.\n    Returns padded sentences.\n    """"""\n    sequence_length = max(len(x) for x in sentences)\n    padded_sentences = []\n    for i in range(len(sentences)):\n        sentence = sentences[i]\n        num_padding = sequence_length - len(sentence)\n        new_sentence = sentence + [padding_word] * num_padding\n        padded_sentences.append(new_sentence)\n    return padded_sentences\n\n\ndef build_vocab(sentences):\n    """"""\n    Builds a vocabulary mapping from word to index based on the sentences.\n    Returns vocabulary mapping and inverse vocabulary mapping.\n    """"""\n    # Build vocabulary\n    word_counts = Counter(itertools.chain(*sentences))\n    # Mapping from index to word\n    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n    # Mapping from word to index\n    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n    return [vocabulary, vocabulary_inv]\n\n\ndef build_input_data(sentences, labels, vocabulary):\n    """"""\n    Maps sentencs and labels to vectors based on a vocabulary.\n    """"""\n    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n    y = np.array(labels)\n    return [x, y]\n\n\ndef load_data():\n    """"""\n    Loads and preprocessed data for the MR dataset.\n    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n    """"""\n    # Load and preprocess data\n    sentences, labels = load_data_and_labels()\n    sentences_padded = pad_sentences(sentences)\n    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n    x, y = build_input_data(sentences_padded, labels, vocabulary)\n    return [x, y, vocabulary, vocabulary_inv]\n\n\ndef batch_iter(data, batch_size, num_epochs):\n    """"""\n    Generates a batch iterator for a dataset.\n    """"""\n    data = np.array(data)\n    data_size = len(data)\n    num_batches_per_epoch = int(len(data)/batch_size) + 1\n    for epoch in range(num_epochs):\n        # Shuffle the data at each epoch\n        shuffle_indices = np.random.permutation(np.arange(data_size))\n        shuffled_data = data[shuffle_indices]\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)\n            yield shuffled_data[start_index:end_index]'"
deep-learning/keras-tutorial/w2v.py,2,"b'from gensim.models import word2vec\nfrom os.path import join, exists, split\nimport os\nimport numpy as np\n\ndef train_word2vec(sentence_matrix, vocabulary_inv,\n                   num_features=300, min_word_count=1, context=10):\n    """"""\n    Trains, saves, loads Word2Vec model\n    Returns initial weights for embedding layer.\n   \n    inputs:\n    sentence_matrix # int matrix: num_sentences x max_sentence_len\n    vocabulary_inv  # dict {str:int}\n    num_features    # Word vector dimensionality                      \n    min_word_count  # Minimum word count                        \n    context         # Context window size \n    """"""\n    model_dir = \'word2vec_models\'\n    model_name = ""{:d}features_{:d}minwords_{:d}context"".format(num_features, min_word_count, context)\n    model_name = join(model_dir, model_name)\n    if exists(model_name):\n        embedding_model = word2vec.Word2Vec.load(model_name)\n        print(\'Loading existing Word2Vec model \\\'%s\\\'\' % split(model_name)[-1])\n    else:\n        # Set values for various parameters\n        num_workers = 2       # Number of threads to run in parallel\n        downsampling = 1e-3   # Downsample setting for frequent words\n        \n        # Initialize and train the model\n        print(""Training Word2Vec model..."")\n        sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n        embedding_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n                            size=num_features, min_count = min_word_count, \\\n                            window = context, sample = downsampling)\n        \n        # If we don\'t plan to train the model any further, calling \n        # init_sims will make the model much more memory-efficient.\n        embedding_model.init_sims(replace=True)\n        \n        # Saving the model for later use. You can load it later using Word2Vec.load()\n        if not exists(model_dir):\n            os.mkdir(model_dir)\n        print(\'Saving Word2Vec model \\\'%s\\\'\' % split(model_name)[-1])\n        embedding_model.save(model_name)\n    \n    #  add unknown words\n    embedding_weights = [np.array([embedding_model[w] if w in embedding_model\\\n                                                        else np.random.uniform(-0.25,0.25,embedding_model.vector_size)\\\n                                                        for w in vocabulary_inv])]\n    return embedding_weights\n\nif __name__==\'__main__\':\n    import data_helpers\n    print(""Loading data..."")\n    x, _, _, vocabulary_inv = data_helpers.load_data()\n    w = train_word2vec(x, vocabulary_inv)\n'"
deep-learning/tensor-flow-examples/input_data.py,0,"b'""""""Functions for downloading and reading MNIST data.""""""\nfrom __future__ import print_function\nimport gzip\nimport os\nimport urllib\nimport numpy\nSOURCE_URL = \'http://yann.lecun.com/exdb/mnist/\'\ndef maybe_download(filename, work_directory):\n  """"""Download the data from Yann\'s website, unless it\'s already here.""""""\n  if not os.path.exists(work_directory):\n    os.mkdir(work_directory)\n  filepath = os.path.join(work_directory, filename)\n  if not os.path.exists(filepath):\n    filepath, _ = urllib.urlretrieve(SOURCE_URL + filename, filepath)\n    statinfo = os.stat(filepath)\n    print(\'Succesfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  return filepath\ndef _read32(bytestream):\n  dt = numpy.dtype(numpy.uint32).newbyteorder(\'>\')\n  return numpy.frombuffer(bytestream.read(4), dtype=dt)\ndef extract_images(filename):\n  """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].""""""\n  print(\'Extracting\', filename)\n  with gzip.open(filename) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2051:\n      raise ValueError(\n          \'Invalid magic number %d in MNIST image file: %s\' %\n          (magic, filename))\n    num_images = _read32(bytestream)\n    rows = _read32(bytestream)\n    cols = _read32(bytestream)\n    buf = bytestream.read(rows * cols * num_images)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n    data = data.reshape(num_images, rows, cols, 1)\n    return data\ndef dense_to_one_hot(labels_dense, num_classes=10):\n  """"""Convert class labels from scalars to one-hot vectors.""""""\n  num_labels = labels_dense.shape[0]\n  index_offset = numpy.arange(num_labels) * num_classes\n  labels_one_hot = numpy.zeros((num_labels, num_classes))\n  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n  return labels_one_hot\ndef extract_labels(filename, one_hot=False):\n  """"""Extract the labels into a 1D uint8 numpy array [index].""""""\n  print(\'Extracting\', filename)\n  with gzip.open(filename) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2049:\n      raise ValueError(\n          \'Invalid magic number %d in MNIST label file: %s\' %\n          (magic, filename))\n    num_items = _read32(bytestream)\n    buf = bytestream.read(num_items)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n    if one_hot:\n      return dense_to_one_hot(labels)\n    return labels\nclass DataSet(object):\n  def __init__(self, images, labels, fake_data=False):\n    if fake_data:\n      self._num_examples = 10000\n    else:\n      assert images.shape[0] == labels.shape[0], (\n          ""images.shape: %s labels.shape: %s"" % (images.shape,\n                                                 labels.shape))\n      self._num_examples = images.shape[0]\n      # Convert shape from [num examples, rows, columns, depth]\n      # to [num examples, rows*columns] (assuming depth == 1)\n      assert images.shape[3] == 1\n      images = images.reshape(images.shape[0],\n                              images.shape[1] * images.shape[2])\n      # Convert from [0, 255] -> [0.0, 1.0].\n      images = images.astype(numpy.float32)\n      images = numpy.multiply(images, 1.0 / 255.0)\n    self._images = images\n    self._labels = labels\n    self._epochs_completed = 0\n    self._index_in_epoch = 0\n  @property\n  def images(self):\n    return self._images\n  @property\n  def labels(self):\n    return self._labels\n  @property\n  def num_examples(self):\n    return self._num_examples\n  @property\n  def epochs_completed(self):\n    return self._epochs_completed\n  def next_batch(self, batch_size, fake_data=False):\n    """"""Return the next `batch_size` examples from this data set.""""""\n    if fake_data:\n      fake_image = [1.0 for _ in xrange(784)]\n      fake_label = 0\n      return [fake_image for _ in xrange(batch_size)], [\n          fake_label for _ in xrange(batch_size)]\n    start = self._index_in_epoch\n    self._index_in_epoch += batch_size\n    if self._index_in_epoch > self._num_examples:\n      # Finished epoch\n      self._epochs_completed += 1\n      # Shuffle the data\n      perm = numpy.arange(self._num_examples)\n      numpy.random.shuffle(perm)\n      self._images = self._images[perm]\n      self._labels = self._labels[perm]\n      # Start next epoch\n      start = 0\n      self._index_in_epoch = batch_size\n      assert batch_size <= self._num_examples\n    end = self._index_in_epoch\n    return self._images[start:end], self._labels[start:end]\ndef read_data_sets(train_dir, fake_data=False, one_hot=False):\n  class DataSets(object):\n    pass\n  data_sets = DataSets()\n  if fake_data:\n    data_sets.train = DataSet([], [], fake_data=True)\n    data_sets.validation = DataSet([], [], fake_data=True)\n    data_sets.test = DataSet([], [], fake_data=True)\n    return data_sets\n  TRAIN_IMAGES = \'train-images-idx3-ubyte.gz\'\n  TRAIN_LABELS = \'train-labels-idx1-ubyte.gz\'\n  TEST_IMAGES = \'t10k-images-idx3-ubyte.gz\'\n  TEST_LABELS = \'t10k-labels-idx1-ubyte.gz\'\n  VALIDATION_SIZE = 5000\n  local_file = maybe_download(TRAIN_IMAGES, train_dir)\n  train_images = extract_images(local_file)\n  local_file = maybe_download(TRAIN_LABELS, train_dir)\n  train_labels = extract_labels(local_file, one_hot=one_hot)\n  local_file = maybe_download(TEST_IMAGES, train_dir)\n  test_images = extract_images(local_file)\n  local_file = maybe_download(TEST_LABELS, train_dir)\n  test_labels = extract_labels(local_file, one_hot=one_hot)\n  validation_images = train_images[:VALIDATION_SIZE]\n  validation_labels = train_labels[:VALIDATION_SIZE]\n  train_images = train_images[VALIDATION_SIZE:]\n  train_labels = train_labels[VALIDATION_SIZE:]\n  data_sets.train = DataSet(train_images, train_labels)\n  data_sets.validation = DataSet(validation_images, validation_labels)\n  data_sets.test = DataSet(test_images, test_labels)\n  return data_sets'"
deep-learning/tensor-flow-examples/multigpu_basics.py,2,"b'#Multi GPU Basic example\n\'\'\'\nThis tutorial requires your machine to have 2 GPUs\n""/cpu:0"": The CPU of your machine.\n""/gpu:0"": The first GPU of your machine\n""/gpu:1"": The second GPU of your machine\n\'\'\'\n\nimport numpy as np\nimport tensorflow as tf\nimport datetime\n\n#Processing Units logs\nlog_device_placement = True\n\n#num of multiplications to perform\nn = 10\n\n\'\'\'\nExample: compute A^n + B^n on 2 GPUs\nResults on 8 cores with 2 GTX-980:\n * Single GPU computation time: 0:00:11.277449\n * Multi GPU computation time: 0:00:07.131701\n\'\'\'\n#Create random large matrix\nA = np.random.rand(1e4, 1e4).astype(\'float32\')\nB = np.random.rand(1e4, 1e4).astype(\'float32\')\n\n# Creates a graph to store results\nc1 = []\nc2 = []\n\ndef matpow(M, n):\n    if n < 1: #Abstract cases where n < 1\n        return M\n    else:\n        return tf.matmul(M, matpow(M, n-1))\n\n\'\'\'\nSingle GPU computing\n\'\'\'\nwith tf.device(\'/gpu:0\'):\n    a = tf.constant(A)\n    b = tf.constant(B)\n    #compute A^n and B^n and store results in c1\n    c1.append(matpow(a, n))\n    c1.append(matpow(b, n))\n\nwith tf.device(\'/cpu:0\'):\n  sum = tf.add_n(c1) #Addition of all elements in c1, i.e. A^n + B^n\n\nt1_1 = datetime.datetime.now()\nwith tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n    # Runs the op.\n    sess.run(sum)\nt2_1 = datetime.datetime.now()\n\n\n\'\'\'\nMulti GPU computing\n\'\'\'\n#GPU:0 computes A^n\nwith tf.device(\'/gpu:0\'):\n    #compute A^n and store result in c2\n    a = tf.constant(A)\n    c2.append(matpow(a, n))\n\n#GPU:1 computes B^n\nwith tf.device(\'/gpu:1\'):\n    #compute B^n and store result in c2\n    b = tf.constant(B)\n    c2.append(matpow(b, n))\n\nwith tf.device(\'/cpu:0\'):\n  sum = tf.add_n(c2) #Addition of all elements in c2, i.e. A^n + B^n\n\nt1_2 = datetime.datetime.now()\nwith tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n    # Runs the op.\n    sess.run(sum)\nt2_2 = datetime.datetime.now()\n\n\nprint ""Single GPU computation time: "" + str(t2_1-t1_1)\nprint ""Multi GPU computation time: "" + str(t2_2-t1_2)'"
matplotlib/tests/__init__.py,0,b''
numpy/tests/__init__.py,0,b''
pandas/tests/__init__.py,0,b''
python-data/tests/__init__.py,0,b''
python-data/tests/test_transform_util.py,0,"b""from nose.tools import assert_equal\nfrom ..transform_util import TransformUtil\n\n\nclass TestTransformUtil():\n\n    states = [' Alabama ', 'Georgia!', 'Georgia', 'georgia', \\\n          'FlOrIda', 'south carolina##', 'West virginia?']\n    \n    expected_output = ['Alabama',\n                       'Georgia',\n                       'Georgia',\n                       'Georgia',\n                       'Florida',\n                       'South Carolina',\n                       'West Virginia']\n    \n    def test_remove_punctuation(self):\n        assert_equal(TransformUtil.remove_punctuation('!#?'), '')\n        \n    def test_map_remove_punctuation(self):\n        # Map applies a function to a collection\n        output = map(TransformUtil.remove_punctuation, self.states)\n        assert_equal('!#?' not in output, True)\n\n    def test_clean_strings(self):\n        clean_ops = [str.strip, TransformUtil.remove_punctuation, str.title] \n        output = TransformUtil.clean_strings(self.states, clean_ops)\n        assert_equal(output, self.expected_output)"""
python-data/tests/test_type_util.py,0,"b""from nose.tools import assert_equal\nfrom ..type_util import TypeUtil\n\n\nclass TestUtil():\n\n    def test_is_iterable(self):\n        assert_equal(TypeUtil.is_iterable('foo'), True)\n        assert_equal(TypeUtil.is_iterable(7), False)\n\n    def test_convert_to_list(self):\n        assert_equal(isinstance(TypeUtil.convert_to_list('foo'), list), True)\n        assert_equal(isinstance(TypeUtil.convert_to_list(7), list), False)"""
scikit-learn/fig_code/ML_flow_chart.py,0,"b'""""""\nTutorial Diagrams\n-----------------\n\nThis script plots the flow-charts used in the scikit-learn tutorials.\n""""""\n\nimport numpy as np\nimport pylab as pl\nfrom matplotlib.patches import Circle, Rectangle, Polygon, Arrow, FancyArrow\n\ndef create_base(box_bg = \'#CCCCCC\',\n                arrow1 = \'#88CCFF\',\n                arrow2 = \'#88FF88\',\n                supervised=True):\n    fig = pl.figure(figsize=(9, 6), facecolor=\'w\')\n    ax = pl.axes((0, 0, 1, 1),\n                 xticks=[], yticks=[], frameon=False)\n    ax.set_xlim(0, 9)\n    ax.set_ylim(0, 6)\n\n    patches = [Rectangle((0.3, 3.6), 1.5, 1.8, zorder=1, fc=box_bg),\n               Rectangle((0.5, 3.8), 1.5, 1.8, zorder=2, fc=box_bg),\n               Rectangle((0.7, 4.0), 1.5, 1.8, zorder=3, fc=box_bg),\n               \n               Rectangle((2.9, 3.6), 0.2, 1.8, fc=box_bg),\n               Rectangle((3.1, 3.8), 0.2, 1.8, fc=box_bg),\n               Rectangle((3.3, 4.0), 0.2, 1.8, fc=box_bg),\n               \n               Rectangle((0.3, 0.2), 1.5, 1.8, fc=box_bg),\n               \n               Rectangle((2.9, 0.2), 0.2, 1.8, fc=box_bg),\n               \n               Circle((5.5, 3.5), 1.0, fc=box_bg),\n               \n               Polygon([[5.5, 1.7],\n                        [6.1, 1.1],\n                        [5.5, 0.5],\n                        [4.9, 1.1]], fc=box_bg),\n               \n               FancyArrow(2.3, 4.6, 0.35, 0, fc=arrow1,\n                          width=0.25, head_width=0.5, head_length=0.2),\n               \n               FancyArrow(3.75, 4.2, 0.5, -0.2, fc=arrow1,\n                          width=0.25, head_width=0.5, head_length=0.2),\n               \n               FancyArrow(5.5, 2.4, 0, -0.4, fc=arrow1,\n                          width=0.25, head_width=0.5, head_length=0.2),\n               \n               FancyArrow(2.0, 1.1, 0.5, 0, fc=arrow2,\n                          width=0.25, head_width=0.5, head_length=0.2),\n               \n               FancyArrow(3.3, 1.1, 1.3, 0, fc=arrow2,\n                          width=0.25, head_width=0.5, head_length=0.2),\n               \n               FancyArrow(6.2, 1.1, 0.8, 0, fc=arrow2,\n                          width=0.25, head_width=0.5, head_length=0.2)]\n\n    if supervised:\n        patches += [Rectangle((0.3, 2.4), 1.5, 0.5, zorder=1, fc=box_bg),\n                    Rectangle((0.5, 2.6), 1.5, 0.5, zorder=2, fc=box_bg),\n                    Rectangle((0.7, 2.8), 1.5, 0.5, zorder=3, fc=box_bg),\n                    FancyArrow(2.3, 2.9, 2.0, 0, fc=arrow1,\n                               width=0.25, head_width=0.5, head_length=0.2),\n                    Rectangle((7.3, 0.85), 1.5, 0.5, fc=box_bg)]\n    else:\n        patches += [Rectangle((7.3, 0.2), 1.5, 1.8, fc=box_bg)]\n    \n    for p in patches:\n        ax.add_patch(p)\n        \n    pl.text(1.45, 4.9, ""Training\\nText,\\nDocuments,\\nImages,\\netc."",\n            ha=\'center\', va=\'center\', fontsize=14)\n    \n    pl.text(3.6, 4.9, ""Feature\\nVectors"", \n            ha=\'left\', va=\'center\', fontsize=14)\n    \n    pl.text(5.5, 3.5, ""Machine\\nLearning\\nAlgorithm"",\n            ha=\'center\', va=\'center\', fontsize=14)\n    \n    pl.text(1.05, 1.1, ""New Text,\\nDocument,\\nImage,\\netc."",\n            ha=\'center\', va=\'center\', fontsize=14)\n    \n    pl.text(3.3, 1.7, ""Feature\\nVector"", \n            ha=\'left\', va=\'center\', fontsize=14)\n    \n    pl.text(5.5, 1.1, ""Predictive\\nModel"", \n            ha=\'center\', va=\'center\', fontsize=12)\n\n    if supervised:\n        pl.text(1.45, 3.05, ""Labels"",\n                ha=\'center\', va=\'center\', fontsize=14)\n    \n        pl.text(8.05, 1.1, ""Expected\\nLabel"",\n                ha=\'center\', va=\'center\', fontsize=14)\n        pl.text(8.8, 5.8, ""Supervised Learning Model"",\n                ha=\'right\', va=\'top\', fontsize=18)\n\n    else:\n        pl.text(8.05, 1.1,\n                ""Likelihood\\nor Cluster ID\\nor Better\\nRepresentation"",\n                ha=\'center\', va=\'center\', fontsize=12)\n        pl.text(8.8, 5.8, ""Unsupervised Learning Model"",\n                ha=\'right\', va=\'top\', fontsize=18)\n        \n        \n\ndef plot_supervised_chart(annotate=False):\n    create_base(supervised=True)\n    if annotate:\n        fontdict = dict(color=\'r\', weight=\'bold\', size=14)\n        pl.text(1.9, 4.55, \'X = vec.fit_transform(input)\',\n                fontdict=fontdict,\n                rotation=20, ha=\'left\', va=\'bottom\')\n        pl.text(3.7, 3.2, \'clf.fit(X, y)\',\n                fontdict=fontdict,\n                rotation=20, ha=\'left\', va=\'bottom\')\n        pl.text(1.7, 1.5, \'X_new = vec.transform(input)\',\n                fontdict=fontdict,\n                rotation=20, ha=\'left\', va=\'bottom\')\n        pl.text(6.1, 1.5, \'y_new = clf.predict(X_new)\',\n                fontdict=fontdict,\n                rotation=20, ha=\'left\', va=\'bottom\')\n\ndef plot_unsupervised_chart():\n    create_base(supervised=False)\n\n\nif __name__ == \'__main__\':\n    plot_supervised_chart(False)\n    plot_supervised_chart(True)\n    plot_unsupervised_chart()\n    pl.show()\n\n\n'"
scikit-learn/fig_code/__init__.py,0,b'from .data import *\nfrom .figures import *\n\nfrom .sgd_separator import plot_sgd_separator\nfrom .linear_regression import plot_linear_regression\nfrom .helpers import plot_iris_knn\n'
scikit-learn/fig_code/data.py,2,"b'import numpy as np\n\n\ndef linear_data_sample(N=40, rseed=0, m=3, b=-2):\n    rng = np.random.RandomState(rseed)\n\n    x = 10 * rng.rand(N)\n    dy = m / 2 * (1 + rng.rand(N))\n    y = m * x + b + dy * rng.randn(N)\n\n    return (x, y, dy)\n\n\ndef linear_data_sample_big_errs(N=40, rseed=0, m=3, b=-2):\n    rng = np.random.RandomState(rseed)\n\n    x = 10 * rng.rand(N)\n    dy = m / 2 * (1 + rng.rand(N))\n    dy[20:25] *= 10\n    y = m * x + b + dy * rng.randn(N)\n\n    return (x, y, dy)\n\n\ndef sample_light_curve(phased=True):\n    from astroML.datasets import fetch_LINEAR_sample\n    data = fetch_LINEAR_sample()\n    t, y, dy = data[18525697].T\n\n    if phased:\n        P_best = 0.580313015651\n        t /= P_best\n\n    return (t, y, dy)\n    \n\ndef sample_light_curve_2(phased=True):\n    from astroML.datasets import fetch_LINEAR_sample\n    data = fetch_LINEAR_sample()\n    t, y, dy = data[10022663].T\n\n    if phased:\n        P_best = 0.61596079804\n        t /= P_best\n\n    return (t, y, dy)\n    \n'"
scikit-learn/fig_code/figures.py,12,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\n\ndef plot_venn_diagram():\n    fig, ax = plt.subplots(subplot_kw=dict(frameon=False, xticks=[], yticks=[]))\n    ax.add_patch(plt.Circle((0.3, 0.3), 0.3, fc=\'red\', alpha=0.5))\n    ax.add_patch(plt.Circle((0.6, 0.3), 0.3, fc=\'blue\', alpha=0.5))\n    ax.add_patch(plt.Rectangle((-0.1, -0.1), 1.1, 0.8, fc=\'none\', ec=\'black\'))\n    ax.text(0.2, 0.3, \'$x$\', size=30, ha=\'center\', va=\'center\')\n    ax.text(0.7, 0.3, \'$y$\', size=30, ha=\'center\', va=\'center\')\n    ax.text(0.0, 0.6, \'$I$\', size=30)\n    ax.axis(\'equal\')\n\n\ndef plot_example_decision_tree():\n    fig = plt.figure(figsize=(10, 4))\n    ax = fig.add_axes([0, 0, 0.8, 1], frameon=False, xticks=[], yticks=[])\n    ax.set_title(\'Example Decision Tree: Animal Classification\', size=24)\n\n    def text(ax, x, y, t, size=20, **kwargs):\n        ax.text(x, y, t,\n                ha=\'center\', va=\'center\', size=size,\n                bbox=dict(boxstyle=\'round\', ec=\'k\', fc=\'w\'), **kwargs)\n\n    text(ax, 0.5, 0.9, ""How big is\\nthe animal?"", 20)\n    text(ax, 0.3, 0.6, ""Does the animal\\nhave horns?"", 18)\n    text(ax, 0.7, 0.6, ""Does the animal\\nhave two legs?"", 18)\n    text(ax, 0.12, 0.3, ""Are the horns\\nlonger than 10cm?"", 14)\n    text(ax, 0.38, 0.3, ""Is the animal\\nwearing a collar?"", 14)\n    text(ax, 0.62, 0.3, ""Does the animal\\nhave wings?"", 14)\n    text(ax, 0.88, 0.3, ""Does the animal\\nhave a tail?"", 14)\n\n    text(ax, 0.4, 0.75, ""> 1m"", 12, alpha=0.4)\n    text(ax, 0.6, 0.75, ""< 1m"", 12, alpha=0.4)\n\n    text(ax, 0.21, 0.45, ""yes"", 12, alpha=0.4)\n    text(ax, 0.34, 0.45, ""no"", 12, alpha=0.4)\n\n    text(ax, 0.66, 0.45, ""yes"", 12, alpha=0.4)\n    text(ax, 0.79, 0.45, ""no"", 12, alpha=0.4)\n\n    ax.plot([0.3, 0.5, 0.7], [0.6, 0.9, 0.6], \'-k\')\n    ax.plot([0.12, 0.3, 0.38], [0.3, 0.6, 0.3], \'-k\')\n    ax.plot([0.62, 0.7, 0.88], [0.3, 0.6, 0.3], \'-k\')\n    ax.plot([0.0, 0.12, 0.20], [0.0, 0.3, 0.0], \'--k\')\n    ax.plot([0.28, 0.38, 0.48], [0.0, 0.3, 0.0], \'--k\')\n    ax.plot([0.52, 0.62, 0.72], [0.0, 0.3, 0.0], \'--k\')\n    ax.plot([0.8, 0.88, 1.0], [0.0, 0.3, 0.0], \'--k\')\n    ax.axis([0, 1, 0, 1])\n\n\ndef visualize_tree(estimator, X, y, boundaries=True,\n                   xlim=None, ylim=None):\n    estimator.fit(X, y)\n\n    if xlim is None:\n        xlim = (X[:, 0].min() - 0.1, X[:, 0].max() + 0.1)\n    if ylim is None:\n        ylim = (X[:, 1].min() - 0.1, X[:, 1].max() + 0.1)\n\n    x_min, x_max = xlim\n    y_min, y_max = ylim\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                         np.linspace(y_min, y_max, 100))\n    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, alpha=0.2, cmap=\'rainbow\')\n    plt.clim(y.min(), y.max())\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\'rainbow\')\n    plt.axis(\'off\')\n\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)        \n    plt.clim(y.min(), y.max())\n    \n    # Plot the decision boundaries\n    def plot_boundaries(i, xlim, ylim):\n        if i < 0:\n            return\n\n        tree = estimator.tree_\n        \n        if tree.feature[i] == 0:\n            plt.plot([tree.threshold[i], tree.threshold[i]], ylim, \'-k\')\n            plot_boundaries(tree.children_left[i],\n                            [xlim[0], tree.threshold[i]], ylim)\n            plot_boundaries(tree.children_right[i],\n                            [tree.threshold[i], xlim[1]], ylim)\n        \n        elif tree.feature[i] == 1:\n            plt.plot(xlim, [tree.threshold[i], tree.threshold[i]], \'-k\')\n            plot_boundaries(tree.children_left[i], xlim,\n                            [ylim[0], tree.threshold[i]])\n            plot_boundaries(tree.children_right[i], xlim,\n                            [tree.threshold[i], ylim[1]])\n            \n    if boundaries:\n        plot_boundaries(0, plt.xlim(), plt.ylim())\n\n\ndef plot_tree_interactive(X, y):\n    from sklearn.tree import DecisionTreeClassifier\n\n    def interactive_tree(depth=1):\n        clf = DecisionTreeClassifier(max_depth=depth, random_state=0)\n        visualize_tree(clf, X, y)\n\n    from IPython.html.widgets import interact\n    return interact(interactive_tree, depth=[1, 5])\n\n\ndef plot_kmeans_interactive(min_clusters=1, max_clusters=6):\n    from IPython.html.widgets import interact\n    from sklearn.metrics.pairwise import euclidean_distances\n    from sklearn.datasets.samples_generator import make_blobs\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\'ignore\')\n\n        X, y = make_blobs(n_samples=300, centers=4,\n                          random_state=0, cluster_std=0.60)\n\n        def _kmeans_step(frame=0, n_clusters=4):\n            rng = np.random.RandomState(2)\n            labels = np.zeros(X.shape[0])\n            centers = rng.randn(n_clusters, 2)\n\n            nsteps = frame // 3\n\n            for i in range(nsteps + 1):\n                old_centers = centers\n                if i < nsteps or frame % 3 > 0:\n                    dist = euclidean_distances(X, centers)\n                    labels = dist.argmin(1)\n\n                if i < nsteps or frame % 3 > 1:\n                    centers = np.array([X[labels == j].mean(0)\n                                        for j in range(n_clusters)])\n                    nans = np.isnan(centers)\n                    centers[nans] = old_centers[nans]\n\n\n            # plot the data and cluster centers\n            plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap=\'rainbow\',\n                        vmin=0, vmax=n_clusters - 1);\n            plt.scatter(old_centers[:, 0], old_centers[:, 1], marker=\'o\',\n                        c=np.arange(n_clusters),\n                        s=200, cmap=\'rainbow\')\n            plt.scatter(old_centers[:, 0], old_centers[:, 1], marker=\'o\',\n                        c=\'black\', s=50)\n\n            # plot new centers if third frame\n            if frame % 3 == 2:\n                for i in range(n_clusters):\n                    plt.annotate(\'\', centers[i], old_centers[i], \n                                 arrowprops=dict(arrowstyle=\'->\', linewidth=1))\n                plt.scatter(centers[:, 0], centers[:, 1], marker=\'o\',\n                            c=np.arange(n_clusters),\n                            s=200, cmap=\'rainbow\')\n                plt.scatter(centers[:, 0], centers[:, 1], marker=\'o\',\n                            c=\'black\', s=50)\n\n            plt.xlim(-4, 4)\n            plt.ylim(-2, 10)\n\n            if frame % 3 == 1:\n                plt.text(3.8, 9.5, ""1. Reassign points to nearest centroid"",\n                         ha=\'right\', va=\'top\', size=14)\n            elif frame % 3 == 2:\n                plt.text(3.8, 9.5, ""2. Update centroids to cluster means"",\n                         ha=\'right\', va=\'top\', size=14)\n\n    \n    return interact(_kmeans_step, frame=[0, 50],\n                    n_clusters=[min_clusters, max_clusters])\n\n\ndef plot_image_components(x, coefficients=None, mean=0, components=None,\n                          imshape=(8, 8), n_components=6, fontsize=12):\n    if coefficients is None:\n        coefficients = x\n        \n    if components is None:\n        components = np.eye(len(coefficients), len(x))\n        \n    mean = np.zeros_like(x) + mean\n        \n\n    fig = plt.figure(figsize=(1.2 * (5 + n_components), 1.2 * 2))\n    g = plt.GridSpec(2, 5 + n_components, hspace=0.3)\n\n    def show(i, j, x, title=None):\n        ax = fig.add_subplot(g[i, j], xticks=[], yticks=[])\n        ax.imshow(x.reshape(imshape), interpolation=\'nearest\')\n        if title:\n            ax.set_title(title, fontsize=fontsize)\n\n    show(slice(2), slice(2), x, ""True"")\n\n    approx = mean.copy()\n    show(0, 2, np.zeros_like(x) + mean, r\'$\\mu$\')\n    show(1, 2, approx, r\'$1 \\cdot \\mu$\')\n\n    for i in range(0, n_components):\n        approx = approx + coefficients[i] * components[i]\n        show(0, i + 3, components[i], r\'$c_{0}$\'.format(i + 1))\n        show(1, i + 3, approx,\n             r""${0:.2f} \\cdot c_{1}$"".format(coefficients[i], i + 1))\n        plt.gca().text(0, 1.05, \'$+$\', ha=\'right\', va=\'bottom\',\n                       transform=plt.gca().transAxes, fontsize=fontsize)\n\n    show(slice(2), slice(-2, None), approx, ""Approx"")\n\n\ndef plot_pca_interactive(data, n_components=6):\n    from sklearn.decomposition import PCA\n    from IPython.html.widgets import interact\n\n    pca = PCA(n_components=n_components)\n    Xproj = pca.fit_transform(data)\n\n    def show_decomp(i=0):\n        plot_image_components(data[i], Xproj[i],\n                              pca.mean_, pca.components_)\n    \n    interact(show_decomp, i=(0, data.shape[0] - 1));\n'"
scikit-learn/fig_code/helpers.py,9,"b'""""""\nSmall helpers for code that is not shown in the notebooks\n""""""\n\nfrom sklearn import neighbors, datasets, linear_model\nimport pylab as pl\nimport numpy as np\nfrom matplotlib.colors import ListedColormap\n\n# Create color maps for 3-class classification problem, as with iris\ncmap_light = ListedColormap([\'#FFAAAA\', \'#AAFFAA\', \'#AAAAFF\'])\ncmap_bold = ListedColormap([\'#FF0000\', \'#00FF00\', \'#0000FF\'])\n\ndef plot_iris_knn():\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]  # we only take the first two features. We could\n                        # avoid this ugly slicing by using a two-dim dataset\n    y = iris.target\n\n    knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X, y)\n\n    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                         np.linspace(y_min, y_max, 100))\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    pl.figure()\n    pl.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    pl.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n    pl.xlabel(\'sepal length (cm)\')\n    pl.ylabel(\'sepal width (cm)\')\n    pl.axis(\'tight\')\n\n\ndef plot_polynomial_regression():\n    rng = np.random.RandomState(0)\n    x = 2*rng.rand(100) - 1\n\n    f = lambda t: 1.2 * t**2 + .1 * t**3 - .4 * t **5 - .5 * t ** 9\n    y = f(x) + .4 * rng.normal(size=100)\n\n    x_test = np.linspace(-1, 1, 100)\n\n    pl.figure()\n    pl.scatter(x, y, s=4)\n\n    X = np.array([x**i for i in range(5)]).T\n    X_test = np.array([x_test**i for i in range(5)]).T\n    regr = linear_model.LinearRegression()\n    regr.fit(X, y)\n    pl.plot(x_test, regr.predict(X_test), label=\'4th order\')\n\n    X = np.array([x**i for i in range(10)]).T\n    X_test = np.array([x_test**i for i in range(10)]).T\n    regr = linear_model.LinearRegression()\n    regr.fit(X, y)\n    pl.plot(x_test, regr.predict(X_test), label=\'9th order\')\n\n    pl.legend(loc=\'best\')\n    pl.axis(\'tight\')\n    pl.title(\'Fitting a 4th and a 9th order polynomial\')\n\n    pl.figure()\n    pl.scatter(x, y, s=4)\n    pl.plot(x_test, f(x_test), label=""truth"")\n    pl.axis(\'tight\')\n    pl.title(\'Ground truth (9th order polynomial)\')\n\n\n'"
scikit-learn/fig_code/linear_regression.py,3,"b""import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n\ndef plot_linear_regression():\n    a = 0.5\n    b = 1.0\n\n    # x from 0 to 10\n    x = 30 * np.random.random(20)\n\n    # y = a*x + b with noise\n    y = a * x + b + np.random.normal(size=x.shape)\n\n    # create a linear regression classifier\n    clf = LinearRegression()\n    clf.fit(x[:, None], y)\n\n    # predict y from the data\n    x_new = np.linspace(0, 30, 100)\n    y_new = clf.predict(x_new[:, None])\n\n    # plot the results\n    ax = plt.axes()\n    ax.scatter(x, y)\n    ax.plot(x_new, y_new)\n\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n\n    ax.axis('tight')\n\n\nif __name__ == '__main__':\n    plot_linear_regression()\n    plt.show()\n"""
scikit-learn/fig_code/sgd_separator.py,5,"b'import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.datasets.samples_generator import make_blobs\n\ndef plot_sgd_separator():\n    # we create 50 separable points\n    X, Y = make_blobs(n_samples=50, centers=2,\n                      random_state=0, cluster_std=0.60)\n\n    # fit the model\n    clf = SGDClassifier(loss=""hinge"", alpha=0.01,\n                        n_iter=200, fit_intercept=True)\n    clf.fit(X, Y)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    xx = np.linspace(-1, 5, 10)\n    yy = np.linspace(-1, 5, 10)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    Z = np.empty(X1.shape)\n    for (i, j), val in np.ndenumerate(X1):\n        x1 = val\n        x2 = X2[i, j]\n        p = clf.decision_function([x1, x2])\n        Z[i, j] = p[0]\n    levels = [-1.0, 0.0, 1.0]\n    linestyles = [\'dashed\', \'solid\', \'dashed\']\n    colors = \'k\'\n\n    ax = plt.axes()\n    ax.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\n    ax.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n\n    ax.axis(\'tight\')\n\n\nif __name__ == \'__main__\':\n    plot_sgd_separator()\n    plt.show()\n'"
scikit-learn/fig_code/svm_gui.py,7,"b'""""""\n==========\nLibsvm GUI\n==========\n\nA simple graphical frontend for Libsvm mainly intended for didactic\npurposes. You can create data points by point and click and visualize\nthe decision region induced by different kernels and parameter settings.\n\nTo create positive examples click the left mouse button; to create\nnegative examples click the right button.\n\nIf all examples are from the same class, it uses a one-class SVM.\n\n""""""\nfrom __future__ import division, print_function\n\nprint(__doc__)\n\n# Author: Peter Prettenhoer <peter.prettenhofer@gmail.com>\n#\n# License: BSD 3 clause\n\nimport matplotlib\nmatplotlib.use(\'TkAgg\')\n\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\nfrom matplotlib.backends.backend_tkagg import NavigationToolbar2TkAgg\nfrom matplotlib.figure import Figure\nfrom matplotlib.contour import ContourSet\n\nimport Tkinter as Tk\nimport sys\nimport numpy as np\n\nfrom sklearn import svm\nfrom sklearn.datasets import dump_svmlight_file\nfrom sklearn.externals.six.moves import xrange\n\ny_min, y_max = -50, 50\nx_min, x_max = -50, 50\n\n\nclass Model(object):\n    """"""The Model which hold the data. It implements the\n    observable in the observer pattern and notifies the\n    registered observers on change event.\n    """"""\n\n    def __init__(self):\n        self.observers = []\n        self.surface = None\n        self.data = []\n        self.cls = None\n        self.surface_type = 0\n\n    def changed(self, event):\n        """"""Notify the observers. """"""\n        for observer in self.observers:\n            observer.update(event, self)\n\n    def add_observer(self, observer):\n        """"""Register an observer. """"""\n        self.observers.append(observer)\n\n    def set_surface(self, surface):\n        self.surface = surface\n\n    def dump_svmlight_file(self, file):\n        data = np.array(self.data)\n        X = data[:, 0:2]\n        y = data[:, 2]\n        dump_svmlight_file(X, y, file)\n\n\nclass Controller(object):\n    def __init__(self, model):\n        self.model = model\n        self.kernel = Tk.IntVar()\n        self.surface_type = Tk.IntVar()\n        # Whether or not a model has been fitted\n        self.fitted = False\n\n    def fit(self):\n        print(""fit the model"")\n        train = np.array(self.model.data)\n        X = train[:, 0:2]\n        y = train[:, 2]\n\n        C = float(self.complexity.get())\n        gamma = float(self.gamma.get())\n        coef0 = float(self.coef0.get())\n        degree = int(self.degree.get())\n        kernel_map = {0: ""linear"", 1: ""rbf"", 2: ""poly""}\n        if len(np.unique(y)) == 1:\n            clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],\n                                  gamma=gamma, coef0=coef0, degree=degree)\n            clf.fit(X)\n        else:\n            clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,\n                          gamma=gamma, coef0=coef0, degree=degree)\n            clf.fit(X, y)\n        if hasattr(clf, \'score\'):\n            print(""Accuracy:"", clf.score(X, y) * 100)\n        X1, X2, Z = self.decision_surface(clf)\n        self.model.clf = clf\n        self.model.set_surface((X1, X2, Z))\n        self.model.surface_type = self.surface_type.get()\n        self.fitted = True\n        self.model.changed(""surface"")\n\n    def decision_surface(self, cls):\n        delta = 1\n        x = np.arange(x_min, x_max + delta, delta)\n        y = np.arange(y_min, y_max + delta, delta)\n        X1, X2 = np.meshgrid(x, y)\n        Z = cls.decision_function(np.c_[X1.ravel(), X2.ravel()])\n        Z = Z.reshape(X1.shape)\n        return X1, X2, Z\n\n    def clear_data(self):\n        self.model.data = []\n        self.fitted = False\n        self.model.changed(""clear"")\n\n    def add_example(self, x, y, label):\n        self.model.data.append((x, y, label))\n        self.model.changed(""example_added"")\n\n        # update decision surface if already fitted.\n        self.refit()\n\n    def refit(self):\n        """"""Refit the model if already fitted. """"""\n        if self.fitted:\n            self.fit()\n\n\nclass View(object):\n    """"""Test docstring. """"""\n    def __init__(self, root, controller):\n        f = Figure()\n        ax = f.add_subplot(111)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_xlim((x_min, x_max))\n        ax.set_ylim((y_min, y_max))\n        canvas = FigureCanvasTkAgg(f, master=root)\n        canvas.show()\n        canvas.get_tk_widget().pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n        canvas._tkcanvas.pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n        canvas.mpl_connect(\'key_press_event\', self.onkeypress)\n        canvas.mpl_connect(\'key_release_event\', self.onkeyrelease)\n        canvas.mpl_connect(\'button_press_event\', self.onclick)\n        toolbar = NavigationToolbar2TkAgg(canvas, root)\n        toolbar.update()\n        self.shift_down = False\n        self.controllbar = ControllBar(root, controller)\n        self.f = f\n        self.ax = ax\n        self.canvas = canvas\n        self.controller = controller\n        self.contours = []\n        self.c_labels = None\n        self.plot_kernels()\n\n    def plot_kernels(self):\n        self.ax.text(-50, -60, ""Linear: $u^T v$"")\n        self.ax.text(-20, -60, ""RBF: $\\exp (-\\gamma \\| u-v \\|^2)$"")\n        self.ax.text(10, -60, ""Poly: $(\\gamma \\, u^T v + r)^d$"")\n\n    def onkeypress(self, event):\n        if event.key == ""shift"":\n            self.shift_down = True\n\n    def onkeyrelease(self, event):\n        if event.key == ""shift"":\n            self.shift_down = False\n\n    def onclick(self, event):\n        if event.xdata and event.ydata:\n            if self.shift_down or event.button == 3:\n                self.controller.add_example(event.xdata, event.ydata, -1)\n            elif event.button == 1:\n                self.controller.add_example(event.xdata, event.ydata, 1)\n\n    def update_example(self, model, idx):\n        x, y, l = model.data[idx]\n        if l == 1:\n            color = \'w\'\n        elif l == -1:\n            color = \'k\'\n        self.ax.plot([x], [y], ""%so"" % color, scalex=0.0, scaley=0.0)\n\n    def update(self, event, model):\n        if event == ""examples_loaded"":\n            for i in xrange(len(model.data)):\n                self.update_example(model, i)\n\n        if event == ""example_added"":\n            self.update_example(model, -1)\n\n        if event == ""clear"":\n            self.ax.clear()\n            self.ax.set_xticks([])\n            self.ax.set_yticks([])\n            self.contours = []\n            self.c_labels = None\n            self.plot_kernels()\n\n        if event == ""surface"":\n            self.remove_surface()\n            self.plot_support_vectors(model.clf.support_vectors_)\n            self.plot_decision_surface(model.surface, model.surface_type)\n\n        self.canvas.draw()\n\n    def remove_surface(self):\n        """"""Remove old decision surface.""""""\n        if len(self.contours) > 0:\n            for contour in self.contours:\n                if isinstance(contour, ContourSet):\n                    for lineset in contour.collections:\n                        lineset.remove()\n                else:\n                    contour.remove()\n            self.contours = []\n\n    def plot_support_vectors(self, support_vectors):\n        """"""Plot the support vectors by placing circles over the\n        corresponding data points and adds the circle collection\n        to the contours list.""""""\n        cs = self.ax.scatter(support_vectors[:, 0], support_vectors[:, 1],\n                             s=80, edgecolors=""k"", facecolors=""none"")\n        self.contours.append(cs)\n\n    def plot_decision_surface(self, surface, type):\n        X1, X2, Z = surface\n        if type == 0:\n            levels = [-1.0, 0.0, 1.0]\n            linestyles = [\'dashed\', \'solid\', \'dashed\']\n            colors = \'k\'\n            self.contours.append(self.ax.contour(X1, X2, Z, levels,\n                                                 colors=colors,\n                                                 linestyles=linestyles))\n        elif type == 1:\n            self.contours.append(self.ax.contourf(X1, X2, Z, 10,\n                                                  cmap=matplotlib.cm.bone,\n                                                  origin=\'lower\', alpha=0.85))\n            self.contours.append(self.ax.contour(X1, X2, Z, [0.0], colors=\'k\',\n                                                 linestyles=[\'solid\']))\n        else:\n            raise ValueError(""surface type unknown"")\n\n\nclass ControllBar(object):\n    def __init__(self, root, controller):\n        fm = Tk.Frame(root)\n        kernel_group = Tk.Frame(fm)\n        Tk.Radiobutton(kernel_group, text=""Linear"", variable=controller.kernel,\n                       value=0, command=controller.refit).pack(anchor=Tk.W)\n        Tk.Radiobutton(kernel_group, text=""RBF"", variable=controller.kernel,\n                       value=1, command=controller.refit).pack(anchor=Tk.W)\n        Tk.Radiobutton(kernel_group, text=""Poly"", variable=controller.kernel,\n                       value=2, command=controller.refit).pack(anchor=Tk.W)\n        kernel_group.pack(side=Tk.LEFT)\n\n        valbox = Tk.Frame(fm)\n        controller.complexity = Tk.StringVar()\n        controller.complexity.set(""1.0"")\n        c = Tk.Frame(valbox)\n        Tk.Label(c, text=""C:"", anchor=""e"", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(c, width=6, textvariable=controller.complexity).pack(\n            side=Tk.LEFT)\n        c.pack()\n\n        controller.gamma = Tk.StringVar()\n        controller.gamma.set(""0.01"")\n        g = Tk.Frame(valbox)\n        Tk.Label(g, text=""gamma:"", anchor=""e"", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(g, width=6, textvariable=controller.gamma).pack(side=Tk.LEFT)\n        g.pack()\n\n        controller.degree = Tk.StringVar()\n        controller.degree.set(""3"")\n        d = Tk.Frame(valbox)\n        Tk.Label(d, text=""degree:"", anchor=""e"", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(d, width=6, textvariable=controller.degree).pack(side=Tk.LEFT)\n        d.pack()\n\n        controller.coef0 = Tk.StringVar()\n        controller.coef0.set(""0"")\n        r = Tk.Frame(valbox)\n        Tk.Label(r, text=""coef0:"", anchor=""e"", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(r, width=6, textvariable=controller.coef0).pack(side=Tk.LEFT)\n        r.pack()\n        valbox.pack(side=Tk.LEFT)\n\n        cmap_group = Tk.Frame(fm)\n        Tk.Radiobutton(cmap_group, text=""Hyperplanes"",\n                       variable=controller.surface_type, value=0,\n                       command=controller.refit).pack(anchor=Tk.W)\n        Tk.Radiobutton(cmap_group, text=""Surface"",\n                       variable=controller.surface_type, value=1,\n                       command=controller.refit).pack(anchor=Tk.W)\n\n        cmap_group.pack(side=Tk.LEFT)\n\n        train_button = Tk.Button(fm, text=\'Fit\', width=5,\n                                 command=controller.fit)\n        train_button.pack()\n        fm.pack(side=Tk.LEFT)\n        Tk.Button(fm, text=\'Clear\', width=5,\n                  command=controller.clear_data).pack(side=Tk.LEFT)\n\n\ndef get_parser():\n    from optparse import OptionParser\n    op = OptionParser()\n    op.add_option(""--output"",\n                  action=""store"", type=""str"", dest=""output"",\n                  help=""Path where to dump data."")\n    return op\n\n\ndef main(argv):\n    op = get_parser()\n    opts, args = op.parse_args(argv[1:])\n    root = Tk.Tk()\n    model = Model()\n    controller = Controller(model)\n    root.wm_title(""Scikit-learn Libsvm GUI"")\n    view = View(root, controller)\n    model.add_observer(view)\n    Tk.mainloop()\n\n    if opts.output:\n        model.dump_svmlight_file(opts.output)\n\nif __name__ == ""__main__"":\n    main(sys.argv)\n'"
scikit-learn/tests/__init__.py,0,b''
scipy/tests/__init__.py,0,b''
deep-learning/keras-tutorial/deep_learning_models/imagenet_utils.py,1,"b""import numpy as np\nimport json\n\nfrom keras.utils.data_utils import get_file\nfrom keras import backend as K\n\nCLASS_INDEX = None\nCLASS_INDEX_PATH = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n\n\ndef preprocess_input(x, dim_ordering='default'):\n    if dim_ordering == 'default':\n        dim_ordering = K.image_dim_ordering()\n    assert dim_ordering in {'tf', 'th'}\n\n    if dim_ordering == 'th':\n        x[:, 0, :, :] -= 103.939\n        x[:, 1, :, :] -= 116.779\n        x[:, 2, :, :] -= 123.68\n        # 'RGB'->'BGR'\n        x = x[:, ::-1, :, :]\n    else:\n        x[:, :, :, 0] -= 103.939\n        x[:, :, :, 1] -= 116.779\n        x[:, :, :, 2] -= 123.68\n        # 'RGB'->'BGR'\n        x = x[:, :, :, ::-1]\n    return x\n\n\ndef decode_predictions(preds):\n    global CLASS_INDEX\n    assert len(preds.shape) == 2 and preds.shape[1] == 1000\n    if CLASS_INDEX is None:\n        fpath = get_file('imagenet_class_index.json',\n                         CLASS_INDEX_PATH,\n                         cache_subdir='models')\n        CLASS_INDEX = json.load(open(fpath))\n    indices = np.argmax(preds, axis=-1)\n    results = []\n    for i in indices:\n        results.append(CLASS_INDEX[str(i)])\n    return results\n"""
deep-learning/keras-tutorial/deep_learning_models/resnet50.py,1,"b'# -*- coding: utf-8 -*-\n\'\'\'ResNet50 model for Keras.\n\n# Reference:\n\n- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\nAdapted from code contributed by BigMoyan.\n\'\'\'\nfrom __future__ import print_function\n\nimport numpy as np\nimport warnings\n\nfrom keras.layers import merge, Input\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\nfrom keras.layers import BatchNormalization\nfrom keras.models import Model\nfrom keras.preprocessing import image\nimport keras.backend as K\nfrom keras.utils.layer_utils import convert_all_kernels_in_model\nfrom keras.utils.data_utils import get_file\n\n\nTH_WEIGHTS_PATH = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/resnet50_weights_th_dim_ordering_th_kernels.h5\'\nTF_WEIGHTS_PATH = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/resnet50_weights_tf_dim_ordering_tf_kernels.h5\'\nTH_WEIGHTS_PATH_NO_TOP = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/resnet50_weights_th_dim_ordering_th_kernels_notop.h5\'\nTF_WEIGHTS_PATH_NO_TOP = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\'\n\n\ndef identity_block(input_tensor, kernel_size, filters, stage, block):\n    \'\'\'The identity_block is the block that has no conv layer at shortcut\n\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the nb_filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: \'a\',\'b\'..., current block label, used for generating layer names\n    \'\'\'\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    if K.image_dim_ordering() == \'tf\':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n    conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n    bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n    x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + \'2a\')(input_tensor)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2a\')(x)\n    x = Activation(\'relu\')(x)\n\n    x = Convolution2D(nb_filter2, kernel_size, kernel_size,\n                      border_mode=\'same\', name=conv_name_base + \'2b\')(x)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2b\')(x)\n    x = Activation(\'relu\')(x)\n\n    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + \'2c\')(x)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2c\')(x)\n\n    x = merge([x, input_tensor], mode=\'sum\')\n    x = Activation(\'relu\')(x)\n    return x\n\n\ndef conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n    \'\'\'conv_block is the block that has a conv layer at shortcut\n\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the nb_filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: \'a\',\'b\'..., current block label, used for generating layer names\n\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    \'\'\'\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    if K.image_dim_ordering() == \'tf\':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n    conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n    bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n    x = Convolution2D(nb_filter1, 1, 1, subsample=strides,\n                      name=conv_name_base + \'2a\')(input_tensor)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2a\')(x)\n    x = Activation(\'relu\')(x)\n\n    x = Convolution2D(nb_filter2, kernel_size, kernel_size, border_mode=\'same\',\n                      name=conv_name_base + \'2b\')(x)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2b\')(x)\n    x = Activation(\'relu\')(x)\n\n    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + \'2c\')(x)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2c\')(x)\n\n    shortcut = Convolution2D(nb_filter3, 1, 1, subsample=strides,\n                             name=conv_name_base + \'1\')(input_tensor)\n    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + \'1\')(shortcut)\n\n    x = merge([x, shortcut], mode=\'sum\')\n    x = Activation(\'relu\')(x)\n    return x\n\n\ndef ResNet50(include_top=True, weights=\'imagenet\',\n             input_tensor=None):\n    \'\'\'Instantiate the ResNet50 architecture,\n    optionally loading weights pre-trained\n    on ImageNet. Note that when using TensorFlow,\n    for best performance you should set\n    `image_dim_ordering=""tf""` in your Keras config\n    at ~/.keras/keras.json.\n\n    The model and the weights are compatible with both\n    TensorFlow and Theano. The dimension ordering\n    convention used by the model is the one\n    specified in your Keras config file.\n\n    # Arguments\n        include_top: whether to include the 3 fully-connected\n            layers at the top of the network.\n        weights: one of `None` (random initialization)\n            or ""imagenet"" (pre-training on ImageNet).\n        input_tensor: optional Keras tensor (i.e. xput of `layers.Input()`)\n            to use as image input for the model.\n\n    # Returns\n        A Keras model instance.\n    \'\'\'\n    if weights not in {\'imagenet\', None}:\n        raise ValueError(\'The `weights` argument should be either \'\n                         \'`None` (random initialization) or `imagenet` \'\n                         \'(pre-training on ImageNet).\')\n    # Determine proper input shape\n    if K.image_dim_ordering() == \'th\':\n        if include_top:\n            input_shape = (3, 224, 224)\n        else:\n            input_shape = (3, None, None)\n    else:\n        if include_top:\n            input_shape = (224, 224, 3)\n        else:\n            input_shape = (None, None, 3)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor)\n        else:\n            img_input = input_tensor\n    if K.image_dim_ordering() == \'tf\':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n\n    x = ZeroPadding2D((3, 3))(img_input)\n    x = Convolution2D(64, 7, 7, subsample=(2, 2), name=\'conv1\')(x)\n    x = BatchNormalization(axis=bn_axis, name=\'bn_conv1\')(x)\n    x = Activation(\'relu\')(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n    x = conv_block(x, 3, [64, 64, 256], stage=2, block=\'a\', strides=(1, 1))\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block=\'b\')\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block=\'c\')\n\n    x = conv_block(x, 3, [128, 128, 512], stage=3, block=\'a\')\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block=\'b\')\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block=\'c\')\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block=\'d\')\n\n    x = conv_block(x, 3, [256, 256, 1024], stage=4, block=\'a\')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'b\')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'c\')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'d\')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'e\')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'f\')\n\n    x = conv_block(x, 3, [512, 512, 2048], stage=5, block=\'a\')\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block=\'b\')\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block=\'c\')\n\n    x = AveragePooling2D((7, 7), name=\'avg_pool\')(x)\n\n    if include_top:\n        x = Flatten()(x)\n        x = Dense(1000, activation=\'softmax\', name=\'fc1000\')(x)\n\n    model = Model(img_input, x)\n\n    # load weights\n    if weights == \'imagenet\':\n        print(\'K.image_dim_ordering:\', K.image_dim_ordering())\n        if K.image_dim_ordering() == \'th\':\n            if include_top:\n                weights_path = get_file(\'resnet50_weights_th_dim_ordering_th_kernels.h5\',\n                                        TH_WEIGHTS_PATH,\n                                        cache_subdir=\'models\')\n            else:\n                weights_path = get_file(\'resnet50_weights_th_dim_ordering_th_kernels_notop.h5\',\n                                        TH_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir=\'models\')\n            model.load_weights(weights_path)\n            if K.backend() == \'tensorflow\':\n                warnings.warn(\'You are using the TensorFlow backend, yet you \'\n                              \'are using the Theano \'\n                              \'image dimension ordering convention \'\n                              \'(`image_dim_ordering=""th""`). \'\n                              \'For best performance, set \'\n                              \'`image_dim_ordering=""tf""` in \'\n                              \'your Keras config \'\n                              \'at ~/.keras/keras.json.\')\n                convert_all_kernels_in_model(model)\n        else:\n            if include_top:\n                weights_path = get_file(\'resnet50_weights_tf_dim_ordering_tf_kernels.h5\',\n                                        TF_WEIGHTS_PATH,\n                                        cache_subdir=\'models\')\n            else:\n                weights_path = get_file(\'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\',\n                                        TF_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir=\'models\')\n            model.load_weights(weights_path)\n            if K.backend() == \'theano\':\n                convert_all_kernels_in_model(model)\n    return model\n\n\nif __name__ == \'__main__\':\n    model = ResNet50(include_top=True, weights=\'imagenet\')\n\n    img_path = \'elephant.jpg\'\n    img = image.load_img(img_path, target_size=(224, 224))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    print(\'Input image shape:\', x.shape)\n\n    preds = model.predict(x)\n    print(\'Predicted:\', decode_predictions(preds))\n'"
deep-learning/keras-tutorial/deep_learning_models/vgg16.py,1,"b'# -*- coding: utf-8 -*-\n\'\'\'VGG16 model for Keras.\n\n# Reference:\n\n- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n\n\'\'\'\nfrom __future__ import print_function\n\nimport numpy as np\nimport warnings\n\nfrom keras.models import Model\nfrom keras.layers import Flatten, Dense, Input\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.preprocessing import image\nfrom keras.utils.layer_utils import convert_all_kernels_in_model\nfrom keras.utils.data_utils import get_file\nfrom keras import backend as K\n# from imagenet_utils import decode_predictions, preprocess_input\n\n\nTH_WEIGHTS_PATH = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels.h5\'\nTF_WEIGHTS_PATH = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\'\nTH_WEIGHTS_PATH_NO_TOP = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels_notop.h5\'\nTF_WEIGHTS_PATH_NO_TOP = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\'\n\n\ndef VGG16(include_top=True, weights=\'imagenet\',\n          input_tensor=None):\n    \'\'\'Instantiate the VGG16 architecture,\n    optionally loading weights pre-trained\n    on ImageNet. Note that when using TensorFlow,\n    for best performance you should set\n    `image_dim_ordering=""tf""` in your Keras config\n    at ~/.keras/keras.json.\n\n    The model and the weights are compatible with both\n    TensorFlow and Theano. The dimension ordering\n    convention used by the model is the one\n    specified in your Keras config file.\n\n    # Arguments\n        include_top: whether to include the 3 fully-connected\n            layers at the top of the network.\n        weights: one of `None` (random initialization)\n            or ""imagenet"" (pre-training on ImageNet).\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n\n    # Returns\n        A Keras model instance.\n    \'\'\'\n    if weights not in {\'imagenet\', None}:\n        raise ValueError(\'The `weights` argument should be either \'\n                         \'`None` (random initialization) or `imagenet` \'\n                         \'(pre-training on ImageNet).\')\n    # Determine proper input shape\n    if K.image_dim_ordering() == \'th\':\n        if include_top:\n            input_shape = (3, 224, 224)\n        else:\n            input_shape = (3, None, None)\n    else:\n        if include_top:\n            input_shape = (224, 224, 3)\n        else:\n            input_shape = (None, None, 3)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor)\n        else:\n            img_input = input_tensor\n    # Block 1\n    x = Convolution2D(64, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block1_conv1\')(img_input)\n    x = Convolution2D(64, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block1_conv2\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block1_pool\')(x)\n\n    # Block 2\n    x = Convolution2D(128, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block2_conv1\')(x)\n    x = Convolution2D(128, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block2_conv2\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block2_pool\')(x)\n\n    # Block 3\n    x = Convolution2D(256, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block3_conv1\')(x)\n    x = Convolution2D(256, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block3_conv2\')(x)\n    x = Convolution2D(256, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block3_conv3\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block3_pool\')(x)\n\n    # Block 4\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block4_conv1\')(x)\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block4_conv2\')(x)\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block4_conv3\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block4_pool\')(x)\n\n    # Block 5\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block5_conv1\')(x)\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block5_conv2\')(x)\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block5_conv3\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block5_pool\')(x)\n\n    if include_top:\n        # Classification block\n        x = Flatten(name=\'flatten\')(x)\n        x = Dense(4096, activation=\'relu\', name=\'fc1\')(x)\n        x = Dense(4096, activation=\'relu\', name=\'fc2\')(x)\n        x = Dense(1000, activation=\'softmax\', name=\'predictions\')(x)\n\n    # Create model\n    model = Model(img_input, x)\n\n    # load weights\n    if weights == \'imagenet\':\n        print(\'K.image_dim_ordering:\', K.image_dim_ordering())\n        if K.image_dim_ordering() == \'th\':\n            if include_top:\n                weights_path = get_file(\'vgg16_weights_th_dim_ordering_th_kernels.h5\',\n                                        TH_WEIGHTS_PATH,\n                                        cache_subdir=\'models\')\n            else:\n                weights_path = get_file(\'vgg16_weights_th_dim_ordering_th_kernels_notop.h5\',\n                                        TH_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir=\'models\')\n            model.load_weights(weights_path)\n            if K.backend() == \'tensorflow\':\n                warnings.warn(\'You are using the TensorFlow backend, yet you \'\n                              \'are using the Theano \'\n                              \'image dimension ordering convention \'\n                              \'(`image_dim_ordering=""th""`). \'\n                              \'For best performance, set \'\n                              \'`image_dim_ordering=""tf""` in \'\n                              \'your Keras config \'\n                              \'at ~/.keras/keras.json.\')\n                convert_all_kernels_in_model(model)\n        else:\n            if include_top:\n                weights_path = get_file(\'vgg16_weights_tf_dim_ordering_tf_kernels.h5\',\n                                        TF_WEIGHTS_PATH,\n                                        cache_subdir=\'models\')\n            else:\n                weights_path = get_file(\'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\',\n                                        TF_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir=\'models\')\n            model.load_weights(weights_path)\n            if K.backend() == \'theano\':\n                convert_all_kernels_in_model(model)\n    return model\n\n\nif __name__ == \'__main__\':\n    model = VGG16(include_top=True, weights=\'imagenet\')\n\n    img_path = \'elephant.jpg\'\n    img = image.load_img(img_path, target_size=(224, 224))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    print(\'Input image shape:\', x.shape)\n\n    preds = model.predict(x)\n    print(\'Predicted:\', decode_predictions(preds))\n'"
deep-learning/keras-tutorial/deep_learning_models/vgg19.py,1,"b'# -*- coding: utf-8 -*-\n\'\'\'VGG19 model for Keras.\n\n# Reference:\n\n- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n\n\'\'\'\nfrom __future__ import print_function\n\nimport numpy as np\nimport warnings\n\nfrom keras.models import Model\nfrom keras.layers import Flatten, Dense, Input\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.preprocessing import image\nfrom keras.utils.layer_utils import convert_all_kernels_in_model\nfrom keras.utils.data_utils import get_file\nfrom keras import backend as K\n\n\nTH_WEIGHTS_PATH = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_th_dim_ordering_th_kernels.h5\'\nTF_WEIGHTS_PATH = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5\'\nTH_WEIGHTS_PATH_NO_TOP = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_th_dim_ordering_th_kernels_notop.h5\'\nTF_WEIGHTS_PATH_NO_TOP = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\'\n\n\ndef VGG19(include_top=True, weights=\'imagenet\',\n          input_tensor=None):\n    \'\'\'Instantiate the VGG19 architecture,\n    optionally loading weights pre-trained\n    on ImageNet. Note that when using TensorFlow,\n    for best performance you should set\n    `image_dim_ordering=""tf""` in your Keras config\n    at ~/.keras/keras.json.\n\n    The model and the weights are compatible with both\n    TensorFlow and Theano. The dimension ordering\n    convention used by the model is the one\n    specified in your Keras config file.\n\n    # Arguments\n        include_top: whether to include the 3 fully-connected\n            layers at the top of the network.\n        weights: one of `None` (random initialization)\n            or ""imagenet"" (pre-training on ImageNet).\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n\n    # Returns\n        A Keras model instance.\n    \'\'\'\n    if weights not in {\'imagenet\', None}:\n        raise ValueError(\'The `weights` argument should be either \'\n                         \'`None` (random initialization) or `imagenet` \'\n                         \'(pre-training on ImageNet).\')\n    # Determine proper input shape\n    if K.image_dim_ordering() == \'th\':\n        if include_top:\n            input_shape = (3, 224, 224)\n        else:\n            input_shape = (3, None, None)\n    else:\n        if include_top:\n            input_shape = (224, 224, 3)\n        else:\n            input_shape = (None, None, 3)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor)\n        else:\n            img_input = input_tensor\n    # Block 1\n    x = Convolution2D(64, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block1_conv1\')(img_input)\n    x = Convolution2D(64, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block1_conv2\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block1_pool\')(x)\n\n    # Block 2\n    x = Convolution2D(128, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block2_conv1\')(x)\n    x = Convolution2D(128, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block2_conv2\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block2_pool\')(x)\n\n    # Block 3\n    x = Convolution2D(256, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block3_conv1\')(x)\n    x = Convolution2D(256, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block3_conv2\')(x)\n    x = Convolution2D(256, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block3_conv3\')(x)\n    x = Convolution2D(256, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block3_conv4\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block3_pool\')(x)\n\n    # Block 4\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block4_conv1\')(x)\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block4_conv2\')(x)\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block4_conv3\')(x)\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block4_conv4\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block4_pool\')(x)\n\n    # Block 5\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block5_conv1\')(x)\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block5_conv2\')(x)\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block5_conv3\')(x)\n    x = Convolution2D(512, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'block5_conv4\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block5_pool\')(x)\n\n    if include_top:\n        # Classification block\n        x = Flatten(name=\'flatten\')(x)\n        x = Dense(4096, activation=\'relu\', name=\'fc1\')(x)\n        x = Dense(4096, activation=\'relu\', name=\'fc2\')(x)\n        x = Dense(1000, activation=\'softmax\', name=\'predictions\')(x)\n\n    # Create model\n    model = Model(img_input, x)\n\n    # load weights\n    if weights == \'imagenet\':\n        print(\'K.image_dim_ordering:\', K.image_dim_ordering())\n        if K.image_dim_ordering() == \'th\':\n            if include_top:\n                weights_path = get_file(\'vgg19_weights_th_dim_ordering_th_kernels.h5\',\n                                        TH_WEIGHTS_PATH,\n                                        cache_subdir=\'models\')\n            else:\n                weights_path = get_file(\'vgg19_weights_th_dim_ordering_th_kernels_notop.h5\',\n                                        TH_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir=\'models\')\n            model.load_weights(weights_path)\n            if K.backend() == \'tensorflow\':\n                warnings.warn(\'You are using the TensorFlow backend, yet you \'\n                              \'are using the Theano \'\n                              \'image dimension ordering convention \'\n                              \'(`image_dim_ordering=""th""`). \'\n                              \'For best performance, set \'\n                              \'`image_dim_ordering=""tf""` in \'\n                              \'your Keras config \'\n                              \'at ~/.keras/keras.json.\')\n                convert_all_kernels_in_model(model)\n        else:\n            if include_top:\n                weights_path = get_file(\'vgg19_weights_tf_dim_ordering_tf_kernels.h5\',\n                                        TF_WEIGHTS_PATH,\n                                        cache_subdir=\'models\')\n            else:\n                weights_path = get_file(\'vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\',\n                                        TF_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir=\'models\')\n            model.load_weights(weights_path)\n            if K.backend() == \'theano\':\n                convert_all_kernels_in_model(model)\n    return model\n\n\nif __name__ == \'__main__\':\n    model = VGG19(include_top=True, weights=\'imagenet\')\n\n    img_path = \'cat.jpg\'\n    img = image.load_img(img_path, target_size=(224, 224))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    print(\'Input image shape:\', x.shape)\n\n    preds = model.predict(x)\n    print(\'Predicted:\', decode_predictions(preds))\n'"
deep-learning/keras-tutorial/solutions/sol_111.py,0,"b'ann = ANN(2, 10, 1)\n%timeit -n 1 -r 1 ann.train(zip(X,y), iterations=2)\nplot_decision_boundary(ann)\nplt.title(""Our next model with 10 hidden units"")\n'"
deep-learning/keras-tutorial/solutions/sol_112.py,0,"b'ann = ANN(2, 10, 1)\n%timeit -n 1 -r 1 ann.train(zip(X,y), iterations=100)\nplot_decision_boundary(ann)\nplt.title(""Our model with 10 hidden units and 100 iterations"")\n'"
deep-learning/theano-tutorial/intro_theano/utils.py,0,"b'"""""" This file contains different utility functions that are not connected\nin anyway to the networks presented in the tutorials, but rather help in\nprocessing the outputs into a more understandable way.\n\nFor example ``tile_raster_images`` helps in generating a easy to grasp\nimage from a set of samples or weights.\n""""""\n\n\nimport numpy\nfrom six.moves import xrange\n\n\ndef scale_to_unit_interval(ndar, eps=1e-8):\n    """""" Scales all values in the ndarray ndar to be between 0 and 1 """"""\n    ndar = ndar.copy()\n    ndar -= ndar.min()\n    ndar *= 1.0 / (ndar.max() + eps)\n    return ndar\n\n\ndef tile_raster_images(X, img_shape, tile_shape, tile_spacing=(0, 0),\n                       scale_rows_to_unit_interval=True,\n                       output_pixel_vals=True):\n    """"""\n    Transform an array with one flattened image per row, into an array in\n    which images are reshaped and layed out like tiles on a floor.\n\n    This function is useful for visualizing datasets whose rows are images,\n    and also columns of matrices for transforming those rows\n    (such as the first layer of a neural net).\n\n    :type X: a 2-D ndarray or a tuple of 4 channels, elements of which can\n    be 2-D ndarrays or None;\n    :param X: a 2-D array in which every row is a flattened image.\n\n    :type img_shape: tuple; (height, width)\n    :param img_shape: the original shape of each image\n\n    :type tile_shape: tuple; (rows, cols)\n    :param tile_shape: the number of images to tile (rows, cols)\n\n    :param output_pixel_vals: if output should be pixel values (i.e. int8\n    values) or floats\n\n    :param scale_rows_to_unit_interval: if the values need to be scaled before\n    being plotted to [0,1] or not\n\n\n    :returns: array suitable for viewing as an image.\n    (See:`Image.fromarray`.)\n    :rtype: a 2-d array with same dtype as X.\n\n    """"""\n\n    assert len(img_shape) == 2\n    assert len(tile_shape) == 2\n    assert len(tile_spacing) == 2\n\n    # The expression below can be re-written in a more C style as\n    # follows :\n    #\n    # out_shape    = [0,0]\n    # out_shape[0] = (img_shape[0]+tile_spacing[0])*tile_shape[0] -\n    #                tile_spacing[0]\n    # out_shape[1] = (img_shape[1]+tile_spacing[1])*tile_shape[1] -\n    #                tile_spacing[1]\n    out_shape = [\n        (ishp + tsp) * tshp - tsp\n        for ishp, tshp, tsp in zip(img_shape, tile_shape, tile_spacing)\n    ]\n\n    if isinstance(X, tuple):\n        assert len(X) == 4\n        # Create an output numpy ndarray to store the image\n        if output_pixel_vals:\n            out_array = numpy.zeros((out_shape[0], out_shape[1], 4),\n                                    dtype=\'uint8\')\n        else:\n            out_array = numpy.zeros((out_shape[0], out_shape[1], 4),\n                                    dtype=X.dtype)\n\n        #colors default to 0, alpha defaults to 1 (opaque)\n        if output_pixel_vals:\n            channel_defaults = [0, 0, 0, 255]\n        else:\n            channel_defaults = [0., 0., 0., 1.]\n\n        for i in xrange(4):\n            if X[i] is None:\n                # if channel is None, fill it with zeros of the correct\n                # dtype\n                dt = out_array.dtype\n                if output_pixel_vals:\n                    dt = \'uint8\'\n                out_array[:, :, i] = numpy.zeros(\n                    out_shape,\n                    dtype=dt\n                ) + channel_defaults[i]\n            else:\n                # use a recurrent call to compute the channel and store it\n                # in the output\n                out_array[:, :, i] = tile_raster_images(\n                    X[i], img_shape, tile_shape, tile_spacing,\n                    scale_rows_to_unit_interval, output_pixel_vals)\n        return out_array\n\n    else:\n        # if we are dealing with only one channel\n        H, W = img_shape\n        Hs, Ws = tile_spacing\n\n        # generate a matrix to store the output\n        dt = X.dtype\n        if output_pixel_vals:\n            dt = \'uint8\'\n        out_array = numpy.zeros(out_shape, dtype=dt)\n\n        for tile_row in xrange(tile_shape[0]):\n            for tile_col in xrange(tile_shape[1]):\n                if tile_row * tile_shape[1] + tile_col < X.shape[0]:\n                    this_x = X[tile_row * tile_shape[1] + tile_col]\n                    if scale_rows_to_unit_interval:\n                        # if we should scale values to be between 0 and 1\n                        # do this by calling the `scale_to_unit_interval`\n                        # function\n                        this_img = scale_to_unit_interval(\n                            this_x.reshape(img_shape))\n                    else:\n                        this_img = this_x.reshape(img_shape)\n                    # add the slice to the corresponding position in the\n                    # output array\n                    c = 1\n                    if output_pixel_vals:\n                        c = 255\n                    out_array[\n                        tile_row * (H + Hs): tile_row * (H + Hs) + H,\n                        tile_col * (W + Ws): tile_col * (W + Ws) + W\n                    ] = this_img * c\n        return out_array\n'"
deep-learning/theano-tutorial/rnn_tutorial/lstm_text.py,0,"b'import cPickle as pkl\nimport time\n\nimport numpy\nimport theano\nfrom theano import config\nimport theano.tensor as T\nfrom theano.tensor.nnet import categorical_crossentropy\n\nfrom fuel.datasets import TextFile\nfrom fuel.streams import DataStream\nfrom fuel.schemes import ConstantScheme\nfrom fuel.transformers import Batch, Padding\n\n\n# These files can be downloaded from\n# http://www-etud.iro.umontreal.ca/~brakelp/train.txt.gz\n# http://www-etud.iro.umontreal.ca/~brakelp/dictionary.pkl\n# don\'t forget to change the paths and gunzip train.txt.gz\nTRAIN_FILE = \'/u/brakelp/temp/traindata.txt\'\nVAL_FILE = \'/u/brakelp/temp/valdata.txt\'\nDICT_FILE = \'/u/brakelp/temp/dictionary.pkl\'\n\n\ndef sequence_categorical_crossentropy(prediction, targets, mask):\n    prediction_flat = prediction.reshape(((prediction.shape[0] *\n                                           prediction.shape[1]),\n                                          prediction.shape[2]), ndim=2)\n    targets_flat = targets.flatten()\n    mask_flat = mask.flatten()\n    ce = categorical_crossentropy(prediction_flat, targets_flat)\n    return T.sum(ce * mask_flat)\n\n\ndef gauss_weight(ndim_in, ndim_out=None, sd=.005):\n    if ndim_out is None:\n        ndim_out = ndim_in\n    W = numpy.random.randn(ndim_in, ndim_out) * sd\n    return numpy.asarray(W, dtype=config.floatX)\n\n\nclass LogisticRegression(object):\n    """"""Multi-class Logistic Regression Class\n\n    The logistic regression is fully described by a weight matrix :math:`W`\n    and bias vector :math:`b`. Classification is done by projecting data\n    points onto a set of hyperplanes, the distance to which is used to\n    determine a class membership probability.\n    """"""\n\n    def __init__(self, input, n_in, n_out):\n        """""" Initialize the parameters of the logistic regression\n\n        :type input: theano.tensor.TensorType\n        :param input: symbolic variable that describes the input of the\n                      architecture (one minibatch)\n\n        :type n_in: int\n        :param n_in: number of input units, the dimension of the space in\n                     which the datapoints lie\n\n        :type n_out: int\n        :param n_out: number of output units, the dimension of the space in\n                      which the labels lie\n\n        """"""\n\n        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n        self.W = theano.shared(value=numpy.zeros((n_in, n_out),\n                                                 dtype=theano.config.floatX),\n                               name=\'W\', borrow=True)\n        # initialize the baises b as a vector of n_out 0s\n        self.b = theano.shared(value=numpy.zeros((n_out,),\n                                                 dtype=theano.config.floatX),\n                               name=\'b\', borrow=True)\n\n        # compute vector of class-membership probabilities in symbolic form\n        energy = T.dot(input, self.W) + self.b\n        energy_exp = T.exp(energy - T.max(energy, 2)[:, :, None])\n        pmf = energy_exp / energy_exp.sum(2)[:, :, None]\n        self.p_y_given_x = pmf\n\n        # compute prediction as class whose probability is maximal in\n        # symbolic form\n        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n\n        # parameters of the model\n        self.params = [self.W, self.b]\n\n\ndef index_dot(indices, w):\n    return w[indices.flatten()]\n\n\nclass LstmLayer:\n\n    def __init__(self, rng, input, mask, n_in, n_h):\n\n        # Init params\n        self.W_i = theano.shared(gauss_weight(n_in, n_h), \'W_i\', borrow=True)\n        self.W_f = theano.shared(gauss_weight(n_in, n_h), \'W_f\', borrow=True)\n        self.W_c = theano.shared(gauss_weight(n_in, n_h), \'W_c\', borrow=True)\n        self.W_o = theano.shared(gauss_weight(n_in, n_h), \'W_o\', borrow=True)\n\n        self.U_i = theano.shared(gauss_weight(n_h), \'U_i\', borrow=True)\n        self.U_f = theano.shared(gauss_weight(n_h), \'U_f\', borrow=True)\n        self.U_c = theano.shared(gauss_weight(n_h), \'U_c\', borrow=True)\n        self.U_o = theano.shared(gauss_weight(n_h), \'U_o\', borrow=True)\n\n        self.b_i = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 \'b_i\', borrow=True)\n        self.b_f = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 \'b_f\', borrow=True)\n        self.b_c = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 \'b_c\', borrow=True)\n        self.b_o = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 \'b_o\', borrow=True)\n\n        self.params = [self.W_i, self.W_f, self.W_c, self.W_o,\n                       self.U_i, self.U_f, self.U_c, self.U_o,\n                       self.b_i, self.b_f, self.b_c, self.b_o]\n\n        outputs_info = [T.zeros((input.shape[1], n_h)),\n                        T.zeros((input.shape[1], n_h))]\n\n        rval, updates = theano.scan(self._step,\n                                    sequences=[mask, input],\n                                    outputs_info=outputs_info)\n\n        # self.output is in the format (batchsize, n_h)\n        self.output = rval[0]\n\n    def _step(self, m_, x_, h_, c_):\n\n        i_preact = (index_dot(x_, self.W_i) +\n                    T.dot(h_, self.U_i) + self.b_i)\n        i = T.nnet.sigmoid(i_preact)\n\n        f_preact = (index_dot(x_, self.W_f) +\n                    T.dot(h_, self.U_f) + self.b_f)\n        f = T.nnet.sigmoid(f_preact)\n\n        o_preact = (index_dot(x_, self.W_o) +\n                    T.dot(h_, self.U_o) + self.b_o)\n        o = T.nnet.sigmoid(o_preact)\n\n        c_preact = (index_dot(x_, self.W_c) +\n                    T.dot(h_, self.U_c) + self.b_c)\n        c = T.tanh(c_preact)\n\n        c = f * c_ + i * c\n        c = m_[:, None] * c + (1. - m_)[:, None] * c_\n\n        h = o * T.tanh(c)\n        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n\n        return h, c\n\n\ndef train_model(batch_size=100, n_h=50, n_epochs=40):\n\n    # Load the datasets with Fuel\n    dictionary = pkl.load(open(DICT_FILE, \'r\'))\n    dictionary[\'~\'] = len(dictionary)\n    reverse_mapping = dict((j, i) for i, j in dictionary.items())\n\n    print(""Loading the data"")\n    train = TextFile(files=[TRAIN_FILE],\n                     dictionary=dictionary,\n                     unk_token=\'~\',\n                     level=\'character\',\n                     preprocess=str.lower,\n                     bos_token=None,\n                     eos_token=None)\n\n    train_stream = DataStream.default_stream(train)\n\n    # organize data in batches and pad shorter sequences with zeros\n    train_stream = Batch(train_stream,\n                         iteration_scheme=ConstantScheme(batch_size))\n    train_stream = Padding(train_stream)\n\n    # idem dito for the validation text\n    val = TextFile(files=[VAL_FILE],\n                     dictionary=dictionary,\n                     unk_token=\'~\',\n                     level=\'character\',\n                     preprocess=str.lower,\n                     bos_token=None,\n                     eos_token=None)\n\n    val_stream = DataStream.default_stream(val)\n\n    # organize data in batches and pad shorter sequences with zeros\n    val_stream = Batch(val_stream,\n                         iteration_scheme=ConstantScheme(batch_size))\n    val_stream = Padding(val_stream)\n\n    print(\'Building model\')\n\n    # Set the random number generator\' seeds for consistency\n    rng = numpy.random.RandomState(12345)\n\n    x = T.lmatrix(\'x\')\n    mask = T.matrix(\'mask\')\n\n    # Construct the LSTM layer\n    recurrent_layer = LstmLayer(rng=rng, input=x, mask=mask, n_in=111, n_h=n_h)\n\n    logreg_layer = LogisticRegression(input=recurrent_layer.output[:-1],\n                                      n_in=n_h, n_out=111)\n\n    cost = sequence_categorical_crossentropy(logreg_layer.p_y_given_x,\n                                             x[1:],\n                                             mask[1:]) / batch_size\n\n    # create a list of all model parameters to be fit by gradient descent\n    params = logreg_layer.params + recurrent_layer.params\n\n    # create a list of gradients for all model parameters\n    grads = T.grad(cost, params)\n\n    # update_model is a function that updates the model parameters by\n    # SGD Since this model has many parameters, it would be tedious to\n    # manually create an update rule for each model parameter. We thus\n    # create the updates list by automatically looping over all\n    # (params[i], grads[i]) pairs.\n    learning_rate = 0.1\n    updates = [\n        (param_i, param_i - learning_rate * grad_i)\n        for param_i, grad_i in zip(params, grads)\n    ]\n\n    update_model = theano.function([x, mask], cost, updates=updates)\n\n    evaluate_model = theano.function([x, mask], cost)\n\n    # Define and compile a function for generating a sequence step by step.\n    x_t = T.iscalar()\n    h_p = T.vector()\n    c_p = T.vector()\n    h_t, c_t = recurrent_layer._step(T.ones(1), x_t, h_p, c_p)\n    energy = T.dot(h_t, logreg_layer.W) + logreg_layer.b\n\n    energy_exp = T.exp(energy - T.max(energy, 1)[:, None])\n\n    output = energy_exp / energy_exp.sum(1)[:, None]\n    single_step = theano.function([x_t, h_p, c_p], [output, h_t, c_t])\n\n    start_time = time.clock()\n\n    iteration = 0\n\n    for epoch in range(n_epochs):\n        print \'epoch:\', epoch\n\n        for x_, mask_ in train_stream.get_epoch_iterator():\n            iteration += 1\n\n            cross_entropy = update_model(x_.T, mask_.T)\n\n\n            # Generate some text after each 20 minibatches\n            if iteration % 40 == 0:\n                try:\n                    prediction = numpy.ones(111, dtype=config.floatX) / 111.0\n                    h_p = numpy.zeros((n_h,), dtype=config.floatX)\n                    c_p = numpy.zeros((n_h,), dtype=config.floatX)\n                    initial = \'the meaning of life is \'\n                    sentence = initial\n                    for char in initial:\n                        x_t = dictionary[char]\n                        prediction, h_p, c_p = single_step(x_t, h_p.flatten(),\n                                                           c_p.flatten())\n                    sample = numpy.random.multinomial(1, prediction.flatten())\n                    for i in range(450):\n                        x_t = numpy.argmax(sample)\n                        prediction, h_p, c_p = single_step(x_t, h_p.flatten(),\n                                                           c_p.flatten())\n                        sentence += reverse_mapping[x_t]\n                        sample = numpy.random.multinomial(1, prediction.flatten())\n                    print \'LSTM: ""\' + sentence + \'""\'\n                except ValueError:\n                    print \'Something went wrong during sentence generation.\'\n\n            if iteration % 40 == 0:\n                print \'epoch:\', epoch, \'  minibatch:\', iteration\n                val_scores = []\n                for x_val, mask_val in val_stream.get_epoch_iterator():\n                    val_scores.append(evaluate_model(x_val.T, mask_val.T))\n                print \'Average validation CE per sentence:\', numpy.mean(val_scores)\n\n    end_time = time.clock()\n    print(\'Optimization complete.\')\n    print(\'The code ran for %.2fm\' % ((end_time - start_time) / 60.))\n\n\nif __name__ == \'__main__\':\n    train_model()\n'"
deep-learning/theano-tutorial/rnn_tutorial/rnn_precompile.py,0,"b'""""""This file is only here to speed up the execution of notebooks.\n\nIt contains a subset of the code defined in simple_rnn.ipynb and\nlstm_text.ipynb, in particular the code compiling Theano function.\nExecuting this script first will populate the cache of compiled C code,\nwhich will make subsequent compilations faster.\n\nThe use case is to run this script in the background when a demo VM\nsuch as the one for NVIDIA\'s qwikLABS, so that the compilation phase\nstarted from the notebooks is faster.\n\n""""""\nimport numpy\n\nimport theano\nimport theano.tensor as T\n\nfrom theano import config\nfrom theano.tensor.nnet import categorical_crossentropy\n\n\nfloatX = theano.config.floatX\n\n\n# simple_rnn.ipynb\n\nclass SimpleRNN(object):\n    def __init__(self, input_dim, recurrent_dim):\n        w_xh = numpy.random.normal(0, .01, (input_dim, recurrent_dim))\n        w_hh = numpy.random.normal(0, .02, (recurrent_dim, recurrent_dim))\n        self.w_xh = theano.shared(numpy.asarray(w_xh, dtype=floatX), name=\'w_xh\')\n        self.w_hh = theano.shared(numpy.asarray(w_hh, dtype=floatX), name=\'w_hh\')\n        self.b_h = theano.shared(numpy.zeros((recurrent_dim,), dtype=floatX), name=\'b_h\')\n        self.parameters = [self.w_xh, self.w_hh, self.b_h]\n\n    def _step(self, input_t, previous):\n        return T.tanh(T.dot(previous, self.w_hh) + input_t)\n\n    def __call__(self, x):\n        x_w_xh = T.dot(x, self.w_xh) + self.b_h\n        result, updates = theano.scan(self._step,\n                                      sequences=[x_w_xh],\n                                      outputs_info=[T.zeros_like(self.b_h)])\n        return result\n\n\nw_ho_np = numpy.random.normal(0, .01, (15, 1))\nw_ho = theano.shared(numpy.asarray(w_ho_np, dtype=floatX), name=\'w_ho\')\nb_o = theano.shared(numpy.zeros((1,), dtype=floatX), name=\'b_o\')\n\nx = T.matrix(\'x\')\nmy_rnn = SimpleRNN(1, 15)\nhidden = my_rnn(x)\nprediction = T.dot(hidden, w_ho) + b_o\nparameters = my_rnn.parameters + [w_ho, b_o]\nl2 = sum((p**2).sum() for p in parameters)\nmse = T.mean((prediction[:-1] - x[1:])**2)\ncost = mse + .0001 * l2\ngradient = T.grad(cost, wrt=parameters)\n\nlr = .3\nupdates = [(par, par - lr * gra) for par, gra in zip(parameters, gradient)]\nupdate_model = theano.function([x], cost, updates=updates)\nget_cost = theano.function([x], mse)\npredict = theano.function([x], prediction)\nget_hidden = theano.function([x], hidden)\nget_gradient = theano.function([x], gradient)\n\npredict = theano.function([x], prediction)\n\n# Generating sequences\n\nx_t = T.vector()\nh_p = T.vector()\npreactivation = T.dot(x_t, my_rnn.w_xh) + my_rnn.b_h\nh_t = my_rnn._step(preactivation, h_p)\no_t = T.dot(h_t, w_ho) + b_o\n\nsingle_step = theano.function([x_t, h_p], [o_t, h_t])\n\n# lstm_text.ipynb\n\ndef gauss_weight(rng, ndim_in, ndim_out=None, sd=.005):\n    if ndim_out is None:\n        ndim_out = ndim_in\n    W = rng.randn(ndim_in, ndim_out) * sd\n    return numpy.asarray(W, dtype=config.floatX)\n\n\ndef index_dot(indices, w):\n    return w[indices.flatten()]\n\n\nclass LstmLayer:\n\n    def __init__(self, rng, input, mask, n_in, n_h):\n\n        # Init params\n        self.W_i = theano.shared(gauss_weight(rng, n_in, n_h), \'W_i\', borrow=True)\n        self.W_f = theano.shared(gauss_weight(rng, n_in, n_h), \'W_f\', borrow=True)\n        self.W_c = theano.shared(gauss_weight(rng, n_in, n_h), \'W_c\', borrow=True)\n        self.W_o = theano.shared(gauss_weight(rng, n_in, n_h), \'W_o\', borrow=True)\n\n        self.U_i = theano.shared(gauss_weight(rng, n_h), \'U_i\', borrow=True)\n        self.U_f = theano.shared(gauss_weight(rng, n_h), \'U_f\', borrow=True)\n        self.U_c = theano.shared(gauss_weight(rng, n_h), \'U_c\', borrow=True)\n        self.U_o = theano.shared(gauss_weight(rng, n_h), \'U_o\', borrow=True)\n\n        self.b_i = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 \'b_i\', borrow=True)\n        self.b_f = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 \'b_f\', borrow=True)\n        self.b_c = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 \'b_c\', borrow=True)\n        self.b_o = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 \'b_o\', borrow=True)\n\n        self.params = [self.W_i, self.W_f, self.W_c, self.W_o,\n                       self.U_i, self.U_f, self.U_c, self.U_o,\n                       self.b_i, self.b_f, self.b_c, self.b_o]\n\n        outputs_info = [T.zeros((input.shape[1], n_h)),\n                        T.zeros((input.shape[1], n_h))]\n\n        rval, updates = theano.scan(self._step,\n                                    sequences=[mask, input],\n                                    outputs_info=outputs_info)\n\n        # self.output is in the format (length, batchsize, n_h)\n        self.output = rval[0]\n\n    def _step(self, m_, x_, h_, c_):\n\n        i_preact = (index_dot(x_, self.W_i) +\n                    T.dot(h_, self.U_i) + self.b_i)\n        i = T.nnet.sigmoid(i_preact)\n\n        f_preact = (index_dot(x_, self.W_f) +\n                    T.dot(h_, self.U_f) + self.b_f)\n        f = T.nnet.sigmoid(f_preact)\n\n        o_preact = (index_dot(x_, self.W_o) +\n                    T.dot(h_, self.U_o) + self.b_o)\n        o = T.nnet.sigmoid(o_preact)\n\n        c_preact = (index_dot(x_, self.W_c) +\n                    T.dot(h_, self.U_c) + self.b_c)\n        c = T.tanh(c_preact)\n\n        c = f * c_ + i * c\n        c = m_[:, None] * c + (1. - m_)[:, None] * c_\n\n        h = o * T.tanh(c)\n        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n\n        return h, c\n\n\ndef sequence_categorical_crossentropy(prediction, targets, mask):\n    prediction_flat = prediction.reshape(((prediction.shape[0] *\n                                           prediction.shape[1]),\n                                          prediction.shape[2]), ndim=2)\n    targets_flat = targets.flatten()\n    mask_flat = mask.flatten()\n    ce = categorical_crossentropy(prediction_flat, targets_flat)\n    return T.sum(ce * mask_flat)\n\n\nclass LogisticRegression(object):\n\n    def __init__(self, rng, input, n_in, n_out):\n\n        W = gauss_weight(rng, n_in, n_out)\n        self.W = theano.shared(value=numpy.asarray(W, dtype=theano.config.floatX),\n                               name=\'W\', borrow=True)\n        # initialize the biases b as a vector of n_out 0s\n        self.b = theano.shared(value=numpy.zeros((n_out,),\n                                                 dtype=theano.config.floatX),\n                               name=\'b\', borrow=True)\n\n        # compute vector of class-membership probabilities in symbolic form\n        energy = T.dot(input, self.W) + self.b\n        energy_exp = T.exp(energy - T.max(energy, axis=2, keepdims=True))\n        pmf = energy_exp / energy_exp.sum(axis=2, keepdims=True)\n        self.p_y_given_x = pmf\n        self.params = [self.W, self.b]\n\nbatch_size = 100\nn_h = 50\n\n# The Theano graph\n# Set the random number generator\' seeds for consistency\nrng = numpy.random.RandomState(12345)\n\nx = T.lmatrix(\'x\')\nmask = T.matrix(\'mask\')\n\n# Construct an LSTM layer and a logistic regression layer\nrecurrent_layer = LstmLayer(rng=rng, input=x, mask=mask, n_in=111, n_h=n_h)\nlogreg_layer = LogisticRegression(rng=rng, input=recurrent_layer.output[:-1],\n                                  n_in=n_h, n_out=111)\n\n# define a cost variable to optimize\ncost = sequence_categorical_crossentropy(logreg_layer.p_y_given_x,\n                                         x[1:],\n                                         mask[1:]) / batch_size\n\n# create a list of all model parameters to be fit by gradient descent\nparams = logreg_layer.params + recurrent_layer.params\n\n# create a list of gradients for all model parameters\ngrads = T.grad(cost, params)\n\nlearning_rate = 0.1\nupdates = [\n    (param_i, param_i - learning_rate * grad_i)\n    for param_i, grad_i in zip(params, grads)\n]\n\nupdate_model = theano.function([x, mask], cost, updates=updates)\n\nevaluate_model = theano.function([x, mask], cost)\n\n# Generating Sequences\nx_t = T.iscalar()\nh_p = T.vector()\nc_p = T.vector()\nh_t, c_t = recurrent_layer._step(T.ones(1), x_t, h_p, c_p)\nenergy = T.dot(h_t, logreg_layer.W) + logreg_layer.b\n\nenergy_exp = T.exp(energy - T.max(energy, axis=1, keepdims=True))\n\noutput = energy_exp / energy_exp.sum(axis=1, keepdims=True)\nsingle_step = theano.function([x_t, h_p, c_p], [output, h_t, c_t])\n'"
deep-learning/theano-tutorial/rnn_tutorial/synthetic.py,12,"b'import collections\nimport numpy as np\n\n\ndef mackey_glass(sample_len=1000, tau=17, seed=None, n_samples = 1):\n    \'\'\'\n    mackey_glass(sample_len=1000, tau=17, seed = None, n_samples = 1) -> input\n    Generate the Mackey Glass time-series. Parameters are:\n        - sample_len: length of the time-series in timesteps. Default is 1000.\n        - tau: delay of the MG - system. Commonly used values are tau=17 (mild \n          chaos) and tau=30 (moderate chaos). Default is 17.\n        - seed: to seed the random generator, can be used to generate the same\n          timeseries at each invocation.\n        - n_samples : number of samples to generate\n    \'\'\'\n    delta_t = 10\n    history_len = tau * delta_t \n    # Initial conditions for the history of the system\n    timeseries = 1.2\n    \n    if seed is not None:\n        np.random.seed(seed)\n\n    samples = []\n\n    for _ in range(n_samples):\n        history = collections.deque(1.2 * np.ones(history_len) + 0.2 * \\\n                                    (np.random.rand(history_len) - 0.5))\n        # Preallocate the array for the time-series\n        inp = np.zeros((sample_len,1))\n        \n        for timestep in range(sample_len):\n            for _ in range(delta_t):\n                xtau = history.popleft()\n                history.append(timeseries)\n                timeseries = history[-1] + (0.2 * xtau / (1.0 + xtau ** 10) - \\\n                             0.1 * history[-1]) / delta_t\n            inp[timestep] = timeseries\n        \n        # Squash timeseries through tanh\n        inp = np.tanh(inp - 1)\n        samples.append(inp)\n    return samples\n\n\ndef mso(sample_len=1000, n_samples = 1):\n    \'\'\'\n    mso(sample_len=1000, n_samples = 1) -> input\n    Generate the Multiple Sinewave Oscillator time-series, a sum of two sines\n    with incommensurable periods. Parameters are:\n        - sample_len: length of the time-series in timesteps\n        - n_samples: number of samples to generate\n    \'\'\'\n    signals = []\n    for _ in range(n_samples):\n        phase = np.random.rand()\n        x = np.atleast_2d(np.arange(sample_len)).T\n        signals.append(np.sin(0.2 * x + phase) + np.sin(0.311 * x + phase))\n    return signals\n\n\ndef lorentz(sample_len=1000, sigma=10, rho=28, beta=8 / 3, step=0.01):\n    """"""This function generates a Lorentz time series of length sample_len,\n    with standard parameters sigma, rho and beta. \n    """"""\n\n    x = np.zeros([sample_len])\n    y = np.zeros([sample_len])\n    z = np.zeros([sample_len])\n\n    # Initial conditions taken from \'Chaos and Time Series Analysis\', J. Sprott\n    x[0] = 0;\n    y[0] = -0.01;\n    z[0] = 9;\n\n    for t in range(sample_len - 1):\n        x[t + 1] = x[t] + sigma * (y[t] - x[t]) * step\n        y[t + 1] = y[t] + (x[t] * (rho - z[t]) - y[t]) * step\n        z[t + 1] = z[t] + (x[t] * y[t] - beta * z[t]) * step\n\n    x.shape += (1,)\n    y.shape += (1,)\n    z.shape += (1,)\n\n    return np.concatenate((x, y, z), axis=1)\n'"
deep-learning/theano-tutorial/scan_tutorial/scan_ex1_solution.py,2,"b'import theano\nimport theano.tensor as T\nimport numpy as np\n\ncoefficients = T.vector(""coefficients"")\nx = T.scalar(""x"")\nmax_coefficients_supported = 10000\n\n\ndef step(coeff, power, prior_value, free_var):\n    return prior_value + (coeff * (free_var ** power))\n\n# Generate the components of the polynomial\nfull_range = T.arange(max_coefficients_supported)\noutputs_info = np.zeros((), dtype=theano.config.floatX)\n\ncomponents, updates = theano.scan(fn=step,\n                                  sequences=[coefficients, full_range],\n                                  outputs_info=outputs_info,\n                                  non_sequences=x)\n\npolynomial = components[-1]\ncalculate_polynomial = theano.function(inputs=[coefficients, x],\n                                       outputs=polynomial,\n                                       updates=updates)\n\ntest_coeff = np.asarray([1, 0, 2], dtype=theano.config.floatX)\nprint(calculate_polynomial(test_coeff, 3))\n'"
deep-learning/theano-tutorial/scan_tutorial/scan_ex2_solution.py,1,"b'import theano\nimport theano.tensor as T\nimport numpy as np\n\nprobabilities = T.vector()\nnb_samples = T.iscalar()\n\nrng = T.shared_randomstreams.RandomStreams(1234)\n\n\ndef sample_from_pvect(pvect):\n    """""" Provided utility function: given a symbolic vector of\n    probabilities (which MUST sum to 1), sample one element\n    and return its index.\n    """"""\n    onehot_sample = rng.multinomial(n=1, pvals=pvect)\n    sample = onehot_sample.argmax()\n    return sample\n\n\ndef set_p_to_zero(pvect, i):\n    """""" Provided utility function: given a symbolic vector of\n    probabilities and an index \'i\', set the probability of the\n    i-th element to 0 and renormalize the probabilities so they\n    sum to 1.\n    """"""\n    new_pvect = T.set_subtensor(pvect[i], 0.)\n    new_pvect = new_pvect / new_pvect.sum()\n    return new_pvect\n\n\ndef step(p):\n    sample = sample_from_pvect(p)\n    new_p = set_p_to_zero(p, sample)\n    return new_p, sample\n\noutput, updates = theano.scan(fn=step,\n                              outputs_info=[probabilities, None],\n                              n_steps=nb_samples)\n\nmodified_probabilities, samples = output\n\nf = theano.function(inputs=[probabilities, nb_samples],\n                    outputs=[samples],\n                    updates=updates)\n\n# Testing the function\ntest_probs = np.asarray([0.6, 0.3, 0.1], dtype=theano.config.floatX)\nfor i in range(10):\n    print(f(test_probs, 2))\n'"
