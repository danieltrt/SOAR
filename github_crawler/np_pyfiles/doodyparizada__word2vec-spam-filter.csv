file_path,api_count,code
server/app.py,3,"b'import json\nimport sys\nfrom math import log\n\nfrom flask import request, Flask, jsonify\nimport numpy as np\n\nfrom model import DB\nfrom generate import generate_matrix, normalize_matrix, normalize_vector\nfrom flask_cors import CORS\n\n\napp = Flask(__name__)\nCORS(app)\n\nGLOVE = \'../corpus/glove.6B.300d.txt\'\nFREQ = \'../corpus/enwiki-20150602-words-frequency.txt\'\n\niweights = {}\nvocab = {}\nivocab = {}\nWORD_LIST = \'\'\nW_norm = None\nmessages = []\n\nEPSILON = 0.95\nDEFAULT_WEIGHT = 15\n\n\ndef init():\n    """"""read glove file and generate a word matrix""""""\n    global W_norm, WORD_LIST, vocab, ivocab, iweights \n    sys.stderr.write(\'initializing word vectors\')\n    word_vectors = []\n    \n    # open and parse word vector file\n    with open(GLOVE, \'r\') as f:\n        for i, line in enumerate(f):\n            vals = line.rstrip().split(\' \')\n            vector = [float(x) for x in vals[1:]]\n            word = vals[0]\n            word_vectors.append((word, vector))\n            if i % 10000 == 0:\n                sys.stderr.write(\'.\')\n    \n    WORD_LIST += \'\\n\'.join(w for w, _ in word_vectors)\n    W, vocab, ivocab = generate_matrix(word_vectors)\n    W_norm = normalize_matrix(W)\n\n    sys.stderr.write(\'\\ninitializing word weights\')\n    max_freq = None\n    with open(FREQ, \'r\') as f:\n        for i, line in enumerate(f):\n            vals = line.rstrip().split(\' \')\n            word = vals[0]\n            freq = int(vals[1])\n            max_freq = max_freq or freq  # the first iteration will set max_freq. The first line is the highest freq\n            if word in vocab:\n                iweights[vocab[word]] = freq_to_weight(freq, max_freq)\n            if i % 10000 == 0:\n                sys.stderr.write(\'.\')\n\n    sys.stderr.write(\'\\ndone!\\n\')\n\n\ndef get_vector(idx):\n    """"""return the weighted vector for an index.""""""\n    return (W_norm[idx, :] * iweights.get(idx, DEFAULT_WEIGHT))\n\n\ndef freq_to_weight(freq, max_freq):\n    """"""calculate a vector weight for a frequency.""""""\n    # taken from https://www.wikiwand.com/en/Word_lists_by_frequency\n    return 0.5 - log(float(freq)/max_freq, 2)\n\n    \ndef generate_spam_matrix(report_threashold):\n    """"""\n    put all known spam vectors in a matrix\n    """"""\n    db = DB.load()\n    word_vectors = [(word, rm.vector)\n                    for word, rm in db.reported_messages.items()\n                    if rm.reports >= report_threashold]\n    return generate_matrix(word_vectors)\n\n\ndef closest_spam(vector, report_threashold=3):\n    """"""given a vector, return the closest spam messages and distance.""""""\n    W, vocab, ivocab = generate_spam_matrix(report_threashold=report_threashold)\n\n    if not vocab:  # means empty db\n        return \'\', 0\n\n    vector = normalize_vector(vector)\n\n    dist = np.dot(W, vector.T)\n\n    a = np.argsort(-dist)[:1]  # currently returns generator of 3 most closest\n    for x in a:\n        return ivocab[x], float(dist[x])\n\n    return \'\', 0\n\ndef tokenize_message(message):\n    """"""return a list of normalized words.""""""\n    return (message\n            .lower()\n            .replace(""."", "" ."")\n            .replace("","", "" ,"")\n            .replace(""?"", "" ?"")\n            .replace(""!"", "" !"")\n            .replace("":"", "" :"")\n            .replace(""\'s"", "" \'s"")\n            .split())\n\n\ndef message_to_vector(message):\n    """"""sums up all known vectors of a given message.""""""\n    vector = np.zeros(W_norm[0, :].shape)\n    for term in tokenize_message(message):\n        if term in vocab:\n            vector += get_vector(vocab[term])\n    return vector\n\n\n@app.route(\'/words/list\')\ndef word_list():\n    """"""return word list. ordered by indexes.""""""\n    return WORD_LIST\n\n\n@app.route(\'/words/vector\')\ndef word_vectors():\n    """"""retrun vectors for the words by given ids.""""""\n    ids = {int(i) for i in request.args[\'ids\'].split(\',\')}\n\n    return jsonify({\'words\':\n                    {i: {\'vector\': get_vector(i).tolist()}\n                     for i in ids}})\n\n\n@app.route(\'/spam/detect\')\ndef detect_spam():\n    """"""the given vector should not be normalized. normalization happens on server.""""""\n    vector = [float(i) for i in request.args[\'vector\'].split(\',\')]\n    msg, dist = closest_spam(vector)\n    is_spam = dist > EPSILON\n    return jsonify({\'spam\': is_spam,\n                    \'confidence\': dist,\n                    \'meta\': msg})\n\n\n@app.route(\'/spam/report\', methods=[\'POST\'])\ndef report_spam():\n    """"""if spam message already exists or is close to a known message add a report count. else add as new entry in db.""""""\n    data = request.get_json()\n    reported_message = data[\'message\']\n    vector = message_to_vector(reported_message)\n\n    similar_msg, dist = closest_spam(vector, 0)\n\n    db = DB.load()\n    if dist > EPSILON:\n        db.reported_messages[similar_msg].reports += 1\n    else:\n        db.add_new_message(reported_message, normalize_vector(vector).tolist())\n\n    db.save()\n    return jsonify({})\n\n@app.route(\'/messages\', methods=[\'POST\', \'GET\'])\ndef message_handler():\n    global messages\n    if request.method == \'POST\':\n        messages.append(request.get_json()[\'message\'])\n        return jsonify({})\n    else:\n        if messages:\n            return jsonify({\'message\': messages.pop(0)})\n        return jsonify({})\n\n\nif __name__ == \'__main__\':\n    init()\n    app.run()\n'"
server/generate.py,6,"b'import numpy as np\n\n\ndef normalize_vector(vector):\n    vector = np.array(vector)\n    vec_norm = np.zeros(vector.shape)\n    d = (np.sum(vector ** 2,) ** (0.5))\n    vec_norm = (vector.T / d).T\n    return vec_norm\n\n\n# XXX todo see if we can unite vector and matrix normalization\ndef normalize_matrix(W):\n    # normalize each word vector to unit variance\n    W_norm = np.zeros(W.shape)\n    d = (np.sum(W ** 2, 1) ** (0.5))\n    W_norm = (W.T / d).T\n    return W_norm\n\n\ndef generate_matrix(word_vectors):\n    """"""given a list of word,vector pairs generate matrix and vocab dicts"""""" \n    vectors = dict(word_vectors)\n    words = [w for w, _ in word_vectors]\n\n    vocab = {w: idx for idx, w in enumerate(words)}\n    ivocab = {idx: w for idx, w in enumerate(words)}\n        \n    vocab_size = len(vectors)\n    vector_dim = len(vectors.values()[0]) if vectors else 0\n    W = np.zeros((vocab_size, vector_dim))\n    for word, v in vectors.items():\n        W[vocab[word], :] = v\n    return W, vocab, ivocab\n'"
server/model.py,0,"b""import json\n\nfrom schematics.models import Model\nfrom schematics.types import StringType, ListType, IntType, FloatType, ModelType, DictType\n\n\nFILENAME = '../database/db.json'\n\n\nclass DB(Model):\n    class ReportedMessage(Model):\n        reports = IntType()\n        vector = ListType(FloatType)\n\n    reported_messages = DictType(ModelType(ReportedMessage), default={})\n    \n    def add_new_message(self, reported_message, vector):\n        rm = self.ReportedMessage()\n        rm.reports = 1\n        rm.vector = vector\n        self.reported_messages[reported_message] = rm\n    \n    @classmethod\n    def load(cls):\n        with open(FILENAME, 'r') as f:\n            return DB(json.loads(f.read()))\n\n    def save(self):\n        string = json.dumps(self.to_primitive())\n        with open(FILENAME, 'w') as f:\n            f.write(string)\n"""
