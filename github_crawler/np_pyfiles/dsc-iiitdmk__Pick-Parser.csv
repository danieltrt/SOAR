file_path,api_count,code
app.py,0,"b'import flask\nfrom generate_files import *\nfrom flask import request\nimport io\n\napp = flask.Flask(__name__)\napp.config[""DEBUG""] = True\n\n\n@app.route(\'/\',methods=[\'GET\'])\ndef home():\n\treturn ""Welcome to CrackHire Resume Parsing Portal.""\n\n@app.route(\'/parse\',methods=[\'GET\'])\ndef parse_resume():\n\tbase_path = request.args.get(\'dir\')\n\tfile_name = base_path+"".pdf""\n\tbase_path = ""resumes/""+base_path\n\tprint(base_path,file_name)\n\tmodel_path = ""model/""\n\ttext_file_name = ""out_text.txt""\n\tsummary_file_name = ""summary_text.txt""\n\tgenerate_summary(base_path,file_name,model_path,text_file_name,summary_file_name)\n\tf = io.open(base_path+""/""+summary_file_name,""r"",encoding=""utf-8"")\n\ttext = f.read()\n\treturn text\n\n\napp.run(host=""127.0.0.1"",port=3001)\n'"
generate_files.py,0,"b'from pdf_to_png import *\nimport spacy\nimport io\n\ndef generate_summary(base_path,resume_file,model_path,text_file_name,summary_file_name):\n\tgenerate_images_and_text_from_pdf(base_path+""/""+resume_file,base_path)\n\tnlp = spacy.load(model_path)\n\tf = io.open(base_path+""/""+text_file_name,""r"",encoding=""utf-8"")\n\ttext = f.read()\n\tfw = io.open(base_path+""/""+summary_file_name,""w"",encoding=""utf-8"")\n\tdoc_to_test = nlp(text)\n\td = {}\n\tfor ent in doc_to_test.ents:\n\t\td[ent.label_]=[]\n\tfor ent in doc_to_test.ents:\n\t\td[ent.label_].append(ent.text)\n\n\tfor i in set(d.keys()):\n\t\tfw.write(""\\n\\n"")\n\t\tfw.write(i +"":""+""\\n"")\n\t\tfor j in set(d[i]):\n\t\t\tfw.write(j.replace(\'\\n\',\'\')+""\\n"")\n\tf.close()\n\tfw.close()\n\n\n#base_path = ""resumes/ashu""\n\n#generate_images_and_text_from_pdf(base_path+""/ashu.pdf"",base_path)\n\n#nlp = spacy.load(""model/"") \n\n#print(""Model : "",nlp)\n\n#f = io.open(base_path+""/out_text.txt"",""r"",encoding=""utf-8"")\n#text = f.read()\n#print(text)\n\n#fw = io.open(base_path+""/summary_text.txt"",""w"",encoding=""utf-8"")\n#doc_to_test=nlp(text)\n#d={}\n#for ent in doc_to_test.ents:\n#    d[ent.label_]=[]\n#for ent in doc_to_test.ents:\n#    d[ent.label_].append(ent.text)\n\n#for i in set(d.keys()):\n\n#    fw.write(""\\n\\n"")\n#    fw.write(i +"":""+""\\n"")\n#    for j in set(d[i]):\n#        fw.write(j.replace(\'\\n\',\'\')+""\\n"")\n\n#f.close()\n#fw.close()\n\n'"
pdf_to_png.py,0,"b'from PIL import Image \nimport pytesseract \nimport sys \nfrom pdf2image import convert_from_path \nimport os \n\n#pytesseract.pytesseract.tesseract_cmd = ""/usr/share/tesseract-ocr/4.00/tessdata""\n\ndef generate_images_and_text_from_pdf(PDF_file,target_dir):\n    pages = convert_from_path(PDF_file, 500)\n    image_counter = 1\n\n    for page in pages:\n        filename = target_dir+""/page_""+str(image_counter)+"".jpg""\n        page.save(filename, \'JPEG\') \n        image_counter = image_counter + 1\n\n    filelimit = image_counter-1\n\n    outfile = target_dir+""/out_text.txt""\n\n    f = open(outfile, ""w"",encoding=""utf-8"") \n\n    for i in range(1, filelimit + 1): \n        filename = target_dir+""/page_""+str(i)+"".jpg""\n        text = str(((pytesseract.image_to_string(Image.open(filename)))))\n        text = text.replace(\'-\\n\', \'\') \n        f.write(text) \n\n    f.close()\n'"
train.py,0,"b'############################################  NOTE  ########################################################\r\n#\r\n#           Creates NER training data in Spacy format from JSON downloaded from Dataturks.\r\n#\r\n#           Outputs the Spacy training data which can be used for Spacy training.\r\n#\r\n############################################################################################################\r\nimport json\r\nimport random\r\nimport re\r\nimport io\r\nimport logging\r\nfrom sklearn.metrics import classification_report\r\nfrom sklearn.metrics import precision_recall_fscore_support\r\nfrom spacy.gold import GoldParse\r\nfrom spacy.scorer import Scorer\r\nfrom spacy.util import minibatch, compounding\r\nfrom sklearn.metrics import accuracy_score\r\n\r\ndef convert_dataturks_to_spacy(dataturks_JSON_FilePath):\r\n    try:\r\n        training_data = []\r\n        lines=[]\r\n        with open(dataturks_JSON_FilePath, \'r\',encoding=""utf8"") as f:\r\n            lines = f.read()\r\n        lines = json.loads(lines)\r\n        for line in lines[\'object\']:\r\n            data = line\r\n            text = data[\'content\']\r\n            entities = []\r\n            for annotation in data[\'annotation\']:\r\n                #only a single point in text annotation.\r\n                point = annotation[\'points\'][0]\r\n                labels = annotation[\'label\']\r\n                # handle both list of labels or a single label.\r\n                if not isinstance(labels, list):\r\n                    labels = [labels]\r\n\r\n                for label in labels:\r\n                    #dataturks indices are both inclusive [start, end] but spacy is not [start, end)\r\n                    entities.append((point[\'start\'], point[\'end\'] + 1 ,label))\r\n\r\n\r\n            training_data.append((text, {""entities"" : entities}))\r\n\r\n        return training_data\r\n    except Exception as e:\r\n        logging.exception(""Unable to process "" + dataturks_JSON_FilePath + ""\\n"" + ""error = "" + str(e))\r\n        return None\r\n\r\ndef trim_entity_spans(data: list) -> list:\r\n    """"""Removes leading and trailing white spaces from entity spans.\r\n\r\n    Args:\r\n        data (list): The data to be cleaned in spaCy JSON format.\r\n\r\n    Returns:\r\n        list: The cleaned data.\r\n    """"""\r\n    invalid_span_tokens = re.compile(r\'\\s\')\r\n\r\n    cleaned_data = []\r\n    for text, annotations in data:\r\n        entities = annotations[\'entities\']\r\n        valid_entities = []\r\n        for start, end, label in entities:\r\n            valid_start = start\r\n            valid_end = end\r\n            while valid_start < len(text) and invalid_span_tokens.match(\r\n                    text[valid_start]):\r\n                valid_start += 1\r\n            while valid_end > 1 and invalid_span_tokens.match(\r\n                    text[valid_end - 1]):\r\n                valid_end -= 1\r\n            valid_entities.append([valid_start, valid_end, label])\r\n        cleaned_data.append([text, {\'entities\': valid_entities}])\r\n\r\n    return cleaned_data\r\n\r\nimport spacy\r\n################### Train Spacy NER.###########\r\ndef train_spacy():\r\n\r\n    TRAIN_DATA = convert_dataturks_to_spacy(""data/traindata.json"")\r\n    nlp = spacy.blank(\'en\')  # create blank Language class\r\n    # create the built-in pipeline components and add them to the pipeline\r\n    # nlp.create_pipe works for built-ins that are registered with spaCy\r\n    if \'ner\' not in nlp.pipe_names:\r\n        ner = nlp.create_pipe(\'ner\')\r\n        nlp.add_pipe(ner, last=True)\r\n\r\n    TRAIN_DATA = trim_entity_spans(TRAIN_DATA)\r\n\r\n    # add labels\r\n    for _, annotations in TRAIN_DATA:\r\n         for ent in annotations.get(\'entities\'):\r\n            ner.add_label(ent[2])\r\n\r\n    # get names of other pipes to disable them during training\r\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \'ner\']\r\n    with nlp.disable_pipes(*other_pipes):  # only train NER\r\n        optimizer = nlp.begin_training()\r\n        for itn in range(10):\r\n            print(""Statring iteration "" + str(itn))\r\n            # random.shuffle(TRAIN_DATA)\r\n            losses = {}\r\n            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\r\n            for batch in batches:\r\n                text, annotations = zip(*batch)\r\n                # print(text,annotations)\r\n            # for text, annotations in TRAIN_DATA:\r\n                nlp.update(\r\n                    text,  # batch of texts\r\n                    annotations,  # batch of annotations\r\n                    drop=0.2,  # dropout - make it harder to memorise data\r\n                    sgd=optimizer,  # callable to update weights\r\n                    losses=losses)\r\n                # print(losses)\r\n            print(losses)\r\n    #test the model and evaluate it\r\n    examples = convert_dataturks_to_spacy(""data/testdata.json"")\r\n    tp=0\r\n    tr=0\r\n    tf=0\r\n\r\n    ta=0\r\n    examples = trim_entity_spans(examples)\r\n    c=0\r\n    for text,annot in examples:\r\n\r\n        f=io.open(""tested_summaries/resume""+str(c)+"".txt"",""w"",encoding=""utf-8"")\r\n        doc_to_test=nlp(text)\r\n        d={}\r\n        for ent in doc_to_test.ents:\r\n            d[ent.label_]=[]\r\n        for ent in doc_to_test.ents:\r\n            d[ent.label_].append(ent.text)\r\n\r\n        for i in set(d.keys()):\r\n\r\n            f.write(""\\n\\n"")\r\n            f.write(i +"":""+""\\n"")\r\n            for j in set(d[i]):\r\n                f.write(j.replace(\'\\n\',\'\')+""\\n"")\r\n        d={}\r\n        for ent in doc_to_test.ents:\r\n            d[ent.label_]=[0,0,0,0,0,0]\r\n        for ent in doc_to_test.ents:\r\n            doc_gold_text= nlp.make_doc(text)\r\n            gold = GoldParse(doc_gold_text, entities=annot.get(""entities""))\r\n            y_true = [ent.label_ if ent.label_ in x else \'Not \'+ent.label_ for x in gold.ner]\r\n            y_pred = [x.ent_type_ if x.ent_type_ ==ent.label_ else \'Not \'+ent.label_ for x in doc_to_test]\r\n            if(d[ent.label_][0]==0):\r\n                #f.write(""For Entity ""+ent.label_+""\\n"")\r\n                #f.write(classification_report(y_true, y_pred)+""\\n"")\r\n                (p,r,f,s)= precision_recall_fscore_support(y_true,y_pred,average=\'weighted\')\r\n                a=accuracy_score(y_true,y_pred)\r\n                d[ent.label_][0]=1\r\n                d[ent.label_][1]+=p\r\n                d[ent.label_][2]+=r\r\n                d[ent.label_][3]+=f\r\n                d[ent.label_][4]+=a\r\n                d[ent.label_][5]+=1\r\n        c+=1\r\n    for i in d:\r\n        print(""\\n For Entity ""+i+""\\n"")\r\n        print(""Accuracy : ""+str((d[i][4]/d[i][5])*100)+""%"")\r\n        print(""Precision : ""+str(d[i][1]/d[i][5]))\r\n        print(""Recall : ""+str(d[i][2]/d[i][5]))\r\n        print(""F-score : ""+str(d[i][3]/d[i][5]))\r\n    nlp.to_disk(""model/"")\r\ntrain_spacy()\r\n'"
