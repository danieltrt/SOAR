file_path,api_count,code
setup.py,0,"b""import io\nimport os\nfrom setuptools import setup\n\nversion_txt = os.path.join(os.path.dirname(__file__), 'aghasher', 'version.txt')\nwith open(version_txt, 'r') as f:\n    version = f.read().strip()\n\nsetup(\n    author='Daniel Steinberg',\n    author_email='ds@dannyadam.com',\n    classifiers=[\n        'Development Status :: 4 - Beta',\n        'Intended Audience :: Developers',\n        'Intended Audience :: Science/Research',\n        'Topic :: Scientific/Engineering',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        'Topic :: Scientific/Engineering :: Information Analysis',\n        'License :: OSI Approved :: MIT License',\n        'Operating System :: Unix',\n        'Operating System :: POSIX :: Linux',\n        'Operating System :: MacOS',\n        'Operating System :: Microsoft :: Windows',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.6'\n    ],\n    description='An implementation of Anchor Graph Hashing',\n    install_requires=['numpy', 'scipy'],\n    keywords=['anchor-graph-hashing', 'hashing', 'locality-sensitive-hashing', 'machine-learning'],\n    license='MIT',\n    long_description=io.open('README.md', encoding='utf8').read(),\n    long_description_content_type='text/markdown',\n    name='aghasher',\n    package_data={'aghasher': ['version.txt']},\n    packages=['aghasher'],\n    url='https://github.com/dstein64/aghasher',\n    version=version,\n)\n"""
aghasher/__init__.py,0,"b""import os\n\nfrom .core import AnchorGraphHasher\n\nversion_txt = os.path.join(os.path.dirname(__file__), 'version.txt')\nwith open(version_txt, 'r') as f:\n    __version__ = f.read().strip()\n"""
aghasher/core.py,21,"b""import numpy as np\nimport scipy.sparse\nimport scipy.linalg\nimport scipy.io\n\nimport aghasher.utils as utils\n\n\nclass AnchorGraphHasher:\n    def __init__(self, W, anchors, nn_anchors, sigma):\n        self.W = W\n        self.anchors = anchors\n        self.nn_anchors = nn_anchors\n        self.sigma = sigma\n\n    @classmethod\n    def train(cls, X, anchors, num_hashbits=12, nn_anchors=2, sigma=None):\n        m = anchors.shape[0]\n        # num_hashbits must be less than num anchors because we get m-1\n        # eigenvalues from an m-by-m matrix.\n        # (m-1 since we omit eigenvalue=1)\n        if num_hashbits >= m:\n            valerr = (\n                'The number of hash bits ({}) must be less than the number of '\n                'anchors ({}).'\n            ).format(num_hashbits, m)\n            raise ValueError(valerr)\n        Z, sigma = cls._Z(X, anchors, nn_anchors, sigma)\n        W = cls._W(Z, num_hashbits)\n        H = cls._hash(Z, W)\n        agh = cls(W, anchors, nn_anchors, sigma)\n        return agh, H\n\n    def hash(self, X):\n        Z, _ = self._Z(X, self.anchors, self.nn_anchors, self.sigma)\n        return self._hash(Z, self.W)\n\n    @staticmethod\n    def _hash(Z, W):\n        H = Z.dot(W)\n        return H > 0\n\n    @staticmethod\n    def test(H_train, H_test, y_train, y_test, radius=2):\n        # Flatten arrays\n        y_test = y_test.ravel()\n        y_train = y_train.ravel()\n        ntest = H_test.shape[0]\n\n        hamdis = utils.pdist2(H_train, H_test, 'hamming')\n\n        precision = np.zeros(ntest)\n        for j in range(ntest):\n            ham = hamdis[:, j]\n            lst = np.flatnonzero(ham <= radius)\n            ln = len(lst)\n            if ln == 0:\n                precision[j] = 0\n            else:\n                numerator = len(np.flatnonzero(y_train[lst] == y_test[j]))\n                precision[j] = numerator / float(ln)\n\n        return np.mean(precision)\n\n    @staticmethod\n    def _W(Z, num_hashbits):\n        # The extra steps here are for compatibility with sparse matrices.\n        s = np.asarray(Z.sum(0)).ravel()\n        isrl = np.diag(np.power(s, -0.5))  # isrl = inverse square root of lambda\n        ztz = Z.T.dot(Z)  # ztz = Z transpose Z\n        if scipy.sparse.issparse(ztz):\n            ztz = ztz.todense()\n        M = np.dot(isrl, np.dot(ztz, isrl))\n        eigenvalues, V = scipy.linalg.eig(M)  # there is also a numpy.linalg.eig\n        I = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[I]\n        V = V[:, I]\n\n        # This is also essentially what they do in the matlab reference, since a check for\n        # equality to 1 doesn't work because of floating point precision.\n        if eigenvalues[0] > 0.99999999:\n            eigenvalues = eigenvalues[1:]\n            V = V[:, 1:]\n        eigenvalues = eigenvalues[0:num_hashbits]\n        V = V[:, 0:num_hashbits]\n        # The paper also multiplies by sqrt(n), but their matlab reference code doesn't.\n        # It isn't necessary.\n\n        W = np.dot(isrl, np.dot(V, np.diag(np.power(eigenvalues, -0.5))))\n        return W\n\n    @staticmethod\n    def _Z(X, anchors, nn_anchors, sigma):\n        n = X.shape[0]\n        m = anchors.shape[0]\n\n        sqdist = utils.pdist2(X, anchors, 'sqeuclidean')\n        val = np.zeros((n, nn_anchors))\n        pos = np.zeros((n, nn_anchors), dtype=np.int)\n        for i in range(nn_anchors):\n            pos[:, i] = np.argmin(sqdist, 1)\n            val[:, i] = sqdist[np.arange(len(sqdist)), pos[:, i]]\n            sqdist[np.arange(n), pos[:, i]] = float('inf')\n\n        if sigma is None:\n            dist = np.sqrt(val[:, nn_anchors - 1])\n            sigma = np.mean(dist) / np.sqrt(2)\n\n        # Calculate formula (2) from the paper. This calculation differs from the reference matlab.\n        # In the matlab, the RBF kernel's exponent only has sigma^2 in the denominator. Here, 2 * sigma^2.\n        # This is accounted for when auto-calculating sigma above by dividing by sqrt(2).\n\n        # Work in log space and then exponentiate, to avoid the floating point issues. For the\n        # denominator, the following code avoids even more precision issues, by relying on the fact that\n        # the log of the sum of exponentials, equals some constant plus the log of sum of exponentials\n        # of numbers subtracted by the constant:\n        #  log(sum_i(exp(x_i))) = m + log(sum_i(exp(x_i-m)))\n\n        c = 2 * np.power(sigma, 2)  # bandwidth parameter\n        exponent = -val / c  # exponent of RBF kernel\n        shift = np.amin(exponent, 1, keepdims=True)\n        denom = np.log(np.sum(np.exp(exponent - shift), 1, keepdims=True)) + shift\n        val = np.exp(exponent - denom)\n\n        Z = scipy.sparse.lil_matrix((n, m))\n        for i in range(nn_anchors):\n            Z[np.arange(n), pos[:, i]] = val[:, i]\n        Z = scipy.sparse.csr_matrix(Z)\n\n        return Z, sigma\n"""
aghasher/utils.py,5,"b""import os\n\nimport numpy as np\n\n\ndef pdist2(X, Y, metric):\n    # scipy has a cdist function that works like matlab's pdist2 function.\n    # For square euclidean distance it is slow for the version of scipy you have.\n    # For details on its slowness, see https://github.com/scipy/scipy/issues/3251\n    # In your tests, it took over 16 seconds versus less than 4 seconds for the\n    # implementation below (where X has 69,000 elements and Y had 300).\n    # (this has squared Euclidean distances).\n    metric = metric.lower()\n    if metric == 'sqeuclidean':\n        X = X.astype('float64')\n        Y = Y.astype('float64')\n        nx = X.shape[0]\n        ny = Y.shape[0]\n        XX = np.tile((X ** 2).sum(1), (ny, 1)).T\n        YY = np.tile((Y ** 2).sum(1), (nx, 1))\n        XY = X.dot(Y.T)\n        sqeuc = XX + YY - 2 * XY\n        # Make negatives equal to zero. This arises due to floating point\n        # precision issues. Negatives will be very close to zero (IIRC around\n        # -1e-10 or maybe even closer to zero). Any better fix? you exhibited the\n        # floating point issue on two machines using the same code and data,\n        # but not on a third. the inconsistent occurrence of the issue could\n        # possibly be due to differences in numpy/blas versions across machines.\n        return np.clip(sqeuc, 0, np.inf)\n    elif metric == 'hamming':\n        # scipy cdist supports hamming distance, but is twice as slow as yours\n        # (even before multiplying by dim, and casting as int), possibly because\n        # it supports non-booleans, but I'm not sure...\n        # Looping over data points in X and Y, and calculating hamming distance\n        # to put in a hamdis matrix is too slow. This vectorized solution works\n        # faster.\n        hashbits = X.shape[1]\n        # Use high bitwidth int to prevent overflow (i.e., as opposed to int8\n        # which could result in overflow when hashbits >= 64).\n        X_int = (2 * X.astype('int')) - 1\n        Y_int = (2 * Y.astype('int')) - 1\n        hamdis = hashbits - ((hashbits + X_int.dot(Y_int.T)) / 2)\n        return hamdis\n    else:\n        valerr = 'Unsupported Metric: %s' % (metric,)\n        raise ValueError(valerr)\n\n\ndef standardize(X):\n    # Assumes columns contain variables/features, and rows contain\n    # observations/instances.\n    means = np.mean(X, 0, keepdims=True)\n    stds = np.std(X, 0, keepdims=True)\n    return (X - means) / stds\n"""
tests/test_aghasher.py,1,"b""import os\nimport unittest\n\nimport numpy as np\n\nfrom aghasher import AnchorGraphHasher\n\n\nclass TestAghasher(unittest.TestCase):\n    def test_aghasher(self):\n        with np.load(os.path.join(os.path.dirname(__file__), 'data.npz')) as data:\n            X_train = data['X_train']\n            X_test = data['X_test']\n            y_train = data['y_train']\n            y_test = data['y_test']\n            anchors = data['anchors']\n\n        radius = 2  # hamming radius 2 precision\n        sigma = None  # sigma None means sigma auto-calculated\n        nn_anchors = 2  # number-of-nearest anchors\n\n        # Maps number of bits for embedding to the expected precision.\n        expected_precision_lookup = {\n            12: 0.6990520452891523,\n            16: 0.7785310451160524,\n            24: 0.8551476847057504,\n            32: 0.8735075625522515,\n            48: 0.8769409352078942,\n            64: 0.8767468477596928,\n        }\n\n        for num_bits, expected_precision in expected_precision_lookup.items():\n            agh, H_train = AnchorGraphHasher.train(\n                X_train, anchors, num_bits, nn_anchors, sigma)\n            H_test = agh.hash(X_test)\n            precision = AnchorGraphHasher.test(\n                H_train, H_test, y_train, y_test, radius)\n            self.assertAlmostEqual(precision, expected_precision)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
