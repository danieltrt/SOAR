file_path,api_count,code
Config/augmentation_options.py,2,"b'augmentations = {\n    ""meta"": [\n        {\n            ""no"": 1,\n            ""augmentation"": """"""iaa.Sequential([iaa.Affine(translate_px={""x"": -40}),\n                                        iaa.AdditiveGaussianNoise(scale=0.1*255)])"""""",\n        },\n        {\n            ""no"": 2,\n            ""augmentation"": """"""iaa.Sequential([iaa.Affine(translate_px={""x"": -40}),\n                                        iaa.AdditiveGaussianNoise(scale=0.1*255)], random_order=True)"""""",\n        },\n        {\n            ""no"": 3,\n            ""augmentation"": """"""iaa.SomeOf(2, [iaa.Affine(rotate=45), iaa.AdditiveGaussianNoise(scale=0.2*255),\n                                       iaa.Add(50, per_channel=True), iaa.Sharpen(alpha=0.5)])"""""",\n        },\n        {\n            ""no"": 4,\n            ""augmentation"": """"""iaa.SomeOf((0, None), [iaa.Affine(rotate=45), iaa.AdditiveGaussianNoise(scale=0.2*255),\n                                               iaa.Add(50, per_channel=True), iaa.Sharpen(alpha=0.5)])"""""",\n        },\n        {\n            ""no"": 5,\n            ""augmentation"": """"""iaa.SomeOf(2, [iaa.Affine(rotate=45), iaa.AdditiveGaussianNoise(scale=0.2*255),\n                                       iaa.Add(50, per_channel=True), iaa.Sharpen(alpha=0.5)],\n                                   random_order=True)"""""",\n        },\n        {\n            ""no"": 6,\n            ""augmentation"": """"""iaa.OneOf([iaa.Affine(rotate=45), iaa.AdditiveGaussianNoise(scale=0.2*255),\n                                   iaa.Add(50, per_channel=True), iaa.Sharpen(alpha=0.5)])"""""",\n        },\n        {\n            ""no"": 7,\n            ""augmentation"": """"""iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=2.0))"""""",\n        },\n        {\n            ""no"": 8,\n            ""augmentation"": """"""iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=2.0), iaa.Sequential(\n            [iaa.Affine(rotate=45), iaa.Sharpen(alpha=1.0)]))"""""",\n        },\n        {\n            ""no"": 9,\n            ""augmentation"": """"""iaa.WithChannels(0, iaa.Add((10, 100)))"""""",\n        },\n        {\n            ""no"": 10,\n            ""augmentation"": """"""iaa.WithChannels(0, iaa.Affine(rotate=(0, 45)))"""""",\n        },\n        {""no"": 11, ""augmentation"": """"""iaa.Identity()""""""},\n        {""no"": 12, ""augmentation"": """"""iaa.Noop()""""""},\n        {\n            ""no"": 13,\n            ""augmentation"": """"""iaa.Sequential([iaa.AssertShape((None, 32, 32, 3)), iaa.Fliplr(0.5)])"""""",\n        },\n        {\n            ""no"": 14,\n            ""augmentation"": """"""iaa.Sequential([iaa.AssertShape((None, (32, 64), 32, [1, 3])), iaa.Fliplr(0.5)])"""""",\n        },\n        {""no"": 15, ""augmentation"": """"""iaa.ChannelShuffle(0.35)""""""},\n        {\n            ""no"": 16,\n            ""augmentation"": """"""iaa.ChannelShuffle(0.35, channels=[0, 1])"""""",\n        },\n        {\n            ""no"": 17,\n            ""augmentation"": """"""iaa.Sequential([iaa.Affine(translate_px={""x"": (-100, 100)}),\n                                        iaa.RemoveCBAsByOutOfImageFraction(0.5)])"""""",\n        },\n        {\n            ""no"": 18,\n            ""augmentation"": """"""iaa.Sequential([iaa.Affine(translate_px={""x"": (-100, 100)}),\n                                        iaa.ClipCBAsToImagePlanes()])"""""",\n        },\n    ],\n    ""arithmetic"": [\n        {""no"": 1, ""augmentation"": """"""iaa.Add((-40, 40))""""""},\n        {""no"": 2, ""augmentation"": """"""iaa.Add((-40, 40), per_channel=0.5)""""""},\n        {""no"": 3, ""augmentation"": """"""iaa.AddElementwise((-40, 40))""""""},\n        {\n            ""no"": 4,\n            ""augmentation"": """"""iaa.AddElementwise((-40, 40), per_channel=0.5)"""""",\n        },\n        {\n            ""no"": 5,\n            ""augmentation"": """"""iaa.AdditiveGaussianNoise(scale=(0, 0.2 * 255))"""""",\n        },\n        {\n            ""no"": 6,\n            ""augmentation"": """"""iaa.AdditiveGaussianNoise(scale=0.2 * 255)"""""",\n        },\n        {\n            ""no"": 7,\n            ""augmentation"": """"""iaa.AdditiveGaussianNoise(scale=0.2 * 255, per_channel=True)"""""",\n        },\n        {\n            ""no"": 8,\n            ""augmentation"": """"""iaa.AdditiveLaplaceNoise(scale=(0, 0.2 * 255))"""""",\n        },\n        {\n            ""no"": 9,\n            ""augmentation"": """"""iaa.AdditiveLaplaceNoise(scale=0.2 * 255)"""""",\n        },\n        {\n            ""no"": 10,\n            ""augmentation"": """"""iaa.AdditiveLaplaceNoise(scale=0.2 * 255, per_channel=True)"""""",\n        },\n        {""no"": 11, ""augmentation"": """"""iaa.AdditivePoissonNoise(40)""""""},\n        {""no"": 12, ""augmentation"": """"""iaa.AdditivePoissonNoise(12)""""""},\n        {""no"": 13, ""augmentation"": """"""iaa.Multiply((0.5, 1.5))""""""},\n        {\n            ""no"": 14,\n            ""augmentation"": """"""iaa.Multiply((0.5, 1.5), per_channel=0.5)"""""",\n        },\n        {""no"": 15, ""augmentation"": """"""iaa.MultiplyElementwise((0.5, 1.5))""""""},\n        {\n            ""no"": 16,\n            ""augmentation"": """"""iaa.MultiplyElementwise((0.5, 1.5), per_channel=0.5)"""""",\n        },\n        {""no"": 17, ""augmentation"": """"""iaa.Cutout(nb_iterations=2)""""""},\n        {\n            ""no"": 18,\n            ""augmentation"": """"""iaa.Cutout(nb_iterations=(1, 5), size=0.2, squared=False)"""""",\n        },\n        {\n            ""no"": 19,\n            ""augmentation"": """"""iaa.Cutout(fill_mode=""constant"", cval=255)"""""",\n        },\n        {\n            ""no"": 20,\n            ""augmentation"": """"""iaa.Cutout(fill_mode=""constant"", cval=(0, 255), fill_per_channel=0.5)"""""",\n        },\n        {\n            ""no"": 21,\n            ""augmentation"": """"""iaa.Cutout(fill_mode=""gaussian"", fill_per_channel=True)"""""",\n        },\n        {""no"": 22, ""augmentation"": """"""iaa.Dropout(p=(0, 0.2))""""""},\n        {\n            ""no"": 23,\n            ""augmentation"": """"""iaa.Dropout(p=(0, 0.2), per_channel=0.5)"""""",\n        },\n        {\n            ""no"": 24,\n            ""augmentation"": """"""iaa.CoarseDropout(0.02, size_percent=0.5)"""""",\n        },\n        {\n            ""no"": 25,\n            ""augmentation"": """"""iaa.CoarseDropout((0.0, 0.05), size_percent=(0.02, 0.25))"""""",\n        },\n        {\n            ""no"": 26,\n            ""augmentation"": """"""iaa.CoarseDropout(0.02, size_percent=0.15, per_channel=0.5)"""""",\n        },\n        {""no"": 27, ""augmentation"": """"""iaa.Dropout2d(p=0.5)""""""},\n        {\n            ""no"": 28,\n            ""augmentation"": """"""iaa.Dropout2d(p=0.5, nb_keep_channels=0)"""""",\n        },\n        {\n            ""no"": 29,\n            ""augmentation"": """"""iaa.ReplaceElementwise(0.1, [0, 255])"""""",\n        },\n        {\n            ""no"": 30,\n            ""augmentation"": """"""iaa.ReplaceElementwise(0.1, [0, 255], per_channel=0.5)"""""",\n        },\n        {\n            ""no"": 31,\n            ""augmentation"": """"""iaa.ReplaceElementwise(0.1, iap.Normal(128, 0.4 * 128), per_channel=0.5)"""""",\n        },\n        {\n            ""no"": 32,\n            ""augmentation"": """"""iaa.ReplaceElementwise(iap.FromLowerResolution(iap.Binomial(0.1), size_px=8),\n                             iap.Normal(128, 0.4 * 128), per_channel=0.5)"""""",\n        },\n        {""no"": 33, ""augmentation"": """"""iaa.SaltAndPepper(0.1)""""""},\n        {\n            ""no"": 34,\n            ""augmentation"": """"""iaa.SaltAndPepper(0.1, per_channel=True)"""""",\n        },\n        {\n            ""no"": 35,\n            ""augmentation"": """"""iaa.CoarseSaltAndPepper(0.05, size_percent=(0.01, 0.1))"""""",\n        },\n        {\n            ""no"": 36,\n            ""augmentation"": """"""iaa.CoarseSaltAndPepper(0.05, size_percent=(0.01, 0.1), per_channel=True)"""""",\n        },\n        {""no"": 37, ""augmentation"": """"""iaa.Pepper(0.1)""""""},\n        {""no"": 38, ""augmentation"": """"""iaa.Invert(0.5)""""""},\n        {""no"": 39, ""augmentation"": """"""iaa.Invert(0.25, per_channel=0.5)""""""},\n        {\n            ""no"": 40,\n            ""augmentation"": """"""iaa.Solarize(0.5, threshold=(32, 128))"""""",\n        },\n        {\n            ""no"": 41,\n            ""augmentation"": """"""iaa.JpegCompression(compression=(70, 99))"""""",\n        },\n    ],\n    ""artistic"": [\n        {""no"": 1, ""augmentation"": """"""iaa.Cartoon()""""""},\n        {\n            ""no"": 2,\n            ""augmentation"": """"""iaa.Cartoon(blur_ksize=3, segmentation_size=1.0,\n                                    saturation=2.0, edge_prevalence=1.0)"""""",\n        },\n    ],\n    ""blend"": [\n        {\n            ""no"": 1,\n            ""augmentation"": """"""iaa.BlendAlpha(0, iaa.Affine(rotate=(-20, 20)))"""""",\n        },\n        {\n            ""no"": 2,\n            ""augmentation"": """"""iaa.BlendAlpha((0.0, 1.0), foreground=iaa.Add(100), background=iaa.Multiply(0.2))"""""",\n        },\n        {\n            ""no"": 3,\n            ""augmentation"": """"""iaa.BlendAlpha([0.25, 0.75], iaa.MedianBlur(13))"""""",\n        },\n        {\n            ""no"": 4,\n            ""augmentation"": """"""iaa.BlendAlphaMask(\n            iaa.InvertMaskGen(0.5, iaa.VerticalLinearGradientMaskGen()), iaa.Clouds())"""""",\n        },\n        {\n            ""no"": 5,\n            ""augmentation"": """"""iaa.BlendAlphaElementwise(0.5, iaa.Grayscale(1.0))"""""",\n        },\n        {\n            ""no"": 6,\n            ""augmentation"": """"""iaa.BlendAlphaElementwise((0, 1.0), iaa.AddToHue(100))"""""",\n        },\n        {\n            ""no"": 7,\n            ""augmentation"": """"""iaa.BlendAlphaElementwise(\n            (0.0, 1.0), iaa.Affine(rotate=(-20, 20)), per_channel=0.5)"""""",\n        },\n        {\n            ""no"": 8,\n            ""augmentation"": """"""iaa.BlendAlphaElementwise(\n            (0.0, 1.0), foreground=iaa.Add(100), background=iaa.Multiply(0.2))"""""",\n        },\n        {\n            ""no"": 9,\n            ""augmentation"": """"""iaa.BlendAlphaElementwise([0.25, 0.75], iaa.MedianBlur(13))"""""",\n        },\n        {\n            ""no"": 10,\n            ""augmentation"": """"""iaa.BlendAlphaSimplexNoise(iaa.EdgeDetect(1.0))"""""",\n        },\n        {\n            ""no"": 11,\n            ""augmentation"": """"""iaa.BlendAlphaSimplexNoise(\n            iaa.EdgeDetect(1.0), upscale_method=""nearest"")"""""",\n        },\n        {\n            ""no"": 12,\n            ""augmentation"": """"""iaa.BlendAlphaSimplexNoise(\n            iaa.EdgeDetect(1.0), upscale_method=""linear"")"""""",\n        },\n        {\n            ""no"": 13,\n            ""augmentation"": """"""iaa.BlendAlphaSimplexNoise(\n            iaa.EdgeDetect(1.0), sigmoid_thresh=iap.Normal(10.0, 5.0))"""""",\n        },\n        {\n            ""no"": 14,\n            ""augmentation"": """"""iaa.BlendAlphaFrequencyNoise(\n            upscale_method=""linear"", exponent=-2, sigmoid=False)"""""",\n        },\n        {\n            ""no"": 15,\n            ""augmentation"": """"""iaa.BlendAlphaFrequencyNoise(sigmoid_thresh=iap.Normal(10.0, 5.0))"""""",\n        },\n        {\n            ""no"": 16,\n            ""augmentation"": """"""iaa.BlendAlphaSomeColors(iaa.Grayscale(1.0))"""""",\n        },\n        {\n            ""no"": 17,\n            ""augmentation"": """"""iaa.BlendAlphaSomeColors(iaa.TotalDropout(1.0))"""""",\n        },\n        {\n            ""no"": 18,\n            ""augmentation"": """"""iaa.BlendAlphaSomeColors(\n            iaa.MultiplySaturation(0.5), iaa.MultiplySaturation(1.5))"""""",\n        },\n        {\n            ""no"": 19,\n            ""augmentation"": """"""iaa.BlendAlphaSomeColors(\n            iaa.AveragePooling(7), alpha=[0.0, 1.0], smoothness=0.0)"""""",\n        },\n        {\n            ""no"": 20,\n            ""augmentation"": """"""iaa.BlendAlphaSomeColors(\n            iaa.AveragePooling(7), nb_bins=2, smoothness=0.0)"""""",\n        },\n        {\n            ""no"": 21,\n            ""augmentation"": """"""iaa.BlendAlphaSomeColors(\n            iaa.AveragePooling(7), from_colorspace=""BGR"")"""""",\n        },\n        {\n            ""no"": 22,\n            ""augmentation"": """"""iaa.BlendAlphaHorizontalLinearGradient(\n            iaa.AddToHue((-100, 100)))"""""",\n        },\n        {\n            ""no"": 23,\n            ""augmentation"": """"""iaa.BlendAlphaHorizontalLinearGradient(\n            iaa.TotalDropout(1.0), min_value=0.2, max_value=0.8)"""""",\n        },\n        {\n            ""no"": 24,\n            ""augmentation"": """"""iaa.BlendAlphaHorizontalLinearGradient(iaa.AveragePooling(11), start_at=(0.0, 1.0),\n                                                               end_at=(0.0, 1.0))"""""",\n        },\n        {\n            ""no"": 25,\n            ""augmentation"": """"""iaa.BlendAlphaVerticalLinearGradient(iaa.AddToHue((-100, 100)))"""""",\n        },\n        {\n            ""no"": 26,\n            ""augmentation"": """"""a26 = iaa.BlendAlphaVerticalLinearGradient(\n            iaa.TotalDropout(1.0), min_value=0.2, max_value=0.8)"""""",\n        },\n        {\n            ""no"": 27,\n            ""augmentation"": """"""iaa.BlendAlphaVerticalLinearGradient(iaa.AveragePooling(11), \n                                           start_at=(0.0, 1.0), end_at=(0.0, 1.0))"""""",\n        },\n        {\n            ""no"": 28,\n            ""augmentation"": """"""iaa.BlendAlphaVerticalLinearGradient(\n            iaa.Clouds(), start_at=(0.15, 0.35), end_at=0.0)"""""",\n        },\n        {\n            ""no"": 29,\n            ""augmentation"": """"""iaa.BlendAlphaRegularGrid(\n            nb_rows=(4, 6), nb_cols=(1, 4), foreground=iaa.Multiply(0.0))"""""",\n        },\n        {\n            ""no"": 30,\n            ""augmentation"": """"""iaa.BlendAlphaRegularGrid(nb_rows=2, nb_cols=2, foreground=iaa.Multiply(0.0),\n                                background=iaa.AveragePooling(8), alpha=[0.0, 0.0, 1.0])"""""",\n        },\n        {\n            ""no"": 31,\n            ""augmentation"": """"""iaa.BlendAlphaCheckerboard(nb_rows=2, nb_cols=(1, 4), \n                                 foreground=iaa.AddToHue((-100, 100)))"""""",\n        },\n        {\n            ""no"": 32,\n            ""augmentation"": """"""iaa.BlendAlphaBoundingBoxes(""person"", foreground=iaa.Grayscale(1.0))"""""",\n        },\n        {\n            ""no"": 33,\n            ""augmentation"": """"""iaa.BlendAlphaBoundingBoxes([""person"", ""car""], \n        foreground=iaa.AddToHue((-255, 255)))"""""",\n        },\n        {\n            ""no"": 34,\n            ""augmentation"": """"""iaa.BlendAlphaBoundingBoxes(\n            [""person"", ""car""], foreground=iaa.AddToHue((-255, 255)), nb_sample_labels=1)"""""",\n        },\n    ],\n    ""gaussian_blur"": [\n        {""no"": 1, ""augmentation"": """"""iaa.AverageBlur(k=(2, 11))""""""},\n        {""no"": 2, ""augmentation"": """"""iaa.AverageBlur(k=((5, 11), (1, 3)))""""""},\n        {""no"": 3, ""augmentation"": """"""iaa.MedianBlur(k=(3, 11))""""""},\n        {\n            ""no"": 4,\n            ""augmentation"": """"""iaa.BilateralBlur(d=(3, 10), sigma_color=(10, 250), \n                            sigma_space=(10, 250))"""""",\n        },\n        {""no"": 5, ""augmentation"": """"""iaa.MotionBlur(k=15)""""""},\n        {""no"": 6, ""augmentation"": """"""iaa.MotionBlur(k=15, angle=[-45, 45])""""""},\n        {""no"": 7, ""augmentation"": """"""iaa.MeanShiftBlur()""""""},\n    ],\n    ""color"": [\n        {\n            ""no"": 1,\n            ""augmentation"": """"""iaa.WithColorspace(\n            to_colorspace=""HSV"", from_colorspace=""RGB"", children=iaa.WithChannels(\n                0, iaa.Add((0, 50))))"""""",\n        },\n        {\n            ""no"": 2,\n            ""augmentation"": """"""iaa.WithBrightnessChannels(iaa.Add((-50, 50)))"""""",\n        },\n        {\n            ""no"": 3,\n            ""augmentation"": """"""iaa.WithBrightnessChannels(iaa.Add((-50, 50)), to_colorspace=[\n            iaa.CSPACE_Lab, iaa.CSPACE_HSV])"""""",\n        },\n        {\n            ""no"": 4,\n            ""augmentation"": """"""iaa.WithBrightnessChannels(iaa.Add((-50, 50)), from_colorspace=iaa.CSPACE_BGR)"""""",\n        },\n        {\n            ""no"": 5,\n            ""augmentation"": """"""iaa.MultiplyAndAddToBrightness(mul=(0.5, 1.5), add=(-30, 30))"""""",\n        },\n        {""no"": 6, ""augmentation"": """"""iaa.MultiplyBrightness((0.5, 1.5))""""""},\n        {""no"": 7, ""augmentation"": """"""iaa.AddToBrightness((-30, 30))""""""},\n        {\n            ""no"": 8,\n            ""augmentation"": """"""iaa.WithHueAndSaturation(iaa.WithChannels(0, iaa.Add((0, 50))))"""""",\n        },\n        {\n            ""no"": 9,\n            ""augmentation"": """"""iaa.WithHueAndSaturation(\n            [iaa.WithChannels(0, iaa.Add((-30, 10))), iaa.WithChannels(1, [\n                iaa.Multiply((0.5, 1.5)), iaa.LinearContrast((0.75, 1.25))])])"""""",\n        },\n        {\n            ""no"": 10,\n            ""augmentation"": """"""iaa.MultiplyHueAndSaturation((0.5, 1.5), per_channel=True)"""""",\n        },\n        {\n            ""no"": 11,\n            ""augmentation"": """"""iaa.MultiplyHueAndSaturation(mul_hue=(0.5, 1.5))"""""",\n        },\n        {\n            ""no"": 12,\n            ""augmentation"": """"""iaa.MultiplyHueAndSaturation(mul_saturation=(0.5, 1.5))"""""",\n        },\n        {""no"": 13, ""augmentation"": """"""iaa.MultiplyHue((0.5, 1.5))""""""},\n        {""no"": 14, ""augmentation"": """"""iaa.MultiplySaturation((0.5, 1.5))""""""},\n        {""no"": 15, ""augmentation"": """"""iaa.RemoveSaturation()""""""},\n        {""no"": 16, ""augmentation"": """"""iaa.RemoveSaturation(1.0)""""""},\n        {\n            ""no"": 17,\n            ""augmentation"": """"""iaa.RemoveSaturation(from_colorspace=iaa.CSPACE_BGR)"""""",\n        },\n        {\n            ""no"": 18,\n            ""augmentation"": """"""iaa.AddToHueAndSaturation((-50, 50), per_channel=True)"""""",\n        },\n        {""no"": 19, ""augmentation"": """"""iaa.AddToHue((-50, 50))""""""},\n        {""no"": 20, ""augmentation"": """"""iaa.AddToSaturation((-50, 50))""""""},\n        {\n            ""no"": 21,\n            ""augmentation"": """"""iaa.Sequential([iaa.ChangeColorspace(\n            from_colorspace=""RGB"", to_colorspace=""HSV""), iaa.WithChannels(\n            0, iaa.Add((50, 100))), iaa.ChangeColorspace(from_colorspace=""HSV"", to_colorspace=""RGB"")])"""""",\n        },\n        {""no"": 22, ""augmentation"": """"""iaa.Grayscale(alpha=(0.0, 1.0))""""""},\n        {\n            ""no"": 23,\n            ""augmentation"": """"""iaa.ChangeColorTemperature((1100, 10000))"""""",\n        },\n        {""no"": 24, ""augmentation"": """"""iaa.KMeansColorQuantization()""""""},\n        {\n            ""no"": 25,\n            ""augmentation"": """"""iaa.KMeansColorQuantization(n_colors=8)"""""",\n        },\n        {\n            ""no"": 26,\n            ""augmentation"": """"""iaa.KMeansColorQuantization(n_colors=(4, 16))"""""",\n        },\n        {\n            ""no"": 27,\n            ""augmentation"": """"""iaa.KMeansColorQuantization(from_colorspace=iaa.ChangeColorspace.BGR)"""""",\n        },\n        {\n            ""no"": 28,\n            ""augmentation"": """"""iaa.KMeansColorQuantization(\n            to_colorspace=[iaa.ChangeColorspace.RGB, iaa.ChangeColorspace.HSV])"""""",\n        },\n        {""no"": 29, ""augmentation"": """"""iaa.UniformColorQuantization()""""""},\n        {\n            ""no"": 30,\n            ""augmentation"": """"""iaa.UniformColorQuantization(n_colors=8)"""""",\n        },\n        {\n            ""no"": 31,\n            ""augmentation"": """"""iaa.UniformColorQuantization(n_colors=(4, 16))"""""",\n        },\n        {\n            ""no"": 32,\n            ""augmentation"": """"""iaa.UniformColorQuantization(\n            from_colorspace=iaa.ChangeColorspace.BGR, to_colorspace=[\n                iaa.ChangeColorspace.RGB, iaa.ChangeColorspace.HSV])"""""",\n        },\n        {\n            ""no"": 33,\n            ""augmentation"": """"""iaa.UniformColorQuantizationToNBits()"""""",\n        },\n        {\n            ""no"": 34,\n            ""augmentation"": """"""iaa.UniformColorQuantizationToNBits(nb_bits=(2, 8))"""""",\n        },\n        {\n            ""no"": 35,\n            ""augmentation"": """"""iaa.UniformColorQuantizationToNBits(\n            from_colorspace=iaa.CSPACE_BGR, to_colorspace=[\n                iaa.CSPACE_RGB, iaa.CSPACE_HSV])"""""",\n        },\n    ],\n    ""contrast"": [\n        {""no"": 1, ""augmentation"": """"""iaa.GammaContrast((0.5, 2.0))""""""},\n        {\n            ""no"": 2,\n            ""augmentation"": """"""iaa.GammaContrast((0.5, 2.0), per_channel=True)"""""",\n        },\n        {\n            ""no"": 3,\n            ""augmentation"": """"""iaa.SigmoidContrast(gain=(3, 10), cutoff=(0.4, 0.6))"""""",\n        },\n        {\n            ""no"": 4,\n            ""augmentation"": """"""iaa.SigmoidContrast(gain=(3, 10), cutoff=(0.4, 0.6), per_channel=True)"""""",\n        },\n        {""no"": 5, ""augmentation"": """"""iaa.LogContrast(gain=(0.6, 1.4))""""""},\n        {\n            ""no"": 6,\n            ""augmentation"": """"""iaa.LogContrast(gain=(0.6, 1.4), per_channel=True)"""""",\n        },\n        {""no"": 7, ""augmentation"": """"""iaa.LinearContrast((0.4, 1.6))""""""},\n        {\n            ""no"": 8,\n            ""augmentation"": """"""iaa.LinearContrast((0.4, 1.6), per_channel=True)"""""",\n        },\n        {""no"": 9, ""augmentation"": """"""iaa.AllChannelsCLAHE()""""""},\n        {\n            ""no"": 10,\n            ""augmentation"": """"""iaa.AllChannelsCLAHE(clip_limit=(1, 10))"""""",\n        },\n        {\n            ""no"": 11,\n            ""augmentation"": """"""iaa.AllChannelsCLAHE(clip_limit=(1, 10), per_channel=True)"""""",\n        },\n        {""no"": 12, ""augmentation"": """"""iaa.CLAHE()""""""},\n        {""no"": 13, ""augmentation"": """"""iaa.CLAHE(clip_limit=(1, 10))""""""},\n        {""no"": 14, ""augmentation"": """"""iaa.CLAHE(tile_grid_size_px=(3, 21))""""""},\n        {\n            ""no"": 15,\n            ""augmentation"": """"""iaa.CLAHE(tile_grid_size_px=iap.Discretize(\n            iap.Normal(loc=7, scale=2)), tile_grid_size_px_min=3)"""""",\n        },\n        {\n            ""no"": 16,\n            ""augmentation"": """"""iaa.CLAHE(tile_grid_size_px=((3, 21), [3, 5, 7]))"""""",\n        },\n        {\n            ""no"": 17,\n            ""augmentation"": """"""iaa.CLAHE(from_colorspace=iaa.CLAHE.BGR, to_colorspace=iaa.CLAHE.HSV)"""""",\n        },\n        {\n            ""no"": 18,\n            ""augmentation"": """"""iaa.AllChannelsHistogramEqualization()"""""",\n        },\n        {\n            ""no"": 19,\n            ""augmentation"": """"""iaa.Alpha((0.0, 1.0), iaa.AllChannelsHistogramEqualization())"""""",\n        },\n        {""no"": 20, ""augmentation"": """"""iaa.HistogramEqualization()""""""},\n        {\n            ""no"": 21,\n            ""augmentation"": """"""iaa.Alpha((0.0, 1.0), iaa.HistogramEqualization())"""""",\n        },\n        {\n            ""no"": 22,\n            ""augmentation"": """"""iaa.HistogramEqualization(\n            from_colorspace=iaa.HistogramEqualization.BGR, \n            to_colorspace=iaa.HistogramEqualization.HSV)"""""",\n        },\n    ],\n    ""convolution"": [\n        {\n            ""no"": 1,\n            ""augmentation"": """"""iaa.Convolve(matrix=np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]]))"""""",\n        },\n        {\n            ""no"": 2,\n            ""augmentation"": """"""iaa.Convolve(matrix=np.array([[0, 0, 0], [0, -4, 1], [0, 2, 1]]))"""""",\n        },\n        {\n            ""no"": 3,\n            ""augmentation"": """"""iaa.Sharpen(alpha=(0.0, 1.0), lightness=(0.75, 2.0))"""""",\n        },\n        {\n            ""no"": 4,\n            ""augmentation"": """"""iaa.Emboss(alpha=(0.0, 1.0), strength=(0.5, 1.5))"""""",\n        },\n        {""no"": 5, ""augmentation"": """"""iaa.EdgeDetect(alpha=(0.0, 1.0))""""""},\n        {\n            ""no"": 6,\n            ""augmentation"": """"""iaa.DirectedEdgeDetect(alpha=(0.0, 1.0), direction=(0.0, 1.0))"""""",\n        },\n    ],\n    ""edges"": [\n        {""no"": 1, ""augmentation"": """"""iaa.Canny()""""""},\n        {""no"": 2, ""augmentation"": """"""iaa.Canny(alpha=(0.0, 0.5))""""""},\n        {\n            ""no"": 3,\n            ""augmentation"": """"""iaa.Canny(alpha=(0.0, 0.5), colorizer=iaa.RandomColorsBinaryImageColorizer(\n            color_true=255, color_false=0))"""""",\n        },\n        {\n            ""no"": 4,\n            ""augmentation"": """"""iaa.Canny(alpha=(0.5, 1.0), sobel_kernel_size=[3, 7])"""""",\n        },\n        {\n            ""no"": 5,\n            ""augmentation"": """"""iaa.Alpha((0.0, 1.0), iaa.Canny(alpha=1), iaa.MedianBlur(13))"""""",\n        },\n    ],\n    ""flip"": [\n        {""no"": 1, ""augmentation"": """"""iaa.Fliplr(0.5)""""""},\n        {""no"": 2, ""augmentation"": """"""iaa.Flipud(0.5)""""""},\n    ],\n    ""geometric"": [\n        {""no"": 1, ""augmentation"": """"""iaa.Affine(scale=(0.5, 1.5))""""""},\n        {\n            ""no"": 2,\n            ""augmentation"": """"""iaa.Affine(scale={""x"": (0.5, 1.5), ""y"": (0.5, 1.5)})"""""",\n        },\n        {\n            ""no"": 3,\n            ""augmentation"": """"""iaa.Affine(translate_percent={""x"": (-0.2, 0.2), ""y"": (-0.2, 0.2)})"""""",\n        },\n        {\n            ""no"": 4,\n            ""augmentation"": """"""iaa.Affine(translate_px={""x"": (-20, 20), ""y"": (-20, 20)})"""""",\n        },\n        {""no"": 5, ""augmentation"": """"""iaa.Affine(rotate=(-45, 45))""""""},\n        {""no"": 6, ""augmentation"": """"""iaa.Affine(shear=(-16, 16))""""""},\n        {\n            ""no"": 7,\n            ""augmentation"": """"""iaa.Affine(translate_percent={""x"": -0.20}, mode=ia.ALL, cval=(0, 255))"""""",\n        },\n        {""no"": 8, ""augmentation"": """"""iaa.ScaleX((0.5, 1.5))""""""},\n        {""no"": 9, ""augmentation"": """"""iaa.ScaleY((0.5, 1.5))""""""},\n        {""no"": 10, ""augmentation"": """"""iaa.TranslateX(px=(-20, 20))""""""},\n        {""no"": 11, ""augmentation"": """"""iaa.TranslateX(percent=(-0.1, 0.1))""""""},\n        {""no"": 12, ""augmentation"": """"""iaa.TranslateY(px=(-20, 20))""""""},\n        {""no"": 13, ""augmentation"": """"""iaa.TranslateY(percent=(-0.1, 0.1))""""""},\n        {""no"": 14, ""augmentation"": """"""iaa.Rotate((-45, 45))""""""},\n        {""no"": 15, ""augmentation"": """"""iaa.ShearX((-20, 20))""""""},\n        {""no"": 16, ""augmentation"": """"""iaa.ShearY((-20, 20))""""""},\n        {\n            ""no"": 17,\n            ""augmentation"": """"""iaa.PiecewiseAffine(scale=(0.01, 0.05))"""""",\n        },\n        {\n            ""no"": 18,\n            ""augmentation"": """"""iaa.PerspectiveTransform(scale=(0.01, 0.15))"""""",\n        },\n        {\n            ""no"": 19,\n            ""augmentation"": """"""iaa.PerspectiveTransform(scale=(0.01, 0.15), keep_size=False)"""""",\n        },\n        {\n            ""no"": 20,\n            ""augmentation"": """"""iaa.ElasticTransformation(alpha=(0, 5.0), sigma=0.25)"""""",\n        },\n        {""no"": 21, ""augmentation"": """"""iaa.Rot90(1)""""""},\n        {""no"": 22, ""augmentation"": """"""iaa.Rot90([1, 3])""""""},\n        {""no"": 23, ""augmentation"": """"""iaa.Rot90((1, 3))""""""},\n        {""no"": 24, ""augmentation"": """"""iaa.Rot90((1, 3), keep_size=False)""""""},\n        {\n            ""no"": 25,\n            ""augmentation"": """"""iaa.WithPolarWarping(iaa.CropAndPad(percent=(-0.1, 0.1)))"""""",\n        },\n        {\n            ""no"": 26,\n            ""augmentation"": """"""iaa.WithPolarWarping(iaa.Affine(\n            translate_percent={""x"": (-0.1, 0.1), ""y"": (-0.1, 0.1)}))"""""",\n        },\n        {\n            ""no"": 27,\n            ""augmentation"": """"""iaa.WithPolarWarping(iaa.AveragePooling((2, 8)))"""""",\n        },\n        {""no"": 28, ""augmentation"": """"""iaa.Jigsaw(nb_rows=10, nb_cols=10)""""""},\n        {\n            ""no"": 29,\n            ""augmentation"": """"""iaa.Jigsaw(nb_rows=(1, 4), nb_cols=(1, 4))"""""",\n        },\n        {\n            ""no"": 30,\n            ""augmentation"": """"""iaa.Jigsaw(nb_rows=10, nb_cols=10, max_steps=(1, 5))"""""",\n        },\n    ],\n    ""corrupt_like"": [\n        {\n            ""no"": 1,\n            ""augmentation"": """"""iaa.imgcorruptlike.GaussianNoise(severity=2)"""""",\n        },\n        {\n            ""no"": 2,\n            ""augmentation"": """"""iaa.imgcorruptlike.ShotNoise(severity=2)"""""",\n        },\n        {\n            ""no"": 3,\n            ""augmentation"": """"""iaa.imgcorruptlike.ImpulseNoise(severity=2)"""""",\n        },\n        {\n            ""no"": 4,\n            ""augmentation"": """"""iaa.imgcorruptlike.SpeckleNoise(severity=2)"""""",\n        },\n        {\n            ""no"": 5,\n            ""augmentation"": """"""iaa.imgcorruptlike.GaussianBlur(severity=2)"""""",\n        },\n        {\n            ""no"": 6,\n            ""augmentation"": """"""iaa.imgcorruptlike.GlassBlur(severity=2)"""""",\n        },\n        {\n            ""no"": 7,\n            ""augmentation"": """"""iaa.imgcorruptlike.DefocusBlur(severity=2)"""""",\n        },\n        {\n            ""no"": 8,\n            ""augmentation"": """"""iaa.imgcorruptlike.MotionBlur(severity=2)"""""",\n        },\n        {\n            ""no"": 9,\n            ""augmentation"": """"""iaa.imgcorruptlike.ZoomBlur(severity=2)"""""",\n        },\n        {""no"": 10, ""augmentation"": """"""iaa.imgcorruptlike.Fog(severity=2)""""""},\n        {""no"": 11, ""augmentation"": """"""iaa.imgcorruptlike.Frost(severity=2)""""""},\n        {""no"": 12, ""augmentation"": """"""iaa.imgcorruptlike.Snow(severity=2)""""""},\n        {\n            ""no"": 13,\n            ""augmentation"": """"""iaa.imgcorruptlike.Spatter(severity=2)"""""",\n        },\n        {\n            ""no"": 14,\n            ""augmentation"": """"""iaa.imgcorruptlike.Contrast(severity=2)"""""",\n        },\n        {\n            ""no"": 15,\n            ""augmentation"": """"""iaa.imgcorruptlike.Brightness(severity=2)"""""",\n        },\n        {\n            ""no"": 16,\n            ""augmentation"": """"""iaa.imgcorruptlike.Saturate(severity=2)"""""",\n        },\n        {\n            ""no"": 17,\n            ""augmentation"": """"""iaa.imgcorruptlike.JpegCompression(severity=2)"""""",\n        },\n        {\n            ""no"": 18,\n            ""augmentation"": """"""iaa.imgcorruptlike.Pixelate(severity=2)"""""",\n        },\n        {\n            ""no"": 19,\n            ""augmentation"": """"""iaa.imgcorruptlike.ElasticTransform(severity=2)"""""",\n        },\n    ],\n    ""pi_like"": [\n        {\n            ""no"": 1,\n            ""augmentation"": """"""iaa.Solarize(0.5, threshold=(32, 128))"""""",\n        },\n        {""no"": 2, ""augmentation"": """"""iaa.pillike.Equalize()""""""},\n        {""no"": 3, ""augmentation"": """"""iaa.pillike.Autocontrast()""""""},\n        {\n            ""no"": 4,\n            ""augmentation"": """"""iaa.pillike.Autocontrast((10, 20), per_channel=True)"""""",\n        },\n        {""no"": 5, ""augmentation"": """"""iaa.pillike.EnhanceColor()""""""},\n        {""no"": 6, ""augmentation"": """"""iaa.pillike.EnhanceContrast()""""""},\n        {""no"": 7, ""augmentation"": """"""iaa.pillike.EnhanceBrightness()""""""},\n        {""no"": 8, ""augmentation"": """"""iaa.pillike.EnhanceSharpness()""""""},\n        {""no"": 9, ""augmentation"": """"""iaa.pillike.FilterBlur()""""""},\n        {""no"": 10, ""augmentation"": """"""iaa.pillike.FilterSmooth()""""""},\n        {""no"": 11, ""augmentation"": """"""iaa.pillike.FilterSmoothMore()""""""},\n        {""no"": 12, ""augmentation"": """"""iaa.pillike.FilterEdgeEnhance()""""""},\n        {""no"": 13, ""augmentation"": """"""iaa.pillike.FilterEdgeEnhanceMore()""""""},\n        {""no"": 14, ""augmentation"": """"""iaa.pillike.FilterFindEdges()""""""},\n        {""no"": 15, ""augmentation"": """"""iaa.pillike.FilterContour()""""""},\n        {""no"": 16, ""augmentation"": """"""iaa.pillike.FilterEmboss()""""""},\n        {""no"": 17, ""augmentation"": """"""iaa.pillike.FilterSharpen()""""""},\n        {""no"": 18, ""augmentation"": """"""iaa.pillike.FilterDetail()""""""},\n        {\n            ""no"": 19,\n            ""augmentation"": """"""iaa.pillike.Affine(scale={""x"": (0.8, 1.2), ""y"": (0.5, 1.5)})"""""",\n        },\n        {\n            ""no"": 20,\n            ""augmentation"": """"""iaa.pillike.Affine(translate_px={""x"": 0, ""y"": [-10, 10]}, \n                                           fillcolor=128)"""""",\n        },\n        {\n            ""no"": 21,\n            ""augmentation"": """"""iaa.pillike.Affine(rotate=(-20, 20), fillcolor=(0, 256))"""""",\n        },\n    ],\n    ""pooling"": [\n        {""no"": 1, ""augmentation"": """"""iaa.AveragePooling(2)""""""},\n        {\n            ""no"": 2,\n            ""augmentation"": """"""iaa.AveragePooling(2, keep_size=False)"""""",\n        },\n        {""no"": 3, ""augmentation"": """"""iaa.AveragePooling([2, 8])""""""},\n        {""no"": 4, ""augmentation"": """"""iaa.AveragePooling((1, 7))""""""},\n        {""no"": 5, ""augmentation"": """"""iaa.AveragePooling(((1, 7), (1, 7)))""""""},\n        {""no"": 6, ""augmentation"": """"""iaa.MaxPooling(2)""""""},\n        {""no"": 7, ""augmentation"": """"""iaa.MaxPooling(2, keep_size=False)""""""},\n        {""no"": 8, ""augmentation"": """"""iaa.MaxPooling([2, 8])""""""},\n        {""no"": 9, ""augmentation"": """"""iaa.MaxPooling((1, 7))""""""},\n        {""no"": 10, ""augmentation"": """"""iaa.MaxPooling(((1, 7), (1, 7)))""""""},\n        {""no"": 11, ""augmentation"": """"""iaa.MinPooling(2)""""""},\n        {""no"": 12, ""augmentation"": """"""iaa.MinPooling(2, keep_size=False)""""""},\n        {""no"": 13, ""augmentation"": """"""iaa.MinPooling([2, 8])""""""},\n        {""no"": 14, ""augmentation"": """"""iaa.MinPooling((1, 7))""""""},\n        {""no"": 15, ""augmentation"": """"""iaa.MinPooling(((1, 7), (1, 7)))""""""},\n        {""no"": 16, ""augmentation"": """"""iaa.MedianPooling(2)""""""},\n        {\n            ""no"": 17,\n            ""augmentation"": """"""iaa.MedianPooling(2, keep_size=False)"""""",\n        },\n        {""no"": 18, ""augmentation"": """"""iaa.MedianPooling([2, 8])""""""},\n        {""no"": 19, ""augmentation"": """"""iaa.MedianPooling((1, 7))""""""},\n        {""no"": 20, ""augmentation"": """"""iaa.MedianPooling(((1, 7), (1, 7)))""""""},\n    ],\n    ""segmentation"": [\n        {\n            ""no"": 1,\n            ""augmentation"": """"""iaa.Superpixels(p_replace=0.5, n_segments=64)"""""",\n        },\n        {\n            ""no"": 2,\n            ""augmentation"": """"""iaa.Superpixels(p_replace=(0.1, 1.0), n_segments=(16, 128))"""""",\n        },\n        {""no"": 3, ""augmentation"": """"""iaa.UniformVoronoi((100, 500))""""""},\n        {\n            ""no"": 4,\n            ""augmentation"": """"""iaa.UniformVoronoi(250, p_replace=0.9, max_size=None)"""""",\n        },\n        {""no"": 5, ""augmentation"": """"""iaa.RegularGridVoronoi(10, 20)""""""},\n        {\n            ""no"": 6,\n            ""augmentation"": """"""iaa.RegularGridVoronoi((10, 30), 20, p_drop_points=0.0, \n                                               p_replace=0.9, max_size=None)"""""",\n        },\n        {\n            ""no"": 7,\n            ""augmentation"": """"""iaa.RelativeRegularGridVoronoi(0.1, 0.25)"""""",\n        },\n        {\n            ""no"": 8,\n            ""augmentation"": """"""iaa.RelativeRegularGridVoronoi(\n            (0.03, 0.1), 0.1, p_drop_points=0.0, p_replace=0.9, max_size=512)"""""",\n        },\n    ],\n    ""size"": [\n        {\n            ""no"": 1,\n            ""augmentation"": """"""iaa.Resize({""height"": 32, ""width"": 64})"""""",\n        },\n        {\n            ""no"": 2,\n            ""augmentation"": """"""iaa.Resize({""height"": 32, ""width"": ""keep-aspect-ratio""})"""""",\n        },\n        {""no"": 3, ""augmentation"": """"""iaa.Resize((0.5, 1.0))""""""},\n        {\n            ""no"": 4,\n            ""augmentation"": """"""iaa.Resize({""height"": (0.5, 0.75), ""width"": [16, 32, 64]})"""""",\n        },\n        {""no"": 5, ""augmentation"": """"""iaa.CropAndPad(percent=(-0.25, 0.25))""""""},\n        {\n            ""no"": 6,\n            ""augmentation"": """"""iaa.CropAndPad(percent=(0, 0.2), pad_mode=[""constant"", ""edge""], pad_cval=(0, 128))"""""",\n        },\n        {\n            ""no"": 7,\n            ""augmentation"": """"""iaa.CropAndPad(px=((0, 30), (0, 10), (0, 30), (0, 10)), \n                                       pad_mode=ia.ALL, pad_cval=(0, 128))"""""",\n        },\n        {\n            ""no"": 8,\n            ""augmentation"": """"""iaa.CropAndPad(px=(-10, 10),sample_independently=False)"""""",\n        },\n        {\n            ""no"": 9,\n            ""augmentation"": """"""iaa.PadToFixedSize(width=100, height=100)"""""",\n        },\n        {\n            ""no"": 10,\n            ""augmentation"": """"""iaa.PadToFixedSize(width=100, height=100, position=""center"")"""""",\n        },\n        {\n            ""no"": 11,\n            ""augmentation"": """"""iaa.PadToFixedSize(width=100, height=100, pad_mode=ia.ALL)"""""",\n        },\n        {\n            ""no"": 12,\n            ""augmentation"": """"""iaa.Sequential([iaa.PadToFixedSize(width=100, height=100), \n                                        iaa.CropToFixedSize(width=100, height=100)])"""""",\n        },\n        {\n            ""no"": 13,\n            ""augmentation"": """"""iaa.CropToFixedSize(width=100, height=100)"""""",\n        },\n        {\n            ""no"": 14,\n            ""augmentation"": """"""iaa.CropToFixedSize(width=100, height=100, position=""center"")"""""",\n        },\n        {\n            ""no"": 15,\n            ""augmentation"": """"""iaa.Sequential([iaa.PadToFixedSize(width=100, height=100), \n                                        iaa.CropToFixedSize(width=100, height=100)])"""""",\n        },\n        {\n            ""no"": 16,\n            ""augmentation"": """"""iaa.PadToMultiplesOf(height_multiple=10, width_multiple=6)"""""",\n        },\n        {\n            ""no"": 17,\n            ""augmentation"": """"""iaa.CropToMultiplesOf(height_multiple=10, width_multiple=6)"""""",\n        },\n        {\n            ""no"": 18,\n            ""augmentation"": """"""iaa.CropToPowersOf(height_base=3, width_base=2)"""""",\n        },\n        {\n            ""no"": 19,\n            ""augmentation"": """"""iaa.PadToPowersOf(height_base=3, width_base=2)"""""",\n        },\n        {""no"": 20, ""augmentation"": """"""iaa.CropToAspectRatio(2.0)""""""},\n        {""no"": 21, ""augmentation"": """"""iaa.PadToAspectRatio(2.0)""""""},\n        {""no"": 22, ""augmentation"": """"""iaa.CropToSquare()""""""},\n        {""no"": 23, ""augmentation"": """"""iaa.PadToSquare()""""""},\n        {\n            ""no"": 24,\n            ""augmentation"": """"""iaa.CenterPadToFixedSize(height=20, width=30)"""""",\n        },\n        {\n            ""no"": 25,\n            ""augmentation"": """"""iaa.CenterCropToFixedSize(height=20, width=10)"""""",\n        },\n        {\n            ""no"": 26,\n            ""augmentation"": """"""iaa.CenterCropToMultiplesOf(height_multiple=10, width_multiple=6)"""""",\n        },\n        {\n            ""no"": 27,\n            ""augmentation"": """"""iaa.CenterPadToMultiplesOf(height_multiple=10, width_multiple=6)"""""",\n        },\n        {\n            ""no"": 28,\n            ""augmentation"": """"""iaa.CropToPowersOf(height_base=3, width_base=2)"""""",\n        },\n        {\n            ""no"": 29,\n            ""augmentation"": """"""iaa.CenterPadToPowersOf(height_base=3, width_base=2)"""""",\n        },\n        {""no"": 30, ""augmentation"": """"""iaa.CenterCropToAspectRatio(2.0)""""""},\n        {""no"": 31, ""augmentation"": """"""iaa.PadToAspectRatio(2.0)""""""},\n        {""no"": 32, ""augmentation"": """"""iaa.CenterCropToSquare()""""""},\n        {""no"": 33, ""augmentation"": """"""iaa.CenterPadToSquare()""""""},\n        {\n            ""no"": 34,\n            ""augmentation"": """"""iaa.KeepSizeByResize(iaa.Crop((20, 40), keep_size=False))"""""",\n        },\n        {\n            ""no"": 35,\n            ""augmentation"": """"""iaa.KeepSizeByResize(iaa.Crop((20, 40), keep_size=False), interpolation=""nearest"")"""""",\n        },\n        {\n            ""no"": 36,\n            ""augmentation"": """"""iaa.KeepSizeByResize(\n            iaa.Crop((20, 40), keep_size=False), interpolation=[""nearest"", ""cubic""], \n            interpolation_heatmaps=iaa.KeepSizeByResize.SAME_AS_IMAGES,\n            interpolation_segmaps=iaa.KeepSizeByResize.NO_RESIZE)"""""",\n        },\n    ],\n}\n\npreset_1 = [\n    [\n        [\n            {""sequence_group"": ""meta"", ""no"": 7},\n            {""sequence_group"": ""meta"", ""no"": 9},\n            {""sequence_group"": ""arithmetic"", ""no"": 25},\n            {""sequence_group"": ""flip"", ""no"": 1},\n            {""sequence_group"": ""flip"", ""no"": 2},\n        ],\n        [\n            {""sequence_group"": ""arithmetic"", ""no"": 6},\n            {""sequence_group"": ""arithmetic"", ""no"": 10},\n            {""sequence_group"": ""arithmetic"", ""no"": 14},\n            {""sequence_group"": ""arithmetic"", ""no"": 27},\n            {""sequence_group"": ""arithmetic"", ""no"": 28},\n            {""sequence_group"": ""arithmetic"", ""no"": 36},\n        ],\n    ],\n    [\n        [\n            {""sequence_group"": ""arithmetic"", ""no"": 26},\n            {""sequence_group"": ""arithmetic"", ""no"": 29},\n            {""sequence_group"": ""flip"", ""no"": 1},\n            {""sequence_group"": ""flip"", ""no"": 2},\n        ],\n        [\n            {""sequence_group"": ""arithmetic"", ""no"": 40},\n            {""sequence_group"": ""artistic"", ""no"": 1},\n            {""sequence_group"": ""artistic"", ""no"": 2},\n            {""sequence_group"": ""blend"", ""no"": 2},\n            {""sequence_group"": ""blend"", ""no"": 4},\n            {""sequence_group"": ""blend"", ""no"": 5},\n        ],\n    ],\n    [\n        [\n            {""sequence_group"": ""blend"", ""no"": 8},\n            {""sequence_group"": ""blend"", ""no"": 18},\n            {""sequence_group"": ""blend"", ""no"": 19},\n            {""sequence_group"": ""flip"", ""no"": 1},\n            {""sequence_group"": ""flip"", ""no"": 2},\n        ],\n        [\n            {""sequence_group"": ""blend"", ""no"": 11},\n            {""sequence_group"": ""blend"", ""no"": 12},\n            {""sequence_group"": ""blend"", ""no"": 13},\n            {""sequence_group"": ""blend"", ""no"": 31},\n            {""sequence_group"": ""gaussian_blur"", ""no"": 3},\n        ],\n    ],\n    [\n        [\n            {""sequence_group"": ""gaussian_blur"", ""no"": 5},\n            {""sequence_group"": ""color"", ""no"": 1},\n            {""sequence_group"": ""color"", ""no"": 5},\n            {""sequence_group"": ""flip"", ""no"": 1},\n            {""sequence_group"": ""flip"", ""no"": 2},\n        ],\n        [\n            {""sequence_group"": ""color"", ""no"": 9},\n            {""sequence_group"": ""color"", ""no"": 10},\n            {""sequence_group"": ""color"", ""no"": 11},\n            {""sequence_group"": ""color"", ""no"": 15},\n            {""sequence_group"": ""color"", ""no"": 16},\n            {""sequence_group"": ""blend"", ""no"": 18},\n        ],\n    ],\n    [\n        [\n            {""sequence_group"": ""color"", ""no"": 21},\n            {""sequence_group"": ""contrast"", ""no"": 1},\n            {""sequence_group"": ""gaussian_blur"", ""no"": 1},\n            {""sequence_group"": ""flip"", ""no"": 1},\n            {""sequence_group"": ""flip"", ""no"": 2},\n        ],\n        [\n            {""sequence_group"": ""color"", ""no"": 23},\n            {""sequence_group"": ""color"", ""no"": 29},\n            {""sequence_group"": ""contrast"", ""no"": 2},\n            {""sequence_group"": ""contrast"", ""no"": 4},\n            {""sequence_group"": ""contrast"", ""no"": 6},\n            {""sequence_group"": ""contrast"", ""no"": 7},\n        ],\n    ],\n    [\n        [\n            {""sequence_group"": ""contrast"", ""no"": 20},\n            {""sequence_group"": ""convolution"", ""no"": 4},\n            {""sequence_group"": ""flip"", ""no"": 1},\n            {""sequence_group"": ""flip"", ""no"": 2},\n        ],\n        [\n            {""sequence_group"": ""contrast"", ""no"": 9},\n            {""sequence_group"": ""contrast"", ""no"": 10},\n            {""sequence_group"": ""contrast"", ""no"": 14},\n            {""sequence_group"": ""contrast"", ""no"": 18},\n            {""sequence_group"": ""convolution"", ""no"": 6},\n        ],\n    ],\n    [\n        [\n            {""sequence_group"": ""flip"", ""no"": 1},\n            {""sequence_group"": ""flip"", ""no"": 2},\n            {""sequence_group"": ""corrupt_like"", ""no"": 3},\n        ],\n        [\n            {""sequence_group"": ""edges"", ""no"": 2},\n            {""sequence_group"": ""edges"", ""no"": 5},\n            {""sequence_group"": ""pi_like"", ""no"": 13},\n        ],\n    ],\n    [\n        [\n            {""sequence_group"": ""flip"", ""no"": 1},\n            {""sequence_group"": ""flip"", ""no"": 2},\n            {""sequence_group"": ""pi_like"", ""no"": 5},\n        ],\n        [\n            {""sequence_group"": ""corrupt_like"", ""no"": 10},\n            {""sequence_group"": ""corrupt_like"", ""no"": 11},\n            {""sequence_group"": ""corrupt_like"", ""no"": 12},\n            {""sequence_group"": ""corrupt_like"", ""no"": 13},\n            {""sequence_group"": ""pi_like"", ""no"": 1},\n            {""sequence_group"": ""pi_like"", ""no"": 2},\n            {""sequence_group"": ""pi_like"", ""no"": 3},\n            {""sequence_group"": ""pi_like"", ""no"": 5},\n            {""sequence_group"": ""pi_like"", ""no"": 14},\n        ],\n    ],\n]\n'"
Config/set_annotation_conf.py,0,"b'import json\nimport os\n\n\ndef set_voc_tags(\n    tree,\n    folder,\n    filename,\n    path,\n    size,\n    width,\n    height,\n    depth,\n    obj,\n    obj_name,\n    box,\n    x0,\n    y0,\n    x1,\n    y1,\n    conf_file=\'voc_conf.json\',\n    indent=4,\n    sort_keys=False,\n):\n    """"""\n    Create/modify json voc annotation tags.\n    Args:\n        tree: xml tree tag.\n        folder: Image folder tag.\n        filename: Image file tag.\n        path: Path to image tag.\n        size: Image size tag.\n        width: Image width tag.\n        height: Image height tag.\n        depth: Image depth tag.\n        obj: Object tag.\n        obj_name: Object name tag.\n        box: Bounding box tag.\n        x0: Start x coordinate tag.\n        y0: Start y coordinate tag.\n        x1: End x coordinate tag.\n        y1: End y coordinate tag.\n        conf_file: Configuration file name.\n        indent: json output indent.\n        sort_keys: Sort json output keys.\n\n    Returns:\n        None.\n    """"""\n    if conf_file in os.listdir(\'.\'):\n        os.remove(os.path.join(os.getcwd(), conf_file))\n    conf = {\n        \'Tree\': {\n            \'Tree Tag\': tree,\n            \'Folder\': folder,\n            \'Filename\': filename,\n            \'Path\': path,\n        },\n        \'Size\': {\n            \'Size Tag\': size,\n            \'Width\': width,\n            \'Height\': height,\n            \'Depth\': depth,\n        },\n        \'Object\': {\n            \'Object Tag\': obj,\n            \'Object Name\': obj_name,\n            \'Object Box\': {\n                \'Object Box Tag\': box,\n                \'X0\': x0,\n                \'Y0\': y0,\n                \'X1\': x1,\n                \'Y1\': y1,\n            },\n        },\n    }\n\n    with open(conf_file, \'w\') as conf_out:\n        json.dump(conf, conf_out, indent=indent, sort_keys=sort_keys)\n\n\nif __name__ == \'__main__\':\n    tree_tag = \'annotation\'\n    folder_tag = \'folder\'\n    file_tag = \'filename\'\n    path_tag = \'path\'\n    size_tag = \'size\'\n    width_tag = \'width\'\n    height_tag = \'height\'\n    depth_tag = \'depth\'\n    object_tag = \'object\'\n    object_name = \'name\'\n    box_tag = \'bndbox\'\n    x_min_tag = \'xmin\'\n    y_min_tag = \'ymin\'\n    x_max_tag = \'xmax\'\n    y_max_tag = \'ymax\'\n    set_voc_tags(\n        tree_tag,\n        folder_tag,\n        file_tag,\n        path_tag,\n        size_tag,\n        width_tag,\n        height_tag,\n        depth_tag,\n        object_tag,\n        object_name,\n        box_tag,\n        x_min_tag,\n        y_min_tag,\n        x_max_tag,\n        y_max_tag,\n    )\n'"
Helpers/anchors.py,17,"b'import numpy as np\nfrom Helpers.visual_tools import visualization_wrapper\nfrom Helpers.utils import default_logger, timer\n\n\ndef iou(relative_sizes, centroids, k):\n    """"""\n    Calculate intersection over union for relative box sizes.\n    Args:\n        relative_sizes: 2D array of relative box sizes.\n        centroids: 2D array of shape(k, 2)\n        k: int, number of clusters.\n\n    Returns:\n        IOU array.\n    """"""\n    n = relative_sizes.shape[0]\n    box_area = relative_sizes[:, 0] * relative_sizes[:, 1]\n    box_area = box_area.repeat(k)\n    box_area = np.reshape(box_area, (n, k))\n    cluster_area = centroids[:, 0] * centroids[:, 1]\n    cluster_area = np.tile(cluster_area, [1, n])\n    cluster_area = np.reshape(cluster_area, (n, k))\n    box_w_matrix = np.reshape(relative_sizes[:, 0].repeat(k), (n, k))\n    cluster_w_matrix = np.reshape(np.tile(centroids[:, 0], (1, n)), (n, k))\n    min_w_matrix = np.minimum(cluster_w_matrix, box_w_matrix)\n    box_h_matrix = np.reshape(relative_sizes[:, 1].repeat(k), (n, k))\n    cluster_h_matrix = np.reshape(np.tile(centroids[:, 1], (1, n)), (n, k))\n    min_h_matrix = np.minimum(cluster_h_matrix, box_h_matrix)\n    inter_area = np.multiply(min_w_matrix, min_h_matrix)\n    result = inter_area / (box_area + cluster_area - inter_area)\n    return result\n\n\n@timer(default_logger)\n@visualization_wrapper\ndef k_means(relative_sizes, k, distance_func=np.median, frame=None):\n    """"""\n    Calculate optimal anchor relative sizes.\n    Args:\n        relative_sizes: 2D array of relative box sizes.\n        k: int, number of clusters.\n        distance_func: function to calculate distance.\n        frame: pandas DataFrame with the annotation data(for visualization purposes).\n\n    Returns:\n        Optimal relative sizes.\n    """"""\n    box_number = relative_sizes.shape[0]\n    last_nearest = np.zeros((box_number,))\n    centroids = relative_sizes[np.random.randint(0, box_number, k)]\n    old_distances = np.zeros((relative_sizes.shape[0], k))\n    iteration = 0\n    while True:\n        distances = 1 - iou(relative_sizes, centroids, k)\n        print(\n            f\'Iteration: {iteration} Loss: \'\n            f\'{np.sum(np.abs(distances - old_distances))}\'\n        )\n        old_distances = distances.copy()\n        iteration += 1\n        current_nearest = np.argmin(distances, axis=1)\n        if (last_nearest == current_nearest).all():\n            default_logger.info(\n                f\'Generated {len(centroids)} anchors in \'\n                f\'{iteration} iterations\'\n            )\n            return centroids, frame\n        for anchor in range(k):\n            centroids[anchor] = distance_func(\n                relative_sizes[current_nearest == anchor], axis=0\n            )\n        last_nearest = current_nearest\n\n\ndef generate_anchors(width, height, centroids):\n    """"""\n    Generate anchors for image of size(width, height)\n    Args:\n        width: Width of image.\n        height: Height of image.\n        centroids: Output of k-means.\n\n    Returns:\n        2D array of resulting anchors.\n    """"""\n    return (centroids * np.array([width, height])).astype(int)\n'"
Helpers/annotation_parsers.py,1,"b'from xml.etree import ElementTree\nfrom time import perf_counter\nimport pandas as pd\nimport json\nimport os\nimport numpy as np\nfrom pathlib import Path\nfrom Helpers.visual_tools import visualization_wrapper\nfrom Helpers.utils import ratios_to_coordinates, default_logger\n\n\ndef get_tree_item(parent, tag, file_path, find_all=False):\n    """"""\n    Get item from xml tree element.\n    Args:\n        parent: Parent in xml element tree\n        tag: tag to look for.\n        file_path: Current xml file being handled.\n        find_all: If True, all elements found will be returned.\n\n    Returns:\n        Tag item.\n    """"""\n    target = parent.find(tag)\n    if find_all:\n        target = parent.findall(tag)\n    if target is None:\n        raise ValueError(f\'Could not find {tag} in {file_path}\')\n    return target\n\n\ndef parse_voc_file(file_path, voc_conf):\n    """"""\n    Parse voc annotation from xml file.\n    Args:\n        file_path: Path to xml file.\n        voc_conf: voc configuration file.\n\n    Returns:\n        A list of image annotations.\n    """"""\n    assert os.path.exists(file_path)\n    image_data = []\n    with open(voc_conf) as json_data:\n        tags = json.load(json_data)\n    tree = ElementTree.parse(file_path)\n    image_path = get_tree_item(tree, tags[\'Tree\'][\'Path\'], file_path).text\n    size_item = get_tree_item(tree, tags[\'Size\'][\'Size Tag\'], file_path)\n    image_width = get_tree_item(\n        size_item, tags[\'Size\'][\'Width\'], file_path\n    ).text\n    image_height = get_tree_item(\n        size_item, tags[\'Size\'][\'Height\'], file_path\n    ).text\n    for item in get_tree_item(\n        tree, tags[\'Object\'][\'Object Tag\'], file_path, True\n    ):\n        name = get_tree_item(\n            item, tags[\'Object\'][\'Object Name\'], file_path\n        ).text\n        box_item = get_tree_item(\n            item, tags[\'Object\'][\'Object Box\'][\'Object Box Tag\'], file_path\n        )\n        x0 = get_tree_item(\n            box_item, tags[\'Object\'][\'Object Box\'][\'X0\'], file_path\n        ).text\n        y0 = get_tree_item(\n            box_item, tags[\'Object\'][\'Object Box\'][\'Y0\'], file_path\n        ).text\n        x1 = get_tree_item(\n            box_item, tags[\'Object\'][\'Object Box\'][\'X1\'], file_path\n        ).text\n        y1 = get_tree_item(\n            box_item, tags[\'Object\'][\'Object Box\'][\'Y1\'], file_path\n        ).text\n        image_data.append(\n            [image_path, name, image_width, image_height, x0, y0, x1, y1]\n        )\n    return image_data\n\n\ndef adjust_frame(frame, cache_file=None):\n    """"""\n    Add relative width, relative height and object ids to annotation pandas DataFrame.\n    Args:\n        frame: pandas DataFrame containing coordinates instead of relative labels.\n        cache_file: cache_file: csv file name containing current session labels.\n\n    Returns:\n        Frame with the new columns\n    """"""\n    object_id = 1\n    for item in frame.columns[2:]:\n        frame[item] = frame[item].astype(float).astype(int)\n    frame[\'Relative Width\'] = (frame[\'X_max\'] - frame[\'X_min\']) / frame[\n        \'Image Width\'\n    ]\n    frame[\'Relative Height\'] = (frame[\'Y_max\'] - frame[\'Y_min\']) / frame[\n        \'Image Height\'\n    ]\n    for object_name in list(frame[\'Object Name\'].drop_duplicates()):\n        frame.loc[frame[\'Object Name\'] == object_name, \'Object ID\'] = object_id\n        object_id += 1\n    if cache_file:\n        frame.to_csv(\n            os.path.join(\'..\', \'Output\', \'Data\', cache_file), index=False\n        )\n    print(f\'Parsed labels:\\n{frame[""Object Name""].value_counts()}\')\n    return frame\n\n\n@visualization_wrapper\ndef parse_voc_folder(folder_path, voc_conf):\n    """"""\n    Parse a folder containing voc xml annotation files.\n    Args:\n        folder_path: Folder containing voc xml annotation files.\n        voc_conf: Path to voc json configuration file.\n\n    Returns:\n        pandas DataFrame with the annotations.\n    """"""\n    assert os.path.exists(folder_path)\n    cache_path = os.path.join(\'..\', \'Output\', \'Data\', \'parsed_from_xml.csv\')\n    if os.path.exists(cache_path):\n        frame = pd.read_csv(cache_path)\n        print(\n            f\'Labels retrieved from cache:\'\n            f\'\\n{frame[""Object Name""].value_counts()}\'\n        )\n        return frame\n    image_data = []\n    frame_columns = [\n        \'Image Path\',\n        \'Object Name\',\n        \'Image Width\',\n        \'Image Height\',\n        \'X_min\',\n        \'Y_min\',\n        \'X_max\',\n        \'Y_max\',\n    ]\n    xml_files = [\n        file_name\n        for file_name in os.listdir(folder_path)\n        if file_name.endswith(\'.xml\')\n    ]\n    for file_name in xml_files:\n        annotation_path = os.path.join(folder_path, file_name)\n        image_labels = parse_voc_file(annotation_path, voc_conf)\n        image_data.extend(image_labels)\n    frame = pd.DataFrame(image_data, columns=frame_columns)\n    classes = frame[\'Object Name\'].drop_duplicates()\n    default_logger.info(f\'Read {len(xml_files)} xml files\')\n    default_logger.info(\n        f\'Received {len(frame)} labels containing \' f\'{len(classes)} classes\'\n    )\n    if frame.empty:\n        raise ValueError(\n            f\'No labels were found in {os.path.abspath(folder_path)}\'\n        )\n    frame = adjust_frame(frame, \'parsed_from_xml.csv\')\n    return frame\n\n\n@visualization_wrapper\ndef adjust_non_voc_csv(csv_file, image_path, image_width, image_height):\n    """"""\n    Read relative data and return adjusted frame accordingly.\n    Args:\n        csv_file: .csv file containing the following columns:\n        [Image, Object Name, Object Index, bx, by, bw, bh]\n        image_path: Path prefix to be added.\n        image_width: image width.\n        image_height: image height\n    Returns:\n        pandas DataFrame with the following columns:\n        [\'Image Path\', \'Object Name\', \'Image Width\', \'Image Height\', \'X_min\',\n       \'Y_min\', \'X_max\', \'Y_max\', \'Relative Width\', \'Relative Height\',\n       \'Object ID\']\n    """"""\n    image_path = Path(image_path).absolute().resolve()\n    coordinates = []\n    old_frame = pd.read_csv(csv_file)\n    new_frame = pd.DataFrame()\n    new_frame[\'Image Path\'] = old_frame[\'Image\'].apply(\n        lambda item: os.path.join(image_path, item)\n    )\n    new_frame[\'Object Name\'] = old_frame[\'Object Name\']\n    new_frame[\'Image Width\'] = image_width\n    new_frame[\'Image Height\'] = image_height\n    new_frame[\'Relative Width\'] = old_frame[\'bw\']\n    new_frame[\'Relative Height\'] = old_frame[\'bh\']\n    new_frame[\'Object ID\'] = old_frame[\'Object Index\'] + 1\n    for index, row in old_frame.iterrows():\n        image, object_name, object_index, bx, by, bw, bh = row\n        co = ratios_to_coordinates(bx, by, bw, bh, image_width, image_height)\n        coordinates.append(co)\n    (\n        new_frame[\'X_min\'],\n        new_frame[\'Y_min\'],\n        new_frame[\'X_max\'],\n        new_frame[\'Y_max\'],\n    ) = np.array(coordinates).T\n    new_frame[[\'X_min\', \'Y_min\', \'X_max\', \'Y_max\']] = new_frame[\n        [\'X_min\', \'Y_min\', \'X_max\', \'Y_max\']\n    ].astype(\'int64\')\n    print(f\'Parsed labels:\\n{new_frame[""Object Name""].value_counts()}\')\n    classes = new_frame[\'Object Name\'].drop_duplicates()\n    default_logger.info(\n        f\'Adjustment from existing received {len(new_frame)} labels containing \'\n        f\'{len(classes)} classes\'\n    )\n    default_logger.info(f\'Added prefix to images: {image_path}\')\n    return new_frame[\n        [\n            \'Image Path\',\n            \'Object Name\',\n            \'Image Width\',\n            \'Image Height\',\n            \'X_min\',\n            \'Y_min\',\n            \'X_max\',\n            \'Y_max\',\n            \'Relative Width\',\n            \'Relative Height\',\n            \'Object ID\',\n        ]\n    ]\n'"
Helpers/augmentor.py,4,"b'import os\nimport cv2\nimport imagesize\nimport numpy as np\nimport pandas as pd\nfrom imgaug import augmenters as iaa\nfrom imgaug import parameters as iap\nimport imgaug as ia\nfrom pathlib import Path\nfrom imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom Helpers.utils import ratios_to_coordinates, timer, default_logger\nfrom Helpers.annotation_parsers import adjust_non_voc_csv\n\n\nclass DataAugment:\n    """"""\n    A tool for augmenting image data sets with bounding box support.\n    """"""\n\n    def __init__(\n        self,\n        labels_file,\n        augmentation_map,\n        workers=32,\n        converted_coordinates_file=None,\n        image_folder=None,\n    ):\n        """"""\n        Initialize augmentation session.\n        Args:\n            labels_file: cvv file containing relative image labels\n            augmentation_map: A structured dictionary containing categorized augmentation\n            sequences.\n            workers: Parallel threads.\n            converted_coordinates_file: csv file containing converted from relative\n            to coordinates.\n            image_folder: Folder containing images other than Data/Photos/\n        """"""\n        assert all([ia, iaa, iap])\n        self.labels_file = labels_file\n        self.mapping = pd.read_csv(labels_file)\n        self.image_folder = (\n            Path(os.path.join(\'..\', \'Data\', \'Photos\')).absolute().resolve()\n        )\n        if image_folder:\n            self.image_folder = Path(image_folder).absolute().resolve()\n        self.image_paths = [\n            Path(os.path.join(self.image_folder, image)).absolute().resolve()\n            for image in os.listdir(self.image_folder)\n            if not image.startswith(\'.\')\n        ]\n        self.image_paths_copy = self.image_paths.copy()\n        if not self.image_paths:\n            default_logger.error(\n                f\'Augmentation aborted: no photos found in {self.image_folder}\'\n            )\n            raise ValueError(f\'No photos given\')\n        self.image_width, self.image_height = imagesize.get(\n            self.image_paths[0]\n        )\n        self.converted_coordinates = (\n            pd.read_csv(converted_coordinates_file)\n            if converted_coordinates_file\n            else self.relative_to_coordinates()\n        )\n        self.converted_groups = self.converted_coordinates.groupby(\'image\')\n        self.augmentation_data = []\n        self.augmentation_sequences = []\n        self.augmentation_map = augmentation_map\n        self.workers = workers\n        self.augmented_images = 0\n        self.total_images = len(self.image_paths)\n        self.session_id = np.random.randint(10 ** 6, (10 ** 7))\n\n    def create_sequences(self, sequences):\n        """"""\n        Create sequences for imgaug.augmenters.Sequential().\n        Args:\n            sequences: A list of dictionaries with each dictionary\n            containing the following:\n            -sequence_group: str, one of self.augmentation_map keys including:\n                [\'meta\', \'arithmetic\', \'artistic\', \'blend\', \'gaussian_blur\', \'color\',\n                \'contrast\', \'convolution\', \'edges\', \'flip\', \'geometric\', \'corrupt_like\',\n                \'pi_like\', \'pooling\', \'segmentation\', \'size\']\n            -no: augmentation number (\'no\' key in self.augmentation_map > chosen sequence_group)\n                Example:\n                    sequences = (\n                    [[{\'sequence_group\': \'meta\', \'no\': 5},\n                      {\'sequence_group\': \'arithmetic\', \'no\': 3}],\n                     [{\'sequence_group\': \'arithmetic\', \'no\': 2}]]\n                    )\n        Returns:\n            The list of augmentation sequences that will be applied over images.\n        """"""\n        total_target = (len(sequences) * len(self.image_paths)) + len(\n            self.image_paths\n        )\n        default_logger.info(f\'Total images(old + augmented): {total_target}\')\n        for group in sequences:\n            some_ofs = [\n                self.augmentation_map[item[\'sequence_group\']][item[\'no\'] - 1]\n                for item in group[0]\n            ]\n            one_ofs = [\n                self.augmentation_map[item[\'sequence_group\']][item[\'no\'] - 1]\n                for item in group[1]\n            ]\n            some_of_aug = [item[\'augmentation\'] for item in some_ofs]\n            one_of_aug = [item[\'augmentation\'] for item in one_ofs]\n            some_of_seq = iaa.SomeOf(\n                (0, 5), [eval(item) for item in some_of_aug]\n            )\n            one_of_seq = iaa.OneOf([eval(item) for item in one_of_aug])\n            self.augmentation_sequences.append(\n                iaa.Sequential([some_of_seq, one_of_seq], random_order=True)\n            )\n        return self.augmentation_sequences\n\n    @staticmethod\n    def load_image(image_path, new_size=None):\n        """"""\n        Load image.\n        Args:\n            image_path: Path to image to load.\n            new_size: new image dimensions(tuple).\n\n        Returns:\n            numpy array(image), image_path\n        """"""\n        assert os.path.exists(image_path), f\'{image_path} does not exist\'\n        image = cv2.imread(image_path)\n        if new_size:\n            return cv2.resize(image, new_size)\n        return image, image_path\n\n    def calculate_ratios(self, x1, y1, x2, y2):\n        """"""\n        Calculate relative object ratios in the labeled image.\n        Args:\n            x1: Start x coordinate.\n            y1: Start y coordinate.\n            x2: End x coordinate.\n            y2: End y coordinate.\n\n        Return:\n            bx: Relative center x coordinate.\n            by: Relative center y coordinate.\n            bw: Relative box width.\n            bh: Relative box height.\n        """"""\n        box_width = abs(x2 - x1)\n        box_height = abs(y2 - y1)\n        bx = 1 - (\n            (self.image_width - min(x1, x2) + (box_width / 2))\n            / self.image_width\n        )\n        by = 1 - (\n            (self.image_height - min(y1, y2) + (box_height / 2))\n            / self.image_height\n        )\n        bw = box_width / self.image_width\n        bh = box_height / self.image_height\n        return bx, by, bw, bh\n\n    def relative_to_coordinates(self, out_file=None):\n        """"""\n        Convert relative coordinates in self.mapping\n        to coordinates.\n        Args:\n            out_file: path to new converted csv.\n\n        Returns:\n            pandas DataFrame with the new coordinates.\n        """"""\n        items_to_save = []\n        for index, data in self.mapping.iterrows():\n            image_name, object_name, object_index, bx, by, bw, bh = data\n            x1, y1, x2, y2 = ratios_to_coordinates(\n                bx, by, bw, bh, self.image_width, self.image_height\n            )\n            items_to_save.append(\n                [\n                    image_name,\n                    x1,\n                    y1,\n                    x2,\n                    y2,\n                    object_name,\n                    object_index,\n                    bx,\n                    by,\n                    bw,\n                    bh,\n                ]\n            )\n        new_data = pd.DataFrame(\n            items_to_save,\n            columns=[\n                \'image\',\n                \'x1\',\n                \'y1\',\n                \'x2\',\n                \'y2\',\n                \'object_type\',\n                \'object_id\',\n                \'bx\',\n                \'by\',\n                \'bw\',\n                \'bh\',\n            ],\n        )\n        new_data[[\'x1\', \'y1\', \'x2\', \'y2\']] = new_data[\n            [\'x1\', \'y1\', \'x2\', \'y2\']\n        ].astype(\'int64\')\n        if out_file:\n            new_data.to_csv(out_file, index=False)\n        default_logger.info(\n            f\'Converted labels in {self.labels_file} to coordinates\'\n        )\n        return new_data\n\n    def get_image_data(self, image_path):\n        """"""\n        Get image data including bounding boxes and object names.\n        Args:\n            image_path: Path to image.\n\n        Returns:\n            pandas DataFrame with full data for image.\n        """"""\n        image_name = os.path.basename(image_path)\n        return self.converted_groups.get_group(image_name)\n\n    def get_bounding_boxes_over_image(self, image_path):\n        """"""\n        Get BoundingBoxesOnImage object.\n        Args:\n            image_path: single path.\n\n        Returns:\n            BoundingBoxesOnImage, frame_before.\n        """"""\n        boxes = []\n        frame_before = self.get_image_data(image_path)\n        for item in frame_before[[\'x1\', \'y1\', \'x2\', \'y2\']].values:\n            boxes.append(BoundingBox(*item))\n        return (\n            BoundingBoxesOnImage(\n                boxes, shape=(self.image_height, self.image_width)\n            ),\n            frame_before,\n        )\n\n    def load_batch(self, new_size, batch_size):\n        """"""\n        Load a batch of images in memory for augmentation.\n        Args:\n            new_size: new image size(tuple).\n            batch_size: Number of images to load for augmentation.\n\n        Returns:\n            numpy array of shape (batch_size, height, width, channels)\n        """"""\n        batch = [\n            f\'{self.image_paths_copy.pop()!s}\'\n            for _ in range(batch_size)\n            if self.image_paths_copy\n        ]\n        loaded = []\n        paths = []\n        with ThreadPoolExecutor(max_workers=self.workers) as executor:\n            future_images = {\n                executor.submit(\n                    self.load_image, image_path, new_size\n                ): image_path\n                for image_path in batch\n            }\n            for future_image in as_completed(future_images):\n                image, image_path = future_image.result()\n                loaded.append(image)\n                paths.append(image_path)\n        return np.array(loaded), paths\n\n    def update_data(\n        self, bbs_aug, frame_before, image_aug, new_name, new_path\n    ):\n        """"""\n        Update new bounding boxes data and save augmented image.\n        Args:\n            bbs_aug: Augmented bounding boxes\n            frame_before: pandas DataFrame containing pre-augmentation data.\n            image_aug: Augmented image as numpy nd array.\n            new_name: new image name to save image.\n            new_path: path to save the image.\n\n        Returns:\n            None\n        """"""\n        frame_after = pd.DataFrame(\n            bbs_aug.bounding_boxes, columns=[\'x1y1\', \'x2y2\']\n        )\n        if (\n            frame_after.empty\n        ):  # some post-augmentation photos do not contain bounding boxes\n            default_logger.warning(\n                f\'\\nskipping image: {new_name}: no bounding boxes after \'\n                f\'augmentation\'\n            )\n            return\n        frame_after = pd.DataFrame(\n            np.hstack(\n                (frame_after[\'x1y1\'].tolist(), frame_after[\'x2y2\'].tolist())\n            ),\n            columns=[\'x1\', \'y1\', \'x2\', \'y2\'],\n        ).astype(\'int64\')\n        frame_after[\'object_type\'] = frame_before[\'object_type\'].values\n        frame_after[\'object_id\'] = frame_before[\'object_id\'].values\n        frame_after[\'image\'] = new_name\n        for index, row in frame_after.iterrows():\n            x1, y1, x2, y2, object_type, object_id, image_name = row\n            bx, by, bw, bh = self.calculate_ratios(x1, y1, x2, y2)\n            self.augmentation_data.append(\n                [image_name, object_type, object_id, bx, by, bw, bh]\n            )\n        cv2.imwrite(new_path, image_aug)\n\n    def augment_image(self, image, image_path):\n        """"""\n        Perform augmentation and save image.\n        Args:\n            image: image to augment.\n            image_path: Path to image.\n\n        Returns:\n            None\n        """"""\n        current_sequence = 1\n        for augmentation_sequence in self.augmentation_sequences:\n            new_image_name = (\n                f\'aug-{self.session_id}-sequence-{current_sequence}\'\n                f\'-{os.path.basename(image_path)}\'\n            )\n            new_image_path = os.path.join(self.image_folder, new_image_name)\n            bbs, frame_before = self.get_bounding_boxes_over_image(image_path)\n            augmented_image, augmented_boxes = augmentation_sequence(\n                image=image, bounding_boxes=bbs\n            )\n            self.update_data(\n                augmented_boxes,\n                frame_before,\n                augmented_image,\n                new_image_name,\n                new_image_path,\n            )\n            current_sequence += 1\n            self.augmented_images += 1\n        current = os.path.basename(image_path)\n        completed = (\n            f\'{self.augmented_images}/\'\n            f\'{self.total_images * len(self.augmentation_sequences)}\'\n        )\n        percent = (\n            self.augmented_images\n            / (self.total_images * len(self.augmentation_sequences))\n            * 100\n        )\n        print(\n            f\'\\raugmenting {current}\\t{completed}\\t{percent}% completed\',\n            end=\'\',\n        )\n\n    @timer(default_logger)\n    def augment_photos_folder(self, batch_size=64, new_size=None):\n        """"""\n        Augment photos in Data/Photos/\n        Args:\n            batch_size: Size of each augmentation batch.\n            new_size: tuple, new image size.\n\n        Returns:\n            None\n        """"""\n        default_logger.info(\n            f\'Started augmentation with {self.workers} workers\'\n        )\n        default_logger.info(f\'Total images to augment: {self.total_images}\')\n        default_logger.info(f\'Session assigned id: {self.session_id}\')\n        with ThreadPoolExecutor(max_workers=self.workers) as executor:\n            while self.image_paths_copy:\n                current_batch, current_paths = self.load_batch(\n                    new_size, batch_size\n                )\n                future_augmentations = {\n                    executor.submit(self.augment_image, image, path): path\n                    for image, path in zip(current_batch, current_paths)\n                }\n                for future_augmented in as_completed(future_augmentations):\n                    future_augmented.result()\n        default_logger.info(f\'Augmentation completed\')\n        augmentation_frame = pd.DataFrame(\n            self.augmentation_data, columns=self.mapping.columns\n        )\n        saving_path = os.path.join(\n            \'..\', \'Output\', \'Data\', f\'augmented_data_plus_original.csv\'\n        )\n        combined = pd.concat([self.mapping, augmentation_frame])\n        for item in [\'bx\', \'by\', \'bw\', \'bh\']:\n            combined = combined.drop(combined[combined[item] > 1].index)\n        combined.to_csv(saving_path, index=False)\n        default_logger.info(f\'Saved old + augmented labels to {saving_path}\')\n        adjusted_combined = adjust_non_voc_csv(\n            saving_path, self.image_folder, self.image_width, self.image_height\n        )\n        adjusted_saving_path = saving_path.replace(\'augmented\', \'adjusted_aug\')\n        adjusted_combined.to_csv(adjusted_saving_path, index=False)\n        default_logger.info(\n            f\'Saved old + augmented (adjusted) labels to {adjusted_saving_path}\'\n        )\n        return adjusted_combined\n\n    @staticmethod\n    def display_windows(out, move_after):\n        """"""\n        Display image before/after augmentation.\n        Args:\n            out: A list containing images to display\n            move_after: tuple(x, y) coordinates to move the \'after\' window\n            display window.\n\n        Returns:\n            None\n        """"""\n        for bef, aft in out:\n            cv2.imshow(\'before\', bef)\n            cv2.imshow(\'after\', aft)\n            cv2.moveWindow(\'after\', *move_after)\n            cv2.waitKey(0)\n            cv2.destroyAllWindows()\n\n    def display_with_bbs(self, augment, images, image_paths, out, move_after):\n        """"""\n        Display augmentation results(before/after) with bounding boxes.\n        Args:\n            augment: imgaug augmentation sequence.\n            images: numpy array of images.\n            image_paths: A list of paths of images.\n            out: A list to hold display results.\n            move_after: tuple(x, y) coordinates to move the \'after\' window.\n\n        Returns:\n            None\n        """"""\n        for image, image_path in zip(images, image_paths):\n            bbs, frame_before = self.get_bounding_boxes_over_image(image_path)\n            image_aug, bbs_aug = augment(image=image, bounding_boxes=bbs)\n            image_before = bbs.draw_on_image(image, size=2)\n            image_after = bbs_aug.draw_on_image(\n                image_aug, size=2, color=[0, 0, 255]\n            )\n            out.append([image_before, image_after])\n        self.display_windows(out, move_after)\n        out.clear()\n\n    def preview_augmentations(\n        self,\n        tensor_size,\n        sequence_group,\n        display_boxes=False,\n        move_after=(600, 0),\n    ):\n        """"""\n        Preview all augmentation options on images in self.paths\n         in selected sequence_group\n        Args:\n            tensor_size: The number of images to display from self.paths\n            sequence_group: str, one of the following:\n                - meta\n                - arithmetic\n                - artistic\n                - blend\n                - gaussian_blur\n                - color\n                - contrast\n                - convolution\n                - edges\n                - flip\n                - geometric\n                - corrupt_like\n                - pi_like\n                - pooling\n                - segmentation\n                - size\n            display_boxes: Preview augmentations with bounding boxes before/after(if any)\n            move_after: tuple(x, y) coordinates to move the \'after\' window.\n\n        Returns:\n            None\n        """"""\n        images = [\n            cv2.imread(f\'{image}\') for image in self.image_paths[:tensor_size]\n        ]\n        image_tensor = np.array(images)\n        to_display = []\n        for item in self.augmentation_map[sequence_group]:\n            print(item[\'no\'], item[\'augmentation\'])\n            try:\n                current_augment = eval(item[\'augmentation\'])\n                if not display_boxes:\n                    to_display = current_augment(images=image_tensor)\n                    self.display_windows(\n                        zip(image_tensor, to_display), move_after\n                    )\n                if display_boxes:\n                    self.display_with_bbs(\n                        current_augment,\n                        images,\n                        self.image_paths[:tensor_size],\n                        to_display,\n                        move_after,\n                    )\n            except Exception as e:\n                print(f\'{e} for {item}\')\n'"
Helpers/dataset_handlers.py,2,"b'import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport hashlib\nimport os\nfrom pathlib import Path\nfrom Helpers.utils import default_logger\n\n\ndef get_feature_map():\n    """"""\n    Get tf.train.Example features.\n\n    Returns:\n        features\n    """"""\n    features = {\n        \'image_width\': tf.io.FixedLenFeature([], tf.int64),\n        \'image_height\': tf.io.FixedLenFeature([], tf.int64),\n        \'image_path\': tf.io.FixedLenFeature([], tf.string),\n        \'image_file\': tf.io.FixedLenFeature([], tf.string),\n        \'image_key\': tf.io.FixedLenFeature([], tf.string),\n        \'image_data\': tf.io.FixedLenFeature([], tf.string),\n        \'image_format\': tf.io.FixedLenFeature([], tf.string),\n        \'x_min\': tf.io.VarLenFeature(tf.float32),\n        \'y_min\': tf.io.VarLenFeature(tf.float32),\n        \'x_max\': tf.io.VarLenFeature(tf.float32),\n        \'y_max\': tf.io.VarLenFeature(tf.float32),\n        \'object_name\': tf.io.VarLenFeature(tf.string),\n        \'object_id\': tf.io.VarLenFeature(tf.int64),\n    }\n    return features\n\n\ndef create_example(separate_data, key, image_data):\n    """"""\n    Create tf.train.Example object.\n    Args:\n        separate_data: numpy tensor of 1 image data.\n        key: output of hashlib.sha256()\n        image_data: raw image data.\n\n    Returns:\n        tf.train.Example object.\n    """"""\n    [\n        image,\n        object_name,\n        image_width,\n        image_height,\n        x_min,\n        y_min,\n        x_max,\n        y_max,\n        _,\n        _,\n        object_id,\n    ] = separate_data\n    image_file_name = os.path.split(image[0])[-1]\n    image_format = image_file_name.split(\'.\')[-1]\n    features = {\n        \'image_height\': tf.train.Feature(\n            int64_list=tf.train.Int64List(value=[image_height[0]])\n        ),\n        \'image_width\': tf.train.Feature(\n            int64_list=tf.train.Int64List(value=[image_width[0]])\n        ),\n        \'image_path\': tf.train.Feature(\n            bytes_list=tf.train.BytesList(value=[image[0].encode(\'utf-8\')])\n        ),\n        \'image_file\': tf.train.Feature(\n            bytes_list=tf.train.BytesList(\n                value=[image_file_name.encode(\'utf8\')]\n            )\n        ),\n        \'image_key\': tf.train.Feature(\n            bytes_list=tf.train.BytesList(value=[key.encode(\'utf8\')])\n        ),\n        \'image_data\': tf.train.Feature(\n            bytes_list=tf.train.BytesList(value=[image_data])\n        ),\n        \'image_format\': tf.train.Feature(\n            bytes_list=tf.train.BytesList(value=[image_format.encode(\'utf8\')])\n        ),\n        \'x_min\': tf.train.Feature(float_list=tf.train.FloatList(value=x_min)),\n        \'y_min\': tf.train.Feature(float_list=tf.train.FloatList(value=y_min)),\n        \'x_max\': tf.train.Feature(float_list=tf.train.FloatList(value=x_max)),\n        \'y_max\': tf.train.Feature(float_list=tf.train.FloatList(value=y_max)),\n        \'object_name\': tf.train.Feature(\n            bytes_list=tf.train.BytesList(value=object_name)\n        ),\n        \'object_id\': tf.train.Feature(\n            int64_list=tf.train.Int64List(value=object_id)\n        ),\n    }\n    return tf.train.Example(features=tf.train.Features(feature=features))\n\n\ndef read_example(\n    example,\n    feature_map,\n    class_table,\n    max_boxes,\n    new_size=None,\n    get_features=False,\n):\n    """"""\n    Read single a single example from a TFRecord file.\n    Args:\n        example: nd tensor.\n        feature_map: A dictionary of feature names mapped to tf.io objects.\n        class_table: StaticHashTable object.\n        max_boxes: Maximum number of boxes per image\n        new_size: w, h new image size\n        get_features: If True, features will be returned.\n\n    Returns:\n        x_train, y_train\n    """"""\n    features = tf.io.parse_single_example(example, feature_map)\n    x_train = tf.image.decode_png(features[\'image_data\'], channels=3)\n    if new_size:\n        x_train = tf.image.resize(x_train, new_size)\n    object_name = tf.sparse.to_dense(features[\'object_name\'])\n    label = tf.cast(class_table.lookup(object_name), tf.float32)\n    y_train = tf.stack(\n        [\n            tf.sparse.to_dense(features[feature])\n            for feature in [\'x_min\', \'y_min\', \'x_max\', \'y_max\']\n        ]\n        + [label],\n        1,\n    )\n    padding = [[0, max_boxes - tf.shape(y_train)[0]], [0, 0]]\n    y_train = tf.pad(y_train, padding)\n    if get_features:\n        return x_train, y_train, features\n    return x_train, y_train\n\n\ndef write_tf_record(output_path, groups, data, trainer=None):\n    """"""\n    Write data to TFRecord.\n    Args:\n        output_path: Full path to save.\n        groups: pandas GroupBy object.\n        data: pandas DataFrame\n        trainer: Main.Trainer object.\n\n    Returns:\n        None\n    """"""\n    print(f\'Processing {os.path.split(output_path)[-1]}\')\n    if trainer:\n        if \'train\' in output_path:\n            trainer.train_tf_record = output_path\n        if \'test\' in output_path:\n            trainer.valid_tf_record = output_path\n    with tf.io.TFRecordWriter(output_path) as r_writer:\n        for current_image, (image_path, objects) in enumerate(groups, 1):\n            print(\n                f\'\\rBuilding example: {current_image}/{len(groups)} ... \'\n                f\'{os.path.split(image_path)[-1]} \'\n                f\'{round(100 * (current_image / len(groups)))}% completed\',\n                end=\'\',\n            )\n            separate_data = pd.DataFrame(\n                objects, columns=data.columns\n            ).T.to_numpy()\n            (\n                image_width,\n                image_height,\n                x_min,\n                y_min,\n                x_max,\n                y_max,\n            ) = separate_data[2:8]\n            x_min /= image_width\n            x_max /= image_width\n            y_min /= image_height\n            y_max /= image_height\n            try:\n                image_data = open(image_path, \'rb\').read()\n                key = hashlib.sha256(image_data).hexdigest()\n                training_example = create_example(\n                    separate_data, key, image_data\n                )\n                r_writer.write(training_example.SerializeToString())\n            except Exception as e:\n                default_logger.error(e)\n    print()\n\n\ndef save_tfr(data, output_folder, dataset_name, test_size=None, trainer=None):\n    """"""\n    Transform and save dataset into TFRecord format.\n    Args:\n        data: pandas DataFrame with adjusted labels.\n        output_folder: Path to folder where TFRecord(s) will be saved.\n        dataset_name: str name of the dataset.\n        test_size: relative test subset size.\n        trainer: Main.Trainer object\n\n    Returns:\n        None\n    """"""\n    data[\'Object Name\'] = data[\'Object Name\'].apply(\n        lambda x: x.encode(\'utf-8\')\n    )\n    data[\'Object ID\'] = data[\'Object ID\'].astype(int)\n    data[data.dtypes[data.dtypes == \'int64\'].index] = data[\n        data.dtypes[data.dtypes == \'int64\'].index\n    ].apply(abs)\n    data.to_csv(\n        os.path.join(\'..\', \'Data\', \'TFRecords\', \'full_data.csv\'), index=False\n    )\n    groups = np.array(data.groupby(\'Image Path\'))\n    np.random.shuffle(groups)\n    if test_size:\n        assert (\n            0 < test_size < 1\n        ), f\'test_size must be 0 < test_size < 1 and {test_size} is given\'\n        separation_index = int((1 - test_size) * len(groups))\n        training_set = groups[:separation_index]\n        test_set = groups[separation_index:]\n        training_frame = pd.concat([item[1] for item in training_set])\n        test_frame = pd.concat([item[1] for item in test_set])\n        training_frame.to_csv(\n            os.path.join(\'..\', \'Data\', \'TFRecords\', \'training_data.csv\'),\n            index=False,\n        )\n        test_frame.to_csv(\n            os.path.join(\'..\', \'Data\', \'TFRecords\', \'test_data.csv\'),\n            index=False,\n        )\n        training_path = str(\n            Path(os.path.join(output_folder, f\'{dataset_name}_train.tfrecord\'))\n            .absolute()\n            .resolve()\n        )\n        test_path = str(\n            Path(os.path.join(output_folder, f\'{dataset_name}_test.tfrecord\'))\n            .absolute()\n            .resolve()\n        )\n        write_tf_record(training_path, training_set, data, trainer)\n        default_logger.info(f\'Saved training TFRecord: {training_path}\')\n        write_tf_record(test_path, test_set, data, trainer)\n        default_logger.info(f\'Saved validation TFRecord: {test_path}\')\n        return\n    tf_record_path = os.path.join(output_folder, f\'{dataset_name}.tfrecord\')\n    write_tf_record(tf_record_path, groups, data, trainer)\n    default_logger.info(f\'Saved TFRecord {tf_record_path}\')\n\n\ndef read_tfr(\n    tf_record_file,\n    classes_file,\n    feature_map,\n    max_boxes,\n    classes_delimiter=\'\\n\',\n    new_size=None,\n    get_features=False,\n):\n    """"""\n    Read and load dataset from TFRecord file.\n    Args:\n        tf_record_file: Path to TFRecord file.\n        classes_file: file containing classes.\n        feature_map: A dictionary of feature names mapped to tf.io objects.\n        max_boxes: Maximum number of boxes per image.\n        classes_delimiter: delimiter in classes_file.\n        new_size: w, h new image size\n        get_features: If True, features will be returned.\n\n    Returns:\n        MapDataset object.\n    """"""\n    tf_record_file = str(Path(tf_record_file).absolute().resolve())\n    text_init = tf.lookup.TextFileInitializer(\n        classes_file, tf.string, 0, tf.int64, -1, delimiter=classes_delimiter\n    )\n    class_table = tf.lookup.StaticHashTable(text_init, -1)\n    files = tf.data.Dataset.list_files(tf_record_file)\n    dataset = files.flat_map(tf.data.TFRecordDataset)\n    default_logger.info(f\'Read TFRecord: {tf_record_file}\')\n    return dataset.map(\n        lambda x: read_example(\n            x, feature_map, class_table, max_boxes, new_size, get_features\n        )\n    )\n'"
Helpers/utils.py,4,"b'import tensorflow as tf\nfrom tensorflow.keras.losses import (\n    sparse_categorical_crossentropy,\n    binary_crossentropy,\n)\nimport logging\nfrom logging import handlers\nfrom time import perf_counter\nimport os\nimport numpy as np\nimport pandas as pd\nfrom xml.etree.ElementTree import SubElement\nfrom xml.etree import ElementTree\nfrom lxml import etree\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.python.util.tf_export import keras_export\nimport tensorflow_addons as tfa\nfrom tensorflow.python.keras.utils import tf_utils\n\ntfa.options.TF_ADDONS_PY_OPS = True\n\n\ndef get_logger():\n    """"""\n    Initialize logger configuration.\n\n    Returns:\n        logger.\n    """"""\n    formatter = logging.Formatter(\n        \'%(asctime)s %(name)s.%(funcName)s +%(lineno)s: \'\n        \'%(levelname)-8s [%(process)d] %(message)s\'\n    )\n    logger = logging.getLogger(\'session_log\')\n    logger.setLevel(logging.DEBUG)\n    file_title = os.path.join(\'Logs\', \'session.log\')\n    if \'Logs\' not in os.listdir():\n        file_title = f\'{os.path.join("".."", file_title)}\'\n    file_handler = handlers.RotatingFileHandler(file_title, backupCount=10)\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(formatter)\n    logger.addHandler(console_handler)\n    return logger\n\n\ndefault_logger = get_logger()\n\n\ndef timer(logger):\n    """"""\n    Timer wrapper.\n    logger: logging.RootLogger object\n\n    Returns:\n        timed\n    """"""\n\n    def timed(func):\n        def wrapper(*args, **kwargs):\n            start_time = perf_counter()\n            result = func(*args, **kwargs)\n            total_time = perf_counter() - start_time\n            if logger is not None:\n                logger.info(\n                    f\'{func.__name__} execution time: \' f\'{total_time} seconds\'\n                )\n            if result is not None:\n                return result\n\n        return wrapper\n\n    return timed\n\n\ndef ratios_to_coordinates(bx, by, bw, bh, width, height):\n    """"""\n    Convert relative coordinates to actual coordinates.\n    Args:\n        bx: Relative center x coordinate.\n        by: Relative center y coordinate.\n        bw: Relative box width.\n        bh: Relative box height.\n        width: Image batch width.\n        height: Image batch height.\n\n    Return:\n        x1: x coordinate.\n        y1: y coordinate.\n        x2: x1 + Bounding box width.\n        y2: y1 + Bounding box height.\n    """"""\n    w, h = bw * width, bh * height\n    x, y = bx * width + (w / 2), by * height + (h / 2)\n    return x, y, x + w, y + h\n\n\ndef transform_images(x_train, size):\n    """"""\n    Resize image tensor.\n    Args:\n        x_train: Image tensor.\n        size: new (width, height)\n    """"""\n    x_train = tf.image.resize(x_train, (size, size))\n    return x_train / 255\n\n\n@tf.function\ndef transform_targets_for_output(y_true, grid_size, anchor_idxs):\n    n = tf.shape(y_true)[0]\n    y_true_out = tf.zeros(\n        (n, grid_size, grid_size, tf.shape(anchor_idxs)[0], 6)\n    )\n    anchor_idxs = tf.cast(anchor_idxs, tf.int32)\n    indexes = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n    idx = 0\n    for i in tf.range(n):\n        for j in tf.range(tf.shape(y_true)[1]):\n            if tf.equal(y_true[i][j][2], 0):\n                continue\n            anchor_eq = tf.equal(\n                anchor_idxs, tf.cast(y_true[i][j][5], tf.int32)\n            )\n            if tf.reduce_any(anchor_eq):\n                box = y_true[i][j][0:4]\n                box_xy = (y_true[i][j][0:2] + y_true[i][j][2:4]) / 2\n                anchor_idx = tf.cast(tf.where(anchor_eq), tf.int32)\n                grid_xy = tf.cast(box_xy // (1 / grid_size), tf.int32)\n                indexes = indexes.write(\n                    idx, [i, grid_xy[1], grid_xy[0], anchor_idx[0][0]]\n                )\n                updates = updates.write(\n                    idx, [box[0], box[1], box[2], box[3], 1, y_true[i][j][4]]\n                )\n                idx += 1\n    return tf.tensor_scatter_nd_update(\n        y_true_out, indexes.stack(), updates.stack()\n    )\n\n\ndef transform_targets(y_train, anchors, anchor_masks, size):\n    y_outs = []\n    grid_size = size // 32\n    anchors = tf.cast(anchors, tf.float32)\n    anchor_area = anchors[..., 0] * anchors[..., 1]\n    box_wh = y_train[..., 2:4] - y_train[..., 0:2]\n    box_wh = tf.tile(\n        tf.expand_dims(box_wh, -2), (1, 1, tf.shape(anchors)[0], 1)\n    )\n    box_area = box_wh[..., 0] * box_wh[..., 1]\n    intersection = tf.minimum(box_wh[..., 0], anchors[..., 0]) * tf.minimum(\n        box_wh[..., 1], anchors[..., 1]\n    )\n    iou = intersection / (box_area + anchor_area - intersection)\n    anchor_idx = tf.cast(tf.argmax(iou, axis=-1), tf.float32)\n    anchor_idx = tf.expand_dims(anchor_idx, axis=-1)\n    y_train = tf.concat([y_train, anchor_idx], axis=-1)\n    for anchor_idxs in anchor_masks:\n        y_outs.append(\n            transform_targets_for_output(y_train, grid_size, anchor_idxs)\n        )\n        grid_size *= 2\n    return tuple(y_outs)\n\n\ndef broadcast_iou(box_1, box_2):\n    box_1 = tf.expand_dims(box_1, -2)\n    box_2 = tf.expand_dims(box_2, 0)\n    new_shape = tf.broadcast_dynamic_shape(tf.shape(box_1), tf.shape(box_2))\n    box_1 = tf.broadcast_to(box_1, new_shape)\n    box_2 = tf.broadcast_to(box_2, new_shape)\n    int_w = tf.maximum(\n        tf.minimum(box_1[..., 2], box_2[..., 2])\n        - tf.maximum(box_1[..., 0], box_2[..., 0]),\n        0,\n    )\n    int_h = tf.maximum(\n        tf.minimum(box_1[..., 3], box_2[..., 3])\n        - tf.maximum(box_1[..., 1], box_2[..., 1]),\n        0,\n    )\n    int_area = int_w * int_h\n    box_1_area = (box_1[..., 2] - box_1[..., 0]) * (\n        box_1[..., 3] - box_1[..., 1]\n    )\n    box_2_area = (box_2[..., 2] - box_2[..., 0]) * (\n        box_2[..., 3] - box_2[..., 1]\n    )\n    return int_area / (box_1_area + box_2_area - int_area)\n\n\ndef get_boxes(pred, anchors, classes):\n    grid_size = tf.shape(pred)[1]\n    box_xy, box_wh, object_probability, class_probabilities = tf.split(\n        pred, (2, 2, 1, classes), axis=-1\n    )\n    box_xy = tf.sigmoid(box_xy)\n    object_probability = tf.sigmoid(object_probability)\n    class_probabilities = tf.sigmoid(class_probabilities)\n    pred_box = tf.concat((box_xy, box_wh), axis=-1)\n    grid = tf.meshgrid(tf.range(grid_size), tf.range(grid_size))\n    grid = tf.expand_dims(tf.stack(grid, axis=-1), axis=2)\n    box_xy = (box_xy + tf.cast(grid, tf.float32)) / tf.cast(\n        grid_size, tf.float32\n    )\n    box_wh = tf.exp(box_wh) * anchors\n    box_x1y1 = box_xy - box_wh / 2\n    box_x2y2 = box_xy + box_wh / 2\n    bbox = tf.concat([box_x1y1, box_x2y2], axis=-1)\n    return bbox, object_probability, class_probabilities, pred_box\n\n\ndef calculate_loss(anchors, classes=80, ignore_thresh=0.5):\n    def yolo_loss(y_true, y_pred):\n        pred_box, pred_obj, pred_class, pred_xywh = get_boxes(\n            y_pred, anchors, classes\n        )\n        pred_xy = pred_xywh[..., 0:2]\n        pred_wh = pred_xywh[..., 2:4]\n        true_box, true_obj, true_class_idx = tf.split(\n            y_true, (4, 1, 1), axis=-1\n        )\n        true_xy = (true_box[..., 0:2] + true_box[..., 2:4]) / 2\n        true_wh = true_box[..., 2:4] - true_box[..., 0:2]\n        box_loss_scale = 2 - true_wh[..., 0] * true_wh[..., 1]\n        grid_size = tf.shape(y_true)[1]\n        grid = tf.meshgrid(tf.range(grid_size), tf.range(grid_size))\n        grid = tf.expand_dims(tf.stack(grid, axis=-1), axis=2)\n        true_xy = true_xy * tf.cast(grid_size, tf.float32) - tf.cast(\n            grid, tf.float32\n        )\n        true_wh = tf.math.log(true_wh / anchors)\n        true_wh = tf.where(\n            tf.math.is_inf(true_wh), tf.zeros_like(true_wh), true_wh\n        )\n        obj_mask = tf.squeeze(true_obj, -1)\n        best_iou = tf.map_fn(\n            lambda x: tf.reduce_max(\n                broadcast_iou(\n                    x[0], tf.boolean_mask(x[1], tf.cast(x[2], tf.bool))\n                ),\n                axis=-1,\n            ),\n            (pred_box, true_box, obj_mask),\n            tf.float32,\n        )\n        ignore_mask = tf.cast(best_iou < ignore_thresh, tf.float32)\n        xy_loss = (\n            obj_mask\n            * box_loss_scale\n            * tf.reduce_sum(tf.square(true_xy - pred_xy), axis=-1)\n        )\n        wh_loss = (\n            obj_mask\n            * box_loss_scale\n            * tf.reduce_sum(tf.square(true_wh - pred_wh), axis=-1)\n        )\n        obj_loss = binary_crossentropy(true_obj, pred_obj)\n        obj_loss = (\n            obj_mask * obj_loss + (1 - obj_mask) * ignore_mask * obj_loss\n        )\n        class_loss = obj_mask * sparse_categorical_crossentropy(\n            true_class_idx, pred_class\n        )\n        xy_loss = tf.reduce_sum(xy_loss, axis=(1, 2, 3))\n        wh_loss = tf.reduce_sum(wh_loss, axis=(1, 2, 3))\n        obj_loss = tf.reduce_sum(obj_loss, axis=(1, 2, 3))\n        class_loss = tf.reduce_sum(class_loss, axis=(1, 2, 3))\n        return xy_loss + wh_loss + obj_loss + class_loss\n\n    return yolo_loss\n\n\ndef add_xml_path(xml_file, path):\n    """"""\n    Add a path element to the xml file and save.\n    Args:\n        xml_file: .xml file path.\n        path: str, path to add.\n\n    Returns:\n        None\n    """"""\n    tree = ElementTree.parse(xml_file)\n    top = tree.getroot()\n    folder_tag = tree.find(\'folder\')\n    folder_tag.text = path\n    file_name_tag = tree.find(\'filename\')\n    path_tag = SubElement(top, \'path\')\n    path_tag.text = os.path.join(folder_tag.text, file_name_tag.text)\n    rough_string = ElementTree.tostring(top, \'utf8\')\n    root = etree.fromstring(rough_string)\n    pretty = etree.tostring(root, pretty_print=True, encoding=\'utf-8\').replace(\n        \'  \'.encode(), \'\\t\'.encode()\n    )\n    os.remove(xml_file)\n    with open(xml_file, \'wb\') as output:\n        output.write(pretty)\n\n\ndef get_detection_data(image, image_name, outputs, class_names):\n    """"""\n    Organize predictions of a single image into a pandas DataFrame.\n    Args:\n        image: Image as a numpy array.\n        image_name: str, name to write in the image column.\n        outputs: Outputs from inference_model.predict()\n        class_names: A list of object class names.\n\n    Returns:\n        data: pandas DataFrame with the detections.\n    """"""\n    nums = outputs[-1]\n    boxes, scores, classes = 3 * [None]\n    if isinstance(outputs[0], np.ndarray):\n        boxes, scores, classes = [\n            item[0][: int(nums)] for item in outputs[:-1]\n        ]\n    if not isinstance(outputs[0], np.ndarray):\n        boxes, scores, classes = [\n            item[0][: int(nums)].numpy() for item in outputs[:-1]\n        ]\n    w, h = np.flip(image.shape[0:2])\n    data = pd.DataFrame(boxes, columns=[\'x1\', \'y1\', \'x2\', \'y2\'])\n    data[[\'x1\', \'x2\']] = (data[[\'x1\', \'x2\']] * w).astype(\'int64\')\n    data[[\'y1\', \'y2\']] = (data[[\'y1\', \'y2\']] * h).astype(\'int64\')\n    data[\'object_name\'] = np.array(class_names)[classes.astype(\'int64\')]\n    data[\'image\'] = image_name\n    data[\'score\'] = scores\n    data[\'image_width\'] = w\n    data[\'image_height\'] = h\n    data = data[\n        [\n            \'image\',\n            \'object_name\',\n            \'x1\',\n            \'y1\',\n            \'x2\',\n            \'y2\',\n            \'score\',\n            \'image_width\',\n            \'image_height\',\n        ]\n    ]\n    return data\n\n\ndef activate_gpu():\n    """"""\n    Check for GPU existence and if found, activate.\n\n    Returns:\n        None\n    """"""\n    physical_devices = tf.config.experimental.list_physical_devices(\'GPU\')\n    if len(physical_devices) > 0:\n        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n        default_logger.info(\'GPU activated\')\n\n\ndef calculate_ratios(x1, y1, x2, y2, width, height):\n    """"""\n    Calculate relative object ratios in the labeled image.\n    Args:\n        x1: Start x coordinate.\n        y1: Start y coordinate.\n        x2: End x coordinate.\n        y2: End y coordinate.\n        width: Bounding box width.\n        height: Bounding box height.\n\n    Return:\n        bx: Relative center x coordinate.\n        by: Relative center y coordinate.\n        bw: Relative box width.\n        bh: Relative box height.\n    """"""\n    box_width = abs(x2 - x1)\n    box_height = abs(y2 - y1)\n    bx = 1 - ((width - min(x1, x2) + (box_width / 2)) / width)\n    by = 1 - ((height - min(y1, y2) + (box_height / 2)) / height)\n    bw = box_width / width\n    bh = box_height / height\n    return bx, by, bw, bh\n\n\ndef calculate_display_data(\n    prediction_file, classes_file, img_width, img_height, out\n):\n    """"""\n    Convert coordinates to relative labels.\n\n    prediction_file: csv file containing label coordinates.\n    classes_file: .txt file containing object classes.\n    img_width: Image width.\n    img_height: Image height.\n    out: Output path.\n\n    Returns:\n         None\n    """"""\n    preds = pd.read_csv(prediction_file)\n    rows = []\n    indices = {\n        item: ind\n        for ind, item in enumerate(\n            [item.strip() for item in open(classes_file).readlines()]\n        )\n    }\n    for ind, item in preds.iterrows():\n        img, obj, xx1, yy1, xx2, yy2, score, imgw, imgh, dk = item.values\n        bxx, byy, bww, bhh = calculate_ratios(\n            xx1, yy1, xx2, yy2, img_width, img_height\n        )\n        rows.append([img, obj, indices[obj], bxx, byy, bww, bhh])\n    fr = pd.DataFrame(\n        rows,\n        columns=[\n            \'Image\',\n            \'Object Name\',\n            \'Object Index\',\n            \'bx\',\n            \'by\',\n            \'bw\',\n            \'bh\',\n        ],\n    )\n    fr.to_csv(out, index=False)\n\n\n@keras_export(\'keras.layers.Mish\')\nclass Mish(Layer):\n    """"""\n    Mish new activation function\n    """"""\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.supports_masking = True\n\n    def call(self, inputs, *args, **kwargs):\n        return tfa.activations.mish(inputs)\n\n    @tf_utils.shape_type_conversion\n    def compute_output_shape(self, input_shape):\n        return input_shape\n'"
Helpers/visual_tools.py,1,"b'import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport imagesize\nimport cv2\nimport os\nimport sys\nfrom pathlib import Path\nfrom Helpers.utils import default_logger\n\nif sys.platform == \'darwin\':\n    plt.switch_backend(\'Qt5Agg\')\n\n\ndef save_fig(title, save_figures=True):\n    """"""\n    Save generated figures to Output folder.\n    Args:\n        title: Figure title also the image to save file name.\n        save_figures: If True, figure will be saved\n\n    Returns:\n        None\n    """"""\n    if save_figures:\n        saving_path = str(\n            Path(os.path.join(\'..\', \'Output\', \'Plots\', f\'{title}.png\'))\n            .absolute()\n            .resolve()\n        )\n        if os.path.exists(saving_path):\n            return\n        plt.savefig(saving_path)\n        default_logger.info(f\'Saved figure {saving_path}\')\n        plt.close()\n\n\ndef visualize_box_relative_sizes(frame, save_result=True):\n    """"""\n    Scatter plot annotation box relative sizes.\n    Args:\n        frame: pandas DataFrame with the annotation data.\n        save_result: If True, figure will be saved\n\n    Returns:\n        None\n    """"""\n    title = f\'Relative width and height for {frame.shape[0]} boxes.\'\n    if os.path.exists(\n        os.path.join(\'..\', \'Output\', \'Plots\', f\'{title}.png\')\n    ) or (frame is None):\n        return\n    sns.scatterplot(\n        x=frame[\'Relative Width\'],\n        y=frame[\'Relative Height\'],\n        hue=frame[\'Object Name\'],\n        palette=\'gist_rainbow\',\n    )\n    plt.title(title)\n    save_fig(title, save_result)\n\n\ndef visualize_k_means_output(centroids, frame, save_result=True):\n    """"""\n    Visualize centroids and anchor box dimensions calculated.\n    Args:\n        centroids: 2D array of shape(k, 2) output of k-means.\n        frame: pandas DataFrame with the annotation data.\n        save_result: If True, figure will be saved\n\n    Returns:\n        None\n    """"""\n    title = (\n        f\'{centroids.shape[0]} Centroids representing relative anchor sizes.\'\n    )\n    if os.path.exists(\n        os.path.join(\'..\', \'Output\', \'Plots\', f\'{title}.png\')\n    ) or (frame is None):\n        return\n    fig, ax = plt.subplots()\n    visualize_box_relative_sizes(frame)\n    plt.title(title)\n    ax.scatter(centroids[:, 0], centroids[:, 1], marker=\'*\', s=200, c=\'black\')\n    save_fig(title, save_result)\n\n\ndef visualize_boxes(relative_anchors, sample_image, save_result=True):\n    """"""\n    Visualize anchor boxes output of k-means.\n    Args:\n        relative_anchors: Output of k-means.\n        sample_image: Path to image to display as background.\n        save_result: If True, figure will be saved\n\n    Returns:\n        None\n    """"""\n    title = \'Generated anchors relative to sample image size\'\n    if os.path.exists(os.path.join(\'..\', \'Output\', \'Plots\', f\'{title}.png\')):\n        return\n    img = cv2.imread(sample_image)\n    width, height = imagesize.get(sample_image)\n    center = int(width / 2), int(height / 2)\n    for relative_w, relative_h in relative_anchors:\n        box_width = relative_w * width\n        box_height = relative_h * height\n        x0 = int(center[0] - (box_width / 2))\n        y0 = int(center[1] - (box_height / 2))\n        x1 = int(x0 + box_width)\n        y1 = int(y0 + box_height)\n        cv2.rectangle(img, (x0, y0), (x1, y1), (255, 0, 0), 4)\n    plt.imshow(img)\n    plt.grid()\n    plt.title(title)\n    save_fig(title, save_result)\n\n\ndef visualize_pr(calculated, save_results=True, fig_prefix=\'\'):\n    """"""\n    Visualize precision and recall curves(post-training evaluation)\n    Args:\n        calculated: pandas DataFrame with combined average precisions \n            that were calculated separately on each object class.\n        save_results: If True, plots will be saved to Output folder.\n        fig_prefix: str, prefix to add to save path.\n\n    Returns:\n        None\n    """"""\n    for item in calculated[\'object_name\'].drop_duplicates().values:\n        plt.figure()\n        title = (\n            f\'{fig_prefix} Precision and recall curve for {fig_prefix} {item}\'\n        )\n        plt.title(title)\n        recall = calculated[calculated[\'object_name\'] == item][\'recall\'].values\n        precision = calculated[calculated[\'object_name\'] == item][\n            \'precision\'\n        ].values\n        plt.plot(recall, precision)\n        plt.xlabel(\'recall\')\n        plt.ylabel(\'precision\')\n        plt.title(title)\n        save_fig(title, save_results)\n\n\ndef plot_compare_bar(col1, stats, fig_prefix=\'\', col2=None):\n    """"""\n    Plot a bar chart comparison between 2 evaluation statistics or 1 statistic.\n    Args:\n        col1: column name present in frame.\n        stats: pandas DataFrame with evaluation statistics.\n        fig_prefix: str, prefix to add to save path.\n        col2: second column name present in frame to compare with col1.\n\n    Returns:\n        None\n    """"""\n    stats = stats.sort_values(by=col1)\n    ind = np.arange(len(stats))\n    width = 0.4\n    fig, ax = plt.subplots(figsize=(9, 6))\n    ax.barh(ind, stats[col1], width, color=\'red\', label=col1)\n    if col2:\n        ax.barh(ind + width, stats[col2], width, color=\'blue\', label=col2)\n\n    ax.set(\n        yticks=ind + width,\n        yticklabels=stats[\'Class Name\'],\n        ylim=[2 * width - 1, len(stats)],\n        title=(\n            f\'{fig_prefix} {col1} vs {col2} evaluation results\'\n            if col2\n            else f\'{fig_prefix} {col1} evaluation results\'\n        ),\n    )\n    for patch in ax.patches:\n        pw = patch.get_width()\n        _, y = patch.get_xy()\n        color = patch.get_facecolor()\n        ax.text(\n            pw + 3,\n            y + width / 2,\n            str(pw),\n            color=color,\n            verticalalignment=\'center\',\n        )\n    ax.legend(loc=\'lower right\')\n\n\ndef visualize_evaluation_stats(stats, fig_prefix=\'\', save_results=True):\n    """"""\n    Visualize True positives vs False positives, actual vs. detections\n        and average precision.\n    Args:\n        stats: pandas DataFrame with evaluation statistics.\n        fig_prefix: str, prefix to add to save path.\n        save_results: If True, plots will be saved to Output folder.\n\n    Returns:\n        None\n    """"""\n    plot_compare_bar(\'True Positives\', stats, fig_prefix, \'False Positives\')\n    save_fig(\'True positives vs False positives.png\', save_results)\n    plot_compare_bar(\'Actual\', stats, fig_prefix, \'Detections\')\n    save_fig(\'Actual vs Detections.png\', save_results)\n    plot_compare_bar(\'Average Precision\', stats, fig_prefix)\n    save_fig(f\'{fig_prefix} Average Precision.png\', save_results)\n\n\ndef visualization_wrapper(to_visualize):\n    """"""\n    Wrapper for visualization.\n    Args:\n        to_visualize: function to visualize.\n\n    Returns:\n        to_visualize\n    """"""\n\n    def visualized(*args, **kwargs):\n        result = to_visualize(*args, **kwargs)\n        if to_visualize.__name__ in [\'parse_voc_folder\', \'adjust_non_voc_csv\']:\n            visualize_box_relative_sizes(result)\n            plt.show()\n        if to_visualize.__name__ == \'k_means\':\n            all_args = list(kwargs.values()) + list(args)\n            if not any([isinstance(item, pd.DataFrame) for item in all_args]):\n                return result\n            visualize_k_means_output(*result)\n            plt.show()\n            visualize_boxes(\n                result[0], os.path.join(\'..\', \'Samples\', \'sample_image.png\')\n            )\n            plt.show()\n        return result\n\n    return visualized\n'"
Main/detector.py,2,"b'import numpy as np\nimport cv2\nimport tensorflow as tf\nimport os\nimport sys\n\nsys.path.append(\'..\')\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom Main.models import BaseModel\nfrom Helpers.utils import (\n    get_detection_data,\n    activate_gpu,\n    transform_images,\n    default_logger,\n    timer,\n)\n\n\nclass Detector(BaseModel):\n    """"""Tool for detection on photos/videos""""""\n\n    def __init__(\n        self,\n        input_shape,\n        model_configuration,\n        classes_file,\n        anchors=None,\n        masks=None,\n        max_boxes=100,\n        iou_threshold=0.5,\n        score_threshold=0.5,\n    ):\n        """"""\n        Initialize detection settings.\n\n        Args:\n            input_shape: tuple, (n, n, c)\n            model_configuration: Path to DarkNet cfg file.\n            classes_file: File containing class names \\n delimited.\n            anchors: numpy array of (w, h) pairs.\n            masks: numpy array of masks.\n            max_boxes: Maximum boxes of the TFRecords provided(if any) or\n                maximum boxes setting.\n            iou_threshold: float, values less than the threshold are ignored.\n            score_threshold: float, values less than the threshold are ignored.\n        """"""\n        self.class_names = [\n            item.strip() for item in open(classes_file).readlines()\n        ]\n        self.box_colors = {\n            class_name: color\n            for class_name, color in zip(\n                self.class_names,\n                [\n                    list(np.random.random(size=3) * 256)\n                    for _ in range(len(self.class_names))\n                ],\n            )\n        }\n        super().__init__(\n            input_shape=input_shape,\n            model_configuration=model_configuration,\n            classes=len(self.class_names),\n            anchors=anchors,\n            masks=masks,\n            max_boxes=max_boxes,\n            iou_threshold=iou_threshold,\n            score_threshold=score_threshold,\n        )\n        activate_gpu()\n\n    def detect_image(self, image_data, image_name):\n        """"""\n        Given an image, get detections for the image.\n        Args:\n            image_data: image as numpy array/tf.Tensor\n            image_name: str, name of the image\n\n        Returns:\n            pandas DataFrame with detections found.\n        """"""\n        image = tf.expand_dims(image_data, 0)\n        resized = transform_images(image, self.input_shape[0])\n        out = self.inference_model.predict(resized)\n        if isinstance(image_data, np.ndarray):\n            adjusted = cv2.cvtColor(image_data, cv2.COLOR_RGB2BGR)\n        else:\n            adjusted = cv2.cvtColor(image_data.numpy(), cv2.COLOR_RGB2BGR)\n        detections = get_detection_data(\n            adjusted, image_name, out, self.class_names,\n        )\n        return detections, adjusted\n\n    def draw_on_image(self, adjusted, detections):\n        """"""\n        Draw bounding boxes over the image.\n        Args:\n            adjusted: BGR image.\n            detections: pandas DataFrame containing detections\n\n        Returns:\n            None\n        """"""\n        for index, row in detections.iterrows():\n            img, obj, x1, y1, x2, y2, score, *_ = row.values\n            color = self.box_colors.get(obj)\n            cv2.rectangle(adjusted, (x1, y1), (x2, y2), color, 2)\n            cv2.putText(\n                adjusted,\n                f\'{obj}-{round(score, 2)}\',\n                (x1, y1 - 10),\n                cv2.FONT_HERSHEY_COMPLEX_SMALL,\n                0.6,\n                color,\n                1,\n            )\n\n    def predict_on_image(self, image_path):\n        """"""\n        Detect, draw detections and save result to Output folder.\n        Args:\n            image_path: Path to image.\n\n        Returns:\n            None\n        """"""\n        image_name = os.path.basename(image_path)\n        image_data = tf.image.decode_image(\n            open(image_path, \'rb\').read(), channels=3\n        )\n        detections, adjusted = self.detect_image(image_data, image_name)\n        self.draw_on_image(adjusted, detections)\n        saving_path = os.path.join(\n            \'..\', \'Output\', \'Detections\', f\'predicted-{image_name}\'\n        )\n        cv2.imwrite(saving_path, adjusted)\n\n    @timer(default_logger)\n    def predict_photos(\n        self, photos, trained_weights, batch_size=32, workers=16\n    ):\n        """"""\n        Predict a list of image paths and save results to output folder.\n        Args:\n            photos: A list of image paths.\n            trained_weights: .weights or .tf file\n            batch_size: Prediction batch size.\n            workers: Parallel predictions.\n\n        Returns:\n            None\n        """"""\n        self.create_models()\n        self.load_weights(trained_weights)\n        to_predict = photos.copy()\n        with ThreadPoolExecutor(max_workers=workers) as executor:\n            predicted = 1\n            done = []\n            total_photos = len(photos)\n            while to_predict:\n                current_batch = [\n                    to_predict.pop() for _ in range(batch_size) if to_predict\n                ]\n                future_predictions = {\n                    executor.submit(self.predict_on_image, image): image\n                    for image in current_batch\n                }\n                for future_prediction in as_completed(future_predictions):\n                    future_prediction.result()\n                    completed = f\'{predicted}/{total_photos}\'\n                    current_image = future_predictions[future_prediction]\n                    percent = (predicted / total_photos) * 100\n                    print(\n                        f\'\\rpredicting {os.path.basename(current_image)} \'\n                        f\'{completed}\\t{percent}% completed\',\n                        end=\'\',\n                    )\n                    predicted += 1\n                    done.append(current_image)\n            for item in done:\n                default_logger.info(f\'Saved prediction: {item}\')\n\n    @timer(default_logger)\n    def detect_video(\n        self, video, trained_weights, codec=\'mp4v\', display=False\n    ):\n        """"""\n        Perform detection on a video, stream(optional) and save results.\n        Args:\n            video: Path to video file.\n            trained_weights: .tf or .weights file\n            codec: str ex: mp4v\n            display: If True, detections will be displayed during\n                the detection operation.\n\n        Returns:\n            None\n        """"""\n        self.create_models()\n        self.load_weights(trained_weights)\n        vid = cv2.VideoCapture(video)\n        length = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n        width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        fps = int(vid.get(cv2.CAP_PROP_FPS))\n        current = 1\n        codec = cv2.VideoWriter_fourcc(*codec)\n        out = os.path.join(\'..\', \'Output\', \'Detections\', \'predicted_vid.mp4\')\n        writer = cv2.VideoWriter(out, codec, fps, (width, height))\n        while vid.isOpened():\n            _, frame = vid.read()\n            detections, adjusted = self.detect_image(frame, f\'frame_{current}\')\n            self.draw_on_image(adjusted, detections)\n            writer.write(adjusted)\n            completed = f\'{(current / length) * 100}% completed\'\n            print(\n                f\'\\rframe {current}/{length}\\tdetections: \'\n                f\'{len(detections)}\\tcompleted: {completed}\',\n                end=\'\',\n            )\n            if display:\n                cv2.imshow(f\'frame {current}\', adjusted)\n            current += 1\n            if cv2.waitKey(1) == ord(\'q\'):\n                default_logger.info(\n                    f\'Video detection stopped by user {current}/{length} \'\n                    f\'frames completed\'\n                )\n                break\n'"
Main/evaluator.py,2,"b'import cv2\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport os\nimport sys\n\nsys.path.append(\'..\')\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom Main.models import BaseModel\nfrom Helpers.dataset_handlers import read_tfr, get_feature_map\nfrom Helpers.utils import (\n    transform_images,\n    get_detection_data,\n    default_logger,\n    timer,\n)\nfrom Helpers.visual_tools import visualize_pr, visualize_evaluation_stats\n\n\nclass Evaluator(BaseModel):\n    def __init__(\n        self,\n        input_shape,\n        model_configuration,\n        train_tf_record,\n        valid_tf_record,\n        classes_file,\n        anchors=None,\n        masks=None,\n        max_boxes=100,\n        iou_threshold=0.5,\n        score_threshold=0.5,\n    ):\n        """"""\n        Evaluate a trained model.\n        Args:\n            input_shape: input_shape: tuple, (n, n, c)\n            model_configuration: Path to model configuration file.\n            train_tf_record: Path to training TFRecord file.\n            valid_tf_record: Path to validation TFRecord file.\n            classes_file: File containing class names \\n delimited.\n            anchors: numpy array of (w, h) pairs.\n            masks: numpy array of masks.\n            max_boxes: Maximum boxes of the TFRecords provided.\n            iou_threshold: Minimum overlap value.\n            score_threshold: Minimum confidence for detection to count\n                as true positive.\n        """"""\n        self.classes_file = classes_file\n        self.class_names = [\n            item.strip() for item in open(classes_file).readlines()\n        ]\n        super().__init__(\n            input_shape,\n            model_configuration,\n            len(self.class_names),\n            anchors,\n            masks,\n            max_boxes,\n            iou_threshold,\n            score_threshold,\n        )\n        self.train_tf_record = train_tf_record\n        self.valid_tf_record = valid_tf_record\n        self.train_dataset_size = sum(\n            1 for _ in tf.data.TFRecordDataset(train_tf_record)\n        )\n        self.valid_dataset_size = sum(\n            1 for _ in tf.data.TFRecordDataset(valid_tf_record)\n        )\n        self.dataset_size = self.train_dataset_size + self.valid_dataset_size\n        self.predicted = 1\n\n    def predict_image(self, image_data, features):\n        """"""\n        Make predictions on a single image from the TFRecord.\n        Args:\n            image_data: image as numpy array\n            features: features of the TFRecord.\n\n        Returns:\n            pandas DataFrame with detection data.\n        """"""\n        image_path = bytes.decode(features[\'image_path\'].numpy())\n        image_name = os.path.basename(image_path)\n        image = tf.expand_dims(image_data, 0)\n        resized = transform_images(image, self.input_shape[0])\n        outs = self.inference_model(resized)\n        adjusted = cv2.cvtColor(image_data.numpy(), cv2.COLOR_RGB2BGR)\n        result = (\n            get_detection_data(adjusted, image_name, outs, self.class_names),\n            image_name,\n        )\n        return result\n\n    @staticmethod\n    def get_dataset_next(dataset):\n        try:\n            return next(dataset)\n        except tf.errors.UnknownError as e:  # sometimes encountered when reading from google drive\n            default_logger.error(\n                f\'Error occurred during reading from dataset\\n{e}\'\n            )\n\n    def predict_dataset(\n        self, dataset, workers=16, split=\'train\', batch_size=64\n    ):\n        """"""\n        Predict entire dataset.\n        Args:\n            dataset: MapDataset object.\n            workers: Parallel predictions.\n            split: str representation of the dataset \'train\' or \'valid\'\n            batch_size: Prediction batch size.\n\n        Returns:\n            pandas DataFrame with entire dataset predictions.\n        """"""\n        predictions = []\n        sizes = {\n            \'train\': self.train_dataset_size,\n            \'valid\': self.valid_dataset_size,\n        }\n        size = sizes[split]\n        current_prediction = 0\n        with ThreadPoolExecutor(max_workers=workers) as executor:\n            while current_prediction < size:\n                current_batch = []\n                for _ in range(min(batch_size, size - current_prediction)):\n                    item = self.get_dataset_next(dataset)\n                    if item is not None:\n                        current_batch.append(item)\n                future_predictions = {\n                    executor.submit(\n                        self.predict_image, img_data, features\n                    ): features[\'image_path\']\n                    for img_data, labels, features in current_batch\n                }\n                for future_prediction in as_completed(future_predictions):\n                    result, completed_image = future_prediction.result()\n                    predictions.append(result)\n                    completed = f\'{self.predicted}/{self.dataset_size}\'\n                    percent = (self.predicted / self.dataset_size) * 100\n                    print(\n                        f\'\\rpredicting {completed_image} {completed}\\t{percent}% completed\',\n                        end=\'\',\n                    )\n                    self.predicted += 1\n                    current_prediction += 1\n        return pd.concat(predictions)\n\n    @timer(default_logger)\n    def make_predictions(\n        self,\n        trained_weights,\n        merge=False,\n        workers=16,\n        shuffle_buffer=512,\n        batch_size=64,\n    ):\n        """"""\n        Make predictions on both training and validation data sets\n            and save results as csv in Output folder.\n        Args:\n            trained_weights: Trained .tf weights or .weights file(in case self.classes = 80).\n            merge: If True a single file will be saved for training\n                and validation sets predictions combined.\n            workers: Parallel predictions.\n            shuffle_buffer: int, shuffle dataset buffer size.\n            batch_size: Prediction batch size.\n\n        Returns:\n            1 combined pandas DataFrame for entire dataset predictions\n                or 2 pandas DataFrame(s) for training and validation\n                data sets respectively.\n        """"""\n        self.create_models()\n        self.load_weights(trained_weights)\n        features = get_feature_map()\n        train_dataset = read_tfr(\n            self.train_tf_record,\n            self.classes_file,\n            features,\n            self.max_boxes,\n            get_features=True,\n        )\n        valid_dataset = read_tfr(\n            self.valid_tf_record,\n            self.classes_file,\n            features,\n            self.max_boxes,\n            get_features=True,\n        )\n        train_dataset.shuffle(shuffle_buffer)\n        valid_dataset.shuffle(shuffle_buffer)\n        train_dataset = iter(train_dataset)\n        valid_dataset = iter(valid_dataset)\n        train_predictions = self.predict_dataset(\n            train_dataset, workers, \'train\', batch_size\n        )\n        valid_predictions = self.predict_dataset(\n            valid_dataset, workers, \'valid\', batch_size\n        )\n        if merge:\n            predictions = pd.concat([train_predictions, valid_predictions])\n            save_path = os.path.join(\n                \'..\', \'Output\', \'Data\', \'full_dataset_predictions.csv\'\n            )\n            predictions.to_csv(save_path, index=False)\n            return predictions\n        train_path = os.path.join(\n            \'..\', \'Output\', \'Data\', \'train_dataset_predictions.csv\'\n        )\n        valid_path = os.path.join(\n            \'..\', \'Output\', \'Data\', \'valid_dataset_predictions.csv\'\n        )\n        train_predictions.to_csv(train_path, index=False)\n        valid_predictions.to_csv(valid_path, index=False)\n        return train_predictions, valid_predictions\n\n    @staticmethod\n    def get_area(frame, columns):\n        """"""\n        Calculate bounding boxes areas.\n        Args:\n            frame: pandas DataFrame that contains prediction data.\n            columns: column names that represent x1, y1, x2, y2.\n\n        Returns:\n            pandas Series(area column)\n        """"""\n        x1, y1, x2, y2 = [frame[column] for column in columns]\n        return (x2 - x1) * (y2 - y1)\n\n    def get_true_positives(self, detections, actual, min_overlaps):\n        """"""\n        Filter True positive detections out of all detections.\n        Args:\n            detections: pandas DataFrame with all detections.\n            actual: pandas DataFrame with real data.\n            min_overlaps: a float value between 0 and 1, or a dictionary\n                containing each class in self.class_names mapped to its\n                minimum overlap\n\n        Returns:\n            pandas DataFrame that contains detections that satisfy\n                True positive constraints.\n        """"""\n        if detections.empty:\n            raise ValueError(f\'Empty predictions frame\')\n        if isinstance(min_overlaps, float):\n            assert 0 <= min_overlaps < 1, (\n                f\'min_overlaps should be \'\n                f\'between 0 and 1, {min_overlaps} is given\'\n            )\n        if isinstance(min_overlaps, dict):\n            assert all(\n                [0 < min_overlap < 1 for min_overlap in min_overlaps.values()]\n            )\n            assert all([obj in min_overlaps for obj in self.class_names]), (\n                f\'{[item for item in self.class_names if item not in min_overlaps]} \'\n                f\'are missing in min_overlaps\'\n            )\n        actual = actual.rename(\n            columns={\'Image Path\': \'image\', \'Object Name\': \'object_name\'}\n        )\n        actual[\'image\'] = actual[\'image\'].apply(lambda x: os.path.split(x)[-1])\n        random_gen = np.random.default_rng()\n        if \'detection_key\' not in detections.columns:\n            detection_keys = random_gen.choice(\n                len(detections), size=len(detections), replace=False\n            )\n            detections[\'detection_key\'] = detection_keys\n        total_frame = actual.merge(detections, on=[\'image\', \'object_name\'])\n        assert (\n            not total_frame.empty\n        ), \'No common image names found between actual and detections\'\n        total_frame[\'x_max_common\'] = total_frame[[\'X_max\', \'x2\']].min(1)\n        total_frame[\'x_min_common\'] = total_frame[[\'X_min\', \'x1\']].max(1)\n        total_frame[\'y_max_common\'] = total_frame[[\'Y_max\', \'y2\']].min(1)\n        total_frame[\'y_min_common\'] = total_frame[[\'Y_min\', \'y1\']].max(1)\n        true_intersect = (\n            total_frame[\'x_max_common\'] > total_frame[\'x_min_common\']\n        ) & (total_frame[\'y_max_common\'] > total_frame[\'y_min_common\'])\n        total_frame = total_frame[true_intersect]\n        actual_areas = self.get_area(\n            total_frame, [\'X_min\', \'Y_min\', \'X_max\', \'Y_max\']\n        )\n        predicted_areas = self.get_area(total_frame, [\'x1\', \'y1\', \'x2\', \'y2\'])\n        intersect_areas = self.get_area(\n            total_frame,\n            [\'x_min_common\', \'y_min_common\', \'x_max_common\', \'y_max_common\'],\n        )\n        iou_areas = intersect_areas / (\n            actual_areas + predicted_areas - intersect_areas\n        )\n        total_frame[\'iou\'] = iou_areas\n        if isinstance(min_overlaps, float):\n            return total_frame[total_frame[\'iou\'] >= min_overlaps]\n        if isinstance(min_overlaps, dict):\n            class_data = [\n                (name, total_frame[total_frame[\'object_name\'] == name])\n                for name in self.class_names\n            ]\n            thresholds = [min_overlaps[item[0]] for item in class_data]\n            frames = [\n                item[1][item[1][\'iou\'] >= threshold]\n                for (item, threshold) in zip(class_data, thresholds)\n                if not item[1].empty\n            ]\n            return pd.concat(frames)\n\n    @staticmethod\n    def get_false_positives(detections, true_positive):\n        """"""\n        Filter out False positives in all detections.\n        Args:\n            detections: pandas DataFrame with detection data.\n            true_positive: pandas DataFrame with True positive data.\n\n        Returns:\n            pandas DataFrame with False positives.\n        """"""\n        keys_before = detections[\'detection_key\'].values\n        keys_after = true_positive[\'detection_key\'].values\n        false_keys = np.where(np.isin(keys_before, keys_after, invert=True))\n        false_keys = keys_before[false_keys]\n        false_positives = detections.set_index(\'detection_key\').loc[false_keys]\n        return false_positives.reset_index()\n\n    @staticmethod\n    def combine_results(true_positive, false_positive):\n        """"""\n        Combine True positives and False positives.\n        Args:\n            true_positive: pandas DataFrame with True positive data.\n            false_positive: pandas DataFrame with False positive data.\n\n        Returns:\n            pandas DataFrame with all detections combined.\n        """"""\n        true_positive[\'true_positive\'] = 1\n        true_positive[\'false_positive\'] = 0\n        true_positive = true_positive[\n            [\n                \'image\',\n                \'object_name\',\n                \'score\',\n                \'x_min_common\',\n                \'y_min_common\',\n                \'x_max_common\',\n                \'y_max_common\',\n                \'iou\',\n                \'image_width\',\n                \'image_height\',\n                \'true_positive\',\n                \'false_positive\',\n                \'detection_key\',\n            ]\n        ]\n        true_positive = true_positive.rename(\n            columns={\n                \'x_min_common\': \'x1\',\n                \'y_min_common\': \'y1\',\n                \'x_max_common\': \'x2\',\n                \'y_max_common\': \'y2\',\n            }\n        )\n        false_positive[\'iou\'] = 0\n        false_positive[\'true_positive\'] = 0\n        false_positive[\'false_positive\'] = 1\n        false_positive = false_positive[\n            [\n                \'image\',\n                \'object_name\',\n                \'score\',\n                \'x1\',\n                \'y1\',\n                \'x2\',\n                \'y2\',\n                \'iou\',\n                \'image_width\',\n                \'image_height\',\n                \'true_positive\',\n                \'false_positive\',\n                \'detection_key\',\n            ]\n        ]\n        return pd.concat([true_positive, false_positive])\n\n    def calculate_stats(\n        self,\n        actual_data,\n        detection_data,\n        true_positives,\n        false_positives,\n        combined,\n    ):\n        """"""\n        Calculate prediction statistics for every class in self.class_names.\n        Args:\n            actual_data: pandas DataFrame with real data.\n            detection_data: pandas DataFrame with all detection data before filtration.\n            true_positives: pandas DataFrame with True positives.\n            false_positives: pandas DataFrame with False positives.\n            combined: pandas DataFrame with True and False positives combined.\n\n        Returns:\n            pandas DataFrame with statistics for all classes.\n        """"""\n        class_stats = []\n        for class_name in self.class_names:\n            stats = dict()\n            stats[\'Class Name\'] = class_name\n            stats[\'Average Precision\'] = (\n                combined[combined[\'object_name\'] == class_name][\n                    \'average_precision\'\n                ].sum()\n                * 100\n            )\n            stats[\'Actual\'] = len(\n                actual_data[actual_data[""Object Name""] == class_name]\n            )\n            stats[\'Detections\'] = len(\n                detection_data[detection_data[""object_name""] == class_name]\n            )\n            stats[\'True Positives\'] = len(\n                true_positives[true_positives[""object_name""] == class_name]\n            )\n            stats[\'False Positives\'] = len(\n                false_positives[false_positives[""object_name""] == class_name]\n            )\n            stats[\'Combined\'] = len(\n                combined[combined[""object_name""] == class_name]\n            )\n            class_stats.append(stats)\n        total_stats = pd.DataFrame(class_stats).sort_values(\n            by=\'Average Precision\', ascending=False\n        )\n        return total_stats\n\n    @staticmethod\n    def calculate_ap(combined, total_actual):\n        """"""\n        Calculate average precision for a single object class.\n        Args:\n            combined: pandas DataFrame with True and False positives combined.\n            total_actual: Total number of actual object class boxes.\n\n        Returns:\n            pandas DataFrame with average precisions calculated.\n        """"""\n        combined = combined.sort_values(\n            by=\'score\', ascending=False\n        ).reset_index(drop=True)\n        combined[\'acc_tp\'] = combined[\'true_positive\'].cumsum()\n        combined[\'acc_fp\'] = combined[\'false_positive\'].cumsum()\n        combined[\'precision\'] = combined[\'acc_tp\'] / (\n            combined[\'acc_tp\'] + combined[\'acc_fp\']\n        )\n        combined[\'recall\'] = combined[\'acc_tp\'] / total_actual\n        combined[\'m_pre1\'] = combined[\'precision\'].shift(1, fill_value=0)\n        combined[\'m_pre\'] = combined[[\'m_pre1\', \'precision\']].max(axis=1)\n        combined[\'m_rec1\'] = combined[\'recall\'].shift(1, fill_value=0)\n        combined.loc[\n            combined[\'m_rec1\'] != combined[\'recall\'], \'valid_m_rec\'\n        ] = 1\n        combined[\'average_precision\'] = (\n            combined[\'recall\'] - combined[\'m_rec1\']\n        ) * combined[\'m_pre\']\n        return combined\n\n    @timer(default_logger)\n    def calculate_map(\n        self,\n        prediction_data,\n        actual_data,\n        min_overlaps,\n        display_stats=False,\n        fig_prefix=\'\',\n        save_figs=True,\n        plot_results=True,\n    ):\n        """"""\n        Calculate mAP(mean average precision) for the trained model.\n        Args:\n            prediction_data: pandas DataFrame containing predictions.\n            actual_data: pandas DataFrame containing actual data.\n            min_overlaps: a float value between 0 and 1, or a dictionary\n                containing each class in self.class_names mapped to its\n                minimum overlap\n            display_stats: If True, statistics will be displayed.\n            fig_prefix: Prefix for plot titles.\n            save_figs: If True, figures will be saved.\n            plot_results: If True, results will be calculated.\n\n        Returns:\n            pandas DataFrame with statistics, mAP score.\n        """"""\n        actual_data[\'Object Name\'] = actual_data[\'Object Name\'].apply(\n            lambda x: x.replace(""b\'"", \'\').replace(""\'"", \'\')\n        )\n        class_counts = actual_data[\'Object Name\'].value_counts().to_dict()\n        true_positives = self.get_true_positives(\n            prediction_data, actual_data, min_overlaps\n        )\n        false_positives = self.get_false_positives(\n            prediction_data, true_positives\n        )\n        combined = self.combine_results(true_positives, false_positives)\n        class_groups = combined.groupby(\'object_name\')\n        calculated = pd.concat(\n            [\n                self.calculate_ap(group, class_counts.get(object_name))\n                for object_name, group in class_groups\n            ]\n        )\n        stats = self.calculate_stats(\n            actual_data,\n            prediction_data,\n            true_positives,\n            false_positives,\n            calculated,\n        )\n        map_score = stats[\'Average Precision\'].mean()\n        if display_stats:\n            pd.set_option(\n                \'display.max_rows\',\n                None,\n                \'display.max_columns\',\n                None,\n                \'display.width\',\n                None,\n            )\n            print(stats.sort_values(by=\'Average Precision\', ascending=False))\n            print(f\'mAP score: {map_score}%\')\n            pd.reset_option(\'display.[max_rows, max_columns, width]\')\n        if plot_results:\n            visualize_pr(calculated, save_figs, fig_prefix)\n            visualize_evaluation_stats(stats, fig_prefix)\n        return stats, map_score\n'"
Main/models.py,13,"b'from tensorflow.keras.layers import (\n    ZeroPadding2D,\n    BatchNormalization,\n    LeakyReLU,\n    Conv2D,\n    Add,\n    Input,\n    UpSampling2D,\n    Concatenate,\n    Lambda,\n    MaxPooling2D,\n)\nimport sys\n\nsys.path.append(\'..\')\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras import Model\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport io\nimport configparser\nfrom collections import defaultdict\nfrom Helpers.utils import get_boxes, timer, default_logger, Mish\n\n\nclass BaseModel:\n    def __init__(\n        self,\n        input_shape,\n        model_configuration,\n        classes=80,\n        anchors=None,\n        masks=None,\n        max_boxes=100,\n        iou_threshold=0.5,\n        score_threshold=0.5,\n    ):\n        """"""\n        Initialize yolo model.\n        Args:\n            input_shape: tuple(n, n, c)\n            model_configuration: Path to DarkNet cfg file containing configuration.\n            classes: Number of classes(defaults to 80 for Coco objects)\n            anchors: numpy array of anchors (x, y) pairs\n            masks: numpy array of masks.\n            max_boxes: Maximum boxes in a single image.\n            iou_threshold: Minimum overlap that counts as a valid detection.\n            score_threshold: Minimum confidence that counts as a valid detection.\n        """"""\n        assert any(\n            (\n                \'3\' in model_configuration,\n                \'4\' in model_configuration,\n                \'Invalid model configuration\',\n            )\n        )\n        self.version_anchors = {\n            \'v3\': np.array(\n                [\n                    (10, 13),\n                    (16, 30),\n                    (33, 23),\n                    (30, 61),\n                    (62, 45),\n                    (59, 119),\n                    (116, 90),\n                    (156, 198),\n                    (373, 326),\n                ],\n                np.float32,\n            ),\n            \'v4\': np.array(\n                [\n                    (12, 16),\n                    (19, 36),\n                    (40, 28),\n                    (36, 75),\n                    (76, 55),\n                    (72, 146),\n                    (142, 110),\n                    (192, 243),\n                    (459, 401),\n                ]\n            ),\n        }\n        self.version_masks = {\n            \'v3\': np.array([[6, 7, 8], [3, 4, 5], [0, 1, 2]]),\n            \'v4\': np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]),\n        }\n        self.current_layer = 1\n        self.input_shape = input_shape\n        self.classes = classes\n        self.anchors = anchors\n        if anchors is None:\n            if \'3\' in model_configuration:\n                self.anchors = self.version_anchors[\'v3\']\n            if \'4\' in model_configuration:\n                self.anchors = self.version_anchors[\'v4\']\n        if self.anchors[0][0] > 1:\n            self.anchors = self.anchors / input_shape[0]\n        self.masks = masks\n        if masks is None:\n            if \'3\' in model_configuration:\n                self.masks = self.version_masks[\'v3\']\n            if \'4\' in model_configuration:\n                self.masks = self.version_masks[\'v4\']\n        self.funcs = (\n            ZeroPadding2D,\n            BatchNormalization,\n            LeakyReLU,\n            Conv2D,\n            Add,\n            Input,\n            UpSampling2D,\n            Concatenate,\n            Lambda,\n            Mish,\n            MaxPooling2D,\n        )\n        self.func_names = [\n            \'zero_padding\',\n            \'batch_normalization\',\n            \'leaky_relu\',\n            \'conv2d\',\n            \'add\',\n            \'input\',\n            \'up_sample\',\n            \'concat\',\n            \'lambda\',\n            \'mish\',\n            \'maxpool2d\',\n        ]\n        self.layer_names = {\n            func.__name__: f\'layer_CURRENT_LAYER_{name}\'\n            for func, name in zip(self.funcs, self.func_names)\n        }\n        self.shortcuts = []\n        self.previous_layer = None\n        self.training_model = None\n        self.inference_model = None\n        self.output_indices = []\n        self.output_layers = []\n        self.max_boxes = max_boxes\n        self.iou_threshold = iou_threshold\n        self.score_threshold = score_threshold\n        self.model_configuration = model_configuration\n        self.model_layers = []\n\n    def apply_func(self, func, x=None, *args, **kwargs):\n        """"""\n        Apply a function from self.funcs and increment layer count.\n        Args:\n            func: func from self.funcs.\n            x: image tensor.\n            *args: func args\n            **kwargs: func kwargs\n\n        Returns:\n            result of func\n        """"""\n        name = self.layer_names[func.__name__].replace(\n            \'CURRENT_LAYER\', f\'{self.current_layer}\'\n        )\n        result = func(name=name, *args, **kwargs)\n        self.current_layer += 1\n        if x is not None:\n            return result(x)\n        return result\n\n    def read_dark_net_cfg(self):\n        """"""\n        Read model configuration from DarkNet cfg file.\n\n        Returns:\n            output_stream\n        """"""\n        section_counters = defaultdict(int)\n        output_stream = io.StringIO()\n        with open(self.model_configuration) as cfg:\n            for line in cfg:\n                if line.startswith(\'[\'):\n                    section = line.strip().strip(\'[]\')\n                    adjusted_section = f\'{section}_{section_counters[section]}\'\n                    section_counters[section] += 1\n                    line = line.replace(section, adjusted_section)\n                output_stream.write(line)\n        output_stream.seek(0)\n        return output_stream\n\n    def get_nms(self, outputs):\n        """"""\n        Apply non-max suppression and get valid detections.\n        Args:\n            outputs: yolo model outputs.\n\n        Returns:\n            boxes, scores, classes, valid_detections\n        """"""\n        boxes, conf, type_ = [], [], []\n        for output in outputs:\n            boxes.append(\n                tf.reshape(\n                    output[0],\n                    (tf.shape(output[0])[0], -1, tf.shape(output[0])[-1]),\n                )\n            )\n            conf.append(\n                tf.reshape(\n                    output[1],\n                    (tf.shape(output[1])[0], -1, tf.shape(output[1])[-1]),\n                )\n            )\n            type_.append(\n                tf.reshape(\n                    output[2],\n                    (tf.shape(output[2])[0], -1, tf.shape(output[2])[-1]),\n                )\n            )\n        bbox = tf.concat(boxes, axis=1)\n        confidence = tf.concat(conf, axis=1)\n        class_probabilities = tf.concat(type_, axis=1)\n        scores = confidence * class_probabilities\n        (\n            boxes,\n            scores,\n            classes,\n            valid_detections,\n        ) = tf.image.combined_non_max_suppression(\n            boxes=tf.reshape(bbox, (tf.shape(bbox)[0], -1, 1, 4)),\n            scores=tf.reshape(\n                scores, (tf.shape(scores)[0], -1, tf.shape(scores)[-1])\n            ),\n            max_output_size_per_class=self.max_boxes,\n            max_total_size=self.max_boxes,\n            iou_threshold=self.iou_threshold,\n            score_threshold=self.score_threshold,\n        )\n        return boxes, scores, classes, valid_detections\n\n    def create_convolution(self, cfg_parser, section):\n        """"""\n        Create convolution layers.\n        Args:\n            cfg_parser: Model configuration cfg parser.\n            section: cfg section\n\n        Returns:\n            None\n        """"""\n        filters = int(cfg_parser[section][\'filters\'])\n        size = int(cfg_parser[section][\'size\'])\n        stride = int(cfg_parser[section][\'stride\'])\n        pad = int(cfg_parser[section][\'pad\'])\n        activation = cfg_parser[section][\'activation\']\n        batch_normalize = \'batch_normalize\' in cfg_parser[section]\n        padding = \'same\' if pad == 1 and stride == 1 else \'valid\'\n        if filters == 255:\n            filters = 3 * (self.classes + 5)\n        if stride > 1:\n            self.previous_layer = self.apply_func(\n                ZeroPadding2D, self.previous_layer, ((1, 0), (1, 0))\n            )\n        convolution_layer = self.apply_func(\n            Conv2D,\n            self.previous_layer,\n            filters=filters,\n            kernel_size=size,\n            strides=(stride, stride),\n            use_bias=not batch_normalize,\n            padding=padding,\n            kernel_regularizer=l2(0.0005),\n        )\n        if batch_normalize:\n            convolution_layer = self.apply_func(\n                BatchNormalization, convolution_layer\n            )\n        self.previous_layer = convolution_layer\n        if activation == \'linear\':\n            self.model_layers.append(self.previous_layer)\n        if activation == \'leaky\':\n            act_layer = self.apply_func(\n                LeakyReLU, self.previous_layer, alpha=0.1\n            )\n            self.previous_layer = act_layer\n            self.model_layers.append(act_layer)\n        if activation == \'mish\':\n            act_layer = self.apply_func(Mish, self.previous_layer)\n            self.previous_layer = act_layer\n            self.model_layers.append(act_layer)\n\n    def create_route(self, cfg_parser, section):\n        """"""\n        Create concatenation layer.\n        Args:\n            cfg_parser: Model configuration cfg parser.\n            section: cfg section\n\n        Returns:\n            None\n        """"""\n        ids = [int(i) for i in cfg_parser[section][\'layers\'].split(\',\')]\n        layers = [self.model_layers[i] for i in ids]\n        if len(layers) > 1:\n            concatenate_layer = self.apply_func(Concatenate, layers)\n            self.model_layers.append(concatenate_layer)\n            self.previous_layer = concatenate_layer\n        else:\n            skip_layer = layers[0]\n            self.model_layers.append(skip_layer)\n            self.previous_layer = skip_layer\n\n    def create_max_pool(self, cfg_parser, section):\n        """"""\n        Create max pooling layer.\n        Args:\n            cfg_parser: Model configuration cfg parser.\n            section: cfg section\n\n        Returns:\n            None\n        """"""\n        size = int(cfg_parser[section][\'size\'])\n        stride = int(cfg_parser[section][\'stride\'])\n        layer = self.apply_func(\n            MaxPooling2D,\n            self.previous_layer,\n            pool_size=(size, size),\n            strides=(stride, stride),\n            padding=\'same\',\n        )\n        self.model_layers.append(layer)\n        self.previous_layer = layer\n\n    def create_shortcut(self, cfg_parser, section):\n        """"""\n        Create shortcut layer.\n        Args:\n            cfg_parser: Model configuration cfg parser.\n            section: cfg section\n\n        Returns:\n            None\n        """"""\n        index = int(cfg_parser[section][\'from\'])\n        activation = cfg_parser[section][\'activation\']\n        assert activation == \'linear\', \'Only linear activation supported.\'\n        layer = self.apply_func(\n            Add, [self.model_layers[index], self.previous_layer]\n        )\n        self.model_layers.append(layer)\n        self.previous_layer = layer\n\n    def create_up_sample(self, cfg_parser, section):\n        """"""\n        Create up sample layer\n        Args:\n            cfg_parser: Model configuration cfg parser.\n            section: cfg section\n\n        Returns:\n            None\n        """"""\n        stride = int(cfg_parser[section][\'stride\'])\n        assert stride == 2, \'Only stride=2 supported.\'\n        layer = self.apply_func(UpSampling2D, self.previous_layer, stride)\n        self.model_layers.append(layer)\n        self.previous_layer = layer\n\n    def create_output_layer(self):\n        """"""\n        Create output layer.\n\n        Returns:\n            None\n        """"""\n        self.output_indices.append(len(self.model_layers))\n        x = self.model_layers[-1]\n        x = self.apply_func(\n            Lambda,\n            x,\n            lambda item: tf.reshape(\n                item,\n                (\n                    -1,\n                    tf.shape(item)[1],\n                    tf.shape(item)[2],\n                    3,\n                    self.classes + 5,\n                ),\n            ),\n        )\n        self.model_layers.append(x)\n        self.previous_layer = self.model_layers[-1]\n\n    def create_section(self, section, cfg_parser):\n        """"""\n        Create a section from the model configuration file.\n        Args:\n            cfg_parser: Model configuration cfg parser.\n            section: cfg section\n\n        Returns:\n            None\n        """"""\n        if section.startswith(\'convolutional\'):\n            self.create_convolution(cfg_parser, section)\n        if section.startswith(\'route\'):\n            self.create_route(cfg_parser, section)\n        if section.startswith(\'maxpool\'):\n            self.create_max_pool(cfg_parser, section)\n        if section.startswith(\'shortcut\'):\n            self.create_shortcut(cfg_parser, section)\n        if section.startswith(\'upsample\'):\n            self.create_up_sample(cfg_parser, section)\n        if section.startswith(\'yolo\'):\n            self.create_output_layer()\n\n    @timer(default_logger)\n    def create_models(self):\n        """"""\n        Create training and inference yolo models.\n\n        Returns:\n            training, inference models\n        """"""\n        input_initial = self.apply_func(Input, shape=self.input_shape)\n        cfg_out = self.read_dark_net_cfg()\n        cfg_parser = configparser.ConfigParser()\n        cfg_parser.read_file(cfg_out)\n        self.output_indices = []\n        self.previous_layer = input_initial\n        for section in cfg_parser.sections():\n            self.create_section(section, cfg_parser)\n        if len(self.output_indices) == 0:\n            self.output_indices.append(len(self.model_layers) - 1)\n        self.output_layers.extend(\n            [self.model_layers[i] for i in self.output_indices]\n        )\n        if \'4\' in self.model_configuration:\n            self.output_layers.reverse()\n        self.training_model = Model(\n            inputs=input_initial, outputs=self.output_layers\n        )\n        output_0, output_1, output_2 = self.output_layers\n        boxes_0 = self.apply_func(\n            Lambda,\n            output_0,\n            lambda item: get_boxes(\n                item, self.anchors[self.masks[0]], self.classes\n            ),\n        )\n        boxes_1 = self.apply_func(\n            Lambda,\n            output_1,\n            lambda item: get_boxes(\n                item, self.anchors[self.masks[1]], self.classes\n            ),\n        )\n        boxes_2 = self.apply_func(\n            Lambda,\n            output_2,\n            lambda item: get_boxes(\n                item, self.anchors[self.masks[2]], self.classes\n            ),\n        )\n        outputs = self.apply_func(\n            Lambda,\n            (boxes_0[:3], boxes_1[:3], boxes_2[:3]),\n            lambda item: self.get_nms(item),\n        )\n        self.inference_model = Model(\n            input_initial, outputs, name=\'inference_model\'\n        )\n        default_logger.info(\'Training and inference models created\')\n        return self.training_model, self.inference_model\n\n    @timer(default_logger)\n    def load_weights(self, weights_file):\n        """"""\n        Load DarkNet weights or checkpoint/pre-trained weights.\n        Args:\n            weights_file: .weights or .tf file path.\n\n        Returns:\n            None\n        """"""\n        assert weights_file.split(\'.\')[-1] in [\n            \'tf\',\n            \'weights\',\n        ], \'Invalid weights file\'\n        assert (\n            self.classes == 80 if weights_file.endswith(\'.weights\') else 1\n        ), f\'DarkNet model should contain 80 classes, {self.classes} is given.\'\n        if weights_file.endswith(\'.tf\'):\n            self.training_model.load_weights(weights_file)\n            default_logger.info(f\'Loaded weights: {weights_file} ... success\')\n            return\n        with open(weights_file, \'rb\') as weights_data:\n            default_logger.info(f\'Loading pre-trained weights ...\')\n            major, minor, revision, seen, _ = np.fromfile(\n                weights_data, dtype=np.int32, count=5\n            )\n            self.model_layers = [\n                layer\n                for layer in self.training_model.layers\n                if id(layer) not in [id(item) for item in self.output_layers]\n            ]\n            self.model_layers.sort(\n                key=lambda layer: int(layer.name.split(\'_\')[1])\n            )\n            self.model_layers.extend(self.output_layers)\n            for i, layer in enumerate(self.model_layers):\n                current_read = weights_data.tell()\n                total_size = os.fstat(weights_data.fileno()).st_size\n                if current_read == total_size:\n                    break\n                print(\n                    f\'\\r{round(100 * (current_read / total_size))}%\\\n                    t{current_read}/{total_size}\',\n                    end=\'\',\n                )\n                if \'conv2d\' not in layer.name:\n                    continue\n                next_layer = self.model_layers[i + 1]\n                b_norm_layer = (\n                    next_layer\n                    if \'batch_normalization\' in next_layer.name\n                    else None\n                )\n                filters = layer.filters\n                kernel_size = layer.kernel_size[0]\n                input_dimension = layer.get_input_shape_at(-1)[-1]\n                convolution_bias = (\n                    np.fromfile(weights_data, dtype=np.float32, count=filters)\n                    if b_norm_layer is None\n                    else None\n                )\n                bn_weights = (\n                    np.fromfile(\n                        weights_data, dtype=np.float32, count=4 * filters\n                    ).reshape((4, filters))[[1, 0, 2, 3]]\n                    if (b_norm_layer is not None)\n                    else None\n                )\n                convolution_shape = (\n                    filters,\n                    input_dimension,\n                    kernel_size,\n                    kernel_size,\n                )\n                convolution_weights = (\n                    np.fromfile(\n                        weights_data,\n                        dtype=np.float32,\n                        count=np.product(convolution_shape),\n                    )\n                    .reshape(convolution_shape)\n                    .transpose([2, 3, 1, 0])\n                )\n                if b_norm_layer is None:\n                    try:\n                        layer.set_weights(\n                            [convolution_weights, convolution_bias]\n                        )\n                    except ValueError:\n                        pass\n                if b_norm_layer is not None:\n                    layer.set_weights([convolution_weights])\n                    b_norm_layer.set_weights(bn_weights)\n            assert len(weights_data.read()) == 0, \'failed to read all data\'\n        default_logger.info(f\'Loaded weights: {weights_file} ... success\')\n        print()\n'"
Main/trainer.py,1,"b'import tensorflow as tf\nimport os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport sys\n\nsys.path.append(\'..\')\nfrom tensorflow.keras.callbacks import (\n    ReduceLROnPlateau,\n    TensorBoard,\n    ModelCheckpoint,\n    Callback,\n    EarlyStopping,\n)\nimport shutil\nfrom Helpers.dataset_handlers import read_tfr, save_tfr, get_feature_map\nfrom Helpers.annotation_parsers import parse_voc_folder\nfrom Helpers.anchors import k_means, generate_anchors\nfrom Helpers.augmentor import DataAugment\nfrom Config.augmentation_options import augmentations\nfrom Main.models import BaseModel\nfrom Helpers.utils import transform_images, transform_targets\nfrom Helpers.annotation_parsers import adjust_non_voc_csv\nfrom Helpers.utils import calculate_loss, timer, default_logger, activate_gpu\nfrom Main.evaluator import Evaluator\n\n\nclass Trainer(BaseModel):\n    """"""\n    Create a training instance.\n    """"""\n\n    def __init__(\n        self,\n        input_shape,\n        model_configuration,\n        classes_file,\n        image_width,\n        image_height,\n        train_tf_record=None,\n        valid_tf_record=None,\n        anchors=None,\n        masks=None,\n        max_boxes=100,\n        iou_threshold=0.5,\n        score_threshold=0.5,\n    ):\n        """"""\n        Initialize training.\n        Args:\n            model_configuration: Path to DarkNet cfg file.\n            input_shape: tuple, (n, n, c)\n            anchors: numpy array of (w, h) pairs.\n            masks: numpy array of masks.\n            max_boxes: Maximum boxes of the TFRecords provided(if any) or\n                maximum boxes setting.\n            iou_threshold: float, values less than the threshold are ignored.\n            score_threshold: float, values less than the threshold are ignored.\n        """"""\n        self.classes_file = classes_file\n        self.class_names = [\n            item.strip() for item in open(classes_file).readlines()\n        ]\n        super().__init__(\n            input_shape,\n            model_configuration,\n            len(self.class_names),\n            anchors,\n            masks,\n            max_boxes,\n            iou_threshold,\n            score_threshold,\n        )\n        self.train_tf_record = train_tf_record\n        self.valid_tf_record = valid_tf_record\n        self.image_folder = (\n            Path(os.path.join(\'..\', \'Data\', \'Photos\')).absolute().resolve()\n        )\n        self.image_width = image_width\n        self.image_height = image_height\n\n    def get_adjusted_labels(self, configuration):\n        """"""\n        Adjust labels according to given configuration.\n        Args:\n            configuration: A dictionary containing any of the following keys:\n                - relative_labels\n                - from_xml\n                - coordinate_labels\n\n        Returns:\n            pandas DataFrame with adjusted labels.\n        """"""\n        labels_frame = None\n        check = 0\n        if configuration.get(\'relative_labels\'):\n            labels_frame = adjust_non_voc_csv(\n                configuration[\'relative_labels\'],\n                self.image_folder,\n                self.image_width,\n                self.image_height,\n            )\n            check += 1\n        if configuration.get(\'from_xml\'):\n            if check:\n                raise ValueError(f\'Got more than one configuration\')\n            labels_frame = parse_voc_folder(\n                os.path.join(\'..\', \'Data\', \'XML Labels\'),\n                os.path.join(\'..\', \'Config\', \'voc_conf.json\'),\n            )\n            labels_frame.to_csv(\n                os.path.join(\'..\', \'Output\', \'Data\', \'parsed_from_xml.csv\'),\n                index=False,\n            )\n            check += 1\n        if configuration.get(\'coordinate_labels\'):\n            if check:\n                raise ValueError(f\'Got more than one configuration\')\n            labels_frame = pd.read_csv(configuration[\'coordinate_labels\'])\n            check += 1\n        return labels_frame\n\n    def generate_new_anchors(self, new_anchors_conf):\n        """"""\n        Create new anchors according to given configuration.\n        Args:\n            new_anchors_conf: A dictionary containing the following keys:\n                - anchor_no\n                and one of the following:\n                    - relative_labels\n                    - from_xml\n                    - coordinate_labels\n            new_anchors_conf: A dictionary with the following keys:\n            - anchor_no: number of anchors to generate\n        Returns:\n            None\n        """"""\n        anchor_no = new_anchors_conf.get(\'anchor_no\')\n        if not anchor_no:\n            raise ValueError(f\'No ""anchor_no"" found in new_anchors_conf\')\n        labels_frame = self.get_adjusted_labels(new_anchors_conf)\n        relative_dims = np.array(\n            list(\n                zip(\n                    labels_frame[\'Relative Width\'],\n                    labels_frame[\'Relative Height\'],\n                )\n            )\n        )\n        centroids, _ = k_means(relative_dims, anchor_no, frame=labels_frame)\n        self.anchors = (\n            generate_anchors(self.image_width, self.image_height, centroids)\n            / self.input_shape[0]\n        )\n        default_logger.info(\'Changed default anchors to generated ones\')\n\n    def generate_new_frame(self, new_dataset_conf):\n        """"""\n        Create new labels frame according to given configuration.\n\n        Returns:\n            pandas DataFrame adjusted for building the dataset containing\n            labels or labels and augmented labels combined\n        """"""\n        if not new_dataset_conf.get(\'dataset_name\'):\n            raise ValueError(\'dataset_name not found in new_dataset_conf\')\n        labels_frame = self.get_adjusted_labels(new_dataset_conf)\n        if new_dataset_conf.get(\'augmentation\'):\n            labels_frame = self.augment_photos(new_dataset_conf)\n        return labels_frame\n\n    def initialize_dataset(self, tf_record, batch_size, shuffle_buffer=512):\n        """"""\n        Initialize and prepare TFRecord dataset for training.\n        Args:\n            tf_record: TFRecord file.\n            batch_size: int, training batch size\n            shuffle_buffer: Buffer size for shuffling dataset.\n\n        Returns:\n            dataset.\n        """"""\n        dataset = read_tfr(\n            tf_record, self.classes_file, get_feature_map(), self.max_boxes\n        )\n        dataset = dataset.shuffle(shuffle_buffer)\n        dataset = dataset.batch(batch_size)\n        dataset = dataset.map(\n            lambda x, y: (\n                transform_images(x, self.input_shape[0]),\n                transform_targets(\n                    y, self.anchors, self.masks, self.input_shape[0]\n                ),\n            )\n        )\n        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n        return dataset\n\n    @staticmethod\n    def augment_photos(new_dataset_conf):\n        """"""\n        Augment photos in self.image_paths\n        Args:\n            new_dataset_conf: New dataset configuration dict.\n\n        Returns:\n            pandas DataFrame with both original and augmented data.\n        """"""\n        sequences = new_dataset_conf.get(\'sequences\')\n        relative_labels = new_dataset_conf.get(\'relative_labels\')\n        coordinate_labels = new_dataset_conf.get(\'coordinate_labels\')\n        workers = new_dataset_conf.get(\'aug_workers\')\n        batch_size = new_dataset_conf.get(\'aug_batch_size\')\n        if not sequences:\n            raise ValueError(f\'""sequences"" not found in new_dataset_conf\')\n        if not relative_labels:\n            raise ValueError(f\'No ""relative_labels"" found in new_dataset_conf\')\n        augment = DataAugment(\n            relative_labels, augmentations, workers or 32, coordinate_labels\n        )\n        augment.create_sequences(sequences)\n        return augment.augment_photos_folder(batch_size or 64)\n\n    @timer(default_logger)\n    def evaluate(\n        self,\n        weights_file,\n        merge,\n        workers,\n        shuffle_buffer,\n        min_overlaps,\n        display_stats=True,\n        plot_stats=True,\n        save_figs=True,\n    ):\n        """"""\n        Evaluate on training and validation datasets.\n        Args:\n            weights_file: Path to trained .tf file.\n            merge: If False, training and validation datasets will be evaluated separately.\n            workers: Parallel predictions.\n            shuffle_buffer: Buffer size for shuffling datasets.\n            min_overlaps: a float value between 0 and 1, or a dictionary\n                containing each class in self.class_names mapped to its\n                minimum overlap\n            display_stats: If True evaluation statistics will be printed.\n            plot_stats: If True, evaluation statistics will be plotted including\n                precision and recall curves and mAP\n            save_figs: If True, resulting plots will be save to Output folder.\n\n        Returns:\n            stats, map_score.\n        """"""\n        default_logger.info(\'Starting evaluation ...\')\n        evaluator = Evaluator(\n            self.input_shape,\n            self.model_configuration,\n            self.train_tf_record,\n            self.valid_tf_record,\n            self.classes_file,\n            self.anchors,\n            self.masks,\n            self.max_boxes,\n            self.iou_threshold,\n            self.score_threshold,\n        )\n        predictions = evaluator.make_predictions(\n            weights_file, merge, workers, shuffle_buffer\n        )\n        if isinstance(predictions, tuple):\n            training_predictions, valid_predictions = predictions\n            if any([training_predictions.empty, valid_predictions.empty]):\n                default_logger.info(\n                    \'Aborting evaluations, no detections found\'\n                )\n                return\n            training_actual = pd.read_csv(\n                os.path.join(\'..\', \'Data\', \'TFRecords\', \'training_data.csv\')\n            )\n            valid_actual = pd.read_csv(\n                os.path.join(\'..\', \'Data\', \'TFRecords\', \'test_data.csv\')\n            )\n            training_stats, training_map = evaluator.calculate_map(\n                training_predictions,\n                training_actual,\n                min_overlaps,\n                display_stats,\n                \'Train\',\n                save_figs,\n                plot_stats,\n            )\n            valid_stats, valid_map = evaluator.calculate_map(\n                valid_predictions,\n                valid_actual,\n                min_overlaps,\n                display_stats,\n                \'Valid\',\n                save_figs,\n                plot_stats,\n            )\n            return training_stats, training_map, valid_stats, valid_map\n        actual_data = pd.read_csv(\n            os.path.join(\'..\', \'Data\', \'TFRecords\', \'full_data.csv\')\n        )\n        if predictions.empty:\n            default_logger.info(\'Aborting evaluations, no detections found\')\n            return\n        stats, map_score = evaluator.calculate_map(\n            predictions,\n            actual_data,\n            min_overlaps,\n            display_stats,\n            save_figs=save_figs,\n            plot_results=plot_stats,\n        )\n        return stats, map_score\n\n    @staticmethod\n    def clear_outputs():\n        """"""\n        Clear Output folder.\n\n        Returns:\n            None\n        """"""\n        for folder_name in os.listdir(os.path.join(\'..\', \'Output\')):\n            if not folder_name.startswith(\'.\'):\n                full_path = (\n                    Path(os.path.join(\'..\', \'Output\', folder_name))\n                    .absolute()\n                    .resolve()\n                )\n                for file_name in os.listdir(full_path):\n                    full_file_path = Path(os.path.join(full_path, file_name))\n                    if os.path.isdir(full_file_path):\n                        shutil.rmtree(full_file_path)\n                    else:\n                        os.remove(full_file_path)\n                    default_logger.info(\n                        f\'Deleted old output: {full_file_path}\'\n                    )\n\n    def create_new_dataset(self, new_dataset_conf):\n        """"""\n        Create a new TFRecord dataset.\n        Args:\n            new_dataset_conf: A dictionary containing the following keys:\n                - dataset_name(required) str representing a name for the dataset\n                - test_size(optional) ex: 0.1\n                - augmentation(optional) True or False\n                - sequences(required if augmentation is True)\n                - aug_workers(optional if augmentation is True) defaults to 32.\n                - aug_batch_size(optional if augmentation is True) defaults to 64.\n                And one of the following is required:\n                    - relative_labels: Path to csv file with the following columns:\n                    [\'Image\', \'Object Name\', \'Object Index\', \'bx\', \'by\', \'bw\', \'bh\']\n                    - coordinate_labels: Path to csv file with the following columns:\n                    [\'Image Path\', \'Object Name\', \'Image Width\', \'Image Height\',\n                    \'X_min\', \'Y_min\', \'X_max\', \'Y_max\', \'Relative Width\', \'Relative Height\',\n                    \'Object ID\']\n                    - from_xml: True or False to parse from XML Labels folder.\n        """"""\n        default_logger.info(f\'Generating new dataset ...\')\n        test_size = new_dataset_conf.get(\'test_size\')\n        labels_frame = self.generate_new_frame(new_dataset_conf)\n        save_tfr(\n            labels_frame,\n            os.path.join(\'..\', \'Data\', \'TFRecords\'),\n            new_dataset_conf[\'dataset_name\'],\n            test_size,\n            self,\n        )\n\n    def check_tf_records(self):\n        """"""\n        Ensure TFRecords are specified to start training.\n\n        Returns:\n            None\n        """"""\n        if not self.train_tf_record:\n            issue = \'No training TFRecord specified\'\n            default_logger.error(issue)\n            raise ValueError(issue)\n        if not self.valid_tf_record:\n            issue = \'No validation TFRecord specified\'\n            default_logger.error(issue)\n            raise ValueError(issue)\n\n    @staticmethod\n    def create_callbacks(checkpoint_path):\n        """"""\n        Create a list of tf.keras.callbacks.\n        Args:\n            checkpoint_path: Full path to checkpoint.\n\n        Returns:\n            callbacks.\n        """"""\n        return [\n            ReduceLROnPlateau(verbose=1, patience=4),\n            ModelCheckpoint(\n                os.path.join(checkpoint_path),\n                verbose=1,\n                save_weights_only=True,\n            ),\n            TensorBoard(log_dir=os.path.join(\'..\', \'Logs\')),\n            EarlyStopping(monitor=\'val_loss\', patience=6, verbose=1),\n        ]\n\n    @timer(default_logger)\n    def train(\n        self,\n        epochs,\n        batch_size,\n        learning_rate,\n        new_anchors_conf=None,\n        new_dataset_conf=None,\n        dataset_name=None,\n        weights=None,\n        evaluate=True,\n        merge_evaluation=True,\n        evaluation_workers=8,\n        shuffle_buffer=512,\n        min_overlaps=None,\n        display_stats=True,\n        plot_stats=True,\n        save_figs=True,\n        clear_outputs=False,\n        n_epoch_eval=None,\n    ):\n        """"""\n        Train on the dataset.\n        Args:\n            epochs: Number of training epochs.\n            batch_size: Training batch size.\n            learning_rate: non-negative value.\n            new_anchors_conf: A dictionary containing anchor generation configuration.\n            new_dataset_conf: A dictionary containing dataset generation configuration.\n            dataset_name: Name of the dataset for model checkpoints.\n            weights: .tf or .weights file\n            evaluate: If False, the trained model will not be evaluated after training.\n            merge_evaluation: If False, training and validation maps will\n                be calculated separately.\n            evaluation_workers: Parallel predictions.\n            shuffle_buffer: Buffer size for shuffling datasets.\n            min_overlaps: a float value between 0 and 1, or a dictionary\n                containing each class in self.class_names mapped to its\n                minimum overlap\n            display_stats: If True and evaluate=True, evaluation statistics will be displayed.\n            plot_stats: If True, Precision and recall curves as well as\n                comparative bar charts will be plotted\n            save_figs: If True and plot_stats=True, figures will be saved\n            clear_outputs: If True, old outputs will be cleared\n            n_epoch_eval: Conduct evaluation every n epoch.\n\n        Returns:\n            history object, pandas DataFrame with statistics, mAP score.\n        """"""\n        min_overlaps = min_overlaps or 0.5\n        if clear_outputs:\n            self.clear_outputs()\n        activate_gpu()\n        default_logger.info(f\'Starting training ...\')\n        if new_anchors_conf:\n            default_logger.info(f\'Generating new anchors ...\')\n            self.generate_new_anchors(new_anchors_conf)\n        self.create_models()\n        if weights:\n            self.load_weights(weights)\n        if new_dataset_conf:\n            self.create_new_dataset(new_dataset_conf)\n        self.check_tf_records()\n        training_dataset = self.initialize_dataset(\n            self.train_tf_record, batch_size, shuffle_buffer\n        )\n        valid_dataset = self.initialize_dataset(\n            self.valid_tf_record, batch_size, shuffle_buffer\n        )\n        optimizer = tf.keras.optimizers.Adam(learning_rate)\n        loss = [\n            calculate_loss(\n                self.anchors[mask], self.classes, self.iou_threshold\n            )\n            for mask in self.masks\n        ]\n        self.training_model.compile(optimizer=optimizer, loss=loss)\n        checkpoint_path = os.path.join(\n            \'..\', \'Models\', f\'{dataset_name or ""trained""}_model.tf\'\n        )\n        callbacks = self.create_callbacks(checkpoint_path)\n        if n_epoch_eval:\n            mid_train_eval = MidTrainingEvaluator(\n                self.input_shape,\n                self.model_configuration,\n                self.classes_file,\n                self.image_width,\n                self.image_height,\n                self.train_tf_record,\n                self.valid_tf_record,\n                self.anchors,\n                self.masks,\n                self.max_boxes,\n                self.iou_threshold,\n                self.score_threshold,\n                n_epoch_eval,\n                merge_evaluation,\n                evaluation_workers,\n                shuffle_buffer,\n                min_overlaps,\n                display_stats,\n                plot_stats,\n                save_figs,\n                checkpoint_path,\n            )\n            callbacks.append(mid_train_eval)\n        history = self.training_model.fit(\n            training_dataset,\n            epochs=epochs,\n            callbacks=callbacks,\n            validation_data=valid_dataset,\n        )\n        default_logger.info(\'Training complete\')\n        if evaluate:\n            evaluations = self.evaluate(\n                checkpoint_path,\n                merge_evaluation,\n                evaluation_workers,\n                shuffle_buffer,\n                min_overlaps,\n                display_stats,\n                plot_stats,\n                save_figs,\n            )\n            return evaluations, history\n        return history\n\n\nclass MidTrainingEvaluator(Callback, Trainer):\n    """"""\n    Tool to evaluate trained model on the go(during the training, every n epochs).\n    """"""\n\n    def __init__(\n        self,\n        input_shape,\n        model_configuration,\n        classes_file,\n        image_width,\n        image_height,\n        train_tf_record,\n        valid_tf_record,\n        anchors,\n        masks,\n        max_boxes,\n        iou_threshold,\n        score_threshold,\n        n_epochs,\n        merge,\n        workers,\n        shuffle_buffer,\n        min_overlaps,\n        display_stats,\n        plot_stats,\n        save_figs,\n        weights_file,\n    ):\n        """"""\n        Initialize mid-training evaluation settings.\n        Args:\n            input_shape: tuple, (n, n, c)\n            model_configuration: Path to DarkNet cfg file.\n            classes_file: File containing class names \\n delimited.\n            image_width: Width of the original image.\n            image_height: Height of the original image.\n            train_tf_record: TFRecord file.\n            valid_tf_record: TFRecord file.\n            anchors: numpy array of (w, h) pairs.\n            masks: numpy array of masks.\n            max_boxes: Maximum boxes of the TFRecords provided(if any) or\n                maximum boxes setting.\n            iou_threshold: float, values less than the threshold are ignored.\n            score_threshold: float, values less than the threshold are ignored.\n            n_epochs: int, perform evaluation every n epochs\n            merge: If True, The whole dataset(train + valid) will be evaluated\n            workers: Parallel predictions\n            shuffle_buffer: Buffer size for shuffling datasets\n            min_overlaps: a float value between 0 and 1, or a dictionary\n                containing each class in self.class_names mapped to its\n                minimum overlap\n            display_stats: If True, statistics will be displayed at the end.\n            plot_stats: If True, precision and recall curves as well as\n                comparison bar charts will be plotted.\n            save_figs: If True and display_stats, plots will be save to Output folder\n            weights_file: .tf file(most recent checkpoint)\n        """"""\n        Trainer.__init__(\n            self,\n            input_shape,\n            model_configuration,\n            classes_file,\n            image_width,\n            image_height,\n            train_tf_record,\n            valid_tf_record,\n            anchors,\n            masks,\n            max_boxes,\n            iou_threshold,\n            score_threshold,\n        )\n        self.n_epochs = n_epochs\n        self.evaluation_args = [\n            weights_file,\n            merge,\n            workers,\n            shuffle_buffer,\n            min_overlaps,\n            display_stats,\n            plot_stats,\n            save_figs,\n        ]\n\n    def on_epoch_end(self, epoch, logs=None):\n        """"""\n        Start evaluation in valid epochs.\n        Args:\n            epoch: int, epoch number.\n            logs: dict, TensorBoard log.\n\n        Returns:\n            None\n        """"""\n        if not (epoch + 1) % self.n_epochs == 0:\n            return\n        self.evaluate(*self.evaluation_args)\n        evaluation_dir = str(\n            Path(\n                os.path.join(\n                    \'..\', \'Output\', \'Evaluation\', f\'epoch-{epoch}-evaluation\'\n                )\n            )\n            .absolute()\n            .resolve()\n        )\n        os.mkdir(evaluation_dir)\n        current_predictions = [\n            str(\n                Path(os.path.join(\'..\', \'Output\', \'Data\', item))\n                .absolute()\n                .resolve()\n            )\n            for item in os.listdir(os.path.join(\'..\', \'Output\', \'Data\'))\n        ]\n        current_figures = [\n            str(\n                Path(os.path.join(\'..\', \'Output\', \'Plots\', item))\n                .absolute()\n                .resolve()\n            )\n            for item in os.listdir(os.path.join(\'..\', \'Output\', \'Plots\'))\n        ]\n        current_files = current_predictions + current_figures\n        for file_path in current_files:\n            if os.path.isfile(file_path):\n                file_name = os.path.basename(file_path)\n                new_path = os.path.join(evaluation_dir, file_name)\n                shutil.move(file_path, new_path)\n'"
