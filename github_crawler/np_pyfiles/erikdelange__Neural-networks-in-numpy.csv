file_path,api_count,code
2x2_image_predefined_1.py,4,"b'# Two layer neural network capable of processing multiple inputs in one pass. Pre-defined weights and biases.\r\n# Shape definition is done via the weight arrays W1, W2. Bias is not separate but included as an additional weight.\r\n# Is able to recognize if an image contains a vertical bar or not.\r\n\r\n\r\nimport numpy as np\r\n\r\nimport dataset\r\n\r\nW1 = np.array([[+1, -2, +1, -2, -1], [-2, +1, -2, +1, -1]])  # layer 1: 2 nodes with 4 inputs per node + bias input\r\nW2 = np.array([[+1, +1, 0]])  # layer 2: 1 node with 2 inputs + bias input\r\n\r\nX, Y = dataset.image2x2_set(no_of_sets=1, vertical_only=True)\r\n\r\na = X.T\r\n\r\nfor w in [W1, W2]:\r\n    z = w.dot(np.vstack([a, np.ones([1, a.shape[1]])]))  # add the bias input which is always 1\r\n    a = np.where(z > 0, 1, 0)  # step activation\r\n\r\nprint(""predict:"", a)\r\nprint(""desired:"", Y.T)\r\nprint(""error  :"", (Y.T - a))\r\n'"
2x2_image_predefined_2.py,5,"b'# Two layer neural network capable of processing multiple inputs in one pass. Pre-defined weights and biases.\r\n# Shape definition is done via the weight arrays W1, W2. Bias is separate.\r\n# Is able to recognize if an image contains a vertical bar or not.\r\n\r\nimport numpy as np\r\n\r\nimport dataset\r\n\r\nW1 = np.array([[+1, -2, +1, -2], [-2, +1, -2, +1]])  # layer 1: 2 nodes with 4 inputs per node\r\nW2 = np.array([[+1, +1]])  # layer 2: 1 node with 2 inputs + bias input\r\nB1 = np.array([[-1], [-1]])  # bias for nodes in layer 1\r\nB2 = np.array([[0]])  # bias for nodes in layer 2\r\n\r\nX, Y = dataset.image2x2_set(no_of_sets=2, vertical_only=True)\r\n\r\na = X.T\r\n\r\nfor w, b in zip([W1, W2], [B1, B2]):\r\n    z = w.dot(a) + b\r\n    a = np.where(z > 0, 1, 0)  # step activation\r\n\r\nprint(""predict:"", a)\r\nprint(""desired:"", Y.T)\r\nprint(""error  :"", (Y.T - a))\r\n'"
2x2_image_recognition.py,14,"b'# A trainable neural network for 2x2 image recognition.\r\n# The network shape is hardcoded via the W1, W2, W3 arrays (i.e. 3 layers, not counting the inputs).\r\n# Is able to recognize 4 different shapes in a 2x2 pixel image (see dataset.py for details on this).\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nimport dataset\r\nfrom activation import sigmoid\r\n\r\nnp.set_printoptions(formatter={""float"": ""{: 0.3f}"".format}, linewidth=np.inf)\r\nnp.random.seed(1)\r\n\r\nW1 = np.random.normal(size=(16, 4))  # layer 1 weights: 16 nodes with 4 inputs per node\r\nW2 = np.random.normal(size=(8, 16))  # layer 2 weights: 8 nodes with 16 inputs per node\r\nW3 = np.random.normal(size=(4, 8))  # layer 3 weights: 4 nodes with 8 inputs per node\r\n\r\nB1 = np.random.random(size=(16, 1))  # layer 1 bias: 16 nodes (by definition each with 1 bias)\r\nB2 = np.random.random(size=(8, 1))  # layer 2 bias: 8 nodes (by definition each with 1 bias)\r\nB3 = np.random.random(size=(4, 1))  # layer 3 bias: 4 nodes (by definition each with 1 bias)\r\n\r\n\r\ndef forward(x, predict=True):\r\n    a0 = x.T\r\n    z1 = W1.dot(a0) + B1\r\n    a1 = sigmoid(z1)\r\n    z2 = W2.dot(a1) + B2\r\n    a2 = sigmoid(z2)\r\n    z3 = W3.dot(a2) + B3\r\n    a3 = sigmoid(z3)\r\n\r\n    if predict is False:\r\n        return a0, z1, a1, z2, a2, z3, a3\r\n    return a3\r\n\r\n\r\ndef train(x, y, iterations=10000, learning_rate=0.1):\r\n    global W1, W2, W3, B1, B2, B3, error\r\n    m = x.shape[0]\r\n    error = []\r\n\r\n    for _ in range(iterations):\r\n        a0, z1, a1, z2, a2, z3, a3 = forward(x, predict=False)\r\n\r\n        da3 = a3 - y.T\r\n        dz3 = da3 * sigmoid(z3, derivative=True)\r\n        dw3 = dz3.dot(a2.T) / m\r\n        db3 = np.sum(dz3, axis=1, keepdims=True) / m\r\n\r\n        da2 = W3.T.dot(dz3)\r\n        dz2 = np.multiply(da2, sigmoid(z2, derivative=True))\r\n        dw2 = dz2.dot(a1.T) / m\r\n        db2 = np.sum(dz2, axis=1, keepdims=True) / m\r\n\r\n        da1 = W2.T.dot(dz2)\r\n        dz1 = np.multiply(da1, sigmoid(z1, derivative=True))\r\n        dw1 = dz1.dot(a0.T) / m\r\n        db1 = np.sum(dz1, axis=1, keepdims=True) / m\r\n\r\n        W1 -= learning_rate * dw1\r\n        B1 -= learning_rate * db1\r\n        W2 -= learning_rate * dw2\r\n        B2 -= learning_rate * db2\r\n        W3 -= learning_rate * dw3\r\n        B3 -= learning_rate * db3\r\n\r\n        error.append(np.average(da2 ** 2))\r\n\r\n    return error\r\n\r\n\r\nX, Y = dataset.image2x2_set()\r\n\r\nerror = train(X, Y, iterations=50000, learning_rate=0.2)\r\n\r\nplt.plot(error)\r\nplt.xlabel(""training iterations"")\r\nplt.ylabel(""mse"")\r\n\r\nY_hat = forward(X)\r\n\r\nprint(""predict:"", Y_hat)\r\nprint(""desired:"", Y.T)\r\nprint(""error  :"", Y.T - Y_hat)\r\n\r\nplt.show()\r\n'"
activation.py,10,"b'# activation functions for usage in a neural network\r\n\r\nimport numpy as np\r\n\r\n\r\ndef sigmoid(z, derivative=False):\r\n    if derivative:\r\n        z = sigmoid(z)\r\n        return z * (1 - z)\r\n    z = np.clip(z, -500, 500)  # avoid overflow\r\n    return 1 / (1 + np.exp(-z))\r\n\r\n\r\ndef linear(z, derivative=False):\r\n    if derivative:\r\n        return np.ones(z.shape)\r\n    return z\r\n\r\n\r\ndef gaussian(z, derivative=False):\r\n    if derivative:\r\n        return -2 * z * np.exp(-z ** 2)\r\n    return np.exp(-z ** 2)\r\n\r\n\r\ndef tanh(z, derivative=False):\r\n    if derivative:\r\n        return 1.0 - np.tanh(z) ** 2\r\n    return np.tanh(z)\r\n\r\n\r\ndef relu(z, derivative=False):\r\n    if derivative:\r\n        np.greater(z, 0)\r\n    return np.maximum(0, z)\r\n\r\n\r\ndef softplus(z, derivative=False):\r\n    # A ReLU alternative which is really differentiable\r\n    if derivative:\r\n        z = softplus(z)\r\n        return z * (1 - z)\r\n    return np.log(1.0 + np.exp(-abs(z))) + max(z, 0)  # variant on np.log(1.0 + np.exp(z)) which cannot overflow\r\n'"
backpropagation.py,0,"b'# Backpropagation example\r\n# A neural network of 1 layer with 1 node, without a bias.\r\n# Goal is to train the network so that W is such that input of X results in output of Y\r\n# Great explanation by Mikael Laine at https://www.youtube.com/watch?v=8d6jf7s6_Qs\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\nX = 1.50  # input\r\nY = 0.75  # desired output\r\n\r\nW = -0.12  # random initial weight\r\n\r\nlearning_rate = 0.001\r\niterations = 1000\r\nerror = []\r\n\r\nprint(""initial :"", ""W = {:0.3f}"".format(W))\r\n\r\nfor _ in range(iterations):\r\n    # forward pass\r\n    a = W * X  # actual output\r\n\r\n    # back propagation\r\n    e = (a - Y) ** 2  # error measured via MSE\r\n    error.append(e)\r\n\r\n    da = 2 * (a - Y)  # da = d_e / d_a = derivative of e = how changes in a change e\r\n    dw = X  # dw = d_a / d_W = X = derivative of a = how changes in W change a\r\n    W -= learning_rate * da * dw  # adjust W to negative of gradient (+ chain rule)\r\n\r\nprint(""result  : W = {:0.3f} (after {} iterations)"".format(W, iterations))\r\nprint(""expected: W = {:0.3f}"".format(Y / X))\r\n\r\nplt.plot(range(iterations), error)\r\nplt.title(""MSE (mean squared error)"")\r\nplt.xlabel(""training iterations"")\r\nplt.ylabel(""mse"")\r\nplt.show()\r\n'"
cubic.py,11,"b'# Third degree polynomial solved by a neural network\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nfrom activation import sigmoid, linear\r\n\r\nnp.set_printoptions(formatter={""float"": ""{: 0.3f}"".format}, linewidth=np.inf)\r\nnp.random.seed(1)\r\n\r\nx = np.linspace(-5, +5, 1000)\r\ny = x ** 3 - 15 * x\r\n\r\nX = x.reshape(-1, 1)  # from [x] to [1][x]\r\nY = y.reshape(-1, 1)\r\n\r\nn_input_nodes = 1\r\nn_hidden_nodes = 8  # the higher the number of nodes the better the fit (e.g. try values from 2 to 10)\r\nn_output_nodes = 1\r\n\r\nW1 = np.random.normal(size=(n_hidden_nodes, n_input_nodes))  # layer 1 weights\r\nW2 = np.random.normal(size=(n_output_nodes, n_hidden_nodes))  # layer 2 weights\r\n\r\nB1 = np.random.random(size=(n_hidden_nodes, 1))  # layer 1 bias\r\nB2 = np.random.random(size=(n_output_nodes, 1))  # layer 2 bias\r\n\r\n\r\ndef forward(x, predict=True):\r\n    a0 = x.T\r\n    z1 = W1.dot(a0) + B1\r\n    a1 = sigmoid(z1)\r\n    z2 = W2.dot(a1) + B2\r\n    a2 = linear(z2)\r\n    if predict is False:\r\n        return a0, z1, a1, z2, a2\r\n    return a2\r\n\r\n\r\ndef train(x, y, iterations=50000, learning_rate=0.001):\r\n    global W1, W2, B1, B2, error\r\n    m = x.shape[0]\r\n    error = []\r\n\r\n    for _ in range(iterations):\r\n        a0, z1, a1, z2, a2 = forward(x, predict=False)\r\n\r\n        da2 = a2 - y.T\r\n        dz2 = da2 * linear(z2, derivative=True)\r\n        dw2 = dz2.dot(a1.T) / m\r\n        db2 = np.sum(dz2, axis=1, keepdims=True) / m\r\n\r\n        da1 = W2.T.dot(dz2)\r\n        dz1 = np.multiply(da1, sigmoid(z1, derivative=True))\r\n        dw1 = dz1.dot(a0.T) / m\r\n        db1 = np.sum(dz1, axis=1, keepdims=True) / m\r\n\r\n        W1 -= learning_rate * dw1\r\n        B1 -= learning_rate * db1\r\n        W2 -= learning_rate * dw2\r\n        B2 -= learning_rate * db2\r\n\r\n        error.append(np.average(da2 ** 2))\r\n\r\n    return error\r\n\r\n\r\nerror = train(X, Y, iterations=10000, learning_rate=0.002)  # lower learning rates give a better fit\r\n\r\nplt.plot(error)\r\nplt.title(""MSE (mean squared error)"")\r\nplt.xlabel(""training iterations"")\r\nplt.ylabel(""mse"")\r\nplt.show()\r\n\r\nY_hat = forward(X)\r\n\r\nplt.plot(x, y, label=r""function: $x^3 - 15x$"")\r\nplt.plot(x, Y_hat.T, label=""prediction ({} nodes)"".format(n_hidden_nodes))\r\nplt.legend()\r\nplt.show()\r\n'"
dataset.py,9,"b'# example datasets which can be used in training and evaluating a neural network\r\n\r\nimport numpy as np\r\n\r\n\r\ndef xor_2_input():\r\n    """""" Truth table for a XOR operation with 2 inputs. """"""\r\n    X = np.array([[0, 0],\r\n                  [0, 1],\r\n                  [1, 0],\r\n                  [1, 1]])  # input\r\n\r\n    Y = np.array([[0.],\r\n                  [1.],\r\n                  [1.],\r\n                  [0.]])  # output (true if odd number of inputs is true)\r\n\r\n    return X, Y\r\n\r\n\r\ndef xor_3_input():\r\n    """""" Truth table for a XOR operation with 3 inputs. """"""\r\n    X = np.array([[0, 0, 0],\r\n                  [0, 0, 1],\r\n                  [0, 1, 0],\r\n                  [0, 1, 1],\r\n                  [1, 0, 0],\r\n                  [1, 0, 1],\r\n                  [1, 1, 0],\r\n                  [1, 1, 1]])  # input\r\n\r\n    Y = np.array([[0.],\r\n                  [1.],\r\n                  [1.],\r\n                  [0.],\r\n                  [1.],\r\n                  [0.],\r\n                  [0.],\r\n                  [1.]])  # output (true if odd number of inputs is true)\r\n\r\n    return X, Y\r\n\r\n\r\ndef image2x2(vertical_only=False):\r\n    """""" Flattened 2 x 2 pixel images, each stored as 1D array of 4 long.\r\n        Index of pixel in array: 0 | 1\r\n                                 -----\r\n                                 2 | 3\r\n    """"""\r\n    X = np.array([\r\n        [0, 0, 0, 0],  # solid white\r\n        [1, 1, 1, 1],  # solid black\r\n        [1, 1, 0, 0],  # horizontal line, top\r\n        [0, 0, 1, 1],  # horizontal line, bottom\r\n        [1, 0, 1, 0],  # vertical line, left\r\n        [0, 1, 0, 1],  # vertical line, right\r\n        [1, 0, 0, 1],  # diagonal line, top-left bottom-right\r\n        [0, 1, 1, 0],  # diagonal line, bottom-left top-right\r\n        [1, 0, 0, 0],  # unknown shape\r\n        [0, 1, 0, 0],  # unknown shape\r\n        [0, 0, 1, 0],  # unknown shape\r\n        [0, 0, 0, 1],  # unknown shape\r\n        [1, 1, 0, 1],  # unknown shape\r\n        [1, 1, 1, 0],  # unknown shape\r\n        [0, 1, 1, 1],  # unknown shape\r\n        [1, 0, 1, 1]  # unknown shape\r\n    ])  # input: all possible image shapes (in black and white)\r\n\r\n    Y = np.array([\r\n        [1, 0, 0, 0],  # solid\r\n        [1, 0, 0, 0],  # solid\r\n        [0, 1, 0, 0],  # horizontal\r\n        [0, 1, 0, 0],  # horizontal\r\n        [0, 0, 1, 0],  # vertical\r\n        [0, 0, 1, 0],  # vertical\r\n        [0, 0, 0, 1],  # diagonal\r\n        [0, 0, 0, 1],  # diagonal\r\n        [0, 0, 0, 0],  # unknown shape\r\n        [0, 0, 0, 0],  # unknown shape\r\n        [0, 0, 0, 0],  # unknown shape\r\n        [0, 0, 0, 0],  # unknown shape\r\n        [0, 0, 0, 0],  # unknown shape\r\n        [0, 0, 0, 0],  # unknown shape\r\n        [0, 0, 0, 0],  # unknown shape\r\n        [0, 0, 0, 0]  # unknown shape\r\n    ])  # output: all possible labels (one-hot vector)\r\n\r\n    Y_vertical_only = np.array([[0], [0], [0], [0], [1], [1], [0], [0],\r\n                                [0], [0], [0], [0], [0], [0], [0], [0]])  # output: labels for vertical line only\r\n\r\n    return X, Y if vertical_only is False else Y_vertical_only\r\n\r\n\r\ndef image2x2_set(no_of_sets=1, vertical_only=False):\r\n    """""" Return one or more sets of grayscale images. Every set contains all possible shapes (so 16 images). """"""\r\n    grey_range = 0.2\r\n    images = []\r\n    labels = []\r\n\r\n    img, lbl = image2x2(vertical_only=vertical_only)  # get the black and white images\r\n\r\n    for _ in range(no_of_sets):  # convert to grayscale\r\n        for (image, label) in zip(img, lbl):\r\n            images.append([np.random.uniform(0.00, grey_range) if not p else np.random.uniform(1 - grey_range, 1.00)\r\n                           for p in image])\r\n            labels.append(label)\r\n\r\n    return np.array(images), np.array(labels)\r\n'"
linear.py,7,"b'# A neural network which approximates linear function y = 2x + 3.\r\n# The network has 1 layer with 1 node, which has 1 input (and a bias).\r\n# As there is no activation effectively this node is a linear function.\r\n# After +/- 10.000 iterations W should be close to 2 and B should be close to 3.\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nnp.set_printoptions(formatter={""float"": ""{: 0.3f}"".format}, linewidth=np.inf)\r\nnp.random.seed(1)\r\n\r\nX = np.array([[0], [1], [2], [3], [4]])  # X = input (here: 5 values)\r\nY = 2 * X + 3  # Y = output: y = 2x + 3 (as many values as there are X\'s)\r\n\r\nW = np.random.normal(scale=0.1, size=(1, 1))  # layer: (1, 1) = 1 node with 1 input\r\nB = np.random.normal(scale=0.1, size=(1, 1))  # bias: (1, 1) = for 1 node (and by definition only 1 bias value per node)\r\n\r\nlearning_rate = 0.001\r\niterations = 10000\r\nerror = []\r\n\r\nprint(""initial :"", ""W ="", W, ""B ="", B, ""(random initialization)"")\r\n\r\nm = X.shape[0]\r\n\r\nfor _ in range(iterations):\r\n    # forward pass\r\n    a = W.dot(X.T) + B\r\n\r\n    # back propagation\r\n    da = a - Y.T  # da = error\r\n    dz = da  # no activation\r\n    dw = dz.dot(X) / m\r\n    db = np.sum(dz, axis=1, keepdims=True) / m\r\n\r\n    W -= learning_rate * dw\r\n    B -= learning_rate * db\r\n\r\n    error.append(np.average(da ** 2))\r\n\r\nprint(""result  :"", ""W ="", W, ""B ="", B, ""(after {} iterations)"".format(iterations))\r\nprint(""expected: W = 2, B = 3"")\r\n\r\nplt.plot(range(iterations), error)\r\nplt.title(""MSE (mean squared error)"")\r\nplt.xlabel(""training iterations"")\r\nplt.ylabel(""mse"")\r\nplt.show()\r\n'"
network1.py,8,"b'# Fully configurable neural network in numpy\r\n#\r\n# Usage: NN(shape_tuple, activation_tuple)\r\n# shape_tuple: (nodes_in_input_layer, nodes_in_hidden_layer_1, ..., nodes_in_output_layer)\r\n# activation_tuple: (None, activation_function_for_hidden_layer_1, ..., activation_function_for_output_layer)\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nimport dataset\r\nfrom activation import sigmoid\r\n\r\nnp.set_printoptions(formatter={""float"": ""{: 0.3f}"".format}, linewidth=np.inf)\r\nnp.random.seed(1)\r\n\r\n\r\nclass NN:\r\n    def __init__(self, shape, activation=None):\r\n        self.num_layers = len(shape) - 1  # the input layer does not count as a layer\r\n        self.weight = []\r\n        self.bias = []\r\n\r\n        if activation is None:  # activation tuple is optional\r\n            self.activation = [sigmoid for _ in range(self.num_layers)]\r\n        else:\r\n            if len(shape) != len(activation):\r\n                raise ValueError(""nr of layers ({}) must match activations ({})"".format(len(shape), len(activation)))\r\n            self.activation = activation[1:]  # skip the activation function for the input layer\r\n\r\n        self.a = []  # layers output after activation, input to the next layer\r\n        self.z = []  # layers results before activation\r\n        self.dw = []  # empty array used for weight update during training\r\n        self.db = []  # empty array used for bias update during training\r\n\r\n        for (layer1, layer2) in zip(shape[:-1], shape[1:]):\r\n            self.weight.append(np.random.normal(size=(layer2, layer1)))\r\n            self.bias.append(np.random.normal(size=(layer2, 1)))\r\n            self.dw.append(np.zeros((layer2, layer1)))\r\n            self.db.append(np.zeros((layer2, 1)))\r\n\r\n    def forward(self, x):\r\n        self.a = [x.T]  # a[0] is the input for layer 1 (layer 0 is the input layer)\r\n        self.z = [None]\r\n\r\n        for (weight, bias, activation) in zip(self.weight, self.bias, self.activation):\r\n            self.z.append(weight.dot(self.a[-1]) + bias)\r\n            self.a.append(activation(self.z[-1]))\r\n\r\n        return self.a[-1].T\r\n\r\n    def back_propagation(self, x, y, learning_rate=0.1, momentum=0.5):\r\n        m = x.shape[0]\r\n        delta_w = []\r\n        delta_b = []\r\n\r\n        y_hat = self.forward(x)\r\n        error = np.sum((y_hat - y) ** 2)\r\n\r\n        for index in reversed(range(1, self.num_layers + 1)):\r\n            if index == self.num_layers:\r\n                da = self.a[index] - y.T\r\n            else:\r\n                da = self.weight[index].T.dot(dz)\r\n            dz = da * self.activation[index - 1](self.z[index], derivative=True)\r\n            dw = dz.dot(self.a[index - 1].T) / m\r\n            db = np.sum(dz, axis=1, keepdims=True) / m\r\n\r\n            delta_w.append(dw)\r\n            delta_b.append(db)\r\n\r\n        for (index, dw, db) in zip(reversed(range(self.num_layers)), delta_w, delta_b):\r\n            self.dw[index] = learning_rate * dw + momentum * self.dw[index]\r\n            self.weight[index] -= self.dw[index]\r\n            self.db[index] = learning_rate * db + momentum * self.db[index]\r\n            self.bias[index] -= self.db[index]\r\n\r\n        return error\r\n\r\n    def train(self, x, y, iterations=10000, learning_rate=0.2, momentum=0.5, verbose=True):\r\n        min_error = 1e-5\r\n        loss = []\r\n\r\n        for i in range(iterations + 1):\r\n            error = self.back_propagation(x, y, learning_rate=learning_rate, momentum=momentum)\r\n            loss.append(error)\r\n            if verbose:\r\n                if i % 2500 == 0:\r\n                    print(""iteration {:5d} error: {:0.6f}"".format(i, error))\r\n                if error <= min_error:\r\n                    print(""minimum error {} reached at iteration {}"".format(min_error, i))\r\n                    break\r\n\r\n        return loss\r\n\r\n\r\nX, Y = dataset.xor_3_input()\r\n\r\nnetwork = NN((3, 3, 1), (None, sigmoid, sigmoid))\r\n\r\nerror = network.train(X, Y, 50000, learning_rate=0.2)\r\n\r\nplt.plot(error)\r\nplt.xlabel(""training iterations"")\r\nplt.ylabel(""mse"")\r\n\r\nY_hat = network.forward(X)\r\n\r\nprint(""predict:"", Y_hat.T)\r\nprint(""desired:"", Y.T)\r\nprint(""loss   :"", (Y - Y_hat).T)\r\n\r\nplt.show()\r\n'"
xor.py,10,"b'# A neural network which approximates a logical operation (AND, OR, XOR, NAND).\r\n# The shape of the network is hardcoded: one hidden and one output layer (the input\r\n# layer does not count as a layer). Configuration of the layers is done via weight\r\n# arrays W1 and W2. The number of nodes is defined using variables n_input_nodes,\r\n# n_hidden_nodes and n_output_nodes.\r\n#\r\n# The example shows that the code of forward() and train() does not need to change\r\n# when the number of nodes changes (as long as the number of layers remains the same).\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nimport dataset\r\nfrom activation import sigmoid\r\n\r\nnp.set_printoptions(formatter={""float"": ""{: 0.3f}"".format}, linewidth=np.inf)\r\nnp.random.seed(1)\r\n\r\ntype = ""xor_3_input""  # choose ""xor_2_input"" or ""xor_3_input""\r\n\r\nif type == ""xor_2_input"":\r\n    X, Y = dataset.xor_2_input()\r\n    n_input_nodes = 2\r\n    n_hidden_nodes = 2\r\n    n_output_nodes = 1\r\nelse:  # type == ""xor_3_input"":\r\n    X, Y = dataset.xor_3_input()\r\n    n_input_nodes = 3\r\n    n_hidden_nodes = 3\r\n    n_output_nodes = 1\r\n\r\nW1 = np.random.normal(size=(n_hidden_nodes, n_input_nodes))  # layer 1 weights\r\nW2 = np.random.normal(size=(n_output_nodes, n_hidden_nodes))  # layer 2 weights\r\n\r\nB1 = np.random.random(size=(n_hidden_nodes, 1))  # layer 1 bias\r\nB2 = np.random.random(size=(n_output_nodes, 1))  # layer 2 bias\r\n\r\n\r\ndef forward(x, predict=True):\r\n    a0 = x.T\r\n    z1 = W1.dot(a0) + B1\r\n    a1 = sigmoid(z1)\r\n    z2 = W2.dot(a1) + B2\r\n    a2 = sigmoid(z2)\r\n    if predict is False:\r\n        return a0, z1, a1, z2, a2\r\n    return a2\r\n\r\n\r\ndef train(x, y, iterations=10000, learning_rate=0.1):\r\n    global W1, W2, B1, B2, error\r\n    m = x.shape[0]\r\n    error = []\r\n\r\n    for _ in range(iterations):\r\n        a0, z1, a1, z2, a2 = forward(x, predict=False)\r\n\r\n        da2 = a2 - y.T\r\n        dz2 = da2 * sigmoid(z2, derivative=True)\r\n        dw2 = dz2.dot(a1.T) / m\r\n        db2 = np.sum(dz2, axis=1, keepdims=True) / m\r\n\r\n        da1 = W2.T.dot(dz2)\r\n        dz1 = np.multiply(da1, sigmoid(z1, derivative=True))\r\n        dw1 = dz1.dot(a0.T) / m\r\n        db1 = np.sum(dz1, axis=1, keepdims=True) / m\r\n\r\n        W1 -= learning_rate * dw1\r\n        B1 -= learning_rate * db1\r\n        W2 -= learning_rate * dw2\r\n        B2 -= learning_rate * db2\r\n\r\n        error.append(np.average(da2 ** 2))\r\n\r\n    return error\r\n\r\n\r\nerror = train(X, Y, iterations=50000, learning_rate=0.2)\r\n\r\nplt.plot(error)\r\nplt.xlabel(""training iterations"")\r\nplt.ylabel(""mse"")\r\n\r\nY_hat = forward(X)\r\n\r\nprint(""predict:"", Y_hat)\r\nprint(""desired:"", Y.T)\r\nprint(""error  :"", Y.T - Y_hat)\r\n\r\nplt.show()\r\n'"
