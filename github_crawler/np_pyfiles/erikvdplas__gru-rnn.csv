file_path,api_count,code
main.py,58,"b""import numpy as np\n\n# Seed random\nnp.random.seed(0)\n\n# Read data and setup maps for integer encoding and decoding.\ndata = open('input.txt', 'r').read()\nchars = sorted(list(set(data))) # Sort makes model predictable (if seeded).\ndata_size, vocab_size = len(data), len(chars)\nprint('data has %d characters, %d unique.' % (data_size, vocab_size))\nchar_to_ix = { ch:i for i,ch in enumerate(chars) }\nix_to_char = { i:ch for i,ch in enumerate(chars) }\n\n# Activation functions\n# NOTE: Derivatives are calculated using outcomes of their primitives (which are already calculated during forward prop).\ndef sigmoid(input, deriv=False):\n    if deriv:\n        return input*(1-input)\n    else:\n        return 1 / (1 + np.exp(-input))\n\ndef tanh(input, deriv=False):\n    if deriv:\n        return 1 - input ** 2\n    else:\n        return np.tanh(input)\n\n# Derivative is directly calculated in backprop (in combination with cross-entropy loss function).\ndef softmax(input):\n    # Subtraction of max value improves numerical stability.\n    e_input = np.exp(input - np.max(input))\n    return e_input / e_input.sum()\n\n# Hyper parameters\nN, h_size, o_size = vocab_size, vocab_size, vocab_size # Hidden size is set to vocab_size, assuming that level of abstractness is approximately proportional to vocab_size (but can be set to any other value).\nseq_length = 25 # Longer sequence lengths allow for lengthier latent dependencies to be trained.\nlearning_rate = 1e-1\n\n# Model parameter initialization\nWz = np.random.rand(h_size, N) * 0.1 - 0.05\nUz = np.random.rand(h_size, h_size) * 0.1 - 0.05\nbz = np.zeros((h_size, 1))\n\nWr = np.random.rand(h_size, N) * 0.1 - 0.05\nUr = np.random.rand(h_size, h_size) * 0.1 - 0.05\nbr = np.zeros((h_size, 1))\n\nWh = np.random.rand(h_size, N) * 0.1 - 0.05\nUh = np.random.rand(h_size, h_size) * 0.1 - 0.05\nbh = np.zeros((h_size, 1))\n\nWy = np.random.rand(o_size, h_size) * 0.1 - 0.05\nby = np.zeros((o_size, 1))\n\ndef lossFun(inputs, targets, hprev):\n    # Initialize variables\n    x, z, r, h_hat, h, y, p = {}, {}, {}, {}, {-1: hprev}, {}, {} # Dictionaries contain variables for each timestep.\n    sequence_loss = 0\n\n    # Forward prop\n    for t in range(len(inputs)):\n        # Set up one-hot encoded input\n        x[t] = np.zeros((vocab_size, 1))\n        x[t][inputs[t]] = 1\n        \n        # Calculate update and reset gates\n        z[t] = sigmoid(np.dot(Wz, x[t]) + np.dot(Uz, h[t-1]) + bz)\n        r[t] = sigmoid(np.dot(Wr, x[t]) + np.dot(Ur, h[t-1]) + br)\n        \n        # Calculate hidden units\n        h_hat[t] = tanh(np.dot(Wh, x[t]) + np.dot(Uh, np.multiply(r[t], h[t-1])) + bh)\n        h[t] = np.multiply(z[t], h[t-1]) + np.multiply((1 - z[t]), h_hat[t])\n        \n        # Regular output unit\n        y[t] = np.dot(Wy, h[t]) + by\n        \n        # Probability distribution\n        p[t] = softmax(y[t])\n        \n        # Cross-entropy loss\n        loss = -np.sum(np.log(p[t][targets[t]]))\n        sequence_loss += loss\n\n    # Parameter gradient initialization\n    dWy, dWh, dWr, dWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n    dUh, dUr, dUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n    dby, dbh, dbr, dbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n    dhnext = np.zeros_like(h[0])\n    \n    # Backward prop\n    for t in reversed(range(len(inputs))):\n        # \xe2\x88\x82loss/\xe2\x88\x82y\n        dy = np.copy(p[t])\n        dy[targets[t]] -= 1\n        \n        # \xe2\x88\x82loss/\xe2\x88\x82Wy and \xe2\x88\x82loss/\xe2\x88\x82by\n        dWy += np.dot(dy, h[t].T)\n        dby += dy\n        \n        # Intermediary derivatives\n        dh = np.dot(Wy.T, dy) + dhnext\n        dh_hat = np.multiply(dh, (1 - z[t]))\n        dh_hat_l = dh_hat * tanh(h_hat[t], deriv=True)\n        \n        # \xe2\x88\x82loss/\xe2\x88\x82Wh, \xe2\x88\x82loss/\xe2\x88\x82Uh and \xe2\x88\x82loss/\xe2\x88\x82bh\n        dWh += np.dot(dh_hat_l, x[t].T)\n        dUh += np.dot(dh_hat_l, np.multiply(r[t], h[t-1]).T)\n        dbh += dh_hat_l\n        \n        # Intermediary derivatives\n        drhp = np.dot(Uh.T, dh_hat_l)\n        dr = np.multiply(drhp, h[t-1])\n        dr_l = dr * sigmoid(r[t], deriv=True)\n        \n        # \xe2\x88\x82loss/\xe2\x88\x82Wr, \xe2\x88\x82loss/\xe2\x88\x82Ur and \xe2\x88\x82loss/\xe2\x88\x82br\n        dWr += np.dot(dr_l, x[t].T)\n        dUr += np.dot(dr_l, h[t-1].T)\n        dbr += dr_l\n        \n        # Intermediary derivatives\n        dz = np.multiply(dh, h[t-1] - h_hat[t])\n        dz_l = dz * sigmoid(z[t], deriv=True)\n        \n        # \xe2\x88\x82loss/\xe2\x88\x82Wz, \xe2\x88\x82loss/\xe2\x88\x82Uz and \xe2\x88\x82loss/\xe2\x88\x82bz\n        dWz += np.dot(dz_l, x[t].T)\n        dUz += np.dot(dz_l, h[t-1].T)\n        dbz += dz_l\n        \n        # All influences of previous layer to loss\n        dh_fz_inner = np.dot(Uz.T, dz_l)\n        dh_fz = np.multiply(dh, z[t])\n        dh_fhh = np.multiply(drhp, r[t])\n        dh_fr = np.dot(Ur.T, dr_l)\n        \n        # \xe2\x88\x82loss/\xe2\x88\x82h\xf0\x9d\x91\xa1\xe2\x82\x8b\xe2\x82\x81\n        dhnext = dh_fz_inner + dh_fz + dh_fhh + dh_fr\n\n    return sequence_loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, h[len(inputs) - 1]\n\ndef sample(h, seed_ix, n):\n    # Initialize first word of sample ('seed') as one-hot encoded vector.\n    x = np.zeros((vocab_size, 1))\n    x[seed_ix] = 1\n    ixes = [seed_ix]\n    \n    for t in range(n):\n        # Calculate update and reset gates\n        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n        \n        # Calculate hidden units\n        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n        \n        # Regular output unit\n        y = np.dot(Wy, h) + by\n        \n        # Probability distribution\n        p = softmax(y)\n\n        # Choose next char according to the distribution\n        ix = np.random.choice(range(vocab_size), p=p.ravel())\n        x = np.zeros((vocab_size, 1))\n        x[ix] = 1\n        ixes.append(ix)\n    \n    return ixes\n\n# Initialize sampling parameters and memory gradients (for adagrad)\nn, p = 0, 0\nmdWy, mdWh, mdWr, mdWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\nmdUh, mdUr, mdUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\nmdby, mdbh, mdbr, mdbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\nsmooth_loss = -np.log(1.0/vocab_size)*seq_length\n\nprint_interval = 100\n\nwhile True:\n    # Reset memory if appropriate\n    if p + seq_length + 1 >= len(data) or n == 0:\n        hprev = np.zeros((h_size, 1))\n        p = 0\n    \n    # Get input and target sequence\n    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n\n    # Occasionally sample from model and print result\n    if n % print_interval == 0:\n        sample_ix = sample(hprev, inputs[0], 1000)\n        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n        print('----\\n%s\\n----' % (txt, ))\n\n    # Get gradients for current model based on input and target sequences\n    loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, hprev = lossFun(inputs, targets, hprev)\n    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n\n    # Occasionally print loss information\n    if n % print_interval == 0:\n        print('iter %d, loss: %f, smooth loss: %f' % (n, loss, smooth_loss))\n\n    # Update model with adagrad (stochastic) gradient descent\n    for param, dparam, mem in zip([Wy,  Wh,  Wr,  Wz,  Uh,  Ur,  Uz,  by,  bh,  br,  bz],\n                                  [dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz],\n                                  [mdWy,mdWh,mdWr,mdWz,mdUh,mdUr,mdUz,mdby,mdbh,mdbr,mdbz]):\n        np.clip(dparam, -5, 5, out=dparam)\n        mem += dparam * dparam\n        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # Small added term for numerical stability\n\n    # Prepare for next iteration\n    p += seq_length\n    n += 1\n"""
