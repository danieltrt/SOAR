file_path,api_count,code
my_autograd.py,22,"b'import numpy as np\n\nclass Variable():\n    __counter = 0\n    def __init__(self,data,is_leaf=True,backward_fun=None):\n        if backward_fun is None and not is_leaf:\n            raise ValueError(\'non leaf nodes require backward_fun\')\n        if np.isscalar(data):\n            data = np.ones(1)*data\n        if not isinstance(data,np.ndarray):\n            raise ValueError(f\'data should be of type ""numpy.ndarray"" or a scalar,but received {type(data)}\')\n        self.data = data\n        self.id = Variable.__counter\n        Variable.__counter += 1\n        self.is_leaf = is_leaf\n        self.prev = []\n        self.backward_fun = backward_fun\n        self.zero_grad()\n\n    def backward(self):\n        self.backward_fun(dy=self.grad)\n\n    def zero_grad(self):\n        self.grad = np.zeros(self.data.shape)\n\n    def step(self,lr):\n        self.data -= lr*self.grad\n\n\n    def __repr__(self):\n        return f\'Variable(id:{self.id},prev:{list(map(lambda a:a.id,self.prev))},is_leaf:{self.is_leaf})\\n\'\n\n\n\ndef plus(a,b):\n    if not (isinstance(a,Variable) and isinstance(b,Variable)):\n        raise ValueError(\'a,b needs to be a Variable instance\')\n    def b_fun(dy):\n        b.grad += dy\n        a.grad += dy\n\n    res = Variable(a.data+b.data,is_leaf=False,backward_fun=b_fun)\n    res.prev.extend([a,b])\n    return res\n\ndef plus_bcast(a,b):\n    """"""\n    a being a matrix(mini-batch output m*n)\n    b being a vector(bias n)\n    """"""\n    if not (isinstance(a,Variable) and isinstance(b,Variable)):\n        raise ValueError(\'a,b needs to be a Variable instance\')\n    def b_fun(dy):\n        b.grad += dy.sum(axis=0)\n        a.grad += dy\n\n    res = Variable(a.data+b.data,is_leaf=False,backward_fun=b_fun)\n    res.prev.extend([a,b])\n    return res\n\n\n# def absolute(a):\n#     if not (isinstance(a,Variable)):\n#         raise ValueError(\'a needs to be a Variable\')\n#     def b_fun(dy=1):\n#         mask = np.ones(dy.shape)\n#         mask[a.data<0]=-1\n#         a.grad += mask*dy\n#\n#     res = Variable(np.abs(a.data),is_leaf=False,backward_fun=b_fun)\n#     res.prev.append(a)\n#     return res\n\ndef minus(a,b):\n    if not (isinstance(a,Variable) and isinstance(b,Variable)):\n        raise ValueError(\'a,b needs to be a Variable instance\')\n    def b_fun(dy):\n        b.grad += -dy\n        a.grad += dy\n    res = Variable(a.data-b.data,is_leaf=False,backward_fun=b_fun)\n    res.prev.extend([a,b])\n    return res\n\ndef sumel(a):\n    if not (isinstance(a,Variable)):\n        raise ValueError(\'a needs to be a Variable\')\n    def b_fun(dy=1):\n        a.grad += np.ones(a.data.shape)*dy\n\n    res = Variable(np.sum(a.data),is_leaf=False,backward_fun=b_fun)\n    res.prev.append(a)\n    return res\n\ndef transpose(a):\n    if not (isinstance(a,Variable)):\n        raise ValueError(\'a needs to be a Variable\')\n    def b_fun(dy=1):\n        a.grad += dy.T\n\n    res = Variable(a.data.T,is_leaf=False,backward_fun=b_fun)\n    res.prev.append(a)\n    return res\n\ndef dot(a,b):\n    if not (isinstance(a,Variable) and isinstance(b,Variable)):\n            raise ValueError(\'a,b needs to be a Variable instance\')\n    def b_fun(dy):\n        if np.isscalar(dy):\n            dy = np.ones(1)*dy\n        a.grad += np.dot(dy,b.data.T)\n        b.grad += np.dot(a.data.T,dy)\n    res = Variable(np.dot(a.data,b.data),is_leaf=False,backward_fun=b_fun)\n    res.prev.extend([a,b])\n    return res\n\ndef multiply(a,b):\n    if not (isinstance(a,Variable) and isinstance(b,Variable)):\n            raise ValueError(\'a,b needs to be a Variable instance\')\n    def b_fun(dy):\n        if np.isscalar(dy):\n            dy = np.ones(1)*dy\n        a.grad += np.multiply(dy,b.data)\n        b.grad += np.multiply(dy,a.data)\n    res = Variable(np.multiply(a.data,b.data),is_leaf=False,backward_fun=b_fun)\n    res.prev.extend([a,b])\n    return res\n\ndef c_mul(a,c):\n    if not (isinstance(a,Variable) and isinstance(c,(int, float))):\n        raise ValueError(\'a needs to be a Variable, c needs to be one of (int, float)\')\n    def b_fun(dy=1):\n        a.grad += dy*c\n    res = Variable(a.data*c,is_leaf=False,backward_fun=b_fun)\n    res.prev.append(a)\n    return res\n\ndef relu(a):\n    if not (isinstance(a,Variable)):\n        raise ValueError(\'a needs to be a Variable\')\n    def b_fun(dy=1):\n        a.grad[a.data>0] += dy[a.data>0]\n\n    res = Variable(np.maximum(a.data, 0),is_leaf=False,backward_fun=b_fun)\n    res.prev.append(a)\n    return res\n\ndef __top_sort(var):\n    vars_seen = set()\n    top_sort = []\n    def top_sort_helper(vr):\n        if (vr in vars_seen) or vr.is_leaf:\n            pass\n        else:\n            vars_seen.add(vr)\n            for pvar in vr.prev:\n                top_sort_helper(pvar)\n            top_sort.append(vr)\n    top_sort_helper(var)\n    return top_sort\n\ndef backward_graph(var):\n    if not isinstance(var,Variable):\n        raise ValueError(\'var needs to be a Variable instance\')\n    tsorted = __top_sort(var)\n\n    var.grad=np.ones(var.data.shape)\n    for var in reversed(tsorted):\n        var.backward()\n\n\nclass LinearLayer():\n    def __init__(self,features_inp,features_out):\n        super(LinearLayer, self).__init__()\n        std = 1.0/features_inp\n        self.w = Variable(np.random.uniform(-std,std,(features_inp,features_out)))\n        self.b = Variable(np.random.uniform(-std,std,features_out))\n\n    def forward(self, inp):\n        return plus_bcast(dot(inp,self.w),self.b)\n\n    def zero_grad(self):\n        self.w.zero_grad()\n        self.b.zero_grad()\n\n    def step(self,lr):\n        self.w.step(lr)\n        self.b.step(lr)\n'"
setup_regression.py,11,"b'""""""\nGenerating target function, featurizer, and train/validation/test splits for the Lasso homework.\n\nAuthor: David S. Rosenberg <david.davidr@gmail.com>\nLicense: Creative Commons Attribution 4.0 International License\n""""""\n\nfrom sklearn.model_selection import train_test_split\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\n\n\ndef step_fn_generator(stepLoc=0):\n    def f(x):\n        ret = np.zeros(len(x))\n        ret[x >= stepLoc] = 1\n        return ret\n    return f\n\ndef linear_comb_generator(fns, coefs):\n    def f(x):\n        return sum(fns[i](x) * coefs[i] for i in range(len(fns)))\n    return f\n\ndef get_target_and_featurizer(num_basis_fns = 100, num_nonzero = 10, coefs_true=None):\n    # We\'ll create a basis of step functions on the interval [0,1]. We\'ll then\n    # construct a linear combination of these step functions to be our target\n    # function. We\'ll construct a function to ""featurize"" an input in [0,1]\n    # into the evaluations of all functions in the basis on the input.\n    # Optionally: If coefs_true is provided, they will be used to generate\n    # target_fn and featurize.\n    # Construct basis, to be used for generating target function\n\n    if coefs_true is not None:\n        num_basis_fns = len(coefs_true)\n    else:\n        nonzero_indices = np.random.choice(num_basis_fns, num_nonzero)\n        coefs_true = np.zeros(num_basis_fns)\n        coefs_true[nonzero_indices] = np.random.randn(num_nonzero)\n\n    all_basis_fns = [step_fn_generator(stepLoc=s)\n                     for s in np.linspace(0, 1, num_basis_fns, endpoint=False)]\n\n    # Construct target function (the Bayes prediction function)\n    target_fn = linear_comb_generator(all_basis_fns, coefs_true)\n\n    def featurize(x):\n        n = len(x)\n        # Featurize input values in [0,1]\n        X_ftrs = np.empty((n, num_basis_fns))\n        for ftr_num in range(num_basis_fns):\n            X_ftrs[:, ftr_num] = all_basis_fns[ftr_num](x)\n        return X_ftrs\n\n    return target_fn, coefs_true, featurize\n\ndef generate_data(target_fn, n=1000, noise_scale=.25, tdof=6):\n    # Construct dataset\n    x = np.sort(np.random.rand(n)) #chooses uniformly from [0,1)\n    y_target = target_fn(x)\n    y = y_target + noise_scale * np.random.standard_t(tdof,n)\n    return x, y\n\ndef get_data_splits(x, y, test_frac=.2):\n    ## Based on https://github.com/ageron/handson-ml/blob/master/02_end_to_end_machine_learning_project.ipynb\n    n = len(y)\n    shuffled_indices = np.random.permutation(n)\n    n_test = int(n * test_frac)\n    n_train = n - n_test\n    indices_test = shuffled_indices[:n_test]\n    indices_train = shuffled_indices[n_test:]\n    y_train = y[indices_train]\n    x_train = x[indices_train]\n    y_test = y[indices_test]\n    x_test = x[indices_test]\n    return x_train, y_train, x_test, y_test\n\ndef generate_problem(n=200, num_basis_fns=400, num_nonzero=10, noise_scale=.25, tdof=6, test_frac=.2, write_problem=False, file_name=""lasso_data.pickle""):\n    target_fn, coefs_true, featurize = get_target_and_featurizer(num_basis_fns, num_nonzero)\n    x, y = generate_data(target_fn, n, noise_scale, tdof)\n    x_train, y_train, x_test, y_test = get_data_splits(x, y, test_frac)\n\n    if write_problem:\n        print (""Saving problem to disk."")\n        data = {""coefs_true"":coefs_true, ""x_train"":x_train,\n                ""y_train"":np.copy(y_train), ""x_test"":x_test, ""y_test"":y_test}\n        with open(file_name, \'wb\') as outfile:\n            pickle.dump(data, outfile, protocol=2)\n\n    return x_train, y_train, x_test, y_test, target_fn, coefs_true, featurize\n\ndef reconstitute_problem(coefs_true, x_train, y_train, x_test, y_test):\n    target_fn, coefs_true, featurize = get_target_and_featurizer(coefs_true=coefs_true)\n    return x_train, y_train, x_test, y_test, target_fn, coefs_true, featurize\n\ndef load_problem(file_name):\n    f_myfile = open(file_name, \'rb\')\n    data = pickle.load(f_myfile)\n    f_myfile.close()\n    return reconstitute_problem(data[""coefs_true""], data[""x_train""], data[""y_train""],\n                                data[""x_test""], data[""y_test""])\n\ndef main():\n    lasso_data_fname = ""lasso_data.pickle""\n    LOAD_PROBLEM=True\n    GENERATE_PROBLEM=False\n    WRITE_PROBLEM=False\n    if GENERATE_PROBLEM:\n        n=1000\n        test_frac=.9\n        num_basis_fns=400\n        num_nonzero=10\n        noise_scale=.25 # scale factor on noise\n        tdof = 6 # degrees of freedom of t-distribution generating noise\n        x_train, y_train, x_val, y_val, target_fn, coefs_true, featurize = generate_problem(n=n, num_basis_fns=num_basis_fns, num_nonzero=num_nonzero, noise_scale=noise_scale, test_frac=test_frac, write_problem=WRITE_PROBLEM, file_name=lasso_data_fname)\n\n    if LOAD_PROBLEM:\n        x_train, y_train, x_val, y_val, target_fn, coefs_true, featurize = load_problem(lasso_data_fname)\n\n    # Let\'s plot the target function (i.e. the Bayes prediction function) as\n    # well as the noisy observations from the training data, as a function of\n    # the original input space, which is the interval [0,1).\n    fig, ax = plt.subplots()\n    plt.scatter(x_train, y_train, s=3, color=\'k\', label=\'Training data\')\n    x = np.arange(0,1,.001)\n    ax.plot(x, target_fn(x), \'r\', label=\'Target function (i.e. Bayes prediction function)\')\n    legend = ax.legend(loc=\'upper center\', shadow=True)\n    plt.show(block=False)\n\nif __name__ == \'__main__\':\n  main()\n'"
