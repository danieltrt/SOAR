file_path,api_count,code
fabfile.py,0,"b""# coding: utf-8\nfrom __future__ import unicode_literals, print_function\n\nimport contextlib\nfrom pathlib import Path\nfrom fabric.api import local, lcd, env, settings, prefix\nfrom os import path, environ\nimport shutil\nimport sys\n\n\nPWD = path.dirname(__file__)\nENV = environ['VENV_DIR'] if 'VENV_DIR' in environ else '.env'\nVENV_DIR = Path(PWD) / ENV\n\n\n@contextlib.contextmanager\ndef virtualenv(name, create=False, python='/usr/bin/python3.6'):\n    python = Path(python).resolve()\n    env_path = VENV_DIR\n    if create:\n        if env_path.exists():\n            shutil.rmtree(str(env_path))\n        local('{python} -m venv {env_path}'.format(python=python, env_path=VENV_DIR))\n    def wrapped_local(cmd, env_vars=[], capture=False, direct=False):\n        return local('source {}/bin/activate && {}'.format(env_path, cmd),\n                     shell='/bin/bash', capture=False)\n    yield wrapped_local\n\n\ndef env(lang='python3.6'):\n    if VENV_DIR.exists():\n        local('rm -rf {env}'.format(env=VENV_DIR))\n    if lang.startswith('python3'):\n        local('{lang} -m venv {env}'.format(lang=lang, env=VENV_DIR))\n    else:\n        local('{lang} -m pip install virtualenv --no-cache-dir'.format(lang=lang))\n        local('{lang} -m virtualenv {env} --no-cache-dir'.format(lang=lang, env=VENV_DIR))\n    with virtualenv(VENV_DIR) as venv_local:\n        print(venv_local('python --version', capture=True))\n        venv_local('pip install --upgrade setuptools --no-cache-dir')\n        venv_local('pip install pytest --no-cache-dir')\n        venv_local('pip install wheel --no-cache-dir')\n        venv_local('pip install -r requirements.txt --no-cache-dir')\n        venv_local('pip install pex --no-cache-dir')\n\n\n\ndef install():\n    with virtualenv(VENV_DIR) as venv_local:\n        venv_local('pip install dist/*.tar.gz')\n\n\ndef make():\n    with lcd(path.dirname(__file__)):\n        local('export PYTHONPATH=`pwd` && source .env/bin/activate && python setup.py build_ext --inplace',\n            shell='/bin/bash')\n\ndef sdist():\n    with virtualenv(VENV_DIR) as venv_local:\n        with lcd(path.dirname(__file__)):\n            venv_local('python -m pip install -U setuptools')\n            venv_local('python setup.py sdist')\n\ndef wheel():\n    with virtualenv(VENV_DIR) as venv_local:\n        with lcd(path.dirname(__file__)):\n            venv_local('python setup.py bdist_wheel')\n\n\ndef clean():\n    with lcd(path.dirname(__file__)):\n        local('rm -f dist/*.whl')\n        local('rm -f dist/*.pex')\n        with virtualenv(VENV_DIR) as venv_local:\n            venv_local('python setup.py clean --all')\n\n\ndef test():\n    with virtualenv(VENV_DIR) as venv_local:\n        with lcd(path.dirname(__file__)):\n            venv_local('PYTHONPATH=`pwd` pytest -x tests')\n"""
setup.py,0,"b'#!/usr/bin/env python\nimport shutil\nimport os\n\n# This is maybe not the best place to put this,\n# but we need to tell OSX to build for 10.7.\n# Otherwise, wheels don\'t work. We can\'t use 10.6,\n# it doesn\'t compile.\n# if ""MACOSX_DEPLOYMENT_TARGET"" not in os.environ:\n#    os.environ[""MACOSX_DEPLOYMENT_TARGET""] = ""10.7""\n\nimport contextlib\nimport io\nimport os.path\nimport json\nimport tempfile\nimport shutil\nimport distutils.command.build_ext\nfrom distutils.ccompiler import new_compiler\nimport subprocess\nimport sys\nfrom setuptools import Extension, setup\nimport platform\n\ntry:\n    import cython\n\n    use_cython = True\nexcept ImportError:\n    use_cython = False\n\nMOD_NAMES = [""blis.cy"", ""blis.py""]\n\nprint(""BLIS_COMPILER?"", os.environ.get(""BLIS_COMPILER"", ""None""))\n\n\ndef clean(path):\n    if os.path.exists(os.path.join(PWD, ""build"")):\n        shutil.rmtree(os.path.join(PWD, ""build""))\n    for name in MOD_NAMES:\n        name = name.replace(""."", ""/"")\n        for ext in ["".so"", "".html"", "".cpp"", "".c""]:\n            file_path = os.path.join(path, name + ext)\n            if os.path.exists(file_path):\n                os.unlink(file_path)\n\n\ndef locate_windows_llvm():\n    # first check if the LLVM_HOME env variable is in use\n    if ""LLVM_HOME"" in os.environ:\n        home = os.environ[""LLVM_HOME""]\n        return os.path.join(home, ""bin"", ""clang.exe"")\n    else:\n        # otherwise, search the PATH for clang.exe\n        clang = find_in_path(""clang.exe"", os.environ[""PATH""])\n        if clang is None:\n            clang = r""C:\\Program Files\\LLVM\\bin\\clang.exe""\n        return clang\n\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    # adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = os.path.join(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\n# By subclassing build_extensions we have the actual compiler that will be used\n# which is really known only after finalize_options\n# http://stackoverflow.com/questions/724664/python-distutils-how-to-get-a-compiler-that-is-going-to-be-used\nclass build_ext_options:\n    def build_options(self):\n        if hasattr(self.compiler, ""initialize""):\n            self.compiler.initialize()\n        self.compiler.platform = sys.platform[:6]\n        if self.compiler.compiler_type == ""msvc"":\n            include_dirs = list(self.compiler.include_dirs)\n            library_dirs = list(self.compiler.library_dirs)\n            self.compiler = new_compiler(plat=""nt"", compiler=""unix"")\n            self.compiler.platform = ""nt""\n            self.compiler.compiler_type = ""msvc""\n            self.compiler.compiler = [locate_windows_llvm()]\n            self.compiler.compiler_so = list(self.compiler.compiler)\n            self.compiler.preprocessor = list(self.compiler.compiler)\n            self.compiler.linker = list(self.compiler.compiler) + [""-shared""]\n            self.compiler.linker_so = list(self.compiler.linker)\n            self.compiler.linker_exe = list(self.compiler.linker)\n            self.compiler.archiver = [""llvm-ar""]\n            self.compiler.library_dirs.extend(library_dirs)\n            self.compiler.include_dirs = include_dirs\n\n\nclass ExtensionBuilder(distutils.command.build_ext.build_ext, build_ext_options):\n    def build_extensions(self):\n        build_ext_options.build_options(self)\n        if use_cython:\n            subprocess.check_call([sys.executable, ""bin/cythonize.py""], env=os.environ)\n        compiler = self.get_compiler_name()\n        arch = self.get_arch_name()\n        objects = self.compile_objects(compiler.split(""-"")[0], arch, OBJ_DIR)\n        print(""Compiler"", compiler)\n        if sys.platform == ""msvc"":\n            platform_name = ""windows""\n        elif sys.platform == ""darwin"":\n            platform_name = ""darwin""\n        else:\n            platform_name = ""linux""\n        # Work around max line length in Windows, by making a local directory\n        # for the objects\n        short_dir = ""z""\n        if not os.path.exists(short_dir):\n            os.mkdir(short_dir)\n        short_paths = []\n        for object_path in objects:\n            assert os.path.exists(object_path), object_path\n            dir_name, filename = os.path.split(object_path)\n            new_path = os.path.join(short_dir, filename)\n            shutil.copyfile(object_path, new_path)\n            assert os.path.exists(new_path), new_path\n            short_paths.append(new_path)\n        root = os.path.abspath(os.path.dirname(__file__))\n        for e in self.extensions:\n            e.include_dirs.append(os.path.join(root, ""include""))\n            e.include_dirs.append(\n                os.path.join(INCLUDE, ""%s-%s"" % (platform_name, arch))\n            )\n            e.extra_objects = list(short_paths)\n        distutils.command.build_ext.build_ext.build_extensions(self)\n        shutil.rmtree(short_dir)\n\n    def get_arch_name(self):\n        if ""BLIS_ARCH"" in os.environ:\n            return os.environ[""BLIS_ARCH""]\n        machine = os.uname()[4]\n        if machine == ""aarch64"":\n            # Check if this is a LITTLE core.\n            LITTLE_core = os.system(""test \'Cortex-A53\' = $(lscpu | grep \'Model name\' | awk \'{print $3}\')"")\n            if LITTLE_core == 0:\n                return ""cortexa53""\n            else:\n                # Optimize for big cores.\n                return ""cortexa57""\n        else:\n            return ""x86_64""\n\n    def get_compiler_name(self):\n        if ""BLIS_COMPILER"" in os.environ:\n            return os.environ[""BLIS_COMPILER""]\n        elif os.environ.get(""TRAVIS_OS_NAME"") == ""linux"":\n            return ""gcc-6""\n        name = self.compiler.compiler_type\n        print(name)\n        if name.startswith(""msvc""):\n            return ""msvc""\n        elif name not in (""gcc"", ""clang"", ""icc""):\n            return ""gcc""\n        else:\n            return name\n\n    def compile_objects(self, py_compiler, py_arch, obj_dir):\n        objects = []\n        print(""py_compiler"", py_compiler)\n        if py_compiler == ""msvc"":\n            platform_name = ""windows"" + ""-"" + py_arch\n        elif sys.platform == ""darwin"":\n            platform_name = ""darwin"" + ""-"" + py_arch\n        else:\n            platform_name = ""linux"" + ""-"" + py_arch\n\n        with open(os.path.join(BLIS_DIR, ""make"", ""%s.jsonl"" % platform_name)) as file_:\n            env = {}\n            for line in file_:\n                spec = json.loads(line)\n                if ""environment"" in spec:\n                    env = spec[""environment""]\n                    print(env)\n                    continue\n                _, target_name = os.path.split(spec[""target""])\n                if py_compiler == ""msvc"":\n                    target_name = target_name.replace(""/"", ""\\\\"")\n                    spec[""source""] = spec[""source""].replace(""/"", ""\\\\"")\n                    spec[""include""] = [\n                        inc.replace(""/"", ""\\\\"") for inc in spec[""include""]\n                    ]\n                spec[""include""].append(\n                    ""-I"" + os.path.join(INCLUDE, ""%s"" % platform_name)\n                )\n\n                spec[""target""] = os.path.join(obj_dir, target_name)\n                spec[""source""] = os.path.join(BLIS_DIR, spec[""source""])\n                if ""BLIS_COMPILER"" in os.environ:\n                    spec[""compiler""] = os.environ[""BLIS_COMPILER""]\n                spec[""flags""] = [f for f in spec[""flags""] if ""avx512"" not in f]\n                objects.append(self.build_object(env=env, **spec))\n        return objects\n\n    def build_object(self, compiler, source, target, flags, macros, include, env=None):\n        if os.path.exists(target):\n            return target\n        if not os.path.exists(source):\n            raise IOError(""Cannot find source file: %s"" % source)\n        command = [compiler, ""-c"", source, ""-o"", target]\n        command.extend(flags)\n        command.extend(macros)\n        command.extend(include)\n        print(""[COMMAND]"", "" "".join(command))\n        # TODO: change this to subprocess.run etc. once we drop 2.7\n        subprocess.check_call(command, cwd=BLIS_DIR)\n        return target\n\n\n@contextlib.contextmanager\ndef chdir(new_dir):\n    old_dir = os.getcwd()\n    try:\n        os.chdir(new_dir)\n        sys.path.insert(0, new_dir)\n        yield\n    finally:\n        del sys.path[0]\n        os.chdir(old_dir)\n\n\nPWD = os.path.join(os.path.abspath(os.path.dirname(""."")))\nSRC = os.path.join(PWD, ""blis"")\nBLIS_DIR = os.path.join(SRC, ""_src"")\nINCLUDE = os.path.join(PWD, ""blis"", ""_src"", ""include"")\nCOMPILER = os.environ.get(""BLIS_COMPILER"", ""gcc"")\n\nc_files = []  # get_c_sources(SRC)\n\nif len(sys.argv) > 1 and sys.argv[1] == ""clean"":\n    clean(PWD)\n\nOBJ_DIR = tempfile.mkdtemp()\n\nroot = os.path.abspath(os.path.dirname(__file__))\nwith chdir(root):\n    with open(os.path.join(root, ""blis"", ""about.py"")) as f:\n        about = {}\n        exec(f.read(), about)\n\n    with io.open(os.path.join(root, ""README.md""), encoding=""utf8"") as f:\n        readme = f.read()\n\nsetup(\n    setup_requires=[""numpy>=1.15.0""],\n    install_requires=[""numpy>=1.15.0""],\n    ext_modules=[\n        Extension(\n            ""blis.cy"", [os.path.join(""blis"", ""cy.c"")], extra_compile_args=[""-std=c99""]\n        ),\n        Extension(\n            ""blis.py"", [os.path.join(""blis"", ""py.c"")], extra_compile_args=[""-std=c99""]\n        ),\n    ],\n    cmdclass={""build_ext"": ExtensionBuilder},\n    package_data={\n        """": [""*.json"", ""*.jsonl"", ""*.pyx"", ""*.pxd"", os.path.join(INCLUDE, ""*.h"")]\n        + c_files\n    },\n    name=""blis"",\n    packages=[""blis"", ""blis.tests""],\n    author=about[""__author__""],\n    author_email=about[""__email__""],\n    version=about[""__version__""],\n    url=about[""__uri__""],\n    license=about[""__license__""],\n    description=about[""__summary__""],\n    long_description=readme,\n    long_description_content_type=""text/markdown"",\n    classifiers=[\n        ""Development Status :: 4 - Beta"",\n        ""Environment :: Console"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Information Technology"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: POSIX :: Linux"",\n        ""Operating System :: MacOS :: MacOS X"",\n        ""Programming Language :: Cython"",\n        ""Programming Language :: Python :: 2.7"",\n        ""Programming Language :: Python :: 3.5"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Topic :: Scientific/Engineering"",\n    ],\n)\nshutil.rmtree(OBJ_DIR)\n'"
bin/cythonize.py,0,"b'#!/usr/bin/env python\n"""""" cythonize\n\nCythonize pyx files into C files as needed.\n\nUsage: cythonize [root_dir]\n\nDefault [root_dir] is \'spacy\'.\n\nChecks pyx files to see if they have been changed relative to their\ncorresponding C files.  If they have, then runs cython on these files to\nrecreate the C files.\n\nThe script thinks that the pyx files have changed relative to the C files\nby comparing hashes stored in a database file.\n\nSimple script to invoke Cython (and Tempita) on all .pyx (.pyx.in)\nfiles; while waiting for a proper build system. Uses file hashes to\nfigure out if rebuild is needed.\n\nFor now, this script should be run by developers when changing Cython files\nonly, and the resulting C files checked in, so that end-users (and Python-only\ndevelopers) do not get the Cython/Tempita dependencies.\n\nOriginally written by Dag Sverre Seljebotn, and copied here from:\n\nhttps://raw.github.com/dagss/private-scipy-refactor/cythonize/cythonize.py\n\nNote: this script does not check any of the dependent C libraries; it only\noperates on the Cython .pyx files.\n""""""\n\nfrom __future__ import division, print_function, absolute_import\n\nimport os\nimport re\nimport sys\nimport hashlib\nimport subprocess\n\nHASH_FILE = \'cythonize.dat\'\nDEFAULT_ROOT = \'blis\'\nVENDOR = \'Explosion\'\n\n# WindowsError is not defined on unix systems\ntry:\n    WindowsError\nexcept NameError:\n    WindowsError = None\n\n#\n# Rules\n#\ndef process_pyx(fromfile, tofile):\n    try:\n        from Cython.Compiler.Version import version as cython_version\n        from distutils.version import LooseVersion\n        if LooseVersion(cython_version) < LooseVersion(\'0.19\'):\n            raise Exception(\'Building %s requires Cython >= 0.19\' % VENDOR)\n\n    except ImportError:\n        pass\n\n    flags = [\'--fast-fail\']\n    if tofile.endswith(\'.cpp\'):\n        flags += [\'--cplus\']\n\n    try:\n        try:\n            r = subprocess.call([\'cython\'] + flags + [""-o"", tofile, fromfile])\n            if r != 0:\n                raise Exception(\'Cython failed\')\n        except OSError:\n            # There are ways of installing Cython that don\'t result in a cython\n            # executable on the path, see gh-2397.\n            r = subprocess.call([sys.executable, \'-c\',\n                                 \'import sys; from Cython.Compiler.Main import \'\n                                 \'setuptools_main as main; sys.exit(main())\'] + flags +\n                                 [""-o"", tofile, fromfile])\n            if r != 0:\n                raise Exception(\'Cython failed\')\n    except OSError:\n        raise OSError(\'Cython needs to be installed\')\n\ndef process_tempita_pyx(fromfile, tofile):\n    try:\n        try:\n            from Cython import Tempita as tempita\n        except ImportError:\n            import tempita\n    except ImportError:\n        raise Exception(\'Building %s requires Tempita: \'\n                        \'pip install --user Tempita\' % VENDOR)\n    with open(fromfile, ""r"") as f:\n        tmpl = f.read()\n    pyxcontent = tempita.sub(tmpl)\n    assert fromfile.endswith(\'.pyx.in\')\n    pyxfile = fromfile[:-len(\'.pyx.in\')] + \'.pyx\'\n    with open(pyxfile, ""w"") as f:\n        f.write(pyxcontent)\n    process_pyx(pyxfile, tofile)\n\nrules = {\n    # fromext : function\n    \'.pyx\' : process_pyx,\n    \'.pyx.in\' : process_tempita_pyx\n    }\n#\n# Hash db\n#\ndef load_hashes(filename):\n    # Return { filename : (sha1 of input, sha1 of output) }\n    if os.path.isfile(filename):\n        hashes = {}\n        with open(filename, \'r\') as f:\n            for line in f:\n                filename, inhash, outhash = line.split()\n                hashes[filename] = (inhash, outhash)\n    else:\n        hashes = {}\n    return hashes\n\ndef save_hashes(hash_db, filename):\n    with open(filename, \'w\') as f:\n        for key, value in sorted(hash_db.items()):\n            f.write(""%s %s %s\\n"" % (key, value[0], value[1]))\n\ndef sha1_of_file(filename):\n    h = hashlib.sha1()\n    with open(filename, ""rb"") as f:\n        h.update(f.read())\n    return h.hexdigest()\n\n#\n# Main program\n#\n\ndef normpath(path):\n    path = path.replace(os.sep, \'/\')\n    if path.startswith(\'./\'):\n        path = path[2:]\n    return path\n\ndef get_hash(frompath, topath):\n    from_hash = sha1_of_file(frompath)\n    to_hash = sha1_of_file(topath) if os.path.exists(topath) else None\n    return (from_hash, to_hash)\n\ndef process(path, fromfile, tofile, processor_function, hash_db):\n    fullfrompath = os.path.join(path, fromfile)\n    fulltopath = os.path.join(path, tofile)\n    current_hash = get_hash(fullfrompath, fulltopath)\n    if current_hash == hash_db.get(normpath(fullfrompath), None):\n        print(\'%s has not changed\' % fullfrompath)\n        return\n\n    orig_cwd = os.getcwd()\n    try:\n        os.chdir(path)\n        print(\'Processing %s\' % fullfrompath)\n        processor_function(fromfile, tofile)\n    finally:\n        os.chdir(orig_cwd)\n    # changed target file, recompute hash\n    current_hash = get_hash(fullfrompath, fulltopath)\n    # store hash in db\n    hash_db[normpath(fullfrompath)] = current_hash\n\n\ndef find_process_files(root_dir):\n    hash_db = load_hashes(HASH_FILE)\n    for cur_dir, dirs, files in os.walk(root_dir):\n        for filename in files:\n            in_file = os.path.join(cur_dir, filename + "".in"")\n            if filename.endswith(\'.pyx\') and os.path.isfile(in_file):\n                continue\n            for fromext, function in rules.items():\n                if filename.endswith(fromext):\n                    with open(os.path.join(cur_dir, filename), \'rb\') as f:\n                         data = f.read()\n                         m = re.search(br""^\\s*#\\s*distutils:\\s*language\\s*=\\s*c\\+\\+\\s*$"", data, re.I|re.M)\n                         if m:\n                             toext = "".cpp""\n                         else:\n                             toext = "".c""\n                    fromfile = filename\n                    tofile = filename[:-len(fromext)] + toext\n                    process(cur_dir, fromfile, tofile, function, hash_db)\n                    save_hashes(hash_db, HASH_FILE)\n\ndef main():\n    try:\n        root_dir = sys.argv[1]\n    except IndexError:\n        root_dir = DEFAULT_ROOT\n    find_process_files(root_dir)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
bin/munge_make_log.py,0,"b'import os\nimport sys\nimport json\n\nos_name = sys.argv[1]\narch_name = sys.argv[2]\n\nprint(json.dumps({""environment"": dict(os.environ)}))\nfor line in sys.stdin:\n    if \'flatten-headers.py\' in line:\n        continue\n    line = line.replace(\'include/\' + arch_name, \'include/\' + os_name + \'-\' + arch_name)\n    pieces = line.split()\n    args = {}\n    flags = []\n    macros = []\n    includes = []\n    for i, piece in enumerate(pieces):\n        if i == 0:\n            args[\'compiler\'] = piece\n        elif piece == \'-c\':\n            args[\'source\'] = pieces[i+1]\n        elif piece == \'-o\':\n            args[\'target\'] = pieces[i+1]\n        elif piece.startswith(\'-f\') or piece.startswith(\'-m\') or piece.startswith(\'-O\'):\n            flags.append(piece)\n        elif piece.startswith(\'-std\'):\n            flags.append(piece)\n        elif piece.startswith(\'-D\'):\n            macros.append(piece.replace(\'\\\\\', \'\'))\n        elif piece.startswith(\'-I\'):\n            includes.append(piece)\n    if \'source\' in args:\n        args[\'flags\'] = flags\n        args[\'macros\'] = macros\n        args[\'include\'] = includes\n        print(json.dumps(args))\n'"
blis/__init__.py,0,"b'# Copyright ExplsionAI GmbH, released under BSD.\n\nfrom .cy import init\n\ninit()\n'"
blis/about.py,0,"b'# Copyright ExplosionAI GmbH, released under BSD\n# inspired from:\n\n# https://python-packaging-user-guide.readthedocs.org/en/latest/single_source_version/\n# https://github.com/pypa/warehouse/blob/master/warehouse/__about__.py\n\n__name__ = ""blis""\n__version__ = ""0.4.2""\n__summary__ = (\n    ""The Blis BLAS-like linear algebra library, as a self-contained C-extension.""\n)\n__uri__ = ""https://github.com/explosion/cython-blis""\n__author__ = ""Matthew Honnibal""\n__email__ = ""matt@explosion.ai""\n__license__ = ""BSD""\n__title__ = ""blis""\n__release__ = True\n'"
blis/benchmark.py,0,"b'# Copyright ExplsionAI GmbH, released under BSD.\nimport numpy\nimport numpy.random\nfrom .py import gemm, einsum\nfrom timeit import default_timer as timer\n\nnumpy.random.seed(0)\n\n\ndef create_data(nO, nI, batch_size):\n    X = numpy.zeros((batch_size, nI), dtype=""f"")\n    X += numpy.random.uniform(-1.0, 1.0, X.shape)\n    W = numpy.zeros((nO, nI), dtype=""f"")\n    W += numpy.random.uniform(-1.0, 1.0, W.shape)\n    return X, W\n\n\ndef get_numpy_blas():\n    blas_libs = numpy.__config__.blas_opt_info[""libraries""]\n    return blas_libs[0]\n\n\ndef numpy_gemm(X, W, n=1000):\n    nO, nI = W.shape\n    batch_size = X.shape[0]\n    total = 0.0\n    y = numpy.zeros((batch_size, nO), dtype=""f"")\n    for i in range(n):\n        numpy.dot(X, W, out=y)\n        total += y.sum()\n        y.fill(0)\n    print(""Total:"", total)\n\n\ndef blis_gemm(X, W, n=1000):\n    nO, nI = W.shape\n    batch_size = X.shape[0]\n    total = 0.0\n    y = numpy.zeros((batch_size, nO), dtype=""f"")\n    for i in range(n):\n        gemm(X, W, out=y)\n        total += y.sum()\n        y.fill(0.0)\n    print(""Total:"", total)\n\n\ndef numpy_einsum(X, W, n=1000):\n    nO, nI = W.shape\n    batch_size = X.shape[0]\n    total = 0.0\n    y = numpy.zeros((nO, batch_size), dtype=""f"")\n    for i in range(n):\n        numpy.einsum(""ab,cb->ca"", X, W, out=y)\n        total += y.sum()\n        y.fill(0.0)\n    print(""Total:"", total)\n\n\ndef blis_einsum(X, W, n=1000):\n    nO, nI = W.shape\n    batch_size = X.shape[0]\n    total = 0.0\n    y = numpy.zeros((nO, batch_size), dtype=""f"")\n    for i in range(n):\n        einsum(""ab,cb->ca"", X, W, out=y)\n        total += y.sum()\n        y.fill(0.0)\n    print(""Total:"", total)\n\n\ndef main(nI=128 * 3, nO=128 * 3, batch_size=2000):\n    print(\n        ""Setting up data for gemm. 1000 iters,  ""\n        ""nO={nO} nI={nI} batch_size={batch_size}"".format(**locals())\n    )\n    numpy_blas = get_numpy_blas()\n    X1, W1 = create_data(nI, nO, batch_size)\n    X2 = X1.copy()\n    W2 = W1.copy()\n    print(""Blis gemm..."")\n    start = timer()\n    blis_gemm(X2, W2, n=1000)\n    end = timer()\n    blis_time = end - start\n    print(""%.2f seconds"" % blis_time)\n    print(""Numpy (%s) gemm..."" % numpy_blas)\n    start = timer()\n    numpy_gemm(X1, W1)\n    end = timer()\n    numpy_time = end - start\n    print(""%.2f seconds"" % numpy_time)\n    print(""Blis einsum ab,cb->ca"")\n    start = timer()\n    blis_einsum(X2, W2, n=1000)\n    end = timer()\n    blis_time = end - start\n    print(""%.2f seconds"" % blis_time)\n    print(""Numpy (%s) einsum ab,cb->ca"" % numpy_blas)\n    start = timer()\n    numpy_einsum(X2, W2)\n    end = timer()\n    numpy_time = end - start\n    print(""%.2f seconds"" % numpy_time)\n\n\nif __name__:\n    main()\n'"
tests/blis_tests_common.py,8,"b""from __future__ import print_function\n\nimport numpy as np\nnp.random.seed(0)\nfrom numpy.testing import assert_allclose\n\nfrom hypothesis import given, assume\nfrom hypothesis.strategies import tuples, lists, integers, floats\nfrom hypothesis.extra.numpy import arrays\n\n\ndef lengths(lo=1, hi=10):\n    return integers(min_value=lo, max_value=hi)\n\n\ndef shapes(min_rows=1, max_rows=100, min_cols=1, max_cols=100):\n    return tuples(lengths(lo=min_rows, hi=max_rows), lengths(lo=min_cols, hi=max_cols))\n\n\ndef ndarrays_of_shape(shape, lo=-1000.0, hi=1000.0, dtype='float64'):\n    return arrays(dtype, shape=shape, elements=floats(min_value=lo, max_value=hi))\n    \n\ndef ndarrays(min_len=0, max_len=10, min_val=-10000000.0, max_val=1000000.0, dtype='float64'):\n    return lengths(lo=min_len, hi=max_len).flatmap(\n        lambda n: ndarrays_of_shape(n, lo=min_val, hi=max_val, dtype=dtype))\n\n\ndef matrices(min_rows=1, max_rows=10, min_cols=1, max_cols=10,\n             min_value=-10000000.0, max_value=1000000.0, dtype='float64'):\n    return shapes(min_rows=min_rows, max_rows=max_rows,\n                  min_cols=min_cols, max_cols=max_cols).flatmap(\n                        lambda mn: ndarrays_of_shape(mn, lo=min_value, hi=max_value,\n                                                     dtype=dtype))\n\n\ndef positive_ndarrays(min_len=0, max_len=10, max_val=100000.0, dtype='float64'):\n    return ndarrays(min_len=min_len, max_len=max_len, min_val=0, max_val=max_val,\n                    dtype=dtype)\n\n\ndef negative_ndarrays(min_len=0, max_len=10, min_val=-100000.0, dtype='float64'):\n    return ndarrays(min_len=min_len, max_len=max_len, min_val=min_val, max_val=-1e-10,\n                    dtype=dtype)\n\n\ndef parse_layer(layer_data):\n    # Get the first row, excluding the first column\n    x = layer_data[0,1:]\n    # Get the first column, excluding the first row\n    # .ascontiguousarray is support important here!!!!\n    b = np.ascontiguousarray(layer_data[1:,0], dtype='float64')\n    # Slice out the row and the column used for the X and the bias\n    W = layer_data[1:,1:]\n    assert x.ndim == 1\n    assert b.ndim == 1\n    assert b.shape[0] == W.shape[0]\n    assert x.shape[0] == W.shape[1]\n    assume(not np.isnan(W.sum()))\n    assume(not np.isnan(x.sum()))\n    assume(not np.isnan(b.sum()))\n    assume(not any(np.isinf(val) for val in W.flatten()))\n    assume(not any(np.isinf(val) for val in x))\n    assume(not any(np.isinf(val) for val in b))\n    return x, b, W\n \n\ndef split_row(layer_data):\n    return (layer_data[0,:], layer_data[:,:])\n\n\n"""
tests/test_dotv.py,0,"b""from __future__ import division\nfrom hypothesis import given, assume\nfrom math import sqrt, floor\n\nfrom blis_tests_common import *\nfrom blis.py import dotv\nfrom blis.cy import NO_CONJUGATE, CONJUGATE\n\n\n@given(\n    ndarrays(min_len=10, max_len=100,\n             min_val=-100.0, max_val=100.0, dtype='float64'),\n    ndarrays(min_len=10, max_len=100,\n             min_val=-100.0, max_val=100.0, dtype='float64'),\n)\ndef test_memoryview_double_noconj(A, B):\n    if len(A) < len(B):\n        B = B[:len(A)]\n    else:\n        A = A[:len(B)]\n    assume(A is not None)\n    assume(B is not None)\n    numpy_result = A.dot(B)\n    result = dotv(A, B)\n    assert_allclose([numpy_result], result, atol=1e-3, rtol=1e-3)\n\n\n@given(\n    ndarrays(min_len=10, max_len=100,\n             min_val=-100.0, max_val=100.0, dtype='float32'),\n    ndarrays(min_len=10, max_len=100,\n             min_val=-100.0, max_val=100.0, dtype='float32'),\n)\ndef test_memoryview_float_noconj(A, B):\n    if len(A) < len(B):\n        B = B[:len(A)]\n    else:\n        A = A[:len(B)]\n    assume(A is not None)\n    assume(B is not None)\n    numpy_result = A.dot(B)\n    result = dotv(A, B)\n    assert_allclose([numpy_result], result, atol=1e-3, rtol=1e-3)\n"""
tests/test_gemm.py,4,"b""from __future__ import division\nfrom hypothesis import given, assume\nfrom math import sqrt, floor\n\nfrom blis_tests_common import *\nfrom blis.py import gemm\n\n\ndef _stretch_matrix(data, m, n):\n    orig_len = len(data)\n    orig_m = m\n    orig_n = n\n    ratio = sqrt(len(data) / (m * n))\n    m = int(floor(m * ratio))\n    n = int(floor(n * ratio))\n    data = np.ascontiguousarray(data[:m*n], dtype=data.dtype)\n    return data.reshape((m, n)), m, n\n\ndef _reshape_for_gemm(A, B, a_rows, a_cols, out_cols, dtype, trans_a=False, trans_b=False):\n    A, a_rows, a_cols = _stretch_matrix(A, a_rows, a_cols)\n    if len(B) < a_cols or a_cols < 1:\n        return (None, None, None)\n    b_cols = int(floor(len(B) / a_cols))\n    B = np.ascontiguousarray(B.flatten()[:a_cols*b_cols], dtype=dtype)\n    B = B.reshape((a_cols, b_cols))\n    out_cols = B.shape[1]\n    C = np.zeros(shape=(A.shape[0], B.shape[1]), dtype=dtype)\n    if trans_a:\n        A = np.ascontiguousarray(A.T, dtype=dtype)\n    return A, B, C\n\n\n@given(\n    ndarrays(min_len=10, max_len=100,\n             min_val=-100.0, max_val=100.0, dtype='float64'),\n    ndarrays(min_len=10, max_len=100,\n             min_val=-100.0, max_val=100.0, dtype='float64'),\n    integers(min_value=2, max_value=1000),\n    integers(min_value=2, max_value=1000),\n    integers(min_value=2, max_value=1000))\ndef test_memoryview_double_notrans(A, B, a_rows, a_cols, out_cols):\n    A, B, C = _reshape_for_gemm(A, B, a_rows, a_cols, out_cols, 'float64')\n    assume(A is not None)\n    assume(B is not None)\n    assume(C is not None)\n    assume(A.size >= 1)\n    assume(B.size >= 1)\n    assume(C.size >= 1)\n    gemm(A, B, out=C)\n    numpy_result = A.dot(B)\n    assert_allclose(numpy_result, C, atol=1e-3, rtol=1e-3)\n\n\n@given(\n    ndarrays(min_len=10, max_len=100,\n             min_val=-100.0, max_val=100.0, dtype='float32'),\n    ndarrays(min_len=10, max_len=100,\n             min_val=-100.0, max_val=100.0, dtype='float32'),\n    integers(min_value=2, max_value=1000),\n    integers(min_value=2, max_value=1000),\n    integers(min_value=2, max_value=1000))\ndef test_memoryview_float_notrans(A, B, a_rows, a_cols, out_cols):\n    A, B, C = _reshape_for_gemm(A, B, a_rows, a_cols, out_cols, dtype='float32')\n    assume(A is not None)\n    assume(B is not None)\n    assume(C is not None)\n    assume(A.size >= 1)\n    assume(B.size >= 1)\n    assume(C.size >= 1)\n    gemm(A, B, out=C)\n    numpy_result = A.dot(B)\n    assert_allclose(numpy_result, C, atol=1e-3, rtol=1e-3)\n"""
blis/tests/__init__.py,0,b''
blis/tests/common.py,8,"b'# Copyright ExplsionAI GmbH, released under BSD.\nfrom __future__ import print_function\n\nimport numpy as np\n\nnp.random.seed(0)\nfrom numpy.testing import assert_allclose\n\nfrom hypothesis import given, assume\nfrom hypothesis.strategies import tuples, lists, integers, floats\nfrom hypothesis.extra.numpy import arrays\n\n\ndef lengths(lo=1, hi=10):\n    return integers(min_value=lo, max_value=hi)\n\n\ndef shapes(min_rows=1, max_rows=100, min_cols=1, max_cols=100):\n    return tuples(lengths(lo=min_rows, hi=max_rows), lengths(lo=min_cols, hi=max_cols))\n\n\ndef ndarrays_of_shape(shape, lo=-1000.0, hi=1000.0, dtype=""float64""):\n    return arrays(dtype, shape=shape, elements=floats(min_value=lo, max_value=hi))\n\n\ndef ndarrays(\n    min_len=0, max_len=10, min_val=-10000000.0, max_val=1000000.0, dtype=""float64""\n):\n    return lengths(lo=min_len, hi=max_len).flatmap(\n        lambda n: ndarrays_of_shape(n, lo=min_val, hi=max_val, dtype=dtype)\n    )\n\n\ndef matrices(\n    min_rows=1,\n    max_rows=10,\n    min_cols=1,\n    max_cols=10,\n    min_value=-10000000.0,\n    max_value=1000000.0,\n    dtype=""float64"",\n):\n    return shapes(\n        min_rows=min_rows, max_rows=max_rows, min_cols=min_cols, max_cols=max_cols\n    ).flatmap(lambda mn: ndarrays_of_shape(mn, lo=min_value, hi=max_value, dtype=dtype))\n\n\ndef positive_ndarrays(min_len=0, max_len=10, max_val=100000.0, dtype=""float64""):\n    return ndarrays(\n        min_len=min_len, max_len=max_len, min_val=0, max_val=max_val, dtype=dtype\n    )\n\n\ndef negative_ndarrays(min_len=0, max_len=10, min_val=-100000.0, dtype=""float64""):\n    return ndarrays(\n        min_len=min_len, max_len=max_len, min_val=min_val, max_val=-1e-10, dtype=dtype\n    )\n\n\ndef parse_layer(layer_data):\n    # Get the first row, excluding the first column\n    x = layer_data[0, 1:]\n    # Get the first column, excluding the first row\n    # .ascontiguousarray is support important here!!!!\n    b = np.ascontiguousarray(layer_data[1:, 0], dtype=""float64"")\n    # Slice out the row and the column used for the X and the bias\n    W = layer_data[1:, 1:]\n    assert x.ndim == 1\n    assert b.ndim == 1\n    assert b.shape[0] == W.shape[0]\n    assert x.shape[0] == W.shape[1]\n    assume(not np.isnan(W.sum()))\n    assume(not np.isnan(x.sum()))\n    assume(not np.isnan(b.sum()))\n    assume(not any(np.isinf(val) for val in W.flatten()))\n    assume(not any(np.isinf(val) for val in x))\n    assume(not any(np.isinf(val) for val in b))\n    return x, b, W\n\n\ndef split_row(layer_data):\n    return (layer_data[0, :], layer_data[:, :])\n'"
blis/tests/test_dotv.py,0,"b'# Copyright ExplsionAI GmbH, released under BSD.\nfrom __future__ import division\nfrom hypothesis import given, assume\nfrom math import sqrt, floor\n\nfrom blis.tests.common import *\nfrom blis.py import dotv\nfrom blis.cy import NO_CONJUGATE, CONJUGATE\n\n\n@given(\n    ndarrays(min_len=10, max_len=100, min_val=-100.0, max_val=100.0, dtype=""float64""),\n    ndarrays(min_len=10, max_len=100, min_val=-100.0, max_val=100.0, dtype=""float64""),\n)\ndef test_memoryview_double_noconj(A, B):\n    if len(A) < len(B):\n        B = B[: len(A)]\n    else:\n        A = A[: len(B)]\n    assume(A is not None)\n    assume(B is not None)\n    numpy_result = A.dot(B)\n    result = dotv(A, B)\n    assert_allclose([numpy_result], result, atol=1e-4, rtol=1e-4)\n\n\n@given(\n    ndarrays(min_len=10, max_len=100, min_val=-100.0, max_val=100.0, dtype=""float32""),\n    ndarrays(min_len=10, max_len=100, min_val=-100.0, max_val=100.0, dtype=""float32""),\n)\ndef test_memoryview_float_noconj(A, B):\n    if len(A) < len(B):\n        B = B[: len(A)]\n    else:\n        A = A[: len(B)]\n    assume(A is not None)\n    assume(B is not None)\n    numpy_result = A.dot(B)\n    result = dotv(A, B)\n    assert_allclose([numpy_result], result, atol=1e-4, rtol=1e-4)\n'"
blis/tests/test_gemm.py,4,"b'# Copyright ExplsionAI GmbH, released under BSD.\nfrom __future__ import division\n\nfrom hypothesis import given, assume\nfrom math import sqrt, floor\n\nfrom blis.tests.common import *\nfrom blis.py import gemm\n\n\ndef _stretch_matrix(data, m, n):\n    orig_len = len(data)\n    orig_m = m\n    orig_n = n\n    ratio = sqrt(len(data) / (m * n))\n    m = int(floor(m * ratio))\n    n = int(floor(n * ratio))\n    data = np.ascontiguousarray(data[: m * n], dtype=data.dtype)\n    return data.reshape((m, n)), m, n\n\n\ndef _reshape_for_gemm(\n    A, B, a_rows, a_cols, out_cols, dtype, trans_a=False, trans_b=False\n):\n    A, a_rows, a_cols = _stretch_matrix(A, a_rows, a_cols)\n    if len(B) < a_cols or a_cols < 1:\n        return (None, None, None)\n    b_cols = int(floor(len(B) / a_cols))\n    B = np.ascontiguousarray(B.flatten()[: a_cols * b_cols], dtype=dtype)\n    B = B.reshape((a_cols, b_cols))\n    out_cols = B.shape[1]\n    C = np.zeros(shape=(A.shape[0], B.shape[1]), dtype=dtype)\n    if trans_a:\n        A = np.ascontiguousarray(A.T, dtype=dtype)\n    return A, B, C\n\n\n@given(\n    ndarrays(min_len=10, max_len=100, min_val=-100.0, max_val=100.0, dtype=""float64""),\n    ndarrays(min_len=10, max_len=100, min_val=-100.0, max_val=100.0, dtype=""float64""),\n    integers(min_value=2, max_value=1000),\n    integers(min_value=2, max_value=1000),\n    integers(min_value=2, max_value=1000),\n)\ndef test_memoryview_double_notrans(A, B, a_rows, a_cols, out_cols):\n    A, B, C = _reshape_for_gemm(A, B, a_rows, a_cols, out_cols, ""float64"")\n    assume(A is not None)\n    assume(B is not None)\n    assume(C is not None)\n    assume(A.size >= 1)\n    assume(B.size >= 1)\n    assume(C.size >= 1)\n    gemm(A, B, out=C)\n    numpy_result = A.dot(B)\n    assert_allclose(numpy_result, C, atol=1e-4, rtol=1e-4)\n\n\n@given(\n    ndarrays(min_len=10, max_len=100, min_val=-100.0, max_val=100.0, dtype=""float32""),\n    ndarrays(min_len=10, max_len=100, min_val=-100.0, max_val=100.0, dtype=""float32""),\n    integers(min_value=2, max_value=1000),\n    integers(min_value=2, max_value=1000),\n    integers(min_value=2, max_value=1000),\n)\ndef test_memoryview_float_notrans(A, B, a_rows, a_cols, out_cols):\n    A, B, C = _reshape_for_gemm(A, B, a_rows, a_cols, out_cols, dtype=""float32"")\n    assume(A is not None)\n    assume(B is not None)\n    assume(C is not None)\n    assume(A.size >= 1)\n    assume(B.size >= 1)\n    assume(C.size >= 1)\n    gemm(A, B, out=C)\n    numpy_result = A.dot(B)\n    assert_allclose(numpy_result, C, atol=1e-3, rtol=1e-3)\n'"
