file_path,api_count,code
ch02/Ass_rule.py,0,"b'""""""\r\n\xe5\x85\xb3\xe8\x81\x94\xe8\xa7\x84\xe5\x88\x99/\xe5\xba\x8f\xe5\x88\x97\xe8\xa7\x84\xe5\x88\x99\r\n""""""\r\nfrom itertools import combinations\r\n\r\n\r\ndef comb(lst):\r\n    """"""\r\n    \xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe5\x88\x97\xe8\xa1\xa8\xe9\x87\x8c\xe6\x89\x80\xe6\x9c\x89\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84\xe7\xbb\x84\xe5\x90\x88\r\n    [""a"", ""b"", ""c""]\r\n    >>> [(""a"",), (""b"",), (""c"",), (""a"",""b""), (""a"",""c""), (""b"",""c""), (""a"",""b"",""c"")]\r\n    """"""\r\n    ret = []\r\n    for i in range(1, len(lst) + 1):\r\n        ret += list(combinations(lst, i))\r\n    return ret\r\n\r\n\r\nclass AprLayer(object):\r\n    """"""\xe5\xad\x98\xe6\x94\xbe\xe9\xa1\xb9\xe7\x9b\xae\xe6\x95\xb0\xe9\x87\x8f\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe9\xa1\xb9\xe9\x9b\x86""""""\r\n    def __init__(self):\r\n        self.d = dict()\r\n\r\n\r\nclass AprNode(object):\r\n    """"""\r\n    \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe9\xa1\xb9\xe9\x9b\x86\r\n    node = (""a"", ) \xe6\x88\x96 (""b"",) \xe6\x88\x96 (""a"", ""b"")\r\n    """"""\r\n    def __init__(self, node):\r\n        self.s = set(node)\r\n        self.size = len(self.s)\r\n        self.lnk_nodes = dict()\r\n        self.num = 0\r\n\r\n    def __hash__(self):\r\n        return hash(""__"".join(sorted([str(itm) for itm in list(self.s)])))\r\n\r\n    def __eq__(self, other):\r\n        if ""__"".join(sorted([str(itm) for itm in list(self.s)])) == ""__"".join(\r\n                sorted([str(itm) for itm in list(other.s)])):\r\n            return True\r\n        return False\r\n\r\n    def isSubnode(self, node):\r\n        return self.s.issubset(node.s)\r\n\r\n    def incNum(self, num=1):\r\n        self.num += num\r\n\r\n    def addLnk(self, node):\r\n        self.lnk_nodes[node] = node.s\r\n\r\n\r\nclass AprBlk(object):\r\n    """"""\xe4\xbd\xbfAprLayer\xe5\x92\x8cAprNode\xe4\xb8\xa4\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84\xe7\xbb\x93\xe5\x90\x88""""""\r\n    def __init__(self, data):\r\n        cnt = 0  # \xe8\xae\xa1\xe6\x95\xb0\xe5\x99\xa8\r\n        self.apr_layers = dict()\r\n        self.data_num = len(data)\r\n        for datum in data:\r\n            cnt += 1\r\n            datum = comb(datum)\r\n            # datum=[(""a"",), (""b"",), (""a"",""b"")]\r\n            nodes = [AprNode(da) for da in datum]\r\n            for node in nodes:\r\n                # \xe6\xa0\xb9\xe6\x8d\xae\xe9\xa1\xb9\xe9\x9b\x86\xe7\x9a\x84\xe6\x95\xb0\xe7\x9b\xae\r\n                if node.size not in self.apr_layers:\r\n                    # {1: {(\'a\',): AprNode((\'a\',)), (\'b\',): AprNode((\'b\',))}, 2:{}, 3:{}}\r\n                    self.apr_layers[node.size] = AprLayer()\r\n                if node not in self.apr_layers[node.size].d:\r\n                    self.apr_layers[node.size].d[node] = node\r\n                # \xe8\xb0\x83\xe7\x94\xa8\xe6\x95\xb0\xe9\x87\x8f\xe5\x8a\xa01\r\n                self.apr_layers[node.size].d[node].incNum()\r\n            for node in nodes:\r\n                if node.size == 1:\r\n                    continue\r\n                for sn in node.s:\r\n                    # \xe9\xab\x98\xe9\x98\xb6\xe9\xa1\xb9\xe9\x9b\x86\xe5\x87\x8f\xe5\x8e\xbb\xe4\xb8\x80\xe9\x98\xb6\xe9\xa1\xb9\xe9\x9b\x86\r\n                    # sn -> \'\xe6\xaf\x9b\xe5\xb7\xbe\', set([sn]) -> {\'\xe6\xaf\x9b\xe5\xb7\xbe\'}\r\n                    sub_n = AprNode(node.s - set([sn]))\r\n                    # \xe5\x9c\xa8\xe4\xbd\x8e\xe9\x98\xb6\xe9\xa1\xb9\xe9\x9b\x86\xe4\xb8\x8a\xe5\xbb\xba\xe7\xab\x8b\xe5\x92\x8c\xe9\xab\x98\xe9\x98\xb6\xe9\xa1\xb9\xe9\x9b\x86\xe7\x9a\x84\xe8\x81\x94\xe7\xb3\xbb\r\n                    self.apr_layers[node.size - 1].d[sub_n].addLnk(node)\r\n\r\n    def getFreqItems(self, thd=1, hd=1):\r\n        # thd=1 \xe4\xb8\xba\xe9\x98\x88\xe5\x80\xbc\r\n        freq_items = []\r\n        for layer in self.apr_layers:\r\n            for node in self.apr_layers[layer].d:\r\n                if self.apr_layers[layer].d[node].num < thd:\r\n                    continue\r\n                freq_items.append((self.apr_layers[layer].d[node].s, self.apr_layers[layer].d[node].num))\r\n        # \xe6\xa0\xb9\xe6\x8d\xaenum\xe4\xbb\x8e\xe9\xab\x98\xe5\x88\xb0\xe4\xbd\x8e\xe6\x8e\x92\xe5\xba\x8f\r\n        freq_items.sort(key=lambda x: x[1], reverse=True)\r\n        return freq_items[:hd]\r\n\r\n    def getConf(self, low=True, h_thd=10, l_thd=1, hd=1):\r\n        # h_thd \xe9\xab\x98\xe9\x98\x88\xe5\x80\xbc\r\n        confidence = []\r\n        for layer in self.apr_layers:\r\n            for node in self.apr_layers[layer].d:\r\n                if self.apr_layers[layer].d[node].num < h_thd:\r\n                    continue\r\n                for lnk_node in node.lnk_nodes:\r\n                    if lnk_node.num < l_thd:\r\n                        continue\r\n                    # \xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6=\xe4\xbd\x8e\xe9\x98\xb6\xe9\xa2\x91\xe7\xb9\x81\xe9\xa1\xb9\xe9\x9b\x86\xe6\x89\x80\xe8\xbf\x9e\xe6\x8e\xa5\xe7\x9a\x84\xe9\xab\x98\xe9\x98\xb6\xe9\xa2\x91\xe7\xb9\x81\xe9\xa1\xb9\xe9\x9b\x86\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f/\xe4\xbd\x8e\xe9\x98\xb6\xe9\xa2\x91\xe7\xb9\x81\xe9\xa1\xb9\xe9\x9b\x86\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\r\n                    conf = float(lnk_node.num) / float(node.num)\r\n                    confidence.append([node.s, node.num, lnk_node.s, lnk_node.num, conf])\r\n        # \xe6\xa0\xb9\xe6\x8d\xae\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\xe6\x8e\x92\xe5\xba\x8f\r\n        confidence.sort(key=lambda x: x[4])\r\n        if low:\r\n            # \xe8\xbf\x94\xe5\x9b\x9e\xe4\xbd\x8e\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\r\n            return confidence[:hd]\r\n        else:\r\n            # \xe8\xbf\x94\xe5\x9b\x9e\xe9\xab\x98\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\r\n            return confidence[-hd::-1]\r\n\r\n\r\nclass AssctAnaClass():\r\n    """"""\xe5\x85\xb3\xe8\x81\x94\xe8\xa7\x84\xe5\x88\x99""""""\r\n    def __init__(self):\r\n        self.apr_blk = None\r\n\r\n    def fit(self, data):\r\n        # \xe6\x8b\x9f\xe5\x90\x88\xe6\x95\xb0\xe6\x8d\xae\r\n        self.apr_blk = AprBlk(data)\r\n        return self\r\n\r\n    def get_freq(self, thd=1, hd=1):\r\n        # \xe5\x8f\x96\xe5\x87\xba\xe9\xa2\x91\xe7\xb9\x81\xe9\xa1\xb9\xe9\x9b\x86\r\n        return self.apr_blk.getFreqItems(thd=thd, hd=hd)\r\n\r\n    def get_conf_high(self, thd, h_thd=10):\r\n        # \xe5\x8f\x96\xe5\x87\xba\xe9\xab\x98\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\xe9\xa1\xb9\xe9\x9b\x86\xe7\xbb\x84\xe5\x90\x88\r\n        return self.apr_blk.getConf(low=False, h_thd=h_thd, l_thd=thd)\r\n\r\n    def get_conf_low(self, thd, hd, l_thd=1):\r\n        # \xe5\x8f\x96\xe5\x87\xba\xe4\xbd\x8e\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\xe9\xa1\xb9\xe9\x9b\x86\xe7\xbb\x84\xe5\x90\x88\r\n        return self.apr_blk.getConf(h_thd=thd, l_thd=l_thd, hd=hd)\r\n\r\n\r\ndef main():\r\n    data = [\r\n        [""\xe7\x89\x9b\xe5\xa5\xb6"", ""\xe5\x95\xa4\xe9\x85\x92"", ""\xe5\xb0\xbf\xe5\xb8\x83""],\r\n        [""\xe7\x89\x9b\xe5\xa5\xb6"", ""\xe5\x95\xa4\xe9\x85\x92"", ""\xe5\x92\x96\xe5\x95\xa1"", ""\xe5\xb0\xbf\xe5\xb8\x83""],\r\n        [""\xe9\xa6\x99\xe8\x82\xa0"", ""\xe7\x89\x9b\xe5\xa5\xb6"", ""\xe9\xa5\xbc\xe5\xb9\xb2""],\r\n        [""\xe5\xb0\xbf\xe5\xb8\x83"", ""\xe6\x9e\x9c\xe6\xb1\x81"", ""\xe5\x95\xa4\xe9\x85\x92""],\r\n        [""\xe9\x92\x89\xe5\xad\x90"", ""\xe5\x95\xa4\xe9\x85\x92""],\r\n        [""\xe5\xb0\xbf\xe5\xb8\x83"", ""\xe6\xaf\x9b\xe5\xb7\xbe"", ""\xe9\xa6\x99\xe8\x82\xa0""],\r\n        [""\xe5\x95\xa4\xe9\x85\x92"", ""\xe6\xaf\x9b\xe5\xb7\xbe"", ""\xe5\xb0\xbf\xe5\xb8\x83"", ""\xe9\xa5\xbc\xe5\xb9\xb2""]\r\n    ]\r\n    print(""Freq"", AssctAnaClass().fit(data).get_freq(thd=3, hd=10))\r\n    print(""Conf"", AssctAnaClass().fit(data).get_conf_high(thd=3, h_thd=4))\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
ch02/K-means.py,2,"b'import numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.datasets import make_circles, make_blobs, make_moons\r\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\r\n\r\nn_samples = 1000\r\ncircles = make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\r\nmoons = make_moons(n_samples=n_samples, noise=0.05)\r\nblobs = make_blobs(n_samples=n_samples, random_state=8, center_box=(-1, 1), cluster_std=0.1)\r\nrandom_data = np.random.rand(n_samples, 2), None  # None\xe6\x8c\x87\xe6\xa0\x87\xe6\xb3\xa8\r\ncolors = ""bgrcmyk""\r\ndata = [circles, moons, blobs, random_data]  # \xe5\x9b\x9b\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\r\nmodels = [(""None"", None),  # \xe4\xb8\x8d\xe6\xb7\xbb\xe5\x8a\xa0\xe4\xbb\xbb\xe4\xbd\x95\xe6\xa8\xa1\xe5\x9e\x8b\r\n          (""KMeans"", KMeans(n_clusters=3)),]  # KMeans \xe5\x88\x86\xe6\x88\x903\xe7\xb1\xbb\r\n          # (""DBscan"", DBSCAN(min_samples=3, eps=0.2)),  # \xe5\x88\x86\xe6\x88\x903\xe7\xb1\xbb\xef\xbc\x8c E\xe9\x82\xbb\xe5\x9f\x9f\r\n          # (""Agglomerative"", AgglomerativeClustering(n_clusters=3, linkage=""ward""))]  # \xe6\x8c\x87\xe5\xae\x9award\xe6\x96\xb9\xe6\xb3\x95\r\nf = plt.figure()\r\nfor inx, clt in enumerate(models):\r\n    clt_name, clt_entry = clt\r\n    for i, dataset in enumerate(data):\r\n        X, Y = dataset\r\n        if not clt_entry:\r\n            clt_res = [0 for i in range(len(X))]\r\n        else:\r\n            clt_entry.fit(X)\r\n            clt_res = clt_entry.labels_.astype(np.int)\r\n        f.add_subplot(len(models), len(data), inx*len(data)+i+1)\r\n        plt.title(clt_name)\r\n        [plt.scatter(X[p, 0], X[p, 1], color=colors[clt_res[p]]) for p in range(len(X))]\r\nplt.savefig(""Kmeans.png"")\r\nplt.show()\r\n'"
Spider/ch_Code/sliding_code.py,0,"b'# -*-coding:utf-8 -*-\nimport random\nimport time\n\nfrom selenium.webdriver import ActionChains\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.by import By\nfrom urllib.request import urlretrieve\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport PIL.Image as image\nimport re\n\n\nclass Crack():\n    def __init__(self,keyword):\n        self.url = \'http://bj.gsxt.gov.cn/sydq/loginSydqAction!sydq.dhtml\'\n        self.browser = webdriver.Chrome(\'D:\\\\chromedriver.exe\')\n        self.wait = WebDriverWait(self.browser, 100)\n        self.keyword = keyword\n        self.BORDER = 6\n\n    def open(self):\n        """"""\n        \xe6\x89\x93\xe5\xbc\x80\xe6\xb5\x8f\xe8\xa7\x88\xe5\x99\xa8,\xe5\xb9\xb6\xe8\xbe\x93\xe5\x85\xa5\xe6\x9f\xa5\xe8\xaf\xa2\xe5\x86\x85\xe5\xae\xb9\n        """"""\n        self.browser.get(self.url)\n        keyword = self.wait.until(EC.presence_of_element_located((By.ID, \'keyword_qycx\')))\n        button = self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, \'btn\')))\n        keyword.send_keys(self.keyword)\n        button.click()\n\n    def get_images(self, bg_filename=\'bg.jpg\', fullbg_filename=\'fullbg.jpg\'):\n        """"""\n        \xe8\x8e\xb7\xe5\x8f\x96\xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\xe5\x9b\xbe\xe7\x89\x87\n        :return: \xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84location\xe4\xbf\xa1\xe6\x81\xaf\n        """"""\n        bg = []\n        fullgb = []\n        while bg == [] and fullgb == []:\n            bf = BeautifulSoup(self.browser.page_source, \'lxml\')\n            bg = bf.find_all(\'div\', class_ = \'gt_cut_bg_slice\')\n            fullgb = bf.find_all(\'div\', class_ = \'gt_cut_fullbg_slice\')\n        bg_url = re.findall(\'url\\(\\""(.*)\\""\\);\', bg[0].get(\'style\'))[0].replace(\'webp\', \'jpg\')\n        fullgb_url = re.findall(\'url\\(\\""(.*)\\""\\);\', fullgb[0].get(\'style\'))[0].replace(\'webp\', \'jpg\')\n        bg_location_list = []\n        fullbg_location_list = []\n        for each_bg in bg:\n            location = dict()\n            location[\'x\'] = int(re.findall(\'background-position: (.*)px (.*)px;\',each_bg.get(\'style\'))[0][0])\n            location[\'y\'] = int(re.findall(\'background-position: (.*)px (.*)px;\',each_bg.get(\'style\'))[0][1])\n            bg_location_list.append(location)\n        for each_fullgb in fullgb:\n            location = dict()\n            location[\'x\'] = int(re.findall(\'background-position: (.*)px (.*)px;\',each_fullgb.get(\'style\'))[0][0])\n            location[\'y\'] = int(re.findall(\'background-position: (.*)px (.*)px;\',each_fullgb.get(\'style\'))[0][1])\n            fullbg_location_list.append(location)\n\n        urlretrieve(url=bg_url, filename=bg_filename)\n        print(\'\xe7\xbc\xba\xe5\x8f\xa3\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\x8b\xe8\xbd\xbd\xe5\xae\x8c\xe6\x88\x90\')\n        urlretrieve(url=fullgb_url, filename=fullbg_filename)\n        print(\'\xe8\x83\x8c\xe6\x99\xaf\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\x8b\xe8\xbd\xbd\xe5\xae\x8c\xe6\x88\x90\')\n        return bg_location_list, fullbg_location_list\n\n    def get_merge_image(self, filename, location_list):\n        """"""\n        \xe6\xa0\xb9\xe6\x8d\xae\xe4\xbd\x8d\xe7\xbd\xae\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x90\x88\xe5\xb9\xb6\xe8\xbf\x98\xe5\x8e\x9f\n        :filename:\xe5\x9b\xbe\xe7\x89\x87\n        :location_list:\xe5\x9b\xbe\xe7\x89\x87\xe4\xbd\x8d\xe7\xbd\xae\n        """"""\n        im = image.open(filename)\n        new_im = image.new(\'RGB\', (260, 116))\n        im_list_upper = []\n        im_list_down = []\n\n        for location in location_list:\n            if location[\'y\'] == -58:\n                im_list_upper.append(im.crop((abs(location[\'x\']), 58, abs(location[\'x\']) + 10, 166)))\n            if location[\'y\'] == 0:\n                im_list_down.append(im.crop((abs(location[\'x\']), 0, abs(location[\'x\']) + 10, 58)))\n\n        new_im = image.new(\'RGB\', (260,116))\n\n        x_offset = 0\n        for im in im_list_upper:\n            new_im.paste(im, (x_offset,0))\n            x_offset += im.size[0]\n\n        x_offset = 0\n        for im in im_list_down:\n            new_im.paste(im, (x_offset,58))\n            x_offset += im.size[0]\n\n        new_im.save(filename)\n\n        return new_im\n\n    def is_pixel_equal(self, img1, img2, x, y):\n        """"""\n        \xe5\x88\xa4\xe6\x96\xad\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x83\x8f\xe7\xb4\xa0\xe6\x98\xaf\xe5\x90\xa6\xe7\x9b\xb8\xe5\x90\x8c\n        :param img1: \xe5\x9b\xbe\xe7\x89\x871\n        :param img2: \xe5\x9b\xbe\xe7\x89\x872\n        :param x: \xe4\xbd\x8d\xe7\xbd\xaex\n        :param y: \xe4\xbd\x8d\xe7\xbd\xaey\n        :return: \xe5\x83\x8f\xe7\xb4\xa0\xe6\x98\xaf\xe5\x90\xa6\xe7\x9b\xb8\xe5\x90\x8c\n        """"""\n        # \xe5\x8f\x96\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\x83\x8f\xe7\xb4\xa0\xe7\x82\xb9\n        pix1 = img1.load()[x, y]\n        pix2 = img2.load()[x, y]\n        threshold = 60\n        if (abs(pix1[0] - pix2[0] < threshold) and abs(pix1[1] - pix2[1] < threshold)\n                and abs(pix1[2] - pix2[2] < threshold)):\n            return True\n        else:\n            return False\n\n    def get_gap(self, img1, img2):\n        """"""\n        \xe8\x8e\xb7\xe5\x8f\x96\xe7\xbc\xba\xe5\x8f\xa3\xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f\n        :param img1: \xe4\xb8\x8d\xe5\xb8\xa6\xe7\xbc\xba\xe5\x8f\xa3\xe5\x9b\xbe\xe7\x89\x87\n        :param img2: \xe5\xb8\xa6\xe7\xbc\xba\xe5\x8f\xa3\xe5\x9b\xbe\xe7\x89\x87\n        :return:\n        """"""\n        left = 43\n        for i in range(left, img1.size[0]):\n            for j in range(img1.size[1]):\n                if not self.is_pixel_equal(img1, img2, i, j):\n                    left = i\n                    return left\n        return left\n\n    def get_track(self, distance):\n        """"""\n        \xe6\xa0\xb9\xe6\x8d\xae\xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f\xe8\x8e\xb7\xe5\x8f\x96\xe7\xa7\xbb\xe5\x8a\xa8\xe8\xbd\xa8\xe8\xbf\xb9\n        :param distance: \xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f\n        :return: \xe7\xa7\xbb\xe5\x8a\xa8\xe8\xbd\xa8\xe8\xbf\xb9\n        """"""\n        # \xe7\xa7\xbb\xe5\x8a\xa8\xe8\xbd\xa8\xe8\xbf\xb9\n        track = []\n        # \xe5\xbd\x93\xe5\x89\x8d\xe4\xbd\x8d\xe7\xa7\xbb\n        current = 0\n        # \xe5\x87\x8f\xe9\x80\x9f\xe9\x98\x88\xe5\x80\xbc\n        mid = distance * 4 / 5\n        # \xe8\xae\xa1\xe7\xae\x97\xe9\x97\xb4\xe9\x9a\x94\n        t = 0.2\n        # \xe5\x88\x9d\xe9\x80\x9f\xe5\xba\xa6\n        v = 0\n\n        while current < distance:\n            if current < mid:\n                # \xe5\x8a\xa0\xe9\x80\x9f\xe5\xba\xa6\xe4\xb8\xba\xe6\xad\xa32\n                a = 2\n            else:\n                # \xe5\x8a\xa0\xe9\x80\x9f\xe5\xba\xa6\xe4\xb8\xba\xe8\xb4\x9f3\n                a = -3\n            # \xe5\x88\x9d\xe9\x80\x9f\xe5\xba\xa6v0\n            v0 = v\n            # \xe5\xbd\x93\xe5\x89\x8d\xe9\x80\x9f\xe5\xba\xa6v = v0 + at\n            v = v0 + a * t\n            # \xe7\xa7\xbb\xe5\x8a\xa8\xe8\xb7\x9d\xe7\xa6\xbbx = v0t + 1/2 * a * t^2\n            move = v0 * t + 1 / 2 * a * t * t\n            # \xe5\xbd\x93\xe5\x89\x8d\xe4\xbd\x8d\xe7\xa7\xbb\n            current += move\n            # \xe5\x8a\xa0\xe5\x85\xa5\xe8\xbd\xa8\xe8\xbf\xb9\n            track.append(round(move))\n        return track\n\n    def get_slider(self):\n        """"""\n        \xe8\x8e\xb7\xe5\x8f\x96\xe6\xbb\x91\xe5\x9d\x97\n        :return: \xe6\xbb\x91\xe5\x9d\x97\xe5\xaf\xb9\xe8\xb1\xa1\n        """"""\n        while True:\n            try:\n                slider = self.browser.find_element_by_xpath(""//div[@class=\'gt_slider_knob gt_show\']"")\n                break\n            except:\n                time.sleep(0.5)\n        return slider\n\n    def move_to_gap(self, slider, track):\n        """"""\n        \xe6\x8b\x96\xe5\x8a\xa8\xe6\xbb\x91\xe5\x9d\x97\xe5\x88\xb0\xe7\xbc\xba\xe5\x8f\xa3\xe5\xa4\x84\n        :param slider: \xe6\xbb\x91\xe5\x9d\x97\n        :param track: \xe8\xbd\xa8\xe8\xbf\xb9\n        :return:\n        """"""\n        ActionChains(self.browser).click_and_hold(slider).perform()\n        while track:\n            x = random.choice(track)\n            ActionChains(self.browser).move_by_offset(xoffset=x, yoffset=0).perform()\n            track.remove(x)\n        time.sleep(0.5)\n        ActionChains(self.browser).release().perform()\n\n    def crack(self):\n        # \xe6\x89\x93\xe5\xbc\x80\xe6\xb5\x8f\xe8\xa7\x88\xe5\x99\xa8\n        self.open()\n\n        # \xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x90\x8d\xe5\xad\x97\n        bg_filename = \'bg.jpg\'\n        fullbg_filename = \'fullbg.jpg\'\n\n        # \xe8\x8e\xb7\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\n        bg_location_list, fullbg_location_list = self.get_images(bg_filename, fullbg_filename)\n\n        # \xe6\xa0\xb9\xe6\x8d\xae\xe4\xbd\x8d\xe7\xbd\xae\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x90\x88\xe5\xb9\xb6\xe8\xbf\x98\xe5\x8e\x9f\n        bg_img = self.get_merge_image(bg_filename, bg_location_list)\n        fullbg_img = self.get_merge_image(fullbg_filename, fullbg_location_list)\n\n        # \xe8\x8e\xb7\xe5\x8f\x96\xe7\xbc\xba\xe5\x8f\xa3\xe4\xbd\x8d\xe7\xbd\xae\n        gap = self.get_gap(fullbg_img, bg_img)\n        print(\'\xe7\xbc\xba\xe5\x8f\xa3\xe4\xbd\x8d\xe7\xbd\xae\', gap)\n\n        track = self.get_track(gap-self.BORDER)\n        print(\'\xe6\xbb\x91\xe5\x8a\xa8\xe6\xbb\x91\xe5\x9d\x97\')\n        print(track)\n\n        # \xe7\x82\xb9\xe6\x8c\x89\xe5\x91\xbc\xe5\x87\xba\xe7\xbc\xba\xe5\x8f\xa3\n        slider = self.get_slider()\n        # \xe6\x8b\x96\xe5\x8a\xa8\xe6\xbb\x91\xe5\x9d\x97\xe5\x88\xb0\xe7\xbc\xba\xe5\x8f\xa3\xe5\xa4\x84\n        self.move_to_gap(slider, track)\n\n\nif __name__ == \'__main__\':\n    print(\'\xe5\xbc\x80\xe5\xa7\x8b\xe9\xaa\x8c\xe8\xaf\x81\')\n    crack = Crack(u\'\xe4\xb8\xad\xe5\x9b\xbd\xe7\xa7\xbb\xe5\x8a\xa8\')\n    crack.crack()\n    print(\'\xe9\xaa\x8c\xe8\xaf\x81\xe6\x88\x90\xe5\x8a\x9f\')'"
Spider/ch_Distributedcrawler/Connection.py,0,"b'\xe8\xb4\x9f\xe8\xb4\xa3\xe6\xa0\xb9\xe6\x8d\xaesetting\xe4\xb8\xad\xe9\x85\x8d\xe7\xbd\xae\xe5\xae\x9e\xe4\xbe\x8b\xe5\x8c\x96redis\xe8\xbf\x9e\xe6\x8e\xa5\xe3\x80\x82\xe8\xa2\xabdupefilter\xe5\x92\x8cscheduler\xe8\xb0\x83\xe7\x94\xa8\xef\xbc\x8c\xe6\x80\xbb\xe4\xb9\x8b\xe6\xb6\x89\xe5\x8f\x8a\xe5\x88\xb0redis\xe5\xad\x98\xe5\x8f\x96\xe7\x9a\x84\xe9\x83\xbd\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8\xe5\x88\xb0\xe8\xbf\x99\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9d\x97\xe3\x80\x82\r\n\xe8\xbf\x99\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xbb\xe8\xa6\x81\xe6\x98\xaf\xe5\xae\x9e\xe7\x8e\xb0\xe8\xbf\x9e\xe6\x8e\xa5redis\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xe7\x9a\x84\xe5\x8a\x9f\xe8\x83\xbd\xef\xbc\x8c\xe8\xbf\x99\xe4\xba\x9b\xe8\xbf\x9e\xe6\x8e\xa5\xe6\x8e\xa5\xe5\x8f\xa3\xe5\x9c\xa8\xe5\x85\xb6\xe4\xbb\x96\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xe7\xbb\x8f\xe5\xb8\xb8\xe8\xa2\xab\xe7\x94\xa8\xe5\x88\xb0\r\nimport six\r\nfrom scrapy.utils.misc import load_object\r\nfrom . import defaults\r\n\r\n\r\n# Shortcut maps \'setting name\' -> \'parmater name\'.\r\n# \xe8\xa6\x81\xe6\x83\xb3\xe8\xbf\x9e\xe6\x8e\xa5\xe5\x88\xb0redis\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xef\xbc\x8c\xe5\x92\x8c\xe5\x85\xb6\xe4\xbb\x96\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xe5\xb7\xae\xe4\xb8\x8d\xe5\xa4\x9a\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe4\xb8\x80\xe4\xb8\xaaip\xe5\x9c\xb0\xe5\x9d\x80\xe3\x80\x81\xe7\xab\xaf\xe5\x8f\xa3\xe5\x8f\xb7\xe3\x80\x81\xe7\x94\xa8\xe6\x88\xb7\xe5\x90\x8d\xe5\xaf\x86\xe7\xa0\x81\xef\xbc\x88\xe5\x8f\xaf\xe9\x80\x89\xef\xbc\x89\xe5\x92\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb4\xe5\xbd\xa2\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xe7\xbc\x96\xe5\x8f\xb7\r\n#  Shortcut maps \'setting name\' -> \'parmater name\'.\r\nSETTINGS_PARAMS_MAP = {\r\n    \'REDIS_URL\': \'url\',\r\n    \'REDIS_HOST\': \'host\',\r\n    \'REDIS_PORT\': \'port\',\r\n    \'REDIS_ENCODING\': \'encoding\',\r\n}\r\n\r\n\r\ndef get_redis_from_settings(settings):\r\n    """"""Returns a redis client instance from given Scrapy settings object.\r\n\r\n    This function uses ``get_client`` to instantiate the client and uses\r\n    ``defaults.REDIS_PARAMS`` global as defaults values for the parameters. You\r\n    can override them using the ``REDIS_PARAMS`` setting.\r\n\r\n    Parameters\r\n    ----------\r\n    settings : Settings\r\n        A scrapy settings object. See the supported settings below.\r\n\r\n    Returns\r\n    -------\r\n    server\r\n        Redis client instance.\r\n\r\n    Other Parameters\r\n    ----------------\r\n    REDIS_URL : str, optional\r\n        Server connection URL.\r\n    REDIS_HOST : str, optional\r\n        Server host.\r\n    REDIS_PORT : str, optional\r\n        Server port.\r\n    REDIS_ENCODING : str, optional\r\n        Data encoding.\r\n    REDIS_PARAMS : dict, optional\r\n        Additional client parameters.\r\n\r\n    """"""\r\n    params = defaults.REDIS_PARAMS.copy()\r\n    params.update(settings.getdict(\'REDIS_PARAMS\'))\r\n    # XXX: Deprecate REDIS_* settings.\r\n    for source, dest in SETTINGS_PARAMS_MAP.items():\r\n        val = settings.get(source)\r\n        if val:\r\n            params[dest] = val\r\n\r\n    # Allow ``redis_cls`` to be a path to a class.\r\n    if isinstance(params.get(\'redis_cls\'), six.string_types):\r\n        params[\'redis_cls\'] = load_object(params[\'redis_cls\'])\r\n    #\xe5\x9c\xa8\xe8\xbf\x99\xe9\x87\x8c\xe8\xb0\x83\xe7\x94\xa8get_redis\xe5\x87\xbd\xe6\x95\xb0\r\n    return get_redis(**params)\r\n\r\n\r\n# Backwards compatible alias.\r\nfrom_settings = get_redis_from_settings\r\n\r\n\r\n# \xe8\xbf\x94\xe5\x9b\x9e\xe7\x9a\x84\xe6\x98\xafredis\xe5\xba\x93\xe7\x9a\x84Redis\xe5\xaf\xb9\xe8\xb1\xa1\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe7\x94\xa8\xe6\x9d\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x95\xb0\xe6\x8d\xae\xe6\x93\x8d\xe4\xbd\x9c\xe7\x9a\x84\xe5\xaf\xb9\xe8\xb1\xa1\r\ndef get_redis(**kwargs):\r\n    """"""Returns a redis client instance.\r\n\r\n    Parameters\r\n    ----------\r\n    redis_cls : class, optional\r\n        Defaults to ``redis.StrictRedis``.\r\n    url : str, optional\r\n        If given, ``redis_cls.from_url`` is used to instantiate the class.\r\n    **kwargs\r\n        Extra parameters to be passed to the ``redis_cls`` class.\r\n\r\n    Returns\r\n    -------\r\n    server\r\n        Redis client instance.\r\n\r\n    """"""\r\n    redis_cls = kwargs.pop(\'redis_cls\', defaults.REDIS_CLS)\r\n    url = kwargs.pop(\'url\', None)\r\n    if url:\r\n        return redis_cls.from_url(url, **kwargs)\r\n    else:\r\n        return redis_cls(**kwargs)'"
Spider/ch_Distributedcrawler/Duperfilter.py,0,"b'\xe8\xb4\x9f\xe8\xb4\xa3\xe6\x89\xa7\xe8\xa1\x8crequst\xe7\x9a\x84\xe5\x8e\xbb\xe9\x87\x8d\xef\xbc\x8c\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84\xe5\xbe\x88\xe6\x9c\x89\xe6\x8a\x80\xe5\xb7\xa7\xe6\x80\xa7\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8redis\xe7\x9a\x84set\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84\xe3\x80\x82\xe4\xbd\x86\xe6\x98\xaf\xe6\xb3\xa8\xe6\x84\x8fscheduler\xe5\xb9\xb6\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe5\x85\xb6\xe4\xb8\xad\xe7\x94\xa8\xe4\xba\x8e\xe5\x9c\xa8\xe8\xbf\x99\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9d\x97\xe4\xb8\xad\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84dupefilter\xe9\x94\xae\xe5\x81\x9arequest\xe7\x9a\x84\xe8\xb0\x83\xe5\xba\xa6\xef\xbc\x8c\r\n\xe8\x80\x8c\xe6\x98\xaf\xe4\xbd\xbf\xe7\x94\xa8queue.py\xe6\xa8\xa1\xe5\x9d\x97\xe4\xb8\xad\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84queue\xe3\x80\x82\xe5\xbd\x93request\xe4\xb8\x8d\xe9\x87\x8d\xe5\xa4\x8d\xe6\x97\xb6\xef\xbc\x8c\xe5\xb0\x86\xe5\x85\xb6\xe5\xad\x98\xe5\x85\xa5\xe5\x88\xb0queue\xe4\xb8\xad\xef\xbc\x8c\xe8\xb0\x83\xe5\xba\xa6\xe6\x97\xb6\xe5\xb0\x86\xe5\x85\xb6\xe5\xbc\xb9\xe5\x87\xba\xe3\x80\x82\r\n\r\nimport logging\r\nimport time\r\nfrom scrapy.dupefilters import BaseDupeFilter\r\nfrom scrapy.utils.request import request_fingerprint\r\nfrom . import defaults\r\nfrom .connection import get_redis_from_settings\r\n\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\n# TODO: Rename class to RedisDupeFilter.\r\nclass RFPDupeFilter(BaseDupeFilter):\r\n    """"""Redis-based request duplicates filter.\r\n\r\n    This class can also be used with default Scrapy\'s scheduler.\r\n\r\n    """"""\r\n\r\n    logger = logger\r\n\r\n    def __init__(self, server, key, debug=False):\r\n        """"""Initialize the duplicates filter.\r\n\r\n        Parameters\r\n        ----------\r\n        server : redis.StrictRedis\r\n            The redis server instance.\r\n        key : str\r\n            Redis key Where to store fingerprints.\r\n        debug : bool, optional\r\n            Whether to log filtered requests.\r\n\r\n        """"""\r\n        self.server = server\r\n        self.key = key\r\n        self.debug = debug\r\n        self.logdupes = True\r\n\r\n    @classmethod\r\n    def from_settings(cls, settings):\r\n        """"""Returns an instance from given settings.\r\n\r\n        This uses by default the key ``dupefilter:<timestamp>``. When using the\r\n        ``scrapy_redis.scheduler.Scheduler`` class, this method is not used as\r\n        it needs to pass the spider name in the key.\r\n\r\n        Parameters\r\n        ----------\r\n        settings : scrapy.settings.Settings\r\n\r\n        Returns\r\n        -------\r\n        RFPDupeFilter\r\n            A RFPDupeFilter instance.\r\n\r\n\r\n        """"""\r\n        server = get_redis_from_settings(settings)\r\n        # XXX: This creates one-time key. needed to support to use this\r\n        # class as standalone dupefilter with scrapy\'s default scheduler\r\n        # if scrapy passes spider on open() method this wouldn\'t be needed\r\n        # TODO: Use SCRAPY_JOB env as default and fallback to timestamp.\r\n        key = defaults.DUPEFILTER_KEY % {\'timestamp\': int(time.time())}\r\n        debug = settings.getbool(\'DUPEFILTER_DEBUG\')\r\n        return cls(server, key=key, debug=debug)\r\n\r\n    @classmethod\r\n    def from_crawler(cls, crawler):\r\n        """"""Returns instance from crawler.\r\n\r\n        Parameters\r\n        ----------\r\n        crawler : scrapy.crawler.Crawler\r\n\r\n        Returns\r\n        -------\r\n        RFPDupeFilter\r\n            Instance of RFPDupeFilter.\r\n\r\n        """"""\r\n        return cls.from_settings(crawler.settings)\r\n\r\n    def request_seen(self, request):\r\n        """"""Returns True if request was already seen.\r\n\r\n        Parameters\r\n        ----------\r\n        request : scrapy.http.Request\r\n\r\n        Returns\r\n        -------\r\n        bool\r\n\r\n        """"""\r\n        fp = self.request_fingerprint(request)\r\n        # This returns the number of values added, zero if already exists.\r\n        added = self.server.sadd(self.key, fp)\r\n        return added == 0\r\n\r\n    def request_fingerprint(self, request):\r\n        """"""Returns a fingerprint for a given request.\r\n\r\n        Parameters\r\n        ----------\r\n        request : scrapy.http.Request\r\n\r\n        Returns\r\n        -------\r\n        str\r\n\r\n        """"""\r\n        return request_fingerprint(request)\r\n\r\n    def close(self, reason=\'\'):\r\n        """"""Delete data on close. Called by Scrapy\'s scheduler.\r\n\r\n        Parameters\r\n        ----------\r\n        reason : str, optional\r\n\r\n        """"""\r\n        self.clear()\r\n\r\n    def clear(self):\r\n        """"""Clears fingerprints data.""""""\r\n        self.server.delete(self.key)\r\n\r\n    def log(self, request, spider):\r\n        """"""Logs given request.\r\n\r\n        Parameters\r\n        ----------\r\n        request : scrapy.http.Request\r\n        spider : scrapy.spiders.Spider\r\n\r\n        """"""\r\n        if self.debug:\r\n            msg = ""Filtered duplicate request: %(request)s""\r\n            self.logger.debug(msg, {\'request\': request}, extra={\'spider\': spider})\r\n        elif self.logdupes:\r\n            msg = (""Filtered duplicate request %(request)s""\r\n                   "" - no more duplicates will be shown""\r\n                   "" (see DUPEFILTER_DEBUG to show all duplicates)"")\r\n            self.logger.debug(msg, {\'request\': request}, extra={\'spider\': spider})\r\n            self.logdupes = False\r\n\r\n\xe8\xbf\x99\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe7\x9c\x8b\xe8\xb5\xb7\xe6\x9d\xa5\xe6\xaf\x94\xe8\xbe\x83\xe5\xa4\x8d\xe6\x9d\x82\xef\xbc\x8c\xe9\x87\x8d\xe5\x86\x99\xe4\xba\x86scrapy\xe6\x9c\xac\xe8\xba\xab\xe5\xb7\xb2\xe7\xbb\x8f\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84request\xe5\x88\xa4\xe9\x87\x8d\xe5\x8a\x9f\xe8\x83\xbd\xe3\x80\x82\xe5\x9b\xa0\xe4\xb8\xba\xe6\x9c\xac\xe8\xba\xabscrapy\xe5\x8d\x95\xe6\x9c\xba\xe8\xb7\x91\xe7\x9a\x84\xe8\xaf\x9d\xef\xbc\x8c\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe8\xaf\xbb\xe5\x8f\x96\xe5\x86\x85\xe5\xad\x98\xe4\xb8\xad\xe7\x9a\x84request\xe9\x98\x9f\xe5\x88\x97\xe6\x88\x96\xe8\x80\x85\xe6\x8c\x81\xe4\xb9\x85\xe5\x8c\x96\xe7\x9a\x84request\xe9\x98\x9f\xe5\x88\x97\r\n\xef\xbc\x88scrapy\xe9\xbb\x98\xe8\xae\xa4\xe7\x9a\x84\xe6\x8c\x81\xe4\xb9\x85\xe5\x8c\x96\xe4\xbc\xbc\xe4\xb9\x8e\xe6\x98\xafjson\xe6\xa0\xbc\xe5\xbc\x8f\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\xef\xbc\x8c\xe4\xb8\x8d\xe6\x98\xaf\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xef\xbc\x89\xe5\xb0\xb1\xe8\x83\xbd\xe5\x88\xa4\xe6\x96\xad\xe8\xbf\x99\xe6\xac\xa1\xe8\xa6\x81\xe5\x8f\x91\xe5\x87\xba\xe7\x9a\x84request url\xe6\x98\xaf\xe5\x90\xa6\xe5\xb7\xb2\xe7\xbb\x8f\xe8\xaf\xb7\xe6\xb1\x82\xe8\xbf\x87\xe6\x88\x96\xe8\x80\x85\xe6\xad\xa3\xe5\x9c\xa8\xe8\xb0\x83\xe5\xba\xa6\xef\xbc\x88\xe6\x9c\xac\xe5\x9c\xb0\xe8\xaf\xbb\xe5\xb0\xb1\xe8\xa1\x8c\xe4\xba\x86\xef\xbc\x89\xe3\x80\x82\xe8\x80\x8c\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe8\xb7\x91\xe7\x9a\x84\xe8\xaf\x9d\xef\xbc\x8c\xe5\xb0\xb1\r\n\xe9\x9c\x80\xe8\xa6\x81\xe5\x90\x84\xe4\xb8\xaa\xe4\xb8\xbb\xe6\x9c\xba\xe4\xb8\x8a\xe7\x9a\x84scheduler\xe9\x83\xbd\xe8\xbf\x9e\xe6\x8e\xa5\xe5\x90\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xe7\x9a\x84\xe5\x90\x8c\xe4\xb8\x80\xe4\xb8\xaarequest\xe6\xb1\xa0\xe6\x9d\xa5\xe5\x88\xa4\xe6\x96\xad\xe8\xbf\x99\xe6\xac\xa1\xe7\x9a\x84\xe8\xaf\xb7\xe6\xb1\x82\xe6\x98\xaf\xe5\x90\xa6\xe6\x98\xaf\xe9\x87\x8d\xe5\xa4\x8d\xe7\x9a\x84\xe4\xba\x86\xe3\x80\x82\r\n\r\n\xe5\x9c\xa8\xe8\xbf\x99\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xef\xbc\x8c\xe9\x80\x9a\xe8\xbf\x87\xe7\xbb\xa7\xe6\x89\xbfBaseDupeFilter\xe9\x87\x8d\xe5\x86\x99\xe4\xbb\x96\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe5\x9f\xba\xe4\xba\x8eredis\xe7\x9a\x84\xe5\x88\xa4\xe9\x87\x8d\xe3\x80\x82\xe6\xa0\xb9\xe6\x8d\xae\xe6\xba\x90\xe4\xbb\xa3\xe7\xa0\x81\xe6\x9d\xa5\xe7\x9c\x8b\xef\xbc\x8cscrapy-redis\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86scrapy\xe6\x9c\xac\xe8\xba\xab\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaafingerprint\xe6\x8e\xa5\r\nrequest_fingerprint\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x8e\xa5\xe5\x8f\xa3\xe5\xbe\x88\xe6\x9c\x89\xe8\xb6\xa3\xef\xbc\x8c\xe6\xa0\xb9\xe6\x8d\xaescrapy\xe6\x96\x87\xe6\xa1\xa3\xe6\x89\x80\xe8\xaf\xb4\xef\xbc\x8c\xe4\xbb\x96\xe9\x80\x9a\xe8\xbf\x87hash\xe6\x9d\xa5\xe5\x88\xa4\xe6\x96\xad\xe4\xb8\xa4\xe4\xb8\xaaurl\xe6\x98\xaf\xe5\x90\xa6\xe7\x9b\xb8\xe5\x90\x8c\xef\xbc\x88\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84url\xe4\xbc\x9a\xe7\x94\x9f\xe6\x88\x90\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84hash\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x89\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\xbd\x93\xe4\xb8\xa4\xe4\xb8\xaaurl\xe7\x9a\x84\xe5\x9c\xb0\xe5\x9d\x80\xe7\x9b\xb8\xe5\x90\x8c\xef\xbc\x8c\r\nget\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\xe7\x9b\xb8\xe5\x90\x8c\xe4\xbd\x86\xe6\x98\xaf\xe9\xa1\xba\xe5\xba\x8f\xe4\xb8\x8d\xe5\x90\x8c\xe6\x97\xb6\xef\xbc\x8c\xe4\xb9\x9f\xe4\xbc\x9a\xe7\x94\x9f\xe6\x88\x90\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84hash\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x88\xe8\xbf\x99\xe4\xb8\xaa\xe7\x9c\x9f\xe7\x9a\x84\xe6\xaf\x94\xe8\xbe\x83\xe7\xa5\x9e\xe5\xa5\x87\xe3\x80\x82\xe3\x80\x82\xe3\x80\x82\xef\xbc\x89\xe6\x89\x80\xe4\xbb\xa5scrapy-redis\xe4\xbe\x9d\xe6\x97\xa7\xe4\xbd\xbf\xe7\x94\xa8url\xe7\x9a\x84fingerprint\xe6\x9d\xa5\xe5\x88\xa4\xe6\x96\xadrequest\xe8\xaf\xb7\xe6\xb1\x82\xe6\x98\xaf\xe5\x90\xa6\xe5\xb7\xb2\xe7\xbb\x8f\xe5\x87\xba\xe7\x8e\xb0\xe8\xbf\x87\xe3\x80\x82\r\n\xe8\xbf\x99\xe4\xb8\xaa\xe7\xb1\xbb\xe9\x80\x9a\xe8\xbf\x87\xe8\xbf\x9e\xe6\x8e\xa5redis\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaakey\xe6\x9d\xa5\xe5\x90\x91redis\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaaset\xe4\xb8\xad\xe6\x8f\x92\xe5\x85\xa5fingerprint\xef\xbc\x88\xe8\xbf\x99\xe4\xb8\xaakey\xe5\xaf\xb9\xe4\xba\x8e\xe5\x90\x8c\xe4\xb8\x80\xe7\xa7\x8dspider\xe6\x98\xaf\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xef\xbc\x8credis\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaakey-value\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9ckey\xe6\x98\xaf\xe7\x9b\xb8\r\n\xe5\x90\x8c\xe7\x9a\x84\xef\xbc\x8c\xe8\xae\xbf\xe9\x97\xae\xe5\x88\xb0\xe7\x9a\x84\xe5\x80\xbc\xe5\xb0\xb1\xe6\x98\xaf\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe4\xbd\xbf\xe7\x94\xa8spider\xe5\x90\x8d\xe5\xad\x97+DupeFilter\xe7\x9a\x84key\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe5\x9c\xa8\xe4\xb8\x8d\xe5\x90\x8c\xe4\xb8\xbb\xe6\x9c\xba\xe4\xb8\x8a\xe7\x9a\x84\xe4\xb8\x8d\xe5\x90\x8c\xe7\x88\xac\xe8\x99\xab\xe5\xae\x9e\xe4\xbe\x8b\xef\xbc\x8c\xe5\x8f\xaa\xe8\xa6\x81\xe5\xb1\x9e\xe4\xba\x8e\xe5\x90\x8c\xe4\xb8\x80\xe7\xa7\x8dspider\xef\xbc\x8c\xe5\xb0\xb1\xe4\xbc\x9a\xe8\xae\xbf\xe9\x97\xae\xe5\x88\xb0\xe5\x90\x8c\xe4\xb8\x80\xe4\xb8\xaaset\xef\xbc\x8c\xe8\x80\x8c\xe8\xbf\x99\xe4\xb8\xaaset\r\n\xe5\xb0\xb1\xe6\x98\xaf\xe4\xbb\x96\xe4\xbb\xac\xe7\x9a\x84url\xe5\x88\xa4\xe9\x87\x8d\xe6\xb1\xa0\xef\xbc\x89\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc\xe4\xb8\xba0\xef\xbc\x8c\xe8\xaf\xb4\xe6\x98\x8e\xe8\xaf\xa5set\xe4\xb8\xad\xe8\xaf\xa5fingerprint\xe5\xb7\xb2\xe7\xbb\x8f\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x88\xe5\x9b\xa0\xe4\xb8\xba\xe9\x9b\x86\xe5\x90\x88\xe6\x98\xaf\xe6\xb2\xa1\xe6\x9c\x89\xe9\x87\x8d\xe5\xa4\x8d\xe5\x80\xbc\xe7\x9a\x84\xef\xbc\x89\xef\xbc\x8c\xe5\x88\x99\xe8\xbf\x94\xe5\x9b\x9eFalse\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc\xe4\xb8\xba1\xef\xbc\x8c\xe8\xaf\xb4\xe6\x98\x8e\xe6\xb7\xbb\xe5\x8a\xa0\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaafingerprint\r\n\xe5\x88\xb0set\xe4\xb8\xad\xef\xbc\x8c\xe5\x88\x99\xe8\xaf\xb4\xe6\x98\x8e\xe8\xbf\x99\xe4\xb8\xaarequest\xe6\xb2\xa1\xe6\x9c\x89\xe9\x87\x8d\xe5\xa4\x8d\xef\xbc\x8c\xe4\xba\x8e\xe6\x98\xaf\xe8\xbf\x94\xe5\x9b\x9eTrue\xef\xbc\x8c\xe8\xbf\x98\xe9\xa1\xba\xe4\xbe\xbf\xe6\x8a\x8a\xe6\x96\xb0fingerprint\xe5\x8a\xa0\xe5\x85\xa5\xe5\x88\xb0\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xe4\xb8\xad\xe4\xba\x86\xe3\x80\x82 DupeFilter\xe5\x88\xa4\xe9\x87\x8d\xe4\xbc\x9a\xe5\x9c\xa8scheduler\xe7\xb1\xbb\xe4\xb8\xad\xe7\x94\xa8\xe5\x88\xb0\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaarequest\xe5\x9c\xa8\xe8\xbf\x9b\xe5\x85\xa5\r\n\xe8\xb0\x83\xe5\xba\xa6\xe4\xb9\x8b\xe5\x89\x8d\xe9\x83\xbd\xe8\xa6\x81\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\xa4\xe9\x87\x8d\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe9\x87\x8d\xe5\xa4\x8d\xe5\xb0\xb1\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe5\x8f\x82\xe5\x8a\xa0\xe8\xb0\x83\xe5\xba\xa6\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe8\x88\x8d\xe5\xbc\x83\xe5\xb0\xb1\xe5\xa5\xbd\xe4\xba\x86\xef\xbc\x8c\xe4\xb8\x8d\xe7\x84\xb6\xe5\xb0\xb1\xe6\x98\xaf\xe7\x99\xbd\xe7\x99\xbd\xe6\xb5\xaa\xe8\xb4\xb9\xe8\xb5\x84\xe6\xba\x90\xe3\x80\x82'"
Spider/ch_Distributedcrawler/Pipelines.py,0,"b'\xe8\xbf\x99\xe6\x98\xaf\xe6\x98\xaf\xe7\x94\xa8\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe4\xbd\x9c\xe7\x94\xa8\xe3\x80\x82\xe5\xae\x83\xe5\xb0\x86Item\xe5\xad\x98\xe5\x82\xa8\xe5\x9c\xa8redis\xe4\xb8\xad\xe4\xbb\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe5\xa4\x84\xe7\x90\x86\xe3\x80\x82\xe7\x94\xb1\xe4\xba\x8e\xe5\x9c\xa8\xe8\xbf\x99\xe9\x87\x8c\xe9\x9c\x80\xe8\xa6\x81\xe8\xaf\xbb\xe5\x8f\x96\xe9\x85\x8d\xe7\xbd\xae\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\xb0\xb1\xe7\x94\xa8\xe5\x88\xb0\xe4\xba\x86from_crawler()\xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x82\r\nfrom scrapy.utils.misc import load_object\r\nfrom scrapy.utils.serialize import ScrapyJSONEncoder\r\nfrom twisted.internet.threads import deferToThread\r\n\r\nfrom . import connection, defaults\r\n\r\n\r\ndefault_serialize = ScrapyJSONEncoder().encode\r\n\r\n\r\nclass RedisPipeline(object):\r\n    """"""Pushes serialized item into a redis list/queue\r\n\r\n    Settings\r\n    --------\r\n    REDIS_ITEMS_KEY : str\r\n        Redis key where to store items.\r\n    REDIS_ITEMS_SERIALIZER : str\r\n        Object path to serializer function.\r\n\r\n    """"""\r\n\r\n    def __init__(self, server,\r\n                 key=defaults.PIPELINE_KEY,\r\n                 serialize_func=default_serialize):\r\n        """"""Initialize pipeline.\r\n\r\n        Parameters\r\n        ----------\r\n        server : StrictRedis\r\n            Redis client instance.\r\n        key : str\r\n            Redis key where to store items.\r\n        serialize_func : callable\r\n            Items serializer function.\r\n\r\n        """"""\r\n        self.server = server\r\n        self.key = key\r\n        self.serialize = serialize_func\r\n\r\n    @classmethod\r\n    def from_settings(cls, settings):\r\n        params = {\r\n            \'server\': connection.from_settings(settings),\r\n        }\r\n        if settings.get(\'REDIS_ITEMS_KEY\'):\r\n            params[\'key\'] = settings[\'REDIS_ITEMS_KEY\']\r\n        if settings.get(\'REDIS_ITEMS_SERIALIZER\'):\r\n            params[\'serialize_func\'] = load_object(\r\n                settings[\'REDIS_ITEMS_SERIALIZER\']\r\n            )\r\n\r\n        return cls(**params)\r\n\r\n    @classmethod\r\n    def from_crawler(cls, crawler):\r\n        return cls.from_settings(crawler.settings)\r\n\r\n    def process_item(self, item, spider):\r\n        return deferToThread(self._process_item, item, spider)\r\n\r\n    def _process_item(self, item, spider):\r\n        key = self.item_key(item, spider)\r\n        data = self.serialize(item)\r\n        self.server.rpush(key, data)\r\n        return item\r\n\r\n    def item_key(self, item, spider):\r\n        """"""Returns redis key based on given spider.\r\n\r\n        Override this function to use a different key depending on the item\r\n        and/or spider.\r\n\r\n        """"""\r\n        return self.key % {\'spider\': spider.name}\r\npipelines\xe6\x96\x87\xe4\xbb\xb6\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaaitem pipieline\xe7\xb1\xbb\xef\xbc\x8c\xe5\x92\x8cscrapy\xe7\x9a\x84item pipeline\xe6\x98\xaf\xe5\x90\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe5\xaf\xb9\xe8\xb1\xa1\xef\xbc\x8c\xe9\x80\x9a\xe8\xbf\x87\xe4\xbb\x8esettings\xe4\xb8\xad\xe6\x8b\xbf\xe5\x88\xb0\xe6\x88\x91\xe4\xbb\xac\xe9\x85\x8d\xe7\xbd\xae\xe7\x9a\x84REDIS_ITEMS_KEY\xe4\xbd\x9c\xe4\xb8\xbakey\xef\xbc\x8c\r\n\xe6\x8a\x8aitem\xe4\xb8\xb2\xe8\xa1\x8c\xe5\x8c\x96\xe4\xb9\x8b\xe5\x90\x8e\xe5\xad\x98\xe5\x85\xa5redis\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84value\xe4\xb8\xad\xef\xbc\x88\xe8\xbf\x99\xe4\xb8\xaavalue\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x87\xba\xe5\x87\xba\xe6\x98\xaf\xe4\xb8\xaalist\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\xaaitem\xe6\x98\xaf\xe8\xbf\x99\xe4\xb8\xaalist\xe4\xb8\xad\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbb\x93\xe7\x82\xb9\xef\xbc\x89\xef\xbc\x8c\r\n\xe8\xbf\x99\xe4\xb8\xaapipeline\xe6\x8a\x8a\xe6\x8f\x90\xe5\x8f\x96\xe5\x87\xba\xe7\x9a\x84item\xe5\xad\x98\xe8\xb5\xb7\xe6\x9d\xa5\xef\xbc\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe6\x96\xb9\xe4\xbe\xbf\xe6\x88\x91\xe4\xbb\xac\xe5\xbb\xb6\xe5\x90\x8e\xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82'"
Spider/ch_Distributedcrawler/Queue.py,0,"b'\xe8\xaf\xa5\xe6\x96\x87\xe4\xbb\xb6\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe5\x87\xa0\xe4\xb8\xaa\xe5\xae\xb9\xe5\x99\xa8\xe7\xb1\xbb\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe8\xbf\x99\xe4\xba\x9b\xe5\xae\xb9\xe5\x99\xa8\xe5\x92\x8credis\xe4\xba\xa4\xe4\xba\x92\xe9\xa2\x91\xe7\xb9\x81\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8a\xe8\xbe\xb9picklecompat\xe4\xb8\xad\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\xe5\x8c\x96\xe5\x99\xa8\xe3\x80\x82\xe8\xbf\x99\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84\xe5\x87\xa0\xe4\xb8\xaa\xe5\xae\xb9\xe5\x99\xa8\xe5\xa4\xa7\xe4\xbd\x93\xe7\x9b\xb8\xe5\x90\x8c\xef\xbc\x8c\xe5\x8f\xaa\xe4\xb8\x8d\xe8\xbf\x87\xe4\xb8\x80\xe4\xb8\xaa\xe6\x98\xaf\xe9\x98\x9f\xe5\x88\x97\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x98\xaf\xe6\xa0\x88\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x98\xaf\xe4\xbc\x98\xe5\x85\x88\xe7\xba\xa7\xe9\x98\x9f\xe5\x88\x97\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\x89\xe4\xb8\xaa\xe5\xae\xb9\xe5\x99\xa8\xe5\x88\xb0\xe6\x97\xb6\xe5\x80\x99\xe4\xbc\x9a\xe8\xa2\xabscheduler\xe5\xaf\xb9\xe8\xb1\xa1\xe5\xae\x9e\xe4\xbe\x8b\xe5\x8c\x96\xef\xbc\x8c\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0request\xe7\x9a\x84\xe8\xb0\x83\xe5\xba\xa6\xe3\x80\x82\xe6\xaf\x94\xe5\xa6\x82\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8SpiderQueue\xe6\x9c\x80\xe4\xb8\xba\xe8\xb0\x83\xe5\xba\xa6\xe9\x98\x9f\xe5\x88\x97\xe7\x9a\x84\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe5\x88\xb0\xe6\x97\xb6\xe5\x80\x99request\xe7\x9a\x84\xe8\xb0\x83\xe5\xba\xa6\xe6\x96\xb9\xe6\xb3\x95\xe5\xb0\xb1\xe6\x98\xaf\xe5\x85\x88\xe8\xbf\x9b\xe5\x85\x88\xe5\x87\xba\xef\xbc\x8c\xe8\x80\x8c\xe5\xae\x9e\xe7\x94\xa8SpiderStack\xe5\xb0\xb1\xe6\x98\xaf\xe5\x85\x88\xe8\xbf\x9b\xe5\x90\x8e\xe5\x87\xba\xe4\xba\x86\xe3\x80\x82\r\n\xe4\xbb\x8eSpiderQueue\xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9c\x8b\xe5\x87\xba\xe6\x9d\xa5\xef\xbc\x8c\xe4\xbb\x96\xe7\x9a\x84push\xe5\x87\xbd\xe6\x95\xb0\xe5\xb0\xb1\xe5\x92\x8c\xe5\x85\xb6\xe4\xbb\x96\xe5\xae\xb9\xe5\x99\xa8\xe7\x9a\x84\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe5\x8f\xaa\xe4\xb8\x8d\xe8\xbf\x87push\xe8\xbf\x9b\xe5\x8e\xbb\xe7\x9a\x84request\xe8\xaf\xb7\xe6\xb1\x82\xe5\x85\x88\xe8\xa2\xabscrapy\xe7\x9a\x84\xe6\x8e\xa5\xe5\x8f\xa3request_to_dict\xe5\x8f\x98\xe6\x88\x90\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaadict\xe5\xaf\xb9\xe8\xb1\xa1\xef\xbc\x88\xe5\x9b\xa0\xe4\xb8\xbarequest\xe5\xaf\xb9\xe8\xb1\xa1\xe5\xae\x9e\xe5\x9c\xa8\xe6\x98\xaf\xe6\xaf\x94\xe8\xbe\x83\xe5\xa4\x8d\xe6\x9d\x82\xef\xbc\x8c\xe6\x9c\x89\xe6\x96\xb9\xe6\xb3\x95\xe6\x9c\x89\xe5\xb1\x9e\xe6\x80\xa7\xe4\xb8\x8d\xe5\xa5\xbd\xe4\xb8\xb2\xe8\xa1\x8c\xe5\x8c\x96\xef\xbc\x89\xef\xbc\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbd\xbf\xe7\x94\xa8picklecompat\xe4\xb8\xad\xe7\x9a\x84serializer\xe4\xb8\xb2\xe8\xa1\x8c\xe5\x8c\x96\xe4\xb8\xba\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe7\x89\xb9\xe5\xae\x9a\xe7\x9a\x84key\xe5\xad\x98\xe5\x85\xa5redis\xe4\xb8\xad\xef\xbc\x88\xe8\xaf\xa5key\xe5\x9c\xa8\xe5\x90\x8c\xe4\xb8\x80\xe7\xa7\x8dspider\xe4\xb8\xad\xe6\x98\xaf\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xef\xbc\x89\xe3\x80\x82\xe8\x80\x8c\xe8\xb0\x83\xe7\x94\xa8pop\xe6\x97\xb6\xef\xbc\x8c\xe5\x85\xb6\xe5\xae\x9e\xe5\xb0\xb1\xe6\x98\xaf\xe4\xbb\x8eredis\xe7\x94\xa8\xe9\x82\xa3\xe4\xb8\xaa\xe7\x89\xb9\xe5\xae\x9a\xe7\x9a\x84key\xe5\x8e\xbb\xe8\xaf\xbb\xe5\x85\xb6\xe5\x80\xbc\xef\xbc\x88\xe4\xb8\x80\xe4\xb8\xaalist\xef\xbc\x89\xef\xbc\x8c\xe4\xbb\x8elist\xe4\xb8\xad\xe8\xaf\xbb\xe5\x8f\x96\xe6\x9c\x80\xe6\x97\xa9\xe8\xbf\x9b\xe5\x8e\xbb\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaa\xef\xbc\x8c\xe4\xba\x8e\xe6\x98\xaf\xe5\xb0\xb1\xe5\x85\x88\xe8\xbf\x9b\xe5\x85\x88\xe5\x87\xba\xe4\xba\x86\xe3\x80\x82 \xe8\xbf\x99\xe4\xba\x9b\xe5\xae\xb9\xe5\x99\xa8\xe7\xb1\xbb\xe9\x83\xbd\xe4\xbc\x9a\xe4\xbd\x9c\xe4\xb8\xbascheduler\xe8\xb0\x83\xe5\xba\xa6request\xe7\x9a\x84\xe5\xae\xb9\xe5\x99\xa8\xef\xbc\x8cscheduler\xe5\x9c\xa8\xe6\xaf\x8f\xe4\xb8\xaa\xe4\xb8\xbb\xe6\x9c\xba\xe4\xb8\x8a\xe9\x83\xbd\xe4\xbc\x9a\xe5\xae\x9e\xe4\xbe\x8b\xe5\x8c\x96\xe4\xb8\x80\xe4\xb8\xaa\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe5\x92\x8cspider\xe4\xb8\x80\xe4\xb8\x80\xe5\xaf\xb9\xe5\xba\x94\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe4\xbc\x9a\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaaspider\xe7\x9a\x84\xe5\xa4\x9a\xe4\xb8\xaa\xe5\xae\x9e\xe4\xbe\x8b\xe5\x92\x8c\xe4\xb8\x80\xe4\xb8\xaascheduler\xe7\x9a\x84\xe5\xa4\x9a\xe4\xb8\xaa\xe5\xae\x9e\xe4\xbe\x8b\xe5\xad\x98\xe5\x9c\xa8\xe4\xba\x8e\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe4\xb8\xbb\xe6\x9c\xba\xe4\xb8\x8a\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xbascheduler\xe9\x83\xbd\xe6\x98\xaf\xe7\x94\xa8\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe5\xae\xb9\xe5\x99\xa8\xef\xbc\x8c\xe8\x80\x8c\xe8\xbf\x99\xe4\xba\x9b\xe5\xae\xb9\xe5\x99\xa8\xe9\x83\xbd\xe8\xbf\x9e\xe6\x8e\xa5\xe5\x90\x8c\xe4\xb8\x80\xe4\xb8\xaaredis\xe6\x9c\x8d\xe5\x8a\xa1\xe5\x99\xa8\xef\xbc\x8c\xe5\x8f\x88\xe9\x83\xbd\xe4\xbd\xbf\xe7\x94\xa8spider\xe5\x90\x8d\xe5\x8a\xa0queue\xe6\x9d\xa5\xe4\xbd\x9c\xe4\xb8\xbakey\xe8\xaf\xbb\xe5\x86\x99\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xb8\x8d\xe5\x90\x8c\xe4\xb8\xbb\xe6\x9c\xba\xe4\xb8\x8a\xe7\x9a\x84\xe4\xb8\x8d\xe5\x90\x8c\xe7\x88\xac\xe8\x99\xab\xe5\xae\x9e\xe4\xbe\x8b\xe5\x85\xb1\xe7\x94\xa8\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaarequest\xe8\xb0\x83\xe5\xba\xa6\xe6\xb1\xa0\xef\xbc\x8c\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe7\x88\xac\xe8\x99\xab\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe7\xbb\x9f\xe4\xb8\x80\xe8\xb0\x83\xe5\xba\xa6\xe3\x80\x82\r\n\r\nfrom scrapy.utils.reqser import request_to_dict, request_from_dict\r\nfrom . import picklecompat\r\n\r\n\r\nclass Base(object):\r\n    """"""Per-spider base queue class""""""\r\n\r\n    def __init__(self, server, spider, key, serializer=None):\r\n        """"""Initialize per-spider redis queue.\r\n\r\n        Parameters\r\n        ----------\r\n        server : StrictRedis\r\n            Redis client instance.\r\n        spider : Spider\r\n            Scrapy spider instance.\r\n        key: str\r\n            Redis key where to put and get messages.\r\n        serializer : object\r\n            Serializer object with ``loads`` and ``dumps`` methods.\r\n\r\n        """"""\r\n        if serializer is None:\r\n            # Backward compatibility.\r\n            # TODO: deprecate pickle.\r\n            serializer = picklecompat\r\n        if not hasattr(serializer, \'loads\'):\r\n            raise TypeError(""serializer does not implement \'loads\' function: %r""\r\n                            % serializer)\r\n        if not hasattr(serializer, \'dumps\'):\r\n            raise TypeError(""serializer \'%s\' does not implement \'dumps\' function: %r""\r\n                            % serializer)\r\n\r\n        self.server = server\r\n        self.spider = spider\r\n        self.key = key % {\'spider\': spider.name}\r\n        self.serializer = serializer\r\n\r\n    def _encode_request(self, request):\r\n        """"""Encode a request object""""""\r\n        obj = request_to_dict(request, self.spider)\r\n        return self.serializer.dumps(obj)\r\n\r\n    def _decode_request(self, encoded_request):\r\n        """"""Decode an request previously encoded""""""\r\n        obj = self.serializer.loads(encoded_request)\r\n        return request_from_dict(obj, self.spider)\r\n\r\n    def __len__(self):\r\n        """"""Return the length of the queue""""""\r\n        raise NotImplementedError\r\n\r\n    def push(self, request):\r\n        """"""Push a request""""""\r\n        raise NotImplementedError\r\n\r\n    def pop(self, timeout=0):\r\n        """"""Pop a request""""""\r\n        raise NotImplementedError\r\n\r\n    def clear(self):\r\n        """"""Clear queue/stack""""""\r\n        self.server.delete(self.key)\r\n\r\n\r\nclass FifoQueue(Base):\r\n    """"""Per-spider FIFO queue""""""\r\n\r\n    def __len__(self):\r\n        """"""Return the length of the queue""""""\r\n        return self.server.llen(self.key)\r\n\r\n    def push(self, request):\r\n        """"""Push a request""""""\r\n        self.server.lpush(self.key, self._encode_request(request))\r\n\r\n    def pop(self, timeout=0):\r\n        """"""Pop a request""""""\r\n        if timeout > 0:\r\n            data = self.server.brpop(self.key, timeout)\r\n            if isinstance(data, tuple):\r\n                data = data[1]\r\n        else:\r\n            data = self.server.rpop(self.key)\r\n        if data:\r\n            return self._decode_request(data)\r\n\r\n\r\nclass PriorityQueue(Base):\r\n    """"""Per-spider priority queue abstraction using redis\' sorted set""""""\r\n\r\n    def __len__(self):\r\n        """"""Return the length of the queue""""""\r\n        return self.server.zcard(self.key)\r\n\r\n    def push(self, request):\r\n        """"""Push a request""""""\r\n        data = self._encode_request(request)\r\n        score = -request.priority\r\n        # We don\'t use zadd method as the order of arguments change depending on\r\n        # whether the class is Redis or StrictRedis, and the option of using\r\n        # kwargs only accepts strings, not bytes.\r\n        self.server.execute_command(\'ZADD\', self.key, score, data)\r\n\r\n    def pop(self, timeout=0):\r\n        """"""\r\n        Pop a request\r\n        timeout not support in this queue class\r\n        """"""\r\n        # use atomic range/remove using multi/exec\r\n        pipe = self.server.pipeline()\r\n        pipe.multi()\r\n        pipe.zrange(self.key, 0, 0).zremrangebyrank(self.key, 0, 0)\r\n        results, count = pipe.execute()\r\n        if results:\r\n            return self._decode_request(results[0])\r\n\r\n\r\nclass LifoQueue(Base):\r\n    """"""Per-spider LIFO queue.""""""\r\n\r\n    def __len__(self):\r\n        """"""Return the length of the stack""""""\r\n        return self.server.llen(self.key)\r\n\r\n    def push(self, request):\r\n        """"""Push a request""""""\r\n        self.server.lpush(self.key, self._encode_request(request))\r\n\r\n    def pop(self, timeout=0):\r\n        """"""Pop a request""""""\r\n        if timeout > 0:\r\n            data = self.server.blpop(self.key, timeout)\r\n            if isinstance(data, tuple):\r\n                data = data[1]\r\n        else:\r\n            data = self.server.lpop(self.key)\r\n\r\n        if data:\r\n            return self._decode_request(data)\r\n\r\n\r\n# TODO: Deprecate the use of these names.\r\nSpiderQueue = FifoQueue\r\nSpiderStack = LifoQueue\r\nSpiderPriorityQueue = PriorityQueue'"
Spider/ch_Distributedcrawler/Scheduler.py,0,"b'\xe6\xad\xa4\xe6\x89\xa9\xe5\xb1\x95\xe6\x98\xaf\xe5\xaf\xb9scrapy\xe4\xb8\xad\xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84scheduler\xe7\x9a\x84\xe6\x9b\xbf\xe4\xbb\xa3\xef\xbc\x88\xe5\x9c\xa8settings\xe7\x9a\x84SCHEDULER\xe5\x8f\x98\xe9\x87\x8f\xe4\xb8\xad\xe6\x8c\x87\xe5\x87\xba\xef\xbc\x89\xef\xbc\x8c\xe6\xad\xa3\xe6\x98\xaf\xe5\x88\xa9\xe7\x94\xa8\xe6\xad\xa4\xe6\x89\xa9\xe5\xb1\x95\xe5\xae\x9e\xe7\x8e\xb0crawler\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe8\xb0\x83\xe5\xba\xa6\xe3\x80\x82\r\n\xe5\x85\xb6\xe5\x88\xa9\xe7\x94\xa8\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84\xe6\x9d\xa5\xe8\x87\xaa\xe4\xba\x8equeue\xe4\xb8\xad\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84\xe3\x80\x82scrapy-redis\xe6\x89\x80\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84\xe4\xb8\xa4\xe7\xa7\x8d\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xef\xbc\x9a\xe7\x88\xac\xe8\x99\xab\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe4\xbb\xa5\xe5\x8f\x8aitem\xe5\xa4\x84\xe7\x90\x86\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe5\xb0\xb1\xe6\x98\xaf\xe7\x94\xb1\xe6\xa8\xa1\xe5\x9d\x97\r\nscheduler\xe5\x92\x8c\xe6\xa8\xa1\xe5\x9d\x97pipelines\xe5\xae\x9e\xe7\x8e\xb0\xe3\x80\x82\xe4\xb8\x8a\xe8\xbf\xb0\xe5\x85\xb6\xe5\xae\x83\xe6\xa8\xa1\xe5\x9d\x97\xe4\xbd\x9c\xe4\xb8\xba\xe4\xb8\xba\xe4\xba\x8c\xe8\x80\x85\xe8\xbe\x85\xe5\x8a\xa9\xe7\x9a\x84\xe5\x8a\x9f\xe8\x83\xbd\xe6\xa8\xa1\xe5\x9d\x97.\r\n\r\nimport importlib\r\nimport six\r\nfrom scrapy.utils.misc import load_object\r\nfrom . import connection, defaults\r\n\r\n\r\n# TODO: add SCRAPY_JOB support.\r\nclass Scheduler(object):\r\n    """"""Redis-based scheduler\r\n\r\n    Settings\r\n    --------\r\n    SCHEDULER_PERSIST : bool (default: False)\r\n        Whether to persist or clear redis queue.\r\n    SCHEDULER_FLUSH_ON_START : bool (default: False)\r\n        Whether to flush redis queue on start.\r\n    SCHEDULER_IDLE_BEFORE_CLOSE : int (default: 0)\r\n        How many seconds to wait before closing if no message is received.\r\n    SCHEDULER_QUEUE_KEY : str\r\n        Scheduler redis key.\r\n    SCHEDULER_QUEUE_CLASS : str\r\n        Scheduler queue class.\r\n    SCHEDULER_DUPEFILTER_KEY : str\r\n        Scheduler dupefilter redis key.\r\n    SCHEDULER_DUPEFILTER_CLASS : str\r\n        Scheduler dupefilter class.\r\n    SCHEDULER_SERIALIZER : str\r\n        Scheduler serializer.\r\n\r\n    """"""\r\n\r\n    def __init__(self, server,\r\n                 persist=False,\r\n                 flush_on_start=False,\r\n                 queue_key=defaults.SCHEDULER_QUEUE_KEY,\r\n                 queue_cls=defaults.SCHEDULER_QUEUE_CLASS,\r\n                 dupefilter_key=defaults.SCHEDULER_DUPEFILTER_KEY,\r\n                 dupefilter_cls=defaults.SCHEDULER_DUPEFILTER_CLASS,\r\n                 idle_before_close=0,\r\n                 serializer=None):\r\n        """"""Initialize scheduler.\r\n\r\n        Parameters\r\n        ----------\r\n        server : Redis\r\n            The redis server instance.\r\n        persist : bool\r\n            Whether to flush requests when closing. Default is False.\r\n        flush_on_start : bool\r\n            Whether to flush requests on start. Default is False.\r\n        queue_key : str\r\n            Requests queue key.\r\n        queue_cls : str\r\n            Importable path to the queue class.\r\n        dupefilter_key : str\r\n            Duplicates filter key.\r\n        dupefilter_cls : str\r\n            Importable path to the dupefilter class.\r\n        idle_before_close : int\r\n            Timeout before giving up.\r\n\r\n        """"""\r\n        if idle_before_close < 0:\r\n            raise TypeError(""idle_before_close cannot be negative"")\r\n\r\n        self.server = server\r\n        self.persist = persist\r\n        self.flush_on_start = flush_on_start\r\n        self.queue_key = queue_key\r\n        self.queue_cls = queue_cls\r\n        self.dupefilter_cls = dupefilter_cls\r\n        self.dupefilter_key = dupefilter_key\r\n        self.idle_before_close = idle_before_close\r\n        self.serializer = serializer\r\n        self.stats = None\r\n\r\n    def __len__(self):\r\n        return len(self.queue)\r\n\r\n    @classmethod\r\n    def from_settings(cls, settings):\r\n        kwargs = {\r\n            \'persist\': settings.getbool(\'SCHEDULER_PERSIST\'),\r\n            \'flush_on_start\': settings.getbool(\'SCHEDULER_FLUSH_ON_START\'),\r\n            \'idle_before_close\': settings.getint(\'SCHEDULER_IDLE_BEFORE_CLOSE\'),\r\n        }\r\n\r\n        # If these values are missing, it means we want to use the defaults.\r\n        optional = {\r\n            # TODO: Use custom prefixes for this settings to note that are\r\n            # specific to scrapy-redis.\r\n            \'queue_key\': \'SCHEDULER_QUEUE_KEY\',\r\n            \'queue_cls\': \'SCHEDULER_QUEUE_CLASS\',\r\n            \'dupefilter_key\': \'SCHEDULER_DUPEFILTER_KEY\',\r\n            # We use the default setting name to keep compatibility.\r\n            \'dupefilter_cls\': \'DUPEFILTER_CLASS\',\r\n            \'serializer\': \'SCHEDULER_SERIALIZER\',\r\n        }\r\n        for name, setting_name in optional.items():\r\n            val = settings.get(setting_name)\r\n            if val:\r\n                kwargs[name] = val\r\n\r\n        # Support serializer as a path to a module.\r\n        if isinstance(kwargs.get(\'serializer\'), six.string_types):\r\n            kwargs[\'serializer\'] = importlib.import_module(kwargs[\'serializer\'])\r\n\r\n        server = connection.from_settings(settings)\r\n        # Ensure the connection is working.\r\n        server.ping()\r\n\r\n        return cls(server=server, **kwargs)\r\n\r\n    @classmethod\r\n    def from_crawler(cls, crawler):\r\n        instance = cls.from_settings(crawler.settings)\r\n        # FIXME: for now, stats are only supported from this constructor\r\n        instance.stats = crawler.stats\r\n        return instance\r\n\r\n    def open(self, spider):\r\n        self.spider = spider\r\n\r\n        try:\r\n            self.queue = load_object(self.queue_cls)(\r\n                server=self.server,\r\n                spider=spider,\r\n                key=self.queue_key % {\'spider\': spider.name},\r\n                serializer=self.serializer,\r\n            )\r\n        except TypeError as e:\r\n            raise ValueError(""Failed to instantiate queue class \'%s\': %s"",\r\n                             self.queue_cls, e)\r\n\r\n        try:\r\n            self.df = load_object(self.dupefilter_cls)(\r\n                server=self.server,\r\n                key=self.dupefilter_key % {\'spider\': spider.name},\r\n                debug=spider.settings.getbool(\'DUPEFILTER_DEBUG\'),\r\n            )\r\n        except TypeError as e:\r\n            raise ValueError(""Failed to instantiate dupefilter class \'%s\': %s"",\r\n                             self.dupefilter_cls, e)\r\n\r\n        if self.flush_on_start:\r\n            self.flush()\r\n        # notice if there are requests already in the queue to resume the crawl\r\n        if len(self.queue):\r\n            spider.log(""Resuming crawl (%d requests scheduled)"" % len(self.queue))\r\n\r\n    def close(self, reason):\r\n        if not self.persist:\r\n            self.flush()\r\n\r\n    def flush(self):\r\n        self.df.clear()\r\n        self.queue.clear()\r\n\r\n    def enqueue_request(self, request):\r\n        if not request.dont_filter and self.df.request_seen(request):\r\n            self.df.log(request, self.spider)\r\n            return False\r\n        if self.stats:\r\n            self.stats.inc_value(\'scheduler/enqueued/redis\', spider=self.spider)\r\n        self.queue.push(request)\r\n        return True\r\n\r\n    def next_request(self):\r\n        block_pop_timeout = self.idle_before_close\r\n        request = self.queue.pop(block_pop_timeout)\r\n        if request and self.stats:\r\n            self.stats.inc_value(\'scheduler/dequeued/redis\', spider=self.spider)\r\n        return request\r\n\r\n    def has_pending_requests(self):\r\n        return len(self) > 0\r\n\xe8\xbf\x99\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe9\x87\x8d\xe5\x86\x99\xe4\xba\x86scheduler\xe7\xb1\xbb\xef\xbc\x8c\xe7\x94\xa8\xe6\x9d\xa5\xe4\xbb\xa3\xe6\x9b\xbfscrapy.core.scheduler\xe7\x9a\x84\xe5\x8e\x9f\xe6\x9c\x89\xe8\xb0\x83\xe5\xba\xa6\xe5\x99\xa8\xe3\x80\x82\xe5\x85\xb6\xe5\xae\x9e\xe5\xaf\xb9\xe5\x8e\x9f\xe6\x9c\x89\xe8\xb0\x83\xe5\xba\xa6\xe5\x99\xa8\xe7\x9a\x84\xe9\x80\xbb\xe8\xbe\x91\xe6\xb2\xa1\xe6\x9c\x89\xe5\xbe\x88\xe5\xa4\xa7\xe7\x9a\x84\xe6\x94\xb9\xe5\x8f\x98\xef\xbc\x8c\r\n\xe4\xb8\xbb\xe8\xa6\x81\xe6\x98\xaf\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86redis\xe4\xbd\x9c\xe4\xb8\xba\xe6\x95\xb0\xe6\x8d\xae\xe5\xad\x98\xe5\x82\xa8\xe7\x9a\x84\xe5\xaa\x92\xe4\xbb\x8b\xef\xbc\x8c\xe4\xbb\xa5\xe8\xbe\xbe\xe5\x88\xb0\xe5\x90\x84\xe4\xb8\xaa\xe7\x88\xac\xe8\x99\xab\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe7\xbb\x9f\xe4\xb8\x80\xe8\xb0\x83\xe5\xba\xa6\xe3\x80\x82 scheduler\xe8\xb4\x9f\xe8\xb4\xa3\xe8\xb0\x83\xe5\xba\xa6\xe5\x90\x84\xe4\xb8\xaaspider\xe7\x9a\x84request\xe8\xaf\xb7\xe6\xb1\x82\xef\xbc\x8c\r\nscheduler\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x97\xb6\xef\xbc\x8c\xe9\x80\x9a\xe8\xbf\x87settings\xe6\x96\x87\xe4\xbb\xb6\xe8\xaf\xbb\xe5\x8f\x96queue\xe5\x92\x8cdupefilters\xe7\x9a\x84\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x88\xe4\xb8\x80\xe8\x88\xac\xe5\xb0\xb1\xe7\x94\xa8\xe4\xb8\x8a\xe8\xbe\xb9\xe9\xbb\x98\xe8\xae\xa4\xe7\x9a\x84\xef\xbc\x89\xef\xbc\x8c\xe9\x85\x8d\xe7\xbd\xaequeue\xe5\x92\x8cdupefilters\r\n\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84key\xef\xbc\x88\xe4\xb8\x80\xe8\x88\xac\xe5\xb0\xb1\xe6\x98\xafspider name\xe5\x8a\xa0\xe4\xb8\x8aqueue\xe6\x88\x96\xe8\x80\x85dupefilters\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe5\xaf\xb9\xe4\xba\x8e\xe5\x90\x8c\xe4\xb8\x80\xe7\xa7\x8dspider\xe7\x9a\x84\xe4\xb8\x8d\xe5\x90\x8c\xe5\xae\x9e\xe4\xbe\x8b\xef\xbc\x8c\xe5\xb0\xb1\xe4\xbc\x9a\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x9d\x97\xe4\xba\x86\xef\xbc\x89\xe3\x80\x82\r\n\xe6\xaf\x8f\xe5\xbd\x93\xe4\xb8\x80\xe4\xb8\xaarequest\xe8\xa6\x81\xe8\xa2\xab\xe8\xb0\x83\xe5\xba\xa6\xe6\x97\xb6\xef\xbc\x8cenqueue_request\xe8\xa2\xab\xe8\xb0\x83\xe7\x94\xa8\xef\xbc\x8cscheduler\xe4\xbd\xbf\xe7\x94\xa8dupefilters\xe6\x9d\xa5\xe5\x88\xa4\xe6\x96\xad\xe8\xbf\x99\xe4\xb8\xaaurl\xe6\x98\xaf\xe5\x90\xa6\xe9\x87\x8d\xe5\xa4\x8d\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe9\x87\x8d\xe5\xa4\x8d\xef\xbc\x8c\r\n\xe5\xb0\xb1\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x88\xb0queue\xe7\x9a\x84\xe5\xae\xb9\xe5\x99\xa8\xe4\xb8\xad\xef\xbc\x88\xe5\x85\x88\xe8\xbf\x9b\xe5\x85\x88\xe5\x87\xba\xef\xbc\x8c\xe5\x85\x88\xe8\xbf\x9b\xe5\x90\x8e\xe5\x87\xba\xe5\x92\x8c\xe4\xbc\x98\xe5\x85\x88\xe7\xba\xa7\xe9\x83\xbd\xe5\x8f\xaf\xe4\xbb\xa5\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x9c\xa8settings\xe4\xb8\xad\xe9\x85\x8d\xe7\xbd\xae\xef\xbc\x89\xe3\x80\x82\xe5\xbd\x93\xe8\xb0\x83\xe5\xba\xa6\xe5\xae\x8c\xe6\x88\x90\xe6\x97\xb6\xef\xbc\x8cnext_request\xe8\xa2\xab\xe8\xb0\x83\xe7\x94\xa8\xef\xbc\x8c\r\nscheduler\xe5\xb0\xb1\xe9\x80\x9a\xe8\xbf\x87queue\xe5\xae\xb9\xe5\x99\xa8\xe7\x9a\x84\xe6\x8e\xa5\xe5\x8f\xa3\xef\xbc\x8c\xe5\x8f\x96\xe5\x87\xba\xe4\xb8\x80\xe4\xb8\xaarequest\xef\xbc\x8c\xe6\x8a\x8a\xe4\xbb\x96\xe5\x8f\x91\xe9\x80\x81\xe7\xbb\x99\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84spider\xef\xbc\x8c\xe8\xae\xa9spider\xe8\xbf\x9b\xe8\xa1\x8c\xe7\x88\xac\xe5\x8f\x96\xe5\xb7\xa5\xe4\xbd\x9c\xe3\x80\x82'"
Spider/ch_Distributedcrawler/Spider.py,0,"b'\xe8\xae\xbe\xe8\xae\xa1\xe7\x9a\x84\xe8\xbf\x99\xe4\xb8\xaaspider\xe4\xbb\x8eredis\xe4\xb8\xad\xe8\xaf\xbb\xe5\x8f\x96\xe8\xa6\x81\xe7\x88\xac\xe7\x9a\x84url, \xe7\x84\xb6\xe5\x90\x8e\xe6\x89\xa7\xe8\xa1\x8c\xe7\x88\xac\xe5\x8f\x96\xef\xbc\x8c\xe8\x8b\xa5\xe7\x88\xac\xe5\x8f\x96\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe8\xbf\x94\xe5\x9b\x9e\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84url\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe7\xbb\xa7\xe7\xbb\xad\xe8\xbf\x9b\xe8\xa1\x8c\xe7\x9b\xb4\xe8\x87\xb3\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84request\xe5\xae\x8c\xe6\x88\x90\xe3\x80\x82\xe4\xb9\x8b\xe5\x90\x8e\xe7\xbb\xa7\xe7\xbb\xad\xe4\xbb\x8eredis\xe4\xb8\xad\xe8\xaf\xbb\xe5\x8f\x96url\xef\xbc\x8c\xe5\xbe\xaa\xe7\x8e\xaf\xe8\xbf\x99\xe4\xb8\xaa\xe8\xbf\x87\xe7\xa8\x8b\xe3\x80\x82\r\n\xe5\x88\x86\xe6\x9e\x90\xef\xbc\x9a\xe5\x9c\xa8\xe8\xbf\x99\xe4\xb8\xaaspider\xe4\xb8\xad\xe9\x80\x9a\xe8\xbf\x87connect signals.spider_idle\xe4\xbf\xa1\xe5\x8f\xb7\xe5\xae\x9e\xe7\x8e\xb0\xe5\xaf\xb9crawler\xe7\x8a\xb6\xe6\x80\x81\xe7\x9a\x84\xe7\x9b\x91\xe8\xa7\x86\xe3\x80\x82\xe5\xbd\x93idle\xe6\x97\xb6\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe6\x96\xb0\xe7\x9a\x84make_requests_from_url(url)\xe7\xbb\x99\xe5\xbc\x95\xe6\x93\x8e\xef\xbc\x8c\xe8\xbf\x9b\xe8\x80\x8c\xe4\xba\xa4\xe7\xbb\x99\xe8\xb0\x83\xe5\xba\xa6\xe5\x99\xa8\xe8\xb0\x83\xe5\xba\xa6\xe3\x80\x82\r\n\r\nfrom scrapy import signals\r\nfrom scrapy.exceptions import DontCloseSpider\r\nfrom scrapy.spiders import Spider, CrawlSpider\r\nfrom . import connection, defaults\r\nfrom .utils import bytes_to_str\r\n\r\n\r\nclass RedisMixin(object):\r\n    """"""Mixin class to implement reading urls from a redis queue.""""""\r\n    redis_key = None\r\n    redis_batch_size = None\r\n    redis_encoding = None\r\n\r\n    # Redis client placeholder.\r\n    server = None\r\n\r\n    def start_requests(self):\r\n        """"""Returns a batch of start requests from redis.""""""\r\n        return self.next_requests()\r\n\r\n    def setup_redis(self, crawler=None):\r\n        """"""Setup redis connection and idle signal.\r\n\r\n        This should be called after the spider has set its crawler object.\r\n        """"""\r\n        if self.server is not None:\r\n            return\r\n\r\n        if crawler is None:\r\n            # We allow optional crawler argument to keep backwards\r\n            # compatibility.\r\n            # XXX: Raise a deprecation warning.\r\n            crawler = getattr(self, \'crawler\', None)\r\n\r\n        if crawler is None:\r\n            raise ValueError(""crawler is required"")\r\n\r\n        settings = crawler.settings\r\n\r\n        if self.redis_key is None:\r\n            self.redis_key = settings.get(\r\n                \'REDIS_START_URLS_KEY\', defaults.START_URLS_KEY,\r\n            )\r\n\r\n        self.redis_key = self.redis_key % {\'name\': self.name}\r\n\r\n        if not self.redis_key.strip():\r\n            raise ValueError(""redis_key must not be empty"")\r\n\r\n        if self.redis_batch_size is None:\r\n            # TODO: Deprecate this setting (REDIS_START_URLS_BATCH_SIZE).\r\n            self.redis_batch_size = settings.getint(\r\n                \'REDIS_START_URLS_BATCH_SIZE\',\r\n                settings.getint(\'CONCURRENT_REQUESTS\'),\r\n            )\r\n\r\n        try:\r\n            self.redis_batch_size = int(self.redis_batch_size)\r\n        except (TypeError, ValueError):\r\n            raise ValueError(""redis_batch_size must be an integer"")\r\n\r\n        if self.redis_encoding is None:\r\n            self.redis_encoding = settings.get(\'REDIS_ENCODING\', defaults.REDIS_ENCODING)\r\n\r\n        self.logger.info(""Reading start URLs from redis key \'%(redis_key)s\' ""\r\n                         ""(batch size: %(redis_batch_size)s, encoding: %(redis_encoding)s"",\r\n                         self.__dict__)\r\n\r\n        self.server = connection.from_settings(crawler.settings)\r\n        # The idle signal is called when the spider has no requests left,\r\n        # that\'s when we will schedule new requests from redis queue\r\n        crawler.signals.connect(self.spider_idle, signal=signals.spider_idle)\r\n\r\n    def next_requests(self):\r\n        """"""Returns a request to be scheduled or none.""""""\r\n        use_set = self.settings.getbool(\'REDIS_START_URLS_AS_SET\', defaults.START_URLS_AS_SET)\r\n        fetch_one = self.server.spop if use_set else self.server.lpop\r\n        # XXX: Do we need to use a timeout here?\r\n        found = 0\r\n        # TODO: Use redis pipeline execution.\r\n        while found < self.redis_batch_size:\r\n            data = fetch_one(self.redis_key)\r\n            if not data:\r\n                # Queue empty.\r\n                break\r\n            req = self.make_request_from_data(data)\r\n            if req:\r\n                yield req\r\n                found += 1\r\n            else:\r\n                self.logger.debug(""Request not made from data: %r"", data)\r\n\r\n        if found:\r\n            self.logger.debug(""Read %s requests from \'%s\'"", found, self.redis_key)\r\n\r\n    def make_request_from_data(self, data):\r\n        """"""Returns a Request instance from data coming from Redis.\r\n\r\n        By default, ``data`` is an encoded URL. You can override this method to\r\n        provide your own message decoding.\r\n\r\n        Parameters\r\n        ----------\r\n        data : bytes\r\n            Message from redis.\r\n\r\n        """"""\r\n        url = bytes_to_str(data, self.redis_encoding)\r\n        return self.make_requests_from_url(url)\r\n\r\n    def schedule_next_requests(self):\r\n        """"""Schedules a request if available""""""\r\n        # TODO: While there is capacity, schedule a batch of redis requests.\r\n        for req in self.next_requests():\r\n            self.crawler.engine.crawl(req, spider=self)\r\n\r\n    def spider_idle(self):\r\n        """"""Schedules a request if available, otherwise waits.""""""\r\n        # XXX: Handle a sentinel to close the spider.\r\n        self.schedule_next_requests()\r\n        raise DontCloseSpider\r\n\r\n\r\nclass RedisSpider(RedisMixin, Spider):\r\n    """"""Spider that reads urls from redis queue when idle.\r\n\r\n    Attributes\r\n    ----------\r\n    redis_key : str (default: REDIS_START_URLS_KEY)\r\n        Redis key where to fetch start URLs from..\r\n    redis_batch_size : int (default: CONCURRENT_REQUESTS)\r\n        Number of messages to fetch from redis on each attempt.\r\n    redis_encoding : str (default: REDIS_ENCODING)\r\n        Encoding to use when decoding messages from redis queue.\r\n\r\n    Settings\r\n    --------\r\n    REDIS_START_URLS_KEY : str (default: ""<spider.name>:start_urls"")\r\n        Default Redis key where to fetch start URLs from..\r\n    REDIS_START_URLS_BATCH_SIZE : int (deprecated by CONCURRENT_REQUESTS)\r\n        Default number of messages to fetch from redis on each attempt.\r\n    REDIS_START_URLS_AS_SET : bool (default: False)\r\n        Use SET operations to retrieve messages from the redis queue. If False,\r\n        the messages are retrieve using the LPOP command.\r\n    REDIS_ENCODING : str (default: ""utf-8"")\r\n        Default encoding to use when decoding messages from redis queue.\r\n\r\n    """"""\r\n\r\n    @classmethod\r\n    def from_crawler(self, crawler, *args, **kwargs):\r\n        obj = super(RedisSpider, self).from_crawler(crawler, *args, **kwargs)\r\n        obj.setup_redis(crawler)\r\n        return obj\r\n\r\n\r\nclass RedisCrawlSpider(RedisMixin, CrawlSpider):\r\n    """"""Spider that reads urls from redis queue when idle.\r\n\r\n    Attributes\r\n    ----------\r\n    redis_key : str (default: REDIS_START_URLS_KEY)\r\n        Redis key where to fetch start URLs from..\r\n    redis_batch_size : int (default: CONCURRENT_REQUESTS)\r\n        Number of messages to fetch from redis on each attempt.\r\n    redis_encoding : str (default: REDIS_ENCODING)\r\n        Encoding to use when decoding messages from redis queue.\r\n\r\n    Settings\r\n    --------\r\n    REDIS_START_URLS_KEY : str (default: ""<spider.name>:start_urls"")\r\n        Default Redis key where to fetch start URLs from..\r\n    REDIS_START_URLS_BATCH_SIZE : int (deprecated by CONCURRENT_REQUESTS)\r\n        Default number of messages to fetch from redis on each attempt.\r\n    REDIS_START_URLS_AS_SET : bool (default: True)\r\n        Use SET operations to retrieve messages from the redis queue.\r\n    REDIS_ENCODING : str (default: ""utf-8"")\r\n        Default encoding to use when decoding messages from redis queue.\r\n\r\n    """"""\r\n\r\n    @classmethod\r\n    def from_crawler(self, crawler, *args, **kwargs):\r\n        obj = super(RedisCrawlSpider, self).from_crawler(crawler, *args, **kwargs)\r\n        obj.setup_redis(crawler)\r\n        return obj\r\nspider\xe7\x9a\x84\xe6\x94\xb9\xe5\x8a\xa8\xe4\xb9\x9f\xe4\xb8\x8d\xe6\x98\xaf\xe5\xbe\x88\xe5\xa4\xa7\xef\xbc\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe6\x98\xaf\xe9\x80\x9a\xe8\xbf\x87connect\xe6\x8e\xa5\xe5\x8f\xa3\xef\xbc\x8c\xe7\xbb\x99spider\xe7\xbb\x91\xe5\xae\x9a\xe4\xba\x86spider_idle\xe4\xbf\xa1\xe5\x8f\xb7\xef\xbc\x8cspider\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x97\xb6\xef\xbc\x8c\xe9\x80\x9a\xe8\xbf\x87setup_redis\xe5\x87\xbd\xe6\x95\xb0\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\xa5\xbd\xe5\x92\x8credis\xe7\x9a\x84\xe8\xbf\x9e\xe6\x8e\xa5\xef\xbc\x8c\r\n\xe5\x90\x8e\xe9\x80\x9a\xe8\xbf\x87next_requests\xe5\x87\xbd\xe6\x95\xb0\xe4\xbb\x8eredis\xe4\xb8\xad\xe5\x8f\x96\xe5\x87\xbastrat url\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84key\xe6\x98\xafsettings\xe4\xb8\xadREDIS_START_URLS_AS_SET\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xef\xbc\x88\xe6\xb3\xa8\xe6\x84\x8f\xe4\xba\x86\xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96url\xe6\xb1\xa0\xe5\x92\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8a\xe8\xbe\xb9\xe7\x9a\x84\r\nqueue\xe7\x9a\x84url\xe6\xb1\xa0\xe4\xb8\x8d\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x9c\xe8\xa5\xbf\xef\xbc\x8cqueue\xe7\x9a\x84\xe6\xb1\xa0\xe6\x98\xaf\xe7\x94\xa8\xe4\xba\x8e\xe8\xb0\x83\xe5\xba\xa6\xe7\x9a\x84\xef\xbc\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96url\xe6\xb1\xa0\xe6\x98\xaf\xe5\xad\x98\xe6\x94\xbe\xe5\x85\xa5\xe5\x8f\xa3url\xe7\x9a\x84\xef\xbc\x8c\xe4\xbb\x96\xe4\xbb\xac\xe9\x83\xbd\xe5\xad\x98\xe5\x9c\xa8redis\xe4\xb8\xad\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84key\xe6\x9d\xa5\xe5\x8c\xba\xe5\x88\x86\xef\xbc\x8c\xe5\xb0\xb1\xe5\xbd\x93\xe6\x88\x90\xe6\x98\xaf\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe8\xa1\xa8\xe5\x90\xa7\xef\xbc\x89\xef\xbc\x8c\r\nspider\xe4\xbd\xbf\xe7\x94\xa8\xe5\xb0\x91\xe9\x87\x8f\xe7\x9a\x84start url\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8f\x91\xe5\xb1\x95\xe5\x87\xba\xe5\xbe\x88\xe5\xa4\x9a\xe6\x96\xb0\xe7\x9a\x84url\xef\xbc\x8c\xe8\xbf\x99\xe4\xba\x9burl\xe4\xbc\x9a\xe8\xbf\x9b\xe5\x85\xa5scheduler\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\xa4\xe9\x87\x8d\xe5\x92\x8c\xe8\xb0\x83\xe5\xba\xa6\xe3\x80\x82\xe7\x9b\xb4\xe5\x88\xb0spider\xe8\xb7\x91\xe5\x88\xb0\xe8\xb0\x83\xe5\xba\xa6\xe6\xb1\xa0\xe5\x86\x85\xe6\xb2\xa1\xe6\x9c\x89url\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe4\xbc\x9a\xe8\xa7\xa6\xe5\x8f\x91spider_idle\xe4\xbf\xa1\xe5\x8f\xb7\xef\xbc\x8c\r\n\xe4\xbb\x8e\xe8\x80\x8c\xe8\xa7\xa6\xe5\x8f\x91spider\xe7\x9a\x84next_requests\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\x86\x8d\xe6\xac\xa1\xe4\xbb\x8eredis\xe7\x9a\x84start url\xe6\xb1\xa0\xe4\xb8\xad\xe8\xaf\xbb\xe5\x8f\x96\xe4\xb8\x80\xe4\xba\x9burl\xe3\x80\x82\r\n'"
Spider/ch_Distributedcrawler/picklecompat.py,0,"b'\xe5\x9c\xa8python\xe4\xb8\xad\xef\xbc\x8c\xe4\xb8\x80\xe8\x88\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8pickle\xe7\xb1\xbb\xe6\x9d\xa5\xe8\xbf\x9b\xe8\xa1\x8cpython\xe5\xaf\xb9\xe8\xb1\xa1\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\xe5\x8c\x96\xef\xbc\x8c\xe8\x80\x8ccPickle\xe6\x8f\x90\xe4\xbe\x9b\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaa\xe6\x9b\xb4\xe5\xbf\xab\xe9\x80\x9f\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe6\x8e\xa5\xe5\x8f\xa3\xe3\x80\x82\r\n""""""A pickle wrapper module with protocol=-1 by default.""""""\r\n\r\ntry:\r\n    import cPickle as pickle  # PY2\r\nexcept ImportError:\r\n    import pickle\r\n\r\n\r\ndef loads(s):\r\n    return pickle.loads(s)\r\n\r\n\r\ndef dumps(obj):\r\n    return pickle.dumps(obj, protocol=-1)\r\n\xe8\xbf\x99\xe9\x87\x8c\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86loads\xe5\x92\x8cdumps\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\x85\xb6\xe5\xae\x9e\xe5\xb0\xb1\xe6\x98\xaf\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaa\xe5\xba\x8f\xe5\x88\x97\xe5\x8c\x96\xe5\x99\xa8\xe3\x80\x82\r\n\xe5\x9b\xa0\xe4\xb8\xbaredis\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xe4\xb8\x8d\xe8\x83\xbd\xe5\xad\x98\xe5\x82\xa8\xe5\xa4\x8d\xe6\x9d\x82\xe5\xaf\xb9\xe8\xb1\xa1\xef\xbc\x88key\xe9\x83\xa8\xe5\x88\x86\xe5\x8f\xaa\xe8\x83\xbd\xe6\x98\xaf\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xef\xbc\x8cvalue\xe9\x83\xa8\xe5\x88\x86\xe5\x8f\xaa\xe8\x83\xbd\xe6\x98\xaf\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xef\xbc\x8c\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe5\x88\x97\xe8\xa1\xa8\xef\xbc\x8c\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe9\x9b\x86\xe5\x90\x88\xe5\x92\x8chash\xef\xbc\x89\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe5\xad\x98\xe5\x95\xa5\xe9\x83\xbd\xe8\xa6\x81\xe5\x85\x88\xe4\xb8\xb2\xe8\xa1\x8c\xe5\x8c\x96\xe6\x88\x90\xe6\x96\x87\xe6\x9c\xac\xe6\x89\x8d\xe8\xa1\x8c\xe3\x80\x82\r\n\xe8\xbf\x99\xe9\x87\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe5\xb0\xb1\xe6\x98\xafpython\xe7\x9a\x84pickle\xe6\xa8\xa1\xe5\x9d\x97\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\xbc\xe5\xae\xb9py2\xe5\x92\x8cpy3\xe7\x9a\x84\xe4\xb8\xb2\xe8\xa1\x8c\xe5\x8c\x96\xe5\xb7\xa5\xe5\x85\xb7\xe3\x80\x82\xe8\xbf\x99\xe4\xb8\xaaserializer\xe4\xb8\xbb\xe8\xa6\x81\xe7\x94\xa8\xe4\xba\x8e\xe4\xb8\x80\xe4\xbc\x9a\xe7\x9a\x84scheduler\xe5\xad\x98reuqest\xe5\xaf\xb9\xe8\xb1\xa1\xe3\x80\x82'"
Spider/ch_Haiwang/analysis.py,0,"b'import json\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport jieba.analyse\nimport matplotlib.pyplot as plt\nfrom matplotlib import font_manager as fm\nfrom pyecharts import WordCloud, Style, Geo\n\n\ndef get_data():\n    df = pd.read_csv(""haiwang.csv"", sep="","", header=None, names=[""nickName"",""cityName"",""content"",""approve"",""reply"",""startTime"",""avatarurl"",""score""],encoding=""utf-8"")\n    print(df.columns)\n    return df\n\n\n# \xe6\xb8\x85\xe6\xb4\x97\xe6\x95\xb0\xe6\x8d\xae\ndef clean_data():\n    df = get_data()\n    has_copy = any(df.duplicated())\n    print(has_copy)\n    data_duplicated = df.duplicated().value_counts()\n    print(data_duplicated)  # \xe6\x9f\xa5\xe7\x9c\x8b\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xaf\xe9\x87\x8d\xe5\xa4\x8d\xe7\x9a\x84\n    """"""\n    >>>False    59900\n       True      4450\n       dtype: int64\n    """"""\n    data = df.drop_duplicates(keep=""first"")  # \xe5\x88\xa0\xe6\x8e\x89\xe9\x87\x8d\xe5\xa4\x8d\xe5\x80\xbc, first\xe4\xbf\x9d\xe7\x95\x99\xe6\x9c\x80\xe5\x85\x88\xe7\x9a\x84\n    data = data.reset_index(drop=True)  # \xe5\x88\xa0\xe9\x99\xa4\xe9\x83\xa8\xe5\x88\x86\xe8\xa1\x8c\xe5\x90\x8e\xe9\x87\x8d\xe7\xbd\xae\xe7\xb4\xa2\xe5\xbc\x95\n    data[""startTime""] = pd.to_datetime(data[""startTime""])  # dtype: object -->  dtype: datetime64[ns]\n    data[""content_length""] = data[""content""].apply(len)  # \xe5\xa2\x9e\xe5\x8a\xa0\xe4\xb8\x80\xe5\x88\x97\n    data = data[~data[\'nickName\'].isin(["".""])]\n    return data\n\n\n# \xe6\x9f\xa5\xe7\x9c\x8b\xe6\x95\xb0\xe6\x8d\xae\xe5\x9f\xba\xe6\x9c\xac\xe6\x83\x85\xe5\x86\xb5\ndef analysis1():\n    data = clean_data()\n    print(data.describe())\n    print(data.isnull().any())  # \xe5\x88\xa4\xe6\x96\xad\xe7\xa9\xba\xe5\x80\xbc  cityName\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xe7\xa9\xba\xe5\x80\xbc\n    print(len(data[data.nickName == "".""]))  # 38\xe4\xb8\xaanickName\xe6\x98\xaf \'.\'\n    # \xe5\x88\xa0\xe9\x99\xa4.\n    # data = data[~(data[\'nickName\'] == ""."")]\n    # \xe5\x92\x8c\xe4\xb8\x8a\xe9\x9d\xa2\xe7\xad\x89\xe4\xbb\xb7\n    data = data[~data[\'nickName\'].isin(["".""])]\n    print(data.head())\n    print(data[\'nickName\'].describe())\n    print(data[\'cityName\'].describe())\n    return data\n\n\ndef analysis2():\n    """"""\n    \xe5\x88\x86\xe6\x9e\x90\xe6\x89\x93\xe5\x88\x86score\xe6\x83\x85\xe5\x86\xb5\n    \xe9\xa5\xbc\xe7\x8a\xb6\xe5\x9b\xbe\xe5\x9f\xba\xe6\x9c\xac\xe5\x8f\x82\xe6\x95\xb0\xe9\x83\xbd\xe5\x87\x91\xe9\xbd\x90\xe4\xba\x86\n    """"""\n    data = clean_data()\n    grouped = data.groupby(by=""score"")[""nickName""].size().tail(8)\n    grouped = grouped.sort_values(ascending=False)\n    index = grouped.index\n    values = grouped.values\n\n    # \xe5\xb0\x86\xe6\xa8\xaa\xe3\x80\x81\xe7\xba\xb5\xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe4\xbf\x9d\xe8\xaf\x81\xe9\xa5\xbc\xe5\x9b\xbe\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\xad\xa3\xe5\x9c\x86\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe4\xb8\xba\xe6\xa4\xad\xe5\x9c\x86\n    plt.axes(aspect=\'equal\')\n    plt.subplots(figsize=(10, 7))  # \xe8\xae\xbe\xe7\xbd\xae\xe7\xbb\x98\xe5\x9b\xbe\xe5\x8c\xba\xe5\x9f\x9f\xe5\xa4\xa7\xe5\xb0\x8f\n    # \xe6\x8e\xa7\xe5\x88\xb6x\xe8\xbd\xb4\xe5\x92\x8cy\xe8\xbd\xb4\xe7\x9a\x84\xe8\x8c\x83\xe5\x9b\xb4\n    plt.xlim(0, 4)\n    plt.ylim(0, 4)\n    # \xe7\xbb\x98\xe5\x88\xb6\xe9\xa5\xbc\xe5\x9b\xbe\n    patches, texts, autotexts = plt.pie(x=index,   # \xe7\xbb\x98\xe5\x9b\xbe\xe6\x95\xb0\xe6\x8d\xae\n            labels=values,  # \xe6\xb7\xbb\xe5\x8a\xa0label\n            explode=[0.1, 0, 0, 0, 0, 0, 0, 0],\n            autopct=\'%.1f%%\',  # \xe8\xae\xbe\xe7\xbd\xae\xe7\x99\xbe\xe5\x88\x86\xe6\xaf\x94\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe4\xbf\x9d\xe7\x95\x99\xe4\xb8\x80\xe4\xbd\x8d\xe5\xb0\x8f\xe6\x95\xb0\n            pctdistance=1.2,  # \xe8\xae\xbe\xe7\xbd\xae\xe7\x99\xbe\xe5\x88\x86\xe6\xaf\x94\xe6\xa0\x87\xe7\xad\xbe\xe4\xb8\x8e\xe5\x9c\x86\xe5\xbf\x83\xe7\x9a\x84\xe8\xb7\x9d\xe7\xa6\xbb\n            labeldistance=0.8,  # \xe8\xae\xbe\xe7\xbd\xaevalue\xe4\xb8\x8e\xe5\x9c\x86\xe5\xbf\x83\xe7\x9a\x84\xe8\xb7\x9d\xe7\xa6\xbb\n            startangle=90,  # \xe8\xae\xbe\xe7\xbd\xae\xe9\xa5\xbc\xe5\x9b\xbe\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe8\xa7\x92\xe5\xba\xa6\n            radius=1.5,  # \xe8\xae\xbe\xe7\xbd\xae\xe9\xa5\xbc\xe5\x9b\xbe\xe7\x9a\x84\xe5\x8d\x8a\xe5\xbe\x84\n            shadow=True,  # \xe6\xb7\xbb\xe5\x8a\xa0\xe9\x98\xb4\xe5\xbd\xb1\n            counterclock=True,  # \xe6\x98\xaf\xe5\x90\xa6\xe9\x80\x86\xe6\x97\xb6\xe9\x92\x88\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba\xe9\xa1\xba\xe6\x97\xb6\xe9\x92\x88\xe6\x96\xb9\xe5\x90\x91\n            wedgeprops={\'linewidth\': 1.5, \'edgecolor\': \'green\'},  # \xe8\xae\xbe\xe7\xbd\xae\xe9\xa5\xbc\xe5\x9b\xbe\xe5\x86\x85\xe5\xa4\x96\xe8\xbe\xb9\xe7\x95\x8c\xe7\x9a\x84\xe5\xb1\x9e\xe6\x80\xa7\xe5\x80\xbc\n            textprops={\'fontsize\': 12, \'color\': \'k\'},  # \xe8\xae\xbe\xe7\xbd\xae\xe6\x96\x87\xe6\x9c\xac\xe6\xa0\x87\xe7\xad\xbe\xe7\x9a\x84\xe5\xb1\x9e\xe6\x80\xa7\xe5\x80\xbc\n            center=(1.8, 1.8),  # \xe8\xae\xbe\xe7\xbd\xae\xe9\xa5\xbc\xe5\x9b\xbe\xe7\x9a\x84\xe5\x8e\x9f\xe7\x82\xb9\n            frame=0)  # \xe6\x98\xaf\xe5\x90\xa6\xe6\x98\xbe\xe7\xa4\xba\xe9\xa5\xbc\xe5\x9b\xbe\xe7\x9a\x84\xe5\x9b\xbe\xe6\xa1\x86\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe8\xae\xbe\xe7\xbd\xae\xe6\x98\xbe\xe7\xa4\xba\n    # \xe9\x87\x8d\xe6\x96\xb0\xe8\xae\xbe\xe7\xbd\xae\xe5\xad\x97\xe4\xbd\x93\xe5\xa4\xa7\xe5\xb0\x8f\n    proptease = fm.FontProperties()\n    proptease.set_size(\'small\')\n    plt.setp(autotexts, fontproperties=proptease)\n    plt.setp(texts, fontproperties=proptease)\n\n    # \xe5\x88\xa0\xe9\x99\xa4x\xe8\xbd\xb4\xe5\x92\x8cy\xe8\xbd\xb4\xe7\x9a\x84\xe5\x88\xbb\xe5\xba\xa6\n    plt.xticks(())\n    plt.yticks(())\n    plt.legend()\n    # \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe5\xbd\xa2\n    plt.savefig(\'pie2.png\')\n    plt.show()\n\n\ndef analysis3():\n    """"""\xe5\x88\x86\xe6\x9e\x90\xe8\xaf\x84\xe8\xae\xba\xe6\x97\xb6\xe9\x97\xb4""""""\n    data = clean_data()\n    data[""hour""] = data[""startTime""].dt.hour  # \xe6\x8f\x90\xe5\x8f\x96\xe5\xb0\x8f\xe6\x97\xb6\n    data[""startTime""] = data[""startTime""].dt.date  # \xe6\x8f\x90\xe5\x8f\x96\xe6\x97\xa5\xe6\x9c\x9f\n    need_date = data[[""startTime"", ""hour""]]\n\n    def get_hour_size(data):\n        hour_data = data.groupby(by=""hour"")[""hour""].size().reset_index(name=""count"")\n        return hour_data\n\n    data = need_date.groupby(by=""startTime"").apply(get_hour_size)\n    # print(data)\n    data_reshape = data.pivot_table(index=""startTime"", columns=""hour"", values=""count"")[1:-2]\n    data = data_reshape.describe()\n    print(data)\n    data_mean = data.loc[""mean""]  # \xe5\x9d\x87\xe5\x80\xbc\n    data_std = data.loc[""std""]  # \xe6\x96\xb9\xe5\xb7\xae\n    data_min = data.loc[""min""]  # min\n    data_max = data.loc[""max""]  # max\n\n    # \xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe8\xb4\x9f\xe5\x8f\xb7\xe7\x9a\x84\xe5\xa4\x84\xe7\x90\x86\n    plt.rcParams[\'axes.unicode_minus\'] = False\n\n    plt.title(""24h count"")\n    plt.plot(data_mean.index, data_mean, color=""green"", label=""mean"")\n    plt.plot(data_std.index, data_std, color=""red"", label=""std"")\n    plt.plot(data_min.index, data_min, color=""blue"", label=""min"")\n    plt.plot(data_max.index, data_max, color=""yellow"", label=""max"")\n    plt.legend()\n    plt.xlabel(""one day time"")\n    plt.ylabel(""pub sum"")\n    plt.savefig(\'chart_line.png\')\n    plt.show()\n\n\ndef analysis4():\n    data = clean_data()\n    contents = list(data[""content""].values)\n    try:\n        jieba.analyse.set_stop_words(\'stop_words.txt\')\n        tags = jieba.analyse.extract_tags(str(contents), topK=100, withWeight=True)\n        name = []\n        value = []\n        for v, n in tags:\n            # [(\'\xe5\xa5\xbd\xe7\x9c\x8b\', 0.5783566110162118), (\'\xe7\x89\xb9\xe6\x95\x88\', 0.2966753295335903), (\'\xe4\xb8\x8d\xe9\x94\x99\', 0.22288265823188907),...]\n            name.append(v)\n            value.append(int(n * 10000))\n        wordcloud = WordCloud(width=1300, height=620)\n        wordcloud.add("""", name, value, word_size_range=[20, 100])\n        wordcloud.render()\n    except Exception as e:\n        print(e)\n\n\ndef handle(cities):\n    """"""\xe5\xa4\x84\xe7\x90\x86\xe5\x9c\xb0\xe5\x90\x8d\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe8\xa7\xa3\xe5\x86\xb3\xe5\x9d\x90\xe6\xa0\x87\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xe6\x89\xbe\xe4\xb8\x8d\xe5\x88\xb0\xe5\x9c\xb0\xe5\x90\x8d\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98""""""\n    with open(\n            \'city_coordinates.json\',\n            mode=\'r\', encoding=\'utf-8\') as f:\n        data = json.loads(f.read())  # \xe5\xb0\x86str\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbajson\n\n    # \xe5\xbe\xaa\xe7\x8e\xaf\xe5\x88\xa4\xe6\x96\xad\xe5\xa4\x84\xe7\x90\x86\n    data_new = data.copy()  # \xe6\x8b\xb7\xe8\xb4\x9d\xe6\x89\x80\xe6\x9c\x89\xe5\x9c\xb0\xe5\x90\x8d\xe6\x95\xb0\xe6\x8d\xae\n    for city in set(cities):  # \xe4\xbd\xbf\xe7\x94\xa8set\xe5\x8e\xbb\xe9\x87\x8d\n        # \xe5\xa4\x84\xe7\x90\x86\xe5\x9c\xb0\xe5\x90\x8d\xe4\xb8\xba\xe7\xa9\xba\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n        if city == \'\':\n            while city in cities:\n                cities.remove(city)\n        count = 0\n        for k in data.keys():\n            count += 1\n            if k == city:\n                break\n            if k.startswith(city):\n                # print(k, city)\n                data_new[city] = data[k]\n                break\n            if k.startswith(city[0:-1]) and len(city) >= 3:\n                data_new[city] = data[k]\n                break\n        # \xe5\xa4\x84\xe7\x90\x86\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe7\x9a\x84\xe5\x9c\xb0\xe5\x90\x8d\n        if count == len(data):\n            while city in cities:\n                cities.remove(city)\n\n    # \xe5\x86\x99\xe5\x85\xa5\xe8\xa6\x86\xe7\x9b\x96\xe5\x9d\x90\xe6\xa0\x87\xe6\x96\x87\xe4\xbb\xb6\n    with open(\n            \'city_coordinates.json\',\n            mode=\'w\', encoding=\'utf-8\') as f:\n        f.write(json.dumps(data_new, ensure_ascii=False))  # \xe5\xb0\x86json\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbastr\n\n\ndef analysis5():\n    data = clean_data()\n    cities = list(data[~data[""cityName""].isnull()][""cityName""].values)\n    handle(cities)\n\n    style = Style(\n        title_color=\'#fff\',\n        title_pos=\'center right\',\n        width=1200,\n        height=600,\n        background_color=\'#404a59\'\n    )\n\n    new_cities = Counter(cities).most_common(100)\n    geo = Geo(""\xe3\x80\x8a\xe6\xb5\xb7\xe7\x8e\x8b\xe3\x80\x8b\xe7\xb2\x89\xe4\xb8\x9d\xe5\x88\x86\xe5\xb8\x83"", ""\xe6\x95\xb0\xe6\x8d\xae\xe6\x9d\xa5\xe6\xba\x90\xef\xbc\x9aGithub-fenglei110"", **style.init_style)\n    attr, value = geo.cast(new_cities)\n    geo.add(\'\', attr, value, visual_range=[0, 3000], visual_text_color=\'#fff\', symbol_size=15,is_visualmap=True, is_piecewise=True, visual_split_number=10)\n    geo.render(\'\xe7\xb2\x89\xe4\xb8\x9d\xe4\xbd\x8d\xe7\xbd\xae\xe5\x88\x86\xe5\xb8\x83-GEO.html\')\n\n\nif __name__ == \'__main__\':\n    analysis3()\n\n'"
Spider/ch_Haiwang/Spider/__init__.py,0,b''
Spider/ch_Haiwang/Spider/run.py,0,"b'from scrapy import cmdline\n\ncmdline.execute(""scrapy crawl Haiwang"".split())\n\n'"
Spider/ch_Haiwang/Spider/Spider/__init__.py,0,b''
Spider/ch_Haiwang/Spider/Spider/items.py,0,b'# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# https://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass SpiderItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    pass\n\n\nclass HaiwangItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    nickName = scrapy.Field()  # \xe6\x98\xb5\xe7\xa7\xb0\n    cityName = scrapy.Field()  # \xe5\x9f\x8e\xe5\xb8\x82\n    content = scrapy.Field()  # \xe8\xaf\x84\xe8\xae\xba\n    score = scrapy.Field()  # \xe8\xaf\x84\xe5\x88\x86\n    startTime = scrapy.Field()  # \xe8\xaf\x84\xe8\xae\xba\xe6\x97\xb6\xe9\x97\xb4\n    approve = scrapy.Field()  #\n    reply = scrapy.Field()  # reply num\n    avatarurl = scrapy.Field()  # image url\n'
Spider/ch_Haiwang/Spider/Spider/middlewares.py,0,"b'# -*- coding: utf-8 -*-\n\n# Define here the models for your spider middleware\n#\n# See documentation in:\n# https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n\nfrom scrapy import signals\nfrom fake_useragent import UserAgent\nfrom .tools.xici_ip import GetIp\n\n\nclass RandomUserAgentMiddleware(object):\n    # \xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\x87\xe6\x8d\xa2user-agent\n    def __init__(self, crawler):\n        super(RandomUserAgentMiddleware, self).__init__()\n        self.ua = UserAgent()\n        self.ua_type = crawler.settings.get(""RANDOM_UA_TYPE"", ""random"")\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler)\n\n    def process_request(self, request, spider):\n        def get_ua():\n            return getattr(self.ua, self.ua_type)\n        request.headers.setdefault(\'User-Agent\', get_ua())\n\n\nclass RandomProxyMiddleware(object):\n    # \xe5\x8a\xa8\xe6\x80\x81ip\xe8\xae\xbe\xe7\xbd\xae\n    def process_request(self, request, spider):\n        get_ip = GetIp()\n        request.meta[\'proxy\'] = get_ip.get_random_ip()\n\n\nclass SpiderSpiderMiddleware(object):\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the spider middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_spider_input(self, response, spider):\n        # Called for each response that goes through the spider\n        # middleware and into the spider.\n\n        # Should return None or raise an exception.\n        return None\n\n    def process_spider_output(self, response, result, spider):\n        # Called with the results returned from the Spider, after\n        # it has processed the response.\n\n        # Must return an iterable of Request, dict or Item objects.\n        for i in result:\n            yield i\n\n    def process_spider_exception(self, response, exception, spider):\n        # Called when a spider or process_spider_input() method\n        # (from other spider middleware) raises an exception.\n\n        # Should return either None or an iterable of Response, dict\n        # or Item objects.\n        pass\n\n    def process_start_requests(self, start_requests, spider):\n        # Called with the start requests of the spider, and works\n        # similarly to the process_spider_output() method, except\n        # that it doesn\xe2\x80\x99t have a response associated.\n\n        # Must return only requests (not items).\n        for r in start_requests:\n            yield r\n\n    def spider_opened(self, spider):\n        spider.logger.info(\'Spider opened: %s\' % spider.name)\n\n\nclass SpiderDownloaderMiddleware(object):\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the downloader middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_request(self, request, spider):\n        # Called for each request that goes through the downloader\n        # middleware.\n\n        # Must either:\n        # - return None: continue processing this request\n        # - or return a Response object\n        # - or return a Request object\n        # - or raise IgnoreRequest: process_exception() methods of\n        #   installed downloader middleware will be called\n        return None\n\n    def process_response(self, request, response, spider):\n        # Called with the response returned from the downloader.\n\n        # Must either;\n        # - return a Response object\n        # - return a Request object\n        # - or raise IgnoreRequest\n        return response\n\n    def process_exception(self, request, exception, spider):\n        # Called when a download handler or a process_request()\n        # (from other downloader middleware) raises an exception.\n\n        # Must either:\n        # - return None: continue processing this exception\n        # - return a Response object: stops process_exception() chain\n        # - return a Request object: stops process_exception() chain\n        pass\n\n    def spider_opened(self, spider):\n        spider.logger.info(\'Spider opened: %s\' % spider.name)\n'"
Spider/ch_Haiwang/Spider/Spider/pipelines.py,0,"b'# -*- coding: utf-8 -*-\n\n# Define your item pipelines here\n#\n# Don\'t forget to add your pipeline to the ITEM_PIPELINES setting\n# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html\nimport csv\nimport os\nimport pymongo\nfrom Spider.settings import MONGO_DBNAME, MONGO_HOST, MONGO_PORT, MONGO_SHEETNAME\n\n\nclass SpiderPipeline(object):\n    def process_item(self, item, spider):\n        return item\n\n\nclass HaiwangPipeline(object):\n    def __init__(self):\n        store_file = os.path.dirname(__file__) + \'/spiders/haiwang.csv\'\n        print(store_file)\n        self.file = open(store_file, ""a+"", newline="""", encoding=""utf-8"")\n        self.writer = csv.writer(self.file)\n\n    def process_item(self, item, spider):\n        try:\n            self.writer.writerow((\n                item[""nickName""],\n                item[""cityName""],\n                item[""content""],\n                item[""approve""],\n                item[""reply""],\n                item[""startTime""],\n                item[""avatarurl""],\n                item[""score""]\n            ))\n\n        except Exception as e:\n            print(e.args)\n\n    def close_spider(self, spider):\n        self.file.close()\n\n\nclass MongoPipline(object):\n    def __init__(self):\n        host = MONGO_HOST\n        port = MONGO_PORT\n        dbname = MONGO_DBNAME\n        sheetname = MONGO_SHEETNAME\n\n        client = pymongo.MongoClient(host=host, port=port)\n        # \xe5\xbe\x97\xe5\x88\xb0\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xe5\xaf\xb9\xe8\xb1\xa1\n        mydb = client[dbname]\n        # \xe5\xbe\x97\xe5\x88\xb0\xe8\xa1\xa8\xe5\xaf\xb9\xe8\xb1\xa1\n        self.table = mydb[sheetname]\n\n    def process_item(self, item, spider):\n        dict_item = dict(item)\n        self.table.insert(dict_item)\n        return item\n\n    def close_spider(self, spider):\n        print(""close spider...."")'"
Spider/ch_Haiwang/Spider/Spider/settings.py,0,"b'# -*- coding: utf-8 -*-\n\n# Scrapy settings for Spider project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://doc.scrapy.org/en/latest/topics/settings.html\n#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = \'Spider\'\n\nSPIDER_MODULES = [\'Spider.spiders\']\nNEWSPIDER_MODULE = \'Spider.spiders\'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n# USER_AGENT = \'Spider (+http://www.yourdomain.com)\'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = False\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n# CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\nDOWNLOAD_DELAY = 1\n# The download delay setting will honor only one of:\n# CONCURRENT_REQUESTS_PER_DOMAIN = 16\n# CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\nCOOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n# TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\nDEFAULT_REQUEST_HEADERS = {\n    ""Referer"": ""http://m.maoyan.com/movie/249342/comments?_v_=yes"",\n    ""User-Agent"": ""Mozilla/5.0 Chrome/63.0.3239.26 Mobile Safari/537.36"",\n    ""X-Requested-With"": ""superagent""\n}\n\n# fake_useragent\xe6\x8f\x92\xe4\xbb\xb6\xe7\xbb\xb4\xe6\x8a\xa4\xe4\xba\x86\xe5\xa4\xa7\xe9\x87\x8f\xe7\x9a\x84user-agent, \xe5\x8f\xaf\xe4\xbb\xa5\xe8\x87\xaa\xe8\xa1\x8c\xe9\x80\x89\xe6\x8b\xa9ie/Firefox/Chrome...\nRANDOM_UA_TYPE = ""random""\n\n# Enable or disable spider middlewares\n# See https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n# SPIDER_MIDDLEWARES = {\n#    \'Spider.middlewares.SpiderSpiderMiddleware\': 543,\n# }\n\n# Enable or disable downloader middlewares\n# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\nDOWNLOADER_MIDDLEWARES = {\n   # \'Spider.middlewares.SpiderDownloaderMiddleware\': 543,\n   # \xe5\x8a\xa0\xe5\x85\xa5\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xad\xe9\x97\xb4\xe5\xbb\xba\n   \'Spider.middlewares.RandomUserAgentMiddleware\': 543,\n   \'Spider.middlewares.RandomProxyMiddleware\': 544,\n   # \xe5\x85\xb3\xe9\x97\xad\xe5\x86\x85\xe7\xbd\xaeUserAgentMiddleware\n   \'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware\': None\n}\n\n# Enable or disable extensions\n# See https://doc.scrapy.org/en/latest/topics/extensions.html\n# EXTENSIONS = {\n#    \'scrapy.extensions.telnet.TelnetConsole\': None,\n# }\n\n# Configure item pipelines\n# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html\nITEM_PIPELINES = {\n   # \'Spider.pipelines.SpiderPipeline\': 300,\n   \'Spider.pipelines.MongoPipline\': 299,\n   \'Spider.pipelines.HaiwangPipeline\': 301,\n}\nMONGO_HOST = \'127.0.0.1\'  # ip\nMONGO_PORT = 27017  # port\nMONGO_DBNAME = \'movie\'  # db name\nMONGO_SHEETNAME = \'Haiwang\'  # table name\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://doc.scrapy.org/en/latest/topics/autothrottle.html\n# AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n# AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n# AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n# AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n# AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n# HTTPCACHE_ENABLED = True\n# HTTPCACHE_EXPIRATION_SECS = 0\n# HTTPCACHE_DIR = \'httpcache\'\n# HTTPCACHE_IGNORE_HTTP_CODES = []\n# HTTPCACHE_STORAGE = \'scrapy.extensions.httpcache.FilesystemCacheStorage\'\n'"
Spider/ch_Haiwang/Spider/Spider/spiders/Haiwang.py,0,"b'# -*- coding: utf-8 -*-\nimport scrapy\nimport json\n\nfrom Spider.items import HaiwangItem\n\n\nclass HaiwangSpider(scrapy.Spider):\n    name = \'Haiwang\'\n    allowed_domains = [\'m.maoyan.com\']\n    start_urls = [\'http://m.maoyan.com/mmdb/comments/movie/249342.json?_v_=yes&offset=0&startTime=0\']\n\n    def parse(self, response):\n        print(response.url)\n        data = json.loads(response.text)\n        print(data)\n        item = HaiwangItem()\n        for info in data[""cmts""]:\n            item[""nickName""] = info[""nickName""]\n            item[""cityName""] = info[""cityName""] if ""cityName"" in info else """"\n            item[""content""] = info[""content""]\n            item[""score""] = info[""score""]\n            item[""startTime""] = info[""startTime""]\n            item[""approve""] = info[""approve""]\n            item[""reply""] = info[""reply""]\n            item[""avatarurl""] = info[""avatarurl""]\n            print(item)\n            yield item\n\n        yield scrapy.Request(""http://m.maoyan.com/mmdb/comments/movie/249342.json?_v_=yes&offset=0&startTime={}"".\n                             format(item[""startTime""]), callback=self.parse)\n\n\n'"
Spider/ch_Haiwang/Spider/Spider/spiders/__init__.py,0,b'# This package will contain the spiders of your Scrapy project\n#\n# Please refer to the documentation for information on how to create and manage\n# your spiders.\n'
Spider/ch_Haiwang/Spider/Spider/tools/__init__.py,0,b''
Spider/ch_Haiwang/Spider/Spider/tools/xici_ip.py,0,"b'import requests\nfrom scrapy.selector import Selector\nimport pymysql\n\nconn = pymysql.connect(host=""127.0.0.1"", user=""root"", passwd=""root"", db=""ip_spider"", charset=""utf-8"")\ncursor = conn.cursor()\n\n\ndef crawl_ips():\n    # \xe7\x88\xac\xe5\x8f\x96\xe8\xa5\xbf\xe5\x88\xbaip\xe4\xbb\xa3\xe7\x90\x86\n    headers = {""User-Agent"": ""Mozilla/5.0 Chrome/63.0.3239.26 Mobile Safari/537.36""}\n    for i in range(1568):\n        re = requests.get(""http://www.xicidaili.com/nn/{}"".format(i), headers=headers)\n\n        selector = Selector(text=re.text)\n        all_trs = selector.css(""#ip_list tr"")\n        ip_list = []\n        speed = None\n        for tr in all_trs[1:]:\n            speed_str = tr.css("".bar::attr(title)"").extract()[0]\n            if speed_str:\n                speed = float(speed_str.split(""\xe7\xa7\x92"")[0])\n            all_texts = tr.css(""td::text"").extract()\n\n            ip = all_texts[0]\n            port = all_texts[1]\n            proxy_type = all_texts[5]\n            ip_list.append((ip, port, proxy_type, speed))\n\n        for ip_info in ip_list:\n            cursor.execute(\n                ""insert proxy_ip(ip, port, speed, proxy_type) VALUES(\'{0}\', \'{1}\', {2}, \'HTTP\')"".format(\n                    ip_info[0], ip_info[1], ip_info[3]\n                )\n            )\n            conn.commit()\n\n\nclass GetIp(object):\n    def judge_ip(self, ip, port):\n        http_url = ""http://baidu.com""\n        proxy_url = ""http://{0}:{1}"".format(ip, port)\n        try:\n            proxy_dict = {\n                ""http:"": proxy_url\n            }\n            response = requests.get(http_url, proxies=proxy_dict)\n\n        except Exception as e:\n            print(""invalid ip and port"")\n            self.delete_ip(ip)\n            return False\n        else:\n            code = response.status_code\n            if 300 > code >= 200:\n                print(""effective ip"")\n                return True\n            else:\n                print(""invalid ip and port"")\n                self.delete_ip(ip)\n\n    def delete_ip(self, ip):\n        # mysql\xe4\xb8\xad\xe5\x88\xa0\xe9\x99\xa4\xe6\x97\xa0\xe6\x95\x88\xe7\x9a\x84ip\n        sql = """"""delete from proxy_ip where ip=\'{0}\'"""""".format(ip)\n        cursor.execute(sql)\n        conn.commit()\n        return True\n\n    def get_random_ip(self):\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe5\x8f\x96\xe5\x87\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\xaf\xe7\x94\xa8ip\n        sql = """"""SELECT ip, port FROM proxy_ip ORDER BY RAND() LIMIT 1""""""\n        result = cursor.execute(sql)\n        for ip_info in cursor.fetchall():\n            ip = ip_info[0]\n            port = ip_info[1]\n            judge_re = self.judge_ip(ip, port)\n            if judge_re:\n                return ""http://{0}:{1}"".format(ip, port)\n            else:\n                return self.get_random_ip()\n\n\nif __name__ == \'__main__\':\n    IP = GetIp()\n    IP.get_random_ip()'"
