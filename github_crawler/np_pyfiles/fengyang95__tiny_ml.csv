file_path,api_count,code
tinyml/__init__.py,0,b''
tinyml/bayes/NaiveBayesClassifier.py,19,"b""import numpy as np\n# \xe5\x8f\xaa\xe8\x80\x83\xe8\x99\x91\xe7\xa6\xbb\xe6\x95\xa3\xe5\x80\xbc\nclass NaiveBayesClassifier:\n    def __init__(self,n_classes=2):\n        self.n_classes=n_classes\n        self.priori_P={}\n        self.conditional_P={}\n        self.N={}\n        pass\n\n    def fit(self,X,y):\n        for i in range(self.n_classes):\n            # \xe5\x85\xac\xe5\xbc\x8f 7.19\n            self.priori_P[i]=(len(y[y==i])+1)/(len(y)+self.n_classes)\n        for col in range(X.shape[1]):\n            self.N[col]=len(np.unique(X[:,col]))\n            self.conditional_P[col]={}\n            for row in range(X.shape[0]):\n                val=X[row,col]\n                if val not in self.conditional_P[col].keys():\n                    self.conditional_P[col][val]={}\n                    for i in range(self.n_classes):\n                        D_xi=np.where(X[:,col]==val)\n                        D_c=np.where(y==i)\n                        D_cxi=len(np.intersect1d(D_xi,D_c))\n                        # \xe5\x85\xac\xe5\xbc\x8f 7.20\n                        self.conditional_P[col][val][i]=(D_cxi+1)/(len(y[y==i])+self.N[col])\n                else:\n                    continue\n\n    def predict(self,X):\n        pred_y=[]\n        for i in range(len(X)):\n            p=np.ones((self.n_classes,))\n            for j in range(self.n_classes):\n                p[j]=self.priori_P[j]\n            for col in range(X.shape[1]):\n                val=X[i,col]\n                for j in range(self.n_classes):\n                    p[j]*=self.conditional_P[col][val][j]\n            pred_y.append(np.argmax(p))\n        return np.array(pred_y)\n# \xe8\xbf\x9e\xe7\xbb\xad\xe5\x80\xbc\nclass NaiveBayesClassifierContinuous:\n    def __init__(self,n_classes=2):\n        self.n_classes=n_classes\n        self.priori_P={}\n\n    def fit(self,X,y):\n        self.mus=np.zeros((self.n_classes,X.shape[1]))\n        self.sigmas=np.zeros((self.n_classes,X.shape[1]))\n\n        for c in range(self.n_classes):\n            # \xe5\x85\xac\xe5\xbc\x8f 7.19\n            self.priori_P[c]=(len(y[y==c]))/(len(y))\n            X_c=X[np.where(y==c)]\n\n            self.mus[c]=np.mean(X_c,axis=0)\n            self.sigmas[c]=np.std(X_c,axis=0)\n\n    def predict(self,X):\n        pred_y=[]\n        for i in range(len(X)):\n            p=np.ones((self.n_classes,))\n            for c in range(self.n_classes):\n                p[c]=self.priori_P[c]\n                for col in range(X.shape[1]):\n                    x=X[i,col]\n                    p[c]*=1./(np.sqrt(2*np.pi)*self.sigmas[c,col])*np.exp(-(x-self.mus[c,col])**2/(2*self.sigmas[c,col]**2))\n            pred_y.append(np.argmax(p))\n        return np.array(pred_y)\n\nif __name__=='__main__':\n    X = np.array([[0, 0, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0],\n                                [1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0],\n                                [2, 0, 0, 0, 0, 0], [0, 1, 0, 0, 1, 1],\n                                [1, 1, 0, 1, 1, 1], [1, 1, 0, 0, 1, 0],\n                                [1, 1, 1, 1, 1, 0], [0, 2, 2, 0, 2, 1],\n                                [2, 2, 2, 2, 2, 0], [2, 0, 0, 2, 2, 1],\n                                [0, 1, 0, 1, 0, 0], [2, 1, 1, 1, 0, 0],\n                                [1, 1, 0, 0, 1, 1], [2, 0, 0, 2, 2, 0],\n                                [0, 0, 1, 1, 1, 0]])\n    y = np.array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n    X_test=np.array([[0, 0, 1, 0, 0, 0], [1, 0, 1, 0, 0, 0],\n                    [1, 1, 0, 1, 1, 0], [1, 0, 1, 1, 1, 0],\n                     [1, 1, 0, 0, 1, 1], [2, 0, 0, 2, 2, 0],\n                     [0, 0, 1, 1, 1, 0],\n                     [2, 0, 0, 2, 2, 0],\n                     [0, 0, 1, 1, 1, 0]\n                     ])\n\n    naive_bayes=NaiveBayesClassifier(n_classes=2)\n    naive_bayes.fit(X,y)\n    print('self.PrirP:',naive_bayes.priori_P)\n    print('self.CondiP:',naive_bayes.conditional_P)\n    pred_y=naive_bayes.predict(X_test)\n    print('pred_y:',pred_y)\n\n\n"""
tinyml/bayes/__init__.py,0,b''
tinyml/cluster/AGNES.py,12,"b""import numpy as np\nimport matplotlib.pyplot as plt\n\nclass AGNES:\n    def __init__(self,k=3,dist_type='AVG'):\n        self.k=k\n        self.labels_=None\n        self.C={}\n        self.dist_func=None\n        if dist_type=='MIN':\n            self.dist_func=self.mindist\n        elif dist_type=='MAX':\n            self.dist_func=self.maxdist\n        else:\n            self.dist_func=self.avgdist\n\n    # p215 \xe5\x9b\xbe9.11 AGNES\xe7\xae\x97\xe6\xb3\x95\n    def fit(self,X):\n        for j in range(X.shape[0]):\n            self.C[j]=set()\n            self.C[j].add(j)\n        M=1e10*np.ones((X.shape[0],X.shape[0]),dtype=np.float32)\n        for i in range(X.shape[0]):\n            for j in range(i+1,X.shape[0]):\n                M[i,j]=self.dist_func(X,self.C[i],self.C[j])\n                M[j,i]=M[i,j]\n        q=X.shape[0]\n        while q>self.k:\n            index=np.argmin(M)\n            i_=index//M.shape[1]\n            j_=index%M.shape[1]\n            self.C[i_]=set(self.C[i_].union(self.C[j_]))\n            #print(self.C[i_])\n            for j in range(j_+1,q):\n                self.C[j-1]=set(self.C[j])\n            del self.C[q-1]\n            M=np.delete(M,[j_],axis=0)\n            M=np.delete(M,[j_],axis=1)\n            for j in range(q-1):\n                if i_!=j:\n                    M[i_,j]=self.dist_func(X,self.C[i_],self.C[j])\n                    M[j,i_]=M[i_,j]\n            q-=1\n        self.labels_=np.zeros((X.shape[0],),dtype=np.int32)\n        for i in range(self.k):\n            self.labels_[list(self.C[i])] = i\n\n    @classmethod\n    def mindist(cls,X,Ci,Cj):\n        Xi=X[list(Ci)]\n        Xj=X[list(Cj)]\n        min=1e10\n        for i in range(len(Xi)):\n            d=np.sqrt(np.sum((Xi[i]-Xj)**2,axis=1))\n            dmin=np.min(d)\n            if dmin<min:\n                min=dmin\n        return min\n\n    @classmethod\n    def maxdist(cls,X,Ci,Cj):\n        Xi=X[list(Ci)]\n        Xj=X[list(Cj)]\n        max=0\n        for i in range(len(Xi)):\n            d=np.sqrt(np.sum((Xi[i]-Xj)**2,axis=1))\n            dmax=np.max(d)\n            if dmax>max:\n                max=dmax\n        return max\n\n    @classmethod\n    def avgdist(cls,X,Ci,Cj):\n        Xi=X[list(Ci)]\n        Xj=X[list(Cj)]\n        sum=0.\n        for i in range(len(Xi)):\n            d=np.sqrt(np.sum((Xi[i]-Xj)**2,axis=1))\n            sum+=np.sum(d)\n        dist=sum/(len(Ci)*len(Cj))\n        return dist\n\n\n\n\nif __name__=='__main__':\n    X=np.array([[0.697,0.460],[0.774,0.376],[0.634,0.264],[0.608,0.318],[0.556,0.215],\n                [0.403,0.237],[0.481,0.149],[0.437,0.211],[0.666,0.091],[0.243,0.267],\n                [0.245,0.057],[0.343,0.099],[0.639,0.161],[0.657,0.198],[0.360,0.370],\n                [0.593,0.042],[0.719,0.103],[0.359,0.188],[0.339,0.241],[0.282,0.257],\n                [0.748,0.232],[0.714,0.346],[0.483,0.312],[0.478,0.437],[0.525,0.369],\n                [0.751,0.489],[0.532,0.472],[0.473,0.376],[0.725,0.445],[0.446,0.459]])\n\n    X_test=X\n    agnes=AGNES()\n    agnes.fit(X)\n    print('C:', agnes.C)\n    print(agnes.labels_)\n    plt.figure(12)\n    plt.subplot(121)\n    plt.scatter(X[:, 0], X[:, 1], c=agnes.labels_)\n    plt.title('tinyml')\n\n    from sklearn.cluster.hierarchical import AgglomerativeClustering\n    sklearn_agnes=AgglomerativeClustering(n_clusters=7,affinity='l2',linkage='average')\n    sklearn_agnes.fit(X)\n    print(sklearn_agnes.labels_)\n    plt.subplot(122)\n    plt.scatter(X[:,0],X[:,1],c=sklearn_agnes.labels_)\n    plt.title('sklearn')\n    plt.show()\n\n\n\n\n\n"""
tinyml/cluster/DBSCAN.py,4,"b""import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom queue import Queue\nrandom.seed(1)\n\nclass DBSCAN:\n    def __init__(self,epsilon=0.11,min_pts=5):\n        self.epsilon=epsilon\n        self.min_pts=min_pts\n        self.labels_=None\n        self.C=None\n        self.Omega=set()\n        self.N_epsilon={}\n\n    # p213 \xe5\x9b\xbe9.9 DBSCAN\xe7\xae\x97\xe6\xb3\x95\n    def fit(self,X):\n        self.C={}\n        for j in range(X.shape[0]):\n            dist=np.sqrt(np.sum((X-X[j])**2,axis=1))\n            self.N_epsilon[j]=np.where(dist<=self.epsilon)[0]\n            if len(self.N_epsilon[j])>=self.min_pts:\n                self.Omega.add(j)\n        self.k=0\n        Gamma=set(range(X.shape[0]))\n        while len(self.Omega)>0:\n            Gamma_old=set(Gamma)\n            o=random.sample(list(self.Omega),1)[0]\n            Q=Queue()\n            Q.put(o)\n            Gamma.remove(o)\n            while not Q.empty():\n                q=Q.get()\n                if len(self.N_epsilon[q])>=self.min_pts:\n                    Delta=set(self.N_epsilon[q]).intersection(set(Gamma))\n                    for delta in Delta:\n                        Q.put(delta)\n                        Gamma.remove(delta)\n            self.C[self.k]=Gamma_old.difference(Gamma)\n            self.Omega=self.Omega.difference(self.C[self.k])\n            self.k += 1\n        self.labels_=np.zeros((X.shape[0],),dtype=np.int32)\n        for i in range(self.k):\n            self.labels_[list(self.C[i])]=i\n\n\nif __name__=='__main__':\n    X=np.array([[0.697,0.460],[0.774,0.376],[0.634,0.264],[0.608,0.318],[0.556,0.215],\n                [0.403,0.237],[0.481,0.149],[0.437,0.211],[0.666,0.091],[0.243,0.267],\n                [0.245,0.057],[0.343,0.099],[0.639,0.161],[0.657,0.198],[0.360,0.370],\n                [0.593,0.042],[0.719,0.103],[0.359,0.188],[0.339,0.241],[0.282,0.257],\n                [0.748,0.232],[0.714,0.346],[0.483,0.312],[0.478,0.437],[0.525,0.369],\n                [0.751,0.489],[0.532,0.472],[0.473,0.376],[0.725,0.445],[0.446,0.459]])\n\n    dbscan=DBSCAN()\n    dbscan.fit(X)\n    print('C:',dbscan.C)\n    print(dbscan.labels_)\n    plt.figure(12)\n    plt.subplot(121)\n    plt.scatter(X[:,0],X[:,1],c=dbscan.labels_)\n    plt.title('tinyml')\n\n    import sklearn.cluster as cluster\n    sklearn_DBSCAN=cluster.DBSCAN(eps=0.11,min_samples=5,metric='l2')\n    sklearn_DBSCAN.fit(X)\n    print(sklearn_DBSCAN.labels_)\n    plt.subplot(122)\n    plt.scatter(X[:,0],X[:,1],c=sklearn_DBSCAN.labels_)\n    plt.title('sklearn')\n    plt.show()\n\n"""
tinyml/cluster/GaussianMixture.py,12,"b""import numpy as np\nimport matplotlib.pyplot as plt\n\nclass GaussianMixture:\n    def __init__(self,k=3,max_iter=50):\n        self.k=k\n        self.max_iter=max_iter\n        self.labels_=None\n        self.C=None\n        self.alpha=None\n        self.mu=None\n        self.cov=None\n        self.gamma=None\n        pass\n\n    # p210 \xe5\x9b\xbe9.6 \xe9\xab\x98\xe6\x96\xaf\xe6\xb7\xb7\xe5\x90\x88\xe8\x81\x9a\xe7\xb1\xbb\xe7\xae\x97\xe6\xb3\x95\n    def fit(self,X):\n        # p210\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb9\xe6\xb3\x95\n        self.alpha=np.zeros((self.k,))\n        for i in range(self.k):\n            self.alpha[i]=1./self.k\n        mu_indices=[5,21,26]\n        self.mu=X[mu_indices]\n        self.cov=np.array([[[0.1,0.],[0.0,0.1]],[[0.1,0.],[0.,0.1]],[[0.1,0.],[0.,0.1]]])\n\n        self.gamma=np.zeros((X.shape[0],self.k))\n        for _ in range(self.max_iter):\n            for j in range(X.shape[0]):\n                alpha_p=np.zeros((self.k,))\n                sum=0.\n                for i in range(self.k):\n                    alpha_p[i]=self.alpha[i]*self._p(X[j],self.mu[i],self.cov[i])\n                    sum+=alpha_p[i]\n                self.gamma[j,:]=alpha_p/sum\n\n            for i in range(self.k):\n                sum_gamma_i=np.sum(self.gamma[:,i])\n                self.mu[i]=X.T.dot(self.gamma[:,i])/sum_gamma_i\n                numerator=0.\n                for j in range(X.shape[0]):\n                    numerator+=(self.gamma[j,i]*((X[j]-self.mu[i]).reshape(-1,1).dot((X[j]-self.mu[i]).reshape(1,-1))))\n                self.cov[i]=numerator/sum_gamma_i\n                self.alpha[i]=sum_gamma_i/X.shape[0]\n        self.labels_=np.argmax(self.gamma,axis=1)\n        self.C={}\n        for i in range(self.k):\n            self.C[i]=[]\n        for j in range(len(self.labels_)):\n            self.C[self.labels_[j]].append(j)\n\n    def predict(self,X):\n        gamma = np.zeros((X.shape[0], self.k))\n        for j in range(X.shape[0]):\n            alpha_p = np.zeros((self.k,))\n            sum = 0.\n            for i in range(self.k):\n                alpha_p[i] = self.alpha[i] * self._p(X[j], self.mu[i], self.cov[i])\n                sum += alpha_p[i]\n            gamma[j, :] = alpha_p / sum\n        return np.argmax(gamma,axis=1)\n\n\n    # \xe5\x85\xac\xe5\xbc\x8f 9.28\n    @classmethod\n    def _p(cls,x,mu,cov):\n        exp_coef=-0.5*((x-mu).T.dot(np.linalg.inv(cov)).dot(x-mu))\n        p=np.exp(exp_coef)/(np.power(2*np.pi,mu.shape[0]/2)*np.sqrt(np.linalg.det(cov)))\n        return p\n\nif __name__=='__main__':\n    X=np.array([[0.697,0.460],[0.774,0.376],[0.634,0.264],[0.608,0.318],[0.556,0.215],\n                [0.403,0.237],[0.481,0.149],[0.437,0.211],[0.666,0.091],[0.243,0.267],\n                [0.245,0.057],[0.343,0.099],[0.639,0.161],[0.657,0.198],[0.360,0.370],\n                [0.593,0.042],[0.719,0.103],[0.359,0.188],[0.339,0.241],[0.282,0.257],\n                [0.748,0.232],[0.714,0.346],[0.483,0.312],[0.478,0.437],[0.525,0.369],\n                [0.751,0.489],[0.532,0.472],[0.473,0.376],[0.725,0.445],[0.446,0.459]])\n\n    X_test=X\n    gmm=GaussianMixture(k=3,max_iter=50)\n    gmm.fit(X)\n    print(gmm.C)\n    print(gmm.labels_)\n    print(gmm.predict(X_test))\n    plt.scatter(X[:, 0], X[:, 1], c=gmm.labels_)\n    plt.scatter(gmm.mu[:, 0], gmm.mu[:, 1],c=range(gmm.k), marker='+')\n    plt.title('tinyml')\n    plt.show()\n\n\n    from sklearn.mixture import GaussianMixture\n\n    sklearn_gmm = GaussianMixture(n_components=3, covariance_type='full',\n                                  max_iter=50).fit(X)\n    labels=sklearn_gmm.predict(X)\n    print(labels)\n    plt.scatter(X[:,0],X[:,1],c=labels)\n    plt.title('sklearn')\n    plt.show()\n\n\n\n\n\n"""
tinyml/cluster/KMeans.py,18,"b""import numpy as np\nimport random\nimport matplotlib.pyplot as plt\nrandom.seed(1)\n\nclass KMeans:\n    def __init__(self,k=2):\n        self.labels_=None\n        self.mu=None\n        self.k=k\n\n    def init(self,X,method='kmeans++',random_state=False):\n        if method=='kmeans++':\n            if random_state is False:\n                np.random.seed(0)\n            mus=[X[np.random.randint(0,len(X))]]\n            while len(mus)<self.k:\n                Dxs=[]\n                array_mus=np.array(mus)\n                for x in X:\n                    Dx=np.sum(np.sqrt(np.sum((x-array_mus)**2,axis=1)))\n                    Dxs.append(Dx)\n                Dxs=np.array(Dxs)\n                index=np.argmax(Dxs)\n                mus.append(X[index])\n            self.mu=np.array(mus)\n\n\n        elif method=='default':\n            self.mu = X[random.sample(range(X.shape[0]), self.k)]\n\n        else:\n            raise NotImplementedError\n\n    # p203\xe5\x9b\xbe9.2\xe7\xae\x97\xe6\xb3\x95\xe6\xb5\x81\xe7\xa8\x8b\n    def fit(self,X):\n        self.init(X,'kmeans++')\n        while True:\n            C={}\n            for i in range(self.k):\n                C[i]=[]\n            for j in range(X.shape[0]):\n                d=np.sqrt(np.sum((X[j]-self.mu)**2,axis=1))\n                lambda_j=np.argmin(d)\n                C[lambda_j].append(j)\n            mu_=np.zeros((self.k,X.shape[1]))\n            for i in range(self.k):\n                mu_[i]=np.mean(X[C[i]],axis=0)\n            if np.sum((mu_-self.mu)**2)<1e-8:\n                self.C=C\n                break\n            else:\n                self.mu=mu_\n        self.labels_=np.zeros((X.shape[0],),dtype=np.int32)\n        for i in range(self.k):\n            self.labels_[C[i]]=i\n\n    def predict(self,X):\n        preds=[]\n        for j in range(X.shape[0]):\n            d=np.zeros((self.k,))\n            for i in range(self.k):\n                d[i]=np.sqrt(np.sum((X[j]-self.mu[i])**2))\n            preds.append(np.argmin(d))\n        return np.array(preds)\n\nif __name__=='__main__':\n    # p202 \xe8\xa5\xbf\xe7\x93\x9c\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x864.0\n    X=np.array([[0.697,0.460],[0.774,0.376],[0.634,0.264],[0.608,0.318],[0.556,0.215],\n                [0.403,0.237],[0.481,0.149],[0.437,0.211],[0.666,0.091],[0.243,0.267],\n                [0.245,0.057],[0.343,0.099],[0.639,0.161],[0.657,0.198],[0.360,0.370],\n                [0.593,0.042],[0.719,0.103],[0.359,0.188],[0.339,0.241],[0.282,0.257],\n                [0.748,0.232],[0.714,0.346],[0.483,0.312],[0.478,0.437],[0.525,0.369],\n                [0.751,0.489],[0.532,0.472],[0.473,0.376],[0.725,0.445],[0.446,0.459]])\n\n    kmeans=KMeans(k=3)\n    kmeans.fit(X)\n    print(kmeans.C)\n    print(kmeans.labels_)\n    print(kmeans.predict(X))\n    plt.figure(12)\n    plt.subplot(121)\n    plt.scatter(X[:,0],X[:,1],c=kmeans.labels_)\n    plt.scatter(kmeans.mu[:,0],kmeans.mu[:,1],c=range(kmeans.k),marker='+')\n    plt.title('tinyml')\n\n    from sklearn.cluster import KMeans\n    sklearn_kmeans=KMeans(n_clusters=3)\n    sklearn_kmeans.fit(X)\n    print(sklearn_kmeans.labels_)\n    plt.subplot(122)\n    plt.scatter(X[:,0],X[:,1],c=sklearn_kmeans.labels_)\n    plt.title('sklearn')\n    plt.show()\n\n\n\n"""
tinyml/cluster/LVQ.py,14,"b'import numpy as np\nimport random\nimport matplotlib.pyplot as plt\nrandom.seed(10)\nclass LVQ:\n    def __init__(self,t,eta=0.1,max_iter=400):\n        # t[i]\xe8\xa1\xa8\xe7\xa4\xba\xe7\xac\xaci\xe4\xb8\xaa\xe5\x8e\x9f\xe5\x9e\x8b\xe5\x90\x91\xe9\x87\x8f\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\n        self.t=t\n        # p[i]\xe8\xa1\xa8\xe7\xa4\xba\xe7\xac\xaci\xe4\xb8\xaa\xe5\x8e\x9f\xe5\x9e\x8b\xe5\x90\x91\xe9\x87\x8f\xe7\x9a\x84\xe5\x80\xbc\n        self.p=None\n        self.c=len(np.unique(t))\n        self.q=len(t)\n        self.eta=eta\n        self.max_iter=max_iter\n        self.C=None\n        self.labels_=None\n\n    # p205 \xe5\x9b\xbe9.4 \xe5\xad\xa6\xe4\xb9\xa0\xe5\x90\x91\xe9\x87\x8f\xe9\x87\x8f\xe5\x8c\x96\xe7\xae\x97\xe6\xb3\x95\n    def fit(self,X,y):\n        C={}\n        for i in range(self.q):\n            C[i]=[]\n        self.p=np.zeros((len(self.t),X.shape[1]))\n\n        # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8e\x9f\xe5\x9e\x8b\xe5\x90\x91\xe9\x87\x8f \xe4\xbb\x8ep\xe7\xb1\xbb\xe5\x88\xab\xe6\xa0\x87\xe8\xae\xb0\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84X\xe4\xb8\xad\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa91\xe4\xb8\xaa\xe4\xbd\x9c\xe4\xb8\xba\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8e\x9f\xe5\x9e\x8b\xe5\x90\x91\xe9\x87\x8f\n        for i in range(self.q):\n            candidate_indices=np.where(y==self.t[i])[0]\n            target_indice=random.sample(list(candidate_indices),1)\n            self.p[i]=X[target_indice]\n        """"""\n        # \xe4\xb9\xa6\xe4\xb8\x8ap\xe7\x9a\x84\xe9\x80\x89\xe5\x8f\x96\n        indices=[4,11,17,22,28]\n        self.p=X[indices]\n        """"""\n        for _ in range(self.max_iter):\n            j=random.sample(list(range(len(y))),1)\n            d=np.sqrt(np.sum((X[j]-self.p)**2,axis=1))\n            i_=np.argmin(d)\n            old_p=self.p\n            if y[j]==t[i_]:\n                self.p[i_]=self.p[i_]+self.eta*(X[j]-self.p[i_])\n            else:\n                self.p[i_]=self.p[i_]-self.eta*(X[j]-self.p[i_])\n\n        for j in range(X.shape[0]):\n            d=np.sqrt(np.sum((X[j]-self.p)**2,axis=1))\n            i_=np.argmin(d)\n            C[i_].append(j)\n        self.C=C\n        self.labels_ = np.zeros((X.shape[0],), dtype=np.int32)\n        for i in range(self.q):\n            self.labels_[C[i]] = i\n\n\n    def predict(self,X):\n        preds_y=[]\n        for j in range(X.shape[0]):\n            d=np.sqrt(np.sum((X[j]-self.p)**2,axis=1))\n            i_=np.argmin(d)\n            preds_y.append(self.t[i_])\n        return np.array(preds_y)\n\n\nif __name__==\'__main__\':\n    X=np.array([[0.697,0.460],[0.774,0.376],[0.634,0.264],[0.608,0.318],[0.556,0.215],\n                [0.403,0.237],[0.481,0.149],[0.437,0.211],[0.666,0.091],[0.243,0.267],\n                [0.245,0.057],[0.343,0.099],[0.639,0.161],[0.657,0.198],[0.360,0.370],\n                [0.593,0.042],[0.719,0.103],[0.359,0.188],[0.339,0.241],[0.282,0.257],\n                [0.748,0.232],[0.714,0.346],[0.483,0.312],[0.478,0.437],[0.525,0.369],\n                [0.751,0.489],[0.532,0.472],[0.473,0.376],[0.725,0.445],[0.446,0.459]])\n    y=np.zeros((X.shape[0],),dtype=np.int32)\n    y[range(9,21)]=1\n    t=np.array([0,1,1,0,0],dtype=np.int32)\n\n    print(y)\n    X_test=X\n    lvq=LVQ(t)\n    lvq.fit(X,y)\n    print(lvq.C)\n    print(lvq.labels_)\n    print(lvq.predict(X))\n    plt.scatter(X[:, 0], X[:, 1], c=lvq.labels_)\n    plt.scatter(lvq.p[:, 0], lvq.p[:, 1], c=range(len(lvq.p)), marker=\'+\')\n    plt.title(\'tinyml\')\n    plt.show()\n\n'"
tinyml/cluster/__init__.py,0,b''
tinyml/compare/__init__.py,0,b''
tinyml/compare/compare_classification.py,5,"b'from sklearn.metrics import mean_squared_error\r\nimport numpy as np\r\n\r\ndef train_and_eval(data,classifier):\r\n    train_X, train_y, test_X, test_y=data\r\n    classifier.fit(train_X,train_y)\r\n    preds_y=classifier.predict(test_X)\r\n    accuracy=len(preds_y[preds_y==test_y])/len(preds_y)\r\n    return accuracy\r\n\r\nfrom sklearn.datasets import load_iris,load_breast_cancer,load_wine\r\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\r\nfrom sklearn.model_selection import train_test_split\r\nimport sklearn.tree as tree\r\n\r\nfrom tinyml.bayes.NaiveBayesClassifier import NaiveBayesClassifierContinuous as tinymlNaiveBayesClassifier\r\nfrom tinyml.discriminant_analysis.LDA import LDA as tinymlLDA\r\nfrom tinyml.discriminant_analysis.GDA import GDA as tinymlGDA\r\nfrom tinyml.ensemble.AdaBoostClassifier import AdaBoostClassifier as tinymlAdaboostClassifier\r\nfrom tinyml.linear_model.LogisticRegression import LogisticRegression as tinymlLogisticRegression\r\nfrom tinyml.svm.SVC import SVC as tinymlSVC\r\nfrom tinyml.tree.DecisionTreeClassifier import DecisionTreeClassifier as tinymlDecsionTreeClassifier\r\n\r\nfrom sklearn.ensemble import AdaBoostClassifier as sklearnAdaboostClassifier\r\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as sklearnLDA\r\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as sklearnGDA\r\nfrom sklearn.linear_model import LogisticRegression as sklearnLogisticRegression\r\nfrom sklearn.svm import LinearSVC,SVC\r\nfrom sklearn.naive_bayes import GaussianNB as sklearnNaiveBayes\r\nfrom sklearn.tree import DecisionTreeClassifier as sklearnDecisionTreeClassifier\r\n\r\nif __name__==\'__main__\':\r\n\r\n\r\n    X, y=load_breast_cancer(return_X_y=True)\r\n    print(X[:,7:])\r\n    X=X[:,:7]\r\n    X = MinMaxScaler().fit_transform(X)\r\n\r\n    #y=(2*y-1).astype(np.int)\r\n    n_classes=2\r\n\r\n    train_X, test_X, train_y, test_y=train_test_split(X, y, test_size=0.3,random_state=0)\r\n\r\n    data= train_X, train_y, test_X, test_y\r\n\r\n    acc_tinyml_naivebayes=train_and_eval(data,tinymlNaiveBayesClassifier(n_classes=n_classes))\r\n    print(\'tinyml accuracy NaiveBayes:\',acc_tinyml_naivebayes)\r\n    acc_sklearn_naivebayes=train_and_eval(data,sklearnNaiveBayes())\r\n    print(\'sklearn accuracy NaiveBayes:\',acc_sklearn_naivebayes)\r\n\r\n\r\n\r\n    acc_tinyml_adaboost_classifier=train_and_eval((train_X,(train_y*2-1).astype(np.int),\r\n                                                   test_X,(test_y*2-1).astype(np.int)),tinymlAdaboostClassifier(n_estimators=100,base_estimator=tree.DecisionTreeClassifier(max_depth=1,random_state=False),method=\'re-weighting\'))\r\n    print(\'tinyml accuracy AdaboostClassifier:\',acc_tinyml_adaboost_classifier)\r\n    acc_sklearn_adaboost_classifier=train_and_eval(data,sklearnAdaboostClassifier(n_estimators=100, random_state=False, algorithm=\'SAMME\',\r\n                                                    base_estimator=tree.DecisionTreeClassifier(max_depth=1,random_state=False)))\r\n    print(\'sklearn accuracy AdaboostClassifier:\',acc_sklearn_adaboost_classifier)\r\n\r\n    acc_tinyml_lda_classifier=train_and_eval(data,tinymlLDA())\r\n    print(\'tinyml accuracy LDA:\',acc_tinyml_lda_classifier)\r\n    acc_sklearn_lda_classifier=train_and_eval(data,sklearnLDA())\r\n    print(\'sklearn accuracy LDA:\',acc_sklearn_lda_classifier)\r\n\r\n    acc_tinyml_gda_classifier=train_and_eval(data,tinymlGDA())\r\n    print(\'tinyml accuracy GDA:\',acc_tinyml_gda_classifier)\r\n    acc_sklearn_gda_classifier=train_and_eval(data,sklearnGDA())\r\n    print(\'sklearn accuracy QDA:\',acc_sklearn_gda_classifier)\r\n\r\n    acc_tinyml_logistic=train_and_eval(data,tinymlLogisticRegression(max_iter=100,use_matrix=False))\r\n    print(\'tinyml accuracy Logistic:\',acc_tinyml_logistic)\r\n    acc_sklearn_logistic=train_and_eval(data,sklearnLogisticRegression(max_iter=100,solver=\'newton-cg\'))\r\n    print(\'sklearn acccuracy Logistic:\',acc_sklearn_logistic)\r\n\r\n    tinyml_svc=tinymlSVC(max_iter=100,kernel=\'rbf\',C=1)\r\n    tinyml_svc.fit(train_X, (train_y*2-1).astype(int))\r\n    preds_y = np.sign(tinyml_svc.predict(test_X))\r\n    acc_tinyml_SVC = len(preds_y[preds_y == (2*test_y-1).astype(np.int)]) / len(preds_y)\r\n    print(\'tinyml accuracy SVC:\',acc_tinyml_SVC)\r\n\r\n    acc_sklearn_SVC=train_and_eval(data,SVC(kernel=\'rbf\'))\r\n    print(\'sklearn accuracy SVC:\',acc_sklearn_SVC)\r\n\r\n    """"""\r\n    acc_tinyml_decision_tree_classifier=train_and_eval(data,tinymlDecsionTreeClassifier(tree_type=\'ID3\',k_classes=2))\r\n    print(\'tinyml accuracy decision tree:\',acc_tinyml_decision_tree_classifier)\r\n    acc_sklearn_decision_tree_classifier=train_and_eval(data,sklearnDecisionTreeClassifier())\r\n    print(\'sklearn accuracy decison tree:\',acc_sklearn_decision_tree_classifier) \r\n    """"""\r\n'"
tinyml/compare/compare_clustering.py,1,"b""from tinyml.cluster.KMeans import KMeans as tinymlKMeans\r\nfrom tinyml.cluster.AGNES import AGNES as tinymlAGNES\r\nfrom tinyml.cluster.DBSCAN import DBSCAN as tinymlDBSCAN\r\nfrom tinyml.cluster.GaussianMixture import GaussianMixture as tinymlGaussianMixture\r\nfrom tinyml.cluster.LVQ import LVQ as tinymlLVQ\r\n\r\nfrom sklearn.cluster.hierarchical import AgglomerativeClustering as sklearnAGNES\r\nfrom sklearn.cluster import DBSCAN as sklearnDBSCAN\r\nfrom sklearn.cluster import KMeans as sklearnKMeans\r\nfrom sklearn.mixture import GaussianMixture as sklearnGaussianMixture\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nif __name__=='__main__':\r\n    # p202 \xe8\xa5\xbf\xe7\x93\x9c\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x864.0\r\n    X = np.array([[0.697, 0.460], [0.774, 0.376], [0.634, 0.264], [0.608, 0.318], [0.556, 0.215],\r\n                  [0.403, 0.237], [0.481, 0.149], [0.437, 0.211], [0.666, 0.091], [0.243, 0.267],\r\n                  [0.245, 0.057], [0.343, 0.099], [0.639, 0.161], [0.657, 0.198], [0.360, 0.370],\r\n                  [0.593, 0.042], [0.719, 0.103], [0.359, 0.188], [0.339, 0.241], [0.282, 0.257],\r\n                  [0.748, 0.232], [0.714, 0.346], [0.483, 0.312], [0.478, 0.437], [0.525, 0.369],\r\n                  [0.751, 0.489], [0.532, 0.472], [0.473, 0.376], [0.725, 0.445], [0.446, 0.459]])\r\n\r\n    # KMeans\r\n    tinyml_kmeans = tinymlKMeans(k=3)\r\n    tinyml_kmeans.fit(X)\r\n    plt.scatter(X[:, 0], X[:, 1], c=tinyml_kmeans.labels_)\r\n    plt.scatter(tinyml_kmeans.mu[:, 0], tinyml_kmeans.mu[:, 1], c=range(tinyml_kmeans.k), marker='+')\r\n    plt.title('tinyml KMeans')\r\n    plt.savefig('./cluster_result/tinyml_KMeans.jpg')\r\n    plt.show()\r\n\r\n    sklearn_kmeans = sklearnKMeans(n_clusters=3)\r\n    sklearn_kmeans.fit(X)\r\n    plt.scatter(X[:, 0], X[:, 1], c=sklearn_kmeans.labels_)\r\n    plt.scatter(sklearn_kmeans.cluster_centers_[:,0],sklearn_kmeans.cluster_centers_[:,1],c=range(sklearn_kmeans.n_clusters),marker='+')\r\n    plt.title('sklearn KMeans')\r\n    plt.savefig('./cluster_result/sklearn_KMeans.jpg')\r\n    plt.show()\r\n\r\n    # DBSCAN\r\n    tinyml_dbscan = tinymlDBSCAN()\r\n    tinyml_dbscan.fit(X)\r\n    plt.scatter(X[:, 0], X[:, 1], c=tinyml_dbscan.labels_)\r\n    plt.title('tinyml DBSCAN')\r\n    plt.savefig('./cluster_result/tinyml_DBSCAN.jpg')\r\n    plt.show()\r\n\r\n    sklearn_DBSCAN =sklearnDBSCAN(eps=0.11, min_samples=5, metric='l2')\r\n    sklearn_DBSCAN.fit(X)\r\n    plt.scatter(X[:, 0], X[:, 1], c=sklearn_DBSCAN.labels_)\r\n    plt.title('sklearn DBSCAN')\r\n    plt.savefig('./cluster_result/sklearn_DBSCAN.jpg')\r\n    plt.show()\r\n\r\n    # GMM\r\n    tinyml_gmm = tinymlGaussianMixture(k=3, max_iter=50)\r\n    tinyml_gmm.fit(X)\r\n    plt.scatter(X[:, 0], X[:, 1], c=tinyml_gmm.labels_)\r\n    plt.scatter(tinyml_gmm.mu[:, 0], tinyml_gmm.mu[:, 1], c=range(tinyml_gmm.k), marker='+')\r\n    plt.title('tinyml GMM')\r\n    plt.savefig('./cluster_result/tinyml_GMM.jpg')\r\n    plt.show()\r\n\r\n    sklearn_gmm = sklearnGaussianMixture(n_components=3, covariance_type='full',\r\n                                  max_iter=50).fit(X)\r\n    labels = sklearn_gmm.predict(X)\r\n    plt.scatter(X[:, 0], X[:, 1], c=labels)\r\n    plt.scatter(sklearn_gmm.means_[:,0],sklearn_gmm.means_[:,1],c=range(sklearn_gmm.n_components),marker='+')\r\n    plt.title('sklearn GMM')\r\n    plt.savefig('./cluster_result/sklearn_GMM.jpg')\r\n    plt.show()\r\n\r\n    # AGNES\r\n    tinyml_agnes = tinymlAGNES(k=3)\r\n    tinyml_agnes.fit(X)\r\n    plt.scatter(X[:, 0], X[:, 1], c=tinyml_agnes.labels_)\r\n    plt.title('tinyml AGNES')\r\n    plt.savefig('./cluster_result/tinyml_AGNES.jpg')\r\n    plt.show()\r\n\r\n    sklearn_agnes = sklearnAGNES(n_clusters=3, affinity='l2', linkage='average')\r\n    sklearn_agnes.fit(X)\r\n    plt.scatter(X[:, 0], X[:, 1], c=sklearn_agnes.labels_)\r\n    plt.title('sklearn AGNES')\r\n    plt.savefig('./cluster_result/sklearn_AGNES.jpg')\r\n    plt.show()\r\n\r\n\r\n"""
tinyml/compare/compare_dimension_reduction.py,0,"b'from tinyml.dimension_reduction.PCA import PCA as tinymlPCA\r\nfrom tinyml.dimension_reduction.KernelPCA import KernelPCA as tinymlKernalPCA\r\nfrom tinyml.dimension_reduction.LLE import LLE as tinymlLLE\r\nfrom tinyml.dimension_reduction.Isomap import Isomap as tinymlIsomap\r\nfrom tinyml.dimension_reduction.MDS import MDS as tinymlMDS\r\n\r\nfrom sklearn.decomposition import PCA as sklearnPCA\r\nfrom sklearn.decomposition import KernelPCA as sklearnKernalPCA\r\nfrom sklearn.manifold import LocallyLinearEmbedding as sklearnLLE\r\nfrom sklearn.manifold import MDS as sklearnMDS\r\nfrom sklearn.manifold import Isomap as sklearnIsomap\r\n\r\nfrom sklearn.datasets import load_iris\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nif __name__==\'__main__\':\r\n    from sklearn.datasets import make_s_curve\r\n\r\n    X, y = make_s_curve(n_samples=500,\r\n                        noise=0.1,\r\n                        random_state=0)\r\n\r\n    # PCA\r\n    tinyml_pca = tinymlPCA(d_=2)\r\n    X_=tinyml_pca.fit_transform(X)\r\n    plt.scatter(X_[:, 0], X_[:, 1], c=y)\r\n    plt.title(\'tinyml PCA\')\r\n    plt.savefig(\'./dimension_reduction_result/tinyml_PCA.jpg\')\r\n    plt.show()\r\n\r\n    sklearn_pca=sklearnPCA(n_components=2,svd_solver=\'full\')\r\n    X_=sklearn_pca.fit_transform(X)\r\n    plt.scatter(X_[:, 0], X_[:, 1], c=y)\r\n    plt.title(\'sklearn PCA\')\r\n    plt.savefig(\'./dimension_reduction_result/sklearn_PCA.jpg\')\r\n    plt.show()\r\n\r\n    # KPCA\r\n    tinyml_kpca = tinymlKernalPCA(d_=2, kernel=\'rbf\',gamma=0.5)\r\n    X_ = tinyml_kpca.fit_transform(X)\r\n    plt.scatter(X_[:, 0], X_[:, 1], c=y)\r\n    plt.title(\'tinyml KernalPCA\')\r\n    plt.savefig(\'./dimension_reduction_result/tinyml_KernalPCA.jpg\')\r\n    plt.show()\r\n\r\n    sklearn_kpca = sklearnKernalPCA(n_components=2, kernel=\'rbf\', gamma=0.5)\r\n    X_ = sklearn_kpca.fit_transform(X)\r\n    plt.scatter(X_[:, 0], X_[:, 1], c=y)\r\n    plt.title(\'sklearn KernalPCA\')\r\n    plt.savefig(\'./dimension_reduction_result/sklearn_KernalPCA.jpg\')\r\n    plt.show()\r\n\r\n    # LLE\r\n\r\n    tinyml_lle = tinymlLLE(d_=2, k=30,reg=1e-3)\r\n    X_ = tinyml_lle.fit_transform(X)\r\n    plt.scatter(X_[:, 0], X_[:, 1], c=y)\r\n    plt.title(\'tinyml LLE\')\r\n    plt.savefig(\'./dimension_reduction_result/tinyml_LLE.jpg\')\r\n    plt.show()\r\n\r\n    sklearn_lle= sklearnLLE(n_components=2,n_neighbors=30,reg=1e-3)\r\n    X_ = sklearn_lle.fit_transform(X)\r\n    plt.scatter(X_[:, 0], X_[:, 1], c=y)\r\n    plt.title(\'sklearn LLE\')\r\n    plt.savefig(\'./dimension_reduction_result/sklearn_LLE.jpg\')\r\n    plt.show()\r\n\r\n    # MDS\r\n\r\n    tinyml_mds = tinymlMDS(d_=2)\r\n    X_ = tinyml_mds.fit_transform(X)\r\n    plt.scatter(X_[:, 0], X_[:, 1], c=y)\r\n    plt.title(\'tinyml MDS\')\r\n    plt.savefig(\'./dimension_reduction_result/tinyml_MDS.jpg\')\r\n    plt.show()\r\n\r\n    sklearn_mds = sklearnMDS(n_components=2,metric=True,random_state=False)\r\n    X_ = sklearn_mds.fit_transform(X)\r\n    plt.scatter(X_[:, 0], X_[:, 1], c=y)\r\n    plt.title(\'sklearn MDS\')\r\n    plt.savefig(\'./dimension_reduction_result/sklearn_MDS.jpg\')\r\n    plt.show()\r\n\r\n    """"""\r\n    # Isomap\r\n    tinyml_isomap = tinymlIsomap(k=5,d_=2)\r\n    X_ = tinyml_isomap.fit_transform(X)\r\n    plt.scatter(X_[:, 0], X_[:, 1], c=y)\r\n    plt.title(\'tinyml Isomap\')\r\n    plt.savefig(\'./dimension_reduction_result/tinyml_Isomap.jpg\')\r\n    plt.show()\r\n\r\n    sklearn_isomap = sklearnIsomap(n_neighbors=5, n_components=2,path_method=\'auto\')\r\n    X_ = sklearn_isomap.fit_transform(X)\r\n    plt.scatter(X_[:, 0], X_[:, 1], c=y)\r\n    plt.title(\'sklearn Isomap\')\r\n    plt.savefig(\'./dimension_reduction_result/sklearn_Isomap.jpg\')\r\n    plt.show()\r\n    """"""\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
tinyml/compare/compare_regresssor.py,0,"b""from sklearn.metrics import mean_squared_error\r\ndef train_and_eval(data,regressor):\r\n    train_X, train_y, test_X, test_y=data\r\n    regressor.fit(train_X,train_y)\r\n    preds_y=regressor.predict(test_X)\r\n    mse=mean_squared_error(test_y,preds_y)\r\n    del regressor\r\n    return mse\r\n\r\nfrom sklearn.datasets import load_boston,load_diabetes\r\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\r\nfrom sklearn.model_selection import train_test_split\r\nimport sklearn.tree as tree\r\nfrom tinyml.linear_model.LinearRegression import LinearRegression as tinymlLinearRegression\r\nfrom tinyml.linear_model.SGDRegressor import SGDRegressor as tinymlSGDRegressor\r\nfrom tinyml.ensemble.GradientBoostingRegressor import GradientBoostingRegressor as tinymlGradientBoostingRegressor\r\nfrom tinyml.ensemble.RandomForestRegressor import RandomForestRegressor as tinymlRandomForestRegressor\r\nfrom tinyml.ensemble.XGBRegressor import XGBRegressor as tinymlXGBRegressor\r\nfrom tinyml.tree.DecisionTreeRegressor import DecisionTreeRegressor as tinymlDecisionTreeRegressor\r\n\r\nfrom sklearn.linear_model import LinearRegression as sklearnLinearRegression\r\nfrom sklearn.linear_model import SGDRegressor as sklearnSGDRegressor\r\nfrom sklearn.tree import DecisionTreeRegressor as sklearnDecisonTreeRegressor\r\nfrom sklearn.ensemble import RandomForestRegressor as sklearnRnadomForestRegressor\r\nfrom sklearn.ensemble import GradientBoostingRegressor as sklearnGradientBoostRegressor\r\nfrom xgboost import XGBRegressor\r\n\r\nif __name__=='__main__':\r\n\r\n\r\n    boston_X,boston_y=load_boston(return_X_y=True)\r\n\r\n    boston_train_X,boston_test_X,boston_train_y,boston_test_y=train_test_split(boston_X,boston_y,test_size=0.3,random_state=0)\r\n\r\n    data=boston_train_X,boston_train_y,boston_test_X,boston_test_y\r\n\r\n    rmse_tinyml_linear_regression=train_and_eval(data,tinymlLinearRegression())\r\n    print('tinyml LinearRegression:',rmse_tinyml_linear_regression)\r\n    rmse_sklearn_linear_regression=train_and_eval(data,sklearnLinearRegression())\r\n    print('sklearn LinearRegression:',rmse_sklearn_linear_regression)\r\n    print('\\n')\r\n    std_scaler=StandardScaler()\r\n    std_scaler.fit(boston_train_X)\r\n    X_train=std_scaler.transform(boston_train_X)\r\n    X_test=std_scaler.transform(boston_test_X)\r\n    rmse_tinyml_sgd_regressor=train_and_eval((X_train,boston_train_y,X_test,boston_test_y),tinymlSGDRegressor(max_iter=200,penalty='l1',alpha=1e-3,l1_ratio=0.5))\r\n    print('tinyml SGDRegressor:',rmse_tinyml_sgd_regressor)\r\n    rmse_sklearn_sgd_regressor=train_and_eval((X_train,boston_train_y,X_test,boston_test_y),sklearnSGDRegressor(max_iter=200,penalty='l1',alpha=1e-3))\r\n    print('sklearn SGDRegressor:',rmse_sklearn_sgd_regressor)\r\n    print('\\n')\r\n    rmse_tinyml_decision_tree_regressor=train_and_eval(data,tinymlDecisionTreeRegressor(min_samples_split=20,min_samples_leaf=5))\r\n    print('tinyml DecisionTreeRegressor:',rmse_tinyml_decision_tree_regressor)\r\n    rmse_sklearn_decision_tree_regressor=train_and_eval(data,sklearnDecisonTreeRegressor(min_samples_split=20,min_samples_leaf=5,random_state=False))\r\n    print('sklearn DecisionTreeRegressor:',rmse_sklearn_decision_tree_regressor)\r\n    print('\\n')\r\n    rmse_tinyml_random_forest_tree_regressor = train_and_eval(data, tinymlRandomForestRegressor(\r\n            base_estimator=tinymlDecisionTreeRegressor,\r\n            n_estimators=100, min_samples_leaf=5, min_samples_split=15))\r\n    print('tinyml RandomForestRegressor:', rmse_tinyml_random_forest_tree_regressor)\r\n    rmse_sklearn_random_forest_tree_regressor=train_and_eval(data,sklearnRnadomForestRegressor(n_estimators=100, min_samples_leaf=5, min_samples_split=15, random_state=False))\r\n    print('sklearn RandomForestRegressor:',rmse_tinyml_random_forest_tree_regressor)\r\n\r\n    rmse_tinyml_gradient_boost_regressor = train_and_eval(data,\r\n                                                          tinymlGradientBoostingRegressor(n_estimators=500,\r\n                                                                                          base_estimator=tree.DecisionTreeRegressor(\r\n                                                                                              min_samples_split=15,\r\n                                                                                              min_samples_leaf=5,\r\n                                                                                              random_state=False)))\r\n\r\n    print('tinyml GradientBoostRegressor:', rmse_tinyml_gradient_boost_regressor)\r\n    rmse_sklearn_gradient_boost_regressor=train_and_eval(data,\r\n                                                         sklearnGradientBoostRegressor(n_estimators=500,min_samples_leaf=5,min_samples_split=15,random_state=False))\r\n    print('sklearn GradientBoostRegressor:',rmse_sklearn_gradient_boost_regressor)\r\n\r\n    rmse_tinyml_xgbregressor = train_and_eval(data,\r\n                                              tinymlXGBRegressor(n_estimators=100, max_depth=3, gamma=0.))\r\n    print('tinyml XGBRegressor:', rmse_tinyml_xgbregressor)\r\n    rmse_xgboost_xgbregressor=train_and_eval(data,XGBRegressor(max_depth=3,learning_rate=0.1,n_estimators=100,gamma=0,reg_lambda=1))\r\n    print('xgboost XGBRegressor:',rmse_xgboost_xgbregressor)\r\n\r\n\r\n\r\n"""
tinyml/dimension_reduction/Isomap.py,15,"b""import numpy as np\n# \xe7\x94\xa8Floyd_Warshall\xe7\xae\x97\xe6\xb3\x95\xe7\xae\x97\xe5\x87\xba\xe7\x9a\x84dist\xe5\x92\x8csklearn\xe6\x9c\x89\xe5\xb7\xae\xe5\xbc\x82\n# MDS\xe4\xb9\x9f\xe6\x9c\x89\xe5\xb7\xae\xe5\xbc\x82\nclass Isomap:\n    def __init__(self,k=5,d_=2):\n        self.d_=d_\n        self.k=k\n        self.dist_matrix_=None\n\n    @staticmethod\n    def Floyd_Warshall(Dist):\n        m = Dist.shape[0]\n        for k in range(m):\n            for i in range(m):\n                for j in range(m):\n                    Dist[i, j] = min(Dist[i,j],Dist[i, k] + Dist[k, j])\n        return Dist\n\n    def fit(self,X):\n        m = X.shape[0]\n        Dist = np.zeros((m, m), dtype=np.float32)\n        self.Omega = np.zeros((m, m), dtype=np.float32)\n        for i in range(m):\n            Dist[i, :] = np.sqrt(np.sum((X[i] - X) ** 2, axis=1))\n            inf_index=np.argsort(Dist[i,:])[self.k+1:]\n            Dist[i,inf_index]=float('inf')\n        Dist=Isomap.Floyd_Warshall(Dist)\n        self.dist_matrix_=Dist\n        # \xe4\xbd\xbf\xe7\x94\xa8MDS\xe4\xb8\xad\xe7\x9a\x84\xe6\xad\xa5\xe9\xaa\xa4\n        Dist_i2 = np.mean(Dist, axis=1).reshape(-1, 1)\n        Dist_j2 = np.mean(Dist, axis=0).reshape(1, -1)\n        dist_2 = np.mean(Dist)\n        B_new = -0.5 * (Dist - Dist_i2 - Dist_j2 + dist_2)\n        # \xe7\x94\xa8eig\xe5\x92\x8ceigh\xe5\x87\xbd\xe6\x95\xb0\xe5\x88\x86\xe8\xa7\xa3\xe5\x87\xba\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe7\xac\xa6\xe5\x8f\xb7\xe4\xbd\x8d\xe4\xb8\x8d\xe5\x90\x8c\n        #values, vectors = np.linalg.eig(B_new)\n        values,vectors=np.linalg.eigh(B_new)\n        idx = np.argsort(values)[::-1]\n        self.values_ = values[idx][:self.d_]\n        # print('values:',self.values_)\n        self.vectors_ = vectors[:, idx][:, :self.d_]\n        self.Z = self.vectors_.dot(np.diag(np.sqrt(self.values_))).real\n\n\n    def fit_transform(self,X):\n        self.fit(X)\n        return self.Z\n        pass\n\nif __name__=='__main__':\n    X=np.array([[0.697,0.460],[0.774,0.376],[0.634,0.264],[0.608,0.318],[0.556,0.215],\n                [0.403,0.237],[0.481,0.149],[0.437,0.211],[0.666,0.091],[0.243,0.267],\n                [0.245,0.057],[0.343,0.099],[0.639,0.161],[0.657,0.198],[0.360,0.370],\n                [0.593,0.042],[0.719,0.103],[0.359,0.188],[0.339,0.241],[0.282,0.257],\n                [0.748,0.232],[0.714,0.346],[0.483,0.312],[0.478,0.437],[0.525,0.369],\n                [0.751,0.489],[0.532,0.472],[0.473,0.376],[0.725,0.445],[0.446,0.459]])\n\n    X=np.c_[X,X]\n    isomap=Isomap(k=5,d_=2)\n    Z=isomap.fit_transform(X)\n    print('tinyml:')\n    print(Z)\n\n    import sklearn.manifold as manifold\n    sklearn_Isomap=manifold.Isomap(n_neighbors=5, n_components=2,path_method='auto')\n    Z2=sklearn_Isomap.fit_transform(X)\n    print('sklearn')\n    print(Z2)\n\n    print('dist_matrix_diff:',np.sum((isomap.dist_matrix_-sklearn_Isomap.dist_matrix_)**2))\n    print('Z diff:',np.sum((Z-Z2)**2))"""
tinyml/dimension_reduction/KernelPCA.py,11,"b""import numpy as np\n# \xe7\xba\xbf\xe6\x80\xa7\xe6\xa0\xb8 \xe4\xb8\x8e sklearn \xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x80\xe8\x87\xb4\n#\xe3\x80\x80\xe5\x85\xb6\xe4\xbb\x96\xe6\xa0\xb8\xe9\x83\xbd\xe4\xb8\x8esklearn\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x8d\xe4\xb8\x80\xe8\x87\xb4!!! \xe8\xbf\x98\xe6\xb2\xa1\xe6\x89\xbe\xe5\x88\xb0\xe5\x8e\x9f\xe5\x9b\xa0\n\nclass KernelPCA:\n    def __init__(self,d_=2,kernel='linear',gamma=None,coef0=1.,degress=3):\n        self.d_=d_\n        self.W=None\n        self.mean_x=None\n        self.V=None\n        self.kernel=kernel\n        self.coef0=coef0\n        self.degress=degress\n        if gamma is None:\n            self.gamma=1./self.d_\n        else:\n            self.gamma=gamma\n\n    def kernel_func(self,kernel,x1,x2):\n        if kernel=='linear':\n            return x1.dot(x2.T)\n        elif kernel=='rbf':\n            return np.exp(-self.gamma*(np.sum((x1-x2)**2)))\n        elif kernel=='poly':\n            return np.power(self.gamma*(x1.dot(x2.T)+1)+self.coef0,self.degress)\n        elif kernel=='sigmoid':\n            return np.tanh(self.gamma*(x1.dot(x2.T))+self.coef0)\n\n    def computeK(self,X,kernel):\n        m=X.shape[0]\n        K=np.zeros((m,m))\n        for i in range(m):\n            for j in range(m):\n                if i<=j:\n                    K[i,j]=self.kernel_func(kernel,X[i],X[j])\n                else:\n                    K[i,j]=K[j,i]\n        return K\n\n    # p233 \xe5\x85\xac\xe5\xbc\x8f10.24\n    def fit(self,X):\n        self.mean_x=np.mean(X,axis=0)\n        X_new=X-self.mean_x\n        K=self.computeK(X_new,kernel=self.kernel)\n        # sklearn\xe5\xae\x9e\xe7\x8e\xb0\xe7\x94\xa8\xe7\x9a\x84eigh\xe5\x88\x86\xe8\xa7\xa3\n        values,vectors = np.linalg.eigh(K)\n        idx = values.argsort()[::-1]\n        # \xe8\xbf\x99\xe4\xb8\x80\xe6\xad\xa5\xe4\xb8\x8d\xe5\x8f\xaf\xe5\xb0\x91\n        vectors/=np.sqrt(values)\n        self.alphas_= vectors[:, idx][:, :self.d_]\n        self.lambdas_= values[idx][:self.d_]\n\n    # \xe5\x85\xac\xe5\xbc\x8f 10.25\n    def fit_transform(self,X):\n        self.fit(X)\n        X = X - self.mean_x\n        m=X.shape[0]\n        self.Z=np.zeros((m,self.d_))\n        for k in range(m):\n            for j in range(self.d_):\n                sum=0.\n                for i in range(m):\n                    sum+= self.alphas_[i, j] * (self.kernel_func(self.kernel, X[i], X[k]))\n                self.Z[k,j]=sum\n        return self.Z\n\n\nif __name__=='__main__':\n    X=np.array([[0.697,0.460],[0.774,0.376],[0.634,0.264],[0.608,0.318],[0.556,0.215],\n                [0.403,0.237],[0.481,0.149],[0.437,0.211],[0.666,0.091],[0.243,0.267],\n                [0.245,0.057],[0.343,0.099],[0.639,0.161],[0.657,0.198],[0.360,0.370],\n                [0.593,0.042],[0.719,0.103],[0.359,0.188],[0.339,0.241],[0.282,0.257],\n                [0.748,0.232],[0.714,0.346],[0.483,0.312],[0.478,0.437],[0.525,0.369],\n                [0.751,0.489],[0.532,0.472],[0.473,0.376],[0.725,0.445],[0.446,0.459]])\n\n    X=np.c_[X,X]\n    kpca=KernelPCA(d_=2, kernel='linear', gamma=1. / 2)\n    Z=kpca.fit_transform(X)\n    print('tinyml:')\n    #print('lambdas:', kpca.lambdas_)\n    #print('alphas:', kpca.alphas_)\n    print(Z)\n\n    import sklearn.decomposition as decomposition\n    sklearn_KPCA=decomposition.KernelPCA(n_components=2, kernel='linear', gamma=1. / 2, eigen_solver='dense', random_state=False)\n    Z2=sklearn_KPCA.fit_transform(X)\n    print('sklearn')\n    #print('lambdas:',sklearn_KPCA.lambdas_)\n    #print('alphas:',sklearn_KPCA.alphas_)\n    print(Z2)\n\n    print('Z diff:',np.sum((Z-Z2)**2))\n\n\n\n"""
tinyml/dimension_reduction/LLE.py,14,"b'import numpy as np\nimport scipy\n""""""\nOmega\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x8f\x82\xe8\x80\x83\xe8\xbf\x99\xe7\xaf\x87blog\n[\xe5\xb1\x80\xe9\x83\xa8\xe7\xba\xbf\xe6\x80\xa7\xe5\xb5\x8c\xe5\x85\xa5(LLE)\xe5\x8e\x9f\xe7\x90\x86\xe6\x80\xbb\xe7\xbb\x93](https://www.cnblogs.com/pinard/p/6266408.html?utm_source=itdadao&utm_medium=referral)\n""""""\nclass LLE:\n    def __init__(self,d_=2,k=6,reg=1e-3):\n        self.d_=d_\n        self.k=k\n        self.reg=reg\n\n    # p237 \xe5\x9b\xbe10.10 LLE\xe7\xae\x97\xe6\xb3\x95\n    def fit(self,X):\n        m=X.shape[0]\n        Dist=np.zeros((m,m),dtype=np.float32)\n        self.Omega=np.zeros((m,m),dtype=np.float32)\n        self.Q={}\n        for i in range(m):\n            Dist[i,:]=np.sqrt(np.sum((X[i]-X)**2,axis=1))\n            self.Q[i]=np.argsort(Dist[i,:])[1:self.k+1]\n            self.compute_omega(i,X)\n\n        self.M=np.matmul((np.identity(m)-self.Omega).T,(np.identity(m)-self.Omega))\n        w,v=np.linalg.eig(self.M)\n        index=np.argsort(w)\n        self.Z=v[:,index][:,1:1+self.d_]\n\n    def fit_transform(self,X):\n        self.fit(X)\n        return self.Z\n\n    def compute_omega(self,i,X):\n        Z=(X[i]-X[self.Q[i]]).dot((X[i]-X[self.Q[i]]).T)\n        Z+= self.reg * np.trace(Z) * np.identity(self.k)\n        Ik=np.ones((self.k,))\n        Zinv=np.linalg.inv(Z)\n        self.Omega[i, self.Q[i]]=np.matmul(Zinv,Ik)/(Ik.T.dot(Zinv).dot(Ik))\n\nif __name__==\'__main__\':\n    X = np.array([[0.697, 0.460], [0.774, 0.376], [0.634, 0.264], [0.608, 0.318], [0.556, 0.215],\n                  [0.403, 0.237], [0.481, 0.149], [0.437, 0.211], [0.666, 0.091], [0.243, 0.267],\n                  [0.245, 0.057], [0.343, 0.099], [0.639, 0.161], [0.657, 0.198], [0.360, 0.370],\n                  [0.593, 0.042], [0.719, 0.103], [0.359, 0.188], [0.339, 0.241], [0.282, 0.257],\n                  [0.748, 0.232], [0.714, 0.346], [0.483, 0.312], [0.478, 0.437], [0.525, 0.369],\n                  [0.751, 0.489], [0.532, 0.472], [0.473, 0.376], [0.725, 0.445], [0.446, 0.459]])\n    X = np.c_[X, X]\n    lle = LLE(d_=2, k=5,reg=1e-3)\n    Z = lle.fit_transform(X)\n    print(Z)\n\n    import sklearn.manifold as manifold\n    sklearn_LLE= manifold.LocallyLinearEmbedding(n_components=2,n_neighbors=5,reg=1e-3)\n    Z2 = sklearn_LLE.fit_transform(X)\n    print(Z2)\n\n    print(\'check diff:\',np.sum((Z2-Z)**2))\n'"
tinyml/dimension_reduction/MDS.py,13,"b'import numpy as np\nimport matplotlib.pyplot as plt\n# \xe4\xb8\x8d\xe7\x9f\xa5\xe9\x81\x93\xe5\xa6\x82\xe4\xbd\x95\xe9\xaa\x8c\xe8\xaf\x81\xe6\xad\xa3\xe7\xa1\xae\xe6\x80\xa7\xef\xbc\x8csklearn\xe4\xb8\xad\xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0\xe6\x96\xb9\xe5\xbc\x8f\xe5\x92\x8c\xe8\xa5\xbf\xe7\x93\x9c\xe4\xb9\xa6\xe4\xb8\xad\xe4\xb8\x8d\xe4\xb8\x80\xe8\x87\xb4,sklearn\xe4\xb8\xad\xe7\x94\xa8\xe7\x9a\x84smacof\xe6\x96\xb9\xe6\xb3\x95\n# \xe5\x92\x8c\xe8\x87\xaa\xe5\xb7\xb1\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84KernelPCA\xe7\xba\xbf\xe6\x80\xa7\xe6\xa0\xb8\xe6\x97\xb6\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x80\xe8\x87\xb4\n\nclass MDS:\n    def __init__(self,d_=2):\n        self.d_=d_\n        self.Z=None\n        self.values_=None\n        self.vectors_=None\n\n    # p229 \xe5\x9b\xbe10.3 MDS\xe7\xae\x97\xe6\xb3\x95\n    def fit(self,X):\n        m=X.shape[0]\n        B=X.dot(X.T)\n        Dist_2=np.zeros((m,m),dtype=np.float32)\n        for i in range(m):\n            for j in range(m):\n                Dist_2[i,j]=B[i,i]+B[j,j]-2*B[i,j]\n        Dist_i2=np.mean(Dist_2,axis=1).reshape(-1,1)\n        Dist_j2=np.mean(Dist_2,axis=0).reshape(1,-1)\n        dist_2=np.mean(Dist_2)\n        B_new=-0.5*(Dist_2-Dist_i2-Dist_j2+dist_2)\n\n        """"""\n        B_new=np.zeros((m,m))\n        for i in range(m):\n            for j in range(m):\n                B_new[i,j]=-0.5*(Dist_2[i,j]-Dist_i2[i,0]-Dist_j2[0,j]+dist_2)\n        """"""\n        # \xe7\x94\xa8eig\xe5\x92\x8ceigh\xe5\x87\xbd\xe6\x95\xb0\xe5\x88\x86\xe8\xa7\xa3\xe5\x87\xba\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe7\xac\xa6\xe5\x8f\xb7\xe4\xbd\x8d\xe4\xb8\x8d\xe5\x90\x8c\n        values,vectors=np.linalg.eig(B_new)\n        #values,vectors=np.linalg.eigh(B_new)\n        idx=np.argsort(values)[::-1]\n        self.values_=values[idx][:self.d_]\n        # print(\'values:\',self.values_)\n        self.vectors_=vectors[:,idx][:,:self.d_]\n        self.Z=self.vectors_.dot(np.diag(np.sqrt(self.values_))).real\n\n    def fit_transform(self,X):\n        self.fit(X)\n        return self.Z\n\n\nif __name__==\'__main__\':\n    X=np.array([[0.697,0.460],[0.774,0.376],[0.634,0.264],[0.608,0.318],[0.556,0.215],\n                [0.403,0.237],[0.481,0.149],[0.437,0.211],[0.666,0.091],[0.243,0.267],\n                [0.245,0.057],[0.343,0.099],[0.639,0.161],[0.657,0.198],[0.360,0.370],\n                [0.593,0.042],[0.719,0.103],[0.359,0.188],[0.339,0.241],[0.282,0.257],\n                [0.748,0.232],[0.714,0.346],[0.483,0.312],[0.478,0.437],[0.525,0.369],\n                [0.751,0.489],[0.532,0.472],[0.473,0.376],[0.725,0.445],[0.446,0.459]])\n\n    X=np.c_[X,X]\n    mds=MDS(d_=2)\n    Z=mds.fit_transform(np.array(X))\n    print(Z)\n\n    """"""\n    import sklearn.manifold as manifold\n    sklearn_MDS=manifold.MDS(n_components=2,metric=True,random_state=False)\n    Z2=sklearn_MDS.fit_transform(X)\n    print(Z2)\n    print(\'diff:\',np.sum((Z-Z2)**2))\n    """"""\n\n\n\n\n'"
tinyml/dimension_reduction/PCA.py,5,"b""import numpy as np\n\nclass PCA:\n    def __init__(self,d_=2):\n        self.d_=d_\n        self.W=None\n        self.mean_x=None\n        self.V=None\n\n    # p231 \xe5\x9b\xbe10.5 PCA\xe7\xae\x97\xe6\xb3\x95\n    def fit(self,X):\n        self.mean_x=np.mean(X,axis=0)\n        X_new=X-self.mean_x\n        covM=X_new.T.dot(X_new)\n        v,w = np.linalg.eig(covM)\n        idx = v.argsort()[::-1]\n        self.W=w[:,idx][:,:self.d_]\n        self.V=v[idx][:self.d_]\n\n\n    def fit_transform(self,X):\n        self.fit(X)\n        X=X-self.mean_x\n        new_X=X.dot(self.W)\n        return new_X\n\n\nif __name__=='__main__':\n    X=np.array([[0.697,0.460],[0.774,0.376],[0.634,0.264],[0.608,0.318],[0.556,0.215],\n                [0.403,0.237],[0.481,0.149],[0.437,0.211],[0.666,0.091],[0.243,0.267],\n                [0.245,0.057],[0.343,0.099],[0.639,0.161],[0.657,0.198],[0.360,0.370],\n                [0.593,0.042],[0.719,0.103],[0.359,0.188],[0.339,0.241],[0.282,0.257],\n                [0.748,0.232],[0.714,0.346],[0.483,0.312],[0.478,0.437],[0.525,0.369],\n                [0.751,0.489],[0.532,0.472],[0.473,0.376],[0.725,0.445],[0.446,0.459]])\n    X=np.c_[X,X]\n\n    pca=PCA(d_=2)\n    Z=pca.fit_transform(X)\n    print(Z)\n\n    import sklearn.decomposition as decomposition\n    sklearn_PCA=decomposition.PCA(n_components=2,svd_solver='full')\n    Z2=sklearn_PCA.fit_transform(X)\n    print(Z2)\n\n    print('diff:',np.sum((Z-Z2)**2))\n"""
tinyml/dimension_reduction/__init__.py,0,b''
tinyml/discriminant_analysis/GDA.py,12,"b'import numpy as np\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n""""""\nGaussian Discriminant Analysis \nhttps://see.stanford.edu/materials/aimlcs229/cs229-notes2.pdf\nhttps://zhuanlan.zhihu.com/p/37476759\n""""""\nclass GDA:\n    def __init__(self):\n        self.Phi=None\n        self.mu0=None\n        self.mu1=None\n        self.Sigma=None\n        self.n=None\n        pass\n\n    def fit(self, X, y):\n        m=X.shape[0]\n        self.n=X.shape[1]\n        bincount=np.bincount(y)\n        assert bincount.shape==(2,)\n        self.Phi=bincount[1]*1./m\n        zeros_indices=np.where(y==0)\n        one_indices=np.where(y==1)\n        self.mu0=np.mean(X[zeros_indices],axis=0)\n        self.mu1=np.mean(X[one_indices],axis=0)\n        self.Sigma=np.zeros((self.n,self.n))\n        for i in range(m):\n            if y[i]==0:\n                tmp=(X[i]-self.mu0).T.dot((X[i]-self.mu0))\n                self.Sigma+=tmp\n            else:\n                tmp=(X[i]-self.mu1).reshape(-1,1).dot((X[i]-self.mu1).reshape(1,-1))\n                self.Sigma+=tmp\n\n        self.Sigma=(X[zeros_indices]-self.mu0).T.dot(X[zeros_indices]-self.mu0)+(X[one_indices]-self.mu1).T.dot(X[one_indices]-self.mu1)\n        self.Sigma=self.Sigma/m\n\n\n    def predict_proba(self, X):\n        probs=[]\n        m=X.shape[0]\n        p0=1-self.Phi\n        p1=self.Phi\n        denominator=np.power(2*np.pi,self.n/2)*np.sqrt(np.linalg.det(self.Sigma))\n        for i in range(m):\n            px_y0=np.exp(-0.5*(X[i]-self.mu0).dot(np.linalg.inv(self.Sigma)).dot((X[i]-self.mu0).T))/denominator\n            px_y1 = np.exp(-0.5 * (X[i] - self.mu1).dot(np.linalg.inv(self.Sigma)).dot((X[i] - self.mu1).T)) /denominator\n            p_y0=px_y0*p0\n            p_y1=px_y1*p1\n            probs.append([p_y0/(p_y0+p_y1),p_y1/(p_y0+p_y1)])\n        return np.array(probs)\n\n    def predict(self, X):\n        p = self.predict_proba(X)\n        res = np.argmax(p, axis=1)\n        return res\n\n\nif __name__ == \'__main__\':\n    np.random.seed(42)\n    breast_data = load_breast_cancer()\n    X, y = breast_data.data, breast_data.target\n    X=MinMaxScaler().fit_transform(X)\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n    gda = GDA()\n    gda.fit(X_train, y_train)\n    lda_prob = gda.predict_proba(X_test)\n    lda_pred = gda.predict(X_test)\n    print(\'gda_prob:\', lda_prob)\n    print(\'gda_pred:\', lda_pred)\n    print(\'accuracy:\',len(y_test[y_test ==lda_pred]) * 1. / len(y_test))\n\n'"
tinyml/discriminant_analysis/LDA.py,14,"b""from sklearn import discriminant_analysis\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nclass LDA:\n    def __init__(self):\n        self.omega=None\n        self.omiga_mu_0=None\n        self.omiga_mu_1=None\n        pass\n\n    # \xe3\x80\x8a\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe3\x80\x8b p61\n    def fit(self,X,y):\n        n_samples = X.shape[0]\n        extra = np.ones((n_samples,))\n        X = np.c_[X, extra]\n        X_0=X[np.where(y==0)]\n        X_1=X[np.where(y==1)]\n        mu_0=np.mean(X_0,axis=0)\n        mu_1=np.mean(X_1,axis=0)\n        S_omega=X_0.T.dot(X_0)+X_1.T.dot(X_1)\n        invS_omega=np.linalg.inv(S_omega)\n        self.omega=invS_omega.dot(mu_0 - mu_1)\n        self.omega_mu_0=self.omega.T.dot(mu_0)\n        self.omega_mu_1=self.omega.T.dot(mu_1)\n        pass\n\n    # \xe4\xb9\xa6\xe4\xb8\x8a\xe6\xb2\xa1\xe8\xae\xb2\xe6\x80\x8e\xe4\xb9\x88\xe5\x88\xa4\xe6\x96\xad\xe5\x88\x86\xe7\xb1\xbb\n    # \xe9\x87\x87\xe7\x94\xa8\xe8\xb7\x9d\xe7\xa6\xbb\xe5\xba\xa6\xe9\x87\x8f\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97X\xe5\x88\xb0\xe4\xb8\xa4\xe4\xb8\xaa\xe6\x8a\x95\xe5\xbd\xb1\xe4\xb8\xad\xe5\xbf\x83\xe7\x9a\x84L2\xe8\xb7\x9d\xe7\xa6\xbb\xef\xbc\x8c\xe5\x88\x86\xe7\xb1\xbb\xe4\xb8\xba\xe8\xb7\x9d\xe7\xa6\xbb\xe6\x9b\xb4\xe8\xbf\x91\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\xe3\x80\x82\n    def predict_proba(self,X):\n        if self.omega is None:\n            raise RuntimeError('cant predict before fit')\n        n_samples = X.shape[0]\n        extra = np.ones((n_samples,))\n        X = np.c_[X, extra]\n        omega_mu = X.dot(self.omega)\n        d1=np.sqrt((omega_mu-self.omega_mu_1)**2)\n        d0=np.sqrt((omega_mu-self.omega_mu_0)**2)\n        prob_0=d1/(d0+d1)\n        prob_1=1-prob_0\n        return np.column_stack([prob_0, prob_1])\n\n    def predict(self,X):\n        p = self.predict_proba(X)\n        res = np.argmax(p, axis=1)\n        return res\n\n\nif __name__=='__main__':\n    np.random.seed(42)\n    breast_data = load_breast_cancer()\n    X, y = breast_data.data, breast_data.target\n    X = MinMaxScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n    lda = LDA()\n    lda.fit(X_train, y_train)\n    lda_prob = lda.predict_proba(X_test)\n    lda_pred = lda.predict(X_test)\n    #print('tinyml lda_prob:', lda_prob)\n    #print('tinyml lda_pred:', lda_pred)\n    print('tinyml accuracy:', len(y_test[y_test == lda_pred]) * 1. / len(y_test))\n\n\n    sklearn_lda = discriminant_analysis.LinearDiscriminantAnalysis()\n    sklearn_lda.fit(X_train,y_train)\n    sklearn_prob=sklearn_lda.predict_proba(X_test)\n    sklearn_pred=sklearn_lda.predict(X_test)\n    #print('sklearn prob:',sklearn_prob)\n    #print('sklearn pred:',sklearn_pred)\n    print('sklearn accuracy:',len(y_test[y_test==sklearn_pred])*1./len(y_test))\n"""
tinyml/discriminant_analysis/__init__.py,0,b''
tinyml/ensemble/AdaBoostClassifier.py,11,"b'""""""\n\xe5\x8f\xaa\xe9\x92\x88\xe5\xaf\xb92\xe5\x88\x86\xe7\xb1\xbb\n\xe8\x87\xaa\xe5\xb7\xb1\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84DecisionTreeClassifier\xe6\xb2\xa1\xe6\x9c\x89\xe5\xae\x9e\xe7\x8e\xb0 sample_weight\xe5\x8f\x82\xe6\x95\xb0\n\xe9\x87\x8d\xe7\x82\xb9\xe5\x9c\xa8AdaBoost\xef\xbc\x8c \xe4\xbd\xbf\xe7\x94\xa8sklearn\xe7\x9a\x84DecisionTreeClassifier\xe4\xbd\x9c\xe4\xb8\xba\xe5\x9f\xba\xe5\xad\xa6\xe4\xb9\xa0\xe5\x99\xa8\n""""""\nimport numpy as np\nimport copy\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier as sklearnAdaBoostClassifier\nimport sklearn.datasets as datasets\n\nclass AdaBoostClassifier:\n    def __init__(self, base_estimator=None, n_estimators=300,method=\'re-weighting\'):\n        self.base_estimator = base_estimator\n        self.n_estimators = n_estimators\n        self.method=method\n        self.hs_ = []\n        self.epsilons_ = []\n        self.alphas_ = []\n        self.Ds_ = []\n\n    def fit(self, X, y):\n        m = X.shape[0]\n        self.Ds_.append(np.ones((m,)) / m)\n        for t in range(self.n_estimators):\n            ht = self.base_estimator\n            if self.method==\'re-weighting\':\n                ht.fit(X, y, self.Ds_[t])\n            elif self.method==\'re-sampling\':\n                sample_indices=np.random.choice(range(m),size=m,p=self.Ds_[t])\n                ht.fit(X[sample_indices],y[sample_indices])\n            y_pred = ht.predict(X).astype(np.int32)\n            valid_indices = (y != y_pred)\n            mask = np.ones((len(y),))\n            mask[valid_indices] = 0\n            epsilon_t = 1 - np.sum(self.Ds_[t] * mask)\n            if epsilon_t > 0.5:\n                break\n            self.hs_.append(copy.copy(ht))\n            self.epsilons_.append(epsilon_t)\n            alpha_t = 0.5 * np.log((1 - epsilon_t) / epsilon_t)\n            self.alphas_.append(alpha_t)\n            self.Ds_.append(self.Ds_[t] * np.exp(-alpha_t * y * y_pred))\n            self.Ds_[t + 1] = self.Ds_[t + 1] / np.sum(self.Ds_[t + 1])\n\n\n    @classmethod\n    def calc_epsilon(clf, D, y_target, y_pred):\n        return 1 - np.sum(D[y_target == y_pred])\n\n    def predict(self, X):\n        H=np.zeros((X.shape[0],))\n        for t in range(len(self.alphas_)):\n           H+=(self.alphas_[t]*self.hs_[t].predict(X))\n        return np.sign(H)\n\n\nif __name__ == \'__main__\':\n    breast_data = datasets.load_breast_cancer()\n    X, y = breast_data.data, breast_data.target\n    y = 2 * y - 1\n    X_train, y_train = X[:200], y[:200]\n    X_test, y_test = X[200:], y[200:]\n    base_estimator=DecisionTreeClassifier(max_depth=1,random_state=False)\n\n    sklearn_decision_tree = DecisionTreeClassifier(max_depth=1)\n    sklearn_decision_tree.fit(X_train, y_train)\n    y_pred_decison_tree = sklearn_decision_tree.predict(X_test)\n    print(\'single decision tree:\', len(y_test[y_pred_decison_tree == y_test]) * 1.0 / len(y_test))\n\n    print(\'tinyml:\')\n    adaboost_clf = AdaBoostClassifier(n_estimators=100,base_estimator=base_estimator,method=\'re-weighting\')\n    adaboost_clf.fit(X_train, y_train)\n    y_pred = adaboost_clf.predict(X_test)\n    print(\'adaboost y_pred:\', len(y_test[y_pred == y_test]) * 1. / len(y_test))\n\n    print(\'sklearn:\')\n    sklearn_adboost_clf = sklearnAdaBoostClassifier(n_estimators=100, random_state=False, algorithm=\'SAMME\',\n                                                    base_estimator=base_estimator)\n    sklearn_adboost_clf.fit(X_train, y_train)\n    sklearn_y_pred = sklearn_adboost_clf.predict(X_test)\n    print(\'sklearn adaboost y_pred:\', len(y_test[y_test == sklearn_y_pred]) * 1. / len(y_test))\n\n'"
tinyml/ensemble/GradientBoostingRegressor.py,3,"b'import numpy as np\nfrom sklearn import datasets\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import ensemble\nimport copy\nfrom sklearn import tree\n\n""""""\nloss\xe4\xbd\xbf\xe7\x94\xa8\xe5\x9d\x87\xe6\x96\xb9\xe8\xaf\xaf\xe5\xb7\xae\n\xe6\xae\x8b\xe5\xb7\xae\xe4\xb8\xba y-y_pred\n\xe6\x9d\x8e\xe8\x88\xaa\xe3\x80\x8a\xe7\xbb\x9f\xe8\xae\xa1\xe5\xad\xa6\xe4\xb9\xa0\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x8b p151\n""""""\nclass GradientBoostingRegressor:\n    def __init__(self,base_estimator=None,n_estimators=10,lr=0.1):\n        self.base_estimator=base_estimator\n        self.n_esimators=n_estimators\n        self.estimators=[]\n        self.lr=lr\n        self.mean=None\n\n    def fit(self,X,y):\n        F0_x=np.ones_like(y)*np.mean(y)\n        y_pred=F0_x\n        self.mean=np.mean(y)\n        for i in range(self.n_esimators):\n            hm=copy.deepcopy(self.base_estimator)\n            hm.fit(X,y-y_pred)\n            self.estimators.append(hm)\n            y_pred=y_pred+self.lr*hm.predict(X)\n\n    def predict(self,X):\n        y=self.mean*np.ones((X.shape[0],))\n        for i in range(self.n_esimators):\n            y=y+self.lr*self.estimators[i].predict(X)\n        return y\n\n\nif __name__==\'__main__\':\n    breast_data = datasets.load_boston()\n    X, y = breast_data.data, breast_data.target\n    print(X.shape)\n    X_train, y_train = X[:400], y[:400]\n    X_test, y_test = X[400:], y[400:]\n\n    sklearn_decisiontree_reg=tree.DecisionTreeRegressor(min_samples_split=15, min_samples_leaf=5,random_state=False)\n    sklearn_decisiontree_reg.fit(X_train, y_train)\n    decisiontree_pred=sklearn_decisiontree_reg.predict(X_test)\n    print(\'base estimator:\',mean_squared_error(y_test,decisiontree_pred))\n\n    tinyml_gbdt_reg=GradientBoostingRegressor(n_estimators=500, base_estimator=tree.DecisionTreeRegressor(min_samples_split=15, min_samples_leaf=5, random_state=False))\n    tinyml_gbdt_reg.fit(X_train, y_train)\n    y_pred=tinyml_gbdt_reg.predict(X_test)\n    print(\'tinyml mse:\',mean_squared_error(y_test,y_pred))\n\n\n    sklearn_gbdt_reg=ensemble.GradientBoostingRegressor(n_estimators=500,min_samples_leaf=5,min_samples_split=15,random_state=False)\n    sklearn_gbdt_reg.fit(X_train,y_train)\n    sklearn_pred=sklearn_gbdt_reg.predict(X_test)\n    print(\'sklearn mse:\',mean_squared_error(y_test,sklearn_pred))\n'"
tinyml/ensemble/RandomForestRegressor.py,1,"b""import numpy as np\nfrom sklearn import datasets,ensemble,tree\nfrom sklearn.metrics import mean_squared_error\n\nclass RandomForestRegressor:\n    def __init__(self,base_estimator,n_estimators=10,min_samples_leaf=5,min_samples_split=15):\n        self.base_estimator=base_estimator\n        self.n_estimators=n_estimators\n        self.min_samples_split=min_samples_split\n        self.min_samples_leaf=min_samples_leaf\n        self.estimators_=[]\n\n    def fit(self,X,y):\n        for t in range(self.n_estimators):\n            estimator_t=self.base_estimator(random_state=True,min_samples_split=self.min_samples_split,min_samples_leaf=self.min_samples_leaf)\n            estimator_t.fit(X,y)\n            self.estimators_.append(estimator_t)\n\n    def predict(self,X):\n        preds=[]\n        for t in range(self.n_estimators):\n            preds.append(self.estimators_[t].predict(X))\n        return np.mean(np.array(preds),axis=0)\n\n\nif __name__=='__main__':\n    breast_data = datasets.load_boston()\n    X, y = breast_data.data, breast_data.target\n    X_train, y_train = X[:400], y[:400]\n    X_test, y_test = X[400:], y[400:]\n\n    tinyml_decisiontree_reg=tree.DecisionTreeRegressor(min_samples_split=20, min_samples_leaf=5,random_state=True)\n    tinyml_decisiontree_reg.fit(X_train, y_train)\n    decisiontree_pred=tinyml_decisiontree_reg.predict(X_test)\n    print('base estimator:',mean_squared_error(y_test,decisiontree_pred))\n\n    tinyml_rf_reg=RandomForestRegressor(n_estimators=100, base_estimator=tree.DecisionTreeRegressor)\n    tinyml_rf_reg.fit(X_train,y_train)\n    y_pred=tinyml_rf_reg.predict(X_test)\n    print('tinyml rf mse:',mean_squared_error(y_test,y_pred))\n\n    sklearn_rf_reg=ensemble.RandomForestRegressor(n_estimators=100, min_samples_leaf=5, min_samples_split=20, random_state=False)\n    sklearn_rf_reg.fit(X_train, y_train)\n    sklearn_pred=sklearn_rf_reg.predict(X_test)\n    print('sklearn mse:',mean_squared_error(y_test,sklearn_pred))\n"""
tinyml/ensemble/XGBRegressor.py,24,"b'import numpy as np\nimport abc\nfrom sklearn import datasets,tree\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nnp.random.seed(1)\n\nclass LossBase(object):\n    def __init__(self,y_target,y_pred):\n        self.y_target=y_target\n        self.y_pred=y_pred\n        pass\n\n    @abc.abstractmethod\n    def forward(self):\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def g(self):\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def h(self):\n        raise NotImplementedError\n\nclass MSELoss(LossBase):\n    def __init__(self,y_target,y_pred):\n        super(MSELoss,self).__init__(y_target,y_pred)\n\n    def forward(self):\n        return (self.y_target-self.y_pred)**2\n\n    def g(self):\n        return 2*(self.y_pred-self.y_target)\n\n    def h(self):\n        return 2*np.ones_like(self.y_target)\n\nclass CART:\n\n    def __init__(self, reg_lambda=1, gamma=0., max_depth=3,col_sample_ratio=0.5,row_sample_ratio=1.):\n        self.reg_lambda=reg_lambda\n        self.gamma=gamma\n        self.max_depth=max_depth\n        self.tree = None\n        self.leaf_nodes=0\n        self.obj_val=0.\n        self.col_sample_ratio=col_sample_ratio\n        self.row_sample_ratio=row_sample_ratio\n\n    def fit(self, X, y,g,h):\n        D = {}\n        D[\'X\'] = X\n        D[\'y\'] = y\n        A = np.arange(X.shape[1])\n        m=len(y)\n        self.tree = self.TreeGenerate(D,A,g,h,np.array(range(m)),0)\n        self.obj_val=-0.5*self.obj_val+self.gamma*self.leaf_nodes\n\n    def predict(self, X):\n        if self.tree is None:\n            raise RuntimeError(\'cant predict before fit\')\n        y_pred = []\n        for i in range(X.shape[0]):\n            tree = self.tree\n            x = X[i]\n            while True:\n                if not isinstance(tree, dict):\n                    y_pred.append(tree)\n                    break\n                a = list(tree.keys())[0]\n                tree = tree[a]\n                if isinstance(tree, dict):\n                    val = x[a]\n                    split_val=float(list(tree.keys())[0][1:])\n                    if val<=split_val:\n                        tree=tree[list(tree.keys())[0]]\n                    else:\n                        tree=tree[list(tree.keys())[1]]\n                else:\n                    y_pred.append(tree)\n                    break\n        return np.array(y_pred)\n\n    def TreeGenerate(self, D, A,g,h,indices,depth):\n        X = D[\'X\']\n        if depth>self.max_depth:\n            G=np.sum(g[indices])\n            H=np.sum(h[indices])\n            w=-(G/(H+self.reg_lambda))\n            self.obj_val+=(G**2/(H+self.reg_lambda))\n            self.leaf_nodes+=1\n            return w\n        split_j=None\n        split_s=None\n        max_gain=0.\n\n        col_sample_indices=np.random.choice(A,size=int(len(A)*self.col_sample_ratio))\n        indices=np.random.choice(indices,size=int(len(indices)*self.row_sample_ratio))\n\n        for j in A:\n            if j not in col_sample_indices:\n                continue\n            for s in np.unique(X[:,j]):\n                tmp_left=np.where(X[indices,j]<=s)[0]\n                tmp_right=np.where(X[indices,j]>s)[0]\n                if len(tmp_left)<1 or len(tmp_right)<1:\n                    continue\n                left_indices=indices[tmp_left]\n                right_indices=indices[tmp_right]\n                G_L=np.sum(g[left_indices])\n                G_R=np.sum(g[right_indices])\n                H_L=np.sum(h[left_indices])\n                H_R=np.sum(h[right_indices])\n                gain=  (G_L ** 2 / (H_L + self.reg_lambda) + G_R ** 2 / (H_R + self.reg_lambda) - (G_L + G_R) ** 2 / (H_L + H_R + self.reg_lambda)) - self.gamma\n                if gain>max_gain:\n                    split_j=j\n                    split_s=s\n                    max_gain=gain\n\n        if split_j is None:\n            G = np.sum(g[indices])\n            H = np.sum(h[indices])\n            w = -(G / (H + self.reg_lambda))\n            self.obj_val += (G ** 2 / (H + self.reg_lambda))\n            self.leaf_nodes += 1\n            return w\n\n        tree = {split_j: {}}\n        left_indices=indices[np.where(X[indices,split_j]<=split_s)[0]]\n        right_indices=indices[np.where(X[indices,split_j]>split_s)[0]]\n        tree[split_j][\'l\'+str(split_s)]=self.TreeGenerate(D,A,g,h,left_indices,depth+1)\n        tree[split_j][\'r\'+str(split_s)]=self.TreeGenerate(D,A,g,h,right_indices,depth+1)\n        # \xe5\xbd\x93\xe5\x89\x8d\xe8\x8a\x82\xe7\x82\xb9\xe5\x80\xbc\n        tree[split_j][\'val\']= -(np.sum(g[indices]) / (np.sum(h[indices]) + self.reg_lambda))\n        return tree\n\n""""""\n\xe4\xbd\xbf\xe7\x94\xa8MSELoss\n\xe6\x8c\x89\xe7\x85\xa7\xe9\x99\x88\xe5\xa4\xa9\xe5\xa5\x87\xe7\x9a\x84xgboost PPT\xe5\xae\x9e\xe7\x8e\xb0\n""""""\nclass XGBRegressor:\n    def __init__(self, reg_lambda=1, gamma=0., max_depth=5, n_estimators=250, eta=.1):\n        self.reg_lambda=reg_lambda\n        self.gamma=gamma\n        self.max_depth=max_depth\n        self.n_estimators=n_estimators\n        self.eta=eta\n        self.mean=None\n        self.estimators_=[]\n\n    def fit(self,X,y):\n        self.mean=np.mean(y)\n        y_pred = np.ones_like(y)*self.mean\n        loss = MSELoss(y, y_pred)\n        g, h = loss.g(), loss.h()\n        for t in range(self.n_estimators):\n            estimator_t=CART(self.reg_lambda, self.gamma, self.max_depth)\n            y_target=y-y_pred\n            estimator_t.fit(X,y_target,g,h)\n            self.estimators_.append(estimator_t)\n            y_pred+=(self.eta*estimator_t.predict(X))\n            loss=MSELoss(y,y_pred)\n            g,h=loss.g(),loss.h()\n\n    def predict(self,X):\n        y_pred=np.ones((X.shape[0],))*self.mean\n        for t in range(self.n_estimators):\n            y_pred+=(self.eta*self.estimators_[t].predict(X))\n        return y_pred\n\nif __name__==\'__main__\':\n    breast_data = datasets.load_boston()\n    X, y = breast_data.data, breast_data.target\n\n    X_train, y_train = X[:400], y[:400]\n    X_test, y_test = X[400:], y[400:]\n\n    sklearn_decisiontree_reg=tree.DecisionTreeRegressor(min_samples_split=15, min_samples_leaf=5,random_state=False)\n    sklearn_decisiontree_reg.fit(X_train, y_train)\n    decisiontree_pred=sklearn_decisiontree_reg.predict(X_test)\n    print(\'base estimator:\',mean_squared_error(y_test,decisiontree_pred))\n\n    tinyml_gbdt_reg=XGBRegressor(n_estimators=100,max_depth=3,gamma=0.)\n    tinyml_gbdt_reg.fit(X_train, y_train)\n    y_pred=tinyml_gbdt_reg.predict(X_test)\n    print(\'tinyml mse:\',mean_squared_error(y_test,y_pred))\n\n    xgb_reg=xgb.sklearn.XGBRegressor(max_depth=3,learning_rate=0.1,n_estimators=100,gamma=0,reg_lambda=1)\n    xgb_reg.fit(X_train,y_train)\n    xgb_pred=xgb_reg.predict(X_test)\n    print(\'xgb  mse:\',mean_squared_error(y_test,xgb_pred))\n'"
tinyml/ensemble/__init__.py,0,b''
tinyml/factorization_machine/FMClassifier.py,4,"b""import numpy as np\r\nfrom sklearn import linear_model\r\nfrom sklearn.datasets import load_breast_cancer\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nimport math\r\nnp.random.seed(0)\r\nimport torch\r\nfrom torch import nn,optim\r\nclass SGDFMClassifier:\r\n    class FMClassifier(nn.Module):\r\n        def __init__(self,n_features,loss='logistic',degree=2,n_components=2):\r\n            super(SGDFMClassifier.FMClassifier,self).__init__()\r\n            self.loss=loss\r\n            self.degree=degree\r\n            self.n_components=n_components\r\n            self.linear=nn.Linear(n_features,1)\r\n            self.v=nn.Parameter(torch.Tensor(n_features,self.n_components))\r\n            stdev=1./math.sqrt(self.v.size(1))\r\n            self.v.data.uniform_(-stdev,stdev)\r\n            self.sigmoid=nn.Sigmoid()\r\n\r\n        def forward(self,X):\r\n            y=self.linear(X)+0.5*torch.sum(torch.pow(torch.mm(X,self.v),2)-\r\n                                           torch.mm(torch.pow(X,2),torch.pow(self.v,2)))\r\n            return self.sigmoid(y)\r\n\r\n    def __init__(self,max_iter=100000,learning_rate=0.005):\r\n        self.max_iter=max_iter\r\n        self.learning_rate=learning_rate\r\n        self.criterion=nn.BCELoss()\r\n        self.fitted=False\r\n\r\n    def fit(self,X,y):\r\n        n_feature=X.shape[1]\r\n        self.model=self.FMClassifier(n_feature)\r\n        self.optimizer=optim.SGD(self.model.parameters(),lr=self.learning_rate)\r\n        X=torch.from_numpy(X.astype(np.float32))\r\n        y=torch.from_numpy(y.astype(np.float32))\r\n        for epoch in range(self.max_iter):\r\n            y_predict=self.model(X)[:,0]\r\n            loss=self.criterion(y_predict,y)\r\n            #print('epoch:',epoch,' loss.item():',loss.item())\r\n            self.optimizer.zero_grad()\r\n            loss.backward()\r\n            self.optimizer.step()\r\n\r\n    def predict(self,X):\r\n        X = torch.from_numpy(X.astype(np.float32))\r\n        with torch.no_grad():\r\n            y_pred = self.model(X).detach().numpy()\r\n            y_pred[y_pred>0.5]=1\r\n            y_pred[y_pred<=0.5]=0\r\n        return y_pred[:,0]\r\n\r\nif __name__=='__main__':\r\n    breast_data = load_breast_cancer()\r\n    X, y = breast_data.data[:, :7], breast_data.target\r\n    X = MinMaxScaler().fit_transform(X)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\r\n\r\n    torch_mfclassifier = SGDFMClassifier(20000, 0.001)\r\n    torch_mfclassifier.fit(X_train, y_train)\r\n    torch_pred = torch_mfclassifier.predict(X_test)\r\n    print('torch accuracy:', len(y_test[y_test == torch_pred]) / len(y_test))"""
tinyml/factorization_machine/__init__.py,0,b''
tinyml/feature_selection/ReliefFeatureSelection.py,9,"b""import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.datasets import load_breast_cancer\nimport random\n\n# \xe5\xa4\x84\xe7\x90\x86\xe8\xbf\x9e\xe7\xbb\xad\xe5\x9e\x8b\nclass ReliefFeatureSelection:\n    def __init__(self,sample_ratio=0.5,k=5,seed=None):\n        self.feature_importances_=None\n        self.k=k\n        self.sample_ratio=sample_ratio\n        self.seed=seed\n        random.seed(self.seed)\n\n    def fit(self,X,y):\n        m,n=X.shape\n        self.feature_importances_=np.zeros((n,))\n        for t in range(self.k):\n            indices=random.sample(range(m),int(m*self.sample_ratio))\n            subX,suby=X[indices],y[indices]\n            self.feature_importances_+=self._fit(subX,suby)\n        self.feature_importances_/=self.k\n\n\n    def transform(self,X,k_features):\n        choosed_indices=np.argsort(self.feature_importances_)[::-1][:k_features]\n        return X[:,choosed_indices]\n\n    def _fit(self,subX,suby):\n        label_to_indices = {}\n        labels = np.unique(suby)\n        for label in labels:\n            label_to_indices[label] = list(np.where(suby == label)[0])\n        m, n = subX.shape\n        feature_scores_ = np.zeros((n,))\n        for j in range(n):\n            for i in range(m):\n                label_i = suby[i]\n                xi_nhs = (subX[i, j] - subX[label_to_indices[label_i], j]) ** 2\n                if len(xi_nhs) == 1:\n                    xi_nh = 0\n                else:\n                    xi_nh = np.sort(xi_nhs)[1]\n                feature_scores_[j] -= xi_nh\n                for label in labels:\n                    if label == label_i:\n                        continue\n                    xi_nm = np.sort((subX[i, j] - subX[label_to_indices[label], j]) ** 2)[0]\n                    feature_scores_[j] += (xi_nm * len(label_to_indices[label]) / m)\n        return feature_scores_\n\n\nif __name__=='__main__':\n    breast_data = load_breast_cancer()\n    subX, suby = breast_data.data, breast_data.target\n    scaler=MinMaxScaler()\n    subX=scaler.fit_transform(subX)\n    reliefF=ReliefFeatureSelection()\n    reliefF.fit(subX, suby)\n    print('relief feature_importances:',reliefF.feature_importances_)\n    print('sorted:',np.argsort(reliefF.feature_importances_))\n\n    import skrebate.relieff as relieff\n    skrebate_reliefF=relieff.ReliefF()\n    skrebate_reliefF.fit(subX, suby)\n    print('skrebate feature_importances_:',skrebate_reliefF.feature_importances_)\n    print('sorted:',np.argsort(skrebate_reliefF.feature_importances_))\n\n\n\n\n\n"""
tinyml/feature_selection/__init__.py,0,b''
tinyml/linear_model/LinearRegression.py,9,"b'import numpy as np\nfrom sklearn import linear_model\n\n\nclass LinearRegression:\n    def __init__(self):\n        self.w=None\n        self.n_features=None\n\n    def fit(self,X,y):\n        """"""\n        w=(X^TX)^{-1}X^Ty\n        """"""\n        assert isinstance(X,np.ndarray) and isinstance(y,np.ndarray)\n        assert X.ndim==2 and y.ndim==1\n        assert y.shape[0]==X.shape[0]\n        n_samples = X.shape[0]\n        self.n_features=X.shape[1]\n        extra=np.ones((n_samples,))\n        X=np.c_[X,extra]\n        if self.n_features<n_samples:\n            self.w=np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n        else:\n            raise ValueError(\'dont have enough samples\')\n\n    def predict(self,X):\n        n_samples=X.shape[0]\n        extra = np.ones((n_samples,))\n        X = np.c_[X, extra]\n        if self.w is None:\n            raise RuntimeError(\'cant predict before fit\')\n        y_=X.dot(self.w)\n        return y_\n\nif __name__==\'__main__\':\n    X=np.array([[1.0,0.5,0.5],[1.0,1.0,0.3],[-0.1,1.2,0.5],[1.5,2.4,3.2],[1.3,0.2,1.4]])\n    y=np.array([1,0.5,1.5,2,-0.3])\n    lr=LinearRegression()\n    lr.fit(X,y)\n    X_test=np.array([[1.3,1,3.2],[-1.2,1.2,0.8]])\n    y_pre=lr.predict(X_test)\n    print(y_pre)\n\n    sklearn_lr=linear_model.LinearRegression()\n    sklearn_lr.fit(X,y)\n    sklearn_y_pre=sklearn_lr.predict(X_test)\n    print(sklearn_y_pre)\n\n    ridge_reg = linear_model.Ridge(alpha=0., solver=\'lsqr\')\n    ridge_reg.fit(X, y)\n    ridge_y_pre=ridge_reg.predict(X_test)\n    print(ridge_y_pre)\n\n\n'"
tinyml/linear_model/LocallyWeightedLinearRegression.py,9,"b'import numpy as np\nimport matplotlib.pyplot as plt\n""""""\nimplementation of Locally weighted linear regression in http://cs229.stanford.edu/notes/cs229-notes1.pdf\n""""""\nclass LocallyWeightedLinearRegression:\n    def __init__(self,tau):\n        self.tau=tau\n        self.w=None\n\n    def fit_predict(self,X,y,checkpoint_x):\n        m = X.shape[0]\n        self.n_features = X.shape[1]\n        extra = np.ones((m,))\n        X = np.c_[X, extra]\n        checkpoint_x=np.r_[checkpoint_x,1]\n        self.X=X\n        self.y=y\n        self.checkpoint_x=checkpoint_x\n        weight=np.zeros((m,))\n        for i in range(m):\n            weight[i]=np.exp(-(X[i]-checkpoint_x).dot((X[i]-checkpoint_x).T)/(2*(self.tau**2)))\n        weight_matrix=np.diag(weight)\n        self.w=np.linalg.inv(X.T.dot(weight_matrix).dot(X)).dot(X.T).dot(weight_matrix).dot(y)\n        return checkpoint_x.dot(self.w)\n\n    def fit_transform(self,X,y,checkArray):\n        m=len(y)\n        preds=[]\n        for i in range(m):\n            preds.append(self.fit_predict(X,y,checkArray[i]))\n        return np.array(preds)\n\n\nif __name__==\'__main__\':\n    X=np.linspace(0,30,100)\n    y=X**2+2\n    X=X.reshape(-1,1)\n    lr=LocallyWeightedLinearRegression(tau=100)\n    y_pred=lr.fit_transform(X,y,X)\n    plt.plot(X,y,label=\'gt\')\n    plt.plot(X,y_pred,label=\'pred\')\n    plt.legend()\n    plt.show()\n\n'"
tinyml/linear_model/LogisticRegression.py,18,"b'import numpy as np\nfrom sklearn import linear_model\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nnp.random.seed(42)\nimport torch\nfrom torch import nn,optim\n\n\nclass SGDLogisticRegression:\n    class LogisticRegressionModel(nn.Module):\n        def __init__(self,n_features):\n            super(SGDLogisticRegression.LogisticRegressionModel,self).__init__()\n            self.linear=nn.Linear(n_features,1)\n            self.sigmoid=nn.Sigmoid()\n\n        def forward(self,X):\n            return self.sigmoid(self.linear(X))\n\n    def __init__(self,max_iter=100000,learning_rate=0.005):\n        self.max_iter=max_iter\n        self.learning_rate=learning_rate\n        self.criterion=nn.BCELoss()\n        self.fitted=False\n\n    def fit(self,X,y):\n        n_feature=X.shape[1]\n        self.model=SGDLogisticRegression.LogisticRegressionModel(n_feature)\n        self.optimizer=optim.SGD(self.model.parameters(),lr=self.learning_rate)\n        X=torch.from_numpy(X.astype(np.float32))\n        y=torch.from_numpy(y.astype(np.float32))\n        for epoch in range(self.max_iter):\n            y_predict=self.model(X)[:,0]\n            loss=self.criterion(y_predict,y)\n            #print(\'epoch:\',epoch,\' loss.item():\',loss.item())\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n    def predict(self,X):\n        X = torch.from_numpy(X.astype(np.float32))\n        with torch.no_grad():\n            y_pred = self.model(X).detach().numpy()\n            y_pred[y_pred>0.5]=1\n            y_pred[y_pred<=0.5]=0\n        return y_pred[:,0]\n\n\nclass LogisticRegression:\n    def __init__(self,max_iter=100,use_matrix=True):\n        self.beta=None\n        self.n_features=None\n        self.max_iter=max_iter\n        self.use_Hessian=use_matrix\n\n    def fit(self,X,y):\n        n_samples=X.shape[0]\n        self.n_features=X.shape[1]\n        extra=np.ones((n_samples,))\n        X=np.c_[X,extra]\n        self.beta=np.random.random((X.shape[1],))\n        for i in range(self.max_iter):\n            if self.use_Hessian is not True:\n                dldbeta=self._dldbeta(X,y,self.beta)\n                dldldbetadbeta=self._dldldbetadbeta(X,self.beta)\n                self.beta-=(1./dldldbetadbeta*dldbeta)\n            else:\n                dldbeta = self._dldbeta(X, y, self.beta)\n                dldldbetadbeta = self._dldldbetadbeta_matrix(X, self.beta)\n                self.beta -= (np.linalg.inv(dldldbetadbeta).dot(dldbeta))\n\n\n\n    @staticmethod\n    def _dldbeta(X,y,beta):\n        # \xe3\x80\x8a\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe3\x80\x8b \xe5\x85\xac\xe5\xbc\x8f 3.30\n        m=X.shape[0]\n        sum=np.zeros(X.shape[1],).T\n        for i in range(m):\n            sum+=X[i]*(y[i]-np.exp(X[i].dot(beta))/(1+np.exp(X[i].dot(beta))))\n        return -sum\n\n    @staticmethod\n    def _dldldbetadbeta_matrix(X,beta):\n        m=X.shape[0]\n        Hessian=np.zeros((X.shape[1],X.shape[1]))\n        for i in range(m):\n            p1 = np.exp(X[i].dot(beta)) / (1 + np.exp(X[i].dot(beta)))\n            tmp=X[i].reshape((-1,1))\n            Hessian+=tmp.dot(tmp.T)*p1*(1-p1)\n        return Hessian\n\n    @staticmethod\n    def _dldldbetadbeta(X,beta):\n        # \xe3\x80\x8a\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe3\x80\x8b\xe5\x85\xac\xe5\xbc\x8f 3.31\n        m=X.shape[0]\n        sum=0.\n        for i in range(m):\n            p1=np.exp(X[i].dot(beta))/(1+np.exp(X[i].dot(beta)))\n            sum+=X[i].dot(X[i].T)*p1*(1-p1)\n        return sum\n\n    def predict_proba(self,X):\n        n_samples = X.shape[0]\n        extra = np.ones((n_samples,))\n        X = np.c_[X, extra]\n        if self.beta is None:\n            raise RuntimeError(\'cant predict before fit\')\n        p1 = np.exp(X.dot(self.beta)) / (1 + np.exp(X.dot(self.beta)))\n        p0 = 1 - p1\n        return np.c_[p0,p1]\n\n    def predict(self,X):\n        p=self.predict_proba(X)\n        res=np.argmax(p,axis=1)\n        return res\n\n\nif __name__==\'__main__\':\n    breast_data = load_breast_cancer()\n    X, y = breast_data.data[:,:7], breast_data.target\n    X = MinMaxScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n    tinyml_logisticreg = LogisticRegression(max_iter=100,use_matrix=True)\n    tinyml_logisticreg.fit(X_train, y_train)\n    lda_prob = tinyml_logisticreg.predict_proba(X_test)\n\n\n    lda_pred = tinyml_logisticreg.predict(X_test)\n    # print(\'tinyml logistic_prob:\', lda_prob)\n    # print(\'tinyml logistic_pred:\', lda_pred)\n    print(\'tinyml accuracy:\', len(y_test[y_test == lda_pred]) * 1. / len(y_test))\n\n    sklearn_logsticreg = linear_model.LogisticRegression(max_iter=100,solver=\'newton-cg\')\n    sklearn_logsticreg.fit(X_train, y_train)\n    sklearn_prob = sklearn_logsticreg.predict_proba(X_test)\n    sklearn_pred = sklearn_logsticreg.predict(X_test)\n    # print(\'sklearn prob:\',sklearn_prob)\n    # print(\'sklearn pred:\',sklearn_pred)\n    print(\'sklearn accuracy:\', len(y_test[y_test == sklearn_pred]) * 1. / len(y_test))\n\n    torch_sgd_logisticreg=SGDLogisticRegression(100000,0.01)\n    torch_sgd_logisticreg.fit(X_train,y_train)\n    torch_pred=torch_sgd_logisticreg.predict(X_test)\n    print(\'torch accuracy:\',len(y_test[y_test==torch_pred])/len(y_test))\n\n    # expected output\n    """"""\n    tinyml accuracy: 0.9590643274853801\n    sklearn accuracy: 0.9298245614035088\n    torch accuracy: 0.9532163742690059\n    """"""\n\n\n\n\n\n\n\n'"
tinyml/linear_model/SGDRegressor.py,12,"b""import numpy as np\nfrom sklearn import linear_model\n\n# \xe9\x87\x87\xe7\x94\xa8MSE\xe4\xbd\x9c\xe4\xb8\xba\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n# penalty = 'l2' \xe5\x88\x99\xe4\xb8\xba Ridge Regression\n# penalty = 'l1' \xe5\x88\x99\xe4\xb8\xba Lasso Regression\n# penalty = 'l1l2' \xe5\x88\x99\xe4\xb8\xba Elastic Net\n# alpha \xe4\xb8\xba \xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe7\xb3\xbb\xe6\x95\xb0\n\n# https://wwdguu.github.io/2018/09/01/%C2%96HOMLWSLATF-ch4/\nnp.random.seed(1)\nclass SGDRegressor:\n    def __init__(self,max_iter=100,penalty=None,alpha=1e-3,l1_ratio=0.5):\n        self.w = None\n        self.n_features = None\n        self.penalty=penalty\n        self.alpha=alpha\n        self.l1_ratio=l1_ratio\n        self.max_iter=max_iter\n\n    #\n    def fit(self, X, y):\n        assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray)\n        assert y.shape[0] == X.shape[0]\n        n_samples = X.shape[0]\n        self.n_features = X.shape[1]\n        extra = np.ones((n_samples,1))\n        X = np.c_[X,extra]\n        self.w=np.random.randn(X.shape[1],1)\n        for iter in range(self.max_iter):\n            for i in range(n_samples):\n                sample_index=np.random.randint(n_samples)\n                x_sample=X[sample_index:sample_index+1]\n                y_sample=y[sample_index:sample_index+1]\n                lr=SGDRegressor.learning_schedule(iter*n_samples+i)\n                # \xe6\xb1\x82\xe5\xaf\xbc\n                grad=2*x_sample.T.dot(x_sample.dot(self.w)-y_sample)\n                if self.penalty is not None:\n                    # Ridge\n                    if self.penalty=='l2':\n                        grad+=self.alpha*self.w\n                    # Lasso\n                    elif self.penalty=='l1':\n                        grad+=self.alpha*np.sign(self.w)\n                    # Elastic Net\n                    elif self.penalty=='l1l2':\n                        grad+=(self.alpha*self.l1_ratio*np.sign(self.w)+\n                               (1-self.l1_ratio)*self.alpha*self.w)\n\n                self.w=self.w-lr*grad\n\n\n    def predict(self, X):\n\n        n_samples = X.shape[0]\n        extra = np.ones((n_samples,1))\n        X = np.c_[X,extra]\n        if self.w is None:\n            raise RuntimeError('cant predict before fit')\n        y_ = X.dot(self.w)\n        return y_\n\n    @staticmethod\n    def learning_schedule(t):\n        return 5 / (t + 50)\n\n\nif __name__ == '__main__':\n    X = 2 * np.random.rand(100,1)\n    y = 4 + 3 * X + np.random.randn(100,1)\n    y=y.ravel()\n    print(X.shape)\n    print(y.shape)\n    lr = SGDRegressor(max_iter=200,penalty='l1l2',alpha=1e-3,l1_ratio=0.5)\n    lr.fit(X, y)\n    print('w:',lr.w)\n\n    sklearn_lr = linear_model.SGDRegressor(max_iter=200,penalty='l1',alpha=1e-3)\n    sklearn_lr.fit(X, y)\n    print(sklearn_lr.coef_)\n    print(sklearn_lr.intercept_)\n\n"""
tinyml/linear_model/__init__.py,0,b''
tinyml/metrices/__init__.py,0,b''
tinyml/metrices/curves.py,12,"b""import numpy as np\r\ndef precision_recall_curve(y_true,pred_prob):\r\n    probs=sorted(list(pred_prob),reverse=True)\r\n    Rs=[]\r\n    Ps=[]\r\n    for i in range(1,len(probs)):\r\n        thresh=probs[i]\r\n        preds_p=np.where(pred_prob>=thresh)[0]\r\n        preds_n=np.where(pred_prob<thresh)[0]\r\n        TP=len(np.where(y_true[preds_p]==1)[0])\r\n        FP=len(preds_p)-TP\r\n        FN=len(np.where(y_true[preds_n]==1)[0])\r\n        #TN=len(preds_n)-FN\r\n        R=TP/(TP+FN)\r\n        S=TP/(TP+FP)\r\n        Rs.append(R)\r\n        Ps.append(S)\r\n\r\n    return np.array(Ps),np.array(Rs)\r\n\r\ndef roc_curve(y_true,pred_prob):\r\n    probs=sorted(list(pred_prob),reverse=True)\r\n    TPRs=[]\r\n    FPRs=[]\r\n    for i in range(1,len(probs)):\r\n        thresh = probs[i]\r\n        preds_p = np.where(pred_prob >=thresh)[0]\r\n        preds_n = np.where(pred_prob <thresh)[0]\r\n        TP = len(np.where(y_true[preds_p] == 1)[0])\r\n        FP = len(preds_p) - TP\r\n        FN = len(np.where(y_true[preds_n] == 1)[0])\r\n        # TN=len(preds_n)-FN\r\n        TN=len(preds_n)-FN\r\n        TPRs.append(TP/(TP+FN))\r\n        FPRs.append(FP/(TN+FP))\r\n    return np.array(FPRs),np.array(TPRs)\r\n\r\ndef roc_auc_score(y_true,pred_prob):\r\n    FPRs,TPRs=roc_curve(y_true,pred_prob)\r\n    auc=0.\r\n    for i in range(0,len(FPRs)-1):\r\n        auc+=0.5*(FPRs[i+1]-FPRs[i])*(TPRs[i+1]+TPRs[i])\r\n    return auc\r\n\r\nif __name__=='__main__':\r\n    y_true=np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0])\r\n    pred_prob=np.array([0.7,0.9,0.2,0.8,0.3,0.64,0.53,0.12,0.34,0.52,0.98,0.03,0.32,0.4,\r\n                        0.8,0.21,0.01,0.67,0.32,0.08,0.05,0.8,0.34,0.8])\r\n\r\n    import matplotlib.pyplot as plt\r\n    Ps,Rs=precision_recall_curve(y_true,pred_prob)\r\n    plt.plot(Rs,Ps,label='tinyml')\r\n\r\n    from sklearn.metrics import precision_recall_curve as sklearn_pr_curve\r\n    Ps,Rs,_=sklearn_pr_curve(y_true,pred_prob)\r\n    plt.plot(Rs,Ps,label='sklearn')\r\n    plt.legend()\r\n    plt.title('PRC')\r\n    plt.show()\r\n\r\n    FPR,TPR=roc_curve(y_true,pred_prob)\r\n    plt.plot(FPR,TPR,label='tinyml')\r\n    print('tinyml_auc:',roc_auc_score(y_true,pred_prob))\r\n    from sklearn.metrics import roc_curve as sklearn_roc_curve\r\n    from sklearn.metrics import roc_auc_score as sklearn_roc_auc_score\r\n    FPR,TPR,_=sklearn_roc_curve(y_true,pred_prob)\r\n    plt.plot(FPR,TPR,label='sklearn')\r\n    plt.legend()\r\n    plt.title('ROC')\r\n    plt.show()\r\n    print('sklearn auc:',sklearn_roc_auc_score(y_true,pred_prob))\r\n"""
tinyml/svm/SVC.py,21,"b'import numpy as np\n\n""""""\n[\xe7\x9f\xa5\xe4\xb9\x8e\xe4\xb8\x93\xe6\xa0\x8f\xef\xbc\x9a\xe6\x94\xaf\xe6\x8c\x81\xe5\x90\x91\xe9\x87\x8f\xe6\x9c\xba(SVM)\xe2\x80\x94\xe2\x80\x94SMO\xe7\xae\x97\xe6\xb3\x95](https://zhuanlan.zhihu.com/p/32152421)\n""""""\nnp.random.seed(1)\n\nclass SVC:\n    def __init__(self,max_iter=100,C=1,kernel=\'rbf\',sigma=1):\n        self.b=0.\n        self.alpha=None\n        self.max_iter=max_iter\n        self.C=C\n        self.kernel=kernel\n        self.K=None\n        self.X=None\n        self.y=None\n        if kernel==\'rbf\':\n            self.sigma=sigma\n        pass\n\n    def kernel_func(self,kernel,x1,x2):\n        if kernel==\'linear\':\n            return x1.dot(x2.T)\n        elif kernel==\'rbf\':\n            return np.exp(-(np.sum((x1-x2)**2))/(2*self.sigma*self.sigma))\n\n    def computeK(self,X,kernel):\n        m=X.shape[0]\n        K=np.zeros((m,m))\n        for i in range(m):\n            for j in range(m):\n                if i<=j:\n                    K[i,j]=self.kernel_func(kernel,X[i],X[j])\n                else:\n                    K[i,j]=K[j,i]\n        return K\n\n    def compute_u(self,X,y):\n        u = np.zeros((X.shape[0],))\n        for j in range(X.shape[0]):\n            u[j]=np.sum(y*self.alpha*self.K[:,j])+self.b\n        return u\n\n    def checkKKT(self,u,y,i):\n        if self.alpha[i]<self.C and y[i]*u[i]<=1:\n            return False\n        if self.alpha[i]>0 and y[i]*u[i]>=1:\n            return False\n        if (self.alpha[i]==0 or self.alpha[i]==self.C) and y[i]*u[i]==1:\n            return False\n        return True\n\n\n    def fit(self,X,y):\n        self.X=X\n        self.y=y\n        self.K=self.computeK(X,self.kernel)\n        self.alpha=np.random.random((X.shape[0],))\n        self.omiga=np.zeros((X.shape[0],))\n\n        for _ in range(self.max_iter):\n            u = self.compute_u(X, y)\n            finish=True\n            for i in range(X.shape[0]):\n                if not self.checkKKT(u,y,i):\n                    finish=False\n                    y_indices=np.delete(np.arange(X.shape[0]),i)\n                    j=y_indices[int(np.random.random()*len(y_indices))]\n                    E_i=np.sum(self.alpha*y*self.K[:,i])+self.b-y[i]\n                    E_j=np.sum(self.alpha*y*self.K[:,j])+self.b-y[j]\n                    if y[i]!=y[j]:\n                        L=max(0,self.alpha[j]-self.alpha[i])\n                        H=min(self.C,self.C+self.alpha[j]-self.alpha[i])\n                    else:\n                        L=max(0,self.alpha[j]+self.alpha[i]-self.C)\n                        H=min(self.C,self.alpha[j]+self.alpha[i])\n                    eta=self.K[i,i]+self.K[j,j]-2*self.K[i,j]\n                    alpha2_new_unc=self.alpha[j]+y[j]*(E_i-E_j)/eta\n                    alpha2_old=self.alpha[j]\n                    alpha1_old=self.alpha[i]\n                    if alpha2_new_unc>H:\n                        self.alpha[j]=H\n                    elif alpha2_new_unc<L:\n                        self.alpha[j]=L\n                    else:\n                        self.alpha[j]=alpha2_new_unc\n                    self.alpha[i]=alpha1_old+y[i]*y[j]*(alpha2_old-self.alpha[j])\n                    b1_new=-E_i-y[i]*self.K[i,i]*(self.alpha[i]-alpha1_old)-y[j]*self.K[j,i]*(self.alpha[j]-alpha2_old)+self.b\n                    b2_new=-E_j-y[i]*self.K[i,j]*(self.alpha[i]-alpha1_old)-y[j]*self.K[j,j]*(self.alpha[j]-alpha2_old)+self.b\n                    if self.alpha[i]>0 and self.alpha[i]<self.C:\n                        self.b=b1_new\n                    elif self.alpha[j]>0 and self.alpha[j]<self.C:\n                        self.b=b2_new\n                    else:\n                        self.b=(b1_new+b2_new)/2\n            if finish:\n                break\n\n\n\n    def predict(self,X):\n        y_preds=[]\n        for i in range(X.shape[0]):\n            K=np.zeros((len(self.y),))\n            support_indices=np.where(self.alpha>0)[0]\n            for j in support_indices:\n                K[j]=self.kernel_func(self.kernel,self.X[j],X[i])\n            y_pred=np.sum(self.y[support_indices]*self.alpha[support_indices]*K[support_indices].T)\n            y_pred+=self.b\n            y_preds.append(y_pred)\n        return np.array(y_preds)\n\n\nif __name__==\'__main__\':\n\n    # \xe6\xb5\x8b\xe8\xaf\x95 \xe7\xba\xbf\xe6\x80\xa7\xe6\xa0\xb8\n    X = np.array([[2, -1], [3, -2], [1, 0], [0,1],[-2,1],[-1.3,0.3],[-0.2,-0.8],[2.3,-3.3],[-2,-4],[7,8]])\n    y = np.array([1, 1, 1, 1,-1,-1,-1,-1,-1,1])\n    svc=SVC(max_iter=100,kernel=\'linear\',C=1)\n\n    """"""\n    # \xe6\xb5\x8b\xe8\xaf\x95rbf\xe6\xa0\xb8\n    X=np.array([[1,0],[-1,0],[0,-1],[0,1],[2,np.sqrt(5)],[2,-np.sqrt(5)],[-2,np.sqrt(5)],[-2,-np.sqrt(5)],[300,400]])\n    y=np.array([-1,-1,-1,-1,1,1,1,1,1])\n    svc=SVC(max_iter=100,kernel=\'rbf\',C=1)\n    """"""\n    svc.fit(X,y)\n    print(\'alpha:\',svc.alpha)\n    print(\'b:\',svc.b)\n    pred_y=svc.predict(np.array([[1,0],[-0.2,-0.1],[0,1]]))\n    print(\'pred_y1:\',pred_y)\n    pred_y=np.sign(pred_y)\n    print(\'pred_y:\',pred_y)\n\n\n\n\n'"
tinyml/svm/__init__.py,0,b''
tinyml/tree/DecisionTreeClassifier.py,33,"b'import numpy as np\n""""""\n\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe5\x86\xb3\xe7\xad\x96\xe6\xa0\x91\xe5\xae\x9e\xe7\x8e\xb0\xef\xbc\x8c\xe7\xae\x97\xe6\xb3\x95\xe5\x8f\x82\xe8\x80\x83 \xe5\x91\xa8\xe5\xbf\x97\xe5\x8d\x8e\xe3\x80\x8a\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe3\x80\x8b\xe4\xb8\x80\xe4\xb9\xa6\n\xe5\x8f\xaa\xe5\xa4\x84\xe7\x90\x86\xe7\xa6\xbb\xe6\x95\xa3\xe5\x80\xbc\xef\xbc\x8c\xe4\xb8\x8d\xe8\x80\x83\xe8\x99\x91\xe7\xbc\xba\xe5\xa4\xb1\xe5\x80\xbc\n""""""\nfrom tinyml.tree.treePlotter import createPlot\nnp.random.seed(100)\nclass DecisionTreeClassifier:\n    """"""\n    \xe5\x86\xb3\xe7\xad\x96\xe6\xa0\x91\xe5\x88\x86\xe7\xb1\xbb\n    """"""\n    def __init__(self,tree_type=\'ID3\',k_classes=2):\n        self.tree_type=tree_type\n        self.k_classes=k_classes\n        if tree_type==\'ID3\':\n            self.gain_func=self.Gain\n        elif tree_type==\'CART\':\n            self.gain_func=self.GiniIndex\n        elif tree_type==\'C45\':\n            self.gain_func=self.GainRatio\n        else:\n            raise ValueError(\'must be ID3 or CART or C45\')\n        self.tree=None\n\n    def fit(self,X,y):\n        D={}\n        D[\'X\']=X\n        D[\'y\']=y\n        A=np.arange(X.shape[1])\n        aVs={}\n        for a in A:\n            aVs[a]=np.unique(X[:,a])\n        self.tree=self.TreeGenerate(D,A,aVs)\n\n    def predict(self,X):\n        if self.tree is None:\n            raise RuntimeError(\'cant predict before fit\')\n        y_pred=[]\n        for i in range(X.shape[0]):\n            tree = self.tree\n            x=X[i]\n            while True:\n                if not isinstance(tree,dict):\n                    y_pred.append(tree)\n                    break\n                a=list(tree.keys())[0]\n                tree=tree[a]\n                if isinstance(tree,dict):\n                    val = x[a]\n                    tree = tree[val]\n                else:\n                    y_pred.append(tree)\n                    break\n        return np.array(y_pred)\n\n\n    # p74 \xe5\x9b\xbe4.2\xe7\xae\x97\xe6\xb3\x95\xe5\x9b\xbe\n    def TreeGenerate(self,D,A,aVs):\n        X=D[\'X\']\n        y=D[\'y\']\n        # \xe6\x83\x85\xe5\xbd\xa21\n        unique_classes=np.unique(y)\n        if len(unique_classes)==1:\n            return unique_classes[0]\n        flag=True\n        for a in A:\n            if(len(np.unique(X[:,a]))>1):\n                flag=False\n                break\n        # \xe6\x83\x85\xe5\xbd\xa22\n        if flag:\n            return np.argmax(np.bincount(y))\n\n        gains=np.zeros((len(A),))\n        if self.tree_type==\'C45\':\n            gains=np.zeros((len(A),2))\n        for i in range(len(A)):\n            gains[i]=self.gain_func(D,A[i])\n        #print(gains)\n        subA=None\n        if self.tree_type==\'CART\':\n            a_best=A[np.argmin(gains)]\n            subA=np.delete(A,np.argmin(gains))\n        elif self.tree_type==\'ID3\':\n            a_best=A[np.argmax(gains)]\n            subA=np.delete(A,np.argmax(gains))\n        elif self.tree_type==\'C45\':\n            gain_mean=np.mean(gains[:,0])\n            higher_than_mean_indices=np.where(gains[:,0]>=gain_mean)\n            higher_than_mean=gains[higher_than_mean_indices,1][0]\n            index=higher_than_mean_indices[0][np.argmax(higher_than_mean)]\n            a_best=A[index]\n            subA=np.delete(A,index)\n\n        tree={a_best:{}}\n\n        for av in aVs[a_best]:\n            indices=np.where(X[:,a_best]==av)\n            Dv={}\n            Dv[\'X\']=X[indices]\n            Dv[\'y\']=y[indices]\n            if len(Dv[\'y\'])==0:\n                tree[a_best][av]=np.argmax(np.bincount(y))\n            else:\n                tree[a_best][av]=self.TreeGenerate(Dv,subA,aVs)\n        return tree\n\n\n\n    @classmethod\n    def Ent(cls,D):\n        """"""\n         \xe3\x80\x8a\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe3\x80\x8b \xe5\x85\xac\xe5\xbc\x8f4.1 \xe4\xbf\xa1\xe6\x81\xaf\xe7\x86\xb5\n        :param D: \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n        :return: \xe4\xbf\xa1\xe6\x81\xaf\xe7\x86\xb5\n        """"""\n        y=D[\'y\']\n        bin_count=np.bincount(y)\n        total=len(y)\n        ent=0.\n        for k in range(len(bin_count)):\n            p_k=bin_count[k]/total\n            if p_k!=0:\n                 ent+=p_k*np.log2(p_k)\n        return -ent\n\n    @classmethod\n    def Gain(cls,D,a):\n        """"""\n        \xe3\x80\x8a\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe3\x80\x8b \xe5\x85\xac\xe5\xbc\x8f4.2 \xe4\xbf\xa1\xe6\x81\xaf\xe5\xa2\x9e\xe7\x9b\x8a\n        a\xe8\xa1\xa8\xe7\xa4\xba\xe5\xb1\x9e\xe6\x80\xa7\xe5\x88\x97 index\n        """"""\n        X=D[\'X\']\n        y=D[\'y\']\n        aV=np.unique(X[:,a])\n        sum=0.\n        for v in range(len(aV)):\n            Dv={}\n            indices=np.where(X[:,a]==aV[v])\n            Dv[\'X\']=X[indices]\n            Dv[\'y\']=y[indices]\n            ent=cls.Ent(Dv)\n            sum+=(len(Dv[\'y\'])/len(y)*ent)\n        gain=cls.Ent(D)-sum\n        return gain\n\n    @classmethod\n    def Gini(cls,D):\n        """"""\n        \xe3\x80\x8a\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe3\x80\x8b \xe5\x85\xac\xe5\xbc\x8f4.5\n        """"""\n        y = D[\'y\']\n        bin_count = np.bincount(y)\n        total = len(y)\n        ent = 0.\n        for k in range(len(bin_count)):\n            p_k = bin_count[k] / total\n            ent+=p_k**2\n        return 1-ent\n\n    @classmethod\n    def GiniIndex(cls,D,a):\n        """"""\n        \xe5\x85\xac\xe5\xbc\x8f4.6\n        """"""\n        X = D[\'X\']\n        y = D[\'y\']\n        aV = np.unique(X[:, a])\n        sum = 0.\n        for v in range(len(aV)):\n            Dv = {}\n            indices = np.where(X[:, a] == aV[v])\n            Dv[\'X\'] = X[indices]\n            Dv[\'y\'] = y[indices]\n            ent = cls.Gini(Dv)\n            sum += (len(Dv[\'y\']) / len(y) * ent)\n        gain = sum\n        return gain\n\n    @classmethod\n    def GainRatio(cls,D,a):\n        """"""\n        \xe5\x85\xac\xe5\xbc\x8f4.3 4.4\n        """"""\n        X = D[\'X\']\n        y = D[\'y\']\n        aV = np.unique(X[:, a])\n        sum = 0.\n        intrinsic_value=0.\n        for v in range(len(aV)):\n            Dv = {}\n            indices = np.where(X[:, a] == aV[v])\n            Dv[\'X\'] = X[indices]\n            Dv[\'y\'] = y[indices]\n            ent = cls.Ent(Dv)\n            sum += (len(Dv[\'y\']) / len(y) * ent)\n            intrinsic_value+=(len(Dv[\'y\'])/len(y))*np.log2(len(Dv[\'y\'])/len(y))\n        gain = cls.Ent(D) - sum\n        intrinsic_value=-intrinsic_value\n        gain_ratio=gain/intrinsic_value\n        return np.array([gain,gain_ratio])\n\nif __name__==\'__main__\':\n    watermelon_data = np.array([[0, 0, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0],\n                                [1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0],\n                                [2, 0, 0, 0, 0, 0], [0, 1, 0, 0, 1, 1],\n                                [1, 1, 0, 1, 1, 1], [1, 1, 0, 0, 1, 0],\n                                [1, 1, 1, 1, 1, 0], [0, 2, 2, 0, 2, 1],\n                                [2, 2, 2, 2, 2, 0], [2, 0, 0, 2, 2, 1],\n                                [0, 1, 0, 1, 0, 0], [2, 1, 1, 1, 0, 0],\n                                [1, 1, 0, 0, 1, 1], [2, 0, 0, 2, 2, 0],\n                                [0, 0, 1, 1, 1, 0]])\n    label = np.array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n    X_test=np.array([[0, 0, 1, 0, 0, 0], [1, 0, 1, 0, 0, 0],\n                        [1, 1, 0, 1, 1, 0], [1, 0, 1, 1, 1, 0],\n                     [1, 1, 0, 0, 1, 1], [2, 0, 0, 2, 2, 0],\n                     [0, 0, 1, 1, 1, 0]])\n\n    decision_clf=DecisionTreeClassifier(tree_type=\'ID3\')\n    decision_clf.fit(watermelon_data,label)\n    print(decision_clf.tree)\n    createPlot(decision_clf.tree)\n\n    y_pred=decision_clf.predict(X_test)\n    print(\'y_pred:\',y_pred)\n\n\n\n\n\n\n\n\n\n'"
tinyml/tree/DecisionTreeRegressor.py,12,"b'import numpy as np\nfrom tinyml.tree import treePlotter\nimport sklearn.datasets as datasets\nfrom sklearn.metrics import mean_squared_error\nimport sklearn.tree as tree\nimport graphviz\n\nclass DecisionTreeRegressor:\n    """"""\n    \xe3\x80\x8a\xe7\xbb\x9f\xe8\xae\xa1\xe5\xad\xa6\xe4\xb9\xa0\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x8b p69 \xe6\x9c\x80\xe5\xb0\x8f\xe4\xba\x8c\xe4\xb9\x98\xe5\x9b\x9e\xe5\xbd\x92\xe6\xa0\x91\n    """"""\n    def __init__(self, min_samples_split=3,min_samples_leaf=1,random_state=False):\n        self.min_samples_split=min_samples_split\n        self.min_samples_leaf=min_samples_leaf\n        self.random=random_state\n        self.tree = None\n\n    def fit(self, X, y):\n        D = {}\n        D[\'X\'] = X\n        D[\'y\'] = y\n        A = np.arange(X.shape[1])\n        self.tree = self.TreeGenerate(D, A)\n\n    def predict(self, X):\n        if self.tree is None:\n            raise RuntimeError(\'cant predict before fit\')\n        y_pred = []\n        for i in range(X.shape[0]):\n            tree = self.tree\n            x = X[i]\n            while True:\n                if not isinstance(tree, dict):\n                    y_pred.append(tree)\n                    break\n                a = list(tree.keys())[0]\n                tree = tree[a]\n                if isinstance(tree, dict):\n                    val = x[a]\n                    split_val=float(list(tree.keys())[0][1:])\n                    if val<=split_val:\n                        tree=tree[list(tree.keys())[0]]\n                    else:\n                        tree=tree[list(tree.keys())[1]]\n                else:\n                    y_pred.append(tree)\n                    break\n        return np.array(y_pred)\n\n    def TreeGenerate(self, D, A):\n        X = D[\'X\']\n        y = D[\'y\']\n        if len(y)<=self.min_samples_split:\n            return np.mean(y)\n        split_j=None\n        split_s=None\n        min_val=1.e10\n        select_A=A\n        if self.random is True:\n            d=len(A)\n            select_A=np.random.choice(A,size=int(d//2),replace=False)\n        for j in select_A:\n            for s in np.unique(X[:,j]):\n                left_indices=np.where(X[:,j]<=s)[0]\n                right_indices=np.where(X[:,j]>s)[0]\n                if len(left_indices)<self.min_samples_leaf or len(right_indices)<self.min_samples_leaf:\n                    continue\n                val=np.sum((y[left_indices]-np.mean(y[left_indices]))**2)+np.sum((y[right_indices]-np.mean(y[right_indices]))**2)\n                if val<min_val:\n                    split_j=j\n                    split_s=s\n                    min_val=val\n\n        if split_j is None:\n            return np.mean(y)\n        tree = {split_j: {}}\n        left_indices=np.where(X[:,split_j]<=split_s)[0]\n        right_indices=np.where(X[:,split_j]>split_s)[0]\n        D_left, D_right = {},{}\n        D_left[\'X\'],D_left[\'y\'] = X[left_indices],y[left_indices]\n        D_right[\'X\'],D_right[\'y\']=X[right_indices],y[right_indices]\n        tree[split_j][\'l\'+str(split_s)]=self.TreeGenerate(D_left,A)\n        tree[split_j][\'r\'+str(split_s)]=self.TreeGenerate(D_right,A)\n        # \xe5\xbd\x93\xe5\x89\x8d\xe8\x8a\x82\xe7\x82\xb9\xe5\x80\xbc\n        tree[split_j][\'val\']=np.mean(y)\n        return tree\n\n\nif __name__==\'__main__\':\n    breast_data = datasets.load_boston()\n    X, y = breast_data.data, breast_data.target\n    X_train, y_train = X[:200], y[:200]\n    X_test, y_test = X[200:], y[200:]\n\n\n    decisiontree_reg=DecisionTreeRegressor(min_samples_split=20,min_samples_leaf=5)\n    decisiontree_reg.fit(X_train,y_train)\n    print(decisiontree_reg.tree)\n    treePlotter.createPlot(decisiontree_reg.tree)\n    y_pred=decisiontree_reg.predict(X_test)\n    print(\'tinyml mse:\',mean_squared_error(y_test,y_pred))\n\n\n    sklearn_reg=tree.DecisionTreeRegressor(min_samples_split=20,min_samples_leaf=5,random_state=False)\n    sklearn_reg.fit(X_train,y_train)\n    print(sklearn_reg.feature_importances_)\n    sklearn_pred=sklearn_reg.predict(X_test)\n    print(\'sklearn mse:\',mean_squared_error(y_test,sklearn_pred))\n    dot_data=tree.export_graphviz(sklearn_reg,out_file=None)\n    graph=graphviz.Source(dot_data)\n'"
tinyml/tree/__init__.py,0,b''
tinyml/tree/treePlotter.py,0,"b'import matplotlib.pyplot as plt\nfrom pylab import mpl\n\n""""""\n\xe3\x80\x8aMachine Learning in Action\xe3\x80\x8b\xe4\xb8\x80\xe4\xb9\xa6\xe4\xb8\xad\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe5\x86\xb3\xe7\xad\x96\xe6\xa0\x91\xe7\x9a\x84\xe4\xbb\xa3\xe7\xa0\x81\n""""""\n\nmpl.rcParams[\'font.sans-serif\'] = [\'FangSong\'] # \xe6\x8c\x87\xe5\xae\x9a\xe9\xbb\x98\xe8\xae\xa4\xe5\xad\x97\xe4\xbd\x93\nmpl.rcParams[\'axes.unicode_minus\'] = False # \xe8\xa7\xa3\xe5\x86\xb3\xe4\xbf\x9d\xe5\xad\x98\xe5\x9b\xbe\xe5\x83\x8f\xe6\x98\xaf\xe8\xb4\x9f\xe5\x8f\xb7\'-\'\xe6\x98\xbe\xe7\xa4\xba\xe4\xb8\xba\xe6\x96\xb9\xe5\x9d\x97\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\ndecisionNode = dict(boxstyle=""sawtooth"", fc=""0.8"")\nleafNode = dict(boxstyle=""round4"", fc=""0.8"")\narrow_args = dict(arrowstyle=""<-"")\n\ndef plotNode(nodeTxt, centerPt, parentPt, nodeType):\n    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords=\'axes fraction\', \\\n                            xytext=centerPt, textcoords=\'axes fraction\', \\\n                            va=""center"", ha=""center"", bbox=nodeType, arrowprops=arrow_args)\n\ndef getNumLeafs(myTree):\n    numLeafs = 0\n    firstStr = list(myTree.keys())[0]\n    secondDict = myTree[firstStr]\n    for key in secondDict.keys():\n        if type(secondDict[key]).__name__ == \'dict\':\n            numLeafs += getNumLeafs(secondDict[key])\n        else:\n            numLeafs += 1\n    return numLeafs\n\ndef getTreeDepth(myTree):\n    maxDepth = 0\n    firstStr = list(myTree.keys())[0]\n    secondDict = myTree[firstStr]\n    for key in secondDict.keys():\n        if type(secondDict[key]).__name__ == \'dict\':\n            thisDepth = getTreeDepth(secondDict[key]) + 1\n        else:\n            thisDepth = 1\n        if thisDepth > maxDepth:\n            maxDepth = thisDepth\n    return maxDepth\n\ndef plotMidText(cntrPt, parentPt, txtString):\n    xMid = (parentPt[0] - cntrPt[0]) / 2.0 + cntrPt[0]\n    yMid = (parentPt[1] - cntrPt[1]) / 2.0 + cntrPt[1]\n    createPlot.ax1.text(xMid, yMid, txtString)\n\ndef plotTree(myTree, parentPt, nodeTxt):\n    numLeafs = getNumLeafs(myTree)\n    depth = getTreeDepth(myTree)\n    firstStr = list(myTree.keys())[0]\n    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs)) / 2.0 / plotTree.totalw, plotTree.yOff)\n    plotMidText(cntrPt, parentPt, nodeTxt)\n    plotNode(firstStr, cntrPt, parentPt, decisionNode)\n    secondDict = myTree[firstStr]\n    plotTree.yOff = plotTree.yOff - 1.0 / plotTree.totalD\n    for key in secondDict.keys():\n        if type(secondDict[key]).__name__ == \'dict\':\n            plotTree(secondDict[key], cntrPt, str(key))\n        else:\n            plotTree.xOff = plotTree.xOff + 1.0 / plotTree.totalw\n            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)\n            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))\n    plotTree.yOff = plotTree.yOff + 1.0 / plotTree.totalD\n\ndef createPlot(inTree):\n    fig = plt.figure(1, facecolor=\'white\')\n    fig.clf()\n    axprops = dict(xticks=[], yticks=[])\n    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)\n    plotTree.totalw = float(getNumLeafs(inTree))\n    plotTree.totalD = float(getTreeDepth(inTree))\n    plotTree.xOff = -0.5 / plotTree.totalw\n    plotTree.yOff = 1.0\n    plotTree(inTree, (0.5, 1.0), \'\')\n    plt.show()\n'"
