file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\nfrom runpy import run_path as run_path\nfrom pathlib import Path\n\nhere = Path(__file__).parent.absolute()\n\nd = run_path(here / ""transonic/_version.py"")\n__version__ = d[""__version__""]\n__about__ = d[""__about__""]\n\nprint(__about__)\n\nsetup(\n    version=__version__,\n    packages=find_packages(exclude=[""doc"", ""tmp"", ""data_tests""]),\n    entry_points={""console_scripts"": [""transonic = transonic.run:run""]},\n)\n'"
_transonic_testing/for_test_exterior_import_jit.py,3,b'import gast\nimport numpy as np\nfrom .for_test_exterior_import_jit_2 import func_import_2\n\n\nconst = 1\nfoo = 1\n\n\ndef func_import_multilevel():\n    return const + np.pi - np.pi + func_import_2()\n\n\ndef func_import():\n    return const + np.pi - np.pi\n\n\ndef func_import2():\n    return const + np.pi - np.pi\n\n\ndef use_gast():\n    print(gast)\n'
_transonic_testing/for_test_exterior_import_jit_2.py,0,b'import gast\nimport numpy as np\n\n\nconst = 1\nfoo = 1\n\n\ndef func_import_2():\n    return const\n\n\ndef use_gast():\n    print(gast)\n'
_transonic_testing/for_test_init.py,4,"b'import numpy as np\n\nfrom transonic import Transonic, boost, Array, Union, const\n\n\n# transonic def func(int, float)\n\n\n@boost\ndef func(a, b):\n    i: int\n    i = 2\n    c: float = 0.1 + i\n    return a + b + c\n\n\ndef func_tmp(arg):\n    return arg ** 2\n\n\ndef func0(a, b):\n    return a + b\n\n\nfunc0_boosted = boost(func0)\n\nA = Union[int, Array[int, ""1d"", ""C""]]\n\n\n@boost\ndef func2(a: A, b: float):\n    return a - func_tmp(b)\n\n\nA1 = Array[int, ""1d"", ""C"", ""memview""]\n\n\n@boost\ndef func3(c: const(A1)):\n    return c[0] + 1\n\n\nts = Transonic()\n\n\ndef func1(a, b):\n    n = 10\n\n    if ts.is_transpiled:\n        result = ts.use_block(""block0"")\n    else:\n        # transonic block (\n        #     float a, b;\n        #     int n\n        # )\n\n        result = 0.0\n        for _ in range(n):\n            result += a ** 2 + b ** 3\n\n\n@boost\nclass Transmitter:\n\n    freq: float\n\n    def __init__(self, freq):\n        self.freq = float(freq)\n\n    @boost\n    def __call__(self, inp: ""float[]""):\n        """"""My docstring""""""\n        return inp * np.exp(np.arange(len(inp)) * self.freq * 1j)\n\n    @boost\n    def other_func(self):\n        return 2 * self.freq\n\n\ndef check_class():\n    inp = np.ones(2)\n    freq = 1.0\n    trans = Transmitter(freq)\n\n    def for_check(freq, inp):\n        return inp * np.exp(np.arange(len(inp)) * freq * 1j)\n\n    assert np.allclose(trans(inp), for_check(freq, inp))\n    assert trans.other_func() == 2 * freq\n\n\nif __name__ == ""__main__"":\n    check_class()\n'"
_transonic_testing/for_test_justintime.py,3,"b'import numpy as np\n\nfrom transonic import jit, boost\nfrom transonic.mpi import Path\n\n\ndef func0(a):\n    return 2 * a\n\n\ndef func():\n    return 1\n\n\nfunc0_jitted = jit(func0)\n\n\n@jit\ndef func1(a: ""int[][] or float[]"", l: ""int list""):\n    tmp = np.exp(sum(l))\n    result = tmp * a * func0(a) + func()\n    return result\n\n\n@jit\ndef func2(a):\n    return a\n\n\n# weird but done on purpose for a better coverage\nPath(__file__).touch()\n\n\n@jit()\ndef func2(a):\n    return a\n\n\n@jit\ndef func_dict(d: ""str: float dict""):\n    return d.popitem()\n\n\n@jit\ndef fib(n: int):\n    """"""fibonacci""""""\n    if n < 2:\n        return n\n    return fib(n - 1) + fib(n - 2)\n\n\n@jit\ndef use_fib():\n    return [fib(n) for n in [1, 3, 5]]\n\n\n@boost\nclass MyClass:\n    def __init__(self):\n        self.attr0 = self.attr1 = 1\n\n    @jit()\n    def myfunc(self, arg):\n        return self.attr1 + self.attr0 + np.abs(arg)\n\n    @jit\n    def myfunc1(self):\n        return self.attr0\n\n    def check(self):\n        assert self.myfunc(1) == 3\n\n\n# FIXME support multilevel imported jitted function call in a jitted function\nfrom .for_test_exterior_import_jit import foo, func_import, func_import2\nfrom numpy import pi\n\nconst = 1\n\njitted_func_import = jit(func_import)\njitted_func_import2 = jit(func_import2)\n\n\n@jit\ndef main(add: int):\n    return (\n        add\n        + foo\n        + pi\n        - pi\n        - jitted_func_import()\n        + func_import2()\n        + func0_jitted(1 / 2)\n        + const\n    )\n\n\n@boost\nclass MyClass2:\n    def __init__(self):\n        self.attr0 = self.attr1 = 1\n\n    @jit()\n    def myfunc(self, arg):\n        return self.attr1 + self.attr0 + np.abs(arg) + func_import()\n\n    def check(self):\n        assert self.myfunc(1) == 4\n'"
data_tests/add_inline.py,0,"b'from transonic import boost\n\nT = int\n\n\n@boost(inline=True)\ndef add(a: T, b: T) -> T:\n    return a + b\n\n\n@boost\ndef use_add(n: int = 10000):\n    tmp: T = 0\n    _: T\n    for _ in range(n):\n        tmp = add(tmp, 1)\n    return tmp\n'"
data_tests/assign_func_boost.py,0,b'from transonic import boost\n\n\ndef func(x: int):\n    return x ** 2\n\n\ndef func2(x: int):\n    return x ** 2\n\n\nfunc_boosted = boost(func)\n'
data_tests/assign_func_jit.py,0,b'from transonic import jit\n\n\ndef func(x):\n    return x ** 2\n\n\ndef func2(x):\n    return x ** 2\n\n\nfunc_jitted = jit(func)\n'
data_tests/block_fluidsim.py,0,"b'from transonic import Transonic\n\nts = Transonic()\n\n\nclass MyClass:\n    def _time_step_RK2(self):\n        dt = self.deltat\n        diss, diss2 = self.exact_linear_coefs.get_updated_coefs()\n\n        compute_tendencies = self.sim.tendencies_nonlin\n        state_spect = self.sim.state.state_spect\n\n        tendencies_n = compute_tendencies()\n\n        state_spect_n12 = self._state_spect_tmp\n\n        if ts.is_transpiled:\n            ts.use_block(""rk2_step0"")\n        else:\n            # transonic block (\n            #     complex128[][][] state_spect_n12, state_spect,\n            #                      tendencies_n;\n            #     float64[][] diss2;\n            #     float dt\n            # )\n            state_spect_n12[:] = (state_spect + dt / 2 * tendencies_n) * diss2\n\n        print(""hello!"")\n'"
data_tests/blocks_type_hints.py,3,"b'import numpy as np\n\nfrom transonic import Transonic, Type, NDim, Array\n\nT = Type(float, complex)\nN = NDim(1, 2)\nA = Array[T, N]\nA1 = Array[T, N + 1]\n\nts = Transonic()\n\n\nclass MyClass:\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n\n    def compute(self, n):\n\n        a = self.a\n        b = self.b\n\n        if ts.is_transpiled:\n            result = ts.use_block(""block0"")\n        else:\n            # transonic block (\n            #     A a; A1 b;\n            #     int n\n            # )\n\n            # transonic block (\n            #     int[:] a, b;\n            #     float n\n            # )\n\n            result = a ** 2 + b.mean() ** 3 + n\n\n        return result\n\n\nif __name__ == ""__main__"":\n\n    shape = 100, 100\n    a = np.random.rand(*shape)\n    b = np.random.rand(*shape)\n\n    obj = MyClass(a, b)\n\n    obj.compute(10)\n\n    if ts.is_transpiled:\n        ret = obj.compute(10)\n        ts.is_transpiled = False\n        ret1 = obj.compute(10)\n        ts.is_transpiled = True\n        assert np.allclose(ret, ret1)\n        print(""allclose OK"")\n'"
data_tests/boosted_class_use_import.py,1,"b'import numpy as np\n\nfrom transonic import boost\nfrom exterior_import_boost import func_import\n\n\n@boost\nclass MyClass2:\n    attr0: int\n    attr1: int\n\n    def __init__(self):\n        self.attr0 = self.attr1 = 1\n\n    @boost\n    def myfunc(self, arg: int):\n        return self.attr1 + self.attr0 + np.abs(arg) + func_import()\n\n    def check(self):\n        assert self.myfunc(1) == 5\n'"
data_tests/boosted_func_use_import.py,1,"b'import numpy as np\n\nfrom transonic import boost\nfrom exterior_import_boost import func_import\n\n# transonic def func(\n#  float[][],\n#  float[][]\n# )\n# transonic def func(int[][], float[][])\n\n\n@boost\ndef func(a, b):\n    return (a * np.log(b)).max() + func_import()\n'"
data_tests/class_blocks.py,6,"b'import numpy as np\n\nfrom transonic import Transonic\n\nts = Transonic()\n\n\nclass MyClass:\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n\n    def compute(self, n):\n\n        a = self.a\n        b = self.b\n\n        if ts.is_transpiled:\n            result = ts.use_block(""block0"")\n        else:\n            # foo\n            # transonic block (\n            #     float[][] a, b;\n            #     int n\n            # ) bar\n            # foo\n\n            # transonic block (\n            #     float[][][] a, b;\n            #     int n\n            # )\n            # foobar\n\n            result = np.zeros_like(a)\n            for _ in range(n):\n                result += a ** 2 + b ** 3\n\n        a = result\n\n        if ts.is_transpiled:\n            result = ts.use_block(""block1"")\n        else:\n            # transonic block (\n            #     float[][] a, b;\n            #     int n\n            # )\n\n            # transonic block (\n            #     float[][][] a, b;\n            #     int n\n            # )\n            result = np.zeros_like(a)\n            for _ in range(n):\n                result += a ** 2 + b ** 3\n\n        return result\n\n\nif __name__ == ""__main__"":\n\n    shape = 2, 2\n    a = np.random.rand(*shape)\n    b = np.random.rand(*shape)\n\n    obj = MyClass(a, b)\n\n    ret0 = obj.compute(10)\n\n    print(\n        ""(is_transpiled, is_compiling, is_compiled)"",\n        (ts.is_transpiled, ts.is_compiling, ts.is_compiled),\n    )\n\n    if ts.is_transpiled:\n        ret = obj.compute(10)\n        assert np.allclose(ret, ret0), ret - ret0\n        ts.is_transpiled = False\n        ret1 = obj.compute(10)\n        ts.is_transpiled = True\n        assert np.allclose(ret, ret1), ret - ret1\n        print(""allclose OK"")\n'"
data_tests/class_rec_calls.py,0,"b'from transonic import boost\n\n\n@boost\nclass Myclass:\n\n    attr: int\n    attr2: int\n\n    def __init__(self):\n        self.attr = 1\n        self.attr2 = 2\n\n    @boost\n    def func(self, arg: int):\n        if self.func(arg - 1) < 1:\n            return 1\n        else:\n            a = self.func(arg - 1) * self.func(arg - 1)\n            return a + self.attr * self.attr2 * arg + self.func(arg - 1)\n'"
data_tests/classic.py,1,"b'import numpy as np\n\nfrom transonic import boost\n\n# transonic def func(\n#  float[][],\n#  float[][]\n# )\n# transonic def func(int[][], float[][])\n\n\n@boost\ndef func(a, b):\n    return (a * np.log(b)).max()\n'"
data_tests/default_params.py,0,"b'from transonic import boost, Optional\n\n\n@boost\ndef func(a: int = 1, b: Optional[str] = None, c: float = 1.0):\n    print(b)\n    return a + c\n'"
data_tests/exterior_import_boost.py,1,b'import gast\nimport numpy as np\nfrom exterior_import_boost_2 import func_import_2\n\n\nconst = 1\nfoo = 1\n\n\ndef func_import():\n    return const + func_import_2() + np.pi - np.pi\n\n\ndef use_gast():\n    print(gast)\n'
data_tests/exterior_import_boost_2.py,0,b'import gast\nimport numpy as np\n\n\nconst = 1\nfoo = 1\n\n\ndef func_import_2():\n    return const\n\n\ndef use_gast():\n    print(gast)\n'
data_tests/methods.py,5,"b'import numpy as np\n\nfrom transonic import boost, Array\n\nA = Array[float, ""2d""]\n\n\n@boost\nclass Transmitter:\n\n    freq: float\n    arr: A\n\n    def __init__(self, freq):\n        self.freq = float(freq)\n        self.arr = np.ones((2, 2))\n\n    @boost\n    def __call__(self, inp: ""float[]""):\n        """"""My docstring""""""\n        return inp * np.exp(np.arange(len(inp)) * self.freq * 1j), self.arr\n\n\nif __name__ == ""__main__"":\n    inp = np.ones(2)\n    freq = 1.0\n    trans = Transmitter(freq)\n\n    def for_check(freq, inp):\n        return inp * np.exp(np.arange(len(inp)) * freq * 1j)\n\n    assert np.allclose(trans(inp), for_check(freq, inp))\n'"
data_tests/mixed_classic_type_hint.py,2,"b'import numpy as np\n\nfrom transonic import boost\n\n# transonic def func(float[][], float[][])\n# transonic def func(int[][], float[][])\n\n\n@boost\ndef func(a: float, b: float):\n    return (a * np.log(b)).max()\n\n\n@boost\ndef func1(a: int, b: float):\n    return a * np.cos(b)\n'"
data_tests/no_arg.py,0,b'from transonic import boost\n\n\n@boost\ndef func():\n    return 1\n\n\n# transonic def func2()\n@boost\ndef func2():\n    return 1\n'
data_tests/no_pythran_.py,0,b'a = 2\n\n\ndef myfunc():\n    print(a)\n'
data_tests/row_sum_boost.py,5,"b'import numpy as np\n\nfrom transonic import boost, Array\n\nT_index = np.int32\n# we use a type variable because it can be replaced by a fused type.\nT = np.int64\nA1d_i = Array[T_index, ""1d""]\nA1d = Array[T, ""1d""]\nA2d = Array[T, ""2d""]\nV1d_i = Array[T_index, ""1d"", ""memview""]\nV1d = Array[T, ""1d"", ""memview""]\nV2d = Array[T, ""2d"", ""memview""]\n\n\n@boost\ndef row_sum(arr: A2d, columns: A1d_i):\n    return arr.T[columns].sum(0)\n\n\n@boost(boundscheck=False, wraparound=False, cdivision=True, nonecheck=False)\ndef row_sum_loops(arr: V2d, columns: V1d_i):\n    # locals type annotations are used only for Cython\n    i: T_index\n    j: T_index\n    sum_: T\n    # arr.dtype not supported for memoryview\n    dtype = type(arr[0, 0])\n    res: V1d = np.empty(arr.shape[0], dtype=dtype)\n    for i in range(arr.shape[0]):\n        sum_ = dtype(0)\n        for j in range(columns.shape[0]):\n            sum_ += arr[i, columns[j]]\n        res[i] = sum_\n    return res\n\n\nif __name__ == ""__main__"":\n\n    from util import check, bench\n\n    functions = [row_sum, row_sum_loops]\n    arr = np.arange(1_000_000).reshape(1_000, 1_000)\n    columns = np.arange(1, 1000, 2, dtype=T_index)\n\n    check(functions, arr, columns)\n    bench(functions, arr, columns)\n'"
data_tests/type_hint_notemplate.py,4,"b'from functools import partial\n\nimport numpy as np\n\nimport transonic as ts\nfrom transonic import Type, NDim, Array, Union\n\nT = Type(int, np.complex128)\nN = NDim(1, 3)\n\nA = Array[T, N]\nA1 = Array[np.float32, N + 1]\n\nA3d = Array[np.float32, ""3d""]\nN1 = NDim(4, 5)\nN1 = NDim(4, 5)\n\nT = Type(int, np.complex128)\n\n\n@ts.boost\ndef compute(a: A, b: A, c: T, d: Union[A, A1], e: str):\n    print(e)\n    tmp = a + b\n    if 1 and 2:\n        tmp *= 2\n    return tmp\n\n\nmain = partial(lambda x: x, lambda x: x)\n'"
doc/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Transonic documentation build configuration file, copied from a file created by\n# sphinx-quickstart on Sun Mar  2 12:15:31 2014.\n#\n# This file is execfile() with the current directory set to its containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# import sys\n# import os\n\nimport transonic\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n# sys.path.insert(0, os.path.abspath(\'../scripts\'))\n# sys.path.insert(0, os.path.abspath(\'./\'))\n\n# -- General configuration ----------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.doctest"",\n    ""sphinx.ext.todo"",\n    # \'sphinx.ext.pngmath\',\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.viewcode"",\n    ""sphinx.ext.autosummary"",\n    ""numpydoc"",\n    ""nbsphinx"",\n    # \'mathmacro\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# The suffix of source filenames.\nsource_suffix = ["".rst"", "".md""]\n\nsource_parsers = {"".md"": ""recommonmark.parser.CommonMarkParser""}\n\n# The encoding of source files.\n# source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = ""index""\n\n# General information about the project.\nproject = u""Transonic""\ncopyright = u""2018, Pierre Augier""\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n\n# The full version, including alpha/beta/rc tags.\nrelease = transonic.__version__\n# The short X.Y version.\nversion = release.split(""."")\nversion = ""{}.{}.{}"".format(version[0], version[1], version[2])\n\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n# language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n# today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\n    ""_build"",\n    ""**.ipynb_checkpoints"",\n    ""for_dev/cython_bugs/**/*.md"",\n    ""for_dev/scikit-image/README.md"",\n]\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n# default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = ""sphinx""\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n\n# -- Options for HTML output --------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n# html_theme = \'default\'\nhtml_theme = ""sphinx_rtd_theme""\n\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n# html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n# html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\nhtml_short_title = ""Transonic "" + release\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n# html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n# html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = []\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n# html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n# html_domain_indices = True\n\n# If false, no index is generated.\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n# html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n# html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n# html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n# html_file_suffix = None\n\n# Output file base name for HTML help builder.\n# htmlhelp_basename = \'fluidfftdoc\'\n\n\n# -- Options for LaTeX output --------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\'papersize\': \'letterpaper\',\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\'pointsize\': \'10pt\',\n    # Additional stuff for the LaTeX preamble.\n    #\'preamble\': \'\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass [howto/manual]).\nlatex_documents = [\n    (\n        ""index"",\n        ""transonic.tex"",\n        u""Transonic Documentation"",\n        u""Pierre Augier"",\n        ""manual"",\n    )\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n# latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n# latex_appendices = []\n\n# If false, no module index is generated.\n# latex_domain_indices = True\n\n\n# -- Options for manual page output --------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (""index"", ""Transonic"", u""Transonic Documentation"", [u""Pierre Augier""], 1)\n]\n\n# If true, show URL addresses after external links.\n# man_show_urls = False\n\n\n# -- Options for Texinfo output ------------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        ""index"",\n        ""Transonic"",\n        u""Transonic Documentation"",\n        u""Pierre Augier"",\n        ""Transonic"",\n        ""One line description of project."",\n        ""Miscellaneous"",\n    )\n]\n\n# Documents to append as an appendix to all manuals.\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n# texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n# texinfo_show_urls = \'footnote\'\n\n\n# -- Other options ---------------------------------------------------------\n\nnumpydoc_show_class_members = False\n\nautosummary_generate = True\nautodoc_default_options = {""show-inheritance"": None}\nautodoc_member_order = ""bysource""\n\ntodo_include_todos = True\n'"
tmp/cython_class.py,0,"b'from transonic import boost\n\n@boost\nclass MyClass:\n\n    a : int\n\n    def __init__(self, a):\n        self.a = a\n    @boost\n    def method(self, b : int):\n        return self.a + b\n    \n    @boost\n    def method2(self, b : int):\n        return self.a + b\n@boost\nclass MyClass2:\n\n    a : int\n\n    def __init__(self, a):\n        self.a = a\n    @boost\n    def method(self, b : int):\n        o : int\n        o = 1\n        return self.a + b + o\n\n    @boost\n    def method2(self, b : int):\n        return self.a + b\n\n    def method3(self, b : int):\n        return self.a + b'"
tmp/cython_jit.py,0,"b'from transonic import jit\n\n\n@jit\ndef func(a,b):\n    return a+b\n\nfunc(0,1)'"
tmp/cython_multitype_example.py,2,"b'from transonic import boost, Type\nimport numpy as np\n\nT = Type(int, float)\nT2 = Type(float, complex)\n\n@boost\ndef func(m : T, n : T2):\n    return np.add(1,n)\n\n@boost\ndef func1(m : T, n : float = 2):\n    t : int = 5.5\n    return np.add(1,n) + t\n'"
tmp/detect_extensions.py,0,"b'from transonic.dist import init_transonic_extensions\n\n\nres = init_transonic_extensions(""transonic"", ""cython"")\nprint(res)'"
transonic/__init__.py,0,"b'from transonic._version import __version__\n\nfrom transonic.aheadoftime import Transonic, boost\nfrom transonic.backends import set_backend_for_this_module\nfrom transonic.config import set_backend\nfrom transonic.compiler import wait_for_all_extensions\nfrom transonic.justintime import jit, set_compile_jit\nfrom transonic.util import set_compile_at_import\nfrom transonic.typing import (\n    Array,\n    NDim,\n    Type,\n    Union,\n    List,\n    Tuple,\n    Dict,\n    Set,\n    str2type,\n    typeof,\n    Optional,\n    const,\n)\n\n\n__all__ = [\n    ""__version__"",\n    ""Transonic"",\n    ""boost"",\n    ""jit"",\n    ""Array"",\n    ""NDim"",\n    ""Type"",\n    ""List"",\n    ""Dict"",\n    ""Tuple"",\n    ""Set"",\n    ""Union"",\n    ""Optional"",\n    ""set_backend"",\n    ""set_backend_for_this_module"",\n    ""set_compile_jit"",\n    ""set_compile_at_import"",\n    ""str2type"",\n    ""typeof"",\n    ""wait_for_all_extensions"",\n]\n'"
transonic/_version.py,0,"b'__version__ = ""0.4.2""\n\ntry:\n    from pyfiglet import figlet_format\n\n    __about__ = figlet_format(""transonic"", font=""big"")\nexcept ImportError:\n    __about__ = r""""""\n _                                   _\n| |                                 (_)\n| |_ _ __ __ _ _ __  ___  ___  _ __  _  ___\n| __| \'__/ _` | \'_ \\/ __|/ _ \\| \'_ \\| |/ __|\n| |_| | | (_| | | | \\__ \\ (_) | | | | | (__\n \\__|_|  \\__,_|_| |_|___/\\___/|_| |_|_|\\___|\n\n""""""\n\n__about__ = __about__.rstrip() + f""{17 * \' \'} v. {__version__}\\n""\n'"
transonic/aheadoftime.py,0,"b'""""""User runtime API for the ahead-of-time compilation\n=====================================================\n\nUser API\n--------\n\n.. autofunction:: boost\n\n.. autoclass:: Transonic\n   :members:\n   :private-members:\n\nInternal API\n------------\n\n.. autofunction:: _get_transonic_calling_module\n\n.. autoclass:: CheckCompiling\n   :members:\n   :private-members:\n\n""""""\n\nimport inspect\nimport time\nimport subprocess\nimport os\nimport functools\nimport sys\n\nfrom transonic.backends import backends, get_backend_name_module\nfrom transonic.config import has_to_replace, backend_default\nfrom transonic.log import logger\nfrom transonic import mpi\nfrom transonic.mpi import Path\n\nfrom transonic.util import (\n    get_module_name,\n    has_to_compile_at_import,\n    import_from_path,\n    has_to_build,\n    modification_date,\n    is_method,\n    write_if_has_to_write,\n    find_module_name_from_path,\n)\n\nif mpi.nb_proc == 1:\n    mpi.has_to_build = has_to_build\n    mpi.modification_date = modification_date\n\nis_transpiling = False\nmodules_backends = {backend_name: {} for backend_name in backends.keys()}\nmodules = modules_backends[backend_default]\n\n\ndef _get_transonic_calling_module(backend_name: str = None, index_frame: int = 2):\n    """"""Get the Transonic instance corresponding to the calling module\n\n    Parameters\n    ----------\n\n    index_frame : int\n\n      Index (in :code:`inspect.stack()`) of the frame to be selected\n\n    """"""\n\n    try:\n        frame = inspect.stack()[index_frame]\n    except IndexError:\n        print(""index_frame"", index_frame)\n        print([frame[1] for frame in inspect.stack()])\n        raise\n\n    module_name = get_module_name(frame)\n\n    if backend_name is None:\n        backend_name = get_backend_name_module(module_name)\n\n    modules = modules_backends[backend_name]\n\n    if module_name in modules:\n        ts = modules[module_name]\n        if (\n            ts.is_transpiling != is_transpiling\n            or ts._compile_at_import_at_creation != has_to_compile_at_import()\n            or (\n                hasattr(ts, ""path_mod"")\n                and ts.path_backend.exists()\n                and mpi.has_to_build(ts.path_backend, ts.path_mod)\n            )\n        ):\n            ts = Transonic(frame=frame, reuse=False, backend=backend_name)\n    else:\n        ts = Transonic(frame=frame, reuse=False, backend=backend_name)\n\n    return ts\n\n\ndef boost(\n    obj=None,\n    backend: str = None,\n    inline=False,\n    boundscheck=True,\n    wraparound=True,\n    cdivision=False,\n    nonecheck=True,\n    nogil=False,\n):\n    """"""Decorator to declare that an object can be accelerated\n\n    Parameters\n    ----------\n\n    obj: a function, a method or a class\n\n    """"""\n    if backend is not None and not isinstance(backend, str):\n        raise TypeError\n\n    ts = _get_transonic_calling_module(backend_name=backend)\n\n    decor = ts.boost(\n        inline=inline,\n        nogil=nogil,\n        boundscheck=boundscheck,\n        wraparound=wraparound,\n        cdivision=cdivision,\n        nonecheck=nonecheck,\n    )\n    if callable(obj) or isinstance(obj, type):\n        return decor(obj)\n    else:\n        return decor\n\n\nclass CheckCompiling:\n    """"""Check if the module is being compiled and replace the module and the function""""""\n\n    def __init__(self, ts, func):\n        self.has_been_replaced = False\n        self.ts = ts\n        self.func = func\n\n    def __call__(self, *args, **kwargs):\n\n        if self.has_been_replaced:\n            return self.func(*args, **kwargs)\n\n        ts = self.ts\n        if ts.is_compiling and not ts.process.is_alive(raise_if_error=True):\n            ts.is_compiling = False\n            time.sleep(0.1)\n            ts.module_backend = import_from_path(\n                ts.path_extension, ts.module_backend.__name__\n            )\n            assert ts.backend.check_if_compiled(self.ts.module_backend)\n            ts.is_compiled = True\n\n        if not ts.is_compiling:\n            self.func = getattr(ts.module_backend, self.func.__name__)\n            self.has_been_replaced = True\n\n        return self.func(*args, **kwargs)\n\n\nclass Transonic:\n    """"""\n    Representation of a module using ahead-of-time transonic commands\n\n    Parameters\n    ----------\n\n    use_transonified : bool (optional, default True)\n\n      If False, don\'t use the pythranized versions at run time\n\n    frame : int (optional)\n\n      (Internal) Index (in :code:`inspect.stack()`) of the frame to be selected\n\n    reuse : bool (optional, default True)\n\n      (Internal) If True, do not recreate an instance.\n\n    """"""\n\n    def __init__(\n        self, use_transonified=True, frame=None, reuse=True, backend=None\n    ):\n\n        if frame is None:\n            frame = inspect.stack()[1]\n\n        self.module_name = module_name = get_module_name(frame)\n\n        if backend is None:\n            backend = get_backend_name_module(module_name)\n        if isinstance(backend, str):\n            backend = backends[backend]\n        self.backend = backend\n        modules = modules_backends[backend.name]\n\n        self._compile_at_import_at_creation = has_to_compile_at_import()\n\n        if reuse and module_name in modules:\n            ts = modules[module_name]\n            for key, value in ts.__dict__.items():\n                self.__dict__[key] = value\n            return\n\n        self.is_transpiling = is_transpiling\n        self.has_to_replace = has_to_replace\n\n        if is_transpiling:\n            self.functions = {}\n            self.classes = {}\n            self.signatures_func = {}\n            self.is_transpiled = False\n            self.is_compiled = False\n            return\n\n        self.is_compiling = False\n\n        if not use_transonified or not has_to_replace:\n            self.is_transpiled = False\n            self.is_compiled = False\n            return\n\n        if ""."" in module_name:\n            package, module_short_name = module_name.rsplit(""."", 1)\n            module_backend_name = package + "".""\n        else:\n            module_short_name = module_name\n            module_backend_name = """"\n\n        module_backend_name += f""__{backend.name}__."" + module_short_name\n\n        self.path_mod = path_mod = Path(frame.filename)\n\n        suffix = "".py""\n        self.path_backend = path_backend = (\n            path_mod.parent / f""__{backend.name}__"" / (module_short_name + suffix)\n        )\n\n        path_ext = None\n\n        if has_to_compile_at_import() and path_mod.exists():\n            if mpi.has_to_build(path_backend, path_mod):\n                if path_backend.exists():\n                    time_backend = mpi.modification_date(path_backend)\n                else:\n                    time_backend = 0\n\n                returncode = None\n                if mpi.rank == 0:\n                    print(f""Running transonic on file {path_mod}... "", end="""")\n                    # better to do this in another process because the file is already run...\n                    os.environ[""TRANSONIC_NO_MPI""] = ""1""\n                    returncode = subprocess.call(\n                        [\n                            sys.executable,\n                            ""-m"",\n                            ""transonic.run"",\n                            ""-nc"",\n                            str(path_mod),\n                        ]\n                    )\n                    del os.environ[""TRANSONIC_NO_MPI""]\n                returncode = mpi.bcast(returncode)\n\n                if returncode != 0:\n                    raise RuntimeError(\n                        f""transonic does not manage to produce the {backend.name_capitalized} ""\n                        f""file for {path_mod}""\n                    )\n\n                if mpi.rank == 0:\n                    print(""Done!"")\n\n                path_ext = path_backend.with_name(\n                    backend.name_ext_from_path_backend(path_backend)\n                )\n\n                time_backend_after = mpi.modification_date(path_backend)\n                # We have to touch the files to signal that they are up-to-date\n                if time_backend_after == time_backend and mpi.rank == 0:\n                    if not has_to_build(path_ext, path_backend):\n                        path_backend.touch()\n                        if path_ext.exists():\n                            path_ext.touch()\n                    else:\n                        path_backend.touch()\n\n        path_ext = path_ext or path_backend.with_name(\n            backend.name_ext_from_path_backend(path_backend)\n        )\n\n        self.path_extension = path_ext\n        if (\n            has_to_compile_at_import()\n            and path_mod.exists()\n            and not self.path_extension.exists()\n        ):\n            if mpi.rank == 0:\n                print(\n                    f""Launching {backend.name_capitalized} to compile a new extension...""\n                )\n            self.is_compiling, self.process = backend.compile_extension(\n                path_backend, name_ext_file=self.path_extension.name\n            )\n            self.is_compiled = not self.is_compiling\n\n        self.is_transpiled = True\n\n        if not path_ext.exists() and not self.is_compiling:\n            path_ext_alt = path_backend.with_suffix(backend.suffix_extension)\n            if path_ext_alt.exists():\n                self.path_extension = path_ext = path_ext_alt\n\n        self.reload_module_backend(module_backend_name)\n\n        if not self.is_transpiled:\n            logger.warning(\n                f""Module {path_mod} has not been compiled for ""\n                f""Transonic-{backend.name_capitalized}""\n            )\n        else:\n            self.is_compiled = backend.check_if_compiled(self.module_backend)\n            if self.is_compiled:\n                module = inspect.getmodule(frame[0])\n                # module can be None if (at least) it has been run with runpy\n                if module is not None:\n                    if backend.name == ""pythran"":\n                        module.__pythran__ = self.module_backend.__pythran__\n                    module.__transonic__ = self.module_backend.__transonic__\n\n            if hasattr(self.module_backend, ""arguments_blocks""):\n                self.arguments_blocks = getattr(\n                    self.module_backend, ""arguments_blocks""\n                )\n\n        modules[module_name] = self\n\n    def reload_module_backend(self, module_backend_name=None):\n        if module_backend_name is None:\n            module_backend_name = self.module_backend.__name__\n        if self.path_extension.exists() and not self.is_compiling:\n            self.module_backend = import_from_path(\n                self.path_extension, module_backend_name\n            )\n        elif self.path_backend.exists():\n            self.module_backend = import_from_path(\n                self.path_backend, module_backend_name\n            )\n        else:\n            self.is_transpiled = False\n            self.is_compiled = False\n\n    def transonic_def(self, func):\n        """"""Decorator used for functions\n\n        Parameters\n        ----------\n\n        func: a function\n\n        """"""\n        if is_method(func):\n            return self.transonic_def_method(func)\n\n        if is_transpiling or not has_to_replace or not self.is_transpiled:\n            return func\n\n        if not hasattr(self.module_backend, func.__name__):\n            self.reload_module_backend()\n\n        try:\n            func_tmp = getattr(self.module_backend, func.__name__)\n        except AttributeError:\n            # TODO: improve what happens in this case\n            logger.warning(\n                f""{self.backend.name_capitalized} file does not seem to be up-to-date:\\n""\n                f""{self.module_backend}\\nfunc: {func.__name__}""\n            )\n            func_tmp = func\n\n        if self.is_compiling:\n            return functools.wraps(func)(CheckCompiling(self, func_tmp))\n\n        return func_tmp\n\n    def transonic_def_method(self, func):\n        """"""Decorator used for methods\n\n        Parameters\n        ----------\n\n        func: a function\n\n        """"""\n\n        if is_transpiling or not has_to_replace or not self.is_transpiled:\n            return func\n\n        return TransonicTemporaryMethod(func)\n\n    def boost(self, **kwargs):\n        """"""Universal decorator for AOT compilation\n\n        Used for functions, methods and classes.\n        """"""\n        return self._boost_decor\n\n    def _boost_decor(self, obj):\n        """"""Universal decorator for AOT compilation\n\n        Used for functions, methods and classes.\n        """"""\n        if isinstance(obj, type):\n            return self.transonic_class(obj)\n        else:\n            return self.transonic_def(obj)\n\n    def transonic_class(self, cls: type):\n        """"""Decorator used for classes\n\n        Parameters\n        ----------\n\n        cls: a class\n\n        """"""\n        if is_transpiling:\n            return cls\n\n        jit_methods = {\n            key: value\n            for key, value in cls.__dict__.items()\n            if isinstance(value, TransonicTemporaryJITMethod)\n        }\n\n        if jit_methods:\n            cls = jit_class(cls, jit_methods, self.backend)\n\n        if not has_to_replace or not self.is_transpiled:\n            return cls\n\n        cls_name = cls.__name__\n\n        for key, value in cls.__dict__.items():\n            if not isinstance(value, TransonicTemporaryMethod):\n                continue\n            func = value.func\n            func_name = func.__name__\n\n            name_backend_func = f""__for_method__{cls_name}__{func_name}""\n            name_var_code_new_method = (\n                f""__code_new_method__{cls_name}__{func_name}""\n            )\n\n            if not hasattr(self.module_backend, name_backend_func):\n                self.reload_module_backend()\n\n            try:\n                backend_func = getattr(self.module_backend, name_backend_func)\n                code_new_method = getattr(\n                    self.module_backend, name_var_code_new_method\n                )\n            except AttributeError:\n                # TODO: improve what happens in this case\n                raise RuntimeError(\n                    f""{self.backend.name_capitalized} file does not seem to be up-to-date.""\n                )\n                # setattr(cls, key, func)\n            else:\n                namespace = {""backend_func"": backend_func}\n                exec(code_new_method, namespace)\n                setattr(cls, key, functools.wraps(func)(namespace[""new_method""]))\n        return cls\n\n    def use_block(self, name):\n        """"""Use the pythranized version of a code block\n\n        Parameters\n        ----------\n\n        name : str\n\n          The name of the block.\n\n        """"""\n        if not self.is_transpiled:\n            raise ValueError(\n                ""`use_block` has to be used protected by `if ts.is_transpiled`""\n            )\n\n        if self.is_compiling and not self.process.is_alive(raise_if_error=True):\n            self.is_compiling = False\n            time.sleep(0.1)\n            self.module_backend = import_from_path(\n                self.path_extension, self.module_backend.__name__\n            )\n            assert self.backend.check_if_compiled(self.module_backend)\n            self.is_compiled = True\n\n        func = getattr(self.module_backend, name)\n        argument_names = self.arguments_blocks[name]\n\n        frame = inspect.currentframe()\n        try:\n            locals_caller = frame.f_back.f_locals\n        finally:\n            del frame\n\n        arguments = [locals_caller[name] for name in argument_names]\n        return func(*arguments)\n\n\nclass TransonicTemporaryMethod:\n    """"""Internal temporary class for methods""""""\n\n    def __init__(self, func):\n        self.func = func\n\n    def __call__(self, self_bis, *args, **kwargs):\n        raise RuntimeError(\n            ""Did you forget to decorate a class using methods decorated ""\n            ""with transonic? Please decorate it with @boost.""\n        )\n\n\nclass TransonicTemporaryJITMethod:\n    """"""Internal temporary class for JIT methods""""""\n\n    __transonic__ = ""jit_method""\n\n    def __init__(self, func, native, xsimd, openmp):\n        self.func = func\n        self.native = native\n        self.xsimd = xsimd\n        self.openmp = openmp\n\n    def __call__(self, self_bis, *args, **kwargs):\n        raise RuntimeError(\n            ""Did you forget to decorate a class using methods decorated ""\n            ""with transonic? Please decorate it with @boost.""\n        )\n\n\ndef jit_class(cls, jit_methods, backend):\n    """"""Modify the class by replacing jit methods\n\n    1. create a Python file with @jit functions and methods\n    2. import the file\n    3. replace the methods\n\n    """"""\n    if not has_to_replace:\n        return cls\n\n    cls_name = cls.__name__\n    mod_name = cls.__module__\n    module = sys.modules[mod_name]\n\n    if mod_name == ""__main__"":\n        mod_name = find_module_name_from_path(module.__file__)\n\n    path_jit_class = mpi.Path(backend.jit.path_class)\n\n    # 1. create a Python file with @jit functions and methods\n    python_path_dir = path_jit_class / mod_name.replace(""."", os.path.sep)\n    python_path = python_path_dir / (cls_name + "".py"")\n\n    if mpi.has_to_build(python_path, module.__file__):\n        from transonic.justintime import _get_module_jit\n\n        mod = _get_module_jit(backend_name=backend.name, index_frame=5)\n        if mpi.rank == 0:\n            python_path = mpi.PathSeq(python_path)\n            python_code = (\n                mod.info_analysis[""codes_dependance_classes""][cls_name] + ""\\n""\n            )\n            python_code += backend.jit.produce_code_class(cls)\n            write_if_has_to_write(python_path, python_code)\n            python_path = mpi.Path(python_path)\n        mpi.barrier()\n\n    # 2. import the file\n    python_mod_name = path_jit_class.name + ""."" + mod_name + ""."" + cls_name\n    module = import_from_path(python_path, python_mod_name)\n\n    # 3. replace the methods\n    for name_method, method in jit_methods.items():\n        func = method.func\n        name_new_method = f""__new_method__{cls.__name__}__{name_method}""\n        new_method = getattr(module, name_new_method)\n        setattr(cls, name_method, functools.wraps(func)(new_method))\n\n    return cls\n'"
transonic/compiler.py,0,"b'""""""Use Pythran to create extensions\n===================================\n\nUser API\n--------\n\n.. autofunction:: wait_for_all_extensions\n\nInternal API\n------------\n\n.. autofunction:: make_hex\n\n.. autoclass:: SchedulerPopen\n   :members:\n   :private-members:\n\n.. autofunction:: compile_extension\n\n""""""\n\nimport multiprocessing\nimport subprocess\nimport time\nfrom typing import Union, Optional\nimport sysconfig\nimport hashlib\nimport sys\nimport os\nfrom datetime import datetime\n\nfrom transonic import mpi\nfrom transonic.mpi import Path, PathSeq\nfrom transonic.log import logger\n\next_suffix = sysconfig.get_config_var(""EXT_SUFFIX"") or "".so""\n\n\ndef modification_date(filename):\n    """"""Get the modification date of a file""""""\n    return datetime.fromtimestamp(os.path.getmtime(str(filename)))\n\n\ndef has_to_build(output_file: Path, input_file: Path):\n    """"""Check if a file has to be (re)built""""""\n    output_file = PathSeq(output_file)\n    input_file = PathSeq(input_file)\n    if not output_file.exists():\n        return True\n    mod_date_output = modification_date(output_file)\n    if mod_date_output < modification_date(input_file):\n        return True\n    return False\n\n\ndef make_hex(src):\n    """"""Produce a hash from a sting""""""\n    return hashlib.md5(src.encode(""utf8"")).hexdigest()\n\n\nclass SchedulerPopen:\n    """"""Limit the number of compilations performed in parallel\n\n    """"""\n\n    deltat = 0.2\n\n    def __init__(self, parallel=True):\n        if mpi.rank > 0:\n            return\n        self.processes = []\n        if parallel:\n            self.limit_nb_processes = max(1, multiprocessing.cpu_count() // 2)\n        else:\n            self.limit_nb_processes = 1\n\n    def block_until_avail(self, parallel=True):\n\n        if mpi.rank == 0:\n            if parallel:\n                limit = self.limit_nb_processes\n            else:\n                limit = 1\n\n            while len(self.processes) >= limit:\n                time.sleep(self.deltat)\n                self.processes = [\n                    process\n                    for process in self.processes\n                    if process.is_alive_root()\n                ]\n\n        mpi.barrier(timeout=None)\n\n    def wait_for_all_extensions(self):\n        """"""Wait until all compilation processes are done""""""\n        if mpi.rank == 0:\n            while self.processes:\n                time.sleep(self.deltat)\n                self.processes = [\n                    process\n                    for process in self.processes\n                    if process.is_alive_root()\n                ]\n\n        mpi.barrier(timeout=None)\n\n    def compile_extension(\n        self,\n        path: Path,\n        backend: str,\n        name_ext_file: str,\n        native=False,\n        xsimd=False,\n        openmp=False,\n        str_accelerator_flags: Optional[str] = None,\n        parallel=True,\n        force=True,\n    ):\n\n        if not force:\n            path_out = path.with_name(name_ext_file)\n            if not has_to_build(path_out, path):\n                logger.warning(\n                    f""Do not {backend}ize {path} because it seems up-to-date ""\n                    ""(but the compilation options may have changed). ""\n                    ""You can force the compilation with the option -f.""\n                )\n                return\n\n        if mpi.rank == 0:\n            logger.info(f""Schedule {backend}ization of file {path}"")\n\n        if str_accelerator_flags is not None:\n            flags = str_accelerator_flags.strip().split()\n        else:\n            flags = []\n\n        def update_flags(flag):\n            if flag not in flags:\n                flags.append(flag)\n\n        if native and os.name != ""nt"":\n            update_flags(""-march=native"")\n\n        if xsimd:\n            update_flags(""-DUSE_XSIMD"")\n\n        if openmp:\n            update_flags(""-fopenmp"")\n\n        if logger.is_enable_for(""debug""):\n            update_flags(""-v"")\n\n        words_command = [\n            sys.executable,\n            ""-m"",\n            ""transonic_cl.run_backend"",\n            path.name,\n            ""-b"",\n            backend,\n        ]\n\n        words_command.extend((""-o"", name_ext_file))\n\n        words_command.extend(flags)\n\n        cwd = path.parent\n\n        self.block_until_avail(parallel)\n\n        process = None\n        if mpi.rank == 0:\n            stdout = stderr = subprocess.PIPE\n            process = subprocess.Popen(\n                words_command,\n                cwd=cwd,\n                stdout=stdout,\n                stderr=stderr,\n                universal_newlines=True,\n            )\n\n        process = mpi.ShellProcessMPI(process)\n\n        if mpi.rank == 0:\n            self.processes.append(process)\n        return process\n\n\nscheduler = SchedulerPopen()\n\n\ndef wait_for_all_extensions():\n    """"""Wait until all compilation processes are done""""""\n    scheduler.wait_for_all_extensions()\n\n\ndef compile_extension(\n    path: Union[Path, str],\n    backend: str,\n    name_ext_file: str,\n    native=False,\n    xsimd=False,\n    openmp=False,\n    str_accelerator_flags: Optional[str] = None,\n    parallel=False,\n    force=False,\n):\n    print(""compile extension"")\n    if not isinstance(path, Path):\n        path = Path(path)\n\n    # return the process\n    return scheduler.compile_extension(\n        path,\n        backend,\n        name_ext_file,\n        native=native,\n        xsimd=xsimd,\n        openmp=openmp,\n        str_accelerator_flags=str_accelerator_flags,\n        parallel=parallel,\n        force=force,\n    )\n'"
transonic/config.py,0,"b'""""""Configuration\n================\n\nTransonic is sensible to the environment variables:\n\n- :code:`TRANSONIC_DIR` can be set to control where the cached files are\n  saved.\n\n- :code:`TRANSONIC_COMPILE_AT_IMPORT` can be set to enable a mode for which\n  Transonic compiles at import time the Pythran file associated with the\n  imported module. This behavior can also be triggered programmatically by using\n  the function :code:`set_compile_at_import`.\n\n- :code:`TRANSONIC_NO_REPLACE` can be set to disable all code replacements.\n  This is useful only when measuring code coverage.\n\n- :code:`FLUID_COMPILE_JIT` can be set to false to disable the\n  compilation of jited functions. This can be useful for unittests.\n\nBye the way, for performance, it is important to configure Pythran with a file\n`~/.pythranrc\n<https://pythran.readthedocs.io/en/latest/MANUAL.html#customizing-your-pythranrc>`_:\n\nFor Linux, I use:\n\n.. code:: raw\n\n    [pythran]\n    complex_hook = True\n\n    [compiler]\n    CXX=clang++-6.0\n    CC=clang-6.0\n    blas=openblas\n\nUser API\n--------\n\n.. autofunction:: set_backend\n\n""""""\n\nimport os\nfrom pathlib import Path\nfrom distutils.util import strtobool\nfrom warnings import warn\n\npath_root = Path(os.environ.get(""TRANSONIC_DIR"", Path.home() / "".transonic""))\n\nhas_to_replace = not strtobool(os.environ.get(""TRANSONIC_NO_REPLACE"", ""0""))\n\nbackend_default = ""pythran""\nbackend_set_by_user = False\n\n\ndef set_backend(backend: str):\n    """"""Set the ""global variable"" backend_default""""""\n\n    backend = backend.lower()\n    supported_backends = [""pythran"", ""cython"", ""numba"", ""python""]\n    if backend not in supported_backends:\n        raise ValueError(f""backend {backend} not supported"")\n\n    global backend_default, backend_set_by_user\n\n    backend_default = backend\n    backend_set_by_user = True\n\n    # warning: this import has to be here!\n    from transonic.util import can_import_accelerator\n\n    if not can_import_accelerator(backend):\n        warn(f\'Backend set to ""{backend}"" but accelerator not importable\')\n\n\ntry:\n    _backend = os.environ[""TRANSONIC_BACKEND""]\nexcept KeyError:\n    pass\nelse:\n    set_backend(_backend)\n'"
transonic/dist.py,0,"b'""""""Utilities for the setup.py files\n===================================\n\nUser API\n--------\n\nProvides the classes PythranBuildExt and PythranExtension to be used in the\nsetup.py.\n\n.. autofunction:: detect_transonic_extensions\n\n.. autofunction:: init_transonic_extensions\n\n""""""\n\nimport os\nimport sys\nfrom pathlib import Path\nfrom distutils.sysconfig import get_config_var\nfrom typing import Iterable\nfrom concurrent.futures import ThreadPoolExecutor as Pool\nimport re\n\nfrom distutils.command.build_ext import build_ext as DistutilsBuildExt\n\ntry:\n    from Cython.Distutils.build_ext import build_ext as CythonBuildExt\n    from Cython.Build import cythonize\nexcept ImportError:\n    build_ext_classes = [DistutilsBuildExt]\n    can_import_cython = False\nelse:\n    build_ext_classes = [CythonBuildExt]\n    can_import_cython = True\n\ntry:\n    from pythran.dist import PythranBuildExt, PythranExtension\nexcept ImportError:\n    PythranBuildExt = object\n    PythranExtension = object\n    can_import_pythran = False\nelse:\n    build_ext_classes.insert(0, PythranBuildExt)\n    can_import_pythran = True\n\nfrom .util import modification_date\n\nfrom transonic.config import backend_default\nfrom transonic.backends import make_backend_files, backends\nfrom transonic.util import can_import_accelerator\n\n__all__ = [\n    ""PythranBuildExt"",\n    ""PythranExtension"",\n    ""can_import_pythran"",\n    ""detect_transonic_extensions"",\n    ""init_pythran_extensions"",\n    ""init_transonic_extensions"",\n    ""get_logger"",\n    ""ParallelBuildExt"",\n    ""make_backend_files"",\n]\n\n\ndef get_logger(name):\n    """"""Returns a logger instance using ``colorlog`` package if available; else\n    defaults to ``logging`` standard library.\n\n    """"""\n    try:\n        import colorlog as logging\n\n        handler = logging.StreamHandler()\n        handler.setFormatter(\n            logging.ColoredFormatter(""%(log_color)s%(levelname)s: %(message)s"")\n        )\n    except ImportError:\n        import logging\n\n        handler = logging.StreamHandler()\n\n    logger = logging.getLogger(name)\n    logger.addHandler(handler)\n    return logger\n\n\ndef detect_transonic_extensions(\n    name_package: str, backend: str = backend_default\n) -> Iterable[str]:\n    """"""Recursively scans a package for Pythran extensions to build, and returns a\n    list of strings, where each string is a module name. The package should be\n    present in the current working directory.\n\n    """"""\n    if backend != ""numba"" and not can_import_accelerator(backend):\n        return []\n    ext_names = []\n    if not os.path.exists(str(name_package)):\n        raise FileNotFoundError(f""Check the name of the package: {name_package}"")\n\n    backend = backends[backend]\n    if backend.suffix_extension == "".py"":\n        # we have to filter out the ""extensions""\n        pattern = re.compile(""_[a-f0-9]{32}.py$"")\n\n    extension = "".py""\n    for root, dirs, files in os.walk(str(name_package)):\n        path_dir = Path(root)\n        for name in files:\n            if (\n                path_dir.name == f""__{backend.name}__""\n                and name.endswith(extension)\n                and not name.startswith(""__ext__"")\n            ):\n                path = path_dir / name\n                if (\n                    backend.suffix_extension == "".py""\n                    and pattern.search(name) is not None\n                ):\n                    continue\n                ext_names.append(\n                    str(path).replace(os.path.sep, ""."").split(extension)[0]\n                )\n\n    return ext_names\n\n\ndef init_pythran_extensions(\n    name_package: str,\n    include_dirs: Iterable[str] = (),\n    compile_args: Iterable[str] = (),\n    exclude_exts: Iterable[str] = (),\n    logger=None,\n):\n    return init_transonic_extensions(\n        name_package, ""pythran"", include_dirs, compile_args, exclude_exts, logger\n    )\n\n\ndef init_transonic_extensions(\n    name_package: str,\n    backend: str = backend_default,\n    include_dirs: Iterable[str] = (),\n    compile_args: Iterable[str] = (),\n    exclude_exts: Iterable[str] = (),\n    logger=None,\n    inplace=None,\n    annotate=False,\n):\n    """"""Detects pythran extensions under a package and returns a list of\n    Extension instances ready to be passed into the ``setup()`` function.\n\n    Parameters\n    ----------\n    name_package:\n\n        Package to be recursively scanned for Pythran extensions.\n\n    backend : str\n\n        Only initialize extensions for this backend. If None, initialize\n        extensions for the default backend (set by an environment variable).\n\n    include_dirs:\n\n        Directories to include while building extensions, for e.g.:\n        ``numpy.get_include()``\n\n    compile_args:\n\n        Arguments to be used while compiling extensions\n\n    exclude_ext:\n\n        Extensions to be excluded from the detected list.\n\n    """"""\n    modules = detect_transonic_extensions(name_package, backend)\n    if not modules:\n        return []\n\n    if backend == ""numba"":\n        # very special case for Numba\n        paths = [Path(mod.replace(""."", os.path.sep) + "".py"") for mod in modules]\n        backends[""numba""].compile_extensions(paths, None)\n        return []\n    elif backend == ""pythran"":\n        BackendExtension = PythranExtension\n    elif backend == ""cython"":\n\n        def BackendExtension(mod, files):\n            # to avoid a bug: Cython does not take into account .pxd file\n            ext = cythonize(files[0], annotate=annotate, language_level=3)[0]\n            ext.name = mod\n            return ext\n\n    else:\n        return []\n\n    if len(exclude_exts) > 0 and logger:\n        logger.info(\n            ""Files in the packages "" + str(exclude_exts) + "" will not be built.""\n        )\n\n    if inplace is None:\n        inplace = ""develop"" in sys.argv or (\n            ""build_ext"" in sys.argv and ""--inplace"" in sys.argv\n        )\n\n    extensions = []\n    for mod in modules:\n        package = mod.rsplit(""."", 1)[0]\n        if any(package == excluded for excluded in exclude_exts):\n            continue\n        base_file = mod.replace(""."", os.path.sep)\n        py_file = base_file + "".py""\n        suffix = get_config_var(""EXT_SUFFIX"")\n        bin_file = base_file + suffix\n        if (\n            not inplace\n            or not os.path.exists(bin_file)\n            or modification_date(bin_file) < modification_date(py_file)\n        ):\n\n            if logger:\n                logger.info(\n                    ""Extension has to be built: {} -> {} "".format(\n                        py_file, os.path.basename(bin_file)\n                    )\n                )\n\n            pext = BackendExtension(mod, [py_file])\n            if isinstance(include_dirs, str):\n                include_dirs = [include_dirs]\n            pext.include_dirs.extend(include_dirs)\n            pext.extra_compile_args.extend(compile_args)\n            extensions.append(pext)\n\n    return extensions\n\n\nclass ParallelBuildExt(*build_ext_classes):\n    @property\n    def logger(self):\n        try:\n            import colorlog as logging\n        except ImportError:\n            import logging\n\n        logger = logging.getLogger(self.logger_name)\n        return logger\n\n    def initialize_options(self):\n        """"""Modify the following to packaging specific needs.""""""\n        super().initialize_options()\n        self.logger_name = ""transonic""\n        self.num_jobs_env_var = """"\n        self.ignoreflags = (""-Wstrict-prototypes"",)\n        self.ignoreflags_startswith = (""-axMIC_"", ""-diag-disable:"")\n\n    def finalize_options(self):\n        """"""Only changed to support setting ``self.parallel`` automatically.""""""\n        if self.parallel is None:\n            self.parallel = self.get_num_jobs()\n\n        super().finalize_options()\n        self.logger.debug(f""Parallel build enabled with {self.parallel} jobs"")\n        self.logger.debug(f""Base classes: {build_ext_classes}"")\n\n    def get_num_jobs(self):\n        try:\n            num_jobs = int(os.environ[self.num_jobs_env_var])\n        except KeyError:\n            import multiprocessing\n\n            num_jobs = multiprocessing.cpu_count()\n\n            try:\n                from psutil import virtual_memory\n            except ImportError:\n                pass\n            else:\n                avail_memory_in_Go = virtual_memory().available / 1e9\n                limit_num_jobs = round(avail_memory_in_Go / 3)\n                num_jobs = min(num_jobs, limit_num_jobs)\n        return num_jobs\n\n    def build_extensions(self):\n        self.check_extensions_list(self.extensions)\n\n        for ext in self.extensions:\n            try:\n                # For Cython extensions\n                ext.sources = self.cython_sources(ext.sources, ext)\n            except AttributeError:\n                pass\n\n        # Invoke Distutils build_extensions method which respects\n        # parallel building. Cython\'s build_ext ignores this\n        DistutilsBuildExt.build_extensions(self)\n\n    def _build_extensions_parallel(self):\n        """"""A slightly modified version\n        ``distutils.command.build_ext.build_ext._build_extensions_parallel``\n        which:\n\n        - filters out some problematic compiler flags\n        - separates extensions of different types into different thread pools.\n\n        """"""\n        logger = self.logger\n        if hasattr(self.compiler, ""compiler_so""):\n            self.compiler.compiler_so = [\n                key\n                for key in self.compiler.compiler_so\n                if key not in self.ignoreflags\n                and all(\n                    [not key.startswith(s) for s in self.ignoreflags_startswith]\n                )\n            ]\n\n        # Set of all extension types\n        ext_types = {type(ext) for ext in self.extensions}\n        extensions_by_type = {T: [] for T in ext_types}\n        for ext in self.extensions:\n            extensions_by_type[ext.__class__].append(ext)\n\n        def names(exts):\n            return [ext.name for ext in exts]\n\n        # Separate building extensions of different types to avoid race conditions\n        num_jobs = self.parallel\n        for exts in extensions_by_type.values():\n            logger.info(f""Start build_extension: {names(exts)}"")\n\n            with Pool(num_jobs) as pool:\n                pool.map(self.build_extension, exts)\n\n            logger.info(f""Stop build_extension: {names(exts)}"")\n'"
transonic/justintime.py,0,"b'""""""Cached JIT compilation\n=========================\n\nUser API\n--------\n\n.. autofunction:: jit\n\n.. autofunction:: set_compile_jit\n\nInternal API\n------------\n\n.. autoclass:: ModuleJIT\n   :members:\n   :private-members:\n\n.. autofunction:: _get_module_jit\n\n.. autoclass:: JIT\n   :members:\n   :private-members:\n\nNotes\n-----\n\nSerge talked about @jit (see https://gist.github.com/serge-sans-paille/28c86d2b33cd561ba5e50081716b2cf4)\n\nIt\'s indeed a good idea!\n\n- At import time, we create one .py file per jit function.\n\n- At run time, we create (and complete when needed) a corresponding\n  .pythran file with signature(s).\n\n  The jit decorator:\n\n  * at the first call, get the types, create the .pythran file and call\n    Pythran.\n\n  * then, try to call the pythran function and if it fails with\n    a Pythran TypeError, correct the .pythran file and recompile.\n\nNote: During the compilation (the ""warmup"" of the JIT), the Python function is\nused.\n\n""""""\n\nimport inspect\nimport itertools\nimport os\nimport sys\nimport time\nfrom distutils.util import strtobool\nfrom functools import wraps\n\nfrom transonic.analyses.justintime import analysis_jit\nfrom transonic.aheadoftime import TransonicTemporaryJITMethod\nfrom transonic.backends import backends, get_backend_name_module\nfrom transonic.config import has_to_replace, backend_default\nfrom transonic.log import logger\nfrom transonic import mpi\nfrom transonic.util import (\n    get_module_name,\n    has_to_build,\n    path_root,\n    get_info_from_ipython,\n    make_hex,\n    has_to_compile_at_import,\n    import_from_path,\n    is_method,\n    write_if_has_to_write,\n    can_import_accelerator,\n    format_str,\n)\n\nmodules_backends = {backend_name: {} for backend_name in backends.keys()}\nmodules = modules_backends[backend_default]\n\n_COMPILE_JIT = strtobool(os.environ.get(""TRANSONIC_COMPILE_JIT"", ""True""))\n\n\ndef set_compile_jit(value):\n    global _COMPILE_JIT\n    _COMPILE_JIT = value\n\n\nclass ModuleJIT:\n    """"""Representation of a module using jit""""""\n\n    def __init__(self, backend_name: str, frame=None):\n\n        self.backend_name = backend_name\n        if frame is None:\n            frame = inspect.stack()[1]\n\n        self.filename = frame.filename\n        if self.filename.startswith(""<ipython-""):\n            self.is_dummy_file = True\n            self._ipython_src, self.filename = get_info_from_ipython()\n            self.module_name = self.filename\n        else:\n            self.is_dummy_file = False\n            self.module_name = get_module_name(frame)\n        modules_backends[backend_name][self.module_name] = self\n        self.used_functions = {}\n        self.jit_functions = {}\n\n        (\n            jitted_dicts,\n            codes_dependance,\n            codes_dependance_classes,\n            code_ext,\n            special,\n        ) = analysis_jit(self.get_source(), self.filename, backend_name)\n\n        self.info_analysis = {\n            ""jitted_dicts"": jitted_dicts,\n            ""codes_dependance"": codes_dependance,\n            ""codes_dependance_classes"": codes_dependance_classes,\n            ""special"": special,\n        }\n\n        self.backend = backend = backends[backend_name]\n        path_jit = mpi.Path(backend.jit.path_base)\n        path_jit_class = mpi.Path(backend.jit.path_class)\n\n        # TODO: check if these files have to be written here...\n        # Write exterior code for functions\n        for file_name, code in code_ext[""function""].items():\n            path_ext = path_jit / self.module_name.replace(""."", os.path.sep)\n            path_ext_file = path_ext / (file_name + "".py"")\n            write_if_has_to_write(path_ext_file, format_str(code), logger.info)\n\n        # Write exterior code for classes\n        for file_name, code in code_ext[""class""].items():\n            path_ext = path_jit_class / self.module_name.replace(""."", os.path.sep)\n            path_ext_file = path_ext / (file_name + "".py"")\n            write_if_has_to_write(path_ext_file, format_str(code), logger.info)\n\n    def get_source(self):\n        if self.is_dummy_file:\n            return self._ipython_src\n        try:\n            mod = sys.modules[self.module_name]\n        except KeyError:\n            with open(self.filename) as file:\n                return file.read()\n        else:\n            return inspect.getsource(mod)\n\n\ndef _get_module_jit(backend_name: str = None, index_frame: int = 2, frame=None):\n    """"""Get the ModuleJIT instance corresponding to the calling module\n\n    Parameters\n    ----------\n\n    index_frame : int\n\n      Index (in :code:`inspect.stack()`) of the frame to be selected\n\n    """"""\n\n    if frame is None:\n        try:\n            frame = inspect.stack()[index_frame]\n        except IndexError:\n            logger.error(\n                f""index_frame {index_frame}""\n                f""{[frame[1] for frame in inspect.stack()]}""\n            )\n            raise\n\n    module_name = get_module_name(frame)\n\n    if backend_name is None:\n        backend_name = get_backend_name_module(module_name)\n\n    modules = modules_backends[backend_name]\n\n    if module_name in modules:\n        return modules[module_name]\n    else:\n        return ModuleJIT(backend_name=backend_name, frame=frame)\n\n\ndef jit(func=None, backend: str = None, native=False, xsimd=False, openmp=False):\n    """"""Decorator to record that the function has to be jit compiled\n\n    """"""\n    frame = inspect.stack()[1]\n    decor = JIT(frame, backend=backend, native=native, xsimd=xsimd, openmp=openmp)\n    if callable(func):\n        return decor(func)\n    else:\n        return decor\n\n\nclass JIT:\n    """"""Decorator used internally by the public jit decorator\n    """"""\n\n    def __init__(\n        self, frame, backend: str, native=False, xsimd=False, openmp=False\n    ):\n\n        self.mod = _get_module_jit(backend, frame=frame)\n\n        self.backend = self.mod.backend\n        self.native = native\n        self.xsimd = xsimd\n        self.openmp = openmp\n        self._decorator_no_arg = False\n\n        self.backend_func = None\n        self.compiling = False\n        self.process = None\n\n    def __call__(self, func):\n        if not has_to_replace:\n            return func\n\n        if is_method(func):\n            return TransonicTemporaryJITMethod(\n                func, self.native, self.xsimd, self.openmp\n            )\n\n        if not can_import_accelerator(self.backend.name):\n            logger.warning(\n                ""Cannot accelerate a jitted function because ""\n                f""{self.backend.name_capitalized} is not importable.""\n            )\n            return func\n\n        func_name = func.__name__\n\n        backend = self.backend\n        mod = self.mod\n        mod.jit_functions[func_name] = self\n        module_name = mod.module_name\n\n        path_jit = mpi.Path(backend.jit.path_base)\n        path_backend = path_jit / module_name.replace(""."", os.path.sep)\n\n        if mpi.rank == 0:\n            path_backend.mkdir(parents=True, exist_ok=True)\n        mpi.barrier()\n\n        path_backend = (path_backend / func_name).with_suffix("".py"")\n        if backend.suffix_header:\n            path_backend_header = path_backend.with_suffix(backend.suffix_header)\n        else:\n            path_backend_header = False\n\n        if path_backend.exists():\n            if not mod.is_dummy_file and has_to_build(path_backend, mod.filename):\n                has_to_write = True\n            else:\n                has_to_write = False\n        else:\n            has_to_write = True\n\n        src = None\n\n        if has_to_write:\n            src, has_to_write = backend.jit.make_backend_source(\n                mod.info_analysis, func, path_backend\n            )\n\n            if has_to_write and mpi.rank == 0:\n                logger.debug(f""write code in file {path_backend}"")\n                with open(path_backend, ""w"") as file:\n                    file.write(src)\n                    file.flush()\n\n        if src is None and mpi.rank == 0:\n            with open(path_backend) as file:\n                src = file.read()\n\n        hex_src = None\n        name_mod = None\n        if mpi.rank == 0:\n            # hash from src (to produce the extension name)\n            hex_src = make_hex(src)\n            name_mod = ""."".join(\n                path_backend.absolute()\n                .relative_to(path_root)\n                .with_suffix("""")\n                .parts\n            )\n\n        hex_src = mpi.bcast(hex_src)\n        name_mod = mpi.bcast(name_mod)\n\n        def backenize_with_new_header(arg_types=""no types""):\n\n            header_object = backend.jit.make_new_header(func, arg_types)\n\n            header_code = backend.jit.merge_old_and_new_header(\n                path_backend_header, header_object, func\n            )\n            backend.jit.write_new_header(\n                path_backend_header, header_code, arg_types\n            )\n\n            # compute the new path of the extension\n            hex_header = make_hex(header_code)\n            # if mpi.nb_proc > 1:\n            #     hex_header0 = mpi.bcast(hex_header)\n            #     assert hex_header0 == hex_header\n            name_ext_file = (\n                func_name\n                + ""_""\n                + hex_src\n                + ""_""\n                + hex_header\n                + backend.suffix_extension\n            )\n            self.path_extension = path_backend.with_name(name_ext_file)\n\n            self.compiling, self.process = backend.compile_extension(\n                path_backend,\n                name_ext_file,\n                native=self.native,\n                xsimd=self.xsimd,\n                openmp=self.openmp,\n            )\n\n            # for backend like numba\n            if not self.compiling:\n                backend_module = import_from_path(self.path_extension, name_mod)\n                assert backend.check_if_compiled(backend_module)\n                self.backend_func = getattr(backend_module, func_name)\n\n        ext_files = None\n        if mpi.rank == 0:\n            glob_name_ext_file = (\n                func_name + ""_"" + hex_src + ""_*"" + backend.suffix_extension\n            )\n            ext_files = list(\n                mpi.PathSeq(path_backend).parent.glob(glob_name_ext_file)\n            )\n        ext_files = mpi.bcast(ext_files)\n\n        if not ext_files:\n            if has_to_compile_at_import() and _COMPILE_JIT:\n                backenize_with_new_header()\n            self.backend_func = None\n        else:\n            path_ext = max(ext_files, key=lambda p: p.stat().st_ctime)\n            backend_module = import_from_path(path_ext, name_mod)\n            self.backend_func = getattr(backend_module, func_name)\n\n        # this is the function that will be called by the user\n        @wraps(func)\n        def type_collector(*args, **kwargs):\n\n            if self.compiling:\n                if not self.process.is_alive(raise_if_error=True):\n                    self.compiling = False\n                    time.sleep(0.1)\n                    backend_module = import_from_path(\n                        self.path_extension, name_mod\n                    )\n                    assert backend.check_if_compiled(backend_module)\n                    self.backend_func = getattr(backend_module, func_name)\n\n            try:\n                return self.backend_func(*args, **kwargs)\n            except TypeError as err:\n                # need to compiled or recompile\n                error = False\n                if self.backend_func:\n                    error = str(err)\n                    if (\n                        error.startswith(""Invalid call to pythranized function `"")\n                        and "" (reshaped)"" in error\n                    ):\n                        logger.error(\n                            ""It seems that a jitted Pythran function has been called ""\n                            \'with a ""reshaped"" array which is not supported by Pythran.\'\n                        )\n                        raise\n                    logger.debug(error)\n\n            if self.compiling or not _COMPILE_JIT:\n                return func(*args, **kwargs)\n\n            if (\n                self.backend_func\n                and error\n                and error.startswith(""Invalid call to pythranized function `"")\n            ):\n                logger.debug(error)\n                logger.info(\n                    f""{backend.name_capitalized} function `{func_name}` called with new types.""\n                )\n                logger.debug(\n                    ""Transonic is going to recompute the function for the new types.""\n                )\n\n            arg_types = [\n                backend.jit.compute_typename_from_object(arg)\n                for arg in itertools.chain(args, kwargs.values())\n            ]\n\n            backenize_with_new_header(arg_types)\n            return func(*args, **kwargs)\n\n        return type_collector\n'"
transonic/log.py,0,"b'""""""Logging\n==========\n\nDefines the transonic logger (variable :code:`logger`).\n\n""""""\n\nimport logging\nfrom types import MethodType\nimport os\n\nlogger = logging.getLogger(""transonic"")\n\n# Initialize logging\ntry:\n    # Set a nice colored output\n    from colorlog import ColoredFormatter\n\n    formatter = ColoredFormatter(\n        ""%(log_color)s%(levelname)-8s%(reset)s %(blue)s%(message)s"",\n        log_colors={\n            ""DEBUG"": ""cyan"",\n            ""INFO"": ""green"",\n            ""WARNING"": ""yellow"",\n            ""ERROR"": ""red"",\n            ""CRITICAL"": ""red"",\n        },\n    )\n    stream = logging.StreamHandler()\n    stream.setFormatter(formatter)\n    logger.addHandler(stream)\nexcept ImportError:\n    # No color available, use default config\n    logging.basicConfig(format=""%(levelname)s: %(message)s"")\n    logger.info(""Disabling color, you really want to install colorlog."")\n\n\ndef _get_level_number(level):\n    level = level.lower()\n    if level == ""info"":\n        return logging.INFO\n    elif level == ""debug"":\n        return logging.DEBUG\n    elif level == ""warning"":\n        return logging.WARNING\n    else:\n        raise ValueError\n\n\ndef set_level(self, level):\n    """"""Set logging level""""""\n    if not level:\n        self.setLevel(0)\n        return\n\n    level = _get_level_number(level)\n\n    self.setLevel(level)\n\n\ndef get_level(self):\n    return logging.getLevelName(self.getEffectiveLevel()).lower()\n\n\ndef is_enable_for(self, level):\n    return _get_level_number(level) >= self.getEffectiveLevel()\n\n\nlogger.set_level = MethodType(set_level, logger)\nlogger.get_level = MethodType(get_level, logger)\nlogger.is_enable_for = MethodType(is_enable_for, logger)\n\nif os.getenv(""TRANSONIC_DEBUG""):\n    level = ""debug""\nelse:\n    level = ""info""\n\nlogger.set_level(level)\n'"
transonic/mpi.py,0,"b'""""""Minimalist MPI module\n========================\n\n""""""\n\nimport os\nfrom pathlib import Path\nfrom time import time, sleep\n\nif ""TRANSONIC_NO_MPI"" in os.environ:\n    nb_proc = 1\n    rank = 0\nelse:\n    try:\n        from fluiddyn.util import mpi as _mpi\n    except ImportError:\n        try:\n            from mpi4py import MPI\n        except ImportError:\n            nb_proc = 1\n            rank = 0\n        else:\n            comm = MPI.COMM_WORLD\n            nb_proc = comm.size\n            rank = comm.Get_rank()\n    else:\n        rank = _mpi.rank\n        nb_proc = _mpi.nb_proc\n        if nb_proc > 1:\n            comm = _mpi.comm\n\nif nb_proc > 1:\n    comm = comm.Clone()\n\n\nif nb_proc == 1:\n\n    def bcast(value, root=0):\n        return value\n\n    def barrier(timeout=None):\n        pass\n\n\nelse:\n\n    _tag = 3 * 7 * 9 * sum(ord(letter) for letter in ""mpi transonic"")\n\n    def bcast(value, root=0, timeout=5.0, tag=_tag):\n        """"""MPI broadcast\n\n        Should do something similar to::\n\n          value = comm.bcast(value, root=root)\n\n        but with a timeout\n\n        """"""\n\n        time_comm = time()\n\n        requests = []\n        if rank == root:\n            for irank in range(nb_proc):\n                if irank == rank:\n                    continue\n                requests.append(comm.isend(value, dest=irank, tag=tag))\n        else:\n            requests.append(comm.irecv(source=root, tag=tag))\n\n        for request in requests:\n            while True:\n                ready, received = request.test()\n                if ready:\n                    break\n                if time() - time_comm > timeout:\n                    raise TimeoutError(f""rank = {rank}, value = {value}"")\n                sleep(0.1)\n\n        if rank != root:\n            value = received\n\n        # send back something small to say that we received the value\n        requests = []\n        if rank == root:\n            for irank in range(nb_proc):\n                if irank == rank:\n                    continue\n                requests.append(comm.irecv(source=irank, tag=tag + 100))\n        else:\n            requests.append(comm.isend(""ok"", dest=0, tag=tag + 100))\n\n        for request in requests:\n            while True:\n                ready, received = request.test()\n                if ready:\n                    break\n                if time() - time_comm > timeout:\n                    raise TimeoutError(f""rank = {rank}, value = {value}"")\n                sleep(0.1)\n\n        return value\n\n    def barrier(timeout=5):\n        if timeout is not None:\n            bcast(""barrier"", timeout=timeout, tag=_tag + 4)\n        comm.barrier()\n\n\nclass ShellProcessMPI:\n    def __init__(self, process, root=0):\n\n        if rank != root:\n            assert process is None\n\n        self.process = process\n        self.root = root\n\n    def is_alive(self, raise_if_error=False):\n        if rank == self.root:\n            is_alive = self.is_alive_root(raise_if_error=raise_if_error)\n        else:\n            is_alive = None\n        return bcast(is_alive, self.root)\n\n    def is_alive_root(self, raise_if_error=False):\n        process = self.process\n        is_alive_ = process.poll() is None\n\n        if not is_alive_ and process.returncode:\n            if process.stdout:\n                print(process.stdout.read(), flush=True)\n            if process.stderr:\n                from transonic.log import logger\n\n                logger.error(process.stderr.read())\n\n            if raise_if_error:\n                raise RuntimeError(\n                    ""Error while compiling code generated by Transonic.""\n                )\n\n        return is_alive_\n\n\nPathSeq = Path\n\nif nb_proc > 1:\n\n    class PathMPI(type(Path())):\n        def exists(self):\n            answer = None\n            if rank == 0:\n                answer = super().exists()\n\n            ret = bcast((""exists"", answer), tag=_tag + 1)\n\n            if len(ret) != 2 and ret[0] != ""exists"":\n                print(rank, ""bug!!!!"", ret, flush=1)\n                raise RuntimeError\n\n            kind, answer = ret\n\n            return answer\n\n        def unlink(self):\n            if rank == 0:\n                super().unlink()\n            comm.barrier()\n\n    Path = PathMPI\n\n    def has_to_build(output_file: Path, input_file: Path):\n        from . import util\n\n        ret = None\n        if rank == 0:\n            ret = util.has_to_build(output_file, input_file)\n        return bcast(ret, tag=_tag + 2)\n\n    def modification_date(filename):\n        from . import util\n\n        ret = None\n        if rank == 0:\n            ret = util.modification_date(filename)\n        return bcast(ret, tag=_tag + 3)\n\n\nif __name__ == ""__main__"":\n    p = Path.home() / ""Dev""\n    print(p.exists(), type(p))\n    p = PathSeq(p)\n    print(p, type(p))\n'"
transonic/path_data_tests.py,0,"b'from pathlib import Path\n\nhere = Path(__file__).parent.absolute()\npath_data_tests = here.parent / ""data_tests""\n'"
transonic/run.py,0,"b'""""""Command line transonic\n============================\n\nInternal API\n------------\n\n.. autofunction:: run\n\n.. autofunction:: parse_args\n\n""""""\n\nimport argparse\nfrom pathlib import Path\nfrom glob import glob\nimport sys\n\nfrom transonic import __version__\n\nfrom transonic.compiler import wait_for_all_extensions\n\nfrom .backends import backends\nfrom transonic.config import backend_default\nfrom transonic.log import logger\nfrom transonic.util import (\n    has_to_build,\n    clear_cached_extensions,\n    can_import_accelerator,\n)\n\ndoc = """"""\ntransonic: easily speedup your Python code with Pythran\n\n""""""\n\n\ndef run():\n    """"""Run the transonic commandline\n\n    See :code:`transonic -h`\n    """"""\n    args = parse_args()\n\n    if args.version:\n        print(__version__)\n        return\n\n    if not args.path and not args.clear_cache:\n        logger.warning(""No python files given. Nothing to do! \xe2\x9c\xa8 \xf0\x9f\x8d\xb0 \xe2\x9c\xa8."")\n        return\n\n    if args.clear_cache:\n        clear_cached_extensions(args.clear_cache, args.force, args.backend)\n        return\n\n    if args.verbose is None:\n        logger.set_level(None)\n    elif args.verbose == 1:\n        logger.set_level(""info"")\n    elif args.verbose > 1:\n        logger.set_level(""debug"")\n    logger.info(args)\n\n    path = args.path\n\n    if isinstance(path, list) and len(path) == 1:\n        path = path[0]\n\n    if isinstance(path, list):\n        paths = path\n    else:\n        path = Path(path)\n        if path.is_file():\n            paths = (path,)\n        elif path.is_dir():\n            paths = path.glob(""*.py"")\n        else:\n            paths = glob(str(path))\n\n    if not paths:\n        logger.error(f""No input file found (args.path = {args.path})"")\n        sys.exit(1)\n\n    backend = backends[args.backend]\n    backend.make_backend_files(paths, force=args.force)\n\n    if args.no_compile:\n        return\n\n    if not can_import_accelerator(backend.name):\n        logger.warning(\n            f""Since {backend.name_capitalized} is not importable, ""\n            ""Transonic cannot properly compile a file.""\n        )\n        return\n\n    # find pythran files not already compiled\n    backends_paths = []\n\n    for path in paths:\n        path = Path(path)\n        backend_path = path.parent / str(f""__{backend.name}__"") / path.name\n        ext_path = backend_path.with_name(\n            backend.name_ext_from_path_backend(backend_path)\n        )\n        if backend_path.exists() and has_to_build(ext_path, backend_path):\n            backends_paths.append(backend_path)\n\n    backend.compile_extensions(\n        backends_paths,\n        str_accelerator_flags=args.accelerator_flags,\n        parallel=True,\n        force=args.force,\n    )\n\n    if not args.no_blocking:\n        wait_for_all_extensions()\n\n\ndef parse_args():\n    """"""Parse the arguments""""""\n    parser = argparse.ArgumentParser(\n        description=doc, formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    parser.add_argument(""path"", help=""Path file or directory."", nargs=""*"")\n\n    parser.add_argument(\n        ""-f"",\n        ""--force"",\n        help=""proceed even if the files seem up-to-date"",\n        action=""store_true"",\n    )\n\n    parser.add_argument(\n        ""-V"", ""--version"", help=""print version and exit"", action=""store_true""\n    )\n\n    parser.add_argument(""-v"", ""--verbose"", help=""verbose mode"", action=""count"")\n\n    parser.add_argument(\n        ""-b"",\n        ""--backend"",\n        help=(""Backend (pythran, cython, numba or python)""),\n        type=str,\n        default=backend_default,\n    )\n\n    parser.add_argument(\n        ""-nc"",\n        ""--no-compile"",\n        help=""do not compile the Pythran/Cython/... files"",\n        action=""store_true"",\n    )\n\n    parser.add_argument(\n        ""-nb"",\n        ""--no-blocking"",\n        help=""launch the compilation in the background and return"",\n        action=""store_true"",\n    )\n\n    parser.add_argument(\n        ""-pf"",\n        ""--pythran-flags"",\n        help=(""Depreciated: use -af""),\n        type=str,\n        default="""",\n    )\n\n    parser.add_argument(\n        ""-af"",\n        ""--accelerator-flags"",\n        help=(\n            ""Flags sent to the accelerator. ""\n            \'Default is """". \'\n            ""There has to be atleast one space in the passed string! ""\n            ""Examples:\\n""\n            \'`transonic foo.py -af ""-march=native ""` or\\n\'\n            \'`transonic foo.py -af ""-march=native -DUSE_XSIMD -Ofast""`\\n\'\n        ),\n        type=str,\n        default="""",\n    )\n\n    parser.add_argument(\n        ""-cc"",\n        ""--clear-cache"",\n        help=(""clear the cached extensions for a module""),\n        type=str,\n        # default="""",\n    )\n\n    args = parser.parse_args()\n    if args.pythran_flags != """":\n        raise DeprecationWarning(""-pf is deprecated. Use -af instead!"")\n\n    return args\n\n\nif __name__ == ""__main__"":\n    run()\n'"
transonic/signatures.py,0,"b'""""""Compute function signatures\n==============================\n\nInternal API\n------------\n\n.. autofunction:: _format_types_as_backend_types\n\n.. autofunction:: compute_signatures_from_typeobjects\n\n.. autofunction:: _make_signature_from_template_variables\n\n.. autofunction:: make_signatures_from_typehinted_func\n\n""""""\n\nimport itertools\nimport inspect\nfrom typing import List\n\nfrom transonic.typing import format_type_as_backend_type, str2type\n\n\ndef _format_types_as_backend_types(types, backend_type_formatter, **kwargs):\n    """"""Compute a list of Pythran/Cython/... types\n\n    """"""\n    backend_types = []\n    for type_ in types:\n        backend_types.append(\n            format_type_as_backend_type(type_, backend_type_formatter, **kwargs)\n        )\n\n    # TODO: handle this with an exception\n    if ""_empty"" in backend_types:\n        raise ValueError(\n            ""At least one annotation type lacking in a signature.\\n""\n            f""types = {types}""\n        )\n\n    return backend_types\n\n\ndef compute_signatures_from_typeobjects(\n    types_in, backend_type_formatter\n) -> List[List[str]]:\n    """"""Compute a list of lists (signatures) of strings (backend types)\n\n    """"""\n    if isinstance(types_in, dict):\n        types_in = types_in.values()\n\n    types = []\n    for type_ in types_in:\n        if isinstance(type_, str):\n            type_ = str2type(type_)\n        types.append(type_)\n\n    template_parameters = set()\n    for type_ in types:\n        if hasattr(type_, ""get_template_parameters""):\n            template_parameters.update(type_.get_template_parameters())\n\n    if not template_parameters:\n        if ""_empty"" in types:\n            raise ValueError(\n                ""At least one annotation type lacking in a signature.\\n""\n                f""types = {types}""\n            )\n        str_types = [\n            format_type_as_backend_type(type_, backend_type_formatter)\n            for type_ in types\n        ]\n        return (str_types,)\n\n    if not all(param.values for param in template_parameters):\n        raise ValueError(\n            f""{template_parameters}, {[param.values for param in template_parameters]}""\n        )\n\n    values_template_parameters = {}\n    for param in template_parameters:\n        values_template_parameters[param.__name__] = param.values\n\n    backend_types = []\n    names = values_template_parameters.keys()\n    for set_types in itertools.product(*values_template_parameters.values()):\n        template_variables = dict(zip(names, set_types))\n\n        backend_types.append(\n            _format_types_as_backend_types(\n                types, backend_type_formatter, **template_variables\n            )\n        )\n\n    return backend_types\n\n\ndef _make_signature_from_template_variables(\n    func, backend_type_formatter, _signature=None, as_list_str=False, **kwargs\n):\n    """"""Create signature for a function with values for the template types\n\n    (This function should only be used in\n    :func:`make_signatures_from_typehinted_func`)\n\n    Parameters\n    ----------\n\n    func: a function\n\n    kwargs : dict\n\n        The template types and their value\n\n    """"""\n    if _signature is None:\n        _signature = inspect.signature(func)\n\n    types = [param.annotation for param in _signature.parameters.values()]\n\n    backend_types = _format_types_as_backend_types(\n        types, backend_type_formatter, **kwargs\n    )\n\n    # ""multiply"" the signatures to take into account the ""or"" syntax\n    multi_pythran_types = [\n        _ for _ in itertools.product(*[t.split("" or "") for t in backend_types])\n    ]\n    signatures = []\n    for backend_types in multi_pythran_types:\n        if as_list_str:\n            signature = backend_types\n        else:\n            signature = f""{func.__name__}("" + "", "".join(backend_types) + "")""\n        signatures.append(signature)\n\n    return signatures\n\n\ndef make_signatures_from_typehinted_func(\n    func, backend_type_formatter, as_list_str=False\n):\n    """"""Make the signatures from annotations if it is possible\n\n    Useful when there are only ""not templated"" types.\n\n    """"""\n    annotations = func.__annotations__\n\n    if not annotations:\n        return tuple()\n\n    for key, value in annotations.items():\n        if isinstance(value, str):\n            annotations[key] = str2type(value)\n\n    types = annotations.values()\n\n    template_parameters = []\n    for type_ in types:\n        if hasattr(type_, ""get_template_parameters""):\n            template_parameters.extend(type_.get_template_parameters())\n    template_parameters = set(template_parameters)\n\n    _signature = inspect.signature(func)\n\n    if not template_parameters:\n        signatures = _make_signature_from_template_variables(\n            func,\n            backend_type_formatter,\n            _signature=_signature,\n            as_list_str=as_list_str,\n        )\n        return signatures\n\n    if not all(param.values for param in template_parameters):\n        return tuple()\n\n    values_template_parameters = {}\n    for param in template_parameters:\n        values_template_parameters[param.__name__] = param.values\n\n    names = values_template_parameters.keys()\n    signatures = []\n    for set_types in itertools.product(*values_template_parameters.values()):\n        template_variables = dict(zip(names, set_types))\n        signatures.extend(\n            _make_signature_from_template_variables(\n                func,\n                backend_type_formatter,\n                _signature=_signature,\n                as_list_str=as_list_str,\n                **template_variables,\n            )\n        )\n\n    return signatures\n'"
transonic/test_build_ext.py,0,"b'import os\nfrom pathlib import Path\nimport runpy\nimport shutil\nfrom contextlib import suppress\n\nimport pytest\n\nfrom transonic.dist import make_backend_files\nfrom transonic.mpi import nb_proc\nfrom transonic.path_data_tests import path_data_tests\nfrom transonic.config import backend_default\n\ncwd = Path.cwd().absolute()\nsetup_dir = path_data_tests / ""test_packaging""\n\n\ndef setup_module():\n    os.chdir(setup_dir)\n    transonic_src_paths = [setup_dir / ""add.py""]\n    make_backend_files(transonic_src_paths)\n\n\n@pytest.mark.skipif(\n    backend_default == ""python"", reason=""Speedup Python backend tests""\n)\n@pytest.mark.skipif(not path_data_tests.exists(), reason=""no data tests"")\n@pytest.mark.skipif(nb_proc > 1, reason=""No build_ext in MPI"")\ndef test_buildext():\n    os.chdir(setup_dir)\n    runpy.run_path(str(setup_dir / ""setup.py""))\n\n\ndef teardown_module():\n    os.chdir(cwd)\n    for namedir in (""build"", f""__{backend_default}__"", ""__pycache__""):\n        with suppress(FileNotFoundError):\n            shutil.rmtree(setup_dir / namedir)\n\n    to_remove = list(setup_dir.glob(""*.h"")) + list(setup_dir.glob(""*.so""))\n    for path in to_remove:\n        os.remove(path)\n'"
transonic/test_dist.py,0,"b'import shutil\nfrom distutils.core import Distribution\nfrom pprint import pformat\n\nimport pytest\n\nfrom transonic.config import backend_default\n\nfrom transonic.dist import (\n    detect_transonic_extensions,\n    modification_date,\n    make_backend_files,\n    ParallelBuildExt,\n    get_logger,\n)\n\nfrom transonic.mpi import nb_proc\nfrom transonic.path_data_tests import path_data_tests\nfrom transonic.util import can_import_accelerator\n\n\n@pytest.mark.skipif(not path_data_tests.exists(), reason=""no data tests"")\n@pytest.mark.skipif(nb_proc > 1, reason=""No dist in MPI"")\ndef test_detect_backend_extensions():\n\n    shutil.rmtree(path_data_tests / f""__{backend_default}__"", ignore_errors=True)\n\n    names = [\n        ""assign_func_boost.py"",\n        ""assign_func_jit.py"",\n        ""block_fluidsim.py"",\n        ""blocks_type_hints.py"",\n        ""boosted_func_use_import.py"",\n        # ""boosted_class_use_import.py"",  # was forgotten...\n        ""class_blocks.py"",\n        ""classic.py"",\n        # ""class_rec_calls.py"",\n        # ""methods.py"",\n        ""mixed_classic_type_hint.py"",\n        # ""no_arg.py"",\n        ""type_hint_notemplate.py"",\n        ""no_pythran_.py"",\n    ]\n\n    make_backend_files((path_data_tests / name for name in names))\n    ext_names = detect_transonic_extensions(path_data_tests)\n\n    if can_import_accelerator():\n        # -2 files (no_pythran.py and assign_fun_jit.py)\n        number_not_transonized = 2\n\n        if len(ext_names) != len(names) - number_not_transonized:\n            print(""ext_names:\\n"", pformat(sorted(ext_names)), sep="""")\n            print(""names:\\n"", pformat(sorted(names)), sep="""")\n            raise RuntimeError\n\n    shutil.rmtree(path_data_tests / f""__{backend_default}__"", ignore_errors=True)\n\n\n@pytest.mark.skipif(not path_data_tests.exists(), reason=""no data tests"")\n@pytest.mark.skipif(nb_proc > 1, reason=""No dist in MPI"")\ndef test_modification_date():\n\n    modification_date(path_data_tests / ""no_pythran_.py"")\n    get_logger(""bar"")\n\n\n@pytest.mark.skipif(not path_data_tests.exists(), reason=""no data tests"")\n@pytest.mark.skipif(nb_proc > 1, reason=""No dist in MPI"")\ndef test_build_ext():\n    dist = Distribution()\n    build_ext = ParallelBuildExt(dist)\n\n    build_ext.initialize_options()\n    build_ext.parallel = 1\n    build_ext.finalize_options()\n'"
transonic/test_init.py,0,"b'import importlib\nfrom shutil import rmtree\n\nfrom transonic import Transonic, mpi\nfrom transonic.compiler import wait_for_all_extensions\nfrom transonic.config import backend_default\nfrom transonic.mpi import Path\n\n\ndef test_not_transonified():\n\n    path_for_test = (\n        Path(__file__).parent.parent / ""_transonic_testing/for_test_init.py""\n    )\n    path_output = path_for_test.parent / f""__{backend_default}__""\n\n    if path_output.exists() and mpi.rank == 0:\n        rmtree(path_output)\n    mpi.barrier()\n\n    from _transonic_testing import for_test_init\n\n    importlib.reload(for_test_init)\n\n    from _transonic_testing.for_test_init import func, func1, check_class\n\n    func(1, 3.14)\n    func1(1.1, 2.2)\n    check_class()\n\n\ndef test_use_pythran_false():\n    Transonic(use_transonified=False)\n\n\ndef test_assign_boosted_func():\n    from _transonic_testing.for_test_init import func0, func0_boosted\n\n    func02 = func0(2, 6)\n    result = func0_boosted(2, 6)\n    wait_for_all_extensions()\n    assert func02 == func0(2, 6)\n    assert result == func0(2, 6)\n'"
transonic/test_init_transonified.py,0,"b'import importlib\nimport unittest\nimport os\nimport time\n\nfrom transonic.backends import backends\nfrom transonic.config import backend_default\nfrom transonic.util import (\n    has_to_compile_at_import,\n    ext_suffix,\n    can_import_accelerator,\n)\nfrom transonic.aheadoftime import modules\nfrom transonic import mpi\n\nbackend = backends[backend_default]\n\nmodule_name = ""_transonic_testing.for_test_init""\n\n\nclass TestsInit(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.path_for_test = (\n            mpi.Path(__file__).parent.parent\n            / ""_transonic_testing/for_test_init.py""\n        )\n\n        assert cls.path_for_test.exists()\n\n        cls.path_backend = path_backend = (\n            cls.path_for_test.parent\n            / f""__{backend_default}__""\n            / cls.path_for_test.name\n        )\n\n        cls.path_ext = path_backend.with_name(\n            backend.name_ext_from_path_backend(path_backend)\n        )\n\n    @classmethod\n    def tearDownClass(cls):\n        # cls.path_backend.unlink()\n        if cls.path_ext.exists():\n            cls.path_ext.unlink()\n\n        path_ext = cls.path_ext.with_suffix(ext_suffix)\n        if path_ext.exists():\n            path_ext.unlink()\n\n        try:\n            os.environ.pop(""TRANSONIC_COMPILE_AT_IMPORT"")\n        except KeyError:\n            pass\n\n        print(mpi.rank, ""end tearDownClass"")\n\n    def test_transonified(self):\n\n        print(mpi.rank, ""start test"", flush=1)\n\n        try:\n            os.environ.pop(""TRANSONIC_COMPILE_AT_IMPORT"")\n        except KeyError:\n            pass\n\n        try:\n            del modules[module_name]\n        except KeyError:\n            pass\n\n        assert not has_to_compile_at_import()\n\n        print(mpi.rank, ""before if self.path_backend.exists()"", flush=1)\n\n        if self.path_backend.exists():\n\n            print(mpi.rank, ""before self.path_backend.unlink()"", flush=1)\n\n            self.path_backend.unlink()\n\n        print(mpi.rank, ""before make_backend_file(self.path_for_test)"", flush=1)\n        if mpi.rank == 0:\n            backend.make_backend_file(self.path_for_test)\n\n        print(mpi.rank, ""after make_backend_file(self.path_for_test)"", flush=1)\n        mpi.barrier()\n\n        from _transonic_testing import for_test_init\n\n        importlib.reload(for_test_init)\n\n        assert self.path_backend.exists()\n        assert for_test_init.ts.is_transpiled\n\n        for_test_init.func(1, 3.14)\n        for_test_init.func1(1.1, 2.2)\n        for_test_init.check_class()\n\n    @unittest.skipIf(\n        not can_import_accelerator(),\n        f""{backend.name} is required for TRANSONIC_COMPILE_AT_IMPORT"",\n    )\n    def test_pythranize(self):\n\n        os.environ[""TRANSONIC_COMPILE_AT_IMPORT""] = ""1""\n\n        try:\n            del modules[module_name]\n        except KeyError:\n            pass\n\n        assert has_to_compile_at_import()\n\n        if self.path_backend.exists():\n            self.path_backend.unlink()\n\n        if self.path_ext.exists():\n            self.path_ext.unlink()\n\n        path_ext = self.path_ext.with_suffix(ext_suffix)\n        if path_ext.exists():\n            path_ext.unlink()\n\n        from _transonic_testing import for_test_init\n\n        if not for_test_init.ts.is_compiling:\n            importlib.reload(for_test_init)\n\n        assert module_name in modules, modules\n        assert self.path_backend.exists()\n\n        ts = for_test_init.ts\n\n        assert ts.is_transpiled\n        if backend.needs_compilation:\n            assert ts.is_compiling\n            assert not ts.is_compiled\n        else:\n            assert ts.is_compiled\n\n        for_test_init.func(1, 3.14)\n        for_test_init.func1(1.1, 2.2)\n\n        while not ts.is_compiled:\n            time.sleep(0.1)\n            for_test_init.func(1, 3.14)\n            for_test_init.func1(1.1, 2.2)\n\n        assert not ts.is_compiling\n        assert ts.is_compiled\n\n        for_test_init.func(1, 3.14)\n        for_test_init.func1(1.1, 2.2)\n        for_test_init.check_class()\n'"
transonic/test_justintime.py,1,"b'import sys\nimport os\nfrom time import sleep\nfrom shutil import rmtree\n\nimport numpy as np\nimport pytest\n\nfrom transonic.backends import backends\nfrom transonic.compiler import scheduler, wait_for_all_extensions\nfrom transonic.justintime import modules\nfrom transonic import mpi\nfrom transonic.util import can_import_accelerator\nfrom transonic.config import backend_default\n\nbackend = backends[backend_default]\nscheduler.nb_cpus = 2\n\nmodule_name = ""_transonic_testing.for_test_justintime""\n\nstr_relative_path = module_name.replace(""."", os.path.sep)\n\npath_jit = backend.jit.path_base\npath_jit_classes = backend.jit.path_class\n\npath_jit_dir = path_jit / str_relative_path\npath_classes_dir = path_jit_classes / str_relative_path\npath_classes_dir1 = path_jit / path_jit_classes.name / str_relative_path\n\nif mpi.rank == 0:\n    if path_jit.exists():\n        rmtree(path_jit_dir, ignore_errors=True)\n    if path_classes_dir.exists():\n        rmtree(path_classes_dir)\n    if path_classes_dir1.exists():\n        rmtree(path_classes_dir1)\n\nmpi.barrier()\n\n\n@pytest.mark.skipif(backend_default == ""numba"", reason=""Not supported by Numba"")\ndef test_jit():\n    from _transonic_testing.for_test_justintime import func1\n\n    a = np.arange(2)\n    b = [1, 2]\n\n    for _ in range(2):\n        func1(a, b)\n        sleep(0.1)\n\n    wait_for_all_extensions()\n    func1(a, b)\n\n\ndef test_fib():\n    from _transonic_testing.for_test_justintime import fib, use_fib\n\n    fib2 = fib(2)\n    result = use_fib()\n    wait_for_all_extensions()\n    assert fib2 == fib(2)\n    assert result == use_fib()\n\n    fib15 = fib(1.5)\n    wait_for_all_extensions()\n    assert fib15 == fib(1.5)\n\n\ndef test_jit_simple():\n\n    from _transonic_testing.for_test_justintime import func2\n\n    func2(1)\n\n    if not can_import_accelerator():\n        return\n\n    mod = modules[module_name]\n    cjit = mod.jit_functions[""func2""]\n\n    for _ in range(100):\n        func2(1)\n        sleep(0.1)\n        if not cjit.compiling:\n            sleep(0.1)\n            func2(1)\n            break\n\n    del sys.modules[module_name]\n    del modules[module_name]\n\n    from _transonic_testing.for_test_justintime import func2\n\n    func2(1)\n\n\n@pytest.mark.skipif(backend_default == ""numba"", reason=""Not supported by Numba"")\ndef test_jit_dict():\n    from _transonic_testing.for_test_justintime import func_dict\n\n    d = dict(a=1, b=2)\n    func_dict(d)\n\n    if not can_import_accelerator():\n        return\n\n    mod = modules[module_name]\n    cjit = mod.jit_functions[""func_dict""]\n\n    d = dict(a=1, b=2)\n    func_dict(d)\n\n    wait_for_all_extensions()\n\n    for _ in range(100):\n        d = dict(a=1, b=2)\n        func_dict(d)\n        sleep(0.1)\n        if not cjit.compiling:\n            sleep(0.1)\n            func_dict(d)\n            break\n\n\ndef test_jit_method():\n    from _transonic_testing.for_test_justintime import MyClass\n\n    obj = MyClass()\n    obj.check()\n\n    if not can_import_accelerator():\n        return\n\n    obj = MyClass()\n    obj.check()\n\n    wait_for_all_extensions()\n\n    obj.check()\n\n\n@pytest.mark.skipif(backend_default == ""numba"", reason=""Not supported by Numba"")\ndef test_jit_method2():\n    from _transonic_testing.for_test_justintime import MyClass2\n\n    obj = MyClass2()\n    obj.check()\n\n    if not can_import_accelerator():\n        return\n\n    obj = MyClass2()\n    obj.check()\n\n    wait_for_all_extensions()\n\n    obj.check()\n\n\ndef test_func0():\n    from _transonic_testing.for_test_justintime import func0, func0_jitted\n\n    func02 = func0(2.1)\n    result = func0_jitted(2.1)\n    wait_for_all_extensions()\n    assert func02 == func0(2.1)\n    assert result == func0(2.1)\n\n\n# jitted function that uses a local function, a jitted local function\n# and a jitted imported function (with a different name)\ndef test_main():\n    from _transonic_testing.for_test_justintime import main\n\n    main_res = main(2)\n    wait_for_all_extensions()\n    assert main_res == 5\n\n\ndef test_jit_imported():\n    from _transonic_testing.for_test_justintime import (\n        jitted_func_import,\n        func_import,\n    )\n\n    result = jitted_func_import()\n    wait_for_all_extensions()\n    assert result == jitted_func_import() == func_import()\n'"
transonic/test_run.py,0,"b'import sys\nimport time\nimport os\nfrom shutil import rmtree\n\nimport pytest\n\nfrom transonic import util\nfrom transonic.config import backend_default\nfrom transonic.mpi import nb_proc\nfrom transonic.path_data_tests import path_data_tests\nfrom transonic.run import run\n\n\npath_dir_out = path_data_tests / f""__{backend_default}__""\nheader_suffixes = {""pythran"": "".pythran"", ""cython"": "".pxd""}\n\n\n@pytest.mark.skipif(not path_data_tests.exists(), reason=""no data tests"")\n@pytest.mark.skipif(nb_proc > 1, reason=""No commandline in MPI"")\ndef test_create_pythran_files():\n\n    if path_dir_out.exists():\n        rmtree(path_dir_out)\n\n    if os.name != ""nt"":\n        sys.argv = f""transonic -nc {path_data_tests / \'*.py\'}"".split()\n        run()\n\n    sys.argv = f""transonic -nc {path_data_tests}"".split()\n    run()\n\n    paths = tuple(path_data_tests.glob(""*.py*""))\n    sys.argv = [""transonic"", ""-nc""] + [str(path) for path in paths]\n    run()\n\n    # At this point, we can compare the produced files with saved files.\n    # For exterior files, we can\'t compare cause transonic changes their names\n    no_compare = [\n        ""no_pythran_.py"",\n        ""assign_func_jit.py"",\n        ""exterior_import_boost.py"",\n        ""exterior_import_boost_2.py"",\n    ]\n    for path in paths:\n        if path.name in no_compare:\n            continue\n\n        __backend__path = path.parent / f""__{backend_default}__"" / path.name\n        assert __backend__path.exists()\n        saved_path = (\n            path.parent / ""saved__backend__"" / backend_default / path.name\n        )\n        assert saved_path.exists()\n\n        with open(__backend__path) as file:\n            code = file.read()\n        with open(saved_path) as file:\n            saved_code = file.read()\n\n        code = code.split(""__transonic__ = "", 1)[0]\n        saved_code = saved_code.split(""__transonic__ = "", 1)[0]\n\n        if backend_default in header_suffixes:\n            suffix = header_suffixes[backend_default]\n\n            header_path = __backend__path.with_suffix(suffix)\n            assert header_path.exists()\n            with open(header_path) as file:\n                code += file.read()\n\n            header_path = saved_path.with_suffix(suffix)\n            assert header_path.exists()\n            with open(header_path) as file:\n                saved_code += file.read()\n\n        # warning: it is a very strong requirement!\n        assert code == saved_code, path\n\n\n@pytest.mark.skipif(not path_data_tests.exists(), reason=""no data tests"")\n@pytest.mark.skipif(nb_proc > 1, reason=""No commandline in MPI"")\ndef test_create_pythran_simple():\n\n    sys.argv = ""transonic --version"".split()\n    run()\n\n    sys.argv = [""transonic""]\n    run()\n\n\n@pytest.mark.skipif(not path_data_tests.exists(), reason=""no data tests"")\n@pytest.mark.skipif(nb_proc > 1, reason=""No commandline in MPI"")\ndef test_create_trans_classic():\n\n    util.input = lambda: ""y""\n\n    if path_dir_out.exists():\n        rmtree(path_dir_out)\n\n    path_file = path_data_tests / ""classic.py""\n    sys.argv = f""transonic -nc {path_file}"".split()\n    run()\n\n    print(""after first build"")\n    run()\n\n    time.sleep(0.02)\n    path_file.touch()\n    print(""after touch"")\n\n    run()\n\n    path_file_pythran = path_data_tests / f""__{backend_default}__/classic.py""\n    path_file_pythran.unlink()\n\n    print(""after unlink"")\n    run()\n\n    sys.argv = f""transonic -cc {path_file}"".split()\n    run()\n\n    sys.argv = f""transonic {path_file}"".split()\n    run()\n\n    sys.argv = f""transonic -nc {path_file_pythran}"".split()\n    run()\n'"
transonic/test_typing.py,8,"b'import numpy as np\n\nfrom transonic.typing import (\n    Array,\n    NDim,\n    str2type,\n    UnionMeta,\n    List,\n    ListMeta,\n    Dict,\n    DictMeta,\n    Set,\n    SetMeta,\n    typeof,\n    str2shape,\n    MemLayout,\n    Optional,\n    const,\n)\n\nfrom transonic.backends.typing import base_type_formatter\n\n\ndef compare_array_types(A0, A1):\n\n    assert A0.dtype == A1.dtype\n\n    if len(A0.ndim.values) > 1:\n        raise NotImplementedError\n\n    if len(A1.ndim.values) > 1:\n        raise NotImplementedError\n\n    assert A0.ndim.values[0] == A1.ndim.values[0]\n\n\ndef test_NDim():\n    N = NDim(1, 3)\n    repr(N + 1)\n    repr(N - 1)\n\n\ndef test_str2type_simple():\n    assert str2type(""int"") == np.int\n    assert str2type(""float"") == np.float\n    assert str2type(""uint32"") == np.uint32\n\n\ndef test_str2type_arrays():\n    A1 = Array[int, ""1d""]\n    compare_array_types(str2type(""int[]""), A1)\n    compare_array_types(str2type(""int[:]""), A1)\n\n    A2 = Array[np.uint32, ""2d""]\n    compare_array_types(str2type(""uint32[:,:]""), A2)\n\n\ndef test_str2type_or():\n    result = str2type(""int or float"")\n    assert isinstance(result, UnionMeta)\n    assert result.types == (int, float)\n\n    result = str2type(""int or float[]"")\n    assert isinstance(result, UnionMeta)\n    compare_array_types(result.types[1], Array[float, ""1d""])\n\n\ndef test_list():\n    L = List[List[int]]\n    repr(L)\n    assert isinstance(L, ListMeta)\n    assert L.format_as_backend_type(base_type_formatter) == ""int list list""\n\n\ndef test_dict():\n    D = Dict[str, int]\n    repr(D)\n    assert isinstance(D, DictMeta)\n    assert D.format_as_backend_type(base_type_formatter) == ""str: int dict""\n\n\ndef test_set():\n    str2type(""int set"")\n    S = Set[""str""]\n    S.get_template_parameters()\n    repr(S)\n    assert isinstance(S, SetMeta)\n    assert S.format_as_backend_type(base_type_formatter) == ""str set""\n\n\ndef test_tuple():\n    T = str2type(""(int, float[:, :])"")\n    T.get_template_parameters()\n    assert repr(T) == \'Tuple[int, Array[float, ""2d""]]\'\n    assert T.format_as_backend_type(base_type_formatter) == ""(int, float64[:, :])""\n\n\ndef test_typeof_simple():\n    assert typeof(1) is int\n    assert typeof(1.0) is float\n    assert typeof(1j) is complex\n    assert typeof(""foo"") is str\n\n\ndef test_typeof_list():\n    L = typeof([[1, 2], [3, 4]])\n    assert isinstance(L, ListMeta)\n    assert L.format_as_backend_type(base_type_formatter) == ""int list list""\n\n\ndef test_typeof_dict():\n    D = typeof({""a"": 0, ""b"": 1})\n    assert isinstance(D, DictMeta)\n    assert D.format_as_backend_type(base_type_formatter) == ""str: int dict""\n\n\ndef test_typeof_set():\n    S = typeof({""a"", ""b""})\n    assert isinstance(S, SetMeta)\n    assert S.format_as_backend_type(base_type_formatter) == ""str set""\n\n\ndef test_typeof_array():\n    A = typeof(np.ones((2, 2)))\n    compare_array_types(A, Array[np.float64, ""2d""])\n\n\ndef test_typeof_np_scalar():\n    T = typeof(np.ones(1)[0])\n    assert T is np.float64\n\n\ndef test_shape():\n\n    assert str2shape(""[]"") == (None,)\n    assert str2shape(""[:]"") == (None,)\n    assert str2shape(""[][]"") == (None,) * 2\n    assert str2shape(""[][ ]"") == (None,) * 2\n\n    assert str2shape(""[:,:]"") == (None,) * 2\n    assert str2shape(""[: ,:,:, ]"") == (None,) * 3\n    assert str2shape(""[3 ,:,:]"") == (3, None, None)\n    assert str2shape(""[ : , :,3]"") == (None, None, 3)\n\n    A = Array[int, ""[: ,:, 3]""]\n    assert A.shape == (None, None, 3)\n    assert A.ndim.values[0] == 3\n    assert repr(A) == \'Array[int, ""[:,:,3]""]\'\n\n    assert (\n        base_type_formatter.make_array_code(\n            int, 2, (3, None), False, MemLayout.C_or_F, positive_indices=False\n        )\n        == ""int[3, :]""\n    )\n\n\ndef test_optional():\n    assert repr(Optional[int]) == ""Union[int, None]""\n\n\ndef test_const():\n    A = str2type(""int[]"")\n    B = const(A)\n    assert A.format_as_backend_type(\n        base_type_formatter\n    ) == B.format_as_backend_type(base_type_formatter)\n    assert repr(B) == \'const(Array[int, ""1d""])\'\n'"
transonic/test_util.py,0,"b'from transonic import util\nfrom transonic.util import query_yes_no, timeit, print_versions, timeit_verbose\n\n\ndef test_query_yes_no():\n\n    util.input = lambda: ""y""\n\n    query_yes_no(""test"", default=""y"")\n    query_yes_no(""test"", default=""n"")\n    query_yes_no(""test"")\n\n    util.input = lambda: """"\n    query_yes_no(""test"", default=""y"")\n\n    query_yes_no(""test"", force=True)\n\n\ndef test_timeit():\n    a = 1\n    b = 2\n    timeit(""a + b"", total_duration=0.001, globals=locals())\n\n\ndef test_timeit_verbose():\n    a = 1\n    b = 2\n    norm = timeit_verbose(""a + b"", total_duration=0.001, globals=locals())\n    timeit_verbose(""a + b"", total_duration=0.001, globals=locals(), norm=norm)\n\n\ndef test_print_versions():\n    print_versions()\n'"
transonic/typing.py,9,"b'""""""Create Pythran signatures from type hints\n============================================\n\nUser API\n--------\n\n.. autoclass:: Type\n   :members:\n\n.. autoclass:: NDim\n   :members:\n\n.. autoclass:: Array\n   :members:\n   :private-members:\n\n.. autoclass:: List\n   :members:\n   :private-members:\n\n.. autoclass:: Tuple\n   :members:\n   :private-members:\n\n.. autoclass:: Dict\n   :members:\n   :private-members:\n\n.. autoclass:: Set\n   :members:\n   :private-members:\n\n.. autoclass:: Union\n   :members:\n   :private-members:\n\n.. autofunction:: str2type\n\n.. autofunction:: typeof\n\n.. autofunction:: const\n\nInternal API\n------------\n\n.. autoclass:: TemplateVar\n   :members:\n   :private-members:\n\n.. autoclass:: ArrayMeta\n   :members:\n   :private-members:\n\n.. autoclass:: ListMeta\n   :members:\n   :private-members:\n\n.. autoclass:: DictMeta\n   :members:\n   :private-members:\n\n.. autofunction:: format_type_as_backend_type\n\n.. autoclass:: ConstType\n   :members:\n   :private-members:\n\n""""""\nimport re\nfrom enum import Enum, auto\nimport itertools\n\nimport numpy as np\n\nfrom transonic.util import get_name_calling_module\n\nnames_template_variables = {}\n\n\nclass FusedType:\n    def is_fused_type(self):\n        raise NotImplementedError\n\n    def get_all_formatted_backend_types(self, type_formatter):\n\n        template_params = self.get_template_parameters()\n        values_template_parameters = {\n            param.__name__: param.values for param in template_params\n        }\n        names = tuple(values_template_parameters.keys())\n        formatted_types = []\n        for set_types in itertools.product(*values_template_parameters.values()):\n            template_variables = dict(zip(names, set_types))\n            formatted_types.append(\n                format_type_as_backend_type(\n                    self, type_formatter, **template_variables\n                )\n            )\n        return formatted_types\n\n\nclass TemplateVar:\n    """"""Base class for template variables\n\n    >>> T = TemplateVar(""T"")\n    >>> T = TemplateVar(""T"", int, float)\n\n    >>> T = TemplateVar()\n    Traceback (most recent call last):\n        ...\n    ValueError\n\n    >>> T = TemplateVar(1)\n    Traceback (most recent call last):\n        ...\n    TypeError: (1,) [False]\n    """"""\n\n    _type_values = type\n    _letter = ""T""\n\n    def get_template_parameters(self):\n        return (self,)\n\n    def __init__(self, *args, name_calling_module=None):\n\n        if not args:\n            raise ValueError\n\n        if name_calling_module is None:\n            name_calling_module = get_name_calling_module()\n\n        if name_calling_module not in names_template_variables:\n            names_template_variables[name_calling_module] = {}\n\n        names_variables = names_template_variables[name_calling_module]\n\n        if type(self) not in names_variables:\n            names_variables[type(self)] = set()\n\n        names_already_used = names_variables[type(self)]\n\n        if self._is_correct_for_name(args[0]):\n            self.__name__ = args[0]\n            args = args[1:]\n        else:\n            index_var = len(names_already_used)\n\n            while self._letter + str(index_var) in names_already_used:\n                index_var += 1\n\n            self.__name__ = self._letter + str(index_var)\n\n        self.values = args\n\n        names_already_used.add(self.__name__)\n\n        self._check_type_values()\n\n    def _is_correct_for_name(self, arg):\n        return isinstance(arg, str)\n\n    def _check_type_values(self):\n        if not all(isinstance(value, self._type_values) for value in self.values):\n            raise TypeError(\n                f""{self.values} ""\n                f""{[isinstance(value, self._type_values) for value in self.values]}""\n            )\n\n    def has_multiple_values(self):\n        return len(self.values) > 1\n\n\nclass Type(TemplateVar, FusedType):\n    """"""Template variable representing the dtype of an array.\n\n    As a user, it is useful only for fused types.\n\n    >>> Type(int, float)\n    Type(int, float)\n\n    """"""\n\n    def __repr__(self):\n        repr_values = []\n        for value in self.values:\n            if hasattr(value, ""__name__""):\n                repr_values.append(value.__name__)\n            else:\n                repr_values.append(repr(value))\n        return f""Type({\', \'.join(repr_values)})""\n\n    def format_as_backend_type(self, backend_type_formatter, **kwargs):\n\n        dtype = None\n\n        for key, value in kwargs.items():\n            if key == self.__name__:\n                dtype = value\n                break\n\n        if dtype is None:\n            raise ValueError\n\n        return dtype.__name__\n\n    def is_fused_type(self):\n        return len(self.values) > 1\n\n    def short_repr(self):\n        long_repr = repr(self)\n        replaced_by = {""("": ""I"", "")"": ""I"", "", "": ""_""}\n        for replaced, replacer in replaced_by.items():\n            long_repr = long_repr.replace(replaced, replacer)\n        return long_repr\n\n\nclass NDim(TemplateVar):\n    """"""Template variable representing the number of dimension of an array.\n\n    As a user, it is useful only for fused types.\n\n    >>> N = NDim(1, 2)\n    >>> N1 = N + 1\n\n    """"""\n\n    _type_values = int\n    _letter = ""N""\n\n    def __init__(self, *args, shift=0, name_calling_module=None):\n\n        if name_calling_module is None:\n            name_calling_module = get_name_calling_module()\n\n        super().__init__(*args, name_calling_module=name_calling_module)\n        self.shift = shift\n\n    def __repr__(self):\n        if len(self.values) == 1:\n            name = f\'""{self.values[0]}d""\'\n        else:\n            name = f""NDim({\', \'.join(repr(v) for v in self.values)})""\n\n        if self.shift == 0:\n            return name\n        elif self.shift < 0:\n            return name + f"" - {-self.shift}""\n        elif self.shift > 0:\n            return name + f"" + {self.shift}""\n        else:\n            raise RuntimeError\n\n    def __add__(self, number):\n        name_calling_module = get_name_calling_module()\n        return type(self)(\n            self.__name__,\n            *self.values,\n            shift=number,\n            name_calling_module=name_calling_module,\n        )\n\n    def __sub__(self, number):\n        name_calling_module = get_name_calling_module()\n        return type(self)(\n            self.__name__,\n            *self.values,\n            shift=-number,\n            name_calling_module=name_calling_module,\n        )\n\n    def short_repr(self):\n        long_repr = repr(self)\n        replaced_by = {\n            \'""\': """",\n            ""("": ""I"",\n            "")"": ""I"",\n            "" - "": ""m"",\n            "" + "": ""p"",\n            "", "": ""_"",\n        }\n        for replaced, replacer in replaced_by.items():\n            long_repr = long_repr.replace(replaced, replacer)\n        return long_repr\n\n\nclass UnionVar(TemplateVar):\n    """"""TemplateVar used for the Union type""""""\n\n    _type_values = (type, type(None))\n    _letter = ""U""\n\n\nclass Meta(type, FusedType):\n    """"""Type of the Transonic types (used to create metaclasses)""""""\n\n    def __call__(cls, *args, **kwargs):\n        raise RuntimeError(""Transonic types are not meant to be instantiated"")\n\n    def is_fused_type(self):\n        template_parameters = self.get_template_parameters()\n        for template_parameter in template_parameters:\n            if hasattr(template_parameter, ""is_fused_type""):\n                if template_parameter.is_fused_type():\n                    return True\n            if hasattr(template_parameter, ""has_multiple_values""):\n                if template_parameter.has_multiple_values():\n                    return True\n        return False\n\n\nclass MemLayout(Enum):\n    C = auto()\n    F = auto()\n    C_or_F = auto()\n    strided = auto()\n\n    def __repr__(self):\n        return f\'""{self.name}""\'\n\n\ndef str2shape(str_shape):\n    assert str_shape.startswith(""["") and str_shape.endswith(""]"")\n    str_shape = str_shape.replace("" "", """")\n    if str_shape == ""[]"":\n        return (None,)\n    n = str_shape.count(""]"")\n    if n > 1:\n        return (None,) * n\n    shape = []\n    for symbol in str_shape[1:-1].split("",""):\n        if symbol == "":"":\n            value = None\n        elif symbol == """":\n            continue\n        else:\n            value = int(symbol)\n        shape.append(value)\n    return tuple(shape)\n\n\ndef shape2str(shape):\n    symbols = ["":"" if value is None else str(value) for value in shape]\n    tmp = "","".join(symbols)\n    return f\'""[{tmp}]""\'\n\n\nclass ArrayMeta(Meta):\n    """"""Metaclass for the Array class""""""\n\n    def __getitem__(self, parameters):\n\n        if not isinstance(parameters, tuple):\n            parameters = (parameters,)\n\n        dtype = None\n        ndim = None\n        memview = False\n        mem_layout = MemLayout.C_or_F\n        shape = None\n        positive_indices = False\n        params_filtered = []\n        for param in parameters:\n            if param is None:\n                continue\n            if isinstance(param, (Type, type, np.dtype)):\n                if dtype is not None:\n                    raise ValueError(\n                        ""Array should be defined with only one variable defining ""\n                        ""the types. For more than one type, use ""\n                        ""for example Type(float, int)""\n                    )\n                if isinstance(param, np.dtype):\n                    param = param.type\n\n                dtype = param\n\n            if isinstance(param, NDim):\n                if ndim is not None:\n                    raise ValueError(\n                        ""Array should be defined with only ""\n                        ""one NDim. For more than one dimension, use ""\n                        ""for example NDim(2, 3).""\n                    )\n                ndim = param\n\n            if (\n                isinstance(param, str)\n                and param[-1] == ""d""\n                and param[:-1].isnumeric()\n            ):\n                try:\n                    tmp = int(param[:-1])\n                except ValueError:\n                    pass\n                else:\n                    if ndim is not None:\n                        raise ValueError(\n                            ""Array should be defined with only ""\n                            ""one string fixing the number of dimension. ""\n                            ""Use for example NDim(2, 3).""\n                        )\n                    param = ndim = NDim(\n                        tmp, name_calling_module=get_name_calling_module()\n                    )\n\n            if isinstance(param, str):\n                param = param.strip()\n                if param == ""memview"":\n                    memview = True\n                    continue\n                if param == ""positive_indices"":\n                    positive_indices = True\n                    continue\n                if param.startswith(""["") and param.endswith(""]""):\n                    shape = str2shape(param)\n                    continue\n                try:\n                    mem_layout = MemLayout[param]\n                    continue\n                except KeyError:\n                    pass\n                raise ValueError(f""{param} cannot be interpretted..."")\n\n            params_filtered.append(param)\n\n        if shape is not None:\n            if ndim is None:\n                ndim = NDim(\n                    len(shape), name_calling_module=get_name_calling_module()\n                )\n                params_filtered.append(ndim)\n            elif ndim != len(shape):\n                raise ValueError(""ndim != len(shape)"")\n            if not any(shape):\n                shape = None\n\n        if dtype is None:\n            raise ValueError(""No way to determine the dtype of the array"")\n\n        if ndim is None:\n            raise ValueError(""No way to determine the ndim of the array"")\n\n        parameters = {p.__name__: p for p in params_filtered}\n\n        assert isinstance(ndim, NDim)\n\n        if hasattr(dtype, ""short_repr""):\n            dtype_name = dtype.short_repr()\n        else:\n            dtype_name = dtype.__name__\n\n        return type(\n            f""Array_{dtype_name}_{ndim.short_repr()}"",\n            (Array,),\n            {\n                ""dtype"": dtype,\n                ""ndim"": ndim,\n                ""parameters"": parameters,\n                ""memview"": memview,\n                ""mem_layout"": mem_layout,\n                ""shape"": shape,\n                ""positive_indices"": positive_indices,\n            },\n        )\n\n    def get_parameters(self):\n        return getattr(self, ""parameters"", dict())\n\n    def get_template_parameters(self):\n        return tuple(\n            param\n            for param in self.get_parameters().values()\n            if isinstance(param, TemplateVar)\n        )\n\n    def __repr__(self):\n        if not hasattr(self, ""parameters""):\n            return super().__repr__()\n\n        if self.shape is not None:\n            parameters = [\n                param\n                for param in self.parameters.values()\n                if not isinstance(param, NDim)\n            ]\n        else:\n            parameters = self.parameters.values()\n\n        strings = []\n        for p in parameters:\n            if isinstance(p, type):\n                string = p.__name__\n            else:\n                string = repr(p)\n            strings.append(string)\n\n        if self.shape is not None:\n            strings.append(shape2str(self.shape))\n\n        if self.memview:\n            strings.append(\'""memview""\')\n\n        if self.mem_layout is not MemLayout.C_or_F:\n            strings.append(repr(self.mem_layout))\n\n        if self.positive_indices:\n            strings.append(\'""positive_indices""\')\n\n        return f""Array[{\', \'.join(strings)}]""\n\n    def format_as_backend_type(self, backend_type_formatter, **kwargs):\n\n        dtype = ndim = None\n\n        for var in self.parameters.values():\n            if isinstance(var, Type) and var.values:\n                dtype = var.values[0]\n            elif isinstance(var, NDim) and var.values:\n                ndim = var.values[0]\n            elif isinstance(var, type):\n                dtype = var\n\n        for key, value in kwargs.items():\n            try:\n                template_var = self.parameters[key]\n            except KeyError:\n                continue\n\n            if isinstance(template_var, Type):\n                dtype = value\n            elif isinstance(template_var, NDim):\n                ndim = value + template_var.shift\n            else:\n                raise ValueError\n\n        if dtype is None or ndim is None:\n            raise ValueError\n\n        memview = kwargs.get(""memview"", self.memview)\n        return backend_type_formatter.make_array_code(\n            dtype,\n            ndim,\n            self.shape,\n            memview,\n            self.mem_layout,\n            self.positive_indices,\n        )\n\n\nclass Array(metaclass=ArrayMeta):\n    """"""Represent a Numpy array.\n\n    >>> Array[int, ""2d""]\n    Array[int, ""2d""]\n\n    >>> Array[int, ""2d"", ""C""]\n    Array[int, ""2d"", ""C""]\n\n    >>> Array[int, ""2d"", ""F""]\n    Array[int, ""2d"", ""F""]\n\n    >>> Array[int, ""2d"", ""strided""]\n    Array[int, ""2d"", ""strided""]\n\n    Fused types:\n\n    >>> Array[Type(int, float), ""1d""]\n    Array[Type(int, float), ""1d""]\n\n    >>> Array[float, NDim(2, 3)]\n    Array[float, NDim(2, 3)]\n\n    >>> Array[int, ""1d"", ""C"", ""positive_indices""]\n    Array[int, ""1d"", ""C"", ""positive_indices""]\n\n    """"""\n\n\nclass UnionMeta(Meta):\n    """"""Metaclass for the Union class""""""\n\n    def __getitem__(self, types):\n\n        types_in = types\n        if not isinstance(types_in, tuple):\n            types_in = (types_in,)\n\n        types = []\n        for type_ in types_in:\n            if isinstance(type_, str):\n                type_ = str2type(type_)\n            types.append(type_)\n        types = tuple(types)\n\n        name_calling_module = get_name_calling_module()\n        template_var = UnionVar(*types, name_calling_module=name_calling_module)\n\n        short_repr = []\n        for value in types:\n            if hasattr(value, ""short_repr""):\n                short_repr.append(value.short_repr())\n            elif hasattr(value, ""__name__""):\n                short_repr.append(value.__name__)\n            else:\n                short_repr.append(repr(value))\n\n        return type(\n            f""Union{\'_\'.join(short_repr)}"",\n            (Union,),\n            {""types"": types, ""template_var"": template_var},\n        )\n\n    def get_template_parameters(self):\n        template_params = []\n        for type_ in self.types:\n            if hasattr(type_, ""get_template_parameters""):\n                template_params.extend(type_.get_template_parameters())\n        template_params.append(self.template_var)\n        return tuple(template_params)\n\n    def __repr__(self):\n        strings = []\n        for p in self.types:\n            if isinstance(p, Meta):\n                string = repr(p)\n            elif isinstance(p, type):\n                string = p.__name__\n            else:\n                string = repr(p)\n            strings.append(string)\n        return ""Union["" + "", "".join(strings) + ""]""\n\n    def format_as_backend_type(self, backend_type_formatter, **kwargs):\n        type_ = kwargs.pop(self.template_var.__name__)\n        return format_type_as_backend_type(\n            type_, backend_type_formatter, **kwargs\n        )\n\n    def short_repr(self):\n        return self.__name__\n\n\nclass Union(metaclass=UnionMeta):\n    """"""Similar to typing.Union\n\n    >>> Union[float, Array[int, ""1d""]]\n    Union[float, Array[int, ""1d""]]\n\n    """"""\n\n\nclass ListMeta(Meta):\n    """"""Metaclass for the List class""""""\n\n    def __getitem__(self, type_elem):\n        if isinstance(type_elem, str):\n            type_elem = str2type(type_elem)\n        return type(""ListBis"", (List,), {""type_elem"": type_elem})\n\n    def get_template_parameters(self):\n        if hasattr(self.type_elem, ""get_template_parameters""):\n            return self.type_elem.get_template_parameters()\n        return tuple()\n\n    def __repr__(self):\n        if isinstance(self.type_elem, Meta):\n            string = repr(self.type_elem)\n        elif isinstance(self.type_elem, type):\n            string = self.type_elem.__name__\n        else:\n            string = repr(self.type_elem)\n        return f""List[{string}]""\n\n    def format_as_backend_type(self, backend_type_formatter, **kwargs):\n        return backend_type_formatter.make_list_code(self.type_elem, **kwargs)\n\n\nclass List(metaclass=ListMeta):\n    """"""Similar to typing.List\n\n    >>> List[List[int]]\n    List[List[int]]\n\n    """"""\n\n\nclass DictMeta(Meta):\n    """"""Metaclass for the Dict class""""""\n\n    def __getitem__(self, types):\n        type_keys, type_values = types\n        if isinstance(type_keys, str):\n            type_keys = str2type(type_keys)\n        if isinstance(type_values, str):\n            type_values = str2type(type_values)\n        return type(\n            ""DictBis"",\n            (Dict,),\n            {""type_keys"": type_keys, ""type_values"": type_values},\n        )\n\n    def get_template_parameters(self):\n        template_params = []\n        if hasattr(self.type_keys, ""get_template_parameters""):\n            template_params.extend(self.type_keys.get_template_parameters())\n        if hasattr(self.type_values, ""get_template_parameters""):\n            template_params.extend(self.type_values.get_template_parameters())\n        return template_params\n\n    def __repr__(self):\n        if isinstance(self.type_keys, type):\n            key = self.type_keys.__name__\n        else:\n            key = repr(self.type_keys)\n        if isinstance(self.type_values, type):\n            value = self.type_values.__name__\n        else:\n            value = repr(self.type_values)\n        return f""Dict[{key}, {value}]""\n\n    def format_as_backend_type(self, backend_type_formatter, **kwargs):\n        return backend_type_formatter.make_dict_code(\n            self.type_keys, self.type_values, **kwargs\n        )\n\n\nclass Dict(metaclass=DictMeta):\n    """"""Similar to typing.Dict\n\n    >>> Dict[str, int]\n    Dict[str, int]\n\n    """"""\n\n\nclass SetMeta(Meta):\n    """"""Metaclass for the Set class""""""\n\n    def __getitem__(self, type_keys):\n        if isinstance(type_keys, str):\n            type_keys = str2type(type_keys)\n        return type(""SetBis"", (Set,), {""type_keys"": type_keys})\n\n    def get_template_parameters(self):\n        if hasattr(self.type_keys, ""get_template_parameters""):\n            return self.type_keys.get_template_parameters()\n        else:\n            return tuple()\n\n    def __repr__(self):\n        if isinstance(self.type_keys, type):\n            key = self.type_keys.__name__\n        else:\n            key = repr(self.type_keys)\n        return f""Set[{key}]""\n\n    def format_as_backend_type(self, backend_type_formatter, **kwargs):\n        return backend_type_formatter.make_set_code(self.type_keys, **kwargs)\n\n\nclass Set(metaclass=SetMeta):\n    """"""Similar to typing.Set\n\n    >>> Set[str]\n    Set[str]\n\n    """"""\n\n\nclass TupleMeta(Meta):\n    """"""Metaclass for the Tuple class""""""\n\n    def __getitem__(self, types):\n\n        if not isinstance(types, tuple):\n            types = (types,)\n\n        trans_types = []\n        for type_in in types:\n            if isinstance(type_in, str):\n                type_in(str2type(type_in))\n            trans_types.append(type_in)\n\n        return type(""TupleBis"", (Tuple,), {""types"": trans_types})\n\n    def get_template_parameters(self):\n        template_params = []\n        for type_ in self.types:\n            if hasattr(type_, ""get_template_parameters""):\n                template_params.extend(type_.get_template_parameters())\n        return tuple(template_params)\n\n    def __repr__(self):\n        strings = []\n        for type_ in self.types:\n            if isinstance(type_, Meta):\n                name = repr(type_)\n            elif isinstance(type_, type):\n                name = type_.__name__\n            else:\n                name = repr(type_)\n            strings.append(name)\n        return f""Tuple[{\', \'.join(strings)}]""\n\n    def format_as_backend_type(self, backend_type_formatter, **kwargs):\n        return backend_type_formatter.make_tuple_code(self.types, **kwargs)\n\n\nclass Tuple(metaclass=TupleMeta):\n    """"""Similar to typing.Tuple\n\n    >>> Tuple[int, Array[int, ""2d""]]\n    Tuple[int, Array[int, ""2d""]]\n\n    """"""\n\n\nclass OptionalMeta(Meta):\n    def __getitem__(self, type_):\n        return Union[type_, None]\n\n\nclass Optional(metaclass=OptionalMeta):\n    """"""Similar to typing.Optional\n\n    >>> Optional[int]\n    Union[int, None]\n\n    """"""\n\n\ndef format_type_as_backend_type(type_, backend_type_formatter, **kwargs):\n    """"""Format a Transonic type as a backend (Pythran, Cython, ...) type\n\n    """"""\n    if type_ is None:\n        # None has a special meaning for typing...\n        return ""None""\n\n    if isinstance(type_, str):\n        type_ = str2type(type_)\n\n    if hasattr(type_, ""format_as_backend_type""):\n        backend_type = type_.format_as_backend_type(\n            backend_type_formatter, **kwargs\n        )\n    elif hasattr(type_, ""__name__""):\n        backend_type = type_.__name__\n    else:\n        print(""type_"", type_, type(type_))\n        raise RuntimeError(f""type_: {type_}"")\n\n    assert backend_type is not None\n\n    return backend_type_formatter.normalize_type_name(backend_type)\n\n\ndef str2type(str_type):\n    """"""Compute a Transonic type from a string\n\n    >>> str2type(""int[:,:]"")\n    Array[int, ""2d""]\n\n    >>> str2type(""int or float[]"")\n    Union[int, Array[float, ""1d""]]\n\n    >>> str2type(""(int, float[:, :])"")\n    Tuple[int, Array[float, ""2d""]]\n    """"""\n\n    str_type = str_type.strip()\n\n    if "" or "" in str_type:\n        subtypes = str_type.split("" or "")\n        return Union[tuple(str2type(subtype) for subtype in subtypes)]\n\n    try:\n        return eval(str_type)\n    except (TypeError, SyntaxError, NameError):\n        # not a simple type\n        pass\n\n    # could be a numpy type\n    try:\n        if not str_type.startswith(""np.""):\n            dtype = ""np."" + str_type\n        else:\n            dtype = str_type\n        return eval(dtype, {""np"": np})\n    except (TypeError, SyntaxError, AttributeError):\n        pass\n\n    if str_type.startswith(""("") and str_type.endswith("")""):\n        re_comma = re.compile(r"",(?![^\\[]*\\])(?![^\\(]*\\))"")\n        return Tuple[\n            tuple(\n                str2type(word) for word in re_comma.split(str_type[1:-1]) if word\n            )\n        ]\n\n    words = [word for word in str_type.split("" "") if word]\n\n    if words[-1] == ""list"":\n        return List["" "".join(words[:-1])]\n\n    if words[-1] == ""dict"":\n        if len(words) != 3:\n            raise NotImplementedError(f""words: {words}"")\n        key = words[0][:-1]\n        value = words[1]\n        return Dict[key, value]\n\n    if words[-1] == ""set"":\n        if len(words) != 2:\n            raise NotImplementedError(f""words: {words}"")\n        key = words[0]\n        return Set[key]\n\n    # str_type should be of the form ""int[]""\n    if ""["" not in str_type:\n        raise ValueError(f""Can\'t determine the Transonic type from \'{str_type}\'"")\n\n    dtype, str_shape = str_type.split(""["", 1)\n    if not dtype.startswith(""np.""):\n        dtype = ""np."" + dtype\n    str_shape = ""["" + str_shape\n    dtype = eval(dtype, {""np"": np})\n    return Array[dtype, str_shape]\n\n\n_simple_types = (int, float, complex, str)\n\n\ndef typeof(obj):\n    """"""Compute the Transonic type corresponding to a Python object\n\n    Supports:\n\n    - simple Python types (int, float, complex, str)\n    - homogeneous list, dict and set\n    - tuple\n    - numpy scalars\n    - numpy arrays\n\n    """"""\n    if isinstance(obj, _simple_types):\n        return type(obj)\n\n    if isinstance(obj, tuple):\n        return Tuple[tuple(typeof(elem) for elem in obj)]\n\n    if isinstance(obj, (list, dict, set)) and not obj:\n        raise ValueError(\n            f""Cannot determine the full type of an empty {type(obj)}""\n        )\n\n    if isinstance(obj, list):\n        type_elem = type(obj[0])\n        if not all(isinstance(elem, type_elem) for elem in obj):\n            raise ValueError(""The list {obj} is not homogeneous in type"")\n\n        return List[typeof(obj[0])]\n\n    if isinstance(obj, (dict, set)):\n        key = next(iter(obj))\n        type_key = type(key)\n        if not all(isinstance(key, type_key) for key in obj):\n            raise ValueError(""The dict {obj} is not homogeneous in type"")\n\n        if isinstance(obj, dict):\n            value = next(iter(obj.values()))\n            type_value = type(value)\n            if not all(isinstance(value, type_value) for value in obj.values()):\n                raise ValueError(""The dict {obj} is not homogeneous in type"")\n            return Dict[typeof(key), typeof(value)]\n        else:\n            return Set[typeof(key)]\n\n    # TODO: Tuple\n    if isinstance(obj, tuple):\n        raise NotImplementedError\n\n    if isinstance(obj, np.ndarray):\n        if np.isscalar(obj):\n            return obj.dtype.type\n\n        # TODO: deeper analysis\n        return Array[obj.dtype, f""{obj.ndim}d""]\n\n    if isinstance(obj, np.generic):\n        return type(obj)\n\n    raise NotImplementedError(\n        f""Not able to determine the full type of {obj} (of type {type(obj)})""\n    )\n\n\nclass ConstType(Type):\n    """"""Private API class for const""""""\n\n    def __init__(self, type_):\n        self.type = type_\n\n    def format_as_backend_type(self, backend_type_formatter, **kwargs):\n        return backend_type_formatter.make_const_code(\n            format_type_as_backend_type(\n                self.type, backend_type_formatter, **kwargs\n            )\n        )\n\n    def __repr__(self):\n        return f""const({repr(self.type)})""\n\n    def is_fused_type(self):\n        return self.type.is_fused_type()\n\n    def get_template_parameters(self):\n        return self.type.get_template_parameters()\n\n    def short_repr(self):\n\n        if hasattr(self.type, ""short_repr""):\n            short_repr_type = self.type.short_repr()\n        else:\n            short_repr_type = repr(self.type)\n\n        return f""constI{short_repr_type}I""\n\n\ndef const(type_):\n    """"""Declare a type as constant (``const`` C/Cython keyword)""""""\n    return ConstType(type_)\n'"
transonic/util.py,0,"b'""""""Internal utilities\n=====================\n\nPublic API\n----------\n\n.. autofunction:: set_compile_at_import\n\nInternal API\n------------\n\n.. autofunction:: find_module_name_from_path\n\n.. autofunction:: get_module_name\n\n.. autofunction:: modification_date\n\n.. autofunction:: has_to_build\n\n.. autofunction:: get_source_without_decorator\n\n.. autoclass:: TypeHintRemover\n   :members:\n   :private-members:\n\n.. autofunction:: strip_typehints\n\n.. autofunction:: get_ipython_input\n\n.. autofunction:: get_info_from_ipython\n\n.. autofunction:: has_to_compile_at_import\n\n.. autofunction:: import_from_path\n\n.. autofunction:: query_yes_no\n\n.. autofunction:: clear_cached_extensions\n\n.. autofunction:: is_method\n\n.. autofunction:: has_to_write\n\n.. autofunction:: write_if_has_to_write\n\n""""""\n\nimport os\nimport sys\nimport inspect\nimport re\nfrom pathlib import Path\nimport importlib.util\nfrom distutils.util import strtobool\nimport shutil\nfrom textwrap import dedent\nfrom typing import Callable\n\nimport gast as ast\n\nfrom transonic.config import backend_default\n\ntry:\n    # since black is still beta (in 03/2019), we cannot impose a version :-(\n    import black\nexcept ImportError:\n    import autopep8\n\n    def format_str(src_contents):\n        return autopep8.fix_code(src_contents)\n\n\nelse:\n    try:\n        _mode = black.FileMode(line_length=82)\n    except TypeError:\n\n        def format_str(src_contents: str):\n            return black.format_str(src_contents, line_length=82)\n\n    else:\n\n        def format_str(src_contents: str):\n            return black.format_str(src_contents, mode=_mode)\n\n\ntry:\n    from IPython.core.getipython import get_ipython\nexcept ImportError:\n    pass\n\nfrom transonic import __version__\nfrom transonic.analyses import extast\n\nfrom transonic.compiler import (\n    ext_suffix,\n    make_hex,\n    modification_date,\n    has_to_build,\n)\n\nfrom transonic.config import path_root\n\n\n__all__ = [""modification_date"", ""has_to_build""]\n\n\ndef can_import_accelerator(backend: str = backend_default):\n    if backend == ""pythran"":\n        try:\n            import pythran\n        except ImportError:\n            return False\n    elif backend == ""cython"":\n        try:\n            import cython\n        except ImportError:\n            return False\n    elif backend == ""numba"":\n        try:\n            import numba\n        except ImportError:\n            return False\n    elif backend == ""python"":\n        return True\n    else:\n        raise NotImplementedError\n    return True\n\n\ndef print_versions(accelerators=None):\n\n    print(f""Transonic {__version__}"")\n\n    if accelerators is None or ""pythran"" in accelerators:\n        try:\n            import pythran\n        except ImportError:\n            print(""Pythran not importable"")\n        else:\n            print(f""Pythran {pythran.__version__}"")\n\n    if accelerators is None or ""numba"" in accelerators:\n        try:\n            import numba\n        except ImportError:\n            print(""Numba not importable"")\n        else:\n            print(f""Numba {numba.__version__}"")\n\n    if accelerators is None or ""cython"" in accelerators:\n        try:\n            import Cython\n        except ImportError:\n            print(""Cython not importable"")\n        else:\n            print(f""Cython {Cython.__version__}"")\n\n\ndef find_module_name_from_path(path_py: Path):\n    """"""Find the module name from the path of a Python file\n\n    It is done by looking to ``sys.path`` to see how the module can be imported.\n\n    """"""\n    path_py = Path(path_py)\n\n    cwd = Path.cwd()\n    path = path_py.absolute().parent\n    module_name = path_py.stem\n\n    # special case for jit_classes\n    special_dir = ""__jit_class__""\n    if special_dir in path.parts:\n        tmp = [special_dir]\n        name_pack = ""."".join(path.parts[path.parts.index(special_dir) + 1 :])\n        if name_pack:\n            tmp.append(name_pack)\n        tmp.append(module_name)\n        return ""."".join(tmp)\n\n    while path.parents:\n        if path == cwd or str(path) in sys.path:\n            return module_name\n\n        module_name = path.name + ""."" + module_name\n        path = path.parent\n\n    return path_py.stem\n\n\ndef get_module_name(frame):\n    """"""Get the full module name""""""\n    module = inspect.getmodule(frame[0])\n    filename = None\n    if module is not None:\n        module_name = module.__name__\n        if module_name in (""__main__"", ""<run_path>""):\n            filename = frame.filename\n    else:\n        filename = frame.filename\n\n    if filename is not None:\n        module_name = find_module_name_from_path(Path(filename))\n\n    if module_name is None:\n        # ipython ?\n        src, module_name = get_info_from_ipython()\n\n    return module_name\n\n\ndef get_name_calling_module(index_frame: int = 1):\n    try:\n        frame = inspect.stack()[index_frame]\n    except IndexError:\n        print(""index_frame"", index_frame)\n        print([frame[1] for frame in inspect.stack()])\n        raise\n\n    return get_module_name(frame)\n\n\ndef get_source_without_decorator(func: Callable):\n    """"""Get the source of a function without its decorator""""""\n    src = inspect.getsource(func)\n    src = dedent(src)\n    return strip_typehints(re.sub(r""@.*?\\sdef\\s"", ""def "", src))\n\n\nclass TypeHintRemover(ast.NodeTransformer):\n    """"""Strip the type hints\n\n    from https://stackoverflow.com/a/42734810/1779806\n    """"""\n\n    def visit_FunctionDef(self, fdef):\n        # remove the return type defintion\n        fdef.returns = None\n        # remove all argument annotations\n        if fdef.args.args:\n            for arg in fdef.args.args:\n                arg.annotation = None\n\n        body = []\n        for node in fdef.body:\n            if isinstance(node, ast.AnnAssign):\n                if node.value is None:\n                    continue\n                node = ast.Assign(targets=[node.target], value=node.value)\n            body.append(node)\n        fdef.body = body\n\n        return fdef\n\n\ndef strip_typehints(source):\n    """"""Strip the type hints from a function""""""\n    source = format_str(source)\n    # parse the source code into an AST\n    parsed_source = ast.parse(source)\n    # remove all type annotations, function return type definitions\n    # and import statements from \'typing\'\n    transformed = TypeHintRemover().visit(parsed_source)\n    # convert the AST back to source code\n    striped_code = extast.unparse(transformed)\n    return striped_code\n\n\ndef make_code_from_fdef_node(fdef):\n    transformed = TypeHintRemover().visit(fdef)\n    # convert the AST back to source code\n    code = extast.unparse(transformed)\n    return format_str(code)\n\n\ndef get_ipython_input(last=True):\n    """"""Get the input code when called from IPython""""""\n    ip = get_ipython()\n\n    hist_raw = ip.history_manager.input_hist_raw\n    if last:\n        return hist_raw[-1]\n    else:\n        return ""\\n"".join(hist_raw)\n\n\ndef get_info_from_ipython():\n    """"""Get the input code and a ""filename"" when called from IPython""""""\n    src = get_ipython_input()\n    hex_input = make_hex(src)\n    dummy_filename = ""__ipython__"" + hex_input\n    return src, dummy_filename\n\n\n_PYTHRANIZE_AT_IMPORT = None\n\n\ndef set_compile_at_import(value=True):\n    """"""Control the ""compile_at_import"" mode""""""\n    global _PYTHRANIZE_AT_IMPORT\n    _PYTHRANIZE_AT_IMPORT = value\n\n\ndef has_to_compile_at_import():\n    """"""Check if transonic has to pythranize at import time""""""\n    if _PYTHRANIZE_AT_IMPORT is not None:\n        return _PYTHRANIZE_AT_IMPORT\n    return ""TRANSONIC_COMPILE_AT_IMPORT"" in os.environ\n\n\ndef import_from_path(path: Path, module_name: str):\n    """"""Import a .py file or an extension from its path\n\n    """"""\n    if not path.exists():\n        raise ImportError(\n            f""File {path} does not exist. ""\n            f""[path.name for path in path.parent.glob(\'*\')]:\\n{[path.name for path in path.parent.glob(\'*\')]}\\n""\n        )\n\n    if ""."" in module_name:\n        package_name, mod_name = module_name.rsplit(""."", 1)\n        name_file = path.name.split(""."", 1)[0]\n        if mod_name != name_file:\n            module_name = ""."".join((package_name, name_file))\n    else:\n        module_name = path.stem\n\n    if module_name in sys.modules:\n        module = sys.modules[module_name]\n\n        if module.__file__.endswith(ext_suffix) and Path(module.__file__) == path:\n            return module\n\n    spec = importlib.util.spec_from_file_location(module_name, path)\n    # for potential ""local imports"" in the module\n    sys.path.insert(0, str(path.parent))\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    # clean sys.path\n    sys.path.pop(0)\n    # fix bug Numba\n    sys.modules[module_name] = module\n    return module\n\n\ndef query_yes_no(question: str, default: str = None, force: bool = False):\n    """"""User yes or no query""""""\n    if force:\n        return True\n\n    if default is None:\n        end = ""(y/n)""\n        default = """"\n    elif default == ""y"":\n        end = ""([y]/n)""\n    elif default == ""n"":\n        end = ""(y/[n])""\n\n    print(f""{question} {end}"")\n    while True:\n        answer = input()\n        if answer == """":\n            answer = default\n        try:\n            return strtobool(answer)\n        except ValueError:\n            print(\'Please respond with ""y"" or ""n"".\')\n\n\ndef clear_cached_extensions(module_name: str, force: bool, backend: str):\n    """"""Delete the cached extensions related to a module\n\n    """"""\n\n    from transonic.backends import backends\n    from transonic import mpi\n\n    backend = backends[backend]\n    path_jit = mpi.Path(backend.jit.path_base)\n\n    if module_name.endswith("".py""):\n        module_name = module_name[:-3]\n\n    if os.path.sep not in module_name:\n        relative_path = module_name.replace(""."", os.path.sep)\n    else:\n        relative_path = module_name\n\n    path_pythran_dir_jit = path_jit / relative_path\n\n    relative_path = Path(relative_path)\n\n    path_pythran = relative_path.parent / (\n        ""__{backend.name}__/"" + relative_path.name + "".py""\n    )\n    path_ext = path_pythran.with_name(\n        backend.name_ext_from_path_backend(path_pythran)\n    )\n\n    if not path_pythran_dir_jit.exists() and not (\n        path_pythran.exists() or path_ext.exists()\n    ):\n        print(\n            f""Not able to find cached extensions corresponding to {module_name}""\n        )\n        print(""Nothing to do! \xe2\x9c\xa8 \xf0\x9f\x8d\xb0 \xe2\x9c\xa8."")\n        return\n\n    if path_pythran_dir_jit.exists() and query_yes_no(\n        f""Do you confirm that you want to delete the cached files for {module_name}"",\n        default=""y"",\n        force=force,\n    ):\n        print(f""Remove directory {path_pythran_dir_jit}"")\n        shutil.rmtree(path_pythran_dir_jit)\n\n    if path_pythran.exists() or path_ext.exists():\n        if query_yes_no(\n            f""Do you confirm that you want to delete the AOT cache for {module_name}"",\n            default=""y"",\n            force=force,\n        ):\n            for path in (path_pythran, path_ext):\n                if path.exists():\n                    path.unlink()\n\n\ndef is_method(func):\n    """"""Determine wether a function is going to be used as a method""""""\n    signature = inspect.signature(func)\n    try:\n        answer = next(iter(signature.parameters.keys())) == ""self""\n    except StopIteration:\n        answer = False\n\n    return answer\n\n\ndef has_to_write(path_file: Path, new_code: str):\n    """"""Check if a file exists and contains a code""""""\n    if path_file.exists():\n        with open(path_file, ""r"") as file:\n            source_code = file.read()\n        if new_code == source_code:\n            return False\n        else:\n            return True\n    else:\n        return True\n\n\ndef write_if_has_to_write(\n    path_file: Path, new_code: str, logger=None, force=False\n):\n    """"""Write a file if it doesn\'t exist or doesn\'t contain a particular code""""""\n    written = False\n    if has_to_write(path_file, new_code) or force:\n        written = True\n        path_file.parent.mkdir(exist_ok=True, parents=True)\n        with open(path_file, ""w"") as file:\n            file.write(new_code)\n        if logger:\n            logger(f""{path_file} written"")\n    return written\n\n\ndef timeit(stmt=""pass"", setup=""pass"", total_duration=2, globals=None):\n    """"""timeit with a approximate total_duration""""""\n    from timeit import Timer\n\n    timer = Timer(stmt, setup, globals=globals)\n    timer.timeit(10)\n    duration1 = timer.timeit(10) / 10\n    number = max(1, int(round(total_duration / duration1)))\n\n    if number > 200:\n        repeat = int(round(number / 100))\n        number = int(round(number / repeat))\n    else:\n        repeat = 1\n\n    if number * duration1 > 2 * total_duration:\n        raise RuntimeError(""number * duration1 > 2 * total_duration"")\n\n    duration = min(timer.repeat(repeat=repeat, number=number))\n    return duration / number\n\n\ndef timeit_verbose(stmt, setup=""pass"", total_duration=2, globals=None, norm=None):\n    ret = result = timeit(\n        stmt, setup=setup, total_duration=total_duration, globals=globals\n    )\n    if norm is None:\n        norm = result\n    result /= norm\n    print(f""{stmt.split(\'(\')[0]:33s}: {result:.3f} * norm"")\n    if result == 1.0:\n        print(f""norm = {norm:.2e} s"")\n    return ret\n'"
transonic_cl/__init__.py,0,b''
transonic_cl/cython.py,0,b'def decor_1_value(value):\n    return lambda x: x\n\n\ncdivision = nonecheck = wraparound = boundscheck = decor_1_value\n\n\ndef nogil(func):\n    return func\n'
transonic_cl/cythonize.py,1,"b'import sys\nfrom distutils.core import setup\n\nfrom Cython.Build import cythonize\nimport numpy as np\n\npath = sys.argv.pop()\nsys.argv.extend((""build_ext"", ""--inplace""))\n\nsetup(\n    ext_modules=cythonize(path, language_level=3), include_dirs=[np.get_include()]\n)\n'"
transonic_cl/run_backend.py,0,"b'""""""Command line\n===============\n\nInternal API\n------------\n\n.. autofunction:: main\n\n""""""\n\nimport subprocess\nimport sys\nimport logging\nfrom pathlib import Path\nimport sysconfig\nfrom time import time, sleep\nfrom shutil import copyfile\nimport os\n\nlogger = logging.getLogger(""transonic"")\nlogger.setLevel(logging.INFO)\n\next_suffix = sysconfig.get_config_var(""EXT_SUFFIX"") or "".so""\n\n\ndef main():\n    """"""Minimal layer above the Pythran commandline""""""\n\n    if ""-b"" in sys.argv:\n        if sys.argv[sys.argv.index(""-b"") + 1]:\n            index = sys.argv.index(""-b"")\n            backend = sys.argv[index + 1]\n            del sys.argv[index]\n            del sys.argv[index]\n        else:\n            raise ValueError(""No backend is specified afert -b"")\n    else:\n        raise ValueError(""No backend is specified"")\n\n    if backend in (""python"", ""numba""):\n        return\n\n    compiling_name = backend.capitalize() + ""izing""\n\n    assert sys.argv[0].endswith(\n        os.path.sep.join((""transonic_cl"", ""run_backend.py""))\n    )\n\n    args = sys.argv[1:]\n    name = args[0]\n    path = Path.cwd() / name\n\n    if ""-o"" in args:\n        index_output = args.index(""-o"") + 1\n        name_out = args[index_output]\n    else:\n        name_out = Path(name).with_suffix(ext_suffix).name\n\n    name_out_base = name_out.split(""."", 1)[0]\n\n    if ""-o"" in args:\n        if backend == ""pythran"":\n            name_tmp = name_out_base + "".tmp""\n            args[index_output] = name_tmp\n        elif backend == ""cython"":\n            name_tmp = name_out_base + "".py""\n            copyfile(name, name_tmp)\n            copyfile(name.split(""."", 1)[0] + "".pxd"", name_out_base + "".pxd"")\n            name = name_tmp\n\n        path_tmp = Path(name_tmp)\n        path_out = path_tmp.with_suffix(ext_suffix)\n    else:\n        path_out = Path(name_out)\n\n    name_lock = Path(name_out_base + "".lock"")\n\n    if name_lock.exists():\n        print(\n            f""lock file {name_lock.absolute()} present: ""\n            ""waiting for completion of the compilation"",\n            flush=True,\n        )\n        time_out_lock = 3600  # (s) let\'s hope it\'s enough\n        time_start = time()\n        while name_lock.exists() and time() - time_start < time_out_lock:\n            sleep(1)\n        if time() - time_start >= time_out_lock:\n            logger.error(f""Remove lock file {name_lock.absolute()}"")\n            name_lock.unlink()\n            raise TimeoutError(\n                f""Stop waiting for a lock file to be deleted {name_lock.absolute()}""\n            )\n\n        assert not name_lock.exists()\n        sleep(1)\n        if not path_out.exists():\n            raise RuntimeError(\n                f""After lock file were deleted, {path_out.absolute()} not created""\n            )\n\n        return\n\n    if ""-v"" in args:\n        # no capture_output\n        stdout = stderr = None\n    else:\n        stdout = stderr = subprocess.PIPE\n\n    print(f""{compiling_name} {path}"", flush=True)\n    if backend == ""pythran"":\n        args.insert(0, ""pythran"")\n    elif backend == ""cython"":\n        args = [sys.executable, ""-m"", ""transonic_cl.cythonize"", name]\n    name_lock.touch()\n    try:\n        completed_process = subprocess.run(\n            args, stdout=stdout, stderr=stderr, universal_newlines=True\n        )\n    except Exception:\n        pass\n    finally:\n        name_lock.unlink()\n    if backend == ""pythran"" and ""-o"" in args and path_tmp.exists():\n        path_tmp.rename(path_out)\n    elif backend == ""cython"":\n        path_tmp.with_suffix("".c"").unlink()\n        path_tmp.with_suffix("".pxd"").unlink()\n        path_tmp.unlink()\n\n    if path_out.exists():\n        print(f""File {path_out.absolute()} created by {backend}"")\n    else:\n        logger.error(\n            f""Error! File {path_out.absolute()} has not been created by {backend}""\n        )\n        try:\n            completed_process\n        except NameError:\n            pass\n        else:\n            if completed_process.stdout:\n                print(\n                    f""{backend.capitalize()} stdout:\\n{completed_process.stdout}""\n                )\n            if completed_process.stderr:\n                logger.error(\n                    f""{backend.capitalize()} stderr:\\n{completed_process.stderr}""\n                )\n        sys.exit(1)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
data_tests/test_packaging/__init__.py,0,b''
data_tests/test_packaging/add.py,0,"b'from transonic import boost\n\nA = ""int[]""\n\n\n@boost\ndef add(f: A, g: A):\n    return f + g\n'"
data_tests/test_packaging/setup.py,1,"b'from distutils.core import setup, Extension\nfrom pathlib import Path\nimport numpy as np\nfrom transonic.dist import (\n    make_backend_files,\n    init_transonic_extensions,\n    ParallelBuildExt,\n)\n\npath_sources = Path(__file__).parent.absolute()\ninclude_dirs = [np.get_include()]\n\ntry:\n    from Cython.Build import cythonize\nexcept ImportError:\n    extensions = [\n        Extension(\n            ""add_cython"",\n            include_dirs=[str(path_sources)] + include_dirs,\n            libraries=[""m""],\n            library_dirs=[],\n            sources=[str(path_sources / ""add_cython.c"")],\n        )\n    ]\n    print(extensions)\nelse:\n    extensions = cythonize(\n        Extension(""add_cython"", [""add_cython.pyx""], include_dirs=include_dirs)\n    )\n\nmake_backend_files([path_sources / ""add.py""])\nextensions.extend(init_transonic_extensions(""."", include_dirs=include_dirs))\n\nsetup(\n    name=""add"",\n    ext_modules=extensions,\n    script_name=""setup.py"",\n    script_args=[""build_ext""],\n    cmdclass=dict(build_ext=ParallelBuildExt),\n)\n'"
doc/examples/blocks.py,4,"b'import numpy as np\n\nfrom transonic import Transonic\n\nts = Transonic()\n\n# don\'t define classes in Pythran file! Here, no problem...\n\n\nclass MyClass:\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n\n    def compute(self, n):\n\n        a = self.a\n        b = self.b\n\n        if ts.is_transpiled:\n            result = ts.use_block(""block0"")\n        else:\n            # transonic block (\n            #     float[][] a, b;\n            #     int n\n            # )\n            # transonic block (\n            #     float[][][] a, b;\n            #     int n\n            # )\n            result = np.zeros_like(a)\n            for _ in range(n):\n                result += a ** 2 + b ** 3\n\n        return result\n\n\nif __name__ == ""__main__"":\n\n    shape = 100, 100\n    a = np.random.rand(*shape)\n    b = np.random.rand(*shape)\n\n    obj = MyClass(a, b)\n\n    obj.compute(10)\n\n    if ts.is_transpiled:\n        ret = obj.compute(10)\n        ts.is_transpiled = False\n        ret1 = obj.compute(10)\n        ts.is_transpiled = True\n        assert np.allclose(ret, ret1)\n        print(""allclose OK"")\n'"
doc/examples/blocks_type_hints.py,4,"b'import numpy as np\n\nfrom transonic import Transonic, Type, NDim, Array\n\nT = Type(float, complex)\nN = NDim(2, 3)\nA = Array[T, N]\nA1 = Array[T, N + 1]\n\nts = Transonic()\n\n\nclass MyClass:\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n\n    def compute(self, n):\n\n        a = self.a\n        b = self.b\n\n        if ts.is_transpiled:\n            result = ts.use_block(""block0"")\n        else:\n            # transonic block (A a, b; A1 c; int n)\n            # transonic block (int a, b, c; float n)\n\n            result = np.zeros_like(a)\n            for _ in range(n):\n                result += a ** 2 + b ** 3\n\n        return result\n\n\nif __name__ == ""__main__"":\n\n    shape = 100, 100\n    a = np.random.rand(*shape)\n    b = np.random.rand(*shape)\n\n    obj = MyClass(a, b)\n\n    obj.compute(10)\n\n    if ts.is_transpiled:\n        ret = obj.compute(10)\n        ts.is_transpiled = False\n        ret1 = obj.compute(10)\n        ts.is_transpiled = True\n        assert np.allclose(ret, ret1)\n        print(""allclose OK"")\n'"
doc/examples/classic.py,3,"b'import numpy as np\n\n# don\'t import any random modules in a Pythran file. Here, no problem!\nfrom fluiddyn.util import mpi\n\nfrom transonic import boost\n\n# transonic def func(float[][], float[][])\n# transonic def func(int[][], float[][])\n\n\n@boost\ndef func(a, b):\n    return (a * np.log(b)).max()\n\n\nif __name__ == ""__main__"":\n\n    n0, n1 = 100, 200\n    a0 = np.random.rand(n0, n1)\n    a1 = np.random.rand(n0, n1)\n\n    result = func(a0, a1)\n    if mpi.nb_proc > 1:\n        result = mpi.comm.allreduce(result, op=mpi.MPI.MAX)\n    mpi.printby0(result)\n\n    a0 = (1000 * a0).astype(int)\n    result = func(a0, a1)\n    if mpi.nb_proc > 1:\n        result = mpi.comm.allreduce(result, op=mpi.MPI.MAX)\n    mpi.printby0(result)\n'"
doc/examples/classic_func_using_func.py,1,"b'import numpy as np\nfrom local_module import multiply\n\nfrom transonic import boost\n\n# transonic def func(float[][], float[][])\n# transonic def func(int[][], float[][])\n\n\ndef my_log(b):\n    return np.log(b)\n\n\n@boost\ndef func(a, b):\n    c = multiply(a,b)\n    return (c * my_log(b)).max()\n'"
doc/examples/jit_dict.py,0,"b'from transonic import jit, wait_for_all_extensions\n\n\n@jit\ndef func_dict(d: ""str: float dict""):\n    return d.popitem()\n\n\nd = {""a"": 0, ""b"": 1}\nresult = func_dict(d)\n\nwait_for_all_extensions()\n\nd1 = {""a"": 0, ""b"": 1}\nresult1 = func_dict(d1)\n\nassert d == d1\nassert result == result1\n'"
doc/examples/jit_fib.py,0,"b'from transonic import jit, wait_for_all_extensions\n\n\n@jit\ndef fib(n: int):\n    """"""fibonacci""""""\n    if n < 2:\n        return n\n    return fib(n - 1) + fib(n - 2)\n\n\n@jit\ndef use_fib():\n    return [fib(n) for n in [1, 3, 5]]\n\n\nfib2 = fib(2)\nresult = use_fib()\nwait_for_all_extensions()\nassert fib2 == fib(2)\nassert result == use_fib()\n'"
doc/examples/local_module.py,0,"b'def multiply(a, b):\n    return a * b'"
doc/examples/methods.py,4,"b'import numpy as np\n\nfrom transonic import boost\n\n\n@boost\nclass Transmitter:\n\n    freq: float\n\n    def __init__(self, freq):\n        self.freq = float(freq)\n\n    @boost\n    def __call__(self, inp: ""float[]""):\n        """"""My docstring""""""\n        return inp * np.exp(np.arange(len(inp)) * self.freq * 1j)\n\n\nif __name__ == ""__main__"":\n    inp = np.ones(2)\n    freq = 1.0\n    trans = Transmitter(freq)\n\n    def for_check(freq, inp):\n        return inp * np.exp(np.arange(len(inp)) * freq * 1j)\n\n    assert np.allclose(trans(inp), for_check(freq, inp))\n'"
doc/examples/methods_jit.py,0,"b'from transonic import boost, jit\n\n\n@boost\nclass OtherClass:\n    """"""Note that there is no type annotations at all!\n\n    The Pythran signature is created at run time with the types of the\n    attributes and the arguments.\n\n    """"""\n\n    def __init__(self, arg):\n        self.attr0 = self.attr1 = 2 * arg\n\n    @jit\n    def calcul(self, a, b):\n        return a * self.attr0 + b * self.attr1\n'"
doc/examples/methods_using_function.py,0,"b'from transonic import boost\n\n\ndef add(a, b):\n    return a + b\n\n\n@boost\nclass MyClass:\n    attr0: float\n    attr1: float\n\n    def __init__(self, arg):\n        self.attr0 = self.attr1 = 2 * float(arg)\n\n    @boost\n    def compute(self, number: int):\n        result = 0.0\n        for _ in range(number):\n            result += add(self.attr0, self.attr1)\n'"
doc/examples/mixed_classic_type_hint.py,2,"b""import numpy as np\n\n# don't import skimage in a Pythran file. Here, no problem!\nfrom skimage.filters import sobel\n\nfrom transonic import boost\n\n# transonic def func(float[][], float[][])\n# transonic def func(int[][], float[][])\n\n\n@boost\ndef func(a: float, b: float):\n    return (a * np.log(b)).max()\n\n\n@boost\ndef func1(a: int, b: float):\n    return a * np.cos(b)\n"""
doc/examples/type_hints_notemplate.py,2,"b'import numpy as np\nfrom transonic import Type, NDim, Array, boost\n\nT = Type(int, np.complex128)\nN = NDim(1, 3)\n\nA = Array[T, N]\nA1 = Array[np.float32, N + 1]\n\n\n@boost\ndef compute(a: A, b: A, c: T, d: A1, e: str):\n    print(e)\n    tmp = a + b\n    return tmp\n'"
doc/examples/type_hints_simple.py,0,"b""# don't import h5py and mpi4py in a Pythran file, here, no problem!\nimport h5py\nimport mpi4py\n\nfrom transonic import boost\n\n\n@boost\ndef myfunc(a: int, b: float):\n    return a * b\n"""
doc/examples/using_jit.py,2,"b'import numpy as np\n\nfrom transonic import jit\n\n\ndef func0(a, b):\n    return a + b\n\n\n@jit\ndef func1(a: int, b: int):\n    print(""b"", b)\n    return np.exp(a) * b * func0(a, b)\n\n\nif __name__ == ""__main__"":\n\n    from time import sleep\n\n    a = b = np.zeros([2, 3])\n\n    for i in range(20):\n        print(f""{i}, call with arrays"")\n        func1(a, b)\n        print(f""{i}, call with numbers"")\n        func1(1, 1.5)\n        sleep(1)\n'"
doc/examples/using_jit_diff_types.py,0,"b'from transonic import jit, Type\n\nT = Type(int, float)\n\n\n@jit()\ndef func(a: T, b: T):\n    return a * b\n\n\nif __name__ == ""__main__"":\n\n    from time import sleep\n\n    a_i = b_i = 1\n    a_f = b_f = 1.0\n\n    for _ in range(10):\n        print(_, end="","", flush=True)\n        func(a_i, b_i)\n        sleep(1)\n\n    print()\n\n    for _ in range(10):\n        print(_, end="","", flush=True)\n        func(a_f, b_f)\n        sleep(1)\n'"
tmp/analyses/analyse.py,0,"b'from pathlib import Path\n\nfrom transonic.analyses.util import print_dumped\n\n\nfrom transonic.backends.pythran import make_pythran_code\n\npath_examples = Path(""examples"")\n\npaths = sorted(path_examples.glob(""*.py""))\npath = paths[4]\n\n# path = Path(""~/Dev/fluidsim/fluidsim/base/time_stepping/pseudo_spect.py"")\n\ncode = make_pythran_code(path)\n\nprint(code)\n'"
tmp/analyses/filter_body.py,0,"b'from pathlib import Path\n\nimport gast as ast\nfrom transonic.analyses import beniget\n\n\nfrom transonic.analyses.util import (\n    get_annotations,\n    print_ast,\n    print_dump,\n    print_unparsed,\n    filter_code_typevars\n)\n\n\npath_examples = Path(""examples"")\n\nfiles = sorted(path_examples.glob(""*.py""))\n\nwith open(files[1]) as file:\n    code = file.read()\n\n\n\nprint(""ast.parse"")\nmodule = ast.parse(code)\n\nprint(""compute DefUseChains"")\nduc = beniget.DefUseChains()\nduc.visit(module)\n\nprint(""compute Ancestors"")\nancestors = beniget.Ancestors()\nancestors.visit(module)\n\n\ncode_filtered = filter_code_typevars(module, duc, ancestors)\n\nprint(code_filtered)'"
tmp/analyses/recreate_annotations.py,0,"b'import gast as ast\n\nfrom transonic.analyses.util import get_annotations\n\ncode = """"""\n\ntype_ = float\n\ndef myfunc(a: int, b: type_):\n    pass\n\n""""""\n\nmodule = ast.parse(code)\nfdef = module.body[1]\n\nprint(get_annotations(fdef, {""type_"": float}))'"
tmp/var_annot/analyze.py,1,"b'\nimport gast as ast\n\nfrom transonic.analyses.capturex import CaptureX\nfrom transonic.analyses import extast\nfrom transonic.analyses import analyse_aot\nfrom transonic.analyses.util import print_dumped, print_unparsed\n\nfrom transonic.backends import backends\n\nbackend = backends[""cython""]\n\npath_file = ""simple.py""\n\nwith open(path_file) as file:\n    code = file.read()\n\nboosted_dicts, code_dependance, annotations, blocks, code_ext = analyse_aot(code, path_file)\n\nmodule = extast.parse(code)\n\nfor node in module.body:\n    if isinstance(node, ast.FunctionDef):\n        fdef = node\n        break\n\n# annotations_locals = annotations[""__locals__""]\n\n# annot = annotations[""functions""][fdef.name]\n\n# fdef.body = []\n# fdef.decorator_list = []\n\n# print_unparsed(fdef)\n\nprint(""\\n"".join(backend.get_signatures(fdef.name, fdef, annotations)))\n\n# print_dumped(""""""\n# @cython.locals(result=np.float64_t, i=cython.int)\n# def mysum(arr_input):\n#     pass\n# """""")\n'"
tmp/var_annot/simple.py,1,"b'import numpy as np\n\nfrom transonic import Type, NDim, Array, boost\n\nT = Type(np.float64, np.complex128)\nN = NDim(1)\nA = Array[T, N]\n\n\n@boost\ndef func(a: A):\n    i: int\n    n: int = a.shape[0]\n\n    for i in range(n):\n        a[i] = a[i] + 1.\n'"
transonic/analyses/__init__.py,0,"b'""""""Code analyses\n================\n\n.. autosummary::\n   :toctree:\n\n    blocks_if\n    capturex\n    extast\n    justintime\n    parser\n    util\n\n""""""\n\nfrom pprint import pformat\n\nimport gast as ast\nfrom transonic.analyses import beniget\n\nfrom transonic.log import logger\n\nfrom .util import (\n    filter_code_typevars,\n    get_annotations,\n    print_dumped,\n    print_unparsed,\n    find_path,\n    get_exterior_code,\n    extract_variable_annotations,\n    extract_returns_annotation,\n)\nfrom .capturex import CaptureX\nfrom .blocks_if import get_block_definitions\nfrom .parser import parse_transonic_def_commands\nfrom . import extast\n\n\n__all__ = [""print_dumped"", ""print_unparsed""]\n\n\ndef compute_ancestors_chains(module_node):\n    """"""Create Beniget objects""""""\n\n    ancestors = beniget.Ancestors()\n    ancestors.visit(module_node)\n\n    duc = beniget.DefUseChains()\n    duc.visit(module_node)\n\n    udc = beniget.UseDefChains(duc)\n\n    return ancestors, duc, udc\n\n\ndef find_decorated_function(module, function_name: str, pathfile: str = None):\n    ext_module = False\n    for node in module.body:\n        if isinstance(node, ast.FunctionDef):\n            if node.name == function_name:\n                return node, ext_module\n        # look for function_name in the imports of module\n        if isinstance(node, ast.ImportFrom):\n            for func in node.names:\n                if func.name == function_name:\n                    # find and read the imported module file\n                    name, path = find_path(node, pathfile)\n                    with open(path) as file:\n                        ext_module = extast.parse(file.read())\n                    # find the definition of function_name in the imported module\n                    node, _ = find_decorated_function(ext_module, function_name)\n                    return node, ext_module\n    raise RuntimeError\n\n\ndef get_decorated_dicts(\n    module, ancestors, duc, pathfile: str, backend_name, decorator=""boost""\n):\n    """"""Get the definitions of the decorated functions and classes""""""\n\n    kinds = (""functions"", ""functions_ext"", ""methods"", ""classes"")\n\n    backend_names = (""pythran"", ""cython"", ""numba"", ""python"")\n    decorated_dicts = {\n        kind: {name: {} for name in backend_names} for kind in kinds\n    }\n\n    def add_definition(node_using_decorator):\n        """"""\n\n        NotImplemented:\n\n        decor = boost(inline=1)\n\n        isinstance(node_using_decorator, ast.Call)\n        isinstance(node_parent, ast.Assign)\n        node_parent.value.keywords\n\n        """"""\n\n        backend_name_def = backend_name\n        decorated_dict = None\n        ext_module = False\n        node_parent = ancestors.parent(node_using_decorator)\n\n        # first, get the keywords\n        keywords = None\n        if isinstance(node_using_decorator, ast.Call):\n            # decorator used in the form boost(...)\n            if node_using_decorator.keywords:\n                # decorator used in the form boost(key=value)\n                keywords = {\n                    keyword.arg: eval(extast.unparse(keyword.value))\n                    for keyword in node_using_decorator.keywords\n                }\n                backend_name_def = keywords.get(""backend"", backend_name)\n\n        if isinstance(\n            node_using_decorator, (ast.FunctionDef, ast.ClassDef)\n        ) and isinstance(node_parent, (ast.Module, ast.ClassDef)):\n            """"""case\n\n            @boost\n            def f():\n                ...\n\n            """"""\n            definition_node = node_using_decorator\n\n        elif isinstance(node_using_decorator, ast.Call) and isinstance(\n            node_parent, (ast.FunctionDef, ast.ClassDef)\n        ):\n            """"""\n            @boost(inline=1)\n            def f():\n                pass\n            """"""\n\n            definition_node = node_parent\n\n        elif isinstance(node_using_decorator, ast.Call) and isinstance(\n            node_parent, ast.Assign\n        ):\n            """"""\n            f1_ = boost(f1)\n            or\n            decor = boost(foo=True)\n            """"""\n\n            call = node_using_decorator\n            if call.keywords:\n                ""decor = boost(foo=True)""\n                raise NotImplementedError\n            else:\n                ""f1_ = boost(f1)""\n                if len(call.args) > 1:\n                    raise NotImplementedError\n                func_name = call.args[0].id\n\n                definition_node, ext_module = find_decorated_function(\n                    module, func_name, pathfile\n                )\n\n        elif isinstance(node_using_decorator, ast.Call) and isinstance(\n            node_parent, ast.Call\n        ):\n            """"""f1_ = boost(inline=1)(f1)""""""\n\n            call = node_parent\n            if call.keywords or len(call.args) > 1:\n                raise NotImplementedError\n            func_name = call.args[0].id\n            definition_node, ext_module = find_decorated_function(\n                module, func_name, pathfile\n            )\n\n        else:\n            raise NotImplementedError\n\n        if keywords is not None:\n            definition_node._transonic_keywords = keywords\n\n        # if the definition node is in an imported module\n        if ext_module:\n            decorated_dict = decorated_dicts[""functions_ext""][backend_name_def]\n            decorated_dict[func_name] = (definition_node, ext_module)\n        # If the definition node is not imported\n        else:\n            func_name = definition_node.name\n            if isinstance(definition_node, ast.FunctionDef):\n                parent = ancestors.parent(definition_node)\n                if isinstance(parent, ast.ClassDef):\n                    decorated_dict = decorated_dicts[""methods""][backend_name_def]\n                    func_name = (parent.name, func_name)\n                else:\n                    decorated_dict = decorated_dicts[""functions""][\n                        backend_name_def\n                    ]\n            elif isinstance(definition_node, ast.ClassDef):\n                decorated_dict = decorated_dicts[""classes""][backend_name_def]\n            else:\n                raise RuntimeError\n            if decorated_dict is not None:\n                decorated_dict[func_name] = definition_node\n\n    # we first need to find the node where transonic.boost is defined...\n    for node in module.body:\n        if isinstance(node, ast.Import):\n            for alias in node.names:\n                if alias.name == ""transonic"":\n                    transonic_def_node = alias\n                    transonic_def = duc.chains[transonic_def_node]\n                    for user in transonic_def.users():\n                        for user1 in user.users():\n                            if (\n                                isinstance(user1.node, ast.Attribute)\n                                and user1.node.attr == decorator\n                            ):\n                                definition_node = ancestors.parent(user1.node)\n                                add_definition(definition_node)\n        elif isinstance(node, ast.ImportFrom):\n            if node.module == ""transonic"":\n                for alias in node.names:\n                    if alias.name == decorator:\n                        boost_def_node = alias\n                        boost_def = duc.chains[boost_def_node]\n                        for user in boost_def.users():\n                            definition_node = ancestors.parent(user.node)\n                            add_definition(definition_node)\n\n    return decorated_dicts\n\n\ndef get_types_from_transonic_signature(signature: str, function_name: str):\n    types = []\n    tmp = signature[len(function_name) + 1 : -1]\n\n    while tmp:\n        try:\n            index_comma = tmp.index("","")\n        except ValueError:\n            tmp = tmp.strip()\n            if tmp:\n                types.append(tmp)\n            break\n\n        try:\n            index_open_bracket = tmp.index(""["")\n        except ValueError:\n            index_open_bracket = None\n\n        if index_open_bracket is None or index_comma < index_open_bracket:\n            type_, tmp = tmp.split("","", 1)\n\n        else:\n            index_close_bracket = tmp.index(""]"")\n            # cover int[][] and int[:, :] but could be buggy!\n            type0 = tmp[:index_close_bracket]\n            type1, tmp = tmp[index_close_bracket:].split("","", 1)\n\n            type_ = type0 + type1\n\n        types.append(type_.strip())\n\n    return types\n\n\ndef analyse_aot(code, pathfile, backend_name):\n    """"""Gather the informations for ``@boost`` and blocks""""""\n    debug = logger.debug\n\n    debug(""extast.parse"")\n    module = extast.parse(code)\n\n    debug(""compute ancestors and chains"")\n    ancestors, duc, udc = compute_ancestors_chains(module)\n\n    debug(""filter_code_typevars"")\n    code_dependance_annotations = filter_code_typevars(module, duc, ancestors)\n    debug(code_dependance_annotations)\n\n    debug(""find boosted objects"")\n    boosted_dicts = get_decorated_dicts(\n        module, ancestors, duc, None, backend_name\n    )\n    debug(pformat(boosted_dicts))\n\n    debug(""compute the annotations"")\n\n    from transonic import aheadoftime\n\n    aheadoftime.is_transpiling = True\n\n    def_nodes = [\n        def_node\n        for boosted_dict in boosted_dicts.values()\n        for boosted_dict_backend in boosted_dict.values()\n        for def_node in boosted_dict_backend.values()\n    ]\n\n    capturex = CaptureX(\n        def_nodes,\n        module,\n        ancestors,\n        defuse_chains=duc,\n        usedef_chains=udc,\n        consider_annotations=""only"",\n    )\n\n    code_dependance_annotations = capturex.make_code_external()\n\n    namespace = {}\n    exec(code_dependance_annotations, namespace)\n\n    aheadoftime.is_transpiling = False\n\n    annotations = {}\n\n    for kind, boosted_dict in boosted_dicts.items():\n        annotations[kind] = {}\n        for boosted_dict_backend in boosted_dict.values():\n            for key, definition in boosted_dict_backend.items():\n                ann = get_annotations(definition, namespace)\n                if ann != {}:\n                    annotations[kind][key] = ann\n\n    debug(pformat(annotations))\n\n    debug(""get_block_definitions"")\n    blocks = get_block_definitions(code, module, ancestors, duc, udc)\n\n    info_analysis = {\n        ""ancestors"": ancestors,\n        ""duc"": duc,\n        ""udc"": udc,\n        ""module"": module,\n        ""def_nodes"": def_nodes,\n    }\n\n    for block in blocks:\n        block.parse_comments(namespace, info_analysis)\n\n    debug(pformat(blocks))\n\n    debug(""compute code dependance:"")\n\n    # remove the decorator (boost) to compute the code dependance\n    # + do not consider the class annotations\n    for def_node in def_nodes:\n        def_node.decorator_list = []\n        if isinstance(def_node, ast.ClassDef):\n            def_node.body = [\n                node\n                for node in def_node.body\n                if not isinstance(node, ast.AnnAssign)\n            ]\n\n    blocks_for_capturex = [block.ast_code for block in blocks]\n\n    capturex = CaptureX(\n        def_nodes,\n        module,\n        ancestors=ancestors,\n        defuse_chains=duc,\n        usedef_chains=udc,\n        consider_annotations=False,\n        blocks=blocks_for_capturex,\n    )\n    code_ext = {""function"": {}, ""class"": {}}\n    code_dependance = capturex.make_code_external()\n    # TODO implement class for new backends + debug this code :-)\n    if boosted_dicts[""functions""][""pythran""]:\n        func = next(iter(boosted_dicts[""functions""][""pythran""]))\n        code_dependance, code_ext, _, _ = get_exterior_code(\n            {func: code_dependance}, pathfile, classes=""function"", relative=True\n        )\n        code_dependance = code_dependance[func]\n    if boosted_dicts[""classes""][""pythran""]:\n        cls = next(iter(boosted_dicts[""classes""][""pythran""]))\n        code_dependance, code_ext, _, _ = get_exterior_code(\n            {cls: code_dependance}, pathfile, classes=""class"", relative=True\n        )\n        code_dependance = code_dependance[cls]\n    debug(code_dependance)\n\n    debug(""parse_transonic_def_commands"")\n    signatures_p = parse_transonic_def_commands(code)\n\n    annotations[""__in_comments__""] = {}\n    annotations[""__locals__""] = {}\n    annotations[""__returns__""] = {}\n\n    for functions_backend in boosted_dicts[""functions""].values():\n        for name_func, fdef in functions_backend.items():\n            try:\n                signatures = signatures_p[name_func]\n            except KeyError:\n                signatures = tuple()\n            arg_names = [arg.id for arg in fdef.args.args]\n            annotations_sign = []\n            for sig in signatures:\n                types = [\n                    type_.strip()\n                    for type_ in sig[len(fdef.name) + 1 : -1].split("","")\n                ]\n                annotations_sign.append(dict(zip(arg_names, types)))\n            if annotations_sign:\n                annotations[""__in_comments__""][name_func] = annotations_sign\n\n            # locals: variable annotations\n            annotations_locals = extract_variable_annotations(fdef, namespace)\n            if annotations_locals:\n                annotations[""__locals__""][name_func] = annotations_locals\n\n            if fdef.returns:\n                annotations[""__returns__""][\n                    name_func\n                ] = extract_returns_annotation(fdef.returns, namespace)\n\n    debug(""annotations:\\n"" + pformat(annotations))\n\n    return boosted_dicts, code_dependance, annotations, blocks, code_ext\n'"
transonic/analyses/beniget.py,0,"b'import gast as ast\n\nfrom beniget import Ancestors, DefUseChains as DUC, UseDefChains\n\nfrom beniget.beniget import Def\n\n\n__all__ = [""Ancestors"", ""DefUseChains"", ""UseDefChains""]\n\n\nclass DefUseChains(DUC):\n    def visit_List(self, node):\n        if isinstance(node.ctx, ast.Load):\n            dnode = self.chains.setdefault(node, Def(node))\n            for elt in node.elts:\n                if isinstance(elt, CommentLine):\n                    continue\n                self.visit(elt).add_user(dnode)\n            return dnode\n        # unfortunately, destructured node are marked as Load,\n        # only the parent List/Tuple is marked as Store\n        elif isinstance(node.ctx, ast.Store):\n            return self.visit_Destructured(node)\n\n    visit_Tuple = visit_List\n\n\n# this import has to be after the definition of DefUseChains\nfrom transonic.analyses.extast import CommentLine  # noqa: E402\n'"
transonic/analyses/blocks_if.py,0,"b'""""""Analyses for ""if blocks""\n===========================\n\n""""""\n\nimport gast as ast\n\nfrom transonic.analyses import extast\nfrom transonic.analyses.util import gather_rawcode_comments\nfrom transonic.analyses.capturex import CaptureX\n\n\nclass BlockDefinition:\n    """"""Represent a block definition""""""\n\n    def __init__(self, **kwargs):\n        self.kwargs = kwargs\n        self.__dict__.update(**kwargs)\n\n    def __repr__(self):\n        return repr(self.kwargs)\n\n    def parse_comments(self, namespace=None, info_analysis=None):\n        self.signatures = get_signatures_from_comments(\n            self.comments, namespace, info_analysis\n        )\n\n\ndef get_block_definitions(code, module, ancestors, duc, udc):\n    """"""Get all ""if"" block definitions""""""\n    blocks = []\n    node_transonic_obj = None\n    for node in module.body:\n        if isinstance(node, ast.ImportFrom) and node.module == ""transonic"":\n            for alias in node.names:\n                if alias.name == ""Transonic"":\n                    Transonic_node = alias\n                    node_transonic_obj = None\n                    for user in duc.chains[Transonic_node].users():\n                        parent = ancestors.parent(user.node)\n                        if isinstance(parent, ast.Call):\n                            grandparent = ancestors.parent(parent)\n                            if len(grandparent.targets) != 1:\n                                continue\n                            node_transonic_obj = grandparent.targets[0]\n                    if node_transonic_obj is not None:\n                        break\n\n    if node_transonic_obj is None:\n        return blocks\n\n    def_ = duc.chains[node_transonic_obj]\n    nodes_using_ts = [user.node for user in def_.users()]\n\n    for user in def_.users():\n        for user1 in user.users():\n            if isinstance(user1.node, ast.Attribute):\n                attribute = user1.node\n                if attribute.attr == ""is_transpiled"":\n                    parent = ancestors.parent(attribute)\n                    if isinstance(parent, ast.If):\n                        # it could be the begining of a block\n                        if_node = parent\n                        if len(parent.body) != 1:\n                            # no it\'s not a block definition\n                            continue\n                        node = parent.body[0]\n                        call = node.value\n                        if isinstance(node, ast.Expr):\n                            results = []\n                        elif isinstance(node, ast.Assign):\n                            results = [target.id for target in node.targets]\n                        else:\n                            # no it\'s not a block definition\n                            continue\n\n                        attribute = call.func\n                        ts_node = attribute.value\n                        if (\n                            ts_node not in nodes_using_ts\n                            or attribute.attr != ""use_block""\n                        ):\n                            # no it\'s not a block definition\n                            continue\n\n                        try:\n                            # gast >= 0.3.0 (py3.8)\n                            name_block = call.args[0].value\n                        except AttributeError:\n                            name_block = call.args[0].s\n\n                        rawcode, comments = gather_rawcode_comments(if_node, code)\n\n                        # if we are here, it\'s a block definition\n                        blocks.append(\n                            BlockDefinition(\n                                name=name_block,\n                                results=results,\n                                ast_code=if_node.orelse,\n                                comments=comments,\n                                rawcode=rawcode,\n                            )\n                        )\n\n    return blocks\n\n\ndef find_index_closing_parenthesis(string: str):\n    """"""Find the index of the closing parenthesis""""""\n    assert string.startswith(""(""), ""string has to start with \'(\'""\n    stack = []\n    for index, letter in enumerate(string):\n        if letter == ""("":\n            stack.append(letter)\n        elif letter == "")"":\n            stack.pop()\n            if not stack:\n                return index\n\n    raise SyntaxError(f""Transonic syntax error for string {string}"")\n\n\ndef find_last_def_node(variable, module):\n    for node in module.body[::-1]:\n        if isinstance(node, ast.Assign):\n            for target in node.targets:\n                if target.id == variable:\n                    return node\n    raise RuntimeError\n\n\ndef find_def_code(\n    variables: set, def_nodes: list, module: dict, ancestors, udc, duc\n):\n\n    nodes_def_vars = [\n        find_last_def_node(variable, module) for variable in variables\n    ]\n\n    capturex = CaptureX(\n        list(nodes_def_vars) + def_nodes,\n        module,\n        ancestors,\n        defuse_chains=duc,\n        usedef_chains=udc,\n    )\n\n    lines_ext = []\n    for node in capturex.external:\n        lines_ext.append(extast.unparse(node).strip())\n\n    for node in nodes_def_vars:\n        line = extast.unparse(node).strip()\n        if line not in lines_ext:\n            lines_ext.append(line)\n\n    return ""\\n"".join(lines_ext)\n\n\ndef get_signatures_from_comments(comments, namespace=None, info_analysis=None):\n    """"""Get the blocks signatures for a block""""""\n    if namespace is None:\n        namespace = {}\n\n    comments = comments.replace(""#"", """").replace(""\\n"", """")\n\n    signatures_tmp = [\n        sig.split(""->"", 1)[0].strip()\n        for sig in comments.split(""transonic block"")[1:]\n    ]\n\n    signatures = []\n    for sig in signatures_tmp:\n        if sig.startswith(""(""):\n            sig = sig[1 : find_index_closing_parenthesis(sig)]\n        signatures.append(sig)\n\n    types_NameError = set()\n    for sig_str in signatures:\n        type_vars_strs = [tmp.strip() for tmp in sig_str.split("";"")]\n        type_vars_strs = [tmp for tmp in type_vars_strs if tmp]\n\n        for type_vars_str in type_vars_strs:\n            type_, vars_str = type_vars_str.strip().split("" "", 1)\n            if type_ not in namespace:\n                try:\n                    eval(type_)\n                except NameError:\n                    types_NameError.add(type_)\n                except (SyntaxError, TypeError):\n                    pass\n\n    if types_NameError:\n        # create a namespace for the variables defined at the module level\n        code_def_var = find_def_code(\n            types_NameError,\n            info_analysis[""def_nodes""],\n            info_analysis[""module""],\n            info_analysis[""ancestors""],\n            info_analysis[""udc""],\n            info_analysis[""duc""],\n        )\n\n        namespace = {}\n        exec(code_def_var, namespace)\n\n    tmp = signatures\n    signatures = []\n    for sig_str in tmp:\n        sig_dict = {}\n        type_vars_strs = [tmp.strip() for tmp in sig_str.split("";"")]\n        type_vars_strs = [tmp for tmp in type_vars_strs if tmp]\n        for type_vars_str in type_vars_strs:\n            type_, vars_str = type_vars_str.strip().split("" "", 1)\n            if type_ in namespace:\n                type_ = namespace[type_]\n            else:\n                try:\n                    type_ = eval(type_)\n                except (SyntaxError, TypeError):\n                    pass\n            for var_str in vars_str.split("",""):\n                var_str = var_str.strip()\n                sig_dict[var_str] = type_\n        signatures.append(sig_dict)\n    return signatures\n'"
transonic/analyses/capturex.py,2,"b'""""""Capture the external nodes used in functions\n===============================================\n""""""\n\nimport gast as ast\n\nfrom transonic.analyses import beniget\nfrom transonic.analyses import extast\n\n\nclass CaptureX(ast.NodeVisitor):\n    """"""Capture the external nodes used in functions, classes and blocks""""""\n\n    def __init__(\n        self,\n        functions,\n        module_node,\n        ancestors=None,\n        defuse_chains=None,\n        usedef_chains=None,\n        consider_annotations=True,\n        blocks=None,\n    ):\n\n        if defuse_chains is None:\n            self.du_chains = du = beniget.DefUseChains()\n            du.visit(module_node)\n            self.ud_chains = beniget.UseDefChains(du)\n            self.ancestors = beniget.Ancestors()\n            self.ancestors.visit(module_node)\n        else:\n            self.du_chains = defuse_chains\n            self.ud_chains = usedef_chains\n            self.ancestors = ancestors\n\n        self.consider_annotations = consider_annotations\n        self._annot = None\n\n        self.external = []\n        self.visited_external = set()\n\n        self.functions = functions\n        for func in functions:\n            self.func = func\n            self.visit(func)\n\n        if blocks is None:\n            return\n\n        for nodes in blocks:\n            if nodes:\n                node = nodes[0]\n                self.func = ancestors.parentFunction(node)\n                for node in nodes:\n                    self.visit(node)\n\n    def visit_Name(self, node):\n\n        parent_node = self.ancestors.parents(node)[-1]\n        if (\n            isinstance(parent_node, ast.FunctionDef)\n            and node == parent_node.returns\n            and not self.consider_annotations\n        ):\n            return\n\n        # register load of identifiers not locally defined\n        if isinstance(node.ctx, ast.Load):\n            try:\n                self.ud_chains.chains[node]\n            except KeyError:\n                if not (\n                    isinstance(parent_node, ast.FunctionDef)\n                    and node == parent_node.returns\n                ):\n                    raise\n                from warnings import warn\n\n                warn(f""BUG Beniget (node.id={node.id}), but we try to continue!"")\n                return\n\n            for def_ in self.ud_chains.chains[node]:\n                try:\n                    parents = self.ancestors.parents(def_.node)\n                except KeyError:\n                    return  # a builtin\n                if self.func not in parents:\n                    if isinstance(def_.node, ast.FunctionDef):\n                        defining_node = def_.node\n                    else:\n                        defining_node = self.ancestors.parentStmt(def_.node)\n                    if defining_node not in self.visited_external:\n                        self.rec(defining_node)\n                        if defining_node in self.functions:\n                            return\n\n                        if (\n                            self.consider_annotations == ""only""\n                            and self._annot is None\n                        ):\n                            return\n\n                        if defining_node not in self.visited_external:\n                            self.visited_external.add(defining_node)\n                            self.external.append(defining_node)\n        elif (\n            isinstance(node.ctx, (ast.Param, ast.Store))\n            and self.consider_annotations\n        ):\n            if node.annotation is None:\n                return\n            self._annot = node.annotation\n            self.visit(node.annotation)\n            self._annot = None\n\n    def visit_ClassDef(self, node_class):\n        for node in node_class.body:\n            if isinstance(node, ast.AnnAssign):\n                self._annot = node.annotation\n                self.visit(node)\n                self._annot = None\n\n    def visit_AnnAssign(self, node):\n        if self.consider_annotations:\n            self._annot = node.annotation\n            self.visit(node.annotation)\n            self._annot = None\n\n        if node.value is not None and self.consider_annotations != ""only"":\n            self.visit(node.value)\n\n    def rec(self, node):\n        ""walk definitions to find their operands\'s def""\n        if node is self.func:\n            return\n\n        if isinstance(node, ast.Assign):\n            self.visit(node.value)\n        elif isinstance(node, ast.FunctionDef):\n            # tmp: to deal with @include\n            node.decorator_list = []\n\n            old_func = self.func\n            self.func = node\n            self.visit(node)\n            self.func = old_func\n\n        # TODO: implement this for AugAssign etc\n\n    def make_code_external(self):\n        code = []\n        for node in self.external:\n            code.append(extast.unparse(node).strip())\n        return ""\\n"".join(code)\n\n\nif __name__ == ""__main__"":\n\n    code = ""a = 1; b = [a, a]\\ndef foo():\\n return b""\n    # code = ""a = 1; b = len([a, a])\\ndef foo():\\n return b""\n    # code = ""import numpy as np\\na = np.int(1)\\ndef foo():\\n return np.zeros(a)""\n\n    code = """"""\n\na = 1\n\ndef fooo():\n    return 1\n\ndef foo():\n    return a + fooo()\n\ndef bar():\n    return foo()\n\n    """"""\n\n    module = extast.parse(code)\n    function = module.body[3]\n    capturex = CaptureX((function,), module)\n\n    print(capturex.make_code_external())\n'"
transonic/analyses/extast.py,1,"b'""""""Extended AST with CommentLine nodes\n======================================\n\n""""""\n\nfrom io import StringIO\nfrom copy import deepcopy\n\nimport gast as ast\nimport astunparse\n\nfrom transonic.analyses import beniget\n\n\nclass CommentLine(ast.AST):\n    """"""""New AST node representing a comment line""""""\n\n    _fields = (""s"",)\n\n    def __init__(self, s, lineno=None):\n        super().__init__()\n        self.s = s\n        if lineno is not None:\n            self.lineno = lineno\n\n    def __deepcopy__(self, memo):\n        try:\n            lineno = self.lineno\n        except AttributeError:\n            lineno = None\n        return self.__class__(deepcopy(self.s), lineno)\n\n\nclass UnparserExtended(astunparse.Unparser):\n    """"""Unparser for extented AST""""""\n\n    def __init__(self, tree, file, with_comments=True):\n        self.with_comments = with_comments\n        super().__init__(tree, file=file)\n\n    boolops = {ast.And: ""and"", ast.Or: ""or""}\n\n    def _CommentLine(self, node):\n        if self.with_comments:\n            self.write(f""\\n{\'    \'* self._indent}{node.s}"")\n\n    def _Name(self, t):\n        self.write(t.id)\n        if t.annotation is not None:\n            self.write("": "")\n            self.dispatch(t.annotation)\n\n\ndef parse(code, *args, **kwargs):\n    """"""Parse a code and produce the extended AST""""""\n    tree = ast.parse(code, *args, **kwargs)\n    CommentInserter(tree, code)\n    return tree\n\n\ndef unparse(tree, with_comments=True):\n    """"""Unparse the extended AST""""""\n    v = StringIO()\n    UnparserExtended(tree, file=v, with_comments=with_comments)\n    return v.getvalue()\n\n\nclass CommentInserter(ast.NodeVisitor):\n    """"""Insert the CommentLine nodes in an AST tree""""""\n\n    def __init__(self, tree, code):\n        self.tree = tree\n\n        self.ancestors = beniget.Ancestors()\n        self.ancestors.visit(tree)\n\n        self.code = code\n        self.lines = code.split(""\\n"")\n\n        self.lines_comments = []\n        self.lineno_comments = []\n        for index, line in enumerate(self.lines):\n            if line.strip().startswith(""#""):\n                self.lineno_comments.append(index + 1)\n                self.lines_comments.append(line)\n\n        if not self.lineno_comments:\n            # nothing to do\n            return\n\n        self.last_line_comment = max(self.lineno_comments)\n\n        self.index_comment = 0\n\n        self.node_after = {}\n        self.node_before = []\n\n        self.previous_node = None\n\n        self._done = False\n\n        self.visit(tree)\n\n        self.node_before = {\n            self.lineno_comments[index]: node_before\n            for index, node_before in enumerate(self.node_before)\n        }\n\n        self.modify_tree()\n\n    def visit(self, node):\n\n        if self._done:\n            return\n\n        if hasattr(node, ""lineno""):\n\n            linenos = self.lineno_comments\n\n            while node.lineno > linenos[self.index_comment]:\n                self.node_after[linenos[self.index_comment]] = node\n                if linenos[self.index_comment] == self.last_line_comment:\n                    self._done = True\n                    return\n                self.index_comment += 1\n\n            while len(self.node_after) > len(self.node_before):\n                self.node_before.append(self.previous_node)\n\n        self.previous_node = node\n\n        super().visit(node)\n\n    def modify_tree(self):\n\n        # todo: debug this buggy code!\n\n        for lineno, line in zip(self.lineno_comments, self.lines_comments):\n            try:\n                node_after = self.node_after[lineno]\n            except KeyError:\n                self.tree.body.append(CommentLine(line, lineno))\n                continue\n            else:\n                self.insert_comment_in_parent_before_node(\n                    line, lineno, node_after\n                )\n                continue\n\n            try:\n                node_before = self.node_before[lineno]\n            except KeyError:\n                pass\n            else:\n                if isinstance(node_before, ast.Module):\n                    self.tree.body.insert(0, CommentLine(line, lineno))\n\n    def insert_comment_in_parent_before_node(self, line, lineno, node):\n\n        parent = self.ancestors.parent(node)\n        comment = line.strip()\n\n        for field, value in ast.iter_fields(parent):\n            if isinstance(value, list):\n                if node in value:\n                    index = value.index(node)\n                    value.insert(index, CommentLine(comment, lineno))\n\n\ntry:\n    ast.Constant\nexcept AttributeError:\n    # gast < 0.3.0\n    class Constant(ast.Str):\n        def __init__(self, value):\n            super().__init__(value)\n            self.value = value\n\n    class Name(ast.Name):\n        def __init__(self, id=""annotations"", ctx=ast.Store(), annotation=None):\n            super().__init__(id, ctx, annotation)\n\n    class FunctionDef(ast.FunctionDef):\n        def __init__(\n            self,\n            name,\n            args,\n            body,\n            decorator_list=None,\n            returns=None,\n            type_comment=None,\n        ):\n            if decorator_list is None:\n                decorator_list = []\n            super().__init__(name, args, body, decorator_list, returns)\n\n\nelse:\n    # gast >= 0.3.0\n    class Constant(ast.Constant):\n        def __init__(self, value):\n            super().__init__(value, None)\n\n    class Name(ast.Name):\n        def __init__(self, id=""annotations"", ctx=ast.Store(), annotation=None):\n            super().__init__(id, ctx, annotation, None)\n\n    class FunctionDef(ast.FunctionDef):\n        def __init__(\n            self,\n            name,\n            args,\n            body,\n            decorator_list=None,\n            returns=None,\n            type_comment=None,\n        ):\n            if decorator_list is None:\n                decorator_list = []\n            super().__init__(\n                name, args, body, decorator_list, returns, type_comment\n            )\n\n\nif __name__ == ""__main__"":\n    code = """"""\n# comment 0\n# comment 1\nif True:\n# comment 1bis\n    # comment 2\n    # comment 3\n    var_if = 1\n    # comment after var_if\n    if False:\n        var_ifif = 1\n        # comment ifif\n    # comment if end\n# comment 4\nelse:\n    # comment else\n    var_else = 1\n# comment after else\na += 1\n# comment after last\n    """"""\n\n    code = """"""\nfrom transonic import Transonic\nimport numpy as np\nimport foo\n\nts = Transonic()\na = n = 1\nb = np.ones(2)\n\nif ts.is_transpiled:\n    result = ts.use_block(""block0"")\nelse:\n    # transonic block (\n    #     A a; A1 b;\n    #     float n\n    # )\n\n    # transonic block (\n    #     int[:] a, b;\n    #     float n\n    # )\n\n    result = a ** 2 + b.mean() ** 3 + foo.bar(n)\n\n    """"""\n\n    tree = parse(code)\n\n    print(unparse(tree, with_comments=True))\n\n    from transonic.analyses import compute_ancestors_chains\n\n    compute_ancestors_chains(tree)\n'"
transonic/analyses/justintime.py,0,"b'""""""Analyses for ``@jit``\n========================\n\n""""""\n\nfrom transonic.analyses import extast\nfrom transonic.analyses import compute_ancestors_chains, get_decorated_dicts\nfrom transonic.analyses.capturex import CaptureX\n\nfrom transonic.log import logger\nfrom transonic.analyses.util import get_exterior_code\n\n\ndef analysis_jit(code, pathfile, backend_name):\n    """"""Gather the informations for ``@jit`` with an ast analysis""""""\n    debug = logger.debug\n\n    debug(""extast.parse"")\n    module = extast.parse(code)\n\n    debug(""compute ancestors and chains"")\n    ancestors, duc, udc = compute_ancestors_chains(module)\n\n    jitted_dicts = get_decorated_dicts(\n        module, ancestors, duc, pathfile, backend_name, decorator=""jit""\n    )\n\n    jitted_dicts = dict(\n        functions=jitted_dicts[""functions""][backend_name],\n        functions_ext=jitted_dicts[""functions_ext""][backend_name],\n        methods=jitted_dicts[""methods""][backend_name],\n        classes=jitted_dicts[""classes""][backend_name],\n    )\n\n    debug(""compute code dependance"")\n\n    def_nodes_dict = {\n        key: def_node\n        for kind in [""functions""]\n        for key, def_node in jitted_dicts[kind].items()\n    }\n\n    codes_dependance = {}\n\n    # get code dependance of a decorated function from a imported module\n    for func_name, (func_node, imported_module) in jitted_dicts[\n        ""functions_ext""\n    ].items():\n        capturex = CaptureX(\n            (func_node,), imported_module, consider_annotations=False\n        )\n        codes_dependance[func_name] = capturex.make_code_external()\n\n    # remove the decorator (jit) to compute the code dependance\n    for key, def_node in def_nodes_dict.items():\n        def_node.decorator_list = []\n\n        capturex = CaptureX(\n            (def_node,),\n            module,\n            ancestors=ancestors,\n            defuse_chains=duc,\n            usedef_chains=udc,\n            consider_annotations=False,\n        )\n\n        codes_dependance[key] = capturex.make_code_external()\n\n    debug(codes_dependance)\n\n    def_nodes_dict = {}\n\n    for (class_name, method_name), def_node in jitted_dicts[""methods""].items():\n        if class_name not in def_nodes_dict:\n            def_nodes_dict[class_name] = []\n\n        def_nodes_dict[class_name].append(def_node)\n\n    codes_dependance_classes = {}\n\n    for key, def_nodes in def_nodes_dict.items():\n\n        for def_node in def_nodes:\n            def_node.decorator_list = []\n\n        capturex = CaptureX(\n            def_nodes,\n            module,\n            ancestors=ancestors,\n            defuse_chains=duc,\n            usedef_chains=udc,\n            consider_annotations=False,\n        )\n\n        codes_dependance_classes[key] = capturex.make_code_external()\n\n    special = []\n    spe = []\n    if jitted_dicts[""methods""]:\n        codes_dependance_classes, code_ext, jitted_dicts, spe = get_exterior_code(\n            codes_dependance_classes,\n            pathfile,\n            previous_file_name=None,\n            classes=""class"",\n            relative=False,\n            jitted_dicts=jitted_dicts,\n        )\n    special = special + spe\n    codes_dependance, code_ext, jitted_dicts, spe = get_exterior_code(\n        codes_dependance,\n        pathfile,\n        previous_file_name=None,\n        classes=""function"",\n        relative=False,\n        jitted_dicts=jitted_dicts,\n    )\n    special = special + spe\n\n    return (\n        jitted_dicts,\n        codes_dependance,\n        codes_dependance_classes,\n        code_ext,\n        special,\n    )\n'"
transonic/analyses/parser.py,0,"b'""""""Parser using tokenize\n========================\n\n""""""\n\nfrom tokenize import tokenize, COMMENT\nfrom io import BytesIO\n\n\ndef parse_transonic_def_commands(code: str):\n    """"""Parse the code of a .py file and return data""""""\n\n    functions = set()\n    signatures_func = {}\n\n    in_def = False\n\n    g = tokenize(BytesIO(code.encode(""utf-8"")).readline)\n    for toknum, tokval, a, b, c in g:\n        if toknum == COMMENT and tokval.startswith(""# transonic def ""):\n            in_def = True\n            signature_func = tokval.split(""# transonic def "", 1)[1]\n            name_func = signature_func.split(""("")[0]\n            functions.add(name_func)\n\n            if name_func not in signatures_func:\n                signatures_func[name_func] = []\n\n            if "")"" in tokval:\n                in_def = False\n                signatures_func[name_func].append(signature_func)\n\n        if in_def:\n            if toknum == COMMENT:\n                if ""# transonic def "" in tokval:\n                    tokval = tokval.split(""("", 1)[1]\n                signature_func += tokval.replace(""#"", """").strip()\n                if "")"" in tokval:\n                    in_def = False\n                    signatures_func[name_func].append(signature_func)\n\n    return signatures_func\n'"
transonic/analyses/test_analyses.py,0,"b'from transonic.analyses import get_types_from_transonic_signature\n\n\ndef assert_types_from_sig(signature, func_name, types):\n    assert get_types_from_transonic_signature(signature, func_name) == types\n\n\ndef test_simple():\n    assert_types_from_sig(\n        ""my_func(int, float64, )"", ""my_func"", [""int"", ""float64""]\n    )\n\n\ndef test_cython_style():\n    assert_types_from_sig(\n        ""foo(int, int, int64[:, :], float64[:, :], float64[:, :], int64[:], int64[:, :, :], float64, float64, int, float64 )"",\n        ""foo"",\n        [\n            ""int"",\n            ""int"",\n            ""int64[:, :]"",\n            ""float64[:, :]"",\n            ""float64[:, :]"",\n            ""int64[:]"",\n            ""int64[:, :, :]"",\n            ""float64"",\n            ""float64"",\n            ""int"",\n            ""float64"",\n        ],\n    )\n\n\ndef test_c_style():\n    assert_types_from_sig(\n        ""bar(int, int[] [] , int64[:, :], float64 )"",\n        ""bar"",\n        [""int"", ""int[] []"", ""int64[:, :]"", ""float64""],\n    )\n'"
transonic/analyses/test_util.py,0,"b'from transonic.analyses.extast import parse\nfrom transonic.analyses.util import print_dumped, print_unparsed\n\nsource = ""def func(): return True""\n\n\ndef test_print_dumped():\n    print_dumped(source)\n\n\ndef test_print_unparsed():\n    node = parse(source)\n    print_unparsed(node)\n'"
transonic/analyses/util.py,0,"b'""""""Utilities for the analyses\n=============================\n\n""""""\nimport re\nfrom pathlib import Path\nfrom textwrap import dedent\n\nimport gast as ast\nimport astunparse\n\nfrom transonic.analyses import beniget\nfrom transonic.analyses import extast\n\n\ndef print_dumped(source):\n    """"""Pretty print the AST""""""\n    if isinstance(source, str):\n        module = extast.parse(source)\n        if len(module.body) == 1:\n            node = module.body[0]\n        else:\n            node = module\n    else:\n        node = source\n    print(astunparse.dump(node))\n\n\ndef print_unparsed(node):\n    """"""Print the code corresponding to a tree or a node""""""\n    print(extast.unparse(node))\n\n\nfrom transonic.analyses.capturex import CaptureX\n\n\ndef _fill_ast_annotations_function(function_def, ast_annotations):\n\n    dict_node = ast_annotations.value\n    for arg in function_def.args.args:\n        if arg.annotation is not None:\n            try:\n                name = arg.arg\n            except AttributeError:\n                name = arg.id\n\n            dict_node.keys.append(extast.Constant(value=name))\n            dict_node.values.append(arg.annotation)\n\n\ndef _fill_ast_annotations_class(class_def, ast_annotations):\n\n    dict_node = ast_annotations.value\n    for node in class_def.body:\n        if not isinstance(node, ast.AnnAssign):\n            continue\n\n        if node.annotation is not None:\n            name = node.target.id\n            dict_node.keys.append(extast.Constant(value=name))\n            dict_node.values.append(node.annotation)\n\n\ndef get_annotations(object_def, namespace):\n    """"""Create the annotations from a definition node""""""\n\n    # print_dump(object_def)\n\n    ast_annotations = ast.Assign(\n        targets=[extast.Name(""annotations"", ast.Store())],\n        value=ast.Dict(keys=[], values=[]),\n    )\n\n    if isinstance(object_def, ast.FunctionDef):\n        _fill_ast_annotations_function(object_def, ast_annotations)\n    elif isinstance(object_def, ast.ClassDef):\n        _fill_ast_annotations_class(object_def, ast_annotations)\n    else:\n        raise NotImplementedError\n\n    # print_dump(ast_annotations)\n\n    source = extast.unparse(ast_annotations)\n\n    try:\n        del namespace[""__builtins__""]\n    except KeyError:\n        pass\n    exec(source, namespace)\n    return namespace[""annotations""]\n\n\ndef filter_code_typevars(module, duc, ancestors):\n    """"""Create a filtered code with what is needed to create the annotations""""""\n\n    module_filtered = ast.Module()\n    kept = module_filtered.body = []\n    suppressed = set()\n\n    def fill_suppressed(def_):\n        for user in def_.users():\n            parent_in_body = ancestors.parents(user.node)[1]\n            suppressed.add(parent_in_body)\n            fill_suppressed(user)\n\n    for node in module.body:\n        if node in suppressed:\n            continue\n\n        if isinstance(node, ast.Import):\n            if node.names[0].name in [""transonic"", ""numpy""]:\n                kept.append(node)\n            else:\n                def_ = duc.chains[node.names[0]]\n                fill_suppressed(def_)\n            #     suppressed.add()\n        elif isinstance(node, ast.ImportFrom):\n            if node.module in [""transonic"", ""numpy""]:\n                kept.append(node)\n\n        elif isinstance(node, (ast.Assign, ast.AugAssign)):\n            kept.append(node)\n\n    return extast.unparse(module_filtered)\n\n\nclass AnalyseLines(ast.NodeVisitor):\n    """"""Analyse to find the last line of a node""""""\n\n    def __init__(self, main_node):\n        if isinstance(main_node, ast.Module):\n            self.line_start = 1\n        else:\n            self.line_start = main_node.lineno\n        self.line_last = self.line_start\n        self.visit(main_node)\n\n    def generic_visit(self, node):\n        if hasattr(node, ""lineno""):\n            if node.lineno > self.line_last:\n                self.line_last = node.lineno\n        super().generic_visit(node)\n\n    def get_lines(self):\n        return self.line_start, self.line_last\n\n    def get_code(self, source):\n        lines = source.split(""\\n"")\n\n        stop = self.line_last\n        nb_lines = len(lines)\n        if nb_lines != stop:\n            # find next not empty and not comment line\n            ind = stop - 1\n            next_line = """"\n            while ind + 1 < nb_lines and next_line == """":\n                ind += 1\n                next_line = lines[ind].strip()\n                if next_line.startswith(""#""):\n                    next_line = """"\n\n            if any(\n                next_line.startswith(character) for character in ("")"", ""]"", ""}"")\n            ):\n                stop = ind + 1\n\n        return ""\\n"".join(lines[self.line_start - 1 : stop])\n\n\ndef gather_rawcode_comments(node, code_module):\n    """"""Get the comments in a node""""""\n    analysis = AnalyseLines(node)\n    rawcode = dedent(analysis.get_code(code_module))\n    comments = dedent(\n        ""\\n"".join(\n            line for line in rawcode.split(""\\n"") if line.strip().startswith(""#"")\n        )\n    )\n    return rawcode, comments\n\n\n# TODO complete the list\npackages_supported_by_pythran = [""numpy"", ""math"", ""functools"", ""cmath"", ""scipy""]\n\n\ndef find_path(node: object, pathfile: str):\n    """""" Return the path of node (instance of ast.Import or ast.ImportFrom)\n    """"""\n    # FIXME find path in non local imports\n    name = str()\n    path = str()\n\n    if isinstance(node, ast.ImportFrom):\n        name = node.module\n        if name in packages_supported_by_pythran:\n            return None, None\n        else:\n            parent = Path(pathfile).parent\n            path = parent / (str(name.replace(""."", ""/"")) + "".py"")\n    else:\n        if node.names[0].name in packages_supported_by_pythran:\n            pass\n        else:\n            # TODO deal with an ast.Import\n            raise NotImplementedError\n    return name, path\n\n\ndef change_import_name(\n    code_dep: str, changed_node: object, func_name: str, relative: str = None\n):\n    """"""Change the name of changed_node in code_dep by adding ""__"" + func + ""__""\n    at the beginning of the imported module, and return the modified code\n    """"""\n    mod = extast.parse(code_dep)\n    for node in mod.body:\n        if extast.unparse(node) == extast.unparse(changed_node):\n            if isinstance(node, ast.ImportFrom):\n                node.module = f""__ext__{func_name}__{node.module}""\n            elif isinstance(node, ast.Import):\n                node.names[0].name = f""__ext__{func_name}__{node.names[0].name}""\n        if not relative:\n            node.level = 0\n    return extast.unparse(mod)\n\n\ndef filter_external_code(module: object, names: list):\n    """""" Filter the module to keep only the necessary nodes\n        needed by functions or class in the parameter names\n    """"""\n    code_dependance_annotations = """"\n    lines_code = []\n    for node in module.body:\n        for name in names:\n            if isinstance(node, ast.FunctionDef):\n                if node.name == extast.unparse(name).rstrip(""\\n\\r""):\n                    ancestors = beniget.Ancestors()\n                    ancestors.visit(module)\n                    duc = beniget.DefUseChains()\n                    duc.visit(module)\n                    udc = beniget.UseDefChains(duc)\n                    capturex = CaptureX(\n                        [node],\n                        module,\n                        ancestors,\n                        defuse_chains=duc,\n                        usedef_chains=udc,\n                        consider_annotations=None,\n                    )\n                    lines_code.append(str(extast.unparse(node)))\n                    code_dependance_annotations = capturex.make_code_external()\n            if isinstance(node, ast.Assign):\n                if node.targets[0].id == extast.unparse(name).rstrip(""\\n\\r""):\n                    lines_code.append(str(extast.unparse(node)))\n            if isinstance(node, ast.ClassDef):\n                if node.name == extast.unparse(name).rstrip(""\\n\\r""):\n                    lines_code.append(str(extast.unparse(node)))\n\n    return code_dependance_annotations + ""\\n"".join(lines_code)\n\n\ndef adapt_code_dependance(func: str, codes_dependance: str, jitted_dicts: dict):\n    """"""\n    Adapt code_dependance to the call of a jitted function in a jitted function:\n        - Remove the import transonic\n        - Remove the jitted function statement (i.e func = jit(func))\n        - Add a import statement to the jitted function\n        - remove the definition of the jitted function if its on the file, or remove the import statement\n    """"""\n    special = []\n    module = extast.parse(codes_dependance)\n    module_body = module.body.copy()\n    jitted_functions = []\n    for node in module_body:\n        # remove the transonic import\n        if isinstance(node, ast.ImportFrom):\n            if node.module == ""transonic"":\n                module.body.remove(node)\n        # remove the jitted function by jit() (i.e. func = jit(func))\n        # and add the import statement for the jitted function\n        elif (\n            isinstance(node, ast.Assign)\n            and isinstance(node.value, ast.Call)\n            and node.value.func.id == ""jit""\n        ):\n            # if assigned name different from jitted function name\n            if node.targets[0].id != node.value.args[0].id:\n                # change function names in jitted dict\n                # The list ""special"" contains functions that has to be write from jitted_dicts\n                # see transonic.justintime:287\n                def_func = extast.unparse(jitted_dicts[""functions""][func])\n                spl = re.split(r""(\\W+)"", def_func)\n                spl = [\n                    node.value.args[0].id if x == node.targets[0].id else x\n                    for x in spl\n                ]\n                st = """".join(str(e) for e in spl)\n                jitted_dicts[""functions""][func] = extast.parse(st)\n                special.append(func)\n                jitted_functions.append(node.value.args[0].id)\n            else:\n                jitted_functions.append(node.targets[0].id)\n            module.body.remove(node)\n            module.body.insert(\n                0,\n                [\n                    extast.parse(\n                        ""from ""\n                        + node.value.args[0].id\n                        + "" import ""\n                        + node.value.args[0].id\n                    )\n                ],\n            )\n    # remove the definition:\n    for node in module_body:\n        # of the jitted function in the file\n        if isinstance(node, ast.FunctionDef):\n            if node.name in jitted_functions:\n                module.body.remove(node)\n        # of the jitted imported function\n        if isinstance(node, ast.ImportFrom):\n            for name in node.names:\n                if name.name in jitted_functions:\n                    node.names.remove(name)\n            # remove the importFrom if no function is imported\n            if not node.names:\n                module.body.remove(node)\n    return extast.unparse(module), jitted_dicts, special, jitted_functions\n\n\ncode_ext = {""function"": {}, ""class"": {}}\n\n\ndef get_exterior_code(\n    codes_dependance: dict,\n    pathfile: str,\n    previous_file_name=None,\n    classes: str = None,\n    relative: bool = None,\n    jitted_dicts: dict = None,\n):\n    """""" Get all imported functions needed by boosted functions and methods at multiple levels,\n        (i.e get functions needed by functions imported by boosted function) and add them into code_ext\n    """"""\n    special = []\n    treated = []\n    for func, dep in codes_dependance.items():\n        if not dep:\n            continue\n        module_ext = extast.parse(dep)\n        for node in module_ext.body:\n            if not isinstance(node, (ast.ImportFrom, ast.Import)):\n                continue\n            # get the path of the imported module\n            file_name, file_path = find_path(node, pathfile)\n            # a jitted function or method needs another jitted function\n            if file_name == ""transonic"":\n                (\n                    codes_dependance[func],\n                    jitted_dicts,\n                    spe,\n                    treat,\n                ) = adapt_code_dependance(\n                    func, codes_dependance[func], jitted_dicts\n                )\n                # the ""special"" list signals that jitted functions has to be written\n                # from jitted_dicts (see transonic/justintime.py:287)\n                special = special + spe\n                treated = treated + treat\n\n    for func, dep in codes_dependance.items():\n        if not dep:\n            continue\n        module_ext = extast.parse(dep)\n        for node in module_ext.body:\n            if not isinstance(node, (ast.ImportFrom, ast.Import)):\n                continue\n            # get the path of the imported module\n            file_name, file_path = find_path(node, pathfile)\n            # a jitted function or method needs another jitted function\n            if not (file_name and file_name not in treated):\n                continue\n            new_file_name = f""__ext__{func}__{file_name}""\n            # get the content of the file\n            try:\n                with open(str(file_path), ""r"") as file:\n                    content = file.read()\n            except:\n                raise NotImplementedError(file_name + "" can no be found"")\n            mod = extast.parse(content)\n            # filter the code and add it to code_ext dict\n            code_ext[classes][new_file_name] = str(\n                filter_external_code(mod, node.names)\n            )\n            # change imported module names\n            codes_dependance[func] = change_import_name(\n                codes_dependance[func], node, func, relative\n            )\n            # recursively get the exterior codes\n            if code_ext[classes][new_file_name]:\n                get_exterior_code(\n                    {func: code_ext[classes][new_file_name]},\n                    pathfile,\n                    new_file_name,\n                    classes,\n                )\n                if previous_file_name:\n                    code_ext[classes][previous_file_name] = change_import_name(\n                        code_ext[classes][previous_file_name],\n                        node,\n                        func,\n                        relative,\n                    )\n    return codes_dependance, code_ext, jitted_dicts, special\n\n\ndef extract_variable_annotations(fdef, namespace):\n    variable_types = {}\n\n    for node in fdef.body:\n        if isinstance(node, ast.AnnAssign):\n            name = node.target.id\n            source = extast.unparse(node.annotation)\n            result = eval(source, namespace)\n            variable_types[name] = result\n\n    return variable_types\n\n\ndef extract_returns_annotation(returns, namespace):\n    if returns is None:\n        return\n    source = extast.unparse(returns)\n    return eval(source, namespace)\n'"
transonic/backends/__init__.py,0,"b'""""""Backends for different accelerators\n======================================\n\n.. autosummary::\n   :toctree:\n\n   base\n   base_jit\n   for_classes\n\n.. autosummary::\n   :toctree:\n\n   pythran\n   cython\n   numba\n   py\n\nUser API\n--------\n\n.. autofunction:: set_backend_for_this_module\n\n.. autofunction:: make_backend_files\n\n""""""\n\nfrom pathlib import Path\nfrom typing import Iterable\nimport inspect\n\nfrom transonic.config import backend_default\nfrom transonic.util import get_module_name\n\nfrom .py import PythonBackend\nfrom .pythran import PythranBackend\nfrom .cython import CythonBackend\nfrom .numba import NumbaBackend\n\nbackends = dict(\n    pythran=PythranBackend(),\n    cython=CythonBackend(),\n    numba=NumbaBackend(),\n    python=PythonBackend(),\n)\n\nbackend_default_modules = {}\n\n\ndef set_backend_for_this_module(backend_name):\n    """"""Programmatically set a backend for a module""""""\n    backend_name = backend_name.lower()\n    frame = inspect.stack()[1]\n    module_name = get_module_name(frame)\n    if backend_name not in backends.keys():\n        raise ValueError(f""Bad backend value ({backend_name})"")\n    backend_default_modules[module_name] = backend_name\n\n\ndef get_backend_name_module(module_name):\n    return backend_default_modules.get(module_name, backend_default)\n\n\ndef make_backend_files(\n    paths: Iterable[Path],\n    force=False,\n    log_level=None,\n    backend: str = backend_default,\n):\n    """"""Create Pythran files from a list of Python files""""""\n    backend = backends[backend]\n    backend.make_backend_files(paths, force, log_level)\n'"
transonic/backends/base.py,0,"b'""""""Base class for the Transonic backends\n========================================\n\nInternal API\n------------\n\n.. autoclass:: Backend\n   :members:\n   :private-members:\n\n""""""\n\nfrom pathlib import Path\nfrom textwrap import indent\nfrom typing import Iterable, Optional\n\n# from pprint import pprint\n\nimport transonic\n\nfrom transonic.analyses import extast, analyse_aot\nfrom transonic.log import logger\nfrom transonic.compiler import compile_extension, ext_suffix\nfrom transonic import mpi\nfrom transonic.mpi import PathSeq\nfrom transonic.signatures import compute_signatures_from_typeobjects\n\nfrom transonic.util import (\n    has_to_build,\n    format_str,\n    write_if_has_to_write,\n    TypeHintRemover,\n    make_hex,\n)\n\nfrom .base_jit import SubBackendJIT\nfrom .for_classes import make_new_code_method_from_nodes\nfrom .typing import TypeFormatter\n\n\nclass Backend:\n    """"""Base class for the Transonic backends""""""\n\n    backend_name = ""base""\n    suffix_backend = "".py""\n    suffix_header = None\n    suffix_extension = ext_suffix\n    keyword_export = ""export""\n    _SubBackendJIT = SubBackendJIT\n    needs_compilation = True\n    _TypeFormatter = TypeFormatter\n\n    def __init__(self):\n        self.name = self.backend_name\n        self.name_capitalized = self.name.capitalize()\n        self.type_formatter = self._TypeFormatter(self.name)\n        self.jit = self._SubBackendJIT(self.name, self.type_formatter)\n\n    def _make_code_from_fdef_node(self, fdef):\n        transformed = TypeHintRemover().visit(fdef)\n        # convert the AST back to source code\n        code = extast.unparse(transformed)\n        return format_str(code)\n\n    def make_backend_files(self, paths_py, force=False, log_level=None):\n        """"""Create backend files from a list of Python files""""""\n\n        if log_level is not None:\n            logger.set_level(log_level)\n\n        paths_out = []\n        for path in paths_py:\n            with open(path) as file:\n                code = file.read()\n            analyse = analyse_aot(code, path, self.name)\n            path_out = self.make_backend_file(path, analyse, force=force)\n            if path_out:\n                paths_out.append(path_out)\n\n        if paths_out:\n            nb_files = len(paths_out)\n            if nb_files == 1:\n                conjug = ""s""\n            else:\n                conjug = """"\n\n            if self.needs_compilation:\n                logger.warning(\n                    f""{nb_files} files created or updated need{conjug}""\n                    f"" to be {self.name}ized""\n                )\n\n        return paths_out\n\n    def make_backend_file(\n        self, path_py: Path, analyse=None, force=False, log_level=None\n    ):\n        """"""Create a Python file from a Python file (if necessary)""""""\n\n        if log_level is not None:\n            logger.set_level(log_level)\n\n        path_py = Path(path_py)\n\n        if not path_py.exists():\n            raise FileNotFoundError(f""Input file {path_py} not found"")\n\n        if path_py.absolute().parent.name == f""__{self.name}__"":\n            logger.debug(f""skip file {path_py}"")\n            return None, None, None\n        if not path_py.name.endswith("".py""):\n            raise ValueError(\n                ""transonic only processes Python file. Cannot process {path_py}""\n            )\n\n        path_dir = path_py.parent / str(f""__{self.name}__"")\n        path_backend = (path_dir / path_py.name).with_suffix(self.suffix_backend)\n\n        if not has_to_build(path_backend, path_py) and not force:\n            logger.warning(f""File {path_backend} already up-to-date."")\n            return None, None, None\n\n        if path_dir is None:\n            return\n\n        if not analyse:\n            with open(path_py) as file:\n                code = file.read()\n            analyse = analyse_aot(code, path_py, self.name)\n\n        code_backend, codes_ext, code_header = self._make_backend_code(\n            path_py, analyse\n        )\n        if not code_backend:\n            return\n        logger.debug(f""code_{self.name}:\\n{code_backend}"")\n\n        for file_name, code in codes_ext[""function""].items():\n            path_ext_file = path_dir / (file_name + "".py"")\n            write_if_has_to_write(\n                path_ext_file, format_str(code), logger.info, force\n            )\n\n        for file_name, code in codes_ext[""class""].items():\n            path_ext_file = (\n                path_dir.parent / f""__{self.name}__"" / (file_name + "".py"")\n            )\n            write_if_has_to_write(\n                path_ext_file, format_str(code), logger.info, force\n            )\n\n        written = write_if_has_to_write(\n            path_backend, code_backend, logger.info, force\n        )\n\n        if not written:\n            logger.warning(f""Code in file {path_backend} already up-to-date."")\n            return\n\n        if self.suffix_header:\n            path_header = (path_dir / path_py.name).with_suffix(\n                self.suffix_header\n            )\n            write_if_has_to_write(path_header, code_header, logger.info, force)\n\n        logger.info(f""File {path_backend} updated"")\n\n        return path_backend\n\n    def _make_first_lines_header(self):\n        return []\n\n    def _make_beginning_code(self):\n        return """"\n\n    def _make_backend_code(self, path_py, analyse):\n        """"""Create a backend code from a Python file""""""\n\n        boosted_dicts, code_dependance, annotations, blocks, codes_ext = analyse\n\n        boosted_dicts = {\n            key: value[self.name] for key, value in boosted_dicts.items()\n        }\n\n        lines_code = [""\\n"" + code_dependance + ""\\n""]\n        lines_header = self._make_first_lines_header()\n        # Deal with functions\n        for fdef in boosted_dicts[""functions""].values():\n            signatures_func = self._make_header_1_function(fdef, annotations)\n            if signatures_func:\n                lines_header.extend(signatures_func)\n            code_function = self._make_code_from_fdef_node(fdef)\n            lines_code.append(code_function)\n\n        # Deal with methods\n        signatures, code_for_meths = self._make_code_methods(\n            boosted_dicts, annotations, path_py\n        )\n        lines_code.extend(code_for_meths)\n        if signatures:\n            lines_header.extend(signatures)\n\n        # Deal with blocks\n        signatures, code_blocks = self._make_code_blocks(blocks)\n        lines_code.extend(code_blocks)\n        if signatures:\n            lines_header.extend(signatures)\n\n        code = ""\\n"".join(lines_code).strip()\n\n        if code:\n            code = self._make_beginning_code() + code\n            self._append_line_header_variable(lines_header, ""__transonic__"")\n            code += f""\\n\\n__transonic__ = (\'{transonic.__version__}\',)""\n\n        return format_str(code), codes_ext, ""\\n"".join(lines_header).strip() + ""\\n""\n\n    def _append_line_header_variable(self, lines_header, name_variable):\n        pass\n\n    def _make_code_blocks(self, blocks):\n        code = []\n        signatures_blocks = []\n        for block in blocks:\n\n            str_variables = "", "".join(block.signatures[0].keys())\n            fdef_block = extast.ast.parse(\n                f""""""def {block.name}({str_variables}):pass""""""\n            ).body[0]\n\n            # TODO: locals_types for blocks\n            locals_types = None\n            signatures_blocks.extend(\n                self._make_header_from_fdef_annotations(\n                    fdef_block, block.signatures, locals_types\n                )\n            )\n\n            code.append(f""\\ndef {block.name}({str_variables}):\\n"")\n            code.append(indent(extast.unparse(block.ast_code), ""    ""))\n            if block.results:\n                code.append(f""    return {\', \'.join(block.results)}\\n"")\n\n        arguments_blocks = {\n            block.name: list(block.signatures[0].keys()) for block in blocks\n        }\n\n        if arguments_blocks:\n            self._append_line_header_variable(\n                signatures_blocks, ""arguments_blocks""\n            )\n            code.append(f""arguments_blocks = {str(arguments_blocks)}\\n"")\n        return signatures_blocks, code\n\n    def _make_code_methods(self, boosted_dicts, annotations, path_py):\n        meths_code = []\n        header_lines = []\n        for (class_name, meth_name), fdef in boosted_dicts[""methods""].items():\n            signatures, code_for_meth = self._make_code_method(\n                class_name, fdef, meth_name, annotations, boosted_dicts\n            )\n            meths_code.append(code_for_meth)\n            header_lines.extend(signatures)\n        return header_lines, meths_code\n\n    def _make_code_method(\n        self, class_name, fdef, meth_name, annotations, boosted_dicts\n    ):\n        class_def = boosted_dicts[""classes""][class_name]\n\n        if class_name in annotations[""classes""]:\n            annotations_class = annotations[""classes""][class_name]\n        else:\n            annotations_class = {}\n\n        if (class_name, meth_name) in annotations[""methods""]:\n            annotations_meth = annotations[""methods""][(class_name, meth_name)]\n        else:\n            annotations_meth = {}\n\n        meth_name = fdef.name\n        python_code, attributes, _ = make_new_code_method_from_nodes(\n            class_def, fdef\n        )\n\n        for attr in attributes:\n            if attr not in annotations_class:\n                raise NotImplementedError(\n                    f""self.{attr} used but {attr} not in class annotations""\n                )\n        types_attrs = {\n            ""self_"" + attr: annotations_class[attr] for attr in attributes\n        }\n        types_pythran = {**types_attrs, **annotations_meth}\n\n        # TODO: locals_types for methods\n        locals_types = None\n        signatures_method = self._make_header_from_fdef_annotations(\n            extast.parse(python_code).body[0], [types_pythran], locals_types\n        )\n\n        str_self_dot_attributes = "", "".join(""self."" + attr for attr in attributes)\n        args_func = [arg.id for arg in fdef.args.args[1:]]\n        str_args_func = "", "".join(args_func)\n\n        defaults = fdef.args.defaults\n        nb_defaults = len(defaults)\n        nb_args = len(fdef.args.args)\n        nb_no_defaults = nb_args - nb_defaults - 1\n\n        str_args_value_func = []\n        ind_default = 0\n        for ind, arg in enumerate(fdef.args.args[1:]):\n            name = arg.id\n            if ind < nb_no_defaults:\n                str_args_value_func.append(f""{name}"")\n            else:\n                default = extast.unparse(defaults[ind_default]).strip()\n                str_args_value_func.append(f""{name}={default}"")\n                ind_default += 1\n\n        str_args_value_func = "", "".join(str_args_value_func)\n\n        if str_self_dot_attributes:\n            str_args_backend_func = "", "".join(\n                (str_self_dot_attributes, str_args_func)\n            )\n        else:\n            str_args_backend_func = str_args_func\n\n        name_var_code_new_method = f""__code_new_method__{class_name}__{meth_name}""\n\n        self._append_line_header_variable(\n            signatures_method, name_var_code_new_method\n        )\n        python_code += (\n            f\'\\n{name_var_code_new_method} = """"""\\n\\n\'\n            f""def new_method(self, {str_args_value_func}):\\n""\n            f""    return backend_func({str_args_backend_func})""\n            \'\\n\\n""""""\\n\'\n        )\n\n        return signatures_method, format_str(python_code)\n\n    def _make_header_1_function(self, fdef, annotations):\n        raise NotImplementedError\n\n    def _make_header_from_fdef_annotations(\n        self, fdef, annotations, locals_types=None, returns=None\n    ):\n        raise NotImplementedError\n\n    def name_ext_from_path_backend(self, path_backend):\n        """"""Return an extension name given the path of a Pythran file""""""\n\n        name = None\n        if mpi.rank == 0:\n            path_backend = PathSeq(path_backend)\n            if path_backend.exists():\n                with open(path_backend) as file:\n                    src = file.read()\n                # quick fix to recompile when the header has been changed\n                for suffix in ("".pythran"", "".pxd""):\n                    path_header = path_backend.with_suffix(suffix)\n                    if path_header.exists():\n                        with open(path_header) as file:\n                            src += file.read()\n            else:\n                src = """"\n\n            name = path_backend.stem + ""_"" + make_hex(src) + self.suffix_extension\n\n        return mpi.bcast(name)\n\n    def compile_extensions(\n        self,\n        paths: Iterable[Path],\n        str_accelerator_flags: str,\n        parallel=True,\n        force=True,\n    ):\n        for path in paths:\n            self.compile_extension(\n                path,\n                str_accelerator_flags=str_accelerator_flags,\n                parallel=parallel,\n                force=force,\n            )\n\n    def compile_extension(\n        self,\n        path_backend,\n        name_ext_file=None,\n        native=False,\n        xsimd=False,\n        openmp=False,\n        str_accelerator_flags: Optional[str] = None,\n        parallel=True,\n        force=True,\n    ):\n        raise NotImplementedError\n\n\nclass BackendAOT(Backend):\n    """"""Backend for ahead-of-time compilers""""""\n\n    def check_if_compiled(self, module):\n        try:\n            path = module.__file__\n        except AttributeError:\n            return True\n\n        return not path.endswith("".py"")\n\n    def compile_extension(\n        self,\n        path_backend,\n        name_ext_file=None,\n        native=False,\n        xsimd=False,\n        openmp=False,\n        str_accelerator_flags: Optional[str] = None,\n        parallel=True,\n        force=True,\n    ):\n        if name_ext_file is None:\n            name_ext_file = self.name_ext_from_path_backend(path_backend)\n\n        compiling = True\n        process = compile_extension(\n            path_backend,\n            self.name,\n            name_ext_file,\n            native=native,\n            xsimd=xsimd,\n            openmp=openmp,\n            str_accelerator_flags=str_accelerator_flags,\n            parallel=parallel,\n            force=force,\n        )\n        return compiling, process\n\n    def _make_header_1_function(self, fdef, annotations):\n\n        try:\n            annots = annotations[""__in_comments__""][fdef.name]\n        except KeyError:\n            annots = []\n\n        try:\n            annot = annotations[""functions""][fdef.name]\n        except KeyError:\n            pass\n        else:\n            annots.append(annot)\n\n        locals_types = annotations[""__locals__""].get(fdef.name, None)\n        returns = annotations[""__returns__""].get(fdef.name, None)\n\n        return self._make_header_from_fdef_annotations(\n            fdef, annots, locals_types, returns\n        )\n\n    def _make_header_from_fdef_annotations(\n        self, fdef, annotations, locals_types=None, returns=None\n    ):\n        signatures_as_lists_strings = []\n        for annot in annotations:\n            # print(""DEBUG, annot"")\n            # pprint(annot)\n            signatures_as_lists_strings.extend(\n                compute_signatures_from_typeobjects(annot, self.type_formatter)\n            )\n\n        # print(""DEBUG, signatures_as_lists_strings"")\n        # pprint(signatures_as_lists_strings)\n\n        return self._make_header_from_fdef_signatures(\n            fdef,\n            signatures_as_lists_strings,\n            locals_types=locals_types,\n            returns=returns,\n        )\n\n\nclass BackendJIT(Backend):\n    """"""Backend for just-in-time compilers""""""\n\n    suffix_extension = "".py""\n    needs_compilation = False\n\n    def check_if_compiled(self, module):\n        return True\n\n    def _make_header_1_function(self, fdef, annotations):\n        return []\n\n    def _make_first_lines_header(self):\n        return []\n\n    def _make_header_from_fdef_annotations(\n        self, fdef, annotations, locals_types=None, returns=None\n    ):\n        return []\n'"
transonic/backends/base_jit.py,0,"b'""""""SubBackend class for Transonic JIT compilation\n=================================================\n\nInternal API\n------------\n\n.. autoclass:: SubBackendJIT\n   :members:\n   :private-members:\n\n""""""\n\nimport re\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept ImportError:\n    np = None\n\nfrom transonic.analyses import extast\nfrom transonic.signatures import make_signatures_from_typehinted_func\nfrom transonic.log import logger\nfrom transonic import mpi\nfrom transonic.typing import format_type_as_backend_type, typeof\nfrom transonic.util import get_source_without_decorator, path_root\n\nfrom .for_classes import produce_code_class\n\n\nclass SubBackendJIT:\n    """"""Sub-class for Transonic JIT compilation""""""\n\n    def __init__(self, name, type_formatter):\n        self.name = name\n        self.name_capitalized = name.capitalize()\n        self.type_formatter = type_formatter\n\n        self.path_base = Path(path_root) / self.name / ""__jit__""\n        self.path_class = self.path_base.parent / ""__jit_class__""\n\n        if mpi.rank == 0:\n            self.path_base.mkdir(parents=True, exist_ok=True)\n            self.path_class.mkdir(parents=True, exist_ok=True)\n        mpi.barrier()\n\n    def make_backend_source(self, info_analysis, func, path_backend):\n        func_name = func.__name__\n        jitted_dicts = info_analysis[""jitted_dicts""]\n        src = info_analysis[""codes_dependance""][func_name]\n        if func_name in info_analysis[""special""]:\n            if func_name in jitted_dicts[""functions""]:\n                src += extast.unparse(jitted_dicts[""functions""][func_name])\n            elif func_name in jitted_dicts[""methods""]:\n                src += extast.unparse(jitted_dicts[""methods""][func_name])\n        else:\n            # TODO find a prettier solution to remove decorator for cython\n            # than doing two times a regex\n            src += re.sub(\n                r""@.*?\\sdef\\s"", ""def "", get_source_without_decorator(func)\n            )\n        has_to_write = True\n        if path_backend.exists() and mpi.rank == 0:\n            with open(path_backend) as file:\n                src_old = file.read()\n            if src_old == src:\n                has_to_write = False\n\n        return src, has_to_write\n\n    def make_new_header(self, func, arg_types):\n        # Include signature comming from type hints\n        signatures = make_signatures_from_typehinted_func(\n            func, self.type_formatter\n        )\n        exports = set(f""export {signature}"" for signature in signatures)\n\n        if arg_types != ""no types"":\n            exports.add(f""export {func.__name__}({\', \'.join(arg_types)})"")\n        return exports\n\n    def merge_old_and_new_header(self, path_backend_header, header, func):\n\n        try:\n            path_backend_header_exists = path_backend_header.exists()\n        except TimeoutError:\n            raise RuntimeError(\n                f""A MPI communication in Transonic failed when compiling ""\n                f""function {func}. This usually arises when a jitted ""\n                ""function has to be compiled in MPI and is only called ""\n                f""by one process (rank={mpi.rank}).""\n            )\n\n        if path_backend_header_exists:\n            # get the old signature(s)\n            header_old = self._load_old_header(path_backend_header)\n            # FIXME: what do we do with the old signatures?\n            header = self._merge_header_objects(header, header_old)\n\n        return self._make_header_code(header)\n\n    def _load_old_header(self, path_backend_header):\n        exports_old = None\n        if mpi.rank == 0:\n            with open(path_backend_header) as file:\n                exports_old = [export.strip() for export in file.readlines()]\n        exports_old = mpi.bcast(exports_old)\n        return exports_old\n\n    def _merge_header_objects(self, header, header_old):\n        header.update(header_old)\n        return header\n\n    def _make_header_code(self, header):\n        return ""\\n"".join(sorted(header)) + ""\\n""\n\n    def write_new_header(self, path_backend_header, header, arg_types):\n        mpi.barrier()\n        if mpi.rank == 0:\n            logger.debug(\n                f""write {self.name_capitalized} signature in file ""\n                f""{path_backend_header} with types\\n{arg_types}""\n            )\n            with open(path_backend_header, ""w"") as file:\n                file.write(header)\n                file.flush()\n\n    def compute_typename_from_object(self, obj: object):\n        """"""return the backend type name""""""\n        transonic_type = typeof(obj)\n        return format_type_as_backend_type(transonic_type, self.type_formatter)\n\n    def produce_code_class(self, cls):\n        return produce_code_class(cls)\n'"
transonic/backends/cython.py,6,"b'""""""Cython Backend\n=================\n\nInternal API\n------------\n\n.. autoclass:: HeaderFunction\n   :members:\n   :private-members:\n\n.. autoclass:: SubBackendJITCython\n   :members:\n   :private-members:\n\n.. autoclass:: CythonBackend\n   :members:\n   :private-members:\n\n""""""\nimport copy\nimport inspect\n\nfrom warnings import warn\n\nfrom transonic.analyses.extast import unparse, ast, FunctionDef, Name\nfrom transonic.signatures import make_signatures_from_typehinted_func\nfrom transonic.typing import format_type_as_backend_type, MemLayout\n\nfrom .base import BackendAOT, TypeHintRemover, format_str\nfrom .base_jit import SubBackendJIT\nfrom .typing import TypeFormatter\n\n\ndef normalize_type_name_for_array(name):\n    if name == ""bool_"":\n        return ""np.uint8""\n    if any(name.endswith(str(number)) for number in (8, 16, 32, 64, 128)):\n        return ""np."" + name\n    if name in (""int"", ""float"", ""complex""):\n        return ""np."" + name\n    return name\n\n\nclass TypeFormatterCython(TypeFormatter):\n    def normalize_type_name(self, name):\n        if any(name.endswith(str(number)) for number in (8, 16, 32, 64, 128)):\n            return ""np."" + name + ""_t""\n        if name in (""int"", ""float"", ""complex"", ""str""):\n            return f""cython.{name}""\n        return name\n\n    def make_array_code(\n        self, dtype, ndim, shape, memview, mem_layout, positive_indices\n    ):\n        dtype = normalize_type_name_for_array(dtype.__name__)\n        if ndim == 0:\n            return dtype\n\n        if memview:\n            return memoryview_type(dtype, ndim, mem_layout)\n        else:\n            return np_ndarray_type(dtype, ndim, mem_layout, positive_indices)\n\n    def make_dict_code(self, type_keys, type_values, **kwargs):\n        return ""dict""\n\n    def make_set_code(self, type_keys, **kwargs):\n        return ""set""\n\n    def make_list_code(self, type_elem, **kwargs):\n        return ""list""\n\n    def make_tuple_code(self, types, **kwargs):\n        return ""tuple""\n\n    def make_const_code(self, code):\n        return ""const "" + code\n\n\ndef memoryview_type(dtype, ndim, mem_layout) -> str:\n    ndim_F = 0\n    ndim_C = 0\n    if mem_layout is MemLayout.C:\n        ndim_C = 1\n        ndim -= 1\n    elif mem_layout is MemLayout.F:\n        ndim_F = 1\n        ndim -= 1\n    end = "", "".join([""::1""] * ndim_F + ["":""] * ndim + [""::1""] * ndim_C)\n    return f""{dtype}_t[{end}]""\n\n\ndef np_ndarray_type(dtype, ndim, mem_layout, positive_indices) -> str:\n    if mem_layout is MemLayout.C:\n        mode = \', mode=""c""\'\n    elif mem_layout is MemLayout.F:\n        mode = \', mode=""f""\'\n    else:\n        mode = """"\n\n    if positive_indices:\n        positive_indices = "", negative_indices=False""\n    else:\n        positive_indices = """"\n\n    return f""np.ndarray[{dtype}_t, ndim={ndim}{mode}{positive_indices}]""\n\n\nclass HeaderFunction:\n    def __init__(\n        self,\n        path=None,\n        name=None,\n        arguments=None,\n        types: dict = None,\n        imports=None,\n    ):\n\n        if path is not None:\n            self.path = path\n            with open(path) as file:\n                lines = file.readlines()\n\n            last_line = lines[-1]\n            assert last_line.startswith(""cpdef "")\n            name = last_line.split("" "", 1)[1].split(""("", 1)[0]\n\n            parts = [\n                part.strip()\n                for part in """".join(lines[:-1]).split(""ctypedef fused "")\n            ]\n            imports = parts[0]\n\n            types = {}\n\n            for part in parts[1:]:\n                assert part.startswith(f""__{name}_"")\n                lines = part.split(""\\n"")\n                arg_name = lines[0].split(f""__{name}_"", 1)[1].split("":"", 1)[0]\n                types[arg_name] = set(line.strip() for line in lines[1:])\n\n        if types is None:\n            if arguments is None:\n                raise ValueError\n            types = {key: set() for key in arguments}\n\n        if arguments is None:\n            arguments = types.keys()\n\n        self.arguments = arguments\n        self.name = name\n        self.types = types\n        self.imports = imports\n\n    def make_code(self):\n\n        bits = [self.imports + ""\\n\\n""]\n\n        for arg, types in self.types.items():\n            bits.append(f""ctypedef fused __{self.name}_{arg}:\\n"")\n            for type_ in sorted(types):\n                bits.append(f""    {type_}\\n"")\n            bits.append(""\\n"")\n\n        tmp = "", "".join(f""__{self.name}_{arg} {arg}"" for arg in self.types)\n        bits.append(f""cpdef {self.name}({tmp})"")\n        code = """".join(bits)\n        return code\n\n    def add_signature(self, new_types):\n        for new_type, set_types in zip(new_types, self.types.values()):\n            set_types.add(new_type)\n\n    def update_with_other_header(self, other):\n        if self.name != other.name:\n            raise ValueError\n        if self.types.keys() != other.types.keys():\n            raise ValueError\n        for key, value in other.types.items():\n            self.types[key].update(value)\n\n\nclass SubBackendJITCython(SubBackendJIT):\n    def make_new_header(self, func, arg_types):\n        # Include signature comming from type hints\n        header = HeaderFunction(\n            name=func.__name__,\n            arguments=list(inspect.signature(func).parameters.keys()),\n            imports=""import cython\\n\\nimport numpy as np\\ncimport numpy as np\\n"",\n        )\n\n        signatures = make_signatures_from_typehinted_func(\n            func, self.type_formatter, as_list_str=True\n        )\n\n        for signature in signatures:\n            header.add_signature(signature)\n\n        if arg_types != ""no types"":\n            header.add_signature(arg_types)\n\n        return header\n\n    def _load_old_header(self, path_backend_header):\n        return HeaderFunction(path=path_backend_header)\n\n    def _merge_header_objects(self, header, header_old):\n        header.update_with_other_header(header_old)\n        return header\n\n    def _make_header_code(self, header):\n        return header.make_code()\n\n\nclass CythonBackend(BackendAOT):\n    """"""Main class for the Cython backend""""""\n\n    backend_name = ""cython""\n    suffix_header = "".pxd""\n    keyword_export = ""cpdef""\n    _SubBackendJIT = SubBackendJITCython\n    _TypeFormatter = TypeFormatterCython\n\n    def _make_first_lines_header(self):\n        return [""import cython\\n\\nimport numpy as np\\ncimport numpy as np\\n""]\n\n    def _make_header_from_fdef_annotations(\n        self, fdef, annotations: dict, locals_types=None, returns=None\n    ):\n\n        if hasattr(fdef, ""_transonic_keywords""):\n            decorator_keywords = fdef._transonic_keywords\n        else:\n            decorator_keywords = {}\n\n        inline = decorator_keywords.get(""inline"", False)\n        inline = ""inline "" if inline else """"\n\n        fdef = FunctionDef(name=fdef.name, args=copy.deepcopy(fdef.args), body=[])\n\n        assert isinstance(annotations, list)\n\n        if len(annotations) > 1:\n            warn(\n                ""Cython backend only supports one set of annotations. ""\n                ""Please use Transonic fused types.""\n            )\n\n        try:\n            annotations = annotations[0]\n        except IndexError:\n            annotations = {}\n\n        transonic_types = set(annotations.values())\n\n        if locals_types:\n            transonic_types.update(locals_types.values())\n\n        if returns:\n            transonic_types.add(returns)\n\n        transonic_types = sorted(transonic_types, key=repr)\n\n        template_parameters = set()\n        for ttype in transonic_types:\n            if hasattr(ttype, ""get_template_parameters""):\n                template_parameters.update(ttype.get_template_parameters())\n\n        template_parameters = sorted(template_parameters, key=repr)\n\n        transonic_fused_types = [\n            ttype\n            for ttype in transonic_types\n            if hasattr(ttype, ""is_fused_type"") and ttype.is_fused_type()\n        ]\n\n        if not all(param.values for param in template_parameters):\n            raise ValueError(\n                f""{template_parameters}, {[param.values for param in template_parameters]}""\n            )\n\n        cython_fused_types = {}\n\n        def get_ttype_name(ttype):\n            if hasattr(ttype, ""short_repr""):\n                ttype_name = ttype.short_repr()\n            elif hasattr(ttype, ""__name__""):\n                ttype_name = ttype.__name__\n            elif isinstance(ttype, str):\n                ttype_name = ttype\n            else:\n                raise RuntimeError\n            return ttype_name\n\n        for ttype in transonic_fused_types:\n            ttype_name = get_ttype_name(ttype)\n            name_cython_type = f""__{fdef.name}__{ttype_name}""\n\n            cython_types = ttype.get_all_formatted_backend_types(\n                self.type_formatter\n            )\n            if ""None"" in cython_types:\n                cython_types.remove(""None"")\n            cython_fused_types[name_cython_type] = cython_types\n\n        signatures_func = []\n\n        for name, possible_types in cython_fused_types.items():\n            ctypedef = [f""ctypedef fused {name}:\\n""]\n            for possible_type in sorted(set(possible_types)):\n                ctypedef.append(f""   {possible_type}\\n"")\n            signatures_func.append("""".join(ctypedef))\n\n        def get_name_cython_type(ttype):\n            ttype_name = get_ttype_name(ttype)\n            name_cython_type = f""__{fdef.name}__{ttype_name}""\n            if name_cython_type in cython_fused_types:\n                return name_cython_type\n            return format_type_as_backend_type(ttype, self.type_formatter)\n\n        # change function parameters\n        if fdef.args.defaults:\n            name_start = Name(""*"", ast.Param())\n            fdef.args.defaults = [name_start] * len(fdef.args.defaults)\n        for name in fdef.args.args:\n            name.annotation = None\n            if annotations:\n                ttype = annotations[name.id]\n                name_cython_type = get_name_cython_type(ttype)\n            else:\n                name_cython_type = ""object""\n            name.id = f""{name_cython_type} {name.id}""\n\n        if locals_types is not None and locals_types:\n            # note: np.ndarray not supported by Cython in ""locals""\n            # TODO: thus, fused types not supported here\n            locals_types = "", "".join(\n                f""{k}={format_type_as_backend_type(v, self.type_formatter, memview=True)}""\n                for k, v in locals_types.items()\n            )\n            signatures_func.append(f""@cython.locals({locals_types})"")\n\n        if returns is not None:\n            ttype = returns\n            name_cython_type = get_name_cython_type(ttype)\n            returns = name_cython_type + "" ""\n        else:\n            returns = """"\n\n        def_keyword = ""cpdef""\n        signatures_func.append(\n            f""{def_keyword} {inline}{returns}{unparse(fdef).strip()[4:-1]}\\n""\n        )\n        return signatures_func\n\n    def _make_code_from_fdef_node(self, fdef):\n\n        if hasattr(fdef, ""_transonic_keywords""):\n            decorator_keywords = fdef._transonic_keywords\n        else:\n            decorator_keywords = {}\n\n        parts = []\n\n        if not decorator_keywords.get(""boundscheck"", True):\n            parts.append(""@cython.boundscheck(False)"")\n\n        if not decorator_keywords.get(""wraparound"", True):\n            parts.append(""@cython.wraparound(False)"")\n\n        if decorator_keywords.get(""cdivision"", False):\n            parts.append(""@cython.cdivision(True)"")\n\n        if not decorator_keywords.get(""nonecheck"", True):\n            parts.append(""@cython.noneckeck(False)"")\n\n        if decorator_keywords.get(""nogil"", False):\n            parts.append(""@cython.nogil"")\n\n        transformed = TypeHintRemover().visit(fdef)\n        # convert the AST back to source code\n        parts.append(unparse(transformed))\n\n        return format_str(""\\n"".join(parts))\n\n    def _make_beginning_code(self):\n        return (\n            ""try:\\n""\n            ""    import cython\\n""\n            ""except ImportError:\\n""\n            ""    from transonic_cl import cython\\n\\n""\n        )\n'"
transonic/backends/for_classes.py,0,"b'""""""Make the Pythran files for the classes\n=========================================\n\n""""""\n\nfrom tokenize import tokenize, untokenize, NAME, OP\n\n# from token import tok_name\nimport inspect\nfrom io import BytesIO\n\nfrom transonic.signatures import compute_signatures_from_typeobjects\n\n# from transonic.log import logger\nfrom transonic.util import (\n    get_source_without_decorator,\n    format_str,\n    make_code_from_fdef_node,\n)\n\nfrom .typing import base_type_formatter\n\n\ndef produce_code_class(cls):\n    pythran_code = """"\n    for key, value in cls.__dict__.items():\n        if hasattr(value, ""__transonic__"") and value.__transonic__ in (\n            ""trans_def_method"",\n            ""jit_method"",\n        ):\n            pythran_code += make_code_method_jit(cls, key)\n    return pythran_code\n\n\ndef make_code_method_jit(cls, func_name):\n\n    func = cls.__dict__[func_name]\n    func = func.func\n\n    new_code, attributes, name_new_func = make_new_code_method_from_objects(\n        cls, func\n    )\n\n    try:\n        cls_annotations = cls.__annotations__\n    except AttributeError:\n        cls_annotations = {}\n\n    types_attrs = [\n        cls_annotations[attr] for attr in attributes if attr in cls_annotations\n    ]\n\n    signature = inspect.signature(func)\n    types_func = [param.annotation for param in signature.parameters.values()][1:]\n    types_pythran = types_attrs + types_func\n\n    transonic_signatures = ""\\n""\n\n    try:\n        signatures_as_lists_strings = compute_signatures_from_typeobjects(\n            types_pythran, base_type_formatter\n        )\n    except ValueError:\n        signatures_as_lists_strings = []\n\n    for signature_as_strings in signatures_as_lists_strings:\n        transonic_signatures += (\n            f""# transonic def {name_new_func}(""\n            + "", "".join(signature_as_strings)\n            + "")\\n""\n        )\n\n    new_code = ""from transonic import jit\\n\\n@jit\\n"" + new_code\n\n    python_code = transonic_signatures + ""\\n"" + new_code\n\n    str_self_dot_attributes = "", "".join(""self."" + attr for attr in attributes)\n    args_func = list(signature.parameters.keys())[1:]\n    str_args_func = "", "".join(args_func)\n\n    str_args_value_func = """"\n    for param, value in signature.parameters.items():\n        if param == ""self"":\n            continue\n        elif value.default is value.empty:\n            str_args_value_func += f""{param}, ""\n        else:\n            str_args_value_func += f""{param}={value.default}, ""\n\n    str_args_value_func = str_args_value_func.rstrip("", "")\n\n    name_new_method = f""__new_method__{cls.__name__}__{func_name}""\n    python_code += (\n        f""\\ndef {name_new_method}""\n        f""(self, {str_args_value_func}):\\n""\n        f""    return {name_new_func}({str_self_dot_attributes}, {str_args_func})""\n        ""\\n""\n    )\n\n    python_code = format_str(python_code)\n\n    return python_code\n\n\ndef make_new_code_method_from_nodes(class_def, fdef):\n    source = make_code_from_fdef_node(fdef)\n    return make_new_code_method_from_source(source, fdef.name, class_def.name)\n\n\ndef make_new_code_method_from_objects(cls, func):\n    source = get_source_without_decorator(func)\n    return make_new_code_method_from_source(source, func.__name__, cls.__name__)\n\n\ndef make_new_code_method_from_source(source, func_name, cls_name):\n\n    tokens = []\n    attributes = set()\n\n    using_self = False\n\n    g = tokenize(BytesIO(source.encode(""utf-8"")).readline)\n    for toknum, tokval, _, _, _ in g:\n        # logger.debug((tok_name[toknum], tokval))\n\n        if using_self == ""self"":\n            if toknum == OP and tokval == ""."":\n                using_self = tokval\n                continue\n            elif toknum == OP and tokval in ("","", "")""):\n                tokens.append((NAME, ""self""))\n                using_self = False\n            else:\n                raise NotImplementedError(\n                    f""self{tokval} not supported by Transonic""\n                )\n\n        if using_self == ""."":\n            if toknum == NAME:\n                using_self = False\n                tokens.append((NAME, ""self_"" + tokval))\n                attributes.add(tokval)\n                continue\n            else:\n                raise NotImplementedError\n\n        if toknum == NAME and tokval == ""self"":\n            using_self = ""self""\n            continue\n\n        tokens.append((toknum, tokval))\n\n    attributes = sorted(attributes)\n\n    attributes_self = [""self_"" + attr for attr in attributes]\n\n    index_self = tokens.index((NAME, ""self""))\n\n    tokens_attr = []\n    for ind, attr in enumerate(attributes_self):\n        tokens_attr.append((NAME, attr))\n        tokens_attr.append((OP, "",""))\n\n    if tokens[index_self + 1] == (OP, "",""):\n        del tokens[index_self + 1]\n\n    tokens = tokens[:index_self] + tokens_attr + tokens[index_self + 1 :]\n\n    index_func_name = tokens.index((NAME, func_name))\n    name_new_func = f""__for_method__{cls_name}__{func_name}""\n    tokens[index_func_name] = (NAME, name_new_func)\n    # change recursive calls\n    if func_name in attributes:\n        attributes.remove(func_name)\n        index_rec_calls = [\n            index\n            for index, (name, value) in enumerate(tokens)\n            if value == ""self_"" + func_name\n        ]\n        # delete the occurrence of ""self_"" + func_name in function parameter\n        del tokens[index_rec_calls[0] + 1]\n        del tokens[index_rec_calls[0]]\n        # consider the two deletes\n        offset = -2\n        # adapt all recurrence calls\n        for ind in index_rec_calls[1:]:\n            # adapt the index to the inserts and deletes\n            ind += offset\n            tokens[ind] = (tokens[ind][0], name_new_func)\n            # put the attributes in parameter\n            for attr in reversed(attributes):\n                tokens.insert(ind + 2, (1, "",""))\n                tokens.insert(ind + 2, (1, ""self_"" + attr))\n            # consider the inserts\n            offset += len(attributes) * 2\n    new_code = untokenize(tokens).decode(""utf-8"")\n\n    return new_code, attributes, name_new_func\n'"
transonic/backends/for_test_set_backend.py,0,"b'from transonic import boost, jit, set_backend_for_this_module, Transonic\n\nset_backend_for_this_module(""python"")\n\nts = Transonic()\n\n\n@boost\ndef func():\n    return 0\n\n\n@jit\ndef func_jit():\n    return 0\n'"
transonic/backends/numba.py,0,"b'""""""Numba backend\n================\n\nInternal API\n------------\n\n.. autoclass:: SubBackendJITNumba\n   :members:\n   :private-members:\n\n.. autoclass:: NumbaBackend\n   :members:\n   :private-members:\n\n""""""\n\nfrom typing import Optional\n\nfrom transonic.analyses.extast import parse, unparse, CommentLine, ast\nfrom transonic.util import format_str\n\nfrom .py import PythonBackend, SubBackendJITPython\n\n\ndef add_numba_comments(code):\n    """"""Add Numba code in Python comments""""""\n    mod = parse(code)\n    new_body = [CommentLine(""# __protected__ from numba import njit"")]\n\n    for node in mod.body:\n        if isinstance(node, ast.FunctionDef):\n            new_body.append(CommentLine(""# __protected__ @njit""))\n        new_body.append(node)\n\n    mod.body = new_body\n    return format_str(unparse(mod))\n\n\nclass SubBackendJITNumba(SubBackendJITPython):\n    def make_backend_source(self, info_analysis, func, path_backend):\n        src, has_to_write = super().make_backend_source(\n            info_analysis, func, path_backend\n        )\n\n        if not src:\n            return src, has_to_write\n\n        return add_numba_comments(src), has_to_write\n\n\nclass NumbaBackend(PythonBackend):\n    """"""Main class for the Numba backend""""""\n\n    backend_name = ""numba""\n    _SubBackendJIT = SubBackendJITNumba\n\n    def compile_extension(\n        self,\n        path_backend,\n        name_ext_file=None,\n        native=False,\n        xsimd=False,\n        openmp=False,\n        str_accelerator_flags: Optional[str] = None,\n        parallel=True,\n        force=True,\n    ):\n        if name_ext_file is None:\n            name_ext_file = self.name_ext_from_path_backend(path_backend)\n\n        with open(path_backend) as file:\n            source = file.read()\n\n        source = source.replace(""# __protected__ "", """")\n\n        with open(path_backend.with_name(name_ext_file), ""w"") as file:\n            file.write(format_str(source))\n\n        compiling = False\n        process = None\n        return compiling, process\n\n    def _make_backend_code(self, path_py, analyse):\n        """"""Create a backend code from a Python file""""""\n        code, codes_ext, header = super()._make_backend_code(path_py, analyse)\n\n        if not code:\n            return code, codes_ext, header\n\n        return add_numba_comments(code), codes_ext, header\n'"
transonic/backends/py.py,0,"b'""""""Python backend\n=================\n\nInternal API\n------------\n\n.. autoclass:: SubBackendJITPython\n   :members:\n   :private-members:\n\n.. autoclass:: PythonBackend\n   :members:\n   :private-members:\n\n""""""\n\nfrom shutil import copyfile\nfrom typing import Optional\n\nfrom .base import BackendJIT\nfrom .base_jit import SubBackendJIT\n\n\nclass SubBackendJITPython(SubBackendJIT):\n    def make_new_header(self, func, arg_types):\n        return """"\n\n    def merge_old_and_new_header(self, path_backend_header, header, func):\n        return """"\n\n    def write_new_header(self, path_backend_header, header, arg_types):\n        pass\n\n\nclass PythonBackend(BackendJIT):\n    """"""Main class for the Python backend""""""\n\n    backend_name = ""python""\n    _SubBackendJIT = SubBackendJITPython\n\n    def compile_extension(\n        self,\n        path_backend,\n        name_ext_file=None,\n        native=False,\n        xsimd=False,\n        openmp=False,\n        str_accelerator_flags: Optional[str] = None,\n        parallel=True,\n        force=True,\n    ):\n        if name_ext_file is None:\n            name_ext_file = self.name_ext_from_path_backend(path_backend)\n\n        copyfile(path_backend, path_backend.with_name(name_ext_file))\n        compiling = False\n        process = None\n        return compiling, process\n'"
transonic/backends/pythran.py,0,"b'""""""Pythran Backend\n==================\n\nInternal API\n------------\n\n.. autoclass:: PythranBackend\n   :members:\n   :private-members:\n\n""""""\n\nfrom .base import BackendAOT\n\n\nclass PythranBackend(BackendAOT):\n    """"""Main class for the Pythran backend""""""\n\n    backend_name = ""pythran""\n    suffix_header = "".pythran""\n\n    def check_if_compiled(self, module):\n        return hasattr(module, ""__pythran__"")\n\n    def _append_line_header_variable(self, lines_header, name_variable):\n        lines_header.append(f""export {name_variable}\\n"")\n\n    def _make_header_from_fdef_signatures(\n        self, fdef, signatures_as_lists_strings, locals_types=None, returns=None\n    ):\n        nb_defaults = len(fdef.args.defaults)\n        if nb_defaults:\n            new_signatures = []\n            for signature_as_strings in signatures_as_lists_strings:\n                for nb_args_not_given in range(1, nb_defaults + 1):\n                    new_signatures.append(\n                        signature_as_strings[:-nb_args_not_given]\n                    )\n            signatures_as_lists_strings.extend(new_signatures)\n\n        signatures_func = set(\n            f""export {fdef.name}({\', \'.join(signature_as_strings)})""\n            for signature_as_strings in signatures_as_lists_strings\n        )\n\n        if not fdef.args.args and not signatures_func:\n            signatures_func.add(f""export {fdef.name}()"")\n\n        signatures_func = sorted(signatures_func)\n        if signatures_func:\n            signatures_func[-1] = signatures_func[-1] + ""\\n""\n        return signatures_func\n'"
transonic/backends/test_backends.py,0,"b'from transonic.config import backend_default\n\nfrom transonic.justintime import modules_backends\n\n\ndef test_set_backend_for_this_module():\n    from .for_test_set_backend import ts, func\n\n    assert ts.backend.name == ""python""\n    assert func() == 0\n\n    module_name = ""transonic.backends.for_test_set_backend""\n\n    assert module_name in modules_backends[""python""]\n\n    if backend_default == ""python"":\n        return\n\n    assert module_name not in modules_backends[backend_default]\n'"
transonic/backends/test_cython.py,7,"b'import numpy as np\n\nfrom transonic import Array, const\nfrom transonic.backends import backends\n\nbackend = backends[""cython""]\ntype_formatter = backend.type_formatter\n\n\ndef compare(result, dtype, ndim, memview, mem_layout=None, positive_indices=None):\n    A = Array[dtype, ndim, memview, mem_layout, positive_indices]\n    assert A.format_as_backend_type(type_formatter) == result\n\n\ndef test_memview():\n    memview = ""memview""\n    compare(""np.int_t[:, ::1]"", int, ""2d"", memview, ""C"")\n    compare(""np.int_t[:, :, :]"", int, ""3d"", memview, ""strided"")\n    compare(""np.int32_t[::1, :]"", np.int32, ""2d"", memview, ""F"")\n\n\ndef test_array():\n    memview = None\n    compare(\'np.ndarray[np.int_t, ndim=2, mode=""c""]\', int, ""2d"", memview, ""C"")\n    compare(""np.ndarray[np.int_t, ndim=3]"", int, ""3d"", memview, ""strided"")\n    compare(\n        \'np.ndarray[np.int32_t, ndim=2, mode=""f""]\', np.int32, ""2d"", memview, ""F""\n    )\n    compare(\n        ""np.ndarray[np.int_t, ndim=2, negative_indices=False]"",\n        int,\n        ""2d"",\n        memview,\n        positive_indices=""positive_indices"",\n    )\n\n\ndef test_const():\n    A = Array[int, ""2d""]\n    assert ""const "" + A.format_as_backend_type(type_formatter) == const(\n        A\n    ).format_as_backend_type(type_formatter)\n'"
transonic/backends/test_typing.py,1,"b'import numpy as np\n\nfrom transonic import Array\n\nfrom .typing import base_type_formatter\n\n\ndef compare(dtype, ndim, mem_layout, result):\n    A = Array[dtype, ndim, mem_layout]\n    assert A.format_as_backend_type(base_type_formatter) == result\n\n\ndef test_array():\n    compare(int, ""2d"", ""C"", ""int[:, :] order(C)"")\n    compare(int, ""3d"", ""strided"", ""int[::, ::, ::]"")\n    compare(np.int32, ""2d"", ""F"", ""int32[:, :] order(F)"")\n'"
transonic/backends/typing.py,0,"b'from transonic.typing import format_type_as_backend_type, MemLayout\n\nnormalized_types = {""float"": ""float64"", ""complex"": ""complex128""}\n\n\nclass TypeFormatter:\n    def __init__(self, backend_name=None):\n        self.backend_name = backend_name\n\n    def normalize_type_name(self, name):\n        try:\n            return normalized_types[name]\n        except KeyError:\n            return name\n\n    def make_array_code(\n        self, dtype, ndim, shape, memview, mem_layout, positive_indices\n    ):\n        dtype = self.normalize_type_name(dtype.__name__)\n        if ndim == 0:\n            return dtype\n        one_dim = ["":""]\n        if mem_layout is MemLayout.strided:\n            one_dim = [""::""]\n        for_shape = one_dim * ndim\n        if shape is not None:\n            assert ndim == len(shape)\n            for index, value in enumerate(shape):\n                if value is not None:\n                    for_shape[index] = str(value)\n        result = f""{dtype}[{\', \'.join(for_shape)}]""\n        if mem_layout is MemLayout.C:\n            result += "" order(C)""\n        elif mem_layout is MemLayout.F:\n            result += "" order(F)""\n        return result\n\n    def make_dict_code(self, type_keys, type_values, **kwargs):\n        key = format_type_as_backend_type(type_keys, self, **kwargs)\n        value = format_type_as_backend_type(type_values, self, **kwargs)\n        return f""{key}: {value} dict""\n\n    def make_set_code(self, type_keys, **kwargs):\n        key = format_type_as_backend_type(type_keys, self, **kwargs)\n        return f""{key} set""\n\n    def make_list_code(self, type_elem, **kwargs):\n        return format_type_as_backend_type(type_elem, self, **kwargs) + "" list""\n\n    def make_tuple_code(self, types, **kwargs):\n        strings = [\n            format_type_as_backend_type(type_, self, **kwargs) for type_ in types\n        ]\n        return f""({\', \'.join(strings)})""\n\n    def make_const_code(self, code):\n        return code\n\n\nbase_type_formatter = TypeFormatter()\n'"
data_tests/saved__backend__/cython/add_inline.py,0,"b'try:\n    import cython\nexcept ImportError:\n    from transonic_cl import cython\n\n\ndef add(a, b):\n    return a + b\n\n\ndef use_add(n=10000):\n    tmp = 0\n    for _ in range(n):\n        tmp = add(tmp, 1)\n    return tmp\n\n\n__transonic__ = (""0.3.3"",)\n'"
data_tests/saved__backend__/cython/assign_func_boost.py,0,"b'try:\n    import cython\nexcept ImportError:\n    from transonic_cl import cython\n\n\ndef func(x):\n    return x ** 2\n\n\n__transonic__ = (""0.3.3"",)\n'"
data_tests/saved__backend__/cython/block_fluidsim.py,0,"b'try:\n    import cython\nexcept ImportError:\n    from transonic_cl import cython\n\n\ndef rk2_step0(state_spect_n12, state_spect, tendencies_n, diss2, dt):\n\n    # transonic block (\n    #     complex128[][][] state_spect_n12, state_spect,\n    #                      tendencies_n;\n    #     float64[][] diss2;\n    #     float dt\n    # )\n    state_spect_n12[:] = (state_spect + ((dt / 2) * tendencies_n)) * diss2\n\n\narguments_blocks = {\n    ""rk2_step0"": [""state_spect_n12"", ""state_spect"", ""tendencies_n"", ""diss2"", ""dt""]\n}\n\n__transonic__ = (""0.3.3"",)\n'"
data_tests/saved__backend__/cython/blocks_type_hints.py,0,"b'try:\n    import cython\nexcept ImportError:\n    from transonic_cl import cython\n\n\ndef block0(a, b, n):\n\n    # transonic block (\n    #     A a; A1 b;\n    #     int n\n    # )\n    # transonic block (\n    #     int[:] a, b;\n    #     float n\n    # )\n    result = ((a ** 2) + (b.mean() ** 3)) + n\n\n    return result\n\n\narguments_blocks = {""block0"": [""a"", ""b"", ""n""]}\n\n__transonic__ = (""0.3.3"",)\n'"
data_tests/saved__backend__/cython/boosted_class_use_import.py,1,"b'try:\n    import cython\nexcept ImportError:\n    from transonic_cl import cython\n\nimport numpy as np\nfrom exterior_import_boost import func_import\n\n\ndef __for_method__MyClass2__myfunc(self_attr0, self_attr1, arg):\n    return ((self_attr1 + self_attr0) + np.abs(arg)) + func_import()\n\n\n__code_new_method__MyClass2__myfunc = """"""\n\ndef new_method(self, arg):\n    return backend_func(self.attr0, self.attr1, arg)\n\n""""""\n\n__transonic__ = (""0.3.3"",)\n'"
data_tests/saved__backend__/cython/boosted_func_use_import.py,1,"b'try:\n    import cython\nexcept ImportError:\n    from transonic_cl import cython\n\nimport numpy as np\nfrom exterior_import_boost import func_import\n\n\ndef func(a, b):\n    return (a * np.log(b)).max() + func_import()\n\n\n__transonic__ = (""0.3.3"",)\n'"
data_tests/saved__backend__/cython/class_blocks.py,2,"b'try:\n    import cython\nexcept ImportError:\n    from transonic_cl import cython\n\nimport numpy as np\n\n\ndef block0(a, b, n):\n\n    # foo\n    # transonic block (\n    #     float[][] a, b;\n    #     int n\n    # ) bar\n    # foo\n    # transonic block (\n    #     float[][][] a, b;\n    #     int n\n    # )\n    # foobar\n    result = np.zeros_like(a)\n    for _ in range(n):\n        result += (a ** 2) + (b ** 3)\n\n    return result\n\n\ndef block1(a, b, n):\n\n    # transonic block (\n    #     float[][] a, b;\n    #     int n\n    # )\n    # transonic block (\n    #     float[][][] a, b;\n    #     int n\n    # )\n    result = np.zeros_like(a)\n    for _ in range(n):\n        result += (a ** 2) + (b ** 3)\n\n    return result\n\n\narguments_blocks = {""block0"": [""a"", ""b"", ""n""], ""block1"": [""a"", ""b"", ""n""]}\n\n__transonic__ = (""0.3.3"",)\n'"
data_tests/saved__backend__/cython/class_rec_calls.py,0,"b'try:\n    import cython\nexcept ImportError:\n    from transonic_cl import cython\n\n\ndef __for_method__Myclass__func(self_attr, self_attr2, arg):\n    if __for_method__Myclass__func(self_attr, self_attr2, (arg - 1)) < 1:\n        return 1\n    else:\n        a = __for_method__Myclass__func(\n            self_attr, self_attr2, (arg - 1)\n        ) * __for_method__Myclass__func(self_attr, self_attr2, (arg - 1))\n        return (\n            a + ((self_attr * self_attr2) * arg)\n        ) + __for_method__Myclass__func(self_attr, self_attr2, (arg - 1))\n\n\n__code_new_method__Myclass__func = """"""\n\ndef new_method(self, arg):\n    return backend_func(self.attr, self.attr2, arg)\n\n""""""\n\n__transonic__ = (""0.3.3"",)\n'"
data_tests/saved__backend__/cython/classic.py,1,"b'try:\n    import cython\nexcept ImportError:\n    from transonic_cl import cython\n\nimport numpy as np\n\n\ndef func(a, b):\n    return (a * np.log(b)).max()\n\n\n__transonic__ = (""0.3.3"",)\n'"
data_tests/saved__backend__/cython/default_params.py,0,"b'try:\n    import cython\nexcept ImportError:\n    from transonic_cl import cython\n\n\ndef func(a=1, b=None, c=1.0):\n    print(b)\n    return a + c\n\n\n__transonic__ = (""0.4.0"",)\n'"
data_tests/saved__backend__/cython/methods.py,1,"b'try:\n    import cython\nexcept ImportError:\n    from transonic_cl import cython\n\nimport numpy as np\n\n\ndef __for_method__Transmitter____call__(self_arr, self_freq, inp):\n    ""My docstring""\n    return ((inp * np.exp(((np.arange(len(inp)) * self_freq) * 1j))), self_arr)\n\n\n__code_new_method__Transmitter____call__ = """"""\n\ndef new_method(self, inp):\n    return backend_func(self.arr, self.freq, inp)\n\n""""""\n\n__transonic__ = (""0.3.3"",)\n'"
data_tests/saved__backend__/cython/mixed_classic_type_hint.py,2,"b'try:\n    import cython\nexcept ImportError:\n    from transonic_cl import cython\n\nimport numpy as np\n\n\ndef func(a, b):\n    return (a * np.log(b)).max()\n\n\ndef func1(a, b):\n    return a * np.cos(b)\n\n\n__transonic__ = (""0.3.3"",)\n'"
data_tests/saved__backend__/cython/no_arg.py,0,"b'try:\n    import cython\nexcept ImportError:\n    from transonic_cl import cython\n\n\ndef func():\n    return 1\n\n\ndef func2():\n    return 1\n\n\n__transonic__ = (""0.3.3"",)\n'"
data_tests/saved__backend__/cython/row_sum_boost.py,1,"b'try:\n    import cython\nexcept ImportError:\n    from transonic_cl import cython\n\nimport numpy as np\n\n\ndef row_sum(arr, columns):\n    return arr.T[columns].sum(0)\n\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\n@cython.cdivision(True)\n@cython.noneckeck(False)\ndef row_sum_loops(arr, columns):\n    # locals type annotations are used only for Cython\n    # arr.dtype not supported for memoryview\n    dtype = type(arr[(0, 0)])\n    res = np.empty(arr.shape[0], dtype=dtype)\n    for i in range(arr.shape[0]):\n        sum_ = dtype(0)\n        for j in range(columns.shape[0]):\n            sum_ += arr[(i, columns[j])]\n        res[i] = sum_\n    return res\n\n\n__transonic__ = (""0.3.3"",)\n'"
data_tests/saved__backend__/cython/type_hint_notemplate.py,0,"b'try:\n    import cython\nexcept ImportError:\n    from transonic_cl import cython\n\n\ndef compute(a, b, c, d, e):\n    print(e)\n    tmp = a + b\n    if 1 and 2:\n        tmp *= 2\n    return tmp\n\n\n__transonic__ = (""0.3.3"",)\n'"
data_tests/saved__backend__/numba/add_inline.py,0,"b'# __protected__ from numba import njit\n# __protected__ @njit\n\n\ndef add(a, b):\n    return a + b\n\n\n# __protected__ @njit\n\n\ndef use_add(n=10000):\n    tmp = 0\n    for _ in range(n):\n        tmp = add(tmp, 1)\n    return tmp\n\n\n__transonic__ = (""0.3.2"",)\n'"
data_tests/saved__backend__/numba/assign_func_boost.py,0,"b'# __protected__ from numba import njit\n# __protected__ @njit\n\n\ndef func(x):\n    return x ** 2\n\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/numba/block_fluidsim.py,0,"b'# __protected__ from numba import njit\n# __protected__ @njit\n\n\ndef rk2_step0(state_spect_n12, state_spect, tendencies_n, diss2, dt):\n    # transonic block (\n    #     complex128[][][] state_spect_n12, state_spect,\n    #                      tendencies_n;\n    #     float64[][] diss2;\n    #     float dt\n    # )\n    state_spect_n12[:] = (state_spect + ((dt / 2) * tendencies_n)) * diss2\n\n\narguments_blocks = {\n    ""rk2_step0"": [""state_spect_n12"", ""state_spect"", ""tendencies_n"", ""diss2"", ""dt""]\n}\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/numba/blocks_type_hints.py,0,"b'# __protected__ from numba import njit\n# __protected__ @njit\n\n\ndef block0(a, b, n):\n    # transonic block (\n    #     A a; A1 b;\n    #     int n\n    # )\n    # transonic block (\n    #     int[:] a, b;\n    #     float n\n    # )\n    result = ((a ** 2) + (b.mean() ** 3)) + n\n    return result\n\n\narguments_blocks = {""block0"": [""a"", ""b"", ""n""]}\n__transonic__ = (""0.4.2"",)\n'"
data_tests/saved__backend__/numba/boosted_class_use_import.py,1,"b'# __protected__ from numba import njit\nimport numpy as np\nfrom exterior_import_boost import func_import\n\n# __protected__ @njit\n\n\ndef __for_method__MyClass2__myfunc(self_attr0, self_attr1, arg):\n    return ((self_attr1 + self_attr0) + np.abs(arg)) + func_import()\n\n\n__code_new_method__MyClass2__myfunc = ""\\n\\ndef new_method(self, arg):\\n    return backend_func(self.attr0, self.attr1, arg)\\n\\n""\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/numba/boosted_func_use_import.py,1,"b'# __protected__ from numba import njit\nimport numpy as np\nfrom exterior_import_boost import func_import\n\n# __protected__ @njit\n\n\ndef func(a, b):\n    return (a * np.log(b)).max() + func_import()\n\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/numba/class_blocks.py,2,"b'# __protected__ from numba import njit\nimport numpy as np\n\n# __protected__ @njit\n\n\ndef block0(a, b, n):\n    # foo\n    # transonic block (\n    #     float[][] a, b;\n    #     int n\n    # ) bar\n    # foo\n    # transonic block (\n    #     float[][][] a, b;\n    #     int n\n    # )\n    # foobar\n    result = np.zeros_like(a)\n    for _ in range(n):\n        result += (a ** 2) + (b ** 3)\n    return result\n\n\n# __protected__ @njit\n\n\ndef block1(a, b, n):\n    # transonic block (\n    #     float[][] a, b;\n    #     int n\n    # )\n    # transonic block (\n    #     float[][][] a, b;\n    #     int n\n    # )\n    result = np.zeros_like(a)\n    for _ in range(n):\n        result += (a ** 2) + (b ** 3)\n    return result\n\n\narguments_blocks = {""block0"": [""a"", ""b"", ""n""], ""block1"": [""a"", ""b"", ""n""]}\n__transonic__ = (""0.4.2"",)\n'"
data_tests/saved__backend__/numba/class_rec_calls.py,0,"b'# __protected__ from numba import njit\n# __protected__ @njit\n\n\ndef __for_method__Myclass__func(self_attr, self_attr2, arg):\n    if __for_method__Myclass__func(self_attr, self_attr2, (arg - 1)) < 1:\n        return 1\n    else:\n        a = __for_method__Myclass__func(\n            self_attr, self_attr2, (arg - 1)\n        ) * __for_method__Myclass__func(self_attr, self_attr2, (arg - 1))\n        return (\n            a + ((self_attr * self_attr2) * arg)\n        ) + __for_method__Myclass__func(self_attr, self_attr2, (arg - 1))\n\n\n__code_new_method__Myclass__func = ""\\n\\ndef new_method(self, arg):\\n    return backend_func(self.attr, self.attr2, arg)\\n\\n""\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/numba/classic.py,1,"b'# __protected__ from numba import njit\nimport numpy as np\n\n# __protected__ @njit\n\n\ndef func(a, b):\n    return (a * np.log(b)).max()\n\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/numba/default_params.py,0,"b'# __protected__ from numba import njit\n# __protected__ @njit\n\n\ndef func(a=1, b=None, c=1.0):\n    print(b)\n    return a + c\n\n\n__transonic__ = (""0.4.0"",)\n'"
data_tests/saved__backend__/numba/methods.py,1,"b'# __protected__ from numba import njit\nimport numpy as np\n\n# __protected__ @njit\n\n\ndef __for_method__Transmitter____call__(self_arr, self_freq, inp):\n    ""My docstring""\n    return ((inp * np.exp(((np.arange(len(inp)) * self_freq) * 1j))), self_arr)\n\n\n__code_new_method__Transmitter____call__ = ""\\n\\ndef new_method(self, inp):\\n    return backend_func(self.arr, self.freq, inp)\\n\\n""\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/numba/mixed_classic_type_hint.py,2,"b'# __protected__ from numba import njit\nimport numpy as np\n\n# __protected__ @njit\n\n\ndef func(a, b):\n    return (a * np.log(b)).max()\n\n\n# __protected__ @njit\n\n\ndef func1(a, b):\n    return a * np.cos(b)\n\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/numba/no_arg.py,0,"b'# __protected__ from numba import njit\n# __protected__ @njit\n\n\ndef func():\n    return 1\n\n\n# __protected__ @njit\n\n\ndef func2():\n    return 1\n\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/numba/row_sum_boost.py,1,"b'# __protected__ from numba import njit\nimport numpy as np\n\n# __protected__ @njit\n\n\ndef row_sum(arr, columns):\n    return arr.T[columns].sum(0)\n\n\n# __protected__ @njit\n\n\ndef row_sum_loops(arr, columns):\n    # locals type annotations are used only for Cython\n    # arr.dtype not supported for memoryview\n    dtype = type(arr[(0, 0)])\n    res = np.empty(arr.shape[0], dtype=dtype)\n    for i in range(arr.shape[0]):\n        sum_ = dtype(0)\n        for j in range(columns.shape[0]):\n            sum_ += arr[(i, columns[j])]\n        res[i] = sum_\n    return res\n\n\n__transonic__ = (""0.3.3.post0"",)\n'"
data_tests/saved__backend__/numba/type_hint_notemplate.py,0,"b'# __protected__ from numba import njit\n# __protected__ @njit\n\n\ndef compute(a, b, c, d, e):\n    print(e)\n    tmp = a + b\n    if 1 and 2:\n        tmp *= 2\n    return tmp\n\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/python/add_inline.py,0,"b'def add(a, b):\n    return a + b\n\n\ndef use_add(n=10000):\n    tmp = 0\n    for _ in range(n):\n        tmp = add(tmp, 1)\n    return tmp\n\n\n__transonic__ = (""0.3.2"",)\n'"
data_tests/saved__backend__/python/assign_func_boost.py,0,"b'def func(x):\n    return x ** 2\n\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/python/block_fluidsim.py,0,"b'def rk2_step0(state_spect_n12, state_spect, tendencies_n, diss2, dt):\n\n    # transonic block (\n    #     complex128[][][] state_spect_n12, state_spect,\n    #                      tendencies_n;\n    #     float64[][] diss2;\n    #     float dt\n    # )\n    state_spect_n12[:] = (state_spect + ((dt / 2) * tendencies_n)) * diss2\n\n\narguments_blocks = {\n    ""rk2_step0"": [""state_spect_n12"", ""state_spect"", ""tendencies_n"", ""diss2"", ""dt""]\n}\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/python/blocks_type_hints.py,0,"b'def block0(a, b, n):\n\n    # transonic block (\n    #     A a; A1 b;\n    #     int n\n    # )\n    # transonic block (\n    #     int[:] a, b;\n    #     float n\n    # )\n    result = ((a ** 2) + (b.mean() ** 3)) + n\n\n    return result\n\n\narguments_blocks = {""block0"": [""a"", ""b"", ""n""]}\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/python/boosted_class_use_import.py,1,"b'import numpy as np\nfrom exterior_import_boost import func_import\n\n\ndef __for_method__MyClass2__myfunc(self_attr0, self_attr1, arg):\n    return ((self_attr1 + self_attr0) + np.abs(arg)) + func_import()\n\n\n__code_new_method__MyClass2__myfunc = """"""\n\ndef new_method(self, arg):\n    return backend_func(self.attr0, self.attr1, arg)\n\n""""""\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/python/boosted_func_use_import.py,1,"b'import numpy as np\nfrom exterior_import_boost import func_import\n\n\ndef func(a, b):\n    return (a * np.log(b)).max() + func_import()\n\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/python/class_blocks.py,2,"b'import numpy as np\n\n\ndef block0(a, b, n):\n\n    # foo\n    # transonic block (\n    #     float[][] a, b;\n    #     int n\n    # ) bar\n    # foo\n    # transonic block (\n    #     float[][][] a, b;\n    #     int n\n    # )\n    # foobar\n    result = np.zeros_like(a)\n    for _ in range(n):\n        result += (a ** 2) + (b ** 3)\n\n    return result\n\n\ndef block1(a, b, n):\n\n    # transonic block (\n    #     float[][] a, b;\n    #     int n\n    # )\n    # transonic block (\n    #     float[][][] a, b;\n    #     int n\n    # )\n    result = np.zeros_like(a)\n    for _ in range(n):\n        result += (a ** 2) + (b ** 3)\n\n    return result\n\n\narguments_blocks = {""block0"": [""a"", ""b"", ""n""], ""block1"": [""a"", ""b"", ""n""]}\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/python/class_rec_calls.py,0,"b'def __for_method__Myclass__func(self_attr, self_attr2, arg):\n    if __for_method__Myclass__func(self_attr, self_attr2, (arg - 1)) < 1:\n        return 1\n    else:\n        a = __for_method__Myclass__func(\n            self_attr, self_attr2, (arg - 1)\n        ) * __for_method__Myclass__func(self_attr, self_attr2, (arg - 1))\n        return (\n            a + ((self_attr * self_attr2) * arg)\n        ) + __for_method__Myclass__func(self_attr, self_attr2, (arg - 1))\n\n\n__code_new_method__Myclass__func = """"""\n\ndef new_method(self, arg):\n    return backend_func(self.attr, self.attr2, arg)\n\n""""""\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/python/classic.py,1,"b'import numpy as np\n\n\ndef func(a, b):\n    return (a * np.log(b)).max()\n\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/python/default_params.py,0,"b'def func(a=1, b=None, c=1.0):\n    print(b)\n    return a + c\n\n\n__transonic__ = (""0.4.0"",)\n'"
data_tests/saved__backend__/python/methods.py,1,"b'import numpy as np\n\n\ndef __for_method__Transmitter____call__(self_arr, self_freq, inp):\n    ""My docstring""\n    return ((inp * np.exp(((np.arange(len(inp)) * self_freq) * 1j))), self_arr)\n\n\n__code_new_method__Transmitter____call__ = """"""\n\ndef new_method(self, inp):\n    return backend_func(self.arr, self.freq, inp)\n\n""""""\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/python/mixed_classic_type_hint.py,2,"b'import numpy as np\n\n\ndef func(a, b):\n    return (a * np.log(b)).max()\n\n\ndef func1(a, b):\n    return a * np.cos(b)\n\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/python/no_arg.py,0,"b'def func():\n    return 1\n\n\ndef func2():\n    return 1\n\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/python/row_sum_boost.py,1,"b'import numpy as np\n\n\ndef row_sum(arr, columns):\n    return arr.T[columns].sum(0)\n\n\ndef row_sum_loops(arr, columns):\n    # locals type annotations are used only for Cython\n    # arr.dtype not supported for memoryview\n    dtype = type(arr[(0, 0)])\n    res = np.empty(arr.shape[0], dtype=dtype)\n    for i in range(arr.shape[0]):\n        sum_ = dtype(0)\n        for j in range(columns.shape[0]):\n            sum_ += arr[(i, columns[j])]\n        res[i] = sum_\n    return res\n\n\n__transonic__ = (""0.3.3.post0"",)\n'"
data_tests/saved__backend__/python/type_hint_notemplate.py,0,"b'def compute(a, b, c, d, e):\n    print(e)\n    tmp = a + b\n    if 1 and 2:\n        tmp *= 2\n    return tmp\n\n\n__transonic__ = (""0.3.0.post0"",)\n'"
data_tests/saved__backend__/pythran/add_inline.py,0,"b'def add(a, b):\n    return a + b\n\n\ndef use_add(n=10000):\n    tmp = 0\n    for _ in range(n):\n        tmp = add(tmp, 1)\n    return tmp\n\n\n__transonic__ = (""0.3.2"",)\n'"
data_tests/saved__backend__/pythran/assign_func_boost.py,0,"b'def func(x):\n    return x ** 2\n\n\n__transonic__ = (""0.3.0"",)\n'"
data_tests/saved__backend__/pythran/block_fluidsim.py,0,"b'def rk2_step0(state_spect_n12, state_spect, tendencies_n, diss2, dt):\n\n    # transonic block (\n    #     complex128[][][] state_spect_n12, state_spect,\n    #                      tendencies_n;\n    #     float64[][] diss2;\n    #     float dt\n    # )\n    state_spect_n12[:] = (state_spect + ((dt / 2) * tendencies_n)) * diss2\n\n\narguments_blocks = {\n    ""rk2_step0"": [""state_spect_n12"", ""state_spect"", ""tendencies_n"", ""diss2"", ""dt""]\n}\n\n__transonic__ = (""0.3.0"",)\n'"
data_tests/saved__backend__/pythran/blocks_type_hints.py,0,"b'def block0(a, b, n):\n\n    # transonic block (\n    #     A a; A1 b;\n    #     int n\n    # )\n    # transonic block (\n    #     int[:] a, b;\n    #     float n\n    # )\n    result = ((a ** 2) + (b.mean() ** 3)) + n\n\n    return result\n\n\narguments_blocks = {""block0"": [""a"", ""b"", ""n""]}\n\n__transonic__ = (""0.3.0"",)\n'"
data_tests/saved__backend__/pythran/boosted_class_use_import.py,1,"b'import numpy as np\nfrom __ext__MyClass2__exterior_import_boost import func_import\n\n\ndef __for_method__MyClass2__myfunc(self_attr0, self_attr1, arg):\n    return ((self_attr1 + self_attr0) + np.abs(arg)) + func_import()\n\n\n__code_new_method__MyClass2__myfunc = """"""\n\ndef new_method(self, arg):\n    return backend_func(self.attr0, self.attr1, arg)\n\n""""""\n\n__transonic__ = (""0.3.0"",)\n'"
data_tests/saved__backend__/pythran/boosted_func_use_import.py,1,"b'import numpy as np\nfrom __ext__func__exterior_import_boost import func_import\n\n\ndef func(a, b):\n    return (a * np.log(b)).max() + func_import()\n\n\n__transonic__ = (""0.3.0"",)\n'"
data_tests/saved__backend__/pythran/class_blocks.py,2,"b'import numpy as np\n\n\ndef block0(a, b, n):\n\n    # foo\n    # transonic block (\n    #     float[][] a, b;\n    #     int n\n    # ) bar\n    # foo\n    # transonic block (\n    #     float[][][] a, b;\n    #     int n\n    # )\n    # foobar\n    result = np.zeros_like(a)\n    for _ in range(n):\n        result += (a ** 2) + (b ** 3)\n\n    return result\n\n\ndef block1(a, b, n):\n\n    # transonic block (\n    #     float[][] a, b;\n    #     int n\n    # )\n    # transonic block (\n    #     float[][][] a, b;\n    #     int n\n    # )\n    result = np.zeros_like(a)\n    for _ in range(n):\n        result += (a ** 2) + (b ** 3)\n\n    return result\n\n\narguments_blocks = {""block0"": [""a"", ""b"", ""n""], ""block1"": [""a"", ""b"", ""n""]}\n\n__transonic__ = (""0.3.0"",)\n'"
data_tests/saved__backend__/pythran/class_rec_calls.py,0,"b'def __for_method__Myclass__func(self_attr, self_attr2, arg):\n    if __for_method__Myclass__func(self_attr, self_attr2, (arg - 1)) < 1:\n        return 1\n    else:\n        a = __for_method__Myclass__func(\n            self_attr, self_attr2, (arg - 1)\n        ) * __for_method__Myclass__func(self_attr, self_attr2, (arg - 1))\n        return (\n            a + ((self_attr * self_attr2) * arg)\n        ) + __for_method__Myclass__func(self_attr, self_attr2, (arg - 1))\n\n\n__code_new_method__Myclass__func = """"""\n\ndef new_method(self, arg):\n    return backend_func(self.attr, self.attr2, arg)\n\n""""""\n\n__transonic__ = (""0.3.0"",)\n'"
data_tests/saved__backend__/pythran/classic.py,1,"b'import numpy as np\n\n\ndef func(a, b):\n    return (a * np.log(b)).max()\n\n\n__transonic__ = (""0.3.0"",)\n'"
data_tests/saved__backend__/pythran/default_params.py,0,"b'def func(a=1, b=None, c=1.0):\n    print(b)\n    return a + c\n\n\n__transonic__ = (""0.4.0"",)\n'"
data_tests/saved__backend__/pythran/methods.py,1,"b'import numpy as np\n\n\ndef __for_method__Transmitter____call__(self_arr, self_freq, inp):\n    ""My docstring""\n    return ((inp * np.exp(((np.arange(len(inp)) * self_freq) * 1j))), self_arr)\n\n\n__code_new_method__Transmitter____call__ = """"""\n\ndef new_method(self, inp):\n    return backend_func(self.arr, self.freq, inp)\n\n""""""\n\n__transonic__ = (""0.3.0"",)\n'"
data_tests/saved__backend__/pythran/mixed_classic_type_hint.py,2,"b'import numpy as np\n\n\ndef func(a, b):\n    return (a * np.log(b)).max()\n\n\ndef func1(a, b):\n    return a * np.cos(b)\n\n\n__transonic__ = (""0.3.0"",)\n'"
data_tests/saved__backend__/pythran/no_arg.py,0,"b'def func():\n    return 1\n\n\ndef func2():\n    return 1\n\n\n__transonic__ = (""0.3.0"",)\n'"
data_tests/saved__backend__/pythran/row_sum_boost.py,1,"b'import numpy as np\n\n\ndef row_sum(arr, columns):\n    return arr.T[columns].sum(0)\n\n\ndef row_sum_loops(arr, columns):\n    # locals type annotations are used only for Cython\n    # arr.dtype not supported for memoryview\n    dtype = type(arr[(0, 0)])\n    res = np.empty(arr.shape[0], dtype=dtype)\n    for i in range(arr.shape[0]):\n        sum_ = dtype(0)\n        for j in range(columns.shape[0]):\n            sum_ += arr[(i, columns[j])]\n        res[i] = sum_\n    return res\n\n\n__transonic__ = (""0.3.3.post0"",)\n'"
data_tests/saved__backend__/pythran/type_hint_notemplate.py,0,"b'def compute(a, b, c, d, e):\n    print(e)\n    tmp = a + b\n    if 1 and 2:\n        tmp *= 2\n    return tmp\n\n\n__transonic__ = (""0.3.0"",)\n'"
doc/examples/bench_expr_broadcast/bench.py,6,"b'import numpy as np\n\nfrom transonic import boost, Array\n\nA = Array[float, ""3d""]\nA1 = Array[float, ""1d""]\n\n\ndef expr(a, b):\n    return np.arctan2(2 * np.exp(a) ** 2 + 4 * np.log(a * b) ** 3, 2 / a)\n\n\ndef broadcast(a: A, b: A1, out: A):\n    out[:] = expr(a, b)\n\n\ndef broadcast_loops(a: A, b: A1, out: A):\n    n0, n1, n2 = a.shape\n    for i0 in range(n0):\n        for i1 in range(n1):\n            for i2 in range(n2):\n                out[i0, i1, i2] = expr(a[i0, i1, i2], b[i2])\n\n\nbroadcast_pythran = boost(backend=""pythran"")(broadcast)\nbroadcast_numba = boost(backend=""numba"")(broadcast)\n\nbroadcast_loops_pythran = boost(backend=""pythran"")(broadcast_loops)\nbroadcast_loops_numba = boost(backend=""numba"")(broadcast_loops)\n\n\nif __name__ == ""__main__"":\n\n    from transonic.util import print_versions, timeit_verbose\n\n    print_versions()\n\n    shape = (4, 4, 64)\n    a = np.linspace(1, 100, np.prod(shape)).reshape(shape)\n    b = np.linspace(1, 100, shape[-1])\n    out = np.empty_like(a)\n    loc = locals()\n\n    broadcast(a, b, out)\n    out_loops = np.empty_like(a)\n    broadcast_loops(a, b, out_loops)\n    assert np.allclose(out, out_loops)\n\n    print()\n    norm = timeit_verbose(""broadcast(a, b, out)"", globals=loc)\n\n    for backend in (""numba"", ""pythran""):\n        timeit_verbose(f""broadcast_{backend}(a, b, out)"", globals=loc, norm=norm)\n        timeit_verbose(f""broadcast_loops_{backend}(a, b, out)"", globals=loc, norm=norm)\n'"
doc/examples/bench_proj_perp/bench.py,11,"b'import numpy as np\nfrom transonic import boost, Array, Type\n\nA = Array[Type(np.float64, np.complex128), ""3d""]\nAf = ""float[:,:,:]""\nA = Af  # issue fused type with Cython\n\n\ndef proj(vx: A, vy: A, vz: A, kx: Af, ky: Af, kz: Af, inv_k_square_nozero: Af):\n    tmp = (kx * vx + ky * vy + kz * vz) * inv_k_square_nozero\n    vx -= kx * tmp\n    vy -= ky * tmp\n    vz -= kz * tmp\n\n\ndef proj_loop(\n    vx: A, vy: A, vz: A, kx: Af, ky: Af, kz: Af, inv_k_square_nozero: Af\n):\n\n    # type annotations only useful for Cython\n    n0: int\n    n1: int\n    n2: int\n    i0: int\n    i1: int\n    i2: int\n    tmp: float\n\n    n0, n1, n2 = kx.shape[0], kx.shape[1], kx.shape[2]\n\n    for i0 in range(n0):\n        for i1 in range(n1):\n            for i2 in range(n2):\n                tmp = (\n                    kx[i0, i1, i2] * vx[i0, i1, i2]\n                    + ky[i0, i1, i2] * vy[i0, i1, i2]\n                    + kz[i0, i1, i2] * vz[i0, i1, i2]\n                ) * inv_k_square_nozero[i0, i1, i2]\n\n                vx[i0, i1, i2] -= kx[i0, i1, i2] * tmp\n                vy[i0, i1, i2] -= ky[i0, i1, i2] * tmp\n                vz[i0, i1, i2] -= kz[i0, i1, i2] * tmp\n\n\nproj_pythran = boost(backend=""pythran"")(proj)\nproj_numba = boost(backend=""numba"")(proj)\nproj_cython = boost(backend=""cython"")(proj)\n\nproj_loop_pythran = boost(backend=""pythran"")(proj_loop)\nproj_loop_numba = boost(backend=""numba"")(proj_loop)\nproj_loop_cython = boost(backend=""cython"", boundscheck=False, wraparound=False)(\n    proj_loop\n)\n\n\nif __name__ == ""__main__"":\n    from textwrap import dedent\n\n    from transonic.util import print_versions, timeit_verbose\n\n    loc = locals()\n\n    print_versions()\n\n    setup = dedent(\n        """"""\n        shape = n0, n1, n2 = 64, 512, 512\n        k0 = np.linspace(0, 100, n0)\n        k1 = np.linspace(0, 100, n1)\n        k2 = np.linspace(0, 100, n2)\n        K1, K0, K2 = np.meshgrid(k1, k0, k2, copy=False)\n        kz = np.ascontiguousarray(K0)\n        ky = np.ascontiguousarray(K1)\n        kx = np.ascontiguousarray(K2)\n\n        k_square_nozero = K0 ** 2 + K1 ** 2 + K2 ** 2\n        k_square_nozero[0, 0, 0] = 1e-14\n        inv_k_square_nozero = 1.0 / k_square_nozero\n\n        vx = np.ones(shape)\n        vy = np.ones(shape)\n        vz = np.ones(shape)\n    """"""\n    )\n\n    print()\n    norm = timeit_verbose(\n        ""proj(vx, vy, vz, kx, ky, kz, inv_k_square_nozero)"",\n        setup=setup,\n        globals=loc,\n    )\n\n    for backend in (""cython"", ""numba"", ""pythran""):\n        timeit_verbose(\n            f""proj_{backend}(vx, vy, vz, kx, ky, kz, inv_k_square_nozero)"",\n            setup=setup,\n            globals=loc,\n            norm=norm,\n        )\n        timeit_verbose(\n            f""proj_loop_{backend}(vx, vy, vz, kx, ky, kz, inv_k_square_nozero)"",\n            setup=setup,\n            globals=loc,\n            norm=norm,\n        )\n'"
doc/examples/bench_row_sum/bench.py,0,"b'from subprocess import getoutput\nimport sys\n\ndecorator = ""boost""\nif ""jit"" in sys.argv:\n    decorator = ""jit""\n\nprint(f""With decorator {decorator}:"")\n\n\ndef get_times(backend):\n    output = getoutput(\n        f""TRANSONIC_BACKEND=\'{backend}\' python row_sum_{decorator}.py""\n    )\n    lines = output.split(""\\n"")\n    index_backend = 1\n    if decorator == ""jit"":\n        index_backend = 2\n    assert lines[index_backend] == backend.capitalize()\n    time_high = float(lines[index_backend + 1].split()[1])\n    time_low = float(lines[index_backend + 2].split()[1])\n    return time_high, time_low\n\n\nbackend = ""python""\ntime_high, time_low = get_times(backend)\n\ntime_ref = time_high\n\n\ndef print_result(backend, time_high, time_low):\n    print(backend.capitalize())\n    print(f""high level: {time_high:.2e} s  (= {time_high/time_ref:5.2f} * norm)"")\n    print(f""low level:  {time_low:.2e} s  (= {time_low/time_ref:5.2f} * norm)\\n"")\n\n\nprint_result(backend, time_high, time_low)\n\nfor backend in (""cython"", ""numba"", ""pythran""):\n    time_high, time_low = get_times(backend)\n    print_result(backend, time_high, time_low)\n'"
doc/examples/bench_row_sum/row_sum_boost.py,5,"b'import numpy as np\n\nfrom transonic import boost, Array, const\n\nT_index = np.int32\n# we use a type variable because it can be replaced by a fused type.\nT = np.int64\nA1d_i = Array[T_index, ""1d""]\nA1d = Array[T, ""1d""]\nA2d = Array[T, ""2d""]\nV1d_i = Array[T_index, ""1d"", ""memview""]\nV1d = Array[T, ""1d"", ""memview""]\nV2d = Array[T, ""2d"", ""memview""]\n\n\n@boost\ndef row_sum(arr: A2d, columns: A1d_i):\n    return arr.T[columns].sum(0)\n\n\n@boost(boundscheck=False, wraparound=False)\ndef row_sum_loops(arr: const(V2d), columns: const(V1d_i)):\n    # locals type annotations are used only for Cython\n    i: T_index\n    j: T_index\n    sum_: T\n    # arr.dtype not supported for memoryview\n    dtype = type(arr[0, 0])\n    res: V1d = np.empty(arr.shape[0], dtype=dtype)\n    for i in range(arr.shape[0]):\n        sum_ = dtype(0)\n        for j in range(columns.shape[0]):\n            sum_ += arr[i, columns[j]]\n        res[i] = sum_\n    return res\n\n\nif __name__ == ""__main__"":\n\n    from util import check, bench\n\n    functions = [row_sum, row_sum_loops]\n    arr = np.arange(1_000_000).reshape(1_000, 1_000)\n    columns = np.arange(1, 1000, 2, dtype=T_index)\n\n    check(functions, arr, columns)\n    bench(functions, arr, columns)\n'"
doc/examples/bench_row_sum/row_sum_jit.py,3,"b'import numpy as np\n\nfrom transonic import jit\n\n\n@jit(native=True, xsimd=True)\ndef row_sum(arr, columns):\n    return arr.T[columns].sum(0)\n\n\n@jit(native=True, xsimd=True)\ndef row_sum_loops(arr, columns):\n    res = np.empty(arr.shape[0], dtype=arr.dtype)\n    for i in range(arr.shape[0]):\n        sum_ = 0\n        for j in range(columns.shape[0]):\n            sum_ += arr[i, columns[j]]\n        res[i] = sum_\n    return res\n\n\nif __name__ == ""__main__"":\n\n    from transonic import wait_for_all_extensions\n\n    from util import check, bench\n\n    functions = [row_sum, row_sum_loops]\n    arr = np.arange(1_000_000).reshape(1_000, 1_000)\n    columns = np.arange(1, 1000, 2)\n\n    check(functions, arr, columns)\n    wait_for_all_extensions()\n    check(functions, arr, columns)\n    bench(functions, arr, columns)\n'"
doc/examples/bench_row_sum/util.py,1,"b'import numpy as np\n\nfrom transonic.util import timeit\nfrom transonic.config import backend_default\n\n\ndef check(functions, arr, columns):\n    res0 = functions[0](arr, columns)\n    for func in functions[1:]:\n        assert np.allclose(res0, func(arr, columns))\n    print(""Checks passed: results are consistent"")\n\n\ndef bench(functions, arr, columns):\n    print(backend_default.capitalize())\n    for func in functions:\n        result = timeit(""func(arr, columns)"", globals=locals())\n        print(f""{func.__name__:20s} {result:.3e} s"")\n    print()\n'"
doc/examples/bench_trigo/bench.py,8,"b'import numpy as np\n\nfrom transonic import boost, Array\n\nA = Array[float, ""1d""]\n\n\ndef fxfy(ft: A, fn: A, theta: A):\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n    fx = cos_theta * ft - sin_theta * fn\n    fy = sin_theta * ft + cos_theta * fn\n    return fx, fy\n\n\ndef fxfy_loops(ft: A, fn: A, theta: A):\n    n0 = theta.size\n    fx = np.empty_like(ft)\n    fy = np.empty_like(fn)\n    for index in range(n0):\n        sin_theta = np.sin(theta[index])\n        cos_theta = np.cos(theta[index])\n        fx[index] = cos_theta * ft[index] - sin_theta * fn[index]\n        fy[index] = sin_theta * ft[index] + cos_theta * fn[index]\n    return fx, fy\n\n\nfxfy_pythran = boost(backend=""pythran"")(fxfy)\nfxfy_numba = boost(backend=""numba"")(fxfy)\n\nfxfy_loops_pythran = boost(backend=""pythran"")(fxfy_loops)\nfxfy_loops_numba = boost(backend=""numba"")(fxfy_loops)\n\n\nif __name__ == ""__main__"":\n\n    from transonic.util import print_versions, timeit_verbose\n\n    print_versions()\n\n    theta = np.linspace(0, 2 * np.pi, 10000)\n    ft = 2.5 * theta\n    fv = 1.5 * theta\n    loc = locals()\n\n    out = fxfy(ft, fv, theta)\n    out_loops = fxfy_loops(ft, fv, theta)\n    assert np.allclose(out, out_loops)\n\n    print()\n    norm = timeit_verbose(""fxfy(ft, fv, theta)"", globals=loc)\n\n    for backend in (""numba"", ""pythran""):\n        timeit_verbose(f""fxfy_{backend}(ft, fv, theta)"", globals=loc, norm=norm)\n        timeit_verbose(\n            f""fxfy_loops_{backend}(ft, fv, theta)"", globals=loc, norm=norm\n        )\n'"
doc/examples/import_local_module/mod0.py,1,"b'import numpy as np\n\nfrom transonic import jit, boost\n\nfrom mod1 import func_import\n\n@boost\nclass MyClass2:\n    def __init__(self):\n        self.attr0 = self.attr1 = 1\n\n    @jit()\n    def myfunc(self, arg):\n        return self.attr1 + self.attr0 + np.abs(arg) + func_import()\n\n    def check(self):\n        assert self.myfunc(1) == 4\n'"
doc/examples/import_local_module/mod1.py,1,b'\nimport numpy as np\n\nconst = 1\n\n\ndef func_import():\n    return const + np.pi - np.pi\n'
doc/examples/inlined/add.py,0,"b'from transonic import boost\n\nT = int\n\n\n@boost(inline=True)\ndef add(a: T, b: T) -> T:\n    return a + b\n\n\n@boost\ndef use_add(n: int = 10000):\n    tmp: T = 0\n    _: T\n    for _ in range(n):\n        tmp = add(tmp, 1)\n    return tmp\n\n\nif __name__ == ""__main__"":\n\n    from transonic.util import timeit\n\n    n = 100000\n    assert n == use_add(n)\n\n    print(f""{timeit(\'use_add(n)\', globals=locals()): .2e} s"")\n'"
doc/examples/inlined/bug_beniget.py,0,"b'\nimport gast as ast\nimport beniget\n\nmod = ast.parse(""""""\nT = int\ndef func() -> T:\n    return 1\n"""""")\n\nfdef = mod.body[1]\nnode = fdef.returns\n\ndu = beniget.DefUseChains()\ndu.visit(mod)\n\ndu.chains[node]\n\nud = beniget.UseDefChains(du)\n\nud.chains[node]\n'"
doc/examples/inlined/try_analysis.py,0,b'from transonic import boost\n\n\n@boost\ndef f():\n    pass\n\n\n@boost(inline=1)\ndef f():\n    pass\n\n\ndef f1():\n    pass\n\n\ndef f2():\n    pass\n\n\nf1_ = boost(f1)\nf1_ = boost(inline=1)(f1)\n\n\n# decor = boost(inline=1)\n# f1_ = decor(f1)\n# f2_ = decor(f2)\n'
doc/examples/issues/issue29_blocks_classes.py,0,"b'""""""\nSee https://foss.heptapod.net/fluiddyn/transonic/issues/6\n\n""""""\nfrom transonic import boost, with_blocks, block\n\n\ndef non_pythranizable(arg):\n    """"""represent a function that can not be compiled by Pythran""""""\n    return arg\n\n\n@boost\nclass MyClass:\n\n    attr0: int\n\n    @with_blocks\n    def func(self, arg: int):\n\n        a = non_pythranizable(arg)\n\n        with block(a=float):\n            tmp = a + arg + self.attr0\n\n        return non_pythranizable(tmp)\n'"
doc/examples/issues/issue29_blocks_func.py,0,"b'""""""\nSee https://foss.heptapod.net/fluiddyn/transonic/issues/6\n\n""""""\nfrom transonic import with_blocks, block\n\n\ndef non_pythranizable(arg):\n    """"""represent a function that can not be compiled by Pythran""""""\n    return arg\n\n\n@with_blocks\ndef func0(arg: int):\n\n    a = non_pythranizable(arg)\n\n    with block(a=float):\n        return a + arg\n\n\n@with_blocks\ndef func1(arg: int):\n\n    a = non_pythranizable(arg)\n\n    with block(a=float):\n        tmp = a + arg\n\n    return non_pythranizable(tmp)\n'"
doc/examples/writing_benchmarks/bench_aot.py,5,"b'from transonic import boost, Array\nimport numba\n\nimport numpy as np\n\nImage = Array[np.float64, ""2d"", ""C""]\n\n\ndef laplace_numpy(image: Image):\n    """"""Laplace operator in NumPy for 2D images.""""""\n    laplacian = (\n        image[:-2, 1:-1]\n        + image[2:, 1:-1]\n        + image[1:-1, :-2]\n        + image[1:-1, 2:]\n        - 4 * image[1:-1, 1:-1]\n    )\n    thresh = np.abs(laplacian) > 0.05\n    return thresh\n\n\ndef laplace_loops(image: Image):\n    """"""Laplace operator for 2D images.""""""\n    h = image.shape[0]\n    w = image.shape[1]\n    laplacian = np.empty((h - 2, w - 2), np.uint8)\n    for i in range(1, h - 1):\n        for j in range(1, w - 1):\n            laplacian[i - 1, j - 1] = (\n                np.abs(\n                    image[i - 1, j]\n                    + image[i + 1, j]\n                    + image[i, j - 1]\n                    + image[i, j + 1]\n                    - 4 * image[i, j]\n                )\n                > 0.05\n            )\n    return laplacian\n\n\nlaplace_transonic_pythran = boost(backend=""pythran"")(laplace_numpy)\nlaplace_transonic_cython = boost(backend=""cython"")(laplace_numpy)\nlaplace_transonic_numba = boost(backend=""numba"")(laplace_numpy)\nlaplace_transonic_python = boost(backend=""python"")(laplace_numpy)\nlaplace_numba = numba.njit(laplace_numpy)\n\n\nlaplace_loops_transonic_pythran = boost(backend=""pythran"")(laplace_loops)\nlaplace_loops_transonic_python = boost(backend=""python"")(laplace_loops)\nlaplace_loops_transonic_numba = boost(backend=""numba"")(laplace_loops)\nlaplace_loops_numba = numba.njit(laplace_loops)\n\n\n# For Cython, we need to add more type annotations\n\n@boost(backend=""cython"", boundscheck=False, wraparound=False)\ndef laplace_loops_transonic_cython(image: Image):\n    """"""Laplace operator for 2D images.""""""\n    i: int\n    j: int\n    h: int = image.shape[0]\n    w: int = image.shape[1]\n    laplacian: Array[np.uint8, ""2d""] = np.empty((h - 2, w - 2), np.uint8)\n    for i in range(1, h - 1):\n        for j in range(1, w - 1):\n            laplacian[i - 1, j - 1] = (\n                abs(\n                    image[i - 1, j]\n                    + image[i + 1, j]\n                    + image[i, j - 1]\n                    + image[i, j + 1]\n                    - 4 * image[i, j]\n                )\n                > 0.05\n            )\n    return laplacian\n\n\nif __name__ == ""__main__"":\n\n    from skimage.data import astronaut\n    from skimage.color import rgb2gray\n\n    image = astronaut()\n    image = rgb2gray(image)\n\n    # call these functions to warm them\n    laplace_transonic_numba(image)\n    laplace_loops_transonic_numba(image)\n    laplace_numba(image)\n    laplace_loops_numba(image)\n\n    from transonic.util import timeit\n    from transonic import __version__\n    import pythran\n\n    loc = locals()\n\n    def bench(call, norm=None):\n        ret = result = timeit(call, globals=loc)\n        if norm is None:\n            norm = result\n        result /= norm\n        print(f""{call.split(\'(\')[0]:33s}: {result:.2f}"")\n        return ret\n\n    print(\n        f""transonic {__version__}\\n""\n        f""pythran {pythran.__version__}\\n""\n        f""numba {numba.__version__}\\n""\n    )\n\n    norm = bench(""laplace_transonic_pythran(image)"")\n    print(f""norm = {norm:.2e} s"")\n    bench(""laplace_loops_transonic_pythran(image)"", norm=norm)\n    bench(""laplace_transonic_cython(image)"", norm=norm)\n    bench(""laplace_loops_transonic_cython(image)"", norm=norm)\n    bench(""laplace_numba(image)"", norm=norm)\n    bench(""laplace_transonic_numba(image)"", norm=norm)\n    bench(""laplace_loops_numba(image)"", norm=norm)\n    bench(""laplace_loops_transonic_numba(image)"", norm=norm)\n    bench(""laplace_numpy(image)"", norm=norm)\n    bench(""laplace_transonic_python(image)"", norm=norm)\n'"
doc/examples/writing_benchmarks/bench_jit.py,0,"b'from transonic import jit\nimport numba\n\nfrom pure_numpy import laplace_numpy, laplace_loops\n\nlaplace_transonic_pythran = jit(native=True, xsimd=True)(laplace_numpy)\nlaplace_transonic_python = jit(backend=""python"")(laplace_numpy)\nlaplace_transonic_numba = jit(backend=""numba"")(laplace_numpy)\nlaplace_numba = numba.njit(laplace_numpy)\n\nlaplace_transonic_pythran_loops = jit(native=True, xsimd=True)(laplace_loops)\nlaplace_transonic_python_loops = jit(backend=""python"")(laplace_loops)\nlaplace_transonic_numba_loops = jit(backend=""numba"")(laplace_loops)\nlaplace_numba_loops = numba.njit(laplace_loops)\n\nif __name__ == ""__main__"":\n    from transonic import wait_for_all_extensions\n\n    from skimage.data import astronaut\n    from skimage.color import rgb2gray\n\n    image = astronaut()\n    image = rgb2gray(image)\n\n    # warm the functions\n    laplace_transonic_python(image)\n    laplace_transonic_pythran(image)\n    laplace_transonic_pythran_loops(image)\n    laplace_transonic_numba(image)\n    laplace_transonic_numba_loops(image)\n    laplace_numba(image)\n    laplace_numba_loops(image)\n\n    wait_for_all_extensions()\n\n    # again warming\n    laplace_transonic_numba(image)\n    laplace_transonic_numba_loops(image)\n\n    from transonic.util import timeit\n    from transonic import __version__\n    import pythran\n\n    loc = locals()\n\n    def bench(call, norm=None):\n        ret = result = timeit(call, globals=loc)\n        if norm is None:\n            norm = result\n        result /= norm\n        print(f""{call.split(\'(\')[0]:33s}: {result:.2f}"")\n        return ret\n\n    print(\n        f""transonic {__version__}\\n""\n        f""pythran {pythran.__version__}\\n""\n        f""numba {numba.__version__}\\n""\n    )\n\n    norm = bench(""laplace_transonic_pythran(image)"")\n    print(f""norm = {norm:.2e} s"")\n    bench(""laplace_transonic_pythran_loops(image)"", norm=norm)\n    bench(""laplace_numba(image)"", norm=norm)\n    bench(""laplace_transonic_numba(image)"", norm=norm)\n    bench(""laplace_numba_loops(image)"", norm=norm)\n    bench(""laplace_transonic_numba_loops(image)"", norm=norm)\n    bench(""laplace_numpy(image)"", norm=norm)\n    bench(""laplace_transonic_python(image)"", norm=norm)\n'"
doc/examples/writing_benchmarks/pure_numpy.py,3,"b'import numpy as np\n\n\ndef laplace_numpy(image):\n    """"""Laplace operator in NumPy for 2D images.""""""\n    laplacian = (\n        image[:-2, 1:-1]\n        + image[2:, 1:-1]\n        + image[1:-1, :-2]\n        + image[1:-1, 2:]\n        - 4 * image[1:-1, 1:-1]\n    )\n    thresh = np.abs(laplacian) > 0.05\n    return thresh\n\n\ndef laplace_loops(image):\n    """"""Laplace operator for 2D images.""""""\n    h = image.shape[0]\n    w = image.shape[1]\n    laplacian = np.empty((h - 2, w - 2), np.uint8)\n    for i in range(1, h - 1):\n        for j in range(1, w - 1):\n            laplacian[i - 1, j - 1] = (\n                np.abs(\n                    image[i - 1, j]\n                    + image[i + 1, j]\n                    + image[i, j - 1]\n                    + image[i, j + 1]\n                    - 4 * image[i, j]\n                )\n                > 0.05\n            )\n    return laplacian\n'"
doc/for_dev/scikit-image/bench_all.py,0,"b'from bench_util import bench_one, statements\n\nfor mod_name, func_name in statements.keys():\n    bench_one(mod_name)\n'"
doc/for_dev/scikit-image/bench_one.py,0,"b'import argparse\nfrom bench_util import bench_one\n\nparser = argparse.ArgumentParser(description=""Run one benchmark"")\n\nparser.add_argument(""module"", help=""Module name"")\n\nargs = parser.parse_args()\n\nbench_one(args.module)\n'"
doc/for_dev/scikit-image/bench_util.py,0,"b'from subprocess import getoutput\nfrom pathlib import Path\n\nfrom transonic.util import timeit\n\nstatements = {\n    (""cmorph"", ""_dilate""): ""_dilate(image, selem, out, shift_x, shift_y)"",\n    (\n        ""_greyreconstruct"",\n        ""reconstruction_loop"",\n    ): ""reconstruction_loop(ranks, prev, next_, strides, current_idx, image_stride)"",\n}\n\nimport_from_skimage = {\n    (\n        ""_greyreconstruct"",\n        ""reconstruction_loop"",\n    ): ""from skimage.morphology._greyreconstruct import reconstruction_loop""\n}\n\n\ndef bench_one(name_module=""cmorph"", func=None, total_duration=2):\n\n    if func is not None:\n        raise NotImplementedError\n\n    functions = [\n        (mod, func_) for (mod, func_) in statements.keys() if mod == name_module\n    ]\n\n    if not functions:\n        raise ValueError(f""bad name_module: {name_module}"")\n\n    name_function = functions[0][1]\n\n    print(f""module: {name_module}"")\n    stmt = statements[(name_module, name_function)]\n    print(stmt)\n\n    path_setup = Path(""setup_codes"") / f""{name_module}_{name_function}.py""\n\n    if not path_setup.exists():\n        raise RuntimeError\n\n    with open(path_setup) as file:\n        setup = file.read()\n\n    if (name_module, name_function) in import_from_skimage:\n        setup_from_skimage = setup.replace(\n            f""from future.{name_module} import {name_function}"",\n            import_from_skimage[(name_module, name_function)],\n        )\n        time = timeit(stmt, setup_from_skimage, total_duration=total_duration)\n        print(f""{\'from skimage\':18s} {time:.2e} s"")\n\n    setup_pyx = setup.replace(\n        f""from future.{name_module} import"", f""from pyx.{name_module} import""\n    )\n\n    code = f""""""\nfrom transonic.util import timeit\nsetup = \'\'\'{setup}\'\'\'\nstmt = \'\'\'{stmt}\'\'\'\nprint(timeit(stmt, setup, total_duration={total_duration}))\n    """"""\n\n    time_old = timeit(stmt, setup_pyx, total_duration=total_duration)\n\n    print(f""cython pyx skimage {time_old:.2e} s  (= norm)"")\n\n    with open(""tmp.py"", ""w"") as file:\n        file.write(code)\n\n    for backend in (""cython"", ""pythran"", ""numba""):\n        time = float(getoutput(f""TRANSONIC_BACKEND=\'{backend}\' python tmp.py""))\n        print(f""{backend:18s} {time:.2e} s  (= {time/time_old:.2f} * norm)"")\n\n    # print(getoutput(""TRANSONIC_NO_REPLACE=1 python tmp.py""))\n\n    if (name_module, name_function) not in import_from_skimage:\n        return\n\n    setup_from_skimage = setup.replace(\n        f""from future.{name_module} import {name_function}"",\n        import_from_skimage[(name_module, name_function)],\n    )\n    time = timeit(stmt, setup_from_skimage, total_duration=total_duration)\n\n    print(f""{\'from skimage\':18s} {time:.2e} s  (= {time/time_old:.2f} * norm)"")\n'"
doc/for_dev/scikit-image/setup.py,2,"b'import os\nimport sys\nfrom distutils.core import setup\nfrom pathlib import Path\n\nimport numpy as np\n\nfrom transonic.dist import make_backend_files, init_transonic_extensions\n\npath_here = Path(__file__).parent.absolute()\ninclude_dirs = [np.get_include()]\n\npack_name = ""future""\n\npaths = tuple((path_here / pack_name).glob(""*.py""))\n\nfor backend in (""pythran"", ""cython"", ""numba""):\n    make_backend_files(paths, backend=backend)\n\nextensions = []\nif ""egg_info"" not in sys.argv:\n    extensions = init_transonic_extensions(\n        pack_name,\n        backend=""pythran"",\n        include_dirs=[np.get_include()],\n        compile_args=(""-O3"", ""-DUSE_XSIMD""),\n        inplace=True,\n    )\n    extensions.extend(\n        init_transonic_extensions(\n            pack_name, backend=""cython"", inplace=True, annotate=True\n        )\n    )\n    init_transonic_extensions(pack_name, backend=""numba"")\n\n\nsetup(\n    name=pack_name,\n    ext_modules=extensions,\n    # script_name=""setup.py"",\n    script_args=[""build_ext"", ""--inplace""],\n    # cmdclass=dict(build_ext=ParallelBuildExt),\n)\n'"
doc/for_dev/scikit-image/setup_pyx.py,1,"b'from distutils.core import setup, Extension\nfrom pathlib import Path\n\nimport numpy as np\nfrom Cython.Build import cythonize\n\npath_here = Path(__file__).parent.absolute()\ninclude_dirs = [np.get_include()]\n\npyx_files = (path_here / ""pyx"").glob(""*.pyx"")\nextensions = []\nfor pyx_file in pyx_files:\n    name = pyx_file.name.split(""."", 1)[0]\n    extensions.append(\n        Extension(""pyx."" + name, [str(pyx_file)], include_dirs=include_dirs)\n    )\n\nextensions = cythonize(extensions, language_level=3, annotate=True)\n\nsetup(\n    name=""pyx"",\n    ext_modules=extensions,\n    script_args=[""build_ext"", ""--inplace""],\n)\n'"
tmp/analyses/examples/0_classic.py,2,"b'import numpy as np\n\nfrom transonic import boost\nimport transonic as ts\n\nmyconst0 = 0\nmyconst1 = 2*myconst0\n\n# transonic def func(\n#  float[][],\n#  float[][]\n# )\n# transonic def func(int[][], float[][])\n\n\n@boost\ndef func(a: int, b: int):\n    return myconst1 * (a * np.log(b)).max()\n\n\n# transonic def func1(int, float)\n\n\n@ts.boost\ndef func1(a, b):\n    return (a * np.cos(b)).max()\n\n\n@boost\nclass Foo:\n    @ts.boost\n    def meth_foo(self):\n        return func1(0, 1)\n\n\n@ts.boost\nclass Bar:\n    @boost\n    def meth_bar(self):\n        pass\n'"
tmp/analyses/examples/1_type_hint.py,4,"b'\nimport transonic as ts\nfrom transonic import Type, NDim, Array, Union\n\nimport numpy as np\nimport skimage\n\nT = Type(int, np.complex128)\n\ndim = 2\ndim += 1\n\nN = NDim(1, dim)\n\nA = Array[T, N]\nA1 = Array[np.float32, N + 1]\n\nA3d = Array[np.float32, ""3d""]\nN1 = NDim(4, 5)\nN1 = NDim(4, 5)\n\nT = Type(int, np.complex128)\n\na_type_var = ""hello""\nmyconst = 0\n\ncdict = skimage.color.color_dict\n\n@ts.boost\ndef compute(a: A, b: A, c: T, d: Union[A, A1], e: str):\n    print(e)\n    tmp = a + b + myconst\n    return tmp\n'"
tmp/analyses/examples/2_mixed_classic_type_hint.py,2,"b'import numpy as np\n\nfrom transonic import boost\n\n# transonic def func(float[][], float[][])\n# transonic def func(int[][], float[][])\n\n\n@boost\ndef func(a: float, b: float):\n    return (a * np.log(b)).max()\n\n\n@boost\ndef func1(a: int, b: float):\n    return a * np.cos(b)\n'"
tmp/analyses/examples/3_methods.py,4,"b'import numpy as np\n\nfrom transonic import boost\n\ntype_ = float\n\n\n@boost\nclass Transmitter:\n\n    freq: type_\n\n    def __init__(self, freq):\n        self.freq = float(freq)\n\n    @boost\n    def __call__(self, inp: ""float[]"", arg_default: int = 1):\n        """"""My docstring""""""\n        return inp * np.exp(np.arange(len(inp)) * self.freq * 1j)\n\n\nif __name__ == ""__main__"":\n    inp = np.ones(2)\n    freq = 1.0\n    trans = Transmitter(freq)\n\n    def for_check(freq, inp):\n        return inp * np.exp(np.arange(len(inp)) * freq * 1j)\n\n    assert np.allclose(trans(inp), for_check(freq, inp))\n'"
tmp/analyses/examples/4_class_blocks.py,6,"b'import numpy as np\n\nfrom transonic import Transonic\n\nts = Transonic()\n\n\nclass MyClass:\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n\n    def compute(self, n: int):\n\n        a = self.a\n        b = self.b\n\n        if ts.is_transpiled:\n            result = ts.use_block(""block0"")\n        else:\n            # transonic block (\n            #     float[][] a, b;\n            # )\n\n            # transonic block (\n            #     float[][][] a, b;\n            # )\n            result = np.zeros_like(a)\n            for _ in range(n):\n                result += a ** 2 + b ** 3\n\n        a = result\n\n        if ts.is_transpiled:\n            result = ts.use_block(""block1"")\n        else:\n            # transonic block (\n            #     float[][] a, b;\n            # )\n\n            # transonic block (\n            #     float[][][] a, b;\n            # )\n            result = np.zeros_like(a)\n            for _ in range(n):\n                result += a ** 2 + b ** 3\n\n        return result\n\n\nif __name__ == ""__main__"":\n\n    shape = 2, 2\n    a = np.random.rand(*shape)\n    b = np.random.rand(*shape)\n\n    obj = MyClass(a, b)\n\n    ret0 = obj.compute(10)\n\n    print(""(is_transpiled, is_compiling, is_compiled)"", (ts.is_transpiled, ts.is_compiling, ts.is_compiled))\n\n    if ts.is_transpiled:\n        ret = obj.compute(10)\n        assert np.allclose(ret, ret0), (ret - ret0)\n        ts.is_transpiled = False\n        ret1 = obj.compute(10)\n        ts.is_transpiled = True\n        assert np.allclose(ret, ret1), (ret - ret1)\n        print(""allclose OK"")\n'"
tmp/analyses/examples/5_blocks_type_hints.py,3,"b'\n\n\nimport numpy as np\n\nimport foo\n\nfrom transonic import Transonic, Type, NDim, Array\n\nT = Type(float, complex)\nN = NDim(1, 2)\nA = Array[T, N]\nA1 = Array[T, N + 1]\n\nts = Transonic()\n\n\nclass MyClass:\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n\n    def compute(self, n):\n\n        a = self.a\n        b = self.b\n\n        if ts.is_transpiled:\n            result = ts.use_block(""block0"")\n        else:\n            # transonic block (\n            #     A a; A1 b;\n            #     float n\n            # )\n\n            # transonic block (\n            #     int[:] a, b;\n            #     float n\n            # )\n\n            result = a ** 2 + b.mean() ** 3 + foo.bar(n)\n\n        return result\n\n\nif __name__ == ""__main__"":\n\n    shape = 100, 100\n    a = np.random.rand(*shape)\n    b = np.random.rand(*shape)\n\n    obj = MyClass(a, b)\n\n    obj.compute(10)\n\n    if ts.is_transpiled:\n        ret = obj.compute(10)\n        ts.is_transpiled = False\n        ret1 = obj.compute(10)\n        ts.is_transpiled = True\n        assert np.allclose(ret, ret1)\n        print(""allclose OK"")\n'"
tmp/analyses/examples/6_issue29_blocks_func.py,0,"b'""""""\nSee https://foss.heptapod.net/fluiddyn/transonic/issues/6\n\n""""""\nfrom transonic import with_blocks, block\n\nT = int\n\n\ndef non_pythranizable(arg):\n    """"""represent a function that can not be compiled by Pythran""""""\n    return arg\n\n\n@with_blocks\ndef func0(arg: int):\n\n    a = b = non_pythranizable(arg)\n    c: float = 1.2\n\n    with block():\n        # transonic signature(\n        #     T a, b\n        # )\n\n        # transonic signature(\n        a: int\n        b: float\n        # )\n\n        return a + b + c + arg\n\n\n@with_blocks\ndef func1(arg: int):\n\n    a = non_pythranizable(arg)\n\n    with block():\n        a: int\n        tmp = a + arg\n\n    return non_pythranizable(tmp)\n'"
tmp/analyses/examples/7_issue29_blocks_classes.py,0,"b'""""""\nSee https://foss.heptapod.net/fluiddyn/transonic/issues/6\n\n""""""\nfrom transonic import boost, with_blocks, block\n\n\ndef non_pythranizable(arg):\n    """"""represent a function that can not be compiled by Pythran""""""\n    return arg\n\n\n@boost\nclass MyClass:\n\n    attr0: int\n\n    @with_blocks\n    def func(self, arg: int):\n\n        a = non_pythranizable(arg)\n\n        with block(a=float):\n            tmp = a + arg + self.attr0\n\n        return non_pythranizable(tmp)\n'"
doc/examples/packages/package_cythran/check.py,1,"b'import numpy as np\nfrom transonic.aheadoftime import modules_backends\n\nfrom numba.targets.registry import CPUDispatcher\n\nfrom package_cythran.calcul import laplace\nfrom package_cythran.util import func\nfrom package_cythran.other import func_numba\n\nlaplace(np.ones((2, 2), dtype=np.int32))\nfunc(1)\nfunc(2.)\n\nts = modules_backends[""pythran""][""package_cythran.calcul""]\nassert ts.backend.name == ""pythran""\n\nts = modules_backends[""cython""][""package_cythran.util""]\nassert ts.backend.name == ""cython""\n\nts = modules_backends[""numba""][""package_cythran.other""]\nassert ts.backend.name == ""numba""\n\nassert isinstance(func_numba, CPUDispatcher)\n\nprint(""everything looks alright!"")\n'"
doc/examples/packages/package_cythran/setup.py,1,"b'""""""\nNo pyproject.toml file because in some cases, isolated build cannot be used.\n\n""""""\nimport sys\nimport os\nfrom pathlib import Path\n\nfrom runpy import run_path\n\nfrom setuptools.dist import Distribution\nfrom setuptools import setup, find_packages\n\nif sys.version_info[:2] < (3, 6):\n    raise RuntimeError(""Python version >= 3.6 required."")\n\n\ndef install_setup_requires():\n    dist = Distribution()\n    # Honor setup.cfg\'s options.\n    dist.parse_config_files(ignore_option_errors=True)\n    if dist.setup_requires:\n        dist.fetch_build_eggs(dist.setup_requires)\n\n\ninstall_setup_requires()\n\nfrom transonic.dist import make_backend_files, init_transonic_extensions\nimport numpy as np\n\nhere = Path(__file__).parent.absolute()\n\npack_name = ""package_cythran""\npack_dir = here / pack_name\n\n# Get the version from the relevant file\nversion = run_path(pack_name + ""/_version.py"")\n__version__ = version[""__version__""]\n\ninstall_requires = [""transonic"", ""numpy"", ""matplotlib"", ""numba""]\n\nrelative_paths = [""calcul.py""]\nmake_backend_files(\n    [pack_dir / path for path in relative_paths], backend=""pythran""\n)\n\nrelative_paths = [""util.py""]\nmake_backend_files([pack_dir / path for path in relative_paths], backend=""cython"")\n\nrelative_paths = [""other.py""]\nmake_backend_files([pack_dir / path for path in relative_paths], backend=""numba"")\n\n\nextensions = []\nif ""egg_info"" not in sys.argv:\n    compile_arch = os.getenv(""CARCH"", ""native"")\n    extensions = init_transonic_extensions(\n        pack_name,\n        backend=""pythran"",\n        include_dirs=[np.get_include()],\n        compile_args=(""-O3"", f""-march={compile_arch}"", ""-DUSE_XSIMD""),\n    )\n    extensions.extend(init_transonic_extensions(pack_name, backend=""cython""))\n    init_transonic_extensions(pack_name, backend=""numba"")\n\nsetup(\n    version=__version__,\n    packages=find_packages(exclude=[""doc""]),\n    install_requires=install_requires,\n    ext_modules=extensions,\n)\n'"
doc/examples/packages/package_simple/check.py,1,"b'import numpy as np\n\nfrom transonic.aheadoftime import modules_backends\n\nfrom package_simple.calcul import laplace\nfrom package_simple.util import func\n\nlaplace(np.ones((2, 2), dtype=np.int32))\nfunc(1)\nfunc(2.)\n\nmodules = modules_backends[""pythran""]\n\nts = modules[""package_simple.calcul""]\nassert ts.backend.name == ""pythran""\n\nts = modules[""package_simple.util""]\nassert ts.backend.name == ""pythran""\n\nprint(""everything looks alright!"")\n'"
doc/examples/packages/package_simple/setup.py,1,"b'""""""\nNo pyproject.toml file because in some cases, isolated build cannot be used.\n\n""""""\nimport sys\nimport os\nfrom pathlib import Path\n\nfrom runpy import run_path\n\nfrom setuptools.dist import Distribution\nfrom setuptools import setup, find_packages\n\nif sys.version_info[:2] < (3, 6):\n    raise RuntimeError(""Python version >= 3.6 required."")\n\n\ndef install_setup_requires():\n    dist = Distribution()\n    # Honor setup.cfg\'s options.\n    dist.parse_config_files(ignore_option_errors=True)\n    if dist.setup_requires:\n        dist.fetch_build_eggs(dist.setup_requires)\n\n\ninstall_setup_requires()\n\nfrom transonic.dist import make_backend_files, init_transonic_extensions\nimport numpy as np\n\nhere = Path(__file__).parent.absolute()\n\npack_name = ""package_simple""\npack_dir = here / pack_name\n\n# Get the version from the relevant file\nversion = run_path(pack_name + ""/_version.py"")\n__version__ = version[""__version__""]\n\ninstall_requires = [""transonic"", ""numpy"", ""matplotlib""]\n\nrelative_paths = [""util.py"", ""calcul.py""]\nmake_backend_files([pack_dir / path for path in relative_paths])\n\nextensions = []\nif ""egg_info"" not in sys.argv:\n    compile_arch = os.getenv(""CARCH"", ""native"")\n    extensions = init_transonic_extensions(\n        pack_name,\n        include_dirs=[np.get_include()],\n        compile_args=(""-O3"", f""-march={compile_arch}"", ""-DUSE_XSIMD""),\n    )\n\nsetup(\n    version=__version__,\n    packages=find_packages(exclude=[""doc""]),\n    install_requires=install_requires,\n    ext_modules=extensions,\n)\n'"
doc/for_dev/cython_bugs/ctypedef_buffer/pure.py,0,b'def mysum(arr):\n    ret = arr.dtype.type(0)\n    for i in range(arr.shape[0]):\n        ret += arr[i]\n    return ret\n'
doc/for_dev/cython_bugs/ctypedef_buffer/pure_ctype_memview.py,0,b'\ndef mysum(arr):\n    ret = arr.dtype.type(0)\n    for i in range(arr.shape[0]):\n        ret += arr[i]\n    return ret'
doc/for_dev/cython_bugs/purepy_fusedtype/bug.py,0,b'\ndef func(arg):\n    arr = arg\n    return arr\n'
doc/for_dev/cython_bugs/purepy_nogil/pure_with_pxd.py,0,"b'# cython: language_level=3\nimport cython\n\n\ndef add(a, b):\n    return a + b\n\n\ndef use_add(n):\n    result = 1\n    with cython.nogil:\n        for i in range(n):\n            result = add(result, result)\n    return result\n'"
doc/for_dev/cython_bugs/purepy_nogil/pure_wo_pxd.py,0,"b'# cython: language_level=3\n\nimport cython\n\n# @cython.ccall\n@cython.cfunc\n@cython.inline\n@cython.returns(cython.int)\n@cython.locals(a=cython.int, b=cython.int)\n@cython.nogil\ndef add(a, b):\n    return a + b\n\n\n@cython.ccall\n@cython.locals(n=cython.int, i=Py_ssize_t, result=cython.int)\ndef use_add(n):\n    result = 1\n    with cython.nogil:\n        for i in range(n):\n            result = add(result, result)\n    return result'"
doc/for_dev/cython_bugs/purepy_nogil/pure_wo_pxd_ccall.py,0,"b'# cython: language_level=3\n\nimport cython\n\n@cython.ccall\n@cython.inline\n@cython.returns(cython.int)\n@cython.locals(a=cython.int, b=cython.int)\n@cython.nogil\ndef add(a, b):\n    return a + b\n\n@cython.ccall\n@cython.locals(n=cython.int, i=Py_ssize_t, result=cython.int)\ndef use_add(n):\n    result = 1\n    with cython.nogil:\n        for i in range(n):\n            result = add(result, result)\n    return result\n'"
doc/for_dev/scikit-image/future/__init__.py,0,b''
doc/for_dev/scikit-image/future/_colormixer.py,19,"b'""""""Color Mixer\n\nNumPy does not do overflow checking when adding or multiplying\nintegers, so currently the only way to clip results efficiently\n(without making copies of the data) is with an extension such as this\none.\n\n""""""\n\nimport numpy as np\n\nfrom transonic import boost, Array\n\nAuint8 = ""uint8[:,:,:]""\nA1dC = Array[np.uint8, ""1d"", ""C""]\n\n\n@boost(boundscheck=False, wraparound=False, cdivision=True, nonecheck=False)\ndef add(img: Auint8, stateimg: Auint8, channel: int, amount: int):\n    """"""Add a given amount to a color channel of `stateimg`, and\n    store the result in `img`.  Overflow is clipped.\n\n    Parameters\n    ----------\n    img : (M, N, 3) ndarray of uint8\n        Output image.\n    stateimg : (M, N, 3) ndarray of uint8\n        Input image.\n    channel : int\n        Channel (0 for ""red"", 1 for ""green"", 2 for ""blue"").\n    amount : int\n        Value to add.\n\n    """"""\n    height = img.shape[0]\n    width = img.shape[1]\n\n    k = channel\n    n = amount\n\n    lut: A1dC = np.empty(256, dtype=np.uint8)\n\n    for l in range(256):\n        op_result = l + n\n        if op_result > 255:\n            op_result = 255\n        elif op_result < 0:\n            op_result = 0\n\n        lut[l] = np.uint8(op_result)\n\n    for i in range(height):\n        for j in range(width):\n            img[i, j, k] = lut[stateimg[i, j, k]]\n\n\n@boost(boundscheck=False, wraparound=False, cdivision=True, nonecheck=False)\ndef multiply(img: Auint8, stateimg: Auint8, channel: int, amount: float):\n    """"""Multiply a color channel of `stateimg` by a certain amount, and\n    store the result in `img`.  Overflow is clipped.\n\n    Parameters\n    ----------\n    img : (M, N, 3) ndarray of uint8\n        Output image.\n    stateimg : (M, N, 3) ndarray of uint8\n        Input image.\n    channel : int\n        Channel (0 for ""red"", 1 for ""green"", 2 for ""blue"").\n    amount : float\n        Multiplication factor.\n\n    """"""\n    height = img.shape[0]\n    width = img.shape[1]\n    k = channel\n    n = amount\n\n    lut: A1dC = np.empty(256, dtype=np.uint8)\n\n    for l in range(256):\n        op_result = l * n\n        if op_result > 255:\n            op_result = 255\n        elif op_result < 0:\n            op_result = 0\n\n        lut[l] = np.uint8(op_result)\n\n    for i in range(height):\n        for j in range(width):\n            img[i, j, k] = lut[stateimg[i, j, k]]\n\n\n@boost(boundscheck=False, wraparound=False, cdivision=True, nonecheck=False)\ndef brightness(img: Auint8, stateimg: Auint8, factor: float, offset: int):\n    """"""Modify the brightness of an image.\n    \'factor\' is multiplied to all channels, which are\n    then added by \'amount\'. Overflow is clipped.\n\n    Parameters\n    ----------\n    img : (M, N, 3) ndarray of uint8\n        Output image.\n    stateimg : (M, N, 3) ndarray of uint8\n        Input image.\n    factor : float\n        Multiplication factor.\n    offset : int\n        Ammount to add to each channel.\n\n    """"""\n    height = img.shape[0]\n    width = img.shape[1]\n\n    lut: A1dC = np.empty(256, dtype=np.uint8)\n\n    for k in range(256):\n        op_result = k * factor + offset\n        if op_result > 255:\n            op_result = 255\n        elif op_result < 0:\n            op_result = 0\n\n        lut[k] = np.uint8(op_result)\n\n    for i in range(height):\n        for j in range(width):\n            img[i, j, 0] = lut[stateimg[i, j, 0]]\n            img[i, j, 1] = lut[stateimg[i, j, 1]]\n            img[i, j, 2] = lut[stateimg[i, j, 2]]\n\n\n\n@boost(boundscheck=False, wraparound=False, cdivision=True, nonecheck=False)\ndef sigmoid_gamma(img: Auint8, stateimg: Auint8, alpha: float, beta: float):\n    height = img.shape[0]\n    width = img.shape[1]\n\n    c1 = 1 / (1 + np.exp(beta))\n    c2 = 1 / (1 + np.exp(beta - alpha)) - c1\n\n    lut: A1dC = np.empty(256, dtype=np.uint8)\n\n    # compute the lut\n    for k in range(256):\n        lut[k] = np.uint8(\n            ((1 / (1 + np.exp(beta - (k / 255.0) * alpha))) - c1) * 255 / c2\n        )\n    for i in range(height):\n        for j in range(width):\n            img[i, j, 0] = lut[stateimg[i, j, 0]]\n            img[i, j, 1] = lut[stateimg[i, j, 1]]\n            img[i, j, 2] = lut[stateimg[i, j, 2]]\n\n\n@boost(boundscheck=False, wraparound=False, cdivision=True, nonecheck=False)\ndef gamma(img: Auint8, stateimg: Auint8, gamma_: float):\n    height: np.intp = img.shape[0]\n    width: np.intp = img.shape[1]\n\n    lut: A1dC = np.empty(256, dtype=np.uint8)\n\n    if gamma_ == 0:\n        gamma_ = 0.00000000000000000001\n    gamma_ = 1.0 / gamma_\n\n    # compute the lut\n    k: np.uint8\n    for k in range(256):\n        lut[k] = np.uint8(pow((k / 255.0), gamma_) * 255)\n\n    i: np.intp\n    j: np.intp\n\n    for i in range(height):\n        for j in range(width):\n            img[i, j, 0] = lut[stateimg[i, j, 0]]\n            img[i, j, 1] = lut[stateimg[i, j, 1]]\n            img[i, j, 2] = lut[stateimg[i, j, 2]]\n\n\ndef rgb_2_hsv(RGB, HSV):\n    R, G, B = RGB\n\n    if R > 255:\n        R = 255\n    elif R < 0:\n        R = 0\n\n    if G > 255:\n        G = 255\n    elif G < 0:\n        G = 0\n\n    if B > 255:\n        B = 255\n    elif B < 0:\n        B = 0\n\n    if R < G:\n        MIN = R\n        MAX = G\n    else:\n        MIN = G\n        MAX = R\n\n    if B < MIN:\n        MIN = B\n    elif B > MAX:\n        MAX = B\n    else:\n        pass\n\n    V = MAX / 255.0\n\n    if MAX == MIN:\n        H = 0.0\n    elif MAX == R:\n        H = (60 * (G - B) / (MAX - MIN) + 360) % 360\n    elif MAX == G:\n        H = 60 * (B - R) / (MAX - MIN) + 120\n    else:\n        H = 60 * (R - G) / (MAX - MIN) + 240\n\n    if MAX == 0:\n        S = 0\n    else:\n        S = 1 - MIN / MAX\n\n    HSV[0] = H\n    HSV[1] = S\n    HSV[2] = V\n\n\ndef hsv_2_rgb(HSV, RGB):\n\n    H, S, V = HSV\n\n    if H > 360:\n        H = H % 360\n    elif H < 0:\n        H = 360 - ((-1 * H) % 360)\n    else:\n        pass\n\n    if S > 1:\n        S = 1\n    elif S < 0:\n        S = 0\n    else:\n        pass\n\n    if V > 1:\n        V = 1\n    elif V < 0:\n        V = 0\n    else:\n        pass\n\n    hi = int(H / 60.0) % 6\n    f = (H / 60.0) - int(H / 60.0)\n    p = V * (1 - S)\n    q = V * (1 - f * S)\n    t = V * (1 - (1 - f) * S)\n\n    if hi == 0:\n        r = V\n        g = t\n        b = p\n    elif hi == 1:\n        r = q\n        g = V\n        b = p\n    elif hi == 2:\n        r = p\n        g = V\n        b = t\n    elif hi == 3:\n        r = p\n        g = q\n        b = V\n    elif hi == 4:\n        r = t\n        g = p\n        b = V\n    else:\n        r = V\n        g = p\n        b = q\n\n    RGB[0] = r\n    RGB[1] = g\n    RGB[2] = b\n\n\n@boost(boundscheck=False, wraparound=False, cdivision=True, nonecheck=False)\ndef py_hsv_2_rgb(H: float, S: float, V: float):\n    """"""Convert an HSV value to RGB.\n\n    Automatic clipping.\n\n    Parameters\n    ----------\n    H : float\n        From 0. - 360.\n    S : float\n        From 0. - 1.\n    V : float\n        From 0. - 1.\n\n    Returns\n    -------\n    out : (R, G, B) ints\n        Each from 0 - 255\n\n    conversion convention from here:\n    http://en.wikipedia.org/wiki/HSL_and_HSV\n\n    """"""\n\n    HSV = [H, S, V]\n    RGB = [0] * 3\n\n    hsv_2_rgb(HSV, RGB)\n\n    R = int(RGB[0] * 255)\n    G = int(RGB[1] * 255)\n    B = int(RGB[2] * 255)\n\n    return R, G, B\n\n\n@boost(boundscheck=False, wraparound=False, cdivision=True, nonecheck=False)\ndef py_rgb_2_hsv(R: int, G: int, B: int):\n    """"""Convert an HSV value to RGB.\n\n    Automatic clipping.\n\n    Parameters\n    ----------\n    R : int\n        From 0. - 255.\n    G : int\n        From 0. - 255.\n    B : int\n        From 0. - 255.\n\n    Returns\n    -------\n    out : (H, S, V) floats\n        Ranges (0...360), (0...1), (0...1)\n\n    conversion convention from here:\n    http://en.wikipedia.org/wiki/HSL_and_HSV\n\n    """"""\n    RGB = [float(R), float(G), float(B)]\n    HSV = [0.0] * 3\n\n    rgb_2_hsv(RGB, HSV)\n\n    return HSV\n\n\n@boost(boundscheck=False, wraparound=False, cdivision=True, nonecheck=False)\ndef hsv_add(\n    img: Auint8, stateimg: Auint8, h_amt: float, s_amt: float, v_amt: float\n):\n    """"""Modify the image color by specifying additive HSV Values.\n\n    Since the underlying images are RGB, all three values HSV\n    must be specified at the same time.\n\n    The RGB triplet in the image is converted to HSV, the operation\n    is applied, and then the HSV triplet is converted back to RGB\n\n    HSV values are scaled to H(0. - 360.), S(0. - 1.), V(0. - 1.)\n    then the operation is performed and any overflow is clipped, then the\n    reverse transform is performed. Those are the ranges to keep in mind,\n    when passing in values.\n\n    Parameters\n    ----------\n    img : (M, N, 3) ndarray of uint8\n        Output image.\n    stateimg : (M, N, 3) ndarray of uint8\n        Input image.\n    h_amt : float\n        Ammount to add to H channel.\n    s_amt : float\n        Ammount to add to S channel.\n    v_amt : float\n        Ammount to add to V channel.\n\n    """"""\n    height = img.shape[0]\n    width = img.shape[1]\n\n    HSV = [0.0] * 3\n    RGB = [0.0] * 3\n\n    for i in range(height):\n        for j in range(width):\n            RGB[0] = stateimg[i, j, 0]\n            RGB[1] = stateimg[i, j, 1]\n            RGB[2] = stateimg[i, j, 2]\n\n            rgb_2_hsv(RGB, HSV)\n\n            # Add operation\n            HSV[0] += h_amt\n            HSV[1] += s_amt\n            HSV[2] += v_amt\n\n            hsv_2_rgb(HSV, RGB)\n\n            RGB[0] *= 255\n            RGB[1] *= 255\n            RGB[2] *= 255\n\n            img[i, j, 0] = RGB[0]\n            img[i, j, 1] = RGB[1]\n            img[i, j, 2] = RGB[2]\n\n\n@boost(boundscheck=False, wraparound=False, cdivision=True, nonecheck=False)\ndef hsv_multiply(img: Auint8, stateimg: Auint8, h_amt: float, s_amt: float, v_amt: float):\n    """"""Modify the image color by specifying multiplicative HSV Values.\n\n    Since the underlying images are RGB, all three values HSV\n    must be specified at the same time.\n\n    The RGB triplet in the image is converted to HSV, the operation\n    is applied, and then the HSV triplet is converted back to RGB\n\n    HSV values are scaled to H(0. - 360.), S(0. - 1.), V(0. - 1.)\n    then the operation is performed and any overflow is clipped, then the\n    reverse transform is performed. Those are the ranges to keep in mind,\n    when passing in values.\n\n    Note that since hue is in degrees, it makes no sense to multiply\n    that channel, thus an add operation is performed on the hue. And the\n    values given for h_amt, should be the same as for hsv_add\n\n    Parameters\n    ----------\n    img : (M, N, 3) ndarray of uint8\n        Output image.\n    stateimg : (M, N, 3) ndarray of uint8\n        Input image.\n    h_amt : float\n        Ammount to add to H channel.\n    s_amt : float\n        Ammount by which to multiply S channel.\n    v_amt : float\n        Ammount by which to multiply V channel.\n\n\n    """"""\n    height = img.shape[0]\n    width = img.shape[1]\n\n    HSV = [0.0] * 3\n    RGB = [0.0] * 3\n\n    for i in range(height):\n        for j in range(width):\n            RGB[0] = stateimg[i, j, 0]\n            RGB[1] = stateimg[i, j, 1]\n            RGB[2] = stateimg[i, j, 2]\n\n            rgb_2_hsv(RGB, HSV)\n\n            # Multiply operation\n            HSV[0] += h_amt\n            HSV[1] *= s_amt\n            HSV[2] *= v_amt\n\n            hsv_2_rgb(HSV, RGB)\n\n            RGB[0] *= 255\n            RGB[1] *= 255\n            RGB[2] *= 255\n\n            img[i, j, 0] = RGB[0]\n            img[i, j, 1] = RGB[1]\n            img[i, j, 2] = RGB[2]\n'"
doc/for_dev/scikit-image/future/_convex_hull.py,8,"b'import numpy as np\n\nfrom transonic import boost, Array\n\n\n@boost(wraparound=False, boundscheck=False, cdivision=True, nonecheck=False)\ndef possible_hull(img: ""uint8[:,:]""):\n    """"""Return positions of pixels that possibly belong to the convex hull.\n\n    Parameters\n    ----------\n    img : ndarray of bool\n        Binary input image.\n\n    Returns\n    -------\n    coords : ndarray (cols, 2)\n       The ``(row, column)`` coordinates of all pixels that possibly belong to\n       the convex hull.\n\n    """"""\n    r: np.intp\n    c: np.intp\n    rows: np.intp = img.shape[0]\n    cols: np.intp = img.shape[1]\n\n    # Output: rows storage slots for left boundary pixels\n    #         cols storage slots for top boundary pixels\n    #         rows storage slots for right boundary pixels\n    #         cols storage slots for bottom boundary pixels\n    coords = np.ones((2 * (rows + cols), 2), dtype=np.intp)\n    coords *= -1\n\n    nonzero: Array[np.intp, ""2d"", ""C"", ""memview""] = coords\n\n    rows_cols: np.intp = rows + cols\n    rows_2_cols: np.intp = 2 * rows + cols\n\n    for r in range(rows):\n\n        rows_cols_r = rows_cols + r\n\n        for c in range(cols):\n\n            if img[r, c] != 0:\n\n                rows_c = rows + c\n                rows_2_cols_c = rows_2_cols + c\n\n                # Left check\n                if nonzero[r, 1] == -1:\n                    nonzero[r, 0] = r\n                    nonzero[r, 1] = c\n\n                # Right check\n                elif nonzero[rows_cols_r, 1] < c:\n                    nonzero[rows_cols_r, 0] = r\n                    nonzero[rows_cols_r, 1] = c\n\n                # Top check\n                if nonzero[rows_c, 1] == -1:\n                    nonzero[rows_c, 0] = r\n                    nonzero[rows_c, 1] = c\n\n                # Bottom check\n                elif nonzero[rows_2_cols_c, 0] < r:\n                    nonzero[rows_2_cols_c, 0] = r\n                    nonzero[rows_2_cols_c, 1] = c\n\n    return coords[coords[:, 0] != -1]\n'"
doc/for_dev/scikit-image/future/_greyreconstruct.py,14,"b'""""""\n`reconstruction_loop` originally part of CellProfiler,\ncode licensed under both GPL and BSD licenses.\n\nWebsite: http://www.cellprofiler.org\nCopyright (c) 2003-2009 Massachusetts Institute of Technology\nCopyright (c) 2009-2011 Broad Institute\nAll rights reserved.\nOriginal author: Lee Kamentsky\n\n""""""\nimport numpy as np\n\nfrom transonic import boost, Array, const\n\nAu = Array[np.uint32, ""1d"", ""C"", ""positive_indices""]\nA = Array[np.int32, ""1d"", ""C"", ""positive_indices""]\nM = Array[np.int32, ""1d"", ""C"", ""memview""]\n\n\n@boost(boundscheck=False)\ndef reconstruction_loop(\n    ranks: Au,\n    prev: A,\n    next: A,\n    strides: const(M),\n    current_idx: np.intp,\n    image_stride: np.intp,\n):\n    """"""The inner loop for reconstruction.\n\n    This algorithm uses the rank-order of pixels. If low intensity pixels have\n    a low rank and high intensity pixels have a high rank, then this loop\n    performs reconstruction by dilation. If this ranking is reversed, the\n    result is reconstruction by erosion.\n\n    For each pixel in the seed image, check its neighbors. If its neighbor\'s\n    rank is below that of the current pixel, replace the neighbor\'s rank with\n    the rank of the current pixel. This dilation is limited by the mask, i.e.\n    the rank at each pixel cannot exceed the mask as that pixel.\n\n    Parameters\n    ----------\n    ranks : array\n        The rank order of the flattened seed and mask images.\n    prev, next: arrays\n        Indices of previous and next pixels in rank sorted order.\n    strides : array\n        Strides to neighbors of the current pixel.\n    current_idx : int\n        Index of highest-ranked pixel used as starting point in loop.\n    image_stride : int\n        Stride between seed image and mask image in `aranks`.\n    """"""\n    current_rank: np.uint32\n    i: np.intp\n    neighbor_idx: np.intp\n    neighbor_rank: np.uint32\n    mask_rank: np.uint32\n    current_link: np.intp\n    nprev: np.int32\n    nnext: np.int32\n    nstrides: np.intp = strides.shape[0]\n\n    while current_idx != -1:\n        if current_idx < image_stride:\n            current_rank = ranks[current_idx]\n            if current_rank == 0:\n                break\n            for i in range(nstrides):\n                neighbor_idx = current_idx + strides[i]\n                neighbor_rank = ranks[neighbor_idx]\n                # Only propagate neighbors ranked below the current rank\n                if neighbor_rank < current_rank:\n                    mask_rank = ranks[neighbor_idx + image_stride]\n                    # Only propagate neighbors ranked below the mask rank\n                    if neighbor_rank < mask_rank:\n                        # Raise the neighbor to the mask rank if\n                        # the mask ranked below the current rank\n                        if mask_rank < current_rank:\n                            current_link = neighbor_idx + image_stride\n                            ranks[neighbor_idx] = mask_rank\n                        else:\n                            current_link = current_idx\n                            ranks[neighbor_idx] = current_rank\n                        # unlink the neighbor\n                        nprev = prev[neighbor_idx]\n                        nnext = next[neighbor_idx]\n                        next[nprev] = nnext\n                        if nnext != -1:\n                            prev[nnext] = nprev\n                        # link to the neighbor after the current link\n                        nnext = next[current_link]\n                        next[neighbor_idx] = nnext\n                        prev[neighbor_idx] = current_link\n                        if nnext >= 0:\n                            prev[nnext] = neighbor_idx\n                            next[current_link] = neighbor_idx\n        current_idx = next[current_idx]\n'"
doc/for_dev/scikit-image/future/_hessian_det_appx.py,2,"b'import numpy as np\n\nfrom transonic import boost\n\n\ndef _clip(x, low, high):\n    """"""Clips coordinate between high and low.\n\n    This method was created so that `hessian_det_appx` does not have to make\n    a Python call.\n\n    Parameters\n    ----------\n    x : int\n        Coordinate to be clipped.\n    low : int\n        The lower bound.\n    high : int\n        The higher bound.\n\n    Returns\n    -------\n    x : int\n        `x` clipped between `high` and `low`.\n    """"""\n\n    if x > high:\n        return high\n    if x < low:\n        return low\n    return x\n\n\ndef _integ(img, r, c, rl, cl):\n    """"""Integrate over the integral image in the given window\n\n    This method was created so that `hessian_det_appx` does not have to make\n    a Python call.\n\n    Parameters\n    ----------\n    img : array\n        The integral image over which to integrate.\n    r : int\n        The row number of the top left corner.\n    c : int\n        The column number of the top left corner.\n    rl : int\n        The number of rows over which to integrate.\n    cl : int\n        The number of columns over which to integrate.\n\n    Returns\n    -------\n    ans : int\n        The integral over the given window.\n    """"""\n\n    r = _clip(r, 0, img.shape[0] - 1)\n    c = _clip(c, 0, img.shape[1] - 1)\n\n    r2 = _clip(r + rl, 0, img.shape[0] - 1)\n    c2 = _clip(c + cl, 0, img.shape[1] - 1)\n\n    ans = img[r, c] + img[r2, c2] - img[r, c2] - img[r2, c]\n    return max(0, ans)\n\n\n@boost\ndef _hessian_matrix_det(img: ""float64[:,:]"", sigma: np.float64):\n    """"""Computes the approximate Hessian Determinant over an image.\n\n    This method uses box filters over integral images to compute the\n    approximate Hessian Determinant as described in [1]_.\n\n    Parameters\n    ----------\n    img : array\n        The integral image over which to compute Hessian Determinant.\n    sigma : float\n        Standard deviation used for the Gaussian kernel, used for the Hessian\n        matrix\n\n    Returns\n    -------\n    out : array\n        The array of the Determinant of Hessians.\n\n    References\n    ----------\n    .. [1] Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool,\n           ""SURF: Speeded Up Robust Features""\n           ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf\n\n    Notes\n    -----\n    The running time of this method only depends on size of the image. It is\n    independent of `sigma` as one would expect. The downside is that the\n    result for `sigma` less than `3` is not accurate, i.e., not similar to\n    the result obtained if someone computed the Hessian and took it\'s\n    determinant.\n    """"""\n\n    size = int(3 * sigma)\n    height = img.shape[0]\n    width = img.shape[1]\n    s2 = (size - 1) // 2\n    s3 = size // 3\n    w = size\n    out = np.zeros_like(img, dtype=np.double)\n    w_i = 1.0 / size / size\n\n    if size % 2 == 0:\n        size += 1\n\n    for r in range(height):\n        for c in range(width):\n            tl = _integ(img, r - s3, c - s3, s3, s3)  # top left\n            br = _integ(img, r + 1, c + 1, s3, s3)  # bottom right\n            bl = _integ(img, r - s3, c + 1, s3, s3)  # bottom left\n            tr = _integ(img, r + 1, c - s3, s3, s3)  # top right\n\n            dxy = bl + tr - tl - br\n            dxy = -dxy * w_i\n\n            # middle box\n            mid = _integ(img, r - s3 + 1, c - s2, 2 * s3 - 1, w)\n            # sides\n            side = _integ(img, r - s3 + 1, c - s3 // 2, 2 * s3 - 1, s3)\n\n            dxx = mid - 3 * side\n            dxx = -dxx * w_i\n\n            mid = _integ(img, r - s2, c - s3 + 1, w, 2 * s3 - 1)\n            side = _integ(img, r - s3 // 2, c - s3 + 1, s3, 2 * s3 - 1)\n\n            dyy = mid - 3 * side\n            dyy = -dyy * w_i\n\n            out[r, c] = dxx * dyy - 0.81 * (dxy * dxy)\n\n    return out\n'"
doc/for_dev/scikit-image/future/_moments_cy.py,9,"b'import numpy as np\n\nfrom transonic import boost, Array\n\n\n@boost(wraparound=False, cdivision=True, nonecheck=False)\ndef moments_hu(nu: ""float64[:,:]""):\n    hu: Array[np.float64, ""1d"", ""C""] = np.zeros((7,), dtype=np.float64)\n    t0: np.float64 = nu[3, 0] + nu[1, 2]\n    t1: np.float64 = nu[2, 1] + nu[0, 3]\n    q0: np.float64 = t0 * t0\n    q1: np.float64 = t1 * t1\n    n4: np.float64 = 4 * nu[1, 1]\n    s: np.float64 = nu[2, 0] + nu[0, 2]\n    d: np.float64 = nu[2, 0] - nu[0, 2]\n    hu[0] = s\n    hu[1] = d * d + n4 * nu[1, 1]\n    hu[3] = q0 + q1\n    hu[5] = d * (q0 - q1) + n4 * t0 * t1\n    t0 *= q0 - 3 * q1\n    t1 *= 3 * q0 - q1\n    q0 = nu[3, 0] - 3 * nu[1, 2]\n    q1 = 3 * nu[2, 1] - nu[0, 3]\n    hu[2] = q0 * q0 + q1 * q1\n    hu[4] = q0 * t0 + q1 * t1\n    hu[6] = q1 * t0 - q0 * t1\n    return np.asarray(hu)\n'"
doc/for_dev/scikit-image/future/_radon_transform.py,4,"b'import numpy as np\nfrom numpy import cos, sin, floor, ceil, sqrt\n\n\nfrom transonic import boost, Optional\n\n\ndef bilinear_ray_sum(image, theta, ray_position):\n    """"""\n    Compute the projection of an image along a ray.\n\n    Parameters\n    ----------\n    image : 2D array, dtype=float\n        Image to project.\n    theta : float\n        Angle of the projection\n    ray_position : float\n        Position of the ray within the projection\n\n    Returns\n    -------\n    projected_value : float\n        Ray sum along the projection\n    norm_of_weights :\n        A measure of how long the ray\'s path through the reconstruction\n        circle was\n    """"""\n    theta = theta / 180. * np.pi\n    radius = image.shape[0] // 2 - 1\n    projection_center = image.shape[0] // 2\n    rotation_center = image.shape[0] // 2\n    # (s, t) is the (x, y) system rotated by theta\n    t = ray_position - projection_center\n    # s0 is the half-length of the ray\'s path in the reconstruction circle\n    s0 = sqrt(radius * radius - t * t) if radius*radius >= t*t else 0.\n    Ns = 2 * int(ceil(2 * s0))  # number of steps along the ray\n    ray_sum = 0.\n    weight_norm = 0.\n\n    if Ns > 0:\n        # step length between samples\n        ds = 2 * s0 / Ns\n        dx = -ds * cos(theta)\n        dy = -ds * sin(theta)\n        # point of entry of the ray into the reconstruction circle\n        x0 = s0 * cos(theta) - t * sin(theta)\n        y0 = s0 * sin(theta) + t * cos(theta)\n        for k in range(Ns + 1):\n            x = x0 + k * dx\n            y = y0 + k * dy\n            index_i = x + rotation_center\n            index_j = y + rotation_center\n            i = int(floor(index_i))\n            j = int(floor(index_j))\n            di = index_i - floor(index_i)\n            dj = index_j - floor(index_j)\n            # Use linear interpolation between values\n            # Where values fall outside the array, assume zero\n            if i > 0 and j > 0:\n                weight = (1. - di) * (1. - dj) * ds\n                ray_sum += weight * image[i, j]\n                weight_norm += weight * weight\n            if i > 0 and j < image.shape[1] - 1:\n                weight = (1. - di) * dj * ds\n                ray_sum += weight * image[i, j+1]\n                weight_norm += weight * weight\n            if i < image.shape[0] - 1 and j > 0:\n                weight = di * (1 - dj) * ds\n                ray_sum += weight * image[i+1, j]\n                weight_norm += weight * weight\n            if i < image.shape[0] - 1 and j < image.shape[1] - 1:\n                weight = di * dj * ds\n                ray_sum += weight * image[i+1, j+1]\n                weight_norm += weight * weight\n\n    return ray_sum, weight_norm\n\n\ndef bilinear_ray_update(image, image_update, theta, ray_position,\n                        projected_value):\n    """"""Compute the update along a ray using bilinear interpolation.\n\n    Parameters\n    ----------\n    image : 2D array, dtype=float\n        Current reconstruction estimate.\n    image_update : 2D array, dtype=float\n        Array of same shape as ``image``. Updates will be added to this array.\n    theta : float\n        Angle of the projection.\n    ray_position : float\n        Position of the ray within the projection.\n    projected_value : float\n        Projected value (from the sinogram).\n\n    Returns\n    -------\n    deviation :\n        Deviation before updating the image.\n    """"""\n    ray_sum, weight_norm = bilinear_ray_sum(image, theta, ray_position)\n    if weight_norm > 0.:\n        deviation = -(ray_sum - projected_value) / weight_norm\n    else:\n        deviation = 0.\n    theta = theta / 180. * np.pi\n    radius = image.shape[0] // 2 - 1\n    projection_center = image.shape[0] // 2\n    rotation_center = image.shape[0] // 2\n    # (s, t) is the (x, y) system rotated by theta\n    t = ray_position - projection_center\n    # s0 is the half-length of the ray\'s path in the reconstruction circle\n    s0 = sqrt(radius*radius - t*t) if radius*radius >= t*t else 0.\n    Ns = 2 * int(ceil(2 * s0))\n    # beta for equiripple Hamming window\n    hamming_beta = 0.46164\n\n    if Ns > 0:\n        # Step length between samples\n        ds = 2 * s0 / Ns\n        dx = -ds * cos(theta)\n        dy = -ds * sin(theta)\n        # Point of entry of the ray into the reconstruction circle\n        x0 = s0 * cos(theta) - t * sin(theta)\n        y0 = s0 * sin(theta) + t * cos(theta)\n        for k in range(Ns + 1):\n            x = x0 + k * dx\n            y = y0 + k * dy\n            index_i = x + rotation_center\n            index_j = y + rotation_center\n            i = int(floor(index_i))\n            j = int(floor(index_j))\n            di = index_i - floor(index_i)\n            dj = index_j - floor(index_j)\n            hamming_window = ((1 - hamming_beta)\n                              - hamming_beta * cos(2 * np.pi * k / (Ns - 1)))\n            if i > 0 and j > 0:\n                image_update[i, j] += (deviation * (1. - di) * (1. - dj)\n                                       * ds * hamming_window)\n            if i > 0 and j < image.shape[1] - 1:\n                image_update[i, j+1] += (deviation * (1. - di) * dj\n                                         * ds * hamming_window)\n            if i < image.shape[0] - 1 and j > 0:\n                image_update[i+1, j] += (deviation * di * (1 - dj)\n                                         * ds * hamming_window)\n            if i < image.shape[0] - 1 and j < image.shape[1] - 1:\n                image_update[i+1, j+1] += (deviation * di * dj\n                                           * ds * hamming_window)\n\n    return deviation\n\n\n@boost\ndef sart_projection_update(image: ""float64[:,:]"",\n                           theta: float,\n                           projection: ""float64[:]"",\n                           projection_shift: float = 0.):\n    """"""\n    Compute update to a reconstruction estimate from a single projection\n    using bilinear interpolation.\n\n    Parameters\n    ----------\n    image : 2D array, dtype=float\n        Current reconstruction estimate\n    theta : float\n        Angle of the projection\n    projection : 1D array, dtype=float\n        Projected values, taken from the sinogram\n    projection_shift : float\n        Shift the position of the projection by this many pixels before\n        using it to compute an update to the reconstruction estimate\n\n    Returns\n    -------\n    image_update : 2D array, dtype=float\n        Array of same shape as ``image`` containing updates that should be\n        added to ``image`` to improve the reconstruction estimate\n    """"""\n    image_update = np.zeros_like(image)\n    for i in range(projection.shape[0]):\n        ray_position = i + projection_shift\n        bilinear_ray_update(image, image_update, theta, ray_position,\n                            projection[i])\n    return image_update\n'"
doc/for_dev/scikit-image/future/_unwrap_1d.py,0,"b'from numpy import pi\n\nfrom transonic import boost\n\nA = ""float64[:]""\n\n\n@boost\ndef unwrap_1d(image: A, unwrapped_image: A):\n    """"""Phase unwrapping using the naive approach.""""""\n    unwrapped_image[0] = image[0]\n    periods = 0\n    for i in range(1, image.shape[0]):\n        difference = image[i] - image[i - 1]\n        if difference > pi:\n            periods -= 1\n        elif difference < -pi:\n            periods += 1\n        unwrapped_image[i] = image[i] + 2 * pi * periods\n'"
doc/for_dev/scikit-image/future/brief_cy.py,0,"b'from transonic import boost\n\nAi2 = ""int[:,:]""\n\n\n@boost\ndef _brief_loop(\n    image: ""float64[:,:]"",\n    descriptors: ""uint8[:,:]"",\n    keypoints: ""intp[:,:]"",\n    pos0: Ai2,\n    pos1: Ai2,\n):\n    for k in range(len(keypoints)):\n        kr, kc = keypoints[k]\n        for p in range(len(pos0)):\n            pr0, pc0 = pos0[p]\n            pr1, pc1 = pos1[p]\n            descriptors[k, p] = (\n                image[kr + pr0, kc + pc0] < image[kr + pr1, kc + pc1]\n            )\n'"
doc/for_dev/scikit-image/future/cmorph.py,44,"b'import numpy as np\n\nfrom transonic import boost, Optional, Array\n\nA = Array[np.uint8, ""2d"", ""memview""]\n\nA1d = Array[np.intp, ""1d"", ""memview""]\n\n\n@boost(wraparound=False, boundscheck=False, cdivision=True, nonecheck=False)\ndef _dilate(\n    image: A,\n    selem: A,\n    out: Optional[A] = None,\n    shift_x: np.int8 = 0,\n    shift_y: np.int8 = 0,\n):\n    """"""Return greyscale morphological dilation of an image.\n\n    Morphological dilation sets a pixel at (i,j) to the maximum over all pixels\n    in the neighborhood centered at (i,j). Dilation enlarges bright regions\n    and shrinks dark regions.\n\n    Parameters\n    ----------\n\n    image : ndarray\n        Image array.\n    selem : ndarray\n        The neighborhood expressed as a 2-D array of 1\'s and 0\'s.\n    out : ndarray\n        The array to store the result of the morphology. If None, is\n        passed, a new array will be allocated.\n    shift_x, shift_y : bool\n        shift structuring element about center point. This only affects\n        eccentric structuring elements (i.e. selem with even numbered sides).\n\n    Returns\n    -------\n    dilated : uint8 array\n        The result of the morphological dilation.\n    """"""\n\n    rows: np.intp = image.shape[0]\n    cols: np.intp = image.shape[1]\n    srows: np.intp = selem.shape[0]\n    scols: np.intp = selem.shape[1]\n\n    centre_r: np.intp = int(srows / 2) - shift_y\n    centre_c: np.intp = int(scols / 2) - shift_x\n\n    image = np.ascontiguousarray(image)\n    if out is None:\n        out = np.zeros((rows, cols), dtype=np.uint8)\n\n    selem_num: np.intp = np.sum(np.asarray(selem) != 0)\n    sr: A1d = np.empty(selem_num, dtype=np.intp)\n    sc: A1d = np.empty(selem_num, dtype=np.intp)\n\n    s: np.intp = 0\n    r: np.intp\n    c: np.intp\n\n    for r in range(srows):\n        for c in range(scols):\n            if selem[r, c] != 0:\n                sr[s] = r - centre_r\n                sc[s] = c - centre_c\n                s += 1\n\n    local_max: np.uint8\n    value: np.uint8\n    rr: np.intp\n    cc: np.intp\n\n    for r in range(rows):\n        for c in range(cols):\n            local_max = 0\n            for s in range(selem_num):\n                rr = r + sr[s]\n                cc = c + sc[s]\n                if 0 <= rr < rows and 0 <= cc < cols:\n                    value = image[rr, cc]\n                    if value > local_max:\n                        local_max = value\n\n            out[r, c] = local_max\n\n    return np.asarray(out)\n\n\n@boost(wraparound=False, boundscheck=False, cdivision=True, nonecheck=False)\ndef _erode(\n    image: A,\n    selem: A,\n    out: Optional[A] = None,\n    shift_x: np.int8 = 0,\n    shift_y: np.int8 = 0,\n):\n    """"""Return greyscale morphological erosion of an image.\n\n    Morphological erosion sets a pixel at (i,j) to the minimum over all pixels\n    in the neighborhood centered at (i,j). Erosion shrinks bright regions and\n    enlarges dark regions.\n\n    Parameters\n    ----------\n    image : ndarray\n        Image array.\n    selem : ndarray\n        The neighborhood expressed as a 2-D array of 1\'s and 0\'s.\n    out : ndarray\n        The array to store the result of the morphology. If None is\n        passed, a new array will be allocated.\n    shift_x, shift_y : bool\n        shift structuring element about center point. This only affects\n        eccentric structuring elements (i.e. selem with even numbered sides).\n\n    Returns\n    -------\n    eroded : uint8 array\n        The result of the morphological erosion.\n    """"""\n\n    rows: np.intp = image.shape[0]\n    cols: np.intp = image.shape[1]\n    srows: np.intp = selem.shape[0]\n    scols: np.intp = selem.shape[1]\n\n    centre_r: np.intp = int(srows / 2) - shift_y\n    centre_c: np.intp = int(scols / 2) - shift_x\n\n    image = np.ascontiguousarray(image)\n    if out is None:\n        out = np.zeros((rows, cols), dtype=np.uint8)\n\n    selem_num: np.intp = np.sum(np.asarray(selem) != 0)\n    sr: A1d = np.empty(selem_num, dtype=np.intp)\n    sc: A1d = np.empty(selem_num, dtype=np.intp)\n\n    s: np.intp = 0\n    r: np.intp\n    c: np.intp\n\n    for r in range(srows):\n        for c in range(scols):\n            if selem[r, c] != 0:\n                sr[s] = r - centre_r\n                sc[s] = c - centre_c\n                s += 1\n\n    local_max: np.uint8\n    value: np.uint8\n    rr: np.intp\n    cc: np.intp\n\n    for r in range(rows):\n        for c in range(cols):\n            local_min = 255\n            for s in range(selem_num):\n                rr = r + sr[s]\n                cc = c + sc[s]\n                if 0 <= rr < rows and 0 <= cc < cols:\n                    value = image[rr, cc]\n                    if value < local_min:\n                        local_min = value\n\n            out[r, c] = local_min\n\n    return np.asarray(out)\n'"
doc/for_dev/scikit-image/pyx/__init__.py,0,b''
doc/for_dev/scikit-image/setup_codes/_greyreconstruct_reconstruction_loop.py,19,"b'import numpy as np\nfrom future._greyreconstruct import reconstruction_loop\n\nfrom skimage.filters._rank_order import rank_order\n\ny, x = np.mgrid[:20:0.5, :20:0.5]\nbumps = np.sin(x) + np.sin(y)\nh = 0.3\nseed = bumps - h\nmask = bumps\n\nassert tuple(seed.shape) == tuple(mask.shape)\n\nselem = np.ones([3] * seed.ndim, dtype=bool)\noffset = np.array([d // 2 for d in selem.shape])\n\n# Cross out the center of the selem\nselem[tuple(slice(d, d + 1) for d in offset)] = False\n\n# Make padding for edges of reconstructed image so we can ignore boundaries\ndims = np.zeros(seed.ndim + 1, dtype=int)\ndims[1:] = np.array(seed.shape) + (np.array(selem.shape) - 1)\ndims[0] = 2\ninside_slices = tuple(slice(o, o + s) for o, s in zip(offset, seed.shape))\n# Set padded region to minimum image intensity and mask along first axis so\n# we can interleave image and mask pixels when sorting.\npad_value = np.min(seed)\n\nimages = np.full(dims, pad_value, dtype=""float64"")\nimages[(0, *inside_slices)] = seed\nimages[(1, *inside_slices)] = mask\n\n# Create a list of strides across the array to get the neighbors within\n# a flattened array\nvalue_stride = np.array(images.strides[1:]) // images.dtype.itemsize\nimage_stride = np.int64(images.strides[0] // images.dtype.itemsize)\nselem_mgrid = np.mgrid[[slice(-o, d - o) for d, o in zip(selem.shape, offset)]]\nselem_offsets = selem_mgrid[:, selem].transpose()\nnb_strides = np.array(\n    [np.sum(value_stride * selem_offset) for selem_offset in selem_offsets],\n    np.int32,\n)\n\nimages = images.flatten()\n\n# Erosion goes smallest to largest; dilation goes largest to smallest.\nindex_sorted = np.argsort(images).astype(np.int32)\nindex_sorted = index_sorted[::-1]\n\n# Make a linked list of pixels sorted by value. -1 is the list terminator.\nprev = np.full(len(images), -1, np.int32)\nnext_ = np.full(len(images), -1, np.int32)\nprev[index_sorted[1:]] = index_sorted[:-1]\nnext_[index_sorted[:-1]] = index_sorted[1:]\n\n# Cython inner-loop compares the rank of pixel values.\nvalue_rank, value_map = rank_order(images)\n\nstart = index_sorted[0]\nranks = np.array(value_rank)\nstrides = nb_strides\ncurrent_idx = np.int64(start)\n'"
doc/for_dev/scikit-image/setup_codes/cmorph__dilate.py,5,"b'import numpy as np\nfrom future.cmorph import _dilate\n\nrows = 1024\ncols = 1024\nsrows = 64\nscols = 64\n\nimage = np.random.randint(0, 255, rows * cols, dtype=np.uint8).reshape(\n    (rows, cols)\n)\nselem = np.random.randint(0, 1, srows * scols, dtype=np.uint8).reshape(\n    (srows, scols)\n)\nout = np.zeros((rows, cols), dtype=np.uint8)\nshift_x = np.int8(2)\nshift_y = np.int8(2)'"
doc/examples/packages/package_cythran/package_cythran/_version.py,0,"b'\n__version__ = ""0.0.1""'"
doc/examples/packages/package_cythran/package_cythran/calcul.py,2,"b'import numpy as np\n\nfrom transonic import boost, Type, Array, NDim, set_backend_for_this_module\n\nset_backend_for_this_module(""pythran"")\n\nT = Type(np.int32, np.float64, np.float32)\nA = Array[T, NDim(2)]\n\n\n@boost\ndef laplace(image: A):\n    """"""Laplace operator in NumPy for 2D images.""""""\n    laplacian = (\n        image[:-2, 1:-1]\n        + image[2:, 1:-1]\n        + image[1:-1, :-2]\n        + image[1:-1, 2:]\n        - 4 * image[1:-1, 1:-1]\n    )\n    thresh = np.abs(laplacian) > 0.05\n    return thresh\n'"
doc/examples/packages/package_cythran/package_cythran/other.py,0,"b'from transonic import boost, set_backend_for_this_module\n\nset_backend_for_this_module(""numba"")\n\n\n@boost\ndef func_numba(a):\n    return a ** 2\n'"
doc/examples/packages/package_cythran/package_cythran/util.py,1,"b'\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom transonic import boost, Type, set_backend_for_this_module\n\nset_backend_for_this_module(""cython"")\n\nT = Type(float, int)\n\n@boost\ndef func(a: T):\n    b = 1\n    return a * np.sin(b)\n'"
doc/examples/packages/package_simple/package_simple/_version.py,0,"b'\n__version__ = ""0.0.1""'"
doc/examples/packages/package_simple/package_simple/calcul.py,2,"b'import numpy as np\n\nfrom transonic import boost, Type, Array, NDim\n\n\nT = Type(np.int32, np.float64, np.float32)\nA = Array[T, NDim(2)]\n\n\n@boost\ndef laplace(image: A):\n    """"""Laplace operator in NumPy for 2D images.""""""\n    laplacian = (\n        image[:-2, 1:-1]\n        + image[2:, 1:-1]\n        + image[1:-1, :-2]\n        + image[1:-1, 2:]\n        - 4 * image[1:-1, 1:-1]\n    )\n    thresh = np.abs(laplacian) > 0.05\n    return thresh\n'"
doc/examples/packages/package_simple/package_simple/util.py,1,"b'\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom transonic import boost, Type\n\n\nT = Type(float, int)\n\n@boost\ndef func(a: T):\n    b = 1\n    return a * np.sin(b)\n'"
