file_path,api_count,code
prototypes/dp.py,0,b''
prototypes/gaussian_process.py,27,"b""import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.spatial.distance import euclidean\n\n\ndef kernel_rbf(x1, x2, params):\n    return params[0] * np.exp(-0.5 / params[1] * euclidean(x1, x2))\n    \n    \ndef kernel_rbf_d(x1, x2, params):\n    jacobian_mat = np.zeros(len(params), float)\n    d = euclidean(x1, x2)\n    \n    jacobian_mat[0] = np.exp(-0.5 * params[1] * d)\n    if d < 1e-5:\n        jacobian_mat[1] = 1e-5\n    else:\n        jacobian_mat[1] = params[0] * np.exp(-0.5 / (params[1]) * d) * (-0.5 * d)\n\n    return jacobian_mat\n\n\ndef covariance_matrix(X, kernel, params, beta):\n    N, D = X.shape\n    # Computing the covariance matrix\n    C = np.zeros((N, N), dtype=float)\n    for row in range(N):\n        for col in range(N):\n            if col == row:\n                C[row, col] = kernel(X[row], X[col], params) + 1. / beta\n            else:\n                C[row, col] = kernel(X[row], X[col], params)\n    return C\n    \n    \ndef objective_func(params, X, y, kernel, kernel_d, beta):\n    if len(params) < 2:\n        pass\n    N, D = X.shape\n    log_beta = np.log(beta)\n    C = covariance_matrix(X, kernel, params, beta)\n    llh =  -0.5 * np.log(np.linalg.det(C)) - 0.5 * \\\n    np.dot( np.dot( y.T, np.linalg.inv(C)), y) - N / 2. * np.log(2 * np.pi) + \\\n    0.5 * (log_beta * log_beta)\n    \n    print( 'objective: ', llh )\n    return -llh\n    \n    \ndef objective_func_derivatives(params, X, y, kernel, kernel_d, beta):\n    N, D = X.shape\n    n_params = len(params)\n    d_llh = np.zeros(n_params)\n    dC = np.zeros((N, N, n_params))\n    C = covariance_matrix(X, kernel, params, beta)\n    iC = np.linalg.inv(C)\n    \n    for row in range(N):\n        for col in range(N):\n            dC[row, col] = kernel_d(X[row], X[col], params)\n    \n    for i in range(n_params):\n        d_llh[i] = -0.5 * np.trace(np.dot(iC, dC[:, :, i])) + 0.5 * \\\n        np.dot(np.dot(np.dot(np.dot( y.T, iC ), dC[:, :, i]), iC), y)\n        \n    return -d_llh\n    \n\nclass GaussianProcess(object):\n    def __init__(self, kernel, beta, normalize=True):\n        self._C = None\n        self._X = None\n        self._y = None\n        self._normalize = normalize\n        \n        if kernel == 'rbf':\n            self._kernel = kernel_rbf\n            self._kernel_d = kernel_rbf_d\n            self._n_params = 2\n            \n        self._beta = beta\n        self._params = None\n        \n    def fit(self, X, y, init_params=None):\n        if init_params is None:        \n            init_params = np.zeros(self._n_params) + 0.5\n\n        if self._normalize:\n            X_mean = np.mean(X, axis=0)\n            X_std = np.std(X, axis=0)\n            y_mean = np.mean(y, axis=0)\n            y_std = np.std(y, axis=0)\n            X_std[X_std == 0.] = 1.\n            y_std[y_std == 0.] = 1.\n            # center and scale X if necessary\n            X = (X - X_mean) / X_std\n            y = (y - y_mean) / y_std\n                    \n        # y = y[:, 0]\n        \n        # estimating the parameters\n        result = minimize(objective_func, init_params, \\\n            args=(X, y, self._kernel, self._kernel_d, self._beta), \\\n            method='BFGS', jac=objective_func_derivatives, \\\n            options={'disp': True, 'gtol': 1})\n        \n        self._params = result.x\n        \n        # Calculating covariance matrix with the learned parameters\n        C = covariance_matrix(X, self._kernel, self._params, self._beta)\n        self._C = C\n        self._X = X\n        self._y = y\n    \n    def predict(self, X):\n        N, D = X.shape\n        ks = np.zeros((N, self._X.shape[0]))\n        \n        for row in range(N):\n            for col in range(self._X.shape[0]):\n                ks[row, col] = self._kernel(X[row], self._X[col], self._params)\n            \n        t_means = np.zeros(N)\n        t_covs = np.zeros(N)\n        iC = np.linalg.inv(self._C)\n        for i in range(N):\n            t_means[i] = np.dot( np.dot( ks[i].T, iC), self._y)\n            t_covs[i] = self._kernel(X[i], X[i], self._params) + self._beta - \\\n                np.dot( np.dot( ks[i].T, iC), ks[i])\n        \n        return t_means, t_covs\n        \n    def score(self, X, y):\n        t_means, t_covs = self.predict(X)\n        return np.abs(t_means - y).sum()\n\n\nif __name__ == '__main__':\n    import sklearn\n    from sklearn import datasets\n    from sklearn.cross_validation import train_test_split\n    from sklearn.preprocessing import MinMaxScaler\n    \n    data = datasets.load_boston()\n    \n    X = data['data']\n    # mms = MinMaxScaler()\n    # X = mms.fit_transform(X)\n    y = data['target'][:, np.newaxis]\n    # y = y / y.max()\n    \n    Xtrain, Xtest, ytrain, ytest = \\\n        train_test_split(X, y, test_size=0.4, random_state=None)\n    \n    gp = GaussianProcess(kernel='rbf', beta=1.0, normalize=False)\n    gp.fit(Xtrain, ytrain, init_params=np.array([1, 15]))\n    # print(gp.score(Xtest, ytest))"""
prototypes/test_george.py,5,"b""import sys\nsys.path.append('/Users/phanquochuy/Projects/minimind/prototypes/george/build/lib.macosx-10.10-x86_64-2.7')\nimport numpy as np\nimport george\nfrom george.kernels import ExpSquaredKernel\n\n# Generate some fake noisy data.\nx = 10 * np.sort(np.random.rand(10))\nyerr = 0.2 * np.ones_like(x)\ny = np.sin(x) + yerr * np.random.randn(len(x))\n\n# Set up the Gaussian process.\nkernel = ExpSquaredKernel(1.0)\ngp = george.GP(kernel)\n\n# Pre-compute the factorization of the matrix.\ngp.compute(x, yerr)\ngp.optimize(x, y, verbose=True)\n\n# Compute the log likelihood.\nprint(gp.lnlikelihood(y))\n\nt = np.linspace(0, 10, 500)\nmu, cov = gp.predict(y, t)\nstd = np.sqrt(np.diag(cov))"""
prototypes/test_gpy.py,0,"b'""""""Run a standard Gaussian process regression on the Rogers and Girolami olympics data.""""""\nimport GPy\ndata = GPy.util.datasets.olympic_100m_men()\noptimize = True\nplot = True\n\n# create simple GP Model\nm = GPy.models.GPRegression(data[\'X\'], data[\'Y\'])\n\n# set the lengthscale to be something sensible (defaults to 1)\nm[\'rbf.lengthscale\'] = 10\nif optimize:\n    m.optimize(\'bfgs\', max_iters=200)\n\nif plot:\n    m.plot(plot_limits=(1850, 2050))\n'"
prototypes/test_image_upsampling.py,9,"b""import numpy as np\nimport scipy as sp \nfrom scipy.stats import multivariate_normal\nfrom matplotlib import pyplot as plt\n\ndef k(x, a = -0.75):\n    if abs(x) <= 1:\n        return (a + 2.) * abs(x)**3 - (a + 3.) * abs(x) ** 2. + 1\n    elif 1 < abs(x) <= 2:\n        return a * abs(x) ** 3. - 5. * a * abs(x) **2. + 8. * a * abs(x) - 4. * a \n    else:\n        return 0.0\n        \n# x, y = np.mgrid[-1:1:.01, -1:1:.01]\n# pos = np.empty(x.shape + (2,))\n# pos[:, :, 0] = x; pos[:, :, 1] = y\n\n# rv = multivariate_normal([0.0, 0.0], [[2.0, 0.3], [0.3, 2.0]])\n# plt.contourf(x, y, rv.pdf(pos))\n\nA = np.random.rand(5, 5)\nB = A.copy()\n\n# A = np.kron(A, np.ones((2, 2), dtype=float))\nalpha = 5\nC = np.zeros((A.shape[0] * alpha, A.shape[1] * alpha), dtype=float)\nfor i in range(A.shape[0]): \n    for j in range(A.shape[1]):\n        C[i * alpha, j * alpha] = A[i, j]\n\n# K = np.array([k(xx) for xx in [-2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2]])\nK = np.array([k(xx) for xx in np.linspace(-2, 2, 11)])\nfor r in range(C.shape[0]):\n    C[r, :] = np.convolve(C[r], K, 'same')\n\nfor c in range(A.shape[1]):\n    C[:, c] = np.convolve(C[:, c].flatten(), K, 'same')\n\nplt.subplot(121)\nplt.imshow(C, interpolation='bicubic')\n\nplt.subplot(122)\nplt.imshow(C, interpolation='nearest')\nplt.show()"""
