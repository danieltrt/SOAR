file_path,api_count,code
decisionTree.py,0,"b'import pandas as pd \nimport numpy as np \nimport math\n\n# Handle training data so it can be loaded once, then referenced from there\n# Make any alteracations to number of features used in training data\n# Read in original data and get subset table with columns:\n## Is_Home_or_Away\n## Is_Opponent_in_AP25_Preseason\n## Label\nDF_TRAIN = pd.read_csv(\'Dataset-football-train.txt\',sep=\'\\t\')\nDF_TRAIN = DF_TRAIN[[\'Is_Home_or_Away\',\'Is_Opponent_in_AP25_Preseason\',\'Media\',\'Label\']]\n\n\nclass Tree:\n\tdef __init__(self,observationIDs,features,currLvl=0,subTree={},bestFeature=None,majorityLabel=None,parentMajorityLabel=None):\n\t\tself.observationIDs = observationIDs\n\t\tself.features = features\n\t\tself.currLvl = currLvl\n\t\tself.subTree = subTree\n\t\tself.bestFeature = bestFeature\n\t\tself.majorityLabel = majorityLabel\n\t\tself.parentMajorityLabel = parentMajorityLabel\n\t\tself.setBestFeatureID(bestFeature)\n\n\t# predicts using a tree and \n\t# observation: [Is_Home_or_Away, Is_Opponent_in_AP25_Preseason, Media]\n\n\tdef setBestFeatureID(self, feature):\n\t\tidx = None\n\t\tif feature == \'Is_Home_or_Away\':\n\t\t\tidx = 0\n\t\telif feature == \'Is_Opponent_in_AP25_Preseason\':\n\t\t\tidx = 1\n\t\telse:\n\t\t\tidx = 2\n\t\tself.bestFeatureID = int(idx)\n\ndef predict(tree, obs):\n\tif tree.bestFeature == None:\n\t\treturn tree.majorityLabel\n\tfeatVal = obs[tree.bestFeatureID]\n\tif not featVal in tree.subTree: # val with no subtree\n\t\treturn tree.majorityLabel\n\telse: # recurse on subtree\n\t\treturn predict(tree.subTree[featVal],obs)\n\ndef displayDecisionTree(tree):\n\tprint(\'\\t\'*tree.currLvl + \'(lvl {}) {}\'.format(tree.currLvl,tree.majorityLabel))\n\tif tree.bestFeature == None:\n\t\treturn\n\n\tprint(\'\\t\'*tree.currLvl + \'{}\'.format(tree.bestFeature) + \': \')\n\tfor [val,subTree] in sorted(tree.subTree.items()):\n\t\tprint(\'\\t\'*(tree.currLvl+1) + \'choice: {}\'.format(val))\n\t\tdisplayDecisionTree(subTree)\n\ndef Entropy(ns):\n\tentropy = 0.0\n\ttotal = sum(ns)\n\tfor x in ns:\n\t\tentropy += -1.0*x/total*math.log(1.0*x/total,2)\n\treturn entropy\n\n# Information Gain\ndef IG(observationIDs, feature):\n\t# get smaller dataframe\n\tdf = DF_TRAIN.loc[observationIDs]\n\t# populate counts for Wins/Losses for each category of the feature\n\tlabelCountDict = {}\n\tvalueLabelCountDict = {}\n\tfor index, row in df.iterrows():\n\t\tlabel = row[\'Label\']\n\t\tif not label in labelCountDict:\n\t\t\tlabelCountDict[label] = 0 # this specific label was not found so insert 0 count\n\t\tlabelCountDict[label] += 1\n\t\tfeatureValue = row[feature]\n\t\tif not featureValue in valueLabelCountDict:\n\t\t\tvalueLabelCountDict[featureValue] = {} # this specific feature value not found so insert empty dict\n\t\tif not label in valueLabelCountDict[featureValue]:\n\t\t\tvalueLabelCountDict[featureValue][label] = 0 # this specific label was not found for this feature value so insert 0 count\n\t\tvalueLabelCountDict[featureValue][label] += 1\n\n\tns = []\n\tfor [label,count] in labelCountDict.items():\n\t\tns.append(count)\n\n\tH_Y = Entropy(ns)\n\n\tH_Y_X = 0.0\n\tfor [featureValue, labelCountDict] in valueLabelCountDict.items():\n\t\tnsHYX = []\n\t\tfor [label,count] in labelCountDict.items():\n\t\t\tnsHYX.append(count)\n\t\tH_Y_X += 1.0*sum(nsHYX)/len(df)*Entropy(nsHYX)\n\treturn H_Y - H_Y_X\n\ndef GR(observationIDs, feature):\n\tig = IG(observationIDs,feature)\n\tif ig == 0:\n\t\treturn 0\n\tdf = DF_TRAIN.loc[observationIDs]\n\tvalueLabelDict = {}\n\tfor index, row in df.iterrows():\n\t\tlabel = row[\'Label\']\n\t\tfeatureValue = row[feature]\n\t\tif featureValue not in valueLabelDict:\n\t\t\tvalueLabelDict[featureValue] = 0\n\t\tvalueLabelDict[featureValue] += 1\n\tns = []\n\tfor [val,count] in valueLabelDict.items():\n\t\tns.append(count)\n\tent = Entropy(ns)\n\treturn float(ig)/ent\n\ndef fillDecisionTree(tree,decisionTreeAlgo):\n\t# find the majorityLabel\n\tdf = DF_TRAIN.loc[tree.observationIDs] # smaller df\n\tcounts = df[\'Label\'].value_counts()\n\tmajorityLabel = df[\'Label\'].value_counts().idxmax()\n\tif len(counts) > 1:\n\t\tif counts[\'Win\'] == counts[\'Lose\']:\n\t\t\tmajorityLabel = tree.parentMajorityLabel\n\ttree.majorityLabel = majorityLabel\n\n\t# exit if only one label\n\tif len(counts) == 1:\n\t\treturn\n\t# exit if no features left\n\tif len(tree.features) == 0: \n\t\treturn\n\n\t# find best feature\n\tfeatureValueDict = {}\n\tfor feature in tree.features: \n\t\tif decisionTreeAlgo == \'ID3\':\n\t\t\tmetricScore = IG(tree.observationIDs,feature)\n\t\tif decisionTreeAlgo == \'C45\':\n\t\t\tmetricScore = GR(tree.observationIDs,feature)\n\t\tfeatureValueDict[feature] = metricScore\n\tbestFeature, bestFeatureValue = sorted(featureValueDict.items(),reverse=True)[0]\n\t# exit if IG or GR is 0\n\tif bestFeatureValue == 0.0:\n\t\treturn\n\ttree.bestFeature = bestFeature\n\n\t# find subset of features\n\tsubFeatures = set()\n\tfor feature in tree.features:\n\t\tif feature == bestFeature: # skip the current best feature\n\t\t\tcontinue\n\t\tsubFeatures.add(feature)\n\t\n\t# find best feature id\n\tbestFeatureIdx = 0\n\tif bestFeature == \'Is_Home_or_Away\':\n\t\tbestFeatureIdx = 0\n\telif bestFeature == \'Is_Opponent_in_AP25_Preseason\':\n\t\tbestFeatureIdx = 1\n\telse:\n\t\tbestFeatureIdx = 2\n\t\n\t# find subset of observations\n\tsubObservationsDict = {}\n\tfor obs in tree.observationIDs:\n\t\tval = DF_TRAIN.values[obs][bestFeatureIdx]\n\t\tif not val in subObservationsDict:\n\t\t\tsubObservationsDict[val] = set()\n\t\tsubObservationsDict[val].add(obs)\n\n\tfor [val,obs] in subObservationsDict.items():\n\n\t\ttree.subTree[val] = Tree(obs, subFeatures, tree.currLvl + 1,{},None,None,majorityLabel)\n\t\t\n\t\tfillDecisionTree(tree.subTree[val],decisionTreeAlgo)\n\ndef predictAndAnalyze(tree, data):\n\tTP = 0\n\tFN = 0\n\tFP = 0\n\tTN = 0\n\tfor obs in data:\n\t\tprediction = predict(tree,obs)\n\t\tground = obs[3]\n\t\tif prediction == \'Win\' and ground == \'Win\':\n\t\t\tTP += 1\n\t\tif prediction == \'Win\' and ground == \'Lose\':\n\t\t\tFP += 1\n\t\tif prediction == \'Lose\' and ground == \'Win\':\n\t\t\tFN += 1\n\t\tif prediction == \'Lose\' and ground == \'Lose\':\n\t\t\tTN += 1\n\n\taccuracy = float(TP+TN)/len(data)\n\tprecision = float(TP)/(TP + FP)\n\trecall = float(TP)/(TP + FN)\n\tF1 = 2*(recall*precision)/(recall+precision)\n\tprint(\'\\nAnalysis:\')\n\tprint(\'accuracy = {}\'.format(accuracy))\n\tprint(\'precision = {}\'.format(precision))\n\tprint(\'recall = {}\'.format(recall))\n\tprint(\'F1 score = {}\'.format(F1))\n\n\n# read in original data and get subset table with columns:\n## Is_Home_or_Away\n## Is_Opponent_in_AP25_Preseason\n## Label\ndfTest = pd.read_csv(\'Dataset-football-test.txt\',sep=\'\\t\')\ndfTest = dfTest[[\'Is_Home_or_Away\',\'Is_Opponent_in_AP25_Preseason\',\'Media\',\'Label\']]\n\n# obsIDs, features, lvl subTree, bestFeature, majority label, parent majority label\ninitialObservationIDs = set(range(len(DF_TRAIN)))\ninitialFeatures = set(dfTest.columns.values[:-1])\n\n# prompt user\nprint(""Which decision tree algorithm would you like to use (\'ID3\' or \'C45)?"")\nalgoChoice = str(raw_input())\nif algoChoice not in {\'ID3\',\'C45\'}:\n\tprint(""Invalid algorithm choice. You must choose \'ID3\' or \'C45\'"")\n\texit()\n\nprint(""choice: {}"".format(algoChoice))\n\nMyTree = Tree(initialObservationIDs,initialFeatures)\nfillDecisionTree(MyTree,algoChoice)\n\nprint(\'My Decision Tree:\')\ndisplayDecisionTree(MyTree)\n\n\nprint(\'Predicted Labels of Test Data:\')\npredictAndAnalyze(MyTree,dfTest.values)\n\n'"
