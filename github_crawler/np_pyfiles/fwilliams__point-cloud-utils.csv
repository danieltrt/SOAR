file_path,api_count,code
setup.py,0,"b'import setuptools\nimport os\nimport re\nimport sys\nimport platform\nimport subprocess\nimport warnings\n\nfrom distutils.version import LooseVersion\nfrom setuptools.command.build_ext import build_ext\n\n\nclass CMakeExtension(setuptools.Extension):\n    def __init__(self, name, sourcedir=\'\', cmake_args=(), exclude_arch=False):\n        setuptools.Extension.__init__(self, name, sources=[])\n        self.sourcedir = os.path.abspath(sourcedir)\n        self.cmake_args = cmake_args\n        self.exclude_arch = exclude_arch\n\n\nclass CMakeBuild(build_ext):\n    def run(self):\n        if os.path.exists(\'.git\'):\n            subprocess.check_call([\'git\', \'submodule\', \'update\', \'--init\', \'--recursive\'])\n\n        try:\n            out = subprocess.check_output([\'cmake\', \'--version\'])\n        except OSError:\n            raise RuntimeError(\n                ""CMake must be installed to build the following extensions: "" +\n                "", "".join(e.name for e in self.extensions))\n\n        if platform.system() == ""Windows"":\n            cmake_version = LooseVersion(re.search(r\'version\\s*([\\d.]+)\', out.decode()).group(1))\n            if cmake_version < \'3.2.0\':\n                raise RuntimeError(""CMake >= 3.2.0 is required on Windows"")\n\n        for ext in self.extensions:\n            self.build_extension(ext)\n\n    def build_extension(self, ext):\n        extdir = os.path.abspath(os.path.dirname(self.get_ext_fullpath(ext.name)))\n        extdir = os.path.join(extdir, ""point_cloud_utils"")\n        cmake_args = [\'-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=\' + extdir, \'-DPYTHON_EXECUTABLE=\' + sys.executable]\n        cmake_args.extend(ext.cmake_args)\n\n        cfg = \'Debug\' if self.debug else \'Release\'\n        build_args = [\'--config\', cfg]\n\n        if cfg == \'Debug\':\n            warnings.warn(""Building extension %s in debug mode"" % ext.name)\n\n        if platform.system() == ""Windows"":\n            cmake_args += [\'-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_{}={}\'.format(cfg.upper(), extdir)]\n            if os.environ.get(\'CMAKE_GENERATOR\') != ""NMake Makefiles"":\n                if sys.maxsize > 2 ** 32 and not ext.exclude_arch:\n                    cmake_args += [\'-A\', \'x64\']\n                build_args += [\'--\', \'/m\']\n        else:\n            cmake_args += [\'-DCMAKE_BUILD_TYPE=\' + cfg]\n            build_args += [\'--\', \'-j2\']\n\n        env = os.environ.copy()\n        env[\'CXXFLAGS\'] = \'{} -DVERSION_INFO=\\\\""{}\\\\""\'.format(env.get(\'CXXFLAGS\', \'\'), self.distribution.get_version())\n        if not os.path.exists(self.build_temp):\n            os.makedirs(self.build_temp)\n        subprocess.check_call([\'cmake\'] + cmake_args + [ext.sourcedir], cwd=self.build_temp, env=env)\n        subprocess.check_call([\'cmake\', \'--build\', \'.\'] + build_args, cwd=self.build_temp)\n        print()  # Add an empty line for cleaner output\n\n\ndef main():\n    with open(""README.md"", ""r"") as fh:\n        long_description = fh.read()\n\n    cmake_args = []\n    exclude_arch = False\n    if \'USE_MKL\' in os.environ or \'--use-mkl\' in sys.argv:\n        cmake_args.append(\'-DEIGEN_WITH_MKL=ON\')\n        sys.argv.remove(\'--use-mkl\')\n    if \'EXCLUDE_ARCH\' in os.environ or \'--exclude-arch\' in sys.argv:\n        exclude_arch = True\n        sys.argv.remove(\'--exclude-arch\')\n\n    setuptools.setup(\n        name=""point-cloud-utils"",\n        version=""0.15.1"",\n        author=""Francis Williams"",\n        author_email=""francis@fwilliams.info"",\n        description=""A Python Library of utilities for point clouds"",\n        long_description=long_description,\n        long_description_content_type=""text/markdown"",\n        url=""https://github.com/fwilliams/py-sample-mesh"",\n        packages=setuptools.find_packages(),\n        classifiers=[\n            ""Programming Language :: C++"",\n            ""Programming Language :: Python :: 3"",\n            ""Programming Language :: Python :: 2.7"",\n            ""License :: OSI Approved :: GNU General Public License v3 (GPLv3)"",\n        ],\n        ext_modules=[CMakeExtension(\'point_cloud_utils\', cmake_args=cmake_args, exclude_arch=exclude_arch)],\n        cmdclass=dict(build_ext=CMakeBuild),\n        zip_safe=False,\n        install_requires=[\n            \'numpy\',\n            \'scipy\'\n        ],\n        test_suite=""tests""\n    )\n\n\nif __name__ == ""__main__"":\n    main()\n\n'"
examples/plot_morton_knn_2d.py,4,"b'import point_cloud_utils as pcu\nimport numpy as np\nfrom mayavi import mlab\nfrom scipy.spatial import cKDTree\nk = 50\n\ndiv = 1.0 / 1000.0\npts = (np.random.rand(10_000, 3) / div).astype(np.int32)\nqpts = (np.random.rand(100, 3) / div).astype(np.int32)\npts[:, 2] = 0.0\nqpts[:, 2] = 0.0\n\ncodes = pcu.morton_encode(pts)\ncodes_sorted_idx = np.argsort(codes)\ncodes_sorted = codes[codes_sorted_idx]\n\nqcodes = pcu.morton_encode(qpts)\n\nnn_idx = pcu.morton_knn(codes_sorted, qcodes, k)\nnn_pts = pcu.morton_decode(codes_sorted[nn_idx[0]])\n\nkdt = cKDTree(pts)\n_, nn_gt_idx = kdt.query(np.array([qpts[0]]), k=k)\nprint(nn_gt_idx.shape)\nnn_gt_pts = pts[nn_gt_idx[0]]\n\nmlab.points3d(pts[:, 0], pts[:, 1], pts[:, 2], scale_factor=5.0)\nmlab.points3d([qpts[0, 0]], [qpts[0, 1]], [qpts[0, 2]], scale_factor=7.0, color=(1.0, 0.0, 0.0))\nmlab.points3d(nn_pts[:, 0], nn_pts[:, 1], nn_pts[:, 2], scale_factor=9.0, color=(0.0, 0.0, 1.0), opacity=0.5)\nmlab.points3d(nn_gt_pts[:, 0], nn_gt_pts[:, 1], nn_gt_pts[:, 2], scale_factor=9.0, color=(0.0, 1.0, 0.0), opacity=0.5)\nmlab.show()\n'"
examples/plot_morton_knn_3d.py,4,"b'import point_cloud_utils as pcu\nimport numpy as np\nfrom mayavi import mlab\nfrom scipy.spatial import cKDTree\nk = 50\n\ndiv = 1.0 / 1000.0\npts = (np.random.rand(10_000, 3) / div).astype(np.int32)\nqpts = (np.random.rand(100, 3) / div).astype(np.int32)\n\ncodes = pcu.morton_encode(pts)\ncodes_sorted_idx = np.argsort(codes)\ncodes_sorted = codes[codes_sorted_idx]\n\nqcodes = pcu.morton_encode(qpts)\n\nnn_idx = pcu.morton_knn(codes_sorted, qcodes, k)\nnn_pts = pcu.morton_decode(codes_sorted[nn_idx[0]])\n\nkdt = cKDTree(pts)\n_, nn_gt_idx = kdt.query(np.array([qpts[0]]), k=k)\nprint(nn_gt_idx.shape)\nnn_gt_pts = pts[nn_gt_idx[0]]\n\nmlab.points3d(pts[:, 0], pts[:, 1], pts[:, 2], scale_factor=5.0)\nmlab.points3d([qpts[0, 0]], [qpts[0, 1]], [qpts[0, 2]], scale_factor=7.0, color=(1.0, 0.0, 0.0))\nmlab.points3d(nn_pts[:, 0], nn_pts[:, 1], nn_pts[:, 2], scale_factor=9.0, color=(0.0, 0.0, 1.0), opacity=0.5)\nmlab.points3d(nn_gt_pts[:, 0], nn_gt_pts[:, 1], nn_gt_pts[:, 2], scale_factor=9.0, color=(0.0, 1.0, 0.0), opacity=0.5)\nmlab.show()\n'"
examples/test_morton_different_sizes.py,20,"b'import point_cloud_utils as pcu\nimport numpy as np\n\n# Lots of data\ndiv = 1.0 / 1000.0\npts = np.random.rand(1_000_000, 3) / div\npts_int = pts.astype(np.int32)\n\nqpts = np.random.rand(10_000, 3) / div\nqpts_int = qpts.astype(np.int32)\n\ncodes = pcu.morton_encode(pts_int)\ncodes_sort_idx = np.argsort(codes)\ncodes_sorted = codes[codes_sort_idx]\n\nqcodes = pcu.morton_encode(qpts_int)\n\nnn_idx = pcu.morton_knn(codes_sorted, qcodes, 7)\ncodes_sorted[nn_idx]\nprint(""0 --------------------------------"")\nprint(nn_idx.shape)\nprint(nn_idx)\nprint(""\\n"")\n\n# 1nn\ndiv = 1.0 / 1000.0\npts = np.random.rand(1_000_000, 3) / div\npts_int = pts.astype(np.int32)\n\nqpts = np.random.rand(10_000, 3) / div\nqpts_int = qpts.astype(np.int32)\n\ncodes = pcu.morton_encode(pts_int)\ncodes_sort_idx = np.argsort(codes)\ncodes_sorted = codes[codes_sort_idx]\n\nqcodes = pcu.morton_encode(qpts_int)\n\nnn_idx = pcu.morton_knn(codes_sorted, qcodes, 1)\nprint(""1 --------------------------------"")\nprint(nn_idx.shape)\nprint(nn_idx)\nprint(""\\n"")\n\n\n# Tiny amount of data to hit the boundary cases\ndiv = 1.0 / 1000.0\npts = np.random.rand(10, 3) / div\npts_int = pts.astype(np.int32)\n\nqpts = np.random.rand(10_000, 3) / div\nqpts_int = qpts.astype(np.int32)\n\ncodes = pcu.morton_encode(pts_int)\ncodes_sort_idx = np.argsort(codes)\ncodes_sorted = codes[codes_sort_idx]\n\nqcodes = pcu.morton_encode(qpts_int)\n\nnn_idx = pcu.morton_knn(codes_sorted, qcodes, 7)\nprint(""2 --------------------------------"")\nprint(nn_idx.shape)\nprint(nn_idx)\nprint(""\\n"")\n\n\n# k > data\ndiv = 1.0 / 1000.0\npts = np.random.rand(10, 3) / div\npts_int = pts.astype(np.int32)\n\nqpts = np.random.rand(10_000, 3) / div\nqpts_int = qpts.astype(np.int32)\n\ncodes = pcu.morton_encode(pts_int)\ncodes_sort_idx = np.argsort(codes)\ncodes_sorted = codes[codes_sort_idx]\n\nqcodes = pcu.morton_encode(qpts_int)\n\nnn_idx = pcu.morton_knn(codes_sorted, qcodes, 15)\nprint(""3 --------------------------------"")\nprint(nn_idx.shape)\nprint(nn_idx)\n'"
examples/test_morton_timings.py,5,"b'import point_cloud_utils as pcu\nimport numpy as np\nimport time\n\nstart_time = time.time()\n\ndiv = 1.0 / 1000.0\n\npts = np.random.rand(1_000_000, 3) / div\npts_int = pts.astype(np.int32)\n\nqpts = np.random.rand(10_000, 3) / div\nqpts_int = qpts.astype(np.int32)\n\ninit_time = time.time()\n\n\ncodes = pcu.morton_encode(pts_int, sort=False)\ncodes_time = time.time()\n\nidxs = np.argsort(codes)\nsort_time1 = time.time()\ncodes = codes[idxs]\nsort_time2 = time.time()\n\nqcodes = pcu.morton_encode(qpts_int)\nqcodes_time = time.time()\n\nnn_idx = pcu.morton_knn(codes, qcodes, k=20)\nknn_time = time.time()\n\nprint(nn_idx.shape)\n\nprint(""Timing results: "")\nprint(""  Init time %f ms"" % (1000 * (init_time - start_time)))\nprint(""  Codes time %f ms"" % (1000 * (codes_time - init_time)))\nprint(""  Sort time 1 %f ms"" % (1000 * (sort_time1 - codes_time)))\nprint(""  Sort time 2 %f ms"" % (1000 * (sort_time2 - sort_time1)))\nprint(""  QCodes time %f ms"" % (1000 * (qcodes_time - sort_time2)))\nprint(""  KNN time %f ms"" % (1000 * (knn_time - qcodes_time)))\n'"
point_cloud_utils/__init__.py,0,b'from  .pcu_internal import *\nfrom .sinkhorn import *\n'
point_cloud_utils/sinkhorn.py,25,"b'import numpy as np\n\ndef pairwise_distances(a, b, p=2):\n    """"""\n    Compute the pairwise distance matrix between a and b which both have size [m, n, d] or [n, d]. The result is a tensor of\n    size [m, n, n] (or [n, n]) whose entry [m, i, j] contains the distance_tensor between a[m, i, :] and b[m, j, :].\n    :param a: A tensor containing m batches of n points of dimension d. i.e. of size [m, n, d]\n    :param b: A tensor containing m batches of n points of dimension d. i.e. of size [m, n, d]\n    :param p: Norm to use for the distance_tensor\n    :return: A tensor containing the pairwise distance_tensor between each pair of inputs in a batch.\n    """"""\n\n    squeezed = False\n    if len(a.shape) == 2 and len(b.shape) == 2:\n       a = a[np.newaxis, :, :]\n       b = b[np.newaxis, :, :]\n       squeezed = True\n       \n    if len(a.shape) != 3:\n        raise ValueError(""Invalid shape for a. Must be [m, n, d] or [n, d] but got"", a.shape)\n    if len(b.shape) != 3:\n        raise ValueError(""Invalid shape for a. Must be [m, n, d] or [n, d] but got"", b.shape)\n\n    ret = np.power(np.abs(a[:, :, np.newaxis, :] - b[:, np.newaxis, :, :]), p).sum(3)\n    if squeezed:\n        ret = np.squeeze(ret)\n\n    return ret\n\n\ndef chamfer(a, b):\n    """"""\n    Compute the chamfer distance between two sets of vectors, a, and b\n    :param a: A m-sized minibatch of point sets in R^d. i.e. shape [m, n_a, d]\n    :param b: A m-sized minibatch of point sets in R^d. i.e. shape [m, n_b, d]\n    :return: A [m] shaped tensor storing the Chamfer distance between each minibatch entry\n    """"""\n    M = pairwise_distances(a, b)\n    if len(M.shape) == 2:\n        M = M[np.newaxis, :, :]\n    return M.min(1).sum(1) + M.min(2).sum(1)\n\n\ndef sinkhorn(a, b, M, eps, max_iters=100, stop_thresh=1e-3):\n    """"""\n    Compute the Sinkhorn divergence between two sum of dirac delta distributions, U, and V.\n    This implementation is numerically stable with float32.\n    :param a: A m-sized minibatch of weights for each dirac in the first distribution, U. i.e. shape = [m, n]\n    :param b: A m-sized minibatch of weights for each dirac in the second distribution, V. i.e. shape = [m, n]\n    :param M: A minibatch of n-by-n tensors storing the distance between each pair of diracs in U and V.\n             i.e. shape = [m, n, n] and each i.e. M[k, i, j] = ||u[k,_i] - v[k, j]||\n    :param eps: The reciprocal of the sinkhorn regularization parameter\n    :param max_iters: The maximum number of Sinkhorn iterations\n    :param stop_thresh: Stop if the change in iterates is below this value\n    :return:\n    """"""\n    # a and b are tensors of size [nb, m] and [nb, n]\n    # M is a tensor of size [nb, m, n]\n\n    M = np.squeeze(M)\n    a = np.squeeze(a)\n    b = np.squeeze(b)\n    squeezed = False\n\n    if len(M.shape) == 2 and len(a.shape) == 1 and len(b.shape) == 1:\n        M = M[np.newaxis, :, :]\n        a = a[np.newaxis, :]\n        b = b[np.newaxis, :]\n        squeezed = True\n    elif len(M.shape) == 2 and len(a.shape) != 1:\n        raise ValueError(""Invalid shape for a %s, expected [m,] where m is the number of samples in a and ""\n                         ""M has shape [m, n]"" % str(a.shape))\n    elif len(M.shape) == 2 and len(b.shape) != 1:\n        raise ValueError(""Invalid shape for a %s, expected [m,] where n is the number of samples in a and ""\n                         ""M has shape [m, n]"" % str(b.shape))\n\n    if len(M.shape) != 3:\n        raise ValueError(""Got unexpected shape for M %s, should be [nb, m, n] where nb is batch size, and ""\n                         ""m and n are the number of samples in the two input measures."" % str(M.shape))\n    elif len(M.shape) == 3 and len(a.shape) != 2:\n        raise ValueError(""Invalid shape for a %s, expected [nb, m]  where nb is batch size, m is the number of samples ""\n                         ""in a and M has shape [nb, m, n]"" % str(a.shape))\n    elif len(M.shape) == 3 and len(b.shape) != 2:\n        raise ValueError(""Invalid shape for a %s, expected [nb, m]  where nb is batch size, m is the number of samples ""\n                         ""in a and M has shape [nb, m, n]"" % str(b.shape))\n\n    nb = M.shape[0]\n    m = M.shape[1]\n    n = M.shape[2]\n\n    if a.dtype != b.dtype or a.dtype != M.dtype:\n        raise ValueError(""Tensors a, b, and M must have the same dtype got: dtype(a) = %s, dtype(b) = %s, dtype(M) = %s""\n                         % (str(a.dtype), str(b.dtype), str(M.dtype)))\n    if a.shape != (nb, m):\n        raise ValueError(""Got unexpected shape for tensor a (%s). Expected [nb, m] where M has shape [nb, m, n]."" %\n                         str(a.shape))\n    if b.shape != (nb, n):\n        raise ValueError(""Got unexpected shape for tensor b (%s). Expected [nb, n] where M has shape [nb, m, n]."" %\n                         str(b.shape))\n\n    u = np.zeros_like(a)\n    v = np.zeros_like(b)\n\n    M_t = np.transpose(M, axes=(0, 2, 1))\n\n    def stabilized_log_sum_exp(x):\n        max_x = x.max(2)\n        x = x - max_x[:, :, np.newaxis]\n        ret = np.log(np.sum(np.exp(x), axis=2)) + max_x\n        return ret\n\n    for current_iter in range(max_iters):\n        u_prev = u\n        v_prev = v\n\n        summand_u = (-M + np.expand_dims(v, 1)) / eps\n        u = eps * (np.log(a) - stabilized_log_sum_exp(summand_u))\n\n        summand_v = (-M_t + np.expand_dims(u, 1)) / eps\n        v = eps * (np.log(b) - stabilized_log_sum_exp(summand_v))\n\n        err_u = np.sum(np.abs(u_prev-u), axis=1).max()\n        err_v = np.sum(np.abs(v_prev-v), axis=1).max()\n\n        if err_u < stop_thresh and err_v < stop_thresh:\n            break\n\n    log_P = (-M + np.expand_dims(u, 2) + np.expand_dims(v, 1)) / eps\n\n    P = np.exp(log_P)\n\n    if squeezed:\n        P = np.squeeze(P)\n\n    return P\n'"
tests/__init__.py,0,b''
tests/test_examples.py,33,"b'from __future__ import print_function\nimport unittest\nimport os\n\n\nclass TestDenseBindings(unittest.TestCase):\n    def setUp(self):\n        self.test_path = os.path.join(os.path.dirname(\n            os.path.realpath(__file__)), "".."", ""data"")\n\n    def test_poisson_disk_sampling(self):\n        import point_cloud_utils as pcu\n        import numpy as np\n\n        # v is a nv by 3 NumPy array of vertices\n        # f is an nf by 3 NumPy array of face indexes into v\n        # n is a nv by 3 NumPy array of vertex normals if they are specified, otherwise an empty array\n        v, f, n = pcu.read_obj(os.path.join(self.test_path, ""cube_twist.obj""))\n        bbox = np.max(v, axis=0) - np.min(v, axis=0)\n        bbox_diag = np.linalg.norm(bbox)\n\n        # Generate very dense  random samples on the mesh (v, f, n)\n        # Note that this function works with no normals, just pass in an empty array np.array([], dtype=v.dtype)\n        # v_dense is an array with shape (100*v.shape[0], 3) where each row is a point on the mesh (v, f)\n        # n_dense is an array with shape (100*v.shape[0], 3) where each row is a the normal of a point in v_dense\n        v_dense, n_dense = pcu.sample_mesh_random(v, f, n, num_samples=v.shape[0] * 100)\n\n        # Downsample v_dense to be from a blue noise distribution:\n        #\n        # v_poisson is a downsampled version of v where points are separated by approximately\n        # `radius` distance, use_geodesic_distance indicates that the distance should be measured on the mesh.\n        #\n        # n_poisson are the corresponding normals of v_poisson\n        v_poisson, n_poisson = pcu.prune_point_cloud_poisson_disk(v_dense, n_dense, 0.1*bbox_diag)\n\n        v_poisson, n_poisson = pcu.sample_mesh_poisson_disk(\n            v, f, n, num_samples=7777, use_geodesic_distance=True)\n\n        v_poisson, n_poisson = pcu.sample_mesh_poisson_disk(\n            v, f, n, num_samples=-1, radius=0.01*bbox_diag, use_geodesic_distance=True)\n\n\n\n    def test_lloyd_relaxation(self):\n        import point_cloud_utils as pcu\n\n        # v is a nv by 3 NumPy array of vertices\n        # f is an nf by 3 NumPy array of face indexes into v\n        v, f, n = pcu.read_obj(os.path.join(self.test_path, ""cube_twist.obj""))\n\n        # Generate 1000 points on the mesh with Lloyd\'s algorithm\n        samples = pcu.sample_mesh_lloyd(v, f, 1000)\n\n        # Generate 100 points on the unit square with Lloyd\'s algorithm\n        samples_2d = pcu.lloyd_2d(100)\n\n        # Generate 100 points on the unit cube with Lloyd\'s algorithm\n        samples_3d = pcu.lloyd_3d(100)\n\n    def test_sinkhorn(self):\n        import point_cloud_utils as pcu\n        import numpy as np\n\n        # a and b are arrays where each row contains a point\n        # Note that the point sets can have different sizes (e.g [100, 3], [111, 3])\n        a = np.random.rand(100, 3)\n        b = np.random.rand(100, 3)\n\n        # M is a 100x100 array where each entry  (i, j) is the squared distance between point a[i, :] and b[j, :]\n        M = pcu.pairwise_distances(a, b)\n\n        # w_a and w_b are masses assigned to each point. In this case each point is weighted equally.\n        w_a = np.ones(a.shape[0])\n        w_b = np.ones(b.shape[0])\n\n        # P is the transport matrix between a and b, eps is a regularization parameter, smaller epsilons lead to\n        # better approximation of the true Wasserstein distance at the expense of slower convergence\n        P = pcu.sinkhorn(w_a, w_b, M, eps=1e-3)\n\n        # To get the distance as a number just compute the frobenius inner product <M, P>\n        sinkhorn_dist = (M * P).sum()\n\n    def test_batched_sinkhorn(self):\n        import point_cloud_utils as pcu\n        import numpy as np\n\n        # a and b are each contain 10 batches each of which contain 100 points  of dimension 3\n        # i.e. a[i, :, :] is the i^th point set which contains 100 points\n        # Note that the point sets can have different sizes (e.g [10, 100, 3], [10, 111, 3])\n        a = np.random.rand(10, 100, 3)\n        b = np.random.rand(10, 100, 3)\n\n        # M is a 10x100x100 array where each entry (k, i, j) is the squared distance between point a[k, i, :]\n        # and b[k, j, :]\n        M = pcu.pairwise_distances(a, b)\n\n        # w_a and w_b are masses assigned to each point. In this case each point is weighted equally.\n        w_a = np.ones(a.shape[:2])\n        w_b = np.ones(b.shape[:2])\n\n        # P is the transport matrix between a and b, eps is a regularization parameter, smaller epsilons lead to\n        # better approximation of the true Wasserstein distance at the expense of slower convergence\n        P = pcu.sinkhorn(w_a, w_b, M, eps=1e-3)\n\n        # To get the distance as a number just compute the frobenius inner product <M, P>\n        sinkhorn_dist = (M * P).sum()\n\n    def test_chamfer(self):\n        import point_cloud_utils as pcu\n        import numpy as np\n\n        # a and b are arrays where each row contains a point\n        # Note that the point sets can have different sizes (e.g [100, 3], [111, 3])\n        a = np.random.rand(100, 3)\n        b = np.random.rand(100, 3)\n\n        chamfer_dist = pcu.chamfer(a, b)\n\n    def test_batched_chamfer(self):\n        import point_cloud_utils as pcu\n        import numpy as np\n\n        # a and b are each contain 10 batches each of which contain 100 points  of dimension 3\n        # i.e. a[i, :, :] is the i^th point set which contains 100 points\n        # Note that the point sets can have different sizes (e.g [10, 100, 3], [10, 111, 3])\n        a = np.random.rand(10, 100, 3)\n        b = np.random.rand(10, 100, 3)\n\n        chamfer_dist = pcu.chamfer(a, b)\n\n    def test_hausdorff_and_nearest_neighbor(self):\n        import point_cloud_utils as pcu\n        import numpy as np\n\n        # Generate two random point sets\n        a = np.random.rand(1000, 3)\n        b = np.random.rand(500, 3)\n\n        # dists_a_to_b is of shape (a.shape[0],) and contains the shortest squared distance\n        # between each point in a and the points in b\n        # corrs_a_to_b is of shape (a.shape[0],) and contains the index into b of the\n        # closest point for each point in a\n        dists_a_to_b, corrs_a_to_b = pcu.point_cloud_distance(a, b)\n\n        # Compute each one sided squared Hausdorff distances\n        hausdorff_a_to_b = pcu.hausdorff(a, b)\n        hausdorff_b_to_a = pcu.hausdorff(b, a)\n\n        # Take a max of the one sided squared  distances to get the two sided Hausdorff distance\n        hausdorff_dist = max(hausdorff_a_to_b, hausdorff_b_to_a)\n\n        # Find the index pairs of the two points with maximum shortest distancce\n        hausdorff_b_to_a, idx_b, idx_a = pcu.hausdorff(b, a, return_index=True)\n        self.assertAlmostEqual(np.sum((a[idx_a] - b[idx_b])**2), hausdorff_b_to_a)\n\n    def test_estimate_normals(self):\n        import point_cloud_utils as pcu\n        import numpy as np\n\n        # v is a nv by 3 NumPy array of vertices\n        # f is an nf by 3 NumPy array of face indexes into v\n        # n is a nv by 3 NumPy array of vertex normals if they are specified, otherwise an empty array\n        v, f, n = pcu.read_obj(os.path.join(self.test_path, ""cube_twist.obj""))\n\n        # Estimate normals for the point set, v using 12 nearest neighbors per point\n        n = pcu.estimate_normals(v, k=12)\n        self.assertEqual(n.shape, v.shape)\n\n    def test_morton_coding_big_data(self):\n        import point_cloud_utils as pcu\n        import numpy as np\n        import os\n        if os.name == \'nt\':\n            num_pts = 10000\n            num_qpts = 100\n        else:\n            num_pts = 1000000\n            num_qpts = 10000\n        div = 1.0 / 1000.0\n        pts = np.random.rand(num_pts, 3) / div\n        pts_int = pts.astype(np.int32)\n\n        qpts = np.random.rand(num_qpts, 3) / div\n        qpts_int = qpts.astype(np.int32)\n\n        codes = pcu.morton_encode(pts_int)\n        codes_sort_idx = np.argsort(codes)\n        codes_sorted = codes[codes_sort_idx]\n\n        qcodes = pcu.morton_encode(qpts_int)\n\n        nn_idx = pcu.morton_knn(codes_sorted, qcodes, 7)\n        codes_sorted[nn_idx]\n        self.assertEqual(nn_idx.shape, (num_qpts, 7))\n\n    def test_morton_coding_small_data(self):\n        import point_cloud_utils as pcu\n        import numpy as np\n        div = 1.0 / 1000.0\n        pts = np.random.rand(10, 3) / div\n        pts_int = pts.astype(np.int32)\n\n        qpts = np.random.rand(10000, 3) / div\n        qpts_int = qpts.astype(np.int32)\n\n        codes = pcu.morton_encode(pts_int)\n        codes_sort_idx = np.argsort(codes)\n        codes_sorted = codes[codes_sort_idx]\n\n        qcodes = pcu.morton_encode(qpts_int)\n\n        nn_idx = pcu.morton_knn(codes_sorted, qcodes, 7)\n        codes_sorted[nn_idx]\n        self.assertEqual(nn_idx.shape, (10000, 7))\n\n    def test_morton_coding_tiny_data(self):\n        import point_cloud_utils as pcu\n        import numpy as np\n        div = 1.0 / 1000.0\n        pts = np.random.rand(10, 3) / div\n        pts_int = pts.astype(np.int32)\n\n        qpts = np.random.rand(10000, 3) / div\n        qpts_int = qpts.astype(np.int32)\n\n        codes = pcu.morton_encode(pts_int)\n        codes_sort_idx = np.argsort(codes)\n        codes_sorted = codes[codes_sort_idx]\n\n        qcodes = pcu.morton_encode(qpts_int)\n\n        nn_idx = pcu.morton_knn(codes_sorted, qcodes, 15)\n        codes_sorted[nn_idx]\n        self.assertEqual(nn_idx.shape, (10000, 10))\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
external/geogram/tests/lib/VorpatestLibrary.py,0,"b'#\n#      \\V (O |R |P /A |L |I |N |E\n# (C) Bruno Levy, INRIA - ALICE, 2012,2013\n#\n#   Confidential - proprietary software\n#\n# This file contains the RobotFramework test library for executing test\n# programs in the framework of the Vorpaline project.\n#\n\nimport os\nimport shutil\nimport sys\nimport subprocess\nfrom robot.api import logger\nfrom robot.libraries.BuiltIn import BuiltIn\n\n__version__ = \'0.1\'\n\n# These variables are used for testing the VorpatestLibrary\n# as a standalone executable\n_MOCKUP_TEST_VARIABLES = {\n    \'${EXECDIR}\': \'.\',\n    \'${SUITE SOURCE}\': \'.\',\n    \'${TEST NAME}\': \'exec.dir\',\n    \'${TEST STATUS}\': \'PASS\'\n}\n\ndef _get_test_variables():\n    """"""\n    Returns the dictionary of test variables.\n\n    If the library is invoked as a standalone executable,\n    return a mockup dictionary, otherwise return the RobotFramework\n    test variable dictionary.\n    """"""\n    if __name__ == ""__main__"":\n        return _MOCKUP_TEST_VARIABLES\n    else:\n        return BuiltIn().get_variables()\n\n\n\n##############################################################################\n# Vorpaline execution wrappers (valgrind, callgrind, ...)\n\nclass _ExecutionWrapper:\n    """"""\n    Execution wrapper base class.\n    Actual execution wrappers must reimplement function wrap_command()\n    """"""\n\n    # Index used to generate output file names and guarantee their uniqueness\n    # during the whole suite execution.\n    _file_index = 1\n\n\n    def wrap_command(self, prog_name, command, preserve_files):\n        """"""\n        Returns a new command that executes the specified command under\n        control of the wrapper.\n\n        Arguments:\n        prog_name -- name of the executable\n        command -- the command to execute\n        preserve_files -- must be filled with the list of files (log-files,\n        output-files) to preserve at the end of the test execution\n        """"""\n        raise NotImplementedError(""Pure virtual method wrap_command() called"")\n\n\n    def output_filename(self, prefix, suffix):\n        """"""\n        Generates a output filename from a prefix and a suffix.\n        """"""\n        filename = ""%s%s%s"" % (prefix, _ExecutionWrapper._file_index, suffix)\n        _ExecutionWrapper._file_index += 1\n        return filename\n\n\nclass _Valgrind(_ExecutionWrapper):\n    """""" Valgrind execution wrapper """"""\n\n    _command = [\n        \'valgrind\',\n        \'--verbose\',\n        \'--xml=yes\',\n        \'--leak-check=full\',\n        \'--show-reachable=yes\',\n        \'--error-limit=no\',\n    ]\n\n    _outfile_ext = \'.memcheck\'\n    _option_outfile = \'--xml-file=%s\'\n    _suppressions_ext = \'.supp\'\n    _option_suppressions = \'--suppressions=%s\'\n\n    def wrap_command(self, prog_name, command, preserve_files):\n        out_file = self.output_filename(prog_name, _Valgrind._outfile_ext)\n        preserve_files.append(out_file)\n        options = [_Valgrind._option_outfile % out_file]\n\n        # Check suppression files:\n\n        test_variables = _get_test_variables()\n\n        supression_files = [\n            # Suppression file given by option --with-valgrind-suppressions=...\n            os.getenv(\'VORPALINE_WITH_VALGRIND_SUPPRESSIONS\'),\n\n            # Suppression file present in the test suite directory\n            os.path.join(test_variables[\'${SUITE SOURCE}\'], \'valgrind.supp\'),\n\n            # Suppression file present in the execution directory\n            os.path.join(test_variables[\'${EXECDIR}\'], \'valgrind.supp\'),\n        ]\n\n        for file in supression_files:\n            #sys.stderr.write(""** Checking suppression file %s\\n"" % file)\n            if (file != None) and os.path.isfile(file):\n                options += [_Valgrind._option_suppressions % file]\n                break\n\n        return _Valgrind._command + options + command\n\n\nclass _Callgrind(_ExecutionWrapper):\n    """""" Callgrind execution wrapper """"""\n\n    _command = [\n        \'callgrind\',\n    ]\n\n    _outfile_ext = \'.callgrind\'\n    _option_outfile = \'--callgrind-out-file=%s\'\n\n    def wrap_command(self, prog_name, command, preserve_files):\n        out_file = self.output_filename(prog_name, _Callgrind._outfile_ext)\n        preserve_files.append(out_file)\n        return _Callgrind._command + \\\n            [_Callgrind._option_outfile % out_file] + \\\n            command\n\n\n##############################################################################\n# RobotFramework test library for Vorpaline\n\nclass VorpatestLibrary:\n    """"""\n    RobotFramework test library for Vorpaline.\n\n    It defines 3 main functions that can be invoked from RobotFramework test\n    cases. Each function relies on environment variable VORPALINE_BIN_DIR to\n    locate programs to execute:\n    run_vorpaline -- executes vorpaline\n    run_vorpastat -- executes vorpastat\n    run_command -- executes any program\n    """"""\n\n    # Static variables used by all testcase executions\n    _initialized = False\n    _bin_dir = None\n    _test_index = None\n    _exec_wrapper = None\n\n    def __init__(self):\n        """"""\n        Initializes the test library instance.\n        Note that a new instance of the the library is created for EACH\n        testcase execution. Testsuite globals must be kept in VorpatestLibrary\n        static class variables.\n        """"""\n\n        if VorpatestLibrary._initialized:\n            return\n\n        sys.stderr.write(""** Initializing VorpatestLibrary\\n"")\n\n        VorpatestLibrary._initialized = True\n\n        # Initialize the path to the vorpaline executables\n        VorpatestLibrary._bin_dir = os.getenv(\'VORPALINE_BIN_DIR\')\n        if VorpatestLibrary._bin_dir is None:\n            raise RuntimeError(""Environment variable VORPALINE_BIN_DIR is not set"")\n\n        # Check for execution with valgrind\n        if os.getenv(\'VORPALINE_WITH_VALGRIND\') != None:\n            VorpatestLibrary._exec_wrapper = _Valgrind()\n\n        # Check for execution with callgrind\n        elif os.getenv(\'VORPALINE_WITH_CALLGRIND\') != None:\n            VorpatestLibrary._exec_wrapper = _Callgrind()\n\n        # Initialize the test index\n        VorpatestLibrary._test_index = 0\n\n\n    ######################################################################\n    # Public functions:\n\n    def prepare_test(self):\n        """"""\n        Prepares Vorpaline test execution.\n\n        This function creates the test execution directory where vorpaline\n        output file will be stored.\n        """"""\n\n        VorpatestLibrary._test_index += 1\n        self._status = -1\n        self._preserve_files = []\n\n        test_variables = _get_test_variables()\n\n        test_name = test_variables[\'${TEST NAME}\']\n        logger.info(""Setup test %s"" % test_name)\n\n        test_name = test_name.replace(\' \', \'_\').replace(\'.\', \'_\')\n        test_name = ""%03d_%s"" % (VorpatestLibrary._test_index, test_name)\n\n        self._execdir = os.path.join(test_variables[\'${EXECDIR}\'], \'run\', test_name)\n        self._log(""Execution directory: %s"" % self._execdir)\n\n        shutil.rmtree(self._execdir, True)\n        os.makedirs(self._execdir)\n        os.chdir(self._execdir)\n\n\n    def run_vorpaline(self, input_file, *options):\n        """"""\n        Run Vorpaline on the specified input file.\n        The output file ""out.meshb"" is implicitely generated in the test\n        execution directory created by prepare_test().\n\n        Arguments:\n        input_file -- path to the vorpaline input file\n        options -- vorpaline options\n        """"""\n\n        self._input_file = input_file\n        args = list(options) + [self._input_file, \'out.meshb\']\n        self._run_command(""vorpaline"", args)\n\n\n    def run_vorpastat(self, *options):\n        """"""\n        Run Vorpastat to compare the vorpaline input file and the output file\n        ""out.meshb"" generated by run_vorpaline.\n        TODO: the output of vorpastats must be later inspected to check\n        various metrics against the expected values.\n\n        Arguments:\n        options -- vorpastat options\n        """"""\n\n        args = list(options) + [self._input_file, \'out.meshb\']\n        self._run_command(""vorpastat"", args)\n\n\n    def run_command(self, prog_name, *args):\n        """"""\n        Run a command and capture the standard and error output\n\n        Arguments:\n        prog_name -- name of the executable\n        args -- command arguments given as a list\n        """"""\n        self._run_command(prog_name, list(args))\n\n\n    def cleanup_test(self):\n        """"""\n        Cleanup after successful test execution.\n\n        This removes the execution directory created by prepare_test(), only\n        if the test was successful. Execution directories of failed tests are\n        preserved for later inspection.\n        """"""\n        test_variables = _get_test_variables()\n        test_status = test_variables[\'${TEST STATUS}\']\n        if test_status == \'FAIL\':\n            # Test failed: keep directory for later inspection\n            return\n\n        # Test passed: remove the execution directory but preserve all\n        # important log files, if any (valgrind, gcov, ...)\n\n        if len(self._preserve_files) == 0:\n            shutil.rmtree(self._execdir, True)\n            return\n\n        # Move all the files to preserve to a temporary directory\n\n        backup_dir = self._execdir + \'.preserve\'\n        os.makedirs(backup_dir)\n        for file in self._preserve_files:\n            shutil.move(file, backup_dir)\n\n        # Delete the execution directory and rename the temporary directory\n\n        shutil.rmtree(self._execdir, True)\n        os.rename(backup_dir, self._execdir)\n\n\n    ######################################################################\n    # Private functions:\n\n    def _run_command(self, prog_name, args):\n        """"""\n        Run a command and capture the standard and error output\n\n        This function executes the given command under control of the\n        subprocess library. If the execution is driven by valgrind (Unix\n        only), the command is wrapped in a valgrind command and then executed.\n\n        Arguments:\n        prog_name -- name of the executable\n        args -- command arguments given as a list\n        """"""\n\n        # Build the command with the fullpath to the executable\n\n        command = [os.path.join(VorpatestLibrary._bin_dir, prog_name)] + args\n\n        # Debug: uncomment the following to debug the library for valgrind\n        # executions.\n        # command = [\'/home/vorpatest/bin/dummy_vorpaline.sh\'] + command\n\n        # Check if execution is controlled by a wrapper\n\n        if VorpatestLibrary._exec_wrapper != None:\n            command = VorpatestLibrary._exec_wrapper.wrap_command(\n                prog_name, command, self._preserve_files\n            )\n\n        self._log(""Run command: %s"" % command)\n        self._status = -1\n\n        try:\n            output = subprocess.check_output(command, stderr=subprocess.STDOUT)\n            self._log(""Command passed"")\n            self._log(""Output: %s"" % output)\n        except subprocess.CalledProcessError as e:\n            self._log(""Command failed!"")\n            self._log(""Return code: %s"" % e.returncode)\n            self._log(""Exception: %s"" % e)\n            self._log(""Output: %s"" % e.output)\n            raise\n        except:\n            (exc_type, exc_value) = sys.exc_info()[:2]\n            self._log(""Command failed!"")\n            self._log(""Exception type: %s"" % exc_type)\n            self._log(""Exception value: %s"" % exc_value)\n            raise\n\n        self._status = 0\n\n\n    def _log(self, args):\n        print ""*INFO*"",args,""\\n""\n\n\n######################################################################\n# Define a main program for testing VorpatestLibrary\n\ndef main():\n    """""" For testing the VorpatestLibrary """"""\n    testlib = VorpatestLibrary()\n    testlib.prepare_test()\n    testlib.run_vorpaline(*sys.argv[1:])\n    testlib.run_vorpastat()\n    testlib.cleanup_test()\n\nif __name__ == ""__main__"":\n    main()\n\n'"
