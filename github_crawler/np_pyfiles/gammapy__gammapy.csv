file_path,api_count,code
setup.py,1,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport setuptools\nfrom Cython.Build import cythonize\nfrom distutils.extension import Extension\nimport numpy as np\n\n\ndef make_cython_extension(filename):\n    return Extension(\n        filename.strip("".pyx"").replace(""/"", "".""),\n        [filename],\n        include_dirs=[np.get_include()],\n    )\n\n\ncython_files = [\n    ""gammapy/stats/fit_statistics_cython.pyx"",\n]\n\next_modules = cythonize([make_cython_extension(_) for _ in cython_files])\n\nsetuptools.setup(use_scm_version=True, ext_modules=ext_modules)\n'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n#\n# Gammapy documentation build configuration file.\n\nimport datetime\nfrom pkg_resources import get_distribution\n\n# Load all of the global Astropy configuration\nfrom sphinx_astropy.conf import *\n\n# Load utils docs functions\nfrom gammapy.utils.docs import gammapy_sphinx_ext_activate\nfrom gammapy.utils.docs import gammapy_sphinx_notebooks\n\n# Sphinx-gallery config\nfrom sphinx_gallery.sorting import FileNameSortKey\n\n# Get configuration information from setup.cfg\nfrom configparser import ConfigParser\n\nconf = ConfigParser()\nconf.read([os.path.join(os.path.dirname(__file__), "".."", ""setup.cfg"")])\nsetup_cfg = dict(conf.items(""metadata""))\n\nplot_html_show_source_link = False\n\n\n# -- General configuration ----------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n# needs_sphinx = \'1.1\'\n\n# We currently want to link to the latest development version of the astropy docs,\n# so we override the `intersphinx_mapping` entry pointing to the stable docs version\n# that is listed in `astropy/sphinx/conf.py`.\nintersphinx_mapping.pop(""h5py"", None)\nintersphinx_mapping[""matplotlib""] = (""https://matplotlib.org/"", None)\nintersphinx_mapping[""astropy""] = (""http://docs.astropy.org/en/latest/"", None)\nintersphinx_mapping[""regions""] = (\n    ""https://astropy-regions.readthedocs.io/en/latest/"",\n    None,\n)\nintersphinx_mapping[""reproject""] = (""https://reproject.readthedocs.io/en/latest/"", None)\nintersphinx_mapping[""naima""] = (""https://naima.readthedocs.io/en/latest/"", None)\nintersphinx_mapping[""gadf""] = (\n    ""https://gamma-astro-data-formats.readthedocs.io/en/latest/"",\n    None,\n)\nintersphinx_mapping[""iminuit""] = (""https://iminuit.readthedocs.io/en/latest/"", None)\nintersphinx_mapping[""pandas""] = (""https://pandas.pydata.org/pandas-docs/stable/"", None)\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns.append(""_templates"")\nexclude_patterns.append(""_static"")\nexclude_patterns.append(""**.ipynb_checkpoints"")\n\n#\n# -- nbsphinx settings\nextensions.extend(\n    [\n        ""nbsphinx"",\n        ""sphinx_click.ext"",\n        ""IPython.sphinxext.ipython_console_highlighting"",\n        ""sphinx.ext.mathjax"",\n        ""sphinx_gallery.gen_gallery"",\n    ]\n)\nnbsphinx_execute = ""never""\n\n# --\n\n# This is added to the end of RST files - a good place to put substitutions to\n# be used globally.\nrst_epilog += """"""\n""""""\n\n# -- Project information ------------------------------------------------------\n\n# This does not *have* to match the package name, but typically does\nproject = setup_cfg[""name""]\nauthor = setup_cfg[""author""]\ncopyright = ""{}, {}"".format(datetime.datetime.now().year, setup_cfg[""author""])\n\nversion = get_distribution(project).version\nrelease = version\n\n# -- Options for HTML output ---------------------------------------------------\n\n# A NOTE ON HTML THEMES\n# The global astropy configuration uses a custom theme, \'bootstrap-astropy\',\n# which is installed along with astropy. A different theme can be used or\n# the options for this theme can be modified by overriding some of the\n# variables set in the global configuration. The variables set in the\n# global configuration are listed below, commented out.\n\n# html_theme_options = {\n#    \'logotext1\': \'gamma\',  # white,  semi-bold\n#    \'logotext2\': \'py\',  # orange, light\n#    \'logotext3\': \':docs\'  # white,  light\n# }\n\nhtml_theme_options = {\n    ""canonical_url"": setup_cfg[""url_docs""],\n    ""analytics_id"": """",\n    ""logo_only"": False,\n    ""display_version"": True,\n    ""prev_next_buttons_location"": ""bottom"",\n    # Toc options\n    ""collapse_navigation"": False,\n    ""sticky_navigation"": True,\n    ""navigation_depth"": 4,\n}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# To use a different custom theme, add the directory containing the theme.\n# html_theme_path = []\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes. To override the custom theme, set this to the\n# name of a builtin theme or the name of a custom theme in html_theme_path.\nhtml_theme = ""sphinx_rtd_theme""\n\n# Custom sidebar templates, maps document names to template names.\n# html_sidebars = {}\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n# html_favicon = \'\'\n\n# TODO: set this image also in the title bar\n# (html_logo is not the right option)\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n# html_last_updated_fmt = \'\'\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\nhtml_title = ""{} v{}"".format(project, release)\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = project + ""doc""\n\n# Static files to copy after template files\nhtml_static_path = [""_static""]\n\ngammapy_sphinx_ext_activate()\n\n# Integration of notebooks\ngammapy_sphinx_notebooks(setup_cfg)\n\n\n# Theme style\n# html_style = \'\'\ndef setup(app):\n    app.add_stylesheet(""gammapy.css"")\n    app.add_javascript(""copybutton.js"")\n    app.add_javascript(""gammapy.js"")\n\n\n# copybutton.js provides hide/show button for python prompts >>>\n# slightly modified to work on RTD theme from javascript file in easydev package\n# https://github.com/cokelaer/easydev/blob/master/easydev/share/copybutton.js\n\n\nhtml_favicon = os.path.join(html_static_path[0], ""gammapy_logo.ico"")\n\n# -- Options for LaTeX output --------------------------------------------------\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass [howto/manual]).\nlatex_documents = [\n    (""index"", project + "".tex"", project + "" Documentation"", author, ""manual"")\n]\n\n# -- Options for manual page output --------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(""index"", project.lower(), project + "" Documentation"", [author], 1)]\n\n# -- Other options --\n\ngithub_issues_url = ""https://github.com/gammapy/gammapy/issues/""\n\n# http://sphinx-automodapi.readthedocs.io/en/latest/automodapi.html\n# show inherited members for classes\nautomodsumm_inherited_members = True\n\n# In `about.rst` and `references.rst` we are giving lists of citations\n# (e.g. papers using Gammapy) that partly aren\'t referenced from anywhere\n# in the Gammapy docs. This is normal, but Sphinx emits a warning.\n# The following config option suppresses the warning.\n# http://www.sphinx-doc.org/en/stable/rest.html#citations\n# http://www.sphinx-doc.org/en/stable/config.html#confval-suppress_warnings\nsuppress_warnings = [""ref.citation""]\n\n# nitpicky = True\n\nsphinx_gallery_conf = {\n    ""examples_dirs"": [""../examples/models""],  # path to your example scripts\n    ""gallery_dirs"": [\n        ""modeling/gallery""\n    ],  # path to where to save gallery generated output\n    ""within_subsection_order"": FileNameSortKey,\n    ""download_all_examples"": False,\n    ""capture_repr"": (),\n    ""min_reported_time"": 10000,\n    ""show_memory"": False,\n    ""line_numbers"": False,\n    ""reference_url"": {\n        # The module you locally document uses None\n        ""gammapy"": None,\n    },\n}\n'"
examples/example_2_gauss.py,1,"b'import numpy as np\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nfrom gammapy.cube import MapDataset, make_map_exposure_true_energy\nfrom gammapy.irf import EffectiveAreaTable2D\nfrom gammapy.maps import MapAxis, WcsGeom\nfrom gammapy.modeling import Fit\nfrom gammapy.modeling.models import (\n    GaussianSpatialModel,\n    PowerLawSpectralModel,\n    SkyModel,\n)\n\nfilename = ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits""\naeff = EffectiveAreaTable2D.read(filename, hdu=""EFFECTIVE AREA"")\n\n# Define sky model to simulate the data\nlon_0_1 = 0.2\nlon_0_2 = 0.4\nlat_0_1 = 0.1\nlat_0_2 = 0.6\n\nspatial_model_1 = GaussianSpatialModel(\n    lon_0=lon_0_1 * u.deg, lat_0=lat_0_1 * u.deg, sigma=""0.3 deg"", frame=""galactic""\n)\nspatial_model_2 = GaussianSpatialModel(\n    lon_0=lon_0_2 * u.deg, lat_0=lat_0_2 * u.deg, sigma=""0.2 deg"", frame=""galactic""\n)\n\nspectral_model_1 = PowerLawSpectralModel(\n    index=3, amplitude=""1e-11 cm-2 s-1 TeV-1"", reference=""1 TeV""\n)\n\nspectral_model_2 = PowerLawSpectralModel(\n    index=3, amplitude=""1e-11 cm-2 s-1 TeV-1"", reference=""1 TeV""\n)\n\nsky_model_1 = SkyModel(\n    spatial_model=spatial_model_1, spectral_model=spectral_model_1, name=""source-1""\n)\n\nsky_model_2 = SkyModel(\n    spatial_model=spatial_model_2, spectral_model=spectral_model_2, name=""source-2""\n)\n\nmodels = sky_model_1 + sky_model_2\n\n# Define map geometry\naxis = MapAxis.from_edges(np.logspace(-1.0, 1.0, 10), unit=""TeV"", name=""energy"")\ngeom = WcsGeom.create(\n    skydir=(0, 0), binsz=0.02, width=(2, 2), frame=""galactic"", axes=[axis]\n)\n\n\n# Define some observation parameters\n# we are not simulating many pointings / observations\npointing = SkyCoord(0.2, 0.5, unit=""deg"", frame=""galactic"")\nlivetime = 20 * u.hour\n\nexposure_map = make_map_exposure_true_energy(\n    pointing=pointing, livetime=livetime, aeff=aeff, geom=geom\n)\n\ndataset = MapDataset(models=models, exposure=exposure_map)\nnpred = dataset.npred()\n\ndataset.fake()\n\nfit = Fit([dataset])\nresults = fit.run()\n\nprint(results)\nprint(models)\n'"
examples/exptest.py,6,"b'""""""\nAn example for the exptest variability test.\n\n- Simulate constant rate events for several observations.\n- Check that the ``mr`` distribution is a standard normal, as expected.\n""""""\nimport numpy as np\nfrom scipy.stats import norm\nfrom astropy.table import Table\nimport matplotlib.pyplot as plt\nfrom gammapy.time import exptest\n\n\ndef simulation(n_obs):\n    """"""Simulate time series data.\n\n    Produce table with one row per event.\n    - obs_id -- Observation ID\n    - mjd -- event time\n      - mjd is filled randomly between 0 and 1\n    - expCount -- expected counts, aka acceptance\n      - expcount is filled with 1\n    """"""\n    table = Table()\n\n    # For every observation, decide somewhat randomly how many events to simulate\n    n_event_per_obs = np.random.random_integers(20, 30, n_obs)\n    n_events = n_event_per_obs.sum()\n\n    table[""obs_id""] = np.repeat(np.arange(len(n_event_per_obs)), n_event_per_obs)\n\n    mjd_random = np.random.uniform(0, 1, n_events)\n    mjd = np.sort(mjd_random)\n    table[""mjd""] = mjd\n\n    table[""expCount""] = 1\n\n    return table\n\n\ndef exptest_multi(table_events):\n    """"""Compute mr value for each run and whole dataset.\n\n    Parameter\n    ---------\n    table : `astropy.table.Table`\n        Input data\n\n    Returns\n    -------\n    mr_list : list\n        List of `mr` values.\n    """"""\n    # Make table with one row per observation\n    table_obs = Table()\n    table_obs[""obs_id""] = np.unique(table_events[""obs_id""])\n    res = table_events.group_by(""obs_id"")\n    n_events = np.array(res.groups.aggregate(sum)[""expCount""])\n    table_obs[""n_events""] = n_events\n\n    mr_list = []\n\n    time_delta_all = []\n\n    for i in range(0, len(table_obs)):\n        time_delta_each_run = []\n\n        for j in range(0, len(table_events) - 1):\n            if (\n                table_obs[""n_events""][i] > 20\n                and table_obs[""obs_id""][i] == table_events[""obs_id""][j]\n                and table_events[""obs_id""][j] == table_events[""obs_id""][j + 1]\n            ):\n                time_delta = (table_events[""mjd""][j + 1] - table_events[""mjd""][j]) / 2\n                time_delta *= (\n                    table_events[""expCount""][j + 1] + table_events[""expCount""][j]\n                )\n                time_delta_each_run.append(time_delta)\n                time_delta_all.append(time_delta)\n\n        if len(time_delta_each_run) == 0:\n            continue\n        mr = exptest(time_delta_each_run)\n        mr_list.append(mr)\n        print(""mr value: "", mr, ""   "", table_obs[""obs_id""][i])\n        del time_delta_each_run[:]\n\n    overallm = exptest(time_delta_all)\n    print(""Mr for the whole dataset: "", overallm)\n\n    return mr_list\n\n\ndef plot(m_value):\n    """"""Plot histogram of mr value for each run.\n\n    A normal distribution is expected for non-flaring sources.\n    """"""\n    (mu, sigma) = norm.fit(m_value)\n    n, bins, patches = plt.hist(\n        m_value, bins=30, normed=1, facecolor=""green"", alpha=0.75\n    )\n    plt.mlab.normpdf(bins, mu, sigma)\n    print(""mu:{:10.3f}"".format(mu), "" sigma:{:10.4f}"".format(sigma))\n    plt.xlim(-3, 3)\n    plt.xlabel(""Mr value"")\n    plt.ylabel(""counts"")\n    title = ""Histogram of IQ: mu={:.3f}, sigma={:.3f}"".format(mu, sigma)\n    plt.title(title)\n    plt.grid(True)\n    plt.show()\n\n\nif __name__ == ""__main__"":\n    table = simulation(100)\n    m_value = exptest_multi(table)\n    plot(m_value)\n'"
examples/survey_map.py,0,"b'""""""Make a survey counts map.\n\nWe create an all-sky map in AIT projection\nfor the HESS DL3 DR1 dataset.\n""""""\nimport logging\nfrom gammapy.data import DataStore\nfrom gammapy.maps import Map\n\nlog = logging.getLogger(__name__)\n\n\ndef main():\n    data_store = DataStore.from_dir(""$GAMMAPY_DATA/hess-dl3-dr1"")\n    obs_id = data_store.obs_table[""OBS_ID""]\n    observations = data_store.get_observations(obs_id)\n\n    m = Map.create()\n    for obs in observations:\n        log.info(f""Processing obs_id: {obs.obs_id}"")\n        m.fill_events(obs.events)\n\n    m.write(""survey_map.fits.gz"")\n\n\nif __name__ == ""__main__"":\n    logging.basicConfig(level=logging.INFO)\n    main()\n'"
gammapy/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Gammapy: A Python package for gamma-ray astronomy.\n\n* Code: https://github.com/gammapy/gammapy\n* Docs: https://docs.gammapy.org/\n\nThe top-level `gammapy` namespace is almost empty,\nit only contains this:\n\n::\n\n test              --- Run Gammapy unit tests\n __version__       --- Gammapy version string\n\n\nThe Gammapy functionality is available for import from\nthe following sub-packages (e.g. `gammapy.spectrum`):\n\n::\n\n astro        --- Astrophysical source and population models\n catalog      --- Source catalog tools\n cube         --- Cube analysis\n data         --- Data and observation handling\n detect       --- Source detection tools\n irf          --- Instrument response functions (IRFs)\n maps         --- Sky map data structures\n modeling     --- Models and fitting\n spectrum     --- Spectrum estimation and modeling\n stats        --- Statistics tools\n time         --- Time handling and analysis\n utils        --- Utility functions and classes\n""""""\n\n__all__ = [""__version__"", ""song""]\n\nimport pkg_resources\n\ntry:\n    __version__ = pkg_resources.get_distribution(__name__).version\nexcept pkg_resources.DistributionNotFound:\n    # package is not installed\n    pass\n\n\ndef song(karaoke=False):\n    """"""\n    Listen to the Gammapy song.\n\n    Make sure you listen on good headphones or speakers. You\'ll be not disappointed!\n\n    Parameters\n    ----------\n    karaoke : bool\n        Print lyrics to sing along.\n    """"""\n    import webbrowser\n    import sys\n\n    webbrowser.open(""https://gammapy.org/gammapy_song.mp3"")\n\n    if karaoke:\n        lyrics = (\n            ""\\nGammapy Song Lyrics\\n""\n            ""-------------------\\n\\n""\n            ""Gammapy, gamma-ray data analysis package\\n""\n            ""Gammapy, prototype software CTA science tools\\n\\n""\n            ""Supernova remnants, pulsar winds, AGN, Gamma, Gamma, Gammapy\\n""\n            ""Galactic plane survey, pevatrons, Gammapy, Gamma, Gammapy\\n""\n            ""Gammapy, github, continuous integration, readthedocs, travis, ""\n            ""open source project\\n\\n""\n            ""Gammapy, Gammapy\\n\\n""\n            ""Supernova remnants, pulsar winds, AGN, Gamma, Gamma, Gammapy\\n""\n        )\n\n        centered = ""\\n"".join(f""{s:^80}"" for s in lyrics.split(""\\n""))\n        sys.stdout.write(centered)\n'"
gammapy/__main__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Top-level script environment for Gammapy.\n\nThis is what\'s executed when you run:\n\n    python -m gammapy\n\nSee https://docs.python.org/3/library/__main__.html\n""""""\nimport sys\nfrom .scripts.main import cli\n\nif __name__ == ""__main__"":\n    sys.exit(cli())  # pylint:disable=no-value-for-parameter\n'"
gammapy/conftest.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n# this contains imports plugins that configure py.test for astropy tests.\n# by importing them here in conftest.py they are discoverable by py.test\n# no matter how it is invoked within the source tree.\nimport os\nfrom astropy.tests.helper import enable_deprecations_as_exceptions\nfrom pytest_astropy_header.display import PYTEST_HEADER_MODULES\n\n# TODO: add numpy again once https://github.com/astropy/regions/pull/252 is addressed\nenable_deprecations_as_exceptions(warnings_to_ignore_entire_module=[""numpy"", ""astropy""])\n\n# Declare for which packages version numbers should be displayed\n# when running the tests\nPYTEST_HEADER_MODULES[""cython""] = ""cython""\nPYTEST_HEADER_MODULES[""iminuit""] = ""iminuit""\nPYTEST_HEADER_MODULES[""astropy""] = ""astropy""\nPYTEST_HEADER_MODULES[""regions""] = ""regions""\nPYTEST_HEADER_MODULES[""healpy""] = ""healpy""\nPYTEST_HEADER_MODULES[""sherpa""] = ""sherpa""\nPYTEST_HEADER_MODULES[""gammapy""] = ""gammapy""\nPYTEST_HEADER_MODULES[""naima""] = ""naima""\n\n\ndef pytest_configure(config):\n    """"""Print some info ...""""""\n    from gammapy.utils.testing import has_data\n\n    config.option.astropy_header = True\n\n    print("""")\n    print(""Gammapy test data availability:"")\n\n    has_it = ""yes"" if has_data(""gammapy-data"") else ""no""\n    print(f""gammapy-data ... {has_it}"")\n\n    print(""Gammapy environment variables:"")\n\n    var = os.environ.get(""GAMMAPY_DATA"", ""not set"")\n    print(f""GAMMAPY_DATA = {var}"")\n\n    try:\n        # Switch to non-interactive plotting backend to avoid GUI windows\n        # popping up while running the tests.\n        import matplotlib\n\n        matplotlib.use(""agg"")\n        print(\'Setting matplotlib backend to ""agg"" for the tests.\')\n    except ImportError:\n        pass\n'"
dev/datasets/make_dataset_index.py,0,"b'#!/usr/bin/env python\n""""""Make the gammapy.org static webpage.\n\nThis is very much work in progress.\nProbably we should add a static website build step.\n""""""\nimport hashlib\nimport json\nimport logging\nimport os\nimport shutil\nfrom pathlib import Path\nimport click\n\nlog = logging.getLogger(__name__)\npath_temp = Path(""datasets"")\n\n\ndef hashmd5(path):\n    md5_hash = hashlib.md5()\n    with open(path, ""rb"") as f:\n        for byte_block in iter(lambda: f.read(4096), b""""):\n            md5_hash.update(byte_block)\n    return md5_hash.hexdigest()\n\n\nclass DownloadDataset:\n    """"""DownloadDataset base class.\n\n    The DownloadDataset class has a local_repo property where to scan the content\n    and a base_url to build access links for each file.\n\n    A DownloadDataset has a name as identifier.\n    It also has a description and list of files, each file has a given URL\n    and a path that tells you where the file will be placed when downloaded.\n\n    If you want to add a DownloadDataset, make a new class and add it to the list below.\n\n    The followLinks flag declares how to build the destination paths in the local desktop\n    according to the datasets paths used in the tutorials.\n        GAMMAPY-EXTRA: datasets stored in Gammapy-extra repository\n        JOINT-CRAB: datasets stored in Joint-crab repository\n        OTHERS: datasets stored in others repositories\n    """"""\n\n    followLinks = ""GAMMAPY-EXTRA""\n    base_url = ""https://github.com/gammapy/gammapy-extra/raw/master/datasets""\n    local_repo = Path(os.environ[""GAMMAPY_EXTRA""]) / ""datasets""\n\n    @property\n    def record(self):\n        return {\n            ""name"": self.name,\n            ""description"": self.description,\n            ""files"": list(self.files),\n        }\n\n    @property\n    def pathlist(self):\n        for itempath in (self.local_repo / self.name).glob(""**/*.*""):\n            if not itempath.name.startswith("".""):\n                yield itempath.as_posix().replace(self.local_repo.as_posix() + ""/"", """")\n\n    @property\n    def files(self):\n        for item in self.pathlist:\n\n            if self.followLinks == ""GAMMAPY-EXTRA"":\n                jsonpath = str(Path(item))\n            elif self.followLinks == ""JOINT-CRAB"":\n                jsonpath = str(Path(""joint-crab"") / Path(""spectra"") / Path(item))\n            else:\n                jsonpath = str(Path(self.name) / Path(item).name)\n\n            itempath = self.local_repo / item\n            urlpath = itempath.as_posix().replace(self.local_repo.as_posix(), """")\n            filesize = os.path.getsize(itempath)\n            md5 = hashmd5(itempath)\n            yield {\n                ""path"": jsonpath,\n                ""url"": self.base_url + urlpath,\n                ""filesize"": filesize,\n                ""hashmd5"": md5,\n                ""itempath"": str(itempath),\n            }\n\n\nclass DatasetCTA1DC(DownloadDataset):\n    name = ""cta-1dc""\n    description = ""tbd""\n\n\nclass DatasetDarkMatter(DownloadDataset):\n    name = ""dark_matter_spectra""\n    description = ""tbd""\n\n\nclass DatasetCatalogs(DownloadDataset):\n    name = ""catalogs""\n    description = ""tbd""\n\n\nclass DatasetFermi3FHL(DownloadDataset):\n    name = ""fermi_3fhl""\n    description = ""tbd""\n\n\nclass DatasetHESSDL3DR1(DownloadDataset):\n    name = ""hess-dl3-dr1""\n    description = ""tbd""\n\n\nclass DatasetEBL(DownloadDataset):\n    name = ""ebl""\n    description = ""tbd""\n\n\nclass DatasetTests(DownloadDataset):\n    name = ""tests""\n    description = ""tbd""\n\n\nclass DatasetFigures(DownloadDataset):\n    name = ""figures""\n    description = ""tbd""\n    base_url = ""https://github.com/gammapy/gammapy-extra/raw/master""\n    local_repo = Path(os.environ[""GAMMAPY_EXTRA""])\n\n\nclass DatasetJointCrab(DownloadDataset):\n    name = ""joint-crab""\n    description = ""tbd""\n    base_url = (\n        ""https://github.com/open-gamma-ray-astro/joint-crab/raw/master/results/spectra""\n    )\n    local_repo = Path(os.environ[""JOINT_CRAB""]) / ""results"" / ""spectra""\n\n    followLinks = ""JOINT-CRAB""\n    pathlist = []\n    for itempath in (local_repo).glob(""**/*.*""):\n        if not itempath.name.startswith("".""):\n            pathlist.append(\n                itempath.as_posix().replace(local_repo.as_posix() + ""/"", """")\n            )\n\n\nclass DatasetFermiLat(DownloadDataset):\n\n    name = ""fermi-3fhl""\n    description = ""tbd""\n    base_url = ""https://github.com/gammapy/gammapy-fermi-lat-data/raw/master""\n    local_repo = Path(os.environ[""GAMMAPY_FERMI_LAT_DATA""])\n\n    followLinks = ""Others""\n    pathlist = [\n        str(Path(""3fhl"") / ""allsky"" / ""fermi_3fhl_events_selected.fits.gz""),\n        str(Path(""3fhl"") / ""allsky"" / ""fermi_3fhl_exposure_cube_hpx.fits.gz""),\n        str(Path(""3fhl"") / ""allsky"" / ""fermi_3fhl_psf_gc.fits.gz""),\n        str(Path(""isodiff"") / ""iso_P8R2_SOURCE_V6_v06.txt""),\n    ]\n\n\nclass DatasetFermi3FHLGC(DownloadDataset):\n\n    name = ""fermi-3fhl-gc""\n    description = ""Prepared Fermi-LAT 3FHL dataset of the Galactic center region""\n    base_url = ""https://github.com/gammapy/gammapy-fermi-lat-data/raw/master""\n    local_repo = Path(os.environ[""GAMMAPY_FERMI_LAT_DATA""])\n    basepath = Path(""3fhl"") / ""galactic-center""\n\n    followLinks = ""Others""\n    pathlist = [\n        str(basepath / ""fermi-3fhl-gc-background.fits.gz""),\n        str(basepath / ""fermi-3fhl-gc-background-cube.fits.gz""),\n        str(basepath / ""fermi-3fhl-gc-counts.fits.gz""),\n        str(basepath / ""fermi-3fhl-gc-counts-cube.fits.gz""),\n        str(basepath / ""fermi-3fhl-gc-events.fits.gz""),\n        str(basepath / ""fermi-3fhl-gc-exposure-cube.fits.gz""),\n        str(basepath / ""fermi-3fhl-gc-exposure.fits.gz""),\n        str(basepath / ""fermi-3fhl-gc-psf.fits.gz""),\n        str(basepath / ""fermi-3fhl-gc-psf-cube.fits.gz""),\n        str(basepath / ""gll_iem_v06_gc.fits.gz""),\n    ]\n\n\nclass DatasetFermi3FHLcrab(DownloadDataset):\n\n    name = ""fermi-3fhl-crab""\n    description = ""Prepared Fermi-LAT 3FHL dataset of the Crab Nebula region""\n    base_url = ""https://github.com/gammapy/gammapy-fermi-lat-data/raw/master""\n    local_repo = Path(os.environ[""GAMMAPY_FERMI_LAT_DATA""])\n    basepath = Path(""3fhl"") / ""crab""\n\n    followLinks = ""Others""\n    pathlist = [\n        str(basepath / ""Fermi-LAT-3FHL_data_Fermi-LAT.fits""),\n        str(basepath / ""Fermi-LAT-3FHL_datasets.yaml""),\n        str(basepath / ""Fermi-LAT-3FHL_models.yaml""),\n    ]\n\n\nclass DatasetHAWCcrab(DownloadDataset):\n    name = ""hawc_crab""\n    description = ""tbd""\n\n\nclass DownloadDatasetIndex:\n    path = Path(__file__).parent / ""gammapy-data-index.json""\n    download_datasets = [\n        DatasetCTA1DC,\n        DatasetDarkMatter,\n        DatasetCatalogs,\n        DatasetFermi3FHL,\n        DatasetHESSDL3DR1,\n        DatasetJointCrab,\n        DatasetEBL,\n        DatasetFermi3FHLGC,\n        DatasetFermi3FHLcrab,\n        DatasetHAWCcrab,\n        DatasetTests,\n        DatasetFigures,\n    ]\n\n    def make(self):\n        records = list(self.make_records())\n        for rec in records:\n            for f in rec[""files""]:\n                destination = Path(os.environ[""GAMMAPY_DATA""]) / f[""path""]\n                log.info(f""Moving {f[\'itempath\']}"")\n                shutil.copyfile(f[""itempath""], destination)\n                del f[""itempath""]\n\n        txt = json.dumps(records, indent=True)\n        log.info(""Writing {}"".format(self.path))\n        Path(self.path).write_text(txt)\n\n    def make_records(self):\n        for cls in self.download_datasets:\n            yield cls().record\n\n\n@click.group()\ndef cli():\n    """"""Make a dataset index JSON file to download datasets with gammapy download datasets""""""\n    logging.basicConfig(level=""INFO"")\n\n\n@cli.command(""all"")\n@click.pass_context\ndef cli_all(ctx):\n    """"""Run all steps""""""\n    ctx.invoke(cli_download_dataset_index)\n\n\n@cli.command(""dataset-index"")\ndef cli_download_dataset_index():\n    """"""Generate dataset index JSON file and write files in gammapy-data local repo""""""\n    DownloadDatasetIndex().make()\n\n\nif __name__ == ""__main__"":\n    cli()\n'"
docs/irf/plot_edisp.py,1,"b'""""""Plot energy dispersion example.""""""\nimport numpy as np\nimport astropy.units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.irf import EDispKernel\n\nebounds = np.logspace(-1, 2, 101) * u.TeV\n\nedisp = EDispKernel.from_gauss(e_true=ebounds, e_reco=ebounds, bias=0, sigma=0.3)\n\nedisp.peek()\nplt.show()\n'"
docs/irf/plot_fermi_psf.py,0,"b'""""""Plot Fermi PSF.""""""\nimport matplotlib.pyplot as plt\nfrom gammapy.irf import EnergyDependentTablePSF, PSFKernel\nfrom gammapy.maps import WcsGeom\n\nfilename = ""$GAMMAPY_DATA/tests/unbundled/fermi/psf.fits""\nfermi_psf = EnergyDependentTablePSF.read(filename)\n\npsf = fermi_psf.table_psf_at_energy(energy=""1 GeV"")\ngeom = WcsGeom.create(npix=100, binsz=0.01)\nkernel = PSFKernel.from_table_psf(psf, geom)\n\nplt.imshow(kernel.data)\nplt.colorbar()\nplt.show()\n'"
docs/makers/create_region.py,0,"b'""""""Example how to compute and plot reflected regions.""""""\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nfrom regions import CircleSkyRegion, EllipseAnnulusSkyRegion, RectangleSkyRegion\nimport matplotlib.pyplot as plt\nfrom gammapy.maps import WcsNDMap\n\nposition = SkyCoord(83.63, 22.01, unit=""deg"", frame=""icrs"")\n\non_circle = CircleSkyRegion(position, 0.3 * u.deg)\n\non_ellipse_annulus = EllipseAnnulusSkyRegion(\n    center=position,\n    inner_width=1.5 * u.deg,\n    outer_width=2.5 * u.deg,\n    inner_height=3 * u.deg,\n    outer_height=4 * u.deg,\n    angle=130 * u.deg,\n)\n\nanother_position = SkyCoord(80.3, 22.0, unit=""deg"")\non_rectangle = RectangleSkyRegion(\n    center=another_position, width=2.0 * u.deg, height=4.0 * u.deg, angle=50 * u.deg\n)\n\n# Now we plot those regions. We first create an empty map\nempty_map = WcsNDMap.create(\n    skydir=position, width=10 * u.deg, binsz=0.1 * u.deg, proj=""TAN""\n)\nempty_map.data += 1.0\nempty_map.plot(cmap=""gray"", vmin=0, vmax=1)\n\n# To plot the regions, we convert them to PixelRegion with the map wcs\non_circle.to_pixel(empty_map.geom.wcs).plot()\non_rectangle.to_pixel(empty_map.geom.wcs).plot()\non_ellipse_annulus.to_pixel(empty_map.geom.wcs).plot()\n\nplt.show()\n'"
docs/makers/make_rectangular_reflected_background.py,0,"b'""""""Example how to compute and plot reflected regions.""""""\nimport numpy as np\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nfrom regions import RectangleSkyRegion\nimport matplotlib.pyplot as plt\nfrom gammapy.data import DataStore\nfrom gammapy.datasets import SpectrumDataset\nfrom gammapy.makers import ReflectedRegionsBackgroundMaker, SpectrumDatasetMaker\nfrom gammapy.maps import Map, MapAxis\nfrom gammapy.visualization import plot_spectrum_datasets_off_regions\n\ndata_store = DataStore.from_dir(""$GAMMAPY_DATA/hess-dl3-dr1/"")\nmask = data_store.obs_table[""TARGET_NAME""] == ""Crab""\nobs_ids = data_store.obs_table[""OBS_ID""][mask].data\nobservations = data_store.get_observations(obs_ids)\n\ncrab_position = SkyCoord(83.63, 22.01, unit=""deg"", frame=""icrs"")\n\n# The ON region center is defined in the icrs frame. The angle is defined w.r.t. to its axis.\nrectangle = RectangleSkyRegion(\n    center=crab_position, width=0.5 * u.deg, height=0.4 * u.deg, angle=0 * u.deg\n)\n\n\nbkg_maker = ReflectedRegionsBackgroundMaker(min_distance=0.1 * u.rad)\ndataset_maker = SpectrumDatasetMaker(selection=[""counts""])\n\ne_reco = MapAxis.from_energy_bounds(0.1,100,30, unit=""TeV"")\ndataset_empty = SpectrumDataset.create(e_reco=e_reco, region=rectangle)\n\ndatasets = []\n\nfor obs in observations:\n\n    dataset = dataset_maker.run(dataset_empty.copy(name=f""obs-{obs.obs_id}""), obs)\n    dataset_on_off = bkg_maker.run(observation=obs, dataset=dataset)\n    datasets.append(dataset_on_off)\n\nm = Map.create(skydir=crab_position, width=(8, 8), proj=""TAN"")\n\n_, ax, _ = m.plot(vmin=-1, vmax=0)\n\nrectangle.to_pixel(ax.wcs).plot(ax=ax, color=""black"")\n\nplot_spectrum_datasets_off_regions(datasets=datasets, ax=ax)\nplt.show()\n'"
docs/makers/make_reflected_regions.py,1,"b'""""""Example how to compute and plot reflected regions.""""""\nimport numpy as np\nfrom astropy.coordinates import Angle, SkyCoord\nfrom regions import CircleSkyRegion\nimport matplotlib.pyplot as plt\nfrom gammapy.makers import ReflectedRegionsFinder\nfrom gammapy.maps import WcsNDMap\n\n# Exclude a rectangular region\nexclusion_mask = WcsNDMap.create(npix=(801, 701), binsz=0.01, skydir=(83.6, 23.0))\n\ncoords = exclusion_mask.geom.get_coord().skycoord\nmask = (Angle(""23 deg"") < coords.dec) & (coords.dec < Angle(""24 deg""))\nexclusion_mask.data = np.invert(mask)\n\npos = SkyCoord(83.633, 22.014, unit=""deg"")\nradius = Angle(0.3, ""deg"")\non_region = CircleSkyRegion(pos, radius)\ncenter = SkyCoord(83.633, 24, unit=""deg"")\n\n# One can impose a minimal distance between ON region and first reflected regions\nfinder = ReflectedRegionsFinder(\n    region=on_region,\n    center=center,\n    exclusion_mask=exclusion_mask,\n    min_distance_input=""0.2 rad"",\n)\nfinder.run()\n\nfig1 = plt.figure(1)\nfinder.plot(fig=fig1)\n\n# One can impose a minimal distance between two adjacent regions\nfinder = ReflectedRegionsFinder(\n    region=on_region,\n    center=center,\n    exclusion_mask=exclusion_mask,\n    min_distance_input=""0.2 rad"",\n    min_distance=""0.1 rad"",\n)\nfinder.run()\nfig2 = plt.figure(2)\nfinder.plot(fig=fig2)\n\n# One can impose a maximal number of regions to be extracted\nfinder = ReflectedRegionsFinder(\n    region=on_region,\n    center=center,\n    exclusion_mask=exclusion_mask,\n    min_distance_input=""0.2 rad"",\n    max_region_number=5,\n    min_distance=""0.1 rad"",\n)\nfinder.run()\nfig3 = plt.figure(3)\nfinder.plot(fig=fig3)\n\nplt.show()\n'"
docs/scripts/significance.py,0,"b'""""""Example how to write a command line tool with Click""""""\nimport click\nfrom gammapy.stats import CashCountsStatistic\n\n\n# You can call the callback function for the click command anything you like.\n# `cli` is just a commonly used generic term for ""command line interface"".\n@click.command()\n@click.argument(""n_observed"", type=float)\n@click.argument(""mu_background"", type=float)\n@click.option(\n    ""--value"",\n    type=click.Choice([""sqrt_ts"", ""p_value""]),\n    default=""sqrt_ts"",\n    help=""Significance or p_value"",\n)\ndef cli(n_observed, mu_background, value):\n    """"""Compute significance for a Poisson count observation.\n\n    The significance is the tail probability to observe N_OBSERVED counts\n    or more, given a known background level MU_BACKGROUND.""""""\n    stat = CashCountsStatistic(n_observed, mu_background)\n    if value == ""sqrt_ts"":\n        s = stat.significance\n    else:\n        s = stat.p_value\n\n    print(s)\n\n\nif __name__ == ""__main__"":\n    cli()\n'"
docs/stats/plot_cash_errors.py,1,"b'""""""Example plot showing the profile of the Cash statistic and its connection to excess errors.""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom gammapy.stats import CashCountsStatistic\n\ncount_statistic = CashCountsStatistic(n_on=13, mu_bkg=5.5)\nexcess = count_statistic.excess\n\nerrn = count_statistic.compute_errn(1.0)\nerrp = count_statistic.compute_errp(1.0)\n\nerrn_2sigma = count_statistic.compute_errn(2.0)\nerrp_2sigma = count_statistic.compute_errp(2.0)\n\n# We compute the Cash statistic profile\nmu_signal = np.linspace(-1.5, 25, 100)\nstat_values = count_statistic._stat_fcn(mu_signal)\n\nxmin, xmax = -1.5, 25\nymin, ymax = -42, -28.0\nplt.figure(figsize=(5, 5))\nplt.plot(mu_signal, stat_values, color=""k"")\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\nplt.xlabel(r""Number of expected signal event, $\\mu_{sig}$"")\nplt.ylabel(r""Cash statistic value, TS "")\n\nplt.hlines(\n    count_statistic.TS_max + 1,\n    xmin=excess + errn,\n    xmax=excess + errp,\n    linestyle=""dotted"",\n    color=""r"",\n    label=""1 sigma (68% C.L.)"",\n)\nplt.vlines(\n    excess + errn,\n    ymin=ymin,\n    ymax=count_statistic.TS_max + 1,\n    linestyle=""dotted"",\n    color=""r"",\n)\nplt.vlines(\n    excess + errp,\n    ymin=ymin,\n    ymax=count_statistic.TS_max + 1,\n    linestyle=""dotted"",\n    color=""r"",\n)\n\nplt.hlines(\n    count_statistic.TS_max + 4,\n    xmin=excess + errn_2sigma,\n    xmax=excess + errp_2sigma,\n    linestyle=""dashed"",\n    color=""b"",\n    label=""2 sigma (95% C.L.)"",\n)\nplt.vlines(\n    excess + errn_2sigma,\n    ymin=ymin,\n    ymax=count_statistic.TS_max + 4,\n    linestyle=""dashed"",\n    color=""b"",\n)\nplt.vlines(\n    excess + errp_2sigma,\n    ymin=ymin,\n    ymax=count_statistic.TS_max + 4,\n    linestyle=""dashed"",\n    color=""b"",\n)\n\n\nplt.legend()\n'"
docs/stats/plot_cash_significance.py,1,"b'""""""Example plot showing the profile of the Cash statistic and its connection to significance.""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom gammapy.stats import CashCountsStatistic\n\ncount_statistic = CashCountsStatistic(n_on=13, mu_bkg=5.5)\nexcess = count_statistic.excess\n\n# We compute the Cash statistic profile\nmu_signal = np.linspace(-1.5, 25, 100)\nstat_values = count_statistic._stat_fcn(mu_signal)\n\nxmin, xmax = -1.5, 25\nymin, ymax = -42, -28.0\nplt.figure(figsize=(5, 5))\nplt.plot(mu_signal, stat_values, color=""k"")\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\n\nplt.xlabel(r""Number of expected signal event, $\\mu_{sig}$"")\nplt.ylabel(r""Cash statistic value, TS "")\nplt.vlines(\n    excess,\n    ymin=ymin,\n    ymax=count_statistic.TS_max,\n    linestyle=""dashed"",\n    color=""k"",\n    label=""Best fit"",\n)\nplt.hlines(\n    count_statistic.TS_max, xmin=xmin, xmax=excess, linestyle=""dashed"", color=""k""\n)\nplt.hlines(\n    count_statistic.TS_null,\n    xmin=xmin,\n    xmax=0,\n    linestyle=""dotted"",\n    color=""k"",\n    label=""Null hypothesis"",\n)\nplt.vlines(0, ymin=ymin, ymax=count_statistic.TS_null, linestyle=""dotted"", color=""k"")\n\nplt.vlines(excess, ymin=count_statistic.TS_max, ymax=count_statistic.TS_null, color=""r"")\nplt.hlines(count_statistic.TS_null, xmin=0, xmax=excess, linestyle=""dotted"", color=""r"")\nplt.legend()\n'"
docs/stats/plot_fc_gauss.py,5,"b'""""""\nCompute numerical solution for Gaussian with a boundary at the origin.\n\nProduces Fig. 10 from the Feldman & Cousins paper.\n""""""\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfrom gammapy.stats import (\n    fc_construct_acceptance_intervals_pdfs,\n    fc_fix_limits,\n    fc_get_limits,\n)\n\nsigma = 1\nn_sigma = 10\nn_bins_x = 1000\nstep_width_mu = 0.005\nmu_min = 0\nmu_max = 8\ncl = 0.90\n\nx_bins = np.linspace(-n_sigma * sigma, n_sigma * sigma, n_bins_x, endpoint=True)\nmu_bins = np.linspace(mu_min, mu_max, int(mu_max / step_width_mu + 1), endpoint=True)\n\nmatrix = [\n    dist / sum(dist)\n    for dist in (norm(loc=mu, scale=sigma).pdf(x_bins) for mu in mu_bins)\n]\n\nacceptance_intervals = fc_construct_acceptance_intervals_pdfs(matrix, cl)\n\nLowerLimitNum, UpperLimitNum, _ = fc_get_limits(mu_bins, x_bins, acceptance_intervals)\n\nfc_fix_limits(LowerLimitNum, UpperLimitNum)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nplt.plot(UpperLimitNum, mu_bins, ls=""-"", color=""red"")\nplt.plot(LowerLimitNum, mu_bins, ls=""-"", color=""red"")\n\nplt.grid(True)\nax.xaxis.set_ticks(np.arange(-10, 10, 1))\nax.xaxis.set_ticks(np.arange(-10, 10, 0.2), True)\nax.yaxis.set_ticks(np.arange(0, 8, 0.2), True)\nax.set_xlabel(r""Measured Mean x"")\nax.set_ylabel(r""Mean $\\mu$"")\nplt.axis([-2, 4, 0, 6])\nplt.show()\n'"
docs/stats/plot_fc_poisson.py,2,"b'""""""\nCompute numerical solution for Poisson with background.\n\nProduces Fig. 7 from the Feldman & Cousins paper.\n""""""\nimport numpy as np\nfrom scipy.stats import poisson\nimport matplotlib.pyplot as plt\nfrom gammapy.stats import (\n    fc_construct_acceptance_intervals_pdfs,\n    fc_fix_limits,\n    fc_get_limits,\n)\n\nbackground = 3.0\n\nn_bins_x = 100\nstep_width_mu = 0.005\nmu_min = 0\nmu_max = 50\ncl = 0.90\n\nx_bins = np.arange(0, n_bins_x)\nmu_bins = np.linspace(mu_min, mu_max, int(mu_max / step_width_mu + 1), endpoint=True)\n\nmatrix = [poisson(mu + background).pmf(x_bins) for mu in mu_bins]\n\nacceptance_intervals = fc_construct_acceptance_intervals_pdfs(matrix, cl)\n\nLowerLimitNum, UpperLimitNum, _ = fc_get_limits(mu_bins, x_bins, acceptance_intervals)\n\nfc_fix_limits(LowerLimitNum, UpperLimitNum)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nplt.plot(UpperLimitNum, mu_bins, ls=""-"", color=""red"")\nplt.plot(LowerLimitNum, mu_bins, ls=""-"", color=""red"")\n\nplt.grid(True)\nax.yaxis.set_label_coords(-0.08, 0.5)\nplt.xticks(range(15))\nplt.yticks(range(15))\nax.set_xlabel(r""Measured n"")\nax.set_ylabel(r""Signal Mean $\\mu$"")\nplt.axis([0, 15, 0, 15])\nplt.show()\n'"
docs/stats/plot_wstat_errors.py,1,"b'""""""Example plot showing the profile of the WStat statistic and its connection to excess errors.""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom gammapy.stats import WStatCountsStatistic\n\ncount_statistic = WStatCountsStatistic(n_on=13, n_off=11, alpha=0.5)\nexcess = count_statistic.excess\n\nerrn = count_statistic.compute_errn(1.0)\nerrp = count_statistic.compute_errp(1.0)\n\nerrn_2sigma = count_statistic.compute_errn(2.0)\nerrp_2sigma = count_statistic.compute_errp(2.0)\n\n# We compute the WStat statistic profile\nmu_signal = np.linspace(-2.5, 26, 100)\nstat_values = count_statistic._stat_fcn(mu_signal)\n\nxmin, xmax = -2.5, 26\nymin, ymax = 0, 15\nplt.figure(figsize=(5, 5))\nplt.plot(mu_signal, stat_values, color=""k"")\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\n\nplt.xlabel(r""Number of expected signal event, $\\mu_{sig}$"")\nplt.ylabel(r""WStat value, TS "")\n\nplt.hlines(\n    count_statistic.TS_max + 1,\n    xmin=excess + errn,\n    xmax=excess + errp,\n    linestyle=""dotted"",\n    color=""r"",\n    label=""1 sigma (68% C.L.)"",\n)\nplt.vlines(\n    excess + errn,\n    ymin=ymin,\n    ymax=count_statistic.TS_max + 1,\n    linestyle=""dotted"",\n    color=""r"",\n)\nplt.vlines(\n    excess + errp,\n    ymin=ymin,\n    ymax=count_statistic.TS_max + 1,\n    linestyle=""dotted"",\n    color=""r"",\n)\n\nplt.hlines(\n    count_statistic.TS_max + 4,\n    xmin=excess + errn_2sigma,\n    xmax=excess + errp_2sigma,\n    linestyle=""dashed"",\n    color=""b"",\n    label=""2 sigma (95% C.L.)"",\n)\nplt.vlines(\n    excess + errn_2sigma,\n    ymin=ymin,\n    ymax=count_statistic.TS_max + 4,\n    linestyle=""dashed"",\n    color=""b"",\n)\nplt.vlines(\n    excess + errp_2sigma,\n    ymin=ymin,\n    ymax=count_statistic.TS_max + 4,\n    linestyle=""dashed"",\n    color=""b"",\n)\n\nplt.legend()\n'"
docs/stats/plot_wstat_significance.py,1,"b'""""""Example plot showing the profile of the Wstat statistic and its connection to significance.""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom gammapy.stats import WStatCountsStatistic\n\ncount_statistic = WStatCountsStatistic(n_on=13, n_off=11, alpha=0.5)\nexcess = count_statistic.excess\n\n# We compute the WStat statistic profile\nmu_signal = np.linspace(-2.5, 26, 100)\nstat_values = count_statistic._stat_fcn(mu_signal)\n\nxmin, xmax = -2.5, 26\nymin, ymax = 0, 15\nplt.figure(figsize=(5, 5))\nplt.plot(mu_signal, stat_values, color=""k"")\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\n\nplt.xlabel(r""Number of expected signal event, $\\mu_{sig}$"")\nplt.ylabel(r""WStat value, TS "")\nplt.vlines(\n    excess,\n    ymin=ymin,\n    ymax=count_statistic.TS_max,\n    linestyle=""dashed"",\n    color=""k"",\n    label=""Best fit"",\n)\nplt.hlines(\n    count_statistic.TS_max, xmin=xmin, xmax=excess, linestyle=""dashed"", color=""k""\n)\nplt.hlines(\n    count_statistic.TS_null,\n    xmin=xmin,\n    xmax=0,\n    linestyle=""dotted"",\n    color=""k"",\n    label=""Null hypothesis"",\n)\nplt.vlines(0, ymin=ymin, ymax=count_statistic.TS_null, linestyle=""dotted"", color=""k"")\n\nplt.vlines(excess, ymin=count_statistic.TS_max, ymax=count_statistic.TS_null, color=""r"")\nplt.hlines(count_statistic.TS_null, xmin=0, xmax=excess, linestyle=""dotted"", color=""r"")\nplt.legend()\n'"
docs/visualization/colormap_example.py,0,"b'""""""Plot significance image with HESS and MILAGRO colormap.""""""\nfrom astropy.visualization import LinearStretch\nfrom astropy.visualization.mpl_normalize import ImageNormalize\nimport matplotlib.pyplot as plt\nfrom gammapy.maps import Map\nfrom gammapy.visualization import colormap_hess, colormap_milagro\n\nfilename = ""$GAMMAPY_DATA/tests/unbundled/poisson_stats_image/expected_ts_0.000.fits.gz""\nimage = Map.read(filename, hdu=""SQRT_TS"")\n\n# Plot with the HESS and Milagro colormap\nvmin, vmax, vtransition = -5, 15, 5\nfig = plt.figure(figsize=(15.5, 6))\n\nnormalize = ImageNormalize(vmin=vmin, vmax=vmax, stretch=LinearStretch())\ntransition = normalize(vtransition)\n\nax = fig.add_subplot(121, projection=image.geom.wcs)\ncmap = colormap_hess(transition=transition)\nimage.plot(ax=ax, cmap=cmap, norm=normalize, add_cbar=True)\nplt.title(""HESS-style colormap"")\n\nax = fig.add_subplot(122, projection=image.geom.wcs)\ncmap = colormap_milagro(transition=transition)\nimage.plot(ax=ax, cmap=cmap, norm=normalize, add_cbar=True)\nplt.title(""MILAGRO-style colormap"")\n\nplt.tight_layout()\nplt.show()\n'"
gammapy/analysis/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Gammapy high-level interface (analysis).""""""\nfrom .config import *\nfrom .core import *\n'"
gammapy/analysis/config.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport json\nimport logging\nfrom collections import defaultdict\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import List\nfrom astropy.coordinates import Angle\nfrom astropy.time import Time\nfrom astropy.units import Quantity\nimport yaml\nfrom pydantic import BaseModel, FilePath\nfrom pydantic.utils import deep_update\nfrom gammapy.makers import MapDatasetMaker\nfrom gammapy.utils.scripts import make_path, read_yaml\n\n__all__ = [""AnalysisConfig""]\n\nCONFIG_PATH = Path(__file__).resolve().parent / ""config""\nDOCS_FILE = CONFIG_PATH / ""docs.yaml""\n\nlog = logging.getLogger(__name__)\n\n\nclass AngleType(Angle):\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, v):\n        return Angle(v)\n\n\nclass EnergyType(Quantity):\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, v):\n        v = Quantity(v)\n        if v.unit.physical_type != ""energy"":\n            raise ValueError(f""Invalid unit for energy: {v.unit!r}"")\n        return v\n\n\nclass TimeType(Time):\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, v):\n        return Time(v)\n\n\nclass ReductionTypeEnum(str, Enum):\n    spectrum = ""1d""\n    cube = ""3d""\n\n\nclass FrameEnum(str, Enum):\n    icrs = ""icrs""\n    galactic = ""galactic""\n\n\nclass BackgroundMethodEnum(str, Enum):\n    reflected = ""reflected""\n    fov = ""fov_background""\n    ring = ""ring""\n\n\nclass SafeMaskMethodsEnum(str, Enum):\n    aeff_default = ""aeff-default""\n    aeff_max = ""aeff-max""\n    edisp_bias = ""edisp-bias""\n    offset_max = ""offset-max""\n    bkg_peak = ""bkg-peak""\n\n\nclass MapSelectionEnum(str, Enum):\n    counts = ""counts""\n    exposure = ""exposure""\n    background = ""background""\n    psf = ""psf""\n    edisp = ""edisp""\n\n\nclass GammapyBaseConfig(BaseModel):\n    class Config:\n        validate_all = True\n        validate_assignment = True\n        extra = ""forbid""\n        json_encoders = {\n            Angle: lambda v: f""{v.value} {v.unit}"",\n            Quantity: lambda v: f""{v.value} {v.unit}"",\n            Time: lambda v: f""{v.value}"",\n        }\n\n\nclass SkyCoordConfig(GammapyBaseConfig):\n    frame: FrameEnum = None\n    lon: AngleType = None\n    lat: AngleType = None\n\n\nclass EnergyAxisConfig(GammapyBaseConfig):\n    min: EnergyType = ""0.1 TeV""\n    max: EnergyType = ""10 TeV""\n    nbins: int = 30\n\n\nclass SpatialCircleConfig(GammapyBaseConfig):\n    frame: FrameEnum = None\n    lon: AngleType = None\n    lat: AngleType = None\n    radius: AngleType = None\n\n\nclass EnergyRangeConfig(GammapyBaseConfig):\n    min: EnergyType = ""0.1 TeV""\n    max: EnergyType = ""10 TeV""\n\n\nclass TimeRangeConfig(GammapyBaseConfig):\n    start: TimeType = None\n    stop: TimeType = None\n\n\nclass FluxPointsConfig(GammapyBaseConfig):\n    energy: EnergyAxisConfig = EnergyAxisConfig()\n    source: str = ""source""\n    parameters: dict = {}\n\n\nclass FitConfig(GammapyBaseConfig):\n    fit_range: EnergyRangeConfig = EnergyRangeConfig()\n\n\nclass BackgroundConfig(GammapyBaseConfig):\n    method: BackgroundMethodEnum = None\n    exclusion: FilePath = None\n    parameters: dict = {}\n\n\nclass SafeMaskConfig(GammapyBaseConfig):\n    methods: List[SafeMaskMethodsEnum] = [SafeMaskMethodsEnum.aeff_default]\n    parameters: dict = {}\n\n\nclass EnergyAxesConfig(GammapyBaseConfig):\n    energy: EnergyAxisConfig = EnergyAxisConfig()\n    energy_true: EnergyAxisConfig = EnergyAxisConfig()\n\n\nclass SelectionConfig(GammapyBaseConfig):\n    offset_max: AngleType = ""2.5 deg""\n\n\nclass FovConfig(GammapyBaseConfig):\n    width: AngleType = ""5 deg""\n    height: AngleType = ""5 deg""\n\n\nclass WcsConfig(GammapyBaseConfig):\n    skydir: SkyCoordConfig = SkyCoordConfig()\n    binsize: AngleType = ""0.02 deg""\n    fov: FovConfig = FovConfig()\n    binsize_irf: AngleType = ""0.2 deg""\n\n\nclass GeomConfig(GammapyBaseConfig):\n    wcs: WcsConfig = WcsConfig()\n    selection: SelectionConfig = SelectionConfig()\n    axes: EnergyAxesConfig = EnergyAxesConfig()\n\n\nclass DatasetsConfig(GammapyBaseConfig):\n    type: ReductionTypeEnum = ReductionTypeEnum.spectrum\n    stack: bool = True\n    geom: GeomConfig = GeomConfig()\n    map_selection: List[MapSelectionEnum] = MapDatasetMaker.available_selection\n    background: BackgroundConfig = BackgroundConfig()\n    safe_mask: SafeMaskConfig = SafeMaskConfig()\n    on_region: SpatialCircleConfig = SpatialCircleConfig()\n    containment_correction: bool = True\n\n\nclass ObservationsConfig(GammapyBaseConfig):\n    datastore: Path = Path(""$GAMMAPY_DATA/hess-dl3-dr1/"")\n    obs_ids: List[int] = []\n    obs_file: FilePath = None\n    obs_cone: SpatialCircleConfig = SpatialCircleConfig()\n    obs_time: TimeRangeConfig = TimeRangeConfig()\n\n\nclass LogConfig(GammapyBaseConfig):\n    level: str = ""info""\n    filename: Path = None\n    filemode: str = None\n    format: str = None\n    datefmt: str = None\n\n\nclass GeneralConfig(GammapyBaseConfig):\n    log: LogConfig = LogConfig()\n    outdir: str = "".""\n\n\nclass AnalysisConfig(GammapyBaseConfig):\n    """"""Gammapy analysis configuration.""""""\n\n    general: GeneralConfig = GeneralConfig()\n    observations: ObservationsConfig = ObservationsConfig()\n    datasets: DatasetsConfig = DatasetsConfig()\n    fit: FitConfig = FitConfig()\n    flux_points: FluxPointsConfig = FluxPointsConfig()\n\n    def __str__(self):\n        """"""Display settings in pretty YAML format.""""""\n        info = self.__class__.__name__ + ""\\n\\n\\t""\n        data = self.to_yaml()\n        data = data.replace(""\\n"", ""\\n\\t"")\n        info += data\n        return info.expandtabs(tabsize=4)\n\n    @classmethod\n    def read(cls, path):\n        """"""Reads from YAML file.""""""\n        config = read_yaml(path)\n        return AnalysisConfig(**config)\n\n    @classmethod\n    def from_yaml(cls, config_str):\n        """"""Create from YAML string.""""""\n        settings = yaml.safe_load(config_str)\n        return AnalysisConfig(**settings)\n\n    def write(self, path, overwrite=False):\n        """"""Write to YAML file.""""""\n        path = make_path(path)\n        if path.exists() and not overwrite:\n            raise IOError(f""File exists already: {path}"")\n        path.write_text(self.to_yaml())\n\n    def to_yaml(self):\n        """"""Convert to YAML string.""""""\n        # Here using `dict()` instead of `json()` would be more natural.\n        # We should change this once pydantic adds support for custom encoders\n        # to `dict()`. See https://github.com/samuelcolvin/pydantic/issues/1043\n        config = json.loads(self.json())\n        return yaml.dump(\n            config, sort_keys=False, indent=4, width=80, default_flow_style=None\n        )\n\n    def set_logging(self):\n        """"""Set logging config.\n\n        Calls ``logging.basicConfig``, i.e. adjusts global logging state.\n        """"""\n        self.general.log.level = self.general.log.level.upper()\n        logging.basicConfig(**self.general.log.dict())\n        log.info(""Setting logging config: {!r}"".format(self.general.log.dict()))\n\n    def update(self, config=None):\n        """"""Update config with provided settings.\n\n        Parameters\n        ----------\n        config : string dict or `AnalysisConfig` object\n            Configuration settings provided in dict() syntax.\n        """"""\n        if isinstance(config, str):\n            other = AnalysisConfig.from_yaml(config)\n        elif isinstance(config, AnalysisConfig):\n            other = config\n        else:\n            raise TypeError(f""Invalid type: {config}"")\n\n        config_new = deep_update(\n            self.dict(exclude_defaults=True), other.dict(exclude_defaults=True)\n        )\n        return AnalysisConfig(**config_new)\n\n    @staticmethod\n    def _get_doc_sections():\n        """"""Returns dict with commented docs from docs file""""""\n        doc = defaultdict(str)\n        with open(DOCS_FILE) as f:\n            for line in filter(lambda line: not line.startswith(""---""), f):\n                line = line.strip(""\\n"")\n                if line.startswith(""# Section: ""):\n                    keyword = line.replace(""# Section: "", """")\n                doc[keyword] += line + ""\\n""\n        return doc\n'"
gammapy/analysis/core.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Session class driving the high-level interface API""""""\nimport logging\nfrom astropy.coordinates import SkyCoord\nfrom astropy.table import Table\nfrom regions import CircleSkyRegion\nfrom gammapy.analysis.config import AnalysisConfig\nfrom gammapy.data import DataStore\nfrom gammapy.datasets import Datasets, FluxPointsDataset, MapDataset, SpectrumDataset\nfrom gammapy.estimators import FluxPointsEstimator\nfrom gammapy.makers import (\n    FoVBackgroundMaker,\n    MapDatasetMaker,\n    ReflectedRegionsBackgroundMaker,\n    RingBackgroundMaker,\n    SafeMaskMaker,\n    SpectrumDatasetMaker,\n)\nfrom gammapy.maps import Map, MapAxis, WcsGeom\nfrom gammapy.modeling import Fit\nfrom gammapy.modeling.models import BackgroundModel, Models\nfrom gammapy.utils.scripts import make_path\n\n__all__ = [""Analysis""]\n\nlog = logging.getLogger(__name__)\n\n\nclass Analysis:\n    """"""Config-driven high-level analysis interface.\n\n    It is initialized by default with a set of configuration parameters and values declared in\n    an internal high-level interface model, though the user can also provide configuration\n    parameters passed as a nested dictionary at the moment of instantiation. In that case these\n    parameters will overwrite the default values of those present in the configuration file.\n\n    For more info see  :ref:`analysis`.\n\n    Parameters\n    ----------\n    config : dict or `AnalysisConfig`\n        Configuration options following `AnalysisConfig` schema\n    """"""\n\n    def __init__(self, config):\n        self.config = config\n        self.config.set_logging()\n        self.datastore = None\n        self.observations = None\n        self.datasets = None\n        self.models = None\n        self.fit = None\n        self.fit_result = None\n        self.flux_points = None\n\n    @property\n    def config(self):\n        """"""Analysis configuration (`AnalysisConfig`)""""""\n        return self._config\n\n    @config.setter\n    def config(self, value):\n        if isinstance(value, dict):\n            self._config = AnalysisConfig(**value)\n        elif isinstance(value, AnalysisConfig):\n            self._config = value\n        else:\n            raise TypeError(""config must be dict or AnalysisConfig."")\n\n    def get_observations(self):\n        """"""Fetch observations from the data store according to criteria defined in the configuration.""""""\n        observations_settings = self.config.observations\n        path = make_path(observations_settings.datastore)\n        if path.is_file():\n            self.datastore = DataStore.from_file(path)\n        elif path.is_dir():\n            self.datastore = DataStore.from_dir(path)\n        else:\n            raise FileNotFoundError(f""Datastore not found: {path}"")\n\n        log.info(""Fetching observations."")\n        if (\n            len(observations_settings.obs_ids)\n            and observations_settings.obs_file is not None\n        ):\n            raise ValueError(\n                ""Values for both parameters obs_ids and obs_file are not accepted.""\n            )\n        elif (\n            not len(observations_settings.obs_ids)\n            and observations_settings.obs_file is None\n        ):\n            obs_list = self.datastore.get_observations()\n            ids = [obs.obs_id for obs in obs_list]\n        elif len(observations_settings.obs_ids):\n            obs_list = self.datastore.get_observations(observations_settings.obs_ids)\n            ids = [obs.obs_id for obs in obs_list]\n        else:\n            path = make_path(observations_settings.obs_file)\n            ids = list(Table.read(path, format=""ascii"", data_start=0).columns[0])\n\n        if observations_settings.obs_cone.lon is not None:\n            cone = dict(\n                type=""sky_circle"",\n                frame=observations_settings.obs_cone.frame,\n                lon=observations_settings.obs_cone.lon,\n                lat=observations_settings.obs_cone.lat,\n                radius=observations_settings.obs_cone.radius,\n                border=""0 deg"",\n            )\n            selected_cone = self.datastore.obs_table.select_observations(cone)\n            ids = list(set(ids) & set(selected_cone[""OBS_ID""].tolist()))\n        self.observations = self.datastore.get_observations(ids, skip_missing=True)\n        if observations_settings.obs_time.start is not None:\n            start = observations_settings.obs_time.start\n            stop = observations_settings.obs_time.stop\n            self.observations = self.observations.select_time([(start, stop)])\n        log.info(f""Number of selected observations: {len(self.observations)}"")\n        for obs in self.observations:\n            log.debug(obs)\n\n    def get_datasets(self):\n        """"""Produce reduced datasets.""""""\n        datasets_settings = self.config.datasets\n        if not self.observations or len(self.observations) == 0:\n            raise RuntimeError(""No observations have been selected."")\n\n        if datasets_settings.type == ""1d"":\n            self._spectrum_extraction()\n        else:  # 3d\n            self._map_making()\n\n    def set_models(self, models):\n        """"""Set models on datasets.\n\n        Parameters\n        ----------\n        models : `~gammapy.modeling.models.Models` or str\n            Models object or YAML models string\n        """"""\n        if not self.datasets or len(self.datasets) == 0:\n            raise RuntimeError(""Missing datasets"")\n\n        log.info(f""Reading model."")\n        if isinstance(models, str):\n            self.models = Models.from_yaml(models)\n        elif isinstance(models, Models):\n            self.models = models\n        else:\n            raise TypeError(f""Invalid type: {models!r}"")\n\n        self.datasets.models.extend(self.models)\n\n        log.info(self.models)\n\n    def read_models(self, path):\n        """"""Read models from YAML file.""""""\n        path = make_path(path)\n        models = Models.read(path)\n        self.set_models(models)\n\n    def run_fit(self, optimize_opts=None):\n        """"""Fitting reduced datasets to model.""""""\n        if not self.models:\n            raise RuntimeError(""Missing models"")\n\n        fit_settings = self.config.fit\n        for dataset in self.datasets:\n            if fit_settings.fit_range:\n                e_min = fit_settings.fit_range.min\n                e_max = fit_settings.fit_range.max\n                dataset.mask_fit = dataset.counts.geom.energy_mask(e_min, e_max)\n\n        log.info(""Fitting datasets."")\n        self.fit = Fit(self.datasets)\n        self.fit_result = self.fit.run(optimize_opts=optimize_opts)\n        log.info(self.fit_result)\n\n    def get_flux_points(self):\n        """"""Calculate flux points for a specific model component.""""""\n        if not self.fit:\n            raise RuntimeError(""No results available from Fit."")\n\n        fp_settings = self.config.flux_points\n        log.info(""Calculating flux points."")\n        e_edges = self._make_energy_axis(fp_settings.energy).edges\n        flux_point_estimator = FluxPointsEstimator(\n            e_edges=e_edges, source=fp_settings.source, **fp_settings.parameters,\n        )\n        fp = flux_point_estimator.run(datasets=self.datasets)\n        fp.table[""is_ul""] = fp.table[""ts""] < 4\n        self.flux_points = FluxPointsDataset(\n            data=fp, models=self.models[fp_settings.source]\n        )\n        cols = [""e_ref"", ""ref_flux"", ""dnde"", ""dnde_ul"", ""dnde_err"", ""is_ul""]\n        log.info(""\\n{}"".format(self.flux_points.data.table[cols]))\n\n    def update_config(self, config):\n        self.config = self.config.update(config=config)\n\n    def _create_geometry(self):\n        """"""Create the geometry.""""""\n        geom_params = {}\n        geom_settings = self.config.datasets.geom\n        skydir_settings = geom_settings.wcs.skydir\n        if skydir_settings.lon is not None:\n            skydir = SkyCoord(\n                skydir_settings.lon, skydir_settings.lat, frame=skydir_settings.frame\n            )\n            geom_params[""skydir""] = skydir\n        if skydir_settings.frame == ""icrs"":\n            geom_params[""frame""] = ""icrs""\n        if skydir_settings.frame == ""galactic"":\n            geom_params[""frame""] = ""galactic""\n        axes = [self._make_energy_axis(geom_settings.axes.energy)]\n        geom_params[""axes""] = axes\n        geom_params[""binsz""] = geom_settings.wcs.binsize\n        width = geom_settings.wcs.fov.width.to(""deg"").value\n        height = geom_settings.wcs.fov.height.to(""deg"").value\n        geom_params[""width""] = (width, height)\n        return WcsGeom.create(**geom_params)\n\n    def _map_making(self):\n        """"""Make maps and datasets for 3d analysis.""""""\n        datasets_settings = self.config.datasets\n        log.info(""Creating geometry."")\n        geom = self._create_geometry()\n        geom_settings = datasets_settings.geom\n        geom_irf = dict(energy_axis_true=None, binsz_irf=None)\n        if geom_settings.axes.energy_true.min is not None:\n            geom_irf[""energy_axis_true""] = self._make_energy_axis(\n                geom_settings.axes.energy_true, name=""energy_true""\n            )\n        geom_irf[""binsz_irf""] = geom_settings.wcs.binsize_irf.to(""deg"").value\n        offset_max = geom_settings.selection.offset_max\n        log.info(""Creating datasets."")\n\n        maker = MapDatasetMaker(selection=datasets_settings.map_selection)\n\n        safe_mask_selection = datasets_settings.safe_mask.methods\n        safe_mask_settings = datasets_settings.safe_mask.parameters\n        maker_safe_mask = SafeMaskMaker(\n            methods=safe_mask_selection, **safe_mask_settings\n        )\n\n        bkg_maker_config = {}\n        if datasets_settings.background.exclusion:\n            exclusion_region = Map.read(datasets_settings.background.exclusion)\n            bkg_maker_config[""exclusion_mask""] = exclusion_region\n        bkg_maker_config.update(datasets_settings.background.parameters)\n\n        bkg_method = datasets_settings.background.method\n        if bkg_method == ""fov_background"":\n            log.debug(f""Creating FoVBackgroundMaker with arguments {bkg_maker_config}"")\n            bkg_maker = FoVBackgroundMaker(**bkg_maker_config)\n        elif bkg_method == ""ring"":\n            bkg_maker = RingBackgroundMaker(**bkg_maker_config)\n            log.debug(f""Creating RingBackgroundMaker with arguments {bkg_maker_config}"")\n            if datasets_settings.geom.axes.energy.nbins > 1:\n                raise ValueError(\n                    ""You need to define a single-bin energy geometry for your dataset.""\n                )\n        else:\n            bkg_maker = None\n            log.warning(\n                f""No background maker set for 3d analysis. Check configuration.""\n            )\n\n        stacked = MapDataset.create(geom=geom, name=""stacked"", **geom_irf)\n\n        if datasets_settings.stack:\n            for obs in self.observations:\n                log.info(f""Processing observation {obs.obs_id}"")\n                cutout = stacked.cutout(obs.pointing_radec, width=2 * offset_max)\n                dataset = maker.run(cutout, obs)\n                dataset = maker_safe_mask.run(dataset, obs)\n                if bkg_maker is not None:\n                    dataset = bkg_maker.run(dataset)\n                if bkg_method == ""ring"":\n                    dataset.models = Models([BackgroundModel(dataset.background)])\n                log.debug(dataset)\n                stacked.stack(dataset)\n            datasets = [stacked]\n        else:\n            datasets = []\n            for obs in self.observations:\n                log.info(f""Processing observation {obs.obs_id}"")\n                cutout = stacked.cutout(obs.pointing_radec, width=2 * offset_max)\n                dataset = maker.run(cutout, obs)\n                dataset = maker_safe_mask.run(dataset, obs)\n                if bkg_maker is not None:\n                    dataset = bkg_maker.run(dataset)\n                log.debug(dataset)\n                datasets.append(dataset)\n        self.datasets = Datasets(datasets)\n\n    def _spectrum_extraction(self):\n        """"""Run all steps for the spectrum extraction.""""""\n        log.info(""Reducing spectrum datasets."")\n        datasets_settings = self.config.datasets\n        on_lon = datasets_settings.on_region.lon\n        on_lat = datasets_settings.on_region.lat\n        on_center = SkyCoord(on_lon, on_lat, frame=datasets_settings.on_region.frame)\n        on_region = CircleSkyRegion(on_center, datasets_settings.on_region.radius)\n\n        maker_config = {}\n        if datasets_settings.containment_correction:\n            maker_config[\n                ""containment_correction""\n            ] = datasets_settings.containment_correction\n        e_reco = self._make_energy_axis(datasets_settings.geom.axes.energy)\n\n        maker_config[""selection""] = [""counts"", ""aeff"", ""edisp""]\n        dataset_maker = SpectrumDatasetMaker(**maker_config)\n\n        bkg_maker_config = {}\n        if datasets_settings.background.exclusion:\n            exclusion_region = Map.read(datasets_settings.background.exclusion)\n            bkg_maker_config[""exclusion_mask""] = exclusion_region\n        bkg_maker_config.update(datasets_settings.background.parameters)\n        bkg_method = datasets_settings.background.method\n        if bkg_method == ""reflected"":\n            bkg_maker = ReflectedRegionsBackgroundMaker(**bkg_maker_config)\n            log.debug(\n                f""Creating ReflectedRegionsBackgroundMaker with arguments {bkg_maker_config}""\n            )\n        else:\n            bkg_maker = None\n            log.warning(\n                f""No background maker set for 1d analysis. Check configuration.""\n            )\n\n        safe_mask_selection = datasets_settings.safe_mask.methods\n        safe_mask_settings = datasets_settings.safe_mask.parameters\n        safe_mask_maker = SafeMaskMaker(\n            methods=safe_mask_selection, **safe_mask_settings\n        )\n\n        e_true = self._make_energy_axis(\n            datasets_settings.geom.axes.energy_true, name=""energy_true""\n        )\n\n        reference = SpectrumDataset.create(\n            e_reco=e_reco, e_true=e_true, region=on_region\n        )\n\n        datasets = []\n        for obs in self.observations:\n            log.info(f""Processing observation {obs.obs_id}"")\n            dataset = dataset_maker.run(reference.copy(), obs)\n            if bkg_maker is not None:\n                dataset = bkg_maker.run(dataset, obs)\n                if dataset.counts_off is None:\n                    log.info(\n                        f""No OFF region found for observation {obs.obs_id}. Discarding.""\n                    )\n                    continue\n            dataset = safe_mask_maker.run(dataset, obs)\n            log.debug(dataset)\n            datasets.append(dataset)\n\n        self.datasets = Datasets(datasets)\n\n        if datasets_settings.stack:\n            stacked = self.datasets.stack_reduce(name=""stacked"")\n            self.datasets = Datasets([stacked])\n\n    @staticmethod\n    def _make_energy_axis(axis, name=""energy""):\n        return MapAxis.from_bounds(\n            name=name,\n            lo_bnd=axis.min.value,\n            hi_bnd=axis.max.to_value(axis.min.unit),\n            nbin=axis.nbins,\n            unit=axis.min.unit,\n            interp=""log"",\n            node_type=""edges"",\n        )\n'"
gammapy/astro/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Astrophysical source and population models.""""""\n'"
gammapy/catalog/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Source catalogs.""""""\nfrom gammapy.utils.registry import Registry\nfrom .core import *\nfrom .fermi import *\nfrom .gammacat import *\nfrom .hawc import *\nfrom .hess import *\n\nSOURCE_CATALOGS = Registry(\n    [\n        SourceCatalogGammaCat,\n        SourceCatalogHGPS,\n        SourceCatalog2HWC,\n        SourceCatalog3FGL,\n        SourceCatalog4FGL,\n        SourceCatalog2FHL,\n        SourceCatalog3FHL,\n    ]\n)\n""""""Registry of source catalogs in Gammapy.""""""\n\n__all__ = [\n    ""SOURCE_CATALOGS"",\n    ""SourceCatalog"",\n    ""SourceCatalogObject"",\n    ""SourceCatalogObjectHGPS"",\n    ""SourceCatalogObject2FHL"",\n    ""SourceCatalogObject3FHL"",\n    ""SourceCatalogObject3FGL"",\n    ""SourceCatalogObject4FGL"",\n    ""SourceCatalogObject2HWC"",\n    ""SourceCatalogObjectGammaCat"",\n    ""SourceCatalogObjectHGPSComponent"",\n    ""SourceCatalogLargeScaleHGPS"",\n]\n\n__all__.extend(cls.__name__ for cls in SOURCE_CATALOGS)\n'"
gammapy/catalog/core.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Source catalog and object base classes.""""""\nimport abc\nimport numbers\nfrom astropy.coordinates import SkyCoord\nfrom astropy.utils import lazyproperty\nfrom gammapy.utils.table import table_from_row_data, table_row_to_dict\n\n__all__ = [""SourceCatalog"", ""SourceCatalogObject""]\n\n\n# https://pydanny.blogspot.com/2011/11/loving-bunch-class.html\nclass Bunch(dict):\n    def __init__(self, **kw):\n        dict.__init__(self, kw)\n        self.__dict__.update(kw)\n\n\nclass SourceCatalogObject:\n    """"""Source catalog object.\n\n    This class can be used directly, but it\'s mostly used as a\n    base class for the other source catalog classes.\n\n    The catalog data on this source is stored in the `source.data`\n    attribute as a dict.\n\n    The source catalog object is decoupled from the source catalog,\n    it doesn\'t hold a reference back to it, except for a key\n    ``_row_index`` of type ``int`` that links to the catalog table\n    row the source information comes from.\n    """"""\n\n    _source_name_key = ""Source_Name""\n    _row_index_key = ""_row_index""\n\n    def __init__(self, data, data_extended=None):\n        self.data = Bunch(**data)\n        if data_extended:\n            self.data_extended = Bunch(**data_extended)\n\n    @property\n    def name(self):\n        """"""Source name (str)""""""\n        name = self.data[self._source_name_key]\n        return name.strip()\n\n    @property\n    def row_index(self):\n        """"""Row index of source in catalog (int)""""""\n        return self.data[self._row_index_key]\n\n    @property\n    def position(self):\n        """"""Source position (`~astropy.coordinates.SkyCoord`).""""""\n        table = table_from_row_data([self.data])\n        return _skycoord_from_table(table)[0]\n\n\nclass SourceCatalog(abc.ABC):\n    """"""Generic source catalog.\n\n    This class can be used directly, but it\'s mostly used as a\n    base class for the other source catalog classes.\n\n    This is a thin wrapper around `~astropy.table.Table`,\n    which is stored in the ``catalog.table`` attribute.\n\n    Parameters\n    ----------\n    table : `~astropy.table.Table`\n        Table with catalog data.\n    source_name_key : str\n        Column with source name information\n    source_name_alias : tuple of str\n        Columns with source name aliases. This will allow accessing the source\n        row by alias names as well.\n    """"""\n\n    @classmethod\n    @abc.abstractmethod\n    def description(cls):\n        """"""Catalog description (str).""""""\n        pass\n\n    source_object_class = SourceCatalogObject\n    """"""Source class (`SourceCatalogObject`).""""""\n\n    def __init__(self, table, source_name_key=""Source_Name"", source_name_alias=()):\n        self.table = table\n        self._source_name_key = source_name_key\n        self._source_name_alias = source_name_alias\n\n    def __str__(self):\n        return (\n            f""{self.__class__.__name__}:\\n""\n            f""    name: {self.name}\\n""\n            f""    description: {self.description}\\n""\n            f""    sources: {len(self.table)}\\n""\n        )\n\n    @lazyproperty\n    def _name_to_index_cache(self):\n        # Make a dict for quick lookup: source name -> row index\n        names = dict()\n        for idx, row in enumerate(self.table):\n            name = row[self._source_name_key]\n            names[name.strip()] = idx\n            for alias_column in self._source_name_alias:\n                for alias in row[alias_column].split("",""):\n                    if not alias == """":\n                        names[alias.strip()] = idx\n        return names\n\n    def row_index(self, name):\n        """"""Look up row index of source by name.\n\n        Parameters\n        ----------\n        name : str\n            Source name\n\n        Returns\n        -------\n        index : int\n            Row index of source in table\n        """"""\n        index = self._name_to_index_cache[name]\n        row = self.table[index]\n        # check if name lookup is correct other wise recompute _name_to_index_cache\n\n        possible_names = [row[self._source_name_key]]\n        for alias_column in self._source_name_alias:\n            possible_names += row[alias_column].split("","")\n\n        if name not in possible_names:\n            self.__dict__.pop(""_name_to_index_cache"")\n            index = self._name_to_index_cache[name]\n\n        return index\n\n    def source_name(self, index):\n        """"""Look up source name by row index.\n\n        Parameters\n        ----------\n        index : int\n            Row index of source in table\n        """"""\n        source_name_col = self.table[self._source_name_key]\n        name = source_name_col[index]\n        return name.strip()\n\n    def __getitem__(self, key):\n        """"""Get source by name.\n\n        Parameters\n        ----------\n        key : str or int\n            Source name or row index\n\n        Returns\n        -------\n        source : `SourceCatalogObject`\n            An object representing one source\n        """"""\n        if isinstance(key, str):\n            index = self.row_index(key)\n        elif isinstance(key, numbers.Integral):\n            index = key\n        else:\n            raise TypeError(f""Invalid key: {key!r}, {type(key)}\\n"")\n\n        return self._make_source_object(index)\n\n    def _make_source_object(self, index):\n        """"""Make one source object.\n\n        Parameters\n        ----------\n        index : int\n            Row index\n\n        Returns\n        -------\n        source : `SourceCatalogObject`\n            Source object\n        """"""\n        data = table_row_to_dict(self.table[index])\n        data[SourceCatalogObject._row_index_key] = index\n\n        if ""Extended_Source_Name"" in data:\n            name_extended = data[""Extended_Source_Name""].strip()\n        elif ""Source_Name"" in data:\n            name_extended = data[""Source_Name""].strip()\n        else:\n            name_extended = None\n        try:\n            idx = self._lookup_extended_source_idx[name_extended]\n            data_extended = table_row_to_dict(self.extended_sources_table[idx])\n        except (KeyError, AttributeError):\n            data_extended = None\n\n        source = self.source_object_class(data, data_extended)\n        return source\n\n    @lazyproperty\n    def _lookup_extended_source_idx(self):\n        names = [_.strip() for _ in self.extended_sources_table[""Source_Name""]]\n        idx = range(len(names))\n        return dict(zip(names, idx))\n\n    @property\n    def positions(self):\n        """"""Source positions (`~astropy.coordinates.SkyCoord`).""""""\n        return _skycoord_from_table(self.table)\n\n\ndef _skycoord_from_table(table):\n    keys = table.colnames\n\n    if {""RAJ2000"", ""DEJ2000""}.issubset(keys):\n        lon, lat, frame = ""RAJ2000"", ""DEJ2000"", ""icrs""\n    elif {""RA"", ""DEC""}.issubset(keys):\n        lon, lat, frame = ""RA"", ""DEC"", ""icrs""\n    elif {""ra"", ""dec""}.issubset(keys):\n        lon, lat, frame = ""ra"", ""dec"", ""icrs""\n    else:\n        raise KeyError(""No column GLON / GLAT or RA / DEC or RAJ2000 / DEJ2000 found."")\n\n    unit = table[lon].unit.to_string() if table[lon].unit else ""deg""\n\n    return SkyCoord(table[lon], table[lat], unit=unit, frame=frame)\n'"
gammapy/catalog/fermi.py,25,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Fermi catalog and source classes.""""""\nimport abc\nimport warnings\nimport numpy as np\nimport astropy.units as u\nfrom astropy.table import Column, Table\nfrom astropy.time import Time\nfrom astropy.wcs import FITSFixedWarning\nfrom gammapy.estimators import FluxPoints, LightCurve\nfrom gammapy.modeling.models import (\n    DiskSpatialModel,\n    GaussianSpatialModel,\n    Model,\n    PointSpatialModel,\n    SkyModel,\n    TemplateSpatialModel,\n)\nfrom gammapy.utils.gauss import Gauss2DPDF\nfrom gammapy.utils.scripts import make_path\nfrom gammapy.utils.table import table_standardise_units_inplace\nfrom .core import SourceCatalog, SourceCatalogObject\n\n__all__ = [\n    ""SourceCatalogObject4FGL"",\n    ""SourceCatalogObject3FGL"",\n    ""SourceCatalogObject2FHL"",\n    ""SourceCatalogObject3FHL"",\n    ""SourceCatalog4FGL"",\n    ""SourceCatalog3FGL"",\n    ""SourceCatalog2FHL"",\n    ""SourceCatalog3FHL"",\n]\n\n\ndef compute_flux_points_ul(quantity, quantity_errp):\n    """"""Compute UL value for fermi flux points.\n\n    See https://arxiv.org/pdf/1501.02003.pdf (page 30)\n    """"""\n    return 2 * quantity_errp + quantity\n\n\nclass SourceCatalogObjectFermiBase(SourceCatalogObject, abc.ABC):\n    """"""Base class for Fermi-LAT catalogs.""""""\n\n    asso = [""ASSOC1"", ""ASSOC2"", ""ASSOC_TEV"", ""ASSOC_GAM1"", ""ASSOC_GAM2"", ""ASSOC_GAM3""]\n\n    def __str__(self):\n        return self.info()\n\n    def info(self, info=""all""):\n        """"""Summary info string.\n\n        Parameters\n        ----------\n        info : {\'all\', \'basic\', \'more\', \'position\', \'spectral\',\'lightcurve\'}\n            Comma separated list of options\n        """"""\n        if info == ""all"":\n            info = ""basic,more,position,spectral,lightcurve""\n\n        ss = """"\n        ops = info.split("","")\n        if ""basic"" in ops:\n            ss += self._info_basic()\n        if ""more"" in ops:\n            ss += self._info_more()\n        if ""position"" in ops:\n            ss += self._info_position()\n            if not self.is_pointlike:\n                ss += self._info_morphology()\n        if ""spectral"" in ops:\n            ss += self._info_spectral_fit()\n            ss += self._info_spectral_points()\n        if ""lightcurve"" in ops:\n            ss += self._info_lightcurve()\n        return ss\n\n    def _info_basic(self):\n        d = self.data\n        keys = self.asso\n        ss = ""\\n*** Basic info ***\\n\\n""\n        ss += ""Catalog row index (zero-based) : {}\\n"".format(self.row_index)\n        ss += ""{:<20s} : {}\\n"".format(""Source name"", self.name)\n        if ""Extended_Source_Name"" in d:\n            ss += ""{:<20s} : {}\\n"".format(""Extended name"", d[""Extended_Source_Name""])\n\n        def get_nonentry_keys(keys):\n            vals = [d[_].strip() for _ in keys]\n            return "", "".join([_ for _ in vals if _ != """"])\n\n        associations = get_nonentry_keys(keys)\n        ss += ""{:<16s} : {}\\n"".format(""Associations"", associations)\n        try:\n            ss += ""{:<16s} : {:.3f}\\n"".format(""ASSOC_PROB_BAY"", d[""ASSOC_PROB_BAY""])\n            ss += ""{:<16s} : {:.3f}\\n"".format(""ASSOC_PROB_LR"", d[""ASSOC_PROB_LR""])\n        except (KeyError):\n            pass\n        try:\n            ss += ""{:<16s} : {}\\n"".format(""Class1"", d[""CLASS1""])\n        except (KeyError):\n            ss += ""{:<16s} : {}\\n"".format(""Class"", d[""CLASS""])\n        try:\n            ss += ""{:<16s} : {}\\n"".format(""Class2"", d[""CLASS2""])\n        except (KeyError):\n            pass\n        ss += ""{:<16s} : {}\\n"".format(""TeVCat flag"", d.get(""TEVCAT_FLAG"", ""N/A""))\n        return ss\n\n    @abc.abstractmethod\n    def _info_more(self):\n        pass\n\n    def _info_position(self):\n        d = self.data\n        ss = ""\\n*** Position info ***\\n\\n""\n        ss += ""{:<20s} : {:.3f}\\n"".format(""RA"", d[""RAJ2000""])\n        ss += ""{:<20s} : {:.3f}\\n"".format(""DEC"", d[""DEJ2000""])\n        ss += ""{:<20s} : {:.3f}\\n"".format(""GLON"", d[""GLON""])\n        ss += ""{:<20s} : {:.3f}\\n"".format(""GLAT"", d[""GLAT""])\n\n        ss += ""\\n""\n        ss += ""{:<20s} : {:.4f}\\n"".format(""Semimajor (68%)"", d[""Conf_68_SemiMajor""])\n        ss += ""{:<20s} : {:.4f}\\n"".format(""Semiminor (68%)"", d[""Conf_68_SemiMinor""])\n        ss += ""{:<20s} : {:.2f}\\n"".format(""Position angle (68%)"", d[""Conf_68_PosAng""])\n        ss += ""{:<20s} : {:.4f}\\n"".format(""Semimajor (95%)"", d[""Conf_95_SemiMajor""])\n        ss += ""{:<20s} : {:.4f}\\n"".format(""Semiminor (95%)"", d[""Conf_95_SemiMinor""])\n        ss += ""{:<20s} : {:.2f}\\n"".format(""Position angle (95%)"", d[""Conf_95_PosAng""])\n        ss += ""{:<20s} : {:.0f}\\n"".format(""ROI number"", d[""ROI_num""])\n        return ss\n\n    def _info_morphology(self):\n        e = self.data_extended\n        ss = ""\\n*** Extended source information ***\\n\\n""\n        ss += ""{:<16s} : {}\\n"".format(""Model form"", e[""Model_Form""])\n        ss += ""{:<16s} : {:.4f}\\n"".format(""Model semimajor"", e[""Model_SemiMajor""])\n        ss += ""{:<16s} : {:.4f}\\n"".format(""Model semiminor"", e[""Model_SemiMinor""])\n        ss += ""{:<16s} : {:.4f}\\n"".format(""Position angle"", e[""Model_PosAng""])\n        try:\n            ss += ""{:<16s} : {}\\n"".format(""Spatial function"", e[""Spatial_Function""])\n        except KeyError:\n            pass\n        ss += ""{:<16s} : {}\\n\\n"".format(""Spatial filename"", e[""Spatial_Filename""])\n        return ss\n\n    def _info_spectral_fit(self):\n        return ""\\n""\n\n    def _info_spectral_points(self):\n        ss = ""\\n*** Spectral points ***\\n\\n""\n        lines = self.flux_points.table_formatted.pformat(max_width=-1, max_lines=-1)\n        ss += ""\\n"".join(lines)\n        return ss\n\n    def _info_lightcurve(self):\n        return ""\\n""\n\n    @property\n    def is_pointlike(self):\n        return self.data[""Extended_Source_Name""].strip() == """"\n\n    # FIXME: this should be renamed `set_position_error`,\n    # and `phi_0` isn\'t filled correctly, other parameters missing\n    # see https://github.com/gammapy/gammapy/pull/2533#issuecomment-553329049\n    def _set_spatial_errors(self, model):\n        d = self.data\n\n        if ""Pos_err_68"" in d:\n            percent = 0.68\n            semi_minor = d[""Pos_err_68""]\n            semi_major = d[""Pos_err_68""]\n            phi_0 = 0.0\n        else:\n            percent = 0.95\n            semi_minor = d[""Conf_95_SemiMinor""]\n            semi_major = d[""Conf_95_SemiMajor""]\n            phi_0 = d[""Conf_95_PosAng""]\n\n        if np.isnan(phi_0):\n            phi_0 = 0.0 * u.deg\n\n        scale_1sigma = Gauss2DPDF().containment_radius(percent)\n        lat_err = semi_major / scale_1sigma\n        lon_err = semi_minor / scale_1sigma / np.cos(d[""DEJ2000""])\n\n        if model.tag != ""TemplateSpatialModel"":\n            model.parameters[""lon_0""].error = lon_err\n            model.parameters[""lat_0""].error = lat_err\n            model.phi_0 = phi_0\n\n    def sky_model(self):\n        """"""Sky model (`~gammapy.modeling.models.SkyModel`).""""""\n        return SkyModel(\n            spatial_model=self.spatial_model(),\n            spectral_model=self.spectral_model(),\n            name=self.name,\n        )\n\n\nclass SourceCatalogObject4FGL(SourceCatalogObjectFermiBase):\n    """"""One source from the Fermi-LAT 4FGL catalog.\n\n    Catalog is represented by `~gammapy.catalog.SourceCatalog4FGL`.\n    """"""\n\n    asso = [\n        ""ASSOC1"",\n        ""ASSOC2"",\n        ""ASSOC_TEV"",\n        ""ASSOC_FGL"",\n        ""ASSOC_FHL"",\n        ""ASSOC_GAM1"",\n        ""ASSOC_GAM2"",\n        ""ASSOC_GAM3"",\n    ]\n    _ebounds = u.Quantity([50, 100, 300, 1000, 3000, 10000, 30000, 300000], ""MeV"")\n\n    def _info_more(self):\n        d = self.data\n        ss = ""\\n*** Other info ***\\n\\n""\n        fmt = ""{:<32s} : {:.3f}\\n""\n        ss += fmt.format(""Significance (100 MeV - 1 TeV)"", d[""Signif_Avg""])\n        ss += ""{:<32s} : {:.1f}\\n"".format(""Npred"", d[""Npred""])\n        ss += ""\\n{:<20s} : {}\\n"".format(""Other flags"", d[""Flags""])\n        return ss\n\n    def _info_spectral_fit(self):\n        d = self.data\n        spec_type = d[""SpectrumType""].strip()\n\n        ss = ""\\n*** Spectral info ***\\n\\n""\n\n        ss += ""{:<45s} : {}\\n"".format(""Spectrum type"", d[""SpectrumType""])\n        fmt = ""{:<45s} : {:.3f}\\n""\n        ss += fmt.format(""Detection significance (100 MeV - 1 TeV)"", d[""Signif_Avg""])\n\n        if spec_type == ""PowerLaw"":\n            tag = ""PL""\n        elif spec_type == ""LogParabola"":\n            tag = ""LP""\n            ss += ""{:<45s} : {:.4f} +- {:.5f}\\n"".format(\n                ""beta"", d[""LP_beta""], d[""Unc_LP_beta""]\n            )\n            ss += ""{:<45s} : {:.1f}\\n"".format(""Significance curvature"", d[""LP_SigCurv""])\n\n        elif spec_type == ""PLSuperExpCutoff"":\n            tag = ""PLEC""\n            fmt = ""{:<45s} : {} +- {}\\n""\n            ss += fmt.format(\n                ""Exponential factor"", d[""PLEC_Expfactor""], d[""Unc_PLEC_Expfactor""]\n            )\n            ss += ""{:<45s} : {} +- {}\\n"".format(\n                ""Super-exponential cutoff index"",\n                d[""PLEC_Exp_Index""],\n                d[""Unc_PLEC_Exp_Index""],\n            )\n            ss += ""{:<45s} : {:.1f}\\n"".format(\n                ""Significance curvature"", d[""PLEC_SigCurv""]\n            )\n\n        else:\n            raise ValueError(f""Invalid spec_type: {spec_type!r}"")\n\n        ss += ""{:<45s} : {:.0f} {}\\n"".format(\n            ""Pivot energy"", d[""Pivot_Energy""].value, d[""Pivot_Energy""].unit\n        )\n\n        fmt = ""{:<45s} : {:.3f} +- {:.3f}\\n""\n        ss += fmt.format(\n            ""Spectral index"", d[tag + ""_Index""], d[""Unc_"" + tag + ""_Index""]\n        )\n\n        fmt = ""{:<45s} : {:.3} +- {:.3} {}\\n""\n        ss += fmt.format(\n            ""Flux Density at pivot energy"",\n            d[tag + ""_Flux_Density""].value,\n            d[""Unc_"" + tag + ""_Flux_Density""].value,\n            ""cm-2 MeV-1 s-1"",\n        )\n\n        fmt = ""{:<45s} : {:.3} +- {:.3} {}\\n""\n        ss += fmt.format(\n            ""Integral flux (1 - 100 GeV)"",\n            d[""Flux1000""].value,\n            d[""Unc_Flux1000""].value,\n            ""cm-2 s-1"",\n        )\n\n        fmt = ""{:<45s} : {:.3} +- {:.3} {}\\n""\n        ss += fmt.format(\n            ""Energy flux (100 MeV - 100 GeV)"",\n            d[""Energy_Flux100""].value,\n            d[""Unc_Energy_Flux100""].value,\n            ""erg cm-2 s-1"",\n        )\n\n        return ss\n\n    def _info_lightcurve(self):\n        d = self.data\n        ss = ""\\n*** Lightcurve info ***\\n\\n""\n        ss += ""Lightcurve measured in the energy band: 100 MeV - 100 GeV\\n\\n""\n\n        ss += ""{:<15s} : {:.3f}\\n"".format(""Variability index"", d[""Variability_Index""])\n\n        if np.isfinite(d[""Flux_Peak""]):\n            ss += ""{:<40s} : {:.3f}\\n"".format(\n                ""Significance peak (100 MeV - 100 GeV)"", d[""Signif_Peak""]\n            )\n\n            fmt = ""{:<40s} : {:.3} +- {:.3} cm^-2 s^-1\\n""\n            ss += fmt.format(\n                ""Integral flux peak (100 MeV - 100 GeV)"",\n                d[""Flux_Peak""].value,\n                d[""Unc_Flux_Peak""].value,\n            )\n\n            # TODO: give time as UTC string, not MET\n            ss += ""{:<40s} : {:.3} s (Mission elapsed time)\\n"".format(\n                ""Time peak"", d[""Time_Peak""].value\n            )\n            peak_interval = d[""Peak_Interval""].to_value(""day"")\n            ss += ""{:<40s} : {:.3} day\\n"".format(""Peak interval"", peak_interval)\n        else:\n            ss += ""\\nNo peak measured for this source.\\n""\n\n        # TODO: Add a lightcurve table with d[\'Flux_History\'] and d[\'Unc_Flux_History\']\n\n        return ss\n\n    def spatial_model(self):\n        """"""Spatial model (`~gammapy.modeling.models.SpatialModel`).""""""\n        d = self.data\n        ra = d[""RAJ2000""]\n        dec = d[""DEJ2000""]\n\n        if self.is_pointlike:\n            model = PointSpatialModel(lon_0=ra, lat_0=dec, frame=""icrs"")\n        else:\n            de = self.data_extended\n            morph_type = de[""Model_Form""].strip()\n            e = (1 - (de[""Model_SemiMinor""] / de[""Model_SemiMajor""]) ** 2.0) ** 0.5\n            sigma = de[""Model_SemiMajor""]\n            phi = de[""Model_PosAng""]\n            if morph_type == ""Disk"":\n                r_0 = de[""Model_SemiMajor""]\n                model = DiskSpatialModel(\n                    lon_0=ra, lat_0=dec, r_0=r_0, e=e, phi=phi, frame=""icrs""\n                )\n            elif morph_type in [""Map"", ""Ring"", ""2D Gaussian x2""]:\n                filename = de[""Spatial_Filename""].strip()\n                path = make_path(\n                    ""$GAMMAPY_DATA/catalogs/fermi/LAT_extended_sources_8years/Templates/""\n                )\n                with warnings.catch_warnings():  # ignore FITS units warnings\n                    warnings.simplefilter(""ignore"", FITSFixedWarning)\n                    model = TemplateSpatialModel.read(path / filename)\n            elif morph_type == ""2D Gaussian"":\n                model = GaussianSpatialModel(\n                    lon_0=ra, lat_0=dec, sigma=sigma, e=e, phi=phi, frame=""icrs""\n                )\n            else:\n                raise ValueError(f""Invalid spatial model: {morph_type!r}"")\n        self._set_spatial_errors(model)\n        return model\n\n    def spectral_model(self):\n        """"""Best fit spectral model (`~gammapy.modeling.models.SpectralModel`).""""""\n        spec_type = self.data[""SpectrumType""].strip()\n\n        if spec_type == ""PowerLaw"":\n            tag = ""PowerLawSpectralModel""\n            pars = {\n                ""reference"": self.data[""Pivot_Energy""],\n                ""amplitude"": self.data[""PL_Flux_Density""],\n                ""index"": self.data[""PL_Index""],\n            }\n            errs = {\n                ""amplitude"": self.data[""Unc_PL_Flux_Density""],\n                ""index"": self.data[""Unc_PL_Index""],\n            }\n        elif spec_type == ""LogParabola"":\n            tag = ""LogParabolaSpectralModel""\n            pars = {\n                ""reference"": self.data[""Pivot_Energy""],\n                ""amplitude"": self.data[""LP_Flux_Density""],\n                ""alpha"": self.data[""LP_Index""],\n                ""beta"": self.data[""LP_beta""],\n            }\n            errs = {\n                ""amplitude"": self.data[""Unc_LP_Flux_Density""],\n                ""alpha"": self.data[""Unc_LP_Index""],\n                ""beta"": self.data[""Unc_LP_beta""],\n            }\n        elif spec_type == ""PLSuperExpCutoff"":\n            tag = ""SuperExpCutoffPowerLaw4FGLSpectralModel""\n            pars = {\n                ""reference"": self.data[""Pivot_Energy""],\n                ""amplitude"": self.data[""PLEC_Flux_Density""],\n                ""index_1"": self.data[""PLEC_Index""],\n                ""index_2"": self.data[""PLEC_Exp_Index""],\n                ""expfactor"": self.data[""PLEC_Expfactor""],\n            }\n            errs = {\n                ""amplitude"": self.data[""Unc_PLEC_Flux_Density""],\n                ""index_1"": self.data[""Unc_PLEC_Index""],\n                ""index_2"": np.nan_to_num(self.data[""Unc_PLEC_Exp_Index""]),\n                ""expfactor"": self.data[""Unc_PLEC_Expfactor""],\n            }\n        else:\n            raise ValueError(f""Invalid spec_type: {spec_type!r}"")\n\n        model = Model.create(tag, **pars)\n\n        for name, value in errs.items():\n            model.parameters[name].error = value\n\n        return model\n\n    @property\n    def flux_points(self):\n        """"""Flux points (`~gammapy.spectrum.FluxPoints`).""""""\n        table = Table()\n        table.meta[""SED_TYPE""] = ""flux""\n\n        table[""e_min""] = self._ebounds[:-1]\n        table[""e_max""] = self._ebounds[1:]\n\n        flux = self._get_flux_values(""Flux_Band"")\n        flux_err = self._get_flux_values(""Unc_Flux_Band"")\n        table[""flux""] = flux\n        table[""flux_errn""] = np.abs(flux_err[:, 0])\n        table[""flux_errp""] = flux_err[:, 1]\n\n        nuFnu = self._get_flux_values(""nuFnu_Band"", ""erg cm-2 s-1"")\n        table[""e2dnde""] = nuFnu\n        table[""e2dnde_errn""] = np.abs(nuFnu * flux_err[:, 0] / flux)\n        table[""e2dnde_errp""] = nuFnu * flux_err[:, 1] / flux\n\n        is_ul = np.isnan(table[""flux_errn""])\n        table[""is_ul""] = is_ul\n\n        # handle upper limits\n        table[""flux_ul""] = np.nan * flux_err.unit\n        flux_ul = compute_flux_points_ul(table[""flux""], table[""flux_errp""])\n        table[""flux_ul""][is_ul] = flux_ul[is_ul]\n\n        # handle upper limits\n        table[""e2dnde_ul""] = np.nan * nuFnu.unit\n        e2dnde_ul = compute_flux_points_ul(table[""e2dnde""], table[""e2dnde_errp""])\n        table[""e2dnde_ul""][is_ul] = e2dnde_ul[is_ul]\n\n        # Square root of test statistic\n        table[""sqrt_TS""] = self.data[""Sqrt_TS_Band""]\n        return FluxPoints(table)\n\n    def _get_flux_values(self, prefix, unit=""cm-2 s-1""):\n        values = self.data[prefix]\n        return u.Quantity(values, unit)\n\n    @property\n    def lightcurve(self):\n        """"""Lightcurve (`~gammapy.time.LightCurve`).""""""\n        flux = self.data[""Flux_History""]\n\n        # Flux error is given as asymmetric high/low\n        flux_errn = -self.data[""Unc_Flux_History""][:, 0]\n        flux_errp = self.data[""Unc_Flux_History""][:, 1]\n\n        # Really the time binning is stored in a separate HDU in the FITS\n        # catalog file called `Hist_Start`, with a single column `Hist_Start`\n        # giving the time binning in MET (mission elapsed time)\n        # This is not available here for now.\n        # TODO: read that info in `SourceCatalog3FGL` and pass it down to the\n        # `SourceCatalogObject3FGL` object somehow.\n\n        # For now, we just hard-code the start and stop time and assume\n        # equally-spaced time intervals. This is roughly correct,\n        # for plotting the difference doesn\'t matter, only for analysis\n        time_start = Time(""2008-08-04T15:43:36.0000"")\n        time_end = Time(""2016-08-02T05:44:11.9999"")\n        n_points = len(flux)\n        time_step = (time_end - time_start) / n_points\n        time_bounds = time_start + np.arange(n_points + 1) * time_step\n\n        table = Table(\n            [\n                Column(time_bounds[:-1].utc.mjd, ""time_min""),\n                Column(time_bounds[1:].utc.mjd, ""time_max""),\n                Column(flux, ""flux""),\n                Column(flux_errp, ""flux_errp""),\n                Column(flux_errn, ""flux_errn""),\n            ]\n        )\n        return LightCurve(table)\n\n\nclass SourceCatalogObject3FGL(SourceCatalogObjectFermiBase):\n    """"""One source from the Fermi-LAT 3FGL catalog.\n\n    Catalog is represented by `~gammapy.catalog.SourceCatalog3FGL`.\n    """"""\n\n    _ebounds = u.Quantity([100, 300, 1000, 3000, 10000, 100000], ""MeV"")\n    _ebounds_suffix = [""100_300"", ""300_1000"", ""1000_3000"", ""3000_10000"", ""10000_100000""]\n    energy_range = u.Quantity([100, 100000], ""MeV"")\n    """"""Energy range used for the catalog.\n\n    Paper says that analysis uses data up to 300 GeV,\n    but results are all quoted up to 100 GeV only to\n    be consistent with previous catalogs.\n    """"""\n\n    def _info_more(self):\n        d = self.data\n        ss = ""\\n*** Other info ***\\n\\n""\n        ss += ""{:<20s} : {}\\n"".format(""Other flags"", d[""Flags""])\n        return ss\n\n    def _info_spectral_fit(self):\n        d = self.data\n        spec_type = d[""SpectrumType""].strip()\n\n        ss = ""\\n*** Spectral info ***\\n\\n""\n\n        ss += ""{:<45s} : {}\\n"".format(""Spectrum type"", d[""SpectrumType""])\n        fmt = ""{:<45s} : {:.3f}\\n""\n        ss += fmt.format(""Detection significance (100 MeV - 300 GeV)"", d[""Signif_Avg""])\n        ss += ""{:<45s} : {:.1f}\\n"".format(""Significance curvature"", d[""Signif_Curve""])\n\n        if spec_type == ""PowerLaw"":\n            pass\n        elif spec_type == ""LogParabola"":\n            ss += ""{:<45s} : {} +- {}\\n"".format(""beta"", d[""beta""], d[""Unc_beta""])\n        elif spec_type in [""PLExpCutoff"", ""PlSuperExpCutoff""]:\n            fmt = ""{:<45s} : {:.0f} +- {:.0f} {}\\n""\n            ss += fmt.format(\n                ""Cutoff energy"",\n                d[""Cutoff""].value,\n                d[""Unc_Cutoff""].value,\n                d[""Cutoff""].unit,\n            )\n        elif spec_type == ""PLSuperExpCutoff"":\n            ss += ""{:<45s} : {} +- {}\\n"".format(\n                ""Super-exponential cutoff index"", d[""Exp_Index""], d[""Unc_Exp_Index""]\n            )\n        else:\n            raise ValueError(f""Invalid spec_type: {spec_type!r}"")\n\n        ss += ""{:<45s} : {:.0f} {}\\n"".format(\n            ""Pivot energy"", d[""Pivot_Energy""].value, d[""Pivot_Energy""].unit\n        )\n\n        ss += ""{:<45s} : {:.3f}\\n"".format(\n            ""Power law spectral index"", d[""PowerLaw_Index""]\n        )\n\n        fmt = ""{:<45s} : {:.3f} +- {:.3f}\\n""\n        ss += fmt.format(""Spectral index"", d[""Spectral_Index""], d[""Unc_Spectral_Index""])\n\n        fmt = ""{:<45s} : {:.3} +- {:.3} {}\\n""\n        ss += fmt.format(\n            ""Flux Density at pivot energy"",\n            d[""Flux_Density""].value,\n            d[""Unc_Flux_Density""].value,\n            ""cm-2 MeV-1 s-1"",\n        )\n\n        fmt = ""{:<45s} : {:.3} +- {:.3} {}\\n""\n        ss += fmt.format(\n            ""Integral flux (1 - 100 GeV)"",\n            d[""Flux1000""].value,\n            d[""Unc_Flux1000""].value,\n            ""cm-2 s-1"",\n        )\n\n        fmt = ""{:<45s} : {:.3} +- {:.3} {}\\n""\n        ss += fmt.format(\n            ""Energy flux (100 MeV - 100 GeV)"",\n            d[""Energy_Flux100""].value,\n            d[""Unc_Energy_Flux100""].value,\n            ""erg cm-2 s-1"",\n        )\n\n        return ss\n\n    def _info_lightcurve(self):\n        d = self.data\n        ss = ""\\n*** Lightcurve info ***\\n\\n""\n        ss += ""Lightcurve measured in the energy band: 100 MeV - 100 GeV\\n\\n""\n\n        ss += ""{:<15s} : {:.3f}\\n"".format(""Variability index"", d[""Variability_Index""])\n\n        if np.isfinite(d[""Flux_Peak""]):\n            ss += ""{:<40s} : {:.3f}\\n"".format(\n                ""Significance peak (100 MeV - 100 GeV)"", d[""Signif_Peak""]\n            )\n\n            fmt = ""{:<40s} : {:.3} +- {:.3} cm^-2 s^-1\\n""\n            ss += fmt.format(\n                ""Integral flux peak (100 MeV - 100 GeV)"",\n                d[""Flux_Peak""].value,\n                d[""Unc_Flux_Peak""].value,\n            )\n\n            # TODO: give time as UTC string, not MET\n            ss += ""{:<40s} : {:.3} s (Mission elapsed time)\\n"".format(\n                ""Time peak"", d[""Time_Peak""].value\n            )\n            peak_interval = d[""Peak_Interval""].to_value(""day"")\n            ss += ""{:<40s} : {:.3} day\\n"".format(""Peak interval"", peak_interval)\n        else:\n            ss += ""\\nNo peak measured for this source.\\n""\n\n        # TODO: Add a lightcurve table with d[\'Flux_History\'] and d[\'Unc_Flux_History\']\n\n        return ss\n\n    def spectral_model(self):\n        """"""Best fit spectral model (`~gammapy.modeling.models.SpectralModel`).""""""\n        spec_type = self.data[""SpectrumType""].strip()\n\n        if spec_type == ""PowerLaw"":\n            tag = ""PowerLawSpectralModel""\n            pars = {\n                ""amplitude"": self.data[""Flux_Density""],\n                ""reference"": self.data[""Pivot_Energy""],\n                ""index"": self.data[""Spectral_Index""],\n            }\n            errs = {\n                ""amplitude"": self.data[""Unc_Flux_Density""],\n                ""index"": self.data[""Unc_Spectral_Index""],\n            }\n        elif spec_type == ""PLExpCutoff"":\n            tag = ""ExpCutoffPowerLaw3FGLSpectralModel""\n            pars = {\n                ""amplitude"": self.data[""Flux_Density""],\n                ""reference"": self.data[""Pivot_Energy""],\n                ""index"": self.data[""Spectral_Index""],\n                ""ecut"": self.data[""Cutoff""],\n            }\n            errs = {\n                ""amplitude"": self.data[""Unc_Flux_Density""],\n                ""index"": self.data[""Unc_Spectral_Index""],\n                ""ecut"": self.data[""Unc_Cutoff""],\n            }\n        elif spec_type == ""LogParabola"":\n            tag = ""LogParabolaSpectralModel""\n            pars = {\n                ""amplitude"": self.data[""Flux_Density""],\n                ""reference"": self.data[""Pivot_Energy""],\n                ""alpha"": self.data[""Spectral_Index""],\n                ""beta"": self.data[""beta""],\n            }\n            errs = {\n                ""amplitude"": self.data[""Unc_Flux_Density""],\n                ""alpha"": self.data[""Unc_Spectral_Index""],\n                ""beta"": self.data[""Unc_beta""],\n            }\n        elif spec_type == ""PLSuperExpCutoff"":\n            tag = ""SuperExpCutoffPowerLaw3FGLSpectralModel""\n            pars = {\n                ""amplitude"": self.data[""Flux_Density""],\n                ""reference"": self.data[""Pivot_Energy""],\n                ""index_1"": self.data[""Spectral_Index""],\n                ""index_2"": self.data[""Exp_Index""],\n                ""ecut"": self.data[""Cutoff""],\n            }\n            errs = {\n                ""amplitude"": self.data[""Unc_Flux_Density""],\n                ""index_1"": self.data[""Unc_Spectral_Index""],\n                ""index_2"": self.data[""Unc_Exp_Index""],\n                ""ecut"": self.data[""Unc_Cutoff""],\n            }\n        else:\n            raise ValueError(f""Invalid spec_type: {spec_type!r}"")\n\n        model = Model.create(tag, **pars)\n\n        for name, value in errs.items():\n            model.parameters[name].error = value\n\n        return model\n\n    def spatial_model(self):\n        """"""Spatial model (`~gammapy.modeling.models.SpatialModel`).""""""\n        d = self.data\n        ra = d[""RAJ2000""]\n        dec = d[""DEJ2000""]\n\n        if self.is_pointlike:\n            model = PointSpatialModel(lon_0=ra, lat_0=dec, frame=""icrs"")\n        else:\n            de = self.data_extended\n            morph_type = de[""Model_Form""].strip()\n            e = (1 - (de[""Model_SemiMinor""] / de[""Model_SemiMajor""]) ** 2.0) ** 0.5\n            sigma = de[""Model_SemiMajor""]\n            phi = de[""Model_PosAng""]\n            if morph_type == ""Disk"":\n                r_0 = de[""Model_SemiMajor""]\n                model = DiskSpatialModel(\n                    lon_0=ra, lat_0=dec, r_0=r_0, e=e, phi=phi, frame=""icrs""\n                )\n            elif morph_type in [""Map"", ""Ring"", ""2D Gaussian x2""]:\n                filename = de[""Spatial_Filename""].strip()\n                path = make_path(\n                    ""$GAMMAPY_DATA/catalogs/fermi/Extended_archive_v15/Templates/""\n                )\n                model = TemplateSpatialModel.read(path / filename)\n            elif morph_type == ""2D Gaussian"":\n                model = GaussianSpatialModel(\n                    lon_0=ra, lat_0=dec, sigma=sigma, e=e, phi=phi, frame=""icrs""\n                )\n            else:\n                raise ValueError(f""Invalid spatial model: {morph_type!r}"")\n        self._set_spatial_errors(model)\n        return model\n\n    @property\n    def flux_points(self):\n        """"""Flux points (`~gammapy.spectrum.FluxPoints`).""""""\n        table = Table()\n        table.meta[""SED_TYPE""] = ""flux""\n\n        table[""e_min""] = self._ebounds[:-1]\n        table[""e_max""] = self._ebounds[1:]\n\n        flux = self._get_flux_values(""Flux"")\n        flux_err = self._get_flux_values(""Unc_Flux"")\n        table[""flux""] = flux\n        table[""flux_errn""] = np.abs(flux_err[:, 0])\n        table[""flux_errp""] = flux_err[:, 1]\n\n        nuFnu = self._get_flux_values(""nuFnu"", ""erg cm-2 s-1"")\n        table[""e2dnde""] = nuFnu\n        table[""e2dnde_errn""] = np.abs(nuFnu * flux_err[:, 0] / flux)\n        table[""e2dnde_errp""] = nuFnu * flux_err[:, 1] / flux\n\n        is_ul = np.isnan(table[""flux_errn""])\n        table[""is_ul""] = is_ul\n\n        # handle upper limits\n        table[""flux_ul""] = np.nan * flux_err.unit\n        flux_ul = compute_flux_points_ul(table[""flux""], table[""flux_errp""])\n        table[""flux_ul""][is_ul] = flux_ul[is_ul]\n\n        # handle upper limits\n        table[""e2dnde_ul""] = np.nan * nuFnu.unit\n        e2dnde_ul = compute_flux_points_ul(table[""e2dnde""], table[""e2dnde_errp""])\n        table[""e2dnde_ul""][is_ul] = e2dnde_ul[is_ul]\n\n        # Square root of test statistic\n        table[""sqrt_TS""] = [self.data[""Sqrt_TS"" + _] for _ in self._ebounds_suffix]\n        return FluxPoints(table)\n\n    def _get_flux_values(self, prefix, unit=""cm-2 s-1""):\n        values = [self.data[prefix + _] for _ in self._ebounds_suffix]\n        return u.Quantity(values, unit)\n\n    @property\n    def lightcurve(self):\n        """"""Lightcurve (`~gammapy.time.LightCurve`).""""""\n        flux = self.data[""Flux_History""]\n\n        # Flux error is given as asymmetric high/low\n        flux_errn = -self.data[""Unc_Flux_History""][:, 0]\n        flux_errp = self.data[""Unc_Flux_History""][:, 1]\n\n        # Really the time binning is stored in a separate HDU in the FITS\n        # catalog file called `Hist_Start`, with a single column `Hist_Start`\n        # giving the time binning in MET (mission elapsed time)\n        # This is not available here for now.\n        # TODO: read that info in `SourceCatalog3FGL` and pass it down to the\n        # `SourceCatalogObject3FGL` object somehow.\n\n        # For now, we just hard-code the start and stop time and assume\n        # equally-spaced time intervals. This is roughly correct,\n        # for plotting the difference doesn\'t matter, only for analysis\n        time_start = Time(""2008-08-02T00:33:19"")\n        time_end = Time(""2012-07-31T22:45:47"")\n        n_points = len(flux)\n        time_step = (time_end - time_start) / n_points\n        time_bounds = time_start + np.arange(n_points + 1) * time_step\n\n        table = Table(\n            [\n                Column(time_bounds[:-1].utc.mjd, ""time_min""),\n                Column(time_bounds[1:].utc.mjd, ""time_max""),\n                Column(flux, ""flux""),\n                Column(flux_errp, ""flux_errp""),\n                Column(flux_errn, ""flux_errn""),\n            ]\n        )\n        return LightCurve(table)\n\n\nclass SourceCatalogObject2FHL(SourceCatalogObjectFermiBase):\n    """"""One source from the Fermi-LAT 2FHL catalog.\n\n    Catalog is represented by `~gammapy.catalog.SourceCatalog2FHL`.\n    """"""\n\n    asso = [""ASSOC"", ""3FGL_Name"", ""1FHL_Name"", ""TeVCat_Name""]\n    _ebounds = u.Quantity([50, 171, 585, 2000], ""GeV"")\n    _ebounds_suffix = [""50_171"", ""171_585"", ""585_2000""]\n    energy_range = u.Quantity([0.05, 2], ""TeV"")\n    """"""Energy range used for the catalog.""""""\n\n    def _info_more(self):\n        d = self.data\n        ss = ""\\n*** Other info ***\\n\\n""\n        fmt = ""{:<32s} : {:.3f}\\n""\n        ss += fmt.format(""Test statistic (50 GeV - 2 TeV)"", d[""TS""])\n        return ss\n\n    def _info_position(self):\n        d = self.data\n        ss = ""\\n*** Position info ***\\n\\n""\n        ss += ""{:<20s} : {:.3f}\\n"".format(""RA"", d[""RAJ2000""])\n        ss += ""{:<20s} : {:.3f}\\n"".format(""DEC"", d[""DEJ2000""])\n        ss += ""{:<20s} : {:.3f}\\n"".format(""GLON"", d[""GLON""])\n        ss += ""{:<20s} : {:.3f}\\n"".format(""GLAT"", d[""GLAT""])\n\n        ss += ""\\n""\n        ss += ""{:<20s} : {:.4f}\\n"".format(""Error on position (68%)"", d[""Pos_err_68""])\n        ss += ""{:<20s} : {:.0f}\\n"".format(""ROI number"", d[""ROI""])\n        return ss\n\n    def _info_spectral_fit(self):\n        d = self.data\n\n        ss = ""\\n*** Spectral fit info ***\\n\\n""\n\n        fmt = ""{:<32s} : {:.3f} +- {:.3f}\\n""\n        ss += fmt.format(\n            ""Power-law spectral index"", d[""Spectral_Index""], d[""Unc_Spectral_Index""]\n        )\n\n        ss += ""{:<32s} : {:.3} +- {:.3} {}\\n"".format(\n            ""Integral flux (50 GeV - 2 TeV)"",\n            d[""Flux50""].value,\n            d[""Unc_Flux50""].value,\n            ""cm-2 s-1"",\n        )\n\n        ss += ""{:<32s} : {:.3} +- {:.3} {}\\n"".format(\n            ""Energy flux (50 GeV - 2 TeV)"",\n            d[""Energy_Flux50""].value,\n            d[""Unc_Energy_Flux50""].value,\n            ""erg cm-2 s-1"",\n        )\n\n        return ss\n\n    @property\n    def is_pointlike(self):\n        return self.data[""Source_Name""].strip()[-1] != ""e""\n\n    def spatial_model(self):\n        """"""Spatial model (`~gammapy.modeling.models.SpatialModel`).""""""\n        d = self.data\n        ra = d[""RAJ2000""]\n        dec = d[""DEJ2000""]\n\n        if self.is_pointlike:\n            model = PointSpatialModel(lon_0=ra, lat_0=dec, frame=""icrs"")\n        else:\n            de = self.data_extended\n            morph_type = de[""Model_Form""].strip()\n            e = (1 - (de[""Model_SemiMinor""] / de[""Model_SemiMajor""]) ** 2.0) ** 0.5\n            sigma = de[""Model_SemiMajor""]\n            phi = de[""Model_PosAng""]\n            if morph_type in [""Disk"", ""Elliptical Disk""]:\n                r_0 = de[""Model_SemiMajor""]\n                model = DiskSpatialModel(\n                    lon_0=ra, lat_0=dec, r_0=r_0, e=e, phi=phi, frame=""icrs""\n                )\n            elif morph_type in [""Map"", ""Ring"", ""2D Gaussian x2""]:\n                filename = de[""Spatial_Filename""].strip()\n                path = make_path(\n                    ""$GAMMAPY_DATA/catalogs/fermi/Extended_archive_v15/Templates/""\n                )\n                return TemplateSpatialModel.read(path / filename)\n            elif morph_type in [""2D Gaussian"", ""Elliptical 2D Gaussian""]:\n                model = GaussianSpatialModel(\n                    lon_0=ra, lat_0=dec, sigma=sigma, e=e, phi=phi, frame=""icrs""\n                )\n            else:\n                raise ValueError(f""Invalid spatial model: {morph_type!r}"")\n\n        self._set_spatial_errors(model)\n        return model\n\n    def spectral_model(self):\n        """"""Best fit spectral model (`~gammapy.modeling.models.SpectralModel`).""""""\n        tag = ""PowerLaw2SpectralModel""\n        pars = {\n            ""amplitude"": self.data[""Flux50""],\n            ""emin"": self.energy_range[0],\n            ""emax"": self.energy_range[1],\n            ""index"": self.data[""Spectral_Index""],\n        }\n        errs = {\n            ""amplitude"": self.data[""Unc_Flux50""],\n            ""index"": self.data[""Unc_Spectral_Index""],\n        }\n\n        model = Model.create(tag, **pars)\n\n        for name, value in errs.items():\n            model.parameters[name].error = value\n\n        return model\n\n    @property\n    def flux_points(self):\n        """"""Integral flux points (`~gammapy.spectrum.FluxPoints`).""""""\n        table = Table()\n        table.meta[""SED_TYPE""] = ""flux""\n        table[""e_min""] = self._ebounds[:-1]\n        table[""e_max""] = self._ebounds[1:]\n        table[""flux""] = self._get_flux_values(""Flux"")\n        flux_err = self._get_flux_values(""Unc_Flux"")\n        table[""flux_errn""] = np.abs(flux_err[:, 0])\n        table[""flux_errp""] = flux_err[:, 1]\n\n        # handle upper limits\n        is_ul = np.isnan(table[""flux_errn""])\n        table[""is_ul""] = is_ul\n        table[""flux_ul""] = np.nan * flux_err.unit\n        flux_ul = compute_flux_points_ul(table[""flux""], table[""flux_errp""])\n        table[""flux_ul""][is_ul] = flux_ul[is_ul]\n        return FluxPoints(table)\n\n    def _get_flux_values(self, prefix, unit=""cm-2 s-1""):\n        values = [self.data[prefix + _ + ""GeV""] for _ in self._ebounds_suffix]\n        return u.Quantity(values, unit)\n\n\nclass SourceCatalogObject3FHL(SourceCatalogObjectFermiBase):\n    """"""One source from the Fermi-LAT 3FHL catalog.\n\n    Catalog is represented by `~gammapy.catalog.SourceCatalog3FHL`.\n    """"""\n\n    asso = [""ASSOC1"", ""ASSOC2"", ""ASSOC_TEV"", ""ASSOC_GAM""]\n    energy_range = u.Quantity([0.01, 2], ""TeV"")\n    """"""Energy range used for the catalog.""""""\n\n    _ebounds = u.Quantity([10, 20, 50, 150, 500, 2000], ""GeV"")\n\n    def _info_position(self):\n        d = self.data\n        ss = ""\\n*** Position info ***\\n\\n""\n        ss += ""{:<20s} : {:.3f}\\n"".format(""RA"", d[""RAJ2000""])\n        ss += ""{:<20s} : {:.3f}\\n"".format(""DEC"", d[""DEJ2000""])\n        ss += ""{:<20s} : {:.3f}\\n"".format(""GLON"", d[""GLON""])\n        ss += ""{:<20s} : {:.3f}\\n"".format(""GLAT"", d[""GLAT""])\n\n        # TODO: All sources are non-elliptical; just give one number for radius?\n        ss += ""\\n""\n        ss += ""{:<20s} : {:.4f}\\n"".format(""Semimajor (95%)"", d[""Conf_95_SemiMajor""])\n        ss += ""{:<20s} : {:.4f}\\n"".format(""Semiminor (95%)"", d[""Conf_95_SemiMinor""])\n        ss += ""{:<20s} : {:.2f}\\n"".format(""Position angle (95%)"", d[""Conf_95_PosAng""])\n        ss += ""{:<20s} : {:.0f}\\n"".format(""ROI number"", d[""ROI_num""])\n\n        return ss\n\n    def _info_spectral_fit(self):\n        d = self.data\n        spec_type = d[""SpectrumType""].strip()\n\n        ss = ""\\n*** Spectral fit info ***\\n\\n""\n\n        ss += ""{:<32s} : {}\\n"".format(""Spectrum type"", d[""SpectrumType""])\n        ss += ""{:<32s} : {:.1f}\\n"".format(""Significance curvature"", d[""Signif_Curve""])\n\n        # Power-law parameters are always given; give in any case\n        fmt = ""{:<32s} : {:.3f} +- {:.3f}\\n""\n        ss += fmt.format(\n            ""Power-law spectral index"", d[""PowerLaw_Index""], d[""Unc_PowerLaw_Index""]\n        )\n\n        if spec_type == ""PowerLaw"":\n            pass\n        elif spec_type == ""LogParabola"":\n            fmt = ""{:<32s} : {:.3f} +- {:.3f}\\n""\n            ss += fmt.format(\n                ""LogParabolaSpectralModel spectral index"",\n                d[""Spectral_Index""],\n                d[""Unc_Spectral_Index""],\n            )\n\n            ss += ""{:<32s} : {:.3f} +- {:.3f}\\n"".format(\n                ""LogParabolaSpectralModel beta"", d[""beta""], d[""Unc_beta""]\n            )\n        else:\n            raise ValueError(f""Invalid spec_type: {spec_type!r}"")\n\n        ss += ""{:<32s} : {:.1f} {}\\n"".format(\n            ""Pivot energy"", d[""Pivot_Energy""].value, d[""Pivot_Energy""].unit\n        )\n\n        ss += ""{:<32s} : {:.3} +- {:.3} {}\\n"".format(\n            ""Flux Density at pivot energy"",\n            d[""Flux_Density""].value,\n            d[""Unc_Flux_Density""].value,\n            ""cm-2 GeV-1 s-1"",\n        )\n\n        ss += ""{:<32s} : {:.3} +- {:.3} {}\\n"".format(\n            ""Integral flux (10 GeV - 1 TeV)"",\n            d[""Flux""].value,\n            d[""Unc_Flux""].value,\n            ""cm-2 s-1"",\n        )\n\n        ss += ""{:<32s} : {:.3} +- {:.3} {}\\n"".format(\n            ""Energy flux (10 GeV - TeV)"",\n            d[""Energy_Flux""].value,\n            d[""Unc_Energy_Flux""].value,\n            ""erg cm-2 s-1"",\n        )\n\n        return ss\n\n    def _info_more(self):\n        d = self.data\n        ss = ""\\n*** Other info ***\\n\\n""\n\n        fmt = ""{:<32s} : {:.3f}\\n""\n        ss += fmt.format(""Significance (10 GeV - 2 TeV)"", d[""Signif_Avg""])\n        ss += ""{:<32s} : {:.1f}\\n"".format(""Npred"", d[""Npred""])\n\n        ss += ""\\n{:<16s} : {:.3f} {}\\n"".format(\n            ""HEP Energy"", d[""HEP_Energy""].value, d[""HEP_Energy""].unit\n        )\n        ss += ""{:<16s} : {:.3f}\\n"".format(""HEP Probability"", d[""HEP_Prob""])\n\n        ss += ""{:<16s} : {}\\n"".format(""Bayesian Blocks"", d[""Variability_BayesBlocks""])\n\n        ss += ""{:<16s} : {:.3f}\\n"".format(""Redshift"", d[""Redshift""])\n        ss += ""{:<16s} : {:.3} {}\\n"".format(\n            ""NuPeak_obs"", d[""NuPeak_obs""].value, d[""NuPeak_obs""].unit\n        )\n\n        return ss\n\n    def spectral_model(self):\n        """"""Best fit spectral model (`~gammapy.modeling.models.SpectralModel`).""""""\n        d = self.data\n        spec_type = self.data[""SpectrumType""].strip()\n\n        if spec_type == ""PowerLaw"":\n            tag = ""PowerLawSpectralModel""\n            pars = {\n                ""reference"": d[""Pivot_Energy""],\n                ""amplitude"": d[""Flux_Density""],\n                ""index"": d[""PowerLaw_Index""],\n            }\n            errs = {\n                ""amplitude"": d[""Unc_Flux_Density""],\n                ""index"": d[""Unc_PowerLaw_Index""],\n            }\n        elif spec_type == ""LogParabola"":\n            tag = ""LogParabolaSpectralModel""\n            pars = {\n                ""reference"": d[""Pivot_Energy""],\n                ""amplitude"": d[""Flux_Density""],\n                ""alpha"": d[""Spectral_Index""],\n                ""beta"": d[""beta""],\n            }\n            errs = {\n                ""amplitude"": d[""Unc_Flux_Density""],\n                ""alpha"": d[""Unc_Spectral_Index""],\n                ""beta"": d[""Unc_beta""],\n            }\n        else:\n            raise ValueError(f""Invalid spec_type: {spec_type!r}"")\n\n        model = Model.create(tag, **pars)\n\n        for name, value in errs.items():\n            model.parameters[name].error = value\n\n        return model\n\n    @property\n    def flux_points(self):\n        """"""Flux points (`~gammapy.spectrum.FluxPoints`).""""""\n        table = Table()\n        table.meta[""SED_TYPE""] = ""flux""\n        table[""e_min""] = self._ebounds[:-1]\n        table[""e_max""] = self._ebounds[1:]\n\n        flux = self.data[""Flux_Band""]\n        flux_err = self.data[""Unc_Flux_Band""]\n        e2dnde = self.data[""nuFnu""]\n\n        table[""flux""] = flux\n        table[""flux_errn""] = np.abs(flux_err[:, 0])\n        table[""flux_errp""] = flux_err[:, 1]\n\n        table[""e2dnde""] = e2dnde\n        table[""e2dnde_errn""] = np.abs(e2dnde * flux_err[:, 0] / flux)\n        table[""e2dnde_errp""] = e2dnde * flux_err[:, 1] / flux\n\n        is_ul = np.isnan(table[""flux_errn""])\n        table[""is_ul""] = is_ul\n\n        # handle upper limits\n        table[""flux_ul""] = np.nan * flux_err.unit\n        flux_ul = compute_flux_points_ul(table[""flux""], table[""flux_errp""])\n        table[""flux_ul""][is_ul] = flux_ul[is_ul]\n\n        table[""e2dnde_ul""] = np.nan * e2dnde.unit\n        e2dnde_ul = compute_flux_points_ul(table[""e2dnde""], table[""e2dnde_errp""])\n        table[""e2dnde_ul""][is_ul] = e2dnde_ul[is_ul]\n\n        # Square root of test statistic\n        table[""sqrt_ts""] = self.data[""Sqrt_TS_Band""]\n        return FluxPoints(table)\n\n    def spatial_model(self):\n        """"""Source spatial model (`~gammapy.modeling.models.SpatialModel`).""""""\n        d = self.data\n        ra = d[""RAJ2000""]\n        dec = d[""DEJ2000""]\n\n        if self.is_pointlike:\n            model = PointSpatialModel(lon_0=ra, lat_0=dec, frame=""icrs"")\n        else:\n            de = self.data_extended\n            morph_type = de[""Spatial_Function""].strip()\n            e = (1 - (de[""Model_SemiMinor""] / de[""Model_SemiMajor""]) ** 2.0) ** 0.5\n            sigma = de[""Model_SemiMajor""]\n            phi = de[""Model_PosAng""]\n            if morph_type == ""RadialDisk"":\n                r_0 = de[""Model_SemiMajor""]\n                model = DiskSpatialModel(\n                    lon_0=ra, lat_0=dec, r_0=r_0, e=e, phi=phi, frame=""icrs""\n                )\n            elif morph_type in [""SpatialMap""]:\n                filename = de[""Spatial_Filename""].strip()\n                path = make_path(\n                    ""$GAMMAPY_DATA/catalogs/fermi/Extended_archive_v18/Templates/""\n                )\n                model = TemplateSpatialModel.read(path / filename)\n            elif morph_type == ""RadialGauss"":\n                model = GaussianSpatialModel(\n                    lon_0=ra, lat_0=dec, sigma=sigma, e=e, phi=phi, frame=""icrs""\n                )\n            else:\n                raise ValueError(f""Invalid morph_type: {morph_type!r}"")\n        self._set_spatial_errors(model)\n        return model\n\n\nclass SourceCatalog3FGL(SourceCatalog):\n    """"""Fermi-LAT 3FGL source catalog.\n\n    - https://ui.adsabs.harvard.edu/#abs/2015ApJS..218...23A\n    - https://fermi.gsfc.nasa.gov/ssc/data/access/lat/4yr_catalog/\n\n    One source is represented by `~gammapy.catalog.SourceCatalogObject3FGL`.\n    """"""\n\n    tag = ""3fgl""\n    description = ""LAT 4-year point source catalog""\n    source_object_class = SourceCatalogObject3FGL\n\n    def __init__(self, filename=""$GAMMAPY_DATA/catalogs/fermi/gll_psc_v16.fit.gz""):\n        filename = make_path(filename)\n\n        with warnings.catch_warnings():  # ignore FITS units warnings\n            warnings.simplefilter(""ignore"", u.UnitsWarning)\n            table = Table.read(filename, hdu=""LAT_Point_Source_Catalog"")\n\n        table_standardise_units_inplace(table)\n\n        source_name_key = ""Source_Name""\n        source_name_alias = (\n            ""Extended_Source_Name"",\n            ""0FGL_Name"",\n            ""1FGL_Name"",\n            ""2FGL_Name"",\n            ""1FHL_Name"",\n            ""ASSOC_TEV"",\n            ""ASSOC1"",\n            ""ASSOC2"",\n        )\n        super().__init__(\n            table=table,\n            source_name_key=source_name_key,\n            source_name_alias=source_name_alias,\n        )\n\n        self.extended_sources_table = Table.read(filename, hdu=""ExtendedSources"")\n\n\nclass SourceCatalog4FGL(SourceCatalog):\n    """"""Fermi-LAT 4FGL source catalog.\n\n    - https://arxiv.org/abs/1902.10045\n    - https://fermi.gsfc.nasa.gov/ssc/data/access/lat/8yr_catalog/\n\n    One source is represented by `~gammapy.catalog.SourceCatalogObject4FGL`.\n    """"""\n\n    tag = ""4fgl""\n    description = ""LAT 8-year point source catalog""\n    source_object_class = SourceCatalogObject4FGL\n\n    def __init__(self, filename=""$GAMMAPY_DATA/catalogs/fermi/gll_psc_v20.fit.gz""):\n        filename = make_path(filename)\n        table = Table.read(filename, hdu=""LAT_Point_Source_Catalog"")\n        table_standardise_units_inplace(table)\n\n        source_name_key = ""Source_Name""\n        source_name_alias = (\n            ""Extended_Source_Name"",\n            ""ASSOC_FGL"",\n            ""ASSOC_FHL"",\n            ""ASSOC_GAM1"",\n            ""ASSOC_GAM2"",\n            ""ASSOC_GAM3"",\n            ""ASSOC_TEV"",\n            ""ASSOC1"",\n            ""ASSOC2"",\n        )\n        super().__init__(\n            table=table,\n            source_name_key=source_name_key,\n            source_name_alias=source_name_alias,\n        )\n\n        self.extended_sources_table = Table.read(filename, hdu=""ExtendedSources"")\n\n\nclass SourceCatalog2FHL(SourceCatalog):\n    """"""Fermi-LAT 2FHL source catalog.\n\n    - https://ui.adsabs.harvard.edu/abs/2016ApJS..222....5A\n    - https://fermi.gsfc.nasa.gov/ssc/data/access/lat/2FHL/\n\n    One source is represented by `~gammapy.catalog.SourceCatalogObject2FHL`.\n    """"""\n\n    tag = ""2fhl""\n    description = ""LAT second high-energy source catalog""\n    source_object_class = SourceCatalogObject2FHL\n\n    def __init__(self, filename=""$GAMMAPY_DATA/catalogs/fermi/gll_psch_v09.fit.gz""):\n        filename = make_path(filename)\n\n        with warnings.catch_warnings():  # ignore FITS units warnings\n            warnings.simplefilter(""ignore"", u.UnitsWarning)\n            table = Table.read(filename, hdu=""2FHL Source Catalog"")\n\n        table_standardise_units_inplace(table)\n\n        source_name_key = ""Source_Name""\n        source_name_alias = (""ASSOC"", ""3FGL_Name"", ""1FHL_Name"", ""TeVCat_Name"")\n        super().__init__(\n            table=table,\n            source_name_key=source_name_key,\n            source_name_alias=source_name_alias,\n        )\n\n        self.extended_sources_table = Table.read(filename, hdu=""Extended Sources"")\n        self.rois = Table.read(filename, hdu=""ROIs"")\n\n\nclass SourceCatalog3FHL(SourceCatalog):\n    """"""Fermi-LAT 3FHL source catalog.\n\n    - https://ui.adsabs.harvard.edu/abs/2017ApJS..232...18A\n    - https://fermi.gsfc.nasa.gov/ssc/data/access/lat/3FHL/\n\n    One source is represented by `~gammapy.catalog.SourceCatalogObject3FHL`.\n    """"""\n\n    tag = ""3fhl""\n    description = ""LAT third high-energy source catalog""\n    source_object_class = SourceCatalogObject3FHL\n\n    def __init__(self, filename=""$GAMMAPY_DATA/catalogs/fermi/gll_psch_v13.fit.gz""):\n        filename = make_path(filename)\n\n        with warnings.catch_warnings():  # ignore FITS units warnings\n            warnings.simplefilter(""ignore"", u.UnitsWarning)\n            table = Table.read(filename, hdu=""LAT_Point_Source_Catalog"")\n\n        table_standardise_units_inplace(table)\n\n        source_name_key = ""Source_Name""\n        source_name_alias = (""ASSOC1"", ""ASSOC2"", ""ASSOC_TEV"", ""ASSOC_GAM"")\n        super().__init__(\n            table=table,\n            source_name_key=source_name_key,\n            source_name_alias=source_name_alias,\n        )\n\n        self.extended_sources_table = Table.read(filename, hdu=""ExtendedSources"")\n        self.rois = Table.read(filename, hdu=""ROIs"")\n        self.energy_bounds_table = Table.read(filename, hdu=""EnergyBounds"")\n'"
gammapy/catalog/gammacat.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Gammacat open TeV source catalog.\n\nhttps://github.com/gammapy/gamma-cat\n""""""\nimport logging\nimport numpy as np\nfrom astropy import units as u\nfrom astropy.table import Table\nfrom gammapy.estimators import FluxPoints\nfrom gammapy.modeling.models import Model, SkyModel\nfrom gammapy.utils.scripts import make_path\nfrom .core import SourceCatalog, SourceCatalogObject\n\n__all__ = [""SourceCatalogGammaCat"", ""SourceCatalogObjectGammaCat""]\n\nlog = logging.getLogger(__name__)\n\n\nclass SourceCatalogObjectGammaCat(SourceCatalogObject):\n    """"""One object from the gamma-cat source catalog.\n\n    Catalog is represented by `~gammapy.catalog.SourceCatalogGammaCat`.\n    """"""\n\n    _source_name_key = ""common_name""\n\n    def __str__(self):\n        return self.info()\n\n    def info(self, info=""all""):\n        """"""Info string.\n\n        Parameters\n        ----------\n        info : {\'all\', \'basic\', \'position, \'model\'}\n            Comma separated list of options\n        """"""\n        if info == ""all"":\n            info = ""basic,position,model""\n\n        ss = """"\n        ops = info.split("","")\n        if ""basic"" in ops:\n            ss += self._info_basic()\n        if ""position"" in ops:\n            ss += self._info_position()\n        if ""model"" in ops:\n            ss += self._info_morph()\n            ss += self._info_spectral_fit()\n            ss += self._info_spectral_points()\n\n        return ss\n\n    def _info_basic(self):\n        """"""Print basic info.""""""\n        d = self.data\n        return (\n            f""\\n*** Basic info ***\\n\\n""\n            f""Catalog row index (zero-based): {self.row_index}\\n""\n            f""Common name: {self.name}\\n""\n            f""Gamma names: {d.gamma_names}\\n""\n            f""Fermi names: {d.fermi_names}\\n""\n            f""Other names: {d.other_names}\\n""\n            f""Location: {d.where}\\n""\n            f""Class: {d.classes}\\n\\n""\n            f""TeVCat ID: {d.tevcat_id}\\n""\n            f""TeVCat 2 ID: {d.tevcat2_id}\\n""\n            f""TeVCat name: {d.tevcat_name}\\n\\n""\n            f""TGeVCat ID: {d.tgevcat_id}\\n""\n            f""TGeVCat name: {d.tgevcat_name}\\n\\n""\n            f""Discoverer: {d.discoverer}\\n""\n            f""Discovery date: {d.discovery_date}\\n""\n            f""Seen by: {d.seen_by}\\n""\n            f""Reference: {d.reference_id}\\n""\n        )\n\n    def _info_position(self):\n        """"""Print position info.""""""\n        d = self.data\n        return (\n            f""\\n*** Position info ***\\n\\n""\n            f""SIMBAD:\\n""\n            f""RA: {d.ra:.3f}\\n""\n            f""DEC: {d.dec:.3f}\\n""\n            f""GLON: {d.glon:.3f}\\n""\n            f""GLAT: {d.glat:.3f}\\n""\n            f""\\nMeasurement:\\n""\n            f""RA: {d.pos_ra:.3f}\\n""\n            f""DEC: {d.pos_dec:.3f}\\n""\n            f""GLON: {d.pos_glon:.3f}\\n""\n            f""GLAT: {d.pos_glat:.3f}\\n""\n            f""Position error: {d.pos_err:.3f}\\n""\n        )\n\n    def _info_morph(self):\n        """"""Print morphology info.""""""\n        d = self.data\n        return (\n            f""\\n*** Morphology info ***\\n\\n""\n            f""Morphology model type: {d.morph_type}\\n""\n            f""Sigma: {d.morph_sigma:.3f}\\n""\n            f""Sigma error: {d.morph_sigma_err:.3f}\\n""\n            f""Sigma2: {d.morph_sigma2:.3f}\\n""\n            f""Sigma2 error: {d.morph_sigma2_err:.3f}\\n""\n            f""Position angle: {d.morph_pa:.3f}\\n""\n            f""Position angle error: {d.morph_pa_err:.3f}\\n""\n            f""Position angle frame: {d.morph_pa_frame}\\n""\n        )\n\n    def _info_spectral_fit(self):\n        """"""Print spectral info.""""""\n        d = self.data\n        ss = f""\\n*** Spectral info ***\\n\\n""\n        ss += f""Significance: {d.significance:.3f}\\n""\n        ss += f""Livetime: {d.livetime:.3f}\\n""\n\n        spec_type = d[""spec_type""]\n        ss += f""\\nSpectrum type: {spec_type}\\n""\n        if spec_type == ""pl"":\n            ss += f""norm: {d.spec_pl_norm:.3} +- {d.spec_pl_norm_err:.3} (stat) +- {d.spec_pl_norm_err_sys:.3} (sys) cm-2 s-1 TeV-1\\n""\n            ss += f""index: {d.spec_pl_index:.3} +- {d.spec_pl_index_err:.3} (stat) +- {d.spec_pl_index_err_sys:.3} (sys)\\n""\n            ss += f""reference: {d.spec_pl_e_ref:.3}\\n""\n        elif spec_type == ""pl2"":\n            ss += f""flux: {d.spec_pl2_flux.value:.3} +- {d.spec_pl2_flux_err.value:.3} (stat) +- {d.spec_pl2_flux_err_sys.value:.3} (sys) cm-2 s-1\\n""\n            ss += f""index: {d.spec_pl2_index:.3} +- {d.spec_pl2_index_err:.3} (stat) +- {d.spec_pl2_index_err_sys:.3} (sys)\\n""\n            ss += f""e_min: {d.spec_pl2_e_min:.3}\\n""\n            ss += f""e_max: {d.spec_pl2_e_max:.3}\\n""\n        elif spec_type == ""ecpl"":\n            ss += f""norm: {d.spec_ecpl_norm.value:.3g} +- {d.spec_ecpl_norm_err.value:.3g} (stat) +- {d.spec_ecpl_norm_err_sys.value:.03g} (sys) cm-2 s-1 TeV-1\\n""\n            ss += f""index: {d.spec_ecpl_index:.3} +- {d.spec_ecpl_index_err:.3} (stat) +- {d.spec_ecpl_index_err_sys:.3} (sys)\\n""\n            ss += f""e_cut: {d.spec_ecpl_e_cut.value:.3} +- {d.spec_ecpl_e_cut_err.value:.3} (stat) +- {d.spec_ecpl_e_cut_err_sys.value:.3} (sys) TeV\\n""\n            ss += f""reference: {d.spec_ecpl_e_ref:.3}\\n""\n        elif spec_type == ""none"":\n            pass\n        else:\n            raise ValueError(f""Invalid spec_type: {spec_type}"")\n\n        ss += f""\\nEnergy range: ({d.spec_erange_min.value:.3}, {d.spec_erange_max.value:.3}) TeV\\n""\n        ss += f""theta: {d.spec_theta:.3}\\n""\n\n        ss += ""\\n\\nDerived fluxes:\\n""\n\n        ss += f""Spectral model norm (1 TeV): {d.spec_dnde_1TeV:.3} +- {d.spec_dnde_1TeV_err:.3} (stat) cm-2 s-1 TeV-1\\n""\n        ss += f""Integrated flux (>1 TeV): {d.spec_flux_1TeV.value:.3} +- {d.spec_flux_1TeV_err.value:.3} (stat) cm-2 s-1\\n""\n        ss += f""Integrated flux (>1 TeV): {d.spec_flux_1TeV_crab:.3f} +- {d.spec_flux_1TeV_crab_err:.3f} (% Crab)\\n""\n        ss += f""Integrated flux (1-10 TeV): {d.spec_eflux_1TeV_10TeV.value:.3} +- {d.spec_eflux_1TeV_10TeV_err.value:.3} (stat) erg cm-2 s-1\\n""\n\n        return ss\n\n    def _info_spectral_points(self):\n        """"""Print spectral points info.""""""\n        d = self.data\n        ss = ""\\n*** Spectral points ***\\n\\n""\n        ss += f""SED reference ID: {d.sed_reference_id}\\n""\n        ss += f""Number of spectral points: {d.sed_n_points}\\n""\n        ss += f""Number of upper limits: {d.sed_n_ul}\\n\\n""\n\n        flux_points = self.flux_points\n        if flux_points is None:\n            ss += ""\\nNo spectral points available for this source.""\n        else:\n            lines = flux_points.table_formatted.pformat(max_width=-1, max_lines=-1)\n            ss += ""\\n"".join(lines)\n\n        return ss + ""\\n""\n\n    def spectral_model(self):\n        """"""Source spectral model (`~gammapy.modeling.models.SpectralModel`).\n\n        Parameter errors are statistical errors only.\n        """"""\n        data = self.data\n        spec_type = data[""spec_type""]\n\n        if spec_type == ""pl"":\n            tag = ""PowerLawSpectralModel""\n            pars = {\n                ""amplitude"": data[""spec_pl_norm""],\n                ""index"": data[""spec_pl_index""],\n                ""reference"": data[""spec_pl_e_ref""],\n            }\n            errs = {\n                ""amplitude"": data[""spec_pl_norm_err""],\n                ""index"": data[""spec_pl_index_err""],\n            }\n        elif spec_type == ""pl2"":\n            e_max = data[""spec_pl2_e_max""]\n            DEFAULT_E_MAX = u.Quantity(1e5, ""TeV"")\n            if np.isnan(e_max.value):\n                e_max = DEFAULT_E_MAX\n\n            tag = ""PowerLaw2SpectralModel""\n            pars = {\n                ""amplitude"": data[""spec_pl2_flux""],\n                ""index"": data[""spec_pl2_index""],\n                ""emin"": data[""spec_pl2_e_min""],\n                ""emax"": e_max,\n            }\n            errs = {\n                ""amplitude"": data[""spec_pl2_flux_err""],\n                ""index"": data[""spec_pl2_index_err""],\n            }\n        elif spec_type == ""ecpl"":\n            tag = ""ExpCutoffPowerLawSpectralModel""\n            pars = {\n                ""amplitude"": data[""spec_ecpl_norm""],\n                ""index"": data[""spec_ecpl_index""],\n                ""lambda_"": 1.0 / data[""spec_ecpl_e_cut""],\n                ""reference"": data[""spec_ecpl_e_ref""],\n            }\n            errs = {\n                ""amplitude"": data[""spec_ecpl_norm_err""],\n                ""index"": data[""spec_ecpl_index_err""],\n                ""lambda_"": data[""spec_ecpl_e_cut_err""] / data[""spec_ecpl_e_cut""] ** 2,\n            }\n        elif spec_type == ""none"":\n            return None\n        else:\n            raise ValueError(f""Invalid spec_type: {spec_type}"")\n\n        model = Model.create(tag, **pars)\n        for name, value in errs.items():\n            model.parameters[name].error = value\n\n        return model\n\n    def spatial_model(self):\n        """"""Source spatial model (`~gammapy.modeling.models.SpatialModel`).\n\n        TODO: add parameter errors!\n        """"""\n        d = self.data\n        morph_type = d[""morph_type""]\n\n        pars = {""lon_0"": d[""glon""], ""lat_0"": d[""glat""], ""frame"": ""galactic""}\n        errs = {\n            ""lat_0"": self.data[""pos_err""],\n            ""lon_0"": self.data[""pos_err""] / np.cos(self.data[""glat""]),\n        }\n\n        if morph_type == ""point"":\n            tag = ""PointSpatialModel""\n        elif morph_type == ""gauss"":\n            # TODO: add infos back once model support elongation\n            # pars[\'x_stddev\'] = d[\'morph_sigma\']\n            # pars[\'y_stddev\'] = d[\'morph_sigma\']\n            # if not np.isnan(d[\'morph_sigma2\']):\n            #     pars[\'y_stddev\'] = d[\'morph_sigma2\']\n            # if not np.isnan(d[\'morph_pa\']):\n            #     # TODO: handle reference frame for rotation angle\n            #     pars[\'theta\'] = Angle(d[\'morph_pa\'], \'deg\').rad\n            tag = ""GaussianSpatialModel""\n            pars[""sigma""] = d[""morph_sigma""]\n        elif morph_type == ""shell"":\n            tag = ""ShellSpatialModel""\n            # TODO: probably we shouldn\'t guess a shell width here!\n            pars[""radius""] = 0.8 * d[""morph_sigma""]\n            pars[""width""] = 0.2 * d[""morph_sigma""]\n        elif morph_type == ""none"":\n            return None\n        else:\n            raise ValueError(f""Invalid morph_type: {morph_type!r}"")\n\n        model = Model.create(tag, **pars)\n\n        for name, value in errs.items():\n            model.parameters[name].error = value\n\n        return model\n\n    def sky_model(self):\n        """"""Source sky model (`~gammapy.modeling.models.SkyModel`).""""""\n        return SkyModel(\n            spatial_model=self.spatial_model(),\n            spectral_model=self.spectral_model(),\n            name=self.name,\n        )\n\n    def _add_source_meta(self, table):\n        """"""Copy over some info to table.meta""""""\n        d = self.data\n        m = table.meta\n        m[""origin""] = ""Data from gamma-cat""\n        m[""source_id""] = d[""source_id""]\n        m[""common_name""] = d[""common_name""]\n        m[""reference_id""] = d[""reference_id""]\n\n    @property\n    def flux_points(self):\n        """"""Differential flux points (`~gammapy.spectrum.FluxPoints`).""""""\n        d = self.data\n        table = Table()\n        table.meta[""SED_TYPE""] = ""dnde""\n        self._add_source_meta(table)\n\n        valid = np.isfinite(d[""sed_e_ref""].value)\n\n        if valid.sum() == 0:\n            return None\n\n        table[""e_ref""] = d[""sed_e_ref""]\n        table[""e_min""] = d[""sed_e_min""]\n        table[""e_max""] = d[""sed_e_max""]\n\n        table[""dnde""] = d[""sed_dnde""]\n        table[""dnde_err""] = d[""sed_dnde_err""]\n        table[""dnde_errn""] = d[""sed_dnde_errn""]\n        table[""dnde_errp""] = d[""sed_dnde_errp""]\n        table[""dnde_ul""] = d[""sed_dnde_ul""]\n\n        # Only keep rows that actually contain information\n        table = table[valid]\n\n        # Only keep columns that actually contain information\n        def _del_nan_col(table, colname):\n            if np.isfinite(table[colname]).sum() == 0:\n                del table[colname]\n\n        for colname in table.colnames:\n            _del_nan_col(table, colname)\n\n        return FluxPoints(table)\n\n\nclass SourceCatalogGammaCat(SourceCatalog):\n    """"""Gammacat open TeV source catalog.\n\n    See: https://github.com/gammapy/gamma-cat\n\n    One source is represented by `~gammapy.catalog.SourceCatalogObjectGammaCat`.\n\n    Parameters\n    ----------\n    filename : str\n        Path to the gamma-cat fits file.\n\n    Examples\n    --------\n    Load the catalog data:\n\n    >>> import astropy.units as u\n    >>> from gammapy.catalog import SourceCatalogGammaCat\n    >>> cat = SourceCatalogGammaCat()\n\n    Access a source by name:\n\n    >>> source = cat[\'Vela Junior\']\n\n    Access source spectral data and plot it:\n\n    >>> energy_range = [1, 10] * u.TeV\n    >>> source.spectral_model().plot(energy_range)\n    >>> source.spectral_model().plot_error(energy_range)\n    >>> source.flux_points.plot()\n    """"""\n\n    tag = ""gamma-cat""\n    description = ""An open catalog of gamma-ray sources""\n    source_object_class = SourceCatalogObjectGammaCat\n\n    def __init__(self, filename=""$GAMMAPY_DATA/catalogs/gammacat/gammacat.fits.gz""):\n        filename = make_path(filename)\n        table = Table.read(filename, hdu=1)\n\n        source_name_key = ""common_name""\n        source_name_alias = (""other_names"", ""gamma_names"")\n        super().__init__(\n            table=table,\n            source_name_key=source_name_key,\n            source_name_alias=source_name_alias,\n        )\n'"
gammapy/catalog/hawc.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""HAWC catalogs (https://www.hawc-observatory.org).""""""\nimport numpy as np\nfrom astropy.table import Table\nfrom gammapy.modeling.models import Model, SkyModel\nfrom gammapy.utils.scripts import make_path\nfrom .core import SourceCatalog, SourceCatalogObject\n\n__all__ = [""SourceCatalog2HWC"", ""SourceCatalogObject2HWC""]\n\n\nclass SourceCatalogObject2HWC(SourceCatalogObject):\n    """"""One source from the HAWC 2HWC catalog.\n\n    Catalog is represented by `~gammapy.catalog.SourceCatalog2HWC`.\n    """"""\n\n    _source_name_key = ""source_name""\n\n    def __str__(self):\n        return self.info()\n\n    def info(self, info=""all""):\n        """"""Summary info string.\n\n        Parameters\n        ----------\n        info : {\'all\', \'basic\', \'position\', \'spectrum\'}\n            Comma separated list of options\n        """"""\n        if info == ""all"":\n            info = ""basic,position,spectrum""\n\n        ss = """"\n        ops = info.split("","")\n        if ""basic"" in ops:\n            ss += self._info_basic()\n        if ""position"" in ops:\n            ss += self._info_position()\n        if ""spectrum"" in ops:\n            ss += self._info_spectrum()\n\n        return ss\n\n    def _info_basic(self):\n        """"""Print basic info.""""""\n        return (\n            f""\\n*** Basic info ***\\n\\n""\n            f""Catalog row index (zero-based) : {self.row_index}\\n""\n            f""Source name : {self.name}\\n""\n        )\n\n    def _info_position(self):\n        """"""Print position info.""""""\n        return (\n            f""\\n*** Position info ***\\n\\n""\n            f""RA: {self.data.ra:.3f}\\n""\n            f""DEC: {self.data.dec:.3f}\\n""\n            f""GLON: {self.data.glon:.3f}\\n""\n            f""GLAT: {self.data.glat:.3f}\\n""\n            f""Position error: {self.data.pos_err:.3f}\\n""\n        )\n\n    @staticmethod\n    def _info_spectrum_one(d, idx):\n        ss = f""Spectrum {idx}:\\n""\n        val, err = d[f""spec{idx}_dnde""].value, d[f""spec{idx}_dnde_err""].value\n        ss += f""Flux at 7 TeV: {val:.3} +- {err:.3} cm-2 s-1 TeV-1\\n""\n        val, err = d[f""spec{idx}_index""], d[f""spec{idx}_index_err""]\n        ss += f""Spectral index: {val:.3f} +- {err:.3f}\\n""\n        radius = d[f""spec{idx}_radius""]\n        ss += f""Test Radius: {radius:1}\\n\\n""\n        return ss\n\n    def _info_spectrum(self):\n        """"""Print spectral info.""""""\n        ss = ""\\n*** Spectral info ***\\n\\n""\n        ss += self._info_spectrum_one(self.data, 0)\n\n        if self.n_models == 2:\n            ss += self._info_spectrum_one(self.data, 1)\n        else:\n            ss += ""No second spectrum available for this source""\n\n        return ss\n\n    @property\n    def n_models(self):\n        """"""Number of models (1 or 2).""""""\n        return 1 if np.isnan(self.data.spec1_dnde) else 2\n\n    def _get_idx(self, which):\n        if which == ""point"":\n            return 0\n        elif which == ""extended"":\n            if self.n_models == 2:\n                return 1\n            else:\n                raise ValueError(f""No extended source analysis available: {self.name}"")\n        else:\n            raise ValueError(f""Invalid which: {which!r}"")\n\n    def spectral_model(self, which=""point""):\n        """"""Spectral model (`~gammapy.modeling.models.PowerLawSpectralModel`).\n\n        * ``which=""point""`` -- Spectral model under the point source assumption.\n        * ``which=""extended""`` -- Spectral model under the extended source assumption.\n          Only available for some sources. Raise ValueError if not available.\n        """"""\n        idx = self._get_idx(which)\n\n        pars = {\n            ""reference"": ""7 TeV"",\n            ""amplitude"": self.data[f""spec{idx}_dnde""],\n            ""index"": -self.data[f""spec{idx}_index""],\n        }\n\n        errs = {\n            ""amplitude"": self.data[f""spec{idx}_dnde_err""],\n            ""index"": self.data[f""spec{idx}_index_err""],\n        }\n\n        model = Model.create(""PowerLawSpectralModel"", **pars)\n\n        for name, value in errs.items():\n            model.parameters[name].error = value\n\n        return model\n\n    def spatial_model(self, which=""point""):\n        """"""Spatial model (`~gammapy.modeling.models.SpatialModel`).\n\n        * ``which=""point""`` - `~gammapy.modeling.models.PointSpatialModel`\n        * ``which=""extended""`` - `~gammapy.modeling.models.DiskSpatialModel`.\n          Only available for some sources. Raise ValueError if not available.\n        """"""\n        idx = self._get_idx(which)\n        pars = {""lon_0"": self.data.glon, ""lat_0"": self.data.glat, ""frame"": ""galactic""}\n\n        if idx == 0:\n            tag = ""PointSpatialModel""\n        else:\n            tag = ""DiskSpatialModel""\n            pars[""r_0""] = self.data[f""spec{idx}_radius""]\n\n        errs = {\n            ""lat_0"": self.data.pos_err,\n            ""lon_0"": self.data.pos_err / np.cos(self.data.glat),\n        }\n\n        model = Model.create(tag, **pars)\n\n        for name, value in errs.items():\n            model.parameters[name].error = value\n\n        return model\n\n    def sky_model(self, which=""point""):\n        """"""Sky model (`~gammapy.modeling.models.SkyModel`).\n\n        * ``which=""point""`` - Sky model for point source analysis\n        * ``which=""extended""`` - Sky model for extended source analysis.\n          Only available for some sources. Raise ValueError if not available.\n\n        According to the paper, the radius of the extended source model is only a rough estimate\n        of the source size, based on the residual excess..\n        """"""\n        return SkyModel(\n            spatial_model=self.spatial_model(which),\n            spectral_model=self.spectral_model(which),\n            name=self.name,\n        )\n\n\nclass SourceCatalog2HWC(SourceCatalog):\n    """"""HAWC 2HWC catalog.\n\n    One source is represented by `~gammapy.catalog.SourceCatalogObject2HWC`.\n\n    The data is from tables 2 and 3 in the paper [1]_.\n\n    The catalog table contains 40 rows / sources.\n    The paper mentions 39 sources e.g. in the abstract.\n    The difference is due to Geminga, which was detected as two ""sources"" by the algorithm\n    used to make the catalog, but then in the discussion considered as one source.\n\n    References\n    ----------\n    .. [1] Abeysekara et al, ""The 2HWC HAWC Observatory Gamma Ray Catalog"",\n       On ADS: `2017ApJ...843...40A <https://ui.adsabs.harvard.edu/abs/2017ApJ...843...40A>`__\n    """"""\n\n    tag = ""2hwc""\n    """"""Catalog name""""""\n\n    description = ""2HWC catalog from the HAWC observatory""\n    """"""Catalog description""""""\n\n    source_object_class = SourceCatalogObject2HWC\n\n    def __init__(self, filename=""$GAMMAPY_DATA/catalogs/2HWC.ecsv""):\n        table = Table.read(make_path(filename), format=""ascii.ecsv"")\n\n        source_name_key = ""source_name""\n\n        super().__init__(table=table, source_name_key=source_name_key)\n'"
gammapy/catalog/hess.py,5,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""HESS Galactic plane survey (HGPS) catalog.""""""\nimport numpy as np\nimport astropy.units as u\nfrom astropy.coordinates import Angle\nfrom astropy.modeling.models import Gaussian1D\nfrom astropy.table import Table\nfrom gammapy.estimators import FluxPoints\nfrom gammapy.modeling.models import Model, Models, SkyModel\nfrom gammapy.utils.interpolation import ScaledRegularGridInterpolator\nfrom gammapy.utils.scripts import make_path\nfrom gammapy.utils.table import table_row_to_dict\nfrom .core import SourceCatalog, SourceCatalogObject\n\n__all__ = [\n    ""SourceCatalogHGPS"",\n    ""SourceCatalogObjectHGPS"",\n    ""SourceCatalogObjectHGPSComponent"",\n    ""SourceCatalogLargeScaleHGPS"",\n]\n\n# Flux factor, used for printing\nFF = 1e-12\n\n# Multiplicative factor to go from cm^-2 s^-1 to % Crab for integral flux > 1 TeV\n# Here we use the same Crab reference that\'s used in the HGPS paper\n# CRAB = crab_integral_flux(energy_min=1, reference=\'hess_ecpl\')\nFLUX_TO_CRAB = 100 / 2.26e-11\nFLUX_TO_CRAB_DIFF = 100 / 3.5060459323111307e-11\n\n\nclass SourceCatalogObjectHGPSComponent(SourceCatalogObject):\n    """"""One Gaussian component from the HGPS catalog.\n\n    See Also\n    --------\n    SourceCatalogHGPS, SourceCatalogObjectHGPS\n    """"""\n\n    _source_name_key = ""Component_ID""\n\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):\n        return f""{self.__class__.__name__}({self.name!r})""\n\n    def __str__(self):\n        """"""Pretty-print source data""""""\n        d = self.data\n        ss = ""Component {}:\\n"".format(d[""Component_ID""])\n        fmt = ""{:<20s} : {:8.3f} +/- {:.3f} deg\\n""\n        ss += fmt.format(""GLON"", d[""GLON""].value, d[""GLON_Err""].value)\n        ss += fmt.format(""GLAT"", d[""GLAT""].value, d[""GLAT_Err""].value)\n        fmt = ""{:<20s} : {:.3f} +/- {:.3f} deg\\n""\n        ss += fmt.format(""Size"", d[""Size""].value, d[""Size_Err""].value)\n        val, err = d[""Flux_Map""].value, d[""Flux_Map_Err""].value\n        fmt = ""{:<20s} : ({:.2f} +/- {:.2f}) x 10^-12 cm^-2 s^-1 = ({:.1f} +/- {:.1f}) % Crab""\n        ss += fmt.format(\n            ""Flux (>1 TeV)"", val / FF, err / FF, val * FLUX_TO_CRAB, err * FLUX_TO_CRAB\n        )\n        return ss\n\n    @property\n    def name(self):\n        """"""Source name (str)""""""\n        return self.data[self._source_name_key]\n\n    def spatial_model(self):\n        """"""Component spatial model (`~gammapy.modeling.models.GaussianSpatialModel`).""""""\n        d = self.data\n        tag = ""GaussianSpatialModel""\n        pars = {\n            ""lon_0"": d[""GLON""],\n            ""lat_0"": d[""GLAT""],\n            ""sigma"": d[""Size""],\n            ""frame"": ""galactic"",\n        }\n        errs = {""lon_0"": d[""GLON_Err""], ""lat_0"": d[""GLAT_Err""], ""sigma"": d[""Size_Err""]}\n        model = Model.create(tag, **pars)\n\n        for name, value in errs.items():\n            model.parameters[name].error = value\n\n        return model\n\n\nclass SourceCatalogObjectHGPS(SourceCatalogObject):\n    """"""One object from the HGPS catalog.\n\n    The catalog is represented by `SourceCatalogHGPS`.\n    Examples are given there.\n    """"""\n\n    def __repr__(self):\n        return f""{self.__class__.__name__}({self.name!r})""\n\n    def __str__(self):\n        return self.info()\n\n    def info(self, info=""all""):\n        """"""Info string.\n\n        Parameters\n        ----------\n        info : {\'all\', \'basic\', \'map\', \'spec\', \'flux_points\', \'components\', \'associations\', \'id\'}\n            Comma separated list of options\n        """"""\n        if info == ""all"":\n            info = ""basic,associations,id,map,spec,flux_points,components""\n\n        ss = """"\n        ops = info.split("","")\n        if ""basic"" in ops:\n            ss += self._info_basic()\n        if ""map"" in ops:\n            ss += self._info_map()\n        if ""spec"" in ops:\n            ss += self._info_spec()\n        if ""flux_points"" in ops:\n            ss += self._info_flux_points()\n        if ""components"" in ops and hasattr(self, ""components""):\n            ss += self._info_components()\n        if ""associations"" in ops:\n            ss += self._info_associations()\n        if ""id"" in ops and hasattr(self, ""identification_info""):\n            ss += self._info_id()\n        return ss\n\n    def _info_basic(self):\n        """"""Print basic info.""""""\n        d = self.data\n        ss = ""\\n*** Basic info ***\\n\\n""\n        ss += ""Catalog row index (zero-based) : {}\\n"".format(self.row_index)\n        ss += ""{:<20s} : {}\\n"".format(""Source name"", self.name)\n\n        ss += ""{:<20s} : {}\\n"".format(""Analysis reference"", d[""Analysis_Reference""])\n        ss += ""{:<20s} : {}\\n"".format(""Source class"", d[""Source_Class""])\n        ss += ""{:<20s} : {}\\n"".format(""Identified object"", d[""Identified_Object""])\n        ss += ""{:<20s} : {}\\n"".format(""Gamma-Cat id"", d[""Gamma_Cat_Source_ID""])\n        ss += ""\\n""\n        return ss\n\n    def _info_associations(self):\n        ss = ""\\n*** Source associations info ***\\n\\n""\n        lines = self.associations.pformat(max_width=-1, max_lines=-1)\n        ss += ""\\n"".join(lines)\n        return ss + ""\\n""\n\n    def _info_id(self):\n        ss = ""\\n*** Source identification info ***\\n\\n""\n        ss += ""\\n"".join(f""{k}: {v}"" for k, v in self.identification_info.items())\n        return ss + ""\\n""\n\n    def _info_map(self):\n        """"""Print info from map analysis.""""""\n        d = self.data\n        ss = ""\\n*** Info from map analysis ***\\n\\n""\n\n        ra_str = Angle(d[""RAJ2000""], ""deg"").to_string(unit=""hour"", precision=0)\n        dec_str = Angle(d[""DEJ2000""], ""deg"").to_string(unit=""deg"", precision=0)\n        ss += ""{:<20s} : {:8.3f} = {}\\n"".format(""RA"", d[""RAJ2000""], ra_str)\n        ss += ""{:<20s} : {:8.3f} = {}\\n"".format(""DEC"", d[""DEJ2000""], dec_str)\n\n        ss += ""{:<20s} : {:8.3f} +/- {:.3f} deg\\n"".format(\n            ""GLON"", d[""GLON""].value, d[""GLON_Err""].value\n        )\n        ss += ""{:<20s} : {:8.3f} +/- {:.3f} deg\\n"".format(\n            ""GLAT"", d[""GLAT""].value, d[""GLAT_Err""].value\n        )\n\n        ss += ""{:<20s} : {:.3f}\\n"".format(""Position Error (68%)"", d[""Pos_Err_68""])\n        ss += ""{:<20s} : {:.3f}\\n"".format(""Position Error (95%)"", d[""Pos_Err_95""])\n\n        ss += ""{:<20s} : {:.0f}\\n"".format(""ROI number"", d[""ROI_Number""])\n        ss += ""{:<20s} : {}\\n"".format(""Spatial model"", d[""Spatial_Model""])\n        ss += ""{:<20s} : {}\\n"".format(""Spatial components"", d[""Components""])\n\n        ss += ""{:<20s} : {:.1f}\\n"".format(""TS"", d[""Sqrt_TS""] ** 2)\n        ss += ""{:<20s} : {:.1f}\\n"".format(""sqrt(TS)"", d[""Sqrt_TS""])\n\n        ss += ""{:<20s} : {:.3f} +/- {:.3f} (UL: {:.3f}) deg\\n"".format(\n            ""Size"", d[""Size""].value, d[""Size_Err""].value, d[""Size_UL""].value\n        )\n\n        ss += ""{:<20s} : {:.3f}\\n"".format(""R70"", d[""R70""])\n        ss += ""{:<20s} : {:.3f}\\n"".format(""RSpec"", d[""RSpec""])\n\n        ss += ""{:<20s} : {:.1f}\\n"".format(""Total model excess"", d[""Excess_Model_Total""])\n        ss += ""{:<20s} : {:.1f}\\n"".format(""Excess in RSpec"", d[""Excess_RSpec""])\n        ss += ""{:<20s} : {:.1f}\\n"".format(\n            ""Model Excess in RSpec"", d[""Excess_RSpec_Model""]\n        )\n        ss += ""{:<20s} : {:.1f}\\n"".format(""Background in RSpec"", d[""Background_RSpec""])\n\n        ss += ""{:<20s} : {:.1f} hours\\n"".format(""Livetime"", d[""Livetime""].value)\n\n        ss += ""{:<20s} : {:.2f}\\n"".format(""Energy threshold"", d[""Energy_Threshold""])\n\n        val, err = d[""Flux_Map""].value, d[""Flux_Map_Err""].value\n        ss += ""{:<20s} : ({:.3f} +/- {:.3f}) x 10^-12 cm^-2 s^-1 = ({:.2f} +/- {:.2f}) % Crab\\n"".format(\n            ""Source flux (>1 TeV)"",\n            val / FF,\n            err / FF,\n            val * FLUX_TO_CRAB,\n            err * FLUX_TO_CRAB,\n        )\n\n        ss += ""\\nFluxes in RSpec (> 1 TeV):\\n""\n\n        ss += ""{:<30s} : {:.3f} x 10^-12 cm^-2 s^-1 = {:5.2f} % Crab\\n"".format(\n            ""Map measurement"",\n            d[""Flux_Map_RSpec_Data""].value / FF,\n            d[""Flux_Map_RSpec_Data""].value * FLUX_TO_CRAB,\n        )\n\n        ss += ""{:<30s} : {:.3f} x 10^-12 cm^-2 s^-1 = {:5.2f} % Crab\\n"".format(\n            ""Source model"",\n            d[""Flux_Map_RSpec_Source""].value / FF,\n            d[""Flux_Map_RSpec_Source""].value * FLUX_TO_CRAB,\n        )\n\n        ss += ""{:<30s} : {:.3f} x 10^-12 cm^-2 s^-1 = {:5.2f} % Crab\\n"".format(\n            ""Other component model"",\n            d[""Flux_Map_RSpec_Other""].value / FF,\n            d[""Flux_Map_RSpec_Other""].value * FLUX_TO_CRAB,\n        )\n\n        ss += ""{:<30s} : {:.3f} x 10^-12 cm^-2 s^-1 = {:5.2f} % Crab\\n"".format(\n            ""Large scale component model"",\n            d[""Flux_Map_RSpec_LS""].value / FF,\n            d[""Flux_Map_RSpec_LS""].value * FLUX_TO_CRAB,\n        )\n\n        ss += ""{:<30s} : {:.3f} x 10^-12 cm^-2 s^-1 = {:5.2f} % Crab\\n"".format(\n            ""Total model"",\n            d[""Flux_Map_RSpec_Total""].value / FF,\n            d[""Flux_Map_RSpec_Total""].value * FLUX_TO_CRAB,\n        )\n\n        ss += ""{:<35s} : {:5.1f} %\\n"".format(\n            ""Containment in RSpec"", 100 * d[""Containment_RSpec""]\n        )\n        ss += ""{:<35s} : {:5.1f} %\\n"".format(\n            ""Contamination in RSpec"", 100 * d[""Contamination_RSpec""]\n        )\n        label, val = (\n            ""Flux correction (RSpec -> Total)"",\n            100 * d[""Flux_Correction_RSpec_To_Total""],\n        )\n        ss += f""{label:<35s} : {val:5.1f} %\\n""\n        label, val = (\n            ""Flux correction (Total -> RSpec)"",\n            100 * (1 / d[""Flux_Correction_RSpec_To_Total""]),\n        )\n        ss += f""{label:<35s} : {val:5.1f} %\\n""\n\n        return ss\n\n    def _info_spec(self):\n        """"""Print info from spectral analysis.""""""\n        d = self.data\n        ss = ""\\n*** Info from spectral analysis ***\\n\\n""\n\n        ss += ""{:<20s} : {:.1f} hours\\n"".format(""Livetime"", d[""Livetime_Spec""].value)\n\n        lo = d[""Energy_Range_Spec_Min""].value\n        hi = d[""Energy_Range_Spec_Max""].value\n        ss += ""{:<20s} : {:.2f} to {:.2f} TeV\\n"".format(""Energy range:"", lo, hi)\n\n        ss += ""{:<20s} : {:.1f}\\n"".format(""Background"", d[""Background_Spec""])\n        ss += ""{:<20s} : {:.1f}\\n"".format(""Excess"", d[""Excess_Spec""])\n        ss += ""{:<20s} : {}\\n"".format(""Spectral model"", d[""Spectral_Model""])\n\n        val = d[""TS_ECPL_over_PL""]\n        ss += ""{:<20s} : {:.1f}\\n"".format(""TS ECPL over PL"", val)\n\n        val = d[""Flux_Spec_Int_1TeV""].value\n        err = d[""Flux_Spec_Int_1TeV_Err""].value\n        ss += ""{:<20s} : ({:.3f} +/- {:.3f}) x 10^-12 cm^-2 s^-1  = ({:.2f} +/- {:.2f}) % Crab\\n"".format(\n            ""Best-fit model flux(> 1 TeV)"",\n            val / FF,\n            err / FF,\n            val * FLUX_TO_CRAB,\n            err * FLUX_TO_CRAB,\n        )\n\n        val = d[""Flux_Spec_Energy_1_10_TeV""].value\n        err = d[""Flux_Spec_Energy_1_10_TeV_Err""].value\n        ss += ""{:<20s} : ({:.3f} +/- {:.3f}) x 10^-12 erg cm^-2 s^-1\\n"".format(\n            ""Best-fit model energy flux(1 to 10 TeV)"", val / FF, err / FF\n        )\n\n        ss += self._info_spec_pl()\n        ss += self._info_spec_ecpl()\n\n        return ss\n\n    def _info_spec_pl(self):\n        d = self.data\n        ss = ""{:<20s} : {:.2f}\\n"".format(""Pivot energy"", d[""Energy_Spec_PL_Pivot""])\n\n        val = d[""Flux_Spec_PL_Diff_Pivot""].value\n        err = d[""Flux_Spec_PL_Diff_Pivot_Err""].value\n        ss += ""{:<20s} : ({:.3f} +/- {:.3f}) x 10^-12 cm^-2 s^-1 TeV^-1  = ({:.2f} +/- {:.2f}) % Crab\\n"".format(\n            ""Flux at pivot energy"",\n            val / FF,\n            err / FF,\n            val * FLUX_TO_CRAB,\n            err * FLUX_TO_CRAB_DIFF,\n        )\n\n        val = d[""Flux_Spec_PL_Int_1TeV""].value\n        err = d[""Flux_Spec_PL_Int_1TeV_Err""].value\n        ss += ""{:<20s} : ({:.3f} +/- {:.3f}) x 10^-12 cm^-2 s^-1  = ({:.2f} +/- {:.2f}) % Crab\\n"".format(\n            ""PL   Flux(> 1 TeV)"",\n            val / FF,\n            err / FF,\n            val * FLUX_TO_CRAB,\n            err * FLUX_TO_CRAB,\n        )\n\n        val = d[""Flux_Spec_PL_Diff_1TeV""].value\n        err = d[""Flux_Spec_PL_Diff_1TeV_Err""].value\n        ss += ""{:<20s} : ({:.3f} +/- {:.3f}) x 10^-12 cm^-2 s^-1 TeV^-1  = ({:.2f} +/- {:.2f}) % Crab\\n"".format(\n            ""PL   Flux(@ 1 TeV)"",\n            val / FF,\n            err / FF,\n            val * FLUX_TO_CRAB,\n            err * FLUX_TO_CRAB_DIFF,\n        )\n\n        val = d[""Index_Spec_PL""]\n        err = d[""Index_Spec_PL_Err""]\n        ss += ""{:<20s} : {:.2f} +/- {:.2f}\\n"".format(""PL   Index"", val, err)\n\n        return ss\n\n    def _info_spec_ecpl(self):\n        d = self.data\n        ss = """"\n        # ss = \'{:<20s} : {:.1f}\\n\'.format(\'Pivot energy\', d[\'Energy_Spec_ECPL_Pivot\'])\n\n        val = d[""Flux_Spec_ECPL_Diff_1TeV""].value\n        err = d[""Flux_Spec_ECPL_Diff_1TeV_Err""].value\n        ss += ""{:<20s} : ({:.3f} +/- {:.3f}) x 10^-12 cm^-2 s^-1 TeV^-1  = ({:.2f} +/- {:.2f}) % Crab\\n"".format(\n            ""ECPL   Flux(@ 1 TeV)"",\n            val / FF,\n            err / FF,\n            val * FLUX_TO_CRAB,\n            err * FLUX_TO_CRAB_DIFF,\n        )\n\n        val = d[""Flux_Spec_ECPL_Int_1TeV""].value\n        err = d[""Flux_Spec_ECPL_Int_1TeV_Err""].value\n        ss += ""{:<20s} : ({:.3f} +/- {:.3f}) x 10^-12 cm^-2 s^-1  = ({:.2f} +/- {:.2f}) % Crab\\n"".format(\n            ""ECPL   Flux(> 1 TeV)"",\n            val / FF,\n            err / FF,\n            val * FLUX_TO_CRAB,\n            err * FLUX_TO_CRAB,\n        )\n\n        val = d[""Index_Spec_ECPL""]\n        err = d[""Index_Spec_ECPL_Err""]\n        ss += ""{:<20s} : {:.2f} +/- {:.2f}\\n"".format(""ECPL Index"", val, err)\n\n        val = d[""Lambda_Spec_ECPL""].value\n        err = d[""Lambda_Spec_ECPL_Err""].value\n        ss += ""{:<20s} : {:.3f} +/- {:.3f} TeV^-1\\n"".format(""ECPL Lambda"", val, err)\n\n        # Use Gaussian analytical error propagation,\n        # tested against the uncertainties package\n        err = err / val ** 2\n        val = 1.0 / val\n\n        ss += ""{:<20s} : {:.2f} +/- {:.2f} TeV\\n"".format(""ECPL E_cut"", val, err)\n\n        return ss\n\n    def _info_flux_points(self):\n        """"""Print flux point results""""""\n        d = self.data\n        ss = ""\\n*** Flux points info ***\\n\\n""\n        ss += ""Number of flux points: {}\\n"".format(d[""N_Flux_Points""])\n        ss += ""Flux points table: \\n\\n""\n        lines = self.flux_points.table_formatted.pformat(max_width=-1, max_lines=-1)\n        ss += ""\\n"".join(lines)\n        return ss + ""\\n""\n\n    def _info_components(self):\n        """"""Print info about the components.""""""\n        ss = ""\\n*** Gaussian component info ***\\n\\n""\n        ss += ""Number of components: {}\\n"".format(len(self.components))\n        ss += ""{:<20s} : {}\\n\\n"".format(""Spatial components"", self.data[""Components""])\n\n        for component in self.components:\n            ss += str(component)\n            ss += ""\\n\\n""\n\n        return ss\n\n    @property\n    def energy_range(self):\n        """"""Spectral model energy range (`~astropy.units.Quantity` with length 2).""""""\n        emin, emax = (\n            self.data[""Energy_Range_Spec_Min""],\n            self.data[""Energy_Range_Spec_Max""],\n        )\n\n        if np.isnan(emin):\n            emin = u.Quantity(0.2, ""TeV"")\n\n        if np.isnan(emax):\n            emax = u.Quantity(50, ""TeV"")\n\n        return u.Quantity([emin, emax], ""TeV"")\n\n    def spectral_model(self, which=""best""):\n        """"""Spectral model (`~gammapy.modeling.models.SpectralModel`).\n\n        One of the following models (given by ``Spectral_Model`` in the catalog):\n\n        - ``PL`` : `~gammapy.modeling.models.PowerLawSpectralModel`\n        - ``ECPL`` : `~gammapy.modeling.models.ExpCutoffPowerLawSpectralModel`\n\n        Parameters\n        ----------\n        which : {\'best\', \'pl\', \'ecpl\'}\n            Which spectral model\n        """"""\n        data = self.data\n\n        if which == ""best"":\n            spec_type = self.data[""Spectral_Model""].strip().lower()\n        elif which in {""pl"", ""ecpl""}:\n            spec_type = which\n        else:\n            raise ValueError(f""Invalid selection: which = {which!r}"")\n\n        if spec_type == ""pl"":\n            tag = ""PowerLawSpectralModel""\n            pars = {\n                ""index"": data[""Index_Spec_PL""],\n                ""amplitude"": data[""Flux_Spec_PL_Diff_Pivot""],\n                ""reference"": data[""Energy_Spec_PL_Pivot""],\n            }\n            errs = {\n                ""amplitude"": data[""Flux_Spec_PL_Diff_Pivot_Err""],\n                ""index"": data[""Index_Spec_PL_Err""],\n            }\n        elif spec_type == ""ecpl"":\n            tag = ""ExpCutoffPowerLawSpectralModel""\n            pars = {\n                ""index"": data[""Index_Spec_ECPL""],\n                ""amplitude"": data[""Flux_Spec_ECPL_Diff_Pivot""],\n                ""reference"": data[""Energy_Spec_ECPL_Pivot""],\n                ""lambda_"": data[""Lambda_Spec_ECPL""],\n            }\n            errs = {\n                ""index"": data[""Index_Spec_ECPL_Err""],\n                ""amplitude"": data[""Flux_Spec_ECPL_Diff_Pivot_Err""],\n                ""lambda_"": data[""Lambda_Spec_ECPL_Err""],\n            }\n        else:\n            raise ValueError(f""Invalid spec_type: {spec_type}"")\n\n        model = Model.create(tag, **pars)\n        errs[""reference""] = 0 * u.TeV\n\n        for name, value in errs.items():\n            model.parameters[name].error = value\n\n        return model\n\n    def spatial_model(self):\n        """"""Spatial model (`~gammapy.modeling.models.SpatialModel`).\n\n        One of the following models (given by ``Spatial_Model`` in the catalog):\n\n        - ``Point-Like`` or has a size upper limit : `~gammapy.modeling.models.PointSpatialModel`\n        - ``Gaussian``: `~gammapy.modeling.models.GaussianSpatialModel`\n        - ``2-Gaussian`` or ``3-Gaussian``: composite model (using ``+`` with Gaussians)\n        - ``Shell``: `~gammapy.modeling.models.ShellSpatialModel`\n        """"""\n        d = self.data\n        pars = {""lon_0"": d[""GLON""], ""lat_0"": d[""GLAT""], ""frame"": ""galactic""}\n        errs = {""lon_0"": d[""GLON_Err""], ""lat_0"": d[""GLAT_Err""]}\n\n        spatial_type = self.data[""Spatial_Model""]\n\n        if spatial_type == ""Point-Like"":\n            tag = ""PointSpatialModel""\n        elif spatial_type == ""Gaussian"":\n            tag = ""GaussianSpatialModel""\n            pars[""sigma""] = d[""Size""]\n            errs[""sigma""] = d[""Size_Err""]\n        elif spatial_type in {""2-Gaussian"", ""3-Gaussian""}:\n            raise ValueError(""For Gaussian or Multi-Gaussian models, use sky_model()!"")\n        elif spatial_type == ""Shell"":\n            # HGPS contains no information on shell width\n            # Here we assume a 5% shell width for all shells.\n            tag = ""ShellSpatialModel""\n            pars[""radius""] = 0.95 * d[""Size""]\n            pars[""width""] = d[""Size""] - pars[""radius""]\n            errs[""radius""] = d[""Size_Err""]\n        else:\n            raise ValueError(f""Invalid spatial_type: {spatial_type}"")\n\n        model = Model.create(tag, **pars)\n        for name, value in errs.items():\n            model.parameters[name].error = value\n        return model\n\n    def sky_model(self, which=""best""):\n        """"""Source sky model.\n\n        Parameters\n        ----------\n        which : {\'best\', \'pl\', \'ecpl\'}\n            Which spectral model\n\n        Returns\n        -------\n        sky_model : `~gammapy.modeling.models.SkyModel`\n            Sky model of the catalog object.\n        """"""\n        spatial_type = self.data[""Spatial_Model""]\n        if spatial_type in {""2-Gaussian"", ""3-Gaussian""}:\n            models = []\n            for component in self.components:\n                spectral_model = self.spectral_model(which=which)\n                weight = component.data[""Flux_Map""] / self.data[""Flux_Map""]\n                spectral_model.parameters[""amplitude""].value *= weight\n                model = SkyModel(\n                    spatial_model=component.spatial_model(),\n                    spectral_model=spectral_model,\n                    name=component.name,\n                )\n                models.append(model)\n\n            return Models(models)\n        else:\n            return SkyModel(\n                spatial_model=self.spatial_model(),\n                spectral_model=self.spectral_model(which=which),\n                name=self.name,\n            )\n\n    @property\n    def flux_points(self):\n        """"""Flux points (`~gammapy.spectrum.FluxPoints`).""""""\n        table = Table()\n        table.meta[""SED_TYPE""] = ""dnde""\n        mask = ~np.isnan(self.data[""Flux_Points_Energy""])\n\n        table[""e_ref""] = self.data[""Flux_Points_Energy""][mask]\n        table[""e_min""] = self.data[""Flux_Points_Energy_Min""][mask]\n        table[""e_max""] = self.data[""Flux_Points_Energy_Max""][mask]\n\n        table[""dnde""] = self.data[""Flux_Points_Flux""][mask]\n        table[""dnde_errn""] = self.data[""Flux_Points_Flux_Err_Lo""][mask]\n        table[""dnde_errp""] = self.data[""Flux_Points_Flux_Err_Hi""][mask]\n        table[""dnde_ul""] = self.data[""Flux_Points_Flux_UL""][mask]\n        table[""is_ul""] = self.data[""Flux_Points_Flux_Is_UL""][mask].astype(""bool"")\n\n        return FluxPoints(table)\n\n\nclass SourceCatalogHGPS(SourceCatalog):\n    """"""HESS Galactic plane survey (HGPS) source catalog.\n\n    Reference: https://www.mpi-hd.mpg.de/hfm/HESS/hgps/\n\n    One source is represented by `SourceCatalogObjectHGPS`.\n\n    Examples\n    --------\n    Let\'s assume you have downloaded the HGPS catalog FITS file, e.g. via:\n\n    .. code-block:: bash\n\n        curl -O https://www.mpi-hd.mpg.de/hfm/HESS/hgps/data/hgps_catalog_v1.fits.gz\n\n    Then you can load it up like this:\n\n    >>> from gammapy.catalog import SourceCatalogHGPS\n    >>> filename = \'hgps_catalog_v1.fits.gz\'\n    >>> cat = SourceCatalogHGPS(filename)\n\n    Access a source by name:\n\n    >>> source = cat[\'HESS J1843-033\']\n    >>> print(source)\n\n    Access source spectral data and plot it:\n\n    >>> source.spectral_model().plot(source.energy_range)\n    >>> source.spectral_model().plot_error(source.energy_range)\n    >>> source.flux_points.plot()\n\n    Gaussian component information can be accessed as well,\n    either via the source, or via the catalog:\n\n    >>> source.components\n    >>> cat.gaussian_component(83)\n    """"""\n\n    tag = ""hgps""\n    """"""Source catalog name (str).""""""\n\n    description = ""H.E.S.S. Galactic plane survey (HGPS) source catalog""\n    """"""Source catalog description (str).""""""\n\n    source_object_class = SourceCatalogObjectHGPS\n\n    def __init__(\n        self,\n        filename=""$GAMMAPY_DATA/catalogs/hgps_catalog_v1.fits.gz"",\n        hdu=""HGPS_SOURCES"",\n    ):\n        filename = make_path(filename)\n        table = Table.read(filename, hdu=hdu)\n\n        source_name_alias = (""Identified_Object"",)\n        super().__init__(table=table, source_name_alias=source_name_alias)\n\n        self._table_components = Table.read(filename, hdu=""HGPS_GAUSS_COMPONENTS"")\n        self._table_associations = Table.read(filename, hdu=""HGPS_ASSOCIATIONS"")\n        self._table_associations[""Separation""].format = "".6f""\n        self._table_identifications = Table.read(filename, hdu=""HGPS_IDENTIFICATIONS"")\n        self._table_large_scale_component = Table.read(\n            filename, hdu=""HGPS_LARGE_SCALE_COMPONENT""\n        )\n\n    @property\n    def table_components(self):\n        """"""Gaussian component table (`~astropy.table.Table`)""""""\n        return self._table_components\n\n    @property\n    def table_associations(self):\n        """"""Source association table (`~astropy.table.Table`)""""""\n        return self._table_associations\n\n    @property\n    def table_identifications(self):\n        """"""Source identification table (`~astropy.table.Table`)""""""\n        return self._table_identifications\n\n    @property\n    def table_large_scale_component(self):\n        """"""Large scale component table (`~astropy.table.Table`)""""""\n        return self._table_large_scale_component\n\n    @property\n    def large_scale_component(self):\n        """"""Large scale component model (`~gammapy.catalog.SourceCatalogLargeScaleHGPS`).""""""\n        return SourceCatalogLargeScaleHGPS(self.table_large_scale_component)\n\n    def _make_source_object(self, index):\n        """"""Make `SourceCatalogObject` for given row index""""""\n        source = super()._make_source_object(index)\n\n        if source.data[""Components""] != """":\n            source.components = list(self._get_gaussian_components(source))\n\n        self._attach_association_info(source)\n\n        if source.data[""Source_Class""] != ""Unid"":\n            self._attach_identification_info(source)\n\n        return source\n\n    def _get_gaussian_components(self, source):\n        for name in source.data[""Components""].split("", ""):\n            row_index = int(name.split()[-1]) - 1\n            yield self.gaussian_component(row_index)\n\n    def _attach_association_info(self, source):\n        t = self.table_associations\n        mask = source.data[""Source_Name""] == t[""Source_Name""]\n        source.associations = t[mask]\n\n    def _attach_identification_info(self, source):\n        t = self._table_identifications\n        idx = np.nonzero(source.name == t[""Source_Name""])[0][0]\n        source.identification_info = table_row_to_dict(t[idx])\n\n    def gaussian_component(self, row_idx):\n        """"""Gaussian component (`SourceCatalogObjectHGPSComponent`).""""""\n        data = table_row_to_dict(self.table_components[row_idx])\n        data[SourceCatalogObject._row_index_key] = row_idx\n        return SourceCatalogObjectHGPSComponent(data=data)\n\n\nclass SourceCatalogLargeScaleHGPS:\n    """"""Gaussian band model.\n\n    This 2-dimensional model is Gaussian in ``y`` for a given ``x``,\n    and the Gaussian parameters can vary in ``x``.\n\n    One application of this model is the diffuse emission along the\n    Galactic plane, i.e. ``x = GLON`` and ``y = GLAT``.\n\n    Parameters\n    ----------\n    table : `~astropy.table.Table`\n        Table of Gaussian parameters.\n        ``x``, ``amplitude``, ``mean``, ``stddev``.\n    interp_kwargs : dict\n        Keyword arguments passed to `ScaledRegularGridInterpolator`\n    """"""\n\n    def __init__(self, table, interp_kwargs=None):\n        interp_kwargs = interp_kwargs or {}\n        interp_kwargs.setdefault(""values_scale"", ""lin"")\n\n        self.table = table\n        glon = Angle(self.table[""GLON""]).wrap_at(""180d"")\n\n        interps = {}\n\n        for column in table.colnames:\n            values = self.table[column].quantity\n            interp = ScaledRegularGridInterpolator((glon,), values, **interp_kwargs)\n            interps[column] = interp\n\n        self._interp = interps\n\n    def _interpolate_parameter(self, parname, glon):\n        glon = glon.wrap_at(""180d"")\n        return self._interp[parname]((np.asanyarray(glon),), clip=False)\n\n    def peak_brightness(self, glon):\n        """"""Peak brightness at a given longitude.\n\n        Parameters\n        ----------\n        glon : `~astropy.coordinates.Angle`\n            Galactic Longitude.\n        """"""\n        return self._interpolate_parameter(""Surface_Brightness"", glon)\n\n    def peak_brightness_error(self, glon):\n        """"""Peak brightness error at a given longitude.\n\n        Parameters\n        ----------\n        glon : `~astropy.coordinates.Angle`\n            Galactic Longitude.\n        """"""\n        return self._interpolate_parameter(""Surface_Brightness_Err"", glon)\n\n    def width(self, glon):\n        """"""Width at a given longitude.\n\n        Parameters\n        ----------\n        glon : `~astropy.coordinates.Angle`\n            Galactic Longitude.\n        """"""\n        return self._interpolate_parameter(""Width"", glon)\n\n    def width_error(self, glon):\n        """"""Width error at a given longitude.\n\n        Parameters\n        ----------\n        glon : `~astropy.coordinates.Angle`\n            Galactic Longitude.\n        """"""\n        return self._interpolate_parameter(""Width_Err"", glon)\n\n    def peak_latitude(self, glon):\n        """"""Peak position at a given longitude.\n\n        Parameters\n        ----------\n        glon : `~astropy.coordinates.Angle`\n            Galactic Longitude.\n        """"""\n        return self._interpolate_parameter(""GLAT"", glon)\n\n    def peak_latitude_error(self, glon):\n        """"""Peak position error at a given longitude.\n\n        Parameters\n        ----------\n        glon : `~astropy.coordinates.Angle`\n            Galactic Longitude.\n        """"""\n        return self._interpolate_parameter(""GLAT_Err"", glon)\n\n    def evaluate(self, position):\n        """"""Evaluate model at a given position.\n\n        Parameters\n        ----------\n        position : `~astropy.coordinates.SkyCoord`\n            Position on the sky.\n        """"""\n        glon, glat = position.galactic.l, position.galactic.b\n        width = self.width(glon)\n        amplitude = self.peak_brightness(glon)\n        mean = self.peak_latitude(glon)\n        return Gaussian1D.evaluate(glat, amplitude=amplitude, mean=mean, stddev=width)\n'"
gammapy/data/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Data and observation handling.""""""\nfrom .data_store import *\nfrom .event_list import *\nfrom .filters import *\nfrom .gti import *\nfrom .hdu_index_table import *\nfrom .obs_table import *\nfrom .observations import *\nfrom .observers import *\nfrom .pointing import *\n'"
gammapy/data/data_store.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport subprocess\nfrom pathlib import Path\nfrom astropy.coordinates import SkyCoord\nfrom astropy.io import fits\nfrom gammapy.utils.scripts import make_path\nfrom gammapy.utils.table import table_row_to_dict\nfrom gammapy.utils.testing import Checker\nfrom .hdu_index_table import HDUIndexTable\nfrom .obs_table import ObservationTable, ObservationTableChecker\nfrom .observations import Observation, ObservationChecker, Observations\n\n__all__ = [""DataStore""]\n\nlog = logging.getLogger(__name__)\n\n\nclass DataStore:\n    """"""IACT data store.\n\n    The data selection and access happens using an observation\n    and an HDU index file as described at :ref:`gadf:iact-storage`.\n\n    For a usage example see `cta.html <../notebooks/cta.html>`__\n\n    Parameters\n    ----------\n    hdu_table : `~gammapy.data.HDUIndexTable`\n        HDU index table\n    obs_table : `~gammapy.data.ObservationTable`\n        Observation index table\n\n    Examples\n    --------\n    Here\'s an example how to create a `DataStore` to access H.E.S.S. data:\n\n    >>> from gammapy.data import DataStore\n    >>> data_store = DataStore.from_dir(\'$GAMMAPY_DATA/hess-dl3-dr1\')\n    >>> data_store.info()\n    """"""\n\n    DEFAULT_HDU_TABLE = ""hdu-index.fits.gz""\n    """"""Default HDU table filename.""""""\n\n    DEFAULT_OBS_TABLE = ""obs-index.fits.gz""\n    """"""Default observation table filename.""""""\n\n    def __init__(self, hdu_table=None, obs_table=None):\n        self.hdu_table = hdu_table\n        self.obs_table = obs_table\n\n    def __str__(self):\n        return self.info(show=False)\n\n    @classmethod\n    def from_file(cls, filename, hdu_hdu=""HDU_INDEX"", hdu_obs=""OBS_INDEX""):\n        """"""Create from a FITS file.\n\n        The FITS file must contain both index files.\n\n        Parameters\n        ----------\n        filename : str, Path\n            FITS filename\n        hdu_hdu : str or int\n            FITS HDU name or number for the HDU index table\n        hdu_obs : str or int\n            FITS HDU name or number for the observation index table\n        """"""\n        filename = make_path(filename)\n\n        hdu_table = HDUIndexTable.read(filename, hdu=hdu_hdu, format=""fits"")\n\n        obs_table = ObservationTable.read(filename, hdu=hdu_obs, format=""fits"")\n\n        return cls(hdu_table=hdu_table, obs_table=obs_table)\n\n    @classmethod\n    def from_dir(cls, base_dir, hdu_table_filename=None, obs_table_filename=None):\n        """"""Create from a directory.\n\n        Parameters\n        ----------\n        base_dir : str, Path\n            Base directory of the data files.\n        hdu_table_filename : str, Path\n            Filename of the HDU index file. May be specified either relative\n            to `base_dir` or as an absolute path. If None, the default filename\n            will be looked for.\n        obs_table_filename : str, Path\n            Filename of the observation index file. May be specified either relative\n            to `base_dir` or as an absolute path. If None, the default filename\n            will be looked for.\n        """"""\n        base_dir = make_path(base_dir)\n\n        if hdu_table_filename:\n            hdu_table_filename = make_path(hdu_table_filename)\n            if (base_dir / hdu_table_filename).exists():\n                hdu_table_filename = base_dir / hdu_table_filename\n        else:\n            hdu_table_filename = base_dir / cls.DEFAULT_HDU_TABLE\n\n        if obs_table_filename:\n            obs_table_filename = make_path(obs_table_filename)\n            if (base_dir / obs_table_filename).exists():\n                obs_table_filename = base_dir / obs_table_filename\n        else:\n            obs_table_filename = base_dir / cls.DEFAULT_OBS_TABLE\n\n        if not hdu_table_filename.exists():\n            raise OSError(f""File not found: {hdu_table_filename}"")\n        log.debug(f""Reading {hdu_table_filename}"")\n        hdu_table = HDUIndexTable.read(hdu_table_filename, format=""fits"")\n        hdu_table.meta[""BASE_DIR""] = str(base_dir)\n\n        if not obs_table_filename.exists():\n            raise OSError(f""File not found: {obs_table_filename}"")\n        log.debug(f""Reading {obs_table_filename}"")\n        obs_table = ObservationTable.read(obs_table_filename, format=""fits"")\n\n        return cls(hdu_table=hdu_table, obs_table=obs_table)\n\n    @classmethod\n    def from_events_files(cls, paths):\n        """"""Create from a list of event filenames.\n\n        HDU and observation index tables will be created from the EVENTS header.\n\n        IRFs are found only if you have a ``CALDB`` environment variable set,\n        and if the EVENTS files contain the following keys:\n\n        - ``TELESCOP`` (example: ``TELESCOP = CTA``)\n        - ``CALDB`` (example: ``CALDB = 1dc``)\n        - ``IRF`` (example: ``IRF = South_z20_50h``)\n\n        This method is useful specifically if you want to load data simulated\n        with `ctobssim`_\n\n        .. _ctobssim: http://cta.irap.omp.eu/ctools/users/reference_manual/ctobssim.html\n\n        Examples\n        --------\n        This is how you can access a single event list::\n\n            from gammapy.data import DataStore\n            path = ""$GAMMAPY_DATA/cta-1dc/data/baseline/gps/gps_baseline_110380.fits""\n            data_store = DataStore.from_events_files([path])\n            observations = data_store.get_observations()\n\n        You can now analyse this data as usual (see any Gammapy tutorial).\n\n        If you have multiple event files, you have to make the list. Here\'s an example\n        using ``Path.glob`` to get a list of all events files in a given folder::\n\n            import os\n            from pathlib import Path\n            path = Path(os.environ[""GAMMAPY_DATA""]) / ""cta-1dc/data""\n            paths = list(path.rglob(""*.fits""))\n            data_store = DataStore.from_events_files(paths)\n            observations = data_store.get_observations()\n\n        Note that you have a lot of flexibility to select the observations you want,\n        by having a few lines of custom code to prepare ``paths``, or to select a\n        subset via a method on the ``data_store`` or the ``observations`` objects.\n\n        If you want to generate HDU and observation index files, write the tables to disk::\n\n            data_store.hdu_table.write(""hdu-index.fits.gz"")\n            data_store.obs_table.write(""obs-index.fits.gz"")\n        """"""\n        return DataStoreMaker(paths).run()\n\n    def info(self, show=True):\n        """"""Print some info.""""""\n        s = ""Data store:\\n""\n        s += self.hdu_table.summary()\n        s += ""\\n\\n""\n        s += self.obs_table.summary()\n\n        if show:\n            print(s)\n        else:\n            return s\n\n    def obs(self, obs_id):\n        """"""Access a given `~gammapy.data.Observation`.\n\n        Parameters\n        ----------\n        obs_id : int\n            Observation ID.\n\n        Returns\n        -------\n        observation : `~gammapy.data.Observation`\n            Observation container\n        """"""\n        if obs_id not in self.obs_table[""OBS_ID""]:\n            raise ValueError(f""OBS_ID = {obs_id} not in obs index table."")\n\n        if obs_id not in self.hdu_table[""OBS_ID""]:\n            raise ValueError(f""OBS_ID = {obs_id} not in HDU index table."")\n\n        row = self.obs_table.select_obs_id(obs_id=obs_id)[0]\n        obs_info = table_row_to_dict(row)\n\n        aeff_hdu = self.hdu_table.hdu_location(obs_id=obs_id, hdu_type=""aeff"")\n        edisp_hdu = self.hdu_table.hdu_location(obs_id=obs_id, hdu_type=""edisp"")\n        bkg_hdu = self.hdu_table.hdu_location(obs_id=obs_id, hdu_type=""bkg"")\n        psf_hdu = self.hdu_table.hdu_location(obs_id=obs_id, hdu_type=""psf"")\n        events_hdu = self.hdu_table.hdu_location(obs_id=obs_id, hdu_type=""events"")\n        gti_hdu = self.hdu_table.hdu_location(obs_id=obs_id, hdu_type=""gti"")\n\n        return Observation(\n            obs_id=int(obs_id),\n            obs_info=obs_info,\n            bkg=bkg_hdu,\n            aeff=aeff_hdu,\n            edisp=edisp_hdu,\n            events=events_hdu,\n            gti=gti_hdu,\n            psf=psf_hdu,\n        )\n\n    def get_observations(self, obs_id=None, skip_missing=False):\n        """"""Generate a `~gammapy.data.Observations`.\n\n        Parameters\n        ----------\n        obs_id : list\n            Observation IDs (default of ``None`` means ""all"")\n        skip_missing : bool, optional\n            Skip missing observations, default: False\n\n        Returns\n        -------\n        observations : `~gammapy.data.Observations`\n            Container holding a list of `~gammapy.data.Observation`\n        """"""\n        if obs_id is None:\n            obs_id = self.obs_table[""OBS_ID""].data\n\n        obs_list = []\n        for _ in obs_id:\n            try:\n                obs = self.obs(_)\n            except ValueError as err:\n                if skip_missing:\n                    log.warning(f""Skipping missing obs_id: {_!r}"")\n                    continue\n                else:\n                    raise err\n            else:\n                obs_list.append(obs)\n        return Observations(obs_list)\n\n    def copy_obs(self, obs_id, outdir, hdu_class=None, verbose=False, overwrite=False):\n        """"""Create a new `~gammapy.data.DataStore` containing a subset of observations.\n\n        Parameters\n        ----------\n        obs_id : array-like, `~gammapy.data.ObservationTable`\n            List of observations to copy\n        outdir : str, Path\n            Directory for the new store\n        hdu_class : list of str\n            see :attr:`gammapy.data.HDUIndexTable.VALID_HDU_CLASS`\n        verbose : bool\n            Print copied files\n        overwrite : bool\n            Overwrite\n        """"""\n        outdir = make_path(outdir)\n\n        if not outdir.is_dir():\n            raise OSError(f""Not a directory: outdir={outdir}"")\n\n        if isinstance(obs_id, ObservationTable):\n            obs_id = obs_id[""OBS_ID""].data\n\n        hdutable = self.hdu_table\n        hdutable.add_index(""OBS_ID"")\n        with hdutable.index_mode(""discard_on_copy""):\n            subhdutable = hdutable.loc[obs_id]\n        if hdu_class is not None:\n            subhdutable.add_index(""HDU_CLASS"")\n            with subhdutable.index_mode(""discard_on_copy""):\n                subhdutable = subhdutable.loc[hdu_class]\n        subobstable = self.obs_table.select_obs_id(obs_id)\n\n        for idx in range(len(subhdutable)):\n            # Changes to the file structure could be made here\n            loc = subhdutable.location_info(idx)\n            targetdir = outdir / loc.file_dir\n            targetdir.mkdir(exist_ok=True, parents=True)\n            cmd = [""cp""]\n            if verbose:\n                cmd += [""-v""]\n            if not overwrite:\n                cmd += [""-n""]\n            cmd += [str(loc.path()), str(targetdir)]\n            subprocess.run(cmd)\n\n        filename = outdir / self.DEFAULT_HDU_TABLE\n        subhdutable.write(filename, format=""fits"", overwrite=overwrite)\n\n        filename = outdir / self.DEFAULT_OBS_TABLE\n        subobstable.write(str(filename), format=""fits"", overwrite=overwrite)\n\n    def check(self, checks=""all""):\n        """"""Check index tables and data files.\n\n        This is a generator that yields a list of dicts.\n        """"""\n        checker = DataStoreChecker(self)\n        return checker.run(checks=checks)\n\n\nclass DataStoreChecker(Checker):\n    """"""Check data store.\n\n    Checks data format and a bit about the content.\n    """"""\n\n    CHECKS = {\n        ""obs_table"": ""check_obs_table"",\n        ""hdu_table"": ""check_hdu_table"",\n        ""observations"": ""check_observations"",\n        ""consistency"": ""check_consistency"",\n    }\n\n    def __init__(self, data_store):\n        self.data_store = data_store\n\n    def check_obs_table(self):\n        """"""Checks for the observation index table.""""""\n        yield from ObservationTableChecker(self.data_store.obs_table).run()\n\n    def check_hdu_table(self):\n        """"""Checks for the HDU index table.""""""\n        t = self.data_store.hdu_table\n        m = t.meta\n        if m.get(""HDUCLAS1"", """") != ""INDEX"":\n            yield {\n                ""level"": ""error"",\n                ""hdu"": ""hdu-index"",\n                ""msg"": ""Invalid header key. Must have HDUCLAS1=INDEX"",\n            }\n        if m.get(""HDUCLAS2"", """") != ""HDU"":\n            yield {\n                ""level"": ""error"",\n                ""hdu"": ""hdu-index"",\n                ""msg"": ""Invalid header key. Must have HDUCLAS2=HDU"",\n            }\n\n        # Check that all HDU in the data files exist\n        for idx in range(len(t)):\n            location_info = t.location_info(idx)\n            try:\n                location_info.get_hdu()\n            except KeyError:\n                yield {\n                    ""level"": ""error"",\n                    ""msg"": f""HDU not found: {location_info.__dict__!r}"",\n                }\n\n    def check_consistency(self):\n        """"""Check consistency between multiple HDUs.""""""\n        # obs and HDU index should have the same OBS_ID\n        obs_table_obs_id = set(self.data_store.obs_table[""OBS_ID""])\n        hdu_table_obs_id = set(self.data_store.hdu_table[""OBS_ID""])\n        if not obs_table_obs_id == hdu_table_obs_id:\n            yield {\n                ""level"": ""error"",\n                ""msg"": ""Inconsistent OBS_ID in obs and HDU index tables"",\n            }\n\n        # TODO: obs table and events header should have the same times\n\n    def check_observations(self):\n        """"""Perform some sanity checks for all observations.""""""\n        for obs_id in self.data_store.obs_table[""OBS_ID""]:\n            obs = self.data_store.obs(obs_id)\n            yield from ObservationChecker(obs).run()\n\n\nclass DataStoreMaker:\n    """"""Create data store index tables.\n\n    This is a multi-step process coded as a class.\n    Users will usually call this via `DataStore.from_events_files`.\n    """"""\n\n    def __init__(self, paths):\n        if isinstance(paths, (str, Path)):\n            raise TypeError(""Need list of paths, not a single string or Path object."")\n\n        self.paths = [make_path(path) for path in paths]\n\n        # Cache for EVENTS file header information, to avoid multiple reads\n        self._events_info = {}\n\n    def run(self):\n        hdu_table = self.make_hdu_table()\n        obs_table = self.make_obs_table()\n        return DataStore(hdu_table=hdu_table, obs_table=obs_table)\n\n    def get_events_info(self, path):\n        if path not in self._events_info:\n            self._events_info[path] = self.read_events_info(path)\n\n        return self._events_info[path]\n\n    def get_obs_info(self, path):\n        # We could add or remove info here depending on what we want in the obs table\n        return self.get_events_info(path)\n\n    @staticmethod\n    def read_events_info(path):\n        log.debug(f""Reading {path}"")\n        with fits.open(path, memmap=False) as hdu_list:\n            header = hdu_list[""EVENTS""].header\n\n        na_int, na_str = -1, ""NOT AVAILABLE""\n\n        info = {}\n        # Note: for some reason `header[""OBS_ID""]` is sometimes `str`, maybe trailing whitespace\n        info[""OBS_ID""] = int(header[""OBS_ID""])\n        info[""RA_PNT""] = header[""RA_PNT""]\n        info[""DEC_PNT""] = header[""DEC_PNT""]\n        pos = SkyCoord(info[""RA_PNT""], info[""DEC_PNT""], unit=""deg"").galactic\n        info[""GLON_PNT""] = pos.l.deg\n        info[""GLAT_PNT""] = pos.b.deg\n        info[""ZEN_PNT""] = 90 - float(header[""ALT_PNT""])\n        info[""ALT_PNT""] = header[""ALT_PNT""]\n        info[""AZ_PNT""] = header[""AZ_PNT""]\n        info[""ONTIME""] = header[""ONTIME""]\n        info[""LIVETIME""] = header[""LIVETIME""]\n        info[""DEADC""] = header[""DEADC""]\n        info[""TSTART""] = header[""TSTART""]\n        info[""TSTOP""] = header[""TSTOP""]\n        info[""DATE-OBS""] = header.get(""DATE_OBS"", na_str)\n        info[""TIME-OBS""] = header.get(""TIME_OBS"", na_str)\n        info[""DATE-END""] = header.get(""DATE_END"", na_str)\n        info[""TIME-END""] = header.get(""TIME_END"", na_str)\n        info[""N_TELS""] = header.get(""N_TELS"", na_int)\n        info[""OBJECT""] = header.get(""OBJECT"", na_str)\n\n        # This is the info needed to link from EVENTS to IRFs\n        info[""TELESCOP""] = header.get(""TELESCOP"", na_str)\n        info[""CALDB""] = header.get(""CALDB"", na_str)\n        info[""IRF""] = header.get(""IRF"", na_str)\n\n        # Not part of the spec, but good to know from which file the info comes\n        info[""EVENTS_FILENAME""] = str(path)\n        info[""EVENT_COUNT""] = header[""NAXIS2""]\n\n        # gti = Table.read(filename, hdu=\'GTI\')\n        # info[\'GTI_START\'] = gti[\'START\'][0]\n        # info[\'GTI_STOP\'] = gti[\'STOP\'][0]\n\n        return info\n\n    def make_obs_table(self):\n        rows = []\n        for path in self.paths:\n            row = self.get_obs_info(path)\n            rows.append(row)\n\n        names = list(rows[0].keys())\n        table = ObservationTable(rows=rows, names=names)\n\n        table[""RA_PNT""].unit = ""deg""\n        table[""DEC_PNT""].unit = ""deg""\n        table[""GLON_PNT""].unit = ""deg""\n        table[""GLAT_PNT""].unit = ""deg""\n        table[""ZEN_PNT""].unit = ""deg""\n        table[""ALT_PNT""].unit = ""deg""\n        table[""AZ_PNT""].unit = ""deg""\n        table[""ONTIME""].unit = ""s""\n        table[""LIVETIME""].unit = ""s""\n        table[""TSTART""].unit = ""s""\n        table[""TSTOP""].unit = ""s""\n\n        # TODO: Values copied from one of the EVENTS headers\n        # TODO: check consistency for all EVENTS files and handle inconsistent case\n        # Transform times to first ref time? Or raise error for now?\n        # Test by combining some HESS & CTA runs?\n        m = table.meta\n        m[""MJDREFI""] = 51544\n        m[""MJDREFF""] = 5.0000000000e-01\n        m[""TIMEUNIT""] = ""s""\n        m[""TIMESYS""] = ""TT""\n        m[""TIMEREF""] = ""LOCAL""\n\n        m[""HDUCLASS""] = ""GADF""\n        m[""HDUDOC""] = ""https://github.com/open-gamma-ray-astro/gamma-astro-data-formats""\n        m[""HDUVERS""] = ""0.2""\n        m[""HDUCLAS1""] = ""INDEX""\n        m[""HDUCLAS2""] = ""OBS""\n\n        return table\n\n    def make_hdu_table(self):\n        rows = []\n        for path in self.paths:\n            rows.extend(self.get_hdu_table_rows(path))\n\n        names = list(rows[0].keys())\n        # names = [\'OBS_ID\', \'HDU_TYPE\', \'HDU_CLASS\', \'FILE_DIR\', \'FILE_NAME\', \'HDU_NAME\']\n\n        table = HDUIndexTable(rows=rows, names=names)\n\n        m = table.meta\n        m[""HDUCLASS""] = ""GADF""\n        m[""HDUDOC""] = ""https://github.com/open-gamma-ray-astro/gamma-astro-data-formats""\n        m[""HDUVERS""] = ""0.2""\n        m[""HDUCLAS1""] = ""INDEX""\n        m[""HDUCLAS2""] = ""HDU""\n\n        return table\n\n    def get_hdu_table_rows(self, path):\n        events_info = self.get_events_info(path)\n\n        info = dict(\n            OBS_ID=events_info[""OBS_ID""],\n            FILE_DIR=path.parent.as_posix(),\n            FILE_NAME=path.name,\n        )\n        yield dict(HDU_TYPE=""events"", HDU_CLASS=""events"", HDU_NAME=""EVENTS"", **info)\n        yield dict(HDU_TYPE=""gti"", HDU_CLASS=""gti"", HDU_NAME=""GTI"", **info)\n\n        caldb_irf = CalDBIRF.from_meta(events_info)\n        info = dict(\n            OBS_ID=events_info[""OBS_ID""],\n            FILE_DIR=caldb_irf.file_dir,\n            FILE_NAME=caldb_irf.file_name,\n        )\n        yield dict(\n            HDU_TYPE=""aeff"", HDU_CLASS=""aeff_2d"", HDU_NAME=""EFFECTIVE AREA"", **info\n        )\n        yield dict(\n            HDU_TYPE=""edisp"", HDU_CLASS=""edisp_2d"", HDU_NAME=""ENERGY DISPERSION"", **info\n        )\n        yield dict(\n            HDU_TYPE=""psf"",\n            HDU_CLASS=""psf_3gauss"",\n            HDU_NAME=""POINT SPREAD FUNCTION"",\n            **info,\n        )\n        yield dict(HDU_TYPE=""bkg"", HDU_CLASS=""bkg_3d"", HDU_NAME=""BACKGROUND"", **info)\n\n\n# TODO: load IRF file, and infer HDU_CLASS from IRF file contents!\nclass CalDBIRF:\n    """"""Helper class to work with IRFs in CALDB format.""""""\n\n    def __init__(self, telescop, caldb, irf):\n        self.telescop = telescop\n        self.caldb = caldb\n        self.irf = irf\n\n    @classmethod\n    def from_meta(cls, meta):\n        return cls(telescop=meta[""TELESCOP""], caldb=meta[""CALDB""], irf=meta[""IRF""])\n\n    @property\n    def file_dir(self):\n        # In CTA 1DC the header key is ""CTA"", but the directory is lower-case ""cta""\n        telescop = self.telescop.lower()\n        return f""$CALDB/data/{telescop}/{self.caldb}/bcf/{self.irf}""\n\n    @property\n    def file_name(self):\n        return ""irf_file.fits""\n'"
gammapy/data/event_list.py,14,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport collections\nimport logging\nimport numpy as np\nfrom astropy.coordinates import AltAz, Angle, SkyCoord\nfrom astropy.coordinates.angle_utilities import angular_separation\nfrom astropy.table import Table\nfrom astropy.table import vstack as vstack_tables\nfrom astropy.units import Quantity, Unit\nfrom gammapy.maps import MapAxis, MapCoord, WcsNDMap\nfrom gammapy.utils.fits import earth_location_from_dict\nfrom gammapy.utils.regions import make_region\nfrom gammapy.utils.scripts import make_path\nfrom gammapy.utils.testing import Checker\nfrom gammapy.utils.time import time_ref_from_dict\n\n__all__ = [""EventListBase"", ""EventList"", ""EventListLAT""]\n\nlog = logging.getLogger(__name__)\n\n\nclass EventListBase:\n    """"""Event list.\n\n    This class represents the base for two different event lists:\n    - EventList: targeted for IACT event lists\n    - EventListLAT: targeted for Fermi-LAT event lists\n\n    Event list data is stored in ``table`` (`~astropy.table.Table`) data member.\n\n    The most important reconstructed event parameters\n    are available as the following columns:\n\n    - ``TIME`` - Mission elapsed time (sec)\n    - ``RA``, ``DEC`` - ICRS system position (deg)\n    - ``ENERGY`` - Energy (usually MeV for Fermi and TeV for IACTs)\n\n    Note that ``TIME`` is usually sorted, but sometimes it is not.\n    E.g. when simulating data, or processing it in certain ways.\n    So generally any analysis code should assume ``TIME`` is not sorted.\n\n    Other optional (columns) that are sometimes useful for high-level analysis:\n\n    - ``GLON``, ``GLAT`` - Galactic coordinates (deg)\n    - ``DETX``, ``DETY`` - Field of view coordinates (deg)\n\n    Note that when reading data for analysis you shouldn\'t use those\n    values directly, but access them via properties which create objects\n    of the appropriate class:\n\n    - `time` for ``TIME``\n    - `radec` for ``RA``, ``DEC``\n    - `energy` for ``ENERGY``\n    - `galactic` for ``GLON``, ``GLAT``\n\n    Parameters\n    ----------\n    table : `~astropy.table.Table`\n        Event list table\n    """"""\n\n    def __init__(self, table):\n        self.table = table\n\n    @classmethod\n    def read(cls, filename, **kwargs):\n        """"""Read from FITS file.\n\n        Format specification: :ref:`gadf:iact-events`\n\n        Parameters\n        ----------\n        filename : `pathlib.Path`, str\n            Filename\n        """"""\n        filename = make_path(filename)\n        kwargs.setdefault(""hdu"", ""EVENTS"")\n        table = Table.read(filename, **kwargs)\n        return cls(table=table)\n\n    @classmethod\n    def stack(cls, event_lists, **kwargs):\n        """"""Stack (concatenate) list of event lists.\n\n        Calls `~astropy.table.vstack`.\n\n        Parameters\n        ----------\n        event_lists : list\n            list of `~gammapy.data.EventList` to stack\n        """"""\n        tables = [_.table for _ in event_lists]\n        stacked_table = vstack_tables(tables, **kwargs)\n        return cls(stacked_table)\n\n    def __str__(self):\n        ss = (\n            ""EventList info:\\n""\n            f""- Number of events: {len(self.table)}\\n""\n            f""- Median energy: {np.median(self.energy.value):.3g} {self.energy.unit}\\n""\n        )\n\n        if ""OBS_ID"" in self.table.meta:\n            ss += ""- OBS_ID = {}"".format(self.table.meta[""OBS_ID""])\n\n        # TODO: add time, RA, DEC and if present GLON, GLAT info ...\n\n        if ""AZ"" in self.table.colnames:\n            # TODO: azimuth should be circular median\n            ss += ""- Median azimuth: {}\\n"".format(np.median(self.table[""AZ""]))\n\n        if ""ALT"" in self.table.colnames:\n            ss += ""- Median altitude: {}\\n"".format(np.median(self.table[""ALT""]))\n\n        return ss\n\n    @property\n    def time_ref(self):\n        """"""Time reference (`~astropy.time.Time`).""""""\n        return time_ref_from_dict(self.table.meta)\n\n    @property\n    def time(self):\n        """"""Event times (`~astropy.time.Time`).\n\n        Notes\n        -----\n        Times are automatically converted to 64-bit floats.\n        With 32-bit floats times will be incorrect by a few seconds\n        when e.g. adding them to the reference time.\n        """"""\n        met = Quantity(self.table[""TIME""].astype(""float64""), ""second"")\n        return self.time_ref + met\n\n    @property\n    def observation_time_start(self):\n        """"""Observation start time (`~astropy.time.Time`).""""""\n        return self.time_ref + Quantity(self.table.meta[""TSTART""], ""second"")\n\n    @property\n    def observation_time_end(self):\n        """"""Observation stop time (`~astropy.time.Time`).""""""\n        return self.time_ref + Quantity(self.table.meta[""TSTOP""], ""second"")\n\n    @property\n    def radec(self):\n        """"""Event RA / DEC sky coordinates (`~astropy.coordinates.SkyCoord`).""""""\n        lon, lat = self.table[""RA""], self.table[""DEC""]\n        return SkyCoord(lon, lat, unit=""deg"", frame=""icrs"")\n\n    @property\n    def galactic(self):\n        """"""Event Galactic sky coordinates (`~astropy.coordinates.SkyCoord`).\n\n        Always computed from RA / DEC using Astropy.\n        """"""\n        return self.radec.galactic\n\n    @property\n    def energy(self):\n        """"""Event energies (`~astropy.units.Quantity`).""""""\n        return self.table[""ENERGY""].quantity\n\n    def select_row_subset(self, row_specifier):\n        """"""Select table row subset.\n\n        Parameters\n        ----------\n        row_specifier : slice, int, or array of ints\n            Specification for rows to select,\n            passed on to ``self.table[row_specifier]``.\n\n        Returns\n        -------\n        event_list : `EventList`\n            New event list with table row subset selected\n\n        Examples\n        --------\n        Use a boolean mask as ``row_specifier``:\n\n            mask = events.table[\'FOO\'] > 42\n            events2 = events.select_row_subset(mask)\n\n        Use row index array as ``row_specifier``:\n\n            idx = np.where(events.table[\'FOO\'] > 42)[0]\n            events2 = events.select_row_subset(idx)\n        """"""\n        table = self.table[row_specifier]\n        return self.__class__(table=table)\n\n    def select_energy(self, energy_band):\n        """"""Select events in energy band.\n\n        Parameters\n        ----------\n        energy_band : `~astropy.units.Quantity`\n            Energy band ``[energy_min, energy_max)``\n\n        Returns\n        -------\n        event_list : `EventList`\n            Copy of event list with selection applied.\n\n        Examples\n        --------\n        >>> from astropy.units import Quantity\n        >>> from gammapy.data import EventList\n        >>> event_list = EventList.read(\'events.fits\')\n        >>> energy_band = Quantity([1, 20], \'TeV\')\n        >>> event_list = event_list.select_energy()\n        """"""\n        energy = self.energy\n        mask = energy_band[0] <= energy\n        mask &= energy < energy_band[1]\n        return self.select_row_subset(mask)\n\n    def select_time(self, time_interval):\n        """"""Select events in time interval.\n\n        Parameters\n        ----------\n        time_interval : `astropy.time.Time`\n            Start time (inclusive) and stop time (exclusive) for the selection.\n\n        Returns\n        -------\n        events : `EventList`\n            Copy of event list with selection applied.\n        """"""\n        time = self.time\n        mask = time_interval[0] <= time\n        mask &= time < time_interval[1]\n        return self.select_row_subset(mask)\n\n    def select_region(self, region, wcs=None):\n        """"""Select events in given region.\n\n        Parameters\n        ----------\n        region : `~regions.SkyRegion` or str\n            Sky region or string defining a sky region\n        wcs : `~astropy.wcs.WCS`\n            World coordinate system transformation\n\n        Returns\n        -------\n        event_list : `EventList`\n            Copy of event list with selection applied.\n        """"""\n        region = make_region(region)\n        mask = region.contains(self.radec, wcs)\n        return self.select_row_subset(mask)\n\n    def select_parameter(self, parameter, band):\n        """"""Select events with respect to a specified parameter.\n\n        Parameters\n        ----------\n        parameter : str\n            Parameter used for the selection. Must be present in `self.table`.\n        band : tuple or `astropy.units.Quantity`\n            Min and max value for the parameter to be selected (min <= parameter < max).\n            If parameter is not dimensionless you have to provide a Quantity.\n\n        Returns\n        -------\n        event_list : `EventList`\n            Copy of event list with selection applied.\n\n        Examples\n        --------\n        >>> from gammapy.data import EventList\n        >>> event_list = EventList.read(\'events.fits\')\n        >>> phase_region = (0.3, 0.5)\n        >>> event_list = event_list.select_parameter(parameter=\'PHASE\', band=phase_region)\n        """"""\n        mask = band[0] <= self.table[parameter].quantity\n        mask &= self.table[parameter].quantity < band[1]\n        return self.select_row_subset(mask)\n\n    def _default_plot_ebounds(self):\n        energy = self.energy\n        return MapAxis.from_energy_bounds(energy.min(), energy.max(), 50).edges\n\n    def plot_energy(self, ax=None, ebounds=None, **kwargs):\n        """"""Plot counts as a function of energy.""""""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        kwargs.setdefault(""log"", True)\n        kwargs.setdefault(""histtype"", ""step"")\n\n        if ebounds is None:\n            ebounds = self._default_plot_ebounds()\n\n        unit = ebounds.unit\n\n        ax.hist(self.energy.to_value(unit), bins=ebounds.value, **kwargs)\n        ax.loglog()\n        ax.set_xlabel(f""Energy ({unit})"")\n        ax.set_ylabel(""Counts"")\n        return ax\n\n    def plot_time(self, ax=None):\n        """"""Plots an event rate time curve.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes` or None\n            Axes\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axes\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        # Note the events are not necessarily in time order\n        time = self.table[""TIME""]\n        time = time - np.min(time)\n\n        ax.set_xlabel(""Time (sec)"")\n        ax.set_ylabel(""Counts"")\n        y, x_edges = np.histogram(time, bins=30)\n        # x = (x_edges[1:] + x_edges[:-1]) / 2\n        xerr = np.diff(x_edges) / 2\n        x = x_edges[:-1] + xerr\n        yerr = np.sqrt(y)\n\n        ax.errorbar(x=x, y=y, xerr=xerr, yerr=yerr, fmt=""none"")\n\n        return ax\n\n    def plot_offset2_distribution(self, ax=None, center=None, **kwargs):\n        """"""Plot offset^2 distribution of the events.\n\n        The distribution shown in this plot is for this quantity::\n\n            offset = center.separation(events.radec).deg\n            offset2 = offset ** 2\n\n        Note that this method is just for a quicklook plot.\n\n        If you want to do computations with the offset or offset^2 values, you can\n        use the line above. As an example, here\'s how to compute the 68% event\n        containment radius using `numpy.percentile`::\n\n            import numpy as np\n            r68 = np.percentile(offset, q=68)\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes` (optional)\n            Axes\n        center : `astropy.coordinates.SkyCoord`\n            Center position for the offset^2 distribution.\n            Default is the observation pointing position.\n        **kwargs :\n            Extra keyword arguments are passed to `matplotlib.pyplot.hist`.\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axes\n\n        Examples\n        --------\n        Load an example event list:\n\n        >>> from gammapy.data import EventList\n        >>> events = EventList.read(\'$GAMMAPY_DATA/hess-dl3-dr1/data/hess_dl3_dr1_obs_id_023523.fits.gz\')\n\n        Plot the offset^2 distribution wrt. the observation pointing position\n        (this is a commonly used plot to check the background spatial distribution):\n\n        >>> events.plot_offset2_distribution()\n\n        Plot the offset^2 distribution wrt. the Crab pulsar position\n        (this is commonly used to check both the gamma-ray signal and the background spatial distribution):\n\n        >>> import numpy as np\n        >>> from astropy.coordinates import SkyCoord\n        >>> center = SkyCoord(83.63307, 22.01449, unit=\'deg\')\n        >>> bins = np.linspace(start=0, stop=0.3 ** 2, num=30)\n        >>> events.plot_offset2_distribution(center=center, bins=bins)\n\n        Note how we passed the ``bins`` option of `matplotlib.pyplot.hist` to control the histogram binning,\n        in this case 30 bins ranging from 0 to (0.3 deg)^2.\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        if center is None:\n            center = self.pointing_radec\n\n        offset2 = center.separation(self.radec).deg ** 2\n\n        ax.hist(offset2, **kwargs)\n        ax.set_xlabel(""Offset^2 (deg^2)"")\n        ax.set_ylabel(""Counts"")\n\n        return ax\n\n    def plot_energy_offset(self, ax=None):\n        """"""Plot counts histogram with energy and offset axes.""""""\n        import matplotlib.pyplot as plt\n        from matplotlib.colors import LogNorm\n\n        ax = plt.gca() if ax is None else ax\n\n        energy_bounds = self._default_plot_ebounds().to_value(""TeV"")\n        offsetangle = self.pointing_radec.separation(self.radec).max().to(""deg"").value\n        offset_bounds = np.linspace(0, offsetangle, 30)\n\n        counts = np.histogram2d(\n            x=self.energy.value,\n            y=self.offset.value,\n            bins=(energy_bounds, offset_bounds),\n        )[0]\n\n        ax.pcolormesh(energy_bounds, offset_bounds, counts.T, norm=LogNorm())\n        ax.set_xscale(""log"")\n        ax.set_xlabel(f""Energy ({self.energy.unit})"")\n        ax.set_ylabel(f""Offset ({self.offset.unit})"")\n\n    def check(self, checks=""all""):\n        """"""Run checks.\n\n        This is a generator that yields a list of dicts.\n        """"""\n        checker = EventListChecker(self)\n        return checker.run(checks=checks)\n\n    def map_coord(self, geom):\n        """"""Event map coordinates for a given geometry.\n\n        Parameters\n        ----------\n        geom : `~gammapy.maps.Geom`\n            Geometry\n\n        Returns\n        -------\n        coord : `~gammapy.maps.MapCoord`\n            Coordinates\n        """"""\n        coord = {""skycoord"": self.radec}\n\n        cols = {k.upper(): v for k, v in self.table.columns.items()}\n\n        for axis in geom.axes:\n            try:\n                col = cols[axis.name.upper()]\n                coord[axis.name] = Quantity(col).to(axis.unit)\n            except KeyError:\n                raise KeyError(f""Column not found in event list: {axis.name!r}"")\n\n        return MapCoord.create(coord)\n\n    def select_map_mask(self, mask):\n        """"""Select events inside a mask (`EventList`).\n\n        Parameters\n        ----------\n        mask : `~gammapy.maps.Map`\n            Mask\n        """"""\n        coord = self.map_coord(mask.geom)\n        values = mask.get_by_coord(coord)\n        valid = values > 0\n        return self.select_row_subset(valid)\n\n\nclass EventList(EventListBase):\n    """"""Event list for IACT dataset.\n\n    Data format specification: :ref:`gadf:iact-events`\n\n    For further information, see the base class: `~gammapy.data.EventListBase`.\n\n    Parameters\n    ----------\n    table : `~astropy.table.Table`\n        Event list table\n\n    Examples\n    --------\n    To load an example H.E.S.S. event list:\n\n    >>> from gammapy.data import EventList\n    >>> filename = \'$GAMMAPY_DATA/hess-dl3-dr1/data/hess_dl3_dr1_obs_id_023523.fits.gz\'\n    >>> events = EventList.read(filename)\n    """"""\n\n    @property\n    def observatory_earth_location(self):\n        """"""Observatory location (`~astropy.coordinates.EarthLocation`).""""""\n        return earth_location_from_dict(self.table.meta)\n\n    @property\n    def observation_time_duration(self):\n        """"""Observation time duration in seconds (`~astropy.units.Quantity`).\n\n        This is a keyword related to IACTs\n        The wall time, including dead-time.\n        """"""\n        return Quantity(self.table.meta[""ONTIME""], ""second"")\n\n    @property\n    def observation_live_time_duration(self):\n        """"""Live-time duration in seconds (`~astropy.units.Quantity`).\n\n        The dead-time-corrected observation time.\n\n        - In Fermi-LAT it is automatically provided in the header of the event list.\n        - In IACTs is computed as ``t_live = t_observation * (1 - f_dead)``\n\n        where ``f_dead`` is the dead-time fraction.\n        """"""\n        return Quantity(self.table.meta[""LIVETIME""], ""second"")\n\n    @property\n    def observation_dead_time_fraction(self):\n        """"""Dead-time fraction (float).\n\n        This is a keyword related to IACTs\n        Defined as dead-time over observation time.\n\n        Dead-time is defined as the time during the observation\n        where the detector didn\'t record events:\n        http://en.wikipedia.org/wiki/Dead_time\n        https://ui.adsabs.harvard.edu/abs/2004APh....22..285F\n\n        The dead-time fraction is used in the live-time computation,\n        which in turn is used in the exposure and flux computation.\n        """"""\n        return 1 - self.table.meta[""DEADC""]\n\n    @property\n    def altaz_frame(self):\n        """"""ALT / AZ frame (`~astropy.coordinates.AltAz`).""""""\n        return AltAz(obstime=self.time, location=self.observatory_earth_location)\n\n    @property\n    def altaz(self):\n        """"""ALT / AZ position computed from RA / DEC (`~astropy.coordinates.SkyCoord`).""""""\n        return self.radec.transform_to(self.altaz_frame)\n\n    @property\n    def altaz_from_table(self):\n        """"""ALT / AZ position from table (`~astropy.coordinates.SkyCoord`).""""""\n        lon = self.table[""AZ""]\n        lat = self.table[""ALT""]\n        return SkyCoord(lon, lat, unit=""deg"", frame=self.altaz_frame)\n\n    @property\n    def pointing_radec(self):\n        """"""Pointing RA / DEC sky coordinates (`~astropy.coordinates.SkyCoord`).""""""\n        info = self.table.meta\n        lon, lat = info[""RA_PNT""], info[""DEC_PNT""]\n        return SkyCoord(lon, lat, unit=""deg"", frame=""icrs"")\n\n    @property\n    def offset(self):\n        """"""Event offset from the array pointing position (`~astropy.coordinates.Angle`).""""""\n        position = self.radec\n        center = self.pointing_radec\n        offset = center.separation(position)\n        return Angle(offset, unit=""deg"")\n\n    def select_offset(self, offset_band):\n        """"""Select events in offset band.\n\n        Parameters\n        ----------\n        offset_band : `~astropy.coordinates.Angle`\n            offset band ``[offset_min, offset_max)``\n\n        Returns\n        -------\n        event_list : `EventList`\n            Copy of event list with selection applied.\n        """"""\n        offset = self.offset\n        mask = offset_band[0] <= offset\n        mask &= offset < offset_band[1]\n        return self.select_row_subset(mask)\n\n    def peek(self):\n        """"""Quick look plots.""""""\n        import matplotlib.pyplot as plt\n\n        fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 8))\n        self.plot_energy(ax=axes[0, 0])\n\n        off_max = self.pointing_radec.separation(self.radec).max().deg\n        bins = np.linspace(start=0, stop=off_max ** 2, num=30)\n        self.plot_offset2_distribution(ax=axes[0, 1], bins=bins)\n\n        self.plot_time(ax=axes[0, 2])\n\n        axes[1, 0].axis(""off"")\n        m = self._counts_image()\n        ax = plt.subplot(2, 3, 4, projection=m.geom.wcs)\n        m.plot(ax=ax, stretch=""linear"")\n\n        self.plot_energy_offset(ax=axes[1, 1])\n\n        self._plot_text_summary(ax=axes[1, 2])\n        plt.subplots_adjust(hspace=0.3, wspace=0.2)\n\n    def _plot_text_summary(self, ax):\n        ax.axis(""off"")\n        txt = str(self)\n        ax.text(0, 1, txt, fontsize=12, verticalalignment=""top"")\n\n    def _counts_image(self):\n        width = self.galactic.b.max().deg - self.galactic.b.min().deg\n        opts = {\n            ""width"": (width, width),\n            ""binsz"": 0.05,\n            ""proj"": ""TAN"",\n            ""frame"": ""galactic"",\n            ""skydir"": self.pointing_radec,\n        }\n        m = WcsNDMap.create(**opts)\n        m.fill_by_coord(self.radec)\n        m = m.smooth(width=0.5)\n        return m\n\n    def plot_image(self):\n        """"""Quick look counts map sky plot.""""""\n        m = self._counts_image()\n        m.plot(stretch=""sqrt"")\n\n\nclass EventListLAT(EventListBase):\n    """"""Event list for Fermi-LAT dataset.\n\n    Fermi-LAT data products\n    https://fermi.gsfc.nasa.gov/ssc/data/analysis/documentation/Cicerone/Cicerone_Data/LAT_DP.html\n    Data format specification (columns)\n    https://fermi.gsfc.nasa.gov/ssc/data/analysis/documentation/Cicerone/Cicerone_Data/LAT_Data_Columns.html\n\n    For further information, see the base class: `~gammapy.data.EventListBase`.\n\n    Parameters\n    ----------\n    table : `~astropy.table.Table`\n        Event list table\n\n    Examples\n    --------\n    To load an example Fermi-LAT event list:\n\n    >>> from gammapy.data import EventListLAT\n    >>> filename = ""$GAMMAPY_DATA/fermi-3fhl-gc/fermi-3fhl-gc-events.fits.gz""\n    >>> events = EventListLAT.read(filename)\n    """"""\n\n    def plot_image(self):\n        """"""Quick look counts map sky plot.""""""\n        from gammapy.maps import WcsNDMap\n\n        m = WcsNDMap.create(npix=(360, 180), binsz=1.0, proj=""AIT"", frame=""galactic"")\n        m.fill_by_coord(self.radec)\n        m.plot(stretch=""sqrt"")\n\n\nclass EventListChecker(Checker):\n    """"""Event list checker.\n\n    Data format specification: ref:`gadf:iact-events`\n\n    Parameters\n    ----------\n    event_list : `~gammapy.data.EventList`\n        Event list\n    """"""\n\n    CHECKS = {\n        ""meta"": ""check_meta"",\n        ""columns"": ""check_columns"",\n        ""times"": ""check_times"",\n        ""coordinates_galactic"": ""check_coordinates_galactic"",\n        ""coordinates_altaz"": ""check_coordinates_altaz"",\n    }\n\n    accuracy = {""angle"": Angle(""1 arcsec""), ""time"": Quantity(1, ""microsecond"")}\n\n    # https://gamma-astro-data-formats.readthedocs.io/en/latest/events/events.html#mandatory-header-keywords\n    meta_required = [\n        ""HDUCLASS"",\n        ""HDUDOC"",\n        ""HDUVERS"",\n        ""HDUCLAS1"",\n        ""OBS_ID"",\n        ""TSTART"",\n        ""TSTOP"",\n        ""ONTIME"",\n        ""LIVETIME"",\n        ""DEADC"",\n        ""RA_PNT"",\n        ""DEC_PNT"",\n        # TODO: what to do about these?\n        # They are currently listed as required in the spec,\n        # but I think we should just require ICRS and those\n        # are irrelevant, should not be used.\n        # \'RADECSYS\',\n        # \'EQUINOX\',\n        ""ORIGIN"",\n        ""TELESCOP"",\n        ""INSTRUME"",\n        ""CREATOR"",\n        # https://gamma-astro-data-formats.readthedocs.io/en/latest/general/time.html#time-formats\n        ""MJDREFI"",\n        ""MJDREFF"",\n        ""TIMEUNIT"",\n        ""TIMESYS"",\n        ""TIMEREF"",\n        # https://gamma-astro-data-formats.readthedocs.io/en/latest/general/coordinates.html#coords-location\n        ""GEOLON"",\n        ""GEOLAT"",\n        ""ALTITUDE"",\n    ]\n\n    _col = collections.namedtuple(""col"", [""name"", ""unit""])\n    columns_required = [\n        _col(name=""EVENT_ID"", unit=""""),\n        _col(name=""TIME"", unit=""s""),\n        _col(name=""RA"", unit=""deg""),\n        _col(name=""DEC"", unit=""deg""),\n        _col(name=""ENERGY"", unit=""TeV""),\n    ]\n\n    def __init__(self, event_list):\n        self.event_list = event_list\n\n    def _record(self, level=""info"", msg=None):\n        obs_id = self.event_list.table.meta[""OBS_ID""]\n        return {""level"": level, ""obs_id"": obs_id, ""msg"": msg}\n\n    def check_meta(self):\n        meta_missing = sorted(set(self.meta_required) - set(self.event_list.table.meta))\n        if meta_missing:\n            yield self._record(\n                level=""error"", msg=f""Missing meta keys: {meta_missing!r}""\n            )\n\n    def check_columns(self):\n        t = self.event_list.table\n\n        if len(t) == 0:\n            yield self._record(level=""error"", msg=""Events table has zero rows"")\n\n        for name, unit in self.columns_required:\n            if name not in t.colnames:\n                yield self._record(level=""error"", msg=f""Missing table column: {name!r}"")\n            else:\n                if Unit(unit) != (t[name].unit or """"):\n                    yield self._record(\n                        level=""error"", msg=f""Invalid unit for column: {name!r}""\n                    )\n\n    def check_times(self):\n        dt = (self.event_list.time - self.event_list.observation_time_start).sec\n        if dt.min() < self.accuracy[""time""].to_value(""s""):\n            yield self._record(level=""error"", msg=""Event times before obs start time"")\n\n        dt = (self.event_list.time - self.event_list.observation_time_end).sec\n        if dt.max() > self.accuracy[""time""].to_value(""s""):\n            yield self._record(level=""error"", msg=""Event times after the obs end time"")\n\n        if np.min(np.diff(dt)) <= 0:\n            yield self._record(level=""error"", msg=""Events are not time-ordered."")\n\n    def check_coordinates_galactic(self):\n        """"""Check if RA / DEC matches GLON / GLAT.""""""\n        t = self.event_list.table\n\n        if ""GLON"" not in t.colnames:\n            return\n\n        galactic = SkyCoord(t[""GLON""], t[""GLAT""], unit=""deg"", frame=""galactic"")\n        separation = self.event_list.radec.separation(galactic).to(""arcsec"")\n        if separation.max() > self.accuracy[""angle""]:\n            yield self._record(\n                level=""error"", msg=""GLON / GLAT not consistent with RA / DEC""\n            )\n\n    def check_coordinates_altaz(self):\n        """"""Check if ALT / AZ matches RA / DEC.""""""\n        t = self.event_list.table\n\n        if ""AZ"" not in t.colnames:\n            return\n\n        altaz_astropy = self.event_list.altaz\n        separation = angular_separation(\n            altaz_astropy.data.lon,\n            altaz_astropy.data.lat,\n            t[""AZ""].quantity,\n            t[""ALT""].quantity,\n        )\n        if separation.max() > self.accuracy[""angle""]:\n            yield self._record(\n                level=""error"", msg=""ALT / AZ not consistent with RA / DEC""\n            )\n'"
gammapy/data/filters.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport copy\nimport logging\n\n__all__ = [""ObservationFilter""]\n\nlog = logging.getLogger(__name__)\n\n\nclass ObservationFilter:\n    """"""Holds and applies filters to observation data.\n\n    Parameters\n    ----------\n    time_filter : `astropy.time.Time`\n        Start and stop time of the selected time interval. For now we only support a single time interval.\n    event_filters : list of dict\n        An event filter dictionary needs two keys:\n\n        - **type** : str, one of the keys in `~gammapy.data.ObservationFilter.EVENT_FILTER_TYPES`\n        - **opts** : dict, it is passed on to the method of the `~gammapy.data.EventListBase` class that corresponds to\n          the filter type (see `~gammapy.data.ObservationFilter.EVENT_FILTER_TYPES`)\n\n        The filtered event list will be an intersection of all filters. A union of filters is not supported yet.\n\n    Examples\n    --------\n    >>> from gammapy.data import ObservationFilter, DataStore, Observation\n    >>> from astropy.time import Time\n    >>> from astropy.coordinates import Angle\n    >>>\n    >>> time_filter = Time([\'2021-03-27T20:10:00\', \'2021-03-27T20:20:00\'])\n    >>> phase_filter = {\'type\': \'custom\', \'opts\': dict(parameter=\'PHASE\', band=(0.2, 0.8))}\n    >>>\n    >>> my_obs_filter = ObservationFilter(time_filter=time_filter, event_filters=[phase_filter])\n    >>>\n    >>> ds = DataStore.from_dir(""$GAMMAPY_DATA/cta-1dc/index/gps"")\n    >>> my_obs = ds.obs(obs_id=111630)\n    >>> my_obs.obs_filter = my_obs_filter\n    """"""\n\n    EVENT_FILTER_TYPES = dict(sky_region=""select_region"", custom=""select_parameter"")\n\n    def __init__(self, time_filter=None, event_filters=None):\n        self.time_filter = time_filter\n        self.event_filters = event_filters or []\n\n    def filter_events(self, events):\n        """"""Apply filters to an event list.\n\n        Parameters\n        ----------\n        events : `~gammapy.data.EventListBase`\n            Event list to which the filters will be applied\n\n        Returns\n        -------\n        filtered_events : `~gammapy.data.EventListBase`\n            The filtered event list\n        """"""\n        filtered_events = self._filter_by_time(events)\n\n        for f in self.event_filters:\n            method_str = self.EVENT_FILTER_TYPES[f[""type""]]\n            filtered_events = getattr(filtered_events, method_str)(**f[""opts""])\n\n        return filtered_events\n\n    def filter_gti(self, gti):\n        """"""Apply filters to a GTI table.\n\n        Parameters\n        ----------\n        gti : `~gammapy.data.GTI`\n            GTI table to which the filters will be applied\n\n        Returns\n        -------\n        filtered_gti : `~gammapy.data.GTI`\n            The filtered GTI table\n        """"""\n        return self._filter_by_time(gti)\n\n    def _filter_by_time(self, data):\n        """"""Returns a new time filtered data object.\n\n        Calls the `select_time` method of the data object.\n        """"""\n        if self.time_filter:\n            return data.select_time(self.time_filter)\n        else:\n            return data\n\n    def copy(self):\n        """"""Copy the `ObservationFilter` object.""""""\n        return copy.deepcopy(self)\n'"
gammapy/data/gti.py,4,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom astropy.table import Table, vstack\nfrom astropy.time import Time\nfrom astropy.units import Quantity\nfrom gammapy.utils.scripts import make_path\nfrom gammapy.utils.time import (\n    time_ref_from_dict,\n    time_ref_to_dict,\n    time_relative_to_ref,\n)\n\n__all__ = [""GTI""]\n\n\nclass GTI:\n    """"""Good time intervals (GTI) `~astropy.table.Table`.\n\n    Data format specification: :ref:`gadf:iact-gti`\n\n    Note: at the moment dead-time and live-time is in the\n    EVENTS header ... the GTI header just deals with\n    observation times.\n\n    Parameters\n    ----------\n    table : `~astropy.table.Table`\n        GTI table\n\n    Examples\n    --------\n    Load GTIs for a H.E.S.S. event list:\n\n    >>> from gammapy.data import GTI\n    >>> gti = GTI.read(\'$GAMMAPY_DATA/hess-dl3-dr1//data/hess_dl3_dr1_obs_id_023523.fits.gz\')\n    >>> print(gti)\n    GTI info:\n    - Number of GTIs: 1\n    - Duration: 1687.0 s\n    - Start: 53343.92234009259 MET\n    - Start: 2004-12-04T22:08:10.184 (time standard: TT)\n    - Stop: 53343.94186555556 MET\n    - Stop: 2004-12-04T22:36:17.184 (time standard: TT)\n\n    Load GTIs for a Fermi-LAT event list:\n\n    >>> gti = GTI.read(""$GAMMAPY_DATA/fermi-3fhl-gc/fermi-3fhl-gc-events.fits.gz"")\n    >>> print(gti)\n    GTI info:\n    - Number of GTIs: 39042\n    - Duration: 183139597.9032163 s\n    - Start: 54682.65603794185 MET\n    - Start: 2008-08-04T15:44:41.678 (time standard: TT)\n    - Stop: 57236.96833546296 MET\n    - Stop: 2015-08-02T23:14:24.184 (time standard: TT)\n    """"""\n\n    def __init__(self, table):\n        self.table = table\n\n    def copy(self):\n        return self.__class__(self.table)\n\n    @classmethod\n    def create(cls, start, stop, reference_time=""2000-01-01""):\n        """"""Creates a GTI table from start and stop times.\n\n        Parameters\n        ----------\n        start : `~astropy.units.Quantity`\n            start times w.r.t. reference time\n        stop : `~astropy.units.Quantity`\n            stop times w.r.t. reference time\n        reference_time : `~astropy.time.Time`\n            the reference time to use in GTI definition\n        """"""\n        start = np.atleast_1d(Quantity(start))\n        stop = np.atleast_1d(Quantity(stop))\n        reference_time = Time(reference_time)\n        meta = time_ref_to_dict(reference_time)\n        table = Table({""START"": start.to(""s""), ""STOP"": stop.to(""s"")}, meta=meta)\n        return cls(table)\n\n    @classmethod\n    def read(cls, filename, hdu=""GTI""):\n        """"""Read from FITS file.\n\n        Parameters\n        ----------\n        filename : `pathlib.Path`, str\n            Filename\n        hdu : str\n            hdu name. Default GTI.\n        """"""\n        filename = make_path(filename)\n        table = Table.read(filename, hdu=hdu)\n        return cls(table)\n\n    def write(self, filename, **kwargs):\n        """"""Write to file.""""""\n        self.table.write(make_path(filename), **kwargs)\n\n    def __str__(self):\n        return (\n            ""GTI info:\\n""\n            f""- Number of GTIs: {len(self.table)}\\n""\n            f""- Duration: {self.time_sum}\\n""\n            f""- Start: {self.time_start[0]} MET\\n""\n            f""- Start: {self.time_start[0].fits} (time standard: {self.time_start[-1].scale.upper()})\\n""\n            f""- Stop: {self.time_stop[-1]} MET\\n""\n            f""- Stop: {self.time_stop[-1].fits} (time standard: {self.time_stop[-1].scale.upper()})\\n""\n        )\n\n    @property\n    def time_delta(self):\n        """"""GTI durations in seconds (`~astropy.units.Quantity`).""""""\n        start = self.table[""START""].astype(""float64"")\n        stop = self.table[""STOP""].astype(""float64"")\n        return Quantity(stop - start, ""second"")\n\n    @property\n    def time_ref(self):\n        """"""Time reference (`~astropy.time.Time`).""""""\n        return time_ref_from_dict(self.table.meta)\n\n    @property\n    def time_sum(self):\n        """"""Sum of GTIs in seconds (`~astropy.units.Quantity`).""""""\n        return self.time_delta.sum()\n\n    @property\n    def time_start(self):\n        """"""GTI start times (`~astropy.time.Time`).""""""\n        met = Quantity(self.table[""START""].astype(""float64""), ""second"")\n        return self.time_ref + met\n\n    @property\n    def time_stop(self):\n        """"""GTI end times (`~astropy.time.Time`).""""""\n        met = Quantity(self.table[""STOP""].astype(""float64""), ""second"")\n        return self.time_ref + met\n\n    def select_time(self, time_interval):\n        """"""Select and crop GTIs in time interval.\n\n        Parameters\n        ----------\n        time_interval : `astropy.time.Time`\n            Start and stop time for the selection.\n\n        Returns\n        -------\n        gti : `GTI`\n            Copy of the GTI table with selection applied.\n        """"""\n        # get GTIs that fall within the time_interval\n        mask = self.time_start < time_interval[1]\n        mask &= self.time_stop > time_interval[0]\n        gti_within = self.table[mask]\n\n        # crop the GTIs\n        start_met = time_relative_to_ref(time_interval[0], self.table.meta)\n        stop_met = time_relative_to_ref(time_interval[1], self.table.meta)\n        np.clip(\n            gti_within[""START""],\n            start_met.value,\n            stop_met.value,\n            out=gti_within[""START""],\n        )\n        np.clip(\n            gti_within[""STOP""], start_met.value, stop_met.value, out=gti_within[""STOP""]\n        )\n\n        return self.__class__(gti_within)\n\n    def stack(self, other):\n        """"""Stack with another GTI.\n\n        This simply changes the time reference of the second GTI table\n        and stack the two tables. No logic is applied to the intervals.\n\n        Parameters\n        ----------\n        other : `~gammapy.data.GTI`\n            GTI to stack to self\n\n        Returns\n        -------\n        new_gti : `~gammapy.data.GTI`\n            New GTI\n        """"""\n        start = (other.time_start - self.time_ref).sec\n        end = (other.time_stop - self.time_ref).sec\n        table = Table({""START"": start, ""STOP"": end}, names=[""START"", ""STOP""])\n        return self.__class__(vstack([self.table, table]))\n\n    def union(self):\n        """"""Union of overlapping time intervals.\n\n        Returns a new `~gammapy.data.GTI` object.\n\n        Intervals that touch will be merged, e.g.\n        ``(1, 2)`` and ``(2, 3)`` will result in ``(1, 3)``.\n        """"""\n        # Algorithm to merge overlapping intervals is well-known,\n        # see e.g. https://stackoverflow.com/a/43600953/498873\n\n        table = self.table.copy()\n        table.sort(""START"")\n\n        # We use Python dict instead of astropy.table.Row objects,\n        # because on some versions modifying Row entries doesn\'t behave as expected\n        merged = [{""START"": table[0][""START""], ""STOP"": table[0][""STOP""]}]\n        for row in table[1:]:\n            interval = {""START"": row[""START""], ""STOP"": row[""STOP""]}\n            if merged[-1][""STOP""] <= interval[""START""]:\n                merged.append(interval)\n            else:\n                merged[-1][""STOP""] = max(interval[""STOP""], merged[-1][""STOP""])\n\n        merged = Table(rows=merged, names=[""START"", ""STOP""], meta=self.table.meta)\n        return self.__class__(merged)\n'"
gammapy/data/hdu_index_table.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport sys\nimport numpy as np\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom astropy.utils import lazyproperty\nfrom gammapy.utils.scripts import make_path\n\n__all__ = [""HDULocation"", ""HDUIndexTable""]\n\nlog = logging.getLogger(__name__)\n\n\nclass HDULocation:\n    """"""HDU localisation, loading and Gammapy object mapper.\n\n    This represents one row in `HDUIndexTable`.\n\n    It\'s more a helper class, that is wrapped by `~gammapy.data.Observation`,\n    usually those objects will be used to access data.\n\n    See also :ref:`gadf:hdu-index`.\n    """"""\n\n    def __init__(\n        self, obs_id, hdu_type, hdu_class, base_dir, file_dir, file_name, hdu_name\n    ):\n        self.obs_id = obs_id\n        self.hdu_type = hdu_type\n        self.hdu_class = hdu_class\n        self.base_dir = base_dir\n        self.file_dir = file_dir\n        self.file_name = file_name\n        self.hdu_name = hdu_name\n\n    def info(self, file=None):\n        """"""Print some summary info to stdout.""""""\n        if not file:\n            file = sys.stdout\n\n        print(f""OBS_ID = {self.obs_id}"", file=file)\n        print(f""HDU_TYPE = {self.hdu_type}"", file=file)\n        print(f""HDU_CLASS = {self.hdu_class}"", file=file)\n        print(f""BASE_DIR = {self.base_dir}"", file=file)\n        print(f""FILE_DIR = {self.file_dir}"", file=file)\n        print(f""FILE_NAME = {self.file_name}"", file=file)\n        print(f""HDU_NAME = {self.hdu_name}"", file=file)\n\n    def path(self, abs_path=True):\n        """"""Full filename path.\n\n        Include ``base_dir`` if ``abs_path`` is True.\n        """"""\n        if abs_path:\n            return make_path(self.base_dir) / self.file_dir / self.file_name\n        else:\n            return make_path(self.file_dir) / self.file_name\n\n    def get_hdu(self):\n        """"""Get HDU.""""""\n        filename = self.path(abs_path=True)\n        # Here we\'re intentionally not calling `with fits.open`\n        # because we don\'t want the file to remain open.\n        hdu_list = fits.open(filename, memmap=False)\n        return hdu_list[self.hdu_name]\n\n    def load(self):\n        """"""Load HDU as appropriate class.\n\n        TODO: this should probably go via an extensible registry.\n        """"""\n        hdu_class = self.hdu_class\n        filename = self.path()\n        hdu = self.hdu_name\n\n        if hdu_class == ""events"":\n            from gammapy.data import EventList\n\n            return EventList.read(filename, hdu=hdu)\n        elif hdu_class == ""gti"":\n            from gammapy.data import GTI\n\n            return GTI.read(filename, hdu=hdu)\n        elif hdu_class == ""aeff_2d"":\n            from gammapy.irf import EffectiveAreaTable2D\n\n            return EffectiveAreaTable2D.read(filename, hdu=hdu)\n        elif hdu_class == ""edisp_2d"":\n            from gammapy.irf import EnergyDispersion2D\n\n            return EnergyDispersion2D.read(filename, hdu=hdu)\n        elif hdu_class == ""psf_table"":\n            from gammapy.irf import PSF3D\n\n            return PSF3D.read(filename, hdu=hdu)\n        elif hdu_class == ""psf_3gauss"":\n            from gammapy.irf import EnergyDependentMultiGaussPSF\n\n            return EnergyDependentMultiGaussPSF.read(filename, hdu=hdu)\n        elif hdu_class == ""psf_king"":\n            from gammapy.irf import PSFKing\n\n            return PSFKing.read(filename, hdu=hdu)\n        elif hdu_class == ""bkg_2d"":\n            from gammapy.irf import Background2D\n\n            return Background2D.read(filename, hdu=hdu)\n        elif hdu_class == ""bkg_3d"":\n            from gammapy.irf import Background3D\n\n            return Background3D.read(filename, hdu=hdu)\n        else:\n            raise ValueError(f""Invalid hdu_class: {hdu_class}"")\n\n\nclass HDUIndexTable(Table):\n    """"""HDU index table.\n\n    See :ref:`gadf:hdu-index`.\n    """"""\n\n    VALID_HDU_TYPE = [""events"", ""gti"", ""aeff"", ""edisp"", ""psf"", ""bkg""]\n    """"""Valid values for `HDU_TYPE`.""""""\n\n    VALID_HDU_CLASS = [\n        ""events"",\n        ""gti"",\n        ""aeff_2d"",\n        ""edisp_2d"",\n        ""psf_table"",\n        ""psf_3gauss"",\n        ""psf_king"",\n        ""bkg_2d"",\n        ""bkg_3d"",\n    ]\n    """"""Valid values for `HDU_CLASS`.""""""\n\n    @classmethod\n    def read(cls, filename, **kwargs):\n        """"""Read :ref:`gadf:hdu-index`.\n\n        Parameters\n        ----------\n        filename : `pathlib.Path`, str\n            Filename\n        """"""\n        filename = make_path(filename)\n        table = super().read(filename, **kwargs)\n        table.meta[""BASE_DIR""] = filename.parent.as_posix()\n\n        return table\n\n    @property\n    def base_dir(self):\n        """"""Base directory.""""""\n        return make_path(self.meta.get(""BASE_DIR"", """"))\n\n    def hdu_location(self, obs_id, hdu_type=None, hdu_class=None):\n        """"""Create `HDULocation` for a given selection.\n\n        Parameters\n        ----------\n        obs_id : int\n            Observation ID\n        hdu_type : str\n            HDU type (see `~gammapy.data.HDUIndexTable.VALID_HDU_TYPE`)\n        hdu_class : str\n            HDU class (see `~gammapy.data.HDUIndexTable.VALID_HDU_CLASS`)\n\n        Returns\n        -------\n        location : `~gammapy.data.HDULocation`\n            HDU location\n        """"""\n        self._validate_selection(obs_id=obs_id, hdu_type=hdu_type, hdu_class=hdu_class)\n\n        idx = self.row_idx(obs_id=obs_id, hdu_type=hdu_type, hdu_class=hdu_class)\n\n        if len(idx) == 1:\n            idx = idx[0]\n        elif len(idx) == 0:\n            log.warning(\n                f""No HDU found matching: OBS_ID = {obs_id}, HDU_TYPE = {hdu_type}, HDU_CLASS = {hdu_class}""\n            )\n            return None\n        else:\n            idx = idx[0]\n            log.warning(\n                f""Found multiple HDU matching: OBS_ID = {obs_id}, HDU_TYPE = {hdu_type}, HDU_CLASS = {hdu_class}.""\n                f"" Returning the first entry, which has ""\n                f""HDU_TYPE = {self[idx][\'HDU_TYPE\']} and HDU_CLASS = {self[idx][\'HDU_CLASS\']}""\n            )\n\n        return self.location_info(idx)\n\n    def _validate_selection(self, obs_id, hdu_type, hdu_class):\n        """"""Validate HDU selection.\n\n        The goal is to give helpful error messages to the user.\n        """"""\n        if hdu_type is None and hdu_class is None:\n            raise ValueError(""You have to specify `hdu_type` or `hdu_class`."")\n\n        if hdu_type and hdu_type not in self.VALID_HDU_TYPE:\n            valid = [str(_) for _ in self.VALID_HDU_TYPE]\n            raise ValueError(f""Invalid hdu_type: {hdu_type}. Valid values are: {valid}"")\n\n        if hdu_class and hdu_class not in self.VALID_HDU_CLASS:\n            valid = [str(_) for _ in self.VALID_HDU_CLASS]\n            raise ValueError(\n                f""Invalid hdu_class: {hdu_class}. Valid values are: {valid}""\n            )\n\n        if obs_id not in self[""OBS_ID""]:\n            raise IndexError(f""No entry available with OBS_ID = {obs_id}"")\n\n    def row_idx(self, obs_id, hdu_type=None, hdu_class=None):\n        """"""Table row indices for a given selection.\n\n        Parameters\n        ----------\n        obs_id : int\n            Observation ID\n        hdu_type : str\n            HDU type (see `~gammapy.data.HDUIndexTable.VALID_HDU_TYPE`)\n        hdu_class : str\n            HDU class (see `~gammapy.data.HDUIndexTable.VALID_HDU_CLASS`)\n\n        Returns\n        -------\n        idx : list of int\n            List of row indices matching the selection.\n        """"""\n        selection = self[""OBS_ID""] == obs_id\n\n        if hdu_class:\n            is_hdu_class = self._hdu_class_stripped == hdu_class\n            selection &= is_hdu_class\n\n        if hdu_type:\n            is_hdu_type = self._hdu_type_stripped == hdu_type\n            selection &= is_hdu_type\n\n        idx = np.where(selection)[0]\n        return list(idx)\n\n    def location_info(self, idx):\n        """"""Create `HDULocation` for a given row index.""""""\n        row = self[idx]\n        return HDULocation(\n            obs_id=row[""OBS_ID""],\n            hdu_type=row[""HDU_TYPE""].strip(),\n            hdu_class=row[""HDU_CLASS""].strip(),\n            base_dir=self.base_dir.as_posix(),\n            file_dir=row[""FILE_DIR""].strip(),\n            file_name=row[""FILE_NAME""].strip(),\n            hdu_name=row[""HDU_NAME""].strip(),\n        )\n\n    @lazyproperty\n    def _hdu_class_stripped(self):\n        return np.array([_.strip() for _ in self[""HDU_CLASS""]])\n\n    @lazyproperty\n    def _hdu_type_stripped(self):\n        return np.array([_.strip() for _ in self[""HDU_TYPE""]])\n\n    @lazyproperty\n    def obs_id_unique(self):\n        """"""Observation IDs (unique).""""""\n        return np.unique(np.sort(self[""OBS_ID""]))\n\n    @lazyproperty\n    def hdu_type_unique(self):\n        """"""HDU types (unique).""""""\n        return list(np.unique(np.sort([_.strip() for _ in self[""HDU_TYPE""]])))\n\n    @lazyproperty\n    def hdu_class_unique(self):\n        """"""HDU classes (unique).""""""\n        return list(np.unique(np.sort([_.strip() for _ in self[""HDU_CLASS""]])))\n\n    def summary(self):\n        """"""Summary report (str).""""""\n        obs_id = self.obs_id_unique\n        return (\n            ""HDU index table:\\n""\n            f""BASE_DIR: {self.base_dir}\\n""\n            f""Rows: {len(self)}\\n""\n            f""OBS_ID: {obs_id[0]} -- {obs_id[-1]}\\n""\n            f""HDU_TYPE: {self.hdu_type_unique}\\n""\n            f""HDU_CLASS: {self.hdu_class_unique}\\n""\n        )\n'"
gammapy/data/obs_table.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom collections import namedtuple\nimport numpy as np\nfrom astropy.coordinates import Angle, SkyCoord\nfrom astropy.table import Table\nfrom astropy.units import Quantity, Unit\nfrom astropy.utils import lazyproperty\nfrom gammapy.utils.regions import SphericalCircleSkyRegion\nfrom gammapy.utils.scripts import make_path\nfrom gammapy.utils.testing import Checker\nfrom gammapy.utils.time import time_ref_from_dict\nfrom .gti import GTI\n\n__all__ = [""ObservationTable""]\n\n\nclass ObservationTable(Table):\n    """"""Observation table.\n\n    Data format specification: :ref:`gadf:obs-index`\n    """"""\n\n    @classmethod\n    def read(cls, filename, **kwargs):\n        """"""Read an observation table from file.\n\n        Parameters\n        ----------\n        filename : `pathlib.Path`, str\n            Filename\n        """"""\n        return super().read(make_path(filename), **kwargs)\n\n    @property\n    def pointing_radec(self):\n        """"""Pointing positions as ICRS (`~astropy.coordinates.SkyCoord`).""""""\n        return SkyCoord(self[""RA_PNT""], self[""DEC_PNT""], unit=""deg"", frame=""icrs"")\n\n    @property\n    def pointing_galactic(self):\n        """"""Pointing positions as Galactic (`~astropy.coordinates.SkyCoord`).""""""\n        return SkyCoord(\n            self[""GLON_PNT""], self[""GLAT_PNT""], unit=""deg"", frame=""galactic""\n        )\n\n    @property\n    def time_ref(self):\n        """"""Time reference (`~astropy.time.Time`).""""""\n        return time_ref_from_dict(self.meta)\n\n    @property\n    def time_start(self):\n        """"""Observation start time (`~astropy.time.Time`).""""""\n        return self.time_ref + Quantity(self[""TSTART""], ""second"")\n\n    @property\n    def time_stop(self):\n        """"""Observation stop time (`~astropy.time.Time`).""""""\n        return self.time_ref + Quantity(self[""TSTOP""], ""second"")\n\n    @lazyproperty\n    def _index_dict(self):\n        """"""Dict containing row index for all obs ids.""""""\n        # TODO: Switch to http://docs.astropy.org/en/latest/table/indexing.html once it is more stable\n        temp = zip(self[""OBS_ID""], np.arange(len(self)))\n        return dict(temp)\n\n    def get_obs_idx(self, obs_id):\n        """"""Get row index for given ``obs_id``.\n\n        Raises KeyError if observation is not available.\n\n        Parameters\n        ----------\n        obs_id : int, list\n            observation ids\n\n        Returns\n        -------\n        idx : list\n            indices corresponding to obs_id\n        """"""\n        idx = [self._index_dict[key] for key in np.atleast_1d(obs_id)]\n        return idx\n\n    def select_obs_id(self, obs_id):\n        """"""Get `~gammapy.data.ObservationTable` containing only ``obs_id``.\n\n        Raises KeyError if observation is not available.\n\n        Parameters\n        ----------\n        obs_id: int, list\n            observation ids\n        """"""\n        return self[self.get_obs_idx(obs_id)]\n\n    def summary(self):\n        """"""Summary info string (str).""""""\n        obs_name = self.meta.get(""OBSERVATORY_NAME"", ""N/A"")\n        return (\n            f""Observation table:\\n""\n            f""Observatory name: {obs_name!r}\\n""\n            f""Number of observations: {len(self)}\\n""\n        )\n\n    def select_range(self, selection_variable, value_range, inverted=False):\n        """"""Make an observation table, applying some selection.\n\n        Generic function to apply a 1D box selection (min, max) to a\n        table on any variable that is in the observation table and can\n        be casted into a `~astropy.units.Quantity` object.\n\n        If the range length is 0 (min = max), the selection is applied\n        to the exact value indicated by the min value. This is useful\n        for selection of exact values, for instance in discrete\n        variables like the number of telescopes.\n\n        If the inverted flag is activated, the selection is applied to\n        keep all elements outside the selected range.\n\n        Parameters\n        ----------\n        selection_variable : str\n            Name of variable to apply a cut (it should exist on the table).\n        value_range : `~astropy.units.Quantity`-like\n            Allowed range of values (min, max). The type should be\n            consistent with the selection_variable.\n        inverted : bool, optional\n            Invert selection: keep all entries outside the (min, max) range.\n\n        Returns\n        -------\n        obs_table : `~gammapy.data.ObservationTable`\n            Observation table after selection.\n        """"""\n        value_range = Quantity(value_range)\n\n        # read values into a quantity in case units have to be taken into account\n        value = Quantity(self[selection_variable])\n\n        mask = (value_range[0] <= value) & (value < value_range[1])\n\n        if np.allclose(value_range[0].value, value_range[1].value):\n            mask = value_range[0] == value\n\n        if inverted:\n            mask = np.invert(mask)\n\n        return self[mask]\n\n    def select_time_range(self, time_range, partial_overlap=False, inverted=False):\n        """"""Make an observation table, applying a time selection.\n\n        Apply a 1D box selection (min, max) to a\n        table on any time variable that is in the observation table.\n        It supports absolute times in `~astropy.time.Time` format.\n\n        If the inverted flag is activated, the selection is applied to\n        keep all elements outside the selected range.\n\n        Parameters\n        ----------\n        time_range : `~astropy.time.Time`\n            Allowed time range (min, max).\n        partial_overlap : bool, optional\n            Include partially overlapping observations. Default is False\n        inverted : bool, optional\n            Invert selection: keep all entries outside the (min, max) range.\n\n        Returns\n        -------\n        obs_table : `~gammapy.data.ObservationTable`\n            Observation table after selection.\n        """"""\n        tstart = self.time_start\n        tstop = self.time_stop\n\n        if partial_overlap is False:\n            mask1 = time_range[0] <= tstart\n            mask2 = time_range[1] >= tstop\n        else:\n            mask1 = time_range[0] <= tstop\n            mask2 = time_range[1] >= tstart\n\n        mask = mask1 & mask2\n\n        if inverted:\n            mask = np.invert(mask)\n\n        return self[mask]\n\n    def select_observations(self, selection=None):\n        """"""Select subset of observations.\n\n        Returns a new observation table representing the subset.\n\n        There are 3 main kinds of selection criteria, according to the\n        value of the **type** keyword in the **selection** dictionary:\n\n        - sky regions\n\n        - time intervals (min, max)\n\n        - intervals (min, max) on any other parameter present in the\n          observation table, that can be casted into an\n          `~astropy.units.Quantity` object\n\n        Allowed selection criteria are interpreted using the following\n        keywords in the **selection** dictionary under the **type** key.\n\n        - ``sky_circle`` is a circular region centered in the coordinate\n           marked by the **lon** and **lat** keywords, and radius **radius**;\n           uses `~gammapy.catalog.select_sky_circle`\n\n        - ``time_box`` is a 1D selection criterion acting on the observation\n          start time (**TSTART**); the interval is set via the\n          **time_range** keyword; uses\n          `~gammapy.data.ObservationTable.select_time_range`\n\n        - ``par_box`` is a 1D selection criterion acting on any\n          parameter defined in the observation table that can be casted\n          into an `~astropy.units.Quantity` object; the parameter name\n          and interval can be specified using the keywords **variable** and\n          **value_range** respectively; min = max selects exact\n          values of the parameter; uses\n          `~gammapy.data.ObservationTable.select_range`\n\n        In all cases, the selection can be inverted by activating the\n        **inverted** flag, in which case, the selection is applied to keep all\n        elements outside the selected range.\n\n        A few examples of selection criteria are given below.\n\n        Parameters\n        ----------\n        selection : dict\n            Dictionary with a few keywords for applying selection cuts.\n\n        Returns\n        -------\n        obs_table : `~gammapy.data.ObservationTable`\n            Observation table after selection.\n\n        Examples\n        --------\n        >>> selection = dict(type=\'sky_circle\', frame=\'galactic\',\n        ...                  lon=Angle(0, \'deg\'),\n        ...                  lat=Angle(0, \'deg\'),\n        ...                  radius=Angle(5, \'deg\'),\n        ...                  border=Angle(2, \'deg\'))\n        >>> selected_obs_table = obs_table.select_observations(selection)\n\n        >>> selection = dict(type=\'time_box\',\n        ...                  time_range=Time([\'2012-01-01T01:00:00\', \'2012-01-01T02:00:00\']))\n        >>> selected_obs_table = obs_table.select_observations(selection)\n\n        >>> selection = dict(type=\'par_box\', variable=\'ALT\',\n        ...                  value_range=Angle([60., 70.], \'deg\'))\n        >>> selected_obs_table = obs_table.select_observations(selection)\n\n        >>> selection = dict(type=\'par_box\', variable=\'OBS_ID\',\n        ...                  value_range=[2, 5])\n        >>> selected_obs_table = obs_table.select_observations(selection)\n\n        >>> selection = dict(type=\'par_box\', variable=\'N_TELS\',\n        ...                  value_range=[4, 4])\n        >>> selected_obs_table = obs_table.select_observations(selection)\n        """"""\n        if ""inverted"" not in selection:\n            selection[""inverted""] = False\n        if ""partial_overlap"" not in selection:\n            selection[""partial_overlap""] = False\n\n        if selection[""type""] == ""sky_circle"":\n            lon = Angle(selection[""lon""], ""deg"")\n            lat = Angle(selection[""lat""], ""deg"")\n            radius = Angle(selection[""radius""])\n            if ""border"" in selection:\n                border = Angle(selection[""border""])\n            else:\n                border = Angle(0, ""deg"")\n            region = SphericalCircleSkyRegion(\n                center=SkyCoord(lon, lat, frame=selection[""frame""]),\n                radius=radius + border,\n            )\n            mask = region.contains(self.pointing_radec)\n            if selection[""inverted""]:\n                mask = np.invert(mask)\n            return self[mask]\n        elif selection[""type""] == ""time_box"":\n            return self.select_time_range(\n                selection[""time_range""],\n                selection[""partial_overlap""],\n                selection[""inverted""],\n            )\n        elif selection[""type""] == ""par_box"":\n            return self.select_range(\n                selection[""variable""], selection[""value_range""], selection[""inverted""]\n            )\n        else:\n            raise ValueError(f""Invalid selection type: {selection[\'type\']}"")\n\n    def create_gti(self, obs_id):\n        """"""Returns a GTI table containing TSTART and TSTOP from the table.\n\n        TODO: This method can be removed once GTI tables are explicitly required in Gammapy.\n\n        Parameters\n        ----------\n        obs_id : int\n            ID of the observation for which the GTI table will be created\n\n        Returns\n        -------\n        gti : `~gammapy.data.GTI`\n            GTI table containing one row (TSTART and TSTOP of the observation with ``obs_id``)\n        """"""\n        meta = dict(\n            EXTNAME=""GTI"",\n            HDUCLASS=""GADF"",\n            HDUDOC=""https://github.com/open-gamma-ray-astro/gamma-astro-data-formats"",\n            HDUVERS=""0.2"",\n            HDUCLAS1=""GTI"",\n            MJDREFI=self.meta[""MJDREFI""],\n            MJDREFF=self.meta[""MJDREFF""],\n            TIMEUNIT=self.meta.get(""TIMEUNIT"", ""s""),\n            TIMESYS=self.meta[""TIMESYS""],\n            TIMEREF=self.meta.get(""TIMEREF"", ""LOCAL""),\n        )\n\n        obs = self.select_obs_id(obs_id)\n\n        gti = Table(meta=meta)\n        gti[""START""] = obs[""TSTART""].quantity.to(""s"")\n        gti[""STOP""] = obs[""TSTOP""].quantity.to(""s"")\n\n        return GTI(gti)\n\n\nclass ObservationTableChecker(Checker):\n    """"""Event list checker.\n\n    Data format specification: ref:`gadf:iact-events`\n\n    Parameters\n    ----------\n    event_list : `~gammapy.data.EventList`\n        Event list\n    """"""\n\n    CHECKS = {\n        ""meta"": ""check_meta"",\n        ""columns"": ""check_columns"",\n        # ""times"": ""check_times"",\n        # ""coordinates_galactic"": ""check_coordinates_galactic"",\n        # ""coordinates_altaz"": ""check_coordinates_altaz"",\n    }\n\n    # accuracy = {""angle"": Angle(""1 arcsec""), ""time"": Quantity(1, ""microsecond"")}\n\n    # https://gamma-astro-data-formats.readthedocs.io/en/latest/events/events.html#mandatory-header-keywords\n    meta_required = [\n        ""HDUCLASS"",\n        ""HDUDOC"",\n        ""HDUVERS"",\n        ""HDUCLAS1"",\n        ""HDUCLAS2"",\n        # https://gamma-astro-data-formats.readthedocs.io/en/latest/general/time.html#time-formats\n        ""MJDREFI"",\n        ""MJDREFF"",\n        ""TIMEUNIT"",\n        ""TIMESYS"",\n        ""TIMEREF"",\n        # https://gamma-astro-data-formats.readthedocs.io/en/latest/general/coordinates.html#coords-location\n        ""GEOLON"",\n        ""GEOLAT"",\n        ""ALTITUDE"",\n    ]\n\n    _col = namedtuple(""col"", [""name"", ""unit""])\n    columns_required = [\n        _col(name=""OBS_ID"", unit=""""),\n        _col(name=""RA_PNT"", unit=""deg""),\n        _col(name=""DEC_PNT"", unit=""deg""),\n        _col(name=""TSTART"", unit=""s""),\n        _col(name=""TSTOP"", unit=""s""),\n    ]\n\n    def __init__(self, obs_table):\n        self.obs_table = obs_table\n\n    def _record(self, level=""info"", msg=None):\n        return {""level"": level, ""hdu"": ""obs-index"", ""msg"": msg}\n\n    def check_meta(self):\n        m = self.obs_table.meta\n\n        meta_missing = sorted(set(self.meta_required) - set(m))\n        if meta_missing:\n            yield self._record(\n                level=""error"", msg=f""Missing meta keys: {meta_missing!r}""\n            )\n\n        if m.get(""HDUCLAS1"", """") != ""INDEX"":\n            yield self._record(level=""error"", msg=""HDUCLAS1 must be INDEX"")\n        if m.get(""HDUCLAS2"", """") != ""OBS"":\n            yield self._record(level=""error"", msg=""HDUCLAS2 must be OBS"")\n\n    def check_columns(self):\n        t = self.obs_table\n\n        if len(t) == 0:\n            yield self._record(level=""error"", msg=""Observation table has zero rows"")\n\n        for name, unit in self.columns_required:\n            if name not in t.colnames:\n                yield self._record(level=""error"", msg=f""Missing table column: {name!r}"")\n            else:\n                if Unit(unit) != (t[name].unit or """"):\n                    yield self._record(\n                        level=""error"", msg=f""Invalid unit for column: {name!r}""\n                    )\n'"
gammapy/data/observations.py,12,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport collections.abc\nimport copy\nimport logging\nimport numpy as np\nfrom astropy.coordinates import SkyCoord\nfrom astropy.time import Time\nfrom astropy.units import Quantity\nfrom gammapy.irf import Background3D\nfrom gammapy.utils.fits import LazyFitsData, earth_location_from_dict\nfrom gammapy.utils.testing import Checker\nfrom .event_list import EventListChecker\nfrom .filters import ObservationFilter\nfrom .gti import GTI\nfrom .pointing import FixedPointingInfo\n\n__all__ = [""Observation"", ""Observations""]\n\nlog = logging.getLogger(__name__)\n\n\nclass Observation:\n    """"""In-memory observation.\n\n    Parameters\n    ----------\n    obs_id : int\n        Observation id\n    obs_info : dict\n        Observation info dict\n    aeff : `~gammapy.irf.EffectiveAreaTable2D`\n        Effective area\n    edisp : `~gammapy.irf.EnergyDispersion2D`\n        Energy dispersion\n    psf : `~gammapy.irf.PSF3D`\n        Point spread function\n    bkg : `~gammapy.irf.Background3D`\n        Background rate model\n    gti : `~gammapy.data.GTI`\n        Table with GTI start and stop time\n    events : `~gammapy.data.EventList`\n        Event list\n    obs_filter : `ObservationFilter`\n        Observation filter.\n    """"""\n\n    aeff = LazyFitsData(cache=False)\n    edisp = LazyFitsData(cache=False)\n    psf = LazyFitsData(cache=False)\n    bkg = LazyFitsData(cache=False)\n    _events = LazyFitsData(cache=False)\n    _gti = LazyFitsData(cache=False)\n\n    def __init__(\n        self,\n        obs_id=None,\n        obs_info=None,\n        gti=None,\n        aeff=None,\n        edisp=None,\n        psf=None,\n        bkg=None,\n        events=None,\n        obs_filter=None,\n    ):\n        self.obs_id = obs_id\n        self.obs_info = obs_info\n        self.aeff = aeff\n        self.edisp = edisp\n        self.psf = psf\n        self.bkg = bkg\n        self._gti = gti\n        self._events = events\n        self.obs_filter = obs_filter or ObservationFilter()\n\n    @property\n    def events(self):\n        events = self.obs_filter.filter_events(self._events)\n        return events\n\n    @property\n    def gti(self):\n        events = self.obs_filter.filter_gti(self._gti)\n        return events\n\n    @staticmethod\n    def _get_obs_info(pointing, deadtime_fraction):\n        """"""Create obs info dict from in memory data""""""\n        return {\n            ""RA_PNT"": pointing.icrs.ra.deg,\n            ""DEC_PNT"": pointing.icrs.dec.deg,\n            ""DEADC"": 1 - deadtime_fraction,\n        }\n\n    @classmethod\n    def create(\n        cls,\n        pointing,\n        obs_id=0,\n        livetime=None,\n        tstart=None,\n        tstop=None,\n        irfs=None,\n        deadtime_fraction=0.0,\n        reference_time=""2000-01-01"",\n    ):\n        """"""Create an observation.\n\n        User must either provide the livetime, or the start and stop times.\n\n        Parameters\n        ----------\n        pointing : `~astropy.coordinates.SkyCoord`\n            Pointing position\n        obs_id : int\n            Observation ID as identifier\n        livetime : ~astropy.units.Quantity`\n            Livetime exposure of the simulated observation\n        tstart : `~astropy.units.Quantity`\n            Start time of observation w.r.t reference_time\n        tstop : `~astropy.units.Quantity` w.r.t reference_time\n            Stop time of observation\n        irfs : dict\n            IRFs used for simulating the observation: `bkg`, `aeff`, `psf`, `edisp`\n        deadtime_fraction : float, optional\n            Deadtime fraction, defaults to 0\n        reference_time : `~astropy.time.Time`\n            the reference time to use in GTI definition\n\n        Returns\n        -------\n        obs : `gammapy.data.MemoryObservation`\n        """"""\n        if tstart is None:\n            tstart = Quantity(0.0, ""hr"")\n\n        if tstop is None:\n            tstop = tstart + Quantity(livetime)\n\n        gti = GTI.create([tstart], [tstop], reference_time=reference_time)\n\n        obs_info = cls._get_obs_info(\n            pointing=pointing, deadtime_fraction=deadtime_fraction\n        )\n\n        return cls(\n            obs_id=obs_id,\n            obs_info=obs_info,\n            gti=gti,\n            aeff=irfs.get(""aeff""),\n            bkg=irfs.get(""bkg""),\n            edisp=irfs.get(""edisp""),\n            psf=irfs.get(""psf""),\n        )\n\n    @property\n    def tstart(self):\n        """"""Observation start time (`~astropy.time.Time`).""""""\n        return self.gti.time_start[0]\n\n    @property\n    def tstop(self):\n        """"""Observation stop time (`~astropy.time.Time`).""""""\n        return self.gti.time_stop[0]\n\n    @property\n    def observation_time_duration(self):\n        """"""Observation time duration in seconds (`~astropy.units.Quantity`).\n\n        The wall time, including dead-time.\n        """"""\n        return self.gti.time_sum\n\n    @property\n    def observation_live_time_duration(self):\n        """"""Live-time duration in seconds (`~astropy.units.Quantity`).\n\n        The dead-time-corrected observation time.\n\n        Computed as ``t_live = t_observation * (1 - f_dead)``\n        where ``f_dead`` is the dead-time fraction.\n        """"""\n        return self.observation_time_duration * (\n            1 - self.observation_dead_time_fraction\n        )\n\n    @property\n    def observation_dead_time_fraction(self):\n        """"""Dead-time fraction (float).\n\n        Defined as dead-time over observation time.\n\n        Dead-time is defined as the time during the observation\n        where the detector didn\'t record events:\n        https://en.wikipedia.org/wiki/Dead_time\n        https://ui.adsabs.harvard.edu/abs/2004APh....22..285F\n\n        The dead-time fraction is used in the live-time computation,\n        which in turn is used in the exposure and flux computation.\n        """"""\n        return 1 - self.obs_info[""DEADC""]\n\n    @property\n    def pointing_radec(self):\n        """"""Pointing RA / DEC sky coordinates (`~astropy.coordinates.SkyCoord`).""""""\n        lon, lat = (\n            self.obs_info.get(""RA_PNT"", np.nan),\n            self.obs_info.get(""DEC_PNT"", np.nan),\n        )\n        return SkyCoord(lon, lat, unit=""deg"", frame=""icrs"")\n\n    @property\n    def pointing_altaz(self):\n        """"""Pointing ALT / AZ sky coordinates (`~astropy.coordinates.SkyCoord`).""""""\n        alt, az = (\n            self.obs_info.get(""ALT_PNT"", np.nan),\n            self.obs_info.get(""AZ_PNT"", np.nan),\n        )\n        return SkyCoord(az, alt, unit=""deg"", frame=""altaz"")\n\n    @property\n    def pointing_zen(self):\n        """"""Pointing zenith angle sky (`~astropy.units.Quantity`).""""""\n        return Quantity(self.obs_info.get(""ZEN_PNT"", np.nan), unit=""deg"")\n\n    @property\n    def fixed_pointing_info(self):\n        """"""Fixed pointing info for this observation (`FixedPointingInfo`).""""""\n        return FixedPointingInfo(self.events.table.meta)\n\n    @property\n    def target_radec(self):\n        """"""Target RA / DEC sky coordinates (`~astropy.coordinates.SkyCoord`).""""""\n        lon, lat = (\n            self.obs_info.get(""RA_OBJ"", np.nan),\n            self.obs_info.get(""DEC_OBJ"", np.nan),\n        )\n        return SkyCoord(lon, lat, unit=""deg"", frame=""icrs"")\n\n    @property\n    def observatory_earth_location(self):\n        """"""Observatory location (`~astropy.coordinates.EarthLocation`).""""""\n        return earth_location_from_dict(self.obs_info)\n\n    @property\n    def muoneff(self):\n        """"""Observation muon efficiency.""""""\n        return self.obs_info.get(""MUONEFF"", 1)\n\n    def __str__(self):\n        ra = self.pointing_radec.ra.deg\n        dec = self.pointing_radec.dec.deg\n\n        pointing = f""{ra:.1f} deg, {dec:.1f} deg\\n""\n        # TODO: Which target was observed?\n        # TODO: print info about available HDUs for this observation ...\n        return (\n            f""{self.__class__.__name__}\\n\\n""\n            f""\\tobs id            : {self.obs_id} \\n ""\n            f""\\ttstart            : {self.tstart.mjd:.2f}\\n""\n            f""\\ttstop             : {self.tstop.mjd:.2f}\\n""\n            f""\\tduration          : {self.observation_time_duration:.2f}\\n""\n            f""\\tpointing (icrs)   : {pointing}\\n""\n            f""\\tdeadtime fraction : {self.observation_dead_time_fraction:.1%}\\n""\n        )\n\n    def check(self, checks=""all""):\n        """"""Run checks.\n\n        This is a generator that yields a list of dicts.\n        """"""\n        checker = ObservationChecker(self)\n        return checker.run(checks=checks)\n\n    def peek(self, figsize=(12, 10)):\n        """"""Quick-look plots in a few panels.\n\n        Parameters\n        ----------\n        figszie : tuple\n            Figure size\n        """"""\n        import matplotlib.pyplot as plt\n\n        fig, ((ax_aeff, ax_bkg), (ax_psf, ax_edisp)) = plt.subplots(\n            nrows=2,\n            ncols=2,\n            figsize=figsize,\n            gridspec_kw={""wspace"": 0.25, ""hspace"": 0.25},\n        )\n\n        self.aeff.plot(ax=ax_aeff)\n\n        try:\n            if isinstance(self.bkg, Background3D):\n                bkg = self.bkg.to_2d()\n            else:\n                bkg = self.bkg\n\n            bkg.plot(ax=ax_bkg)\n        except IndexError:\n            logging.warning(f""No background model found for obs {self.obs_id}."")\n\n        self.psf.plot_containment_vs_energy(ax=ax_psf)\n        self.edisp.plot_bias(ax=ax_edisp, add_cbar=True)\n\n        ax_aeff.set_title(""Effective area"")\n        ax_bkg.set_title(""Background rate"")\n        ax_psf.set_title(""Point spread function"")\n        ax_edisp.set_title(""Energy dispersion"")\n\n    def select_time(self, time_interval):\n        """"""Select a time interval of the observation.\n\n        Parameters\n        ----------\n        time_interval : `astropy.time.Time`\n            Start and stop time of the selected time interval.\n            For now we only support a single time interval.\n\n        Returns\n        -------\n        new_obs : `~gammapy.data.Observation`\n            A new observation instance of the specified time interval\n        """"""\n        new_obs_filter = self.obs_filter.copy()\n        new_obs_filter.time_filter = time_interval\n        obs = copy.deepcopy(self)\n        obs.obs_filter = new_obs_filter\n        return obs\n\n\nclass Observations(collections.abc.MutableSequence):\n    """"""Container class that holds a list of observations.\n\n    Parameters\n    ----------\n    observations : list\n        A list of `~gammapy.data.Observation`\n    """"""\n\n    def __init__(self, observations=None):\n        self._observations = observations or []\n\n    def __getitem__(self, key):\n        return self._observations[self.index(key)]\n\n    def __delitem__(self, key):\n        del self._observations[self.index(key)]\n\n    def __setitem__(self, key, obs):\n        if isinstance(obs, Observation):\n            self._observations[self.index(key)] = obs\n        else:\n            raise TypeError(f""Invalid type: {type(obs)!r}"")\n\n    def insert(self, idx, obs):\n        if isinstance(obs, Observation):\n            self._observations.insert(idx, obs)\n        else:\n            raise TypeError(f""Invalid type: {type(obs)!r}"")\n\n    def __len__(self):\n        return len(self._observations)\n\n    def __str__(self):\n        s = self.__class__.__name__ + ""\\n""\n        s += ""Number of observations: {}\\n"".format(len(self))\n        for obs in self:\n            s += str(obs)\n        return s\n\n    def index(self, key):\n        if isinstance(key, (int, slice)):\n            return key\n        elif isinstance(key, str):\n            return self.ids.index(key)\n        elif isinstance(key, Observation):\n            return self._observations.index(key)\n        else:\n            raise TypeError(f""Invalid type: {type(key)!r}"")\n\n    @property\n    def ids(self):\n        """"""List of obs IDs (`list`)""""""\n        return [str(obs.obs_id) for obs in self]\n\n    def select_time(self, time_intervals):\n        """"""Select a time interval of the observations.\n\n        Parameters\n        ----------\n        time_intervals : `astropy.time.Time` or list of `astropy.time.Time`\n            list of Start and stop time of the time intervals or one Time interval\n\n        Returns\n        -------\n        new_observations : `~gammapy.data.Observations`\n            A new Observations instance of the specified time intervals\n        """"""\n        new_obs_list = []\n        if isinstance(time_intervals, Time):\n            time_intervals = [time_intervals]\n\n        for time_interval in time_intervals:\n            for obs in self:\n                if (obs.tstart < time_interval[1]) & (obs.tstop > time_interval[0]):\n                    new_obs = obs.select_time(time_interval)\n                    new_obs_list.append(new_obs)\n\n        return self.__class__(new_obs_list)\n\n    def _ipython_key_completions_(self):\n        return self.ids\n\n\nclass ObservationChecker(Checker):\n    """"""Check an observation.\n\n    Checks data format and a bit about the content.\n    """"""\n\n    CHECKS = {\n        ""events"": ""check_events"",\n        ""gti"": ""check_gti"",\n        ""aeff"": ""check_aeff"",\n        ""edisp"": ""check_edisp"",\n        ""psf"": ""check_psf"",\n    }\n\n    def __init__(self, observation):\n        self.observation = observation\n\n    def _record(self, level=""info"", msg=None):\n        return {""level"": level, ""obs_id"": self.observation.obs_id, ""msg"": msg}\n\n    def check_events(self):\n        yield self._record(level=""debug"", msg=""Starting events check"")\n\n        try:\n            events = self.observation.events\n        except Exception:\n            yield self._record(level=""warning"", msg=""Loading events failed"")\n            return\n\n        yield from EventListChecker(events).run()\n\n    # TODO: split this out into a GTIChecker\n    def check_gti(self):\n        yield self._record(level=""debug"", msg=""Starting gti check"")\n\n        try:\n            gti = self.observation.gti\n        except Exception:\n            yield self._record(level=""warning"", msg=""Loading GTI failed"")\n            return\n\n        if len(gti.table) == 0:\n            yield self._record(level=""error"", msg=""GTI table has zero rows"")\n\n        columns_required = [""START"", ""STOP""]\n        for name in columns_required:\n            if name not in gti.table.colnames:\n                yield self._record(level=""error"", msg=f""Missing table column: {name!r}"")\n\n        # TODO: Check that header keywords agree with table entries\n        # TSTART, TSTOP, MJDREFI, MJDREFF\n\n        # Check that START and STOP times are consecutive\n        # times = np.ravel(self.table[\'START\'], self.table[\'STOP\'])\n        # # TODO: not sure this is correct ... add test with a multi-gti table from Fermi.\n        # if not np.all(np.diff(times) >= 0):\n        #     yield \'GTIs are not consecutive or sorted.\'\n\n    # TODO: add reference times for all instruments and check for this\n    # Use TELESCOP header key to check which instrument it is.\n    def _check_times(self):\n        """"""Check if various times are consistent.\n\n        The headers and tables of the FITS EVENTS and GTI extension\n        contain various observation and event time information.\n        """"""\n        # http://fermi.gsfc.nasa.gov/ssc/data/analysis/documentation/Cicerone/Cicerone_Data/Time_in_ScienceTools.html\n        # https://hess-confluence.desy.de/confluence/display/HESS/HESS+FITS+data+-+References+and+checks#HESSFITSdata-Referencesandchecks-Time\n        telescope_met_refs = {\n            ""FERMI"": Time(""2001-01-01T00:00:00""),\n            ""HESS"": Time(""2001-01-01T00:00:00""),\n        }\n\n        meta = self.dset.event_list.table.meta\n        telescope = meta[""TELESCOP""]\n\n        if telescope in telescope_met_refs.keys():\n            dt = self.time_ref - telescope_met_refs[telescope]\n            if dt > self.accuracy[""time""]:\n                yield self._record(\n                    level=""error"", msg=""Reference time incorrect for telescope""\n                )\n\n    def check_aeff(self):\n        yield self._record(level=""debug"", msg=""Starting aeff check"")\n\n        try:\n            aeff = self.observation.aeff\n        except Exception:\n            yield self._record(level=""warning"", msg=""Loading aeff failed"")\n            return\n\n        # Check that thresholds are meaningful for aeff\n        if (\n            ""LO_THRES"" in aeff.meta\n            and ""HI_THRES"" in aeff.meta\n            and aeff.meta[""LO_THRES""] >= aeff.meta[""HI_THRES""]\n        ):\n            yield self._record(\n                level=""error"", msg=""LO_THRES >= HI_THRES in effective area meta data""\n            )\n\n        # Check that data isn\'t all null\n        if np.max(aeff.data.data) <= 0:\n            yield self._record(\n                level=""error"", msg=""maximum entry of effective area is <= 0""\n            )\n\n    def check_edisp(self):\n        yield self._record(level=""debug"", msg=""Starting edisp check"")\n\n        try:\n            edisp = self.observation.edisp\n        except Exception:\n            yield self._record(level=""warning"", msg=""Loading edisp failed"")\n            return\n\n        # Check that data isn\'t all null\n        if np.max(edisp.data.data) <= 0:\n            yield self._record(level=""error"", msg=""maximum entry of edisp is <= 0"")\n\n    def check_psf(self):\n        yield self._record(level=""debug"", msg=""Starting psf check"")\n\n        try:\n            self.observation.psf\n        except Exception:\n            yield self._record(level=""warning"", msg=""Loading psf failed"")\n            return\n\n        # TODO: implement some basic check\n        # The following doesn\'t work, because EnergyDependentMultiGaussPSF\n        # has no attribute `data`\n        # Check that data isn\'t all null\n        # if np.max(psf.data.data) <= 0:\n        #     yield self._record(\n        #         level=""error"", msg=""maximum entry of psf is <= 0""\n        #     )\n'"
gammapy/data/observers.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Location of gamma-ray observatories.""""""\nfrom astropy.coordinates import EarthLocation\n\n__all__ = [""observatory_locations""]\n\n\nobservatory_locations = {}\n""""""Gamma-ray observatory locations (dict).\n\nThis is a dict with observatory names as keys\nand values of type `~astropy.coordinates.EarthLocation`.\n\nNot that with ``EarthLocation`` the orientation of angles is as follows:\n\n- longitude is east for positive values and west for negative values\n- latitude is north for positive values and south for negative values\n\nAvailable observatories (alphabetical order):\n\n- ``cta_south`` and ``cta_north`` for CTA, see\n  `Website <https://www.cta-observatory.org/>`__ and\n  `Wikipedia <https://en.wikipedia.org/wiki/Cherenkov_Telescope_Array>`__\n- ``hawc`` for HAWC, see\n  `Website <https://www.hawc-observatory.org/>`__ and\n  `Wikipedia <https://en.wikipedia.org/wiki/High_Altitude_Water_Cherenkov_Experiment>`__\n- ``hegra`` for HEGRA, see `Wikipedia <https://en.wikipedia.org/wiki/HEGRA>`__\n- ``hess`` for HESS, see\n  `Website <https://www.mpi-hd.mpg.de/hfm/HESS/>`__ and\n  `Wikipedia <https://en.wikipedia.org/wiki/HESS>`__\n- ``magic`` for MAGIC, see\n  `Website <https://wwwmagic.mpp.mpg.de/>`__ and\n  `Wikipedia <https://en.wikipedia.org/wiki/MAGIC_(telescope)>`__\n- ``milagro`` for MILAGRO, see\n  `Wikipedia <https://en.wikipedia.org/wiki/Milagro_(experiment)>`__)\n- ``veritas`` for VERITAS, see\n  `Website <https://veritas.sao.arizona.edu/>`__ and `Wikipedia <https://en.wikipedia.org/wiki/VERITAS>`__\n- ``whipple`` for WHIPPLE, see `Wikipedia <https://en.wikipedia.org/wiki/Fred_Lawrence_Whipple_Observatory>`__\n\nExamples\n--------\n\n>>> from gammapy.data import observatory_locations\n>>> observatory_locations[\'hess\']\n>>> list(observatory_locations.keys())\n""""""\n\n# Values from https://www.cta-observatory.org/about/array-locations/chile/\n# Latitude: 24d41m0.34s South, Longitude: 70d18m58.84s West, Height: not given\n# Email from Gernot Maier on Sep 8, 2017, stating what they use in the CTA MC group:\n# lon=-70.31634499364885d, lat=-24.68342915473787d, height=2150m\nobservatory_locations[""cta_south""] = EarthLocation(\n    lon=""-70d18m58.84s"", lat=""-24d41m0.34s"", height=""2150m""\n)\n\n# Values from https://www.cta-observatory.org/about/array-locations/la-palma/\n# Latitude: 28d45m43.7904s North, Longitude: 17d53m31.218s West, Height: 2200 m\n# Email from Gernot Maier on Sep 8, 2017, stating what they use in the CTA MC group for MST-1:\n# lon=-17.891571d, lat=28.762158d, height=2147m\nobservatory_locations[""cta_north""] = EarthLocation(\n    lon=""-17d53m31.218s"", lat=""28d45m43.7904s"", height=""2147m""\n)\n\n# HAWC location taken from https://arxiv.org/pdf/1108.6034v2.pdf\nobservatory_locations[""hawc""] = EarthLocation(\n    lon=""-97d18m34s"", lat=""18d59m48s"", height=""4100m""\n)\n\n# https://en.wikipedia.org/wiki/HEGRA\nobservatory_locations[""hegra""] = EarthLocation(\n    lon=""28d45m42s"", lat=""17d53m27s"", height=""2200m""\n)\n\n# Precision position of HESS from the HESS software (slightly different from Wikipedia)\nobservatory_locations[""hess""] = EarthLocation(\n    lon=""16d30m00.8s"", lat=""-23d16m18.4s"", height=""1835m""\n)\n\nobservatory_locations[""magic""] = EarthLocation(\n    lon=""-17d53m24s"", lat=""28d45m43s"", height=""2200m""\n)\n\nobservatory_locations[""milagro""] = EarthLocation(\n    lon=""-106.67625d"", lat=""35.87835d"", height=""2530m""\n)\n\nobservatory_locations[""veritas""] = EarthLocation(\n    lon=""-110d57m07.77s"", lat=""31d40m30.21s"", height=""1268m""\n)\n\n# WHIPPLE coordinates taken from the Observatory Wikipedia page:\n# https://en.wikipedia.org/wiki/Fred_Lawrence_Whipple_Observatory\nobservatory_locations[""whipple""] = EarthLocation(\n    lon=""-110d52m42s"", lat=""31d40m52s"", height=""2606m""\n)\n'"
gammapy/data/pointing.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport scipy.interpolate\nfrom astropy.coordinates import AltAz, CartesianRepresentation, SkyCoord\nfrom astropy.table import Table\nfrom astropy.units import Quantity\nfrom astropy.utils import lazyproperty\nfrom gammapy.utils.fits import earth_location_from_dict\nfrom gammapy.utils.scripts import make_path\nfrom gammapy.utils.time import time_ref_from_dict\n\n__all__ = [""FixedPointingInfo"", ""PointingInfo""]\n\n\nclass FixedPointingInfo:\n    """"""IACT array pointing info.\n\n    Data format specification: :ref:`gadf:iact-pnt`\n\n    Parameters\n    ----------\n    meta : `~astropy.table.Table.meta`\n        Meta header info from Table on pointing\n\n    Examples\n    --------\n    >>> from gammapy.data import PointingInfo\n    >>> path = \'$GAMMAPY_DATA/tests/pointing_table.fits.gz\'\n    >>> pointing_info = PointingInfo.read(path)\n    >>> print(pointing_info)\n\n    Note: In order to reproduce the example you need the tests datasets folder.\n    You may download it with the command\n    ``gammapy download datasets --tests --out $GAMMAPY_DATA``\n    """"""\n\n    def __init__(self, meta):\n        self.meta = meta\n\n    @classmethod\n    def read(cls, filename, hdu=""EVENTS""):\n        """"""Read pointing information table from file to obtain the metadata.\n\n        Parameters\n        ----------\n        filename : str\n            File name\n        hdu : int or str\n            HDU number or name\n\n        Returns\n        -------\n        pointing_info : `PointingInfo`\n            Pointing info object\n        """"""\n        filename = make_path(filename)\n        table = Table.read(filename, hdu=hdu)\n        return cls(meta=table.meta)\n\n    @lazyproperty\n    def location(self):\n        """"""Observatory location (`~astropy.coordinates.EarthLocation`).""""""\n        return earth_location_from_dict(self.meta)\n\n    @lazyproperty\n    def time_ref(self):\n        """"""Time reference (`~astropy.time.Time`).""""""\n        return time_ref_from_dict(self.meta)\n\n    @lazyproperty\n    def time_start(self):\n        """"""Start time (`~astropy.time.Time`).""""""\n        t_start = Quantity(self.meta[""TSTART""], ""second"")\n        return self.time_ref + t_start\n\n    @lazyproperty\n    def time_stop(self):\n        """"""Stop time (`~astropy.time.Time`).""""""\n        t_stop = Quantity(self.meta[""TSTOP""], ""second"")\n        return self.time_ref + t_stop\n\n    @lazyproperty\n    def obstime(self):\n        """"""Average observation time for the observation (`~astropy.time.Time`).""""""\n        return self.time_start + self.duration / 2\n\n    @lazyproperty\n    def duration(self):\n        """"""Pointing duration (`~astropy.time.TimeDelta`).\n\n        The time difference between the TSTART and TSTOP.\n        """"""\n        return self.time_stop - self.time_start\n\n    @lazyproperty\n    def radec(self):\n        """"""RA/DEC pointing position from table (`~astropy.coordinates.SkyCoord`).""""""\n        ra = self.meta[""RA_PNT""]\n        dec = self.meta[""DEC_PNT""]\n        return SkyCoord(ra, dec, unit=""deg"", frame=""icrs"")\n\n    @lazyproperty\n    def altaz_frame(self):\n        """"""ALT / AZ frame (`~astropy.coordinates.AltAz`).""""""\n        return AltAz(obstime=self.obstime, location=self.location)\n\n    @lazyproperty\n    def altaz(self):\n        """"""ALT/AZ pointing position computed from RA/DEC (`~astropy.coordinates.SkyCoord`).""""""\n        return self.radec.transform_to(self.altaz_frame)\n\n\nclass PointingInfo:\n    """"""IACT array pointing info.\n\n    Data format specification: :ref:`gadf:iact-pnt`\n\n    Parameters\n    ----------\n    table : `~astropy.table.Table`\n        Table (with meta header info) on pointing\n\n    Examples\n    --------\n    >>> from gammapy.data import PointingInfo\n    >>> pointing_info = PointingInfo.read(\'$GAMMAPY_DATA/tests/pointing_table.fits.gz\')\n    >>> print(pointing_info)\n\n    Note: In order to reproduce the example you need the tests datasets folder.\n    You may download it with the command\n    ``gammapy download datasets --tests --out $GAMMAPY_DATA``\n    """"""\n\n    def __init__(self, table):\n        self.table = table\n\n    @classmethod\n    def read(cls, filename, hdu=""POINTING""):\n        """"""Read `PointingInfo` table from file.\n\n        Parameters\n        ----------\n        filename : str\n            File name\n        hdu : int or str\n            HDU number or name\n\n        Returns\n        -------\n        pointing_info : `PointingInfo`\n            Pointing info object\n        """"""\n        filename = make_path(filename)\n        table = Table.read(filename, hdu=hdu)\n        return cls(table=table)\n\n    def __str__(self):\n        ss = ""Pointing info:\\n\\n""\n        ss += f""Location:     {self.location.geodetic}\\n""\n        m = self.table.meta\n        ss += ""MJDREFI, MJDREFF, TIMESYS = {}\\n"".format(\n            (m[""MJDREFI""], m[""MJDREFF""], m[""TIMESYS""])\n        )\n        ss += f""Time ref:     {self.time_ref.fits}\\n""\n        ss += f""Time ref:     {self.time_ref.mjd} MJD (TT)\\n""\n        sec = self.duration.to(""second"").value\n        hour = self.duration.to(""hour"").value\n        ss += f""Duration:     {sec} sec = {hour} hours\\n""\n        ss += ""Table length: {}\\n"".format(len(self.table))\n\n        ss += ""\\nSTART:\\n"" + self._str_for_index(0) + ""\\n""\n        ss += ""\\nEND:\\n"" + self._str_for_index(-1) + ""\\n""\n\n        return ss\n\n    def _str_for_index(self, idx):\n        """"""Information for one point in the pointing table.""""""\n        ss = ""Time:  {}\\n"".format(self.time[idx].fits)\n        ss += ""Time:  {} MJD (TT)\\n"".format(self.time[idx].mjd)\n        ss += ""RADEC: {} deg\\n"".format(self.radec[idx].to_string())\n        ss += ""ALTAZ: {} deg\\n"".format(self.altaz[idx].to_string())\n        return ss\n\n    @lazyproperty\n    def location(self):\n        """"""Observatory location (`~astropy.coordinates.EarthLocation`).""""""\n        return earth_location_from_dict(self.table.meta)\n\n    @lazyproperty\n    def time_ref(self):\n        """"""Time reference (`~astropy.time.Time`).""""""\n        return time_ref_from_dict(self.table.meta)\n\n    @lazyproperty\n    def duration(self):\n        """"""Pointing table duration (`~astropy.time.TimeDelta`).\n\n        The time difference between the first and last entry.\n        """"""\n        return self.time[-1] - self.time[0]\n\n    @lazyproperty\n    def time(self):\n        """"""Time array (`~astropy.time.Time`).""""""\n        met = Quantity(self.table[""TIME""].astype(""float64""), ""second"")\n        time = self.time_ref + met\n        return time.tt\n\n    @lazyproperty\n    def radec(self):\n        """"""RA / DEC position from table (`~astropy.coordinates.SkyCoord`).""""""\n        lon = self.table[""RA_PNT""]\n        lat = self.table[""DEC_PNT""]\n        return SkyCoord(lon, lat, unit=""deg"", frame=""icrs"")\n\n    @lazyproperty\n    def altaz_frame(self):\n        """"""ALT / AZ frame (`~astropy.coordinates.AltAz`).""""""\n        return AltAz(obstime=self.time, location=self.location)\n\n    @lazyproperty\n    def altaz(self):\n        """"""ALT / AZ position computed from RA / DEC (`~astropy.coordinates.SkyCoord`).""""""\n        return self.radec.transform_to(self.altaz_frame)\n\n    @lazyproperty\n    def altaz_from_table(self):\n        """"""ALT / AZ position from table (`~astropy.coordinates.SkyCoord`).""""""\n        lon = self.table[""AZ_PNT""]\n        lat = self.table[""ALT_PNT""]\n        return SkyCoord(lon, lat, unit=""deg"", frame=self.altaz_frame)\n\n    def altaz_interpolate(self, time):\n        """"""Interpolate pointing for a given time.""""""\n        t_new = time.mjd\n        t = self.time.mjd\n        xyz = self.altaz.cartesian\n        x_new = scipy.interpolate.interp1d(t, xyz.x)(t_new)\n        y_new = scipy.interpolate.interp1d(t, xyz.y)(t_new)\n        z_new = scipy.interpolate.interp1d(t, xyz.z)(t_new)\n        xyz_new = CartesianRepresentation(x_new, y_new, z_new)\n        altaz_frame = AltAz(obstime=time, location=self.location)\n        return SkyCoord(\n            xyz_new, frame=altaz_frame, unit=""deg"", representation_type=""unitspherical""\n        )\n'"
gammapy/datasets/__init__.py,0,"b'from gammapy.utils.registry import Registry\nfrom .core import *\nfrom .flux_points import *\nfrom .map import *\nfrom .simulate import *\nfrom .spectrum import *\n\nDATASETS = Registry([MapDataset, SpectrumDatasetOnOff, FluxPointsDataset])\n\n__all__ = [""Dataset"", ""Datasets"", ""MapDatasetOnOff"", ""SpectrumDataset""]\n__all__.extend(cls.__name__ for cls in DATASETS)\n'"
gammapy/datasets/core.py,3,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport abc\nimport collections.abc\nimport copy\nimport numpy as np\nfrom gammapy.maps import Map\nfrom gammapy.modeling.models import Models, ProperModels\nfrom gammapy.utils.scripts import make_name, make_path, read_yaml, write_yaml\nfrom gammapy.utils.table import table_from_row_data\n\n__all__ = [""Dataset"", ""Datasets""]\n\n\nclass Dataset(abc.ABC):\n    """"""Dataset abstract base class.\n\n    TODO: add tutorial how to create your own dataset types.\n\n    For now, see existing examples in Gammapy how this works:\n\n    - `gammapy.cube.MapDataset`\n    - `gammapy.spectrum.SpectrumDataset`\n    - `gammapy.spectrum.FluxPointsDataset`\n    """"""\n\n    _residuals_labels = {\n        ""diff"": ""data - model"",\n        ""diff/model"": ""(data - model) / model"",\n        ""diff/sqrt(model)"": ""(data - model) / sqrt(model)"",\n    }\n\n    @property\n    def mask(self):\n        """"""Combined fit and safe mask""""""\n        mask_safe = (\n            self.mask_safe.data if isinstance(self.mask_safe, Map) else self.mask_safe\n        )\n        mask_fit = (\n            self.mask_fit.data if isinstance(self.mask_fit, Map) else self.mask_fit\n        )\n        if mask_safe is not None and mask_fit is not None:\n            mask = mask_safe & mask_fit\n        elif mask_fit is not None:\n            mask = mask_fit\n        elif mask_safe is not None:\n            mask = mask_safe\n        else:\n            mask = None\n        return mask\n\n    def stat_sum(self):\n        """"""Total statistic given the current model parameters.""""""\n        stat = self.stat_array()\n\n        if self.mask is not None:\n            stat = stat[self.mask]\n\n        return np.sum(stat, dtype=np.float64)\n\n    @abc.abstractmethod\n    def stat_array(self):\n        """"""Statistic array, one value per data point.""""""\n\n    def copy(self, name=None):\n        """"""A deep copy.""""""\n        new = copy.deepcopy(self)\n        name = make_name(name)\n        new._name = name\n        # propagate new dataset name\n        if new._models is not None:\n            for m in new._models:\n                if m.datasets_names is not None:\n                    for k, d in enumerate(m.datasets_names):\n                        if d == self.name:\n                            m.datasets_names[k] = name\n                    if hasattr(new, ""background_model"") and m == new.background_model:\n                        m._name = name + ""-bkg""\n        return new\n\n    @staticmethod\n    def _compute_residuals(data, model, method=""diff""):\n        with np.errstate(invalid=""ignore""):\n            if method == ""diff"":\n                residuals = data - model\n            elif method == ""diff/model"":\n                residuals = (data - model) / model\n            elif method == ""diff/sqrt(model)"":\n                residuals = (data - model) / np.sqrt(model)\n            else:\n                raise AttributeError(\n                    f""Invalid method: {method!r}. Choose between \'diff\',""\n                    "" \'diff/model\' and \'diff/sqrt(model)\'""\n                )\n        return residuals\n\n\nclass Datasets(collections.abc.MutableSequence):\n    """"""Dataset collection.\n\n    Parameters\n    ----------\n    datasets : `Dataset` or list of `Dataset`\n        Datasets\n    """"""\n\n    def __init__(self, datasets=None):\n        if datasets is None:\n            datasets = []\n\n        if isinstance(datasets, Datasets):\n            datasets = datasets._datasets\n        elif isinstance(datasets, Dataset):\n            datasets = [datasets]\n        elif not isinstance(datasets, list):\n            raise TypeError(f""Invalid type: {datasets!r}"")\n\n        unique_names = []\n        for dataset in datasets:\n            if dataset.name in unique_names:\n                raise (ValueError(""Dataset names must be unique""))\n            unique_names.append(dataset.name)\n\n        self._datasets = datasets\n\n    @property\n    def parameters(self):\n        """"""Unique parameters (`~gammapy.modeling.Parameters`).\n\n        Duplicate parameter objects have been removed.\n        The order of the unique parameters remains.\n        """"""\n        return self.models.parameters.unique_parameters\n\n    @property\n    def models(self):\n        """"""Unique models (`~gammapy.modeling.Models`).\n\n        Duplicate model objects have been removed.\n        The order of the unique models remains.\n        """"""\n        return ProperModels(self)\n\n    @property\n    def names(self):\n        return [d.name for d in self._datasets]\n\n    @property\n    def is_all_same_type(self):\n        """"""Whether all contained datasets are of the same type""""""\n        return len(set(_.__class__ for _ in self)) == 1\n\n    @property\n    def is_all_same_shape(self):\n        """"""Whether all contained datasets have the same data shape""""""\n        return len(set(_.data_shape for _ in self)) == 1\n\n    @property\n    def is_all_same_energy_shape(self):\n        """"""Whether all contained datasets have the same data shape""""""\n        return len(set(_.data_shape[0] for _ in self)) == 1\n\n    def stat_sum(self):\n        """"""Compute joint likelihood""""""\n        stat_sum = 0\n        # TODO: add parallel evaluation of likelihoods\n        for dataset in self:\n            stat_sum += dataset.stat_sum()\n        return stat_sum\n\n    def __str__(self):\n        str_ = self.__class__.__name__ + ""\\n""\n        str_ += ""--------\\n""\n\n        for idx, dataset in enumerate(self):\n            str_ += f""idx={idx}, id={hex(id(dataset))!r}, name={dataset.name!r}\\n""\n\n        return str_\n\n    def copy(self):\n        """"""A deep copy.""""""\n        return copy.deepcopy(self)\n\n    @classmethod\n    def read(cls, path, filedata=""_datasets.yaml"", filemodel=""_models.yaml""):\n        """"""De-serialize datasets from YAML and FITS files.\n\n        Parameters\n        ----------\n        path : str, Path\n            Base directory of the datasets files.\n        filedata : str\n            file path or name of yaml datasets file\n        filemodel : str\n            file path or name of yaml models file\n\n        Returns\n        -------\n        dataset : \'gammapy.modeling.Datasets\'\n            Datasets\n        """"""\n        from . import DATASETS\n\n        path = make_path(path)\n\n        if (path / filedata).exists():\n            filedata = path / filedata\n        else:\n            filedata = make_path(filedata)\n        if (path / filemodel).exists():\n            filemodel = path / filemodel\n        else:\n            filemodel = make_path(filemodel)\n\n        models = Models.read(filemodel)\n        data_list = read_yaml(filedata)\n\n        datasets = []\n        for data in data_list[""datasets""]:\n            if (path / data[""filename""]).exists():\n                data[""filename""] = str(make_path(path / data[""filename""]))\n            dataset = DATASETS.get_cls(data[""type""]).from_dict(data, models)\n            datasets.append(dataset)\n        return cls(datasets)\n\n    def write(self, path, prefix="""", overwrite=False):\n        """"""Serialize datasets to YAML and FITS files.\n\n        Parameters\n        ----------\n        path : `pathlib.Path`\n            path to write files\n        prefix : str\n            common prefix of file names\n        overwrite : bool\n            overwrite datasets FITS files\n        """"""\n\n        path = make_path(path).resolve()\n        datasets_dictlist = []\n        for dataset in self._datasets:\n            filename = f""{prefix}_data_{dataset.name}.fits""\n            dataset.write(path / filename, overwrite)\n            datasets_dictlist.append(dataset.to_dict(filename=filename))\n        datasets_dict = {""datasets"": datasets_dictlist}\n\n        write_yaml(datasets_dict, path / f""{prefix}_datasets.yaml"", sort_keys=False)\n        self.models.write(path / f""{prefix}_models.yaml"", overwrite=overwrite)\n\n    def stack_reduce(self, name=None):\n        """"""Reduce the Datasets to a unique Dataset by stacking them together.\n\n        This works only if all Dataset are of the same type and if a proper\n        in-place stack method exists for the Dataset type.\n\n        Returns\n        -------\n        dataset : ~gammapy.utils.Dataset\n            the stacked dataset\n        """"""\n        if not self.is_all_same_type:\n            raise ValueError(\n                ""Stacking impossible: all Datasets contained are not of a unique type.""\n            )\n\n        dataset = self[0].copy(name=name)\n        for ds in self[1:]:\n            dataset.stack(ds)\n        return dataset\n\n    def info_table(self, cumulative=False, region=None):\n        """"""Get info table for datasets.\n\n        Parameters\n        ----------\n        cumulative : bool\n            Cumulate info across all observations\n\n        Returns\n        -------\n        info_table : `~astropy.table.Table`\n            Info table.\n        """"""\n        if not self.is_all_same_type:\n            raise ValueError(""Info table not supported for mixed dataset type."")\n\n        stacked = self[0].copy(name=self[0].name)\n\n        rows = [stacked.info_dict()]\n\n        for dataset in self[1:]:\n            if cumulative:\n                stacked.stack(dataset)\n                row = stacked.info_dict()\n            else:\n                row = dataset.info_dict()\n\n            rows.append(row)\n\n        return table_from_row_data(rows=rows)\n\n    def __getitem__(self, key):\n        return self._datasets[self.index(key)]\n\n    def __delitem__(self, key):\n        del self._datasets[self.index(key)]\n\n    def __setitem__(self, key, dataset):\n        if isinstance(dataset, Dataset):\n            if dataset.name in self.names:\n                raise (ValueError(""Dataset names must be unique""))\n            self._datasets[self.index(key)] = dataset\n        else:\n            raise TypeError(f""Invalid type: {type(dataset)!r}"")\n\n    def insert(self, idx, dataset):\n        if isinstance(dataset, Dataset):\n            if dataset.name in self.names:\n                raise (ValueError(""Dataset names must be unique""))\n            self._datasets.insert(idx, dataset)\n        else:\n            raise TypeError(f""Invalid type: {type(dataset)!r}"")\n\n    def index(self, key):\n        if isinstance(key, (int, slice)):\n            return key\n        elif isinstance(key, str):\n            return self.names.index(key)\n        elif isinstance(key, Dataset):\n            return self._datasets.index(key)\n        else:\n            raise TypeError(f""Invalid type: {type(key)!r}"")\n\n    def __len__(self):\n        return len(self._datasets)\n'"
gammapy/datasets/flux_points.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport numpy as np\nfrom astropy import units as u\nfrom astropy.table import Table\nfrom gammapy.modeling.models import Models, ProperModels\nfrom gammapy.utils.scripts import make_name, make_path\nfrom .core import Dataset\n\nlog = logging.getLogger(__name__)\n\n__all__ = [""FluxPointsDataset""]\n\n\nclass FluxPointsDataset(Dataset):\n    """"""\n    Fit a set of flux points with a parametric model.\n\n    Parameters\n    ----------\n    models : `~gammapy.modeling.models.Models`\n        Models (only spectral part needs to be set)\n    data : `~gammapy.spectrum.FluxPoints`\n        Flux points.\n    mask_fit : `numpy.ndarray`\n        Mask to apply for fitting\n    mask_safe : `numpy.ndarray`\n        Mask defining the safe data range.\n\n    Examples\n    --------\n    Load flux points from file and fit with a power-law model::\n\n        from gammapy.modeling import Fit\n        from gammapy.modeling.models import PowerLawSpectralModel, SkyModel\n        from gammapy.estimators import FluxPoints\n        from gammapy.datasets import FluxPointsDataset\n\n        filename = ""$GAMMAPY_DATA/tests/spectrum/flux_points/diff_flux_points.fits""\n        flux_points = FluxPoints.read(filename)\n\n        model = SkyModel(spectral_model=PowerLawSpectralModel())\n\n        dataset = FluxPointsDataset(model, flux_points)\n        fit = Fit([dataset])\n        result = fit.run()\n        print(result)\n        print(result.parameters.to_table())\n\n    Note: In order to reproduce the example you need the tests datasets folder.\n    You may download it with the command\n    ``gammapy download datasets --tests --out $GAMMAPY_DATA``\n    """"""\n\n    stat_type = ""chi2""\n    tag = ""FluxPointsDataset""\n\n    def __init__(self, models, data, mask_fit=None, mask_safe=None, name=None):\n        self.data = data\n        self.mask_fit = mask_fit\n        self._name = make_name(name)\n        self.models = models\n\n        if data.sed_type != ""dnde"":\n            raise ValueError(""Currently only flux points of type \'dnde\' are supported."")\n\n        if mask_safe is None:\n            mask_safe = np.isfinite(data.table[""dnde""])\n\n        self.mask_safe = mask_safe\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def models(self):\n        return ProperModels(self)\n\n    @models.setter\n    def models(self, models):\n        if models is None:\n            self._models = None\n        else:\n            self._models = Models(models)\n\n    def write(self, filename, overwrite=True, **kwargs):\n        """"""Write flux point dataset to file.\n\n        Parameters\n        ----------\n        filename : str\n            Filename to write to.\n        overwrite : bool\n            Overwrite existing file.\n        **kwargs : dict\n             Keyword arguments passed to `~astropy.table.Table.write`.\n        """"""\n        table = self.data.table.copy()\n        if self.mask_fit is None:\n            mask_fit = self.mask_safe\n        else:\n            mask_fit = self.mask_fit\n\n        table[""mask_fit""] = mask_fit\n        table[""mask_safe""] = self.mask_safe\n        table.write(filename, overwrite=overwrite, **kwargs)\n\n    @classmethod\n    def from_dict(cls, data, models):\n        """"""Create flux point dataset from dict.\n\n        Parameters\n        ----------\n        data : dict\n            Dict containing data to create dataset from.\n        models : list of `SkyModel`\n            List of model components.\n\n        Returns\n        -------\n        dataset : `FluxPointsDataset`\n            Flux point datasets.\n        """"""\n        from gammapy.estimators import FluxPoints\n\n        filename = make_path(data[""filename""])\n        table = Table.read(filename)\n        mask_fit = table[""mask_fit""].data.astype(""bool"")\n        mask_safe = table[""mask_safe""].data.astype(""bool"")\n        table.remove_columns([""mask_fit"", ""mask_safe""])\n        return cls(\n            models=models,\n            name=data[""name""],\n            data=FluxPoints(table),\n            mask_fit=mask_fit,\n            mask_safe=mask_safe,\n        )\n\n    def to_dict(self, filename=""""):\n        """"""Convert to dict for YAML serialization.""""""\n        return {\n            ""name"": self.name,\n            ""type"": self.tag,\n            ""filename"": str(filename),\n        }\n\n    def __str__(self):\n        str_ = f""{self.__class__.__name__}\\n""\n        str_ += ""-"" * len(self.__class__.__name__) + ""\\n""\n        str_ += ""\\n""\n\n        str_ += ""\\t{:32}: {} \\n\\n"".format(""Name"", self.name)\n\n        # data section\n        n_bins = 0\n        if self.data is not None:\n            n_bins = len(self.data.table)\n        str_ += ""\\t{:32}: {} \\n"".format(""Number of total flux points"", n_bins)\n\n        n_fit_bins = 0\n        if self.mask is not None:\n            n_fit_bins = np.sum(self.mask.data)\n        str_ += ""\\t{:32}: {} \\n\\n"".format(""Number of fit bins"", n_fit_bins)\n\n        # likelihood section\n        str_ += ""\\t{:32}: {}\\n"".format(""Fit statistic type"", self.stat_type)\n\n        stat = np.nan\n        if self.data is not None and self.models is not None:\n            stat = self.stat_sum()\n        str_ += ""\\t{:32}: {:.2f}\\n\\n"".format(""Fit statistic value (-2 log(L))"", stat)\n\n        # model section\n        n_models = 0\n        if self.models is not None:\n            n_models = len(self.models)\n\n        str_ += ""\\t{:32}: {} \\n"".format(""Number of models"", n_models)\n\n        str_ += ""\\t{:32}: {}\\n"".format(\n            ""Number of parameters"", len(self.models.parameters)\n        )\n        str_ += ""\\t{:32}: {}\\n\\n"".format(\n            ""Number of free parameters"", len(self.models.parameters.free_parameters)\n        )\n\n        if self.models is not None:\n            str_ += ""\\t"" + ""\\n\\t"".join(str(self.models).split(""\\n"")[2:])\n\n        return str_.expandtabs(tabsize=2)\n\n    def data_shape(self):\n        """"""Shape of the flux points data (tuple).""""""\n        return self.data.e_ref.shape\n\n    def flux_pred(self):\n        """"""Compute predicted flux.""""""\n        flux = 0.0\n        for model in self.models:\n            flux += model.spectral_model(self.data.e_ref)\n        return flux\n\n    def stat_array(self):\n        """"""Fit statistic array.""""""\n        model = self.flux_pred()\n        data = self.data.table[""dnde""].quantity\n        sigma = self.data.table[""dnde_err""].quantity\n        return ((data - model) / sigma).to_value("""") ** 2\n\n    def residuals(self, method=""diff""):\n        """"""Compute the flux point residuals ().\n\n        Parameters\n        ----------\n        method: {""diff"", ""diff/model"", ""diff/sqrt(model)""}\n            Method used to compute the residuals. Available options are:\n                - `diff` (default): data - model\n                - `diff/model`: (data - model) / model\n                - `diff/sqrt(model)`: (data - model) / sqrt(model)\n                - `norm=\'sqrt_model\'` for: (flux points - model)/sqrt(model)\n\n\n        Returns\n        -------\n        residuals : `~numpy.ndarray`\n            Residuals array.\n        """"""\n        fp = self.data\n        data = fp.table[fp.sed_type]\n\n        model = self.flux_pred()\n\n        residuals = self._compute_residuals(data, model, method)\n        # Remove residuals for upper_limits\n        residuals[fp.is_ul] = np.nan\n        return residuals\n\n    def peek(self, method=""diff/model"", **kwargs):\n        """"""Plot flux points, best fit model and residuals.\n\n        Parameters\n        ----------\n        method : {""diff"", ""diff/model"", ""diff/sqrt(model)""}\n            Method used to compute the residuals, see `MapDataset.residuals()`\n        """"""\n        from matplotlib.gridspec import GridSpec\n        import matplotlib.pyplot as plt\n\n        gs = GridSpec(7, 1)\n\n        ax_spectrum = plt.subplot(gs[:5, :])\n        self.plot_spectrum(ax=ax_spectrum, **kwargs)\n\n        ax_spectrum.set_xticks([])\n\n        ax_residuals = plt.subplot(gs[5:, :])\n        self.plot_residuals(ax=ax_residuals, method=method)\n        return ax_spectrum, ax_residuals\n\n    @property\n    def _e_range(self):\n        try:\n            return u.Quantity([self.data.e_min.min(), self.data.e_max.max()])\n        except KeyError:\n            return u.Quantity([self.data.e_ref.min(), self.data.e_ref.max()])\n\n    @property\n    def _e_unit(self):\n        return self.data.e_ref.unit\n\n    def plot_residuals(self, ax=None, method=""diff"", **kwargs):\n        """"""Plot flux point residuals.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.pyplot.Axes`\n            Axes object.\n        method : {""diff"", ""diff/model"", ""diff/sqrt(model)""}\n            Method used to compute the residuals, see `MapDataset.residuals()`\n        **kwargs : dict\n            Keyword arguments passed to `~matplotlib.pyplot.errorbar`.\n\n        Returns\n        -------\n        ax : `~matplotlib.pyplot.Axes`\n            Axes object.\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        residuals = self.residuals(method=method)\n\n        fp = self.data\n\n        xerr = fp._plot_get_energy_err()\n        if xerr is not None:\n            xerr = xerr[0].to_value(self._e_unit), xerr[1].to_value(self._e_unit)\n\n        model = self.flux_pred()\n        yerr = fp._plot_get_flux_err(fp.sed_type)\n\n        if method == ""diff"":\n            unit = yerr[0].unit\n            yerr = yerr[0].to_value(unit), yerr[1].to_value(unit)\n        elif method == ""diff/model"":\n            unit = """"\n            yerr = (yerr[0] / model).to_value(""""), (yerr[1] / model).to_value(unit)\n        else:\n            raise ValueError(""Invalid method, choose between \'diff\' and \'diff/model\'"")\n\n        kwargs.setdefault(""marker"", ""+"")\n        kwargs.setdefault(""ls"", ""None"")\n        kwargs.setdefault(""color"", ""black"")\n\n        ax.errorbar(\n            self.data.e_ref.value, residuals.value, xerr=xerr, yerr=yerr, **kwargs\n        )\n\n        # format axes\n        ax.axhline(0, color=""black"", lw=0.5)\n        ax.set_ylabel(""Residuals {}"".format(unit.__str__()))\n        ax.set_xlabel(f""Energy ({self._e_unit})"")\n        ax.set_xscale(""log"")\n        ax.set_xlim(self._e_range.to_value(self._e_unit))\n        y_max = 2 * np.nanmax(residuals).value\n        ax.set_ylim(-y_max, y_max)\n        return ax\n\n    def plot_spectrum(self, ax=None, fp_kwargs=None, model_kwargs=None):\n        """"""\n        Plot spectrum including flux points and model.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.pyplot.Axes`\n            Axes object.\n        fp_kwargs : dict\n            Keyword arguments passed to `FluxPoints.plot`.\n        model_kwargs : dict\n            Keywords passed to `SpectralModel.plot` and `SpectralModel.plot_error`\n\n        Returns\n        -------\n        ax : `~matplotlib.pyplot.Axes`\n            Axes object.\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n        fp_kwargs = {} if fp_kwargs is None else fp_kwargs\n        model_kwargs = {} if model_kwargs is None else model_kwargs\n\n        kwargs = {\n            ""flux_unit"": ""erg-1 cm-2 s-1"",\n            ""energy_unit"": ""TeV"",\n            ""energy_power"": 2,\n        }\n\n        # plot flux points\n        plot_kwargs = kwargs.copy()\n        plot_kwargs.update(fp_kwargs)\n        plot_kwargs.setdefault(""label"", ""Flux points"")\n        ax = self.data.plot(ax=ax, **plot_kwargs)\n\n        plot_kwargs = kwargs.copy()\n        plot_kwargs.setdefault(""energy_range"", self._e_range)\n        plot_kwargs.setdefault(""zorder"", 10)\n        plot_kwargs.update(model_kwargs)\n        plot_kwargs.setdefault(""label"", ""Best fit model"")\n\n        for model in self.models:\n            if model.datasets_names is None or self.name in model.datasets_names:\n                model.spectral_model.plot(ax=ax, **plot_kwargs)\n\n        plot_kwargs.setdefault(""color"", ax.lines[-1].get_color())\n        del plot_kwargs[""label""]\n\n        for model in self.models:\n            if model.datasets_names is None or self.name in model.datasets_names:\n                if not np.all(model == 0):\n                    model.spectral_model.plot_error(ax=ax, **plot_kwargs)\n\n        # format axes\n        ax.set_xlim(self._e_range.to_value(self._e_unit))\n        return ax\n'"
gammapy/datasets/map.py,56,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport numpy as np\nimport astropy.units as u\nfrom astropy.io import fits\nfrom astropy.nddata.utils import NoOverlapError\nfrom astropy.table import Table\nfrom astropy.utils import lazyproperty\nfrom regions import CircleSkyRegion, RectangleSkyRegion\nfrom gammapy.data import GTI\nfrom gammapy.irf import EDispKernel, EffectiveAreaTable\nfrom gammapy.irf.edisp_map import EDispMap, EDispKernelMap\nfrom gammapy.irf.psf_kernel import PSFKernel\nfrom gammapy.irf.psf_map import PSFMap\nfrom gammapy.maps import Map, MapAxis\nfrom gammapy.modeling.models import BackgroundModel, Models, ProperModels\nfrom gammapy.stats import cash, cash_sum_cython, wstat\nfrom gammapy.utils.random import get_random_state\nfrom gammapy.utils.scripts import make_name, make_path\nfrom .core import Dataset\n\n__all__ = [""MapDataset"", ""MapDatasetOnOff""]\n\nlog = logging.getLogger(__name__)\n\nCUTOUT_MARGIN = 0.1 * u.deg\nRAD_MAX = 0.66\nRAD_AXIS_DEFAULT = MapAxis.from_bounds(\n    0, RAD_MAX, nbin=66, node_type=""edges"", name=""theta"", unit=""deg""\n)\nMIGRA_AXIS_DEFAULT = MapAxis.from_bounds(\n    0.2, 5, nbin=48, node_type=""edges"", name=""migra""\n)\n\nBINSZ_IRF_DEFAULT = 0.2\n\n\nclass MapDataset(Dataset):\n    """"""Perform sky model likelihood fit on maps.\n\n    Parameters\n    ----------\n    models : `~gammapy.modeling.models.Models`\n        Source sky models.\n    counts : `~gammapy.maps.WcsNDMap`\n        Counts cube\n    exposure : `~gammapy.maps.WcsNDMap`\n        Exposure cube\n    mask_fit : `~gammapy.maps.WcsNDMap`\n        Mask to apply to the likelihood for fitting.\n    psf : `~gammapy.cube.PSFKernel` or `~gammapy.cube.PSFMap`\n        PSF kernel\n    edisp : `~gammapy.irf.EDispKernel` or `~gammapy.cube.EDispMap`\n        Energy dispersion kernel\n    evaluation_mode : {""local"", ""global""}\n        Model evaluation mode.\n        The ""local"" mode evaluates the model components on smaller grids to save computation time.\n        This mode is recommended for local optimization algorithms.\n        The ""global"" evaluation mode evaluates the model components on the full map.\n        This mode is recommended for global optimization algorithms.\n    mask_safe : `~gammapy.maps.WcsNDMap`\n        Mask defining the safe data range.\n    gti : `~gammapy.data.GTI`\n        GTI of the observation or union of GTI if it is a stacked observation\n    """"""\n\n    stat_type = ""cash""\n    tag = ""MapDataset""\n\n    def __init__(\n        self,\n        models=None,\n        counts=None,\n        exposure=None,\n        mask_fit=None,\n        psf=None,\n        edisp=None,\n        name=None,\n        evaluation_mode=""local"",\n        mask_safe=None,\n        gti=None,\n    ):\n        if mask_fit is not None and mask_fit.data.dtype != np.dtype(""bool""):\n            raise ValueError(""mask data must have dtype bool"")\n\n        if mask_safe is not None and mask_safe.data.dtype != np.dtype(""bool""):\n            raise ValueError(""mask data must have dtype bool"")\n\n        self._name = make_name(name)\n        self._background_model = None\n        self.evaluation_mode = evaluation_mode\n        self.counts = counts\n        self.exposure = exposure\n        self.mask_fit = mask_fit\n        self.psf = psf\n        self.edisp = edisp\n        self.mask_safe = mask_safe\n        self.models = models\n        self.gti = gti\n\n        # check whether a reference geom is defined\n        _ = self._geom\n\n    @property\n    def name(self):\n        return self._name\n\n    def __str__(self):\n        str_ = f""{self.__class__.__name__}\\n""\n        str_ += ""-"" * len(self.__class__.__name__) + ""\\n""\n        str_ += ""\\n""\n\n        str_ += ""\\t{:32}: {} \\n\\n"".format(""Name"", self.name)\n\n        counts = np.nan\n        if self.counts is not None:\n            counts = np.sum(self.counts.data)\n        str_ += ""\\t{:32}: {:.0f} \\n"".format(""Total counts"", counts)\n\n        npred = np.nan\n        if self.models is not None:\n            npred = np.sum(self.npred().data)\n        str_ += ""\\t{:32}: {:.2f}\\n"".format(""Total predicted counts"", npred)\n\n        background = np.nan\n        if self.background_model is not None:\n            background = np.sum(self.background_model.evaluate().data)\n        str_ += ""\\t{:32}: {:.2f}\\n\\n"".format(""Total background counts"", background)\n\n        exposure_min, exposure_max, exposure_unit = np.nan, np.nan, """"\n        if self.exposure is not None:\n            if self.mask_safe is not None:\n                mask = self.mask_safe.reduce_over_axes(np.logical_or).data\n                if not mask.any():\n                    mask = None\n            else:\n                mask = None\n            exposure_min = np.min(self.exposure.data[..., mask])\n            exposure_max = np.max(self.exposure.data[..., mask])\n            exposure_unit = self.exposure.unit\n\n        str_ += ""\\t{:32}: {:.2e} {}\\n"".format(\n            ""Exposure min"", exposure_min, exposure_unit\n        )\n        str_ += ""\\t{:32}: {:.2e} {}\\n\\n"".format(\n            ""Exposure max"", exposure_max, exposure_unit\n        )\n\n        # data section\n        n_bins = 0\n        if self.counts is not None:\n            n_bins = self.counts.data.size\n        str_ += ""\\t{:32}: {} \\n"".format(""Number of total bins"", n_bins)\n\n        n_fit_bins = 0\n        if self.mask is not None:\n            n_fit_bins = np.sum(self.mask.data)\n        str_ += ""\\t{:32}: {} \\n\\n"".format(""Number of fit bins"", n_fit_bins)\n\n        # likelihood section\n        str_ += ""\\t{:32}: {}\\n"".format(""Fit statistic type"", self.stat_type)\n\n        stat = np.nan\n        if self.counts is not None and self.models is not None:\n            stat = self.stat_sum()\n        str_ += ""\\t{:32}: {:.2f}\\n\\n"".format(""Fit statistic value (-2 log(L))"", stat)\n\n        # model section\n        n_models, n_pars, n_free_pars = 0, 0, 0\n        if self.models is not None:\n            n_models = len(self.models)\n            n_pars = len(self.models.parameters)\n            n_free_pars = len(self.models.parameters.free_parameters)\n\n        str_ += ""\\t{:32}: {} \\n"".format(""Number of models"", n_models)\n        str_ += ""\\t{:32}: {}\\n"".format(""Number of parameters"", n_pars)\n        str_ += ""\\t{:32}: {}\\n\\n"".format(""Number of free parameters"", n_free_pars)\n\n        if self.models is not None:\n            str_ += ""\\t"" + ""\\n\\t"".join(str(self.models).split(""\\n"")[2:])\n\n        return str_.expandtabs(tabsize=2)\n\n    @property\n    def models(self):\n        """"""Models (`~gammapy.modeling.models.Models`).""""""\n        return ProperModels(self)\n\n    @property\n    def background_model(self):\n        return self._background_model\n\n    @models.setter\n    def models(self, models):\n        if models is None:\n            self._models = None\n        else:\n            self._models = Models(models)\n\n        # TODO: clean this up (probably by removing)\n        if self.models is not None:\n            for model in self.models:\n                if isinstance(model, BackgroundModel):\n                    if model.datasets_names is not None:\n                        if self.name in model.datasets_names:\n                            self._background_model = model\n                            break\n            else:\n                log.warning(f""No background model defined for dataset {self.name}"")\n        self._evaluators = {}\n\n    @property\n    def evaluators(self):\n        """"""Model evaluators""""""\n\n        if self.models:\n            for model in self.models:\n                evaluator = self._evaluators.get(model)\n\n                if evaluator is None:\n                    evaluator = MapEvaluator(\n                        model=model, evaluation_mode=self.evaluation_mode, gti=self.gti\n                    )\n                    self._evaluators[model] = evaluator\n\n                # if the model component drifts out of its support the evaluator has\n                # has to be updated\n                if evaluator.needs_update:\n                    evaluator.update(self.exposure, self.psf, self.edisp, self._geom)\n\n        keys = list(self._evaluators.keys())\n        for key in keys:\n            if key not in self.models:\n                del self._evaluators[key]\n\n        return self._evaluators\n\n    @property\n    def _geom(self):\n        """"""Main analysis geometry""""""\n        if self.counts is not None:\n            return self.counts.geom\n        elif self.background_model is not None:\n            return self.background_model.map.geom\n        elif self.mask_safe is not None:\n            return self.mask_safe.geom\n        elif self.mask_fit is not None:\n            return self.mask_fit.geom\n        else:\n            raise ValueError(\n                ""Either \'counts\', \'background_model\', \'mask_fit\'""\n                "" or \'mask_safe\' must be defined.""\n            )\n\n    @property\n    def data_shape(self):\n        """"""Shape of the counts or background data (tuple)""""""\n        return self._geom.data_shape\n\n    def npred(self):\n        """"""Predicted source and background counts (`~gammapy.maps.Map`).""""""\n        npred_total = Map.from_geom(self._geom, dtype=float)\n\n        evaluators = self.evaluators\n\n        for evaluator in evaluators.values():\n            if evaluator.contributes:\n                npred = evaluator.compute_npred()\n                npred_total.stack(npred)\n\n        return npred_total\n\n    @classmethod\n    def from_geoms(\n        cls,\n        geom,\n        geom_exposure,\n        geom_psf,\n        geom_edisp,\n        reference_time=""2000-01-01"",\n        name=None,\n        **kwargs,\n    ):\n        """"""\n        Create a MapDataset object with zero filled maps according to the specified geometries\n\n        Parameters\n        ----------\n        geom : `Geom`\n            geometry for the counts and background maps\n        geom_exposure : `Geom`\n            geometry for the exposure map\n        geom_psf : `Geom`\n            geometry for the psf map\n        geom_edisp : `Geom`\n            geometry for the energy dispersion kernel map.\n            If geom_edisp has a migra axis, this wil create an EDispMap instead.\n        reference_time : `~astropy.time.Time`\n            the reference time to use in GTI definition\n        name : str\n            Name of the returned dataset.\n\n        Returns\n        -------\n        empty_maps : `MapDataset`\n            A MapDataset containing zero filled maps\n        """"""\n        name = make_name(name)\n        kwargs = kwargs.copy()\n        kwargs[""name""] = name\n        kwargs[""counts""] = Map.from_geom(geom, unit="""")\n\n        background = Map.from_geom(geom, unit="""")\n        kwargs[""models""] = Models(\n            [BackgroundModel(background, name=name + ""-bkg"", datasets_names=[name])]\n        )\n        kwargs[""exposure""] = Map.from_geom(geom_exposure, unit=""m2 s"")\n\n        if geom_edisp.axes[0].name.lower() == ""energy"":\n            kwargs[""edisp""] = EDispKernelMap.from_geom(geom_edisp)\n        else:\n            kwargs[""edisp""] = EDispMap.from_geom(geom_edisp)\n\n        kwargs[""psf""] = PSFMap.from_geom(geom_psf)\n\n        kwargs.setdefault(\n            ""gti"", GTI.create([] * u.s, [] * u.s, reference_time=reference_time)\n        )\n        kwargs[""mask_safe""] = Map.from_geom(geom, unit="""", dtype=bool)\n\n        return cls(**kwargs)\n\n    @classmethod\n    def create(\n        cls,\n        geom,\n        energy_axis_true=None,\n        migra_axis=None,\n        rad_axis=None,\n        binsz_irf=None,\n        reference_time=""2000-01-01"",\n        name=None,\n        **kwargs,\n    ):\n        """"""Create a MapDataset object with zero filled maps.\n\n        Parameters\n        ----------\n        geom : `~gammapy.maps.WcsGeom`\n            Reference target geometry in reco energy, used for counts and background maps\n        energy_axis_true : `~gammapy.maps.MapAxis`\n            True energy axis used for IRF maps\n        migra_axis : `~gammapy.maps.MapAxis`\n            If set, this provides the migration axis for the energy dispersion map.\n            If not set, an EDispKernelMap is produced instead. Default is None\n        rad_axis : `~gammapy.maps.MapAxis`\n            Rad axis for the psf map\n        binsz_irf : float\n            IRF Map pixel size in degrees.\n        reference_time : `~astropy.time.Time`\n            the reference time to use in GTI definition\n        name : str\n            Name of the returned dataset.\n\n        Returns\n        -------\n        empty_maps : `MapDataset`\n            A MapDataset containing zero filled maps\n        """"""\n        rad_axis = rad_axis or RAD_AXIS_DEFAULT\n\n        if energy_axis_true is not None:\n            if energy_axis_true.name != ""energy_true"":\n                raise ValueError(""True enery axis name must be \'energy_true\'"")\n        else:\n            energy_axis_true = geom.get_axis_by_name(""energy"").copy(name=""energy_true"")\n\n        binsz_irf = binsz_irf or BINSZ_IRF_DEFAULT\n        geom_image = geom.to_image()\n        geom_exposure = geom_image.to_cube([energy_axis_true])\n        geom_irf = geom_image.to_binsz(binsz=binsz_irf)\n        geom_psf = geom_irf.to_cube([rad_axis, energy_axis_true])\n        if migra_axis:\n            geom_edisp = geom_irf.to_cube([migra_axis, energy_axis_true])\n        else:\n            geom_edisp = geom_irf.to_cube(\n                [geom.get_axis_by_name(""energy""), energy_axis_true]\n            )\n\n        return cls.from_geoms(\n            geom,\n            geom_exposure,\n            geom_psf,\n            geom_edisp,\n            reference_time=reference_time,\n            name=name,\n            **kwargs,\n        )\n\n    def stack(self, other):\n        """"""Stack another dataset in place.\n\n        Parameters\n        ----------\n        other: `~gammapy.cube.MapDataset` or `~gammapy.cube.MapDatasetOnOff`\n            Map dataset to be stacked with this one. If other is an on-off\n            dataset alpha * counts_off is used as a background model.\n        """"""\n        if self.mask_safe is None:\n            self.mask_safe = Map.from_geom(\n                self._geom, data=np.ones_like(self.data_shape)\n            )\n\n        if other.mask_safe is None:\n            other_mask_safe = Map.from_geom(\n                other._geom, data=np.ones_like(other.data_shape)\n            )\n        else:\n            other_mask_safe = other.mask_safe\n\n        if self.counts and other.counts:\n            self.counts *= self.mask_safe\n            self.counts.stack(other.counts, weights=other_mask_safe)\n\n        if self.exposure and other.exposure:\n            mask_image = self.mask_safe.reduce_over_axes(func=np.logical_or)\n            self.exposure *= mask_image.data\n            # TODO: apply energy dependent mask to exposure. Does this require\n            #  a mask_safe in true energy?\n            mask_image_other = other_mask_safe.reduce_over_axes(func=np.logical_or)\n            self.exposure.stack(other.exposure, weights=mask_image_other)\n\n        # TODO: unify background model handling\n        if other.stat_type == ""wstat"":\n            background_model = BackgroundModel(other.background)\n        else:\n            background_model = other.background_model\n\n        if self.background_model and background_model:\n            self._background_model.map *= self.mask_safe\n            self._background_model.stack(background_model, other_mask_safe)\n            self.models = Models([self.background_model])\n        else:\n            self.models = None\n\n        if self.psf and other.psf:\n            if isinstance(self.psf, PSFMap) and isinstance(other.psf, PSFMap):\n                mask_irf = self._mask_safe_irf(self.psf.psf_map, mask_image)\n                self.psf.psf_map *= mask_irf.data\n                self.psf.exposure_map *= mask_irf.data\n\n                mask_image_other = other_mask_safe.reduce_over_axes(func=np.logical_or)\n                mask_irf_other = self._mask_safe_irf(\n                    other.psf.psf_map, mask_image_other\n                )\n                self.psf.stack(other.psf, weights=mask_irf_other)\n            else:\n                raise ValueError(""Stacking of PSF kernels not supported"")\n\n        if self.edisp and other.edisp:\n            if isinstance(self.edisp, EDispMap) and isinstance(other.edisp, EDispMap):\n                mask_irf = self._mask_safe_irf(self.edisp.edisp_map, mask_image)\n                self.edisp.edisp_map *= mask_irf.data\n                self.edisp.exposure_map *= mask_irf.data\n\n                mask_image_other = other_mask_safe.reduce_over_axes(func=np.logical_or)\n                mask_irf_other = self._mask_safe_irf(\n                    other.edisp.edisp_map, mask_image_other\n                )\n                self.edisp.stack(other.edisp, weights=mask_irf_other)\n            elif isinstance(self.edisp, EDispKernelMap) and isinstance(\n                other.edisp, EDispKernelMap\n            ):\n                mask_irf = self._mask_safe_irf(self.edisp.edisp_map, mask_image)\n                self.edisp.edisp_map *= mask_irf.data\n                self.edisp.exposure_map *= mask_irf.data\n\n                mask_image_other = other_mask_safe.reduce_over_axes(func=np.logical_or)\n                mask_irf_other = self._mask_safe_irf(\n                    other.edisp.edisp_map, mask_image_other\n                )\n                self.edisp.stack(other.edisp, weights=mask_irf_other)\n            else:\n                raise ValueError(""Stacking of edisp kernels not supported"")\n\n        self.mask_safe.stack(other_mask_safe)\n\n        if self.gti and other.gti:\n            self.gti = self.gti.stack(other.gti).union()\n\n    @staticmethod\n    def _mask_safe_irf(irf_map, mask):\n        geom = irf_map.geom.to_image()\n        coords = geom.get_coord()\n        data = mask.get_by_coord(coords).astype(bool)\n        return Map.from_geom(geom=geom, data=data)\n\n    def stat_array(self):\n        """"""Likelihood per bin given the current model parameters""""""\n        return cash(n_on=self.counts.data, mu_on=self.npred().data)\n\n    def residuals(self, method=""diff""):\n        """"""Compute residuals map.\n\n        Parameters\n        ----------\n        method: {""diff"", ""diff/model"", ""diff/sqrt(model)""}\n            Method used to compute the residuals. Available options are:\n                - ""diff"" (default): data - model\n                - ""diff/model"": (data - model) / model\n                - ""diff/sqrt(model)"": (data - model) / sqrt(model)\n\n        Returns\n        -------\n        residuals : `gammapy.maps.WcsNDMap`\n            Residual map.\n        """"""\n        return self._compute_residuals(self.counts, self.npred(), method=method)\n\n    def plot_residuals(\n        self,\n        method=""diff"",\n        smooth_kernel=""gauss"",\n        smooth_radius=""0.1 deg"",\n        region=None,\n        figsize=(12, 4),\n        **kwargs,\n    ):\n        """"""\n        Plot spatial and spectral residuals.\n\n        The spectral residuals are extracted from the provided region, and the\n        normalization used for the residuals computation can be controlled using\n        the method parameter. If no region is passed, only the spatial\n        residuals are shown.\n\n        Parameters\n        ----------\n        method : {""diff"", ""diff/model"", ""diff/sqrt(model)""}\n            Method used to compute the residuals, see `MapDataset.residuals()`\n        smooth_kernel : {\'gauss\', \'box\'}\n            Kernel shape.\n        smooth_radius: `~astropy.units.Quantity`, str or float\n            Smoothing width given as quantity or float. If a float is given it\n            is interpreted as smoothing width in pixels.\n        region: `~regions.Region`\n            Region (pixel or sky regions accepted)\n        figsize : tuple\n            Figure size used for the plotting.\n        **kwargs : dict\n            Keyword arguments passed to `~matplotlib.pyplot.imshow`.\n\n        Returns\n        -------\n        ax_image, ax_spec : `~matplotlib.pyplot.Axes`,\n            Image and spectrum axes.\n        """"""\n        import matplotlib.pyplot as plt\n\n        fig = plt.figure(figsize=figsize)\n\n        counts, npred = self.counts, self.npred()\n\n        if self.mask is not None:\n            counts = counts * self.mask\n            npred = npred * self.mask\n\n        counts_spatial = counts.sum_over_axes().smooth(\n            width=smooth_radius, kernel=smooth_kernel\n        )\n        npred_spatial = npred.sum_over_axes().smooth(\n            width=smooth_radius, kernel=smooth_kernel\n        )\n        spatial_residuals = self._compute_residuals(\n            counts_spatial, npred_spatial, method\n        )\n\n        if self.mask_safe is not None:\n            mask = self.mask_safe.reduce_over_axes(func=np.logical_or)\n            spatial_residuals.data[~mask.data] = np.nan\n\n        # If no region is provided, skip spectral residuals\n        ncols = 2 if region is not None else 1\n        ax_image = fig.add_subplot(1, ncols, 1, projection=spatial_residuals.geom.wcs)\n        ax_spec = None\n\n        kwargs.setdefault(""cmap"", ""coolwarm"")\n        kwargs.setdefault(""stretch"", ""linear"")\n        kwargs.setdefault(""vmin"", -5)\n        kwargs.setdefault(""vmax"", 5)\n        spatial_residuals.plot(ax=ax_image, add_cbar=True, **kwargs)\n\n        # Spectral residuals\n        if region:\n            ax_spec = fig.add_subplot(1, 2, 2)\n            counts_spec = counts.get_spectrum(region=region)\n            npred_spec = npred.get_spectrum(region=region)\n            residuals = self._compute_residuals(counts_spec, npred_spec, method)\n            ax = residuals.plot()\n            ax.set_yscale(""linear"")\n            ax.axhline(0, color=""black"", lw=0.5)\n\n            y_max = 2 * np.nanmax(residuals.data)\n            plt.ylim(-y_max, y_max)\n            label = self._residuals_labels[method]\n            plt.ylabel(f""Residuals ({label})"")\n\n            # Overlay spectral extraction region on the spatial residuals\n            pix_region = region.to_pixel(wcs=spatial_residuals.geom.wcs)\n            pix_region.plot(ax=ax_image)\n\n        return ax_image, ax_spec\n\n    @lazyproperty\n    def _counts_data(self):\n        return self.counts.data.astype(float)\n\n    def stat_sum(self):\n        """"""Total likelihood given the current model parameters.""""""\n        counts, npred = self._counts_data, self.npred().data\n\n        if self.mask is not None:\n            return cash_sum_cython(counts[self.mask.data], npred[self.mask.data])\n        else:\n            return cash_sum_cython(counts.ravel(), npred.ravel())\n\n    def fake(self, random_state=""random-seed""):\n        """"""Simulate fake counts for the current model and reduced IRFs.\n\n        This method overwrites the counts defined on the dataset object.\n\n        Parameters\n        ----------\n        random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n                Defines random number generator initialisation.\n                Passed to `~gammapy.utils.random.get_random_state`.\n        """"""\n        random_state = get_random_state(random_state)\n        npred = self.npred()\n        npred.data = random_state.poisson(npred.data)\n        self.counts = npred\n\n    def to_hdulist(self):\n        """"""Convert map dataset to list of HDUs.\n\n        Returns\n        -------\n        hdulist : `~astropy.io.fits.HDUList`\n            Map dataset list of HDUs.\n        """"""\n        # TODO: what todo about the model and background model parameters?\n        exclude_primary = slice(1, None)\n\n        hdu_primary = fits.PrimaryHDU()\n        hdulist = fits.HDUList([hdu_primary])\n        if self.counts is not None:\n            hdulist += self.counts.to_hdulist(hdu=""counts"")[exclude_primary]\n\n        if self.exposure is not None:\n            hdulist += self.exposure.to_hdulist(hdu=""exposure"")[exclude_primary]\n\n        if self.background_model is not None:\n            hdulist += self.background_model.map.to_hdulist(hdu=""background"")[\n                exclude_primary\n            ]\n\n        if self.edisp is not None:\n            if isinstance(self.edisp, EDispKernel):\n                hdus = self.edisp.to_hdulist()\n                hdus[""MATRIX""].name = ""edisp_matrix""\n                hdus[""EBOUNDS""].name = ""edisp_matrix_ebounds""\n                hdulist.append(hdus[""EDISP_MATRIX""])\n                hdulist.append(hdus[""EDISP_MATRIX_EBOUNDS""])\n            else:\n                hdulist += self.edisp.edisp_map.to_hdulist(hdu=""EDISP"")[exclude_primary]\n                hdulist += self.edisp.exposure_map.to_hdulist(hdu=""edisp_exposure"")[\n                    exclude_primary\n                ]\n\n        if self.psf is not None:\n            if isinstance(self.psf, PSFKernel):\n                hdulist += self.psf.psf_kernel_map.to_hdulist(hdu=""psf_kernel"")[\n                    exclude_primary\n                ]\n            else:\n                hdulist += self.psf.psf_map.to_hdulist(hdu=""psf"")[exclude_primary]\n                hdulist += self.psf.exposure_map.to_hdulist(hdu=""psf_exposure"")[\n                    exclude_primary\n                ]\n\n        if self.mask_safe is not None:\n            mask_safe_int = self.mask_safe.copy()\n            mask_safe_int.data = mask_safe_int.data.astype(int)\n            hdulist += mask_safe_int.to_hdulist(hdu=""mask_safe"")[exclude_primary]\n\n        if self.mask_fit is not None:\n            mask_fit_int = self.mask_fit.copy()\n            mask_fit_int.data = mask_fit_int.data.astype(int)\n            hdulist += mask_fit_int.to_hdulist(hdu=""mask_fit"")[exclude_primary]\n\n        if self.gti is not None:\n            hdulist.append(fits.BinTableHDU(self.gti.table, name=""GTI""))\n\n        return hdulist\n\n    @classmethod\n    def from_hdulist(cls, hdulist, name=None):\n        """"""Create map dataset from list of HDUs.\n\n        Parameters\n        ----------\n        hdulist : `~astropy.io.fits.HDUList`\n            List of HDUs.\n        name : str\n            Name of the new dataset.\n\n        Returns\n        -------\n        dataset : `MapDataset`\n            Map dataset.\n        """"""\n        name = make_name(name)\n        kwargs = {""name"": name}\n\n        if ""COUNTS"" in hdulist:\n            kwargs[""counts""] = Map.from_hdulist(hdulist, hdu=""counts"")\n\n        if ""EXPOSURE"" in hdulist:\n            exposure = Map.from_hdulist(hdulist, hdu=""exposure"")\n            if exposure.geom.axes[0].name == ""energy"":\n                exposure.geom.axes[0].name = ""energy_true""\n            kwargs[""exposure""] = exposure\n\n        if ""BACKGROUND"" in hdulist:\n            background_map = Map.from_hdulist(hdulist, hdu=""background"")\n            kwargs[""models""] = Models(\n                [\n                    BackgroundModel(\n                        background_map, datasets_names=[name], name=name + ""-bkg""\n                    )\n                ]\n            )\n\n        if ""EDISP_MATRIX"" in hdulist:\n            kwargs[""edisp""] = EDispKernel.from_hdulist(\n                hdulist, hdu1=""EDISP_MATRIX"", hdu2=""EDISP_MATRIX_EBOUNDS""\n            )\n        if ""EDISP"" in hdulist:\n            edisp_map = Map.from_hdulist(hdulist, hdu=""edisp"")\n            exposure_map = Map.from_hdulist(hdulist, hdu=""edisp_exposure"")\n            if edisp_map.geom.axes[0].name == ""energy"":\n                kwargs[""edisp""] = EDispKernelMap(edisp_map, exposure_map)\n            else:\n                kwargs[""edisp""] = EDispMap(edisp_map, exposure_map)\n\n        if ""PSF_KERNEL"" in hdulist:\n            psf_map = Map.from_hdulist(hdulist, hdu=""psf_kernel"")\n            kwargs[""psf""] = PSFKernel(psf_map)\n        if ""PSF"" in hdulist:\n            psf_map = Map.from_hdulist(hdulist, hdu=""psf"")\n            exposure_map = Map.from_hdulist(hdulist, hdu=""psf_exposure"")\n            kwargs[""psf""] = PSFMap(psf_map, exposure_map)\n\n        if ""MASK_SAFE"" in hdulist:\n            mask_safe = Map.from_hdulist(hdulist, hdu=""mask_safe"")\n            mask_safe.data = mask_safe.data.astype(bool)\n            kwargs[""mask_safe""] = mask_safe\n\n        if ""MASK_FIT"" in hdulist:\n            mask_fit = Map.from_hdulist(hdulist, hdu=""mask_fit"")\n            mask_fit.data = mask_fit.data.astype(bool)\n            kwargs[""mask_fit""] = mask_fit\n\n        if ""GTI"" in hdulist:\n            gti = GTI(Table.read(hdulist, hdu=""GTI""))\n            kwargs[""gti""] = gti\n\n        return cls(**kwargs)\n\n    def write(self, filename, overwrite=False):\n        """"""Write map dataset to file.\n\n        Parameters\n        ----------\n        filename : str\n            Filename to write to.\n        overwrite : bool\n            Overwrite file if it exists.\n        """"""\n        self.to_hdulist().writeto(make_path(filename), overwrite=overwrite)\n\n    @classmethod\n    def read(cls, filename, name=None):\n        """"""Read map dataset from file.\n\n        Parameters\n        ----------\n        filename : str\n            Filename to read from.\n        name : str\n            Name of the new dataset.\n\n        Returns\n        -------\n        dataset : `MapDataset`\n            Map dataset.\n        """"""\n        with fits.open(make_path(filename), memmap=False) as hdulist:\n            return cls.from_hdulist(hdulist, name=name)\n\n    @classmethod\n    def from_dict(cls, data, models):\n        """"""Create from dicts and models list generated from YAML serialization.""""""\n\n        filename = make_path(data[""filename""])\n        dataset = cls.read(filename, name=data[""name""])\n\n        for model in models:\n            if (\n                isinstance(model, BackgroundModel)\n                and model.filename is None\n                and dataset.name == model.datasets_names[0]\n            ):\n                model.map = dataset.background_model.map\n\n        dataset.models = models\n        return dataset\n\n    def to_dict(self, filename=""""):\n        """"""Convert to dict for YAML serialization.""""""\n        return {\n            ""name"": self.name,\n            ""type"": self.tag,\n            ""filename"": str(filename),\n        }\n\n    def info_dict(self, region=None):\n        """"""Basic info dict with summary statistics\n\n        If a region is passed, then a spectrum dataset is\n        extracted, and the corresponding info returned.\n\n        Parameters\n        ----------\n        region : `~regions.SkyRegion`, optional\n            the input ON region on which to extract the spectrum\n\n        Returns\n        -------\n        info_dict : dict\n            Dictionary with summary info.\n        """"""\n        if self.gti is not None:\n            if region is None:\n                region = RectangleSkyRegion(\n                    center=self._geom.center_skydir,\n                    width=self._geom.width[0][0],\n                    height=self._geom.width[1][0],\n                )\n            info = self.to_spectrum_dataset(on_region=region).info_dict()\n        else:\n            info = dict()\n            if self.counts:\n                info[""counts""] = np.sum(self.counts.data)\n            if self.background_model:\n                info[""background""] = np.sum(self.background_model.evaluate().data)\n                info[""excess""] = info[""counts""] - info[""background""]\n\n            info[""npred""] = np.sum(self.npred())\n            if self.mask_safe is not None:\n                mask = self.mask_safe.reduce_over_axes(np.logical_or).data\n                if not mask.any():\n                    mask = None\n            else:\n                mask = None\n            if self.exposure:\n                exposure_min = np.min(self.exposure.data[..., mask])\n                exposure_max = np.max(self.exposure.data[..., mask])\n                info[""aeff_min""] = exposure_min * self.exposure.unit\n                info[""aeff_max""] = exposure_max * self.exposure.unit\n\n        info[""name""] = self.name\n\n        return info\n\n    def to_spectrum_dataset(self, on_region, containment_correction=False, name=None):\n        """"""Return a ~gammapy.spectrum.SpectrumDataset from on_region.\n\n        Counts and background are summed in the on_region.\n\n        Effective area is taken from the average exposure divided by the livetime.\n        Here we assume it is the sum of the GTIs.\n\n        The energy dispersion kernel is obtained at the on_region center.\n        Only regions with centers are supported.\n\n        The model is not exported to the ~gammapy.spectrum.SpectrumDataset.\n        It must be set after the dataset extraction.\n\n        Parameters\n        ----------\n        on_region : `~regions.SkyRegion`\n            the input ON region on which to extract the spectrum\n        containment_correction : bool\n            Apply containment correction for point sources and circular on regions\n        name : str\n            Name of the new dataset.\n\n        Returns\n        -------\n        dataset : `~gammapy.spectrum.SpectrumDataset`\n            the resulting reduced dataset\n        """"""\n        from .spectrum import SpectrumDataset\n\n        kwargs = {""gti"": self.gti, ""name"": name}\n\n        if self.gti is not None:\n            kwargs[""livetime""] = self.gti.time_sum\n        else:\n            raise ValueError(""No GTI in `MapDataset`, cannot compute livetime"")\n\n        if self.counts is not None:\n            kwargs[""counts""] = self.counts.get_spectrum(on_region, np.sum)\n\n        if self.background_model is not None:\n            kwargs[""background""] = self.background_model.evaluate().get_spectrum(\n                on_region, np.sum\n            )\n\n        if self.exposure is not None:\n            exposure = self.exposure.get_spectrum(on_region, np.mean)\n            energy = exposure.geom.axes[0].edges\n            kwargs[""aeff""] = EffectiveAreaTable(\n                energy_lo=energy[:-1],\n                energy_hi=energy[1:],\n                data=exposure.quantity[:, 0, 0] / kwargs[""livetime""],\n            )\n\n        if containment_correction:\n            if not isinstance(on_region, CircleSkyRegion):\n                raise TypeError(\n                    ""Containement correction is only supported for""\n                    "" `CircleSkyRegion`.""\n                )\n            elif self.psf is None or isinstance(self.psf, PSFKernel):\n                raise ValueError(""No PSFMap set. Containement correction impossible"")\n            else:\n                psf = self.psf.get_energy_dependent_table_psf(on_region.center)\n                containment = psf.containment(\n                    kwargs[""aeff""].energy.center, on_region.radius\n                )\n                kwargs[""aeff""].data.data *= containment.squeeze()\n\n        if self.edisp is not None:\n            if isinstance(self.edisp, EDispKernel):\n                edisp = self.edisp\n            elif isinstance(self.edisp, EDispKernelMap):\n                edisp = self.edisp.get_edisp_kernel(on_region.center)\n            else:\n                axis = self._geom.get_axis_by_name(""energy"")\n                edisp = self.edisp.get_edisp_kernel(on_region.center, e_reco=axis.edges)\n            kwargs[""edisp""] = edisp\n\n        return SpectrumDataset(**kwargs)\n\n    def to_image(self, spectrum=None, name=None):\n        """"""Create images by summing over the energy axis.\n\n        Exposure is weighted with an assumed spectrum,\n        resulting in a weighted mean exposure image.\n\n        Currently the PSFMap and EdispMap are dropped from the\n        resulting image dataset.\n\n        Parameters\n        ----------\n        spectrum : `~gammapy.modeling.models.SpectralModel`\n            Spectral model to compute the weights.\n            Default is power-law with spectral index of 2.\n        name : str\n            Name of the new dataset.\n\n        Returns\n        -------\n        dataset : `MapDataset`\n            Map dataset containing images.\n        """"""\n        from gammapy.makers.utils import _map_spectrum_weight\n\n        name = make_name(name)\n        kwargs = {}\n        kwargs[""name""] = name\n        kwargs[""gti""] = self.gti\n\n        if self.mask_safe is not None:\n            mask_safe = self.mask_safe\n            kwargs[""mask_safe""] = mask_safe.reduce_over_axes(\n                func=np.logical_or, keepdims=True\n            )\n        else:\n            mask_safe = 1\n\n        if self.counts is not None:\n            counts = self.counts * mask_safe\n            kwargs[""counts""] = counts.sum_over_axes(keepdims=True)\n\n        if self.exposure is not None:\n            exposure = _map_spectrum_weight(self.exposure, spectrum)\n            kwargs[""exposure""] = exposure.sum_over_axes(keepdims=True)\n\n        if self.background_model is not None:\n            background = self.background_model.evaluate() * mask_safe\n            background = background.sum_over_axes(keepdims=True)\n            kwargs[""models""] = Models(\n                [BackgroundModel(background, datasets_names=[name])]\n            )\n\n        if self.psf is not None:\n            # TODO: implement PSFKernel.to_image()\n            if not isinstance(self.psf, PSFKernel):\n                kwargs[""psf""] = self.psf.to_image(spectrum=spectrum, keepdims=True)\n            else:\n                # assume exposure at center position\n                kwargs[""psf""] = None\n\n        return self.__class__(**kwargs)\n\n    def cutout(self, position, width, mode=""trim"", name=None):\n        """"""Cutout map dataset.\n\n        Parameters\n        ----------\n        position : `~astropy.coordinates.SkyCoord`\n            Center position of the cutout region.\n        width : tuple of `~astropy.coordinates.Angle`\n            Angular sizes of the region in (lon, lat) in that specific order.\n            If only one value is passed, a square region is extracted.\n        mode : {\'trim\', \'partial\', \'strict\'}\n            Mode option for Cutout2D, for details see `~astropy.nddata.utils.Cutout2D`.\n        name : str\n            Name of the new dataset.\n\n        Returns\n        -------\n        cutout : `MapDataset`\n            Cutout map dataset.\n        """"""\n        name = make_name(name)\n        kwargs = {""gti"": self.gti, ""name"": name}\n        cutout_kwargs = {""position"": position, ""width"": width, ""mode"": mode}\n\n        if self.counts is not None:\n            kwargs[""counts""] = self.counts.cutout(**cutout_kwargs)\n\n        if self.exposure is not None:\n            kwargs[""exposure""] = self.exposure.cutout(**cutout_kwargs)\n\n        if self.background_model is not None:\n            model = self.background_model.cutout(**cutout_kwargs, name=name + ""-bkg"")\n            model.datasets_names = [name]\n            kwargs[""models""] = model\n\n        if self.edisp is not None:\n            kwargs[""edisp""] = self.edisp.cutout(**cutout_kwargs)\n\n        if self.psf is not None:\n            kwargs[""psf""] = self.psf.cutout(**cutout_kwargs)\n\n        if self.mask_safe is not None:\n            kwargs[""mask_safe""] = self.mask_safe.cutout(**cutout_kwargs)\n\n        if self.mask_fit is not None:\n            kwargs[""mask_fit""] = self.mask_fit.cutout(**cutout_kwargs)\n\n        return self.__class__(**kwargs)\n\n\nclass MapDatasetOnOff(MapDataset):\n    """"""Map dataset for on-off likelihood fitting.\n\n    Parameters\n    ----------\n    models : `~gammapy.modeling.models.Models`\n        Source sky models.\n    counts : `~gammapy.maps.WcsNDMap`\n        Counts cube\n    counts_off : `~gammapy.maps.WcsNDMap`\n        Ring-convolved counts cube\n    acceptance : `~gammapy.maps.WcsNDMap`\n        Acceptance from the IRFs\n    acceptance_off : `~gammapy.maps.WcsNDMap`\n        Acceptance off\n    exposure : `~gammapy.maps.WcsNDMap`\n        Exposure cube\n    mask_fit : `~numpy.ndarray`\n        Mask to apply to the likelihood for fitting.\n    psf : `~gammapy.cube.PSFKernel`\n        PSF kernel\n    edisp : `~gammapy.irf.EDispKernel`\n        Energy dispersion\n    evaluation_mode : {""local"", ""global""}\n        Model evaluation mode.\n        The ""local"" mode evaluates the model components on smaller grids to save computation time.\n        This mode is recommended for local optimization algorithms.\n        The ""global"" evaluation mode evaluates the model components on the full map.\n        This mode is recommended for global optimization algorithms.\n    mask_safe : `~numpy.ndarray`\n        Mask defining the safe data range.\n    gti : `~gammapy.data.GTI`\n        GTI of the observation or union of GTI if it is a stacked observation\n    name : str\n        Name of the dataset.\n\n    """"""\n\n    stat_type = ""wstat""\n    tag = ""MapDatasetOnOff""\n\n    def __init__(\n        self,\n        models=None,\n        counts=None,\n        counts_off=None,\n        acceptance=None,\n        acceptance_off=None,\n        exposure=None,\n        mask_fit=None,\n        psf=None,\n        edisp=None,\n        name=None,\n        evaluation_mode=""local"",\n        mask_safe=None,\n        gti=None,\n    ):\n        if mask_fit is not None and mask_fit.dtype != np.dtype(""bool""):\n            raise ValueError(""mask data must have dtype bool"")\n\n        self.evaluation_mode = evaluation_mode\n        self.counts = counts\n        self.counts_off = counts_off\n        self.exposure = exposure\n\n        if np.isscalar(acceptance):\n            acceptance = Map.from_geom(\n                self._geom, data=np.ones(self.data_shape) * acceptance\n            )\n\n        if np.isscalar(acceptance_off):\n            acceptance_off = Map.from_geom(\n                self._geom, data=np.ones(self.data_shape) * acceptance_off\n            )\n\n        self.acceptance = acceptance\n        self.acceptance_off = acceptance_off\n        self._background_model = None\n        self.mask_fit = mask_fit\n        self.psf = psf\n        self.edisp = edisp\n        self._name = make_name(name)\n        self.models = models\n        self.mask_safe = mask_safe\n        self.gti = gti\n\n    def __str__(self):\n        str_ = super().__str__()\n\n        counts_off = np.nan\n        if self.counts_off is not None:\n            counts_off = np.sum(self.counts_off.data)\n        str_ += ""\\t{:32}: {:.0f} \\n"".format(""Total counts_off"", counts_off)\n\n        acceptance = np.nan\n        if self.acceptance is not None:\n            acceptance = np.sum(self.acceptance.data)\n        str_ += ""\\t{:32}: {:.0f} \\n"".format(""Acceptance"", acceptance)\n\n        acceptance_off = np.nan\n        if self.acceptance_off is not None:\n            acceptance_off = np.sum(self.acceptance_off.data)\n        str_ += ""\\t{:32}: {:.0f} \\n"".format(""Acceptance off"", acceptance_off)\n\n        return str_.expandtabs(tabsize=4)\n\n    @property\n    def alpha(self):\n        """"""Exposure ratio between signal and background regions""""""\n        alpha = self.acceptance / self.acceptance_off\n        alpha.data = np.nan_to_num(alpha.data)\n        return alpha\n\n    @property\n    def background(self):\n        """"""Predicted background in the on region.\n\n        Notice that this definition is valid under the assumption of cash statistic.\n        """"""\n        return self.alpha * self.counts_off\n\n    @property\n    def excess(self):\n        """"""Excess (counts - alpha * counts_off)""""""\n        return self.counts.data - self.background.data\n\n    def stat_array(self):\n        """"""Likelihood per bin given the current model parameters""""""\n        mu_sig = self.npred().data\n        on_stat_ = wstat(\n            n_on=self.counts.data,\n            n_off=self.counts_off.data,\n            alpha=list(self.alpha.data),\n            mu_sig=mu_sig,\n        )\n        return np.nan_to_num(on_stat_)\n\n    @classmethod\n    def from_geoms(\n        cls,\n        geom,\n        geom_exposure,\n        geom_psf,\n        geom_edisp,\n        reference_time=""2000-01-01"",\n        name=None,\n        **kwargs,\n    ):\n        """"""\n        Create a MapDatasetOnOff object with zero filled maps according to the specified geometries\n\n        Parameters\n        ----------\n        geom : `gammapy.maps.WcsGeom`\n            geometry for the counts, counts_off, acceptance and acceptance_off maps\n        geom_exposure : `gammapy.maps.WcsGeom`\n            geometry for the exposure map\n        geom_psf : `gammapy.maps.WcsGeom`\n            geometry for the psf map\n        geom_edisp : `gammapy.maps.WcsGeom`\n            geometry for the energy dispersion kernel map.\n            If geom_edisp has a migra axis, this wil create an EDispMap instead.\n        reference_time : `~astropy.time.Time`\n            the reference time to use in GTI definition\n        name : str\n            Name of the returned dataset.\n\n        Returns\n        -------\n        empty_maps : `MapDatasetOnOff`\n            A MapDatasetOnOff containing zero filled maps\n        """"""\n        kwargs = kwargs.copy()\n        kwargs[""name""] = name\n\n        for key in [""counts"", ""counts_off"", ""acceptance"", ""acceptance_off""]:\n            kwargs[key] = Map.from_geom(geom, unit="""")\n\n        kwargs[""exposure""] = Map.from_geom(geom_exposure, unit=""m2 s"")\n        if geom_edisp.axes[0].name.lower() == ""energy"":\n            kwargs[""edisp""] = EDispKernelMap.from_geom(geom_edisp)\n        else:\n            kwargs[""edisp""] = EDispMap.from_geom(geom_edisp)\n\n        kwargs[""psf""] = PSFMap.from_geom(geom_psf)\n        kwargs[""gti""] = GTI.create([] * u.s, [] * u.s, reference_time=reference_time)\n        kwargs[""mask_safe""] = Map.from_geom(geom, dtype=bool)\n\n        return cls(**kwargs)\n\n    @classmethod\n    def from_map_dataset(\n        cls, dataset, acceptance, acceptance_off, counts_off=None, name=None\n    ):\n        """"""Create spectrum dataseton off from another dataset.\n\n        Parameters\n        ----------\n        dataset : `MapDataset`\n            Spectrum dataset defining counts, edisp, aeff, livetime etc.\n        acceptance : `Map`\n            Relative background efficiency in the on region.\n        acceptance_off : `Map`\n            Relative background efficiency in the off region.\n        counts_off : `Map`\n            Off counts map . If the dataset provides a background model,\n            and no off counts are defined. The off counts are deferred from\n            counts_off / alpha.\n        name : str\n            Name of the returned dataset.\n\n        Returns\n        -------\n        dataset : `MapDatasetOnOff`\n            Map dataset on off.\n\n        """"""\n        kwargs = {""name"": name}\n\n        if counts_off is None and dataset.background_model is not None:\n            alpha = acceptance / acceptance_off\n            kwargs[""counts_off""] = dataset.background_model.evaluate() / alpha\n\n        return cls(\n            counts=dataset.counts,\n            exposure=dataset.exposure,\n            counts_off=counts_off,\n            edisp=dataset.edisp,\n            gti=dataset.gti,\n            mask_safe=dataset.mask_safe,\n            mask_fit=dataset.mask_fit,\n            acceptance=acceptance,\n            acceptance_off=acceptance_off,\n            name=dataset.name,\n            evaluation_mode=dataset.evaluation_mode,\n        )\n\n    @property\n    def _is_stackable(self):\n        """"""Check if the Dataset contains enough information to be stacked""""""\n        if (\n            self.acceptance_off is None\n            or self.acceptance is None\n            or self.counts_off is None\n        ):\n            return False\n        else:\n            return True\n\n    def stack(self, other):\n        r""""""Stack another dataset in place.\n\n        The ``acceptance`` of the stacked dataset is normalized to 1,\n        and the stacked ``acceptance_off`` is scaled so that:\n\n        .. math::\n            \\alpha_\\text{stacked} =\n            \\frac{1}{a_\\text{off}} =\n            \\frac{\\alpha_1\\text{OFF}_1 + \\alpha_2\\text{OFF}_2}{\\text{OFF}_1 + OFF_2}\n\n        Parameters\n        ----------\n        other : `MapDatasetOnOff`\n            Other dataset\n        """"""\n        if not isinstance(other, MapDatasetOnOff):\n            raise TypeError(""Incompatible types for MapDatasetOnOff stacking"")\n\n        if not self._is_stackable or not other._is_stackable:\n            raise ValueError(""Cannot stack incomplete MapDatsetOnOff."")\n\n        # Factor containing: self.alpha * self.counts_off + other.alpha * other.counts_off\n        tmp_factor = (self.alpha * self.counts_off).copy()\n        tmp_factor.data[~self.mask_safe.data] = 0\n        tmp_factor.stack(other.alpha * other.counts_off, weights=other.mask_safe)\n\n        # Stack the off counts (in place)\n        self.counts_off.data[~self.mask_safe.data] = 0\n        self.counts_off.stack(other.counts_off, weights=other.mask_safe)\n\n        self.acceptance_off = self.counts_off / tmp_factor\n        self.acceptance.data = np.ones(self.data_shape)\n\n        super().stack(other)\n\n    def stat_sum(self):\n        """"""Total likelihood given the current model parameters.""""""\n        return Dataset.stat_sum(self)\n\n    def fake(self, background_model, random_state=""random-seed""):\n        """"""Simulate fake counts (on and off) for the current model and reduced IRFs.\n\n        This method overwrites the counts defined on the dataset object.\n\n        Parameters\n        ----------\n        random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n                Defines random number generator initialisation.\n                Passed to `~gammapy.utils.random.get_random_state`.\n        """"""\n        random_state = get_random_state(random_state)\n        npred = self.npred()\n        npred.data = random_state.poisson(npred.data)\n\n        npred_bkg = background_model.copy()\n        npred_bkg.data = random_state.poisson(npred_bkg.data)\n\n        self.counts = npred + npred_bkg\n\n        npred_off = background_model / self.alpha\n        npred_off.data = random_state.poisson(npred_off.data)\n        self.counts_off = npred_off\n\n    def to_hdulist(self):\n        """"""Convert map dataset to list of HDUs.\n\n        Returns\n        -------\n        hdulist : `~astropy.io.fits.HDUList`\n            Map dataset list of HDUs.\n        """"""\n        hdulist = super().to_hdulist()\n        exclude_primary = slice(1, None)\n\n        if self.counts_off is not None:\n            hdulist += self.counts_off.to_hdulist(hdu=""counts_off"")[exclude_primary]\n\n        if self.acceptance is not None:\n            hdulist += self.acceptance.to_hdulist(hdu=""acceptance"")[exclude_primary]\n\n        if self.acceptance_off is not None:\n            hdulist += self.acceptance_off.to_hdulist(hdu=""acceptance_off"")[\n                exclude_primary\n            ]\n\n        return hdulist\n\n    @classmethod\n    def from_hdulist(cls, hdulist, name=None):\n        """"""Create map dataset from list of HDUs.\n\n        Parameters\n        ----------\n        hdulist : `~astropy.io.fits.HDUList`\n            List of HDUs.\n        name : str\n            Name of the new dataset.\n\n        Returns\n        -------\n        dataset : `MapDataset`\n            Map dataset.\n        """"""\n        kwargs = {}\n        kwargs[""name""] = name\n        if ""COUNTS"" in hdulist:\n            kwargs[""counts""] = Map.from_hdulist(hdulist, hdu=""counts"")\n\n        if ""COUNTS_OFF"" in hdulist:\n            kwargs[""counts_off""] = Map.from_hdulist(hdulist, hdu=""counts_off"")\n\n        if ""ACCEPTANCE"" in hdulist:\n            kwargs[""acceptance""] = Map.from_hdulist(hdulist, hdu=""acceptance"")\n\n        if ""ACCEPTANCE_OFF"" in hdulist:\n            kwargs[""acceptance_off""] = Map.from_hdulist(hdulist, hdu=""acceptance_off"")\n\n        if ""EXPOSURE"" in hdulist:\n            kwargs[""exposure""] = Map.from_hdulist(hdulist, hdu=""exposure"")\n\n        if ""EDISP_MATRIX"" in hdulist:\n            kwargs[""edisp""] = EDispKernel.from_hdulist(\n                hdulist, hdu1=""EDISP_MATRIX"", hdu2=""EDISP_MATRIX_EBOUNDS""\n            )\n\n        if ""PSF_KERNEL"" in hdulist:\n            psf_map = Map.from_hdulist(hdulist, hdu=""psf_kernel"")\n            kwargs[""psf""] = PSFKernel(psf_map)\n\n        if ""MASK_SAFE"" in hdulist:\n            mask_safe = Map.from_hdulist(hdulist, hdu=""mask_safe"")\n            mask_safe.data = mask_safe.data.astype(bool)\n            kwargs[""mask_safe""] = mask_safe\n\n        if ""MASK_FIT"" in hdulist:\n            mask_fit = Map.from_hdulist(hdulist, hdu=""mask_fit"")\n            mask_fit.data = mask_fit.data.astype(bool)\n            kwargs[""mask_fit""] = mask_fit\n\n        if ""GTI"" in hdulist:\n            gti = GTI(Table.read(hdulist, hdu=""GTI""))\n            kwargs[""gti""] = gti\n        return cls(**kwargs)\n\n    def info_dict(self, region=None):\n        """"""Basic info dict with summary statistics\n\n        If a region is passed, then a spectrum dataset is\n        extracted, and the corresponding info returned.\n\n        Parameters\n        ----------\n        region : `~regions.SkyRegion`, optional\n            the input ON region on which to extract the spectrum\n\n        Returns\n        -------\n        info_dict : dict\n            Dictionary with summary info.\n        """"""\n        info = super().info_dict(region)\n        info[""name""] = self.name\n        if self.gti is None:\n            if self.counts_off is not None:\n                info[""counts_off""] = np.sum(self.counts_off.data)\n\n            if self.acceptance is not None:\n                info[""acceptance""] = np.sum(self.acceptance.data)\n\n            if self.acceptance_off is not None:\n                info[""acceptance_off""] = np.sum(self.acceptance_off.data)\n\n            info[""excess""] = np.sum(self.excess.data)\n        return info\n\n    def to_spectrum_dataset(self, on_region, containment_correction=False, name=None):\n        """"""Return a ~gammapy.spectrum.SpectrumDatasetOnOff from on_region.\n\n        Counts and OFF counts are summed in the on_region.\n\n        Acceptance is the average of all acceptances while acceptance OFF\n        is taken such that number of excess is preserved in the on_region.\n\n        Effective area is taken from the average exposure divided by the livetime.\n        Here we assume it is the sum of the GTIs.\n\n        The energy dispersion kernel is obtained at the on_region center.\n        Only regions with centers are supported.\n\n        The model is not exported to the ~gammapy.spectrum.SpectrumDataset.\n        It must be set after the dataset extraction.\n\n        Parameters\n        ----------\n        on_region : `~regions.SkyRegion`\n            the input ON region on which to extract the spectrum\n        containment_correction : bool\n            Apply containment correction for point sources and circular on regions\n        name : str\n            Name of the new dataset.\n\n        Returns\n        -------\n        dataset : `~gammapy.spectrum.SpectrumDatasetOnOff`\n            the resulting reduced dataset\n        """"""\n        from .spectrum import SpectrumDatasetOnOff\n\n        dataset = super().to_spectrum_dataset(on_region, containment_correction, name)\n\n        kwargs = {}\n        if self.counts_off is not None:\n            kwargs[""counts_off""] = self.counts_off.get_spectrum(on_region, np.sum)\n\n        if self.acceptance is not None:\n            kwargs[""acceptance""] = self.acceptance.get_spectrum(on_region, np.mean)\n            background = self.background.get_spectrum(on_region, np.sum)\n            kwargs[""acceptance_off""] = (\n                kwargs[""acceptance""] * kwargs[""counts_off""] / background\n            )\n\n        return SpectrumDatasetOnOff.from_spectrum_dataset(dataset=dataset, **kwargs)\n\n    def cutout(self, position, width, mode=""trim"", name=None):\n        """"""Cutout map dataset.\n\n        Parameters\n        ----------\n        position : `~astropy.coordinates.SkyCoord`\n            Center position of the cutout region.\n        width : tuple of `~astropy.coordinates.Angle`\n            Angular sizes of the region in (lon, lat) in that specific order.\n            If only one value is passed, a square region is extracted.\n        mode : {\'trim\', \'partial\', \'strict\'}\n            Mode option for Cutout2D, for details see `~astropy.nddata.utils.Cutout2D`.\n        name : str\n            Name of the new dataset.\n\n        Returns\n        -------\n        cutout : `MapDatasetOnOff`\n            Cutout map dataset.\n        """"""\n        cutout_kwargs = {\n            ""position"": position,\n            ""width"": width,\n            ""mode"": mode,\n            ""name"": name,\n        }\n\n        cutout_dataset = super().cutout(**cutout_kwargs)\n\n        del cutout_kwargs[""name""]\n\n        if self.counts_off is not None:\n            cutout_dataset.counts_off = self.counts_off.cutout(**cutout_kwargs)\n\n        if self.acceptance is not None:\n            cutout_dataset.acceptance = self.acceptance.cutout(**cutout_kwargs)\n\n        if self.acceptance_off is not None:\n            cutout_dataset.acceptance_off = self.acceptance_off.cutout(**cutout_kwargs)\n\n        return cutout_dataset\n\n    def to_image(self, spectrum=None, name=None):\n        """"""Create images by summing over the energy axis.\n\n        Exposure is weighted with an assumed spectrum,\n        resulting in a weighted mean exposure image.\n\n        Currently the PSFMap and EdispMap are dropped from the\n        resulting image dataset.\n\n        Parameters\n        ----------\n        spectrum : `~gammapy.modeling.models.SpectralModel`\n            Spectral model to compute the weights.\n            Default is power-law with spectral index of 2.\n        name : str\n            Name of the new dataset.\n\n        Returns\n        -------\n        dataset : `MapDatasetOnOff`\n            Map dataset containing images.\n        """"""\n        kwargs = {""name"": name}\n        dataset = super().to_image(spectrum, name)\n\n        if self.mask_safe is not None:\n            mask_safe = self.mask_safe\n        else:\n            mask_safe = 1\n\n        if self.counts_off is not None:\n            counts_off = self.counts_off * mask_safe\n            kwargs[""counts_off""] = counts_off.sum_over_axes(keepdims=True)\n\n        if self.acceptance is not None:\n            acceptance = self.acceptance * mask_safe\n            kwargs[""acceptance""] = acceptance.sum_over_axes(keepdims=True)\n\n            background = self.background * mask_safe\n            background = background.sum_over_axes(keepdims=True)\n            kwargs[""acceptance_off""] = (\n                kwargs[""acceptance""] * kwargs[""counts_off""] / background\n            )\n\n        return self.from_map_dataset(dataset, **kwargs)\n\n\nclass MapEvaluator:\n    """"""Sky model evaluation on maps.\n\n    This evaluates a sky model on a 3D map and convolves with the IRFs,\n    and returns a map of the predicted counts.\n    Note that background counts are not added.\n\n    For now, we only make it work for 3D WCS maps with an energy axis.\n    No HPX, no other axes, those can be added later here or via new\n    separate model evaluator classes.\n\n    Parameters\n    ----------\n    model : `~gammapy.modeling.models.SkyModel`\n        Sky model\n    exposure : `~gammapy.maps.Map`\n        Exposure map\n    psf : `~gammapy.cube.PSFKernel`\n        PSF kernel\n    edisp : `~gammapy.irf.EDispKernel`\n        Energy dispersion\n    gti : `~gammapy.data.GTI`\n        GTI of the observation or union of GTI if it is a stacked observation\n    evaluation_mode : {""local"", ""global""}\n        Model evaluation mode.\n    """"""\n\n    def __init__(\n        self,\n        model=None,\n        exposure=None,\n        psf=None,\n        edisp=None,\n        gti=None,\n        evaluation_mode=""local"",\n    ):\n        self.model = model\n        self.exposure = exposure\n        self.psf = psf\n        self.edisp = edisp\n        self.gti = gti\n        self.contributes = True\n\n        if evaluation_mode not in {""local"", ""global""}:\n            raise ValueError(f""Invalid evaluation_mode: {evaluation_mode!r}"")\n\n        self.evaluation_mode = evaluation_mode\n\n        # TODO: this is preliminary solution until we have further unified the model handling\n        if isinstance(model, BackgroundModel):\n            self.compute_npred = model.evaluate\n            self.evaluation_mode = ""global""\n\n    @property\n    def geom(self):\n        """"""True energy map geometry (`~gammapy.maps.Geom`)""""""\n        return self.exposure.geom\n\n    @property\n    def needs_update(self):\n        """"""Check whether the model component has drifted away from its support.""""""\n        # TODO: simplify and clean up\n        if isinstance(self.model, BackgroundModel):\n            return False\n        elif self.exposure is None:\n            return True\n        elif self.evaluation_mode == ""global"" or self.model.evaluation_radius is None:\n            return False\n        else:\n            position = self.model.position\n            separation = self._init_position.separation(position)\n            update = separation > (self.model.evaluation_radius + CUTOUT_MARGIN)\n        return update\n\n    def update(self, exposure, psf, edisp, geom):\n        """"""Update MapEvaluator, based on the current position of the model component.\n\n        Parameters\n        ----------\n        exposure : `~gammapy.maps.Map`\n            Exposure map.\n        psf : `gammapy.cube.PSFMap`\n            PSF map.\n        edisp : `gammapy.cube.EDispMap`\n            Edisp map.\n        geom : `WcsGeom`\n            Counts geom\n        """"""\n        # TODO: simplify and clean up\n        log.debug(""Updating model evaluator"")\n        # cache current position of the model component\n\n        if isinstance(edisp, EDispKernelMap):\n            self.edisp = edisp.get_edisp_kernel(self.model.position)\n        elif isinstance(edisp, EDispMap):\n            e_reco = geom.get_axis_by_name(""energy"").edges\n            self.edisp = edisp.get_edisp_kernel(self.model.position, e_reco=e_reco)\n        else:\n            self.edisp = edisp\n\n        if isinstance(psf, PSFMap):\n            self.psf = psf.get_psf_kernel(self.model.position, geom=exposure.geom)\n        else:\n            self.psf = psf\n\n        if self.evaluation_mode == ""local"" and self.model.evaluation_radius is not None:\n            self._init_position = self.model.position\n            if self.psf is not None:\n                psf_width = np.max(self.psf.psf_kernel_map.geom.width)\n            else:\n                psf_width = 0 * u.deg\n\n            width = psf_width + 2 * (self.model.evaluation_radius + CUTOUT_MARGIN)\n            try:\n                self.exposure = exposure.cutout(\n                    position=self.model.position, width=width\n                )\n                self.contributes = True\n            except (NoOverlapError, ValueError):\n                self.contributes = False\n        else:\n            self.exposure = exposure\n\n    def compute_dnde(self):\n        """"""Compute model differential flux at map pixel centers.\n\n        Returns\n        -------\n        model_map : `~gammapy.maps.Map`\n            Sky cube with data filled with evaluated model values.\n            Units: ``cm-2 s-1 TeV-1 deg-2``\n        """"""\n        return self.model.evaluate_geom(self.geom, self.gti)\n\n    def compute_flux(self):\n        """"""Compute model integral flux over map pixel volumes.\n\n        For now, we simply multiply dnde with bin volume.\n        """"""\n        return self.model.integrate_geom(self.geom, self.gti)\n\n    def apply_exposure(self, flux):\n        """"""Compute npred cube\n\n        For now just divide flux cube by exposure\n        """"""\n        npred = (flux.quantity * self.exposure.quantity).to_value("""")\n        return Map.from_geom(self.geom, data=npred, unit="""")\n\n    def apply_psf(self, npred):\n        """"""Convolve npred cube with PSF""""""\n        tmp = npred.convolve(self.psf)\n        tmp.data[tmp.data < 0.0] = 0\n        return tmp\n\n    def apply_edisp(self, npred):\n        """"""Convolve map data with energy dispersion.\n\n        Parameters\n        ----------\n        npred : `~gammapy.maps.Map`\n            Predicted counts in true energy bins\n\n        Returns\n        -------\n        npred_reco : `~gammapy.maps.Map`\n            Predicted counts in reco energy bins\n        """"""\n        return npred.apply_edisp(self.edisp)\n\n    def compute_npred(self):\n        """"""\n        Evaluate model predicted counts.\n\n        Returns\n        -------\n        npred : `~gammapy.maps.Map`\n            Predicted counts on the map (in reco energy bins)\n        """"""\n        flux = self.compute_flux()\n\n        if self.model.apply_irf[""exposure""]:\n            npred = self.apply_exposure(flux)\n\n        if self.psf and self.model.apply_irf[""psf""]:\n            npred = self.apply_psf(npred)\n\n        if self.model.apply_irf[""edisp""]:\n            npred = self.apply_edisp(npred)\n\n        return npred\n'"
gammapy/datasets/simulate.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Simulate observations""""""\nimport copy\nimport numpy as np\nfrom astropy.coordinates import SkyCoord, SkyOffsetFrame\nimport astropy.units as u\nfrom astropy.table import Table\nimport gammapy\nfrom gammapy.data import EventList\nfrom gammapy.maps import MapCoord\nfrom gammapy.modeling.models import BackgroundModel, ConstantTemporalModel\nfrom gammapy.utils.random import get_random_state\n\n\n__all__ = [""MapDatasetEventSampler""]\n\n\nclass MapDatasetEventSampler:\n    """"""Sample events from a map dataset\n\n    Parameters\n    ----------\n    random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n        Defines random number generator initialisation.\n        Passed to `~gammapy.utils.random.get_random_state`.\n    """"""\n\n    def __init__(self, random_state=""random-seed""):\n        self.random_state = get_random_state(random_state)\n\n    def _sample_coord_time(self, npred, temporal_model, gti):\n        n_events = self.random_state.poisson(np.sum(npred.data))\n\n        coords = npred.sample_coord(n_events=n_events, random_state=self.random_state)\n\n        table = Table()\n        try:\n            energy = coords[""energy_true""]\n        except KeyError:\n            energy = coords[""energy""]\n\n        table[""ENERGY_TRUE""] = energy\n        table[""RA_TRUE""] = coords.skycoord.icrs.ra.to(""deg"")\n        table[""DEC_TRUE""] = coords.skycoord.icrs.dec.to(""deg"")\n\n        time_start, time_stop, time_ref = (gti.time_start, gti.time_stop, gti.time_ref)\n        time = temporal_model.sample_time(\n            n_events=n_events,\n            t_min=time_start,\n            t_max=time_stop,\n            random_state=self.random_state,\n        )\n        table[""TIME""] = u.Quantity(((time.mjd - time_ref.mjd) * u.day).to(u.s)).to(""s"")\n        return table\n\n    def sample_sources(self, dataset):\n        """"""Sample source model components.\n\n        Parameters\n        ----------\n        dataset : `~gammapy.cube.MapDataset`\n            Map dataset.\n\n        Returns\n        -------\n        events : `~gammapy.data.EventList`\n            Event list\n        """"""\n        events_all = []\n        for idx, model in enumerate(dataset.models):\n            if isinstance(model, BackgroundModel):\n                continue\n\n            evaluator = dataset.evaluators.get(model)\n\n            evaluator = copy.deepcopy(evaluator)\n            evaluator.model.apply_irf[""psf""] = False\n            evaluator.model.apply_irf[""edisp""] = False\n            npred = evaluator.compute_npred()\n\n            if hasattr(model, ""temporal_model""):\n                if getattr(model, ""temporal_model"") is None:\n                    temporal_model = ConstantTemporalModel()\n                else:\n                    temporal_model = model.temporal_model\n            else:\n                temporal_model = ConstantTemporalModel()\n\n            table = self._sample_coord_time(npred, temporal_model, dataset.gti)\n            if len(table) > 0:\n                table[""MC_ID""] = idx + 1\n            else:\n                mcid = table.Column(name=""MC_ID"", length=0, dtype=int)\n                table.add_column(mcid)\n            events_all.append(EventList(table))\n\n        return EventList.stack(events_all)\n\n    def sample_background(self, dataset):\n        """"""Sample background\n\n        Parameters\n        ----------\n        dataset : `~gammapy.cube.MapDataset`\n            Map dataset\n\n        Returns\n        -------\n        events : `gammapy.data.EventList`\n            Background events\n        """"""\n        background = dataset.background_model.evaluate()\n\n        temporal_model = ConstantTemporalModel()\n\n        table = self._sample_coord_time(background, temporal_model, dataset.gti)\n\n        table[""MC_ID""] = 0\n        table[""ENERGY""] = table[""ENERGY_TRUE""]\n        table[""RA""] = table[""RA_TRUE""]\n        table[""DEC""] = table[""DEC_TRUE""]\n\n        return EventList(table)\n\n    def sample_edisp(self, edisp_map, events):\n        """"""Sample energy dispersion map.\n\n        Parameters\n        ----------\n        edisp_map : `~gammapy.cube.EDispMap`\n            Energy dispersion map\n        events : `~gammapy.data.EventList`\n            Event list with the true energies\n\n        Returns\n        -------\n        events : `~gammapy.data.EventList`\n            Event list with reconstructed energy column.\n        """"""\n        coord = MapCoord(\n            {\n                ""lon"": events.table[""RA_TRUE""].quantity,\n                ""lat"": events.table[""DEC_TRUE""].quantity,\n                ""energy_true"": events.table[""ENERGY_TRUE""].quantity,\n            },\n            frame=""icrs"",\n        )\n\n        coords_reco = edisp_map.sample_coord(coord, self.random_state)\n        events.table[""ENERGY""] = coords_reco[""energy""]\n        return events\n\n    def sample_psf(self, psf_map, events):\n        """"""Sample psf map.\n\n        Parameters\n        ----------\n        psf_map : `~gammapy.cube.PSFMap`\n            PSF map.\n        events : `~gammapy.data.EventList`\n            Event list.\n\n        Returns\n        -------\n        events : `~gammapy.data.EventList`\n            Event list with reconstructed position columns.\n        """"""\n        coord = MapCoord(\n            {\n                ""lon"": events.table[""RA_TRUE""].quantity,\n                ""lat"": events.table[""DEC_TRUE""].quantity,\n                ""energy_true"": events.table[""ENERGY_TRUE""].quantity,\n            },\n            frame=""icrs"",\n        )\n\n        coords_reco = psf_map.sample_coord(coord, self.random_state)\n        events.table[""RA""] = coords_reco[""lon""] * u.deg\n        events.table[""DEC""] = coords_reco[""lat""] * u.deg\n        return events\n\n    @staticmethod\n    def event_det_coords(observation, events):\n        """"""Add columns of detector coordinates (DETX-DETY) to the event list.\n\n        Parameters\n        ----------\n        observation : `~gammapy.data.Observation`\n            In memory observation.\n        events : `~gammapy.data.EventList`\n            Event list.\n\n        Returns\n        -------\n        events : `~gammapy.data.EventList`\n            Event list with columns of event detector coordinates.\n        """"""\n        sky_coord = SkyCoord(events.table[""RA""], events.table[""DEC""], frame=""icrs"")\n        frame = SkyOffsetFrame(origin=observation.pointing_radec.icrs)\n        pseudo_fov_coord = sky_coord.transform_to(frame)\n\n        events.table[""DETX""] = pseudo_fov_coord.lon\n        events.table[""DETY""] = pseudo_fov_coord.lat\n        return events\n\n    @staticmethod\n    def event_list_meta(dataset, observation):\n        """"""Event list meta info.\n\n        Parameters\n        ----------\n        dataset : `~gammapy.cube.MapDataset`\n            Map dataset.\n        observation : `~gammapy.data.Observation`\n            In memory observation.\n\n        Returns\n        -------\n        meta : dict\n            Meta dictionary.\n        """"""\n        # See: https://gamma-astro-data-formats.readthedocs.io/en/latest/events/events.html#mandatory-header-keywords\n        meta = {}\n\n        meta[""HDUCLAS1""] = ""EVENTS""\n        meta[""EXTNAME""] = ""EVENTS""\n        meta[\n            ""HDUDOC""\n        ] = ""https://github.com/open-gamma-ray-astro/gamma-astro-data-formats""\n        meta[""HDUVERS""] = ""0.2""\n        meta[""HDUCLASS""] = ""GADF""\n\n        meta[""OBS_ID""] = observation.obs_id\n\n        meta[""TSTART""] = (\n            ((observation.tstart.mjd - dataset.gti.time_ref.mjd) * u.day).to(u.s).value\n        )\n        meta[""TSTOP""] = (\n            ((observation.tstop.mjd - dataset.gti.time_ref.mjd) * u.day).to(u.s).value\n        )\n        meta[""ONTIME""] = observation.observation_time_duration.to(""s"").value\n        meta[""LIVETIME""] = observation.observation_live_time_duration.to(""s"").value\n        meta[""DEADC""] = observation.observation_dead_time_fraction\n\n        meta[""RA_PNT""] = observation.pointing_radec.icrs.ra.deg\n        meta[""DEC_PNT""] = observation.pointing_radec.icrs.dec.deg\n\n        meta[""EQUINOX""] = ""J2000""\n        meta[""RADECSYS""] = ""icrs""\n\n        meta[""CREATOR""] = ""Gammapy {}"".format(gammapy.__version__)\n        meta[""EUNIT""] = ""TeV""\n        meta[""EVTVER""] = """"\n\n        meta[""OBSERVER""] = ""Gammapy user""\n        meta[""DSTYP1""] = ""TIME""\n        meta[""DSUNI1""] = ""s""\n        meta[""DSVAL1""] = ""TABLE""\n        meta[""DSREF1""] = "":GTI""\n        meta[""DSTYP2""] = ""ENERGY""\n        meta[""DSUNI2""] = ""TeV""\n        meta[\n            ""DSVAL2""\n        ] = f\'{dataset._geom.get_axis_by_name(""energy"").edges.min().value}:{dataset._geom.get_axis_by_name(""energy"").edges.max().value}\'\n        meta[""DSTYP3""] = ""POS(RA,DEC)     ""\n        meta[\n            ""DSVAL3""\n        ] = f""CIRCLE({observation.pointing_radec.ra.deg},{observation.pointing_radec.dec.deg},{dataset.models[0].evaluation_radius.value})""\n        meta[""DSUNI3""] = ""deg             ""\n        meta[""NDSKEYS""] = "" 3 ""\n\n        if len(dataset.models) > 1:\n            if dataset.models[1] != dataset.background_model:\n                meta[""OBJECT""] = dataset.models[1].name\n                meta[""RA_OBJ""] = dataset.models[1].position.icrs.ra.deg\n                meta[""DEC_OBJ""] = dataset.models[1].position.icrs.dec.deg\n            else:\n                meta[""OBJECT""] = dataset.models[0].name\n                meta[""RA_OBJ""] = dataset.models[0].position.icrs.ra.deg\n                meta[""DEC_OBJ""] = dataset.models[0].position.icrs.dec.deg\n        else:\n            meta[""OBJECT""] = dataset.models[0].name\n            meta[""RA_OBJ""] = dataset.models[0].position.icrs.ra.deg\n            meta[""DEC_OBJ""] = dataset.models[0].position.icrs.dec.deg\n\n        meta[""TELAPSE""] = dataset.gti.time_sum.to(""s"").value\n        meta[""MJDREFI""] = int(dataset.gti.time_ref.mjd)\n        meta[""MJDREFF""] = dataset.gti.time_ref.mjd % 1\n        meta[""TIMEUNIT""] = ""s""\n        meta[""TIMESYS""] = dataset.gti.time_ref.scale\n        meta[""TIMEREF""] = ""LOCAL""\n        meta[""DATE-OBS""] = dataset.gti.time_start.isot[0][0:10]\n        meta[""DATE-END""] = dataset.gti.time_stop.isot[0][0:10]\n        meta[""TIME-OBS""] = dataset.gti.time_start.isot[0][11:23]\n        meta[""TIME-END""] = dataset.gti.time_stop.isot[0][11:23]\n        meta[""TIMEDEL""] = 1e-9\n        meta[""CONV_DEP""] = 0\n        meta[""CONV_RA""] = 0\n        meta[""CONV_DEC""] = 0\n\n        for idx, model in enumerate(dataset.models):\n            meta[""MID{:05d}"".format(idx + 1)] = idx + 1\n            meta[""MMN{:05d}"".format(idx + 1)] = model.name\n        meta[""NMCIDS""] = len(dataset.models)\n\n        # Necessary for DataStore, but they should be ALT and AZ instead!\n        meta[""ALTITUDE""] = observation.aeff.meta[""CBD50001""][7:-4]\n        meta[""ALT_PNT""] = observation.aeff.meta[""CBD50001""][7:-4]\n        meta[""AZ_PNT""] = observation.aeff.meta[""CBD60001""][8:-4]\n\n        # TO DO: these keywords should be taken from the IRF of the dataset\n        meta[""ORIGIN""] = ""Gammapy""\n        meta[""CALDB""] = observation.aeff.meta[""CBD20001""][8:-1]\n        meta[""IRF""] = observation.aeff.meta[""CBD10001""][5:-2]\n        meta[""TELESCOP""] = observation.aeff.meta[""TELESCOP""]\n        meta[""INSTRUME""] = observation.aeff.meta[""INSTRUME""]\n        meta[""N_TELS""] = """"\n        meta[""TELLIST""] = """"\n        meta[""GEOLON""] = """"\n        meta[""GEOLAT""] = """"\n        # TO BE ADDED\n        #        meta[""CREATED""] = """"\n        #        meta[""OBS_MODE""] = """"\n        #        meta[""EV_CLASS""] = """"\n\n        return meta\n\n    def run(self, dataset, observation=None):\n        """"""Run the event sampler, applying IRF corrections.\n\n        Parameters\n        ----------\n        dataset : `~gammapy.cube.MapDataset`\n            Map dataset\n        observation : `~gammapy.data.Observation`\n            In memory observation.\n        edisp : Bool\n            It allows to include or exclude the Edisp in the simulation.\n\n        Returns\n        -------\n        events : `~gammapy.data.EventList`\n            Event list.\n        """"""\n        if len(dataset.models) > 1:\n            events_src = self.sample_sources(dataset)\n\n            if len(events_src.table) > 0:\n                if dataset.psf:\n                    events_src = self.sample_psf(dataset.psf, events_src)\n                else:\n                    events_src.table[""RA""] = events_src.table[""RA_TRUE""]\n                    events_src.table[""DEC""] = events_src.table[""DEC_TRUE""]\n\n                if dataset.edisp:\n                    events_src = self.sample_edisp(dataset.edisp, events_src)\n                else:\n                    events_src.table[""ENERGY""] = events_src.table[""ENERGY_TRUE""]\n\n            if dataset.background_model:\n                events_bkg = self.sample_background(dataset)\n                events = EventList.stack([events_bkg, events_src])\n            else:\n                events = events_src\n\n        if len(dataset.models) == 1 and dataset.background_model is not None:\n            events_bkg = self.sample_background(dataset)\n            events = EventList.stack([events_bkg])\n\n        events = self.event_det_coords(observation, events)\n        events.table[""EVENT_ID""] = np.arange(len(events.table))\n        events.table.meta = self.event_list_meta(dataset, observation)\n\n        geom = dataset._geom\n        selection = geom.contains(events.map_coord(geom))\n        return events.select_row_subset(selection)\n'"
gammapy/datasets/spectrum.py,31,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom pathlib import Path\nimport numpy as np\nfrom astropy import units as u\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom gammapy.data import GTI\nfrom gammapy.datasets import Dataset\nfrom gammapy.irf import EDispKernel, EffectiveAreaTable, IRFStacker\nfrom gammapy.maps import RegionGeom, RegionNDMap\nfrom gammapy.modeling.models import Models, ProperModels\nfrom gammapy.stats import CashCountsStatistic, WStatCountsStatistic, cash, wstat\nfrom gammapy.utils.fits import energy_axis_to_ebounds\nfrom gammapy.utils.random import get_random_state\nfrom gammapy.utils.scripts import make_name, make_path\nfrom .map import MapEvaluator\n\n__all__ = [\n    ""SpectrumDatasetOnOff"",\n    ""SpectrumDataset"",\n]\n\n\nclass SpectrumDataset(Dataset):\n    """"""Spectrum dataset for likelihood fitting.\n\n    The spectrum dataset bundles reduced counts data, with a spectral model,\n    background model and instrument response function to compute the fit-statistic\n    given the current model and data.\n\n    Parameters\n    ----------\n    models : `~gammapy.modeling.models.Models`\n        Fit model\n    counts : `~gammapy.maps.RegionNDMap`\n        Counts spectrum\n    livetime : `~astropy.units.Quantity`\n        Livetime\n    aeff : `~gammapy.irf.EffectiveAreaTable`\n        Effective area\n    edisp : `~gammapy.irf.EDispKernel`\n        Energy dispersion\n    background : `~gammapy.maps.RegionNDMap`\n        Background to use for the fit.\n    mask_safe : `~gammapy.maps.RegionNDMap`\n        Mask defining the safe data range.\n    mask_fit : `~gammapy.maps.RegionNDMap`\n        Mask to apply to the likelihood for fitting.\n    name : str\n        Dataset name.\n    gti : `~gammapy.data.GTI`\n        GTI of the observation or union of GTI if it is a stacked observation\n\n    See Also\n    --------\n    SpectrumDatasetOnOff, FluxPointsDataset, gammapy.cube.MapDataset\n    """"""\n\n    stat_type = ""cash""\n    tag = ""SpectrumDataset""\n\n    def __init__(\n        self,\n        models=None,\n        counts=None,\n        livetime=None,\n        aeff=None,\n        edisp=None,\n        background=None,\n        mask_safe=None,\n        mask_fit=None,\n        name=None,\n        gti=None,\n    ):\n\n        if mask_fit is not None and mask_fit.dtype != np.dtype(""bool""):\n            raise ValueError(""mask data must have dtype bool"")\n\n        self.counts = counts\n\n        if livetime is not None:\n            livetime = u.Quantity(livetime)\n\n        self.livetime = livetime\n        self.mask_fit = mask_fit\n        self.aeff = aeff\n        self.edisp = edisp\n        self.background = background\n        self.mask_safe = mask_safe\n        self.gti = gti\n\n        self._name = make_name(name)\n        self.models = models\n\n    @property\n    def name(self):\n        return self._name\n\n    def __str__(self):\n        str_ = self.__class__.__name__ + ""\\n""\n        str_ += ""-"" * len(self.__class__.__name__) + ""\\n""\n        str_ += ""\\n""\n\n        str_ += ""\\t{:32}: {} \\n\\n"".format(""Name"", self.name)\n\n        counts = np.nan\n        if self.counts is not None:\n            counts = np.sum(self.counts.data)\n        str_ += ""\\t{:32}: {:.0f} \\n"".format(""Total counts"", counts)\n\n        npred = np.nan\n        if self.models is not None:\n            npred = np.sum(self.npred().data)\n        str_ += ""\\t{:32}: {:.2f}\\n"".format(""Total predicted counts"", npred)\n\n        counts_off = np.nan\n        if getattr(self, ""counts_off"", None) is not None:\n            counts_off = np.sum(self.counts_off.data)\n            str_ += ""\\t{:32}: {:.2f}\\n\\n"".format(""Total off counts"", counts_off)\n\n        background = np.nan\n        if getattr(self, ""background"", None) is not None:\n            background = np.sum(self.background.data)\n            str_ += ""\\t{:32}: {:.2f}\\n\\n"".format(""Total background counts"", background)\n\n        aeff_min, aeff_max, aeff_unit = np.nan, np.nan, """"\n        if self.aeff is not None:\n            try:\n                aeff_min = np.min(\n                    self.aeff.data.data.value[self.aeff.data.data.value > 0]\n                )\n            except ValueError:\n                aeff_min = 0\n            aeff_max = np.max(self.aeff.data.data.value)\n            aeff_unit = self.aeff.data.data.unit\n\n        str_ += ""\\t{:32}: {:.2e} {}\\n"".format(""Effective area min"", aeff_min, aeff_unit)\n        str_ += ""\\t{:32}: {:.2e} {}\\n\\n"".format(\n            ""Effective area max"", aeff_max, aeff_unit\n        )\n\n        livetime = np.nan\n        if self.livetime is not None:\n            livetime = self.livetime\n        str_ += ""\\t{:32}: {:.2e}\\n\\n"".format(""Livetime"", livetime)\n\n        # data section\n        n_bins = 0\n        if self.counts is not None:\n            n_bins = self.counts.data.size\n        str_ += ""\\t{:32}: {} \\n"".format(""Number of total bins"", n_bins)\n\n        n_fit_bins = 0\n        if self.mask is not None:\n            n_fit_bins = np.sum(self.mask)\n        str_ += ""\\t{:32}: {} \\n\\n"".format(""Number of fit bins"", n_fit_bins)\n\n        # likelihood section\n        str_ += ""\\t{:32}: {}\\n"".format(""Fit statistic type"", self.stat_type)\n\n        stat = np.nan\n        if self.models is not None and self.counts is not None:\n            stat = self.stat_sum()\n        str_ += ""\\t{:32}: {:.2f}\\n\\n"".format(""Fit statistic value (-2 log(L))"", stat)\n\n        n_pars, n_free_pars = 0, 0\n        if self.models is not None:\n            n_pars = len(self.models.parameters)\n            n_free_pars = len(self.models.parameters.free_parameters)\n\n        str_ += ""\\t{:32}: {}\\n"".format(""Number of parameters"", n_pars)\n        str_ += ""\\t{:32}: {}\\n\\n"".format(""Number of free parameters"", n_free_pars)\n\n        if self.models is not None:\n            str_ += ""\\t"" + ""\\n\\t"".join(str(self.models).split(""\\n"")[2:])\n\n        return str_.expandtabs(tabsize=2)\n\n    @property\n    def evaluators(self):\n        """"""Model evaluators""""""\n\n        if self.models:\n            for model in self.models:\n                evaluator = self._evaluators.get(model)\n\n                if evaluator is None:\n                    evaluator = MapEvaluator(\n                        model=model,\n                        exposure=self.exposure,\n                        edisp=self.edisp,\n                        gti=self.gti,\n                    )\n                    self._evaluators[model] = evaluator\n\n        keys = list(self._evaluators.keys())\n        for key in keys:\n            if key not in self.models:\n                del self._evaluators[key]\n\n        return self._evaluators\n\n    @property\n    def models(self):\n        """"""Models (`gammapy.modeling.models.Models`).""""""\n        return ProperModels(self)\n\n    @models.setter\n    def models(self, models):\n        if models is None:\n            self._models = None\n        else:\n            self._models = Models(models)\n        # reset evaluators\n        self._evaluators = {}\n\n    @property\n    def mask_safe(self):\n        if self._mask_safe is None:\n            data = np.ones(self._geom.data_shape, dtype=bool)\n            return RegionNDMap.from_geom(self._geom, data=data)\n        else:\n            return self._mask_safe\n\n    @mask_safe.setter\n    def mask_safe(self, mask):\n        if mask is None or isinstance(mask, RegionNDMap):\n            self._mask_safe = mask\n        else:\n            raise ValueError(f""Must be `RegionNDMap` and not {type(mask)}"")\n\n    @property\n    def _energy_axis(self):\n        if self.counts is not None:\n            e_axis = self.counts.geom.get_axis_by_name(""energy"")\n        elif self.edisp is not None:\n            e_axis = self.edisp.data.axis(""energy"")\n        elif self.aeff is not None:\n            # assume e_reco = e_true\n            e_axis = self.aeff.data.axis(""energy_true"")\n        return e_axis\n\n    @property\n    def _geom(self):\n        """"""Main analysis geometry""""""\n        if self.counts is not None:\n            return self.counts.geom\n        elif self.background is not None:\n            return self.background.geom\n        else:\n            raise ValueError(""Either \'counts\', \'background\' must be defined."")\n\n    @property\n    def data_shape(self):\n        """"""Shape of the counts data""""""\n        return self._geom.data_shape\n\n    def npred_sig(self):\n        """"""Predicted counts from source model (`RegionNDMap`).""""""\n        npred_total = RegionNDMap.from_geom(self._geom)\n\n        for key in self.evaluators:\n            npred = self.evaluators[key].compute_npred()\n            npred_total.stack(npred)\n\n        return npred_total\n\n    def npred(self):\n        """"""Return npred map (model + background)""""""\n        npred = self.npred_sig()\n\n        if self.background:\n            npred += self.background\n\n        return npred\n\n    def stat_array(self):\n        """"""Likelihood per bin given the current model parameters""""""\n        return cash(n_on=self.counts.data, mu_on=self.npred().data)\n\n    @property\n    def excess(self):\n        """"""Excess (counts - alpha * counts_off)""""""\n        return self.counts - self.background\n\n    @property\n    def exposure(self):\n        """"""Excess (aeff * livetime)""""""\n        data = self.livetime * self.aeff.data.data\n        geom = RegionGeom(region=None, axes=[self.aeff.energy])\n        return RegionNDMap.from_geom(geom=geom, data=data.value, unit=data.unit)\n\n    def fake(self, random_state=""random-seed""):\n        """"""Simulate fake counts for the current model and reduced irfs.\n\n        This method overwrites the counts defined on the dataset object.\n\n        Parameters\n        ----------\n        random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n            Defines random number generator initialisation.\n            Passed to `~gammapy.utils.random.get_random_state`.\n        """"""\n        random_state = get_random_state(random_state)\n        npred = self.npred()\n        npred.data = random_state.poisson(npred.data)\n        self.counts = npred\n\n    @property\n    def energy_range(self):\n        """"""Energy range defined by the safe mask""""""\n        energy = self._energy_axis.edges\n        e_min, e_max = energy[:-1], energy[1:]\n\n        if self.mask_safe is not None:\n            if self.mask_safe.data.any():\n                e_min = e_min[self.mask_safe.data[:, 0, 0]]\n                e_max = e_max[self.mask_safe.data[:, 0, 0]]\n            else:\n                return None, None\n\n        return u.Quantity([e_min.min(), e_max.max()])\n\n    def plot_fit(self):\n        """"""Plot counts and residuals in two panels.\n\n        Calls ``plot_counts`` and ``plot_residuals``.\n        """"""\n        from matplotlib.gridspec import GridSpec\n        import matplotlib.pyplot as plt\n\n        gs = GridSpec(7, 1)\n\n        ax_spectrum = plt.subplot(gs[:5, :])\n        self.plot_counts(ax=ax_spectrum)\n\n        ax_spectrum.set_xticks([] * u.TeV)\n\n        ax_residuals = plt.subplot(gs[5:, :])\n        self.plot_residuals(ax=ax_residuals)\n        return ax_spectrum, ax_residuals\n\n    @property\n    def _e_unit(self):\n        return self._geom.axes[0].unit\n\n    def _plot_energy_range(self, ax):\n        e_min, e_max = self.energy_range\n        kwargs = {""color"": ""black"", ""linestyle"": ""dashed""}\n        ax.axvline(e_min.to_value(self._e_unit), label=""fit range"", **kwargs)\n        ax.axvline(e_max.to_value(self._e_unit), **kwargs)\n\n    def plot_counts(self, ax=None):\n        """"""Plot predicted and detected counts.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.pyplot.Axes`\n            Axes object.\n\n        Returns\n        -------\n        ax : `~matplotlib.pyplot.Axes`\n            Axes object.\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        self.npred_sig().plot(ax=ax, label=""mu_src"")\n        self.excess.plot(ax=ax, label=""Excess"")\n        self._plot_energy_range(ax=ax)\n\n        ax.legend(numpoints=1)\n        ax.set_title("""")\n        return ax\n\n    def residuals(self, method=""diff""):\n        """"""Compute the spectral residuals.\n\n        Parameters\n        ----------\n        method : {""diff"", ""diff/model"", ""diff/sqrt(model)""}\n            Method used to compute the residuals. Available options are:\n                - ``diff`` (default): data - model\n                - ``diff/model``: (data - model) / model\n                - ``diff/sqrt(model)``: (data - model) / sqrt(model)\n\n        Returns\n        -------\n        residuals : `RegionNDMap`\n            Residual spectrum\n        """"""\n        residuals = self._compute_residuals(self.counts, self.npred(), method)\n        return residuals\n\n    def plot_residuals(self, method=""diff"", ax=None, **kwargs):\n        """"""Plot residuals.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.pyplot.Axes`\n            Axes object.\n        method : {""diff"", ""diff/model"", ""diff/sqrt(model)""}\n            Normalization used to compute the residuals, see `SpectrumDataset.residuals()`\n        **kwargs : dict\n            Keywords passed to `RegionNDMap.plot()`\n\n        Returns\n        -------\n        ax : `~matplotlib.pyplot.Axes`\n            Axes object.\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        residuals = self.residuals(method=method)\n        label = self._residuals_labels[method]\n\n        residuals.plot(ax=ax, color=""black"", **kwargs)\n        ax.axhline(0, color=""black"", lw=0.5)\n\n        ax.set_xlabel(f""Energy [{self._e_unit}]"")\n        ax.set_ylabel(f""Residuals ({label})"")\n        ax.set_yscale(""linear"")\n\n        ymax = 1.2 * np.nanmax(residuals.data)\n        ax.set_ylim(-ymax, ymax)\n        return ax\n\n    @classmethod\n    def create(\n        cls, e_reco, e_true=None, region=None, reference_time=""2000-01-01"", name=None\n    ):\n        """"""Creates empty spectrum dataset.\n\n        Empty containers are created with the correct geometry.\n        counts, background and aeff are zero and edisp is diagonal.\n\n        The safe_mask is set to False in every bin.\n\n        Parameters\n        ----------\n        e_reco : `~gammapy.maps.MapAxis`\n            counts energy axis. Its name must be ""energy"".\n        e_true : `~gammapy.maps.MapAxis`\n            effective area table energy axis. Its name must be ""energy-true"".\n            If not set use reco energy values. Default : None\n        region : `~regions.SkyRegion`\n            Region to define the dataset for.\n        reference_time : `~astropy.time.Time`\n            reference time of the dataset, Default is ""2000-01-01""\n        """"""\n        if e_true is None:\n            e_true = e_reco.copy(name=""energy_true"")\n\n        if region is None:\n            region = ""icrs;circle(0, 0, 1)""\n\n        counts = RegionNDMap.create(region=region, axes=[e_reco])\n        background = RegionNDMap.create(region=region, axes=[e_reco])\n\n        aeff = EffectiveAreaTable(\n            e_true.edges[:-1],\n            e_true.edges[1:],\n            np.zeros(e_true.edges[:-1].shape) * u.m ** 2,\n        )\n        edisp = EDispKernel.from_diagonal_response(e_true.edges, e_reco.edges)\n        mask_safe = RegionNDMap.from_geom(counts.geom, dtype=""bool"")\n        gti = GTI.create(u.Quantity([], ""s""), u.Quantity([], ""s""), reference_time)\n        livetime = gti.time_sum\n\n        return SpectrumDataset(\n            counts=counts,\n            aeff=aeff,\n            edisp=edisp,\n            mask_safe=mask_safe,\n            background=background,\n            livetime=livetime,\n            gti=gti,\n            name=name,\n        )\n\n    def stack(self, other):\n        r""""""Stack this dataset with another one.\n\n        Safe mask is applied to compute the stacked counts vector.\n        Counts outside each dataset safe mask are lost.\n\n        Stacking is performed in-place.\n\n        The stacking of 2 datasets is implemented as follows.\n        Here, :math:`k` denotes a bin in reconstructed energy and :math:`j = {1,2}` is the dataset number\n\n        The ``mask_safe`` of each dataset is defined as:\n\n        .. math::\n            \\epsilon_{jk} =\\left\\{\\begin{array}{cl} 1, &\n            \\mbox{if bin k is inside the energy thresholds}\\\\ 0, &\n            \\mbox{otherwise} \\end{array}\\right.\n\n        Then the total ``counts`` and model background ``bkg`` are computed according to:\n\n        .. math::\n            \\overline{\\mathrm{n_{on}}}_k =  \\mathrm{n_{on}}_{1k} \\cdot \\epsilon_{1k} +\n             \\mathrm{n_{on}}_{2k} \\cdot \\epsilon_{2k}\n\n            \\overline{bkg}_k = bkg_{1k} \\cdot \\epsilon_{1k} +\n             bkg_{2k} \\cdot \\epsilon_{2k}\n\n        The stacked ``safe_mask`` is then:\n\n        .. math::\n            \\overline{\\epsilon_k} = \\epsilon_{1k} OR \\epsilon_{2k}\n\n        Please refer to the `~gammapy.irf.IRFStacker` for the description\n        of how the IRFs are stacked.\n\n        Parameters\n        ----------\n        other : `~gammapy.spectrum.SpectrumDataset`\n            the dataset to stack to the current one\n        """"""\n        if not isinstance(other, SpectrumDataset):\n            raise TypeError(""Incompatible types for SpectrumDataset stacking"")\n\n        if self.counts is not None:\n            self.counts *= self.mask_safe\n            self.counts.stack(other.counts, weights=other.mask_safe)\n\n        if self.background is not None and self.stat_type == ""cash"":\n            self.background *= self.mask_safe\n            self.background.stack(other.background, weights=other.mask_safe)\n\n        if self.aeff is not None:\n            if self.livetime is None or other.livetime is None:\n                raise ValueError(""IRF stacking requires livetime for both datasets."")\n\n            irf_stacker = IRFStacker(\n                list_aeff=[self.aeff, other.aeff],\n                list_livetime=[self.livetime, other.livetime],\n                list_edisp=[self.edisp, other.edisp],\n                list_low_threshold=[self.energy_range[0], other.energy_range[0]],\n                list_high_threshold=[self.energy_range[1], other.energy_range[1]],\n            )\n            irf_stacker.stack_aeff()\n            if self.edisp is not None:\n                irf_stacker.stack_edisp()\n                self.edisp = irf_stacker.stacked_edisp\n            self.aeff = irf_stacker.stacked_aeff\n\n        if self.mask_safe is not None and other.mask_safe is not None:\n            self.mask_safe.stack(other.mask_safe)\n\n        if self.gti is not None:\n            self.gti = self.gti.stack(other.gti).union()\n\n        # TODO: for the moment, since dead time is not accounted for, livetime cannot be the sum of GTIs\n        if self.livetime is not None:\n            self.livetime += other.livetime\n\n    def peek(self, figsize=(16, 4)):\n        """"""Quick-look summary plots.""""""\n        import matplotlib.pyplot as plt\n\n        e_min, e_max = self.energy_range\n\n        _, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=figsize)\n\n        ax1.set_title(""Counts"")\n\n        if isinstance(self, SpectrumDatasetOnOff) and self.counts_off is not None:\n            self.background.plot_hist(\n                ax=ax1, label=""alpha * N_off"",\n            )\n        elif self.background is not None:\n            self.background.plot_hist(\n                ax=ax1, label=""background"",\n            )\n\n        self.counts.plot_hist(\n            ax=ax1, label=""n_on"",\n        )\n\n        e_unit = e_min.unit\n        ax1.set_xlim(0.7 * e_min.to_value(e_unit), 1.3 * e_max.to_value(e_unit))\n        self._plot_energy_range(ax=ax1)\n        ax1.legend(numpoints=1)\n\n        ax2.set_title(""Effective Area"")\n        e_unit = self.aeff.energy.unit\n        self.aeff.plot(ax=ax2,)\n        ax2.set_xlim(0.7 * e_min.to_value(e_unit), 1.3 * e_max.to_value(e_unit))\n        self._plot_energy_range(ax=ax2)\n\n        ax3.set_title(""Energy Dispersion"")\n        if self.edisp is not None:\n            self.edisp.plot_matrix(ax=ax3)\n\n        # TODO: optimize layout\n        plt.subplots_adjust(wspace=0.3)\n\n    def info_dict(self, in_safe_energy_range=True):\n        """"""Info dict with summary statistics, summed over energy\n\n        Parameters\n        ----------\n        in_safe_energy_range : bool\n            Whether to sum only in the safe energy range\n\n        Returns\n        -------\n        info_dict : dict\n            Dictionary with summary info.\n        """"""\n        info = dict()\n        mask = self.mask_safe.data if in_safe_energy_range else slice(None)\n\n        info[""name""] = self.name\n        info[""livetime""] = self.livetime.copy()\n\n        info[""n_on""] = self.counts.data[mask].sum()\n\n        info[""background""] = self.background.data[mask].sum()\n        info[""excess""] = self.excess.data[mask].sum()\n        info[""significance""] = CashCountsStatistic(\n            self.counts.data[mask].sum(), self.background.data[mask].sum(),\n        ).significance\n\n        info[""background_rate""] = info[""background""] / info[""livetime""]\n        info[""gamma_rate""] = info[""excess""] / info[""livetime""]\n        return info\n\n\nclass SpectrumDatasetOnOff(SpectrumDataset):\n    """"""Spectrum dataset for on-off likelihood fitting.\n\n    The on-off spectrum dataset bundles reduced counts data, off counts data,\n    with a spectral model, relative background efficiency and instrument\n    response functions to compute the fit-statistic given the current model\n    and data.\n\n    Parameters\n    ----------\n    models : `~gammapy.modeling.models.Models`\n        Fit model\n    counts : `~gammapy.maps.RegionNDMap`\n        ON Counts spectrum\n    counts_off : `~gammapy.maps.RegionNDMap`\n        OFF Counts spectrum\n    livetime : `~astropy.units.Quantity`\n        Livetime\n    aeff : `~gammapy.irf.EffectiveAreaTable`\n        Effective area\n    edisp : `~gammapy.irf.EDispKernel`\n        Energy dispersion\n    mask_safe : `~gammapy.maps.RegionNDMap`\n        Mask defining the safe data range.\n    mask_fit : `~gammapy.maps.RegionNDMap`\n        Mask to apply to the likelihood for fitting.\n    acceptance : `~gammapy.maps.RegionNDMap` or float\n        Relative background efficiency in the on region.\n    acceptance_off : `~gammapy.maps.RegionNDMap` or float\n        Relative background efficiency in the off region.\n    name : str\n        Name of the dataset.\n    gti : `~gammapy.data.GTI`\n        GTI of the observation or union of GTI if it is a stacked observation\n\n    See Also\n    --------\n    SpectrumDataset, FluxPointsDataset, MapDataset\n    """"""\n\n    stat_type = ""wstat""\n    tag = ""SpectrumDatasetOnOff""\n\n    def __init__(\n        self,\n        models=None,\n        counts=None,\n        counts_off=None,\n        livetime=None,\n        aeff=None,\n        edisp=None,\n        mask_safe=None,\n        mask_fit=None,\n        acceptance=None,\n        acceptance_off=None,\n        name=None,\n        gti=None,\n    ):\n\n        self.counts = counts\n        self.counts_off = counts_off\n\n        if livetime is not None:\n            livetime = u.Quantity(livetime)\n\n        self.livetime = livetime\n        self.mask_fit = mask_fit\n        self.aeff = aeff\n        self.edisp = edisp\n        self.mask_safe = mask_safe\n\n        if np.isscalar(acceptance):\n            data = np.ones(self._geom.data_shape) * acceptance\n            acceptance = RegionNDMap.from_geom(self._geom, data=data)\n\n        self.acceptance = acceptance\n\n        if np.isscalar(acceptance_off):\n            data = np.ones(self._geom.data_shape) * acceptance_off\n            acceptance_off = RegionNDMap.from_geom(self._geom, data=data)\n\n        self.acceptance_off = acceptance_off\n\n        self._evaluators = {}\n        self._name = make_name(name)\n        self.gti = gti\n        self.models = models\n\n    def __str__(self):\n        str_ = super().__str__()\n\n        str_list = str_.split(""\\n"")\n\n        acceptance = np.nan\n        if self.acceptance is not None:\n            acceptance = np.mean(self.acceptance.data)\n\n        str_acc = ""\\t{:32}: {}\\n"".format(""Acceptance mean:"", acceptance)\n        str_list.insert(16, str_acc)\n        str_ = ""\\n"".join(str_list)\n        return str_.expandtabs(tabsize=2)\n\n    @property\n    def background(self):\n        """"""""""""\n        return self.alpha * self.counts_off.data\n\n    @property\n    def alpha(self):\n        """"""Exposure ratio between signal and background regions""""""\n        return self.acceptance / self.acceptance_off\n\n    @property\n    def _geom(self):\n        """"""Main analysis geometry""""""\n        if self.counts is not None:\n            return self.counts.geom\n        elif self.counts_off is not None:\n            return self.counts_off.geom\n        elif self.acceptance is not None:\n            return self.acceptance.geom\n        elif self.acceptance_off is not None:\n            return self.acceptance_off.geom\n        else:\n            raise ValueError(\n                ""Either \'counts\', \'counts_off\', \'acceptance\' or \'acceptance_of\' must be defined.""\n            )\n\n    def stat_array(self):\n        """"""Likelihood per bin given the current model parameters""""""\n        mu_sig = self.npred_sig().data\n        on_stat_ = wstat(\n            n_on=self.counts.data,\n            n_off=self.counts_off.data,\n            alpha=self.alpha.data,\n            mu_sig=mu_sig,\n        )\n        return np.nan_to_num(on_stat_)\n\n    def fake(self, background_model, random_state=""random-seed""):\n        """"""Simulate fake counts for the current model and reduced irfs.\n\n        This method overwrites the counts and off counts defined on the dataset object.\n\n        Parameters\n        ----------\n        background_model : `~gammapy.maps.RegionNDMap`\n            Background model.\n        random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n            Defines random number generator initialisation.\n            Passed to `~gammapy.utils.random.get_random_state`.\n        """"""\n        random_state = get_random_state(random_state)\n\n        npred_sig = self.npred_sig()\n        npred_sig.data = random_state.poisson(npred_sig.data)\n\n        npred_bkg = background_model.copy()\n        npred_bkg.data = random_state.poisson(npred_bkg.data)\n\n        self.counts = npred_sig + npred_bkg\n\n        npred_off = background_model / self.alpha\n        npred_off.data = random_state.poisson(npred_off.data)\n        self.counts_off = npred_off\n\n    @classmethod\n    def create(\n        cls, e_reco, e_true=None, region=None, reference_time=""2000-01-01"", name=None\n    ):\n        """"""Create empty SpectrumDatasetOnOff.\n\n        Empty containers are created with the correct geometry.\n        counts, counts_off and aeff are zero and edisp is diagonal.\n\n        The safe_mask is set to False in every bin.\n\n        Parameters\n        ----------\n        e_reco : `~gammapy.maps.MapAxis`\n            counts energy axis. Its name must be ""energy"".\n        e_true : `~gammapy.maps.MapAxis`\n            effective area table energy axis. Its name must be ""energy-true"".\n            If not set use reco energy values. Default : None\n        region : `~regions.SkyRegion`\n            Region to define the dataset for.\n        reference_time : `~astropy.time.Time`\n            reference time of the dataset, Default is ""2000-01-01""\n        """"""\n        dataset = super().create(\n            e_reco=e_reco,\n            e_true=e_true,\n            region=region,\n            reference_time=reference_time,\n            name=name,\n        )\n\n        counts_off = dataset.counts.copy()\n        acceptance = RegionNDMap.from_geom(counts_off.geom, dtype=int)\n        acceptance.data += 1\n\n        acceptance_off = RegionNDMap.from_geom(counts_off.geom, dtype=int)\n        acceptance_off.data += 1\n\n        return cls.from_spectrum_dataset(\n            dataset=dataset,\n            acceptance=acceptance,\n            acceptance_off=acceptance_off,\n            counts_off=counts_off,\n        )\n\n    @classmethod\n    def read(cls, filename):\n        """"""Read from file\n\n        For now, filename is assumed to the name of a PHA file where BKG file, ARF, and RMF names\n        must be set in the PHA header and be present in the same folder\n\n        Parameters\n        ----------\n        filename : str\n            OGIP PHA file to read\n        """"""\n        raise NotImplementedError(\n            ""To read from an OGIP fits file use SpectrumDatasetOnOff.from_ogip_files.""\n        )\n\n    def _is_stackable(self):\n        """"""Check if the Dataset contains enough information to be stacked""""""\n        if (\n            self.acceptance_off is None\n            or self.acceptance is None\n            or self.counts_off is None\n        ):\n            return False\n        else:\n            return True\n\n    def stack(self, other):\n        r""""""Stack this dataset with another one.\n\n        Safe mask is applied to compute the stacked counts vector.\n        Counts outside each dataset safe mask are lost.\n\n        Stacking is performed in-place.\n\n        The stacking of 2 datasets is implemented as follows.\n        Here, :math:`k`  denotes a bin in reconstructed energy and :math:`j = {1,2}` is the dataset number\n\n        The ``mask_safe`` of each dataset is defined as:\n\n        .. math::\n            \\epsilon_{jk} =\\left\\{\\begin{array}{cl} 1, &\n            \\mbox{if k is inside the energy thresholds}\\\\ 0, &\n            \\mbox{otherwise} \\end{array}\\right.\n\n        Then the total ``counts`` and ``counts_off`` are computed according to:\n\n        .. math::\n            \\overline{\\mathrm{n_{on}}}_k =  \\mathrm{n_{on}}_{1k} \\cdot \\epsilon_{1k} +\n            \\mathrm{n_{on}}_{2k} \\cdot \\epsilon_{2k}\n\n            \\overline{\\mathrm{n_{off}}}_k = \\mathrm{n_{off}}_{1k} \\cdot \\epsilon_{1k} +\n            \\mathrm{n_{off}}_{2k} \\cdot \\epsilon_{2k}\n\n        The stacked ``safe_mask`` is then:\n\n        .. math::\n            \\overline{\\epsilon_k} = \\epsilon_{1k} OR \\epsilon_{2k}\n\n        In each energy bin :math:`k`, the count excess is computed taking into account the ON ``acceptance``,\n        :math:`a_{on}_k` and the OFF one: ``acceptance_off``, :math:`a_{off}_k`. They define\n        the :math:`\\alpha_k=a_{on}_k/a_{off}_k` factors such that :math:`n_{ex}_k = n_{on}_k - \\alpha_k n_{off}_k`.\n        We define the stacked value of :math:`\\overline{{a}_{on}}_k = 1` so that:\n\n        .. math::\n            \\overline{{a}_{off}}_k = \\frac{\\overline{\\mathrm {n_{off}}}}{\\alpha_{1k} \\cdot\n            \\mathrm{n_{off}}_{1k} \\cdot \\epsilon_{1k} + \\alpha_{2k} \\cdot \\mathrm{n_{off}}_{2k} \\cdot \\epsilon_{2k}}\n\n        Please refer to the `~gammapy.irf.IRFStacker` for the description\n        of how the IRFs are stacked.\n\n        Parameters\n        ----------\n        other : `~gammapy.spectrum.SpectrumDatasetOnOff`\n            the dataset to stack to the current one\n\n        Examples\n        --------\n        >>> from gammapy.datasets import SpectrumDatasetOnOff\n        >>> obs_ids = [23523, 23526, 23559, 23592]\n        >>> datasets = []\n        >>> for obs in obs_ids:\n        >>>     filename = ""$GAMMAPY_DATA/joint-crab/spectra/hess/pha_obs{}.fits""\n        >>>     ds = SpectrumDatasetOnOff.from_ogip_files(filename.format(obs))\n        >>>     datasets.append(ds)\n        >>> stacked = datasets[0]\n        >>> for ds in datasets[1:]:\n        >>>     stacked.stack(ds)\n        >>> print(stacked.livetime)\n        6313.8116406202325 s\n        """"""\n        if not isinstance(other, SpectrumDatasetOnOff):\n            raise TypeError(""Incompatible types for SpectrumDatasetOnOff stacking"")\n\n        # We assume here that counts_off, acceptance and acceptance_off are well defined.\n        if not self._is_stackable() or not other._is_stackable():\n            raise ValueError(""Cannot stack incomplete SpectrumDatsetOnOff."")\n\n        geom = self.counts.geom\n        total_off = RegionNDMap.from_geom(geom)\n        total_alpha = RegionNDMap.from_geom(geom)\n\n        total_off.stack(self.counts_off, weights=self.mask_safe)\n        total_off.stack(other.counts_off, weights=other.mask_safe)\n\n        total_alpha.stack(self.alpha * self.counts_off, weights=self.mask_safe)\n        total_alpha.stack(other.alpha * other.counts_off, weights=other.mask_safe)\n\n        with np.errstate(divide=""ignore"", invalid=""ignore""):\n            acceptance_off = total_off / total_alpha\n            average_alpha = total_alpha.data.sum() / total_off.data.sum()\n\n        # For the bins where the stacked OFF counts equal 0, the alpha value is performed by weighting on the total\n        # OFF counts of each run\n        is_zero = total_off.data == 0\n        acceptance_off.data[is_zero] = 1 / average_alpha\n\n        self.acceptance = RegionNDMap.from_geom(geom)\n        self.acceptance.data += 1\n        self.acceptance_off = acceptance_off\n\n        if self.counts_off is not None:\n            self.counts_off *= self.mask_safe\n            self.counts_off.stack(other.counts_off, weights=other.mask_safe)\n\n        super().stack(other)\n\n    def to_ogip_files(self, outdir=None, use_sherpa=False, overwrite=False):\n        """"""Write OGIP files.\n\n        If you want to use the written files with Sherpa you have to set the\n        ``use_sherpa`` flag. Then all files will be written in units \'keV\' and\n        \'cm2\'.\n\n        The naming scheme is fixed, with {name} the dataset name:\n\n        * PHA file is named pha_obs{name}.fits\n        * BKG file is named bkg_obs{name}.fits\n        * ARF file is named arf_obs{name}.fits\n        * RMF file is named rmf_obs{name}.fits\n\n        Parameters\n        ----------\n        outdir : `pathlib.Path`\n            output directory, default: pwd\n        use_sherpa : bool, optional\n            Write Sherpa compliant files, default: False\n        overwrite : bool\n            Overwrite existing files?\n        """"""\n        # TODO: refactor and reduce amount of code duplication\n        outdir = Path.cwd() if outdir is None else make_path(outdir)\n        outdir.mkdir(exist_ok=True, parents=True)\n\n        phafile = f""pha_obs{self.name}.fits""\n\n        bkgfile = phafile.replace(""pha"", ""bkg"")\n        arffile = phafile.replace(""pha"", ""arf"")\n        rmffile = phafile.replace(""pha"", ""rmf"")\n\n        counts_table = self.counts.to_table()\n        counts_table[""QUALITY""] = np.logical_not(self.mask_safe.data[:, 0, 0])\n        counts_table[""BACKSCAL""] = self.acceptance.data[:, 0, 0]\n        counts_table[""AREASCAL""] = np.ones(self.acceptance.data.size)\n        meta = self._ogip_meta()\n\n        meta[""respfile""] = rmffile\n        meta[""backfile""] = bkgfile\n        meta[""ancrfile""] = arffile\n        meta[""hduclas2""] = ""TOTAL""\n        counts_table.meta = meta\n\n        name = counts_table.meta[""name""]\n        hdu = fits.BinTableHDU(counts_table, name=name)\n        hdulist = fits.HDUList([fits.PrimaryHDU(), hdu, self._ebounds_hdu(use_sherpa)])\n\n        if self.gti is not None:\n            hdu = fits.BinTableHDU(self.gti.table, name=""GTI"")\n            hdulist.append(hdu)\n\n        if self.counts.geom._region is not None and self.counts.geom.wcs is not None:\n            region_table = self.counts.geom._to_region_table()\n            region_hdu = fits.BinTableHDU(region_table, name=""REGION"")\n            hdulist.append(region_hdu)\n\n        hdulist.writeto(outdir / phafile, overwrite=overwrite)\n\n        self.aeff.write(outdir / arffile, overwrite=overwrite, use_sherpa=use_sherpa)\n\n        if self.counts_off is not None:\n            counts_off_table = self.counts_off.to_table()\n            counts_off_table[""QUALITY""] = np.logical_not(self.mask_safe.data[:, 0, 0])\n            counts_off_table[""BACKSCAL""] = self.acceptance_off.data[:, 0, 0]\n            counts_off_table[""AREASCAL""] = np.ones(self.acceptance.data.size)\n            meta = self._ogip_meta()\n            meta[""hduclas2""] = ""BKG""\n\n            counts_off_table.meta = meta\n            name = counts_off_table.meta[""name""]\n            hdu = fits.BinTableHDU(counts_off_table, name=name)\n            hdulist = fits.HDUList(\n                [fits.PrimaryHDU(), hdu, self._ebounds_hdu(use_sherpa)]\n            )\n            if (\n                self.counts_off.geom._region is not None\n                and self.counts_off.geom.wcs is not None\n            ):\n                region_table = self.counts_off.geom._to_region_table()\n                region_hdu = fits.BinTableHDU(region_table, name=""REGION"")\n                hdulist.append(region_hdu)\n\n            hdulist.writeto(outdir / bkgfile, overwrite=overwrite)\n\n        if self.edisp is not None:\n            self.edisp.write(\n                outdir / rmffile, overwrite=overwrite, use_sherpa=use_sherpa\n            )\n\n    def _ebounds_hdu(self, use_sherpa):\n        energy = self.counts.geom.axes[0].edges\n\n        if use_sherpa:\n            energy = energy.to(""keV"")\n\n        return energy_axis_to_ebounds(energy)\n\n    def _ogip_meta(self):\n        """"""Meta info for the OGIP data format""""""\n        return {\n            ""name"": ""SPECTRUM"",\n            ""hduclass"": ""OGIP"",\n            ""hduclas1"": ""SPECTRUM"",\n            ""corrscal"": """",\n            ""chantype"": ""PHA"",\n            ""detchans"": self.counts.geom.axes[0].nbin,\n            ""filter"": ""None"",\n            ""corrfile"": """",\n            ""poisserr"": True,\n            ""hduclas3"": ""COUNT"",\n            ""hduclas4"": ""TYPE:1"",\n            ""lo_thres"": self.energy_range[0].to_value(""TeV""),\n            ""hi_thres"": self.energy_range[1].to_value(""TeV""),\n            ""exposure"": self.livetime.to_value(""s""),\n            ""obs_id"": self.name,\n        }\n\n    @classmethod\n    def from_ogip_files(cls, filename):\n        """"""Read `~gammapy.spectrum.SpectrumDatasetOnOff` from OGIP files.\n\n        BKG file, ARF, and RMF must be set in the PHA header and be present in\n        the same folder.\n\n        The naming scheme is fixed to the following scheme:\n\n        * PHA file is named ``pha_obs{name}.fits``\n        * BKG file is named ``bkg_obs{name}.fits``\n        * ARF file is named ``arf_obs{name}.fits``\n        * RMF file is named ``rmf_obs{name}.fits``\n          with ``{name}`` the dataset name.\n\n        Parameters\n        ----------\n        filename : str\n            OGIP PHA file to read\n        """"""\n        filename = make_path(filename)\n        dirname = filename.parent\n\n        with fits.open(filename, memmap=False) as hdulist:\n            counts = RegionNDMap.from_hdulist(hdulist, format=""ogip"")\n            acceptance = RegionNDMap.from_hdulist(\n                hdulist, format=""ogip"", ogip_column=""BACKSCAL""\n            )\n            if ""GTI"" in hdulist:\n                gti = GTI(Table.read(hdulist[""GTI""]))\n            else:\n                gti = None\n\n            mask_safe = RegionNDMap.from_hdulist(\n                hdulist, format=""ogip"", ogip_column=""QUALITY""\n            )\n            mask_safe.data = np.logical_not(mask_safe.data)\n\n        phafile = filename.name\n\n        try:\n            rmffile = phafile.replace(""pha"", ""rmf"")\n            energy_dispersion = EDispKernel.read(dirname / rmffile)\n        except OSError:\n            # TODO : Add logger and echo warning\n            energy_dispersion = None\n\n        try:\n            bkgfile = phafile.replace(""pha"", ""bkg"")\n            with fits.open(dirname / bkgfile, memmap=False) as hdulist:\n                counts_off = RegionNDMap.from_hdulist(hdulist, format=""ogip"")\n                acceptance_off = RegionNDMap.from_hdulist(\n                    hdulist, ogip_column=""BACKSCAL""\n                )\n        except OSError:\n            # TODO : Add logger and echo warning\n            counts_off, acceptance_off = None, None\n\n        arffile = phafile.replace(""pha"", ""arf"")\n        aeff = EffectiveAreaTable.read(dirname / arffile)\n\n        return cls(\n            counts=counts,\n            aeff=aeff,\n            counts_off=counts_off,\n            edisp=energy_dispersion,\n            livetime=counts.meta[""EXPOSURE""] * u.s,\n            mask_safe=mask_safe,\n            acceptance=acceptance,\n            acceptance_off=acceptance_off,\n            name=str(counts.meta[""OBS_ID""]),\n            gti=gti,\n        )\n\n    def info_dict(self, in_safe_energy_range=True, **kwargs):\n        """"""Info dict with summary statistics, summed over energy\n\n        Parameters\n        ----------\n        in_safe_energy_range : bool\n            Whether to sum only in the safe energy range\n\n        Returns\n        -------\n        info_dict : dict\n            Dictionary with summary info.\n        """"""\n        info = super().info_dict(in_safe_energy_range)\n        mask = self.mask_safe.data if in_safe_energy_range else slice(None)\n\n        # TODO: handle energy dependent a_on / a_off\n        info[""a_on""] = self.acceptance.data[0, 0, 0].copy()\n\n        if self.counts_off is not None:\n            info[""n_off""] = self.counts_off.data[mask].sum()\n            info[""a_off""] = self.acceptance_off.data[0, 0, 0].copy()\n        else:\n            info[""n_off""] = 0\n            info[""a_off""] = 1\n\n        info[""alpha""] = self.alpha.data[0, 0, 0].copy()\n        info[""significance""] = WStatCountsStatistic(\n            self.counts.data[mask].sum(),\n            self.counts_off.data[mask].sum(),\n            self.alpha.data[0, 0, 0],\n        ).significance\n\n        return info\n\n    def to_dict(self, filename, *args, **kwargs):\n        """"""Convert to dict for YAML serialization.""""""\n        outdir = Path(filename).parent\n        filename = str(outdir / f""pha_obs{self.name}.fits"")\n\n        return {\n            ""name"": self.name,\n            ""type"": self.tag,\n            ""filename"": filename,\n        }\n\n    def write(self, filename, overwrite):\n        """"""Write spectrum dataset on off to file.\n\n        Currently only the OGIP format is supported\n\n        Parameters\n        ----------\n        filename : str\n            Filename to write to.\n        overwrite : bool\n            Overwrite existing file.\n        """"""\n        outdir = Path(filename).parent\n        self.to_ogip_files(outdir=outdir, overwrite=overwrite)\n\n    @classmethod\n    def from_dict(cls, data, models):\n        """"""Create flux point dataset from dict.\n\n        Parameters\n        ----------\n        data : dict\n            Dict containing data to create dataset from.\n        models : list of `SkyModel`\n            List of model components.\n\n        Returns\n        -------\n        dataset : `SpectrumDatasetOnOff`\n            Spectrum dataset on off.\n\n        """"""\n\n        filename = make_path(data[""filename""])\n        dataset = cls.from_ogip_files(filename=filename)\n        dataset.mask_fit = None\n        dataset.models = models\n        return dataset\n\n    @classmethod\n    def from_spectrum_dataset(\n        cls, dataset, acceptance, acceptance_off, counts_off=None\n    ):\n        """"""Create spectrum dataseton off from another dataset.\n\n        Parameters\n        ----------\n        dataset : `SpectrumDataset`\n            Spectrum dataset defining counts, edisp, aeff, livetime etc.\n        acceptance : `~numpy.array` or float\n            Relative background efficiency in the on region.\n        acceptance_off : `~numpy.array` or float\n            Relative background efficiency in the off region.\n        counts_off : `~gammapy.maps.RegionNDMap`\n            Off counts spectrum . If the dataset provides a background model,\n            and no off counts are defined. The off counts are deferred from\n            counts_off / alpha.\n\n        Returns\n        -------\n        dataset : `SpectrumDatasetOnOff`\n            Spectrum dataset on off.\n\n        """"""\n        if counts_off is None and dataset.background is not None:\n            alpha = acceptance / acceptance_off\n            counts_off = dataset.background / alpha\n\n        return cls(\n            models=dataset.models,\n            counts=dataset.counts,\n            aeff=dataset.aeff,\n            counts_off=counts_off,\n            edisp=dataset.edisp,\n            livetime=dataset.livetime,\n            mask_safe=dataset.mask_safe,\n            mask_fit=dataset.mask_fit,\n            acceptance=acceptance,\n            acceptance_off=acceptance_off,\n            gti=dataset.gti,\n            name=dataset.name,\n        )\n'"
gammapy/estimators/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Estimators.""""""\nfrom .asmooth_map import *\nfrom .excess_map import *\nfrom .flux_point import *\nfrom .lightcurve import *\nfrom .profile import *\nfrom .sensitivity import *\nfrom .ts_map import *\n'"
gammapy/estimators/asmooth_map.py,12,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Implementation of adaptive smoothing algorithms.""""""\nimport numpy as np\nfrom astropy.convolution import Gaussian2DKernel, Tophat2DKernel\nfrom astropy.coordinates import Angle\nfrom gammapy.datasets import MapDatasetOnOff\nfrom gammapy.maps import WcsNDMap\nfrom gammapy.stats import CashCountsStatistic\nfrom gammapy.utils.array import scale_cube\n\n__all__ = [""ASmoothMapEstimator""]\n\n\ndef _significance_asmooth(counts, background):\n    """"""Significance according to formula (5) in asmooth paper.""""""\n    return (counts - background) / np.sqrt(counts + background)\n\n\nclass ASmoothMapEstimator:\n    """"""Adaptively smooth counts image.\n\n    Achieves a roughly constant significance of features across the whole image.\n\n    Algorithm based on https://ui.adsabs.harvard.edu/abs/2006MNRAS.368...65E\n\n    The algorithm was slightly adapted to also allow Li & Ma  to estimate the\n    significance of a feature in the image.\n\n    Parameters\n    ----------\n    scales : `~astropy.units.Quantity`\n        Smoothing scales.\n    kernel : `astropy.convolution.Kernel`\n        Smoothing kernel.\n    method : {\'asmooth\', \'lima\'}\n        Significance estimation method.\n    threshold : float\n        Significance threshold.\n    """"""\n\n    def __init__(self, scales, kernel=Gaussian2DKernel, method=""lima"", threshold=5):\n        self.parameters = {\n            ""kernel"": kernel,\n            ""method"": method,\n            ""threshold"": threshold,\n            ""scales"": scales,\n        }\n\n    def kernels(self, pixel_scale):\n        """"""\n        Ring kernels according to the specified method.\n\n        Parameters\n        ----------\n        pixel_scale : `~astropy.coordinates.Angle`\n            Sky image pixel scale\n\n        Returns\n        -------\n        kernels : list\n            List of `~astropy.convolution.Kernel`\n        """"""\n        p = self.parameters\n        scales = p[""scales""].to_value(""deg"") / Angle(pixel_scale).deg\n\n        kernels = []\n        for scale in scales:  # .value:\n            kernel = p[""kernel""](scale, mode=""oversample"")\n            # TODO: check if normalizing here makes sense\n            kernel.normalize(""peak"")\n            kernels.append(kernel)\n\n        return kernels\n\n    @staticmethod\n    def _significance_cube(cubes, method):\n        if method in {""lima""}:\n            scube = CashCountsStatistic(\n                cubes[""counts""], cubes[""background""]\n            ).significance\n        elif method == ""asmooth"":\n            scube = _significance_asmooth(cubes[""counts""], cubes[""background""])\n        elif method == ""ts"":\n            raise NotImplementedError()\n        else:\n            raise ValueError(\n                ""Not a valid significance estimation method.""\n                "" Choose one of the following: \'lima\' or \'asmooth\'""\n            )\n        return scube\n\n    def run(self, dataset):\n        """"""\n        Run adaptive smoothing on input MapDataset.\n        The latter should have\n\n        Parameters\n        ----------\n        dataset : `~gammapy.cube.MapDataset` or `~gammapy.cube.MapDatasetOnOff`\n            the input dataset (with one bin in energy at most)\n        Returns\n        -------\n        images : dict of `~gammapy.maps.WcsNDMap`\n            Smoothed images; keys are:\n                * \'counts\'\n                * \'background\'\n                * \'flux\' (optional)\n                * \'scales\'\n                * \'significance\'.\n        """"""\n        # Check dimensionality\n        if len(dataset.data_shape) == 3:\n            if dataset.data_shape[0] != 1:\n                raise ValueError(\n                    ""ASmoothMapEstimator.run() requires a dataset with 1 energy bin at most.""\n                )\n\n        counts = dataset.counts.sum_over_axes(keepdims=False)\n\n        background = dataset.npred()\n        if isinstance(dataset, MapDatasetOnOff):\n            background += dataset.background\n        background = background.sum_over_axes(keepdims=False)\n\n        if dataset.exposure is not None:\n            exposure = dataset.exposure.sum_over_axes(keepdims=False)\n        else:\n            exposure = None\n\n        return self.estimate_maps(counts, background, exposure)\n\n    def estimate_maps(self, counts, background, exposure=None):\n        """"""\n        Run adaptive smoothing on input Maps.\n\n        Parameters\n        ----------\n        counts : `~gammapy.maps.Map`\n            counts map\n        background : `~gammapy.maps.Map`\n            estimated background counts map\n        exposure : `~gammapy.maps.Map`\n            exposure map. If set, it will produce a flux smoothed map.\n\n        Returns\n        -------\n        images : dict of `~gammapy.maps.WcsNDMap`\n            Smoothed images; keys are:\n                * \'counts\'\n                * \'background\'\n                * \'flux\' (optional)\n                * \'scales\'\n                * \'significance\'.\n        """"""\n\n        pixel_scale = counts.geom.pixel_scales.mean()\n        kernels = self.kernels(pixel_scale)\n\n        cubes = {}\n        cubes[""counts""] = scale_cube(counts.data, kernels)\n\n        if background is not None:\n            cubes[""background""] = scale_cube(background.data, kernels)\n        else:\n            # TODO: Estimate background with asmooth method\n            raise ValueError(""Background estimation required."")\n\n        if exposure is not None:\n            flux = (counts - background) / exposure\n            cubes[""flux""] = scale_cube(flux.data, kernels)\n\n        cubes[""significance""] = self._significance_cube(\n            cubes, method=self.parameters[""method""]\n        )\n\n        smoothed = self._reduce_cubes(cubes, kernels)\n\n        result = {}\n\n        for key in [""counts"", ""background"", ""scale"", ""significance""]:\n            data = smoothed[key]\n\n            # set remaining pixels with significance < threshold to mean value\n            if key in [""counts"", ""background""]:\n                mask = np.isnan(data)\n                data[mask] = np.mean(locals()[key].data[mask])\n                result[key] = WcsNDMap(counts.geom, data, unit=counts.unit)\n            else:\n                result[key] = WcsNDMap(counts.geom, data, unit=""deg"")\n\n        if exposure is not None:\n            data = smoothed[""flux""]\n            mask = np.isnan(data)\n            data[mask] = np.mean(flux.data[mask])\n            result[""flux""] = WcsNDMap(counts.geom, data, unit=flux.unit)\n\n        return result\n\n    def _reduce_cubes(self, cubes, kernels):\n        """"""\n        Combine scale cube to image.\n\n        Parameters\n        ----------\n        cubes : dict\n            Data cubes\n        """"""\n        p = self.parameters\n        shape = cubes[""counts""].shape[:2]\n        smoothed = {}\n\n        # Init smoothed data arrays\n        for key in [""counts"", ""background"", ""scale"", ""significance"", ""flux""]:\n            smoothed[key] = np.tile(np.nan, shape)\n\n        for idx, scale in enumerate(p[""scales""]):\n            # slice out 2D image at index idx out of cube\n            slice_ = np.s_[:, :, idx]\n\n            mask = np.isnan(smoothed[""counts""])\n            mask = (cubes[""significance""][slice_] > p[""threshold""]) & mask\n\n            smoothed[""scale""][mask] = scale\n            smoothed[""significance""][mask] = cubes[""significance""][slice_][mask]\n\n            # renormalize smoothed data arrays\n            norm = kernels[idx].array.sum()\n            for key in [""counts"", ""background""]:\n                smoothed[key][mask] = cubes[key][slice_][mask] / norm\n            if ""flux"" in cubes:\n                smoothed[""flux""][mask] = cubes[""flux""][slice_][mask] / norm\n\n        return smoothed\n\n    @staticmethod\n    def get_scales(n_scales, factor=np.sqrt(2), kernel=Gaussian2DKernel):\n        """"""Create list of Gaussian widths.""""""\n        if kernel == Gaussian2DKernel:\n            sigma_0 = 1.0 / np.sqrt(9 * np.pi)\n        elif kernel == Tophat2DKernel:\n            sigma_0 = 1.0 / np.sqrt(np.pi)\n\n        return sigma_0 * factor ** np.arange(n_scales)\n'"
gammapy/estimators/excess_map.py,4,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport copy\nimport logging\nimport numpy as np\nfrom astropy.convolution import Tophat2DKernel\nfrom astropy.coordinates import Angle\nfrom gammapy.datasets import MapDataset, MapDatasetOnOff\nfrom gammapy.maps import Map\nfrom gammapy.stats import CashCountsStatistic, WStatCountsStatistic\n\n__all__ = [\n    ""ExcessMapEstimator"",\n]\n\nlog = logging.getLogger(__name__)\n\n\ndef convolved_map_dataset_counts_statistics(dataset, kernel):\n    """"""Return CountsDataset objects containing smoothed maps from the MapDataset""""""\n    # Kernel is modified later make a copy here\n    kernel = copy.deepcopy(kernel)\n    kernel.normalize(""peak"")\n\n    # fft convolution adds numerical noise, to ensure integer results we call\n    # np.rint\n    n_on_conv = np.rint(dataset.counts.convolve(kernel.array).data)\n\n    if isinstance(dataset, MapDatasetOnOff):\n        background = dataset.background\n        background.data[dataset.acceptance_off.data == 0] = 0.0\n        background_conv = background.convolve(kernel.array).data\n\n        n_off_conv = dataset.counts_off.convolve(kernel.array).data\n\n        with np.errstate(invalid=""ignore"", divide=""ignore""):\n            alpha_conv = background_conv / n_off_conv\n\n        return WStatCountsStatistic(n_on_conv.data, n_off_conv.data, alpha_conv.data)\n    else:\n        background_conv = dataset.npred().convolve(kernel.array).data\n        return CashCountsStatistic(n_on_conv.data, background_conv.data)\n\n\nclass ExcessMapEstimator:\n    """"""Computes correlated excess, significance and errors for MapDatasets.\n\n    Parameters\n    ----------\n    correlation_radius : ~astropy.coordinate.Angle\n        correlation radius to use\n    n_sigma : float\n        Confidence level for the asymmetric errors expressed in number of sigma.\n        Default is 1.\n    n_sigma_ul : float\n        Confidence level for the upper limits expressed in number of sigma.\n        Default is 3.\n    """"""\n\n    def __init__(self, correlation_radius=""0.1 deg"", nsigma=1, nsigma_ul=3):\n        self.correlation_radius = correlation_radius\n        self.nsigma = nsigma\n        self.nsigma_ul = nsigma_ul\n\n    @property\n    def correlation_radius(self):\n        return self._correlation_radius\n\n    @correlation_radius.setter\n    def correlation_radius(self, correlation_radius):\n        """"""Sets radius""""""\n        self._correlation_radius = Angle(correlation_radius)\n\n    def run(self, dataset, steps=""all""):\n        """"""Compute correlated excess, Li & Ma significance and flux maps\n\n        Parameters\n        ----------\n        dataset : `~gammapy.datasets.MapDataset` or `~gammapy.datasets.MapDatasetOnOff`\n            input image-like dataset\n        steps : list of str\n            Which steps to execute. Available options are:\n\n                * ""ts"": estimate delta TS and significance\n                * ""err"": estimate symmetric error\n                * ""errn-errp"": estimate asymmetric errors.\n                * ""ul"": estimate upper limits.\n\n            By default all steps are executed.\n\n        Returns\n        -------\n        images : dict\n            Dictionary containing result correlated maps. Keys are:\n\n                * counts : correlated counts map\n                * background : correlated background map\n                * excess : correlated excess map\n                * ts : delta TS map\n                * significance : sqrt(delta TS), or Li-Ma significance map\n                * err : symmetric error map (from covariance)\n                * errn : negative error map\n                * errp : positive error map\n                * ul : upper limit map\n\n        """"""\n        if not isinstance(dataset, MapDataset):\n            raise ValueError(""Unsupported dataset type"")\n\n        pixel_size = np.mean(np.abs(dataset.counts.geom.wcs.wcs.cdelt))\n        size = self.correlation_radius.deg / pixel_size\n        kernel = Tophat2DKernel(size)\n\n        geom = dataset.counts.geom\n\n        self.counts_stat = convolved_map_dataset_counts_statistics(dataset, kernel)\n\n        n_on = Map.from_geom(geom, data=self.counts_stat.n_on)\n        bkg = Map.from_geom(geom, data=self.counts_stat.n_on - self.counts_stat.excess)\n        excess = Map.from_geom(geom, data=self.counts_stat.excess)\n\n        result = {""counts"": n_on, ""background"": bkg, ""excess"": excess}\n\n        if steps == ""all"":\n            steps = [""ts"", ""err"", ""errn-errp"", ""ul""]\n\n        if ""ts"" in steps:\n            tsmap = Map.from_geom(geom, data=self.counts_stat.delta_ts)\n            significance = Map.from_geom(geom, data=self.counts_stat.significance)\n            result.update({""ts"": tsmap, ""significance"": significance})\n\n        if ""err"" in steps:\n            err = Map.from_geom(geom, data=self.counts_stat.error)\n            result.update({""err"": err})\n\n        if ""errn-errp"" in steps:\n            errn = Map.from_geom(geom, data=self.counts_stat.compute_errn(self.nsigma))\n            errp = Map.from_geom(geom, data=self.counts_stat.compute_errp(self.nsigma))\n            result.update({""errn"": errn, ""errp"": errp})\n\n        if ""ul"" in steps:\n            ul = Map.from_geom(\n                geom, data=self.counts_stat.compute_upper_limit(self.nsigma_ul)\n            )\n            result.update({""ul"": ul})\n        return result\n'"
gammapy/estimators/flux.py,9,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport numpy as np\nfrom astropy import units as u\nfrom gammapy.estimators.parameter_estimator import ParameterEstimator\nfrom gammapy.modeling.models import ScaleSpectralModel\n\nlog = logging.getLogger(__name__)\n\n\nclass FluxEstimator(ParameterEstimator):\n    """"""Flux estimator.\n\n    Estimates flux for a given list of datasets with their model in a given energy range.\n\n    To estimate the model flux the amplitude of the reference spectral model is\n    fitted within the energy range. The amplitude is re-normalized using the ""norm"" parameter,\n    which specifies the deviation of the flux from the reference model in this\n    energy range.\n\n    Parameters\n    ----------\n    source : str or int\n        For which source in the model to compute the flux.\n    energy_range : `~astropy.units.Quantity`\n        the energy interval on which to compute the flux\n    norm_min : float\n        Minimum value for the norm used for the fit statistic profile evaluation.\n    norm_max : float\n        Maximum value for the norm used for the fit statistic profile evaluation.\n    norm_n_values : int\n        Number of norm values used for the fit statistic profile.\n    norm_values : `numpy.ndarray`\n        Array of norm values to be used for the fit statistic profile.\n    sigma : int\n        Sigma to use for asymmetric error computation.\n    sigma_ul : int\n        Sigma to use for upper limit computation.\n    reoptimize : bool\n        Re-optimize other free model parameters.\n    """"""\n\n    def __init__(\n        self,\n        source,\n        energy_range,\n        norm_min=0.2,\n        norm_max=5,\n        norm_n_values=11,\n        norm_values=None,\n        sigma=1,\n        sigma_ul=3,\n        reoptimize=True,\n    ):\n\n        if norm_values is None:\n            norm_values = np.logspace(\n                np.log10(norm_min), np.log10(norm_max), norm_n_values\n            )\n        self.norm_values = norm_values\n\n        self.source = source\n\n        self.energy_range = energy_range\n\n        super().__init__(\n            sigma, sigma_ul, reoptimize,\n        )\n\n    @property\n    def energy_range(self):\n        return self._energy_range\n\n    @energy_range.setter\n    def energy_range(self, energy_range):\n        if len(energy_range) != 2:\n            raise ValueError(""Incorrect size of energy_range"")\n\n        emin = u.Quantity(energy_range[0])\n        emax = u.Quantity(energy_range[1])\n\n        if emin >= emax:\n            raise ValueError(""Incorrect energy_range for Flux Estimator"")\n        self._energy_range = [emin, emax]\n\n    @property\n    def e_ref(self):\n        return np.sqrt(self.energy_range[0] * self.energy_range[1])\n\n    def __str__(self):\n        s = f""{self.__class__.__name__}:\\n""\n        s += str(self.datasets) + ""\\n""\n        s += str(self.model) + ""\\n""\n        return s\n\n    def _set_model(self, datasets, model):\n        # set the model on all datasets\n        for dataset in datasets:\n            dataset.models[self.source].spectral_model = model\n\n    def _prepare_result(self, model):\n        """"""Prepare the result dictionnary""""""\n        return {\n            ""e_ref"": self.e_ref,\n            ""e_min"": self.energy_range[0],\n            ""e_max"": self.energy_range[1],\n            ""ref_dnde"": model(self.e_ref),\n            ""ref_flux"": model.integral(self.energy_range[0], self.energy_range[1]),\n            ""ref_eflux"": model.energy_flux(self.energy_range[0], self.energy_range[1]),\n            ""ref_e2dnde"": model(self.e_ref) * self.e_ref ** 2,\n        }\n\n    def _prepare_steps(self, steps):\n        """"""Adapt the steps to the ParameterEstimator format.""""""\n        if ""norm-scan"" in steps:\n            steps.remove(""norm-scan"")\n            steps.append(""scan"")\n        if ""norm-err"" in steps:\n            steps.remove(""norm-err"")\n            steps.append(""err"")\n        if steps == ""all"":\n            steps = [""err"", ""ts"", ""errp-errn"", ""ul"", ""scan""]\n        return steps\n\n    def run(self, datasets, steps=""all""):\n        """"""Estimate flux for a given energy range.\n\n        The fit is performed in the energy range provided by the dataset masks.\n        The input energy range is used only to compute the flux normalization.\n\n        Parameters\n        ----------\n        datasets : list of `~gammapy.spectrum.SpectrumDataset`\n            Spectrum datasets.\n        steps : list of str\n            Which steps to execute. Available options are:\n\n                * ""norm-err"": estimate symmetric error.\n                * ""errn-errp"": estimate asymmetric errors.\n                * ""ul"": estimate upper limits.\n                * ""ts"": estimate ts and sqrt(ts) values.\n                * ""norm-scan"": estimate fit statistic profiles.\n\n            By default all steps are executed.\n\n        Returns\n        -------\n        result : dict\n            Dict with results for the flux point.\n        """"""\n        datasets = self._check_datasets(datasets)\n\n        if not datasets.is_all_same_type or not datasets.is_all_same_energy_shape:\n            raise ValueError(\n                ""Flux point estimation requires a list of datasets""\n                "" of the same type and data shape.""\n            )\n        dataset = datasets[0]\n\n        ref_model = dataset.models[self.source].spectral_model\n\n        scale_model = ScaleSpectralModel(ref_model)\n        scale_model.norm.min = 0\n        scale_model.norm.max = 1e5\n\n        self._set_model(datasets, scale_model)\n\n        steps = self._prepare_steps(steps)\n        result = self._prepare_result(scale_model.model)\n\n        scale_model.norm.value = 1.0\n        scale_model.norm.frozen = False\n\n        result.update(\n            super().run(\n                datasets,\n                scale_model.norm,\n                steps,\n                null_value=0,\n                scan_values=self.norm_values,\n            )\n        )\n        self._set_model(datasets, ref_model)\n        return result\n\n    def _return_nan_result(self, model, steps=""all""):\n        steps = self._prepare_steps(steps)\n        result = self._prepare_result(model)\n        result.update({""norm"": np.nan, ""stat"": np.nan, ""success"": False})\n        if ""err"" in steps:\n            result.update({""norm_err"": np.nan})\n        if ""ts"" in steps:\n            result.update({""sqrt_ts"": np.nan, ""ts"": np.nan, ""null_value"": np.nan})\n        if ""errp-errn"" in steps:\n            result.update({""norm_errp"": np.nan, ""norm_errn"": np.nan})\n        if ""ul"" in steps:\n            result.update({""norm_ul"": np.nan})\n        if ""scan"" in steps:\n            nans = np.nan * np.empty_like(self.norm_values)\n            result.update({""norm_scan"": nans, ""stat_scan"": nans})\n        return result\n'"
gammapy/estimators/flux_point.py,17,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport numpy as np\nfrom astropy import units as u\nfrom astropy.io.registry import IORegistryError\nfrom astropy.table import Table, vstack\nfrom gammapy.datasets import MapDataset\nfrom gammapy.modeling.models import PowerLawSpectralModel\nfrom gammapy.utils.interpolation import interpolate_profile\nfrom gammapy.utils.scripts import make_path\nfrom gammapy.utils.table import table_from_row_data, table_standardise_units_copy\nfrom .flux import FluxEstimator\n\n\n__all__ = [""FluxPoints"", ""FluxPointsEstimator""]\n\nlog = logging.getLogger(__name__)\n\nREQUIRED_COLUMNS = {\n    ""dnde"": [""e_ref"", ""dnde""],\n    ""e2dnde"": [""e_ref"", ""e2dnde""],\n    ""flux"": [""e_min"", ""e_max"", ""flux""],\n    ""eflux"": [""e_min"", ""e_max"", ""eflux""],\n    # TODO: extend required columns\n    ""likelihood"": [\n        ""e_min"",\n        ""e_max"",\n        ""e_ref"",\n        ""ref_dnde"",\n        ""norm"",\n        ""norm_scan"",\n        ""stat_scan"",\n    ],\n}\n\nOPTIONAL_COLUMNS = {\n    ""dnde"": [""dnde_err"", ""dnde_errp"", ""dnde_errn"", ""dnde_ul"", ""is_ul""],\n    ""e2dnde"": [""e2dnde_err"", ""e2dnde_errp"", ""e2dnde_errn"", ""e2dnde_ul"", ""is_ul""],\n    ""flux"": [""flux_err"", ""flux_errp"", ""flux_errn"", ""flux_ul"", ""is_ul""],\n    ""eflux"": [""eflux_err"", ""eflux_errp"", ""eflux_errn"", ""eflux_ul"", ""is_ul""],\n}\n\nDEFAULT_UNIT = {\n    ""dnde"": u.Unit(""cm-2 s-1 TeV-1""),\n    ""e2dnde"": u.Unit(""erg cm-2 s-1""),\n    ""flux"": u.Unit(""cm-2 s-1""),\n    ""eflux"": u.Unit(""erg cm-2 s-1""),\n}\n\n\nclass FluxPoints:\n    """"""Flux points container.\n\n    The supported formats are described here: :ref:`gadf:flux-points`\n\n    In summary, the following formats and minimum required columns are:\n\n    * Format ``dnde``: columns ``e_ref`` and ``dnde``\n    * Format ``e2dnde``: columns ``e_ref``, ``e2dnde``\n    * Format ``flux``: columns ``e_min``, ``e_max``, ``flux``\n    * Format ``eflux``: columns ``e_min``, ``e_max``, ``eflux``\n\n    Parameters\n    ----------\n    table : `~astropy.table.Table`\n        Table with flux point data\n\n    Attributes\n    ----------\n    table : `~astropy.table.Table`\n        Table with flux point data\n\n    Examples\n    --------\n    The `FluxPoints` object is most easily created by reading a file with\n    flux points given in one of the formats documented above::\n\n        from gammapy.spectrum import FluxPoints\n        filename = \'$GAMMAPY_DATA/hawc_crab/HAWC19_flux_points.fits\'\n        flux_points = FluxPoints.read(filename)\n        flux_points.plot()\n\n    An instance of `FluxPoints` can also be created by passing an instance of\n    `astropy.table.Table`, which contains the required columns, such as `\'e_ref\'`\n    and `\'dnde\'`. The corresponding `sed_type` has to be defined in the meta data\n    of the table::\n\n        from astropy import units as u\n        from astropy.table import Table\n        from gammapy.spectrum import FluxPoints\n        from gammapy.modeling.models import PowerLawSpectralModel\n\n        table = Table()\n        pwl = PowerLawSpectralModel()\n        e_ref = np.logspace(0, 2, 7) * u.TeV\n        table[\'e_ref\'] = e_ref\n        table[\'dnde\'] = pwl(e_ref)\n        table.meta[\'SED_TYPE\'] = \'dnde\'\n\n        flux_points = FluxPoints(table)\n        flux_points.plot()\n\n    If you have flux points in a different data format, the format can be changed\n    by renaming the table columns and adding meta data::\n\n\n        from astropy import units as u\n        from astropy.table import Table\n        from gammapy.spectrum import FluxPoints\n\n        table = Table.read(\'$GAMMAPY_DATA/tests/spectrum/flux_points/flux_points_ctb_37b.txt\',\n                           format=\'ascii.csv\', delimiter=\' \', comment=\'#\')\n        table.meta[\'SED_TYPE\'] = \'dnde\'\n        table.rename_column(\'Differential_Flux\', \'dnde\')\n        table[\'dnde\'].unit = \'cm-2 s-1 TeV-1\'\n\n        table.rename_column(\'lower_error\', \'dnde_errn\')\n        table[\'dnde_errn\'].unit = \'cm-2 s-1 TeV-1\'\n\n        table.rename_column(\'upper_error\', \'dnde_errp\')\n        table[\'dnde_errp\'].unit = \'cm-2 s-1 TeV-1\'\n\n        table.rename_column(\'E\', \'e_ref\')\n        table[\'e_ref\'].unit = \'TeV\'\n\n        flux_points = FluxPoints(table)\n        flux_points.plot()\n\n    Note: In order to reproduce the example you need the tests datasets folder.\n    You may download it with the command\n    ``gammapy download datasets --tests --out $GAMMAPY_DATA``\n    """"""\n\n    def __init__(self, table):\n        self.table = table_standardise_units_copy(table)\n        # validate that the table is a valid representation\n        # of the given flux point sed type\n        self._validate_table(self.table, table.meta[""SED_TYPE""])\n\n    def __repr__(self):\n        return f""{self.__class__.__name__}(sed_type={self.sed_type!r}, n_points={len(self.table)})""\n\n    @property\n    def table_formatted(self):\n        """"""Return formatted version of the flux points table. Used for pretty printing""""""\n        table = self.table.copy()\n\n        for column in table.colnames:\n            if column.startswith((""dnde"", ""eflux"", ""flux"", ""e2dnde"", ""ref"")):\n                table[column].format = "".3e""\n            elif column.startswith(\n                (""e_min"", ""e_max"", ""e_ref"", ""sqrt_ts"", ""norm"", ""ts"", ""stat"")\n            ):\n                table[column].format = "".3f""\n\n        return table\n\n    @classmethod\n    def read(cls, filename, **kwargs):\n        """"""Read flux points.\n\n        Parameters\n        ----------\n        filename : str\n            Filename\n        kwargs : dict\n            Keyword arguments passed to `astropy.table.Table.read`.\n        """"""\n        filename = make_path(filename)\n        try:\n            table = Table.read(filename, **kwargs)\n        except IORegistryError:\n            kwargs.setdefault(""format"", ""ascii.ecsv"")\n            table = Table.read(filename, **kwargs)\n\n        if ""SED_TYPE"" not in table.meta.keys():\n            sed_type = cls._guess_sed_type(table)\n            table.meta[""SED_TYPE""] = sed_type\n\n        # TODO: check sign and factor 2 here\n        # https://github.com/gammapy/gammapy/pull/2546#issuecomment-554274318\n        # The idea below is to support the format here:\n        # https://gamma-astro-data-formats.readthedocs.io/en/latest/spectra/flux_points/index.html#likelihood-columns\n        # but internally to go to the uniform ""stat""\n\n        if ""loglike"" in table.colnames and ""stat"" not in table.colnames:\n            table[""stat""] = 2 * table[""loglike""]\n\n        if ""loglike_null"" in table.colnames and ""stat_null"" not in table.colnames:\n            table[""stat_null""] = 2 * table[""loglike_null""]\n\n        if ""dloglike_scan"" in table.colnames and ""stat_scan"" not in table.colnames:\n            table[""stat_scan""] = 2 * table[""dloglike_scan""]\n\n        return cls(table=table)\n\n    def write(self, filename, **kwargs):\n        """"""Write flux points.\n\n        Parameters\n        ----------\n        filename : str\n            Filename\n        kwargs : dict\n            Keyword arguments passed to `astropy.table.Table.write`.\n        """"""\n        filename = make_path(filename)\n        try:\n            self.table.write(filename, **kwargs)\n        except IORegistryError:\n            kwargs.setdefault(""format"", ""ascii.ecsv"")\n            self.table.write(filename, **kwargs)\n\n    @classmethod\n    def stack(cls, flux_points):\n        """"""Create flux points by stacking list of flux points.\n\n        The first `FluxPoints` object in the list is taken as a reference to infer\n        column names and units for the stacked object.\n\n        Parameters\n        ----------\n        flux_points : list of `FluxPoints`\n            List of flux points to stack.\n\n        Returns\n        -------\n        flux_points : `FluxPoints`\n            Flux points without upper limit points.\n        """"""\n        reference = flux_points[0].table\n\n        tables = []\n        for _ in flux_points:\n            table = _.table\n            for colname in reference.colnames:\n                column = reference[colname]\n                if column.unit:\n                    table[colname] = table[colname].quantity.to(column.unit)\n            tables.append(table[reference.colnames])\n\n        table_stacked = vstack(tables)\n        table_stacked.meta[""SED_TYPE""] = reference.meta[""SED_TYPE""]\n\n        return cls(table_stacked)\n\n    def drop_ul(self):\n        """"""Drop upper limit flux points.\n\n        Returns\n        -------\n        flux_points : `FluxPoints`\n            Flux points with upper limit points removed.\n\n        Examples\n        --------\n        >>> from gammapy.spectrum import FluxPoints\n        >>> filename = \'$GAMMAPY_DATA/tests/spectrum/flux_points/flux_points.fits\'\n        >>> flux_points = FluxPoints.read(filename)\n        >>> print(flux_points)\n        FluxPoints(sed_type=""flux"", n_points=24)\n        >>> print(flux_points.drop_ul())\n        FluxPoints(sed_type=""flux"", n_points=19)\n\n        Note: In order to reproduce the example you need the tests datasets folder.\n        You may download it with the command\n        ``gammapy download datasets --tests --out $GAMMAPY_DATA``\n        """"""\n        table_drop_ul = self.table[~self.is_ul]\n        return self.__class__(table_drop_ul)\n\n    def _flux_to_dnde(self, e_ref, table, model, pwl_approx):\n        if model is None:\n            model = PowerLawSpectralModel()\n\n        e_min, e_max = self.e_min, self.e_max\n\n        flux = table[""flux""].quantity\n        dnde = self._dnde_from_flux(flux, model, e_ref, e_min, e_max, pwl_approx)\n\n        # Add to result table\n        table[""e_ref""] = e_ref\n        table[""dnde""] = dnde\n\n        if ""flux_err"" in table.colnames:\n            table[""dnde_err""] = dnde * table[""flux_err""].quantity / flux\n\n        if ""flux_errn"" in table.colnames:\n            table[""dnde_errn""] = dnde * table[""flux_errn""].quantity / flux\n            table[""dnde_errp""] = dnde * table[""flux_errp""].quantity / flux\n\n        if ""flux_ul"" in table.colnames:\n            flux_ul = table[""flux_ul""].quantity\n            dnde_ul = self._dnde_from_flux(\n                flux_ul, model, e_ref, e_min, e_max, pwl_approx\n            )\n            table[""dnde_ul""] = dnde_ul\n\n        return table\n\n    @staticmethod\n    def _dnde_to_e2dnde(e_ref, table):\n        for suffix in ["""", ""_ul"", ""_err"", ""_errp"", ""_errn""]:\n            try:\n                data = table[""dnde"" + suffix].quantity\n                table[""e2dnde"" + suffix] = (e_ref ** 2 * data).to(\n                    DEFAULT_UNIT[""e2dnde""]\n                )\n            except KeyError:\n                continue\n\n        return table\n\n    @staticmethod\n    def _e2dnde_to_dnde(e_ref, table):\n        for suffix in ["""", ""_ul"", ""_err"", ""_errp"", ""_errn""]:\n            try:\n                data = table[""e2dnde"" + suffix].quantity\n                table[""dnde"" + suffix] = (data / e_ref ** 2).to(DEFAULT_UNIT[""dnde""])\n            except KeyError:\n                continue\n\n        return table\n\n    def to_sed_type(self, sed_type, method=""log_center"", model=None, pwl_approx=False):\n        """"""Convert to a different SED type (return new `FluxPoints`).\n\n        See: https://ui.adsabs.harvard.edu/abs/1995NIMPA.355..541L for details\n        on the `\'lafferty\'` method.\n\n        Parameters\n        ----------\n        sed_type : {\'dnde\'}\n             SED type to convert to.\n        model : `~gammapy.modeling.models.SpectralModel`\n            Spectral model assumption.  Note that the value of the amplitude parameter\n            does not matter. Still it is recommended to use something with the right\n            scale and units. E.g. `amplitude = 1e-12 * u.Unit(\'cm-2 s-1 TeV-1\')`\n        method : {\'lafferty\', \'log_center\', \'table\'}\n            Flux points `e_ref` estimation method:\n\n                * `\'laferty\'` Lafferty & Wyatt model-based e_ref\n                * `\'log_center\'` log bin center e_ref\n                * `\'table\'` using column \'e_ref\' from input flux_points\n        pwl_approx : bool\n            Use local power law appoximation at e_ref to compute differential flux\n            from the integral flux. This method is used by the Fermi-LAT catalogs.\n\n        Returns\n        -------\n        flux_points : `FluxPoints`\n            Flux points including differential quantity columns `dnde`\n            and `dnde_err` (optional), `dnde_ul` (optional).\n\n        Examples\n        --------\n        >>> from gammapy.spectrum import FluxPoints\n        >>> from gammapy.modeling.models import PowerLawSpectralModel\n        >>> filename = \'$GAMMAPY_DATA/tests/spectrum/flux_points/flux_points.fits\'\n        >>> flux_points = FluxPoints.read(filename)\n        >>> model = PowerLawSpectralModel(index=2.2)\n        >>> flux_points_dnde = flux_points.to_sed_type(\'dnde\', model=model)\n\n        Note: In order to reproduce the example you need the tests datasets folder.\n        You may download it with the command\n        ``gammapy download datasets --tests --out $GAMMAPY_DATA``\n        """"""\n        # TODO: implement other directions.\n        table = self.table.copy()\n\n        if self.sed_type == ""flux"" and sed_type == ""dnde"":\n            # Compute e_ref\n            if method == ""table"":\n                e_ref = table[""e_ref""].quantity\n            elif method == ""log_center"":\n                e_ref = np.sqrt(self.e_min * self.e_max)\n            elif method == ""lafferty"":\n                # set e_ref that it represents the mean dnde in the given energy bin\n                e_ref = self._e_ref_lafferty(model, self.e_min, self.e_max)\n            else:\n                raise ValueError(f""Invalid method: {method}"")\n            table = self._flux_to_dnde(e_ref, table, model, pwl_approx)\n\n        elif self.sed_type == ""dnde"" and sed_type == ""e2dnde"":\n            table = self._dnde_to_e2dnde(self.e_ref, table)\n\n        elif self.sed_type == ""e2dnde"" and sed_type == ""dnde"":\n            table = self._e2dnde_to_dnde(self.e_ref, table)\n\n        elif self.sed_type == ""likelihood"" and sed_type in [""dnde"", ""flux"", ""eflux""]:\n            for suffix in ["""", ""_ul"", ""_err"", ""_errp"", ""_errn""]:\n                try:\n                    table[sed_type + suffix] = (\n                        table[""ref_"" + sed_type] * table[""norm"" + suffix]\n                    )\n                except KeyError:\n                    continue\n        else:\n            raise NotImplementedError\n\n        table.meta[""SED_TYPE""] = sed_type\n        return FluxPoints(table)\n\n    @staticmethod\n    def _e_ref_lafferty(model, e_min, e_max):\n        """"""Helper for `to_sed_type`.\n\n        Compute e_ref that the value at e_ref corresponds\n        to the mean value between e_min and e_max.\n        """"""\n        flux = model.integral(e_min, e_max)\n        dnde_mean = flux / (e_max - e_min)\n        return model.inverse(dnde_mean)\n\n    @staticmethod\n    def _dnde_from_flux(flux, model, e_ref, e_min, e_max, pwl_approx):\n        """"""Helper for `to_sed_type`.\n\n        Compute dnde under the assumption that flux equals expected\n        flux from model.\n        """"""\n        dnde_model = model(e_ref)\n\n        if pwl_approx:\n            index = model.spectral_index(e_ref)\n            flux_model = PowerLawSpectralModel.evaluate_integral(\n                emin=e_min,\n                emax=e_max,\n                index=index,\n                reference=e_ref,\n                amplitude=dnde_model,\n            )\n        else:\n            flux_model = model.integral(e_min, e_max, intervals=True)\n\n        return dnde_model * (flux / flux_model)\n\n    @property\n    def sed_type(self):\n        """"""SED type (str).\n\n        One of: {\'dnde\', \'e2dnde\', \'flux\', \'eflux\'}\n        """"""\n        return self.table.meta[""SED_TYPE""]\n\n    @staticmethod\n    def _guess_sed_type(table):\n        """"""Guess SED type from table content.""""""\n        valid_sed_types = list(REQUIRED_COLUMNS.keys())\n        for sed_type in valid_sed_types:\n            required = set(REQUIRED_COLUMNS[sed_type])\n            if required.issubset(table.colnames):\n                return sed_type\n\n    @staticmethod\n    def _guess_sed_type_from_unit(unit):\n        """"""Guess SED type from unit.""""""\n        for sed_type, default_unit in DEFAULT_UNIT.items():\n            if unit.is_equivalent(default_unit):\n                return sed_type\n\n    @staticmethod\n    def _validate_table(table, sed_type):\n        """"""Validate input table.""""""\n        required = set(REQUIRED_COLUMNS[sed_type])\n\n        if not required.issubset(table.colnames):\n            missing = required.difference(table.colnames)\n            raise ValueError(\n                ""Missing columns for sed type \'{}\':"" "" {}"".format(sed_type, missing)\n            )\n\n    @staticmethod\n    def _get_y_energy_unit(y_unit):\n        """"""Get energy part of the given y unit.""""""\n        try:\n            return [_ for _ in y_unit.bases if _.physical_type == ""energy""][0]\n        except IndexError:\n            return u.Unit(""TeV"")\n\n    def _plot_get_energy_err(self):\n        """"""Compute energy error for given sed type""""""\n        try:\n            e_min = self.table[""e_min""].quantity\n            e_max = self.table[""e_max""].quantity\n            e_ref = self.e_ref\n            x_err = ((e_ref - e_min), (e_max - e_ref))\n        except KeyError:\n            x_err = None\n        return x_err\n\n    def _plot_get_flux_err(self, sed_type=None):\n        """"""Compute flux error for given sed type""""""\n        try:\n            # asymmetric error\n            y_errn = self.table[sed_type + ""_errn""].quantity\n            y_errp = self.table[sed_type + ""_errp""].quantity\n            y_err = (y_errn, y_errp)\n        except KeyError:\n            try:\n                # symmetric error\n                y_err = self.table[sed_type + ""_err""].quantity\n                y_err = (y_err, y_err)\n            except KeyError:\n                # no error at all\n                y_err = None\n        return y_err\n\n    @property\n    def is_ul(self):\n        try:\n            return self.table[""is_ul""].data.astype(""bool"")\n        except KeyError:\n            return np.isnan(self.table[self.sed_type])\n\n    @property\n    def e_ref(self):\n        """"""Reference energy.\n\n        Defined by `e_ref` column in `FluxPoints.table` or computed as log\n        center, if `e_min` and `e_max` columns are present in `FluxPoints.table`.\n\n        Returns\n        -------\n        e_ref : `~astropy.units.Quantity`\n            Reference energy.\n        """"""\n        try:\n            return self.table[""e_ref""].quantity\n        except KeyError:\n            return np.sqrt(self.e_min * self.e_max)\n\n    @property\n    def e_edges(self):\n        """"""Edges of the energy bin.\n\n        Returns\n        -------\n        e_edges : `~astropy.units.Quantity`\n            Energy edges.\n        """"""\n        e_edges = list(self.e_min)\n        e_edges += [self.e_max[-1]]\n        return u.Quantity(e_edges, self.e_min.unit, copy=False)\n\n    @property\n    def e_min(self):\n        """"""Lower bound of energy bin.\n\n        Defined by `e_min` column in `FluxPoints.table`.\n\n        Returns\n        -------\n        e_min : `~astropy.units.Quantity`\n            Lower bound of energy bin.\n        """"""\n        return self.table[""e_min""].quantity\n\n    @property\n    def e_max(self):\n        """"""Upper bound of energy bin.\n\n        Defined by ``e_max`` column in ``table``.\n\n        Returns\n        -------\n        e_max : `~astropy.units.Quantity`\n            Upper bound of energy bin.\n        """"""\n        return self.table[""e_max""].quantity\n\n    def plot(\n        self, ax=None, energy_unit=""TeV"", flux_unit=None, energy_power=0, **kwargs\n    ):\n        """"""Plot flux points.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`\n            Axis object to plot on.\n        energy_unit : str, `~astropy.units.Unit`, optional\n            Unit of the energy axis\n        flux_unit : str, `~astropy.units.Unit`, optional\n            Unit of the flux axis\n        energy_power : int\n            Power of energy to multiply y axis with\n        kwargs : dict\n            Keyword arguments passed to :func:`matplotlib.pyplot.errorbar`\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axis object\n        """"""\n        import matplotlib.pyplot as plt\n\n        if ax is None:\n            ax = plt.gca()\n\n        sed_type = self.sed_type\n        y_unit = u.Unit(flux_unit or DEFAULT_UNIT[sed_type])\n\n        y = self.table[sed_type].quantity.to(y_unit)\n        x = self.e_ref.to(energy_unit)\n\n        # get errors and ul\n        is_ul = self.is_ul\n        x_err_all = self._plot_get_energy_err()\n        y_err_all = self._plot_get_flux_err(sed_type)\n\n        # handle energy power\n        e_unit = self._get_y_energy_unit(y_unit)\n        y_unit = y.unit * e_unit ** energy_power\n        y = (y * np.power(x, energy_power)).to(y_unit)\n\n        y_err, x_err = None, None\n\n        if y_err_all:\n            y_errn = (y_err_all[0] * np.power(x, energy_power)).to(y_unit)\n            y_errp = (y_err_all[1] * np.power(x, energy_power)).to(y_unit)\n            y_err = (y_errn[~is_ul].to_value(y_unit), y_errp[~is_ul].to_value(y_unit))\n\n        if x_err_all:\n            x_errn, x_errp = x_err_all\n            x_err = (\n                x_errn[~is_ul].to_value(energy_unit),\n                x_errp[~is_ul].to_value(energy_unit),\n            )\n\n        # set flux points plotting defaults\n        kwargs.setdefault(""marker"", ""+"")\n        kwargs.setdefault(""ls"", ""None"")\n\n        ebar = ax.errorbar(\n            x[~is_ul].value, y[~is_ul].value, yerr=y_err, xerr=x_err, **kwargs\n        )\n\n        if is_ul.any():\n            if x_err_all:\n                x_errn, x_errp = x_err_all\n                x_err = (\n                    x_errn[is_ul].to_value(energy_unit),\n                    x_errp[is_ul].to_value(energy_unit),\n                )\n\n            y_ul = self.table[sed_type + ""_ul""].quantity\n            y_ul = (y_ul * np.power(x, energy_power)).to(y_unit)\n\n            y_err = (0.5 * y_ul[is_ul].value, np.zeros_like(y_ul[is_ul].value))\n\n            kwargs.setdefault(""color"", ebar[0].get_color())\n\n            # pop label keyword to avoid that it appears twice in the legend\n            kwargs.pop(""label"", None)\n            ax.errorbar(\n                x[is_ul].value,\n                y_ul[is_ul].value,\n                xerr=x_err,\n                yerr=y_err,\n                uplims=True,\n                **kwargs,\n            )\n\n        ax.set_xscale(""log"", nonposx=""clip"")\n        ax.set_yscale(""log"", nonposy=""clip"")\n        ax.set_xlabel(f""Energy ({energy_unit})"")\n        ax.set_ylabel(f""{self.sed_type} ({y_unit})"")\n        return ax\n\n    def plot_ts_profiles(\n        self,\n        ax=None,\n        energy_unit=""TeV"",\n        add_cbar=True,\n        y_values=None,\n        y_unit=None,\n        **kwargs,\n    ):\n        """"""Plot fit statistic SED profiles as a density plot.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`\n            Axis object to plot on.\n        energy_unit : str, `~astropy.units.Unit`, optional\n            Unit of the energy axis\n        y_values : `astropy.units.Quantity`\n            Array of y-values to use for the fit statistic profile evaluation.\n        y_unit : str or `astropy.units.Unit`\n            Unit to use for the y-axis.\n        add_cbar : bool\n            Whether to add a colorbar to the plot.\n        kwargs : dict\n            Keyword arguments passed to :func:`matplotlib.pyplot.pcolormesh`\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axis object\n        """"""\n        import matplotlib.pyplot as plt\n\n        if ax is None:\n            ax = plt.gca()\n\n        self._validate_table(self.table, ""likelihood"")\n        y_unit = u.Unit(y_unit or DEFAULT_UNIT[self.sed_type])\n\n        if y_values is None:\n            ref_values = self.table[""ref_"" + self.sed_type].quantity\n            y_values = np.logspace(\n                np.log10(0.2 * ref_values.value.min()),\n                np.log10(5 * ref_values.value.max()),\n                500,\n            )\n            y_values = u.Quantity(y_values, y_unit, copy=False)\n\n        x = self.e_edges.to(energy_unit)\n\n        # Compute fit statistic ""image"" one energy bin at a time\n        # by interpolating e2dnde at the log bin centers\n        z = np.empty((len(self.table), len(y_values)))\n        for idx, row in enumerate(self.table):\n            y_ref = self.table[""ref_"" + self.sed_type].quantity[idx]\n            norm = (y_values / y_ref).to_value("""")\n            norm_scan = row[""norm_scan""]\n            ts_scan = row[""stat_scan""] - row[""stat""]\n            interp = interpolate_profile(norm_scan, ts_scan)\n            z[idx] = interp((norm,))\n\n        kwargs.setdefault(""vmax"", 0)\n        kwargs.setdefault(""vmin"", -4)\n        kwargs.setdefault(""zorder"", 0)\n        kwargs.setdefault(""cmap"", ""Blues"")\n        kwargs.setdefault(""linewidths"", 0)\n\n        # clipped values are set to NaN so that they appear white on the plot\n        z[-z < kwargs[""vmin""]] = np.nan\n        caxes = ax.pcolormesh(x.value, y_values.value, -z.T, **kwargs)\n        ax.set_xscale(""log"", nonposx=""clip"")\n        ax.set_yscale(""log"", nonposy=""clip"")\n        ax.set_xlabel(f""Energy ({energy_unit})"")\n        ax.set_ylabel(f""{self.sed_type} ({y_values.unit})"")\n\n        if add_cbar:\n            label = ""fit statistic difference""\n            ax.figure.colorbar(caxes, ax=ax, label=label)\n\n        return ax\n\n\nclass FluxPointsEstimator(FluxEstimator):\n    """"""Flux points estimator.\n\n    Estimates flux points for a given list of datasets, energies and spectral model.\n\n    To estimate the flux point the amplitude of the reference spectral model is\n    fitted within the energy range defined by the energy group. This is done for\n    each group independently. The amplitude is re-normalized using the ""norm"" parameter,\n    which specifies the deviation of the flux from the reference model in this\n    energy group. See https://gamma-astro-data-formats.readthedocs.io/en/latest/spectra/binned_likelihoods/index.html\n    for details.\n\n    The method is also described in the Fermi-LAT catalog paper\n    https://ui.adsabs.harvard.edu/#abs/2015ApJS..218...23A\n    or the HESS Galactic Plane Survey paper\n    https://ui.adsabs.harvard.edu/#abs/2018A%26A...612A...1H\n\n    Parameters\n    ----------\n    e_edges : `~astropy.units.Quantity`\n        Energy edges of the flux point bins.\n    source : str or int\n        For which source in the model to compute the flux points.\n    norm_min : float\n        Minimum value for the norm used for the fit statistic profile evaluation.\n    norm_max : float\n        Maximum value for the norm used for the fit statistic profile evaluation.\n    norm_n_values : int\n        Number of norm values used for the fit statistic profile.\n    norm_values : `numpy.ndarray`\n        Array of norm values to be used for the fit statistic profile.\n    sigma : int\n        Sigma to use for asymmetric error computation.\n    sigma_ul : int\n        Sigma to use for upper limit computation.\n    reoptimize : bool\n        Re-optimize other free model parameters.\n    """"""\n\n    def __init__(\n        self,\n        e_edges,\n        source=0,\n        norm_min=0.2,\n        norm_max=5,\n        norm_n_values=11,\n        norm_values=None,\n        sigma=1,\n        sigma_ul=2,\n        reoptimize=False,\n    ):\n        self.e_edges = e_edges\n        super().__init__(\n            source,\n            e_edges[:2],\n            norm_min,\n            norm_max,\n            norm_n_values,\n            norm_values,\n            sigma,\n            sigma_ul,\n            reoptimize,\n        )\n        self._contribute_to_stat = False\n\n    def _freeze_empty_background(self):\n        counts_all = self._estimate_counts()[""counts""]\n\n        for counts, dataset in zip(counts_all, self.datasets):\n            if isinstance(dataset, MapDataset) and counts == 0:\n                if dataset.background_model is not None:\n                    dataset.background_model.parameters.freeze_all()\n\n    @property\n    def e_groups(self):\n        """"""Energy grouping table `~astropy.table.Table`""""""\n        dataset = self.datasets[0]\n        energy_axis = dataset.counts.geom.get_axis_by_name(""energy"")\n        return energy_axis.group_table(self.e_edges)\n\n    def __str__(self):\n        s = f""{self.__class__.__name__}:\\n""\n        s += str(self.e_edges) + ""\\n""\n        return s\n\n    def run(self, datasets, steps=""all""):\n        """"""Run the flux point estimator for all energy groups.\n\n        Parameters\n        ----------\n        datasets : list of `~gammapy.spectrum.SpectrumDataset`\n            Spectrum datasets.\n        steps : list of str\n            Which steps to execute. See `estimate_flux_point` for details\n            and available options.\n\n        Returns\n        -------\n        flux_points : `FluxPoints`\n            Estimated flux points.\n        """"""\n        datasets = self._check_datasets(datasets)\n\n        if not datasets.is_all_same_type or not datasets.is_all_same_energy_shape:\n            raise ValueError(\n                ""Flux point estimation requires a list of datasets""\n                "" of the same type and data shape.""\n            )\n        self.datasets = datasets.copy()\n\n        rows = []\n        for e_group in self.e_groups:\n            if e_group[""bin_type""].strip() != ""normal"":\n                log.debug(""Skipping under-/ overflow bin in flux point estimation."")\n                continue\n\n            row = self._estimate_flux_point(e_group, steps=steps)\n            rows.append(row)\n\n        table = table_from_row_data(rows=rows, meta={""SED_TYPE"": ""likelihood""})\n        return FluxPoints(table).to_sed_type(""dnde"")\n\n    def _energy_mask(self, e_group, dataset):\n        energy_mask = np.zeros(dataset.data_shape)\n        energy_mask[e_group[""idx_min""] : e_group[""idx_max""] + 1] = 1\n        return energy_mask.astype(bool)\n\n    def _estimate_flux_point(self, e_group, steps=""all""):\n        """"""Estimate flux point for a single energy group.\n\n        Parameters\n        ----------\n        e_group : `~astropy.table.Row`\n            Energy group to compute the flux point for.\n        steps : list of str\n            Which steps to execute. Available options are:\n\n                * ""norm-err"": estimate symmetric error.\n                * ""errn-errp"": estimate asymmetric errors.\n                * ""ul"": estimate upper limits.\n                * ""ts"": estimate ts and sqrt(ts) values.\n                * ""norm-scan"": estimate fit statistic profiles.\n\n            By default all steps are executed.\n\n        Returns\n        -------\n        result : dict\n            Dict with results for the flux point.\n        """"""\n        e_min, e_max = e_group[""energy_min""], e_group[""energy_max""]\n        self.energy_range = [e_min, e_max]\n\n        for dataset in self.datasets:\n            dataset.mask_fit = self._energy_mask(e_group=e_group, dataset=dataset)\n            mask = dataset.mask_fit\n\n            if dataset.mask_safe is not None:\n                mask &= dataset.mask_safe\n\n            self._contribute_to_stat |= mask.any()\n\n        if not self._contribute_to_stat:\n            model = self.datasets[0].models[self.source].spectral_model\n            result = self._return_nan_result(model, steps=steps)\n            result.update(self._estimate_counts())\n            return result\n\n        with self.datasets.parameters.restore_values:\n\n            self._freeze_empty_background()\n\n            result = super().run(self.datasets, steps=steps)\n            result.update(self._estimate_counts())\n        return result\n\n    def _estimate_counts(self):\n        """"""Estimate counts for the flux point.\n\n        Returns\n        -------\n        result : dict\n            Dict with an array with one entry per dataset with counts for the flux point.\n        """"""\n        if not self._contribute_to_stat:\n            return {""counts"": np.zeros(len(self.datasets))}\n\n        counts = []\n        for dataset in self.datasets:\n            mask = dataset.mask_fit\n            if dataset.mask_safe is not None:\n                mask &= dataset.mask_safe\n\n            counts.append(dataset.counts.data[mask].sum())\n\n        return {""counts"": np.array(counts, dtype=int)}\n'"
gammapy/estimators/lightcurve.py,15,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport numpy as np\nimport astropy.units as u\nfrom astropy.table import Table\nfrom astropy.time import Time\nfrom gammapy.datasets import Datasets\nfrom gammapy.utils.scripts import make_path\nfrom gammapy.utils.table import table_from_row_data\nfrom .flux import FluxEstimator\nfrom .flux_point import FluxPoints\n\n\n__all__ = [""LightCurve"", ""LightCurveEstimator""]\n\nlog = logging.getLogger(__name__)\n\n\nclass LightCurve:\n    """"""Lightcurve container.\n\n    The lightcurve data is stored in ``table``.\n\n    For now we only support times stored in MJD format!\n\n    TODO: specification of format is work in progress\n    See https://github.com/open-gamma-ray-astro/gamma-astro-data-formats/pull/61\n\n    Usage: :ref:`time`\n\n    Parameters\n    ----------\n    table : `~astropy.table.Table`\n        Table with lightcurve data\n    """"""\n\n    def __init__(self, table):\n        self.table = table\n\n    def __repr__(self):\n        return f""{self.__class__.__name__}(len={len(self.table)})""\n\n    @property\n    def time_scale(self):\n        """"""Time scale (str).\n\n        Taken from table ""TIMESYS"" header.\n        Common values: ""TT"" or ""UTC"".\n        Assumed default is ""UTC"".\n        """"""\n        return self.table.meta.get(""TIMESYS"", ""utc"")\n\n    @property\n    def time_format(self):\n        """"""Time format (str).""""""\n        return ""mjd""\n\n    # @property\n    # def time_ref(self):\n    #     """"""Time reference (`~astropy.time.Time`).""""""\n    #     return time_ref_from_dict(self.table.meta)\n\n    def _make_time(self, colname):\n        val = self.table[colname].data\n        scale = self.time_scale\n        format = self.time_format\n        return Time(val, scale=scale, format=format)\n\n    @property\n    def time(self):\n        """"""Time (`~astropy.time.Time`).""""""\n        return self.time_mid\n\n    @property\n    def time_min(self):\n        """"""Time bin start (`~astropy.time.Time`).""""""\n        return self._make_time(""time_min"")\n\n    @property\n    def time_max(self):\n        """"""Time bin end (`~astropy.time.Time`).""""""\n        return self._make_time(""time_max"")\n\n    @property\n    def time_mid(self):\n        """"""Time bin center (`~astropy.time.Time`).""""""\n        return self.time_min + 0.5 * self.time_delta\n\n    @property\n    def time_delta(self):\n        """"""Time bin width (`~astropy.time.TimeDelta`).""""""\n        return self.time_max - self.time_min\n\n    @classmethod\n    def read(cls, filename, **kwargs):\n        """"""Read from file.\n\n        Parameters\n        ----------\n        filename : str\n            Filename\n        kwargs : dict\n            Keyword arguments passed to `astropy.table.Table.read`.\n        """"""\n        table = Table.read(make_path(filename), **kwargs)\n        return cls(table=table)\n\n    def write(self, filename, **kwargs):\n        """"""Write to file.\n\n        Parameters\n        ----------\n        filename : str\n            Filename\n        kwargs : dict\n            Keyword arguments passed to `astropy.table.Table.write`.\n        """"""\n        self.table.write(make_path(filename), **kwargs)\n\n    def plot(self, ax=None, time_format=""mjd"", flux_unit=""cm-2 s-1"", **kwargs):\n        """"""Plot flux points.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`, optional.\n            The `~matplotlib.axes.Axes` object to be drawn on.\n            If None, uses the current `~matplotlib.axes.Axes`.\n        time_format : {\'mjd\', \'iso\'}, optional\n            If \'iso\', the x axis will contain Matplotlib dates.\n            For formatting these dates see: https://matplotlib.org/gallery/ticks_and_spines/date_demo_rrule.html\n        flux_unit : str, `~astropy.units.Unit`, optional\n            Unit of the flux axis\n        kwargs : dict\n            Keyword arguments passed to :func:`matplotlib.pyplot.errorbar`\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axis object\n        """"""\n        import matplotlib.pyplot as plt\n        from matplotlib.dates import DateFormatter\n\n        if ax is None:\n            ax = plt.gca()\n\n        x, xerr = self._get_times_and_errors(time_format)\n        y, yerr = self._get_fluxes_and_errors(flux_unit)\n        is_ul, yul = self._get_flux_uls(flux_unit)\n\n        # length of the ul arrow\n        ul_arr = (\n            np.nanmax(np.concatenate((y[~is_ul], yul[is_ul])))\n            - np.nanmin(np.concatenate((y[~is_ul], yul[is_ul])))\n        ) * 0.1\n\n        # join fluxes and upper limits for the plot\n        y[is_ul] = yul[is_ul]\n        yerr[0][is_ul] = ul_arr\n\n        # set plotting defaults and plot\n        kwargs.setdefault(""marker"", ""+"")\n        kwargs.setdefault(""ls"", ""None"")\n\n        ax.errorbar(x=x, y=y, xerr=xerr, yerr=yerr, uplims=is_ul, **kwargs)\n        ax.set_xlabel(""Time ({})"".format(time_format.upper()))\n        ax.set_ylabel(""Flux ({:FITS})"".format(u.Unit(flux_unit)))\n        if time_format == ""iso"":\n            ax.xaxis.set_major_formatter(DateFormatter(""%Y-%m-%d %H:%M:%S""))\n            plt.setp(\n                ax.xaxis.get_majorticklabels(),\n                rotation=30,\n                ha=""right"",\n                rotation_mode=""anchor"",\n            )\n\n        return ax\n\n    def _get_fluxes_and_errors(self, unit=""cm-2 s-1""):\n        """"""Extract fluxes and corresponding errors\n\n        Helper function for the plot method.\n\n        Parameters\n        ----------\n        unit : str, `~astropy.units.Unit`, optional\n            Unit of the returned flux and errors values\n\n        Returns\n        -------\n        y : `numpy.ndarray`\n            Flux values\n        (yn, yp) : tuple of `numpy.ndarray`\n            Flux error values\n        """"""\n        y = self.table[""flux""].quantity.to(unit)\n\n        if all(k in self.table.colnames for k in [""flux_errp"", ""flux_errn""]):\n            yp = self.table[""flux_errp""].quantity.to(unit)\n            yn = self.table[""flux_errn""].quantity.to(unit)\n        elif ""flux_err"" in self.table.colnames:\n            yp = self.table[""flux_err""].quantity.to(unit)\n            yn = self.table[""flux_err""].quantity.to(unit)\n        else:\n            yp, yn = np.zeros_like(y), np.zeros_like(y)\n\n        return y.value, (yn.value, yp.value)\n\n    def _get_flux_uls(self, unit=""cm-2 s-1""):\n        """"""Extract flux upper limits\n\n        Helper function for the plot method.\n\n        Parameters\n        ----------\n        unit : str, `~astropy.units.Unit`, optional\n            Unit of the returned flux upper limit values\n\n        Returns\n        -------\n        is_ul : `numpy.ndarray`\n            Is flux point is an upper limit? (boolean array)\n        yul : `numpy.ndarray`\n            Flux upper limit values\n        """"""\n        try:\n            is_ul = self.table[""is_ul""].data.astype(""bool"")\n        except KeyError:\n            is_ul = np.zeros_like(self.table[""flux""]).data.astype(""bool"")\n\n        if is_ul.any():\n            yul = self.table[""flux_ul""].quantity.to(unit)\n        else:\n            yul = np.zeros_like(self.table[""flux""]).quantity\n            yul[:] = np.nan\n\n        return is_ul, yul.value\n\n    def _get_times_and_errors(self, time_format=""mjd""):\n        """"""Extract times and corresponding errors\n\n        Helper function for the plot method.\n\n        Parameters\n        ----------\n        time_format : {\'mjd\', \'iso\'}, optional\n            Time format of the times. If \'iso\', times and errors will be\n            returned as `~datetime.datetime` and `~datetime.timedelta` objects\n\n        Returns\n        -------\n        x : `~numpy.ndarray` or of `~datetime.datetime`\n            Time values or `~datetime.datetime` instances if \'iso\' is chosen\n            as time format\n        (xn, xp) : tuple of `numpy.ndarray` of `~datetime.timedelta`\n            Tuple of time error values or `~datetime.timedelta` instances if\n            \'iso\' is chosen as time format\n        """"""\n        x = self.time\n\n        try:\n            xn, xp = x - self.time_min, self.time_max - x\n        except KeyError:\n            xn, xp = x - x, x - x\n\n        if time_format == ""iso"":\n            x = x.datetime\n            xn = xn.to_datetime()\n            xp = xp.to_datetime()\n        elif time_format == ""mjd"":\n            x = x.mjd\n            xn = xn.to(""day"").value\n            xp = xp.to(""day"").value\n        else:\n            raise ValueError(f""Invalid time_format: {time_format}"")\n\n        return x, (xn, xp)\n\n\ndef group_datasets_in_time_interval(datasets, time_intervals, atol=""1e-6 s""):\n    """"""Compute the table with the info on the group to which belong each dataset.\n\n    The Tstart and Tstop are stored in MJD from a scale in ""utc"".\n\n    Parameters\n    ----------\n    datasets : list of `~gammapy.spectrum.SpectrumDataset` or `~gammapy.cube.MapDataset`\n        Spectrum or Map datasets.\n    time_intervals : list of `astropy.time.Time`\n        Start and stop time for each interval to compute the LC\n    atol : `~astropy.units.Quantity`\n        Tolerance value for time comparison with different scale. Default 1e-6 sec.\n\n    Returns\n    -------\n    table_info : `~astropy.table.Table`\n        Contains the grouping info for each dataset\n    """"""\n    dataset_group_ID_table = Table(\n        names=(""Name"", ""Tstart"", ""Tstop"", ""Bin_type"", ""Group_ID""),\n        meta={""name"": ""first table""},\n        dtype=(""S10"", ""f8"", ""f8"", ""S10"", ""i8""),\n    )\n    time_intervals_lowedges = Time(\n        [time_interval[0] for time_interval in time_intervals]\n    )\n    time_intervals_upedges = Time(\n        [time_interval[1] for time_interval in time_intervals]\n    )\n\n    for dataset in datasets:\n        tstart = dataset.gti.time_start[0]\n        tstop = dataset.gti.time_stop[-1]\n        mask1 = tstart >= time_intervals_lowedges - atol\n        mask2 = tstop <= time_intervals_upedges + atol\n        mask = mask1 & mask2\n        if np.any(mask):\n            group_index = np.where(mask)[0]\n            bin_type = """"\n        else:\n            group_index = -1\n            if np.any(mask1):\n                bin_type = ""Overflow""\n            elif np.any(mask2):\n                bin_type = ""Underflow""\n            else:\n                bin_type = ""Outflow""\n        dataset_group_ID_table.add_row(\n            [dataset.name, tstart.utc.mjd, tstop.utc.mjd, bin_type, group_index]\n        )\n\n    return dataset_group_ID_table\n\n\nclass LightCurveEstimator(FluxEstimator):\n    """"""Compute light curve.\n\n    The estimator will fit the source model component to datasets in each of the time intervals\n    provided.\n\n    If no time intervals are provided, the estimator will use the time intervals defined by the datasets GTIs.\n\n    To be included in the estimation, the dataset must have their GTI fully overlapping a time interval.\n\n    Parameters\n    ----------\n    time_intervals : list of `astropy.time.Time`\n        Start and stop time for each interval to compute the LC\n    source : str\n        For which source in the model to compute the flux points. Default is 0\n    energy_range : tuple of `~astropy.units.Quantity`\n        Energy range on which to compute the flux. Default is 1-10 TeV\n    atol : `~astropy.units.Quantity`\n        Tolerance value for time comparison with different scale. Default 1e-6 sec.\n    norm_min : float\n        Minimum value for the norm used for the fit statistic profile evaluation.\n    norm_max : float\n        Maximum value for the norm used for the fit statistic profile evaluation.\n    norm_n_values : int\n        Number of norm values used for the fit statistic profile.\n    norm_values : `numpy.ndarray`\n        Array of norm values to be used for the fit statistic profile.\n    sigma : int\n        Sigma to use for asymmetric error computation.\n    sigma_ul : int\n        Sigma to use for upper limit computation.\n    reoptimize : bool\n        reoptimize other parameters during fit statistic scan?\n    """"""\n\n    def __init__(\n        self,\n        time_intervals=None,\n        source=0,\n        energy_range=[1.0, 10.0] * u.TeV,\n        atol=""1e-6 s"",\n        norm_min=0.2,\n        norm_max=5,\n        norm_n_values=11,\n        norm_values=None,\n        sigma=1,\n        sigma_ul=2,\n        reoptimize=False,\n    ):\n\n        self.input_time_intervals = time_intervals\n\n        self.group_table_info = None\n        self.atol = u.Quantity(atol)\n\n        super().__init__(\n            source,\n            energy_range,\n            norm_min,\n            norm_max,\n            norm_n_values,\n            norm_values,\n            sigma,\n            sigma_ul,\n            reoptimize,\n        )\n\n    def _check_and_sort_time_intervals(self, time_intervals):\n        """"""Sort the time_intervals by increasing time if not already ordered correctly.\n\n        Parameters\n        ----------\n        time_intervals : list of `astropy.time.Time`\n            Start and stop time for each interval to compute the LC\n        """"""\n        time_start = Time([interval[0] for interval in time_intervals])\n        time_stop = Time([interval[1] for interval in time_intervals])\n        sorted_indices = time_start.argsort()\n        time_start_sorted = time_start[sorted_indices]\n        time_stop_sorted = time_stop[sorted_indices]\n        diff_time_stop = np.diff(time_stop_sorted)\n        diff_time_interval_edges = time_start_sorted[1:] - time_stop_sorted[:-1]\n        if np.any(diff_time_stop < 0) or np.any(diff_time_interval_edges < 0):\n            raise ValueError(""LightCurveEstimator requires non-overlapping time bins."")\n        else:\n            return [\n                Time([tstart, tstop])\n                for tstart, tstop in zip(time_start_sorted, time_stop_sorted)\n            ]\n\n    def run(self, datasets, steps=""all""):\n        """"""Run light curve extraction.\n\n        Normalize integral and energy flux between emin and emax.\n\n        Parameters\n        ----------\n        datasets : list of `~gammapy.spectrum.SpectrumDataset` or `~gammapy.cube.MapDataset`\n            Spectrum or Map datasets.\n        steps : list of str\n            Which steps to execute. Available options are:\n\n                * ""err"": estimate symmetric error.\n                * ""errn-errp"": estimate asymmetric errors.\n                * ""ul"": estimate upper limits.\n                * ""ts"": estimate ts and sqrt(ts) values.\n                * ""norm-scan"": estimate fit statistic profiles.\n\n            By default all steps are executed.\n\n        Returns\n        -------\n        lightcurve : `~gammapy.time.LightCurve`\n            the Light Curve object\n        """"""\n        if self.input_time_intervals is None:\n            time_intervals = [\n                Time([d.gti.time_start[0], d.gti.time_stop[-1]]) for d in datasets\n            ]\n        else:\n            time_intervals = self.input_time_intervals\n\n        time_intervals = self._check_and_sort_time_intervals(time_intervals)\n\n        rows = []\n        self.group_table_info = group_datasets_in_time_interval(\n            datasets=datasets, time_intervals=time_intervals, atol=self.atol\n        )\n        if np.all(self.group_table_info[""Group_ID""] == -1):\n            raise ValueError(""LightCurveEstimator: No datasets in time intervals"")\n\n        for igroup, time_interval in enumerate(time_intervals):\n            index_dataset = np.where(self.group_table_info[""Group_ID""] == igroup)[0]\n            if len(index_dataset) == 0:\n                log.debug(""No Dataset for the time interval "" + str(igroup))\n                continue\n\n            row = {""time_min"": time_interval[0].mjd, ""time_max"": time_interval[1].mjd}\n            interval_list_dataset = Datasets([datasets[int(_)] for _ in index_dataset])\n            row.update(\n                self.estimate_time_bin_flux(interval_list_dataset, time_interval, steps)\n            )\n            rows.append(row)\n        table = table_from_row_data(rows=rows, meta={""SED_TYPE"": ""likelihood""})\n        table = FluxPoints(table).to_sed_type(""flux"").table\n        return LightCurve(table)\n\n    def estimate_time_bin_flux(self, datasets, time_interval, steps=""all""):\n        """"""Estimate flux point for a single energy group.\n\n        Parameters\n        ----------\n        datasets : `~gammapy.modeling.Datasets`\n            the list of dataset object\n        time_interval : astropy.time.Time`\n            Start and stop time for each interval\n        steps : list of str\n            Which steps to execute. Available options are:\n\n                * ""err"": estimate symmetric error.\n                * ""errn-errp"": estimate asymmetric errors.\n                * ""ul"": estimate upper limits.\n                * ""ts"": estimate ts and sqrt(ts) values.\n                * ""norm-scan"": estimate likelihood profiles.\n\n            By default all steps are executed.\n\n        Returns\n        -------\n        result : dict\n            Dict with results for the flux point.\n        """"""\n        result = super().run(datasets, steps=steps)\n        result.update(self._estimate_counts(datasets))\n        if not result.pop(""success""):\n            log.warning(\n                ""Fit failed for time bin between {t_min} and {t_max},"".format(\n                    t_min=time_interval[0].mjd, t_max=time_interval[1].mjd\n                )\n            )\n        return result\n\n    def _estimate_counts(self, datasets):\n        """"""Estimate counts for the flux point.\n\n        Parameters\n        ----------\n        datasets : `~gammapy.modeling.Datasets`\n            the list of dataset object\n\n        Returns\n        -------\n        result : dict\n            Dict with an array with one entry per dataset with counts for the flux point.\n        """"""\n\n        counts = []\n        for dataset in datasets:\n            mask = dataset.mask\n            counts.append(dataset.counts.data[mask].sum())\n\n        return {""counts"": np.array(counts, dtype=int).sum()}\n'"
gammapy/estimators/parameter_estimator.py,8,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport numpy as np\nfrom gammapy.datasets import Datasets\nfrom gammapy.modeling import Fit\n\nlog = logging.getLogger(__name__)\n\n\nclass ParameterEstimator:\n    """"""Model parameter estimator.\n\n    Estimates a model parameter for a group of datasets.\n    Compute best fit value, symmetric and asymmetric errors, delta TS for a given null value\n    as well as parameter upper limit and fit statistic profile.\n\n    Parameters\n    ----------\n    sigma : int\n        Sigma to use for asymmetric error computation.\n    sigma_ul : int\n        Sigma to use for upper limit computation.\n    reoptimize : bool\n        Re-optimize other free model parameters. Default is True.\n    n_scan_values : int\n        Number of values used to scan fit stat profile\n    scan_n_err : float\n        Range to scan in number of parameter error\n\n    """"""\n\n    def __init__(\n        self, sigma=1, sigma_ul=2, reoptimize=True, n_scan_values=30, scan_n_err=3,\n    ):\n        self.sigma = sigma\n        self.sigma_ul = sigma_ul\n        self.reoptimize = reoptimize\n        self.n_scan_values = n_scan_values\n        self.scan_n_err = scan_n_err\n\n    def __str__(self):\n        s = f""{self.__class__.__name__}:\\n""\n        s += str(self.datasets) + ""\\n""\n        return s\n\n    def _check_datasets(self, datasets):\n        """"""Check datasets geometry consistency and return Datasets object""""""\n        if not isinstance(datasets, Datasets):\n            datasets = Datasets(datasets)\n\n        return datasets\n\n    def _freeze_parameters(self, parameter):\n        """"""Freeze all other parameters""""""\n        for par in self.datasets.parameters:\n            if par is not parameter:\n                par.frozen = True\n\n    def _compute_scan_values(self, value, value_error, par_min, par_max):\n        """"""Define parameter value range to be scanned""""""\n        min_range = value - self.scan_n_err * value_error\n        if not np.isnan(par_min):\n            min_range = np.maximum(par_min, min_range)\n        max_range = value + self.scan_n_err * value_error\n        if not np.isnan(par_max):\n            max_range = np.minimum(par_max, max_range)\n\n        return np.linspace(min_range, max_range, self.n_scan_values)\n\n    def _find_best_fit(self, parameter):\n        """"""Find the best fit solution and store results.""""""\n        fit_result = self.fit.optimize()\n\n        if fit_result.success:\n            value = parameter.value\n        else:\n            value = np.nan\n\n        result = {\n            parameter.name: value,\n            ""stat"": fit_result.total_stat,\n            ""success"": fit_result.success,\n        }\n\n        self.fit_result = fit_result\n        return result\n\n    def _estimate_ts_for_null_value(self, parameter, null_value=1e-150):\n        """"""Returns the fit statistic value for a given null value of the parameter.""""""\n        with self.datasets.parameters.restore_values:\n            parameter.value = null_value\n            parameter.frozen = True\n            result = self.fit.optimize()\n        if not result.success:\n            log.warning(\n                ""Fit failed for parameter null value, returning NaN. Check input null value.""\n            )\n            return np.nan\n        return result.total_stat\n\n    def run(\n        self, datasets, parameter, steps=""all"", null_value=1e-150, scan_values=None\n    ):\n        """"""Run the parameter estimator.\n\n        Parameters\n        ----------\n        datasets : `~gammapy.datasets.Datasets`\n            The datasets used to estimate the model parameter\n        parameter : `~gammapy.modeling.Parameter`\n            the parameter to be estimated\n        steps : list of str\n            Which steps to execute. Available options are:\n                * ""err"": estimate symmetric error from covariance\n                * ""ts"": estimate delta TS with parameter null (reference) value\n                * ""errn-errp"": estimate asymmetric errors.\n                * ""ul"": estimate upper limits.\n                * ""scan"": estimate fit statistic profiles.\n\n            By default all steps are executed.\n        null_value : float\n            the null value to be used for delta TS estimation.\n            Default is 1e-150 since 0 can be an issue for some parameters.\n        scan_values : `numpy.ndarray`\n            Array of parameter values to be used for the fit statistic profile.\n            If set to None, scan values are automatically calculated. Default is None.\n\n        Returns\n        -------\n        result : dict\n            Dict with the various parameter estimation values.\n        """"""\n        self.datasets = self._check_datasets(datasets)\n        self.fit = Fit(datasets)\n        self.fit_result = None\n\n        with self.datasets.parameters.restore_values:\n\n            if not self.reoptimize:\n                self._freeze_parameters(parameter)\n\n            if steps == ""all"":\n                steps = [""err"", ""ts"", ""errp-errn"", ""ul"", ""scan""]\n\n            result = self._find_best_fit(parameter)\n            TS1 = result[""stat""]\n\n            value_max = result[parameter.name]\n\n            if ""err"" in steps:\n                res = self.fit.covariance()\n                value_err = res.parameters[parameter].error\n                result.update({f""{parameter.name}_err"": value_err})\n\n            if ""errp-errn"" in steps:\n                res = self.fit.confidence(parameter=parameter, sigma=self.sigma)\n                result.update(\n                    {\n                        f""{parameter.name}_errp"": res[""errp""],\n                        f""{parameter.name}_errn"": res[""errn""],\n                    }\n                )\n\n            if ""ul"" in steps:\n                res = self.fit.confidence(parameter=parameter, sigma=self.sigma_ul)\n                result.update({f""{parameter.name}_ul"": res[""errp""] + value_max})\n\n            if ""ts"" in steps:\n                TS0 = self._estimate_ts_for_null_value(parameter, null_value)\n                res = TS0 - TS1\n                result.update(\n                    {""sqrt_ts"": np.sqrt(res), ""ts"": res, ""null_value"": null_value}\n                )\n                # TODO: should not need this\n                self.fit.optimize()\n\n            if ""scan"" in steps:\n                if scan_values is None:\n                    scan_values = self._compute_scan_values(\n                        value_max, value_err, parameter.min, parameter.max\n                    )\n\n                res = self.fit.stat_profile(\n                    parameter, values=scan_values, reoptimize=self.reoptimize\n                )\n                result.update(\n                    {f""{parameter.name}_scan"": res[""values""], ""stat_scan"": res[""stat""]}\n                )\n        return result\n'"
gammapy/estimators/profile.py,16,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Tools to create profiles (i.e. 1D ""slices"" from 2D images).""""""\nimport numpy as np\nimport scipy.ndimage\nfrom astropy import units as u\nfrom astropy.convolution import Box1DKernel, Gaussian1DKernel\nfrom astropy.coordinates import Angle\nfrom astropy.table import Table\n\n__all__ = [""ImageProfile"", ""ImageProfileEstimator""]\n\n\n# TODO: implement measuring profile along arbitrary directions\n# TODO: think better about error handling. e.g. MC based methods\nclass ImageProfileEstimator:\n    """"""Estimate profile from image.\n\n    Parameters\n    ----------\n    x_edges : `~astropy.coordinates.Angle`\n        Coordinate edges to define a custom measument grid (optional).\n    method : [\'sum\', \'mean\']\n        Compute sum or mean within profile bins.\n    axis : [\'lon\', \'lat\', \'radial\']\n        Along which axis to estimate the profile.\n    center : `~astropy.coordinates.SkyCoord`\n        Center coordinate for the radial profile option.\n\n    Examples\n    --------\n    This example shows how to compute a counts profile for the Fermi galactic\n    center region::\n\n        import matplotlib.pyplot as plt\n        from gammapy.maps import ImageProfileEstimator\n        from gammapy.maps import Map\n        from astropy import units as u\n\n        # load example data\n        filename = \'$GAMMAPY_DATA/fermi-3fhl-gc/fermi-3fhl-gc-counts.fits.gz\'\n        fermi_cts = Map.read(filename)\n\n        # set up profile estimator and run\n        p = ImageProfileEstimator(axis=\'lon\', method=\'sum\')\n        profile = p.run(fermi_cts)\n\n        # smooth profile and plot\n        smoothed = profile.smooth(kernel=\'gauss\')\n        smoothed.peek()\n        plt.show()\n    """"""\n\n    def __init__(self, x_edges=None, method=""sum"", axis=""lon"", center=None):\n        self._x_edges = x_edges\n\n        if method not in [""sum"", ""mean""]:\n            raise ValueError(""Not a valid method, choose either \'sum\' or \'mean\'"")\n\n        if axis not in [""lon"", ""lat"", ""radial""]:\n            raise ValueError(""Not a valid axis, choose either \'lon\' or \'lat\'"")\n\n        if method == ""radial"" and center is None:\n            raise ValueError(""Please provide center coordinate for radial profiles"")\n\n        self.parameters = {""method"": method, ""axis"": axis, ""center"": center}\n\n    def _get_x_edges(self, image):\n        if self._x_edges is not None:\n            return self._x_edges\n\n        p = self.parameters\n        coordinates = image.geom.get_coord(mode=""edges"").skycoord\n\n        if p[""axis""] == ""lat"":\n            x_edges = coordinates[:, 0].data.lat\n        elif p[""axis""] == ""lon"":\n            lon = coordinates[0, :].data.lon\n            x_edges = lon.wrap_at(""180d"")\n        elif p[""axis""] == ""radial"":\n            rad_step = image.geom.pixel_scales.mean()\n            corners = [0, 0, -1, -1], [0, -1, 0, -1]\n            rad_max = coordinates[corners].separation(p[""center""]).max()\n            x_edges = Angle(np.arange(0, rad_max.deg, rad_step.deg), unit=""deg"")\n\n        return x_edges\n\n    def _estimate_profile(self, image, image_err, mask):\n        p = self.parameters\n        labels = self._label_image(image, mask)\n\n        profile_err = None\n\n        index = np.arange(1, len(self._get_x_edges(image)))\n\n        if p[""method""] == ""sum"":\n            profile = scipy.ndimage.sum(image.data, labels.data, index)\n\n            if image.unit.is_equivalent(""counts""):\n                profile_err = np.sqrt(profile)\n            elif image_err:\n                # gaussian error propagation\n                err_sum = scipy.ndimage.sum(image_err.data ** 2, labels.data, index)\n                profile_err = np.sqrt(err_sum)\n\n        elif p[""method""] == ""mean"":\n            # gaussian error propagation\n            profile = scipy.ndimage.mean(image.data, labels.data, index)\n            if image_err:\n                N = scipy.ndimage.sum(~np.isnan(image_err.data), labels.data, index)\n                err_sum = scipy.ndimage.sum(image_err.data ** 2, labels.data, index)\n                profile_err = np.sqrt(err_sum) / N\n\n        return profile, profile_err\n\n    def _label_image(self, image, mask=None):\n        p = self.parameters\n\n        coordinates = image.geom.get_coord().skycoord\n        x_edges = self._get_x_edges(image)\n\n        if p[""axis""] == ""lon"":\n            lon = coordinates.data.lon.wrap_at(""180d"")\n            data = np.digitize(lon.degree, x_edges.deg)\n\n        elif p[""axis""] == ""lat"":\n            lat = coordinates.data.lat\n            data = np.digitize(lat.degree, x_edges.deg)\n\n        elif p[""axis""] == ""radial"":\n            separation = coordinates.separation(p[""center""])\n            data = np.digitize(separation.degree, x_edges.deg)\n\n        if mask is not None:\n            # assign masked values to background\n            data[mask.data] = 0\n\n        return image.copy(data=data)\n\n    def run(self, image, image_err=None, mask=None):\n        """"""Run image profile estimator.\n\n        Parameters\n        ----------\n        image : `~gammapy.maps.Map`\n            Input image to run profile estimator on.\n        image_err : `~gammapy.maps.Map`\n            Input error image to run profile estimator on.\n        mask : `~gammapy.maps.Map`\n            Optional mask to exclude regions from the measurement.\n\n        Returns\n        -------\n        profile : `ImageProfile`\n            Result image profile object.\n        """"""\n        p = self.parameters\n\n        if image.unit.is_equivalent(""count""):\n            image_err = image.copy(data=np.sqrt(image.data))\n\n        profile, profile_err = self._estimate_profile(image, image_err, mask)\n\n        result = Table()\n        x_edges = self._get_x_edges(image)\n        result[""x_min""] = x_edges[:-1]\n        result[""x_max""] = x_edges[1:]\n        result[""x_ref""] = (x_edges[:-1] + x_edges[1:]) / 2\n        result[""profile""] = profile * image.unit\n\n        if profile_err is not None:\n            result[""profile_err""] = profile_err * image.unit\n\n        result.meta[""PROFILE_TYPE""] = p[""axis""]\n\n        return ImageProfile(result)\n\n\nclass ImageProfile:\n    """"""Image profile class.\n\n    The image profile data is stored in `~astropy.table.Table` object, with the\n    following columns:\n\n        * `x_ref` Coordinate bin center (required).\n        * `x_min` Coordinate bin minimum (optional).\n        * `x_max` Coordinate bin maximum (optional).\n        * `profile` Image profile data (required).\n        * `profile_err` Image profile data error (optional).\n\n    Parameters\n    ----------\n    table : `~astropy.table.Table`\n        Table instance with the columns specified as above.\n    """"""\n\n    def __init__(self, table):\n        self.table = table\n\n    def smooth(self, kernel=""box"", radius=""0.1 deg"", **kwargs):\n        r""""""Smooth profile with error propagation.\n\n        Smoothing is described by a convolution:\n\n        .. math::\n            x_j = \\sum_i x_{(j - i)} h_i\n\n        Where :math:`h_i` are the coefficients of the convolution kernel.\n\n        The corresponding error on :math:`x_j` is then estimated using Gaussian\n        error propagation, neglecting correlations between the individual\n        :math:`x_{(j - i)}`:\n\n        .. math::\n            \\Delta x_j = \\sqrt{\\sum_i \\Delta x^{2}_{(j - i)} h^{2}_i}\n\n        Parameters\n        ----------\n        kernel : {\'gauss\', \'box\'}\n            Kernel shape\n        radius : `~astropy.units.Quantity`, str or float\n            Smoothing width given as quantity or float. If a float is given it\n            is interpreted as smoothing width in pixels. If an (angular) quantity\n            is given it is converted to pixels using `xref[1] - x_ref[0]`.\n        kwargs : dict\n            Keyword arguments passed to `~scipy.ndimage.uniform_filter`\n            (\'box\') and `~scipy.ndimage.gaussian_filter` (\'gauss\').\n\n        Returns\n        -------\n        profile : `ImageProfile`\n            Smoothed image profile.\n        """"""\n        table = self.table.copy()\n        profile = table[""profile""]\n\n        radius = u.Quantity(radius)\n        radius = np.abs(radius / np.diff(self.x_ref))[0]\n        width = 2 * radius.value + 1\n\n        if kernel == ""box"":\n            smoothed = scipy.ndimage.uniform_filter(\n                profile.astype(""float""), width, **kwargs\n            )\n            # renormalize data\n            if table[""profile""].unit.is_equivalent(""count""):\n                smoothed *= int(width)\n                smoothed_err = np.sqrt(smoothed)\n            elif ""profile_err"" in table.colnames:\n                profile_err = table[""profile_err""]\n                # use gaussian error propagation\n                box = Box1DKernel(width)\n                err_sum = scipy.ndimage.convolve(profile_err ** 2, box.array ** 2)\n                smoothed_err = np.sqrt(err_sum)\n        elif kernel == ""gauss"":\n            smoothed = scipy.ndimage.gaussian_filter(\n                profile.astype(""float""), width, **kwargs\n            )\n            # use gaussian error propagation\n            if ""profile_err"" in table.colnames:\n                profile_err = table[""profile_err""]\n                gauss = Gaussian1DKernel(width)\n                err_sum = scipy.ndimage.convolve(profile_err ** 2, gauss.array ** 2)\n                smoothed_err = np.sqrt(err_sum)\n        else:\n            raise ValueError(""Not valid kernel choose either \'box\' or \'gauss\'"")\n\n        table[""profile""] = smoothed * self.table[""profile""].unit\n        if ""profile_err"" in table.colnames:\n            table[""profile_err""] = smoothed_err * self.table[""profile""].unit\n        return self.__class__(table)\n\n    def plot(self, ax=None, **kwargs):\n        """"""Plot image profile.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`\n            Axes object\n        **kwargs : dict\n            Keyword arguments passed to `~matplotlib.axes.Axes.plot`\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axes object\n        """"""\n        import matplotlib.pyplot as plt\n\n        if ax is None:\n            ax = plt.gca()\n\n        y = self.table[""profile""].data\n        x = self.x_ref.value\n        ax.plot(x, y, **kwargs)\n        ax.set_xlabel(""lon"")\n        ax.set_ylabel(""profile"")\n        ax.set_xlim(x.max(), x.min())\n        return ax\n\n    def plot_err(self, ax=None, **kwargs):\n        """"""Plot image profile error as band.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`\n            Axes object\n        **kwargs : dict\n            Keyword arguments passed to plt.fill_between()\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axes object\n        """"""\n        import matplotlib.pyplot as plt\n\n        if ax is None:\n            ax = plt.gca()\n\n        y = self.table[""profile""].data\n        ymin = y - self.table[""profile_err""].data\n        ymax = y + self.table[""profile_err""].data\n        x = self.x_ref.value\n\n        # plotting defaults\n        kwargs.setdefault(""alpha"", 0.5)\n\n        ax.fill_between(x, ymin, ymax, **kwargs)\n        ax.set_xlabel(""x (deg)"")\n        ax.set_ylabel(""profile"")\n        return ax\n\n    @property\n    def x_ref(self):\n        """"""Reference x coordinates.""""""\n        return self.table[""x_ref""].quantity\n\n    @property\n    def x_min(self):\n        """"""Min. x coordinates.""""""\n        return self.table[""x_min""].quantity\n\n    @property\n    def x_max(self):\n        """"""Max. x coordinates.""""""\n        return self.table[""x_max""].quantity\n\n    @property\n    def profile(self):\n        """"""Image profile quantity.""""""\n        return self.table[""profile""].quantity\n\n    @property\n    def profile_err(self):\n        """"""Image profile error quantity.""""""\n        try:\n            return self.table[""profile_err""].quantity\n        except KeyError:\n            return None\n\n    def peek(self, figsize=(8, 4.5), **kwargs):\n        """"""Show image profile and error.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Keyword arguments passed to `ImageProfile.plot_profile()`\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axes object\n        """"""\n        import matplotlib.pyplot as plt\n\n        fig = plt.figure(figsize=figsize)\n        ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n        ax = self.plot(ax, **kwargs)\n\n        if ""profile_err"" in self.table.colnames:\n            ax = self.plot_err(ax, color=kwargs.get(""c""))\n\n        return ax\n\n    def normalize(self, mode=""peak""):\n        """"""Normalize profile to peak value or integral.\n\n        Parameters\n        ----------\n        mode : [\'integral\', \'peak\']\n            Normalize image profile so that it integrates to unity (\'integral\')\n            or the maximum value corresponds to one (\'peak\').\n\n        Returns\n        -------\n        profile : `ImageProfile`\n            Normalized image profile.\n        """"""\n        table = self.table.copy()\n        profile = self.table[""profile""]\n        if mode == ""peak"":\n            norm = np.nanmax(profile)\n        elif mode == ""integral"":\n            norm = np.nansum(profile)\n        else:\n            raise ValueError(f""Invalide normalization mode: {mode!r}"")\n\n        table[""profile""] /= norm\n\n        if ""profile_err"" in table.colnames:\n            table[""profile_err""] /= norm\n\n        return self.__class__(table)\n'"
gammapy/estimators/sensitivity.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom astropy.table import Column, Table\nfrom gammapy.modeling.models import PowerLawSpectralModel, SkyModel\nfrom gammapy.stats import WStatCountsStatistic\n\n__all__ = [""SensitivityEstimator""]\n\n\nclass SensitivityEstimator:\n    """"""Estimate differential sensitivity.\n\n    This class allows to determine for each reconstructed energy bin the flux\n    associated to the number of gamma-ray events for which the significance is\n    ``sigma``, and being larger than ``gamma_min`` and ``bkg_sys`` percent\n    larger than the number of background events in the ON region.\n\n\n    Parameters\n    ----------\n    spectrum : `SpectralModel`\n        Spectral model assumption\n    sigma : float, optional\n        Minimum significance\n    gamma_min : float, optional\n        Minimum number of gamma-rays\n\n    Examples\n    --------\n    For a usage example see `cta_sensitivity.html <../notebooks/cta_sensitivity.html>`__\n\n    """"""\n\n    def __init__(\n        self, spectrum=None, sigma=5.0, gamma_min=10,\n    ):\n\n        if spectrum is None:\n            spectrum = PowerLawSpectralModel(index=2, amplitude=""1 cm-2 s-1 TeV-1"")\n\n        self.spectrum = spectrum\n        self.sigma = sigma\n        self.gamma_min = gamma_min\n\n    def estimate_min_excess(self, dataset):\n        """"""Estimate minimum excess to reach the given significance.\n\n        Parameters\n        ----------\n        dataset : `SpectrumDataset`\n            Spectrum dataset\n\n        Returns\n        -------\n        excess : `RegionNDMap`\n            Minimal excess\n        """"""\n        n_off = dataset.counts_off.data\n\n        stat = WStatCountsStatistic(\n            n_on=np.ones_like(n_off), n_off=n_off, alpha=dataset.alpha.data\n        )\n        excess_counts = stat.excess_matching_significance(self.sigma)\n        is_gamma_limited = excess_counts < self.gamma_min\n        excess_counts[is_gamma_limited] = self.gamma_min\n        excess = dataset.background.copy()\n        excess.data = excess_counts\n        return excess\n\n    def estimate_min_e2dnde(self, excess, dataset):\n        """"""Estimate dnde from given min. excess\n\n        Parameters\n        ----------\n        excess : `RegionNDMap`\n            Minimal excess\n        dataset : `SpectrumDataset`\n            Spectrum dataset\n\n        Returns\n        -------\n        e2dnde : `~astropy.units.Quantity`\n            Minimal differential flux.\n        """"""\n        energy = dataset.background.geom.axes[0].center\n\n        dataset.models = SkyModel(spectral_model=self.spectrum)\n        npred = dataset.npred()\n\n        phi_0 = excess / npred\n\n        dnde_model = self.spectrum(energy=energy)\n        dnde = phi_0.data[:, 0, 0] * dnde_model * energy ** 2\n        return dnde.to(""erg / (cm2 s)"")\n\n    def _get_criterion(self, excess):\n        is_gamma_limited = excess == self.gamma_min\n        criterion = np.chararray(excess.shape, itemsize=12)\n        criterion[is_gamma_limited] = ""gamma""\n        criterion[~is_gamma_limited] = ""significance""\n        return criterion\n\n    def run(self, dataset):\n        """"""Run the sensitivty estimation\n\n        Parameters\n        ----------\n        dataset : `SpectrumDatasetOnOff`\n            Dataset to compute sensitivty for.\n\n        Returns\n        -------\n        sensitivity : `~astropy.table.Table`\n            Sensitivity table\n        """"""\n        energy = dataset.edisp.e_reco.center\n        excess = self.estimate_min_excess(dataset)\n        e2dnde = self.estimate_min_e2dnde(excess, dataset)\n        criterion = self._get_criterion(excess.data.squeeze())\n\n        return Table(\n            [\n                Column(\n                    data=energy,\n                    name=""energy"",\n                    format=""5g"",\n                    description=""Reconstructed Energy"",\n                ),\n                Column(\n                    data=e2dnde,\n                    name=""e2dnde"",\n                    format=""5g"",\n                    description=""Energy squared times differential flux"",\n                ),\n                Column(\n                    data=excess.data.squeeze(),\n                    name=""excess"",\n                    format=""5g"",\n                    description=""Number of excess counts in the bin"",\n                ),\n                Column(\n                    data=dataset.background.data.squeeze(),\n                    name=""background"",\n                    format=""5g"",\n                    description=""Number of background counts in the bin"",\n                ),\n                Column(\n                    data=criterion,\n                    name=""criterion"",\n                    description=""Sensitivity-limiting criterion"",\n                ),\n            ]\n        )\n'"
gammapy/estimators/ts_map.py,21,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Functions to compute TS images.""""""\nimport functools\nimport logging\nimport warnings\nimport numpy as np\nimport scipy.optimize\nfrom astropy.coordinates import Angle\nfrom gammapy.datasets.map import MapEvaluator\nfrom gammapy.maps import Map, WcsGeom\nfrom gammapy.modeling.models import PointSpatialModel, PowerLawSpectralModel, SkyModel\nfrom gammapy.stats import (\n    amplitude_bounds_cython,\n    cash,\n    cash_sum_cython,\n    f_cash_root_cython,\n    x_best_leastsq,\n)\nfrom gammapy.utils.array import shape_2N, symmetric_crop_pad_width\n\n__all__ = [""TSMapEstimator""]\n\nlog = logging.getLogger(__name__)\n\nFLUX_FACTOR = 1e-12\nMAX_NITER = 20\nRTOL = 1e-3\n\n\ndef round_up_to_odd(f):\n    return int(np.ceil(f) // 2 * 2 + 1)\n\n\ndef _extract_array(array, shape, position):\n    """"""Helper function to extract parts of a larger array.\n\n    Simple implementation of an array extract function , because\n    `~astropy.ndata.utils.extract_array` introduces too much overhead.`\n\n    Parameters\n    ----------\n    array : `~numpy.ndarray`\n        The array from which to extract.\n    shape : tuple or int\n        The shape of the extracted array.\n    position : tuple of numbers or number\n        The position of the small array\'s center with respect to the\n        large array.\n    """"""\n    x_width = shape[1] // 2\n    y_width = shape[0] // 2\n    y_lo = position[0] - y_width\n    y_hi = position[0] + y_width + 1\n    x_lo = position[1] - x_width\n    x_hi = position[1] + x_width + 1\n    return array[y_lo:y_hi, x_lo:x_hi]\n\n\ndef f_cash(x, counts, background, model):\n    """"""Wrapper for cash statistics, that defines the model function.\n\n    Parameters\n    ----------\n    x : float\n        Model amplitude.\n    counts : `~numpy.ndarray`\n        Count image slice, where model is defined.\n    background : `~numpy.ndarray`\n        Background image slice, where model is defined.\n    model : `~numpy.ndarray`\n        Source template (multiplied with exposure).\n    """"""\n    return cash_sum_cython(\n        counts.ravel(), (background + x * FLUX_FACTOR * model).ravel()\n    )\n\n\nclass TSMapEstimator:\n    r""""""Compute TS map from a MapDataset using different optimization methods.\n\n    The map is computed fitting by a single parameter amplitude fit. The fit is\n    simplified by finding roots of the the derivative of the fit statistics using\n    various root finding algorithms. The approach is described in Appendix A\n    in Stewart (2009).\n\n    Parameters\n    ----------\n    model : `~gammapy.modeling.model.SkyModel`\n        Source model kernel. If set to None, assume point source model, PointSpatialModel.\n    kernel_width : `~astropy.coordinates.Angle`\n        Width of the kernel to use: the kernel will be truncated at this size\n    downsampling_factor : int\n        Sample down the input maps to speed up the computation. Only integer\n        values that are a multiple of 2 are allowed. Note that the kernel is\n        not sampled down, but must be provided with the downsampled bin size.\n    method : str (\'root\')\n        The following options are available:\n\n        * ``\'root brentq\'`` (default)\n            Fit amplitude by finding the roots of the the derivative of the fit\n            statistics using the brentq method.\n        * ``\'root newton\'``\n            Fit amplitude by finding the roots of the the derivative of the fit\n            statistics using Newton\'s method.\n        * ``\'leastsq iter\'``\n            Fit the amplitude by an iterative least square fit, that can be solved\n            analytically.\n    error_method : [\'covar\', \'conf\']\n        Error estimation method.\n    error_sigma : int (1)\n        Sigma for flux error.\n    ul_method : [\'covar\', \'conf\']\n        Upper limit estimation method.\n    ul_sigma : int (2)\n        Sigma for flux upper limits.\n    threshold : float (None)\n        If the TS value corresponding to the initial flux estimate is not above\n        this threshold, the optimizing step is omitted to save computing time.\n    rtol : float (0.001)\n        Relative precision of the flux estimate. Used as a stopping criterion for\n        the amplitude fit.\n\n    Notes\n    -----\n    Negative :math:`TS` values are defined as following:\n\n    .. math::\n        TS = \\left \\{\n                 \\begin{array}{ll}\n                   -TS \\text{ if } F < 0 \\\\\n                    TS \\text{ else}\n                 \\end{array}\n               \\right.\n\n    Where :math:`F` is the fitted flux amplitude.\n\n    References\n    ----------\n    [Stewart2009]_\n    """"""\n\n    def __init__(\n        self,\n        model=None,\n        kernel_width=""0.2 deg"",\n        downsampling_factor=None,\n        method=""root brentq"",\n        error_method=""covar"",\n        error_sigma=1,\n        ul_method=""covar"",\n        ul_sigma=2,\n        threshold=None,\n        rtol=0.001,\n    ):\n        if method not in [""root brentq"", ""root newton"", ""leastsq iter""]:\n            raise ValueError(f""Not a valid method: \'{method}\'"")\n\n        if error_method not in [""covar"", ""conf""]:\n            raise ValueError(f""Not a valid error method \'{error_method}\'"")\n\n        self.kernel_width = Angle(kernel_width)\n\n        if model is None:\n            model = SkyModel(\n                spectral_model=PowerLawSpectralModel(),\n                spatial_model=PointSpatialModel(),\n            )\n\n        self.model = model\n        self.downsampling_factor = downsampling_factor\n\n        self.parameters = {\n            ""method"": method,\n            ""error_method"": error_method,\n            ""error_sigma"": error_sigma,\n            ""ul_method"": ul_method,\n            ""ul_sigma"": ul_sigma,\n            ""threshold"": threshold,\n            ""rtol"": rtol,\n        }\n\n    def get_kernel(self, dataset):\n        """"""Set the convolution kernel for the input dataset.\n\n        Convolves the model with the PSFKernel at the center of the dataset.\n        If no PSFMap or PSFKernel is found the dataset, the model is used without convolution.\n        """"""\n        # TODO: further simplify the code below\n        geom = dataset.counts.geom\n\n        if self.downsampling_factor:\n            geom = geom.downsample(self.downsampling_factor)\n\n        model = self.model.copy()\n        model.spatial_model.position = geom.center_skydir\n\n        binsz = np.mean(geom.pixel_scales)\n        width_pix = self.kernel_width / binsz\n\n        npix = round_up_to_odd(width_pix.to_value(""""))\n\n        axis = dataset.exposure.geom.get_axis_by_name(""energy_true"")\n\n        geom = WcsGeom.create(\n            skydir=model.position, proj=""TAN"", npix=npix, axes=[axis], binsz=binsz\n        )\n\n        exposure = Map.from_geom(geom, unit=""cm2 s1"")\n        exposure.data += 1.0\n\n        # We use global evaluation mode to not modify the geometry\n        evaluator = MapEvaluator(model, evaluation_mode=""global"")\n        evaluator.update(exposure, dataset.psf, dataset.edisp, dataset.counts.geom)\n\n        kernel = evaluator.compute_npred().sum_over_axes()\n        kernel.data /= kernel.data.sum()\n\n        if (self.kernel_width > geom.width).any():\n            raise ValueError(\n                ""Kernel shape larger than map shape, please adjust""\n                "" size of the kernel""\n            )\n        return kernel\n\n    @staticmethod\n    def flux_default(dataset, kernel):\n        """"""Estimate default flux map using a given kernel.\n\n        Parameters\n        ----------\n        dataset : `~gammapy.cube.MapDataset`\n            Input dataset.\n        kernel : `~numpy.ndarray`\n            Source model kernel.\n\n        Returns\n        -------\n        flux_approx : `~gammapy.maps.WcsNDMap`\n            Approximate flux map (2D).\n        """"""\n        flux = dataset.counts - dataset.npred()\n        flux = flux.sum_over_axes(keepdims=False)\n        flux /= dataset.exposure.sum_over_axes(keepdims=False)\n        flux /= np.sum(kernel ** 2)\n        return flux.convolve(kernel)\n\n    @staticmethod\n    def mask_default(exposure, background, kernel):\n        """"""Compute default mask where to estimate TS values.\n\n        Parameters\n        ----------\n        exposure : `~gammapy.maps.Map`\n            Input exposure map.\n        background : `~gammapy.maps.Map`\n            Input background map.\n        kernel : `~numpy.ndarray`\n            Source model kernel.\n\n        Returns\n        -------\n        mask : `gammapy.maps.WcsNDMap`\n            Mask map.\n        """"""\n        mask = np.zeros(exposure.data.shape, dtype=int)\n\n        # mask boundary\n        slice_x = slice(kernel.shape[1] // 2, -kernel.shape[1] // 2 + 1)\n        slice_y = slice(kernel.shape[0] // 2, -kernel.shape[0] // 2 + 1)\n        mask[slice_y, slice_x] = 1\n\n        # positions where exposure == 0 are not processed\n        mask &= exposure.data > 0\n\n        # in some image there are pixels, which have exposure, but zero\n        # background, which doesn\'t make sense and causes the TS computation\n        # to fail, this is a temporary fix\n        mask[background.data == 0] = 0\n\n        return exposure.copy(data=mask.astype(""int""), unit="""")\n\n    @staticmethod\n    def sqrt_ts(map_ts):\n        r""""""Compute sqrt(TS) map.\n\n        Compute sqrt(TS) as defined by:\n\n        .. math::\n            \\sqrt{TS} = \\left \\{\n            \\begin{array}{ll}\n              -\\sqrt{-TS} & : \\text{if} \\ TS < 0 \\\\\n              \\sqrt{TS} & : \\text{else}\n            \\end{array}\n            \\right.\n\n        Parameters\n        ----------\n        map_ts : `gammapy.maps.WcsNDMap`\n            Input TS map.\n\n        Returns\n        -------\n        sqrt_ts : `gammapy.maps.WcsNDMap`\n            Sqrt(TS) map.\n        """"""\n        with np.errstate(invalid=""ignore"", divide=""ignore""):\n            ts = map_ts.data\n            sqrt_ts = np.where(ts > 0, np.sqrt(ts), -np.sqrt(-ts))\n        return map_ts.copy(data=sqrt_ts)\n\n    def run(self, dataset, steps=""all""):\n        """"""\n        Run TS map estimation.\n\n        Requires a MapDataset with counts, exposure and background_model\n        properly set to run.\n\n        Parameters\n        ----------\n        dataset : `~gammapy.datasets.MapDataset`\n            Input MapDataset.\n        steps : list of str or \'all\'\n            Which maps to compute. Available options are:\n\n                * ""ts"": estimate delta TS and significance (sqrt_ts)\n                * ""flux-err"": estimate symmetric error on flux.\n                * ""flux-ul"": estimate upper limits on flux.\n\n            By default all steps are executed.\n\n        Returns\n        -------\n        maps : dict\n             Dictionary containing result maps. Keys are:\n\n                * ts : delta TS map\n                * sqrt_ts : sqrt(delta TS), or significance map\n                * flux : flux map\n                * flux_err : symmetric error map\n                * flux_ul : upper limit map\n\n        """"""\n        p = self.parameters\n\n        # First create 2D map arrays\n        counts = dataset.counts.sum_over_axes(keepdims=False)\n        background = dataset.npred().sum_over_axes(keepdims=False)\n        exposure = dataset.exposure.sum_over_axes(keepdims=False)\n\n        kernel = self.get_kernel(dataset)\n\n        if dataset.mask is not None:\n            mask = counts.copy(data=(dataset.mask.sum(axis=0) > 0).astype(""int""))\n        else:\n            mask = counts.copy(data=np.ones_like(counts).astype(""int""))\n\n        if self.downsampling_factor:\n            shape = counts.data.shape\n            pad_width = symmetric_crop_pad_width(shape, shape_2N(shape))[0]\n\n            counts = counts.pad(pad_width).downsample(\n                self.downsampling_factor, preserve_counts=True\n            )\n            background = background.pad(pad_width).downsample(\n                self.downsampling_factor, preserve_counts=True\n            )\n            exposure = exposure.pad(pad_width).downsample(\n                self.downsampling_factor, preserve_counts=False\n            )\n            mask = mask.pad(pad_width).downsample(\n                self.downsampling_factor, preserve_counts=False\n            )\n            mask.data = mask.data.astype(""int"")\n\n        mask.data &= self.mask_default(exposure, background, kernel.data).data\n\n        if steps == ""all"":\n            steps = [""ts"", ""sqrt_ts"", ""flux"", ""flux_err"", ""flux_ul"", ""niter""]\n\n        result = {}\n        for name in steps:\n            data = np.nan * np.ones_like(counts.data)\n            result[name] = counts.copy(data=data)\n\n        flux_map = self.flux_default(dataset, kernel.data)\n\n        if p[""threshold""] or p[""method""] == ""root newton"":\n            flux = flux_map.data\n        else:\n            flux = None\n\n        # prepare dtype for cython methods\n        counts_array = counts.data.astype(float)\n        background_array = background.data.astype(float)\n        exposure_array = exposure.data.astype(float)\n\n        # Compute null statistics per pixel for the whole image\n        c_0 = cash(counts_array, background_array)\n\n        error_method = p[""error_method""] if ""flux_err"" in steps else ""none""\n        ul_method = p[""ul_method""] if ""flux_ul"" in steps else ""none""\n\n        wrap = functools.partial(\n            _ts_value,\n            counts=counts_array,\n            exposure=exposure_array,\n            background=background_array,\n            c_0=c_0,\n            kernel=kernel.data,\n            flux=flux,\n            method=p[""method""],\n            error_method=error_method,\n            threshold=p[""threshold""],\n            error_sigma=p[""error_sigma""],\n            ul_method=ul_method,\n            ul_sigma=p[""ul_sigma""],\n            rtol=p[""rtol""],\n        )\n\n        x, y = np.where(np.squeeze(mask.data))\n        positions = list(zip(x, y))\n        results = list(map(wrap, positions))\n\n        # Set TS values at given positions\n        j, i = zip(*positions)\n        for name in [""ts"", ""flux"", ""niter""]:\n            result[name].data[j, i] = [_[name] for _ in results]\n\n        if ""flux_err"" in steps:\n            result[""flux_err""].data[j, i] = [_[""flux_err""] for _ in results]\n\n        if ""flux_ul"" in steps:\n            result[""flux_ul""].data[j, i] = [_[""flux_ul""] for _ in results]\n\n        # Compute sqrt(TS) values\n        if ""sqrt_ts"" in steps:\n            result[""sqrt_ts""] = self.sqrt_ts(result[""ts""])\n\n        if self.downsampling_factor:\n            for name in steps:\n                order = 0 if name == ""niter"" else 1\n                result[name] = result[name].upsample(\n                    factor=self.downsampling_factor, preserve_counts=False, order=order\n                )\n                result[name] = result[name].crop(crop_width=pad_width)\n\n        # Set correct units\n        if ""flux"" in steps:\n            result[""flux""].unit = flux_map.unit\n        if ""flux_err"" in steps:\n            result[""flux_err""].unit = flux_map.unit\n        if ""flux_ul"" in steps:\n            result[""flux_ul""].unit = flux_map.unit\n\n        return result\n\n    def __repr__(self):\n        p = self.parameters\n        info = self.__class__.__name__\n        info += ""\\n\\nParameters:\\n\\n""\n        for key in p:\n            info += f""\\t{key:13s}: {p[key]}\\n""\n        return info\n\n\ndef _ts_value(\n    position,\n    counts,\n    exposure,\n    background,\n    c_0,\n    kernel,\n    flux,\n    method,\n    error_method,\n    error_sigma,\n    ul_method,\n    ul_sigma,\n    threshold,\n    rtol,\n):\n    """"""Compute TS value at a given pixel position.\n\n    Uses approach described in Stewart (2009).\n\n    Parameters\n    ----------\n    position : tuple (i, j)\n        Pixel position.\n    counts : `~numpy.ndarray`\n        Counts image\n    background : `~numpy.ndarray`\n        Background image\n    exposure : `~numpy.ndarray`\n        Exposure image\n    kernel : `astropy.convolution.Kernel2D`\n        Source model kernel\n    flux : `~numpy.ndarray`\n        Flux image. The flux value at the given pixel position is used as\n        starting value for the minimization.\n\n    Returns\n    -------\n    TS : float\n        TS value at the given pixel position.\n    """"""\n    # Get data slices\n    counts_ = _extract_array(counts, kernel.shape, position)\n    background_ = _extract_array(background, kernel.shape, position)\n    exposure_ = _extract_array(exposure, kernel.shape, position)\n    c_0_ = _extract_array(c_0, kernel.shape, position)\n\n    model = exposure_ * kernel\n\n    c_0 = c_0_.sum()\n\n    if threshold is not None:\n        with np.errstate(invalid=""ignore"", divide=""ignore""):\n            amplitude = flux[position]\n            c_1 = f_cash(amplitude / FLUX_FACTOR, counts_, background_, model)\n        # Don\'t fit if pixel significance is low\n        if c_0 - c_1 < threshold:\n            result = {}\n            result[""ts""] = (c_0 - c_1) * np.sign(amplitude)\n            result[""flux""] = amplitude\n            result[""niter""] = 0\n            result[""flux_err""] = np.nan\n            result[""flux_ul""] = np.nan\n            return result\n\n    if method == ""root brentq"":\n        amplitude, niter = _root_amplitude_brentq(\n            counts_, background_, model, rtol=rtol\n        )\n    elif method == ""root newton"":\n        amplitude, niter = _root_amplitude(\n            counts_, background_, model, flux[position], rtol=rtol\n        )\n    elif method == ""leastsq iter"":\n        amplitude, niter = _leastsq_iter_amplitude(\n            counts_, background_, model, rtol=rtol\n        )\n    else:\n        raise ValueError(f""Invalid method: {method}"")\n\n    with np.errstate(invalid=""ignore"", divide=""ignore""):\n        c_1 = f_cash(amplitude, counts_, background_, model)\n\n    result = {}\n    result[""ts""] = (c_0 - c_1) * np.sign(amplitude)\n    result[""flux""] = amplitude * FLUX_FACTOR\n    result[""niter""] = niter\n\n    if error_method == ""covar"":\n        flux_err = _compute_flux_err_covar(amplitude, counts_, background_, model)\n        result[""flux_err""] = flux_err * error_sigma\n    elif error_method == ""conf"":\n        flux_err = _compute_flux_err_conf(\n            amplitude, counts_, background_, model, c_1, error_sigma\n        )\n        result[""flux_err""] = FLUX_FACTOR * flux_err\n\n    if ul_method == ""covar"":\n        result[""flux_ul""] = result[""flux""] + ul_sigma * result[""flux_err""]\n    elif ul_method == ""conf"":\n        flux_ul = _compute_flux_err_conf(\n            amplitude, counts_, background_, model, c_1, ul_sigma\n        )\n        result[""flux_ul""] = FLUX_FACTOR * flux_ul + result[""flux""]\n    return result\n\n\ndef _leastsq_iter_amplitude(counts, background, model, maxiter=MAX_NITER, rtol=RTOL):\n    """"""Fit amplitude using an iterative least squares algorithm.\n\n    Parameters\n    ----------\n    counts : `~numpy.ndarray`\n        Slice of counts image\n    background : `~numpy.ndarray`\n        Slice of background image\n    model : `~numpy.ndarray`\n        Model template to fit.\n    maxiter : int\n        Maximum number of iterations.\n    rtol : float\n        Relative flux error.\n\n    Returns\n    -------\n    amplitude : float\n        Fitted flux amplitude.\n    niter : int\n        Number of function evaluations needed for the fit.\n    """"""\n    bounds = amplitude_bounds_cython(counts, background, model)\n    amplitude_min, amplitude_max, amplitude_min_total = bounds\n\n    if not counts.sum() > 0:\n        return amplitude_min_total, 0\n\n    weights = np.ones(model.shape)\n\n    x_old = 0\n    for i in range(maxiter):\n        x = x_best_leastsq(counts, background, model, weights)\n        if abs((x - x_old) / x) < rtol:\n            return max(x / FLUX_FACTOR, amplitude_min_total), i + 1\n        else:\n            weights = x * model + background\n            x_old = x\n    return max(x / FLUX_FACTOR, amplitude_min_total), MAX_NITER\n\n\ndef _root_amplitude(counts, background, model, flux, rtol=RTOL):\n    """"""Fit amplitude by finding roots using newton algorithm.\n\n    See Appendix A Stewart (2009).\n\n    Parameters\n    ----------\n    counts : `~numpy.ndarray`\n        Slice of count image\n    background : `~numpy.ndarray`\n        Slice of background image\n    model : `~numpy.ndarray`\n        Model template to fit.\n    flux : float\n        Starting value for the fit.\n\n    Returns\n    -------\n    amplitude : float\n        Fitted flux amplitude.\n    niter : int\n        Number of function evaluations needed for the fit.\n    """"""\n    args = (counts, background, model)\n    with warnings.catch_warnings():\n        warnings.simplefilter(""ignore"")\n        try:\n            return (\n                scipy.optimize.newton(\n                    f_cash_root_cython, flux, args=args, maxiter=MAX_NITER, tol=rtol\n                ),\n                0,\n            )\n        except RuntimeError:\n            # Where the root finding fails NaN is set as amplitude\n            return np.nan, MAX_NITER\n\n\ndef _root_amplitude_brentq(counts, background, model, rtol=RTOL):\n    """"""Fit amplitude by finding roots using Brent algorithm.\n\n    See Appendix A Stewart (2009).\n\n    Parameters\n    ----------\n    counts : `~numpy.ndarray`\n        Slice of count image\n    background : `~numpy.ndarray`\n        Slice of background image\n    model : `~numpy.ndarray`\n        Model template to fit.\n\n    Returns\n    -------\n    amplitude : float\n        Fitted flux amplitude.\n    niter : int\n        Number of function evaluations needed for the fit.\n    """"""\n    # Compute amplitude bounds and assert counts > 0\n    bounds = amplitude_bounds_cython(counts, background, model)\n    amplitude_min, amplitude_max, amplitude_min_total = bounds\n\n    if not counts.sum() > 0:\n        return amplitude_min_total, 0\n\n    args = (counts, background, model)\n    with warnings.catch_warnings():\n        warnings.simplefilter(""ignore"")\n        try:\n            result = scipy.optimize.brentq(\n                f_cash_root_cython,\n                amplitude_min,\n                amplitude_max,\n                args=args,\n                maxiter=MAX_NITER,\n                full_output=True,\n                rtol=rtol,\n            )\n            return max(result[0], amplitude_min_total), result[1].iterations\n        except (RuntimeError, ValueError):\n            # Where the root finding fails NaN is set as amplitude\n            return np.nan, MAX_NITER\n\n\ndef _compute_flux_err_covar(x, counts, background, model):\n    """"""\n    Compute amplitude errors using inverse 2nd derivative method.\n    """"""\n    with np.errstate(invalid=""ignore"", divide=""ignore""):\n        stat = (model ** 2 * counts) / (background + x * FLUX_FACTOR * model) ** 2\n        return np.sqrt(1.0 / stat.sum())\n\n\ndef _compute_flux_err_conf(amplitude, counts, background, model, c_1, error_sigma):\n    """"""\n    Compute amplitude errors using likelihood profile method.\n    """"""\n\n    def ts_diff(x, counts, background, model):\n        return (c_1 + error_sigma ** 2) - f_cash(x, counts, background, model)\n\n    args = (counts, background, model)\n\n    amplitude_max = amplitude + 1e4\n    with warnings.catch_warnings():\n        warnings.simplefilter(""ignore"")\n        try:\n            result = scipy.optimize.brentq(\n                ts_diff,\n                amplitude,\n                amplitude_max,\n                args=args,\n                maxiter=MAX_NITER,\n                rtol=1e-3,\n            )\n            return result - amplitude\n        except (RuntimeError, ValueError):\n            # Where the root finding fails NaN is set as amplitude\n            return np.nan\n'"
gammapy/estimators/utils.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nimport scipy.ndimage\nfrom astropy.coordinates import SkyCoord\nfrom astropy.table import Table\nfrom gammapy.maps import WcsNDMap\n\n__all__ = [""find_peaks""]\n\n\ndef find_peaks(image, threshold, min_distance=1):\n    """"""Find local peaks in an image.\n\n    This is a very simple peak finder, that finds local peaks\n    (i.e. maxima) in images above a given ``threshold`` within\n    a given ``min_distance`` around each given pixel.\n\n    If you get multiple spurious detections near a peak, usually\n    it\'s best to smooth the image a bit, or to compute it using\n    a different method in the first place to result in a smooth image.\n    You can also increase the ``min_distance`` parameter.\n\n    The output table contains one row per peak and the following columns:\n\n    - ``x`` and ``y`` are the pixel coordinates (first pixel at zero)\n    - ``ra`` and ``dec`` are the RA / DEC sky coordinates (ICRS frame)\n    - ``value`` is the pixel value\n\n    It is sorted by peak value, starting with the highest value.\n\n    If there are no pixel values above the threshold, an empty table is returned.\n\n    There are more featureful peak finding and source detection methods\n    e.g. in the ``photutils`` or ``scikit-image`` Python packages.\n\n    Parameters\n    ----------\n    image : `~gammapy.maps.WcsNDMap`\n        2D map\n    threshold : float or array-like\n        The data value or pixel-wise data values to be used for the\n        detection threshold.  A 2D ``threshold`` must have the same\n        shape as tha map ``data``.\n    min_distance : int\n        Minimum pixel distance between peaks.\n        Smallest possible value and default is 1 pixel.\n\n    Returns\n    -------\n    output : `~astropy.table.Table`\n        Table with parameters of detected peaks\n    """"""\n    # Input validation\n\n    if not isinstance(image, WcsNDMap):\n        raise TypeError(""find_peaks only supports WcsNDMap"")\n\n    if not image.geom.is_image:\n        raise ValueError(""find_peaks only supports 2D images"")\n\n    size = 2 * min_distance + 1\n\n    # Remove non-finite values to avoid warnings or spurious detection\n    data = image.data.copy()\n    data[~np.isfinite(data)] = np.nanmin(data)\n\n    # Handle edge case of constant data; treat as no peak\n    if np.all(data == data.flat[0]):\n        return Table()\n\n    # Run peak finder\n    data_max = scipy.ndimage.maximum_filter(data, size=size, mode=""constant"")\n    mask = (data == data_max) & (data > threshold)\n    y, x = mask.nonzero()\n    value = data[y, x]\n\n    # Make and return results table\n\n    if len(value) == 0:\n        return Table()\n\n    coord = SkyCoord.from_pixel(x, y, wcs=image.geom.wcs).icrs\n\n    table = Table()\n    table[""value""] = value * image.unit\n    table[""x""] = x\n    table[""y""] = y\n    table[""ra""] = coord.ra\n    table[""dec""] = coord.dec\n\n    table[""ra""].format = "".5f""\n    table[""dec""].format = "".5f""\n    table[""value""].format = "".5g""\n\n    table.sort(""value"")\n    table.reverse()\n\n    return table\n'"
gammapy/extern/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""\nThis package contains `extern` code, i.e. code that we just copied\nhere into the `gammapy.extern` package, because we wanted to use it,\nbut not have an extra dependency (these are single-file external packages).\n\nAlphabetical order:\n\n* ``xmltodict.py`` for easily converting XML from / to Python dicts\n  Origin: https://github.com/martinblech/xmltodict/blob/master/xmltodict.py\n* ``zeros.py`` - Modified copy of the file from ``scipy.optimize``,\n  used by the TS map computation code.\n* ``skimage.py`` - utility functions copied from scikit-image\n  They have the same BSD license as we do.\n""""""\n'"
gammapy/extern/skimage.py,11,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Functions copied from scikit-image (git master on August 7, 2018)""""""\nfrom warnings import warn\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\n\ndef view_as_blocks(arr_in, block_shape):\n    """"""Block view of the input n-dimensional array (using re-striding).\n\n    Blocks are non-overlapping views of the input array.\n\n    Parameters\n    ----------\n    arr_in : ndarray\n        N-d input array.\n    block_shape : tuple\n        The shape of the block. Each dimension must divide evenly into the\n        corresponding dimensions of `arr_in`.\n\n    Returns\n    -------\n    arr_out : ndarray\n        Block view of the input array.  If `arr_in` is non-contiguous, a copy\n        is made.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from skimage.util.shape import view_as_blocks\n    >>> A = np.arange(4*4).reshape(4,4)\n    >>> A\n    array([[ 0,  1,  2,  3],\n           [ 4,  5,  6,  7],\n           [ 8,  9, 10, 11],\n           [12, 13, 14, 15]])\n    >>> B = view_as_blocks(A, block_shape=(2, 2))\n    >>> B[0, 0]\n    array([[0, 1],\n           [4, 5]])\n    >>> B[0, 1]\n    array([[2, 3],\n           [6, 7]])\n    >>> B[1, 0, 1, 1]\n    13\n\n    >>> A = np.arange(4*4*6).reshape(4,4,6)\n    >>> A  # doctest: +NORMALIZE_WHITESPACE\n    array([[[ 0,  1,  2,  3,  4,  5],\n            [ 6,  7,  8,  9, 10, 11],\n            [12, 13, 14, 15, 16, 17],\n            [18, 19, 20, 21, 22, 23]],\n           [[24, 25, 26, 27, 28, 29],\n            [30, 31, 32, 33, 34, 35],\n            [36, 37, 38, 39, 40, 41],\n            [42, 43, 44, 45, 46, 47]],\n           [[48, 49, 50, 51, 52, 53],\n            [54, 55, 56, 57, 58, 59],\n            [60, 61, 62, 63, 64, 65],\n            [66, 67, 68, 69, 70, 71]],\n           [[72, 73, 74, 75, 76, 77],\n            [78, 79, 80, 81, 82, 83],\n            [84, 85, 86, 87, 88, 89],\n            [90, 91, 92, 93, 94, 95]]])\n    >>> B = view_as_blocks(A, block_shape=(1, 2, 2))\n    >>> B.shape\n    (4, 2, 3, 1, 2, 2)\n    >>> B[2:, 0, 2]  # doctest: +NORMALIZE_WHITESPACE\n    array([[[[52, 53],\n             [58, 59]]],\n           [[[76, 77],\n             [82, 83]]]])\n    """"""\n    if not isinstance(block_shape, tuple):\n        raise TypeError(\'block needs to be a tuple\')\n\n    block_shape = np.array(block_shape)\n    if (block_shape <= 0).any():\n        raise ValueError(""\'block_shape\' elements must be strictly positive"")\n\n    if block_shape.size != arr_in.ndim:\n        raise ValueError(""\'block_shape\' must have the same length ""\n                         ""as \'arr_in.shape\'"")\n\n    arr_shape = np.array(arr_in.shape)\n    if (arr_shape % block_shape).sum() != 0:\n        raise ValueError(""\'block_shape\' is not compatible with \'arr_in\'"")\n\n    # -- restride the array to build the block view\n\n    if not arr_in.flags.contiguous:\n        warn(RuntimeWarning(""Cannot provide views on a non-contiguous input ""\n                            ""array without copying.""))\n\n    arr_in = np.ascontiguousarray(arr_in)\n\n    new_shape = tuple(arr_shape // block_shape) + tuple(block_shape)\n    new_strides = tuple(arr_in.strides * block_shape) + arr_in.strides\n\n    arr_out = as_strided(arr_in, shape=new_shape, strides=new_strides)\n\n    return arr_out\n\n\ndef block_reduce(image, block_size, func=np.sum, cval=0):\n    """"""Down-sample image by applying function to local blocks.\n    Parameters\n    ----------\n    image : ndarray\n        N-dimensional input image.\n    block_size : array_like\n        Array containing down-sampling integer factor along each axis.\n    func : callable\n        Function object which is used to calculate the return value for each\n        local block. This function must implement an ``axis`` parameter such\n        as ``numpy.sum`` or ``numpy.min``.\n    cval : float\n        Constant padding value if image is not perfectly divisible by the\n        block size.\n    Returns\n    -------\n    image : ndarray\n        Down-sampled image with same number of dimensions as input image.\n    Examples\n    --------\n    >>> from skimage.measure import block_reduce\n    >>> image = np.arange(3*3*4).reshape(3, 3, 4)\n    >>> image # doctest: +NORMALIZE_WHITESPACE\n    array([[[ 0,  1,  2,  3],\n            [ 4,  5,  6,  7],\n            [ 8,  9, 10, 11]],\n           [[12, 13, 14, 15],\n            [16, 17, 18, 19],\n            [20, 21, 22, 23]],\n           [[24, 25, 26, 27],\n            [28, 29, 30, 31],\n            [32, 33, 34, 35]]])\n    >>> block_reduce(image, block_size=(3, 3, 1), func=np.mean)\n    array([[[ 16.,  17.,  18.,  19.]]])\n    >>> image_max1 = block_reduce(image, block_size=(1, 3, 4), func=np.max)\n    >>> image_max1 # doctest: +NORMALIZE_WHITESPACE\n    array([[[11]],\n           [[23]],\n           [[35]]])\n    >>> image_max2 = block_reduce(image, block_size=(3, 1, 4), func=np.max)\n    >>> image_max2 # doctest: +NORMALIZE_WHITESPACE\n    array([[[27],\n            [31],\n            [35]]])\n    """"""\n\n    if len(block_size) != image.ndim:\n        raise ValueError(""`block_size` must have the same length ""\n                         ""as `image.shape`."")\n\n    pad_width = []\n    for i in range(len(block_size)):\n        if block_size[i] < 1:\n            raise ValueError(""Down-sampling factors must be >= 1. Use ""\n                             ""`skimage.transform.resize` to up-sample an ""\n                             ""image."")\n        if image.shape[i] % block_size[i] != 0:\n            after_width = block_size[i] - (image.shape[i] % block_size[i])\n        else:\n            after_width = 0\n        pad_width.append((0, after_width))\n\n    image = np.pad(image, pad_width=pad_width, mode=\'constant\',\n                   constant_values=cval)\n\n    out = view_as_blocks(image, block_size)\n\n    for i in range(len(out.shape) // 2):\n        out = func(out, axis=-1)\n\n    return out\n'"
gammapy/extern/xmltodict.py,0,"b'#!/usr/bin/env python\n""Makes working with XML feel like you are working with JSON""\n\nfrom xml.parsers import expat\nfrom xml.sax.saxutils import XMLGenerator\nfrom xml.sax.xmlreader import AttributesImpl\n\ntry:  # pragma no cover\n    from cStringIO import StringIO\nexcept ImportError:  # pragma no cover\n    try:\n        from StringIO import StringIO\n    except ImportError:\n        from io import StringIO\ntry:  # pragma no cover\n    from collections import OrderedDict\nexcept ImportError:  # pragma no cover\n    try:\n        from ordereddict import OrderedDict\n    except ImportError:\n        OrderedDict = dict\n\ntry:  # pragma no cover\n    _basestring = basestring\nexcept NameError:  # pragma no cover\n    _basestring = str\ntry:  # pragma no cover\n    _unicode = unicode\nexcept NameError:  # pragma no cover\n    _unicode = str\n\n__author__ = \'Martin Blech\'\n__version__ = \'0.9.0\'\n__license__ = \'MIT\'\n\n\nclass ParsingInterrupted(Exception):\n    pass\n\n\nclass _DictSAXHandler(object):\n    def __init__(self,\n                 item_depth=0,\n                 item_callback=lambda *args: True,\n                 xml_attribs=True,\n                 attr_prefix=\'@\',\n                 cdata_key=\'#text\',\n                 force_cdata=False,\n                 cdata_separator=\'\',\n                 postprocessor=None,\n                 dict_constructor=OrderedDict,\n                 strip_whitespace=True,\n                 namespace_separator=\':\',\n                 namespaces=None):\n        self.path = []\n        self.stack = []\n        self.data = None\n        self.item = None\n        self.item_depth = item_depth\n        self.xml_attribs = xml_attribs\n        self.item_callback = item_callback\n        self.attr_prefix = attr_prefix\n        self.cdata_key = cdata_key\n        self.force_cdata = force_cdata\n        self.cdata_separator = cdata_separator\n        self.postprocessor = postprocessor\n        self.dict_constructor = dict_constructor\n        self.strip_whitespace = strip_whitespace\n        self.namespace_separator = namespace_separator\n        self.namespaces = namespaces\n\n    def _build_name(self, full_name):\n        if not self.namespaces:\n            return full_name\n        i = full_name.rfind(self.namespace_separator)\n        if i == -1:\n            return full_name\n        namespace, name = full_name[:i], full_name[i+1:]\n        short_namespace = self.namespaces.get(namespace, namespace)\n        if not short_namespace:\n            return name\n        else:\n            return self.namespace_separator.join((short_namespace, name))\n\n    def _attrs_to_dict(self, attrs):\n        if isinstance(attrs, dict):\n            return attrs\n        return self.dict_constructor(zip(attrs[0::2], attrs[1::2]))\n\n    def startElement(self, full_name, attrs):\n        name = self._build_name(full_name)\n        attrs = self._attrs_to_dict(attrs)\n        self.path.append((name, attrs or None))\n        if len(self.path) > self.item_depth:\n            self.stack.append((self.item, self.data))\n            if self.xml_attribs:\n                attrs = self.dict_constructor(\n                    (self.attr_prefix+key, value)\n                    for (key, value) in attrs.items())\n            else:\n                attrs = None\n            self.item = attrs or None\n            self.data = None\n\n    def endElement(self, full_name):\n        name = self._build_name(full_name)\n        if len(self.path) == self.item_depth:\n            item = self.item\n            if item is None:\n                item = self.data\n            should_continue = self.item_callback(self.path, item)\n            if not should_continue:\n                raise ParsingInterrupted()\n        if len(self.stack):\n            item, data = self.item, self.data\n            self.item, self.data = self.stack.pop()\n            if self.strip_whitespace and data is not None:\n                data = data.strip() or None\n            if data and self.force_cdata and item is None:\n                item = self.dict_constructor()\n            if item is not None:\n                if data:\n                    self.push_data(item, self.cdata_key, data)\n                self.item = self.push_data(self.item, name, item)\n            else:\n                self.item = self.push_data(self.item, name, data)\n        else:\n            self.item = self.data = None\n        self.path.pop()\n\n    def characters(self, data):\n        if not self.data:\n            self.data = data\n        else:\n            self.data += self.cdata_separator + data\n\n    def push_data(self, item, key, data):\n        if self.postprocessor is not None:\n            result = self.postprocessor(self.path, key, data)\n            if result is None:\n                return item\n            key, data = result\n        if item is None:\n            item = self.dict_constructor()\n        try:\n            value = item[key]\n            if isinstance(value, list):\n                value.append(data)\n            else:\n                item[key] = [value, data]\n        except KeyError:\n            item[key] = data\n        return item\n\n\ndef parse(xml_input, encoding=None, expat=expat, process_namespaces=False,\n          namespace_separator=\':\', **kwargs):\n    """"""Parse the given XML input and convert it into a dictionary.\n\n    `xml_input` can either be a `string` or a file-like object.\n\n    If `xml_attribs` is `True`, element attributes are put in the dictionary\n    among regular child elements, using `@` as a prefix to avoid collisions. If\n    set to `False`, they are just ignored.\n\n    Simple example::\n\n        >>> import xmltodict\n        >>> doc = xmltodict.parse(\\""\\""\\""\n        ... <a prop=""x"">\n        ...   <b>1</b>\n        ...   <b>2</b>\n        ... </a>\n        ... \\""\\""\\"")\n        >>> doc[\'a\'][\'@prop\']\n        u\'x\'\n        >>> doc[\'a\'][\'b\']\n        [u\'1\', u\'2\']\n\n    If `item_depth` is `0`, the function returns a dictionary for the root\n    element (default behavior). Otherwise, it calls `item_callback` every time\n    an item at the specified depth is found and returns `None` in the end\n    (streaming mode).\n\n    The callback function receives two parameters: the `path` from the document\n    root to the item (name-attribs pairs), and the `item` (dict). If the\n    callback\'s return value is false-ish, parsing will be stopped with the\n    :class:`ParsingInterrupted` exception.\n\n    Streaming example::\n\n        >>> def handle(path, item):\n        ...     print \'path:%s item:%s\' % (path, item)\n        ...     return True\n        ...\n        >>> xmltodict.parse(\\""\\""\\""\n        ... <a prop=""x"">\n        ...   <b>1</b>\n        ...   <b>2</b>\n        ... </a>\\""\\""\\"", item_depth=2, item_callback=handle)\n        path:[(u\'a\', {u\'prop\': u\'x\'}), (u\'b\', None)] item:1\n        path:[(u\'a\', {u\'prop\': u\'x\'}), (u\'b\', None)] item:2\n\n    The optional argument `postprocessor` is a function that takes `path`,\n    `key` and `value` as positional arguments and returns a new `(key, value)`\n    pair where both `key` and `value` may have changed. Usage example::\n\n        >>> def postprocessor(path, key, value):\n        ...     try:\n        ...         return key + \':int\', int(value)\n        ...     except (ValueError, TypeError):\n        ...         return key, value\n        >>> xmltodict.parse(\'<a><b>1</b><b>2</b><b>x</b></a>\',\n        ...                 postprocessor=postprocessor)\n        OrderedDict([(u\'a\', OrderedDict([(u\'b:int\', [1, 2]), (u\'b\', u\'x\')]))])\n\n    You can pass an alternate version of `expat` (such as `defusedexpat`) by\n    using the `expat` parameter. E.g:\n\n        >>> import defusedexpat\n        >>> xmltodict.parse(\'<a>hello</a>\', expat=defusedexpat.pyexpat)\n        OrderedDict([(u\'a\', u\'hello\')])\n\n    """"""\n    handler = _DictSAXHandler(namespace_separator=namespace_separator,\n                              **kwargs)\n    if isinstance(xml_input, _unicode):\n        if not encoding:\n            encoding = \'utf-8\'\n        xml_input = xml_input.encode(encoding)\n    if not process_namespaces:\n        namespace_separator = None\n    parser = expat.ParserCreate(\n        encoding,\n        namespace_separator\n    )\n    try:\n        parser.ordered_attributes = True\n    except AttributeError:\n        # Jython\'s expat does not support ordered_attributes\n        pass\n    parser.StartElementHandler = handler.startElement\n    parser.EndElementHandler = handler.endElement\n    parser.CharacterDataHandler = handler.characters\n    parser.buffer_text = True\n    try:\n        parser.ParseFile(xml_input)\n    except (TypeError, AttributeError):\n        parser.Parse(xml_input, True)\n    return handler.item\n\n\ndef _emit(key, value, content_handler,\n          attr_prefix=\'@\',\n          cdata_key=\'#text\',\n          depth=0,\n          preprocessor=None,\n          pretty=False,\n          newl=\'\\n\',\n          indent=\'\\t\'):\n    if preprocessor is not None:\n        result = preprocessor(key, value)\n        if result is None:\n            return\n        key, value = result\n    if not isinstance(value, (list, tuple)):\n        value = [value]\n    if depth == 0 and len(value) > 1:\n        raise ValueError(\'document with multiple roots\')\n    for v in value:\n        if v is None:\n            v = OrderedDict()\n        elif not isinstance(v, dict):\n            v = _unicode(v)\n        if isinstance(v, _basestring):\n            v = OrderedDict(((cdata_key, v),))\n        cdata = None\n        attrs = OrderedDict()\n        children = []\n        for ik, iv in v.items():\n            if ik == cdata_key:\n                cdata = iv\n                continue\n            if ik.startswith(attr_prefix):\n                attrs[ik[len(attr_prefix):]] = iv\n                continue\n            children.append((ik, iv))\n        if pretty:\n            content_handler.ignorableWhitespace(depth * indent)\n        content_handler.startElement(key, AttributesImpl(attrs))\n        if pretty and children:\n            content_handler.ignorableWhitespace(newl)\n        for child_key, child_value in children:\n            _emit(child_key, child_value, content_handler,\n                  attr_prefix, cdata_key, depth+1, preprocessor,\n                  pretty, newl, indent)\n        if cdata is not None:\n            content_handler.characters(cdata)\n        if pretty and children:\n            content_handler.ignorableWhitespace(depth * indent)\n        content_handler.endElement(key)\n        if pretty and depth:\n            content_handler.ignorableWhitespace(newl)\n\n\ndef unparse(input_dict, output=None, encoding=\'utf-8\', full_document=True,\n            **kwargs):\n    """"""Emit an XML document for the given `input_dict` (reverse of `parse`).\n\n    The resulting XML document is returned as a string, but if `output` (a\n    file-like object) is specified, it is written there instead.\n\n    Dictionary keys prefixed with `attr_prefix` (default=`\'@\'`) are interpreted\n    as XML node attributes, whereas keys equal to `cdata_key`\n    (default=`\'#text\'`) are treated as character data.\n\n    The `pretty` parameter (default=`False`) enables pretty-printing. In this\n    mode, lines are terminated with `\'\\n\'` and indented with `\'\\t\'`, but this\n    can be customized with the `newl` and `indent` parameters.\n\n    """"""\n    ((key, value),) = input_dict.items()\n    must_return = False\n    if output is None:\n        output = StringIO()\n        must_return = True\n    content_handler = XMLGenerator(output, encoding)\n    if full_document:\n        content_handler.startDocument()\n    _emit(key, value, content_handler, **kwargs)\n    if full_document:\n        content_handler.endDocument()\n    if must_return:\n        value = output.getvalue()\n        try:  # pragma no cover\n            value = value.decode(encoding)\n        except AttributeError:  # pragma no cover\n            pass\n        return value\n\nif __name__ == \'__main__\':  # pragma: no cover\n    import sys\n    import marshal\n\n    (item_depth,) = sys.argv[1:]\n    item_depth = int(item_depth)\n\n    def handle_item(path, item):\n        marshal.dump((path, item), sys.stdout)\n        return True\n\n    try:\n        root = parse(sys.stdin,\n                     item_depth=item_depth,\n                     item_callback=handle_item,\n                     dict_constructor=dict)\n        if item_depth == 0:\n            handle_item([], root)\n    except KeyboardInterrupt:\n        pass\n'"
gammapy/irf/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""\nInstrument response functions (IRFs).\n""""""\nfrom .background import *\nfrom .edisp_kernel import *\nfrom .edisp_map import *\nfrom .effective_area import *\nfrom .energy_dispersion import *\nfrom .io import *\nfrom .irf_reduce import *\nfrom .irf_stack import *\nfrom .psf_3d import *\nfrom .psf_gauss import *\nfrom .psf_kernel import *\nfrom .psf_king import *\nfrom .psf_map import *\nfrom .psf_table import *\n'"
gammapy/irf/background.py,18,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport numpy as np\nimport astropy.units as u\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom gammapy.maps import MapAxis\nfrom gammapy.maps.utils import edges_from_lo_hi\nfrom gammapy.utils.integrate import trapz_loglog\nfrom gammapy.utils.nddata import NDDataArray\nfrom gammapy.utils.scripts import make_path\n\n__all__ = [""Background3D"", ""Background2D""]\n\nlog = logging.getLogger(__name__)\n\n\nclass Background3D:\n    """"""Background 3D.\n\n    Data format specification: :ref:`gadf:bkg_3d`\n\n    Parameters\n    ----------\n    energy_lo, energy_hi : `~astropy.units.Quantity`\n        Energy binning\n    fov_lon_lo, fov_lon_hi : `~astropy.units.Quantity`\n        FOV coordinate X-axis binning.\n    fov_lat_lo, fov_lat_hi : `~astropy.units.Quantity`\n        FOV coordinate Y-axis binning.\n    data : `~astropy.units.Quantity`\n        Background rate (usually: ``s^-1 MeV^-1 sr^-1``)\n\n    Examples\n    --------\n    Here\'s an example you can use to learn about this class:\n\n    >>> from gammapy.irf import Background3D\n    >>> filename = \'$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits\'\n    >>> bkg_3d = Background3D.read(filename, hdu=\'BACKGROUND\')\n    >>> print(bkg_3d)\n    Background3D\n    NDDataArray summary info\n    energy         : size =    21, min =  0.016 TeV, max = 158.489 TeV\n    fov_lon           : size =    36, min = -5.833 deg, max =  5.833 deg\n    fov_lat           : size =    36, min = -5.833 deg, max =  5.833 deg\n    Data           : size = 27216, min =  0.000 1 / (MeV s sr), max =  0.421 1 / (MeV s sr)\n    """"""\n\n    default_interp_kwargs = dict(\n        bounds_error=False, fill_value=None, values_scale=""log""\n    )\n    """"""Default Interpolation kwargs for `~gammapy.utils.nddata.NDDataArray`. Extrapolate.""""""\n\n    def __init__(\n        self,\n        energy_lo,\n        energy_hi,\n        fov_lon_lo,\n        fov_lon_hi,\n        fov_lat_lo,\n        fov_lat_hi,\n        data,\n        meta=None,\n        interp_kwargs=None,\n    ):\n        if interp_kwargs is None:\n            interp_kwargs = self.default_interp_kwargs\n\n        e_edges = edges_from_lo_hi(energy_lo, energy_hi)\n        energy_axis = MapAxis.from_edges(e_edges, interp=""log"", name=""energy"")\n\n        fov_lon_edges = edges_from_lo_hi(fov_lon_lo, fov_lon_hi)\n        fov_lon_axis = MapAxis.from_edges(fov_lon_edges, interp=""lin"", name=""fov_lon"")\n\n        fov_lat_edges = edges_from_lo_hi(fov_lat_lo, fov_lat_hi)\n        fov_lat_axis = MapAxis.from_edges(fov_lat_edges, interp=""lin"", name=""fov_lat"")\n\n        self.data = NDDataArray(\n            axes=[energy_axis, fov_lon_axis, fov_lat_axis],\n            data=data,\n            interp_kwargs=interp_kwargs,\n        )\n        self.meta = meta or {}\n\n    def __str__(self):\n        ss = self.__class__.__name__\n        ss += f""\\n{self.data}""\n        return ss\n\n    @classmethod\n    def from_table(cls, table):\n        """"""Read from `~astropy.table.Table`.""""""\n        # Spec says key should be ""BKG"", but there are files around\n        # (e.g. CTA 1DC) that use ""BGD"". For now we support both\n        if ""BKG"" in table.colnames:\n            bkg_name = ""BKG""\n        elif ""BGD"" in table.colnames:\n            bkg_name = ""BGD""\n        else:\n            raise ValueError(\'Invalid column names. Need ""BKG"" or ""BGD"".\')\n\n        data_unit = u.Unit(table[bkg_name].unit, parse_strict=""silent"")\n        if isinstance(data_unit, u.UnrecognizedUnit):\n            data_unit = u.Unit(""s-1 MeV-1 sr-1"")\n            log.warning(\n                ""Invalid unit found in background table! Assuming (s-1 MeV-1 sr-1)""\n            )\n\n        return cls(\n            energy_lo=table[""ENERG_LO""].quantity[0],\n            energy_hi=table[""ENERG_HI""].quantity[0],\n            fov_lon_lo=table[""DETX_LO""].quantity[0],\n            fov_lon_hi=table[""DETX_HI""].quantity[0],\n            fov_lat_lo=table[""DETY_LO""].quantity[0],\n            fov_lat_hi=table[""DETY_HI""].quantity[0],\n            data=table[bkg_name].data[0] * data_unit,\n            meta=table.meta,\n        )\n\n    @classmethod\n    def from_hdulist(cls, hdulist, hdu=""BACKGROUND""):\n        """"""Create from `~astropy.io.fits.HDUList`.""""""\n        return cls.from_table(Table.read(hdulist[hdu]))\n\n    @classmethod\n    def read(cls, filename, hdu=""BACKGROUND""):\n        """"""Read from file.""""""\n        with fits.open(make_path(filename), memmap=False) as hdulist:\n            return cls.from_hdulist(hdulist, hdu=hdu)\n\n    def to_table(self):\n        """"""Convert to `~astropy.table.Table`.""""""\n        meta = self.meta.copy()\n\n        detx = self.data.axis(""fov_lon"").edges\n        dety = self.data.axis(""fov_lat"").edges\n        energy = self.data.axis(""energy"").edges\n\n        table = Table(meta=meta)\n        table[""DETX_LO""] = detx[:-1][np.newaxis]\n        table[""DETX_HI""] = detx[1:][np.newaxis]\n        table[""DETY_LO""] = dety[:-1][np.newaxis]\n        table[""DETY_HI""] = dety[1:][np.newaxis]\n        table[""ENERG_LO""] = energy[:-1][np.newaxis]\n        table[""ENERG_HI""] = energy[1:][np.newaxis]\n        table[""BKG""] = self.data.data[np.newaxis]\n        return table\n\n    def to_fits(self, name=""BACKGROUND""):\n        """"""Convert to `~astropy.io.fits.BinTableHDU`.""""""\n        return fits.BinTableHDU(self.to_table(), name=name)\n\n    def evaluate(self, fov_lon, fov_lat, energy_reco, method=""linear"", **kwargs):\n        """"""Evaluate at given FOV position and energy.\n\n        Parameters\n        ----------\n        fov_lon, fov_lat : `~astropy.coordinates.Angle`\n            FOV coordinates expecting in AltAz frame.\n        energy_reco : `~astropy.units.Quantity`\n            energy on which you want to interpolate. Same dimension than fov_lat and fov_lat\n        method : str {\'linear\', \'nearest\'}, optional\n            Interpolation method\n        kwargs : dict\n            option for interpolation for `~scipy.interpolate.RegularGridInterpolator`\n\n        Returns\n        -------\n        array : `~astropy.units.Quantity`\n            Interpolated values, axis order is the same as for the NDData array\n        """"""\n        values = self.data.evaluate(\n            fov_lon=fov_lon,\n            fov_lat=fov_lat,\n            energy=energy_reco,\n            method=method,\n            **kwargs,\n        )\n        return values\n\n    def evaluate_integrate(\n        self, fov_lon, fov_lat, energy_reco, method=""linear"", **kwargs\n    ):\n        """"""Integrate in a given energy band.\n\n        Parameters\n        ----------\n        fov_lon, fov_lat : `~astropy.coordinates.Angle`\n            FOV coordinates expecting in AltAz frame.\n        energy_reco: `~astropy.units.Quantity`\n            Reconstructed energy edges.\n        method : {\'linear\', \'nearest\'}, optional\n            Interpolation method\n\n        Returns\n        -------\n        array : `~astropy.units.Quantity`\n            Returns 2D array with axes offset\n        """"""\n        data = self.evaluate(fov_lon, fov_lat, energy_reco, method=method)\n        return trapz_loglog(data, energy_reco, axis=0)\n\n    def to_2d(self):\n        """"""Convert to `Background2D`.\n\n        This takes the values at Y = 0 and X >= 0.\n        """"""\n        idx_lon = self.data.axis(""fov_lon"").coord_to_idx(0 * u.deg)[0]\n        idx_lat = self.data.axis(""fov_lat"").coord_to_idx(0 * u.deg)[0]\n        data = self.data.data[:, idx_lon:, idx_lat].copy()\n\n        energy = self.data.axis(""energy"").edges\n        offset = self.data.axis(""fov_lon"").edges[idx_lon:]\n\n        return Background2D(\n            energy_lo=energy[:-1],\n            energy_hi=energy[1:],\n            offset_lo=offset[:-1],\n            offset_hi=offset[1:],\n            data=data,\n        )\n\n    def peek(self, figsize=(10, 8)):\n        return self.to_2d().peek(figsize)\n\n\nclass Background2D:\n    """"""Background 2D.\n\n    Data format specification: :ref:`gadf:bkg_2d`\n\n    Parameters\n    ----------\n    energy_lo, energy_hi : `~astropy.units.Quantity`\n        Energy binning\n    offset_lo, offset_hi : `~astropy.units.Quantity`\n        FOV coordinate offset-axis binning\n    data : `~astropy.units.Quantity`\n        Background rate (usually: ``s^-1 MeV^-1 sr^-1``)\n    """"""\n\n    default_interp_kwargs = dict(bounds_error=False, fill_value=None)\n    """"""Default Interpolation kwargs for `~gammapy.utils.nddata.NDDataArray`. Extrapolate.""""""\n\n    def __init__(\n        self,\n        energy_lo,\n        energy_hi,\n        offset_lo,\n        offset_hi,\n        data,\n        meta=None,\n        interp_kwargs=None,\n    ):\n        if interp_kwargs is None:\n            interp_kwargs = self.default_interp_kwargs\n\n        e_edges = edges_from_lo_hi(energy_lo, energy_hi)\n        energy_axis = MapAxis.from_edges(e_edges, interp=""log"", name=""energy"")\n\n        offset_edges = edges_from_lo_hi(offset_lo, offset_hi)\n        offset_axis = MapAxis.from_edges(offset_edges, interp=""lin"", name=""offset"")\n\n        self.data = NDDataArray(\n            axes=[energy_axis, offset_axis], data=data, interp_kwargs=interp_kwargs\n        )\n        self.meta = meta or {}\n\n    def __str__(self):\n        ss = self.__class__.__name__\n        ss += f""\\n{self.data}""\n        return ss\n\n    @classmethod\n    def from_table(cls, table):\n        """"""Read from `~astropy.table.Table`.""""""\n        # Spec says key should be ""BKG"", but there are files around\n        # (e.g. CTA 1DC) that use ""BGD"". For now we support both\n        if ""BKG"" in table.colnames:\n            bkg_name = ""BKG""\n        elif ""BGD"" in table.colnames:\n            bkg_name = ""BGD""\n        else:\n            raise ValueError(\'Invalid column names. Need ""BKG"" or ""BGD"".\')\n\n        data_unit = u.Unit(table[bkg_name].unit, parse_strict=""silent"")\n        if isinstance(data_unit, u.UnrecognizedUnit):\n            data_unit = u.Unit(""s-1 MeV-1 sr-1"")\n            log.warning(\n                ""Invalid unit found in background table! Assuming (s-1 MeV-1 sr-1)""\n            )\n        return cls(\n            energy_lo=table[""ENERG_LO""].quantity[0],\n            energy_hi=table[""ENERG_HI""].quantity[0],\n            offset_lo=table[""THETA_LO""].quantity[0],\n            offset_hi=table[""THETA_HI""].quantity[0],\n            data=table[bkg_name].data[0] * data_unit,\n            meta=table.meta,\n        )\n\n    @classmethod\n    def from_hdulist(cls, hdulist, hdu=""BACKGROUND""):\n        """"""Create from `~astropy.io.fits.HDUList`.""""""\n        return cls.from_table(Table.read(hdulist[hdu]))\n\n    @classmethod\n    def read(cls, filename, hdu=""BACKGROUND""):\n        """"""Read from file.""""""\n        with fits.open(make_path(filename), memmap=False) as hdulist:\n            return cls.from_hdulist(hdulist, hdu=hdu)\n\n    def to_table(self):\n        """"""Convert to `~astropy.table.Table`.""""""\n        meta = self.meta.copy()\n        table = Table(meta=meta)\n\n        theta = self.data.axis(""offset"").edges\n        energy = self.data.axis(""energy"").edges\n\n        table[""THETA_LO""] = theta[:-1][np.newaxis]\n        table[""THETA_HI""] = theta[1:][np.newaxis]\n        table[""ENERG_LO""] = energy[:-1][np.newaxis]\n        table[""ENERG_HI""] = energy[1:][np.newaxis]\n        table[""BKG""] = self.data.data[np.newaxis]\n        return table\n\n    def to_fits(self, name=""BACKGROUND""):\n        """"""Convert to `~astropy.io.fits.BinTableHDU`.""""""\n        return fits.BinTableHDU(self.to_table(), name=name)\n\n    def evaluate(self, fov_lon, fov_lat, energy_reco, method=""linear"", **kwargs):\n        """"""Evaluate at a given FOV position and energy.\n\n        The fov_lon, fov_lat, energy_reco has to have the same shape\n        since this is a set of points on which you want to evaluate.\n\n        To have the same API than background 3D for the\n        background evaluation, the offset is ``fov_altaz_lon``.\n\n        Parameters\n        ----------\n        fov_lon, fov_lat : `~astropy.coordinates.Angle`\n            FOV coordinates expecting in AltAz frame, same shape than energy_reco\n        energy_reco : `~astropy.units.Quantity`\n            Reconstructed energy, same dimension than fov_lat and fov_lat\n        method : str {\'linear\', \'nearest\'}, optional\n            Interpolation method\n        kwargs : dict\n            option for interpolation for `~scipy.interpolate.RegularGridInterpolator`\n\n        Returns\n        -------\n        array : `~astropy.units.Quantity`\n            Interpolated values, axis order is the same as for the NDData array\n        """"""\n        offset = np.sqrt(fov_lon ** 2 + fov_lat ** 2)\n        return self.data.evaluate(\n            offset=offset, energy=energy_reco, method=method, **kwargs\n        )\n\n    def evaluate_integrate(self, fov_lon, fov_lat, energy_reco, method=""linear""):\n        """"""Evaluate at given FOV position and energy, by integrating over the energy range.\n\n        Parameters\n        ----------\n        fov_lon, fov_lat : `~astropy.coordinates.Angle`\n            FOV coordinates expecting in AltAz frame.\n        energy_reco: `~astropy.units.Quantity`\n            Reconstructed energy edges.\n        method : {\'linear\', \'nearest\'}, optional\n            Interpolation method\n\n        Returns\n        -------\n        array : `~astropy.units.Quantity`\n            Returns 2D array with axes offset\n        """"""\n        data = self.evaluate(fov_lon, fov_lat, energy_reco, method=method)\n        return trapz_loglog(data, energy_reco, axis=0)\n\n    def to_3d(self):\n        """"""Convert to `Background3D`.\n\n        Fill in a radially symmetric way.\n        """"""\n        raise NotImplementedError\n\n    def plot(self, ax=None, add_cbar=True, **kwargs):\n        """"""Plot energy offset dependence of the background model.\n        """"""\n        import matplotlib.pyplot as plt\n        from matplotlib.colors import LogNorm\n\n        ax = plt.gca() if ax is None else ax\n\n        x = self.data.axis(""energy"").edges.to_value(""TeV"")\n        y = self.data.axis(""offset"").edges.to_value(""deg"")\n        z = self.data.data.T.value\n\n        kwargs.setdefault(""cmap"", ""GnBu"")\n        kwargs.setdefault(""edgecolors"", ""face"")\n\n        caxes = ax.pcolormesh(x, y, z, norm=LogNorm(), **kwargs)\n        ax.set_xscale(""log"")\n        ax.set_ylabel(f""Offset (deg)"")\n        ax.set_xlabel(f""Energy (TeV)"")\n\n        xmin, xmax = x.min(), x.max()\n        ax.set_xlim(xmin, xmax)\n\n        if add_cbar:\n            label = f""Background rate ({self.data.data.unit})""\n            ax.figure.colorbar(caxes, ax=ax, label=label)\n\n    def plot_offset_dependence(self, ax=None, offset=None, energy=None, **kwargs):\n        """"""Plot background rate versus offset for a given energy.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        offset : `~astropy.coordinates.Angle`\n            Offset axis\n        energy : `~astropy.units.Quantity`\n            Energy\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axis\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        if energy is None:\n            e_min, e_max = np.log10(self.data.axis(""energy"").center.value[[0, -1]])\n            energy = np.logspace(e_min, e_max, 4) * self.data.axis(""energy"").unit\n\n        if offset is None:\n            offset = self.data.axis(""offset"").center\n\n        for ee in energy:\n            bkg = self.data.evaluate(offset=offset, energy=ee)\n            if np.isnan(bkg).all():\n                continue\n            label = f""energy = {ee:.1f}""\n            ax.plot(offset, bkg.value, label=label, **kwargs)\n\n        ax.set_xlabel(f""Offset ({self.data.axis(\'offset\').unit})"")\n        ax.set_ylabel(f""Background rate ({self.data.data.unit})"")\n        ax.set_yscale(""log"")\n        ax.legend(loc=""upper right"")\n        return ax\n\n    def plot_energy_dependence(self, ax=None, offset=None, energy=None, **kwargs):\n        """"""Plot background rate versus energy for a given offset.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        offset : `~astropy.coordinates.Angle`\n            Offset\n        energy : `~astropy.units.Quantity`\n            Energy axis\n        kwargs : dict\n            Forwarded tp plt.plot()\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axis\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        if offset is None:\n            off_min, off_max = self.data.axis(""offset"").center.value[[0, -1]]\n            offset = np.linspace(off_min, off_max, 4) * self.data.axis(""offset"").unit\n\n        if energy is None:\n            energy = self.data.axis(""energy"").center\n\n        for off in offset:\n            bkg = self.data.evaluate(offset=off, energy=energy)\n            label = f""offset = {off:.1f}""\n            ax.plot(energy, bkg.value, label=label, **kwargs)\n\n        ax.set_xscale(""log"")\n        ax.set_yscale(""log"")\n        ax.set_xlabel(f""Energy [{energy.unit}]"")\n        ax.set_ylabel(f""Background rate ({self.data.data.unit})"")\n        ax.set_xlim(min(energy.value), max(energy.value))\n        ax.legend(loc=""best"")\n\n        return ax\n\n    def plot_spectrum(self, ax=None, **kwargs):\n        """"""Plot angle integrated background rate versus energy.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        kwargs : dict\n            Forwarded tp plt.plot()\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axis\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n        offset = self.data.axis(""offset"").edges\n        energy = self.data.axis(""energy"").center\n\n        bkg = []\n        for ee in energy:\n            data = self.data.evaluate(offset=offset, energy=ee)\n            val = np.nansum(trapz_loglog(data, offset, axis=0))\n            bkg.append(val.value)\n\n        ax.plot(energy, bkg, label=""integrated spectrum"", **kwargs)\n\n        unit = self.data.data.unit * offset.unit * offset.unit\n\n        ax.set_xscale(""log"")\n        ax.set_yscale(""log"")\n        ax.set_xlabel(f""Energy [{energy.unit}]"")\n        ax.set_ylabel(f""Background rate ({unit})"")\n        ax.set_xlim(min(energy.value), max(energy.value))\n        ax.legend(loc=""best"")\n\n        return ax\n\n    def peek(self, figsize=(10, 8)):\n        """"""Quick-look summary plots.""""""\n        import matplotlib.pyplot as plt\n\n        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=figsize)\n        self.plot(ax=axes[1][1])\n        self.plot_offset_dependence(ax=axes[0][0])\n        self.plot_energy_dependence(ax=axes[1][0])\n        self.plot_spectrum(ax=axes[0][1])\n        plt.tight_layout()\n'"
gammapy/irf/edisp_kernel.py,30,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom astropy.units import Quantity\nfrom gammapy.maps import MapAxis\nfrom gammapy.maps.utils import edges_from_lo_hi\nfrom gammapy.utils.fits import energy_axis_to_ebounds\nfrom gammapy.utils.nddata import NDDataArray\nfrom gammapy.utils.scripts import make_path\n\n__all__ = [""EDispKernel""]\n\n\nclass EDispKernel:\n    """"""Energy dispersion matrix.\n\n    Data format specification: :ref:`gadf:ogip-rmf`\n\n    Parameters\n    ----------\n    e_true_lo, e_true_hi : `~astropy.units.Quantity`\n        True energy axis binning\n    e_reco_lo, e_reco_hi : `~astropy.units.Quantity`\n        Reconstructed energy axis binning\n    data : array_like\n        2-dim energy dispersion matrix\n\n    Examples\n    --------\n    Create a Gaussian energy dispersion matrix::\n\n        import numpy as np\n        import astropy.units as u\n        from gammapy.irf import EDispKernel\n        energy = np.logspace(0, 1, 101) * u.TeV\n        edisp = EDispKernel.from_gauss(\n            e_true=energy, e_reco=energy,\n            sigma=0.1, bias=0,\n        )\n\n    Have a quick look:\n\n    >>> print(edisp)\n    >>> edisp.peek()\n\n    See Also\n    --------\n    EnergyDispersion2D\n    """"""\n\n    default_interp_kwargs = dict(bounds_error=False, fill_value=0, method=""nearest"")\n    """"""Default Interpolation kwargs for `~NDDataArray`. Fill zeros and do not\n    interpolate""""""\n\n    def __init__(\n        self,\n        e_true_lo,\n        e_true_hi,\n        e_reco_lo,\n        e_reco_hi,\n        data,\n        interp_kwargs=None,\n        meta=None,\n    ):\n        if interp_kwargs is None:\n            interp_kwargs = self.default_interp_kwargs\n\n        e_true_edges = edges_from_lo_hi(e_true_lo, e_true_hi)\n        e_true_axis = MapAxis.from_edges(e_true_edges, interp=""log"", name=""energy_true"")\n\n        e_reco_edges = edges_from_lo_hi(e_reco_lo, e_reco_hi)\n        e_reco_axis = MapAxis.from_edges(e_reco_edges, interp=""log"", name=""energy"")\n\n        self.data = NDDataArray(\n            axes=[e_true_axis, e_reco_axis], data=data, interp_kwargs=interp_kwargs\n        )\n        self.meta = meta or {}\n\n    def __str__(self):\n        ss = self.__class__.__name__\n        ss += f""\\n{self.data}""\n        return ss\n\n    @property\n    def e_reco(self):\n        """"""Reconstructed energy axis (`~gammapy.maps.MapAxis`)""""""\n        return self.data.axis(""energy"")\n\n    @property\n    def e_true(self):\n        """"""True energy axis (`~gammapy.maps.MapAxis`)""""""\n        return self.data.axis(""energy_true"")\n\n    @property\n    def pdf_matrix(self):\n        """"""Energy dispersion PDF matrix (`~numpy.ndarray`).\n\n        Rows (first index): True Energy\n        Columns (second index): Reco Energy\n        """"""\n        return self.data.data.value\n\n    def pdf_in_safe_range(self, lo_threshold, hi_threshold):\n        """"""PDF matrix with bins outside threshold set to 0.\n\n        Parameters\n        ----------\n        lo_threshold : `~astropy.units.Quantity`\n            Low reco energy threshold\n        hi_threshold : `~astropy.units.Quantity`\n            High reco energy threshold\n        """"""\n        data = self.pdf_matrix.copy()\n        energy = self.e_reco.edges\n\n        if lo_threshold is None and hi_threshold is None:\n            idx = slice(None)\n        else:\n            idx = (energy[:-1] < lo_threshold) | (energy[1:] > hi_threshold)\n        data[:, idx] = 0\n        return data\n\n    @classmethod\n    def from_gauss(cls, e_true, e_reco, sigma, bias, pdf_threshold=1e-6):\n        """"""Create Gaussian energy dispersion matrix (`EnergyDispersion`).\n\n        Calls :func:`gammapy.irf.EnergyDispersion2D.from_gauss`\n\n        Parameters\n        ----------\n        e_true : `~astropy.units.Quantity`\n            Bin edges of true energy axis\n        e_reco : `~astropy.units.Quantity`\n            Bin edges of reconstructed energy axis\n        bias : float or `~numpy.ndarray`\n            Center of Gaussian energy dispersion, bias\n        sigma : float or `~numpy.ndarray`\n            RMS width of Gaussian energy dispersion, resolution\n        pdf_threshold : float, optional\n            Zero suppression threshold\n        """"""\n        from .energy_dispersion import EnergyDispersion2D\n\n        migra = np.linspace(1.0 / 3, 3, 200)\n        # A dummy offset axis (need length 2 for interpolation to work)\n        offset = Quantity([0, 1, 2], ""deg"")\n\n        edisp = EnergyDispersion2D.from_gauss(\n            e_true=e_true,\n            migra=migra,\n            sigma=sigma,\n            bias=bias,\n            offset=offset,\n            pdf_threshold=pdf_threshold,\n        )\n        return edisp.to_energy_dispersion(offset=offset[0], e_reco=e_reco)\n\n    @classmethod\n    def from_diagonal_response(cls, e_true, e_reco=None):\n        """"""Create energy dispersion from a diagonal response, i.e. perfect energy resolution\n\n        This creates the matrix corresponding to a perfect energy response.\n        It contains ones where the e_true center is inside the e_reco bin.\n        It is a square diagonal matrix if e_true = e_reco.\n\n        This is useful in cases where code always applies an edisp,\n        but you don\'t want it to do anything.\n\n        Parameters\n        ----------\n        e_true, e_reco : `~astropy.units.Quantity`\n            Energy bounds for true and reconstructed energy axis\n\n        Examples\n        --------\n        If ``e_true`` equals ``e_reco``, you get a diagonal matrix::\n\n            e_true = [0.5, 1, 2, 4, 6] * u.TeV\n            edisp = EnergyDispersion.from_diagonal_response(e_true)\n            edisp.plot_matrix()\n\n        Example with different energy binnings::\n\n            e_true = [0.5, 1, 2, 4, 6] * u.TeV\n            e_reco = [2, 4, 6] * u.TeV\n            edisp = EnergyDispersion.from_diagonal_response(e_true, e_reco)\n            edisp.plot_matrix()\n        """"""\n        if e_reco is None:\n            e_reco = e_true\n\n        e_true_center = 0.5 * (e_true[1:] + e_true[:-1])\n        etrue_2d, ereco_lo_2d = np.meshgrid(e_true_center, e_reco[:-1])\n        etrue_2d, ereco_hi_2d = np.meshgrid(e_true_center, e_reco[1:])\n\n        data = np.logical_and(etrue_2d >= ereco_lo_2d, etrue_2d < ereco_hi_2d)\n        data = np.transpose(data).astype(""float"")\n\n        return cls(\n            e_true_lo=e_true[:-1],\n            e_true_hi=e_true[1:],\n            e_reco_lo=e_reco[:-1],\n            e_reco_hi=e_reco[1:],\n            data=data,\n        )\n\n    @classmethod\n    def from_hdulist(cls, hdulist, hdu1=""MATRIX"", hdu2=""EBOUNDS""):\n        """"""Create `EnergyDispersion` object from `~astropy.io.fits.HDUList`.\n\n        Parameters\n        ----------\n        hdulist : `~astropy.io.fits.HDUList`\n            HDU list with ``MATRIX`` and ``EBOUNDS`` extensions.\n        hdu1 : str, optional\n            HDU containing the energy dispersion matrix, default: MATRIX\n        hdu2 : str, optional\n            HDU containing the energy axis information, default, EBOUNDS\n        """"""\n        matrix_hdu = hdulist[hdu1]\n        ebounds_hdu = hdulist[hdu2]\n\n        data = matrix_hdu.data\n        header = matrix_hdu.header\n\n        pdf_matrix = np.zeros([len(data), header[""DETCHANS""]], dtype=np.float64)\n\n        for i, l in enumerate(data):\n            if l.field(""N_GRP""):\n                m_start = 0\n                for k in range(l.field(""N_GRP"")):\n                    pdf_matrix[\n                        i,\n                        l.field(""F_CHAN"")[k] : l.field(""F_CHAN"")[k]\n                        + l.field(""N_CHAN"")[k],\n                    ] = l.field(""MATRIX"")[m_start : m_start + l.field(""N_CHAN"")[k]]\n                    m_start += l.field(""N_CHAN"")[k]\n\n        unit = ebounds_hdu.header.get(""TUNIT2"")\n        e_reco_lo = Quantity(ebounds_hdu.data[""E_MIN""], unit=unit)\n        e_reco_hi = Quantity(ebounds_hdu.data[""E_MAX""], unit=unit)\n\n        unit = matrix_hdu.header.get(""TUNIT1"")\n        e_true_lo = Quantity(matrix_hdu.data[""ENERG_LO""], unit=unit)\n        e_true_hi = Quantity(matrix_hdu.data[""ENERG_HI""], unit=unit)\n\n        return cls(\n            e_true_lo=e_true_lo,\n            e_true_hi=e_true_hi,\n            e_reco_lo=e_reco_lo,\n            e_reco_hi=e_reco_hi,\n            data=pdf_matrix,\n        )\n\n    @classmethod\n    def read(cls, filename, hdu1=""MATRIX"", hdu2=""EBOUNDS""):\n        """"""Read from file.\n\n        Parameters\n        ----------\n        filename : `pathlib.Path`, str\n            File to read\n        hdu1 : str, optional\n            HDU containing the energy dispersion matrix, default: MATRIX\n        hdu2 : str, optional\n            HDU containing the energy axis information, default, EBOUNDS\n        """"""\n        with fits.open(make_path(filename), memmap=False) as hdulist:\n            return cls.from_hdulist(hdulist, hdu1=hdu1, hdu2=hdu2)\n\n    def to_hdulist(self, use_sherpa=False, **kwargs):\n        """"""Convert RMF to FITS HDU list format.\n\n        Parameters\n        ----------\n        header : `~astropy.io.fits.Header`\n            Header to be written in the fits file.\n        energy_unit : str\n            Unit in which the energy is written in the HDU list\n\n        Returns\n        -------\n        hdulist : `~astropy.io.fits.HDUList`\n            RMF in HDU list format.\n\n        Notes\n        -----\n        For more info on the RMF FITS file format see:\n        https://heasarc.gsfc.nasa.gov/docs/heasarc/caldb/docs/summary/cal_gen_92_002_summary.html\n        """"""\n        # Cannot use table_to_fits here due to variable length array\n        # http://docs.astropy.org/en/v1.0.4/io/fits/usage/unfamiliar.html\n\n        table = self.to_table()\n        name = table.meta.pop(""name"")\n\n        header = fits.Header()\n        header.update(table.meta)\n\n        if use_sherpa:\n            table[""ENERG_HI""] = table[""ENERG_HI""].quantity.to(""keV"")\n            table[""ENERG_LO""] = table[""ENERG_LO""].quantity.to(""keV"")\n\n        cols = table.columns\n        c0 = fits.Column(\n            name=cols[0].name, format=""E"", array=cols[0], unit=str(cols[0].unit)\n        )\n        c1 = fits.Column(\n            name=cols[1].name, format=""E"", array=cols[1], unit=str(cols[1].unit)\n        )\n        c2 = fits.Column(name=cols[2].name, format=""I"", array=cols[2])\n        c3 = fits.Column(name=cols[3].name, format=""PI()"", array=cols[3])\n        c4 = fits.Column(name=cols[4].name, format=""PI()"", array=cols[4])\n        c5 = fits.Column(name=cols[5].name, format=""PE()"", array=cols[5])\n\n        hdu = fits.BinTableHDU.from_columns(\n            [c0, c1, c2, c3, c4, c5], header=header, name=name\n        )\n\n        energy = self.e_reco.edges\n\n        if use_sherpa:\n            energy = energy.to(""keV"")\n\n        ebounds = energy_axis_to_ebounds(energy)\n        prim_hdu = fits.PrimaryHDU()\n\n        return fits.HDUList([prim_hdu, hdu, ebounds])\n\n    def to_table(self):\n        """"""Convert to `~astropy.table.Table`.\n\n        The output table is in the OGIP RMF format.\n        https://heasarc.gsfc.nasa.gov/docs/heasarc/caldb/docs/memos/cal_gen_92_002/cal_gen_92_002.html#Tab:1\n        """"""\n        rows = self.pdf_matrix.shape[0]\n        n_grp = []\n        f_chan = np.ndarray(dtype=np.object, shape=rows)\n        n_chan = np.ndarray(dtype=np.object, shape=rows)\n        matrix = np.ndarray(dtype=np.object, shape=rows)\n\n        # Make RMF type matrix\n        for i, row in enumerate(self.data.data.value):\n            pos = np.nonzero(row)[0]\n            borders = np.where(np.diff(pos) != 1)[0]\n            # add 1 to borders for correct behaviour of np.split\n            groups = np.asarray(np.split(pos, borders + 1))\n            n_grp_temp = groups.shape[0] if groups.size > 0 else 1\n            n_chan_temp = np.asarray([val.size for val in groups])\n            try:\n                f_chan_temp = np.asarray([val[0] for val in groups])\n            except IndexError:\n                f_chan_temp = np.zeros(1)\n\n            n_grp.append(n_grp_temp)\n            f_chan[i] = f_chan_temp\n            n_chan[i] = n_chan_temp\n            matrix[i] = row[pos]\n\n        n_grp = np.asarray(n_grp, dtype=np.int16)\n\n        # Get total number of groups and channel subsets\n        numgrp, numelt = 0, 0\n        for val, val2 in zip(n_grp, n_chan):\n            numgrp += np.sum(val)\n            numelt += np.sum(val2)\n\n        table = Table()\n\n        energy = self.e_true.edges\n        table[""ENERG_LO""] = energy[:-1]\n        table[""ENERG_HI""] = energy[1:]\n        table[""N_GRP""] = n_grp\n        table[""F_CHAN""] = f_chan\n        table[""N_CHAN""] = n_chan\n        table[""MATRIX""] = matrix\n\n        table.meta = {\n            ""name"": ""MATRIX"",\n            ""chantype"": ""PHA"",\n            ""hduclass"": ""OGIP"",\n            ""hduclas1"": ""RESPONSE"",\n            ""hduclas2"": ""RSP_MATRIX"",\n            ""detchans"": self.e_reco.nbin,\n            ""numgrp"": numgrp,\n            ""numelt"": numelt,\n            ""tlmin4"": 0,\n        }\n\n        return table\n\n    def write(self, filename, use_sherpa=False, **kwargs):\n        """"""Write to file.""""""\n        filename = make_path(filename)\n        self.to_hdulist(use_sherpa=use_sherpa).writeto(filename, **kwargs)\n\n    def get_resolution(self, e_true):\n        """"""Get energy resolution for a given true energy.\n\n        The resolution is given as a percentage of the true energy\n\n        Parameters\n        ----------\n        e_true : `~astropy.units.Quantity`\n            True energy\n        """"""\n        var = self._get_variance(e_true)\n        idx_true = self.e_true.coord_to_idx(e_true)\n        e_true_real = self.e_true.center[idx_true]\n        return np.sqrt(var) / e_true_real\n\n    def get_bias(self, e_true):\n        r""""""Get reconstruction bias for a given true energy.\n\n        Bias is defined as\n\n        .. math:: \\frac{E_{reco}-E_{true}}{E_{true}}\n\n        Parameters\n        ----------\n        e_true : `~astropy.units.Quantity`\n            True energy\n        """"""\n        e_reco = self.get_mean(e_true)\n        idx_true = self.e_true.coord_to_idx(e_true)\n        e_true_real = self.e_true.center[idx_true]\n        bias = (e_reco - e_true_real) / e_true_real\n        return bias\n\n    def get_bias_energy(self, bias, emin=None, emax=None):\n        """"""Find energy corresponding to a given bias.\n\n        In case the solution is not unique, provide the ``emin`` or ``emax`` arguments\n        to limit the solution to the given range.  By default the peak energy of the\n        bias is chosen as ``emin``.\n\n        Parameters\n        ----------\n        bias : float\n            Bias value.\n        emin : `~astropy.units.Quantity`\n            Lower bracket value in case solution is not unique.\n        emax : `~astropy.units.Quantity`\n            Upper bracket value in case solution is not unique.\n\n        Returns\n        -------\n        bias_energy : `~astropy.units.Quantity`\n            Reconstructed energy corresponding to the given bias.\n        """"""\n        from gammapy.modeling.models import TemplateSpectralModel\n\n        e_true = self.e_true.center\n        values = self.get_bias(e_true)\n\n        if emin is None:\n            # use the peak bias energy as default minimum\n            emin = e_true[np.nanargmax(values)]\n        if emax is None:\n            emax = e_true[-1]\n\n        bias_spectrum = TemplateSpectralModel(e_true, values)\n        e_true_bias = bias_spectrum.inverse(Quantity(bias), emin=emin, emax=emax)\n\n        # return reconstructed energy\n        return e_true_bias * (1 + bias)\n\n    def get_mean(self, e_true):\n        """"""Get mean reconstructed energy for a given true energy.""""""\n        # find pdf for true energies\n        idx = self.e_true.coord_to_idx(e_true)\n        pdf = self.data.data[idx]\n\n        # compute sum along reconstructed energy\n        # axis to determine the mean\n        norm = np.sum(pdf, axis=-1)\n        temp = np.sum(pdf * self.e_reco.center, axis=-1)\n\n        with np.errstate(invalid=""ignore""):\n            # corm can be zero\n            mean = np.nan_to_num(temp / norm)\n\n        return mean\n\n    def _get_variance(self, e_true):\n        """"""Get variance of log reconstructed energy.""""""\n        # evaluate the pdf at given true energies\n        idx = self.e_true.coord_to_idx(e_true)\n        pdf = self.data.data[idx]\n\n        # compute mean\n        mean = self.get_mean(e_true)\n\n        # create array of reconstructed-energy nodes\n        # for each given true energy value\n        # (first axis is reconstructed energy)\n        erec = self.e_reco.center\n        erec = np.repeat(erec, max(np.sum(mean.shape), 1)).reshape(\n            erec.shape + mean.shape\n        )\n\n        # compute deviation from mean\n        # (and move reconstructed energy axis to last axis)\n        temp_ = (erec - mean) ** 2\n        temp = np.rollaxis(temp_, 1)\n\n        # compute sum along reconstructed energy\n        # axis to determine the variance\n        norm = np.sum(pdf, axis=-1)\n        var = np.sum(temp * pdf, axis=-1)\n\n        return var / norm\n\n    def plot_matrix(self, ax=None, show_energy=None, add_cbar=False, **kwargs):\n        """"""Plot PDF matrix.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        show_energy : `~astropy.units.Quantity`, optional\n            Show energy, e.g. threshold, as vertical line\n        add_cbar : bool\n            Add a colorbar to the plot.\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axis\n        """"""\n        import matplotlib.pyplot as plt\n        from matplotlib.colors import PowerNorm\n\n        kwargs.setdefault(""cmap"", ""GnBu"")\n        norm = PowerNorm(gamma=0.5)\n        kwargs.setdefault(""norm"", norm)\n\n        ax = plt.gca() if ax is None else ax\n\n        e_true = self.e_true.edges\n        e_reco = self.e_reco.edges\n        x = e_true.value\n        y = e_reco.value\n        z = self.pdf_matrix\n        caxes = ax.pcolormesh(x, y, z.T, **kwargs)\n\n        if show_energy is not None:\n            ener_val = show_energy.to_value(self.reco_energy.unit)\n            ax.hlines(ener_val, 0, 200200, linestyles=""dashed"")\n\n        if add_cbar:\n            label = ""Probability density (A.U.)""\n            cbar = ax.figure.colorbar(caxes, ax=ax, label=label)\n\n        ax.set_xlabel(fr""$E_\\mathrm{{True}}$ [{e_true.unit}]"")\n        ax.set_ylabel(fr""$E_\\mathrm{{Reco}}$ [{e_reco.unit}]"")\n        ax.set_xscale(""log"")\n        ax.set_yscale(""log"")\n        ax.set_xlim(x.min(), x.max())\n        ax.set_ylim(y.min(), y.max())\n        return ax\n\n    def plot_bias(self, ax=None, **kwargs):\n        """"""Plot reconstruction bias.\n\n        See `~gammapy.irf.EnergyDispersion.get_bias` method.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        x = self.e_true.center.to_value(""TeV"")\n        y = self.get_bias(self.e_true.center)\n\n        ax.plot(x, y, **kwargs)\n        ax.set_xlabel(r""$E_\\mathrm{{True}}$ [TeV]"")\n        ax.set_ylabel(r""($E_\\mathrm{{True}} - E_\\mathrm{{Reco}} / E_\\mathrm{{True}}$)"")\n        ax.set_xscale(""log"")\n        return ax\n\n    def peek(self, figsize=(15, 5)):\n        """"""Quick-look summary plot.""""""\n        import matplotlib.pyplot as plt\n\n        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=figsize)\n        self.plot_bias(ax=axes[0])\n        self.plot_matrix(ax=axes[1])\n        plt.tight_layout()\n'"
gammapy/irf/edisp_map.py,20,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom scipy.interpolate import interp1d\nfrom gammapy.maps import Map, MapAxis, MapCoord, WcsGeom\nfrom gammapy.utils.random import InverseCDFSampler, get_random_state\nfrom .edisp_kernel import EDispKernel\nfrom .irf_map import IRFMap\n\n__all__ = [""EDispMap"", ""EDispKernelMap""]\n\n\ndef get_overlap_fraction(energy_axis, energy_axis_true):\n    a_min = energy_axis.edges[:-1]\n    a_max = energy_axis.edges[1:]\n\n    b_min = energy_axis_true.edges[:-1][:, np.newaxis]\n    b_max = energy_axis_true.edges[1:][:, np.newaxis]\n\n    xmin = np.fmin(a_max, b_max)\n    xmax = np.fmax(a_min, b_min)\n    return np.clip(xmin - xmax, 0, np.inf) / (b_max - b_min)\n\n\nclass EDispMap(IRFMap):\n    """"""Energy dispersion map.\n\n    Parameters\n    ----------\n    edisp_map : `~gammapy.maps.Map`\n        the input Energy Dispersion Map. Should be a Map with 2 non spatial axes.\n        migra and true energy axes should be given in this specific order.\n    exposure_map : `~gammapy.maps.Map`, optional\n        Associated exposure map. Needs to have a consistent map geometry.\n\n    Examples\n    --------\n    ::\n\n        import numpy as np\n        from astropy import units as u\n        from astropy.coordinates import SkyCoord\n        from gammapy.maps import WcsGeom, MapAxis\n        from gammapy.irf import EnergyDispersion2D, EffectiveAreaTable2D\n        from gammapy.cube import make_edisp_map, make_map_exposure_true_energy\n\n        # Define energy dispersion map geometry\n        energy_axis = MapAxis.from_edges(np.logspace(-1, 1, 4), unit=""TeV"", name=""energy"")\n        migra_axis = MapAxis.from_edges(np.linspace(0, 3, 100), name=""migra"")\n        pointing = SkyCoord(0, 0, unit=""deg"")\n        max_offset = 4 * u.deg\n        geom = WcsGeom.create(\n            binsz=0.25 * u.deg,\n            width=10 * u.deg,\n            skydir=pointing,\n            axes=[migra_axis, energy_axis],\n        )\n\n        # Extract EnergyDispersion2D from CTA 1DC IRF\n        filename = ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits""\n        edisp2D = EnergyDispersion2D.read(filename, hdu=""ENERGY DISPERSION"")\n        aeff2d = EffectiveAreaTable2D.read(filename, hdu=""EFFECTIVE AREA"")\n\n        # Create the exposure map\n        exposure_geom = geom.to_image().to_cube([energy_axis])\n        exposure_map = make_map_exposure_true_energy(pointing, ""1 h"", aeff2d, exposure_geom)\n\n        # create the EDispMap for the specified pointing\n        edisp_map = make_edisp_map(edisp2D, pointing, geom, max_offset, exposure_map)\n\n        # Get an Energy Dispersion (1D) at any position in the image\n        pos = SkyCoord(2.0, 2.5, unit=""deg"")\n        e_reco = np.logspace(-1.0, 1.0, 10) * u.TeV\n        edisp = edisp_map.get_edisp_kernel(pos=pos, e_reco=e_reco)\n\n        # Write map to disk\n        edisp_map.write(""edisp_map.fits"")\n    """"""\n\n    _hdu_name = ""edisp""\n\n    def __init__(self, edisp_map, exposure_map):\n        if edisp_map.geom.axes[1].name.upper() != ""ENERGY_TRUE"":\n            raise ValueError(""Incorrect energy axis position in input Map"")\n\n        if edisp_map.geom.axes[0].name.upper() != ""MIGRA"":\n            raise ValueError(""Incorrect migra axis position in input Map"")\n\n        super().__init__(irf_map=edisp_map, exposure_map=exposure_map)\n\n    @property\n    def edisp_map(self):\n        return self._irf_map\n\n    @edisp_map.setter\n    def edisp_map(self, value):\n        self._irf_map = value\n\n    def get_edisp_kernel(self, position, e_reco):\n        """"""Get energy dispersion at a given position.\n\n        Parameters\n        ----------\n        position : `~astropy.coordinates.SkyCoord`\n            the target position. Should be a single coordinates\n        e_reco : `~astropy.units.Quantity`\n            Reconstructed energy axis binning\n\n        Returns\n        -------\n        edisp : `~gammapy.irf.EnergyDispersion`\n            the energy dispersion (i.e. rmf object)\n        """"""\n        if position.size != 1:\n            raise ValueError(\n                ""EnergyDispersion can be extracted at one single position only.""\n            )\n\n        energy_axis = self.edisp_map.geom.get_axis_by_name(""energy_true"")\n        migra_axis = self.edisp_map.geom.get_axis_by_name(""migra"")\n\n        coords = {\n            ""skycoord"": position,\n            ""migra"": migra_axis.center.reshape((-1, 1, 1, 1)),\n            ""energy_true"": energy_axis.center.reshape((1, -1, 1, 1)),\n        }\n\n        # Interpolate in the EDisp map. Squeeze to remove dimensions of length 1\n        values = self.edisp_map.interp_by_coord(coords) * self.edisp_map.unit\n        edisp_values = values[:, :, 0, 0]\n\n        data = []\n\n        for idx, e_true in enumerate(energy_axis.center):\n            # migration value of e_reco bounds\n            migra = e_reco / e_true\n\n            cumsum = np.insert(edisp_values[:, idx], 0, 0).cumsum()\n            with np.errstate(invalid=""ignore""):\n                cumsum = np.nan_to_num(cumsum / cumsum[-1])\n\n            f = interp1d(\n                migra_axis.edges.value,\n                cumsum,\n                kind=""linear"",\n                bounds_error=False,\n                fill_value=(0, 1),\n            )\n\n            # We compute the difference between 2 successive bounds in e_reco\n            # to get integral over reco energy bin\n            integral = np.diff(np.clip(f(migra), a_min=0, a_max=1))\n            data.append(integral)\n\n        return EDispKernel(\n            e_true_lo=energy_axis.edges[:-1],\n            e_true_hi=energy_axis.edges[1:],\n            e_reco_lo=e_reco[:-1],\n            e_reco_hi=e_reco[1:],\n            data=data,\n        )\n\n    @classmethod\n    def from_geom(cls, geom):\n        """"""Create edisp map from geom.\n\n        By default a diagonal edisp matrix is created.\n\n        Parameters\n        ----------\n        geom : `Geom`\n            Edisp map geometry.\n\n        Returns\n        -------\n        edisp_map : `EDispMap`\n            Energy dispersion map.\n        """"""\n        if ""energy_true"" not in [ax.name for ax in geom.axes]:\n            raise ValueError(""EDispMap requires true energy axis"")\n\n        geom_exposure_edisp = geom.squash(axis=""migra"")\n        exposure_edisp = Map.from_geom(geom_exposure_edisp, unit=""m2 s"")\n\n        migra_axis = geom.get_axis_by_name(""migra"")\n        edisp_map = Map.from_geom(geom, unit="""")\n        migra_0 = migra_axis.coord_to_pix(1)\n\n        # distribute over two pixels\n        migra = geom.get_idx()[2]\n        data = np.abs(migra - migra_0)\n        data = np.where(data < 1, 1 - data, 0)\n        edisp_map.quantity = data\n        return cls(edisp_map, exposure_edisp)\n\n    def sample_coord(self, map_coord, random_state=0):\n        """"""Apply the energy dispersion corrections on the coordinates of a set of simulated events.\n\n        Parameters\n        ----------\n        map_coord : `~gammapy.maps.MapCoord` object.\n            Sequence of coordinates and energies of sampled events.\n        random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n            Defines random number generator initialisation.\n            Passed to `~gammapy.utils.random.get_random_state`.\n\n        Returns\n        -------\n        `~gammapy.maps.MapCoord`.\n            Sequence of Edisp-corrected coordinates of the input map_coord map.\n        """"""\n        random_state = get_random_state(random_state)\n        migra_axis = self.edisp_map.geom.get_axis_by_name(""migra"")\n\n        coord = {\n            ""skycoord"": map_coord.skycoord.reshape(-1, 1),\n            ""energy_true"": map_coord[""energy_true""].reshape(-1, 1),\n            ""migra"": migra_axis.center,\n        }\n\n        pdf_edisp = self.edisp_map.interp_by_coord(coord)\n\n        sample_edisp = InverseCDFSampler(pdf_edisp, axis=1, random_state=random_state)\n        pix_edisp = sample_edisp.sample_axis()\n        migra = migra_axis.pix_to_coord(pix_edisp)\n\n        energy_reco = map_coord[""energy_true""] * migra\n\n        return MapCoord.create({""skycoord"": map_coord.skycoord, ""energy"": energy_reco})\n\n    @classmethod\n    def from_diagonal_response(cls, energy_axis_true, migra_axis=None):\n        """"""Create an allsky EDisp map with diagonal response.\n\n        Parameters\n        ----------\n        energy_axis_true : `MapAxis`\n            True energy axis\n        migra_axis : `MapAxis`\n            Migra axis\n\n        Returns\n        -------\n        edisp_map : `EDispMap`\n            Energy dispersion map.\n        """"""\n        migra_res = 1e-5\n        migra_axis_default = MapAxis.from_bounds(\n            1 - migra_res, 1 + migra_res, nbin=3, name=""migra"", node_type=""edges""\n        )\n\n        migra_axis = migra_axis or migra_axis_default\n\n        geom = WcsGeom.create(\n            npix=(2, 1), proj=""CAR"", binsz=180, axes=[migra_axis, energy_axis_true]\n        )\n\n        return cls.from_geom(geom)\n\n    def to_edisp_kernel_map(self, energy_axis):\n        """"""Convert to map with edisp kernels\n\n        Parameters\n        ----------\n        e_reco : `MapAxis`\n            Reconstructed enrgy axis.\n\n        Returns\n        -------\n        edisp : `EDispKernelMap`\n            Energy dispersion kernel map.\n        """"""\n        axis = 0\n        energy_axis_true = self.edisp_map.geom.get_axis_by_name(""energy_true"")\n        migra_axis = self.edisp_map.geom.get_axis_by_name(""migra"")\n\n        data = []\n\n        for idx, e_true in enumerate(energy_axis_true.center):\n            # migration value of e_reco bounds\n            migra = energy_axis.edges / e_true\n\n            edisp_e_true = self.edisp_map.slice_by_idx({""energy_true"": idx})\n\n            cumsum = np.insert(edisp_e_true.data, 0, 0, axis=axis).cumsum(axis=axis)\n            with np.errstate(invalid=""ignore""):\n                cumsum = np.nan_to_num(cumsum / cumsum[slice(-2, -1)])\n\n            f = interp1d(\n                migra_axis.edges.value,\n                cumsum,\n                kind=""linear"",\n                bounds_error=False,\n                fill_value=(0, 1),\n                axis=axis,\n            )\n\n            integral = np.diff(np.clip(f(migra), a_min=0, a_max=1), axis=axis)\n            data.append(integral)\n\n        data = np.stack(data)\n\n        geom_image = self.edisp_map.geom.to_image()\n        geom = geom_image.to_cube([energy_axis, energy_axis_true])\n        edisp_kernel_map = Map.from_geom(geom=geom, data=data)\n        exposure_map = None\n        if self.exposure_map is not None:\n            exposure_map = Map.from_geom(\n                geom.squash(axis=energy_axis.name),\n                data=self.exposure_map.data,\n                unit=self.exposure_map.unit,\n                meta=self.exposure_map.meta,\n            )\n\n        return EDispKernelMap(\n            edisp_kernel_map=edisp_kernel_map, exposure_map=exposure_map\n        )\n\n\nclass EDispKernelMap(IRFMap):\n    """"""Energy dispersion kernel map.\n\n    Parameters\n    ----------\n    edisp_kernel_map : `~gammapy.maps.Map`\n        The input energy dispersion kernel map. Should be a Map with 2 non spatial axes.\n        Reconstructed and and true energy axes should be given in this specific order.\n    exposure_map : `~gammapy.maps.Map`, optional\n        Associated exposure map. Needs to have a consistent map geometry.\n\n    """"""\n\n    _hdu_name = ""edisp""\n\n    def __init__(self, edisp_kernel_map, exposure_map):\n        if edisp_kernel_map.geom.axes[1].name.upper() != ""ENERGY_TRUE"":\n            raise ValueError(""Incorrect energy axis position in input Map"")\n\n        if edisp_kernel_map.geom.axes[0].name.upper() != ""ENERGY"":\n            raise ValueError(""Incorrect migra axis position in input Map"")\n\n        super().__init__(irf_map=edisp_kernel_map, exposure_map=exposure_map)\n\n    @property\n    def edisp_map(self):\n        return self._irf_map\n\n    @edisp_map.setter\n    def edisp_map(self, value):\n        self._irf_map = value\n\n    @classmethod\n    def from_geom(cls, geom):\n        """"""Create edisp map from geom.\n\n        By default a diagonal edisp matrix is created.\n\n        Parameters\n        ----------\n        geom : `Geom`\n            Edisp map geometry.\n\n        Returns\n        -------\n        edisp_map : `EDispKernelMap`\n            Energy dispersion kernel map.\n        """"""\n        axis_names = [ax.name for ax in geom.axes]\n\n        if ""energy_true"" not in axis_names:\n            raise ValueError(""EDispKernelMap requires true energy axis"")\n\n        if ""energy"" not in axis_names:\n            raise ValueError(""EDispKernelMap requires energy axis"")\n\n        geom_exposure = geom.squash(axis=""energy"")\n        exposure = Map.from_geom(geom_exposure, unit=""m2 s"")\n\n        energy_axis = geom.get_axis_by_name(""energy"")\n        energy_axis_true = geom.get_axis_by_name(""energy_true"")\n\n        data = get_overlap_fraction(energy_axis, energy_axis_true)\n\n        edisp_kernel_map = Map.from_geom(geom, unit="""")\n        edisp_kernel_map.quantity += data[:, :, np.newaxis, np.newaxis]\n        return cls(edisp_kernel_map=edisp_kernel_map, exposure_map=exposure)\n\n    def get_edisp_kernel(self, position):\n        """"""Get energy dispersion at a given position.\n\n        Parameters\n        ----------\n        position : `~astropy.coordinates.SkyCoord`\n            the target position. Should be a single coordinates\n\n        Returns\n        -------\n        edisp : `~gammapy.irf.EnergyDispersion`\n            the energy dispersion (i.e. rmf object)\n        """"""\n        energy_true_axis = self.edisp_map.geom.get_axis_by_name(""energy_true"")\n        energy_axis = self.edisp_map.geom.get_axis_by_name(""energy"")\n\n        coords = {\n            ""skycoord"": position,\n            ""energy"": energy_axis.center,\n            ""energy_true"": energy_true_axis.center.reshape((-1, 1)),\n        }\n\n        data = self.edisp_map.get_by_coord(coords)\n\n        return EDispKernel(\n            e_true_lo=energy_true_axis.edges[:-1],\n            e_true_hi=energy_true_axis.edges[1:],\n            e_reco_lo=energy_axis.edges[:-1],\n            e_reco_hi=energy_axis.edges[1:],\n            data=data,\n        )\n\n    @classmethod\n    def from_diagonal_response(cls, energy_axis, energy_axis_true):\n        """"""Create an all-sky energy dispersion map with diagonal response.\n\n        Parameters\n        ----------\n        energy_axis : `MapAxis`\n            Energy axis.\n        energy_axis_true : `MapAxis`\n            True energy axis\n\n        Returns\n        -------\n        edisp_map : `EDispKernelMap`\n            Energy dispersion kernel map.\n        """"""\n        geom = WcsGeom.create(\n            npix=(2, 1), proj=""CAR"", binsz=180, axes=[energy_axis, energy_axis_true]\n        )\n        return cls.from_geom(geom)\n'"
gammapy/irf/effective_area.py,25,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nimport astropy.units as u\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom gammapy.maps import MapAxis\nfrom gammapy.maps.utils import edges_from_lo_hi\nfrom gammapy.utils.nddata import NDDataArray\nfrom gammapy.utils.scripts import make_path\n\n__all__ = [""EffectiveAreaTable"", ""EffectiveAreaTable2D""]\n\n\nclass EffectiveAreaTable:\n    """"""Effective area table.\n\n    TODO: Document\n\n    Parameters\n    ----------\n    energy_lo, energy_hi : `~astropy.units.Quantity`\n        Energy axis bin edges\n    data : `~astropy.units.Quantity`\n        Effective area\n\n    Examples\n    --------\n    Plot parametrized effective area for HESS, HESS2 and CTA.\n\n    .. plot::\n        :include-source:\n\n        import numpy as np\n        import matplotlib.pyplot as plt\n        import astropy.units as u\n        from gammapy.irf import EffectiveAreaTable\n\n        energy = np.logspace(-3, 3, 100) * u.TeV\n\n        for instrument in [\'HESS\', \'HESS2\', \'CTA\']:\n            aeff = EffectiveAreaTable.from_parametrization(energy, instrument)\n            ax = aeff.plot(label=instrument)\n\n        ax.set_yscale(\'log\')\n        ax.set_xlim([1e-3, 1e3])\n        ax.set_ylim([1e3, 1e12])\n        plt.legend(loc=\'best\')\n        plt.show()\n\n    Find energy where the effective area is at 10% of its maximum value\n\n    >>> import numpy as np\n    >>> import astropy.units as u\n    >>> from gammapy.irf import EffectiveAreaTable\n    >>> energy = np.logspace(-1, 2) * u.TeV\n    >>> aeff_max = aeff.max_area\n    >>> print(aeff_max).to(\'m2\')\n    156909.413371 m2\n    >>> energy_threshold = aeff.find_energy(0.1 * aeff_max)\n    >>> print(energy_threshold)\n    0.185368478744 TeV\n    """"""\n\n    def __init__(self, energy_lo, energy_hi, data, meta=None):\n\n        e_edges = edges_from_lo_hi(energy_lo, energy_hi)\n        energy_axis = MapAxis.from_edges(e_edges, interp=""log"", name=""energy_true"")\n\n        interp_kwargs = {""extrapolate"": False, ""bounds_error"": False}\n        self.data = NDDataArray(\n            axes=[energy_axis], data=data, interp_kwargs=interp_kwargs\n        )\n        self.meta = meta or {}\n\n    @property\n    def energy(self):\n        return self.data.axis(""energy_true"")\n\n    def plot(self, ax=None, energy=None, show_energy=None, **kwargs):\n        """"""Plot effective area.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        energy : `~astropy.units.Quantity`\n            Energy nodes\n        show_energy : `~astropy.units.Quantity`, optional\n            Show energy, e.g. threshold, as vertical line\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axis\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        kwargs.setdefault(""lw"", 2)\n\n        if energy is None:\n            energy = self.energy.center\n\n        eff_area = self.data.evaluate(energy_true=energy)\n\n        xerr = (\n            (energy - self.energy.edges[:-1]).value,\n            (self.energy.edges[1:] - energy).value,\n        )\n\n        ax.errorbar(energy.value, eff_area.value, xerr=xerr, **kwargs)\n        if show_energy is not None:\n            ener_val = u.Quantity(show_energy).to_value(self.energy.unit)\n            ax.vlines(ener_val, 0, 1.1 * self.max_area.value, linestyles=""dashed"")\n        ax.set_xscale(""log"")\n        ax.set_xlabel(f""Energy [{self.energy.unit}]"")\n        ax.set_ylabel(f""Effective Area [{self.data.data.unit}]"")\n\n        return ax\n\n    @classmethod\n    def from_parametrization(cls, energy, instrument=""HESS""):\n        r""""""Create parametrized effective area.\n\n        Parametrizations of the effective areas of different Cherenkov\n        telescopes taken from Appendix B of Abramowski et al. (2010), see\n        https://ui.adsabs.harvard.edu/abs/2010MNRAS.402.1342A .\n\n        .. math::\n            A_{eff}(E) = g_1 \\left(\\frac{E}{\\mathrm{MeV}}\\right)^{-g_2}\\exp{\\left(-\\frac{g_3}{E}\\right)}\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy binning, analytic function is evaluated at log centers\n        instrument : {\'HESS\', \'HESS2\', \'CTA\'}\n            Instrument name\n        """"""\n        energy = u.Quantity(energy)\n        # Put the parameters g in a dictionary.\n        # Units: g1 (cm^2), g2 (), g3 (MeV)\n        # Note that whereas in the paper the parameter index is 1-based,\n        # here it is 0-based\n        pars = {\n            ""HESS"": [6.85e9, 0.0891, 5e5],\n            ""HESS2"": [2.05e9, 0.0891, 1e5],\n            ""CTA"": [1.71e11, 0.0891, 1e5],\n        }\n\n        if instrument not in pars.keys():\n            ss = f""Unknown instrument: {instrument}\\n""\n            ss += ""Valid instruments: HESS, HESS2, CTA""\n            raise ValueError(ss)\n\n        xx = MapAxis.from_edges(energy, interp=""log"").center.to_value(""MeV"")\n\n        g1 = pars[instrument][0]\n        g2 = pars[instrument][1]\n        g3 = -pars[instrument][2]\n\n        value = g1 * xx ** (-g2) * np.exp(g3 / xx)\n        data = u.Quantity(value, ""cm2"", copy=False)\n\n        return cls(energy_lo=energy[:-1], energy_hi=energy[1:], data=data)\n\n    @classmethod\n    def from_constant(cls, energy, value):\n        """"""Create constant value effective area.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy binning, analytic function is evaluated at log centers\n        value : `~astropy.units.Quantity`\n            Effective area\n        """"""\n        data = np.ones((len(energy) - 1)) * u.Quantity(value)\n        return cls(energy_lo=energy[:-1], energy_hi=energy[1:], data=data)\n\n    @classmethod\n    def from_table(cls, table):\n        """"""Create from `~astropy.table.Table` in ARF format.\n\n        Data format specification: :ref:`gadf:ogip-arf`\n        """"""\n        energy_lo = table[""ENERG_LO""].quantity\n        energy_hi = table[""ENERG_HI""].quantity\n        data = table[""SPECRESP""].quantity\n        return cls(energy_lo=energy_lo, energy_hi=energy_hi, data=data)\n\n    @classmethod\n    def from_hdulist(cls, hdulist, hdu=""SPECRESP""):\n        """"""Create from `~astropy.io.fits.HDUList`.""""""\n        return cls.from_table(Table.read(hdulist[hdu]))\n\n    @classmethod\n    def read(cls, filename, hdu=""SPECRESP""):\n        """"""Read from file.""""""\n        filename = make_path(filename)\n        with fits.open(filename, memmap=False) as hdulist:\n            try:\n                return cls.from_hdulist(hdulist, hdu=hdu)\n            except KeyError:\n                raise ValueError(\n                    f""File {filename} contains no HDU {hdu!r}\\n""\n                    f""Available: {[_.name for _ in hdulist]}""\n                )\n\n    def to_table(self):\n        """"""Convert to `~astropy.table.Table` in ARF format.\n\n        Data format specification: :ref:`gadf:ogip-arf`\n        """"""\n        table = Table()\n        table.meta = {\n            ""EXTNAME"": ""SPECRESP"",\n            ""hduclass"": ""OGIP"",\n            ""hduclas1"": ""RESPONSE"",\n            ""hduclas2"": ""SPECRESP"",\n        }\n\n        energy = self.energy.edges\n        table[""ENERG_LO""] = energy[:-1]\n        table[""ENERG_HI""] = energy[1:]\n        table[""SPECRESP""] = self.evaluate_fill_nan()\n        return table\n\n    def to_hdulist(self, name=None, use_sherpa=False):\n        """"""Convert to `~astropy.io.fits.HDUList`.""""""\n        table = self.to_table()\n\n        if use_sherpa:\n            table[""ENERG_HI""] = table[""ENERG_HI""].quantity.to(""keV"")\n            table[""ENERG_LO""] = table[""ENERG_LO""].quantity.to(""keV"")\n            table[""SPECRESP""] = table[""SPECRESP""].quantity.to(""cm2"")\n\n        return fits.HDUList([fits.PrimaryHDU(), fits.BinTableHDU(table, name=name)])\n\n    def write(self, filename, use_sherpa=False, **kwargs):\n        """"""Write to file.""""""\n        filename = make_path(filename)\n        self.to_hdulist(use_sherpa=use_sherpa).writeto(filename, **kwargs)\n\n    def evaluate_fill_nan(self, **kwargs):\n        """"""Modified evaluate function.\n\n        Calls :func:`gammapy.utils.nddata.NDDataArray.evaluate` and replaces\n        possible nan values. Below the finite range the effective area is set\n        to zero and above to value of the last valid note. This is needed since\n        other codes, e.g. sherpa, don\'t like nan values in FITS files. Make\n        sure that the replacement happens outside of the energy range, where\n        the `~gammapy.irf.EffectiveAreaTable` is used.\n        """"""\n        retval = self.data.evaluate(**kwargs)\n        idx = np.where(np.isfinite(retval))[0]\n        retval[np.arange(idx[0])] = 0\n        retval[np.arange(idx[-1], len(retval))] = retval[idx[-1]]\n        return retval\n\n    @property\n    def max_area(self):\n        """"""Maximum effective area.""""""\n        cleaned_data = self.data.data[np.where(~np.isnan(self.data.data))]\n        return cleaned_data.max()\n\n    def find_energy(self, aeff, emin=None, emax=None):\n        """"""Find energy for a given effective area.\n\n        In case the solution is not unique, provide the `emin` or `emax` arguments\n        to limit the solution to the given range. By default the peak energy of the\n        effective area is chosen as `emax`.\n\n        Parameters\n        ----------\n        aeff : `~astropy.units.Quantity`\n            Effective area value\n        emin : `~astropy.units.Quantity`\n            Lower bracket value in case solution is not unique.\n        emax : `~astropy.units.Quantity`\n            Upper bracket value in case solution is not unique.\n\n        Returns\n        -------\n        energy : `~astropy.units.Quantity`\n            Energy corresponding to the given aeff.\n        """"""\n        from gammapy.modeling.models import TemplateSpectralModel\n\n        energy = self.energy.center\n\n        if emin is None:\n            emin = energy[0]\n        if emax is None:\n            # use the peak effective area as a default for the energy maximum\n            emax = energy[np.argmax(self.data.data)]\n\n        aeff_spectrum = TemplateSpectralModel(energy, self.data.data)\n        return aeff_spectrum.inverse(aeff, emin=emin, emax=emax)\n\n\nclass EffectiveAreaTable2D:\n    """"""2D effective area table.\n\n    Data format specification: :ref:`gadf:aeff_2d`\n\n    Parameters\n    ----------\n    energy_lo, energy_hi : `~astropy.units.Quantity`\n        Energy binning\n    offset_lo, offset_hi : `~astropy.units.Quantity`\n        Field of view offset angle.\n    data : `~astropy.units.Quantity`\n        Effective area\n\n    Examples\n    --------\n    Here\'s an example you can use to learn about this class:\n\n    >>> from gammapy.irf import EffectiveAreaTable2D\n    >>> filename = \'$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits\'\n    >>> aeff = EffectiveAreaTable2D.read(filename, hdu=\'EFFECTIVE AREA\')\n    >>> print(aeff)\n    EffectiveAreaTable2D\n    NDDataArray summary info\n    energy         : size =    42, min =  0.014 TeV, max = 177.828 TeV\n    offset         : size =     6, min =  0.500 deg, max =  5.500 deg\n    Data           : size =   252, min =  0.000 m2, max = 5371581.000 m2\n\n    Here\'s another one, created from scratch, without reading a file:\n\n    >>> from gammapy.irf import EffectiveAreaTable2D\n    >>> import astropy.units as u\n    >>> import numpy as np\n    >>> energy = np.logspace(0,1,11) * u.TeV\n    >>> offset = np.linspace(0,1,4) * u.deg\n    >>> data = np.ones(shape=(10,3)) * u.cm * u.cm\n    >>> aeff = EffectiveAreaTable2D(energy_lo=energy[:-1], energy_hi=energy[1:], offset_lo=offset[:-1],\n    >>>                             offset_hi=offset[1:], data= data)\n    >>> print(aeff)\n    Data array summary info\n    energy         : size =    11, min =  1.000 TeV, max = 10.000 TeV\n    offset         : size =     4, min =  0.000 deg, max =  1.000 deg\n    Data           : size =    30, min =  1.000 cm2, max =  1.000 cm2\n    """"""\n\n    default_interp_kwargs = dict(bounds_error=False, fill_value=None)\n    """"""Default Interpolation kwargs for `~NDDataArray`. Extrapolate.""""""\n\n    def __init__(\n        self,\n        energy_lo,\n        energy_hi,\n        offset_lo,\n        offset_hi,\n        data,\n        meta=None,\n        interp_kwargs=None,\n    ):\n\n        if interp_kwargs is None:\n            interp_kwargs = self.default_interp_kwargs\n\n        e_edges = edges_from_lo_hi(energy_lo, energy_hi)\n        energy_axis = MapAxis.from_edges(e_edges, interp=""log"", name=""energy_true"")\n\n        # TODO: for some reason the H.E.S.S. DL3 files contain the same values for offset_hi and offset_lo\n        if np.allclose(offset_lo.to_value(""deg""), offset_hi.to_value(""deg"")):\n            offset_axis = MapAxis.from_nodes(offset_lo, interp=""lin"", name=""offset"")\n        else:\n            offset_edges = edges_from_lo_hi(offset_lo, offset_hi)\n            offset_axis = MapAxis.from_edges(offset_edges, interp=""lin"", name=""offset"")\n\n        self.data = NDDataArray(\n            axes=[energy_axis, offset_axis], data=data, interp_kwargs=interp_kwargs\n        )\n        self.meta = meta or {}\n\n    def __str__(self):\n        ss = self.__class__.__name__\n        ss += f""\\n{self.data}""\n        return ss\n\n    @property\n    def low_threshold(self):\n        """"""Low energy threshold""""""\n        return self.meta[""LO_THRES""] * u.TeV\n\n    @property\n    def high_threshold(self):\n        """"""High energy threshold""""""\n        return self.meta[""HI_THRES""] * u.TeV\n\n    @classmethod\n    def from_table(cls, table):\n        """"""Read from `~astropy.table.Table`.""""""\n        return cls(\n            energy_lo=table[""ENERG_LO""].quantity[0],\n            energy_hi=table[""ENERG_HI""].quantity[0],\n            offset_lo=table[""THETA_LO""].quantity[0],\n            offset_hi=table[""THETA_HI""].quantity[0],\n            data=table[""EFFAREA""].quantity[0].transpose(),\n            meta=table.meta,\n        )\n\n    @classmethod\n    def from_hdulist(cls, hdulist, hdu=""EFFECTIVE AREA""):\n        """"""Create from `~astropy.io.fits.HDUList`.""""""\n        return cls.from_table(Table.read(hdulist[hdu]))\n\n    @classmethod\n    def read(cls, filename, hdu=""EFFECTIVE AREA""):\n        """"""Read from file.""""""\n        with fits.open(make_path(filename), memmap=False) as hdulist:\n            return cls.from_hdulist(hdulist, hdu=hdu)\n\n    def to_effective_area_table(self, offset, energy=None):\n        """"""Evaluate at a given offset and return `~gammapy.irf.EffectiveAreaTable`.\n\n        Parameters\n        ----------\n        offset : `~astropy.coordinates.Angle`\n            Offset\n        energy : `~astropy.units.Quantity`\n            Energy axis bin edges\n        """"""\n        if energy is None:\n            energy = self.data.axis(""energy_true"").edges\n\n        area = self.data.evaluate(\n            offset=offset, energy_true=MapAxis.from_edges(energy, interp=""log"").center\n        )\n\n        return EffectiveAreaTable(\n            energy_lo=energy[:-1], energy_hi=energy[1:], data=area\n        )\n\n    def plot_energy_dependence(self, ax=None, offset=None, energy=None, **kwargs):\n        """"""Plot effective area versus energy for a given offset.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        offset : `~astropy.coordinates.Angle`\n            Offset\n        energy : `~astropy.units.Quantity`\n            Energy axis\n        kwargs : dict\n            Forwarded tp plt.plot()\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axis\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        if offset is None:\n            off_min, off_max = self.data.axis(""offset"").center[[0, -1]]\n            offset = np.linspace(off_min.value, off_max.value, 4) * off_min.unit\n\n        if energy is None:\n            energy = self.data.axis(""energy_true"").center\n\n        for off in offset:\n            area = self.data.evaluate(offset=off, energy_true=energy)\n            label = f""offset = {off:.1f}""\n            ax.plot(energy, area.value, label=label, **kwargs)\n\n        ax.set_xscale(""log"")\n        ax.set_xlabel(f""Energy [{energy.unit}]"")\n        ax.set_ylabel(f""Effective Area [{self.data.data.unit}]"")\n        ax.set_xlim(min(energy.value), max(energy.value))\n        ax.legend(loc=""upper left"")\n\n        return ax\n\n    def plot_offset_dependence(self, ax=None, offset=None, energy=None, **kwargs):\n        """"""Plot effective area versus offset for a given energy.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        offset : `~astropy.coordinates.Angle`\n            Offset axis\n        energy : `~astropy.units.Quantity`\n            Energy\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axis\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        if energy is None:\n            e_min, e_max = np.log10(self.data.axis(""energy_true"").center.value[[0, -1]])\n            energy = np.logspace(e_min, e_max, 4) * self.data.axis(""energy_true"").unit\n\n        if offset is None:\n            offset = self.data.axis(""offset"").center\n\n        for ee in energy:\n            area = self.data.evaluate(offset=offset, energy_true=ee)\n            area /= np.nanmax(area)\n            if np.isnan(area).all():\n                continue\n            label = f""energy = {ee:.1f}""\n            ax.plot(offset, area, label=label, **kwargs)\n\n        ax.set_ylim(0, 1.1)\n        ax.set_xlabel(f""Offset ({self.data.axis(\'offset\').unit})"")\n        ax.set_ylabel(""Relative Effective Area"")\n        ax.legend(loc=""best"")\n\n        return ax\n\n    def plot(self, ax=None, add_cbar=True, **kwargs):\n        """"""Plot effective area image.""""""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        energy = self.data.axis(""energy_true"").edges\n        offset = self.data.axis(""offset"").edges\n        aeff = self.data.evaluate(offset=offset, energy_true=energy[:, np.newaxis])\n\n        vmin, vmax = np.nanmin(aeff.value), np.nanmax(aeff.value)\n\n        kwargs.setdefault(""cmap"", ""GnBu"")\n        kwargs.setdefault(""edgecolors"", ""face"")\n        kwargs.setdefault(""vmin"", vmin)\n        kwargs.setdefault(""vmax"", vmax)\n\n        caxes = ax.pcolormesh(energy.value, offset.value, aeff.value.T, **kwargs)\n\n        ax.set_xscale(""log"")\n        ax.set_ylabel(f""Offset ({offset.unit})"")\n        ax.set_xlabel(f""Energy ({energy.unit})"")\n\n        xmin, xmax = energy.value.min(), energy.value.max()\n        ax.set_xlim(xmin, xmax)\n\n        if add_cbar:\n            label = f""Effective Area ({aeff.unit})""\n            ax.figure.colorbar(caxes, ax=ax, label=label)\n\n        return ax\n\n    def peek(self, figsize=(15, 5)):\n        """"""Quick-look summary plots.""""""\n        import matplotlib.pyplot as plt\n\n        fig, axes = plt.subplots(nrows=1, ncols=3, figsize=figsize)\n        self.plot(ax=axes[2])\n        self.plot_energy_dependence(ax=axes[0])\n        self.plot_offset_dependence(ax=axes[1])\n        plt.tight_layout()\n\n    def to_table(self):\n        """"""Convert to `~astropy.table.Table`.""""""\n        meta = self.meta.copy()\n\n        energy = self.data.axis(""energy_true"").edges\n        theta = self.data.axis(""offset"").edges\n\n        table = Table(meta=meta)\n        table[""ENERG_LO""] = energy[:-1][np.newaxis]\n        table[""ENERG_HI""] = energy[1:][np.newaxis]\n        table[""THETA_LO""] = theta[:-1][np.newaxis]\n        table[""THETA_HI""] = theta[1:][np.newaxis]\n        table[""EFFAREA""] = self.data.data.T[np.newaxis]\n        return table\n\n    def to_fits(self, name=""EFFECTIVE AREA""):\n        """"""Convert to `~astropy.io.fits.BinTableHDU`.""""""\n        return fits.BinTableHDU(self.to_table(), name=name)\n'"
gammapy/irf/energy_dispersion.py,18,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nimport scipy.special\nfrom scipy.interpolate import interp1d\nfrom astropy.coordinates import Angle\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom astropy.units import Quantity\nfrom gammapy.maps import MapAxis\nfrom gammapy.maps.utils import edges_from_lo_hi\nfrom gammapy.utils.nddata import NDDataArray\nfrom gammapy.utils.scripts import make_path\nfrom .edisp_kernel import EDispKernel\n\n__all__ = [""EnergyDispersion2D""]\n\n\nclass EnergyDispersion2D:\n    """"""Offset-dependent energy dispersion matrix.\n\n    Data format specification: :ref:`gadf:edisp_2d`\n\n    Parameters\n    ----------\n    e_true_lo, e_true_hi : `~astropy.units.Quantity`\n        True energy axis binning\n    migra_lo, migra_hi : `~numpy.ndarray`\n        Energy migration axis binning\n    offset_lo, offset_hi : `~astropy.coordinates.Angle`\n        Field of view offset axis binning\n    data : `~numpy.ndarray`\n        Energy dispersion probability density\n\n    Examples\n    --------\n    Read energy dispersion IRF from disk:\n\n    >>> from gammapy.maps import MapAxis\n    >>> from gammapy.irf import EnergyDispersion2D\n    >>> filename = \'$GAMMAPY_DATA/hess-dl3-dr1/data/hess_dl3_dr1_obs_id_020136.fits.gz\'\n    >>> edisp2d = EnergyDispersion2D.read(filename, hdu=""EDISP"")\n\n    Create energy dispersion matrix (`~gammapy.irf.EnergyDispersion`)\n    for a given field of view offset and energy binning:\n\n    >>> energy = MapAxis.from_bounds(0.1, 20, nbin=60, unit=""TeV"", interp=""log"").edges\n    >>> edisp = edisp2d.to_energy_dispersion(offset=\'1.2 deg\', e_reco=energy, e_true=energy)\n\n    See Also\n    --------\n    EnergyDispersion\n    """"""\n\n    default_interp_kwargs = dict(bounds_error=False, fill_value=None)\n    """"""Default Interpolation kwargs for `~gammapy.utils.nddata.NDDataArray`. Extrapolate.""""""\n\n    def __init__(\n        self,\n        e_true_lo,\n        e_true_hi,\n        migra_lo,\n        migra_hi,\n        offset_lo,\n        offset_hi,\n        data,\n        interp_kwargs=None,\n        meta=None,\n    ):\n        if interp_kwargs is None:\n            interp_kwargs = self.default_interp_kwargs\n\n        e_true_edges = edges_from_lo_hi(e_true_lo, e_true_hi)\n        e_true_axis = MapAxis.from_edges(e_true_edges, interp=""log"", name=""energy_true"")\n\n        migra_edges = edges_from_lo_hi(migra_lo, migra_hi)\n        migra_axis = MapAxis.from_edges(\n            migra_edges, interp=""lin"", name=""migra"", unit=""""\n        )\n\n        # TODO: for some reason the H.E.S.S. DL3 files contain the same values for offset_hi and offset_lo\n        if np.allclose(offset_lo.to_value(""deg""), offset_hi.to_value(""deg"")):\n            offset_axis = MapAxis.from_nodes(offset_lo, interp=""lin"", name=""offset"")\n        else:\n            offset_edges = edges_from_lo_hi(offset_lo, offset_hi)\n            offset_axis = MapAxis.from_edges(offset_edges, interp=""lin"", name=""offset"")\n\n        axes = [e_true_axis, migra_axis, offset_axis]\n\n        self.data = NDDataArray(axes=axes, data=data, interp_kwargs=interp_kwargs)\n        self.meta = meta or {}\n\n    def __str__(self):\n        ss = self.__class__.__name__\n        ss += f""\\n{self.data}""\n        return ss\n\n    @classmethod\n    def from_gauss(cls, e_true, migra, bias, sigma, offset, pdf_threshold=1e-6):\n        """"""Create Gaussian energy dispersion matrix (`EnergyDispersion2D`).\n\n        The output matrix will be Gaussian in (e_true / e_reco).\n\n        The ``bias`` and ``sigma`` should be either floats or arrays of same dimension than\n        ``e_true``. ``bias`` refers to the mean value of the ``migra``\n        distribution minus one, i.e. ``bias=0`` means no bias.\n\n        Note that, the output matrix is flat in offset.\n\n        Parameters\n        ----------\n        e_true : `~astropy.units.Quantity`\n            Bin edges of true energy axis\n        migra : `~astropy.units.Quantity`\n            Bin edges of migra axis\n        bias : float or `~numpy.ndarray`\n            Center of Gaussian energy dispersion, bias\n        sigma : float or `~numpy.ndarray`\n            RMS width of Gaussian energy dispersion, resolution\n        offset : `~astropy.units.Quantity`\n            Bin edges of offset\n        pdf_threshold : float, optional\n            Zero suppression threshold\n        """"""\n        e_true = Quantity(e_true)\n        # erf does not work with Quantities\n        true = MapAxis.from_edges(e_true, interp=""log"").center.to_value(""TeV"")\n\n        true2d, migra2d = np.meshgrid(true, migra)\n\n        migra2d_lo = migra2d[:-1, :]\n        migra2d_hi = migra2d[1:, :]\n\n        # Analytical formula for integral of Gaussian\n        s = np.sqrt(2) * sigma\n        t1 = (migra2d_hi - 1 - bias) / s\n        t2 = (migra2d_lo - 1 - bias) / s\n        pdf = (scipy.special.erf(t1) - scipy.special.erf(t2)) / 2\n\n        pdf_array = pdf.T[:, :, np.newaxis] * np.ones(len(offset) - 1)\n\n        pdf_array[pdf_array < pdf_threshold] = 0\n\n        return cls(\n            e_true[:-1],\n            e_true[1:],\n            migra[:-1],\n            migra[1:],\n            offset[:-1],\n            offset[1:],\n            pdf_array,\n        )\n\n    @classmethod\n    def from_table(cls, table):\n        """"""Create from `~astropy.table.Table`.""""""\n        if ""ENERG_LO"" in table.colnames:\n            e_lo = table[""ENERG_LO""].quantity[0]\n            e_hi = table[""ENERG_HI""].quantity[0]\n        elif ""ETRUE_LO"" in table.colnames:\n            e_lo = table[""ETRUE_LO""].quantity[0]\n            e_hi = table[""ETRUE_HI""].quantity[0]\n        else:\n            raise ValueError(\n                \'Invalid column names. Need ""ENERG_LO/ENERG_HI"" or ""ETRUE_LO/ETRUE_HI""\'\n            )\n        o_lo = table[""THETA_LO""].quantity[0]\n        o_hi = table[""THETA_HI""].quantity[0]\n        m_lo = table[""MIGRA_LO""].quantity[0]\n        m_hi = table[""MIGRA_HI""].quantity[0]\n\n        # TODO Why does this need to be transposed?\n        matrix = table[""MATRIX""].quantity[0].transpose()\n\n        return cls(\n            e_true_lo=e_lo,\n            e_true_hi=e_hi,\n            offset_lo=o_lo,\n            offset_hi=o_hi,\n            migra_lo=m_lo,\n            migra_hi=m_hi,\n            data=matrix,\n        )\n\n    @classmethod\n    def from_hdulist(cls, hdulist, hdu=""edisp_2d""):\n        """"""Create from `~astropy.io.fits.HDUList`.""""""\n        return cls.from_table(Table.read(hdulist[hdu]))\n\n    @classmethod\n    def read(cls, filename, hdu=""edisp_2d""):\n        """"""Read from FITS file.\n\n        Parameters\n        ----------\n        filename : str\n            File name\n        """"""\n        with fits.open(make_path(filename), memmap=False) as hdulist:\n            return cls.from_hdulist(hdulist, hdu)\n\n    def to_energy_dispersion(self, offset, e_true=None, e_reco=None):\n        """"""Detector response R(Delta E_reco, Delta E_true)\n\n        Probability to reconstruct an energy in a given true energy band\n        in a given reconstructed energy band\n\n        Parameters\n        ----------\n        offset : `~astropy.coordinates.Angle`\n            Offset\n        e_true : `~astropy.units.Quantity`, None\n            True energy axis\n        e_reco : `~astropy.units.Quantity`\n            Reconstructed energy axis\n\n        Returns\n        -------\n        edisp : `~gammapy.irf.EnergyDispersion`\n            Energy dispersion matrix\n        """"""\n        offset = Angle(offset)\n        e_true = self.data.axis(""energy_true"").edges if e_true is None else e_true\n        e_reco = self.data.axis(""energy_true"").edges if e_reco is None else e_reco\n\n        data = []\n        for energy in MapAxis.from_edges(e_true, interp=""log"").center:\n            vec = self.get_response(offset=offset, e_true=energy, e_reco=e_reco)\n            data.append(vec)\n\n        data = np.asarray(data)\n        e_lo, e_hi = e_true[:-1], e_true[1:]\n        ereco_lo, ereco_hi = (e_reco[:-1], e_reco[1:])\n\n        return EDispKernel(\n            e_true_lo=e_lo,\n            e_true_hi=e_hi,\n            e_reco_lo=ereco_lo,\n            e_reco_hi=ereco_hi,\n            data=data,\n        )\n\n    def get_response(self, offset, e_true, e_reco=None):\n        """"""Detector response R(Delta E_reco, E_true)\n\n        Probability to reconstruct a given true energy in a given reconstructed\n        energy band. In each reco bin, you integrate with a riemann sum over\n        the default migra bin of your analysis.\n\n        Parameters\n        ----------\n        e_true : `~astropy.units.Quantity`\n            True energy\n        e_reco : `~astropy.units.Quantity`, None\n            Reconstructed energy axis\n        offset : `~astropy.coordinates.Angle`\n            Offset\n\n        Returns\n        -------\n        rv : `~numpy.ndarray`\n            Redistribution vector\n        """"""\n        e_true = Quantity(e_true)\n\n        migra_axis = self.data.axis(""migra"")\n\n        if e_reco is None:\n            # Default: e_reco nodes = migra nodes * e_true nodes\n            e_reco = migra_axis.edges * e_true\n        else:\n            # Translate given e_reco binning to migra at bin center\n            e_reco = Quantity(e_reco)\n\n        # migration value of e_reco bounds\n        migra = e_reco / e_true\n\n        values = self.data.evaluate(\n            offset=offset, energy_true=e_true, migra=migra_axis.center\n        )\n\n        cumsum = np.insert(values, 0, 0).cumsum()\n\n        with np.errstate(invalid=""ignore""):\n            cumsum = np.nan_to_num(cumsum / cumsum[-1])\n\n        f = interp1d(\n            migra_axis.edges.value,\n            cumsum,\n            kind=""linear"",\n            bounds_error=False,\n            fill_value=(0, 1),\n        )\n\n        # We compute the difference between 2 successive bounds in e_reco\n        # to get integral over reco energy bin\n        integral = np.diff(np.clip(f(migra), a_min=0, a_max=1))\n\n        return integral\n\n    def plot_migration(self, ax=None, offset=None, e_true=None, migra=None, **kwargs):\n        """"""Plot energy dispersion for given offset and true energy.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        offset : `~astropy.coordinates.Angle`, optional\n            Offset\n        e_true : `~astropy.units.Quantity`, optional\n            True energy\n        migra : `~numpy.ndarray`, optional\n            Migration nodes\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axis\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        if offset is None:\n            offset = Angle([1], ""deg"")\n        else:\n            offset = np.atleast_1d(Angle(offset))\n\n        if e_true is None:\n            e_true = Quantity([0.1, 1, 10], ""TeV"")\n        else:\n            e_true = np.atleast_1d(Quantity(e_true))\n\n        migra = self.data.axis(""migra"").center if migra is None else migra\n\n        for ener in e_true:\n            for off in offset:\n                disp = self.data.evaluate(offset=off, energy_true=ener, migra=migra)\n                label = f""offset = {off:.1f}\\nenergy = {ener:.1f}""\n                ax.plot(migra, disp, label=label, **kwargs)\n\n        ax.set_xlabel(r""$E_\\mathrm{{Reco}} / E_\\mathrm{{True}}$"")\n        ax.set_ylabel(""Probability density"")\n        ax.legend(loc=""upper left"")\n\n        return ax\n\n    def plot_bias(self, ax=None, offset=None, add_cbar=False, **kwargs):\n        """"""Plot migration as a function of true energy for a given offset.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        offset : `~astropy.coordinates.Angle`, optional\n            Offset\n        add_cbar : bool\n            Add a colorbar to the plot.\n        kwargs : dict\n            Keyword arguments passed to `~matplotlib.pyplot.pcolormesh`.\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`\n            Axis\n        """"""\n        from matplotlib.colors import PowerNorm\n        import matplotlib.pyplot as plt\n\n        kwargs.setdefault(""cmap"", ""GnBu"")\n        kwargs.setdefault(""norm"", PowerNorm(gamma=0.5))\n\n        ax = plt.gca() if ax is None else ax\n\n        if offset is None:\n            offset = Angle(1, ""deg"")\n\n        e_true = self.data.axis(""energy_true"")\n        migra = self.data.axis(""migra"")\n\n        x = e_true.edges.value\n        y = migra.edges.value\n\n        z = self.data.evaluate(\n            offset=offset,\n            energy_true=e_true.center.reshape(1, -1, 1),\n            migra=migra.center.reshape(1, 1, -1),\n        ).value[0]\n\n        caxes = ax.pcolormesh(x, y, z.T, **kwargs)\n\n        if add_cbar:\n            label = ""Probability density (A.U.)""\n            ax.figure.colorbar(caxes, ax=ax, label=label)\n\n        ax.set_xlabel(fr""$E_\\mathrm{{True}}$ [{e_true.unit}]"")\n        ax.set_ylabel(r""$E_\\mathrm{{Reco}} / E_\\mathrm{{True}}$"")\n        ax.set_xlim(x.min(), x.max())\n        ax.set_ylim(y.min(), y.max())\n        ax.set_xscale(""log"")\n        return ax\n\n    def peek(self, figsize=(15, 5)):\n        """"""Quick-look summary plots.\n\n        Parameters\n        ----------\n        figsize : (float, float)\n            Size of the resulting plot\n        """"""\n        import matplotlib.pyplot as plt\n\n        fig, axes = plt.subplots(nrows=1, ncols=3, figsize=figsize)\n        self.plot_bias(ax=axes[0])\n        self.plot_migration(ax=axes[1])\n        edisp = self.to_energy_dispersion(offset=""1 deg"")\n        edisp.plot_matrix(ax=axes[2])\n\n        plt.tight_layout()\n\n    def to_table(self):\n        """"""Convert to `~astropy.table.Table`.""""""\n        meta = self.meta.copy()\n\n        energy = self.data.axis(""energy_true"").edges\n        migra = self.data.axis(""migra"").edges\n        theta = self.data.axis(""offset"").edges\n\n        table = Table(meta=meta)\n        table[""ENERG_LO""] = energy[:-1][np.newaxis]\n        table[""ENERG_HI""] = energy[1:][np.newaxis]\n        table[""MIGRA_LO""] = migra[:-1][np.newaxis]\n        table[""MIGRA_HI""] = migra[1:][np.newaxis]\n        table[""THETA_LO""] = theta[:-1][np.newaxis]\n        table[""THETA_HI""] = theta[1:][np.newaxis]\n        table[""MATRIX""] = self.data.data.T[np.newaxis]\n        return table\n\n    def to_fits(self, name=""ENERGY DISPERSION""):\n        """"""Convert to `~astropy.io.fits.BinTable`.""""""\n        return fits.BinTableHDU(self.to_table(), name=name)\n'"
gammapy/irf/io.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom .background import Background3D\nfrom .effective_area import EffectiveAreaTable2D\nfrom .energy_dispersion import EnergyDispersion2D\nfrom .psf_gauss import EnergyDependentMultiGaussPSF\n\n__all__ = [""load_cta_irfs""]\n\n\ndef load_cta_irfs(filename):\n    """"""load CTA instrument response function and return a dictionary container.\n\n    The IRF format should be compliant with the one discussed\n    at http://gamma-astro-data-formats.readthedocs.io/en/latest/irfs/.\n\n    The various IRFs are accessible with the following keys:\n\n    - \'aeff\' is a `~gammapy.irf.EffectiveAreaTable2D`\n    - \'edisp\'  is a `~gammapy.irf.EnergyDispersion2D`\n    - \'psf\' is a `~gammapy.irf.EnergyDependentMultiGaussPSF`\n    - \'bkg\' is a  `~gammapy.irf.Background3D`\n\n    Parameters\n    ----------\n    filename : str\n        the input filename. Default is\n\n    Returns\n    -------\n    cta_irf : dict\n        the IRF dictionary\n\n    Examples\n    --------\n    Access the CTA 1DC IRFs stored in the gammapy datasets\n\n    .. code-block:: python\n\n        from gammapy.irf import load_cta_irfs\n        cta_irf = load_cta_irfs(""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits"")\n        print(cta_irf[\'aeff\'])\n    """"""\n    aeff = EffectiveAreaTable2D.read(filename, hdu=""EFFECTIVE AREA"")\n    bkg = Background3D.read(filename, hdu=""BACKGROUND"")\n    edisp = EnergyDispersion2D.read(filename, hdu=""ENERGY DISPERSION"")\n    psf = EnergyDependentMultiGaussPSF.read(filename, hdu=""POINT SPREAD FUNCTION"")\n\n    return dict(aeff=aeff, bkg=bkg, edisp=edisp, psf=psf)\n'"
gammapy/irf/irf_map.py,3,"b'from copy import deepcopy\nimport numpy as np\nfrom astropy.io import fits\nfrom gammapy.maps import Map\n\n\nclass IRFMap:\n    """"""IRF map base class""""""\n\n    def __init__(self, irf_map, exposure_map):\n        self._irf_map = irf_map\n        self.exposure_map = exposure_map\n\n    @classmethod\n    def from_hdulist(\n        cls,\n        hdulist,\n        hdu=None,\n        hdu_bands=None,\n        exposure_hdu=None,\n        exposure_hdu_bands=None,\n    ):\n        """"""Create from `~astropy.io.fits.HDUList`.\n\n        Parameters\n        ----------\n        hdulist : `~astropy.fits.HDUList`\n            HDU list.\n        hdu : str\n            Name or index of the HDU with the IRF map.\n        hdu_bands : str\n            Name or index of the HDU with the IRF map BANDS table.\n        exposure_hdu : str\n            Name or index of the HDU with the exposure map data.\n        exposure_hdu_bands : str\n            Name or index of the HDU with the exposure map BANDS table.\n\n        Returns\n        -------\n        irf_map : `IRFMap`\n            IRF map.\n        """"""\n        if hdu is None:\n            hdu = cls._hdu_name\n\n        irf_map = Map.from_hdulist(hdulist, hdu=hdu, hdu_bands=hdu_bands)\n\n        if exposure_hdu is None:\n            exposure_hdu = cls._hdu_name + ""_exposure""\n\n        if exposure_hdu in hdulist:\n            exposure_map = Map.from_hdulist(\n                hdulist, hdu=exposure_hdu, hdu_bands=exposure_hdu_bands\n            )\n        else:\n            exposure_map = None\n\n        return cls(irf_map, exposure_map)\n\n    @classmethod\n    def read(cls, filename):\n        """"""Read an IRF_map from file and create corresponding object""""""\n        with fits.open(filename, memmap=False) as hdulist:\n            return cls.from_hdulist(hdulist)\n\n    def to_hdulist(self):\n        """"""Convert to `~astropy.io.fits.HDUList`.\n\n        Returns\n        -------\n        hdu_list : `~astropy.io.fits.HDUList`\n            HDU list.\n        """"""\n        hdulist = self._irf_map.to_hdulist(hdu=self._hdu_name)\n\n        exposure_hdu = self._hdu_name + ""_exposure""\n\n        if self.exposure_map is not None:\n            new_hdulist = self.exposure_map.to_hdulist(hdu=exposure_hdu)\n            hdulist.extend(new_hdulist[1:])\n\n        return hdulist\n\n    def write(self, filename, overwrite=False, **kwargs):\n        """"""Write IRF map to fits""""""\n        hdulist = self.to_hdulist(**kwargs)\n        hdulist.writeto(filename, overwrite=overwrite)\n\n    def stack(self, other, weights=None):\n        """"""Stack IRF map with another one in place.\n\n        Parameters\n        ----------\n        other : `~gammapy.irf.IRFMap`\n            IRF map to be stacked with this one.\n        weights : `~gammapy.maps.Map`\n            Map with stacking weights.\n\n        """"""\n        if self.exposure_map is None or other.exposure_map is None:\n            raise ValueError(f""Missing exposure map for {self.__class__.__name__}.stack"")\n\n        cutout_info = other._irf_map.geom.cutout_info\n\n        if cutout_info is not None:\n            slices = cutout_info[""parent-slices""]\n            parent_slices = Ellipsis, slices[0], slices[1]\n        else:\n            parent_slices = None\n\n        self._irf_map.data[parent_slices] *= self.exposure_map.data[parent_slices]\n        self._irf_map.stack(other._irf_map * other.exposure_map.data, weights=weights)\n\n        # stack exposure map\n        if weights and ""energy"" in weights.geom.axes_names:\n            weights = weights.reduce_over_axes(func=np.logical_or, axes=[""energy""], keepdims=True)\n        self.exposure_map.stack(other.exposure_map, weights=weights)\n\n        with np.errstate(invalid=""ignore""):\n            self._irf_map.data[parent_slices] /= self.exposure_map.data[parent_slices]\n            self._irf_map.data = np.nan_to_num(self._irf_map.data)\n\n    def copy(self):\n        """"""Copy IRF map""""""\n        return deepcopy(self)\n\n    def cutout(self, position, width, mode=""trim""):\n        """"""Cutout IRF map.\n\n        Parameters\n        ----------\n        position : `~astropy.coordinates.SkyCoord`\n            Center position of the cutout region.\n        width : tuple of `~astropy.coordinates.Angle`\n            Angular sizes of the region in (lon, lat) in that specific order.\n            If only one value is passed, a square region is extracted.\n        mode : {\'trim\', \'partial\', \'strict\'}\n            Mode option for Cutout2D, for details see `~astropy.nddata.utils.Cutout2D`.\n\n        Returns\n        -------\n        cutout : `IRFMap`\n            Cutout IRF map.\n        """"""\n        irf_map = self._irf_map.cutout(position, width, mode)\n        exposure_map = self.exposure_map.cutout(position, width, mode)\n        return self.__class__(irf_map, exposure_map=exposure_map)\n'"
gammapy/irf/irf_reduce.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nfrom .psf_table import EnergyDependentTablePSF\n\n__all__ = [""make_psf"", ""make_mean_psf""]\n\nlog = logging.getLogger(__name__)\n\n\ndef make_psf(observation, position, energy=None, rad=None):\n    """"""Make energy-dependent PSF for a given source position.\n\n    Parameters\n    ----------\n    observation : `~gammapy.data.Observation`\n        Observation for which to compute the PSF\n    position : `~astropy.coordinates.SkyCoord`\n        Position at which to compute the PSF\n    energy : `~astropy.units.Quantity`\n        1-dim energy array for the output PSF.\n        If none is given, the energy array of the PSF from the observation is used.\n    rad : `~astropy.coordinates.Angle`\n        1-dim offset wrt source position array for the output PSF.\n        If none is given, the offset array of the PSF from the observation is used.\n\n    Returns\n    -------\n    psf : `~gammapy.irf.EnergyDependentTablePSF`\n        Energy dependent psf table\n    """"""\n    offset = position.separation(observation.pointing_radec)\n\n    if energy is None:\n        energy = observation.psf.to_energy_dependent_table_psf(theta=offset).energy\n\n    if rad is None:\n        rad = observation.psf.to_energy_dependent_table_psf(theta=offset).rad\n\n    psf_value = observation.psf.to_energy_dependent_table_psf(\n        theta=offset, rad=rad\n    ).evaluate(energy)\n\n    arf = observation.aeff.data.evaluate(offset=offset, energy_true=energy)\n    exposure = arf * observation.observation_live_time_duration\n\n    psf = EnergyDependentTablePSF(\n        energy=energy, rad=rad, exposure=exposure, psf_value=psf_value\n    )\n    return psf\n\n\ndef make_mean_psf(observations, position, energy=None, rad=None):\n    """"""Compute mean energy-dependent PSF.\n\n    Parameters\n    ----------\n    observations : `~gammapy.data.Observations`\n        Observations for which to compute the PSF\n    position : `~astropy.coordinates.SkyCoord`\n        Position at which to compute the PSF\n    energy : `~astropy.units.Quantity`\n        1-dim energy array for the output PSF.\n        If none is given, the energy array of the PSF from the first\n        observation is used.\n    rad : `~astropy.coordinates.Angle`\n        1-dim offset wrt source position array for the output PSF.\n        If none is given, the energy array of the PSF from the first\n        observation is used.\n\n    Returns\n    -------\n    psf : `~gammapy.irf.EnergyDependentTablePSF`\n        Mean PSF\n    """"""\n    for idx, observation in enumerate(observations):\n        psf = make_psf(observation, position, energy, rad)\n        if idx == 0:\n            stacked_psf = psf\n        else:\n            stacked_psf = stacked_psf.stack(psf)\n    return stacked_psf\n'"
gammapy/irf/irf_stack.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport numpy as np\nfrom astropy.units import Quantity\nfrom .edisp_kernel import EDispKernel\nfrom .effective_area import EffectiveAreaTable\n\n__all__ = [""IRFStacker""]\n\nlog = logging.getLogger(__name__)\n\n\nclass IRFStacker:\n    r""""""\n    Stack instrument response functions.\n\n    Compute mean effective area and the mean energy dispersion for a given for a\n    given list of instrument response functions. Results are stored as\n    attributes.\n\n    The stacking of :math:`j` elements is implemented as follows.  :math:`k`\n    and :math:`l` denote a bin in reconstructed and true energy, respectively.\n\n    .. math::\n        \\epsilon_{jk} =\\left\\{\\begin{array}{cl} 1, & \\mbox{if\n            bin k is inside the energy thresholds}\\\\ 0, & \\mbox{otherwise} \\end{array}\\right.\n\n        \\overline{t} = \\sum_{j} t_i\n\n        \\overline{\\mathrm{aeff}}_l = \\frac{\\sum_{j}\\mathrm{aeff}_{jl}\n            \\cdot t_j}{\\overline{t}}\n\n        \\overline{\\mathrm{edisp}}_{kl} = \\frac{\\sum_{j} \\mathrm{edisp}_{jkl}\n            \\cdot \\mathrm{aeff}_{jl} \\cdot t_j \\cdot \\epsilon_{jk}}{\\sum_{j} \\mathrm{aeff}_{jl}\n            \\cdot t_j}\n\n    Parameters\n    ----------\n    list_aeff : list\n        list of `~gammapy.irf.EffectiveAreaTable`\n    list_livetime : list\n        list of `~astropy.units.Quantity` (livetime)\n    list_edisp : list\n        list of `~gammapy.irf.EDispKernel`\n    list_low_threshold : list\n        list of low energy threshold, optional for effective area mean computation\n    list_high_threshold : list\n        list of high energy threshold, optional for effective area mean computation\n    """"""\n\n    def __init__(\n        self,\n        list_aeff,\n        list_livetime,\n        list_edisp=None,\n        list_low_threshold=None,\n        list_high_threshold=None,\n    ):\n        self.list_aeff = list_aeff\n        self.list_livetime = Quantity(list_livetime)\n        self.list_edisp = list_edisp\n        self.list_low_threshold = list_low_threshold\n        self.list_high_threshold = list_high_threshold\n        self.stacked_aeff = None\n        self.stacked_edisp = None\n\n    def stack_aeff(self):\n        """"""\n        Compute mean effective area (`~gammapy.irf.EffectiveAreaTable`).\n        """"""\n        nbins = self.list_aeff[0].energy.nbin\n        aefft = Quantity(np.zeros(nbins), ""cm2 s"")\n        livetime_tot = np.sum(self.list_livetime)\n\n        for i, aeff in enumerate(self.list_aeff):\n            aeff_data = aeff.evaluate_fill_nan()\n            aefft_current = aeff_data * self.list_livetime[i]\n            aefft += aefft_current\n\n        stacked_data = aefft / livetime_tot\n\n        energy = self.list_aeff[0].energy.edges\n        self.stacked_aeff = EffectiveAreaTable(\n            energy_lo=energy[:-1], energy_hi=energy[1:], data=stacked_data.to(""cm2"")\n        )\n\n    def stack_edisp(self):\n        """"""\n        Compute mean energy dispersion (`~gammapy.irf.EDispKernel`).\n        """"""\n        reco_bins = self.list_edisp[0].e_reco.nbin\n        true_bins = self.list_edisp[0].e_true.nbin\n\n        aefft = Quantity(np.zeros(true_bins), ""cm2 s"")\n        temp = np.zeros(shape=(reco_bins, true_bins))\n        aefftedisp = Quantity(temp, ""cm2 s"")\n\n        for i, edisp in enumerate(self.list_edisp):\n            aeff_data = self.list_aeff[i].evaluate_fill_nan()\n            aefft_current = aeff_data * self.list_livetime[i]\n            aefft += aefft_current\n            edisp_data = edisp.pdf_in_safe_range(\n                self.list_low_threshold[i], self.list_high_threshold[i]\n            )\n\n            aefftedisp += edisp_data.transpose() * aefft_current\n\n        with np.errstate(divide=""ignore"", invalid=""ignore""):\n            stacked_edisp = np.nan_to_num(aefftedisp / aefft)\n\n        e_true = self.list_edisp[0].e_true.edges\n        e_reco = self.list_edisp[0].e_reco.edges\n        self.stacked_edisp = EDispKernel(\n            e_true_lo=e_true[:-1],\n            e_true_hi=e_true[1:],\n            e_reco_lo=e_reco[:-1],\n            e_reco_hi=e_reco[1:],\n            data=stacked_edisp.transpose(),\n        )\n'"
gammapy/irf/psf_3d.py,11,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom astropy import units as u\nfrom astropy.coordinates import Angle\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom astropy.utils import lazyproperty\nfrom gammapy.maps import MapAxis\nfrom gammapy.utils.array import array_stats_str\nfrom gammapy.utils.interpolation import ScaledRegularGridInterpolator\nfrom gammapy.utils.scripts import make_path\nfrom .psf_table import EnergyDependentTablePSF, TablePSF\n\n__all__ = [""PSF3D""]\n\n\nclass PSF3D:\n    """"""PSF with axes: energy, offset, rad.\n\n    Data format specification: :ref:`gadf:psf_table`\n\n    Parameters\n    ----------\n    energy_lo : `~astropy.units.Quantity`\n        Energy bins lower edges (1-dim)\n    energy_hi : `~astropy.units.Quantity`\n        Energy bins upper edges (1-dim)\n    offset : `~astropy.coordinates.Angle`\n        Offset angle (1-dim)\n    rad_lo : `~astropy.coordinates.Angle`\n        Offset angle bins lower edges\n    rad_hi : `~astropy.coordinates.Angle`\n        Offset angle bins upper edges\n    psf_value : `~astropy.units.Quantity`\n        PSF (3-dim with axes: psf[rad_index, offset_index, energy_index]\n    energy_thresh_lo : `~astropy.units.Quantity`\n        Lower energy threshold.\n    energy_thresh_hi : `~astropy.units.Quantity`\n        Upper energy threshold.\n    """"""\n\n    def __init__(\n        self,\n        energy_lo,\n        energy_hi,\n        offset,\n        rad_lo,\n        rad_hi,\n        psf_value,\n        energy_thresh_lo=u.Quantity(0.1, ""TeV""),\n        energy_thresh_hi=u.Quantity(100, ""TeV""),\n        interp_kwargs=None,\n    ):\n        self.energy_lo = energy_lo.to(""TeV"")\n        self.energy_hi = energy_hi.to(""TeV"")\n        self.offset = Angle(offset)\n        self.rad_lo = Angle(rad_lo)\n        self.rad_hi = Angle(rad_hi)\n        self.psf_value = psf_value.to(""sr^-1"")\n        self.energy_thresh_lo = energy_thresh_lo.to(""TeV"")\n        self.energy_thresh_hi = energy_thresh_hi.to(""TeV"")\n\n        self._interp_kwargs = interp_kwargs or {}\n\n    @lazyproperty\n    def _interpolate(self):\n        energy = self._energy_logcenter()\n        offset = self.offset.to(""deg"")\n        rad = self._rad_center()\n\n        return ScaledRegularGridInterpolator(\n            points=(rad, offset, energy), values=self.psf_value, **self._interp_kwargs\n        )\n\n    def info(self):\n        """"""Print some basic info.\n        """"""\n        ss = ""\\nSummary PSF3D info\\n""\n        ss += ""---------------------\\n""\n        ss += array_stats_str(self.energy_lo, ""energy_lo"")\n        ss += array_stats_str(self.energy_hi, ""energy_hi"")\n        ss += array_stats_str(self.offset, ""offset"")\n        ss += array_stats_str(self.rad_lo, ""rad_lo"")\n        ss += array_stats_str(self.rad_hi, ""rad_hi"")\n        ss += array_stats_str(self.psf_value, ""psf_value"")\n\n        # TODO: should quote containment values also\n\n        return ss\n\n    def _energy_logcenter(self):\n        """"""Get logcenters of energy bins.\n\n        Returns\n        -------\n        energies : `~astropy.units.Quantity`\n            Logcenters of energy bins\n        """"""\n        return np.sqrt(self.energy_lo * self.energy_hi)\n\n    def _rad_center(self):\n        """"""Get centers of rad bins (`~astropy.coordinates.Angle` in deg).\n        """"""\n        return ((self.rad_hi + self.rad_lo) / 2).to(""deg"")\n\n    @classmethod\n    def read(cls, filename, hdu=""PSF_2D_TABLE""):\n        """"""Create `PSF3D` from FITS file.\n\n        Parameters\n        ----------\n        filename : str\n            File name\n        hdu : str\n            HDU name\n        """"""\n        table = Table.read(make_path(filename), hdu=hdu)\n        return cls.from_table(table)\n\n    @classmethod\n    def from_table(cls, table):\n        """"""Create `PSF3D` from `~astropy.table.Table`.\n\n        Parameters\n        ----------\n        table : `~astropy.table.Table`\n            Table Table-PSF info.\n        """"""\n        theta_lo = table[""THETA_LO""].quantity[0]\n        theta_hi = table[""THETA_HI""].quantity[0]\n        offset = (theta_hi + theta_lo) / 2\n        offset = Angle(offset, unit=table[""THETA_LO""].unit)\n\n        energy_lo = table[""ENERG_LO""].quantity[0]\n        energy_hi = table[""ENERG_HI""].quantity[0]\n\n        rad_lo = table[""RAD_LO""].quantity[0]\n        rad_hi = table[""RAD_HI""].quantity[0]\n\n        psf_value = table[""RPSF""].quantity[0]\n\n        opts = {}\n        try:\n            opts[""energy_thresh_lo""] = u.Quantity(table.meta[""LO_THRES""], ""TeV"")\n            opts[""energy_thresh_hi""] = u.Quantity(table.meta[""HI_THRES""], ""TeV"")\n        except KeyError:\n            pass\n\n        return cls(energy_lo, energy_hi, offset, rad_lo, rad_hi, psf_value, **opts)\n\n    def to_fits(self):\n        """"""\n        Convert PSF table data to FITS HDU list.\n\n        Returns\n        -------\n        hdu_list : `~astropy.io.fits.HDUList`\n            PSF in HDU list format.\n        """"""\n        # Set up data\n        names = [\n            ""ENERG_LO"",\n            ""ENERG_HI"",\n            ""THETA_LO"",\n            ""THETA_HI"",\n            ""RAD_LO"",\n            ""RAD_HI"",\n            ""RPSF"",\n        ]\n        units = [""TeV"", ""TeV"", ""deg"", ""deg"", ""deg"", ""deg"", ""sr^-1""]\n        data = [\n            self.energy_lo,\n            self.energy_hi,\n            self.offset,\n            self.offset,\n            self.rad_lo,\n            self.rad_hi,\n            self.psf_value,\n        ]\n\n        table = Table()\n        for name_, data_, unit_ in zip(names, data, units):\n            table[name_] = [data_]\n            table[name_].unit = unit_\n\n        hdu = fits.BinTableHDU(table)\n        hdu.header[""LO_THRES""] = self.energy_thresh_lo.value\n        hdu.header[""HI_THRES""] = self.energy_thresh_hi.value\n\n        return fits.HDUList([fits.PrimaryHDU(), hdu])\n\n    def write(self, filename, *args, **kwargs):\n        """"""Write PSF to FITS file.\n\n        Calls `~astropy.io.fits.HDUList.writeto`, forwarding all arguments.\n        """"""\n        self.to_fits().writeto(filename, *args, **kwargs)\n\n    def evaluate(self, energy=None, offset=None, rad=None):\n        """"""Interpolate PSF value at a given offset and energy.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            energy value\n        offset : `~astropy.coordinates.Angle`\n            Offset in the field of view\n        rad : `~astropy.coordinates.Angle`\n            Offset wrt source position\n\n        Returns\n        -------\n        values : `~astropy.units.Quantity`\n            Interpolated value\n        """"""\n        if energy is None:\n            energy = self._energy_logcenter()\n        if offset is None:\n            offset = self.offset\n        if rad is None:\n            rad = self._rad_center()\n\n        rad = np.atleast_1d(u.Quantity(rad))\n        offset = np.atleast_1d(u.Quantity(offset))\n        energy = np.atleast_1d(u.Quantity(energy))\n        return self._interpolate(\n            (\n                rad[:, np.newaxis, np.newaxis],\n                offset[np.newaxis, :, np.newaxis],\n                energy[np.newaxis, np.newaxis, :],\n            )\n        )\n\n    def to_energy_dependent_table_psf(self, theta=""0 deg"", rad=None, exposure=None):\n        """"""\n        Convert PSF3D in EnergyDependentTablePSF.\n\n        Parameters\n        ----------\n        theta : `~astropy.coordinates.Angle`\n            Offset in the field of view\n        rad : `~astropy.coordinates.Angle`\n            Offset from PSF center used for evaluating the PSF on a grid.\n            Default is the ``rad`` from this PSF.\n        exposure : `~astropy.units.Quantity`\n            Energy dependent exposure. Should be in units equivalent to \'cm^2 s\'.\n            Default exposure = 1.\n\n        Returns\n        -------\n        table_psf : `~gammapy.irf.EnergyDependentTablePSF`\n            Energy-dependent PSF\n        """"""\n        theta = Angle(theta)\n        energies = self._energy_logcenter()\n\n        if rad is None:\n            rad = self._rad_center()\n        else:\n            rad = Angle(rad)\n\n        psf_value = self.evaluate(offset=theta, rad=rad).squeeze()\n        return EnergyDependentTablePSF(\n            energy=energies, rad=rad, exposure=exposure, psf_value=psf_value.T\n        )\n\n    def to_table_psf(self, energy, theta=""0 deg"", **kwargs):\n        """"""Create `~gammapy.irf.TablePSF` at one given energy.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy\n        theta : `~astropy.coordinates.Angle`\n            Offset in the field of view. Default theta = 0 deg\n\n        Returns\n        -------\n        psf : `~gammapy.irf.TablePSF`\n            Table PSF\n        """"""\n        energy = u.Quantity(energy)\n        theta = Angle(theta)\n        psf_value = self.evaluate(energy, theta).squeeze()\n        rad = self._rad_center()\n        return TablePSF(rad, psf_value, **kwargs)\n\n    def containment_radius(\n        self, energy, theta=""0 deg"", fraction=0.68, interp_kwargs=None\n    ):\n        """"""Containment radius.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy\n        theta : `~astropy.coordinates.Angle`\n            Offset in the field of view. Default theta = 0 deg\n        fraction : float\n            Containment fraction. Default fraction = 0.68\n\n        Returns\n        -------\n        radius : `~astropy.units.Quantity`\n            Containment radius in deg\n        """"""\n        energy = np.atleast_1d(u.Quantity(energy))\n        theta = np.atleast_1d(u.Quantity(theta))\n\n        radii = []\n        for t in theta:\n            psf = self.to_energy_dependent_table_psf(theta=t)\n            radii.append(psf.containment_radius(energy, fraction=fraction))\n\n        return u.Quantity(radii).T.squeeze()\n\n    def plot_containment_vs_energy(\n        self, fractions=[0.68, 0.95], thetas=Angle([0, 1], ""deg""), ax=None\n    ):\n        """"""Plot containment fraction as a function of energy.\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        energy = MapAxis.from_energy_bounds(\n            self.energy_lo[0], self.energy_hi[-1], 100\n        ).edges\n\n        for theta in thetas:\n            for fraction in fractions:\n                radius = self.containment_radius(energy, theta, fraction)\n                label = f""{theta.deg} deg, {100 * fraction:.1f}%""\n                ax.plot(energy.value, radius.value, label=label)\n\n        ax.semilogx()\n        ax.legend(loc=""best"")\n        ax.set_xlabel(""Energy (TeV)"")\n        ax.set_ylabel(""Containment radius (deg)"")\n\n    def plot_psf_vs_rad(self, theta=""0 deg"", energy=u.Quantity(1, ""TeV"")):\n        """"""Plot PSF vs rad.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy. Default energy = 1 TeV\n        theta : `~astropy.coordinates.Angle`\n            Offset in the field of view. Default theta = 0 deg\n        """"""\n        theta = Angle(theta)\n        table = self.to_table_psf(energy=energy, theta=theta)\n        return table.plot_psf_vs_rad()\n\n    def plot_containment(\n        self, fraction=0.68, ax=None, show_safe_energy=False, add_cbar=True, **kwargs\n    ):\n        """"""\n        Plot containment image with energy and theta axes.\n\n        Parameters\n        ----------\n        fraction : float\n            Containment fraction between 0 and 1.\n        add_cbar : bool\n            Add a colorbar\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        energy = self._energy_logcenter()\n        offset = self.offset\n\n        # Set up and compute data\n        containment = self.containment_radius(energy, offset, fraction)\n\n        # plotting defaults\n        kwargs.setdefault(""cmap"", ""GnBu"")\n        kwargs.setdefault(""vmin"", np.nanmin(containment.value))\n        kwargs.setdefault(""vmax"", np.nanmax(containment.value))\n\n        # Plotting\n        x = energy.value\n        y = offset.value\n        caxes = ax.pcolormesh(x, y, containment.value.T, **kwargs)\n\n        # Axes labels and ticks, colobar\n        ax.semilogx()\n        ax.set_ylabel(f""Offset ({offset.unit})"")\n        ax.set_xlabel(f""Energy ({energy.unit})"")\n        ax.set_xlim(x.min(), x.max())\n        ax.set_ylim(y.min(), y.max())\n\n        if show_safe_energy:\n            self._plot_safe_energy_range(ax)\n\n        if add_cbar:\n            label = f""Containment radius R{100 * fraction:.0f} ({containment.unit})""\n            ax.figure.colorbar(caxes, ax=ax, label=label)\n\n        return ax\n\n    def _plot_safe_energy_range(self, ax):\n        """"""add safe energy range lines to the plot""""""\n        esafe = self.energy_thresh_lo\n        omin = self.offset.value.min()\n        omax = self.offset.value.max()\n        ax.hlines(y=esafe.value, xmin=omin, xmax=omax)\n        label = f""Safe energy threshold: {esafe:3.2f}""\n        ax.text(x=0.1, y=0.9 * esafe.value, s=label, va=""top"")\n\n    def peek(self, figsize=(15, 5)):\n        """"""Quick-look summary plots.""""""\n        import matplotlib.pyplot as plt\n\n        fig, axes = plt.subplots(nrows=1, ncols=3, figsize=figsize)\n\n        self.plot_containment(fraction=0.68, ax=axes[0])\n        self.plot_containment(fraction=0.95, ax=axes[1])\n        self.plot_containment_vs_energy(ax=axes[2])\n\n        # TODO: implement this plot\n        # psf = self.psf_at_energy_and_theta(energy=\'1 TeV\', theta=\'1 deg\')\n        # psf.plot_components(ax=axes[2])\n\n        plt.tight_layout()\n'"
gammapy/irf/psf_gauss.py,13,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport numpy as np\nfrom astropy.convolution import Gaussian2DKernel\nfrom astropy.coordinates import Angle\nfrom astropy.io import fits\nfrom astropy.stats import gaussian_fwhm_to_sigma\nfrom astropy.table import Table\nfrom astropy.units import Quantity, Unit\nfrom gammapy.maps import MapAxis\nfrom gammapy.utils.array import array_stats_str\nfrom gammapy.utils.gauss import MultiGauss2D\nfrom gammapy.utils.interpolation import ScaledRegularGridInterpolator\nfrom gammapy.utils.scripts import make_path\nfrom .psf_3d import PSF3D\nfrom .psf_table import EnergyDependentTablePSF\n\n__all__ = [""EnergyDependentMultiGaussPSF""]\n\nlog = logging.getLogger(__name__)\n\n\nclass EnergyDependentMultiGaussPSF:\n    """"""\n    Triple Gauss analytical PSF depending on energy and theta.\n\n    To evaluate the PSF call the ``to_energy_dependent_table_psf`` or ``psf_at_energy_and_theta`` methods.\n\n    Parameters\n    ----------\n    energy_lo : `~astropy.units.Quantity`\n        Lower energy boundary of the energy bin.\n    energy_hi : `~astropy.units.Quantity`\n        Upper energy boundary of the energy bin.\n    theta : `~astropy.units.Quantity`\n        Center values of the theta bins.\n    sigmas : list of \'numpy.ndarray\'\n        Triple Gauss sigma parameters, where every entry is\n        a two dimensional \'numpy.ndarray\' containing the sigma\n        value for every given energy and theta.\n    norms : list of \'numpy.ndarray\'\n        Triple Gauss norm parameters, where every entry is\n        a two dimensional \'numpy.ndarray\' containing the norm\n        value for every given energy and theta. Norm corresponds\n        to the value of the Gaussian at theta = 0.\n    energy_thresh_lo : `~astropy.units.Quantity`\n        Lower save energy threshold of the psf.\n    energy_thresh_hi : `~astropy.units.Quantity`\n        Upper save energy threshold of the psf.\n\n    Examples\n    --------\n    Plot R68 of the PSF vs. theta and energy:\n\n    .. plot::\n        :include-source:\n\n        import matplotlib.pyplot as plt\n        from gammapy.irf import EnergyDependentMultiGaussPSF\n        filename = \'$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits\'\n        psf = EnergyDependentMultiGaussPSF.read(filename, hdu=\'POINT SPREAD FUNCTION\')\n        psf.plot_containment(0.68, show_safe_energy=False)\n        plt.show()\n    """"""\n\n    def __init__(\n        self,\n        energy_lo,\n        energy_hi,\n        theta,\n        sigmas,\n        norms,\n        energy_thresh_lo=""0.1 TeV"",\n        energy_thresh_hi=""100 TeV"",\n    ):\n        self.energy_lo = Quantity(energy_lo, ""TeV"")\n        self.energy_hi = Quantity(energy_hi, ""TeV"")\n        self.energy = np.sqrt(self.energy_hi * self.energy_lo)\n        self.theta = Quantity(theta, ""deg"")\n        sigmas[0][sigmas[0] == 0] = 1\n        sigmas[1][sigmas[1] == 0] = 1\n        sigmas[2][sigmas[2] == 0] = 1\n        self.sigmas = sigmas\n\n        self.norms = norms\n        self.energy_thresh_lo = Quantity(energy_thresh_lo, ""TeV"")\n        self.energy_thresh_hi = Quantity(energy_thresh_hi, ""TeV"")\n\n        self._interp_norms = self._setup_interpolators(self.norms)\n        self._interp_sigmas = self._setup_interpolators(self.sigmas)\n\n    def _setup_interpolators(self, values_list):\n        interps = []\n        for values in values_list:\n            interp = ScaledRegularGridInterpolator(\n                points=(self.theta, self.energy), values=values\n            )\n            interps.append(interp)\n        return interps\n\n    @classmethod\n    def read(cls, filename, hdu=""PSF_2D_GAUSS""):\n        """"""Create `EnergyDependentMultiGaussPSF` from FITS file.\n\n        Parameters\n        ----------\n        filename : str\n            File name\n        """"""\n        with fits.open(make_path(filename), memmap=False) as hdulist:\n            return cls.from_fits(hdulist[hdu])\n\n    @classmethod\n    def from_fits(cls, hdu):\n        """"""Create `EnergyDependentMultiGaussPSF` from HDU list.\n\n        Parameters\n        ----------\n        hdu : `~astropy.io.fits.BinTableHDU`\n            HDU\n        """"""\n        energy_lo = Quantity(hdu.data[""ENERG_LO""][0], ""TeV"")\n        energy_hi = Quantity(hdu.data[""ENERG_HI""][0], ""TeV"")\n        theta = Angle(hdu.data[""THETA_LO""][0], ""deg"")\n\n        # Get sigmas\n        shape = (len(theta), len(energy_hi))\n        sigmas = []\n        for key in [""SIGMA_1"", ""SIGMA_2"", ""SIGMA_3""]:\n            sigma = hdu.data[key].reshape(shape).copy()\n            sigmas.append(sigma)\n\n        # Get amplitudes\n        norms = []\n        for key in [""SCALE"", ""AMPL_2"", ""AMPL_3""]:\n            norm = hdu.data[key].reshape(shape).copy()\n            norms.append(norm)\n\n        opts = {}\n        try:\n            opts[""energy_thresh_lo""] = Quantity(hdu.header[""LO_THRES""], ""TeV"")\n            opts[""energy_thresh_hi""] = Quantity(hdu.header[""HI_THRES""], ""TeV"")\n        except KeyError:\n            pass\n\n        return cls(energy_lo, energy_hi, theta, sigmas, norms, **opts)\n\n    def to_fits(self):\n        """"""\n        Convert psf table data to FITS hdu list.\n\n        Returns\n        -------\n        hdu_list : `~astropy.io.fits.HDUList`\n            PSF in HDU list format.\n        """"""\n        # Set up data\n        names = [\n            ""ENERG_LO"",\n            ""ENERG_HI"",\n            ""THETA_LO"",\n            ""THETA_HI"",\n            ""SCALE"",\n            ""SIGMA_1"",\n            ""AMPL_2"",\n            ""SIGMA_2"",\n            ""AMPL_3"",\n            ""SIGMA_3"",\n        ]\n        units = [""TeV"", ""TeV"", ""deg"", ""deg"", """", ""deg"", """", ""deg"", """", ""deg""]\n\n        data = [\n            self.energy_lo,\n            self.energy_hi,\n            self.theta,\n            self.theta,\n            self.norms[0],\n            self.sigmas[0],\n            self.norms[1],\n            self.sigmas[1],\n            self.norms[2],\n            self.sigmas[2],\n        ]\n\n        table = Table()\n        for name_, data_, unit_ in zip(names, data, units):\n            table[name_] = [data_]\n            table[name_].unit = unit_\n\n        # Create hdu and hdu list\n        hdu = fits.BinTableHDU(table)\n        hdu.header[""LO_THRES""] = self.energy_thresh_lo.value\n        hdu.header[""HI_THRES""] = self.energy_thresh_hi.value\n\n        return fits.HDUList([fits.PrimaryHDU(), hdu])\n\n    def write(self, filename, *args, **kwargs):\n        """"""Write PSF to FITS file.\n\n        Calls `~astropy.io.fits.HDUList.writeto`, forwarding all arguments.\n        """"""\n        self.to_fits().writeto(filename, *args, **kwargs)\n\n    def psf_at_energy_and_theta(self, energy, theta):\n        """"""\n        Get `~gammapy.modeling.models.MultiGauss2D` model for given energy and theta.\n\n        No interpolation is used.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy at which a PSF is requested.\n        theta : `~astropy.coordinates.Angle`\n            Offset angle at which a PSF is requested.\n\n        Returns\n        -------\n        psf : `~gammapy.morphology.MultiGauss2D`\n            Multigauss PSF object.\n        """"""\n        energy = Quantity(energy)\n        theta = Quantity(theta)\n\n        pars = {}\n        for name, interp_norm in zip([""scale"", ""A_2"", ""A_3""], self._interp_norms):\n            pars[name] = interp_norm((theta, energy))\n\n        for idx, interp_sigma in enumerate(self._interp_sigmas):\n            pars[f""sigma_{idx + 1}""] = interp_sigma((theta, energy))\n\n        psf = HESSMultiGaussPSF(pars)\n        return psf.to_MultiGauss2D(normalize=True)\n\n    def containment_radius(self, energy, theta, fraction=0.68):\n        """"""Compute containment for all energy and theta values""""""\n        # This is a false positive from pylint\n        # See https://github.com/PyCQA/pylint/issues/2435\n        energies = Quantity(\n            energy\n        ).flatten()  # pylint:disable=assignment-from-no-return\n        thetas = Angle(theta).flatten()\n        radius = np.empty((theta.size, energy.size))\n\n        for idx, energy in enumerate(energies):\n            for jdx, theta in enumerate(thetas):\n                try:\n                    psf = self.psf_at_energy_and_theta(energy, theta)\n                    radius[jdx, idx] = psf.containment_radius(fraction)\n                except ValueError:\n                    log.debug(\n                        f""Computing containment failed for energy = {energy:.2f}""\n                        f"" and theta={theta:.2f}""\n                    )\n                    log.debug(f""Sigmas: {psf.sigmas} Norms: {psf.norms}"")\n                    radius[jdx, idx] = np.nan\n\n        return Angle(radius, ""deg"")\n\n    def plot_containment(\n        self, fraction=0.68, ax=None, show_safe_energy=False, add_cbar=True, **kwargs\n    ):\n        """"""\n        Plot containment image with energy and theta axes.\n\n        Parameters\n        ----------\n        fraction : float\n            Containment fraction between 0 and 1.\n        add_cbar : bool\n            Add a colorbar\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        energy = self.energy_hi\n        offset = self.theta\n\n        # Set up and compute data\n        containment = self.containment_radius(energy, offset, fraction)\n\n        # plotting defaults\n        kwargs.setdefault(""cmap"", ""GnBu"")\n        kwargs.setdefault(""vmin"", np.nanmin(containment.value))\n        kwargs.setdefault(""vmax"", np.nanmax(containment.value))\n\n        # Plotting\n        x = energy.value\n        y = offset.value\n        caxes = ax.pcolormesh(x, y, containment.value, **kwargs)\n\n        # Axes labels and ticks, colobar\n        ax.semilogx()\n        ax.set_ylabel(f""Offset ({offset.unit})"")\n        ax.set_xlabel(f""Energy ({energy.unit})"")\n        ax.set_xlim(x.min(), x.max())\n        ax.set_ylim(y.min(), y.max())\n\n        if show_safe_energy:\n            self._plot_safe_energy_range(ax)\n\n        if add_cbar:\n            label = f""Containment radius R{100 * fraction:.0f} ({containment.unit})""\n            ax.figure.colorbar(caxes, ax=ax, label=label)\n\n        return ax\n\n    def _plot_safe_energy_range(self, ax):\n        """"""add safe energy range lines to the plot""""""\n        esafe = self.energy_thresh_lo\n        omin = self.offset.value.min()\n        omax = self.offset.value.max()\n        ax.hlines(y=esafe.value, xmin=omin, xmax=omax)\n        label = f""Safe energy threshold: {esafe:3.2f}""\n        ax.text(x=0.1, y=0.9 * esafe.value, s=label, va=""top"")\n\n    def plot_containment_vs_energy(\n        self, fractions=[0.68, 0.95], thetas=Angle([0, 1], ""deg""), ax=None, **kwargs\n    ):\n        """"""Plot containment fraction as a function of energy.\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        energy = MapAxis.from_energy_bounds(\n            self.energy_lo[0], self.energy_hi[-1], 100\n        ).edges\n\n        for theta in thetas:\n            for fraction in fractions:\n                radius = self.containment_radius(energy, theta, fraction).squeeze()\n                label = f""{theta.deg} deg, {100 * fraction:.1f}%""\n                ax.plot(energy.value, radius.value, label=label)\n\n        ax.semilogx()\n        ax.legend(loc=""best"")\n        ax.set_xlabel(""Energy (TeV)"")\n        ax.set_ylabel(""Containment radius (deg)"")\n\n    def peek(self, figsize=(15, 5)):\n        """"""Quick-look summary plots.""""""\n        import matplotlib.pyplot as plt\n\n        fig, axes = plt.subplots(nrows=1, ncols=3, figsize=figsize)\n\n        self.plot_containment(fraction=0.68, ax=axes[0])\n        self.plot_containment(fraction=0.95, ax=axes[1])\n        self.plot_containment_vs_energy(ax=axes[2])\n\n        # TODO: implement this plot\n        # psf = self.psf_at_energy_and_theta(energy=\'1 TeV\', theta=\'1 deg\')\n        # psf.plot_components(ax=axes[2])\n\n        plt.tight_layout()\n\n    def info(\n        self,\n        fractions=[0.68, 0.95],\n        energies=Quantity([1.0, 10.0], ""TeV""),\n        thetas=Quantity([0.0], ""deg""),\n    ):\n        """"""\n        Print PSF summary info.\n\n        The containment radius for given fraction, energies and thetas is\n        computed and printed on the command line.\n\n        Parameters\n        ----------\n        fractions : list\n            Containment fraction to compute containment radius for.\n        energies : `~astropy.units.Quantity`\n            Energies to compute containment radius for.\n        thetas : `~astropy.units.Quantity`\n            Thetas to compute containment radius for.\n\n        Returns\n        -------\n        ss : string\n            Formatted string containing the summary info.\n        """"""\n        ss = ""\\nSummary PSF info\\n""\n        ss += ""----------------\\n""\n        ss += array_stats_str(self.theta.to(""deg""), ""Theta"")\n        ss += array_stats_str(self.energy_hi, ""Energy hi"")\n        ss += array_stats_str(self.energy_lo, ""Energy lo"")\n        ss += f""Safe energy threshold lo: {self.energy_thresh_lo:6.3f}\\n""\n        ss += f""Safe energy threshold hi: {self.energy_thresh_hi:6.3f}\\n""\n\n        for fraction in fractions:\n            containment = self.containment_radius(energies, thetas, fraction)\n            for i, energy in enumerate(energies):\n                for j, theta in enumerate(thetas):\n                    radius = containment[j, i]\n                    ss += (\n                        ""{:2.0f}% containment radius at theta = {} and ""\n                        ""E = {:4.1f}: {:5.8f}\\n""\n                        """".format(100 * fraction, theta, energy, radius)\n                    )\n        return ss\n\n    def to_energy_dependent_table_psf(self, theta=None, rad=None, exposure=None):\n        """"""\n        Convert triple Gaussian PSF ot table PSF.\n\n        Parameters\n        ----------\n        theta : `~astropy.coordinates.Angle`\n            Offset in the field of view. Default theta = 0 deg\n        rad : `~astropy.coordinates.Angle`\n            Offset from PSF center used for evaluating the PSF on a grid.\n            Default offset = [0, 0.005, ..., 1.495, 1.5] deg.\n        exposure : `~astropy.units.Quantity`\n            Energy dependent exposure. Should be in units equivalent to \'cm^2 s\'.\n            Default exposure = 1.\n\n        Returns\n        -------\n        tabe_psf : `~gammapy.irf.EnergyDependentTablePSF`\n            Instance of `EnergyDependentTablePSF`.\n        """"""\n        # Convert energies to log center\n        energies = self.energy\n        # Defaults and input handling\n        if theta is None:\n            theta = Angle(0, ""deg"")\n        else:\n            theta = Angle(theta)\n\n        if rad is None:\n            rad = Angle(np.arange(0, 1.5, 0.005), ""deg"")\n        else:\n            rad = Angle(rad).to(""deg"")\n\n        psf_value = Quantity(np.zeros((energies.size, rad.size)), ""deg^-2"")\n\n        for idx, energy in enumerate(energies):\n            psf_gauss = self.psf_at_energy_and_theta(energy, theta)\n            psf_value[idx] = Quantity(psf_gauss(rad), ""deg^-2"")\n\n        return EnergyDependentTablePSF(\n            energy=energies, rad=rad, exposure=exposure, psf_value=psf_value\n        )\n\n    def to_psf3d(self, rad=None):\n        """"""Create a PSF3D from an analytical PSF.\n\n        Parameters\n        ----------\n        rad : `~astropy.units.Quantity` or `~astropy.coordinates.Angle`\n            the array of position errors (rad) on which the PSF3D will be defined\n\n        Returns\n        -------\n        psf3d : `~gammapy.irf.PSF3D`\n            the PSF3D. It will be defined on the same energy and offset values than the input psf.\n        """"""\n        offsets = self.theta\n        energy = self.energy\n        energy_lo = self.energy_lo\n        energy_hi = self.energy_hi\n        if rad is None:\n            rad = np.linspace(0, 0.66, 67) * Unit(\n                ""deg""\n            )  # Arbitrary binning of 0.01 in rad\n        rad_lo = rad[:-1]\n        rad_hi = rad[1:]\n\n        psf_values = np.zeros(\n            (rad_lo.shape[0], offsets.shape[0], energy_lo.shape[0])\n        ) * Unit(""sr-1"")\n\n        for i, offset in enumerate(offsets):\n            psftable = self.to_energy_dependent_table_psf(offset)\n            psf_values[:, i, :] = psftable.evaluate(energy, 0.5 * (rad_lo + rad_hi)).T\n\n        return PSF3D(\n            energy_lo,\n            energy_hi,\n            offsets,\n            rad_lo,\n            rad_hi,\n            psf_values,\n            self.energy_thresh_lo,\n            self.energy_thresh_hi,\n        )\n\n\nclass HESSMultiGaussPSF:\n    """"""Multi-Gauss PSF as represented in the HESS software.\n\n    The 2D Gaussian is represented as a 1D exponential\n    probability density function per offset angle squared:\n    dp / dtheta**2 = [0]*(exp(-x/(2*[1]*[1]))+[2]*exp(-x/(2*[3]*[3]))\n\n    @param source: either a dict of a filename\n\n    The following two parameters control numerical\n    precision / speed. Usually the defaults are fine.\n    @param theta_max: Maximum offset in numerical computations\n    @param npoints: Number of points in numerical computations\n    @param eps: Allowed tolerance on normalization of total P to 1\n    """"""\n\n    def __init__(self, source):\n        if isinstance(source, dict):\n            # Assume source is a dict with correct format\n            self.pars = source\n        else:\n            # Assume source is a filename with correct format\n            self.pars = self._read_ascii(source)\n        # Scale will be computed from normalization anyways,\n        # so any default is fine here\n        self.pars[""scale""] = self.pars.get(""scale"", 1)\n        # This avoids handling the first PSF as a special case\n        self.pars[""A_1""] = self.pars.get(""A_1"", 1)\n\n    def _read_ascii(self, filename):\n        """"""Parse file with parameters.""""""\n        fh = open(filename)  # .readlines()\n        pars = {}\n        for line in fh:\n            try:\n                key, value = line.strip().split()[:2]\n                if key.startswith(""#""):\n                    continue\n                else:\n                    pars[key] = float(value)\n            except ValueError:\n                pass\n        fh.close()\n        return pars\n\n    def n_gauss(self):\n        """"""Count number of Gaussians.""""""\n        return len([_ for _ in self.pars.keys() if ""sigma"" in _])\n\n    def dpdtheta2(self, theta2):\n        """"""dp / dtheta2 at position theta2 = theta ^ 2.""""""\n        theta2 = np.asarray(theta2, ""f"")\n        total = np.zeros_like(theta2)\n        for ii in range(1, self.n_gauss() + 1):\n            A = self.pars[f""A_{ii}""]\n            sigma = self.pars[f""sigma_{ii}""]\n            total += A * np.exp(-theta2 / (2 * sigma ** 2))\n        return self.pars[""scale""] * total\n\n    def to_MultiGauss2D(self, normalize=True):\n        """"""Use this to compute containment angles and fractions.\n\n        Note: We have to set norm = 2 * A * sigma ^ 2, because in\n        MultiGauss2D norm represents the integral, and in HESS A\n        represents the amplitude at 0.\n        """"""\n        sigmas, norms = [], []\n        for ii in range(1, self.n_gauss() + 1):\n            A = self.pars[f""A_{ii}""]\n            sigma = self.pars[f""sigma_{ii}""]\n            norm = self.pars[""scale""] * 2 * A * sigma ** 2\n            sigmas.append(sigma)\n            norms.append(norm)\n        m = MultiGauss2D(sigmas, norms)\n        if normalize:\n            m.normalize()\n        return m\n\n\ndef multi_gauss_psf_kernel(psf_parameters, BINSZ=0.02, NEW_BINSZ=0.02, **kwargs):\n    """"""Create multi-Gauss PSF kernel.\n\n    The Gaussian PSF components are specified via the\n    amplitude at the center and the FWHM.\n    See the example for the exact format.\n\n    Parameters\n    ----------\n    psf_parameters : dict\n        PSF parameters\n    BINSZ : float (0.02)\n        Pixel size used for the given parameters in deg.\n    NEW_BINSZ : float (0.02)\n        New pixel size in deg. USed to change the resolution of the PSF.\n\n    Returns\n    -------\n    psf_kernel : `astropy.convolution.Kernel2D`\n        PSF kernel\n\n    Examples\n    --------\n    >>> psf_pars = dict()\n    >>> psf_pars[\'psf1\'] = dict(ampl=1, fwhm=2.5)\n    >>> psf_pars[\'psf2\'] = dict(ampl=0.06, fwhm=11.14)\n    >>> psf_pars[\'psf3\'] = dict(ampl=0.47, fwhm=5.16)\n    >>> psf_kernel = multi_gauss_psf_kernel(psf_pars, x_size=51)\n    """"""\n    psf = None\n    for ii in range(1, 4):\n        # Convert sigma and amplitude\n        pars = psf_parameters[f""psf{ii}""]\n        sigma = gaussian_fwhm_to_sigma * pars[""fwhm""] * BINSZ / NEW_BINSZ\n        ampl = 2 * np.pi * sigma ** 2 * pars[""ampl""]\n        if psf is None:\n            psf = float(ampl) * Gaussian2DKernel(sigma, **kwargs)\n        else:\n            psf += float(ampl) * Gaussian2DKernel(sigma, **kwargs)\n    psf.normalize()\n    return psf\n'"
gammapy/irf/psf_kernel.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nimport astropy.units as u\nfrom astropy.coordinates import Angle\nfrom astropy.coordinates.angle_utilities import angular_separation\nfrom gammapy.maps import Map, WcsGeom\nfrom gammapy.modeling.models import PowerLawSpectralModel\nfrom gammapy.utils.gauss import Gauss2DPDF\nfrom .psf_table import TablePSF\n\n__all__ = [""PSFKernel""]\n\n\ndef _make_kernel_geom(geom, max_radius):\n    # Create a new geom object with an odd number of pixel and a maximum size\n    # This is useful for PSF kernel creation.\n    center = geom.center_skydir\n    binsz = Angle(np.abs(geom.wcs.wcs.cdelt[0]), ""deg"")\n    max_radius = Angle(max_radius)\n    npix = 2 * int(max_radius.deg / binsz.deg) + 1\n    return WcsGeom.create(\n        skydir=center,\n        binsz=binsz,\n        npix=npix,\n        proj=geom.projection,\n        frame=geom.frame,\n        axes=geom.axes,\n    )\n\n\ndef _compute_kernel_separations(geom, factor):\n    # utility function used for preparing distance to the center of the upsampled geom\n    # TODO : take into account non regular geometry for energy dependent PSF kernel size\n    if geom.is_regular is False:\n        raise ValueError(""Non regular geometries are not supported yet."")\n\n    upsampled_image_geom = geom.to_image().upsample(factor)\n    # get center coordinate\n    center_coord = upsampled_image_geom.center_coord * u.deg\n    # get coordinates\n    map_c = upsampled_image_geom.get_coord()\n    # compute distances to map center\n    separations = angular_separation(\n        center_coord[0], center_coord[1], map_c.lon, map_c.lat\n    )\n\n    # Create map\n    kernel_map = Map.from_geom(geom=upsampled_image_geom.to_cube(axes=geom.axes))\n    return kernel_map, separations\n\n\ndef table_psf_to_kernel_map(table_psf, geom, factor=4):\n    """"""Compute a PSF kernel on a given Geom.\n\n    If the Geom is not an image, the same kernel will be present on all axes.\n\n    The PSF is estimated by oversampling defined by a given factor.\n    The PSF kernel is normalized\n\n    Parameters\n    ----------\n    table_psf : `~gammapy.irf.TablePSF`\n        the input table PSF\n    geom : `~gammapy.maps.Geom`\n        the target geometry. The PSF kernel will be centered on the spatial center.\n    factor : int\n        the oversample factor to compute the PSF\n    """"""\n    # prepare map and compute distances to map center\n    kernel_map, rads = _compute_kernel_separations(geom, factor)\n\n    vals = table_psf.evaluate(rad=rads).value\n    norm = vals.sum()\n\n    for img, idx in kernel_map.iter_by_image():\n        img += vals.reshape(img.shape) / norm\n\n    return kernel_map.downsample(factor, preserve_counts=True)\n\n\ndef energy_dependent_table_psf_to_kernel_map(table_psf, geom, factor=4):\n    """"""Compute an energy dependent PSF kernel on a given Geom.\n\n    The PSF is estimated by oversampling defined by a given factor.\n\n    Parameters\n    ----------\n    table_psf : `~gammapy.irf.EnergyDependentTablePSF`\n        the input table PSF\n    geom : `~gammapy.maps.Geom`\n        the target geometry.\n        The PSF kernel will be centered on the spatial centre.\n        the geometry axes should contain an ""energy"" axis.\n        The kernel will be duplicated along other axes.\n    factor : int\n        the oversample factor to compute the PSF\n    """"""\n    energy_axis = geom.get_axis_by_name(""energy_true"")\n    energy_idx = geom.axes.index(energy_axis)\n\n    # prepare map and compute distances to map center\n    kernel_map, rads = _compute_kernel_separations(geom, factor)\n\n    # loop over images\n    for img, idx in kernel_map.iter_by_image():\n        # TODO: this is super complex. Find or invent a better way!\n        energy = energy_axis.center[idx[energy_idx]]\n        vals = table_psf.evaluate(energy=energy, rad=rads).reshape(img.shape)\n        with np.errstate(invalid=""ignore""):\n            img += vals.value / vals.sum().value\n\n    return kernel_map.downsample(factor, preserve_counts=True)\n\n\nclass PSFKernel:\n    """"""PSF kernel for `~gammapy.maps.Map`.\n\n    This is a container class to store a PSF kernel\n    that can be used to convolve `~gammapy.maps.WcsNDMap` objects.\n    It is usually computed from an `~gammapy.irf.EnergyDependentTablePSF`.\n\n    Parameters\n    ----------\n    psf_kernel_map : `~gammapy.maps.Map`\n        PSF kernel stored in a Map\n\n    Examples\n    --------\n    ::\n\n        import numpy as np\n        from gammapy.maps import Map, WcsGeom, MapAxis\n        from gammapy.irf import EnergyDependentMultiGaussPSF\n        from gammapy.cube import PSFKernel\n        from astropy import units as u\n\n        # Define energy axis\n        energy_axis = MapAxis.from_edges(np.logspace(-1., 1., 4), unit=\'TeV\', name=\'energy\')\n\n        # Create WcsGeom and map\n        geom = WcsGeom.create(binsz=0.02*u.deg, width=2.0*u.deg, axes=[energy_axis])\n        some_map = Map.from_geom(geom)\n        # Fill map at two positions\n        some_map.fill_by_coord([[0.2,0.4],[-0.1,0.6],[0.5,3.6]])\n\n        # Extract EnergyDependentTablePSF from CTA 1DC IRF\n        filename = \'$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits\'\n        psf = EnergyDependentMultiGaussPSF.read(filename, hdu=\'POINT SPREAD FUNCTION\')\n        table_psf = psf.to_energy_dependent_table_psf(theta=0.5*u.deg)\n\n        psf_kernel = PSFKernel.from_table_psf(table_psf,geom, max_radius=1*u.deg)\n\n        # Do the convolution\n        some_map_convolved = some_map.convolve(psf_kernel)\n\n        some_map_convolved.get_image_by_coord(dict(energy=0.6*u.TeV)).plot()\n    """"""\n\n    def __init__(self, psf_kernel_map):\n        self._psf_kernel_map = psf_kernel_map\n\n    @property\n    def data(self):\n        """"""Access the PSFKernel numpy array""""""\n        return self._psf_kernel_map.data\n\n    @property\n    def psf_kernel_map(self):\n        """"""The map object holding the kernel (`~gammapy.maps.Map`)""""""\n        return self._psf_kernel_map\n\n    @classmethod\n    def read(cls, *args, **kwargs):\n        """"""Read kernel Map from file.""""""\n        psf_kernel_map = Map.read(*args, **kwargs)\n        return cls(psf_kernel_map)\n\n    @classmethod\n    def from_table_psf(cls, table_psf, geom, max_radius=None, factor=4):\n        """"""Create a PSF kernel from a TablePSF or an EnergyDependentTablePSF on a given Geom.\n\n        If the Geom is not an image, the same kernel will be present on all axes.\n\n        The PSF is estimated by oversampling defined by a given factor.\n\n        Parameters\n        ----------\n        table_psf : `~gammapy.irf.TablePSF` or `~gammapy.irf.EnergyDependentTablePSF`\n            the input table PSF\n        geom : `~gammapy.maps.WcsGeom`\n            the target geometry. The PSF kernel will be centered on the central pixel.\n            The geometry axes should contain an axis with name ""energy""\n        max_radius : `~astropy.coordinates.Angle`\n            the maximum radius of the PSF kernel.\n        factor : int\n            the oversample factor to compute the PSF\n\n        Returns\n        -------\n        kernel : `~gammapy.cube.PSFKernel`\n            the kernel Map with reduced geometry according to the max_radius\n        """"""\n        # TODO : use PSF containment radius if max_radius is None\n        if max_radius is not None:\n            geom = _make_kernel_geom(geom, max_radius)\n\n        if isinstance(table_psf, TablePSF):\n            return cls(table_psf_to_kernel_map(table_psf, geom, factor))\n        else:\n            return cls(\n                energy_dependent_table_psf_to_kernel_map(table_psf, geom, factor)\n            )\n\n    @classmethod\n    def from_gauss(\n        cls, geom, sigma, max_radius=None, containment_fraction=0.99, factor=4\n    ):\n        """"""Create Gaussian PSF.\n\n        This is used for testing and examples.\n        The map geometry parameters (pixel size, energy bins) are taken from ``geom``.\n        The Gaussian width ``sigma`` is a scalar,\n\n        TODO : support array input if it should vary along the energy axis.\n\n        Parameters\n        ----------\n        geom : `~gammapy.maps.WcsGeom`\n            Map geometry\n        sigma : `~astropy.coordinates.Angle`\n            Gaussian width.\n        max_radius : `~astropy.coordinates.Angle`\n            Desired kernel map size.\n        factor : int\n            Oversample factor to compute the PSF\n\n        Returns\n        -------\n        kernel : `~gammapy.cube.PSFKernel`\n            the kernel Map with reduced geometry according to the max_radius\n        """"""\n        sigma = Angle(sigma)\n\n        if max_radius is None:\n            max_radius = (\n                Gauss2DPDF(sigma.deg).containment_radius(\n                    containment_fraction=containment_fraction\n                )\n                * u.deg\n            )\n\n        max_radius = Angle(max_radius)\n\n        # Create a new geom according to given input\n        geom = _make_kernel_geom(geom, max_radius)\n\n        rad = Angle(np.linspace(0.0, max_radius.deg, 200), ""deg"")\n\n        table_psf = TablePSF.from_shape(shape=""gauss"", width=sigma, rad=rad)\n\n        return cls(table_psf_to_kernel_map(table_psf, geom, factor))\n\n    def write(self, *args, **kwargs):\n        """"""Write the Map object which contains the PSF kernel to file.""""""\n        self.psf_kernel_map.write(*args, **kwargs)\n\n    def to_image(self, spectrum=None, exposure=None, keepdims=True):\n        """"""Transform 3D PSFKernel into a 2D PSFKernel.\n\n        Parameters\n        ----------\n        spectrum : `~gammapy.modeling.models.SpectralModel`\n            Spectral model to compute the weights.\n            Default is power-law with spectral index of 2.\n        exposure : `~astropy.units.Quantity` or `~numpy.ndarray`\n            1D array containing exposure in each true energy bin.\n            It must have the same size as the PSFKernel energy axis.\n            Default is uniform exposure over energy.\n        keepdims : bool\n            If true, the resulting PSFKernel wil keep an energy axis with one bin.\n            Default is True.\n\n        Returns\n        -------\n        weighted_kernel : `~gammapy.irf.PSFKernel`\n            the weighted kernel summed over energy\n        """"""\n        map = self.psf_kernel_map\n\n        if spectrum is None:\n            spectrum = PowerLawSpectralModel(index=2.0)\n\n        if exposure is None:\n            exposure = np.ones(map.geom.axes[0].center.shape)\n        exposure = u.Quantity(exposure)\n        if exposure.shape != map.geom.axes[0].center.shape:\n            raise ValueError(""Incorrect exposure_array shape"")\n\n        # Compute weights vector\n        energy_edges = map.geom.get_axis_by_name(""energy_true"").edges\n        weights = spectrum.integral(\n            emin=energy_edges[:-1], emax=energy_edges[1:], intervals=True\n        )\n        weights *= exposure\n        weights /= weights.sum()\n\n        spectrum_weighted_kernel = map.copy()\n        spectrum_weighted_kernel.quantity *= weights[:, np.newaxis, np.newaxis]\n\n        return self.__class__(spectrum_weighted_kernel.sum_over_axes(keepdims=keepdims))\n'"
gammapy/irf/psf_king.py,8,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport numpy as np\nfrom astropy.coordinates import Angle\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom astropy.units import Quantity\nfrom gammapy.utils.array import array_stats_str\nfrom gammapy.utils.scripts import make_path\nfrom .psf_table import EnergyDependentTablePSF\n\n__all__ = [""PSFKing""]\n\nlog = logging.getLogger(__name__)\n\n\nclass PSFKing:\n    """"""King profile analytical PSF depending on energy and offset.\n\n    This PSF parametrisation and FITS data format is described here: :ref:`gadf:psf_king`.\n\n    Parameters\n    ----------\n    energy_lo : `~astropy.units.Quantity`\n        Lower energy boundary of the energy bin.\n    energy_hi : `~astropy.units.Quantity`\n        Upper energy boundary of the energy bin.\n    offset : `~astropy.coordinates.Angle`\n        Offset nodes (1D)\n    gamma : `~numpy.ndarray`\n        PSF parameter (2D)\n    sigma : `~astropy.coordinates.Angle`\n        PSF parameter (2D)\n    """"""\n\n    def __init__(\n        self,\n        energy_lo,\n        energy_hi,\n        offset,\n        gamma,\n        sigma,\n        energy_thresh_lo=Quantity(0.1, ""TeV""),\n        energy_thresh_hi=Quantity(100, ""TeV""),\n    ):\n        self.energy_lo = energy_lo.to(""TeV"")\n        self.energy_hi = energy_hi.to(""TeV"")\n        self.offset = Angle(offset)\n        self.energy = np.sqrt(self.energy_lo * self.energy_hi)\n        self.gamma = np.asanyarray(gamma)\n        self.sigma = Angle(sigma)\n\n        self.energy_thresh_lo = Quantity(energy_thresh_lo).to(""TeV"")\n        self.energy_thresh_hi = Quantity(energy_thresh_hi).to(""TeV"")\n\n    def info(self):\n        """"""Print some basic info.\n        """"""\n        ss = ""\\nSummary PSFKing info\\n""\n        ss += ""---------------------\\n""\n        ss += array_stats_str(self.offset, ""offset"")\n        ss += array_stats_str(self.energy, ""energy"")\n        ss += array_stats_str(self.gamma, ""gamma"")\n        ss += array_stats_str(self.sigma, ""sigma"")\n\n        # TODO: should quote containment values also\n\n        return ss\n\n    @classmethod\n    def read(cls, filename, hdu=1):\n        """"""Create `PSFKing` from FITS file.\n\n        Parameters\n        ----------\n        filename : str\n            File name\n        """"""\n        # TODO: implement it so that HDUCLASS is used\n        # http://gamma-astro-data-formats.readthedocs.io/en/latest/data_storage/hdu_index/index.html\n\n        table = Table.read(make_path(filename), hdu=hdu)\n        return cls.from_table(table)\n\n    @classmethod\n    def from_table(cls, table):\n        """"""Create `PSFKing` from `~astropy.table.Table`.\n\n        Parameters\n        ----------\n        table : `~astropy.table.Table`\n            Table King PSF info.\n        """"""\n        offset_lo = table[""THETA_LO""].quantity[0]\n        offset_hi = table[""THETA_HI""].quantity[0]\n        offset = (offset_hi + offset_lo) / 2\n        offset = Angle(offset, unit=table[""THETA_LO""].unit)\n\n        energy_lo = table[""ENERG_LO""].quantity[0]\n        energy_hi = table[""ENERG_HI""].quantity[0]\n\n        gamma = table[""GAMMA""].quantity[0]\n        sigma = table[""SIGMA""].quantity[0]\n\n        opts = {}\n        try:\n            opts[""energy_thresh_lo""] = Quantity(table.meta[""LO_THRES""], ""TeV"")\n            opts[""energy_thresh_hi""] = Quantity(table.meta[""HI_THRES""], ""TeV"")\n        except KeyError:\n            pass\n\n        return cls(energy_lo, energy_hi, offset, gamma, sigma, **opts)\n\n    def to_fits(self):\n        """"""\n        Convert PSF table data to FITS HDU list.\n\n        Returns\n        -------\n        hdu_list : `~astropy.io.fits.HDUList`\n            PSF in HDU list format.\n        """"""\n        # Set up data\n        names = [""ENERG_LO"", ""ENERG_HI"", ""THETA_LO"", ""THETA_HI"", ""SIGMA"", ""GAMMA""]\n        units = [""TeV"", ""TeV"", ""deg"", ""deg"", ""deg"", """"]\n        data = [\n            self.energy_lo,\n            self.energy_hi,\n            self.offset,\n            self.offset,\n            self.sigma,\n            self.gamma,\n        ]\n\n        table = Table()\n        for name_, data_, unit_ in zip(names, data, units):\n            table[name_] = [data_]\n            table[name_].unit = unit_\n\n        hdu = fits.BinTableHDU(table)\n        hdu.header[""LO_THRES""] = self.energy_thresh_lo.value\n        hdu.header[""HI_THRES""] = self.energy_thresh_hi.value\n\n        return fits.HDUList([fits.PrimaryHDU(), hdu])\n\n    def write(self, filename, *args, **kwargs):\n        """"""Write PSF to FITS file.\n\n        Calls `~astropy.io.fits.HDUList.writeto`, forwarding all arguments.\n        """"""\n        self.to_fits().writeto(filename, *args, **kwargs)\n\n    @staticmethod\n    def evaluate_direct(r, gamma, sigma):\n        """"""Evaluate the PSF model.\n\n        Formula is given here: :ref:`gadf:psf_king`.\n\n        Parameters\n        ----------\n        r : `~astropy.coordinates.Angle`\n            Offset from PSF center used for evaluating the PSF on a grid\n        gamma : `~astropy.units.Quantity`\n            model parameter, no unit\n        sigma : `~astropy.coordinates.Angle`\n            model parameter\n\n        Returns\n        -------\n        psf_value : `~astropy.units.Quantity`\n            PSF value\n        """"""\n        r2 = r * r\n        sigma2 = sigma * sigma\n\n        with np.errstate(divide=""ignore""):\n            term1 = 1 / (2 * np.pi * sigma2)\n            term2 = 1 - 1 / gamma\n            term3 = (1 + r2 / (2 * gamma * sigma2)) ** (-gamma)\n\n        return term1 * term2 * term3\n\n    def evaluate(self, energy=None, offset=None):\n        """"""Evaluate analytic PSF parameters at a given energy and offset.\n\n        Uses nearest-neighbor interpolation.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            energy value\n        offset : `~astropy.coordinates.Angle`\n            Offset in the field of view\n\n        Returns\n        -------\n        values : `~astropy.units.Quantity`\n            Interpolated value\n        """"""\n        param = dict()\n        energy = Quantity(energy)\n        offset = Angle(offset)\n\n        # Find nearest energy value\n        i = np.argmin(np.abs(self.energy - energy))\n        j = np.argmin(np.abs(self.offset - offset))\n\n        # TODO: Use some kind of interpolation to get PSF\n        # parameters for every energy and theta\n\n        # Select correct gauss parameters for given energy and theta\n        sigma = self.sigma[j][i]\n        gamma = self.gamma[j][i]\n\n        param[""sigma""] = sigma\n        param[""gamma""] = gamma\n        return param\n\n    def to_energy_dependent_table_psf(self, theta=None, rad=None, exposure=None):\n        """"""Convert to energy-dependent table PSF.\n\n        Parameters\n        ----------\n        theta : `~astropy.coordinates.Angle`\n            Offset in the field of view. Default theta = 0 deg\n        rad : `~astropy.coordinates.Angle`\n            Offset from PSF center used for evaluating the PSF on a grid.\n            Default offset = [0, 0.005, ..., 1.495, 1.5] deg.\n        exposure : `~astropy.units.Quantity`\n            Energy dependent exposure. Should be in units equivalent to \'cm^2 s\'.\n            Default exposure = 1.\n\n        Returns\n        -------\n        table_psf : `~gammapy.irf.EnergyDependentTablePSF`\n            Energy-dependent PSF\n        """"""\n        # self.energy is already the logcenter\n        energies = self.energy\n\n        # Defaults\n        theta = theta if theta is not None else Angle(0, ""deg"")\n        rad = rad if rad is not None else Angle(np.arange(0, 1.5, 0.005), ""deg"")\n        psf_value = Quantity(np.empty((len(energies), len(rad))), ""deg^-2"")\n\n        for i, energy in enumerate(energies):\n            param_king = self.evaluate(energy, theta)\n            val = self.evaluate_direct(rad, param_king[""gamma""], param_king[""sigma""])\n            psf_value[i] = Quantity(val, ""deg^-2"")\n\n        return EnergyDependentTablePSF(\n            energy=energies, rad=rad, exposure=exposure, psf_value=psf_value\n        )\n'"
gammapy/irf/psf_map.py,3,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nimport astropy.units as u\nfrom gammapy.maps import Map, MapAxis, MapCoord, WcsGeom\nfrom gammapy.modeling.models import PowerLawSpectralModel\nfrom gammapy.utils.random import InverseCDFSampler, get_random_state\nfrom .irf_map import IRFMap\nfrom .psf_kernel import PSFKernel\nfrom .psf_table import EnergyDependentTablePSF\n\n__all__ = [""PSFMap""]\n\n\nclass PSFMap(IRFMap):\n    """"""Class containing the Map of PSFs and allowing to interact with it.\n\n    Parameters\n    ----------\n    psf_map : `~gammapy.maps.Map`\n        the input PSF Map. Should be a Map with 2 non spatial axes.\n        rad and true energy axes should be given in this specific order.\n    exposure_map : `~gammapy.maps.Map`\n        Associated exposure map. Needs to have a consistent map geometry.\n\n    Examples\n    --------\n    ::\n\n        import numpy as np\n        from astropy import units as u\n        from astropy.coordinates import SkyCoord\n        from gammapy.maps import Map, WcsGeom, MapAxis\n        from gammapy.irf import EnergyDependentMultiGaussPSF, EffectiveAreaTable2D\n        from gammapy.cube import make_psf_map, PSFMap, make_map_exposure_true_energy\n\n        # Define energy axis. Note that the name is fixed.\n        energy_axis = MapAxis.from_edges(np.logspace(-1., 1., 4), unit=\'TeV\', name=\'energy\')\n        # Define rad axis. Again note the axis name\n        rads = np.linspace(0., 0.5, 100) * u.deg\n        rad_axis = MapAxis.from_edges(rads, unit=\'deg\', name=\'theta\')\n\n        # Define parameters\n        pointing = SkyCoord(0., 0., unit=\'deg\')\n        max_offset = 4 * u.deg\n\n        # Create WcsGeom\n        geom = WcsGeom.create(binsz=0.25*u.deg, width=10*u.deg, skydir=pointing, axes=[rad_axis, energy_axis])\n\n        # Extract EnergyDependentTablePSF from CTA 1DC IRF\n        filename = \'$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits\'\n        psf = EnergyDependentMultiGaussPSF.read(filename, hdu=\'POINT SPREAD FUNCTION\')\n        psf3d = psf.to_psf3d(rads)\n        aeff2d = EffectiveAreaTable2D.read(filename, hdu=\'EFFECTIVE AREA\')\n\n        # Create the exposure map\n        exposure_geom = geom.to_image().to_cube([energy_axis])\n        exposure_map = make_map_exposure_true_energy(pointing, ""1 h"", aeff2d, exposure_geom)\n\n        # create the PSFMap for the specified pointing\n        psf_map = make_psf_map(psf3d, pointing, geom, max_offset, exposure_map)\n\n        # Get an EnergyDependentTablePSF at any position in the image\n        psf_table = psf_map.get_energy_dependent_table_psf(SkyCoord(2., 2.5, unit=\'deg\'))\n\n        # Write map to disk\n        psf_map.write(\'psf_map.fits\')\n    """"""\n\n    _hdu_name = ""psf""\n\n    @property\n    def psf_map(self):\n        return self._irf_map\n\n    @psf_map.setter\n    def psf_map(self, value):\n        self._irf_map = value\n\n    def __init__(self, psf_map, exposure_map=None):\n        if psf_map.geom.axes[1].name.upper() != ""ENERGY_TRUE"":\n            raise ValueError(""Incorrect energy axis position in input Map"")\n\n        if psf_map.geom.axes[0].name.upper() != ""THETA"":\n            raise ValueError(""Incorrect theta axis position in input Map"")\n\n        super().__init__(irf_map=psf_map, exposure_map=exposure_map)\n\n    def get_energy_dependent_table_psf(self, position):\n        """"""Get energy-dependent PSF at a given position.\n\n        Parameters\n        ----------\n        position : `~astropy.coordinates.SkyCoord`\n            the target position. Should be a single coordinates\n\n        Returns\n        -------\n        psf_table : `~gammapy.irf.EnergyDependentTablePSF`\n            the table PSF\n        """"""\n        if position.size != 1:\n            raise ValueError(\n                ""EnergyDependentTablePSF can be extracted at one single position only.""\n            )\n\n        energy = self.psf_map.geom.get_axis_by_name(""energy_true"").center\n        rad = self.psf_map.geom.get_axis_by_name(""theta"").center\n\n        coords = {\n            ""skycoord"": position,\n            ""energy_true"": energy.reshape((-1, 1, 1, 1)),\n            ""theta"": rad.reshape((1, -1, 1, 1)),\n        }\n\n        data = self.psf_map.interp_by_coord(coords)\n        psf_values = u.Quantity(data[:, :, 0, 0], unit=self.psf_map.unit, copy=False)\n\n        if self.exposure_map is not None:\n            coords = {\n                ""skycoord"": position,\n                ""energy_true"": energy.reshape((-1, 1, 1)),\n                ""theta"": 0 * u.deg,\n            }\n            data = self.exposure_map.interp_by_coord(coords).squeeze()\n            exposure = data * self.exposure_map.unit\n        else:\n            exposure = None\n\n        # Beware. Need to revert rad and energies to follow the TablePSF scheme.\n        return EnergyDependentTablePSF(\n            energy=energy, rad=rad, psf_value=psf_values, exposure=exposure\n        )\n\n    def get_psf_kernel(self, position, geom, max_radius=None, factor=4):\n        """"""Returns a PSF kernel at the given position.\n\n        The PSF is returned in the form a WcsNDMap defined by the input Geom.\n\n        Parameters\n        ----------\n        position : `~astropy.coordinates.SkyCoord`\n            the target position. Should be a single coordinate\n        geom : `~gammapy.maps.Geom`\n            the target geometry to use\n        max_radius : `~astropy.coordinates.Angle`\n            maximum angular size of the kernel map\n        factor : int\n            oversampling factor to compute the PSF\n\n        Returns\n        -------\n        kernel : `~gammapy.cube.PSFKernel`\n            the resulting kernel\n        """"""\n        table_psf = self.get_energy_dependent_table_psf(position)\n        if max_radius is None:\n            max_radius = np.max(table_psf.rad)\n        return PSFKernel.from_table_psf(table_psf, geom, max_radius, factor)\n\n    def containment_radius_map(self, energy, fraction=0.68):\n        """"""Containment radius map.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Scalar energy at which to compute the containment radius\n        fraction : float\n            the containment fraction (range: 0 to 1)\n\n        Returns\n        -------\n        containment_radius_map : `~gammapy.maps.Map`\n            Containment radius map\n        """"""\n        coords = self.psf_map.geom.to_image().get_coord().skycoord.flatten()\n        m = Map.from_geom(self.psf_map.geom.to_image(), unit=""deg"")\n\n        for coord in coords:\n            psf_table = self.get_energy_dependent_table_psf(coord)\n            containment_radius = psf_table.containment_radius(energy, fraction)\n            m.fill_by_coord(coord, containment_radius)\n\n        return m\n\n    @classmethod\n    def from_geom(cls, geom):\n        """"""Create psf map from geom.\n\n        Parameters\n        ----------\n        geom : `Geom`\n            PSF map geometry.\n\n        Returns\n        -------\n        psf_map : `PSFMap`\n            Point spread function map.\n        """"""\n        geom_exposure_psf = geom.squash(axis=""theta"")\n        exposure_psf = Map.from_geom(geom_exposure_psf, unit=""m2 s"")\n        psf_map = Map.from_geom(geom, unit=""sr-1"")\n        return cls(psf_map, exposure_psf)\n\n    def sample_coord(self, map_coord, random_state=0):\n        """"""Apply PSF corrections on the coordinates of a set of simulated events.\n\n        Parameters\n        ----------\n        map_coord : `~gammapy.maps.MapCoord` object.\n            Sequence of coordinates and energies of sampled events.\n        random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n            Defines random number generator initialisation.\n            Passed to `~gammapy.utils.random.get_random_state`.\n\n        Returns\n        -------\n        corr_coord : `~gammapy.maps.MapCoord` object.\n            Sequence of PSF-corrected coordinates of the input map_coord map.\n        """"""\n\n        random_state = get_random_state(random_state)\n        rad_axis = self.psf_map.geom.get_axis_by_name(""theta"")\n\n        coord = {\n            ""skycoord"": map_coord.skycoord.reshape(-1, 1),\n            ""energy_true"": map_coord[""energy_true""].reshape(-1, 1),\n            ""theta"": rad_axis.center,\n        }\n\n        pdf = (\n            self.psf_map.interp_by_coord(coord)\n            * rad_axis.center.value\n            * rad_axis.bin_width.value\n        )\n\n        sample_pdf = InverseCDFSampler(pdf, axis=1, random_state=random_state)\n        pix_coord = sample_pdf.sample_axis()\n        separation = rad_axis.pix_to_coord(pix_coord)\n\n        position_angle = random_state.uniform(360, size=len(map_coord.lon)) * u.deg\n\n        event_positions = map_coord.skycoord.directional_offset_by(\n            position_angle=position_angle, separation=separation\n        )\n        return MapCoord.create(\n            {""skycoord"": event_positions, ""energy_true"": map_coord[""energy_true""]}\n        )\n\n    @classmethod\n    def from_energy_dependent_table_psf(cls, table_psf):\n        """"""Create PSF map from table PSF object.\n\n        Helper function to create an allsky PSF map from\n        table PSF, which does not depend on position.\n\n        Parameters\n        ----------\n        table_psf : `EnergyDependentTablePSF`\n            Table PSF\n\n        Returns\n        -------\n        psf_map : `PSFMap`\n            Point spread function map.\n        """"""\n        energy_axis = MapAxis.from_nodes(\n            table_psf.energy, name=""energy_true"", interp=""log""\n        )\n        rad_axis = MapAxis.from_nodes(table_psf.rad, name=""theta"")\n\n        geom = WcsGeom.create(\n            npix=(2, 1), proj=""CAR"", binsz=180, axes=[rad_axis, energy_axis]\n        )\n        coords = geom.get_coord()\n\n        # TODO: support broadcasting in .evaluate()\n        data = table_psf._interpolate(\n            (coords[""energy_true""], coords[""theta""])\n        ).to_value(""sr-1"")\n        psf_map = Map.from_geom(geom, data=data, unit=""sr-1"")\n\n        geom_exposure = geom.squash(axis=""theta"")\n\n        data = table_psf.exposure.reshape((-1, 1, 1, 1))\n\n        exposure_map = Map.from_geom(geom_exposure, unit=""cm2 s"")\n        exposure_map.quantity += data\n        return cls(psf_map=psf_map, exposure_map=exposure_map)\n\n    def to_image(self, spectrum=None, keepdims=True):\n        """"""Reduce to a 2-D map after weighing\n        with the associated exposure and a spectrum\n\n        Parameters\n        ----------\n        spectrum : `~gammapy.modeling.models.SpectralModel`, optional\n            Spectral model to compute the weights.\n            Default is power-law with spectral index of 2.\n        keepdims : bool, optional\n            If True, the energy axis is kept with one bin.\n            If False, the axis is removed\n\n\n        Returns\n        -------\n        psf_out : `PSFMap`\n            `PSFMap` with the energy axis summed over\n        """"""\n        from gammapy.makers.utils import _map_spectrum_weight\n\n        if spectrum is None:\n            spectrum = PowerLawSpectralModel(index=2.0)\n\n        exp_weighed = _map_spectrum_weight(self.exposure_map, spectrum)\n        exposure = exp_weighed.sum_over_axes(axes=[""energy_true""], keepdims=keepdims)\n\n        psf_data = exp_weighed.data * self.psf_map.data / exposure.data\n        psf_map = Map.from_geom(geom=self.psf_map.geom, data=psf_data, unit=""sr-1"")\n\n        psf = psf_map.sum_over_axes(axes=[""energy_true""], keepdims=keepdims)\n        return self.__class__(psf_map=psf, exposure_map=exposure)\n'"
gammapy/irf/psf_table.py,22,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport numpy as np\nimport scipy.integrate\nfrom astropy import units as u\nfrom astropy.coordinates import Angle\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom astropy.utils import lazyproperty\nfrom gammapy.maps import MapAxis\nfrom gammapy.utils.array import array_stats_str\nfrom gammapy.utils.gauss import Gauss2DPDF\nfrom gammapy.utils.interpolation import ScaledRegularGridInterpolator\nfrom gammapy.utils.scripts import make_path\n\n__all__ = [""TablePSF"", ""EnergyDependentTablePSF""]\n\nlog = logging.getLogger(__name__)\n\n\nclass TablePSF:\n    """"""Radially-symmetric table PSF.\n\n    Parameters\n    ----------\n    rad : `~astropy.units.Quantity` with angle units\n        Offset wrt source position\n    psf_value : `~astropy.units.Quantity` with sr^-1 units\n        PSF value array\n    interp_kwargs : dict\n        Keyword arguments passed to `ScaledRegularGridInterpolator`\n    """"""\n\n    def __init__(self, rad, psf_value, interp_kwargs=None):\n        self.rad = Angle(rad).to(""rad"")\n        self.psf_value = u.Quantity(psf_value).to(""sr^-1"")\n\n        self._interp_kwargs = interp_kwargs or {}\n\n    @lazyproperty\n    def _interpolate(self):\n        points = (self.rad,)\n        return ScaledRegularGridInterpolator(\n            points=points, values=self.psf_value, **self._interp_kwargs\n        )\n\n    @lazyproperty\n    def _interpolate_containment(self):\n        if self.rad[0] > 0:\n            rad = self.rad.insert(0, 0)\n        else:\n            rad = self.rad\n\n        rad_drad = 2 * np.pi * rad * self.evaluate(rad)\n        values = scipy.integrate.cumtrapz(\n            rad_drad.to_value(""rad-1""), rad.to_value(""rad""), initial=0\n        )\n\n        return ScaledRegularGridInterpolator(points=(rad,), values=values, fill_value=1)\n\n    @classmethod\n    def from_shape(cls, shape, width, rad):\n        """"""Make TablePSF objects with commonly used shapes.\n\n        This function is mostly useful for examples and testing.\n\n        Parameters\n        ----------\n        shape : {\'disk\', \'gauss\'}\n            PSF shape.\n        width : `~astropy.units.Quantity` with angle units\n            PSF width angle (radius for disk, sigma for Gauss).\n        rad : `~astropy.units.Quantity` with angle units\n            Offset angle\n\n        Returns\n        -------\n        psf : `TablePSF`\n            Table PSF\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> from astropy.coordinates import Angle\n        >>> from gammapy.irf import TablePSF\n        >>> rad = Angle(np.linspace(0, 0.7, 100), \'deg\')\n        >>> psf = TablePSF.from_shape(shape=\'gauss\', width=\'0.2 deg\', rad=rad)\n        """"""\n        width = Angle(width)\n        rad = Angle(rad)\n\n        if shape == ""disk"":\n            amplitude = 1 / (np.pi * width.radian ** 2)\n            psf_value = np.where(rad < width, amplitude, 0)\n        elif shape == ""gauss"":\n            gauss2d_pdf = Gauss2DPDF(sigma=width.radian)\n            psf_value = gauss2d_pdf(rad.radian)\n        else:\n            raise ValueError(f""Invalid shape: {shape}"")\n\n        psf_value = u.Quantity(psf_value, ""sr^-1"")\n\n        return cls(rad, psf_value)\n\n    def info(self):\n        """"""Print basic info.""""""\n        ss = array_stats_str(self.rad.deg, ""offset"")\n        ss += f""integral = {self.containment(self.rad[-1])}\\n""\n\n        for containment in [68, 80, 95]:\n            radius = self.containment_radius(0.01 * containment)\n            ss += f""containment radius {radius.deg} deg for {containment}%\\n""\n\n        return ss\n\n    def evaluate(self, rad):\n        r""""""Evaluate PSF.\n\n        The following PSF quantities are available:\n\n        * \'dp_domega\': PDF per 2-dim solid angle :math:`\\Omega` in sr^-1\n\n            .. math:: \\frac{dP}{d\\Omega}\n\n\n        Parameters\n        ----------\n        rad : `~astropy.coordinates.Angle`\n            Offset wrt source position\n\n        Returns\n        -------\n        psf_value : `~astropy.units.Quantity`\n            PSF value\n        """"""\n        rad = np.atleast_1d(u.Quantity(rad))\n        return self._interpolate((rad,))\n\n    def containment(self, rad_max):\n        """"""Compute PSF containment fraction.\n\n        Parameters\n        ----------\n        rad_max : `~astropy.units.Quantity`\n            Offset angle range\n\n        Returns\n        -------\n        integral : float\n            PSF integral\n        """"""\n        rad = np.atleast_1d(rad_max)\n        return self._interpolate_containment((rad,))\n\n    def containment_radius(self, fraction):\n        """"""Containment radius.\n\n        Parameters\n        ----------\n        fraction : array_like\n            Containment fraction (range 0 .. 1)\n\n        Returns\n        -------\n        rad : `~astropy.coordinates.Angle`\n            Containment radius angle\n        """"""\n        rad_max = Angle(np.linspace(0, self.rad[-1].value, 10 * len(self.rad)), ""rad"")\n        containment = self.containment(rad_max=rad_max)\n\n        fraction = np.atleast_1d(fraction)\n\n        fraction_idx = np.argmin(np.abs(containment - fraction[:, np.newaxis]), axis=1)\n        return rad_max[fraction_idx].to(""deg"")\n\n    def normalize(self):\n        """"""Normalize PSF to unit integral.\n\n        Computes the total PSF integral via the :math:`dP / dr` spline\n        and then divides the :math:`dP / dr` array.\n        """"""\n        integral = self.containment(self.rad[-1])\n        self.psf_value /= integral\n\n    def broaden(self, factor, normalize=True):\n        r""""""Broaden PSF by scaling the offset array.\n\n        For a broadening factor :math:`f` and the offset\n        array :math:`r`, the offset array scaled\n        in the following way:\n\n        .. math::\n            r_{new} = f \\times r_{old}\n            \\frac{dP}{dr}(r_{new}) = \\frac{dP}{dr}(r_{old})\n\n        Parameters\n        ----------\n        factor : float\n            Broadening factor\n        normalize : bool\n            Normalize PSF after broadening\n        """"""\n        self.rad *= factor\n        if normalize:\n            self.normalize()\n\n    def plot_psf_vs_rad(self, ax=None, **kwargs):\n        """"""Plot PSF vs radius.\n\n        Parameters\n        ----------\n        ax : ``\n\n        kwargs : dict\n            Keyword arguments passed to `matplotlib.pyplot.plot`\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        ax.plot(self.rad.to_value(""deg""), self.psf_value.to_value(""sr-1""), **kwargs)\n        ax.set_yscale(""log"")\n        ax.set_xlabel(""Radius (deg)"")\n        ax.set_ylabel(""PSF (sr-1)"")\n\n\nclass EnergyDependentTablePSF:\n    """"""Energy-dependent radially-symmetric table PSF (``gtpsf`` format).\n\n    TODO: add references and explanations.\n\n    Parameters\n    ----------\n    energy : `~astropy.units.Quantity`\n        Energy (1-dim)\n    rad : `~astropy.units.Quantity` with angle units\n        Offset angle wrt source position (1-dim)\n    exposure : `~astropy.units.Quantity`\n        Exposure (1-dim)\n    psf_value : `~astropy.units.Quantity`\n        PSF (2-dim with axes: psf[energy_index, offset_index]\n    interp_kwargs : dict\n        Interpolation keyword arguments pass to `ScaledRegularGridInterpolator`.\n    """"""\n\n    def __init__(self, energy, rad, exposure=None, psf_value=None, interp_kwargs=None):\n        self.energy = u.Quantity(energy).to(""GeV"")\n        self.rad = u.Quantity(rad).to(""radian"")\n        if exposure is None:\n            self.exposure = u.Quantity(np.ones(len(energy)), ""cm^2 s"")\n        else:\n            self.exposure = u.Quantity(exposure).to(""cm^2 s"")\n\n        if psf_value is None:\n            self.psf_value = u.Quantity(np.zeros(len(energy), len(rad)), ""sr^-1"")\n        else:\n            self.psf_value = u.Quantity(psf_value).to(""sr^-1"")\n\n        self._interp_kwargs = interp_kwargs or {}\n\n    @lazyproperty\n    def _interpolate(self):\n        points = (self.energy, self.rad)\n        return ScaledRegularGridInterpolator(\n            points=points, values=self.psf_value, **self._interp_kwargs\n        )\n\n    @lazyproperty\n    def _interpolate_containment(self):\n        if self.rad[0] > 0:\n            rad = self.rad.insert(0, 0)\n        else:\n            rad = self.rad\n\n        rad_drad = 2 * np.pi * rad * self.evaluate(energy=self.energy, rad=rad)\n        values = scipy.integrate.cumtrapz(\n            rad_drad.to_value(""rad-1""), rad.to_value(""rad""), initial=0, axis=1\n        )\n\n        points = (self.energy, rad)\n        return ScaledRegularGridInterpolator(points=points, values=values, fill_value=1)\n\n    def __str__(self):\n        ss = ""EnergyDependentTablePSF\\n""\n        ss += ""-----------------------\\n""\n        ss += ""\\nAxis info:\\n""\n        ss += ""  "" + array_stats_str(self.rad.to(""deg""), ""rad"")\n        ss += ""  "" + array_stats_str(self.energy, ""energy"")\n        ss += ""\\nContainment info:\\n""\n        # Print some example containment radii\n        fractions = [0.68, 0.95]\n        energies = u.Quantity([10, 100], ""GeV"")\n        for fraction in fractions:\n            rads = self.containment_radius(energy=energies, fraction=fraction)\n            for energy, rad in zip(energies, rads):\n                ss += f""  {100 * fraction}% containment radius at {energy:3.0f}: {rad:.2f}\\n""\n\n        return ss\n\n    @classmethod\n    def from_fits(cls, hdu_list):\n        """"""Create `EnergyDependentTablePSF` from ``gtpsf`` format HDU list.\n\n        Parameters\n        ----------\n        hdu_list : `~astropy.io.fits.HDUList`\n            HDU list with ``THETA`` and ``PSF`` extensions.\n        """"""\n        rad = Angle(hdu_list[""THETA""].data[""Theta""], ""deg"")\n        energy = u.Quantity(hdu_list[""PSF""].data[""Energy""], ""MeV"")\n        exposure = u.Quantity(hdu_list[""PSF""].data[""Exposure""], ""cm^2 s"")\n        psf_value = u.Quantity(hdu_list[""PSF""].data[""PSF""], ""sr^-1"")\n\n        return cls(energy, rad, exposure, psf_value)\n\n    def to_fits(self):\n        """"""Convert to FITS HDU list format.\n\n        Returns\n        -------\n        hdu_list : `~astropy.io.fits.HDUList`\n            PSF in HDU list format.\n        """"""\n        # TODO: write HEADER keywords as gtpsf\n\n        data = Table([self.rad.to(""deg"")], names=[""Theta""])\n        theta_hdu = fits.BinTableHDU(data=data, name=""THETA"")\n\n        data = Table(\n            [\n                self.energy.to(""MeV""),\n                self.exposure.to(""cm^2 s""),\n                self.psf_value.to(""sr^-1""),\n            ],\n            names=[""Energy"", ""Exposure"", ""PSF""],\n        )\n        psf_hdu = fits.BinTableHDU(data=data, name=""PSF"")\n\n        hdu_list = fits.HDUList([fits.PrimaryHDU(), theta_hdu, psf_hdu])\n        return hdu_list\n\n    @classmethod\n    def read(cls, filename):\n        """"""Create `EnergyDependentTablePSF` from ``gtpsf``-format FITS file.\n\n        Parameters\n        ----------\n        filename : str\n            File name\n        """"""\n        with fits.open(make_path(filename), memmap=False) as hdulist:\n            return cls.from_fits(hdulist)\n\n    def write(self, *args, **kwargs):\n        """"""Write to FITS file.\n\n        Calls `~astropy.io.fits.HDUList.writeto`, forwarding all arguments.\n        """"""\n        self.to_fits().writeto(*args, **kwargs)\n\n    def evaluate(self, energy=None, rad=None, method=""linear""):\n        """"""Evaluate the PSF at a given energy and offset\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy value\n        rad : `~astropy.coordinates.Angle`\n            Offset wrt source position\n        method : {""linear"", ""nearest""}\n            Linear or nearest neighbour interpolation.\n\n        Returns\n        -------\n        values : `~astropy.units.Quantity`\n            Interpolated value\n        """"""\n        if energy is None:\n            energy = self.energy\n\n        if rad is None:\n            rad = self.rad\n\n        energy = np.atleast_1d(u.Quantity(energy))[:, np.newaxis]\n        rad = np.atleast_1d(u.Quantity(rad))\n        return self._interpolate((energy, rad), clip=True, method=method)\n\n    def table_psf_at_energy(self, energy, method=""linear"", **kwargs):\n        """"""Create `~gammapy.irf.TablePSF` at one given energy.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy\n        method : {""linear"", ""nearest""}\n            Linear or nearest neighbour interpolation.\n\n        Returns\n        -------\n        psf : `~gammapy.irf.TablePSF`\n            Table PSF\n        """"""\n        psf_value = self.evaluate(energy=energy, method=method)[0, :]\n        return TablePSF(self.rad, psf_value, **kwargs)\n\n    def table_psf_in_energy_band(self, energy_band, spectrum=None, n_bins=11, **kwargs):\n        """"""Average PSF in a given energy band.\n\n        Expected counts in sub energy bands given the given exposure\n        and spectrum are used as weights.\n\n        Parameters\n        ----------\n        energy_band : `~astropy.units.Quantity`\n            Energy band\n        spectrum : `~gammapy.modeling.models.SpectralModel`\n            Spectral model used for weighting the PSF. Default is a power law\n            with index=2.\n        n_bins : int\n            Number of energy points in the energy band, used to compute the\n            weigthed PSF.\n\n        Returns\n        -------\n        psf : `TablePSF`\n            Table PSF\n        """"""\n        from gammapy.modeling.models import PowerLawSpectralModel, TemplateSpectralModel\n\n        if spectrum is None:\n            spectrum = PowerLawSpectralModel()\n\n        exposure = TemplateSpectralModel(self.energy, self.exposure)\n\n        e_min, e_max = energy_band\n        energy = MapAxis.from_energy_bounds(e_min, e_max, n_bins).edges\n\n        weights = spectrum(energy) * exposure(energy)\n        weights /= weights.sum()\n\n        psf_value = self.evaluate(energy=energy)\n        psf_value_weighted = weights[:, np.newaxis] * psf_value\n        return TablePSF(self.rad, psf_value_weighted.sum(axis=0), **kwargs)\n\n    def containment_radius(self, energy, fraction=0.68):\n        """"""Containment radius.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy\n        fraction : float\n            Containment fraction.\n\n        Returns\n        -------\n        rad : `~astropy.units.Quantity`\n            Containment radius in deg\n        """"""\n        # upsamle for better precision\n        rad_max = Angle(np.linspace(0, self.rad[-1].value, 10 * len(self.rad)), ""rad"")\n        containment = self.containment(energy=energy, rad_max=rad_max)\n\n        # find nearest containment value\n        fraction_idx = np.argmin(np.abs(containment - fraction), axis=1)\n        return rad_max[fraction_idx].to(""deg"")\n\n    def containment(self, energy, rad_max):\n        """"""Compute containment of the PSF.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy\n        rad_max : `~astropy.coordinates.Angle`\n            Maximum offset angle.\n\n        Returns\n        -------\n        fraction : array_like\n            Containment fraction (in range 0 .. 1)\n        """"""\n        energy = np.atleast_1d(u.Quantity(energy))[:, np.newaxis]\n        rad_max = np.atleast_1d(u.Quantity(rad_max))\n        return self._interpolate_containment((energy, rad_max))\n\n    def info(self):\n        """"""Print basic info""""""\n        print(str(self))\n\n    def plot_psf_vs_rad(self, energies=None, ax=None, **kwargs):\n        """"""Plot PSF vs radius.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energies where to plot the PSF.\n        **kwargs : dict\n            Keyword arguments pass to `~matplotlib.pyplot.plot`.\n        """"""\n        import matplotlib.pyplot as plt\n\n        if energies is None:\n            energies = [100, 1000, 10000] * u.GeV\n\n        ax = plt.gca() if ax is None else ax\n\n        for energy in energies:\n            psf_value = np.squeeze(self.evaluate(energy=energy))\n            label = f""{energy:.0f}""\n            ax.plot(\n                self.rad.to_value(""deg""),\n                psf_value.to_value(""sr-1""),\n                label=label,\n                **kwargs,\n            )\n\n        ax.set_yscale(""log"")\n        ax.set_xlabel(""Offset (deg)"")\n        ax.set_ylabel(""PSF (1 / sr)"")\n        plt.legend()\n        return ax\n\n    def plot_containment_vs_energy(\n        self, ax=None, fractions=[0.68, 0.8, 0.95], **kwargs\n    ):\n        """"""Plot containment versus energy.""""""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        for fraction in fractions:\n            rad = self.containment_radius(self.energy, fraction)\n            label = f""{100 * fraction:.1f}% Containment""\n            ax.plot(self.energy.value, rad.value, label=label, **kwargs)\n\n        ax.semilogx()\n        ax.legend(loc=""best"")\n        ax.set_xlabel(""Energy (GeV)"")\n        ax.set_ylabel(""Containment radius (deg)"")\n\n    def plot_exposure_vs_energy(self):\n        """"""Plot exposure versus energy.""""""\n        import matplotlib.pyplot as plt\n\n        plt.figure(figsize=(4, 3))\n        plt.plot(self.energy, self.exposure, color=""black"", lw=3)\n        plt.semilogx()\n        plt.xlabel(""Energy (MeV)"")\n        plt.ylabel(""Exposure (cm^2 s)"")\n        plt.xlim(1e4 / 1.3, 1.3 * 1e6)\n        plt.ylim(0, 1.5e11)\n        plt.tight_layout()\n\n    def stack(self, psf):\n        """"""Stack two EnergyDependentTablePSF objects.s\n\n        Parameters\n        ----------\n        psf : `EnergyDependentTablePSF`\n            PSF to stack.\n\n        Returns\n        -------\n        stacked_psf : `EnergyDependentTablePSF`\n            Stacked PSF.\n\n        """"""\n        exposure = self.exposure + psf.exposure\n        psf_value = self.psf_value.T * self.exposure + psf.psf_value.T * psf.exposure\n\n        with np.errstate(invalid=""ignore""):\n            # exposure can be zero\n            psf_value = np.nan_to_num(psf_value / exposure)\n\n        return self.__class__(\n            energy=self.energy, rad=self.rad, psf_value=psf_value.T, exposure=exposure\n        )\n'"
gammapy/makers/__init__.py,0,b'from .background import *\nfrom .map import *\nfrom .safe import *\nfrom .spectrum import *\n'
gammapy/makers/map.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nfrom gammapy.datasets import MapDataset\nfrom gammapy.irf import EnergyDependentMultiGaussPSF\nfrom gammapy.maps import Map\nfrom gammapy.modeling.models import BackgroundModel\nfrom .utils import (\n    make_edisp_map,\n    make_edisp_kernel_map,\n    make_map_background_irf,\n    make_map_exposure_true_energy,\n    make_psf_map,\n)\n\n__all__ = [""MapDatasetMaker""]\n\nlog = logging.getLogger(__name__)\n\n\nclass MapDatasetMaker:\n    """"""Make maps for a single IACT observation.\n\n    Parameters\n    ----------\n    background_oversampling : int\n        Background evaluation oversampling factor in energy.\n    selection : list\n        List of str, selecting which maps to make.\n        Available: \'counts\', \'exposure\', \'background\', \'psf\', \'edisp\'\n        By default, all maps are made.\n    """"""\n\n    available_selection = [""counts"", ""exposure"", ""background"", ""psf"", ""edisp""]\n\n    def __init__(self, background_oversampling=None, selection=None):\n        self.background_oversampling = background_oversampling\n\n        if selection is None:\n            selection = self.available_selection\n\n        selection = set(selection)\n\n        if not selection.issubset(self.available_selection):\n            difference = selection.difference(self.available_selection)\n            raise ValueError(f""{difference} is not a valid method."")\n\n        self.selection = selection\n\n    @staticmethod\n    def make_counts(geom, observation):\n        """"""Make counts map.\n\n        Parameters\n        ----------\n        geom : `~gammapy.maps.Geom`\n            Reference map geom.\n        observation : `~gammapy.data.Observation`\n            Observation container.\n\n        Returns\n        -------\n        counts : `~gammapy.maps.Map`\n            Counts map.\n        """"""\n        counts = Map.from_geom(geom)\n        counts.fill_events(observation.events)\n        return counts\n\n    @staticmethod\n    def make_exposure(geom, observation):\n        """"""Make exposure map.\n\n        Parameters\n        ----------\n        geom : `~gammapy.maps.Geom`\n            Reference map geom.\n        observation : `~gammapy.data.Observation`\n            Observation container.\n\n        Returns\n        -------\n        exposure : `~gammapy.maps.Map`\n            Exposure map.\n        """"""\n        return make_map_exposure_true_energy(\n            pointing=observation.pointing_radec,\n            livetime=observation.observation_live_time_duration,\n            aeff=observation.aeff,\n            geom=geom,\n        )\n\n    @staticmethod\n    def make_exposure_irf(geom, observation):\n        """"""Make exposure map with irf geometry.\n\n        Parameters\n        ----------\n        geom : `~gammapy.maps.Geom`\n            Reference geom.\n        observation : `~gammapy.data.Observation`\n            Observation container.\n\n        Returns\n        -------\n        exposure : `~gammapy.maps.Map`\n            Exposure map.\n        """"""\n        return make_map_exposure_true_energy(\n            pointing=observation.pointing_radec,\n            livetime=observation.observation_live_time_duration,\n            aeff=observation.aeff,\n            geom=geom,\n        )\n\n    def make_background(self, geom, observation):\n        """"""Make background map.\n\n        Parameters\n        ----------\n        geom : `~gammapy.maps.Geom`\n            Reference geom.\n        observation : `~gammapy.data.Observation`\n            Observation container.\n\n        Returns\n        -------\n        background : `~gammapy.maps.Map`\n            Background map.\n        """"""\n        bkg_coordsys = observation.bkg.meta.get(""FOVALIGN"", ""RADEC"")\n\n        if bkg_coordsys == ""ALTAZ"":\n            pointing = observation.fixed_pointing_info\n        elif bkg_coordsys == ""RADEC"":\n            pointing = observation.pointing_radec\n        else:\n            raise ValueError(\n                f""Invalid background coordinate system: {bkg_coordsys!r}\\n""\n                ""Options: ALTAZ, RADEC""\n            )\n\n        return make_map_background_irf(\n            pointing=pointing,\n            ontime=observation.observation_time_duration,\n            bkg=observation.bkg,\n            geom=geom,\n            oversampling=self.background_oversampling,\n        )\n\n    def make_edisp(self, geom, observation):\n        """"""Make energy dispersion map.\n\n        Parameters\n        ----------\n        geom : `~gammapy.maps.Geom`\n            Reference geom.\n        observation : `~gammapy.data.Observation`\n            Observation container.\n\n        Returns\n        -------\n        edisp : `~gammapy.cube.EDispMap`\n            Edisp map.\n        """"""\n        exposure = self.make_exposure_irf(geom.squash(axis=""migra""), observation)\n\n        return make_edisp_map(\n            edisp=observation.edisp,\n            pointing=observation.pointing_radec,\n            geom=geom,\n            exposure_map=exposure,\n        )\n\n    def make_edisp_kernel(self, geom, observation):\n        """"""Make energy dispersion kernel map.\n\n        Parameters\n        ----------\n        geom : `~gammapy.maps.Geom`\n            Reference geom. Must contain ""energy"" and ""energy_true"" axes in that order.\n        observation : `~gammapy.data.Observation`\n            Observation container.\n\n        Returns\n        -------\n        edisp : `~gammapy.cube.EDispKernelMap`\n            EdispKernel map.\n        """"""\n        exposure = self.make_exposure_irf(geom.squash(axis=""energy""), observation)\n\n        return make_edisp_kernel_map(\n            edisp=observation.edisp,\n            pointing=observation.pointing_radec,\n            geom=geom,\n            exposure_map=exposure,\n        )\n\n    def make_psf(self, geom, observation):\n        """"""Make psf map.\n\n        Parameters\n        ----------\n        geom : `~gammapy.maps.Geom`\n            Reference geom.\n        observation : `~gammapy.data.Observation`\n            Observation container.\n\n        Returns\n        -------\n        psf : `~gammapy.cube.PSFMap`\n            Psf map.\n        """"""\n        psf = observation.psf\n        if isinstance(psf, EnergyDependentMultiGaussPSF):\n            rad_axis = geom.get_axis_by_name(""theta"")\n            psf = psf.to_psf3d(rad=rad_axis.center)\n\n        exposure = self.make_exposure_irf(geom.squash(axis=""theta""), observation)\n\n        return make_psf_map(\n            psf=psf,\n            pointing=observation.pointing_radec,\n            geom=geom,\n            exposure_map=exposure,\n        )\n\n    def run(self, dataset, observation):\n        """"""Make map dataset.\n\n        Parameters\n        ----------\n        dataset : `~gammapy.cube.MapDataset`\n            Reference dataset.\n        observation : `~gammapy.data.Observation`\n            Observation\n\n        Returns\n        -------\n        dataset : `~gammapy.cube.MapDataset`\n            Map dataset.\n        """"""\n        kwargs = {""gti"": observation.gti}\n\n        mask_safe = Map.from_geom(dataset.counts.geom, dtype=bool)\n        mask_safe.data |= True\n\n        kwargs[""mask_safe""] = mask_safe\n\n        if ""counts"" in self.selection:\n            counts = self.make_counts(dataset.counts.geom, observation)\n            kwargs[""counts""] = counts\n\n        if ""exposure"" in self.selection:\n            exposure = self.make_exposure(dataset.exposure.geom, observation)\n            kwargs[""exposure""] = exposure\n\n        if ""background"" in self.selection:\n            background_map = self.make_background(dataset.counts.geom, observation)\n            kwargs[""models""] = BackgroundModel(\n                background_map,\n                name=dataset.name + ""-bkg"",\n                datasets_names=[dataset.name],\n            )\n\n        if ""psf"" in self.selection:\n            psf = self.make_psf(dataset.psf.psf_map.geom, observation)\n            kwargs[""psf""] = psf\n\n        if ""edisp"" in self.selection:\n            if dataset.edisp.edisp_map.geom.axes[0].name.upper() == ""MIGRA"":\n                edisp = self.make_edisp(dataset.edisp.edisp_map.geom, observation)\n            else:\n                edisp = self.make_edisp_kernel(\n                    dataset.edisp.edisp_map.geom, observation\n                )\n\n            kwargs[""edisp""] = edisp\n\n        return MapDataset(name=dataset.name, **kwargs)\n'"
gammapy/makers/safe.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport numpy as np\nfrom astropy.coordinates import Angle\nfrom gammapy.datasets import MapDataset\nfrom gammapy.irf import EffectiveAreaTable, EDispKernelMap\nfrom gammapy.maps import Map, MapCoord\n\n__all__ = [""SafeMaskMaker""]\n\n\nlog = logging.getLogger(__name__)\n\n\nclass SafeMaskMaker:\n    """"""Make safe data range mask for a given observation.\n\n    Parameters\n    ----------\n    methods : {""aeff-default"", ""aeff-max"", ""edisp-bias"", ""offset-max"", ""bkg-peak""}\n        Method to use for the safe energy range. Can be a\n        list with a combination of those. Resulting masks\n        are combined with logical `and`. ""aeff-default""\n        uses the energy ranged specified in the DL3 data\n        files, if available.\n    aeff_percent : float\n        Percentage of the maximal effective area to be used\n        as lower energy threshold for method ""aeff-max"".\n    bias_percent : float\n        Percentage of the energy bias to be used as lower\n        energy threshold for method ""edisp-bias""\n    position : `~astropy.coordinates.SkyCoord`\n        Position at which the `aeff_percent` or `bias_percent` are computed. By default,\n        it uses the position of the center of the map.\n    offset_max : str or `~astropy.units.Quantity`\n        Maximum offset cut.\n    """"""\n\n    available_methods = {\n        ""aeff-default"",\n        ""aeff-max"",\n        ""edisp-bias"",\n        ""offset-max"",\n        ""bkg-peak"",\n    }\n\n    def __init__(\n        self,\n        methods=(""aeff-default"",),\n        aeff_percent=10,\n        bias_percent=10,\n        position=None,\n        offset_max=""3 deg"",\n    ):\n        methods = set(methods)\n\n        if not methods.issubset(self.available_methods):\n            difference = methods.difference(self.available_methods)\n            raise ValueError(f""{difference} is not a valid method."")\n\n        self.methods = methods\n        self.aeff_percent = aeff_percent\n        self.bias_percent = bias_percent\n        self.position = position\n        self.offset_max = Angle(offset_max)\n\n    def make_mask_offset_max(self, dataset, observation):\n        """"""Make maximum offset mask.\n\n        Parameters\n        ----------\n        dataset : `~gammapy.datasets.MapDataset` or `~gammapy.datasets.SpectrumDataset`\n            Dataset to compute mask for.\n        observation: `~gammapy.data.Observation`\n            Observation to compute mask for.\n\n        Returns\n        -------\n        mask_safe : `~numpy.ndarray`\n            Maximum offset mask.\n        """"""\n        separation = dataset._geom.separation(observation.pointing_radec)\n        return separation < self.offset_max\n\n    @staticmethod\n    def make_mask_energy_aeff_default(dataset, observation):\n        """"""Make safe energy mask from aeff default.\n\n        Parameters\n        ----------\n        dataset : `~gammapy.datasets.MapDataset` or `~gammapy.datasets.SpectrumDataset`\n            Dataset to compute mask for.\n        observation: `~gammapy.data.Observation`\n            Observation to compute mask for.\n\n        Returns\n        -------\n        mask_safe : `~numpy.ndarray`\n            Safe data range mask.\n        """"""\n        try:\n            e_max = observation.aeff.high_threshold\n            e_min = observation.aeff.low_threshold\n        except KeyError:\n            log.warning(f""No thresholds defined for obs {observation}"")\n            e_min, e_max = None, None\n\n        return dataset.counts.geom.energy_mask(emin=e_min, emax=e_max)\n\n    def make_mask_energy_aeff_max(self, dataset):\n        """"""Make safe energy mask from effective area maximum value.\n\n        Parameters\n        ----------\n        dataset : `~gammapy.datasets.MapDataset` or `~gammapy.datasets.SpectrumDataset`\n            Dataset to compute mask for.\n\n        Returns\n        -------\n        mask_safe : `~numpy.ndarray`\n            Safe data range mask.\n        """"""\n        geom = dataset._geom\n\n        if isinstance(dataset, MapDataset):\n            position = self.position\n            if position is None:\n                position = dataset.counts.geom.center_skydir\n            exposure = dataset.exposure\n            energy = exposure.geom.get_axis_by_name(""energy_true"")\n            coord = MapCoord.create(\n                {""skycoord"": position, ""energy_true"": energy.center}\n            )\n            exposure_1d = exposure.interp_by_coord(coord)\n            aeff = EffectiveAreaTable(\n                energy_lo=energy.edges[:-1],\n                energy_hi=energy.edges[1:],\n                data=exposure_1d,\n            )\n        else:\n            aeff = dataset.aeff\n\n        aeff_thres = (self.aeff_percent / 100) * aeff.max_area\n        e_min = aeff.find_energy(aeff_thres)\n        return geom.energy_mask(emin=e_min)\n\n    def make_mask_energy_edisp_bias(self, dataset):\n        """"""Make safe energy mask from energy dispersion bias.\n\n        Parameters\n        ----------\n        dataset : `~gammapy.datasets.MapDataset` or `~gammapy.datasets.SpectrumDataset`\n            Dataset to compute mask for.\n\n        Returns\n        -------\n        mask_safe : `~numpy.ndarray`\n            Safe data range mask.\n        """"""\n        edisp, geom = dataset.edisp, dataset._geom\n\n        if isinstance(dataset, MapDataset):\n            position = self.position\n            if position is None:\n                position = dataset.counts.geom.center_skydir\n            e_reco = dataset.counts.geom.get_axis_by_name(""energy"").edges\n            if isinstance(edisp, EDispKernelMap):\n                edisp = edisp.get_edisp_kernel(position)\n            else:\n                edisp = edisp.get_edisp_kernel(position, e_reco)\n\n        e_min = edisp.get_bias_energy(self.bias_percent / 100)\n        return geom.energy_mask(emin=e_min)\n\n    @staticmethod\n    def make_mask_energy_bkg_peak(dataset):\n        """"""Make safe energy mask based on the binned background.\n\n        The energy threshold is defined as the upper edge of the energy\n        bin with the highest predicted background rate. This method is motivated\n        by its use in the HESS DL3 validation paper: https://arxiv.org/pdf/1910.08088.pdf\n\n        Parameters\n        ----------\n        dataset : `~gammapy.datasets.MapDataset` or `~gammapy.datasets.SpectrumDataset`\n            Dataset to compute mask for.\n\n        Returns\n        -------\n        mask_safe : `~numpy.ndarray`\n            Safe data range mask.\n        """"""\n        geom = dataset.counts.geom\n\n        if isinstance(dataset, MapDataset):\n            background_spectrum = dataset.background_model.map.get_spectrum()\n        else:\n            background_spectrum = dataset.background\n\n        idx = np.argmax(background_spectrum.data, axis=0)\n        energy_axis = geom.get_axis_by_name(""energy"")\n        e_min = energy_axis.pix_to_coord(idx)\n        return geom.energy_mask(emin=e_min)\n\n    def run(self, dataset, observation=None):\n        """"""Make safe data range mask.\n\n        Parameters\n        ----------\n        dataset : `~gammapy.datasets.MapDataset` or `~gammapy.datasets.SpectrumDataset`\n            Dataset to compute mask for.\n        observation: `~gammapy.data.Observation`\n            Observation to compute mask for.\n\n        Returns\n        -------\n        dataset : `Dataset`\n            Dataset with defined safe range mask.\n        """"""\n        mask_safe = np.ones(dataset._geom.data_shape, dtype=bool)\n\n        if ""offset-max"" in self.methods:\n            mask_safe &= self.make_mask_offset_max(dataset, observation)\n\n        if ""aeff-default"" in self.methods:\n            mask_safe &= self.make_mask_energy_aeff_default(dataset, observation)\n\n        if ""aeff-max"" in self.methods:\n            mask_safe &= self.make_mask_energy_aeff_max(dataset)\n\n        if ""edisp-bias"" in self.methods:\n            mask_safe &= self.make_mask_energy_edisp_bias(dataset)\n\n        if ""bkg-peak"" in self.methods:\n            mask_safe &= self.make_mask_energy_bkg_peak(dataset)\n\n        dataset.mask_safe = Map.from_geom(dataset._geom, data=mask_safe)\n        return dataset\n'"
gammapy/makers/spectrum.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nfrom astropy import units as u\nfrom regions import CircleSkyRegion\nfrom gammapy.datasets import SpectrumDataset\nfrom gammapy.maps import RegionNDMap\n\n__all__ = [""SpectrumDatasetMaker""]\n\nlog = logging.getLogger(__name__)\n\n\nclass SpectrumDatasetMaker:\n    """"""Make spectrum for a single IACT observation.\n\n    The irfs and background are computed at a single fixed offset,\n    which is recommend only for point-sources.\n\n    Parameters\n    ----------\n    containment_correction : bool\n        Apply containment correction for point sources and circular on regions.\n    selection : list\n        List of str, selecting which maps to make.\n        Available: \'counts\', \'aeff\', \'background\', \'edisp\'\n        By default, all spectra are made.\n    """"""\n\n    available_selection = [""counts"", ""background"", ""aeff"", ""edisp""]\n\n    def __init__(self, containment_correction=False, selection=None):\n        self.containment_correction = containment_correction\n\n        if selection is None:\n            selection = self.available_selection\n\n        self.selection = selection\n\n    @staticmethod\n    def make_counts(geom, observation):\n        """"""Make counts map.\n\n        Parameters\n        ----------\n        geom : `~gammapy.maps.RegionGeom`\n            Reference map geom.\n        observation : `~gammapy.data.Observation`\n            Observation container.\n\n        Returns\n        -------\n        counts : `~gammapy.maps.RegionNDMap`\n            Counts map.\n        """"""\n        counts = RegionNDMap.from_geom(geom)\n        counts.fill_events(observation.events)\n        return counts\n\n    @staticmethod\n    def make_background(geom, observation):\n        """"""Make background.\n\n        Parameters\n        ----------\n        geom : `~gammapy.maps.RegionGeom`\n            Reference map geom.\n        observation: `~gammapy.data.Observation`\n            Observation to compute effective area for.\n\n        Returns\n        -------\n        background : `~gammapy.spectrum.RegionNDMap`\n            Background spectrum\n        """"""\n        offset = observation.pointing_radec.separation(geom.center_skydir)\n        e_reco = geom.get_axis_by_name(""energy"").edges\n\n        bkg = observation.bkg\n\n        data = bkg.evaluate_integrate(\n            fov_lon=0 * u.deg, fov_lat=offset, energy_reco=e_reco\n        )\n\n        data *= geom.solid_angle()\n        data *= observation.observation_time_duration\n        return RegionNDMap.from_geom(geom=geom, data=data.to_value(""""))\n\n    def make_aeff(self, region, energy_axis_true, observation):\n        """"""Make effective area.\n\n        Parameters\n        ----------\n        region : `~regions.SkyRegion`\n            Region to compute background effective area.\n        energy_axis_true : `~gammapy.maps.MapAxis`\n            True energy axis.\n        observation: `~gammapy.data.Observation`\n            Observation to compute effective area for.\n\n        Returns\n        -------\n        aeff : `~gammapy.irf.EffectiveAreaTable`\n            Effective area table.\n        """"""\n        offset = observation.pointing_radec.separation(region.center)\n        aeff = observation.aeff.to_effective_area_table(\n            offset, energy=energy_axis_true.edges\n        )\n\n        if self.containment_correction:\n            if not isinstance(region, CircleSkyRegion):\n                raise TypeError(\n                    ""Containment correction only supported for circular regions.""\n                )\n            psf = observation.psf.to_energy_dependent_table_psf(theta=offset)\n            containment = psf.containment(aeff.energy.center, region.radius)\n            aeff.data.data *= containment.squeeze()\n\n        return aeff\n\n    @staticmethod\n    def make_edisp(position, energy_axis, energy_axis_true, observation):\n        """"""Make energy dispersion.\n\n        Parameters\n        ----------\n        position : `~astropy.coordinates.SkyCoord`\n            Position to compute energy dispersion for.\n        energy_axis : `~gammapy.maps.MapAxis`\n            Reconstructed energy axis.\n        energy_axis_true : `~gammapy.maps.MapAxis`\n            True energy axis.\n        observation: `~gammapy.data.Observation`\n            Observation to compute edisp for.\n\n        Returns\n        -------\n        edisp : `~gammapy.irf.EDispKernel`\n            Energy dispersion\n        """"""\n        offset = observation.pointing_radec.separation(position)\n        return observation.edisp.to_energy_dispersion(\n            offset, e_reco=energy_axis.edges, e_true=energy_axis_true.edges\n        )\n\n    def run(self, dataset, observation):\n        """"""Make spectrum dataset.\n\n        Parameters\n        ----------\n        dataset : `~gammapy.spectrum.SpectrumDataset`\n            Spectrum dataset.\n        observation: `~gammapy.data.Observation`\n            Observation to reduce.\n\n        Returns\n        -------\n        dataset : `~gammapy.spectrum.SpectrumDataset`\n            Spectrum dataset.\n        """"""\n        kwargs = {\n            ""gti"": observation.gti,\n            ""livetime"": observation.observation_live_time_duration,\n        }\n        energy_axis = dataset.counts.geom.get_axis_by_name(""energy"")\n        energy_axis_true = dataset.aeff.data.axis(""energy_true"")\n        region = dataset.counts.geom.region\n\n        if ""counts"" in self.selection:\n            kwargs[""counts""] = self.make_counts(dataset.counts.geom, observation)\n\n        if ""background"" in self.selection:\n            kwargs[""background""] = self.make_background(\n                dataset.counts.geom, observation\n            )\n\n        if ""aeff"" in self.selection:\n            kwargs[""aeff""] = self.make_aeff(region, energy_axis_true, observation)\n\n        if ""edisp"" in self.selection:\n\n            kwargs[""edisp""] = self.make_edisp(\n                region.center, energy_axis, energy_axis_true, observation\n            )\n\n        return SpectrumDataset(name=dataset.name, **kwargs)\n'"
gammapy/makers/utils.py,8,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom astropy.coordinates import SkyOffsetFrame\nfrom gammapy.data import FixedPointingInfo\nfrom gammapy.irf import EDispMap, PSFMap\nfrom gammapy.maps import Map, WcsNDMap\nfrom gammapy.modeling.models import PowerLawSpectralModel\nfrom gammapy.utils.coordinates import sky_to_fov\n\n__all__ = [\n    ""make_map_background_irf"",\n    ""make_edisp_map"",\n    ""make_psf_map"",\n    ""make_map_exposure_true_energy"",\n]\n\n\ndef make_map_exposure_true_energy(pointing, livetime, aeff, geom):\n    """"""Compute exposure map.\n\n    This map has a true energy axis, the exposure is not combined\n    with energy dispersion.\n\n    Parameters\n    ----------\n    pointing : `~astropy.coordinates.SkyCoord`\n        Pointing direction\n    livetime : `~astropy.units.Quantity`\n        Livetime\n    aeff : `~gammapy.irf.EffectiveAreaTable2D`\n        Effective area\n    geom : `~gammapy.maps.WcsGeom`\n        Map geometry (must have an energy axis)\n\n    Returns\n    -------\n    map : `~gammapy.maps.WcsNDMap`\n        Exposure map\n    """"""\n    offset = geom.separation(pointing)\n    energy = geom.get_axis_by_name(""energy_true"").center\n\n    exposure = aeff.data.evaluate(\n        offset=offset, energy_true=energy[:, np.newaxis, np.newaxis]\n    )\n    # TODO: Improve IRF evaluate to preserve energy axis if length 1\n    # For now, we handle that case via this hack:\n    if len(exposure.shape) < 3:\n        exposure = np.expand_dims(exposure.value, 0) * exposure.unit\n\n    exposure = (exposure * livetime).to(""m2 s"")\n\n    return WcsNDMap(geom, exposure.value.reshape(geom.data_shape), unit=exposure.unit)\n\n\ndef _map_spectrum_weight(map, spectrum=None):\n    """"""Weight a map with a spectrum.\n\n    This requires map to have an ""energy"" axis.\n    The weights are normalised so that they sum to 1.\n    The mean and unit of the output image is the same as of the input cube.\n\n    At the moment this is used to get a weighted exposure image.\n\n    Parameters\n    ----------\n    map : `~gammapy.maps.Map`\n        Input map with an ""energy"" axis.\n    spectrum : `~gammapy.modeling.models.SpectralModel`\n        Spectral model to compute the weights.\n        Default is power-law with spectral index of 2.\n\n    Returns\n    -------\n    map_weighted : `~gammapy.maps.Map`\n        Weighted image\n    """"""\n    if spectrum is None:\n        spectrum = PowerLawSpectralModel(index=2.0)\n\n    # Compute weights vector\n    energy_edges = map.geom.get_axis_by_name(""energy_true"").edges\n    weights = spectrum.integral(\n        emin=energy_edges[:-1], emax=energy_edges[1:], intervals=True\n    )\n    weights /= weights.sum()\n    shape = np.ones(len(map.geom.data_shape))\n    shape[0] = -1\n    return map * weights.reshape(shape.astype(int))\n\n\ndef make_map_background_irf(pointing, ontime, bkg, geom, oversampling=None):\n    """"""Compute background map from background IRFs.\n\n    Parameters\n    ----------\n    pointing : `~gammapy.data.FixedPointingInfo` or `~astropy.coordinates.SkyCoord`\n        Observation pointing\n\n        - If a ``FixedPointingInfo`` is passed, FOV coordinates are properly computed.\n        - If a ``SkyCoord`` is passed, FOV frame rotation is not taken into account.\n    ontime : `~astropy.units.Quantity`\n        Observation ontime. i.e. not corrected for deadtime\n        see https://gamma-astro-data-formats.readthedocs.io/en/stable/irfs/full_enclosure/bkg/index.html#notes)\n    bkg : `~gammapy.irf.Background3D`\n        Background rate model\n    geom : `~gammapy.maps.WcsGeom`\n        Reference geometry\n    oversampling: int\n        Oversampling factor in energy, used for the background model evaluation.\n\n    Returns\n    -------\n    background : `~gammapy.maps.WcsNDMap`\n        Background predicted counts sky cube in reco energy\n    """"""\n    # TODO:\n    #  This implementation can be improved in two ways:\n    #  1. Create equal time intervals between TSTART and TSTOP and sum up the\n    #  background IRF for each interval. This is instead of multiplying by\n    #  the total ontime. This then handles the rotation of the FoV.\n    #  2. Use the pointing table (does not currently exist in CTA files) to\n    #  obtain the RA DEC and time for each interval. This then considers that\n    #  the pointing might change slightly over the observation duration\n\n    # Get altaz coords for map\n    if oversampling is not None:\n        geom = geom.upsample(factor=oversampling, axis=""energy"")\n\n    map_coord = geom.to_image().get_coord()\n    sky_coord = map_coord.skycoord\n\n    if isinstance(pointing, FixedPointingInfo):\n        altaz_coord = sky_coord.transform_to(pointing.altaz_frame)\n\n        # Compute FOV coordinates of map relative to pointing\n        fov_lon, fov_lat = sky_to_fov(\n            altaz_coord.az, altaz_coord.alt, pointing.altaz.az, pointing.altaz.alt\n        )\n    else:\n        # Create OffsetFrame\n        frame = SkyOffsetFrame(origin=pointing)\n        pseudo_fov_coord = sky_coord.transform_to(frame)\n        fov_lon = pseudo_fov_coord.lon\n        fov_lat = pseudo_fov_coord.lat\n\n    energies = geom.get_axis_by_name(""energy"").edges\n\n    bkg_de = bkg.evaluate_integrate(\n        fov_lon=fov_lon,\n        fov_lat=fov_lat,\n        energy_reco=energies[:, np.newaxis, np.newaxis],\n    )\n\n    d_omega = geom.to_image().solid_angle()\n    data = (bkg_de * d_omega * ontime).to_value("""")\n    bkg_map = WcsNDMap(geom, data=data)\n\n    if oversampling is not None:\n        bkg_map = bkg_map.downsample(factor=oversampling, axis=""energy"")\n\n    return bkg_map\n\n\ndef make_psf_map(psf, pointing, geom, exposure_map=None):\n    """"""Make a psf map for a single observation\n\n    Expected axes : rad and true energy in this specific order\n    The name of the rad MapAxis is expected to be \'rad\'\n\n    Parameters\n    ----------\n    psf : `~gammapy.irf.PSF3D`\n        the PSF IRF\n    pointing : `~astropy.coordinates.SkyCoord`\n        the pointing direction\n    geom : `~gammapy.maps.Geom`\n        the map geom to be used. It provides the target geometry.\n        rad and true energy axes should be given in this specific order.\n    exposure_map : `~gammapy.maps.Map`, optional\n        the associated exposure map.\n        default is None\n\n    Returns\n    -------\n    psfmap : `~gammapy.cube.PSFMap`\n        the resulting PSF map\n    """"""\n    energy_axis = geom.get_axis_by_name(""energy_true"")\n    energy = energy_axis.center\n\n    rad_axis = geom.get_axis_by_name(""theta"")\n    rad = rad_axis.center\n\n    # Compute separations with pointing position\n    offset = geom.separation(pointing)\n\n    # Compute PSF values\n    # TODO: allow broadcasting in PSF3D.evaluate()\n    psf_values = psf._interpolate(\n        (\n            rad[:, np.newaxis, np.newaxis],\n            offset,\n            energy[:, np.newaxis, np.newaxis, np.newaxis],\n        )\n    )\n\n    # TODO: this probably does not ensure that probability is properly normalized in the PSFMap\n    # Create Map and fill relevant entries\n    data = psf_values.to_value(""sr-1"")\n    psfmap = Map.from_geom(geom, data=data, unit=""sr-1"")\n    return PSFMap(psfmap, exposure_map)\n\n\ndef make_edisp_map(edisp, pointing, geom, exposure_map=None):\n    """"""Make a edisp map for a single observation\n\n    Expected axes : migra and true energy in this specific order\n    The name of the migra MapAxis is expected to be \'migra\'\n\n    Parameters\n    ----------\n    edisp : `~gammapy.irf.EnergyDispersion2D`\n        the 2D Energy Dispersion IRF\n    pointing : `~astropy.coordinates.SkyCoord`\n        the pointing direction\n    geom : `~gammapy.maps.Geom`\n        the map geom to be used. It provides the target geometry.\n        migra and true energy axes should be given in this specific order.\n    exposure_map : `~gammapy.maps.Map`, optional\n        the associated exposure map.\n        default is None\n\n    Returns\n    -------\n    edispmap : `~gammapy.cube.EDispMap`\n        the resulting EDisp map\n    """"""\n    energy_axis = geom.get_axis_by_name(""energy_true"")\n    energy = energy_axis.center\n\n    migra_axis = geom.get_axis_by_name(""migra"")\n    migra = migra_axis.center\n\n    # Compute separations with pointing position\n    offset = geom.separation(pointing)\n\n    # Compute EDisp values\n    edisp_values = edisp.data.evaluate(\n        offset=offset,\n        energy_true=energy[:, np.newaxis, np.newaxis, np.newaxis],\n        migra=migra[:, np.newaxis, np.newaxis],\n    )\n\n    # Create Map and fill relevant entries\n    data = edisp_values.to_value("""")\n    edispmap = Map.from_geom(geom, data=data, unit="""")\n    return EDispMap(edispmap, exposure_map)\n\n\ndef make_edisp_kernel_map(edisp, pointing, geom, exposure_map=None):\n    """"""Make a edisp kernel map for a single observation\n\n    Expected axes : (reco) energy and true energy in this specific order\n    The name of the reco energy MapAxis is expected to be \'energy\'.\n    The name of the true energy MapAxis is expected to be \'energy_true\'.\n\n    Parameters\n    ----------\n    edisp : `~gammapy.irf.EnergyDispersion2D`\n        the 2D Energy Dispersion IRF\n    pointing : `~astropy.coordinates.SkyCoord`\n        the pointing direction\n    geom : `~gammapy.maps.Geom`\n        the map geom to be used. It provides the target geometry.\n        energy and true energy axes should be given in this specific order.\n    exposure_map : `~gammapy.maps.Map`, optional\n        the associated exposure map.\n        default is None\n\n    Returns\n    -------\n    edispmap : `~gammapy.cube.EDispKernelMap`\n        the resulting EDispKernel map\n    """"""\n    # Use EnergyDispersion2D migra axis.\n    migra_axis = edisp.data.axis(""migra"")\n\n    # Create temporary EDispMap Geom\n    new_geom = geom.to_image().to_cube(\n        [migra_axis, geom.get_axis_by_name(""energy_true"")]\n    )\n\n    edisp_map = make_edisp_map(edisp, pointing, new_geom, exposure_map)\n\n    return edisp_map.to_edisp_kernel_map(geom.get_axis_by_name(""energy""))\n'"
gammapy/maps/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Sky maps.""""""\nfrom .base import *\nfrom .geom import *\nfrom .hpx import *\nfrom .hpxmap import *\nfrom .hpxnd import *\nfrom .region import *\nfrom .regionnd import *\nfrom .wcs import *\nfrom .wcsmap import *\nfrom .wcsnd import *\n'"
gammapy/maps/base.py,26,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport abc\nimport copy\nimport inspect\nimport json\nimport numpy as np\nfrom astropy import units as u\nfrom astropy.io import fits\nfrom gammapy.utils.scripts import make_path\nfrom .geom import MapCoord, pix_tuple_to_idx\nfrom .utils import INVALID_VALUE\n\n__all__ = [""Map""]\n\n\nclass Map(abc.ABC):\n    """"""Abstract map class.\n\n    This can represent WCS- or HEALPIX-based maps\n    with 2 spatial dimensions and N non-spatial dimensions.\n\n    Parameters\n    ----------\n    geom : `~gammapy.maps.Geom`\n        Geometry\n    data : `~numpy.ndarray`\n        Data array\n    meta : `dict`\n        Dictionary to store meta data\n    unit : str or `~astropy.units.Unit`\n        Data unit\n    """"""\n\n    def __init__(self, geom, data, meta=None, unit=""""):\n        self._geom = geom\n        self.data = data\n        self.unit = unit\n\n        if meta is None:\n            self.meta = {}\n        else:\n            self.meta = meta\n\n    def _init_copy(self, **kwargs):\n        """"""Init map instance by copying missing init arguments from self.\n        """"""\n        argnames = inspect.getfullargspec(self.__init__).args\n        argnames.remove(""self"")\n        argnames.remove(""dtype"")\n\n        for arg in argnames:\n            value = getattr(self, ""_"" + arg)\n            kwargs.setdefault(arg, copy.deepcopy(value))\n\n        return self.from_geom(**kwargs)\n\n    @property\n    def geom(self):\n        """"""Map geometry (`~gammapy.maps.Geom`)""""""\n        return self._geom\n\n    @property\n    def data(self):\n        """"""Data array (`~numpy.ndarray`)""""""\n        return self._data\n\n    @data.setter\n    def data(self, value):\n        if np.isscalar(value):\n            value = value * np.ones(self.geom.data_shape)\n\n        if isinstance(value, u.Quantity):\n            raise TypeError(""Map data must be a Numpy array. Set unit separately"")\n\n        if not value.shape == self.geom.data_shape:\n            value = value.reshape(self.geom.data_shape)\n\n        self._data = value\n\n    @property\n    def unit(self):\n        """"""Map unit (`~astropy.units.Unit`)""""""\n        return self._unit\n\n    @unit.setter\n    def unit(self, val):\n        self._unit = u.Unit(val)\n\n    @property\n    def meta(self):\n        """"""Map meta (`dict`)""""""\n        return self._meta\n\n    @meta.setter\n    def meta(self, val):\n        self._meta = val\n\n    @property\n    def quantity(self):\n        """"""Map data times unit (`~astropy.units.Quantity`)""""""\n        return u.Quantity(self.data, self.unit, copy=False)\n\n    @quantity.setter\n    def quantity(self, val):\n        val = u.Quantity(val, copy=False)\n        self.data = val.value\n        self.unit = val.unit\n\n    @staticmethod\n    def create(**kwargs):\n        """"""Create an empty map object.\n\n        This method accepts generic options listed below, as well as options\n        for `HpxMap` and `WcsMap` objects. For WCS-specific options, see\n        `WcsMap.create` and for HPX-specific options, see `HpxMap.create`.\n\n        Parameters\n        ----------\n        frame : str\n            Coordinate system, either Galactic (""galactic"") or Equatorial\n            (""icrs"").\n        map_type : {\'wcs\', \'wcs-sparse\', \'hpx\', \'hpx-sparse\', \'region\'}\n            Map type.  Selects the class that will be used to\n            instantiate the map.\n        binsz : float or `~numpy.ndarray`\n            Pixel size in degrees.\n        skydir : `~astropy.coordinates.SkyCoord`\n            Coordinate of map center.\n        axes : list\n            List of `~MapAxis` objects for each non-spatial dimension.\n            If None then the map will be a 2D image.\n        dtype : str\n            Data type, default is \'float32\'\n        unit : str or `~astropy.units.Unit`\n            Data unit.\n        meta : `dict`\n            Dictionary to store meta data.\n        region : `~regions.SkyRegion`\n            Sky region used for the region map.\n\n        Returns\n        -------\n        map : `Map`\n            Empty map object.\n        """"""\n        from .hpxmap import HpxMap\n        from .wcsmap import WcsMap\n        from .regionnd import RegionNDMap\n\n        map_type = kwargs.setdefault(""map_type"", ""wcs"")\n        if ""wcs"" in map_type.lower():\n            return WcsMap.create(**kwargs)\n        elif ""hpx"" in map_type.lower():\n            return HpxMap.create(**kwargs)\n        elif map_type == ""region"":\n            _ = kwargs.pop(""map_type"")\n            return RegionNDMap.create(**kwargs)\n        else:\n            raise ValueError(f""Unrecognized map type: {map_type!r}"")\n\n    @staticmethod\n    def read(filename, hdu=None, hdu_bands=None, map_type=""auto""):\n        """"""Read a map from a FITS file.\n\n        Parameters\n        ----------\n        filename : str or `~pathlib.Path`\n            Name of the FITS file.\n        hdu : str\n            Name or index of the HDU with the map data.\n        hdu_bands : str\n            Name or index of the HDU with the BANDS table.  If not\n            defined this will be inferred from the FITS header of the\n            map HDU.\n        map_type : {\'wcs\', \'wcs-sparse\', \'hpx\', \'hpx-sparse\', \'auto\'}\n            Map type.  Selects the class that will be used to\n            instantiate the map.  The map type should be consistent\n            with the format of the input file.  If map_type is \'auto\'\n            then an appropriate map type will be inferred from the\n            input file.\n\n        Returns\n        -------\n        map_out : `Map`\n            Map object\n        """"""\n        with fits.open(make_path(filename), memmap=False) as hdulist:\n            return Map.from_hdulist(hdulist, hdu, hdu_bands, map_type)\n\n    @staticmethod\n    def from_geom(\n        geom, meta=None, data=None, map_type=""auto"", unit="""", dtype=""float32""\n    ):\n        """"""Generate an empty map from a `Geom` instance.\n\n        Parameters\n        ----------\n        geom : `Geom`\n            Map geometry.\n        data : `numpy.ndarray`\n            data array\n        meta : `dict`\n            Dictionary to store meta data.\n        map_type : {\'wcs\', \'wcs-sparse\', \'hpx\', \'hpx-sparse\', \'auto\'}\n            Map type.  Selects the class that will be used to\n            instantiate the map. The map type should be consistent\n            with the geometry. If map_type is \'auto\' then an\n            appropriate map type will be inferred from type of ``geom``.\n        unit : str or `~astropy.units.Unit`\n            Data unit.\n\n        Returns\n        -------\n        map_out : `Map`\n            Map object\n\n        """"""\n        if map_type == ""auto"":\n\n            from .hpx import HpxGeom\n            from .wcs import WcsGeom\n            from .region import RegionGeom\n\n            if isinstance(geom, HpxGeom):\n                map_type = ""hpx""\n            elif isinstance(geom, WcsGeom):\n                map_type = ""wcs""\n            elif isinstance(geom, RegionGeom):\n                map_type = ""region""\n            else:\n                raise ValueError(""Unrecognized geom type."")\n\n        cls_out = Map._get_map_cls(map_type)\n        return cls_out(geom, data=data, meta=meta, unit=unit, dtype=dtype)\n\n    @staticmethod\n    def from_hdulist(hdulist, hdu=None, hdu_bands=None, map_type=""auto""):\n        """"""Create from `astropy.io.fits.HDUList`.""""""\n        if map_type == ""auto"":\n            map_type = Map._get_map_type(hdulist, hdu)\n        cls_out = Map._get_map_cls(map_type)\n        return cls_out.from_hdulist(hdulist, hdu=hdu, hdu_bands=hdu_bands)\n\n    @staticmethod\n    def _get_meta_from_header(header):\n        """"""Load meta data from a FITS header.""""""\n        if ""META"" in header:\n            return json.loads(header[""META""])\n        else:\n            return {}\n\n    @staticmethod\n    def _get_map_type(hdu_list, hdu_name):\n        """"""Infer map type from a FITS HDU.\n\n        Only read header, never data, to have good performance.\n        """"""\n        if hdu_name is None:\n            # Find the header of the first non-empty HDU\n            header = hdu_list[0].header\n            if header[""NAXIS""] == 0:\n                header = hdu_list[1].header\n        else:\n            header = hdu_list[hdu_name].header\n\n        if (""PIXTYPE"" in header) and (header[""PIXTYPE""] == ""HEALPIX""):\n            return ""hpx""\n        else:\n            return ""wcs""\n\n    @staticmethod\n    def _get_map_cls(map_type):\n        """"""Get map class for given `map_type` string.\n\n        This should probably be a registry dict so that users\n        can add supported map types to the `gammapy.maps` I/O\n        (see e.g. the Astropy table format I/O registry),\n        but that\'s non-trivial to implement without avoiding circular imports.\n        """"""\n        if map_type == ""wcs"":\n            from .wcsnd import WcsNDMap\n\n            return WcsNDMap\n        elif map_type == ""wcs-sparse"":\n            raise NotImplementedError()\n        elif map_type == ""hpx"":\n            from .hpxnd import HpxNDMap\n\n            return HpxNDMap\n        elif map_type == ""hpx-sparse"":\n            raise NotImplementedError()\n        elif map_type == ""region"":\n            from .regionnd import RegionNDMap\n\n            return RegionNDMap\n        else:\n            raise ValueError(f""Unrecognized map type: {map_type!r}"")\n\n    def write(self, filename, overwrite=False, **kwargs):\n        """"""Write to a FITS file.\n\n        Parameters\n        ----------\n        filename : str\n            Output file name.\n        overwrite : bool\n            Overwrite existing file?\n        hdu : str\n            Set the name of the image extension.  By default this will\n            be set to SKYMAP (for BINTABLE HDU) or PRIMARY (for IMAGE\n            HDU).\n        hdu_bands : str\n            Set the name of the bands table extension.  By default this will\n            be set to BANDS.\n        conv : str\n            FITS format convention.  By default files will be written\n            to the gamma-astro-data-formats (GADF) format.  This\n            option can be used to write files that are compliant with\n            format conventions required by specific software (e.g. the\n            Fermi Science Tools).  Supported conventions are \'gadf\',\n            \'fgst-ccube\', \'fgst-ltcube\', \'fgst-bexpcube\',\n            \'fgst-template\', \'fgst-srcmap\', \'fgst-srcmap-sparse\',\n            \'galprop\', and \'galprop2\'.\n        sparse : bool\n            Sparsify the map by dropping pixels with zero amplitude.\n            This option is only compatible with the \'gadf\' format.\n        """"""\n        hdulist = self.to_hdulist(**kwargs)\n        hdulist.writeto(filename, overwrite=overwrite)\n\n    def iter_by_image(self):\n        """"""Iterate over image planes of the map.\n\n        This is a generator yielding ``(data, idx)`` tuples,\n        where ``data`` is a `numpy.ndarray` view of the image plane data,\n        and ``idx`` is a tuple of int, the index of the image plane.\n\n        The image plane index is in data order, so that the data array can be\n        indexed directly. See :ref:`mapiter` for further information.\n        """"""\n        for idx in np.ndindex(self.geom.shape_axes):\n            yield self.data[idx[::-1]], idx[::-1]\n\n    @abc.abstractmethod\n    def sum_over_axes(self, axes=None, keepdims=False):\n        """"""Reduce to a 2D image by summing over non-spatial dimensions.""""""\n        pass\n\n    def coadd(self, map_in, weights=None):\n        """"""Add the contents of ``map_in`` to this map.\n\n        This method can be used to combine maps containing integral quantities (e.g. counts)\n        or differential quantities if the maps have the same binning.\n\n        Parameters\n        ----------\n        map_in : `Map`\n            Input map.\n        weights: `Map` or `~numpy.ndarray`\n            The weight factors while adding\n        """"""\n        if not self.unit.is_equivalent(map_in.unit):\n            raise ValueError(""Incompatible units"")\n\n        # TODO: Check whether geometries are aligned and if so sum the\n        # data vectors directly\n        if weights is not None:\n            map_in = map_in * weights\n        idx = map_in.geom.get_idx()\n        coords = map_in.geom.get_coord()\n        vals = u.Quantity(map_in.get_by_idx(idx), map_in.unit)\n        self.fill_by_coord(coords, vals)\n\n    @abc.abstractmethod\n    def pad(self, pad_width, mode=""constant"", cval=0, order=1):\n        """"""Pad the spatial dimensions of the map.\n\n        Parameters\n        ----------\n        pad_width : {sequence, array_like, int}\n            Number of pixels padded to the edges of each axis.\n        mode : {\'edge\', \'constant\', \'interp\'}\n            Padding mode.  \'edge\' pads with the closest edge value.\n            \'constant\' pads with a constant value. \'interp\' pads with\n            an extrapolated value.\n        cval : float\n            Padding value when mode=\'consant\'.\n        order : int\n            Order of interpolation when mode=\'constant\' (0 =\n            nearest-neighbor, 1 = linear, 2 = quadratic, 3 = cubic).\n\n        Returns\n        -------\n        map : `Map`\n            Padded map.\n\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def crop(self, crop_width):\n        """"""Crop the spatial dimensions of the map.\n\n        Parameters\n        ----------\n        crop_width : {sequence, array_like, int}\n            Number of pixels cropped from the edges of each axis.\n            Defined analogously to ``pad_with`` from `numpy.pad`.\n\n        Returns\n        -------\n        map : `Map`\n            Cropped map.\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def downsample(self, factor, preserve_counts=True, axis=None):\n        """"""Downsample the spatial dimension by a given factor.\n\n        Parameters\n        ----------\n        factor : int\n            Downsampling factor.\n        preserve_counts : bool\n            Preserve the integral over each bin.  This should be true\n            if the map is an integral quantity (e.g. counts) and false if\n            the map is a differential quantity (e.g. intensity).\n        axis : str\n            Which axis to downsample. By default spatial axes are downsampled.\n\n        Returns\n        -------\n        map : `Map`\n            Downsampled map.\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def upsample(self, factor, order=0, preserve_counts=True, axis=None):\n        """"""Upsample the spatial dimension by a given factor.\n\n        Parameters\n        ----------\n        factor : int\n            Upsampling factor.\n        order : int\n            Order of the interpolation used for upsampling.\n        preserve_counts : bool\n            Preserve the integral over each bin.  This should be true\n            if the map is an integral quantity (e.g. counts) and false if\n            the map is a differential quantity (e.g. intensity).\n        axis : str\n            Which axis to upsample. By default spatial axes are upsampled.\n\n\n        Returns\n        -------\n        map : `Map`\n            Upsampled map.\n        """"""\n        pass\n\n    def slice_by_idx(self, slices):\n        """"""Slice sub map from map object.\n\n        For usage examples, see :ref:`mapslicing`.\n\n        Parameters\n        ----------\n        slices : dict\n            Dict of axes names and integers or `slice` object pairs. Contains one\n            element for each non-spatial dimension. For integer indexing the\n            corresponding axes is dropped from the map. Axes not specified in the\n            dict are kept unchanged.\n\n        Returns\n        -------\n        map_out : `Map`\n            Sliced map object.\n        """"""\n        geom = self.geom.slice_by_idx(slices)\n        slices = tuple([slices.get(ax.name, slice(None)) for ax in self.geom.axes])\n        data = self.data[slices[::-1]]\n        return self.__class__(geom=geom, data=data, unit=self.unit, meta=self.meta)\n\n    def get_image_by_coord(self, coords):\n        """"""Return spatial map at the given axis coordinates.\n\n        Parameters\n        ----------\n        coords : tuple or dict\n            Tuple should be ordered as (x_0, ..., x_n) where x_i are coordinates\n            for non-spatial dimensions of the map. Dict should specify the axis\n            names of the non-spatial axes such as {\'axes0\': x_0, ..., \'axesn\': x_n}.\n\n        Returns\n        -------\n        map_out : `Map`\n            Map with spatial dimensions only.\n\n        See Also\n        --------\n        get_image_by_idx, get_image_by_pix\n\n        Examples\n        --------\n        ::\n\n            import numpy as np\n            from gammapy.maps import Map, MapAxis\n            from astropy.coordinates import SkyCoord\n            from astropy import units as u\n\n            # Define map axes\n            energy_axis = MapAxis.from_edges(\n                np.logspace(-1., 1., 4), unit=\'TeV\', name=\'energy\',\n            )\n\n            time_axis = MapAxis.from_edges(\n                np.linspace(0., 10, 20), unit=\'h\', name=\'time\',\n            )\n\n            # Define map center\n            skydir = SkyCoord(0, 0, frame=\'galactic\', unit=\'deg\')\n\n            # Create map\n            m_wcs = Map.create(\n                map_type=\'wcs\',\n                binsz=0.02,\n                skydir=skydir,\n                width=10.0,\n                axes=[energy_axis, time_axis],\n            )\n\n            # Get image by coord tuple\n            image = m_wcs.get_image_by_coord((\'500 GeV\', \'1 h\'))\n\n            # Get image by coord dict with strings\n            image = m_wcs.get_image_by_coord({\'energy\': \'500 GeV\', \'time\': \'1 h\'})\n\n            # Get image by coord dict with quantities\n            image = m_wcs.get_image_by_coord({\'energy\': 0.5 * u.TeV, \'time\': 1 * u.h})\n        """"""\n        if isinstance(coords, tuple):\n            axes_names = [_.name for _ in self.geom.axes]\n            coords = dict(zip(axes_names, coords))\n\n        idx = []\n        for ax in self.geom.axes:\n            value = coords[ax.name]\n            idx.append(ax.coord_to_idx(value))\n\n        return self.get_image_by_idx(idx)\n\n    def get_image_by_pix(self, pix):\n        """"""Return spatial map at the given axis pixel coordinates\n\n        Parameters\n        ----------\n        pix : tuple\n            Tuple of scalar pixel coordinates for each non-spatial dimension of\n            the map. Tuple should be ordered as (I_0, ..., I_n). Pixel coordinates\n            can be either float or integer type.\n\n        See Also\n        --------\n        get_image_by_coord, get_image_by_idx\n\n        Returns\n        -------\n        map_out : `Map`\n            Map with spatial dimensions only.\n        """"""\n        idx = self.geom.pix_to_idx(pix)\n        return self.get_image_by_idx(idx)\n\n    def get_image_by_idx(self, idx):\n        """"""Return spatial map at the given axis pixel indices.\n\n        Parameters\n        ----------\n        idx : tuple\n            Tuple of scalar indices for each non spatial dimension of the map.\n            Tuple should be ordered as (I_0, ..., I_n).\n\n        See Also\n        --------\n        get_image_by_coord, get_image_by_pix\n\n        Returns\n        -------\n        map_out : `Map`\n            Map with spatial dimensions only.\n        """"""\n        if len(idx) != len(self.geom.axes):\n            raise ValueError(""Tuple length must equal number of non-spatial dimensions"")\n\n        # Only support scalar indices per axis\n        idx = tuple([int(_) for _ in idx])\n\n        geom = self.geom.to_image()\n        data = self.data[idx[::-1]]\n        return self.__class__(geom=geom, data=data, unit=self.unit, meta=self.meta)\n\n    def get_by_coord(self, coords):\n        """"""Return map values at the given map coordinates.\n\n        Parameters\n        ----------\n        coords : tuple or `~gammapy.maps.MapCoord`\n            Coordinate arrays for each dimension of the map.  Tuple\n            should be ordered as (lon, lat, x_0, ..., x_n) where x_i\n            are coordinates for non-spatial dimensions of the map.\n\n        Returns\n        -------\n        vals : `~numpy.ndarray`\n           Values of pixels in the map.  np.nan used to flag coords\n           outside of map.\n        """"""\n        coords = MapCoord.create(coords, frame=self.geom.frame)\n        pix = self.geom.coord_to_pix(coords)\n        vals = self.get_by_pix(pix)\n        return vals\n\n    def get_by_pix(self, pix):\n        """"""Return map values at the given pixel coordinates.\n\n        Parameters\n        ----------\n        pix : tuple\n            Tuple of pixel index arrays for each dimension of the map.\n            Tuple should be ordered as (I_lon, I_lat, I_0, ..., I_n)\n            for WCS maps and (I_hpx, I_0, ..., I_n) for HEALPix maps.\n            Pixel indices can be either float or integer type.\n\n        Returns\n        -------\n        vals : `~numpy.ndarray`\n           Array of pixel values.  np.nan used to flag coordinates\n           outside of map\n        """"""\n        # FIXME: Support local indexing here?\n        # FIXME: Support slicing?\n        pix = np.broadcast_arrays(*pix)\n        idx = self.geom.pix_to_idx(pix)\n        vals = self.get_by_idx(idx)\n        mask = self.geom.contains_pix(pix)\n\n        if not mask.all():\n            invalid = INVALID_VALUE[self.data.dtype]\n            vals = vals.astype(type(invalid))\n            vals[~mask] = invalid\n\n        return vals\n\n    @abc.abstractmethod\n    def get_by_idx(self, idx):\n        """"""Return map values at the given pixel indices.\n\n        Parameters\n        ----------\n        idx : tuple\n            Tuple of pixel index arrays for each dimension of the map.\n            Tuple should be ordered as (I_lon, I_lat, I_0, ..., I_n)\n            for WCS maps and (I_hpx, I_0, ..., I_n) for HEALPix maps.\n\n        Returns\n        -------\n        vals : `~numpy.ndarray`\n           Array of pixel values.\n           np.nan used to flag coordinate outside of map\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def interp_by_coord(self, coords, interp=None, fill_value=None):\n        """"""Interpolate map values at the given map coordinates.\n\n        Parameters\n        ----------\n        coords : tuple or `~gammapy.maps.MapCoord`\n            Coordinate arrays for each dimension of the map.  Tuple\n            should be ordered as (lon, lat, x_0, ..., x_n) where x_i\n            are coordinates for non-spatial dimensions of the map.\n\n        interp : {None, \'nearest\', \'linear\', \'cubic\', 0, 1, 2, 3}\n            Method to interpolate data values.  By default no\n            interpolation is performed and the return value will be\n            the amplitude of the pixel encompassing the given\n            coordinate.  Integer values can be used in lieu of strings\n            to choose the interpolation method of the given order\n            (0=\'nearest\', 1=\'linear\', 2=\'quadratic\', 3=\'cubic\').  Note\n            that only \'nearest\' and \'linear\' methods are supported for\n            all map types.\n        fill_value : None or float value\n            The value to use for points outside of the interpolation domain.\n            If None, values outside the domain are extrapolated.\n\n        Returns\n        -------\n        vals : `~numpy.ndarray`\n            Interpolated pixel values.\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def interp_by_pix(self, pix, interp=None, fill_value=None):\n        """"""Interpolate map values at the given pixel coordinates.\n\n        Parameters\n        ----------\n        pix : tuple\n            Tuple of pixel coordinate arrays for each dimension of the\n            map.  Tuple should be ordered as (p_lon, p_lat, p_0, ...,\n            p_n) where p_i are pixel coordinates for non-spatial\n            dimensions of the map.\n\n        interp : {None, \'nearest\', \'linear\', \'cubic\', 0, 1, 2, 3}\n            Method to interpolate data values.  By default no\n            interpolation is performed and the return value will be\n            the amplitude of the pixel encompassing the given\n            coordinate.  Integer values can be used in lieu of strings\n            to choose the interpolation method of the given order\n            (0=\'nearest\', 1=\'linear\', 2=\'quadratic\', 3=\'cubic\').  Note\n            that only \'nearest\' and \'linear\' methods are supported for\n            all map types.\n        fill_value : None or float value\n            The value to use for points outside of the interpolation domain.\n            If None, values outside the domain are extrapolated.\n\n        Returns\n        -------\n        vals : `~numpy.ndarray`\n            Interpolated pixel values.\n        """"""\n        pass\n\n    def fill_events(self, events):\n        """"""Fill event coordinates (`~gammapy.data.EventList`).""""""\n        self.fill_by_coord(events.map_coord(self.geom))\n\n    def fill_by_coord(self, coords, weights=None):\n        """"""Fill pixels at ``coords`` with given ``weights``.\n\n        Parameters\n        ----------\n        coords : tuple or `~gammapy.maps.MapCoord`\n            Coordinate arrays for each dimension of the map.  Tuple\n            should be ordered as (lon, lat, x_0, ..., x_n) where x_i\n            are coordinates for non-spatial dimensions of the map.\n        weights : `~numpy.ndarray`\n            Weights vector. Default is weight of one.\n        """"""\n        idx = self.geom.coord_to_idx(coords)\n        self.fill_by_idx(idx, weights)\n\n    def fill_by_pix(self, pix, weights=None):\n        """"""Fill pixels at ``pix`` with given ``weights``.\n\n        Parameters\n        ----------\n        pix : tuple\n            Tuple of pixel index arrays for each dimension of the map.\n            Tuple should be ordered as (I_lon, I_lat, I_0, ..., I_n)\n            for WCS maps and (I_hpx, I_0, ..., I_n) for HEALPix maps.\n            Pixel indices can be either float or integer type.  Float\n            indices will be rounded to the nearest integer.\n        weights : `~numpy.ndarray`\n            Weights vector. Default is weight of one.\n        """"""\n        idx = pix_tuple_to_idx(pix)\n        return self.fill_by_idx(idx, weights=weights)\n\n    @abc.abstractmethod\n    def fill_by_idx(self, idx, weights=None):\n        """"""Fill pixels at ``idx`` with given ``weights``.\n\n        Parameters\n        ----------\n        idx : tuple\n            Tuple of pixel index arrays for each dimension of the map.\n            Tuple should be ordered as (I_lon, I_lat, I_0, ..., I_n)\n            for WCS maps and (I_hpx, I_0, ..., I_n) for HEALPix maps.\n        weights : `~numpy.ndarray`\n            Weights vector. Default is weight of one.\n        """"""\n        pass\n\n    def set_by_coord(self, coords, vals):\n        """"""Set pixels at ``coords`` with given ``vals``.\n\n        Parameters\n        ----------\n        coords : tuple or `~gammapy.maps.MapCoord`\n            Coordinate arrays for each dimension of the map.  Tuple\n            should be ordered as (lon, lat, x_0, ..., x_n) where x_i\n            are coordinates for non-spatial dimensions of the map.\n        vals : `~numpy.ndarray`\n            Values vector.\n        """"""\n        idx = self.geom.coord_to_pix(coords)\n        self.set_by_pix(idx, vals)\n\n    def set_by_pix(self, pix, vals):\n        """"""Set pixels at ``pix`` with given ``vals``.\n\n        Parameters\n        ----------\n        pix : tuple\n            Tuple of pixel index arrays for each dimension of the map.\n            Tuple should be ordered as (I_lon, I_lat, I_0, ..., I_n)\n            for WCS maps and (I_hpx, I_0, ..., I_n) for HEALPix maps.\n            Pixel indices can be either float or integer type.  Float\n            indices will be rounded to the nearest integer.\n        vals : `~numpy.ndarray`\n            Values vector.\n        """"""\n        idx = pix_tuple_to_idx(pix)\n        return self.set_by_idx(idx, vals)\n\n    @abc.abstractmethod\n    def set_by_idx(self, idx, vals):\n        """"""Set pixels at ``idx`` with given ``vals``.\n\n        Parameters\n        ----------\n        idx : tuple\n            Tuple of pixel index arrays for each dimension of the map.\n            Tuple should be ordered as (I_lon, I_lat, I_0, ..., I_n)\n            for WCS maps and (I_hpx, I_0, ..., I_n) for HEALPix maps.\n        vals : `~numpy.ndarray`\n            Values vector.\n        """"""\n        pass\n\n    def plot_interactive(self, rc_params=None, **kwargs):\n        """"""\n        Plot map with interactive widgets to explore the non spatial axes.\n\n        Parameters\n        ----------\n        rc_params : dict\n            Passed to ``matplotlib.rc_context(rc=rc_params)`` to style the plot.\n        **kwargs : dict\n            Keyword arguments passed to `WcsNDMap.plot`.\n\n        Examples\n        --------\n        You can try this out e.g. using a Fermi-LAT diffuse model cube with an energy axis::\n\n            from gammapy.maps import Map\n\n            m = Map.read(""$GAMMAPY_DATA/fermi_3fhl/gll_iem_v06_cutout.fits"")\n            m.plot_interactive(add_cbar=True, stretch=""sqrt"")\n\n        If you would like to adjust the figure size you can use the ``rc_params`` argument::\n\n            rc_params = {\'figure.figsize\': (12, 6), \'font.size\': 12}\n            m.plot_interactive(rc_params=rc_params)\n        """"""\n        import matplotlib as mpl\n        import matplotlib.pyplot as plt\n        from ipywidgets.widgets.interaction import interact, fixed\n        from ipywidgets import SelectionSlider, RadioButtons\n\n        if self.geom.is_image:\n            raise TypeError(""Use .plot() for 2D Maps"")\n\n        kwargs.setdefault(""interpolation"", ""nearest"")\n        kwargs.setdefault(""origin"", ""lower"")\n        kwargs.setdefault(""cmap"", ""afmhot"")\n\n        rc_params = rc_params or {}\n        stretch = kwargs.pop(""stretch"", ""sqrt"")\n\n        interact_kwargs = {}\n\n        for axis in self.geom.axes:\n            if axis.node_type == ""edges"":\n                options = [\n                    f""{val_min:.2e} - {val_max:.2e} {axis.unit}""\n                    for val_min, val_max in zip(axis.edges[:-1], axis.edges[1:])\n                ]\n            else:\n                options = [f""{val:.2e} {axis.unit}"" for val in axis.center]\n\n            interact_kwargs[axis.name] = SelectionSlider(\n                options=options,\n                description=f""Select {axis.name}:"",\n                continuous_update=False,\n                style={""description_width"": ""initial""},\n                layout={""width"": ""50%""},\n            )\n            interact_kwargs[axis.name + ""_options""] = fixed(options)\n\n        interact_kwargs[""stretch""] = RadioButtons(\n            options=[""linear"", ""sqrt"", ""log""],\n            value=stretch,\n            description=""Select stretch:"",\n            style={""description_width"": ""initial""},\n        )\n\n        @interact(**interact_kwargs)\n        def _plot_interactive(**ikwargs):\n            idx = [\n                ikwargs[ax.name + ""_options""].index(ikwargs[ax.name])\n                for ax in self.geom.axes\n            ]\n            img = self.get_image_by_idx(idx)\n            stretch = ikwargs[""stretch""]\n            with mpl.rc_context(rc=rc_params):\n                fig, ax, cbar = img.plot(stretch=stretch, **kwargs)\n                plt.show()\n\n    def copy(self, **kwargs):\n        """"""Copy map instance and overwrite given attributes, except for geometry.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Keyword arguments to overwrite in the map constructor.\n\n        Returns\n        -------\n        copy : `Map`\n            Copied Map.\n        """"""\n        if ""geom"" in kwargs:\n            raise ValueError(""Can\'t copy and change geometry of the map."")\n        return self._init_copy(**kwargs)\n\n    def apply_edisp(self, edisp):\n        """"""Apply energy dispersion to map. Requires energy axis.\n\n        Parameters\n        ----------\n        edisp : `gammapy.irf.EDispKernel`\n            Energy dispersion matrix\n\n        Returns\n        -------\n        map : `WcsNDMap`\n            Map with energy dispersion applied.\n        """"""\n        # TODO: either use sparse matrix mutiplication or something like edisp.is_diagonal\n        if edisp is not None:\n            loc = self.geom.get_axis_index_by_name(""energy_true"")\n            data = np.rollaxis(self.data, loc, len(self.data.shape))\n            data = np.dot(data, edisp.pdf_matrix)\n            data = np.rollaxis(data, -1, loc)\n            e_reco_axis = edisp.e_reco.copy(name=""energy"")\n        else:\n            data = self.data\n            e_reco_axis = self.geom.get_axis_by_name(""energy_true"").copy(name=""energy"")\n\n        geom_reco = self.geom.to_image().to_cube(axes=[e_reco_axis])\n        return self._init_copy(geom=geom_reco, data=data)\n\n    def __repr__(self):\n        geom = self.geom.__class__.__name__\n        axes = [""skycoord""] if self.geom.is_hpx else [""lon"", ""lat""]\n        axes = axes + [_.name for _ in self.geom.axes]\n\n        return (\n            f""{self.__class__.__name__}\\n\\n""\n            f""\\tgeom  : {geom} \\n ""\n            f""\\taxes  : {axes}\\n""\n            f""\\tshape : {self.geom.data_shape[::-1]}\\n""\n            f""\\tndim  : {self.geom.ndim}\\n""\n            f""\\tunit  : {self.unit}\\n""\n            f""\\tdtype : {self.data.dtype}\\n""\n        )\n\n    def _arithmetics(self, operator, other, copy):\n        """"""Perform arithmetics on maps after checking geometry consistency.""""""\n        if isinstance(other, Map):\n            if self.geom == other.geom:\n                q = other.quantity\n            else:\n                raise ValueError(""Map Arithmetics: Inconsistent geometries."")\n        else:\n            q = u.Quantity(other, copy=False)\n\n        out = self.copy() if copy else self\n        out.quantity = operator(out.quantity, q)\n        return out\n\n    def __add__(self, other):\n        return self._arithmetics(np.add, other, copy=True)\n\n    def __iadd__(self, other):\n        return self._arithmetics(np.add, other, copy=False)\n\n    def __sub__(self, other):\n        return self._arithmetics(np.subtract, other, copy=True)\n\n    def __isub__(self, other):\n        return self._arithmetics(np.subtract, other, copy=False)\n\n    def __mul__(self, other):\n        return self._arithmetics(np.multiply, other, copy=True)\n\n    def __imul__(self, other):\n        return self._arithmetics(np.multiply, other, copy=False)\n\n    def __truediv__(self, other):\n        return self._arithmetics(np.true_divide, other, copy=True)\n\n    def __itruediv__(self, other):\n        return self._arithmetics(np.true_divide, other, copy=False)\n\n    def __le__(self, other):\n        return self._arithmetics(np.less_equal, other, copy=True)\n\n    def __lt__(self, other):\n        return self._arithmetics(np.less, other, copy=True)\n\n    def __ge__(self, other):\n        return self._arithmetics(np.greater_equal, other, copy=True)\n\n    def __gt__(self, other):\n        return self._arithmetics(np.greater, other, copy=True)\n\n    def __eq__(self, other):\n        return self._arithmetics(np.equal, other, copy=True)\n\n    def __ne__(self, other):\n        return self._arithmetics(np.not_equal, other, copy=True)\n\n    def __array__(self):\n        return self.data\n'"
gammapy/maps/geom.py,43,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport abc\nimport copy\nimport inspect\nimport logging\nimport numpy as np\nimport scipy.interpolate\nfrom astropy import units as u\nfrom astropy.coordinates import SkyCoord\nfrom astropy.io import fits\nfrom astropy.table import Column, QTable, Table\nfrom astropy.utils import lazyproperty\nfrom gammapy.utils.interpolation import interpolation_scale\nfrom .utils import INVALID_INDEX, edges_from_lo_hi, find_bands_hdu, find_hdu\n\n__all__ = [""MapCoord"", ""Geom"", ""MapAxis""]\n\nlog = logging.getLogger(__name__)\n\n\ndef make_axes(axes_in):\n    """"""Make a sequence of `~MapAxis` objects.""""""\n    if axes_in is None:\n        return []\n\n    axes_out = []\n    for idx, ax in enumerate(axes_in):\n        if isinstance(ax, np.ndarray):\n            ax = MapAxis(ax)\n\n        if ax.name == """":\n            ax.name = ""axis{}"".format(idx)\n\n        axes_out += [ax]\n\n    return axes_out\n\n\ndef make_axes_cols(axes, axis_names=None):\n    """"""Make FITS table columns for map axes.\n\n    Parameters\n    ----------\n    axes : list\n        Python list of `MapAxis` objects\n\n    Returns\n    -------\n    cols : list\n        Python list of `~astropy.io.fits.Column`\n    """"""\n    size = np.prod([ax.nbin for ax in axes])\n    chan = np.arange(0, size)\n    cols = [fits.Column(""CHANNEL"", ""I"", array=chan)]\n\n    if axis_names is None:\n        axis_names = [ax.name for ax in axes]\n    axis_names = [_.upper() for _ in axis_names]\n\n    axes_ctr = np.meshgrid(*[ax.center for ax in axes])\n    axes_min = np.meshgrid(*[ax.edges[:-1] for ax in axes])\n    axes_max = np.meshgrid(*[ax.edges[1:] for ax in axes])\n\n    for i, (ax, name) in enumerate(zip(axes, axis_names)):\n\n        if name == ""ENERGY"":\n            colnames = [""ENERGY"", ""E_MIN"", ""E_MAX""]\n        else:\n            s = ""AXIS%i"" % i if name == """" else name\n            colnames = [s, s + ""_MIN"", s + ""_MAX""]\n\n        for colname, v in zip(colnames, [axes_ctr, axes_min, axes_max]):\n            array = np.ravel(v[i])\n            unit = ax.unit.to_string(""fits"")\n            cols.append(fits.Column(colname, ""E"", array=array, unit=unit))\n\n    return cols\n\n\ndef energy_axis_from_fgst_ccube(hdu):\n    bands = Table.read(hdu)\n    edges_min = bands[""E_MIN""].quantity\n    edges_max = bands[""E_MAX""].quantity\n    edges = edges_from_lo_hi(edges_min, edges_max)\n    return [MapAxis.from_edges(edges=edges, name=""energy"", interp=""log"")]\n\n\ndef energy_axis_from_fgst_template(hdu):\n    bands = Table.read(hdu)\n\n    allowed_names = [""Energy"", ""ENERGY"", ""energy""]\n    for colname in bands.colnames:\n        if colname in allowed_names:\n            tag = colname\n            break\n\n    nodes = bands[tag].data\n\n    return [\n        MapAxis.from_nodes(nodes=nodes, name=""energy_true"", unit=""MeV"", interp=""log"")\n    ]\n\n\ndef axes_from_bands_hdu(hdu):\n    """"""Read and returns the map axes from a BANDS table.\n\n    Parameters\n    ----------\n    hdu : `~astropy.io.fits.BinTableHDU`\n        The BANDS table HDU.\n\n    Returns\n    -------\n    axes : list of `~MapAxis`\n        List of axis objects.\n    """"""\n    axes = []\n\n    bands = Table.read(hdu)\n\n    for idx in range(5):\n        axcols = bands.meta.get(""AXCOLS{}"".format(idx + 1))\n\n        if axcols is None:\n            break\n\n        colnames = axcols.split("","")\n        node_type = ""edges"" if len(colnames) == 2 else ""center""\n\n        # TODO: check why this extra case is needed\n        if colnames[0] == ""E_MIN"":\n            name = ""energy""\n        else:\n            name = colnames[0].replace(""_MIN"", """").lower()\n\n        interp = bands.meta.get(""INTERP{}"".format(idx + 1), ""lin"")\n\n        if node_type == ""center"":\n            nodes = np.unique(bands[colnames[0]].quantity)\n        else:\n            edges_min = np.unique(bands[colnames[0]].quantity)\n            edges_max = np.unique(bands[colnames[1]].quantity)\n            nodes = edges_from_lo_hi(edges_min, edges_max)\n\n        axis = MapAxis(nodes=nodes, node_type=node_type, interp=interp, name=name)\n        axes.append(axis)\n\n    return axes\n\n\ndef find_and_read_bands(hdu):\n    if hdu is None:\n        return []\n\n    if hdu.name == ""ENERGIES"":\n        axes = energy_axis_from_fgst_template(hdu)\n    elif hdu.name == ""EBOUNDS"":\n        axes = energy_axis_from_fgst_ccube(hdu)\n    else:\n        axes = axes_from_bands_hdu(hdu)\n\n    return axes\n\n\ndef get_shape(param):\n    if param is None:\n        return tuple()\n\n    if not isinstance(param, tuple):\n        param = [param]\n\n    return max([np.array(p, ndmin=1).shape for p in param])\n\n\ndef skycoord_to_lonlat(skycoord, frame=None):\n    """"""Convert SkyCoord to lon, lat, frame.\n\n    Returns\n    -------\n    lon : `~numpy.ndarray`\n        Longitude in degrees.\n    lat : `~numpy.ndarray`\n        Latitude in degrees.\n    """"""\n    if frame:\n        skycoord = skycoord.transform_to(frame)\n\n    return skycoord.data.lon.deg, skycoord.data.lat.deg, skycoord.frame.name\n\n\ndef pix_tuple_to_idx(pix):\n    """"""Convert a tuple of pixel coordinate arrays to a tuple of pixel indices.\n\n    Pixel coordinates are rounded to the closest integer value.\n\n    Parameters\n    ----------\n    pix : tuple\n        Tuple of pixel coordinates with one element for each dimension\n\n    Returns\n    -------\n    idx : `~numpy.ndarray`\n        Array of pixel indices\n    """"""\n    idx = []\n    for p in pix:\n        p = np.array(p, ndmin=1)\n        if np.issubdtype(p.dtype, np.integer):\n            idx += [p]\n        else:\n            p_idx = np.rint(p).astype(int)\n            p_idx[~np.isfinite(p)] = INVALID_INDEX.int\n            idx += [p_idx]\n\n    return tuple(idx)\n\n\ndef coord_to_idx(edges, x, clip=False):\n    """"""Convert axis coordinates ``x`` to bin indices.\n\n    Returns -1 for values below/above the lower/upper edge.\n    """"""\n    x = np.array(x, ndmin=1)\n    ibin = np.digitize(x, edges) - 1\n\n    if clip:\n        ibin[x < edges[0]] = 0\n        ibin[x > edges[-1]] = len(edges) - 1\n    else:\n        with np.errstate(invalid=""ignore""):\n            ibin[x > edges[-1]] = INVALID_INDEX.int\n\n    ibin[~np.isfinite(x)] = INVALID_INDEX.int\n    return ibin\n\n\ndef coord_to_pix(edges, coord, interp=""lin""):\n    """"""Convert axis to pixel coordinates for given interpolation scheme.""""""\n    scale = interpolation_scale(interp)\n\n    interp_fn = scipy.interpolate.interp1d(\n        scale(edges), np.arange(len(edges), dtype=float), fill_value=""extrapolate""\n    )\n\n    return interp_fn(scale(coord))\n\n\ndef pix_to_coord(edges, pix, interp=""lin""):\n    """"""Convert pixel to grid coordinates for given interpolation scheme.""""""\n    scale = interpolation_scale(interp)\n\n    interp_fn = scipy.interpolate.interp1d(\n        np.arange(len(edges), dtype=float), scale(edges), fill_value=""extrapolate""\n    )\n\n    return scale.inverse(interp_fn(pix))\n\n\nclass MapAxis:\n    """"""Class representing an axis of a map.\n\n    Provides methods for\n    transforming to/from axis and pixel coordinates.  An axis is\n    defined by a sequence of node values that lie at the center of\n    each bin.  The pixel coordinate at each node is equal to its index\n    in the node array (0, 1, ..).  Bin edges are offset by 0.5 in\n    pixel coordinates from the nodes such that the lower/upper edge of\n    the first bin is (-0.5,0.5).\n\n    Parameters\n    ----------\n    nodes : `~numpy.ndarray` or `~astropy.units.Quantity`\n        Array of node values.  These will be interpreted as either bin\n        edges or centers according to ``node_type``.\n    interp : str\n        Interpolation method used to transform between axis and pixel\n        coordinates.  Valid options are \'log\', \'lin\', and \'sqrt\'.\n    name : str\n        Axis name\n    node_type : str\n        Flag indicating whether coordinate nodes correspond to pixel\n        edges (node_type = \'edge\') or pixel centers (node_type =\n        \'center\').  \'center\' should be used where the map values are\n        defined at a specific coordinate (e.g. differential\n        quantities). \'edge\' should be used where map values are\n        defined by an integral over coordinate intervals (e.g. a\n        counts histogram).\n    unit : str\n        String specifying the data units.\n    """"""\n\n    # TODO: Add methods to faciliate FITS I/O.\n    # TODO: Cache an interpolation object?\n    def __init__(self, nodes, interp=""lin"", name="""", node_type=""edges"", unit=""""):\n\n        self.name = name\n\n        if len(nodes) != len(np.unique(nodes)):\n            raise ValueError(""MapAxis: node values must be unique"")\n        if ~(np.all(nodes == np.sort(nodes)) or np.all(nodes[::-1] == np.sort(nodes))):\n            raise ValueError(""MapAxis: node values must be sorted"")\n\n        if len(nodes) == 1 and node_type == ""center"":\n            raise ValueError(""Single bins can only be used with node-type \'edges\'"")\n\n        if isinstance(nodes, u.Quantity):\n            unit = nodes.unit if nodes.unit is not None else """"\n            nodes = nodes.value\n        else:\n            nodes = np.array(nodes)\n\n        self._unit = u.Unit(unit)\n        self._nodes = nodes\n        self._node_type = node_type\n        self._interp = interp\n\n        if (self._nodes < 0).any() and interp != ""lin"":\n            raise ValueError(\n                f""Interpolation scaling {interp!r} only support for positive node values.""\n            )\n\n        # Set pixel coordinate of first node\n        if node_type == ""edges"":\n            self._pix_offset = -0.5\n            nbin = len(nodes) - 1\n        elif node_type == ""center"":\n            self._pix_offset = 0.0\n            nbin = len(nodes)\n        else:\n            raise ValueError(f""Invalid node type: {node_type!r}"")\n\n        self._nbin = nbin\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n\n        # TODO: implement an allclose method for MapAxis and call it here\n        if self.edges.shape != other.edges.shape:\n            return False\n        if self.unit.is_equivalent(other.unit) is False:\n            return False\n        return (\n            np.allclose(\n                self.edges.to(other.unit).value, other.edges.value, atol=1e-6, rtol=1e-6\n            )\n            and self._node_type == other._node_type\n            and self._interp == other._interp\n            and self.name.upper() == other.name.upper()\n        )\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        return id(self)\n\n    @property\n    def interp(self):\n        """"""Interpolation scale of the axis.""""""\n        return self._interp\n\n    @property\n    def name(self):\n        """"""Name of the axis.""""""\n        return self._name\n\n    @name.setter\n    def name(self, val):\n        self._name = val\n\n    @lazyproperty\n    def edges(self):\n        """"""Return array of bin edges.""""""\n        pix = np.arange(self.nbin + 1, dtype=float) - 0.5\n        return u.Quantity(self.pix_to_coord(pix), self._unit, copy=False)\n\n    @lazyproperty\n    def center(self):\n        """"""Return array of bin centers.""""""\n        pix = np.arange(self.nbin, dtype=float)\n        return u.Quantity(self.pix_to_coord(pix), self._unit, copy=False)\n\n    @lazyproperty\n    def bin_width(self):\n        """"""Array of bin widths.""""""\n        return np.diff(self.edges)\n\n    @property\n    def nbin(self):\n        """"""Return number of bins.""""""\n        return self._nbin\n\n    @property\n    def node_type(self):\n        """"""Return node type (\'center\' or \'edge\').""""""\n        return self._node_type\n\n    @property\n    def unit(self):\n        """"""Return coordinate axis unit.""""""\n        return self._unit\n\n    @classmethod\n    def from_bounds(cls, lo_bnd, hi_bnd, nbin, **kwargs):\n        """"""Generate an axis object from a lower/upper bound and number of bins.\n\n        If node_type = \'edge\' then bounds correspond to the\n        lower and upper bound of the first and last bin.  If node_type\n        = \'center\' then bounds correspond to the centers of the first\n        and last bin.\n\n        Parameters\n        ----------\n        lo_bnd : float\n            Lower bound of first axis bin.\n        hi_bnd : float\n            Upper bound of last axis bin.\n        nbin : int\n            Number of bins.\n        interp : {\'lin\', \'log\', \'sqrt\'}\n            Interpolation method used to transform between axis and pixel\n            coordinates.  Default: \'lin\'.\n        """"""\n        nbin = int(nbin)\n        interp = kwargs.setdefault(""interp"", ""lin"")\n        node_type = kwargs.setdefault(""node_type"", ""edges"")\n\n        if node_type == ""edges"":\n            nnode = nbin + 1\n        elif node_type == ""center"":\n            nnode = nbin\n        else:\n            raise ValueError(f""Invalid node type: {node_type!r}"")\n\n        if interp == ""lin"":\n            nodes = np.linspace(lo_bnd, hi_bnd, nnode)\n        elif interp == ""log"":\n            nodes = np.exp(np.linspace(np.log(lo_bnd), np.log(hi_bnd), nnode))\n        elif interp == ""sqrt"":\n            nodes = np.linspace(lo_bnd ** 0.5, hi_bnd ** 0.5, nnode) ** 2.0\n        else:\n            raise ValueError(f""Invalid interp: {interp}"")\n\n        return cls(nodes, **kwargs)\n\n    @classmethod\n    def from_energy_bounds(\n        cls, emin, emax, nbin, unit=None, per_decade=False, name=None\n    ):\n        """"""Make an energy axis.\n\n        Used frequently also to make energy grids, by making\n        the axis, and then using ``axis.center`` or ``axis.edges``.\n\n        Parameters\n        ----------\n        emin, emax : `~astropy.units.Quantity`, float\n            Energy range\n        nbin : int\n            Number of bins\n        unit : `~astropy.units.Unit`\n            Energy unit\n        per_decade : bool\n            Whether `nbin` is given per decade.\n        energy_true : bool\n            Whether energy is true energy\n\n        Returns\n        -------\n        axis : `MapAxis`\n            Axis with name ""energy"" and interp ""log"".\n        """"""\n        emin = u.Quantity(emin, unit)\n        emax = u.Quantity(emax, unit)\n\n        if unit is None:\n            unit = emax.unit\n            emin = emin.to(unit)\n\n        if per_decade:\n            nbin = np.ceil(np.log10(emax / emin).value * nbin)\n\n        if name is None:\n            name = ""energy""\n\n        return cls.from_bounds(\n            emin.value, emax.value, nbin=nbin, unit=unit, interp=""log"", name=name\n        )\n\n    @classmethod\n    def from_nodes(cls, nodes, **kwargs):\n        """"""Generate an axis object from a sequence of nodes (bin centers).\n\n        This will create a sequence of bins with edges half-way\n        between the node values.  This method should be used to\n        construct an axis where the bin center should lie at a\n        specific value (e.g. a map of a continuous function).\n\n        Parameters\n        ----------\n        nodes : `~numpy.ndarray`\n            Axis nodes (bin center).\n        interp : {\'lin\', \'log\', \'sqrt\'}\n            Interpolation method used to transform between axis and pixel\n            coordinates.  Default: \'lin\'.\n        """"""\n        if len(nodes) < 1:\n            raise ValueError(""Nodes array must have at least one element."")\n\n        return cls(nodes, node_type=""center"", **kwargs)\n\n    @classmethod\n    def from_edges(cls, edges, **kwargs):\n        """"""Generate an axis object from a sequence of bin edges.\n\n        This method should be used to construct an axis where the bin\n        edges should lie at specific values (e.g. a histogram).  The\n        number of bins will be one less than the number of edges.\n\n        Parameters\n        ----------\n        edges : `~numpy.ndarray`\n            Axis bin edges.\n        interp : {\'lin\', \'log\', \'sqrt\'}\n            Interpolation method used to transform between axis and pixel\n            coordinates.  Default: \'lin\'.\n        """"""\n        if len(edges) < 2:\n            raise ValueError(""Edges array must have at least two elements."")\n\n        return cls(edges, node_type=""edges"", **kwargs)\n\n    def pix_to_coord(self, pix):\n        """"""Transform from pixel to axis coordinates.\n\n        Parameters\n        ----------\n        pix : `~numpy.ndarray`\n            Array of pixel coordinate values.\n\n        Returns\n        -------\n        coord : `~numpy.ndarray`\n            Array of axis coordinate values.\n        """"""\n        pix = pix - self._pix_offset\n        values = pix_to_coord(self._nodes, pix, interp=self._interp)\n        return u.Quantity(values, unit=self.unit, copy=False)\n\n    def coord_to_pix(self, coord):\n        """"""Transform from axis to pixel coordinates.\n\n        Parameters\n        ----------\n        coord : `~numpy.ndarray`\n            Array of axis coordinate values.\n\n        Returns\n        -------\n        pix : `~numpy.ndarray`\n            Array of pixel coordinate values.\n        """"""\n        coord = u.Quantity(coord, self.unit, copy=False).value\n        pix = coord_to_pix(self._nodes, coord, interp=self._interp)\n        return np.array(pix + self._pix_offset, ndmin=1)\n\n    def coord_to_idx(self, coord, clip=False):\n        """"""Transform from axis coordinate to bin index.\n\n        Parameters\n        ----------\n        coord : `~numpy.ndarray`\n            Array of axis coordinate values.\n        clip : bool\n            Choose whether to clip the index to the valid range of the\n            axis.  If false then indices for values outside the axis\n            range will be set -1.\n\n        Returns\n        -------\n        idx : `~numpy.ndarray`\n            Array of bin indices.\n        """"""\n        coord = u.Quantity(coord, self.unit, copy=False).value\n        return coord_to_idx(self.edges.value, coord, clip)\n\n    def slice(self, idx):\n        """"""Create a new axis object by extracting a slice from this axis.\n\n        Parameters\n        ----------\n        idx : slice\n            Slice object selecting a subselection of the axis.\n\n        Returns\n        -------\n        axis : `~MapAxis`\n            Sliced axis object.\n        """"""\n        center = self.center[idx].value\n        idx = self.coord_to_idx(center)\n        # For edge nodes we need to keep N+1 nodes\n        if self._node_type == ""edges"":\n            idx = tuple(list(idx) + [1 + idx[-1]])\n\n        nodes = self._nodes[(idx,)]\n        return MapAxis(\n            nodes,\n            interp=self._interp,\n            name=self._name,\n            node_type=self._node_type,\n            unit=self._unit,\n        )\n\n    def squash(self):\n        """"""Create a new axis object by squashing the axis into one bin.\n\n        Returns\n        -------\n        axis : `~MapAxis`\n            Sliced axis object.\n        """"""\n        # TODO: Decide on handling node_type=center\n        # See https://github.com/gammapy/gammapy/issues/1952\n        return MapAxis.from_bounds(\n            lo_bnd=self.edges[0].value,\n            hi_bnd=self.edges[-1].value,\n            nbin=1,\n            interp=self._interp,\n            name=self._name,\n            unit=self._unit,\n        )\n\n    def __repr__(self):\n        str_ = self.__class__.__name__\n        str_ += ""\\n\\n""\n        fmt = ""\\t{:<10s} : {:<10s}\\n""\n        str_ += fmt.format(""name"", self.name)\n        str_ += fmt.format(""unit"", ""{!r}"".format(str(self.unit)))\n        str_ += fmt.format(""nbins"", str(self.nbin))\n        str_ += fmt.format(""node type"", self.node_type)\n        vals = self.edges if self.node_type == ""edges"" else self.center\n        str_ += fmt.format(f""{self.node_type} min"", ""{:.1e}"".format(vals.min()))\n        str_ += fmt.format(f""{self.node_type} max"", ""{:.1e}"".format(vals.max()))\n        str_ += fmt.format(""interp"", self._interp)\n        return str_\n\n    def _init_copy(self, **kwargs):\n        """"""Init map axis instance by copying missing init arguments from self.\n        """"""\n        argnames = inspect.getfullargspec(self.__init__).args\n        argnames.remove(""self"")\n\n        for arg in argnames:\n            value = getattr(self, ""_"" + arg)\n            kwargs.setdefault(arg, copy.deepcopy(value))\n\n        return self.__class__(**kwargs)\n\n    def copy(self, **kwargs):\n        """"""Copy `MapAxis` instance and overwrite given attributes.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Keyword arguments to overwrite in the map axis constructor.\n\n        Returns\n        -------\n        copy : `MapAxis`\n            Copied map axis.\n        """"""\n        return self._init_copy(**kwargs)\n\n    def group_table(self, edges):\n        """"""Compute bin groups table for the map axis, given coarser bin edges.\n\n        Parameters\n        ----------\n        edges : `~astropy.units.Quantity`\n            Group bin edges.\n\n        Returns\n        -------\n        groups : `~astropy.table.Table`\n            Map axis group table.\n        """"""\n        # TODO: try to simplify this code\n        if not self.node_type == ""edges"":\n            raise ValueError(""Only edge based map axis can be grouped"")\n\n        edges_pix = self.coord_to_pix(edges)\n        edges_pix = np.clip(edges_pix, -0.5, self.nbin - 0.5)\n        edges_idx = np.round(edges_pix + 0.5) - 0.5\n        edges_idx = np.unique(edges_idx)\n        edges_ref = self.pix_to_coord(edges_idx)\n\n        groups = QTable()\n        groups[f""{self.name}_min""] = edges_ref[:-1]\n        groups[f""{self.name}_max""] = edges_ref[1:]\n\n        groups[""idx_min""] = (edges_idx[:-1] + 0.5).astype(int)\n        groups[""idx_max""] = (edges_idx[1:] - 0.5).astype(int)\n\n        if len(groups) == 0:\n            raise ValueError(""No overlap between reference and target edges."")\n\n        groups[""bin_type""] = ""normal   ""\n\n        edge_idx_start, edge_ref_start = edges_idx[0], edges_ref[0]\n        if edge_idx_start > 0:\n            underflow = {\n                ""bin_type"": ""underflow"",\n                ""idx_min"": 0,\n                ""idx_max"": edge_idx_start,\n                f""{self.name}_min"": self.pix_to_coord(-0.5),\n                f""{self.name}_max"": edge_ref_start,\n            }\n            groups.insert_row(0, vals=underflow)\n\n        edge_idx_end, edge_ref_end = edges_idx[-1], edges_ref[-1]\n\n        if edge_idx_end < (self.nbin - 0.5):\n            overflow = {\n                ""bin_type"": ""overflow"",\n                ""idx_min"": edge_idx_end + 1,\n                ""idx_max"": self.nbin - 1,\n                f""{self.name}_min"": edge_ref_end,\n                f""{self.name}_max"": self.pix_to_coord(self.nbin - 0.5),\n            }\n            groups.add_row(vals=overflow)\n\n        group_idx = Column(np.arange(len(groups)))\n        groups.add_column(group_idx, name=""group_idx"", index=0)\n        return groups\n\n    def _up_down_sample(self, nbin):\n        if self.node_type == ""edges"":\n            nodes = self.edges\n        else:\n            nodes = self.center\n\n        lo_bnd, hi_bnd = nodes.min(), nodes.max()\n\n        return self.from_bounds(\n            lo_bnd=lo_bnd.value,\n            hi_bnd=hi_bnd.value,\n            nbin=nbin,\n            interp=self.interp,\n            node_type=self.node_type,\n            unit=self.unit,\n            name=self.name,\n        )\n\n    def upsample(self, factor):\n        """"""Upsample map axis by a given factor.\n\n        Parameters\n        ----------\n        factor : int\n            Upsampling factor.\n\n\n        Returns\n        -------\n        axis : `MapAxis`\n            Usampled map axis.\n\n        """"""\n        nbin = self.nbin * factor\n        return self._up_down_sample(nbin)\n\n    def downsample(self, factor):\n        """"""Downsample map axis by a given factor.\n\n        Parameters\n        ----------\n        factor : int\n            Downsampling factor.\n\n\n        Returns\n        -------\n        axis : `MapAxis`\n            Downsampled map axis.\n\n        """"""\n        nbin = int(self.nbin / factor)\n        return self._up_down_sample(nbin)\n\n\nclass MapCoord:\n    """"""Represents a sequence of n-dimensional map coordinates.\n\n    Contains coordinates for 2 spatial dimensions and an arbitrary\n    number of additional non-spatial dimensions.\n\n    For further information see :ref:`mapcoord`.\n\n    Parameters\n    ----------\n    data : `dict` of `~numpy.ndarray`\n        Dictionary of coordinate arrays.\n    frame : {""icrs"", ""galactic"", None}\n        Spatial coordinate system.  If None then the coordinate system\n        will be set to the native coordinate system of the geometry.\n    match_by_name : bool\n        Match coordinates to axes by name?\n        If false coordinates will be matched by index.\n    """"""\n\n    def __init__(self, data, frame=None, match_by_name=True):\n\n        if ""lon"" not in data or ""lat"" not in data:\n            raise ValueError(""data dictionary must contain axes named \'lon\' and \'lat\'."")\n\n        data = {k: np.atleast_1d(np.asanyarray(v)) for k, v in data.items()}\n        vals = np.broadcast_arrays(*data.values(), subok=True)\n        self._data = dict(zip(data.keys(), vals))\n        self._frame = frame\n        self._match_by_name = match_by_name\n\n    def __getitem__(self, key):\n        if isinstance(key, str):\n            return self._data[key]\n        else:\n            return list(self._data.values())[key]\n\n    def __iter__(self):\n        return iter(self._data.values())\n\n    @property\n    def ndim(self):\n        """"""Number of dimensions.""""""\n        return len(self._data)\n\n    @property\n    def shape(self):\n        """"""Coordinate array shape.""""""\n        return self[0].shape\n\n    @property\n    def size(self):\n        return self[0].size\n\n    @property\n    def lon(self):\n        """"""Longitude coordinate in degrees.""""""\n        return self._data[""lon""]\n\n    @property\n    def lat(self):\n        """"""Latitude coordinate in degrees.""""""\n        return self._data[""lat""]\n\n    @property\n    def theta(self):\n        """"""Theta co-latitude angle in radians.""""""\n        theta = u.Quantity(self.lat, unit=""deg"", copy=False).to_value(""rad"")\n        return np.pi / 2.0 - theta\n\n    @property\n    def phi(self):\n        """"""Phi longitude angle in radians.""""""\n        phi = u.Quantity(self.lon, unit=""deg"", copy=False).to_value(""rad"")\n        return phi\n\n    @property\n    def frame(self):\n        """"""Coordinate system (str).""""""\n        return self._frame\n\n    @property\n    def match_by_name(self):\n        """"""Boolean flag: axis lookup by name (True) or index (False).""""""\n        return self._match_by_name\n\n    @property\n    def skycoord(self):\n        return SkyCoord(self.lon, self.lat, unit=""deg"", frame=self.frame)\n\n    @classmethod\n    def _from_lonlat(cls, coords, frame=None):\n        """"""Create a `~MapCoord` from a tuple of coordinate vectors.\n\n        The first two elements of the tuple should be longitude and latitude in degrees.\n\n        Parameters\n        ----------\n        coords : tuple\n            Tuple of `~numpy.ndarray`.\n\n        Returns\n        -------\n        coord : `~MapCoord`\n            A coordinates object.\n        """"""\n        if isinstance(coords, (list, tuple)):\n            coords_dict = {""lon"": coords[0], ""lat"": coords[1]}\n            for i, c in enumerate(coords[2:]):\n                coords_dict[f""axis{i}""] = c\n        else:\n            raise ValueError(""Unrecognized input type."")\n\n        return cls(coords_dict, frame=frame, match_by_name=False)\n\n    @classmethod\n    def _from_tuple(cls, coords, frame=None):\n        """"""Create from tuple of coordinate vectors.""""""\n        if isinstance(coords[0], (list, np.ndarray)) or np.isscalar(coords[0]):\n            return cls._from_lonlat(coords, frame=frame)\n        elif isinstance(coords[0], SkyCoord):\n            lon, lat, frame = skycoord_to_lonlat(coords[0], frame=frame)\n            coords = (lon, lat) + coords[1:]\n            return cls._from_lonlat(coords, frame=frame)\n        else:\n            raise TypeError(f""Type not supported: {type(coords)!r}"")\n\n    @classmethod\n    def _from_dict(cls, coords, frame=None):\n        """"""Create from a dictionary of coordinate vectors.""""""\n        if ""lon"" in coords and ""lat"" in coords:\n            return cls(coords, frame=frame)\n        elif ""skycoord"" in coords:\n            lon, lat, frame = skycoord_to_lonlat(coords[""skycoord""], frame=frame)\n            coords_dict = {""lon"": lon, ""lat"": lat}\n            for k, v in coords.items():\n                if k == ""skycoord"":\n                    continue\n                coords_dict[k] = v\n            return cls(coords_dict, frame=frame)\n        else:\n            raise ValueError(""coords dict must contain \'lon\'/\'lat\' or \'skycoord\'."")\n\n    @classmethod\n    def create(cls, data, frame=None):\n        """"""Create a new `~MapCoord` object.\n\n        This method can be used to create either unnamed (with tuple input)\n        or named (via dict input) axes.\n\n        Parameters\n        ----------\n        data : tuple, dict, `MapCoord` or `~astropy.coordinates.SkyCoord`\n            Object containing coordinate arrays.\n        frame : {""icrs"", ""galactic"", None}, optional\n            Set the coordinate system for longitude and latitude. If\n            None longitude and latitude will be assumed to be in\n            the coordinate system native to a given map geometry.\n\n        Examples\n        --------\n        >>> from astropy.coordinates import SkyCoord\n        >>> from gammapy.maps import MapCoord\n\n        >>> lon, lat = [1, 2], [2, 3]\n        >>> skycoord = SkyCoord(lon, lat, unit=\'deg\')\n        >>> energy = [1000]\n        >>> c = MapCoord.create((lon,lat))\n        >>> c = MapCoord.create((skycoord,))\n        >>> c = MapCoord.create((lon,lat,energy))\n        >>> c = MapCoord.create(dict(lon=lon,lat=lat))\n        >>> c = MapCoord.create(dict(lon=lon,lat=lat,energy=energy))\n        >>> c = MapCoord.create(dict(skycoord=skycoord,energy=energy))\n        """"""\n        if isinstance(data, cls):\n            if data.frame is None or frame == data.frame:\n                return data\n            else:\n                return data.to_frame(frame)\n        elif isinstance(data, dict):\n            return cls._from_dict(data, frame=frame)\n        elif isinstance(data, (list, tuple)):\n            return cls._from_tuple(data, frame=frame)\n        elif isinstance(data, SkyCoord):\n            return cls._from_tuple((data,), frame=frame)\n        else:\n            raise TypeError(f""Unsupported input type: {type(data)!r}"")\n\n    def to_frame(self, frame):\n        """"""Convert to a different coordinate frame.\n\n        Parameters\n        ----------\n        frame : {""icrs"", ""galactic""}\n            Coordinate system, either Galactic (""galactic"") or Equatorial (""icrs"").\n\n        Returns\n        -------\n        coords : `~MapCoord`\n            A coordinates object.\n        """"""\n        if frame == self.frame:\n            return copy.deepcopy(self)\n        else:\n            lon, lat, frame = skycoord_to_lonlat(self.skycoord, frame=frame)\n            data = copy.deepcopy(self._data)\n            if isinstance(self.lon, u.Quantity):\n                lon = u.Quantity(lon, unit=""deg"", copy=False)\n\n            if isinstance(self.lon, u.Quantity):\n                lat = u.Quantity(lat, unit=""deg"", copy=False)\n\n            data[""lon""] = lon\n            data[""lat""] = lat\n            return self.__class__(data, frame, self._match_by_name)\n\n    def apply_mask(self, mask):\n        """"""Return a masked copy of this coordinate object.\n\n        Parameters\n        ----------\n        mask : `~numpy.ndarray`\n            Boolean mask.\n\n        Returns\n        -------\n        coords : `~MapCoord`\n            A coordinates object.\n        """"""\n        data = {k: v[mask] for k, v in self._data.items()}\n        return self.__class__(data, self.frame, self._match_by_name)\n\n    def copy(self):\n        """"""Copy `MapCoord` object.""""""\n        return copy.deepcopy(self)\n\n    def __repr__(self):\n        return (\n            f""{self.__class__.__name__}\\n\\n""\n            f""\\taxes     : {list(self._data.keys())}\\n""\n            f""\\tshape    : {self.shape[::-1]}\\n""\n            f""\\tndim     : {self.ndim}\\n""\n            f""\\tframe : {self.frame}\\n""\n        )\n\n\nclass Geom(abc.ABC):\n    """"""Map geometry base class.\n\n    See also: `~gammapy.maps.WcsGeom` and `~gammapy.maps.HpxGeom`\n    """"""\n\n    @property\n    @abc.abstractmethod\n    def data_shape(self):\n        """"""Shape of the Numpy data array matching this geometry.""""""\n        pass\n\n    @property\n    @abc.abstractmethod\n    def is_allsky(self):\n        pass\n\n    @property\n    @abc.abstractmethod\n    def center_coord(self):\n        pass\n\n    @property\n    @abc.abstractmethod\n    def center_pix(self):\n        pass\n\n    @property\n    @abc.abstractmethod\n    def center_skydir(self):\n        pass\n\n    @property\n    def axes_names(self):\n        return [ax.name for ax in self.axes]\n\n    @classmethod\n    def from_hdulist(cls, hdulist, hdu=None, hdu_bands=None):\n        """"""Load a geometry object from a FITS HDUList.\n\n        Parameters\n        ----------\n        hdulist :  `~astropy.io.fits.HDUList`\n            HDU list containing HDUs for map data and bands.\n        hdu : str\n            Name or index of the HDU with the map data.\n        hdu_bands : str\n            Name or index of the HDU with the BANDS table.  If not\n            defined this will be inferred from the FITS header of the\n            map HDU.\n\n        Returns\n        -------\n        geom : `~Geom`\n            Geometry object.\n        """"""\n        if hdu is None:\n            hdu = find_hdu(hdulist)\n        else:\n            hdu = hdulist[hdu]\n\n        if hdu_bands is None:\n            hdu_bands = find_bands_hdu(hdulist, hdu)\n\n        if hdu_bands is not None:\n            hdu_bands = hdulist[hdu_bands]\n\n        return cls.from_header(hdu.header, hdu_bands)\n\n    def make_bands_hdu(self, hdu=None, hdu_skymap=None, conv=None):\n        header = fits.Header()\n        self._fill_header_from_axes(header)\n        axis_names = None\n\n        # FIXME: Check whether convention is compatible with\n        # dimensionality of geometry\n\n        if conv == ""fgst-ccube"":\n            hdu = ""EBOUNDS""\n            axis_names = [""energy""]\n        elif conv == ""fgst-template"":\n            hdu = ""ENERGIES""\n            axis_names = [""energy""]\n        elif conv == ""gadf"" and hdu is None:\n            if hdu_skymap:\n                hdu = f""{hdu_skymap}_BANDS""\n            else:\n                hdu = ""BANDS""\n        # else:\n        #     raise ValueError(\'Unknown conv: {}\'.format(conv))\n\n        cols = make_axes_cols(self.axes, axis_names)\n        cols += self._make_bands_cols()\n        return fits.BinTableHDU.from_columns(cols, header, name=hdu)\n\n    @abc.abstractmethod\n    def _make_bands_cols(self):\n        pass\n\n    @abc.abstractmethod\n    def get_idx(self, idx=None, local=False, flat=False):\n        """"""Get tuple of pixel indices for this geometry.\n\n        Returns all pixels in the geometry by default. Pixel indices\n        for a single image plane can be accessed by setting ``idx``\n        to the index tuple of a plane.\n\n        Parameters\n        ----------\n        idx : tuple, optional\n            A tuple of indices with one index for each non-spatial\n            dimension.  If defined only pixels for the image plane with\n            this index will be returned.  If none then all pixels\n            will be returned.\n        local : bool\n            Flag to return local or global pixel indices.  Local\n            indices run from 0 to the number of pixels in a given\n            image plane.\n        flat : bool, optional\n            Return a flattened array containing only indices for\n            pixels contained in the geometry.\n\n        Returns\n        -------\n        idx : tuple\n            Tuple of pixel index vectors with one vector for each\n            dimension.\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def get_coord(self, idx=None, flat=False):\n        """"""Get the coordinate array for this geometry.\n\n        Returns a coordinate array with the same shape as the data\n        array.  Pixels outside the geometry are set to NaN.\n        Coordinates for a single image plane can be accessed by\n        setting ``idx`` to the index tuple of a plane.\n\n        Parameters\n        ----------\n        idx : tuple, optional\n            A tuple of indices with one index for each non-spatial\n            dimension.  If defined only coordinates for the image\n            plane with this index will be returned.  If none then\n            coordinates for all pixels will be returned.\n        flat : bool, optional\n            Return a flattened array containing only coordinates for\n            pixels contained in the geometry.\n\n        Returns\n        -------\n        coords : tuple\n            Tuple of coordinate vectors with one vector for each\n            dimension.\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def coord_to_pix(self, coords):\n        """"""Convert map coordinates to pixel coordinates.\n\n        Parameters\n        ----------\n        coords : tuple\n            Coordinate values in each dimension of the map.  This can\n            either be a tuple of numpy arrays or a MapCoord object.\n            If passed as a tuple then the ordering should be\n            (longitude, latitude, c_0, ..., c_N) where c_i is the\n            coordinate vector for axis i.\n\n        Returns\n        -------\n        pix : tuple\n            Tuple of pixel coordinates in image and band dimensions.\n        """"""\n        pass\n\n    def coord_to_idx(self, coords, clip=False):\n        """"""Convert map coordinates to pixel indices.\n\n        Parameters\n        ----------\n        coords : tuple or `~MapCoord`\n            Coordinate values in each dimension of the map.  This can\n            either be a tuple of numpy arrays or a MapCoord object.\n            If passed as a tuple then the ordering should be\n            (longitude, latitude, c_0, ..., c_N) where c_i is the\n            coordinate vector for axis i.\n        clip : bool\n            Choose whether to clip indices to the valid range of the\n            geometry.  If false then indices for coordinates outside\n            the geometry range will be set -1.\n\n        Returns\n        -------\n        pix : tuple\n            Tuple of pixel indices in image and band dimensions.\n            Elements set to -1 correspond to coordinates outside the\n            map.\n        """"""\n        pix = self.coord_to_pix(coords)\n        return self.pix_to_idx(pix, clip=clip)\n\n    @abc.abstractmethod\n    def pix_to_coord(self, pix):\n        """"""Convert pixel coordinates to map coordinates.\n\n        Parameters\n        ----------\n        pix : tuple\n            Tuple of pixel coordinates.\n\n        Returns\n        -------\n        coords : tuple\n            Tuple of map coordinates.\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def pix_to_idx(self, pix, clip=False):\n        """"""Convert pixel coordinates to pixel indices.\n\n        Returns -1 for pixel coordinates that lie outside of the map.\n\n        Parameters\n        ----------\n        pix : tuple\n            Tuple of pixel coordinates.\n        clip : bool\n            Choose whether to clip indices to the valid range of the\n            geometry.  If false then indices for coordinates outside\n            the geometry range will be set -1.\n\n        Returns\n        -------\n        idx : tuple\n            Tuple of pixel indices.\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def contains(self, coords):\n        """"""Check if a given map coordinate is contained in the geometry.\n\n        Parameters\n        ----------\n        coords : tuple or `~gammapy.maps.MapCoord`\n            Tuple of map coordinates.\n\n        Returns\n        -------\n        containment : `~numpy.ndarray`\n            Bool array.\n        """"""\n        pass\n\n    def contains_pix(self, pix):\n        """"""Check if a given pixel coordinate is contained in the geometry.\n\n        Parameters\n        ----------\n        pix : tuple\n            Tuple of pixel coordinates.\n\n        Returns\n        -------\n        containment : `~numpy.ndarray`\n            Bool array.\n        """"""\n        idx = self.pix_to_idx(pix)\n        return np.all(np.stack([t != INVALID_INDEX.int for t in idx]), axis=0)\n\n    def slice_by_idx(self, slices):\n        """"""Create a new geometry by slicing the non-spatial axes.\n\n        Parameters\n        ----------\n        slices : dict\n            Dict of axes names and integers or `slice` object pairs. Contains one\n            element for each non-spatial dimension. For integer indexing the\n            correspoding axes is dropped from the map. Axes not specified in the\n            dict are kept unchanged.\n\n        Returns\n        -------\n        geom : `~Geom`\n            Sliced geometry.\n        """"""\n        axes = []\n        for ax in self.axes:\n            ax_slice = slices.get(ax.name, slice(None))\n            if isinstance(ax_slice, slice):\n                ax_sliced = ax.slice(ax_slice)\n                axes.append(ax_sliced)\n                # in the case where isinstance(ax_slice, int) the axes is dropped\n\n        return self._init_copy(axes=axes)\n\n    @abc.abstractmethod\n    def to_image(self):\n        """"""Create 2D image geometry (drop non-spatial dimensions).\n\n        Returns\n        -------\n        geom : `~Geom`\n            Image geometry.\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def to_cube(self, axes):\n        """"""Append non-spatial axes to create a higher-dimensional geometry.\n\n        This will result in a new geometry with\n        N+M dimensions where N is the number of current dimensions and\n        M is the number of axes in the list.\n\n        Parameters\n        ----------\n        axes : list\n            Axes that will be appended to this geometry.\n\n        Returns\n        -------\n        geom : `~Geom`\n            Map geometry.\n        """"""\n        pass\n\n    def coord_to_tuple(self, coord):\n        """"""Generate a coordinate tuple compatible with this geometry.\n\n        Parameters\n        ----------\n        coord : `~MapCoord`\n        """"""\n        if self.ndim != coord.ndim:\n            raise ValueError(""ndim mismatch"")\n\n        if not coord.match_by_name:\n            return tuple(coord._data.values())\n\n        coord_tuple = [coord.lon, coord.lat]\n        for ax in self.axes:\n            coord_tuple += [coord[ax.name]]\n\n        return coord_tuple\n\n    @abc.abstractmethod\n    def pad(self, pad_width):\n        """"""\n        Pad the geometry at the edges.\n\n        Parameters\n        ----------\n        pad_width : {sequence, array_like, int}\n            Number of values padded to the edges of each axis.\n\n        Returns\n        -------\n        geom : `~Geom`\n            Padded geometry.\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def crop(self, crop_width):\n        """"""\n        Crop the geometry at the edges.\n\n        Parameters\n        ----------\n        crop_width : {sequence, array_like, int}\n            Number of values cropped from the edges of each axis.\n\n        Returns\n        -------\n        geom : `~Geom`\n            Cropped geometry.\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def downsample(self, factor, axis):\n        """"""Downsample the spatial dimension of the geometry by a given factor.\n\n        Parameters\n        ----------\n        factor : int\n            Downsampling factor.\n        axis : str\n            Axis to downsample.\n\n        Returns\n        -------\n        geom : `~Geom`\n            Downsampled geometry.\n\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def upsample(self, factor, axis):\n        """"""Upsample the spatial dimension of the geometry by a given factor.\n\n        Parameters\n        ----------\n        factor : int\n            Upsampling factor.\n        axis : str\n            Axis to upsample.\n\n        Returns\n        -------\n        geom : `~Geom`\n            Upsampled geometry.\n\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def solid_angle(self):\n        """"""Solid angle (`~astropy.units.Quantity` in ``sr``).""""""\n        pass\n\n    def _fill_header_from_axes(self, header):\n        for idx, ax in enumerate(self.axes, start=1):\n            key = ""AXCOLS%i"" % idx\n            name = ax.name.upper()\n            if ax.name == ""energy"" and ax.node_type == ""edges"":\n                header[key] = ""E_MIN,E_MAX""\n            elif ax.name == ""energy"" and ax.node_type == ""center"":\n                header[key] = ""ENERGY""\n            elif ax.node_type == ""edges"":\n                header[key] = f""{name}_MIN,{name}_MAX""\n            elif ax.node_type == ""center"":\n                header[key] = name\n            else:\n                raise ValueError(f""Invalid node type {ax.node_type!r}"")\n\n            key_interp = ""INTERP%i"" % idx\n            header[key_interp] = ax._interp\n\n    @property\n    def is_image(self):\n        """"""Whether the geom is equivalent to an image without extra dimensions.""""""\n        if self.axes is None:\n            return True\n        return len(self.axes) == 0\n\n    def get_axis_by_name(self, name):\n        """"""Get an axis by name (case in-sensitive).\n\n        Parameters\n        ----------\n        name : str\n           Name of the requested axis\n\n        Returns\n        -------\n        axis : `~gammapy.maps.MapAxis`\n            Axis\n        """"""\n        axes = {axis.name.upper(): axis for axis in self.axes}\n        return axes[name.upper()]\n\n    def get_axis_index_by_name(self, name):\n        """"""Get an axis index by name (case in-sensitive).\n\n        Parameters\n        ----------\n        name : str\n           Axis name\n\n        Returns\n        -------\n        index : int\n            Axis index\n        """"""\n        names = [axis.name.upper() for axis in self.axes]\n        return names.index(name.upper())\n\n    def _init_copy(self, **kwargs):\n        """"""Init map geom instance by copying missing init arguments from self.\n        """"""\n        argnames = inspect.getfullargspec(self.__init__).args\n        argnames.remove(""self"")\n\n        for arg in argnames:\n            value = getattr(self, ""_"" + arg)\n            kwargs.setdefault(arg, copy.deepcopy(value))\n\n        return self.__class__(**kwargs)\n\n    def copy(self, **kwargs):\n        """"""Copy and overwrite given attributes.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Keyword arguments to overwrite in the map geometry constructor.\n\n        Returns\n        -------\n        copy : `Geom`\n            Copied map geometry.\n        """"""\n        return self._init_copy(**kwargs)\n\n    def energy_mask(self, emin=None, emax=None):\n        """"""Create a mask for a given energy range.\n\n        The energy bin must be fully contained to be included in the mask.\n\n        Parameters\n        ----------\n        emin, emax : `~astropy.units.Quantity`\n            Energy range\n\n        Returns\n        -------\n        mask : `~numpy.ndarray`\n            Energy mask\n        """"""\n        # get energy axes and values\n        energy_axis = self.get_axis_by_name(""energy"")\n        edges = energy_axis.edges.reshape((-1, 1, 1))\n\n        # set default values\n        emin = emin if emin is not None else edges[0]\n        emax = emax if emax is not None else edges[-1]\n\n        mask = (edges[:-1] >= emin) & (edges[1:] <= emax)\n        return np.broadcast_to(mask, shape=self.data_shape)\n'"
gammapy/maps/hpx.py,129,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Utilities for dealing with HEALPix projections and mappings.""""""\nimport copy\nimport re\nimport numpy as np\nfrom astropy.coordinates import SkyCoord\nfrom astropy.io import fits\nfrom astropy.units import Quantity\nfrom .geom import (\n    Geom,\n    MapCoord,\n    find_and_read_bands,\n    make_axes,\n    pix_tuple_to_idx,\n    skycoord_to_lonlat,\n)\nfrom .utils import INVALID_INDEX, coordsys_to_frame, frame_to_coordsys\nfrom .wcs import WcsGeom\n\n# Not sure if we should expose this in the docs or not:\n# HPX_FITS_CONVENTIONS, HpxConv\n__all__ = [""HpxGeom""]\n\n# Approximation of the size of HEALPIX pixels (in degrees) for a particular order.\n# Used to convert from HEALPIX to WCS-based projections.\nHPX_ORDER_TO_PIXSIZE = np.array(\n    [32.0, 16.0, 8.0, 4.0, 2.0, 1.0, 0.50, 0.25, 0.1, 0.05, 0.025, 0.01, 0.005, 0.002]\n)\n\n\nclass HpxConv:\n    """"""Data structure to define how a HEALPIX map is stored to FITS.""""""\n\n    def __init__(self, convname, **kwargs):\n        self.convname = convname\n        self.colstring = kwargs.get(""colstring"", ""CHANNEL"")\n        self.firstcol = kwargs.get(""firstcol"", 1)\n        self.hduname = kwargs.get(""hduname"", ""SKYMAP"")\n        self.bands_hdu = kwargs.get(""bands_hdu"", ""EBOUNDS"")\n        self.quantity_type = kwargs.get(""quantity_type"", ""integral"")\n        self.frame = kwargs.get(""frame"", ""COORDSYS"")\n\n    def colname(self, indx):\n        return f""{self.colstring}{indx}""\n\n    @classmethod\n    def create(cls, convname=""gadf""):\n        return copy.deepcopy(HPX_FITS_CONVENTIONS[convname])\n\n\nHPX_FITS_CONVENTIONS = {}\n""""""Various conventions for storing HEALPIX maps in FITS files""""""\nHPX_FITS_CONVENTIONS[None] = HpxConv(""gadf"", bands_hdu=""BANDS"")\nHPX_FITS_CONVENTIONS[""gadf""] = HpxConv(""gadf"", bands_hdu=""BANDS"")\nHPX_FITS_CONVENTIONS[""fgst-ccube""] = HpxConv(""fgst-ccube"")\nHPX_FITS_CONVENTIONS[""fgst-ltcube""] = HpxConv(\n    ""fgst-ltcube"", colstring=""COSBINS"", hduname=""EXPOSURE"", bands_hdu=""CTHETABOUNDS""\n)\nHPX_FITS_CONVENTIONS[""fgst-bexpcube""] = HpxConv(\n    ""fgst-bexpcube"", colstring=""ENERGY"", hduname=""HPXEXPOSURES"", bands_hdu=""ENERGIES""\n)\nHPX_FITS_CONVENTIONS[""fgst-srcmap""] = HpxConv(\n    ""fgst-srcmap"", hduname=None, quantity_type=""differential""\n)\nHPX_FITS_CONVENTIONS[""fgst-template""] = HpxConv(\n    ""fgst-template"", colstring=""ENERGY"", bands_hdu=""ENERGIES""\n)\nHPX_FITS_CONVENTIONS[""fgst-srcmap-sparse""] = HpxConv(\n    ""fgst-srcmap-sparse"", colstring=None, hduname=None, quantity_type=""differential""\n)\nHPX_FITS_CONVENTIONS[""galprop""] = HpxConv(\n    ""galprop"",\n    colstring=""Bin"",\n    hduname=""SKYMAP2"",\n    bands_hdu=""ENERGIES"",\n    quantity_type=""differential"",\n    frame=""COORDTYPE"",\n)\nHPX_FITS_CONVENTIONS[""galprop2""] = HpxConv(\n    ""galprop"",\n    colstring=""Bin"",\n    hduname=""SKYMAP2"",\n    bands_hdu=""ENERGIES"",\n    quantity_type=""differential"",\n)\n\n\ndef unravel_hpx_index(idx, npix):\n    """"""Convert flattened global map index to an index tuple.\n\n    Parameters\n    ----------\n    idx : `~numpy.ndarray`\n        Flat index.\n    npix : `~numpy.ndarray`\n        Number of pixels in each band.\n\n    Returns\n    -------\n    idx : tuple of `~numpy.ndarray`\n        Index array for each dimension of the map.\n    """"""\n    if npix.size == 1:\n        return tuple([idx])\n\n    dpix = np.zeros(npix.size, dtype=""i"")\n    dpix[1:] = np.cumsum(npix.flat[:-1])\n    bidx = np.searchsorted(np.cumsum(npix.flat), idx + 1)\n    pix = idx - dpix[bidx]\n    return tuple([pix] + list(np.unravel_index(bidx, npix.shape)))\n\n\ndef ravel_hpx_index(idx, npix):\n    """"""Convert map index tuple to a flattened index.\n\n    Parameters\n    ----------\n    idx : tuple of `~numpy.ndarray`\n\n    Returns\n    -------\n    idx : `~numpy.ndarray`\n    """"""\n    if len(idx) == 1:\n        return idx[0]\n\n    # TODO: raise exception for indices that are out of bounds\n\n    idx0 = idx[0]\n    idx1 = np.ravel_multi_index(idx[1:], npix.shape, mode=""clip"")\n    npix = np.concatenate((np.array([0]), npix.flat[:-1]))\n\n    return idx0 + np.cumsum(npix)[idx1]\n\n\ndef coords_to_vec(lon, lat):\n    """"""Converts longitude and latitude coordinates to a unit 3-vector.\n\n    Returns\n    -------\n    array(3,n) with v_x[i],v_y[i],v_z[i] = directional cosines\n    """"""\n    phi = np.radians(lon)\n    theta = (np.pi / 2) - np.radians(lat)\n    sin_t = np.sin(theta)\n    cos_t = np.cos(theta)\n\n    x = sin_t * np.cos(phi)\n    y = sin_t * np.sin(phi)\n    z = cos_t\n\n    # Stack them into the output array\n    out = np.vstack((x, y, z)).swapaxes(0, 1)\n    return out\n\n\ndef get_nside_from_pix_size(pixsz):\n    """"""Get the NSIDE that is closest to the given pixel size.\n\n    Parameters\n    ----------\n    pix : `~numpy.ndarray`\n        Pixel size in degrees.\n\n    Returns\n    -------\n    nside : `~numpy.ndarray`\n        NSIDE parameter.\n    """"""\n    import healpy as hp\n\n    pixsz = np.array(pixsz, ndmin=1)\n    nside = 2 ** np.linspace(1, 14, 14, dtype=int)\n    nside_pixsz = np.degrees(hp.nside2resol(nside))\n    return nside[np.argmin(np.abs(nside_pixsz - pixsz[..., None]), axis=-1)]\n\n\ndef get_pix_size_from_nside(nside):\n    """"""Estimate of the pixel size from the HEALPIX nside coordinate.\n\n    This just uses a lookup table to provide a nice round number\n    for each HEALPIX order.\n    """"""\n    order = nside_to_order(nside)\n    if np.any(order < 0) or np.any(order > 13):\n        raise ValueError(f""HEALPIX order must be 0 to 13. Got: {order!r}"")\n\n    return HPX_ORDER_TO_PIXSIZE[order]\n\n\ndef hpx_to_axes(h, npix):\n    """"""Generate a sequence of bin edge vectors corresponding to the axes of a HPX object.\n    """"""\n    x = h.ebins\n    z = np.arange(npix[-1] + 1)\n    return x, z\n\n\ndef make_hpx_to_wcs_mapping(hpx, wcs):\n    """"""Make the pixel mapping from HPX- to a WCS-based geometry.\n\n    Parameters\n    ----------\n    hpx : `~gammapy.maps.HpxGeom`\n       The HEALPIX geometry\n    wcs : `~gammapy.maps.WcsGeom`\n       The WCS geometry\n\n    Returns\n    -------\n    ipix : `~numpy.ndarray`\n        array(nx,ny) of HEALPIX pixel indices for each wcs pixel\n    mult_val : `~numpy.ndarray`\n        array(nx,ny) of 1./number of WCS pixels pointing at each HEALPIX pixel\n    npix : tuple\n        tuple(nx,ny) with the shape of the WCS grid\n    """"""\n    import healpy as hp\n\n    npix = wcs.npix\n\n    # FIXME: Calculation of WCS pixel centers should be moved into a\n    # method of WcsGeom\n    pix_crds = np.dstack(np.meshgrid(np.arange(npix[0]), np.arange(npix[1])))\n    pix_crds = pix_crds.swapaxes(0, 1).reshape((-1, 2))\n    sky_crds = wcs.wcs.wcs_pix2world(pix_crds, 0)\n    sky_crds *= np.radians(1.0)\n    sky_crds[0:, 1] = (np.pi / 2) - sky_crds[0:, 1]\n\n    mask = ~np.any(np.isnan(sky_crds), axis=1)\n    ipix = -1 * np.ones((len(hpx.nside), int(npix[0] * npix[1])), int)\n    m = mask[None, :] * np.ones_like(ipix, dtype=bool)\n\n    ipix[m] = hp.ang2pix(\n        hpx.nside[..., None],\n        sky_crds[:, 1][mask][None, ...],\n        sky_crds[:, 0][mask][None, ...],\n        hpx.nest,\n    ).flatten()\n\n    # Here we are counting the number of HEALPIX pixels each WCS pixel\n    # points to and getting a multiplicative factor that tells use how\n    # to split up the counts in each HEALPIX pixel (by dividing the\n    # corresponding WCS pixels by the number of associated HEALPIX\n    # pixels).\n    mult_val = np.ones_like(ipix, dtype=float)\n    for i, t in enumerate(ipix):\n        count = np.unique(t, return_counts=True)\n        idx = np.searchsorted(count[0], t)\n        mult_val[i, ...] = 1.0 / count[1][idx]\n\n    if hpx.nside.size == 1:\n        ipix = np.squeeze(ipix, axis=0)\n        mult_val = np.squeeze(mult_val, axis=0)\n\n    return ipix, mult_val, npix\n\n\ndef match_hpx_pix(nside, nest, nside_pix, ipix_ring):\n    """"""TODO: document.""""""\n    import healpy as hp\n\n    ipix_in = np.arange(12 * nside * nside)\n    vecs = hp.pix2vec(nside, ipix_in, nest)\n    pix_match = hp.vec2pix(nside_pix, vecs[0], vecs[1], vecs[2]) == ipix_ring\n    return ipix_in[pix_match]\n\n\ndef parse_hpxregion(region):\n    """"""Parse the ``HPX_REG`` header keyword into a list of tokens.\n    """"""\n    m = re.match(r""([A-Za-z\\_]*?)\\((.*?)\\)"", region)\n\n    if m is None:\n        raise ValueError(f""Failed to parse hpx region string: {region!r}"")\n\n    if not m.group(1):\n        return re.split("","", m.group(2))\n    else:\n        return [m.group(1)] + re.split("","", m.group(2))\n\n\ndef get_hpxregion_dir(region, frame):\n    """"""Get the reference direction for a HEALPIX region string.\n\n    Parameters\n    ----------\n    region : str\n        A string describing a HEALPIX region\n    frame : {\'icrs\', \'galactic\'}\n        Coordinate system\n    """"""\n    import healpy as hp\n\n    if region is None:\n        return SkyCoord(0.0, 0.0, frame=frame, unit=""deg"")\n\n    tokens = parse_hpxregion(region)\n    if tokens[0] in [""DISK"", ""DISK_INC""]:\n        lon, lat = float(tokens[1]), float(tokens[2])\n        return SkyCoord(lon, lat, frame=frame, unit=""deg"")\n    elif tokens[0] == ""HPX_PIXEL"":\n        nside_pix = int(tokens[2])\n        ipix_pix = int(tokens[3])\n        if tokens[1] == ""NESTED"":\n            nest_pix = True\n        elif tokens[1] == ""RING"":\n            nest_pix = False\n        else:\n            raise ValueError(f""Invalid ordering scheme: {tokens[1]!r}"")\n        theta, phi = hp.pix2ang(nside_pix, ipix_pix, nest_pix)\n        lat = np.degrees((np.pi / 2) - theta)\n        lon = np.degrees(phi)\n        return SkyCoord(lon, lat, frame=frame, unit=""deg"")\n    else:\n        raise ValueError(f""Invalid region type: {tokens[0]!r}"")\n\n\ndef get_hpxregion_size(region):\n    """"""Get the approximate size of region (in degrees) from a HEALPIX region string.\n    """"""\n    tokens = parse_hpxregion(region)\n    if tokens[0] in {""DISK"", ""DISK_INC""}:\n        return float(tokens[3])\n    elif tokens[0] == ""HPX_PIXEL"":\n        pix_size = get_pix_size_from_nside(int(tokens[2]))\n        return 2.0 * pix_size\n    else:\n        raise ValueError(f""Invalid region type: {tokens[0]!r}"")\n\n\ndef is_power2(n):\n    """"""Check if an integer is a power of 2.""""""\n    return (n > 0) & ((n & (n - 1)) == 0)\n\n\ndef nside_to_order(nside):\n    """"""Compute the ORDER given NSIDE.\n\n    Returns -1 for NSIDE values that are not a power of 2.\n    """"""\n    nside = np.array(nside, ndmin=1)\n    order = -1 * np.ones_like(nside)\n    m = is_power2(nside)\n    order[m] = np.log2(nside[m]).astype(int)\n    return order\n\n\ndef get_superpixels(idx, nside_subpix, nside_superpix, nest=True):\n    """"""Compute the indices of superpixels that contain a subpixel.\n\n    Parameters\n    ----------\n    idx : `~numpy.ndarray`\n        Array of HEALPix pixel indices for subpixels of NSIDE\n        ``nside_subpix``.\n    nside_subpix  : int or `~numpy.ndarray`\n        NSIDE of subpixel.\n    nside_superpix : int or `~numpy.ndarray`\n        NSIDE of superpixel.\n    nest : bool\n        If True, assume NESTED pixel ordering, otherwise, RING pixel\n        ordering.\n\n    Returns\n    -------\n    idx_super : `~numpy.ndarray`\n        Indices of HEALpix pixels of nside ``nside_superpix`` that\n        contain pixel indices ``idx`` of nside ``nside_subpix``.\n    """"""\n    import healpy as hp\n\n    idx = np.array(idx)\n    nside_superpix = np.asarray(nside_superpix)\n    nside_subpix = np.asarray(nside_subpix)\n\n    if not nest:\n        idx = hp.ring2nest(nside_subpix, idx)\n\n    if np.any(~is_power2(nside_superpix)) or np.any(~is_power2(nside_subpix)):\n        raise ValueError(""NSIDE must be a power of 2."")\n\n    ratio = np.array((nside_subpix // nside_superpix) ** 2, ndmin=1)\n    idx //= ratio\n\n    if not nest:\n        m = idx == INVALID_INDEX.int\n        idx[m] = 0\n        idx = hp.nest2ring(nside_superpix, idx)\n        idx[m] = INVALID_INDEX.int\n\n    return idx\n\n\ndef get_subpixels(idx, nside_superpix, nside_subpix, nest=True):\n    """"""Compute the indices of subpixels contained within superpixels.\n\n    This function returns an output array with one additional\n    dimension of size N for subpixel indices where N is the maximum\n    number of subpixels for any pair of ``nside_superpix`` and\n    ``nside_subpix``.  If the number of subpixels is less than N the\n    remaining subpixel indices will be set to -1.\n\n    Parameters\n    ----------\n    idx : `~numpy.ndarray`\n        Array of HEALPix pixel indices for superpixels of NSIDE\n        ``nside_superpix``.\n    nside_superpix : int or `~numpy.ndarray`\n        NSIDE of superpixel.\n    nside_subpix  : int or `~numpy.ndarray`\n        NSIDE of subpixel.\n    nest : bool\n        If True, assume NESTED pixel ordering, otherwise, RING pixel\n        ordering.\n\n    Returns\n    -------\n    idx_sub : `~numpy.ndarray`\n        Indices of HEALpix pixels of nside ``nside_subpix`` contained\n        within pixel indices ``idx`` of nside ``nside_superpix``.\n    """"""\n    import healpy as hp\n\n    if not nest:\n        idx = hp.ring2nest(nside_superpix, idx)\n\n    idx = np.asarray(idx)\n    nside_superpix = np.asarray(nside_superpix)\n    nside_subpix = np.asarray(nside_subpix)\n\n    if np.any(~is_power2(nside_superpix)) or np.any(~is_power2(nside_subpix)):\n        raise ValueError(""NSIDE must be a power of 2."")\n\n    # number of subpixels in each superpixel\n    npix = np.array((nside_subpix // nside_superpix) ** 2, ndmin=1)\n    x = np.arange(np.max(npix), dtype=int)\n    idx = idx * npix\n\n    if not np.all(npix[0] == npix):\n        x = np.broadcast_to(x, idx.shape + x.shape)\n        idx = idx[..., None] + x\n        idx[x >= np.broadcast_to(npix[..., None], x.shape)] = INVALID_INDEX.int\n    else:\n        idx = idx[..., None] + x\n\n    if not nest:\n        m = idx == INVALID_INDEX.int\n        idx[m] = 0\n        idx = hp.nest2ring(nside_subpix[..., None], idx)\n        idx[m] = INVALID_INDEX.int\n\n    return idx\n\n\nclass HpxGeom(Geom):\n    """"""Geometry class for HEALPIX maps.\n\n    This class performs mapping between partial-sky indices (pixel\n    number within a HEALPIX region) and all-sky indices (pixel number\n    within an all-sky HEALPIX map).  Multi-band HEALPIX geometries use\n    a global indexing scheme that assigns a unique pixel number based\n    on the all-sky index and band index.  In the single-band case the\n    global index is the same as the HEALPIX index.\n\n    By default the constructor will return an all-sky map.\n    Partial-sky maps can be defined with the ``region`` argument.\n\n    Parameters\n    ----------\n    nside : `~numpy.ndarray`\n        HEALPIX nside parameter, the total number of pixels is\n        12*nside*nside.  For multi-dimensional maps one can pass\n        either a single nside value or a vector of nside values\n        defining the pixel size for each image plane.  If nside is not\n        a scalar then its dimensionality should match that of the\n        non-spatial axes.\n    nest : bool\n        True -> \'NESTED\', False -> \'RING\' indexing scheme\n    frame : str\n        Coordinate system, ""icrs"" | ""galactic""\n    region : str or tuple\n        Spatial geometry for partial-sky maps.  If none the map will\n        encompass the whole sky.  String input will be parsed\n        according to HPX_REG header keyword conventions.  Tuple\n        input can be used to define an explicit list of pixels\n        encompassed by the geometry.\n    axes : list\n        Axes for non-spatial dimensions.\n    sparse : bool\n        If True defer allocation of partial- to all-sky index mapping\n        arrays.  This option is only compatible with partial-sky maps\n        with an analytic geometry (e.g. DISK).\n    """"""\n\n    is_hpx = True\n\n    def __init__(\n        self, nside, nest=True, frame=""icrs"", region=None, axes=None, sparse=False\n    ):\n\n        # FIXME: Figure out what to do when sparse=True\n        # FIXME: Require NSIDE to be power of two when nest=True\n\n        self._nside = np.array(nside, ndmin=1)\n        self._axes = make_axes(axes)\n        if self.nside.size > 1 and self.nside.shape != self.shape_axes:\n            raise ValueError(\n                ""Wrong dimensionality for nside. nside must ""\n                ""be a scalar or have a dimensionality consistent ""\n                ""with the axes argument.""\n            )\n\n        self._order = nside_to_order(self._nside)\n        self._nest = nest\n        self._frame = frame\n        self._maxpix = 12 * self._nside * self._nside\n        self._maxpix = self._maxpix * np.ones(self.shape_axes, dtype=int)\n        self._sparse = sparse\n\n        self._ipix = None\n        self._rmap = None\n        self._region = region\n        self._create_lookup(region)\n\n        if self._ipix is not None:\n            self._rmap = {}\n            for i, ipix in enumerate(self._ipix.flat):\n                self._rmap[ipix] = i\n\n        self._npix = self._npix * np.ones(self.shape_axes, dtype=int)\n        self._center_skydir = self._get_ref_dir()\n        lon, lat, frame = skycoord_to_lonlat(self._center_skydir)\n        self._center_coord = tuple(\n            [lon, lat]\n            + [ax.pix_to_coord((float(ax.nbin) - 1.0) / 2.0) for ax in self.axes]\n        )\n        self._center_pix = self.coord_to_pix(self._center_coord)\n\n    @property\n    def data_shape(self):\n        """"""Shape of the Numpy data array matching this geometry.""""""\n        npix_shape = [np.max(self.npix)]\n        ax_shape = [ax.nbin for ax in self.axes]\n        return tuple(npix_shape + ax_shape)[::-1]\n\n    def _create_lookup(self, region):\n        """"""Create local-to-global pixel lookup table.""""""\n        if isinstance(region, str):\n            ipix = [\n                self.get_index_list(nside, self._nest, region)\n                for nside in self._nside.flat\n            ]\n            self._ibnd = np.concatenate(\n                [i * np.ones_like(p, dtype=""int16"") for i, p in enumerate(ipix)]\n            )\n            self._ipix = [\n                ravel_hpx_index((p, i * np.ones_like(p)), np.ravel(self._maxpix))\n                for i, p in enumerate(ipix)\n            ]\n            self._region = region\n            self._indxschm = ""EXPLICIT""\n            self._npix = np.array([len(t) for t in self._ipix])\n            if self.nside.ndim > 1:\n                self._npix = self._npix.reshape(self.nside.shape)\n            self._ipix = np.concatenate(self._ipix)\n\n        elif isinstance(region, tuple):\n\n            region = [np.asarray(t) for t in region]\n            m = np.any(np.stack([t >= 0 for t in region]), axis=0)\n            region = [t[m] for t in region]\n\n            self._ipix = ravel_hpx_index(region, self._maxpix)\n            self._ipix = np.unique(self._ipix)\n            region = unravel_hpx_index(self._ipix, self._maxpix)\n            self._region = ""explicit""\n            self._indxschm = ""EXPLICIT""\n            if len(region) == 1:\n                self._npix = np.array([len(region[0])])\n            else:\n                self._npix = np.zeros(self.shape_axes, dtype=int)\n                idx = np.ravel_multi_index(region[1:], self.shape_axes)\n                cnt = np.unique(idx, return_counts=True)\n                self._npix.flat[cnt[0]] = cnt[1]\n\n        elif region is None:\n            self._region = None\n            self._indxschm = ""IMPLICIT""\n            self._npix = self._maxpix\n\n        else:\n            raise ValueError(f""Invalid region string: {region!r}"")\n\n    def local_to_global(self, idx_local):\n        """"""Compute a local index (partial-sky) from a global (all-sky) index.\n\n        Returns\n        -------\n        idx_global : tuple\n            A tuple of pixel index vectors with global HEALPIX pixel indices\n        """"""\n        if self._ipix is None:\n            return idx_local\n\n        if self.nside.size > 1:\n            idx = ravel_hpx_index(idx_local, self._npix)\n        else:\n            idx_tmp = tuple(\n                [idx_local[0]] + [np.zeros(t.shape, dtype=int) for t in idx_local[1:]]\n            )\n            idx = ravel_hpx_index(idx_tmp, self._npix)\n\n        idx_global = unravel_hpx_index(self._ipix[idx], self._maxpix)\n        return idx_global[:1] + tuple(idx_local[1:])\n\n    def global_to_local(self, idx_global, ravel=False):\n        """"""Compute global (all-sky) index from a local (partial-sky) index.\n\n        Parameters\n        ----------\n        idx_global : tuple\n            A tuple of pixel indices with global HEALPix pixel indices.\n        ravel : bool\n            Return a raveled index.\n\n        Returns\n        -------\n        idx_local : tuple\n            A tuple of pixel indices with local HEALPIX pixel indices.\n        """"""\n        if self.nside.size == 1:\n            idx = np.array(idx_global[0], ndmin=1)\n        else:\n            idx = ravel_hpx_index(idx_global, self._maxpix)\n\n        if self._rmap is not None:\n            retval = np.full(idx.size, -1, ""i"")\n            m = np.isin(idx.flat, self._ipix)\n            retval[m] = np.searchsorted(self._ipix, idx.flat[m])\n            retval = retval.reshape(idx.shape)\n        else:\n            retval = idx\n\n        if self.nside.size == 1:\n            idx_local = tuple([retval] + list(idx_global[1:]))\n        else:\n            idx_local = unravel_hpx_index(retval, self._npix)\n\n        m = np.any(np.stack([t == INVALID_INDEX.int for t in idx_local]), axis=0)\n        for i, t in enumerate(idx_local):\n            idx_local[i][m] = INVALID_INDEX.int\n\n        if not ravel:\n            return idx_local\n        else:\n            return ravel_hpx_index(idx_local, self.npix)\n\n    def __getitem__(self, idx_global):\n        """"""Implement global-to-local index lookup.\n\n        For all-sky maps it just returns the input array.  For\n        partial-sky maps it returns the local indices corresponding to\n        the indices in the input array, and -1 for those pixels that\n        are outside the selected region.  For multi-dimensional maps\n        with a different ``NSIDE`` in each band the global index is an\n        unrolled index for both HEALPIX pixel number and image slice.\n\n        Parameters\n        ----------\n        idx_global : `~numpy.ndarray`\n            An array of global (all-sky) pixel indices.  If this is a\n            tuple, list, or array of integers it will be interpreted\n            as a global (raveled) index.  If this argument is a tuple\n            of lists or arrays it will be interpreted as a list of\n            unraveled index vectors.\n\n        Returns\n        -------\n        idx_local : `~numpy.ndarray`\n            An array of local HEALPIX pixel indices.\n        """"""\n        # Convert to tuple representation\n        if (\n            isinstance(idx_global, int)\n            or (isinstance(idx_global, tuple) and isinstance(idx_global[0], int))\n            or isinstance(idx_global, np.ndarray)\n        ):\n            idx_global = unravel_hpx_index(np.array(idx_global, ndmin=1), self._maxpix)\n\n        return self.global_to_local(idx_global, ravel=True)\n\n    def coord_to_pix(self, coords):\n        import healpy as hp\n\n        coords = MapCoord.create(coords, frame=self.frame)\n        theta, phi = coords.theta, coords.phi\n\n        c = self.coord_to_tuple(coords)\n\n        if self.axes:\n            bins = []\n            idxs = []\n            for i, ax in enumerate(self.axes):\n                bins += [ax.coord_to_pix(c[i + 2])]\n                idxs += [ax.coord_to_idx(c[i + 2])]\n\n            # FIXME: Figure out how to handle coordinates out of\n            # bounds of non-spatial dimensions\n            if self.nside.size > 1:\n                nside = self.nside[tuple(idxs)]\n            else:\n                nside = self.nside\n\n            m = ~np.isfinite(theta)\n            theta[m] = 0.0\n            phi[m] = 0.0\n            pix = hp.ang2pix(nside, theta, phi, nest=self.nest)\n            pix = tuple([pix] + bins)\n            if np.any(m):\n                for p in pix:\n                    p[m] = INVALID_INDEX.int\n        else:\n            pix = (hp.ang2pix(self.nside, theta, phi, nest=self.nest),)\n\n        return pix\n\n    def pix_to_coord(self, pix):\n        import healpy as hp\n\n        if self.axes:\n            bins = []\n            vals = []\n            for i, ax in enumerate(self.axes):\n                bins += [pix[1 + i]]\n                vals += [ax.pix_to_coord(pix[1 + i])]\n\n            idxs = pix_tuple_to_idx(bins)\n\n            if self.nside.size > 1:\n                nside = self.nside[idxs]\n            else:\n                nside = self.nside\n\n            ipix = np.round(pix[0]).astype(int)\n            m = ipix == INVALID_INDEX.int\n            ipix[m] = 0\n            theta, phi = hp.pix2ang(nside, ipix, nest=self.nest)\n            coords = [np.degrees(phi), np.degrees(np.pi / 2.0 - theta)]\n            coords = tuple(coords + vals)\n            if np.any(m):\n                for c in coords:\n                    c[m] = INVALID_INDEX.float\n        else:\n            ipix = np.round(pix[0]).astype(int)\n            theta, phi = hp.pix2ang(self.nside, ipix, nest=self.nest)\n            coords = (np.degrees(phi), np.degrees(np.pi / 2.0 - theta))\n\n        return coords\n\n    def pix_to_idx(self, pix, clip=False):\n        # FIXME: Look for better method to clip HPX indices\n        # TODO: copy idx to avoid modifying input pix?\n        # pix_tuple_to_idx seems to always make a copy!?\n        idx = pix_tuple_to_idx(pix)\n        idx_local = self.global_to_local(idx)\n        for i, _ in enumerate(idx):\n\n            if clip:\n                if i > 0:\n                    np.clip(idx[i], 0, self.axes[i - 1].nbin - 1, out=idx[i])\n                else:\n                    np.clip(idx[i], 0, None, out=idx[i])\n            else:\n                if i > 0:\n                    mask = (idx[i] < 0) | (idx[i] >= self.axes[i - 1].nbin)\n                    np.putmask(idx[i], mask, -1)\n                else:\n                    mask = (idx_local[i] < 0) | (idx[i] < 0)\n                    np.putmask(idx[i], mask, -1)\n\n        return tuple(idx)\n\n    def to_slice(self, slices, drop_axes=True):\n        if len(slices) == 0 and self.ndim == 2:\n            return copy.deepcopy(self)\n\n        if len(slices) != self.ndim - 2:\n            raise ValueError()\n\n        nside = np.ones(self.shape_axes, dtype=int) * self.nside\n        nside = np.squeeze(nside[slices])\n\n        axes = [ax.slice(s) for ax, s in zip(self.axes, slices)]\n        if drop_axes:\n            axes = [ax for ax in axes if ax.nbin > 1]\n            slice_dims = [0] + [i + 1 for i, ax in enumerate(axes) if ax.nbin > 1]\n        else:\n            slice_dims = np.arange(self.ndim)\n\n        if self.region == ""explicit"":\n            idx = self.get_idx()\n            slices = (slice(None),) + slices\n            idx = [p[slices[::-1]] for p in idx]\n            idx = [p[p != INVALID_INDEX.int] for p in idx]\n            if drop_axes:\n                idx = [idx[i] for i in range(len(idx)) if i in slice_dims]\n            region = tuple(idx)\n        else:\n            region = self.region\n\n        return self.__class__(nside, self.nest, self.frame, region, axes)\n\n    @property\n    def axes(self):\n        """"""List of non-spatial axes.""""""\n        return self._axes\n\n    @property\n    def shape_axes(self):\n        """"""Shape of non-spatial axes.""""""\n        return tuple([ax.nbin for ax in self._axes])\n\n    @property\n    def ndim(self):\n        """"""Number of dimensions (int).""""""\n        return len(self._axes) + 2\n\n    @property\n    def ordering(self):\n        """"""HEALPix ordering (\'NESTED\' or \'RING\').""""""\n        return ""NESTED"" if self.nest else ""RING""\n\n    @property\n    def nside(self):\n        """"""NSIDE in each band.""""""\n        return self._nside\n\n    @property\n    def order(self):\n        """"""ORDER in each band (``NSIDE = 2 ** ORDER``).\n\n        Set to -1 for bands with NSIDE that is not a power of 2.\n        """"""\n        return self._order\n\n    @property\n    def nest(self):\n        """"""Is HEALPix order nested? (bool).""""""\n        return self._nest\n\n    @property\n    def npix(self):\n        """"""Number of pixels in each band.\n\n        For partial-sky geometries this can\n        be less than the number of pixels for the band NSIDE.\n        """"""\n        return self._npix\n\n    @property\n    def frame(self):\n        return self._frame\n\n    @property\n    def projection(self):\n        """"""Map projection.""""""\n        return ""HPX""\n\n    @property\n    def region(self):\n        """"""Region string.""""""\n        return self._region\n\n    @property\n    def is_allsky(self):\n        """"""Flag for all-sky maps.""""""\n        if self._region is None:\n            return True\n        else:\n            return False\n\n    @property\n    def is_regular(self):\n        """"""Flag identifying whether this geometry is regular in non-spatial dimensions.\n\n        False for multi-resolution or irregular geometries.\n        If true all image planes have the same pixel geometry.\n        """"""\n        if self.nside.size > 1 or self.region == ""explicit"":\n            return False\n        else:\n            return True\n\n    @property\n    def center_coord(self):\n        """"""Map coordinates of the center of the geometry (tuple).""""""\n        return self._center_coord\n\n    @property\n    def center_pix(self):\n        """"""Pixel coordinates of the center of the geometry (tuple).""""""\n        return self._center_pix\n\n    @property\n    def center_skydir(self):\n        """"""Sky coordinate of the center of the geometry.\n\n        Returns\n        -------\n        pix : `~astropy.coordinates.SkyCoord`\n        """"""\n        return self._center_skydir\n\n    @property\n    def ipix(self):\n        """"""HEALPIX pixel and band indices for every pixel in the map.""""""\n        return self.get_idx()\n\n    def to_ud_graded(self, order):\n        """"""Upgrade or downgrade the resolution to the given order.\n\n        This method does not preserve the geometry footprint.\n\n        Returns\n        -------\n        geom : `~HpxGeom`\n            A HEALPix geometry object.\n        """"""\n        if np.any(self.order < 0):\n            raise ValueError(""Upgrade and degrade only implemented for standard maps"")\n\n        axes = copy.deepcopy(self.axes)\n        return self.__class__(\n            2 ** order, self.nest, frame=self.frame, region=self.region, axes=axes\n        )\n\n    def to_swapped(self):\n        """"""Geometry copy with swapped ORDERING (NEST->RING or vice versa).\n\n        Returns\n        -------\n        geom : `~HpxGeom`\n            A HEALPix geometry object.\n        """"""\n        axes = copy.deepcopy(self.axes)\n        return self.__class__(\n            self.nside, not self.nest, frame=self.frame, region=self.region, axes=axes,\n        )\n\n    def to_image(self):\n        return self.__class__(\n            np.max(self.nside), self.nest, frame=self.frame, region=self.region\n        )\n\n    def to_cube(self, axes):\n        axes = copy.deepcopy(self.axes) + axes\n        return self.__class__(\n            np.max(self.nside),\n            self.nest,\n            frame=self.frame,\n            region=self.region,\n            axes=axes,\n        )\n\n    def _get_neighbors(self, idx):\n        import healpy as hp\n\n        nside = self._get_nside(idx)\n        idx_nb = (hp.get_all_neighbours(nside, idx[0], nest=self.nest),)\n        idx_nb += tuple([t[None, ...] * np.ones_like(idx_nb[0]) for t in idx[1:]])\n\n        return idx_nb\n\n    def pad(self, pad_width):\n        if self.is_allsky:\n            raise ValueError(""Cannot pad an all-sky map."")\n\n        idx = self.get_idx(flat=True)\n        idx_r = ravel_hpx_index(idx, self._maxpix)\n\n        # TODO: Pre-filter indices to find those close to the edge\n        idx_nb = self._get_neighbors(idx)\n        idx_nb = ravel_hpx_index(idx_nb, self._maxpix)\n\n        for _ in range(pad_width):\n            mask_edge = np.isin(idx_nb, idx_r, invert=True)\n            idx_edge = idx_nb[mask_edge]\n            idx_edge = np.unique(idx_edge)\n            idx_r = np.sort(np.concatenate((idx_r, idx_edge)))\n            idx_nb = unravel_hpx_index(idx_edge, self._maxpix)\n            idx_nb = self._get_neighbors(idx_nb)\n            idx_nb = ravel_hpx_index(idx_nb, self._maxpix)\n\n        idx = unravel_hpx_index(idx_r, self._maxpix)\n        return self.__class__(\n            self.nside.copy(),\n            self.nest,\n            frame=self.frame,\n            region=idx,\n            axes=copy.deepcopy(self.axes),\n        )\n\n    def crop(self, crop_width):\n        if self.is_allsky:\n            raise ValueError(""Cannot crop an all-sky map."")\n\n        idx = self.get_idx(flat=True)\n        idx_r = ravel_hpx_index(idx, self._maxpix)\n\n        # TODO: Pre-filter indices to find those close to the edge\n        idx_nb = self._get_neighbors(idx)\n        idx_nb = ravel_hpx_index(idx_nb, self._maxpix)\n\n        for _ in range(crop_width):\n            # Mask of pixels that have at least one neighbor not\n            # contained in the geometry\n            mask_edge = np.any(np.isin(idx_nb, idx_r, invert=True), axis=0)\n            idx_r = idx_r[~mask_edge]\n            idx_nb = idx_nb[:, ~mask_edge]\n\n        idx = unravel_hpx_index(idx_r, self._maxpix)\n        return self.__class__(\n            self.nside.copy(),\n            self.nest,\n            frame=self.frame,\n            region=idx,\n            axes=copy.deepcopy(self.axes),\n        )\n\n    def upsample(self, factor):\n        if not is_power2(factor):\n            raise ValueError(""Upsample factor must be a power of 2."")\n\n        if self.is_allsky:\n            return self.__class__(\n                self.nside * factor,\n                self.nest,\n                frame=self.frame,\n                region=self.region,\n                axes=copy.deepcopy(self.axes),\n            )\n\n        idx = list(self.get_idx(flat=True))\n        nside = self._get_nside(idx)\n\n        idx_new = get_subpixels(idx[0], nside, nside * factor, nest=self.nest)\n        for i in range(1, len(idx)):\n            idx[i] = idx[i][..., None] * np.ones(idx_new.shape, dtype=int)\n\n        idx[0] = idx_new\n        return self.__class__(\n            self.nside * factor,\n            self.nest,\n            frame=self.frame,\n            region=tuple(idx),\n            axes=copy.deepcopy(self.axes),\n        )\n\n    def downsample(self, factor):\n        if not is_power2(factor):\n            raise ValueError(""Downsample factor must be a power of 2."")\n\n        if self.is_allsky:\n            return self.__class__(\n                self.nside // factor,\n                self.nest,\n                frame=self.frame,\n                region=self.region,\n                axes=copy.deepcopy(self.axes),\n            )\n\n        idx = list(self.get_idx(flat=True))\n        nside = self._get_nside(idx)\n        idx_new = get_superpixels(idx[0], nside, nside // factor, nest=self.nest)\n        idx[0] = idx_new\n        return self.__class__(\n            self.nside // factor,\n            self.nest,\n            frame=self.frame,\n            region=tuple(idx),\n            axes=copy.deepcopy(self.axes),\n        )\n\n    @classmethod\n    def create(\n        cls,\n        nside=None,\n        binsz=None,\n        nest=True,\n        frame=""icrs"",\n        region=None,\n        axes=None,\n        skydir=None,\n        width=None,\n    ):\n        """"""Create an HpxGeom object.\n\n        Parameters\n        ----------\n        nside : int or `~numpy.ndarray`\n            HEALPix NSIDE parameter.  This parameter sets the size of\n            the spatial pixels in the map.\n        binsz : float or `~numpy.ndarray`\n            Approximate pixel size in degrees.  An NSIDE will be\n            chosen that correponds to a pixel size closest to this\n            value.  This option is superseded by nside.\n        nest : bool\n            True for HEALPIX ""NESTED"" indexing scheme, False for ""RING"" scheme\n        frame : {""icrs"", ""galactic""}, optional\n            Coordinate system, either Galactic (""galactic"") or Equatorial (""icrs"").\n        skydir : tuple or `~astropy.coordinates.SkyCoord`\n            Sky position of map center.  Can be either a SkyCoord\n            object or a tuple of longitude and latitude in deg in the\n            coordinate system of the map.\n        region  : str\n            HPX region string.  Allows for partial-sky maps.\n        width : float\n            Diameter of the map in degrees.  If set the map will\n            encompass all pixels within a circular region centered on\n            ``skydir``.\n        axes : list\n            List of axes for non-spatial dimensions.\n\n        Returns\n        -------\n        geom : `~HpxGeom`\n            A HEALPix geometry object.\n\n        Examples\n        --------\n        >>> from gammapy.maps import HpxGeom, MapAxis\n        >>> axis = MapAxis.from_bounds(0,1,2)\n        >>> geom = HpxGeom.create(nside=16)\n        >>> geom = HpxGeom.create(binsz=0.1, width=10.0)\n        >>> geom = HpxGeom.create(nside=64, width=10.0, axes=[axis])\n        >>> geom = HpxGeom.create(nside=[32,64], width=10.0, axes=[axis])\n        """"""\n        if nside is None and binsz is None:\n            raise ValueError(""Either nside or binsz must be defined."")\n\n        if nside is None and binsz is not None:\n            nside = get_nside_from_pix_size(binsz)\n\n        if skydir is None:\n            lon, lat = (0.0, 0.0)\n        elif isinstance(skydir, tuple):\n            lon, lat = skydir\n        elif isinstance(skydir, SkyCoord):\n            lon, lat, frame = skycoord_to_lonlat(skydir, frame=frame)\n        else:\n            raise ValueError(f""Invalid type for skydir: {type(skydir)!r}"")\n\n        if region is None and width is not None:\n            region = f""DISK({lon},{lat},{width/2})""\n\n        return cls(nside, nest=nest, frame=frame, region=region, axes=axes)\n\n    @staticmethod\n    def identify_hpx_convention(header):\n        """"""Identify the convention used to write this file.""""""\n        # Hopefully the file contains the HPX_CONV keyword specifying\n        # the convention used\n        if ""HPX_CONV"" in header:\n            return header[""HPX_CONV""].lower()\n\n        # Try based on the EXTNAME keyword\n        hduname = header.get(""EXTNAME"", None)\n        if hduname == ""HPXEXPOSURES"":\n            return ""fgst-bexpcube""\n        elif hduname == ""SKYMAP2"":\n            if ""COORDTYPE"" in header.keys():\n                return ""galprop""\n            else:\n                return ""galprop2""\n\n        # Check the name of the first column\n        colname = header[""TTYPE1""]\n        if colname == ""PIX"":\n            colname = header[""TTYPE2""]\n\n        if colname == ""KEY"":\n            return ""fgst-srcmap-sparse""\n        elif colname == ""ENERGY1"":\n            return ""fgst-template""\n        elif colname == ""COSBINS"":\n            return ""fgst-ltcube""\n        elif colname == ""Bin0"":\n            return ""galprop""\n        elif colname == ""CHANNEL1"" or colname == ""CHANNEL0"":\n            if hduname == ""SKYMAP"":\n                return ""fgst-ccube""\n            else:\n                return ""fgst-srcmap""\n        else:\n            raise ValueError(""Could not identify HEALPIX convention"")\n\n    @classmethod\n    def from_header(cls, header, hdu_bands=None, pix=None):\n        """"""Create an HPX object from a FITS header.\n\n        Parameters\n        ----------\n        header : `~astropy.io.fits.Header`\n            The FITS header\n        hdu_bands : `~astropy.io.fits.BinTableHDU`\n            The BANDS table HDU.\n        pix : tuple\n            List of pixel index vectors defining the pixels\n            encompassed by the geometry.  For EXPLICIT geometries with\n            HPX_REG undefined this tuple defines the geometry.\n\n        Returns\n        -------\n        hpx : `~HpxGeom`\n            HEALPix geometry.\n        """"""\n        convname = HpxGeom.identify_hpx_convention(header)\n        conv = HPX_FITS_CONVENTIONS[convname]\n\n        axes = find_and_read_bands(hdu_bands)\n        shape = [ax.nbin for ax in axes]\n\n        if header[""PIXTYPE""] != ""HEALPIX"":\n            raise ValueError(\n                f""Invalid header PIXTYPE: {header[\'PIXTYPE\']} (must be HEALPIX)""\n            )\n\n        if header[""ORDERING""] == ""RING"":\n            nest = False\n        elif header[""ORDERING""] == ""NESTED"":\n            nest = True\n        else:\n            raise ValueError(\n                f""Invalid header ORDERING: {header[\'ORDERING\']} (must be RING or NESTED)""\n            )\n\n        if hdu_bands is not None and ""NSIDE"" in hdu_bands.columns.names:\n            nside = hdu_bands.data.field(""NSIDE"").reshape(shape).astype(int)\n        elif ""NSIDE"" in header:\n            nside = header[""NSIDE""]\n        elif ""ORDER"" in header:\n            nside = 2 ** header[""ORDER""]\n        else:\n            raise ValueError(""Failed to extract NSIDE or ORDER."")\n\n        try:\n            frame = coordsys_to_frame(header[conv.frame])\n        except KeyError:\n            frame = header.get(""COORDSYS"", ""icrs"")\n\n        try:\n            region = header[""HPX_REG""]\n        except KeyError:\n            try:\n                region = header[""HPXREGION""]\n            except KeyError:\n                region = None\n\n        return cls(nside, nest, frame=frame, region=region, axes=axes)\n\n    @classmethod\n    def from_hdu(cls, hdu, hdu_bands=None):\n        """"""Create an HPX object from a BinTable HDU.\n\n        Parameters\n        ----------\n        hdu : `~astropy.io.fits.BinTableHDU`\n            The FITS HDU\n        hdu_bands : `~astropy.io.fits.BinTableHDU`\n            The BANDS table HDU\n\n        Returns\n        -------\n        hpx : `~HpxGeom`\n            HEALPix geometry.\n        """"""\n        # FIXME: Need correct handling of IMPLICIT and EXPLICIT maps\n\n        # if HPX region is not defined then geometry is defined by\n        # the set of all pixels in the table\n        if ""HPX_REG"" not in hdu.header:\n            pix = (hdu.data.field(""PIX""), hdu.data.field(""CHANNEL""))\n        else:\n            pix = None\n\n        return cls.from_header(hdu.header, hdu_bands=hdu_bands, pix=pix)\n\n    def make_header(self, conv=""gadf"", **kwargs):\n        """"""Build and return FITS header for this HEALPIX map.""""""\n        header = fits.Header()\n        conv = kwargs.get(""conv"", HPX_FITS_CONVENTIONS[conv])\n\n        # FIXME: For some sparse maps we may want to allow EXPLICIT\n        # with an empty region string\n        indxschm = kwargs.get(""indxschm"", None)\n\n        if indxschm is None:\n            if self._region is None:\n                indxschm = ""IMPLICIT""\n            elif self.is_regular == 1:\n                indxschm = ""EXPLICIT""\n            else:\n                indxschm = ""LOCAL""\n\n        if ""FGST"" in conv.convname.upper():\n            header[""TELESCOP""] = ""GLAST""\n            header[""INSTRUME""] = ""LAT""\n\n        header[conv.frame] = frame_to_coordsys(self.frame)\n        header[""PIXTYPE""] = ""HEALPIX""\n        header[""ORDERING""] = self.ordering\n        header[""INDXSCHM""] = indxschm\n        header[""ORDER""] = np.max(self._order)\n        header[""NSIDE""] = np.max(self._nside)\n        header[""FIRSTPIX""] = 0\n        header[""LASTPIX""] = np.max(self._maxpix) - 1\n        header[""HPX_CONV""] = conv.convname.upper()\n\n        if self.frame == ""icrs"":\n            header[""EQUINOX""] = (2000.0, ""Equinox of RA & DEC specifications"")\n\n        if self.region:\n            header[""HPX_REG""] = self._region\n\n        return header\n\n    def _make_bands_cols(self):\n        cols = []\n        if self.nside.size > 1:\n            cols += [fits.Column(""NSIDE"", ""I"", array=np.ravel(self.nside))]\n        return cols\n\n    @staticmethod\n    def get_index_list(nside, nest, region):\n        """"""Get list of pixels indices for all the pixels in a region.\n\n        Parameters\n        ----------\n        nside : int\n            HEALPIX nside parameter\n        nest : bool\n            True for \'NESTED\', False = \'RING\'\n        region : str\n            HEALPIX region string\n\n        Returns\n        -------\n        ilist : `~numpy.ndarray`\n            List of pixel indices.\n        """"""\n        import healpy as hp\n\n        # TODO: this should return something more friendly than a tuple\n        # e.g. a namedtuple or a dict\n        tokens = parse_hpxregion(region)\n\n        reg_type = tokens[0]\n        if reg_type == ""DISK"":\n            lon, lat = float(tokens[1]), float(tokens[2])\n            radius = np.radians(float(tokens[3]))\n            vec = coords_to_vec(lon, lat)[0]\n            ilist = hp.query_disc(nside, vec, radius, inclusive=False, nest=nest)\n        elif reg_type == ""DISK_INC"":\n            lon, lat = float(tokens[1]), float(tokens[2])\n            radius = np.radians(float(tokens[3]))\n            vec = coords_to_vec(lon, lat)[0]\n            fact = int(tokens[4])\n            ilist = hp.query_disc(\n                nside, vec, radius, inclusive=True, nest=nest, fact=fact\n            )\n        elif reg_type == ""HPX_PIXEL"":\n            nside_pix = int(tokens[2])\n            if tokens[1] == ""NESTED"":\n                ipix_ring = hp.nest2ring(nside_pix, int(tokens[3]))\n            elif tokens[1] == ""RING"":\n                ipix_ring = int(tokens[3])\n            else:\n                raise ValueError(f""Invalid ordering scheme: {tokens[1]!r}"")\n            ilist = match_hpx_pix(nside, nest, nside_pix, ipix_ring)\n        else:\n            raise ValueError(f""Invalid region type: {reg_type!r}"")\n\n        return ilist\n\n    def _get_ref_dir(self):\n        """"""Compute the reference direction for this geometry.""""""\n        import healpy as hp\n\n        if self.region == ""explicit"":\n            idx = unravel_hpx_index(self._ipix, self._maxpix)\n            nside = self._get_nside(idx)\n            vec = hp.pix2vec(nside, idx[0], nest=self.nest)\n            vec = np.array([np.mean(t) for t in vec])\n            lonlat = hp.vec2ang(vec, lonlat=True)\n            return SkyCoord(lonlat[0], lonlat[1], frame=self.frame, unit=""deg"")\n\n        return get_hpxregion_dir(self.region, self.frame)\n\n    def _get_region_size(self):\n        import healpy as hp\n\n        if self.region is None:\n            return 180.0\n        if self.region == ""explicit"":\n            idx = unravel_hpx_index(self._ipix, self._maxpix)\n            nside = self._get_nside(idx)\n            ang = hp.pix2ang(nside, idx[0], nest=self.nest, lonlat=True)\n            dirs = SkyCoord(ang[0], ang[1], unit=""deg"", frame=self.frame)\n            return np.max(dirs.separation(self.center_skydir).deg)\n\n        return get_hpxregion_size(self.region)\n\n    def _get_nside(self, idx):\n        if self.nside.size > 1:\n            return self.nside[tuple(idx[1:])]\n        else:\n            return self.nside\n\n    def make_wcs(self, proj=""AIT"", oversample=2, drop_axes=True, width_pix=None):\n        """"""Make a WCS projection appropriate for this HPX pixelization.\n\n        Parameters\n        ----------\n        drop_axes : bool\n            Drop non-spatial axes from the\n            HEALPIX geometry.  If False then all dimensions of the\n            HEALPIX geometry will be copied to the WCS geometry.\n        proj : str\n            Projection type of WCS geometry.\n        oversample : float\n            Oversampling factor for WCS map. This will be the\n            approximate ratio of the width of a HPX pixel to a WCS\n            pixel. If this parameter is None then the width will be\n            set from ``width_pix``.\n        width_pix : int\n            Width of the WCS geometry in pixels.  The pixel size will\n            be set to the number of pixels satisfying ``oversample``\n            or ``width_pix`` whichever is smaller.  If this parameter\n            is None then the width will be set from ``oversample``.\n\n        Returns\n        -------\n        wcs : `~gammapy.maps.WcsGeom`\n            WCS geometry\n        """"""\n        pix_size = get_pix_size_from_nside(self.nside)\n        binsz = np.min(pix_size) / oversample\n        width = 2.0 * self._get_region_size() + np.max(pix_size)\n\n        if width_pix is not None and int(width / binsz) > width_pix:\n            binsz = width / width_pix\n\n        if width > 90.0:\n            width = min(360.0, width), min(180.0, width)\n\n        if drop_axes:\n            axes = None\n        else:\n            axes = copy.deepcopy(self.axes)\n\n        return WcsGeom.create(\n            width=width,\n            binsz=binsz,\n            frame=self.frame,\n            axes=axes,\n            skydir=self.center_skydir,\n            proj=proj,\n        )\n\n    def get_idx(self, idx=None, local=False, flat=False):\n        if idx is not None and np.any(np.array(idx) >= np.array(self.shape_axes)):\n            raise ValueError(f""Image index out of range: {idx!r}"")\n\n        # Regular all- and partial-sky maps\n        if self.is_regular:\n\n            pix = [np.arange(np.max(self._npix))]\n            if idx is None:\n                pix += [np.arange(ax.nbin, dtype=int) for ax in self.axes]\n            else:\n                pix += [t for t in idx]\n            pix = np.meshgrid(*pix[::-1], indexing=""ij"", sparse=False)[::-1]\n            pix = self.local_to_global(pix)\n\n        # Non-regular all-sky\n        elif self.is_allsky and not self.is_regular:\n\n            shape = (np.max(self.npix),)\n            if idx is None:\n                shape = shape + self.shape_axes\n            else:\n                shape = shape + (1,) * len(self.axes)\n            pix = [np.full(shape, -1, dtype=int) for i in range(1 + len(self.axes))]\n            for idx_img in np.ndindex(self.shape_axes):\n\n                if idx is not None and idx_img != idx:\n                    continue\n\n                npix = self._npix[idx_img]\n                if idx is None:\n                    s_img = (slice(0, npix),) + idx_img\n                else:\n                    s_img = (slice(0, npix),) + (0,) * len(self.axes)\n\n                pix[0][s_img] = np.arange(self._npix[idx_img])\n                for j in range(len(self.axes)):\n                    pix[j + 1][s_img] = idx_img[j]\n            pix = [p.T for p in pix]\n\n        # Explicit pixel indices\n        else:\n\n            if idx is not None:\n                npix_sum = np.concatenate(([0], np.cumsum(self._npix)))\n                idx_ravel = np.ravel_multi_index(idx, self.shape_axes)\n                s = slice(npix_sum[idx_ravel], npix_sum[idx_ravel + 1])\n            else:\n                s = slice(None)\n            pix_flat = unravel_hpx_index(self._ipix[s], self._maxpix)\n\n            shape = (np.max(self.npix),)\n            if idx is None:\n                shape = shape + self.shape_axes\n            else:\n                shape = shape + (1,) * len(self.axes)\n            pix = [np.full(shape, -1, dtype=int) for _ in range(1 + len(self.axes))]\n\n            for idx_img in np.ndindex(self.shape_axes):\n\n                if idx is not None and idx_img != idx:\n                    continue\n\n                npix = int(self._npix[idx_img])\n                if idx is None:\n                    s_img = (slice(0, npix),) + idx_img\n                else:\n                    s_img = (slice(0, npix),) + (0,) * len(self.axes)\n\n                if self.axes:\n                    m = np.all(\n                        np.stack([pix_flat[i + 1] == t for i, t in enumerate(idx_img)]),\n                        axis=0,\n                    )\n                    pix[0][s_img] = pix_flat[0][m]\n                else:\n                    pix[0][s_img] = pix_flat[0]\n\n                for j in range(len(self.axes)):\n                    pix[j + 1][s_img] = idx_img[j]\n\n            pix = [p.T for p in pix]\n\n        if local:\n            pix = self.global_to_local(pix)\n\n        if flat:\n            pix = tuple([p[p != INVALID_INDEX.int] for p in pix])\n\n        return pix\n\n    def get_coord(self, idx=None, flat=False):\n        pix = self.get_idx(idx=idx, flat=flat)\n        coords = self.pix_to_coord(pix)\n        cdict = {""lon"": coords[0], ""lat"": coords[1]}\n\n        for i, axis in enumerate(self.axes):\n            cdict[axis.name] = coords[i + 2]\n\n        return MapCoord.create(cdict, frame=self.frame)\n\n    def contains(self, coords):\n        idx = self.coord_to_idx(coords)\n        return np.all(np.stack([t != INVALID_INDEX.int for t in idx]), axis=0)\n\n    def solid_angle(self):\n        """"""Solid angle array (`~astropy.units.Quantity` in ``sr``).\n\n        The array has the same dimensionality as ``map.nside``\n        since all pixels have the same solid angle.\n        """"""\n        import healpy as hp\n\n        return Quantity(hp.nside2pixarea(self.nside), ""sr"")\n\n    def __repr__(self):\n        axes = [""skycoord""] + [_.name for _ in self.axes]\n        lon, lat = self.center_skydir.data.lon.deg, self.center_skydir.data.lat.deg\n\n        return (\n            f""{self.__class__.__name__}\\n\\n""\n            f""\\taxes       : {axes}\\n""\n            f""\\tshape      : {self.data_shape[::-1]}\\n""\n            f""\\tndim       : {self.ndim}\\n""\n            f""\\tnside      : {self.nside[0]}\\n""\n            f""\\tnested     : {self.nest}\\n""\n            f""\\tframe   : {self.frame}\\n""\n            f""\\tprojection : {self.projection}\\n""\n            f""\\tcenter     : {lon:.1f} deg, {lat:.1f} deg\\n""\n        )\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n\n        if self._sparse or other._sparse:\n            return NotImplemented\n        if self.is_allsky and other.is_allsky is False:\n            return NotImplemented\n\n        # check overall shape and axes compatibility\n        if self.data_shape != other.data_shape:\n            return False\n\n        for axis, otheraxis in zip(self.axes, other.axes):\n            if axis != otheraxis:\n                return False\n\n        return (\n            self.nside == other.nside\n            and self.frame == other.frame\n            and self.order == other.order\n            and self.nest == other.nest\n        )\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n\nclass HpxToWcsMapping:\n    """"""Stores the indices need to convert from HEALPIX to WCS.\n\n    Parameters\n    ----------\n    hpx : `~HpxGeom`\n        HEALPix geometry object.\n    wcs : `~gammapy.maps.WcsGeom`\n        WCS geometry object.\n    """"""\n\n    def __init__(self, hpx, wcs, ipix, mult_val, npix):\n        self._hpx = hpx\n        self._wcs = wcs\n        self._ipix = ipix\n        self._mult_val = mult_val\n        self._npix = npix\n        self._lmap = self._hpx[self._ipix]\n        self._valid = self._lmap >= 0\n\n    @property\n    def hpx(self):\n        """"""HEALPIX projection.""""""\n        return self._hpx\n\n    @property\n    def wcs(self):\n        """"""WCS projection.""""""\n        return self._wcs\n\n    @property\n    def ipix(self):\n        """"""An array(nx,ny) of the global HEALPIX pixel indices for each WCS pixel.""""""\n        return self._ipix\n\n    @property\n    def mult_val(self):\n        """"""An array(nx,ny) of 1/number of WCS pixels pointing at each HEALPIX pixel.""""""\n        return self._mult_val\n\n    @property\n    def npix(self):\n        """"""A tuple(nx,ny) of the shape of the WCS grid.""""""\n        return self._npix\n\n    @property\n    def lmap(self):\n        """"""Array ``(nx, ny)`` mapping local HEALPIX pixel indices for each WCS pixel.""""""\n        return self._lmap\n\n    @property\n    def valid(self):\n        """"""Array ``(nx, ny)`` of bool: which WCS pixel in inside the HEALPIX region.""""""\n        return self._valid\n\n    @classmethod\n    def create(cls, hpx, wcs):\n        """"""Create HEALPix to WCS geometry pixel mapping.\n\n        Parameters\n        ----------\n        hpx : `~HpxGeom`\n            HEALPix geometry object.\n        wcs : `~gammapy.maps.WcsGeom`\n            WCS geometry object.\n\n        Returns\n        -------\n        hpx2wcs : `~HpxToWcsMapping`\n\n        """"""\n        ipix, mult_val, npix = make_hpx_to_wcs_mapping(hpx, wcs)\n        return cls(hpx, wcs, ipix, mult_val, npix)\n\n    def fill_wcs_map_from_hpx_data(\n        self, hpx_data, wcs_data, normalize=True, fill_nan=True\n    ):\n        """"""Fill the WCS map from the hpx data using the pre-calculated mappings.\n\n        Parameters\n        ----------\n        hpx_data : `~numpy.ndarray`\n            The input HEALPIX data\n        wcs_data : `~numpy.ndarray`\n            The data array being filled\n        normalize : bool\n            True -> preserve integral by splitting HEALPIX values between bins\n        fill_nan : bool\n            Fill pixels outside the HPX geometry with NaN.\n        """"""\n        # FIXME: Do we want to flatten mapping arrays?\n\n        shape = tuple([t.flat[0] for t in self._npix])\n        if self._valid.ndim != 1:\n            shape = hpx_data.shape[:-1] + shape\n\n        valid = np.where(self._valid.reshape(shape))\n        lmap = self._lmap[self._valid]\n        mult_val = self._mult_val[self._valid]\n\n        wcs_slice = [slice(None) for _ in range(wcs_data.ndim - 2)]\n        wcs_slice = tuple(wcs_slice + list(valid)[::-1][:2])\n\n        hpx_slice = [slice(None) for _ in range(wcs_data.ndim - 2)]\n        hpx_slice = tuple(hpx_slice + [lmap])\n\n        if normalize:\n            wcs_data[wcs_slice] = mult_val * hpx_data[hpx_slice]\n        else:\n            wcs_data[wcs_slice] = hpx_data[hpx_slice]\n\n        if fill_nan:\n            valid = np.swapaxes(self._valid.reshape(shape), -1, -2)\n            valid = valid * np.ones_like(wcs_data, dtype=bool)\n            wcs_data[~valid] = np.nan\n\n        return wcs_data\n'"
gammapy/maps/hpxmap.py,1,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport abc\nimport json\nimport numpy as np\nfrom astropy.io import fits\nfrom .base import Map\nfrom .hpx import HpxConv, HpxGeom\nfrom .utils import find_bands_hdu, find_bintable_hdu\n\n__all__ = [""HpxMap""]\n\n\nclass HpxMap(Map):\n    """"""Base class for HEALPIX map classes.\n\n    Parameters\n    ----------\n    geom : `~gammapy.maps.HpxGeom`\n        HEALPix geometry object.\n    data : `~numpy.ndarray`\n        Data array.\n    meta : `dict`\n        Dictionary to store meta data.\n    unit : `~astropy.units.Unit`\n        The map unit\n    """"""\n\n    def __init__(self, geom, data, meta=None, unit=""""):\n        super().__init__(geom, data, meta, unit)\n        self._wcs2d = None\n        self._hpx2wcs = None\n\n    @classmethod\n    def create(\n        cls,\n        nside=None,\n        binsz=None,\n        nest=True,\n        map_type=""hpx"",\n        frame=""icrs"",\n        data=None,\n        skydir=None,\n        width=None,\n        dtype=""float32"",\n        region=None,\n        axes=None,\n        meta=None,\n        unit="""",\n    ):\n        """"""Factory method to create an empty HEALPix map.\n\n        Parameters\n        ----------\n        nside : int or `~numpy.ndarray`\n            HEALPix NSIDE parameter.  This parameter sets the size of\n            the spatial pixels in the map.\n        binsz : float or `~numpy.ndarray`\n            Approximate pixel size in degrees.  An NSIDE will be\n            chosen that correponds to a pixel size closest to this\n            value.  This option is superseded by nside.\n        nest : bool\n            True for HEALPix ""NESTED"" indexing scheme, False for ""RING"" scheme.\n        frame : {""icrs"", ""galactic""}, optional\n            Coordinate system, either Galactic (""galactic"") or Equatorial (""icrs"").\n        skydir : tuple or `~astropy.coordinates.SkyCoord`\n            Sky position of map center.  Can be either a SkyCoord\n            object or a tuple of longitude and latitude in deg in the\n            coordinate system of the map.\n        map_type : {\'hpx\', \'hpx-sparse\'}\n            Map type.  Selects the class that will be used to\n            instantiate the map.\n        width : float\n            Diameter of the map in degrees.  If None then an all-sky\n            geometry will be created.\n        axes : list\n            List of `~MapAxis` objects for each non-spatial dimension.\n        meta : `dict`\n            Dictionary to store meta data.\n        unit : str or `~astropy.units.Unit`\n            The map unit\n\n        Returns\n        -------\n        map : `~HpxMap`\n            A HPX map object.\n        """"""\n        from .hpxnd import HpxNDMap\n\n        hpx = HpxGeom.create(\n            nside=nside,\n            binsz=binsz,\n            nest=nest,\n            frame=frame,\n            region=region,\n            axes=axes,\n            skydir=skydir,\n            width=width,\n        )\n        if cls.__name__ == ""HpxNDMap"":\n            return HpxNDMap(hpx, dtype=dtype, meta=meta, unit=unit)\n        elif map_type == ""hpx"":\n            return HpxNDMap(hpx, dtype=dtype, meta=meta, unit=unit)\n        else:\n            raise ValueError(f""Unrecognized map type: {map_type!r}"")\n\n    @classmethod\n    def from_hdulist(cls, hdu_list, hdu=None, hdu_bands=None):\n        """"""Make a HpxMap object from a FITS HDUList.\n\n        Parameters\n        ----------\n        hdu_list :  `~astropy.io.fits.HDUList`\n            HDU list containing HDUs for map data and bands.\n        hdu : str\n            Name or index of the HDU with the map data.  If None then\n            the method will try to load map data from the first\n            BinTableHDU in the file.\n        hdu_bands : str\n            Name or index of the HDU with the BANDS table.\n\n        Returns\n        -------\n        hpx_map : `HpxMap`\n            Map object\n        """"""\n        if hdu is None:\n            hdu_out = find_bintable_hdu(hdu_list)\n        else:\n            hdu_out = hdu_list[hdu]\n\n        if hdu_bands is None:\n            hdu_bands = find_bands_hdu(hdu_list, hdu_out)\n\n        hdu_bands_out = None\n        if hdu_bands is not None:\n            hdu_bands_out = hdu_list[hdu_bands]\n\n        return cls.from_hdu(hdu_out, hdu_bands_out)\n\n    def to_hdulist(self, hdu=""SKYMAP"", hdu_bands=None, sparse=False, conv=""gadf""):\n        """"""Convert to `~astropy.io.fits.HDUList`.\n\n        Parameters\n        ----------\n        hdu : str\n            The HDU extension name.\n        hdu_bands : str\n            The HDU extension name for BANDS table.\n        sparse : bool\n            Set INDXSCHM to SPARSE and sparsify the map by only\n            writing pixels with non-zero amplitude.\n        conv : {\'fgst-ccube\',\'fgst-template\',\'gadf\',None}, optional\n            FITS format convention.  If None this will be set to the\n            default convention of the map.\n\n        Returns\n        -------\n        hdu_list : `~astropy.io.fits.HDUList`\n        """"""\n        if self.geom.axes:\n            hdu_bands_out = self.geom.make_bands_hdu(\n                hdu=hdu_bands, hdu_skymap=hdu, conv=conv\n            )\n            hdu_bands = hdu_bands_out.name\n        else:\n            hdu_bands_out = None\n            hdu_bands = None\n\n        hdu_out = self.make_hdu(hdu=hdu, hdu_bands=hdu_bands, sparse=sparse, conv=conv)\n        hdu_out.header[""META""] = json.dumps(self.meta)\n        hdu_out.header[""BUNIT""] = self.unit.to_string(""fits"")\n\n        hdu_list = fits.HDUList([fits.PrimaryHDU(), hdu_out])\n\n        if self.geom.axes:\n            hdu_list.append(hdu_bands_out)\n\n        return hdu_list\n\n    @abc.abstractmethod\n    def to_wcs(\n        self,\n        sum_bands=False,\n        normalize=True,\n        proj=""AIT"",\n        oversample=2,\n        width_pix=None,\n        hpx2wcs=None,\n    ):\n        """"""Make a WCS object and convert HEALPIX data into WCS projection.\n\n        Parameters\n        ----------\n        sum_bands : bool\n            Sum over non-spatial axes before reprojecting.  If False\n            then the WCS map will have the same dimensionality as the\n            HEALPix one.\n        normalize : bool\n            Preserve integral by splitting HEALPIX values between bins?\n        proj : str\n            WCS-projection\n        oversample : float\n            Oversampling factor for WCS map. This will be the\n            approximate ratio of the width of a HPX pixel to a WCS\n            pixel. If this parameter is None then the width will be\n            set from ``width_pix``.\n        width_pix : int\n            Width of the WCS geometry in pixels.  The pixel size will\n            be set to the number of pixels satisfying ``oversample``\n            or ``width_pix`` whichever is smaller.  If this parameter\n            is None then the width will be set from ``oversample``.\n        hpx2wcs : `~HpxToWcsMapping`\n            Set the HPX to WCS mapping object that will be used to\n            generate the WCS map.  If none then a new mapping will be\n            generated based on ``proj`` and ``oversample`` arguments.\n\n        Returns\n        -------\n        map_out : `~gammapy.maps.WcsMap`\n            WCS map object.\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def to_swapped(self):\n        """"""Return a new map with the opposite scheme (ring or nested).\n\n        Returns\n        -------\n        map : `~HpxMap`\n            Map object.\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def to_ud_graded(self, nside, preserve_counts=False):\n        """"""Upgrade or downgrade the resolution of the map to the chosen nside.\n\n        Parameters\n        ----------\n        nside : int\n            NSIDE parameter of the new map.\n        preserve_counts : bool\n            Choose whether to preserve counts (total amplitude) or\n            intensity (amplitude per unit solid angle).\n\n        Returns\n        -------\n        map : `~HpxMap`\n            Map object.\n        """"""\n        pass\n\n    def make_hdu(self, hdu=None, hdu_bands=None, sparse=False, conv=None):\n        """"""Make a FITS HDU with input data.\n\n        Parameters\n        ----------\n        hdu : str\n            The HDU extension name.\n        hdu_bands : str\n            The HDU extension name for BANDS table.\n        sparse : bool\n            Set INDXSCHM to SPARSE and sparsify the map by only\n            writing pixels with non-zero amplitude.\n        conv : {\'fgst-ccube\', \'fgst-template\', \'gadf\', None}, optional\n            FITS format convention.  If None this will be set to the\n            default convention of the map.\n\n        Returns\n        -------\n        hdu_out : `~astropy.io.fits.BinTableHDU` or `~astropy.io.fits.ImageHDU`\n            Output HDU containing map data.\n        """"""\n        hpxconv = HpxConv.create(conv)\n        hduname = hpxconv.hduname if hdu is None else hdu\n        hduname_bands = hpxconv.bands_hdu if hdu_bands is None else hdu_bands\n\n        header = self.geom.make_header(conv=conv)\n\n        if self.geom.axes:\n            header[""BANDSHDU""] = hduname_bands\n\n        if sparse:\n            header[""INDXSCHM""] = ""SPARSE""\n\n        cols = []\n        if header[""INDXSCHM""] == ""EXPLICIT"":\n            array = self.geom._ipix\n            cols.append(fits.Column(""PIX"", ""J"", array=array))\n        elif header[""INDXSCHM""] == ""LOCAL"":\n            array = np.arange(self.data.shape[-1])\n            cols.append(fits.Column(""PIX"", ""J"", array=array))\n\n        cols += self._make_cols(header, hpxconv)\n        return fits.BinTableHDU.from_columns(cols, header=header, name=hduname)\n'"
gammapy/maps/hpxnd.py,41,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom astropy.io import fits\nfrom astropy.units import Quantity\nfrom gammapy.utils.units import unit_from_fits_image_hdu\nfrom .geom import MapCoord, pix_tuple_to_idx\nfrom .hpx import HPX_FITS_CONVENTIONS, HpxGeom, HpxToWcsMapping, nside_to_order\nfrom .hpxmap import HpxMap\nfrom .utils import INVALID_INDEX, interp_to_order\n\n__all__ = [""HpxNDMap""]\n\n\nclass HpxNDMap(HpxMap):\n    """"""HEALPix map with any number of non-spatial dimensions.\n\n    This class uses a N+1D numpy array to represent the sequence of\n    HEALPix image planes.  Following the convention of WCS-based maps\n    this class uses a column-wise ordering for the data array with the\n    spatial dimension being tied to the last index of the array.\n\n    Parameters\n    ----------\n    geom : `~gammapy.maps.HpxGeom`\n        HEALPIX geometry object.\n    data : `~numpy.ndarray`\n        HEALPIX data array.\n        If none then an empty array will be allocated.\n    meta : `dict`\n        Dictionary to store meta data.\n    unit : str or `~astropy.units.Unit`\n        The map unit\n    """"""\n\n    def __init__(self, geom, data=None, dtype=""float32"", meta=None, unit=""""):\n        data_shape = geom.data_shape\n\n        if data is None:\n            data = self._make_default_data(geom, data_shape, dtype)\n\n        super().__init__(geom, data, meta, unit)\n        self._wcs2d = None\n        self._hpx2wcs = None\n\n    @staticmethod\n    def _make_default_data(geom, shape_np, dtype):\n        if geom.npix.size > 1:\n            data = np.full(shape_np, np.nan, dtype=dtype)\n            idx = geom.get_idx(local=True)\n            data[idx[::-1]] = 0\n        else:\n            data = np.zeros(shape_np, dtype=dtype)\n\n        return data\n\n    @classmethod\n    def from_hdu(cls, hdu, hdu_bands=None):\n        """"""Make a HpxNDMap object from a FITS HDU.\n\n        Parameters\n        ----------\n        hdu : `~astropy.io.fits.BinTableHDU`\n            The FITS HDU\n        hdu_bands  : `~astropy.io.fits.BinTableHDU`\n            The BANDS table HDU\n        """"""\n        hpx = HpxGeom.from_header(hdu.header, hdu_bands)\n\n        convname = HpxGeom.identify_hpx_convention(hdu.header)\n        hpx_conv = HPX_FITS_CONVENTIONS[convname]\n\n        shape = tuple([ax.nbin for ax in hpx.axes[::-1]])\n        # shape_data = shape + tuple([np.max(hpx.npix)])\n\n        # TODO: Should we support extracting slices?\n\n        meta = cls._get_meta_from_header(hdu.header)\n        unit = unit_from_fits_image_hdu(hdu.header)\n        map_out = cls(hpx, None, meta=meta, unit=unit)\n\n        colnames = hdu.columns.names\n        cnames = []\n        if hdu.header.get(""INDXSCHM"", None) == ""SPARSE"":\n            pix = hdu.data.field(""PIX"")\n            vals = hdu.data.field(""VALUE"")\n            if ""CHANNEL"" in hdu.data.columns.names:\n                chan = hdu.data.field(""CHANNEL"")\n                chan = np.unravel_index(chan, shape)\n                idx = chan + (pix,)\n            else:\n                idx = (pix,)\n\n            map_out.set_by_idx(idx[::-1], vals)\n        else:\n            for c in colnames:\n                if c.find(hpx_conv.colstring) == 0:\n                    cnames.append(c)\n            nbin = len(cnames)\n            if nbin == 1:\n                map_out.data = hdu.data.field(cnames[0])\n            else:\n                for i, cname in enumerate(cnames):\n                    idx = np.unravel_index(i, shape)\n                    map_out.data[idx + (slice(None),)] = hdu.data.field(cname)\n\n        return map_out\n\n    def make_wcs_mapping(\n        self, sum_bands=False, proj=""AIT"", oversample=2, width_pix=None\n    ):\n        """"""Make a HEALPix to WCS mapping object.\n\n        Parameters\n        ----------\n        sum_bands : bool\n            sum over non-spatial dimensions before reprojecting\n        proj  : str\n            WCS-projection\n        oversample : float\n            Oversampling factor for WCS map. This will be the\n            approximate ratio of the width of a HPX pixel to a WCS\n            pixel. If this parameter is None then the width will be\n            set from ``width_pix``.\n        width_pix : int\n            Width of the WCS geometry in pixels.  The pixel size will\n            be set to the number of pixels satisfying ``oversample``\n            or ``width_pix`` whichever is smaller.  If this parameter\n            is None then the width will be set from ``oversample``.\n\n        Returns\n        -------\n        hpx2wcs : `~HpxToWcsMapping`\n\n        """"""\n        self._wcs2d = self.geom.make_wcs(\n            proj=proj, oversample=oversample, width_pix=width_pix, drop_axes=True\n        )\n        self._hpx2wcs = HpxToWcsMapping.create(self.geom, self._wcs2d)\n        return self._hpx2wcs\n\n    def to_wcs(\n        self,\n        sum_bands=False,\n        normalize=True,\n        proj=""AIT"",\n        oversample=2,\n        width_pix=None,\n        hpx2wcs=None,\n    ):\n        from .wcsnd import WcsNDMap\n\n        if sum_bands and self.geom.nside.size > 1:\n            map_sum = self.sum_over_axes()\n            return map_sum.to_wcs(\n                sum_bands=False,\n                normalize=normalize,\n                proj=proj,\n                oversample=oversample,\n                width_pix=width_pix,\n            )\n\n        # FIXME: Check whether the old mapping is still valid and reuse it\n        if hpx2wcs is None:\n            hpx2wcs = self.make_wcs_mapping(\n                oversample=oversample, proj=proj, width_pix=width_pix\n            )\n\n        # FIXME: Need a function to extract a valid shape from npix property\n\n        if sum_bands:\n            axes = np.arange(self.data.ndim - 1)\n            hpx_data = np.apply_over_axes(np.sum, self.data, axes=axes)\n            hpx_data = np.squeeze(hpx_data)\n            wcs_shape = tuple([t.flat[0] for t in hpx2wcs.npix])\n            wcs_data = np.zeros(wcs_shape).T\n            wcs = hpx2wcs.wcs.to_image()\n        else:\n            hpx_data = self.data\n            wcs_shape = tuple([t.flat[0] for t in hpx2wcs.npix]) + self.geom.shape_axes\n            wcs_data = np.zeros(wcs_shape).T\n            wcs = hpx2wcs.wcs.to_cube(self.geom.axes)\n\n        # FIXME: Should reimplement instantiating map first and fill data array\n        hpx2wcs.fill_wcs_map_from_hpx_data(hpx_data, wcs_data, normalize)\n        return WcsNDMap(wcs, wcs_data, unit=self.unit)\n\n    def sum_over_axes(self):\n        """"""Sum over all non-spatial dimensions.\n\n        Returns\n        -------\n        map_out : `~HpxNDMap`\n            Summed map.\n        """"""\n        geom = self.geom.to_image()\n        axis = tuple(range(self.data.ndim - 1))\n        data = np.nansum(self.data, axis=axis)\n        return self._init_copy(geom=geom, data=data)\n\n    def pad(self, pad_width, mode=""constant"", cval=0, order=1):\n        geom = self.geom.pad(pad_width)\n        map_out = self._init_copy(geom=geom, data=None)\n        map_out.coadd(self)\n        coords = geom.get_coord(flat=True)\n        m = self.geom.contains(coords)\n        coords = tuple([c[~m] for c in coords])\n\n        if mode == ""constant"":\n            map_out.set_by_coord(coords, cval)\n        elif mode == ""interp"":\n            # FIXME: These modes don\'t work at present because\n            # interp_by_coord doesn\'t support extrapolation\n            vals = self.interp_by_coord(coords, interp=order)\n            map_out.set_by_coord(coords, vals)\n        else:\n            raise ValueError(f""Unrecognized pad mode: {mode!r}"")\n\n        return map_out\n\n    def crop(self, crop_width):\n        geom = self.geom.crop(crop_width)\n        map_out = self._init_copy(geom=geom, data=None)\n        map_out.coadd(self)\n        return map_out\n\n    def upsample(self, factor, preserve_counts=True):\n        geom = self.geom.upsample(factor)\n        coords = geom.get_coord()\n        data = self.get_by_coord(coords)\n\n        if preserve_counts:\n            data /= factor ** 2\n\n        return self._init_copy(geom=geom, data=data)\n\n    def downsample(self, factor, preserve_counts=True):\n        geom = self.geom.downsample(factor)\n        coords = self.geom.get_coord()\n        vals = self.get_by_coord(coords)\n\n        map_out = self._init_copy(geom=geom, data=None)\n        map_out.fill_by_coord(coords, vals)\n\n        if not preserve_counts:\n            map_out.data /= factor ** 2\n\n        return map_out\n\n    def interp_by_coord(self, coords, interp=1):\n        # inherited docstring\n        coords = MapCoord.create(coords, frame=self.geom.frame)\n\n        order = interp_to_order(interp)\n        if order == 1:\n            return self._interp_by_coord(coords, order)\n        else:\n            raise ValueError(f""Invalid interpolation order: {order!r}"")\n\n    def interp_by_pix(self, pix, interp=None):\n        """"""Interpolate map values at the given pixel coordinates.\n        """"""\n        raise NotImplementedError\n\n    def get_by_idx(self, idx):\n        # inherited docstring\n        idx = pix_tuple_to_idx(idx)\n        idx = self.geom.global_to_local(idx)\n        return self.data.T[idx]\n\n    def _get_interp_weights(self, coords, idxs=None):\n        import healpy as hp\n\n        if idxs is None:\n            idxs = self.geom.coord_to_idx(coords, clip=True)[1:]\n\n        theta, phi = coords.theta, coords.phi\n\n        m = ~np.isfinite(theta)\n        theta[m] = 0\n        phi[m] = 0\n\n        if not self.geom.is_regular:\n            nside = self.geom.nside[tuple(idxs)]\n        else:\n            nside = self.geom.nside\n\n        pix, wts = hp.get_interp_weights(nside, theta, phi, nest=self.geom.nest)\n        wts[:, m] = 0\n        pix[:, m] = INVALID_INDEX.int\n\n        if not self.geom.is_regular:\n            pix_local = [self.geom.global_to_local([pix] + list(idxs))[0]]\n        else:\n            pix_local = [self.geom[pix]]\n\n        # If a pixel lies outside of the geometry set its index to the center pixel\n        m = pix_local[0] == INVALID_INDEX.int\n        if m.any():\n            coords_ctr = [coords.lon, coords.lat]\n            coords_ctr += [ax.pix_to_coord(t) for ax, t in zip(self.geom.axes, idxs)]\n            idx_ctr = self.geom.coord_to_idx(coords_ctr)\n            idx_ctr = self.geom.global_to_local(idx_ctr)\n            pix_local[0][m] = (idx_ctr[0] * np.ones(pix.shape, dtype=int))[m]\n\n        pix_local += [np.broadcast_to(t, pix_local[0].shape) for t in idxs]\n        return pix_local, wts\n\n    def _interp_by_coord(self, coords, order):\n        """"""Linearly interpolate map values.""""""\n        pix, wts = self._get_interp_weights(coords)\n\n        if self.geom.is_image:\n            return np.sum(self.data.T[tuple(pix)] * wts, axis=0)\n\n        val = np.zeros(pix[0].shape[1:])\n\n        # Loop over function values at corners\n        for i in range(2 ** len(self.geom.axes)):\n            pix_i = []\n            wt = np.ones(pix[0].shape[1:])[np.newaxis, ...]\n            for j, ax in enumerate(self.geom.axes):\n                idx = ax.coord_to_idx(coords[ax.name])\n                idx = np.clip(idx, 0, len(ax.center) - 2)\n\n                w = ax.center[idx + 1] - ax.center[idx]\n                c = Quantity(coords[ax.name], ax.center.unit, copy=False).value\n\n                if i & (1 << j):\n                    wt *= (c - ax.center[idx].value) / w.value\n                    pix_i += [idx + 1]\n                else:\n                    wt *= 1.0 - (c - ax.center[idx].value) / w.value\n                    pix_i += [idx]\n\n            if not self.geom.is_regular:\n                pix, wts = self._get_interp_weights(coords, pix_i)\n\n            wts[pix[0] == INVALID_INDEX.int] = 0\n            wt[~np.isfinite(wt)] = 0\n            val += np.nansum(wts * wt * self.data.T[tuple(pix[:1] + pix_i)], axis=0)\n\n        return val\n\n    def fill_by_idx(self, idx, weights=None):\n        idx = pix_tuple_to_idx(idx)\n        msk = np.all(np.stack([t != INVALID_INDEX.int for t in idx]), axis=0)\n        if weights is not None:\n            weights = weights[msk]\n        idx = [t[msk] for t in idx]\n\n        idx_local = list(self.geom.global_to_local(idx))\n        msk = idx_local[0] >= 0\n        idx_local = [t[msk] for t in idx_local]\n        if weights is not None:\n            if isinstance(weights, Quantity):\n                weights = weights.to_value(self.unit)\n            weights = weights[msk]\n\n        idx_local = np.ravel_multi_index(idx_local, self.data.T.shape)\n        idx_local, idx_inv = np.unique(idx_local, return_inverse=True)\n        weights = np.bincount(idx_inv, weights=weights)\n        self.data.T.flat[idx_local] += weights\n\n    def set_by_idx(self, idx, vals):\n        idx = pix_tuple_to_idx(idx)\n        idx_local = self.geom.global_to_local(idx)\n        self.data.T[idx_local] = vals\n\n    def _make_cols(self, header, conv):\n        shape = self.data.shape\n        cols = []\n\n        if header[""INDXSCHM""] == ""SPARSE"":\n            data = self.data.copy()\n            data[~np.isfinite(data)] = 0\n            nonzero = np.where(data > 0)\n            value = data[nonzero].astype(float)\n            pix = self.geom.local_to_global(nonzero[::-1])[0]\n            if len(shape) == 1:\n                cols.append(fits.Column(""PIX"", ""J"", array=pix))\n                cols.append(fits.Column(""VALUE"", ""E"", array=value))\n            else:\n                channel = np.ravel_multi_index(nonzero[:-1], shape[:-1])\n                cols.append(fits.Column(""PIX"", ""J"", array=pix))\n                cols.append(fits.Column(""CHANNEL"", ""I"", array=channel))\n                cols.append(fits.Column(""VALUE"", ""E"", array=value))\n        elif len(shape) == 1:\n            name = conv.colname(indx=conv.firstcol)\n            array = self.data.astype(float)\n            cols.append(fits.Column(name, ""E"", array=array))\n        else:\n            for i, idx in enumerate(np.ndindex(shape[:-1])):\n                name = conv.colname(indx=i + conv.firstcol)\n                array = self.data[idx].astype(float)\n                cols.append(fits.Column(name, ""E"", array=array))\n\n        return cols\n\n    def to_swapped(self):\n        import healpy as hp\n\n        hpx_out = self.geom.to_swapped()\n        map_out = self._init_copy(geom=hpx_out, data=None)\n        idx = self.geom.get_idx(flat=True)\n        vals = self.get_by_idx(idx)\n        if self.geom.nside.size > 1:\n            nside = self.geom.nside[idx[1:]]\n        else:\n            nside = self.geom.nside\n\n        if self.geom.nest:\n            idx_new = tuple([hp.nest2ring(nside, idx[0])]) + idx[1:]\n        else:\n            idx_new = tuple([hp.ring2nest(nside, idx[0])]) + idx[1:]\n\n        map_out.set_by_idx(idx_new, vals)\n        return map_out\n\n    def to_ud_graded(self, nside, preserve_counts=False):\n        # FIXME: Should we remove/deprecate this method?\n\n        order = nside_to_order(nside)\n        new_hpx = self.geom.to_ud_graded(order)\n        map_out = self._init_copy(geom=new_hpx, data=None)\n\n        if np.all(order <= self.geom.order):\n            # Downsample\n            idx = self.geom.get_idx(flat=True)\n            coords = self.geom.pix_to_coord(idx)\n            vals = self.get_by_idx(idx)\n            map_out.fill_by_coord(coords, vals)\n        else:\n            # Upsample\n            idx = new_hpx.get_idx(flat=True)\n            coords = new_hpx.pix_to_coord(idx)\n            vals = self.get_by_coord(coords)\n            m = np.isfinite(vals)\n            map_out.fill_by_coord([c[m] for c in coords], vals[m])\n\n        if not preserve_counts:\n            fact = (2 ** order) ** 2 / (2 ** self.geom.order) ** 2\n            if self.geom.nside.size > 1:\n                fact = fact[..., None]\n            map_out.data *= fact\n\n        return map_out\n\n    def plot(\n        self,\n        method=""raster"",\n        ax=None,\n        normalize=False,\n        proj=""AIT"",\n        oversample=2,\n        width_pix=1000,\n        **kwargs,\n    ):\n        """"""Quickplot method.\n\n        This will generate a visualization of the map by converting to\n        a rasterized WCS image (method=\'raster\') or drawing polygons\n        for each pixel (method=\'poly\').\n\n        Parameters\n        ----------\n        method : {\'raster\',\'poly\'}\n            Method for mapping HEALPix pixels to a two-dimensional\n            image.  Can be set to \'raster\' (rasterization to cartesian\n            image plane) or \'poly\' (explicit polygons for each pixel).\n            WARNING: The \'poly\' method is much slower than \'raster\'\n            and only suitable for maps with less than ~10k pixels.\n        proj : string, optional\n            Any valid WCS projection type.\n        oversample : float\n            Oversampling factor for WCS map. This will be the\n            approximate ratio of the width of a HPX pixel to a WCS\n            pixel. If this parameter is None then the width will be\n            set from ``width_pix``.\n        width_pix : int\n            Width of the WCS geometry in pixels.  The pixel size will\n            be set to the number of pixels satisfying ``oversample``\n            or ``width_pix`` whichever is smaller.  If this parameter\n            is None then the width will be set from ``oversample``.\n        **kwargs : dict\n            Keyword arguments passed to `~matplotlib.pyplot.imshow`.\n\n        Returns\n        -------\n        fig : `~matplotlib.figure.Figure`\n            Figure object.\n        ax : `~astropy.visualization.wcsaxes.WCSAxes`\n            WCS axis object\n        im : `~matplotlib.image.AxesImage` or `~matplotlib.collections.PatchCollection`\n            Image object.\n\n        """"""\n        if method == ""raster"":\n            m = self.to_wcs(\n                sum_bands=True,\n                normalize=normalize,\n                proj=proj,\n                oversample=oversample,\n                width_pix=width_pix,\n            )\n            return m.plot(ax, **kwargs)\n        elif method == ""poly"":\n            return self._plot_poly(proj=proj, ax=ax)\n        else:\n            raise ValueError(f""Invalid method: {method!r}"")\n\n    def _plot_poly(self, proj=""AIT"", step=1, ax=None):\n        """"""Plot the map using a collection of polygons.\n\n        Parameters\n        ----------\n        proj : string, optional\n            Any valid WCS projection type.\n        step : int\n            Set the number vertices that will be computed for each\n            pixel in multiples of 4.\n        """"""\n        # FIXME: At the moment this only works for all-sky maps if the\n        # projection is centered at (0,0)\n\n        # FIXME: Figure out how to force a square aspect-ratio like imshow\n\n        import matplotlib.pyplot as plt\n        from matplotlib.patches import Polygon\n        from matplotlib.collections import PatchCollection\n        import healpy as hp\n\n        wcs = self.geom.make_wcs(proj=proj, oversample=1)\n        if ax is None:\n            fig = plt.gcf()\n            ax = fig.add_subplot(111, projection=wcs.wcs, aspect=""equal"")\n\n        wcs_lonlat = wcs.center_coord[:2]\n        idx = self.geom.get_idx()\n        vtx = hp.boundaries(self.geom.nside, idx[0], nest=self.geom.nest, step=step)\n        theta, phi = hp.vec2ang(np.rollaxis(vtx, 2))\n        theta = theta.reshape((4 * step, -1)).T\n        phi = phi.reshape((4 * step, -1)).T\n\n        patches = []\n        data = []\n\n        def get_angle(x, t):\n            return 180.0 - (180.0 - x + t) % 360.0\n\n        for i, (x, y) in enumerate(zip(phi, theta)):\n\n            lon, lat = np.degrees(x), np.degrees(np.pi / 2.0 - y)\n            # Add a small ofset to avoid vertices wrapping to the\n            # other size of the projection\n            if get_angle(np.median(lon), wcs_lonlat[0].to_value(""deg"")) > 0:\n                idx = wcs.coord_to_pix((lon - 1e-4, lat))\n            else:\n                idx = wcs.coord_to_pix((lon + 1e-4, lat))\n\n            dist = np.max(np.abs(idx[0][0] - idx[0]))\n\n            # Split pixels that wrap around the edges of the projection\n            if dist > wcs.npix[0] / 1.5:\n                lon, lat = np.degrees(x), np.degrees(np.pi / 2.0 - y)\n                lon0 = lon - 1e-4\n                lon1 = lon + 1e-4\n                pix0 = wcs.coord_to_pix((lon0, lat))\n                pix1 = wcs.coord_to_pix((lon1, lat))\n\n                idx0 = np.argsort(pix0[0])\n                idx1 = np.argsort(pix1[0])\n\n                pix0 = (pix0[0][idx0][:3], pix0[1][idx0][:3])\n                pix1 = (pix1[0][idx1][1:], pix1[1][idx1][1:])\n\n                patches.append(Polygon(np.vstack((pix0[0], pix0[1])).T, True))\n                patches.append(Polygon(np.vstack((pix1[0], pix1[1])).T, True))\n                data.append(self.data[i])\n                data.append(self.data[i])\n            else:\n                polygon = Polygon(np.vstack((idx[0], idx[1])).T, True)\n                patches.append(polygon)\n                data.append(self.data[i])\n\n        p = PatchCollection(patches, linewidths=0, edgecolors=""None"")\n        p.set_array(np.array(data))\n        ax.add_collection(p)\n        ax.autoscale_view()\n        ax.coords.grid(color=""w"", linestyle="":"", linewidth=0.5)\n\n        return fig, ax, p\n'"
gammapy/maps/region.py,16,"b'import copy\nimport numpy as np\nfrom astropy import units as u\nfrom astropy.coordinates import SkyCoord\nfrom astropy.table import Table\nfrom astropy.wcs import WCS\nfrom astropy.wcs.utils import proj_plane_pixel_area, wcs_to_celestial_frame\nfrom regions import FITSRegionParser, fits_region_objects_to_table\nfrom gammapy.utils.regions import (\n    compound_region_to_list,\n    list_to_compound_region,\n    make_region,\n)\nfrom .base import MapCoord\nfrom .geom import Geom, MapAxis, make_axes, pix_tuple_to_idx\nfrom .utils import edges_from_lo_hi\nfrom .wcs import WcsGeom\n\n__all__ = [""RegionGeom""]\n\n\nclass RegionGeom(Geom):\n    """"""Map geometry representing a region on the sky.\n\n    Parameters\n    ----------\n    region : `~regions.SkyRegion`\n        Region object.\n    axes : list of `MapAxis`\n        Non-spatial data axes.\n    wcs : `~astropy.wcs.WCS`\n        Optional wcs object to project the region if needed.\n    """"""\n\n    is_image = False\n    is_allsky = False\n    is_hpx = False\n    _slice_spatial_axes = slice(0, 2)\n    _slice_non_spatial_axes = slice(2, None)\n    projection = ""TAN""\n    binsz = 0.01\n\n    def __init__(self, region, axes=None, wcs=None):\n        self._region = region\n        self._axes = make_axes(axes)\n\n        if wcs is None and region is not None:\n            wcs = WcsGeom.create(\n                skydir=region.center,\n                binsz=self.binsz,\n                proj=self.projection,\n                frame=self.frame,\n            ).wcs\n\n        self._wcs = wcs\n        self.ndim = len(self.data_shape)\n\n    @property\n    def frame(self):\n        try:\n            return self.region.center.frame.name\n        except AttributeError:\n            return wcs_to_celestial_frame(self.wcs).name\n\n    @property\n    def width(self):\n        """"""Width of bounding box of the region""""""\n        if self.region is None:\n            raise ValueError(""Region definition required."")\n\n        regions = compound_region_to_list(self.region)\n        regions_pix = [_.to_pixel(self.wcs) for _ in regions]\n\n        bbox = regions_pix[0].bounding_box\n\n        for region_pix in regions_pix[1:]:\n            bbox = bbox.union(region_pix.bounding_box)\n\n        rectangle_pix = bbox.to_region()\n        rectangle = rectangle_pix.to_sky(self.wcs)\n        return u.Quantity([rectangle.width, rectangle.height])\n\n    @property\n    def region(self):\n        return self._region\n\n    @property\n    def axes(self):\n        return self._axes\n\n    @property\n    def wcs(self):\n        return self._wcs\n\n    @property\n    def center_coord(self):\n        """"""(`astropy.coordinates.SkyCoord`)""""""\n        return self.pix_to_coord(self.center_pix)\n\n    @property\n    def center_pix(self):\n        return tuple((np.array(self.data_shape) - 1.0) / 2)[::-1]\n\n    @property\n    def center_skydir(self):\n        """"""Center skydir""""""\n        try:\n            return self.region.center\n        except AttributeError:\n            xp, yp = self.wcs.wcs.crpix\n            return SkyCoord.from_pixel(xp=xp, yp=yp, wcs=self.wcs)\n\n    def contains(self, coords):\n        if self.region is None:\n            raise ValueError(""Region definition required."")\n\n        coords = MapCoord.create(coords)\n        return self.region.contains(coords.skycoord, self.wcs)\n\n    def separation(self, position):\n        return self.center_skydir.separation(position)\n\n    @property\n    def data_shape(self):\n        """"""Shape of the Numpy data array matching this geometry.""""""\n        return self._shape[::-1]\n\n    @property\n    def _shape(self):\n        npix_shape = [1, 1]\n        ax_shape = [ax.nbin for ax in self.axes]\n        return tuple(npix_shape + ax_shape)\n\n    def get_coord(self, frame=None):\n        """"""Get map coordinates from the geometry.\n\n        Returns\n        -------\n        coord : `~MapCoord`\n            Map coordinate object.\n        """"""\n        #TODO: support mode=edges?\n        cdict = {}\n        cdict[""skycoord""] = self.center_skydir.reshape((1, 1))\n\n        if self.axes is not None:\n            coords = []\n            for ax in self.axes:\n                coords.append(ax.center) #.reshape((-1, 1, 1)))\n\n            coords = np.meshgrid(*coords)\n            for idx, ax in enumerate(self.axes):\n                cdict[ax.name] = coords[idx].reshape(self.data_shape)\n\n        if frame is None:\n            frame = self.frame\n\n        return MapCoord.create(cdict, frame=self.frame).to_frame(frame)\n\n    def pad(self):\n        raise NotImplementedError(""Padding of `RegionGeom` not supported"")\n\n    def crop(self):\n        raise NotImplementedError(""Cropping of `RegionGeom` not supported"")\n\n    def solid_angle(self):\n        if self.region is None:\n            raise ValueError(""Region definition required."")\n\n        area = self.region.to_pixel(self.wcs).area\n        solid_angle = area * proj_plane_pixel_area(self.wcs) * u.deg ** 2\n        return solid_angle.to(""sr"")\n\n    def bin_volume(self):\n        bin_volume = self.solid_angle()*np.ones(self.data_shape)\n\n        for idx, ax in enumerate(self.axes):\n            shape = self.ndim * [1]\n            shape[-(idx + 3)] = -1\n            bin_volume = bin_volume * ax.bin_width.reshape(tuple(shape))\n\n        return bin_volume\n\n    def to_cube(self, axes):\n        axes = copy.deepcopy(self.axes) + axes\n        return self._init_copy(axes=axes)\n\n    def to_image(self):\n        return self._init_copy(axes=None)\n\n    def upsample(self, factor, axis):\n        axes = copy.deepcopy(self.axes)\n        idx = self.get_axis_index_by_name(axis)\n        axes[idx] = axes[idx].upsample(factor)\n        return self._init_copy(axes=axes)\n\n    def downsample(self, factor, axis):\n        axes = copy.deepcopy(self.axes)\n        idx = self.get_axis_index_by_name(axis)\n        axes[idx] = axes[idx].downsample(factor)\n        return self._init_copy(axes=axes)\n\n    def pix_to_coord(self, pix):\n        lon = np.where(\n            (-0.5 < pix[0]) & (pix[0] < 0.5),\n            self.center_skydir.data.lon,\n            np.nan * u.deg,\n        )\n        lat = np.where(\n            (-0.5 < pix[1]) & (pix[1] < 0.5),\n            self.center_skydir.data.lat,\n            np.nan * u.deg,\n        )\n        coords = (lon, lat)\n\n        for p, ax in zip(pix[self._slice_non_spatial_axes], self.axes):\n            coords += (ax.pix_to_coord(p),)\n\n        return coords\n\n    def pix_to_idx(self, pix, clip=False):\n        idxs = list(pix_tuple_to_idx(pix))\n\n        for i, idx in enumerate(idxs[self._slice_non_spatial_axes]):\n            if clip:\n                np.clip(idx, 0, self.axes[i].nbin - 1, out=idx)\n            else:\n                np.putmask(idx, (idx < 0) | (idx >= self.axes[i].nbin), -1)\n\n        return tuple(idxs)\n\n    def coord_to_pix(self, coords):\n        if self.region is None:\n            raise ValueError(""Region definition required."")\n\n        coords = MapCoord.create(coords, frame=self.frame)\n        in_region = self.region.contains(coords.skycoord, wcs=self.wcs)\n\n        x = np.zeros(coords.shape)\n        x[~in_region] = np.nan\n\n        y = np.zeros(coords.shape)\n        y[~in_region] = np.nan\n\n        pix = (x, y)\n        for coord, ax in zip(coords[self._slice_non_spatial_axes], self.axes):\n            pix += (ax.coord_to_pix(coord),)\n\n        return pix\n\n    def get_idx(self):\n        idxs = [np.arange(n, dtype=float) for n in self.data_shape[::-1]]\n        return np.meshgrid(*idxs[::-1], indexing=""ij"")[::-1]\n\n\n    def _make_bands_cols(self):\n        pass\n\n    @classmethod\n    def create(cls, region, **kwargs):\n        """"""Create region.\n\n        Parameters\n        ----------\n        region : str or `~regions.SkyRegion`\n            Region\n        axes : list of `MapAxis`\n            Non spatial axes.\n\n        Returns\n        -------\n        geom : `RegionGeom`\n            Region geometry\n        """"""\n        if isinstance(region, str):\n            region = make_region(region)\n\n        return cls(region, **kwargs)\n\n    def __repr__(self):\n        axes = [""lon"", ""lat""] + [_.name for _ in self.axes]\n        try:\n            frame = self.center_skydir.frame.name\n            lon = self.center_skydir.data.lon.deg\n            lat = self.center_skydir.data.lat.deg\n        except AttributeError:\n            frame, lon, lat = """", np.nan, np.nan\n\n        return (\n            f""{self.__class__.__name__}\\n\\n""\n            f""\\tregion     : {self.region.__class__.__name__}\\n""\n            f""\\taxes       : {axes}\\n""\n            f""\\tshape      : {self.data_shape[::-1]}\\n""\n            f""\\tndim       : {self.ndim}\\n""\n            f""\\tframe      : {frame}\\n""\n            f""\\tcenter     : {lon:.1f} deg, {lat:.1f} deg\\n""\n        )\n\n    def __eq__(self, other):\n        # check overall shape and axes compatibility\n        if self.data_shape != other.data_shape:\n            return False\n\n        for axis, otheraxis in zip(self.axes, other.axes):\n            if axis != otheraxis:\n                return False\n\n        # TODO: compare regions\n        return True\n\n    def _to_region_table(self):\n        """"""Export region to a FITS region table.""""""\n        if self.region is None:\n            raise ValueError(""Region definition required."")\n\n        # TODO: make this a to_hdulist() method\n        region_list = compound_region_to_list(self.region)\n        pixel_region_list = []\n        for reg in region_list:\n            pixel_region_list.append(reg.to_pixel(self.wcs))\n        table = fits_region_objects_to_table(pixel_region_list)\n        table.meta.update(self.wcs.to_header())\n        return table\n\n    @classmethod\n    def from_hdulist(cls, hdulist, format=""ogip""):\n        """"""Read region table and convert it to region list.""""""\n\n        if ""REGION"" in hdulist:\n            region_table = Table.read(hdulist[""REGION""])\n            parser = FITSRegionParser(region_table)\n            pix_region = parser.shapes.to_regions()\n            wcs = WCS(region_table.meta)\n\n            regions = []\n            for reg in pix_region:\n                regions.append(reg.to_sky(wcs))\n            region = list_to_compound_region(regions)\n        else:\n            region, wcs = None, None\n\n        ebounds = Table.read(hdulist[""EBOUNDS""])\n        emin = ebounds[""E_MIN""].quantity\n        emax = ebounds[""E_MAX""].quantity\n\n        edges = edges_from_lo_hi(emin, emax)\n        axis = MapAxis.from_edges(edges, interp=""log"", name=""energy"")\n        return cls(region=region, wcs=wcs, axes=[axis])\n\n    def union(self, other):\n        """"""Stack a RegionGeom by making the union""""""\n        if not self == other:\n            print(self, other)\n            raise ValueError(""Can only make union if extra axes are equivalent."")\n        if other.region:\n            if self.region:\n                self._region = self.region.union(other.region)\n            else:\n                self._region = other.region\n'"
gammapy/maps/regionnd.py,12,"b'import numpy as np\nfrom astropy import units as u\nfrom astropy.table import Table\nfrom astropy.visualization import quantity_support\nfrom gammapy.extern.skimage import block_reduce\nfrom gammapy.utils.interpolation import ScaledRegularGridInterpolator\nfrom gammapy.utils.regions import compound_region_to_list\nfrom .base import Map\nfrom .geom import pix_tuple_to_idx\nfrom .region import RegionGeom\nfrom .utils import INVALID_INDEX\n\n__all__ = [""RegionNDMap""]\n\n\nclass RegionNDMap(Map):\n    """"""Region ND map\n\n    Parameters\n    ----------\n    geom : `~gammapy.maps.RegionGeom`\n        Region geometry object.\n    data : `~numpy.ndarray`\n        Data array. If none then an empty array will be allocated.\n    dtype : str, optional\n        Data type, default is float32\n    meta : `dict`\n        Dictionary to store meta data.\n    unit : str or `~astropy.units.Unit`\n        The map unit\n    """"""\n\n    def __init__(self, geom, data=None, dtype=""float32"", meta=None, unit=""""):\n        if data is None:\n            data = np.zeros(geom.data_shape, dtype=dtype)\n\n        self._geom = geom\n        self.data = data\n        self.meta = meta\n        self.unit = u.Unit(unit)\n\n    def plot(self, ax=None, **kwargs):\n        """"""Plot region map.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.pyplot.Axis`\n            Axis used for plotting\n        **kwargs : dict\n            Keyword arguments passed to `~matplotlib.pyplot.errorbar`\n\n        Returns\n        -------\n        ax : `~matplotlib.pyplot.Axis`\n            Axis used for plotting\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = ax or plt.gca()\n\n        kwargs.setdefault(""fmt"", ""."")\n\n        if len(self.geom.axes) > 1:\n            raise TypeError(\n                ""Use `.plot_interactive()` if more the one extra axis is present.""\n            )\n\n        axis = self.geom.axes[0]\n\n        with quantity_support():\n            xerr = (axis.center - axis.edges[:-1], axis.edges[1:] - axis.center)\n            ax.errorbar(axis.center, self.quantity.squeeze(), xerr=xerr, **kwargs)\n\n        if axis.interp == ""log"":\n            ax.set_xscale(""log"")\n\n        ax.set_xlabel(axis.name.capitalize() + f"" [{axis.unit}]"")\n\n        if not self.unit.is_unity():\n            ax.set_ylabel(f""Data [{self.unit}]"")\n\n        ax.set_yscale(""log"")\n        return ax\n\n    def plot_hist(self, ax=None, **kwargs):\n        """"""Plot as histogram.\n\n        kwargs are forwarded to `~matplotlib.pyplot.hist`\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axis` (optional)\n            Axis instance to be used for the plot\n        **kwargs : dict\n            Keyword arguments passed to `~matplotlib.pyplot.hist`\n\n        Returns\n        -------\n        ax : `~matplotlib.pyplot.Axis`\n            Axis used for plotting\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        kwargs.setdefault(""histtype"", ""step"")\n        kwargs.setdefault(""lw"", 2)\n\n        axis = self.geom.axes[0]\n\n        with quantity_support():\n            weights = self.data[:, 0, 0]\n            ax.hist(axis.center.value, bins=axis.edges.value, weights=weights, **kwargs)\n\n        ax.set_xlabel(axis.name.capitalize() + f"" [{axis.unit}]"")\n\n        if not self.unit.is_unity():\n            ax.set_ylabel(f""Data [{self.unit}]"")\n\n        ax.set_xscale(""log"")\n        ax.set_yscale(""log"")\n        return ax\n\n    def plot_interactive(self):\n        raise NotImplementedError(\n            ""Interactive plotting currently not support for RegionNDMap""\n        )\n\n    def plot_region(self, ax=None, **kwargs):\n        """"""Plot region\n\n        Parameters\n        ----------\n        ax : `~astropy.vizualisation.WCSAxes`\n            Axes to plot on.\n        **kwargs : dict\n            Keyword arguments forwarded to `~regions.PixelRegion.as_artist`\n        """"""\n        import matplotlib.pyplot as plt\n        from matplotlib.collections import PatchCollection\n\n        if ax is None:\n            ax = plt.gca()\n\n        regions = compound_region_to_list(self.geom.region)\n        artists = [region.to_pixel(wcs=ax.wcs).as_artist() for region in regions]\n\n        patches = PatchCollection(artists, **kwargs)\n        ax.add_collection(patches)\n        return ax\n\n    @classmethod\n    def create(cls, region, axes=None, dtype=""float32"", meta=None, unit="""", wcs=None):\n        """"""\n\n        Parameters\n        ----------\n        region : str or `~regions.SkyRegion`\n            Region specification\n        axes : list of `MapAxis`\n            Non spatial axes.\n        dtype : str\n            Data type, default is \'float32\'\n        unit : str or `~astropy.units.Unit`\n            Data unit.\n        meta : `dict`\n            Dictionary to store meta data.\n        wcs : `~astropy.wcs.WCS`\n            WCS projection to use for local projections of the region\n\n        Returns\n        -------\n        map : `RegionNDMap`\n            Region map\n        """"""\n        geom = RegionGeom.create(region=region, axes=axes, wcs=wcs)\n        return cls(geom=geom, dtype=dtype, unit=unit, meta=meta)\n\n    def downsample(self, factor, preserve_counts=True, axis=""energy""):\n        geom = self.geom.downsample(factor=factor, axis=axis)\n        block_size = [1] * self.data.ndim\n        idx = self.geom.get_axis_index_by_name(axis)\n        block_size[-(idx + 1)] = factor\n\n        func = np.nansum if preserve_counts else np.nanmean\n        data = block_reduce(self.data, tuple(block_size[::-1]), func=func)\n\n        return self._init_copy(geom=geom, data=data)\n\n    def upsample(self, factor, preserve_counts=True, axis=""energy""):\n        geom = self.geom.upsample(factor=factor, axis=axis)\n        data = self.interp_by_coord(geom.get_coord())\n\n        if preserve_counts:\n            data /= factor\n\n        return self._init_copy(geom=geom, data=data)\n\n    def fill_by_idx(self, idx, weights=None):\n        idx = pix_tuple_to_idx(idx)\n\n        msk = np.all(np.stack([t != INVALID_INDEX.int for t in idx]), axis=0)\n        idx = [t[msk] for t in idx]\n\n        if weights is not None:\n            if isinstance(weights, u.Quantity):\n                weights = weights.to_value(self.unit)\n            weights = weights[msk]\n\n        idx = np.ravel_multi_index(idx, self.data.T.shape)\n        idx, idx_inv = np.unique(idx, return_inverse=True)\n        weights = np.bincount(idx_inv, weights=weights).astype(self.data.dtype)\n        self.data.T.flat[idx] += weights\n\n    def get_by_idx(self, idxs):\n        return self.data[idxs[::-1]]\n\n    def interp_by_coord(self, coords):\n        pix = self.geom.coord_to_pix(coords)\n        return self.interp_by_pix(pix)\n\n    def interp_by_pix(self, pix, method=""linear"", fill_value=None):\n        grid_pix = [np.arange(n, dtype=float) for n in self.data.shape[::-1]]\n\n        if np.any(np.isfinite(self.data)):\n            data = self.data.copy().T\n            data[~np.isfinite(data)] = 0.0\n        else:\n            data = self.data.T\n\n        fn = ScaledRegularGridInterpolator(\n            grid_pix, data, fill_value=fill_value, method=method\n        )\n        return fn(tuple(pix), clip=False)\n\n    def set_by_idx(self, idx, value):\n        self.data[idx[::-1]] = value\n\n    @staticmethod\n    def read(cls, filename):\n        raise NotImplementedError\n\n    def write(self, filename):\n        raise NotImplementedError\n\n    def to_hdulist(self):\n        raise NotImplementedError\n\n    @classmethod\n    def from_hdulist(cls, hdulist, format=""ogip"", ogip_column=""COUNTS""):\n        """"""Create from `~astropy.io.fits.HDUList`.\n\n\n        Parameters\n        ----------\n        hdulist : `~astropy.io.fits.HDUList`\n            HDU list.\n        format : {""ogip""}\n            Format specification\n        ogip_column : str\n            OGIP data format column\n\n        Returns\n        -------\n        region_nd_map : `RegionNDMap`\n            Region map.\n        """"""\n        if format != ""ogip"":\n            raise ValueError(""Only \'ogip\' format supported"")\n\n        table = Table.read(hdulist[""SPECTRUM""])\n        geom = RegionGeom.from_hdulist(hdulist, format=format)\n\n        return cls(geom=geom, data=table[ogip_column].data, meta=table.meta)\n\n    def crop(self):\n        raise NotImplementedError(""Crop is not supported by RegionNDMap"")\n\n    def pad(self):\n        raise NotImplementedError(""Pad is not supported by RegionNDMap"")\n\n    def sum_over_axes(self, keepdims=True):\n        axis = tuple(range(self.data.ndim - 2))\n        geom = self.geom.to_image()\n        if keepdims:\n            for ax in self.geom.axes:\n                geom = geom.to_cube([ax.squash()])\n        data = np.nansum(self.data, axis=axis, keepdims=keepdims)\n        # TODO: summing over the axis can change the unit, handle this correctly\n        return self._init_copy(geom=geom, data=data)\n\n    def stack(self, other, weights=None):\n        """"""Stack other region map into map.\n\n        Parameters\n        ----------\n        other : `RegionNDMap`\n            Other map to stack\n        weights : `RegionNDMap`\n            Array to be used as weights. The spatial geometry must be equivalent\n            to `other` and additional axes must be broadcastable.\n        """"""\n        data = other.data\n\n        # TODO: re-think stacking of regions. Is making the union reasonable?\n        # self.geom.union(other.geom)\n\n        if weights is not None:\n            if not other.geom.to_image() == weights.geom.to_image():\n                raise ValueError(""Incompatible geoms between map and weights"")\n            data = data * weights.data\n\n        self.data += data\n\n    def to_table(self):\n        """"""Convert to `~astropy.table.Table`.\n\n        Data format specification: :ref:`gadf:ogip-pha`\n        """"""\n        energy_axis = self.geom.axes[0]\n        channel = np.arange(energy_axis.nbin, dtype=np.int16)\n        counts = np.array(self.data[:, 0, 0], dtype=np.int32)\n\n        names = [""CHANNEL"", ""COUNTS""]\n        meta = {""name"": ""COUNTS""}\n\n        return Table([channel, counts], names=names, meta=meta)\n'"
gammapy/maps/utils.py,8,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom astropy.io import fits\nfrom gammapy.utils.random import get_random_state\n\n\ndef coordsys_to_frame(coordsys):\n    if coordsys in [""CEL"", ""C""]:\n        return ""icrs""\n    elif coordsys in [""GAL"", ""G""]:\n        return ""galactic""\n    else:\n        raise ValueError(f""Unknown coordinate system: \'{coordsys}\'"")\n\n\ndef frame_to_coordsys(frame):\n    if frame in [""icrs"", ""fk5"", ""fk4""]:\n        return ""CEL""\n    elif frame in [""galactic""]:\n        return ""GAL""\n    else:\n        raise ValueError(f""Unknown coordinate frame \'{frame}\'"")\n\n\nclass InvalidValue:\n    """"""Class to define placeholder for invalid array values.""""""\n\n    float = np.nan\n    int = np.nan\n    bool = np.nan\n\n    def __getitem__(self, dtype):\n        if np.issubdtype(dtype, np.integer):\n            return self.int\n        elif np.issubdtype(dtype, np.floating):\n            return self.float\n        elif np.issubdtype(dtype, np.dtype(bool).type):\n            return self.bool\n        else:\n            raise ValueError(f""No invalid value placeholder defined for {dtype}"")\n\n\nclass InvalidIndex:\n    """"""Class to define placeholder for invalid array indices.""""""\n\n    float = np.nan\n    int = -1\n    bool = False\n\n\nINVALID_VALUE = InvalidValue()\nINVALID_INDEX = InvalidIndex()\n\n\ndef fill_poisson(map_in, mu, random_state=""random-seed""):\n    """"""Fill a map object with a poisson random variable.\n\n    This can be useful for testing, to make a simulated counts image.\n    E.g. filling with ``mu=0.5`` fills the map so that many pixels\n    have value 0 or 1, and a few more ""counts"".\n\n    Parameters\n    ----------\n    map_in : `~gammapy.maps.Map`\n        Input map\n    mu : scalar or `~numpy.ndarray`\n        Expectation value\n    random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n        Defines random number generator initialisation.\n        Passed to `~gammapy.utils.random.get_random_state`.\n    """"""\n    random_state = get_random_state(random_state)\n    idx = map_in.geom.get_idx(flat=True)\n    mu = random_state.poisson(mu, idx[0].shape)\n    map_in.fill_by_idx(idx, mu)\n\n\ndef interp_to_order(interp):\n    """"""Convert interpolation string to order.""""""\n    if isinstance(interp, int):\n        return interp\n\n    order_map = {None: 0, ""nearest"": 0, ""linear"": 1, ""quadratic"": 2, ""cubic"": 3}\n    return order_map.get(interp, None)\n\n\ndef find_bands_hdu(hdu_list, hdu):\n    """"""Discover the extension name of the BANDS HDU.\n\n    Parameters\n    ----------\n    hdu_list : `~astropy.io.fits.HDUList`\n\n    hdu : `~astropy.io.fits.BinTableHDU` or `~astropy.io.fits.ImageHDU`\n\n    Returns\n    -------\n    hduname : str\n        Extension name of the BANDS HDU.  None if no BANDS HDU was found.\n    """"""\n    if ""BANDSHDU"" in hdu.header:\n        return hdu.header[""BANDSHDU""]\n\n    has_cube_data = False\n\n    if (\n        isinstance(hdu, (fits.ImageHDU, fits.PrimaryHDU))\n        and hdu.header.get(""NAXIS"", None) == 3\n    ):\n        has_cube_data = True\n    elif isinstance(hdu, fits.BinTableHDU):\n        if (\n            hdu.header.get(""INDXSCHM"", """") in [""EXPLICIT"", ""IMPLICIT"", """"]\n            and len(hdu.columns) > 1\n        ):\n            has_cube_data = True\n\n    if has_cube_data:\n        if ""EBOUNDS"" in hdu_list:\n            return ""EBOUNDS""\n        elif ""ENERGIES"" in hdu_list:\n            return ""ENERGIES""\n\n    return None\n\n\ndef find_hdu(hdulist):\n    """"""Find the first non-empty HDU.""""""\n    for hdu in hdulist:\n        if hdu.data is not None:\n            return hdu\n\n    raise AttributeError(""No Image or BinTable HDU found."")\n\n\ndef find_image_hdu(hdulist):\n    for hdu in hdulist:\n        if hdu.data is not None and isinstance(hdu, fits.ImageHDU):\n            return hdu\n\n    raise AttributeError(""No Image HDU found."")\n\n\ndef find_bintable_hdu(hdulist):\n    for hdu in hdulist:\n        if hdu.data is not None and isinstance(hdu, fits.BinTableHDU):\n            return hdu\n\n    raise AttributeError(""No BinTable HDU found."")\n\n\ndef edges_from_lo_hi(edges_lo, edges_hi):\n    edges = edges_lo.copy()\n    try:\n        edges = edges.insert(len(edges), edges_hi[-1])\n    except AttributeError:\n        edges = np.insert(edges, len(edges), edges_hi[-1])\n    return edges\n\n\ndef slice_to_str(slice_):\n    return f""{slice_.start}:{slice_.stop}""\n\n\ndef str_to_slice(slice_str):\n    start, stop = slice_str.split("":"")\n    return slice(int(start), int(stop))\n'"
gammapy/maps/wcs.py,54,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport copy\nfrom functools import lru_cache\nimport numpy as np\nimport astropy.units as u\nfrom astropy.coordinates import Angle, SkyCoord\nfrom astropy.io import fits\nfrom astropy.nddata import Cutout2D\nfrom astropy.wcs import WCS\nfrom astropy.wcs.utils import (\n    celestial_frame_to_wcs,\n    proj_plane_pixel_scales,\n    wcs_to_celestial_frame,\n)\nfrom regions import SkyRegion\nfrom .geom import (\n    Geom,\n    MapCoord,\n    find_and_read_bands,\n    get_shape,\n    make_axes,\n    pix_tuple_to_idx,\n    skycoord_to_lonlat,\n)\nfrom .utils import INVALID_INDEX, slice_to_str, str_to_slice\n\n__all__ = [""WcsGeom""]\n\n\ndef _check_width(width):\n    """"""Check and normalise width argument.\n\n    Always returns tuple (lon, lat) as float in degrees.\n    """"""\n    if isinstance(width, tuple):\n        lon = Angle(width[0], ""deg"").deg\n        lat = Angle(width[1], ""deg"").deg\n        return lon, lat\n    else:\n        angle = Angle(width, ""deg"").deg\n        if np.isscalar(angle):\n            return angle, angle\n        else:\n            return tuple(angle)\n\n\ndef cast_to_shape(param, shape, dtype):\n    """"""Cast a tuple of parameter arrays to a given shape.""""""\n    if not isinstance(param, tuple):\n        param = [param]\n\n    param = [np.array(p, ndmin=1, dtype=dtype) for p in param]\n\n    if len(param) == 1:\n        param = [param[0].copy(), param[0].copy()]\n\n    for i, p in enumerate(param):\n\n        if p.size > 1 and p.shape != shape:\n            raise ValueError\n\n        if p.shape == shape:\n            continue\n\n        param[i] = p * np.ones(shape, dtype=dtype)\n\n    return tuple(param)\n\n\ndef get_resampled_wcs(wcs, factor, downsampled):\n    """"""\n    Get resampled WCS object.\n    """"""\n    wcs = wcs.deepcopy()\n\n    if not downsampled:\n        factor = 1.0 / factor\n\n    wcs.wcs.cdelt *= factor\n    wcs.wcs.crpix = (wcs.wcs.crpix - 0.5) / factor + 0.5\n    return wcs\n\n\nclass WcsGeom(Geom):\n    """"""Geometry class for WCS maps.\n\n    This class encapsulates both the WCS transformation object and the\n    the image extent (number of pixels in each dimension).  Provides\n    methods for accessing the properties of the WCS object and\n    performing transformations between pixel and world coordinates.\n\n    Parameters\n    ----------\n    wcs : `~astropy.wcs.WCS`\n        WCS projection object\n    npix : tuple\n        Number of pixels in each spatial dimension\n    cdelt : tuple\n        Pixel size in each image plane.  If none then a constant pixel size will be used.\n    crpix : tuple\n        Reference pixel coordinate in each image plane.\n    axes : list\n        Axes for non-spatial dimensions\n    cutout_info : dict\n        Dict with cutout info, if the `WcsGeom` was created by `WcsGeom.cutout()`\n    """"""\n\n    _slice_spatial_axes = slice(0, 2)\n    _slice_non_spatial_axes = slice(2, None)\n    is_hpx = False\n\n    def __init__(self, wcs, npix, cdelt=None, crpix=None, axes=None, cutout_info=None):\n        self._wcs = wcs\n        self._frame = wcs_to_celestial_frame(wcs).name\n        self._projection = wcs.wcs.ctype[0][5:]\n        self._axes = make_axes(axes)\n\n        if cdelt is None:\n            cdelt = tuple(np.abs(self.wcs.wcs.cdelt))\n\n        # Shape to use for WCS transformations\n        wcs_shape = max([get_shape(t) for t in [npix, cdelt]])\n        self._npix = cast_to_shape(npix, wcs_shape, int)\n        self._cdelt = cast_to_shape(cdelt, wcs_shape, float)\n\n        # By convention CRPIX is indexed from 1\n        if crpix is None:\n            crpix = tuple(1.0 + (np.array(self._npix) - 1.0) / 2.0)\n\n        self._crpix = crpix\n        self._cutout_info = cutout_info\n\n        # define cached methods\n        self.get_coord = lru_cache()(self.get_coord)\n        self.solid_angle = lru_cache()(self.solid_angle)\n        self.bin_volume = lru_cache()(self.bin_volume)\n        self.to_image = lru_cache()(self.to_image)\n\n    @property\n    def data_shape(self):\n        """"""Shape of the Numpy data array matching this geometry.""""""\n        return self._shape[::-1]\n\n    @property\n    def _shape(self):\n        npix_shape = [np.max(self.npix[0]), np.max(self.npix[1])]\n        ax_shape = [ax.nbin for ax in self.axes]\n        return tuple(npix_shape + ax_shape)\n\n    @property\n    def _shape_edges(self):\n        npix_shape = [np.max(self.npix[0]) + 1, np.max(self.npix[1]) + 1]\n        ax_shape = [ax.nbin for ax in self.axes]\n        return tuple(npix_shape + ax_shape)\n\n    @property\n    def shape_axes(self):\n        """"""Shape of non-spatial axes.""""""\n        return self._shape[self._slice_non_spatial_axes]\n\n    @property\n    def wcs(self):\n        """"""WCS projection object.""""""\n        return self._wcs\n\n    @property\n    def frame(self):\n        """"""Coordinate system of the projection.\n\n        Galactic (""galactic"") or Equatorial (""icrs"").\n        """"""\n        return self._frame\n\n    @property\n    def cutout_info(self):\n        """"""Cutout info dict.""""""\n        return self._cutout_info\n\n    @property\n    def projection(self):\n        """"""Map projection.""""""\n        return self._projection\n\n    @property\n    def is_allsky(self):\n        """"""Flag for all-sky maps.""""""\n        if np.all(np.isclose(self._npix[0] * self._cdelt[0], 360.0)):\n            return True\n        else:\n            return False\n\n    @property\n    def is_regular(self):\n        """"""Is this geometry is regular in non-spatial dimensions (bool)?\n\n        - False for multi-resolution or irregular geometries.\n        - True if all image planes have the same pixel geometry.\n        """"""\n        if self.npix[0].size > 1:\n            return False\n        else:\n            return True\n\n    @property\n    def width(self):\n        """"""Tuple with image dimension in deg in longitude and latitude.""""""\n        dlon = self._cdelt[0] * self._npix[0]\n        dlat = self._cdelt[1] * self._npix[1]\n        return (dlon, dlat) * u.deg\n\n    @property\n    def pixel_area(self):\n        """"""Pixel area in deg^2.""""""\n        # FIXME: Correctly compute solid angle for projection\n        return self._cdelt[0] * self._cdelt[1]\n\n    @property\n    def npix(self):\n        """"""Tuple with image dimension in pixels in longitude and latitude.""""""\n        return self._npix\n\n    @property\n    def axes(self):\n        """"""List of non-spatial axes.""""""\n        return self._axes\n\n    @property\n    def ndim(self):\n        return len(self.data_shape)\n\n    @property\n    def center_coord(self):\n        """"""Map coordinate of the center of the geometry.\n\n        Returns\n        -------\n        coord : tuple\n        """"""\n        return self.pix_to_coord(self.center_pix)\n\n    @property\n    def center_pix(self):\n        """"""Pixel coordinate of the center of the geometry.\n\n        Returns\n        -------\n        pix : tuple\n        """"""\n        return tuple((np.array(self.data_shape) - 1.0) / 2)[::-1]\n\n    @property\n    def center_skydir(self):\n        """"""Sky coordinate of the center of the geometry.\n\n        Returns\n        -------\n        pix : `~astropy.coordinates.SkyCoord`\n        """"""\n        return SkyCoord.from_pixel(self.center_pix[0], self.center_pix[1], self.wcs)\n\n    @property\n    def pixel_scales(self):\n        """"""\n        Pixel scale.\n\n        Returns angles along each axis of the image at the CRPIX location once\n        it is projected onto the plane of intermediate world coordinates.\n\n        Returns\n        -------\n        angle: `~astropy.coordinates.Angle`\n        """"""\n        return Angle(proj_plane_pixel_scales(self.wcs), ""deg"")\n\n    @classmethod\n    def create(\n        cls,\n        npix=None,\n        binsz=0.5,\n        proj=""CAR"",\n        frame=""icrs"",\n        refpix=None,\n        axes=None,\n        skydir=None,\n        width=None,\n    ):\n        """"""Create a WCS geometry object.\n\n        Pixelization of the map is set with\n        ``binsz`` and one of either ``npix`` or ``width`` arguments.\n        For maps with non-spatial dimensions a different pixelization\n        can be used for each image plane by passing a list or array\n        argument for any of the pixelization parameters.  If both npix\n        and width are None then an all-sky geometry will be created.\n\n        Parameters\n        ----------\n        npix : int or tuple or list\n            Width of the map in pixels. A tuple will be interpreted as\n            parameters for longitude and latitude axes.  For maps with\n            non-spatial dimensions, list input can be used to define a\n            different map width in each image plane.  This option\n            supersedes width.\n        width : float or tuple or list\n            Width of the map in degrees.  A tuple will be interpreted\n            as parameters for longitude and latitude axes.  For maps\n            with non-spatial dimensions, list input can be used to\n            define a different map width in each image plane.\n        binsz : float or tuple or list\n            Map pixel size in degrees.  A tuple will be interpreted\n            as parameters for longitude and latitude axes.  For maps\n            with non-spatial dimensions, list input can be used to\n            define a different bin size in each image plane.\n        skydir : tuple or `~astropy.coordinates.SkyCoord`\n            Sky position of map center.  Can be either a SkyCoord\n            object or a tuple of longitude and latitude in deg in the\n            coordinate system of the map.\n        frame : {""icrs"", ""galactic""}, optional\n            Coordinate system, either Galactic (""galactic"") or Equatorial (""icrs"").\n        axes : list\n            List of non-spatial axes.\n        proj : string, optional\n            Any valid WCS projection type. Default is \'CAR\' (cartesian).\n        refpix : tuple\n            Reference pixel of the projection.  If None this will be\n            set to the center of the map.\n\n        Returns\n        -------\n        geom : `~WcsGeom`\n            A WCS geometry object.\n\n        Examples\n        --------\n        >>> from gammapy.maps import WcsGeom\n        >>> from gammapy.maps import MapAxis\n        >>> axis = MapAxis.from_bounds(0,1,2)\n        >>> geom = WcsGeom.create(npix=(100,100), binsz=0.1)\n        >>> geom = WcsGeom.create(npix=[100,200], binsz=[0.1,0.05], axes=[axis])\n        >>> geom = WcsGeom.create(width=[5.0,8.0], binsz=[0.1,0.05], axes=[axis])\n        >>> geom = WcsGeom.create(npix=([100,200],[100,200]), binsz=0.1, axes=[axis])\n        """"""\n        if skydir is None:\n            crval = (0.0, 0.0)\n        elif isinstance(skydir, tuple):\n            crval = skydir\n        elif isinstance(skydir, SkyCoord):\n            xref, yref, frame = skycoord_to_lonlat(skydir, frame=frame)\n            crval = (xref, yref)\n        else:\n            raise ValueError(f""Invalid type for skydir: {type(skydir)!r}"")\n\n        if width is not None:\n            width = _check_width(width)\n\n        shape = max([get_shape(t) for t in [npix, binsz, width]])\n        binsz = cast_to_shape(binsz, shape, float)\n\n        # If both npix and width are None then create an all-sky geometry\n        if npix is None and width is None:\n            width = (360.0, 180.0)\n\n        if npix is None:\n            width = cast_to_shape(width, shape, float)\n            npix = (\n                np.rint(width[0] / binsz[0]).astype(int),\n                np.rint(width[1] / binsz[1]).astype(int),\n            )\n        else:\n            npix = cast_to_shape(npix, shape, int)\n\n        if refpix is None:\n            nxpix = int(npix[0].flat[0])\n            nypix = int(npix[1].flat[0])\n            refpix = ((nxpix + 1) / 2.0, (nypix + 1) / 2.0)\n\n        # get frame class\n        frame = SkyCoord(np.nan, np.nan, frame=frame, unit=""deg"").frame\n        wcs = celestial_frame_to_wcs(frame, projection=proj)\n        wcs.wcs.crpix = refpix\n        wcs.wcs.crval = crval\n\n        cdelt = binsz[0].flat[0]\n        wcs.wcs.cdelt = (-cdelt, cdelt)\n\n        wcs.array_shape = npix[0].flat[0], npix[1].flat[0]\n        wcs.wcs.datfix()\n        return cls(wcs, npix, cdelt=binsz, axes=axes)\n\n    @classmethod\n    def from_header(cls, header, hdu_bands=None):\n        """"""Create a WCS geometry object from a FITS header.\n\n        Parameters\n        ----------\n        header : `~astropy.io.fits.Header`\n            The FITS header\n        hdu_bands : `~astropy.io.fits.BinTableHDU`\n            The BANDS table HDU.\n\n        Returns\n        -------\n        wcs : `~WcsGeom`\n            WCS geometry object.\n        """"""\n        wcs = WCS(header, naxis=(1, 2))\n\n        axes = find_and_read_bands(hdu_bands)\n        shape = tuple([ax.nbin for ax in axes])\n\n        if hdu_bands is not None and ""NPIX"" in hdu_bands.columns.names:\n            npix = hdu_bands.data.field(""NPIX"").reshape(shape + (2,))\n            npix = (npix[..., 0], npix[..., 1])\n            cdelt = hdu_bands.data.field(""CDELT"").reshape(shape + (2,))\n            cdelt = (cdelt[..., 0], cdelt[..., 1])\n        elif ""WCSSHAPE"" in header:\n            wcs_shape = eval(header[""WCSSHAPE""])\n            npix = (wcs_shape[0], wcs_shape[1])\n            cdelt = None\n        else:\n            npix = (header[""NAXIS1""], header[""NAXIS2""])\n            cdelt = None\n\n        if ""PSLICE1"" in header:\n            cutout_info = {}\n            cutout_info[""parent-slices""] = (\n                str_to_slice(header[""PSLICE2""]),\n                str_to_slice(header[""PSLICE1""]),\n            )\n            cutout_info[""cutout-slices""] = (\n                str_to_slice(header[""CSLICE2""]),\n                str_to_slice(header[""CSLICE1""]),\n            )\n        else:\n            cutout_info = None\n\n        return cls(wcs, npix, cdelt=cdelt, axes=axes, cutout_info=cutout_info)\n\n    def _make_bands_cols(self, hdu=None, conv=None):\n\n        cols = []\n        if not self.is_regular:\n            cols += [\n                fits.Column(\n                    ""NPIX"",\n                    ""2I"",\n                    dim=""(2)"",\n                    array=np.vstack((np.ravel(self.npix[0]), np.ravel(self.npix[1]))).T,\n                )\n            ]\n            cols += [\n                fits.Column(\n                    ""CDELT"",\n                    ""2E"",\n                    dim=""(2)"",\n                    array=np.vstack(\n                        (np.ravel(self._cdelt[0]), np.ravel(self._cdelt[1]))\n                    ).T,\n                )\n            ]\n            cols += [\n                fits.Column(\n                    ""CRPIX"",\n                    ""2E"",\n                    dim=""(2)"",\n                    array=np.vstack(\n                        (np.ravel(self._crpix[0]), np.ravel(self._crpix[1]))\n                    ).T,\n                )\n            ]\n        return cols\n\n    def make_header(self):\n        header = self.wcs.to_header()\n        self._fill_header_from_axes(header)\n        shape = ""{},{}"".format(np.max(self.npix[0]), np.max(self.npix[1]))\n        for ax in self.axes:\n            shape += f"",{ax.nbin}""\n        header[""WCSSHAPE""] = f""({shape})""\n\n        if self.cutout_info is not None:\n            slices_parent = self.cutout_info[""parent-slices""]\n            slices_cutout = self.cutout_info[""cutout-slices""]\n\n            header[""PSLICE1""] = (slice_to_str(slices_parent[1]), ""Parent slice"")\n            header[""PSLICE2""] = (slice_to_str(slices_parent[0]), ""Parent slice"")\n            header[""CSLICE1""] = (slice_to_str(slices_cutout[1]), ""Cutout slice"")\n            header[""CSLICE2""] = (slice_to_str(slices_cutout[0]), ""Cutout slice"")\n\n        return header\n\n    def get_image_shape(self, idx):\n        """"""Get the shape of the image plane at index ``idx``.""""""\n        if self.is_regular:\n            return int(self.npix[0]), int(self.npix[1])\n        else:\n            return int(self.npix[0][idx]), int(self.npix[1][idx])\n\n    def get_idx(self, idx=None, flat=False):\n        pix = self.get_pix(idx=idx, mode=""center"")\n        if flat:\n            pix = tuple([p[np.isfinite(p)] for p in pix])\n        return pix_tuple_to_idx(pix)\n\n    def _get_pix_all(self, idx=None, mode=""center""):\n        """"""Get idx coordinate array without footprint of the projection applied""""""\n        if mode == ""edges"":\n            shape = self._shape_edges\n        else:\n            shape = self._shape\n\n        if idx is None:\n            pix = [np.arange(n, dtype=float) for n in shape]\n        else:\n            pix = [np.arange(n, dtype=float) for n in shape[self._slice_spatial_axes]]\n            pix += [float(t) for t in idx]\n\n        if mode == ""edges"":\n            for pix_array in pix[self._slice_spatial_axes]:\n                pix_array -= 0.5\n\n        pix = np.meshgrid(*pix[::-1], indexing=""ij"")[::-1]\n        return pix\n\n    def get_pix(self, idx=None, mode=""center""):\n        """"""Get map pix coordinates from the geometry.\n\n        Parameters\n        ----------\n        mode : {\'center\', \'edges\'}\n            Get center or edge pix coordinates for the spatial axes.\n\n        Returns\n        -------\n        coord : tuple\n            Map pix coordinate tuple.\n        """"""\n        pix = self._get_pix_all(idx=idx, mode=mode)\n        coords = self.pix_to_coord(pix)\n        m = np.isfinite(coords[0])\n        for _ in pix:\n            _[~m] = INVALID_INDEX.float\n        return pix\n\n    def get_coord(self, idx=None, flat=False, mode=""center"", frame=None):\n        """"""Get map coordinates from the geometry.\n\n        Parameters\n        ----------\n        mode : {\'center\', \'edges\'}\n            Get center or edge coordinates for the spatial axes.\n\n        Returns\n        -------\n        coord : `~MapCoord`\n            Map coordinate object.\n        """"""\n        pix = self._get_pix_all(idx=idx, mode=mode)\n        coords = self.pix_to_coord(pix)\n\n        if flat:\n            is_finite = np.isfinite(coords[0])\n            coords = tuple([c[is_finite] for c in coords])\n\n        axes_names = [""lon"", ""lat""] + [ax.name for ax in self.axes]\n        cdict = dict(zip(axes_names, coords))\n\n        if frame is None:\n            frame = self.frame\n\n        return MapCoord.create(cdict, frame=self.frame).to_frame(frame)\n\n    def coord_to_pix(self, coords):\n        coords = MapCoord.create(coords, frame=self.frame)\n\n        if coords.size == 0:\n            return tuple([np.array([]) for i in range(coords.ndim)])\n\n        c = self.coord_to_tuple(coords)\n        # Variable Bin Size\n        if not self.is_regular:\n            idxs = tuple(\n                [\n                    np.clip(ax.coord_to_idx(c[i + 2]), 0, ax.nbin - 1)\n                    for i, ax in enumerate(self.axes)\n                ]\n            )\n            crpix = [t[idxs] for t in self._crpix]\n            cdelt = [t[idxs] for t in self._cdelt]\n            pix = world2pix(self.wcs, cdelt, crpix, (coords.lon, coords.lat))\n            pix = list(pix)\n        else:\n            pix = self._wcs.wcs_world2pix(coords.lon, coords.lat, 0)\n\n        for coord, ax in zip(c[self._slice_non_spatial_axes], self.axes):\n            pix += [ax.coord_to_pix(coord)]\n\n        return tuple(pix)\n\n    def pix_to_coord(self, pix):\n        # Variable Bin Size\n        if not self.is_regular:\n            idxs = pix_tuple_to_idx(pix[self._slice_non_spatial_axes])\n            crpix = [t[idxs] for t in self._crpix]\n            cdelt = [t[idxs] for t in self._cdelt]\n            coords = pix2world(self.wcs, cdelt, crpix, pix[self._slice_spatial_axes])\n        else:\n            coords = self._wcs.wcs_pix2world(pix[0], pix[1], 0)\n\n        coords = [\n            u.Quantity(coords[0], unit=""deg"", copy=False),\n            u.Quantity(coords[1], unit=""deg"", copy=False),\n        ]\n\n        for ax, t in zip(self.axes, pix[self._slice_non_spatial_axes]):\n            coords += [ax.pix_to_coord(t)]\n\n        return tuple(coords)\n\n    def pix_to_idx(self, pix, clip=False):\n        # TODO: copy idx to avoid modifying input pix?\n        # pix_tuple_to_idx seems to always make a copy!?\n        idxs = pix_tuple_to_idx(pix)\n        if not self.is_regular:\n            ibin = pix[self._slice_non_spatial_axes]\n            ibin = pix_tuple_to_idx(ibin)\n            for i, ax in enumerate(self.axes):\n                np.clip(ibin[i], 0, ax.nbin - 1, out=ibin[i])\n            npix = (self.npix[0][ibin], self.npix[1][ibin])\n        else:\n            npix = self.npix\n\n        for i, idx in enumerate(idxs):\n            if clip:\n                if i < 2:\n                    np.clip(idxs[i], 0, npix[i], out=idxs[i])\n                else:\n                    np.clip(idxs[i], 0, self.axes[i - 2].nbin - 1, out=idxs[i])\n            else:\n                if i < 2:\n                    np.putmask(idxs[i], (idx < 0) | (idx >= npix[i]), -1)\n                else:\n                    np.putmask(idxs[i], (idx < 0) | (idx >= self.axes[i - 2].nbin), -1)\n\n        return idxs\n\n    def contains(self, coords):\n        idx = self.coord_to_idx(coords)\n        return np.all(np.stack([t != INVALID_INDEX.int for t in idx]), axis=0)\n\n    def to_image(self):\n        npix = (np.max(self._npix[0]), np.max(self._npix[1]))\n        cdelt = (np.max(self._cdelt[0]), np.max(self._cdelt[1]))\n        return self.__class__(\n            self._wcs, npix, cdelt=cdelt, cutout_info=self.cutout_info\n        )\n\n    def to_cube(self, axes):\n        npix = (np.max(self._npix[0]), np.max(self._npix[1]))\n        cdelt = (np.max(self._cdelt[0]), np.max(self._cdelt[1]))\n        axes = copy.deepcopy(self.axes) + axes\n        return self.__class__(\n            self._wcs.deepcopy(),\n            npix,\n            cdelt=cdelt,\n            axes=axes,\n            cutout_info=self.cutout_info,\n        )\n\n    def squash(self, axis):\n        """"""Squash geom axis.\n\n        Parameters\n        ----------\n        axis : str\n            Axis to squash.\n\n        Returns\n        -------\n        geom : `Geom`\n            Geom with squashed axis.\n        """"""\n        _ = self.get_axis_by_name(axis)\n        npix = (np.max(self._npix[0]), np.max(self._npix[1]))\n        cdelt = (np.max(self._cdelt[0]), np.max(self._cdelt[1]))\n\n        axes = []\n        for ax in copy.deepcopy(self.axes):\n            if ax.name == axis:\n                ax = ax.squash()\n            axes.append(ax)\n\n        return self.__class__(\n            self._wcs.deepcopy(),\n            npix,\n            cdelt=cdelt,\n            axes=axes,\n            cutout_info=self.cutout_info,\n        )\n\n    def drop(self, axis):\n        """"""Drop an axis from the geom.\n\n        Parameters\n        ----------\n        axis : str\n            Name of the axis to remove.\n\n        Returns\n            -------\n        geom : `Geom`\n            New geom with the axis removed.\n        """"""\n        _ = self.get_axis_by_name(axis)\n        npix = (np.max(self._npix[0]), np.max(self._npix[1]))\n        cdelt = (np.max(self._cdelt[0]), np.max(self._cdelt[1]))\n\n        axes = []\n        for ax in copy.deepcopy(self.axes):\n            if ax.name == axis:\n                continue\n            axes.append(ax)\n\n        return self.__class__(\n            self._wcs.deepcopy(),\n            npix,\n            cdelt=cdelt,\n            axes=axes,\n            cutout_info=self.cutout_info,\n        )\n\n    def pad(self, pad_width):\n        if np.isscalar(pad_width):\n            pad_width = (pad_width, pad_width)\n\n        npix = (self.npix[0] + 2 * pad_width[0], self.npix[1] + 2 * pad_width[1])\n        wcs = self._wcs.deepcopy()\n        wcs.wcs.crpix += np.array(pad_width)\n        cdelt = copy.deepcopy(self._cdelt)\n        return self.__class__(wcs, npix, cdelt=cdelt, axes=copy.deepcopy(self.axes))\n\n    def crop(self, crop_width):\n        if np.isscalar(crop_width):\n            crop_width = (crop_width, crop_width)\n\n        npix = (self.npix[0] - 2 * crop_width[0], self.npix[1] - 2 * crop_width[1])\n        wcs = self._wcs.deepcopy()\n        wcs.wcs.crpix -= np.array(crop_width)\n        cdelt = copy.deepcopy(self._cdelt)\n        return self.__class__(wcs, npix, cdelt=cdelt, axes=copy.deepcopy(self.axes))\n\n    def downsample(self, factor, axis=None):\n        if axis is None:\n            if np.any(np.mod(self.npix, factor) > 0):\n                raise ValueError(\n                    f""Spatial shape not divisible by factor {factor!r} in all axes.""\n                    f"" You need to pad prior to calling downsample.""\n                )\n\n            npix = (self.npix[0] / factor, self.npix[1] / factor)\n            cdelt = (self._cdelt[0] * factor, self._cdelt[1] * factor)\n            wcs = get_resampled_wcs(self.wcs, factor, True)\n            return self._init_copy(wcs=wcs, npix=npix, cdelt=cdelt)\n        else:\n            if not self.is_regular:\n                raise NotImplementedError(\n                    ""Upsampling in non-spatial axes not supported for irregular geometries""\n                )\n\n            axes = copy.deepcopy(self.axes)\n            idx = self.get_axis_index_by_name(axis)\n            axes[idx] = axes[idx].downsample(factor)\n            return self._init_copy(axes=axes)\n\n    def upsample(self, factor, axis=None):\n        if axis is None:\n            npix = (self.npix[0] * factor, self.npix[1] * factor)\n            cdelt = (self._cdelt[0] / factor, self._cdelt[1] / factor)\n            wcs = get_resampled_wcs(self.wcs, factor, False)\n            return self._init_copy(wcs=wcs, npix=npix, cdelt=cdelt)\n        else:\n            if not self.is_regular:\n                raise NotImplementedError(\n                    ""Upsampling in non-spatial axes not supported for irregular geometries""\n                )\n            axes = copy.deepcopy(self.axes)\n            idx = self.get_axis_index_by_name(axis)\n            axes[idx] = axes[idx].upsample(factor)\n            return self._init_copy(axes=axes)\n\n    def to_binsz(self, binsz):\n        """"""Change pixel size of the geometry.\n\n        Parameters\n        ----------\n        binsz : float or tuple or list\n            New pixel size in degree.\n\n        Returns\n        -------\n        geom : `WcsGeom`\n            Geometry with new pixel size.\n        """"""\n        return self.create(\n            skydir=self.center_skydir,\n            binsz=binsz,\n            width=self.width,\n            proj=self.projection,\n            frame=self.frame,\n            axes=copy.deepcopy(self.axes),\n        )\n\n    def solid_angle(self):\n        """"""Solid angle array (`~astropy.units.Quantity` in ``sr``).\n\n        The array has the same dimension as the WcsGeom object.\n\n        To return solid angles for the spatial dimensions only use::\n\n            WcsGeom.to_image().solid_angle()\n        """"""\n        coord = self.get_coord(mode=""edges"").skycoord\n\n        # define pixel corners\n        low_left = coord[..., :-1, :-1]\n        low_right = coord[..., 1:, :-1]\n        up_left = coord[..., :-1, 1:]\n        up_right = coord[..., 1:, 1:]\n\n        # compute side lengths\n        low = low_left.separation(low_right)\n        left = low_left.separation(up_left)\n        up = up_left.separation(up_right)\n        right = low_right.separation(up_right)\n\n        # compute enclosed angles\n        angle_low_right = low_right.position_angle(up_right) - low_right.position_angle(\n            low_left\n        )\n        angle_up_left = up_left.position_angle(up_right) - low_left.position_angle(\n            up_left\n        )\n\n        # compute area assuming a planar triangle\n        area_low_right = 0.5 * low * right * np.sin(angle_low_right)\n        area_up_left = 0.5 * up * left * np.sin(angle_up_left)\n\n        return u.Quantity(area_low_right + area_up_left, ""sr"", copy=False)\n\n    def bin_volume(self):\n        """"""Bin volume (`~astropy.units.Quantity`)""""""\n        bin_volume = self.to_image().solid_angle()\n\n        for idx, ax in enumerate(self.axes):\n            shape = self.ndim * [1]\n            shape[-(idx + 3)] = -1\n            bin_volume = bin_volume * ax.bin_width.reshape(tuple(shape))\n\n        return bin_volume\n\n    def separation(self, center):\n        """"""Compute sky separation wrt a given center.\n\n        Parameters\n        ----------\n        center : `~astropy.coordinates.SkyCoord`\n            Center position\n\n        Returns\n        -------\n        separation : `~astropy.coordinates.Angle`\n            Separation angle array (2D)\n        """"""\n        coord = self.to_image().get_coord()\n        return center.separation(coord.skycoord)\n\n    def cutout(self, position, width, mode=""trim""):\n        """"""\n        Create a cutout around a given position.\n\n        Parameters\n        ----------\n        position : `~astropy.coordinates.SkyCoord`\n            Center position of the cutout region.\n        width : tuple of `~astropy.coordinates.Angle`\n            Angular sizes of the region in (lon, lat) in that specific order.\n            If only one value is passed, a square region is extracted.\n        mode : {\'trim\', \'partial\', \'strict\'}\n            Mode option for Cutout2D, for details see `~astropy.nddata.utils.Cutout2D`.\n\n        Returns\n        -------\n        cutout : `~gammapy.maps.WcsNDMap`\n            Cutout map\n        """"""\n        width = _check_width(width)\n        dummy_data = np.empty(self.to_image().data_shape)\n        c2d = Cutout2D(\n            data=dummy_data,\n            wcs=self.wcs,\n            position=position,\n            # Cutout2D takes size with order (lat, lon)\n            size=width[::-1] * u.deg,\n            mode=mode,\n        )\n\n        cutout_info = {\n            ""parent-slices"": c2d.slices_original,\n            ""cutout-slices"": c2d.slices_cutout,\n        }\n\n        return self._init_copy(\n            wcs=c2d.wcs, npix=c2d.shape[::-1], cutout_info=cutout_info\n        )\n\n    def region_mask(self, regions, inside=True):\n        """"""Create a mask from a given list of regions\n\n        Parameters\n        ----------\n        regions : list of  `~regions.Region`\n            Python list of regions (pixel or sky regions accepted)\n        inside : bool\n            For ``inside=True``, pixels in the region to True (the default).\n            For ``inside=False``, pixels in the region are False.\n\n        Returns\n        -------\n        mask_map : `~numpy.ndarray` of boolean type\n            Boolean region mask\n\n        Examples\n        --------\n        Make an exclusion mask for a circular region::\n\n            from regions import CircleSkyRegion\n            from astropy.coordinates import SkyCoord, Angle\n            from gammapy.maps import WcsNDMap, WcsGeom\n\n            pos = SkyCoord(0, 0, unit=\'deg\')\n            geom = WcsGeom.create(skydir=pos, npix=100, binsz=0.1)\n\n            region = CircleSkyRegion(\n                SkyCoord(3, 2, unit=\'deg\'),\n                Angle(1, \'deg\'),\n            )\n            mask = geom.region_mask([region], inside=False)\n\n        Note how we made a list with a single region,\n        since this method expects a list of regions.\n\n        The return ``mask`` is a boolean Numpy array.\n        If you want a map object (e.g. for storing in FITS or plotting),\n        this is how you can make the map::\n\n            mask_map = WcsNDMap(geom=geom, data=mask)\n            mask_map.plot()\n        """"""\n        from regions import PixCoord\n\n        if not self.is_regular:\n            raise ValueError(""Multi-resolution maps not supported yet"")\n\n        idx = self.get_idx()\n        pixcoord = PixCoord(idx[0], idx[1])\n\n        mask = np.zeros(self.data_shape, dtype=bool)\n\n        for region in regions:\n            if isinstance(region, SkyRegion):\n                region = region.to_pixel(self.wcs)\n            mask += region.contains(pixcoord)\n\n        if inside is False:\n            np.logical_not(mask, out=mask)\n\n        return mask\n\n    def __repr__(self):\n        axes = [""lon"", ""lat""] + [_.name for _ in self.axes]\n        lon = self.center_skydir.data.lon.deg\n        lat = self.center_skydir.data.lat.deg\n\n        return (\n            f""{self.__class__.__name__}\\n\\n""\n            f""\\taxes       : {axes}\\n""\n            f""\\tshape      : {self.data_shape[::-1]}\\n""\n            f""\\tndim       : {self.ndim}\\n""\n            f""\\tframe      : {self.frame}\\n""\n            f""\\tprojection : {self.projection}\\n""\n            f""\\tcenter     : {lon:.1f} deg, {lat:.1f} deg\\n""\n            f""\\twidth      : {self.width[0][0]:.1f} x {self.width[1][0]:.1f}\\n""\n        )\n\n    def is_aligned(self, other, tolerance=1e-6):\n        """"""Check if WCS and extra axes are aligned.\n\n        Parameters\n        ----------\n        other : `WcsGeom`\n            Other geom.\n        tolerance : float\n            Tolerance for the comparison.\n\n        Returns\n        -------\n        aligned : bool\n            Whether geometries are aligned\n        """"""\n        for axis, otheraxis in zip(self.axes, other.axes):\n            if axis != otheraxis:\n                return False\n\n        # check WCS consistency with a priori tolerance of 1e-6\n        return self.wcs.wcs.compare(other.wcs.wcs, cmp=2, tolerance=tolerance)\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n\n        if not (self.is_regular and other.is_regular):\n            raise NotImplementedError(""Geom comparison is not possible for irregular geometries."")\n\n        # check overall shape and axes compatibility\n        if self.data_shape != other.data_shape:\n            return False\n\n        for axis, otheraxis in zip(self.axes, other.axes):\n            if axis != otheraxis:\n                return False\n\n        # check WCS consistency with a priori tolerance of 1e-6\n        # cmp=1 parameter ensures no comparison with ancillary information\n        # see https://github.com/astropy/astropy/pull/4522/files\n        return self.wcs.wcs.compare(other.wcs.wcs, cmp=1, tolerance=1e-6)\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        return id(self)\n\n\ndef pix2world(wcs, cdelt, crpix, pix):\n    """"""Perform pixel to world coordinate transformation.\n\n    For a WCS projection with a given pixel size (CDELT) and reference pixel\n    (CRPIX). This method can be used to perform WCS transformations\n    for projections with different pixelizations but the same\n    reference coordinate (CRVAL), projection type, and coordinate system.\n\n    Parameters\n    ----------\n    wcs : `astropy.wcs.WCS`\n        WCS transform object.\n    cdelt : tuple\n        Tuple of X/Y pixel size in deg.  Each element should have the\n        same length as ``pix``.\n    crpix : tuple\n        Tuple of reference pixel parameters in X and Y dimensions.  Each\n        element should have the same length as ``pix``.\n    pix : tuple\n        Tuple of pixel coordinates.\n    """"""\n    pix_ratio = [\n        np.abs(wcs.wcs.cdelt[0] / cdelt[0]),\n        np.abs(wcs.wcs.cdelt[1] / cdelt[1]),\n    ]\n    pix = (\n        (pix[0] - (crpix[0] - 1.0)) / pix_ratio[0] + wcs.wcs.crpix[0] - 1.0,\n        (pix[1] - (crpix[1] - 1.0)) / pix_ratio[1] + wcs.wcs.crpix[1] - 1.0,\n    )\n    return wcs.wcs_pix2world(pix[0], pix[1], 0)\n\n\ndef world2pix(wcs, cdelt, crpix, coord):\n    pix_ratio = [\n        np.abs(wcs.wcs.cdelt[0] / cdelt[0]),\n        np.abs(wcs.wcs.cdelt[1] / cdelt[1]),\n    ]\n    pix = wcs.wcs_world2pix(coord[0], coord[1], 0)\n    return (\n        (pix[0] - (wcs.wcs.crpix[0] - 1.0)) * pix_ratio[0] + crpix[0] - 1.0,\n        (pix[1] - (wcs.wcs.crpix[1] - 1.0)) * pix_ratio[1] + crpix[1] - 1.0,\n    )\n'"
gammapy/maps/wcsmap.py,16,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport json\nimport numpy as np\nfrom astropy.io import fits\nfrom .base import Map\nfrom .utils import find_bands_hdu, find_hdu\nfrom .wcs import WcsGeom\n\n__all__ = [""WcsMap""]\n\n\nclass WcsMap(Map):\n    """"""Base class for WCS map classes.\n\n    Parameters\n    ----------\n    geom : `~gammapy.maps.WcsGeom`\n        A WCS geometry object.\n    data : `~numpy.ndarray`\n        Data array.\n    """"""\n\n    @classmethod\n    def create(\n        cls,\n        map_type=""wcs"",\n        npix=None,\n        binsz=0.1,\n        width=None,\n        proj=""CAR"",\n        frame=""icrs"",\n        refpix=None,\n        axes=None,\n        skydir=None,\n        dtype=""float32"",\n        meta=None,\n        unit="""",\n    ):\n        """"""Factory method to create an empty WCS map.\n\n        Parameters\n        ----------\n        map_type : {\'wcs\', \'wcs-sparse\'}\n            Map type.  Selects the class that will be used to\n            instantiate the map.\n        npix : int or tuple or list\n            Width of the map in pixels. A tuple will be interpreted as\n            parameters for longitude and latitude axes.  For maps with\n            non-spatial dimensions, list input can be used to define a\n            different map width in each image plane.  This option\n            supersedes width.\n        width : float or tuple or list\n            Width of the map in degrees.  A tuple will be interpreted\n            as parameters for longitude and latitude axes.  For maps\n            with non-spatial dimensions, list input can be used to\n            define a different map width in each image plane.\n        binsz : float or tuple or list\n            Map pixel size in degrees.  A tuple will be interpreted\n            as parameters for longitude and latitude axes.  For maps\n            with non-spatial dimensions, list input can be used to\n            define a different bin size in each image plane.\n        skydir : tuple or `~astropy.coordinates.SkyCoord`\n            Sky position of map center.  Can be either a SkyCoord\n            object or a tuple of longitude and latitude in deg in the\n            coordinate system of the map.\n        frame : {""icrs"", ""galactic""}, optional\n            Coordinate system, either Galactic (""galactic"") or Equatorial (""icrs"").\n        axes : list\n            List of non-spatial axes.\n        proj : string, optional\n            Any valid WCS projection type. Default is \'CAR\' (cartesian).\n        refpix : tuple\n            Reference pixel of the projection.  If None then this will\n            be chosen to be center of the map.\n        dtype : str, optional\n            Data type, default is float32\n        conv : {\'fgst-ccube\',\'fgst-template\',\'gadf\'}, optional\n            FITS format convention.  Default is \'gadf\'.\n        meta : `dict`\n            Dictionary to store meta data.\n        unit : str or `~astropy.units.Unit`\n            The unit of the map\n\n        Returns\n        -------\n        map : `~WcsMap`\n            A WCS map object.\n        """"""\n        from .wcsnd import WcsNDMap\n\n        geom = WcsGeom.create(\n            npix=npix,\n            binsz=binsz,\n            width=width,\n            proj=proj,\n            skydir=skydir,\n            frame=frame,\n            refpix=refpix,\n            axes=axes,\n        )\n\n        if map_type == ""wcs"":\n            return WcsNDMap(geom, dtype=dtype, meta=meta, unit=unit)\n        elif map_type == ""wcs-sparse"":\n            raise NotImplementedError\n        else:\n            raise ValueError(f""Invalid map type: {map_type!r}"")\n\n    @classmethod\n    def from_hdulist(cls, hdu_list, hdu=None, hdu_bands=None):\n        """"""Make a WcsMap object from a FITS HDUList.\n\n        Parameters\n        ----------\n        hdu_list :  `~astropy.io.fits.HDUList`\n            HDU list containing HDUs for map data and bands.\n        hdu : str\n            Name or index of the HDU with the map data.\n        hdu_bands : str\n            Name or index of the HDU with the BANDS table.\n\n        Returns\n        -------\n        wcs_map : `WcsMap`\n            Map object\n        """"""\n        if hdu is None:\n            hdu = find_hdu(hdu_list)\n        else:\n            hdu = hdu_list[hdu]\n\n        if hdu_bands is None:\n            hdu_bands = find_bands_hdu(hdu_list, hdu)\n\n        if hdu_bands is not None:\n            hdu_bands = hdu_list[hdu_bands]\n\n        return cls.from_hdu(hdu, hdu_bands)\n\n    def to_hdulist(self, hdu=None, hdu_bands=None, sparse=False, conv=""gadf""):\n        """"""Convert to `~astropy.io.fits.HDUList`.\n\n        Parameters\n        ----------\n        hdu : str\n            Name or index of the HDU with the map data.\n        hdu_bands : str\n            Name or index of the HDU with the BANDS table.\n        sparse : bool\n            Sparsify the map by only writing pixels with non-zero\n            amplitude.\n        conv : {\'gadf\', \'fgst-ccube\',\'fgst-template\'}\n            FITS format convention.\n\n        Returns\n        -------\n        hdu_list : `~astropy.io.fits.HDUList`\n\n        """"""\n        if sparse:\n            hdu = ""SKYMAP"" if hdu is None else hdu.upper()\n        else:\n            hdu = ""PRIMARY"" if hdu is None else hdu.upper()\n\n        if sparse and hdu == ""PRIMARY"":\n            raise ValueError(""Sparse maps cannot be written to the PRIMARY HDU."")\n\n        if conv in [""fgst-ccube"", ""fgst-template""]:\n            if self.geom.axes[0].name != ""energy"" or len(self.geom.axes) > 1:\n                raise ValueError(\n                    ""All \'fgst\' formats don\'t support extra axes except for energy.""\n                )\n\n        if self.geom.axes:\n            hdu_bands_out = self.geom.make_bands_hdu(\n                hdu=hdu_bands, hdu_skymap=hdu, conv=conv\n            )\n            hdu_bands = hdu_bands_out.name\n        else:\n            hdu_bands = None\n\n        hdu_out = self.make_hdu(hdu=hdu, hdu_bands=hdu_bands, sparse=sparse, conv=conv)\n\n        hdu_out.header[""META""] = json.dumps(self.meta)\n\n        hdu_out.header[""BUNIT""] = self.unit.to_string(""fits"")\n\n        if hdu == ""PRIMARY"":\n            hdulist = [hdu_out]\n        else:\n            hdulist = [fits.PrimaryHDU(), hdu_out]\n\n        if self.geom.axes:\n            hdulist += [hdu_bands_out]\n\n        return fits.HDUList(hdulist)\n\n    def make_hdu(self, hdu=""SKYMAP"", hdu_bands=None, sparse=False, conv=None):\n        """"""Make a FITS HDU from this map.\n\n        Parameters\n        ----------\n        hdu : str\n            The HDU extension name.\n        hdu_bands : str\n            The HDU extension name for BANDS table.\n        sparse : bool\n            Set INDXSCHM to SPARSE and sparsify the map by only\n            writing pixels with non-zero amplitude.\n\n        Returns\n        -------\n        hdu : `~astropy.io.fits.BinTableHDU` or `~astropy.io.fits.ImageHDU`\n            HDU containing the map data.\n        """"""\n        header = self.geom.make_header()\n\n        if hdu_bands is not None:\n            header[""BANDSHDU""] = hdu_bands\n\n        if sparse:\n            hdu_out = self._make_hdu_sparse(self.data, self.geom.npix, hdu, header)\n        elif hdu == ""PRIMARY"":\n            hdu_out = fits.PrimaryHDU(self.data, header=header)\n        else:\n            hdu_out = fits.ImageHDU(self.data, header=header, name=hdu)\n\n        return hdu_out\n\n    @staticmethod\n    def _make_hdu_sparse(data, npix, hdu, header):\n        shape = data.shape\n\n        # We make a copy, because below we modify `data` to handle non-finite entries\n        # TODO: The code below could probably be simplified to use expressions\n        # that create new arrays instead of in-place modifications\n        # But first: do we want / need the non-finite entry handling at all and always cast to 64-bit float?\n        data = data.copy()\n\n        if len(shape) == 2:\n            data_flat = np.ravel(data)\n            data_flat[~np.isfinite(data_flat)] = 0\n            nonzero = np.where(data_flat > 0)\n            value = data_flat[nonzero].astype(float)\n            cols = [\n                fits.Column(""PIX"", ""J"", array=nonzero[0]),\n                fits.Column(""VALUE"", ""E"", array=value),\n            ]\n        elif npix[0].size == 1:\n            shape_flat = shape[:-2] + (shape[-1] * shape[-2],)\n            data_flat = np.ravel(data).reshape(shape_flat)\n            data_flat[~np.isfinite(data_flat)] = 0\n            nonzero = np.where(data_flat > 0)\n            channel = np.ravel_multi_index(nonzero[:-1], shape[:-2])\n            value = data_flat[nonzero].astype(float)\n            cols = [\n                fits.Column(""PIX"", ""J"", array=nonzero[-1]),\n                fits.Column(""CHANNEL"", ""I"", array=channel),\n                fits.Column(""VALUE"", ""E"", array=value),\n            ]\n        else:\n            data_flat = []\n            channel = []\n            pix = []\n            for i, _ in np.ndenumerate(npix[0]):\n                data_i = np.ravel(data[i[::-1]])\n                data_i[~np.isfinite(data_i)] = 0\n                pix_i = np.where(data_i > 0)\n                data_i = data_i[pix_i]\n                data_flat += [data_i]\n                pix += pix_i\n                channel += [\n                    np.ones(data_i.size, dtype=int)\n                    * np.ravel_multi_index(i[::-1], shape[:-2])\n                ]\n\n            pix = np.concatenate(pix)\n            channel = np.concatenate(channel)\n            value = np.concatenate(data_flat).astype(float)\n\n            cols = [\n                fits.Column(""PIX"", ""J"", array=pix),\n                fits.Column(""CHANNEL"", ""I"", array=channel),\n                fits.Column(""VALUE"", ""E"", array=value),\n            ]\n\n        return fits.BinTableHDU.from_columns(cols, header=header, name=hdu)\n'"
gammapy/maps/wcsnd.py,35,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nfrom collections import OrderedDict\nimport numpy as np\nimport scipy.interpolate\nimport scipy.ndimage\nimport scipy.signal\nimport astropy.units as u\nfrom astropy.convolution import Tophat2DKernel\nfrom astropy.io import fits\nfrom gammapy.extern.skimage import block_reduce\nfrom gammapy.utils.interpolation import ScaledRegularGridInterpolator\nfrom gammapy.utils.random import InverseCDFSampler, get_random_state\nfrom gammapy.utils.units import unit_from_fits_image_hdu\nfrom .geom import MapCoord, pix_tuple_to_idx\nfrom .regionnd import RegionGeom, RegionNDMap\nfrom .utils import INVALID_INDEX, interp_to_order\nfrom .wcsmap import WcsGeom, WcsMap\n\n__all__ = [""WcsNDMap""]\n\nlog = logging.getLogger(__name__)\n\n\nclass WcsNDMap(WcsMap):\n    """"""HEALPix map with any number of non-spatial dimensions.\n\n    This class uses an ND numpy array to store map values. For maps with\n    non-spatial dimensions and variable pixel size it will allocate an\n    array with dimensions commensurate with the largest image plane.\n\n    Parameters\n    ----------\n    geom : `~gammapy.maps.WcsGeom`\n        WCS geometry object.\n    data : `~numpy.ndarray`\n        Data array. If none then an empty array will be allocated.\n    dtype : str, optional\n        Data type, default is float32\n    meta : `dict`\n        Dictionary to store meta data.\n    unit : str or `~astropy.units.Unit`\n        The map unit\n    """"""\n\n    def __init__(self, geom, data=None, dtype=""float32"", meta=None, unit=""""):\n        # TODO: Figure out how to mask pixels for integer data types\n\n        data_shape = geom.data_shape\n\n        if data is None:\n            data = self._make_default_data(geom, data_shape, dtype)\n\n        super().__init__(geom, data, meta, unit)\n\n    @staticmethod\n    def _make_default_data(geom, shape_np, dtype):\n        # Check whether corners of each image plane are valid\n\n        data = np.zeros(shape_np, dtype=dtype)\n\n        if not geom.is_regular or geom.is_allsky:\n            coords = geom.get_coord()\n            is_nan = np.isnan(coords.lon)\n            data[is_nan] = np.nan\n\n        return data\n\n    @classmethod\n    def from_hdu(cls, hdu, hdu_bands=None):\n        """"""Make a WcsNDMap object from a FITS HDU.\n\n        Parameters\n        ----------\n        hdu : `~astropy.io.fits.BinTableHDU` or `~astropy.io.fits.ImageHDU`\n            The map FITS HDU.\n        hdu_bands : `~astropy.io.fits.BinTableHDU`\n            The BANDS table HDU.\n        """"""\n        geom = WcsGeom.from_header(hdu.header, hdu_bands)\n        shape = tuple([ax.nbin for ax in geom.axes])\n        shape_wcs = tuple([np.max(geom.npix[0]), np.max(geom.npix[1])])\n\n        meta = cls._get_meta_from_header(hdu.header)\n        unit = unit_from_fits_image_hdu(hdu.header)\n\n        # TODO: Should we support extracting slices?\n        if isinstance(hdu, fits.BinTableHDU):\n            map_out = cls(geom, meta=meta, unit=unit)\n            pix = hdu.data.field(""PIX"")\n            pix = np.unravel_index(pix, shape_wcs[::-1])\n            vals = hdu.data.field(""VALUE"")\n            if ""CHANNEL"" in hdu.data.columns.names and shape:\n                chan = hdu.data.field(""CHANNEL"")\n                chan = np.unravel_index(chan, shape[::-1])\n                idx = chan + pix\n            else:\n                idx = pix\n\n            map_out.set_by_idx(idx[::-1], vals)\n        else:\n            map_out = cls(geom=geom, meta=meta, data=hdu.data, unit=unit)\n\n        return map_out\n\n    def get_by_idx(self, idx):\n        idx = pix_tuple_to_idx(idx)\n        return self.data.T[idx]\n\n    def interp_by_coord(self, coords, interp=None, fill_value=None):\n\n        if self.geom.is_regular:\n            pix = self.geom.coord_to_pix(coords)\n            return self.interp_by_pix(pix, interp=interp, fill_value=fill_value)\n        else:\n            return self._interp_by_coord_griddata(coords, interp=interp)\n\n    def interp_by_pix(self, pix, interp=None, fill_value=None):\n        """"""Interpolate map values at the given pixel coordinates.\n        """"""\n        if not self.geom.is_regular:\n            raise ValueError(""interp_by_pix only supported for regular geom."")\n\n        order = interp_to_order(interp)\n        if order == 0 or order == 1:\n            return self._interp_by_pix_linear_grid(\n                pix, order=order, fill_value=fill_value\n            )\n        elif order == 2 or order == 3:\n            return self._interp_by_pix_map_coordinates(pix, order=order)\n        else:\n            raise ValueError(f""Invalid interpolation order: {order!r}"")\n\n    def _interp_by_pix_linear_grid(self, pix, order=1, fill_value=None):\n        # TODO: Cache interpolator\n        method_lookup = {0: ""nearest"", 1: ""linear""}\n        try:\n            method = method_lookup[order]\n        except KeyError:\n            raise ValueError(f""Invalid interpolation order: {order!r}"")\n\n        grid_pix = [np.arange(n, dtype=float) for n in self.data.shape[::-1]]\n\n        if np.any(np.isfinite(self.data)):\n            data = self.data.copy().T\n            data[~np.isfinite(data)] = 0.0\n        else:\n            data = self.data.T\n\n        fn = ScaledRegularGridInterpolator(\n            grid_pix, data, fill_value=fill_value, bounds_error=False, method=method\n        )\n        return fn(tuple(pix), clip=False)\n\n    def _interp_by_pix_map_coordinates(self, pix, order=1):\n        pix = tuple(\n            [\n                np.array(x, ndmin=1)\n                if not isinstance(x, np.ndarray) or x.ndim == 0\n                else x\n                for x in pix\n            ]\n        )\n        return scipy.ndimage.map_coordinates(\n            self.data.T, pix, order=order, mode=""nearest""\n        )\n\n    def _interp_by_coord_griddata(self, coords, interp=None):\n        order = interp_to_order(interp)\n        method_lookup = {0: ""nearest"", 1: ""linear"", 3: ""cubic""}\n        method = method_lookup.get(order, None)\n        if method is None:\n            raise ValueError(f""Invalid interp: {interp!r}"")\n\n        grid_coords = tuple(self.geom.get_coord(flat=True))\n        data = self.data[np.isfinite(self.data)]\n        vals = scipy.interpolate.griddata(\n            grid_coords, data, tuple(coords), method=method\n        )\n\n        m = ~np.isfinite(vals)\n        if np.any(m):\n            vals_fill = scipy.interpolate.griddata(\n                grid_coords, data, tuple([c[m] for c in coords]), method=""nearest""\n            )\n            vals[m] = vals_fill\n\n        return vals\n\n    def fill_by_idx(self, idx, weights=None):\n        idx = pix_tuple_to_idx(idx)\n        msk = np.all(np.stack([t != INVALID_INDEX.int for t in idx]), axis=0)\n        idx = [t[msk] for t in idx]\n\n        if weights is not None:\n            if isinstance(weights, u.Quantity):\n                weights = weights.to_value(self.unit)\n            weights = weights[msk]\n\n        idx = np.ravel_multi_index(idx, self.data.T.shape)\n        idx, idx_inv = np.unique(idx, return_inverse=True)\n        weights = np.bincount(idx_inv, weights=weights).astype(self.data.dtype)\n        self.data.T.flat[idx] += weights\n\n    def set_by_idx(self, idx, vals):\n        idx = pix_tuple_to_idx(idx)\n        self.data.T[idx] = vals\n\n    def sum_over_axes(self, axes=None, keepdims=False):\n        """"""To sum map values over all non-spatial axes.\n\n        Parameters\n        ----------\n        keepdims : bool, optional\n            If this is set to true, the axes which are summed over are left in\n            the map with a single bin\n        axes: list\n            Names of MapAxis to reduce over\n            If None, all will summed over\n\n        Returns\n        -------\n        map_out : `~WcsNDMap`\n            Map with non-spatial axes summed over\n        """"""\n        return self.reduce_over_axes(func=np.add, axes=axes, keepdims=keepdims)\n\n    def reduce_over_axes(self, func=np.add, keepdims=False, axes=None):\n        """"""Reduce map over non-spatial axes\n\n        Parameters\n        ----------\n        func : `~numpy.ufunc`\n            Function to use for reducing the data.\n        keepdims : bool, optional\n            If this is set to true, the axes which are summed over are left in\n            the map with a single bin\n        axes: list\n            Names of MapAxis to reduce over\n            If None, all will reduced\n\n        Returns\n        -------\n        map_out : `~WcsNDMap`\n            Map with non-spatial axes reduced\n        """"""\n        if axes is None:\n            axes = [ax.name for ax in self.geom.axes]\n\n        map_out = self.copy()\n        for ax in axes:\n            map_out = map_out.reduce(ax, func=func, keepdims=keepdims)\n        return map_out\n\n    def reduce(self, axis, func=np.add, keepdims=False):\n        """"""Reduce map over a single non-spatial axis\n\n        Parameters\n        ----------\n        axis: str\n            The name of the axis to reduce over\n        func : `~numpy.ufunc`\n            Function to use for reducing the data.\n        keepdims : bool, optional\n            If this is set to true, the axes which are summed over are left in\n            the map with a single bin\n\n\n        Returns\n        -------\n        map_out : `~WcsNDMap`\n            Map with the given non-spatial axes reduced\n        """"""\n        if keepdims:\n            geom = self.geom.squash(axis=axis)\n        else:\n            geom = self.geom.drop(axis=axis)\n\n        names = [ax.name for ax in reversed(self.geom.axes)]\n        idx = names.index(axis)\n        data = func.reduce(\n            self.data, axis=idx, keepdims=keepdims, where=~np.isnan(self.data)\n        )\n        return self._init_copy(geom=geom, data=data)\n\n    def pad(self, pad_width, mode=""constant"", cval=0, order=1):\n        if np.isscalar(pad_width):\n            pad_width = (pad_width, pad_width)\n            pad_width += (0,) * (self.geom.ndim - 2)\n\n        geom = self.geom.pad(pad_width[:2])\n        if self.geom.is_regular and mode != ""interp"":\n            return self._pad_np(geom, pad_width, mode, cval)\n        else:\n            return self._pad_coadd(geom, pad_width, mode, cval, order)\n\n    def _pad_np(self, geom, pad_width, mode, cval):\n        """"""Pad a map using ``numpy.pad``.\n\n        This method only works for regular geometries but should be more\n        efficient when working with large maps.\n        """"""\n        kwargs = {}\n        if mode == ""constant"":\n            kwargs[""constant_values""] = cval\n\n        pad_width = [(t, t) for t in pad_width]\n        data = np.pad(self.data, pad_width[::-1], mode, **kwargs)\n        return self._init_copy(geom=geom, data=data)\n\n    def _pad_coadd(self, geom, pad_width, mode, cval, order):\n        """"""Pad a map manually by coadding the original map with the new map.""""""\n        idx_in = self.geom.get_idx(flat=True)\n        idx_in = tuple([t + w for t, w in zip(idx_in, pad_width)])[::-1]\n        idx_out = geom.get_idx(flat=True)[::-1]\n        map_out = self._init_copy(geom=geom, data=None)\n        map_out.coadd(self)\n\n        if mode == ""constant"":\n            pad_msk = np.zeros_like(map_out.data, dtype=bool)\n            pad_msk[idx_out] = True\n            pad_msk[idx_in] = False\n            map_out.data[pad_msk] = cval\n        elif mode == ""interp"":\n            coords = geom.pix_to_coord(idx_out[::-1])\n            m = self.geom.contains(coords)\n            coords = tuple([c[~m] for c in coords])\n            vals = self.interp_by_coord(coords, interp=order)\n            map_out.set_by_coord(coords, vals)\n        else:\n            raise ValueError(f""Invalid mode: {mode!r}"")\n\n        return map_out\n\n    def crop(self, crop_width):\n        if np.isscalar(crop_width):\n            crop_width = (crop_width, crop_width)\n\n        geom = self.geom.crop(crop_width)\n        if self.geom.is_regular:\n            slices = [slice(None)] * len(self.geom.axes)\n            slices += [\n                slice(crop_width[1], int(self.geom.npix[1] - crop_width[1])),\n                slice(crop_width[0], int(self.geom.npix[0] - crop_width[0])),\n            ]\n            data = self.data[tuple(slices)]\n            map_out = self._init_copy(geom=geom, data=data)\n        else:\n            # FIXME: This could be done more efficiently by\n            # constructing the appropriate slices for each image plane\n            map_out = self._init_copy(geom=geom, data=None)\n            map_out.coadd(self)\n\n        return map_out\n\n    def upsample(self, factor, order=0, preserve_counts=True, axis=None):\n        geom = self.geom.upsample(factor, axis=axis)\n        idx = geom.get_idx()\n\n        if axis is None:\n            pix = (\n                (idx[0] - 0.5 * (factor - 1)) / factor,\n                (idx[1] - 0.5 * (factor - 1)) / factor,\n            ) + idx[2:]\n        else:\n            pix = list(idx)\n            idx_ax = self.geom.get_axis_index_by_name(axis)\n            pix[idx_ax] = (pix[idx_ax] - 0.5 * (factor - 1)) / factor\n\n        data = scipy.ndimage.map_coordinates(\n            self.data.T, tuple(pix), order=order, mode=""nearest""\n        )\n\n        if preserve_counts:\n            if axis is None:\n                data /= factor ** 2\n            else:\n                data /= factor\n\n        return self._init_copy(geom=geom, data=data)\n\n    def downsample(self, factor, preserve_counts=True, axis=None):\n        geom = self.geom.downsample(factor, axis=axis)\n        if axis is None:\n            block_size = (factor, factor) + (1,) * len(self.geom.axes)\n        else:\n            block_size = [1] * self.data.ndim\n            idx = self.geom.get_axis_index_by_name(axis)\n            block_size[-(idx + 1)] = factor\n\n        func = np.nansum if preserve_counts else np.nanmean\n        data = block_reduce(self.data, tuple(block_size[::-1]), func=func)\n\n        return self._init_copy(geom=geom, data=data)\n\n    def plot(self, ax=None, fig=None, add_cbar=False, stretch=""linear"", **kwargs):\n        """"""\n        Plot image on matplotlib WCS axes.\n\n        Parameters\n        ----------\n        ax : `~astropy.visualization.wcsaxes.WCSAxes`, optional\n            WCS axis object to plot on.\n        fig : `~matplotlib.figure.Figure`\n            Figure object.\n        add_cbar : bool\n            Add color bar?\n        stretch : str\n            Passed to `astropy.visualization.simple_norm`.\n        **kwargs : dict\n            Keyword arguments passed to `~matplotlib.pyplot.imshow`.\n\n        Returns\n        -------\n        fig : `~matplotlib.figure.Figure`\n            Figure object.\n        ax : `~astropy.visualization.wcsaxes.WCSAxes`\n            WCS axis object\n        cbar : `~matplotlib.colorbar.Colorbar` or None\n            Colorbar object.\n        """"""\n        import matplotlib.pyplot as plt\n        from astropy.visualization import simple_norm\n        from astropy.visualization.wcsaxes.frame import EllipticalFrame\n\n        if not self.geom.is_image:\n            raise TypeError(""Use .plot_interactive() for Map dimension > 2"")\n\n        if fig is None:\n            fig = plt.gcf()\n\n        if ax is None:\n            if self.geom.is_allsky:\n                ax = fig.add_subplot(\n                    1, 1, 1, projection=self.geom.wcs, frame_class=EllipticalFrame\n                )\n            else:\n                ax = fig.add_subplot(1, 1, 1, projection=self.geom.wcs)\n\n        data = self.data.astype(float)\n\n        kwargs.setdefault(""interpolation"", ""nearest"")\n        kwargs.setdefault(""origin"", ""lower"")\n        kwargs.setdefault(""cmap"", ""afmhot"")\n\n        norm = simple_norm(data[np.isfinite(data)], stretch)\n        kwargs.setdefault(""norm"", norm)\n\n        caxes = ax.imshow(data, **kwargs)\n        cbar = fig.colorbar(caxes, ax=ax, label=str(self.unit)) if add_cbar else None\n\n        if self.geom.is_allsky:\n            ax = self._plot_format_allsky(ax)\n        else:\n            ax = self._plot_format(ax)\n\n        # without this the axis limits are changed when calling scatter\n        ax.autoscale(enable=False)\n        return fig, ax, cbar\n\n    def _plot_format(self, ax):\n        try:\n            ax.coords[""glon""].set_axislabel(""Galactic Longitude"")\n            ax.coords[""glat""].set_axislabel(""Galactic Latitude"")\n        except KeyError:\n            ax.coords[""ra""].set_axislabel(""Right Ascension"")\n            ax.coords[""dec""].set_axislabel(""Declination"")\n        except AttributeError:\n            log.info(""Can\'t set coordinate axes. No WCS information available."")\n        return ax\n\n    def _plot_format_allsky(self, ax):\n        # Remove frame\n        ax.coords.frame.set_linewidth(0)\n\n        # Set plot axis limits\n        xmin, _ = self.geom.coord_to_pix({""lon"": 180, ""lat"": 0})\n        xmax, _ = self.geom.coord_to_pix({""lon"": -180, ""lat"": 0})\n\n        _, ymin = self.geom.coord_to_pix({""lon"": 0, ""lat"": -90})\n        _, ymax = self.geom.coord_to_pix({""lon"": 0, ""lat"": 90})\n\n        ax.set_xlim(xmin, xmax)\n        ax.set_ylim(ymin, ymax)\n\n        ax.text(0, ymax, self.geom.frame + "" coords"")\n\n        # Grid and ticks\n        glon_spacing, glat_spacing = 45, 15\n        lon, lat = ax.coords\n        lon.set_ticks(spacing=glon_spacing * u.deg, color=""w"", alpha=0.8)\n        lat.set_ticks(spacing=glat_spacing * u.deg)\n        lon.set_ticks_visible(False)\n\n        lon.set_major_formatter(""d"")\n        lat.set_major_formatter(""d"")\n\n        lon.set_ticklabel(color=""w"", alpha=0.8)\n        lon.grid(alpha=0.2, linestyle=""solid"", color=""w"")\n        lat.grid(alpha=0.2, linestyle=""solid"", color=""w"")\n        return ax\n\n    def smooth(self, width, kernel=""gauss"", **kwargs):\n        """"""Smooth the map.\n\n        Iterates over 2D image planes, processing one at a time.\n\n        Parameters\n        ----------\n        width : `~astropy.units.Quantity`, str or float\n            Smoothing width given as quantity or float. If a float is given it\n            interpreted as smoothing width in pixels. If an (angular) quantity\n            is given it converted to pixels using ``geom.wcs.wcs.cdelt``.\n            It corresponds to the standard deviation in case of a Gaussian kernel,\n            the radius in case of a disk kernel, and the side length in case\n            of a box kernel.\n        kernel : {\'gauss\', \'disk\', \'box\'}\n            Kernel shape\n        kwargs : dict\n            Keyword arguments passed to `~scipy.ndimage.uniform_filter`\n            (\'box\'), `~scipy.ndimage.gaussian_filter` (\'gauss\') or\n            `~scipy.ndimage.convolve` (\'disk\').\n\n        Returns\n        -------\n        image : `WcsNDMap`\n            Smoothed image (a copy, the original object is unchanged).\n        """"""\n        if isinstance(width, (u.Quantity, str)):\n            width = u.Quantity(width) / self.geom.pixel_scales.mean()\n            width = width.to_value("""")\n\n        smoothed_data = np.empty(self.data.shape, dtype=float)\n\n        for img, idx in self.iter_by_image():\n            img = img.astype(float)\n            if kernel == ""gauss"":\n                data = scipy.ndimage.gaussian_filter(img, width, **kwargs)\n            elif kernel == ""disk"":\n                disk = Tophat2DKernel(width)\n                disk.normalize(""integral"")\n                data = scipy.ndimage.convolve(img, disk.array, **kwargs)\n            elif kernel == ""box"":\n                data = scipy.ndimage.uniform_filter(img, width, **kwargs)\n            else:\n                raise ValueError(f""Invalid kernel: {kernel!r}"")\n            smoothed_data[idx] = data\n\n        return self._init_copy(data=smoothed_data)\n\n    def get_spectrum(self, region=None, func=np.nansum):\n        """"""Extract spectrum in a given region.\n\n        The spectrum can be computed by summing (or, more generally, applying ``func``)\n        along the spatial axes in each energy bin. This occurs only inside the ``region``,\n        which by default is assumed to be the whole spatial extension of the map.\n\n        Parameters\n        ----------\n        region: `~regions.Region`\n             Region (pixel or sky regions accepted).\n        func : numpy.ufunc\n            Function to reduce the data.\n\n        Returns\n        -------\n        spectrum : `~gammapy.maps.RegionNDMap`\n            Spectrum in the given region.\n        """"""\n        energy_axis = self.geom.axes[0]\n\n        if region:\n            mask = self.geom.region_mask([region])\n            data = self.data[mask].reshape(energy_axis.nbin, -1)\n            data = func(data, axis=1)\n        else:\n            data = func(self.data, axis=(1, 2))\n\n        geom = RegionGeom(region=region, axes=[energy_axis])\n        return RegionNDMap(\n            geom=geom, data=data.reshape(geom.data_shape), unit=self.unit\n        )\n\n    def convolve(self, kernel, use_fft=True, **kwargs):\n        """"""\n        Convolve map with a kernel.\n\n        If the kernel is two dimensional, it is applied to all image planes likewise.\n        If the kernel is higher dimensional it must match the map in the number of\n        dimensions and the corresponding kernel is selected for every image plane.\n\n        Parameters\n        ----------\n        kernel : `~gammapy.cube.PSFKernel` or `numpy.ndarray`\n            Convolution kernel.\n        use_fft : bool\n            Use `scipy.signal.fftconvolve` or `scipy.ndimage.convolve`.\n        kwargs : dict\n            Keyword arguments passed to `scipy.signal.fftconvolve` or\n            `scipy.ndimage.convolve`.\n\n        Returns\n        -------\n        map : `WcsNDMap`\n            Convolved map.\n        """"""\n        from gammapy.irf import PSFKernel\n\n        conv_function = scipy.signal.fftconvolve if use_fft else scipy.ndimage.convolve\n        convolved_data = np.empty(self.data.shape, dtype=np.float32)\n        if use_fft:\n            kwargs.setdefault(""mode"", ""same"")\n\n        if isinstance(kernel, PSFKernel):\n            kmap = kernel.psf_kernel_map\n            if not np.allclose(\n                self.geom.pixel_scales.deg, kmap.geom.pixel_scales.deg, rtol=1e-5\n            ):\n                raise ValueError(""Pixel size of kernel and map not compatible."")\n            kernel = kmap.data.astype(np.float32)\n\n        for img, idx in self.iter_by_image():\n            ikern = Ellipsis if kernel.ndim == 2 else idx\n            convolved_data[idx] = conv_function(\n                img.astype(np.float32), kernel[ikern], **kwargs\n            )\n\n        return self._init_copy(data=convolved_data)\n\n    def cutout(self, position, width, mode=""trim""):\n        """"""\n        Create a cutout around a given position.\n\n        Parameters\n        ----------\n        position : `~astropy.coordinates.SkyCoord`\n            Center position of the cutout region.\n        width : tuple of `~astropy.coordinates.Angle`\n            Angular sizes of the region in (lon, lat) in that specific order.\n            If only one value is passed, a square region is extracted.\n        mode : {\'trim\', \'partial\', \'strict\'}\n            Mode option for Cutout2D, for details see `~astropy.nddata.utils.Cutout2D`.\n\n        Returns\n        -------\n        cutout : `~gammapy.maps.WcsNDMap`\n            Cutout map\n        """"""\n        geom_cutout = self.geom.cutout(position=position, width=width, mode=mode)\n\n        slices = geom_cutout.cutout_info[""parent-slices""]\n        parent_slices = Ellipsis, slices[0], slices[1]\n\n        slices = geom_cutout.cutout_info[""cutout-slices""]\n        cutout_slices = Ellipsis, slices[0], slices[1]\n\n        data = np.zeros(shape=geom_cutout.data_shape, dtype=self.data.dtype)\n        data[cutout_slices] = self.data[parent_slices]\n\n        return self._init_copy(geom=geom_cutout, data=data)\n\n    def stack(self, other, weights=None):\n        """"""Stack cutout into map.\n\n        Parameters\n        ----------\n        other : `WcsNDMap`\n            Other map to stack\n        weights : `WcsNDMap`\n            Array to be used as weights. The spatial geometry must be equivalent\n            to `other` and additional axes must be broadcastable.\n        """"""\n        if self.geom == other.geom:\n            parent_slices, cutout_slices = None, None\n        elif self.geom.is_aligned(other.geom):\n            slices = other.geom.cutout_info[""parent-slices""]\n            parent_slices = Ellipsis, slices[0], slices[1]\n\n            slices = other.geom.cutout_info[""cutout-slices""]\n            cutout_slices = Ellipsis, slices[0], slices[1]\n        else:\n            raise ValueError(\n                ""Can only stack equivalent maps or cutout of the same map.""\n            )\n\n        data = other.data[cutout_slices]\n\n        if weights is not None:\n            if not other.geom.to_image() == weights.geom.to_image():\n                raise ValueError(""Incompatible spatial geoms between map and weights"")\n            data = data * weights.data[cutout_slices]\n        self.data[parent_slices] += data\n\n    def sample_coord(self, n_events, random_state=0):\n        """"""Sample position and energy of events.\n\n        Parameters\n        ----------\n        n_events : int\n            Number of events to sample.\n        random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n            Defines random number generator initialisation.\n            Passed to `~gammapy.utils.random.get_random_state`.\n\n        Returns\n        -------\n        coords : `~gammapy.maps.MapCoord` object.\n            Sequence of coordinates and energies of the sampled events.\n        """"""\n\n        random_state = get_random_state(random_state)\n        sampler = InverseCDFSampler(pdf=self.data, random_state=random_state)\n\n        coords_pix = sampler.sample(n_events)\n        coords = self.geom.pix_to_coord(coords_pix[::-1])\n\n        # TODO: pix_to_coord should return a MapCoord object\n        axes_names = [""lon"", ""lat""] + [ax.name for ax in self.geom.axes]\n        cdict = OrderedDict(zip(axes_names, coords))\n\n        return MapCoord.create(cdict, frame=self.geom.frame)\n'"
gammapy/modeling/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Models and fitting.""""""\nfrom .covariance import *\nfrom .fit import *\nfrom .parameter import *\nfrom .sampling import *\n'"
gammapy/modeling/covariance.py,15,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Covariance class""""""\nimport numpy as np\nimport scipy\nfrom .parameter import Parameters\n\n__all__ = [""Covariance""]\n\n\nclass Covariance:\n    """"""Parameter covariance class\n\n    Parameters\n    ----------\n    parameters : `~gammapy.modeling.Parameters`\n        Parameter list\n    data : `~numpy.ndarray`\n        Covariance data array\n\n    """"""\n\n    def __init__(self, parameters, data=None):\n        self.parameters = parameters\n        if data is None:\n            data = np.diag([p.error ** 2 for p in self.parameters])\n\n        self._data = np.asanyarray(data, dtype=float)\n\n    @property\n    def shape(self):\n        """"""Covariance shape""""""\n        npars = len(self.parameters)\n        return npars, npars\n\n    @property\n    def data(self):\n        """"""Covariance data (`~numpy.ndarray`)""""""\n        return self._data\n\n    @data.setter\n    def data(self, value):\n        value = np.asanyarray(value)\n\n        npars = len(self.parameters)\n        shape = (npars, npars)\n        if value.shape != shape:\n            raise ValueError(\n                f""Invalid covariance shape: {value.shape}, expected {shape}""\n            )\n\n        self._data = value\n\n    @staticmethod\n    def _expand_factor_matrix(matrix, parameters):\n        """"""Expand covariance matrix with zeros for frozen parameters""""""\n        npars = len(parameters)\n        matrix_expanded = np.zeros((npars, npars))\n        mask_frozen = [par.frozen for par in parameters]\n        pars_index = [np.where(np.array(parameters) == p)[0][0] for p in parameters]\n        mask_duplicate = [pars_idx != idx for idx, pars_idx in enumerate(pars_index)]\n        mask = np.array(mask_frozen) | np.array(mask_duplicate)\n        free_parameters = ~(mask | mask[:, np.newaxis])\n        matrix_expanded[free_parameters] = matrix.ravel()\n        return matrix_expanded\n\n    @classmethod\n    def from_factor_matrix(cls, parameters, matrix):\n        """"""Set covariance from factor covariance matrix.\n\n        Used in the optimizer interface.\n        """"""\n        npars = len(parameters)\n\n        if not matrix.shape == (npars, npars):\n            matrix = cls._expand_factor_matrix(matrix, parameters)\n\n        scales = [par.scale for par in parameters]\n        scale_matrix = np.outer(scales, scales)\n        data = scale_matrix * matrix\n\n        return cls(parameters, data=data)\n\n    @classmethod\n    def from_stack(cls, covar_list):\n        """"""Stack sub-covariance matrices from list\n\n        Parameters\n        ----------\n        covar_list : list of `Covariance`\n            List of sub-covariances\n\n        Returns\n        -------\n        covar : `Covariance`\n            Stacked covariance\n        """"""\n        parameters = Parameters.from_stack([_.parameters for _ in covar_list])\n\n        covar = cls(parameters)\n\n        for subcovar in covar_list:\n            covar.set_subcovariance(subcovar)\n\n        return covar\n\n    def get_subcovariance(self, parameters):\n        """"""Get sub-covariance matrix\n\n        Parameters\n        ----------\n        parameters : `Parameters`\n            Sub list of parameters.\n\n        Returns\n        -------\n        covariance : `~numpy.ndarray`\n            Sub-covariance.\n        """"""\n        idx = [self.parameters.index(par) for par in parameters]\n        data = self._data[np.ix_(idx, idx)]\n        return self.__class__(parameters=parameters, data=data)\n\n    def set_subcovariance(self, covar):\n        """"""Set sub-covariance matrix\n\n        Parameters\n        ----------\n        parameters : `Parameters`\n            Sub list of parameters.\n\n        """"""\n\n        idx = [self.parameters.index(par) for par in covar.parameters]\n\n        if not np.allclose(self.data[np.ix_(idx, idx)], covar.data):\n            self.data[idx, :] = 0\n            self.data[:, idx] = 0\n\n        self._data[np.ix_(idx, idx)] = covar.data\n\n    def plot_correlation(self, ax=None, **kwargs):\n        """"""Plot correlation matrix.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis to plot on.\n        **kwargs : dict\n            Keyword arguments passed to `~gammapy.visualisation.plot_heatmap`\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n\n        """"""\n        import matplotlib.pyplot as plt\n        from gammapy.visualization import plot_heatmap, annotate_heatmap\n\n        npars = len(self.parameters)\n        figsize = (npars * 0.8, npars * 0.65)\n\n        plt.figure(figsize=figsize)\n\n        ax = plt.gca() if ax is None else ax\n\n        kwargs.setdefault(""cmap"", ""coolwarm"")\n\n        names = self.parameters.names\n        im, cbar = plot_heatmap(\n            data=self.correlation,\n            col_labels=names,\n            row_labels=names,\n            ax=ax,\n            vmin=-1,\n            vmax=1,\n            cbarlabel=""Correlation"",\n            **kwargs,\n        )\n        annotate_heatmap(im=im)\n        return ax\n\n    @property\n    def correlation(self):\n        r""""""Correlation matrix (`numpy.ndarray`).\n\n        Correlation :math:`C` is related to covariance :math:`\\Sigma` via:\n\n        .. math::\n            C_{ij} = \\frac{ \\Sigma_{ij} }{ \\sqrt{\\Sigma_{ii} \\Sigma_{jj}} }\n        """"""\n        err = np.sqrt(np.diag(self.data))\n\n        with np.errstate(invalid=""ignore"", divide=""ignore""):\n            correlation = self.data / np.outer(err, err)\n\n        return np.nan_to_num(correlation)\n\n    @property\n    def scipy_mvn(self):\n        # TODO: use this, as in https://github.com/cdeil/multinorm/blob/master/multinorm.py\n        return scipy.stats.multivariate_normal(\n            self.parameters.values, self.data, allow_singular=True\n        )\n\n    def __str__(self):\n        return str(self.data)\n\n    def __array__(self):\n        return self.data\n'"
gammapy/modeling/fit.py,4,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport numpy as np\nfrom astropy.utils import lazyproperty\nfrom .covariance import Covariance\nfrom .iminuit import confidence_iminuit, covariance_iminuit, mncontour, optimize_iminuit\nfrom .scipy import confidence_scipy, optimize_scipy\nfrom .sherpa import optimize_sherpa\n\n__all__ = [""Fit""]\n\nlog = logging.getLogger(__name__)\n\n\nclass Registry:\n    """"""Registry of available backends for given tasks.\n\n    Gives users the power to extend from their scripts.\n    Used by `Fit` below.\n\n    Not sure if we should call it ""backend"" or ""method"" or something else.\n    Probably we will code up some methods, e.g. for profile analysis ourselves,\n    using scipy or even just Python / Numpy?\n    """"""\n\n    register = {\n        ""optimize"": {\n            ""minuit"": optimize_iminuit,\n            ""sherpa"": optimize_sherpa,\n            ""scipy"": optimize_scipy,\n        },\n        ""covariance"": {\n            ""minuit"": covariance_iminuit,\n            # ""sherpa"": covariance_sherpa,\n            # ""scipy"": covariance_scipy,\n        },\n        ""confidence"": {\n            ""minuit"": confidence_iminuit,\n            # ""sherpa"": confidence_sherpa,\n            ""scipy"": confidence_scipy,\n        },\n    }\n\n    @classmethod\n    def get(cls, task, backend):\n        if task not in cls.register:\n            raise ValueError(f""Unknown task {task!r}"")\n\n        backend_options = cls.register[task]\n\n        if backend not in backend_options:\n            raise ValueError(f""Unknown backend {backend!r} for task {task!r}"")\n\n        return backend_options[backend]\n\n\nregistry = Registry()\n\n\nclass Fit:\n    """"""Fit class.\n\n    The fit class provides a uniform interface to multiple fitting backends.\n    Currently available: ""minuit"", ""sherpa"" and ""scipy""\n\n    Parameters\n    ----------\n    datasets : `Datasets`\n        Datasets\n    """"""\n\n    def __init__(self, datasets):\n        from gammapy.datasets import Datasets\n\n        self.datasets = Datasets(datasets)\n\n    @lazyproperty\n    def _parameters(self):\n        return self.datasets.parameters\n\n    @lazyproperty\n    def _models(self):\n        return self.datasets.models\n\n    def run(self, backend=""minuit"", optimize_opts=None, covariance_opts=None):\n        """"""\n        Run all fitting steps.\n\n        Parameters\n        ----------\n        backend : str\n            Backend used for fitting, default : minuit\n        optimize_opts : dict\n            Options passed to `Fit.optimize`.\n        covariance_opts : dict\n            Options passed to `Fit.covariance`.\n\n        Returns\n        -------\n        fit_result : `FitResult`\n            Results\n        """"""\n        if optimize_opts is None:\n            optimize_opts = {}\n        optimize_result = self.optimize(backend, **optimize_opts)\n\n        if covariance_opts is None:\n            covariance_opts = {}\n\n        if backend not in registry.register[""covariance""]:\n            log.warning(""No covariance estimate - not supported by this backend."")\n            return optimize_result\n\n        covariance_result = self.covariance(backend, **covariance_opts)\n        # TODO: not sure how best to report the results\n        # back or how to form the FitResult object.\n        optimize_result._success = optimize_result.success and covariance_result.success\n\n        return optimize_result\n\n    def optimize(self, backend=""minuit"", **kwargs):\n        """"""Run the optimization.\n\n        Parameters\n        ----------\n        backend : str\n            Which backend to use (see ``gammapy.modeling.registry``)\n        **kwargs : dict\n            Keyword arguments passed to the optimizer. For the `""minuit""` backend\n            see https://iminuit.readthedocs.io/en/latest/api.html#iminuit.Minuit\n            for a detailed description of the available options. If there is an entry\n            \'migrad_opts\', those options will be passed to `iminuit.Minuit.migrad()`.\n\n            For the `""sherpa""` backend you can from the options `method = {""simplex"",  ""levmar"", ""moncar"", ""gridsearch""}`\n            Those methods are described and compared in detail on\n            http://cxc.cfa.harvard.edu/sherpa/methods/index.html. The available\n            options of the optimization methods are described on the following\n            pages in detail:\n\n                * http://cxc.cfa.harvard.edu/sherpa/ahelp/neldermead.html\n                * http://cxc.cfa.harvard.edu/sherpa/ahelp/montecarlo.html\n                * http://cxc.cfa.harvard.edu/sherpa/ahelp/gridsearch.html\n                * http://cxc.cfa.harvard.edu/sherpa/ahelp/levmar.html\n\n            For the `""scipy""` backend the available options are desribed in detail here:\n            https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n\n        Returns\n        -------\n        fit_result : `FitResult`\n            Results\n        """"""\n        parameters = self._parameters\n\n        # TODO: expose options if / when to scale? On the Fit class?\n        if np.all(self._models.covariance.data == 0):\n            parameters.autoscale()\n\n        compute = registry.get(""optimize"", backend)\n        # TODO: change this calling interface!\n        # probably should pass a fit statistic, which has a model, which has parameters\n        # and return something simpler, not a tuple of three things\n        factors, info, optimizer = compute(\n            parameters=parameters, function=self.datasets.stat_sum, **kwargs\n        )\n\n        # TODO: Change to a stateless interface for minuit also, or if we must support\n        # stateful backends, put a proper, backend-agnostic solution for this.\n        # As preliminary solution would like to provide a possibility that the user\n        # can access the Minuit object, because it features a lot useful functionality\n        if backend == ""minuit"":\n            self.minuit = optimizer\n\n        # Copy final results into the parameters object\n        parameters.set_parameter_factors(factors)\n\n        return OptimizeResult(\n            parameters=parameters,\n            total_stat=self.datasets.stat_sum(),\n            backend=backend,\n            method=kwargs.get(""method"", backend),\n            **info,\n        )\n\n    def covariance(self, backend=""minuit"", **kwargs):\n        """"""Estimate the covariance matrix.\n\n        Assumes that the model parameters are already optimised.\n\n        Parameters\n        ----------\n        backend : str\n            Which backend to use (see ``gammapy.modeling.registry``)\n\n        Returns\n        -------\n        result : `CovarianceResult`\n            Results\n        """"""\n        compute = registry.get(""covariance"", backend)\n        parameters = self._parameters\n\n        # TODO: wrap MINUIT in a stateless backend\n        with parameters.restore_values:\n            if backend == ""minuit"":\n                method = ""hesse""\n                if hasattr(self, ""minuit""):\n                    factor_matrix, info = compute(self.minuit)\n                else:\n                    raise RuntimeError(""To use minuit, you must first optimize."")\n            else:\n                method = """"\n                factor_matrix, info = compute(\n                    parameters, self.datasets.stat_sum, **kwargs\n                )\n\n            covariance = Covariance.from_factor_matrix(\n                parameters=self._models.parameters, matrix=factor_matrix\n            )\n            self._models.covariance = covariance\n\n        # TODO: decide what to return, and fill the info correctly!\n        return CovarianceResult(\n            backend=backend,\n            method=method,\n            parameters=parameters,\n            success=info[""success""],\n            message=info[""message""],\n        )\n\n    def confidence(\n        self, parameter, backend=""minuit"", sigma=1, reoptimize=True, **kwargs\n    ):\n        """"""Estimate confidence interval.\n\n        Extra ``kwargs`` are passed to the backend.\n        E.g. `iminuit.Minuit.minos` supports a ``maxcall`` option.\n\n        For the scipy backend ``kwargs`` are forwarded to `~scipy.optimize.brentq`. If the\n        confidence estimation fails, the bracketing interval can be adapted by modifying the\n        the upper bound of the interval (``b``) value.\n\n        Parameters\n        ----------\n        backend : str\n            Which backend to use (see ``gammapy.modeling.registry``)\n        parameter : `~gammapy.modeling.Parameter`\n            Parameter of interest\n        sigma : float\n            Number of standard deviations for the confidence level\n        reoptimize : bool\n            Re-optimize other parameters, when computing the confidence region.\n        **kwargs : dict\n            Keyword argument passed ot the confidence estimation method.\n\n        Returns\n        -------\n        result : dict\n            Dictionary with keys ""errp"", \'errn"", ""success"" and ""nfev"".\n        """"""\n        compute = registry.get(""confidence"", backend)\n        parameters = self._parameters\n        parameter = parameters[parameter]\n\n        # TODO: wrap MINUIT in a stateless backend\n        with parameters.restore_values:\n            if backend == ""minuit"":\n                if hasattr(self, ""minuit""):\n                    # This is ugly. We will access parameters and make a copy\n                    # from the backend, to avoid modifying the state\n                    result = compute(\n                        self.minuit, parameters, parameter, sigma, **kwargs\n                    )\n                else:\n                    raise RuntimeError(""To use minuit, you must first optimize."")\n            else:\n                result = compute(\n                    parameters,\n                    parameter,\n                    self.datasets.stat_sum,\n                    sigma,\n                    reoptimize,\n                    **kwargs,\n                )\n\n        result[""errp""] *= parameter.scale\n        result[""errn""] *= parameter.scale\n        return result\n\n    def stat_profile(\n        self,\n        parameter,\n        values=None,\n        bounds=2,\n        nvalues=11,\n        reoptimize=False,\n        optimize_opts=None,\n    ):\n        """"""Compute fit statistic profile.\n\n        The method used is to vary one parameter, keeping all others fixed.\n        So this is taking a ""slice"" or ""scan"" of the fit statistic.\n\n        See also: `Fit.minos_profile`.\n\n        Parameters\n        ----------\n        parameter : `~gammapy.modeling.Parameter`\n            Parameter of interest\n        values : `~astropy.units.Quantity` (optional)\n            Parameter values to evaluate the fit statistic for.\n        bounds : int or tuple of float\n            When an `int` is passed the bounds are computed from `bounds * sigma`\n            from the best fit value of the parameter, where `sigma` corresponds to\n            the one sigma error on the parameter. If a tuple of floats is given\n            those are taken as the min and max values and ``nvalues`` are linearly\n            spaced between those.\n        nvalues : int\n            Number of parameter grid points to use.\n        reoptimize : bool\n            Re-optimize other parameters, when computing the fit statistic profile.\n\n        Returns\n        -------\n        results : dict\n            Dictionary with keys ""values"" and ""stat"".\n        """"""\n        parameters = self._parameters\n        parameter = parameters[parameter]\n\n        optimize_opts = optimize_opts or {}\n\n        if values is None:\n            if isinstance(bounds, tuple):\n                parmin, parmax = bounds\n            else:\n                if np.isnan(parameter.error):\n                    raise ValueError(""Parameter error is not properly set."")\n                parerr = parameter.error\n                parval = parameter.value\n                parmin, parmax = parval - bounds * parerr, parval + bounds * parerr\n\n            values = np.linspace(parmin, parmax, nvalues)\n\n        stats = []\n        with parameters.restore_values:\n            for value in values:\n                parameter.value = value\n                if reoptimize:\n                    parameter.frozen = True\n                    result = self.optimize(**optimize_opts)\n                    stat = result.total_stat\n                else:\n                    stat = self.datasets.stat_sum()\n                stats.append(stat)\n\n        return {""values"": values, ""stat"": np.array(stats)}\n\n    def stat_contour(self):\n        """"""Compute fit statistic contour.\n\n        The method used is to vary two parameters, keeping all others fixed.\n        So this is taking a ""slice"" or ""scan"" of the fit statistic.\n\n        See also: `Fit.minos_contour`\n\n        Parameters\n        ----------\n        TODO\n\n        Returns\n        -------\n        TODO\n        """"""\n        raise NotImplementedError\n\n    def minos_contour(self, x, y, numpoints=10, sigma=1.0):\n        """"""Compute MINOS contour.\n\n        Calls ``iminuit.Minuit.mncontour``.\n\n        This is a contouring algorithm for a 2D function\n        which is not simply the fit statistic function.\n        That 2D function is given at each point ``(par_1, par_2)``\n        by re-optimising all other free parameters,\n        and taking the fit statistic at that point.\n\n        Very compute-intensive and slow.\n\n        Parameters\n        ----------\n        x, y : `~gammapy.modeling.Parameter`\n            Parameters of interest\n        numpoints : int\n            Number of contour points\n        sigma : float\n            Number of standard deviations for the confidence level\n\n        Returns\n        -------\n        result : dict\n            Dictionary with keys ""x"", ""y"" (Numpy arrays with contour points)\n            and a boolean flag ""success"".\n            The result objects from ``mncontour`` are in the additional\n            keys ""x_info"" and ""y_info"".\n        """"""\n        parameters = self._parameters\n        x = parameters[x]\n        y = parameters[y]\n\n        with parameters.restore_values:\n            result = mncontour(self.minuit, parameters, x, y, numpoints, sigma)\n\n        x = result[""x""] * x.scale\n        y = result[""y""] * y.scale\n\n        return {\n            ""x"": x,\n            ""y"": y,\n            ""success"": result[""success""],\n            ""x_info"": result[""x_info""],\n            ""y_info"": result[""y_info""],\n        }\n\n\nclass FitResult:\n    """"""Fit result base class""""""\n\n    def __init__(self, parameters, backend, method, success, message):\n        self._parameters = parameters\n        self._success = success\n        self._message = message\n        self._backend = backend\n        self._method = method\n\n    @property\n    def parameters(self):\n        """"""Optimizer backend used for the fit.""""""\n        return self._parameters\n\n    @property\n    def backend(self):\n        """"""Optimizer backend used for the fit.""""""\n        return self._backend\n\n    @property\n    def method(self):\n        """"""Optimizer method used for the fit.""""""\n        return self._method\n\n    @property\n    def success(self):\n        """"""Fit success status flag.""""""\n        return self._success\n\n    @property\n    def message(self):\n        """"""Optimizer status message.""""""\n        return self._message\n\n    def __repr__(self):\n        return (\n            f""{self.__class__.__name__}\\n\\n""\n            f""\\tbackend    : {self.backend}\\n""\n            f""\\tmethod     : {self.method}\\n""\n            f""\\tsuccess    : {self.success}\\n""\n            f""\\tmessage    : {self.message}\\n""\n        )\n\n\nclass CovarianceResult(FitResult):\n    """"""Covariance result object.""""""\n\n    pass\n\n\nclass OptimizeResult(FitResult):\n    """"""Optimize result object.""""""\n\n    def __init__(self, nfev, total_stat, **kwargs):\n        self._nfev = nfev\n        self._total_stat = total_stat\n        super().__init__(**kwargs)\n\n    @property\n    def nfev(self):\n        """"""Number of function evaluations.""""""\n        return self._nfev\n\n    @property\n    def total_stat(self):\n        """"""Value of the fit statistic at minimum.""""""\n        return self._total_stat\n\n    def __repr__(self):\n        str_ = super().__repr__()\n        str_ += f""\\tnfev       : {self.nfev}\\n""\n        str_ += f""\\ttotal stat : {self.total_stat:.2f}\\n""\n        return str_\n'"
gammapy/modeling/iminuit.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""iminuit fitting functions.""""""\nimport logging\nimport numpy as np\nfrom .likelihood import Likelihood\n\n__all__ = [""optimize_iminuit"", ""covariance_iminuit"", ""confidence_iminuit"", ""mncontour""]\n\nlog = logging.getLogger(__name__)\n\n\nclass MinuitLikelihood(Likelihood):\n    """"""Likelihood function interface for iminuit.""""""\n\n    def fcn(self, *factors):\n        self.parameters.set_parameter_factors(factors)\n        return self.function()\n\n\ndef optimize_iminuit(parameters, function, **kwargs):\n    """"""iminuit optimization\n\n    Parameters\n    ----------\n    parameters : `~gammapy.modeling.Parameters`\n        Parameters with starting values\n    function : callable\n        Likelihood function\n    **kwargs : dict\n        Options passed to `iminuit.Minuit` constructor. If there is an entry \'migrad_opts\', those options\n        will be passed to `iminuit.Minuit.migrad()`.\n\n    Returns\n    -------\n    result : (factors, info, optimizer)\n        Tuple containing the best fit factors, some info and the optimizer instance.\n    """"""\n    from iminuit import Minuit\n\n    # In Gammapy, we have the factor 2 in the likelihood function\n    # This means `errordef=1` in the Minuit interface is correct\n    kwargs.setdefault(""errordef"", 1)\n    kwargs.setdefault(""print_level"", 0)\n    kwargs.update(make_minuit_par_kwargs(parameters))\n\n    minuit_func = MinuitLikelihood(function, parameters)\n\n    kwargs = kwargs.copy()\n    migrad_opts = kwargs.pop(""migrad_opts"", {})\n    strategy = kwargs.pop(""strategy"", 1)\n    tol = kwargs.pop(""tol"", 0.1)\n    minuit = Minuit(minuit_func.fcn, **kwargs)\n    minuit.tol = tol\n    minuit.strategy = strategy\n    minuit.migrad(**migrad_opts)\n\n    factors = minuit.args\n    info = {\n        ""success"": minuit.migrad_ok(),\n        ""nfev"": minuit.get_num_call_fcn(),\n        ""message"": _get_message(minuit, parameters),\n    }\n    optimizer = minuit\n\n    return factors, info, optimizer\n\n\ndef covariance_iminuit(minuit):\n    # TODO: add minuit.hesse() call once we have better tests\n\n    message, success = ""Hesse terminated successfully."", True\n    try:\n        covariance_factors = minuit.np_covariance()\n    except (TypeError, RuntimeError):\n        N = len(minuit.args)\n        covariance_factors = np.nan * np.ones((N, N))\n        message, success = ""Hesse failed"", False\n    return covariance_factors, {""success"": success, ""message"": message}\n\n\ndef confidence_iminuit(minuit, parameters, parameter, sigma, maxcall=0):\n    # TODO: this is ugly - design something better for translating to MINUIT parameter names.\n    # Maybe a wrapper class MinuitParameters?\n    parameter = parameters[parameter]\n    idx = parameters.free_parameters.index(parameter)\n    var = _make_parname(idx, parameter)\n\n    message, success = ""Minos terminated successfully."", True\n    try:\n        result = minuit.minos(var=var, sigma=sigma, maxcall=maxcall)\n        info = result[var]\n    except RuntimeError as error:\n        message, success = str(error), False\n        info = {""is_valid"": False, ""lower"": np.nan, ""upper"": np.nan, ""nfcn"": 0}\n\n    return {\n        ""success"": success,\n        ""message"": message,\n        ""errp"": info[""upper""],\n        ""errn"": -info[""lower""],\n        ""nfev"": info[""nfcn""],\n    }\n\n\ndef mncontour(minuit, parameters, x, y, numpoints, sigma):\n    par_x = parameters[x]\n    idx_x = parameters.free_parameters.index(par_x)\n    x = _make_parname(idx_x, par_x)\n\n    par_y = parameters[y]\n    idx_y = parameters.free_parameters.index(par_y)\n    y = _make_parname(idx_y, par_y)\n\n    x_info, y_info, contour = minuit.mncontour(x, y, numpoints, sigma)\n    contour = np.array(contour)\n\n    success = x_info[""is_valid""] and y_info[""is_valid""]\n\n    return {\n        ""success"": success,\n        ""x"": contour[:, 0],\n        ""y"": contour[:, 1],\n        ""x_info"": x_info,\n        ""y_info"": y_info,\n    }\n\n\n# this code is copied from https://github.com/iminuit/iminuit/blob/master/iminuit/_minimize.py#L95\ndef _get_message(m, parameters):\n    message = ""Optimization terminated successfully.""\n    success = m.migrad_ok()\n    success &= np.all(np.isfinite([par.value for par in parameters]))\n    if not success:\n        message = ""Optimization failed.""\n        fmin = m.get_fmin()\n        if fmin.has_reached_call_limit:\n            message += "" Call limit was reached.""\n        if fmin.is_above_max_edm:\n            message += "" Estimated distance to minimum too large.""\n    return message\n\n\ndef _make_parnames(parameters):\n    return [_make_parname(idx, par) for idx, par in enumerate(parameters)]\n\n\ndef _make_parname(idx, par):\n    return f""par_{idx:03d}_{par.name}""\n\n\ndef make_minuit_par_kwargs(parameters):\n    """"""Create *Parameter Keyword Arguments* for the `Minuit` constructor.\n\n    See: http://iminuit.readthedocs.io/en/latest/api.html#iminuit.Minuit\n    """"""\n    names = _make_parnames(parameters.free_parameters)\n    kwargs = {""forced_parameters"": names}\n\n    for name, par in zip(names, parameters.free_parameters):\n        kwargs[name] = par.factor\n\n        min_ = None if np.isnan(par.factor_min) else par.factor_min\n        max_ = None if np.isnan(par.factor_max) else par.factor_max\n        kwargs[f""limit_{name}""] = (min_, max_)\n\n        if par.error == 0:\n            error = 1\n        else:\n            error = par.error / par.scale\n\n        kwargs[f""error_{name}""] = error\n\n    return kwargs\n'"
gammapy/modeling/likelihood.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n__all__ = [""Likelihood""]\n\n\n# TODO: get rid of this wrapper class? Or use it in a better way?\nclass Likelihood:\n    """"""Wrapper of the likelihood function used by the optimiser.\n\n    This might become superfluous if we introduce a\n    generic ``Likelihood`` interface and use that directly,\n    or change the ``Fit`` class to work with ``Model``\n    and ``Likelihood`` objects.\n\n    For now, this class does the translation of parameter\n    values and the parameter factors the optimiser sees.\n\n    Parameters\n    ----------\n    parameters : `~gammapy.modeling.Parameters`\n        Parameters with starting values\n    function : callable\n        Likelihood function\n    """"""\n\n    def __init__(self, function, parameters):\n        self.function = function\n        self.parameters = parameters\n\n    def fcn(self, factors):\n        self.parameters.set_parameter_factors(factors)\n        return self.function()\n'"
gammapy/modeling/parameter.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Model parameter classes.""""""\nimport collections.abc\nimport copy\nimport itertools\nimport numpy as np\nfrom astropy import units as u\nfrom gammapy.utils.table import table_from_row_data\n\n__all__ = [""Parameter"", ""Parameters""]\n\n\ndef _get_parameters_str(parameters):\n    str_ = """"\n\n    for par in parameters:\n        if par.name == ""amplitude"":\n            line = ""\\t{:12} {:11}: {:10.2e} {} {:<12s}\\n""\n        else:\n            line = ""\\t{:12} {:11}: {:7.3f} {} {:<12s}\\n""\n\n        frozen = ""(frozen)"" if par.frozen else """"\n        try:\n            error = ""+/- {:7.2f}"".format(parameters.get_error(par))\n        except AttributeError:\n            error = """"\n\n        str_ += line.format(par.name, frozen, par.value, error, par.unit)\n    return str_.expandtabs(tabsize=2)\n\n\nclass Parameter:\n    """"""A model parameter.\n\n    Note that the parameter value has been split into\n    a factor and scale like this::\n\n        value = factor x scale\n\n    Users should interact with the ``value``, ``quantity``\n    or ``min`` and ``max`` properties and consider the fact\n    that there is a ``factor``` and ``scale`` an implementation detail.\n\n    That was introduced for numerical stability in parameter and error\n    estimation methods, only in the Gammapy optimiser interface do we\n    interact with the ``factor``, ``factor_min`` and ``factor_max`` properties,\n    i.e. the optimiser ""sees"" the well-scaled problem.\n\n    Parameters\n    ----------\n    name : str\n        Name\n    value : float or `~astropy.units.Quantity`\n        Value\n    scale : float, optional\n        Scale (sometimes used in fitting)\n    unit : `~astropy.units.Unit` or str, optional\n        Unit\n    min : float, optional\n        Minimum (sometimes used in fitting)\n    max : float, optional\n        Maximum (sometimes used in fitting)\n    frozen : bool, optional\n        Frozen? (used in fitting)\n    """"""\n\n    def __init__(\n        self,\n        name,\n        value,\n        unit="""",\n        scale=1,\n        min=np.nan,\n        max=np.nan,\n        frozen=False,\n        error=0,\n    ):\n        self.name = name\n        self._link_label_io = None\n        self.scale = scale\n\n        # TODO: move this to a setter method that can be called from `__set__` also!\n        # Having it here is bad: behaviour not clear if Quantity and `unit` is passed.\n        if isinstance(value, u.Quantity) or isinstance(value, str):\n            val = u.Quantity(value)\n            self.value = val.value\n            self.unit = val.unit\n        else:\n            self.factor = value\n            self.unit = unit\n\n        self.min = min\n        self.max = max\n        self.frozen = frozen\n        self._error = error\n\n    def __get__(self, instance, owner):\n        if instance is None:\n            return self\n        return instance.__dict__[self.name]\n\n    def __set__(self, instance, value):\n        if isinstance(value, Parameter):\n            instance.__dict__[self.name] = value\n            # TODO: create the link in the parameters list\n            # par = instance.__dict__[self.name]\n            # instance.__dict__[""_parameters""].link(par, value)\n        else:\n            par = instance.__dict__[self.name]\n            raise TypeError(f""Cannot assign {value!r} to parameter {par!r}"")\n\n    @property\n    def error(self):\n        return self._error\n\n    @error.setter\n    def error(self, value):\n        self._error = float(u.Quantity(value, unit=self.unit).value)\n\n    @property\n    def name(self):\n        """"""Name (str).""""""\n        return self._name\n\n    @name.setter\n    def name(self, val):\n        if not isinstance(val, str):\n            raise TypeError(f""Invalid type: {val}, {type(val)}"")\n        self._name = val\n\n    @property\n    def factor(self):\n        """"""Factor (float).""""""\n        return self._factor\n\n    @factor.setter\n    def factor(self, val):\n        self._factor = float(val)\n\n    @property\n    def scale(self):\n        """"""Scale (float).""""""\n        return self._scale\n\n    @scale.setter\n    def scale(self, val):\n        self._scale = float(val)\n\n    @property\n    def unit(self):\n        """"""Unit (`~astropy.units.Unit`).""""""\n        return self._unit\n\n    @unit.setter\n    def unit(self, val):\n        self._unit = u.Unit(val)\n\n    @property\n    def min(self):\n        """"""Minimum (float).""""""\n        return self._min\n\n    @min.setter\n    def min(self, val):\n        self._min = float(val)\n\n    @property\n    def factor_min(self):\n        """"""Factor min (float).\n\n        This ``factor_min = min / scale`` is for the optimizer interface.\n        """"""\n        return self.min / self.scale\n\n    @property\n    def max(self):\n        """"""Maximum (float).""""""\n        return self._max\n\n    @max.setter\n    def max(self, val):\n        self._max = float(val)\n\n    @property\n    def factor_max(self):\n        """"""Factor max (float).\n\n        This ``factor_max = max / scale`` is for the optimizer interface.\n        """"""\n        return self.max / self.scale\n\n    @property\n    def frozen(self):\n        """"""Frozen? (used in fitting) (bool).""""""\n        return self._frozen\n\n    @frozen.setter\n    def frozen(self, val):\n        if not isinstance(val, bool):\n            raise TypeError(f""Invalid type: {val}, {type(val)}"")\n        self._frozen = val\n\n    @property\n    def value(self):\n        """"""Value = factor x scale (float).""""""\n        return self._factor * self._scale\n\n    @value.setter\n    def value(self, val):\n        self._factor = float(val) / self._scale\n\n    @property\n    def quantity(self):\n        """"""Value times unit (`~astropy.units.Quantity`).""""""\n        return self.value * self.unit\n\n    @quantity.setter\n    def quantity(self, val):\n        val = u.Quantity(val, unit=self.unit)\n        self.value = val.value\n        self.unit = val.unit\n\n    def __repr__(self):\n        return (\n            f""{self.__class__.__name__}(name={self.name!r}, value={self.value!r}, ""\n            f""factor={self.factor!r}, scale={self.scale!r}, unit={self.unit!r}, ""\n            f""min={self.min!r}, max={self.max!r}, frozen={self.frozen!r}, id={hex(id(self))})""\n        )\n\n    def copy(self):\n        """"""A deep copy""""""\n        return copy.deepcopy(self)\n\n    def to_dict(self):\n        """"""Convert to dict.""""""\n        output = {\n            ""name"": self.name,\n            ""value"": self.value,\n            ""unit"": self.unit.to_string(""fits""),\n            ""min"": self.min,\n            ""max"": self.max,\n            ""frozen"": self.frozen,\n            ""error"": self.error,\n        }\n\n        if self._link_label_io is not None:\n            output[""link""] = self._link_label_io\n        return output\n\n    def autoscale(self, method=""scale10""):\n        """"""Autoscale the parameters.\n\n        Set ``factor`` and ``scale`` according to ``method``\n\n        Available methods:\n\n        * ``scale10`` sets ``scale`` to power of 10,\n          so that abs(factor) is in the range 1 to 10\n        * ``factor1`` sets ``factor, scale = 1, value``\n\n        In both cases the sign of value is stored in ``factor``,\n        i.e. the ``scale`` is always positive.\n\n        Parameters\n        ----------\n        method : {\'factor1\', \'scale10\'}\n            Method to apply\n        """"""\n        if method == ""scale10"":\n            value = self.value\n            if value != 0:\n                exponent = np.floor(np.log10(np.abs(value)))\n                scale = np.power(10.0, exponent)\n                self.factor = value / scale\n                self.scale = scale\n        elif method == ""factor1"":\n            self.factor, self.scale = 1, self.value\n        else:\n            raise ValueError(f""Invalid method: {method}"")\n\n\nclass Parameters(collections.abc.Sequence):\n    """"""Parameters container.\n\n    - List of `Parameter` objects.\n    - Covariance matrix.\n\n    Parameters\n    ----------\n    parameters : list of `Parameter`\n        List of parameters\n    """"""\n\n    def __init__(self, parameters=None):\n        if parameters is None:\n            parameters = []\n        else:\n            parameters = list(parameters)\n\n        self._parameters = parameters\n\n    @property\n    def values(self):\n        """"""Parameter values (`numpy.ndarray`).""""""\n        return np.array([_.value for _ in self._parameters], dtype=np.float64)\n\n    @classmethod\n    def from_stack(cls, parameters_list):\n        """"""Create `Parameters` by stacking a list of other `Parameters` objects.\n\n        Parameters\n        ----------\n        parameters_list : list of `Parameters`\n            List of `Parameters` objects\n        """"""\n        pars = itertools.chain(*parameters_list)\n        parameters = cls(pars)\n        return parameters\n\n    def copy(self):\n        """"""A deep copy""""""\n        return copy.deepcopy(self)\n\n    @property\n    def free_parameters(self):\n        """"""List of free parameters""""""\n        return self.__class__([par for par in self._parameters if not par.frozen])\n\n    @property\n    def unique_parameters(self):\n        """"""Unique parameters (`Parameters`).""""""\n        return self.__class__(dict.fromkeys(self._parameters))\n\n    @property\n    def names(self):\n        """"""List of parameter names""""""\n        return [par.name for par in self._parameters]\n\n    def index(self, val):\n        """"""Get position index for a given parameter.\n\n        The input can be a parameter object, parameter name (str)\n        or if a parameter index (int) is passed in, it is simply returned.\n        """"""\n        if isinstance(val, int):\n            return val\n        elif isinstance(val, Parameter):\n            return self._parameters.index(val)\n        elif isinstance(val, str):\n            for idx, par in enumerate(self._parameters):\n                if val == par.name:\n                    return idx\n            raise IndexError(f""No parameter: {val!r}"")\n        else:\n            raise TypeError(f""Invalid type: {type(val)!r}"")\n\n    def __getitem__(self, name):\n        """"""Access parameter by name or index""""""\n        idx = self.index(name)\n        return self._parameters[idx]\n\n    def __len__(self):\n        return len(self._parameters)\n\n    def __add__(self, other):\n        if isinstance(other, Parameters):\n            return Parameters.from_stack([self, other])\n        else:\n            raise TypeError(f""Invalid type: {other!r}"")\n\n    def to_dict(self):\n        data = []\n\n        for par in self._parameters:\n            data.append(par.to_dict())\n\n        return data\n\n    def to_table(self):\n        """"""Convert parameter attributes to `~astropy.table.Table`.""""""\n        rows = [p.to_dict() for p in self._parameters]\n        table = table_from_row_data(rows)\n\n        for name in [""value"", ""error"", ""min"", ""max""]:\n            table[name].format = "".3e""\n\n        return table\n\n    def __eq__(self, other):\n        all_equal = np.all([p is p_new for p, p_new in zip(self, other)])\n        return all_equal and len(self) == len(other)\n\n    @classmethod\n    def from_dict(cls, data):\n        parameters = []\n        for par in data:\n            link_label = par.pop(""link"", None)\n            parameter = Parameter(**par)\n            parameter._link_label_io = link_label\n            parameters.append(parameter)\n        return cls(parameters=parameters)\n\n    def set_parameter_factors(self, factors):\n        """"""Set factor of all parameters.\n\n        Used in the optimizer interface.\n        """"""\n        idx = 0\n        for parameter in self._parameters:\n            if not parameter.frozen:\n                parameter.factor = factors[idx]\n                idx += 1\n\n    def autoscale(self, method=""scale10""):\n        """"""Autoscale all parameters.\n\n        See :func:`~gammapy.modeling.Parameter.autoscale`\n\n        Parameters\n        ----------\n        method : {\'factor1\', \'scale10\'}\n            Method to apply\n        """"""\n        for par in self._parameters:\n            par.autoscale(method)\n\n    @property\n    def restore_values(self):\n        """"""Context manager to restore values.\n\n        A copy of the values is made on enter,\n        and those values are restored on exit.\n\n        Examples\n        --------\n        ::\n\n            from gammapy.modeling.models import PowerLawSpectralModel\n            pwl = PowerLawSpectralModel(index=2)\n            with pwl.parameters.restore_values:\n                pwl.parameters[""index""].value = 3\n            print(pwl.parameters[""index""].value)\n        """"""\n        return restore_parameters_values(self)\n\n    def freeze_all(self):\n        """"""Freeze all parameters""""""\n        for par in self._parameters:\n            par.frozen = True\n\n\nclass restore_parameters_values:\n    def __init__(self, parameters):\n        self._parameters = parameters\n        self.values = [_.value for _ in parameters]\n        self.frozen = [_.frozen for _ in parameters]\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, type, value, traceback):\n        for value, par, frozen in zip(self.values, self._parameters, self.frozen):\n            par.value = value\n            par.frozen = frozen\n'"
gammapy/modeling/sampling.py,3,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""MCMC sampling helper functions using ``emcee``.""""""\nimport logging\nimport numpy as np\n\n__all__ = [""uniform_prior"", ""run_mcmc"", ""plot_trace"", ""plot_corner""]\n\nlog = logging.getLogger(__name__)\n\n\n# TODO: so far only works with a uniform prior on parameters\n# as there is no way yet to enter min,mean,max in parameters for normal prior\n# lnprob() uses a uniform prior. hard coded for now.\n\n\ndef uniform_prior(value, umin, umax):\n    """"""Uniform prior distribution.""""""\n    if umin <= value <= umax:\n        return 0.0\n    else:\n        return -np.inf\n\n\ndef normal_prior(value, mean, sigma):\n    """"""Normal prior distribution.""""""\n    return -0.5 * (2 * np.pi * sigma) - (value - mean) ** 2 / (2.0 * sigma)\n\n\ndef par_to_model(dataset, pars):\n    """"""Update model in dataset with a list of free parameters factors""""""\n    for i, p in enumerate(dataset.models.parameters.free_parameters):\n        p.factor = pars[i]\n\n\ndef ln_uniform_prior(dataset):\n    """"""LogLike associated with prior and data/model evaluation.\n\n    Return probability of parameter values according to prior knowledge.\n    Parameter limits should be done here through uniform prior ditributions\n    """"""\n    logprob = 0\n    for par in dataset.models.parameters.free_parameters:\n        logprob += uniform_prior(par.value, par.min, par.max)\n\n    return logprob\n\n\ndef lnprob(pars, dataset):\n    """"""Estimate the likelihood of a model including prior on parameters.""""""\n    # Update model parameters factors inplace\n    for factor, par in zip(pars, dataset.models.parameters.free_parameters):\n        par.factor = factor\n\n    lnprob_priors = ln_uniform_prior(dataset)\n\n    # dataset.likelihood returns Cash statistics values\n    # emcee will maximisise the LogLikelihood so we need -dataset.likelihood\n    total_lnprob = -dataset.stat_sum() + lnprob_priors\n\n    return total_lnprob\n\n\ndef run_mcmc(dataset, nwalkers=8, nrun=1000, threads=1):\n    """"""Run the MCMC sampler.\n\n    Parameters\n    ----------\n    dataset : `~gammapy.modeling.Dataset`\n        Dataset\n    nwalkers : int\n        Number of walkers\n    nrun : int\n        Number of steps each walker takes\n    threads : (optional)\n        Number of threads or processes to use\n\n    Returns\n    -------\n    sampler : `emcee.EnsembleSampler`\n        sampler object containing the trace of all walkers.\n    """"""\n    import emcee\n\n    dataset.models.parameters.autoscale()  # Autoscale parameters\n\n    # Initialize walkers in a ball of relative size 0.5% in all dimensions if the\n    # parameters have been fit, or to 10% otherwise\n    # Handle source position spread differently with a spread of 0.1\xc2\xb0\n    # TODO: the spread of 0.5% below is valid if a pre-fit of the model has been obtained.\n    # currently the run_mcmc() doesn\'t know the status of previous fit.\n    p0var = []\n    pars = []\n    spread = 0.5 / 100\n    spread_pos = 0.1  # in degrees\n    for par in dataset.models.parameters.free_parameters:\n        pars.append(par.factor)\n        if par.name in [""lon_0"", ""lat_0""]:\n            p0var.append(spread_pos / par.scale)\n        else:\n            p0var.append(spread * par.factor)\n\n    ndim = len(pars)\n    p0 = emcee.utils.sample_ball(pars, p0var, nwalkers)\n\n    labels = []\n    for par in dataset.models.parameters.free_parameters:\n        labels.append(par.name)\n        if (par.min is np.nan) and (par.max is np.nan):\n            log.warning(\n                f""Missing prior for parameter: {par.name}.\\nMCMC will likely fail!""\n            )\n\n    log.info(f""Free parameters: {labels}"")\n\n    sampler = emcee.EnsembleSampler(\n        nwalkers, ndim, lnprob, args=[dataset], threads=threads\n    )\n\n    log.info(f""Starting MCMC sampling: nwalkers={nwalkers}, nrun={nrun}"")\n    for idx, result in enumerate(sampler.sample(p0, iterations=nrun)):\n        if idx % (nrun / 4) == 0:\n            log.info(""{:5.0%}"".format(idx / nrun))\n    log.info(""100% => sampling completed"")\n\n    return sampler\n\n\ndef plot_trace(sampler, dataset):\n    """"""\n    Plot the trace of walkers for every steps\n\n    Parameters\n    ----------\n    sampler : `emcee.EnsembleSampler`\n        Sampler object containing the trace of all walkers\n    dataset : `~gammapy.modeling.Dataset`\n        Dataset\n    """"""\n    import matplotlib.pyplot as plt\n\n    labels = [par.name for par in dataset.models.parameters.free_parameters]\n\n    fig, axes = plt.subplots(len(labels), sharex=True)\n\n    for idx, ax in enumerate(axes):\n        ax.plot(sampler.chain[:, :, idx].T, ""-k"", alpha=0.2)\n        ax.set_ylabel(labels[idx])\n\n    plt.xlabel(""Nrun"")\n    plt.show()\n\n\ndef plot_corner(sampler, dataset, nburn=0):\n    """"""Corner plot for each parameter explored by the walkers.\n\n    Parameters\n    ----------\n    sampler : `emcee.EnsembleSampler`\n        Sampler object containing the trace of all walkers\n    dataset : `~gammapy.modeling.Dataset`\n        Dataset\n    nburn : int\n        Number of runs to discard, because considered part of the burn-in phase\n    """"""\n    from corner import corner\n\n    labels = [par.name for par in dataset.models.parameters.free_parameters]\n\n    samples = sampler.chain[:, nburn:, :].reshape((-1, len(labels)))\n\n    corner(samples, labels=labels, quantiles=[0.16, 0.5, 0.84], show_titles=True)\n'"
gammapy/modeling/scipy.py,7,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nimport scipy.optimize\nfrom gammapy.utils.interpolation import interpolate_profile\nfrom .likelihood import Likelihood\n\n__all__ = [\n    ""optimize_scipy"",\n    ""covariance_scipy"",\n    ""confidence_scipy"",\n    ""stat_profile_ul_scipy"",\n]\n\n\ndef optimize_scipy(parameters, function, **kwargs):\n    method = kwargs.pop(""method"", ""Nelder-Mead"")\n    pars = [par.factor for par in parameters.free_parameters]\n\n    bounds = []\n    for par in parameters.free_parameters:\n        parmin = par.factor_min if not np.isnan(par.factor_min) else None\n        parmax = par.factor_max if not np.isnan(par.factor_max) else None\n        bounds.append((parmin, parmax))\n\n    likelihood = Likelihood(function, parameters)\n    result = scipy.optimize.minimize(\n        likelihood.fcn, pars, bounds=bounds, method=method, **kwargs\n    )\n\n    factors = result.x\n    info = {""success"": result.success, ""message"": result.message, ""nfev"": result.nfev}\n    optimizer = None\n\n    return factors, info, optimizer\n\n\nclass TSDifference:\n    """"""Fit statistic function wrapper to compute TS differences""""""\n\n    def __init__(self, function, parameters, parameter, reoptimize, ts_diff):\n        self.stat_null = function()\n        self.parameters = parameters\n        self.function = function\n        self.parameter = parameter\n        self.parameter.frozen = True\n        self.ts_diff = ts_diff\n        self.reoptimize = reoptimize\n\n    def fcn(self, factor):\n        self.parameter.factor = factor\n        if self.reoptimize:\n            optimize_scipy(self.parameters, self.function, method=""L-BFGS-B"")\n        value = self.function() - self.stat_null - self.ts_diff\n        return value\n\n\ndef _confidence_scipy_brentq(\n    parameters, parameter, function, sigma, reoptimize, upper=True, **kwargs\n):\n    ts_diff = TSDifference(\n        function, parameters, parameter, reoptimize, ts_diff=sigma ** 2\n    )\n\n    kwargs.setdefault(""a"", parameter.factor)\n\n    bound = parameter.factor_max if upper else parameter.factor_min\n\n    if np.isnan(bound):\n        bound = parameter.factor\n        if upper:\n            bound += 1e2 * parameter.error / parameter.scale\n        else:\n            bound -= 1e2 * parameter.error / parameter.scale\n\n    kwargs.setdefault(""b"", bound)\n\n    message, success = ""Confidence terminated successfully."", True\n\n    try:\n        result = scipy.optimize.brentq(ts_diff.fcn, full_output=True, **kwargs)\n    except ValueError:\n        message = (\n            ""Confidence estimation failed, because bracketing interval""\n            "" does not contain a unique solution. Try setting the interval by hand.""\n        )\n        success = False\n        result = (\n            np.nan,\n            scipy.optimize.RootResults(\n                root=np.nan, iterations=0, function_calls=0, flag=0\n            ),\n        )\n\n    suffix = ""errp"" if upper else ""errn""\n\n    return {\n        ""nfev_"" + suffix: result[1].iterations,\n        suffix: np.abs(result[0] - kwargs[""a""]),\n        ""success_"" + suffix: success,\n        ""message_"" + suffix: message,\n        ""stat_null"": ts_diff.stat_null,\n    }\n\n\ndef confidence_scipy(parameters, parameter, function, sigma, reoptimize=True, **kwargs):\n\n    if len(parameters.free_parameters) <= 1:\n        reoptimize = False\n\n    with parameters.restore_values:\n        result = _confidence_scipy_brentq(\n            parameters=parameters,\n            parameter=parameter,\n            function=function,\n            sigma=sigma,\n            upper=False,\n            reoptimize=reoptimize,\n            **kwargs\n        )\n\n    with parameters.restore_values:\n        result_errp = _confidence_scipy_brentq(\n            parameters=parameters,\n            parameter=parameter,\n            function=function,\n            sigma=sigma,\n            upper=True,\n            reoptimize=reoptimize,\n            **kwargs\n        )\n\n    result.update(result_errp)\n    return result\n\n\n# TODO: implement, e.g. with numdifftools.Hessian\ndef covariance_scipy(parameters, function):\n    raise NotImplementedError\n\n\ndef stat_profile_ul_scipy(\n    value_scan, stat_scan, delta_ts=4, interp_scale=""sqrt"", **kwargs\n):\n    """"""Compute upper limit of a parameter from a likelihood profile.\n\n    Parameters\n    ----------\n    value_scan : `~numpy.ndarray`\n        Array of parameter values.\n    stat_scan : `~numpy.ndarray`\n        Array of delta fit statistic values, with respect to the minimum.\n    delta_ts : float\n        Difference in test statistics for the upper limit.\n    interp_scale : {""sqrt"", ""lin""}\n        Interpolation scale applied to the fit statistic profile. If the profile is\n        of parabolic shape, a ""sqrt"" scaling is recommended. In other cases or\n        for fine sampled profiles a ""lin"" can also be used.\n    **kwargs : dict\n        Keyword arguments passed to `~scipy.optimize.brentq`.\n\n    Returns\n    -------\n    ul : float\n        Upper limit value.\n    """"""\n    interp = interpolate_profile(value_scan, stat_scan, interp_scale=interp_scale)\n\n    def f(x):\n        return interp((x,)) - delta_ts\n\n    idx = np.argmin(stat_scan)\n    norm_best_fit = value_scan[idx]\n    ul = scipy.optimize.brentq(f, a=norm_best_fit, b=value_scan[-1], **kwargs)\n\n    return ul\n'"
gammapy/modeling/sherpa.py,1,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom .likelihood import Likelihood\n\n__all__ = [""optimize_sherpa"", ""covariance_sherpa""]\n\n\ndef get_sherpa_optimizer(name):\n    from sherpa.optmethods import LevMar, NelderMead, MonCar, GridSearch\n\n    return {\n        ""levmar"": LevMar,\n        ""simplex"": NelderMead,\n        ""moncar"": MonCar,\n        ""gridsearch"": GridSearch,\n    }[name]()\n\n\nclass SherpaLikelihood(Likelihood):\n    """"""Likelihood function interface for Sherpa.""""""\n\n    def fcn(self, factors):\n        self.parameters.set_parameter_factors(factors)\n        return self.function(), 0\n\n\ndef optimize_sherpa(parameters, function, **kwargs):\n    """"""Sherpa optimization wrapper method.\n\n    Parameters\n    ----------\n    parameters : `~gammapy.modeling.Parameters`\n        Parameter list with starting values.\n    function : callable\n        Likelihood function\n    **kwargs : dict\n        Options passed to the optimizer instance.\n\n    Returns\n    -------\n    result : (factors, info, optimizer)\n        Tuple containing the best fit factors, some info and the optimizer instance.\n    """"""\n    method = kwargs.pop(""method"", ""simplex"")\n    optimizer = get_sherpa_optimizer(method)\n    optimizer.config.update(kwargs)\n\n    pars = [par.factor for par in parameters.free_parameters]\n    parmins = [par.factor_min for par in parameters.free_parameters]\n    parmaxes = [par.factor_max for par in parameters.free_parameters]\n\n    statfunc = SherpaLikelihood(function, parameters)\n\n    with np.errstate(invalid=""ignore""):\n        result = optimizer.fit(\n            statfunc=statfunc.fcn, pars=pars, parmins=parmins, parmaxes=parmaxes\n        )\n\n    factors = result[1]\n    info = {""success"": result[0], ""message"": result[3], ""nfev"": result[4][""nfev""]}\n\n    return factors, info, optimizer\n\n\ndef covariance_sherpa():\n    raise NotImplementedError\n'"
gammapy/scripts/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Gammapy command line interface (scripts).""""""\n'"
gammapy/scripts/analysis.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport click\nfrom gammapy.analysis import Analysis, AnalysisConfig\n\nlog = logging.getLogger(__name__)\n\n\n@click.command(name=""config"")\n@click.option(\n    ""--filename"",\n    default=""config.yaml"",\n    help=""Filename to store the default configuration values."",\n    show_default=True,\n)\n@click.option(\n    ""--overwrite"", default=False, is_flag=True, help=""Overwrite existing file.""\n)\ndef cli_make_config(filename, overwrite):\n    """"""Writes default configuration file.""""""\n    config = AnalysisConfig()\n    config.write(filename, overwrite=overwrite)\n    log.info(f""Configuration file produced: {filename}"")\n\n\n@click.command(name=""run"")\n@click.option(\n    ""--filename"",\n    default=""config.yaml"",\n    help=""Filename with default configuration values."",\n    show_default=True,\n)\n@click.option(\n    ""--out"",\n    default=""datasets"",\n    help=""Output folder where reduced datasets are stored."",\n    show_default=True,\n)\n@click.option(\n    ""--overwrite"", default=False, is_flag=True, help=""Overwrite existing datasets.""\n)\ndef cli_run_analysis(filename, out, overwrite):\n    """"""Performs automated data reduction process.""""""\n    config = AnalysisConfig.read(filename)\n    config.datasets.background.method = ""reflected""\n    analysis = Analysis(config)\n    analysis.get_observations()\n    analysis.get_datasets()\n    analysis.datasets.write(out, overwrite=overwrite)\n    log.info(f""Datasets stored in {out} folder."")\n'"
gammapy/scripts/check.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Command line tool to check various things about a Gammapy installation.\n\nThis file is called `check` instead of `test` to prevent confusion\nfor developers and the test runner from including it in test collection.\n""""""\nimport logging\nimport warnings\nimport click\n\nlog = logging.getLogger(__name__)\n\n\n@click.group(""check"")\ndef cli_check():\n    """"""Run checks for Gammapy""""""\n\n\n@cli_check.command(""logging"")\ndef cli_check_logging():\n    """"""Check logging""""""\n    log.debug(""this is log.debug() output"")\n    log.info(""this is log.info() output"")\n    log.warning(""this is log.warning() output"")\n    warnings.warn(""this is warnings.warn() output"")\n    print(""this is stdout output"")\n'"
gammapy/scripts/download.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Command line tool to download datasets and notebooks""""""\nimport logging\nimport tarfile\nfrom pathlib import Path\nimport click\nfrom .downloadclasses import ComputePlan, ParallelDownload\n\nBUNDLESIZE = 152  # in MB\nlog = logging.getLogger(__name__)\n\n\ndef progress_download(source, destination):\n    import requests\n    from tqdm import tqdm\n\n    destination.parent.mkdir(parents=True, exist_ok=True)\n    with requests.get(source, stream=True) as r:\n        total_size = (\n            int(r.headers.get(""content-length""))\n            if r.headers.get(""content-length"")\n            else BUNDLESIZE * 1024 * 1024\n        )\n        progress_bar = tqdm(\n            total=total_size, unit=""B"", unit_scale=True, unit_divisor=1024\n        )\n        with open(destination, ""wb"") as f:\n            for chunk in r.iter_content(chunk_size=1024):\n                if chunk:\n                    f.write(chunk)\n                    progress_bar.update(len(chunk))\n    progress_bar.close()\n\n\ndef members(tf):\n    list_members = tf.getmembers()\n    root_folder = list_members[0].name\n    for member in list_members:\n        if member.path.startswith(root_folder):\n            member.path = member.path[len(root_folder) + 1 :]\n            yield member\n\n\ndef extract_bundle(bundle, destination):\n    with tarfile.open(bundle) as tar:\n        tar.extractall(path=destination, members=members(tar))\n    Path(bundle).unlink()\n\n\n@click.command(name=""notebooks"")\n@click.option(""--src"", default="""", help=""Specific notebook to download."")\n@click.option(\n    ""--out"",\n    default=""gammapy-notebooks"",\n    help=""Path where the versioned notebook files will be copied."",\n    show_default=True,\n)\n@click.option(""--release"", default="""", help=""Number of release - ex: 0.12)"")\n@click.option(""--modetutorials"", default=False, hidden=True)\n@click.option(""--silent"", default=True, is_flag=True, hidden=True)\ndef cli_download_notebooks(src, out, release, modetutorials, silent):\n    """"""Download notebooks""""""\n    plan = ComputePlan(src, out, release, ""notebooks"")\n    if release:\n        plan.getenvironment()\n    down = ParallelDownload(\n        plan.getfilelist(),\n        plan.getlocalfolder(),\n        release,\n        ""notebooks"",\n        modetutorials,\n        silent,\n    )\n    down.run()\n    print("""")\n\n\n@click.command(name=""scripts"")\n@click.option(""--src"", default="""", help=""Specific script to download."")\n@click.option(\n    ""--out"",\n    default=""gammapy-scripts"",\n    help=""Path where the versioned python scripts will be copied."",\n    show_default=True,\n)\n@click.option(""--release"", default="""", help=""Number of release - ex: 0.12"")\n@click.option(""--modetutorials"", default=False, hidden=True)\n@click.option(""--silent"", default=True, is_flag=True, hidden=False)\ndef cli_download_scripts(src, out, release, modetutorials, silent):\n    """"""Download scripts""""""\n    plan = ComputePlan(src, out, release, ""scripts"")\n    if release:\n        plan.getenvironment()\n    down = ParallelDownload(\n        plan.getfilelist(),\n        plan.getlocalfolder(),\n        release,\n        ""scripts"",\n        modetutorials,\n        silent,\n    )\n    down.run()\n    print("""")\n\n\n@click.command(name=""datasets"")\n@click.option(""--src"", default="""", help=""Specific dataset to download."")\n@click.option(\n    ""--out"", default=""gammapy-datasets"", help=""Destination folder."", show_default=True,\n)\n@click.option(""--release"", default="""", help=""Number of release - ex: 0.12"")\n@click.option(""--modetutorials"", default=False, hidden=True)\n@click.option(""--silent"", default=True, is_flag=True, hidden=True)\n@click.option(\n    ""--tests"",\n    default=True,\n    is_flag=True,\n    help=""Include datasets needed for tests. [default: True]"",\n    hidden=True,\n)\ndef cli_download_datasets(src, out, release, modetutorials, silent, tests):\n    """"""Download datasets""""""\n    plan = ComputePlan(\n        src, out, release, ""datasets"", modetutorials=modetutorials, download_tests=tests\n    )\n    filelist = plan.getfilelist()\n    localfolder = plan.getlocalfolder()\n    down = ParallelDownload(\n        filelist, localfolder, release, ""datasets"", modetutorials, silent,\n    )\n    # tar bundle\n    if ""bundle"" in filelist:\n        log.info(f""Downloading datasets from {filelist[\'bundle\'][\'url\']}"")\n        tar_destination_file = Path(localfolder) / ""datasets.tar.gz""\n        progress_download(filelist[""bundle""][""url""], tar_destination_file)\n        log.info(f""Extracting {tar_destination_file}"")\n        extract_bundle(tar_destination_file, localfolder)\n\n    # specific collection\n    else:\n        down.run()\n    down.show_info_datasets()\n\n\n@click.command(name=""tutorials"")\n@click.pass_context\n@click.option(""--src"", default="""", help=""Specific tutorial to download."")\n@click.option(\n    ""--out"",\n    default=""gammapy-tutorials"",\n    help=""Path where notebooks and datasets folders will be copied."",\n    show_default=True,\n)\n@click.option(""--release"", default="""", help=""Number of release - ex: 0.12"")\n@click.option(""--modetutorials"", default=True, hidden=True)\n@click.option(""--silent"", default=True, is_flag=True, hidden=True)\ndef cli_download_tutorials(ctx, src, out, release, modetutorials, silent):\n    """"""Download notebooks, scripts and datasets""""""\n    ctx.forward(cli_download_notebooks)\n    ctx.forward(cli_download_scripts)\n    ctx.forward(cli_download_datasets)\n'"
gammapy/scripts/downloadclasses.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Download class for gammapy download CLI.""""""\nimport hashlib\nimport json\nimport logging\nimport sys\nfrom pathlib import Path\nfrom urllib.request import urlopen\nimport yaml\nfrom gammapy import __version__\n\nlog = logging.getLogger(__name__)\n\nRELEASES_BASE_URL = ""https://gammapy.org/download""\nDEV_NBS_YAML_URL = (\n    ""https://raw.githubusercontent.com/gammapy/gammapy/master/tutorials/notebooks.yaml""\n)\nDEV_SCRIPTS_YAML_URL = (\n    ""https://raw.githubusercontent.com/gammapy/gammapy/master/examples/scripts.yaml""\n)\nTAR_BUNDLE = (\n    ""https://github.com/gammapy/gammapy-data/tarball/master""  # Curated datasets bundle\n)\nDEV_DATA_JSON_LOCAL = ""../../dev/datasets/gammapy-data-index.json""  # CI tests\n\n\ndef parse_datafiles(datasearch, datasetslist, download_tests=False):\n    for dataset in datasetslist:\n        if dataset[""name""] == ""tests"" and not download_tests and datasearch != ""tests"":\n            continue\n        if (datasearch == dataset[""name""] or datasearch == """") and dataset.get(\n            ""files"", """"\n        ):\n            for ds in dataset[""files""]:\n                label = ds[""path""]\n                data = {""url"": ds[""url""], ""path"": ds[""path""]}\n                if ""hashmd5"" in ds:\n                    data[""hashmd5""] = ds[""hashmd5""]\n                yield label, data\n\n\ndef parse_imagefiles(notebookslist):\n    for item in notebookslist:\n        record = notebookslist[item]\n        if record.get(""images"", """"):\n            for im in record[""images""]:\n                label = ""im: "" + im\n                path = ""images/"" + im + "".png""\n                filename_img = record[""url""][record[""url""].rfind(""/"") :]\n                url = record[""url""].replace(filename_img, """")\n                url = url + ""/"" + path\n                data = {""url"": url, ""path"": path}\n                yield label, data\n\n\nclass ComputePlan:\n    """"""Generates the whole list of files to download""""""\n\n    def __init__(\n        self, src, outfolder, release, option, modetutorials=False, download_tests=False\n    ):\n        self.src = src\n        self.outfolder = Path(outfolder)\n        self.release = release\n        self.option = option\n        self.modetutorials = modetutorials\n        self.download_tests = download_tests\n        self.listfiles = {}\n        log.info(f""Looking for {self.option}..."")\n\n        if self.release == """" and ""dev"" not in __version__:\n            self.release = __version__\n\n    def getenvironment(self):\n        try:\n            from parfive import Downloader\n        except ImportError:\n            log.error(""To use gammapy download, install the parfive package!"")\n            return\n\n        dl = Downloader(progress=False, file_progress=False)\n        filename_env = ""gammapy-"" + self.release + ""-environment.yml""\n        url_file_env = RELEASES_BASE_URL + ""/install/"" + filename_env\n        filepath_env = str(self.outfolder / filename_env)\n        dl.enqueue_file(url_file_env, path=filepath_env)\n        try:\n            log.info(f""Downloading {url_file_env}"")\n            Path(filepath_env).parent.mkdir(parents=True, exist_ok=True)\n            dl.download()\n        except Exception as ex:\n            log.error(ex)\n            exit()\n\n    def getlocalfolder(self):\n        suffix = f""-{self.release}""\n\n        if self.release == """":\n            suffix += __version__\n        if self.option == ""notebooks"":\n            return self.outfolder / f""notebooks{suffix}""\n        if self.option == ""scripts"":\n            return self.outfolder / f""scripts{suffix}""\n        if self.option == ""datasets"" and self.modetutorials:\n            return self.outfolder / ""datasets""\n        return self.outfolder\n\n    def getonefile(self, keyrec, filetype):\n        if keyrec in self.listfiles:\n            record = self.listfiles[keyrec]\n            self.listfiles = {}\n            self.listfiles[keyrec] = record\n        else:\n            self.listfiles = {}\n            if not self.modetutorials:\n                log.warning(f""{filetype} {self.src} not found"")\n\n    def getfilelist(self):\n        if self.option == ""notebooks"" or self.modetutorials:\n            self.parse_notebooks_yaml()\n            if self.src != """":\n                self.getonefile(""nb: "" + self.src, ""Notebook"")\n            self.listfiles.update(dict(parse_imagefiles(self.listfiles)))\n\n        if (self.option == ""scripts"" or self.modetutorials) and not self.listfiles:\n            self.parse_scripts_yaml()\n            if self.src != """":\n                self.getonefile(""sc: "" + self.src, ""Script"")\n\n        if self.option == ""datasets"":\n            if self.modetutorials and not self.listfiles:\n                sys.exit()\n            # datasets bundle\n            if not self.src:\n                self.listfiles = {""bundle"": {""path"": self.outfolder, ""url"": TAR_BUNDLE}}\n                return self.listfiles\n            # collection of files\n            if self.release:\n                url_datasets_json = (\n                    RELEASES_BASE_URL\n                    + ""/data/gammapy-""\n                    + self.release\n                    + ""-data-index.json""\n                )\n                log.info(f""Reading {url_datasets_json}"")\n                try:\n                    txt = urlopen(url_datasets_json).read().decode(""utf-8"")\n                except Exception as ex:\n                    log.error(ex)\n                    return False\n            else:\n                # for development just use the local index file\n                local_datasets_json = (\n                    Path(__file__).parent / DEV_DATA_JSON_LOCAL\n                ).resolve()\n                log.info(f""Reading {local_datasets_json}"")\n                txt = local_datasets_json.read_text()\n            datasets = json.loads(txt)\n            datafound = {}\n            if not self.modetutorials:\n                datafound.update(\n                    dict(\n                        parse_datafiles(\n                            self.src, datasets, download_tests=self.download_tests\n                        )\n                    )\n                )\n            else:\n                for item in self.listfiles:\n                    record = self.listfiles[item]\n                    if record.get(""datasets"", """"):\n                        for ds in record[""datasets""]:\n                            datafound.update(dict(parse_datafiles(ds, datasets)))\n            self.listfiles = datafound\n            if not datafound:\n                log.info(""No datasets found"")\n                sys.exit()\n\n        return self.listfiles\n\n    def parse_notebooks_yaml(self):\n        url = DEV_NBS_YAML_URL\n        if self.release:\n            filename_nbs = ""gammapy-"" + self.release + ""-tutorials.yml""\n            url = RELEASES_BASE_URL + ""/tutorials/"" + filename_nbs\n\n        log.info(f""Reading {url}"")\n        try:\n            txt = urlopen(url).read().decode(""utf-8"")\n        except Exception as ex:\n            log.error(ex)\n            return False\n\n        for nb in yaml.safe_load(txt):\n            path = nb[""name""] + "".ipynb""\n            label = ""nb: "" + nb[""name""]\n            self.listfiles[label] = {}\n            self.listfiles[label][""url""] = nb[""url""]\n            self.listfiles[label][""path""] = path\n            self.listfiles[label][""datasets""] = []\n            self.listfiles[label][""images""] = []\n            if nb.get(""datasets"", """"):\n                for ds in nb[""datasets""]:\n                    self.listfiles[label][""datasets""].append(ds)\n            if nb.get(""images"", """"):\n                for im in nb[""images""]:\n                    self.listfiles[label][""images""].append(im)\n\n    def parse_scripts_yaml(self):\n        url = DEV_SCRIPTS_YAML_URL\n        if self.release:\n            filename_scripts = ""gammapy-"" + self.release + ""-scripts.yml""\n            url = RELEASES_BASE_URL + ""/tutorials/"" + filename_scripts\n\n        log.info(f""Reading {url}"")\n        try:\n            txt = urlopen(url).read().decode(""utf-8"")\n        except Exception as ex:\n            log.error(ex)\n            return False\n\n        for sc in yaml.safe_load(txt):\n            path = sc[""name""] + "".py""\n            label = ""sc: "" + sc[""name""]\n            self.listfiles[label] = {}\n            self.listfiles[label][""url""] = sc[""url""]\n            self.listfiles[label][""path""] = path\n            self.listfiles[label][""datasets""] = []\n            if sc.get(""datasets"", """"):\n                for ds in sc[""datasets""]:\n                    self.listfiles[label][""datasets""].append(ds)\n\n\nclass ParallelDownload:\n    """"""Manages the process of downloading files""""""\n\n    def __init__(self, listfiles, outfolder, release, option, modetutorials, progress):\n        self.listfiles = listfiles\n        self.outfolder = outfolder\n        self.release = release\n        self.option = option\n        self.modetutorials = modetutorials\n        self.progress = progress\n        self.bar = 0\n\n    def run(self):\n        try:\n            from parfive import Downloader\n        except ImportError:\n            log.error(""To use gammapy download, install the parfive package!"")\n            return\n\n        if self.listfiles:\n            log.info(f""Content will be downloaded in {self.outfolder}"")\n\n        dl = Downloader(progress=self.progress, file_progress=False)\n        for rec in self.listfiles:\n            url = self.listfiles[rec][""url""]\n            path = self.outfolder / self.listfiles[rec][""path""]\n            md5 = """"\n            if ""hashmd5"" in self.listfiles[rec]:\n                md5 = self.listfiles[rec][""hashmd5""]\n            retrieve = True\n            if md5 and path.exists():\n                md5local = hashlib.md5(path.read_bytes()).hexdigest()\n                if md5local == md5:\n                    retrieve = False\n            if retrieve:\n                dl.enqueue_file(url, path=str(path.parent))\n\n        log.info(f""{dl.queued_downloads} files to download."")\n        res = dl.download()\n        log.info(f""{len(res)} files downloaded."")\n        for err in res.errors:\n            _, _, exception = err\n            log.error(f""Error: {exception}"")\n\n    def show_info_datasets(self):\n        print("""")\n        GAMMAPY_DATA = Path.cwd() / self.outfolder\n        if self.modetutorials:\n            GAMMAPY_DATA = Path.cwd() / self.outfolder.parent / ""datasets""\n            if self.release:\n                print(\n                    ""*** Enter the following commands below to get started with this version of Gammapy""\n                )\n                print(f""cd {self.outfolder.parent}"")\n                condaname = ""gammapy-"" + self.release\n                envfilename = condaname + ""-environment.yml""\n                print(f""conda env create -f {envfilename}"")\n                print(f""conda activate {condaname}"")\n                print(""jupyter lab"")\n                print("""")\n        print(""*** You might want to declare GAMMAPY_DATA env variable"")\n        print(f""export GAMMAPY_DATA={GAMMAPY_DATA}"")\n        print("""")\n'"
gammapy/scripts/info.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport importlib\nimport logging\nimport os\nimport platform\nimport sys\nimport warnings\nimport click\nfrom gammapy import __version__\n\nlog = logging.getLogger(__name__)\n\n# Should be in sync with `docs/install/dependencies.rst`\nGAMMAPY_DEPENDENCIES = [\n    # required\n    ""numpy"",\n    ""scipy"",\n    ""astropy"",\n    ""regions"",\n    ""click"",\n    ""yaml"",\n    # ""pydantic"",  # has no __version__\n    # optional\n    ""IPython"",\n    # ""jupyter"",   # has no __version__\n    ""jupyterlab"",\n    ""matplotlib"",\n    ""pandas"",\n    ""healpy"",\n    ""iminuit"",\n    ""sherpa"",\n    ""naima"",\n    ""emcee"",\n    ""corner"",\n    ""parfive"",\n]\n\nGAMMAPY_ENV_VARIABLES = [""GAMMAPY_DATA""]\n\n\n@click.command(name=""info"")\n@click.option(""--system/--no-system"", default=True, help=""Show system info"")\n@click.option(""--version/--no-version"", default=True, help=""Show version"")\n@click.option(\n    ""--dependencies/--no-dependencies"", default=True, help=""Show dependencies""\n)\n@click.option(""--envvar/--no-envvar"", default=True, help=""Show environment variables"")\ndef cli_info(system, version, dependencies, envvar):\n    """"""Display information about Gammapy\n    """"""\n    if system:\n        info = get_info_system()\n        print_info(info=info, title=""System"")\n\n    if version:\n        info = get_info_version()\n        print_info(info=info, title=""Gammapy package"")\n\n    if dependencies:\n        info = get_info_dependencies()\n        print_info(info=info, title=""Other packages"")\n\n    if envvar:\n        info = get_info_envvar()\n        print_info(info=info, title=""Gammapy environment variables"")\n\n\ndef print_info(info, title):\n    """"""Print Gammapy info.""""""\n    info_all = f""\\n{title}:\\n\\n""\n\n    for key, value in info.items():\n        info_all += f""\\t{key:22s} : {value:<10s} \\n""\n\n    print(info_all)\n\n\ndef get_info_system():\n    """"""Get info about user system""""""\n    return {\n        ""python_executable"": sys.executable,\n        ""python_version"": platform.python_version(),\n        ""machine"": platform.machine(),\n        ""system"": platform.system(),\n    }\n\n\ndef get_info_version():\n    """"""Get detailed info about Gammapy version.""""""\n    info = {""version"": __version__}\n    try:\n        path = sys.modules[""gammapy""].__path__[0]\n    except:\n        path = ""unknown""\n    info[""path""] = path\n\n    return info\n\n\ndef get_info_dependencies():\n    """"""Get info about Gammapy dependencies.""""""\n    info = {}\n    for name in GAMMAPY_DEPENDENCIES:\n        try:\n            with warnings.catch_warnings():\n                warnings.simplefilter(""ignore"")\n                module = importlib.import_module(name)\n\n            module_version = getattr(module, ""__version__"", ""no version info found"")\n        except ImportError:\n            module_version = ""not installed""\n        info[name] = module_version\n    return info\n\n\ndef get_info_envvar():\n    """"""Get info about Gammapy environment variables.""""""\n    return {name: os.environ.get(name, ""not set"") for name in GAMMAPY_ENV_VARIABLES}\n'"
gammapy/scripts/jupyter.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Command line tool to perform actions on jupyter notebooks.""""""\nimport logging\nimport os\nimport subprocess\nimport sys\nimport time\nimport click\n\nlog = logging.getLogger(__name__)\n\nOFF = [""GAMMA_CAT"", ""GAMMAPY_DATA"", ""GAMMAPY_EXTRA"", ""GAMMAPY_FERMI_LAT_DATA""]\n\n\n@click.command(name=""run"")\n@click.pass_context\n@click.option(\n    ""--tutor"",\n    is_flag=True,\n    default=False,\n    help=""Tutorials environment?"",\n    show_default=True,\n)\n@click.option(""--kernel"", default=""python3"", help=""Kernel name"", show_default=True)\ndef cli_jupyter_run(ctx, tutor, kernel):\n    """"""Execute Jupyter notebooks.""""""\n    with environment(OFF, tutor, ctx):\n        for path in ctx.obj[""paths""]:\n            notebook_test(path, kernel)\n\n\ndef execute_notebook(path, kernel=""python3"", loglevel=30):\n    """"""Execute a Jupyter notebook.""""""\n    cmd = [\n        sys.executable,\n        ""-m"",\n        ""jupyter"",\n        ""nbconvert"",\n        ""--allow-errors"",\n        f""--log-level={loglevel}"",\n        ""--ExecutePreprocessor.timeout=None"",\n        f""--ExecutePreprocessor.kernel_name={kernel}"",\n        ""--to"",\n        ""notebook"",\n        ""--inplace"",\n        ""--execute"",\n        f""{path}"",\n    ]\n    t = time.time()\n    completed_process = subprocess.run(cmd)\n    t = time.time() - t\n    if completed_process.returncode:\n        log.error(f""Error executing file: {path}"")\n        return False\n    else:\n        log.info(f""   ... Executing duration: {t:.1f} seconds"")\n        return True\n\n\n@click.command(name=""strip"")\n@click.pass_context\ndef cli_jupyter_strip(ctx):\n    """"""Strip output cells.""""""\n    import nbformat\n\n    for path in ctx.obj[""paths""]:\n        rawnb = nbformat.read(str(path), as_version=nbformat.NO_CONVERT)\n\n        rawnb[""metadata""].pop(""pycharm"", None)\n\n        for cell in rawnb.cells:\n            if cell[""cell_type""] == ""code"":\n                cell[""metadata""].pop(""pycharm"", None)\n                cell[""execution_count""] = None\n                cell[""outputs""] = []\n\n        nbformat.write(rawnb, str(path))\n        log.info(f""Strip output cells in notebook: {path}"")\n\n\n@click.command(name=""black"")\n@click.pass_context\ndef cli_jupyter_black(ctx):\n    """"""Format code cells with black.""""""\n    import nbformat\n\n    for path in ctx.obj[""paths""]:\n        rawnb = nbformat.read(str(path), as_version=nbformat.NO_CONVERT)\n        blacknb = BlackNotebook(rawnb)\n        blacknb.blackformat()\n        rawnb = blacknb.rawnb\n        nbformat.write(rawnb, str(path))\n        log.info(f""Applied black to notebook: {path}"")\n\n\nclass BlackNotebook:\n    """"""Manage the process of black formatting.\n\n    Probably this will become available directly in the future.\n\n    See https://github.com/ambv/black/issues/298#issuecomment-476960082\n    """"""\n\n    MAGIC_TAG = ""###-MAGIC TAG-""\n\n    def __init__(self, rawnb):\n        self.rawnb = rawnb\n\n    def blackformat(self):\n        """"""Format code cells.""""""\n        import black\n\n        for cell in self.rawnb.cells:\n            fmt = cell[""source""]\n            if cell[""cell_type""] == ""code"":\n                try:\n                    fmt = ""\\n"".join(self.tag_magics(fmt))\n                    has_semicolon = fmt.endswith("";"")\n                    fmt = black.format_str(\n                        src_contents=fmt, mode=black.FileMode(line_length=79)\n                    ).rstrip()\n                    if has_semicolon:\n                        fmt += "";""\n                except Exception as ex:\n                    log.info(ex)\n                fmt = fmt.replace(self.MAGIC_TAG, """")\n            cell[""source""] = fmt\n\n    def tag_magics(self, cellcode):\n        """"""Comment magic commands.""""""\n        lines = cellcode.splitlines(False)\n        for line in lines:\n            if line.startswith(""%"") or line.startswith(""!""):\n                magic_line = self.MAGIC_TAG + line\n                yield magic_line\n            else:\n                yield line\n\n\n@click.command(name=""test"")\n@click.pass_context\n@click.option(\n    ""--tutor"",\n    is_flag=True,\n    default=False,\n    help=""Tutorials environment?"",\n    show_default=True,\n)\n@click.option(""--kernel"", default=""python3"", help=""Kernel name"", show_default=True)\ndef cli_jupyter_test(ctx, tutor, kernel):\n    """"""Check if Jupyter notebooks are broken.""""""\n    with environment(OFF, tutor, ctx):\n        for path in ctx.obj[""paths""]:\n            notebook_test(path, kernel)\n\n\ndef notebook_test(path, kernel=""python3""):\n    """"""Execute and parse a Jupyter notebook exposing broken cells.""""""\n    import nbformat\n\n    log.info(f""   ... EXECUTING: {path}"")\n    passed = execute_notebook(path, kernel)\n    rawnb = nbformat.read(str(path), as_version=nbformat.NO_CONVERT)\n    report = """"\n\n    for cell in rawnb.cells:\n        if ""outputs"" in cell.keys():\n            for output in cell[""outputs""]:\n                if output[""output_type""] == ""error"":\n                    passed = False\n                    traceitems = [""--TRACEBACK: ""]\n                    for o in output[""traceback""]:\n                        traceitems.append(f""{o}"")\n                    traceback = ""\\n"".join(traceitems)\n                    infos = ""\\n\\n{} in cell [{}]\\n\\n"" ""--SOURCE CODE: \\n{}\\n\\n"".format(\n                        output[""ename""], cell[""execution_count""], cell[""source""]\n                    )\n                    report = infos + traceback\n                    break\n        if not passed:\n            break\n\n    if passed:\n        log.info(""   ... PASSED"")\n        return True\n    else:\n        log.info(""   ... FAILED"")\n        log.info(report)\n        return False\n\n\nclass environment:\n    """"""Helper for setting environmental variables.""""""\n\n    def __init__(self, envs, tutor, ctx):\n        self.envs = envs\n        self.tutor = tutor\n        self.ctx = ctx\n\n    def __enter__(self):\n        self.old = os.environ\n        if self.tutor:\n            for item in self.envs:\n                if item in os.environ:\n                    del os.environ[item]\n                    log.info(f""Unsetting {item} environment variable."")\n            abspath = self.ctx.obj[""pathsrc""].absolute()\n            datapath = abspath.parent / ""datasets""\n            if abspath.is_file():\n                datapath = abspath.parent.parent / ""datasets""\n            os.environ[""GAMMAPY_DATA""] = str(datapath)\n            log.info(f""Setting GAMMAPY_DATA={datapath}"")\n\n    def __exit__(self, type, value, traceback):\n        if self.tutor:\n            os.environ = self.old\n            log.info(""Environment variables recovered."")\n'"
gammapy/scripts/main.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport sys\nimport warnings\nfrom pathlib import Path\nimport click\nfrom gammapy import __version__\n\n\n# We implement the --version following the example from here:\n# http://click.pocoo.org/5/options/#callbacks-and-eager-options\ndef print_version(ctx, param, value):\n    if not value or ctx.resilient_parsing:\n        return\n    print(f""gammapy version {__version__}"")\n    ctx.exit()\n\n\n# http://click.pocoo.org/5/documentation/#help-parameter-customization\nCONTEXT_SETTINGS = dict(help_option_names=[""-h"", ""--help""])\n\n# https://click.palletsprojects.com/en/5.x/python3/#unicode-literals\nclick.disable_unicode_literals_warning = True\n\n\n@click.group(""gammapy"", context_settings=CONTEXT_SETTINGS)\n@click.option(\n    ""--log-level"",\n    default=""info"",\n    help=""Logging verbosity level."",\n    type=click.Choice([""debug"", ""info"", ""warning"", ""error""]),\n)\n@click.option(""--ignore-warnings"", is_flag=True, help=""Ignore warnings?"")\n@click.option(\n    ""--version"",\n    is_flag=True,\n    callback=print_version,\n    expose_value=False,\n    is_eager=True,\n    help=""Print version and exit."",\n)\ndef cli(log_level, ignore_warnings):  # noqa: D301\n    """"""Gammapy command line interface (CLI).\n\n    Gammapy is a Python package for gamma-ray astronomy.\n\n    Use ``--help`` to see available sub-commands, as well as the available\n    arguments and options for each sub-command.\n\n    For further information, see https://gammapy.org/ and https://docs.gammapy.org/\n\n    \\b\n    Examples\n    --------\n\n    \\b\n    $ gammapy --help\n    $ gammapy --version\n    $ gammapy info --help\n    $ gammapy info\n    """"""\n    logging.basicConfig(level=log_level.upper())\n\n    if ignore_warnings:\n        warnings.simplefilter(""ignore"")\n\n\n@cli.group(""analysis"")\ndef cli_analysis():\n    """"""Automation of configuration driven data reduction process.\n\n    \\b\n    Examples\n    --------\n\n    \\b\n    $ gammapy analysis config\n    $ gammapy analysis run\n    $ gammapy analysis config --overwrite\n    $ gammapy analysis config --filename myconfig.yaml\n    $ gammapy analysis run --filename myconfig.yaml\n    """"""\n\n\n@cli.group(""download"", short_help=""Download datasets and notebooks"")\n@click.pass_context\ndef cli_download(ctx):  # noqa: D301\n    """"""Download notebooks, scripts and datasets.\n\n    \\b\n    Download notebooks published as tutorials, example python scripts and the\n    related datasets needed to execute them. It is also possible to download\n    individual notebooks, scrtipts or datasets.\n    \\b\n    - The option `tutorials` will download versioned folders for the notebooks\n    and python scripts into a `gammapy-tutorials` folder created at the current\n    working directory, as well as the datasets needed to reproduce them.\n    \\b\n    - The option `notebooks` will download the notebook files used in the tutorials\n    into a `gammapy-notebooks` folder created at the current working directory.\n    \\b\n    - The option `scripts` will download a collection of example python scripts\n    into a `gammapy-scripts` folder created at the current working directory.\n    \\b\n    - The option `datasets` will download the datasets used by Gammapy into a\n    `gammapy-datasets` folder created at the current working directory.\n\n    \\b\n    Examples\n    --------\n\n    \\b\n    $ gammapy download scripts\n    $ gammapy download notebooks\n    $ gammapy download datasets\n    $ gammapy download tutorials --release 0.8\n    $ gammapy download notebooks --src overview\n    $ gammapy download datasets --src fermi-3fhl-gc --out localfolder/\n    """"""\n\n\n@cli.group(""jupyter"", short_help=""Perform actions on notebooks"")\n@click.option(""--src"", default=""."", help=""Local folder or Jupyter notebook filename."")\n@click.pass_context\ndef cli_jupyter(ctx, src):  # noqa: D301\n    """"""\n    Perform a series of actions on Jupyter notebooks.\n\n    The chosen action is applied by default for every Jupyter notebook present\n    in the current working directory.\n\n    \\b\n    Examples\n    --------\n    \\b\n    $ gammapy jupyter strip\n    $ gammapy jupyter --src mynotebooks.ipynb run\n    $ gammapy jupyter --src myfolder/tutorials test\n    $ gammapy jupyter black\n    """"""\n    log = logging.getLogger(__name__)\n\n    path = Path(src)\n    if not path.exists():\n        log.error(f""File or folder {src} not found."")\n        sys.exit()\n\n    if path.is_dir():\n        paths = list(path.glob(""*.ipynb""))\n    else:\n        paths = [path]\n\n    ctx.obj = {""paths"": paths, ""pathsrc"": path}\n\n\ndef add_subcommands():\n    from .info import cli_info\n\n    cli.add_command(cli_info)\n\n    from .check import cli_check\n\n    cli.add_command(cli_check)\n\n    from .download import cli_download_notebooks\n\n    cli_download.add_command(cli_download_notebooks)\n\n    from .download import cli_download_scripts\n\n    cli_download.add_command(cli_download_scripts)\n\n    from .download import cli_download_datasets\n\n    cli_download.add_command(cli_download_datasets)\n\n    from .download import cli_download_tutorials\n\n    cli_download.add_command(cli_download_tutorials)\n\n    from .jupyter import cli_jupyter_black\n\n    cli_jupyter.add_command(cli_jupyter_black)\n\n    from .jupyter import cli_jupyter_strip\n\n    cli_jupyter.add_command(cli_jupyter_strip)\n\n    from .jupyter import cli_jupyter_run\n\n    cli_jupyter.add_command(cli_jupyter_run)\n\n    from .jupyter import cli_jupyter_test\n\n    cli_jupyter.add_command(cli_jupyter_test)\n\n    from .analysis import cli_make_config\n\n    cli_analysis.add_command(cli_make_config)\n\n    from .analysis import cli_run_analysis\n\n    cli_analysis.add_command(cli_run_analysis)\n\n\nadd_subcommands()\n'"
gammapy/stats/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Statistics.""""""\nfrom .counts_statistic import *\nfrom .feldman_cousins import *\nfrom .fit_statistics import *\nfrom .fit_statistics_cython import *\n'"
gammapy/stats/counts_statistic.py,22,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport abc\nimport numpy as np\nfrom scipy.optimize import brentq, newton\nfrom scipy.stats import chi2\nfrom .fit_statistics import cash, wstat\n\n__all__ = [""WStatCountsStatistic"", ""CashCountsStatistic""]\n\n\nclass CountsStatistic(abc.ABC):\n    @property\n    def excess(self):\n        return self.n_on - self.background\n\n    @property\n    def delta_ts(self):\n        """"""Return TS difference of measured excess versus no excess.""""""\n        # Remove (small) negative delta TS due to error in root finding\n        delta_ts = np.clip(self.TS_null - self.TS_max, 0, None)\n        return delta_ts\n\n    @property\n    def significance(self):\n        """"""Return statistical significance of measured excess.""""""\n        return np.sign(self.excess) * np.sqrt(self.delta_ts)\n\n    @property\n    def p_value(self):\n        """"""Return p_value of measured excess.""""""\n        return chi2.sf(self.delta_ts, 1)\n\n    def compute_errn(self, n_sigma=1.0):\n        """"""Compute downward excess uncertainties.\n\n        Searches the signal value for which the test statistics is n_sigma**2 away from the maximum.\n\n        Parameters\n        ----------\n        n_sigma : float\n            Confidence level of the uncertainty expressed in number of sigma. Default is 1.\n        """"""\n        errn = np.zeros_like(self.n_on, dtype=""float"")\n        min_range = self.excess - 2 * n_sigma * (self.error + 1)\n\n        it = np.nditer(errn, flags=[""multi_index""])\n        while not it.finished:\n            try:\n                res = brentq(\n                    self._stat_fcn,\n                    min_range[it.multi_index],\n                    self.excess[it.multi_index],\n                    args=(self.TS_max[it.multi_index] + n_sigma ** 2, it.multi_index),\n                )\n                errn[it.multi_index] = res - self.excess[it.multi_index]\n            except ValueError:\n                errn[it.multi_index] = -self.n_on[it.multi_index]\n            it.iternext()\n\n        return errn\n\n    def compute_errp(self, n_sigma=1):\n        """"""Compute upward excess uncertainties.\n\n        Searches the signal value for which the test statistics is n_sigma**2 away from the maximum.\n\n        Parameters\n        ----------\n        n_sigma : float\n            Confidence level of the uncertainty expressed in number of sigma. Default is 1.\n        """"""\n        errp = np.zeros_like(self.n_on, dtype=""float"")\n        max_range = self.excess + 2 * n_sigma * (self.error + 1)\n\n        it = np.nditer(errp, flags=[""multi_index""])\n        while not it.finished:\n            errp[it.multi_index] = brentq(\n                self._stat_fcn,\n                self.excess[it.multi_index],\n                max_range[it.multi_index],\n                args=(self.TS_max[it.multi_index] + n_sigma ** 2, it.multi_index),\n            )\n            it.iternext()\n\n        return errp - self.excess\n\n    def compute_upper_limit(self, n_sigma=3):\n        """"""Compute upper limit on the signal.\n\n        Searches the signal value for which the test statistics is n_sigma**2 away from the maximum\n        or from 0 if the measured excess is negative.\n\n        Parameters\n        ----------\n        n_sigma : float\n            Confidence level of the upper limit expressed in number of sigma. Default is 3.\n        """"""\n        ul = np.zeros_like(self.n_on, dtype=""float"")\n\n        min_range = np.maximum(0, self.excess)\n        max_range = min_range + 2 * n_sigma * (self.error + 1)\n        it = np.nditer(ul, flags=[""multi_index""])\n\n        while not it.finished:\n            TS_ref = self._stat_fcn(min_range[it.multi_index], 0.0, it.multi_index)\n\n            ul[it.multi_index] = brentq(\n                self._stat_fcn,\n                min_range[it.multi_index],\n                max_range[it.multi_index],\n                args=(TS_ref + n_sigma ** 2, it.multi_index),\n            )\n            it.iternext()\n\n        return ul\n\n    def excess_matching_significance(self, significance):\n        """"""Compute excess matching a given significance.\n\n        This function is the inverse of `significance`.\n\n        Parameters\n        ----------\n        significance : float\n            Significance\n\n        Returns\n        -------\n        excess : `numpy.ndarray`\n            Excess\n        """"""\n        excess = np.zeros_like(self.background, dtype=""float"")\n        it = np.nditer(excess, flags=[""multi_index""])\n\n        while not it.finished:\n            try:\n                excess[it.multi_index] = newton(\n                    self._excess_matching_significance_fcn,\n                    np.sqrt(self.background[it.multi_index]) * significance,\n                    args=(significance, it.multi_index),\n                )\n            except:\n                excess[it.multi_index] = np.nan\n\n            it.iternext()\n        return excess\n\n\nclass CashCountsStatistic(CountsStatistic):\n    """"""Class to compute statistics (significance, asymmetric errors , ul) for Poisson distributed variable\n    with known background.\n\n    Parameters\n    ----------\n    n_on : int\n        Measured counts\n    mu_bkg : float\n        Expected level of background\n    """"""\n\n    def __init__(self, n_on, mu_bkg):\n        self.n_on = np.asanyarray(n_on)\n        self.mu_bkg = np.asanyarray(mu_bkg)\n\n    @property\n    def background(self):\n        return self.mu_bkg\n\n    @property\n    def error(self):\n        """"""Approximate error from the covariance matrix.""""""\n        return np.sqrt(self.n_on)\n\n    @property\n    def TS_null(self):\n        """"""Stat value for null hypothesis, i.e. 0 expected signal counts""""""\n        return cash(self.n_on, self.mu_bkg + 0)\n\n    @property\n    def TS_max(self):\n        """"""Stat value for best fit hypothesis, i.e. expected signal mu = n_on - mu_bkg""""""\n        return cash(self.n_on, self.n_on)\n\n    def _stat_fcn(self, mu, delta=0, index=None):\n        return cash(self.n_on[index], self.mu_bkg[index] + mu) - delta\n\n    def _excess_matching_significance_fcn(self, excess, significance, index):\n        TS0 = cash(excess + self.background[index], self.mu_bkg[index])\n        TS1 = cash(excess + self.background[index], self.mu_bkg[index] + excess)\n        return np.sign(excess) * np.sqrt(np.clip(TS0 - TS1, 0, None)) - significance\n\n\nclass WStatCountsStatistic(CountsStatistic):\n    """"""Class to compute statistics (significance, asymmetric errors , ul) for Poisson distributed variable\n    with unknown background.\n\n    Parameters\n    ----------\n    n_on : int\n        Measured counts in signal (ON) region\n    n_off : int\n        Measured counts in background only (OFF) region\n    alpha : float\n        Acceptance ratio of ON and OFF measurements\n    """"""\n\n    def __init__(self, n_on, n_off, alpha):\n        self.n_on = np.asanyarray(n_on)\n        self.n_off = np.asanyarray(n_off)\n        self.alpha = np.asanyarray(alpha)\n\n    @property\n    def background(self):\n        return self.alpha * self.n_off\n\n    @property\n    def error(self):\n        """"""Approximate error from the covariance matrix.""""""\n        return np.sqrt(self.n_on + self.alpha ** 2 * self.n_off)\n\n    @property\n    def TS_null(self):\n        """"""Stat value for null hypothesis, i.e. 0 expected signal counts""""""\n        return wstat(self.n_on, self.n_off, self.alpha, 0)\n\n    @property\n    def TS_max(self):\n        """"""Stat value for best fit hypothesis, i.e. expected signal mu = n_on - alpha * n_off""""""\n        return wstat(self.n_on, self.n_off, self.alpha, self.excess)\n\n    def _stat_fcn(self, mu, delta=0, index=None):\n        return wstat(self.n_on[index], self.n_off[index], self.alpha[index], mu) - delta\n\n    def _excess_matching_significance_fcn(self, excess, significance, index):\n        TS0 = wstat(\n            excess + self.background[index], self.n_off[index], self.alpha[index], 0\n        )\n        TS1 = wstat(\n            excess + self.background[index],\n            self.n_off[index],\n            self.alpha[index],\n            excess,\n        )\n        return np.sign(excess) * np.sqrt(np.clip(TS0 - TS1, 0, None)) - significance\n'"
gammapy/stats/feldman_cousins.py,22,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Feldman Cousins algorithm to compute parameter confidence limits.""""""\nimport logging\nimport numpy as np\nimport scipy.stats\n\n__all__ = [\n    ""fc_find_acceptance_interval_gauss"",\n    ""fc_find_acceptance_interval_poisson"",\n    ""fc_construct_acceptance_intervals_pdfs"",\n    ""fc_get_limits"",\n    ""fc_fix_limits"",\n    ""fc_find_limit"",\n    ""fc_find_average_upper_limit"",\n    ""fc_construct_acceptance_intervals"",\n]\n\nlog = logging.getLogger(__name__)\n\n\ndef fc_find_acceptance_interval_gauss(mu, sigma, x_bins, alpha):\n    r""""""\n    Analytical acceptance interval for Gaussian with boundary at the origin.\n\n    .. math:: \\int_{x_{min}}^{x_{max}} P(x|mu)\\mathrm{d}x = alpha\n\n    For more information see :ref:`documentation <feldman_cousins>`.\n\n    Parameters\n    ----------\n    mu : float\n        Mean of the Gaussian\n    sigma : float\n        Width of the Gaussian\n    x_bins : array-like\n        Bins in x\n    alpha : float\n        Desired confidence level\n\n    Returns\n    -------\n    (x_min, x_max) : tuple of floats\n        Acceptance interval\n    """"""\n\n    dist = scipy.stats.norm(loc=mu, scale=sigma)\n\n    x_bin_width = x_bins[1] - x_bins[0]\n\n    p = []\n    r = []\n\n    for x in x_bins:\n        p.append(dist.pdf(x) * x_bin_width)\n        # This is the formula from the FC paper\n        if mu == 0 and sigma == 1:\n            if x < 0:\n                r.append(np.exp(mu * (x - mu * 0.5)))\n            else:\n                r.append(np.exp(-0.5 * np.power((x - mu), 2)))\n        # This is the more general formula\n        else:\n            # Implementing the boundary condition at zero\n            mu_best = max(0, x)\n            prob_mu_best = scipy.stats.norm.pdf(x, loc=mu_best, scale=sigma)\n            # probMuBest should never be zero. Check it just in case.\n            if prob_mu_best == 0.0:\n                r.append(0.0)\n            else:\n                r.append(p[-1] / prob_mu_best)\n\n    p = np.asarray(p)\n    r = np.asarray(r)\n\n    if sum(p) < alpha:\n        raise ValueError(\n            ""X bins don\'t contain enough probability to reach ""\n            ""desired confidence level for this mu!""\n        )\n\n    rank = scipy.stats.rankdata(-r, method=""dense"")\n\n    index_array = np.arange(x_bins.size)\n\n    rank_sorted, index_array_sorted = zip(*sorted(zip(rank, index_array)))\n\n    index_min = index_array_sorted[0]\n    index_max = index_array_sorted[0]\n\n    p_sum = 0\n\n    for i in range(len(rank_sorted)):\n        if index_array_sorted[i] < index_min:\n            index_min = index_array_sorted[i]\n        if index_array_sorted[i] > index_max:\n            index_max = index_array_sorted[i]\n        p_sum += p[index_array_sorted[i]]\n        if p_sum >= alpha:\n            break\n\n    return x_bins[index_min], x_bins[index_max] + x_bin_width\n\n\ndef fc_find_acceptance_interval_poisson(mu, background, x_bins, alpha):\n    r""""""Analytical acceptance interval for Poisson process with background.\n\n    .. math:: \\int_{x_{min}}^{x_{max}} P(x|mu)\\mathrm{d}x = alpha\n\n    For more information see :ref:`documentation <feldman_cousins>`.\n\n    Parameters\n    ----------\n    mu : float\n        Mean of the signal\n    background : float\n        Mean of the background\n    x_bins : array-like\n        Bins in x\n    alpha : float\n        Desired confidence level\n\n    Returns\n    -------\n    (x_min, x_max) : tuple of floats\n        Acceptance interval\n    """"""\n    dist = scipy.stats.poisson(mu=mu + background)\n\n    x_bin_width = x_bins[1] - x_bins[0]\n\n    p = []\n    r = []\n\n    for x in x_bins:\n        p.append(dist.pmf(x))\n        # Implementing the boundary condition at zero\n        muBest = max(0, x - background)\n        probMuBest = scipy.stats.poisson.pmf(x, mu=muBest + background)\n        # probMuBest should never be zero. Check it just in case.\n        if probMuBest == 0.0:\n            r.append(0.0)\n        else:\n            r.append(p[-1] / probMuBest)\n\n    p = np.asarray(p)\n    r = np.asarray(r)\n\n    if sum(p) < alpha:\n        raise ValueError(\n            ""X bins don\'t contain enough probability to reach ""\n            ""desired confidence level for this mu!""\n        )\n\n    rank = scipy.stats.rankdata(-r, method=""dense"")\n\n    index_array = np.arange(x_bins.size)\n\n    rank_sorted, index_array_sorted = zip(*sorted(zip(rank, index_array)))\n\n    index_min = index_array_sorted[0]\n    index_max = index_array_sorted[0]\n\n    p_sum = 0\n\n    for i in range(len(rank_sorted)):\n        if index_array_sorted[i] < index_min:\n            index_min = index_array_sorted[i]\n        if index_array_sorted[i] > index_max:\n            index_max = index_array_sorted[i]\n        p_sum += p[index_array_sorted[i]]\n        if p_sum >= alpha:\n            break\n\n    return x_bins[index_min], x_bins[index_max] + x_bin_width\n\n\ndef fc_construct_acceptance_intervals_pdfs(matrix, alpha):\n    r""""""Numerically choose bins a la Feldman Cousins ordering principle.\n\n    For more information see :ref:`documentation <feldman_cousins>`.\n\n    Parameters\n    ----------\n    matrix : array-like\n        A list of x PDFs for increasing values of mue.\n    alpha : float\n        Desired confidence level\n\n    Returns\n    -------\n    distributions_scaled : ndarray\n        Acceptance intervals (1 means inside, 0 means outside)\n    """"""\n    number_mus = len(matrix)\n\n    distributions_scaled = np.asarray(matrix)\n    distributions_re_scaled = np.asarray(matrix)\n    summed_propability = np.zeros(number_mus)\n\n    # Step 1:\n    # For each x, find the greatest likelihood in the mu direction.\n    # greatest_likelihood is an array of length number_x_bins.\n    greatest_likelihood = np.amax(distributions_scaled, axis=0)\n\n    # Set to some value if none of the bins has an entry to avoid\n    # division by zero\n    greatest_likelihood[greatest_likelihood == 0] = 1\n\n    # Step 2:\n    # Scale all entries by this value\n    distributions_re_scaled /= greatest_likelihood\n\n    # Step 3 (Feldman Cousins Ordering principle):\n    # For each mu, get the largest entry\n    largest_entry = np.argmax(distributions_re_scaled, axis=1)\n    # Set the rank to 1 and add probability\n    for i in range(number_mus):\n        distributions_re_scaled[i][largest_entry[i]] = 1\n        summed_propability[i] += np.sum(\n            np.where(distributions_re_scaled[i] == 1, distributions_scaled[i], 0)\n        )\n        distributions_scaled[i] = np.where(\n            distributions_re_scaled[i] == 1, 1, distributions_scaled[i]\n        )\n\n    # Identify next largest entry not yet ranked. While there are entries\n    # smaller than 1, some bins don\'t have a rank yet.\n    while np.amin(distributions_re_scaled) < 1:\n        # For each mu, this is the largest rank attributed so far.\n        largest_rank = np.amax(distributions_re_scaled, axis=1)\n        # For each mu, this is the largest entry that is not yet a rank.\n        largest_entry = np.where(\n            distributions_re_scaled < 1, distributions_re_scaled, -1\n        )\n        # For each mu, this is the position of the largest entry that is not yet a rank.\n        largest_entry_position = np.argmax(largest_entry, axis=1)\n        # Invalidate indices where there is no maximum (every entry is already a rank)\n        largest_entry_position = [\n            largest_entry_position[i]\n            if largest_entry[i][largest_entry_position[i]] != -1\n            else -1\n            for i in range(len(largest_entry_position))\n        ]\n        # Replace the largest entry with the highest rank so far plus one\n        # Add the probability\n        for i in range(number_mus):\n            if largest_entry_position[i] == -1:\n                continue\n            distributions_re_scaled[i][largest_entry_position[i]] = largest_rank[i] + 1\n            if summed_propability[i] < alpha:\n                summed_propability[i] += distributions_scaled[i][\n                    largest_entry_position[i]\n                ]\n                distributions_scaled[i][largest_entry_position[i]] = 1\n            else:\n                distributions_scaled[i][largest_entry_position[i]] = 0\n\n    return distributions_scaled\n\n\ndef fc_get_limits(mu_bins, x_bins, acceptance_intervals):\n    r""""""Find lower and upper limit from acceptance intervals.\n\n    For more information see :ref:`documentation <feldman_cousins>`.\n\n    Parameters\n    ----------\n    mu_bins : array-like\n        The bins used in mue direction.\n    x_bins : array-like\n        The bins of the x distribution\n    acceptance_intervals : array-like\n        The output of fc_construct_acceptance_intervals_pdfs.\n\n    Returns\n    -------\n    lower_limit : array-like\n        Feldman Cousins lower limit x-coordinates\n    upper_limit : array-like\n        Feldman Cousins upper limit x-coordinates\n    x_values : array-like\n        All the points that are inside the acceptance intervals\n    """"""\n    upper_limit = []\n    lower_limit = []\n    x_values = []\n\n    number_mu = len(mu_bins)\n    number_bins_x = len(x_bins)\n\n    for mu in range(number_mu):\n        upper_limit.append(-1)\n        lower_limit.append(-1)\n        x_values.append([])\n        acceptance_interval = acceptance_intervals[mu]\n        for x in range(number_bins_x):\n            # This point lies in the acceptance interval\n            if acceptance_interval[x] == 1:\n                x_value = x_bins[x]\n                x_values[-1].append(x_value)\n                # Upper limit is first point where this condition is true\n                if upper_limit[-1] == -1:\n                    upper_limit[-1] = x_value\n                # Lower limit is first point after this condition is not true\n                if x == number_bins_x - 1:\n                    lower_limit[-1] = x_value\n                else:\n                    lower_limit[-1] = x_bins[x + 1]\n\n    return lower_limit, upper_limit, x_values\n\n\ndef fc_fix_limits(lower_limit, upper_limit):\n    r""""""Push limits outwards as described in the FC paper.\n\n    For more information see :ref:`documentation <feldman_cousins>`.\n\n    Parameters\n    ----------\n    lower_limit : array-like\n        Feldman Cousins lower limit x-coordinates\n    upper_limit : array-like\n        Feldman Cousins upper limit x-coordinates\n    """"""\n    all_fixed = False\n\n    while not all_fixed:\n        all_fixed = True\n        for j in range(1, len(upper_limit)):\n            if upper_limit[j] < upper_limit[j - 1]:\n                upper_limit[j - 1] = upper_limit[j]\n                all_fixed = False\n        for j in range(1, len(lower_limit)):\n            if lower_limit[j] < lower_limit[j - 1]:\n                lower_limit[j] = lower_limit[j - 1]\n                all_fixed = False\n\n\ndef fc_find_limit(x_value, x_values, y_values):\n    r""""""Find the limit for a given x measurement.\n\n    See also: :ref:`feldman_cousins`\n\n    Parameters\n    ----------\n    x_value : float\n        The measured x value for which the upper limit is wanted.\n    x_values : array-like\n        The x coordinates of the confidence belt.\n    y_values : array-like\n        The y coordinates of the confidence belt.\n\n    Returns\n    -------\n    limit : float\n        The Feldman Cousins limit\n    """"""\n    if x_value > max(x_values):\n        raise ValueError(""Measured x outside of confidence belt!"")\n\n    # Loop through the x-values in reverse order\n    for i in reversed(range(len(x_values))):\n        current_x = x_values[i]\n        # The measured value sits on a bin edge. In this case we want the upper\n        # most point to be conservative, so it\'s the first point where this\n        # condition is true.\n        if x_value == current_x:\n            return y_values[i]\n        # If the current value lies between two bins, take the higher y-value\n        # in order to be conservative.\n        if x_value > current_x:\n            return y_values[i + 1]\n\n\ndef fc_find_average_upper_limit(x_bins, matrix, upper_limit, mu_bins, prob_limit=1e-5):\n    r""""""Calculate the average upper limit for a confidence belt.\n\n    See also: :ref:`feldman_cousins`\n\n    Parameters\n    ----------\n    x_bins : array-like\n        Bins in x direction\n    matrix : array-like\n        A list of x PDFs for increasing values of mue\n        (same as for fc_construct_acceptance_intervals_pdfs).\n    upper_limit : array-like\n        Feldman Cousins upper limit x-coordinates\n    mu_bins : array-like\n        The bins used in mue direction.\n    prob_limit : float\n        Probability value at which x values are no longer considered for the\n        average limit.\n\n    Returns\n    -------\n    average_limit : float\n        Average upper limit\n    """"""\n    average_limit = 0\n    number_points = len(x_bins)\n\n    for i in range(number_points):\n        # Bins with very low probability will not contribute to average limit\n        if matrix[0][i] < prob_limit:\n            continue\n        try:\n            limit = fc_find_limit(x_bins[i], upper_limit, mu_bins)\n        except:\n            log.warning(""Warning: Calculation of average limit incomplete!"")\n            log.warning(""Add more bins in mu direction or decrease prob_limit."")\n            return average_limit\n        average_limit += matrix[0][i] * limit\n\n    return average_limit\n\n\ndef fc_construct_acceptance_intervals(distribution_dict, bins, alpha):\n    r""""""Convenience function that calculates the PDF for the user.\n\n    For more information see :ref:`documentation <feldman_cousins>`.\n\n    Parameters\n    ----------\n    distribution_dict : dict\n        Keys are mu values and value is an array-like list of x values\n    bins : array-like\n        The bins the x distribution will have\n    alpha : float\n        Desired confidence level\n\n    Returns\n    -------\n    acceptance_intervals : ndarray\n        Acceptance intervals (1 means inside, 0 means outside)\n    """"""\n    distributions_scaled = []\n\n    # Histogram gets rid of the last bin, so add one extra\n    bin_width = bins[1] - bins[0]\n    new_bins = np.concatenate((bins, np.array([bins[-1] + bin_width])), axis=0)\n\n    # Histogram and normalise each distribution so it is a real PDF\n    for _, distribution in sorted(distribution_dict.items()):\n        entries = np.histogram(distribution, bins=new_bins)[0]\n        integral = float(sum(entries))\n        distributions_scaled.append(entries / integral)\n\n    acceptance_intervals = fc_construct_acceptance_intervals_pdfs(\n        distributions_scaled, alpha\n    )\n\n    return acceptance_intervals\n'"
gammapy/stats/fit_statistics.py,32,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Common fit statistics used in gamma-ray astronomy.\n\nsee :ref:`fit-statistics`\n""""""\nimport numpy as np\n\n__all__ = [""cash"", ""cstat"", ""wstat"", ""get_wstat_mu_bkg"", ""get_wstat_gof_terms""]\n\nN_ON_MIN = 1e-25\n\n\ndef cash(n_on, mu_on):\n    r""""""Cash statistic, for Poisson data.\n\n    The Cash statistic is defined as:\n\n    .. math::\n        C = 2 \\left( \\mu_{on} - n_{on} \\log \\mu_{on} \\right)\n\n    and :math:`C = 0` where :math:`\\mu <= 0`.\n    For more information see :ref:`fit-statistics`\n\n    Parameters\n    ----------\n    n_on : array_like\n        Observed counts\n    mu_on : array_like\n        Expected counts\n\n    Returns\n    -------\n    stat : ndarray\n        Statistic per bin\n\n    References\n    ----------\n    * `Sherpa statistics page section on the Cash statistic\n      <http://cxc.cfa.harvard.edu/sherpa/statistics/#cash>`_\n    * `Sherpa help page on the Cash statistic\n      <http://cxc.harvard.edu/sherpa/ahelp/cash.html>`_\n    * `Cash 1979, ApJ 228, 939\n      <https://ui.adsabs.harvard.edu/abs/1979ApJ...228..939C>`_\n    """"""\n    n_on = np.asanyarray(n_on)\n    mu_on = np.asanyarray(mu_on)\n\n    # suppress zero division warnings, they are corrected below\n    with np.errstate(divide=""ignore"", invalid=""ignore""):\n        stat = 2 * (mu_on - n_on * np.log(mu_on))\n        stat = np.where(mu_on > 0, stat, 0)\n    return stat\n\n\ndef cstat(n_on, mu_on, n_on_min=N_ON_MIN):\n    r""""""C statistic, for Poisson data.\n\n    The C statistic is defined as\n\n    .. math::\n        C = 2 \\left[ \\mu_{on} - n_{on} + n_{on}\n            (\\log(n_{on}) - log(\\mu_{on}) \\right]\n\n    and :math:`C = 0` where :math:`\\mu_{on} <= 0`.\n\n    ``n_on_min`` handles the case where ``n_on`` is 0 or less and\n    the log cannot be taken.\n    For more information see :ref:`fit-statistics`\n\n    Parameters\n    ----------\n    n_on : array_like\n        Observed counts\n    mu_on : array_like\n        Expected counts\n    n_on_min : array_like\n        ``n_on`` = ``n_on_min`` where ``n_on`` <= ``n_on_min.``\n\n    Returns\n    -------\n    stat : ndarray\n        Statistic per bin\n\n    References\n    ----------\n    * `Sherpa stats page section on the C statistic\n      <http://cxc.cfa.harvard.edu/sherpa/statistics/#cstat>`_\n    * `Sherpa help page on the C statistic\n      <http://cxc.harvard.edu/sherpa/ahelp/cash.html>`_\n    * `Cash 1979, ApJ 228, 939\n      <https://ui.adsabs.harvard.edu/abs/1979ApJ...228..939C>`_\n    """"""\n    n_on = np.asanyarray(n_on, dtype=np.float64)\n    mu_on = np.asanyarray(mu_on, dtype=np.float64)\n    n_on_min = np.asanyarray(n_on_min, dtype=np.float64)\n\n    n_on = np.where(n_on <= n_on_min, n_on_min, n_on)\n\n    term1 = np.log(n_on) - np.log(mu_on)\n    stat = 2 * (mu_on - n_on + n_on * term1)\n    stat = np.where(mu_on > 0, stat, 0)\n\n    return stat\n\n\ndef wstat(n_on, n_off, alpha, mu_sig, mu_bkg=None, extra_terms=True):\n    r""""""W statistic, for Poisson data with Poisson background.\n\n    For a definition of WStat see :ref:`wstat`. If ``mu_bkg`` is not provided\n    it will be calculated according to the profile likelihood formula.\n\n    Parameters\n    ----------\n    n_on : array_like\n        Total observed counts\n    n_off : array_like\n        Total observed background counts\n    alpha : array_like\n        Exposure ratio between on and off region\n    mu_sig : array_like\n        Signal expected counts\n    mu_bkg : array_like, optional\n        Background expected counts\n    extra_terms : bool, optional\n        Add model independent terms to convert stat into goodness-of-fit\n        parameter, default: True\n\n    Returns\n    -------\n    stat : ndarray\n        Statistic per bin\n\n    References\n    ----------\n    * `Habilitation M. de Naurois, p. 141\n      <http://inspirehep.net/record/1122589/files/these_short.pdf>`_\n    * `XSPEC page on Poisson data with Poisson background\n      <https://heasarc.nasa.gov/xanadu/xspec/manual/XSappendixStatistics.html>`_\n    """"""\n    # Note: This is equivalent to what\'s defined on the XSPEC page under the\n    # following assumptions\n    # t_s * m_i = mu_sig\n    # t_b * m_b = mu_bkg\n    # t_s / t_b = alpha\n\n    n_on = np.atleast_1d(np.asanyarray(n_on, dtype=np.float64))\n    n_off = np.atleast_1d(np.asanyarray(n_off, dtype=np.float64))\n    alpha = np.atleast_1d(np.asanyarray(alpha, dtype=np.float64))\n    mu_sig = np.atleast_1d(np.asanyarray(mu_sig, dtype=np.float64))\n\n    if mu_bkg is None:\n        mu_bkg = get_wstat_mu_bkg(n_on, n_off, alpha, mu_sig)\n\n    term1 = mu_sig + (1 + alpha) * mu_bkg\n\n    # suppress zero division warnings, they are corrected below\n    with np.errstate(divide=""ignore"", invalid=""ignore""):\n        # This is a false positive error from pylint\n        # See https://github.com/PyCQA/pylint/issues/2436\n        term2_ = -n_on * np.log(\n            mu_sig + alpha * mu_bkg\n        )  # pylint:disable=invalid-unary-operand-type\n    # Handle n_on == 0\n    condition = n_on == 0\n    term2 = np.where(condition, 0, term2_)\n\n    # suppress zero division warnings, they are corrected below\n    with np.errstate(divide=""ignore"", invalid=""ignore""):\n        # This is a false positive error from pylint\n        # See https://github.com/PyCQA/pylint/issues/2436\n        term3_ = -n_off * np.log(mu_bkg)  # pylint:disable=invalid-unary-operand-type\n    # Handle n_off == 0\n    condition = n_off == 0\n    term3 = np.where(condition, 0, term3_)\n\n    stat = 2 * (term1 + term2 + term3)\n\n    if extra_terms:\n        stat += get_wstat_gof_terms(n_on, n_off)\n\n    return stat\n\n\ndef get_wstat_mu_bkg(n_on, n_off, alpha, mu_sig):\n    """"""Background estimate ``mu_bkg`` for WSTAT.\n\n    See :ref:`wstat`.\n    """"""\n    n_on = np.atleast_1d(np.asanyarray(n_on, dtype=np.float64))\n    n_off = np.atleast_1d(np.asanyarray(n_off, dtype=np.float64))\n    alpha = np.atleast_1d(np.asanyarray(alpha, dtype=np.float64))\n    mu_sig = np.atleast_1d(np.asanyarray(mu_sig, dtype=np.float64))\n\n    # NOTE: Corner cases in the docs are all handled correcty by this formula\n    C = alpha * (n_on + n_off) - (1 + alpha) * mu_sig\n    D = np.sqrt(C ** 2 + 4 * alpha * (alpha + 1) * n_off * mu_sig)\n    mu_bkg = (C + D) / (2 * alpha * (alpha + 1))\n\n    return mu_bkg\n\n\ndef get_wstat_gof_terms(n_on, n_off):\n    """"""Goodness of fit terms for WSTAT.\n\n    See :ref:`wstat`.\n    """"""\n    term = np.zeros(n_on.shape)\n\n    # suppress zero division warnings, they are corrected below\n    with np.errstate(divide=""ignore"", invalid=""ignore""):\n        term1 = -n_on * (1 - np.log(n_on))\n        term2 = -n_off * (1 - np.log(n_off))\n\n    term += np.where(n_on == 0, 0, term1)\n    term += np.where(n_off == 0, 0, term2)\n\n    return 2 * term\n'"
gammapy/tests/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Gammapy integration and system tests.\n\nThis package can be used for tests that involved several\nGammapy sub-packages, or that don\'t fit anywhere else.\n""""""\n'"
gammapy/time/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Time analysis.""""""\nfrom .period import *\nfrom .variability import *\n'"
gammapy/time/period.py,17,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nimport scipy.optimize\n\n__all__ = [""robust_periodogram"", ""plot_periodogram""]\n\n\ndef robust_periodogram(time, flux, flux_err=None, periods=None, loss=""linear"", scale=1):\n    """"""\n    Compute a light curve\'s period.\n\n    A single harmonic model is fitted to the light curve.\n    The periodogram returns the power for each period.\n    The maximum power indicates the period of the light curve, assuming an underlying periodic process.\n\n    The fitting can be done by ordinary least square regression (Lomb-Scargle periodogram) or robust regression.\n    For robust regression, the scipy object `~scipy.optimize.least_squares` is called.\n    For an introduction to robust regression techniques and loss functions, see [1]_ and [2]_.\n\n    The significance of a periodogram peak can be evaluated in terms of a false alarm probability.\n    It can be computed with the `~false_alarm_probability`-method of `~astropy`, assuming Gaussian white noise light curves.\n    For an introduction to the false alarm probability of periodogram peaks, see :ref:`stats-lombscargle`.\n\n    The periodogram is biased by measurement errors, high order modes and sampling of the light curve.\n    To evaluate the impact of the sampling, compute the spectral window function with the `astropy.timeseries.LombScargle` class.\n\n    The function returns a dictionary with the following content:\n\n    - ``periods`` (`numpy.ndarray`) -- Period grid in units of ``t``\n    - ``power`` (`numpy.ndarray`) -- Periodogram peaks at periods of ``pgrid``\n    - ``best_period`` (float) -- Period of the highest periodogram peak\n\n    Parameters\n    ----------\n    time : `numpy.ndarray`\n        Time array of the light curve\n    flux : `numpy.ndarray`\n        Flux array of the light curve\n    flux_err : `numpy.ndarray`\n        Flux error array of the light curve. Default is 1.\n    periods : `numpy.ndarray`\n        Period grid on which the periodogram is performed.\n        If not given, a linear grid will be computed limited by the length of the light curve and the Nyquist frequency.\n    loss : {\'linear\', \'soft_l1\', \'huber\', \'cauchy\', \'arctan\'}\n        Loss function for the robust regression.\n        Default is \'linear\', resulting in the Lomb-Scargle periodogram.\n    scale : float (optional, default=1)\n        Loss scale parameter to define margin between inlier and outlier residuals.\n        If not given, will be set to 1.\n\n    Returns\n    -------\n    results : `dict`\n        Results dictionary (see description above).\n\n    References\n    ----------\n    .. [1] Nikolay Mayorov (2015), ""Robust nonlinear regression in scipy"",\n       see `here <http://scipy-cookbook.readthedocs.io/items/robust_regression.html>`__\n    .. [2] Thieler et at. (2016), ""RobPer: An R Package to Calculate Periodograms for Light Curves Based on Robust Regression"",\n       see `here <https://www.jstatsoft.org/article/view/v069i09>`__\n    """"""\n    if flux_err is None:\n        flux_err = np.ones_like(flux)\n\n    # set up period grid\n    if periods is None:\n        periods = _period_grid(time)\n\n    # compute periodogram\n    psd_data = _robust_regression(time, flux, flux_err, periods, loss, scale)\n\n    # find period of highest periodogram peak\n    best_period = periods[np.argmax(psd_data)]\n\n    return {""periods"": periods, ""power"": psd_data, ""best_period"": best_period}\n\n\ndef _period_grid(time):\n    """"""\n    Generates the period grid for the periodogram\n    """"""\n    number_obs = len(time)\n    length_lc = np.max(time) - np.min(time)\n\n    dt = 2 * length_lc / number_obs\n    max_period = np.rint(length_lc / dt) * dt\n    min_period = dt\n\n    periods = np.arange(min_period, max_period + dt, dt)\n\n    return periods\n\n\ndef _model(beta0, x, period, t, y, dy):\n    """"""\n    Computes the residuals of the periodic model\n    """"""\n    x[:, 1] = np.cos(2 * np.pi * t / period)\n    x[:, 2] = np.sin(2 * np.pi * t / period)\n\n    return (y - np.dot(x, beta0.T)) / dy\n\n\ndef _noise(mu, t, y, dy):\n    """"""\n    Residuals of the noise-only model.\n    """"""\n    return (mu * np.ones(len(t)) - y) / dy\n\n\ndef _robust_regression(time, flux, flux_err, periods, loss, scale):\n    """"""\n    Periodogram peaks for a given loss function and scale.\n    """"""\n    beta0 = np.array([0, 1, 0])\n    mu = np.median(flux)\n    x = np.ones([len(time), len(beta0)])\n    chi_model = np.empty([len(periods)])\n    chi_noise = np.empty([len(periods)])\n\n    for i in range(len(periods)):\n        chi_model[i] = scipy.optimize.least_squares(\n            _model,\n            beta0,\n            loss=loss,\n            f_scale=scale,\n            args=(x, periods[i], time, flux, flux_err),\n        ).cost\n        chi_noise[i] = scipy.optimize.least_squares(\n            _noise, mu, loss=loss, f_scale=scale, args=(time, flux, flux_err)\n        ).cost\n    power = 1 - chi_model / chi_noise\n\n    return power\n\n\ndef plot_periodogram(\n    time, flux, periods, power, flux_err=None, best_period=None, fap=None\n):\n    """"""\n    Plot a light curve and its periodogram.\n\n    The highest period of the periodogram and its false alarm probability (FAP) is added to the plot, if given.\n\n    Parameters\n    ----------\n    time : `numpy.ndarray`\n        Time array of the light curve\n    flux : `numpy.ndarray`\n        Flux array of the light curve\n    periods : `numpy.ndarray`\n        Periods for the periodogram\n    power : `numpy.ndarray`\n        Periodogram peaks of the data\n    flux_err : `numpy.ndarray` (optional, default=None)\n        Flux error array of the light curve.\n        Is set to 0 if not given.\n    best_period : float (optional, default=None)\n        Period of the highest periodogram peak\n    fap : float (optional, default=None)\n        False alarm probability of ``best_period`` under a certain significance criterion.\n\n    Returns\n    -------\n    fig : `~matplotlib.figure.Figure`\n        Matplotlib figure\n    """"""\n    import matplotlib.pyplot as plt\n\n    if flux_err is None:\n        flux_err = np.zeros_like(flux)\n\n    # set up the figure & axes for plotting\n    fig = plt.figure(figsize=(16, 9))\n    grid_spec = plt.GridSpec(2, 1)\n\n    # plot the light curve\n    ax = fig.add_subplot(grid_spec[0, :])\n    ax.errorbar(\n        time, flux, flux_err, fmt=""ok"", label=""light curve"", elinewidth=1.5, capsize=0\n    )\n    ax.set_xlabel(""time"")\n    ax.set_ylabel(""flux"")\n    ax.legend()\n\n    # plot the periodogram\n    ax = fig.add_subplot(grid_spec[1, :])\n    ax.plot(periods, power, c=""k"", label=""periodogram"")\n    # mark the best period and label with significance\n    if best_period is not None:\n        if fap is None:\n            raise ValueError(\n                ""Must give a false alarm probability if you give a best_period""\n            )\n\n        # set precision for period format\n        pre = int(abs(np.floor(np.log10(np.max(np.diff(periods))))))\n        label = ""Detected period p = {:.{}f} with {:.2E} FAP"".format(\n            best_period, pre, fap\n        )\n        ymax = power[periods == best_period]\n        ax.axvline(best_period, ymin=0, ymax=ymax, label=label, c=""r"")\n\n    ax.set_xlabel(""period"")\n    ax.set_ylabel(""power"")\n    ax.set_xlim(0, np.max(periods))\n    ax.legend()\n\n    return fig\n'"
gammapy/time/variability.py,8,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nimport scipy.stats as stats\n\n__all__ = [""compute_fvar"", ""compute_chisq""]\n\n\ndef compute_fvar(flux, flux_err):\n    r""""""Calculate the fractional excess variance.\n\n    This method accesses the the ``FLUX`` and ``FLUX_ERR`` columns\n    from the lightcurve data.\n\n    The fractional excess variance :math:`F_{var}`, an intrinsic\n    variability estimator, is given by\n\n    .. math::\n        F_{var} = \\sqrt{\\frac{S^{2} - \\bar{\\sigma^{2}}}{\\bar{x}^{2}}}.\n\n    It is the excess variance after accounting for the measurement errors\n    on the light curve :math:`\\sigma`. :math:`S` is the variance.\n\n    Parameters\n    ----------\n    flux : `~astropy.units.Quantity`\n        the measured fluxes\n    flux_err : `~astropy.units.Quantity`\n        the error on measured fluxes\n\n    Returns\n    -------\n    fvar, fvar_err : `~numpy.ndarray`\n        Fractional excess variance.\n\n    References\n    ----------\n    .. [Vaughan2003] ""On characterizing the variability properties of X-ray light\n       curves from active galaxies"", Vaughan et al. (2003)\n       https://ui.adsabs.harvard.edu/abs/2003MNRAS.345.1271V\n    """"""\n    flux_mean = np.mean(flux)\n    n_points = len(flux)\n\n    s_square = np.sum((flux - flux_mean) ** 2) / (n_points - 1)\n    sig_square = np.nansum(flux_err ** 2) / n_points\n    fvar = np.sqrt(np.abs(s_square - sig_square)) / flux_mean\n\n    sigxserr_a = np.sqrt(2 / n_points) * (sig_square / flux_mean) ** 2\n    sigxserr_b = np.sqrt(sig_square / n_points) * (2 * fvar / flux_mean)\n    sigxserr = np.sqrt(sigxserr_a ** 2 + sigxserr_b ** 2)\n    fvar_err = sigxserr / (2 * fvar)\n\n    return fvar, fvar_err\n\n\ndef compute_chisq(flux):\n    """"""Calculate the chi-square test for `LightCurve`.\n\n    Chisquare test is a variability estimator. It computes\n    deviations from the expected value here mean value\n\n    Parameters\n    ----------\n    flux : `~astropy.units.Quantity`\n        the measured fluxes\n\n    Returns\n    -------\n    ChiSq, P-value : tuple of float or `~numpy.ndarray`\n        Tuple of Chi-square and P-value\n    """"""\n    yexp = np.mean(flux)\n    yobs = flux.data\n    chi2, pval = stats.chisquare(yobs, yexp)\n    return chi2, pval\n'"
gammapy/utils/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""\nUtility functions and classes used throughout Gammapy.\n\nYou have to import sub-modules of `gammapy.utils` directly,\nthe `gammapy.utils` namespace is empty.\n\nExamples::\n\n    from gammapy.utils.interpolation import ScaledRegularGridInterpolator\n    from gammapy.utils.nddata import NDDataArray\n""""""\n'"
gammapy/utils/array.py,7,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Utility functions to deal with arrays and quantities.""""""\nimport numpy as np\nimport scipy.ndimage\nimport scipy.signal\nfrom astropy.convolution import Gaussian2DKernel\n\n__all__ = [\n    ""array_stats_str"",\n    ""shape_2N"",\n    ""shape_divisible_by"",\n    ""symmetric_crop_pad_width"",\n]\n\n\ndef array_stats_str(x, label=""""):\n    """"""Make a string summarising some stats for an array.\n\n    Parameters\n    ----------\n    x : array-like\n        Array\n    label : str, optional\n        Label\n\n    Returns\n    -------\n    stats_str : str\n        String with array stats\n    """"""\n    x = np.asanyarray(x)\n\n    ss = """"\n    if label:\n        ss += f""{label:15s}: ""\n\n    min = x.min()\n    max = x.max()\n    size = x.size\n\n    fmt = ""size = {size:5d}, min = {min:6.3f}, max = {max:6.3f}\\n""\n    ss += fmt.format(**locals())\n\n    return ss\n\n\ndef shape_2N(shape, N=3):\n    """"""\n    Round a given shape to values that are divisible by 2^N.\n\n    Parameters\n    ----------\n    shape : tuple\n        Input shape.\n    N : int (default = 3), optional\n        Exponent of two.\n\n    Returns\n    -------\n    new_shape : Tuple\n        New shape extended to integers divisible by 2^N\n    """"""\n    shape = np.array(shape)\n    new_shape = shape + (2 ** N - np.mod(shape, 2 ** N))\n    return tuple(new_shape)\n\n\ndef shape_divisible_by(shape, factor):\n    """"""\n    Round a given shape to values that are divisible by factor.\n\n    Parameters\n    ----------\n    shape : tuple\n        Input shape.\n    factor : int\n        Divisor.\n\n    Returns\n    -------\n    new_shape : Tuple\n        New shape extended to integers divisible by factor\n    """"""\n    shape = np.array(shape)\n    new_shape = shape + (shape % factor)\n    return tuple(new_shape)\n\n\ndef symmetric_crop_pad_width(shape, new_shape):\n    """"""\n    Compute symmetric crop or pad width.\n\n    To obtain a new shape from a given old shape of an array.\n\n    Parameters\n    ----------\n    shape : tuple\n        Old shape\n    new_shape : tuple or str\n        New shape\n    """"""\n    xdiff = abs(shape[1] - new_shape[1])\n    ydiff = abs(shape[0] - new_shape[0])\n\n    if (np.array([xdiff, ydiff]) % 2).any():\n        raise ValueError(\n            ""For symmetric crop / pad width, difference to new shape ""\n            ""must be even in all axes.""\n        )\n\n    ywidth = (ydiff // 2, ydiff // 2)\n    xwidth = (xdiff // 2, xdiff // 2)\n    return ywidth, xwidth\n\n\ndef _fftconvolve_wrap(kernel, data):\n    # wrap gaussian filter as a special case, because the gain in\n    # performance is factor ~100\n    if isinstance(kernel, Gaussian2DKernel):\n        width = kernel.model.x_stddev.value\n        norm = kernel.array.sum()\n        return norm * scipy.ndimage.gaussian_filter(data, width)\n    else:\n        return scipy.signal.fftconvolve(\n            data.astype(np.float32), kernel.array, mode=""same""\n        )\n\n\ndef scale_cube(data, kernels):\n    """"""\n    Compute scale space cube.\n\n    Compute scale space cube by convolving the data with a set of kernels and\n    stack the resulting images along the third axis.\n\n    Parameters\n    ----------\n    data : `~numpy.ndarray`\n        Input data.\n    kernels: list of `~astropy.convolution.Kernel`\n        List of convolution kernels.\n\n    Returns\n    -------\n    cube : `~numpy.ndarray`\n        Array of the shape (len(kernels), data.shape)\n    """"""\n    return np.dstack(list([_fftconvolve_wrap(kernel, data) for kernel in kernels]))\n'"
gammapy/utils/docs.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Gammapy documentation generator utilities.\n\nThis is for the Sphinx docs build of Gammapy itself,\nit is used from ``docs/conf.py``.\n\nIt should not be imported or used from other scripts or packages,\nbecause we will change it for our use case whenever we like.\n\nDocutils / Sphinx is notoriously hard to understand / learn about.\nHere\'s some good resources with working examples:\n\n- https://doughellmann.com/blog/2010/05/09/defining-custom-roles-in-sphinx/\n- http://docutils.sourceforge.net/docs/howto/rst-directives.html\n- https://github.com/docutils-mirror/docutils/blob/master/docutils/parsers/rst/directives/images.py\n- https://github.com/sphinx-doc/sphinx/blob/master/sphinx/directives/other.py\n- https://github.com/bokeh/bokeh/tree/master/bokeh/sphinxext\n""""""\nimport os\nfrom distutils.util import strtobool\nfrom pathlib import Path\nimport nbformat\nfrom docutils.parsers.rst.directives import register_directive\nfrom docutils.parsers.rst.directives.body import CodeBlock\nfrom docutils.parsers.rst.directives.images import Image\nfrom docutils.parsers.rst.directives.misc import Include\nfrom nbformat.v4 import new_markdown_cell\nfrom sphinx.util import logging\nfrom gammapy import __version__\nfrom gammapy.analysis import AnalysisConfig\n\ntry:\n    gammapy_data_path = Path(os.environ[""GAMMAPY_DATA""])\n    HAS_GP_DATA = True\nexcept KeyError:\n    HAS_GP_DATA = False\n\nlog = logging.getLogger(__name__)\n\n\nclass HowtoHLI(Include):\n    """"""Directive to insert how-to for high-level interface""""""\n\n    def run(self):\n        raw = """"\n        section = self.arguments[0]\n        doc = AnalysisConfig._get_doc_sections()\n        for keyword in doc.keys():\n            if section == """" or section == keyword:\n                raw += doc[keyword]\n        include_lines = raw.splitlines()\n        codeblock = CodeBlock(\n            self.name,\n            [],\n            self.options,\n            include_lines,  # content\n            self.lineno,\n            self.content_offset,\n            self.block_text,\n            self.state,\n            self.state_machine,\n        )\n        return codeblock.run()\n\n\nclass DocsImage(Image):\n    """"""Directive to add optional images from gammapy-data""""""\n\n    def run(self):\n        filename = self.arguments[0]\n\n        if HAS_GP_DATA:\n            path = gammapy_data_path / ""figures"" / filename\n            if not path.is_file():\n                msg = ""Error in {} directive: File not found: {}"".format(\n                    self.name, path\n                )\n                raise self.error(msg)\n            # Usually Sphinx doesn\'t support absolute paths\n            # But passing a POSIX string of the absolute path\n            # with an extra ""/"" at the start seems to do the trick\n            self.arguments[0] = ""/"" + path.absolute().as_posix()\n        else:\n            self.warning(\n                ""GAMMAPY_DATA not available. ""\n                ""Missing image: name: {!r} filename: {!r}"".format(self.name, filename)\n            )\n            self.options[""alt""] = self.arguments[1] if len(self.arguments) > 1 else """"\n\n        return super().run()\n\n\ndef gammapy_sphinx_ext_activate():\n    if HAS_GP_DATA:\n        log.info(f""*** Found GAMMAPY_DATA = {gammapy_data_path}"")\n        log.info(""*** Nice!"")\n    else:\n        log.info(""*** gammapy-data *not* found."")\n        log.info(""*** Set the GAMMAPY_DATA environment variable!"")\n        log.info(""*** Docs build will be incomplete."")\n\n    # Register our directives and roles with Sphinx\n    register_directive(""gp-image"", DocsImage)\n    register_directive(""gp-howto-hli"", HowtoHLI)\n\n\ndef parse_notebooks(folder, url_docs):\n    """"""Parse Jupyter notebook.\n\n    Modifies raw and html-fixed notebooks so they will not have broken links\n    to other files in the documentation. Adds a box to the sphinx formatted\n    notebooks with info and links to the *.ipynb and *.py files.\n    """"""\n    release_number_binder = __version__\n    if ""dev"" in __version__:\n        release_number_binder = ""master""\n\n    DOWNLOAD_CELL = """"""\n<div class=""alert alert-info"">\n\n**This is a fixed-text formatted version of a Jupyter notebook**\n\n- Try online [![Binder](https://static.mybinder.org/badge.svg)](https://mybinder.org/v2/gh/gammapy/gammapy-webpage/v{release_number_binder}?urlpath=lab/tree/{nb_filename})\n- You can contribute with your own notebooks in this\n[GitHub repository](https://github.com/gammapy/gammapy/tree/master/tutorials).\n- **Source files:**\n[{nb_filename}](../_static/notebooks/{nb_filename}) |\n[{py_filename}](../_static/notebooks/{py_filename})\n</div>\n""""""\n\n    for nbpath in list(folder.glob(""*.ipynb"")):\n        if str(folder) == ""notebooks"":\n\n            # add binder cell\n            nb_filename = str(nbpath).replace(""notebooks/"", """")\n            py_filename = nb_filename.replace(""ipynb"", ""py"")\n            ctx = dict(\n                nb_filename=nb_filename,\n                py_filename=py_filename,\n                release_number_binder=release_number_binder,\n            )\n            strcell = DOWNLOAD_CELL.format(**ctx)\n            rawnb = nbformat.read(str(nbpath), as_version=nbformat.NO_CONVERT)\n\n            if ""nbsphinx"" not in rawnb.metadata:\n                rawnb.metadata[""nbsphinx""] = {""orphan"": bool(""true"")}\n                rawnb.cells.insert(0, new_markdown_cell(strcell))\n\n                # add latex format\n                for cell in rawnb.cells:\n                    if ""outputs"" in cell.keys():\n                        for output in cell[""outputs""]:\n                            if output[""output_type""] == ""execute_result"":\n                                if ""text/latex"" in output[""data""].keys():\n                                    output[""data""][""text/latex""] = output[""data""][\n                                        ""text/latex""\n                                    ].replace(""$"", ""$$"")\n                nbformat.write(rawnb, str(nbpath))\n\n\ndef gammapy_sphinx_notebooks(setup_cfg):\n    """"""Manage creation of Sphinx-formatted notebooks.""""""\n    if not strtobool(setup_cfg[""build_notebooks""]):\n        log.info(""Config build_notebooks is False; skipping notebook processing"")\n        return\n\n    url_docs = setup_cfg[""url_docs""]\n\n    # fix links\n    filled_notebooks_folder = Path(""notebooks"")\n    download_notebooks_folder = Path(""_static"") / ""notebooks""\n\n    if filled_notebooks_folder.is_dir():\n        parse_notebooks(filled_notebooks_folder, url_docs)\n        parse_notebooks(download_notebooks_folder, url_docs)\n'"
gammapy/utils/fits.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""\n.. _utils-fits:\n\nGammapy FITS utilities\n======================\n\n.. _utils-fits-tables:\n\nFITS tables\n-----------\n\nIn Gammapy we use the nice `astropy.table.Table` class a lot to represent all\nkinds of data (e.g. event lists, spectral points, light curves, source catalogs).\nThe most common format to store tables is FITS. In this section we show examples\nand mention some limitations of Table FITS I/O.\n\nAlso, note that if you have the choice, you might want to use a better format\nthan FITS to store tables. All of these are nice and have very good support\nin Astropy: ``ECSV``, ``HDF5``, ``ASDF``.\n\nIn Astropy, there is the `~astropy.table.Table` class with a nice data model\nand API. Let\'s make an example table object that has some metadata on the\ntable and columns of different types:\n\n>>> from astropy.table import Table, Column\n>>> table = Table(meta={\'version\': 42})\n>>> table[\'a\'] = [1, 2]\n>>> table[\'b\'] = Column([1, 2], unit=\'m\', description=\'Velocity\')\n>>> table[\'c\'] = [\'x\', \'yy\']\n>>> table\n<Table length=2>\n  a     b    c\n        m\nint64 int64 str2\n----- ----- ----\n    1     1    x\n    2     2   yy\n>>> table.info()\n<Table length=2>\nname dtype unit description\n---- ----- ---- -----------\n   a int64\n   b int64    m    Velocity\n   c  str2\n\nWriting and reading the table to FITS is easy:\n\n>>> table.write(\'table.fits\')\n>>> table2 = Table.read(\'table.fits\')\n\nand works very nicely, column units and description round-trip:\n\n>>> table2\n<Table length=2>\n  a      b      c\n         m\nint64 float64 bytes2\n----- ------- ------\n    1     1.0      x\n    2     2.0     yy\n>>> table2.info()\n<Table length=2>\nname  dtype  unit description\n---- ------- ---- -----------\n   a   int64\n   b float64    m    Velocity\n   c  bytes2\n\nThis is with Astropy 3.0. In older versions of Astropy this didn\'t use\nto work, namely column description was lost.\n\nLooking at the FITS header and ``table2.meta``, one can see that\nthey are cheating a bit, storing table meta in ``COMMENT``:\n\n>>> fits.open(\'table.fits\')[1].header\nXTENSION= \'BINTABLE\'           / binary table extension\nBITPIX  =                    8 / array data type\nNAXIS   =                    2 / number of array dimensions\nNAXIS1  =                   18 / length of dimension 1\nNAXIS2  =                    2 / length of dimension 2\nPCOUNT  =                    0 / number of group parameters\nGCOUNT  =                    1 / number of groups\nTFIELDS =                    3 / number of table fields\nTTYPE1  = \'a       \'\nTFORM1  = \'K       \'\nTTYPE2  = \'b       \'\nTFORM2  = \'K       \'\nTUNIT2  = \'m       \'\nTTYPE3  = \'c       \'\nTFORM3  = \'2A      \'\nVERSION =                   42\nCOMMENT --BEGIN-ASTROPY-SERIALIZED-COLUMNS--\nCOMMENT datatype:\nCOMMENT - {name: a, datatype: int64}\nCOMMENT - {name: b, unit: m, datatype: int64, description: Velocity}\nCOMMENT - {name: c, datatype: string}\nCOMMENT meta:\nCOMMENT   __serialized_columns__: {}\nCOMMENT --END-ASTROPY-SERIALIZED-COLUMNS--\n>>> table2.meta\nOrderedDict([(\'VERSION\', 42),\n             (\'comments\',\n              [\'--BEGIN-ASTROPY-SERIALIZED-COLUMNS--\',\n               \'datatype:\',\n               \'- {name: a, datatype: int64}\',\n               \'- {name: b, unit: m, datatype: int64, description: Velocity}\',\n               \'- {name: c, datatype: string}\',\n               \'meta:\',\n               \'  __serialized_columns__: {}\',\n               \'--END-ASTROPY-SERIALIZED-COLUMNS--\'])])\n\n\nTODO: we\'ll have to see how to handle this, i.e. if we want that\nbehaviour or not, and how to get consistent output accross Astropy versions.\nSee https://github.com/astropy/astropy/issues/7364\n\nLet\'s make sure for the following examples we have a clean ``table.meta``\nlike we did at the start:\n\n>>> table.meta.pop(\'comments\', None)\n\nIf you want to avoid writing to disk, the way to directly convert between\n`~astropy.table.Table` and `~astropy.io.fits.BinTableHDU` is like this:\n\n>>> hdu = fits.BinTableHDU(table)\n\nThis calls `astropy.io.fits.table_to_hdu` in ``BinTableHDU.__init__``,\ni.e. if you don\'t pass extra options, this is equivalent to\n\n>>> hdu = fits.table_to_hdu(table)\n\nHowever, in this case, the column metadata that is serialized is\ndoesn\'t include the column ``description``.\nTODO: how to get consistent behaviour and FITS headers?\n\n>>> hdu.header\nXTENSION= \'BINTABLE\'           / binary table extension\nBITPIX  =                    8 / array data type\nNAXIS   =                    2 / number of array dimensions\nNAXIS1  =                   18 / length of dimension 1\nNAXIS2  =                    2 / length of dimension 2\nPCOUNT  =                    0 / number of group parameters\nGCOUNT  =                    1 / number of groups\nTFIELDS =                    3 / number of table fields\nVERSION =                   42\nTTYPE1  = \'a       \'\nTFORM1  = \'K       \'\nTTYPE2  = \'b       \'\nTFORM2  = \'K       \'\nTUNIT2  = \'m       \'\nTTYPE3  = \'c       \'\nTFORM3  = \'2A      \'\n\nSomewhat surprisingly, ``Table(hdu)`` doesn\'t work and there is no\n``hdu_to_table`` function; instead you have to call ``Table.read``\nif you want to convert in the other direction:\n\n>>> table2 = Table.read(hdu)\n>>> table2.info()\n<Table length=2>\nname dtype unit\n---- ----- ----\n   a int64\n   b int64    m\n   c  str2\n\n\nAnother trick worth knowing about is how to read and write multiple tables\nto one FITS file. There is support in the ``Table`` API to read any HDU\nfrom a FITS file with multiple HDUs via the ``hdu`` option to ``Table.read``;\nyou can pass an integer HDU index or an HDU extension name string\n(see :ref:`astropy:table_io_fits`).\n\nFor writing (or if you prefer also for reading) multiple tables, you should\nuse the in-memory conversion to HDU objects and the `~astropy.io.fits.HDUList`\nlike this::\n\n    hdu_list = fits.HDUList([\n        fits.PrimaryHDU(),\n        fits.BinTableHDU(table, name=\'spam\'),\n        fits.BinTableHDU(table, name=\'ham\'),\n    ])\n    hdu_list.info()\n    hdu_list.writeto(\'tables.fits\')\n\n\nFor further information on Astropy, see the Astropy docs at\n:ref:`astropy:astropy-table` and :ref:`astropy:table_io_fits`.\n\nWe will have to see if / what we need here in `gammapy.utils.fits`\nas a stable and nice interface on top of what Astropy provides.\n""""""\nimport numpy as np\nfrom astropy.coordinates import Angle, EarthLocation\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom astropy.units import Quantity\n\n__all__ = [""energy_axis_to_ebounds"", ""earth_location_from_dict"", ""LazyFitsData""]\n\n\nclass LazyFitsData(object):\n    """"""A lazy FITS data descriptor.\n\n    Parameters\n    ----------\n    cache : bool\n        Whether to cache the data.\n    """"""\n\n    def __init__(self, cache=True):\n        self.cache = cache\n\n    def __set_name__(self, owner, name):\n        self.name = name\n\n    def __get__(self, instance, objtype):\n        if instance is None:\n            # Accessed on a class, not an instance\n            return self\n\n        value = instance.__dict__.get(self.name)\n        if value is not None:\n            return value\n        else:\n            hdu_loc = instance.__dict__.get(self.name + ""_hdu"")\n            value = hdu_loc.load()\n            if self.cache:\n                instance.__dict__[self.name] = value\n            return value\n\n    def __set__(self, instance, value):\n        from gammapy.data import HDULocation\n\n        if isinstance(value, HDULocation):\n            instance.__dict__[self.name + ""_hdu""] = value\n        else:\n            instance.__dict__[self.name] = value\n\n\ndef energy_axis_to_ebounds(energy):\n    """"""Convert `~astropy.units.Quantity` to OGIP ``EBOUNDS`` extension.\n\n    See https://heasarc.gsfc.nasa.gov/docs/heasarc/caldb/docs/memos/cal_gen_92_002/cal_gen_92_002.html#tth_sEc3.2\n    """"""\n    energy = Quantity(energy)\n    table = Table()\n\n    table[""CHANNEL""] = np.arange(len(energy) - 1, dtype=np.int16)\n    table[""E_MIN""] = energy[:-1]\n    table[""E_MAX""] = energy[1:]\n\n    hdu = fits.BinTableHDU(table)\n\n    header = hdu.header\n    header[""EXTNAME""] = ""EBOUNDS"", ""Name of this binary table extension""\n    header[""TELESCOP""] = ""DUMMY"", ""Mission/satellite name""\n    header[""INSTRUME""] = ""DUMMY"", ""Instrument/detector""\n    header[""FILTER""] = ""None"", ""Filter information""\n    header[""CHANTYPE""] = ""PHA"", ""Type of channels (PHA, PI etc)""\n    header[""DETCHANS""] = len(energy) - 1, ""Total number of detector PHA channels""\n    header[""HDUCLASS""] = ""OGIP"", ""Organisation devising file format""\n    header[""HDUCLAS1""] = ""RESPONSE"", ""File relates to response of instrument""\n    header[""HDUCLAS2""] = ""EBOUNDS"", ""This is an EBOUNDS extension""\n    header[""HDUVERS""] = ""1.2.0"", ""Version of file format""\n\n    return hdu\n\n\ndef ebounds_to_energy_axis(ebounds):\n    """"""Convert ``EBOUNDS`` extension to `~astropy.units.Quantity`\n    """"""\n    table = Table.read(ebounds)\n    emin = table[""E_MIN""].quantity\n    emax = table[""E_MAX""].quantity\n    energy = np.append(emin.value, emax.value[-1]) * emin.unit\n    return energy\n\n\n# TODO: add unit test\ndef earth_location_from_dict(meta):\n    """"""Create `~astropy.coordinates.EarthLocation` from FITS header dict.""""""\n    lon = Angle(meta[""GEOLON""], ""deg"")\n    lat = Angle(meta[""GEOLAT""], ""deg"")\n    # TODO: should we support both here?\n    # Check latest spec if ALTITUDE is used somewhere.\n    if ""GEOALT"" in meta:\n        height = Quantity(meta[""GEOALT""], ""meter"")\n    elif ""ALTITUDE"" in meta:\n        height = Quantity(meta[""ALTITUDE""], ""meter"")\n    else:\n        raise KeyError(""The GEOALT or ALTITUDE header keyword must be set"")\n\n    return EarthLocation(lon=lon, lat=lat, height=height)\n'"
gammapy/utils/gauss.py,30,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Multi-Gaussian distribution utilities (Gammapy internal).""""""\nimport numpy as np\nimport scipy.optimize\n\n\nclass Gauss2DPDF:\n    """"""2D symmetric Gaussian PDF.\n\n    Reference: http://en.wikipedia.org/wiki/Multivariate_normal_distribution#Bivariate_case\n\n    Parameters\n    ----------\n    sigma : float\n        Gaussian width.\n    """"""\n\n    def __init__(self, sigma=1):\n        self.sigma = np.asarray(sigma, np.float64)\n\n    @property\n    def _sigma2(self):\n        """"""Sigma squared (float)""""""\n        return self.sigma * self.sigma\n\n    @property\n    def amplitude(self):\n        """"""PDF amplitude at the center (float)""""""\n        return self.__call(0, 0)\n\n    def __call__(self, x, y=0):\n        """"""dp / (dx dy) at position (x, y)\n\n        Parameters\n        ----------\n        x : `~numpy.ndarray`\n            x coordinate\n        y : `~numpy.ndarray`, optional\n            y coordinate\n\n        Returns\n        -------\n        dpdxdy : `~numpy.ndarray`\n            dp / (dx dy)\n        """"""\n        x = np.asarray(x, dtype=np.float64)\n        y = np.asarray(y, dtype=np.float64)\n\n        theta2 = x * x + y * y\n        amplitude = 1 / (2 * np.pi * self._sigma2)\n        exponent = -0.5 * theta2 / self._sigma2\n        return amplitude * np.exp(exponent)\n\n    def dpdtheta2(self, theta2):\n        """"""dp / dtheta2 at position theta2 = theta ^ 2\n\n        Parameters\n        ----------\n        theta2 : `~numpy.ndarray`\n            Offset squared\n\n        Returns\n        -------\n        dpdtheta2 : `~numpy.ndarray`\n            dp / dtheta2\n        """"""\n        theta2 = np.asarray(theta2, dtype=np.float64)\n\n        amplitude = 1 / (2 * self._sigma2)\n        exponent = -0.5 * theta2 / self._sigma2\n        return amplitude * np.exp(exponent)\n\n    def containment_fraction(self, theta):\n        """"""Containment fraction.\n\n        Parameters\n        ----------\n        theta : `~numpy.ndarray`\n            Offset\n\n        Returns\n        -------\n        containment_fraction : `~numpy.ndarray`\n            Containment fraction\n        """"""\n        theta = np.asarray(theta, dtype=np.float64)\n\n        return 1 - np.exp(-0.5 * theta ** 2 / self._sigma2)\n\n    def containment_radius(self, containment_fraction):\n        """"""Containment angle for a given containment fraction.\n\n        Parameters\n        ----------\n        containment_fraction : `~numpy.ndarray`\n            Containment fraction\n\n        Returns\n        -------\n        containment_radius : `~numpy.ndarray`\n            Containment radius\n        """"""\n        containment_fraction = np.asarray(containment_fraction, dtype=np.float64)\n\n        return self.sigma * np.sqrt(-2 * np.log(1 - containment_fraction))\n\n    def gauss_convolve(self, sigma):\n        """"""Convolve with another Gaussian 2D PDF.\n\n        Parameters\n        ----------\n        sigma : `~numpy.ndarray` or float\n            Gaussian width of the new Gaussian 2D PDF to covolve with.\n\n        Returns\n        -------\n        gauss_convolve : `~gammapy.modeling.models.Gauss2DPDF`\n            Convolution of both Gaussians.\n        """"""\n        sigma = np.asarray(sigma, dtype=np.float64)\n\n        new_sigma = np.sqrt(self._sigma2 + sigma ** 2)\n        return Gauss2DPDF(new_sigma)\n\n\nclass MultiGauss2D:\n    """"""Sum of multiple 2D Gaussians.\n\n    Parameters\n    ----------\n    sigmas : `~numpy.ndarray`\n            widths of the Gaussians to add\n    norms : `~numpy.ndarray`, optional\n            normalizations of the Gaussians to add\n\n    Notes\n    -----\n    * This sum is no longer a PDF, it is not normalized to 1.\n    * The ""norm"" of each component represents the 2D integral,\n      not the amplitude at the origin.\n    """"""\n\n    def __init__(self, sigmas, norms=None):\n        # If no norms are given, you have a PDF.\n        sigmas = np.asarray(sigmas, dtype=np.float64)\n        self.components = [Gauss2DPDF(sigma) for sigma in sigmas]\n\n        if norms is None:\n            self.norms = np.ones(len(self.components))\n        else:\n            self.norms = np.asarray(norms, dtype=np.float64)\n\n    def __call__(self, x, y=0):\n        """"""dp / (dx dy) at position (x, y)\n\n        Parameters\n        ----------\n        x : `~numpy.ndarray`\n            x coordinate\n        y : `~numpy.ndarray`, optional\n            y coordinate\n\n        Returns\n        -------\n        total : `~numpy.ndarray`\n            dp / (dx dy)\n        """"""\n        x = np.asarray(x, dtype=np.float64)\n        y = np.asarray(y, dtype=np.float64)\n\n        total = np.zeros_like(x)\n        for norm, component in zip(self.norms, self.components):\n            total += norm * component(x, y)\n        return total\n\n    @property\n    def n_components(self):\n        """"""Number of components (int)""""""\n        return len(self.components)\n\n    @property\n    def sigmas(self):\n        """"""Array of Gaussian widths (`~numpy.ndarray`)""""""\n        return np.array([_.sigma for _ in self.components])\n\n    @property\n    def integral(self):\n        """"""Integral as sum of norms (`~numpy.ndarray`)""""""\n        return np.nansum(self.norms)\n\n    @property\n    def amplitude(self):\n        """"""Amplitude at the center (float)""""""\n        return self.__call__(0, 0)\n\n    @property\n    def max_sigma(self):\n        """"""Largest Gaussian width (float)""""""\n        return self.sigmas.max()\n\n    @property\n    def eff_sigma(self):\n        r""""""Effective Gaussian width for single-Gauss approximation (float)\n\n        Notes\n        -----\n        The effective Gaussian width is given by:\n\n        .. math:: \\sigma_\\mathrm{eff} = \\sqrt{\\sum_i N_i \\sigma_i^2}\n\n        where ``N`` is normalization and ``sigma`` is width.\n\n        """"""\n        sigma2s = np.array([component._sigma2 for component in self.components])\n        return np.sqrt(np.sum(self.norms * sigma2s))\n\n    def dpdtheta2(self, theta2):\n        """"""dp / dtheta2 at position theta2 = theta ^ 2\n\n        Parameters\n        ----------\n        theta2 : `~numpy.ndarray`\n            Offset squared\n\n        Returns\n        -------\n        dpdtheta2 : `~numpy.ndarray`\n            dp / dtheta2\n        """"""\n        # Actually this is only a PDF if sum(norms) == 1\n        theta2 = np.asarray(theta2, dtype=np.float64)\n\n        total = np.zeros_like(theta2)\n        for norm, component in zip(self.norms, self.components):\n            total += norm * component.dpdtheta2(theta2)\n        return total\n\n    def normalize(self):\n        """"""Normalize function.\n\n        Returns\n        -------\n        norm_multigauss : `~gammapy.modeling.models.MultiGauss2D`\n           normalized function\n        """"""\n        sum = self.integral\n        if sum != 0:\n            self.norms /= sum\n        return self\n\n    def containment_fraction(self, theta):\n        """"""Containment fraction.\n\n        Parameters\n        ----------\n        theta : `~numpy.ndarray`\n            Offset\n\n        Returns\n        -------\n        containment_fraction : `~numpy.ndarray`\n            Containment fraction\n        """"""\n        theta = np.asarray(theta, dtype=np.float64)\n\n        total = np.zeros_like(theta)\n        for norm, component in zip(self.norms, self.components):\n            total += norm * component.containment_fraction(theta)\n\n        return total\n\n    def containment_radius(self, containment_fraction):\n        """"""Containment angle for a given containment fraction.\n\n        Parameters\n        ----------\n        containment_fraction : `~numpy.ndarray`\n            Containment fraction\n\n        Returns\n        -------\n        containment_radius : `~numpy.ndarray`\n            Containment radius\n        """"""\n        # I had big problems with fsolve running into negative thetas.\n        # So instead I\'ll find a theta_max myself so that theta\n        # is in the interval [0, theta_max] and then use good ol brentq\n        if not containment_fraction < self.integral:\n            raise ValueError(\n                ""containment_fraction = {} not possible for integral = {}""\n                """".format(containment_fraction, self.integral)\n            )\n\n        def f(theta):\n            # positive if theta too large\n            return self.containment_fraction(theta) - containment_fraction\n\n        # TODO: if it is an array we have to loop by hand!\n        # containment = np.asarray(containment, dtype=np.float64)\n        # Inital guess for theta\n        theta_max = self.eff_sigma\n        # Expand until we really find a theta_max\n        while f(theta_max) < 0:\n            theta_max *= 2\n        return scipy.optimize.brentq(f, a=0, b=theta_max)\n\n    def match_sigma(self, containment_fraction):\n        """"""Compute equivalent Gauss width.\n\n        Find the sigma of a single-Gaussian distribution that\n        approximates this one, such that theta matches for a given\n        containment.\n\n        Parameters\n        ----------\n        containment_fraction : `~numpy.ndarray`\n            Containment fraction\n\n        Returns\n        -------\n        sigma : `~numpy.ndarray`\n            Equivalent containment radius\n        """"""\n        theta1 = self.containment_radius(containment_fraction)\n        theta2 = Gauss2DPDF(sigma=1).containment_radius(containment_fraction)\n        return theta1 / theta2\n\n    def gauss_convolve(self, sigma, norm=1):\n        """"""Convolve with another Gauss.\n\n        Compute new norms and sigmas of all the components such that\n        the new distribution represents the convolved old distribution\n        by a Gaussian of width sigma and then multiplied by norm.\n\n        This MultiGauss2D is unchanged, a new one is created and returned.\n        This is useful if you need to e.g. compute theta for one PSF\n        and many sigmas.\n\n        Parameters\n        ----------\n        sigma : `~numpy.ndarray` or float\n            Gaussian width of the new Gaussian 2D PDF to covolve with.\n        norm : `~numpy.ndarray` or float\n            Normalization of the new Gaussian 2D PDF to covolve with.\n\n        Returns\n        -------\n        new_multi_gauss_2d : `~gammapy.modeling.models.MultiGauss2D`\n            Convolution as new MultiGauss2D\n        """"""\n        sigma = np.asarray(sigma, dtype=np.float64)\n        norm = np.asarray(norm, dtype=np.float64)\n\n        sigmas, norms = [], []\n        for ii in range(self.n_components):\n            sigmas.append(self.components[ii].gauss_convolve(sigma).sigma)\n            norms.append(self.norms[ii] * norm)\n\n        return MultiGauss2D(sigmas, norms)\n'"
gammapy/utils/integrate.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom .interpolation import LogScale\n\n__all__ = [""evaluate_integral_pwl"", ""trapz_loglog""]\n\n\ndef evaluate_integral_pwl(emin, emax, index, amplitude, reference):\n    """"""Evaluate pwl integral (static function).""""""\n    val = -1 * index + 1\n\n    prefactor = amplitude * reference / val\n    upper = np.power((emax / reference), val)\n    lower = np.power((emin / reference), val)\n    integral = prefactor * (upper - lower)\n\n    mask = np.isclose(val, 0)\n\n    if mask.any():\n        integral[mask] = (amplitude * reference * np.log(emax / emin))[mask]\n\n    return integral\n\n\ndef trapz_loglog(y, x, axis=-1):\n    """"""Integrate using the composite trapezoidal rule in log-log space.\n\n    Integrate `y` (`x`) along given axis in loglog space.\n\n    Parameters\n    ----------\n    y : array_like\n        Input array to integrate.\n    x : array_like, optional\n        Independent variable to integrate over.\n    axis : int, optional\n        Specify the axis.\n\n    Returns\n    -------\n    trapz : float\n        Definite integral as approximated by trapezoidal rule in loglog space.\n    """"""\n    # see https://stackoverflow.com/a/56840428\n    x, y = np.moveaxis(x, axis, 0), np.moveaxis(y, axis, 0)\n\n    emin, emax = x[:-1], x[1:]\n    vals_emin, vals_emax = y[:-1], y[1:]\n\n    # log scale has the build-in zero clipping\n    log = LogScale()\n    index = -log(vals_emin / vals_emax) / log(emin / emax)\n    index[np.isnan(index)] = np.inf\n\n    return evaluate_integral_pwl(\n        emin=emin, emax=emax, index=index, reference=emin, amplitude=vals_emin\n    )\n'"
gammapy/utils/interpolation.py,12,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Interpolation utilities""""""\nimport numpy as np\nimport scipy.interpolate\nfrom astropy import units as u\n\n__all__ = [\n    ""ScaledRegularGridInterpolator"",\n    ""interpolation_scale"",\n    ""interpolate_profile"",\n]\n\n\nclass ScaledRegularGridInterpolator:\n    """"""Thin wrapper around `scipy.interpolate.RegularGridInterpolator`.\n\n    The values are scaled before the interpolation and back-scaled after the\n    interpolation.\n\n    Dimensions of length 1 are ignored in the interpolation of the data.\n\n    Parameters\n    ----------\n    points : tuple of `~numpy.ndarray` or `~astropy.units.Quantity`\n        Tuple of points passed to `RegularGridInterpolator`.\n    values : `~numpy.ndarray`\n        Values passed to `RegularGridInterpolator`.\n    points_scale : tuple of str\n        Interpolation scale used for the points.\n    values_scale : {\'lin\', \'log\', \'sqrt\'}\n        Interpolation scaling applied to values. If the values vary over many magnitudes\n        a \'log\' scaling is recommended.\n    axis : int or None\n        Axis along which to interpolate.\n    **kwargs : dict\n        Keyword arguments passed to `RegularGridInterpolator`.\n    """"""\n\n    def __init__(\n        self,\n        points,\n        values,\n        points_scale=None,\n        values_scale=""lin"",\n        extrapolate=True,\n        axis=None,\n        **kwargs,\n    ):\n\n        if points_scale is None:\n            points_scale = [""lin""] * len(points)\n\n        self.scale_points = [interpolation_scale(scale) for scale in points_scale]\n        self.scale = interpolation_scale(values_scale)\n        self._include_dim = [len(p) > 1 for p in points]\n\n        points_scaled = tuple(\n            [\n                scale(p)\n                for p, scale, _ in zip(points, self.scale_points, self._include_dim)\n                if _\n            ]\n        )\n        values_scaled = self.scale(values).squeeze()\n        self.axis = axis\n\n        if extrapolate:\n            kwargs.setdefault(""bounds_error"", False)\n            kwargs.setdefault(""fill_value"", None)\n\n        if axis is None:\n            # print(points_scaled, values_scaled)\n            self._interpolate = scipy.interpolate.RegularGridInterpolator(\n                points=points_scaled, values=values_scaled, **kwargs\n            )\n        else:\n            self._interpolate = scipy.interpolate.interp1d(\n                points_scaled[0], values_scaled, axis=axis\n            )\n\n    def __call__(self, points, method=""linear"", clip=True, **kwargs):\n        """"""Interpolate data points.\n\n        Parameters\n        ----------\n        points : tuple of `~numpy.ndarray` or `~astropy.units.Quantity`\n            Tuple of coordinate arrays of the form (x_1, x_2, x_3, ...). Arrays are\n            broadcasted internally.\n        method : {""linear"", ""nearest""}\n            Linear or nearest neighbour interpolation.\n        clip : bool\n            Clip values at zero after interpolation.\n        """"""\n\n        points = tuple(\n            [\n                scale(p)\n                for scale, p, _ in zip(self.scale_points, points, self._include_dim)\n                if _\n            ]\n        )\n\n        if self.axis is None:\n            points = np.broadcast_arrays(*points)\n            points_interp = np.stack([_.flat for _ in points]).T\n            values = self._interpolate(points_interp, method, **kwargs)\n            values = self.scale.inverse(values.reshape(points[0].shape))\n        else:\n            values = self._interpolate(points[0])\n            values = self.scale.inverse(values)\n\n        if clip:\n            values = np.clip(values, 0, np.inf)\n\n        return values\n\n\ndef interpolation_scale(scale=""lin""):\n    """"""Interpolation scaling.\n\n    Parameters\n    ----------\n    scale : {""lin"", ""log"", ""sqrt""}\n        Choose interpolation scaling.\n    """"""\n    if scale in [""lin"", ""linear""]:\n        return LinearScale()\n    elif scale == ""log"":\n        return LogScale()\n    elif scale == ""sqrt"":\n        return SqrtScale()\n    else:\n        raise ValueError(f""Not a valid value scaling mode: \'{scale}\'."")\n\n\nclass InterpolationScale:\n    """"""Interpolation scale base class.""""""\n\n    def __call__(self, values):\n        if hasattr(self, ""_unit""):\n            values = values.to_value(self._unit)\n        else:\n            if isinstance(values, u.Quantity):\n                self._unit = values.unit\n                values = values.value\n        return self._scale(values)\n\n    def inverse(self, values):\n        values = self._inverse(self, values)\n        if hasattr(self, ""_unit""):\n            return u.Quantity(values, self._unit, copy=False)\n        else:\n            return values\n\n\nclass LogScale(InterpolationScale):\n    """"""Logarithmic scaling""""""\n\n    tiny = np.finfo(np.float32).tiny\n\n    def _scale(self, values):\n        values = np.clip(values, self.tiny, np.inf)\n        return np.log(values)\n\n    @staticmethod\n    def _inverse(self, values):\n        output = np.exp(values)\n        return np.where(abs(output) - self.tiny <= self.tiny, 0, output)\n\n\nclass SqrtScale(InterpolationScale):\n    """"""Sqrt scaling""""""\n\n    @staticmethod\n    def _scale(values):\n        sign = np.sign(values)\n        return sign * np.sqrt(sign * values)\n\n    @staticmethod\n    def _inverse(self, values):\n        return np.power(values, 2)\n\n\nclass LinearScale(InterpolationScale):\n    """"""Linear scaling""""""\n\n    @staticmethod\n    def _scale(values):\n        return values\n\n    @staticmethod\n    def _inverse(self, values):\n        return values\n\n\ndef interpolate_profile(x, y, interp_scale=""sqrt""):\n    """"""Helper function to interpolate one-dimensional profiles.\n\n    Parameters\n    ----------\n    x : `~numpy.ndarray`\n        Array of x values\n    y : `~numpy.ndarray`\n        Array of y values\n    interp_scale : {""sqrt"", ""lin""}\n        Interpolation scale applied to the profile. If the profile is\n        of parabolic shape, a ""sqrt"" scaling is recommended. In other cases or\n        for fine sampled profiles a ""lin"" can also be used.\n\n    Returns\n    -------\n    interp : `ScaledRegularGridInterpolator`\n        Interpolator\n    """"""\n    sign = np.sign(np.gradient(y))\n    return ScaledRegularGridInterpolator(\n        points=(x,), values=sign * y, values_scale=interp_scale\n    )\n'"
gammapy/utils/nddata.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Utility functions and classes for n-dimensional data and axes.""""""\nimport numpy as np\nfrom astropy.units import Quantity\nfrom .array import array_stats_str\nfrom .interpolation import ScaledRegularGridInterpolator\n\n__all__ = [""NDDataArray""]\n\n\nclass NDDataArray:\n    """"""ND Data Array Base class\n\n    Parameters\n    ----------\n    axes : list\n        List of `~gammapy.utils.nddata.DataAxis`\n    data : `~astropy.units.Quantity`\n        Data\n    meta : dict\n        Meta info\n    interp_kwargs : dict\n        TODO\n    """"""\n\n    default_interp_kwargs = dict(bounds_error=False, values_scale=""lin"")\n    """"""Default interpolation kwargs used to initialize the\n    `scipy.interpolate.RegularGridInterpolator`.  The interpolation behaviour\n    of an individual axis (\'log\', \'linear\') can be passed to the axis on\n    initialization.""""""\n\n    def __init__(self, axes, data=None, meta=None, interp_kwargs=None):\n        self._axes = axes\n        if data is not None:\n            self.data = data\n        self.meta = meta or {}\n        self.interp_kwargs = interp_kwargs or self.default_interp_kwargs\n\n        self._regular_grid_interp = None\n\n    def __str__(self):\n        ss = ""NDDataArray summary info\\n""\n        for axis in self.axes:\n            ss += str(axis)\n        ss += array_stats_str(self.data, ""Data"")\n        return ss\n\n    @property\n    def axes(self):\n        """"""Array holding the axes in correct order""""""\n        return self._axes\n\n    def axis(self, name):\n        """"""Return axis by name""""""\n        try:\n            idx = [_.name for _ in self.axes].index(name)\n        except ValueError:\n            raise ValueError(f""Axis {name} not found"")\n        return self.axes[idx]\n\n    @property\n    def data(self):\n        """"""Array holding the n-dimensional data.""""""\n        return self._data\n\n    @data.setter\n    def data(self, data):\n        """"""Set data.\n\n        Some sanity checks are performed to avoid an invalid array.\n        Also, the interpolator is set to None to avoid unwanted behaviour.\n\n        Parameters\n        ----------\n        data : `~astropy.units.Quantity`, array-like\n            Data array\n        """"""\n        data = Quantity(data)\n        dimension = len(data.shape)\n        if dimension != self.dim:\n            raise ValueError(\n                ""Overall dimensions to not match. ""\n                ""Data: {}, Hist: {}"".format(dimension, self.dim)\n            )\n\n        for dim in np.arange(self.dim):\n            axis = self.axes[dim]\n            if axis.nbin != data.shape[dim]:\n                msg = ""Data shape does not match in dimension {d}\\n""\n                msg += ""Axis {n} : {sa}, Data {sd}""\n                raise ValueError(\n                    msg.format(d=dim, n=axis.name, sa=axis.nbin, sd=data.shape[dim])\n                )\n        self._regular_grid_interp = None\n        self._data = data\n\n    @property\n    def dim(self):\n        """"""Dimension (number of axes)""""""\n        return len(self.axes)\n\n    def evaluate(self, method=None, **kwargs):\n        """"""Evaluate NDData Array\n\n        This function provides a uniform interface to several interpolators.\n        The evaluation nodes are given as ``kwargs``.\n\n        Currently available:\n        `~scipy.interpolate.RegularGridInterpolator`, methods: linear, nearest\n\n        Parameters\n        ----------\n        method : str {\'linear\', \'nearest\'}, optional\n            Interpolation method\n        kwargs : dict\n            Keys are the axis names, Values the evaluation points\n\n        Returns\n        -------\n        array : `~astropy.units.Quantity`\n            Interpolated values, axis order is the same as for the NDData array\n        """"""\n        values = []\n        for idx, axis in enumerate(self.axes):\n            # Extract values for each axis, default: nodes\n            shape = [1] * len(self.axes)\n            shape[idx] = -1\n            default = axis.center.reshape(tuple(shape))\n            temp = Quantity(kwargs.pop(axis.name, default))\n            values.append(np.atleast_1d(temp))\n\n        # This is to catch e.g. typos in axis names\n        if kwargs != {}:\n            raise ValueError(f""Input given for unknown axis: {kwargs}"")\n\n        if self._regular_grid_interp is None:\n            self._add_regular_grid_interp()\n\n        return self._regular_grid_interp(values, method=method, **kwargs)\n\n    def _add_regular_grid_interp(self, interp_kwargs=None):\n        """"""Add `~scipy.interpolate.RegularGridInterpolator`\n\n        http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.interpolate.RegularGridInterpolator.html\n\n        Parameters\n        ----------\n        interp_kwargs : dict, optional\n            Interpolation kwargs\n        """"""\n        if interp_kwargs is None:\n            interp_kwargs = self.interp_kwargs\n\n        points = [a.center for a in self.axes]\n        points_scale = [a.interp for a in self.axes]\n        self._regular_grid_interp = ScaledRegularGridInterpolator(\n            points, self.data, points_scale=points_scale, **interp_kwargs\n        )\n'"
gammapy/utils/regions.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Regions helper functions.\n\nThroughout Gammapy, we use `regions` to represent and work with regions.\n\nhttps://astropy-regions.readthedocs.io\n\nThe functions ``make_region`` and ``make_pixel_region`` should be used\nthroughout Gammapy in all functions that take ``region`` objects as input.\nThey do conversion to a standard form, and some validation.\n\nWe might add in other conveniences and features here, e.g. sky coord contains\nwithout a WCS (see ""sky and pixel regions"" in PIG 10), or some HEALPix integration.\n\nTODO: before Gammapy v1.0, discuss what to do about ``gammapy.utils.regions``.\nOptions: keep as-is, hide from the docs, or to remove it completely\n(if the functionality is available in ``astropy-regions`` directly.\n""""""\nimport operator\nfrom regions import (\n    CircleSkyRegion,\n    CompoundSkyRegion,\n    DS9Parser,\n    PixelRegion,\n    Region,\n    SkyRegion,\n)\n\n__all__ = [""make_region"", ""make_pixel_region""]\n\n\ndef make_region(region):\n    """"""Make region object (`regions.Region`).\n\n    See also:\n\n    * `gammapy.utils.regions.make_pixel_region`\n    * https://astropy-regions.readthedocs.io/en/latest/ds9.html\n    * http://ds9.si.edu/doc/ref/region.html\n\n    Parameters\n    ----------\n    region : `regions.Region` or str\n        Region object or DS9 string representation\n\n    Examples\n    --------\n    If a region object in DS9 string format is given, the corresponding\n    region object is created. Note that in the DS9 format ""image""\n    or ""physical"" coordinates start at 1, whereas `regions.PixCoord`\n    starts at 0 (as does Python, Numpy, Astropy, Gammapy, ...).\n\n    >>> from gammapy.utils.regions import make_region\n    >>> make_region(""image;circle(10,20,3)"")\n    <CirclePixelRegion(PixCoord(x=9.0, y=19.0), radius=3.0)>\n    >>> make_region(""galactic;circle(10,20,3)"")\n    <CircleSkyRegion(<SkyCoord (Galactic): (l, b) in deg\n        (10., 20.)>, radius=3.0 deg)>\n\n    If a region object is passed in, it is returned unchanged:\n\n    >>> region = make_region(""image;circle(10,20,3)"")\n    >>> region2 = make_region(region)\n    >>> region is region2\n    True\n    """"""\n    if isinstance(region, str):\n        # This is basic and works for simple regions\n        # It could be extended to cover more things,\n        # like e.g. compound regions, exclusion regions, ....\n        return DS9Parser(region).shapes[0].to_region()\n    elif isinstance(region, Region):\n        return region\n    else:\n        raise TypeError(f""Invalid type: {region!r}"")\n\n\ndef make_pixel_region(region, wcs=None):\n    """"""Make pixel region object (`regions.PixelRegion`).\n\n    See also: `gammapy.utils.regions.make_region`\n\n    Parameters\n    ----------\n    region : `regions.Region` or str\n        Region object or DS9 string representation\n    wcs : `astropy.wcs.WCS`\n        WCS\n\n    Examples\n    --------\n    >>> from gammapy.maps import WcsGeom\n    >>> from gammapy.utils.regions import make_pixel_region\n    >>> wcs = WcsGeom.create().wcs\n    >>> region = make_pixel_region(""galactic;circle(10,20,3)"", wcs)\n    >>> region\n    <CirclePixelRegion(PixCoord(x=570.9301128316974, y=159.935542455567), radius=6.061376992149382)>\n    """"""\n    if isinstance(region, str):\n        region = make_region(region)\n\n    if isinstance(region, SkyRegion):\n        if wcs is None:\n            raise ValueError(""Need wcs to convert to pixel region"")\n        return region.to_pixel(wcs)\n    elif isinstance(region, PixelRegion):\n        return region\n    else:\n        raise TypeError(f""Invalid type: {region!r}"")\n\n\ndef compound_region_to_list(region):\n    """"""Create list of regions from compound regions.\n\n    Parameters\n    ----------\n    regions : `~regions.CompoundSkyRegion` or `~regions.SkyRegion`\n        Compound sky region\n\n    Returns\n    -------\n    regions : list of `~regions.SkyRegion`\n        List of regions.\n    """"""\n    regions = []\n\n    if isinstance(region, CompoundSkyRegion):\n        if region.operator is operator.or_:\n            regions_1 = compound_region_to_list(region.region1)\n            regions.extend(regions_1)\n\n            regions_2 = compound_region_to_list(region.region2)\n            regions.extend(regions_2)\n        else:\n            raise ValueError(""Only union operator supported"")\n    else:\n        return [region]\n\n    return regions\n\n\ndef list_to_compound_region(regions):\n    """"""Create compound region from list of regions, by creating the union.\n\n    Parameters\n    ----------\n    regions : list of `~regions.SkyRegion`\n        List of regions.\n\n    Returns\n    -------\n    compound : `~regions.CompoundSkyRegion`\n        Compound sky region\n    """"""\n\n    region_union = regions[0]\n\n    for region in regions[1:]:\n        region_union = region_union.union(region)\n\n    return region_union\n\n\nclass SphericalCircleSkyRegion(CircleSkyRegion):\n    """"""Spherical circle sky region.\n\n    TODO: is this separate class a good idea?\n\n    - If yes, we could move it to the ``regions`` package?\n    - If no, we should implement some other solution.\n      Probably the alternative is to add extra methods to\n      the ``CircleSkyRegion`` class and have that support\n      both planar approximation and spherical case?\n      Or we go with the approach to always make a\n      TAN WCS and not have true cone select at all?\n    """"""\n\n    def contains(self, skycoord, wcs=None):\n        """"""Defined by spherical distance.""""""\n        separation = self.center.separation(skycoord)\n        return separation < self.radius\n'"
gammapy/utils/registry.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\n__all__ = [""Registry""]\n\n\nclass Registry(list):\n    """"""Registry class.""""""\n\n    def get_cls(self, tag):\n        for cls in self:\n            if hasattr(cls, ""tag"") and cls.tag == tag:\n                return cls\n        raise KeyError(f""No model found with tag: {tag!r}"")\n\n    def __str__(self):\n        info = ""Registry\\n""\n        info += ""--------\\n\\n""\n\n        len_max = max([len(_.tag) for _ in self])\n\n        for item in self:\n            info += f""\\t{item.tag:{len_max}s}: {item.__name__}\\n""\n\n        return info.expandtabs(tabsize=2)\n'"
gammapy/utils/scripts.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Utils to create scripts and command-line tools""""""\nimport codecs\nimport os.path\nfrom base64 import urlsafe_b64encode\nfrom pathlib import Path\nfrom uuid import uuid4\nimport yaml\n\n__all__ = [""read_yaml"", ""write_yaml"", ""make_path"", ""recursive_merge_dicts""]\n\n\ndef read_yaml(filename, logger=None):\n    """"""Read YAML file.\n\n    Parameters\n    ----------\n    filename : `~pathlib.Path`\n        Filename\n    logger : `~logging.Logger`\n        Logger\n\n    Returns\n    -------\n    data : dict\n        YAML file content as a dict\n    """"""\n    path = make_path(filename)\n    if logger is not None:\n        logger.info(f""Reading {path}"")\n\n    text = path.read_text()\n    return yaml.safe_load(text)\n\n\ndef write_yaml(dictionary, filename, logger=None, sort_keys=True):\n    """"""Write YAML file.\n\n    Parameters\n    ----------\n    dictionary : dict\n        Python dictionary\n    filename : `~pathlib.Path`\n        Filename\n    logger : `~logging.Logger`\n        Logger\n    sort_keys : bool\n        Whether to sort keys.\n    """"""\n    text = yaml.safe_dump(dictionary, default_flow_style=False, sort_keys=sort_keys)\n\n    path = make_path(filename)\n    path.parent.mkdir(exist_ok=True)\n    if logger is not None:\n        logger.info(f""Writing {path}"")\n    path.write_text(text)\n\n\ndef make_name(name=None):\n    if name is None:\n        return urlsafe_b64encode(codecs.decode(uuid4().hex, ""hex"")).decode()[:8]\n    else:\n        return name\n\n\ndef make_path(path):\n    """"""Expand environment variables on `~pathlib.Path` construction.\n\n    Parameters\n    ----------\n    path : str, `pathlib.Path`\n        path to expand\n    """"""\n    # TODO: raise error or warning if environment variables that don\'t resolve are used\n    # e.g. ""spam/$DAMN/ham"" where `$DAMN` is not defined\n    # Otherwise this can result in cryptic errors later on\n    return Path(os.path.expandvars(path))\n\n\ndef recursive_merge_dicts(a, b):\n    """"""Recursively merge two dictionaries.\n\n    Entries in b override entries in a. The built-in update function cannot be\n    used for hierarchical dicts, see:\n    http://stackoverflow.com/questions/3232943/update-value-of-a-nested-dictionary-of-varying-depth/3233356#3233356\n\n    Parameters\n    ----------\n    a : dict\n        dictionary to be merged\n    b : dict\n        dictionary to be merged\n\n    Returns\n    -------\n    c : dict\n        merged dict\n\n    Examples\n    --------\n    >>> from gammapy.utils.scripts import recursive_merge_dicts\n    >>> a = dict(a=42, b=dict(c=43, e=44))\n    >>> b = dict(d=99, b=dict(c=50, g=98))\n    >>> c = recursive_merge_dicts(a, b)\n    >>> print(c)\n    {\'a\': 42, \'b\': {\'c\': 50, \'e\': 44, \'g\': 98}, \'d\': 99}\n    """"""\n    c = a.copy()\n    for k, v in b.items():\n        if k in c and isinstance(c[k], dict):\n            c[k] = recursive_merge_dicts(c[k], v)\n        else:\n            c[k] = v\n    return c\n'"
gammapy/utils/scripts_test.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Test if Jupyter notebooks work.""""""\nimport logging\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nimport pkg_resources\nimport yaml\n\nlog = logging.getLogger(__name__)\n\n\ndef get_scripts():\n    """"""Read `scripts.yaml` info.""""""\n    path = Path(""examples"") / ""scripts.yaml""\n    with path.open() as fh:\n        return yaml.safe_load(fh)\n\n\ndef requirement_missing(script):\n    """"""Check if one of the requirements is missing.""""""\n    if ""requires"" in script:\n        if script[""requires""] is None:\n            return False\n        for package in script[""requires""].split():\n            try:\n                pkg_resources.working_set.require(package)\n            except Exception:\n                return True\n    return False\n\n\ndef script_test(path):\n    """"""Check if example Python script is broken.""""""\n    log.info(f""   ... EXECUTING {path}"")\n\n    cmd = [sys.executable, str(path)]\n    cp = subprocess.run(cmd, stderr=subprocess.PIPE)\n    if cp.returncode:\n        log.info(""   ... FAILED"")\n        log.info(""   ___ TRACEBACK"")\n        log.info(cp.stderr.decode(""utf-8"") + ""\\n\\n"")\n        return False\n    else:\n        log.info(""   ... PASSED"")\n        return True\n\n\ndef main():\n    logging.basicConfig(level=logging.INFO)\n\n    if ""GAMMAPY_DATA"" not in os.environ:\n        log.info(""GAMMAPY_DATA environment variable not set."")\n        log.info(""Running scripts tests requires this environment variable."")\n        log.info(""Exiting now."")\n        sys.exit()\n\n    passed = True\n\n    for script in get_scripts():\n        if requirement_missing(script):\n            log.info(f""Skipping script (missing requirement): {script[\'name\']}"")\n            continue\n\n        filename = script[""name""] + "".py""\n        path = Path(""examples"") / filename\n\n        if not script_test(path):\n            passed = False\n\n    if not passed:\n        sys.exit(""Some tests failed. Existing now."")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
gammapy/utils/table.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Table helper utilities.""""""\nfrom astropy.table import Table\nfrom astropy.units import Quantity\nfrom .units import standardise_unit\n\n__all__ = [\n    ""table_standardise_units_copy"",\n    ""table_standardise_units_inplace"",\n    ""table_row_to_dict"",\n    ""table_from_row_data"",\n]\n\n\ndef table_standardise_units_copy(table):\n    """"""Standardise units for all columns in a table in a copy.\n\n    Calls `~gammapy.utils.units.standardise_unit`.\n\n    Parameters\n    ----------\n    table : `~astropy.table.Table`\n        Input table (won\'t be modified)\n\n    Returns\n    -------\n    table : `~astropy.table.Table`\n        Copy of the input table with standardised column units\n    """"""\n    # Note: we could add an `inplace` option (or variant of this function)\n    # See https://github.com/astropy/astropy/issues/6098\n    table = Table(table)\n    return table_standardise_units_inplace(table)\n\n\ndef table_standardise_units_inplace(table):\n    """"""Standardise units for all columns in a table in place.\n    """"""\n    for column in table.columns.values():\n        if column.unit:\n            column.unit = standardise_unit(column.unit)\n\n    return table\n\n\ndef table_row_to_dict(row, make_quantity=True):\n    """"""Make one source data dict.\n\n    Parameters\n    ----------\n    row : `~astropy.table.Row`\n        Row\n    make_quantity : bool\n        Make quantity values for columns with units\n\n    Returns\n    -------\n    data : `dict`\n        Row data\n    """"""\n    data = {}\n    for name, col in row.columns.items():\n        val = row[name]\n        if make_quantity and col.unit:\n            val = Quantity(val, unit=col.unit)\n        data[name] = val\n    return data\n\n\ndef table_from_row_data(rows, **kwargs):\n    """"""Helper function to create table objects from row data.\n\n    Works with quantities.\n\n    Parameters\n    ----------\n    rows : list\n        List of row data (each row a dict)\n    """"""\n    table = Table(**kwargs)\n    colnames = list(rows[0].keys())\n    for name in colnames:\n        coldata = [_[name] for _ in rows]\n        if isinstance(rows[0][name], Quantity):\n            coldata = Quantity(coldata, unit=rows[0][name].unit)\n        table[name] = coldata\n\n    return table\n'"
gammapy/utils/testing.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Utilities for testing""""""\nimport os\nimport sys\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nfrom astropy.time import Time\n\n__all__ = [\n    ""requires_dependency"",\n    ""requires_data"",\n    ""mpl_plot_check"",\n    ""assert_quantity_allclose"",\n    ""assert_skycoord_allclose"",\n    ""assert_time_allclose"",\n    ""Checker"",\n]\n\n# Cache for `requires_dependency`\n_requires_dependency_cache = dict()\n\n\ndef requires_dependency(name):\n    """"""Decorator to declare required dependencies for tests.\n\n    Examples\n    --------\n    ::\n\n        from gammapy.utils.testing import requires_dependency\n\n        @requires_dependency(\'scipy\')\n        def test_using_scipy():\n            import scipy\n            ...\n    """"""\n    import pytest\n\n    if name in _requires_dependency_cache:\n        skip_it = _requires_dependency_cache[name]\n    else:\n        try:\n            __import__(name)\n            skip_it = False\n        except ImportError:\n            skip_it = True\n\n        _requires_dependency_cache[name] = skip_it\n\n    reason = f""Missing dependency: {name}""\n    return pytest.mark.skipif(skip_it, reason=reason)\n\n\ndef has_data(name):\n    """"""Is a certain set of data available?""""""\n    if name == ""gammapy-extra"":\n        return ""GAMMAPY_EXTRA"" in os.environ\n    elif name == ""gammapy-data"":\n        return ""GAMMAPY_DATA"" in os.environ\n    elif name == ""gamma-cat"":\n        return ""GAMMA_CAT"" in os.environ\n    elif name == ""fermi-lat"":\n        return ""GAMMAPY_FERMI_LAT_DATA"" in os.environ\n    else:\n        raise ValueError(f""Invalid name: {name}"")\n\n\ndef requires_data(name=""gammapy-data""):\n    """"""Decorator to declare required data for tests.\n\n    Examples\n    --------\n    ::\n\n        from gammapy.utils.testing import requires_data\n\n        @requires_data()\n        def test_using_data_files():\n            filename = ""$GAMMAPY_DATA/...""\n            ...\n    """"""\n    import pytest\n\n    if not isinstance(name, str):\n        raise TypeError(\n            ""You must call @requires_data with a name (str). ""\n            ""Usually this:  @requires_data()""\n        )\n\n    skip_it = not has_data(name)\n\n    reason = f""Missing data: {name}""\n    return pytest.mark.skipif(skip_it, reason=reason)\n\n\ndef run_cli(cli, args, exit_code=0):\n    """"""Run Click command line tool.\n\n    Thin wrapper around `click.testing.CliRunner`\n    that prints info to stderr if the command fails.\n\n    Parameters\n    ----------\n    cli : click.Command\n        Click command\n    args : list of str\n        Argument list\n    exit_code : int\n        Expected exit code of the command\n\n    Returns\n    -------\n    result : `click.testing.Result`\n        Result\n    """"""\n    from click.testing import CliRunner\n\n    result = CliRunner().invoke(cli, args, catch_exceptions=False)\n\n    if result.exit_code != exit_code:\n        sys.stderr.write(""Exit code mismatch!\\n"")\n        sys.stderr.write(""Ouput:\\n"")\n        sys.stderr.write(result.output)\n\n    return result\n\n\ndef assert_skycoord_allclose(actual, desired):\n    """"""Assert all-close for `astropy.coordinates.SkyCoord` objects.\n\n    - Frames can be different, aren\'t checked at the moment.\n    """"""\n    assert isinstance(actual, SkyCoord)\n    assert isinstance(desired, SkyCoord)\n    assert_allclose(actual.data.lon.deg, desired.data.lon.deg)\n    assert_allclose(actual.data.lat.deg, desired.data.lat.deg)\n\n\ndef assert_time_allclose(actual, desired, atol=1e-3):\n    """"""Assert all-close for `astropy.time.Time` objects.\n\n    atol is absolute tolerance in seconds.\n    """"""\n    assert isinstance(actual, Time)\n    assert isinstance(desired, Time)\n    assert actual.scale == desired.scale\n    assert actual.format == desired.format\n    dt = actual - desired\n    assert_allclose(dt.sec, 0, rtol=0, atol=atol)\n\n\ndef assert_quantity_allclose(actual, desired, rtol=1.0e-7, atol=None, **kwargs):\n    """"""Assert all-close for `astropy.units.Quantity` objects.\n\n    Requires that ``unit`` is identical, not just that quantities\n    are allclose taking different units into account.\n\n    We prefer this kind of assert for testing, since units\n    should only change on purpose, so this tests more behaviour.\n    """"""\n    # TODO: change this later to explicitly check units are the same!\n    # assert actual.unit == desired.unit\n    args = _unquantify_allclose_arguments(actual, desired, rtol, atol)\n    assert_allclose(*args, **kwargs)\n\n\ndef _unquantify_allclose_arguments(actual, desired, rtol, atol):\n    actual = u.Quantity(actual, subok=True, copy=False)\n\n    desired = u.Quantity(desired, subok=True, copy=False)\n    try:\n        desired = desired.to(actual.unit)\n    except u.UnitsError:\n        raise u.UnitsError(\n            ""Units for \'desired\' ({}) and \'actual\' ({}) ""\n            ""are not convertible"".format(desired.unit, actual.unit)\n        )\n\n    if atol is None:\n        # by default, we assume an absolute tolerance of 0\n        atol = u.Quantity(0)\n    else:\n        atol = u.Quantity(atol, subok=True, copy=False)\n        try:\n            atol = atol.to(actual.unit)\n        except u.UnitsError:\n            raise u.UnitsError(\n                ""Units for \'atol\' ({}) and \'actual\' ({}) ""\n                ""are not convertible"".format(atol.unit, actual.unit)\n            )\n\n    rtol = u.Quantity(rtol, subok=True, copy=False)\n    try:\n        rtol = rtol.to(u.dimensionless_unscaled)\n    except Exception:\n        raise u.UnitsError(""`rtol` should be dimensionless"")\n\n    return actual.value, desired.value, rtol.value, atol.value\n\n\ndef mpl_plot_check():\n    """"""Matplotlib plotting test context manager.\n\n    It create a new figure on __enter__ and calls savefig for the\n    current figure in __exit__. This will trigger a render of the\n    Figure, which can sometimes raise errors if there is a problem.\n\n    This is writing to an in-memory byte buffer, i.e. is faster\n    than writing to disk.\n    """"""\n    import matplotlib.pyplot as plt\n    from io import BytesIO\n\n    class MPLPlotCheck:\n        def __enter__(self):\n            plt.figure()\n\n        def __exit__(self, type, value, traceback):\n            plt.savefig(BytesIO(), format=""png"")\n            plt.close()\n\n    return MPLPlotCheck()\n\n\nclass Checker:\n    """"""Base class for checker classes in Gammapy.""""""\n\n    def run(self, checks=""all""):\n        if checks == ""all"":\n            checks = self.CHECKS.keys()\n\n        unknown_checks = sorted(set(checks).difference(self.CHECKS.keys()))\n        if unknown_checks:\n            raise ValueError(f""Unknown checks: {unknown_checks!r}"")\n\n        for check in checks:\n            method = getattr(self, self.CHECKS[check])\n            yield from method()\n'"
gammapy/utils/time.py,1,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Time related utility functions.""""""\nimport numpy as np\nfrom astropy.time import Time, TimeDelta\n\n__all__ = [\n    ""time_ref_from_dict"",\n    ""time_ref_to_dict"",\n    ""time_relative_to_ref"",\n    ""absolute_time"",\n]\n\n# TODO: implement and document this properly.\n# see https://github.com/gammapy/gammapy/issues/284\nTIME_REF_FERMI = Time(""2001-01-01T00:00:00"")\n\n\ndef time_ref_from_dict(meta, format=""mjd"", scale=""tt""):\n    """"""Calculate the time reference from metadata.\n\n    Parameters\n    ----------\n    meta : dict\n        FITS time standard header info\n\n    Returns\n    -------\n    time : `~astropy.time.Time`\n        Time object with ``format=\'MJD\'``\n    """"""\n    # Note: the float call here is to make sure we use 64-bit\n    mjd = float(meta[""MJDREFI""]) + float(meta[""MJDREFF""])\n    scale = meta.get(""TIMESYS"", scale).lower()\n    return Time(mjd, format=format, scale=scale)\n\n\ndef time_ref_to_dict(time, scale=""tt""):\n    """"""TODO: document and test.\n\n    Parameters\n    ----------\n    time : `~astropy.time.Time`\n        Time object with ``format=\'MJD\'``\n\n    Returns\n    -------\n    meta : dict\n        FITS time standard header info\n    """"""\n    mjd = Time(time, scale=scale).mjd\n    i = np.floor(mjd).astype(np.int64)\n    f = mjd - i\n    return dict(MJDREFI=i, MJDREFF=f, TIMESYS=scale)\n\n\ndef time_relative_to_ref(time, meta):\n    """"""Convert a time using an existing reference.\n\n    The time reference is built as MJDREFI + MJDREFF in units of MJD.\n    The time will be converted to seconds after the reference.\n\n    Parameters\n    ----------\n    time : `~astropy.time.Time`\n        time to be converted\n    meta : dict\n        dictionary with the keywords ``MJDREFI`` and ``MJDREFF``\n\n    Returns\n    -------\n    time_delta : `~astropy.time.TimeDelta`\n        time in seconds after the reference\n    """"""\n    time_ref = time_ref_from_dict(meta)\n    return TimeDelta(time - time_ref, format=""sec"")\n\n\ndef absolute_time(time_delta, meta):\n    """"""Convert a MET into human readable date and time.\n\n    Parameters\n    ----------\n    time_delta : `~astropy.time.TimeDelta`\n        time in seconds after the MET reference\n    meta : dict\n        dictionary with the keywords ``MJDREFI`` and ``MJDREFF``\n\n    Returns\n    -------\n    time : `~astropy.time.Time`\n        absolute time with ``format=\'ISOT\'`` and ``scale=\'UTC\'``\n    """"""\n    time = time_ref_from_dict(meta) + time_delta\n    return Time(time.utc.isot)\n'"
gammapy/utils/tutorials_links.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Process tutorials notebooks for publication in documentation.""""""\nimport logging\nimport re\nfrom configparser import ConfigParser\nfrom pathlib import Path\nfrom gammapy import __version__\n\nlog = logging.getLogger(__name__)\n\nPATH_NBS = Path(""docs/_static/notebooks"")\nPATH_DOC = Path(""docs/_build/html/"")\nPATH_HTM = PATH_DOC / ""notebooks""\nPATH_CFG = Path(__file__).resolve().parent / "".."" / ""..""\n\n# fetch url_docs from setup.cfg\nconf = ConfigParser()\nconf.read(PATH_CFG / ""setup.cfg"")\nsetup_cfg = dict(conf.items(""metadata""))\nurl_docs = setup_cfg[""url_docs""]\n\n# release number in absolute links\nrelease_number_docs = __version__\nif ""dev"" in __version__:\n    release_number_docs = ""dev""\n\n\ndef make_api_links(file_path, file_type):\n    """"""Build links to automodapi documentation.""""""\n\n    start_link = ""../""\n    re_api = re.compile(r\'<span class=""pre"">~gammapy\\.(.*?)</span>\')\n    if file_type == ""ipynb"":\n        start_link = url_docs\n        re_api = re.compile(r""`~gammapy\\.(.*?)`"")\n    txt = file_path.read_text(encoding=""utf-8"")\n\n    for module in re_api.findall(txt):\n\n        # end urls\n        alt_links = []\n        submodules = module.split(""."")\n        if len(submodules) == 1:\n            target = submodules[0]\n            alt_links.append(f""{target}/index.html"")\n        elif len(submodules) == 2:\n            target = f""{submodules[0]}.{submodules[1]}""\n            alt_links.append(f""api/gammapy.{target}.html#gammapy.{target}"")\n            alt_links.append(f""{submodules[0]}/index.html#module-gammapy.{target}"")\n            alt_links.append(\n                f""{submodules[0]}/{submodules[1]}index.html#module-gammapy.{target}""\n            )\n        elif len(submodules) == 3:\n            target = f""{submodules[0]}.{submodules[1]}""\n            alt_links.append(\n                f""api/gammapy.{target}.html#gammapy.{target}.{submodules[2]}""\n            )\n            alt_links.append(\n                f""api/gammapy.{target}.{submodules[2]}.html#gammapy.{target}.{submodules[2]}""\n            )\n        elif len(submodules) == 4:\n            target = f""{submodules[0]}.{submodules[1]}.{submodules[2]}""\n            alt_links.append(\n                f""api/gammapy.{target}.html#gammapy.{target}.{submodules[3]}""\n            )\n        else:\n            continue\n\n        # broken link\n        broken = True\n        for link in alt_links:\n            search_file = re.sub(r""(#.*)$"", """", link)\n            search_path = PATH_DOC / search_file\n            if search_path.is_file():\n                link_api = f""{start_link}{link}""\n                link_api = link_api.replace(""()"", """")\n                broken = False\n                break\n        if broken:\n            if file_type == ""ipynb"":\n                log.warning(f""{str(search_path)} does not exist in {file_path}."")\n            continue\n\n        # replace syntax with link\n        str_api = f\'<span class=""pre"">~gammapy.{module}</span>\'\n        label_api = str_api.replace(\'<span class=""pre"">\', """")\n        label_api = label_api.replace(""</span>"", """")\n        label_api = label_api.replace(""~"", """")\n        replace_api = f""<a href=\'{link_api}\'>{label_api}</a>""\n        if file_type == ""ipynb"":\n            str_api = f""`~gammapy.{module}`""\n            label_api = str_api.replace(""`"", """")\n            label_api = label_api.replace(""~"", """")\n            replace_api = f""[[{label_api}]({link_api})]""\n        txt = txt.replace(str_api, replace_api)\n\n    # modif absolute links to rst/html doc files\n    if file_type == ""ipynb"":\n        url_docs_release = url_docs.replace(""dev"", release_number_docs)\n        txt = txt.replace(url_docs, url_docs_release)\n    else:\n        repl = r""..\\/\\1html\\2""\n        txt = re.sub(\n            pattern=url_docs + r""(.*?)html(\\)|#)"",\n            repl=repl,\n            string=txt,\n            flags=re.M | re.I,\n        )\n\n    file_path.write_text(txt, encoding=""utf-8"")\n\n\ndef main():\n    logging.basicConfig(level=logging.INFO)\n    log.info(""Building API links in notebooks."")\n    for nb_path in list(PATH_NBS.glob(""*.ipynb"")):\n        make_api_links(nb_path, file_type=""ipynb"")\n    for html_path in list(PATH_HTM.glob(""*.html"")):\n        make_api_links(html_path, file_type=""html"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
gammapy/utils/tutorials_process.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Process tutorials notebooks for publication in documentation.""""""\nimport argparse\nimport logging\nimport os\nimport shutil\nimport subprocess\nimport sys\nfrom distutils.util import strtobool\nfrom pathlib import Path\nfrom gammapy.scripts.jupyter import notebook_test\n\nlog = logging.getLogger(__name__)\n\n\ndef ignorefiles(d, files):\n    return [\n        f\n        for f in files\n        if os.path.isfile(os.path.join(d, f))\n        and f[-6:] != "".ipynb""\n        and f[-4:] != "".png""\n    ]\n\n\ndef setup_sphinx_params(args):\n    flagnotebooks = ""True""\n    setupfilename = ""setup.cfg""\n    if not args.nbs:\n        flagnotebooks = ""False""\n    build_notebooks_line = f""build_notebooks = {flagnotebooks}\\n""\n\n    file_str = """"\n    with open(setupfilename) as f:\n        for line in f:\n            if line.startswith(""build_notebooks =""):\n                line = build_notebooks_line\n            file_str += line\n\n    with open(setupfilename, ""w"") as f:\n        f.write(file_str)\n\n\ndef build_notebooks(args):\n    if ""GAMMAPY_DATA"" not in os.environ:\n        log.info(""GAMMAPY_DATA environment variable not set."")\n        log.info(""Running notebook tests requires this environment variable."")\n        log.info(""Exiting now."")\n        sys.exit()\n\n    # prepare folder structure\n    pathsrc = Path(args.src)\n    path_temp = Path(""temp"")\n    path_empty_nbs = Path(""tutorials"")\n    path_filled_nbs = Path(""docs"") / ""notebooks""\n    path_static_nbs = Path(""docs"") / ""_static"" / ""notebooks""\n\n    shutil.rmtree(path_temp, ignore_errors=True)\n    path_temp.mkdir(parents=True, exist_ok=True)\n    path_filled_nbs.mkdir(parents=True, exist_ok=True)\n    path_static_nbs.mkdir(parents=True, exist_ok=True)\n\n    if pathsrc == path_empty_nbs:\n        shutil.rmtree(path_temp, ignore_errors=True)\n        shutil.rmtree(path_static_nbs, ignore_errors=True)\n        shutil.rmtree(path_filled_nbs, ignore_errors=True)\n        shutil.copytree(path_empty_nbs, path_temp, ignore=ignorefiles)\n    elif pathsrc.exists():\n        notebookname = pathsrc.name\n        pathdest = path_temp / notebookname\n        shutil.copyfile(pathsrc, pathdest)\n    else:\n        log.info(""Notebook file does not exist."")\n        sys.exit()\n\n    if args.fmt:\n        subprocess.run(\n            [sys.executable, ""-m"", ""gammapy"", ""jupyter"", ""--src"", ""temp"", ""black""]\n        )\n    subprocess.run(\n        [sys.executable, ""-m"", ""gammapy"", ""jupyter"", ""--src"", ""temp"", ""strip""]\n    )\n\n    # test /run\n    for path in path_temp.glob(""*.ipynb""):\n        notebook_test(path)\n\n    # convert into scripts\n    # copy generated filled notebooks to doc\n    if pathsrc == path_empty_nbs:\n        # copytree is needed to copy subfolder images\n        shutil.copytree(path_empty_nbs, path_static_nbs, ignore=ignorefiles)\n        for path in path_static_nbs.glob(""*.ipynb""):\n            subprocess.run(\n                [\n                    sys.executable,\n                    ""-m"",\n                    ""jupyter"",\n                    ""nbconvert"",\n                    ""--to"",\n                    ""script"",\n                    str(path),\n                ]\n            )\n        shutil.copytree(path_temp, path_filled_nbs, ignore=ignorefiles)\n    else:\n        pathsrc = path_temp / notebookname\n        pathdest = path_static_nbs / notebookname\n        shutil.copyfile(pathsrc, pathdest)\n        subprocess.run(\n            [\n                sys.executable,\n                ""-m"",\n                ""jupyter"",\n                ""nbconvert"",\n                ""--to"",\n                ""script"",\n                str(pathdest),\n            ]\n        )\n        pathdest = path_filled_nbs / notebookname\n        shutil.copyfile(pathsrc, pathdest)\n\n    # tear down\n    shutil.rmtree(path_temp, ignore_errors=True)\n\n\ndef main():\n    logging.basicConfig(level=logging.INFO)\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--src"", help=""Tutorial notebook or folder to process"")\n    parser.add_argument(""--nbs"", help=""Notebooks are considered in Sphinx"")\n    parser.add_argument(""--fmt"", help=""Black format notebooks"")\n    args = parser.parse_args()\n\n    if not args.src:\n        args.src = ""tutorials""\n    if not args.nbs:\n        args.nbs = ""True""\n    if not args.fmt:\n        args.fmt = ""True""\n\n    try:\n        args.nbs = strtobool(args.nbs)\n        args.fmt = strtobool(args.fmt)\n    except Exception as ex:\n        log.error(ex)\n        sys.exit()\n\n    setup_sphinx_params(args)\n\n    if args.nbs:\n        build_notebooks(args)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
gammapy/utils/tutorials_test.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Test if Jupyter notebooks work.""""""\nimport logging\nimport os\nimport shutil\nimport sys\nfrom pathlib import Path\nimport pkg_resources\nimport yaml\nfrom gammapy.scripts.jupyter import notebook_test\n\nlog = logging.getLogger(__name__)\n\n\ndef get_notebooks():\n    """"""Read `notebooks.yaml` info.""""""\n    path = Path(""tutorials"") / ""notebooks.yaml""\n    with path.open() as fh:\n        return yaml.safe_load(fh)\n\n\ndef requirement_missing(notebook):\n    """"""Check if one of the requirements is missing.""""""\n    if ""requires"" in notebook:\n        if notebook[""requires""] is None:\n            return False\n        for package in notebook[""requires""].split():\n            try:\n                pkg_resources.working_set.require(package)\n            except Exception:\n                return True\n    return False\n\n\ndef main():\n    logging.basicConfig(level=logging.INFO)\n\n    if ""GAMMAPY_DATA"" not in os.environ:\n        log.info(""GAMMAPY_DATA environment variable not set."")\n        log.info(""Running notebook tests requires this environment variable."")\n        log.info(""Exiting now."")\n        sys.exit()\n\n    passed = True\n\n    # setup\n    path_temp = Path(""temp"")\n    path_empty_nbs = Path(""tutorials"")\n    shutil.rmtree(path_temp, ignore_errors=True)\n    shutil.copytree(path_empty_nbs, path_temp)\n\n    for notebook in get_notebooks():\n        if requirement_missing(notebook):\n            log.info(f""Skipping notebook (requirement missing): {notebook[\'name\']}"")\n            continue\n        filename = notebook[""name""] + "".ipynb""\n        path = path_temp / filename\n\n        if not notebook_test(path):\n            passed = False\n\n    # tear down\n    shutil.rmtree(path_temp, ignore_errors=True)\n\n    if not passed:\n        sys.exit(""Some tests failed. Existing now."")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
gammapy/utils/units.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Units and Quantity related helper functions""""""\nimport logging\nimport astropy.units as u\n\n__all__ = [""standardise_unit"", ""unit_from_fits_image_hdu""]\n\nlog = logging.getLogger(__name__)\n\n\ndef standardise_unit(unit):\n    """"""Standardise unit.\n\n    Changes applied by this function:\n\n    * Drop ""photon"" == ""ph"" from the unit\n    * Drop ""count"" == ""ct"" from the unit\n\n    Parameters\n    ----------\n    unit : `~astropy.units.Unit` or str\n        Any old unit\n\n    Returns\n    -------\n    unit : `~astropy.units.Unit`\n        Shiny new, standardised unit\n\n    Examples\n    --------\n    >>> from gammapy.utils.units import standardise_unit\n    >>> standardise_unit(\'ph cm-2 s-1\')\n    Unit(""1 / (cm2 s)"")\n    >>> standardise_unit(\'ct cm-2 s-1\')\n    Unit(""1 / (cm2 s)"")\n    >>> standardise_unit(\'cm-2 s-1\')\n    Unit(""1 / (cm2 s)"")\n    """"""\n    unit = u.Unit(unit)\n    bases, powers = [], []\n    for base, power in zip(unit.bases, unit.powers):\n        if str(base) not in {""ph"", ""ct""}:\n            bases.append(base)\n            powers.append(power)\n\n    return u.CompositeUnit(scale=unit.scale, bases=bases, powers=powers)\n\n\ndef unit_from_fits_image_hdu(header):\n    """"""Read unit from a FITS image HDU.\n\n    - The ``BUNIT`` key is used.\n    - `astropy.units.Unit` is called.\n      If the ``BUNIT`` value is invalid, a log warning\n      is emitted and the empty unit is used.\n    - `standardise_unit` is called\n    """"""\n    unit = header.get(""BUNIT"", """")\n\n    try:\n        u.Unit(unit)\n    except ValueError:\n        log.warning(f""Invalid value BUNIT={unit!r} in FITS header. Setting empty unit."")\n        unit = """"\n\n    return standardise_unit(unit)\n'"
gammapy/visualization/__init__.py,0,b'from .cmap import *\nfrom .heatmap import *\nfrom .panel import *\nfrom .utils import *\n'
gammapy/visualization/cmap.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Helper functions and functions for plotting gamma-ray images.""""""\n\n__all__ = [""colormap_hess"", ""colormap_milagro""]\n\n\ndef colormap_hess(transition=0.5, width=0.1):\n    """"""Colormap often used in H.E.S.S. collaboration publications.\n\n    This colormap goes black -> blue -> red -> yellow -> white.\n\n    A sharp blue -> red -> yellow transition is often used for significance images\n    with a value of red at ``transition ~ 5`` or ``transition ~ 7``\n    so that the following effect is achieved:\n\n    - black, blue: non-significant features, not well visible\n    - red: features at the detection threshold ``transition``\n    - yellow, white: significant features, very well visible\n\n    The transition parameter is defined between 0 and 1. To calculate the value\n    from data units an `~astropy.visualization.mpl_normalize.ImageNormalize`\n    instance should be used (see example below).\n\n    Parameters\n    ----------\n    transition : float (default = 0.5)\n        Value of the transition to red (between 0 and 1).\n    width : float (default = 0.5)\n        Width of the blue-red color transition (between 0 and 1).\n\n    Returns\n    -------\n    colormap : `matplotlib.colors.LinearSegmentedColormap`\n        Colormap\n\n    Examples\n    --------\n    >>> from gammapy.visualization import colormap_hess\n    >>> from astropy.visualization.mpl_normalize import ImageNormalize\n    >>> from astropy.visualization import LinearStretch\n    >>> normalize = ImageNormalize(vmin=-5, vmax=15, stretch=LinearStretch())\n    >>> transition = normalize(5)\n    >>> cmap = colormap_hess(transition=transition)\n    """"""\n    from matplotlib.colors import LinearSegmentedColormap\n\n    # Compute normalised values (range 0 to 1) that\n    # correspond to red, blue, yellow.\n    red = float(transition)\n\n    if width > red:\n        blue = 0.1 * red\n    else:\n        blue = red - width\n\n    yellow = 2.0 / 3.0 * (1 - red) + red\n\n    black, white = 0, 1\n\n    # Create custom colormap\n    # List entries: (value, (R, G, B))\n    colors = [\n        (black, ""k""),\n        (blue, (0, 0, 0.8)),\n        (red, ""r""),\n        (yellow, (1.0, 1.0, 0)),\n        (white, ""w""),\n    ]\n\n    return LinearSegmentedColormap.from_list(name=""hess"", colors=colors)\n\n\ndef colormap_milagro(transition=0.5, width=0.0001, huestart=0.6):\n    """"""Colormap often used in Milagro collaboration publications.\n\n    This colormap is gray below ``transition`` and similar to the jet colormap above.\n\n    A sharp gray -> color transition is often used for significance images\n    with a transition value of ``transition ~ 5`` or ``transition ~ 7``,\n    so that the following effect is achieved:\n\n    - gray: non-significant features are not well visible\n    - color: significant features at the detection threshold ``transition``\n\n    Note that this colormap is often criticised for over-exaggerating small differences\n    in significance below and above the gray - color transition threshold.\n\n    The transition parameter is defined between 0 and 1. To calculate the value\n    from data units an `~astropy.visualization.mpl_normalize.ImageNormalize` instance should be\n    used (see example below).\n\n    Parameters\n    ----------\n    transition : float (default = 0.5)\n        Transition value (below: gray, above: color).\n    width : float (default = 0.0001)\n        Width of the transition\n    huestart : float (default = 0.6)\n        Hue of the color at ``transition``\n\n    Returns\n    -------\n    colormap : `~matplotlib.colors.LinearSegmentedColormap`\n        Colormap\n\n    Examples\n    --------\n    >>> from gammapy.visualization import colormap_milagro\n    >>> from astropy.visualization.mpl_normalize import ImageNormalize\n    >>> from astropy.visualization import LinearStretch\n    >>> normalize = ImageNormalize(vmin=-5, vmax=15, stretch=LinearStretch())\n    >>> transition = normalize(5)\n    >>> cmap = colormap_milagro(transition=transition)\n    """"""\n    from colorsys import hls_to_rgb\n    from matplotlib.colors import LinearSegmentedColormap\n\n    # Compute normalised red, blue, yellow values\n    transition = float(transition)\n\n    # Create custom colormap\n    # List entries: (value, (H, L, S))\n    colors = [\n        (0, (1, 1, 0)),\n        (transition - width, (1, 0, 0)),\n        (transition, (huestart, 0.4, 0.5)),\n        (transition + width, (huestart, 0.4, 1)),\n        (0.99, (0, 0.6, 1)),\n        (1, (0, 1, 1)),\n    ]\n\n    # Convert HLS values to RGB values\n    rgb_colors = [(val, hls_to_rgb(*hls)) for (val, hls) in colors]\n\n    return LinearSegmentedColormap.from_list(name=""milagro"", colors=rgb_colors)\n'"
gammapy/visualization/heatmap.py,5,"b'import numpy as np\n\n# taken from the matploltlib documentation\n# https://matplotlib.org/3.1.0/gallery/images_contours_and_fields/image_annotated_heatmap.html#sphx-glr-gallery-images-contours-and-fields-image-annotated-heatmap-py\n\n__all__ = [""plot_heatmap"", ""annotate_heatmap""]\n\n\ndef plot_heatmap(\n    data, row_labels, col_labels, ax=None, cbar_kw={}, cbarlabel="""", **kwargs\n):\n    """"""\n    Create a heatmap from a numpy array and two lists of labels.\n\n    Parameters\n    ----------\n    data\n        A 2D numpy array of shape (N, M).\n    row_labels\n        A list or array of length N with the labels for the rows.\n    col_labels\n        A list or array of length M with the labels for the columns.\n    ax\n        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n        not provided, use current axes or create a new one.  Optional.\n    cbar_kw\n        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n    cbarlabel\n        The label for the colorbar.  Optional.\n    **kwargs\n        All other arguments are forwarded to `imshow`.\n    """"""\n    import matplotlib.pyplot as plt\n\n    if not ax:\n        ax = plt.gca()\n\n    # Plot the heatmap\n    im = ax.imshow(data, **kwargs)\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=""bottom"")\n\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(data.shape[1]))\n    ax.set_yticks(np.arange(data.shape[0]))\n    # ... and label them with the respective list entries.\n    ax.set_xticklabels(col_labels)\n    ax.set_yticklabels(row_labels)\n\n    # Let the horizontal axes labeling appear on top.\n    ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=-30, ha=""right"", rotation_mode=""anchor"")\n\n    # Turn spines off and create white grid.\n    for edge, spine in ax.spines.items():\n        spine.set_visible(False)\n\n    ax.set_xticks(np.arange(data.shape[1] + 1) - 0.5, minor=True)\n    ax.set_yticks(np.arange(data.shape[0] + 1) - 0.5, minor=True)\n    ax.grid(which=""minor"", color=""w"", linestyle=""-"", linewidth=1.5)\n    ax.tick_params(which=""minor"", bottom=False, left=False)\n\n    return im, cbar\n\n\ndef annotate_heatmap(\n    im,\n    data=None,\n    valfmt=""{x:.2f}"",\n    textcolors=[""black"", ""white""],\n    threshold=None,\n    **textkw\n):\n    """"""\n    A function to annotate a heatmap.\n\n    Parameters\n    ----------\n    im\n        The AxesImage to be labeled.\n    data\n        Data used to annotate.  If None, the image\'s data is used.  Optional.\n    valfmt\n        The format of the annotations inside the heatmap.  This should either\n        use the string format method, e.g. ""$ {x:.2f}"", or be a\n        `matplotlib.ticker.Formatter`.  Optional.\n    textcolors\n        A list or array of two color specifications.  The first is used for\n        values below a threshold, the second for those above.  Optional.\n    threshold\n        Value in data units according to which the colors from textcolors are\n        applied.  If None (the default) uses the middle of the colormap as\n        separation.  Optional.\n    **kwargs\n        All other arguments are forwarded to each call to `text` used to create\n        the text labels.\n    """"""\n    import matplotlib\n\n    if not isinstance(data, (list, np.ndarray)):\n        data = im.get_array()\n\n    # Normalize the threshold to the images color range.\n    if threshold is not None:\n        threshold = im.norm(threshold)\n    else:\n        threshold = im.norm(data.max()) / 2.0\n\n    # Set default alignment to center, but allow it to be\n    # overwritten by textkw.\n    kw = dict(horizontalalignment=""center"", verticalalignment=""center"")\n    kw.update(textkw)\n\n    # Get the formatter in case a string is supplied\n    if isinstance(valfmt, str):\n        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n\n    # Loop over the data and create a `Text` for each ""pixel"".\n    # Change the text\'s color depending on the data.\n    texts = []\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n            texts.append(text)\n\n    return texts\n'"
gammapy/visualization/panel.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Helper functions and functions for plotting gamma-ray images.""""""\nimport numpy as np\nfrom astropy.coordinates import Angle\n\n__all__ = [""MapPanelPlotter""]\n\n__doctest_requires__ = {(""colormap_hess"", ""colormap_milagro""): [""matplotlib""]}\n\n\nclass MapPanelPlotter:\n    """"""\n    Map panel plotter class.\n\n    Given a `~matplotlib.pyplot.Figure` object this class creates axes objects\n    using `~matplotlib.gridspec.GridSpec` and plots a given sky map onto these.\n\n    Parameters\n    ----------\n    figure : `~matplotlib.pyplot.figure.`\n        Figure instance.\n    xlim : `~astropy.coordinates.Angle`\n        Angle object specifying the longitude limits.\n    ylim : `~astropy.coordinates.Angle`\n        Angle object specifying the latitude limits.\n    npanels : int\n        Number of panels.\n    **kwargs : dict\n        Keyword arguments passed to `~matplotlib.gridspec.GridSpec`.\n    """"""\n\n    def __init__(self, figure, xlim, ylim, npanels=4, **kwargs):\n        from matplotlib.gridspec import GridSpec\n\n        self.figure = figure\n        self.parameters = {""xlim"": xlim, ""ylim"": ylim, ""npanels"": npanels}\n        self.grid_spec = GridSpec(nrows=npanels, ncols=1, **kwargs)\n\n    def _get_ax_extend(self, ax, panel):\n        """"""Get width and height of the axis in world coordinates.""""""\n        p = self.parameters\n\n        # compute aspect ratio of the axis\n        aspect = ax.bbox.width / ax.bbox.height\n\n        # compute width and height in world coordinates\n        height = np.abs(p[""ylim""].diff())\n        width = aspect * height\n\n        left, bottom = p[""xlim""][0].wrap_at(""180d""), p[""ylim""][0]\n\n        width_all = np.abs(p[""xlim""].wrap_at(""180d"").diff())\n        xoverlap = ((p[""npanels""] * width) - width_all) / (p[""npanels""] - 1.0)\n        if xoverlap < 0:\n            raise ValueError(\n                ""No overlap between panels. Please reduce figure ""\n                ""height or increase vertical space between the panels.""\n            )\n\n        left = left - panel * (width - xoverlap)\n        return left[0], bottom, width, height\n\n    def _set_ax_fov(self, ax, panel):\n        left, bottom, width, height = self._get_ax_extend(ax, panel)\n\n        # set fov\n        xlim = Angle([left, left - width])\n        ylim = Angle([bottom, bottom + height])\n        xlim_pix, ylim_pix = ax.wcs.wcs_world2pix(xlim.deg, ylim.deg, 1)\n\n        ax.set_xlim(*xlim_pix)\n        ax.set_ylim(*ylim_pix)\n        return ax\n\n    def plot_panel(self, map, panel=1, panel_fov=None, **kwargs):\n        """"""\n        Plot sky map on one panel.\n\n        Parameters\n        ----------\n        map : `~gammapy.maps.WcsNDMap`\n            Map to plot.\n        panel : int\n            Which panel to plot on (counted from top).\n        """"""\n        if panel_fov is None:\n            panel_fov = panel\n        spec = self.grid_spec[panel]\n        ax = self.figure.add_subplot(spec, projection=map.geom.wcs)\n        try:\n            ax = map.plot(ax=ax, **kwargs)[1]\n        except AttributeError:\n            ax = map.plot_rgb(ax=ax, **kwargs)\n        ax = self._set_ax_fov(ax, panel_fov)\n        return ax\n\n    def plot(self, map, **kwargs):\n        """"""\n        Plot sky map on all panels.\n\n        Parameters\n        ----------\n        map : `~gammapy.maps.WcsNDMap`\n            Map to plot.\n        """"""\n        p = self.parameters\n        axes = []\n        for panel in range(p[""npanels""]):\n            ax = self.plot_panel(map, panel=panel, **kwargs)\n            axes.append(ax)\n        return axes\n'"
gammapy/visualization/utils.py,6,"b'import numpy as np\n\n__all__ = [""plot_spectrum_datasets_off_regions"", ""plot_contour_line""]\n\n\ndef plot_spectrum_datasets_off_regions(datasets, ax=None):\n    """"""Plot spectrum datasets of regions.\n\n    Parameters\n    ----------\n    datasets : list of `SpectrumDatasetOnOff`\n        List of spectrum on-off datasets\n    """"""\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as mpatches\n\n    ax = plt.gca(projection=datasets[0].counts_off.geom.wcs) or ax\n\n    color_cycle = plt.rcParams[""axes.prop_cycle""]\n    colors = color_cycle.by_key()[""color""]\n    handles = []\n\n    for color, dataset in zip(colors, datasets):\n        kwargs = {""edgecolor"": color, ""facecolor"": ""none""}\n        dataset.counts_off.plot_region(ax=ax, **kwargs)\n\n        # create proxy artist for the custom legend\n        handle = mpatches.Patch(label=dataset.name, **kwargs)\n        handles.append(handle)\n\n    plt.legend(handles=handles)\n\n\ndef plot_contour_line(ax, x, y, **kwargs):\n    """"""Plot smooth curve from contour points""""""\n    from scipy.interpolate import CubicSpline\n\n    # close countour\n    xf = np.append(x, x[0])\n    yf = np.append(y, y[0])\n\n    # curve parametrization must be strictly increasing\n    # so we use the cumulative distance of each point from the first one\n    dist = np.sqrt(np.diff(xf) ** 2.0 + np.diff(yf) ** 2.0)\n    dist = [0] + list(dist)\n    t = np.cumsum(dist)\n    ts = np.linspace(0, t[-1], 50)\n\n    # 1D cubic spline interpolation\n    cs = CubicSpline(t, np.c_[xf, yf], bc_type=""periodic"")\n    out = cs(ts)\n\n    # plot\n    if ""marker"" in kwargs.keys():\n        marker = kwargs.pop(""marker"")\n    else:\n        marker = ""+""\n    if ""color"" in kwargs.keys():\n        color = kwargs.pop(""color"")\n    else:\n        color = ""b""\n\n    ax.plot(out[:, 0], out[:, 1], ""-"", color=color, **kwargs)\n    ax.plot(xf, yf, linestyle=\'\', marker=marker, color=color)\n'"
docs/astro/population/plot_radial_distributions.py,1,"b'""""""Plot radial surface density distributions of Galactic sources.""""""\nimport numpy as np\nimport astropy.units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.astro.population import radial_distributions\nfrom gammapy.utils.random import normalize\n\nradius = np.linspace(0, 20, 100) * u.kpc\n\nfor key in radial_distributions:\n    model = radial_distributions[key]()\n    if model.evolved:\n        linestyle = ""-""\n    else:\n        linestyle = ""--""\n    label = model.__class__.__name__\n    x = radius.value\n    y = normalize(model, 0, radius[-1].value)(radius.value)\n    plt.plot(x, y, linestyle=linestyle, label=label)\n\nplt.xlim(0, radius[-1].value)\nplt.ylim(0, 0.26)\nplt.xlabel(""Galactocentric Distance [kpc]"")\nplt.ylabel(""Normalized Surface Density [kpc^-2]"")\nplt.legend(prop={""size"": 10})\nplt.show()\n'"
docs/astro/population/plot_spiral_arm_models.py,1,"b'""""""Plot Milky Way spiral arm models.""""""\nimport numpy as np\nfrom astropy.units import Quantity\nimport matplotlib.pyplot as plt\nfrom gammapy.astro.population.spatial import FaucherSpiral, ValleeSpiral\n\nfig = plt.figure(figsize=(7, 8))\nrect = [0.12, 0.12, 0.85, 0.85]\nax_cartesian = fig.add_axes(rect)\nax_cartesian.set_aspect(""equal"")\n\nvallee_spiral = ValleeSpiral()\nfaucher_spiral = FaucherSpiral()\n\nradius = Quantity(np.arange(2.1, 10, 0.1), ""kpc"")\n\nfor spiralarm_index in range(4):\n    # TODO: spiral arm index is different for the two spirals\n    # -> spirals by the same name have different colors.\n    # Should we change this in the implementation or here in plotting?\n    color = ""C{}"".format(spiralarm_index)\n\n    # Plot Vallee spiral\n    x, y = vallee_spiral.xy_position(radius=radius, spiralarm_index=spiralarm_index)\n    name = vallee_spiral.spiralarms[spiralarm_index]\n    ax_cartesian.plot(x, y, label=""Vallee "" + name, ls=""-"", color=color)\n\n    # Plot Faucher spiral\n    x, y = faucher_spiral.xy_position(radius=radius, spiralarm_index=spiralarm_index)\n    name = faucher_spiral.spiralarms[spiralarm_index]\n    ax_cartesian.plot(x, y, label=""Faucher "" + name, ls=""-."", color=color)\n\nax_cartesian.plot(vallee_spiral.bar[""x""], vallee_spiral.bar[""y""])\n\nax_cartesian.set_xlabel(""x (kpc)"")\nax_cartesian.set_ylabel(""y (kpc)"")\nax_cartesian.set_xlim(-12, 12)\nax_cartesian.set_ylim(-15, 12)\nax_cartesian.legend(ncol=2, loc=""lower right"")\n\nplt.grid()\nplt.show()\n'"
docs/astro/population/plot_spiral_arms.py,1,"b'""""""Plot Milky Way spiral arms.""""""\nimport numpy as np\nfrom astropy.units import Quantity\nimport matplotlib.pyplot as plt\nfrom gammapy.astro.population import FaucherSpiral, simulate\nfrom gammapy.utils.coordinates import cartesian, polar\n\ncatalog = simulate.make_base_catalog_galactic(\n    n_sources=int(1e4), rad_dis=""YK04"", vel_dis=""H05"", max_age=Quantity(1e6, ""yr"")\n)\n\nspiral = FaucherSpiral()\n\nfig = plt.figure(figsize=(6, 6))\nrect = [0.12, 0.12, 0.85, 0.85]\nax_cartesian = fig.add_axes(rect)\nax_cartesian.set_aspect(""equal"")\n\nax_polar = fig.add_axes(rect, polar=True, frameon=False)\nax_polar.axes.get_xaxis().set_ticklabels([])\nax_polar.axes.get_yaxis().set_ticklabels([])\n\nax_cartesian.plot(\n    catalog[""x""],\n    catalog[""y""],\n    marker=""."",\n    linestyle=""none"",\n    markersize=5,\n    alpha=0.3,\n    fillstyle=""full"",\n)\nax_cartesian.set_xlim(-20, 20)\nax_cartesian.set_ylim(-20, 20)\nax_cartesian.set_xlabel(""x [kpc]"", labelpad=2)\nax_cartesian.set_ylabel(""y [kpc]"", labelpad=-4)\nax_cartesian.plot(\n    0, 8, color=""k"", markersize=10, fillstyle=""none"", marker=""*"", linewidth=2\n)\nax_cartesian.annotate(\n    ""Sun"",\n    xy=(0, 8),\n    xycoords=""data"",\n    xytext=(-15, 15),\n    arrowprops=dict(arrowstyle=""->"", color=""k""),\n    weight=400,\n)\n\nplt.grid(True)\n\n# TODO: document what these magic numbers are or simplify the code\n# `other_idx = [95, 90, 80, 80]` and below `theta_idx = int(other_idx * 0.97)`\nfor spiral_idx, other_idx in zip(range(4), [95, 90, 80, 80]):\n    spiralarm_name = spiral.spiralarms[spiral_idx]\n    theta_0 = spiral.theta_0[spiral_idx].value\n\n    theta = Quantity(np.linspace(theta_0, theta_0 + 2 * np.pi, 100), ""rad"")\n    x, y = spiral.xy_position(theta=theta, spiralarm_index=spiral_idx)\n    ax_cartesian.plot(x.value, y.value, color=""k"")\n    rad, phi = polar(x[other_idx], y[other_idx])\n    x_pos, y_pos = cartesian(rad + Quantity(1, ""kpc""), phi)\n    theta_idx = int(other_idx * 0.97)\n    rotation = theta[theta_idx].to(""deg"").value\n    ax_cartesian.text(\n        x_pos.value,\n        y_pos.value,\n        spiralarm_name,\n        ha=""center"",\n        va=""center"",\n        rotation=rotation - 90,\n        weight=400,\n    )\nplt.show()\n'"
docs/astro/population/plot_velocity_distributions.py,1,"b'""""""Plot velocity distributions of Galactic sources.""""""\nimport numpy as np\nimport astropy.units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.astro.population import velocity_distributions\nfrom gammapy.utils.random import normalize\n\nvelocity = np.linspace(10, 3000, 200) * u.km / u.s\n\nfor key in velocity_distributions:\n    model = velocity_distributions[key]()\n    label = model.__class__.__name__\n    x = velocity.value\n    y = normalize(model, velocity[0].value, velocity[-1].value)(velocity.value)\n    plt.plot(x, y, linestyle=""-"", label=label)\n\nplt.xlim(velocity[0].value, velocity[-1].value)\nplt.ylim(0, 0.005)\nplt.xlabel(""Velocity [km/s]"")\nplt.ylabel(""Probability Density [(km / s)^-1]"")\nplt.semilogx()\nplt.legend(prop={""size"": 10})\nplt.show()\n'"
docs/astro/source/plot_pulsar_spindown.py,1,"b'""""""Plot spin frequency of the pulsar with time.""""""\nimport numpy as np\nfrom astropy.units import Quantity\nimport matplotlib.pyplot as plt\nfrom gammapy.astro.source import Pulsar\n\nt = Quantity(np.logspace(0, 6, 100), ""yr"")\n\npulsar = Pulsar(P_0=Quantity(0.01, ""s""), B=""1e12 G"")\n\nplt.plot(t.value, 1 / pulsar.period(t).cgs.value)\nplt.xlabel(""time [years]"")\nplt.ylabel(""frequency [1/s]"")\nplt.ylim(1e0, 1e2)\nplt.loglog()\nplt.show()\n'"
docs/astro/source/plot_pwn_evolution.py,1,"b'""""""Plot PWN evolution with time.""""""\nimport numpy as np\nfrom astropy.constants import M_sun\nfrom astropy.units import Quantity\nimport matplotlib.pyplot as plt\nfrom gammapy.astro.source import PWN, SNRTrueloveMcKee\n\nt = Quantity(np.logspace(1, 5, 100), ""yr"")\nn_ISM = Quantity(1, ""cm^-3"")\nsnr = SNRTrueloveMcKee(m_ejecta=8 * M_sun, n_ISM=n_ISM)\npwn = PWN(snr=snr)\npwn.pulsar.L_0 = Quantity(1e40, ""erg/s"")\n\nplt.plot(t.value, pwn.radius(t).to(""pc"").value, label=""Radius PWN"")\nplt.plot(t.value, snr.radius_reverse_shock(t).to(""pc"").value, label=""Reverse Shock SNR"")\nplt.plot(t.value, snr.radius(t).to(""pc"").value, label=""Radius SNR"")\n\nplt.xlabel(""time [years]"")\nplt.ylabel(""radius [pc]"")\nplt.legend(loc=4)\nplt.loglog()\nplt.show()\n'"
docs/astro/source/plot_snr_brightness_evolution.py,2,"b'""""""Plot SNR brightness evolution.""""""\nimport numpy as np\nfrom astropy.units import Quantity\nimport matplotlib.pyplot as plt\nfrom gammapy.astro.source import SNR\n\ndensities = Quantity([1, 0.1], ""cm-3"")\nt = Quantity(np.logspace(0, 5, 100), ""yr"")\n\nfor density in densities:\n    snr = SNR(n_ISM=density)\n    F = snr.luminosity_tev(t) / (4 * np.pi * Quantity(1, ""kpc"") ** 2)\n    plt.plot(t.value, F.to(""cm-2 s-1"").value, label=""n_ISM = {}"".format(density.value))\n    plt.vlines(snr.sedov_taylor_begin.to(""yr"").value, 1e-13, 1e-10, linestyle=""--"")\n    plt.vlines(snr.sedov_taylor_end.to(""yr"").value, 1e-13, 1e-10, linestyle=""--"")\n\nplt.xlim(1e2, 1e5)\nplt.ylim(1e-13, 1e-10)\nplt.xlabel(""time [years]"")\nplt.ylabel(""flux @ 1kpc [s^-1 cm^-2]"")\nplt.legend(loc=4)\nplt.loglog()\nplt.show()\n'"
docs/astro/source/plot_snr_radius_evolution.py,1,"b'""""""Plot SNR radius evolution versus time.""""""\nimport numpy as np\nfrom astropy.units import Quantity\nimport matplotlib.pyplot as plt\nfrom gammapy.astro.source import SNR, SNRTrueloveMcKee\n\nsnr_models = [SNR, SNRTrueloveMcKee]\ndensities = Quantity([1, 0.1], ""cm^-3"")\nlinestyles = [""-"", ""--""]\nt = Quantity(np.logspace(0, 5, 100), ""yr"")\n\nfor density in densities:\n    for linestyle, snr_model in zip(linestyles, snr_models):\n        snr = snr_model(n_ISM=density)\n        label = snr.__class__.__name__ + "" (n_ISM = {})"".format(density.value)\n        x = t.value\n        y = snr.radius(t).to(""pc"").value\n        plt.plot(x, y, label=label, linestyle=linestyle)\n\nplt.xlabel(""time [years]"")\nplt.ylabel(""radius [pc]"")\nplt.legend(loc=4)\nplt.loglog()\nplt.show()\n'"
examples/models/spatial/plot_constant.py,0,"b'r""""""\n.. _constant-spatial-model:\n\nConstant Spatial Model\n======================\n\nThis model is a spatially constant model.\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nfrom gammapy.maps import WcsGeom\nfrom gammapy.modeling.models import (\n    ConstantSpatialModel,\n    Models,\n    PowerLawSpectralModel,\n    SkyModel,\n)\n\ngeom = WcsGeom.create(npix=(100, 100), binsz=0.1)\nmodel = ConstantSpatialModel(value=""42 sr-1"")\nmodel.plot(geom=geom, add_cbar=True)\n\n#%%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\npwl = PowerLawSpectralModel()\nconstant = ConstantSpatialModel()\n\nmodel = SkyModel(spectral_model=pwl, spatial_model=constant, name=""pwl-constant-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spatial/plot_disk.py,1,"b'r""""""\n.. _disk-spatial-model:\n\nDisk Spatial Model\n==================\n\nThis is a spatial model parametrising a disk.\n\nBy default, the model is symmetric, i.e. a disk:\n\n.. math::\n\n    \\phi(lon, lat) = \\frac{1}{2 \\pi (1 - \\cos{r_0}) } \\cdot\n            \\begin{cases}\n                1 & \\text{for } \\theta \\leq r_0 \\\n                0 & \\text{for } \\theta > r_0\n            \\end{cases}\n\nwhere :math:`\\theta` is the sky separation. To improve fit convergence of the\nmodel, the sharp edges is smoothed using `~scipy.special.erf`.\n\nIn case an eccentricity (`e`) and rotation angle (:math:`\\phi`) are passed,\nthen the model is an elongated disk (i.e. an ellipse), with a major semiaxis of length :math:`r_0`\nand position angle :math:`\\phi` (increaing counter-clockwise from the North direction).\n\nThe model is defined on the celestial sphere, with a normalization defined by:\n\n.. math::\n\n    \\int_{4\\pi}\\phi(\\text{lon}, \\text{lat}) \\,d\\Omega = 1\\,.\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\n\nimport numpy as np\nfrom astropy.coordinates import Angle\nfrom gammapy.modeling.models import (\n    DiskSpatialModel,\n    Models,\n    PowerLawSpectralModel,\n    SkyModel,\n)\n\nphi = Angle(""30 deg"")\nmodel = DiskSpatialModel(\n    lon_0=""2 deg"", lat_0=""2 deg"", r_0=""1 deg"", e=0.8, phi=""30 deg"", frame=""galactic"",\n)\n\nax = model.plot(add_cbar=True)\n\n# illustrate size parameter\nregion = model.to_region().to_pixel(ax.wcs)\nartist = region.as_artist(facecolor=""none"", edgecolor=""red"")\nax.add_artist(artist)\n\ntransform = ax.get_transform(""galactic"")\nax.scatter(2, 2, transform=transform, s=20, edgecolor=""red"", facecolor=""red"")\nax.text(1.7, 1.85, r""$(l_0, b_0)$"", transform=transform, ha=""center"")\nax.plot([2, 2 + np.sin(phi)], [2, 2 + np.cos(phi)], color=""r"", transform=transform)\nax.vlines(x=2, color=""r"", linestyle=""--"", transform=transform, ymin=0, ymax=5)\nax.text(2.15, 2.3, r""$\\phi$"", transform=transform)\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\npwl = PowerLawSpectralModel()\ngauss = DiskSpatialModel()\n\nmodel = SkyModel(spectral_model=pwl, spatial_model=gauss, name=""pwl-disk-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spatial/plot_gauss.py,1,"b'r""""""\n.. _gaussian-spatial-model:\n\nGaussian Spatial Model\n======================\n\nThis is a spatial model parametrising a Gaussian function.\n\nBy default, the Gaussian is symmetric:\n\n.. math::\n    \\phi(\\text{lon}, \\text{lat}) = N \\times \\exp\\left\\{-\\frac{1}{2}\n        \\frac{1-\\cos \\theta}{1-\\cos \\sigma}\\right\\}\\,,\n\nwhere :math:`\\theta` is the sky separation to the model center. In this case, the\nGaussian is normalized to 1 on the sphere:\n\n.. math::\n    N = \\frac{1}{4\\pi a\\left[1-\\exp(-1/a)\\right]}\\,,\\,\\,\\,\\,\n    a = 1-\\cos \\sigma\\,.\n\nIn the limit of small :math:`\\theta` and :math:`\\sigma`, this definition\nreduces to the usual form:\n\n.. math::\n    \\phi(\\text{lon}, \\text{lat}) = \\frac{1}{2\\pi\\sigma^2} \\exp{\\left(-\\frac{1}{2}\n        \\frac{\\theta^2}{\\sigma^2}\\right)}\\,.\n\nIn case an eccentricity (:math:`e`) and rotation angle (:math:`\\phi`) are passed,\nthen the model is an elongated Gaussian, whose evaluation is performed as in the symmetric case\nbut using the effective radius of the Gaussian:\n\n.. math::\n    \\sigma_{eff}(\\text{lon}, \\text{lat}) = \\sqrt{\n        (\\sigma_M \\sin(\\Delta \\phi))^2 +\n        (\\sigma_m \\cos(\\Delta \\phi))^2\n    }.\n\nHere, :math:`\\sigma_M` (:math:`\\sigma_m`) is the major (minor) semiaxis of the Gaussian, and\n:math:`\\Delta \\phi` is the difference between `phi`, the position angle of the Gaussian, and the\nposition angle of the evaluation point.\n\n**Caveat:** For the asymmetric Gaussian, the model is normalized to 1 on the plane, i.e. in small angle\napproximation: :math:`N = 1/(2 \\pi \\sigma_M \\sigma_m)`. This means that for huge elongated Gaussians on the sky\nthis model is not correctly normalized. However, this approximation is perfectly acceptable for the more\ncommon case of models with modest dimensions: indeed, the error introduced by normalizing on the plane\nrather than on the sphere is below 0.1\\% for Gaussians with radii smaller than ~ 5 deg.\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nimport numpy as np\nfrom astropy.coordinates import Angle\nfrom gammapy.maps import WcsGeom\nfrom gammapy.modeling.models import (\n    GaussianSpatialModel,\n    Models,\n    PowerLawSpectralModel,\n    SkyModel,\n)\n\nphi = Angle(""30 deg"")\nmodel = GaussianSpatialModel(\n    lon_0=""2 deg"", lat_0=""2 deg"", sigma=""1 deg"", e=0.7, phi=phi, frame=""galactic"",\n)\n\ngeom = WcsGeom.create(\n    skydir=model.position, frame=model.frame, width=(4, 4), binsz=0.02\n)\nax = model.plot(geom=geom, add_cbar=True)\n\n# illustrate size parameter\nregion = model.to_region().to_pixel(ax.wcs)\nartist = region.as_artist(facecolor=""none"", edgecolor=""red"")\nax.add_artist(artist)\n\ntransform = ax.get_transform(""galactic"")\nax.scatter(2, 2, transform=transform, s=20, edgecolor=""red"", facecolor=""red"")\nax.text(1.5, 1.85, r""$(l_0, b_0)$"", transform=transform, ha=""center"")\nax.plot([2, 2 + np.sin(phi)], [2, 2 + np.cos(phi)], color=""r"", transform=transform)\nax.vlines(x=2, color=""r"", linestyle=""--"", transform=transform, ymin=-5, ymax=5)\nax.text(2.25, 2.45, r""$\\phi$"", transform=transform)\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\npwl = PowerLawSpectralModel()\ngauss = GaussianSpatialModel()\n\nmodel = SkyModel(spectral_model=pwl, spatial_model=gauss, name=""pwl-gauss-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spatial/plot_point.py,0,"b'r""""""\n.. _point-spatial-model:\n\nPoint Spatial Model\n===================\n\nThis model is a delta function centered in *lon_0* and *lat_0* parameters provided:\n\n.. math:: \\phi(lon, lat) = \\delta{(lon - lon_0, lat - lat_0)}\n\nThe model is defined on the celestial sphere in the coordinate frame provided by the user.\n\nIf the point source is not centered on a pixel, the flux is re-distributed\nacross 4 neighbouring pixels. This ensured that the center of mass position\nis conserved.\n""""""\n\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nfrom astropy.coordinates import SkyCoord\nfrom gammapy.maps import WcsGeom\nfrom gammapy.modeling.models import (\n    Models,\n    PointSpatialModel,\n    PowerLawSpectralModel,\n    SkyModel,\n)\n\nmodel = PointSpatialModel(lon_0=""0.01 deg"", lat_0=""0.01 deg"", frame=""galactic"",)\n\ngeom = WcsGeom.create(\n    skydir=SkyCoord(""0d 0d"", frame=""galactic""), width=(1, 1), binsz=0.1\n)\nmodel.plot(geom=geom, add_cbar=True)\n\n#%%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\npwl = PowerLawSpectralModel()\npoint = PointSpatialModel()\n\nmodel = SkyModel(spectral_model=pwl, spatial_model=point, name=""pwl-point-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spatial/plot_shell.py,0,"b'r""""""\n.. _shell-spatial-model:\n\nShell Spatial Model\n===================\n\nThis is a spatial model parametrizing a projected radiating shell.\n\nThe shell spatial model is defined by the following equations:\n\n.. math::\n    \\phi(lon, lat) = \\frac{3}{2 \\pi (r_{out}^3 - r_{in}^3)} \\cdot\n            \\begin{cases}\n                \\sqrt{r_{out}^2 - \\theta^2} - \\sqrt{r_{in}^2 - \\theta^2} &\n                             \\text{for } \\theta \\lt r_{in} \\\\\n                \\sqrt{r_{out}^2 - \\theta^2} &\n                             \\text{for } r_{in} \\leq \\theta \\lt r_{out} \\\\\n                0 & \\text{for } \\theta > r_{out}\n            \\end{cases}\n\nwhere :math:`\\theta` is the sky separation and :math:`r_{\\text{out}} = r_{\\text{in}}` + width\n\nNote that the normalization is a small angle approximation,\nalthough that approximation is still very good even for 10 deg radius shells.\n\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nfrom gammapy.modeling.models import (\n    Models,\n    PowerLawSpectralModel,\n    ShellSpatialModel,\n    SkyModel,\n)\n\nmodel = ShellSpatialModel(\n    lon_0=""10 deg"", lat_0=""20 deg"", radius=""2 deg"", width=""0.5 deg"", frame=""galactic"",\n)\n\nmodel.plot(add_cbar=True)\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\npwl = PowerLawSpectralModel()\nshell = ShellSpatialModel()\n\nmodel = SkyModel(spectral_model=pwl, spatial_model=shell, name=""pwl-shell-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spatial/plot_template.py,0,"b'r""""""\n.. _template-spatial-model:\n\nTemplate Spatial Model\n======================\n\nThis is a spatial model based on a 2D sky map provided as a template.\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nfrom gammapy.maps import Map\nfrom gammapy.modeling.models import (\n    Models,\n    PowerLawSpectralModel,\n    SkyModel,\n    TemplateSpatialModel,\n)\n\nfilename = ""$GAMMAPY_DATA/catalogs/fermi/Extended_archive_v18/Templates/RXJ1713_2016_250GeV.fits""\n\nm = Map.read(filename)\nm.unit = ""sr^-1""\nmodel = TemplateSpatialModel(m)\n\nmodel.plot(add_cbar=True)\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\npwl = PowerLawSpectralModel()\ntemplate = TemplateSpatialModel(m)\n\nmodel = SkyModel(spectral_model=pwl, spatial_model=template, name=""pwl-template-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spectral/plot_absorbed.py,0,"b'r""""""\n.. _absorbed-spectral-model:\n\nAbsorbed Spectral Model\n=======================\n\nThis model evaluates absorbed spectral model.\n\nThe spectral model is evaluated, and then multiplied with an EBL\nabsorption factor given by\n\n.. math::\n    \\exp{ \\left ( -\\alpha \\times \\tau(E, z) \\right )}\n\nwhere :math:`\\tau(E, z)` is the optical depth predicted by the model\n(`~gammapy.modeling.models.Absorption`), which depends on the energy of the gamma-rays and the\nredshift z of the source, and :math:`\\alpha` is a scale factor\n(default: 1) for the optical depth.\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.modeling.models import (\n    AbsorbedSpectralModel,\n    Absorption,\n    Models,\n    PowerLawSpectralModel,\n    SkyModel,\n)\n\nredshift = 0.117\nabsorption = Absorption.read_builtin(""dominguez"")\n\n# Spectral model corresponding to PKS 2155-304 (quiescent state)\nindex = 3.53\namplitude = 1.81 * 1e-12 * u.Unit(""cm-2 s-1 TeV-1"")\nreference = 1 * u.TeV\npwl = PowerLawSpectralModel(index=index, amplitude=amplitude, reference=reference)\n\n# EBL + PWL model\nmodel = AbsorbedSpectralModel(\n    spectral_model=pwl, absorption=absorption, redshift=redshift\n)\n\nenergy_range = [0.1, 100] * u.TeV\nmodel.plot(energy_range)\nplt.grid(which=""both"")\nplt.ylim(1e-24, 1e-8)\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\nmodel = SkyModel(spectral_model=model, name=""absorbed-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spectral/plot_absorption.py,0,"b'r""""""\n.. _absorption-spectral-model:\n\nAbsorption Spectral Model\n=========================\n\nThis model represents EBL absorption models.\n\nThey are usually used as part of `~gammapy.modeling.models.AbsorbedSpectralModel`\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here we illustrate how to create and plot EBL absorption models for a redshift of 0.5:\n\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.modeling.models import Absorption\n\nredshift = 0.5\ndominguez = Absorption.read_builtin(""dominguez"").table_model(redshift)\nfranceschini = Absorption.read_builtin(""franceschini"").table_model(redshift)\nfinke = Absorption.read_builtin(""finke"").table_model(redshift)\n\nplt.figure()\nenergy_range = [0.08, 3] * u.TeV\nopts = dict(energy_range=energy_range, energy_unit=""TeV"", flux_unit="""")\nfranceschini.plot(label=""Franceschini 2008"", **opts)\nfinke.plot(label=""Finke 2010"", **opts)\ndominguez.plot(label=""Dominguez 2011"", **opts)\n\nplt.ylabel(r""Absorption coefficient [$\\exp{(-\\tau(E))}$]"")\nplt.xlim(energy_range.value)\nplt.ylim(1e-4, 2)\nplt.title(f""EBL models (z={redshift})"")\nplt.grid(which=""both"")\nplt.legend(loc=""best"")\n'"
examples/models/spectral/plot_compound.py,0,"b'r""""""\n.. _compound-spectral-model:\n\nCompound Spectral Model\n=======================\n\nThis model is formed by the arithmetic combination of any two other spectral models.\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nimport operator\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.modeling.models import (\n    CompoundSpectralModel,\n    LogParabolaSpectralModel,\n    Models,\n    PowerLawSpectralModel,\n    SkyModel,\n)\n\nenergy_range = [0.1, 100] * u.TeV\npwl = PowerLawSpectralModel(\n    index=2.0, amplitude=""1e-12 cm-2 s-1 TeV-1"", reference=""1 TeV""\n)\nlp = LogParabolaSpectralModel(\n    amplitude=""1e-12 cm-2 s-1 TeV-1"", reference=""10 TeV"", alpha=2.0, beta=1.0\n)\nmodel = CompoundSpectralModel(pwl, lp, operator.add)\nmodel.plot(energy_range)\nplt.grid(which=""both"")\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\nmodel = SkyModel(spectral_model=model, name=""compound-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spectral/plot_constant_spectral.py,0,"b'r""""""\n.. _constant-spectral-model:\n\nConstant Spectral Model\n=======================\n\nThis model takes a constant value along the spectral range.\n\n    .. math:: \\phi(E) = k\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.modeling.models import ConstantSpectralModel, Models, SkyModel\n\nenergy_range = [0.1, 100] * u.TeV\nmodel = ConstantSpectralModel(const=""1 / (cm2 s TeV)"")\nmodel.plot(energy_range)\nplt.grid(which=""both"")\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\nmodel = SkyModel(spectral_model=model, name=""constant-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spectral/plot_exp_cutoff_powerlaw.py,0,"b'r""""""\n.. _exp-cutoff-powerlaw-spectral-model:\n\nExponential Cutoff Powerlaw Spectral Model\n==========================================\n\nThis model parametrises a cutoff power law spectrum.\n\nIt is defined by the following equation:\n\n.. math::\n    \\phi(E) = \\phi_0 \\cdot \\left(\\frac{E}{E_0}\\right)^{-\\Gamma} \\exp(- {(\\lambda E})^{\\alpha})\n\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.modeling.models import ExpCutoffPowerLawSpectralModel, Models, SkyModel\n\nenergy_range = [0.1, 100] * u.TeV\nmodel = ExpCutoffPowerLawSpectralModel(\n    amplitude=1e-12 * u.Unit(""cm-2 s-1 TeV-1""),\n    index=2,\n    lambda_=0.1 * u.Unit(""TeV-1""),\n    reference=1 * u.TeV,\n)\nmodel.plot(energy_range)\nplt.grid(which=""both"")\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\nmodel = SkyModel(spectral_model=model, name=""exp-cutoff-power-law-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spectral/plot_exp_cutoff_powerlaw_3fgl.py,0,"b'r""""""\n.. _exp-cutoff-powerlaw-3fgl-spectral-model:\n\nExponential Cutoff Powerlaw Spectral Model used for 3FGL\n========================================================\n\nThis model parametrises a cutoff power law spectrum used for 3FGL.\n\nIt is defined by the following equation:\n\n.. math::\n    \\phi(E) = \\phi_0 \\cdot \\left(\\frac{E}{E_0}\\right)^{-\\Gamma}\n              \\exp \\left( \\frac{E_0 - E}{E_{C}} \\right)\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.modeling.models import ExpCutoffPowerLaw3FGLSpectralModel, Models, SkyModel\n\nenergy_range = [0.1, 100] * u.TeV\nmodel = ExpCutoffPowerLaw3FGLSpectralModel(\n    index=2.3 * u.Unit(""""),\n    amplitude=4 / u.cm ** 2 / u.s / u.TeV,\n    reference=1 * u.TeV,\n    ecut=10 * u.TeV,\n)\nmodel.plot(energy_range)\nplt.grid(which=""both"")\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\nmodel = SkyModel(spectral_model=model, name=""exp-cutoff-power-law-3fgl-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spectral/plot_gauss_spectral.py,0,"b'r""""""\n.. _gaussian-spectral-model:\n\nGaussian Spectral Model\n=======================\n\nThis model parametrises a gaussian spectrum.\n\nIt is defined by the following equation:\n\n.. math::\n    \\phi(E) = \\frac{N_0}{\\sigma \\sqrt{2\\pi}}  \\exp{ \\frac{- \\left( E-\\bar{E} \\right)^2 }{2 \\sigma^2} }\n\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.modeling.models import GaussianSpectralModel, Models, SkyModel\n\nenergy_range = [0.1, 100] * u.TeV\nmodel = GaussianSpectralModel(norm=""1e-2 cm-2 s-1"", mean=2 * u.TeV, sigma=0.2 * u.TeV)\nmodel.plot(energy_range)\nplt.grid(which=""both"")\nplt.ylim(1e-24, 1e-1)\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\nmodel = SkyModel(spectral_model=model, name=""gaussian-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spectral/plot_logparabola.py,0,"b'r""""""\n.. _logparabola-spectral-model:\n\nLog Parabola Spectral Model\n===========================\n\nThis model parametrises a log parabola spectrum.\n\nIt is defined by the following equation:\n\n.. math::\n        \\phi(E) = \\phi_0 \\left( \\frac{E}{E_0} \\right) ^ {\n          - \\alpha - \\beta \\log{ \\left( \\frac{E}{E_0} \\right) }\n        }\n\nNote that :math:`log` refers to the natural logarithm. This is consistent\nwith the `Fermi Science Tools\n<https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/source_models.html>`_\nand `ctools\n<http://cta.irap.omp.eu/ctools-devel/users/user_manual/getting_started/models.html#log-parabola>`_.\nThe `Sherpa <http://cxc.harvard.edu/sherpa/ahelp/logparabola.html_\npackage>`_ package, however, uses :math:`log_{10}`. If you have\nparametrization based on :math:`log_{10}` you can use the\n:func:`~gammapy.modeling.models.LogParabolaSpectralModel.from_log10` method.\n\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.modeling.models import LogParabolaSpectralModel, Models, SkyModel\n\nenergy_range = [0.1, 100] * u.TeV\nmodel = LogParabolaSpectralModel(\n    alpha=2.3, amplitude=""1e-12 cm-2 s-1 TeV-1"", reference=1 * u.TeV, beta=0.5,\n)\nmodel.plot(energy_range)\nplt.grid(which=""both"")\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\nmodel = SkyModel(spectral_model=model, name=""log-parabola-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spectral/plot_naima.py,0,"b'r""""""\n.. _naima-spectral-model:\n\nNaima Spectral Model\n=======================\n\nThis class provides an interface with the models defined in the naima models module.\n\nThe model accepts as a positional argument a `Naima <https://naima.readthedocs.io/en/latest/>`_\nradiative `~naima.models` instance, used to compute the non-thermal emission from populations of\nrelativistic electrons or protons due to interactions with the ISM or with radiation and magnetic fields.\n\nOne of the advantages provided by this class consists in the possibility of performing a maximum\nlikelihood spectral fit of the model\'s parameters directly on observations, as opposed to the MCMC\n`fit to flux points <https://naima.readthedocs.io/en/latest/mcmc.html>`_ featured in\nNaima. All the parameters defining the parent population of charged particles are stored as\n`~gammapy.modeling.Parameter` and left free by default. In case that the radiative model is\n`~naima.radiative.Synchrotron`, the magnetic field strength may also be fitted. Parameters can be\nfreezed/unfreezed before the fit, and maximum/minimum values can be set to limit the parameters space to\nthe physically interesting region.\n\n\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here we create and plot a spectral model that convolves an `~gammapy.modeling.models.ExpCutoffPowerLawSpectralModel`\n# electron distribution with an `InverseCompton` radiative model, in the presence of multiple seed photon fields.\n\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\nimport naima\nfrom gammapy.modeling.models import Models, NaimaSpectralModel, SkyModel\n\nparticle_distribution = naima.models.ExponentialCutoffPowerLaw(\n    1e30 / u.eV, 10 * u.TeV, 3.0, 30 * u.TeV\n)\nradiative_model = naima.radiative.InverseCompton(\n    particle_distribution,\n    seed_photon_fields=[""CMB"", [""FIR"", 26.5 * u.K, 0.415 * u.eV / u.cm ** 3]],\n    Eemin=100 * u.GeV,\n)\n\nmodel = NaimaSpectralModel(radiative_model, distance=1.5 * u.kpc)\n\nopts = {\n    ""energy_range"": [10 * u.GeV, 80 * u.TeV],\n    ""energy_power"": 2,\n    ""flux_unit"": ""erg-1 cm-2 s-1"",\n}\n\n# Plot the total inverse Compton emission\nmodel.plot(label=""IC (total)"", **opts)\n\n# Plot the separate contributions from each seed photon field\nfor seed, ls in zip([""CMB"", ""FIR""], [""-"", ""--""]):\n    model = NaimaSpectralModel(radiative_model, seed=seed, distance=1.5 * u.kpc)\n    model.plot(label=f""IC ({seed})"", ls=ls, color=""gray"", **opts)\n\nplt.legend(loc=""best"")\nplt.grid(which=""both"")\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\nmodel = SkyModel(spectral_model=model, name=""naima-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spectral/plot_powerlaw.py,0,"b'r""""""\n.. _powerlaw-spectral-model:\n\nPower Law Spectral Model\n========================\n\nThis model parametrises a power law spectrum.\n\nIt is defined by the following equation:\n\n.. math::\n    \\phi(E) = \\phi_0 \\cdot \\left( \\frac{E}{E_0} \\right)^{-\\Gamma}\n\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.modeling.models import Models, PowerLawSpectralModel, SkyModel\n\nenergy_range = [0.1, 100] * u.TeV\nmodel = PowerLawSpectralModel(\n    index=2, amplitude=""1e-12 TeV-1 cm-2 s-1"", reference=1 * u.TeV,\n)\nmodel.plot(energy_range)\nplt.grid(which=""both"")\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\nmodel = SkyModel(spectral_model=model, name=""power-law-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spectral/plot_powerlaw2.py,0,"b'r""""""\n.. _powerlaw2-spectral-model:\n\nPower Law 2 Spectral Model\n==========================\n\nThis model parametrises a power law spectrum with integral as amplitude parameter.\n\nIt is defined by the following equation:\n\n.. math::\n    \\phi(E) = F_0 \\cdot \\frac{\\Gamma + 1}{E_{0, max}^{-\\Gamma + 1}\n     - E_{0, min}^{-\\Gamma + 1}} \\cdot E^{-\\Gamma}\n\nSee also: https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/source_models.html\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.modeling.models import Models, PowerLaw2SpectralModel, SkyModel\n\nenergy_range = [0.1, 100] * u.TeV\nmodel = PowerLaw2SpectralModel(\n    amplitude=u.Quantity(1e-12, ""cm-2 s-1""), index=2.3, emin=1 * u.TeV, emax=10 * u.TeV,\n)\nmodel.plot(energy_range)\nplt.grid(which=""both"")\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\nmodel = SkyModel(spectral_model=model, name=""power-law2-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spectral/plot_smooth_broken_powerlaw.py,0,"b'r""""""\n.. _smooth-broken-powerlaw-spectral-model:\n\nSmooth Broken Power Law Spectral Model\n======================================\n\nThis model parametrises a smooth broken power law spectrum.\n\nIt is defined by the following equation:\n\n.. math::\n    \\phi(E) = \\phi_0 \\cdot \\left( \\frac{E}{E_0} \\right)^{-\\Gamma1}\\left(1 + \\frac{E}{E_{break}}^{\\frac{\\Gamma2-\\Gamma1}{\\beta}} \\right)^{-\\beta}\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.modeling.models import Models, SkyModel, SmoothBrokenPowerLawSpectralModel\n\nenergy_range = [0.1, 100] * u.TeV\nmodel = SmoothBrokenPowerLawSpectralModel(\n    index1=1.5,\n    index2=2.5,\n    amplitude=""1e-12 TeV-1 cm-2 s-1"",\n    ebreak=""1 TeV"",\n    reference=""1 TeV"",\n    beta=1,\n)\nmodel.plot(energy_range)\nplt.grid(which=""both"")\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\nmodel = SkyModel(spectral_model=model, name=""smooth-broken-power-law-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spectral/plot_super_exp_cutoff_powerlaw_3fgl.py,0,"b'r""""""\n.. _super-exp-cutoff-powerlaw-3fgl-spectral-model:\n\nSuper Exponential Cutoff Power Law Model used for 3FGL\n======================================================\n\nThis model parametrises super exponential cutoff power-law model spectrum used for 3FGL.\n\nIt is defined by the following equation:\n\n.. math::\n    \\phi(E) = \\phi_0 \\cdot \\left(\\frac{E}{E_0}\\right)^{-\\Gamma_1}\n              \\exp \\left( \\left(\\frac{E_0}{E_{C}} \\right)^{\\Gamma_2} -\n                          \\left(\\frac{E}{E_{C}} \\right)^{\\Gamma_2}\n                          \\right)\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.modeling.models import (\n    Models,\n    SkyModel,\n    SuperExpCutoffPowerLaw3FGLSpectralModel,\n)\n\nenergy_range = [0.1, 100] * u.TeV\nmodel = SuperExpCutoffPowerLaw3FGLSpectralModel(\n    index_1=1,\n    index_2=2,\n    amplitude=""1e-12 TeV-1 s-1 cm-2"",\n    reference=""1 TeV"",\n    ecut=""10 TeV"",\n)\nmodel.plot(energy_range)\nplt.grid(which=""both"")\nplt.ylim(1e-24, 1e-10)\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\nmodel = SkyModel(spectral_model=model, name=""super-exp-cutoff-power-law-3fgl-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spectral/plot_super_exp_cutoff_powerlaw_4fgl.py,0,"b'r""""""\n.. _super-exp-cutoff-powerlaw-4fgl-spectral-model:\n\nSuper Exponential Cutoff Power Law Model used for 4FGL\n======================================================\n\nThis model parametrises super exponential cutoff power-law model spectrum used for 4FGL.\n\nIt is defined by the following equation:\n\n.. math::\n    \\phi(E) = \\phi_0 \\cdot \\left(\\frac{E}{E_0}\\right)^{-\\Gamma_1}\n              \\exp \\left(\n                  a \\left( E_0 ^{\\Gamma_2} - E^{\\Gamma_2} \\right)\n              \\right)\n\nSee Equation (3) in https://arxiv.org/pdf/1902.10045.pdf\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.modeling.models import (\n    Models,\n    SkyModel,\n    SuperExpCutoffPowerLaw4FGLSpectralModel,\n)\n\nenergy_range = [0.1, 100] * u.TeV\nmodel = SuperExpCutoffPowerLaw4FGLSpectralModel(\n    index_1=1,\n    index_2=2,\n    amplitude=""1e-12 TeV-1 cm-2 s-1"",\n    reference=""1 TeV"",\n    expfactor=1e-2,\n)\nmodel.plot(energy_range)\nplt.grid(which=""both"")\nplt.ylim(1e-24, 1e-10)\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\nmodel = SkyModel(spectral_model=model, name=""super-exp-cutoff-power-law-4fgl-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
examples/models/spectral/plot_template_spectral.py,2,"b'r""""""\n.. _template-spectral-model:\n\nTemplate Spectral Model\n=======================\n\nThis model is defined by custom tabular values.\n\nThe units returned will be the units of the values array provided at\ninitialization. The model will return values interpolated in\nlog-space, returning 0 for energies outside of the limits of the provided\nenergy array.\n\nThe class implementation follows closely what has been done in\n`naima.models.TemplateSpectralModel`\n""""""\n\n# %%\n# Example plot\n# ------------\n# Here is an example plot of the model:\n\nimport numpy as np\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\nfrom gammapy.modeling.models import Models, SkyModel, TemplateSpectralModel\n\nenergy_range = [0.1, 1] * u.TeV\nenergy = np.array([1e6, 3e6, 1e7, 3e7]) * u.MeV\nvalues = np.array([4.4e-38, 2.0e-38, 8.8e-39, 3.9e-39]) * u.Unit(""MeV-1 s-1 cm-2"")\nmodel = TemplateSpectralModel(energy=energy, values=values)\nmodel.plot(energy_range)\nplt.grid(which=""both"")\n\n# %%\n# YAML representation\n# -------------------\n# Here is an example YAML file using the model:\n\nmodel = SkyModel(spectral_model=model, name=""template-model"")\nmodels = Models([model])\n\nprint(models.to_yaml())\n'"
gammapy/analysis/tests/__init__.py,0,b''
gammapy/analysis/tests/test_analysis.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom pathlib import Path\nimport pytest\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nfrom regions import CircleSkyRegion\nfrom pydantic.error_wrappers import ValidationError\nfrom gammapy.analysis import Analysis, AnalysisConfig\nfrom gammapy.datasets import MapDataset, SpectrumDatasetOnOff\nfrom gammapy.maps import Map, WcsNDMap\nfrom gammapy.modeling.models import Models\nfrom gammapy.utils.testing import requires_data, requires_dependency\n\nCONFIG_PATH = Path(__file__).resolve().parent / "".."" / ""config""\nMODEL_FILE = CONFIG_PATH / ""model.yaml""\nMODEL_FILE_1D = CONFIG_PATH / ""model-1d.yaml""\n\n\ndef get_example_config(which):\n    """"""Example config: which can be 1d or 3d.""""""\n    return AnalysisConfig.read(CONFIG_PATH / f""example-{which}.yaml"")\n\n\ndef test_init():\n    cfg = {""general"": {""outdir"": ""test""}}\n    analysis = Analysis(cfg)\n    assert analysis.config.general.outdir == ""test""\n    with pytest.raises(TypeError):\n        Analysis(""spam"")\n\n\ndef test_update_config():\n    analysis = Analysis(AnalysisConfig())\n    data = {""general"": {""outdir"": ""test""}}\n    config = AnalysisConfig(**data)\n    analysis.update_config(config)\n    assert analysis.config.general.outdir == ""test""\n\n    analysis = Analysis(AnalysisConfig())\n    data = """"""\n    general:\n        outdir: test\n    """"""\n    analysis.update_config(data)\n    assert analysis.config.general.outdir == ""test""\n\n    analysis = Analysis(AnalysisConfig())\n    with pytest.raises(TypeError):\n        analysis.update_config(0)\n\n\ndef test_get_observations_no_datastore():\n    config = AnalysisConfig()\n    analysis = Analysis(config)\n    analysis.config.observations.datastore = ""other""\n    with pytest.raises(FileNotFoundError):\n        analysis.get_observations()\n\n\n@requires_data()\ndef test_get_observations_all():\n    config = AnalysisConfig()\n    analysis = Analysis(config)\n    analysis.config.observations.datastore = ""$GAMMAPY_DATA/cta-1dc/index/gps/""\n    analysis.get_observations()\n    assert len(analysis.observations) == 4\n\n\n@requires_data()\ndef test_get_observations_obs_ids():\n    config = AnalysisConfig()\n    analysis = Analysis(config)\n    analysis.config.observations.datastore = ""$GAMMAPY_DATA/cta-1dc/index/gps/""\n    analysis.config.observations.obs_ids = [""110380""]\n    analysis.get_observations()\n    assert len(analysis.observations) == 1\n\n\n@requires_data()\ndef test_get_observations_obs_cone():\n    config = AnalysisConfig()\n    analysis = Analysis(config)\n    analysis.config.observations.datastore = ""$GAMMAPY_DATA/hess-dl3-dr1""\n    analysis.config.observations.obs_cone = {\n        ""frame"": ""icrs"",\n        ""lon"": ""83d"",\n        ""lat"": ""22d"",\n        ""radius"": ""5d"",\n    }\n    analysis.get_observations()\n    assert len(analysis.observations) == 4\n\n\n@requires_data()\ndef test_get_observations_obs_file(tmp_path):\n    config = AnalysisConfig()\n    analysis = Analysis(config)\n    analysis.get_observations()\n    filename = tmp_path / ""obs_ids.txt""\n    filename.write_text(""20136\\n47829\\n"")\n    analysis.config.observations.obs_file = filename\n    analysis.get_observations()\n    assert len(analysis.observations) == 2\n\n\n@requires_data()\ndef test_get_observations_obs_time(tmp_path):\n    config = AnalysisConfig()\n    analysis = Analysis(config)\n    analysis.config.observations.obs_time = {\n        ""start"": ""2004-03-26"",\n        ""stop"": ""2004-05-26"",\n    }\n    analysis.get_observations()\n    assert len(analysis.observations) == 40\n    analysis.config.observations.obs_ids = [0]\n    with pytest.raises(ValueError):\n        analysis.get_observations()\n\n\n@requires_data()\ndef test_set_models():\n    config = get_example_config(""1d"")\n    analysis = Analysis(config)\n    analysis.get_observations()\n    analysis.get_datasets()\n    models_str = Path(MODEL_FILE).read_text()\n    analysis.set_models(models=models_str)\n    assert isinstance(analysis.models, Models) is True\n    with pytest.raises(TypeError):\n        analysis.set_models(0)\n\n\n@requires_dependency(""iminuit"")\n@requires_data()\ndef test_analysis_1d():\n    cfg = """"""\n    observations:\n        datastore: $GAMMAPY_DATA/hess-dl3-dr1\n        obs_ids: [23523, 23526]\n    datasets:\n        type: 1d\n        background:\n            method: reflected\n        geom:\n            axes:\n                energy_true: {min: 0.01 TeV, max: 300 TeV, nbins: 109}\n        on_region: {frame: icrs, lon: 83.633 deg, lat: 22.014 deg, radius: 0.11 deg}\n        safe_mask:\n            methods: [aeff-default, edisp-bias]\n            parameters: {bias_percent: 10.0}\n        containment_correction: false\n    flux_points:\n        energy: {min: 1 TeV, max: 50 TeV, nbins: 4}\n    """"""\n    config = get_example_config(""1d"")\n    analysis = Analysis(config)\n    analysis.update_config(cfg)\n    analysis.get_observations()\n    analysis.get_datasets()\n    analysis.read_models(MODEL_FILE_1D)\n    analysis.run_fit()\n    analysis.get_flux_points()\n\n    assert len(analysis.datasets) == 2\n    assert len(analysis.flux_points.data.table) == 4\n    dnde = analysis.flux_points.data.table[""dnde""].quantity\n    assert dnde.unit == ""cm-2 s-1 TeV-1""\n\n    assert_allclose(dnde[0].value, 8.116854e-12, rtol=1e-2)\n    assert_allclose(dnde[2].value, 3.547128e-14, rtol=1e-2)\n\n\n@requires_data()\ndef test_geom_analysis_1d():\n    cfg = """"""\n    observations:\n        datastore: $GAMMAPY_DATA/hess-dl3-dr1\n        obs_ids: [23523]\n    datasets:\n        type: 1d\n        background:\n            method: reflected\n        on_region: {frame: icrs, lon: 83.633 deg, lat: 22.014 deg, radius: 0.11 deg}\n        geom:\n            axes:\n                energy: {min: 0.1 TeV, max: 30 TeV, nbins: 20}\n                energy_true: {min: 0.03 TeV, max: 100 TeV, nbins: 50}\n        containment_correction: false\n    flux_points:\n        energy: {min: 1 TeV, max: 50 TeV, nbins: 4}\n    """"""\n    config = get_example_config(""1d"")\n    analysis = Analysis(config)\n    analysis.update_config(cfg)\n    analysis.get_observations()\n    analysis.get_datasets()\n\n    assert len(analysis.datasets) == 1\n    assert len(analysis.datasets[0].aeff.energy.center) == 50\n    assert_allclose(analysis.datasets[0].aeff.energy.edges[0].to_value(""TeV""), 0.03)\n    assert_allclose(analysis.datasets[0].aeff.energy.edges[-1].to_value(""TeV""), 100)\n\n\n@requires_data()\ndef test_exclusion_region(tmp_path):\n    config = get_example_config(""1d"")\n    analysis = Analysis(config)\n    exclusion_region = CircleSkyRegion(center=SkyCoord(""85d 23d""), radius=1 * u.deg)\n    exclusion_mask = Map.create(npix=(150, 150), binsz=0.05, skydir=SkyCoord(""83d 22d""))\n    mask = exclusion_mask.geom.region_mask([exclusion_region], inside=False)\n    exclusion_mask.data = mask.astype(int)\n    filename = tmp_path / ""exclusion.fits""\n    exclusion_mask.write(filename)\n    config.datasets.background.method = ""reflected""\n    config.datasets.background.exclusion = filename\n    analysis.get_observations()\n    analysis.get_datasets()\n    assert len(analysis.datasets) == 2\n\n    config = get_example_config(""3d"")\n    analysis = Analysis(config)\n    analysis.get_observations()\n    analysis.get_datasets()\n    geom = analysis.datasets[0]._geom\n    exclusion = WcsNDMap.from_geom(geom)\n    exclusion.data = geom.region_mask([exclusion_region], inside=False).astype(int)\n    filename = tmp_path / ""exclusion3d.fits""\n    exclusion.write(filename)\n    config.datasets.background.exclusion = filename\n    analysis.get_datasets()\n    assert len(analysis.datasets) == 1\n\n\n@requires_dependency(""iminuit"")\n@requires_data()\ndef test_analysis_1d_stacked():\n    cfg = """"""\n    datasets:\n        geom:\n            axes:\n                energy_true: {min: 0.03 TeV, max: 100 TeV, nbins: 50}\n        background:\n            method: reflected\n    """"""\n\n    config = get_example_config(""1d"")\n    analysis = Analysis(config)\n    analysis.update_config(cfg)\n    analysis.config.datasets.stack = True\n    analysis.get_observations()\n    analysis.get_datasets()\n    analysis.read_models(MODEL_FILE_1D)\n    analysis.run_fit()\n\n    assert len(analysis.datasets) == 1\n    assert_allclose(analysis.datasets[""stacked""].counts.data.sum(), 184)\n    pars = analysis.fit_result.parameters\n\n    assert_allclose(pars[""index""].value, 2.76913, rtol=1e-2)\n    assert_allclose(pars[""amplitude""].value, 5.496388e-11, rtol=1e-2)\n\n\n@requires_data()\ndef test_analysis_ring_background():\n    config = get_example_config(""3d"")\n    config.datasets.background.method = ""ring""\n    config.datasets.background.parameters = {""r_in"": ""0.7 deg"", ""width"": ""0.7 deg""}\n    config.datasets.geom.axes.energy.nbins = 1\n    analysis = Analysis(config)\n    analysis.get_observations()\n    analysis.get_datasets()\n    assert isinstance(analysis.datasets[0], MapDataset)\n    assert_allclose(\n        analysis.datasets[0].background_model.map.data[0, 10, 10], 0.091552, rtol=1e-5\n    )\n\n\n@requires_data()\ndef test_analysis_ring_3d():\n    config = get_example_config(""3d"")\n    config.datasets.background.method = ""ring""\n    config.datasets.background.parameters = {""r_in"": ""0.7 deg"", ""width"": ""0.7 deg""}\n    analysis = Analysis(config)\n    analysis.get_observations()\n    with pytest.raises(ValueError):\n        analysis.get_datasets()\n\n\n@requires_data()\ndef test_analysis_no_bkg():\n    config = get_example_config(""1d"")\n    analysis = Analysis(config)\n    analysis.get_observations()\n    analysis.get_datasets()\n    assert isinstance(analysis.datasets[0], SpectrumDatasetOnOff) is False\n\n    config = get_example_config(""3d"")\n    config.datasets.background.method = None\n    analysis = Analysis(config)\n    analysis.get_observations()\n    analysis.get_datasets()\n    assert isinstance(analysis.datasets[0], MapDataset) is True\n\n\n@requires_dependency(""iminuit"")\n@requires_data()\ndef test_analysis_3d():\n    config = get_example_config(""3d"")\n    analysis = Analysis(config)\n    analysis.get_observations()\n    analysis.get_datasets()\n    analysis.read_models(MODEL_FILE)\n    analysis.datasets[""stacked""].background_model.tilt.frozen = False\n    analysis.run_fit()\n    analysis.get_flux_points()\n\n    assert len(analysis.datasets) == 1\n    assert len(analysis.fit_result.parameters) == 8\n    res = analysis.fit_result.parameters\n    assert res[""amplitude""].unit == ""cm-2 s-1 TeV-1""\n    assert len(analysis.flux_points.data.table) == 2\n    dnde = analysis.flux_points.data.table[""dnde""].quantity\n\n    assert_allclose(dnde[0].value, 1.340073e-11, rtol=1e-2)\n    assert_allclose(dnde[-1].value, 3.483509e-13, rtol=1e-2)\n    assert_allclose(res[""index""].value, 3.097613, rtol=1e-2)\n    assert_allclose(res[""tilt""].value, -0.207792, rtol=1e-2)\n\n\n@requires_data()\ndef test_analysis_3d_joint_datasets():\n    config = get_example_config(""3d"")\n    config.datasets.stack = False\n    analysis = Analysis(config)\n    analysis.get_observations()\n    analysis.get_datasets()\n    assert len(analysis.datasets) == 2\n    assert_allclose(analysis.datasets[0].background_model.norm.value, 1.031743694988066)\n    assert_allclose(analysis.datasets[0].background_model.tilt.value, 0.0)\n    assert_allclose(\n        analysis.datasets[1].background_model.norm.value, 0.9776349021876344\n    )\n\n\n@requires_dependency(""iminuit"")\n@requires_data()\ndef test_usage_errors():\n    config = get_example_config(""1d"")\n    analysis = Analysis(config)\n    with pytest.raises(RuntimeError):\n        analysis.get_datasets()\n    with pytest.raises(RuntimeError):\n        analysis.read_models(MODEL_FILE)\n    with pytest.raises(RuntimeError):\n        analysis.run_fit()\n    with pytest.raises(RuntimeError):\n        analysis.get_flux_points()\n    with pytest.raises(ValidationError):\n        analysis.config.datasets.type = ""None""\n'"
gammapy/analysis/tests/test_config.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom pathlib import Path\nimport pytest\nfrom astropy.coordinates import Angle\nfrom astropy.time import Time\nfrom astropy.units import Quantity\nfrom pydantic import ValidationError\nfrom gammapy.analysis.config import AnalysisConfig, FrameEnum, GeneralConfig\n\nCONFIG_PATH = Path(__file__).resolve().parent / "".."" / ""config""\nDOC_FILE = CONFIG_PATH / ""docs.yaml""\n\n\ndef test_config_default_types():\n    config = AnalysisConfig()\n    assert config.observations.obs_cone.frame is None\n    assert config.observations.obs_cone.lon is None\n    assert config.observations.obs_cone.lat is None\n    assert config.observations.obs_cone.radius is None\n    assert config.observations.obs_time.start is None\n    assert config.observations.obs_time.stop is None\n    assert config.datasets.geom.wcs.skydir.frame is None\n    assert config.datasets.geom.wcs.skydir.lon is None\n    assert config.datasets.geom.wcs.skydir.lat is None\n    assert isinstance(config.datasets.geom.wcs.binsize, Angle)\n    assert isinstance(config.datasets.geom.wcs.binsize_irf, Angle)\n    assert isinstance(config.datasets.geom.axes.energy.min, Quantity)\n    assert isinstance(config.datasets.geom.axes.energy.max, Quantity)\n    assert isinstance(config.datasets.geom.axes.energy_true.min, Quantity)\n    assert isinstance(config.datasets.geom.axes.energy_true.max, Quantity)\n    assert isinstance(config.datasets.geom.selection.offset_max, Angle)\n    assert isinstance(config.fit.fit_range.min, Quantity)\n    assert isinstance(config.fit.fit_range.max, Quantity)\n\n\ndef test_config_not_default_types():\n    config = AnalysisConfig()\n    config.observations.obs_cone = {\n        ""frame"": ""galactic"",\n        ""lon"": ""83.633 deg"",\n        ""lat"": ""22.014 deg"",\n        ""radius"": ""1 deg"",\n    }\n    assert isinstance(config.observations.obs_cone.frame, FrameEnum)\n    assert isinstance(config.observations.obs_cone.lon, Angle)\n    assert isinstance(config.observations.obs_cone.lat, Angle)\n    assert isinstance(config.observations.obs_cone.radius, Angle)\n    config.observations.obs_time.start = ""2019-12-01""\n    assert isinstance(config.observations.obs_time.start, Time)\n    with pytest.raises(ValueError):\n        config.flux_points.energy.min = ""1 deg""\n\n\ndef test_config_basics():\n    config = AnalysisConfig()\n    assert ""AnalysisConfig"" in str(config)\n    config = AnalysisConfig.read(DOC_FILE)\n    assert config.general.outdir == "".""\n\n\ndef test_config_create_from_dict():\n    data = {""general"": {""log"": {""level"": ""warning""}}}\n    config = AnalysisConfig(**data)\n    assert config.general.log.level == ""warning""\n\n\ndef test_config_create_from_yaml():\n    config = AnalysisConfig.read(DOC_FILE)\n    assert isinstance(config.general, GeneralConfig)\n    config_str = Path(DOC_FILE).read_text()\n    config = AnalysisConfig.from_yaml(config_str)\n    assert isinstance(config.general, GeneralConfig)\n\n\ndef test_config_to_yaml(tmp_path):\n    config = AnalysisConfig()\n    assert ""level: info"" in config.to_yaml()\n    config = AnalysisConfig()\n    fpath = Path(tmp_path) / ""temp.yaml""\n    config.write(fpath)\n    text = Path(fpath).read_text()\n    assert ""stack"" in text\n    with pytest.raises(IOError):\n        config.write(fpath)\n\n\ndef test_get_doc_sections():\n    config = AnalysisConfig()\n    doc = config._get_doc_sections()\n    assert ""general"" in doc.keys()\n\n\ndef test_safe_mask_config_validation():\n    config = AnalysisConfig()\n    # Check empty list is accepted\n    config.datasets.safe_mask.methods = []\n\n    with pytest.raises(ValidationError):\n        config.datasets.safe_mask.methods = [""bad""]\n'"
gammapy/astro/darkmatter/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Dark matter spatial and spectral models.""""""\nfrom .spectra import *\nfrom .utils import *\n'"
gammapy/astro/darkmatter/profiles.py,1,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Dark matter profiles.""""""\nimport abc\nimport numpy as np\nimport astropy.units as u\nfrom gammapy.modeling import Parameter, Parameters\nfrom gammapy.modeling.models.spectral import integrate_spectrum\n\n__all__ = [\n    ""DMProfile"",\n    ""NFWProfile"",\n    ""EinastoProfile"",\n    ""IsothermalProfile"",\n    ""BurkertProfile"",\n    ""MooreProfile"",\n]\n\n\nclass DMProfile(abc.ABC):\n    """"""DMProfile model base class.""""""\n\n    LOCAL_DENSITY = 0.3 * u.GeV / (u.cm ** 3)\n    """"""Local dark matter density as given in refenrece 2""""""\n    DISTANCE_GC = 8.33 * u.kpc\n    """"""Distance to the Galactic Center as given in reference 2""""""\n\n    def __call__(self, radius):\n        """"""Call evaluate method of derived classes.""""""\n        kwargs = {par.name: par.quantity for par in self.parameters}\n        return self.evaluate(radius, **kwargs)\n\n    def scale_to_local_density(self):\n        """"""Scale to local density.""""""\n        scale = (self.LOCAL_DENSITY / self(self.DISTANCE_GC)).to_value("""")\n        self.parameters[""rho_s""].value *= scale\n\n    def _eval_squared(self, radius):\n        """"""Squared density at given radius.""""""\n        return self(radius) ** 2\n\n    def integral(self, rmin, rmax, **kwargs):\n        r""""""Integrate squared dark matter profile numerically.\n\n        .. math::\n            F(r_{min}, r_{max}) = \\int_{r_{min}}^{r_{max}}\\rho(r)^2 dr\n\n        Parameters\n        ----------\n        rmin, rmax : `~astropy.units.Quantity`\n            Lower and upper bound of integration range.\n        **kwargs : dict\n            Keyword arguments passed to :func:`~gammapy.utils.integrate.integrate_spectrum`\n        """"""\n        integral = integrate_spectrum(self._eval_squared, rmin, rmax, **kwargs)\n        return integral.to(""GeV2 / cm5"")\n\n\nclass NFWProfile(DMProfile):\n    r""""""NFW Profile.\n\n    .. math::\n        \\rho(r) = \\rho_s \\frac{r_s}{r}\\left(1 + \\frac{r}{r_s}\\right)^{-2}\n\n    Parameters\n    ----------\n    r_s : `~astropy.units.Quantity`\n        Scale radius, :math:`r_s`\n    rho_s : `~astropy.units.Quantity`\n        Characteristic density, :math:`\\rho_s`\n\n    References\n    ----------\n    * `1997ApJ...490..493 <https://ui.adsabs.harvard.edu/abs/1997ApJ...490..493N>`_\n    * `2011JCAP...03..051 <https://ui.adsabs.harvard.edu/abs/2011JCAP...03..051>`_\n    """"""\n\n    DEFAULT_SCALE_RADIUS = 24.42 * u.kpc\n    """"""Default scale radius as given in reference 2""""""\n\n    def __init__(self, r_s=None, rho_s=1 * u.Unit(""GeV / cm3"")):\n        r_s = self.DEFAULT_SCALE_RADIUS if r_s is None else r_s\n        self.parameters = Parameters(\n            [Parameter(""r_s"", u.Quantity(r_s)), Parameter(""rho_s"", u.Quantity(rho_s))]\n        )\n\n    @staticmethod\n    def evaluate(radius, r_s, rho_s):\n        rr = radius / r_s\n        return rho_s / (rr * (1 + rr) ** 2)\n\n\nclass EinastoProfile(DMProfile):\n    r""""""Einasto Profile.\n\n    .. math::\n        \\rho(r) = \\rho_s \\exp{\n            \\left(-\\frac{2}{\\alpha}\\left[\n            \\left(\\frac{r}{r_s}\\right)^{\\alpha} - 1\\right] \\right)}\n\n    Parameters\n    ----------\n    r_s : `~astropy.units.Quantity`\n        Scale radius, :math:`r_s`\n    alpha : `~astropy.units.Quantity`\n        :math:`\\alpha`\n    rho_s : `~astropy.units.Quantity`\n        Characteristic density, :math:`\\rho_s`\n\n    References\n    ----------\n    * `1965TrAlm...5...87E <https://ui.adsabs.harvard.edu/abs/1965TrAlm...5...87E>`_\n    * `2011JCAP...03..051 <https://ui.adsabs.harvard.edu/abs/2011JCAP...03..051>`_\n    """"""\n\n    DEFAULT_SCALE_RADIUS = 28.44 * u.kpc\n    """"""Default scale radius as given in reference 2""""""\n    DEFAULT_ALPHA = 0.17\n    """"""Default scale radius as given in reference 2""""""\n\n    def __init__(self, r_s=None, alpha=None, rho_s=1 * u.Unit(""GeV / cm3"")):\n        alpha = self.DEFAULT_ALPHA if alpha is None else alpha\n        r_s = self.DEFAULT_SCALE_RADIUS if r_s is None else r_s\n\n        self.parameters = Parameters(\n            [\n                Parameter(""r_s"", u.Quantity(r_s)),\n                Parameter(""alpha"", u.Quantity(alpha)),\n                Parameter(""rho_s"", u.Quantity(rho_s)),\n            ]\n        )\n\n    @staticmethod\n    def evaluate(radius, r_s, alpha, rho_s):\n        rr = radius / r_s\n        exponent = (2 / alpha) * (rr ** alpha - 1)\n        return rho_s * np.exp(-1 * exponent)\n\n\nclass IsothermalProfile(DMProfile):\n    r""""""Isothermal Profile.\n\n    .. math:: \\rho(r) = \\frac{\\rho_s}{1 + (r/r_s)^2}\n\n    Parameters\n    ----------\n    r_s : `~astropy.units.Quantity`\n        Scale radius, :math:`r_s`\n\n    References\n    ----------\n    * `1991MNRAS.249..523B <https://ui.adsabs.harvard.edu/abs/1991MNRAS.249..523B>`_\n    * `2011JCAP...03..051 <https://ui.adsabs.harvard.edu/abs/2011JCAP...03..051>`_\n    """"""\n\n    DEFAULT_SCALE_RADIUS = 4.38 * u.kpc\n    """"""Default scale radius as given in reference 2""""""\n\n    def __init__(self, r_s=None, rho_s=1 * u.Unit(""GeV / cm3"")):\n        r_s = self.DEFAULT_SCALE_RADIUS if r_s is None else r_s\n\n        self.parameters = Parameters(\n            [Parameter(""r_s"", u.Quantity(r_s)), Parameter(""rho_s"", u.Quantity(rho_s))]\n        )\n\n    @staticmethod\n    def evaluate(radius, r_s, rho_s):\n        rr = radius / r_s\n        return rho_s / (1 + rr ** 2)\n\n\nclass BurkertProfile(DMProfile):\n    r""""""Burkert Profile.\n\n    .. math:: \\rho(r) = \\frac{\\rho_s}{(1 + r/r_s)(1 + (r/r_s)^2)}\n\n    Parameters\n    ----------\n    r_s : `~astropy.units.Quantity`\n        Scale radius, :math:`r_s`\n\n    References\n    ----------\n    * `1995ApJ...447L..25B <https://ui.adsabs.harvard.edu/abs/1995ApJ...447L..25B>`_\n    * `2011JCAP...03..051 <https://ui.adsabs.harvard.edu/abs/2011JCAP...03..051>`_\n    """"""\n\n    DEFAULT_SCALE_RADIUS = 12.67 * u.kpc\n    """"""Default scale radius as given in reference 2""""""\n\n    def __init__(self, r_s=None, rho_s=1 * u.Unit(""GeV / cm3"")):\n        r_s = self.DEFAULT_SCALE_RADIUS if r_s is None else r_s\n\n        self.parameters = Parameters(\n            [Parameter(""r_s"", u.Quantity(r_s)), Parameter(""rho_s"", u.Quantity(rho_s))]\n        )\n\n    @staticmethod\n    def evaluate(radius, r_s, rho_s):\n        rr = radius / r_s\n        return rho_s / ((1 + rr) * (1 + rr ** 2))\n\n\nclass MooreProfile(DMProfile):\n    r""""""Moore Profile.\n\n    .. math::\n        \\rho(r) = \\rho_s \\left(\\frac{r_s}{r}\\right)^{1.16}\n        \\left(1 + \\frac{r}{r_s} \\right)^{-1.84}\n\n    Parameters\n    ----------\n    r_s : `~astropy.units.Quantity`\n        Scale radius, :math:`r_s`\n\n    References\n    ----------\n    * `2004MNRAS.353..624D <https://ui.adsabs.harvard.edu/abs/2004MNRAS.353..624D>`_\n    * `2011JCAP...03..051 <https://ui.adsabs.harvard.edu/abs/2011JCAP...03..051>`_\n    """"""\n\n    DEFAULT_SCALE_RADIUS = 30.28 * u.kpc\n    """"""Default scale radius as given in reference 2""""""\n\n    def __init__(self, r_s=None, rho_s=1 * u.Unit(""GeV / cm3"")):\n        r_s = self.DEFAULT_SCALE_RADIUS if r_s is None else r_s\n\n        self.parameters = Parameters(\n            [Parameter(""r_s"", u.Quantity(r_s)), Parameter(""rho_s"", u.Quantity(rho_s))]\n        )\n\n    @staticmethod\n    def evaluate(radius, r_s, rho_s):\n        rr = radius / r_s\n        rr_ = r_s / radius\n        return rho_s * rr_ ** 1.16 * (1 + rr) ** (-1.84)\n'"
gammapy/astro/darkmatter/spectra.py,4,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Dark matter spectra.""""""\nimport numpy as np\nimport astropy.units as u\nfrom astropy.table import Table\nfrom gammapy.modeling import Parameter\nfrom gammapy.modeling.models import SpectralModel, TemplateSpectralModel\nfrom gammapy.utils.interpolation import LogScale\nfrom gammapy.utils.scripts import make_path\n\n__all__ = [""PrimaryFlux"", ""DarkMatterAnnihilationSpectralModel""]\n\n\nclass PrimaryFlux:\n    """"""DM-annihilation gamma-ray spectra.\n\n    Based on the precomputed models by Cirelli et al. (2016). All available\n    annihilation channels can be found there. The dark matter mass will be set\n    to the nearest available value. The spectra will be available as\n    `~gammapy.modeling.models.TemplateSpectralModel` for a chosen dark matter mass and\n    annihilation channel.\n\n    References\n    ----------\n    * `2011JCAP...03..051 <https://ui.adsabs.harvard.edu/abs/2011JCAP...03..051>`_\n    * Cirelli et al (2016): http://www.marcocirelli.net/PPPC4DMID.html\n    """"""\n\n    channel_registry = {\n        ""eL"": ""eL"",\n        ""eR"": ""eR"",\n        ""e"": ""e"",\n        ""muL"": r""\\[Mu]L"",\n        ""muR"": r""\\[Mu]R"",\n        ""mu"": r""\\[Mu]"",\n        ""tauL"": r""\\[Tau]L"",\n        ""tauR"": r""\\[Tau]R"",\n        ""tau"": r""\\[Tau]"",\n        ""q"": ""q"",\n        ""c"": ""c"",\n        ""b"": ""b"",\n        ""t"": ""t"",\n        ""WL"": ""WL"",\n        ""WT"": ""WT"",\n        ""W"": ""W"",\n        ""ZL"": ""ZL"",\n        ""ZT"": ""ZT"",\n        ""Z"": ""Z"",\n        ""g"": ""g"",\n        ""gamma"": r""\\[Gamma]"",\n        ""h"": ""h"",\n        ""nu_e"": r""\\[Nu]e"",\n        ""nu_mu"": r""\\[Nu]\\[Mu]"",\n        ""nu_tau"": r""\\[Nu]\\[Tau]"",\n        ""V->e"": ""V->e"",\n        ""V->mu"": r""V->\\[Mu]"",\n        ""V->tau"": r""V->\\[Tau]"",\n    }\n\n    table_filename = ""$GAMMAPY_DATA/dark_matter_spectra/AtProduction_gammas.dat""\n\n    def __init__(self, mDM, channel):\n\n        self.table_path = make_path(self.table_filename)\n        if not self.table_path.exists():\n            raise FileNotFoundError(\n                f""\\n\\nFile not found: {self.table_filename}\\n""\n                ""You may download the dataset needed with the following command:\\n""\n                ""gammapy download datasets --src dark_matter_spectra""\n            )\n        else:\n            self.table = Table.read(\n                str(self.table_path),\n                format=""ascii.fast_basic"",\n                guess=False,\n                delimiter="" "",\n            )\n\n        self.mDM = mDM\n        self.channel = channel\n\n    @property\n    def mDM(self):\n        """"""Dark matter mass.""""""\n        return self._mDM\n\n    @mDM.setter\n    def mDM(self, mDM):\n        mDM_vals = self.table[""mDM""].data\n        mDM_ = u.Quantity(mDM).to_value(""GeV"")\n        interp_idx = np.argmin(np.abs(mDM_vals - mDM_))\n        self._mDM = u.Quantity(mDM_vals[interp_idx], ""GeV"")\n\n    @property\n    def allowed_channels(self):\n        """"""List of allowed annihilation channels.""""""\n        return list(self.channel_registry.keys())\n\n    @property\n    def channel(self):\n        """"""Annihilation channel (str).""""""\n        return self._channel\n\n    @channel.setter\n    def channel(self, channel):\n        if channel not in self.allowed_channels:\n            raise ValueError(\n                f""Invalid channel: {channel}\\nAvailable: {self.allowed_channels}\\n""\n            )\n        else:\n            self._channel = channel\n\n    @property\n    def table_model(self):\n        """"""Spectrum as `~gammapy.modeling.models.TemplateSpectralModel`.""""""\n        subtable = self.table[self.table[""mDM""] == self.mDM.value]\n        energies = (10 ** subtable[""Log[10,x]""]) * self.mDM\n        channel_name = self.channel_registry[self.channel]\n        dN_dlogx = subtable[channel_name]\n        dN_dE = dN_dlogx / (energies * np.log(10))\n        return TemplateSpectralModel(\n            energy=energies,\n            values=dN_dE,\n            interp_kwargs={""fill_value"": np.log(LogScale.tiny)},\n        )\n\n\nclass DarkMatterAnnihilationSpectralModel(SpectralModel):\n    r""""""Dark matter annihilation spectral model.\n\n    The gamma-ray flux is computed as follows:\n\n    .. math::\n        \\frac{\\mathrm d \\phi}{\\mathrm d E} =\n        \\frac{\\langle \\sigma\\nu \\rangle}{4\\pi k m^2_{\\mathrm{DM}}}\n        \\frac{\\mathrm d N}{\\mathrm dE} \\times J(\\Delta\\Omega)\n\n    Parameters\n    ----------\n    mass : `~astropy.units.Quantity`\n        Dark matter mass\n    channel : str\n        Annihilation channel for `~gammapy.astro.darkmatter.PrimaryFlux`\n    scale : float\n        Scale parameter for model fitting\n    jfactor : `~astropy.units.Quantity`\n        Integrated J-Factor needed when `~gammapy.modeling.models.PointSpatialModel` spatial model is used\n    z: float\n        Redshift value\n    k: int\n        Type of dark matter particle (k:2 Majorana, k:4 Dirac)\n\n    Examples\n    --------\n    This is how to instantiate a `DarkMatterAnnihilationSpectralModel` model::\n\n        from astropy import units as u\n        from gammapy.astro.darkmatter import DarkMatterAnnihilationSpectralModel\n\n        channel = ""b""\n        massDM = 5000*u.Unit(""GeV"")\n        jfactor = 3.41e19 * u.Unit(""GeV2 cm-5"")\n        modelDM = DarkMatterAnnihilationSpectralModel(mass=massDM, channel=channel, jfactor=jfactor)\n\n    References\n    ----------\n    * `2011JCAP...03..051 <https://ui.adsabs.harvard.edu/abs/2011JCAP...03..051>`_\n    """"""\n\n    THERMAL_RELIC_CROSS_SECTION = 3e-26 * u.Unit(""cm3 s-1"")\n    """"""Thermally averaged annihilation cross-section""""""\n\n    scale = Parameter(""scale"", 1)\n\n    def __init__(self, mass, channel, scale=scale.quantity, jfactor=1, z=0, k=2):\n        self.k = k\n        self.z = z\n        self.mass = mass\n        self.channel = channel\n        self.jfactor = jfactor\n        self.primary_flux = PrimaryFlux(mass, channel=self.channel).table_model\n        super().__init__(scale=scale)\n\n    def evaluate(self, energy, scale):\n        """"""Evaluate dark matter annihilation model.""""""\n        flux = (\n            scale\n            * self.jfactor\n            * self.THERMAL_RELIC_CROSS_SECTION\n            * self.primary_flux(energy=energy * (1 + self.z))\n            / self.k\n            / self.mass\n            / self.mass\n            / (4 * np.pi)\n        )\n        return flux\n'"
gammapy/astro/darkmatter/utils.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Utilities to compute J-factor maps.""""""\nimport astropy.units as u\n\n__all__ = [""JFactory""]\n\n\nclass JFactory:\n    """"""Compute J-Factor maps.\n\n    All J-Factors are computed for annihilation. The assumed dark matter\n    profiles will be centered on the center of the map.\n\n    Parameters\n    ----------\n    geom : `~gammapy.maps.WcsGeom`\n        Reference geometry\n    profile : `~gammapy.astro.darkmatter.profiles.DMProfile`\n        Dark matter profile\n    distance : `~astropy.units.Quantity`\n        Distance to convert angular scale of the map\n    """"""\n\n    def __init__(self, geom, profile, distance):\n        self.geom = geom\n        self.profile = profile\n        self.distance = distance\n\n    def compute_differential_jfactor(self):\n        r""""""Compute differential J-Factor.\n\n        .. math::\n            \\frac{\\mathrm d J}{\\mathrm d \\Omega} =\n            \\int_{\\mathrm{LoS}} \\mathrm d r \\rho(r)\n        """"""\n        # TODO: Needs to be implemented more efficiently\n        separation = self.geom.separation(self.geom.center_skydir)\n        rmin = separation.rad * self.distance\n        rmax = self.distance\n        val = [self.profile.integral(_, rmax) for _ in rmin.flatten()]\n        jfact = u.Quantity(val).to(""GeV2 cm-5"").reshape(rmin.shape)\n        return jfact / u.steradian\n\n    def compute_jfactor(self):\n        r""""""Compute astrophysical J-Factor.\n\n        .. math::\n            J(\\Delta\\Omega) =\n           \\int_{\\Delta\\Omega} \\mathrm d \\Omega^{\\prime}\n           \\frac{\\mathrm d J}{\\mathrm d \\Omega^{\\prime}}\n        """"""\n        diff_jfact = self.compute_differential_jfactor()\n        return diff_jfact * self.geom.to_image().solid_angle()\n'"
gammapy/astro/population/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Astrophysical population models.""""""\nfrom .simulate import *\nfrom .spatial import *\nfrom .velocity import *\n'"
gammapy/astro/population/simulate.py,7,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Simulate source catalogs.""""""\nimport numpy as np\nfrom astropy.coordinates import SkyCoord\nfrom astropy.table import Column, Table\nfrom astropy.units import Quantity\nfrom gammapy.astro.source import PWN, SNR, Pulsar, SNRTrueloveMcKee\nfrom gammapy.utils import coordinates as astrometry\nfrom gammapy.utils.random import (\n    draw,\n    get_random_state,\n    pdf,\n    sample_sphere,\n    sample_sphere_distance,\n)\nfrom .spatial import (\n    RMAX,\n    RMIN,\n    ZMAX,\n    ZMIN,\n    Exponential,\n    FaucherSpiral,\n    radial_distributions,\n)\nfrom .velocity import VMAX, VMIN, velocity_distributions\n\n__all__ = [\n    ""make_catalog_random_positions_cube"",\n    ""make_catalog_random_positions_sphere"",\n    ""make_base_catalog_galactic"",\n    ""add_snr_parameters"",\n    ""add_pulsar_parameters"",\n    ""add_pwn_parameters"",\n    ""add_observed_parameters"",\n]\n\n\ndef make_catalog_random_positions_cube(\n    size=100, dimension=3, distance_max=""1 pc"", random_state=""random-seed""\n):\n    """"""Make a catalog of sources randomly distributed on a line, square or cube.\n\n    This can be used to study basic source population distribution effects,\n    e.g. what the distance distribution looks like, or for a given luminosity\n    function what the resulting flux distributions are for different spatial\n    configurations.\n\n    Parameters\n    ----------\n    size : int\n        Number of sources\n    dimension : {1, 2, 3}\n        Number of dimensions\n    distance_max : `~astropy.units.Quantity`\n        Maximum distance\n    random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n        Defines random number generator initialisation.\n        Passed to `~gammapy.utils.random.get_random_state`.\n\n    Returns\n    -------\n    table : `~astropy.table.Table`\n        Table with 3D position cartesian coordinates.\n        Columns: x (pc), y (pc), z (pc)\n    """"""\n    distance_max = Quantity(distance_max).to_value(""pc"")\n    random_state = get_random_state(random_state)\n\n    # Generate positions 1D, 2D, or 3D\n    if dimension == 1:\n        x = random_state.uniform(-distance_max, distance_max, size)\n        y, z = 0, 0\n    elif dimension == 2:\n        x = random_state.uniform(-distance_max, distance_max, size)\n        y = random_state.uniform(-distance_max, distance_max, size)\n        z = 0\n    elif dimension == 3:\n        x = random_state.uniform(-distance_max, distance_max, size)\n        y = random_state.uniform(-distance_max, distance_max, size)\n        z = random_state.uniform(-distance_max, distance_max, size)\n    else:\n        raise ValueError(f""Invalid dimension: {dimension}"")\n\n    table = Table()\n    table[""x""] = Column(x, unit=""pc"", description=""Cartesian coordinate"")\n    table[""y""] = Column(y, unit=""pc"", description=""Cartesian coordinate"")\n    table[""z""] = Column(z, unit=""pc"", description=""Cartesian coordinate"")\n\n    return table\n\n\ndef make_catalog_random_positions_sphere(\n    size=100, distance_min=""0 pc"", distance_max=""1 pc"", random_state=""random-seed""\n):\n    """"""Sample random source locations in a sphere.\n\n    This can be used to generate an isotropic source population\n    in a sphere, e.g. to represent extra-galactic sources.\n\n    Parameters\n    ----------\n    size : int\n        Number of sources\n    distance_min, distance_max : `~astropy.units.Quantity`\n        Minimum and maximum distance\n    random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n        Defines random number generator initialisation.\n        Passed to `~gammapy.utils.random.get_random_state`.\n\n    Returns\n    -------\n    catalog : `~astropy.table.Table`\n        Table with 3D position spherical coordinates.\n        Columns: lon (deg), lat (deg), distance(pc)\n    """"""\n    distance_min = Quantity(distance_min).to_value(""pc"")\n    distance_max = Quantity(distance_max).to_value(""pc"")\n    random_state = get_random_state(random_state)\n\n    lon, lat = sample_sphere(size, random_state=random_state)\n    distance = sample_sphere_distance(distance_min, distance_max, size, random_state)\n\n    table = Table()\n\n    table[""lon""] = Column(lon, unit=""rad"", description=""Spherical coordinate"")\n    table[""lat""] = Column(lat, unit=""rad"", description=""Spherical coordinate"")\n    table[""distance""] = Column(distance, unit=""pc"", description=""Spherical coordinate"")\n\n    return table\n\n\ndef make_base_catalog_galactic(\n    n_sources,\n    rad_dis=""YK04"",\n    vel_dis=""H05"",\n    max_age=""1e6 yr"",\n    spiralarms=True,\n    n_ISM=""1 cm-3"",\n    random_state=""random-seed"",\n):\n    """"""Make a catalog of Galactic sources, with basic source parameters.\n\n    Choose a radial distribution, a velocity distribution, the number\n    of pulsars n_pulsars, the maximal age max_age[years] and the fraction\n    of the individual morphtypes. There\'s an option spiralarms. If set on\n    True a spiralarm modeling after Faucher&Kaspi is included.\n\n    max_age and n_sources effectively correspond to s SN rate:\n    SN_rate = n_sources / max_age\n\n    Parameters\n    ----------\n    n_sources : int\n        Number of sources to simulate\n    rad_dis : callable\n        Radial surface density distribution of sources\n    vel_dis : callable\n        Proper motion velocity distribution of sources\n    max_age : `~astropy.units.Quantity`\n        Maximal age of the source\n    spiralarms : bool\n        Include a spiralarm model in the catalog?\n    n_ISM : `~astropy.units.Quantity`\n        Density of the interstellar medium\n    random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n        Defines random number generator initialisation.\n        Passed to `~gammapy.utils.random.get_random_state`.\n\n    Returns\n    -------\n    table : `~astropy.table.Table`\n        Catalog of simulated source positions and proper velocities\n    """"""\n    max_age = Quantity(max_age).to_value(""yr"")\n    n_ISM = Quantity(n_ISM).to(""cm-3"")\n    random_state = get_random_state(random_state)\n\n    if isinstance(rad_dis, str):\n        rad_dis = radial_distributions[rad_dis]\n\n    if isinstance(vel_dis, str):\n        vel_dis = velocity_distributions[vel_dis]\n\n    # Draw random values for the age\n    age = random_state.uniform(0, max_age, n_sources)\n    age = Quantity(age, ""yr"")\n\n    # Draw spatial distribution\n    r = draw(\n        RMIN.to_value(""kpc""),\n        RMAX.to_value(""kpc""),\n        n_sources,\n        pdf(rad_dis()),\n        random_state=random_state,\n    )\n    r = Quantity(r, ""kpc"")\n\n    if spiralarms:\n        r, theta, spiralarm = FaucherSpiral()(r, random_state=random_state)\n    else:\n        theta = Quantity(random_state.uniform(0, 2 * np.pi, n_sources), ""rad"")\n        spiralarm = None\n\n    x, y = astrometry.cartesian(r, theta)\n\n    z = draw(\n        ZMIN.to_value(""kpc""),\n        ZMAX.to_value(""kpc""),\n        n_sources,\n        Exponential(),\n        random_state=random_state,\n    )\n    z = Quantity(z, ""kpc"")\n\n    # Draw values from velocity distribution\n    v = draw(\n        VMIN.to_value(""km/s""),\n        VMAX.to_value(""km/s""),\n        n_sources,\n        vel_dis(),\n        random_state=random_state,\n    )\n    v = Quantity(v, ""km/s"")\n\n    # Draw random direction of initial velocity\n    theta = Quantity(random_state.uniform(0, np.pi, x.size), ""rad"")\n    phi = Quantity(random_state.uniform(0, 2 * np.pi, x.size), ""rad"")\n\n    # Compute new position\n    dx, dy, dz, vx, vy, vz = astrometry.motion_since_birth(v, age, theta, phi)\n\n    # Add displacement to birth position\n    x_moved = x + dx\n    y_moved = y + dy\n    z_moved = z + dz\n\n    table = Table()\n    table[""age""] = Column(age, unit=""yr"", description=""Age of the source"")\n    table[""n_ISM""] = Column(n_ISM, description=""Interstellar medium density"")\n    if spiralarms:\n        table[""spiralarm""] = Column(spiralarm, description=""Which spiralarm?"")\n\n    table[""x_birth""] = Column(x, description=""Galactocentric x coordinate at birth"")\n    table[""y_birth""] = Column(y, description=""Galactocentric y coordinate at birth"")\n    table[""z_birth""] = Column(z, description=""Galactocentric z coordinate at birth"")\n\n    table[""x""] = Column(x_moved.to(""kpc""), description=""Galactocentric x coordinate"")\n    table[""y""] = Column(y_moved.to(""kpc""), description=""Galactocentric y coordinate"")\n    table[""z""] = Column(z_moved.to(""kpc""), description=""Galactocentric z coordinate"")\n\n    table[""vx""] = Column(vx, description=""Galactocentric velocity in x direction"")\n    table[""vy""] = Column(vy, description=""Galactocentric velocity in y direction"")\n    table[""vz""] = Column(vz, description=""Galactocentric velocity in z direction"")\n    table[""v_abs""] = Column(v, description=""Galactocentric velocity (absolute)"")\n\n    return table\n\n\ndef add_snr_parameters(table):\n    """"""Add SNR parameters to the table.\n\n    TODO: document\n    """"""\n    # Read relevant columns\n    age = table[""age""].quantity\n    n_ISM = table[""n_ISM""].quantity\n\n    # Compute properties\n    snr = SNR(n_ISM=n_ISM)\n    E_SN = snr.e_sn * np.ones(len(table))\n    r_out = snr.radius(age)\n    r_in = snr.radius_inner(age)\n    L_SNR = snr.luminosity_tev(age)\n\n    # Add columns to table\n    table[""E_SN""] = Column(E_SN, description=""SNR kinetic energy"")\n    table[""r_out""] = Column(r_out.to(""pc""), description=""SNR outer radius"")\n    table[""r_in""] = Column(r_in.to(""pc""), description=""SNR inner radius"")\n    table[""L_SNR""] = Column(L_SNR, description=""SNR photon rate above 1 TeV"")\n    return table\n\n\ndef add_pulsar_parameters(\n    table,\n    B_mean=12.05,\n    B_stdv=0.55,\n    P_mean=0.3,\n    P_stdv=0.15,\n    random_state=""random-seed"",\n):\n    """"""Add pulsar parameters to the table.\n\n    For the initial normal distribution of period and logB can exist the following\n    Parameters: B_mean=12.05[log Gauss], B_stdv=0.55, P_mean=0.3[s], P_stdv=0.15\n\n    Parameters\n    ----------\n    random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n        Defines random number generator initialisation.\n        Passed to `~gammapy.utils.random.get_random_state`.\n    """"""\n    random_state = get_random_state(random_state)\n    # Read relevant columns\n    age = table[""age""].quantity\n\n    # Draw the initial values for the period and magnetic field\n    def p_dist(x):\n        return np.exp(-0.5 * ((x - P_mean) / P_stdv) ** 2)\n\n    p0_birth = draw(0, 2, len(table), p_dist, random_state=random_state)\n    p0_birth = Quantity(p0_birth, ""s"")\n\n    log10_b_psr = random_state.normal(B_mean, B_stdv, len(table))\n    b_psr = Quantity(10 ** log10_b_psr, ""G"")\n\n    # Compute pulsar parameters\n    psr = Pulsar(p0_birth, b_psr)\n    p0 = psr.period(age)\n    p1 = psr.period_dot(age)\n    p1_birth = psr.P_dot_0\n    tau = psr.tau(age)\n    tau_0 = psr.tau_0\n    l_psr = psr.luminosity_spindown(age)\n    l0_psr = psr.L_0\n\n    # Add columns to table\n    table[""P0""] = Column(p0, unit=""s"", description=""Pulsar period"")\n    table[""P1""] = Column(p1, unit="""", description=""Pulsar period derivative"")\n    table[""P0_birth""] = Column(p0_birth, unit=""s"", description=""Pulsar birth period"")\n    table[""P1_birth""] = Column(\n        p1_birth, unit="""", description=""Pulsar birth period derivative""\n    )\n    table[""CharAge""] = Column(tau, unit=""yr"", description=""Pulsar characteristic age"")\n    table[""Tau0""] = Column(tau_0, unit=""yr"")\n    table[""L_PSR""] = Column(l_psr, unit=""erg s-1"")\n    table[""L0_PSR""] = Column(l0_psr, unit=""erg s-1"")\n    table[""B_PSR""] = Column(\n        b_psr, unit=""Gauss"", description=""Pulsar magnetic field at the poles""\n    )\n    return table\n\n\ndef add_pwn_parameters(table):\n    """"""Add PWN parameters to the table.\n\n    TODO: document\n    """"""\n    # Some of the computations (specifically `pwn.radius`) aren\'t vectorised\n    # across all parameters; so here we loop over source parameters explicitly\n\n    results = []\n\n    for idx in range(len(table)):\n        age = table[""age""].quantity[idx]\n        E_SN = table[""E_SN""].quantity[idx]\n        n_ISM = table[""n_ISM""].quantity[idx]\n        P0_birth = table[""P0_birth""].quantity[idx]\n        b_psr = table[""B_PSR""].quantity[idx]\n\n        # Compute properties\n        pulsar = Pulsar(P0_birth, b_psr)\n        snr = SNRTrueloveMcKee(e_sn=E_SN, n_ISM=n_ISM)\n        pwn = PWN(pulsar, snr)\n        r_out_pwn = pwn.radius(age).to_value(""pc"")\n        results.append(dict(r_out_pwn=r_out_pwn))\n\n    # Add columns to table\n    table[""r_out_PWN""] = Column(\n        [_[""r_out_pwn""] for _ in results], unit=""pc"", description=""PWN outer radius""\n    )\n    return table\n\n\ndef add_observed_parameters(table, obs_pos=None):\n    """"""Add observable parameters (such as sky position or distance).\n\n    Input table columns: x, y, z, extension, luminosity\n\n    Output table columns: distance, glon, glat, flux, angular_extension\n\n    Position of observer in cartesian coordinates.\n    Center of galaxy as origin, x-axis goes trough sun.\n\n    Parameters\n    ----------\n    table : `~astropy.table.Table`\n        Input table\n    obs_pos : tuple or None\n        Observation position (X, Y, Z) in Galactocentric coordinates (default: Earth)\n\n    Returns\n    -------\n    table : `~astropy.table.Table`\n        Modified input table with columns added\n    """"""\n    obs_pos = obs_pos or [astrometry.D_SUN_TO_GALACTIC_CENTER, 0, 0]\n\n    # Get data\n    x, y, z = table[""x""].quantity, table[""y""].quantity, table[""z""].quantity\n    vx, vy, vz = table[""vx""].quantity, table[""vy""].quantity, table[""vz""].quantity\n\n    distance, glon, glat = astrometry.galactic(x, y, z, obs_pos=obs_pos)\n\n    # Compute projected velocity\n    v_glon, v_glat = astrometry.velocity_glon_glat(x, y, z, vx, vy, vz)\n\n    coordinate = SkyCoord(glon, glat, unit=""deg"", frame=""galactic"").transform_to(""icrs"")\n    ra, dec = coordinate.ra.deg, coordinate.dec.deg\n\n    # Add columns to table\n    table[""distance""] = Column(\n        distance, unit=""pc"", description=""Distance observer to source center""\n    )\n    table[""GLON""] = Column(glon, unit=""deg"", description=""Galactic longitude"")\n    table[""GLAT""] = Column(glat, unit=""deg"", description=""Galactic latitude"")\n    table[""VGLON""] = Column(\n        v_glon.to(""deg/Myr""),\n        unit=""deg/Myr"",\n        description=""Velocity in Galactic longitude"",\n    )\n    table[""VGLAT""] = Column(\n        v_glat.to(""deg/Myr""),\n        unit=""deg/Myr"",\n        description=""Velocity in Galactic latitude"",\n    )\n    table[""RA""] = Column(ra, unit=""deg"", description=""Right ascension"")\n    table[""DEC""] = Column(dec, unit=""deg"", description=""Declination"")\n\n    try:\n        luminosity = table[""luminosity""]\n        flux = luminosity / (4 * np.pi * distance ** 2)\n        table[""flux""] = Column(flux.value, unit=flux.unit, description=""Source flux"")\n    except KeyError:\n        pass\n\n    try:\n        extension = table[""extension""]\n        angular_extension = np.degrees(np.arctan(extension / distance))\n        table[""angular_extension""] = Column(\n            angular_extension,\n            unit=""deg"",\n            description=""Source angular radius (i.e. half-diameter)"",\n        )\n    except KeyError:\n        pass\n\n    return table\n'"
gammapy/astro/population/spatial.py,24,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Galactic radial source distribution probability density functions.""""""\nimport numpy as np\nfrom astropy.modeling import Fittable1DModel, Parameter\nfrom astropy.units import Quantity\nfrom gammapy.utils.coordinates import D_SUN_TO_GALACTIC_CENTER, cartesian, polar\nfrom gammapy.utils.random import get_random_state\n\n__all__ = [\n    ""CaseBattacharya1998"",\n    ""FaucherKaspi2006"",\n    ""Lorimer2006"",\n    ""Paczynski1990"",\n    ""YusifovKucuk2004"",\n    ""YusifovKucuk2004B"",\n    ""Exponential"",\n    ""LogSpiral"",\n    ""FaucherSpiral"",\n    ""ValleeSpiral"",\n    ""radial_distributions"",\n]\n\n# Simulation range used for random number drawing\nRMIN, RMAX = Quantity([0, 20], ""kpc"")\nZMIN, ZMAX = Quantity([-0.5, 0.5], ""kpc"")\n\n\nclass Paczynski1990(Fittable1DModel):\n    r""""""Radial distribution of the birth surface density of neutron stars - Paczynski 1990.\n\n    .. math::\n        f(r) = A r_{exp}^{-2} \\exp \\left(-\\frac{r}{r_{exp}} \\right)\n\n    Reference: https://ui.adsabs.harvard.edu/abs/1990ApJ...348..485P (Formula (2))\n\n    Parameters\n    ----------\n    amplitude : float\n        See formula\n    r_exp : float\n        See formula\n\n    See Also\n    --------\n    CaseBattacharya1998, YusifovKucuk2004, Lorimer2006, YusifovKucuk2004B,\n    FaucherKaspi2006, Exponential\n    """"""\n\n    amplitude = Parameter()\n    r_exp = Parameter()\n    evolved = False\n\n    def __init__(self, amplitude=1, r_exp=4.5, **kwargs):\n        super().__init__(amplitude=amplitude, r_exp=r_exp, **kwargs)\n\n    @staticmethod\n    def evaluate(r, amplitude, r_exp):\n        """"""Evaluate model.""""""\n        return amplitude * r_exp ** -2 * np.exp(-r / r_exp)\n\n\nclass CaseBattacharya1998(Fittable1DModel):\n    r""""""Radial distribution of the surface density of supernova remnants in the galaxy - Case & Battacharya 1998.\n\n    .. math::\n        f(r) = A \\left( \\frac{r}{r_{\\odot}} \\right) ^ \\alpha \\exp\n        \\left[ -\\beta \\left( \\frac{ r - r_{\\odot}}{r_{\\odot}} \\right) \\right]\n\n    Reference: https://ui.adsabs.harvard.edu/abs/1998ApJ...504..761C (Formula (14))\n\n    Parameters\n    ----------\n    amplitude : float\n        See model formula\n    alpha : float\n        See model formula\n    beta : float\n        See model formula\n\n    See Also\n    --------\n    Paczynski1990, YusifovKucuk2004, Lorimer2006, YusifovKucuk2004B,\n    FaucherKaspi2006, Exponential\n    """"""\n\n    amplitude = Parameter()\n    alpha = Parameter()\n    beta = Parameter()\n    evolved = True\n\n    def __init__(self, amplitude=1.0, alpha=2, beta=3.53, **kwargs):\n        super().__init__(amplitude=amplitude, alpha=alpha, beta=beta, **kwargs)\n\n    @staticmethod\n    def evaluate(r, amplitude, alpha, beta):\n        """"""Evaluate model.""""""\n        d_sun = D_SUN_TO_GALACTIC_CENTER.value\n        term1 = (r / d_sun) ** alpha\n        term2 = np.exp(-beta * (r - d_sun) / d_sun)\n        return amplitude * term1 * term2\n\n\nclass YusifovKucuk2004(Fittable1DModel):\n    r""""""Radial distribution of the surface density of pulsars in the galaxy - Yusifov & Kucuk 2004.\n\n    .. math::\n        f(r) = A \\left ( \\frac{r + r_1}{r_{\\odot} + r_1} \\right )^a \\exp\n        \\left [-b \\left( \\frac{r - r_{\\odot}}{r_{\\odot} + r_1} \\right ) \\right ]\n\n    Used by Faucher-Guigere and Kaspi. Density at ``r = 0`` is nonzero.\n\n    Reference: https://ui.adsabs.harvard.edu/abs/2004A%26A...422..545Y (Formula (15))\n\n    Parameters\n    ----------\n    amplitude : float\n        See model formula\n    a : float\n        See model formula\n    b : float\n        See model formula\n    r_1 : float\n        See model formula\n\n    See Also\n    --------\n    CaseBattacharya1998, Paczynski1990, Lorimer2006, YusifovKucuk2004B,\n    FaucherKaspi2006, Exponential\n    """"""\n\n    amplitude = Parameter()\n    a = Parameter()\n    b = Parameter()\n    r_1 = Parameter()\n    evolved = True\n\n    def __init__(self, amplitude=1, a=1.64, b=4.01, r_1=0.55, **kwargs):\n        super().__init__(amplitude=amplitude, a=a, b=b, r_1=r_1, **kwargs)\n\n    @staticmethod\n    def evaluate(r, amplitude, a, b, r_1):\n        """"""Evaluate model.""""""\n        d_sun = D_SUN_TO_GALACTIC_CENTER.value\n        term1 = ((r + r_1) / (d_sun + r_1)) ** a\n        term2 = np.exp(-b * (r - d_sun) / (d_sun + r_1))\n        return amplitude * term1 * term2\n\n\nclass YusifovKucuk2004B(Fittable1DModel):\n    r""""""Radial distribution of the surface density of OB stars in the galaxy - Yusifov & Kucuk 2004.\n\n    .. math::\n        f(r) = A \\left( \\frac{r}{r_{\\odot}} \\right) ^ a\n        \\exp \\left[ -b \\left( \\frac{r}{r_{\\odot}} \\right) \\right]\n\n    Derived empirically from OB-stars distribution.\n\n    Reference: https://ui.adsabs.harvard.edu/abs/2004A%26A...422..545Y (Formula (17))\n\n    Parameters\n    ----------\n    amplitude : float\n        See model formula\n    a : float\n        See model formula\n    b : float\n        See model formula\n\n    See Also\n    --------\n    CaseBattacharya1998, Paczynski1990, YusifovKucuk2004, Lorimer2006,\n    FaucherKaspi2006, Exponential\n    """"""\n\n    amplitude = Parameter()\n    a = Parameter()\n    b = Parameter()\n    evolved = False\n\n    def __init__(self, amplitude=1, a=4, b=6.8, **kwargs):\n        super().__init__(amplitude=amplitude, a=a, b=b, **kwargs)\n\n    @staticmethod\n    def evaluate(r, amplitude, a, b):\n        """"""Evaluate model.""""""\n        d_sun = D_SUN_TO_GALACTIC_CENTER.value\n        return amplitude * (r / d_sun) ** a * np.exp(-b * (r / d_sun))\n\n\nclass FaucherKaspi2006(Fittable1DModel):\n    r""""""Radial distribution of the birth surface density of pulsars in the galaxy - Faucher-Giguere & Kaspi 2006.\n\n    .. math::\n        f(r) = A \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\n        \\left(- \\frac{(r - r_0)^2}{2 \\sigma ^ 2}\\right)\n\n    Reference: https://ui.adsabs.harvard.edu/abs/2006ApJ...643..332F (Appendix B)\n\n    Parameters\n    ----------\n    amplitude : float\n        See model formula\n    r_0 : float\n        See model formula\n    sigma : float\n        See model formula\n\n    See Also\n    --------\n    CaseBattacharya1998, Paczynski1990, YusifovKucuk2004, Lorimer2006,\n    YusifovKucuk2004B, Exponential\n    """"""\n\n    amplitude = Parameter()\n    r_0 = Parameter()\n    sigma = Parameter()\n    evolved = False\n\n    def __init__(self, amplitude=1, r_0=7.04, sigma=1.83, **kwargs):\n        super().__init__(amplitude=amplitude, r_0=r_0, sigma=sigma, **kwargs)\n\n    @staticmethod\n    def evaluate(r, amplitude, r_0, sigma):\n        """"""Evaluate model.""""""\n        term1 = 1.0 / np.sqrt(2 * np.pi * sigma)\n        term2 = np.exp(-((r - r_0) ** 2) / (2 * sigma ** 2))\n        return amplitude * term1 * term2\n\n\nclass Lorimer2006(Fittable1DModel):\n    r""""""Radial distribution of the surface density of pulsars in the galaxy - Lorimer 2006.\n\n    .. math::\n        f(r) = A \\left( \\frac{r}{r_{\\odot}} \\right) ^ B \\exp\n        \\left[ -C \\left( \\frac{r - r_{\\odot}}{r_{\\odot}} \\right) \\right]\n\n    Reference: https://ui.adsabs.harvard.edu/abs/2006MNRAS.372..777L (Formula (10))\n\n    Parameters\n    ----------\n    amplitude : float\n        See model formula\n    B : float\n        See model formula\n    C : float\n        See model formula\n\n    See Also\n    --------\n    CaseBattacharya1998, Paczynski1990, YusifovKucuk2004, Lorimer2006,\n    YusifovKucuk2004B, FaucherKaspi2006\n    """"""\n\n    amplitude = Parameter()\n    B = Parameter()\n    C = Parameter()\n    evolved = True\n\n    def __init__(self, amplitude=1, B=1.9, C=5.0, **kwargs):\n        super().__init__(amplitude=amplitude, B=B, C=C, **kwargs)\n\n    @staticmethod\n    def evaluate(r, amplitude, B, C):\n        """"""Evaluate model.""""""\n        d_sun = D_SUN_TO_GALACTIC_CENTER.value\n        term1 = (r / d_sun) ** B\n        term2 = np.exp(-C * (r - d_sun) / d_sun)\n        return amplitude * term1 * term2\n\n\nclass Exponential(Fittable1DModel):\n    r""""""Exponential distribution.\n\n    .. math::\n        f(z) = A \\exp \\left(- \\frac{|z|}{z_0} \\right)\n\n    Usually used for height distribution above the Galactic plane,\n    with 0.05 kpc as a commonly used birth height distribution.\n\n    Parameters\n    ----------\n    amplitude : float\n        See model formula\n    z_0 : float\n        Scale height of the distribution\n\n    See Also\n    --------\n    CaseBattacharya1998, Paczynski1990, YusifovKucuk2004, Lorimer2006,\n    YusifovKucuk2004B, FaucherKaspi2006, Exponential\n    """"""\n\n    amplitude = Parameter()\n    z_0 = Parameter()\n    evolved = False\n\n    def __init__(self, amplitude=1, z_0=0.05, **kwargs):\n        super().__init__(amplitude=amplitude, z_0=z_0, **kwargs)\n\n    @staticmethod\n    def evaluate(z, amplitude, z_0):\n        """"""Evaluate model.""""""\n        return amplitude * np.exp(-np.abs(z) / z_0)\n\n\nclass LogSpiral:\n    """"""Logarithmic spiral.\n\n    Reference: http://en.wikipedia.org/wiki/Logarithmic_spiral\n    """"""\n\n    def xy_position(self, theta=None, radius=None, spiralarm_index=0):\n        """"""Compute (x, y) position for a given angle or radius.\n\n        Parameters\n        ----------\n        theta : array_like\n            Angle (deg)\n        radius : array_like\n            Radius (kpc)\n        spiralarm_index : int\n            Spiral arm index\n\n        Returns\n        -------\n        x, y : array_like\n            Position (x, y)\n        """"""\n        if (theta is None) and not (radius is None):\n            theta = self.theta(radius, spiralarm_index=spiralarm_index)\n        elif (radius is None) and not (theta is None):\n            radius = self.radius(theta, spiralarm_index=spiralarm_index)\n        else:\n            raise ValueError(""Specify only one of: theta, radius"")\n\n        theta = np.radians(theta)\n        x = radius * np.cos(theta)\n        y = radius * np.sin(theta)\n        return x, y\n\n    def radius(self, theta, spiralarm_index):\n        """"""Radius for a given angle.\n\n        Parameters\n        ----------\n        theta : array_like\n            Angle (deg)\n        spiralarm_index : int\n            Spiral arm index\n\n        Returns\n        -------\n        radius : array_like\n            Radius (kpc)\n        """"""\n        k = self.k[spiralarm_index]\n        r_0 = self.r_0[spiralarm_index]\n        theta_0 = self.theta_0[spiralarm_index]\n        d_theta = np.radians(theta - theta_0)\n        radius = r_0 * np.exp(d_theta / k)\n        return radius\n\n    def theta(self, radius, spiralarm_index):\n        """"""Angle for a given radius.\n\n        Parameters\n        ----------\n        radius : array_like\n            Radius (kpc)\n        spiralarm_index : int\n            Spiral arm index\n\n        Returns\n        -------\n        theta : array_like\n            Angle (deg)\n        """"""\n        k = self.k[spiralarm_index]\n        r_0 = self.r_0[spiralarm_index]\n        theta_0 = self.theta_0[spiralarm_index]\n        theta_0 = np.radians(theta_0)\n        theta = k * np.log(radius / r_0) + theta_0\n        return np.degrees(theta)\n\n\nclass FaucherSpiral(LogSpiral):\n    """"""Milky way spiral arm used in Faucher et al (2006).\n\n    Reference: https://ui.adsabs.harvard.edu/abs/2006ApJ...643..332F\n    """"""\n\n    # Parameters\n    k = Quantity([4.25, 4.25, 4.89, 4.89], ""rad"")\n    r_0 = Quantity([3.48, 3.48, 4.9, 4.9], ""kpc"")\n    theta_0 = Quantity([1.57, 4.71, 4.09, 0.95], ""rad"")\n    spiralarms = np.array([""Norma"", ""Carina Sagittarius"", ""Perseus"", ""Crux Scutum""])\n\n    @staticmethod\n    def _blur(radius, theta, amount=0.07, random_state=""random-seed""):\n        """"""Blur the positions around the centroid of the spiralarm.\n\n        The given positions are blurred by drawing a displacement in radius from\n        a normal distribution, with sigma = amount * radius. And a direction\n        theta from a uniform distribution in the interval [0, 2 * pi].\n\n        Parameters\n        ----------\n        radius : `~astropy.units.Quantity`\n            Radius coordinate\n        theta : `~astropy.units.Quantity`\n            Angle coordinate\n        amount: float, optional\n            Amount of blurring of the position, given as a fraction of `radius`.\n        random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n            Defines random number generator initialisation.\n            Passed to `~gammapy.utils.random.get_random_state`.\n        """"""\n        random_state = get_random_state(random_state)\n\n        dr = Quantity(abs(random_state.normal(0, amount * radius, radius.size)), ""kpc"")\n        dtheta = Quantity(random_state.uniform(0, 2 * np.pi, radius.size), ""rad"")\n        x, y = cartesian(radius, theta)\n        dx, dy = cartesian(dr, dtheta)\n        return polar(x + dx, y + dy)\n\n    @staticmethod\n    def _gc_correction(\n        radius, theta, r_corr=Quantity(2.857, ""kpc""), random_state=""random-seed""\n    ):\n        """"""Correction of source distribution towards the galactic center.\n\n        To avoid spiralarm features near the Galactic Center, the position angle theta\n        is blurred by a certain amount towards the GC.\n\n        Parameters\n        ----------\n        radius : `~astropy.units.Quantity`\n            Radius coordinate\n        theta : `~astropy.units.Quantity`\n            Angle coordinate\n        r_corr : `~astropy.units.Quantity`, optional\n            Scale of the correction towards the GC\n        random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n            Defines random number generator initialisation.\n            Passed to `~gammapy.utils.random.get_random_state`.\n        """"""\n        random_state = get_random_state(random_state)\n\n        theta_corr = Quantity(random_state.uniform(0, 2 * np.pi, radius.size), ""rad"")\n        return radius, theta + theta_corr * np.exp(-radius / r_corr)\n\n    def __call__(self, radius, blur=True, random_state=""random-seed""):\n        """"""Draw random position from spiral arm distribution.\n\n        Returns the corresponding angle theta[rad] to a given radius[kpc] and number of spiralarm.\n        Possible numbers are:\n\n        * Norma = 0,\n        * Carina Sagittarius = 1,\n        * Perseus = 2\n        * Crux Scutum = 3.\n\n        Parameters\n        ----------\n        random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n            Defines random number generator initialisation.\n            Passed to `~gammapy.utils.random.get_random_state`.\n\n        Returns\n        -------\n        Returns dx and dy, if blurring= true.\n        """"""\n        random_state = get_random_state(random_state)\n\n        # Choose spiral arm\n        N = random_state.randint(0, 4, radius.size)\n        theta = self.k[N] * np.log(radius / self.r_0[N]) + self.theta_0[N]\n        spiralarm = self.spiralarms[N]\n\n        if blur:  # Apply blurring model according to Faucher\n            radius, theta = self._blur(radius, theta, random_state=random_state)\n            radius, theta = self._gc_correction(\n                radius, theta, random_state=random_state\n            )\n        return radius, theta, spiralarm\n\n\nclass ValleeSpiral(LogSpiral):\n    """"""Milky way spiral arm model from Vallee (2008).\n\n    Reference: https://ui.adsabs.harvard.edu/abs/2008AJ....135.1301V\n    """"""\n\n    # Model parameters\n    p = Quantity(12.8, ""deg"")  # pitch angle in deg\n    m = 4  # number of spiral arms\n    r_sun = Quantity(7.6, ""kpc"")  # distance sun to Galactic center in kpc\n    r_0 = Quantity(2.1, ""kpc"")  # spiral inner radius in kpc\n    theta_0 = Quantity(-20, ""deg"")  # Norma spiral arm start angle\n    bar_radius = Quantity(3.0, ""kpc"")  # Radius of the galactic bar (not equal r_0!)\n\n    spiralarms = np.array([""Norma"", ""Perseus"", ""Carina Sagittarius"", ""Crux Scutum""])\n\n    def __init__(self):\n        self.r_0 = self.r_0 * np.ones(4)\n        self.theta_0 = self.theta_0 + Quantity([0, 90, 180, 270], ""deg"")\n        self.k = Quantity(1.0 / np.tan(np.radians(self.p.value)) * np.ones(4), ""rad"")\n\n        # Compute start and end point of the bar\n        x_0, y_0 = self.xy_position(radius=self.bar_radius, spiralarm_index=0)\n        x_1, y_1 = self.xy_position(radius=self.bar_radius, spiralarm_index=2)\n        self.bar = dict(x=Quantity([x_0, x_1]), y=Quantity([y_0, y_1]))\n\n\n""""""Radial distribution (dict mapping names to classes).""""""\nradial_distributions = {\n    ""CB98"": CaseBattacharya1998,\n    ""F06"": FaucherKaspi2006,\n    ""L06"": Lorimer2006,\n    ""P90"": Paczynski1990,\n    ""YK04"": YusifovKucuk2004,\n    ""YK04B"": YusifovKucuk2004B,\n}\n'"
gammapy/astro/population/velocity.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Pulsar velocity distribution models.""""""\nimport numpy as np\nfrom astropy.modeling import Fittable1DModel, Parameter\nfrom astropy.units import Quantity\n\n__all__ = [\n    ""FaucherKaspi2006VelocityMaxwellian"",\n    ""FaucherKaspi2006VelocityBimodal"",\n    ""Paczynski1990Velocity"",\n    ""velocity_distributions"",\n]\n\n# Simulation range used for random number drawing\nVMIN, VMAX = Quantity([0, 4000], ""km/s"")\n\n\nclass FaucherKaspi2006VelocityMaxwellian(Fittable1DModel):\n    r""""""Maxwellian pulsar velocity distribution.\n\n    .. math::\n        f(v) = A \\sqrt{ \\frac{2}{\\pi}} \\frac{v ^ 2}{\\sigma ^ 3 }\n               \\exp \\left(-\\frac{v ^ 2}{2 \\sigma ^ 2} \\right)\n\n    Reference: https://ui.adsabs.harvard.edu/abs/2006ApJ...643..332F\n\n    Parameters\n    ----------\n    amplitude : float\n        Value of the integral\n    sigma : float\n        Velocity parameter (km s^-1)\n    """"""\n\n    amplitude = Parameter()\n    sigma = Parameter()\n\n    def __init__(self, amplitude=1, sigma=265, **kwargs):\n        super().__init__(amplitude=amplitude, sigma=sigma, **kwargs)\n\n    @staticmethod\n    def evaluate(v, amplitude, sigma):\n        """"""One dimensional velocity model function.""""""\n        term1 = np.sqrt(2 / np.pi) * v ** 2 / sigma ** 3\n        term2 = np.exp(-(v ** 2) / (2 * sigma ** 2))\n        return term1 * term2\n\n\nclass FaucherKaspi2006VelocityBimodal(Fittable1DModel):\n    r""""""Bimodal pulsar velocity distribution - Faucher & Kaspi (2006).\n\n    .. math::\n        f(v) = A\\sqrt{\\frac{2}{\\pi}} v^2 \\left[\\frac{w}{\\sigma_1^3}\n        \\exp \\left(-\\frac{v^2}{2\\sigma_1^2} \\right) + \\frac{1-w}{\\sigma_2^3}\n        \\exp \\left(-\\frac{v^2}{2\\sigma_2^2} \\right) \\right]\n\n    Reference: https://ui.adsabs.harvard.edu/abs/2006ApJ...643..332F (Formula (7))\n\n    Parameters\n    ----------\n    amplitude : float\n        Value of the integral\n    sigma1 : float\n        See model formula\n    sigma2 : float\n        See model formula\n    w : float\n        See model formula\n    """"""\n\n    amplitude = Parameter()\n    sigma_1 = Parameter()\n    sigma_2 = Parameter()\n    w = Parameter()\n\n    def __init__(self, amplitude=1, sigma_1=160, sigma_2=780, w=0.9, **kwargs):\n        super().__init__(\n            amplitude=amplitude, sigma_1=sigma_1, sigma_2=sigma_2, w=w, **kwargs\n        )\n\n    @staticmethod\n    def evaluate(v, amplitude, sigma_1, sigma_2, w):\n        """"""One dimensional Faucher-Guigere & Kaspi 2006 velocity model function.""""""\n        A = amplitude * np.sqrt(2 / np.pi) * v ** 2\n        term1 = (w / sigma_1 ** 3) * np.exp(-(v ** 2) / (2 * sigma_1 ** 2))\n        term2 = (1 - w) / sigma_2 ** 3 * np.exp(-(v ** 2) / (2 * sigma_2 ** 2))\n        return A * (term1 + term2)\n\n\nclass Paczynski1990Velocity(Fittable1DModel):\n    r""""""Distribution by Lyne 1982 and adopted by Paczynski and Faucher.\n\n    .. math::\n        f(v) = A\\frac{4}{\\pi} \\frac{1}{v_0 \\left[1 + (v / v_0) ^ 2 \\right] ^ 2}\n\n    Reference: https://ui.adsabs.harvard.edu/abs/1990ApJ...348..485P (Formula (3))\n\n    Parameters\n    ----------\n    amplitude : float\n        Value of the integral\n    v_0 : float\n        Velocity parameter (km s^-1)\n    """"""\n\n    amplitude = Parameter()\n    v_0 = Parameter()\n\n    def __init__(self, amplitude=1, v_0=560, **kwargs):\n        super().__init__(amplitude=amplitude, v_0=v_0, **kwargs)\n\n    @staticmethod\n    def evaluate(v, amplitude, v_0):\n        """"""One dimensional Paczynski 1990 velocity model function.""""""\n        return amplitude * 4.0 / (np.pi * v_0 * (1 + (v / v_0) ** 2) ** 2)\n\n\n""""""Velocity distributions (dict mapping names to classes).""""""\nvelocity_distributions = {\n    ""H05"": FaucherKaspi2006VelocityMaxwellian,\n    ""F06B"": FaucherKaspi2006VelocityBimodal,\n    ""F06P"": Paczynski1990Velocity,\n}\n'"
gammapy/astro/source/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Astrophysical source models.""""""\nfrom .pulsar import *\nfrom .pwn import *\nfrom .snr import *\n'"
gammapy/astro/source/pulsar.py,4,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Pulsar source models.""""""\nimport numpy as np\nfrom astropy.units import Quantity\n\n__all__ = [""Pulsar"", ""SimplePulsar""]\n\nDEFAULT_I = Quantity(1e45, ""g cm2"")\n""""""Pulsar default moment of inertia""""""\n\nDEFAULT_R = Quantity(1e6, ""cm"")\n""""""Pulsar default radius of the neutron star""""""\n\nB_CONST = Quantity(3.2e19, ""gauss s^(-1/2)"")\n""""""Pulsar default magnetic field constant""""""\n\n\nclass SimplePulsar:\n    """"""Magnetic dipole spin-down model for a pulsar.\n\n    Reference: http://www.cv.nrao.edu/course/astr534/Pulsars.html\n\n    Parameters\n    ----------\n    P : `~astropy.units.Quantity`\n        Rotation period (sec)\n    P_dot : `~astropy.units.Quantity`\n        Rotation period derivative (sec sec^-1)\n    I : `~astropy.units.Quantity`\n        Moment of inertia (g cm^2)\n    R : `~astropy.units.Quantity`\n        Radius of the pulsar (cm)\n    """"""\n\n    def __init__(self, P, P_dot, I=DEFAULT_I, R=DEFAULT_R):\n        self.P = Quantity(P, ""s"")\n        self.P_dot = P_dot\n        self.I = I\n        self.R = R\n\n    @property\n    def luminosity_spindown(self):\n        r""""""Spin-down luminosity (`~astropy.units.Quantity`).\n\n        .. math:: \\dot{L} = 4\\pi^2 I \\frac{\\dot{P}}{P^{3}}\n        """"""\n        return 4 * np.pi ** 2 * self.I * self.P_dot / self.P ** 3\n\n    @property\n    def tau(self):\n        r""""""Characteristic age (`~astropy.units.Quantity`).\n\n        .. math:: \\tau = \\frac{P}{2\\dot{P}}\n        """"""\n        return (self.P / (2 * self.P_dot)).to(""yr"")\n\n    @property\n    def magnetic_field(self):\n        r""""""Magnetic field strength at the polar cap (`~astropy.units.Quantity`).\n\n        .. math:: B = 3.2 \\cdot 10^{19} (P\\dot{P})^{1/2} \\text{ Gauss}\n        """"""\n        return B_CONST * np.sqrt(self.P * self.P_dot)\n\n\nclass Pulsar:\n    """"""Magnetic dipole spin-down pulsar model.\n\n    Reference: http://www.cv.nrao.edu/course/astr534/Pulsars.html\n\n    Parameters\n    ----------\n    P_0 : float\n        Period at birth\n    B : `~astropy.units.Quantity`\n        Magnetic field strength at the poles (Gauss)\n    n : float\n        Spin-down braking index\n    I : float\n        Moment of inertia\n    R : float\n        Radius\n    """"""\n\n    def __init__(\n        self, P_0=""0.1 s"", B=""1e10 G"", n=3, I=DEFAULT_I, R=DEFAULT_R, age=None, L_0=None\n    ):\n        P_0 = Quantity(P_0, ""s"")\n        B = Quantity(B, ""G"")\n\n        self.I = I\n        self.R = R\n        self.P_0 = P_0\n        self.B = B\n        self.P_dot_0 = (B / B_CONST) ** 2 / P_0\n        self.tau_0 = P_0 / (2 * self.P_dot_0)\n        self.n = float(n)\n        self.beta = -(n + 1.0) / (n - 1.0)\n        if age is not None:\n            self.age = Quantity(age, ""yr"")\n        if L_0 is None:\n            self.L_0 = 4 * np.pi ** 2 * self.I * self.P_dot_0 / self.P_0 ** 3\n\n    def luminosity_spindown(self, t):\n        r""""""Spin down luminosity.\n\n        .. math::\n            \\dot{L}(t) = \\dot{L}_0 \\left(1 + \\frac{t}{\\tau_0}\\right)^{-\\frac{n + 1}{n - 1}}\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the pulsar\n        """"""\n        t = Quantity(t, ""yr"")\n        return self.L_0 * (1 + (t / self.tau_0)) ** self.beta\n\n    def energy_integrated(self, t):\n        r""""""Total energy released by a given time.\n\n        Time-integrated spin-down luminosity since birth.\n\n        .. math:: E(t) = \\dot{L}_0 \\tau_0 \\frac{t}{t + \\tau_0}\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the pulsar.\n        """"""\n        t = Quantity(t, ""yr"")\n        return self.L_0 * self.tau_0 * (t / (t + self.tau_0))\n\n    def period(self, t):\n        r""""""Rotation period.\n\n        .. math::\n            P(t) = P_0 \\left(1 + \\frac{t}{\\tau_0}\\right)^{\\frac{1}{n - 1}}\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the pulsar\n        """"""\n        t = Quantity(t, ""yr"")\n        return self.P_0 * (1 + (t / self.tau_0)) ** (1.0 / (self.n - 1))\n\n    def period_dot(self, t):\n        r""""""Period derivative at age t.\n\n        P_dot for a given period and magnetic field B, assuming a dipole\n        spin-down.\n\n        .. math:: \\dot{P}(t) = \\frac{B^2}{3.2 \\cdot 10^{19} P(t)}\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the pulsar.\n        """"""\n        t = Quantity(t, ""yr"")\n        return self.B ** 2 / (self.period(t) * B_CONST ** 2)\n\n    def tau(self, t):\n        r""""""Characteristic age at real age t.\n\n        .. math:: \\tau = \\frac{P}{2\\dot{P}}\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the pulsar.\n        """"""\n        t = Quantity(t, ""yr"")\n        return self.period(t) / 2 * self.period_dot(t)\n\n    def magnetic_field(self, t):\n        r""""""Magnetic field at polar cap (assumed constant).\n\n        .. math::\n            B = 3.2 \\cdot 10^{19} (P\\dot{P})^{1/2} \\text{ Gauss}\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the pulsar.\n        """"""\n        t = Quantity(t, ""yr"")\n        return B_CONST * np.sqrt(self.period(t) * self.period_dot(t))\n'"
gammapy/astro/source/pwn.py,3,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Pulsar wind nebula (PWN) source models.""""""\nimport numpy as np\nimport scipy.optimize\nimport astropy.constants\nfrom astropy.units import Quantity\nfrom astropy.utils import lazyproperty\nfrom .pulsar import Pulsar\nfrom .snr import SNRTrueloveMcKee\n\n__all__ = [""PWN""]\n\n\nclass PWN:\n    """"""Simple pulsar wind nebula (PWN) evolution model.\n\n    Parameters\n    ----------\n    pulsar : `~gammapy.astro.source.Pulsar`\n        Pulsar model instance.\n    snr : `~gammapy.astro.source.SNRTrueloveMcKee`\n        SNR model instance\n    eta_e : float\n        Fraction of energy going into electrons.\n    eta_B : float\n        Fraction of energy going into magnetic fields.\n    age : `~astropy.units.Quantity`\n        Age of the PWN.\n    morphology : str\n        Morphology model of the PWN\n    """"""\n\n    def __init__(\n        self,\n        pulsar=Pulsar(),\n        snr=SNRTrueloveMcKee(),\n        eta_e=0.999,\n        eta_B=0.001,\n        morphology=""Gaussian2D"",\n        age=None,\n    ):\n        self.pulsar = pulsar\n        if not isinstance(snr, SNRTrueloveMcKee):\n            raise ValueError(""SNR must be instance of SNRTrueloveMcKee"")\n        self.snr = snr\n        self.eta_e = eta_e\n        self.eta_B = eta_B\n        self.morphology = morphology\n        if age is not None:\n            self.age = Quantity(age, ""yr"")\n\n    def _radius_free_expansion(self, t):\n        """"""Radius at age t during free expansion phase.\n\n        Reference: https://ui.adsabs.harvard.edu/abs/2006ARA%26A..44...17G (Formula 8).\n        """"""\n        term1 = (self.snr.e_sn ** 3 * self.pulsar.L_0 ** 2) / (self.snr.m_ejecta ** 5)\n        return (1.44 * term1 ** (1.0 / 10) * t ** (6.0 / 5)).cgs\n\n    @lazyproperty\n    def _collision_time(self):\n        """"""Time of collision between the PWN and the reverse shock of the SNR.\n\n        Returns\n        -------\n        t_coll : `~astropy.units.Quantity`\n            Time of collision.\n        """"""\n\n        def time_coll(t):\n            t = Quantity(t, ""yr"")\n            r_pwn = self._radius_free_expansion(t).to_value(""cm"")\n            r_shock = self.snr.radius_reverse_shock(t).to_value(""cm"")\n            return r_pwn - r_shock\n\n        # 4e3 years is a typical value that works for fsolve\n        return Quantity(scipy.optimize.fsolve(time_coll, 4e3), ""yr"")\n\n    def radius(self, t):\n        r""""""Radius of the PWN at age t.\n\n        During the free expansion phase the radius of the PWN evolves like:\n\n        .. math::\n            R_{PWN}(t) = 1.44 \\left(\\frac{E_{SN}^3\\dot{E}_0^2}\n            {M_{ej}^5}\\right)^{1/10}t^{6/5}\n            \\text{pc}\n\n        After the collision with the reverse shock of the SNR, the radius is\n        assumed to be constant (See `~gammapy.astro.source.SNRTrueloveMcKee.radius_reverse_shock`).\n\n        Reference: https://ui.adsabs.harvard.edu/abs/2006ARA%26A..44...17G (Formula 8).\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the SNR\n        """"""\n        t = Quantity(t, ""yr"")\n        r_collision = self._radius_free_expansion(self._collision_time)\n        r = np.where(\n            t < self._collision_time,\n            self._radius_free_expansion(t).value,\n            r_collision.value,\n        )\n        return Quantity(r, ""cm"")\n\n    def magnetic_field(self, t):\n        """"""Estimate of the magnetic field inside the PWN.\n\n        By assuming that a certain fraction of the spin down energy is\n        converted to magnetic field energy an estimation of the magnetic\n        field can be derived.\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the SNR\n        """"""\n        t = Quantity(t, ""yr"")\n        energy = self.pulsar.energy_integrated(t)\n        volume = 4.0 / 3 * np.pi * self.radius(t) ** 3\n        return np.sqrt(2 * astropy.constants.mu0 * self.eta_B * energy / volume)\n'"
gammapy/astro/source/snr.py,5,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Supernova remnant (SNR) source models.""""""\nimport numpy as np\nimport astropy.constants\nfrom astropy.units import Quantity\nfrom astropy.utils import lazyproperty\n\n__all__ = [""SNR"", ""SNRTrueloveMcKee""]\n\n\nclass SNR:\n    """"""Simple supernova remnant (SNR) evolution model.\n\n    The model is based on the Sedov-Taylor solution for strong explosions.\n\n    Reference: https://ui.adsabs.harvard.edu/abs/1950RSPSA.201..159T\n\n    Parameters\n    ----------\n    e_sn : `~astropy.units.Quantity`\n        SNR energy (erg), equal to the SN energy after neutrino losses\n    theta : `~astropy.units.Quantity`\n        Fraction of E_SN that goes into cosmic rays\n    n_ISM : `~astropy.units.Quantity`\n        ISM density (g cm^-3)\n    m_ejecta : `~astropy.units.Quantity`\n        Ejecta mass (g)\n    t_stop : `~astropy.units.Quantity`\n        Post-shock temperature where gamma-ray emission stops\n    """"""\n\n    def __init__(\n        self,\n        e_sn=""1e51 erg"",\n        theta=Quantity(0.1),\n        n_ISM=Quantity(1, ""cm-3""),\n        m_ejecta=astropy.constants.M_sun,\n        t_stop=Quantity(1e6, ""K""),\n        age=None,\n        morphology=""Shell2D"",\n        spectral_index=2.1,\n    ):\n        self.e_sn = Quantity(e_sn, ""erg"")\n        self.theta = theta\n        self.rho_ISM = n_ISM * astropy.constants.m_p\n        self.n_ISM = n_ISM\n        self.m_ejecta = m_ejecta\n        self.t_stop = t_stop\n        self.morphology = morphology\n        self.spectral_index = spectral_index\n        if age is not None:\n            self.age = Quantity(age, ""yr"")\n\n    def radius(self, t):\n        r""""""Outer shell radius at age t.\n\n        The radius during the free expansion phase is given by:\n\n        .. math::\n            r_{SNR}(t) \\approx 0.01\n            \\left(\\frac{E_{SN}}{10^{51}erg}\\right)^{1/2}\n            \\left(\\frac{M_{ej}}{M_{\\odot}}\\right)^{-1/2} t\n            \\text{ pc}\n\n        The radius during the Sedov-Taylor phase evolves like:\n\n        .. math::\n            r_{SNR}(t) \\approx \\left(\\frac{E_{SN}}{\\rho_{ISM}}\\right)^{1/5}t^{2/5}\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the SNR\n        """"""\n        t = Quantity(t, ""yr"")\n        r = np.where(\n            t > self.sedov_taylor_begin,\n            self._radius_sedov_taylor(t).to_value(""cm""),\n            self._radius_free_expansion(t).to_value(""cm""),\n        )\n        return Quantity(r, ""cm"")\n\n    def _radius_free_expansion(self, t):\n        """"""Shock radius at age t during free expansion phase.\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the SNR\n        """"""\n        # proportional constant for the free expansion phase\n        term_1 = (self.e_sn / Quantity(1e51, ""erg"")) ** (1.0 / 2)\n        term_2 = (self.m_ejecta / astropy.constants.M_sun) ** (-1.0 / 2)\n        return Quantity(0.01, ""pc/yr"") * term_1 * term_2 * t\n\n    def _radius_sedov_taylor(self, t):\n        """"""Shock radius  at age t  during Sedov Taylor phase.\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the SNR\n        """"""\n        R_FE = self._radius_free_expansion(self.sedov_taylor_begin)\n        return R_FE * (t / self.sedov_taylor_begin) ** (2.0 / 5)\n\n    def radius_inner(self, t, fraction=0.0914):\n        """"""Inner radius  at age t  of the SNR shell.\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the SNR\n        """"""\n        return self.radius(t) * (1 - fraction)\n\n    def luminosity_tev(self, t, energy_min=""1 TeV""):\n        r""""""Gamma-ray luminosity above ``energy_min`` at age ``t``.\n\n        The luminosity is assumed constant in a given age interval and zero\n        before and after. The assumed spectral index is 2.1.\n\n        The gamma-ray luminosity above 1 TeV is given by:\n\n        .. math::\n            L_{\\gamma}(\\geq 1TeV) \\approx 10^{34} \\theta\n            \\left(\\frac{E_{SN}}{10^{51} erg}\\right)\n            \\left(\\frac{\\rho_{ISM}}{1.66\\cdot 10^{-24} g/cm^{3}} \\right)\n            \\text{ s}^{-1}\n\n        Reference: https://ui.adsabs.harvard.edu/abs/1994A%26A...287..959D (Formula (7)).\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the SNR\n        energy_min : `~astropy.units.Quantity`\n            Lower energy limit for the luminosity\n        """"""\n        t = Quantity(t, ""yr"")\n        energy_min = Quantity(energy_min, ""TeV"")\n\n        # Flux in 1 k distance according to Drury formula 9\n        term_0 = energy_min / Quantity(1, ""TeV"")\n        term_1 = self.e_sn / Quantity(1e51, ""erg"")\n        term_2 = self.rho_ISM / (Quantity(1, ""cm-3"") * astropy.constants.m_p)\n        L = self.theta * term_0 ** (1 - self.spectral_index) * term_1 * term_2\n\n        # Corresponding luminosity\n        L = np.select(\n            [t <= self.sedov_taylor_begin, t <= self.sedov_taylor_end], [0, L]\n        )\n        return Quantity(1.0768e34, ""s-1"") * L\n\n    @lazyproperty\n    def sedov_taylor_begin(self):\n        r""""""Characteristic time scale when the Sedov-Taylor phase of the SNR\'s evolution begins.\n\n        The beginning of the Sedov-Taylor phase of the SNR is defined by the condition,\n        that the swept up mass of the surrounding medium equals the mass of the\n        ejected mass.\n\n        The time scale is given by:\n\n        .. math::\n            t_{begin} \\approx 200\n            \\left(\\frac{E_{SN}}{10^{51}erg}\\right)^{-1/2}\n            \\left(\\frac{M_{ej}}{M_{\\odot}}\\right)^{5/6}\n            \\left(\\frac{\\rho_{ISM}}{10^{-24}g/cm^3}\\right)^{-1/3}\n            \\text{yr}\n        """"""\n        term1 = (self.e_sn / Quantity(1e51, ""erg"")) ** (-1.0 / 2)\n        term2 = (self.m_ejecta / astropy.constants.M_sun) ** (5.0 / 6)\n        term3 = (self.rho_ISM / (Quantity(1, ""cm-3"") * astropy.constants.m_p)) ** (\n            -1.0 / 3\n        )\n        return Quantity(200, ""yr"") * term1 * term2 * term3\n\n    @lazyproperty\n    def sedov_taylor_end(self):\n        r""""""Characteristic time scale when the Sedov-Taylor phase of the SNR\'s evolution ends.\n\n        The end of the Sedov-Taylor phase of the SNR is defined by the condition, that the\n        temperature at the shock drops below T = 10^6 K.\n\n        The time scale is given by:\n\n        .. math::\n            t_{end} \\approx 43000\n            \\left(\\frac{m}{1.66\\cdot 10^{-24}g}\\right)^{5/6}\n            \\left(\\frac{E_{SN}}{10^{51}erg}\\right)^{1/3}\n            \\left(\\frac{\\rho_{ISM}}{1.66\\cdot 10^{-24}g/cm^3}\\right)^{-1/3}\n            \\text{yr}\n        """"""\n        term1 = (\n            3\n            * astropy.constants.m_p.cgs\n            / (100 * astropy.constants.k_B.cgs * self.t_stop)\n        )\n        term2 = (self.e_sn / self.rho_ISM) ** (2.0 / 5)\n        return ((term1 * term2) ** (5.0 / 6)).to(""yr"")\n\n\nclass SNRTrueloveMcKee(SNR):\n    """"""SNR model according to Truelove & McKee (1999).\n\n    Reference: https://ui.adsabs.harvard.edu/abs/1999ApJS..120..299T\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # Characteristic dimensions\n        self.r_c = self.m_ejecta ** (1.0 / 3) * self.rho_ISM ** (-1.0 / 3)\n        self.t_c = (\n            self.e_sn ** (-1.0 / 2)\n            * self.m_ejecta ** (5.0 / 6)\n            * self.rho_ISM ** (-1.0 / 3)\n        )\n\n    def radius(self, t):\n        r""""""Outer shell radius at age t.\n\n        The radius during the free expansion phase is given by:\n\n        .. math::\n            R_{SNR}(t) = 1.12R_{ch}\\left(\\frac{t}{t_{ch}}\\right)^{2/3}\n\n        The radius during the Sedov-Taylor phase evolves like:\n\n        .. math::\n            R_{SNR}(t) = \\left[R_{SNR, ST}^{5/2} + \\left(2.026\\frac{E_{SN}}\n            {\\rho_{ISM}}\\right)^{1/2}(t - t_{ST})\\right]^{2/5}\n\n        Using the characteristic dimensions:\n\n        .. math::\n            R_{ch} = M_{ej}^{1/3}\\rho_{ISM}^{-1/3} \\ \\\n            \\text{and} \\ \\ t_{ch} = E_{SN}^{-1/2}M_{ej}^{5/6}\\rho_{ISM}^{-1/3}\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the SNR\n        """"""\n        t = Quantity(t, ""yr"")\n\n        # Evaluate `_radius_sedov_taylor` on `t > self.sedov_taylor_begin`\n        # only to avoid a warning\n        r = np.empty(t.shape, dtype=np.float64)\n        mask = t > self.sedov_taylor_begin\n        r[mask] = self._radius_sedov_taylor(t[mask]).to_value(""cm"")\n        r[~mask] = self._radius_free_expansion(t[~mask]).to_value(""cm"")\n        return Quantity(r, ""cm"")\n\n    def _radius_free_expansion(self, t):\n        """"""Shock radius at age t during free expansion phase.\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the SNR\n        """"""\n        return 1.12 * self.r_c * (t / self.t_c) ** (2.0 / 3)\n\n    def _radius_sedov_taylor(self, t):\n        """"""Shock radius  at age t during Sedov Taylor phase.\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the SNR\n        """"""\n        term1 = self._radius_free_expansion(self.sedov_taylor_begin) ** (5.0 / 2)\n        term2 = (2.026 * (self.e_sn / self.rho_ISM)) ** (1.0 / 2)\n        return (term1 + term2 * (t - self.sedov_taylor_begin)) ** (2.0 / 5)\n\n    @lazyproperty\n    def sedov_taylor_begin(self):\n        r""""""Characteristic time scale when the Sedov-Taylor phase starts.\n\n        Given by :math:`t_{ST} \\approx 0.52 t_{ch}`.\n        """"""\n        return 0.52 * self.t_c\n\n    def radius_reverse_shock(self, t):\n        r""""""Reverse shock radius at age t.\n\n        Initially the reverse shock co-evolves with the radius of the SNR:\n\n        .. math::\n            R_{RS}(t) = \\frac{1}{1.19}r_{SNR}(t)\n\n        After a time :math:`t_{core} \\simeq 0.25t_{ch}` the reverse shock reaches\n        the core and then propagates as:\n\n        .. math::\n            R_{RS}(t) = \\left[1.49 - 0.16 \\frac{t - t_{core}}{t_{ch}} - 0.46\n            \\ln \\left(\\frac{t}{t_{core}}\\right)\\right]\\frac{R_{ch}}{t_{ch}}t\n\n        Parameters\n        ----------\n        t : `~astropy.units.Quantity`\n            Time after birth of the SNR\n        """"""\n        t = Quantity(t, ""yr"")\n\n        # Time when reverse shock reaches the ""core""\n        t_core = 0.25 * self.t_c\n\n        term1 = (t - t_core) / (self.t_c)\n        term2 = 1.49 - 0.16 * term1 - 0.46 * np.log(t / t_core)\n        R_1 = self._radius_free_expansion(t) / 1.19\n        R_RS = term2 * (self.r_c / self.t_c) * t\n        r = np.where(t < t_core, R_1.to_value(""cm""), R_RS.to_value(""cm""))\n        return Quantity(r, ""cm"")\n'"
gammapy/catalog/tests/__init__.py,0,b''
gammapy/catalog/tests/test_core.py,1,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom astropy.table import Column, Table\nfrom astropy.units import Quantity\nfrom gammapy.catalog import SourceCatalog\nfrom gammapy.utils.testing import assert_quantity_allclose\n\n\nclass SomeSourceCatalog(SourceCatalog):\n    """"""Minimal test source catalog class for unit tests.""""""\n\n    name = ""test123""\n    description = ""Test source catalog""\n\n\ndef make_test_catalog():\n    table = Table()\n    table[""Source_Name""] = [""a"", ""bb"", ""ccc""]\n    table[""RA""] = Column([42.2, 43.3, 44.4], unit=""deg"")\n    table[""DEC""] = Column([1, 2, 3], unit=""deg"")\n    return SomeSourceCatalog(table)\n\n\nclass TestSourceCatalog:\n    def setup(self):\n        self.cat = make_test_catalog()\n\n    def test_str(self):\n        assert ""description"" in str(self.cat)\n\n    def test_table(self):\n        assert_allclose(self.cat.table[""RA""][1], 43.3)\n\n    def test_row_index(self):\n        idx = self.cat.row_index(name=""bb"")\n        assert idx == 1\n\n        with pytest.raises(KeyError):\n            self.cat.row_index(name=""invalid"")\n\n    def test_source_name(self):\n        name = self.cat.source_name(index=1)\n        assert name == ""bb""\n\n        with pytest.raises(IndexError):\n            self.cat.source_name(index=99)\n\n        with pytest.raises(IndexError):\n            self.cat.source_name(""invalid"")\n\n    def test_getitem(self):\n        source = self.cat[""a""]\n        assert source.data[""Source_Name""] == ""a""\n\n        source = self.cat[0]\n        assert source.data[""Source_Name""] == ""a""\n\n        source = self.cat[np.int32(0)]\n        assert source.data[""Source_Name""] == ""a""\n\n        with pytest.raises(KeyError):\n            self.cat[""invalid""]\n\n        with pytest.raises(IndexError):\n            self.cat[99]\n\n        with pytest.raises(TypeError):\n            self.cat[1.2]\n\n    def test_positions(self):\n        positions = self.cat.positions\n        assert len(positions) == 3\n\n\nclass TestSourceCatalogObject:\n    def setup(self):\n        self.cat = make_test_catalog()\n        self.source = self.cat[""bb""]\n\n    def test_name(self):\n        assert self.source.name == ""bb""\n\n    def test_row_index(self):\n        assert self.source.row_index == 1\n\n    def test_data(self):\n        d = self.source.data\n        assert isinstance(d, dict)\n\n        assert isinstance(d[""RA""], Quantity)\n        assert_quantity_allclose(d[""RA""], Quantity(43.3, ""deg""))\n\n        assert isinstance(d[""DEC""], Quantity)\n        assert_quantity_allclose(d[""DEC""], Quantity(2, ""deg""))\n\n    def test_position(self):\n        position = self.source.position\n        assert_allclose(position.ra.deg, 43.3)\n        assert_allclose(position.dec.deg, 2)\n'"
gammapy/catalog/tests/test_fermi.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom astropy import units as u\nfrom astropy.time import Time\nfrom astropy.utils.data import get_pkg_data_filename\nfrom gammapy.catalog import (\n    SourceCatalog2FHL,\n    SourceCatalog3FGL,\n    SourceCatalog3FHL,\n    SourceCatalog4FGL,\n)\nfrom gammapy.modeling.models import (\n    ExpCutoffPowerLaw3FGLSpectralModel,\n    LogParabolaSpectralModel,\n    PowerLaw2SpectralModel,\n    PowerLawSpectralModel,\n    SuperExpCutoffPowerLaw3FGLSpectralModel,\n    SuperExpCutoffPowerLaw4FGLSpectralModel,\n)\nfrom gammapy.utils.gauss import Gauss2DPDF\nfrom gammapy.utils.testing import (\n    assert_quantity_allclose,\n    assert_time_allclose,\n    requires_data,\n)\n\nSOURCES_4FGL = [\n    dict(\n        idx=0,\n        name=""4FGL J0000.3-7355"",\n        str_ref_file=""data/4fgl_J0000.3-7355.txt"",\n        spec_type=PowerLawSpectralModel,\n        dnde=u.Quantity(2.9476e-11, ""cm-2 s-1 GeV-1""),\n        dnde_err=u.Quantity(5.3318e-12, ""cm-2 s-1 GeV-1""),\n    ),\n    dict(\n        idx=3,\n        name=""4FGL J0001.5+2113"",\n        str_ref_file=""data/4fgl_J0001.5+2113.txt"",\n        spec_type=LogParabolaSpectralModel,\n        dnde=u.Quantity(2.8545e-8, ""cm-2 s-1 GeV-1""),\n        dnde_err=u.Quantity(1.3324e-9, ""cm-2 s-1 GeV-1""),\n    ),\n    dict(\n        idx=7,\n        name=""4FGL J0002.8+6217"",\n        str_ref_file=""data/4fgl_J0002.8+6217.txt"",\n        spec_type=SuperExpCutoffPowerLaw4FGLSpectralModel,\n        dnde=u.Quantity(2.084e-09, ""cm-2 s-1 GeV-1""),\n        dnde_err=u.Quantity(1.0885e-10, ""cm-2 s-1 GeV-1""),\n    ),\n    dict(\n        idx=2718,\n        name=""4FGL J1409.1-6121e"",\n        str_ref_file=""data/4fgl_J1409.1-6121e.txt"",\n        spec_type=LogParabolaSpectralModel,\n        dnde=u.Quantity(1.3237202133031811e-12, ""cm-2 s-1 MeV-1""),\n        dnde_err=u.Quantity(4.513233455580648e-14, ""cm-2 s-1 MeV-1""),\n    ),\n]\n\nSOURCES_3FGL = [\n    dict(\n        idx=0,\n        name=""3FGL J0000.1+6545"",\n        str_ref_file=""data/3fgl_J0000.1+6545.txt"",\n        spec_type=PowerLawSpectralModel,\n        dnde=u.Quantity(1.4351261e-9, ""cm-2 s-1 GeV-1""),\n        dnde_err=u.Quantity(2.1356270e-10, ""cm-2 s-1 GeV-1""),\n    ),\n    dict(\n        idx=4,\n        name=""3FGL J0001.4+2120"",\n        str_ref_file=""data/3fgl_J0001.4+2120.txt"",\n        spec_type=LogParabolaSpectralModel,\n        dnde=u.Quantity(8.3828599e-10, ""cm-2 s-1 GeV-1""),\n        dnde_err=u.Quantity(2.6713238e-10, ""cm-2 s-1 GeV-1""),\n    ),\n    dict(\n        idx=55,\n        name=""3FGL J0023.4+0923"",\n        str_ref_file=""data/3fgl_J0023.4+0923.txt"",\n        spec_type=ExpCutoffPowerLaw3FGLSpectralModel,\n        dnde=u.Quantity(1.8666925e-09, ""cm-2 s-1 GeV-1""),\n        dnde_err=u.Quantity(2.2068837e-10, ""cm-2 s-1 GeV-1""),\n    ),\n    dict(\n        idx=960,\n        name=""3FGL J0835.3-4510"",\n        str_ref_file=""data/3fgl_J0835.3-4510.txt"",\n        spec_type=SuperExpCutoffPowerLaw3FGLSpectralModel,\n        dnde=u.Quantity(1.6547128794756733e-06, ""cm-2 s-1 GeV-1""),\n        dnde_err=u.Quantity(1.6621504e-11, ""cm-2 s-1 MeV-1""),\n    ),\n]\n\nSOURCES_2FHL = [\n    dict(\n        idx=221,\n        name=""2FHL J1445.1-0329"",\n        str_ref_file=""data/2fhl_j1445.1-0329.txt"",\n        spec_type=PowerLaw2SpectralModel,\n        dnde=u.Quantity(1.065463448091757e-10, ""cm-2 s-1 TeV-1""),\n        dnde_err=u.Quantity(4.9691205387540815e-11, ""cm-2 s-1 TeV-1""),\n    ),\n    dict(\n        idx=134,\n        name=""2FHL J0822.6-4250e"",\n        str_ref_file=""data/2fhl_j0822.6-4250e.txt"",\n        spec_type=LogParabolaSpectralModel,\n        dnde=u.Quantity(2.46548351696472e-10, ""cm-2 s-1 TeV-1""),\n        dnde_err=u.Quantity(9.771755529198772e-11, ""cm-2 s-1 TeV-1""),\n    ),\n]\n\nSOURCES_3FHL = [\n    dict(\n        idx=352,\n        name=""3FHL J0534.5+2201"",\n        spec_type=PowerLawSpectralModel,\n        dnde=u.Quantity(6.3848912826152664e-12, ""cm-2 s-1 GeV-1""),\n        dnde_err=u.Quantity(2.679593524691324e-13, ""cm-2 s-1 GeV-1""),\n    ),\n    dict(\n        idx=1442,\n        name=""3FHL J2158.8-3013"",\n        spec_type=LogParabolaSpectralModel,\n        dnde=u.Quantity(2.056998292908196e-12, ""cm-2 s-1 GeV-1""),\n        dnde_err=u.Quantity(4.219030630302381e-13, ""cm-2 s-1 GeV-1""),\n    ),\n]\n\n\n@requires_data()\nclass TestFermi4FGLObject:\n    @classmethod\n    def setup_class(cls):\n        cls.cat = SourceCatalog4FGL()\n        cls.source_name = ""4FGL J0534.5+2200""\n        cls.source = cls.cat[cls.source_name]\n\n    def test_name(self):\n        assert self.source.name == self.source_name\n\n    def test_row_index(self):\n        assert self.source.row_index == 995\n\n    @pytest.mark.parametrize(""ref"", SOURCES_4FGL, ids=lambda _: _[""name""])\n    def test_str(self, ref):\n        actual = str(self.cat[ref[""idx""]])\n        expected = open(get_pkg_data_filename(ref[""str_ref_file""])).read()\n        assert actual == expected\n\n    @pytest.mark.parametrize(""ref"", SOURCES_4FGL, ids=lambda _: _[""name""])\n    def test_spectral_model(self, ref):\n        model = self.cat[ref[""idx""]].spectral_model()\n\n        e_ref = model.reference.quantity\n        dnde, dnde_err = model.evaluate_error(e_ref)\n        assert isinstance(model, ref[""spec_type""])\n        assert_quantity_allclose(dnde, ref[""dnde""], rtol=1e-4)\n        assert_quantity_allclose(dnde_err, ref[""dnde_err""], rtol=1e-4)\n\n    def test_spatial_model(self):\n        model = self.cat[""4FGL J0000.3-7355""].spatial_model()\n        assert model.tag == ""PointSpatialModel""\n        assert model.frame == ""icrs""\n        p = model.parameters\n        assert_allclose(p[""lon_0""].value, 0.0983)\n        assert_allclose(p[""lat_0""].value, -73.921997)\n        pos_err = model.position_error\n        assert_allclose(pos_err.angle.value, -62.7)\n        assert_allclose(0.5 * pos_err.height.value, 0.0525, rtol=1e-4)\n        assert_allclose(0.5 * pos_err.width.value, 0.051, rtol=1e-4)\n        assert_allclose(model.position.ra.value, pos_err.center.ra.value)\n        assert_allclose(model.position.dec.value, pos_err.center.dec.value)\n\n        model = self.cat[""4FGL J1409.1-6121e""].spatial_model()\n        assert model.tag == ""DiskSpatialModel""\n        assert model.frame == ""icrs""\n        p = model.parameters\n        assert_allclose(p[""lon_0""].value, 212.294006)\n        assert_allclose(p[""lat_0""].value, -61.353001)\n        assert_allclose(p[""r_0""].value, 0.7331369519233704)\n\n        model = self.cat[""4FGL J0617.2+2234e""].spatial_model()\n        assert model.tag == ""GaussianSpatialModel""\n        assert model.frame == ""icrs""\n        p = model.parameters\n        assert_allclose(p[""lon_0""].value, 94.309998)\n        assert_allclose(p[""lat_0""].value, 22.58)\n        assert_allclose(p[""sigma""].value, 0.27)\n\n        model = self.cat[""4FGL J1443.0-6227e""].spatial_model()\n        assert model.tag == ""TemplateSpatialModel""\n        assert model.frame == ""fk5""\n        assert model.normalize is True\n\n    @pytest.mark.parametrize(""ref"", SOURCES_4FGL, ids=lambda _: _[""name""])\n    def test_sky_model(self, ref):\n        self.cat[ref[""idx""]].sky_model\n\n    def test_flux_points(self):\n        flux_points = self.source.flux_points\n\n        assert len(flux_points.table) == 7\n        assert ""flux_ul"" in flux_points.table.colnames\n        assert flux_points.sed_type == ""flux""\n\n        desired = [\n            2.2378458e-06,\n            1.4318283e-06,\n            5.4776939e-07,\n            1.2769708e-07,\n            2.5820052e-08,\n            2.3897000e-09,\n            7.1766204e-11,\n        ]\n        assert_allclose(flux_points.table[""flux""].data, desired, rtol=1e-5)\n\n    def test_flux_points_ul(self):\n        source = self.cat[""4FGL J0000.3-7355""]\n        flux_points = source.flux_points\n\n        desired = [\n            4.13504750e-08,\n            3.80519616e-09,\n            np.nan,\n            np.nan,\n            np.nan,\n            np.nan,\n            7.99699456e-12,\n        ]\n        assert_allclose(flux_points.table[""flux_ul""].data, desired, rtol=1e-5)\n\n    def test_lightcurve(self):\n        lc = self.source.lightcurve\n        table = lc.table\n\n        assert len(table) == 8\n        assert table.colnames == [\n            ""time_min"",\n            ""time_max"",\n            ""flux"",\n            ""flux_errp"",\n            ""flux_errn"",\n        ]\n\n        expected = Time(54682.655277777776, format=""mjd"", scale=""utc"")\n        assert_time_allclose(lc.time_min[0], expected)\n\n        expected = Time(55047.603239293836, format=""mjd"", scale=""utc"")\n        assert_time_allclose(lc.time_max[0], expected)\n\n        assert table[""flux""].unit == ""cm-2 s-1""\n        assert_allclose(table[""flux""][0], 2.2122326e-06, rtol=1e-3)\n\n        assert table[""flux_errp""].unit == ""cm-2 s-1""\n        assert_allclose(table[""flux_errp""][0], 2.3099371e-08, rtol=1e-3)\n\n        assert table[""flux_errn""].unit == ""cm-2 s-1""\n        assert_allclose(table[""flux_errn""][0], 2.3099371e-08, rtol=1e-3)\n\n\n@requires_data()\nclass TestFermi3FGLObject:\n    @classmethod\n    def setup_class(cls):\n        cls.cat = SourceCatalog3FGL()\n        # Use 3FGL J0534.5+2201 (Crab) as a test source\n        cls.source_name = ""3FGL J0534.5+2201""\n        cls.source = cls.cat[cls.source_name]\n\n    def test_name(self):\n        assert self.source.name == self.source_name\n\n    def test_row_index(self):\n        assert self.source.row_index == 621\n\n    def test_data(self):\n        assert_allclose(self.source.data[""Signif_Avg""], 30.669872283935547)\n\n    def test_position(self):\n        position = self.source.position\n        assert_allclose(position.ra.deg, 83.637199, atol=1e-3)\n        assert_allclose(position.dec.deg, 22.024099, atol=1e-3)\n\n    @pytest.mark.parametrize(""ref"", SOURCES_3FGL, ids=lambda _: _[""name""])\n    def test_str(self, ref):\n        actual = str(self.cat[ref[""idx""]])\n        expected = open(get_pkg_data_filename(ref[""str_ref_file""])).read()\n        assert actual == expected\n\n    @pytest.mark.parametrize(""ref"", SOURCES_3FGL, ids=lambda _: _[""name""])\n    def test_spectral_model(self, ref):\n        model = self.cat[ref[""idx""]].spectral_model()\n\n        dnde, dnde_err = model.evaluate_error(1 * u.GeV)\n\n        assert isinstance(model, ref[""spec_type""])\n        assert_quantity_allclose(dnde, ref[""dnde""])\n        assert_quantity_allclose(dnde_err, ref[""dnde_err""], rtol=1e-3)\n\n    def test_spatial_model(self):\n        model = self.cat[0].spatial_model()\n        assert model.tag == ""PointSpatialModel""\n        assert model.frame == ""icrs""\n        p = model.parameters\n        assert_allclose(p[""lon_0""].value, 0.0377)\n        assert_allclose(p[""lat_0""].value, 65.751701)\n\n        model = self.cat[122].spatial_model()\n        assert model.tag == ""GaussianSpatialModel""\n        assert model.frame == ""icrs""\n        p = model.parameters\n        assert_allclose(p[""lon_0""].value, 14.75)\n        assert_allclose(p[""lat_0""].value, -72.699997)\n        assert_allclose(p[""sigma""].value, 1.35)\n\n        model = self.cat[955].spatial_model()\n        assert model.tag == ""DiskSpatialModel""\n        assert model.frame == ""icrs""\n        p = model.parameters\n        assert_allclose(p[""lon_0""].value, 128.287201)\n        assert_allclose(p[""lat_0""].value, -45.190102)\n        assert_allclose(p[""r_0""].value, 0.91)\n\n        model = self.cat[602].spatial_model()\n        assert model.tag == ""TemplateSpatialModel""\n        assert model.frame == ""fk5""\n        assert model.normalize is True\n\n        model = self.cat[""3FGL J0000.2-3738""].spatial_model()\n        pos_err = model.position_error\n        assert_allclose(pos_err.angle.value, -88.55)\n        assert_allclose(0.5 * pos_err.height.value, 0.0731, rtol=1e-4)\n        assert_allclose(0.5 * pos_err.width.value, 0.0676, rtol=1e-4)\n        assert_allclose(model.position.ra.value, pos_err.center.ra.value)\n        assert_allclose(model.position.dec.value, pos_err.center.dec.value)\n\n    @pytest.mark.parametrize(""ref"", SOURCES_3FGL, ids=lambda _: _[""name""])\n    def test_sky_model(self, ref):\n        self.cat[ref[""idx""]].sky_model()\n\n    def test_flux_points(self):\n        flux_points = self.source.flux_points\n\n        assert len(flux_points.table) == 5\n        assert ""flux_ul"" in flux_points.table.colnames\n        assert flux_points.sed_type == ""flux""\n\n        desired = [1.645888e-06, 5.445407e-07, 1.255338e-07, 2.545524e-08, 2.263189e-09]\n        assert_allclose(flux_points.table[""flux""].data, desired, rtol=1e-5)\n\n    def test_flux_points_ul(self):\n        source = self.cat[""3FGL J0000.2-3738""]\n        flux_points = source.flux_points\n\n        desired = [4.096391e-09, 6.680059e-10, np.nan, np.nan, np.nan]\n        assert_allclose(flux_points.table[""flux_ul""].data, desired, rtol=1e-5)\n\n    def test_lightcurve(self):\n        lc = self.source.lightcurve\n        table = lc.table\n\n        assert len(table) == 48\n        assert table.colnames == [\n            ""time_min"",\n            ""time_max"",\n            ""flux"",\n            ""flux_errp"",\n            ""flux_errn"",\n        ]\n\n        expected = Time(54680.02313657408, format=""mjd"", scale=""utc"")\n        assert_time_allclose(lc.time_min[0], expected)\n\n        expected = Time(54710.43824797454, format=""mjd"", scale=""utc"")\n        assert_time_allclose(lc.time_max[0], expected)\n\n        assert table[""flux""].unit == ""cm-2 s-1""\n        assert_allclose(table[""flux""][0], 2.384e-06, rtol=1e-3)\n\n        assert table[""flux_errp""].unit == ""cm-2 s-1""\n        assert_allclose(table[""flux_errp""][0], 8.071e-08, rtol=1e-3)\n\n        assert table[""flux_errn""].unit == ""cm-2 s-1""\n        assert_allclose(table[""flux_errn""][0], 8.071e-08, rtol=1e-3)\n\n    def test_crab_alias(self):\n        for name in [\n            ""Crab"",\n            ""3FGL J0534.5+2201"",\n            ""1FHL J0534.5+2201"",\n            ""PSR J0534+2200"",\n        ]:\n            assert self.cat[name].row_index == 621\n\n\n@requires_data()\nclass TestFermi2FHLObject:\n    @classmethod\n    def setup_class(cls):\n        cls.cat = SourceCatalog2FHL()\n        # Use 2FHL J0534.5+2201 (Crab) as a test source\n        cls.source_name = ""2FHL J0534.5+2201""\n        cls.source = cls.cat[cls.source_name]\n\n    def test_name(self):\n        assert self.source.name == self.source_name\n\n    def test_position(self):\n        position = self.source.position\n        assert_allclose(position.ra.deg, 83.634102, atol=1e-3)\n        assert_allclose(position.dec.deg, 22.0215, atol=1e-3)\n\n    @pytest.mark.parametrize(""ref"", SOURCES_2FHL, ids=lambda _: _[""name""])\n    def test_str(self, ref):\n        actual = str(self.cat[ref[""idx""]])\n        expected = open(get_pkg_data_filename(ref[""str_ref_file""])).read()\n        assert actual == expected\n\n    def test_spectral_model(self):\n        model = self.source.spectral_model()\n        energy = u.Quantity(100, ""GeV"")\n        desired = u.Quantity(6.8700477298e-12, ""cm-2 GeV-1 s-1"")\n        assert_quantity_allclose(model(energy), desired)\n\n    def test_flux_points(self):\n        # test flux point on  PKS 2155-304\n        src = self.cat[""PKS 2155-304""]\n        flux_points = src.flux_points\n        actual = flux_points.table[""flux""]\n        desired = [2.866363e-10, 6.118736e-11, 3.257970e-16] * u.Unit(""cm-2 s-1"")\n        assert_quantity_allclose(actual, desired)\n\n        actual = flux_points.table[""flux_ul""]\n        desired = [np.nan, np.nan, 1.294092e-11] * u.Unit(""cm-2 s-1"")\n        assert_quantity_allclose(actual, desired, rtol=1e-3)\n\n    def test_spatial_model(self):\n        model = self.cat[221].spatial_model()\n        assert model.tag == ""PointSpatialModel""\n        assert model.frame == ""icrs""\n        p = model.parameters\n        assert_allclose(p[""lon_0""].value, 221.281998, rtol=1e-5)\n        assert_allclose(p[""lat_0""].value, -3.4943, rtol=1e-5)\n\n        model = self.cat[""2FHL J1304.5-4353""].spatial_model()\n        pos_err = model.position_error\n        scale = Gauss2DPDF().containment_radius(0.95) / Gauss2DPDF().containment_radius(\n            0.68\n        )\n        assert_allclose(pos_err.height.value, 2 * 0.041987 * scale, rtol=1e-4)\n        assert_allclose(pos_err.width.value, 2 * 0.041987 * scale, rtol=1e-4)\n        assert_allclose(model.position.ra.value, pos_err.center.ra.value)\n        assert_allclose(model.position.dec.value, pos_err.center.dec.value)\n\n        model = self.cat[97].spatial_model()\n        assert model.tag == ""GaussianSpatialModel""\n        assert model.frame == ""icrs""\n        p = model.parameters\n        assert_allclose(p[""lon_0""].value, 94.309998, rtol=1e-5)\n        assert_allclose(p[""lat_0""].value, 22.58, rtol=1e-5)\n        assert_allclose(p[""sigma""].value, 0.27)\n\n        model = self.cat[134].spatial_model()\n        assert model.tag == ""DiskSpatialModel""\n        assert model.frame == ""icrs""\n        p = model.parameters\n        assert_allclose(p[""lon_0""].value, 125.660004, rtol=1e-5)\n        assert_allclose(p[""lat_0""].value, -42.84, rtol=1e-5)\n        assert_allclose(p[""r_0""].value, 0.37)\n\n        model = self.cat[256].spatial_model()\n        assert model.tag == ""TemplateSpatialModel""\n        assert model.frame == ""fk5""\n        assert model.normalize is True\n        # TODO: have to check the extended template used for RX J1713,\n        # for now I guess it\'s the same than for 3FGL\n        # and added a copy with the name given by 2FHL in gammapy-extra\n\n\n@requires_data()\nclass TestFermi3FHLObject:\n    @classmethod\n    def setup_class(cls):\n        cls.cat = SourceCatalog3FHL()\n        # Use 3FHL J0534.5+2201 (Crab) as a test source\n        cls.source_name = ""3FHL J0534.5+2201""\n        cls.source = cls.cat[cls.source_name]\n\n    def test_name(self):\n        assert self.source.name == self.source_name\n\n    def test_row_index(self):\n        assert self.source.row_index == 352\n\n    def test_data(self):\n        assert_allclose(self.source.data[""Signif_Avg""], 168.64082)\n\n    def test_str(self):\n        actual = str(self.cat[""3FHL J2301.9+5855e""])  # an extended source\n        expected = open(get_pkg_data_filename(""data/3fhl_j2301.9+5855e.txt"")).read()\n        assert actual == expected\n\n    def test_position(self):\n        position = self.source.position\n        assert_allclose(position.ra.deg, 83.634834, atol=1e-3)\n        assert_allclose(position.dec.deg, 22.019203, atol=1e-3)\n\n    @pytest.mark.parametrize(""ref"", SOURCES_3FHL, ids=lambda _: _[""name""])\n    def test_spectral_model(self, ref):\n        model = self.cat[ref[""idx""]].spectral_model()\n\n        dnde, dnde_err = model.evaluate_error(100 * u.GeV)\n\n        assert isinstance(model, ref[""spec_type""])\n        assert_quantity_allclose(dnde, ref[""dnde""])\n        assert_quantity_allclose(dnde_err, ref[""dnde_err""], rtol=1e-3)\n\n    @pytest.mark.parametrize(""ref"", SOURCES_3FHL, ids=lambda _: _[""name""])\n    def test_spatial_model(self, ref):\n        model = self.cat[ref[""idx""]].spatial_model()\n        assert model.frame == ""icrs""\n\n        model = self.cat[""3FHL J0002.1-6728""].spatial_model()\n        pos_err = model.position_error\n        assert_allclose(0.5 * pos_err.height.value, 0.035713, rtol=1e-4)\n        assert_allclose(0.5 * pos_err.width.value, 0.035713, rtol=1e-4)\n        assert_allclose(model.position.ra.value, pos_err.center.ra.value)\n        assert_allclose(model.position.dec.value, pos_err.center.dec.value)\n\n    @pytest.mark.parametrize(""ref"", SOURCES_3FHL, ids=lambda _: _[""name""])\n    def test_sky_model(self, ref):\n        self.cat[ref[""idx""]].sky_model()\n\n    def test_flux_points(self):\n        flux_points = self.source.flux_points\n\n        assert len(flux_points.table) == 5\n        assert ""flux_ul"" in flux_points.table.colnames\n\n        desired = [5.169889e-09, 2.245024e-09, 9.243175e-10, 2.758956e-10, 6.684021e-11]\n        assert_allclose(flux_points.table[""flux""].data, desired, rtol=1e-3)\n\n    def test_crab_alias(self):\n        for name in [""Crab Nebula"", ""3FHL J0534.5+2201"", ""3FGL J0534.5+2201i""]:\n            assert self.cat[name].row_index == 352\n\n\n@requires_data()\nclass TestSourceCatalog3FGL:\n    @classmethod\n    def setup_class(cls):\n        cls.cat = SourceCatalog3FGL()\n\n    def test_main_table(self):\n        assert len(self.cat.table) == 3034\n\n    def test_extended_sources(self):\n        table = self.cat.extended_sources_table\n        assert len(table) == 25\n\n\n@requires_data()\nclass TestSourceCatalog2FHL:\n    @classmethod\n    def setup_class(cls):\n        cls.cat = SourceCatalog2FHL()\n\n    def test_main_table(self):\n        assert len(self.cat.table) == 360\n\n    def test_extended_sources(self):\n        table = self.cat.extended_sources_table\n        assert len(table) == 25\n\n    def test_crab_alias(self):\n        for name in [""Crab"", ""3FGL J0534.5+2201i"", ""1FHL J0534.5+2201""]:\n            assert self.cat[name].row_index == 85\n\n\n@requires_data()\nclass TestSourceCatalog3FHL:\n    @classmethod\n    def setup_class(cls):\n        cls.cat = SourceCatalog3FHL()\n\n    def test_main_table(self):\n        assert len(self.cat.table) == 1556\n\n    def test_extended_sources(self):\n        table = self.cat.extended_sources_table\n        assert len(table) == 55\n'"
gammapy/catalog/tests/test_gammacat.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nfrom astropy import units as u\nfrom astropy.utils.data import get_pkg_data_filename\nfrom gammapy.catalog import SourceCatalogGammaCat\nfrom gammapy.utils.gauss import Gauss2DPDF\nfrom gammapy.utils.testing import assert_quantity_allclose, requires_data\n\nSOURCES = [\n    {\n        ""name"": ""Vela X"",\n        ""str_ref_file"": ""data/gammacat_vela_x.txt"",\n        ""spec_type"": ""ecpl"",\n        ""dnde_1TeV"": 1.36e-11 * u.Unit(""cm-2 s-1 TeV-1""),\n        ""dnde_1TeV_err"": 7.531e-13 * u.Unit(""cm-2 s-1 TeV-1""),\n        ""flux_1TeV"": 2.104e-11 * u.Unit(""cm-2 s-1""),\n        ""eflux_1_10TeV"": 9.265778680255336e-11 * u.Unit(""erg cm-2 s-1""),\n        ""n_flux_points"": 24,\n        ""spatial_model"": ""GaussianSpatialModel"",\n        ""ra"": 128.287003,\n        ""dec"": -45.189999,\n    },\n    {\n        ""name"": ""HESS J1848-018"",\n        ""str_ref_file"": ""data/gammacat_hess_j1848-018.txt"",\n        ""spec_type"": ""pl"",\n        ""dnde_1TeV"": 3.7e-12 * u.Unit(""cm-2 s-1 TeV-1""),\n        ""dnde_1TeV_err"": 4e-13 * u.Unit(""cm-2 s-1 TeV-1""),\n        ""flux_1TeV"": 2.056e-12 * u.Unit(""cm-2 s-1""),\n        ""eflux_1_10TeV"": 6.235650344765057e-12 * u.Unit(""erg cm-2 s-1""),\n        ""n_flux_points"": 11,\n        ""spatial_model"": ""GaussianSpatialModel"",\n        ""ra"": 282.119995,\n        ""dec"": -1.792,\n    },\n    {\n        ""name"": ""HESS J1813-178"",\n        ""str_ref_file"": ""data/gammacat_hess_j1813-178.txt"",\n        ""spec_type"": ""pl2"",\n        ""dnde_1TeV"": 2.678e-12 * u.Unit(""cm-2 s-1 TeV-1""),\n        ""dnde_1TeV_err"": 2.55e-13 * u.Unit(""cm-2 s-1 TeV-1""),\n        ""flux_1TeV"": 2.457e-12 * u.Unit(""cm-2 s-1""),\n        ""eflux_1_10TeV"": 8.923614018939419e-12 * u.Unit(""erg cm-2 s-1""),\n        ""n_flux_points"": 13,\n        ""spatial_model"": ""GaussianSpatialModel"",\n        ""ra"": 273.362915,\n        ""dec"": -17.84889,\n    },\n]\n\n\n@pytest.fixture(scope=""session"")\ndef gammacat():\n    filename = ""$GAMMAPY_DATA/catalogs/gammacat/gammacat.fits.gz""\n    return SourceCatalogGammaCat(filename=filename)\n\n\n@requires_data()\nclass TestSourceCatalogGammaCat:\n    def test_source_table(self, gammacat):\n        assert gammacat.tag == ""gamma-cat""\n        assert len(gammacat.table) == 162\n\n    def test_positions(self, gammacat):\n        assert len(gammacat.positions) == 162\n\n    def test_w28_alias_names(self, gammacat):\n        for name in [\n            ""W28"",\n            ""HESS J1801-233"",\n            ""W 28"",\n            ""SNR G6.4-0.1"",\n            ""SNR G006.4-00.1"",\n            ""GRO J1801-2320"",\n        ]:\n            assert gammacat[name].row_index == 112\n\n\n@requires_data()\nclass TestSourceCatalogObjectGammaCat:\n    def test_data(self, gammacat):\n        source = gammacat[0]\n\n        assert isinstance(source.data, dict)\n        assert source.data[""common_name""] == ""CTA 1""\n        assert_quantity_allclose(source.data[""dec""], 72.782997 * u.deg)\n\n    @pytest.mark.parametrize(""ref"", SOURCES, ids=lambda _: _[""name""])\n    def test_str(self, gammacat, ref):\n        actual = str(gammacat[ref[""name""]])\n        expected = open(get_pkg_data_filename(ref[""str_ref_file""])).read()\n        assert actual == expected\n\n    @pytest.mark.parametrize(""ref"", SOURCES, ids=lambda _: _[""name""])\n    def test_spectral_model(self, gammacat, ref):\n        source = gammacat[ref[""name""]]\n        spectral_model = source.spectral_model()\n\n        assert source.data[""spec_type""] == ref[""spec_type""]\n\n        e_min, e_max, e_inf = [1, 10, 1e10] * u.TeV\n\n        dne = spectral_model(e_min)\n        flux = spectral_model.integral(emin=e_min, emax=e_inf)\n        eflux = spectral_model.energy_flux(emin=e_min, emax=e_max).to(""erg cm-2 s-1"")\n\n        assert_quantity_allclose(dne, ref[""dnde_1TeV""], rtol=1e-3)\n        assert_quantity_allclose(flux, ref[""flux_1TeV""], rtol=1e-3)\n        assert_quantity_allclose(eflux, ref[""eflux_1_10TeV""], rtol=1e-3)\n\n    @pytest.mark.parametrize(""ref"", SOURCES, ids=lambda _: _[""name""])\n    def test_spectral_model_err(self, gammacat, ref):\n        source = gammacat[ref[""name""]]\n        spectral_model = source.spectral_model()\n\n        e_min, e_max, e_inf = [1, 10, 1e10] * u.TeV\n\n        dnde, dnde_err = spectral_model.evaluate_error(e_min)\n\n        assert_quantity_allclose(dnde, ref[""dnde_1TeV""], rtol=1e-3)\n        assert_quantity_allclose(dnde_err, ref[""dnde_1TeV_err""], rtol=1e-3)\n\n    @pytest.mark.parametrize(""ref"", SOURCES, ids=lambda _: _[""name""])\n    def test_flux_points(self, gammacat, ref):\n        source = gammacat[ref[""name""]]\n\n        flux_points = source.flux_points\n\n        assert len(flux_points.table) == ref[""n_flux_points""]\n\n    @pytest.mark.parametrize(""ref"", SOURCES, ids=lambda _: _[""name""])\n    def test_position(self, gammacat, ref):\n        source = gammacat[ref[""name""]]\n\n        position = source.position\n\n        assert_allclose(position.ra.deg, ref[""ra""], atol=1e-3)\n        assert_allclose(position.dec.deg, ref[""dec""], atol=1e-3)\n\n    @pytest.mark.parametrize(""ref"", SOURCES, ids=lambda _: _[""name""])\n    def test_spatial_model(self, gammacat, ref):\n        source = gammacat[ref[""name""]]\n\n        spatial_model = source.spatial_model()\n        assert spatial_model.frame == ""galactic""\n\n        # TODO: put better asserts on model properties\n        # TODO: add a point and shell source -> separate list of sources for morphology test parametrization?\n        assert spatial_model.__class__.__name__ == ref[""spatial_model""]\n\n        model = gammacat[""HESS J1634-472""].spatial_model()\n        pos_err = model.position_error\n        scale_r95 = Gauss2DPDF().containment_radius(0.95)\n        assert_allclose(pos_err.height.value, 2 * 0.044721 * scale_r95, rtol=1e-4)\n        assert_allclose(pos_err.width.value, 2 * 0.044721 * scale_r95, rtol=1e-4)\n        assert_allclose(model.position.l.value, pos_err.center.l.value)\n        assert_allclose(model.position.b.value, pos_err.center.b.value)\n\n    @pytest.mark.parametrize(""ref"", SOURCES, ids=lambda _: _[""name""])\n    def test_sky_model(self, gammacat, ref):\n        gammacat[ref[""name""]].sky_model()\n'"
gammapy/catalog/tests/test_hawc.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.utils.data import get_pkg_data_filename\nfrom gammapy.catalog import SourceCatalog2HWC\nfrom gammapy.modeling.models import (\n    DiskSpatialModel,\n    PointSpatialModel,\n    PowerLawSpectralModel,\n)\nfrom gammapy.utils.gauss import Gauss2DPDF\nfrom gammapy.utils.testing import requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef cat():\n    return SourceCatalog2HWC()\n\n\n@requires_data()\nclass TestSourceCatalog2HWC:\n    @staticmethod\n    def test_source_table(cat):\n        assert cat.tag == ""2hwc""\n        assert len(cat.table) == 40\n\n    @staticmethod\n    def test_positions(cat):\n        assert len(cat.positions) == 40\n\n\n@requires_data()\nclass TestSourceCatalogObject2HWC:\n    @staticmethod\n    def test_data(cat):\n        assert cat[0].data[""source_name""] == ""2HWC J0534+220""\n        assert cat[0].n_models == 1\n\n        assert cat[1].data[""source_name""] == ""2HWC J0631+169""\n        assert cat[1].n_models == 2\n\n    @staticmethod\n    def test_str(cat):\n        expected = open(get_pkg_data_filename(""data/2hwc_j0534+220.txt"")).read()\n        assert str(cat[0]) == expected\n\n        expected = open(get_pkg_data_filename(""data/2hwc_j0631+169.txt"")).read()\n        assert str(cat[1]) == expected\n\n    @staticmethod\n    def test_position(cat):\n        position = cat[0].position\n        assert_allclose(position.ra.deg, 83.628, atol=1e-3)\n        assert_allclose(position.dec.deg, 22.024, atol=1e-3)\n\n    @staticmethod\n    def test_sky_model(cat):\n        model = cat[1].sky_model(""extended"")\n        assert model.name == ""2HWC J0631+169""\n        assert isinstance(model.spectral_model, PowerLawSpectralModel)\n        assert isinstance(model.spatial_model, DiskSpatialModel)\n\n        with pytest.raises(ValueError):\n            cat[0].sky_model(""extended"")\n\n    @staticmethod\n    def test_spectral_model(cat):\n        m = cat[0].spectral_model()\n        dnde, dnde_err = m.evaluate_error(1 * u.TeV)\n        assert dnde.unit == ""cm-2 s-1 TeV-1""\n        assert_allclose(dnde.value, 2.802365e-11, rtol=1e-3)\n        assert_allclose(dnde_err.value, 6.537506e-13, rtol=1e-3)\n\n    @staticmethod\n    def test_spatial_model(cat):\n        m = cat[1].spatial_model()\n        # p = m.parameters\n\n        assert isinstance(m, PointSpatialModel)\n        assert m.lon_0.unit == ""deg""\n        assert_allclose(m.lon_0.value, 195.614, atol=1e-2)\n        # TODO: add assert on position error\n        # assert_allclose(p.error(""lon_0""), tbd)\n        assert m.lat_0.unit == ""deg""\n        assert_allclose(m.lat_0.value, 3.507, atol=1e-2)\n        assert m.frame == ""galactic""\n\n        m = cat[1].spatial_model(""extended"")\n\n        assert isinstance(m, DiskSpatialModel)\n        assert m.lon_0.unit == ""deg""\n        assert_allclose(m.lon_0.value, 195.614, atol=1e-10)\n        assert m.lat_0.unit == ""deg""\n        assert_allclose(m.lat_0.value, 3.507, atol=1e-10)\n        assert m.frame == ""galactic""\n        assert m.r_0.unit == ""deg""\n        assert_allclose(m.r_0.value, 2.0, atol=1e-3)\n\n        model = cat[""2HWC J0534+220""].spatial_model()\n        pos_err = model.position_error\n        scale_r95 = Gauss2DPDF().containment_radius(0.95)\n        assert_allclose(pos_err.height.value, 2 * 0.057 * scale_r95, rtol=1e-4)\n        assert_allclose(pos_err.width.value, 2 * 0.057 * scale_r95, rtol=1e-4)\n        assert_allclose(model.position.l.value, pos_err.center.l.value)\n        assert_allclose(model.position.b.value, pos_err.center.b.value)\n'"
gammapy/catalog/tests/test_hess.py,3,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom astropy import units as u\nfrom astropy.coordinates import Angle, SkyCoord\nfrom astropy.table import Table\nfrom astropy.utils.data import get_pkg_data_filename\nfrom gammapy.catalog import SourceCatalogHGPS, SourceCatalogLargeScaleHGPS\nfrom gammapy.modeling.models import (\n    ExpCutoffPowerLawSpectralModel,\n    PowerLawSpectralModel,\n)\nfrom gammapy.utils.gauss import Gauss2DPDF\nfrom gammapy.utils.testing import assert_quantity_allclose, requires_data\n\nSOURCES = [\n    {""idx"": 33, ""name"": ""HESS J1713-397"", ""str_ref_file"": ""data/hess_j1713-397.txt""},\n    {""idx"": 54, ""name"": ""HESS J1825-137"", ""str_ref_file"": ""data/hess_j1825-137.txt""},\n    {""idx"": 76, ""name"": ""HESS J1930+188"", ""str_ref_file"": ""data/hess_j1930+188.txt""},\n]\n\n\n@pytest.fixture(scope=""session"")\ndef cat():\n    return SourceCatalogHGPS(""$GAMMAPY_DATA/catalogs/hgps_catalog_v1.fits.gz"")\n\n\n@requires_data()\nclass TestSourceCatalogHGPS:\n    @staticmethod\n    def test_source_table(cat):\n        assert cat.tag == ""hgps""\n        assert len(cat.table) == 78\n\n    @staticmethod\n    def test_positions(cat):\n        assert len(cat.positions) == 78\n\n    @staticmethod\n    def test_table_components(cat):\n        assert len(cat.table_components) == 98\n\n    @staticmethod\n    def test_table_associations(cat):\n        assert len(cat.table_associations) == 223\n\n    @staticmethod\n    def test_table_identifications(cat):\n        assert len(cat.table_identifications) == 31\n\n    @staticmethod\n    def test_gaussian_component(cat):\n        # Row index starts at 0, component numbers at 1\n        # Thus we expect `HGPSC 084` at row 83\n        c = cat.gaussian_component(83)\n        assert c.name == ""HGPSC 084""\n\n    @staticmethod\n    def test_large_scale_component(cat):\n        assert isinstance(cat.large_scale_component, SourceCatalogLargeScaleHGPS)\n\n\n@requires_data()\nclass TestSourceCatalogObjectHGPS:\n    @pytest.fixture(scope=""class"")\n    def source(self, cat):\n        return cat[""HESS J1843-033""]\n\n    @staticmethod\n    def test_basics(source):\n        assert source.name == ""HESS J1843-033""\n        assert source.row_index == 64\n        data = source.data\n        assert data[""Source_Class""] == ""Unid""\n        assert ""SourceCatalogObjectHGPS"" in repr(source)\n\n        ss = str(source)\n        assert ""Source name          : HESS J1843-033"" in ss\n        assert ""Component HGPSC 083:"" in ss\n\n    @staticmethod\n    @pytest.mark.parametrize(""ref"", SOURCES)\n    def test_str(cat, ref):\n        actual = str(cat[ref[""idx""]])\n        expected = open(get_pkg_data_filename(ref[""str_ref_file""])).read()\n        assert actual == expected\n\n    @staticmethod\n    def test_position(source):\n        position = source.position\n        assert_allclose(position.ra.deg, 280.95162964)\n        assert_allclose(position.dec.deg, -3.55410194)\n\n    @staticmethod\n    def test_components(source):\n        components = source.components\n        assert len(components) == 2\n        c = components[1]\n        assert c.name == ""HGPSC 084""\n\n    @staticmethod\n    def test_energy_range(source):\n        energy_range = source.energy_range\n        assert energy_range.unit == ""TeV""\n        assert_allclose(energy_range.value, [0.21544346, 61.89658356])\n\n    @staticmethod\n    def test_spectral_model_pl(cat):\n        source = cat[""HESS J1843-033""]\n\n        model = source.spectral_model()\n\n        assert isinstance(model, PowerLawSpectralModel)\n        pars = model.parameters\n        assert_allclose(pars[""amplitude""].value, 9.140179932365378e-13)\n        assert_allclose(pars[""index""].value, 2.1513476371765137)\n        assert_allclose(pars[""reference""].value, 1.867810606956482)\n\n    @staticmethod\n    def test_spectral_model_ecpl(cat):\n        source = cat[""HESS J0835-455""]\n\n        model = source.spectral_model()\n        assert isinstance(model, ExpCutoffPowerLawSpectralModel)\n\n        pars = model.parameters\n        assert_allclose(pars[""amplitude""].value, 6.408420542586617e-12)\n        assert_allclose(pars[""index""].value, 1.3543991614920847)\n        assert_allclose(pars[""reference""].value, 1.696938754239)\n        assert_allclose(pars[""lambda_""].value, 0.081517637)\n\n        assert_allclose(pars[""amplitude""].error, 3.260472e-13, rtol=1e-3)\n        assert_allclose(pars[""index""].error, 0.077331, atol=0.001)\n        assert_allclose(pars[""reference""].error, 0)\n        assert_allclose(pars[""lambda_""].error, 0.011535, atol=0.001)\n\n        model = source.spectral_model(""pl"")\n        assert isinstance(model, PowerLawSpectralModel)\n\n        pars = model.parameters\n        assert_allclose(pars[""amplitude""].value, 1.833056926733856e-12)\n        assert_allclose(pars[""index""].value, 1.8913707)\n        assert_allclose(pars[""reference""].value, 3.0176312923431396)\n\n        assert_allclose(pars[""amplitude""].error, 6.992061e-14, rtol=1e-3)\n        assert_allclose(pars[""index""].error, 0.028383, atol=0.001)\n        assert_allclose(pars[""reference""].error, 0)\n\n    @staticmethod\n    def test_position_error(cat):\n        scale_r95 = Gauss2DPDF().containment_radius(0.95)\n\n        model = cat[""HESS J1729-345""].spatial_model()\n        pos_err = model.position_error\n        assert_allclose(pos_err.angle.value, 0.0)\n        assert_allclose(pos_err.height.value, 2 * 0.0414315 * scale_r95, rtol=1e-4)\n        assert_allclose(pos_err.width.value, 2 * 0.0344351 * scale_r95, rtol=1e-4)\n        assert_allclose(model.position.l.value, pos_err.center.l.value)\n        assert_allclose(model.position.b.value, pos_err.center.b.value)\n\n        model = cat[""HESS J1858+020""].spatial_model()\n        pos_err = model.position_error\n        assert_allclose(pos_err.angle.value, 90.0)\n        assert_allclose(pos_err.height.value, 2 * 0.0222614 * scale_r95, rtol=1e-4)\n        assert_allclose(pos_err.width.value, 2 * 0.0145084 * scale_r95, rtol=1e-4)\n        assert_allclose(model.position.l.value, pos_err.center.l.value)\n        assert_allclose(model.position.b.value, pos_err.center.b.value)\n\n    @staticmethod\n    def test_sky_model_point(cat):\n        model = cat[""HESS J1826-148""].sky_model()\n        p = model.parameters\n        assert_allclose(p[""amplitude""].value, 9.815771242691063e-13)\n        assert_allclose(p[""lon_0""].value, 16.882482528686523)\n        assert_allclose(p[""lat_0""].value, -1.2889292240142822)\n\n    @staticmethod\n    def test_sky_model_gaussian(cat):\n        model = cat[""HESS J1119-614""].sky_model()\n        p = model.parameters\n        assert_allclose(p[""amplitude""].value, 7.959899015960725e-13)\n        assert_allclose(p[""lon_0""].value, 292.128082)\n        assert_allclose(p[""lat_0""].value, -0.5332353711128235)\n        assert_allclose(p[""sigma""].value, 0.09785966575145721)\n\n    @staticmethod\n    def test_sky_model_gaussian2(cat):\n        models = cat[""HESS J1843-033""].sky_model()\n\n        p = models[0].parameters\n        assert_allclose(p[""amplitude""].value, 4.259815e-13, rtol=1e-5)\n        assert_allclose(p[""lon_0""].value, 29.047216415405273)\n        assert_allclose(p[""lat_0""].value, 0.24389676749706268)\n        assert_allclose(p[""sigma""].value, 0.12499100714921951)\n\n        p = models[1].parameters\n        assert_allclose(p[""amplitude""].value, 4.880365e-13, rtol=1e-5)\n        assert_allclose(p[""lon_0""].value, 28.77037811279297)\n        assert_allclose(p[""lat_0""].value, -0.0727819949388504)\n        assert_allclose(p[""sigma""].value, 0.2294706553220749)\n\n    @staticmethod\n    def test_sky_model_gaussian3(cat):\n        models = cat[""HESS J1825-137""].sky_model()\n\n        p = models[0].parameters\n        assert_allclose(p[""amplitude""].value, 1.8952104218765842e-11)\n        assert_allclose(p[""lon_0""].value, 16.988601684570312)\n        assert_allclose(p[""lat_0""].value, -0.4913068115711212)\n        assert_allclose(p[""sigma""].value, 0.47650089859962463)\n\n        p = models[1].parameters\n        assert_allclose(p[""amplitude""].value, 4.4639763971527836e-11)\n        assert_allclose(p[""lon_0""].value, 17.71169090270996)\n        assert_allclose(p[""lat_0""].value, -0.6598004102706909)\n        assert_allclose(p[""sigma""].value, 0.3910967707633972)\n\n        p = models[2].parameters\n        assert_allclose(p[""amplitude""].value, 5.870712920658374e-12)\n        assert_allclose(p[""lon_0""].value, 17.840524673461914)\n        assert_allclose(p[""lat_0""].value, -0.7057178020477295)\n        assert_allclose(p[""sigma""].value, 0.10932201147079468)\n\n    @staticmethod\n    def test_sky_model_gaussian_extern(cat):\n        # special test for the only extern source with a gaussian morphology\n        model = cat[""HESS J1801-233""].sky_model()\n        p = model.parameters\n        assert_allclose(p[""amplitude""].value, 7.499999970031479e-13)\n        assert_allclose(p[""lon_0""].value, 6.656888961791992)\n        assert_allclose(p[""lat_0""].value, -0.267688125371933)\n        assert_allclose(p[""sigma""].value, 0.17)\n\n    @staticmethod\n    def test_sky_model_shell(cat):\n        model = cat[""Vela Junior""].sky_model()\n        p = model.parameters\n        assert_allclose(p[""amplitude""].value, 3.2163001428830995e-11)\n        assert_allclose(p[""lon_0""].value, 266.287384)\n        assert_allclose(p[""lat_0""].value, -1.243260383605957)\n        assert_allclose(p[""radius""].value, 0.95)\n        assert_allclose(p[""width""].value, 0.05)\n\n\n@requires_data()\nclass TestSourceCatalogObjectHGPSComponent:\n    @pytest.fixture(scope=""class"")\n    def component(self, cat):\n        return cat.gaussian_component(83)\n\n    @staticmethod\n    def test_repr(component):\n        assert ""SourceCatalogObjectHGPSComponent"" in repr(component)\n\n    @staticmethod\n    def test_str(component):\n        assert ""Component HGPSC 084"" in str(component)\n\n    @staticmethod\n    def test_name(component):\n        assert component.name == ""HGPSC 084""\n\n    @staticmethod\n    def test_index(component):\n        assert component.row_index == 83\n\n    @staticmethod\n    def test_spatial_model(component):\n        model = component.spatial_model()\n        assert model.frame == ""galactic""\n        p = model.parameters\n        assert_allclose(p[""lon_0""].value, 28.77037811279297)\n        assert_allclose(p[""lon_0""].error, 0.058748625218868256)\n        assert_allclose(p[""lat_0""].value, -0.0727819949388504)\n        assert_allclose(p[""lat_0""].error, 0.06880396604537964)\n        assert_allclose(p[""sigma""].value, 0.2294706553220749)\n        assert_allclose(p[""sigma""].error, 0.04618723690509796)\n\n\nclass TestSourceCatalogLargeScaleHGPS:\n    def setup(self):\n        table = Table()\n        table[""GLON""] = [-30, -10, 10, 20] * u.deg\n        table[""Surface_Brightness""] = [0, 1, 10, 0] * u.Unit(""cm-2 s-1 sr-1"")\n        table[""GLAT""] = [-1, 0, 1, 0] * u.deg\n        table[""Width""] = [0.4, 0.5, 0.3, 1.0] * u.deg\n        self.table = table\n        self.model = SourceCatalogLargeScaleHGPS(table)\n\n    def test_evaluate(self):\n        x = np.linspace(-100, 20, 5)\n        y = np.linspace(-2, 2, 7)\n        x, y = np.meshgrid(x, y)\n        coords = SkyCoord(x, y, unit=""deg"", frame=""galactic"")\n        image = self.model.evaluate(coords)\n        desired = 1.223962643740966 * u.Unit(""cm-2 s-1 sr-1"")\n        assert_quantity_allclose(image.sum(), desired)\n\n    def test_parvals(self):\n        glon = Angle(10, unit=""deg"")\n        assert_quantity_allclose(\n            self.model.peak_brightness(glon), 10 * u.Unit(""cm-2 s-1 sr-1"")\n        )\n        assert_quantity_allclose(self.model.peak_latitude(glon), 1 * u.deg)\n        assert_quantity_allclose(self.model.width(glon), 0.3 * u.deg)\n'"
gammapy/data/tests/__init__.py,0,b''
gammapy/data/tests/test_data_store.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport os\nfrom pathlib import Path\nimport pytest\nfrom gammapy.data import DataStore\nfrom gammapy.utils.testing import requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef data_store():\n    return DataStore.from_dir(""$GAMMAPY_DATA/hess-dl3-dr1/"")\n\n\n@requires_data()\ndef test_datastore_hd_hap(data_store):\n    """"""Test HESS HAP-HD data access.""""""\n    obs = data_store.obs(obs_id=23523)\n\n    assert obs.events.__class__.__name__ == ""EventList""\n    assert obs.gti.__class__.__name__ == ""GTI""\n    assert obs.aeff.__class__.__name__ == ""EffectiveAreaTable2D""\n    assert obs.edisp.__class__.__name__ == ""EnergyDispersion2D""\n    assert obs.psf.__class__.__name__ == ""PSF3D""\n\n\n@requires_data()\ndef test_datastore_from_dir():\n    """"""Test the `from_dir` method.""""""\n    data_store_rel_path = DataStore.from_dir(\n        ""$GAMMAPY_DATA/hess-dl3-dr1/"", ""hdu-index.fits.gz"", ""obs-index.fits.gz""\n    )\n\n    data_store_abs_path = DataStore.from_dir(\n        ""$GAMMAPY_DATA/hess-dl3-dr1/"",\n        ""$GAMMAPY_DATA/hess-dl3-dr1/hdu-index.fits.gz"",\n        ""$GAMMAPY_DATA/hess-dl3-dr1/obs-index.fits.gz"",\n    )\n\n    assert ""Data store"" in data_store_rel_path.info(show=False)\n    assert ""Data store"" in data_store_abs_path.info(show=False)\n\n\n@requires_data()\ndef test_datastore_from_file():\n    filename = ""$GAMMAPY_DATA/hess-dl3-dr1/hdu-index.fits.gz""\n    data_store = DataStore.from_file(filename)\n    obs = data_store.obs(obs_id=23523)\n    # Check that things can be loaded:\n    obs.events\n    obs.bkg\n\n\n@requires_data()\ndef test_datastore_from_events():\n    # Test that `DataStore.from_events_files` works.\n    # The real tests for `DataStoreMaker` are below.\n    path = ""$GAMMAPY_DATA/cta-1dc/data/baseline/gps/gps_baseline_110380.fits""\n    data_store = DataStore.from_events_files([path])\n    assert len(data_store.obs_table) == 1\n    assert len(data_store.hdu_table) == 6\n\n\n@requires_data()\ndef test_datastore_get_observations(data_store):\n    """"""Test loading data and IRF files via the DataStore""""""\n    observations = data_store.get_observations([23523, 23592])\n    assert observations[0].obs_id == 23523\n\n    # Test that default is all observations\n    observations = data_store.get_observations()\n    assert len(observations) == 105\n\n    with pytest.raises(ValueError):\n        data_store.get_observations([11111, 23592])\n\n    observations = data_store.get_observations([11111, 23523], skip_missing=True)\n    assert observations[0].obs_id == 23523\n\n\n@requires_data()\ndef test_datastore_copy_obs(tmp_path, data_store):\n    data_store.copy_obs([23523, 23592], tmp_path, overwrite=True)\n\n    substore = DataStore.from_dir(tmp_path)\n\n    assert str(substore.hdu_table.base_dir) == str(tmp_path)\n    assert len(substore.obs_table) == 2\n\n    desired = data_store.obs(23523)\n    actual = substore.obs(23523)\n\n    assert str(actual.events.table) == str(desired.events.table)\n\n\n@requires_data()\ndef test_datastore_copy_obs_subset(tmp_path, data_store):\n    # Copy only certain HDU classes\n    data_store.copy_obs([23523, 23592], tmp_path, hdu_class=[""events""])\n\n    substore = DataStore.from_dir(tmp_path)\n    assert len(substore.hdu_table) == 2\n\n\n@requires_data()\nclass TestDataStoreChecker:\n    def setup(self):\n        self.data_store = DataStore.from_dir(""$GAMMAPY_DATA/cta-1dc/index/gps"")\n\n    def test_check_all(self):\n        records = list(self.data_store.check())\n        assert len(records) == 32\n\n\n@requires_data(""gammapy-data"")\nclass TestDataStoreMaker:\n    def setup(self):\n        paths = [\n            f""$GAMMAPY_DATA/cta-1dc/data/baseline/gps/gps_baseline_{obs_id:06d}.fits""\n            for obs_id in [110380, 111140, 111630, 111159]\n        ]\n        self.data_store = DataStore.from_events_files(paths)\n\n        # Useful for debugging:\n        # self.data_store.hdu_table.write(""hdu-index.fits.gz"", overwrite=True)\n        # self.data_store.obs_table.write(""obs-index.fits.gz"", overwrite=True)\n\n    def test_obs_table(self):\n        table = self.data_store.obs_table\n        assert table.__class__.__name__ == ""ObservationTable""\n        assert len(table) == 4\n        assert len(table.colnames) == 24\n\n        # TODO: implement https://github.com/gammapy/gammapy/issues/1218 and add tests here\n        # assert table.time_start[0].iso == ""spam""\n        # assert table.time_start[-1].iso == ""spam""\n\n    def test_hdu_table(self):\n        table = self.data_store.hdu_table\n        assert table.__class__.__name__ == ""HDUIndexTable""\n        assert len(table) == 24\n        hdu_class = [""events"", ""gti"", ""aeff_2d"", ""edisp_2d"", ""psf_3gauss"", ""bkg_3d""]\n        assert list(self.data_store.hdu_table[""HDU_CLASS""]) == 4 * hdu_class\n\n        assert table[""FILE_DIR""][2] == ""$CALDB/data/cta/1dc/bcf/South_z20_50h""\n\n    def test_observation(self, monkeypatch):\n        """"""Check that one observation can be accessed OK""""""\n        obs = self.data_store.obs(110380)\n        assert obs.obs_id == 110380\n\n        assert obs.events.time[0].iso == ""2021-01-21 12:00:03.045""\n        assert obs.gti.time_start[0].iso == ""2021-01-21 12:00:00.000""\n\n        # Note: IRF access requires the CALDB env var\n        caldb_path = Path(os.environ[""GAMMAPY_DATA""]) / Path(""cta-1dc/caldb"")\n        monkeypatch.setenv(""CALDB"", str(caldb_path))\n\n        assert obs.aeff.__class__.__name__ == ""EffectiveAreaTable2D""\n        assert obs.bkg.__class__.__name__ == ""Background3D""\n        assert obs.edisp.__class__.__name__ == ""EnergyDispersion2D""\n        assert obs.psf.__class__.__name__ == ""EnergyDependentMultiGaussPSF""\n'"
gammapy/data/tests/test_event_list.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom numpy.testing import assert_allclose\nfrom astropy import units as u\nfrom astropy.coordinates import SkyCoord\nfrom astropy.table import Table\nfrom regions import CircleSkyRegion, RectangleSkyRegion\nfrom gammapy.data import EventList, EventListBase, EventListLAT\nfrom gammapy.maps import Map, MapAxis, WcsGeom\nfrom gammapy.utils.testing import mpl_plot_check, requires_data, requires_dependency\n\n\n@requires_data()\nclass TestEventListBase:\n    def setup_class(self):\n        self.events = EventListBase.read(\n            ""$GAMMAPY_DATA/hess-dl3-dr1/data/hess_dl3_dr1_obs_id_020136.fits.gz""\n        )\n\n    def test_select_parameter(self):\n        events = self.events.select_parameter(""ENERGY"", (0.8 * u.TeV, 5.0 * u.TeV))\n        assert len(events.table) == 2716\n\n\n@requires_data()\nclass TestEventListHESS:\n    def setup_class(self):\n        self.events = EventList.read(\n            ""$GAMMAPY_DATA/hess-dl3-dr1/data/hess_dl3_dr1_obs_id_020136.fits.gz""\n        )\n\n    def test_basics(self):\n        assert ""EventList"" in str(self.events)\n\n        assert len(self.events.table) == 11243\n        assert self.events.time[0].iso == ""2004-03-26 02:57:47.004""\n        assert self.events.radec[0].to_string() == ""229.239 -58.3417""\n        assert self.events.galactic[0].to_string(precision=2) == ""321.07 -0.69""\n        assert self.events.altaz[0].to_string() == ""193.338 53.258""\n        assert_allclose(self.events.offset[0].value, 0.54000974, rtol=1e-5)\n\n        energy = self.events.energy[0]\n        assert energy.unit == ""TeV""\n        assert_allclose(energy.value, 0.55890286)\n\n        lon, lat, height = self.events.observatory_earth_location.to_geodetic()\n        assert lon.unit == ""deg""\n        assert_allclose(lon.value, 16.5002222222222)\n        assert lat.unit == ""deg""\n        assert_allclose(lat.value, -23.2717777777778)\n        assert height.unit == ""m""\n        assert_allclose(height.value, 1835)\n\n    def test_observation_time_duration(self):\n        dt = self.events.observation_time_duration\n        assert dt.unit == ""s""\n        assert_allclose(dt.value, 1682)\n\n    def test_observation_live_time_duration(self):\n        dt = self.events.observation_live_time_duration\n        assert dt.unit == ""s""\n        assert_allclose(dt.value, 1521.026855)\n\n    def test_observation_dead_time_fraction(self):\n        deadc = self.events.observation_dead_time_fraction\n        assert_allclose(deadc, 0.095703, rtol=1e-3)\n\n    def test_altaz(self):\n        altaz = self.events.altaz\n        assert_allclose(altaz[0].az.deg, 193.337965, atol=1e-3)\n        assert_allclose(altaz[0].alt.deg, 53.258024, atol=1e-3)\n        # TODO: add asserts for frame properties\n\n    def test_stack(self):\n        event_lists = [self.events] * 2\n        stacked_list = EventList.stack(event_lists)\n        assert len(stacked_list.table) == 11243 * 2\n\n    def test_offset_selection(self):\n        offset_range = u.Quantity([0.5, 1.0]*u.deg)\n        new_list = self.events.select_offset(offset_range)\n        assert len(new_list.table) == 1820\n\n    @requires_dependency(""matplotlib"")\n    def test_plot_time(self):\n        with mpl_plot_check():\n            self.events.plot_time()\n\n    @requires_dependency(""matplotlib"")\n    def test_plot_energy(self):\n        with mpl_plot_check():\n            self.events.plot_energy()\n\n    @requires_dependency(""matplotlib"")\n    def test_plot_offset2_distribution(self):\n        with mpl_plot_check():\n            self.events.plot_offset2_distribution()\n\n    @requires_dependency(""matplotlib"")\n    def test_plot_energy_offset(self):\n        with mpl_plot_check():\n            self.events.plot_energy_offset()\n\n    @requires_dependency(""matplotlib"")\n    def test_plot_image(self):\n        with mpl_plot_check():\n            self.events.plot_image()\n\n    @requires_dependency(""matplotlib"")\n    def test_peek(self):\n        with mpl_plot_check():\n            self.events.peek()\n\n\n@requires_data()\nclass TestEventListFermi:\n    def setup_class(self):\n        self.events = EventListLAT.read(\n            ""$GAMMAPY_DATA/fermi-3fhl-gc/fermi-3fhl-gc-events.fits.gz""\n        )\n\n    def test_basics(self):\n        assert ""EventList"" in str(self.events)\n        assert len(self.events.table) == 32843\n\n    @requires_dependency(""matplotlib"")\n    def test_plot_image(self):\n        with mpl_plot_check():\n            self.events.plot_image()\n\n\n@requires_data()\nclass TestEventListChecker:\n    def setup_class(self):\n        self.event_list = EventList.read(\n            ""$GAMMAPY_DATA/cta-1dc/data/baseline/gps/gps_baseline_111140.fits""\n        )\n\n    def test_check_all(self):\n        records = list(self.event_list.check())\n        assert len(records) == 3\n\n\nclass TestEventSelection:\n    def setup_class(self):\n        table = Table()\n        table[""RA""] = [0.0, 0.0, 0.0, 10.0] * u.deg\n        table[""DEC""] = [0.0, 0.9, 10.0, 10.0] * u.deg\n        table[""ENERGY""] = [1.0, 1.5, 1.5, 10.0] * u.TeV\n        table[""OFFSET""] = [0.1, 0.5, 1.0, 1.5] * u.deg\n\n        self.events = EventListBase(table)\n\n        center1 = SkyCoord(0.0, 0.0, frame=""icrs"", unit=""deg"")\n        on_region1 = CircleSkyRegion(center1, radius=1.0 * u.deg)\n        center2 = SkyCoord(0.0, 10.0, frame=""icrs"", unit=""deg"")\n        on_region2 = RectangleSkyRegion(center2, width=0.5 * u.deg, height=0.3 * u.deg)\n        self.on_regions = [on_region1, on_region2]\n\n    def test_region_select(self):\n        geom = WcsGeom.create(skydir=(0, 0), binsz=0.2, width=4.0 * u.deg, proj=""TAN"")\n        new_list = self.events.select_region(self.on_regions[0], geom.wcs)\n        assert len(new_list.table) == 2\n\n        union_region = self.on_regions[0].union(self.on_regions[1])\n        new_list = self.events.select_region(union_region, geom.wcs)\n        assert len(new_list.table) == 3\n\n        region_string = ""fk5;box(0,10, 0.25, 0.15)""\n        new_list = self.events.select_region(region_string, geom.wcs)\n        assert len(new_list.table) == 1\n\n    def test_map_select(self):\n        axis = MapAxis.from_edges((0.5, 2.0), unit=""TeV"", name=""ENERGY"")\n        geom = WcsGeom.create(\n            skydir=(0, 0), binsz=0.2, width=4.0 * u.deg, proj=""TAN"", axes=[axis]\n        )\n\n        mask_data = geom.region_mask(regions=[self.on_regions[0]], inside=True)\n        mask = Map.from_geom(geom, data=mask_data)\n        new_list = self.events.select_map_mask(mask)\n        assert len(new_list.table) == 2\n\n    def test_select_energy(self):\n        energy_band = u.Quantity([1, 10], \'TeV\')\n        new_list = self.events.select_energy(energy_band)\n        assert len(new_list.table) == 3\n'"
gammapy/data/tests/test_filters.py,3,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom astropy import units as u\nfrom astropy.coordinates import Angle, SkyCoord\nfrom astropy.time import Time\nfrom astropy.units import Quantity\nfrom gammapy.data import GTI, DataStore, EventListBase, ObservationFilter\nfrom gammapy.utils.regions import SphericalCircleSkyRegion\nfrom gammapy.utils.testing import assert_time_allclose, requires_data\n\n\ndef test_event_filter_types():\n    for method_str in ObservationFilter.EVENT_FILTER_TYPES.values():\n        assert hasattr(EventListBase, method_str)\n\n\n@pytest.fixture(scope=""session"")\ndef observation():\n    ds = DataStore.from_dir(""$GAMMAPY_DATA/hess-dl3-dr1/"")\n    return ds.obs(20136)\n\n\n@requires_data()\ndef test_empty_observation_filter(observation):\n    empty_obs_filter = ObservationFilter()\n\n    events = observation.events\n    filtered_events = empty_obs_filter.filter_events(events)\n    assert filtered_events == events\n\n    gti = observation.gti\n    filtered_gti = empty_obs_filter.filter_gti(gti)\n    assert filtered_gti == gti\n\n\n@requires_data()\ndef test_filter_events(observation):\n    custom_filter = {\n        ""type"": ""custom"",\n        ""opts"": {""parameter"": ""ENERGY"", ""band"": Quantity([0.8 * u.TeV, 10.0 * u.TeV])},\n    }\n\n    target_position = SkyCoord(ra=229.2, dec=-58.3, unit=""deg"", frame=""icrs"")\n    region_radius = Angle(""0.2 deg"")\n    region = SphericalCircleSkyRegion(center=target_position, radius=region_radius)\n    region_filter = {""type"": ""sky_region"", ""opts"": {""region"": region}}\n\n    time_filter = Time([53090.12, 53090.13], format=""mjd"", scale=""tt"")\n\n    obs_filter = ObservationFilter(\n        event_filters=[custom_filter, region_filter], time_filter=time_filter\n    )\n\n    events = observation.events\n    filtered_events = obs_filter.filter_events(events)\n\n    assert np.all(\n        (filtered_events.energy >= 0.8 * u.TeV)\n        & (filtered_events.energy < 10.0 * u.TeV)\n    )\n    assert np.all(\n        (filtered_events.time >= time_filter[0])\n        & (filtered_events.time < time_filter[1])\n    )\n    assert np.all(region.center.separation(filtered_events.radec) < region_radius)\n\n\n@requires_data()\ndef test_filter_gti(observation):\n    time_filter = Time([53090.125, 53090.130], format=""mjd"", scale=""tt"")\n\n    obs_filter = ObservationFilter(time_filter=time_filter)\n\n    gti = observation.gti\n    filtered_gti = obs_filter.filter_gti(gti)\n\n    assert isinstance(filtered_gti, GTI)\n    assert_time_allclose(filtered_gti.time_start, time_filter[0])\n    assert_time_allclose(filtered_gti.time_stop, time_filter[1])\n'"
gammapy/data/tests/test_gti.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.table import Table\nfrom astropy.time import Time\nfrom gammapy.data import GTI\nfrom gammapy.utils.testing import assert_time_allclose, requires_data\nfrom gammapy.utils.time import time_ref_to_dict\n\n\n@requires_data()\ndef test_gti_hess():\n    gti = GTI.read(""$GAMMAPY_DATA/hess-dl3-dr1/data/hess_dl3_dr1_obs_id_020136.fits.gz"")\n    assert ""GTI"" in str(gti)\n    assert len(gti.table) == 1\n\n    assert gti.time_delta[0].unit == ""s""\n    assert_allclose(gti.time_delta[0].value, 1682)\n    assert_allclose(gti.time_sum.value, 1682)\n\n    expected = Time(53090.123451203704, format=""mjd"", scale=""tt"")\n    assert_time_allclose(gti.time_start[0], expected)\n\n    expected = Time(53090.14291879629, format=""mjd"", scale=""tt"")\n    assert_time_allclose(gti.time_stop[0], expected)\n\n\n@requires_data()\ndef test_gti_fermi():\n    gti = GTI.read(""$GAMMAPY_DATA/fermi-3fhl-gc/fermi-3fhl-gc-events.fits.gz"")\n    assert ""GTI"" in str(gti)\n    assert len(gti.table) == 39042\n\n    assert gti.time_delta[0].unit == ""s""\n    assert_allclose(gti.time_delta[0].value, 651.598893)\n    assert_allclose(gti.time_sum.value, 1.831396e08)\n\n    expected = Time(54682.65603794185, format=""mjd"", scale=""tt"")\n    assert_time_allclose(gti.time_start[0], expected)\n\n    expected = Time(54682.66357959571, format=""mjd"", scale=""tt"")\n    assert_time_allclose(gti.time_stop[0], expected)\n\n\n@requires_data()\n@pytest.mark.parametrize(\n    ""time_interval, expected_length, expected_times"",\n    [\n        (\n            Time(\n                [""2008-08-04T16:21:00"", ""2008-08-04T19:10:00""],\n                format=""isot"",\n                scale=""tt"",\n            ),\n            2,\n            Time(\n                [""2008-08-04T16:21:00"", ""2008-08-04T19:10:00""],\n                format=""isot"",\n                scale=""tt"",\n            ),\n        ),\n        (\n            Time([54682.68125, 54682.79861111], format=""mjd"", scale=""tt""),\n            2,\n            Time([54682.68125, 54682.79861111], format=""mjd"", scale=""tt""),\n        ),\n        (\n            Time([10.0, 100000.0], format=""mjd"", scale=""tt""),\n            39042,\n            Time([54682.65603794185, 57236.96833546296], format=""mjd"", scale=""tt""),\n        ),\n        (Time([10.0, 20.0], format=""mjd"", scale=""tt""), 0, None),\n    ],\n)\ndef test_select_time(time_interval, expected_length, expected_times):\n    gti = GTI.read(""$GAMMAPY_DATA/fermi-3fhl-gc/fermi-3fhl-gc-events.fits.gz"")\n\n    gti_selected = gti.select_time(time_interval)\n\n    assert len(gti_selected.table) == expected_length\n\n    if expected_length != 0:\n        expected_times.format = ""mjd""\n        assert_time_allclose(gti_selected.time_start[0], expected_times[0])\n        assert_time_allclose(gti_selected.time_stop[-1], expected_times[1])\n\n\ndef make_gti(times, time_ref=""2010-01-01""):\n    meta = time_ref_to_dict(time_ref)\n    table = Table(times, meta=meta)\n    return GTI(table)\n\n\ndef test_gti_stack():\n    time_ref = Time(""2010-01-01"")\n    gti1 = make_gti({""START"": [0, 2], ""STOP"": [1, 3]}, time_ref=time_ref)\n    gti2 = make_gti({""START"": [4], ""STOP"": [5]}, time_ref=time_ref + 10 * u.s)\n\n    gti = gti1.stack(gti2)\n\n    assert_time_allclose(gti.time_ref, gti1.time_ref)\n    assert_allclose(gti.table[""START""], [0, 2, 14])\n    assert_allclose(gti.table[""STOP""], [1, 3, 15])\n\n\ndef test_gti_union():\n    gti = make_gti({""START"": [5, 6, 1, 2], ""STOP"": [8, 7, 3, 4]})\n\n    gti = gti.union()\n\n    assert_allclose(gti.table[""START""], [1, 5])\n    assert_allclose(gti.table[""STOP""], [4, 8])\n\n\ndef test_gti_create():\n    start = u.Quantity([1, 2], ""min"")\n    stop = u.Quantity([1.5, 2.5], ""min"")\n    time_ref = Time(""2010-01-01 00:00:00.0"")\n\n    gti = GTI.create(start, stop, time_ref)\n\n    assert len(gti.table) == 2\n    assert_allclose(gti.time_ref.mjd, time_ref.tt.mjd)\n    assert_allclose(gti.table[""START""], start.to_value(""s""))\n\n\ndef test_gti_write(tmp_path):\n    gti = make_gti({""START"": [5, 6, 1, 2], ""STOP"": [8, 7, 3, 4]})\n\n    gti.write(tmp_path / ""tmp.fits"")\n    new_gti = GTI.read(tmp_path / ""tmp.fits"")\n\n    assert_time_allclose(new_gti.time_start, gti.time_start)\n    assert_time_allclose(new_gti.time_stop, gti.time_stop)\n    assert new_gti.table.meta[""MJDREFF""] == gti.table.meta[""MJDREFF""]\n'"
gammapy/data/tests/test_hdu_index_table.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom pathlib import Path\nimport pytest\nfrom gammapy.data import HDUIndexTable\nfrom gammapy.utils.scripts import make_path\nfrom gammapy.utils.testing import requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef hdu_index_table():\n    table = HDUIndexTable(\n        rows=[\n            {\n                ""OBS_ID"": 42,\n                ""HDU_TYPE"": ""events"",\n                ""HDU_CLASS"": ""spam42"",\n                ""FILE_DIR"": ""a"",\n                ""FILE_NAME"": ""b"",\n                ""HDU_NAME"": ""c"",\n            }\n        ]\n    )\n    table.meta[""BASE_DIR""] = ""spam""\n    return table\n\n\ndef test_hdu_index_table(hdu_index_table):\n    """"""\n    This test ensures that the HDUIndexTable works in a case-insensitive\n    way concerning the values in the ``HDU_CLASS`` and ``HDU_TYPE`` columns.\n\n    We ended up with this, because initially the HDU index table spec used\n    lower-case, but then the ``HDUCLAS`` header keys to all the HDUs\n    (e.g. EVENTS, IRFs, ...) were defined in upper-case.\n\n    So for consistency we changed to all upper-case in the spec also for the HDU\n    index table, just with a mention that values should be treated in a case-insensitive\n    manner for backward compatibility with existing index tables.\n\n    See https://github.com/open-gamma-ray-astro/gamma-astro-data-formats/issues/118\n    """"""\n    location = hdu_index_table.hdu_location(obs_id=42, hdu_type=""events"")\n    assert location.path().as_posix() == ""spam/a/b""\n\n    location = hdu_index_table.hdu_location(obs_id=42, hdu_type=""bkg"")\n    assert location is None\n\n    assert hdu_index_table.summary().startswith(""HDU index table"")\n\n\n@requires_data()\ndef test_hdu_index_table_hd_hap(capfd):\n    """"""Test HESS HAP-HD data access.""""""\n    hdu_index = HDUIndexTable.read(""$GAMMAPY_DATA/hess-dl3-dr1/hdu-index.fits.gz"")\n\n    assert ""BASE_DIR"" in hdu_index.meta\n    assert hdu_index.base_dir == make_path(""$GAMMAPY_DATA/hess-dl3-dr1"")\n\n    # A few valid queries\n\n    location = hdu_index.hdu_location(obs_id=23523, hdu_type=""events"")\n\n    # We test here the std out\n    location.info()\n    out, err = capfd.readouterr()\n    lines = out.split(\'\\n\')\n    assert len(lines) == 8\n\n    hdu = location.get_hdu()\n    assert hdu.name == ""EVENTS""\n\n    # The next line is to check if the HDU is still accessible\n    # See https://github.com/gammapy/gammapy/issues/1775\n    assert hdu.filebytes() == 224640\n\n    assert location.path(abs_path=False) == Path(\n        ""data/hess_dl3_dr1_obs_id_023523.fits.gz""\n    )\n    path1 = str(location.path(abs_path=True))\n    path2 = str(location.path(abs_path=False))\n    assert path1.endswith(path2)\n\n    location = hdu_index.hdu_location(obs_id=23523, hdu_class=""psf_table"")\n    assert location.path(abs_path=False) == Path(\n        ""data/hess_dl3_dr1_obs_id_023523.fits.gz""\n    )\n\n    location = hdu_index.hdu_location(obs_id=23523, hdu_type=""psf"")\n    assert location.path(abs_path=False) == Path(\n        ""data/hess_dl3_dr1_obs_id_023523.fits.gz""\n    )\n\n    location = hdu_index.hdu_location(obs_id=23523, hdu_type=""bkg"")\n    assert location.path(abs_path=False) == Path(\n        ""data/hess_dl3_dr1_obs_id_023523.fits.gz""\n    )\n\n    # A few invalid queries\n\n    with pytest.raises(IndexError) as excinfo:\n        hdu_index.hdu_location(obs_id=42, hdu_class=""psf_3gauss"")\n    msg = ""No entry available with OBS_ID = 42""\n    assert str(excinfo.value) == msg\n\n    with pytest.raises(ValueError) as excinfo:\n        hdu_index.hdu_location(obs_id=23523)\n    msg = ""You have to specify `hdu_type` or `hdu_class`.""\n    assert str(excinfo.value) == msg\n\n    with pytest.raises(ValueError) as excinfo:\n        hdu_index.hdu_location(obs_id=23523, hdu_type=""invalid"")\n    msg = ""Invalid hdu_type: invalid. Valid values are: [\'events\', \'gti\', \'aeff\', \'edisp\', \'psf\', \'bkg\']""\n    assert str(excinfo.value) == msg\n\n    with pytest.raises(ValueError) as excinfo:\n        hdu_index.hdu_location(obs_id=23523, hdu_class=""invalid"")\n    msg = ""Invalid hdu_class: invalid. Valid values are: [\'events\', \'gti\', \'aeff_2d\', \'edisp_2d\', \'psf_table\', \'psf_3gauss\', \'psf_king\', \'bkg_2d\', \'bkg_3d\']""\n    assert str(excinfo.value) == msg\n\n    location = hdu_index.hdu_location(obs_id=23523, hdu_class=""psf_king"")\n    assert location == None\n\n    location = hdu_index.hdu_location(obs_id=23523, hdu_class=""bkg_2d"")\n    assert location == None\n'"
gammapy/data/tests/test_obs_table.py,10,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom astropy.coordinates import AltAz, Angle, SkyCoord\nfrom astropy.time import Time, TimeDelta\nfrom astropy.units import Quantity\nfrom gammapy.data import GTI, observatory_locations\nfrom gammapy.data.obs_table import ObservationTable, ObservationTableChecker\nfrom gammapy.utils.random import get_random_state, sample_sphere\nfrom gammapy.utils.testing import (\n    assert_quantity_allclose,\n    assert_time_allclose,\n    requires_data,\n)\nfrom gammapy.utils.time import time_ref_from_dict, time_relative_to_ref\n\n\ndef make_test_observation_table(\n    observatory_name=""hess"",\n    n_obs=10,\n    az_range=Angle([0, 360], ""deg""),\n    alt_range=Angle([45, 90], ""deg""),\n    date_range=(Time(""2010-01-01""), Time(""2015-01-01"")),\n    use_abs_time=False,\n    n_tels_range=(3, 4),\n    random_state=""random-seed"",\n):\n    """"""Make a test observation table.\n    Create an observation table following a specific pattern.\n    For the moment, only random observation tables are created.\n    The observation table is created according to a specific\n    observatory, and randomizing the observation pointingpositions\n    in a specified az-alt range.\n    If a *date_range* is specified, the starting time\n    of the observations will be restricted to the specified interval.\n    These parameters are interpreted as date, the precise hour of the\n    day is ignored, unless the end date is closer than 1 day to the\n    starting date, in which case, the precise time of the day is also\n    considered.\n    In addition, a range can be specified for the number of telescopes.\n    Parameters\n    ----------\n    observatory_name : str, optional\n        Name of the observatory; a list of choices is given in\n        `~gammapy.data.observatory_locations`.\n    n_obs : int, optional\n        Number of observations for the obs table.\n    az_range : `~astropy.coordinates.Angle`, optional\n        Azimuth angle range (start, end) for random generation of\n        observation pointing positions.\n    alt_range : `~astropy.coordinates.Angle`, optional\n        Altitude angle range (start, end) for random generation of\n        observation pointing positions.\n    date_range : `~astropy.time.Time`, optional\n        Date range (start, end) for random generation of observation\n        start time.\n    use_abs_time : bool, optional\n        Use absolute UTC times instead of [MET]_ seconds after the reference.\n    n_tels_range : int, optional\n        Range (start, end) of number of telescopes participating in\n        the observations.\n    random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}, optional\n        Defines random number generator initialisation.\n        Passed to `~gammapy.utils.random.get_random_state`.\n    Returns\n    -------\n    obs_table : `~gammapy.data.ObservationTable`\n        Observation table.\n    """"""\n    random_state = get_random_state(random_state)\n\n    n_obs_start = 1\n\n    obs_table = ObservationTable()\n\n    # build a time reference as the start of 2010\n    dateref = Time(""2010-01-01T00:00:00"")\n    dateref_mjd_fra, dateref_mjd_int = np.modf(dateref.mjd)\n\n    # define table header\n    obs_table.meta[""OBSERVATORY_NAME""] = observatory_name\n    obs_table.meta[""MJDREFI""] = dateref_mjd_int\n    obs_table.meta[""MJDREFF""] = dateref_mjd_fra\n    obs_table.meta[""TIMESYS""] = ""TT""\n    obs_table.meta[""TIMEUNIT""] = ""s""\n    obs_table.meta[""TIMEREF""] = ""LOCAL""\n    if use_abs_time:\n        # show the observation times in UTC\n        obs_table.meta[""TIME_FORMAT""] = ""absolute""\n    else:\n        # show the observation times in seconds after the reference\n        obs_table.meta[""TIME_FORMAT""] = ""relative""\n    header = obs_table.meta\n\n    # obs id\n    obs_id = np.arange(n_obs_start, n_obs_start + n_obs)\n    obs_table[""OBS_ID""] = obs_id\n\n    # obs time: 30 min\n    ontime = Quantity(30.0 * np.ones_like(obs_id), ""minute"").to(""second"")\n    obs_table[""ONTIME""] = ontime\n\n    # livetime: 25 min\n    time_live = Quantity(25.0 * np.ones_like(obs_id), ""minute"").to(""second"")\n    obs_table[""LIVETIME""] = time_live\n\n    # start time\n    #  - random points between the start of 2010 and the end of 2014 (unless\n    # otherwise specified)\n    #  - using the start of 2010 as a reference time for the header of the table\n    #  - observations restrict to night time (only if specified time interval is\n    # more than 1 day)\n    #  - considering start of astronomical day at midday: implicit in setting\n    # the start of the night, when generating random night hours\n    datestart = date_range[0]\n    dateend = date_range[1]\n    time_start = random_state.uniform(datestart.mjd, dateend.mjd, len(obs_id))\n    time_start = Time(time_start, format=""mjd"", scale=""utc"")\n\n    # check if time interval selected is more than 1 day\n    if (dateend - datestart).jd > 1.0:\n        # keep only the integer part (i.e. the day, not the fraction of the day)\n        time_start_f, time_start_i = np.modf(time_start.mjd)\n        time_start = Time(time_start_i, format=""mjd"", scale=""utc"")\n\n        # random generation of night hours: 6 h (from 22 h to 4 h), leaving 1/2 h\n        # time for the last run to finish\n        night_start = Quantity(22.0, ""hour"")\n        night_duration = Quantity(5.5, ""hour"")\n        hour_start = random_state.uniform(\n            night_start.value, night_start.value + night_duration.value, len(obs_id)\n        )\n        hour_start = Quantity(hour_start, ""hour"")\n\n        # add night hour to integer part of MJD\n        time_start += hour_start\n\n    if use_abs_time:\n        # show the observation times in UTC\n        time_start = Time(time_start.isot)\n    else:\n        # show the observation times in seconds after the reference\n        time_start = time_relative_to_ref(time_start, header)\n        # converting to quantity (better treatment of units)\n        time_start = Quantity(time_start.sec, ""second"")\n\n    obs_table[""TSTART""] = time_start\n\n    # stop time\n    # calculated as TSTART + ONTIME\n    if use_abs_time:\n        time_stop = Time(obs_table[""TSTART""])\n        time_stop += TimeDelta(obs_table[""ONTIME""])\n    else:\n        time_stop = TimeDelta(obs_table[""TSTART""])\n        time_stop += TimeDelta(obs_table[""ONTIME""])\n        # converting to quantity (better treatment of units)\n        time_stop = Quantity(time_stop.sec, ""second"")\n\n    obs_table[""TSTOP""] = time_stop\n\n    # az, alt\n    # random points in a portion of sphere; default: above 45 deg altitude\n    az, alt = sample_sphere(\n        size=len(obs_id),\n        lon_range=az_range,\n        lat_range=alt_range,\n        random_state=random_state,\n    )\n    az = Angle(az, ""deg"")\n    alt = Angle(alt, ""deg"")\n    obs_table[""AZ""] = az\n    obs_table[""ALT""] = alt\n\n    # RA, dec\n    # derive from az, alt taking into account that alt, az represent the values\n    # at the middle of the observation, i.e. at time_ref + (TIME_START + TIME_STOP)/2\n    # (or better: time_ref + TIME_START + (TIME_OBSERVATION/2))\n    # in use_abs_time mode, the time_ref should not be added, since it\'s already included\n    # in TIME_START and TIME_STOP\n    az = Angle(obs_table[""AZ""])\n    alt = Angle(obs_table[""ALT""])\n    if use_abs_time:\n        obstime = Time(obs_table[""TSTART""])\n        obstime += TimeDelta(obs_table[""ONTIME""]) / 2.0\n    else:\n        obstime = time_ref_from_dict(obs_table.meta)\n        obstime += TimeDelta(obs_table[""TSTART""])\n        obstime += TimeDelta(obs_table[""ONTIME""]) / 2.0\n    location = observatory_locations[observatory_name]\n    altaz_frame = AltAz(obstime=obstime, location=location)\n    alt_az_coord = SkyCoord(az, alt, frame=altaz_frame)\n    sky_coord = alt_az_coord.transform_to(""icrs"")\n    obs_table[""RA_PNT""] = sky_coord.ra\n    obs_table[""DEC_PNT""] = sky_coord.dec\n\n    # positions\n\n    # number of telescopes\n    # random integers in a specified range; default: between 3 and 4\n    n_tels = random_state.randint(n_tels_range[0], n_tels_range[1] + 1, len(obs_id))\n    obs_table[""N_TELS""] = n_tels\n\n    # muon efficiency\n    # random between 0.6 and 1.0\n    muoneff = random_state.uniform(low=0.6, high=1.0, size=len(obs_id))\n    obs_table[""MUONEFF""] = muoneff\n\n    return obs_table\n\n\ndef test_basics():\n    random_state = np.random.RandomState(seed=0)\n    obs_table = make_test_observation_table(n_obs=10, random_state=random_state)\n    assert obs_table.summary().startswith(""Observation table"")\n\n\ndef test_select_parameter_box():\n    # create random observation table\n    random_state = np.random.RandomState(seed=0)\n    obs_table = make_test_observation_table(n_obs=10, random_state=random_state)\n\n    # select some pars and check the corresponding values in the columns\n\n    # test box selection in obs_id\n    variable = ""OBS_ID""\n    value_range = [2, 5]\n    selection = dict(type=""par_box"", variable=variable, value_range=value_range)\n    selected_obs_table = obs_table.select_observations(selection)\n    assert len(selected_obs_table) == 3\n    assert (value_range[0] <= selected_obs_table[variable]).all()\n    assert (selected_obs_table[variable] < value_range[1]).all()\n\n    # test box selection in obs_id inverted\n    selection = dict(\n        type=""par_box"", variable=variable, value_range=value_range, inverted=True\n    )\n    selected_obs_table = obs_table.select_observations(selection)\n    assert len(selected_obs_table) == 7\n    assert (\n        (value_range[0] > selected_obs_table[variable])\n        | (selected_obs_table[variable] >= value_range[1])\n    ).all()\n\n    # test box selection in alt\n    variable = ""ALT""\n    value_range = Angle([60.0, 70.0], ""deg"")\n    selection = dict(type=""par_box"", variable=variable, value_range=value_range)\n    selected_obs_table = obs_table.select_observations(selection)\n    assert (value_range[0] < Angle(selected_obs_table[variable])).all()\n    assert (Angle(selected_obs_table[variable]) < value_range[1]).all()\n\n\ndef test_select_time_box():\n    # create random observation table with very close (in time)\n    # observations (and times in absolute times)\n    datestart = Time(""2012-01-01T00:30:00"")\n    dateend = Time(""2012-01-01T02:30:00"")\n    random_state = np.random.RandomState(seed=0)\n    obs_table_time = make_test_observation_table(\n        n_obs=10,\n        date_range=(datestart, dateend),\n        use_abs_time=False,\n        random_state=random_state,\n    )\n\n    # test box selection in time: (time_start, time_stop) within value_range\n    value_range = Time([""2012-01-01T01:00:00"", ""2012-01-01T02:00:00""])\n    selection = dict(type=""time_box"", time_range=value_range)\n    selected_obs_table = obs_table_time.select_observations(selection)\n    time_start = selected_obs_table.time_start\n    time_stop = selected_obs_table.time_stop\n\n    assert (value_range[0] < time_start).all() & (time_stop < value_range[1]).all()\n\n    # Test selection with partial overlap of observations and value_range\n    selection_partial = dict(\n        type=""time_box"", time_range=value_range, partial_overlap=True\n    )\n    selected_obs_table = obs_table_time.select_observations(selection_partial)\n    time_start = selected_obs_table.time_start\n    time_stop = selected_obs_table.time_stop\n\n    assert (value_range[1] > time_start).all() & (time_stop > value_range[0]).all()\n\n\ndef test_select_sky_regions():\n    random_state = np.random.RandomState(seed=0)\n    obs_table = make_test_observation_table(n_obs=100, random_state=random_state)\n\n    selection = dict(\n        type=""sky_circle"",\n        frame=""galactic"",\n        lon=""0 deg"",\n        lat=""0 deg"",\n        radius=""50 deg"",\n        border=""2 deg"",\n    )\n    obs_table = obs_table.select_observations(selection)\n    assert len(obs_table) == 32\n\n    selection = dict(\n        type=""sky_circle"", frame=""galactic"", lon=""0 deg"", lat=""0 deg"", radius=""50 deg""\n    )\n    obs_table = obs_table.select_observations(selection)\n    assert len(obs_table) == 30\n\n\ndef test_create_gti():\n    date_start = Time(""2012-01-01T00:30:00"")\n    date_end = Time(""2012-01-01T02:30:00"")\n    random_state = np.random.RandomState(seed=0)\n    obs_table = make_test_observation_table(\n        n_obs=1, date_range=(date_start, date_end), random_state=random_state\n    )\n\n    gti = obs_table.create_gti(obs_id=1)\n\n    met_ref = time_ref_from_dict(obs_table.meta)\n    time_start = met_ref + Quantity(obs_table[0][""TSTART""].astype(""float64""), ""second"")\n    time_stop = met_ref + Quantity(obs_table[0][""TSTOP""].astype(""float64""), ""second"")\n\n    assert isinstance(gti, GTI)\n    assert_time_allclose(gti.time_start, time_start)\n    assert_time_allclose(gti.time_stop, time_stop)\n    assert_quantity_allclose(\n        gti.time_sum, Quantity(obs_table[0][""ONTIME""].astype(""float64""), ""second"")\n    )\n\n\n@requires_data()\ndef test_observation_table_checker():\n    path = ""$GAMMAPY_DATA/cta-1dc/index/gps/obs-index.fits.gz""\n    obs_table = ObservationTable.read(path)\n    checker = ObservationTableChecker(obs_table)\n\n    records = list(checker.run())\n    assert len(records) == 1\n'"
gammapy/data/tests/test_observations.py,3,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nfrom astropy.time import Time\nfrom gammapy.data import DataStore, Observation\nfrom gammapy.irf import load_cta_irfs\nfrom gammapy.utils.testing import (\n    assert_skycoord_allclose,\n    assert_time_allclose,\n    mpl_plot_check,\n    requires_data,\n    requires_dependency,\n)\n\n\n@pytest.fixture(scope=""session"")\ndef data_store():\n    return DataStore.from_dir(""$GAMMAPY_DATA/hess-dl3-dr1/"")\n\n\n@requires_data()\ndef test_observation(data_store):\n    """"""Test Observation class""""""\n    obs = data_store.obs(23523)\n\n    assert_time_allclose(obs.tstart, Time(53343.92234009259, scale=""tt"", format=""mjd""))\n    assert_time_allclose(obs.tstop, Time(53343.94186555556, scale=""tt"", format=""mjd""))\n\n    c = SkyCoord(83.63333129882812, 21.51444435119629, unit=""deg"")\n    assert_skycoord_allclose(obs.pointing_radec, c)\n\n    c = SkyCoord(22.481705, 41.38979, unit=""deg"")\n    assert_skycoord_allclose(obs.pointing_altaz, c)\n\n    c = SkyCoord(83.63333129882812, 22.01444435119629, unit=""deg"")\n    assert_skycoord_allclose(obs.target_radec, c)\n\n\n@requires_dependency(""matplotlib"")\n@requires_data()\ndef test_observation_peek(data_store):\n    obs = data_store.obs(23523)\n\n    with mpl_plot_check():\n        obs.peek()\n\n    obs.bkg = None\n    with mpl_plot_check():\n        obs.peek()\n\n\n@requires_data()\n@pytest.mark.parametrize(\n    ""time_interval, expected_times"",\n    [\n        (\n            Time(\n                [""2004-12-04T22:10:00"", ""2004-12-04T22:30:00""],\n                format=""isot"",\n                scale=""tt"",\n            ),\n            Time(\n                [""2004-12-04T22:10:00"", ""2004-12-04T22:30:00""],\n                format=""isot"",\n                scale=""tt"",\n            ),\n        ),\n        (\n            Time([53343.930, 53343.940], format=""mjd"", scale=""tt""),\n            Time([53343.930, 53343.940], format=""mjd"", scale=""tt""),\n        ),\n        (\n            Time([10.0, 100000.0], format=""mjd"", scale=""tt""),\n            Time([53343.92234009, 53343.94186563], format=""mjd"", scale=""tt""),\n        ),\n        (Time([10.0, 20.0], format=""mjd"", scale=""tt""), None),\n    ],\n)\ndef test_observation_select_time(data_store, time_interval, expected_times):\n    obs = data_store.obs(23523)\n\n    new_obs = obs.select_time(time_interval)\n\n    if expected_times:\n        expected_times.format = ""mjd""\n        assert np.all(\n            (new_obs.events.time >= expected_times[0])\n            & (new_obs.events.time < expected_times[1])\n        )\n        assert_time_allclose(new_obs.gti.time_start[0], expected_times[0], atol=0.01)\n        assert_time_allclose(new_obs.gti.time_stop[-1], expected_times[1], atol=0.01)\n    else:\n        assert len(new_obs.events.table) == 0\n        assert len(new_obs.gti.table) == 0\n\n\n@requires_data()\n@pytest.mark.parametrize(\n    ""time_interval, expected_times, expected_nr_of_obs"",\n    [\n        (\n            Time([53090.130, 53090.140], format=""mjd"", scale=""tt""),\n            Time([53090.130, 53090.140], format=""mjd"", scale=""tt""),\n            1,\n        ),\n        (\n            Time([53090.130, 53091.110], format=""mjd"", scale=""tt""),\n            Time([53090.130, 53091.110], format=""mjd"", scale=""tt""),\n            3,\n        ),\n        (\n            Time([10.0, 53111.0230], format=""mjd"", scale=""tt""),\n            Time([53090.1234512, 53111.0230], format=""mjd"", scale=""tt""),\n            8,\n        ),\n        (Time([10.0, 20.0], format=""mjd"", scale=""tt""), None, 0),\n    ],\n)\ndef test_observations_select_time(\n    data_store, time_interval, expected_times, expected_nr_of_obs\n):\n    obs_ids = data_store.obs_table[""OBS_ID""][:8]\n    obss = data_store.get_observations(obs_ids)\n\n    new_obss = obss.select_time(time_interval)\n\n    assert len(new_obss) == expected_nr_of_obs\n\n    if expected_nr_of_obs > 0:\n        assert new_obss[0].events.time[0] >= expected_times[0]\n        assert new_obss[-1].events.time[-1] < expected_times[1]\n        assert_time_allclose(\n            new_obss[0].gti.time_start[0], expected_times[0], atol=0.01\n        )\n        assert_time_allclose(\n            new_obss[-1].gti.time_stop[-1], expected_times[1], atol=0.01\n        )\n\n\n@requires_data()\ndef test_observations_mutation(data_store):\n    obs_ids = data_store.obs_table[""OBS_ID""][:4]\n    obss = data_store.get_observations(obs_ids)\n    assert obss.ids == [""20136"", ""20137"", ""20151"", ""20275""]\n\n    obs_id = data_store.obs_table[""OBS_ID""][4]\n    obs = data_store.get_observations([obs_id])[0]\n\n    obss.append(obs)\n    assert obss.ids == [""20136"", ""20137"", ""20151"", ""20275"", ""20282""]\n    obss.insert(0, obs)\n    assert obss.ids == [""20282"", ""20136"", ""20137"", ""20151"", ""20275"", ""20282""]\n    obss.pop(0)\n    assert obss.ids == [""20136"", ""20137"", ""20151"", ""20275"", ""20282""]\n    obs3 = obss[3]\n    obss.pop(obss.ids[3])\n    assert obss.ids == [""20136"", ""20137"", ""20151"", ""20282""]\n    obss.insert(3, obs3)\n    assert obss.ids == [""20136"", ""20137"", ""20151"", ""20275"", ""20282""]\n    obss.extend([obs])\n    assert obss.ids == [""20136"", ""20137"", ""20151"", ""20275"", ""20282"", ""20282""]\n    obss.remove(obs)\n    assert obss.ids == [""20136"", ""20137"", ""20151"", ""20275"", ""20282""]\n    obss[0]= obs\n    assert obss.ids == [""20282"", ""20137"", ""20151"", ""20275"", ""20282""]\n\n    with pytest.raises(TypeError):\n        obss.insert(5,\'bad\')\n\n    with pytest.raises(TypeError):\n        obss[5]=\'bad\'\n\n    with pytest.raises(TypeError):\n        obss[[\'1\',\'2\']]\n\n\n@requires_data()\ndef test_observations_str(data_store):\n    obs_ids = data_store.obs_table[""OBS_ID""][:4]\n    obss = data_store.get_observations(obs_ids)\n    actual = obss.__str__()\n\n    assert actual.split(\'\\n\')[1] == ""Number of observations: 4""\n\n@requires_data()\ndef test_observations_select_time_time_intervals_list(data_store):\n    obs_ids = data_store.obs_table[""OBS_ID""][:8]\n    obss = data_store.get_observations(obs_ids)\n    # third time interval is out of the observations time range\n    time_intervals = [\n        Time([53090.130, 53090.140], format=""mjd"", scale=""tt""),\n        Time([53110.011, 53110.019], format=""mjd"", scale=""tt""),\n        Time([53112.345, 53112.42], format=""mjd"", scale=""tt""),\n    ]\n    new_obss = obss.select_time(time_intervals)\n\n    assert len(new_obss) == 2\n    assert new_obss[0].events.time[0] >= time_intervals[0][0]\n    assert new_obss[0].events.time[-1] < time_intervals[0][1]\n    assert new_obss[1].events.time[0] >= time_intervals[1][0]\n    assert new_obss[1].events.time[-1] < time_intervals[1][1]\n    assert_time_allclose(new_obss[0].gti.time_start[0], time_intervals[0][0])\n    assert_time_allclose(new_obss[0].gti.time_stop[-1], time_intervals[0][1])\n    assert_time_allclose(new_obss[1].gti.time_start[0], time_intervals[1][0])\n    assert_time_allclose(new_obss[1].gti.time_stop[-1], time_intervals[1][1])\n\n\n@requires_data()\ndef test_observation():\n    livetime = 5.0 * u.hr\n    pointing = SkyCoord(0, 0, unit=""deg"", frame=""galactic"")\n    irfs = load_cta_irfs(\n        ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits""\n    )\n\n    obs = Observation.create(\n        pointing, livetime=livetime, irfs=irfs, deadtime_fraction=0.1\n    )\n\n    assert_skycoord_allclose(obs.pointing_radec, pointing.icrs)\n    assert_allclose(obs.observation_live_time_duration, 0.9 * livetime)\n    assert_allclose(obs.target_radec.ra, np.nan)\n    assert_allclose(obs.pointing_zen, np.nan)\n    assert_allclose(obs.muoneff, 1)\n\n\n@requires_data()\nclass TestObservationChecker:\n    def setup(self):\n        self.data_store = DataStore.from_dir(""$GAMMAPY_DATA/cta-1dc/index/gps"")\n\n    def test_check_all(self):\n        observation = self.data_store.obs(111140)\n        records = list(observation.check())\n        assert len(records) == 8\n\n\n    def test_checker_bad(self):\n        for index in range(5):\n            self.data_store.hdu_table[index][\'FILE_NAME\']=\'bad\'\n\n        observation = self.data_store.obs(110380)\n        records = list(observation.check())\n        assert len(records) == 10\n        assert records[1][\'msg\'] == \'Loading events failed\'\n        assert records[3][\'msg\'] == \'Loading GTI failed\'\n        assert records[5][\'msg\'] == \'Loading aeff failed\'\n        assert records[7][\'msg\'] == \'Loading edisp failed\'\n        assert records[9][\'msg\'] == \'Loading psf failed\'\n\n'"
gammapy/data/tests/test_observers.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom numpy.testing import assert_allclose\nfrom astropy.coordinates import Angle\nfrom gammapy.data import observatory_locations\n\n\ndef test_observatory_locations():\n    location = observatory_locations[""hess""]\n    assert_allclose(location.lon.deg, Angle(""16d30m00.8s"").deg)\n    assert_allclose(location.lat.deg, Angle(""-23d16m18.4s"").deg)\n    assert_allclose(location.height.value, 1835)\n    assert str(location.height.unit) == ""m""\n'"
gammapy/data/tests/test_pointing.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom numpy.testing import assert_allclose\nfrom astropy.time import Time\nfrom gammapy.data import FixedPointingInfo, PointingInfo\nfrom gammapy.utils.testing import assert_time_allclose, requires_data\n\n\n@requires_data()\nclass TestFixedPointingInfo:\n    @classmethod\n    def setup_class(cls):\n        filename = ""$GAMMAPY_DATA/tests/pointing_table.fits.gz""\n        cls.fpi = FixedPointingInfo.read(filename)\n\n    def test_location(self):\n        lon, lat, height = self.fpi.location.geodetic\n        assert_allclose(lon.deg, 16.5002222222222)\n        assert_allclose(lat.deg, -23.2717777777778)\n        assert_allclose(height.value, 1834.999999999783)\n\n    def test_time_ref(self):\n        expected = Time(51910.00074287037, format=""mjd"", scale=""tt"")\n        assert_time_allclose(self.fpi.time_ref, expected)\n\n    def test_time_start(self):\n        time = self.fpi.time_start\n        expected = Time(53025.826414166666, format=""mjd"", scale=""tt"")\n        assert_time_allclose(time, expected)\n\n    def test_time_stop(self):\n        time = self.fpi.time_stop\n        expected = Time(53025.844770648146, format=""mjd"", scale=""tt"")\n        assert_time_allclose(time, expected)\n\n    def test_duration(self):\n        duration = self.fpi.duration\n        assert_allclose(duration.sec, 1586.0000000044238)\n\n    def test_radec(self):\n        pos = self.fpi.radec\n        assert_allclose(pos.ra.deg, 83.633333333333)\n        assert_allclose(pos.dec.deg, 24.51444444)\n        assert pos.name == ""icrs""\n\n    def test_altaz(self):\n        pos = self.fpi.altaz\n        assert_allclose(pos.az.deg, 7.48272)\n        assert_allclose(pos.alt.deg, 41.84191)\n        assert pos.name == ""altaz""\n\n\n@requires_data()\nclass TestPointingInfo:\n    @classmethod\n    def setup_class(cls):\n        filename = ""$GAMMAPY_DATA/tests/pointing_table.fits.gz""\n        cls.pointing_info = PointingInfo.read(filename)\n\n    def test_str(self):\n        ss = str(self.pointing_info)\n        assert ""Pointing info"" in ss\n\n    def test_location(self):\n        lon, lat, height = self.pointing_info.location.geodetic\n        assert_allclose(lon.deg, 16.5002222222222)\n        assert_allclose(lat.deg, -23.2717777777778)\n        assert_allclose(height.value, 1834.999999999783)\n\n    def test_time_ref(self):\n        expected = Time(51910.00074287037, format=""mjd"", scale=""tt"")\n        assert_time_allclose(self.pointing_info.time_ref, expected)\n\n    def test_table(self):\n        assert len(self.pointing_info.table) == 100\n\n    def test_time(self):\n        time = self.pointing_info.time\n        assert len(time) == 100\n        expected = Time(53025.826414166666, format=""mjd"", scale=""tt"")\n        assert_time_allclose(time[0], expected)\n\n    def test_duration(self):\n        duration = self.pointing_info.duration\n        assert_allclose(duration.sec, 1586.0000000044238)\n\n    def test_radec(self):\n        pos = self.pointing_info.radec[0]\n        assert_allclose(pos.ra.deg, 83.633333333333)\n        assert_allclose(pos.dec.deg, 24.51444444)\n        assert pos.name == ""icrs""\n\n    def test_altaz(self):\n        pos = self.pointing_info.altaz[0]\n        assert_allclose(pos.az.deg, 11.45751357)\n        assert_allclose(pos.alt.deg, 41.34088901)\n        assert pos.name == ""altaz""\n\n    def test_altaz_from_table(self):\n        pos = self.pointing_info.altaz_from_table[0]\n        assert_allclose(pos.az.deg, 11.20432353385406)\n        assert_allclose(pos.alt.deg, 41.37921408774436)\n        assert pos.name == ""altaz""\n\n    def test_altaz_interpolate(self):\n        time = self.pointing_info.time[0]\n        pos = self.pointing_info.altaz_interpolate(time)\n        assert_allclose(pos.az.deg, 11.45751357)\n        assert_allclose(pos.alt.deg, 41.34088901)\n        assert pos.name == ""altaz""\n'"
gammapy/datasets/tests/__init__.py,0,b''
gammapy/datasets/tests/test_datasets.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nfrom gammapy.datasets import Datasets\nfrom gammapy.modeling.tests.test_fit import MyDataset\n\n\n@pytest.fixture(scope=""session"")\ndef datasets():\n    return Datasets([MyDataset(name=""test-1""), MyDataset(name=""test-2"")])\n\n\ndef test_datasets_init(datasets):\n    # Passing a Python list of `Dataset` objects should work\n    Datasets(list(datasets))\n\n    # Passing an existing `Datasets` object should work\n    Datasets(datasets)\n\n\ndef test_datasets_types(datasets):\n    assert datasets.is_all_same_type\n\n\ndef test_datasets_likelihood(datasets):\n    likelihood = datasets.stat_sum()\n    assert_allclose(likelihood, 0)\n\n\ndef test_datasets_str(datasets):\n    assert ""Datasets"" in str(datasets)\n\n\ndef test_datasets_getitem(datasets):\n    assert datasets[""test-1""].name == ""test-1""\n    assert datasets[""test-2""].name == ""test-2""\n\n\ndef test_names(datasets):\n    assert datasets.names == [""test-1"", ""test-2""]\n\n\ndef test_Datasets_mutation():\n    dat = MyDataset(name=""test-1"")\n    dats = Datasets([MyDataset(name=""test-2""), MyDataset(name=""test-3"")])\n    dats2 = Datasets([MyDataset(name=""test-4""), MyDataset(name=""test-5"")])\n\n    dats.insert(0, dat)\n    assert dats.names == [""test-1"", ""test-2"", ""test-3""]\n\n    dats.extend(dats2)\n    assert dats.names == [""test-1"", ""test-2"", ""test-3"", ""test-4"", ""test-5""]\n\n    dat3 = dats[3]\n    dats.remove(dats[3])\n    assert dats.names == [""test-1"", ""test-2"", ""test-3"", ""test-5""]\n    dats.append(dat3)\n    assert dats.names == [""test-1"", ""test-2"", ""test-3"", ""test-5"", ""test-4""]\n    dats.pop(3)\n    assert dats.names == [""test-1"", ""test-2"", ""test-3"", ""test-4""]\n\n    with pytest.raises(ValueError, match=""Dataset names must be unique""):\n        dats.append(dat)\n    with pytest.raises(ValueError, match=""Dataset names must be unique""):\n        dats.insert(0, dat)\n    with pytest.raises(ValueError, match=""Dataset names must be unique""):\n        dats.extend(dats2)\n'"
gammapy/datasets/tests/test_flux_points.py,3,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom gammapy.datasets import Datasets, FluxPointsDataset\nfrom gammapy.estimators import FluxPoints\nfrom gammapy.modeling import Fit\nfrom gammapy.modeling.models import PowerLawSpectralModel, SkyModel\nfrom gammapy.utils.testing import mpl_plot_check, requires_data, requires_dependency\n\n\n@pytest.fixture()\ndef fit(dataset):\n    return Fit([dataset])\n\n\n@pytest.fixture()\ndef dataset():\n    path = ""$GAMMAPY_DATA/tests/spectrum/flux_points/diff_flux_points.fits""\n    data = FluxPoints.read(path)\n    data.table[""e_ref""] = data.e_ref.to(""TeV"")\n    model = SkyModel(\n        spectral_model=PowerLawSpectralModel(\n            index=2.3, amplitude=""2e-13 cm-2 s-1 TeV-1"", reference=""1 TeV""\n        )\n    )\n    dataset = FluxPointsDataset(model, data)\n    return dataset\n\n\n@requires_data()\ndef test_flux_point_dataset_serialization(tmp_path):\n    path = ""$GAMMAPY_DATA/tests/spectrum/flux_points/diff_flux_points.fits""\n    data = FluxPoints.read(path)\n    data.table[""e_ref""] = data.e_ref.to(""TeV"")\n    spectral_model = PowerLawSpectralModel(\n        index=2.3, amplitude=""2e-13 cm-2 s-1 TeV-1"", reference=""1 TeV""\n    )\n    model = SkyModel(spectral_model=spectral_model, name=""test_model"")\n    dataset = FluxPointsDataset(model, data, name=""test_dataset"")\n\n    Datasets([dataset]).write(tmp_path, prefix=""tmp"")\n    datasets = Datasets.read(tmp_path, ""tmp_datasets.yaml"", ""tmp_models.yaml"")\n    new_dataset = datasets[0]\n    assert_allclose(new_dataset.data.table[""dnde""], dataset.data.table[""dnde""], 1e-4)\n    if dataset.mask_fit is None:\n        assert np.all(new_dataset.mask_fit == dataset.mask_safe)\n    assert np.all(new_dataset.mask_safe == dataset.mask_safe)\n    assert new_dataset.name == ""test_dataset""\n\n\n@requires_data()\ndef test_flux_point_dataset_str(dataset):\n    assert ""FluxPointsDataset"" in str(dataset)\n\n\n@requires_data()\nclass TestFluxPointFit:\n    @requires_dependency(""iminuit"")\n    def test_fit_pwl_minuit(self, fit):\n        result = fit.run(backend=""minuit"")\n        self.assert_result(result)\n\n    @requires_dependency(""sherpa"")\n    def test_fit_pwl_sherpa(self, fit):\n        result = fit.optimize(backend=""sherpa"", method=""simplex"")\n        self.assert_result(result)\n\n    @staticmethod\n    def assert_result(result):\n        assert result.success\n        assert_allclose(result.total_stat, 25.2059, rtol=1e-3)\n\n        index = result.parameters[""index""]\n        assert_allclose(index.value, 2.216, rtol=1e-3)\n\n        amplitude = result.parameters[""amplitude""]\n        assert_allclose(amplitude.value, 2.1616e-13, rtol=1e-3)\n\n        reference = result.parameters[""reference""]\n        assert_allclose(reference.value, 1, rtol=1e-8)\n\n    @staticmethod\n    @requires_dependency(""iminuit"")\n    def test_stat_profile(fit):\n\n        result = fit.run(backend=""minuit"")\n\n        profile = fit.stat_profile(""amplitude"", nvalues=3, bounds=1)\n\n        ts_diff = profile[""stat""] - result.total_stat\n        assert_allclose(ts_diff, [110.1, 0, 110.1], rtol=1e-2, atol=1e-7)\n\n        value = result.parameters[""amplitude""].value\n        err = result.parameters[""amplitude""].error\n        values = np.array([value - err, value, value + err])\n\n        profile = fit.stat_profile(""amplitude"", values=values)\n\n        ts_diff = profile[""stat""] - result.total_stat\n        assert_allclose(ts_diff, [110.1, 0, 110.1], rtol=1e-2, atol=1e-7)\n\n    @staticmethod\n    @requires_dependency(""matplotlib"")\n    def test_fp_dataset_peek(fit):\n        fp_dataset = fit.datasets[0]\n\n        with mpl_plot_check():\n            fp_dataset.peek(method=""diff/model"")\n'"
gammapy/datasets/tests/test_io.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom numpy.testing import assert_allclose\nfrom gammapy.datasets import Datasets\nfrom gammapy.modeling.models import Models\nfrom gammapy.utils.testing import requires_data, requires_dependency\nfrom gammapy.modeling import Fit\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_datasets_to_io(tmp_path):\n    path = ""$GAMMAPY_DATA/tests/models""\n    filedata = ""gc_example_datasets.yaml""\n    filemodel = ""gc_example_models.yaml""\n\n    datasets = Datasets.read(path, filedata, filemodel)\n\n    assert len(datasets) == 2\n    print(list(datasets.models))\n    assert len(datasets.models) == 5\n    dataset0 = datasets[0]\n    assert dataset0.name == ""gc""\n    assert dataset0.counts.data.sum() == 6824\n    assert_allclose(dataset0.exposure.data.sum(), 2072125400000.0, atol=0.1)\n    assert dataset0.psf is not None\n    assert dataset0.edisp is not None\n\n    assert_allclose(dataset0.background_model.evaluate().data.sum(), 4094.2, atol=0.1)\n\n    assert dataset0.background_model.name == ""background_irf_gc""\n\n    dataset1 = datasets[1]\n    assert dataset1.name == ""g09""\n    assert dataset1.background_model.name == ""background_irf_g09""\n\n    assert (\n        dataset0.models[""gll_iem_v06_cutout""] == dataset1.models[""gll_iem_v06_cutout""]\n    )\n\n    assert isinstance(dataset0.models, Models)\n    assert len(dataset0.models) == 3\n    assert dataset0.models[0].name == ""gc""\n    assert dataset0.models[1].name == ""gll_iem_v06_cutout""\n    assert dataset0.models[2].name == ""background_irf_gc""\n\n    assert (\n        dataset0.models[""background_irf_gc""].parameters[""norm""]\n        is dataset1.models[""background_irf_g09""].parameters[""norm""]\n    )\n\n    assert (\n        dataset0.models[""gc""].parameters[""reference""]\n        is dataset1.models[""g09""].parameters[""reference""]\n    )\n    assert_allclose(dataset1.models[""g09""].parameters[""lon_0""].value, 0.9, atol=0.1)\n\n    datasets.write(tmp_path, prefix=""written"")\n    datasets_read = Datasets.read(\n        tmp_path, ""written_datasets.yaml"", ""written_models.yaml""\n    )\n\n    assert len(datasets.parameters) == 21\n\n    assert len(datasets_read) == 2\n    dataset0 = datasets_read[0]\n    assert dataset0.counts.data.sum() == 6824\n    assert_allclose(dataset0.exposure.data.sum(), 2072125400000.0, atol=0.1)\n    assert dataset0.psf is not None\n    assert dataset0.edisp is not None\n    assert_allclose(dataset0.background_model.evaluate().data.sum(), 4094.2, atol=0.1)\n\n    Fit(datasets).run()\n    assert_allclose(\n        datasets.models[""background_irf_g09""].covariance,\n        datasets.models[""background_irf_gc""].covariance,\n    )\n\n    dataset_copy = dataset0.copy(name=""dataset0-copy"")\n    assert dataset_copy.background_model.datasets_names == [""dataset0-copy""]\n'"
gammapy/datasets/tests/test_map.py,36,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nfrom regions import CircleSkyRegion\nfrom gammapy.data import GTI\nfrom gammapy.datasets import Datasets, MapDataset, MapDatasetOnOff\nfrom gammapy.irf import (\n    EDispMap,\n    EDispKernelMap,\n    EDispKernel,\n    EffectiveAreaTable2D,\n    EnergyDependentMultiGaussPSF,\n    PSFMap,\n)\nfrom gammapy.makers.utils import make_map_exposure_true_energy\nfrom gammapy.maps import Map, MapAxis, WcsGeom, WcsNDMap\nfrom gammapy.modeling import Fit\nfrom gammapy.modeling.models import (\n    BackgroundModel,\n    GaussianSpatialModel,\n    Models,\n    PowerLawSpectralModel,\n    SkyModel,\n)\nfrom gammapy.utils.testing import mpl_plot_check, requires_data, requires_dependency\n\n\n@pytest.fixture\ndef geom():\n    axis = MapAxis.from_energy_bounds(""0.1 TeV"", ""10 TeV"", nbin=2)\n    return WcsGeom.create(\n        skydir=(266.40498829, -28.93617776),\n        binsz=0.02,\n        width=(2, 2),\n        frame=""icrs"",\n        axes=[axis],\n    )\n\n\n@pytest.fixture\ndef geom_etrue():\n    axis = MapAxis.from_energy_bounds(""0.1 TeV"", ""10 TeV"", nbin=3, name=""energy_true"")\n    return WcsGeom.create(\n        skydir=(266.40498829, -28.93617776),\n        binsz=0.02,\n        width=(2, 2),\n        frame=""icrs"",\n        axes=[axis],\n    )\n\n\n@pytest.fixture\ndef geom_image():\n    ebounds_true = np.logspace(-1.0, 1.0, 2)\n    axis = MapAxis.from_edges(ebounds_true, name=""energy"", unit=u.TeV, interp=""log"")\n    return WcsGeom.create(\n        skydir=(0, 0), binsz=0.02, width=(2, 2), frame=""galactic"", axes=[axis]\n    )\n\n\ndef get_exposure(geom_etrue):\n    filename = (\n        ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits""\n    )\n    aeff = EffectiveAreaTable2D.read(filename, hdu=""EFFECTIVE AREA"")\n\n    exposure_map = make_map_exposure_true_energy(\n        pointing=SkyCoord(1, 0.5, unit=""deg"", frame=""galactic""),\n        livetime=""1 hour"",\n        aeff=aeff,\n        geom=geom_etrue,\n    )\n    return exposure_map\n\n\ndef get_psf():\n    filename = (\n        ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits""\n    )\n    psf = EnergyDependentMultiGaussPSF.read(filename, hdu=""POINT SPREAD FUNCTION"")\n\n    table_psf = psf.to_energy_dependent_table_psf(theta=0.5 * u.deg)\n    psf_map = PSFMap.from_energy_dependent_table_psf(table_psf)\n    return psf_map\n\n\n@pytest.fixture\ndef sky_model():\n    spatial_model = GaussianSpatialModel(\n        lon_0=""0.2 deg"", lat_0=""0.1 deg"", sigma=""0.2 deg"", frame=""galactic""\n    )\n    spectral_model = PowerLawSpectralModel(\n        index=3, amplitude=""1e-11 cm-2 s-1 TeV-1"", reference=""1 TeV""\n    )\n    return SkyModel(spatial_model=spatial_model, spectral_model=spectral_model)\n\n\ndef get_map_dataset(\n    sky_model, geom, geom_etrue, edisp=""edispmap"", name=""test"", **kwargs\n):\n    """"""Returns a MapDatasets""""""\n    # define background model\n    m = Map.from_geom(geom)\n    m.quantity = 0.2 * np.ones(m.data.shape)\n    background_model = BackgroundModel(m, datasets_names=[name])\n\n    psf = get_psf()\n    exposure = get_exposure(geom_etrue)\n\n    e_reco = geom.get_axis_by_name(""energy"")\n    e_true = geom_etrue.get_axis_by_name(""energy_true"")\n\n    if edisp == ""edispmap"":\n        edisp = EDispMap.from_diagonal_response(energy_axis_true=e_true)\n    elif edisp == ""edispkernelmap"":\n        edisp = EDispKernelMap.from_diagonal_response(\n            energy_axis=e_reco, energy_axis_true=e_true\n        )\n    elif edisp == ""edispkernel"":\n        edisp = EDispKernel.from_diagonal_response(\n            e_true=e_true.edges, e_reco=e_reco.edges\n        )\n    else:\n        edisp = None\n\n    # define fit mask\n    center = sky_model.spatial_model.position\n    circle = CircleSkyRegion(center=center, radius=1 * u.deg)\n    mask_fit = background_model.map.geom.region_mask([circle])\n    mask_fit = Map.from_geom(geom, data=mask_fit)\n\n    return MapDataset(\n        models=[sky_model, background_model],\n        exposure=exposure,\n        psf=psf,\n        edisp=edisp,\n        mask_fit=mask_fit,\n        name=name,\n        **kwargs\n    )\n\n\n@requires_data()\ndef test_map_dataset_str(sky_model, geom, geom_etrue):\n    dataset = get_map_dataset(sky_model, geom, geom_etrue)\n    dataset.counts = dataset.npred()\n    dataset.mask_safe = dataset.mask_fit\n    assert ""MapDataset"" in str(dataset)\n    assert ""(frozen)"" in str(dataset)\n    assert ""background"" in str(dataset)\n\n    dataset.mask_safe = None\n    assert ""MapDataset"" in str(dataset)\n\n\n@requires_data()\ndef test_fake(sky_model, geom, geom_etrue):\n    """"""Test the fake dataset""""""\n    dataset = get_map_dataset(sky_model, geom, geom_etrue)\n    npred = dataset.npred()\n    assert np.all(npred.data >= 0)  # npred must be positive\n    dataset.counts = npred\n    real_dataset = dataset.copy()\n    dataset.fake(314)\n\n    assert real_dataset.counts.data.shape == dataset.counts.data.shape\n    assert_allclose(real_dataset.counts.data.sum(), 9525.755807)\n    assert_allclose(dataset.counts.data.sum(), 9723)\n\n\n@pytest.mark.xfail\n@requires_data()\ndef test_different_exposure_unit(sky_model, geom):\n    dataset_ref = get_map_dataset(sky_model, geom, geom, edisp=""None"")\n    npred_ref = dataset_ref.npred()\n\n    ebounds_true = np.logspace(2, 4, 3)\n    axis = MapAxis.from_edges(ebounds_true, name=""energy"", unit=""GeV"", interp=""log"")\n    geom_gev = WcsGeom.create(\n        skydir=(266.40498829, -28.93617776),\n        binsz=0.02,\n        width=(2, 2),\n        frame=""icrs"",\n        axes=[axis],\n    )\n\n    dataset = get_map_dataset(sky_model, geom, geom_gev, edisp=""None"")\n    npred = dataset.npred()\n\n    assert_allclose(npred.data[0, 50, 50], npred_ref.data[0, 50, 50])\n\n\n@pytest.mark.parametrize((""edisp_mode""), [""edispmap"", ""edispkernelmap"", ""edispkernel""])\n@requires_data()\ndef test_to_spectrum_dataset(sky_model, geom, geom_etrue, edisp_mode):\n    dataset_ref = get_map_dataset(sky_model, geom, geom_etrue, edisp=edisp_mode)\n\n    dataset_ref.counts = dataset_ref.background_model.map * 0.0\n    dataset_ref.counts.data[1, 50, 50] = 1\n    dataset_ref.counts.data[1, 60, 50] = 1\n\n    gti = GTI.create([0 * u.s], [1 * u.h], reference_time=""2010-01-01T00:00:00"")\n    dataset_ref.gti = gti\n    on_region = CircleSkyRegion(center=geom.center_skydir, radius=0.05 * u.deg)\n    spectrum_dataset = dataset_ref.to_spectrum_dataset(on_region)\n    spectrum_dataset_corrected = dataset_ref.to_spectrum_dataset(\n        on_region, containment_correction=True\n    )\n\n    assert np.sum(spectrum_dataset.counts.data) == 1\n    assert spectrum_dataset.data_shape == (2, 1, 1)\n    assert spectrum_dataset.background.geom.axes[0].nbin == 2\n    assert spectrum_dataset.aeff.energy.nbin == 3\n    assert spectrum_dataset.aeff.data.data.unit == ""m2""\n    assert spectrum_dataset.edisp.e_reco.nbin == 2\n    assert spectrum_dataset.edisp.e_true.nbin == 3\n    assert spectrum_dataset_corrected.aeff.data.data.unit == ""m2""\n    assert_allclose(spectrum_dataset.aeff.data.data.value[1], 853023.423047, rtol=1e-5)\n    assert_allclose(\n        spectrum_dataset_corrected.aeff.data.data.value[1], 559476.3357, rtol=1e-5\n    )\n\n\n@requires_data()\ndef test_info_dict(sky_model, geom, geom_etrue):\n    dataset = get_map_dataset(sky_model, geom, geom_etrue)\n    dataset.counts = dataset.npred()\n    info_dict = dataset.info_dict()\n\n    assert_allclose(info_dict[""counts""], 9526, rtol=1e-3)\n    assert_allclose(info_dict[""background""], 4000.0)\n    assert_allclose(info_dict[""excess""], 5525.756, rtol=1e-3)\n    assert_allclose(info_dict[""aeff_min""].value, 8.32e8, rtol=1e-3)\n    assert_allclose(info_dict[""aeff_max""].value, 1.105e10, rtol=1e-3)\n    assert info_dict[""aeff_max""].unit == ""m2 s""\n    assert info_dict[""name""] == ""test""\n\n    gti = GTI.create([0 * u.s], [1 * u.h], reference_time=""2010-01-01T00:00:00"")\n    dataset.gti = gti\n    info_dict = dataset.info_dict()\n    assert_allclose(info_dict[""n_on""], 9526, rtol=1e-3)\n    assert_allclose(info_dict[""background""], 4000.0, rtol=1e-3)\n    assert_allclose(info_dict[""significance""], 74.024180, rtol=1e-3)\n    assert_allclose(info_dict[""excess""], 5525.756, rtol=1e-3)\n    assert_allclose(info_dict[""livetime""].value, 3600)\n\n    assert info_dict[""name""] == ""test""\n\n\n@requires_data()\ndef test_to_image(geom):\n\n    counts = Map.read(""$GAMMAPY_DATA/fermi-3fhl-gc/fermi-3fhl-gc-counts-cube.fits.gz"")\n    background = Map.read(\n        ""$GAMMAPY_DATA/fermi-3fhl-gc/fermi-3fhl-gc-background-cube.fits.gz""\n    )\n    background = BackgroundModel(background, datasets_names=[""fermi""])\n\n    exposure = Map.read(\n        ""$GAMMAPY_DATA/fermi-3fhl-gc/fermi-3fhl-gc-exposure-cube.fits.gz""\n    )\n    exposure = exposure.sum_over_axes(keepdims=True)\n    dataset = MapDataset(\n        counts=counts, models=[background], exposure=exposure, name=""fermi""\n    )\n    dataset_im = dataset.to_image()\n    assert dataset_im.mask_safe is None\n    assert dataset_im.counts.data.sum() == dataset.counts.data.sum()\n    assert_allclose(dataset_im.background_model.map.data.sum(), 28548.625, rtol=1e-5)\n\n    ebounds = np.logspace(-1.0, 1.0, 3)\n    axis = MapAxis.from_edges(ebounds, name=""energy"", unit=u.TeV, interp=""log"")\n    geom = WcsGeom.create(\n        skydir=(0, 0), binsz=0.5, width=(1, 1), frame=""icrs"", axes=[axis]\n    )\n    dataset = MapDataset.create(geom)\n\n    # Check map_safe handling\n    data = np.array([[[False, True], [True, True]], [[False, False], [True, True]]])\n    dataset.mask_safe = WcsNDMap.from_geom(geom=geom, data=data)\n\n    dataset_im = dataset.to_image()\n    assert dataset_im.mask_safe.data.dtype == bool\n\n    desired = np.array([[False, True], [True, True]])\n    assert (dataset_im.mask_safe.data == desired).all()\n\n    # Check that missing entries in the dataset do not break\n    dataset_copy = dataset.copy()\n    dataset_copy.exposure = None\n    dataset_copy._background_model = None\n    dataset_im = dataset_copy.to_image()\n    assert dataset_im.exposure is None\n    assert dataset_im.background_model == None\n\n    dataset_copy = dataset.copy()\n    dataset_copy.counts = None\n    dataset_im = dataset_copy.to_image()\n    assert dataset_im.counts is None\n\n\n@requires_data()\ndef test_map_dataset_fits_io(tmp_path, sky_model, geom, geom_etrue):\n    dataset = get_map_dataset(sky_model, geom, geom_etrue)\n    dataset.counts = dataset.npred()\n    dataset.mask_safe = dataset.mask_fit\n    gti = GTI.create([0 * u.s], [1 * u.h], reference_time=""2010-01-01T00:00:00"")\n    dataset.gti = gti\n\n    hdulist = dataset.to_hdulist()\n    actual = [hdu.name for hdu in hdulist]\n\n    desired = [\n        ""PRIMARY"",\n        ""COUNTS"",\n        ""COUNTS_BANDS"",\n        ""EXPOSURE"",\n        ""EXPOSURE_BANDS"",\n        ""BACKGROUND"",\n        ""BACKGROUND_BANDS"",\n        ""EDISP"",\n        ""EDISP_BANDS"",\n        ""EDISP_EXPOSURE"",\n        ""EDISP_EXPOSURE_BANDS"",\n        ""PSF"",\n        ""PSF_BANDS"",\n        ""PSF_EXPOSURE"",\n        ""PSF_EXPOSURE_BANDS"",\n        ""MASK_SAFE"",\n        ""MASK_SAFE_BANDS"",\n        ""MASK_FIT"",\n        ""MASK_FIT_BANDS"",\n        ""GTI"",\n    ]\n\n    assert actual == desired\n\n    dataset.write(tmp_path / ""test.fits"")\n\n    dataset_new = MapDataset.read(tmp_path / ""test.fits"")\n    assert len(dataset_new.models) == 1\n    assert dataset_new.mask.dtype == bool\n\n    assert_allclose(dataset.counts.data, dataset_new.counts.data)\n    assert_allclose(\n        dataset.background_model.map.data, dataset_new.background_model.map.data\n    )\n    assert_allclose(dataset.edisp.edisp_map.data, dataset_new.edisp.edisp_map.data)\n    assert_allclose(dataset.psf.psf_map.data, dataset_new.psf.psf_map.data)\n    assert_allclose(dataset.exposure.data, dataset_new.exposure.data)\n    assert_allclose(dataset.mask_fit.data, dataset_new.mask_fit.data)\n    assert_allclose(dataset.mask_safe.data, dataset_new.mask_safe.data)\n\n    assert dataset.counts.geom == dataset_new.counts.geom\n    assert dataset.exposure.geom == dataset_new.exposure.geom\n    assert dataset.background_model.map.geom == dataset_new.background_model.map.geom\n    assert dataset.edisp.edisp_map.geom == dataset_new.edisp.edisp_map.geom\n\n    assert_allclose(\n        dataset.gti.time_sum.to_value(""s""), dataset_new.gti.time_sum.to_value(""s"")\n    )\n\n    # To test io of psf and edisp map\n    stacked = MapDataset.create(geom)\n    stacked.write(""test.fits"", overwrite=True)\n    stacked1 = MapDataset.read(""test.fits"")\n    assert stacked1.psf.psf_map is not None\n    assert stacked1.psf.exposure_map is not None\n    assert stacked1.edisp.edisp_map is not None\n    assert stacked1.edisp.exposure_map is not None\n    assert stacked.mask.dtype == bool\n\n    assert_allclose(stacked1.psf.psf_map, stacked.psf.psf_map)\n    assert_allclose(stacked1.edisp.edisp_map, stacked.edisp.edisp_map)\n\n\n@requires_dependency(""iminuit"")\n@requires_dependency(""matplotlib"")\n@requires_data()\ndef test_map_fit(sky_model, geom, geom_etrue):\n    dataset_1 = get_map_dataset(\n        sky_model, geom, geom_etrue, evaluation_mode=""local"", name=""test-1""\n    )\n    dataset_1.background_model.norm.value = 0.5\n    dataset_1.counts = dataset_1.npred()\n\n    dataset_2 = get_map_dataset(\n        sky_model, geom, geom_etrue, evaluation_mode=""global"", name=""test-2""\n    )\n    dataset_2.counts = dataset_2.npred()\n\n    sky_model.parameters[""sigma""].frozen = True\n\n    dataset_1.background_model.norm.value = 0.49\n    dataset_2.background_model.norm.value = 0.99\n\n    fit = Fit([dataset_1, dataset_2])\n    result = fit.run()\n\n    assert result.success\n    assert ""minuit"" in repr(result)\n\n    npred = dataset_1.npred().data.sum()\n    assert_allclose(npred, 7525.790688, rtol=1e-3)\n    assert_allclose(result.total_stat, 21700.253246, rtol=1e-3)\n\n    pars = result.parameters\n    assert_allclose(pars[""lon_0""].value, 0.2, rtol=1e-2)\n    assert_allclose(pars[""lon_0""].error, 0.002244, rtol=1e-2)\n\n    assert_allclose(pars[""index""].value, 3, rtol=1e-2)\n    assert_allclose(pars[""index""].error, 0.024277, rtol=1e-2)\n\n    assert_allclose(pars[""amplitude""].value, 1e-11, rtol=1e-2)\n    assert_allclose(pars[""amplitude""].error, 4.216154e-13, rtol=1e-2)\n\n    # background norm 1\n    assert_allclose(pars[8].value, 0.5, rtol=1e-2)\n    assert_allclose(pars[8].error, 0.015811, rtol=1e-2)\n\n    # background norm 2\n    assert_allclose(pars[11].value, 1, rtol=1e-2)\n    assert_allclose(pars[11].error, 0.02147, rtol=1e-2)\n\n    # test mask_safe evaluation\n    mask_safe = geom.energy_mask(emin=1 * u.TeV)\n    dataset_1.mask_safe = Map.from_geom(geom, data=mask_safe)\n    dataset_2.mask_safe = Map.from_geom(geom, data=mask_safe)\n\n    stat = fit.datasets.stat_sum()\n    assert_allclose(stat, 14824.282955)\n\n    # test model evaluation outside image\n\n    dataset_1.models[0].spatial_model.lon_0.value = 150\n    dataset_1.npred()\n    assert not dataset_1._evaluators[dataset_1.models[0]].contributes\n\n    region = sky_model.spatial_model.to_region()\n    with mpl_plot_check():\n        dataset_1.plot_residuals(region=region)\n\n\n@requires_dependency(""iminuit"")\n@requires_data()\ndef test_map_fit_one_energy_bin(sky_model, geom_image):\n    energy_axis = geom_image.get_axis_by_name(""energy"")\n    geom_etrue = geom_image.to_image().to_cube([energy_axis.copy(name=""energy_true"")])\n\n    dataset = get_map_dataset(sky_model, geom_image, geom_etrue)\n    sky_model.spectral_model.index.value = 3.0\n    sky_model.spectral_model.index.frozen = True\n    dataset.background_model.norm.value = 0.5\n\n    dataset.counts = dataset.npred()\n\n    # Move a bit away from the best-fit point, to make sure the optimiser runs\n    sky_model.parameters[""sigma""].value = 0.21\n    dataset.background_model.parameters[""norm""].frozen = True\n\n    fit = Fit([dataset])\n    result = fit.run()\n\n    assert result.success\n\n    npred = dataset.npred().data.sum()\n    assert_allclose(npred, 16538.124036, rtol=1e-3)\n    assert_allclose(result.total_stat, -34844.125047, rtol=1e-3)\n\n    pars = result.parameters\n\n    assert_allclose(pars[""lon_0""].value, 0.2, rtol=1e-2)\n    assert_allclose(pars[""lon_0""].error, 0.001689, rtol=1e-2)\n\n    assert_allclose(pars[""sigma""].value, 0.2, rtol=1e-2)\n    assert_allclose(pars[""sigma""].error, 0.00092, rtol=1e-2)\n\n    assert_allclose(pars[""amplitude""].value, 1e-11, rtol=1e-2)\n    assert_allclose(pars[""amplitude""].error, 8.127593e-14, rtol=1e-2)\n\n\ndef test_create():\n    # tests empty datasets created\n    rad_axis = MapAxis(nodes=np.linspace(0.0, 1.0, 51), unit=""deg"", name=""theta"")\n    e_reco = MapAxis.from_edges(\n        np.logspace(-1.0, 1.0, 3), name=""energy"", unit=u.TeV, interp=""log""\n    )\n    e_true = MapAxis.from_edges(\n        np.logspace(-1.0, 1.0, 4), name=""energy_true"", unit=u.TeV, interp=""log""\n    )\n    geom = WcsGeom.create(binsz=0.02, width=(2, 2), axes=[e_reco])\n    empty_dataset = MapDataset.create(\n        geom=geom, energy_axis_true=e_true, rad_axis=rad_axis\n    )\n\n    assert empty_dataset.counts.data.shape == (2, 100, 100)\n\n    assert empty_dataset.exposure.data.shape == (3, 100, 100)\n\n    assert empty_dataset.psf.psf_map.data.shape == (3, 50, 10, 10)\n    assert empty_dataset.psf.exposure_map.data.shape == (3, 1, 10, 10)\n\n    assert isinstance(empty_dataset.edisp, EDispKernelMap)\n    assert empty_dataset.edisp.edisp_map.data.shape == (3, 2, 10, 10)\n    assert empty_dataset.edisp.exposure_map.data.shape == (3, 1, 10, 10)\n    assert_allclose(empty_dataset.edisp.edisp_map.data.sum(), 300)\n\n    assert_allclose(empty_dataset.gti.time_delta, 0.0 * u.s)\n\n\ndef test_create_with_migra(tmp_path):\n    # tests empty datasets created\n    migra_axis = MapAxis(nodes=np.linspace(0.0, 3.0, 51), unit="""", name=""migra"")\n    rad_axis = MapAxis(nodes=np.linspace(0.0, 1.0, 51), unit=""deg"", name=""theta"")\n    e_reco = MapAxis.from_edges(\n        np.logspace(-1.0, 1.0, 3), name=""energy"", unit=u.TeV, interp=""log""\n    )\n    e_true = MapAxis.from_edges(\n        np.logspace(-1.0, 1.0, 4), name=""energy_true"", unit=u.TeV, interp=""log""\n    )\n    geom = WcsGeom.create(binsz=0.02, width=(2, 2), axes=[e_reco])\n    empty_dataset = MapDataset.create(\n        geom=geom, energy_axis_true=e_true, migra_axis=migra_axis, rad_axis=rad_axis\n    )\n\n    empty_dataset.write(tmp_path / ""test.fits"")\n\n    dataset_new = MapDataset.read(tmp_path / ""test.fits"")\n\n    assert isinstance(empty_dataset.edisp, EDispMap)\n    assert empty_dataset.edisp.edisp_map.data.shape == (3, 50, 10, 10)\n    assert empty_dataset.edisp.exposure_map.data.shape == (3, 1, 10, 10)\n    assert_allclose(empty_dataset.edisp.edisp_map.data.sum(), 300)\n\n    assert_allclose(empty_dataset.gti.time_delta, 0.0 * u.s)\n\n    assert isinstance(dataset_new.edisp, EDispMap)\n    assert dataset_new.edisp.edisp_map.data.shape == (3, 50, 10, 10)\n\n\n@requires_data()\ndef test_stack(geom, geom_etrue):\n    m = Map.from_geom(geom)\n    m.quantity = 0.2 * np.ones(m.data.shape)\n    background_model1 = BackgroundModel(\n        m, name=""dataset-1-bkg"", datasets_names=[""dataset-1""]\n    )\n    c_map1 = Map.from_geom(geom)\n    c_map1.quantity = 0.3 * np.ones(c_map1.data.shape)\n    mask1 = np.ones(m.data.shape, dtype=bool)\n    mask1[0][0][0:10] = False\n    mask1 = Map.from_geom(geom, data=mask1)\n\n    dataset1 = MapDataset(\n        counts=c_map1,\n        models=[background_model1],\n        exposure=get_exposure(geom_etrue),\n        mask_safe=mask1,\n        name=""dataset-1"",\n    )\n\n    c_map2 = Map.from_geom(geom)\n    c_map2.quantity = 0.1 * np.ones(c_map2.data.shape)\n    background_model2 = BackgroundModel(\n        m, norm=0.5, name=""dataset-2-bkg"", datasets_names=[""dataset-2""]\n    )\n    mask2 = np.ones(m.data.shape, dtype=bool)\n    mask2[0][3] = False\n    mask2 = Map.from_geom(geom, data=mask2)\n\n    dataset2 = MapDataset(\n        counts=c_map2,\n        models=[background_model2],\n        exposure=get_exposure(geom_etrue),\n        mask_safe=mask2,\n        name=""dataset-2"",\n    )\n    dataset1.stack(dataset2)\n    assert_allclose(dataset1.counts.data.sum(), 7987)\n    assert_allclose(dataset1.background_model.map.data.sum(), 5987)\n    assert_allclose(dataset1.exposure.data, 2.0 * dataset2.exposure.data)\n    assert_allclose(dataset1.mask_safe.data.sum(), 20000)\n    assert len(dataset1.models) == 1\n\n\n@requires_data()\ndef test_stack_simple_edisp(sky_model, geom, geom_etrue):\n    dataset1 = get_map_dataset(sky_model, geom, geom_etrue, edisp=""edispkernel"")\n    dataset2 = get_map_dataset(sky_model, geom, geom_etrue, edisp=""edispkernel"")\n\n    with pytest.raises(ValueError):\n        dataset1.stack(dataset2)\n\n\ndef to_cube(image):\n    # introduce a fake enery axis for now\n    axis = MapAxis.from_edges([1, 10] * u.TeV, name=""energy"")\n    geom = image.geom.to_cube([axis])\n    return WcsNDMap.from_geom(geom=geom, data=image.data)\n\n\n@pytest.fixture\ndef images():\n    """"""Load some `counts`, `counts_off`, `acceptance_on`, `acceptance_off"" images""""""\n    filename = ""$GAMMAPY_DATA/tests/unbundled/hess/survey/hess_survey_snippet.fits.gz""\n    return {\n        ""counts"": to_cube(WcsNDMap.read(filename, hdu=""ON"")),\n        ""counts_off"": to_cube(WcsNDMap.read(filename, hdu=""OFF"")),\n        ""acceptance"": to_cube(WcsNDMap.read(filename, hdu=""ONEXPOSURE"")),\n        ""acceptance_off"": to_cube(WcsNDMap.read(filename, hdu=""OFFEXPOSURE"")),\n        ""exposure"": to_cube(WcsNDMap.read(filename, hdu=""EXPGAMMAMAP"")),\n        ""background"": to_cube(WcsNDMap.read(filename, hdu=""BACKGROUND"")),\n    }\n\n\ndef get_map_dataset_onoff(images, **kwargs):\n    """"""Returns a MapDatasetOnOff""""""\n    mask_geom = images[""counts""].geom\n    mask_data = np.ones(images[""counts""].data.shape, dtype=bool)\n    mask_safe = Map.from_geom(mask_geom, data=mask_data)\n\n    return MapDatasetOnOff(\n        counts=images[""counts""],\n        counts_off=images[""counts_off""],\n        acceptance=images[""acceptance""],\n        acceptance_off=images[""acceptance_off""],\n        exposure=images[""exposure""],\n        mask_safe=mask_safe,\n        **kwargs\n    )\n\n\n@requires_data()\ndef test_map_dataset_on_off_fits_io(images, tmp_path):\n    dataset = get_map_dataset_onoff(images)\n    gti = GTI.create([0 * u.s], [1 * u.h], reference_time=""2010-01-01T00:00:00"")\n    dataset.gti = gti\n\n    hdulist = dataset.to_hdulist()\n    actual = [hdu.name for hdu in hdulist]\n\n    desired = [\n        ""PRIMARY"",\n        ""COUNTS"",\n        ""COUNTS_BANDS"",\n        ""EXPOSURE"",\n        ""EXPOSURE_BANDS"",\n        ""MASK_SAFE"",\n        ""MASK_SAFE_BANDS"",\n        ""GTI"",\n        ""COUNTS_OFF"",\n        ""COUNTS_OFF_BANDS"",\n        ""ACCEPTANCE"",\n        ""ACCEPTANCE_BANDS"",\n        ""ACCEPTANCE_OFF"",\n        ""ACCEPTANCE_OFF_BANDS"",\n    ]\n\n    assert actual == desired\n\n    dataset.write(tmp_path / ""test.fits"")\n\n    dataset_new = MapDatasetOnOff.read(tmp_path / ""test.fits"")\n    assert len(dataset_new.models) == 0\n    assert dataset_new.mask.dtype == bool\n\n    assert_allclose(dataset.counts.data, dataset_new.counts.data)\n    assert_allclose(dataset.counts_off.data, dataset_new.counts_off.data)\n    assert_allclose(dataset.acceptance.data, dataset_new.acceptance.data)\n    assert_allclose(dataset.acceptance_off.data, dataset_new.acceptance_off.data)\n    assert_allclose(dataset.exposure.data, dataset_new.exposure.data)\n    assert_allclose(dataset.mask_safe, dataset_new.mask_safe)\n\n    assert np.all(dataset.mask_safe.data == dataset_new.mask_safe.data) == True\n    assert dataset.mask_safe.geom == dataset_new.mask_safe.geom\n    assert dataset.counts.geom == dataset_new.counts.geom\n    assert dataset.exposure.geom == dataset_new.exposure.geom\n\n    assert_allclose(\n        dataset.gti.time_sum.to_value(""s""), dataset_new.gti.time_sum.to_value(""s"")\n    )\n\n\ndef test_create_onoff(geom):\n    # tests empty datasets created\n\n    migra_axis = MapAxis(nodes=np.linspace(0.0, 3.0, 51), unit="""", name=""migra"")\n    rad_axis = MapAxis(nodes=np.linspace(0.0, 1.0, 51), unit=""deg"", name=""theta"")\n    energy_axis = geom.get_axis_by_name(""energy"").copy(name=""energy_true"")\n\n    empty_dataset = MapDatasetOnOff.create(geom, energy_axis, migra_axis, rad_axis)\n\n    assert_allclose(empty_dataset.counts.data.sum(), 0.0)\n    assert_allclose(empty_dataset.counts_off.data.sum(), 0.0)\n    assert_allclose(empty_dataset.acceptance.data.sum(), 0.0)\n    assert_allclose(empty_dataset.acceptance_off.data.sum(), 0.0)\n\n    assert empty_dataset.psf.psf_map.data.shape == (2, 50, 10, 10)\n    assert empty_dataset.psf.exposure_map.data.shape == (2, 1, 10, 10)\n\n    assert empty_dataset.edisp.edisp_map.data.shape == (2, 50, 10, 10)\n    assert empty_dataset.edisp.exposure_map.data.shape == (2, 1, 10, 10)\n\n    assert_allclose(empty_dataset.edisp.edisp_map.data.sum(), 200)\n\n    assert_allclose(empty_dataset.gti.time_delta, 0.0 * u.s)\n\n\n@requires_data()\ndef test_map_dataset_onoff_str(images):\n    dataset = get_map_dataset_onoff(images)\n    assert ""MapDatasetOnOff"" in str(dataset)\n\n\n@requires_data()\ndef test_stack_onoff(images):\n    dataset = get_map_dataset_onoff(images)\n    stacked = dataset.copy()\n\n    stacked.stack(dataset)\n\n    assert_allclose(stacked.counts.data.sum(), 2 * dataset.counts.data.sum())\n    assert_allclose(stacked.counts_off.data.sum(), 2 * dataset.counts_off.data.sum())\n    assert_allclose(\n        stacked.acceptance.data.sum(), dataset.data_shape[1] * dataset.data_shape[2]\n    )\n    assert_allclose(\n        np.nansum(stacked.acceptance_off.data),\n        np.nansum(\n            dataset.counts_off.data / (dataset.counts_off.data * dataset.alpha.data)\n        ),\n    )\n    assert_allclose(stacked.exposure.data, 2.0 * dataset.exposure.data)\n\n\n@pytest.mark.xfail\ndef test_stack_onoff_cutout(geom_image):\n    # Test stacking of cutouts\n    dataset = MapDatasetOnOff.create(geom_image)\n    gti = GTI.create([0 * u.s], [1 * u.h], reference_time=""2010-01-01T00:00:00"")\n    dataset.gti = gti\n\n    geom_cutout = geom_image.cutout(position=geom_image.center_skydir, width=1 * u.deg)\n    dataset_cutout = dataset.create(geom_cutout)\n\n    dataset.stack(dataset_cutout)\n\n    assert_allclose(dataset.counts.data.sum(), dataset_cutout.counts.data.sum())\n    assert_allclose(dataset.counts_off.data.sum(), dataset_cutout.counts_off.data.sum())\n    assert_allclose(dataset.alpha.data.sum(), dataset_cutout.alpha.data.sum())\n    assert_allclose(dataset.exposure.data.sum(), dataset_cutout.exposure.data.sum())\n    assert dataset_cutout.name != dataset.name\n\n\ndef test_datasets_io_no_model(tmpdir):\n    axis = MapAxis.from_energy_bounds(""1 TeV"", ""10 TeV"", nbin=2)\n    geom = WcsGeom.create(npix=(5, 5), axes=[axis])\n    dataset_1 = MapDataset.create(geom, name=""1"")\n    dataset_2 = MapDataset.create(geom, name=""2"")\n\n    datasets = Datasets([dataset_1, dataset_2])\n\n    datasets.write(path=tmpdir, prefix=""test"")\n\n    filename_1 = tmpdir / ""test_data_1.fits""\n    assert filename_1.exists()\n\n    filename_2 = tmpdir / ""test_data_2.fits""\n    assert filename_2.exists()\n\n\n@requires_data()\ndef test_map_dataset_on_off_to_spectrum_dataset(images):\n    e_reco = MapAxis.from_bounds(0.1, 10.0, 1, name=""energy"", unit=u.TeV, interp=""log"")\n    new_images = dict()\n    for key, image in images.items():\n        new_images[key] = Map.from_geom(\n            image.geom.to_cube([e_reco]), data=image.data[np.newaxis, :, :]\n        )\n    dataset = get_map_dataset_onoff(new_images)\n    gti = GTI.create([0 * u.s], [1 * u.h], reference_time=""2010-01-01T00:00:00"")\n    dataset.gti = gti\n\n    on_region = CircleSkyRegion(\n        center=dataset.counts.geom.center_skydir, radius=0.1 * u.deg\n    )\n    spectrum_dataset = dataset.to_spectrum_dataset(on_region)\n\n    assert spectrum_dataset.counts.data[0] == 8\n    assert spectrum_dataset.data_shape == (1, 1, 1)\n    assert spectrum_dataset.counts_off.data[0] == 33914\n    assert_allclose(spectrum_dataset.alpha.data[0], 0.0002143, atol=1e-7)\n\n    excess_map = new_images[""counts""] - new_images[""background""]\n    excess_true = excess_map.get_spectrum(on_region, np.sum).data[0]\n\n    excess = spectrum_dataset.excess.data[0]\n    assert_allclose(excess, excess_true, atol=1e-6)\n\n    assert spectrum_dataset.name != dataset.name\n\n\n@requires_data()\ndef test_map_dataset_on_off_cutout(images):\n    dataset = get_map_dataset_onoff(images)\n    gti = GTI.create([0 * u.s], [1 * u.h], reference_time=""2010-01-01T00:00:00"")\n    dataset.gti = gti\n\n    cutout_dataset = dataset.cutout(\n        images[""counts""].geom.center_skydir, [""1 deg"", ""1 deg""]\n    )\n\n    assert cutout_dataset.counts.data.shape == (1, 50, 50)\n    assert cutout_dataset.counts_off.data.shape == (1, 50, 50)\n    assert cutout_dataset.acceptance.data.shape == (1, 50, 50)\n    assert cutout_dataset.acceptance_off.data.shape == (1, 50, 50)\n    assert cutout_dataset.background_model == None\n    assert cutout_dataset.name != dataset.name\n\n\ndef test_map_dataset_on_off_fake(geom):\n    rad_axis = MapAxis(nodes=np.linspace(0.0, 1.0, 51), unit=""deg"", name=""theta"")\n    energy_true_axis = geom.get_axis_by_name(""energy"").copy(name=""energy_true"")\n\n    empty_dataset = MapDataset.create(geom, energy_true_axis, rad_axis=rad_axis)\n    empty_dataset = MapDatasetOnOff.from_map_dataset(\n        empty_dataset, acceptance=1, acceptance_off=10.0\n    )\n\n    empty_dataset.acceptance_off.data[0, 50, 50] = 0\n    background_map = Map.from_geom(geom, data=1)\n    empty_dataset.fake(background_map, random_state=42)\n\n    assert_allclose(empty_dataset.counts.data[0, 50, 50], 0)\n    assert_allclose(empty_dataset.counts.data.mean(), 0.99445, rtol=1e-3)\n    assert_allclose(empty_dataset.counts_off.data.mean(), 10.00055, rtol=1e-3)\n\n\n@requires_data()\ndef test_map_dataset_on_off_to_image():\n    axis = MapAxis.from_energy_bounds(1, 10, 2, unit=""TeV"")\n    geom = WcsGeom.create(npix=(10, 10), binsz=0.05, axes=[axis])\n\n    counts = Map.from_geom(geom, data=np.ones((2, 10, 10)))\n    counts_off = Map.from_geom(geom, data=np.ones((2, 10, 10)))\n    acceptance = Map.from_geom(geom, data=np.ones((2, 10, 10)))\n    acceptance_off = Map.from_geom(geom, data=np.ones((2, 10, 10)))\n    acceptance_off *= 2\n\n    dataset = MapDatasetOnOff(\n        counts=counts,\n        counts_off=counts_off,\n        acceptance=acceptance,\n        acceptance_off=acceptance_off,\n    )\n    image_dataset = dataset.to_image()\n\n    assert image_dataset.counts.data.shape == (1, 10, 10)\n    assert image_dataset.acceptance_off.data.shape == (1, 10, 10)\n    assert_allclose(image_dataset.acceptance, 2)\n    assert_allclose(image_dataset.acceptance_off, 4)\n    assert_allclose(image_dataset.counts_off, 2)\n    assert image_dataset.name != dataset.name\n\n    # Try with a safe_mask\n    mask_safe = Map.from_geom(geom, data=np.ones((2, 10, 10), dtype=""bool""))\n    mask_safe.data[0] = 0\n    dataset.mask_safe = mask_safe\n    image_dataset = dataset.to_image()\n\n    assert_allclose(image_dataset.acceptance, 1)\n    assert_allclose(image_dataset.acceptance_off, 2)\n    assert_allclose(image_dataset.counts_off, 1)\n\n\ndef test_map_dataset_geom(geom, sky_model):\n    e_true = MapAxis.from_energy_bounds(""1 TeV"", ""10 TeV"", nbin=5, name=""energy_true"")\n    dataset = MapDataset.create(geom, energy_axis_true=e_true)\n    dataset.counts = None\n    dataset._background_model = None\n    with pytest.raises(AttributeError):\n        dataset.background_model = None\n\n    dataset.models = sky_model\n\n    npred = dataset.npred()\n    assert npred.geom == geom\n\n    dataset.mask_safe = None\n\n    with pytest.raises(ValueError):\n        dataset._geom\n\n\n@requires_data()\ndef test_names(geom, geom_etrue, sky_model):\n\n    m = Map.from_geom(geom)\n    m.quantity = 0.2 * np.ones(m.data.shape)\n    background_model1 = BackgroundModel(m, name=""bkg1"", datasets_names=[""test""])\n    assert background_model1.name == ""bkg1""\n\n    c_map1 = Map.from_geom(geom)\n    c_map1.quantity = 0.3 * np.ones(c_map1.data.shape)\n\n    model1 = sky_model.copy()\n    assert model1.name != sky_model.name\n    model1 = sky_model.copy(name=""model1"")\n    assert model1.name == ""model1""\n    model2 = sky_model.copy(name=""model2"")\n\n    dataset1 = MapDataset(\n        counts=c_map1,\n        models=Models([model1, model2, background_model1]),\n        exposure=get_exposure(geom_etrue),\n        name=""test"",\n    )\n\n    dataset2 = dataset1.copy()\n    assert dataset2.name != dataset1.name\n    assert dataset2.background_model\n    dataset2 = dataset1.copy(name=""dataset2"")\n    assert dataset2.name == ""dataset2""\n    assert dataset2.background_model.name == ""dataset2-bkg""\n    assert dataset2.background_model is not dataset1.background_model\n    assert dataset2.models.names == [""model1"", ""model2"", ""dataset2-bkg""]\n    assert dataset2.models is not dataset1.models\n\n\ndef test_stack_dataset_dataset_on_off():\n    axis = MapAxis.from_edges([1, 10] * u.TeV, name=""energy"")\n    geom = WcsGeom.create(width=1, axes=[axis])\n\n    gti = GTI.create([0 * u.s], [1 * u.h])\n\n    dataset = MapDataset.create(geom, gti=gti)\n    dataset_on_off = MapDatasetOnOff.create(geom, gti=gti)\n    dataset_on_off.mask_safe.data += True\n\n    dataset_on_off.acceptance_off += 5\n    dataset_on_off.acceptance += 1\n    dataset_on_off.counts_off += 1\n    dataset.stack(dataset_on_off)\n\n    assert_allclose(dataset.background_model.map.data, 0.2)\n\n\n@requires_data()\ndef test_info_dict_on_off(images):\n    dataset = get_map_dataset_onoff(images)\n    info_dict = dataset.info_dict()\n    assert_allclose(info_dict[""counts""], 4299, rtol=1e-3)\n    assert_allclose(info_dict[""excess""], -22.52, rtol=1e-3)\n    assert_allclose(info_dict[""aeff_min""].value, 0.0, rtol=1e-3)\n    assert_allclose(info_dict[""aeff_max""].value, 3.4298378e09, rtol=1e-3)\n    assert_allclose(info_dict[""npred""], 0.0, rtol=1e-3)\n    assert_allclose(info_dict[""counts_off""], 20407510.0, rtol=1e-3)\n    assert_allclose(info_dict[""acceptance""], 4272.7075, rtol=1e-3)\n    assert_allclose(info_dict[""acceptance_off""], 20175596.0, rtol=1e-3)\n\n    gti = GTI.create([0 * u.s], [1 * u.h], reference_time=""2010-01-01T00:00:00"")\n    dataset.gti = gti\n    info_dict = dataset.info_dict()\n    assert_allclose(info_dict[""n_on""], 4299)\n    assert_allclose(info_dict[""n_off""], 20407510.0)\n    assert_allclose(info_dict[""a_on""], 0.068363324)\n    assert_allclose(info_dict[""a_off""], 322.83185)\n    assert_allclose(info_dict[""alpha""], 0.0002117614)\n    assert_allclose(info_dict[""excess""], -22.52295)\n    assert_allclose(info_dict[""livetime""].value, 3600)\n'"
gammapy/datasets/tests/test_simulate.py,3,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom astropy.time import Time\nfrom gammapy.data import GTI, DataStore, Observation\nfrom gammapy.datasets import MapDatasetEventSampler\nfrom gammapy.datasets.tests.test_map import get_map_dataset\nfrom gammapy.irf import load_cta_irfs\nfrom gammapy.maps import MapAxis, WcsGeom\nfrom gammapy.modeling.models import (\n    GaussianSpatialModel,\n    LightCurveTemplateTemporalModel,\n    PowerLawSpectralModel,\n    SkyModel,\n)\nfrom gammapy.utils.testing import requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef dataset():\n    position = SkyCoord(0.0, 0.0, frame=""galactic"", unit=""deg"")\n    energy_axis = MapAxis.from_bounds(\n        1, 10, nbin=3, unit=""TeV"", name=""energy"", interp=""log""\n    )\n\n    spatial_model = GaussianSpatialModel(\n        lon_0=""0 deg"", lat_0=""0 deg"", sigma=""0.2 deg"", frame=""galactic""\n    )\n\n    spectral_model = PowerLawSpectralModel(amplitude=""1e-11 cm-2 s-1 TeV-1"")\n\n    t_min = 0 * u.s\n    t_max = 1000 * u.s\n\n    time = np.arange(t_max.value) * u.s\n    tau = u.Quantity(""2e2 s"")\n    norm = np.exp(-time / tau)\n\n    table = Table()\n    table[""TIME""] = time\n    table[""NORM""] = norm / norm.max()\n    t_ref = Time(""2000-01-01"")\n    table.meta = dict(MJDREFI=t_ref.mjd, MJDREFF=0, TIMEUNIT=""s"")\n    temporal_model = LightCurveTemplateTemporalModel(table)\n\n    skymodel = SkyModel(\n        spatial_model=spatial_model,\n        spectral_model=spectral_model,\n        temporal_model=temporal_model,\n    )\n\n    geom = WcsGeom.create(\n        skydir=position, binsz=1, width=""5 deg"", frame=""galactic"", axes=[energy_axis]\n    )\n\n    gti = GTI.create(start=t_min, stop=t_max, reference_time=t_ref)\n\n    geom_true = geom.copy()\n    geom_true.axes[0].name = ""energy_true""\n\n    dataset = get_map_dataset(\n        sky_model=skymodel, geom=geom, geom_etrue=geom_true, edisp=""edispmap""\n    )\n    dataset.gti = gti\n\n    return dataset\n\n\n@requires_data()\ndef test_mde_sample_sources(dataset):\n    sampler = MapDatasetEventSampler(random_state=0)\n    events = sampler.sample_sources(dataset=dataset)\n\n    assert len(events.table[""ENERGY_TRUE""]) == 348\n    assert_allclose(events.table[""ENERGY_TRUE""][0], 3.536090852832, rtol=1e-5)\n    assert events.table[""ENERGY_TRUE""].unit == ""TeV""\n\n    assert_allclose(events.table[""RA_TRUE""][0], 266.296592804, rtol=1e-5)\n    assert events.table[""RA_TRUE""].unit == ""deg""\n\n    assert_allclose(events.table[""DEC_TRUE""][0], -29.056521954, rtol=1e-5)\n    assert events.table[""DEC_TRUE""].unit == ""deg""\n\n    assert_allclose(events.table[""TIME""][0], 32.73797244764, rtol=1e-5)\n    assert events.table[""TIME""].unit == ""s""\n\n    assert_allclose(events.table[""MC_ID""][0], 1, rtol=1e-5)\n\n\n@requires_data()\ndef test_mde_sample_weak_src(dataset):\n    irfs = load_cta_irfs(\n        ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits""\n    )\n    livetime = 10.0 * u.hr\n    pointing = SkyCoord(0, 0, unit=""deg"", frame=""galactic"")\n    obs = Observation.create(\n        obs_id=1001, pointing=pointing, livetime=livetime, irfs=irfs\n    )\n\n    dataset_weak_src = dataset.copy()\n    dataset_weak_src.models[0].parameters[""amplitude""].value = 1e-25\n\n    sampler = MapDatasetEventSampler(random_state=0)\n    events = sampler.run(dataset=dataset_weak_src, observation=obs)\n\n    assert len(events.table) == 18\n    assert_allclose(\n        len(np.where(events.table[""MC_ID""] == 0)[0]), len(events.table), rtol=1e-5\n    )\n\n\n@requires_data()\ndef test_mde_sample_background(dataset):\n    sampler = MapDatasetEventSampler(random_state=0)\n    events = sampler.sample_background(dataset=dataset)\n\n    assert len(events.table[""ENERGY""]) == 15\n    assert_allclose(events.table[""ENERGY""][0], 1.894698, rtol=1e-5)\n    assert events.table[""ENERGY""].unit == ""TeV""\n\n    assert_allclose(events.table[""RA""][0], 266.454448, rtol=1e-5)\n    assert events.table[""RA""].unit == ""deg""\n\n    assert_allclose(events.table[""DEC""][0], -30.870316, rtol=1e-5)\n    assert events.table[""DEC""].unit == ""deg""\n\n    assert events.table[""DEC_TRUE""][0] == events.table[""DEC""][0]\n\n    assert_allclose(events.table[""MC_ID""][0], 0, rtol=1e-5)\n\n\n@requires_data()\ndef test_mde_sample_psf(dataset):\n    sampler = MapDatasetEventSampler(random_state=0)\n    events = sampler.sample_sources(dataset=dataset)\n    events = sampler.sample_psf(dataset.psf, events)\n\n    assert len(events.table) == 348\n    assert_allclose(events.table[""ENERGY_TRUE""][0], 3.536090852832, rtol=1e-5)\n    assert events.table[""ENERGY_TRUE""].unit == ""TeV""\n\n    assert_allclose(events.table[""RA""][0], 266.2757792220, rtol=1e-5)\n    assert events.table[""RA""].unit == ""deg""\n\n    assert_allclose(events.table[""DEC""][0], -29.030257126, rtol=1e-5)\n    assert events.table[""DEC""].unit == ""deg""\n\n\n@requires_data()\ndef test_mde_sample_edisp(dataset):\n    sampler = MapDatasetEventSampler(random_state=0)\n    events = sampler.sample_sources(dataset=dataset)\n    events = sampler.sample_edisp(dataset.edisp, events)\n\n    assert len(events.table) == 348\n    assert_allclose(events.table[""ENERGY""][0], 3.53609085283, rtol=1e-5)\n    assert events.table[""ENERGY""].unit == ""TeV""\n\n    assert_allclose(events.table[""RA_TRUE""][0], 266.296592804, rtol=1e-5)\n    assert events.table[""RA_TRUE""].unit == ""deg""\n\n    assert_allclose(events.table[""DEC_TRUE""][0], -29.05652195, rtol=1e-5)\n    assert events.table[""DEC_TRUE""].unit == ""deg""\n\n    assert_allclose(events.table[""MC_ID""][0], 1, rtol=1e-5)\n\n\n@requires_data()\ndef test_event_det_coords(dataset):\n    irfs = load_cta_irfs(\n        ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits""\n    )\n    livetime = 1.0 * u.hr\n    pointing = SkyCoord(0, 0, unit=""deg"", frame=""galactic"")\n    obs = Observation.create(\n        obs_id=1001, pointing=pointing, livetime=livetime, irfs=irfs\n    )\n\n    sampler = MapDatasetEventSampler(random_state=0)\n    events = sampler.run(dataset=dataset, observation=obs)\n\n    assert len(events.table) == 374\n    assert_allclose(events.table[""DETX""][0], -2.44563584, rtol=1e-5)\n    assert events.table[""DETX""].unit == ""deg""\n\n    assert_allclose(events.table[""DETY""][0], 0.01414569, rtol=1e-5)\n    assert events.table[""DETY""].unit == ""deg""\n\n\n@requires_data()\ndef test_mde_run(dataset):\n    irfs = load_cta_irfs(\n        ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits""\n    )\n    livetime = 1.0 * u.hr\n    pointing = SkyCoord(0, 0, unit=""deg"", frame=""galactic"")\n    obs = Observation.create(\n        obs_id=1001, pointing=pointing, livetime=livetime, irfs=irfs\n    )\n\n    sampler = MapDatasetEventSampler(random_state=0)\n    events = sampler.run(dataset=dataset, observation=obs)\n\n    dataset_bkg = dataset.copy()\n    dataset_bkg.models = dataset_bkg.models[1]\n    events_bkg = sampler.run(dataset=dataset_bkg, observation=obs)\n\n    assert len(events.table) == 374\n    assert_allclose(events.table[""ENERGY""][0], 4.09979515940, rtol=1e-5)\n    assert_allclose(events.table[""RA""][0], 263.611383742, rtol=1e-5)\n    assert_allclose(events.table[""DEC""][0], -28.89318805, rtol=1e-5)\n\n    assert len(events_bkg.table) == 10\n    assert_allclose(events_bkg.table[""ENERGY""][0], 2.84808850102, rtol=1e-5)\n    assert_allclose(events_bkg.table[""RA""][0], 266.6138405848, rtol=1e-5)\n    assert_allclose(events_bkg.table[""DEC""][0], -29.0489180785, rtol=1e-5)\n    assert_allclose(events_bkg.table[""MC_ID""][0], 0, rtol=1e-5)\n\n    meta = events.table.meta\n\n    assert meta[""HDUCLAS1""] == ""EVENTS""\n    assert meta[""EXTNAME""] == ""EVENTS""\n    assert (\n        meta[""HDUDOC""]\n        == ""https://github.com/open-gamma-ray-astro/gamma-astro-data-formats""\n    )\n    assert meta[""HDUVERS""] == ""0.2""\n    assert meta[""HDUCLASS""] == ""GADF""\n    assert meta[""OBS_ID""] == 1001\n    assert_allclose(meta[""TSTART""], 0.0)\n    assert_allclose(meta[""TSTOP""], 3600.0)\n    assert_allclose(meta[""ONTIME""], 3600.0)\n    assert_allclose(meta[""LIVETIME""], 3600.0)\n    assert_allclose(meta[""DEADC""], 0.0)\n    assert_allclose(meta[""RA_PNT""], 266.4049882865447)\n    assert_allclose(meta[""DEC_PNT""], -28.936177761791473)\n    assert meta[""EQUINOX""] == ""J2000""\n    assert meta[""RADECSYS""] == ""icrs""\n    assert ""Gammapy"" in meta[""CREATOR""]\n    assert meta[""EUNIT""] == ""TeV""\n    assert meta[""EVTVER""] == """"\n    assert meta[""OBSERVER""] == ""Gammapy user""\n    assert meta[""DSTYP1""] == ""TIME""\n    assert meta[""DSUNI1""] == ""s""\n    assert meta[""DSVAL1""] == ""TABLE""\n    assert meta[""DSREF1""] == "":GTI""\n    assert meta[""DSTYP2""] == ""ENERGY""\n    assert meta[""DSUNI2""] == ""TeV""\n    assert "":"" in meta[""DSVAL2""]\n    assert meta[""DSTYP3""] == ""POS(RA,DEC)     ""\n    assert ""CIRCLE"" in meta[""DSVAL3""]\n    assert meta[""DSUNI3""] == ""deg             ""\n    assert meta[""NDSKEYS""] == "" 3 ""\n    assert_allclose(meta[""RA_OBJ""], 266.4049882865447)\n    assert_allclose(meta[""DEC_OBJ""], -28.936177761791473)\n    assert_allclose(meta[""TELAPSE""], 1000.0)\n    assert_allclose(meta[""MJDREFI""], 51544)\n    assert_allclose(meta[""MJDREFF""], 0.0007428703684126958)\n    assert meta[""TIMEUNIT""] == ""s""\n    assert meta[""TIMESYS""] == ""tt""\n    assert meta[""TIMEREF""] == ""LOCAL""\n    assert meta[""DATE-OBS""] == ""2000-01-01""\n    assert meta[""DATE-END""] == ""2000-01-01""\n    assert meta[""TIME-OBS""] == ""00:01:04.184""\n    assert meta[""TIME-END""] == ""00:17:44.184""\n    assert_allclose(meta[""TIMEDEL""], 1e-09)\n    assert meta[""CONV_DEP""] == 0\n    assert meta[""CONV_RA""] == 0\n    assert meta[""CONV_DEC""] == 0\n    assert meta[""MID00001""] == 1\n    assert meta[""MID00002""] == 2\n    assert meta[""NMCIDS""] == 2\n    assert meta[""ALTITUDE""] == ""20.000""\n    assert meta[""ALT_PNT""] == ""20.000""\n    assert meta[""AZ_PNT""] == ""0.000""\n    assert meta[""ORIGIN""] == ""Gammapy""\n    assert meta[""CALDB""] == ""1dc""\n    assert meta[""IRF""] == ""South_z20_50""\n    assert meta[""TELESCOP""] == ""CTA""\n    assert meta[""INSTRUME""] == ""1DC""\n    assert meta[""N_TELS""] == """"\n    assert meta[""TELLIST""] == """"\n    assert meta[""GEOLON""] == """"\n    assert meta[""GEOLAT""] == """"\n\n\n@requires_data()\ndef test_mde_run_switchoff(dataset):\n    irfs = load_cta_irfs(\n        ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits""\n    )\n    livetime = 1.0 * u.hr\n    pointing = SkyCoord(0, 0, unit=""deg"", frame=""galactic"")\n    obs = Observation.create(\n        obs_id=1001, pointing=pointing, livetime=livetime, irfs=irfs\n    )\n\n    dataset.psf = None\n    dataset.edisp = None\n    dataset._background_model = None\n\n    sampler = MapDatasetEventSampler(random_state=0)\n    events = sampler.run(dataset=dataset, observation=obs)\n\n    assert len(events.table) == 348\n    assert_allclose(events.table[""ENERGY""][0], 3.5360908528324453, rtol=1e-5)\n    assert_allclose(events.table[""RA""][0], 266.2965928042457, rtol=1e-5)\n    assert_allclose(events.table[""DEC""][0], -29.056521954411902, rtol=1e-5)\n\n    meta = events.table.meta\n\n    assert meta[""RA_PNT""] == 266.4049882865447\n    assert meta[""ONTIME""] == 3600.0\n    assert meta[""OBS_ID""] == 1001\n    assert meta[""RADECSYS""] == ""icrs""\n\n\n@requires_data()\ndef test_events_datastore(tmp_path, dataset):\n    irfs = load_cta_irfs(\n        ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits""\n    )\n    livetime = 10.0 * u.hr\n    pointing = SkyCoord(0, 0, unit=""deg"", frame=""galactic"")\n    obs = Observation.create(\n        obs_id=1001, pointing=pointing, livetime=livetime, irfs=irfs\n    )\n\n    sampler = MapDatasetEventSampler(random_state=0)\n    events = sampler.run(dataset=dataset, observation=obs)\n\n    primary_hdu = fits.PrimaryHDU()\n    hdu_evt = fits.BinTableHDU(events.table)\n    hdu_gti = fits.BinTableHDU(dataset.gti.table, name=""GTI"")\n    hdu_all = fits.HDUList([primary_hdu, hdu_evt, hdu_gti])\n    hdu_all.writeto(str(tmp_path / ""events.fits""))\n\n    DataStore.from_events_files([str(tmp_path / ""events.fits"")])\n'"
gammapy/datasets/tests/test_spectrum.py,21,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.table import Table\nfrom astropy.time import Time\nfrom gammapy.data import GTI\nfrom gammapy.datasets import Datasets, SpectrumDataset, SpectrumDatasetOnOff\nfrom gammapy.irf import EDispKernel, EffectiveAreaTable\nfrom gammapy.maps import MapAxis, RegionGeom, RegionNDMap, WcsGeom\nfrom gammapy.modeling import Fit\nfrom gammapy.modeling.models import (\n    ConstantSpectralModel,\n    ConstantTemporalModel,\n    ExpCutoffPowerLawSpectralModel,\n    Models,\n    PowerLawSpectralModel,\n    SkyModel,\n)\nfrom gammapy.utils.random import get_random_state\nfrom gammapy.utils.regions import compound_region_to_list, make_region\nfrom gammapy.utils.testing import (\n    assert_time_allclose,\n    mpl_plot_check,\n    requires_data,\n    requires_dependency,\n)\nfrom gammapy.utils.time import time_ref_to_dict\n\n\n@pytest.fixture()\ndef spectrum_dataset():\n    energy = np.logspace(-1, 1, 31) * u.TeV\n    livetime = 100 * u.s\n\n    pwl = PowerLawSpectralModel(\n        index=2.1, amplitude=""1e5 cm-2 s-1 TeV-1"", reference=""0.1 TeV"",\n    )\n\n    temp_mod = ConstantTemporalModel()\n\n    model = SkyModel(spectral_model=pwl, temporal_model=temp_mod, name=""test-source"")\n    aeff = EffectiveAreaTable.from_constant(energy, ""1 cm2"")\n\n    axis = MapAxis.from_edges(energy, interp=""log"", name=""energy"")\n    background = RegionNDMap.create(region=""icrs;circle(0, 0, 0.1)"", axes=[axis])\n\n    bkg_rate = np.ones(30) / u.s\n    background.quantity = bkg_rate * livetime\n\n    start = [1, 3, 5] * u.day\n    stop = [2, 3.5, 6] * u.day\n    t_ref = Time(55555, format=""mjd"")\n    gti = GTI.create(start, stop, reference_time=t_ref)\n\n    dataset = SpectrumDataset(\n        models=model,\n        aeff=aeff,\n        livetime=livetime,\n        background=background,\n        name=""test"",\n        gti=gti,\n    )\n    dataset.fake(random_state=23)\n    return dataset\n\n\ndef test_data_shape(spectrum_dataset):\n    assert spectrum_dataset.data_shape[0] == 30\n\n\ndef test_str(spectrum_dataset):\n    assert ""SpectrumDataset"" in str(spectrum_dataset)\n\n\ndef test_energy_range(spectrum_dataset):\n    energy_range = spectrum_dataset.energy_range\n    assert energy_range.unit == u.TeV\n    assert_allclose(energy_range.to_value(""TeV""), [0.1, 10.0])\n\n\ndef test_info_dict(spectrum_dataset):\n    info_dict = spectrum_dataset.info_dict()\n\n    assert_allclose(info_dict[""n_on""], 907010)\n    assert_allclose(info_dict[""background""], 3000.0)\n\n    assert_allclose(info_dict[""significance""], 2924.522174)\n    assert_allclose(info_dict[""excess""], 904010)\n    assert_allclose(info_dict[""livetime""].value, 1e2)\n\n    assert info_dict[""name""] == ""test""\n\n\ndef test_incorrect_mask(spectrum_dataset):\n    mask_fit = np.ones(30, dtype=np.dtype(""float""))\n    with pytest.raises(ValueError):\n        SpectrumDataset(\n            counts=spectrum_dataset.counts.copy(), livetime=""1h"", mask_fit=mask_fit,\n        )\n\n\ndef test_set_model(spectrum_dataset):\n    spectrum_dataset = spectrum_dataset.copy()\n    spectral_model = PowerLawSpectralModel()\n    model = SkyModel(spectral_model=spectral_model, name=""test"")\n    spectrum_dataset.models = model\n    assert spectrum_dataset.models[""test""] is model\n\n    models = Models([model])\n    spectrum_dataset.models = models\n    assert spectrum_dataset.models[""test""] is model\n\n\ndef test_npred_models():\n    e_reco = MapAxis.from_energy_bounds(""1 TeV"", ""10 TeV"", nbin=3)\n    spectrum_dataset = SpectrumDataset.create(e_reco=e_reco)\n    spectrum_dataset.livetime = 1 * u.h\n    spectrum_dataset.aeff.data.data += 1e10 * u.Unit(""cm2"")\n\n    pwl_1 = PowerLawSpectralModel(index=2)\n    pwl_2 = PowerLawSpectralModel(index=2)\n    model_1 = SkyModel(spectral_model=pwl_1)\n    model_2 = SkyModel(spectral_model=pwl_2)\n\n    spectrum_dataset.models = Models([model_1, model_2])\n\n    npred = spectrum_dataset.npred()\n\n    assert_allclose(npred.data.sum(), 64.8)\n\n\n@requires_dependency(""iminuit"")\ndef test_fit(spectrum_dataset):\n    """"""Simple CASH fit to the on vector""""""\n    fit = Fit([spectrum_dataset])\n    result = fit.run()\n\n    # assert result.success\n    assert ""minuit"" in repr(result)\n\n    npred = spectrum_dataset.npred().data.sum()\n    assert_allclose(npred, 907012.186399, rtol=1e-3)\n    assert_allclose(result.total_stat, -18087404.624, rtol=1e-3)\n\n    pars = result.parameters\n    assert_allclose(pars[""index""].value, 2.1, rtol=1e-2)\n    assert_allclose(pars[""index""].error, 0.00127, rtol=1e-2)\n\n    assert_allclose(pars[""amplitude""].value, 1e5, rtol=1e-3)\n    assert_allclose(pars[""amplitude""].error, 153.450, rtol=1e-2)\n\n\ndef test_spectrum_dataset_create():\n    e_reco = MapAxis.from_edges(u.Quantity([0.1, 1, 10.0], ""TeV""), name=""energy"")\n    e_true = MapAxis.from_edges(\n        u.Quantity([0.05, 0.5, 5, 20.0], ""TeV""), name=""energy_true""\n    )\n    empty_spectrum_dataset = SpectrumDataset.create(e_reco, e_true, name=""test"")\n\n    assert empty_spectrum_dataset.name == ""test""\n    assert empty_spectrum_dataset.counts.data.sum() == 0\n    assert empty_spectrum_dataset.data_shape[0] == 2\n    assert empty_spectrum_dataset.background.data.sum() == 0\n    assert empty_spectrum_dataset.background.geom.axes[0].nbin == 2\n    assert empty_spectrum_dataset.aeff.data.axis(""energy_true"").nbin == 3\n    assert empty_spectrum_dataset.edisp.data.axis(""energy"").nbin == 2\n    assert empty_spectrum_dataset.livetime.value == 0\n    assert len(empty_spectrum_dataset.gti.table) == 0\n    assert empty_spectrum_dataset.energy_range[0] is None\n    assert_allclose(empty_spectrum_dataset.mask_safe, 0)\n\n\ndef test_spectrum_dataset_stack_diagonal_safe_mask(spectrum_dataset):\n    geom = spectrum_dataset.counts.geom\n\n    energy = np.logspace(-1, 1, 31) * u.TeV\n    aeff = EffectiveAreaTable.from_parametrization(energy, ""HESS"")\n    edisp = EDispKernel.from_diagonal_response(energy, energy)\n    livetime = 100 * u.s\n    background = spectrum_dataset.background\n    spectrum_dataset1 = SpectrumDataset(\n        counts=spectrum_dataset.counts.copy(),\n        livetime=livetime,\n        aeff=aeff,\n        edisp=edisp,\n        background=background.copy(),\n    )\n\n    livetime2 = 0.5 * livetime\n    aeff2 = EffectiveAreaTable(energy[:-1], energy[1:], 2 * aeff.data.data)\n    bkg2 = RegionNDMap.from_geom(geom=geom, data=2 * background.data)\n\n    geom = spectrum_dataset.counts.geom\n    data = np.ones(spectrum_dataset.data_shape, dtype=""bool"")\n    data[0] = False\n    safe_mask2 = RegionNDMap.from_geom(geom=geom, data=data)\n\n    spectrum_dataset2 = SpectrumDataset(\n        counts=spectrum_dataset.counts.copy(),\n        livetime=livetime2,\n        aeff=aeff2,\n        edisp=edisp,\n        background=bkg2,\n        mask_safe=safe_mask2,\n    )\n    spectrum_dataset1.stack(spectrum_dataset2)\n\n    reference = spectrum_dataset.counts.data\n    assert_allclose(spectrum_dataset1.counts.data[1:], reference[1:] * 2)\n    assert_allclose(spectrum_dataset1.counts.data[0], 141363)\n    assert spectrum_dataset1.livetime == 1.5 * livetime\n    assert_allclose(spectrum_dataset1.background.data[1:], 3 * background.data[1:])\n    assert_allclose(spectrum_dataset1.background.data[0], background.data[0])\n    assert_allclose(\n        spectrum_dataset1.aeff.data.data.to_value(""m2""),\n        4.0 / 3 * aeff.data.data.to_value(""m2""),\n    )\n    assert_allclose(spectrum_dataset1.edisp.pdf_matrix[1:], edisp.pdf_matrix[1:])\n    assert_allclose(spectrum_dataset1.edisp.pdf_matrix[0], 0.5 * edisp.pdf_matrix[0])\n\n\ndef test_spectrum_dataset_stack_nondiagonal_no_bkg(spectrum_dataset):\n    energy = spectrum_dataset.counts.geom.axes[0].edges\n\n    aeff = EffectiveAreaTable.from_parametrization(energy, ""HESS"")\n    edisp1 = EDispKernel.from_gauss(energy, energy, 0.1, 0)\n    livetime = 100 * u.s\n    spectrum_dataset1 = SpectrumDataset(\n        counts=spectrum_dataset.counts.copy(),\n        livetime=livetime,\n        aeff=aeff,\n        edisp=edisp1,\n    )\n\n    livetime2 = livetime\n    aeff2 = EffectiveAreaTable(energy[:-1], energy[1:], aeff.data.data)\n    edisp2 = EDispKernel.from_gauss(energy, energy, 0.2, 0.0)\n    spectrum_dataset2 = SpectrumDataset(\n        counts=spectrum_dataset.counts.copy(),\n        livetime=livetime2,\n        aeff=aeff2,\n        edisp=edisp2,\n    )\n    spectrum_dataset1.stack(spectrum_dataset2)\n\n    assert spectrum_dataset1.background is None\n    assert spectrum_dataset1.livetime == 2 * livetime\n    assert_allclose(\n        spectrum_dataset1.aeff.data.data.to_value(""m2""), aeff.data.data.to_value(""m2"")\n    )\n    assert_allclose(spectrum_dataset1.edisp.get_bias(1 * u.TeV), 0.0, atol=1.2e-3)\n    assert_allclose(\n        spectrum_dataset1.edisp.get_resolution(1 * u.TeV), 0.1581, atol=1e-2\n    )\n\n\n@requires_dependency(""matplotlib"")\ndef test_peek(spectrum_dataset):\n    with mpl_plot_check():\n        spectrum_dataset.peek()\n\n    with mpl_plot_check():\n        spectrum_dataset.plot_fit()\n\n    spectrum_dataset.edisp = None\n    with mpl_plot_check():\n        spectrum_dataset.peek()\n\n\nclass TestSpectrumOnOff:\n    """""" Test ON OFF SpectrumDataset""""""\n\n    def setup(self):\n        etrue = np.logspace(-1, 1, 10) * u.TeV\n        self.e_true = MapAxis.from_edges(etrue, name=""energy_true"")\n        ereco = np.logspace(-1, 1, 5) * u.TeV\n        elo = ereco[:-1]\n        ehi = ereco[1:]\n        self.e_reco = MapAxis.from_edges(ereco, name=""energy"")\n        self.aeff = EffectiveAreaTable(etrue[:-1], etrue[1:], np.ones(9) * u.cm ** 2)\n        self.edisp = EDispKernel.from_diagonal_response(etrue, ereco)\n\n        start = u.Quantity([0], ""s"")\n        stop = u.Quantity([1000], ""s"")\n        time_ref = Time(""2010-01-01 00:00:00.0"")\n        self.gti = GTI.create(start, stop, time_ref)\n        self.livetime = self.gti.time_sum\n\n        self.on_region = make_region(""icrs;circle(0.,1.,0.1)"")\n        off_region = make_region(""icrs;box(0.,1.,0.1, 0.2,30)"")\n        self.off_region = off_region.union(\n            make_region(""icrs;box(-1.,-1.,0.1, 0.2,150)"")\n        )\n        self.wcs = WcsGeom.create(npix=300, binsz=0.01, frame=""icrs"").wcs\n\n        data = np.ones(elo.shape)\n        data[-1] = 0  # to test stats calculation with empty bins\n\n        axis = MapAxis.from_edges(ereco, name=""energy"", interp=""log"")\n        self.on_counts = RegionNDMap.create(\n            region=self.on_region, wcs=self.wcs, axes=[axis]\n        )\n        self.on_counts.data += 1\n        self.on_counts.data[-1] = 0\n\n        self.off_counts = RegionNDMap.create(\n            region=self.off_region, wcs=self.wcs, axes=[axis]\n        )\n        self.off_counts.data += 10\n\n        acceptance = RegionNDMap.from_geom(self.on_counts.geom)\n        acceptance.data += 1\n\n        data = np.ones(elo.shape)\n        data[-1] = 0\n\n        acceptance_off = RegionNDMap.from_geom(self.off_counts.geom)\n        acceptance_off.data += 10\n\n        self.dataset = SpectrumDatasetOnOff(\n            counts=self.on_counts,\n            counts_off=self.off_counts,\n            aeff=self.aeff,\n            edisp=self.edisp,\n            livetime=self.livetime,\n            acceptance=acceptance,\n            acceptance_off=acceptance_off,\n            name=""test"",\n            gti=self.gti,\n        )\n\n    def test_spectrumdatasetonoff_create(self):\n        e_reco = MapAxis.from_edges(u.Quantity([0.1, 1, 10.0], ""TeV""), name=""energy"")\n        e_true = MapAxis.from_edges(\n            u.Quantity([0.05, 0.5, 5, 20.0], ""TeV""), name=""energy_true""\n        )\n        empty_dataset = SpectrumDatasetOnOff.create(e_reco, e_true)\n\n        assert empty_dataset.counts.data.sum() == 0\n        assert empty_dataset.data_shape[0] == 2\n        assert empty_dataset.counts_off.data.sum() == 0\n        assert empty_dataset.counts_off.geom.axes[0].nbin == 2\n        assert_allclose(empty_dataset.acceptance_off, 1)\n        assert_allclose(empty_dataset.acceptance, 1)\n        assert empty_dataset.acceptance.data.shape[0] == 2\n        assert empty_dataset.acceptance_off.data.shape[0] == 2\n        assert empty_dataset.livetime.value == 0\n        assert len(empty_dataset.gti.table) == 0\n        assert empty_dataset.energy_range[0] is None\n\n    def test_create_stack(self):\n        stacked = SpectrumDatasetOnOff.create(self.e_reco, self.e_true)\n        stacked.stack(self.dataset)\n        assert_allclose(stacked.energy_range.value, self.dataset.energy_range.value)\n\n    def test_alpha(self):\n        assert self.dataset.alpha.data.shape == (4, 1, 1)\n        assert_allclose(self.dataset.alpha.data, 0.1)\n\n    def test_npred_no_edisp(self):\n        const = 1 * u.Unit(""cm-2 s-1 TeV-1"")\n        model = SkyModel(spectral_model=ConstantSpectralModel(const=const))\n        livetime = 1 * u.s\n\n        e_reco = self.on_counts.geom.axes[0].edges\n        aeff = EffectiveAreaTable(e_reco[:-1], e_reco[1:], np.ones(4) * u.cm ** 2)\n        dataset = SpectrumDatasetOnOff(\n            counts=self.on_counts,\n            counts_off=self.off_counts,\n            aeff=aeff,\n            models=model,\n            livetime=livetime,\n        )\n\n        energy = aeff.energy.edges\n        expected = aeff.data.data[0] * (energy[-1] - energy[0]) * const * livetime\n\n        assert_allclose(dataset.npred_sig().data.sum(), expected.value)\n\n    @requires_dependency(""matplotlib"")\n    def test_peek(self):\n        dataset = self.dataset.copy()\n        dataset.models = SkyModel(spectral_model=PowerLawSpectralModel())\n\n        with mpl_plot_check():\n            dataset.peek()\n\n    @requires_dependency(""matplotlib"")\n    def test_plot_fit(self):\n        dataset = self.dataset.copy()\n        dataset.models = SkyModel(spectral_model=PowerLawSpectralModel())\n\n        with mpl_plot_check():\n            dataset.plot_fit()\n\n    @requires_dependency(""matplotlib"")\n    def test_plot_off_regions(self):\n        from gammapy.visualization import plot_spectrum_datasets_off_regions\n\n        with mpl_plot_check():\n            plot_spectrum_datasets_off_regions([self.dataset])\n\n    def test_to_from_ogip_files(self, tmp_path):\n        dataset = self.dataset.copy(name=""test"")\n        dataset.to_ogip_files(outdir=tmp_path)\n        newdataset = SpectrumDatasetOnOff.from_ogip_files(tmp_path / ""pha_obstest.fits"")\n\n        expected_regions = compound_region_to_list(self.off_counts.geom.region)\n        regions = compound_region_to_list(newdataset.counts_off.geom.region)\n\n        assert_allclose(self.on_counts.data, newdataset.counts.data)\n        assert_allclose(self.off_counts.data, newdataset.counts_off.data)\n        assert_allclose(self.edisp.pdf_matrix, newdataset.edisp.pdf_matrix)\n        assert_time_allclose(newdataset.gti.time_start, dataset.gti.time_start)\n\n        assert len(regions) == len(expected_regions)\n        assert regions[0].center.is_equivalent_frame(expected_regions[0].center)\n        assert_allclose(regions[1].angle, expected_regions[1].angle)\n\n    def test_to_from_ogip_files_no_edisp(self, tmp_path):\n\n        mask_safe = RegionNDMap.from_geom(self.on_counts.geom, dtype=bool)\n        mask_safe.data += True\n\n        dataset = SpectrumDatasetOnOff(\n            counts=self.on_counts,\n            aeff=self.aeff,\n            livetime=self.livetime,\n            mask_safe=mask_safe,\n            acceptance=1,\n            name=""test"",\n        )\n        dataset.to_ogip_files(outdir=tmp_path)\n        newdataset = SpectrumDatasetOnOff.from_ogip_files(tmp_path / ""pha_obstest.fits"")\n\n        assert_allclose(self.on_counts.data, newdataset.counts.data)\n        assert newdataset.counts_off is None\n        assert newdataset.edisp is None\n        assert newdataset.gti is None\n\n    def test_energy_mask(self):\n        mask = self.dataset.counts.geom.energy_mask(emin=0.3 * u.TeV, emax=6 * u.TeV)\n        desired = [False, True, True, False]\n        assert_allclose(mask[:, 0, 0], desired)\n\n        mask = self.dataset.counts.geom.energy_mask(emax=6 * u.TeV)\n        desired = [True, True, True, False]\n        assert_allclose(mask[:, 0, 0], desired)\n\n        mask = self.dataset.counts.geom.energy_mask(emin=1 * u.TeV)\n        desired = [False, False, True, True]\n        assert_allclose(mask[:, 0, 0], desired)\n\n    def test_str(self):\n        model = SkyModel(spectral_model=PowerLawSpectralModel())\n        dataset = SpectrumDatasetOnOff(\n            counts=self.on_counts,\n            counts_off=self.off_counts,\n            models=model,\n            aeff=self.aeff,\n            livetime=self.livetime,\n            edisp=self.edisp,\n            acceptance=1,\n            acceptance_off=10,\n        )\n        assert ""SpectrumDatasetOnOff"" in str(dataset)\n        assert ""wstat"" in str(dataset)\n\n    def test_fake(self):\n        """"""Test the fake dataset""""""\n        source_model = SkyModel(spectral_model=PowerLawSpectralModel())\n        dataset = SpectrumDatasetOnOff(\n            counts=self.on_counts,\n            counts_off=self.off_counts,\n            models=source_model,\n            aeff=self.aeff,\n            livetime=self.livetime,\n            edisp=self.edisp,\n            acceptance=1,\n            acceptance_off=10,\n        )\n        real_dataset = dataset.copy()\n\n        background = RegionNDMap.from_geom(dataset.counts.geom)\n        background.data += 1\n        dataset.fake(background_model=background, random_state=314)\n\n        assert real_dataset.counts.data.shape == dataset.counts.data.shape\n        assert real_dataset.counts_off.data.shape == dataset.counts_off.data.shape\n        assert dataset.counts_off.data.sum() == 39\n        assert dataset.counts.data.sum() == 5\n\n    def test_info_dict(self):\n        info_dict = self.dataset.info_dict()\n\n        assert_allclose(info_dict[""n_on""], 3)\n        assert_allclose(info_dict[""n_off""], 40)\n        assert_allclose(info_dict[""a_on""], 1)\n        assert_allclose(info_dict[""a_off""], 10)\n\n        assert_allclose(info_dict[""alpha""], 0.1)\n        assert_allclose(info_dict[""excess""], -1)\n        assert_allclose(info_dict[""livetime""].value, 1e3)\n\n        assert info_dict[""name""] == ""test""\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\nclass TestSpectralFit:\n    """"""Test fit in astrophysical scenario""""""\n\n    def setup(self):\n        path = ""$GAMMAPY_DATA/joint-crab/spectra/hess/""\n        self.datasets = Datasets(\n            [\n                SpectrumDatasetOnOff.from_ogip_files(path + ""pha_obs23523.fits""),\n                SpectrumDatasetOnOff.from_ogip_files(path + ""pha_obs23592.fits""),\n            ]\n        )\n\n        self.pwl = SkyModel(\n            spectral_model=PowerLawSpectralModel(\n                index=2, amplitude=1e-12 * u.Unit(""cm-2 s-1 TeV-1""), reference=1 * u.TeV\n            )\n        )\n\n        self.ecpl = SkyModel(\n            spectral_model=ExpCutoffPowerLawSpectralModel(\n                index=2,\n                amplitude=1e-12 * u.Unit(""cm-2 s-1 TeV-1""),\n                reference=1 * u.TeV,\n                lambda_=0.1 / u.TeV,\n            )\n        )\n\n        # Example fit for one observation\n        self.datasets[0].models = self.pwl\n        self.fit = Fit([self.datasets[0]])\n\n    def set_model(self, model):\n        for obs in self.datasets:\n            obs.models = model\n\n    @requires_dependency(""iminuit"")\n    def test_basic_results(self):\n        self.set_model(self.pwl)\n        result = self.fit.run()\n        pars = self.fit.datasets.parameters\n\n        assert self.pwl is self.datasets[0].models[0]\n\n        assert_allclose(result.total_stat, 38.343, rtol=1e-3)\n        assert_allclose(pars[""index""].value, 2.817, rtol=1e-3)\n        assert pars[""amplitude""].unit == ""cm-2 s-1 TeV-1""\n        assert_allclose(pars[""amplitude""].value, 5.142e-11, rtol=1e-3)\n        assert_allclose(self.datasets[0].npred().data[60], 0.6102, rtol=1e-3)\n        pars.to_table()\n\n    def test_basic_errors(self):\n        self.set_model(self.pwl)\n        result = self.fit.run()\n        pars = result.parameters\n\n        assert_allclose(pars[""index""].error, 0.156045, rtol=1e-3)\n        assert_allclose(pars[""amplitude""].error, 6.473577e-12, rtol=1e-3)\n        pars.to_table()\n\n    def test_ecpl_fit(self):\n        self.set_model(self.ecpl)\n        fit = Fit([self.datasets[0]])\n        fit.run()\n\n        actual = fit.datasets.parameters[""lambda_""].quantity\n        assert actual.unit == ""TeV-1""\n        assert_allclose(actual.value, 0.145215, rtol=1e-2)\n\n    def test_joint_fit(self):\n        self.set_model(self.pwl)\n        fit = Fit(self.datasets)\n        fit.run()\n        actual = fit.datasets.parameters[""index""].value\n        assert_allclose(actual, 2.7806, rtol=1e-3)\n\n        actual = fit.datasets.parameters[""amplitude""].quantity\n        assert actual.unit == ""cm-2 s-1 TeV-1""\n        assert_allclose(actual.value, 5.200e-11, rtol=1e-3)\n\n    def test_stats(self):\n        dataset = self.datasets[0]\n        dataset.models = self.pwl\n\n        fit = Fit([dataset])\n        result = fit.run()\n\n        stats = dataset.stat_array()\n        actual = np.sum(stats[dataset.mask_safe])\n\n        desired = result.total_stat\n        assert_allclose(actual, desired)\n\n    def test_fit_range(self):\n        # Fit range not restriced fit range should be the thresholds\n        obs = self.datasets[0]\n        actual = obs.energy_range[0]\n\n        assert actual.unit == ""keV""\n        assert_allclose(actual.value, 8.912509e08)\n\n    def test_no_edisp(self):\n        dataset = self.datasets[0].copy()\n\n        # Bring aeff in RECO space\n        energy = dataset.counts.geom.axes[0].center\n        data = dataset.aeff.data.evaluate(energy_true=energy)\n        e_edges = dataset.counts.geom.axes[0].edges\n\n        dataset.aeff = EffectiveAreaTable(\n            data=data, energy_lo=e_edges[:-1], energy_hi=e_edges[1:]\n        )\n        dataset.edisp = None\n        dataset.models = self.pwl\n\n        fit = Fit([dataset])\n        result = fit.run()\n        assert_allclose(result.parameters[""index""].value, 2.7961, atol=0.02)\n\n    def test_stacked_fit(self):\n        dataset = self.datasets[0].copy()\n        dataset.stack(self.datasets[1])\n        dataset.models = SkyModel(PowerLawSpectralModel())\n\n        fit = Fit([dataset])\n        result = fit.run()\n        pars = result.parameters\n\n        assert_allclose(pars[""index""].value, 2.7767, rtol=1e-3)\n        assert u.Unit(pars[""amplitude""].unit) == ""cm-2 s-1 TeV-1""\n        assert_allclose(pars[""amplitude""].value, 5.191e-11, rtol=1e-3)\n\n\ndef _read_hess_obs():\n    path = ""$GAMMAPY_DATA/joint-crab/spectra/hess/""\n    obs1 = SpectrumDatasetOnOff.from_ogip_files(path + ""pha_obs23523.fits"")\n    obs2 = SpectrumDatasetOnOff.from_ogip_files(path + ""pha_obs23592.fits"")\n    return [obs1, obs2]\n\n\ndef make_gti(times, time_ref=""2010-01-01""):\n    meta = time_ref_to_dict(time_ref)\n    table = Table(times, meta=meta)\n    return GTI(table)\n\n\n@requires_data(""gammapy-data"")\ndef make_observation_list():\n    """"""obs with dummy IRF""""""\n    nbin = 3\n    energy = np.logspace(-1, 1, nbin + 1) * u.TeV\n    livetime = 2 * u.h\n    data_on = np.arange(nbin)\n    dataoff_1 = np.ones(3)\n    dataoff_2 = np.ones(3) * 3\n    dataoff_1[1] = 0\n    dataoff_2[1] = 0\n\n    axis = MapAxis.from_edges(energy, name=""energy"", interp=""log"")\n    geom = RegionGeom(region=None, axes=[axis])\n\n    on_vector = RegionNDMap.from_geom(geom=geom, data=data_on)\n    off_vector1 = RegionNDMap.from_geom(geom=geom, data=dataoff_1)\n    off_vector2 = RegionNDMap.from_geom(geom=geom, data=dataoff_2)\n    mask_safe = RegionNDMap.from_geom(geom, dtype=bool)\n    mask_safe.data += True\n\n    aeff = EffectiveAreaTable.from_constant(energy, ""1 cm2"")\n    edisp = EDispKernel.from_gauss(e_true=energy, e_reco=energy, sigma=0.2, bias=0)\n\n    time_ref = Time(""2010-01-01"")\n    gti1 = make_gti({""START"": [5, 6, 1, 2], ""STOP"": [8, 7, 3, 4]}, time_ref=time_ref)\n    gti2 = make_gti({""START"": [14], ""STOP"": [15]}, time_ref=time_ref)\n\n    obs1 = SpectrumDatasetOnOff(\n        counts=on_vector,\n        counts_off=off_vector1,\n        aeff=aeff,\n        edisp=edisp,\n        livetime=livetime,\n        mask_safe=mask_safe,\n        acceptance=1,\n        acceptance_off=2,\n        name=""1"",\n        gti=gti1,\n    )\n    obs2 = SpectrumDatasetOnOff(\n        counts=on_vector,\n        counts_off=off_vector2,\n        aeff=aeff,\n        edisp=edisp,\n        livetime=livetime,\n        mask_safe=mask_safe,\n        acceptance=1,\n        acceptance_off=4,\n        name=""2"",\n        gti=gti2,\n    )\n\n    obs_list = [obs1, obs2]\n    return obs_list\n\n\n@requires_data(""gammapy-data"")\nclass TestSpectrumDatasetOnOffStack:\n    def setup(self):\n        self.datasets = _read_hess_obs()\n        # Change threshold to make stuff more interesting\n\n        geom = self.datasets[0]._geom\n        data = geom.energy_mask(emin=1.2 * u.TeV, emax=50 * u.TeV)\n        self.datasets[0].mask_safe = RegionNDMap.from_geom(geom=geom, data=data)\n\n        data = geom.energy_mask(emax=20 * u.TeV)\n        self.datasets[1].mask_safe.data &= data\n\n        self.stacked_dataset = self.datasets[0].copy()\n        self.stacked_dataset.stack(self.datasets[1])\n\n    def test_basic(self):\n        obs_1, obs_2 = self.datasets\n\n        counts1 = obs_1.counts.data[obs_1.mask_safe].sum()\n        counts2 = obs_2.counts.data[obs_2.mask_safe].sum()\n        summed_counts = counts1 + counts2\n\n        stacked_counts = self.stacked_dataset.counts.data.sum()\n\n        off1 = obs_1.counts_off.data[obs_1.mask_safe].sum()\n        off2 = obs_2.counts_off.data[obs_2.mask_safe].sum()\n        summed_off = off1 + off2\n        stacked_off = self.stacked_dataset.counts_off.data.sum()\n\n        assert summed_counts == stacked_counts\n        assert summed_off == stacked_off\n\n    def test_thresholds(self):\n        e_min, e_max = self.stacked_dataset.energy_range\n\n        assert e_min.unit == ""keV""\n        assert_allclose(e_min.value, 8.912509e08, rtol=1e-3)\n\n        assert e_max.unit == ""keV""\n        assert_allclose(e_max.value, 4.466836e10, rtol=1e-3)\n\n    def test_verify_npred(self):\n        """"""Veryfing npred is preserved during the stacking""""""\n        pwl = SkyModel(\n            spectral_model=PowerLawSpectralModel(\n                index=2, amplitude=2e-11 * u.Unit(""cm-2 s-1 TeV-1""), reference=1 * u.TeV\n            )\n        )\n        self.stacked_dataset.models = pwl\n\n        npred_stacked = self.stacked_dataset.npred().data\n        npred_stacked[~self.stacked_dataset.mask_safe.data] = 0\n        npred_summed = np.zeros_like(npred_stacked)\n\n        for obs in self.datasets:\n            obs.models = pwl\n            npred_summed[obs.mask_safe] += obs.npred().data[obs.mask_safe]\n\n        assert_allclose(npred_stacked, npred_summed)\n\n    def test_stack_backscal(self):\n        """"""Verify backscal stacking """"""\n        obs1, obs2 = make_observation_list()\n        obs1.stack(obs2)\n        assert_allclose(obs1.alpha.data[0], 1.25 / 4.0)\n        # When the OFF stack observation counts=0, the alpha is averaged on the total OFF counts for each run.\n        assert_allclose(obs1.alpha.data[1], 2.5 / 8.0)\n\n    def test_stack_gti(self):\n        obs1, obs2 = make_observation_list()\n        obs1.stack(obs2)\n        table_gti = Table({""START"": [1.0, 5.0, 14.0], ""STOP"": [4.0, 8.0, 15.0]})\n        table_gti_stacked_obs = obs1.gti.table\n        assert_allclose(table_gti_stacked_obs[""START""], table_gti[""START""])\n        assert_allclose(table_gti_stacked_obs[""STOP""], table_gti[""STOP""])\n\n\n@requires_data(""gammapy-data"")\ndef test_datasets_stack_reduce():\n    obs_ids = [23523, 23526, 23559, 23592]\n    dataset_list = []\n    for obs in obs_ids:\n        filename = ""$GAMMAPY_DATA/joint-crab/spectra/hess/pha_obs{}.fits""\n        ds = SpectrumDatasetOnOff.from_ogip_files(filename.format(obs))\n        dataset_list.append(ds)\n    datasets = Datasets(dataset_list)\n    stacked = datasets.stack_reduce(name=""stacked"")\n    assert_allclose(stacked.livetime.to_value(""s""), 6313.8116406202325)\n\n    info_table = datasets.info_table()\n    assert_allclose(info_table[""n_on""], [124, 126, 119, 90])\n\n    info_table_cum = datasets.info_table(cumulative=True)\n    assert_allclose(info_table_cum[""n_on""], [124, 250, 369, 459])\n    assert stacked.name == ""stacked""\n\n\ndef test_spectrum_dataset_on_off_to_yaml(tmpdir):\n    spectrum_datasets_on_off = make_observation_list()\n    datasets = Datasets(spectrum_datasets_on_off)\n    datasets.write(path=tmpdir)\n    datasets_read = Datasets.read(tmpdir, ""_datasets.yaml"", ""_models.yaml"")\n    assert len(datasets_read) == len(datasets)\n    assert datasets_read[0].name == datasets[0].name\n    assert datasets_read[1].name == datasets[1].name\n    assert datasets_read[1].counts.data.sum() == datasets[1].counts.data.sum()\n\n\ndef test_stack_no_livetime():\n    e_reco = MapAxis.from_energy_bounds(1, 10, 3, name=""energy"", unit=""TeV"")\n    dataset_1 = SpectrumDataset.create(e_reco=e_reco)\n    dataset_1.livetime = None\n    dataset_2 = dataset_1.copy()\n\n    with pytest.raises(ValueError):\n        dataset_1.stack(dataset_2)\n\n\n@requires_dependency(""iminuit"")\nclass TestFit:\n    """"""Test fit on counts spectra without any IRFs""""""\n\n    def setup(self):\n        self.nbins = 30\n        energy = np.logspace(-1, 1, self.nbins + 1) * u.TeV\n        self.source_model = SkyModel(\n            spectral_model=PowerLawSpectralModel(\n                index=2, amplitude=1e5 * u.Unit(""cm-2 s-1 TeV-1""), reference=0.1 * u.TeV\n            )\n        )\n        bkg_model = PowerLawSpectralModel(\n            index=3, amplitude=1e4 * u.Unit(""cm-2 s-1 TeV-1""), reference=0.1 * u.TeV\n        )\n\n        self.alpha = 0.1\n        random_state = get_random_state(23)\n        npred = self.source_model.spectral_model.integral(energy[:-1], energy[1:]).value\n        source_counts = random_state.poisson(npred)\n\n        axis = MapAxis.from_edges(energy, name=""energy"", interp=""log"")\n        geom = RegionGeom(region=None, axes=[axis])\n\n        self.src = RegionNDMap.from_geom(geom=geom, data=source_counts)\n\n        self.src.livetime = 1 * u.s\n        self.aeff = EffectiveAreaTable.from_constant(energy, ""1 cm2"")\n\n        npred_bkg = bkg_model.integral(energy[:-1], energy[1:]).value\n\n        bkg_counts = random_state.poisson(npred_bkg)\n        off_counts = random_state.poisson(npred_bkg * 1.0 / self.alpha)\n        self.bkg = RegionNDMap.from_geom(geom=geom, data=bkg_counts)\n        self.off = RegionNDMap.from_geom(geom=geom, data=off_counts)\n\n    def test_cash(self):\n        """"""Simple CASH fit to the on vector""""""\n        dataset = SpectrumDataset(\n            models=self.source_model,\n            counts=self.src,\n            aeff=self.aeff,\n            livetime=self.src.livetime,\n        )\n\n        npred = dataset.npred().data\n        assert_allclose(npred[5], 660.5171, rtol=1e-5)\n\n        stat_val = dataset.stat_sum()\n        assert_allclose(stat_val, -107346.5291, rtol=1e-5)\n\n        self.source_model.parameters[""index""].value = 1.12\n\n        fit = Fit([dataset])\n        result = fit.run()\n\n        # These values are check with sherpa fits, do not change\n        pars = result.parameters\n        assert_allclose(pars[""index""].value, 1.995525, rtol=1e-3)\n        assert_allclose(pars[""amplitude""].value, 100245.9, rtol=1e-3)\n\n    def test_wstat(self):\n        """"""WStat with on source and background spectrum""""""\n        on_vector = self.src.copy()\n        on_vector.data += self.bkg.data\n        dataset = SpectrumDatasetOnOff(\n            counts=on_vector,\n            counts_off=self.off,\n            aeff=self.aeff,\n            livetime=self.src.livetime,\n            acceptance=1,\n            acceptance_off=1 / self.alpha,\n        )\n        dataset.models = self.source_model\n\n        self.source_model.parameters.index = 1.12\n\n        fit = Fit([dataset])\n        result = fit.run()\n        pars = self.source_model.parameters\n\n        assert_allclose(pars[""index""].value, 1.997342, rtol=1e-3)\n        assert_allclose(pars[""amplitude""].value, 100245.187067, rtol=1e-3)\n        assert_allclose(result.total_stat, 30.022316, rtol=1e-3)\n\n    def test_fit_range(self):\n        """"""Test fit range without complication of thresholds""""""\n        geom = self.src.geom\n        mask_safe = RegionNDMap.from_geom(geom, dtype=bool)\n        mask_safe.data += True\n\n        dataset = SpectrumDatasetOnOff(counts=self.src, mask_safe=mask_safe)\n\n        assert np.sum(dataset.mask_safe) == self.nbins\n        e_min, e_max = dataset.energy_range\n\n        assert_allclose(e_max.value, 10)\n        assert_allclose(e_min.value, 0.1)\n\n    def test_stat_profile(self):\n        geom = self.src.geom\n        mask_safe = RegionNDMap.from_geom(geom, dtype=bool)\n        mask_safe.data += True\n\n        dataset = SpectrumDataset(\n            models=self.source_model,\n            aeff=self.aeff,\n            livetime=self.src.livetime,\n            counts=self.src,\n            mask_safe=mask_safe,\n        )\n        fit = Fit([dataset])\n        result = fit.run()\n        true_idx = result.parameters[""index""].value\n        values = np.linspace(0.95 * true_idx, 1.05 * true_idx, 100)\n        profile = fit.stat_profile(""index"", values=values)\n        actual = values[np.argmin(profile[""stat""])]\n        assert_allclose(actual, true_idx, rtol=0.01)\n'"
gammapy/estimators/tests/__init__.py,0,b''
gammapy/estimators/tests/test_asmooth_map.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.convolution import Tophat2DKernel\nfrom gammapy.datasets import Datasets, MapDatasetOnOff\nfrom gammapy.estimators import ASmoothMapEstimator\nfrom gammapy.maps import Map, WcsNDMap\nfrom gammapy.utils.testing import requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef input_maps():\n    filename = ""$GAMMAPY_DATA/tests/unbundled/poisson_stats_image/input_all.fits.gz""\n    return {\n        ""counts"": Map.read(filename, hdu=""counts""),\n        ""background"": Map.read(filename, hdu=""background""),\n    }\n\n\n@pytest.fixture(scope=""session"")\ndef input_dataset():\n    datasets = Datasets.read(\n        ""$GAMMAPY_DATA/fermi-3fhl-crab"",\n        filedata=""Fermi-LAT-3FHL_datasets.yaml"",\n        filemodel=""Fermi-LAT-3FHL_models.yaml"",\n    )\n    dataset = datasets[0]\n    dataset.psf = None\n    return dataset\n\n\n@requires_data()\ndef test_asmooth(input_maps):\n    kernel = Tophat2DKernel\n    scales = ASmoothMapEstimator.get_scales(3, factor=2, kernel=kernel) * 0.1 * u.deg\n\n    asmooth = ASmoothMapEstimator(\n        scales=scales, kernel=kernel, method=""lima"", threshold=2.5\n    )\n    smoothed = asmooth.estimate_maps(input_maps[""counts""], input_maps[""background""])\n\n    desired = {\n        ""counts"": 6.454327,\n        ""background"": 1.0,\n        ""scale"": 0.056419,\n        ""significance"": 18.125747,\n    }\n\n    for name in smoothed:\n        actual = smoothed[name].data[100, 100]\n        assert_allclose(actual, desired[name], rtol=1e-5)\n\n\n@requires_data()\ndef test_asmooth_dataset(input_dataset):\n    kernel = Tophat2DKernel\n    scales = ASmoothMapEstimator.get_scales(3, factor=2, kernel=kernel) * 0.1 * u.deg\n\n    asmooth = ASmoothMapEstimator(\n        scales=scales, kernel=kernel, method=""lima"", threshold=2.5\n    )\n\n    # First check that is fails if don\'t use to_image()\n    with pytest.raises(ValueError):\n        asmooth.run(input_dataset)\n\n    smoothed = asmooth.run(input_dataset.to_image())\n\n    assert smoothed[""flux""].data.shape == (40, 50)\n    assert smoothed[""flux""].unit == u.Unit(""cm-2s-1"")\n    assert smoothed[""counts""].unit == u.Unit("""")\n    assert smoothed[""background""].unit == u.Unit("""")\n    assert smoothed[""scale""].unit == u.Unit(""deg"")\n\n    desired = {\n        ""counts"": 369.479167,\n        ""background"": 0.13461,\n        ""scale"": 0.056419,\n        ""significance"": 74.677406,\n        ""flux"": 1.237284e-09,\n    }\n\n    for name in smoothed:\n        actual = smoothed[name].data[20, 25]\n        assert_allclose(actual, desired[name], rtol=1e-5)\n\n\ndef test_asmooth_map_dataset_on_off():\n    kernel = Tophat2DKernel\n    scales = ASmoothMapEstimator.get_scales(3, factor=2, kernel=kernel) * 0.1 * u.deg\n\n    asmooth = ASmoothMapEstimator(\n        kernel=kernel, scales=scales, method=""lima"", threshold=2.5\n    )\n\n    counts = WcsNDMap.create(npix=(50, 50), binsz=0.02, unit="""")\n    counts += 2\n    counts_off = WcsNDMap.create(npix=(50, 50), binsz=0.02, unit="""")\n    counts_off += 3\n\n    acceptance = WcsNDMap.create(npix=(50, 50), binsz=0.02, unit="""")\n    acceptance += 1\n\n    acceptance_off = WcsNDMap.create(npix=(50, 50), binsz=0.02, unit="""")\n    acceptance_off += 3\n\n    dataset = MapDatasetOnOff(\n        counts=counts,\n        counts_off=counts_off,\n        acceptance=acceptance,\n        acceptance_off=acceptance_off,\n    )\n\n    smoothed = asmooth.run(dataset)\n    assert_allclose(smoothed[""counts""].data[25, 25], 2)\n    assert_allclose(smoothed[""background""].data[25, 25], 1)\n    assert_allclose(smoothed[""significance""].data[25, 25], 4.391334)\n'"
gammapy/estimators/tests/test_excess_map.py,1,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom gammapy.datasets import MapDataset, MapDatasetOnOff\nfrom gammapy.estimators import ExcessMapEstimator\nfrom gammapy.maps import Map, MapAxis, WcsGeom\nfrom gammapy.modeling.models import BackgroundModel\nfrom gammapy.utils.testing import requires_data\n\n\ndef image_to_cube(input_map, e_min, e_max):\n    e_min = u.Quantity(e_min)\n    e_max = u.Quantity(e_max)\n    axis = MapAxis.from_energy_bounds(e_min, e_max, nbin=1)\n    geom = input_map.geom.to_cube([axis])\n    return Map.from_geom(geom, data=input_map.data[np.newaxis, :, :])\n\n\n@pytest.fixture\ndef simple_dataset():\n    axis = MapAxis.from_energy_bounds(0.1, 10, 1, unit=""TeV"")\n    geom = WcsGeom.create(npix=20, binsz=0.02, axes=[axis])\n    dataset = MapDataset.create(geom)\n    dataset.mask_safe += 1\n    dataset.counts += 2\n    dataset.background_model.map += 1\n    return dataset\n\n\n@pytest.fixture\ndef simple_dataset_on_off():\n    axis = MapAxis.from_energy_bounds(0.1, 10, 1, unit=""TeV"")\n    geom = WcsGeom.create(npix=20, binsz=0.02, axes=[axis])\n    dataset = MapDatasetOnOff.create(geom)\n    dataset.mask_safe += 1\n    dataset.counts += 2\n    dataset.counts_off += 1\n    dataset.acceptance += 1\n    dataset.acceptance_off += 1\n    return dataset\n\n\n@requires_data()\ndef test_compute_lima_image():\n    """"""\n    Test Li & Ma image against TS image for Tophat kernel\n    """"""\n    filename = ""$GAMMAPY_DATA/tests/unbundled/poisson_stats_image/input_all.fits.gz""\n    counts = Map.read(filename, hdu=""counts"")\n    counts = image_to_cube(counts, ""1 GeV"", ""100 GeV"")\n    background = Map.read(filename, hdu=""background"")\n    background = image_to_cube(background, ""1 GeV"", ""100 GeV"")\n    background_model = BackgroundModel(background)\n    dataset = MapDataset(counts=counts)\n    dataset.models = background_model\n\n    estimator = ExcessMapEstimator(""0.1 deg"")\n    result_lima = estimator.run(dataset, steps=""ts"")\n\n    assert_allclose(result_lima[""significance""].data[:, 100, 100], 30.814916, atol=1e-3)\n    assert_allclose(result_lima[""significance""].data[:, 1, 1], 0.164, atol=1e-3)\n\n\n@requires_data()\ndef test_compute_lima_on_off_image():\n    """"""\n    Test Li & Ma image with snippet from the H.E.S.S. survey data.\n    """"""\n    filename = ""$GAMMAPY_DATA/tests/unbundled/hess/survey/hess_survey_snippet.fits.gz""\n    n_on = Map.read(filename, hdu=""ON"")\n    counts = image_to_cube(n_on, ""1 TeV"", ""100 TeV"")\n    n_off = Map.read(filename, hdu=""OFF"")\n    counts_off = image_to_cube(n_off, ""1 TeV"", ""100 TeV"")\n    a_on = Map.read(filename, hdu=""ONEXPOSURE"")\n    acceptance = image_to_cube(a_on, ""1 TeV"", ""100 TeV"")\n    a_off = Map.read(filename, hdu=""OFFEXPOSURE"")\n    acceptance_off = image_to_cube(a_off, ""1 TeV"", ""100 TeV"")\n    dataset = MapDatasetOnOff(\n        counts=counts,\n        counts_off=counts_off,\n        acceptance=acceptance,\n        acceptance_off=acceptance_off,\n    )\n\n    significance = Map.read(filename, hdu=""SIGNIFICANCE"")\n    significance = image_to_cube(significance, ""1 TeV"", ""10 TeV"")\n    estimator = ExcessMapEstimator(""0.1 deg"")\n    results = estimator.run(dataset, steps=""ts"")\n\n    # Reproduce safe significance threshold from HESS software\n    results[""significance""].data[results[""counts""].data < 5] = 0\n\n    # crop the image at the boundaries, because the reference image\n    # is cut out from a large map, there is no way to reproduce the\n    # result with regular boundary handling\n    actual = results[""significance""].crop((11, 11)).data\n    desired = significance.crop((11, 11)).data\n\n    # Set boundary to NaN in reference image\n    # The absolute tolerance is low because the method used here is slightly different from the one used in HGPS\n    # n_off is convolved as well to ensure the method applies to true ON-OFF datasets\n    assert_allclose(actual, desired, atol=0.2)\n\n\ndef test_significance_map_estimator_incorrect_dataset():\n    with pytest.raises(ValueError):\n        ExcessMapEstimator(""bad"")\n\n\ndef test_significance_map_estimator_map_dataset(simple_dataset):\n    estimator = ExcessMapEstimator(0.1 * u.deg)\n    result = estimator.run(simple_dataset, steps=""all"")\n\n    assert_allclose(result[""counts""].data[0, 10, 10], 162)\n    assert_allclose(result[""excess""].data[0, 10, 10], 81)\n    assert_allclose(result[""background""].data[0, 10, 10], 81)\n    assert_allclose(result[""significance""].data[0, 10, 10], 7.910732, atol=1e-5)\n    assert_allclose(result[""err""].data[0, 10, 10], 12.727922, atol=1e-3)\n    assert_allclose(result[""errp""].data[0, 10, 10], 13.063328, atol=1e-3)\n    assert_allclose(result[""errn""].data[0, 10, 10], -12.396716, atol=1e-3)\n    assert_allclose(result[""ul""].data[0, 10, 10], 122.240837, atol=1e-3)\n\n\ndef test_significance_map_estimator_map_dataset_on_off(simple_dataset_on_off):\n    estimator = ExcessMapEstimator(0.11 * u.deg)\n    result = estimator.run(simple_dataset_on_off, steps=[""ts""])\n\n    assert_allclose(result[""counts""].data[0, 10, 10], 194)\n    assert_allclose(result[""excess""].data[0, 10, 10], 97)\n    assert_allclose(result[""background""].data[0, 10, 10], 97)\n    assert_allclose(result[""significance""].data[0, 10, 10], 5.741116, atol=1e-5)\n'"
gammapy/estimators/tests/test_flux.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom gammapy.datasets import Datasets, SpectrumDatasetOnOff\nfrom gammapy.estimators.flux import FluxEstimator\nfrom gammapy.modeling.models import PowerLawSpectralModel, SkyModel\nfrom gammapy.utils.testing import requires_data, requires_dependency\n\n\n@pytest.fixture(scope=""session"")\ndef fermi_datasets():\n    fermi_datasets = Datasets.read(\n        ""$GAMMAPY_DATA/fermi-3fhl-crab"",\n        ""Fermi-LAT-3FHL_datasets.yaml"",\n        ""Fermi-LAT-3FHL_models.yaml"",\n    )\n    return fermi_datasets\n\n\n@pytest.fixture(scope=""session"")\ndef hess_datasets():\n    datasets = Datasets([])\n    pwl = PowerLawSpectralModel(amplitude=""3.5e-11 cm-2s-1TeV-1"", index=2.7)\n    model = SkyModel(spectral_model=pwl, name=""Crab"")\n\n    for obsid in [23523, 23526]:\n        dataset = SpectrumDatasetOnOff.from_ogip_files(\n            f""$GAMMAPY_DATA/joint-crab/spectra/hess/pha_obs{obsid}.fits""\n        )\n        dataset.models = model\n        datasets.append(dataset)\n\n    return datasets\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_flux_estimator_fermi_no_reoptimization(fermi_datasets):\n    estimator = FluxEstimator(\n        0,\n        energy_range=[""1 GeV"", ""100 GeV""],\n        norm_n_values=5,\n        norm_min=0.5,\n        norm_max=2,\n        reoptimize=False,\n    )\n    result = estimator.run(fermi_datasets)\n\n    assert_allclose(result[""norm""], 0.970614, atol=1e-3)\n    assert_allclose(result[""ts""], 29695.720611, atol=1e-3)\n    assert_allclose(result[""norm_err""], 0.01998, atol=1e-3)\n    assert_allclose(result[""norm_errn""], 0.0199, atol=1e-3)\n    assert_allclose(result[""norm_errp""], 0.0199, atol=1e-3)\n    assert len(result[""norm_scan""]) == 5\n    assert_allclose(result[""norm_scan""][0], 0.5)\n    assert_allclose(result[""norm_scan""][-1], 2)\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_flux_estimator_fermi_with_reoptimization(fermi_datasets):\n    estimator = FluxEstimator(0, energy_range=[""1 GeV"", ""100 GeV""], reoptimize=True)\n    result = estimator.run(fermi_datasets, steps=[""err"", ""ts""])\n\n    assert_allclose(result[""norm""], 0.970614, atol=1e-3)\n    assert_allclose(result[""ts""], 13005.903067, atol=1e-3)\n    assert_allclose(result[""norm_err""], 0.01998, atol=1e-3)\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_flux_estimator_1d(hess_datasets):\n    estimator = FluxEstimator(source=""Crab"", energy_range=[1, 10] * u.TeV)\n    result = estimator.run(hess_datasets, steps=[""err"", ""ts"", ""errp-errn"", ""ul""])\n\n    assert_allclose(result[""norm""], 1.176789, atol=1e-3)\n    assert_allclose(result[""ts""], 693.111777, atol=1e-3)\n    assert_allclose(result[""norm_err""], 0.078087, atol=1e-3)\n    assert_allclose(result[""norm_errn""], 0.078046, atol=1e-3)\n    assert_allclose(result[""norm_errp""], 0.081665, atol=1e-3)\n    assert_allclose(result[""norm_ul""], 1.431722, atol=1e-3)\n\n\n@requires_data()\ndef test_inhomogeneous_datasets(fermi_datasets, hess_datasets):\n    fermi_datasets.append(hess_datasets[0])\n    with pytest.raises(ValueError):\n        estimator = FluxEstimator(source=0, energy_range=[1, 10] * u.TeV)\n        estimator.run(fermi_datasets)\n\n\n@requires_data()\ndef test_flux_estimator_incorrect_energy_range():\n    with pytest.raises(ValueError):\n        FluxEstimator(source=""Crab"", energy_range=[1, 3, 10] * u.TeV)\n    with pytest.raises(ValueError):\n        FluxEstimator(source=""Crab"", energy_range=[10, 1] * u.TeV)\n'"
gammapy/estimators/tests/test_flux_point.py,18,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.table import Table\nfrom gammapy.catalog.fermi import SourceCatalog3FGL\nfrom gammapy.estimators import FluxPoints\nfrom gammapy.modeling.models import PowerLawSpectralModel, SpectralModel\nfrom gammapy.utils.testing import (\n    assert_quantity_allclose,\n    mpl_plot_check,\n    requires_data,\n    requires_dependency,\n)\n\nFLUX_POINTS_FILES = [\n    ""diff_flux_points.ecsv"",\n    ""diff_flux_points.fits"",\n    ""flux_points.ecsv"",\n    ""flux_points.fits"",\n]\n\n\nclass LWTestModel(SpectralModel):\n    @staticmethod\n    def evaluate(x):\n        return 1e4 * np.exp(-6 * x)\n\n    def integral(self, xmin, xmax, **kwargs):\n        return -1.0 / 6 * 1e4 * (np.exp(-6 * xmax) - np.exp(-6 * xmin))\n\n    def inverse(self, y):\n        return -1.0 / 6 * np.log(y * 1e-4)\n\n\nclass XSqrTestModel(SpectralModel):\n    @staticmethod\n    def evaluate(x):\n        return x ** 2\n\n    def integral(self, xmin, xmax, **kwargs):\n        return 1.0 / 3 * (xmax ** 3 - xmin ** 2)\n\n    def inverse(self, y):\n        return np.sqrt(y)\n\n\nclass ExpTestModel(SpectralModel):\n    @staticmethod\n    def evaluate(x):\n        return np.exp(x * u.Unit(""1 / TeV""))\n\n    def integral(self, xmin, xmax, **kwargs):\n        return np.exp(xmax * u.Unit(""1 / TeV"")) - np.exp(xmin * u.Unit(""1 / TeV""))\n\n    def inverse(self, y):\n        return np.log(y * u.TeV) * u.TeV\n\n\ndef test_e_ref_lafferty():\n    """"""\n    Tests Lafferty & Wyatt x-point method.\n\n    Using input function g(x) = 10^4 exp(-6x) against\n    check values from paper Lafferty & Wyatt. Nucl. Instr. and Meth. in Phys.\n    Res. A 355 (1995) 541-547, p. 542 Table 1\n    """"""\n    # These are the results from the paper\n    desired = np.array([0.048, 0.190, 0.428, 0.762])\n\n    model = LWTestModel()\n    e_min = np.array([0.0, 0.1, 0.3, 0.6])\n    e_max = np.array([0.1, 0.3, 0.6, 1.0])\n    actual = FluxPoints._e_ref_lafferty(model, e_min, e_max)\n    assert_allclose(actual, desired, atol=1e-3)\n\n\ndef test_dnde_from_flux():\n    """"""Tests y-value normalization adjustment method.\n    """"""\n    e_min = np.array([10, 20, 30, 40])\n    e_max = np.array([20, 30, 40, 50])\n    flux = np.array([42, 52, 62, 72])  # \'True\' integral flux in this test bin\n\n    # Get values\n    model = XSqrTestModel()\n    e_ref = FluxPoints._e_ref_lafferty(model, e_min, e_max)\n    dnde = FluxPoints._dnde_from_flux(\n        flux, model, e_ref, e_min, e_max, pwl_approx=False\n    )\n\n    # Set up test case comparison\n    dnde_model = model(e_ref)\n\n    # Test comparison result\n    desired = model.integral(e_min, e_max)\n    # Test output result\n    actual = flux * (dnde_model / dnde)\n    # Compare\n    assert_allclose(actual, desired, rtol=1e-6)\n\n\n@pytest.mark.parametrize(""method"", [""table"", ""lafferty"", ""log_center""])\ndef test_compute_flux_points_dnde_exp(method):\n    """"""\n    Tests against analytical result or result from gammapy.spectrum.powerlaw.\n    """"""\n    model = ExpTestModel()\n\n    e_min = [1.0, 10.0] * u.TeV\n    e_max = [10.0, 100.0] * u.TeV\n\n    table = Table()\n    table.meta[""SED_TYPE""] = ""flux""\n    table[""e_min""] = e_min\n    table[""e_max""] = e_max\n\n    flux = model.integral(e_min, e_max)\n    table[""flux""] = flux\n\n    if method == ""log_center"":\n        e_ref = np.sqrt(e_min * e_max)\n    elif method == ""table"":\n        e_ref = [2.0, 20.0] * u.TeV\n        table[""e_ref""] = e_ref\n    elif method == ""lafferty"":\n        e_ref = FluxPoints._e_ref_lafferty(model, e_min, e_max)\n\n    result = FluxPoints(table).to_sed_type(""dnde"", model=model, method=method)\n\n    # Test energy\n    actual = result.e_ref\n    assert_quantity_allclose(actual, e_ref, rtol=1e-8)\n\n    # Test flux\n    actual = result.table[""dnde""].quantity\n    desired = model(e_ref)\n    assert_quantity_allclose(actual, desired, rtol=1e-8)\n\n\n@pytest.fixture(params=FLUX_POINTS_FILES, scope=""session"")\ndef flux_points(request):\n    path = ""$GAMMAPY_DATA/tests/spectrum/flux_points/"" + request.param\n    return FluxPoints.read(path)\n\n\n@pytest.fixture(scope=""session"")\ndef flux_points_likelihood():\n    path = ""$GAMMAPY_DATA/tests/spectrum/flux_points/binlike.fits""\n    return FluxPoints.read(path).to_sed_type(""dnde"")\n\n\n@requires_data()\nclass TestFluxPoints:\n    def test_info(self, flux_points):\n        info = str(flux_points)\n        assert flux_points.sed_type in info\n\n    def test_e_ref(self, flux_points):\n        actual = flux_points.e_ref\n        if flux_points.sed_type == ""dnde"":\n            pass\n        elif flux_points.sed_type == ""flux"":\n            desired = np.sqrt(flux_points.e_min * flux_points.e_max)\n            assert_quantity_allclose(actual, desired)\n\n    def test_e_min(self, flux_points):\n        if flux_points.sed_type == ""dnde"":\n            pass\n        elif flux_points.sed_type == ""flux"":\n            actual = flux_points.e_min\n            desired = 299530.97 * u.MeV\n            assert_quantity_allclose(actual.sum(), desired)\n\n    def test_e_max(self, flux_points):\n        if flux_points.sed_type == ""dnde"":\n            pass\n        elif flux_points.sed_type == ""flux"":\n            actual = flux_points.e_max\n            desired = 399430.975 * u.MeV\n            assert_quantity_allclose(actual.sum(), desired)\n\n    def test_write_fits(self, tmp_path, flux_points):\n        flux_points.write(tmp_path / ""tmp.fits"")\n        actual = FluxPoints.read(tmp_path / ""tmp.fits"")\n        assert str(flux_points) == str(actual)\n\n    def test_write_ecsv(self, tmp_path, flux_points):\n        flux_points.write(tmp_path / ""flux_points.ecsv"")\n        actual = FluxPoints.read(tmp_path / ""flux_points.ecsv"")\n        assert str(flux_points) == str(actual)\n\n    def test_drop_ul(self, flux_points):\n        flux_points = flux_points.drop_ul()\n        assert not np.any(flux_points.is_ul)\n\n    def test_stack(self, flux_points):\n        stacked = FluxPoints.stack([flux_points, flux_points])\n        assert len(stacked.table) == 2 * len(flux_points.table)\n        assert stacked.sed_type == flux_points.sed_type\n\n    @requires_dependency(""matplotlib"")\n    def test_plot(self, flux_points):\n        with mpl_plot_check():\n            flux_points.plot()\n\n    @requires_dependency(""matplotlib"")\n    def test_plot_likelihood(self, flux_points_likelihood):\n        with mpl_plot_check():\n            flux_points_likelihood.plot_ts_profiles()\n\n\n@requires_data()\ndef test_compute_flux_points_dnde_fermi():\n    """"""\n    Test compute_flux_points_dnde on fermi source.\n    """"""\n    fermi_3fgl = SourceCatalog3FGL()\n    source = fermi_3fgl[""3FGL J0835.3-4510""]\n    flux_points = source.flux_points.to_sed_type(\n        ""dnde"", model=source.spectral_model(), method=""log_center"", pwl_approx=True\n    )\n    for column in [""dnde"", ""dnde_errn"", ""dnde_errp"", ""dnde_ul""]:\n        actual = flux_points.table[""e2"" + column].quantity\n        desired = flux_points.table[column].quantity * flux_points.e_ref ** 2\n        assert_quantity_allclose(actual[:-1], desired[:-1], rtol=1e-1)\n\n\n@pytest.fixture(scope=""session"")\ndef model():\n    return PowerLawSpectralModel()\n\n\n@pytest.fixture(scope=""session"")\ndef flux_points_dnde(model):\n    e_ref = [np.sqrt(10), np.sqrt(10 * 100)] * u.TeV\n    table = Table()\n    table.meta[""SED_TYPE""] = ""dnde""\n    table[""e_ref""] = e_ref\n    table[""dnde""] = model(e_ref)\n    return FluxPoints(table)\n\n\n@pytest.fixture(scope=""session"")\ndef flux_points_e2dnde(model):\n    e_ref = [np.sqrt(10), np.sqrt(10 * 100)] * u.TeV\n    table = Table()\n    table.meta[""SED_TYPE""] = ""e2dnde""\n    table[""e_ref""] = e_ref\n    table[""e2dnde""] = (model(e_ref) * e_ref ** 2).to(""erg cm-2 s-1"")\n    return FluxPoints(table)\n\n\n@pytest.fixture(scope=""session"")\ndef flux_points_flux(model):\n    e_min = [1, 10] * u.TeV\n    e_max = [10, 100] * u.TeV\n\n    table = Table()\n    table.meta[""SED_TYPE""] = ""flux""\n    table[""e_min""] = e_min\n    table[""e_max""] = e_max\n    table[""flux""] = model.integral(e_min, e_max)\n    return FluxPoints(table)\n\n\ndef test_dnde_to_e2dnde(flux_points_dnde, flux_points_e2dnde):\n    actual = flux_points_dnde.to_sed_type(""e2dnde"").table\n    desired = flux_points_e2dnde.table\n    assert_allclose(actual[""e2dnde""], desired[""e2dnde""])\n\n\ndef test_e2dnde_to_dnde(flux_points_e2dnde, flux_points_dnde):\n    actual = flux_points_e2dnde.to_sed_type(""dnde"").table\n    desired = flux_points_dnde.table\n    assert_allclose(actual[""dnde""], desired[""dnde""])\n\n\ndef test_flux_to_dnde(flux_points_flux, flux_points_dnde):\n    actual = flux_points_flux.to_sed_type(""dnde"", method=""log_center"").table\n    desired = flux_points_dnde.table\n    assert_allclose(actual[""e_ref""], desired[""e_ref""])\n    assert_allclose(actual[""dnde""], desired[""dnde""])\n'"
gammapy/estimators/tests/test_flux_point_estimator.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom astropy import units as u\nfrom astropy.coordinates import SkyCoord\nfrom gammapy.data import Observation\nfrom gammapy.datasets import MapDataset, SpectrumDatasetOnOff\nfrom gammapy.estimators import FluxPointsEstimator\nfrom gammapy.irf import EffectiveAreaTable, load_cta_irfs\nfrom gammapy.makers import MapDatasetMaker\nfrom gammapy.maps import MapAxis, RegionGeom, RegionNDMap, WcsGeom\nfrom gammapy.modeling.models import (\n    ExpCutoffPowerLawSpectralModel,\n    GaussianSpatialModel,\n    PowerLawSpectralModel,\n    SkyModel,\n)\nfrom gammapy.utils.testing import requires_data, requires_dependency\n\n\n# TODO: use pregenerate data instead\ndef simulate_spectrum_dataset(model, random_state=0):\n    edges = np.logspace(-0.5, 1.5, 21) * u.TeV\n    energy_axis = MapAxis.from_edges(edges, interp=""log"", name=""energy"")\n\n    aeff = EffectiveAreaTable.from_parametrization(energy=edges)\n    bkg_model = SkyModel(\n        spectral_model=PowerLawSpectralModel(\n            index=2.5, amplitude=""1e-12 cm-2 s-1 TeV-1""\n        ),\n        name=""background"",\n    )\n    bkg_model.spectral_model.amplitude.frozen = True\n    bkg_model.spectral_model.index.frozen = True\n\n    geom = RegionGeom(region=None, axes=[energy_axis])\n    acceptance = RegionNDMap.from_geom(geom=geom, data=1)\n\n    dataset = SpectrumDatasetOnOff(\n        aeff=aeff, livetime=100 * u.h, acceptance=acceptance, acceptance_off=5\n    )\n    dataset.models = bkg_model\n    bkg_npred = dataset.npred_sig()\n\n    dataset.models = model\n    dataset.fake(random_state=random_state, background_model=bkg_npred)\n    return dataset\n\n\ndef create_fpe(model):\n    model = SkyModel(spectral_model=model, name=""source"")\n    dataset = simulate_spectrum_dataset(model)\n    e_edges = [0.1, 1, 10, 100] * u.TeV\n    dataset.models = model\n    fpe = FluxPointsEstimator(e_edges=e_edges, norm_n_values=11, source=""source"")\n    datasets = [dataset]\n    return datasets, fpe\n\n\ndef simulate_map_dataset(random_state=0, name=None):\n    irfs = load_cta_irfs(\n        ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits""\n    )\n\n    skydir = SkyCoord(""0 deg"", ""0 deg"", frame=""galactic"")\n    edges = np.logspace(-1, 2, 15) * u.TeV\n    energy_axis = MapAxis.from_edges(edges=edges, name=""energy"", interp=""log"")\n\n    geom = WcsGeom.create(\n        skydir=skydir, width=(4, 4), binsz=0.1, axes=[energy_axis], frame=""galactic""\n    )\n\n    gauss = GaussianSpatialModel(\n        lon_0=""0 deg"", lat_0=""0 deg"", sigma=""0.4 deg"", frame=""galactic""\n    )\n    pwl = PowerLawSpectralModel(amplitude=""1e-11 cm-2 s-1 TeV-1"")\n    skymodel = SkyModel(spatial_model=gauss, spectral_model=pwl, name=""source"")\n\n    obs = Observation.create(pointing=skydir, livetime=1 * u.h, irfs=irfs)\n    empty = MapDataset.create(geom, name=name)\n    maker = MapDatasetMaker(selection=[""exposure"", ""background"", ""psf"", ""edisp""])\n    dataset = maker.run(empty, obs)\n\n    dataset.models.append(skymodel)\n    dataset.fake(random_state=random_state)\n    return dataset\n\n\n@pytest.fixture(scope=""session"")\ndef fpe_map_pwl():\n    dataset_1 = simulate_map_dataset(name=""test-map-pwl"")\n    dataset_2 = dataset_1.copy(name=""test-map-pwl-2"")\n    dataset_2.models = dataset_1.models\n\n    dataset_2.mask_safe = RegionNDMap.from_geom(dataset_2.counts.geom, dtype=bool)\n\n    e_edges = [0.1, 1, 10, 100] * u.TeV\n    datasets = [dataset_1, dataset_2]\n    fpe = FluxPointsEstimator(e_edges=e_edges, norm_n_values=3, source=""source"")\n    return datasets, fpe\n\n\n@pytest.fixture(scope=""session"")\ndef fpe_map_pwl_reoptimize():\n    dataset = simulate_map_dataset()\n    e_edges = [1, 10] * u.TeV\n    dataset.models.parameters[""lon_0""].frozen = True\n    dataset.models.parameters[""lat_0""].frozen = True\n    #    dataset.models.parameters[""index""].frozen = True\n    dataset.models.parameters[""sigma""].frozen = True\n    datasets = [dataset]\n    fpe = FluxPointsEstimator(\n        e_edges=e_edges, norm_values=[1], reoptimize=True, source=""source""\n    )\n    return datasets, fpe\n\n\n@pytest.fixture(scope=""session"")\ndef fpe_pwl():\n    return create_fpe(PowerLawSpectralModel())\n\n\n@pytest.fixture(scope=""session"")\ndef fpe_ecpl():\n    return create_fpe(ExpCutoffPowerLawSpectralModel(lambda_=""1 TeV-1""))\n\n\nclass TestFluxPointsEstimator:\n    @staticmethod\n    def test_str(fpe_pwl):\n        datasets, fpe = fpe_pwl\n        assert ""FluxPointsEstimator"" in str(fpe)\n\n    @pytest.mark.xfail\n    @staticmethod\n    @requires_dependency(""iminuit"")\n    def test_run_pwl(fpe_pwl):\n        datasets, fpe = fpe_pwl\n\n        fp = fpe.run(datasets)\n        actual = fp.table[""norm""].data\n        assert_allclose(actual, [1.081434, 0.91077, 0.922176], rtol=1e-3)\n\n        actual = fp.table[""norm_err""].data\n        assert_allclose(actual, [0.066374, 0.061025, 0.179729], rtol=1e-2)\n\n        actual = fp.table[""norm_errn""].data\n        assert_allclose(actual, [0.065803, 0.060403, 0.171376], rtol=1e-2)\n\n        actual = fp.table[""norm_errp""].data\n        assert_allclose(actual, [0.06695, 0.061652, 0.18839], rtol=1e-2)\n\n        actual = fp.table[""counts""].data.squeeze()\n        assert_allclose(actual, [1490, 748, 43])\n\n        actual = fp.table[""norm_ul""].data\n        assert_allclose(actual, [1.216227, 1.035472, 1.316878], rtol=1e-2)\n\n        actual = fp.table[""sqrt_ts""].data\n        assert_allclose(actual, [18.568429, 18.054651, 7.057121], rtol=1e-2)\n\n        actual = fp.table[""norm_scan""][0][[0, 5, -1]]\n        assert_allclose(actual, [0.2, 1, 5])\n\n        actual = fp.table[""stat_scan""][0][[0, 5, -1]]\n        assert_allclose(actual, [220.368653, 4.301011, 1881.626454], rtol=1e-2)\n\n    @staticmethod\n    @requires_dependency(""iminuit"")\n    @requires_data()\n    def test_run_map_pwl(fpe_map_pwl):\n        datasets, fpe = fpe_map_pwl\n        fp = fpe.run(datasets)\n\n        actual = fp.table[""norm""].data\n        assert_allclose(actual, [0.974726, 0.96342 , 0.994251], rtol=1e-2)\n\n        actual = fp.table[""norm_err""].data\n        assert_allclose(actual, [0.067637, 0.052022, 0.087059], rtol=3e-2)\n\n        actual = fp.table[""counts""].data\n        assert_allclose(actual, [[44611, 0], [1923, 0], [282, 0]])\n\n        actual = fp.table[""norm_ul""].data\n        assert_allclose(actual, [1.111852, 1.07004 , 1.17829], rtol=1e-2)\n\n        actual = fp.table[""sqrt_ts""].data\n        assert_allclose(actual, [16.681221, 28.408676, 21.91912], rtol=1e-2)\n\n        actual = fp.table[""norm_scan""][0]\n        assert_allclose(actual, [0.2, 1, 5])\n\n        actual = fp.table[""stat_scan""][0] - fp.table[""stat""][0]\n        assert_allclose(actual, [1.628746e+02, 1.416280e-01, 2.006994e+03], rtol=1e-2)\n\n    @staticmethod\n    @requires_dependency(""iminuit"")\n    @requires_data()\n    def test_run_map_pwl_reoptimize(fpe_map_pwl_reoptimize):\n        datasets, fpe = fpe_map_pwl_reoptimize\n        fp = fpe.run(datasets, steps=[""err"", ""norm-scan"", ""ts""])\n\n        actual = fp.table[""norm""].data\n        assert_allclose(actual, 0.962368, rtol=1e-2)\n\n        actual = fp.table[""norm_err""].data\n        assert_allclose(actual, 0.054011, rtol=1e-2)\n\n        actual = fp.table[""sqrt_ts""].data\n        assert_allclose(actual, 25.196859, rtol=1e-2)\n\n        actual = fp.table[""norm_scan""][0]\n        assert_allclose(actual, 1)\n\n        actual = fp.table[""stat_scan""][0] - fp.table[""stat""][0]\n        assert_allclose(actual, 0.480491, rtol=1e-2)\n\n\ndef test_no_likelihood_contribution():\n    dataset = simulate_spectrum_dataset(\n        SkyModel(spectral_model=PowerLawSpectralModel(), name=""source"")\n    )\n    dataset.mask_safe = RegionNDMap.from_geom(dataset.counts.geom, dtype=bool)\n\n    fpe = FluxPointsEstimator(e_edges=[1, 3, 10] * u.TeV, source=""source"")\n    fp = fpe.run([dataset])\n\n    assert np.isnan(fp.table[""norm""]).all()\n    assert np.isnan(fp.table[""norm_err""]).all()\n    assert np.isnan(fp.table[""norm_ul""]).all()\n    assert np.isnan(fp.table[""norm_scan""]).all()\n    assert_allclose(fp.table[""counts""], 0)\n\n\ndef test_mask_shape():\n    axis = MapAxis.from_edges([1, 3, 10], unit=""TeV"", interp=""log"", name=""energy"")\n    geom_1 = WcsGeom.create(binsz=1, width=3, axes=[axis])\n    geom_2 = WcsGeom.create(binsz=1, width=5, axes=[axis])\n\n    dataset_1 = MapDataset.create(geom_1)\n    dataset_2 = MapDataset.create(geom_2)\n    dataset_1.psf = None\n    dataset_2.psf = None\n    dataset_1.edisp = None\n    dataset_2.edisp = None\n\n    model = SkyModel(\n        spectral_model=PowerLawSpectralModel(),\n        spatial_model=GaussianSpatialModel(),\n        name=""source"",\n    )\n\n    dataset_1.models = model\n    dataset_2.models = model\n\n    fpe = FluxPointsEstimator(e_edges=[1, 10] * u.TeV, source=""source"")\n\n    fp = fpe.run([dataset_2, dataset_1])\n\n    assert_allclose(fp.table[""counts""], 0)\n'"
gammapy/estimators/tests/test_lightcurve.py,6,"b'import datetime\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.table import Column, Table\nfrom astropy.time import Time\nfrom gammapy.data import GTI\nfrom gammapy.estimators import LightCurve, LightCurveEstimator\nfrom gammapy.estimators.tests.test_flux_point_estimator import (\n    simulate_map_dataset,\n    simulate_spectrum_dataset,\n)\nfrom gammapy.maps import RegionNDMap\nfrom gammapy.modeling.models import PowerLawSpectralModel, SkyModel\nfrom gammapy.utils.testing import mpl_plot_check, requires_data, requires_dependency\n\n\n@pytest.fixture(scope=""session"")\ndef lc():\n    meta = dict(TIMESYS=""utc"")\n\n    table = Table(\n        meta=meta,\n        data=[\n            Column(Time([""2010-01-01"", ""2010-01-03""]).mjd, ""time_min""),\n            Column(Time([""2010-01-03"", ""2010-01-10""]).mjd, ""time_max""),\n            Column([1e-11, 3e-11], ""flux"", unit=""cm-2 s-1""),\n            Column([0.1e-11, 0.3e-11], ""flux_err"", unit=""cm-2 s-1""),\n            Column([np.nan, 3.6e-11], ""flux_ul"", unit=""cm-2 s-1""),\n            Column([False, True], ""is_ul""),\n        ],\n    )\n\n    return LightCurve(table=table)\n\n\ndef test_lightcurve_repr(lc):\n    assert repr(lc) == ""LightCurve(len=2)""\n\n\ndef test_lightcurve_properties_time(lc):\n    assert lc.time_scale == ""utc""\n    assert lc.time_format == ""mjd""\n\n    # Time-related attributes\n    time = lc.time\n    assert time.scale == ""utc""\n    assert time.format == ""mjd""\n    assert_allclose(time.mjd, [55198, 55202.5])\n\n    assert_allclose(lc.time_min.mjd, [55197, 55199])\n    assert_allclose(lc.time_max.mjd, [55199, 55206])\n\n    # Note: I\'m not sure why the time delta has this scale and format\n    time_delta = lc.time_delta\n    assert time_delta.scale == ""tai""\n    assert time_delta.format == ""jd""\n    assert_allclose(time_delta.jd, [2, 7])\n\n\ndef test_lightcurve_properties_flux(lc):\n    flux = lc.table[""flux""].quantity\n    assert flux.unit == ""cm-2 s-1""\n    assert_allclose(flux.value, [1e-11, 3e-11])\n\n\n# TODO: extend these tests to cover other time scales.\n# In those cases, CSV should not round-trip because there\n# is no header info in CSV to store the time scale!\n\n\n@pytest.mark.parametrize(""format"", [""fits"", ""ascii.ecsv"", ""ascii.csv""])\ndef test_lightcurve_read_write(tmp_path, lc, format):\n    lc.write(tmp_path / ""tmp"", format=format)\n    lc = LightCurve.read(tmp_path / ""tmp"", format=format)\n\n    # Check if time-related info round-trips\n    time = lc.time\n    assert time.scale == ""utc""\n    assert time.format == ""mjd""\n    assert_allclose(time.mjd, [55198, 55202.5])\n\n\n@requires_dependency(""matplotlib"")\ndef test_lightcurve_plot(lc):\n    with mpl_plot_check():\n        lc.plot()\n\n\n@pytest.mark.parametrize(""flux_unit"", [""cm-2 s-1""])\ndef test_lightcurve_plot_flux(lc, flux_unit):\n    f, ferr = lc._get_fluxes_and_errors(flux_unit)\n    assert_allclose(f, [1e-11, 3e-11])\n    assert_allclose(ferr, ([0.1e-11, 0.3e-11], [0.1e-11, 0.3e-11]))\n\n\n@pytest.mark.parametrize(""flux_unit"", [""cm-2 s-1""])\ndef test_lightcurve_plot_flux_ul(lc, flux_unit):\n    is_ul, ful = lc._get_flux_uls(flux_unit)\n    assert_allclose(is_ul, [False, True])\n    assert_allclose(ful, [np.nan, 3.6e-11])\n\n\ndef test_lightcurve_plot_time(lc):\n    t, terr = lc._get_times_and_errors(""mjd"")\n    assert np.array_equal(t, [55198.0, 55202.5])\n    assert np.array_equal(terr, [[1.0, 3.5], [1.0, 3.5]])\n\n    t, terr = lc._get_times_and_errors(""iso"")\n    assert np.array_equal(\n        t, [datetime.datetime(2010, 1, 2), datetime.datetime(2010, 1, 6, 12)]\n    )\n    assert np.array_equal(\n        terr,\n        [\n            [datetime.timedelta(1), datetime.timedelta(3.5)],\n            [datetime.timedelta(1), datetime.timedelta(3.5)],\n        ],\n    )\n\n\ndef get_spectrum_datasets():\n    model = SkyModel(spectral_model=PowerLawSpectralModel())\n    dataset_1 = simulate_spectrum_dataset(model=model, random_state=0)\n    dataset_1._name = ""dataset_1""\n    gti1 = GTI.create(""0h"", ""1h"", ""2010-01-01T00:00:00"")\n    dataset_1.gti = gti1\n\n    dataset_2 = simulate_spectrum_dataset(model=model, random_state=1)\n    dataset_2._name = ""dataset_2""\n    gti2 = GTI.create(""1h"", ""2h"", ""2010-01-01T00:00:00"")\n    dataset_2.gti = gti2\n\n    return [dataset_1, dataset_2]\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_group_datasets_in_time_interval():\n    # Doing a LC on one hour bin\n    datasets = get_spectrum_datasets()\n    time_intervals = [\n        Time([""2010-01-01T00:00:00"", ""2010-01-01T01:00:00""]),\n        Time([""2010-01-01T01:00:00"", ""2010-01-01T02:00:00""]),\n    ]\n    estimator = LightCurveEstimator(\n        energy_range=[1, 10] * u.TeV, norm_n_values=3, time_intervals=time_intervals\n    )\n    steps = [""err"", ""counts"", ""ts"", ""norm-scan""]\n    estimator.run(datasets, steps=steps)\n\n    assert len(estimator.group_table_info) == 2\n    assert estimator.group_table_info[""Name""][0] == ""dataset_1""\n    assert_allclose(estimator.group_table_info[""Tstart""], [55197.0, 55197.04166666667])\n    assert_allclose(\n        estimator.group_table_info[""Tstop""], [55197.04166666667, 55197.083333333336]\n    )\n    assert_allclose(estimator.group_table_info[""Group_ID""], [0, 1])\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_group_datasets_in_time_interval_outflows():\n    datasets = get_spectrum_datasets()\n    # Check Overflow\n    time_intervals = [\n        Time([""2010-01-01T00:00:00"", ""2010-01-01T00:55:00""]),\n        Time([""2010-01-01T01:00:00"", ""2010-01-01T02:00:00""]),\n    ]\n    estimator = LightCurveEstimator(\n        energy_range=[1, 10] * u.TeV, norm_n_values=3, time_intervals=time_intervals\n    )\n    steps = [""err"", ""counts"", ""ts"", ""norm-scan""]\n    estimator.run(datasets, steps=steps)\n    assert estimator.group_table_info[""Bin_type""][0] == ""Overflow""\n\n    # Check underflow\n    time_intervals = [\n        Time([""2010-01-01T00:05:00"", ""2010-01-01T01:00:00""]),\n        Time([""2010-01-01T01:00:00"", ""2010-01-01T02:00:00""]),\n    ]\n    estimator = LightCurveEstimator(\n        energy_range=[1, 10] * u.TeV, norm_n_values=3, time_intervals=time_intervals\n    )\n    steps = [""err"", ""counts"", ""ts"", ""norm-scan""]\n    estimator.run(datasets, steps=steps)\n    assert estimator.group_table_info[""Bin_type""][0] == ""Underflow""\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_lightcurve_estimator_spectrum_datasets():\n    # Doing a LC on one hour bin\n    datasets = get_spectrum_datasets()\n    time_intervals = [\n        Time([""2010-01-01T00:00:00"", ""2010-01-01T01:00:00""]),\n        Time([""2010-01-01T01:00:00"", ""2010-01-01T02:00:00""]),\n    ]\n    estimator = LightCurveEstimator(\n        energy_range=[1, 100] * u.TeV, norm_n_values=3, time_intervals=time_intervals\n    )\n    lightcurve = estimator.run(datasets)\n    assert_allclose(lightcurve.table[""time_min""], [55197.0, 55197.041667])\n    assert_allclose(lightcurve.table[""time_max""], [55197.041667, 55197.083333])\n    assert_allclose(lightcurve.table[""e_ref""], [10, 10])\n    assert_allclose(lightcurve.table[""e_min""], [1, 1])\n    assert_allclose(lightcurve.table[""e_max""], [100, 100])\n    assert_allclose(lightcurve.table[""ref_dnde""], [1e-14, 1e-14])\n    assert_allclose(lightcurve.table[""ref_flux""], [9.9e-13, 9.9e-13])\n    assert_allclose(lightcurve.table[""ref_eflux""], [4.60517e-12, 4.60517e-12])\n    assert_allclose(lightcurve.table[""ref_e2dnde""], [1e-12, 1e-12])\n    assert_allclose(lightcurve.table[""stat""], [23.302288, 22.457766], rtol=1e-5)\n    assert_allclose(lightcurve.table[""norm""], [0.988107, 0.948108], rtol=1e-2)\n    assert_allclose(lightcurve.table[""norm_err""], [0.04493, 0.041469], rtol=1e-2)\n    assert_allclose(lightcurve.table[""counts""], [2281, 2222])\n    assert_allclose(lightcurve.table[""norm_errp""], [0.044252, 0.043771], rtol=1e-2)\n    assert_allclose(lightcurve.table[""norm_errn""], [0.04374, 0.043521], rtol=1e-2)\n    assert_allclose(lightcurve.table[""norm_ul""], [1.077213, 1.036237], rtol=1e-2)\n    assert_allclose(lightcurve.table[""sqrt_ts""], [26.773925, 25.796426], rtol=1e-2)\n    assert_allclose(lightcurve.table[""ts""], [716.843084, 665.455601], rtol=1e-2)\n    assert_allclose(lightcurve.table[0][""norm_scan""], [0.2, 1.0, 5.0])\n    assert_allclose(\n        lightcurve.table[0][""stat_scan""],\n        [444.426957, 23.375417, 3945.382802],\n        rtol=1e-5,\n    )\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_lightcurve_estimator_spectrum_datasets_withmaskfit():\n    # Doing a LC on one hour bin\n    datasets = get_spectrum_datasets()\n    time_intervals = [\n        Time([""2010-01-01T00:00:00"", ""2010-01-01T01:00:00""]),\n        Time([""2010-01-01T01:00:00"", ""2010-01-01T02:00:00""]),\n    ]\n\n    e_min_fit = 1 * u.TeV\n    e_max_fit = 3 * u.TeV\n    for dataset in datasets:\n        geom = dataset.counts.geom\n        data = geom.energy_mask(emin=e_min_fit, emax=e_max_fit)\n        dataset.mask_fit = RegionNDMap.from_geom(geom, data=data, dtype=bool)\n\n    steps = [""err"", ""counts"", ""ts"", ""norm-scan""]\n    estimator = LightCurveEstimator(\n        energy_range=[1, 100] * u.TeV, norm_n_values=3, time_intervals=time_intervals\n    )\n    lightcurve = estimator.run(datasets, steps=steps)\n    assert_allclose(lightcurve.table[""time_min""], [55197.0, 55197.041667])\n    assert_allclose(lightcurve.table[""time_max""], [55197.041667, 55197.083333])\n    assert_allclose(lightcurve.table[""stat""], [6.60304, 0.421047], rtol=1e-3)\n    assert_allclose(lightcurve.table[""norm""], [0.885082, 0.967022], rtol=1e-3)\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_lightcurve_estimator_spectrum_datasets_default():\n    # Test default time interval: each time interval is equal to the gti of each dataset, here one hour\n    datasets = get_spectrum_datasets()\n    estimator = LightCurveEstimator(energy_range=[1, 100] * u.TeV, norm_n_values=3)\n    steps = [""err"", ""counts"", ""ts"", ""norm-scan""]\n    lightcurve = estimator.run(datasets, steps=steps)\n    assert_allclose(lightcurve.table[""time_min""], [55197.0, 55197.041667])\n    assert_allclose(lightcurve.table[""time_max""], [55197.041667, 55197.083333])\n    assert_allclose(lightcurve.table[""norm""], [0.988107, 0.948108], rtol=1e-3)\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_lightcurve_estimator_spectrum_datasets_notordered():\n    # Test that if the time intervals given are not ordered in time, it is first ordered correctly and then\n    # compute as expected\n    datasets = get_spectrum_datasets()\n    time_intervals = [\n        Time([""2010-01-01T01:00:00"", ""2010-01-01T02:00:00""]),\n        Time([""2010-01-01T00:00:00"", ""2010-01-01T01:00:00""]),\n    ]\n    estimator = LightCurveEstimator(\n        energy_range=[1, 100] * u.TeV, norm_n_values=3, time_intervals=time_intervals\n    )\n    steps = [""err"", ""counts"", ""ts"", ""norm-scan""]\n    lightcurve = estimator.run(datasets, steps=steps)\n    assert_allclose(lightcurve.table[""time_min""], [55197.0, 55197.041667])\n    assert_allclose(lightcurve.table[""time_max""], [55197.041667, 55197.083333])\n    assert_allclose(lightcurve.table[""norm""], [0.988107, 0.948108], rtol=1e-3)\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_lightcurve_estimator_spectrum_datasets_largerbin():\n    # Test all dataset in a single LC bin, here two hours\n    datasets = get_spectrum_datasets()\n    time_intervals = [Time([""2010-01-01T00:00:00"", ""2010-01-01T02:00:00""])]\n    estimator = LightCurveEstimator(\n        energy_range=[1, 100] * u.TeV, norm_n_values=3, time_intervals=time_intervals\n    )\n    steps = [""err"", ""counts"", ""ts"", ""norm-scan""]\n    lightcurve = estimator.run(datasets, steps=steps)\n\n    assert_allclose(lightcurve.table[""time_min""], [55197.0])\n    assert_allclose(lightcurve.table[""time_max""], [55197.083333])\n    assert_allclose(lightcurve.table[""e_ref""], [10])\n    assert_allclose(lightcurve.table[""e_min""], [1])\n    assert_allclose(lightcurve.table[""e_max""], [100])\n    assert_allclose(lightcurve.table[""ref_dnde""], [1e-14])\n    assert_allclose(lightcurve.table[""ref_flux""], [9.9e-13])\n    assert_allclose(lightcurve.table[""ref_eflux""], [4.60517e-12])\n    assert_allclose(lightcurve.table[""ref_e2dnde""], [1e-12])\n    assert_allclose(lightcurve.table[""stat""], [46.177981], rtol=1e-5)\n    assert_allclose(lightcurve.table[""norm""], [0.968049], rtol=1e-5)\n    assert_allclose(lightcurve.table[""norm_err""], [0.030982], rtol=1e-3)\n    assert_allclose(lightcurve.table[""ts""], [1381.880757], rtol=1e-4)\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_lightcurve_estimator_spectrum_datasets_timeoverlaped():\n    # Check that it returns a ValueError if the time intervals overlapped\n    datasets = get_spectrum_datasets()\n    time_intervals = [\n        Time([""2010-01-01T00:00:00"", ""2010-01-01T01:30:00""]),\n        Time([""2010-01-01T01:00:00"", ""2010-01-01T02:00:00""]),\n    ]\n    with pytest.raises(ValueError) as excinfo:\n        estimator = LightCurveEstimator(norm_n_values=3, time_intervals=time_intervals)\n        estimator.run(datasets)\n    msg = ""LightCurveEstimator requires non-overlapping time bins.""\n    assert str(excinfo.value) == msg\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_lightcurve_estimator_spectrum_datasets_gti_not_include_in_time_intervals():\n    # Check that it returns a ValueError if the time intervals are smaller than the dataset GTI.\n    datasets = get_spectrum_datasets()\n    time_intervals = [\n        Time([""2010-01-01T00:00:00"", ""2010-01-01T00:05:00""]),\n        Time([""2010-01-01T01:00:00"", ""2010-01-01T01:05:00""]),\n    ]\n    estimator = LightCurveEstimator(\n        energy_range=[1, 100] * u.TeV, norm_n_values=3, time_intervals=time_intervals\n    )\n    with pytest.raises(ValueError) as excinfo:\n        steps = [""err"", ""counts"", ""ts"", ""norm-scan""]\n        estimator.run(datasets, steps=steps)\n    msg = ""LightCurveEstimator: No datasets in time intervals""\n    assert str(excinfo.value) == msg\n\n\ndef get_map_datasets():\n    dataset_1 = simulate_map_dataset(random_state=0, name=""dataset_1"")\n    gti1 = GTI.create(""0 h"", ""1 h"", ""2010-01-01T00:00:00"")\n    dataset_1.gti = gti1\n\n    dataset_2 = simulate_map_dataset(random_state=1, name=""dataset_2"")\n    gti2 = GTI.create(""1 h"", ""2 h"", ""2010-01-01T00:00:00"")\n    dataset_2.gti = gti2\n\n    model = dataset_1.models[""source""].copy(""test_source"")\n    dataset_1.models.pop(""source"")\n    dataset_2.models.pop(""source"")\n    dataset_1.models.append(model)\n    dataset_2.models.append(model)\n\n    return [dataset_1, dataset_2]\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_lightcurve_estimator_map_datasets():\n    datasets = get_map_datasets()\n\n    time_intervals = [\n        Time([""2010-01-01T00:00:00"", ""2010-01-01T01:00:00""]),\n        Time([""2010-01-01T01:00:00"", ""2010-01-01T02:00:00""]),\n    ]\n    estimator = LightCurveEstimator(\n        energy_range=[1, 100] * u.TeV,\n        source=""test_source"",\n        time_intervals=time_intervals,\n    )\n    steps = [""err"", ""counts"", ""ts"", ""norm-scan""]\n    lightcurve = estimator.run(datasets, steps=steps)\n    assert_allclose(lightcurve.table[""time_min""], [55197.0, 55197.041667])\n    assert_allclose(lightcurve.table[""time_max""], [55197.041667, 55197.083333])\n    assert_allclose(lightcurve.table[""e_ref""], [10, 10])\n    assert_allclose(lightcurve.table[""e_min""], [1, 1])\n    assert_allclose(lightcurve.table[""e_max""], [100, 100])\n    assert_allclose(lightcurve.table[""ref_dnde""], [1e-13, 1e-13])\n    assert_allclose(lightcurve.table[""ref_flux""], [9.9e-12, 9.9e-12])\n    assert_allclose(lightcurve.table[""ref_eflux""], [4.60517e-11, 4.60517e-11])\n    assert_allclose(lightcurve.table[""ref_e2dnde""], [1e-11, 1e-11])\n    assert_allclose(lightcurve.table[""stat""], [-87412.393367, -89856.129206], rtol=1e-2)\n    assert_allclose(lightcurve.table[""norm""], [0.972535, 0.995933], rtol=1e-2)\n    assert_allclose(lightcurve.table[""norm_err""], [0.037293, 0.037806], rtol=1e-2)\n    assert_allclose(lightcurve.table[""sqrt_ts""], [39.568557, 39.934953], rtol=1e-2)\n    assert_allclose(lightcurve.table[""ts""], [1565.670741, 1594.800492], rtol=1e-2)\n\n    datasets = get_map_datasets()\n\n    time_intervals2 = [Time([""2010-01-01T00:00:00"", ""2010-01-01T02:00:00""])]\n    estimator2 = LightCurveEstimator(\n        energy_range=[1, 100] * u.TeV,\n        source=""test_source"",\n        time_intervals=time_intervals2,\n    )\n    lightcurve2 = estimator2.run(datasets)\n\n    assert_allclose(lightcurve2.table[""time_min""], [55197.0])\n    assert_allclose(lightcurve2.table[""time_max""], [55197.083333])\n    assert_allclose(lightcurve2.table[""e_ref""], [10])\n    assert_allclose(lightcurve2.table[""e_min""], [1])\n    assert_allclose(lightcurve2.table[""e_max""], [100])\n    assert_allclose(lightcurve2.table[""ref_dnde""], [1e-13])\n    assert_allclose(lightcurve2.table[""ref_flux""], [9.9e-12])\n    assert_allclose(lightcurve2.table[""ref_eflux""], [4.60517e-11])\n    assert_allclose(lightcurve2.table[""ref_e2dnde""], [1e-11])\n    assert_allclose(lightcurve2.table[""stat""], [-177267.775615], rtol=1e-2)\n    assert_allclose(lightcurve2.table[""norm""], [0.983672], rtol=1e-2)\n    assert_allclose(lightcurve2.table[""norm_err""], [0.026545], rtol=1e-2)\n    assert_allclose(lightcurve.table[""counts""], [46816, 47399])\n    assert_allclose(lightcurve2.table[""ts""], [3160.275], rtol=1e-2)\n'"
gammapy/estimators/tests/test_parameter_estimator.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nfrom gammapy.datasets import Datasets, SpectrumDatasetOnOff\nfrom gammapy.estimators.parameter_estimator import ParameterEstimator\nfrom gammapy.modeling.models import PowerLawSpectralModel, SkyModel\nfrom gammapy.utils.testing import requires_data\n\npytest.importorskip(""iminuit"")\n\n\n@pytest.fixture\ndef crab_datasets_1d():\n    filename = ""$GAMMAPY_DATA/joint-crab/spectra/hess/pha_obs23523.fits""\n    dataset = SpectrumDatasetOnOff.from_ogip_files(filename)\n    datasets = Datasets([dataset])\n    return datasets\n\n\n@pytest.fixture\ndef PLmodel():\n    return PowerLawSpectralModel(amplitude=""3e-11 cm-2s-1TeV-1"", index=2.7)\n\n\n@pytest.fixture\ndef crab_datasets_fermi():\n    return Datasets.read(\n        ""$GAMMAPY_DATA/fermi-3fhl-crab/Fermi-LAT-3FHL_datasets.yaml"",\n        ""$GAMMAPY_DATA/fermi-3fhl-crab/Fermi-LAT-3FHL_models.yaml"",\n    )\n\n\n@requires_data()\ndef test_parameter_estimator_1d(crab_datasets_1d, PLmodel):\n    datasets = crab_datasets_1d\n    for dataset in datasets:\n        dataset.models = SkyModel(spectral_model=PLmodel, name=""Crab"")\n\n    estimator = ParameterEstimator(n_scan_values=10)\n\n    result = estimator.run(datasets, PLmodel.amplitude, steps=""all"")\n\n    assert_allclose(result[""amplitude""], 5.142843823441639e-11, rtol=1e-3)\n    assert_allclose(result[""amplitude_err""], 6.0075e-12, rtol=1e-3)\n    assert_allclose(result[""ts""], 353.2092043652601, rtol=1e-3)\n    assert_allclose(result[""amplitude_errp""], 6.703e-12, rtol=5e-3)\n    assert_allclose(result[""amplitude_errn""], 6.152e-12, rtol=5e-3)\n\n    # Add test for scan\n    assert_allclose(result[""amplitude_scan""].shape, 10)\n\n\n@pytest.mark.xfail\n@requires_data()\ndef test_parameter_estimator_3d(crab_datasets_fermi):\n    datasets = crab_datasets_fermi\n    parameter = datasets[0].models.parameters[""amplitude""]\n    estimator = ParameterEstimator()\n\n    result = estimator.run(datasets, parameter, steps=[""ts"", ""err""])\n\n    assert_allclose(result[""amplitude""], 0.328839, rtol=1e-3)\n    assert_allclose(result[""amplitude_err""], 0.002801, rtol=1e-3)\n    assert_allclose(result[""ts""], 13005.938702, rtol=1e-3)\n\n\n@pytest.mark.xfail\n@requires_data()\ndef test_parameter_estimator_3d_no_reoptimization(crab_datasets_fermi):\n    datasets = crab_datasets_fermi\n    parameter = datasets[0].models.parameters[""amplitude""]\n    estimator = ParameterEstimator(reoptimize=False, n_scan_values=10)\n    alpha_value = datasets[0].models.parameters[""alpha""].value\n\n    result = estimator.run(datasets, parameter, steps=""all"")\n\n    assert not datasets[0].models.parameters[""alpha""].frozen\n    assert_allclose(datasets[0].models.parameters[""alpha""].value, alpha_value)\n    assert_allclose(result[""amplitude""], 0.331505, rtol=1e-4)\n    assert_allclose(result[""amplitude_scan""].shape, 10)\n    assert_allclose(result[""amplitude_scan""][0], 0.312406, atol=1e-3)\n'"
gammapy/estimators/tests/test_profile.py,11,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom astropy import units as u\nfrom astropy.coordinates import Angle, SkyCoord\nfrom astropy.table import Table\nfrom gammapy.estimators import ImageProfile, ImageProfileEstimator\nfrom gammapy.maps import WcsGeom, WcsNDMap\nfrom gammapy.utils.testing import (\n    assert_quantity_allclose,\n    mpl_plot_check,\n    requires_dependency,\n)\n\n\n@pytest.fixture(scope=""session"")\ndef checkerboard_image():\n    nxpix, nypix = 10, 6\n\n    # set up data as a checkerboard of 0.5 and 1.5, so that the mean and sum\n    # are not compeletely trivial to compute\n    data = 1.5 * np.ones((nypix, nxpix))\n    data[slice(0, nypix + 1, 2), slice(0, nxpix + 1, 2)] = 0.5\n    data[slice(1, nypix + 1, 2), slice(1, nxpix + 1, 2)] = 0.5\n\n    geom = WcsGeom.create(npix=(nxpix, nypix), frame=""galactic"", binsz=0.02)\n    return WcsNDMap(geom=geom, data=data, unit=""cm-2 s-1"")\n\n\n@pytest.fixture(scope=""session"")\ndef cosine_profile():\n    table = Table()\n    table[""x_ref""] = np.linspace(-90, 90, 11) * u.deg\n    table[""profile""] = np.cos(table[""x_ref""].to(""rad"")) * u.Unit(""cm-2 s-1"")\n    table[""profile_err""] = 0.1 * table[""profile""]\n    return ImageProfile(table)\n\n\nclass TestImageProfileEstimator:\n    @staticmethod\n    def test_lat_profile_sum(checkerboard_image):\n        p = ImageProfileEstimator(axis=""lat"", method=""sum"")\n        profile = p.run(checkerboard_image)\n\n        desired = 10 * np.ones(6) * u.Unit(""cm-2 s-1"")\n        assert_quantity_allclose(profile.profile, desired)\n\n    @staticmethod\n    def test_lon_profile_sum(checkerboard_image):\n        p = ImageProfileEstimator(axis=""lon"", method=""sum"")\n        profile = p.run(checkerboard_image)\n\n        desired = 6 * np.ones(10) * u.Unit(""cm-2 s-1"")\n        assert_quantity_allclose(profile.profile, desired)\n\n    @staticmethod\n    def test_radial_profile_sum(checkerboard_image):\n        center = SkyCoord(0, 0, unit=""deg"", frame=""galactic"")\n        p = ImageProfileEstimator(axis=""radial"", method=""sum"", center=center)\n        profile = p.run(checkerboard_image)\n\n        desired = [4.0, 8.0, 20.0, 12.0, 12.0] * u.Unit(""cm-2 s-1"")\n        assert_quantity_allclose(profile.profile, desired)\n\n    @staticmethod\n    def test_lat_profile_mean(checkerboard_image):\n        p = ImageProfileEstimator(axis=""lat"", method=""mean"")\n        profile = p.run(checkerboard_image)\n\n        desired = np.ones(6) * u.Unit(""cm-2 s-1"")\n        assert_quantity_allclose(profile.profile, desired)\n\n    @staticmethod\n    def test_lon_profile_mean(checkerboard_image):\n        p = ImageProfileEstimator(axis=""lon"", method=""mean"")\n        profile = p.run(checkerboard_image)\n\n        desired = np.ones(10) * u.Unit(""cm-2 s-1"")\n        assert_quantity_allclose(profile.profile, desired)\n\n    @staticmethod\n    def test_x_edges_lat(checkerboard_image):\n        x_edges = Angle(np.linspace(-0.06, 0.06, 4), ""deg"")\n\n        p = ImageProfileEstimator(x_edges=x_edges, axis=""lat"", method=""sum"")\n        profile = p.run(checkerboard_image)\n\n        desired = 20 * np.ones(3) * u.Unit(""cm-2 s-1"")\n        assert_quantity_allclose(profile.profile, desired)\n\n    @staticmethod\n    def test_x_edges_lon(checkerboard_image):\n        x_edges = Angle(np.linspace(-0.1, 0.1, 6), ""deg"")\n\n        p = ImageProfileEstimator(x_edges=x_edges, axis=""lon"", method=""sum"")\n        profile = p.run(checkerboard_image)\n\n        desired = 12 * np.ones(5) * u.Unit(""cm-2 s-1"")\n        assert_quantity_allclose(profile.profile, desired)\n\n\nclass TestImageProfile:\n    @staticmethod\n    def test_normalize(cosine_profile):\n        normalized = cosine_profile.normalize(mode=""integral"")\n        profile = normalized.profile\n        assert_quantity_allclose(profile.sum(), 1 * u.Unit(""cm-2 s-1""))\n\n        normalized = cosine_profile.normalize(mode=""peak"")\n        profile = normalized.profile\n        assert_quantity_allclose(profile.max(), 1 * u.Unit(""cm-2 s-1""))\n\n    @staticmethod\n    def test_profile_x_edges(cosine_profile):\n        assert_quantity_allclose(cosine_profile.x_ref.sum(), 0 * u.deg)\n\n    @staticmethod\n    @pytest.mark.parametrize(""kernel"", [""gauss"", ""box""])\n    def test_smooth(cosine_profile, kernel):\n        # smoothing should preserve the mean\n        desired_mean = cosine_profile.profile.mean()\n        smoothed = cosine_profile.smooth(kernel, radius=3)\n\n        assert_quantity_allclose(smoothed.profile.mean(), desired_mean)\n\n        # smoothing should decrease errors\n        assert smoothed.profile_err.mean() < cosine_profile.profile_err.mean()\n\n    @staticmethod\n    @requires_dependency(""matplotlib"")\n    def test_peek(cosine_profile):\n        with mpl_plot_check():\n            cosine_profile.peek()\n'"
gammapy/estimators/tests/test_sensitivity.py,1,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom gammapy.datasets import SpectrumDataset, SpectrumDatasetOnOff\nfrom gammapy.estimators import SensitivityEstimator\nfrom gammapy.irf import EDispKernel, EffectiveAreaTable\nfrom gammapy.maps import MapAxis, RegionNDMap\n\n\n@pytest.fixture()\ndef spectrum_dataset():\n    e_true = np.logspace(0, 1, 21) * u.TeV\n    e_reco = MapAxis.from_energy_bounds(""1 TeV"", ""10 TeV"", nbin=4)\n    aeff = EffectiveAreaTable.from_constant(value=1e6 * u.m ** 2, energy=e_true)\n    edisp = EDispKernel.from_diagonal_response(e_true, e_reco.edges)\n\n    background = RegionNDMap.create(region=""icrs;circle(0, 0, 0.1)"", axes=[e_reco])\n    background.data += 3600\n    background.data[-1] *= 1e-3\n    return SpectrumDataset(aeff=aeff, livetime=""1h"", edisp=edisp, background=background)\n\n\ndef test_cta_sensitivity_estimator(spectrum_dataset):\n    dataset_on_off = SpectrumDatasetOnOff.from_spectrum_dataset(\n        dataset=spectrum_dataset, acceptance=1, acceptance_off=5\n    )\n    sens = SensitivityEstimator(gamma_min=20)\n    table = sens.run(dataset_on_off)\n\n    assert len(table) == 4\n    assert table.colnames == [""energy"", ""e2dnde"", ""excess"", ""background"", ""criterion""]\n    assert table[""energy""].unit == ""TeV""\n    assert table[""e2dnde""].unit == ""erg / (cm2 s)""\n\n    row = table[0]\n    assert_allclose(row[""energy""], 1.33352, rtol=1e-3)\n    assert_allclose(row[""e2dnde""], 3.40101e-11, rtol=1e-3)\n    assert_allclose(row[""excess""], 334.454, rtol=1e-3)\n    assert_allclose(row[""background""], 3600, rtol=1e-3)\n    assert row[""criterion""] == ""significance""\n\n    row = table[3]\n    assert_allclose(row[""energy""], 7.49894, rtol=1e-3)\n    assert_allclose(row[""e2dnde""], 1.14367e-11, rtol=1e-3)\n    assert_allclose(row[""excess""], 20, rtol=1e-3)\n    assert_allclose(row[""background""], 3.6, rtol=1e-3)\n    assert row[""criterion""] == ""gamma""\n'"
gammapy/estimators/tests/test_ts_map.py,9,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.coordinates import Angle\nfrom gammapy.datasets import MapDataset\nfrom gammapy.estimators import TSMapEstimator\nfrom gammapy.irf import EnergyDependentTablePSF, PSFMap\nfrom gammapy.maps import Map, MapAxis\nfrom gammapy.modeling.models import (\n    BackgroundModel,\n    GaussianSpatialModel,\n    PowerLawSpectralModel,\n    SkyModel,\n)\nfrom gammapy.utils.testing import requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef input_dataset():\n    filename = ""$GAMMAPY_DATA/tests/unbundled/poisson_stats_image/input_all.fits.gz""\n\n    energy = MapAxis.from_energy_bounds(""0.1 TeV"", ""1 TeV"", 1)\n    energy_true = MapAxis.from_energy_bounds(""0.1 TeV"", ""1 TeV"", 1, name=""energy_true"")\n\n    counts2D = Map.read(filename, hdu=""counts"")\n    counts = Map.from_geom(\n        counts2D.geom.to_cube([energy]),\n        data=counts2D.data[np.newaxis, :, :],\n        unit=counts2D.unit,\n    )\n    exposure2D = Map.read(filename, hdu=""exposure"")\n    exposure = Map.from_geom(\n        exposure2D.geom.to_cube([energy_true]),\n        data=exposure2D.data[np.newaxis, :, :],\n        unit=""cm2s"",  # no unit in header?\n    )\n\n    background2D = Map.read(filename, hdu=""background"")\n    background = Map.from_geom(\n        background2D.geom.to_cube([energy]),\n        data=background2D.data[np.newaxis, :, :],\n        unit=background2D.unit,\n    )\n    name = ""test-dataset""\n    background_model = BackgroundModel(background, datasets_names=[name])\n\n    # add mask\n    mask2D = np.ones_like(background2D.data).astype(""bool"")\n    mask2D[0:40, :] = False\n    mask = Map.from_geom(\n        background2D.geom.to_cube([energy]), data=mask2D[np.newaxis, :, :],\n    )\n    return MapDataset(\n        counts=counts,\n        exposure=exposure,\n        models=background_model,\n        mask_safe=mask,\n        name=name,\n    )\n\n\n@pytest.fixture(scope=""session"")\ndef fermi_dataset():\n    size = Angle(""3 deg"", ""3.5 deg"")\n    counts = Map.read(""$GAMMAPY_DATA/fermi-3fhl-gc/fermi-3fhl-gc-counts-cube.fits.gz"")\n    counts = counts.cutout(counts.geom.center_skydir, size)\n\n    background = Map.read(\n        ""$GAMMAPY_DATA/fermi-3fhl-gc/fermi-3fhl-gc-background-cube.fits.gz""\n    )\n    background = background.cutout(background.geom.center_skydir, size)\n    background = BackgroundModel(background, datasets_names=[""fermi-3fhl-gc""])\n\n    exposure = Map.read(\n        ""$GAMMAPY_DATA/fermi-3fhl-gc/fermi-3fhl-gc-exposure-cube.fits.gz""\n    )\n    exposure = exposure.cutout(exposure.geom.center_skydir, size)\n    exposure.unit = ""cm2s""\n    mask_safe = counts.copy(data=np.ones_like(counts.data).astype(""bool""))\n\n    psf = EnergyDependentTablePSF.read(\n        ""$GAMMAPY_DATA/fermi-3fhl-gc/fermi-3fhl-gc-psf-cube.fits.gz""\n    )\n    psfmap = PSFMap.from_energy_dependent_table_psf(psf)\n\n    dataset = MapDataset(\n        counts=counts,\n        models=[background],\n        exposure=exposure,\n        mask_safe=mask_safe,\n        psf=psfmap,\n        name=""fermi-3fhl-gc"",\n    )\n    dataset = dataset.to_image()\n\n    return dataset\n\n\n@requires_data()\ndef test_compute_ts_map(input_dataset):\n    """"""Minimal test of compute_ts_image""""""\n    spatial_model = GaussianSpatialModel(sigma=""0.1 deg"")\n    spectral_model = PowerLawSpectralModel(index=2)\n    model = SkyModel(spatial_model=spatial_model, spectral_model=spectral_model)\n    ts_estimator = TSMapEstimator(\n        model=model, method=""leastsq iter"", threshold=1, kernel_width=""1 deg""\n    )\n    result = ts_estimator.run(input_dataset)\n\n    assert ""leastsq iter"" in repr(ts_estimator)\n    assert_allclose(result[""ts""].data[99, 99], 1704.23, rtol=1e-2)\n    assert_allclose(result[""niter""].data[99, 99], 3)\n    assert_allclose(result[""flux""].data[99, 99], 1.02e-09, rtol=1e-2)\n    assert_allclose(result[""flux_err""].data[99, 99], 3.84e-11, rtol=1e-2)\n    assert_allclose(result[""flux_ul""].data[99, 99], 1.10e-09, rtol=1e-2)\n\n    assert result[""flux""].unit == u.Unit(""cm-2s-1"")\n    assert result[""flux_err""].unit == u.Unit(""cm-2s-1"")\n    assert result[""flux_ul""].unit == u.Unit(""cm-2s-1"")\n\n    # Check mask is correctly taken into account\n    assert np.isnan(result[""ts""].data[30, 40])\n\n\n@requires_data()\ndef test_compute_ts_map_psf(fermi_dataset):\n    estimator = TSMapEstimator(kernel_width=""1 deg"")\n    result = estimator.run(fermi_dataset)\n\n    assert ""root brentq"" in repr(estimator)\n    assert_allclose(result[""ts""].data[29, 29], 852.1548, rtol=1e-2)\n    assert_allclose(result[""niter""].data[29, 29], 7)\n    assert_allclose(result[""flux""].data[29, 29], 1.419909e-09, rtol=1e-2)\n    assert_allclose(result[""flux_err""].data[29, 29], 8.245766e-11, rtol=1e-2)\n    assert_allclose(result[""flux_ul""].data[29, 29], 1.584825e-09, rtol=1e-2)\n    assert result[""flux""].unit == u.Unit(""cm-2s-1"")\n    assert result[""flux_err""].unit == u.Unit(""cm-2s-1"")\n    assert result[""flux_ul""].unit == u.Unit(""cm-2s-1"")\n\n\n@requires_data()\ndef test_compute_ts_map_newton(input_dataset):\n    """"""Minimal test of compute_ts_image""""""\n    spatial_model = GaussianSpatialModel(sigma=""0.1 deg"")\n    spectral_model = PowerLawSpectralModel(index=2)\n    model = SkyModel(spatial_model=spatial_model, spectral_model=spectral_model)\n\n    ts_estimator = TSMapEstimator(\n        model=model, method=""root newton"", threshold=1, kernel_width=""1 deg""\n    )\n    result = ts_estimator.run(input_dataset)\n\n    assert ""root newton"" in repr(ts_estimator)\n    assert_allclose(result[""ts""].data[99, 99], 1714.23, rtol=1e-2)\n    assert_allclose(result[""niter""].data[99, 99], 0)\n    assert_allclose(result[""flux""].data[99, 99], 1.02e-09, rtol=1e-2)\n    assert_allclose(result[""flux_err""].data[99, 99], 3.84e-11, rtol=1e-2)\n    assert_allclose(result[""flux_ul""].data[99, 99], 1.10e-09, rtol=1e-2)\n\n    assert result[""flux""].unit == u.Unit(""cm-2s-1"")\n    assert result[""flux_err""].unit == u.Unit(""cm-2s-1"")\n    assert result[""flux_ul""].unit == u.Unit(""cm-2s-1"")\n\n    # Check mask is correctly taken into account\n    assert np.isnan(result[""ts""].data[30, 40])\n\n\n@requires_data()\ndef test_compute_ts_map_downsampled(input_dataset):\n    """"""Minimal test of compute_ts_image""""""\n    spatial_model = GaussianSpatialModel(sigma=""0.11 deg"")\n    spectral_model = PowerLawSpectralModel(index=2)\n    model = SkyModel(spatial_model=spatial_model, spectral_model=spectral_model)\n\n    ts_estimator = TSMapEstimator(\n        model=model,\n        downsampling_factor=2,\n        method=""root brentq"",\n        error_method=""conf"",\n        ul_method=""conf"",\n        kernel_width=""1 deg"",\n    )\n    result = ts_estimator.run(input_dataset)\n\n    assert_allclose(result[""ts""].data[99, 99], 1661.49, rtol=1e-2)\n    assert_allclose(result[""niter""].data[99, 99], 9)\n    assert_allclose(result[""flux""].data[99, 99], 1.065988e-09, rtol=1e-2)\n    assert_allclose(result[""flux_err""].data[99, 99], 4.005628e-11, rtol=1e-2)\n    assert_allclose(result[""flux_ul""].data[99, 99], 1.147133e-09, rtol=1e-2)\n\n    assert result[""flux""].unit == u.Unit(""cm-2s-1"")\n    assert result[""flux_err""].unit == u.Unit(""cm-2s-1"")\n    assert result[""flux_ul""].unit == u.Unit(""cm-2s-1"")\n\n    # Check mask is correctly taken into account\n    assert np.isnan(result[""ts""].data[30, 40])\n\n\n@requires_data()\ndef test_large_kernel(input_dataset):\n    """"""Minimal test of compute_ts_image""""""\n    spatial_model = GaussianSpatialModel(sigma=""4 deg"")\n    spectral_model = PowerLawSpectralModel(index=2)\n    model = SkyModel(spatial_model=spatial_model, spectral_model=spectral_model)\n    ts_estimator = TSMapEstimator(model=model, kernel_width=""4 deg"")\n\n    with pytest.raises(ValueError):\n        ts_estimator.run(input_dataset)\n\n\ndef test_incorrect_method():\n    model = GaussianSpatialModel(sigma=""0.2 deg"")\n    with pytest.raises(ValueError):\n        TSMapEstimator(model, method=""bad"")\n    with pytest.raises(ValueError):\n        TSMapEstimator(model, error_method=""bad"")\n'"
gammapy/estimators/tests/test_utils.py,1,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom gammapy.estimators.utils import find_peaks\nfrom gammapy.maps import Map\n\n\nclass TestFindPeaks:\n    def test_simple(self):\n        """"""Test a simple example""""""\n        image = Map.create(npix=(10, 5), unit=""s"")\n        image.data[3, 3] = 11\n        image.data[3, 4] = 10\n        image.data[3, 5] = 12\n        image.data[3, 6] = np.nan\n        image.data[0, 9] = 1e20\n\n        table = find_peaks(image, threshold=3)\n\n        assert len(table) == 3\n        assert table[""value""].unit == ""s""\n        assert table[""ra""].unit == ""deg""\n        assert table[""dec""].unit == ""deg""\n\n        row = table[0]\n        assert tuple((row[""x""], row[""y""])) == (9, 0)\n        assert_allclose(row[""value""], 1e20)\n        assert_allclose(row[""ra""], 359.55)\n        assert_allclose(row[""dec""], -0.2)\n\n        row = table[1]\n        assert tuple((row[""x""], row[""y""])) == (5, 3)\n        assert_allclose(row[""value""], 12)\n\n    def test_no_peak(self):\n        image = Map.create(npix=(10, 5))\n        image.data[3, 5] = 12\n\n        table = find_peaks(image, threshold=12.1)\n        assert len(table) == 0\n\n    def test_constant(self):\n        image = Map.create(npix=(10, 5))\n\n        table = find_peaks(image, threshold=3)\n        assert len(table) == 0\n'"
gammapy/irf/tests/__init__.py,0,b''
gammapy/irf/tests/test_background.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom gammapy.irf import Background2D, Background3D\nfrom gammapy.utils.testing import mpl_plot_check, requires_data, requires_dependency\n\n\n@pytest.fixture(scope=""session"")\ndef bkg_3d():\n    """"""Example with simple values to test evaluate""""""\n    energy = [0.1, 10, 1000] * u.TeV\n    fov_lon = [0, 1, 2, 3] * u.deg\n    fov_lat = [0, 1, 2, 3] * u.deg\n\n    data = np.ones((2, 3, 3)) * u.Unit(""s-1 GeV-1 sr-1"")\n    # Axis order is (energy, fov_lon, fov_lat)\n    # data.value[1, 0, 0] = 1\n    data.value[1, 1, 1] = 100\n    return Background3D(\n        energy_lo=energy[:-1],\n        energy_hi=energy[1:],\n        fov_lon_lo=fov_lon[:-1],\n        fov_lon_hi=fov_lon[1:],\n        fov_lat_lo=fov_lat[:-1],\n        fov_lat_hi=fov_lat[1:],\n        data=data,\n    )\n\n\n@requires_data()\ndef test_background_3d_basics(bkg_3d):\n    assert ""NDDataArray summary info"" in str(bkg_3d.data)\n\n    axis = bkg_3d.data.axis(""energy"")\n    assert axis.nbin == 2\n    assert axis.unit == ""TeV""\n\n    axis = bkg_3d.data.axis(""fov_lon"")\n    assert axis.nbin == 3\n    assert axis.unit == ""deg""\n\n    axis = bkg_3d.data.axis(""fov_lat"")\n    assert axis.nbin == 3\n    assert axis.unit == ""deg""\n\n    data = bkg_3d.data.data\n    assert data.shape == (2, 3, 3)\n    assert data.unit == ""s-1 GeV-1 sr-1""\n\n    bkg_2d = bkg_3d.to_2d()\n    assert bkg_2d.data.data.shape == (2, 3)\n\n\ndef test_background_3d_read_write(tmp_path, bkg_3d):\n    bkg_3d.to_fits().writeto(tmp_path / ""bkg3d.fits"")\n    bkg_3d_2 = Background3D.read(tmp_path / ""bkg3d.fits"")\n\n    axis = bkg_3d_2.data.axis(""energy"")\n    assert axis.nbin == 2\n    assert axis.unit == ""TeV""\n\n    axis = bkg_3d_2.data.axis(""fov_lon"")\n    assert axis.nbin == 3\n    assert axis.unit == ""deg""\n\n    axis = bkg_3d_2.data.axis(""fov_lat"")\n    assert axis.nbin == 3\n    assert axis.unit == ""deg""\n\n    data = bkg_3d_2.data.data\n    assert data.shape == (2, 3, 3)\n    assert data.unit == ""s-1 GeV-1 sr-1""\n\n\ndef test_background_3d_evaluate(bkg_3d):\n    # Evaluate at nodes where we put a non-zero value\n    res = bkg_3d.evaluate(\n        fov_lon=[0.5, 1.5] * u.deg,\n        fov_lat=[0.5, 1.5] * u.deg,\n        energy_reco=[100, 100] * u.TeV,\n    )\n    assert_allclose(res.value, [1, 100])\n    assert res.shape == (2,)\n    assert res.unit == ""s-1 GeV-1 sr-1""\n\n    res = bkg_3d.evaluate(\n        fov_lon=[1, 0.5] * u.deg,\n        fov_lat=[1, 0.5] * u.deg,\n        energy_reco=[100, 100] * u.TeV,\n    )\n    assert_allclose(res.value, [3.162278, 1], rtol=1e-5)\n\n    res = bkg_3d.evaluate(\n        fov_lon=[[1, 0.5], [1, 0.5]] * u.deg,\n        fov_lat=[[1, 0.5], [1, 0.5]] * u.deg,\n        energy_reco=[[1, 1], [100, 100]] * u.TeV,\n    )\n    assert_allclose(res.value, [[1, 1], [3.162278, 1]], rtol=1e-5)\n    assert res.shape == (2, 2)\n\n\ndef test_background_3d_integrate(bkg_3d):\n    # Example has bkg rate = 4 s-1 MeV-1 sr-1 at this node:\n    # fov_lon=1.5 deg, fov_lat=1.5 deg, energy=100 TeV\n\n    rate = bkg_3d.evaluate_integrate(\n        fov_lon=[1.5, 1.5] * u.deg,\n        fov_lat=[1.5, 1.5] * u.deg,\n        energy_reco=[100, 100 + 2e-6] * u.TeV,\n    )\n    assert rate.shape == (1,)\n\n    # Expect approximately `rate * de`\n    # with `rate = 4 s-1 sr-1 MeV-1` and `de = 2 MeV`\n    assert_allclose(rate.to(""s-1 sr-1"").value, 0.2, rtol=1e-5)\n\n    rate = bkg_3d.evaluate_integrate(\n        fov_lon=0.5 * u.deg, fov_lat=0.5 * u.deg, energy_reco=[1, 100] * u.TeV\n    )\n    assert_allclose(rate.to(""s-1 sr-1"").value, 99000)\n\n    rate = bkg_3d.evaluate_integrate(\n        fov_lon=[[1, 0.5], [1, 0.5]] * u.deg,\n        fov_lat=[[1, 1], [0.5, 0.5]] * u.deg,\n        energy_reco=[[1, 1], [100, 100]] * u.TeV,\n    )\n    assert rate.shape == (1, 2)\n    assert_allclose(rate.to(""s-1 sr-1"").value, [[99000.0, 99000.0]], rtol=1e-5)\n\n\n@requires_data()\ndef test_background_3D_read():\n    filename = (\n        ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits""\n    )\n    bkg = Background3D.read(filename)\n    data = bkg.data.data\n    assert data.shape == (21, 36, 36)\n    assert data.unit == ""s-1 MeV-1 sr-1""\n\n\n@requires_dependency(""matplotlib"")\ndef test_plot(bkg_2d):\n    with mpl_plot_check():\n        bkg_2d.peek()\n\n\n@pytest.fixture(scope=""session"")\ndef bkg_2d():\n    """"""A simple Background2D test case""""""\n    energy = [0.1, 10, 1000] * u.TeV\n    offset = [0, 1, 2, 3] * u.deg\n    data = np.zeros((2, 3)) * u.Unit(""s-1 MeV-1 sr-1"")\n    data.value[1, 0] = 2\n    data.value[1, 1] = 4\n    return Background2D(\n        energy_lo=energy[:-1],\n        energy_hi=energy[1:],\n        offset_lo=offset[:-1],\n        offset_hi=offset[1:],\n        data=data,\n    )\n\n\ndef test_background_2d_evaluate(bkg_2d):\n    # TODO: the test cases here can probably be improved a bit\n    # There\'s some redundancy, and no case exactly at a node in energy\n\n    # Evaluate at log center between nodes in energy\n    res = bkg_2d.evaluate(\n        fov_lon=[1, 0.5] * u.deg, fov_lat=0 * u.deg, energy_reco=[1, 1] * u.TeV\n    )\n    assert_allclose(res.value, [0, 0])\n    assert res.shape == (2,)\n    assert res.unit == ""s-1 MeV-1 sr-1""\n\n    res = bkg_2d.evaluate(\n        fov_lon=[1, 0.5] * u.deg, fov_lat=0 * u.deg, energy_reco=[100, 100] * u.TeV\n    )\n    assert_allclose(res.value, [3, 2])\n    res = bkg_2d.evaluate(\n        fov_lon=[[1, 0.5], [1, 0.5]] * u.deg,\n        fov_lat=0 * u.deg,\n        energy_reco=[[1, 1], [100, 100]] * u.TeV,\n    )\n\n    assert_allclose(res.value, [[0, 0], [3, 2]])\n    assert res.shape == (2, 2)\n\n    res = bkg_2d.evaluate(\n        fov_lon=[1, 1] * u.deg, fov_lat=0 * u.deg, energy_reco=[1, 100] * u.TeV\n    )\n    assert_allclose(res.value, [0, 3])\n    assert res.shape == (2,)\n\n\ndef test_background_2d_read_write(tmp_path, bkg_2d):\n    bkg_2d.to_fits().writeto(tmp_path / ""tmp.fits"")\n    bkg_2d_2 = Background2D.read(tmp_path / ""tmp.fits"")\n\n    axis = bkg_2d_2.data.axis(""energy"")\n    assert axis.nbin == 2\n    assert axis.unit == ""TeV""\n\n    axis = bkg_2d_2.data.axis(""offset"")\n    assert axis.nbin == 3\n    assert axis.unit == ""deg""\n\n    data = bkg_2d_2.data.data\n    assert data.shape == (2, 3)\n    assert data.unit == ""s-1 MeV-1 sr-1""\n\n\ndef test_background_2d_integrate(bkg_2d):\n    # TODO: change test case to something better (with known answer)\n    # e.g. constant spectrum or power-law.\n\n    rate = bkg_2d.evaluate_integrate(\n        fov_lon=[1, 0.5] * u.deg, fov_lat=[0, 0] * u.deg, energy_reco=[0.1, 0.5] * u.TeV\n    )\n\n    assert rate.shape == (1,)\n    assert_allclose(rate.to(""s-1 sr-1"").value[0], [0, 0])\n\n    rate = bkg_2d.evaluate_integrate(\n        fov_lon=[1, 0.5] * u.deg, fov_lat=[0, 0] * u.deg, energy_reco=[1, 100] * u.TeV\n    )\n    assert_allclose(rate.to(""s-1 sr-1"").value, 0)\n\n    rate = bkg_2d.evaluate_integrate(\n        fov_lon=[[1, 0.5], [1, 0.5]] * u.deg,\n        fov_lat=0 * u.deg,\n        energy_reco=[1, 100] * u.TeV,\n    )\n    assert rate.shape == (1, 2)\n    assert_allclose(rate.value, [[0, 198]])\n\n\n@requires_dependency(""matplotlib"")\ndef test_plot(bkg_2d):\n    with mpl_plot_check():\n        bkg_2d.plot()\n\n    with mpl_plot_check():\n        bkg_2d.plot_energy_dependence()\n\n    with mpl_plot_check():\n        bkg_2d.plot_offset_dependence()\n\n    with mpl_plot_check():\n        bkg_2d.plot_spectrum()\n'"
gammapy/irf/tests/test_edisp_map.py,10,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nfrom astropy.units import Unit\nfrom gammapy.irf import (\n    EDispKernelMap,\n    EDispMap,\n    EffectiveAreaTable2D,\n    EnergyDispersion2D,\n)\nfrom gammapy.makers.utils import make_edisp_map, make_map_exposure_true_energy\nfrom gammapy.maps import Map, MapAxis, MapCoord, WcsGeom\n\n\ndef fake_aeff2d(area=1e6 * u.m ** 2):\n    offsets = np.array((0.0, 1.0, 2.0, 3.0)) * u.deg\n    energy = np.logspace(-1, 1, 5) * u.TeV\n    energy_lo = energy[:-1]\n    energy_hi = energy[1:]\n\n    aeff_values = np.ones((4, 3)) * area\n\n    return EffectiveAreaTable2D(\n        energy_lo,\n        energy_hi,\n        offset_lo=offsets[:-1],\n        offset_hi=offsets[1:],\n        data=aeff_values,\n    )\n\n\ndef make_edisp_map_test():\n    etrue = [0.2, 0.7, 1.5, 2.0, 10.0] * u.TeV\n    migra = np.linspace(0.0, 3.0, 51)\n    offsets = np.array((0.0, 1.0, 2.0, 3.0)) * u.deg\n\n    pointing = SkyCoord(0, 0, unit=""deg"")\n    energy_axis = MapAxis(\n        nodes=[0.2, 0.7, 1.5, 2.0, 10.0],\n        unit=""TeV"",\n        name=""energy_true"",\n        node_type=""edges"",\n        interp=""log"",\n    )\n    migra_axis = MapAxis(nodes=np.linspace(0.0, 3.0, 51), unit="""", name=""migra"")\n\n    edisp2d = EnergyDispersion2D.from_gauss(etrue, migra, 0.0, 0.2, offsets)\n\n    geom = WcsGeom.create(\n        skydir=pointing, binsz=1.0, width=5.0, axes=[migra_axis, energy_axis]\n    )\n\n    aeff2d = fake_aeff2d()\n    exposure_geom = geom.squash(axis=""migra"")\n    exposure_map = make_map_exposure_true_energy(pointing, ""1 h"", aeff2d, exposure_geom)\n\n    return make_edisp_map(edisp2d, pointing, geom, exposure_map)\n\n\ndef test_make_edisp_map():\n    energy_axis = MapAxis(\n        nodes=[0.2, 0.7, 1.5, 2.0, 10.0],\n        unit=""TeV"",\n        name=""energy_true"",\n        node_type=""edges"",\n        interp=""log"",\n    )\n    migra_axis = MapAxis(nodes=np.linspace(0.0, 3.0, 51), unit="""", name=""migra"")\n\n    edmap = make_edisp_map_test()\n\n    assert edmap.edisp_map.geom.axes[0] == migra_axis\n    assert edmap.edisp_map.geom.axes[1] == energy_axis\n    assert edmap.edisp_map.unit == Unit("""")\n    assert edmap.edisp_map.data.shape == (4, 50, 5, 5)\n\n\ndef test_edisp_map_to_from_hdulist():\n    edmap = make_edisp_map_test()\n    hdulist = edmap.to_hdulist()\n    assert ""EDISP"" in hdulist\n    assert ""EDISP_BANDS"" in hdulist\n    assert ""EDISP_EXPOSURE"" in hdulist\n    assert ""EDISP_EXPOSURE_BANDS"" in hdulist\n\n    new_edmap = EDispMap.from_hdulist(hdulist)\n    assert_allclose(edmap.edisp_map.data, new_edmap.edisp_map.data)\n    assert new_edmap.edisp_map.geom == edmap.edisp_map.geom\n    assert new_edmap.exposure_map.geom == edmap.exposure_map.geom\n\n\ndef test_edisp_map_read_write(tmp_path):\n    edisp_map = make_edisp_map_test()\n\n    edisp_map.write(tmp_path / ""tmp.fits"")\n    new_edmap = EDispMap.read(tmp_path / ""tmp.fits"")\n\n    assert_allclose(edisp_map.edisp_map.quantity, new_edmap.edisp_map.quantity)\n\n\ndef test_edisp_map_to_energydispersion():\n    edmap = make_edisp_map_test()\n\n    position = SkyCoord(0, 0, unit=""deg"")\n    e_reco = np.logspace(-0.3, 0.2, 200) * u.TeV\n\n    edisp = edmap.get_edisp_kernel(position, e_reco)\n    # Note that the bias and resolution are rather poorly evaluated on an EnergyDispersion object\n    assert_allclose(edisp.get_bias(e_true=1.0 * u.TeV), 0.0, atol=3e-2)\n    assert_allclose(edisp.get_resolution(e_true=1.0 * u.TeV), 0.2, atol=3e-2)\n\n\ndef test_edisp_map_stacking():\n    edmap1 = make_edisp_map_test()\n    edmap2 = make_edisp_map_test()\n    edmap2.exposure_map.quantity *= 2\n\n    edmap_stack = edmap1.copy()\n    edmap_stack.stack(edmap2)\n    assert_allclose(edmap_stack.edisp_map.data, edmap1.edisp_map.data)\n    assert_allclose(edmap_stack.exposure_map.data, edmap1.exposure_map.data * 3)\n\n\ndef test_sample_coord():\n    edisp_map = make_edisp_map_test()\n\n    coords = MapCoord(\n        {""lon"": [0, 0] * u.deg, ""lat"": [0, 0.5] * u.deg, ""energy_true"": [1, 3] * u.TeV},\n        frame=""icrs"",\n    )\n\n    coords_corrected = edisp_map.sample_coord(map_coord=coords)\n\n    assert len(coords_corrected[""energy""]) == 2\n    assert coords_corrected[""energy""].unit == ""TeV""\n    assert_allclose(coords_corrected[""energy""].value, [1.024664, 3.34484], rtol=1e-5)\n\n\n@pytest.mark.parametrize(""position"", [""0d 0d"", ""180d 0d"", ""0d 90d"", ""180d -90d""])\ndef test_edisp_from_diagonal_response(position):\n    position = SkyCoord(position)\n    energy_axis_true = MapAxis.from_energy_bounds(\n        ""0.3 TeV"", ""10 TeV"", nbin=31, name=""energy_true""\n    )\n    edisp_map = EDispMap.from_diagonal_response(energy_axis_true)\n\n    e_reco = energy_axis_true.edges\n    edisp_kernel = edisp_map.get_edisp_kernel(position, e_reco=e_reco)\n\n    sum_kernel = np.sum(edisp_kernel.data.data, axis=1).data\n\n    # We exclude the first and last bin, where there is no\n    # e_reco to contribute to\n    assert_allclose(sum_kernel[1:-1], 1)\n\ndef test_edisp_map_to_edisp_kernel_map():\n    energy_axis = MapAxis.from_energy_bounds(""1 TeV"", ""10 TeV"", nbin=5)\n\n    energy_axis_true = MapAxis.from_energy_bounds(\n        ""0.3 TeV"", ""30 TeV"", nbin=10, per_decade=True, name=""energy_true""\n    )\n    migra_axis = MapAxis(nodes=np.linspace(0.0, 3.0, 51), unit="""", name=""migra"")\n\n    edisp_map = EDispMap.from_diagonal_response(energy_axis_true,migra_axis)\n\n    edisp_kernel_map = edisp_map.to_edisp_kernel_map(energy_axis)\n    position = SkyCoord(0, 0, unit=""deg"")\n    kernel = edisp_kernel_map.get_edisp_kernel(position)\n\n    assert edisp_kernel_map.exposure_map.geom.axes[0].name == \'energy\'\n    actual = kernel.pdf_matrix.sum(axis=0)\n    assert_allclose(actual, 2.0)\n\ndef test_edisp_kernel_map_stack():\n    energy_axis = MapAxis.from_energy_bounds(""1 TeV"", ""10 TeV"", nbin=5)\n\n    energy_axis_true = MapAxis.from_energy_bounds(\n        ""0.3 TeV"", ""30 TeV"", nbin=10, per_decade=True, name=""energy_true""\n    )\n\n    edisp_1 = EDispKernelMap.from_diagonal_response(\n        energy_axis=energy_axis, energy_axis_true=energy_axis_true\n    )\n    edisp_1.exposure_map.data += 1\n\n    edisp_2 = EDispKernelMap.from_diagonal_response(\n        energy_axis=energy_axis, energy_axis_true=energy_axis_true\n    )\n    edisp_2.exposure_map.data += 2\n\n    geom = edisp_1.edisp_map.geom\n    data = geom.energy_mask(emin=2 * u.TeV)\n    weights = Map.from_geom(geom=geom, data=data)\n    edisp_1.stack(edisp_2, weights=weights)\n\n    position = SkyCoord(0, 0, unit=""deg"")\n    kernel = edisp_1.get_edisp_kernel(position)\n\n    actual = kernel.pdf_matrix.sum(axis=0)\n    exposure = edisp_1.exposure_map.data[:,0,0,0]\n\n    assert_allclose(actual, [2./3., 2./3., 2.0, 2.0, 2.0])\n    assert_allclose(exposure, 3.)\n\ndef test__incorrect_edisp_kernel_map_stack():\n    energy_axis = MapAxis.from_energy_bounds(""1 TeV"", ""10 TeV"", nbin=5)\n\n    energy_axis_true = MapAxis.from_energy_bounds(\n        ""0.3 TeV"", ""30 TeV"", nbin=10, per_decade=True, name=""energy_true""\n    )\n\n    edisp_1 = EDispKernelMap.from_diagonal_response(\n        energy_axis=energy_axis, energy_axis_true=energy_axis_true\n    )\n    edisp_1.exposure_map.data += 1\n\n    edisp_2 = EDispKernelMap.from_diagonal_response(\n        energy_axis=energy_axis, energy_axis_true=energy_axis_true\n    )\n    edisp_2.exposure_map = None\n\n    with pytest.raises(ValueError) as except_info:\n        edisp_1.stack(edisp_2)\n    assert except_info.match(""Missing exposure map for EDispKernelMap.stack"")'"
gammapy/irf/tests/test_effective_area.py,9,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose, assert_equal\nimport astropy.units as u\nfrom gammapy.irf import EffectiveAreaTable, EffectiveAreaTable2D\nfrom gammapy.utils.testing import (\n    assert_quantity_allclose,\n    mpl_plot_check,\n    requires_data,\n    requires_dependency,\n)\n\n\n@pytest.fixture(scope=""session"")\ndef aeff():\n    filename = ""$GAMMAPY_DATA/hess-dl3-dr1/data/hess_dl3_dr1_obs_id_023523.fits.gz""\n    return EffectiveAreaTable2D.read(filename, hdu=""AEFF"")\n\n\nclass TestEffectiveAreaTable2D:\n\n    # TODO: split this out into separate tests, especially the plotting\n    # Add I/O test\n    @staticmethod\n    @requires_data()\n    def test(aeff):\n        assert aeff.data.axis(""energy_true"").nbin == 96\n        assert aeff.data.axis(""offset"").nbin == 6\n        assert aeff.data.data.shape == (96, 6)\n\n        assert aeff.data.axis(""energy_true"").unit == ""TeV""\n        assert aeff.data.axis(""offset"").unit == ""deg""\n        assert aeff.data.data.unit == ""m2""\n\n        assert_quantity_allclose(aeff.high_threshold, 100 * u.TeV, rtol=1e-3)\n        assert_quantity_allclose(aeff.low_threshold, 0.870964 * u.TeV, rtol=1e-3)\n\n        test_val = aeff.data.evaluate(energy_true=""14 TeV"", offset=""0.2 deg"")\n        assert_allclose(test_val.value, 683177.5, rtol=1e-3)\n\n        # Test ARF export\n        offset = 0.236 * u.deg\n        e_axis = np.logspace(0, 1, 20) * u.TeV\n        effareafrom2d = aeff.to_effective_area_table(offset, e_axis)\n\n        energy = np.sqrt(e_axis[:-1] * e_axis[1:])\n        area = aeff.data.evaluate(energy_true=energy, offset=offset)\n        effarea1d = EffectiveAreaTable(\n            energy_lo=e_axis[:-1], energy_hi=e_axis[1:], data=area\n        )\n\n        actual = effareafrom2d.data.evaluate(energy_true=""2.34 TeV"")\n        desired = effarea1d.data.evaluate(energy_true=""2.34 TeV"")\n        assert_equal(actual, desired)\n\n        # Test ARF export #2\n        offset = 1.2 * u.deg\n        actual = aeff.to_effective_area_table(offset=offset).data.data\n        desired = aeff.data.evaluate(offset=offset)\n        assert_allclose(actual.value, desired.value.squeeze(), rtol=1e-9)\n\n    @staticmethod\n    @requires_dependency(""matplotlib"")\n    @requires_data()\n    def test_plot(aeff):\n        with mpl_plot_check():\n            aeff.plot()\n\n        with mpl_plot_check():\n            aeff.plot_energy_dependence()\n\n        with mpl_plot_check():\n            aeff.plot_offset_dependence()\n\n\nclass TestEffectiveAreaTable:\n    @staticmethod\n    @requires_dependency(""matplotlib"")\n    @requires_data()\n    def test_EffectiveAreaTable(tmp_path, aeff):\n        arf = aeff.to_effective_area_table(offset=0.3 * u.deg)\n\n        assert_quantity_allclose(arf.data.evaluate(), arf.data.data)\n\n        with mpl_plot_check():\n            arf.plot()\n\n        arf.write(tmp_path / ""tmp.fits"")\n        arf2 = EffectiveAreaTable.read(tmp_path / ""tmp.fits"")\n\n        assert_quantity_allclose(arf.data.evaluate(), arf2.data.evaluate())\n\n        test_aeff = 0.6 * arf.max_area\n        node_above = np.where(arf.data.data > test_aeff)[0][0]\n        energy = arf.data.axis(""energy_true"")\n        ener_above = energy.center[node_above]\n        ener_below = energy.center[node_above - 1]\n        test_ener = arf.find_energy(test_aeff)\n\n        assert ener_below < test_ener and test_ener < ener_above\n\n        elo_threshold = arf.find_energy(0.1 * arf.max_area)\n        assert elo_threshold.unit == ""TeV""\n        assert_allclose(elo_threshold.value, 0.554086, rtol=1e-3)\n\n        ehi_threshold = arf.find_energy(\n            0.9 * arf.max_area, emin=30 * u.TeV, emax=100 * u.TeV\n        )\n        assert ehi_threshold.unit == ""TeV""\n        assert_allclose(ehi_threshold.value, 53.347217, rtol=1e-3)\n\n        # Test evaluation outside safe range\n        data = [np.nan, np.nan, 0, 0, 1, 2, 3, np.nan, np.nan]\n        energy = np.logspace(0, 10, 10) * u.TeV\n        aeff = EffectiveAreaTable(\n            data=data, energy_lo=energy[:-1], energy_hi=energy[1:]\n        )\n        vals = aeff.evaluate_fill_nan()\n        assert vals[1] == 0\n        assert vals[-1] == 3\n\n    @staticmethod\n    def test_from_parametrization():\n        # Log center of this is 100 GeV\n        energy = [80, 125] * u.GeV\n        area_ref = 1.65469579e07 * u.cm ** 2\n\n        area = EffectiveAreaTable.from_parametrization(energy, ""HESS"")\n\n        assert_allclose(area.data.data, area_ref)\n        assert area.data.data.unit == area_ref.unit\n\n        # Log center of this is 0.1, 2 TeV\n        energy = [0.08, 0.125, 32] * u.TeV\n        area_ref = [1.65469579e07, 1.46451957e09] * u.cm * u.cm\n\n        area = EffectiveAreaTable.from_parametrization(energy, ""HESS"")\n        assert_allclose(area.data.data, area_ref)\n        assert area.data.data.unit == area_ref.unit\n\n        # TODO: Use this to test interpolation behaviour etc.\n\n    @staticmethod\n    def test_write():\n        energy = np.logspace(0, 1, 11) * u.TeV\n        energy_lo = energy[:-1]\n        energy_hi = energy[1:]\n        offset = np.linspace(0, 1, 4) * u.deg\n        offset_lo = offset[:-1]\n        offset_hi = offset[1:]\n        data = np.ones(shape=(len(energy_lo), len(offset_lo))) * u.cm * u.cm\n\n        aeff = EffectiveAreaTable2D(\n            energy_lo=energy_lo,\n            energy_hi=energy_hi,\n            offset_lo=offset_lo,\n            offset_hi=offset_hi,\n            data=data,\n        )\n        hdu = aeff.to_fits()\n        assert_equal(\n            hdu.data[""ENERG_LO""][0], aeff.data.axis(""energy_true"").edges[:-1].value\n        )\n        assert hdu.header[""TUNIT1""] == aeff.data.axis(""energy_true"").unit\n\n\ndef test_compute_thresholds_from_parametrization():\n    energy = np.logspace(-2, 2.0, 100) * u.TeV\n    aeff = EffectiveAreaTable.from_parametrization(energy=energy)\n\n    thresh_lo = aeff.find_energy(aeff=0.1 * aeff.max_area)\n    e_max = aeff.energy.edges[-1]\n    thresh_hi = aeff.find_energy(aeff=0.9 * aeff.max_area, emin=0.1 * e_max, emax=e_max)\n\n    assert_allclose(thresh_lo.to(""TeV"").value, 0.18557, rtol=1e-4)\n    assert_allclose(thresh_hi.to(""TeV"").value, 43.818, rtol=1e-4)\n'"
gammapy/irf/tests/test_energy_dispersion.py,15,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom numpy.testing import assert_allclose, assert_equal\nimport astropy.units as u\nfrom astropy.coordinates import Angle\nfrom gammapy.irf import EDispKernel, EnergyDispersion2D\nfrom gammapy.maps import MapAxis\nfrom gammapy.utils.testing import mpl_plot_check, requires_data, requires_dependency\n\n\nclass TestEDispKernel:\n    def setup(self):\n        self.e_true = np.logspace(0, 1, 101) * u.TeV\n        self.e_reco = self.e_true\n        self.resolution = 0.1\n        self.bias = 0\n        self.edisp = EDispKernel.from_gauss(\n            e_true=self.e_true,\n            e_reco=self.e_reco,\n            pdf_threshold=1e-7,\n            sigma=self.resolution,\n            bias=self.bias,\n        )\n\n    def test_from_diagonal_response(self):\n        e_true = [0.5, 1, 2, 4, 6] * u.TeV\n        e_reco = [2, 4, 6] * u.TeV\n\n        edisp = EDispKernel.from_diagonal_response(e_true, e_reco)\n\n        assert edisp.pdf_matrix.shape == (4, 2)\n        expected = [[0, 0], [0, 0], [1, 0], [0, 1]]\n\n        assert_equal(edisp.pdf_matrix, expected)\n\n        # Test square matrix\n        edisp = EDispKernel.from_diagonal_response(e_true)\n        assert_allclose(edisp.e_reco.edges.value, e_true.value)\n        assert edisp.e_reco.unit == ""TeV""\n        assert_equal(edisp.pdf_matrix[0][0], 1)\n        assert_equal(edisp.pdf_matrix[2][0], 0)\n        assert edisp.pdf_matrix.sum() == 4\n\n    def test_str(self):\n        assert ""EDispKernel"" in str(self.edisp)\n\n    def test_evaluate(self):\n        # Check for correct normalization\n        pdf = self.edisp.data.evaluate(energy_true=3.34 * u.TeV)\n        assert_allclose(np.sum(pdf), 1, atol=1e-2)\n\n    def test_get_bias(self):\n        bias = self.edisp.get_bias(3.34 * u.TeV)\n        assert_allclose(bias, self.bias, atol=1e-2)\n\n    def test_get_resolution(self):\n        resolution = self.edisp.get_resolution(3.34 * u.TeV)\n        assert_allclose(resolution, self.resolution, atol=1e-2)\n\n    def test_io(self, tmp_path):\n        indices = np.array([[1, 3, 6], [3, 3, 2]])\n        desired = self.edisp.pdf_matrix[indices]\n        self.edisp.write(tmp_path / ""tmp.fits"")\n        edisp2 = EDispKernel.read(tmp_path / ""tmp.fits"")\n        actual = edisp2.pdf_matrix[indices]\n        assert_allclose(actual, desired)\n\n    @requires_dependency(""matplotlib"")\n    def test_plot_matrix(self):\n        with mpl_plot_check():\n            self.edisp.plot_matrix()\n\n    @requires_dependency(""matplotlib"")\n    def test_plot_bias(self):\n        with mpl_plot_check():\n            self.edisp.plot_bias()\n\n    @requires_dependency(""matplotlib"")\n    def test_peek(self):\n        with mpl_plot_check():\n            self.edisp.peek()\n\n\n@requires_data()\nclass TestEnergyDispersion2D:\n    @classmethod\n    def setup_class(cls):\n        filename = ""$GAMMAPY_DATA/hess-dl3-dr1/data/hess_dl3_dr1_obs_id_020136.fits.gz""\n        cls.edisp = EnergyDispersion2D.read(filename, hdu=""EDISP"")\n\n        # Make a test case\n        e_true = np.logspace(-1.0, 2.0, 51) * u.TeV\n        migra = np.linspace(0.0, 4.0, 1001)\n        offset = np.linspace(0.0, 2.5, 5) * u.deg\n        sigma = 0.15 / (e_true[:-1] / (1 * u.TeV)).value ** 0.3\n        bias = 1e-3 * (e_true[:-1] - 1 * u.TeV).value\n        cls.edisp2 = EnergyDispersion2D.from_gauss(e_true, migra, bias, sigma, offset)\n\n    def test_str(self):\n        assert ""EnergyDispersion2D"" in str(self.edisp)\n\n    def test_evaluation(self):\n        # Check output shape\n        energy = [1, 2] * u.TeV\n        migra = np.array([0.98, 0.97, 0.7])\n        offset = [0.1, 0.2, 0.3, 0.4] * u.deg\n        actual = self.edisp.data.evaluate(\n            energy_true=energy.reshape(-1, 1, 1),\n            migra=migra.reshape(1, -1, 1),\n            offset=offset.reshape(1, 1, -1),\n        )\n        assert_allclose(actual.shape, (2, 3, 4))\n\n        # Check evaluation at all nodes\n        actual = self.edisp.data.evaluate().shape\n        desired = (\n            self.edisp.data.axis(""energy_true"").nbin,\n            self.edisp.data.axis(""migra"").nbin,\n            self.edisp.data.axis(""offset"").nbin,\n        )\n        assert_equal(actual, desired)\n\n    def test_get_response(self):\n        pdf = self.edisp2.get_response(offset=0.7 * u.deg, e_true=1 * u.TeV)\n        assert_allclose(pdf.sum(), 1)\n        assert_allclose(pdf.max(), 0.010421, rtol=1e-4)\n\n    def test_exporter(self):\n        # Check RMF exporter\n        offset = Angle(0.612, ""deg"")\n        e_reco = MapAxis.from_energy_bounds(1, 10, 7, ""TeV"").edges\n        e_true = MapAxis.from_energy_bounds(0.8, 5, 5, ""TeV"").edges\n        rmf = self.edisp.to_energy_dispersion(offset, e_true=e_true, e_reco=e_reco)\n        assert_allclose(rmf.data.data[2, 3], 0.08, atol=5e-2)  # same tolerance as above\n        actual = rmf.pdf_matrix[2]\n        e_val = np.sqrt(e_true[2] * e_true[3])\n        desired = self.edisp.get_response(offset, e_val, e_reco)\n        assert_equal(actual, desired)\n\n    def test_write(self):\n        energy_lo = np.logspace(0, 1, 11)[:-1] * u.TeV\n        energy_hi = np.logspace(0, 1, 11)[1:] * u.TeV\n        offset_lo = np.linspace(0, 1, 4)[:-1] * u.deg\n        offset_hi = np.linspace(0, 1, 4)[1:] * u.deg\n        migra_lo = np.linspace(0, 3, 4)[:-1]\n        migra_hi = np.linspace(0, 3, 4)[1:]\n\n        data = (\n            np.ones(shape=(len(energy_lo), len(migra_lo), len(offset_lo))) * u.cm * u.cm\n        )\n\n        edisp = EnergyDispersion2D(\n            e_true_lo=energy_lo,\n            e_true_hi=energy_hi,\n            migra_lo=migra_lo,\n            migra_hi=migra_hi,\n            offset_lo=offset_lo,\n            offset_hi=offset_hi,\n            data=data,\n        )\n\n        hdu = edisp.to_fits()\n        energy = edisp.data.axis(""energy_true"").edges\n        assert_equal(hdu.data[""ENERG_LO""][0], energy[:-1].value)\n        assert hdu.header[""TUNIT1""] == edisp.data.axis(""energy_true"").unit\n\n    @requires_dependency(""matplotlib"")\n    def test_plot_migration(self):\n        with mpl_plot_check():\n            self.edisp.plot_migration()\n\n    @requires_dependency(""matplotlib"")\n    def test_plot_bias(self):\n        with mpl_plot_check():\n            self.edisp.plot_bias()\n\n    @requires_dependency(""matplotlib"")\n    def test_peek(self):\n        with mpl_plot_check():\n            self.edisp.peek()\n\n\n@requires_data(""gammapy-data"")\ndef test_get_bias_energy():\n    """"""Obs read from file""""""\n    rmffile = ""$GAMMAPY_DATA/joint-crab/spectra/hess/rmf_obs23523.fits""\n    edisp = EDispKernel.read(rmffile)\n    thresh_lo = edisp.get_bias_energy(0.1)\n    assert_allclose(thresh_lo.to(""TeV"").value, 0.9174, rtol=1e-4)\n'"
gammapy/irf/tests/test_io.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom numpy.testing import assert_allclose\nfrom astropy.units import Quantity\nfrom gammapy.irf import load_cta_irfs\nfrom gammapy.utils.testing import requires_data\n\n\n@requires_data()\ndef test_cta_irf():\n    """"""Test that CTA IRFs can be loaded and evaluated.""""""\n    irf = load_cta_irfs(\n        ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits""\n    )\n\n    energy = Quantity(1, ""TeV"")\n    offset = Quantity(3, ""deg"")\n\n    val = irf[""aeff""].data.evaluate(energy_true=energy, offset=offset)\n    assert_allclose(val.value, 545269.4675, rtol=1e-5)\n    assert val.unit == ""m2""\n\n    val = irf[""edisp""].data.evaluate(offset=offset, energy_true=energy, migra=1)\n    assert_allclose(val.value, 3183.6882, rtol=1e-5)\n    assert val.unit == """"\n\n    psf = irf[""psf""].psf_at_energy_and_theta(energy=energy, theta=offset)\n    val = psf(Quantity(0.1, ""deg""))\n    assert_allclose(val, 4.868832183, rtol=1e-5)\n\n    val = irf[""bkg""].data.evaluate(energy=energy, fov_lon=offset, fov_lat=""0 deg"")\n    assert_allclose(val.value, 9.400071e-05, rtol=1e-5)\n    assert val.unit == ""1 / (MeV s sr)""\n'"
gammapy/irf/tests/test_irf_reduce.py,3,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom astropy.coordinates import Angle, SkyCoord\nfrom gammapy.data import DataStore\nfrom gammapy.irf import make_mean_psf, make_psf\nfrom gammapy.maps import MapAxis\nfrom gammapy.utils.testing import requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef data_store():\n    return DataStore.from_dir(""$GAMMAPY_DATA/hess-dl3-dr1/"")\n\n\n@requires_data()\n@pytest.mark.parametrize(\n    ""pars"",\n    [\n        {\n            ""energy"": None,\n            ""rad"": None,\n            ""energy_shape"": (32,),\n            ""psf_energy"": 865.9643,\n            ""rad_shape"": (144,),\n            ""psf_rad"": 0.0015362848,\n            ""psf_exposure"": 3.14711e12,\n            ""psf_value_shape"": (32, 144),\n            ""psf_value"": 4369.96391,\n        },\n        {\n            ""energy"": MapAxis.from_energy_bounds(1, 10, 100, ""TeV"").edges,\n            ""rad"": None,\n            ""energy_shape"": (101,),\n            ""psf_energy"": 1412.537545,\n            ""rad_shape"": (144,),\n            ""psf_rad"": 0.0015362848,\n            ""psf_exposure"": 4.688142e12,\n            ""psf_value_shape"": (101, 144),\n            ""psf_value"": 3726.58798,\n        },\n        {\n            ""energy"": None,\n            ""rad"": Angle(np.arange(0, 2, 0.002), ""deg""),\n            ""energy_shape"": (32,),\n            ""psf_energy"": 865.9643,\n            ""rad_shape"": (1000,),\n            ""psf_rad"": 0.000524,\n            ""psf_exposure"": 3.14711e12,\n            ""psf_value_shape"": (32, 1000),\n            ""psf_value"": 25888.5047,\n        },\n        {\n            ""energy"": MapAxis.from_energy_bounds(1, 10, 100, ""TeV"").edges,\n            ""rad"": Angle(np.arange(0, 2, 0.002), ""deg""),\n            ""energy_shape"": (101,),\n            ""psf_energy"": 1412.537545,\n            ""rad_shape"": (1000,),\n            ""psf_rad"": 0.000524,\n            ""psf_exposure"": 4.688142e12,\n            ""psf_value_shape"": (101, 1000),\n            ""psf_value"": 22723.879272,\n        },\n    ],\n)\ndef test_make_psf(pars, data_store):\n    psf = make_psf(\n        data_store.obs(23523),\n        position=SkyCoord(83.63, 22.01, unit=""deg""),\n        energy=pars[""energy""],\n        rad=pars[""rad""],\n    )\n\n    assert psf.energy.unit == ""GeV""\n    assert psf.energy.shape == pars[""energy_shape""]\n    assert_allclose(psf.energy.value[15], pars[""psf_energy""], rtol=1e-3)\n\n    assert psf.rad.unit == ""rad""\n    assert psf.rad.shape == pars[""rad_shape""]\n    assert_allclose(psf.rad.value[15], pars[""psf_rad""], rtol=1e-3)\n\n    assert psf.exposure.unit == ""cm2 s""\n    assert psf.exposure.shape == pars[""energy_shape""]\n    assert_allclose(psf.exposure.value[15], pars[""psf_exposure""], rtol=1e-3)\n\n    assert psf.psf_value.unit == ""sr-1""\n    assert psf.psf_value.shape == pars[""psf_value_shape""]\n    assert_allclose(psf.psf_value.value[15, 50], pars[""psf_value""], rtol=1e-3)\n\n\n@requires_data()\ndef test_make_mean_psf(data_store):\n    position = SkyCoord(83.63, 22.01, unit=""deg"")\n\n    observations = data_store.get_observations([23523, 23526])\n    psf = make_mean_psf(observations, position=position)\n\n    assert not np.isnan(psf.psf_value.value).any()\n    assert_allclose(psf.psf_value.value[22, 22], 12206.1665)\n'"
gammapy/irf/tests/test_irf_write.py,13,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.io import fits\nfrom gammapy.irf import Background3D, EffectiveAreaTable2D, EnergyDispersion2D\n\n\nclass TestIRFWrite:\n    def setup(self):\n        self.energy_lo = np.logspace(0, 1, 11)[:-1] * u.TeV\n        self.energy_hi = np.logspace(0, 1, 11)[1:] * u.TeV\n        self.offset_lo = np.linspace(0, 1, 4)[:-1] * u.deg\n        self.offset_hi = np.linspace(0, 1, 4)[1:] * u.deg\n        self.migra_lo = np.linspace(0, 3, 4)[:-1]\n        self.migra_hi = np.linspace(0, 3, 4)[1:]\n        self.fov_lon_lo = np.linspace(-6, 6, 11)[:-1] * u.deg\n        self.fov_lon_hi = np.linspace(-6, 6, 11)[1:] * u.deg\n        self.fov_lat_lo = np.linspace(-6, 6, 11)[:-1] * u.deg\n        self.fov_lat_hi = np.linspace(-6, 6, 11)[1:] * u.deg\n        self.aeff_data = np.random.rand(10, 3) * u.cm * u.cm\n        self.edisp_data = np.random.rand(10, 3, 3)\n        self.bkg_data = np.random.rand(10, 10, 10) / u.MeV / u.s / u.sr\n        self.aeff = EffectiveAreaTable2D(\n            energy_lo=self.energy_lo,\n            energy_hi=self.energy_hi,\n            offset_lo=self.offset_lo,\n            offset_hi=self.offset_hi,\n            data=self.aeff_data,\n        )\n        self.edisp = EnergyDispersion2D(\n            e_true_lo=self.energy_lo,\n            e_true_hi=self.energy_hi,\n            migra_lo=self.migra_lo,\n            migra_hi=self.migra_hi,\n            offset_lo=self.offset_lo,\n            offset_hi=self.offset_hi,\n            data=self.edisp_data,\n        )\n        self.bkg = Background3D(\n            energy_lo=self.energy_lo,\n            energy_hi=self.energy_hi,\n            fov_lon_lo=self.fov_lon_lo,\n            fov_lon_hi=self.fov_lon_hi,\n            fov_lat_lo=self.fov_lat_lo,\n            fov_lat_hi=self.fov_lat_hi,\n            data=self.bkg_data,\n        )\n\n    def test_array_to_container(self):\n        assert_allclose(self.aeff.data.data, self.aeff_data)\n        assert_allclose(self.edisp.data.data, self.edisp_data)\n        assert_allclose(self.bkg.data.data, self.bkg_data)\n\n    def test_container_to_table(self):\n        assert_allclose(self.aeff.to_table()[""ENERG_LO""].quantity[0], self.energy_lo)\n        assert_allclose(self.edisp.to_table()[""ENERG_LO""].quantity[0], self.energy_lo)\n        assert_allclose(self.bkg.to_table()[""ENERG_LO""].quantity[0], self.energy_lo)\n\n        assert_allclose(self.aeff.to_table()[""EFFAREA""].quantity[0].T, self.aeff_data)\n        assert_allclose(self.edisp.to_table()[""MATRIX""].quantity[0].T, self.edisp_data)\n        assert_allclose(self.bkg.to_table()[""BKG""].quantity[0], self.bkg_data)\n\n        assert self.aeff.to_table()[""EFFAREA""].quantity[0].unit == self.aeff_data.unit\n        assert self.bkg.to_table()[""BKG""].quantity[0].unit == self.bkg_data.unit\n\n    def test_container_to_fits(self):\n        assert_allclose(self.aeff.to_table()[""ENERG_LO""].quantity[0], self.energy_lo)\n\n        assert self.aeff.to_fits().header[""EXTNAME""] == ""EFFECTIVE AREA""\n        assert self.edisp.to_fits().header[""EXTNAME""] == ""ENERGY DISPERSION""\n        assert self.bkg.to_fits().header[""EXTNAME""] == ""BACKGROUND""\n\n        assert self.aeff.to_fits(name=""TEST"").header[""EXTNAME""] == ""TEST""\n        assert self.edisp.to_fits(name=""TEST"").header[""EXTNAME""] == ""TEST""\n        assert self.bkg.to_fits(name=""TEST"").header[""EXTNAME""] == ""TEST""\n\n        hdu = self.aeff.to_fits()\n        assert_allclose(\n            hdu.data[hdu.header[""TTYPE1""]][0], self.aeff.data.axes[0].edges[:-1].value\n        )\n        hdu = self.aeff.to_fits()\n        assert_allclose(hdu.data[hdu.header[""TTYPE5""]][0].T, self.aeff.data.data.value)\n\n        hdu = self.edisp.to_fits()\n        assert_allclose(\n            hdu.data[hdu.header[""TTYPE1""]][0], self.edisp.data.axes[0].edges[:-1].value\n        )\n        hdu = self.edisp.to_fits()\n        assert_allclose(hdu.data[hdu.header[""TTYPE7""]][0].T, self.edisp.data.data.value)\n\n        hdu = self.bkg.to_fits()\n        assert_allclose(\n            hdu.data[hdu.header[""TTYPE1""]][0], self.bkg.data.axes[1].edges[:-1].value\n        )\n        hdu = self.bkg.to_fits()\n        assert_allclose(hdu.data[hdu.header[""TTYPE7""]][0], self.bkg.data.data.value)\n\n    def test_writeread(self, tmp_path):\n        path = tmp_path / ""tmp.fits""\n        fits.HDUList(\n            [\n                fits.PrimaryHDU(),\n                self.aeff.to_fits(),\n                self.edisp.to_fits(),\n                self.bkg.to_fits(),\n            ]\n        ).writeto(path)\n\n        read_aeff = EffectiveAreaTable2D.read(path, hdu=""EFFECTIVE AREA"")\n        assert_allclose(read_aeff.data.data, self.aeff_data)\n\n        read_edisp = EnergyDispersion2D.read(path, hdu=""ENERGY DISPERSION"")\n        assert_allclose(read_edisp.data.data, self.edisp_data)\n\n        read_bkg = Background3D.read(path, hdu=""BACKGROUND"")\n        assert_allclose(read_bkg.data.data, self.bkg_data)\n'"
gammapy/irf/tests/test_psf_3d.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nfrom astropy import units as u\nfrom gammapy.irf import PSF3D\nfrom gammapy.utils.testing import mpl_plot_check, requires_data, requires_dependency\n\n\n@pytest.fixture(scope=""session"")\ndef psf_3d():\n    filename = ""$GAMMAPY_DATA/hess-dl3-dr1/data/hess_dl3_dr1_obs_id_023523.fits.gz""\n    return PSF3D.read(filename, hdu=""PSF"")\n\n\n@requires_data()\ndef test_psf_3d_basics(psf_3d):\n    assert_allclose(psf_3d.rad_lo[-1].value, 0.659048, rtol=1e-5)\n    assert psf_3d.rad_lo.shape == (144,)\n    assert psf_3d.rad_lo.unit == ""deg""\n\n    assert_allclose(psf_3d.energy_lo[0].value, 0.01)\n    assert psf_3d.energy_lo.shape == (32,)\n    assert psf_3d.energy_lo.unit == ""TeV""\n\n    assert psf_3d.psf_value.shape == (144, 6, 32)\n    assert psf_3d.psf_value.unit == ""sr-1""\n\n    assert_allclose(psf_3d.energy_thresh_lo.value, 0.01)\n    assert psf_3d.energy_lo.unit == ""TeV""\n\n    assert ""PSF3D"" in psf_3d.info()\n\n\n@requires_data()\ndef test_psf_3d_evaluate(psf_3d):\n    q = psf_3d.evaluate(energy=""1 TeV"", offset=""0.3 deg"", rad=""0.1 deg"")\n    assert_allclose(q.value, 25889.505886)\n    # TODO: is this the shape we want here?\n    assert q.shape == (1, 1, 1)\n    assert q.unit == ""sr-1""\n\n\n@requires_data()\ndef test_to_energy_dependent_table_psf(psf_3d):\n    psf = psf_3d.to_energy_dependent_table_psf()\n    assert psf.psf_value.shape == (32, 144)\n    radius = psf.table_psf_at_energy(""1 TeV"").containment_radius(0.68).deg\n    assert_allclose(radius, 0.124733, atol=1e-4)\n\n\n@requires_data()\ndef test_psf_3d_containment_radius(psf_3d):\n    q = psf_3d.containment_radius(energy=""1 TeV"")\n    assert_allclose(q.value, 0.124733, rtol=1e-2)\n    assert q.isscalar\n    assert q.unit == ""deg""\n\n    q = psf_3d.containment_radius(energy=[1, 3] * u.TeV)\n    assert_allclose(q.value, [0.124733, 0.13762], rtol=1e-2)\n    assert q.shape == (2,)\n\n\n@requires_data()\ndef test_psf_3d_write(psf_3d, tmp_path):\n    psf_3d.write(tmp_path / ""tmp.fits"")\n    psf_3d = PSF3D.read(tmp_path / ""tmp.fits"", hdu=1)\n\n    assert_allclose(psf_3d.energy_lo[0].value, 0.01)\n\n\n@requires_data()\n@requires_dependency(""matplotlib"")\ndef test_psf_3d_plot_vs_rad(psf_3d):\n    with mpl_plot_check():\n        psf_3d.plot_psf_vs_rad()\n\n\n@requires_data()\n@requires_dependency(""matplotlib"")\ndef test_psf_3d_plot_containment(psf_3d):\n    with mpl_plot_check():\n        psf_3d.plot_containment(show_safe_energy=True)\n\n\n@requires_data()\n@requires_dependency(""matplotlib"")\ndef test_psf_3d_peek(psf_3d):\n    with mpl_plot_check():\n        psf_3d.peek()\n'"
gammapy/irf/tests/test_psf_gauss.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose, assert_almost_equal\nfrom astropy import units as u\nfrom astropy.io import fits\nfrom astropy.utils.data import get_pkg_data_filename\nfrom gammapy.irf.psf_gauss import (\n    EnergyDependentMultiGaussPSF,\n    HESSMultiGaussPSF,\n    multi_gauss_psf_kernel,\n)\nfrom gammapy.utils.testing import mpl_plot_check, requires_data, requires_dependency\n\n\n@requires_data()\nclass TestEnergyDependentMultiGaussPSF:\n    @pytest.fixture(scope=""session"")\n    def psf(self):\n        filename = ""$GAMMAPY_DATA/tests/unbundled/irfs/psf.fits""\n        return EnergyDependentMultiGaussPSF.read(filename, hdu=""POINT SPREAD FUNCTION"")\n\n    def test_info(self, psf):\n        info_str = open(get_pkg_data_filename(""data/psf_info.txt"")).read()\n\n        assert psf.info() == info_str\n\n    def test_write(self, tmp_path, psf):\n        psf.write(tmp_path / ""tmp.fits"")\n\n        with fits.open(tmp_path / ""tmp.fits"", memmap=False) as hdu_list:\n            assert len(hdu_list) == 2\n\n    def test_to_table_psf(self, psf):\n        energy = 1 * u.TeV\n        theta = 0 * u.deg\n\n        rad = np.linspace(0, 2, 300) * u.deg\n        table_psf = psf.to_energy_dependent_table_psf(theta, rad=rad)\n\n        psf_at_energy = psf.psf_at_energy_and_theta(energy, theta)\n\n        containment = [0.68, 0.8, 0.9]\n        desired = [psf_at_energy.containment_radius(_) for _ in containment]\n\n        table_psf_at_energy = table_psf.table_psf_at_energy(energy)\n        actual = table_psf_at_energy.containment_radius(containment)\n\n        assert_allclose(desired, actual.degree, rtol=1e-2)\n\n    def test_to_psf3d(self, psf):\n        rads = np.linspace(0.0, 1.0, 101) * u.deg\n        psf_3d = psf.to_psf3d(rads)\n        assert psf_3d.rad_lo.shape == (100,)\n        assert psf_3d.rad_lo.unit == ""deg""\n\n        theta = 0.5 * u.deg\n        energy = 0.5 * u.TeV\n\n        containment = [0.68, 0.8, 0.9]\n        desired = np.array(\n            [psf.containment_radius(energy, theta, _).value for _ in containment]\n        )\n        actual = np.array(\n            [psf_3d.containment_radius(energy, theta, _).value for _ in containment]\n        )\n        assert_allclose(np.squeeze(desired), actual, atol=0.005)\n\n    @requires_dependency(""matplotlib"")\n    def test_peek(self, psf):\n        with mpl_plot_check():\n            psf.peek()\n\n\n@requires_data()\ndef test_psf_cta_1dc():\n    filename = (\n        ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits""\n    )\n    psf_irf = EnergyDependentMultiGaussPSF.read(filename, hdu=""POINT SPREAD FUNCTION"")\n\n    # Check that PSF is filled with 0 for energy / offset where no PSF info is given.\n    # This is needed so that stacked PSF computation doesn\'t error out,\n    # trying to interpolate for observations / energies where this occurs.\n    psf = psf_irf.to_energy_dependent_table_psf(""4.5 deg"")\n    psf = psf.table_psf_at_energy(""0.05 TeV"")\n    assert_allclose(psf.evaluate(rad=""0.03 deg"").value, 0)\n\n    # Check that evaluation works for an energy / offset where an energy is available\n    psf = psf_irf.to_energy_dependent_table_psf(""2 deg"")\n    psf = psf.table_psf_at_energy(""1 TeV"")\n    assert_allclose(psf.containment_radius(0.68).deg, 0.053838, atol=1e-4)\n\n\nclass TestHESS:\n    @staticmethod\n    def test_dpdtheta2():\n        """"""Check that the amplitudes and sigmas were converted correctly in\n        HESS.to_MultiGauss2D() by comparing the dpdtheta2 distribution.\n\n        Note that we set normalize=False in the to_MultiGauss2D call,\n        which is necessary because the HESS PSF is *not* normalized\n        correcly by the HESS software, it is usually a few % off.\n\n        Also quite interesting is to look at the norms, since they\n        represent the fractions of gammas in each of the three components.\n\n        integral: 0.981723\n        sigmas:   [ 0.0219206   0.0905762   0.0426358]\n        norms:    [ 0.29085818  0.20162012  0.48924452]\n\n        So in this case the HESS PSF \'scale\' is 2% too low\n        and e.g. the wide sigma = 0.09 deg PSF component contains 20%\n        of the events.\n        """"""\n        filename = get_pkg_data_filename(""data/psf.txt"")\n        hess = HESSMultiGaussPSF(filename)\n        m = hess.to_MultiGauss2D(normalize=False)\n\n        for theta in np.linspace(0, 1, 10):\n            val_hess = hess.dpdtheta2(theta ** 2)\n            val_m = m.dpdtheta2(theta ** 2)\n            assert_almost_equal(val_hess, val_m, decimal=4)\n\n    @staticmethod\n    def test_gc():\n        """"""Compare the containment radii computed with the HESS software\n        with those found by using MultiGauss2D.\n\n        This test fails for r95, where the HESS software gives a theta\n        which is 10% higher. Probably the triple-Gauss doesn\'t represent\n        the PSF will in the core or the fitting was bad or the\n        HESS software has very large binning errors (they compute\n        containment radius from the theta2 histogram directly, not\n        using the triple-Gauss approximation).""""""\n        vals = [\n            (68, 0.0663391),\n            # TODO: check why this was different before\n            # (95, 0.173846),  # 0.15310963243226974\n            (95, 0.15310967713539758),\n            (10, 0.0162602),\n            (40, 0.0379536),\n            (80, 0.088608),\n        ]\n        filename = get_pkg_data_filename(""data/psf.txt"")\n        hess = HESSMultiGaussPSF(filename)\n        m = hess.to_MultiGauss2D()\n        assert_almost_equal(m.integral, 1)\n        for containment, theta in vals:\n            actual = m.containment_radius(containment / 100.0)\n            assert_almost_equal(actual, theta, decimal=2)\n\n\ndef test_multi_gauss_psf_kernel():\n    psf_data = {\n        ""psf1"": {""ampl"": 1, ""fwhm"": 2.5496814916215014},\n        ""psf2"": {""ampl"": 0.062025099992752075, ""fwhm"": 11.149272133127273},\n        ""psf3"": {""ampl"": 0.47460201382637024, ""fwhm"": 5.164014607542117},\n    }\n    psf_kernel = multi_gauss_psf_kernel(psf_data, x_size=51)\n\n    assert_allclose(psf_kernel.array[25, 25], 0.05047558713797154)\n    assert_allclose(psf_kernel.array[23, 29], 0.003259483464443567)\n'"
gammapy/irf/tests/test_psf_kernel.py,4,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.coordinates import Angle\nfrom gammapy.irf import PSFKernel, TablePSF\nfrom gammapy.maps import MapAxis, WcsGeom\n\n\ndef test_table_psf_to_kernel_map():\n    sigma = 0.5 * u.deg\n    binsz = 0.1 * u.deg\n    geom = WcsGeom.create(binsz=binsz, npix=150)\n\n    rad = Angle(np.linspace(0.0, 3 * sigma.to(""deg"").value, 100), ""deg"")\n    table_psf = TablePSF.from_shape(shape=""gauss"", width=sigma, rad=rad)\n    kernel = PSFKernel.from_table_psf(table_psf, geom)\n    kernel_array = kernel.psf_kernel_map.data\n\n    # Is normalization OK?\n    assert_allclose(kernel_array.sum(), 1.0, atol=1e-5)\n\n    # maximum at the center of map?\n    ind = np.unravel_index(np.argmax(kernel_array, axis=None), kernel_array.shape)\n    # absolute tolerance at 0.5 because of even number of pixel here\n    assert_allclose(ind, geom.center_pix, atol=0.5)\n\n\ndef test_psf_kernel_from_gauss_read_write(tmp_path):\n    sigma = 0.5 * u.deg\n    binsz = 0.1 * u.deg\n    geom = WcsGeom.create(binsz=binsz, npix=150, axes=[MapAxis((0, 1, 2))])\n\n    kernel = PSFKernel.from_gauss(geom, sigma)\n\n    # Check that both maps are identical\n    assert_allclose(kernel.psf_kernel_map.data[0], kernel.psf_kernel_map.data[1])\n\n    # Is there an odd number of pixels\n    assert_allclose(np.array(kernel.psf_kernel_map.geom.npix) % 2, 1)\n\n    kernel.write(tmp_path / ""tmp.fits"", overwrite=True)\n    kernel2 = PSFKernel.read(tmp_path / ""tmp.fits"")\n    assert_allclose(kernel.psf_kernel_map.data, kernel2.psf_kernel_map.data)\n\n\ndef test_psf_kernel_to_image():\n    sigma1 = 0.5 * u.deg\n    sigma2 = 0.2 * u.deg\n    binsz = 0.1 * u.deg\n\n    axis = MapAxis.from_energy_bounds(1, 10, 2, unit=""TeV"", name=""energy_true"")\n    geom = WcsGeom.create(binsz=binsz, npix=50, axes=[axis])\n\n    rad = Angle(np.linspace(0.0, 1.5 * sigma1.to(""deg"").value, 100), ""deg"")\n    table_psf1 = TablePSF.from_shape(shape=""disk"", width=sigma1, rad=rad)\n    table_psf2 = TablePSF.from_shape(shape=""disk"", width=sigma2, rad=rad)\n\n    kernel1 = PSFKernel.from_table_psf(table_psf1, geom)\n    kernel2 = PSFKernel.from_table_psf(table_psf2, geom)\n\n    kernel1.psf_kernel_map.data[1, :, :] = kernel2.psf_kernel_map.data[1, :, :]\n\n    kernel_image_1 = kernel1.to_image()\n    kernel_image_2 = kernel1.to_image(exposure=[1, 2])\n\n    assert_allclose(kernel_image_1.psf_kernel_map.data.sum(), 1.0, atol=1e-5)\n    assert_allclose(kernel_image_1.psf_kernel_map.data[0, 25, 25], 0.028415, atol=1e-5)\n    assert_allclose(kernel_image_1.psf_kernel_map.data[0, 22, 22], 0.009806, atol=1e-5)\n    assert_allclose(kernel_image_1.psf_kernel_map.data[0, 20, 20], 0.0, atol=1e-5)\n\n    assert_allclose(kernel_image_2.psf_kernel_map.data.sum(), 1.0, atol=1e-5)\n    assert_allclose(\n        kernel_image_2.psf_kernel_map.data[0, 25, 25], 0.03791383, atol=1e-5\n    )\n    assert_allclose(kernel_image_2.psf_kernel_map.data[0, 22, 22], 0.0079069, atol=1e-5)\n    assert_allclose(kernel_image_2.psf_kernel_map.data[0, 20, 20], 0.0, atol=1e-5)\n'"
gammapy/irf/tests/test_psf_king.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom astropy.coordinates import Angle\nfrom gammapy.irf import PSFKing\nfrom gammapy.utils.testing import assert_quantity_allclose, requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef psf_king():\n    return PSFKing.read(""$GAMMAPY_DATA/tests/hess_psf_king_023523.fits.gz"")\n\n\n@requires_data()\ndef test_psf_king_evaluate(psf_king):\n    param_off1 = psf_king.evaluate(energy=""1 TeV"", offset=""0 deg"")\n    param_off2 = psf_king.evaluate(""1 TeV"", ""1 deg"")\n\n    assert_quantity_allclose(param_off1[""gamma""], psf_king.gamma[0, 8])\n    assert_quantity_allclose(param_off2[""gamma""], psf_king.gamma[2, 8])\n    assert_quantity_allclose(param_off1[""sigma""], psf_king.sigma[0, 8])\n    assert_quantity_allclose(param_off2[""sigma""], psf_king.sigma[2, 8])\n\n\n@requires_data()\ndef test_psf_king_to_table(psf_king):\n    theta1 = Angle(0, ""deg"")\n    theta2 = Angle(1, ""deg"")\n    psf_king_table_off1 = psf_king.to_energy_dependent_table_psf(theta=theta1)\n    psf_king_table_off2 = psf_king.to_energy_dependent_table_psf(theta=theta2)\n    offset = Angle(1, ""deg"")\n    # energy = Quantity(1, ""TeV"") match with bin number 8\n    # offset equal 1 degre match with the bin 200 in the psf_table\n    value_off1 = psf_king.evaluate_direct(\n        offset, psf_king.gamma[0, 8], psf_king.sigma[0, 8]\n    )\n    value_off2 = psf_king.evaluate_direct(\n        offset, psf_king.gamma[2, 8], psf_king.sigma[2, 8]\n    )\n    # Test that the value at 1 degree in the histogram for the energy 1 Tev and theta=0 or 1 degree is equal to the one\n    # obtained from the self.evaluate_direct() method at 1 degree\n    assert_quantity_allclose(psf_king_table_off1.psf_value[8, 200], value_off1)\n    assert_quantity_allclose(psf_king_table_off2.psf_value[8, 200], value_off2)\n\n    # Test that the integral value is close to one\n    bin_off = psf_king_table_off1.rad[1] - psf_king_table_off1.rad[0]\n    integral = np.sum(\n        psf_king_table_off1.psf_value[8] * 2 * np.pi * psf_king_table_off1.rad * bin_off\n    )\n    assert_quantity_allclose(integral, 1, atol=0.03)\n\n\n@requires_data()\ndef test_psf_king_write(psf_king, tmp_path):\n    psf_king.write(tmp_path / ""tmp.fits"")\n    psf_king2 = PSFKing.read(tmp_path / ""tmp.fits"")\n\n    assert_quantity_allclose(psf_king2.energy, psf_king.energy)\n    assert_quantity_allclose(psf_king2.offset, psf_king.offset)\n    assert_quantity_allclose(psf_king2.gamma, psf_king.gamma)\n    assert_quantity_allclose(psf_king2.sigma, psf_king.sigma)\n'"
gammapy/irf/tests/test_psf_map.py,20,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nfrom astropy.units import Unit\nfrom gammapy.data import DataStore\nfrom gammapy.irf import PSF3D, EffectiveAreaTable2D, EnergyDependentTablePSF, PSFMap\nfrom gammapy.makers.utils import make_map_exposure_true_energy, make_psf_map\nfrom gammapy.maps import MapAxis, MapCoord, WcsGeom\nfrom gammapy.maps.utils import edges_from_lo_hi\nfrom gammapy.utils.testing import requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef data_store():\n    return DataStore.from_dir(""$GAMMAPY_DATA/hess-dl3-dr1/"")\n\n\ndef fake_psf3d(sigma=0.15 * u.deg, shape=""gauss""):\n    offsets = np.array((0.0, 1.0, 2.0, 3.0)) * u.deg\n    energy = np.logspace(-1, 1, 5) * u.TeV\n    energy_lo = energy[:-1]\n    energy_hi = energy[1:]\n    energy = np.sqrt(energy_lo * energy_hi)\n    rad = np.linspace(0, 1.0, 101) * u.deg\n    rad_lo = rad[:-1]\n    rad_hi = rad[1:]\n\n    O, R, E = np.meshgrid(offsets, rad, energy)\n\n    Rmid = 0.5 * (R[:-1] + R[1:])\n    if shape == ""gauss"":\n        val = np.exp(-0.5 * Rmid ** 2 / sigma ** 2)\n    else:\n        val = Rmid < sigma\n    drad = 2 * np.pi * (np.cos(R[:-1]) - np.cos(R[1:])) * u.Unit(""sr"")\n    psf_values = val / ((val * drad).sum(0)[0])\n\n    return PSF3D(energy_lo, energy_hi, offsets, rad_lo, rad_hi, psf_values)\n\n\ndef fake_aeff2d(area=1e6 * u.m ** 2):\n    offsets = np.array((0.0, 1.0, 2.0, 3.0)) * u.deg\n    energy = np.logspace(-1, 1, 5) * u.TeV\n    energy_lo = energy[:-1]\n    energy_hi = energy[1:]\n\n    aeff_values = np.ones((4, 3)) * area\n\n    return EffectiveAreaTable2D(\n        energy_lo,\n        energy_hi,\n        offset_lo=offsets[:-1],\n        offset_hi=offsets[1:],\n        data=aeff_values,\n    )\n\n\ndef test_make_psf_map():\n    psf = fake_psf3d(0.3 * u.deg)\n\n    pointing = SkyCoord(0, 0, unit=""deg"")\n    energy_axis = MapAxis(\n        nodes=[0.2, 0.7, 1.5, 2.0, 10.0], unit=""TeV"", name=""energy_true""\n    )\n    rad_axis = MapAxis(nodes=np.linspace(0.0, 1.0, 51), unit=""deg"", name=""theta"")\n\n    geom = WcsGeom.create(\n        skydir=pointing, binsz=0.2, width=5, axes=[rad_axis, energy_axis]\n    )\n\n    psfmap = make_psf_map(psf, pointing, geom)\n\n    assert psfmap.psf_map.geom.axes[0] == rad_axis\n    assert psfmap.psf_map.geom.axes[1] == energy_axis\n    assert psfmap.psf_map.unit == Unit(""sr-1"")\n    assert psfmap.psf_map.data.shape == (4, 50, 25, 25)\n\n\ndef make_test_psfmap(size, shape=""gauss""):\n    psf = fake_psf3d(size, shape)\n    aeff2d = fake_aeff2d()\n\n    pointing = SkyCoord(0, 0, unit=""deg"")\n    energy_axis = MapAxis(\n        nodes=[0.2, 0.7, 1.5, 2.0, 10.0], unit=""TeV"", name=""energy_true""\n    )\n    rad_axis = MapAxis.from_nodes(\n        nodes=np.linspace(0.0, 0.6, 50), unit=""deg"", name=""theta""\n    )\n\n    geom = WcsGeom.create(\n        skydir=pointing, binsz=0.2, width=5, axes=[rad_axis, energy_axis]\n    )\n\n    exposure_geom = geom.squash(axis=""theta"")\n\n    exposure_map = make_map_exposure_true_energy(pointing, ""1 h"", aeff2d, exposure_geom)\n\n    return make_psf_map(psf, pointing, geom, exposure_map)\n\n\ndef test_psfmap_to_table_psf():\n    psfmap = make_test_psfmap(0.15 * u.deg)\n    psf = fake_psf3d(0.15 * u.deg)\n    # Extract EnergyDependentTablePSF\n    table_psf = psfmap.get_energy_dependent_table_psf(SkyCoord(0, 0, unit=""deg""))\n\n    # Check that containment radius is consistent between psf_table and psf3d\n    assert_allclose(\n        table_psf.containment_radius(1 * u.TeV, 0.9)[0],\n        psf.containment_radius(1 * u.TeV, theta=0 * u.deg, fraction=0.9),\n        rtol=1e-2,\n    )\n    assert_allclose(\n        table_psf.containment_radius(1 * u.TeV, 0.5)[0],\n        psf.containment_radius(1 * u.TeV, theta=0 * u.deg, fraction=0.5),\n        rtol=1e-2,\n    )\n\n\ndef test_psfmap_to_psf_kernel():\n    psfmap = make_test_psfmap(0.15 * u.deg)\n\n    energy_axis = psfmap.psf_map.geom.axes[1]\n    # create PSFKernel\n    kern_geom = WcsGeom.create(binsz=0.02, width=5.0, axes=[energy_axis])\n    psfkernel = psfmap.get_psf_kernel(\n        SkyCoord(1, 1, unit=""deg""), kern_geom, max_radius=1 * u.deg\n    )\n    assert_allclose(psfkernel.psf_kernel_map.data.sum(axis=(1, 2)), 1.0, atol=1e-7)\n\n\ndef test_psfmap_to_from_hdulist():\n    psfmap = make_test_psfmap(0.15 * u.deg)\n    hdulist = psfmap.to_hdulist()\n    assert ""PSF"" in hdulist\n    assert ""PSF_BANDS"" in hdulist\n    assert ""PSF_EXPOSURE"" in hdulist\n    assert ""PSF_EXPOSURE_BANDS"" in hdulist\n\n    new_psfmap = PSFMap.from_hdulist(hdulist)\n    assert_allclose(psfmap.psf_map.data, new_psfmap.psf_map.data)\n    assert new_psfmap.psf_map.geom == psfmap.psf_map.geom\n    assert new_psfmap.exposure_map.geom == psfmap.exposure_map.geom\n\n\ndef test_psfmap_read_write(tmp_path):\n    psfmap = make_test_psfmap(0.15 * u.deg)\n\n    psfmap.write(tmp_path / ""tmp.fits"")\n    new_psfmap = PSFMap.read(tmp_path / ""tmp.fits"")\n\n    assert_allclose(psfmap.psf_map.quantity, new_psfmap.psf_map.quantity)\n\n\ndef test_containment_radius_map():\n    psf = fake_psf3d(0.15 * u.deg)\n    pointing = SkyCoord(0, 0, unit=""deg"")\n    energy_axis = MapAxis(nodes=[0.2, 1, 2], unit=""TeV"", name=""energy_true"")\n    psf_theta_axis = MapAxis(nodes=np.linspace(0.0, 0.6, 30), unit=""deg"", name=""theta"")\n    geom = WcsGeom.create(\n        skydir=pointing, binsz=0.5, width=(4, 3), axes=[psf_theta_axis, energy_axis]\n    )\n\n    psfmap = make_psf_map(psf=psf, pointing=pointing, geom=geom)\n    m = psfmap.containment_radius_map(1 * u.TeV)\n    coord = SkyCoord(0.3, 0, unit=""deg"")\n    val = m.interp_by_coord(coord)\n    assert_allclose(val, 0.226477, rtol=1e-3)\n\n\ndef test_psfmap_stacking():\n    psfmap1 = make_test_psfmap(0.1 * u.deg, shape=""flat"")\n    psfmap2 = make_test_psfmap(0.1 * u.deg, shape=""flat"")\n    psfmap2.exposure_map.quantity *= 2\n\n    psfmap_stack = psfmap1.copy()\n    psfmap_stack.stack(psfmap2)\n    assert_allclose(psfmap_stack.psf_map.data, psfmap1.psf_map.data)\n    assert_allclose(psfmap_stack.exposure_map.data, psfmap1.exposure_map.data * 3)\n\n    psfmap3 = make_test_psfmap(0.3 * u.deg, shape=""flat"")\n\n    psfmap_stack = psfmap1.copy()\n    psfmap_stack.stack(psfmap3)\n\n    assert_allclose(psfmap_stack.psf_map.data[0, 40, 20, 20], 0.0)\n    assert_allclose(psfmap_stack.psf_map.data[0, 20, 20, 20], 5805.28955078125)\n    assert_allclose(psfmap_stack.psf_map.data[0, 0, 20, 20], 58052.78955078125)\n\n\n# TODO: add a test comparing make_mean_psf and PSFMap.stack for a set of observations in an Observations\n\n\ndef test_sample_coord():\n    psf_map = make_test_psfmap(0.1 * u.deg, shape=""gauss"")\n\n    coords_in = MapCoord(\n        {""lon"": [0, 0] * u.deg, ""lat"": [0, 0.5] * u.deg, ""energy_true"": [1, 3] * u.TeV},\n        frame=""icrs"",\n    )\n\n    coords = psf_map.sample_coord(map_coord=coords_in)\n    assert coords.frame == ""icrs""\n    assert len(coords.lon) == 2\n    assert_allclose(coords.lon, [0.07498478, 0.04274561], rtol=1e-3)\n    assert_allclose(coords.lat, [-0.10173629, 0.34703959], rtol=1e-3)\n\n\ndef test_sample_coord_gauss():\n    psf_map = make_test_psfmap(0.1 * u.deg, shape=""gauss"")\n\n    lon, lat = np.zeros(10000) * u.deg, np.zeros(10000) * u.deg\n    energy = np.ones(10000) * u.TeV\n    coords_in = MapCoord.create(\n        {""lon"": lon, ""lat"": lat, ""energy_true"": energy}, frame=""icrs""\n    )\n    coords = psf_map.sample_coord(coords_in)\n\n    assert_allclose(np.mean(coords.skycoord.data.lon.wrap_at(""180d"").deg), 0, atol=2e-3)\n    assert_allclose(np.mean(coords.lat), 0, atol=2e-3)\n\n\ndef make_psf_map_obs(geom, obs):\n    exposure_map = make_map_exposure_true_energy(\n        geom=geom.squash(axis=""theta""),\n        pointing=obs.pointing_radec,\n        aeff=obs.aeff,\n        livetime=obs.observation_live_time_duration,\n    )\n\n    psf_map = make_psf_map(\n        geom=geom, psf=obs.psf, pointing=obs.pointing_radec, exposure_map=exposure_map\n    )\n    return psf_map\n\n\n@requires_data()\n@pytest.mark.parametrize(\n    ""pars"",\n    [\n        {\n            ""energy"": None,\n            ""rad"": None,\n            ""energy_shape"": (32,),\n            ""psf_energy"": 865.9643,\n            ""rad_shape"": (144,),\n            ""psf_rad"": 0.0015362848,\n            ""psf_exposure"": 3.14711e12,\n            ""psf_value_shape"": (32, 144),\n            ""psf_value"": 4369.96391,\n        },\n        {\n            ""energy"": MapAxis.from_energy_bounds(1, 10, 100, ""TeV"", name=""energy_true""),\n            ""rad"": None,\n            ""energy_shape"": (100,),\n            ""psf_energy"": 1428.893959,\n            ""rad_shape"": (144,),\n            ""psf_rad"": 0.0015362848,\n            ""psf_exposure"": 4.723409e12,\n            ""psf_value_shape"": (100, 144),\n            ""psf_value"": 3719.21488,\n        },\n        {\n            ""energy"": None,\n            ""rad"": MapAxis.from_nodes(np.arange(0, 2, 0.002), unit=""deg"", name=""theta""),\n            ""energy_shape"": (32,),\n            ""psf_energy"": 865.9643,\n            ""rad_shape"": (1000,),\n            ""psf_rad"": 0.000524,\n            ""psf_exposure"": 3.14711e12,\n            ""psf_value_shape"": (32, 1000),\n            ""psf_value"": 25888.5047,\n        },\n        {\n            ""energy"": MapAxis.from_energy_bounds(1, 10, 100, ""TeV"", name=""energy_true""),\n            ""rad"": MapAxis.from_nodes(np.arange(0, 2, 0.002), unit=""deg"", name=""theta""),\n            ""energy_shape"": (100,),\n            ""psf_energy"": 1428.893959,\n            ""rad_shape"": (1000,),\n            ""psf_rad"": 0.000524,\n            ""psf_exposure"": 4.723409e12,\n            ""psf_value_shape"": (100, 1000),\n            ""psf_value"": 22561.543595,\n        },\n    ],\n)\ndef test_make_psf(pars, data_store):\n    obs = data_store.obs(23523)\n    psf = obs.psf\n\n    if pars[""energy""] is None:\n        edges = edges_from_lo_hi(psf.energy_lo, psf.energy_hi)\n        energy_axis = MapAxis.from_edges(edges, interp=""log"", name=""energy_true"")\n    else:\n        energy_axis = pars[""energy""]\n\n    if pars[""rad""] is None:\n        edges = edges_from_lo_hi(psf.rad_lo, psf.rad_hi)\n        rad_axis = MapAxis.from_edges(edges, name=""theta"")\n    else:\n        rad_axis = pars[""rad""]\n\n    position = SkyCoord(83.63, 22.01, unit=""deg"")\n\n    geom = WcsGeom.create(\n        skydir=position, npix=(3, 3), axes=[rad_axis, energy_axis], binsz=0.2\n    )\n    psf_map = make_psf_map_obs(geom, obs)\n    psf = psf_map.get_energy_dependent_table_psf(position)\n\n    assert psf.energy.unit == ""GeV""\n    assert psf.energy.shape == pars[""energy_shape""]\n    assert_allclose(psf.energy.value[15], pars[""psf_energy""], rtol=1e-3)\n\n    assert psf.rad.unit == ""rad""\n    assert psf.rad.shape == pars[""rad_shape""]\n    assert_allclose(psf.rad.value[15], pars[""psf_rad""], rtol=1e-3)\n\n    assert psf.exposure.unit == ""cm2 s""\n    assert psf.exposure.shape == pars[""energy_shape""]\n    assert_allclose(psf.exposure.value[15], pars[""psf_exposure""], rtol=1e-3)\n\n    assert psf.psf_value.unit == ""sr-1""\n    assert psf.psf_value.shape == pars[""psf_value_shape""]\n    assert_allclose(psf.psf_value.value[15, 50], pars[""psf_value""], rtol=1e-3)\n\n\n@requires_data()\ndef test_make_mean_psf(data_store):\n    observations = data_store.get_observations([23523, 23526])\n    position = SkyCoord(83.63, 22.01, unit=""deg"")\n\n    psf = observations[0].psf\n\n    edges = edges_from_lo_hi(psf.energy_lo, psf.energy_hi)\n    energy_axis = MapAxis.from_edges(edges, interp=""log"", name=""energy_true"")\n\n    edges = edges_from_lo_hi(psf.rad_lo, psf.rad_hi)\n    rad_axis = MapAxis.from_edges(edges, name=""theta"")\n\n    geom = WcsGeom.create(\n        skydir=position, npix=(3, 3), axes=[rad_axis, energy_axis], binsz=0.2\n    )\n\n    psf_map_1 = make_psf_map_obs(geom, observations[0])\n    psf_map_2 = make_psf_map_obs(geom, observations[1])\n\n    stacked_psf = psf_map_1.copy()\n    stacked_psf.stack(psf_map_2)\n\n    psf = stacked_psf.get_energy_dependent_table_psf(position)\n\n    assert not np.isnan(psf.psf_value.value).any()\n    assert_allclose(psf.psf_value.value[22, 22], 12206.1665, rtol=1e-3)\n\n\n@requires_data()\n@pytest.mark.parametrize(""position"", [""0d 0d"", ""180d 0d"", ""0d 90d"", ""180d -90d""])\ndef test_psf_map_from_table_psf(position):\n    position = SkyCoord(position)\n    filename = ""$GAMMAPY_DATA/fermi_3fhl/fermi_3fhl_psf_gc.fits.gz""\n    table_psf = EnergyDependentTablePSF.read(filename)\n    psf_map = PSFMap.from_energy_dependent_table_psf(table_psf)\n\n    table_psf_new = psf_map.get_energy_dependent_table_psf(position)\n\n    assert_allclose(table_psf_new.psf_value.value, table_psf.psf_value.value)\n    assert table_psf_new.psf_value.unit == ""sr-1""\n\n    assert_allclose(table_psf_new.exposure.value, table_psf.exposure.value)\n    assert table_psf_new.exposure.unit == ""cm2 s""\n\n\ndef test_to_image():\n    psfmap = make_test_psfmap(0.15 * u.deg)\n\n    psf2D = psfmap.to_image()\n    assert_allclose(psf2D.psf_map.geom.data_shape, (1, 50, 25, 25))\n    assert_allclose(psf2D.exposure_map.geom.data_shape, (1, 1, 25, 25))\n    assert_allclose(psf2D.psf_map.data[0][0][12][12], 23255.41204827, rtol=1e-2)\n'"
gammapy/irf/tests/test_psf_table.py,4,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom astropy import units as u\nfrom astropy.coordinates import Angle\nfrom gammapy.irf import EnergyDependentTablePSF, TablePSF\nfrom gammapy.utils.testing import mpl_plot_check, requires_data, requires_dependency\n\n\nclass TestTablePSF:\n    @staticmethod\n    def test_gauss():\n        # Make an example PSF for testing\n        width = Angle(0.3, ""deg"")\n\n        # containment radius for 80% containment\n        radius = width * np.sqrt(2 * np.log(5))\n\n        rad = Angle(np.linspace(0, 2.3, 1000), ""deg"")\n        psf = TablePSF.from_shape(shape=""gauss"", width=width, rad=rad)\n\n        assert_allclose(psf.containment(radius), 0.8, rtol=1e-4)\n\n        desired = radius.to_value(""deg"")\n        actual = psf.containment_radius(0.8).to_value(""deg"")\n        assert_allclose(actual, desired, rtol=1e-4)\n\n    @staticmethod\n    def test_disk():\n        width = Angle(2, ""deg"")\n        rad = Angle(np.linspace(0, 2.3, 1000), ""deg"")\n        psf = TablePSF.from_shape(shape=""disk"", width=width, rad=rad)\n\n        # test containment\n        radius = Angle(1, ""deg"")\n        actual = psf.containment(radius)\n        desired = (radius / width).to_value("""") ** 2\n        assert_allclose(actual, desired, rtol=1e-4)\n\n        # test containment radius\n        actual = psf.containment_radius(0.25).deg\n        assert_allclose(actual, radius.deg, rtol=1e-4)\n\n        # test info\n        info = psf.info()\n        assert info.find(""integral"") == 58\n\n        # test broaden\n        psf.broaden(2, normalize=True)\n        actual = psf.containment_radius(0.25).deg\n        assert_allclose(actual, 2 * radius.deg, rtol=1e-4)\n\n\n@requires_data()\nclass TestEnergyDependentTablePSF:\n    def setup(self):\n        filename = ""$GAMMAPY_DATA/tests/unbundled/fermi/psf.fits""\n        self.psf = EnergyDependentTablePSF.read(filename)\n\n    def test(self):\n        # TODO: test __init__\n\n        # Test cases\n        energy = u.Quantity(1, ""GeV"")\n\n        psf1 = self.psf.table_psf_at_energy(energy)\n        containment = np.linspace(0, 0.95, 3)\n        actual = psf1.containment_radius(containment).to_value(""deg"")\n        desired = [0.0, 0.195423, 1.036735]\n        assert_allclose(actual, desired, rtol=1e-5)\n\n        # TODO: test average_psf\n        # TODO: test containment_radius\n        # TODO: test containment_fraction\n        # TODO: test info\n        # TODO: test plotting methods\n\n        energy_band = u.Quantity([10, 500], ""GeV"")\n        psf_band = self.psf.table_psf_in_energy_band(energy_band)\n        # TODO: add assert\n\n    @requires_dependency(""matplotlib"")\n    def test_plot(self):\n        with mpl_plot_check():\n            self.psf.plot_containment_vs_energy()\n\n        energy = u.Quantity(1, ""GeV"")\n        psf_1GeV = self.psf.table_psf_at_energy(energy)\n        with mpl_plot_check():\n            psf_1GeV.plot_psf_vs_rad()\n\n    @requires_dependency(""matplotlib"")\n    def test_plot2(self):\n        with mpl_plot_check():\n            self.psf.plot_psf_vs_rad()\n\n    @requires_dependency(""matplotlib"")\n    def test_plot_exposure_vs_energy(self):\n        with mpl_plot_check():\n            self.psf.plot_exposure_vs_energy()\n\n    def test_write(self, tmp_path):\n        self.psf.write(tmp_path / ""test.fits"")\n        new = EnergyDependentTablePSF.read(tmp_path / ""test.fits"")\n        assert_allclose(new.rad.to_value(""deg""), self.psf.rad.to_value(""deg""))\n        assert_allclose(new.energy.to_value(""GeV""), self.psf.energy.to_value(""GeV""))\n        assert_allclose(new.psf_value.value, self.psf.psf_value.value)\n\n    def test_repr(self):\n        info = str(self.psf)\n        assert ""Containment"" in info\n'"
gammapy/makers/background/__init__.py,0,b'from .fov import *\nfrom .phase import *\nfrom .reflected import *\nfrom .ring import *\n'
gammapy/makers/background/fov.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""FoV background estimation.""""""\nimport logging\nfrom gammapy.maps import Map\n\n__all__ = [""FoVBackgroundMaker""]\n\nlog = logging.getLogger(__name__)\n\n\nclass FoVBackgroundMaker:\n    """"""Normalize template background on the whole field-of-view.\n\n    The dataset background model can be simply scaled (method=""scale"") or fitted (method=""fit"")\n    on the dataset counts.\n\n    The normalization is performed outside the exclusion mask that is passed on init.\n\n    If a SkyModel is set on the input dataset and method is \'fit\', its are frozen during\n    the fov normalization fit.\n\n    Parameters\n    ----------\n    method : str in [\'fit\', \'scale\']\n        the normalization method to be applied. Default \'scale\'.\n    exclusion_mask : `~gammapy.maps.WcsNDMap`\n        Exclusion mask\n    """"""\n\n    def __init__(self, method=""scale"", exclusion_mask=None):\n        if method in [""fit"", ""scale""]:\n            self.method = method\n        else:\n            raise ValueError(f""Incorrect method for FoVBackgroundMaker: {method}."")\n        self.exclusion_mask = exclusion_mask\n\n    def run(self, dataset):\n        """"""Run FoV background maker.\n\n        Fit the background model norm\n\n        Parameters\n        ----------\n        dataset : `~gammapy.cube.fit.MapDataset`\n            Input map dataset.\n\n        """"""\n        mask_fit = dataset.mask_fit\n        dataset.mask_fit = self._reproject_exclusion_mask(dataset)\n\n        if self.method is ""fit"":\n            self._fit_bkg(dataset)\n        else:\n            self._scale_bkg(dataset)\n\n        dataset.mask_fit = mask_fit\n        return dataset\n\n    def _reproject_exclusion_mask(self, dataset):\n        """"""Reproject the exclusion on the dataset geometry""""""\n        mask_map = Map.from_geom(dataset.counts.geom)\n        if self.exclusion_mask is not None:\n            coords = dataset.counts.geom.get_coord()\n            vals = self.exclusion_mask.get_by_coord(coords)\n            mask_map.data += vals\n        else:\n            mask_map.data[...] = 1\n\n        return mask_map.data.astype(""bool"")\n\n    def _fit_bkg(self, dataset):\n        """"""Fit the FoV background model on the dataset counts data""""""\n        from gammapy.modeling import Fit\n        from gammapy.datasets import Datasets\n\n        # freeze all model components not related to background model\n        datasets = Datasets([dataset])\n\n        parameters_frozen = []\n        for par in datasets.parameters:\n            parameters_frozen.append(par.frozen)\n            if par not in dataset.background_model.parameters:\n                par.frozen = True\n\n        fit = Fit(datasets)\n        fit_result = fit.run()\n        if fit_result.success is False:\n            log.info(\n                f""FoVBackgroundMaker failed. No fit convergence for {dataset.name}.""\n            )\n\n        # Unfreeze parameters\n        for idx, par in enumerate(datasets.parameters):\n            par.frozen = parameters_frozen[idx]\n\n    def _scale_bkg(self, dataset):\n        """"""Fit the FoV background model on the dataset counts data""""""\n        mask = dataset.mask\n        count_tot = dataset.counts.data[mask].sum()\n        bkg_tot = dataset.background_model.map.data[mask].sum()\n\n        if count_tot <= 0.0:\n            log.info(\n                f""FoVBackgroundMaker failed. No counts found outside exclusion mask for {dataset.name}.""\n            )\n        elif bkg_tot <= 0.0:\n            log.info(\n                f""FoVBackgroundMaker failed. No positive background found outside exclusion mask for {dataset.name}.""\n            )\n        else:\n            scale = count_tot / bkg_tot\n            dataset.background_model.norm.value = scale\n'"
gammapy/makers/background/phase.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom gammapy.data import EventList\nfrom gammapy.datasets import SpectrumDatasetOnOff\nfrom gammapy.maps import RegionNDMap\n\n__all__ = [""PhaseBackgroundMaker""]\n\n\nclass PhaseBackgroundMaker:\n    """"""Background estimation with on and off phases.\n\n    TODO: For a usage example see future notebook.\n\n    TODO: The phase interval has to be between 0 and 1.\n    Cases like [-0.1, 0.1], for example, are still not supported.\n\n    Parameters\n    ----------\n    on_phase : `tuple` or list of tuples\n        on-phase defined by the two edges of each interval (edges are excluded)\n    off_phase : `tuple` or list of tuples\n        off-phase defined by the two edges of each interval (edges are excluded)\n    """"""\n\n    def __init__(self, on_phase, off_phase):\n        self.on_phase = self._check_intervals(on_phase)\n        self.off_phase = self._check_intervals(off_phase)\n\n    def __str__(self):\n        s = self.__class__.__name__\n        s += f""\\n{self.on_phase}""\n        s += f""\\n{self.off_phase}""\n        return s\n\n    @staticmethod\n    def _make_counts(dataset, observation, phases):\n\n        event_lists = []\n        for interval in phases:\n            events = observation.events.select_parameter(\n                parameter=""PHASE"", band=interval\n            )\n            event_lists.append(events)\n\n        events = EventList.stack(event_lists)\n        counts = RegionNDMap.from_geom(dataset.counts.geom)\n        counts.fill_events(events)\n        return counts\n\n    def make_counts_off(self, dataset, observation):\n        """"""Make off counts.\n\n        Parameters\n        ----------\n        dataset : `SpectrumDataset`\n            Input dataset.\n        observation : `DatastoreObservation`\n            Data store observation.\n\n        Returns\n        -------\n        counts_off : `RegionNDMap`\n            Off counts.\n        """"""\n        return self._make_counts(dataset, observation, self.off_phase)\n\n    def make_counts(self, dataset, observation):\n        """"""Make off counts.\n\n        Parameters\n        ----------\n        dataset : `SpectrumDataset`\n            Input dataset.\n        observation : `DatastoreObservation`\n            Data store observation.\n\n        Returns\n        -------\n        counts_off : `RegionNDMap`\n            Off counts.\n        """"""\n        return self._make_counts(dataset, observation, self.on_phase)\n\n    def run(self, dataset, observation):\n        """"""Run all steps.\n\n        Parameters\n        ----------\n        dataset : `SpectrumDataset`\n            Input dataset.\n        observation : `Observation`\n            Data store observation.\n\n        Returns\n        -------\n        dataset_on_off : `SpectrumDatasetOnOff`\n            On off dataset.\n        """"""\n        counts_off = self.make_counts_off(dataset, observation)\n        counts = self.make_counts(dataset, observation)\n        acceptance = np.sum([_[1] - _[0] for _ in self.on_phase])\n        acceptance_off = np.sum([_[1] - _[0] for _ in self.off_phase])\n\n        dataset_on_off = SpectrumDatasetOnOff.from_spectrum_dataset(\n            dataset=dataset,\n            counts_off=counts_off,\n            acceptance=acceptance,\n            acceptance_off=acceptance_off,\n        )\n        dataset_on_off.counts = counts\n        return dataset_on_off\n\n    @staticmethod\n    def _check_intervals(intervals):\n        """"""Split phase intervals that go beyond phase 1""""""\n        if isinstance(intervals, tuple):\n            intervals = [intervals]\n\n        for phase_interval in intervals:\n            if phase_interval[0] > phase_interval[1]:\n                intervals.remove(phase_interval)\n                intervals.append([phase_interval[0], 1])\n                intervals.append([0, phase_interval[1]])\n        return intervals\n'"
gammapy/makers/background/reflected.py,5,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport logging\nimport numpy as np\nfrom astropy import units as u\nfrom astropy.coordinates import Angle\nfrom regions import PixCoord\nfrom gammapy.datasets import SpectrumDatasetOnOff\nfrom gammapy.maps import RegionGeom, RegionNDMap, WcsNDMap\nfrom gammapy.utils.regions import list_to_compound_region\n\n__all__ = [""ReflectedRegionsFinder"", ""ReflectedRegionsBackgroundMaker""]\n\nlog = logging.getLogger(__name__)\n\n\nclass ReflectedRegionsFinder:\n    """"""Find reflected regions.\n\n    This class is responsible for placing :ref:`region_reflected` for a given\n    input region and pointing position. It converts to pixel coordinates\n    internally assuming a tangent projection at center position.\n\n    If the center lies inside the input region, no reflected regions\n    can be found.\n\n    If you want to make a\n    background estimate for an IACT observation using the reflected regions\n    method, see also `~gammapy.spectrum.ReflectedRegionsBackgroundMaker`\n\n    Parameters\n    ----------\n    region : `~regions.SkyRegion`\n        Region to rotate\n    center : `~astropy.coordinates.SkyCoord`\n        Rotation point\n    angle_increment : `~astropy.coordinates.Angle`, optional\n        Rotation angle applied when a region falls in an excluded region.\n    min_distance : `~astropy.coordinates.Angle`, optional\n        Minimal distance between two consecutive reflected regions\n    min_distance_input : `~astropy.coordinates.Angle`, optional\n        Minimal distance from input region\n    max_region_number : int, optional\n        Maximum number of regions to use\n    exclusion_mask : `~gammapy.maps.WcsNDMap`, optional\n        Exclusion mask\n    binsz : `~astropy.coordinates.Angle`\n        Bin size of the reference map used for region finding.\n\n    Examples\n    --------\n    >>> from astropy.coordinates import SkyCoord, Angle\n    >>> from regions import CircleSkyRegion\n    >>> from gammapy.makers import ReflectedRegionsFinder\n    >>> pointing = SkyCoord(83.2, 22.7, unit=\'deg\', frame=\'icrs\')\n    >>> target_position = SkyCoord(80.2, 23.5, unit=\'deg\', frame=\'icrs\')\n    >>> theta = Angle(0.4, \'deg\')\n    >>> on_region = CircleSkyRegion(target_position, theta)\n    >>> finder = ReflectedRegionsFinder(min_distance_input=\'1 rad\', region=on_region, center=pointing)\n    >>> finder.run()\n    >>> print(finder.reflected_regions[0])\n    Region: CircleSkyRegion\n    center: <SkyCoord (Galactic): (l, b) in deg\n        ( 184.9367087, -8.37920222)>\n        radius: 0.400147197682 deg\n    """"""\n\n    def __init__(\n        self,\n        region,\n        center,\n        angle_increment=""0.1 rad"",\n        min_distance=""0 rad"",\n        min_distance_input=""0.1 rad"",\n        max_region_number=10000,\n        exclusion_mask=None,\n        binsz=""0.01 deg"",\n    ):\n        self.region = region\n        self.center = center\n\n        self.angle_increment = Angle(angle_increment)\n        if self.angle_increment <= Angle(0, ""deg""):\n            raise ValueError(""angle_increment is too small"")\n\n        self.min_distance = Angle(min_distance)\n        self.min_distance_input = Angle(min_distance_input)\n        self.exclusion_mask = exclusion_mask\n        self.max_region_number = max_region_number\n        self.reflected_regions = None\n        self.reference_map = None\n        self.binsz = Angle(binsz)\n\n    def run(self):\n        """"""Run all steps.\n        """"""\n        self.reference_map = self.make_reference_map(\n            self.region, self.center, self.binsz\n        )\n        if self.exclusion_mask is not None:\n            coords = self.reference_map.geom.get_coord()\n            vals = self.exclusion_mask.get_by_coord(coords)\n            self.reference_map.data += vals\n        else:\n            self.reference_map.data += 1\n\n        # Check if center is contained in region\n        if self.region.contains(self.center, self.reference_map.geom.wcs):\n            self.reflected_regions = []\n        else:\n            self.setup()\n            self.find_regions()\n\n    @staticmethod\n    def make_reference_map(region, center, binsz=""0.01 deg"", min_width=""0.3 deg""):\n        """"""Create empty reference map.\n\n        The size of the map is chosen such that all reflected regions are\n        contained on the image.\n        To do so, the reference map width is taken to be 4 times the distance between\n        the target region center and the rotation point. This distance is larger than\n        the typical dimension of the region itself (otherwise the rotation point would\n        lie inside the region). A minimal width value is added by default in case the\n        region center and the rotation center are too close.\n\n        The WCS of the map is the TAN projection at the `center` in the coordinate\n        system used by the `region` center.\n\n        Parameters\n        ----------\n        region : `~regions.SkyRegion`\n            Region to rotate\n        center : `~astropy.coordinates.SkyCoord`\n            Rotation point\n        binsz : `~astropy.coordinates.Angle`\n            Reference map bin size.\n        min_width : `~astropy.coordinates.Angle`\n            Minimal map width.\n\n        Returns\n        -------\n        reference_map : `~gammapy.maps.WcsNDMap`\n            Map containing the region\n        """"""\n        frame = region.center.frame.name\n\n        # width is the full width of an image (not the radius)\n        width = 4 * region.center.separation(center) + Angle(min_width)\n\n        return WcsNDMap.create(\n            skydir=center, binsz=binsz, width=width, frame=frame, proj=""TAN""\n        )\n\n    @staticmethod\n    def _region_angular_size(pixels, center):\n        """"""Compute maximum angular size of a group of pixels as seen from center.\n\n        This assumes that the center lies outside the group of pixel\n\n        Parameters\n        ----------\n        pixels : `~astropy.regions.PixCoord`\n            the pixels coordinates\n        center : `~astropy.regions.PixCoord`\n            the center coordinate in pixels\n\n        Returns\n        -------\n        angular_size : `~astropy.coordinates.Angle`\n            the maximum angular size\n        """"""\n        newX, newY = center.x - pixels.x, center.y - pixels.y\n        angles = Angle(np.arctan2(newX, newY), ""rad"")\n        angular_size = np.max(angles) - np.min(angles)\n\n        if angular_size.value > np.pi:\n            angular_size = np.max(angles.wrap_at(0 * u.rad)) - np.min(\n                angles.wrap_at(0 * u.rad)\n            )\n\n        return angular_size\n\n    def setup(self):\n        """"""Compute parameters for reflected regions algorithm.""""""\n        geom = self.reference_map.geom\n        self._pix_region = self.region.to_pixel(geom.wcs)\n        self._pix_center = PixCoord.from_sky(self.center, geom.wcs)\n\n        # Make the ON reference map\n        mask = geom.region_mask([self.region], inside=True)\n        # on_reference_map = WcsNDMap(geom=geom, data=mask)\n\n        # Extract all pixcoords in the geom\n        X, Y = geom.get_pix()\n        ONpixels = PixCoord(X[mask], Y[mask])\n\n        # find excluded PixCoords\n        mask = self.reference_map.data == 0\n        self.excluded_pixcoords = PixCoord(X[mask], Y[mask])\n\n        # Minimum angle a region has to be moved to not overlap with previous one\n        min_ang = self._region_angular_size(ONpixels, self._pix_center)\n\n        # Add required minimal distance between two off regions\n        self._min_ang = min_ang + self.min_distance\n\n        # Maximum possible angle before regions is reached again\n        self._max_angle = Angle(""360deg"") - self._min_ang - self.min_distance_input\n\n    def find_regions(self):\n        """"""Find reflected regions.""""""\n        curr_angle = self._min_ang + self.min_distance_input\n        reflected_regions = []\n\n        while curr_angle < self._max_angle:\n            test_reg = self._pix_region.rotate(self._pix_center, curr_angle)\n            if not np.any(test_reg.contains(self.excluded_pixcoords)):\n                region = test_reg.to_sky(self.reference_map.geom.wcs)\n                reflected_regions.append(region)\n\n                curr_angle += self._min_ang\n                if self.max_region_number <= len(reflected_regions):\n                    break\n            else:\n                curr_angle = curr_angle + self.angle_increment\n\n        self.reflected_regions = reflected_regions\n\n    def plot(self, fig=None, ax=None):\n        """"""Standard debug plot.\n\n        See example here: :ref:\'regions_reflected\'.\n        """"""\n        fig, ax, cbar = self.reference_map.plot(\n            fig=fig, ax=ax, cmap=""gray"", vmin=0, vmax=1\n        )\n        wcs = self.reference_map.geom.wcs\n\n        on_patch = self.region.to_pixel(wcs=wcs).as_artist(edgecolor=""red"", alpha=0.6)\n        ax.add_patch(on_patch)\n\n        for off in self.reflected_regions:\n            tmp = off.to_pixel(wcs=wcs)\n            off_patch = tmp.as_artist(edgecolor=""blue"", alpha=0.6)\n            ax.add_patch(off_patch)\n\n            xx, yy = self.center.to_pixel(wcs)\n            ax.plot(xx, yy, marker=""+"", color=""green"", markersize=20, linewidth=5)\n\n        return fig, ax\n\n\nclass ReflectedRegionsBackgroundMaker:\n    """"""Reflected regions background maker.\n\n    Parameters\n    ----------\n    angle_increment : `~astropy.coordinates.Angle`, optional\n        Rotation angle applied when a region falls in an excluded region.\n    min_distance : `~astropy.coordinates.Angle`, optional\n        Minimal distance between two consecutive reflected regions\n    min_distance_input : `~astropy.coordinates.Angle`, optional\n        Minimal distance from input region\n    max_region_number : int, optional\n        Maximum number of regions to use\n    exclusion_mask : `~gammapy.maps.WcsNDMap`, optional\n        Exclusion mask\n    binsz : `~astropy.coordinates.Angle`\n        Bin size of the reference map used for region finding.\n    """"""\n\n    def __init__(\n        self,\n        angle_increment=""0.1 rad"",\n        min_distance=""0 rad"",\n        min_distance_input=""0.1 rad"",\n        max_region_number=10000,\n        exclusion_mask=None,\n        binsz=""0.01 deg"",\n    ):\n        self.binsz = binsz\n        self.exclusion_mask = exclusion_mask\n        self.angle_increment = Angle(angle_increment)\n        self.min_distance = Angle(min_distance)\n        self.min_distance_input = Angle(min_distance_input)\n        self.max_region_number = max_region_number\n\n    def _get_finder(self, dataset, observation):\n        return ReflectedRegionsFinder(\n            binsz=self.binsz,\n            exclusion_mask=self.exclusion_mask,\n            center=observation.pointing_radec,\n            region=dataset.counts.geom.region,\n            min_distance=self.min_distance,\n            min_distance_input=self.min_distance_input,\n            max_region_number=self.max_region_number,\n            angle_increment=self.angle_increment,\n        )\n\n    def make_counts_off(self, dataset, observation):\n        """"""Make off counts.\n\n        Parameters\n        ----------\n        dataset : `SpectrumDataset`\n            Spectrum dataset.\n        observation : `DatastoreObservation`\n            Data store observation.\n\n\n        Returns\n        -------\n        counts_off : `RegionNDMap`\n            Off counts.\n        """"""\n        finder = self._get_finder(dataset, observation)\n        finder.run()\n\n        energy_axis = dataset.counts.geom.get_axis_by_name(""energy"")\n\n        if len(finder.reflected_regions) > 0:\n            region_union = list_to_compound_region(finder.reflected_regions)\n            wcs = finder.reference_map.geom.wcs\n            geom = RegionGeom.create(region=region_union, axes=[energy_axis], wcs=wcs)\n            counts_off = RegionNDMap.from_geom(geom=geom)\n            counts_off.fill_events(observation.events)\n            acceptance_off = len(finder.reflected_regions)\n        else:\n            # if no OFF regions are found, off is set to None and acceptance_off to zero\n            counts_off = None\n            acceptance_off = 0\n        return counts_off, acceptance_off\n\n    def run(self, dataset, observation):\n        """"""Run reflected regions background maker\n\n        Parameters\n        ----------\n        dataset : `SpectrumDataset`\n            Spectrum dataset.\n        observation : `DatastoreObservation`\n            Data store observation.\n\n        Returns\n        -------\n        dataset_on_off : `SpectrumDatasetOnOff`\n            On off dataset.\n        """"""\n        counts_off, acceptance_off = self.make_counts_off(dataset, observation)\n\n        return SpectrumDatasetOnOff.from_spectrum_dataset(\n            dataset=dataset,\n            acceptance=1,\n            acceptance_off=acceptance_off,\n            counts_off=counts_off,\n        )\n'"
gammapy/makers/background/ring.py,17,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Ring background estimation.""""""\nimport itertools\nimport numpy as np\nfrom astropy.convolution import Ring2DKernel, Tophat2DKernel\nfrom astropy.coordinates import Angle\nfrom gammapy.maps import Map\nfrom gammapy.utils.array import scale_cube\n\n__all__ = [""AdaptiveRingBackgroundMaker"", ""RingBackgroundMaker""]\n\n\nclass AdaptiveRingBackgroundMaker:\n    """"""Adaptive ring background algorithm.\n\n    This algorithm extends the `RingBackgroundMaker` method by adapting the\n    size of the ring to achieve a minimum on / off exposure ratio (alpha) in regions\n    where the area to estimate the background from is limited.\n\n    Parameters\n    ----------\n    r_in : `~astropy.units.Quantity`\n        Inner radius of the ring.\n    r_out_max : `~astropy.units.Quantity`\n        Maximal outer radius of the ring.\n    width : `~astropy.units.Quantity`\n        Width of the ring.\n    stepsize : `~astropy.units.Quantity`\n        Stepsize used for increasing the radius.\n    threshold_alpha : float\n        Threshold on alpha above which the adaptive ring takes action.\n    theta : `~astropy.units.Quantity`\n        Integration radius used for alpha computation.\n    method : {\'fixed_width\', \'fixed_r_in\'}\n        Adaptive ring method.\n    exclusion_mask : `~gammapy.maps.WcsNDMap`\n        Exclusion mask\n\n    See Also\n    --------\n    RingBackgroundMaker\n    """"""\n\n    def __init__(\n        self,\n        r_in,\n        r_out_max,\n        width,\n        stepsize=""0.02 deg"",\n        threshold_alpha=0.1,\n        theta=""0.22 deg"",\n        method=""fixed_width"",\n        exclusion_mask=None,\n    ):\n        if method not in [""fixed_width"", ""fixed_r_in""]:\n            raise ValueError(""Not a valid adaptive ring method."")\n\n        self.r_in = Angle(r_in)\n        self.r_out_max = Angle(r_out_max)\n        self.width = Angle(width)\n        self.stepsize = Angle(stepsize)\n        self.threshold_alpha = threshold_alpha\n        self.theta = Angle(theta)\n        self.method = method\n        self.exclusion_mask = exclusion_mask\n\n    def kernels(self, image):\n        """"""Ring kernels according to the specified method.\n\n        Parameters\n        ----------\n        image : `~gammapy.maps.WcsNDMap`\n            Map specifying the WCS information.\n\n        Returns\n        -------\n        kernels : list\n            List of `~astropy.convolution.Ring2DKernel`\n        """"""\n        scale = image.geom.pixel_scales[0]\n        r_in = (self.r_in / scale).to_value("""")\n        r_out_max = (self.r_out_max / scale).to_value("""")\n        width = (self.width / scale).to_value("""")\n        stepsize = (self.stepsize / scale).to_value("""")\n\n        if self.method == ""fixed_width"":\n            r_ins = np.arange(r_in, (r_out_max - width), stepsize)\n            widths = [width]\n        elif self.method == ""fixed_r_in"":\n            widths = np.arange(width, (r_out_max - r_in), stepsize)\n            r_ins = [r_in]\n        else:\n            raise ValueError(f""Invalid method: {self.method!r}"")\n\n        kernels = []\n        for r_in, width in itertools.product(r_ins, widths):\n            kernel = Ring2DKernel(r_in, width)\n            kernel.normalize(""peak"")\n            kernels.append(kernel)\n\n        return kernels\n\n    @staticmethod\n    def _alpha_approx_cube(cubes):\n        acceptance = cubes[""acceptance""]\n        acceptance_off = cubes[""acceptance_off""]\n        with np.errstate(divide=""ignore"", invalid=""ignore""):\n            alpha_approx = np.where(\n                acceptance_off > 0, acceptance / acceptance_off, np.inf\n            )\n\n        return alpha_approx\n\n    def _reduce_cubes(self, cubes, dataset):\n        """"""Compute off and off acceptance map.\n\n        Calulated by reducing the cubes. The data is\n        iterated along the third axis (i.e. increasing ring sizes), the value\n        with the first approximate alpha < threshold is taken.\n        """"""\n        threshold = self.threshold_alpha\n\n        alpha_approx_cube = self._alpha_approx_cube(cubes)\n        counts_off_cube = cubes[""counts_off""]\n        acceptance_off_cube = cubes[""acceptance_off""]\n        acceptance_cube = cubes[""acceptance""]\n\n        shape = alpha_approx_cube.shape[:2]\n        counts_off = np.tile(np.nan, shape)\n        acceptance_off = np.tile(np.nan, shape)\n        acceptance = np.tile(np.nan, shape)\n\n        for idx in np.arange(alpha_approx_cube.shape[-1]):\n            mask = (alpha_approx_cube[:, :, idx] <= threshold) & np.isnan(counts_off)\n            counts_off[mask] = counts_off_cube[:, :, idx][mask]\n            acceptance_off[mask] = acceptance_off_cube[:, :, idx][mask]\n            acceptance[mask] = acceptance_cube[:, :, idx][mask]\n\n        counts = dataset.counts\n        acceptance = counts.copy(data=acceptance[np.newaxis, Ellipsis])\n        acceptance_off = counts.copy(data=acceptance_off[np.newaxis, Ellipsis])\n        counts_off = counts.copy(data=counts_off[np.newaxis, Ellipsis])\n\n        return acceptance, acceptance_off, counts_off\n\n    def make_cubes(self, dataset):\n        """"""Make acceptance, off acceptance, off counts cubes\n\n        Parameters\n        ----------\n        dataset : `~gammapy.cube.MapDataset`\n            Input map dataset.\n\n        Returns\n        -------\n        cubes : dict of `~gammapy.maps.WcsNDMap`\n            Dictionary containing ``counts_off``, ``acceptance`` and ``acceptance_off`` cubes.\n        """"""\n        counts = dataset.counts\n        background = dataset.background_model.map\n        kernels = self.kernels(counts)\n\n        if self.exclusion_mask is not None:\n            # reproject exclusion mask\n            coords = counts.geom.get_coord()\n            data = self.exclusion_mask.get_by_coord(coords)\n            exclusion = Map.from_geom(geom=counts.geom, data=data)\n        else:\n            data = np.ones(counts.geom.data_shape, dtype=bool)\n            exclusion = Map.from_geom(geom=counts.geom, data=data)\n\n        cubes = {}\n        cubes[""counts_off""] = scale_cube(\n            (counts.data * exclusion.data)[0, Ellipsis], kernels\n        )\n        cubes[""acceptance_off""] = scale_cube(\n            (background.data * exclusion.data)[0, Ellipsis], kernels\n        )\n\n        scale = background.geom.pixel_scales[0].to(""deg"")\n        theta = self.theta * scale\n        tophat = Tophat2DKernel(theta.value)\n        tophat.normalize(""peak"")\n        acceptance = background.convolve(tophat.array)\n        acceptance_data = acceptance.data[0, Ellipsis]\n        cubes[""acceptance""] = np.repeat(\n            acceptance_data[Ellipsis, np.newaxis], len(kernels), axis=2\n        )\n\n        return cubes\n\n    def run(self, dataset):\n        """"""Run adaptive ring background maker\n\n        Parameters\n        ----------\n        dataset : `~gammapy.cube.fit.MapDataset`\n            Input map dataset.\n\n        Returns\n        -------\n        dataset_on_off : `~gammapy.cube.fit.MapDatasetOnOff`\n            On off dataset.\n        """"""\n        from gammapy.datasets import MapDatasetOnOff\n\n        cubes = self.make_cubes(dataset)\n        acceptance, acceptance_off, counts_off = self._reduce_cubes(cubes, dataset)\n\n        mask_safe = dataset.mask_safe.copy()\n        not_has_off_acceptance = acceptance_off.data <= 0\n        mask_safe.data[not_has_off_acceptance] = 0\n\n        dataset_on_off = MapDatasetOnOff.from_map_dataset(\n            dataset=dataset,\n            counts_off=counts_off,\n            acceptance=acceptance,\n            acceptance_off=acceptance_off,\n            name=dataset.name,\n        )\n\n        dataset_on_off.mask_safe = mask_safe\n        return dataset_on_off\n\n\nclass RingBackgroundMaker:\n    """"""Ring background method for cartesian coordinates.\n\n    - Step 1: apply exclusion mask\n    - Step 2: ring-correlate\n\n    Parameters\n    ----------\n    r_in : `~astropy.units.Quantity`\n        Inner ring radius\n    width : `~astropy.units.Quantity`\n        Ring width\n    exclusion_mask : `~gammapy.maps.WcsNDMap`\n        Exclusion mask\n\n    See Also\n    --------\n    AdaptiveRingBackgroundEstimator\n    """"""\n\n    def __init__(self, r_in, width, exclusion_mask=None):\n        self.r_in = Angle(r_in)\n        self.width = Angle(width)\n        self.exclusion_mask = exclusion_mask\n\n    def kernel(self, image):\n        """"""Ring kernel.\n\n        Parameters\n        ----------\n        image : `~gammapy.maps.WcsNDMap`\n            Input Map\n\n        Returns\n        -------\n        ring : `~astropy.convolution.Ring2DKernel`\n            Ring kernel.\n        """"""\n        scale = image.geom.pixel_scales[0].to(""deg"")\n        r_in = self.r_in.to(""deg"") / scale\n        width = self.width.to(""deg"") / scale\n\n        ring = Ring2DKernel(r_in.value, width.value)\n        ring.normalize(""peak"")\n        return ring\n\n    def make_maps_off(self, dataset):\n        """"""Make off maps\n\n        Parameters\n        ----------\n        dataset : `~gammapy.cube.fit.MapDataset`\n            Input map dataset.\n\n        Returns\n        -------\n        maps_off : dict of `~gammapy.maps.WcsNDMap`\n            Dictionary containing `counts_off` and `acceptance_off` maps.\n        """"""\n        counts = dataset.counts\n        background = dataset.background_model.map\n\n        if self.exclusion_mask is not None:\n            # reproject exclusion mask\n            coords = counts.geom.get_coord()\n            data = self.exclusion_mask.get_by_coord(coords)\n            exclusion = Map.from_geom(geom=counts.geom, data=data)\n        else:\n            data = np.ones(counts.geom.data_shape, dtype=bool)\n            exclusion = Map.from_geom(geom=counts.geom, data=data)\n\n        maps_off = {}\n        ring = self.kernel(counts)\n\n        counts_excluded = counts * exclusion\n        maps_off[""counts_off""] = counts_excluded.convolve(ring.array)\n\n        background_excluded = background * exclusion\n        maps_off[""acceptance_off""] = background_excluded.convolve(ring.array)\n\n        return maps_off\n\n    def run(self, dataset):\n        """"""Run ring background maker\n\n        Parameters\n        ----------\n        dataset : `~gammapy.cube.fit.MapDataset`\n            Input map dataset.\n\n        Returns\n        -------\n        dataset_on_off : `~gammapy.cube.fit.MapDatasetOnOff`\n            On off dataset.\n        """"""\n        from gammapy.datasets import MapDatasetOnOff\n\n        maps_off = self.make_maps_off(dataset)\n        maps_off[""acceptance""] = dataset.background_model.map\n\n        mask_safe = dataset.mask_safe.copy()\n        not_has_off_acceptance = maps_off[""acceptance_off""].data <= 0\n        mask_safe.data[not_has_off_acceptance] = 0\n\n        dataset_on_off = MapDatasetOnOff.from_map_dataset(\n            dataset=dataset, name=dataset.name, **maps_off\n        )\n\n        dataset_on_off.mask_safe = mask_safe\n        return dataset_on_off\n\n    def __str__(self):\n        return (\n            ""RingBackground parameters: \\n""\n            f""r_in : {self.parameters[\'r_in\']}\\n""\n            f""width: {self.parameters[\'width\']}\\n""\n            f""Exclusion mask: {self.exclusion_mask}""\n        )\n'"
gammapy/makers/tests/__init__.py,0,b''
gammapy/makers/tests/test_map.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nimport numpy as np\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nfrom gammapy.data import DataStore\nfrom gammapy.datasets import MapDataset\nfrom gammapy.irf import EDispMap, EDispKernelMap\nfrom gammapy.makers import MapDatasetMaker, SafeMaskMaker\nfrom gammapy.maps import Map, MapAxis, WcsGeom\nfrom gammapy.utils.testing import requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef observations():\n    data_store = DataStore.from_dir(""$GAMMAPY_DATA/cta-1dc/index/gps/"")\n    obs_id = [110380, 111140]\n    return data_store.get_observations(obs_id)\n\n\ndef geom(ebounds, binsz=0.5):\n    skydir = SkyCoord(0, -1, unit=""deg"", frame=""galactic"")\n    energy_axis = MapAxis.from_edges(ebounds, name=""energy"", unit=""TeV"", interp=""log"")\n    return WcsGeom.create(\n        skydir=skydir, binsz=binsz, width=(10, 5), frame=""galactic"", axes=[energy_axis]\n    )\n\n\n@requires_data()\n@pytest.mark.parametrize(\n    ""pars"",\n    [\n        {\n            # Default, same e_true and reco\n            ""geom"": geom(ebounds=[0.1, 1, 10]),\n            ""e_true"": None,\n            ""counts"": 34366,\n            ""exposure"": 9.995376e08,\n            ""exposure_image"": 7.921993e10,\n            ""background"": 27989.05,\n            ""binsz_irf"": 0.5,\n            ""migra"": None,\n        },\n        {\n            # Test single energy bin\n            ""geom"": geom(ebounds=[0.1, 10]),\n            ""e_true"": None,\n            ""counts"": 34366,\n            ""exposure"": 5.843302e08,\n            ""exposure_image"": 1.16866e11,\n            ""background"": 30424.451,\n            ""binsz_irf"": 0.5,\n            ""migra"": None,\n        },\n        {\n            # Test single energy bin with exclusion mask\n            ""geom"": geom(ebounds=[0.1, 10]),\n            ""e_true"": None,\n            ""exclusion_mask"": Map.from_geom(geom(ebounds=[0.1, 10])),\n            ""counts"": 34366,\n            ""exposure"": 5.843302e08,\n            ""exposure_image"": 1.16866e11,\n            ""background"": 30424.451,\n            ""binsz_irf"": 0.5,\n            ""migra"": None,\n        },\n        {\n            # Test for different e_true and e_reco bins\n            ""geom"": geom(ebounds=[0.1, 1, 10]),\n            ""e_true"": MapAxis.from_edges(\n                [0.1, 0.5, 2.5, 10.0], name=""energy_true"", unit=""TeV"", interp=""log""\n            ),\n            ""counts"": 34366,\n            ""exposure"": 9.951827e08,\n            ""exposure_image"": 6.492968e10,\n            ""background"": 28760.283,\n            ""background_oversampling"": 2,\n            ""binsz_irf"": 0.5,\n            ""migra"": None,\n        },\n        {\n            # Test for different e_true and e_reco and spatial bins\n            ""geom"": geom(ebounds=[0.1, 1, 10]),\n            ""e_true"": MapAxis.from_edges(\n                [0.1, 0.5, 2.5, 10.0], name=""energy_true"", unit=""TeV"", interp=""log""\n            ),\n            ""counts"": 34366,\n            ""exposure"": 9.951827e08,\n            ""exposure_image"": 6.492968e10,\n            ""background"": 28760.283,\n            ""background_oversampling"": 2,\n            ""binsz_irf"": 1.0,\n            ""migra"": None,\n        },\n        {\n            # Test for different e_true and e_reco and use edispmap\n            ""geom"": geom(ebounds=[0.1, 1, 10]),\n            ""e_true"": MapAxis.from_edges(\n                [0.1, 0.5, 2.5, 10.0], name=""energy_true"", unit=""TeV"", interp=""log""\n            ),\n            ""counts"": 34366,\n            ""exposure"": 9.951827e08,\n            ""exposure_image"": 6.492968e10,\n            ""background"": 28760.283,\n            ""background_oversampling"": 2,\n            ""binsz_irf"": 0.5,\n            ""migra"": MapAxis.from_edges(np.linspace(0.,3.,100), name=""migra"", unit=""""),\n        },\n    ],\n)\ndef test_map_maker(pars, observations):\n    stacked = MapDataset.create(\n        geom=pars[""geom""], energy_axis_true=pars[""e_true""], binsz_irf=pars[""binsz_irf""], migra_axis=pars[""migra""]\n    )\n\n    maker = MapDatasetMaker(background_oversampling=pars.get(""background_oversampling""))\n    safe_mask_maker = SafeMaskMaker(methods=[""offset-max""], offset_max=""2 deg"")\n\n    for obs in observations:\n        cutout = stacked.cutout(position=obs.pointing_radec, width=""4 deg"")\n        dataset = maker.run(cutout, obs)\n        dataset = safe_mask_maker.run(dataset, obs)\n        stacked.stack(dataset)\n\n    counts = stacked.counts\n    assert counts.unit == """"\n    assert_allclose(counts.data.sum(), pars[""counts""], rtol=1e-5)\n\n    exposure = stacked.exposure\n    assert exposure.unit == ""m2 s""\n    assert_allclose(exposure.data.mean(), pars[""exposure""], rtol=3e-3)\n\n    background = stacked.background_model.map\n    assert background.unit == """"\n    assert_allclose(background.data.sum(), pars[""background""], rtol=1e-4)\n\n    image_dataset = stacked.to_image()\n\n    counts = image_dataset.counts\n    assert counts.unit == """"\n    assert_allclose(counts.data.sum(), pars[""counts""], rtol=1e-4)\n\n    exposure = image_dataset.exposure\n    assert exposure.unit == ""m2 s""\n    assert_allclose(exposure.data.sum(), pars[""exposure_image""], rtol=1e-3)\n\n    background = image_dataset.background_model.map\n    assert background.unit == """"\n    assert_allclose(background.data.sum(), pars[""background""], rtol=1e-4)\n\n\n@requires_data()\ndef test_map_maker_obs(observations):\n    # Test for different spatial geoms and etrue, ereco bins\n\n    geom_reco = geom(ebounds=[0.1, 1, 10])\n    e_true = MapAxis.from_edges(\n        [0.1, 0.5, 2.5, 10.0], name=""energy_true"", unit=""TeV"", interp=""log""\n    )\n\n    reference = MapDataset.create(\n        geom=geom_reco, energy_axis_true=e_true, binsz_irf=1.0\n    )\n\n    maker_obs = MapDatasetMaker()\n\n    map_dataset = maker_obs.run(reference, observations[0])\n    assert map_dataset.counts.geom == geom_reco\n    assert map_dataset.background_model.map.geom == geom_reco\n    assert isinstance(map_dataset.edisp, EDispKernelMap)\n    assert map_dataset.edisp.edisp_map.data.shape == (3, 2, 5, 10)\n    assert map_dataset.edisp.exposure_map.data.shape == (3, 1, 5, 10)\n    assert map_dataset.psf.psf_map.data.shape == (3, 66, 5, 10)\n    assert map_dataset.psf.exposure_map.data.shape == (3, 1, 5, 10)\n    assert_allclose(map_dataset.gti.time_delta, 1800.0 * u.s)\n\n@requires_data()\ndef test_map_maker_obs_with_migra(observations):\n    # Test for different spatial geoms and etrue, ereco bins\n    migra = MapAxis.from_edges(np.linspace(0,2.,50), unit=\'\', name=\'migra\')\n    geom_reco = geom(ebounds=[0.1, 1, 10])\n    e_true = MapAxis.from_edges(\n        [0.1, 0.5, 2.5, 10.0], name=""energy_true"", unit=""TeV"", interp=""log""\n    )\n\n    reference = MapDataset.create(\n        geom=geom_reco, energy_axis_true=e_true, migra_axis=migra, binsz_irf=1.0\n    )\n\n    maker_obs = MapDatasetMaker()\n\n    map_dataset = maker_obs.run(reference, observations[0])\n    assert map_dataset.counts.geom == geom_reco\n    assert isinstance(map_dataset.edisp, EDispMap)\n    assert map_dataset.edisp.edisp_map.data.shape == (3, 49, 5, 10)\n    assert map_dataset.edisp.exposure_map.data.shape == (3, 1, 5, 10)\n'"
gammapy/makers/tests/test_safe.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nfrom gammapy.data import DataStore\nfrom gammapy.datasets import MapDataset\nfrom gammapy.makers import MapDatasetMaker, SafeMaskMaker\nfrom gammapy.maps import MapAxis, WcsGeom\nfrom gammapy.utils.testing import requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef observations():\n    data_store = DataStore.from_dir(""$GAMMAPY_DATA/cta-1dc/index/gps/"")\n    obs_id = [110380, 111140]\n    return data_store.get_observations(obs_id)\n\n\n@requires_data()\ndef test_safe_mask_maker(observations):\n    obs = observations[0]\n\n    axis = MapAxis.from_bounds(\n        0.1, 10, nbin=16, unit=""TeV"", name=""energy"", interp=""log""\n    )\n    axis_true = MapAxis.from_bounds(\n        0.1, 50, nbin=30, unit=""TeV"", name=""energy_true"", interp=""log""\n    )\n    geom = WcsGeom.create(npix=(11, 11), axes=[axis], skydir=obs.pointing_radec)\n\n    empty_dataset = MapDataset.create(geom=geom, energy_axis_true=axis_true)\n    dataset_maker = MapDatasetMaker()\n    safe_mask_maker = SafeMaskMaker(\n        offset_max=""3 deg"", bias_percent=0.02, position=obs.pointing_radec\n    )\n\n    dataset = dataset_maker.run(empty_dataset, obs)\n\n    mask_offset = safe_mask_maker.make_mask_offset_max(dataset=dataset, observation=obs)\n    assert_allclose(mask_offset.sum(), 109)\n\n    mask_energy_aeff_default = safe_mask_maker.make_mask_energy_aeff_default(\n        dataset=dataset, observation=obs\n    )\n    assert_allclose(mask_energy_aeff_default.sum(), 1936)\n\n    mask_aeff_max = safe_mask_maker.make_mask_energy_aeff_max(dataset)\n    assert_allclose(mask_aeff_max.sum(), 1210)\n\n    mask_edisp_bias = safe_mask_maker.make_mask_energy_edisp_bias(dataset)\n    assert_allclose(mask_edisp_bias.sum(), 1815)\n\n    mask_bkg_peak = safe_mask_maker.make_mask_energy_bkg_peak(dataset)\n    assert_allclose(mask_bkg_peak.sum(), 1815)\n'"
gammapy/makers/tests/test_spectrum.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.coordinates import Angle, SkyCoord\nfrom regions import CircleSkyRegion\nfrom gammapy.data import DataStore\nfrom gammapy.datasets import SpectrumDataset\nfrom gammapy.makers import (\n    ReflectedRegionsBackgroundMaker,\n    SafeMaskMaker,\n    SpectrumDatasetMaker,\n)\nfrom gammapy.maps import WcsGeom, WcsNDMap, MapAxis\nfrom gammapy.utils.testing import assert_quantity_allclose, requires_data\n\n\n@pytest.fixture\ndef observations_hess_dl3():\n    """"""HESS DL3 observation list.""""""\n    datastore = DataStore.from_dir(""$GAMMAPY_DATA/hess-dl3-dr1/"")\n    obs_ids = [23523, 23526]\n    return datastore.get_observations(obs_ids)\n\n\n@pytest.fixture\ndef observations_cta_dc1():\n    """"""CTA DC1 observation list.""""""\n    datastore = DataStore.from_dir(""$GAMMAPY_DATA/cta-1dc/index/gps/"")\n    obs_ids = [110380, 111140]\n    return datastore.get_observations(obs_ids)\n\n\n@pytest.fixture()\ndef spectrum_dataset_gc():\n    e_reco = MapAxis.from_edges(np.logspace(0, 2, 5) * u.TeV, name=""energy"")\n    e_true = MapAxis.from_edges(np.logspace(-0.5, 2, 11) * u.TeV, name=""energy_true"")\n    pos = SkyCoord(0.0, 0.0, unit=""deg"", frame=""galactic"")\n    radius = Angle(0.11, ""deg"")\n    region = CircleSkyRegion(pos, radius)\n    return SpectrumDataset.create(e_reco, e_true, region=region)\n\n\n@pytest.fixture()\ndef spectrum_dataset_crab():\n    e_reco = MapAxis.from_edges(np.logspace(0, 2, 5) * u.TeV, name=""energy"")\n    e_true = MapAxis.from_edges(np.logspace(-0.5, 2, 11) * u.TeV, name=""energy_true"")\n    pos = SkyCoord(83.63, 22.01, unit=""deg"", frame=""icrs"")\n    radius = Angle(0.11, ""deg"")\n    region = CircleSkyRegion(pos, radius)\n    return SpectrumDataset.create(e_reco, e_true, region=region)\n\n\n@pytest.fixture()\ndef spectrum_dataset_crab_fine():\n    e_true = MapAxis.from_edges(np.logspace(-2, 2.5, 109) * u.TeV, name=""energy_true"")\n    e_reco = MapAxis.from_edges(np.logspace(-2, 2, 73) * u.TeV, name=""energy"")\n    pos = SkyCoord(83.63, 22.01, unit=""deg"", frame=""icrs"")\n    radius = Angle(0.11, ""deg"")\n    region = CircleSkyRegion(pos, radius)\n    return SpectrumDataset.create(e_reco, e_true, region=region)\n\n\n@pytest.fixture\ndef reflected_regions_bkg_maker():\n    pos = SkyCoord(83.63, 22.01, unit=""deg"", frame=""icrs"")\n    exclusion_region = CircleSkyRegion(pos, Angle(0.3, ""deg""))\n    geom = WcsGeom.create(skydir=pos, binsz=0.02, width=10.0)\n    mask = geom.region_mask([exclusion_region], inside=False)\n    exclusion_mask = WcsNDMap(geom, data=mask)\n\n    return ReflectedRegionsBackgroundMaker(\n        exclusion_mask=exclusion_mask, min_distance_input=""0.2 deg""\n    )\n\n\n@requires_data()\ndef test_spectrum_dataset_maker_hess_dl3(spectrum_dataset_crab, observations_hess_dl3):\n    datasets = []\n    maker = SpectrumDatasetMaker()\n\n    for obs in observations_hess_dl3:\n        dataset = maker.run(spectrum_dataset_crab, obs)\n        datasets.append(dataset)\n\n    assert_allclose(datasets[0].counts.data.sum(), 100)\n    assert_allclose(datasets[1].counts.data.sum(), 92)\n\n    assert_allclose(datasets[0].livetime.value, 1581.736758)\n    assert_allclose(datasets[1].livetime.value, 1572.686724)\n\n    assert_allclose(datasets[0].background.data.sum(), 7.74732, rtol=1e-5)\n    assert_allclose(datasets[1].background.data.sum(), 6.118879, rtol=1e-5)\n\n\n@requires_data()\ndef test_spectrum_dataset_maker_hess_cta(spectrum_dataset_gc, observations_cta_dc1):\n    maker = SpectrumDatasetMaker()\n\n    datasets = []\n\n    for obs in observations_cta_dc1:\n        dataset = maker.run(spectrum_dataset_gc, obs)\n        datasets.append(dataset)\n\n    assert_allclose(datasets[0].counts.data.sum(), 53)\n    assert_allclose(datasets[1].counts.data.sum(), 47)\n\n    assert_allclose(datasets[0].livetime.value, 1764.000034)\n    assert_allclose(datasets[1].livetime.value, 1764.000034)\n\n    assert_allclose(datasets[0].background.data.sum(), 2.238345, rtol=1e-5)\n    assert_allclose(datasets[1].background.data.sum(), 2.164593, rtol=1e-5)\n\n\n@requires_data()\ndef test_safe_mask_maker_dl3(spectrum_dataset_crab, observations_hess_dl3):\n\n    safe_mask_maker = SafeMaskMaker()\n    maker = SpectrumDatasetMaker()\n\n    obs = observations_hess_dl3[0]\n    dataset = maker.run(spectrum_dataset_crab, obs)\n    dataset = safe_mask_maker.run(dataset, obs)\n    assert_allclose(dataset.energy_range[0].value, 1)\n    assert dataset.energy_range[0].unit == ""TeV""\n\n    mask_safe = safe_mask_maker.make_mask_energy_aeff_max(dataset)\n    assert mask_safe.sum() == 4\n\n    mask_safe = safe_mask_maker.make_mask_energy_edisp_bias(dataset)\n    assert mask_safe.sum() == 3\n\n    mask_safe = safe_mask_maker.make_mask_energy_bkg_peak(dataset)\n    assert mask_safe.sum() == 3\n\n\n@requires_data()\ndef test_safe_mask_maker_dc1(spectrum_dataset_gc, observations_cta_dc1):\n    safe_mask_maker = SafeMaskMaker(methods=[""edisp-bias"", ""aeff-max""])\n\n    obs = observations_cta_dc1[0]\n    maker = SpectrumDatasetMaker()\n    dataset = maker.run(spectrum_dataset_gc, obs)\n    dataset = safe_mask_maker.run(dataset, obs)\n    assert_allclose(dataset.energy_range[0].value, 3.162278, rtol=1e-3)\n    assert dataset.energy_range[0].unit == ""TeV""\n\n\n@requires_data()\nclass TestSpectrumMakerChain:\n    @staticmethod\n    @pytest.mark.parametrize(\n        ""pars, results"",\n        [\n            (\n                dict(containment_correction=False),\n                dict(\n                    n_on=125, sigma=18.953014, aeff=580254.9 * u.m ** 2, edisp=0.235864\n                ),\n            ),\n            (\n                dict(containment_correction=True),\n                dict(\n                    n_on=125,\n                    sigma=18.953014,\n                    aeff=361924.746081 * u.m ** 2,\n                    edisp=0.235864,\n                ),\n            ),\n        ],\n    )\n    def test_extract(\n        pars,\n        results,\n        observations_hess_dl3,\n        spectrum_dataset_crab_fine,\n        reflected_regions_bkg_maker,\n    ):\n        """"""Test quantitative output for various configs""""""\n        safe_mask_maker = SafeMaskMaker()\n        maker = SpectrumDatasetMaker(\n            containment_correction=pars[""containment_correction""]\n        )\n\n        obs = observations_hess_dl3[0]\n        dataset = maker.run(spectrum_dataset_crab_fine, obs)\n        dataset = reflected_regions_bkg_maker.run(dataset, obs)\n        dataset = safe_mask_maker.run(dataset, obs)\n\n        aeff_actual = dataset.aeff.data.evaluate(energy_true=5 * u.TeV)\n        edisp_actual = dataset.edisp.data.evaluate(\n            energy_true=5 * u.TeV, energy=5.2 * u.TeV\n        )\n\n        assert_quantity_allclose(aeff_actual, results[""aeff""], rtol=1e-3)\n        assert_quantity_allclose(edisp_actual, results[""edisp""], rtol=1e-3)\n\n        # TODO: Introduce assert_stats_allclose\n        info = dataset.info_dict()\n\n        assert info[""n_on""] == results[""n_on""]\n        assert_allclose(info[""significance""], results[""sigma""], atol=1e-2)\n\n        gti_obs = obs.gti.table\n        gti_dataset = dataset.gti.table\n        assert_allclose(gti_dataset[""START""], gti_obs[""START""])\n        assert_allclose(gti_dataset[""STOP""], gti_obs[""STOP""])\n\n    def test_compute_energy_threshold(\n        self, spectrum_dataset_crab_fine, observations_hess_dl3\n    ):\n\n        maker = SpectrumDatasetMaker(containment_correction=True)\n        safe_mask_maker = SafeMaskMaker(methods=[""aeff-max""], aeff_percent=10)\n\n        obs = observations_hess_dl3[0]\n        dataset = maker.run(spectrum_dataset_crab_fine, obs)\n        dataset = safe_mask_maker.run(dataset, obs)\n\n        actual = dataset.energy_range[0]\n        assert_quantity_allclose(actual, 0.8799225 * u.TeV, rtol=1e-3)\n'"
gammapy/makers/tests/test_utils.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom astropy import units as u\nfrom astropy.coordinates import EarthLocation, SkyCoord\nfrom astropy.time import Time\nfrom gammapy.data import FixedPointingInfo\nfrom gammapy.irf import Background3D, EffectiveAreaTable2D, EnergyDispersion2D\nfrom gammapy.makers.utils import (\n    _map_spectrum_weight,\n    make_map_background_irf,\n    make_map_exposure_true_energy,\n    make_edisp_kernel_map,\n)\nfrom gammapy.maps import HpxGeom, MapAxis, WcsGeom, WcsNDMap\nfrom gammapy.modeling.models import ConstantSpectralModel\nfrom gammapy.utils.testing import requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef aeff():\n    filename = (\n        ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits""\n    )\n    return EffectiveAreaTable2D.read(filename, hdu=""EFFECTIVE AREA"")\n\n\ndef geom(map_type, ebounds):\n    axis = MapAxis.from_edges(ebounds, name=""energy_true"", unit=""TeV"", interp=""log"")\n    if map_type == ""wcs"":\n        return WcsGeom.create(npix=(4, 3), binsz=2, axes=[axis])\n    elif map_type == ""hpx"":\n        return HpxGeom(256, axes=[axis])\n    else:\n        raise ValueError()\n\n\n@requires_data()\n@pytest.mark.parametrize(\n    ""pars"",\n    [\n        {\n            ""geom"": geom(map_type=""wcs"", ebounds=[0.1, 1, 10]),\n            ""shape"": (2, 3, 4),\n            ""sum"": 8.103974e08,\n        },\n        {\n            ""geom"": geom(map_type=""wcs"", ebounds=[0.1, 10]),\n            ""shape"": (1, 3, 4),\n            ""sum"": 2.387916e08,\n        },\n        # TODO: make this work for HPX\n        # \'HpxGeom\' object has no attribute \'separation\'\n        # {\n        #     \'geom\': geom(map_type=\'hpx\', ebounds=[0.1, 1, 10]),\n        #     \'shape\': \'???\',\n        #     \'sum\': \'???\',\n        # },\n    ],\n)\ndef test_make_map_exposure_true_energy(aeff, pars):\n    m = make_map_exposure_true_energy(\n        pointing=SkyCoord(2, 1, unit=""deg""),\n        livetime=""42 s"",\n        aeff=aeff,\n        geom=pars[""geom""],\n    )\n\n    assert m.data.shape == pars[""shape""]\n    assert m.unit == ""m2 s""\n    assert_allclose(m.data.sum(), pars[""sum""], rtol=1e-5)\n\n\ndef test_map_spectrum_weight():\n    axis = MapAxis.from_edges([0.1, 10, 1000], unit=""TeV"", name=""energy_true"")\n    expo_map = WcsNDMap.create(npix=10, binsz=1, axes=[axis], unit=""m2 s"")\n    expo_map.data += 1\n    spectrum = ConstantSpectralModel(const=""42 cm-2 s-1 TeV-1"")\n\n    weighted_expo = _map_spectrum_weight(expo_map, spectrum)\n\n    assert weighted_expo.data.shape == (2, 10, 10)\n    assert weighted_expo.unit == ""m2 s""\n    assert_allclose(weighted_expo.data.sum(), 100)\n\n\n@pytest.fixture(scope=""session"")\ndef fixed_pointing_info():\n    filename = ""$GAMMAPY_DATA/cta-1dc/data/baseline/gps/gps_baseline_110380.fits""\n    return FixedPointingInfo.read(filename)\n\n\n@pytest.fixture(scope=""session"")\ndef fixed_pointing_info_aligned(fixed_pointing_info):\n    # Create Fixed Pointing Info aligined between sky and horizon coordinates\n    # (removes rotation in FoV and results in predictable solid angles)\n    origin = SkyCoord(\n        0,\n        0,\n        unit=""deg"",\n        frame=""icrs"",\n        location=EarthLocation(lat=90 * u.deg, lon=0 * u.deg),\n        obstime=Time(""2000-9-21 12:00:00""),\n    )\n    fpi = fixed_pointing_info\n    meta = fpi.meta.copy()\n    meta[""RA_PNT""] = origin.icrs.ra\n    meta[""DEC_PNT""] = origin.icrs.dec\n    meta[""GEOLON""] = origin.location.lon\n    meta[""GEOLAT""] = origin.location.lat\n    meta[""ALTITUDE""] = origin.location.height\n    time_start = origin.obstime.datetime - fpi.time_ref.datetime\n    meta[""TSTART""] = time_start.total_seconds()\n    meta[""TSTOP""] = meta[""TSTART""] + 60\n    return FixedPointingInfo(meta)\n\n\n@pytest.fixture(scope=""session"")\ndef bkg_3d():\n    filename = (\n        ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits""\n    )\n    return Background3D.read(filename, hdu=""BACKGROUND"")\n\n\ndef bkg_3d_custom(symmetry=""constant""):\n    if symmetry == ""constant"":\n        data = np.ones((2, 3, 3)) * u.Unit(""s-1 MeV-1 sr-1"")\n    elif symmetry == ""symmetric"":\n        data = np.ones((2, 3, 3)) * u.Unit(""s-1 MeV-1 sr-1"")\n        data[:, 1, 1] *= 2\n    elif symmetry == ""asymmetric"":\n        data = np.indices((3, 3))[1] + 1\n        data = np.stack(2 * [data]) * u.Unit(""s-1 MeV-1 sr-1"")\n    else:\n        raise ValueError(f""Unkown value for symmetry: {symmetry}"")\n\n    energy = [0.1, 10, 1000] * u.TeV\n    fov_lon = [-3, -1, 1, 3] * u.deg\n    fov_lat = [-3, -1, 1, 3] * u.deg\n    return Background3D(\n        energy_lo=energy[:-1],\n        energy_hi=energy[1:],\n        fov_lon_lo=fov_lon[:-1],\n        fov_lon_hi=fov_lon[1:],\n        fov_lat_lo=fov_lat[:-1],\n        fov_lat_hi=fov_lat[1:],\n        data=data,\n    )\n\n\ndef make_map_background_irf_with_symmetry(fpi, symmetry=""constant""):\n    axis = MapAxis.from_edges([0.1, 1, 10], name=""energy"", unit=""TeV"", interp=""log"")\n    return make_map_background_irf(\n        pointing=fpi,\n        ontime=""42 s"",\n        bkg=bkg_3d_custom(symmetry),\n        geom=WcsGeom.create(npix=(3, 3), binsz=4, axes=[axis], skydir=fpi.radec),\n    )\n\n\ndef geom(map_type, ebounds, skydir):\n    axis = MapAxis.from_edges(ebounds, name=""energy"", unit=""TeV"", interp=""log"")\n    if map_type == ""wcs"":\n        return WcsGeom.create(npix=(4, 3), binsz=2, axes=[axis], skydir=skydir)\n    elif map_type == ""hpx"":\n        return HpxGeom(256, axes=[axis])\n    else:\n        raise ValueError()\n\n\n@requires_data()\n@pytest.mark.parametrize(\n    ""pars"",\n    [\n        {\n            ""map_type"": ""wcs"",\n            ""ebounds"": [0.1, 1, 10],\n            ""shape"": (2, 3, 4),\n            ""sum"": 1050.930197,\n        },\n        {\n            ""map_type"": ""wcs"",\n            ""ebounds"": [0.1, 10],\n            ""shape"": (1, 3, 4),\n            ""sum"": 1050.9301972,\n        },\n        # TODO: make this work for HPX\n        # \'HpxGeom\' object has no attribute \'separation\'\n        # {\n        #     \'geom\': geom(map_type=\'hpx\', ebounds=[0.1, 1, 10]),\n        #     \'shape\': \'???\',\n        #     \'sum\': \'???\',\n        # },\n    ],\n)\ndef test_make_map_background_irf(bkg_3d, pars, fixed_pointing_info):\n    m = make_map_background_irf(\n        pointing=fixed_pointing_info,\n        ontime=""42 s"",\n        bkg=bkg_3d,\n        geom=geom(\n            map_type=pars[""map_type""],\n            ebounds=pars[""ebounds""],\n            skydir=fixed_pointing_info.radec,\n        ),\n        oversampling=10,\n    )\n\n    assert m.data.shape == pars[""shape""]\n    assert m.unit == """"\n    assert_allclose(m.data.sum(), pars[""sum""], rtol=1e-5)\n\n\n@requires_data()\ndef test_make_map_background_irf_constant(fixed_pointing_info_aligned):\n    m = make_map_background_irf_with_symmetry(\n        fpi=fixed_pointing_info_aligned, symmetry=""constant""\n    )\n    for d in m.data:\n        assert_allclose(d[1, :], d[1, 0])  # Constant along lon\n        assert_allclose(d[0, 1], d[2, 1])  # Symmetric along lat\n        with pytest.raises(AssertionError):\n            # Not constant along lat due to changes in\n            # solid angle (great circle)\n            assert_allclose(d[:, 1], d[0, 1])\n\n\n@requires_data()\ndef test_make_map_background_irf_sym(fixed_pointing_info_aligned):\n    m = make_map_background_irf_with_symmetry(\n        fpi=fixed_pointing_info_aligned, symmetry=""symmetric""\n    )\n    for d in m.data:\n        assert_allclose(d[1, 0], d[1, 2], rtol=1e-4)  # Symmetric along lon\n        assert_allclose(d[0, 1], d[2, 1], rtol=1e-4)  # Symmetric along lat\n\n\n@requires_data()\ndef test_make_map_background_irf_asym(fixed_pointing_info_aligned):\n    m = make_map_background_irf_with_symmetry(\n        fpi=fixed_pointing_info_aligned, symmetry=""asymmetric""\n    )\n    for d in m.data:\n        # TODO:\n        #  Dimensions of skymap data are [energy, lat, lon] (and is\n        #  representated as [lon, lat, energy] in the api, but the bkg irf\n        #  dimensions are currently [energy, lon, lat] - Will be changed in\n        #  the future (perhaps when IRFs use the skymaps class)\n        assert_allclose(d[1, 0], d[1, 2], rtol=1e-4)  # Symmetric along lon\n        with pytest.raises(AssertionError):\n            assert_allclose(d[0, 1], d[2, 1], rtol=1e-4)  # Symmetric along lat\n        assert_allclose(d[0, 1] * 9, d[2, 1], rtol=1e-4)  # Asymmetric along lat\n\n\ndef test_make_edisp_kernel_map():\n    migra = MapAxis.from_edges(np.linspace(0.5, 1.5, 50), unit="""", name=""migra"")\n    etrue = MapAxis.from_energy_bounds(0.5, 2, 6, unit=""TeV"", name=""energy_true"")\n    offset = MapAxis.from_edges(np.linspace(0.0, 2.0, 3), unit=""deg"", name=""offset"")\n    ereco = MapAxis.from_energy_bounds(0.5, 2, 3, unit=""TeV"", name=""energy"")\n\n    edisp = EnergyDispersion2D.from_gauss(\n        etrue.edges, migra.edges, 0, 0.01, offset.edges\n    )\n\n    geom = WcsGeom.create(10, binsz=0.5, axes=[ereco, etrue])\n    pointing = SkyCoord(0, 0, frame=""icrs"", unit=""deg"")\n    edispmap = make_edisp_kernel_map(edisp, pointing, geom)\n\n    kernel = edispmap.get_edisp_kernel(pointing)\n    assert_allclose(kernel.pdf_matrix[:, 0], (1.0, 1.0, 0.0, 0.0, 0.0, 0.0))\n    assert_allclose(kernel.pdf_matrix[:, 1], (0.0, 0.0, 1.0, 1.0, 0.0, 0.0))\n    assert_allclose(kernel.pdf_matrix[:, 2], (0.0, 0.0, 0.0, 0.0, 1.0, 1.0))\n'"
gammapy/maps/tests/__init__.py,0,b''
gammapy/maps/tests/test_base.py,9,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose, assert_equal\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nfrom astropy.units import Quantity, Unit\nfrom gammapy.maps import HpxGeom, HpxNDMap, Map, MapAxis, WcsGeom, WcsNDMap\n\npytest.importorskip(""healpy"")\n\nmap_axes = [\n    MapAxis.from_bounds(1.0, 10.0, 3, interp=""log"", name=""energy""),\n    MapAxis.from_bounds(0.1, 1.0, 4, interp=""log"", name=""time""),\n]\n\nmapbase_args = [\n    (0.1, 10.0, ""wcs"", SkyCoord(0.0, 30.0, unit=""deg""), None, """"),\n    (0.1, 10.0, ""wcs"", SkyCoord(0.0, 30.0, unit=""deg""), map_axes[:1], """"),\n    (0.1, 10.0, ""wcs"", SkyCoord(0.0, 30.0, unit=""deg""), map_axes, ""m^2""),\n    (0.1, 10.0, ""hpx"", SkyCoord(0.0, 30.0, unit=""deg""), None, """"),\n    (0.1, 10.0, ""hpx"", SkyCoord(0.0, 30.0, unit=""deg""), map_axes[:1], """"),\n    (0.1, 10.0, ""hpx"", SkyCoord(0.0, 30.0, unit=""deg""), map_axes, ""s^2""),\n]\n\nmapbase_args_with_axes = [_ for _ in mapbase_args if _[4] is not None]\n\n\n@pytest.mark.parametrize(\n    (""binsz"", ""width"", ""map_type"", ""skydir"", ""axes"", ""unit""), mapbase_args\n)\ndef test_map_create(binsz, width, map_type, skydir, axes, unit):\n    m = Map.create(\n        binsz=binsz, width=width, map_type=map_type, skydir=skydir, axes=axes, unit=unit\n    )\n    assert m.unit == unit\n\n\n@pytest.mark.parametrize(\n    (""binsz"", ""width"", ""map_type"", ""skydir"", ""axes"", ""unit""), mapbase_args_with_axes\n)\ndef test_map_copy(binsz, width, map_type, skydir, axes, unit):\n    m = Map.create(\n        binsz=binsz, width=width, map_type=map_type, skydir=skydir, axes=axes, unit=unit\n    )\n\n    m_copy = m.copy()\n    assert repr(m) == repr(m_copy)\n\n    m_copy = m.copy(unit=""cm-2 s-1"")\n    assert m_copy.unit == ""cm-2 s-1""\n    assert m_copy.unit is not m.unit\n\n    m_copy = m.copy(meta={""is_copy"": True})\n    assert m_copy.meta[""is_copy""]\n    assert m_copy.meta is not m.meta\n\n    m_copy = m.copy(data=42 * np.ones(m.data.shape))\n    assert m_copy.data[(0,) * m_copy.data.ndim] == 42\n    assert m_copy.data is not m.data\n\n\ndef test_map_from_geom():\n    geom = WcsGeom.create(binsz=1.0, width=10.0)\n    m = Map.from_geom(geom)\n    assert isinstance(m, WcsNDMap)\n    assert m.geom.is_image\n\n    geom = HpxGeom.create(binsz=1.0, width=10.0)\n    m = Map.from_geom(geom)\n    assert isinstance(m, HpxNDMap)\n    assert m.geom.is_image\n\n\n@pytest.mark.parametrize(\n    (""binsz"", ""width"", ""map_type"", ""skydir"", ""axes"", ""unit""), mapbase_args_with_axes\n)\ndef test_map_get_image_by_coord(binsz, width, map_type, skydir, axes, unit):\n    m = Map.create(\n        binsz=binsz, width=width, map_type=map_type, skydir=skydir, axes=axes, unit=unit\n    )\n    m.data = np.arange(m.data.size, dtype=float).reshape(m.data.shape)\n\n    coords = (3.456, 0.1234)[: len(m.geom.axes)]\n    m_image = m.get_image_by_coord(coords)\n\n    im_geom = m.geom.to_image()\n    skycoord = im_geom.get_coord().skycoord\n    m_vals = m.get_by_coord((skycoord,) + coords)\n    assert_equal(m_image.data, m_vals)\n\n\n@pytest.mark.parametrize(\n    (""binsz"", ""width"", ""map_type"", ""skydir"", ""axes"", ""unit""), mapbase_args_with_axes\n)\ndef test_map_get_image_by_pix(binsz, width, map_type, skydir, axes, unit):\n    m = Map.create(\n        binsz=binsz, width=width, map_type=map_type, skydir=skydir, axes=axes, unit=unit\n    )\n    pix = (1.2345, 0.1234)[: len(m.geom.axes)]\n    m_image = m.get_image_by_pix(pix)\n\n    im_geom = m.geom.to_image()\n    idx = im_geom.get_idx()\n    m_vals = m.get_by_pix(idx + pix)\n    assert_equal(m_image.data, m_vals)\n\n\n@pytest.mark.parametrize(\n    (""binsz"", ""width"", ""map_type"", ""skydir"", ""axes"", ""unit""), mapbase_args_with_axes\n)\ndef test_map_slice_by_idx(binsz, width, map_type, skydir, axes, unit):\n    m = Map.create(\n        binsz=binsz, width=width, map_type=map_type, skydir=skydir, axes=axes, unit=unit\n    )\n    data = np.arange(m.data.size, dtype=float)\n    m.data = data.reshape(m.data.shape)\n\n    # Test none slicing\n    sliced = m.slice_by_idx({})\n    assert_equal(m.geom.shape_axes, sliced.geom.shape_axes)\n\n    slices = {""energy"": slice(0, 1), ""time"": slice(0, 2)}\n    sliced = m.slice_by_idx(slices)\n    assert not sliced.geom.is_image\n    slices = tuple([slices[ax.name] for ax in m.geom.axes])\n    assert_equal(m.data[slices[::-1]], sliced.data)\n    assert sliced.data.base is data\n\n    slices = {""energy"": 0, ""time"": 1}\n    sliced = m.slice_by_idx(slices)\n    assert sliced.geom.is_image\n    slices = tuple([slices[ax.name] for ax in m.geom.axes])\n    assert_equal(m.data[slices[::-1]], sliced.data)\n    assert sliced.data.base is data\n\n\n@pytest.mark.parametrize(""map_type"", [""wcs"", ""hpx""])\ndef test_map_meta_read_write(map_type):\n    meta = {""user"": ""test""}\n\n    m = Map.create(\n        binsz=0.1,\n        width=10.0,\n        map_type=map_type,\n        skydir=SkyCoord(0.0, 30.0, unit=""deg""),\n        meta=meta,\n    )\n\n    hdulist = m.to_hdulist(hdu=""COUNTS"")\n    header = hdulist[""COUNTS""].header\n\n    assert header[""META""] == \'{""user"": ""test""}\'\n\n    m2 = Map.from_hdulist(hdulist)\n    assert m2.meta == meta\n\n\nunit_args = [(""wcs"", ""s""), (""wcs"", """"), (""wcs"", Unit(""sr"")), (""hpx"", ""m^2"")]\n\n\n@pytest.mark.parametrize((""map_type"", ""unit""), unit_args)\ndef test_map_quantity(map_type, unit):\n    m = Map.create(binsz=0.1, width=10.0, map_type=map_type, unit=unit)\n\n    # This is to test if default constructor with no unit performs as expected\n    if unit is None:\n        unit = """"\n    assert m.quantity.unit == Unit(unit)\n\n    m.quantity = Quantity(np.ones_like(m.data), ""m2"")\n    assert m.unit == ""m2""\n\n\n@pytest.mark.parametrize((""map_type"", ""unit""), unit_args)\ndef test_map_unit_read_write(map_type, unit):\n    m = Map.create(binsz=0.1, width=10.0, map_type=map_type, unit=unit)\n\n    hdu_list = m.to_hdulist(hdu=""COUNTS"")\n    header = hdu_list[""COUNTS""].header\n\n    assert Unit(header[""BUNIT""]) == Unit(unit)\n\n    m2 = Map.from_hdulist(hdu_list)\n    assert m2.unit == unit\n\n\n@pytest.mark.parametrize((""map_type"", ""unit""), unit_args)\ndef test_map_repr(map_type, unit):\n    m = Map.create(binsz=0.1, width=10.0, map_type=map_type, unit=unit)\n    assert m.__class__.__name__ in repr(m)\n\n\ndef test_map_properties():\n    # Test default values and types of all map properties,\n    # as well as the behaviour for the property get and set.\n\n    m = Map.create(npix=(2, 1))\n\n    assert isinstance(m.unit, u.CompositeUnit)\n    assert m.unit == """"\n    m.unit = ""cm-2 s-1""\n    assert m.unit.to_string() == ""1 / (cm2 s)""\n\n    assert isinstance(m.meta, dict)\n    m.meta = {""spam"": 42}\n    assert isinstance(m.meta, dict)\n\n    # The rest of the tests are for the `data` property\n\n    assert isinstance(m.data, np.ndarray)\n    assert m.data.dtype == np.float32\n    assert m.data.shape == (1, 2)\n    assert_equal(m.data, 0)\n\n    # Assigning an array of matching shape stores it away\n    data = np.ones((1, 2))\n    m.data = data\n    assert m.data is data\n\n    # In-place modification += should work as expected\n    m.data = np.array([[42, 43]])\n    data = m.data\n    m.data += 1\n    assert m.data is data\n    assert_equal(m.data, [[43, 44]])\n\n    # Assigning to a slice of the map data should work as expected\n    data = m.data\n    m.data[:, :1] = 99\n    assert m.data is data\n    assert_equal(m.data, [[99, 44]])\n\n    # Assigning something that doesn\'t match raises an appropriate error\n    with pytest.raises(ValueError):\n        m.data = np.ones((1, 3))\n\n\nmap_arithmetics_args = [(""wcs""), (""hpx"")]\n\n\n@pytest.mark.parametrize((""map_type""), map_arithmetics_args)\ndef test_map_arithmetics(map_type):\n\n    m1 = Map.create(binsz=0.1, width=1.0, map_type=map_type, skydir=(0, 0), unit=""m2"")\n\n    m2 = Map.create(binsz=0.1, width=1.0, map_type=map_type, skydir=(0, 0), unit=""m2"")\n    m2.data += 1.0\n\n    # addition\n    m1 += 1 * u.cm ** 2\n    assert m1.unit == u.Unit(""m2"")\n    assert_allclose(m1.data, 1e-4)\n\n    m3 = m1 + m2\n    assert m3.unit == u.Unit(""m2"")\n    assert_allclose(m3.data, 1.0001)\n\n    # substraction\n    m3 -= 1 * u.cm ** 2\n    assert m3.unit == u.Unit(""m2"")\n    assert_allclose(m3.data, 1.0)\n\n    m3 = m2 - m1\n    assert m3.unit == u.Unit(""m2"")\n    assert_allclose(m3.data, 0.9999)\n\n    m4 = Map.create(binsz=0.1, width=1.0, map_type=map_type, skydir=(0, 0), unit=""s"")\n    m4.data += 1.0\n\n    # multiplication\n    m1 *= 1e4\n    assert m1.unit == u.Unit(""m2"")\n    assert_allclose(m1.data, 1)\n\n    m5 = m2 * m4\n    assert m5.unit == u.Unit(""m2s"")\n    assert_allclose(m5.data, 1)\n\n    # division\n    m5 /= 10 * u.s\n    assert m5.unit == u.Unit(""m2"")\n    assert_allclose(m5.data, 0.1)\n\n    # check unit consistency\n    with pytest.raises(u.UnitConversionError):\n        m5 += 1 * u.W\n\n    m1.data *= 0.0\n    m1.unit = """"\n    m1 += 4\n    assert m1.unit == u.Unit("""")\n    assert_allclose(m1.data, 4)\n\n    lt_m2 = m2 < 1.5*u.m**2\n    assert lt_m2.data.dtype == bool\n    assert_allclose(lt_m2, True)\n\n    le_m2 = m2 <= 10000*u.cm**2\n    assert_allclose(le_m2, True)\n\n    gt_m2 = m2 > 15000*u.cm**2\n    assert_allclose(gt_m2, False)\n\n    ge_m2 = m2 >= m2\n    assert_allclose(ge_m2, True)\n\n    eq_m2 = m2 == 500*u.cm**2\n    assert_allclose(eq_m2, False)\n\n    ne_m2 = m2 != 500*u.cm**2\n    assert_allclose(ne_m2, True)\n\ndef test_arithmetics_inconsistent_geom():\n    m_wcs = Map.create(binsz=0.1, width=1.0)\n    m_wcs_incorrect = Map.create(binsz=0.1, width=2.0)\n\n    with pytest.raises(ValueError):\n        m_wcs += m_wcs_incorrect\n\n    m_hpx = Map.create(binsz=0.1, width=1.0, map_type=""hpx"")\n    with pytest.raises(ValueError):\n        m_wcs += m_hpx\n\n\n# TODO: correct serialization for lin axis for energy\n# map_serialization_args = [(""log""), (""lin"")]\n\nmap_serialization_args = [(""log"")]\n\n\n@pytest.mark.parametrize((""interp""), map_serialization_args)\ndef test_arithmetics_after_serialization(tmp_path, interp):\n    axis = MapAxis.from_bounds(\n        1.0, 10.0, 3, interp=interp, name=""energy"", node_type=""center"", unit=""TeV""\n    )\n    m_wcs = Map.create(binsz=0.1, width=1.0, map_type=""wcs"", skydir=(0, 0), axes=[axis])\n    m_wcs += 1\n\n    m_wcs.write(tmp_path / ""tmp.fits"")\n    m_wcs_serialized = Map.read(tmp_path / ""tmp.fits"")\n\n    m_wcs += m_wcs_serialized\n\n    assert_allclose(m_wcs.data, 2.0)\n\n\ndef test_set_scalar():\n    m = Map.create(width=1)\n    m.data = 1\n    assert m.data.shape == (10, 10)\n    assert_allclose(m.data, 1)\n'"
gammapy/maps/tests/test_counts.py,1,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.table import Table\nfrom gammapy.data import EventList\nfrom gammapy.maps import HpxGeom, Map, MapAxis, WcsNDMap\nfrom gammapy.utils.testing import requires_dependency\n\n\n@pytest.fixture()\ndef events():\n    t = Table()\n    t[""EVENT_ID""] = np.array([1, 5], dtype=np.uint16)\n    t[""RA""] = [5, 11] * u.deg\n    t[""DEC""] = [0, 0] * u.deg\n    t[""ENERGY""] = [10, 12] * u.TeV\n    t[""TIME""] = [3, 4] * u.s\n    return EventList(t)\n\n\ndef test_map_fill_events_wcs(events):\n    # 2D map\n    m = Map.create(npix=(2, 1), binsz=10)\n    m.fill_events(events)\n    assert_allclose(m.data, [[1, 0]])\n\n    # 3D with energy axis\n    axis = MapAxis.from_edges([9, 11, 13], name=""energy"", unit=""TeV"")\n    m = Map.create(npix=(2, 1), binsz=10, axes=[axis])\n    m.fill_events(events)\n    assert m.data.sum() == 1\n    assert_allclose(m.data[0, 0, 0], 1)\n\n\n@requires_dependency(""healpy"")\ndef test_map_fill_events_hpx(events):\n    # 2D map\n    m = Map.from_geom(HpxGeom(1))\n    m.fill_events(events)\n    assert m.data[4] == 2\n\n    # 3D with energy axis\n    axis = MapAxis.from_edges([9, 11, 13], name=""energy"", unit=""TeV"")\n    m = Map.from_geom(HpxGeom(1, axes=[axis]))\n    m.fill_events(events)\n    assert m.data[0, 4] == 1\n    assert m.data[1, 4] == 1\n\n\ndef test_map_fill_events_keyerror(events):\n    axis = MapAxis([0, 1, 2], name=""nokey"")\n    m = WcsNDMap.create(binsz=0.1, npix=10, axes=[axis])\n    with pytest.raises(KeyError):\n        m.fill_events(events)\n\n\ndef test_map_fill_events_uint_column(events):\n    # Check that unsigned int column works.\n    # Regression test for https://github.com/gammapy/gammapy/issues/1620\n    axis = MapAxis.from_edges([0, 3, 6], name=""event_id"")\n    m = Map.create(npix=(2, 1), binsz=10, axes=[axis])\n    m.fill_events(events)\n    assert m.data.sum() == 1\n    assert_allclose(m.data[0, 0, 0], 1)\n'"
gammapy/maps/tests/test_geom.py,14,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose, assert_equal\nfrom astropy import units as u\nfrom astropy.coordinates import SkyCoord\nfrom gammapy.maps import MapAxis, MapCoord\n\nmapaxis_geoms = [\n    (np.array([0.25, 0.75, 1.0, 2.0]), ""lin""),\n    (np.array([0.25, 0.75, 1.0, 2.0]), ""log""),\n    (np.array([0.25, 0.75, 1.0, 2.0]), ""sqrt""),\n]\n\nmapaxis_geoms_node_type = [\n    ([0.25, 0.75, 1.0, 2.0], ""lin"", ""edges""),\n    ([0.25, 0.75, 1.0, 2.0], ""log"", ""edges""),\n    ([0.25, 0.75, 1.0, 2.0], ""sqrt"", ""edges""),\n    ([0.25, 0.75, 1.0, 2.0], ""lin"", ""center""),\n    ([0.25, 0.75, 1.0, 2.0], ""log"", ""center""),\n    ([0.25, 0.75, 1.0, 2.0], ""sqrt"", ""center""),\n]\n\n\n@pytest.mark.parametrize((""edges"", ""interp""), mapaxis_geoms)\ndef test_mapaxis_init_from_edges(edges, interp):\n    axis = MapAxis(edges, interp=interp)\n    assert_allclose(axis.edges, edges)\n    assert_allclose(axis.nbin, len(edges) - 1)\n    with pytest.raises(ValueError):\n        MapAxis.from_edges([1])\n        MapAxis.from_edges([0, 1, 1, 2])\n        MapAxis.from_edges([0, 1, 3, 2])\n\n\n@pytest.mark.parametrize((""nodes"", ""interp""), mapaxis_geoms)\ndef test_mapaxis_from_nodes(nodes, interp):\n    axis = MapAxis.from_nodes(nodes, interp=interp)\n    assert_allclose(axis.center, nodes)\n    assert_allclose(axis.nbin, len(nodes))\n    with pytest.raises(ValueError):\n        MapAxis.from_nodes([])\n        MapAxis.from_nodes([0, 1, 1, 2])\n        MapAxis.from_nodes([0, 1, 3, 2])\n\n\n@pytest.mark.parametrize((""nodes"", ""interp""), mapaxis_geoms)\ndef test_mapaxis_from_bounds(nodes, interp):\n    axis = MapAxis.from_bounds(nodes[0], nodes[-1], 3, interp=interp)\n    assert_allclose(axis.edges[0], nodes[0])\n    assert_allclose(axis.edges[-1], nodes[-1])\n    assert_allclose(axis.nbin, 3)\n    with pytest.raises(ValueError):\n        MapAxis.from_bounds(1, 1, 1)\n\n\n@pytest.mark.parametrize((""nodes"", ""interp"", ""node_type""), mapaxis_geoms_node_type)\ndef test_mapaxis_pix_to_coord(nodes, interp, node_type):\n    axis = MapAxis(nodes, interp=interp, node_type=node_type)\n    assert_allclose(axis.center, axis.pix_to_coord(np.arange(axis.nbin, dtype=float)))\n    assert_allclose(\n        np.arange(axis.nbin + 1, dtype=float) - 0.5, axis.coord_to_pix(axis.edges)\n    )\n\n\n@pytest.mark.parametrize((""nodes"", ""interp"", ""node_type""), mapaxis_geoms_node_type)\ndef test_mapaxis_coord_to_idx(nodes, interp, node_type):\n    axis = MapAxis(nodes, interp=interp, node_type=node_type)\n    assert_allclose(np.arange(axis.nbin, dtype=int), axis.coord_to_idx(axis.center))\n\n\n@pytest.mark.parametrize((""nodes"", ""interp"", ""node_type""), mapaxis_geoms_node_type)\ndef test_mapaxis_slice(nodes, interp, node_type):\n    axis = MapAxis(nodes, interp=interp, node_type=node_type)\n    saxis = axis.slice(slice(1, 3))\n    assert_allclose(saxis.nbin, 2)\n    assert_allclose(saxis.center, axis.center[slice(1, 3)])\n\n    axis = MapAxis(nodes, interp=interp, node_type=node_type)\n    saxis = axis.slice(slice(1, None))\n    assert_allclose(saxis.nbin, axis.nbin - 1)\n    assert_allclose(saxis.center, axis.center[slice(1, None)])\n\n    axis = MapAxis(nodes, interp=interp, node_type=node_type)\n    saxis = axis.slice(slice(None, 2))\n    assert_allclose(saxis.nbin, 2)\n    assert_allclose(saxis.center, axis.center[slice(None, 2)])\n\n    axis = MapAxis(nodes, interp=interp, node_type=node_type)\n    saxis = axis.slice(slice(None, -1))\n    assert_allclose(saxis.nbin, axis.nbin - 1)\n    assert_allclose(saxis.center, axis.center[slice(None, -1)])\n\n\ndef test_mapcoords_create():\n    # From existing MapCoord\n    coords_cel = MapCoord.create((0.0, 1.0), frame=""icrs"")\n    coords_gal = MapCoord.create(coords_cel, frame=""galactic"")\n    assert_allclose(coords_gal.lon, coords_cel.skycoord.galactic.l.deg)\n    assert_allclose(coords_gal.lat, coords_cel.skycoord.galactic.b.deg)\n\n    # 2D Tuple of scalars\n    coords = MapCoord.create((0.0, 1.0))\n    assert_allclose(coords.lon, 0.0)\n    assert_allclose(coords.lat, 1.0)\n    assert_allclose(coords[0], 0.0)\n    assert_allclose(coords[1], 1.0)\n    assert coords.frame is None\n    assert coords.ndim == 2\n\n    # 3D Tuple of scalars\n    coords = MapCoord.create((0.0, 1.0, 2.0))\n    assert_allclose(coords[0], 0.0)\n    assert_allclose(coords[1], 1.0)\n    assert_allclose(coords[2], 2.0)\n    assert coords.frame is None\n    assert coords.ndim == 3\n\n    # 2D Tuple w/ NaN coordinates\n    coords = MapCoord.create((np.nan, np.nan))\n\n    # 2D Tuple w/ NaN coordinates\n    lon, lat = np.array([np.nan, 1.0]), np.array([np.nan, 3.0])\n    coords = MapCoord.create((lon, lat))\n    assert_allclose(coords.lon, lon)\n    assert_allclose(coords.lat, lat)\n\n    # 2D Tuple w/ SkyCoord\n    lon, lat = np.array([0.0, 1.0]), np.array([2.0, 3.0])\n    energy = np.array([100.0, 1000.0])\n    skycoord_cel = SkyCoord(lon, lat, unit=""deg"", frame=""icrs"")\n    skycoord_gal = SkyCoord(lon, lat, unit=""deg"", frame=""galactic"")\n    coords = MapCoord.create((skycoord_cel,))\n    assert_allclose(coords.lon, lon)\n    assert_allclose(coords.lat, lat)\n    assert coords.frame == ""icrs""\n    assert coords.ndim == 2\n\n    coords = MapCoord.create((skycoord_gal,))\n    assert_allclose(coords.lon, lon)\n    assert_allclose(coords.lat, lat)\n    assert coords.frame == ""galactic""\n    assert coords.ndim == 2\n\n    # SkyCoord\n    coords = MapCoord.create(skycoord_cel)\n    assert_allclose(coords.lon, lon)\n    assert_allclose(coords.lat, lat)\n    assert coords.frame == ""icrs""\n    assert coords.ndim == 2\n    coords = MapCoord.create(skycoord_gal)\n    assert_allclose(coords.lon, lon)\n    assert_allclose(coords.lat, lat)\n    assert coords.frame == ""galactic""\n    assert coords.ndim == 2\n\n    # 2D dict w/ vectors\n    coords = MapCoord.create(dict(lon=lon, lat=lat))\n    assert_allclose(coords.lon, lon)\n    assert_allclose(coords.lat, lat)\n    assert coords.ndim == 2\n\n    # 3D dict w/ vectors\n    coords = MapCoord.create(dict(lon=lon, lat=lat, energy=energy))\n    assert_allclose(coords.lon, lon)\n    assert_allclose(coords.lat, lat)\n    assert_allclose(coords[""energy""], energy)\n    assert coords.ndim == 3\n\n    # 3D dict w/ SkyCoord\n    coords = MapCoord.create(dict(skycoord=skycoord_cel, energy=energy))\n    assert_allclose(coords.lon, lon)\n    assert_allclose(coords.lat, lat)\n    assert_allclose(coords[""energy""], energy)\n    assert coords.frame == ""icrs""\n    assert coords.ndim == 3\n\n    # 3D dict  w/ vectors\n    coords = MapCoord.create({""energy"": energy, ""lat"": lat, ""lon"": lon})\n    assert_allclose(coords.lon, lon)\n    assert_allclose(coords.lat, lat)\n    assert_allclose(coords[""energy""], energy)\n    assert_allclose(coords[0], energy)\n    assert_allclose(coords[1], lat)\n    assert_allclose(coords[2], lon)\n    assert coords.ndim == 3\n\n    # Quantities\n    coords = MapCoord.create(dict(energy=energy * u.TeV, lat=lat, lon=lon))\n    assert coords[""energy""].unit == ""TeV""\n\n\ndef test_mapcoords_to_frame():\n    lon, lat = np.array([0.0, 1.0]), np.array([2.0, 3.0])\n    energy = np.array([100.0, 1000.0])\n    skycoord_cel = SkyCoord(lon, lat, unit=""deg"", frame=""icrs"")\n    skycoord_gal = SkyCoord(lon, lat, unit=""deg"", frame=""galactic"")\n\n    coords = MapCoord.create(dict(lon=lon, lat=lat, energy=energy), frame=""icrs"")\n    assert coords.frame == ""icrs""\n    assert_allclose(coords.skycoord.transform_to(""icrs"").ra.deg, skycoord_cel.ra.deg)\n    assert_allclose(coords.skycoord.transform_to(""icrs"").dec.deg, skycoord_cel.dec.deg)\n    coords = coords.to_frame(""galactic"")\n    assert coords.frame == ""galactic""\n    assert_allclose(\n        coords.skycoord.transform_to(""galactic"").l.deg, skycoord_cel.galactic.l.deg\n    )\n    assert_allclose(\n        coords.skycoord.transform_to(""galactic"").b.deg, skycoord_cel.galactic.b.deg\n    )\n\n    coords = MapCoord.create(dict(lon=lon, lat=lat, energy=energy), frame=""galactic"")\n    assert coords.frame == ""galactic""\n    assert_allclose(coords.skycoord.transform_to(""galactic"").l.deg, skycoord_gal.l.deg)\n    assert_allclose(coords.skycoord.transform_to(""galactic"").b.deg, skycoord_gal.b.deg)\n    coords = coords.to_frame(""icrs"")\n    assert coords.frame == ""icrs""\n    assert_allclose(\n        coords.skycoord.transform_to(""icrs"").ra.deg, skycoord_gal.icrs.ra.deg\n    )\n    assert_allclose(\n        coords.skycoord.transform_to(""icrs"").dec.deg, skycoord_gal.icrs.dec.deg\n    )\n\n\ndef test_mapaxis_repr():\n    axis = MapAxis([1, 2, 3], name=""test"")\n    assert ""MapAxis"" in repr(axis)\n\n\ndef test_mapcoord_repr():\n    coord = MapCoord({""lon"": 0, ""lat"": 0, ""energy"": 5})\n    assert ""MapCoord"" in repr(coord)\n\n\nnodes_array = np.array([0.25, 0.75, 1.0, 2.0])\nmapaxis_geoms_node_type_unit = [\n    (nodes_array, ""lin"", ""edges"", ""s"", ""TEST"", True),\n    (nodes_array, ""log"", ""edges"", ""s"", ""test"", False),\n    (nodes_array, ""lin"", ""edges"", ""TeV"", ""TEST"", False),\n    (nodes_array, ""sqrt"", ""edges"", ""s"", ""test"", False),\n    (nodes_array, ""lin"", ""center"", ""s"", ""test"", False),\n    (nodes_array + 1e-9, ""lin"", ""edges"", ""s"", ""test"", True),\n    (nodes_array + 1e-3, ""lin"", ""edges"", ""s"", ""test"", False),\n    (nodes_array / 3600.0, ""lin"", ""edges"", ""hr"", ""TEST"", True),\n]\n\n\n@pytest.mark.parametrize(\n    (""nodes"", ""interp"", ""node_type"", ""unit"", ""name"", ""result""),\n    mapaxis_geoms_node_type_unit,\n)\ndef test_mapaxis_equal(nodes, interp, node_type, unit, name, result):\n    axis1 = MapAxis(nodes_array, name=""test"", unit=""s"", interp=""lin"", node_type=""edges"")\n\n    axis2 = MapAxis(nodes, name=name, unit=unit, interp=interp, node_type=node_type)\n\n    assert (axis1 == axis2) is result\n    assert (axis1 != axis2) is not result\n\n\ndef test_squash():\n    axis = MapAxis(\n        nodes=[0, 1, 2, 3], unit=""TeV"", name=""energy"", node_type=""edges"", interp=""lin""\n    )\n    ax_sq = axis.squash()\n\n    assert_allclose(ax_sq.nbin, 1)\n    assert_allclose(axis.edges[0], ax_sq.edges[0])\n    assert_allclose(axis.edges[-1], ax_sq.edges[1])\n    assert_allclose(ax_sq.center, 1.5 * u.TeV)\n\n\ndef test_upsample():\n    axis = MapAxis(\n        nodes=[0, 1, 2, 3], unit=""TeV"", name=""energy"", node_type=""edges"", interp=""lin""\n    )\n    axis_up = axis.upsample(10)\n\n    assert_allclose(axis_up.nbin, 10 * axis.nbin)\n    assert_allclose(axis_up.edges[0], axis.edges[0])\n    assert_allclose(axis_up.edges[-1], axis.edges[-1])\n    assert axis_up.node_type == axis.node_type\n\n\ndef test_downsample():\n    axis = MapAxis(\n        nodes=[0, 1, 2, 3, 4, 5, 6, 7, 8],\n        unit=""TeV"",\n        name=""energy"",\n        node_type=""edges"",\n        interp=""lin"",\n    )\n    axis_down = axis.downsample(2)\n\n    assert_allclose(axis_down.nbin, 0.5 * axis.nbin)\n    assert_allclose(axis_down.edges[0], axis.edges[0])\n    assert_allclose(axis_down.edges[-1], axis.edges[-1])\n    assert axis_down.node_type == axis.node_type\n\n\n@pytest.fixture(scope=""session"")\ndef energy_axis_ref():\n    edges = np.arange(1, 11) * u.TeV\n    return MapAxis.from_edges(edges, name=""energy"")\n\n\ndef test_group_table_basic(energy_axis_ref):\n    e_edges = [1, 2, 10] * u.TeV\n\n    groups = energy_axis_ref.group_table(e_edges)\n\n    assert_allclose(groups[""group_idx""], [0, 1])\n    assert_allclose(groups[""idx_min""], [0, 1])\n    assert_allclose(groups[""idx_max""], [0, 8])\n    assert_allclose(groups[""energy_min""].to_value(""TeV""), [1, 2])\n    assert_allclose(groups[""energy_max""].to_value(""TeV""), [2, 10])\n\n    bin_type = [_.strip() for _ in groups[""bin_type""]]\n    assert_equal(bin_type, [""normal"", ""normal""])\n\n\n@pytest.mark.parametrize(\n    ""e_edges"", [[1.8, 4.8, 7.2] * u.TeV, [2, 5, 7] * u.TeV, [2000, 5000, 7000] * u.GeV]\n)\ndef test_group_table_edges(energy_axis_ref, e_edges):\n    groups = energy_axis_ref.group_table(e_edges)\n\n    assert_allclose(groups[""group_idx""], [0, 1, 2, 3])\n    assert_allclose(groups[""idx_min""], [0, 1, 4, 6])\n    assert_allclose(groups[""idx_max""], [0, 3, 5, 8])\n    assert_allclose(groups[""energy_min""].to_value(""TeV""), [1, 2, 5, 7])\n    assert_allclose(groups[""energy_max""].to_value(""TeV""), [2, 5, 7, 10])\n\n    bin_type = [_.strip() for _ in groups[""bin_type""]]\n    assert_equal(bin_type, [""underflow"", ""normal"", ""normal"", ""overflow""])\n\n\ndef test_group_table_below_range(energy_axis_ref):\n    e_edges = [0.7, 0.8, 1, 4] * u.TeV\n    groups = energy_axis_ref.group_table(e_edges)\n\n    assert_allclose(groups[""group_idx""], [0, 1])\n    assert_allclose(groups[""idx_min""], [0, 3])\n    assert_allclose(groups[""idx_max""], [2, 8])\n    assert_allclose(groups[""energy_min""].to_value(""TeV""), [1, 4])\n    assert_allclose(groups[""energy_max""].to_value(""TeV""), [4, 10])\n\n    bin_type = [_.strip() for _ in groups[""bin_type""]]\n    assert_equal(bin_type, [""normal"", ""overflow""])\n\n\ndef test_group_table_above_range(energy_axis_ref):\n    e_edges = [5, 7, 11, 13] * u.TeV\n    groups = energy_axis_ref.group_table(e_edges)\n\n    assert_allclose(groups[""group_idx""], [0, 1, 2])\n    assert_allclose(groups[""idx_min""], [0, 4, 6])\n    assert_allclose(groups[""idx_max""], [3, 5, 8])\n    assert_allclose(groups[""energy_min""].to_value(""TeV""), [1, 5, 7])\n    assert_allclose(groups[""energy_max""].to_value(""TeV""), [5, 7, 10])\n\n    bin_type = [_.strip() for _ in groups[""bin_type""]]\n    assert_equal(bin_type, [""underflow"", ""normal"", ""normal""])\n\n\ndef test_group_table_outside_range(energy_axis_ref):\n    e_edges = [20, 30, 40] * u.TeV\n\n    with pytest.raises(ValueError):\n        energy_axis_ref.group_table(e_edges)\n\n\ndef test_map_axis_single_bin():\n    with pytest.raises(ValueError):\n        _ = MapAxis.from_nodes([1])\n'"
gammapy/maps/tests/test_hpx.py,110,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom astropy.io import fits\nfrom gammapy.maps import MapAxis, MapCoord\nfrom gammapy.maps.hpx import (\n    HpxGeom,\n    get_hpxregion_dir,\n    get_hpxregion_size,\n    get_pix_size_from_nside,\n    get_subpixels,\n    get_superpixels,\n    make_hpx_to_wcs_mapping,\n    nside_to_order,\n    ravel_hpx_index,\n    unravel_hpx_index,\n)\n\npytest.importorskip(""healpy"")\n\nhpx_allsky_test_geoms = [\n    # 2D All-sky\n    (8, False, ""galactic"", None, None),\n    # 3D All-sky\n    (8, False, ""galactic"", None, [MapAxis(np.logspace(0.0, 3.0, 4))]),\n    # 3D All-sky w/ variable pixel size\n    ([2, 4, 8], False, ""galactic"", None, [MapAxis(np.logspace(0.0, 3.0, 4))]),\n    # 4D All-sky\n    (\n        8,\n        False,\n        ""galactic"",\n        None,\n        [\n            MapAxis(np.logspace(0.0, 3.0, 3), name=""axis0""),\n            MapAxis(np.logspace(0.0, 2.0, 4), name=""axis1""),\n        ],\n    ),\n]\n\nhpx_partialsky_test_geoms = [\n    # 2D Partial-sky\n    (8, False, ""galactic"", ""DISK(110.,75.,10.)"", None),\n    # 3D Partial-sky\n    (8, False, ""galactic"", ""DISK(110.,75.,10.)"", [MapAxis(np.logspace(0.0, 3.0, 4))]),\n    # 3D Partial-sky w/ variable pixel size\n    (\n        [8, 16, 32],\n        False,\n        ""galactic"",\n        ""DISK(110.,75.,10.)"",\n        [MapAxis(np.logspace(0.0, 3.0, 4))],\n    ),\n    # 4D Partial-sky w/ variable pixel size\n    (\n        [[8, 16, 32], [8, 8, 16]],\n        False,\n        ""galactic"",\n        ""DISK(110.,75.,10.)"",\n        [\n            MapAxis(np.logspace(0.0, 3.0, 3), name=""axis0""),\n            MapAxis(np.logspace(0.0, 2.0, 4), name=""axis1""),\n        ],\n    ),\n]\n\nhpx_test_geoms = hpx_allsky_test_geoms + hpx_partialsky_test_geoms\n\n\ndef make_test_coords(geom, lon, lat):\n    coords = [lon, lat] + [ax.center for ax in geom.axes]\n    coords = np.meshgrid(*coords)\n    coords = tuple([np.ravel(t) for t in coords])\n    return MapCoord.create(coords)\n\n\ndef test_unravel_hpx_index():\n    npix = np.array([2, 7])\n    assert_allclose(\n        unravel_hpx_index(np.array([0, 4]), npix), (np.array([0, 2]), np.array([0, 1]))\n    )\n    npix = np.array([[2, 7], [3, 1]])\n    assert_allclose(\n        unravel_hpx_index(np.array([0, 3, 10]), npix),\n        (np.array([0, 1, 1]), np.array([0, 0, 1]), np.array([0, 1, 0])),\n    )\n\n\ndef test_ravel_hpx_index():\n    npix = np.array([2, 7])\n    idx = (np.array([0, 2]), np.array([0, 1]))\n    assert_allclose(ravel_hpx_index(idx, npix), np.array([0, 4]))\n    npix = np.array([[2, 7], [3, 1]])\n    idx = (np.array([0, 1, 1]), np.array([0, 0, 1]), np.array([0, 1, 0]))\n    assert_allclose(ravel_hpx_index(idx, npix), np.array([0, 3, 10]))\n\n\ndef make_test_nside(nside, nside0, nside1):\n    npix = 12 * nside ** 2\n    nside_test = np.concatenate(\n        (nside0 * np.ones(npix // 2, dtype=int), nside1 * np.ones(npix // 2, dtype=int))\n    )\n    return nside_test\n\n\n@pytest.mark.parametrize(\n    (""nside_subpix"", ""nside_superpix"", ""nest""),\n    [\n        (4, 2, True),\n        (8, 2, True),\n        (8, make_test_nside(8, 4, 2), True),\n        (4, 2, False),\n        (8, 2, False),\n        (8, make_test_nside(8, 4, 2), False),\n    ],\n)\ndef test_get_superpixels(nside_subpix, nside_superpix, nest):\n    import healpy as hp\n\n    npix = 12 * nside_subpix ** 2\n    subpix = np.arange(npix)\n    ang_subpix = hp.pix2ang(nside_subpix, subpix, nest=nest)\n    superpix = get_superpixels(subpix, nside_subpix, nside_superpix, nest=nest)\n    pix1 = hp.ang2pix(nside_superpix, *ang_subpix, nest=nest)\n    assert_allclose(superpix, pix1)\n\n    subpix = subpix.reshape((12, -1))\n    if not np.isscalar(nside_subpix):\n        nside_subpix = nside_subpix.reshape((12, -1))\n    if not np.isscalar(nside_superpix):\n        nside_superpix = nside_superpix.reshape((12, -1))\n\n    ang_subpix = hp.pix2ang(nside_subpix, subpix, nest=nest)\n    superpix = get_superpixels(subpix, nside_subpix, nside_superpix, nest=nest)\n    pix1 = hp.ang2pix(nside_superpix, *ang_subpix, nest=nest)\n    assert_allclose(superpix, pix1)\n\n\n@pytest.mark.parametrize(\n    (""nside_superpix"", ""nside_subpix"", ""nest""),\n    [(2, 4, True), (2, 8, True), (2, 4, False), (2, 8, False)],\n)\ndef test_get_subpixels(nside_superpix, nside_subpix, nest):\n    import healpy as hp\n\n    npix = 12 * nside_superpix ** 2\n    superpix = np.arange(npix)\n    subpix = get_subpixels(superpix, nside_superpix, nside_subpix, nest=nest)\n    ang1 = hp.pix2ang(nside_subpix, subpix, nest=nest)\n    pix1 = hp.ang2pix(nside_superpix, *ang1, nest=nest)\n    assert np.all(superpix[..., None] == pix1)\n\n    superpix = superpix.reshape((12, -1))\n    subpix = get_subpixels(superpix, nside_superpix, nside_subpix, nest=nest)\n    ang1 = hp.pix2ang(nside_subpix, subpix, nest=nest)\n    pix1 = hp.ang2pix(nside_superpix, *ang1, nest=nest)\n    assert np.all(superpix[..., None] == pix1)\n\n    pix1 = get_superpixels(subpix, nside_subpix, nside_superpix, nest=nest)\n    assert np.all(superpix[..., None] == pix1)\n\n\ndef test_hpx_global_to_local():\n    ax0 = np.linspace(0.0, 1.0, 3)\n    ax1 = np.linspace(0.0, 1.0, 3)\n\n    # 2D All-sky\n    hpx = HpxGeom(16, False, ""galactic"")\n    assert_allclose(hpx[0], np.array([0]))\n    assert_allclose(hpx[633], np.array([633]))\n    assert_allclose(hpx[0, 633], np.array([0, 633]))\n    assert_allclose(hpx[np.array([0, 633])], np.array([0, 633]))\n\n    # 3D All-sky\n    hpx = HpxGeom(16, False, ""galactic"", axes=[ax0])\n    assert_allclose(\n        hpx[(np.array([177, 177]), np.array([0, 1]))], np.array([177, 177 + 3072])\n    )\n\n    # 2D Partial-sky\n    hpx = HpxGeom(64, False, ""galactic"", region=""DISK(110.,75.,2.)"")\n    assert_allclose(hpx[0, 633, 706], np.array([-1, 0, 2]))\n\n    # 3D Partial-sky\n    hpx = HpxGeom(64, False, ""galactic"", region=""DISK(110.,75.,2.)"", axes=[ax0])\n    assert_allclose(hpx[633], np.array([0]))\n    assert_allclose(hpx[49859], np.array([19]))\n    assert_allclose(hpx[0, 633, 706, 49859, 49935], np.array([-1, 0, 2, 19, 21]))\n    assert_allclose(\n        hpx[np.array([0, 633, 706, 49859, 49935])], np.array([-1, 0, 2, 19, 21])\n    )\n    assert_allclose(\n        hpx[(np.array([0, 633, 706, 707, 783]), np.array([0, 0, 0, 1, 1]))],\n        np.array([-1, 0, 2, 19, 21]),\n    )\n\n    # 3D Partial-sky w/ variable bin size\n    hpx = HpxGeom([32, 64], False, ""galactic"", region=""DISK(110.,75.,2.)"", axes=[ax0])\n\n    assert_allclose(hpx[191], np.array([0]))\n    assert_allclose(hpx[12995], np.array([6]))\n    assert_allclose(hpx[0, 191, 233, 12995], np.array([-1, 0, 2, 6]))\n    assert_allclose(\n        hpx[(np.array([0, 191, 233, 707]), np.array([0, 0, 0, 1]))],\n        np.array([-1, 0, 2, 6]),\n    )\n\n    # 4D Partial-sky w/ variable bin size\n    hpx = HpxGeom(\n        [[16, 32], [32, 64]],\n        False,\n        ""galactic"",\n        region=""DISK(110.,75.,2.)"",\n        axes=[ax0, ax1],\n    )\n    assert_allclose(hpx[3263], np.array([1]))\n    assert_allclose(hpx[28356], np.array([11]))\n    assert_allclose(hpx[(np.array([46]), np.array([0]), np.array([0]))], np.array([0]))\n\n\n@pytest.mark.parametrize(\n    (""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_allsky_test_geoms\n)\ndef test_hpxgeom_init_with_pix(nside, nested, frame, region, axes):\n    geom = HpxGeom(nside, nested, frame, region=region, axes=axes)\n\n    idx0 = geom.get_idx(flat=True)\n    idx1 = tuple([t[::10] for t in idx0])\n    geom = HpxGeom(nside, nested, frame, region=idx0, axes=axes)\n    assert_allclose(idx0, geom.get_idx(flat=True))\n    assert_allclose(len(idx0[0]), np.sum(geom.npix))\n    geom = HpxGeom(nside, nested, frame, region=idx1, axes=axes)\n    assert_allclose(idx1, geom.get_idx(flat=True))\n    assert_allclose(len(idx1[0]), np.sum(geom.npix))\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxgeom_to_slice(nside, nested, frame, region, axes):\n    geom = HpxGeom(nside, nested, frame, region=region, axes=axes)\n    slices = tuple([slice(1, 2) for i in range(2, geom.ndim)])\n    geom_slice = geom.to_slice(slices)\n    assert_allclose(geom_slice.ndim, 2)\n    assert_allclose(geom_slice.npix, np.squeeze(geom.npix[slices]))\n\n    idx = geom.get_idx(flat=True)\n    idx_slice = geom_slice.get_idx(flat=True)\n    if geom.ndim > 2:\n        m = np.all([np.isin(t, [1]) for t in idx[1:]], axis=0)\n        assert_allclose(idx_slice, (idx[0][m],))\n    else:\n        assert_allclose(idx_slice, idx)\n\n    # Test slicing with explicit geometry\n    geom = HpxGeom(nside, nested, frame, region=tuple([t[::3] for t in idx]), axes=axes)\n    geom_slice = geom.to_slice(slices)\n    assert_allclose(geom_slice.ndim, 2)\n    assert_allclose(geom_slice.npix, np.squeeze(geom.npix[slices]))\n\n    idx = geom.get_idx()\n    idx_slice = geom_slice.get_idx()\n    if geom.ndim > 2:\n        m = np.all([np.isin(t, [1]) for t in idx[1:]], axis=0)\n        assert_allclose(idx_slice, (idx[0][m],))\n    else:\n        assert_allclose(idx_slice, idx)\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxgeom_get_pix(nside, nested, frame, region, axes):\n    geom = HpxGeom(nside, nested, frame, region=region, axes=axes)\n    idx = geom.get_idx(local=False, flat=True)\n    idx_local = geom.get_idx(local=True, flat=True)\n    assert_allclose(idx, geom.local_to_global(idx_local))\n\n    if axes is not None:\n        idx_img = geom.get_idx(local=False, idx=tuple([1] * len(axes)), flat=True)\n        idx_img_local = geom.get_idx(local=True, idx=tuple([1] * len(axes)), flat=True)\n        assert_allclose(idx_img, geom.local_to_global(idx_img_local))\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxgeom_coord_to_idx(nside, nested, frame, region, axes):\n    import healpy as hp\n\n    geom = HpxGeom(nside, nested, frame, region=region, axes=axes)\n    lon = np.array([112.5, 135.0, 105.0])\n    lat = np.array([75.3, 75.3, 74.6])\n    coords = make_test_coords(geom, lon, lat)\n    zidx = tuple([ax.coord_to_idx(t) for t, ax in zip(coords[2:], geom.axes)])\n\n    if geom.nside.size > 1:\n        nside = geom.nside[zidx]\n    else:\n        nside = geom.nside\n\n    phi, theta = coords.phi, coords.theta\n    idx = geom.coord_to_idx(coords)\n    assert_allclose(hp.ang2pix(nside, theta, phi), idx[0])\n    for i, z in enumerate(zidx):\n        assert_allclose(z, idx[i + 1])\n\n    # Test w/ coords outside the geometry\n    lon = np.array([0.0, 5.0, 10.0])\n    lat = np.array([75.3, 75.3, 74.6])\n    coords = make_test_coords(geom, lon, lat)\n    zidx = [ax.coord_to_idx(t) for t, ax in zip(coords[2:], geom.axes)]\n\n    idx = geom.coord_to_idx(coords)\n    if geom.region is not None:\n        assert_allclose(np.full_like(coords[0], -1, dtype=int), idx[0])\n\n    idx = geom.coord_to_idx(coords, clip=True)\n    assert np.all(np.not_equal(np.full_like(coords[0], -1, dtype=int), idx[0]))\n\n\ndef test_hpxgeom_coord_to_pix():\n    lon = np.array([110.25, 114.0, 105.0])\n    lat = np.array([75.3, 75.3, 74.6])\n    z0 = np.array([0.5, 1.5, 2.5])\n    z1 = np.array([3.5, 4.5, 5.5])\n    ax0 = np.linspace(0.0, 3.0, 4)\n    ax1 = np.linspace(3.0, 6.0, 4)\n\n    pix64 = np.array([784, 785, 864])\n\n    # 2D all-sky\n    coords = (lon, lat)\n    hpx = HpxGeom(64, False, ""galactic"")\n    assert_allclose(hpx.coord_to_pix(coords)[0], pix64)\n\n    # 2D partial-sky\n    coords = (lon, lat)\n    hpx = HpxGeom(64, False, ""galactic"", region=""DISK(110.,75.,2.)"")\n    assert_allclose(hpx.coord_to_pix(coords)[0], pix64)\n\n    # 3D partial-sky\n    coords = (lon, lat, z0)\n    hpx = HpxGeom(64, False, ""galactic"", region=""DISK(110.,75.,2.)"", axes=[ax0])\n    assert_allclose(hpx.coord_to_pix(coords), (pix64, np.array([0, 1, 2])))\n\n    # 3D partial-sky w/ variable bin size\n    coords = (lon, lat, z0)\n    nside = [16, 32, 64]\n    hpx_bins = [\n        HpxGeom(n, False, ""galactic"", region=""DISK(110.,75.,2.)"") for n in nside\n    ]\n    hpx = HpxGeom(nside, False, ""galactic"", region=""DISK(110.,75.,2.)"", axes=[ax0])\n    for i, (x, y, z) in enumerate(np.vstack(coords).T):\n        pix0 = hpx.coord_to_pix((np.array([x]), np.array([y]), np.array([z])))\n        pix1 = hpx_bins[i].coord_to_pix((np.array([x]), np.array([y])))\n        assert_allclose(pix0[0], pix1[0])\n\n    # 4D partial-sky\n    coords = (lon, lat, z0, z1)\n    hpx = HpxGeom(64, False, ""galactic"", region=""DISK(110.,75.,2.)"", axes=[ax0, ax1])\n    assert_allclose(\n        hpx.coord_to_pix(coords), (pix64, np.array([0, 1, 2]), np.array([0, 1, 2]))\n    )\n\n\ndef test_hpx_nside_to_order():\n    assert_allclose(nside_to_order(64), np.array([6]))\n    assert_allclose(\n        nside_to_order(np.array([10, 32, 42, 64, 128, 256])),\n        np.array([-1, 5, -1, 6, 7, 8]),\n    )\n\n    order = np.linspace(1, 10, 10).astype(int)\n    nside = 2 ** order\n    assert_allclose(nside_to_order(nside), order)\n    assert_allclose(nside_to_order(nside).reshape((2, 5)), order.reshape((2, 5)))\n\n\ndef test_hpx_get_pix_size_from_nside():\n    assert_allclose(\n        get_pix_size_from_nside(np.array([1, 2, 4])), np.array([32.0, 16.0, 8.0])\n    )\n\n\ndef test_hpx_get_hpxregion_size():\n    assert_allclose(get_hpxregion_size(""DISK(110.,75.,2.)""), 2.0)\n\n\ndef test_hpxgeom_get_hpxregion_dir():\n    refdir = get_hpxregion_dir(""DISK(110.,75.,2.)"", ""galactic"")\n    assert_allclose(refdir.l.deg, 110.0)\n    assert_allclose(refdir.b.deg, 75.0)\n\n    refdir = get_hpxregion_dir(None, ""galactic"")\n    assert_allclose(refdir.l.deg, 0.0)\n    assert_allclose(refdir.b.deg, 0.0)\n\n\ndef test_hpxgeom_make_wcs():\n    ax0 = np.linspace(0.0, 3.0, 4)\n\n    hpx = HpxGeom(64, False, ""galactic"", region=""DISK(110.,75.,2.)"")\n    wcs = hpx.make_wcs()\n    assert_allclose(wcs.wcs.wcs.crval, np.array([110.0, 75.0]))\n\n    hpx = HpxGeom(64, False, ""galactic"", region=""DISK(110.,75.,2.)"", axes=[ax0])\n    wcs = hpx.make_wcs()\n    assert_allclose(wcs.wcs.wcs.crval, np.array([110.0, 75.0]))\n\n\ndef test_hpxgeom_get_coord():\n    ax0 = np.linspace(0.0, 3.0, 4)\n\n    # 2D all-sky\n    hpx = HpxGeom(16, False, ""galactic"")\n    c = hpx.get_coord()\n    assert_allclose(c[0][:3], np.array([45.0, 135.0, 225.0]))\n    assert_allclose(c[1][:3], np.array([87.075819, 87.075819, 87.075819]))\n\n    # 3D all-sky\n    hpx = HpxGeom(16, False, ""galactic"", axes=[ax0])\n    c = hpx.get_coord()\n    assert_allclose(c[0][0, :3], np.array([45.0, 135.0, 225.0]))\n    assert_allclose(c[1][0, :3], np.array([87.075819, 87.075819, 87.075819]))\n    assert_allclose(c[2][0, :3], np.array([0.5, 0.5, 0.5]))\n\n    # 2D partial-sky\n    hpx = HpxGeom(64, False, ""galactic"", region=""DISK(110.,75.,2.)"")\n    c = hpx.get_coord()\n    assert_allclose(c[0][:3], np.array([107.5, 112.5, 106.57894737]))\n    assert_allclose(c[1][:3], np.array([76.813533, 76.813533, 76.07742]))\n\n    # 3D partial-sky\n    hpx = HpxGeom(64, False, ""galactic"", region=""DISK(110.,75.,2.)"", axes=[ax0])\n    c = hpx.get_coord()\n    assert_allclose(c[0][0, :3], np.array([107.5, 112.5, 106.57894737]))\n    assert_allclose(c[1][0, :3], np.array([76.813533, 76.813533, 76.07742]))\n    assert_allclose(c[2][0, :3], np.array([0.5, 0.5, 0.5]))\n\n    # 3D partial-sky w/ variable bin size\n    hpx = HpxGeom(\n        [16, 32, 64], False, ""galactic"", region=""DISK(110.,75.,2.)"", axes=[ax0]\n    )\n    c = hpx.get_coord(flat=True)\n    assert_allclose(c[0][:3], np.array([117.0, 103.5, 112.5]))\n    assert_allclose(c[1][:3], np.array([75.340734, 75.340734, 75.340734]))\n    assert_allclose(c[2][:3], np.array([0.5, 1.5, 1.5]))\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxgeom_contains(nside, nested, frame, region, axes):\n    geom = HpxGeom(nside, nested, frame, region=region, axes=axes)\n    coords = geom.get_coord(flat=True)\n    assert_allclose(geom.contains(coords), np.ones_like(coords[0], dtype=bool))\n\n    if axes is not None:\n        coords = [c[0] for c in coords[:2]] + [ax.edges[-1] + 1.0 for ax in axes]\n        assert_allclose(geom.contains(coords), np.zeros((1,), dtype=bool))\n\n    if geom.region is not None:\n        coords = [0.0, 0.0] + [ax.center[0] for ax in geom.axes]\n        assert_allclose(geom.contains(coords), np.zeros((1,), dtype=bool))\n\n\ndef test_make_hpx_to_wcs_mapping():\n    ax0 = np.linspace(0.0, 1.0, 3)\n    hpx = HpxGeom(16, False, ""galactic"", region=""DISK(110.,75.,2.)"")\n    # FIXME construct explicit WCS projection here\n    wcs = hpx.make_wcs()\n    hpx2wcs = make_hpx_to_wcs_mapping(hpx, wcs)\n    assert_allclose(\n        hpx2wcs[0],\n        np.array(\n            [\n                67,\n                46,\n                46,\n                46,\n                46,\n                29,\n                67,\n                67,\n                46,\n                46,\n                46,\n                46,\n                67,\n                67,\n                67,\n                46,\n                46,\n                46,\n                67,\n                67,\n                67,\n                28,\n                28,\n                28,\n                45,\n                45,\n                45,\n                45,\n                28,\n                28,\n                66,\n                45,\n                45,\n                45,\n                45,\n                28,\n            ]\n        ),\n    )\n    assert_allclose(\n        hpx2wcs[1],\n        np.array(\n            [\n                0.11111111,\n                0.09090909,\n                0.09090909,\n                0.09090909,\n                0.09090909,\n                1.0,\n                0.11111111,\n                0.11111111,\n                0.09090909,\n                0.09090909,\n                0.09090909,\n                0.09090909,\n                0.11111111,\n                0.11111111,\n                0.11111111,\n                0.09090909,\n                0.09090909,\n                0.09090909,\n                0.11111111,\n                0.11111111,\n                0.11111111,\n                0.16666667,\n                0.16666667,\n                0.16666667,\n                0.125,\n                0.125,\n                0.125,\n                0.125,\n                0.16666667,\n                0.16666667,\n                1.0,\n                0.125,\n                0.125,\n                0.125,\n                0.125,\n                0.16666667,\n            ]\n        ),\n    )\n\n    hpx = HpxGeom([8, 16], False, ""galactic"", region=""DISK(110.,75.,2.)"", axes=[ax0])\n    hpx2wcs = make_hpx_to_wcs_mapping(hpx, wcs)\n    assert_allclose(\n        hpx2wcs[0],\n        np.array(\n            [\n                [\n                    15,\n                    6,\n                    6,\n                    6,\n                    6,\n                    6,\n                    15,\n                    15,\n                    6,\n                    6,\n                    6,\n                    6,\n                    15,\n                    15,\n                    15,\n                    6,\n                    6,\n                    6,\n                    15,\n                    15,\n                    15,\n                    6,\n                    6,\n                    6,\n                    15,\n                    15,\n                    15,\n                    15,\n                    6,\n                    6,\n                    15,\n                    15,\n                    15,\n                    15,\n                    15,\n                    6,\n                ],\n                [\n                    67,\n                    46,\n                    46,\n                    46,\n                    46,\n                    29,\n                    67,\n                    67,\n                    46,\n                    46,\n                    46,\n                    46,\n                    67,\n                    67,\n                    67,\n                    46,\n                    46,\n                    46,\n                    67,\n                    67,\n                    67,\n                    28,\n                    28,\n                    28,\n                    45,\n                    45,\n                    45,\n                    45,\n                    28,\n                    28,\n                    66,\n                    45,\n                    45,\n                    45,\n                    45,\n                    28,\n                ],\n            ]\n        ),\n    )\n\n\ndef test_hpxgeom_from_header():\n    pars = {\n        ""HPX_REG"": ""DISK(110.,75.,2.)"",\n        ""EXTNAME"": ""SKYMAP"",\n        ""NSIDE"": 2 ** 6,\n        ""ORDER"": 6,\n        ""PIXTYPE"": ""HEALPIX"",\n        ""ORDERING"": ""RING"",\n        ""COORDSYS"": ""CEL"",\n        ""TTYPE1"": ""PIX"",\n        ""TFORM1"": ""K"",\n        ""TTYPE2"": ""CHANNEL1"",\n        ""TFORM2"": ""D"",\n        ""INDXSCHM"": ""EXPLICIT"",\n    }\n    header = fits.Header()\n    header.update(pars)\n    hpx = HpxGeom.from_header(header)\n\n    assert hpx.frame == ""icrs""\n    assert hpx.nest is False\n    assert_allclose(hpx.nside, np.array([64]))\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxgeom_read_write(tmp_path, nside, nested, frame, region, axes):\n    geom0 = HpxGeom(nside, nested, frame, region=region, axes=axes)\n    hdu_bands = geom0.make_bands_hdu(hdu=""BANDS"")\n    hdu_prim = fits.PrimaryHDU()\n    hdu_prim.header.update(geom0.make_header())\n\n    hdulist = fits.HDUList([hdu_prim, hdu_bands])\n    hdulist.writeto(tmp_path / ""tmp.fits"")\n\n    with fits.open(tmp_path / ""tmp.fits"", memmap=False) as hdulist:\n        geom1 = HpxGeom.from_header(hdulist[0].header, hdulist[""BANDS""])\n\n    assert_allclose(geom0.nside, geom1.nside)\n    assert_allclose(geom0.npix, geom1.npix)\n    assert_allclose(geom0.nest, geom1.nest)\n    assert geom0.frame == geom1.frame\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxgeom_upsample(nside, nested, frame, region, axes):\n    # NESTED\n    geom = HpxGeom(nside, True, frame, region=region, axes=axes)\n    geom_up = geom.upsample(2)\n    assert_allclose(2 * geom.nside, geom_up.nside)\n    assert_allclose(4 * geom.npix, geom_up.npix)\n    coords = geom_up.get_coord(flat=True)\n    assert np.all(geom.contains(coords))\n\n    # RING\n    geom = HpxGeom(nside, False, frame, region=region, axes=axes)\n    geom_up = geom.upsample(2)\n    assert_allclose(2 * geom.nside, geom_up.nside)\n    assert_allclose(4 * geom.npix, geom_up.npix)\n    coords = geom_up.get_coord(flat=True)\n    assert np.all(geom.contains(coords))\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxgeom_downsample(nside, nested, frame, region, axes):\n    # NESTED\n    geom = HpxGeom(nside, True, frame, region=region, axes=axes)\n    geom_down = geom.downsample(2)\n    assert_allclose(geom.nside, 2 * geom_down.nside)\n    coords = geom.get_coord(flat=True)\n    assert np.all(geom_down.contains(coords))\n\n    # RING\n    geom = HpxGeom(nside, False, frame, region=region, axes=axes)\n    geom_down = geom.downsample(2)\n    assert_allclose(geom.nside, 2 * geom_down.nside)\n    coords = geom.get_coord(flat=True)\n    assert np.all(geom_down.contains(coords))\n\n\ndef test_hpxgeom_solid_angle():\n    geom = HpxGeom.create(\n        nside=8, frame=""galactic"", axes=[MapAxis.from_edges([0, 2, 3])]\n    )\n\n    solid_angle = geom.solid_angle()\n\n    assert solid_angle.shape == (1,)\n    assert_allclose(solid_angle.value, 0.016362461737446838)\n\n\ndef test_geom_repr():\n    geom = HpxGeom(nside=8)\n    assert geom.__class__.__name__ in repr(geom)\n    assert ""nside"" in repr(geom)\n\n\nhpx_equality_test_geoms = [\n    (16, False, ""galactic"", None, True),\n    (16, True, ""galactic"", None, False),\n    (8, False, ""galactic"", None, False),\n    (16, False, ""icrs"", None, False),\n]\n\n\n@pytest.mark.parametrize(\n    (""nside"", ""nested"", ""frame"", ""region"", ""result""), hpx_equality_test_geoms\n)\ndef test_hpxgeom_equal(nside, nested, frame, region, result):\n    geom0 = HpxGeom(16, False, ""galactic"", region=None)\n    geom1 = HpxGeom(nside, nested, frame, region=region)\n\n    assert (geom0 == geom1) is result\n    assert (geom0 != geom1) is not result\n'"
gammapy/maps/tests/test_hpxmap.py,16,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom astropy import units as u\nfrom astropy.coordinates import SkyCoord\nfrom astropy.io import fits\nfrom gammapy.maps import HpxGeom, HpxMap, HpxNDMap, Map, MapAxis\nfrom gammapy.maps.utils import fill_poisson\nfrom gammapy.utils.testing import mpl_plot_check, requires_dependency\n\npytest.importorskip(""healpy"")\n\naxes1 = [MapAxis(np.logspace(0.0, 3.0, 3), interp=""log"")]\n\nhpx_test_allsky_geoms = [\n    (8, False, ""galactic"", None, None),\n    (8, False, ""galactic"", None, axes1),\n    ([4, 8], False, ""galactic"", None, axes1),\n]\n\nhpx_test_partialsky_geoms = [\n    ([4, 8], False, ""galactic"", ""DISK(110.,75.,30.)"", axes1),\n    (8, False, ""galactic"", ""DISK(110.,75.,10.)"", [MapAxis(np.logspace(0.0, 3.0, 4))]),\n    (\n        8,\n        False,\n        ""galactic"",\n        ""DISK(110.,75.,10.)"",\n        [\n            MapAxis(np.logspace(0.0, 3.0, 4), name=""axis0""),\n            MapAxis(np.logspace(0.0, 2.0, 3), name=""axis1""),\n        ],\n    ),\n]\n\nhpx_test_geoms = hpx_test_allsky_geoms + hpx_test_partialsky_geoms\n\n\ndef create_map(nside, nested, frame, region, axes):\n    return HpxNDMap(\n        HpxGeom(nside=nside, nest=nested, frame=frame, region=region, axes=axes)\n    )\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxmap_init(nside, nested, frame, region, axes):\n    geom = HpxGeom(nside=nside, nest=nested, frame=frame, region=region, axes=axes)\n    shape = [int(np.max(geom.npix))]\n    if axes:\n        shape += [ax.nbin for ax in axes]\n    shape = shape[::-1]\n    data = np.random.uniform(0, 1, shape)\n    m = HpxNDMap(geom)\n    assert m.data.shape == data.shape\n    m = HpxNDMap(geom, data)\n    assert_allclose(m.data, data)\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxmap_create(nside, nested, frame, region, axes):\n    create_map(nside, nested, frame, region, axes)\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxmap_read_write(tmp_path, nside, nested, frame, region, axes):\n    path = tmp_path / ""tmp.fits""\n\n    m = create_map(nside, nested, frame, region, axes)\n    fill_poisson(m, mu=0.5, random_state=0)\n    m.write(path, sparse=True, overwrite=True)\n\n    m2 = HpxNDMap.read(path)\n    m4 = Map.read(path, map_type=""hpx"")\n    msk = np.ones_like(m2.data[...], dtype=bool)\n\n    assert_allclose(m.data[...][msk], m2.data[...][msk])\n    assert_allclose(m.data[...][msk], m4.data[...][msk])\n\n    m.write(path, overwrite=True)\n    m2 = HpxNDMap.read(path)\n    m3 = HpxMap.read(path, map_type=""hpx"")\n    m4 = Map.read(path, map_type=""hpx"")\n    assert_allclose(m.data[...][msk], m2.data[...][msk])\n    assert_allclose(m.data[...][msk], m3.data[...][msk])\n    assert_allclose(m.data[...][msk], m4.data[...][msk])\n\n    # Specify alternate HDU name for IMAGE and BANDS table\n    m.write(path, sparse=True, hdu=""IMAGE"", hdu_bands=""TEST"", overwrite=True)\n    m2 = HpxNDMap.read(path)\n    m3 = Map.read(path)\n    m4 = Map.read(path, map_type=""hpx"")\n\n\ndef test_hpxmap_read_write_fgst(tmp_path):\n    path = tmp_path / ""tmp.fits""\n\n    axis = MapAxis.from_bounds(100.0, 1000.0, 4, name=""energy"", unit=""MeV"")\n\n    # Test Counts Cube\n    m = create_map(8, False, ""galactic"", None, [axis])\n    m.write(path, conv=""fgst-ccube"", overwrite=True)\n    with fits.open(path, memmap=False) as hdulist:\n        assert ""SKYMAP"" in hdulist\n        assert ""EBOUNDS"" in hdulist\n        assert hdulist[""SKYMAP""].header[""HPX_CONV""] == ""FGST-CCUBE""\n        assert hdulist[""SKYMAP""].header[""TTYPE1""] == ""CHANNEL1""\n\n    m2 = Map.read(path)\n\n    # Test Model Cube\n    m.write(path, conv=""fgst-template"", overwrite=True)\n    with fits.open(path, memmap=False) as hdulist:\n        assert ""SKYMAP"" in hdulist\n        assert ""ENERGIES"" in hdulist\n        assert hdulist[""SKYMAP""].header[""HPX_CONV""] == ""FGST-TEMPLATE""\n        assert hdulist[""SKYMAP""].header[""TTYPE1""] == ""ENERGY1""\n\n    m2 = Map.read(path)\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxmap_set_get_by_pix(nside, nested, frame, region, axes):\n    m = create_map(nside, nested, frame, region, axes)\n    coords = m.geom.get_coord(flat=True)\n    idx = m.geom.get_idx(flat=True)\n    m.set_by_pix(idx, coords[0])\n    assert_allclose(coords[0], m.get_by_pix(idx))\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxmap_set_get_by_coord(nside, nested, frame, region, axes):\n    m = create_map(nside, nested, frame, region, axes)\n    coords = m.geom.get_coord(flat=True)\n    m.set_by_coord(coords, coords[0])\n    assert_allclose(coords[0], m.get_by_coord(coords))\n\n    # Test with SkyCoords\n    m = create_map(nside, nested, frame, region, axes)\n    coords = m.geom.get_coord(flat=True)\n    skydir = SkyCoord(coords[0], coords[1], unit=""deg"", frame=m.geom.frame)\n    skydir_cel = skydir.transform_to(""icrs"")\n    skydir_gal = skydir.transform_to(""galactic"")\n    m.set_by_coord((skydir_gal,) + tuple(coords[2:]), coords[0])\n    assert_allclose(coords[0], m.get_by_coord(coords))\n    assert_allclose(\n        m.get_by_coord((skydir_cel,) + tuple(coords[2:])),\n        m.get_by_coord((skydir_gal,) + tuple(coords[2:])),\n    )\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxmap_interp_by_coord(nside, nested, frame, region, axes):\n    m = HpxNDMap(\n        HpxGeom(nside=nside, nest=nested, frame=frame, region=region, axes=axes)\n    )\n    coords = m.geom.get_coord(flat=True)\n    m.set_by_coord(coords, coords[1])\n    assert_allclose(m.get_by_coord(coords), m.interp_by_coord(coords, interp=""linear""))\n\n\ndef test_hpxmap_interp_by_coord_quantities():\n    ax = MapAxis(np.logspace(0.0, 3.0, 3), interp=""log"", name=""energy"", unit=""TeV"")\n    geom = HpxGeom(nside=1, axes=[ax])\n    m = HpxNDMap(geom=geom)\n\n    coords_dict = {""lon"": 99, ""lat"": 42, ""energy"": 1000 * u.GeV}\n\n    coords = m.geom.get_coord(flat=True)\n    m.set_by_coord(coords, coords[""lat""])\n\n    coords_dict[""energy""] = 1 * u.TeV\n    val = m.interp_by_coord(coords_dict)\n    assert_allclose(val, 42, rtol=1e-2)\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxmap_fill_by_coord(nside, nested, frame, region, axes):\n    m = create_map(nside, nested, frame, region, axes)\n    coords = m.geom.get_coord(flat=True)\n    m.fill_by_coord(coords, coords[1])\n    m.fill_by_coord(coords, coords[1])\n    assert_allclose(m.get_by_coord(coords), 2.0 * coords[1])\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxmap_to_wcs(nside, nested, frame, region, axes):\n    m = HpxNDMap(\n        HpxGeom(nside=nside, nest=nested, frame=frame, region=region, axes=axes)\n    )\n    m.to_wcs(sum_bands=False, oversample=2, normalize=False)\n    m.to_wcs(sum_bands=True, oversample=2, normalize=False)\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxmap_swap_scheme(nside, nested, frame, region, axes):\n    m = HpxNDMap(\n        HpxGeom(nside=nside, nest=nested, frame=frame, region=region, axes=axes)\n    )\n    fill_poisson(m, mu=1.0, random_state=0)\n    m2 = m.to_swapped()\n    coords = m.geom.get_coord(flat=True)\n    assert_allclose(m.get_by_coord(coords), m2.get_by_coord(coords))\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxmap_ud_grade(nside, nested, frame, region, axes):\n    m = HpxNDMap(\n        HpxGeom(nside=nside, nest=nested, frame=frame, region=region, axes=axes)\n    )\n    m.to_ud_graded(4)\n\n\n@pytest.mark.parametrize(\n    (""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_partialsky_geoms\n)\ndef test_hpxmap_pad(nside, nested, frame, region, axes):\n    m = HpxNDMap(\n        HpxGeom(nside=nside, nest=nested, frame=frame, region=region, axes=axes)\n    )\n    m.set_by_pix(m.geom.get_idx(flat=True), 1.0)\n    cval = 2.2\n    m_pad = m.pad(1, mode=""constant"", cval=cval)\n    coords_pad = m_pad.geom.get_coord(flat=True)\n    msk = m.geom.contains(coords_pad)\n    coords_out = tuple([c[~msk] for c in coords_pad])\n    assert_allclose(m_pad.get_by_coord(coords_out), cval * np.ones_like(coords_out[0]))\n    coords_in = tuple([c[msk] for c in coords_pad])\n    assert_allclose(m_pad.get_by_coord(coords_in), np.ones_like(coords_in[0]))\n\n\n@pytest.mark.parametrize(\n    (""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_partialsky_geoms\n)\ndef test_hpxmap_crop(nside, nested, frame, region, axes):\n    m = HpxNDMap(\n        HpxGeom(nside=nside, nest=nested, frame=frame, region=region, axes=axes)\n    )\n    m.crop(1)\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxmap_upsample(nside, nested, frame, region, axes):\n    m = HpxNDMap(\n        HpxGeom(nside=nside, nest=nested, frame=frame, region=region, axes=axes),\n        unit=""m2"",\n    )\n    m.set_by_pix(m.geom.get_idx(flat=True), 1.0)\n    m_up = m.upsample(2, preserve_counts=True)\n    assert_allclose(np.nansum(m.data), np.nansum(m_up.data))\n    m_up = m.upsample(2, preserve_counts=False)\n    assert_allclose(4.0 * np.nansum(m.data), np.nansum(m_up.data))\n    assert m.unit == m_up.unit\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxmap_downsample(nside, nested, frame, region, axes):\n    m = HpxNDMap(\n        HpxGeom(nside=nside, nest=nested, frame=frame, region=region, axes=axes),\n        unit=""m2"",\n    )\n    m.set_by_pix(m.geom.get_idx(flat=True), 1.0)\n    m_down = m.downsample(2, preserve_counts=True)\n    assert_allclose(np.nansum(m.data), np.nansum(m_down.data))\n    assert m.unit == m_down.unit\n\n\n@pytest.mark.parametrize((""nside"", ""nested"", ""frame"", ""region"", ""axes""), hpx_test_geoms)\ndef test_hpxmap_sum_over_axes(nside, nested, frame, region, axes):\n    m = HpxNDMap(\n        HpxGeom(nside=nside, nest=nested, frame=frame, region=region, axes=axes)\n    )\n    coords = m.geom.get_coord(flat=True)\n    m.fill_by_coord(coords, coords[0])\n    msum = m.sum_over_axes()\n\n    if m.geom.is_regular:\n        assert_allclose(np.nansum(m.data), np.nansum(msum.data))\n\n\ndef test_coadd_unit():\n    geom = HpxGeom.create(nside=128)\n    m1 = HpxNDMap(geom, unit=""m2"")\n    m2 = HpxNDMap(geom, unit=""cm2"")\n\n    idx = geom.get_idx()\n\n    weights = u.Quantity(np.ones_like(idx[0]), unit=""cm2"")\n    m1.fill_by_idx(idx, weights=weights)\n    assert_allclose(m1.data, 0.0001)\n\n    weights = u.Quantity(np.ones_like(idx[0]), unit=""m2"")\n    m1.fill_by_idx(idx, weights=weights)\n    m1.coadd(m2)\n\n    assert_allclose(m1.data, 1.0001)\n\n\n@requires_dependency(""matplotlib"")\ndef test_plot():\n    m = HpxNDMap.create(binsz=10)\n    with mpl_plot_check():\n        m.plot()\n\n\n@requires_dependency(""matplotlib"")\ndef test_plot_poly():\n    m = HpxNDMap.create(binsz=10)\n    with mpl_plot_check():\n        m.plot(method=""poly"")\n'"
gammapy/maps/tests/test_region.py,4,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nfrom regions import CircleSkyRegion\nfrom gammapy.maps import MapAxis, RegionGeom\n\n\n@pytest.fixture()\ndef region():\n    center = SkyCoord(""0 deg"", ""0 deg"", frame=""galactic"")\n    return CircleSkyRegion(center=center, radius=1 * u.deg)\n\n@pytest.fixture()\ndef energy_axis():\n    return MapAxis.from_energy_bounds(""1 TeV"", ""10 TeV"", nbin=3)\n\n@pytest.fixture()\ndef test_axis():\n    return MapAxis.from_nodes([1,2], unit="""", name=""test"")\n\n\ndef test_create(region):\n    geom = RegionGeom.create(region)\n    assert geom.frame == ""galactic""\n    assert geom.projection == ""TAN""\n    assert not geom.is_image\n    assert not geom.is_allsky\n\n\ndef test_centers(region):\n    geom = RegionGeom.create(region)\n    assert_allclose(geom.center_skydir.l.deg, 0)\n    assert_allclose(geom.center_skydir.b.deg, 0)\n    assert_allclose(geom.center_pix, (0, 0))\n\n    values = [_.value for _ in geom.center_coord]\n    assert_allclose(values, (0, 0))\n\n\ndef test_width(region):\n    geom = RegionGeom.create(region)\n    assert_allclose(geom.width.value, [2.02, 2.02])\n\n\ndef test_create_axis(region, energy_axis, test_axis):\n    geom = RegionGeom.create(region, axes=[energy_axis])\n\n    assert geom.ndim == 3\n    assert len(geom.axes) == 1\n    assert geom.data_shape == (3, 1, 1)\n\n    geom = RegionGeom.create(region, axes=[energy_axis, test_axis])\n    assert geom.ndim == 4\n    assert len(geom.axes) == 2\n    assert geom.data_shape == (2, 3, 1, 1)\n\ndef test_get_coord(region, energy_axis, test_axis):\n    geom = RegionGeom.create(region, axes=[energy_axis])\n    coords = geom.get_coord()\n\n    assert_allclose(coords.lon, 0)\n    assert_allclose(coords.lat, 0)\n    assert_allclose(coords[""energy""].value.squeeze(), [1.467799, 3.162278, 6.812921], rtol=1e-5)\n\n    geom = RegionGeom.create(region, axes=[energy_axis, test_axis])\n    coords = geom.get_coord()\n    assert coords[""lon""].shape == (2, 3, 1, 1)\n    assert coords[""test""].shape == (2, 3, 1, 1)\n    assert_allclose(coords[""energy""].value[1].squeeze(), [1.467799, 3.162278, 6.812921], rtol=1e-5)\n    assert_allclose(coords[""test""].value[:,1].squeeze(), [1,2], rtol=1e-5)\n\n\ndef test_get_idx(region, energy_axis, test_axis):\n    geom = RegionGeom.create(region, axes=[energy_axis])\n    pix = geom.get_idx()\n\n    assert_allclose(pix[0], 0)\n    assert_allclose(pix[1], 0)\n    assert_allclose(pix[2].squeeze(), [0, 1, 2])\n\n    geom = RegionGeom.create(region, axes=[energy_axis, test_axis])\n    pix = geom.get_idx()\n\n    assert pix[0].shape == (2, 3, 1, 1)\n    assert_allclose(pix[0], 0)\n    assert_allclose(pix[1], 0)\n    assert_allclose(pix[2][0].squeeze(), [0, 1, 2])\n\ndef test_coord_to_pix(region, energy_axis, test_axis):\n    geom = RegionGeom.create(region, axes=[energy_axis])\n\n    position = SkyCoord(0, 0, frame=""galactic"", unit=""deg"")\n    coords = {""skycoord"": position, ""energy"": 1 * u.TeV}\n    coords_pix = geom.coord_to_pix(coords)\n\n    assert_allclose(coords_pix[0], 0)\n    assert_allclose(coords_pix[1], 0)\n    assert_allclose(coords_pix[2], -0.5)\n\n    geom = RegionGeom.create(region, axes=[energy_axis, test_axis])\n    coords[""test""] = 2\n    coords_pix = geom.coord_to_pix(coords)\n\n    assert_allclose(coords_pix[0], 0)\n    assert_allclose(coords_pix[1], 0)\n    assert_allclose(coords_pix[2], -0.5)\n    assert_allclose(coords_pix[3], 1)\n\ndef test_pix_to_coord(region, energy_axis):\n    geom = RegionGeom.create(region, axes=[energy_axis])\n\n    pix = (0, 0, 0)\n    coords = geom.pix_to_coord(pix)\n    assert_allclose(coords[0].value, 0)\n    assert_allclose(coords[1].value, 0)\n    assert_allclose(coords[2].value, 1.467799, rtol=1e-5)\n\n    pix = (1, 1, 1)\n    coords = geom.pix_to_coord(pix)\n    assert_allclose(coords[0].value, np.nan)\n    assert_allclose(coords[1].value, np.nan)\n    assert_allclose(coords[2].value, 3.162278, rtol=1e-5)\n\n    pix = (1, 1, 3)\n    coords = geom.pix_to_coord(pix)\n    assert_allclose(coords[2].value, 14.677993, rtol=1e-5)\n\ndef test_pix_to_coord_2axes(region, energy_axis, test_axis):\n    geom = RegionGeom.create(region, axes=[energy_axis, test_axis])\n\n    pix = (0, 0, 0, 0)\n    coords = geom.pix_to_coord(pix)\n    assert_allclose(coords[0].value, 0)\n    assert_allclose(coords[1].value, 0)\n    assert_allclose(coords[2].value, 1.467799, rtol=1e-5)\n    assert_allclose(coords[3].value, 1)\n\n    pix = (0, 0, 0, 2)\n    coords = geom.pix_to_coord(pix)\n    assert_allclose(coords[3].value, 3)\n\n\ndef test_contains(region):\n    geom = RegionGeom.create(region)\n    position = SkyCoord([0, 0], [0, 1.1], frame=""galactic"", unit=""deg"")\n\n    contains = geom.contains(coords={""skycoord"": position})\n    assert_allclose(contains, [1, 0])\n\n\ndef test_solid_angle(region):\n    geom = RegionGeom.create(region)\n    omega = geom.solid_angle()\n\n    assert omega.unit == ""sr""\n    reference = 2 * np.pi * (1 - np.cos(region.radius))\n    assert_allclose(omega.value, reference.value, rtol=1e-3)\n\n\ndef test_bin_volume(region):\n    axis = MapAxis.from_edges([1, 3] * u.TeV, name=""energy"", interp=""log"")\n    geom = RegionGeom.create(region, axes=[axis])\n    volume = geom.bin_volume()\n\n    assert volume.unit == ""sr TeV""\n    reference = 2 * 2 * np.pi * (1 - np.cos(region.radius))\n    assert_allclose(volume.value, reference.value, rtol=1e-3)\n\n\ndef test_separation(region):\n    geom = RegionGeom.create(region)\n\n    position = SkyCoord([0, 0], [0, 1.1], frame=""galactic"", unit=""deg"")\n    separation = geom.separation(position)\n\n    assert_allclose(separation.deg, [0, 1.1])\n\n\ndef test_upsample(region):\n    axis = MapAxis.from_edges([1, 10] * u.TeV, name=""energy"", interp=""log"")\n    geom = RegionGeom.create(region, axes=[axis])\n    geom_up = geom.upsample(factor=2, axis=""energy"")\n\n    assert_allclose(geom_up.axes[0].edges.value, [1.0, 3.162278, 10.0], rtol=1e-5)\n\n\ndef test_downsample(region):\n    axis = MapAxis.from_edges([1, 3.162278, 10] * u.TeV, name=""energy"", interp=""log"")\n    geom = RegionGeom.create(region, axes=[axis])\n    geom_down = geom.downsample(factor=2, axis=""energy"")\n\n    assert_allclose(geom_down.axes[0].edges.value, [1.0, 10.0], rtol=1e-5)\n\n\ndef test_repr(region):\n    axis = MapAxis.from_edges([1, 3.162278, 10] * u.TeV, name=""energy"", interp=""log"")\n    geom = RegionGeom.create(region, axes=[axis])\n\n    assert ""RegionGeom"" in repr(geom)\n    assert ""CircleSkyRegion"" in repr(geom)\n\n\ndef test_eq(region):\n    axis = MapAxis.from_edges([1, 10] * u.TeV, name=""energy"", interp=""log"")\n    geom_1 = RegionGeom.create(region, axes=[axis])\n    geom_2 = RegionGeom.create(region, axes=[axis])\n\n    assert geom_1 == geom_2\n\n    axis = MapAxis.from_edges([1, 100] * u.TeV, name=""energy"", interp=""log"")\n    geom_3 = RegionGeom.create(region, axes=[axis])\n\n    assert not geom_2 == geom_3\n\n\ndef test_to_cube_to_image(region):\n    axis = MapAxis.from_edges([1, 10] * u.TeV, name=""energy"", interp=""log"")\n    geom = RegionGeom.create(region)\n\n    geom_cube = geom.to_cube([axis])\n    assert geom_cube.ndim == 3\n\n    geom = geom_cube.to_image()\n    assert geom.ndim == 2\n'"
gammapy/maps/tests/test_regionnd.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom gammapy.data import EventList\nfrom gammapy.irf import EDispKernel\nfrom gammapy.maps import Map, MapAxis\nfrom gammapy.utils.testing import mpl_plot_check, requires_data, requires_dependency\n\n\n@pytest.fixture\ndef region_map():\n    axis = MapAxis.from_energy_bounds(""1 TeV"", ""10 TeV"", nbin=6, name=""energy"")\n    m = Map.create(\n        region=""icrs;circle(83.63, 21.51, 1)"",\n        map_type=""region"",\n        axes=[axis],\n        unit=""1/TeV"",\n        dtype=np.int,\n    )\n    m.data = np.arange(m.data.size).reshape(m.geom.data_shape)\n    return m\n\n\n@pytest.fixture\ndef region_map_true():\n    axis = MapAxis.from_energy_bounds(""1 TeV"", ""10 TeV"", nbin=6, name=""energy_true"")\n    m = Map.create(\n        region=""icrs;circle(83.63, 21.51, 1)"",\n        map_type=""region"",\n        axes=[axis],\n        unit=""1/TeV"",\n        dtype=np.int,\n    )\n    m.data = np.arange(m.data.size).reshape(m.geom.data_shape)\n    return m\n\n\ndef test_region_nd_map(region_map):\n    assert_allclose(region_map.data.sum(), 15)\n    assert region_map.geom.frame == ""icrs""\n    assert region_map.unit == ""TeV-1""\n    assert region_map.data.dtype == np.int\n    assert ""RegionNDMap"" in str(region_map)\n    assert ""1 / TeV"" in str(region_map)\n\n\n@requires_dependency(""matplotlib"")\ndef test_region_nd_map_plot(region_map):\n    import matplotlib.pyplot as plt\n\n    with mpl_plot_check():\n        region_map.plot()\n\n    ax = plt.subplot(projection=region_map.geom.wcs)\n    with mpl_plot_check():\n        region_map.plot_region(ax=ax)\n\n\ndef test_region_nd_map_misc(region_map):\n    assert_allclose(region_map.sum_over_axes(), 15)\n\n    stacked = region_map.copy()\n    stacked.stack(region_map)\n    assert_allclose(stacked.data.sum(), 30)\n\n    stacked = region_map.copy()\n    weights = Map.from_geom(region_map.geom, dtype=np.int)\n    stacked.stack(region_map, weights=weights)\n    assert_allclose(stacked.data.sum(), 15)\n\n\ndef test_region_nd_map_sample(region_map):\n    upsampled = region_map.upsample(factor=2)\n    assert_allclose(upsampled.data.sum(), 15)\n    assert upsampled.data.shape == (12, 1, 1)\n\n    upsampled = region_map.upsample(factor=2, preserve_counts=False)\n    assert_allclose(upsampled.data[3, 0, 0], 1.25)\n    assert upsampled.data.shape == (12, 1, 1)\n\n    downsampled = region_map.downsample(factor=2)\n    assert_allclose(downsampled.data.sum(), 15)\n    assert_allclose(downsampled.data[2, 0, 0], 9)\n    assert downsampled.data.shape == (3, 1, 1)\n\n    downsampled = region_map.downsample(factor=2, preserve_counts=False)\n    assert_allclose(downsampled.data.sum(), 7.5)\n    assert_allclose(downsampled.data[2, 0, 0], 4.5)\n    assert downsampled.data.shape == (3, 1, 1)\n\n\ndef test_region_nd_map_get(region_map):\n    values = region_map.get_by_idx((0, 0, [2, 3]))\n    assert_allclose(values.squeeze(), [2, 3])\n\n    values = region_map.get_by_pix((0, 0, [2.3, 3.7]))\n    assert_allclose(values.squeeze(), [2, 4])\n\n    energies = region_map.geom.axes[0].center\n    values = region_map.get_by_coord((83.63, 21.51, energies[[0, -1]]))\n    assert_allclose(values.squeeze(), [0, 5])\n\n\ndef test_region_nd_map_set(region_map):\n    region_map = region_map.copy()\n    region_map.set_by_idx((0, 0, [2, 3]), [42, 42])\n    assert_allclose(region_map.data[[2, 3]], 42)\n\n    region_map = region_map.copy()\n    region_map.set_by_pix((0, 0, [2.3, 3.7]), [42, 42])\n    assert_allclose(region_map.data[[2, 3]], 42)\n\n    region_map = region_map.copy()\n    energies = region_map.geom.axes[0].center\n    region_map.set_by_coord((83.63, 21.51, energies[[0, -1]]), [42, 42])\n    assert_allclose(region_map.data[[0, -1]], 42)\n\n\n@requires_data()\ndef test_region_nd_map_fill_events(region_map):\n    filename = ""$GAMMAPY_DATA/hess-dl3-dr1/data/hess_dl3_dr1_obs_id_023523.fits.gz""\n    events = EventList.read(filename)\n    region_map = Map.from_geom(region_map.geom)\n    region_map.fill_events(events)\n\n    assert_allclose(region_map.data.sum(), 665)\n\n\ndef test_apply_edisp(region_map_true):\n    e_true = region_map_true.geom.axes[0].edges\n    e_reco = MapAxis.from_energy_bounds(""1 TeV"", ""10 TeV"", nbin=3).edges\n\n    edisp = EDispKernel.from_diagonal_response(e_true=e_true, e_reco=e_reco)\n\n    m = region_map_true.apply_edisp(edisp)\n    assert m.geom.data_shape == (3, 1, 1)\n\n    e_reco = m.geom.axes[0].edges\n    assert e_reco.unit == ""TeV""\n    assert m.geom.axes[0].name == ""energy""\n    assert_allclose(e_reco[[0, -1]].value, [1, 10])\n'"
gammapy/maps/tests/test_wcs.py,19,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.coordinates import Angle, SkyCoord\nfrom astropy.io import fits\nfrom gammapy.maps import Map, MapAxis, WcsGeom\nfrom gammapy.maps.wcs import _check_width\n\naxes1 = [MapAxis(np.logspace(0.0, 3.0, 3), interp=""log"", name=""energy"")]\naxes2 = [\n    MapAxis(np.logspace(0.0, 3.0, 3), interp=""log"", name=""energy""),\n    MapAxis(np.logspace(1.0, 3.0, 4), interp=""lin""),\n]\nskydir = SkyCoord(110.0, 75.0, unit=""deg"", frame=""icrs"")\n\nwcs_allsky_test_geoms = [\n    (None, 10.0, ""galactic"", ""AIT"", skydir, None),\n    (None, 10.0, ""galactic"", ""AIT"", skydir, axes1),\n    (None, [10.0, 20.0], ""galactic"", ""AIT"", skydir, axes1),\n    (None, 10.0, ""galactic"", ""AIT"", skydir, axes2),\n    (None, [[10.0, 20.0, 30.0], [10.0, 20.0, 30.0]], ""galactic"", ""AIT"", skydir, axes2),\n]\n\nwcs_partialsky_test_geoms = [\n    (10, 0.1, ""galactic"", ""AIT"", skydir, None),\n    (10, 0.1, ""galactic"", ""AIT"", skydir, axes1),\n    (10, [0.1, 0.2], ""galactic"", ""AIT"", skydir, axes1),\n]\n\nwcs_test_geoms = wcs_allsky_test_geoms + wcs_partialsky_test_geoms\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsgeom_init(npix, binsz, frame, proj, skydir, axes):\n    WcsGeom.create(\n        npix=npix, binsz=binsz, skydir=skydir, proj=proj, frame=frame, axes=axes\n    )\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsgeom_get_pix(npix, binsz, frame, proj, skydir, axes):\n    geom = WcsGeom.create(\n        npix=npix, binsz=binsz, skydir=skydir, proj=proj, frame=frame, axes=axes\n    )\n    pix = geom.get_idx()\n    if axes is not None:\n        idx = tuple([1] * len(axes))\n        pix_img = geom.get_idx(idx=idx)\n        m = np.all(np.stack([x == y for x, y in zip(idx, pix[2:])]), axis=0)\n        m2 = pix_img[0] != -1\n        assert_allclose(pix[0][m], np.ravel(pix_img[0][m2]))\n        assert_allclose(pix[1][m], np.ravel(pix_img[1][m2]))\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsgeom_test_pix_to_coord(npix, binsz, frame, proj, skydir, axes):\n    geom = WcsGeom.create(\n        npix=npix, binsz=binsz, skydir=skydir, proj=proj, frame=frame, axes=axes\n    )\n    assert_allclose(geom.get_coord()[0], geom.pix_to_coord(geom.get_idx())[0])\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsgeom_test_coord_to_idx(npix, binsz, frame, proj, skydir, axes):\n    geom = WcsGeom.create(npix=npix, binsz=binsz, proj=proj, frame=frame, axes=axes)\n    assert_allclose(geom.get_idx()[0], geom.coord_to_idx(geom.get_coord())[0])\n\n    if not geom.is_allsky:\n        coords = geom.center_coord[:2] + tuple([ax.center[0] for ax in geom.axes])\n        coords[0][...] += 2.0 * np.max(geom.width[0])\n        idx = geom.coord_to_idx(coords)\n        assert_allclose(np.full_like(coords[0].value, -1, dtype=int), idx[0])\n        idx = geom.coord_to_idx(coords, clip=True)\n        assert np.all(\n            np.not_equal(np.full_like(coords[0].value, -1, dtype=int), idx[0])\n        )\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsgeom_read_write(tmp_path, npix, binsz, frame, proj, skydir, axes):\n    geom0 = WcsGeom.create(npix=npix, binsz=binsz, proj=proj, frame=frame, axes=axes)\n\n    hdu_bands = geom0.make_bands_hdu(hdu=""BANDS"")\n    hdu_prim = fits.PrimaryHDU()\n    hdu_prim.header.update(geom0.make_header())\n\n    hdulist = fits.HDUList([hdu_prim, hdu_bands])\n    hdulist.writeto(tmp_path / ""tmp.fits"")\n\n    with fits.open(tmp_path / ""tmp.fits"", memmap=False) as hdulist:\n        geom1 = WcsGeom.from_header(hdulist[0].header, hdulist[""BANDS""])\n\n    assert_allclose(geom0.npix, geom1.npix)\n    assert geom0.frame == geom1.frame\n\n\ndef test_wcsgeom_to_hdulist():\n    npix, binsz, frame, proj, skydir, axes = wcs_test_geoms[3]\n    geom = WcsGeom.create(npix=npix, binsz=binsz, proj=proj, frame=frame, axes=axes)\n\n    hdu = geom.make_bands_hdu(hdu=""TEST"")\n    assert hdu.header[""AXCOLS1""] == ""E_MIN,E_MAX""\n    assert hdu.header[""AXCOLS2""] == ""AXIS1_MIN,AXIS1_MAX""\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsgeom_contains(npix, binsz, frame, proj, skydir, axes):\n    geom = WcsGeom.create(\n        npix=npix, binsz=binsz, skydir=skydir, proj=proj, frame=frame, axes=axes\n    )\n    coords = geom.get_coord()\n    m = np.isfinite(coords[0])\n    coords = [c[m] for c in coords]\n    assert_allclose(geom.contains(coords), np.ones(coords[0].shape, dtype=bool))\n\n    if axes is not None:\n        coords = [c[0] for c in coords[:2]] + [ax.edges[-1] + 1.0 for ax in axes]\n        assert_allclose(geom.contains(coords), np.zeros((1,), dtype=bool))\n\n    if not geom.is_allsky:\n        coords = [0.0, 0.0] + [ax.center[0] for ax in geom.axes]\n        assert_allclose(geom.contains(coords), np.zeros((1,), dtype=bool))\n\n\ndef test_wcsgeom_solid_angle():\n    # Test using a CAR projection map with an extra axis\n    binsz = 1.0 * u.deg\n    npix = 10\n    geom = WcsGeom.create(\n        skydir=(0, 0),\n        npix=(npix, npix),\n        binsz=binsz,\n        frame=""galactic"",\n        proj=""CAR"",\n        axes=[MapAxis.from_edges([0, 2, 3])],\n    )\n\n    solid_angle = geom.solid_angle()\n\n    # Check array size\n    assert solid_angle.shape == (2, npix, npix)\n\n    # Test at b = 0 deg\n    assert solid_angle.unit == ""sr""\n    assert_allclose(solid_angle.value[0, 5, 5], 0.0003046, rtol=1e-3)\n\n    # Test at b = 5 deg\n    assert_allclose(solid_angle.value[0, 9, 5], 0.0003038, rtol=1e-3)\n\n\ndef test_wcsgeom_solid_angle_symmetry():\n    geom = WcsGeom.create(\n        skydir=(0, 0), frame=""galactic"", npix=(3, 3), binsz=20.0 * u.deg\n    )\n\n    sa = geom.solid_angle()\n\n    assert_allclose(sa[1, :], sa[1, 0])  # Constant along lon\n    assert_allclose(sa[0, 1], sa[2, 1])  # Symmetric along lat\n    with pytest.raises(AssertionError):\n        # Not constant along lat due to changes in solid angle (great circle)\n        assert_allclose(sa[:, 1], sa[0, 1])\n\n\ndef test_wcsgeom_solid_angle_ait():\n    # Pixels that don\'t correspond to locations on ths sky\n    # should have solid angles set to NaN\n    ait_geom = WcsGeom.create(\n        skydir=(0, 0), width=(360, 180), binsz=20, frame=""galactic"", proj=""AIT""\n    )\n    solid_angle = ait_geom.solid_angle().to_value(""deg2"")\n\n    assert_allclose(solid_angle[4, 1], 397.04838)\n    assert_allclose(solid_angle[4, 16], 397.751841)\n    assert_allclose(solid_angle[1, 8], 381.556269)\n    assert_allclose(solid_angle[7, 8], 398.34725)\n\n    assert np.isnan(solid_angle[0, 0])\n\n\ndef test_wcsgeom_separation():\n    geom = WcsGeom.create(\n        skydir=(0, 0),\n        npix=10,\n        binsz=0.1,\n        frame=""galactic"",\n        proj=""CAR"",\n        axes=[MapAxis.from_edges([0, 2, 3])],\n    )\n    position = SkyCoord(1, 0, unit=""deg"", frame=""galactic"").icrs\n    separation = geom.separation(position)\n\n    assert separation.unit == ""deg""\n    assert separation.shape == (10, 10)\n    assert_allclose(separation.value[0, 0], 0.7106291438079875)\n\n    # Make sure it also works for 2D maps as input\n    separation = geom.to_image().separation(position)\n    assert separation.unit == ""deg""\n    assert separation.shape == (10, 10)\n    assert_allclose(separation.value[0, 0], 0.7106291438079875)\n\n\ndef test_cutout():\n    geom = WcsGeom.create(\n        skydir=(0, 0),\n        npix=10,\n        binsz=0.1,\n        frame=""galactic"",\n        proj=""CAR"",\n        axes=[MapAxis.from_edges([0, 2, 3])],\n    )\n    position = SkyCoord(0.1, 0.2, unit=""deg"", frame=""galactic"")\n    cutout_geom = geom.cutout(position=position, width=2 * 0.3 * u.deg, mode=""trim"")\n\n    center_coord = cutout_geom.center_coord\n    assert_allclose(center_coord[0].value, 0.1)\n    assert_allclose(center_coord[1].value, 0.2)\n    assert_allclose(center_coord[2].value, 2.0)\n\n    assert cutout_geom.data_shape == (2, 6, 6)\n\n\ndef test_cutout_info():\n    geom = WcsGeom.create(skydir=(0, 0), npix=10)\n    position = SkyCoord(0, 0, unit=""deg"")\n    cutout_geom = geom.cutout(position=position, width=""2 deg"")\n    assert cutout_geom.cutout_info[""parent-slices""][0].start == 3\n    assert cutout_geom.cutout_info[""parent-slices""][1].start == 3\n\n    assert cutout_geom.cutout_info[""cutout-slices""][0].start == 0\n    assert cutout_geom.cutout_info[""cutout-slices""][1].start == 0\n\n    header = cutout_geom.make_header()\n    assert ""PSLICE1"" in header\n    assert ""PSLICE2"" in header\n    assert ""CSLICE1"" in header\n    assert ""CSLICE2"" in header\n\n    geom = WcsGeom.from_header(header)\n    assert geom.cutout_info[""parent-slices""][0].start == 3\n    assert geom.cutout_info[""parent-slices""][1].start == 3\n\n    assert geom.cutout_info[""cutout-slices""][0].start == 0\n    assert geom.cutout_info[""cutout-slices""][1].start == 0\n\n\ndef test_wcsgeom_get_coord():\n    geom = WcsGeom.create(\n        skydir=(0, 0), npix=(4, 3), binsz=1, frame=""galactic"", proj=""CAR""\n    )\n    coord = geom.get_coord(mode=""edges"")\n    assert_allclose(coord.lon[0, 0].value, 2)\n    assert coord.lon[0, 0].unit == ""deg""\n    assert_allclose(coord.lat[0, 0].value, -1.5)\n    assert coord.lat[0, 0].unit == ""deg""\n\n\ndef test_wcsgeom_instance_cache():\n    geom_1 = WcsGeom.create(npix=(3, 3))\n    geom_2 = WcsGeom.create(npix=(3, 3))\n\n    coord_1, coord_2 = geom_1.get_coord(), geom_2.get_coord()\n\n    assert geom_1.get_coord.cache_info().misses == 1\n    assert geom_2.get_coord.cache_info().misses == 1\n\n    coord_1_cached, coord_2_cached = geom_1.get_coord(), geom_2.get_coord()\n\n    assert geom_1.get_coord.cache_info().hits == 1\n    assert geom_2.get_coord.cache_info().hits == 1\n\n    assert geom_1.get_coord.cache_info().currsize == 1\n    assert geom_2.get_coord.cache_info().currsize == 1\n\n    assert id(coord_1) == id(coord_1_cached)\n    assert id(coord_2) == id(coord_2_cached)\n\n\ndef test_wcsgeom_squash():\n    axis = MapAxis.from_nodes([1, 2, 3], name=""test-axis"")\n    geom = WcsGeom.create(npix=(3, 3), axes=[axis])\n    geom_squashed = geom.squash(axis=""test-axis"")\n    assert geom_squashed.data_shape == (1, 3, 3)\n\n\ndef test_wcsgeom_drop():\n    ax1 = MapAxis.from_nodes([1, 2, 3], name=""ax1"")\n    ax2 = MapAxis.from_nodes([1, 2], name=""ax2"")\n    ax3 = MapAxis.from_nodes([1, 2, 3, 4], name=""ax2"")\n    geom = WcsGeom.create(npix=(3, 3), axes=[ax1, ax2, ax3])\n    geom_drop = geom.drop(axis=""ax1"")\n    assert geom_drop.data_shape == (4, 2, 3, 3)\n\n\ndef test_wcsgeom_get_pix_coords():\n    geom = WcsGeom.create(\n        skydir=(0, 0), npix=(4, 3), binsz=1, frame=""galactic"", proj=""CAR"", axes=axes1\n    )\n    idx_center = geom.get_pix(mode=""center"")\n\n    for idx in idx_center:\n        assert idx.shape == (2, 3, 4)\n        assert_allclose(idx[0, 0, 0], 0)\n\n    idx_edge = geom.get_pix(mode=""edges"")\n    for idx, desired in zip(idx_edge, [-0.5, -0.5, 0]):\n        assert idx.shape == (2, 4, 5)\n        assert_allclose(idx[0, 0, 0], desired)\n\n\ndef test_geom_repr():\n    geom = WcsGeom.create(\n        skydir=(0, 0), npix=(10, 4), binsz=50, frame=""galactic"", proj=""AIT""\n    )\n    assert geom.__class__.__name__ in repr(geom)\n\n\ndef test_geom_refpix():\n    refpix = (400, 300)\n    geom = WcsGeom.create(\n        skydir=(0, 0), npix=(800, 600), refpix=refpix, binsz=0.1, frame=""galactic""\n    )\n    assert_allclose(geom.wcs.wcs.crpix, refpix)\n\n\ndef test_region_mask():\n    from regions import CircleSkyRegion\n\n    geom = WcsGeom.create(npix=(3, 3), binsz=2, proj=""CAR"")\n\n    r1 = CircleSkyRegion(SkyCoord(0, 0, unit=""deg""), 1 * u.deg)\n    r2 = CircleSkyRegion(SkyCoord(20, 20, unit=""deg""), 1 * u.deg)\n    regions = [r1, r2]\n\n    mask = geom.region_mask(regions)  # default inside=True\n    assert mask.dtype == bool\n    assert np.sum(mask) == 1\n\n    mask = geom.region_mask(regions, inside=False)\n    assert np.sum(mask) == 8\n\n\ndef test_energy_mask():\n    energy_axis = MapAxis.from_nodes(\n        [1, 10, 100], interp=""log"", name=""energy"", unit=""TeV""\n    )\n    geom = WcsGeom.create(npix=(1, 1), binsz=1, proj=""CAR"", axes=[energy_axis])\n\n    mask = geom.energy_mask(emin=3 * u.TeV)\n    assert not mask[0, 0, 0]\n    assert mask[1, 0, 0]\n    assert mask[2, 0, 0]\n\n    mask = geom.energy_mask(emax=30 * u.TeV)\n    assert mask[0, 0, 0]\n    assert not mask[1, 0, 0]\n    assert not mask[2, 0, 0]\n\n    mask = geom.energy_mask(emin=3 * u.TeV, emax=40 * u.TeV)\n    assert not mask[0, 0, 0]\n    assert not mask[2, 0, 0]\n    assert mask[1, 0, 0]\n\n\n@pytest.mark.parametrize(\n    (""width"", ""out""),\n    [\n        (10, (10, 10)),\n        ((10 * u.deg).to(""rad""), (10, 10)),\n        ((10, 5), (10, 5)),\n        ((""10 deg"", ""5 deg""), (10, 5)),\n        (Angle([10, 5], ""deg""), (10, 5)),\n        ((10 * u.deg, 5 * u.deg), (10, 5)),\n        ((10, 5) * u.deg, (10, 5)),\n        ([10, 5], (10, 5)),\n        ([""10 deg"", ""5 deg""], (10, 5)),\n        (np.array([10, 5]), (10, 5)),\n    ],\n)\ndef test_check_width(width, out):\n    width = _check_width(width)\n    assert isinstance(width, tuple)\n    assert isinstance(width[0], float)\n    assert isinstance(width[1], float)\n    assert width == out\n\n    geom = WcsGeom.create(width=width, binsz=1.0)\n    assert tuple(geom.npix) == out\n\n\ndef test_check_width_bad_input():\n    with pytest.raises(IndexError):\n        _check_width(width=(10,))\n\n\ndef test_get_axis_index_by_name():\n    e_axis = MapAxis.from_edges([1, 5], name=""energy"")\n    geom = WcsGeom.create(width=5, binsz=1.0, axes=[e_axis])\n    assert geom.get_axis_index_by_name(""Energy"") == 0\n    with pytest.raises(ValueError):\n        geom.get_axis_index_by_name(""time"")\n\n\ntest_axis1 = [MapAxis(nodes=(1, 2, 3, 4), unit=""TeV"", node_type=""center"")]\ntest_axis2 = [\n    MapAxis(nodes=(1, 2, 3, 4), unit=""TeV"", node_type=""center""),\n    MapAxis(nodes=(1, 2, 3), unit=""TeV"", node_type=""center""),\n]\n\nskydir2 = SkyCoord(110.0, 75.0 + 1e-8, unit=""deg"", frame=""icrs"")\nskydir3 = SkyCoord(110.0, 75.0 + 1e-3, unit=""deg"", frame=""icrs"")\n\ncompatibility_test_geoms = [\n    (10, 0.1, ""galactic"", ""CAR"", skydir, test_axis1, True),\n    (10, 0.1, ""galactic"", ""CAR"", skydir2, test_axis1, True),\n    (10, 0.1, ""galactic"", ""CAR"", skydir3, test_axis1, False),\n    (10, 0.1, ""galactic"", ""TAN"", skydir, test_axis1, False),\n    (8, 0.1, ""galactic"", ""CAR"", skydir, test_axis1, False),\n    (10, 0.1, ""galactic"", ""CAR"", skydir, test_axis2, False),\n    (10, 0.1, ""galactic"", ""CAR"", skydir.galactic, test_axis1, True),\n]\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skypos"", ""axes"", ""result""),\n    compatibility_test_geoms,\n)\ndef test_wcs_geom_equal(npix, binsz, frame, proj, skypos, axes, result):\n    geom0 = WcsGeom.create(\n        skydir=skydir, npix=10, binsz=0.1, proj=""CAR"", frame=""galactic"", axes=test_axis1\n    )\n    geom1 = WcsGeom.create(\n        skydir=skypos, npix=npix, binsz=binsz, proj=proj, frame=frame, axes=axes\n    )\n\n    assert (geom0 == geom1) is result\n    assert (geom0 != geom1) is not result\n\ndef test_irregular_geom_equality():\n    axis = MapAxis.from_bounds(1,3,10, name=""axis"", unit="""")\n    geom0 = WcsGeom.create(skydir=(0,0), npix=10, binsz=0.1, axes=[axis])\n    binsizes = np.ones((10))*0.1\n    geom1 = WcsGeom.create(skydir=(0,0), npix=10, binsz=binsizes, axes=[axis])\n\n    with pytest.raises(NotImplementedError):\n        geom0 == geom1\n\n@pytest.mark.parametrize(""node_type"", [""edges"", ""center""])\n@pytest.mark.parametrize(""interp"", [""log"", ""lin"", ""sqrt""])\ndef test_read_write(tmp_path, node_type, interp):\n    # Regression test for MapAxis interp and node_type FITS serialization\n    # https://github.com/gammapy/gammapy/issues/1887\n    e_ax = MapAxis([1, 2], interp, ""energy"", node_type, ""TeV"")\n    t_ax = MapAxis([3, 4], interp, ""time"", node_type, ""s"")\n    m = Map.create(binsz=1, npix=10, axes=[e_ax, t_ax], unit=""m2"")\n\n    # Check what Gammapy writes in the FITS header\n    header = m.make_hdu().header\n    assert header[""INTERP1""] == interp\n    assert header[""INTERP2""] == interp\n\n    # Check that all MapAxis properties are preserved on FITS I/O\n    m.write(tmp_path / ""tmp.fits"", overwrite=True)\n    m2 = Map.read(tmp_path / ""tmp.fits"")\n    assert m2.geom == m.geom\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skypos"", ""axes"", ""result""),\n    compatibility_test_geoms,\n)\ndef test_wcs_geom_to_binsz(npix, binsz, frame, proj, skypos, axes, result):\n    geom = WcsGeom.create(\n        skydir=skydir, npix=10, binsz=0.1, proj=""CAR"", frame=""galactic"", axes=test_axis1\n    )\n\n    geom_new = geom.to_binsz(binsz=0.5)\n\n    assert_allclose(geom_new.pixel_scales.value, 0.5)\n'"
gammapy/maps/tests/test_wcsnd.py,28,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose, assert_equal\nimport astropy.units as u\nfrom astropy.convolution import Box2DKernel, Gaussian2DKernel\nfrom astropy.coordinates import SkyCoord\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom regions import CircleSkyRegion\nfrom gammapy.datasets.map import MapEvaluator\nfrom gammapy.irf import EnergyDependentMultiGaussPSF, PSFKernel\nfrom gammapy.maps import Map, MapAxis, MapCoord, WcsGeom, WcsNDMap\nfrom gammapy.maps.utils import fill_poisson\nfrom gammapy.modeling.models import (\n    GaussianSpatialModel,\n    PowerLawSpectralModel,\n    SkyModel,\n)\nfrom gammapy.utils.testing import mpl_plot_check, requires_data, requires_dependency\n\naxes1 = [MapAxis(np.logspace(0.0, 3.0, 3), interp=""log"", name=""spam"")]\naxes2 = [\n    MapAxis(np.logspace(0.0, 3.0, 3), interp=""log""),\n    MapAxis(np.logspace(1.0, 3.0, 4), interp=""lin""),\n]\nskydir = SkyCoord(110.0, 75.0, unit=""deg"", frame=""icrs"")\n\nwcs_allsky_test_geoms = [\n    (None, 10.0, ""galactic"", ""AIT"", skydir, None),\n    (None, 10.0, ""galactic"", ""AIT"", skydir, axes1),\n    (None, [10.0, 20.0], ""galactic"", ""AIT"", skydir, axes1),\n    (None, 10.0, ""galactic"", ""AIT"", skydir, axes2),\n    (None, [[10.0, 20.0, 30.0], [10.0, 20.0, 30.0]], ""galactic"", ""AIT"", skydir, axes2),\n]\n\nwcs_partialsky_test_geoms = [\n    (10, 1.0, ""galactic"", ""AIT"", skydir, None),\n    (10, 1.0, ""galactic"", ""AIT"", skydir, axes1),\n    (10, [1.0, 2.0], ""galactic"", ""AIT"", skydir, axes1),\n    (10, 1.0, ""galactic"", ""AIT"", skydir, axes2),\n    (10, [[1.0, 2.0, 3.0], [1.0, 2.0, 3.0]], ""galactic"", ""AIT"", skydir, axes2),\n]\n\nwcs_test_geoms = wcs_allsky_test_geoms + wcs_partialsky_test_geoms\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsndmap_init(npix, binsz, frame, proj, skydir, axes):\n    geom = WcsGeom.create(npix=npix, binsz=binsz, proj=proj, frame=frame, axes=axes)\n    m0 = WcsNDMap(geom)\n    coords = m0.geom.get_coord()\n    m0.set_by_coord(coords, coords[1])\n    m1 = WcsNDMap(geom, m0.data)\n    assert_allclose(m0.data, m1.data)\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsndmap_read_write(tmp_path, npix, binsz, frame, proj, skydir, axes):\n    geom = WcsGeom.create(npix=npix, binsz=binsz, proj=proj, frame=frame, axes=axes)\n    path = tmp_path / ""tmp.fits""\n\n    m0 = WcsNDMap(geom)\n    fill_poisson(m0, mu=0.5)\n    m0.write(path, overwrite=True)\n    m1 = WcsNDMap.read(path)\n    m2 = Map.read(path)\n    m3 = Map.read(path, map_type=""wcs"")\n    assert_allclose(m0.data, m1.data)\n    assert_allclose(m0.data, m2.data)\n    assert_allclose(m0.data, m3.data)\n\n    m0.write(path, sparse=True, overwrite=True)\n    m1 = WcsNDMap.read(path)\n    m2 = Map.read(path)\n    m3 = Map.read(path, map_type=""wcs"")\n    assert_allclose(m0.data, m1.data)\n    assert_allclose(m0.data, m2.data)\n    assert_allclose(m0.data, m3.data)\n\n    # Specify alternate HDU name for IMAGE and BANDS table\n    m0.write(path, hdu=""IMAGE"", hdu_bands=""TEST"", overwrite=True)\n    m1 = WcsNDMap.read(path)\n    m2 = Map.read(path)\n    m3 = Map.read(path, map_type=""wcs"")\n\n\ndef test_wcsndmap_read_write_fgst(tmp_path):\n    path = tmp_path / ""tmp.fits""\n\n    axis = MapAxis.from_bounds(100.0, 1000.0, 4, name=""energy"", unit=""MeV"")\n    geom = WcsGeom.create(npix=10, binsz=1.0, proj=""AIT"", frame=""galactic"", axes=[axis])\n\n    # Test Counts Cube\n    m = WcsNDMap(geom)\n    m.write(path, conv=""fgst-ccube"", overwrite=True)\n    with fits.open(path, memmap=False) as hdulist:\n        assert ""EBOUNDS"" in hdulist\n\n    m2 = Map.read(path)\n    assert m2.geom.axes[0].name == ""energy""\n\n    # Test Model Cube\n    m.write(path, conv=""fgst-template"", overwrite=True)\n    with fits.open(path, memmap=False) as hdulist:\n        assert ""ENERGIES"" in hdulist\n\n\n@requires_data()\ndef test_wcsndmap_read_ccube():\n    counts = Map.read(""$GAMMAPY_DATA/fermi-3fhl-gc/fermi-3fhl-gc-counts-cube.fits.gz"")\n    energy_axis = counts.geom.get_axis_by_name(""energy"")\n    # for the 3FGL data the lower energy threshold should be at 10 GeV\n    assert_allclose(energy_axis.edges.min().to_value(""GeV""), 10, rtol=1e-3)\n\n\ndef test_wcs_nd_map_data_transpose_issue(tmp_path):\n    # Regression test for https://github.com/gammapy/gammapy/issues/1346\n\n    # Our test case: a little map with WCS shape (3, 2), i.e. numpy array shape (2, 3)\n    data = np.array([[0, 1, 2], [np.nan, np.inf, -np.inf]])\n    geom = WcsGeom.create(npix=(3, 2))\n\n    # Data should be unmodified after init\n    m = WcsNDMap(data=data, geom=geom)\n    assert_equal(m.data, data)\n\n    # Data should be unmodified if initialised like this\n    m = WcsNDMap(geom=geom)\n    # and then filled via an in-place Numpy array operation\n    m.data += data\n    assert_equal(m.data, data)\n\n    # Data should be unmodified after write / read to normal image format\n    m.write(tmp_path / ""normal.fits.gz"")\n    m2 = Map.read(tmp_path / ""normal.fits.gz"")\n    assert_equal(m2.data, data)\n\n    # Data should be unmodified after write / read to sparse image format\n    m.write(tmp_path / ""sparse.fits.gz"")\n    m2 = Map.read(tmp_path / ""sparse.fits.gz"")\n    assert_equal(m2.data, data)\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsndmap_set_get_by_pix(npix, binsz, frame, proj, skydir, axes):\n    geom = WcsGeom.create(\n        npix=npix, binsz=binsz, skydir=skydir, proj=proj, frame=frame, axes=axes\n    )\n    m = WcsNDMap(geom)\n    coords = m.geom.get_coord()\n    pix = m.geom.get_idx()\n    m.set_by_pix(pix, coords[0])\n    assert_allclose(coords[0].value, m.get_by_pix(pix))\n\n\ndef test_get_by_coord_bool_int():\n    mask = WcsNDMap.create(width=2, dtype=""bool"")\n    coords = {""lon"": [0, 3], ""lat"": [0, 3]}\n    vals = mask.get_by_coord(coords)\n    assert_allclose(vals, [0, np.nan])\n\n    mask = WcsNDMap.create(width=2, dtype=""int"")\n    coords = {""lon"": [0, 3], ""lat"": [0, 3]}\n    vals = mask.get_by_coord(coords)\n    assert_allclose(vals, [0, np.nan])\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsndmap_set_get_by_coord(npix, binsz, frame, proj, skydir, axes):\n    geom = WcsGeom.create(\n        npix=npix, binsz=binsz, skydir=skydir, proj=proj, frame=frame, axes=axes\n    )\n    m = WcsNDMap(geom)\n    coords = m.geom.get_coord()\n    m.set_by_coord(coords, coords[0])\n    assert_allclose(coords[0].value, m.get_by_coord(coords))\n\n    # Test with SkyCoords\n    m = WcsNDMap(geom)\n    coords = m.geom.get_coord()\n    skydir = coords.skycoord\n    skydir_cel = skydir.transform_to(""icrs"")\n    skydir_gal = skydir.transform_to(""galactic"")\n\n    m.set_by_coord((skydir_gal,) + tuple(coords[2:]), coords[0])\n    assert_allclose(coords[0].value, m.get_by_coord(coords))\n    assert_allclose(\n        m.get_by_coord((skydir_cel,) + tuple(coords[2:])),\n        m.get_by_coord((skydir_gal,) + tuple(coords[2:])),\n    )\n\n    # Test with MapCoord\n    m = WcsNDMap(geom)\n    coords = m.geom.get_coord()\n    coords_dict = dict(lon=coords[0], lat=coords[1])\n    if axes:\n        for i, ax in enumerate(axes):\n            coords_dict[ax.name] = coords[i + 2]\n    map_coords = MapCoord.create(coords_dict, frame=frame)\n    m.set_by_coord(map_coords, coords[0])\n    assert_allclose(coords[0].value, m.get_by_coord(map_coords))\n\n\ndef test_set_get_by_coord_quantities():\n    ax = MapAxis(np.logspace(0.0, 3.0, 3), interp=""log"", name=""energy"", unit=""TeV"")\n    geom = WcsGeom.create(binsz=0.1, npix=(3, 4), axes=[ax])\n    m = WcsNDMap(geom)\n    coords_dict = {""lon"": 0, ""lat"": 0, ""energy"": 1000 * u.GeV}\n\n    m.set_by_coord(coords_dict, 42)\n\n    coords_dict[""energy""] = 1 * u.TeV\n    assert_allclose(42, m.get_by_coord(coords_dict))\n\n\ndef qconcatenate(q_1, q_2):\n    """"""Concatenate quantity""""""\n    return u.Quantity(np.concatenate((q_1.value, q_2.value)), unit=q_1.unit)\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsndmap_fill_by_coord(npix, binsz, frame, proj, skydir, axes):\n    geom = WcsGeom.create(\n        npix=npix, binsz=binsz, skydir=skydir, proj=proj, frame=frame, axes=axes\n    )\n    m = WcsNDMap(geom)\n    coords = m.geom.get_coord()\n    fill_coords = tuple([qconcatenate(t, t) for t in coords])\n\n    fill_vals = fill_coords[1]\n    m.fill_by_coord(fill_coords, fill_vals.value)\n    assert_allclose(m.get_by_coord(coords), 2.0 * coords[1].value)\n\n    # Test with SkyCoords\n    m = WcsNDMap(geom)\n    coords = m.geom.get_coord()\n    skydir = coords.skycoord\n    skydir_cel = skydir.transform_to(""icrs"")\n    skydir_gal = skydir.transform_to(""galactic"")\n    fill_coords_cel = (skydir_cel,) + tuple(coords[2:])\n    fill_coords_gal = (skydir_gal,) + tuple(coords[2:])\n    m.fill_by_coord(fill_coords_cel, coords[1].value)\n    m.fill_by_coord(fill_coords_gal, coords[1].value)\n    assert_allclose(m.get_by_coord(coords), 2.0 * coords[1].value)\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsndmap_coadd(npix, binsz, frame, proj, skydir, axes):\n    geom = WcsGeom.create(\n        npix=npix, binsz=binsz, skydir=skydir, proj=proj, frame=frame, axes=axes\n    )\n    m0 = WcsNDMap(geom)\n    m1 = WcsNDMap(geom.upsample(2))\n    coords = m0.geom.get_coord()\n    m1.fill_by_coord(\n        tuple([qconcatenate(t, t) for t in coords]),\n        qconcatenate(coords[1], coords[1]).value,\n    )\n    m0.coadd(m1)\n    assert_allclose(np.nansum(m0.data), np.nansum(m1.data), rtol=1e-4)\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsndmap_interp_by_coord(npix, binsz, frame, proj, skydir, axes):\n    geom = WcsGeom.create(\n        npix=npix, binsz=binsz, skydir=skydir, proj=proj, frame=frame, axes=axes\n    )\n    m = WcsNDMap(geom)\n    coords = m.geom.get_coord(flat=True)\n    m.set_by_coord(coords, coords[1].value)\n    assert_allclose(coords[1].value, m.interp_by_coord(coords, interp=""nearest""))\n    assert_allclose(coords[1].value, m.interp_by_coord(coords, interp=""linear""))\n    assert_allclose(coords[1].value, m.interp_by_coord(coords, interp=1))\n    if geom.is_regular and not geom.is_allsky:\n        assert_allclose(\n            coords[1].to_value(""deg""), m.interp_by_coord(coords, interp=""cubic"")\n        )\n\n\ndef test_interp_by_coord_quantities():\n    ax = MapAxis(\n        np.logspace(0.0, 3.0, 3),\n        interp=""log"",\n        name=""energy"",\n        unit=""TeV"",\n        node_type=""center"",\n    )\n    geom = WcsGeom.create(binsz=0.1, npix=(3, 3), axes=[ax])\n    m = WcsNDMap(geom)\n    coords_dict = {""lon"": 0, ""lat"": 0, ""energy"": 1000 * u.GeV}\n\n    m.set_by_coord(coords_dict, 42)\n\n    coords_dict[""energy""] = 1 * u.TeV\n    assert_allclose(42.0, m.interp_by_coord(coords_dict, interp=""nearest""))\n\n\ndef test_wcsndmap_interp_by_coord_fill_value():\n    # Introduced in https://github.com/gammapy/gammapy/pull/1559/files\n    m = Map.create(npix=(20, 10))\n    m.data += 42\n    # With `fill_value` one should be able to control what gets filled\n    assert_allclose(m.interp_by_coord((99, 0), fill_value=99), 99)\n    # Default is to extrapolate\n    assert_allclose(m.interp_by_coord((99, 0)), 42)\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\n@pytest.mark.parametrize(""keepdims"", [True, False])\ndef test_wcsndmap_sum_over_axes(npix, binsz, frame, proj, skydir, axes, keepdims):\n    geom = WcsGeom.create(npix=npix, binsz=binsz, proj=proj, frame=frame, axes=axes)\n    m = WcsNDMap(geom)\n    coords = m.geom.get_coord()\n    m.fill_by_coord(coords, coords[0].value)\n    msum = m.sum_over_axes(keepdims=keepdims)\n\n    if m.geom.is_regular:\n        assert_allclose(np.nansum(m.data), np.nansum(msum.data))\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsndmap_pad(npix, binsz, frame, proj, skydir, axes):\n    geom = WcsGeom.create(npix=npix, binsz=binsz, proj=proj, frame=frame, axes=axes)\n    m = WcsNDMap(geom)\n    m2 = m.pad(1, mode=""constant"", cval=2.2)\n    if not geom.is_allsky:\n        coords = m2.geom.get_coord()\n        msk = m2.geom.contains(coords)\n        coords = tuple([c[~msk] for c in coords])\n        assert_allclose(m2.get_by_coord(coords), 2.2)\n    m.pad(1, mode=""interp"", order=0)\n    m.pad(1, mode=""interp"")\n\n\ndef test_wcsndmap_pad_cval():\n    geom = WcsGeom.create(npix=(5, 5))\n    m = WcsNDMap.from_geom(geom)\n\n    cval = 1.1\n    m_padded = m.pad(1, mode=""constant"", cval=cval)\n    assert_allclose(m_padded.data[0, 0], cval)\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsndmap_crop(npix, binsz, frame, proj, skydir, axes):\n    geom = WcsGeom.create(npix=npix, binsz=binsz, proj=proj, frame=frame, axes=axes)\n    m = WcsNDMap(geom)\n    m.crop(1)\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsndmap_downsample(npix, binsz, frame, proj, skydir, axes):\n    geom = WcsGeom.create(npix=npix, binsz=binsz, proj=proj, frame=frame, axes=axes)\n    m = WcsNDMap(geom, unit=""m2"")\n    # Check whether we can downsample\n    if np.all(np.mod(geom.npix[0], 2) == 0) and np.all(np.mod(geom.npix[1], 2) == 0):\n        m2 = m.downsample(2, preserve_counts=True)\n        assert_allclose(np.nansum(m.data), np.nansum(m2.data))\n        assert m.unit == m2.unit\n\n\n@pytest.mark.parametrize(\n    (""npix"", ""binsz"", ""frame"", ""proj"", ""skydir"", ""axes""), wcs_test_geoms\n)\ndef test_wcsndmap_upsample(npix, binsz, frame, proj, skydir, axes):\n    geom = WcsGeom.create(npix=npix, binsz=binsz, proj=proj, frame=frame, axes=axes)\n    m = WcsNDMap(geom, unit=""m2"")\n    m2 = m.upsample(2, preserve_counts=True)\n    assert_allclose(np.nansum(m.data), np.nansum(m2.data))\n    assert m.unit == m2.unit\n\n\ndef test_wcsndmap_upsample_axis():\n    axis = MapAxis.from_nodes([1, 2, 3, 4], name=""test"")\n    geom = WcsGeom.create(npix=(4, 4), axes=[axis])\n    m = WcsNDMap(geom, unit=""m2"")\n    m.data += 1\n\n    m2 = m.upsample(2, preserve_counts=True, axis=""test"")\n    assert m2.data.shape == (8, 4, 4)\n    assert_allclose(m.data.sum(), m2.data.sum())\n\n\ndef test_wcsndmap_downsample_axis():\n    axis = MapAxis.from_nodes([1, 2, 3, 4], name=""test"")\n    geom = WcsGeom.create(npix=(4, 4), axes=[axis])\n    m = WcsNDMap(geom, unit=""m2"")\n    m.data += 1\n\n    m2 = m.downsample(2, preserve_counts=True, axis=""test"")\n    assert m2.data.shape == (2, 4, 4)\n\n\ndef test_coadd_unit():\n    geom = WcsGeom.create(npix=(10, 10), binsz=1, proj=""CAR"", frame=""galactic"")\n    m1 = WcsNDMap(geom, data=np.ones((10, 10)), unit=""m2"")\n    m2 = WcsNDMap(geom, data=np.ones((10, 10)), unit=""cm2"")\n\n    m1.coadd(m2)\n\n    assert_allclose(m1.data, 1.0001)\n\n\n@pytest.mark.parametrize(""kernel"", [""gauss"", ""box"", ""disk""])\ndef test_smooth(kernel):\n    axes = [\n        MapAxis(np.logspace(0.0, 3.0, 3), interp=""log""),\n        MapAxis(np.logspace(1.0, 3.0, 4), interp=""lin""),\n    ]\n    geom = WcsGeom.create(\n        npix=(10, 10), binsz=1, proj=""CAR"", frame=""galactic"", axes=axes\n    )\n    m = WcsNDMap(geom, data=np.ones(geom.data_shape), unit=""m2"")\n\n    desired = m.data.sum()\n    smoothed = m.smooth(0.2 * u.deg, kernel)\n    actual = smoothed.data.sum()\n    assert_allclose(actual, desired)\n    assert smoothed.data.dtype == float\n\n\n@pytest.mark.parametrize(""mode"", [""partial"", ""strict"", ""trim""])\ndef test_make_cutout(mode):\n    pos = SkyCoord(0, 0, unit=""deg"", frame=""galactic"")\n    geom = WcsGeom.create(\n        npix=(10, 10), binsz=1, skydir=pos, proj=""CAR"", frame=""galactic"", axes=axes2\n    )\n    m = WcsNDMap(geom, data=np.ones((3, 2, 10, 10)), unit=""m2"")\n    cutout = m.cutout(position=pos, width=(2.0, 3.0) * u.deg, mode=mode)\n    actual = cutout.data.sum()\n    assert_allclose(actual, 36.0)\n    assert_allclose(cutout.geom.shape_axes, m.geom.shape_axes)\n    assert_allclose(cutout.geom.width.to_value(""deg""), [[2.0], [3.0]])\n\n\ndef test_convolve_vs_smooth():\n    axes = [\n        MapAxis(np.logspace(0.0, 3.0, 3), interp=""log""),\n        MapAxis(np.logspace(1.0, 3.0, 4), interp=""lin""),\n    ]\n\n    binsz = 0.05 * u.deg\n    m = WcsNDMap.create(binsz=binsz, width=1.05 * u.deg, axes=axes)\n    m.data[:, :, 10, 10] = 1.0\n\n    desired = m.smooth(kernel=""gauss"", width=0.5 * u.deg, mode=""constant"")\n    gauss = Gaussian2DKernel(10).array\n    actual = m.convolve(kernel=gauss)\n    assert_allclose(actual.data, desired.data, rtol=1e-3)\n\n\n@requires_data()\ndef test_convolve_nd():\n    energy_axis = MapAxis.from_edges(\n        np.logspace(-1.0, 1.0, 4), unit=""TeV"", name=""energy_true""\n    )\n    geom = WcsGeom.create(binsz=0.02 * u.deg, width=4.0 * u.deg, axes=[energy_axis])\n    m = Map.from_geom(geom)\n    m.fill_by_coord([[0.2, 0.4], [-0.1, 0.6], [0.5, 3.6]])\n\n    # TODO : build EnergyDependentTablePSF programmatically rather than using CTA 1DC IRF\n    filename = (\n        ""$GAMMAPY_DATA/cta-1dc/caldb/data/cta//1dc/bcf/South_z20_50h/irf_file.fits""\n    )\n    psf = EnergyDependentMultiGaussPSF.read(filename, hdu=""POINT SPREAD FUNCTION"")\n    table_psf = psf.to_energy_dependent_table_psf(theta=0.5 * u.deg)\n\n    psf_kernel = PSFKernel.from_table_psf(table_psf, geom, max_radius=1 * u.deg)\n\n    assert psf_kernel.psf_kernel_map.data.shape == (3, 101, 101)\n\n    mc = m.convolve(psf_kernel)\n    assert_allclose(mc.data.sum(axis=(1, 2)), [0, 1, 1], atol=1e-5)\n\n    kernel_2d = Box2DKernel(3, mode=""center"")\n    kernel_2d.normalize(""peak"")\n    mc = m.convolve(kernel_2d.array)\n    assert_allclose(mc.data[0, :, :].sum(), 0, atol=1e-5)\n    assert_allclose(mc.data[1, :, :].sum(), 9, atol=1e-5)\n\n\ndef test_convolve_pixel_scale_error():\n    m = WcsNDMap.create(binsz=0.05 * u.deg, width=5 * u.deg)\n    kgeom = WcsGeom.create(binsz=0.04 * u.deg, width=0.5 * u.deg)\n\n    kernel = PSFKernel.from_gauss(kgeom, sigma=0.1 * u.deg, max_radius=1.5 * u.deg)\n\n    with pytest.raises(ValueError):\n        m.convolve(kernel)\n\n\n@requires_dependency(""matplotlib"")\ndef test_plot():\n    m = WcsNDMap.create(binsz=0.1 * u.deg, width=1 * u.deg)\n    with mpl_plot_check():\n        m.plot(add_cbar=True)\n\n\n@requires_dependency(""matplotlib"")\ndef test_plot_allsky():\n    m = WcsNDMap.create(binsz=10 * u.deg)\n    with mpl_plot_check():\n        m.plot()\n\n\ndef test_get_spectrum():\n    axis = MapAxis.from_bounds(1, 10, nbin=3, unit=""TeV"", name=""energy"")\n\n    geom = WcsGeom.create(\n        skydir=(0, 0), width=(2.5, 2.5), binsz=0.5, axes=[axis], frame=""galactic""\n    )\n\n    m = Map.from_geom(geom)\n    m.data += 1\n\n    center = SkyCoord(0, 0, frame=""galactic"", unit=""deg"")\n    region = CircleSkyRegion(center=center, radius=1 * u.deg)\n\n    spec = m.get_spectrum(region=region)\n    assert_allclose(spec.data.squeeze(), [13.0, 13.0, 13.0])\n\n    spec = m.get_spectrum(region=region, func=np.mean)\n    assert_allclose(spec.data.squeeze(), [1.0, 1.0, 1.0])\n\n\ndef get_npred_map():\n    position = SkyCoord(0.0, 0.0, frame=""galactic"", unit=""deg"")\n    energy_axis = MapAxis.from_bounds(\n        1, 100, nbin=30, unit=""TeV"", name=""energy_true"", interp=""log""\n    )\n\n    exposure = Map.create(\n        binsz=0.02,\n        map_type=""wcs"",\n        skydir=position,\n        width=""2 deg"",\n        axes=[energy_axis],\n        frame=""galactic"",\n        unit=""cm2 s"",\n    )\n\n    spatial_model = GaussianSpatialModel(\n        lon_0=""0 deg"", lat_0=""0 deg"", sigma=""0.2 deg"", frame=""galactic""\n    )\n    spectral_model = PowerLawSpectralModel(amplitude=""1e-11 cm-2 s-1 TeV-1"")\n    skymodel = SkyModel(spatial_model=spatial_model, spectral_model=spectral_model)\n\n    exposure.data = 1e14 * np.ones(exposure.data.shape)\n    evaluator = MapEvaluator(model=skymodel, exposure=exposure)\n\n    npred = evaluator.compute_npred()\n    return evaluator, npred\n\n\ndef test_map_sampling():\n    eval, npred = get_npred_map()\n\n    nmap = WcsNDMap(geom=eval.geom, data=npred.data)\n    coords = nmap.sample_coord(n_events=2, random_state=0)\n    skycoord = coords.skycoord\n\n    events = Table()\n    events[""RA_TRUE""] = skycoord.icrs.ra\n    events[""DEC_TRUE""] = skycoord.icrs.dec\n    events[""ENERGY_TRUE""] = coords[""energy_true""]\n\n    assert len(events) == 2\n    assert_allclose(events[""RA_TRUE""].data, [266.307081, 266.442255], rtol=1e-5)\n    assert_allclose(events[""DEC_TRUE""].data, [-28.753408, -28.742696], rtol=1e-5)\n    assert_allclose(events[""ENERGY_TRUE""].data, [2.755397, 1.72316], rtol=1e-5)\n\n    assert coords[""lon""].unit == ""deg""\n    assert coords[""lat""].unit == ""deg""\n    assert coords[""energy_true""].unit == ""TeV""\n\n\ndef test_map_interp_one_bin():\n    m = WcsNDMap.create(npix=(2, 1))\n    m.data = np.array([[1, 2]])\n\n    coords = {""lon"": 0, ""lat"": [0, 0]}\n    data = m.interp_by_coord(coords)\n\n    assert data.shape == (2,)\n    assert_allclose(data, 1.5)\n\n\ndef test_sum_over_axes():\n    # Check summing over a specific axis\n    ax1 = MapAxis.from_nodes([1, 2, 3, 4], name=""ax1"")\n    ax2 = MapAxis.from_nodes([5, 6, 7], name=""ax2"")\n    ax3 = MapAxis.from_nodes([8, 9], name=""ax3"")\n    geom = WcsGeom.create(npix=(5, 5), axes=[ax1, ax2, ax3])\n    m1 = Map.from_geom(geom=geom)\n    m1.data = np.ones(m1.data.shape)\n    m2 = m1.sum_over_axes(axes=[""ax1"", ""ax3""], keepdims=True)\n\n    assert_allclose(m2.geom.data_shape, (1, 3, 1, 5, 5))\n    assert_allclose(m2.data[0][0][0][0][0], 8.0)\n\n    m3 = m1.sum_over_axes(axes=[""ax3"", ""ax2""], keepdims=False)\n    assert_allclose(m3.geom.data_shape, (4, 5, 5))\n    assert_allclose(m3.data[0][0][0], 6.0)\n\n\ndef test_reduce():\n    # Check summing over a specific axis\n    ax1 = MapAxis.from_nodes([1, 2, 3, 4], name=""ax1"")\n    ax2 = MapAxis.from_nodes([5, 6, 7], name=""ax2"")\n    ax3 = MapAxis.from_nodes([8, 9], name=""ax3"")\n    geom = WcsGeom.create(npix=(5, 5), axes=[ax1, ax2, ax3])\n    m1 = Map.from_geom(geom=geom)\n    m1.data = np.ones(m1.data.shape)\n    m2 = m1.reduce(axis=""ax1"", keepdims=True)\n\n    assert_allclose(m2.geom.data_shape, (2, 3, 1, 5, 5))\n    assert_allclose(m2.data[0][0][0][0][0], 4.0)\n\n    m3 = m1.reduce(axis=""ax1"", keepdims=False)\n    assert_allclose(m3.geom.data_shape, (2, 3, 5, 5))\n    assert_allclose(m3.data[0][0][0][0], 4.0)\n'"
gammapy/modeling/models/__init__.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Built-in models in Gammapy.""""""\nfrom gammapy.utils.registry import Registry\nfrom .core import *\nfrom .cube import *\nfrom .spatial import *\nfrom .spectral import *\nfrom .spectral_cosmic_ray import *\nfrom .spectral_crab import *\nfrom .temporal import *\n\nSPATIAL_MODELS = Registry(\n    [\n        ConstantSpatialModel,\n        TemplateSpatialModel,\n        DiskSpatialModel,\n        GaussianSpatialModel,\n        PointSpatialModel,\n        ShellSpatialModel,\n    ]\n)\n""""""Built-in spatial models.""""""\n\nSPECTRAL_MODELS = Registry(\n    [\n        ConstantSpectralModel,\n        CompoundSpectralModel,\n        PowerLawSpectralModel,\n        PowerLaw2SpectralModel,\n        SmoothBrokenPowerLawSpectralModel,\n        ExpCutoffPowerLawSpectralModel,\n        ExpCutoffPowerLaw3FGLSpectralModel,\n        SuperExpCutoffPowerLaw3FGLSpectralModel,\n        SuperExpCutoffPowerLaw4FGLSpectralModel,\n        LogParabolaSpectralModel,\n        TemplateSpectralModel,\n        GaussianSpectralModel,\n        AbsorbedSpectralModel,\n        NaimaSpectralModel,\n        ScaleSpectralModel,\n    ]\n)\n""""""Built-in spectral models.""""""\n\nTEMPORAL_MODELS = Registry([ConstantTemporalModel, LightCurveTemplateTemporalModel,])\n""""""Built-in temporal models.""""""\n\nMODELS = Registry(\n    SPATIAL_MODELS\n    + SPECTRAL_MODELS\n    + TEMPORAL_MODELS\n    + [SkyModel, SkyDiffuseCube, BackgroundModel]\n)\n""""""All built-in models.""""""\n\n\n__all__ = [\n    ""SPATIAL_MODELS"",\n    ""TEMPORAL_MODELS"",\n    ""SPECTRAL_MODELS"",\n    ""SkyModelBase"",\n    ""Models"",\n    ""SkyModel"",\n    ""SkyDiffuseCube"",\n    ""BackgroundModel"",\n    ""create_crab_spectral_model"",\n    ""create_cosmic_ray_spectral_model"",\n    ""Absorption"",\n    ""SpatialModel"",\n    ""SpectralModel"",\n    ""TemporalModel"",\n]\n\n__all__.extend(cls.__name__ for cls in SPATIAL_MODELS)\n__all__.extend(cls.__name__ for cls in SPECTRAL_MODELS)\n__all__.extend(cls.__name__ for cls in TEMPORAL_MODELS)\n'"
gammapy/modeling/models/core.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport collections.abc\nimport copy\nfrom os.path import split\nimport numpy as np\nimport astropy.units as u\nfrom astropy.table import Table\nimport yaml\nfrom gammapy.modeling import Covariance, Parameter, Parameters\nfrom gammapy.utils.scripts import make_name, make_path\n\n\ndef _set_link(shared_register, model):\n    for param in model.parameters:\n        name = param.name\n        link_label = param._link_label_io\n        if link_label is not None:\n            if link_label in shared_register:\n                new_param = shared_register[link_label]\n                setattr(model, name, new_param)\n            else:\n                shared_register[link_label] = param\n    return shared_register\n\n\n__all__ = [""Model"", ""Models"", ""ProperModels""]\n\n\nclass Model:\n    """"""Model base class.""""""\n\n    def __init__(self, **kwargs):\n        # Copy default parameters from the class to the instance\n        default_parameters = self.default_parameters.copy()\n\n        for par in default_parameters:\n            value = kwargs.get(par.name, par)\n\n            if not isinstance(value, Parameter):\n                par.quantity = u.Quantity(value)\n            else:\n                par = value\n\n            setattr(self, par.name, par)\n\n        self._covariance = Covariance(self.parameters)\n\n    def __init_subclass__(cls, **kwargs):\n        # Add parameters list on the model sub-class (not instances)\n        cls.default_parameters = Parameters(\n            [_ for _ in cls.__dict__.values() if isinstance(_, Parameter)]\n        )\n\n    @classmethod\n    def from_parameters(cls, parameters, **kwargs):\n        """"""Create model from parameter list\n\n        Parameters\n        ----------\n        parameters : `Parameters`\n            Parameters for init\n\n        Returns\n        -------\n        model : `Model`\n            Model instance\n        """"""\n        for par in parameters:\n            kwargs[par.name] = par\n        return cls(**kwargs)\n\n    def _check_covariance(self):\n        if not self.parameters == self._covariance.parameters:\n            self._covariance = Covariance(self.parameters)\n\n    @property\n    def covariance(self):\n        self._check_covariance()\n        for par in self.parameters:\n            pars = Parameters([par])\n            covar = Covariance(pars, data=[[par.error ** 2]])\n            self._covariance.set_subcovariance(covar)\n\n        return self._covariance\n\n    @covariance.setter\n    def covariance(self, covariance):\n        self._check_covariance()\n        self._covariance.data = covariance\n\n        for par in self.parameters:\n            pars = Parameters([par])\n            variance = self._covariance.get_subcovariance(pars)\n            par.error = np.sqrt(variance)\n\n    @property\n    def parameters(self):\n        """"""Parameters (`~gammapy.modeling.Parameters`)""""""\n        return Parameters(\n            [getattr(self, name) for name in self.default_parameters.names]\n        )\n\n    def copy(self):\n        """"""A deep copy.""""""\n        return copy.deepcopy(self)\n\n    def to_dict(self):\n        """"""Create dict for YAML serialisation""""""\n        return {""type"": self.tag, ""parameters"": self.parameters.to_dict()}\n\n    @classmethod\n    def from_dict(cls, data):\n        kwargs = {}\n        parameters = Parameters.from_dict(data[""parameters""])\n\n        # TODO: this is a special case for spatial models, maybe better move to `SpatialModel` base class\n        if ""frame"" in data:\n            kwargs[""frame""] = data[""frame""]\n\n        return cls.from_parameters(parameters, **kwargs)\n\n    @staticmethod\n    def create(tag, *args, **kwargs):\n        """"""Create a model instance.\n\n        Examples\n        --------\n        >>> from gammapy.modeling.models import Model\n        >>> spectral_model = Model.create(""PowerLaw2SpectralModel"", amplitude=""1e-10 cm-2 s-1"", index=3)\n        >>> type(spectral_model)\n        gammapy.modeling.models.spectral.PowerLaw2SpectralModel\n        """"""\n        from . import MODELS\n\n        cls = MODELS.get_cls(tag)\n        return cls(*args, **kwargs)\n\n    def __str__(self):\n        return f""{self.__class__.__name__}\\n\\n{self.parameters.to_table()}""\n\n\nclass Models(collections.abc.MutableSequence):\n    """"""Sky model collection.\n\n    Parameters\n    ----------\n    models : `SkyModel`, list of `SkyModel` or `Models`\n        Sky models\n    """"""\n\n    def __init__(self, models=None):\n        if models is None:\n            models = []\n\n        if isinstance(models, Models):\n            models = models._models\n        elif isinstance(models, Model):\n            models = [models]\n        elif not isinstance(models, list):\n            raise TypeError(f""Invalid type: {models!r}"")\n\n        unique_names = []\n        for model in models:\n            if model.name in unique_names:\n                raise (ValueError(""Model names must be unique""))\n            unique_names.append(model.name)\n\n        self._models = models\n        self._covar_file = None\n        self._covariance = Covariance(self.parameters)\n\n    def _check_covariance(self):\n        if not self.parameters == self._covariance.parameters:\n            self._covariance = Covariance.from_stack(\n                [model.covariance for model in self._models]\n            )\n\n    @property\n    def covariance(self):\n        self._check_covariance()\n\n        for model in self._models:\n            self._covariance.set_subcovariance(model.covariance)\n\n        return self._covariance\n\n    @covariance.setter\n    def covariance(self, covariance):\n        self._check_covariance()\n        self._covariance.data = covariance\n\n        for model in self._models:\n            subcovar = self._covariance.get_subcovariance(model.covariance.parameters)\n            model.covariance = subcovar\n\n    @property\n    def parameters(self):\n        return Parameters.from_stack([_.parameters for _ in self._models])\n\n    @property\n    def parameters_unique_names(self):\n        from gammapy.modeling.models import SkyModel\n\n        param_names = []\n        for m in self._models:\n            if isinstance(m, SkyModel):\n                for p in m.parameters:\n                    if (\n                        m.spectral_model is not None\n                        and p in m.spectral_model.parameters\n                    ):\n                        tag = "".spectral.""\n                    elif (\n                        m.spatial_model is not None and p in m.spatial_model.parameters\n                    ):\n                        tag = "".spatial.""\n                    elif (\n                        m.temporal_model is not None\n                        and p in m.temporal_model.parameters\n                    ):\n                        tag = "".temporal.""\n                    param_names.append(m.name + tag + p.name)\n            else:\n                for p in m.parameters:\n                    param_names.append(m.name + ""."" + p.name)\n        return param_names\n\n    @property\n    def names(self):\n        return [m.name for m in self._models]\n\n    @classmethod\n    def read(cls, filename):\n        """"""Read from YAML file.""""""\n        yaml_str = make_path(filename).read_text()\n        path, filename = split(filename)\n        return cls.from_yaml(yaml_str, path=path)\n\n    @classmethod\n    def from_yaml(cls, yaml_str, path=""""):\n        """"""Create from YAML string.""""""\n        data = yaml.safe_load(yaml_str)\n        return cls.from_dict(data, path=path)\n\n    @classmethod\n    def from_dict(cls, data, path=""""):\n        """"""Create from dict.""""""\n        from . import MODELS, SkyModel\n\n        models = []\n\n        for component in data[""components""]:\n            model = MODELS.get_cls(component[""type""]).from_dict(component)\n            models.append(model)\n\n        models = cls(models)\n\n        if ""covariance"" in data:\n            filename = data[""covariance""]\n            path = make_path(path)\n            if not (path / filename).exists():\n                path, filename = split(filename)\n\n            models.read_covariance(path, filename, format=""ascii.fixed_width"")\n\n        shared_register = {}\n        for model in models:\n            if isinstance(model, SkyModel):\n                submodels = [\n                    model.spectral_model,\n                    model.spatial_model,\n                    model.temporal_model,\n                ]\n                for submodel in submodels:\n                    if submodel is not None:\n                        shared_register = _set_link(shared_register, submodel)\n            else:\n                shared_register = _set_link(shared_register, model)\n        return models\n\n    def write(self, path, overwrite=False):\n        """"""Write to YAML file.""""""\n        base_path, _ = split(path)\n        path = make_path(path)\n        base_path = make_path(base_path)\n\n        if path.exists() and not overwrite:\n            raise IOError(f""File exists already: {path}"")\n\n        if self.covariance is not None and len(self.parameters) != 0:\n            filecovar = path.stem + ""_covariance.dat""\n            kwargs = dict(\n                format=""ascii.fixed_width"", delimiter=""|"", overwrite=overwrite\n            )\n            self.write_covariance(base_path / filecovar, **kwargs)\n            self._covar_file = filecovar\n\n        path.write_text(self.to_yaml())\n\n    def to_yaml(self):\n        """"""Convert to YAML string.""""""\n        data = self.to_dict()\n        return yaml.dump(\n            data, sort_keys=False, indent=4, width=80, default_flow_style=None\n        )\n\n    def to_dict(self):\n        """"""Convert to dict.""""""\n        # update linked parameters labels\n        params_list = []\n        params_shared = []\n        for param in self.parameters:\n            if param not in params_list:\n                params_list.append(param)\n            elif param not in params_shared:\n                params_shared.append(param)\n        for param in params_shared:\n            param._link_label_io = param.name + ""@"" + make_name()\n\n        models_data = []\n        for model in self._models:\n            model_data = model.to_dict()\n            models_data.append(model_data)\n        if self._covar_file is not None:\n            return {\n                ""components"": models_data,\n                ""covariance"": str(self._covar_file),\n            }\n        else:\n            return {""components"": models_data}\n\n    def read_covariance(self, path, filename=""_covariance.dat"", **kwargs):\n        """"""Read covariance data from file\n\n        Parameters\n        ----------\n        filename : str\n            Filename\n        **kwargs : dict\n            Keyword arguments passed to `~astropy.table.Table.read`\n\n        """"""\n        path = make_path(path)\n        filepath = str(path / filename)\n        t = Table.read(filepath, **kwargs)\n        t.remove_column(""Parameters"")\n        arr = np.array(t)\n        data = arr.view(float).reshape(arr.shape + (-1,))\n        self.covariance = data\n        self._covar_file = filename\n\n    def write_covariance(self, filename, **kwargs):\n        """"""Write covariance to file\n\n        Parameters\n        ----------\n        filename : str\n            Filename\n        **kwargs : dict\n            Keyword arguments passed to `~astropy.table.Table.write`\n\n        """"""\n        names = self.parameters_unique_names\n        table = Table()\n        table[""Parameters""] = names\n\n        for idx, name in enumerate(names):\n            values = self.covariance.data[idx]\n            table[name] = values\n\n        table.write(make_path(filename), **kwargs)\n\n    def __str__(self):\n        str_ = f""{self.__class__.__name__}\\n\\n""\n\n        for idx, model in enumerate(self):\n            str_ += f""Component {idx}: ""\n            str_ += str(model)\n\n        return str_.expandtabs(tabsize=2)\n\n    def __add__(self, other):\n        if isinstance(other, (Models, list)):\n            return Models([*self, *other])\n        elif isinstance(other, Model):\n            if other.name in self.names:\n                raise (ValueError(""Model names must be unique""))\n            return Models([*self, other])\n        else:\n            raise TypeError(f""Invalid type: {other!r}"")\n\n    def __getitem__(self, key):\n        return self._models[self.index(key)]\n\n    def __delitem__(self, key):\n        del self._models[self.index(key)]\n\n    def __setitem__(self, key, model):\n        from gammapy.modeling.models import SkyModel, SkyDiffuseCube\n\n        if isinstance(model, (SkyModel, SkyDiffuseCube)):\n            self._models[self.index(key)] = model\n        else:\n            raise TypeError(f""Invalid type: {model!r}"")\n\n    def insert(self, idx, model):\n        if model.name in self.names:\n            raise (ValueError(""Model names must be unique""))\n\n        self._models.insert(idx, model)\n\n    def index(self, key):\n        if isinstance(key, (int, slice)):\n            return key\n        elif isinstance(key, str):\n            return self.names.index(key)\n        elif isinstance(key, Model):\n            return self._models.index(key)\n        else:\n            raise TypeError(f""Invalid type: {type(key)!r}"")\n\n    def __len__(self):\n        return len(self._models)\n\n    def _ipython_key_completions_(self):\n        return self.names\n\n    def copy(self):\n        """"""A deep copy.""""""\n        return copy.deepcopy(self)\n\n\nclass ProperModels(Models):\n    """""" Proper Models of a Dataset or Datasets.""""""\n\n    def __init__(self, parent):\n        from gammapy.datasets import Dataset, Datasets\n\n        if isinstance(parent, Dataset):\n            self._datasets = [parent]\n            self._is_dataset = True\n        elif isinstance(parent, Datasets):\n            self._datasets = parent._datasets\n            self._is_dataset = False\n        else:\n            raise TypeError(f""Invalid type: {type(parent)!r}"")\n\n        unique_models = []\n        for d in self._datasets:\n            if d._models is not None:\n                for model in d._models:\n                    if model not in unique_models:\n                        if (\n                            model.datasets_names is None\n                            or d.name in model.datasets_names\n                        ):\n                            unique_models.append(model)\n            else:\n                d._models = Models([])\n            self._models = unique_models\n\n        if self._is_dataset == False:\n            self.force_models_consistency()\n\n        self._covar_file = None\n        self._covariance = Covariance(self.parameters)\n\n    def force_models_consistency(self):\n        """"""Force consistency between models listed in each dataset of a datasets\n        and each model.datasets_names. For example if a model with\n        datasets_names = None appears in only one dataset,\n        it will be automatically added to the others.\n        """"""\n        for d in self._datasets:\n            for m in self._models:\n                if (\n                    m.datasets_names is None or d.name in m.datasets_names\n                ) and m not in d.models:\n                    d.models.append(m)\n\n    def __add__(self, other):\n        if isinstance(other, (Models, list)):\n            pass\n        elif isinstance(other, Model):\n            other = [other]\n        else:\n            raise TypeError(f""Invalid type: {other!r}"")\n        for d in self._datasets:\n            for m in other:\n                if m not in d._models:\n                    d._models.append(m)\n                if (\n                    m.datasets_names is not None\n                    and d.name not in m.datasets_names\n                    and self._is_dataset\n                ):\n                    m.datasets_names.append(d.name)\n\n    def __delitem__(self, key):\n        for d in self._datasets:\n            if key in d.models.names:\n                datasets_names = d.models[key].datasets_names\n                if datasets_names is None or d.name in datasets_names:\n                    d._models.remove(key)\n\n    def __setitem__(self, key, model):\n        from gammapy.modeling.models import SkyModel, SkyDiffuseCube\n\n        for d in self._datasets:\n            if model not in d._models:\n                if isinstance(model, (SkyModel, SkyDiffuseCube)):\n                    d._models[key] = model\n\n                else:\n                    raise TypeError(f""Invalid type: {model!r}"")\n            if (\n                model.datasets_names is not None\n                and d.name not in model.datasets_names\n                and self._is_dataset\n            ):\n                model.datasets_names.append(d.name)\n\n    def insert(self, idx, model):\n        from gammapy.modeling.models import SkyModel, SkyDiffuseCube\n\n        for d in self._datasets:\n            if model not in d._models:\n                if isinstance(model, (SkyModel, SkyDiffuseCube)):\n                    if idx == len(self):\n                        index = len(d._models)\n                    else:\n                        index = idx\n                    d._models.insert(index, model)\n                else:\n                    raise TypeError(f""Invalid type: {model!r}"")\n            if (\n                model.datasets_names is not None\n                and d.name not in model.datasets_names\n                and self._is_dataset\n            ):\n                model.datasets_names.append(d.name)\n\n    def remove(self, value):\n        key = value.name\n        del self[key]\n'"
gammapy/modeling/models/cube.py,11,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Cube models (axes: lon, lat, energy).""""""\nimport copy\nfrom pathlib import Path\nimport numpy as np\nimport astropy.units as u\nfrom gammapy.maps import Map, MapAxis, RegionGeom, WcsGeom\nfrom gammapy.modeling import Covariance, Parameter, Parameters\nfrom gammapy.modeling.parameter import _get_parameters_str\nfrom gammapy.utils.scripts import make_name, make_path\nfrom .core import Model, Models\nfrom .spatial import SpatialModel\nfrom .spectral import SpectralModel\nfrom .temporal import TemporalModel\n\n\nclass SkyModelBase(Model):\n    """"""Sky model base class""""""\n\n    def __add__(self, other):\n        if isinstance(other, (Models, list)):\n            return Models([self, *other])\n        elif isinstance(other, (SkyModel, SkyDiffuseCube)):\n            return Models([self, other])\n        else:\n            raise TypeError(f""Invalid type: {other!r}"")\n\n    def __radd__(self, model):\n        return self.__add__(model)\n\n    def __call__(self, lon, lat, energy, time=None):\n        return self.evaluate(lon, lat, energy, time)\n\n    def evaluate_geom(self, geom, gti=None):\n        coords = geom.get_coord(frame=self.frame)\n        return self(coords.lon, coords.lat, coords[""energy_true""])\n\n\nclass SkyModel(SkyModelBase):\n    """"""Sky model component.\n\n    This model represents a factorised sky model.\n    It has `~gammapy.modeling.Parameters`\n    combining the spatial and spectral parameters.\n\n    Parameters\n    ----------\n    spectral_model : `~gammapy.modeling.models.SpectralModel`\n        Spectral model\n    spatial_model : `~gammapy.modeling.models.SpatialModel`\n        Spatial model (must be normalised to integrate to 1)\n    temporal_model : `~gammapy.modeling.models.temporalModel`\n        Temporal model\n    name : str\n        Model identifier\n    """"""\n\n    tag = ""SkyModel""\n    _apply_irf_default = {""exposure"": True, ""psf"": True, ""edisp"": True}\n\n    def __init__(\n        self,\n        spectral_model,\n        spatial_model=None,\n        temporal_model=None,\n        name=None,\n        apply_irf=None,\n        datasets_names=None,\n    ):\n        self.spatial_model = spatial_model\n        self.spectral_model = spectral_model\n        self.temporal_model = temporal_model\n        self._name = make_name(name)\n\n        if apply_irf is None:\n            apply_irf = self._apply_irf_default.copy()\n\n        self.apply_irf = apply_irf\n        self.datasets_names = datasets_names\n        super().__init__()\n\n    @property\n    def _models(self):\n        models = self.spectral_model, self.spatial_model, self.temporal_model\n        return [_ for _ in models if _]\n\n    def _check_covariance(self):\n        if not self.parameters == self._covariance.parameters:\n            self._covariance = Covariance.from_stack(\n                [model.covariance for model in self._models],\n            )\n\n    @property\n    def covariance(self):\n        self._check_covariance()\n\n        for model in self._models:\n            self._covariance.set_subcovariance(model.covariance)\n\n        return self._covariance\n\n    @covariance.setter\n    def covariance(self, covariance):\n        self._check_covariance()\n        self._covariance.data = covariance\n\n        for model in self._models:\n            subcovar = self._covariance.get_subcovariance(model.covariance.parameters)\n            model.covariance = subcovar\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def parameters(self):\n        parameters = []\n\n        parameters.append(self.spectral_model.parameters)\n\n        if self.spatial_model is not None:\n            parameters.append(self.spatial_model.parameters)\n\n        if self.temporal_model is not None:\n            parameters.append(self.temporal_model.parameters)\n\n        return Parameters.from_stack(parameters)\n\n    @property\n    def spatial_model(self):\n        """"""`~gammapy.modeling.models.SpatialModel`""""""\n        return self._spatial_model\n\n    @spatial_model.setter\n    def spatial_model(self, model):\n        if not (model is None or isinstance(model, SpatialModel)):\n            raise TypeError(f""Invalid type: {model!r}"")\n\n        self._spatial_model = model\n\n    @property\n    def spectral_model(self):\n        """"""`~gammapy.modeling.models.SpectralModel`""""""\n        return self._spectral_model\n\n    @spectral_model.setter\n    def spectral_model(self, model):\n        if not (model is None or isinstance(model, SpectralModel)):\n            raise TypeError(f""Invalid type: {model!r}"")\n\n        self._spectral_model = model\n\n    @property\n    def temporal_model(self):\n        """"""`~gammapy.modeling.models.TemporalModel`""""""\n        return self._temporal_model\n\n    @temporal_model.setter\n    def temporal_model(self, model):\n        if not (model is None or isinstance(model, TemporalModel)):\n            raise TypeError(f""Invalid type: {model!r}"")\n\n        self._temporal_model = model\n\n    @property\n    def position(self):\n        """"""`~astropy.coordinates.SkyCoord`""""""\n        return self.spatial_model.position\n\n    @property\n    def evaluation_radius(self):\n        """"""`~astropy.coordinates.Angle`""""""\n        return self.spatial_model.evaluation_radius\n\n    @property\n    def frame(self):\n        return self.spatial_model.frame\n\n    def __repr__(self):\n        return (\n            f""{self.__class__.__name__}(""\n            f""spatial_model={self.spatial_model!r}, ""\n            f""spectral_model={self.spectral_model!r})""\n            f""temporal_model={self.temporal_model!r})""\n        )\n\n    def evaluate(self, lon, lat, energy, time=None):\n        """"""Evaluate the model at given points.\n\n        The model evaluation follows numpy broadcasting rules.\n\n        Return differential surface brightness cube.\n        At the moment in units: ``cm-2 s-1 TeV-1 deg-2``\n\n        Parameters\n        ----------\n        lon, lat : `~astropy.units.Quantity`\n            Spatial coordinates\n        energy : `~astropy.units.Quantity`\n            Energy coordinate\n        time: `~astropy.time.Time`\n            Time co-ordinate\n\n        Returns\n        -------\n        value : `~astropy.units.Quantity`\n            Model value at the given point.\n        """"""\n        value = self.spectral_model(energy)  # pylint:disable=not-callable\n        # TODO: case if self.temporal_model is not None, introduce time in arguments ?\n\n        if self.spatial_model is not None:\n            value = value * self.spatial_model(lon, lat)  # pylint:disable=not-callable\n\n        if (self.temporal_model is not None) and (time is not None):\n            value = value * self.temporal_model(time)\n\n        return value\n\n    def evaluate_geom(self, geom, gti=None):\n        """"""Evaluate model on `~gammapy.maps.Geom`.""""""\n        energy = geom.get_axis_by_name(""energy_true"").center[:, np.newaxis, np.newaxis]\n        value = self.spectral_model(energy)\n\n        if self.spatial_model:\n            value = value * self.spatial_model.evaluate_geom(geom.to_image())\n\n        if self.temporal_model:\n            integral = self.temporal_model.integral(gti.time_start, gti.time_stop)\n            value = value * np.sum(integral)\n\n        return value\n\n    def integrate_geom(self, geom, gti=None):\n        """"""Integrate model on `~gammapy.maps.Geom`.\n\n        Parameters\n        ----------\n        geom : `Geom`\n            Map geometry\n        gti : `GTI`\n            GIT table\n\n        Returns\n        -------\n        flux : `Map`\n            Predicted flux map\n        """"""\n        energy = geom.get_axis_by_name(""energy_true"").edges\n        value = self.spectral_model.integral(\n            energy[:-1], energy[1:], intervals=True\n        ).reshape((-1, 1, 1))\n\n        if self.spatial_model and not isinstance(geom, RegionGeom):\n            # TODO: integrate spatial model over region to correct for\n            #  containment\n            geom_image = geom.to_image()\n            value = value * self.spatial_model.integrate(geom_image).quantity\n\n        if self.temporal_model:\n            integral = self.temporal_model.integral(gti.time_start, gti.time_stop)\n            value = value * np.sum(integral)\n\n        return Map.from_geom(geom=geom, data=value.value, unit=value.unit)\n\n    def copy(self, name=None, **kwargs):\n        """"""Copy SkyModel""""""\n        if self.spatial_model is not None:\n            spatial_model = self.spatial_model.copy()\n        else:\n            spatial_model = None\n\n        if self.temporal_model is not None:\n            temporal_model = self.temporal_model.copy()\n        else:\n            temporal_model = None\n\n        kwargs.setdefault(""name"", make_name(name))\n        kwargs.setdefault(""spectral_model"", self.spectral_model.copy())\n        kwargs.setdefault(""spatial_model"", spatial_model)\n        kwargs.setdefault(""temporal_model"", temporal_model)\n        kwargs.setdefault(""apply_irf"", self.apply_irf.copy())\n        kwargs.setdefault(""datasets_names"", self.datasets_names)\n\n        return self.__class__(**kwargs)\n\n    def to_dict(self):\n        """"""Create dict for YAML serilisation""""""\n        data = {}\n        data[""name""] = self.name\n        data[""type""] = self.tag\n        data[""spectral""] = self.spectral_model.to_dict()\n\n        if self.spatial_model is not None:\n            data[""spatial""] = self.spatial_model.to_dict()\n\n        if self.temporal_model is not None:\n            data[""temporal""] = self.temporal_model.to_dict()\n\n        if self.apply_irf != self._apply_irf_default:\n            data[""apply_irf""] = self.apply_irf\n\n        if self.datasets_names is not None:\n            data[""datasets_names""] = self.datasets_names\n\n        return data\n\n    @classmethod\n    def from_dict(cls, data):\n        """"""Create SkyModel from dict""""""\n        from gammapy.modeling.models import (\n            SPATIAL_MODELS,\n            SPECTRAL_MODELS,\n            TEMPORAL_MODELS,\n        )\n\n        model_class = SPECTRAL_MODELS.get_cls(data[""spectral""][""type""])\n        spectral_model = model_class.from_dict(data[""spectral""])\n\n        spatial_data = data.get(""spatial"")\n\n        if spatial_data is not None:\n            model_class = SPATIAL_MODELS.get_cls(spatial_data[""type""])\n            spatial_model = model_class.from_dict(spatial_data)\n        else:\n            spatial_model = None\n\n        temporal_data = data.get(""temporal"")\n\n        if temporal_data is not None:\n            model_class = TEMPORAL_MODELS.get_cls(temporal_data[""type""])\n            temporal_model = model_class.from_dict(temporal_data)\n        else:\n            temporal_model = None\n\n        return cls(\n            name=data[""name""],\n            spatial_model=spatial_model,\n            spectral_model=spectral_model,\n            temporal_model=temporal_model,\n            apply_irf=data.get(""apply_irf"", cls._apply_irf_default),\n            datasets_names=data.get(""datasets_names""),\n        )\n\n    def __str__(self):\n        str_ = self.__class__.__name__ + ""\\n\\n""\n        str_ += ""\\t{:26}: {}\\n"".format(""Name"", self.name)\n\n        str_ += ""\\t{:26}: {}\\n"".format(""Datasets names"", self.datasets_names)\n\n        str_ += ""\\t{:26}: {}\\n"".format(""Spectral model type"", self.spectral_model.tag)\n\n        if self.spatial_model is not None:\n            spatial_type = self.spatial_model.tag\n        else:\n            spatial_type = ""None""\n        str_ += ""\\t{:26}: {}\\n"".format(""Spatial  model type"", spatial_type)\n\n        if self.temporal_model is not None:\n            temporal_type = self.temporal_model.tag\n        else:\n            temporal_type = ""None""\n        str_ += ""\\t{:26}: {}\\n"".format(""Temporal model type"", temporal_type)\n\n        str_ += ""\\tParameters:\\n""\n        info = _get_parameters_str(self.parameters)\n        lines = info.split(""\\n"")\n        str_ += ""\\t"" + ""\\n\\t"".join(lines[:-1])\n\n        str_ += ""\\n\\n""\n        return str_.expandtabs(tabsize=2)\n\n\nclass SkyDiffuseCube(SkyModelBase):\n    """"""Cube sky map template model (3D).\n\n    This is for a 3D map with an energy axis.\n    Use `~gammapy.modeling.models.TemplateSpatialModel` for 2D maps.\n\n    Parameters\n    ----------\n    map : `~gammapy.maps.Map`\n        Map template\n    norm : float\n        Norm parameter (multiplied with map values)\n    tilt : float\n        Additional tilt in the spectrum\n    reference : `~astropy.units.Quantity`\n        Reference energy of the tilt.\n    meta : dict, optional\n        Meta information, meta[\'filename\'] will be used for serialization\n    interp_kwargs : dict\n        Interpolation keyword arguments passed to `gammapy.maps.Map.interp_by_coord`.\n        Default arguments are {\'interp\': \'linear\', \'fill_value\': 0}.\n    """"""\n\n    tag = ""SkyDiffuseCube""\n    norm = Parameter(""norm"", 1)\n    tilt = Parameter(""tilt"", 0, unit="""", frozen=True)\n    reference = Parameter(""reference"", ""1 TeV"", frozen=True)\n\n    _apply_irf_default = {""exposure"": True, ""psf"": True, ""edisp"": True}\n\n    def __init__(\n        self,\n        map,\n        norm=norm.quantity,\n        tilt=tilt.quantity,\n        reference=reference.quantity,\n        meta=None,\n        interp_kwargs=None,\n        name=None,\n        filename=None,\n        apply_irf=None,\n        datasets_names=None,\n    ):\n\n        self._name = make_name(name)\n\n        self.map = map\n        self.meta = {} if meta is None else meta\n        self.filename = filename\n\n        interp_kwargs = {} if interp_kwargs is None else interp_kwargs\n        interp_kwargs.setdefault(""interp"", ""linear"")\n        interp_kwargs.setdefault(""fill_value"", 0)\n        self._interp_kwargs = interp_kwargs\n\n        # TODO: onve we have implement a more general and better model caching\n        #  remove this again\n        self._cached_value = None\n        self._cached_coordinates = (None, None, None)\n\n        if apply_irf is None:\n            apply_irf = self._apply_irf_default.copy()\n\n        self.apply_irf = apply_irf\n        self.datasets_names = datasets_names\n        super().__init__(norm=norm, tilt=tilt, reference=reference)\n\n    @property\n    def name(self):\n        return self._name\n\n    @classmethod\n    def read(cls, filename, name=None, **kwargs):\n        """"""Read map from FITS file.\n\n        The default unit used if none is found in the file is ``cm-2 s-1 MeV-1 sr-1``.\n\n        Parameters\n        ----------\n        filename : str\n            FITS image filename.\n        name : str\n            Name of the output model\n            The default used if none is filename.\n        """"""\n        m = Map.read(filename, **kwargs)\n\n        if m.unit == """":\n            m.unit = ""cm-2 s-1 MeV-1 sr-1""\n\n        if name is None:\n            name = Path(filename).stem\n\n        if m.geom.axes[0].name == ""energy"":\n            m.geom.axes[0].name = ""energy_true""\n\n        return cls(m, name=name, filename=filename)\n\n    def _interpolate(self, lon, lat, energy):\n        coord = {\n            ""lon"": lon.to_value(""deg""),\n            ""lat"": lat.to_value(""deg""),\n            ""energy_true"": energy,\n        }\n        return self.map.interp_by_coord(coord, **self._interp_kwargs)\n\n    def evaluate(self, lon, lat, energy, time=None):\n        """"""Evaluate model.\n        passing time does not make sense here - passed just to match arguments\n        of SkyModel.evaluate""""""\n        is_cached_coord = [\n            _ is coord for _, coord in zip((lon, lat, energy), self._cached_coordinates)\n        ]\n\n        # reset cache\n        if not np.all(is_cached_coord):\n            self._cached_value = None\n\n        if self._cached_value is None:\n            self._cached_coordinates = (lon, lat, energy)\n            self._cached_value = self._interpolate(lon, lat, energy)\n\n        norm = self.norm.value\n        tilt = self.tilt.value\n        reference = self.reference.quantity\n\n        tilt_factor = np.power((energy / reference).to(""""), -tilt)\n\n        val = norm * self._cached_value * tilt_factor.value\n        return u.Quantity(val, self.map.unit, copy=False)\n\n    def integrate_geom(self, geom, gti=None):\n        """"""Integrate model on `~gammapy.maps.Geom`.\n\n        Parameters\n        ----------\n        geom : `Geom`\n            Map geometry\n        gti : `GTI`\n            GIT table (currently not being used...)\n\n        Returns\n        -------\n        flux : `Map`\n            Predicted flux map\n        """"""\n        # TODO: implement better integration method?\n        value = self.evaluate_geom(geom)\n        value = value * geom.bin_volume()\n        return Map.from_geom(geom=geom, data=value.value, unit=value.unit)\n\n    def copy(self, name=None):\n        """"""A shallow copy""""""\n        new = copy.copy(self)\n        new._name = make_name(name)\n        return new\n\n    @property\n    def position(self):\n        """"""`~astropy.coordinates.SkyCoord`""""""\n        return self.map.geom.center_skydir\n\n    @property\n    def evaluation_radius(self):\n        """"""`~astropy.coordinates.Angle`""""""\n        return np.max(self.map.geom.width) / 2.0\n\n    @property\n    def frame(self):\n        return self.position.frame.name\n\n    @classmethod\n    def from_dict(cls, data):\n        parameters = Parameters.from_dict(data[""parameters""])\n\n        filename = data[""filename""]\n\n        map_ = cls.read(filename).map\n\n        apply_irf = data.get(""apply_irf"", cls._apply_irf_default)\n        datasets_names = data.get(""datasets_names"")\n        name = data.get(""name"")\n\n        return cls.from_parameters(\n            parameters=parameters,\n            map=map_,\n            apply_irf=apply_irf,\n            datasets_names=datasets_names,\n            filename=filename,\n            name=name,\n        )\n\n    def to_dict(self):\n        data = super().to_dict()\n        data[""name""] = self.name\n        data[""type""] = data.pop(""type"")\n        data[""filename""] = self.filename\n\n        # Move parameters at the end\n        data[""parameters""] = data.pop(""parameters"")\n\n        if self.apply_irf != self._apply_irf_default:\n            data[""apply_irf""] = self.apply_irf\n\n        if self.datasets_names is not None:\n            data[""datasets_names""] = self.datasets_names\n\n        return data\n\n    def __str__(self):\n        str_ = self.__class__.__name__ + ""\\n\\n""\n        str_ += ""\\t{:26}: {}\\n"".format(""Name"", self.name)\n        str_ += ""\\t{:26}: {}\\n"".format(""Datasets names"", self.datasets_names)\n        str_ += ""\\tParameters:\\n""\n        info = _get_parameters_str(self.parameters)\n        lines = info.split(""\\n"")\n        str_ += ""\\t"" + ""\\n\\t"".join(lines[:-1])\n\n        str_ += ""\\n\\n""\n        return str_.expandtabs(tabsize=2)\n\n\nclass BackgroundModel(Model):\n    """"""Background model.\n\n    Create a new map by a tilt and normalization on the available map\n\n    Parameters\n    ----------\n    map : `~gammapy.maps.Map`\n        Background model map\n    norm : float\n        Background normalization\n    tilt : float\n        Additional tilt in the spectrum\n    reference : `~astropy.units.Quantity`\n        Reference energy of the tilt.\n    """"""\n\n    tag = ""BackgroundModel""\n    norm = Parameter(""norm"", 1, unit="""", min=0)\n    tilt = Parameter(""tilt"", 0, unit="""", frozen=True)\n    reference = Parameter(""reference"", ""1 TeV"", frozen=True)\n\n    def __init__(\n        self,\n        map,\n        norm=norm.quantity,\n        tilt=tilt.quantity,\n        reference=reference.quantity,\n        name=None,\n        filename=None,\n        datasets_names=None,\n    ):\n        axis = map.geom.get_axis_by_name(""energy"")\n        if axis.node_type != ""edges"":\n            raise ValueError(\'Need an integrated map, energy axis node_type=""edges""\')\n\n        self.map = map\n\n        self._name = make_name(name)\n        self.filename = filename\n\n        if isinstance(datasets_names, list):\n            if len(datasets_names) != 1:\n                raise ValueError(\n                    ""Currently background models can only be assigned to one dataset.""\n                )\n\n        self.datasets_names = datasets_names\n        super().__init__(norm=norm, tilt=tilt, reference=reference)\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def energy_center(self):\n        """"""True energy axis bin centers (`~astropy.units.Quantity`)""""""\n        energy_axis = self.map.geom.get_axis_by_name(""energy"")\n        energy = energy_axis.center\n        return energy[:, np.newaxis, np.newaxis]\n\n    def evaluate(self):\n        """"""Evaluate background model.\n\n        Returns\n        -------\n        background_map : `~gammapy.maps.Map`\n            Background evaluated on the Map\n        """"""\n        norm = self.norm.value\n        tilt = self.tilt.value\n        reference = self.reference.quantity\n        tilt_factor = np.power((self.energy_center / reference).to(""""), -tilt)\n        back_values = norm * self.map.data * tilt_factor.value\n        return self.map.copy(data=back_values)\n\n    def to_dict(self):\n        data = {}\n        data[""name""] = self.name\n        data.update(super().to_dict())\n\n        if self.filename is not None:\n            data[""filename""] = self.filename\n\n        data[""parameters""] = data.pop(""parameters"")\n\n        if self.datasets_names is not None:\n            data[""datasets_names""] = self.datasets_names\n\n        return data\n\n    @classmethod\n    def from_dict(cls, data):\n        if ""filename"" in data:\n            bkg_map = Map.read(data[""filename""])\n        elif ""map"" in data:\n            bkg_map = data[""map""]\n        else:\n            # TODO: for now create a fake map for serialization,\n            # uptdated in MapDataset.from_dict()\n            axis = MapAxis.from_edges(np.logspace(-1, 1, 2), unit=u.TeV, name=""energy"")\n            geom = WcsGeom.create(\n                skydir=(0, 0), npix=(1, 1), frame=""galactic"", axes=[axis]\n            )\n            bkg_map = Map.from_geom(geom)\n\n        parameters = Parameters.from_dict(data[""parameters""])\n\n        return cls.from_parameters(\n            parameters=parameters,\n            map=bkg_map,\n            name=data[""name""],\n            datasets_names=data.get(""datasets_names""),\n            filename=data.get(""filename""),\n        )\n\n    def copy(self, name=None):\n        """"""A deep copy.""""""\n        new = copy.deepcopy(self)\n        new._name = make_name(name)\n        return new\n\n    def cutout(self, position, width, mode=""trim"", name=None):\n        """"""Cutout background model.\n\n        Parameters\n        ----------\n        position : `~astropy.coordinates.SkyCoord`\n            Center position of the cutout region.\n        width : tuple of `~astropy.coordinates.Angle`\n            Angular sizes of the region in (lon, lat) in that specific order.\n            If only one value is passed, a square region is extracted.\n        mode : {\'trim\', \'partial\', \'strict\'}\n            Mode option for Cutout2D, for details see `~astropy.nddata.utils.Cutout2D`.\n        name : str\n            Name of the returned background model.\n\n        Returns\n        -------\n        cutout : `BackgroundModel`\n            Cutout background model.\n        """"""\n        cutout_kwargs = {""position"": position, ""width"": width, ""mode"": mode}\n\n        bkg_map = self.map.cutout(**cutout_kwargs)\n        parameters = self.parameters.copy()\n        return self.__class__.from_parameters(parameters, map=bkg_map, name=name)\n\n    def stack(self, other, weights=None):\n        """"""Stack background model in place.\n\n        Stacking the background model resets the current parameters values.\n\n        Parameters\n        ----------\n        other : `BackgroundModel`\n            Other background model.\n        """"""\n        bkg = self.evaluate()\n        other_bkg = other.evaluate()\n        bkg.stack(other_bkg, weights=weights)\n        self.map = bkg\n\n        # reset parameter values\n        self.norm.value = 1\n        self.tilt.value = 0\n\n    def __str__(self):\n        str_ = self.__class__.__name__ + ""\\n\\n""\n        str_ += ""\\t{:26}: {}\\n"".format(""Name"", self.name)\n        str_ += ""\\t{:26}: {}\\n"".format(""Datasets names"", self.datasets_names)\n\n        str_ += ""\\tParameters:\\n""\n        info = _get_parameters_str(self.parameters)\n        lines = info.split(""\\n"")\n        str_ += ""\\t"" + ""\\n\\t"".join(lines[:-1])\n\n        str_ += ""\\n\\n""\n        return str_.expandtabs(tabsize=2)\n\n    @property\n    def position(self):\n        """"""`~astropy.coordinates.SkyCoord`""""""\n        return self.map.geom.center_skydir\n\n    @property\n    def evaluation_radius(self):\n        """"""`~astropy.coordinates.Angle`""""""\n        return np.max(self.map.geom.width) / 2.0\n\n\ndef create_fermi_isotropic_diffuse_model(filename, **kwargs):\n    """"""Read Fermi isotropic diffuse model.\n\n    See `LAT Background models <https://fermi.gsfc.nasa.gov/ssc/data/access/lat/BackgroundModels.html>`_\n\n    Parameters\n    ----------\n    filename : str\n        filename\n    kwargs : dict\n        Keyword arguments forwarded to `TemplateSpectralModel`\n\n    Returns\n    -------\n    diffuse_model : `SkyModel`\n        Fermi isotropic diffuse sky model.\n    """"""\n    from .spectral import TemplateSpectralModel\n    from .spatial import ConstantSpatialModel\n\n    vals = np.loadtxt(make_path(filename))\n    energy = u.Quantity(vals[:, 0], ""MeV"", copy=False)\n    values = u.Quantity(vals[:, 1], ""MeV-1 s-1 cm-2"", copy=False)\n\n    spatial_model = ConstantSpatialModel()\n    spectral_model = TemplateSpectralModel(energy=energy, values=values, **kwargs)\n    return SkyModel(\n        spatial_model=spatial_model,\n        spectral_model=spectral_model,\n        name=""fermi-diffuse-iso"",\n    )\n'"
gammapy/modeling/models/spatial.py,41,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Spatial models.""""""\nimport logging\nimport numpy as np\nimport scipy.integrate\nimport scipy.special\nimport astropy.units as u\nfrom astropy.coordinates import Angle, SkyCoord\nfrom astropy.coordinates.angle_utilities import angular_separation, position_angle\nfrom regions import (\n    CircleAnnulusSkyRegion,\n    EllipseSkyRegion,\n    PointSkyRegion,\n    PolygonSkyRegion,\n)\nfrom gammapy.maps import Map, WcsGeom\nfrom gammapy.modeling import Parameter, Parameters\nfrom gammapy.utils.gauss import Gauss2DPDF\nfrom gammapy.utils.scripts import make_path\nfrom .core import Model\n\nlog = logging.getLogger(__name__)\n\n\ndef compute_sigma_eff(lon_0, lat_0, lon, lat, phi, major_axis, e):\n    """"""Effective radius, used for the evaluation of elongated models""""""\n    phi_0 = position_angle(lon_0, lat_0, lon, lat)\n    d_phi = phi - phi_0\n    minor_axis = Angle(major_axis * np.sqrt(1 - e ** 2))\n\n    a2 = (major_axis * np.sin(d_phi)) ** 2\n    b2 = (minor_axis * np.cos(d_phi)) ** 2\n    denominator = np.sqrt(a2 + b2)\n    sigma_eff = major_axis * minor_axis / denominator\n    return minor_axis, sigma_eff\n\n\nclass SpatialModel(Model):\n    """"""Spatial model base class.""""""\n\n    def __init__(self, **kwargs):\n        frame = kwargs.pop(""frame"", ""icrs"")\n        super().__init__(**kwargs)\n        if not hasattr(self, ""frame""):\n            self.frame = frame\n\n    def __call__(self, lon, lat):\n        """"""Call evaluate method""""""\n        kwargs = {par.name: par.quantity for par in self.parameters}\n        return self.evaluate(lon, lat, **kwargs)\n\n    @property\n    def position(self):\n        """"""Spatial model center position""""""\n        lon = self.lon_0.quantity\n        lat = self.lat_0.quantity\n        return SkyCoord(lon, lat, frame=self.frame)\n\n    @position.setter\n    def position(self, skycoord):\n        """"""Spatial model center position""""""\n        coord = skycoord.transform_to(self.frame)\n        self.lon_0.quantity = coord.data.lon\n        self.lat_0.quantity = coord.data.lat\n\n    # TODO: get rid of this!\n    _phi_0 = 0.0\n\n    @property\n    def phi_0(self):\n        return self._phi_0\n\n    @phi_0.setter\n    def phi_0(self, phi_0=0.0):\n        self._phi_0 = phi_0\n\n    @property\n    def position_error(self):\n        """"""Get 95% containment position error as (`~regions.EllipseSkyRegion`)""""""\n        if self.covariance is None:\n            return EllipseSkyRegion(\n                center=self.position,\n                height=np.nan * u.deg,\n                width=np.nan * u.deg,\n                angle=np.nan * u.deg,\n            )\n        pars = self.parameters\n        sub_covar = self.covariance.get_subcovariance([""lon_0"", ""lat_0""]).data.copy()\n        cos_lat = np.cos(self.lat_0.quantity.to_value(""rad""))\n        sub_covar[0, 0] *= cos_lat ** 2.0\n        sub_covar[0, 1] *= cos_lat\n        sub_covar[1, 0] *= cos_lat\n        eig_vals, eig_vecs = np.linalg.eig(sub_covar)\n        lon_err, lat_err = np.sqrt(eig_vals)\n        y_vec = eig_vecs[:, 0]\n        phi = (np.arctan2(y_vec[1], y_vec[0]) * u.rad).to(""deg"") + self.phi_0\n        err = np.sort([lon_err, lat_err])\n        scale_r95 = Gauss2DPDF().containment_radius(0.95)\n        err *= scale_r95\n        if err[1] == lon_err * scale_r95:\n            phi += 90 * u.deg\n            height = 2 * err[1] * pars[""lon_0""].unit\n            width = 2 * err[0] * pars[""lat_0""].unit\n        else:\n            height = 2 * err[1] * pars[""lat_0""].unit\n            width = 2 * err[0] * pars[""lon_0""].unit\n        return EllipseSkyRegion(\n            center=self.position, height=height, width=width, angle=phi\n        )\n\n    def evaluate_geom(self, geom):\n        """"""Evaluate model on `~gammapy.maps.Geom`.""""""\n        coords = geom.get_coord(frame=self.frame)\n        return self(coords.lon, coords.lat)\n\n    def integrate(self, geom):\n        """"""Integrate model on `~gammapy.maps.Geom`.""""""\n        values = self.evaluate_geom(geom)\n        data = values * geom.solid_angle()\n        return Map.from_geom(geom=geom, data=data.value, unit=data.unit)\n\n    def to_dict(self):\n        """"""Create dict for YAML serilisation""""""\n        data = super().to_dict()\n        data[""frame""] = self.frame\n        data[""parameters""] = data.pop(""parameters"")\n        return data\n\n    def plot(self, ax=None, geom=None, **kwargs):\n        """"""Plot spatial model.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        geom : `~gammapy.maps.WcsGeom`, optional\n            Geom to use for plotting.\n        **kwargs : dict\n            Keyword arguments passed to `~gammapy.maps.WcsMap.plot()`\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        """"""\n        if self.evaluation_radius is None and geom is None:\n            raise ValueError(\n                f""{self.__class__.__name__} requires geom to be defined for plotting.""\n            )\n\n        if geom is None:\n            width = 2 * max(self.evaluation_radius, 0.1 * u.deg)\n            geom = WcsGeom.create(\n                skydir=self.position, frame=self.frame, width=width, binsz=0.02\n            )\n\n        data = self.evaluate_geom(geom)\n        m = Map.from_geom(geom, data=data.value, unit=data.unit)\n        _, ax, _ = m.plot(ax=ax, **kwargs)\n\n        return ax\n\n    def plot_error(self, ax=None, **kwargs):\n        """"""Plot position error\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        **kwargs : dict\n            Keyword arguments passed to `~gammapy.maps.WcsMap.plot()`\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        """"""\n        import matplotlib.pyplot as plt\n\n        # plot center position\n        lon, lat = self.lon_0.value, self.lat_0.value\n\n        ax = plt.gca() if ax is None else ax\n\n        kwargs.setdefault(""marker"", ""x"")\n        kwargs.setdefault(""color"", ""red"")\n        kwargs.setdefault(""label"", ""position"")\n\n        ax.scatter(lon, lat, transform=ax.get_transform(self.frame), **kwargs)\n\n        # plot position error\n        if not np.all(self.covariance.data == 0):\n            region = self.position_error.to_pixel(ax.wcs)\n            artist = region.as_artist(facecolor=""none"", edgecolor=kwargs[""color""])\n            ax.add_artist(artist)\n\n        return ax\n\n\nclass PointSpatialModel(SpatialModel):\n    r""""""Point Source.\n\n    For more information see :ref:`point-spatial-model`.\n\n    Parameters\n    ----------\n    lon_0, lat_0 : `~astropy.coordinates.Angle`\n        Center position\n    frame : {""icrs"", ""galactic""}\n        Center position coordinate frame\n    """"""\n\n    tag = ""PointSpatialModel""\n    lon_0 = Parameter(""lon_0"", ""0 deg"")\n    lat_0 = Parameter(""lat_0"", ""0 deg"", min=-90, max=90)\n\n    @property\n    def evaluation_radius(self):\n        """"""Evaluation radius (`~astropy.coordinates.Angle`).\n\n        Set as zero degrees.\n        """"""\n        return 0 * u.deg\n\n    @staticmethod\n    def _grid_weights(x, y, x0, y0):\n        """"""Compute 4-pixel weights such that centroid is preserved.""""""\n        dx = np.abs(x - x0)\n        dx = np.where(dx < 1, 1 - dx, 0)\n\n        dy = np.abs(y - y0)\n        dy = np.where(dy < 1, 1 - dy, 0)\n\n        return dx * dy\n\n    def evaluate_geom(self, geom):\n        """"""Evaluate model on `~gammapy.maps.Geom`.""""""\n        values = self.integrate_geom(geom).data\n        return values / geom.solid_angle()\n\n    def integrate_geom(self, geom):\n        """"""Integrate model on `~gammapy.maps.Geom`\n\n        Parameters\n        ----------\n        geom : `Geom`\n            Map geometry\n\n        Returns\n        -------\n        flux : `Map`\n            Predicted flux map\n        """"""\n        x, y = geom.get_pix()[0:2]\n        x0, y0 = self.position.to_pixel(geom.wcs)\n        data = self._grid_weights(x, y, x0, y0)\n        return Map.from_geom(geom=geom, data=data, unit="""")\n\n    def to_region(self, **kwargs):\n        """"""Model outline (`~regions.PointSkyRegion`).""""""\n        return PointSkyRegion(center=self.position, **kwargs)\n\n\nclass GaussianSpatialModel(SpatialModel):\n    r""""""Two-dimensional Gaussian model.\n\n    For more information see :ref:`gaussian-spatial-model`.\n\n    Parameters\n    ----------\n    lon_0, lat_0 : `~astropy.coordinates.Angle`\n        Center position\n    sigma : `~astropy.coordinates.Angle`\n        Length of the major semiaxis of the Gaussian, in angular units.\n    e : `float`\n        Eccentricity of the Gaussian (:math:`0< e< 1`).\n    phi : `~astropy.coordinates.Angle`\n        Rotation angle :math:`\\phi`: of the major semiaxis.\n        Increases counter-clockwise from the North direction.\n    frame : {""icrs"", ""galactic""}\n        Center position coordinate frame\n    """"""\n\n    tag = ""GaussianSpatialModel""\n\n    lon_0 = Parameter(""lon_0"", ""0 deg"")\n    lat_0 = Parameter(""lat_0"", ""0 deg"", min=-90, max=90)\n    sigma = Parameter(""sigma"", ""1 deg"", min=0)\n    e = Parameter(""e"", 0, min=0, max=1, frozen=True)\n    phi = Parameter(""phi"", ""0 deg"", frozen=True)\n\n    @property\n    def evaluation_radius(self):\n        r""""""Evaluation radius (`~astropy.coordinates.Angle`).\n\n        Set as :math:`5\\sigma`.\n        """"""\n        return 5 * self.parameters[""sigma""].quantity\n\n    @staticmethod\n    def evaluate(lon, lat, lon_0, lat_0, sigma, e, phi):\n        """"""Evaluate model.""""""\n        sep = angular_separation(lon, lat, lon_0, lat_0)\n\n        if e == 0:\n            a = 1.0 - np.cos(sigma)\n            norm = (1 / (4 * np.pi * a * (1.0 - np.exp(-1.0 / a)))).value\n        else:\n            minor_axis, sigma_eff = compute_sigma_eff(\n                lon_0, lat_0, lon, lat, phi, sigma, e\n            )\n            a = 1.0 - np.cos(sigma_eff)\n            norm = (1 / (2 * np.pi * sigma * minor_axis)).to_value(""sr-1"")\n\n        exponent = -0.5 * ((1 - np.cos(sep)) / a)\n        return u.Quantity(norm * np.exp(exponent).value, ""sr-1"", copy=False)\n\n    def to_region(self, **kwargs):\n        """"""Model outline (`~regions.EllipseSkyRegion`).""""""\n        minor_axis = Angle(self.sigma.quantity * np.sqrt(1 - self.e.quantity ** 2))\n        return EllipseSkyRegion(\n            center=self.position,\n            height=2 * self.sigma.quantity,\n            width=2 * minor_axis,\n            angle=self.phi.quantity,\n            **kwargs,\n        )\n\n\nclass DiskSpatialModel(SpatialModel):\n    r""""""Constant disk model.\n\n    For more information see :ref:`disk-spatial-model`.\n\n    Parameters\n    ----------\n    lon_0, lat_0 : `~astropy.coordinates.Angle`\n        Center position\n    r_0 : `~astropy.coordinates.Angle`\n        :math:`a`: length of the major semiaxis, in angular units.\n    e : `float`\n        Eccentricity of the ellipse (:math:`0< e< 1`).\n    phi : `~astropy.coordinates.Angle`\n        Rotation angle :math:`\\phi`: of the major semiaxis.\n        Increases counter-clockwise from the North direction.\n    edge : `~astropy.coordinates.Angle`\n        Width of the edge. The width is defined as the range within the\n        smooth edges of the model drops from 95% to 5% of its amplitude.\n    frame : {""icrs"", ""galactic""}\n        Center position coordinate frame\n    """"""\n\n    tag = ""DiskSpatialModel""\n    lon_0 = Parameter(""lon_0"", ""0 deg"")\n    lat_0 = Parameter(""lat_0"", ""0 deg"", min=-90, max=90)\n    r_0 = Parameter(""r_0"", ""1 deg"", min=0)\n    e = Parameter(""e"", 0, min=0, max=1, frozen=True)\n    phi = Parameter(""phi"", ""0 deg"", frozen=True)\n    edge = Parameter(""edge"", ""0.01 deg"", frozen=True, min=0.01)\n\n    @property\n    def evaluation_radius(self):\n        """"""Evaluation radius (`~astropy.coordinates.Angle`).\n\n        Set to the length of the semi-major axis.\n        """"""\n        return self.r_0.quantity\n\n    @staticmethod\n    def _evaluate_norm_factor(r_0, e):\n        """"""Compute the normalization factor.""""""\n        semi_minor = r_0 * np.sqrt(1 - e ** 2)\n\n        def integral_fcn(x, a, b):\n            A = 1 / np.sin(a) ** 2\n            B = 1 / np.sin(b) ** 2\n            C = A - B\n            cs2 = np.cos(x) ** 2\n\n            return 1 - np.sqrt(1 - 1 / (B + C * cs2))\n\n        return (\n            2\n            * scipy.integrate.quad(\n                lambda x: integral_fcn(x, r_0, semi_minor), 0, np.pi\n            )[0]\n        ) ** -1\n\n    @staticmethod\n    def _evaluate_smooth_edge(x, width):\n        value = (x / width).to_value("""")\n        edge_width_95 = 2.326174307353347\n        return 0.5 * (1 - scipy.special.erf(value * edge_width_95))\n\n    @staticmethod\n    def evaluate(lon, lat, lon_0, lat_0, r_0, e, phi, edge):\n        """"""Evaluate model.""""""\n        sep = angular_separation(lon, lat, lon_0, lat_0)\n\n        if e == 0:\n            sigma_eff = r_0\n        else:\n            sigma_eff = compute_sigma_eff(lon_0, lat_0, lon, lat, phi, r_0, e)[1]\n\n        norm = DiskSpatialModel._evaluate_norm_factor(r_0, e)\n\n        in_ellipse = DiskSpatialModel._evaluate_smooth_edge(sep - sigma_eff, edge)\n        return u.Quantity(norm * in_ellipse, ""sr-1"", copy=False)\n\n    def to_region(self, **kwargs):\n        """"""Model outline (`~regions.EllipseSkyRegion`).""""""\n        minor_axis = Angle(self.r_0.quantity * np.sqrt(1 - self.e.quantity ** 2))\n        return EllipseSkyRegion(\n            center=self.position,\n            height=2 * self.r_0.quantity,\n            width=2 * minor_axis,\n            angle=self.phi.quantity,\n            **kwargs,\n        )\n\n\nclass ShellSpatialModel(SpatialModel):\n    r""""""Shell model.\n\n    For more information see :ref:`shell-spatial-model`.\n\n    Parameters\n    ----------\n    lon_0, lat_0 : `~astropy.coordinates.Angle`\n        Center position\n    radius : `~astropy.coordinates.Angle`\n        Inner radius, :math:`r_{in}`\n    width : `~astropy.coordinates.Angle`\n        Shell width\n    frame : {""icrs"", ""galactic""}\n        Center position coordinate frame\n    """"""\n\n    tag = ""ShellSpatialModel""\n    lon_0 = Parameter(""lon_0"", ""0 deg"")\n    lat_0 = Parameter(""lat_0"", ""0 deg"", min=-90, max=90)\n    radius = Parameter(""radius"", ""1 deg"")\n    width = Parameter(""width"", ""0.2 deg"")\n\n    @property\n    def evaluation_radius(self):\n        r""""""Evaluation radius (`~astropy.coordinates.Angle`).\n\n        Set to :math:`r_\\text{out}`.\n        """"""\n        return self.radius.quantity + self.width.quantity\n\n    @staticmethod\n    def evaluate(lon, lat, lon_0, lat_0, radius, width):\n        """"""Evaluate model.""""""\n        sep = angular_separation(lon, lat, lon_0, lat_0)\n        radius_out = radius + width\n\n        norm = 3 / (2 * np.pi * (radius_out ** 3 - radius ** 3))\n\n        with np.errstate(invalid=""ignore""):\n            # np.where and np.select do not work with quantities, so we use the\n            # workaround with indexing\n            value = np.sqrt(radius_out ** 2 - sep ** 2)\n            mask = [sep < radius]\n            value[mask] = (value - np.sqrt(radius ** 2 - sep ** 2))[mask]\n            value[sep > radius_out] = 0\n\n        return norm * value\n\n    def to_region(self, **kwargs):\n        """"""Model outline (`~regions.CircleAnnulusSkyRegion`).""""""\n        return CircleAnnulusSkyRegion(\n            center=self.position,\n            inner_radius=self.radius.quantity,\n            outer_radius=self.radius.quantity + self.width.quantity,\n            **kwargs,\n        )\n\n\nclass ConstantSpatialModel(SpatialModel):\n    """"""Spatially constant (isotropic) spatial model.\n\n    For more information see :ref:`constant-spatial-model`.\n\n    Parameters\n    ----------\n    value : `~astropy.units.Quantity`\n        Value\n    """"""\n\n    tag = ""ConstantSpatialModel""\n    value = Parameter(""value"", ""1 sr-1"", frozen=True)\n\n    frame = ""icrs""\n    evaluation_radius = None\n    position = SkyCoord(""0 deg"", ""0 deg"", frame=frame)\n\n    def to_dict(self):\n        """"""Create dict for YAML serilisation""""""\n        # redefined to ignore frame attribute from parent class\n        data = super().to_dict()\n        data.pop(""frame"")\n        data[""parameters""] = data.pop(""parameters"")\n        return data\n\n    @staticmethod\n    def evaluate(lon, lat, value):\n        """"""Evaluate model.""""""\n        return value\n\n    @staticmethod\n    def to_region(**kwargs):\n        """"""Model outline (`~regions.EllipseSkyRegion`).""""""\n        return EllipseSkyRegion(\n            center=SkyCoord(np.nan * u.deg, np.nan * u.deg),\n            height=np.nan * u.deg,\n            width=np.nan * u.deg,\n            angle=np.nan * u.deg,\n            **kwargs,\n        )\n\n\nclass TemplateSpatialModel(SpatialModel):\n    """"""Spatial sky map template model (2D).\n\n    This is for a 2D image. The unit of the map has to be equivalent to ``sr-1``.\n\n    Use `~gammapy.modeling.models.SkyDiffuseCube` for 3D cubes with\n    an energy axis.\n\n    For more information see :ref:`template-spatial-model`.\n\n    Parameters\n    ----------\n    map : `~gammapy.maps.Map`\n        Map template. The unit has to be equivalent to ``sr-1``.\n    norm : float\n        Norm parameter (multiplied with map values)\n    meta : dict, optional\n        Meta information, meta[\'filename\'] will be used for serialization\n    normalize : bool\n        Normalize the input map so that it integrates to unity.\n    interp_kwargs : dict\n        Interpolation keyword arguments passed to `gammapy.maps.Map.interp_by_coord`.\n        Default arguments are {\'interp\': \'linear\', \'fill_value\': 0}.\n    """"""\n\n    tag = ""TemplateSpatialModel""\n    norm = Parameter(""norm"", 1)\n\n    def __init__(\n        self,\n        map,\n        norm=norm.quantity,\n        meta=None,\n        normalize=True,\n        interp_kwargs=None,\n        filename=None,\n    ):\n        if (map.data < 0).any():\n            log.warning(""Diffuse map has negative values. Check and fix this!"")\n\n        if filename is not None:\n            filename = str(make_path(filename))\n\n        self.map = map\n        if not self.map.unit.is_equivalent(""sr-1""):\n            raise ValueError(""The map unit should be equivalent to sr-1"")\n        self.normalize = normalize\n        if normalize:\n            # Normalize the diffuse map model so that it integrates to unity.""""""\n            data = self.map.data / self.map.data.sum()\n            data /= self.map.geom.solid_angle().to_value(""sr"")\n            self.map = self.map.copy(data=data, unit=""sr-1"")\n\n        self.meta = dict() if meta is None else meta\n        interp_kwargs = {} if interp_kwargs is None else interp_kwargs\n        interp_kwargs.setdefault(""interp"", ""linear"")\n        interp_kwargs.setdefault(""fill_value"", 0)\n        self._interp_kwargs = interp_kwargs\n        self.filename = filename\n        super().__init__(norm=norm)\n\n    @property\n    def evaluation_radius(self):\n        """"""Evaluation radius (`~astropy.coordinates.Angle`).\n\n        Set to half of the maximal dimension of the map.\n        """"""\n        return np.max(self.map.geom.width) / 2.0\n\n    @classmethod\n    def read(cls, filename, normalize=True, **kwargs):\n        """"""Read spatial template model from FITS image.\n\n        The unit of the map has to be equivalent to ``sr-1``. If no unit the default is ``sr-1``.\n\n        Parameters\n        ----------\n        filename : str\n            FITS image filename.\n        normalize : bool\n            Normalize the input map so that it integrates to unity.\n        kwargs : dict\n            Keyword arguments passed to `Map.read()`.\n        """"""\n        m = Map.read(filename, **kwargs)\n\n        if not m.unit.is_equivalent(""sr-1""):\n            m.unit = ""sr-1""\n            log.warning(\n                ""Spatial template unit is not equivalent to sr^-1, unit changed to sr^-1""\n            )\n\n        return cls(m, normalize=normalize, filename=filename)\n\n    def evaluate(self, lon, lat, norm):\n        """"""Evaluate model.""""""\n        coord = {""lon"": lon.to_value(""deg""), ""lat"": lat.to_value(""deg"")}\n        val = self.map.interp_by_coord(coord, **self._interp_kwargs)\n        return u.Quantity(norm.value * val, self.map.unit, copy=False)\n\n    @property\n    def position(self):\n        """"""`~astropy.coordinates.SkyCoord`""""""\n        return self.map.geom.center_skydir\n\n    @property\n    def frame(self):\n        return self.position.frame.name\n\n    @classmethod\n    def from_dict(cls, data):\n        m = Map.read(data[""filename""])\n\n        if not m.unit.is_equivalent(""sr-1""):\n            m.unit = ""sr-1""\n            log.warning(\n                ""Spatial template unit is not equivalent to sr^-1, unit changed to sr^-1""\n            )\n\n        parameters = Parameters.from_dict(data[""parameters""])\n        return cls.from_parameters(\n            parameters=parameters,\n            map=m,\n            filename=data[""filename""],\n            normalize=data.get(""normalize"", True),\n        )\n\n    def to_dict(self):\n        """"""Create dict for YAML serilisation""""""\n        data = super().to_dict()\n        data[""filename""] = self.filename\n        data[""normalize""] = self.normalize\n        return data\n\n    def to_region(self, **kwargs):\n        """"""Model outline (`~regions.PolygonSkyRegion`).""""""\n        footprint = self.map.geom.wcs.calc_footprint()\n        return PolygonSkyRegion(\n            vertices=SkyCoord(footprint, unit=""deg"", frame=self.frame, **kwargs)\n        )\n'"
gammapy/modeling/models/spectral.py,47,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Spectral models for Gammapy.""""""\nimport operator\nimport numpy as np\nimport scipy.optimize\nimport scipy.special\nimport astropy.units as u\nfrom astropy import constants as const\nfrom astropy.table import Table\nfrom gammapy.maps import MapAxis\nfrom gammapy.maps.utils import edges_from_lo_hi\nfrom gammapy.modeling import Covariance, Parameter, Parameters\nfrom gammapy.utils.integrate import evaluate_integral_pwl, trapz_loglog\nfrom gammapy.utils.interpolation import ScaledRegularGridInterpolator\nfrom gammapy.utils.scripts import make_path\nfrom .core import Model\n\n\ndef integrate_spectrum(func, emin, emax, ndecade=100, intervals=False):\n    """"""Integrate 1d function using the log-log trapezoidal rule.\n\n    If scalar values for xmin and xmax are passed an oversampled grid is generated using the\n    ``ndecade`` keyword argument. If xmin and xmax arrays are passed, no\n    oversampling is performed and the integral is computed in the provided\n    grid.\n\n    Parameters\n    ----------\n    func : callable\n        Function to integrate.\n    emin : `~astropy.units.Quantity`\n        Integration range minimum\n    emax : `~astropy.units.Quantity`\n        Integration range minimum\n    ndecade : int, optional\n        Number of grid points per decade used for the integration.\n        Default : 100.\n    intervals : bool, optional\n        Return integrals in the grid not the sum, default: False\n    """"""\n    if emin.isscalar and emax.isscalar:\n        energies = MapAxis.from_energy_bounds(\n            emin=emin, emax=emax, nbin=ndecade, per_decade=True\n        ).edges\n    else:\n        energies = edges_from_lo_hi(emin, emax)\n\n    values = func(energies)\n\n    integral = trapz_loglog(values, energies)\n\n    if intervals:\n        return integral\n\n    return integral.sum()\n\n\nclass SpectralModel(Model):\n    """"""Spectral model base class.""""""\n\n    def __call__(self, energy):\n        kwargs = {par.name: par.quantity for par in self.parameters}\n        kwargs = self._convert_evaluate_unit(kwargs, energy)\n        return self.evaluate(energy, **kwargs)\n\n    @staticmethod\n    def _convert_evaluate_unit(kwargs_ref, energy):\n        kwargs = {}\n        for name, quantity in kwargs_ref.items():\n            if quantity.unit.physical_type == ""energy"":\n                quantity = quantity.to(energy.unit)\n            kwargs[name] = quantity\n        return kwargs\n\n    def __add__(self, model):\n        if not isinstance(model, SpectralModel):\n            model = ConstantSpectralModel(const=model)\n        return CompoundSpectralModel(self, model, operator.add)\n\n    def __radd__(self, model):\n        return self.__add__(model)\n\n    def __sub__(self, model):\n        if not isinstance(model, SpectralModel):\n            model = ConstantSpectralModel(const=model)\n        return CompoundSpectralModel(self, model, operator.sub)\n\n    def __rsub__(self, model):\n        return self.__sub__(model)\n\n    def _evaluate_gradient(self, energy, eps):\n        n = len(self.parameters)\n        f = self(energy)\n        shape = (n, len(np.atleast_1d(energy)))\n        df_dp = np.zeros(shape)\n\n        for idx, parameter in enumerate(self.parameters):\n            if parameter.frozen or eps[idx] == 0:\n                continue\n\n            parameter.value += eps[idx]\n            df = self(energy) - f\n            df_dp[idx] = df.value / eps[idx]\n\n            # Reset model to original parameter\n            parameter.value -= eps[idx]\n\n        return df_dp\n\n    def evaluate_error(self, energy, epsilon=1e-4):\n        """"""Evaluate spectral model with error propagation.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy at which to evaluate\n        epsilon : float\n            Step size of the gradient evaluation. Given as a\n            fraction of the parameter error.\n\n        Returns\n        -------\n        dnde, dnde_error : tuple of `~astropy.units.Quantity`\n            Tuple of flux and flux error.\n        """"""\n        p_cov = self.covariance\n        eps = np.sqrt(np.diag(p_cov)) * epsilon\n\n        df_dp = self._evaluate_gradient(energy, eps)\n        f_cov = df_dp.T @ p_cov @ df_dp\n        f_err = np.sqrt(np.diagonal(f_cov))\n\n        q = self(energy)\n        return u.Quantity([q.value, f_err], unit=q.unit)\n\n    def integral(self, emin, emax, **kwargs):\n        r""""""Integrate spectral model numerically.\n\n        .. math::\n            F(E_{min}, E_{max}) = \\int_{E_{min}}^{E_{max}} \\phi(E) dE\n\n        If array input for ``emin`` and ``emax`` is given you have to set\n        ``intervals=True`` if you want the integral in each energy bin.\n\n        Parameters\n        ----------\n        emin, emax : `~astropy.units.Quantity`\n            Lower and upper bound of integration range.\n        **kwargs : dict\n            Keyword arguments passed to :func:`~gammapy.utils.integrate.integrate_spectrum`\n        """"""\n        return integrate_spectrum(self, emin, emax, **kwargs)\n\n    def energy_flux(self, emin, emax, **kwargs):\n        r""""""Compute energy flux in given energy range.\n\n        .. math::\n            G(E_{min}, E_{max}) = \\int_{E_{min}}^{E_{max}} E \\phi(E) dE\n\n        Parameters\n        ----------\n        emin, emax : `~astropy.units.Quantity`\n            Lower and upper bound of integration range.\n        **kwargs : dict\n            Keyword arguments passed to func:`~gammapy.utils.integrate.integrate_spectrum`\n        """"""\n\n        def f(x):\n            return x * self(x)\n\n        return integrate_spectrum(f, emin, emax, **kwargs)\n\n    def plot(\n        self,\n        energy_range,\n        ax=None,\n        energy_unit=""TeV"",\n        flux_unit=""cm-2 s-1 TeV-1"",\n        energy_power=0,\n        n_points=100,\n        **kwargs,\n    ):\n        """"""Plot spectral model curve.\n\n        kwargs are forwarded to `matplotlib.pyplot.plot`\n\n        By default a log-log scaling of the axes is used, if you want to change\n        the y axis scaling to linear you can use::\n\n            from gammapy.modeling.models import ExpCutoffPowerLawSpectralModel\n            from astropy import units as u\n\n            pwl = ExpCutoffPowerLawSpectralModel()\n            ax = pwl.plot(energy_range=(0.1, 100) * u.TeV)\n            ax.set_yscale(\'linear\')\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        energy_range : `~astropy.units.Quantity`\n            Plot range\n        energy_unit : str, `~astropy.units.Unit`, optional\n            Unit of the energy axis\n        flux_unit : str, `~astropy.units.Unit`, optional\n            Unit of the flux axis\n        energy_power : int, optional\n            Power of energy to multiply flux axis with\n        n_points : int, optional\n            Number of evaluation nodes\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        emin, emax = energy_range\n        energy = MapAxis.from_energy_bounds(emin, emax, n_points, energy_unit).edges\n\n        # evaluate model\n        flux = self(energy).to(flux_unit)\n\n        y = self._plot_scale_flux(energy, flux, energy_power)\n\n        ax.plot(energy.value, y.value, **kwargs)\n\n        self._plot_format_ax(ax, energy, y, energy_power)\n        return ax\n\n    def plot_error(\n        self,\n        energy_range,\n        ax=None,\n        energy_unit=""TeV"",\n        flux_unit=""cm-2 s-1 TeV-1"",\n        energy_power=0,\n        n_points=100,\n        **kwargs,\n    ):\n        """"""Plot spectral model error band.\n\n        .. note::\n\n            This method calls ``ax.set_yscale(""log"", nonposy=\'clip\')`` and\n            ``ax.set_xscale(""log"", nonposx=\'clip\')`` to create a log-log representation.\n            The additional argument ``nonposx=\'clip\'`` avoids artefacts in the plot,\n            when the error band extends to negative values (see also\n            https://github.com/matplotlib/matplotlib/issues/8623).\n\n            When you call ``plt.loglog()`` or ``plt.semilogy()`` explicitely in your\n            plotting code and the error band extends to negative values, it is not\n            shown correctly. To circumvent this issue also use\n            ``plt.loglog(nonposx=\'clip\', nonposy=\'clip\')``\n            or ``plt.semilogy(nonposy=\'clip\')``.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        energy_range : `~astropy.units.Quantity`\n            Plot range\n        energy_unit : str, `~astropy.units.Unit`, optional\n            Unit of the energy axis\n        flux_unit : str, `~astropy.units.Unit`, optional\n            Unit of the flux axis\n        energy_power : int, optional\n            Power of energy to multiply flux axis with\n        n_points : int, optional\n            Number of evaluation nodes\n        **kwargs : dict\n            Keyword arguments forwarded to `matplotlib.pyplot.fill_between`\n\n        Returns\n        -------\n        ax : `~matplotlib.axes.Axes`, optional\n            Axis\n        """"""\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        kwargs.setdefault(""facecolor"", ""black"")\n        kwargs.setdefault(""alpha"", 0.2)\n        kwargs.setdefault(""linewidth"", 0)\n\n        emin, emax = energy_range\n        energy = MapAxis.from_energy_bounds(emin, emax, n_points, energy_unit).edges\n\n        flux, flux_err = self.evaluate_error(energy).to(flux_unit)\n\n        y_lo = self._plot_scale_flux(energy, flux - flux_err, energy_power)\n        y_hi = self._plot_scale_flux(energy, flux + flux_err, energy_power)\n\n        where = (energy >= energy_range[0]) & (energy <= energy_range[1])\n        ax.fill_between(energy.value, y_lo.value, y_hi.value, where=where, **kwargs)\n\n        self._plot_format_ax(ax, energy, y_lo, energy_power)\n        return ax\n\n    @staticmethod\n    def _plot_format_ax(ax, energy, y, energy_power):\n        ax.set_xlabel(f""Energy [{energy.unit}]"")\n        if energy_power > 0:\n            ax.set_ylabel(f""E{energy_power} * Flux [{y.unit}]"")\n        else:\n            ax.set_ylabel(f""Flux [{y.unit}]"")\n\n        ax.set_xscale(""log"", nonposx=""clip"")\n        ax.set_yscale(""log"", nonposy=""clip"")\n\n    @staticmethod\n    def _plot_scale_flux(energy, flux, energy_power):\n        try:\n            eunit = [_ for _ in flux.unit.bases if _.physical_type == ""energy""][0]\n        except IndexError:\n            eunit = energy.unit\n        y = flux * np.power(energy, energy_power)\n        return y.to(flux.unit * eunit ** energy_power)\n\n    def spectral_index(self, energy, epsilon=1e-5):\n        """"""Compute spectral index at given energy.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy at which to estimate the index\n        epsilon : float\n            Fractional energy increment to use for determining the spectral index.\n\n        Returns\n        -------\n        index : float\n            Estimated spectral index.\n        """"""\n        f1 = self(energy)\n        f2 = self(energy * (1 + epsilon))\n        return np.log(f1 / f2) / np.log(1 + epsilon)\n\n    def inverse(self, value, emin=0.1 * u.TeV, emax=100 * u.TeV):\n        """"""Return energy for a given function value of the spectral model.\n\n        Calls the `scipy.optimize.brentq` numerical root finding method.\n\n        Parameters\n        ----------\n        value : `~astropy.units.Quantity`\n            Function value of the spectral model.\n        emin : `~astropy.units.Quantity`\n            Lower bracket value in case solution is not unique.\n        emax : `~astropy.units.Quantity`\n            Upper bracket value in case solution is not unique.\n\n        Returns\n        -------\n        energy : `~astropy.units.Quantity`\n            Energies at which the model has the given ``value``.\n        """"""\n        eunit = ""TeV""\n\n        energies = []\n        for val in np.atleast_1d(value):\n\n            def f(x):\n                # scale by 1e12 to achieve better precision\n                energy = u.Quantity(x, eunit, copy=False)\n                y = self(energy).to_value(value.unit)\n                return 1e12 * (y - val.value)\n\n            energy = scipy.optimize.brentq(\n                f, emin.to_value(eunit), emax.to_value(eunit)\n            )\n            energies.append(energy)\n\n        return u.Quantity(energies, eunit, copy=False)\n\n\nclass ConstantSpectralModel(SpectralModel):\n    r""""""Constant model.\n\n    For more information see :ref:`constant-spectral-model`.\n\n    Parameters\n    ----------\n    const : `~astropy.units.Quantity`\n        :math:`k`\n    """"""\n\n    tag = ""ConstantSpectralModel""\n    const = Parameter(""const"", ""1e-12 cm-2 s-1 TeV-1"")\n\n    @staticmethod\n    def evaluate(energy, const):\n        """"""Evaluate the model (static function).""""""\n        return np.ones(np.atleast_1d(energy).shape) * const\n\n\nclass CompoundSpectralModel(SpectralModel):\n    """"""Arithmetic combination of two spectral models.\n\n    For more information see :ref:`compound-spectral-model`.\n    """"""\n\n    tag = ""CompoundSpectralModel""\n\n    def __init__(self, model1, model2, operator):\n        self.model1 = model1\n        self.model2 = model2\n        self.operator = operator\n        super().__init__()\n\n    @property\n    def parameters(self):\n        return self.model1.parameters + self.model2.parameters\n\n    def __str__(self):\n        return (\n            f""{self.__class__.__name__}\\n""\n            f""    Component 1 : {self.model1}\\n""\n            f""    Component 2 : {self.model2}\\n""\n            f""    Operator : {self.operator}\\n""\n        )\n\n    def __call__(self, energy):\n        val1 = self.model1(energy)\n        val2 = self.model2(energy)\n        return self.operator(val1, val2)\n\n    def to_dict(self):\n        return {\n            ""model1"": self.model1.to_dict(),\n            ""model2"": self.model2.to_dict(),\n            ""operator"": self.operator,\n        }\n\n\nclass PowerLawSpectralModel(SpectralModel):\n    r""""""Spectral power-law model.\n\n    For more information see :ref:`powerlaw-spectral-model`.\n\n    Parameters\n    ----------\n    index : `~astropy.units.Quantity`\n        :math:`\\Gamma`\n    amplitude : `~astropy.units.Quantity`\n        :math:`\\phi_0`\n    reference : `~astropy.units.Quantity`\n        :math:`E_0`\n    """"""\n\n    tag = ""PowerLawSpectralModel""\n    index = Parameter(""index"", 2.0)\n    amplitude = Parameter(""amplitude"", ""1e-12 cm-2 s-1 TeV-1"")\n    reference = Parameter(""reference"", ""1 TeV"", frozen=True)\n    evaluate_integral = staticmethod(evaluate_integral_pwl)\n\n    @staticmethod\n    def evaluate(energy, index, amplitude, reference):\n        """"""Evaluate the model (static function).""""""\n        return amplitude * np.power((energy / reference), -index)\n\n    @staticmethod\n    def evaluate_energy_flux(emin, emax, index, amplitude, reference):\n        """"""Evaluate the energy flux (static function)""""""\n        val = -1 * index + 2\n\n        prefactor = amplitude * reference ** 2 / val\n        upper = (emax / reference) ** val\n        lower = (emin / reference) ** val\n        energy_flux = prefactor * (upper - lower)\n\n        mask = np.isclose(val, 0)\n\n        if mask.any():\n            # see https://www.wolframalpha.com/input/?i=a+*+x+*+(x%2Fb)+%5E+(-2)\n            # for reference\n            energy_flux[mask] = amplitude * reference ** 2 * np.log(emax / emin)[mask]\n\n        return energy_flux\n\n    def integral(self, emin, emax, **kwargs):\n        r""""""Integrate power law analytically.\n\n        .. math::\n            F(E_{min}, E_{max}) = \\int_{E_{min}}^{E_{max}}\\phi(E)dE = \\left.\n            \\phi_0 \\frac{E_0}{-\\Gamma + 1} \\left( \\frac{E}{E_0} \\right)^{-\\Gamma + 1}\n            \\right \\vert _{E_{min}}^{E_{max}}\n\n        Parameters\n        ----------\n        emin, emax : `~astropy.units.Quantity`\n            Lower and upper bound of integration range\n        """"""\n        kwargs = {par.name: par.quantity for par in self.parameters}\n        kwargs = self._convert_evaluate_unit(kwargs, emin)\n        return self.evaluate_integral(emin=emin, emax=emax, **kwargs)\n\n    def energy_flux(self, emin, emax):\n        r""""""Compute energy flux in given energy range analytically.\n\n        .. math::\n            G(E_{min}, E_{max}) = \\int_{E_{min}}^{E_{max}}E \\phi(E)dE = \\left.\n            \\phi_0 \\frac{E_0^2}{-\\Gamma + 2} \\left( \\frac{E}{E_0} \\right)^{-\\Gamma + 2}\n            \\right \\vert _{E_{min}}^{E_{max}}\n\n        Parameters\n        ----------\n        emin, emax : `~astropy.units.Quantity`\n            Lower and upper bound of integration range.\n        """"""\n        kwargs = {par.name: par.quantity for par in self.parameters}\n        kwargs = self._convert_evaluate_unit(kwargs, emin)\n        return self.evaluate_energy_flux(emin=emin, emax=emax, **kwargs)\n\n    def inverse(self, value):\n        """"""Return energy for a given function value of the spectral model.\n\n        Parameters\n        ----------\n        value : `~astropy.units.Quantity`\n            Function value of the spectral model.\n        """"""\n        base = value / self.amplitude.quantity\n        return self.reference.quantity * np.power(base, -1.0 / self.index.value)\n\n    @property\n    def pivot_energy(self):\n        r""""""The decorrelation energy is defined as:\n\n        .. math::\n\n            E_D = E_0 * \\exp{cov(\\phi_0, \\Gamma) / (\\phi_0 \\Delta \\Gamma^2)}\n\n        Formula (1) in https://arxiv.org/pdf/0910.4881.pdf\n        """"""\n        index_err = self.index.error\n        reference = self.reference.quantity\n        amplitude = self.amplitude.quantity\n        cov_index_ampl = self.covariance.data[0, 1] * amplitude.unit\n        return reference * np.exp(cov_index_ampl / (amplitude * index_err ** 2))\n\n\nclass PowerLaw2SpectralModel(SpectralModel):\n    r""""""Spectral power-law model with integral as amplitude parameter.\n\n    For more information see :ref:`powerlaw2-spectral-model`.\n\n    Parameters\n    ----------\n    index : `~astropy.units.Quantity`\n        Spectral index :math:`\\Gamma`\n    amplitude : `~astropy.units.Quantity`\n        Integral flux :math:`F_0`.\n    emin : `~astropy.units.Quantity`\n        Lower energy limit :math:`E_{0, min}`.\n    emax : `~astropy.units.Quantity`\n        Upper energy limit :math:`E_{0, max}`.\n    """"""\n\n    tag = ""PowerLaw2SpectralModel""\n\n    amplitude = Parameter(""amplitude"", ""1e-12 cm-2 s-1"")\n    index = Parameter(""index"", 2)\n    emin = Parameter(""emin"", ""0.1 TeV"", frozen=True)\n    emax = Parameter(""emax"", ""100 TeV"", frozen=True)\n\n    @staticmethod\n    def evaluate(energy, amplitude, index, emin, emax):\n        """"""Evaluate the model (static function).""""""\n        top = -index + 1\n\n        # to get the energies dimensionless we use a modified formula\n        bottom = emax - emin * (emin / emax) ** (-index)\n        return amplitude * (top / bottom) * np.power(energy / emax, -index)\n\n    def integral(self, emin, emax, **kwargs):\n        r""""""Integrate power law analytically.\n\n        .. math::\n            F(E_{min}, E_{max}) = F_0 \\cdot \\frac{E_{max}^{\\Gamma + 1} \\\n                                - E_{min}^{\\Gamma + 1}}{E_{0, max}^{\\Gamma + 1} \\\n                                - E_{0, min}^{\\Gamma + 1}}\n\n        Parameters\n        ----------\n        emin, emax : `~astropy.units.Quantity`\n            Lower and upper bound of integration range.\n        """"""\n        temp1 = np.power(emax, -self.index.value + 1)\n        temp2 = np.power(emin, -self.index.value + 1)\n        top = temp1 - temp2\n\n        temp1 = np.power(self.emax.quantity, -self.index.value + 1)\n        temp2 = np.power(self.emin.quantity, -self.index.value + 1)\n        bottom = temp1 - temp2\n\n        return self.amplitude.quantity * top / bottom\n\n    def inverse(self, value):\n        """"""Return energy for a given function value of the spectral model.\n\n        Parameters\n        ----------\n        value : `~astropy.units.Quantity`\n            Function value of the spectral model.\n        """"""\n        amplitude = self.amplitude.quantity\n        index = self.index.value\n        emin = self.emin.quantity\n        emax = self.emax.quantity\n\n        # to get the energies dimensionless we use a modified formula\n        top = -index + 1\n        bottom = emax - emin * (emin / emax) ** (-index)\n        term = (bottom / top) * (value / amplitude)\n        return np.power(term.to_value(""""), -1.0 / index) * emax\n\n\nclass SmoothBrokenPowerLawSpectralModel(SpectralModel):\n    r""""""Spectral smooth broken power-law model.\n\n    For more information see :ref:`smooth-broken-powerlaw-spectral-model`.\n\n    Parameters\n    ----------\n    index1 : `~astropy.units.Quantity`\n        :math:`\\Gamma1`\n    index2 : `~astropy.units.Quantity`\n        :math:`\\Gamma2`\n    amplitude : `~astropy.units.Quantity`\n        :math:`\\phi_0`\n    reference : `~astropy.units.Quantity`\n        :math:`E_0`\n    ebreak : `~astropy.units.Quantity`\n        :math:`E_{break}`\n    beta : `~astropy.units.Quantity`\n        :math:`\\beta`\n    """"""\n\n    tag = ""SmoothBrokenPowerLawSpectralModel""\n    index1 = Parameter(""index1"", 2.0)\n    index2 = Parameter(""index2"", 2.0)\n    amplitude = Parameter(""amplitude"", ""1e-12 cm-2 s-1 TeV-1"")\n    ebreak = Parameter(""ebreak"", ""1 TeV"")\n    reference = Parameter(""reference"", ""1 TeV"", frozen=True)\n    beta = Parameter(""beta"", 1, frozen=True)\n\n    @staticmethod\n    def evaluate(energy, index1, index2, amplitude, ebreak, reference, beta):\n        """"""Evaluate the model (static function).""""""\n        beta *= np.sign(index2 - index1)\n        pwl = amplitude * (energy / reference) ** (-index1)\n        brk = (1 + (energy / ebreak) ** ((index2 - index1) / beta)) ** (-beta)\n        return pwl * brk\n\n\nclass ExpCutoffPowerLawSpectralModel(SpectralModel):\n    r""""""Spectral exponential cutoff power-law model.\n\n    For more information see :ref:`exp-cutoff-powerlaw-spectral-model`.\n\n    Parameters\n    ----------\n    index : `~astropy.units.Quantity`\n        :math:`\\Gamma`\n    amplitude : `~astropy.units.Quantity`\n        :math:`\\phi_0`\n    reference : `~astropy.units.Quantity`\n        :math:`E_0`\n    lambda_ : `~astropy.units.Quantity`\n        :math:`\\lambda`\n    alpha : `~astropy.units.Quantity`\n        :math:`\\alpha`\n    """"""\n\n    tag = ""ExpCutoffPowerLawSpectralModel""\n\n    index = Parameter(""index"", 1.5)\n    amplitude = Parameter(""amplitude"", ""1e-12 cm-2 s-1 TeV-1"")\n    reference = Parameter(""reference"", ""1 TeV"", frozen=True)\n    lambda_ = Parameter(""lambda_"", ""0.1 TeV-1"")\n    alpha = Parameter(""alpha"", ""1.0"", frozen=True)\n\n    @staticmethod\n    def evaluate(energy, index, amplitude, reference, lambda_, alpha):\n        """"""Evaluate the model (static function).""""""\n        pwl = amplitude * (energy / reference) ** (-index)\n        cutoff = np.exp(-np.power(energy * lambda_, alpha))\n\n        return pwl * cutoff\n\n    @property\n    def e_peak(self):\n        r""""""Spectral energy distribution peak energy (`~astropy.units.Quantity`).\n\n        This is the peak in E^2 x dN/dE and is given by:\n\n        .. math::\n            E_{Peak} =  \\left(\\frac{2 - \\Gamma}{\\alpha}\\right)^{1/\\alpha} / \\lambda\n        """"""\n        reference = self.reference.quantity\n        index = self.index.quantity\n        lambda_ = self.lambda_.quantity\n        alpha = self.alpha.quantity\n\n        if index >= 2 or lambda_ == 0.0 or alpha == 0.0:\n            return np.nan * reference.unit\n        else:\n            return np.power((2 - index) / alpha, 1 / alpha) / lambda_\n\n\nclass ExpCutoffPowerLaw3FGLSpectralModel(SpectralModel):\n    r""""""Spectral exponential cutoff power-law model used for 3FGL.\n\n    For more information see :ref:`exp-cutoff-powerlaw-3fgl-spectral-model`.\n\n    Parameters\n    ----------\n    index : `~astropy.units.Quantity`\n        :math:`\\Gamma`\n    amplitude : `~astropy.units.Quantity`\n        :math:`\\phi_0`\n    reference : `~astropy.units.Quantity`\n        :math:`E_0`\n    ecut : `~astropy.units.Quantity`\n        :math:`E_{C}`\n    """"""\n\n    tag = ""ExpCutoffPowerLaw3FGLSpectralModel""\n    index = Parameter(""index"", 1.5)\n    amplitude = Parameter(""amplitude"", ""1e-12 cm-2 s-1 TeV-1"")\n    reference = Parameter(""reference"", ""1 TeV"", frozen=True)\n    ecut = Parameter(""ecut"", ""10 TeV"")\n\n    @staticmethod\n    def evaluate(energy, index, amplitude, reference, ecut):\n        """"""Evaluate the model (static function).""""""\n        pwl = amplitude * (energy / reference) ** (-index)\n        cutoff = np.exp((reference - energy) / ecut)\n        return pwl * cutoff\n\n\nclass SuperExpCutoffPowerLaw3FGLSpectralModel(SpectralModel):\n    r""""""Spectral super exponential cutoff power-law model used for 3FGL.\n\n    For more information see :ref:`super-exp-cutoff-powerlaw-3fgl-spectral-model`.\n\n    .. math::\n        \\phi(E) = \\phi_0 \\cdot \\left(\\frac{E}{E_0}\\right)^{-\\Gamma_1}\n                  \\exp \\left( \\left(\\frac{E_0}{E_{C}} \\right)^{\\Gamma_2} -\n                              \\left(\\frac{E}{E_{C}} \\right)^{\\Gamma_2}\n                              \\right)\n\n    Parameters\n    ----------\n    index_1 : `~astropy.units.Quantity`\n        :math:`\\Gamma_1`\n    index_2 : `~astropy.units.Quantity`\n        :math:`\\Gamma_2`\n    amplitude : `~astropy.units.Quantity`\n        :math:`\\phi_0`\n    reference : `~astropy.units.Quantity`\n        :math:`E_0`\n    ecut : `~astropy.units.Quantity`\n        :math:`E_{C}`\n    """"""\n\n    tag = ""SuperExpCutoffPowerLaw3FGLSpectralModel""\n    amplitude = Parameter(""amplitude"", ""1e-12 cm-2 s-1 TeV-1"")\n    reference = Parameter(""reference"", ""1 TeV"", frozen=True)\n    ecut = Parameter(""ecut"", ""10 TeV"")\n    index_1 = Parameter(""index_1"", 1.5)\n    index_2 = Parameter(""index_2"", 2)\n\n    @staticmethod\n    def evaluate(energy, amplitude, reference, ecut, index_1, index_2):\n        """"""Evaluate the model (static function).""""""\n        pwl = amplitude * (energy / reference) ** (-index_1)\n        cutoff = np.exp((reference / ecut) ** index_2 - (energy / ecut) ** index_2)\n        return pwl * cutoff\n\n\nclass SuperExpCutoffPowerLaw4FGLSpectralModel(SpectralModel):\n    r""""""Spectral super exponential cutoff power-law model used for 4FGL.\n\n    For more information see :ref:`super-exp-cutoff-powerlaw-4fgl-spectral-model`.\n\n    Parameters\n    ----------\n    index_1 : `~astropy.units.Quantity`\n        :math:`\\Gamma_1`\n    index_2 : `~astropy.units.Quantity`\n        :math:`\\Gamma_2`\n    amplitude : `~astropy.units.Quantity`\n        :math:`\\phi_0`\n    reference : `~astropy.units.Quantity`\n        :math:`E_0`\n    expfactor : `~astropy.units.Quantity`\n        :math:`a`, given as dimensionless value but\n        internally assumes unit of :math:`[E_0]` power :math:`-\\Gamma_2`\n    """"""\n\n    tag = ""SuperExpCutoffPowerLaw4FGLSpectralModel""\n    amplitude = Parameter(""amplitude"", ""1e-12 cm-2 s-1 TeV-1"")\n    reference = Parameter(""reference"", ""1 TeV"", frozen=True)\n    expfactor = Parameter(""expfactor"", ""1e-2"")\n    index_1 = Parameter(""index_1"", 1.5)\n    index_2 = Parameter(""index_2"", 2)\n\n    @staticmethod\n    def evaluate(energy, amplitude, reference, expfactor, index_1, index_2):\n        """"""Evaluate the model (static function).""""""\n        pwl = amplitude * (energy / reference) ** (-index_1)\n        cutoff = np.exp(\n            expfactor\n            / reference.unit ** index_2\n            * (reference ** index_2 - energy ** index_2)\n        )\n        return pwl * cutoff\n\n\nclass LogParabolaSpectralModel(SpectralModel):\n    r""""""Spectral log parabola model.\n\n    For more information see :ref:`logparabola-spectral-model`.\n\n    Parameters\n    ----------\n    amplitude : `~astropy.units.Quantity`\n        :math:`\\phi_0`\n    reference : `~astropy.units.Quantity`\n        :math:`E_0`\n    alpha : `~astropy.units.Quantity`\n        :math:`\\alpha`\n    beta : `~astropy.units.Quantity`\n        :math:`\\beta`\n    """"""\n\n    tag = ""LogParabolaSpectralModel""\n    amplitude = Parameter(""amplitude"", ""1e-12 cm-2 s-1 TeV-1"")\n    reference = Parameter(""reference"", ""10 TeV"", frozen=True)\n    alpha = Parameter(""alpha"", 2)\n    beta = Parameter(""beta"", 1)\n\n    @classmethod\n    def from_log10(cls, amplitude, reference, alpha, beta):\n        """"""Construct from :math:`log_{10}` parametrization.""""""\n        beta_ = beta / np.log(10)\n        return cls(amplitude=amplitude, reference=reference, alpha=alpha, beta=beta_)\n\n    @staticmethod\n    def evaluate(energy, amplitude, reference, alpha, beta):\n        """"""Evaluate the model (static function).""""""\n        xx = energy / reference\n        exponent = -alpha - beta * np.log(xx)\n        return amplitude * np.power(xx, exponent)\n\n    @property\n    def e_peak(self):\n        r""""""Spectral energy distribution peak energy (`~astropy.units.Quantity`).\n\n        This is the peak in E^2 x dN/dE and is given by:\n\n        .. math::\n            E_{Peak} = E_{0} \\exp{ (2 - \\alpha) / (2 * \\beta)}\n        """"""\n        reference = self.reference.quantity\n        alpha = self.alpha.quantity\n        beta = self.beta.quantity\n        return reference * np.exp((2 - alpha) / (2 * beta))\n\n\nclass TemplateSpectralModel(SpectralModel):\n    """"""A model generated from a table of energy and value arrays.\n\n    For more information see :ref:`template-spectral-model`.\n\n    Parameters\n    ----------\n    energy : `~astropy.units.Quantity`\n        Array of energies at which the model values are given\n    values : array\n        Array with the values of the model at energies ``energy``.\n    norm : float\n        Model scale that is multiplied to the supplied arrays. Defaults to 1.\n    interp_kwargs : dict\n        Interpolation keyword arguments pass to `scipy.interpolate.interp1d`.\n        By default all values outside the interpolation range are set to zero.\n        If you want to apply linear extrapolation you can pass `interp_kwargs={\'fill_value\':\n        \'extrapolate\', \'kind\': \'linear\'}`. If you want to choose the interpolation\n        scaling applied to values, you can use `interp_kwargs={""values_scale"": ""log""}`.\n    meta : dict, optional\n        Meta information, meta[\'filename\'] will be used for serialization\n    """"""\n\n    tag = ""TemplateSpectralModel""\n    norm = Parameter(""norm"", 1, unit="""")\n    tilt = Parameter(""tilt"", 0, unit="""", frozen=True)\n    reference = Parameter(""reference"", ""1 TeV"", frozen=True)\n\n    def __init__(\n        self,\n        energy,\n        values,\n        norm=norm.quantity,\n        tilt=tilt.quantity,\n        reference=reference.quantity,\n        interp_kwargs=None,\n        meta=None,\n    ):\n        self.energy = energy\n        self.values = u.Quantity(values, copy=False)\n        self.meta = dict() if meta is None else meta\n        interp_kwargs = interp_kwargs or {}\n        interp_kwargs.setdefault(""values_scale"", ""log"")\n        interp_kwargs.setdefault(""points_scale"", (""log"",))\n\n        self._evaluate = ScaledRegularGridInterpolator(\n            points=(energy,), values=values, **interp_kwargs\n        )\n\n        super().__init__(norm=norm, tilt=tilt, reference=reference)\n\n    @classmethod\n    def read_xspec_model(cls, filename, param, **kwargs):\n        """"""Read XSPEC table model.\n\n        The input is a table containing absorbed values from a XSPEC model as a\n        function of energy.\n\n        TODO: Format of the file should be described and discussed in\n        https://gamma-astro-data-formats.readthedocs.io/en/latest/index.html\n\n        Parameters\n        ----------\n        filename : str\n            File containing the XSPEC model\n        param : float\n            Model parameter value\n\n        Examples\n        --------\n        Fill table from an EBL model (Franceschini, 2008)\n\n        >>> from gammapy.modeling.models import TemplateSpectralModel\n        >>> filename = \'$GAMMAPY_DATA/ebl/ebl_franceschini.fits.gz\'\n        >>> table_model = TemplateSpectralModel.read_xspec_model(filename=filename, param=0.3)\n        """"""\n        filename = make_path(filename)\n\n        # Check if parameter value is in range\n        table_param = Table.read(filename, hdu=""PARAMETERS"")\n        pmin = table_param[""MINIMUM""]\n        pmax = table_param[""MAXIMUM""]\n        if param < pmin or param > pmax:\n            raise ValueError(f""Out of range: param={param}, min={pmin}, max={pmax}"")\n\n        # Get energy values\n        table_energy = Table.read(filename, hdu=""ENERGIES"")\n        energy_lo = table_energy[""ENERG_LO""]\n        energy_hi = table_energy[""ENERG_HI""]\n\n        # set energy to log-centers\n        energy = np.sqrt(energy_lo * energy_hi)\n\n        # Get spectrum values (no interpolation, take closest value for param)\n        table_spectra = Table.read(filename, hdu=""SPECTRA"")\n        idx = np.abs(table_spectra[""PARAMVAL""] - param).argmin()\n        values = u.Quantity(table_spectra[idx][1], """", copy=False)  # no dimension\n\n        kwargs.setdefault(""interp_kwargs"", {""values_scale"": ""lin""})\n        return cls(energy=energy, values=values, **kwargs)\n\n    def evaluate(self, energy, norm, tilt, reference):\n        """"""Evaluate the model (static function).""""""\n        values = self._evaluate((energy,), clip=True)\n        tilt_factor = np.power(energy / reference, -tilt)\n        return norm * values * tilt_factor\n\n    def to_dict(self):\n        return {\n            ""type"": self.tag,\n            ""parameters"": self.parameters.to_dict(),\n            ""energy"": {\n                ""data"": self.energy.data.tolist(),\n                ""unit"": str(self.energy.unit),\n            },\n            ""values"": {\n                ""data"": self.values.data.tolist(),\n                ""unit"": str(self.values.unit),\n            },\n        }\n\n    @classmethod\n    def from_dict(cls, data):\n        parameters = Parameters.from_dict(data[""parameters""])\n        energy = u.Quantity(data[""energy""][""data""], data[""energy""][""unit""])\n        values = u.Quantity(data[""values""][""data""], data[""values""][""unit""])\n        return cls.from_parameters(parameters, energy=energy, values=values)\n\n\nclass ScaleSpectralModel(SpectralModel):\n    """"""Wrapper to scale another spectral model by a norm factor.\n\n    Parameters\n    ----------\n    model : `SpectralModel`\n        Spectral model to wrap.\n    norm : float\n        Multiplicative norm factor for the model value.\n    """"""\n\n    tag = ""ScaleSpectralModel""\n    norm = Parameter(""norm"", 1, unit="""")\n\n    def __init__(self, model, norm=norm.quantity):\n        self.model = model\n        self._covariance = None\n        super().__init__(norm=norm)\n\n    def evaluate(self, energy, norm):\n        return norm * self.model(energy)\n\n    def integral(self, emin, emax, **kwargs):\n        return self.norm.value * self.model.integral(emin, emax, **kwargs)\n\n\nclass Absorption:\n    r""""""Gamma-ray absorption models.\n\n    For more information see :ref:`absorption-spectral-model`.\n\n    Parameters\n    ----------\n    energy : `~astropy.units.Quantity`\n        Energy node values\n    param : `~astropy.units.Quantity`\n        Parameter node values\n    data : `~astropy.units.Quantity`\n        Model value\n    filename : str\n        Filename of the absorption model used for serialisation.\n    interp_kwargs : dict\n        Interpolation option passed to `ScaledRegularGridInterpolator`.\n        By default the models are extrapolated outside the range. To prevent\n        this and raise an error instead use interp_kwargs = {""extrapolate"": False}\n    """"""\n\n    tag = ""Absorption""\n\n    def __init__(self, energy, param, data, filename=None, interp_kwargs=None):\n        self.data = data\n        self.filename = filename\n        # set values log centers\n        self.param = param\n        self.energy = energy\n\n        interp_kwargs = interp_kwargs or {}\n        interp_kwargs.setdefault(""points_scale"", (""log"", ""lin""))\n        interp_kwargs.setdefault(""extrapolate"", True)\n\n        self._evaluate = ScaledRegularGridInterpolator(\n            points=(self.param, self.energy), values=data, **interp_kwargs\n        )\n\n    def to_dict(self):\n        if self.filename is None:\n            return {\n                ""type"": self.tag,\n                ""energy"": {\n                    ""data"": self.energy.data.tolist(),\n                    ""unit"": str(self.energy.unit),\n                },\n                ""param"": {\n                    ""data"": self.param.data.tolist(),\n                    ""unit"": str(self.param.unit),\n                },\n                ""values"": {\n                    ""data"": self.data.data.tolist(),\n                    ""unit"": str(self.data.unit),\n                },\n            }\n        else:\n            return {""type"": self.tag, ""filename"": self.filename}\n\n    @classmethod\n    def from_dict(cls, data):\n\n        if ""filename"" in data:\n            return cls.read(data[""filename""])\n        else:\n            energy = u.Quantity(data[""energy""][""data""], data[""energy""][""unit""])\n            param = u.Quantity(data[""param""][""data""], data[""param""][""unit""])\n            values = u.Quantity(data[""values""][""data""], data[""values""][""unit""])\n            return cls(energy=energy, param=param, data=values)\n\n    @classmethod\n    def read(cls, filename, interp_kwargs=None):\n        """"""Build object from an XSPEC model.\n\n        Todo: Format of XSPEC binary files should be referenced at https://gamma-astro-data-formats.readthedocs.io/en/latest/\n\n        Parameters\n        ----------\n        filename : str\n            File containing the model.\n        interp_kwargs : dict\n            Interpolation option passed to `ScaledRegularGridInterpolator`.\n\n        Returns\n        -------\n        absorption : `Absorption`\n            Absorption model.\n        """"""\n        # Create EBL data array\n        filename = make_path(filename)\n        table_param = Table.read(filename, hdu=""PARAMETERS"")\n\n        # TODO: for some reason the table contain duplicated values\n        param, idx = np.unique(table_param[0][""VALUE""], return_index=True)\n\n        # Get energy values\n        table_energy = Table.read(filename, hdu=""ENERGIES"")\n        energy_lo = u.Quantity(\n            table_energy[""ENERG_LO""], ""keV"", copy=False\n        )  # unit not stored in file\n        energy_hi = u.Quantity(\n            table_energy[""ENERG_HI""], ""keV"", copy=False\n        )  # unit not stored in file\n        energy = np.sqrt(energy_lo * energy_hi)\n\n        # Get spectrum values\n        table_spectra = Table.read(filename, hdu=""SPECTRA"")\n        data = table_spectra[""INTPSPEC""].data[idx, :]\n        return cls(\n            energy=energy,\n            param=param,\n            data=data,\n            filename=filename,\n            interp_kwargs=interp_kwargs,\n        )\n\n    @classmethod\n    def read_builtin(cls, name, interp_kwargs=None):\n        """"""Read one of the built-in absorption models.\n\n        Parameters\n        ----------\n        name : {\'franceschini\', \'dominguez\', \'finke\'}\n            name of one of the available model in gammapy-data\n\n        References\n        ----------\n        .. [1] Franceschini et al., ""Extragalactic optical-infrared background radiation, its time evolution and the cosmic photon-photon opacity"",\n            `Link <https://ui.adsabs.harvard.edu/abs/2008A%26A...487..837F>`__\n        .. [2] Dominguez et al., "" Extragalactic background light inferred from AEGIS galaxy-SED-type fractions""\n            `Link <https://ui.adsabs.harvard.edu/abs/2011MNRAS.410.2556D>`__\n        .. [3] Finke et al., ""Modeling the Extragalactic Background Light from Stars and Dust""\n            `Link <https://ui.adsabs.harvard.edu/abs/2010ApJ...712..238F>`__\n\n        Returns\n        -------\n        absorption : `Absorption`\n            Absorption model.\n\n        """"""\n        models = dict()\n        models[""franceschini""] = ""$GAMMAPY_DATA/ebl/ebl_franceschini.fits.gz""\n        models[""dominguez""] = ""$GAMMAPY_DATA/ebl/ebl_dominguez11.fits.gz""\n        models[""finke""] = ""$GAMMAPY_DATA/ebl/frd_abs.fits.gz""\n\n        return cls.read(models[name], interp_kwargs=interp_kwargs)\n\n    def table_model(self, parameter):\n        """"""Table model for a given parameter value.\n\n        Parameters\n        ----------\n        parameter : float\n            Parameter value.\n\n        Returns\n        -------\n        template_model : `TemplateSpectralModel`\n            Template spectral model.\n        """"""\n        energy = self.energy\n        values = self.evaluate(energy=energy, parameter=parameter)\n        return TemplateSpectralModel(\n            energy=energy, values=values, interp_kwargs={""values_scale"": ""log""}\n        )\n\n    def evaluate(self, energy, parameter):\n        """"""Evaluate model for energy and parameter value.""""""\n        return np.clip(self._evaluate((parameter, energy)), 0, 1)\n\n\nclass AbsorbedSpectralModel(SpectralModel):\n    r""""""Spectral model with EBL absorption.\n\n    For more information see :ref:`absorbed-spectral-model`.\n\n    Parameters\n    ----------\n    spectral_model : `SpectralModel`\n        Spectral model.\n    absorption : `Absorption`\n        Absorption model.\n    redshift : float\n        Redshift of the absorption model\n    alpha_norm: float\n        Norm of the EBL model\n    """"""\n\n    tag = ""AbsorbedSpectralModel""\n    alpha_norm = Parameter(""alpha_norm"", 1.0, frozen=True)\n    redshift = Parameter(""redshift"", 0.1, frozen=True)\n\n    def __init__(\n        self, spectral_model, absorption, redshift, alpha_norm=alpha_norm.quantity,\n    ):\n        self.spectral_model = spectral_model\n        self.absorption = absorption\n\n        min_ = self.absorption.param.min()\n        max_ = self.absorption.param.max()\n\n        redshift = Parameter(""redshift"", redshift, frozen=True, min=min_, max=max_)\n        super().__init__(redshift=redshift, alpha_norm=alpha_norm)\n\n    def _check_covariance(self):\n        if not self.parameters == self._covariance.parameters:\n            self._covariance = Covariance(self.parameters)\n\n    @property\n    def covariance(self):\n        self._check_covariance()\n        self._covariance.set_subcovariance(self.spectral_model.covariance)\n        return self._covariance\n\n    @covariance.setter\n    def covariance(self, covariance):\n        self._check_covariance()\n        self._covariance.data = covariance\n\n        subcovar = self._covariance.get_subcovariance(\n            self.spectral_model.covariance.parameters\n        )\n        self.spectral_model.covariance = subcovar\n\n    @property\n    def parameters(self):\n        return (\n            Parameters([self.redshift, self.alpha_norm])\n            + self.spectral_model.parameters\n        )\n\n    def evaluate(self, energy, **kwargs):\n        """"""Evaluate the model at a given energy.""""""\n        # assign redshift value and remove it from dictionary\n        # since it does not belong to the spectral model\n        parameter = kwargs.pop(""redshift"")\n        alpha_norm = kwargs.pop(""alpha_norm"")\n\n        dnde = self.spectral_model.evaluate(energy=energy, **kwargs)\n        absorption = self.absorption.evaluate(energy=energy, parameter=parameter)\n        # Power rule: (e ^ a) ^ b = e ^ (a * b)\n        absorption = np.power(absorption, alpha_norm)\n        return dnde * absorption\n\n    def to_dict(self):\n        return {\n            ""type"": self.tag,\n            ""base_model"": self.spectral_model.to_dict(),\n            ""absorption"": self.absorption.to_dict(),\n            ""absorption_parameter"": {""name"": ""redshift"", ""value"": self.redshift.value,},\n            ""parameters"": Parameters([self.redshift, self.alpha_norm]).to_dict(),\n        }\n\n    @classmethod\n    def from_dict(cls, data):\n        from gammapy.modeling.models import SPECTRAL_MODELS\n\n        model_class = SPECTRAL_MODELS.get_cls(data[""base_model""][""type""])\n\n        model = cls(\n            spectral_model=model_class.from_dict(data[""base_model""]),\n            absorption=Absorption.from_dict(data[""absorption""]),\n            redshift=data[""absorption_parameter""][""value""],\n        )\n        return model\n\n\nclass NaimaSpectralModel(SpectralModel):\n    r""""""A wrapper for Naima models.\n\n    For more information see :ref:`naima-spectral-model`.\n\n    Parameters\n    ----------\n    radiative_model : `~naima.models.BaseRadiative`\n        An instance of a radiative model defined in `~naima.models`\n    distance : `~astropy.units.Quantity`, optional\n        Distance to the source. If set to 0, the intrinsic differential\n        luminosity will be returned. Default is 1 kpc\n    seed : str or list of str, optional\n        Seed photon field(s) to be considered for the `radiative_model` flux computation,\n        in case of a `~naima.models.InverseCompton` model. It can be a subset of the\n        `seed_photon_fields` list defining the `radiative_model`. Default is the whole list\n        of photon fields\n    nested_models : dict\n        Additionnal parameters for nested models not supplied by the radiative model,\n        for now this is used  only for synchrotron self-compton model\n    """"""\n\n    tag = ""NaimaSpectralModel""\n\n    def __init__(\n        self, radiative_model, distance=1.0 * u.kpc, seed=None, nested_models=None\n    ):\n        import naima\n\n        self.radiative_model = radiative_model\n        self._particle_distribution = self.radiative_model.particle_distribution\n        self.distance = u.Quantity(distance)\n        self.seed = seed\n\n        if nested_models is None:\n            nested_models = {}\n\n        self.nested_models = nested_models\n\n        if isinstance(self._particle_distribution, naima.models.TableModel):\n            param_names = [""amplitude""]\n        else:\n            param_names = self._particle_distribution.param_names\n\n        parameters = []\n\n        for name in param_names:\n            value = getattr(self._particle_distribution, name)\n            parameter = Parameter(name, value)\n            parameters.append(parameter)\n\n        # In case of a synchrotron radiative model, append B to the fittable parameters\n        if ""B"" in self.radiative_model.param_names:\n            value = getattr(self.radiative_model, ""B"")\n            parameter = Parameter(""B"", value)\n            parameters.append(parameter)\n\n        # In case of a synchrotron self compton model, append B and Rpwn to the fittable parameters\n        if (\n            isinstance(self.radiative_model, naima.models.InverseCompton)\n            and ""SSC"" in self.nested_models\n        ):\n            B = self.nested_models[""SSC""][""B""]\n            radius = self.nested_models[""SSC""][""radius""]\n            parameters.append(Parameter(""B"", B))\n            parameters.append(Parameter(""radius"", radius, frozen=True))\n\n        self.default_parameters = Parameters(parameters)\n        super().__init__()\n\n    def _evaluate_ssc(\n        self, energy,\n    ):\n        """"""\n        Compute photon density spectrum from synchrotron emission for synchrotron self-compton model,\n        assuming uniform synchrotron emissivity inside a sphere of radius R\n        (see Section 4.1 of Atoyan & Aharonian 1996)\n\n        based on :\n        ""https://naima.readthedocs.io/en/latest/examples.html#crab-nebula-ssc-model""\n\n        """"""\n        import naima\n\n        SYN = naima.models.Synchrotron(\n            self._particle_distribution,\n            B=self.B.quantity,\n            Eemax=self.radiative_model.Eemax,\n            Eemin=self.radiative_model.Eemin,\n        )\n\n        Esy = np.logspace(-7, 9, 100) * u.eV\n        Lsy = SYN.flux(Esy, distance=0 * u.cm)  # use distance 0 to get luminosity\n        phn_sy = Lsy / (4 * np.pi * self.radius.quantity ** 2 * const.c) * 2.24\n        # The factor 2.24 comes from the assumption on uniform synchrotron\n        # emissivity inside a sphere\n\n        if ""SSC"" not in self.radiative_model.seed_photon_fields:\n            self.radiative_model.seed_photon_fields[""SSC""] = {\n                ""isotropic"": True,\n                ""type"": ""array"",\n                ""energy"": Esy,\n                ""photon_density"": phn_sy,\n            }\n        else:\n            self.radiative_model.seed_photon_fields[""SSC""][""photon_density""] = phn_sy\n\n        dnde = self.radiative_model.flux(\n            energy, seed=self.seed, distance=self.distance\n        ) + SYN.flux(energy, distance=self.distance)\n        return dnde\n\n    def evaluate(self, energy, **kwargs):\n        """"""Evaluate the model.""""""\n        import naima\n\n        for name, value in kwargs.items():\n            setattr(self._particle_distribution, name, value)\n\n        if ""B"" in self.radiative_model.param_names:\n            self.radiative_model.B = self.B.quantity\n\n        if (\n            isinstance(self.radiative_model, naima.models.InverseCompton)\n            and ""SSC"" in self.nested_models\n        ):\n            dnde = self._evaluate_ssc(energy.flatten())\n        elif self.seed is not None:\n            dnde = self.radiative_model.flux(\n                energy.flatten(), seed=self.seed, distance=self.distance\n            )\n        else:\n            dnde = self.radiative_model.flux(energy.flatten(), distance=self.distance)\n\n        dnde = dnde.reshape(energy.shape)\n        unit = 1 / (energy.unit * u.cm ** 2 * u.s)\n        return dnde.to(unit)\n\n    @classmethod\n    def from_dict(cls, data):\n        raise NotImplementedError(\n            ""Currently the NaimaSpectralModel cannot be read from YAML""\n        )\n\n    @classmethod\n    def from_parameters(cls, parameters, **kwargs):\n        raise NotImplementedError(\n            ""Currently the NaimaSpectralModel cannot be built from a list of parameters.""\n        )\n\n\nclass GaussianSpectralModel(SpectralModel):\n    r""""""Gaussian spectral model.\n\n    For more information see :ref:`gaussian-spectral-model`.\n\n    Parameters\n    ----------\n    norm : `~astropy.units.Quantity`\n        :math:`N_0`\n    mean : `~astropy.units.Quantity`\n        :math:`\\bar{E}`\n    sigma : `~astropy.units.Quantity`\n        :math:`\\sigma`\n    """"""\n\n    tag = ""GaussianSpectralModel""\n    norm = Parameter(""norm"", 1e-12 * u.Unit(""cm-2 s-1""))\n    mean = Parameter(""mean"", 1 * u.TeV)\n    sigma = Parameter(""sigma"", 2 * u.TeV)\n\n    @staticmethod\n    def evaluate(energy, norm, mean, sigma):\n        return (\n            norm\n            / (sigma * np.sqrt(2 * np.pi))\n            * np.exp(-((energy - mean) ** 2) / (2 * sigma ** 2))\n        )\n\n    def integral(self, emin, emax, **kwargs):\n        r""""""Integrate Gaussian analytically.\n\n        .. math::\n            F(E_{min}, E_{max}) = \\frac{N_0}{2} \\left[ erf(\\frac{E - \\bar{E}}{\\sqrt{2} \\sigma})\\right]_{E_{min}}^{E_{max}}\n\n        Parameters\n        ----------\n        emin, emax : `~astropy.units.Quantity`\n            Lower and upper bound of integration range\n        """"""\n        # kwargs are passed to this function but not used\n        # this is to get a consistent API with SpectralModel.integral()\n        u_min = (\n            (emin - self.mean.quantity) / (np.sqrt(2) * self.sigma.quantity)\n        ).to_value("""")\n        u_max = (\n            (emax - self.mean.quantity) / (np.sqrt(2) * self.sigma.quantity)\n        ).to_value("""")\n\n        return (\n            self.norm.quantity\n            / 2\n            * (scipy.special.erf(u_max) - scipy.special.erf(u_min))\n        )\n\n    def energy_flux(self, emin, emax):\n        r""""""Compute energy flux in given energy range analytically.\n\n        .. math::\n            G(E_{min}, E_{max}) =  \\frac{N_0 \\sigma}{\\sqrt{2*\\pi}}* \\left[ - \\exp(\\frac{E_{min}-\\bar{E}}{\\sqrt{2} \\sigma})\n            \\right]_{E_{min}}^{E_{max}} + \\frac{N_0 * \\bar{E}}{2} \\left[ erf(\\frac{E - \\bar{E}}{\\sqrt{2} \\sigma})\n             \\right]_{E_{min}}^{E_{max}}\n\n\n        Parameters\n        ----------\n        emin, emax : `~astropy.units.Quantity`\n            Lower and upper bound of integration range.\n        """"""\n        u_min = (\n            (emin - self.mean.quantity) / (np.sqrt(2) * self.sigma.quantity)\n        ).to_value("""")\n        u_max = (\n            (emax - self.mean.quantity) / (np.sqrt(2) * self.sigma.quantity)\n        ).to_value("""")\n        a = self.norm.quantity * self.sigma.quantity / np.sqrt(2 * np.pi)\n        b = self.norm.quantity * self.mean.quantity / 2\n        return a * (np.exp(-(u_min ** 2)) - np.exp(-(u_max ** 2))) + b * (\n            scipy.special.erf(u_max) - scipy.special.erf(u_min)\n        )\n'"
gammapy/modeling/models/spectral_cosmic_ray.py,5,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Simple models for cosmic ray spectra at Earth.\n\nFor measurements, the ""Database of Charged Cosmic Rays (CRDB)"" is a great resource:\nhttp://lpsc.in2p3.fr/cosmic-rays-db/\n""""""\nimport numpy as np\nfrom astropy import units as u\nfrom gammapy.modeling import Parameter\nfrom .spectral import PowerLawSpectralModel, SpectralModel\n\n\nclass _LogGaussianSpectralModel(SpectralModel):\n    r""""""Log Gaussian spectral model with a weird parametrisation.\n\n    This should not be exposed to end-users as a Gammapy spectral model!\n    See Table 3 in https://ui.adsabs.harvard.edu/abs/2013APh....43..171B\n    """"""\n\n    L = Parameter(""L"", 1e-12 * u.Unit(""cm-2 s-1""))\n    Ep = Parameter(""Ep"", 0.107 * u.TeV)\n    w = Parameter(""w"", 0.776)\n\n    @staticmethod\n    def evaluate(energy, L, Ep, w):\n        return (\n            L\n            / (energy * w * np.sqrt(2 * np.pi))\n            * np.exp(-((np.log(energy / Ep)) ** 2) / (2 * w ** 2))\n        )\n\n\ndef create_cosmic_ray_spectral_model(particle=""proton""):\n    """"""Cosmic a cosmic ray spectral model at Earth.\n\n    These are the spectra assumed in this CTA study:\n    Table 3 in https://ui.adsabs.harvard.edu/abs/2013APh....43..171B\n\n    The spectrum given is a differential flux ``dnde`` in units of\n    ``cm-2 s-1 TeV-1``, as the value integrated over the whole sky.\n    To get a surface brightness you need to compute\n    ``dnde / (4 * np.pi * u.sr)``.\n    To get the ``dnde`` in a region of solid angle ``omega``, you need\n    to compute ``dnde * omega / (4 * np.pi * u.sr)``.\n\n    The hadronic spectra are simple power-laws, the electron spectrum\n    is the sum of  a power law and a log-normal component to model the\n    ""Fermi shoulder"".\n\n    Parameters\n    ----------\n    particle : {\'electron\', \'proton\', \'He\', \'N\', \'Si\', \'Fe\'}\n        Particle type\n\n    Returns\n    -------\n    `~gammapy.modeling.models.SpectralModel`\n        Spectral model (for all-sky cosmic ray flux)\n    """"""\n    omega = 4 * np.pi * u.sr\n    if particle == ""proton"":\n        return PowerLawSpectralModel(\n            amplitude=0.096 * u.Unit(""1 / (m2 s TeV sr)"") * omega,\n            index=2.70,\n            reference=1 * u.TeV,\n        )\n    elif particle == ""N"":\n        return PowerLawSpectralModel(\n            amplitude=0.0719 * u.Unit(""1 / (m2 s TeV sr)"") * omega,\n            index=2.64,\n            reference=1 * u.TeV,\n        )\n    elif particle == ""Si"":\n        return PowerLawSpectralModel(\n            amplitude=0.0284 * u.Unit(""1 / (m2 s TeV sr)"") * omega,\n            index=2.66,\n            reference=1 * u.TeV,\n        )\n    elif particle == ""Fe"":\n        return PowerLawSpectralModel(\n            amplitude=0.0134 * u.Unit(""1 / (m2 s TeV sr)"") * omega,\n            index=2.63,\n            reference=1 * u.TeV,\n        )\n    elif particle == ""electron"":\n        return PowerLawSpectralModel(\n            amplitude=6.85e-5 * u.Unit(""1 / (m2 s TeV sr)"") * omega,\n            index=3.21,\n            reference=1 * u.TeV,\n        ) + _LogGaussianSpectralModel(L=3.19e-3 * u.Unit(""1 / (m2 s sr)"") * omega)\n    else:\n        raise ValueError(f""Invalid particle: {particle!r}"")\n'"
gammapy/modeling/models/spectral_crab.py,4,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom astropy import units as u\nfrom .spectral import (\n    ExpCutoffPowerLawSpectralModel,\n    LogParabolaSpectralModel,\n    PowerLawSpectralModel,\n    SpectralModel,\n)\n\n\nclass MeyerCrabSpectralModel(SpectralModel):\n    """"""Meyer 2010 log polynomial Crab spectral model.\n\n    Reference: https://ui.adsabs.harvard.edu/abs/2010A%26A...523A...2M, Appendix D\n    """"""\n\n    coefficients = [-0.00449161, 0, 0.0473174, -0.179475, -0.53616, -10.2708]\n\n    @staticmethod\n    def evaluate(energy):\n        polynomial = np.poly1d(MeyerCrabSpectralModel.coefficients)\n        log_energy = np.log10(energy.to_value(""TeV""))\n        log_flux = polynomial(log_energy)\n        flux = u.Quantity(np.power(10, log_flux), ""erg / (cm2 s)"", copy=False)\n        return flux / energy ** 2\n\n\ndef create_crab_spectral_model(reference=""meyer""):\n    """"""Create a Crab nebula reference spectral model.\n\n    The Crab nebula is often used as a standard candle in gamma-ray astronomy.\n    Fluxes and sensitivities are often quoted relative to the Crab spectrum.\n\n    The following references are available:\n\n    * \'meyer\', https://ui.adsabs.harvard.edu/abs/2010A%26A...523A...2M, Appendix D\n    * \'hegra\', https://ui.adsabs.harvard.edu/abs/2000ApJ...539..317A\n    * \'hess_pl\' and \'hess_ecpl\': https://ui.adsabs.harvard.edu/abs/2006A%26A...457..899A\n    * \'magic_lp\' and \'magic_ecpl\': https://ui.adsabs.harvard.edu/abs/2015JHEAp...5...30A\n\n    Parameters\n    ----------\n    reference : {\'meyer\', \'hegra\', \'hess_pl\', \'hess_ecpl\', \'magic_lp\', \'magic_ecpl\'}\n        Which reference to use for the spectral model.\n\n    Examples\n    --------\n    Let\'s first import what we need::\n\n        import astropy.units as u\n        from gammapy.spectrum import create_crab_spectral_model\n        from gammapy.modeling.models import PowerLaw\n\n    Plot the \'hess_ecpl\' reference Crab spectrum between 1 TeV and 100 TeV::\n\n        crab_hess_ecpl = create_crab_spectral_model(\'hess_ecpl\')\n        crab_hess_ecpl.plot([1, 100] * u.TeV)\n\n    Use a reference crab spectrum as unit to measure a differential flux (at 10 TeV)::\n\n        >>> pwl = PowerLawSpectralModel(index=2.3, amplitude=1e-12 * u.Unit(\'1 / (cm2 s TeV)\'), reference=1 * u.TeV)\n        >>> crab = create_crab_spectral_model(\'hess_pl\')\n        >>> energy = 10 * u.TeV\n        >>> dnde_cu = (pwl(energy) / crab(energy)).to(\'%\')\n        >>> print(dnde_cu)\n        6.19699156377 %\n\n    And the same for integral fluxes (between 1 and 10 TeV)::\n\n        >>> # compute integral flux in crab units\n        >>> emin, emax = [1, 10] * u.TeV\n        >>> flux_int_cu = (pwl.integral(emin, emax) / crab.integral(emin, emax)).to(\'%\')\n        >>> print(flux_int_cu)\n        3.5350582166 %\n    """"""\n    if reference == ""meyer"":\n        return MeyerCrabSpectralModel()\n    elif reference == ""hegra"":\n        return PowerLawSpectralModel(\n            amplitude=2.83e-11 * u.Unit(""1 / (cm2 s TeV)""),\n            index=2.62,\n            reference=1 * u.TeV,\n        )\n    elif reference == ""hess_pl"":\n        return PowerLawSpectralModel(\n            amplitude=3.45e-11 * u.Unit(""1 / (cm2 s TeV)""),\n            index=2.63,\n            reference=1 * u.TeV,\n        )\n    elif reference == ""hess_ecpl"":\n        return ExpCutoffPowerLawSpectralModel(\n            amplitude=3.76e-11 * u.Unit(""1 / (cm2 s TeV)""),\n            index=2.39,\n            lambda_=1 / (14.3 * u.TeV),\n            reference=1 * u.TeV,\n        )\n    elif reference == ""magic_lp"":\n        return LogParabolaSpectralModel(\n            amplitude=3.23e-11 * u.Unit(""1 / (cm2 s TeV)""),\n            alpha=2.47,\n            beta=0.24 / np.log(10),\n            reference=1 * u.TeV,\n        )\n    elif reference == ""magic_ecpl"":\n        return ExpCutoffPowerLawSpectralModel(\n            amplitude=3.80e-11 * u.Unit(""1 / (cm2 s TeV)""),\n            index=2.21,\n            lambda_=1 / (6.0 * u.TeV),\n            reference=1 * u.TeV,\n        )\n    else:\n        raise ValueError(f""Invalid reference: {reference!r}"")\n'"
gammapy/modeling/models/temporal.py,10,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Time-dependent models.""""""\nimport numpy as np\nimport scipy.interpolate\nfrom astropy import units as u\nfrom astropy.table import Table\nfrom astropy.time import Time\nfrom astropy.utils import lazyproperty\nfrom gammapy.modeling import Parameter\nfrom gammapy.utils.random import InverseCDFSampler, get_random_state\nfrom gammapy.utils.scripts import make_path\nfrom gammapy.utils.time import time_ref_from_dict\nfrom .core import Model\n\n\n# TODO: make this a small ABC to define a uniform interface.\nclass TemporalModel(Model):\n    """"""Temporal model base class.\n    evaluates on  astropy.time.Time objects""""""\n\n    def __call__(self, time):\n        """"""Evaluate model\n\n        Parameters\n        ----------\n        time : `~astropy.time.Time`\n            Time object\n        """"""\n        kwargs = {par.name: par.quantity for par in self.parameters}\n        time = u.Quantity(time.mjd, ""day"")\n        return self.evaluate(time, **kwargs)\n\n    @staticmethod\n    def time_sum(t_min, t_max):\n        """"""\n        Total time between t_min and t_max\n\n        Parameters\n        ----------\n        t_min, t_max: `~astropy.time.Time`\n            Lower and upper bound of integration range\n\n        Returns\n        -------\n        time_sum : `~astropy.time.TimeDelta`\n            Summed time in the intervals.\n\n        """"""\n        return np.sum(t_max - t_min)\n\n\nclass ConstantTemporalModel(TemporalModel):\n    """"""Constant temporal model.""""""\n\n    tag = ""ConstantTemporalModel""\n\n    @staticmethod\n    def evaluate(time):\n        """"""Evaluate at given times.""""""\n        return np.ones(time.shape)\n\n    def integral(self, t_min, t_max):\n        """"""Evaluate the integrated flux within the given time intervals\n\n        Parameters\n        ----------\n        t_min: `~astropy.time.Time`\n            Start times of observation\n        t_max: `~astropy.time.Time`\n            Stop times of observation\n\n        Returns\n        -------\n        norm : `~astropy.units.Quantity`\n            Integrated flux norm on the given time intervals\n        """"""\n        return (t_max - t_min) / self.time_sum(t_min, t_max)\n\n    @staticmethod\n    def sample_time(n_events, t_min, t_max, random_state=0):\n        """"""Sample arrival times of events.\n\n        Parameters\n        ----------\n        n_events : int\n            Number of events to sample.\n        t_min : `~astropy.time.Time`\n            Start time of the sampling.\n        t_max : `~astropy.time.Time`\n            Stop time of the sampling.\n        random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n            Defines random number generator initialisation.\n            Passed to `~gammapy.utils.random.get_random_state`.\n\n        Returns\n        -------\n        time : `~astropy.units.Quantity`\n            Array with times of the sampled events.\n        """"""\n        random_state = get_random_state(random_state)\n\n        t_min = Time(t_min)\n        t_max = Time(t_max)\n\n        t_stop = (t_max - t_min).sec\n\n        time_delta = random_state.uniform(high=t_stop, size=n_events) * u.s\n\n        return t_min + time_delta\n\n\nclass ExpDecayTemporalModel(TemporalModel):\n    """"""Temporal model with an exponential decay.\n\n        ..math::\n                F(t) = exp(t - t_ref)/t0\n\n    Parameters\n    ----------\n    t0 : `~astropy.units.Quantity`\n        Decay time scale\n    t_ref: `~astropy.units.Quantity`\n        The reference time in mjd\n    """"""\n\n    tag = ""ExponentialDecayTemporalModel""\n\n    t0 = Parameter(""t0"", ""1 d"", frozen=False)\n\n    _t_ref_default = Time(""2000-01-01"")\n    t_ref = Parameter(""t_ref"", _t_ref_default.mjd, unit=""day"", frozen=True)\n\n    @staticmethod\n    def evaluate(time, t0, t_ref):\n        """"""Evaluate at given times""""""\n        return np.exp(-(time - t_ref) / t0)\n\n    def integral(self, t_min, t_max):\n        """"""Evaluate the integrated flux within the given time intervals\n\n        Parameters\n        ----------\n        t_min: `~astropy.time.Time`\n            Start times of observation\n        t_max: `~astropy.time.Time`\n            Stop times of observation\n\n        Returns\n        -------\n        norm : float\n            Integrated flux norm on the given time intervals\n        """"""\n        pars = self.parameters\n        t0 = pars[""t0""].quantity\n        t_ref = Time(pars[""t_ref""].quantity, format=""mjd"")\n        value = self.evaluate(t_max, t0, t_ref) - self.evaluate(t_min, t0, t_ref)\n        return -t0 * value / self.time_sum(t_min, t_max)\n\n\nclass GaussianTemporalModel(TemporalModel):\n    """"""A Gaussian temporal profile\n\n    Parameters\n    ----------\n    t_ref: `~astropy.units.Quantity`\n        The reference time in mjd at the peak.\n    sigma : `~astropy.units.Quantity`\n        Width of the gaussian profile.\n    """"""\n\n    tag = ""GaussianTemporalModel""\n\n    _t_ref_default = Time(""2000-01-01"")\n    t_ref = Parameter(""t_ref"", _t_ref_default.mjd, unit=""day"", frozen=False)\n    sigma = Parameter(""sigma"", ""1 d"", frozen=False)\n\n    @staticmethod\n    def evaluate(time, t_ref, sigma):\n        return np.exp(-((time - t_ref) ** 2) / (2 * sigma ** 2))\n\n    def integral(self, t_min, t_max, **kwargs):\n        """"""Evaluate the integrated flux within the given time intervals\n\n        Parameters\n        ----------\n        t_min: `~astropy.time.Time`\n            Start times of observation\n        t_max: `~astropy.time.Time`\n            Stop times of observation\n\n        Returns\n        -------\n        norm : float\n            Integrated flux norm on the given time intervals\n        """"""\n        pars = self.parameters\n        sigma = pars[""sigma""].quantity\n        t_ref = Time(pars[""t_ref""].quantity, format=""mjd"")\n        norm = np.sqrt(np.pi / 2) * sigma\n\n        u_min = (t_min - t_ref) / (np.sqrt(2) * sigma)\n        u_max = (t_max - t_ref) / (np.sqrt(2) * sigma)\n\n        integral = norm * (scipy.special.erf(u_max) - scipy.special.erf(u_min))\n        return integral / self.time_sum(t_min, t_max)\n\n\nclass LightCurveTemplateTemporalModel(TemporalModel):\n    """"""Temporal light curve model.\n\n    The lightcurve is given as a table with columns ``time`` and ``norm``.\n\n    The ``norm`` is supposed to be a unit-less multiplicative factor in the model,\n    to be multiplied with a spectral model.\n\n    The model does linear interpolation for times between the given ``(time, norm)`` values.\n\n    The implementation currently uses `scipy.interpolate.InterpolatedUnivariateSpline`,\n    using degree ``k=1`` to get linear interpolation.\n    This class also contains an ``integral`` method, making the computation of\n    mean fluxes for a given time interval a one-liner.\n\n    Parameters\n    ----------\n    table : `~astropy.table.Table`\n        A table with \'TIME\' vs \'NORM\'\n\n    Examples\n    --------\n    Read an example light curve object:\n\n    >>> from gammapy.modeling.models import LightCurveTemplateTemporalModel\n    >>> path = \'$GAMMAPY_DATA/tests/models/light_curve/lightcrv_PKSB1222+216.fits\'\n    >>> light_curve = LightCurveTemplateTemporalModel.read(path)\n\n    Show basic information about the lightcurve:\n\n    >>> print(light_curve)\n    LightCurve model summary:\n    Start time: 59000.5 MJD\n    End time: 61862.5 MJD\n    Norm min: 0.01551196351647377\n    Norm max: 1.0\n\n    Compute ``norm`` at a given time:\n\n    >>> light_curve.evaluate(46300)\n    0.49059393580053845\n\n    Compute mean ``norm`` in a given time interval:\n\n    >>> light_curve.mean_norm_in_time_interval(46300, 46301)\n    """"""\n\n    tag = ""LightCurveTemplateTemporalModel""\n\n    def __init__(self, table, filename=None):\n        self.table = table\n        if filename is not None:\n            filename = str(make_path(filename))\n        self.filename = filename\n        super().__init__()\n\n    def __str__(self):\n        norm = self.table[""NORM""]\n        return (\n            f""{self.__class__.__name__} model summary:\\n""\n            f""Start time: {self._time[0].mjd} MJD\\n""\n            f""End time: {self._time[-1].mjd} MJD\\n""\n            f""Norm min: {norm.min()}\\n""\n            f""Norm max: {norm.max()}\\n""\n        )\n\n    @classmethod\n    def read(cls, path):\n        """"""Read lightcurve model table from FITS file.\n\n        TODO: This doesn\'t read the XML part of the model yet.\n        """"""\n        filename = str(make_path(path))\n        return cls(Table.read(filename), filename=filename)\n\n    def write(self, path=None, overwrite=False):\n        if path is None:\n            path = self.filename\n        if path is None:\n            raise ValueError(f""filename is required for {self.tag}"")\n        else:\n            self.filename = str(make_path(path))\n            self.table.write(self.filename, overwrite=overwrite)\n\n    @lazyproperty\n    def _interpolator(self, ext=0):\n        x = self._time.value\n        y = self.table[""NORM""].data\n        return scipy.interpolate.InterpolatedUnivariateSpline(x, y, k=1, ext=ext)\n\n    @lazyproperty\n    def _time_ref(self):\n        return time_ref_from_dict(self.table.meta)\n\n    @lazyproperty\n    def _time(self):\n        return self._time_ref + self.table[""TIME""].data * getattr(\n            u, self.table.meta[""TIMEUNIT""]\n        )\n\n    def evaluate(self, time, ext=0):\n        """"""Evaluate for a given time.\n\n        Parameters\n        ----------\n        time : array_like\n            Time since the ``reference`` time.\n        ext : int or str, optional, default: 0\n            Parameter passed to ~scipy.interpolate.InterpolatedUnivariateSpline\n            Controls the extrapolation mode for GTIs outside the range\n            0 or ""extrapolate"", return the extrapolated value.\n            1 or ""zeros"", return 0\n            2 or ""raise"", raise a ValueError\n            3 or ""const"", return the boundary value.\n\n\n        Returns\n        -------\n        norm : array_like\n            Norm at the given times.\n        """"""\n        return self._interpolator(time, ext=ext)\n\n    def integral(self, t_min, t_max):\n        """"""Evaluate the integrated flux within the given time intervals\n\n        Parameters\n        ----------\n        t_min: `~astropy.time.Time`\n            Start times of observation\n        t_max: `~astropy.time.Time`\n            Stop times of observation\n        Returns\n        -------\n        norm: The model integrated flux\n        """"""\n\n        n1 = self._interpolator.antiderivative()(t_max.mjd)\n        n2 = self._interpolator.antiderivative()(t_min.mjd)\n        return u.Quantity(n1 - n2, ""day"") / self.time_sum(t_min, t_max)\n\n    def sample_time(self, n_events, t_min, t_max, t_delta=""1 s"", random_state=0):\n        """"""Sample arrival times of events.\n\n        Parameters\n        ----------\n        n_events : int\n            Number of events to sample.\n        t_min : `~astropy.time.Time`\n            Start time of the sampling.\n        t_max : `~astropy.time.Time`\n            Stop time of the sampling.\n        t_delta : `~astropy.units.Quantity`\n            Time step used for sampling of the temporal model.\n        random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n            Defines random number generator initialisation.\n            Passed to `~gammapy.utils.random.get_random_state`.\n\n        Returns\n        -------\n        time : `~astropy.units.Quantity`\n            Array with times of the sampled events.\n        """"""\n        time_unit = getattr(u, self.table.meta[""TIMEUNIT""])\n\n        t_min = Time(t_min)\n        t_max = Time(t_max)\n        t_delta = u.Quantity(t_delta)\n        random_state = get_random_state(random_state)\n\n        ontime = u.Quantity((t_max - t_min).sec, ""s"")\n        t_stop = ontime.to_value(time_unit)\n\n        # TODO: the separate time unit handling is unfortunate, but the quantity support for np.arange and np.interp\n        #  is still incomplete, refactor once we change to recent numpy and astropy versions\n        t_step = t_delta.to_value(time_unit)\n        t = np.arange(0, t_stop, t_step)\n\n        pdf = self.evaluate(t)\n\n        sampler = InverseCDFSampler(pdf=pdf, random_state=random_state)\n        time_pix = sampler.sample(n_events)[0]\n        time = np.interp(time_pix, np.arange(len(t)), t) * time_unit\n\n        return t_min + time\n\n    @classmethod\n    def from_dict(cls, data):\n        return cls.read(data[""filename""])\n\n    def to_dict(self, overwrite=False):\n        """"""Create dict for YAML serilisation""""""\n        return {""type"": self.tag, ""filename"": self.filename}\n'"
gammapy/modeling/tests/__init__.py,0,b''
gammapy/modeling/tests/test_covariance.py,4,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Unit tests for the Covariance class""""""\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom gammapy.modeling import Covariance, Parameter, Parameters\nfrom gammapy.utils.testing import mpl_plot_check, requires_dependency\n\n\n@pytest.fixture\ndef covariance_diagonal():\n    x = Parameter(""x"", 1, error=0.1)\n    y = Parameter(""y"", 2, error=0.2)\n    z = Parameter(""z"", 3, error=0.3)\n\n    parameters = Parameters([x, y, z])\n    return Covariance(parameters=parameters)\n\n\n@pytest.fixture\ndef covariance(covariance_diagonal):\n    x = covariance_diagonal.parameters[""x""]\n    y = covariance_diagonal.parameters[""y""]\n    parameters = Parameters([x, y])\n    data = np.ones((2, 2))\n    return Covariance(parameters=parameters, data=data)\n\n\ndef test_str(covariance_diagonal):\n    assert ""0.01"" in str(covariance_diagonal)\n\n\ndef test_shape(covariance_diagonal):\n    assert_allclose(covariance_diagonal.shape, (3, 3))\n\n\ndef test_set_data(covariance_diagonal):\n    data = np.ones((2, 2))\n    with pytest.raises(ValueError):\n        covariance_diagonal.data = data\n\n\ndef test_set_subcovariance(covariance_diagonal, covariance):\n    covariance_diagonal.set_subcovariance(covariance)\n    assert_allclose(covariance_diagonal.data[:2, :2], np.ones((2, 2)))\n\n\ndef test_get_subcovariance(covariance_diagonal, covariance):\n    covar = covariance_diagonal.get_subcovariance(covariance.parameters)\n    assert_allclose(np.diag(covar), [0.1 ** 2, 0.2 ** 2])\n\n\ndef test_scipy_mvn(covariance):\n    mvn = covariance.scipy_mvn\n    value = mvn.pdf(2)\n    assert_allclose(value, 0.2489, rtol=1e-3)\n\n\n@requires_dependency(""matplotlib"")\ndef test_plot_correlation(covariance_diagonal):\n    with mpl_plot_check():\n        covariance_diagonal.plot_correlation()\n'"
gammapy/modeling/tests/test_fit.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Unit tests for the Fit class""""""\nimport pytest\nfrom numpy.testing import assert_allclose\nfrom gammapy.datasets import Dataset\nfrom gammapy.modeling import Fit, Parameter\nfrom gammapy.modeling.models import Model, Models\nfrom gammapy.utils.testing import requires_dependency\n\npytest.importorskip(""iminuit"")\n\n\nclass MyModel(Model):\n    x = Parameter(""x"", 2)\n    y = Parameter(""y"", 3e2)\n    z = Parameter(""z"", 4e-2)\n    name = ""test""\n    datasets_names = [""test""]\n\n\nclass MyDataset(Dataset):\n    def __init__(self, name=""test""):\n        self.name = name\n        self._models = Models([MyModel()])\n        self.data_shape = (1,)\n\n    @property\n    def models(self):\n        return self._models\n\n    def stat_sum(self):\n        # self._model.parameters = parameters\n        x, y, z = [p.value for p in self.models.parameters]\n        x_opt, y_opt, z_opt = 2, 3e2, 4e-2\n        return (x - x_opt) ** 2 + (y - y_opt) ** 2 + (z - z_opt) ** 2\n\n    def fcn(self):\n        x, y, z = [p.value for p in self.models.parameters]\n        x_opt, y_opt, z_opt = 2, 3e5, 4e-5\n        x_err, y_err, z_err = 0.2, 3e4, 4e-6\n        return (\n            ((x - x_opt) / x_err) ** 2\n            + ((y - y_opt) / y_err) ** 2\n            + ((z - z_opt) / z_err) ** 2\n        )\n\n    def stat_array(self):\n        """"""Statistic array, one value per data point.""""""\n\n\n@pytest.mark.parametrize(""backend"", [""minuit""])\ndef test_run(backend):\n    dataset = MyDataset()\n    fit = Fit([dataset])\n    result = fit.run(backend=backend)\n    pars = result.parameters\n\n    assert result.success is True\n\n    assert_allclose(pars[""x""].value, 2, rtol=1e-3)\n    assert_allclose(pars[""y""].value, 3e2, rtol=1e-3)\n    assert_allclose(pars[""z""].value, 4e-2, rtol=1e-3)\n\n    assert_allclose(pars[""x""].error, 1, rtol=1e-7)\n    assert_allclose(pars[""y""].error, 1, rtol=1e-7)\n    assert_allclose(pars[""z""].error, 1, rtol=1e-7)\n\n    correlation = dataset.models.covariance.correlation\n    assert_allclose(correlation[0, 1], 0, atol=1e-7)\n    assert_allclose(correlation[0, 2], 0, atol=1e-7)\n    assert_allclose(correlation[1, 2], 0, atol=1e-7)\n\n\n@requires_dependency(""sherpa"")\n@pytest.mark.parametrize(""backend"", [""minuit"", ""sherpa"", ""scipy""])\ndef test_optimize(backend):\n    dataset = MyDataset()\n    fit = Fit([dataset])\n    result = fit.optimize(backend=backend)\n    pars = dataset.models.parameters\n\n    assert result.success is True\n    assert_allclose(result.total_stat, 0)\n\n    assert_allclose(pars[""x""].value, 2, rtol=1e-3)\n    assert_allclose(pars[""y""].value, 3e2, rtol=1e-3)\n    assert_allclose(pars[""z""].value, 4e-2, rtol=1e-3)\n\n\n# TODO: add some extra covariance tests, in addition to run\n# Probably mainly if error message is OK if optimize didn\'t run first.\n# def test_covariance():\n\n\n@pytest.mark.parametrize(""backend"", [""minuit""])\ndef test_confidence(backend):\n    dataset = MyDataset()\n    fit = Fit([dataset])\n    fit.optimize(backend=backend)\n    result = fit.confidence(""x"")\n\n    assert result[""success""] is True\n    assert_allclose(result[""errp""], 1)\n    assert_allclose(result[""errn""], 1)\n\n    # Check that original value state wasn\'t changed\n    assert_allclose(dataset.models.parameters[""x""].value, 2)\n\n\n@pytest.mark.parametrize(""backend"", [""minuit""])\ndef test_confidence_frozen(backend):\n    dataset = MyDataset()\n    dataset.models.parameters[""x""].frozen = True\n    fit = Fit([dataset])\n    fit.optimize(backend=backend)\n    result = fit.confidence(""y"")\n\n    assert result[""success""] is True\n    assert_allclose(result[""errp""], 1)\n    assert_allclose(result[""errn""], 1)\n\n\ndef test_stat_profile():\n    dataset = MyDataset()\n    fit = Fit([dataset])\n    fit.run()\n    result = fit.stat_profile(""x"", nvalues=3)\n\n    assert_allclose(result[""values""], [0, 2, 4], atol=1e-7)\n    assert_allclose(result[""stat""], [4, 0, 4], atol=1e-7)\n\n    # Check that original value state wasn\'t changed\n    assert_allclose(dataset.models.parameters[""x""].value, 2)\n\n\ndef test_stat_profile_reoptimize():\n    dataset = MyDataset()\n    fit = Fit([dataset])\n    fit.run()\n\n    dataset.models.parameters[""y""].value = 0\n    result = fit.stat_profile(""x"", nvalues=3, reoptimize=True)\n\n    assert_allclose(result[""values""], [0, 2, 4], atol=1e-7)\n    assert_allclose(result[""stat""], [4, 0, 4], atol=1e-7)\n\n\ndef test_minos_contour():\n    dataset = MyDataset()\n    dataset.models.parameters[""x""].frozen = True\n    fit = Fit([dataset])\n    fit.optimize(backend=""minuit"")\n    result = fit.minos_contour(""y"", ""z"")\n\n    assert result[""success""] is True\n\n    x = result[""x""]\n    assert_allclose(len(x), 10)\n    assert_allclose(x[0], 299, rtol=1e-5)\n    assert_allclose(x[-1], 299.133975, rtol=1e-5)\n    y = result[""y""]\n    assert_allclose(len(y), 10)\n    assert_allclose(y[0], 0.04, rtol=1e-5)\n    assert_allclose(y[-1], 0.54, rtol=1e-5)\n\n    # Check that original value state wasn\'t changed\n    assert_allclose(dataset.models.parameters[""y""].value, 300)\n'"
gammapy/modeling/tests/test_iminuit.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nfrom gammapy.modeling import Parameter\nfrom gammapy.modeling.iminuit import confidence_iminuit, optimize_iminuit\nfrom gammapy.modeling.models import Model, Models\n\npytest.importorskip(""iminuit"")\n\n\nclass MyModel(Model):\n    x = Parameter(""x"", 2.1, error=0.2)\n    y = Parameter(""y"", 3.1, scale=1e5, error=3e4)\n    z = Parameter(""z"", 4.1, scale=1e-5, error=4e-6)\n    name = ""test""\n    datasets_names = [""test""]\n\n\nclass MyDataset:\n    def __init__(self, name=""test""):\n        self.name = name\n        self.models = Models(MyModel())\n\n    def fcn(self):\n        x, y, z = [p.value for p in self.models.parameters]\n        x_opt, y_opt, z_opt = 2, 3e5, 4e-5\n        x_err, y_err, z_err = 0.2, 3e4, 4e-6\n        return (\n            ((x - x_opt) / x_err) ** 2\n            + ((y - y_opt) / y_err) ** 2\n            + ((z - z_opt) / z_err) ** 2\n        )\n\n\ndef test_iminuit_basic():\n    ds = MyDataset()\n    pars = ds.models.parameters\n    factors, info, minuit = optimize_iminuit(function=ds.fcn, parameters=pars)\n\n    assert info[""success""]\n    assert_allclose(ds.fcn(), 0, atol=1e-5)\n\n    # Check the result in parameters is OK\n    assert_allclose(pars[""x""].value, 2, rtol=1e-3)\n    assert_allclose(pars[""y""].value, 3e5, rtol=1e-3)\n    # Precision of estimate on ""z"" is very poor (0.040488). Why is it so bad?\n    assert_allclose(pars[""z""].value, 4e-5, rtol=2e-2)\n\n    # Check that minuit sees the parameter factors correctly\n    assert_allclose(factors, [2, 3, 4], rtol=1e-3)\n    assert_allclose(minuit.values[""par_000_x""], 2, rtol=1e-3)\n    assert_allclose(minuit.values[""par_001_y""], 3, rtol=1e-3)\n    assert_allclose(minuit.values[""par_002_z""], 4, rtol=1e-3)\n\n\ndef test_iminuit_stepsize():\n    ds = MyDataset()\n    pars = ds.models.parameters\n    factors, info, minuit = optimize_iminuit(function=ds.fcn, parameters=pars)\n\n    assert info[""success""]\n    assert_allclose(ds.fcn(), 0, atol=1e-5)\n    assert_allclose(pars[""x""].value, 2, rtol=1e-3)\n    assert_allclose(pars[""y""].value, 3e5, rtol=1e-3)\n    assert_allclose(pars[""z""].value, 4e-5, rtol=2e-2)\n\n\ndef test_iminuit_frozen():\n    ds = MyDataset()\n    pars = ds.models.parameters\n    pars[""y""].frozen = True\n\n    factors, info, minuit = optimize_iminuit(function=ds.fcn, parameters=pars)\n\n    assert info[""success""]\n\n    assert_allclose(pars[""x""].value, 2, rtol=1e-4)\n    assert_allclose(pars[""y""].value, 3.1e5)\n    assert_allclose(pars[""z""].value, 4.0e-5, rtol=1e-4)\n    assert_allclose(ds.fcn(), 0.111112, rtol=1e-5)\n\n\ndef test_iminuit_limits():\n    ds = MyDataset()\n    pars = ds.models.parameters\n    pars[""y""].min = 301000\n\n    factors, info, minuit = optimize_iminuit(function=ds.fcn, parameters=pars)\n\n    assert info[""success""]\n\n    # Check the result in parameters is OK\n    assert_allclose(pars[""x""].value, 2, rtol=1e-2)\n    assert_allclose(pars[""y""].value, 301000, rtol=1e-3)\n\n    # Check that minuit sees the limit factors correctly\n    states = minuit.get_param_states()\n    assert not states[0][""has_limits""]\n\n    y = states[1]\n    assert y[""has_limits""]\n    assert_allclose(y[""lower_limit""], 3.01)\n\n    # The next assert can be added when we no longer test on iminuit 1.2\n    # See https://github.com/gammapy/gammapy/pull/1771\n    # assert states[1][""upper_limit""] is None\n\n\ndef test_opts():\n    ds = MyDataset()\n    pars = ds.models.parameters\n    factors, info, minuit = optimize_iminuit(\n        function=ds.fcn, parameters=pars, migrad_opts={""ncall"": 20}, tol=1.0, strategy=2\n    )\n    assert info[""nfev""] == 29\n    assert minuit.tol == 1.0\n    assert minuit.strategy == 2\n\n\ndef test_iminuit_confidence():\n    ds = MyDataset()\n    pars = ds.models.parameters\n    factors, info, minuit = optimize_iminuit(function=ds.fcn, parameters=pars)\n\n    assert_allclose(ds.fcn(), 0, atol=1e-5)\n\n    par = pars[""x""]\n    par.min, par.max = 0, 10\n\n    result = confidence_iminuit(minuit=minuit, parameters=pars, parameter=par, sigma=1)\n\n    assert result[""success""]\n\n    assert_allclose(result[""errp""], 0.2)\n    assert_allclose(result[""errn""], 0.2)\n'"
gammapy/modeling/tests/test_parameter.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom gammapy.modeling import Parameter, Parameters\n\n\ndef test_parameter_init():\n    par = Parameter(""spam"", 42, ""deg"")\n    assert par.name == ""spam""\n    assert par.factor == 42\n    assert isinstance(par.factor, float)\n    assert par.scale == 1\n    assert isinstance(par.scale, float)\n    assert par.value == 42\n    assert isinstance(par.value, float)\n    assert par.unit == ""deg""\n    assert par.min is np.nan\n    assert par.max is np.nan\n    assert par.frozen is False\n\n    par = Parameter(""spam"", ""42 deg"")\n    assert par.factor == 42\n    assert par.scale == 1\n    assert par.unit == ""deg""\n\n    with pytest.raises(TypeError):\n        Parameter(1, 2)\n\n\ndef test_parameter_scale():\n    # Basic check how scale is used for value, min, max\n    par = Parameter(""spam"", 42, ""deg"", 10, 400, 500)\n\n    assert par.value == 420\n    assert par.min == 400\n    assert_allclose(par.factor_min, 40)\n    assert par.max == 500\n    assert_allclose(par.factor_max, 50)\n\n    par.value = 70\n    assert par.scale == 10\n    assert_allclose(par.factor, 7)\n\n\ndef test_parameter_quantity():\n    par = Parameter(""spam"", 42, ""deg"", 10)\n\n    quantity = par.quantity\n    assert quantity.unit == ""deg""\n    assert quantity.value == 420\n\n    par.quantity = ""70 deg""\n    assert_allclose(par.factor, 7)\n    assert par.scale == 10\n    assert par.unit == ""deg""\n\n\ndef test_parameter_repr():\n    par = Parameter(""spam"", 42, ""deg"")\n    assert repr(par).startswith(""Parameter(name="")\n\n\ndef test_parameter_to_dict():\n    par = Parameter(""spam"", 42, ""deg"")\n    d = par.to_dict()\n    assert isinstance(d[""unit""], str)\n\n\n@pytest.mark.parametrize(\n    ""method,value,factor,scale"",\n    [\n        # Check method=""scale10"" in detail\n        (""scale10"", 2e-10, 2, 1e-10),\n        (""scale10"", 2e10, 2, 1e10),\n        (""scale10"", -2e-10, -2, 1e-10),\n        (""scale10"", -2e10, -2, 1e10),\n        # Check that results are OK for very large numbers\n        # Regression test for https://github.com/gammapy/gammapy/issues/1883\n        (""scale10"", 9e35, 9, 1e35),\n        # Checks for the simpler method=""factor1""\n        (""factor1"", 2e10, 2, 1e10),\n        (""factor1"", -2e10, -2, 1e10),\n    ],\n)\ndef test_parameter_autoscale(method, value, factor, scale):\n    par = Parameter("""", value)\n    par.autoscale()\n    assert_allclose(par.factor, factor)\n    assert_allclose(par.scale, scale)\n    assert isinstance(par.scale, float)\n\n\n@pytest.fixture()\ndef pars():\n    return Parameters([Parameter(""spam"", 42, ""deg""), Parameter(""ham"", 99, ""TeV"")])\n\n\n@pytest.mark.xfail\ndef test_parameters_basics(pars):\n    # This applies a unit transformation\n    pars[""ham""].error = ""10000 GeV""\n    pars[""spam""].error = 0.1\n    assert_allclose(pars[""spam""].error, 0.1)\n    assert_allclose(pars[1].error, 10)\n\n\ndef test_parameters_copy(pars):\n    pars2 = pars.copy()\n    assert pars is not pars2\n    assert pars[0] is not pars2[0]\n\n\ndef test_parameters_from_stack():\n    a = Parameter(""a"", 1)\n    b = Parameter(""b"", 2)\n    c = Parameter(""c"", 3)\n\n    pars = Parameters([a, b]) + Parameters([]) + Parameters([c])\n    assert pars.names == [""a"", ""b"", ""c""]\n\n\ndef test_unique_parameters():\n    a = Parameter(""a"", 1)\n    b = Parameter(""b"", 2)\n    c = Parameter(""c"", 3)\n    parameters = Parameters([a, b, a, c])\n    assert parameters.names == [""a"", ""b"", ""a"", ""c""]\n    parameters_unique = parameters.unique_parameters\n    assert parameters_unique.names == [""a"", ""b"", ""c""]\n\n\ndef test_parameters_getitem(pars):\n    assert pars[1].name == ""ham""\n    assert pars[""ham""].name == ""ham""\n    assert pars[pars[1]].name == ""ham""\n\n    with pytest.raises(TypeError):\n        pars[42.3]\n\n    with pytest.raises(IndexError):\n        pars[3]\n\n    with pytest.raises(IndexError):\n        pars[""lamb""]\n\n    with pytest.raises(ValueError):\n        pars[Parameter(""bam!"", 99)]\n\n\ndef test_parameters_to_table(pars):\n    pars[""ham""].error = 1e-10\n    table = pars.to_table()\n    assert len(table) == 2\n    assert len(table.columns) == 7\n\n\ndef test_parameters_set_parameter_factors(pars):\n    pars.set_parameter_factors([77, 78])\n    assert_allclose(pars[""spam""].factor, 77)\n    assert_allclose(pars[""spam""].scale, 1)\n    assert_allclose(pars[""ham""].factor, 78)\n    assert_allclose(pars[""ham""].scale, 1)\n\n\ndef test_parameters_autoscale():\n    pars = Parameters([Parameter("""", 20)])\n    pars.autoscale()\n    assert_allclose(pars[0].factor, 2)\n    assert_allclose(pars[0].scale, 10)\n'"
gammapy/modeling/tests/test_sampling.py,1,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nimport pytest\nfrom gammapy.datasets import Datasets\nfrom gammapy.modeling.sampling import run_mcmc, ln_uniform_prior\nfrom gammapy.utils.testing import requires_dependency, requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef dataset():\n    dataset = Datasets.read(\n        ""$GAMMAPY_DATA/fermi-3fhl-crab/"",\n        ""Fermi-LAT-3FHL_datasets.yaml"",\n        ""Fermi-LAT-3FHL_models.yaml"",\n    )\n\n    # Define the free parameters and min, max values\n    parameters = dataset.models.parameters\n    parameters[""lon_0""].frozen = False\n    parameters[""lat_0""].frozen = False\n    parameters[""norm""].frozen = True\n    parameters[""alpha""].frozen = True\n    parameters[""beta""].frozen = True\n    parameters[""lat_0""].min = -90\n    parameters[""lat_0""].max = 90\n    parameters[""lon_0""].min = 0\n    parameters[""lon_0""].max = 360\n    parameters[""amplitude""].min = 0.01 * parameters[""amplitude""].value\n    parameters[""amplitude""].max = 100 * parameters[""amplitude""].value\n\n    return dataset\n\n\n@requires_data()\ndef test_lnprob(dataset):\n    # Testing priors and parameter bounds\n    parameters = dataset.models.parameters\n\n    # paramater is within min, max boundaries\n    assert ln_uniform_prior(dataset) == 0.0\n    # Setting amplitude outside min, max values\n    parameters[""amplitude""].value = 1000\n    assert ln_uniform_prior(dataset) == -np.inf\n\n\n@requires_dependency(""emcee"")\n@requires_data()\ndef test_runmcmc(dataset):\n    # Running a small MCMC on pregenerated datasets\n    import emcee\n\n    sampler = run_mcmc(dataset, nwalkers=6, nrun=10)  # to speedup the test\n    assert isinstance(sampler, emcee.ensemble.EnsembleSampler)\n'"
gammapy/modeling/tests/test_scipy.py,1,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom gammapy.modeling import Parameter, Parameters\nfrom gammapy.modeling.scipy import (\n    confidence_scipy,\n    optimize_scipy,\n    stat_profile_ul_scipy,\n)\n\n\nclass MyDataset:\n    def __init__(self, parameters):\n        self.parameters = parameters\n\n    def fcn(self):\n        x, y, z = [p.value for p in self.parameters]\n        x_opt, y_opt, z_opt = 2, 3e5, 4e-5\n        x_err, y_err, z_err = 0.2, 3e4, 4e-6\n        return (\n            ((x - x_opt) / x_err) ** 2\n            + ((y - y_opt) / y_err) ** 2\n            + ((z - z_opt) / z_err) ** 2\n        )\n\n\n@pytest.fixture()\ndef pars():\n    x = Parameter(""x"", 2.1)\n    y = Parameter(""y"", 3.1, scale=1e5)\n    z = Parameter(""z"", 4.1, scale=1e-5)\n    return Parameters([x, y, z])\n\n\n@pytest.mark.parametrize(""method"", [""Nelder-Mead"", ""L-BFGS-B"", ""Powell"", ""BFGS""])\ndef test_scipy_basic(pars, method):\n    ds = MyDataset(pars)\n    factors, info, optimizer = optimize_scipy(\n        function=ds.fcn, parameters=pars, method=method\n    )\n\n    assert info[""success""]\n    assert_allclose(ds.fcn(), 0, atol=1e-5)\n\n    # Check the result in parameters is OK\n    assert_allclose(pars[""x""].value, 2, rtol=1e-3)\n    assert_allclose(pars[""y""].value, 3e5, rtol=1e-3)\n    assert_allclose(pars[""z""].value, 4e-5, rtol=2e-2)\n\n\n@pytest.mark.parametrize(""method"", [""Nelder-Mead"", ""L-BFGS-B"", ""Powell""])\ndef test_scipy_frozen(pars, method):\n    ds = MyDataset(pars)\n    pars[""y""].frozen = True\n\n    factors, info, optimizer = optimize_scipy(\n        function=ds.fcn, parameters=pars, method=method\n    )\n\n    assert info[""success""]\n\n    assert_allclose(pars[""x""].value, 2, rtol=1e-4)\n    assert_allclose(pars[""y""].value, 3.1e5)\n    assert_allclose(pars[""z""].value, 4.0e-5, rtol=1e-4)\n    assert_allclose(ds.fcn(), 0.1111111, rtol=1e-5)\n\n\n@pytest.mark.parametrize(""method"", [""L-BFGS-B""])\ndef test_scipy_limits(pars, method):\n    ds = MyDataset(pars)\n    pars[""y""].min = 301000\n\n    factors, info, minuit = optimize_scipy(\n        function=ds.fcn, parameters=pars, method=method\n    )\n\n    assert info[""success""]\n\n    # Check the result in parameters is OK\n    assert_allclose(pars[""x""].value, 2, rtol=1e-2)\n    assert_allclose(pars[""y""].value, 301000, rtol=1e-3)\n\n\ndef test_scipy_confidence(pars):\n    ds = MyDataset(pars)\n    factors, info, _ = optimize_scipy(function=ds.fcn, parameters=pars)\n\n    assert_allclose(ds.fcn(), 0, atol=1e-5)\n\n    par = pars[""x""]\n    par.min, par.max = 0, 10\n\n    result = confidence_scipy(function=ds.fcn, parameters=pars, parameter=par, sigma=1)\n\n    assert result[""success_errp""]\n    assert result[""success_errn""]\n\n    assert_allclose(result[""errp""], 0.2, rtol=1e-3)\n    assert_allclose(result[""errn""], 0.2, rtol=1e-3)\n\n\ndef test_stat_profile_ul_scipy():\n    x = np.linspace(-5, 5, 7)\n    y = x ** 2\n    ul = stat_profile_ul_scipy(x, y)\n    assert_allclose(ul, 2)\n\n    ul = stat_profile_ul_scipy(x, x, interp_scale=""lin"")\n    assert_allclose(ul, 4)\n'"
gammapy/modeling/tests/test_sherpa.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nfrom gammapy.modeling import Parameter, Parameters\nfrom gammapy.modeling.sherpa import optimize_sherpa\n\npytest.importorskip(""sherpa"")\n\n\nclass MyDataset:\n    def __init__(self, parameters):\n        self.parameters = parameters\n\n    def fcn(self):\n        x, y, z = [p.value for p in self.parameters]\n        x_opt, y_opt, z_opt = 2, 3e5, 4e-5\n        x_err, y_err, z_err = 0.2, 3e4, 4e-6\n        return (\n            ((x - x_opt) / x_err) ** 2\n            + ((y - y_opt) / y_err) ** 2\n            + ((z - z_opt) / z_err) ** 2\n        )\n\n\n@pytest.fixture()\ndef pars():\n    x = Parameter(""x"", 2.1)\n    y = Parameter(""y"", 3.1, scale=1e5)\n    z = Parameter(""z"", 4.1, scale=1e-5)\n    return Parameters([x, y, z])\n\n\n# TODO: levmar doesn\'t work yet; needs array of statval as return in likelihood\n# method=\'gridsearch\' would require very low tolerance asserts, not added for now\n\n\n@pytest.mark.parametrize(""method"", [""moncar"", ""simplex""])\ndef test_sherpa(method, pars):\n    ds = MyDataset(pars)\n    factors, info, _ = optimize_sherpa(function=ds.fcn, parameters=pars, method=method)\n\n    assert info[""success""]\n    assert_allclose(ds.fcn(), 0, atol=1e-12)\n\n    # Check the result in parameters is OK\n    assert_allclose(pars[""x""].value, 2)\n    assert_allclose(pars[""y""].value, 3e5)\n    assert_allclose(pars[""z""].value, 4e-5)\n\n    # Check that sherpa sees the parameter factors correctly\n    assert_allclose(factors, [2, 3, 4])\n\n\ndef test_sherpa_frozen(pars):\n    ds = MyDataset(pars)\n    pars[""y""].frozen = True\n\n    factors, info, _ = optimize_sherpa(function=ds.fcn, parameters=pars)\n\n    assert info[""success""]\n    assert_allclose(pars[""x""].value, 2)\n    assert_allclose(pars[""y""].value, 3.1e5)\n    assert_allclose(pars[""z""].value, 4.0e-5)\n    assert_allclose(ds.fcn(), 0.11111111, rtol=1e-6)\n\n\ndef test_sherpa_limits(pars):\n    ds = MyDataset(pars)\n    pars[""y""].min = 301000\n\n    factors, info, _ = optimize_sherpa(function=ds.fcn, parameters=pars)\n\n    assert info[""success""]\n\n    # Check the result in parameters is OK\n    assert_allclose(pars[""x""].value, 2, rtol=1e-2)\n    assert_allclose(pars[""y""].value, 301000, rtol=1e-3)\n'"
gammapy/scripts/tests/__init__.py,0,b''
gammapy/scripts/tests/test_analysis.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom gammapy.scripts.main import cli\nfrom gammapy.utils.testing import requires_data, run_cli\nfrom ...analysis.tests.test_analysis import get_example_config\n\n\ndef test_cli_analysis_config(tmp_path):\n    path_config = tmp_path / ""config.yaml""\n    args = [""analysis"", ""config"", f""--filename={path_config}""]\n    run_cli(cli, args)\n    assert path_config.exists()\n\n\n@requires_data()\ndef test_cli_analysis_run(tmp_path):\n    path_config = tmp_path / ""config.yaml""\n    config = get_example_config(""1d"")\n    config.write(path_config)\n    path_datasets = tmp_path / ""datasets""\n    args = [\n        ""analysis"",\n        ""run"",\n        f""--filename={path_config}"",\n        f""--out={path_datasets}"",\n        ""--overwrite"",\n    ]\n    run_cli(cli, args)\n    assert path_datasets.exists()\n'"
gammapy/scripts/tests/test_download.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom gammapy.scripts.main import cli\nfrom gammapy.utils.testing import requires_dependency, run_cli\n\n\n@pytest.fixture(scope=""session"")\ndef config():\n    return {\n        ""release"": ""0.8"",\n        ""dataset"": ""dark_matter_spectra"",\n        ""notebook"": ""astro_dark_matter"",\n        ""imagefile"": ""gammapy_datastore_butler.png"",\n        ""script"": ""example_2_gauss"",\n        ""envfilename"": ""gammapy-0.8-environment.yml"",\n    }\n\n\ndef test_cli_download_help():\n    result = run_cli(cli, [""download"", ""--help""])\n    assert ""Usage"" in result.output\n\n\n@requires_dependency(""parfive"")\n@pytest.mark.remote_data\ndef test_cli_download_datasets(tmp_path, config):\n    args = [\n        ""download"",\n        ""datasets"",\n        f""--src={config[\'dataset\']}"",\n        f""--out={tmp_path}"",\n        f""--release={config[\'release\']}"",\n    ]\n    result = run_cli(cli, args)\n\n    assert (tmp_path / config[""dataset""]).exists()\n    assert ""GAMMAPY_DATA"" in result.output\n\n\n@requires_dependency(""parfive"")\n@pytest.mark.remote_data\ndef test_cli_download_notebooks(tmp_path, config):\n    args = [\n        ""download"",\n        ""notebooks"",\n        f""--src={config[\'notebook\']}"",\n        f""--out={tmp_path}"",\n        f""--release={config[\'release\']}"",\n    ]\n    run_cli(cli, args)\n\n    assert (tmp_path / config[""envfilename""]).exists()\n    path = tmp_path / f""notebooks-{config[\'release\']}/{config[\'notebook\']}.ipynb""\n    assert path.exists()\n\n\n@requires_dependency(""parfive"")\n@pytest.mark.remote_data\ndef test_cli_download_scripts(tmp_path, config):\n    args = [\n        ""download"",\n        ""scripts"",\n        f""--src={config[\'script\']}"",\n        f""--out={tmp_path}"",\n        f""--release={config[\'release\']}"",\n    ]\n    run_cli(cli, args)\n\n    assert (tmp_path / config[""envfilename""]).exists()\n    assert (tmp_path / f""scripts-{config[\'release\']}/{config[\'script\']}.py"").exists()\n\n\n@requires_dependency(""parfive"")\n@pytest.mark.remote_data\ndef test_cli_download_tutorials(tmp_path, config):\n    option_out = f""--out={tmp_path}""\n    nboption_src = f""--src={config[\'notebook\']}""\n    scoption_src = f""--src={config[\'script\']}""\n    option_release = f""--release={config[\'release\']}""\n    dsdirname = ""datasets""\n    nbdirname = f""notebooks-{config[\'release\']}""\n    scdirname = f""scripts-{config[\'release\']}""\n    nbfilename = f""{config[\'notebook\']}.ipynb""\n    scfilename = f""{config[\'script\']}.py""\n\n    args = [""download"", ""tutorials"", nboption_src, option_out, option_release]\n    result = run_cli(cli, args)\n    assert (tmp_path / config[""envfilename""]).exists()\n    assert (tmp_path / nbdirname / nbfilename).exists()\n    assert (tmp_path / dsdirname / config[""dataset""]).exists()\n    assert ""GAMMAPY_DATA"" in result.output\n    assert ""jupyter lab"" in result.output\n\n    args = [""download"", ""tutorials"", scoption_src, option_out, option_release]\n    result = run_cli(cli, args)\n    assert (tmp_path / config[""envfilename""]).exists()\n    assert (tmp_path / scdirname / scfilename).exists()\n    assert ""GAMMAPY_DATA"" in result.output\n    assert ""jupyter lab"" in result.output\n'"
gammapy/scripts/tests/test_info.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom gammapy.scripts.main import cli\nfrom gammapy.utils.testing import run_cli\n\n\ndef test_cli_info_help():\n    result = run_cli(cli, [""info"", ""--help""])\n    assert ""Usage"" in result.output\n\n\ndef test_cli_info_no_args():\n    # No arguments should print all infos\n    result = run_cli(cli, [""info""])\n    assert ""System"" in result.output\n    assert ""Gammapy package"" in result.output\n    assert ""Other packages"" in result.output\n    assert ""Gammapy environment variables"" in result.output\n'"
gammapy/scripts/tests/test_main.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom gammapy import __version__\nfrom gammapy.scripts.main import cli\nfrom gammapy.utils.testing import run_cli\n\n\ndef test_cli_no_args():\n    # No arguments should print help\n    result = run_cli(cli, [])\n    assert ""Usage"" in result.output\n\n\ndef test_cli_help():\n    result = run_cli(cli, [""--help""])\n    assert ""Usage"" in result.output\n\n\ndef test_cli_version():\n    result = run_cli(cli, [""--version""])\n    assert f""gammapy version {__version__}"" in result.output\n\n\ndef test_check_logging():\n    result = run_cli(cli, [""check"", ""logging""])\n    assert f""output"" in result.output\n'"
gammapy/stats/tests/__init__.py,0,b''
gammapy/stats/tests/test_counts_statistic.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom gammapy.stats import CashCountsStatistic, WStatCountsStatistic\n\nref_array = np.ones((3, 2, 4))\n\nvalues = [\n    (1, 2, [-1.0, -0.78339367, 0.433396]),\n    (5, 1, [4.0, 2.84506224, 4.4402745934e-3]),\n    (10, 5, [5.0, 1.96543726, 0.049363650550]),\n    (100, 23, [77.0, 11.8294207, 2.75033324833345e-32]),\n    (1, 20, [-19, -5.65760863, 1.534966634510499e-08]),\n    (5 * ref_array, 1 * ref_array, [4.0, 2.84506224, 4.4402745934e-3]),\n]\n\n\n@pytest.mark.parametrize((""n_on"", ""mu_bkg"", ""result""), values)\ndef test_cash_basic(n_on, mu_bkg, result):\n    stat = CashCountsStatistic(n_on, mu_bkg)\n    excess = stat.excess\n    significance = stat.significance\n    p_value = stat.p_value\n\n    assert_allclose(excess, result[0])\n    assert_allclose(significance, result[1], atol=1e-5)\n    assert_allclose(p_value, result[2], atol=1e-5)\n\n\nvalues = [\n    (1, 2, [-1.0, 1.35767667]),\n    (5, 1, [-1.915916, 2.581106]),\n    (10, 5, [-2.838105, 3.504033]),\n    (100, 23, [-9.669482, 10.336074]),\n    (1, 20, [-1, 1.357677]),\n    (5 * ref_array, 1 * ref_array, [-1.915916, 2.581106]),\n]\n\n\n@pytest.mark.parametrize((""n_on"", ""mu_bkg"", ""result""), values)\ndef test_cash_errors(n_on, mu_bkg, result):\n    stat = CashCountsStatistic(n_on, mu_bkg)\n    errn = stat.compute_errn()\n    errp = stat.compute_errp()\n\n    assert_allclose(errn, result[0], atol=1e-5)\n    assert_allclose(errp, result[1], atol=1e-5)\n\n\nvalues = [\n    (1, 2, [5.869898]),\n    (5, 1, [13.98959]),\n    (10, 5, [17.696064]),\n    (100, 23, [110.07206]),\n    (1, 20, [4.711538]),\n    (5 * ref_array, 1 * ref_array, [13.98959]),\n]\n\n\n@pytest.mark.parametrize((""n_on"", ""mu_bkg"", ""result""), values)\ndef test_cash_ul(n_on, mu_bkg, result):\n    stat = CashCountsStatistic(n_on, mu_bkg)\n    ul = stat.compute_upper_limit()\n\n    assert_allclose(ul, result[0], atol=1e-5)\n\n\nvalues = [\n    (100, 5, 54.012755),\n    (100, -5, -45.631273),\n    (1, -2, np.nan),\n    ([1, 2], 5, [8.327276, 10.550546]),\n]\n\n\n@pytest.mark.parametrize((""mu_bkg"", ""significance"", ""result""), values)\ndef test_cash_excess_matching_significance(mu_bkg, significance, result):\n    stat = CashCountsStatistic(1, mu_bkg)\n    excess = stat.excess_matching_significance(significance)\n\n    assert_allclose(excess, result, atol=1e-3)\n\n\nvalues = [\n    (1, 2, 1, [-1.0, -0.5829220133009171, 0.55994580085]),\n    (5, 1, 1, [4.0, 1.7061745691234782, 0.087975582112]),\n    (10, 5, 0.3, [8.5, 3.5853812867949024, 3.365860865528742e-4]),\n    (10, 23, 0.1, [7.7, 3.443415522820395, 5.74416016688779e-4]),\n    (1, 20, 1.0, [-19, -4.590373638528086, 4.424532535784618e-06]),\n    (\n        5 * ref_array,\n        1 * ref_array,\n        1 * ref_array,\n        [4.0, 1.7061745691234782, 0.087975582112],\n    ),\n]\n\n\n@pytest.mark.parametrize((""n_on"", ""n_off"", ""alpha"", ""result""), values)\ndef test_wstat_basic(n_on, n_off, alpha, result):\n    stat = WStatCountsStatistic(n_on, n_off, alpha)\n    excess = stat.excess\n    significance = stat.significance\n    p_value = stat.p_value\n\n    assert_allclose(excess, result[0])\n    assert_allclose(significance, result[1], atol=1e-5)\n    assert_allclose(p_value, result[2], atol=1e-5)\n\n\nvalues = [\n    (1, 2, 1, [-1.942465, 1.762589]),\n    (5, 1, 1, [-2.310459, 2.718807]),\n    (10, 5, 0.3, [-2.932472, 3.55926]),\n    (10, 23, 0.1, [-2.884366, 3.533279]),\n    (1, 20, 1.0, [-4.897018, 4.299083]),\n    (5 * ref_array, 1 * ref_array, 1 * ref_array, [-2.310459, 2.718807]),\n]\n\n\n@pytest.mark.parametrize((""n_on"", ""n_off"", ""alpha"", ""result""), values)\ndef test_wstat_errors(n_on, n_off, alpha, result):\n    stat = WStatCountsStatistic(n_on, n_off, alpha)\n    errn = stat.compute_errn()\n    errp = stat.compute_errp()\n\n    assert_allclose(errn, result[0], atol=1e-5)\n    assert_allclose(errp, result[1], atol=1e-5)\n\n\nvalues = [\n    (1, 2, 1, [6.272627]),\n    (5, 1, 1, [14.222831]),\n    (10, 5, 0.3, [21.309229]),\n    (10, 23, 0.1, [20.45803]),\n    (1, 20, 1.0, [4.884418]),\n    (5 * ref_array, 1 * ref_array, 1 * ref_array, [14.222831]),\n]\n\n\n@pytest.mark.parametrize((""n_on"", ""n_off"", ""alpha"", ""result""), values)\ndef test_wstat_ul(n_on, n_off, alpha, result):\n    stat = WStatCountsStatistic(n_on, n_off, alpha)\n    ul = stat.compute_upper_limit()\n\n    assert_allclose(ul, result[0], atol=1e-5)\n\n\nvalues = [\n    ([10, 20], [0.1, 0.1], 5, [9.82966, 12.038423]),\n    ([10, 10], [0.1, 0.3], 5, [9.82966, 16.664516]),\n    ([10], [0.1], 3, [4.818497]),\n    (\n        [[10, 20], [10, 20]],\n        [[0.1, 0.1], [0.1, 0.1]],\n        5,\n        [[9.82966, 12.038423], [9.82966, 12.038423]],\n    ),\n]\n\n\n@pytest.mark.parametrize((""n_off"", ""alpha"", ""significance"", ""result""), values)\ndef test_wstat_excess_matching_significance(n_off, alpha, significance, result):\n    stat = WStatCountsStatistic(1, n_off, alpha)\n    excess = stat.excess_matching_significance(significance)\n\n    assert_allclose(excess, result, atol=1e-3)\n'"
gammapy/stats/tests/test_feldman_cousins.py,8,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nimport scipy.stats\nfrom numpy.testing import assert_allclose\nfrom gammapy.stats import (\n    fc_construct_acceptance_intervals,\n    fc_construct_acceptance_intervals_pdfs,\n    fc_find_acceptance_interval_gauss,\n    fc_find_acceptance_interval_poisson,\n    fc_find_average_upper_limit,\n    fc_find_limit,\n    fc_fix_limits,\n    fc_get_limits,\n)\n\n\ndef test_acceptance_interval_gauss():\n    sigma = 1\n    n_sigma = 10\n    n_step = 1000\n    cl = 0.90\n\n    x_bins = np.linspace(-n_sigma * sigma, n_sigma * sigma, n_step, endpoint=True)\n\n    # The test reverses a result from the Feldman and Cousins paper. According\n    # to Table X, for a measured value of 2.6 the 90% confidence interval\n    # should be 1.02 and 4.24. Reversed, that means that for mu=1.02, the\n    # acceptance interval should end at 2.6 and for mu=4.24 should start at\n    # 2.6.\n    (x_min, x_max) = fc_find_acceptance_interval_gauss(1.02, sigma, x_bins, cl)\n    assert_allclose(x_max, 2.6, atol=0.1)\n\n    (x_min, x_max) = fc_find_acceptance_interval_gauss(4.24, sigma, x_bins, cl)\n    assert_allclose(x_min, 2.6, atol=0.1)\n\n    # At mu=0, confidence interval should start at the negative x_bins range.\n    (x_min, x_max) = fc_find_acceptance_interval_gauss(0, sigma, x_bins, cl)\n    assert_allclose(x_min, -n_sigma * sigma)\n\n    # Pass too few x_bins to reach confidence level.\n    x_bins = np.linspace(-sigma, sigma, n_step, endpoint=True)\n    with pytest.raises(ValueError):\n        fc_find_acceptance_interval_gauss(0, 1, x_bins, cl)\n\n\ndef test_acceptance_interval_poisson():\n    background = 0.5\n    n_bins_x = 100\n    cl = 0.90\n\n    x_bins = np.arange(0, n_bins_x)\n\n    # The test reverses a result from the Feldman and Cousins paper. According\n    # to Table IV, for a measured value of 10 the 90% confidence interval\n    # should be 5.00 and 16.00. Reversed that means that for mu=5.0, the\n    # acceptance interval should end at 10 and for mu=16.00 should start at 10.\n    (x_min, x_max) = fc_find_acceptance_interval_poisson(5.00, background, x_bins, cl)\n    assert_allclose(x_max, 10)\n\n    (x_min, x_max) = fc_find_acceptance_interval_poisson(16.00, background, x_bins, cl)\n    assert_allclose(x_min, 10)\n\n    # Pass too few x_bins to reach confidence level.\n    with pytest.raises(ValueError):\n        fc_find_acceptance_interval_poisson(0, 7, x_bins[0:10], cl)\n\n\ndef test_numerical_confidence_interval_pdfs():\n    background = 3.0\n    step_width_mu = 0.005\n    mu_min = 0\n    mu_max = 15\n    n_bins_x = 50\n    cl = 0.90\n\n    x_bins = np.arange(0, n_bins_x)\n    mu_bins = np.linspace(\n        mu_min, mu_max, int(mu_max / step_width_mu) + 1, endpoint=True\n    )\n\n    matrix = [scipy.stats.poisson(mu + background).pmf(x_bins) for mu in mu_bins]\n\n    acceptance_intervals = fc_construct_acceptance_intervals_pdfs(matrix, cl)\n\n    lower_limit_num, upper_limit_num, _ = fc_get_limits(\n        mu_bins, x_bins, acceptance_intervals\n    )\n\n    fc_fix_limits(lower_limit_num, upper_limit_num)\n\n    x_measured = 6\n    upper_limit = fc_find_limit(x_measured, upper_limit_num, mu_bins)\n    lower_limit = fc_find_limit(x_measured, lower_limit_num, mu_bins)\n\n    # Values are taken from Table IV in the Feldman and Cousins paper.\n    assert_allclose(upper_limit, 8.47, atol=0.01)\n    assert_allclose(lower_limit, 0.15, atol=0.01)\n\n    # A value which is not inside the x axis range should raise an exception\n    with pytest.raises(ValueError):\n        fc_find_limit(51, upper_limit_num, mu_bins)\n\n    # Calculate the average upper limit. The upper limit calculated here is\n    # only defined for a small x range, so limit the x bins here so the\n    # calculation of the average limit is meaningful.\n    average_upper_limit = fc_find_average_upper_limit(\n        x_bins, matrix, upper_limit_num, mu_bins\n    )\n\n    # Values are taken from Table XII in the Feldman and Cousins paper.\n    # A higher accuracy would require a higher mu_max, which would increase\n    # the computation time.\n    assert_allclose(average_upper_limit, 4.42, atol=0.1)\n\n\ndef test_numerical_confidence_interval_values():\n    sigma = 1\n    n_sigma = 10\n    n_bins_x = 100\n    step_width_mu = 0.05\n    mu_min = 0\n    mu_max = 8\n    cl = 0.90\n\n    x_bins = np.linspace(-n_sigma * sigma, n_sigma * sigma, n_bins_x, endpoint=True)\n    mu_bins = np.linspace(\n        mu_min, mu_max, int(mu_max / step_width_mu) + 1, endpoint=True\n    )\n\n    rng = np.random.RandomState(seed=0)\n\n    dist = {\n        mu: scipy.stats.norm.rvs(loc=mu, scale=sigma, size=2500, random_state=rng)\n        for mu in mu_bins\n    }\n\n    acceptance_intervals = fc_construct_acceptance_intervals(dist, x_bins, cl)\n\n    lower_limit_num, upper_limit_num, _ = fc_get_limits(\n        mu_bins, x_bins, acceptance_intervals\n    )\n\n    fc_fix_limits(lower_limit_num, upper_limit_num)\n\n    x_measured = 1.7\n    upper_limit = fc_find_limit(x_measured, upper_limit_num, mu_bins)\n\n    # Value taken from Table X in the Feldman and Cousins paper.\n    assert_allclose(upper_limit, 3.34, atol=0.1)\n'"
gammapy/stats/tests/test_fit_statistics.py,6,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom gammapy import stats\n\n\n@pytest.fixture\ndef test_data():\n    """"""Test data for fit statistics tests""""""\n    test_data = dict(\n        mu_sig=[\n            0.59752422,\n            9.13666449,\n            12.98288095,\n            5.56974565,\n            13.52509804,\n            11.81725635,\n            0.47963765,\n            11.17708176,\n            5.18504894,\n            8.30202394,\n        ],\n        n_on=[0, 13, 7, 5, 11, 16, 0, 9, 3, 12],\n        n_off=[0, 7, 4, 0, 18, 7, 1, 5, 12, 25],\n        alpha=[\n            0.83746243,\n            0.17003354,\n            0.26034507,\n            0.69197751,\n            0.89557033,\n            0.34068848,\n            0.0646732,\n            0.86411967,\n            0.29087245,\n            0.74108241,\n        ],\n    )\n\n    test_data[""staterror""] = np.sqrt(test_data[""n_on""])\n\n    return test_data\n\n\n@pytest.fixture\ndef reference_values():\n    """"""Reference values for fit statistics test.\n\n    Produced using sherpa stats module in dev/sherpa/stats/compare_wstat.py\n    """"""\n    return dict(\n        wstat=[\n            1.19504844,\n            0.625311794002,\n            4.25810886127,\n            0.0603765381044,\n            11.7285002468,\n            0.206014834301,\n            1.084611,\n            2.72972381792,\n            4.60602990838,\n            7.51658734973,\n        ],\n        cash=[\n            1.19504844,\n            -39.24635098872072,\n            -9.925081055136996,\n            -6.034002586236575,\n            -30.249839537105466,\n            -55.39143500383233,\n            0.9592753,\n            -21.095413867175516,\n            0.49542219758430406,\n            -34.19193611846045,\n        ],\n        cstat=[\n            1.19504844,\n            1.4423323052792387,\n            3.3176610316373925,\n            0.06037653810442922,\n            0.5038564644586838,\n            1.3314041078406706,\n            0.9592753,\n            0.4546285248764317,\n            1.0870959295929628,\n            1.4458234764515652,\n        ],\n    )\n\n\ndef test_wstat(test_data, reference_values):\n    statsvec = stats.wstat(\n        n_on=test_data[""n_on""],\n        mu_sig=test_data[""mu_sig""],\n        n_off=test_data[""n_off""],\n        alpha=test_data[""alpha""],\n        extra_terms=True,\n    )\n\n    assert_allclose(statsvec, reference_values[""wstat""])\n\n\ndef test_cash(test_data, reference_values):\n    statsvec = stats.cash(n_on=test_data[""n_on""], mu_on=test_data[""mu_sig""])\n    assert_allclose(statsvec, reference_values[""cash""])\n\n\ndef test_cstat(test_data, reference_values):\n    statsvec = stats.cstat(n_on=test_data[""n_on""], mu_on=test_data[""mu_sig""])\n    assert_allclose(statsvec, reference_values[""cstat""])\n\n\ndef test_cash_sum_cython(test_data):\n    counts = np.array(test_data[""n_on""], dtype=float)\n    npred = np.array(test_data[""mu_sig""], dtype=float)\n    stat = stats.cash_sum_cython(counts=counts, npred=npred)\n    ref = stats.cash(counts, npred).sum()\n    assert_allclose(stat, ref)\n\n\ndef test_wstat_corner_cases():\n    """"""test WSTAT formulae for corner cases""""""\n    n_on = 0\n    n_off = 5\n    mu_sig = 2.3\n    alpha = 0.5\n\n    actual = stats.wstat(n_on=n_on, mu_sig=mu_sig, n_off=n_off, alpha=alpha)\n    desired = 2 * (mu_sig + n_off * np.log(1 + alpha))\n    assert_allclose(actual, desired)\n\n    actual = stats.get_wstat_mu_bkg(n_on=n_on, mu_sig=mu_sig, n_off=n_off, alpha=alpha)\n    desired = n_off / (alpha + 1)\n    assert_allclose(actual, desired)\n\n    # n_off = 0 and mu_sig < n_on * (alpha / alpha + 1)\n    n_on = 9\n    n_off = 0\n    mu_sig = 2.3\n    alpha = 0.5\n\n    actual = stats.wstat(n_on=n_on, mu_sig=mu_sig, n_off=n_off, alpha=alpha)\n    desired = -2 * (mu_sig * (1.0 / alpha) + n_on * np.log(alpha / (1 + alpha)))\n    assert_allclose(actual, desired)\n\n    actual = stats.get_wstat_mu_bkg(n_on=n_on, mu_sig=mu_sig, n_off=n_off, alpha=alpha)\n    desired = n_on / (1 + alpha) - (mu_sig / alpha)\n    assert_allclose(actual, desired)\n\n    # n_off = 0 and mu_sig > n_on * (alpha / alpha + 1)\n    n_on = 5\n    n_off = 0\n    mu_sig = 5.3\n    alpha = 0.5\n\n    actual = stats.wstat(n_on=n_on, mu_sig=mu_sig, n_off=n_off, alpha=alpha)\n    desired = 2 * (mu_sig + n_on * (np.log(n_on) - np.log(mu_sig) - 1))\n    assert_allclose(actual, desired)\n\n    actual = stats.get_wstat_mu_bkg(n_on=n_on, mu_sig=mu_sig, n_off=n_off, alpha=alpha)\n    assert_allclose(actual, 0)\n'"
gammapy/time/tests/__init__.py,0,b''
gammapy/time/tests/test_period.py,8,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom astropy.timeseries import LombScargle\nfrom gammapy.time import plot_periodogram, robust_periodogram\nfrom gammapy.utils.testing import requires_dependency\n\n\ndef simulate_test_data(period, amplitude, t_length, n_data, n_obs, n_outliers):\n    """"""\n    Function for creating an unevenly data biased by outliers.\n\n    As underlying model, a single harmonic model is chosen.\n    First, an evenly sampled test data set is generated.\n    It is distorted by randomly drawing data points from an uniform distribution.\n    Outliers are simulated as exponential burst with ten times the\n    amplitude decreasing with a characteristic time length of 1.\n    Flux errors are assumed to be gaussian and homoscedastic.\n\n    Returns arrays for time, flux and flux error and the resolution of the test data.\n\n    Parameters\n    ----------\n    period : float\n        period of model\n    amplitude : float\n        amplitude of model\n    t_length : float\n        length of data set in units of time\n    n_data : int\n        number of points for the test data\n    n_obs : int\n        number of unevenly data points for the observation data\n    n_outliers : int\n        number of outliers in the test data\n\n    Returns\n    ----------\n    t : `numpy.ndarray`\n        time for observation\n    dt : float\n        time resolution of test data\n    y : `numpy.ndarray`\n        flux of observation\n    dy : `numpy.ndarray`\n        flux error of observation\n    """"""\n    rand = np.random.RandomState(42)\n    dt = t_length / n_data\n    t = np.linspace(0, t_length, n_data)\n    t_obs = np.sort(rand.choice(t, n_obs, replace=False))\n    n_outliers = n_outliers\n    dmag = rand.normal(0, 1, n_data) * -(1 ** (rand.randint(2, size=n_data)))\n    dmag_obs = dmag[np.searchsorted(t, t_obs)]\n    outliers = rand.randint(0, t.size, n_outliers)\n    mag = amplitude * np.sin(2 * np.pi * t / period) + dmag\n\n    for n in range(n_outliers):\n        mask = t >= outliers[n]\n        mag[mask] = mag[mask] + 10 * amplitude * np.exp(-1 * (t[mask] - outliers[n]))\n\n    mag_obs = mag[np.searchsorted(t, t_obs)]\n    return dict(t=t_obs, dt=dt, y=mag_obs, dy=dmag_obs)\n\n\ndef fap_astropy(power, freq, t, y, dy, method=dict(baluev=0)):\n    """"""\n    Calls the `~astropy`-method `false_alarm_probability`.\n    Function for estimation of periodogram peak significance.\n    Assumes Gaussian white noise light curve.\n\n    Returns a dict with the false alarm probability for each method.\n\n    Parameters\n    ----------\n    power : `numpy.ndarray`\n        power of periodogram\n    freq : `numpy.ndarray`\n        frequencies at withc the periodogram is computed\n    t, y, dy : `numpy.ndarray`\n        time, flux and flux error or light curve\n    method : dict\n        dictionary of methods with their respective false alarm probability\n\n    Returns\n    -------\n    fap : dict\n        false alarm probability dictionary (see description above).\n    """"""\n    ls = LombScargle(t, y, dy, normalization=""standard"")\n    maximum_frequency = freq.max()\n\n    def _calc_fap(method, **kwargs):\n        return ls.false_alarm_probability(\n            power.max(), maximum_frequency=maximum_frequency, method=method, **kwargs\n        )\n\n    fap = {}\n\n    if ""single"" in method:\n        fap[""single""] = _calc_fap(""single"")\n    if ""naive"" in method:\n        fap[""naive""] = _calc_fap(""naive"")\n    if ""davies"" in method:\n        fap[""davies""] = _calc_fap(""davies"")\n    if ""baluev"" in method:\n        fap[""baluev""] = _calc_fap(""baluev"")\n    if ""bootstrap"" in method:\n        fap[""bootstrap""] = _calc_fap(\n            ""bootstrap"", method_kwds=dict(n_bootstraps=100, random_seed=42)\n        )\n\n    return fap\n\n\n@pytest.mark.parametrize(\n    ""pars"",\n    [\n        dict(\n            period=7,\n            amplitude=2,\n            t_length=100,\n            n_data=1000,\n            n_obs=500,\n            n_outliers=50,\n            periods=np.linspace(0.5, 100, 200),\n            loss=""cauchy"",\n            scale=1,\n            fap=dict(\n                single=2.855680054527823e-93,\n                naive=5.705643031869405e-91,\n                davies=6.752853065345455e-90,\n                baluev=6.752853065345455e-90,\n                bootstrap=0.43,\n            ),\n        )\n    ],\n)\ndef test_period(pars):\n    test_data = simulate_test_data(\n        pars[""period""],\n        pars[""amplitude""],\n        pars[""t_length""],\n        pars[""n_data""],\n        pars[""n_obs""],\n        pars[""n_outliers""],\n    )\n\n    periodogram = robust_periodogram(\n        test_data[""t""],\n        test_data[""y""],\n        test_data[""dy""],\n        periods=pars[""periods""],\n        loss=pars[""loss""],\n        scale=pars[""scale""],\n    )\n\n    fap = fap_astropy(\n        periodogram[""power""],\n        1.0 / periodogram[""periods""],\n        test_data[""t""],\n        test_data[""y""],\n        test_data[""dy""],\n        pars[""fap""],\n    )\n\n    assert_allclose(\n        periodogram[""best_period""], pars[""period""], atol=pars[""periods""].min()\n    )\n    assert_allclose(fap[""single""], pars[""fap""][""single""], rtol=1e-3)\n    assert_allclose(fap[""naive""], pars[""fap""][""naive""], rtol=1e-3)\n    assert_allclose(fap[""davies""], pars[""fap""][""davies""], rtol=1e-3)\n    assert_allclose(fap[""baluev""], pars[""fap""][""baluev""], rtol=1e-3)\n    assert_allclose(fap[""bootstrap""], pars[""fap""][""bootstrap""], rtol=1e-3)\n\n\n@requires_dependency(""matplotlib"")\ndef test_plot_periodogram():\n    pars = dict(\n        period=7,\n        amplitude=2,\n        t_length=100,\n        n_data=1000,\n        n_obs=500,\n        n_outliers=50,\n        loss=""cauchy"",\n        scale=1,\n    )\n    test_data = simulate_test_data(\n        pars[""period""],\n        pars[""amplitude""],\n        pars[""t_length""],\n        pars[""n_data""],\n        pars[""n_obs""],\n        pars[""n_outliers""],\n    )\n\n    periodogram = robust_periodogram(\n        test_data[""t""],\n        test_data[""y""],\n        test_data[""dy""],\n        loss=pars[""loss""],\n        scale=pars[""scale""],\n    )\n\n    fap = fap_astropy(\n        periodogram[""power""],\n        1.0 / periodogram[""periods""],\n        test_data[""t""],\n        test_data[""y""],\n        test_data[""dy""],\n    )\n\n    plot_periodogram(\n        test_data[""t""],\n        test_data[""y""],\n        periodogram[""periods""],\n        periodogram[""power""],\n        test_data[""dy""],\n        periodogram[""best_period""],\n        max(fap.values()),\n    )\n'"
gammapy/time/tests/test_variability.py,1,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom astropy.table import Column, Table\nfrom astropy.time import Time\nfrom gammapy.estimators import LightCurve\nfrom gammapy.time import compute_chisq, compute_fvar\nfrom gammapy.utils.testing import assert_quantity_allclose\n\n\n@pytest.fixture(scope=""session"")\ndef lc():\n    meta = dict(TIMESYS=""utc"")\n\n    table = Table(\n        meta=meta,\n        data=[\n            Column(Time([""2010-01-01"", ""2010-01-03""]).mjd, ""time_min""),\n            Column(Time([""2010-01-03"", ""2010-01-10""]).mjd, ""time_max""),\n            Column([1e-11, 3e-11], ""flux"", unit=""cm-2 s-1""),\n            Column([0.1e-11, 0.3e-11], ""flux_err"", unit=""cm-2 s-1""),\n            Column([np.nan, 3.6e-11], ""flux_ul"", unit=""cm-2 s-1""),\n            Column([False, True], ""is_ul""),\n        ],\n    )\n\n    return LightCurve(table=table)\n\n\ndef test_lightcurve_fvar(lc):\n    flux = lc.table[""flux""].astype(""float64"")\n    flux_err = lc.table[""flux_err""].astype(""float64"")\n    fvar, fvar_err = compute_fvar(flux, flux_err)\n    assert_allclose(fvar, 0.6982120021884471)\n    # Note: the following tolerance is very low in the next assert,\n    # because results differ by ~ 1e-3 between different machines\n    assert_allclose(fvar_err, 0.07905694150420949, rtol=1e-2)\n\n\ndef test_lightcurve_chisq(lc):\n    flux = lc.table[""flux""].astype(""float64"")\n    chi2, pval = compute_chisq(flux)\n    assert_quantity_allclose(chi2, 1.0000000000000001e-11)\n    assert_quantity_allclose(pval, 0.999997476867478)\n'"
gammapy/utils/coordinates/__init__.py,0,"b'""""""Astronomical coordinate calculation utility functions.\n""""""\nfrom .fov import *\nfrom .other import *\n'"
gammapy/utils/coordinates/fov.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom astropy.coordinates import SkyCoord, SkyOffsetFrame\n\n__all__ = [""fov_to_sky"", ""sky_to_fov""]\n\n\ndef fov_to_sky(lon, lat, lon_pnt, lat_pnt):\n    """"""Transform field-of-view coordinates to sky coordinates.\n\n    Parameters\n    ----------\n    lon, lat : `~astropy.units.Quantity`\n        Field-of-view coordinate to be transformed\n    lon_pnt, lat_pnt : `~astropy.units.Quantity`\n        Coordinate specifying the pointing position\n        (i.e. the center of the field of view)\n\n    Returns\n    -------\n    lon_t, lat_t : `~astropy.units.Quantity`\n        Transformed sky coordinate\n    """"""\n    # Create a frame that is centered on the pointing position\n    center = SkyCoord(lon_pnt, lat_pnt)\n    fov_frame = SkyOffsetFrame(origin=center)\n\n    # Define coordinate to be transformed.\n    # Need to switch the sign of the longitude angle here\n    # because this axis is reversed in our definition of the FoV-system\n    target_fov = SkyCoord(-lon, lat, frame=fov_frame)\n\n    # Transform into celestial system (need not be ICRS)\n    target_sky = target_fov.icrs\n\n    return target_sky.ra, target_sky.dec\n\n\ndef sky_to_fov(lon, lat, lon_pnt, lat_pnt):\n    """"""Transform sky coordinates to field-of-view coordinates.\n\n    Parameters\n    ----------\n    lon, lat : `~astropy.units.Quantity`\n        Sky coordinate to be transformed\n    lon_pnt, lat_pnt : `~astropy.units.Quantity`\n        Coordinate specifying the pointing position\n        (i.e. the center of the field of view)\n\n    Returns\n    -------\n    lon_t, lat_t : `~astropy.units.Quantity`\n        Transformed field-of-view coordinate\n    """"""\n    # Create a frame that is centered on the pointing position\n    center = SkyCoord(lon_pnt, lat_pnt)\n    fov_frame = SkyOffsetFrame(origin=center)\n\n    # Define coordinate to be transformed.\n    target_sky = SkyCoord(lon, lat)\n\n    # Transform into FoV-system\n    target_fov = target_sky.transform_to(fov_frame)\n\n    # Switch sign of longitude angle since this axis is\n    # reversed in our definition of the FoV-system\n    return -target_fov.lon, target_fov.lat\n'"
gammapy/utils/coordinates/other.py,14,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Other coordinate and distance-related functions""""""\nimport numpy as np\nfrom astropy.units import Quantity, Unit\n\n__all__ = [\n    ""cartesian"",\n    ""galactic"",\n    ""velocity_glon_glat"",\n    ""motion_since_birth"",\n    ""polar"",\n    ""D_SUN_TO_GALACTIC_CENTER"",\n]\n\n# TODO: replace this with the default from the Galactocentric frame in astropy.coordinates\nD_SUN_TO_GALACTIC_CENTER = Quantity(8.5, ""kpc"")\n""""""Default assumed distance from the Sun to the Galactic center (`~astropy.units.Quantity`)""""""\n\n\ndef cartesian(r, theta):\n    """"""Convert polar coordinates to cartesian coordinates.""""""\n    x = r * np.cos(theta)\n    y = r * np.sin(theta)\n    return x, y\n\n\ndef polar(x, y):\n    """"""Convert cartesian coordinates to polar coordinates.""""""\n    r = np.sqrt(x ** 2 + y ** 2)\n    theta = np.arctan2(y, x)\n    return r, theta\n\n\ndef galactic(x, y, z, obs_pos=None):\n    """"""Compute galactic coordinates lon, lat and distance.\n\n    For given position in cartesian coordinates (kpc).\n    """"""\n    obs_pos = obs_pos or [D_SUN_TO_GALACTIC_CENTER, 0, 0]\n    y_prime = y + D_SUN_TO_GALACTIC_CENTER\n    d = np.sqrt(x ** 2 + y_prime ** 2 + z ** 2)\n    glon = np.arctan2(x, y_prime).to(""deg"")\n    glat = np.arcsin(z / d).to(""deg"")\n    return d, glon, glat\n\n\ndef velocity_glon_glat(x, y, z, vx, vy, vz):\n    """"""\n    Compute projected angular velocity in galactic coordinates.\n\n    Parameters\n    ----------\n    x, y, z : `~astropy.units.Quantity`\n        Position in x, y, z direction\n    vx, vy, vz : `~astropy.units.Quantity`\n        Velocity in x, y, z direction\n\n    Returns\n    -------\n    v_glon, v_glat : `~astropy.units.Quantity`\n        Projected velocity in Galactic sky coordinates\n    """"""\n    y_prime = y + D_SUN_TO_GALACTIC_CENTER\n    d = np.sqrt(x ** 2 + y_prime ** 2 + z ** 2)\n    r = np.sqrt(x ** 2 + y_prime ** 2)\n\n    v_glon = (-y_prime * vx + x * vy) / r ** 2\n    v_glat = vz / (np.sqrt(1 - (z / d) ** 2) * d) - np.sqrt(\n        vx ** 2 + vy ** 2 + vz ** 2\n    ) * z / (np.sqrt(1 - (z / d) ** 2) * d ** 2)\n    return v_glon * Unit(""rad""), v_glat * Unit(""rad"")\n\n\ndef motion_since_birth(v, age, theta, phi):\n    """"""\n    Compute motion of a object with given velocity, direction and age.\n\n    Parameters\n    ----------\n    v : `~astropy.units.Quantity`\n        Absolute value of the velocity\n    age : `~astropy.units.Quantity`\n        Age of the source.\n    theta, phi : `~astropy.units.Quantity`\n        Angular direction of the velocity.\n\n    Returns\n    -------\n    dx, dy, dz : `~astropy.units.Quantity`\n        Displacement in x, y, z direction\n    vx, vy, vz : `~astropy.units.Quantity`\n        Velocity in x, y, z direction\n    """"""\n    vx = v * np.cos(phi) * np.sin(theta)\n    vy = v * np.sin(phi) * np.sin(theta)\n    vz = v * np.cos(theta)\n\n    # Compute new positions\n    dx = vx * age\n    dy = vy * age\n    dz = vz * age\n    return dx, dy, dz, vx, vy, vz\n'"
gammapy/utils/random/__init__.py,0,"b'""""""Random probability distribution helpers.""""""\nfrom .inverse_cdf import *\nfrom .utils import *\n'"
gammapy/utils/random/inverse_cdf.py,10,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom .utils import get_random_state\n\n__all__ = [""InverseCDFSampler""]\n\n\nclass InverseCDFSampler:\n    """"""Inverse CDF sampler.\n\n    It determines a set of random numbers and calculate the cumulative\n    distribution function.\n\n    Parameters\n    ----------\n    pdf : `~gammapy.maps.Map`\n        Map of the predicted source counts.\n    axis : int\n        Axis along which sampling the indexes.\n    random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n        Defines random number generator initialisation.\n        Passed to `~gammapy.utils.random.get_random_state`.\n    """"""\n\n    def __init__(self, pdf, axis=None, random_state=0):\n        self.random_state = get_random_state(random_state)\n        self.axis = axis\n\n        if axis is not None:\n            self.cdf = np.cumsum(pdf, axis=self.axis)\n            self.cdf /= self.cdf[:, [-1]]\n        else:\n            self.pdf_shape = pdf.shape\n\n            pdf = pdf.ravel() / pdf.sum()\n            self.sortindex = np.argsort(pdf, axis=None)\n\n            self.pdf = pdf[self.sortindex]\n            self.cdf = np.cumsum(self.pdf)\n\n    def sample_axis(self):\n        """"""Sample along a given axis.\n\n        Returns\n        -------\n        index : tuple of `~numpy.ndarray`\n            Coordinates of the drawn sample.\n        """"""\n        choices = self.random_state.uniform(high=1, size=len(self.cdf))\n        shape_cdf = self.cdf.shape\n\n        cdf_all = np.insert(self.cdf, 0, 0, axis=1)\n        edges = np.arange(shape_cdf[1] + 1) - 0.5\n\n        pix_coords = []\n\n        for cdf, choice in zip(cdf_all, choices):\n            pix = np.interp(choice, cdf, edges)\n            pix_coords.append(pix)\n\n        return np.array(pix_coords)\n\n    def sample(self, size):\n        """"""Draw sample from the given PDF.\n\n        Parameters\n        ----------\n        size : int\n            Number of samples to draw.\n\n        Returns\n        -------\n        index : tuple of `~numpy.ndarray`\n            Coordinates of the drawn sample.\n        """"""\n        # pick numbers which are uniformly random over the cumulative distribution function\n        choice = self.random_state.uniform(high=1, size=size)\n\n        # find the indices corresponding to this point on the CDF\n        index = np.searchsorted(self.cdf, choice)\n        index = self.sortindex[index]\n\n        # map back to multi-dimensional indexing\n        index = np.unravel_index(index, self.pdf_shape)\n        index = np.vstack(index)\n\n        index = index + self.random_state.uniform(low=-0.5, high=0.5, size=index.shape)\n        return index\n'"
gammapy/utils/random/utils.py,10,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""Helper functions to work with distributions.""""""\nimport numbers\nimport numpy as np\nimport scipy.integrate\nfrom astropy.coordinates import Angle\nfrom astropy.time import TimeDelta\n\n__all__ = [\n    ""get_random_state"",\n    ""sample_sphere"",\n    ""sample_sphere_distance"",\n    ""sample_powerlaw"",\n    ""sample_times"",\n    ""normalize"",\n    ""draw"",\n    ""pdf"",\n]\n\n\ndef normalize(func, x_min, x_max):\n    """"""Normalize a 1D function over a given range.""""""\n\n    def f(x):\n        return func(x) / scipy.integrate.quad(func, x_min, x_max)[0]\n\n    return f\n\n\ndef pdf(func):\n    """"""One-dimensional PDF of a given radial surface density.""""""\n\n    def f(x):\n        return x * func(x)\n\n    return f\n\n\ndef draw(low, high, size, dist, random_state=""random-seed"", *args, **kwargs):\n    """"""Allows drawing of random numbers from any distribution.""""""\n    from .inverse_cdf import InverseCDFSampler\n\n    n = 1000\n    x = np.linspace(low, high, n)\n\n    pdf = dist(x)\n    sampler = InverseCDFSampler(pdf=pdf, random_state=random_state)\n\n    idx = sampler.sample(size)\n    x_sampled = np.interp(idx, np.arange(n), x)\n    return np.squeeze(x_sampled)\n\n\ndef get_random_state(init):\n    """"""Get a `numpy.random.RandomState` instance.\n\n    The purpose of this utility function is to have a flexible way\n    to initialise a `~numpy.random.RandomState` instance,\n    a.k.a. a random number generator (``rng``).\n\n    See :ref:`dev_random` for usage examples and further information.\n\n    Parameters\n    ----------\n    init : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n        Available options to initialise the RandomState object:\n\n        * ``int`` -- new RandomState instance seeded with this integer\n          (calls `~numpy.random.RandomState` with ``seed=init``)\n        * ``\'random-seed\'`` -- new RandomState instance seeded in a random way\n          (calls `~numpy.random.RandomState` with ``seed=None``)\n        * ``\'global-rng\'``, return the RandomState singleton used by ``numpy.random``.\n        * `~numpy.random.RandomState` -- do nothing, return the input.\n\n    Returns\n    -------\n    random_state : `~numpy.random.RandomState`\n        RandomState instance.\n    """"""\n    if isinstance(init, (numbers.Integral, np.integer)):\n        return np.random.RandomState(init)\n    elif init == ""random-seed"":\n        return np.random.RandomState(None)\n    elif init == ""global-rng"":\n        return np.random.mtrand._rand\n    elif isinstance(init, np.random.RandomState):\n        return init\n    else:\n        raise ValueError(\n            ""{} cannot be used to seed a numpy.random.RandomState""\n            "" instance"".format(init)\n        )\n\n\ndef sample_sphere(size, lon_range=None, lat_range=None, random_state=""random-seed""):\n    """"""Sample random points on the sphere.\n\n    Reference: http://mathworld.wolfram.com/SpherePointPicking.html\n\n    Parameters\n    ----------\n    size : int\n        Number of samples to generate\n    lon_range : `~astropy.coordinates.Angle`, optional\n        Longitude range (min, max)\n    lat_range : `~astropy.coordinates.Angle`, optional\n        Latitude range (min, max)\n    random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n        Defines random number generator initialisation.\n        Passed to `~gammapy.utils.random.get_random_state`.\n\n    Returns\n    -------\n    lon, lat: `~astropy.coordinates.Angle`\n        Longitude and latitude coordinate arrays\n    """"""\n    random_state = get_random_state(random_state)\n\n    if lon_range is None:\n        lon_range = Angle([0.0, 360.0], ""deg"")\n\n    if lat_range is None:\n        lat_range = Angle([-90.0, 90.0], ""deg"")\n\n    # Sample random longitude\n    u = random_state.uniform(size=size)\n    lon = lon_range[0] + (lon_range[1] - lon_range[0]) * u\n\n    # Sample random latitude\n    v = random_state.uniform(size=size)\n    z_range = np.sin(lat_range)\n    z = z_range[0] + (z_range[1] - z_range[0]) * v\n    lat = np.arcsin(z)\n\n    return lon, lat\n\n\ndef sample_sphere_distance(\n    distance_min=0, distance_max=1, size=None, random_state=""random-seed""\n):\n    """"""Sample random distances if the 3-dim space density is constant.\n\n    This function uses inverse transform sampling\n    (`Wikipedia <http://en.wikipedia.org/wiki/Inverse_transform_sampling>`__)\n    to generate random distances for an observer located in a 3-dim\n    space with constant source density in the range ``(distance_min, distance_max)``.\n\n    Parameters\n    ----------\n    distance_min, distance_max : float, optional\n        Distance range in which to sample\n    size : int or tuple of ints, optional\n        Output shape. Default: one sample. Passed to `numpy.random.uniform`.\n    random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n        Defines random number generator initialisation.\n        Passed to `~gammapy.utils.random.get_random_state`.\n\n    Returns\n    -------\n    distance : array\n        Array of samples\n    """"""\n    random_state = get_random_state(random_state)\n\n    # Since the differential distribution is dP / dr ~ r ^ 2,\n    # we have a cumulative distribution\n    #     P(r) = a * r ^ 3 + b\n    # with P(r_min) = 0 and P(r_max) = 1 implying\n    #     a = 1 / (r_max ^ 3 - r_min ^ 3)\n    #     b = -a * r_min ** 3\n\n    a = 1.0 / (distance_max ** 3 - distance_min ** 3)\n    b = -a * distance_min ** 3\n\n    # Now for inverse transform sampling we need to use the inverse of\n    #     u = a * r ^ 3 + b\n    # which is\n    #     r = [(u - b)/ a] ^ (1 / 3)\n    u = random_state.uniform(size=size)\n    distance = ((u - b) / a) ** (1.0 / 3)\n\n    return distance\n\n\ndef sample_powerlaw(x_min, x_max, gamma, size=None, random_state=""random-seed""):\n    """"""Sample random values from a power law distribution.\n\n    f(x) = x ** (-gamma) in the range x_min to x_max\n\n    It is assumed that *gamma* is the **differential** spectral index.\n\n    Reference: http://mathworld.wolfram.com/RandomNumber.html\n\n    Parameters\n    ----------\n    x_min : float\n        x range minimum\n    x_max : float\n        x range maximum\n    gamma : float\n        Power law index\n    size : int, optional\n        Number of samples to generate\n    random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n        Defines random number generator initialisation.\n        Passed to `~gammapy.utils.random.get_random_state`.\n\n    Returns\n    -------\n    x : array\n        Array of samples from the distribution\n    """"""\n    random_state = get_random_state(random_state)\n\n    size = int(size)\n\n    exp = -gamma\n    base = random_state.uniform(x_min ** exp, x_max ** exp, size)\n    x = base ** (1 / exp)\n\n    return x\n\n\ndef sample_times(\n    size,\n    rate,\n    dead_time=TimeDelta(0, format=""sec""),\n    return_diff=False,\n    random_state=""random-seed"",\n):\n    """"""Make random times assuming a Poisson process.\n\n    This function can be used to test event time series,\n    to have a comparison what completely random data looks like.\n\n    Can be used in two ways (in either case the return type is `~astropy.time.TimeDelta`):\n\n    * ``return_delta=False`` - Return absolute times, relative to zero (default)\n    * ``return_delta=True`` - Return time differences between consecutive events.\n\n    Parameters\n    ----------\n    size : int\n        Number of samples\n    rate : `~astropy.units.Quantity`\n        Event rate (dimension: 1 / TIME)\n    dead_time : `~astropy.units.Quantity` or `~astropy.time.TimeDelta`, optional\n        Dead time after event (dimension: TIME)\n    return_diff : bool\n        Return time difference between events? (default: no, return absolute times)\n    random_state : {int, \'random-seed\', \'global-rng\', `~numpy.random.RandomState`}\n        Defines random number generator initialisation.\n        Passed to `~gammapy.utils.random.get_random_state`.\n\n    Returns\n    -------\n    time : `~astropy.time.TimeDelta`\n        Time differences (second) after time zero.\n\n    Examples\n    --------\n    Example how to simulate 100 events at a rate of 10 Hz.\n    As expected the last event occurs after about 10 seconds.\n\n    >>> from astropy.units import Quantity\n    >>> from gammapy.time import random_times\n    >>> rate = Quantity(10, \'Hz\')\n    >>> times = random_times(size=100, rate=rate, random_state=0)\n    >>> times[-1]\n    <TimeDelta object: scale=\'None\' format=\'sec\' value=9.186484131475076>\n    """"""\n    random_state = get_random_state(random_state)\n\n    dead_time = TimeDelta(dead_time)\n    scale = (1 / rate).to(""s"").value\n    time_delta = random_state.exponential(scale=scale, size=size)\n    time_delta += dead_time.to(""s"").value\n\n    if return_diff:\n        return TimeDelta(time_delta, format=""sec"")\n    else:\n        time = time_delta.cumsum()\n        return TimeDelta(time, format=""sec"")\n'"
gammapy/utils/tests/__init__.py,0,b''
gammapy/utils/tests/test_array.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom gammapy.utils.array import array_stats_str, shape_2N\n\n\ndef test_array_stats_str():\n    actual = array_stats_str(np.pi, ""pi"")\n    assert actual == ""pi             : size =     1, min =  3.142, max =  3.142\\n""\n\n    actual = array_stats_str([np.pi, 42])\n    assert actual == ""size =     2, min =  3.142, max = 42.000\\n""\n\n\ndef test_shape_2N():\n    shape = (34, 89, 120, 444)\n    expected_shape = (40, 96, 128, 448)\n    assert expected_shape == shape_2N(shape=shape, N=3)\n'"
gammapy/utils/tests/test_fits.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom astropy.io import fits\nfrom astropy.table import Column, Table\nfrom gammapy.utils.fits import ebounds_to_energy_axis\nfrom gammapy.utils.scripts import make_path\nfrom gammapy.utils.testing import requires_data\n\n# TODO: merge this fixture with the one in `test_table.py`.\n# Need to move to conftest or can import?\n@pytest.fixture()\ndef table():\n    t = Table(meta={""version"": 42})\n    t[""a""] = np.array([1, 2], dtype=np.int32)\n    t[""b""] = Column(np.array([1, 2], dtype=np.int64), unit=""m"", description=""Velocity"")\n    t[""b""].meta[""ucd""] = ""spam""\n    t[""c""] = Column([""x"", ""yy""], ""c"")\n    return t\n\n\ndef test_table_fits_io_astropy(table):\n    """"""Test `astropy.table.Table` FITS I/O in Astropy.\n\n    Having these tests in Gammapy is to check / ensure that the features\n    we rely on work properly for all Astropy versions we support in CI\n    (currently Astropy 1.3 and up)\n\n    This is useful, because Table FITS I/O was pretty shaky for a while\n    and incrementally improved over time.\n\n    These are the same examples that we have in the docstring\n    at the top of `gammapy/utils/fits.py`.\n    """"""\n    # Check Table -> BinTableHDU\n    hdu = fits.BinTableHDU(table)\n    assert hdu.header[""TTYPE2""] == ""b""\n    assert hdu.header[""TFORM2""] == ""K""\n    assert hdu.header[""TUNIT2""] == ""m""\n\n    # Check BinTableHDU -> Table\n    table2 = Table.read(hdu)\n    assert isinstance(table2.meta, dict)\n    assert table2.meta == {""VERSION"": 42}\n    assert table2[""b""].unit == ""m""\n    # Note: description doesn\'t come back in older versions of Astropy\n    # that we still support, so we\'re not asserting on that here for now.\n\n@requires_data()\ndef test_ebounds_to_energy_axis():\n    filename = ""$GAMMAPY_DATA/joint-crab/spectra/hess/pha_obs23523.fits""\n    hdul = fits.open(make_path(filename))\n    axis = ebounds_to_energy_axis(hdul[\'EBOUNDS\'])\n\n    assert axis.unit == \'keV\'\n    assert len(axis) == 81\n'"
gammapy/utils/tests/test_gauss.py,1,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nimport scipy.integrate\nfrom numpy.testing import assert_almost_equal, assert_equal\nfrom gammapy.utils.gauss import Gauss2DPDF, MultiGauss2D\n\n\nclass TestGauss2DPDF:\n    """"""Note that we test __call__ and dpdtheta2 by\n    checking that their integrals as advertised are 1.""""""\n\n    def setup(self):\n        self.gs = [Gauss2DPDF(0.1), Gauss2DPDF(1), Gauss2DPDF(1)]\n\n    def test_call(self):\n        # Check that value at origin matches the one given here:\n        # http://en.wikipedia.org/wiki/Multivariate_normal_distribution#Bivariate_case\n        for g in self.gs:\n            actual = g(0, 0)\n            desired = 1 / (2 * np.pi * g.sigma ** 2)\n            assert_almost_equal(actual, desired)\n            # Check that distribution integrates to 1\n            xy_max = 5 * g.sigma  # integration range\n            integral = scipy.integrate.dblquad(\n                g, -xy_max, xy_max, lambda _: -xy_max, lambda _: xy_max\n            )[0]\n            assert_almost_equal(integral, 1, decimal=5)\n\n    def test_dpdtheta2(self):\n        for g in self.gs:\n            theta2_max = (7 * g.sigma) ** 2\n            integral = scipy.integrate.quad(g.dpdtheta2, 0, theta2_max)[0]\n            assert_almost_equal(integral, 1, decimal=5)\n\n    def test_containment(self):\n        for g in self.gs:\n            assert_almost_equal(g.containment_fraction(g.sigma), 0.39346934028736658)\n            assert_almost_equal(g.containment_fraction(2 * g.sigma), 0.8646647167633873)\n\n    def test_theta(self):\n        for g in self.gs:\n            assert_almost_equal(\n                g.containment_radius(0.68) / g.sigma, 1.5095921854516636\n            )\n            assert_almost_equal(\n                g.containment_radius(0.95) / g.sigma, 2.4477468306808161\n            )\n\n    def test_gauss_convolve(self):\n        g = Gauss2DPDF(sigma=3).gauss_convolve(sigma=4)\n        assert_equal(g.sigma, 5)\n\n\nclass TestMultiGauss2D:\n    """"""Note that we test __call__ and dpdtheta2 by\n    checking that their integrals.""""""\n\n    @staticmethod\n    def test_call():\n        m = MultiGauss2D(sigmas=[1, 2], norms=[3, 4])\n        xy_max = 5 * m.max_sigma  # integration range\n        integral = scipy.integrate.dblquad(\n            m, -xy_max, xy_max, lambda _: -xy_max, lambda _: xy_max\n        )[0]\n        assert_almost_equal(integral, 7, decimal=5)\n\n    @staticmethod\n    def test_dpdtheta2():\n        m = MultiGauss2D(sigmas=[1, 2], norms=[3, 4])\n        theta2_max = (7 * m.max_sigma) ** 2\n        integral = scipy.integrate.quad(m.dpdtheta2, 0, theta2_max)[0]\n        assert_almost_equal(integral, 7, decimal=5)\n\n    @staticmethod\n    def test_integral_normalize():\n        m = MultiGauss2D(sigmas=[1, 2], norms=[3, 4])\n        assert_equal(m.integral, 7)\n        m.normalize()\n        assert_equal(m.integral, 1)\n\n    @staticmethod\n    def test_containment():\n        g, g2 = Gauss2DPDF(sigma=1), Gauss2DPDF(sigma=2)\n        m = MultiGauss2D(sigmas=[1])\n        m2 = MultiGauss2D(sigmas=[1, 2], norms=[3, 4])\n        for theta in [0, 0.1, 1, 5]:\n            assert_almost_equal(\n                m.containment_fraction(theta), g.containment_fraction(theta)\n            )\n            actual = m2.containment_fraction(theta)\n            desired = 3 * g.containment_fraction(theta) + 4 * g2.containment_fraction(\n                theta\n            )\n            assert_almost_equal(actual, desired)\n\n    @staticmethod\n    def test_theta():\n        # Closure test\n        m = MultiGauss2D(sigmas=[1, 2], norms=[3, 4])\n        for theta in [0, 0.1, 1, 5]:\n            c = m.containment_fraction(theta)\n            t = m.containment_radius(c)\n            assert_almost_equal(t, theta, decimal=5)\n\n    @staticmethod\n    def test_gauss_convolve():\n        # Convolution must add sigmas in square\n        m = MultiGauss2D(sigmas=[3], norms=[5])\n        m2 = m.gauss_convolve(4, 6)\n        assert_equal(m2.sigmas, [5])\n        assert_almost_equal(m2.integral, 5 * 6)\n        # Check that convolve did not change the original\n        assert_equal(m.sigmas, [3])\n        assert_equal(m.norms, [5])\n'"
gammapy/utils/tests/test_integrate.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom astropy.units import Quantity\nfrom gammapy.modeling.models import PowerLawSpectralModel\nfrom gammapy.utils.integrate import trapz_loglog\nfrom gammapy.utils.testing import assert_quantity_allclose\n\n\ndef test_trapz_loglog():\n    energy = Quantity([1, 10], ""TeV"")\n    pwl = PowerLawSpectralModel(index=2.3)\n\n    ref = pwl.integral(emin=energy[0], emax=energy[1])\n\n    val = trapz_loglog(pwl(energy), energy)\n    assert_quantity_allclose(val, ref)\n'"
gammapy/utils/tests/test_interpolation.py,4,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom gammapy.utils.interpolation import LogScale\nfrom gammapy.utils.testing import assert_allclose\n\n\ndef test_LogScale_inverse():\n    tiny = np.finfo(np.float32).tiny\n    log_scale = LogScale()\n    values = np.array([1, 1e-5, 1e-40])\n    log_values = log_scale(values)\n    assert_allclose(log_values, np.array([0, np.log(1e-5), np.log(tiny)]))\n    inv_values = log_scale.inverse(log_values)\n    assert_allclose(inv_values, np.array([1, 1e-5, 0]))\n'"
gammapy/utils/tests/test_nddata.py,7,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom gammapy.maps import MapAxis\nfrom gammapy.utils.nddata import NDDataArray\n\n\n@pytest.fixture(scope=""session"")\ndef axis_x():\n    return MapAxis.from_nodes([1, 3, 6], name=""x"")\n\n\n@pytest.fixture(scope=""session"")\ndef axis_energy():\n    return MapAxis.from_bounds(\n        0.1, 1000, 2, unit=u.TeV, name=""energy"", interp=""log"", node_type=""edges""\n    )\n\n\n@pytest.fixture(scope=""session"")\ndef axis_offset():\n    return MapAxis.from_nodes([0.2, 0.3, 0.4, 0.5] * u.deg, name=""offset"")\n\n\n@pytest.fixture(scope=""session"")\ndef nddata_1d(axis_x):\n    return NDDataArray(\n        axes=[axis_x],\n        data=[1, -1, 2],\n        interp_kwargs=dict(bounds_error=False, fill_value=None),\n    )\n\n\n@pytest.fixture(scope=""session"")\ndef nddata_2d(axis_energy, axis_offset):\n    return NDDataArray(\n        axes=[axis_energy, axis_offset],\n        data=np.arange(8).reshape(2, 4) * u.cm * u.cm,\n        interp_kwargs=dict(bounds_error=False, fill_value=None),\n    )\n\n\nclass TestNDDataArray:\n    def test_init_error(self):\n        with pytest.raises(ValueError):\n            NDDataArray(\n                axes=[MapAxis.from_nodes([1, 3, 6], name=""x"")],\n                data=np.arange(8).reshape(4, 2),\n            )\n\n    def test_str(self, nddata_1d):\n        assert ""x"" in str(nddata_1d)\n\n    def test_evaluate_shape_1d(self, nddata_1d):\n        # Scalar input\n        out = nddata_1d.evaluate(x=1.5)\n        assert out.shape == (1,)\n\n        # Array input\n        out = nddata_1d.evaluate(x=[0, 1.5])\n        assert out.shape == (2,)\n\n        # No input\n        out = nddata_1d.evaluate()\n        assert out.shape == (3,)\n\n    def test_evaluate_2d(self, nddata_2d):\n        # Case 1: axis1 = scalar, axis2 = array\n        out = nddata_2d.evaluate(energy=1 * u.TeV, offset=[0, 0] * u.deg)\n        assert out.shape == (2,)\n\n        # Case 2: axis1 = array, axis2 = array\n        offset, energy = [0, 0] * u.deg, [1, 1, 1] * u.TeV\n        out = nddata_2d.evaluate(energy=energy[:, np.newaxis], offset=offset)\n        assert out.shape == (3, 2)\n\n        # Case 3: axis1 array, axis2 = 2Darray\n        offset, energy = [0, 0] * u.deg, np.zeros((12, 3)) * u.TeV\n        out = nddata_2d.evaluate(energy=energy[:, :, np.newaxis], offset=offset)\n        assert out.shape == (12, 3, 2)\n\n    @pytest.mark.parametrize(""shape"", [(2,), (3, 2), (4, 2, 3)])\n    def test_evaluate_2d_shape(self, nddata_2d, shape):\n        points = dict(\n            energy=np.ones(shape) * 1 * u.TeV, offset=np.ones(shape) * 0.3 * u.deg\n        )\n        out = nddata_2d.evaluate(**points)\n        assert out.shape == shape\n        assert_allclose(out.value, 1)\n\n        points = dict(\n            energy=np.ones(shape) * 100 * u.TeV, offset=np.ones(shape) * 0.3 * u.deg\n        )\n        out = nddata_2d.evaluate(**points)\n        assert_allclose(out.value, 5)\n\n    def test_evaluate_1d_linear(self, nddata_1d):\n        # This should test all cases of interest:\n        # - evaluate outside node array, i.e. extrapolate: x=0\n        # - evaluate on a given node: x=1\n        # - evaluate in between nodes: x=2\n        # - check that values < 0 are clipped to 0: x=3\n        out = nddata_1d.evaluate(x=[0, 1, 2, 3], method=""linear"")\n        assert_allclose(out, [2, 1, 0, 0])\n\n    def test_evaluate_on_nodes(self, nddata_2d):\n        # evaluating on interpolation nodes should give back the interpolation values\n        out = nddata_2d.evaluate()\n        assert_allclose(out, nddata_2d.data)\n'"
gammapy/utils/tests/test_regions.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\n""""""\nHere we test the functions in `gammapy.utils.regions`.\n\nWe can also add tests for specific functionality and behaviour\nin https://astropy-regions.readthedocs.io that we rely on in Gammapy.\nThat package is still work in progress and not fully developed and\nstable, so need to establish a bit what works and what doesn\'t.\n""""""\nimport pytest\nfrom numpy.testing import assert_allclose, assert_equal\nimport astropy.units as u\nimport regions\nfrom astropy.coordinates import SkyCoord\nfrom gammapy.maps import WcsGeom\nfrom gammapy.utils.regions import (\n    SphericalCircleSkyRegion,\n    make_pixel_region,\n    make_region,\n)\n\n\ndef test_make_region():\n    reg = make_region(""image;circle(10,20,3)"")\n    assert isinstance(reg, regions.CirclePixelRegion)\n    assert reg.center.x == 9\n    assert reg.center.y == 19\n    assert reg.radius == 3\n\n    reg = make_region(""galactic;circle(10,20,3)"")\n    assert reg.center.l.deg == 10\n    assert reg.center.b.deg == 20\n    assert reg.radius.to_value(""deg"") == 3\n\n    # Existing regions should pass through\n    reg2 = make_region(reg)\n    assert reg is reg2\n\n    with pytest.raises(TypeError):\n        make_pixel_region([reg])\n\n\ndef test_make_pixel_region():\n    wcs = WcsGeom.create().wcs\n\n    reg = make_pixel_region(""image;circle(10,20,3)"")\n    assert isinstance(reg, regions.CirclePixelRegion)\n    assert reg.center.x == 9\n    assert reg.center.y == 19\n    assert reg.radius == 3\n\n    reg = make_pixel_region(""galactic;circle(10,20,3)"", wcs)\n    assert isinstance(reg, regions.CirclePixelRegion)\n    assert_allclose(reg.center.x, 570.9301128316974)\n    assert_allclose(reg.center.y, 159.935542455567)\n    assert_allclose(reg.radius, 6.061376992149382)\n\n    with pytest.raises(ValueError):\n        make_pixel_region(""galactic;circle(10,20,3)"")\n\n    with pytest.raises(TypeError):\n        make_pixel_region(99)\n\n\nclass TestSphericalCircleSkyRegion:\n    def setup(self):\n        self.region = SphericalCircleSkyRegion(\n            center=SkyCoord(10 * u.deg, 20 * u.deg), radius=10 * u.deg\n        )\n\n    def test_contains(self):\n        coord = SkyCoord([20.1, 22] * u.deg, 20 * u.deg)\n        mask = self.region.contains(coord)\n        assert_equal(mask, [True, False])\n'"
gammapy/utils/tests/test_scripts.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom gammapy.utils.scripts import recursive_merge_dicts\n\n\ndef test_recursive_merge_dicts():\n    old = dict(a=42, b=dict(c=43, e=44))\n    update = dict(d=99, b=dict(g=98, c=50))\n\n    new = recursive_merge_dicts(old, update)\n    assert new[""b""][""c""] == 50\n    assert new[""b""][""e""] == 44\n    assert new[""b""][""g""] == 98\n    assert new[""a""] == 42\n    assert new[""d""] == 99\n'"
gammapy/utils/tests/test_table.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.table import Column, Table\nfrom gammapy.utils.table import (\n    table_from_row_data,\n    table_row_to_dict,\n    table_standardise_units_copy,\n)\n\n\ndef test_table_standardise_units():\n    table = Table(\n        [\n            Column([1], ""a"", unit=""ph cm-2 s-1""),\n            Column([1], ""b"", unit=""ct cm-2 s-1""),\n            Column([1], ""c"", unit=""cm-2 s-1""),\n            Column([1], ""d""),\n        ]\n    )\n\n    table = table_standardise_units_copy(table)\n\n    assert table[""a""].unit == ""cm-2 s-1""\n    assert table[""b""].unit == ""cm-2 s-1""\n    assert table[""c""].unit == ""cm-2 s-1""\n    assert table[""d""].unit is None\n\n\n@pytest.fixture()\ndef table():\n    return Table(\n        [Column([1, 2], ""a""), Column([1, 2] * u.m, ""b""), Column([""x"", ""yy""], ""c"")]\n    )\n\n\ndef test_table_row_to_dict(table):\n    actual = table_row_to_dict(table[1])\n    expected = {""a"": 2, ""b"": 2 * u.m, ""c"": ""yy""}\n    assert actual == expected\n\n\ndef test_table_from_row_data():\n    rows = [dict(a=1, b=1 * u.m, c=""x""), dict(a=2, b=2 * u.km, c=""yy"")]\n    table = table_from_row_data(rows)\n    assert isinstance(table, Table)\n    assert table[""b""].unit == ""m""\n    assert_allclose(table[""b""].data, [1, 2000])\n'"
gammapy/utils/tests/test_time.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom numpy.testing import assert_allclose\nfrom astropy.time import Time, TimeDelta\nfrom gammapy.utils.time import (\n    absolute_time,\n    time_ref_from_dict,\n    time_ref_to_dict,\n    time_relative_to_ref,\n)\n\n\ndef test_time_ref_from_dict():\n    d = dict(MJDREFI=51910, MJDREFF=0.00074287036841269583)\n    mjd = d[""MJDREFF""] + d[""MJDREFI""]\n\n    time = time_ref_from_dict(d)\n    assert time.format == ""mjd""\n    assert time.scale == ""tt""\n    assert_allclose(time.mjd, mjd)\n\n\ndef test_time_ref_to_dict():\n    time = Time(""2001-01-01T00:00:00"")\n\n    d = time_ref_to_dict(time)\n\n    assert set(d) == {""MJDREFI"", ""MJDREFF"", ""TIMESYS""}\n    assert d[""MJDREFI""] == 51910\n    assert_allclose(d[""MJDREFF""], 0.00074287036841269583)\n    assert d[""TIMESYS""] == ""tt""\n\n\ndef test_time_relative_to_ref():\n    time_ref_dict = dict(MJDREFI=500, MJDREFF=0.5)\n    time_ref = time_ref_from_dict(time_ref_dict)\n    delta_time_1sec = TimeDelta(1.0, format=""sec"")\n    time = time_ref + delta_time_1sec\n\n    delta_time = time_relative_to_ref(time, time_ref_dict)\n\n    assert_allclose(delta_time.sec, delta_time_1sec.sec)\n\n\ndef test_absolute_time():\n    time_ref_dict = dict(MJDREFI=51000, MJDREFF=0.5)\n    time_ref = time_ref_from_dict(time_ref_dict)\n    delta_time_1sec = TimeDelta(1.0, format=""sec"")\n    time = time_ref + delta_time_1sec\n\n    abs_time = absolute_time(delta_time_1sec, time_ref_dict)\n\n    assert abs_time.value == time.utc.isot\n'"
gammapy/utils/tests/test_units.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom gammapy.utils.units import standardise_unit\n\n\ndef test_standardise_unit():\n    assert standardise_unit(""ph cm-2 s-1"") == ""cm-2 s-1""\n    assert standardise_unit(""ct cm-2 s-1"") == ""cm-2 s-1""\n    assert standardise_unit(""cm-2 s-1"") == ""cm-2 s-1""\n'"
gammapy/visualization/tests/__init__.py,0,b''
gammapy/visualization/tests/test_cmap.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom numpy.testing import assert_allclose\nfrom gammapy.utils.testing import requires_dependency\nfrom gammapy.visualization import colormap_hess, colormap_milagro\n\n\ndef _check_cmap_rgb_vals(vals, cmap, vmin=0, vmax=1):\n    """"""Helper function to check RGB values of color images""""""\n    from matplotlib.colors import Normalize\n    from matplotlib.cm import ScalarMappable\n\n    norm = Normalize(vmin, vmax)\n    sm = ScalarMappable(norm=norm, cmap=cmap)\n    for val, rgb_expected in vals:\n        rgb_actual = sm.to_rgba(val)[:-1]\n        assert_allclose(rgb_actual, rgb_expected, atol=1e-5)\n\n\n@requires_dependency(""matplotlib"")\ndef test_colormap_hess():\n    transition = 0.5\n    cmap = colormap_hess(transition=transition)\n    vals = [\n        (0, (0.0, 0.0, 0.0)),\n        (0.25, (0.0, 0.0, 0.50196078)),\n        (0.5, (1.0, 0.0058823529411764722, 0.0)),\n        (0.75, (1.0, 0.75882352941176501, 0.0)),\n        (1, (1.0, 1.0, 1.0)),\n    ]\n    _check_cmap_rgb_vals(vals, cmap)\n\n\n@requires_dependency(""matplotlib"")\ndef test_colormap_milagro():\n    transition = 0.5\n    cmap = colormap_milagro(transition=transition)\n    vals = [\n        (0, (1.0, 1.0, 1.0)),\n        (0.25, (0.4979388, 0.4979388, 0.4979388)),\n        (0.5, (0.00379829, 0.3195442, 0.79772102)),\n        (0.75, (0.51610773, 0.25806707, 0.49033536)),\n        (1.0, (1.0, 1.0, 1.0)),\n    ]\n    _check_cmap_rgb_vals(vals, cmap)\n'"
gammapy/visualization/tests/test_panel.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom astropy.coordinates import Angle\nfrom gammapy.maps import Map\nfrom gammapy.utils.testing import mpl_plot_check, requires_dependency\nfrom gammapy.visualization import MapPanelPlotter\n\n\n@requires_dependency(""matplotlib"")\ndef test_map_panel_plotter():\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure()\n    plotter = MapPanelPlotter(\n        figure=fig, xlim=Angle([-5, 5], ""deg""), ylim=Angle([-2, 2], ""deg""), npanels=2\n    )\n    map_image = Map.create(width=(180, 10), binsz=1)\n\n    with mpl_plot_check():\n        plotter.plot(map_image)\n'"
gammapy/visualization/tests/test_utils.py,3,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom gammapy.utils.testing import mpl_plot_check, requires_dependency\nfrom gammapy.visualization import plot_contour_line\n\n\n@requires_dependency(""matplotlib"")\ndef test_map_panel_plotter():\n    import matplotlib.pyplot as plt\n\n    t = np.linspace(0., 6.1, 10)\n    x = np.cos(t)\n    y = np.sin(t)\n\n    ax = plt.subplot(111)\n    with mpl_plot_check():\n        plot_contour_line(ax, x, y)\n'"
gammapy/astro/darkmatter/tests/__init__.py,0,b''
gammapy/astro/darkmatter/tests/test_profiles.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom gammapy.astro.darkmatter import profiles\nfrom gammapy.utils.testing import assert_quantity_allclose\n\ndm_profiles = [\n    profiles.NFWProfile,\n    profiles.EinastoProfile,\n    profiles.IsothermalProfile,\n    profiles.BurkertProfile,\n    profiles.MooreProfile,\n]\n\n\n@pytest.mark.parametrize(""profile"", dm_profiles)\ndef test_profiles(profile):\n    p = profile()\n    p.scale_to_local_density()\n    actual = p(p.DISTANCE_GC)\n    desired = p.LOCAL_DENSITY\n\n    assert_quantity_allclose(actual, desired)\n'"
gammapy/astro/darkmatter/tests/test_spectra.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport astropy.units as u\nfrom gammapy.astro.darkmatter import DarkMatterAnnihilationSpectralModel, PrimaryFlux\nfrom gammapy.utils.testing import assert_quantity_allclose, requires_data\n\n\n@requires_data()\ndef test_primary_flux():\n    with pytest.raises(ValueError):\n        PrimaryFlux(channel=""Spam"", mDM=1 * u.TeV)\n\n    primflux = PrimaryFlux(channel=""W"", mDM=1 * u.TeV)\n    actual = primflux.table_model(500 * u.GeV)\n    desired = 9.328234e-05 / u.GeV\n    assert_quantity_allclose(actual, desired)\n\n\n@requires_data()\ndef test_DMAnnihilation():\n    channel = ""b""\n    massDM = 5 * u.TeV\n    jfactor = 3.41e19 * u.Unit(""GeV2 cm-5"")\n    emin = 0.01 * u.TeV\n    emax = 10 * u.TeV\n\n    model = DarkMatterAnnihilationSpectralModel(\n        mass=massDM, channel=channel, jfactor=jfactor\n    )\n    integral_flux = model.integral(emin=emin, emax=emax).to(""cm-2 s-1"")\n    differential_flux = model.evaluate(energy=1 * u.TeV, scale=1).to(""cm-2 s-1 TeV-1"")\n\n    assert_quantity_allclose(integral_flux.value, 6.19575457e-14, rtol=1e-5)\n    assert_quantity_allclose(differential_flux.value, 2.97506768e-16, rtol=1e-5)\n'"
gammapy/astro/darkmatter/tests/test_utils.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport astropy.units as u\nfrom gammapy.astro.darkmatter import (\n    DarkMatterAnnihilationSpectralModel,\n    JFactory,\n    profiles,\n)\nfrom gammapy.maps import WcsGeom\nfrom gammapy.utils.testing import assert_quantity_allclose, requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef geom():\n    return WcsGeom.create(binsz=0.5, npix=10)\n\n\n@pytest.fixture(scope=""session"")\ndef jfact(geom):\n    jfactory = JFactory(geom=geom, profile=profiles.NFWProfile(), distance=8 * u.kpc)\n    return jfactory.compute_jfactor()\n\n\n@requires_data()\ndef test_dmfluxmap(jfact):\n    emin = 0.1 * u.TeV\n    emax = 10 * u.TeV\n    massDM = 1 * u.TeV\n    channel = ""W""\n\n    diff_flux = DarkMatterAnnihilationSpectralModel(mass=massDM, channel=channel)\n    int_flux = (jfact * diff_flux.integral(emin=emin, emax=emax)).to(""cm-2 s-1"")\n    actual = int_flux[5, 5]\n    desired = 1.94834138e-12 / u.cm ** 2 / u.s\n    assert_quantity_allclose(actual, desired, rtol=1e-5)\n'"
gammapy/astro/population/tests/__init__.py,0,b''
gammapy/astro/population/tests/test_simulate.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom numpy.testing import assert_allclose, assert_equal\nimport astropy.units as u\nfrom astropy.table import Table\nfrom gammapy.astro.population import (\n    add_observed_parameters,\n    add_pulsar_parameters,\n    add_pwn_parameters,\n    add_snr_parameters,\n    make_base_catalog_galactic,\n    make_catalog_random_positions_cube,\n    make_catalog_random_positions_sphere,\n)\n\n\ndef test_make_catalog_random_positions_cube():\n    table = make_catalog_random_positions_cube(random_state=0)\n    d = table[0]\n\n    assert len(table) == 100\n    assert len(table.colnames) == 3\n\n    assert table[""x""].unit == ""pc""\n    assert_allclose(d[""x""], 0.0976270078546495)\n    assert table[""y""].unit == ""pc""\n    assert_allclose(d[""y""], 0.3556330735924602)\n    assert table[""z""].unit == ""pc""\n    assert_allclose(d[""z""], -0.37640823601179485)\n\n    table = make_catalog_random_positions_cube(dimension=2, random_state=0)\n    assert_equal(table[""z""], 0)\n\n    table = make_catalog_random_positions_cube(dimension=1, random_state=0)\n    assert_equal(table[""y""], 0)\n    assert_equal(table[""z""], 0)\n\n\ndef test_make_catalog_random_positions_sphere():\n    table = make_catalog_random_positions_sphere(random_state=0)\n    d = table[0]\n\n    assert len(table) == 100\n    assert len(table.colnames) == 3\n\n    assert table[""lon""].unit == ""rad""\n    assert_allclose(d[""lon""], 3.4482969442579128)\n    assert table[""lat""].unit == ""rad""\n    assert_allclose(d[""lat""], 0.36359133530192267)\n    assert table[""distance""].unit == ""pc""\n    assert_allclose(d[""distance""], 0.6780943487897606)\n\n\ndef test_make_base_catalog_galactic():\n    table = make_base_catalog_galactic(n_sources=10, random_state=0)\n    d = table[0]\n\n    assert len(table) == 10\n    assert len(table.colnames) == 13\n\n    assert table[""age""].unit == ""yr""\n    assert_allclose(d[""age""], 548813.50392732478)\n    assert table[""n_ISM""].unit == ""cm-3""\n    assert_allclose(d[""n_ISM""], 1.0)\n    assert table[""spiralarm""].unit is None\n    assert d[""spiralarm""] == ""Crux Scutum""\n    assert table[""x_birth""].unit == ""kpc""\n    assert_allclose(d[""x_birth""], -5.856461, atol=1e-5)\n    assert table[""y_birth""].unit == ""kpc""\n    assert_allclose(d[""y_birth""], 3.017292, atol=1e-5)\n    assert table[""z_birth""].unit == ""kpc""\n    assert_allclose(d[""z_birth""], 0.049088, atol=1e-5)\n    assert table[""x""].unit == ""kpc""\n    assert_allclose(d[""x""], -5.941061, atol=1e-5)\n    assert table[""y""].unit == ""kpc""\n    assert_allclose(d[""y""], 3.081642, atol=1e-5)\n    assert table[""z""].unit == ""kpc""\n    assert_allclose(d[""z""], 0.023161, atol=1e-5)\n    assert table[""vx""].unit == ""km/s""\n    assert_allclose(d[""vx""], -150.727104, atol=1e-5)\n    assert table[""vy""].unit == ""km/s""\n    assert_allclose(d[""vy""], 114.648494, atol=1e-5)\n    assert table[""vz""].unit == ""km/s""\n    assert_allclose(d[""vz""], -46.193814, atol=1e-5)\n    assert table[""v_abs""].unit == ""km/s""\n    assert_allclose(d[""v_abs""], 194.927693, atol=1e-5)\n\n\ndef test_add_snr_parameters():\n    table = Table()\n    table[""age""] = [100, 1000] * u.yr\n    table[""n_ISM""] = u.Quantity(1, ""cm-3"")\n\n    table = add_snr_parameters(table)\n\n    assert len(table) == 2\n    assert table.colnames == [""age"", ""n_ISM"", ""E_SN"", ""r_out"", ""r_in"", ""L_SNR""]\n\n    assert table[""E_SN""].unit == ""erg""\n    assert_allclose(table[""E_SN""], 1e51)\n    assert table[""r_out""].unit == ""pc""\n    assert_allclose(table[""r_out""], [1, 3.80730787743])\n    assert table[""r_in""].unit == ""pc""\n    assert_allclose(table[""r_in""], [0.9086, 3.45931993743])\n    assert table[""L_SNR""].unit == ""1 / s""\n    assert_allclose(table[""L_SNR""], [0, 1.0768e33])\n\n\ndef test_add_pulsar_parameters():\n    table = Table()\n    table[""age""] = [100, 1000] * u.yr\n\n    table = add_pulsar_parameters(table, random_state=0)\n\n    assert len(table) == 2\n    assert len(table.colnames) == 10\n\n    assert table[""age""].unit == ""yr""\n    assert_allclose(table[""age""], [100, 1000])\n    assert table[""P0""].unit == ""s""\n    assert_allclose(table[""P0""], [0.214478, 0.246349], atol=1e-5)\n    assert table[""P1""].unit == """"\n    assert_allclose(table[""P1""], [6.310423e-13, 4.198294e-16], atol=1e-5)\n    assert table[""P0_birth""].unit == ""s""\n    assert_allclose(table[""P0_birth""], [0.212418, 0.246336], atol=1e-5)\n    assert table[""P1_birth""].unit == """"\n    assert_allclose(table[""P1_birth""], [6.558773e-13, 4.199198e-16], atol=1e-5)\n    assert table[""CharAge""].unit == ""yr""\n    assert_allclose(table[""CharAge""], [2.207394e-21, 1.638930e-24], atol=1e-5)\n    assert table[""Tau0""].unit == ""yr""\n    assert_allclose(table[""Tau0""], [5.131385e03, 9.294538e06], atol=1e-5)\n    assert table[""L_PSR""].unit == ""erg / s""\n    assert_allclose(table[""L_PSR""], [2.599229e36, 1.108788e33], rtol=1e-5)\n    assert table[""L0_PSR""].unit == ""erg / s""\n    assert_allclose(table[""L0_PSR""], [2.701524e36, 1.109026e33], rtol=1e-5)\n    assert table[""B_PSR""].unit == ""G""\n    assert_allclose(table[""B_PSR""], [1.194420e13, 3.254597e11], rtol=1e-5)\n\n\ndef test_add_pwn_parameters():\n    table = make_base_catalog_galactic(n_sources=10, random_state=0)\n    # To compute PWN parameters we need PSR and SNR parameters first\n    table = add_snr_parameters(table)\n    table = add_pulsar_parameters(table, random_state=0)\n    table = add_pwn_parameters(table)\n    d = table[0]\n\n    assert len(table) == 10\n    assert len(table.colnames) == 27\n\n    assert table[""r_out_PWN""].unit == ""pc""\n    assert_allclose(d[""r_out_PWN""], 1.378224, atol=1e-4)\n\n\ndef test_add_observed_parameters():\n    table = make_base_catalog_galactic(n_sources=10, random_state=0)\n    table = add_observed_parameters(table)\n    d = table[0]\n\n    assert len(table) == 10\n    assert len(table.colnames) == 20\n\n    assert table[""distance""].unit == ""pc""\n    assert_allclose(d[""distance""], 13016.572756, atol=1e-5)\n    assert table[""GLON""].unit == ""deg""\n    assert_allclose(d[""GLON""], -27.156565, atol=1e-5)\n    assert table[""GLAT""].unit == ""deg""\n    assert_allclose(d[""GLAT""], 0.101948, atol=1e-5)\n    assert table[""VGLON""].unit == ""deg / Myr""\n    assert_allclose(d[""VGLON""], 0.368166, atol=1e-5)\n    assert table[""VGLAT""].unit == ""deg / Myr""\n    assert_allclose(d[""VGLAT""], -0.209514, atol=1e-5)\n    assert table[""RA""].unit == ""deg""\n    assert_allclose(d[""RA""], 244.347149, atol=1e-5)\n    assert table[""DEC""].unit == ""deg""\n    assert_allclose(d[""DEC""], -50.410142, atol=1e-5)\n\n\ndef test_chain_all():\n    # Test that running the simulation functions in chain works\n    table = make_base_catalog_galactic(n_sources=10, random_state=0)\n    table = add_snr_parameters(table)\n    table = add_pulsar_parameters(table, random_state=0)\n    table = add_pwn_parameters(table)\n    table = add_observed_parameters(table)\n    d = table[0]\n\n    # Note: the individual functions are tested above.\n    # Here we just run them in a chain and do very basic asserts\n    # on the output so that we make sure we notice changes.\n    assert len(table) == 10\n    assert len(table.colnames) == 34\n\n    assert table[""r_out_PWN""].unit == ""pc""\n    assert_allclose(d[""r_out_PWN""], 1.378224, atol=1e-4)\n    assert table[""RA""].unit == ""deg""\n    assert_allclose(d[""RA""], 244.347149, atol=1e-5)\n'"
gammapy/astro/population/tests/test_spatial.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nfrom gammapy.astro.population.spatial import (\n    CaseBattacharya1998,\n    Exponential,\n    FaucherKaspi2006,\n    Lorimer2006,\n    Paczynski1990,\n    YusifovKucuk2004,\n    YusifovKucuk2004B,\n)\n\ntest_cases = [\n    {\n        ""class"": FaucherKaspi2006,\n        ""x"": [0.1, 1, 10],\n        ""y"": [0.0002221728797095, 0.00127106525755, 0.0797205770877],\n    },\n    {\n        ""class"": Lorimer2006,\n        ""x"": [0.1, 1, 10],\n        ""y"": [0.03020158, 1.41289246, 0.56351182],\n    },\n    {\n        ""class"": Paczynski1990,\n        ""x"": [0.1, 1, 10],\n        ""y"": [0.04829743, 0.03954259, 0.00535151],\n    },\n    {\n        ""class"": YusifovKucuk2004,\n        ""x"": [0.1, 1, 10],\n        ""y"": [0.55044445, 1.5363482, 0.66157715],\n    },\n    {\n        ""class"": YusifovKucuk2004B,\n        ""x"": [0.1, 1, 10],\n        ""y"": [1.76840095e-08, 8.60773150e-05, 6.42641018e-04],\n    },\n    {\n        ""class"": CaseBattacharya1998,\n        ""x"": [0.1, 1, 10],\n        ""y"": [0.00453091, 0.31178967, 0.74237311],\n    },\n    {\n        ""class"": Exponential,\n        ""x"": [0, 0.25, 0.5],\n        ""y"": [1.00000000e00, 6.73794700e-03, 4.53999298e-05],\n    },\n]\n\n\n@pytest.mark.parametrize(""case"", test_cases, ids=lambda _: _[""class""].__name__)\ndef test_spatial_model(case):\n    model = case[""class""]()\n    y = model(case[""x""])\n    assert_allclose(y, case[""y""], rtol=1e-5)\n'"
gammapy/astro/population/tests/test_velocity.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nfrom gammapy.astro.population.velocity import (\n    FaucherKaspi2006VelocityBimodal,\n    FaucherKaspi2006VelocityMaxwellian,\n    Paczynski1990Velocity,\n)\n\ntest_cases = [\n    {\n        ""class"": FaucherKaspi2006VelocityMaxwellian,\n        ""x"": [1, 10],\n        ""y"": [4.28745276e-08, 4.28443169e-06],\n    },\n    {\n        ""class"": FaucherKaspi2006VelocityBimodal,\n        ""x"": [1, 10],\n        ""y"": [1.754811e-07, 1.751425e-05],\n    },\n    {""class"": Paczynski1990Velocity, ""x"": [1, 10], ""y"": [0.00227363, 0.00227219]},\n]\n\n\n@pytest.mark.parametrize(""case"", test_cases, ids=lambda _: _[""class""].__name__)\ndef test_velocity_model(case):\n    model = case[""class""]()\n    y = model(case[""x""])\n    assert_allclose(y, case[""y""], rtol=1e-5)\n'"
gammapy/astro/source/tests/__init__.py,0,b''
gammapy/astro/source/tests/test_pulsar.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport scipy.integrate\nfrom numpy.testing import assert_allclose\nfrom astropy.table import Table\nfrom astropy.units import Quantity\nfrom gammapy.astro.source import Pulsar, SimplePulsar\nfrom gammapy.utils.testing import assert_quantity_allclose\n\npulsar = Pulsar()\ntime = Quantity([1e2, 1e4, 1e6, 1e8], ""yr"")\n\n\ndef get_atnf_catalog_sample():\n    data = """"""\n    NUM   NAME            Gl          Gb       P0        P1        AGE        BSURF     EDOT\n    1     J0006+1834      108.172    -42.985   0.693748  2.10e-15  5.24e+06   1.22e+12  2.48e+32\n    2     J0007+7303      119.660     10.463   0.315873  3.60e-13  1.39e+04   1.08e+13  4.51e+35\n    3     B0011+47        116.497    -14.631   1.240699  5.64e-16  3.48e+07   8.47e+11  1.17e+31\n    7     B0021-72E       305.883    -44.883   0.003536  9.85e-20  5.69e+08   5.97e+08  8.79e+34\n    8     B0021-72F       305.899    -44.892   0.002624  6.45e-20  6.44e+08   4.16e+08  1.41e+35\n    16    J0024-7204O     305.897    -44.889   0.002643  3.04e-20  1.38e+09   2.87e+08  6.49e+34\n    18    J0024-7204Q     305.877    -44.899   0.004033  3.40e-20  1.88e+09   3.75e+08  2.05e+34\n    21    J0024-7204T     305.890    -44.894   0.007588  2.94e-19  4.09e+08   1.51e+09  2.65e+34\n    22    J0024-7204U     305.890    -44.905   0.004343  9.52e-20  7.23e+08   6.51e+08  4.59e+34\n    28    J0026+6320      120.176      0.593   0.318358  1.50e-16  3.36e+07   2.21e+11  1.84e+32\n    """"""\n    return Table.read(data, format=""ascii"")\n\n\ndef test_SimplePulsar_atnf():\n    """"""Test functions against ATNF pulsar catalog values""""""\n    atnf = get_atnf_catalog_sample()\n    simple_pulsar = SimplePulsar(\n        P=Quantity(atnf[""P0""], ""s""), P_dot=Quantity(atnf[""P1""], """")\n    )\n\n    assert_allclose(simple_pulsar.tau.to(""yr"").value, atnf[""AGE""], rtol=0.01)\n\n    edot = simple_pulsar.luminosity_spindown.to(""erg s^-1"").value\n    assert_allclose(edot, atnf[""EDOT""], rtol=0.01)\n\n    bsurf = simple_pulsar.magnetic_field.to(""gauss"").value\n    assert_allclose(bsurf, atnf[""BSURF""], rtol=0.01)\n\n\ndef test_Pulsar_period():\n    """"""Test pulsar period""""""\n    reference = Quantity([0.1, 0.10000031, 0.10003081, 0.10303572], ""s"")\n    assert_quantity_allclose(pulsar.period(time), reference)\n\n\ndef test_Pulsar_peridod_dot():\n    """"""Test pulsar period derivative""""""\n    reference = [9.76562470e-19, 9.76559490e-19, 9.76261682e-19, 9.47790252e-19]\n    assert_allclose(pulsar.period_dot(time), reference)\n\n\ndef test_Pulsar_luminosity_spindown():\n    """"""Test pulsar spin down luminosity""""""\n    reference = [3.85531374e31, 3.85526669e31, 3.85056609e31, 3.42064935e31]\n    assert_allclose(pulsar.luminosity_spindown(time).value, reference)\n\n\ndef test_Pulsar_energy_integrated():\n    """"""Test against numerical integration""""""\n    energies = []\n\n    def lumi(t):\n        t = Quantity(t, ""s"")\n        return pulsar.luminosity_spindown(t).value\n\n    for t_ in time:\n        energy = scipy.integrate.quad(lumi, 0, t_.cgs.value, epsrel=0.01)[0]\n        energies.append(energy)\n    # The last value is quite inaccurate, because integration is over several decades\n    assert_allclose(energies, pulsar.energy_integrated(time).value, rtol=0.2)\n\n\ndef test_Pulsar_magnetic_field():\n    b = pulsar.magnetic_field(time)\n    assert b.unit == ""G""\n    assert pulsar.B.unit == ""G""\n    assert_allclose(b.value, pulsar.B.value)\n'"
gammapy/astro/source/tests/test_pwn.py,1,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom astropy.units import Quantity\nfrom gammapy.astro.source import PWN\n\nt = Quantity([0, 1, 10, 100, 1000, 10000, 100000], ""yr"")\npwn = PWN()\n\n\ndef test_PWN_radius():\n    """"""Test SNR luminosity""""""\n    r = [0, 1.334e14, 2.114e15, 3.350e16, 5.310e17, 6.927e17, 6.927e17]\n    assert_allclose(pwn.radius(t).value, r, rtol=1e-3)\n\n\ndef test_magnetic_field():\n    """"""Test SNR luminosity""""""\n    b = [np.nan, 1.753e-03, 8.788e-05, 4.404e-06, 2.207e-07, 4.685e-07, 1.481e-06]\n    assert_allclose(pwn.magnetic_field(t).to(""gauss"").value, b, rtol=1e-3)\n'"
gammapy/astro/source/tests/test_snr.py,1,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom astropy.units import Quantity\nfrom gammapy.astro.source import SNR, SNRTrueloveMcKee\n\nt = Quantity([0, 1, 10, 100, 1000, 10000], ""yr"")\nsnr = SNR()\nsnr_mckee = SNRTrueloveMcKee()\n\n\ndef test_SNR_luminosity_tev():\n    """"""Test SNR luminosity""""""\n    reference = [0, 0, 0, 0, 1.076e33, 1.076e33]\n    assert_allclose(snr.luminosity_tev(t).value, reference, rtol=1e-3)\n\n\ndef test_SNR_radius():\n    """"""Test SNR radius""""""\n    reference = [0, 3.085e16, 3.085e17, 3.085e18, 1.174e19, 2.950e19]\n    assert_allclose(snr.radius(t).value, reference, rtol=1e-3)\n\n\ndef test_SNR_radius_inner():\n    """"""Test SNR radius""""""\n    reference = (1 - 0.0914) * np.array(\n        [0, 3.085e16, 3.085e17, 3.085e18, 1.174e19, 2.950e19]\n    )\n    assert_allclose(snr.radius_inner(t).value, reference, rtol=1e-3)\n\n\ndef test_SNRTrueloveMcKee_luminosity_tev():\n    """"""Test SNR Truelov McKee luminosity""""""\n    reference = [0, 0, 0, 0, 1.076e33, 1.076e33]\n    assert_allclose(snr_mckee.luminosity_tev(t).value, reference, rtol=1e-3)\n\n\ndef test_SNRTrueloveMcKee_radius():\n    """"""Test SNR RTruelove McKee radius""""""\n    reference = [0, 1.953e17, 9.066e17, 4.208e18, 1.579e19, 4.117e19]\n    assert_allclose(snr_mckee.radius(t).value, reference, rtol=1e-3)\n'"
gammapy/catalog/tests/data/make.py,0,"b'""""""Make test reference data files.""""""\n\nfrom gammapy.catalog import SOURCE_CATALOGS\n\ncat = SOURCE_CATALOGS[""3fgl""]()\nopen(""3fgl_J0000.1+6545.txt"", ""w"").write(str(cat[""3FGL J0000.1+6545""]))\nopen(""3fgl_J0001.4+2120.txt"", ""w"").write(str(cat[""3FGL J0001.4+2120""]))\nopen(""3fgl_J0023.4+0923.txt"", ""w"").write(str(cat[""3FGL J0023.4+0923""]))\nopen(""3fgl_J0835.3-4510.txt"", ""w"").write(str(cat[""3FGL J0835.3-4510""]))\n\ncat = SOURCE_CATALOGS[""4fgl""]()\nopen(""4fgl_J0000.3-7355.txt"", ""w"").write(str(cat[""4FGL J0000.3-7355""]))\nopen(""4fgl_J0001.5+2113.txt"", ""w"").write(str(cat[""4FGL J0001.5+2113""]))\nopen(""4fgl_J0002.8+6217.txt"", ""w"").write(str(cat[""4FGL J0002.8+6217""]))\nopen(""4fgl_J1409.1-6121e.txt"", ""w"").write(str(cat[""4FGL J1409.1-6121e""]))\n\ncat = SOURCE_CATALOGS[""2fhl""]()\nopen(""2fhl_j1445.1-0329.txt"", ""w"").write(str(cat[""2FHL J1445.1-0329""]))\nopen(""2fhl_j0822.6-4250e.txt"", ""w"").write(str(cat[""2FHL J0822.6-4250e""]))\n\ncat = SOURCE_CATALOGS[""3fhl""]()\nopen(""3fhl_j2301.9+5855e.txt"", ""w"").write(str(cat[""3FHL J2301.9+5855e""]))\n\ncat = SOURCE_CATALOGS[""2hwc""]()\nopen(""2hwc_j0534+220.txt"", ""w"").write(str(cat[""2HWC J0534+220""]))\nopen(""2hwc_j0631+169.txt"", ""w"").write(str(cat[""2HWC J0631+169""]))\n\ncat = SOURCE_CATALOGS[""hgps""]()\nopen(""hess_j1713-397.txt"", ""w"").write(str(cat[""HESS J1713-397""]))\nopen(""hess_j1825-137.txt"", ""w"").write(str(cat[""HESS J1825-137""]))\nopen(""hess_j1930+188.txt"", ""w"").write(str(cat[""HESS J1930+188""]))\n\ncat = SOURCE_CATALOGS[""gamma-cat""]()\nopen(""gammacat_hess_j1813-178.txt"", ""w"").write(str(cat[""HESS J1813-178""]))\nopen(""gammacat_hess_j1848-018.txt"", ""w"").write(str(cat[""HESS J1848-018""]))\nopen(""gammacat_vela_x.txt"", ""w"").write(str(cat[""Vela X""]))\n'"
gammapy/makers/background/tests/__init__.py,0,b''
gammapy/makers/background/tests/test_fov.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nfrom astropy.coordinates import Angle, SkyCoord\nfrom regions import CircleSkyRegion\nfrom gammapy.data import DataStore\nfrom gammapy.datasets import MapDataset\nfrom gammapy.makers import FoVBackgroundMaker, MapDatasetMaker, SafeMaskMaker\nfrom gammapy.maps import MapAxis, WcsGeom, WcsNDMap\nfrom gammapy.modeling.models import (\n    GaussianSpatialModel,\n    PowerLawSpectralModel,\n    SkyModel,\n)\nfrom gammapy.utils.testing import requires_data, requires_dependency\n\n\n@pytest.fixture(scope=""session"")\ndef observation():\n    """"""Example observation list for testing.""""""\n    datastore = DataStore.from_dir(""$GAMMAPY_DATA/hess-dl3-dr1/"")\n    obs_id = 23523\n    return datastore.obs(obs_id)\n\n\n@pytest.fixture(scope=""session"")\ndef geom():\n    energy_axis = MapAxis.from_edges([1, 10], unit=""TeV"", name=""energy"", interp=""log"")\n    return WcsGeom.create(\n        skydir=SkyCoord(83.633, 22.014, unit=""deg""),\n        binsz=0.02,\n        width=(5, 5),\n        frame=""galactic"",\n        proj=""CAR"",\n        axes=[energy_axis],\n    )\n\n\n@pytest.fixture(scope=""session"")\ndef reference(geom):\n    return MapDataset.create(geom)\n\n\n@pytest.fixture(scope=""session"")\ndef exclusion_mask(geom):\n    """"""Example mask for testing.""""""\n    pos = SkyCoord(83.633, 22.014, unit=""deg"", frame=""icrs"")\n    region = CircleSkyRegion(pos, Angle(0.3, ""deg""))\n    exclusion = WcsNDMap.from_geom(geom)\n    exclusion.data = geom.region_mask([region], inside=False)\n    return exclusion\n\n\n@pytest.fixture(scope=""session"")\ndef obs_dataset(geom, observation):\n    safe_mask_maker = SafeMaskMaker(methods=[""offset-max""], offset_max=""2 deg"")\n    map_dataset_maker = MapDatasetMaker(selection=[""counts"", ""background"", ""exposure""])\n\n    reference = MapDataset.create(geom)\n    cutout = reference.cutout(\n        observation.pointing_radec, width=""4 deg"", name=""test-fov""\n    )\n\n    dataset = map_dataset_maker.run(cutout, observation)\n    dataset = safe_mask_maker.run(dataset, observation)\n    return dataset\n\n\ndef test_fov_bkg_maker_incorrect_method():\n    with pytest.raises(ValueError):\n        FoVBackgroundMaker(method=""bad"")\n\n\n@requires_data()\ndef test_fov_bkg_maker_scale(obs_dataset, exclusion_mask):\n    fov_bkg_maker = FoVBackgroundMaker(method=""scale"", exclusion_mask=exclusion_mask)\n\n    test_dataset = obs_dataset.copy(name=""test-fov"")\n    dataset = fov_bkg_maker.run(test_dataset)\n\n    assert_allclose(dataset.background_model.norm.value, 0.830789, rtol=1e-4)\n    assert_allclose(dataset.background_model.tilt.value, 0.0, rtol=1e-4)\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_fov_bkg_maker_fit(obs_dataset, exclusion_mask):\n    fov_bkg_maker = FoVBackgroundMaker(method=""fit"", exclusion_mask=exclusion_mask)\n\n    test_dataset = obs_dataset.copy(name=""test-fov"")\n    dataset = fov_bkg_maker.run(test_dataset)\n\n    assert_allclose(dataset.background_model.norm.value, 0.830789, rtol=1e-4)\n    assert_allclose(dataset.background_model.tilt.value, 0.0, rtol=1e-4)\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_fov_bkg_maker_fit_with_source_model(obs_dataset, exclusion_mask):\n    fov_bkg_maker = FoVBackgroundMaker(method=""fit"", exclusion_mask=exclusion_mask)\n\n    test_dataset = obs_dataset.copy(name=""test-fov"")\n    spatial_model = GaussianSpatialModel(\n        lon_0=""0.2 deg"", lat_0=""0.1 deg"", sigma=""0.2 deg"", frame=""galactic""\n    )\n    spectral_model = PowerLawSpectralModel(\n        index=3, amplitude=""1e-11 cm-2 s-1 TeV-1"", reference=""1 TeV""\n    )\n    model = SkyModel(\n        spatial_model=spatial_model, spectral_model=spectral_model, name=""test-source""\n    )\n    test_dataset.models.append(model)\n    dataset = fov_bkg_maker.run(test_dataset)\n\n    # Here we check that source parameters are correctly thawed after fit.\n    assert not dataset.models.parameters[""index""].frozen\n    assert not dataset.models.parameters[""lon_0""].frozen\n    assert not dataset.background_model.norm.frozen\n\n    assert_allclose(dataset.background_model.norm.value, 0.830789, rtol=1e-4)\n    assert_allclose(dataset.background_model.tilt.value, 0.0, rtol=1e-4)\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_fov_bkg_maker_fit_with_tilt(obs_dataset, exclusion_mask):\n    fov_bkg_maker = FoVBackgroundMaker(method=""fit"", exclusion_mask=exclusion_mask)\n\n    test_dataset = obs_dataset.copy(name=""test-fov"")\n    test_dataset.background_model.tilt.frozen = False\n    dataset = fov_bkg_maker.run(test_dataset)\n\n    assert_allclose(dataset.background_model.norm.value, 0.9034, rtol=1e-4)\n    assert_allclose(dataset.background_model.tilt.value, 0.0728, rtol=1e-4)\n\n\n@requires_data()\n@requires_dependency(""iminuit"")\ndef test_fov_bkg_maker_fit_fail(obs_dataset, exclusion_mask):\n    fov_bkg_maker = FoVBackgroundMaker(method=""fit"", exclusion_mask=exclusion_mask)\n\n    test_dataset = obs_dataset.copy(name=""test-fov"")\n    # Putting negative background model to prevent convergence\n    test_dataset.background_model.map.data *= -1\n    dataset = fov_bkg_maker.run(test_dataset)\n\n    assert_allclose(dataset.background_model.norm.value, 1, rtol=1e-4)\n\n\n@requires_data()\ndef test_fov_bkg_maker_scale_fail(obs_dataset, exclusion_mask):\n    fov_bkg_maker = FoVBackgroundMaker(method=""scale"", exclusion_mask=exclusion_mask)\n\n    test_dataset = obs_dataset.copy()\n    # Putting negative background model to prevent correct scaling\n    test_dataset.background_model.map.data *= -1\n    dataset = fov_bkg_maker.run(test_dataset)\n\n    assert_allclose(dataset.background_model.norm.value, 1, rtol=1e-4)\n'"
gammapy/makers/background/tests/test_phase.py,2,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom astropy import units as u\nfrom astropy.coordinates import Angle, SkyCoord\nfrom gammapy.data import DataStore\nfrom gammapy.maps import MapAxis\nfrom gammapy.datasets import SpectrumDataset\nfrom gammapy.makers import PhaseBackgroundMaker, SpectrumDatasetMaker\nfrom gammapy.utils.regions import SphericalCircleSkyRegion\nfrom gammapy.utils.testing import requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef observations():\n    """"""Example observation list for testing.""""""\n    datastore = DataStore.from_dir(""$GAMMAPY_DATA/cta-1dc/index/gps"")\n    return datastore.get_observations([111630])\n\n\n@pytest.fixture(scope=""session"")\ndef phase_bkg_maker():\n    """"""Example background estimator for testing.""""""\n    return PhaseBackgroundMaker(on_phase=(0.5, 0.6), off_phase=(0.7, 1))\n\n\n@requires_data()\ndef test_basic(phase_bkg_maker):\n    assert ""PhaseBackgroundMaker"" in str(phase_bkg_maker)\n\n\n@requires_data()\ndef test_run(observations, phase_bkg_maker):\n\n    maker = SpectrumDatasetMaker()\n\n    e_reco = MapAxis.from_edges(np.logspace(0, 2, 5) * u.TeV, name=""energy"")\n    e_true = MapAxis.from_edges(np.logspace(-0.5, 2, 11) * u.TeV, name=""energy_true"")\n\n    pos = SkyCoord(""08h35m20.65525s"", ""-45d10m35.1545s"", frame=""icrs"")\n    radius = Angle(0.2, ""deg"")\n    region = SphericalCircleSkyRegion(pos, radius)\n\n    dataset_empty = SpectrumDataset.create(e_reco, e_true, region=region)\n\n    obs = observations[""111630""]\n    dataset = maker.run(dataset_empty, obs)\n    dataset_on_off = phase_bkg_maker.run(dataset, obs)\n\n    assert_allclose(dataset_on_off.acceptance, 0.1)\n    assert_allclose(dataset_on_off.acceptance_off, 0.3)\n\n    assert_allclose(dataset_on_off.counts.data.sum(), 28)\n    assert_allclose(dataset_on_off.counts_off.data.sum(), 57)\n\n\n@pytest.mark.parametrize(\n    ""pars"",\n    [\n        {""p_in"": [[0.2, 0.3]], ""p_out"": [[0.2, 0.3]]},\n        {""p_in"": [[0.9, 0.1]], ""p_out"": [[0.9, 1], [0, 0.1]]},\n    ],\n)\ndef test_check_phase_intervals(pars):\n    assert PhaseBackgroundMaker._check_intervals(pars[""p_in""]) == pars[""p_out""]\n'"
gammapy/makers/background/tests/test_reflected.py,4,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.coordinates import Angle, SkyCoord\nfrom regions import (\n    CircleSkyRegion,\n    EllipseAnnulusSkyRegion,\n    EllipseSkyRegion,\n    RectangleSkyRegion,\n)\nfrom gammapy.data import DataStore\nfrom gammapy.datasets import SpectrumDataset\nfrom gammapy.makers import (\n    ReflectedRegionsBackgroundMaker,\n    ReflectedRegionsFinder,\n    SpectrumDatasetMaker,\n)\nfrom gammapy.maps import WcsGeom, WcsNDMap, MapAxis\nfrom gammapy.utils.regions import compound_region_to_list\nfrom gammapy.utils.testing import (\n    assert_quantity_allclose,\n    mpl_plot_check,\n    requires_data,\n    requires_dependency,\n)\n\n\n@pytest.fixture(scope=""session"")\ndef exclusion_mask():\n    """"""Example mask for testing.""""""\n    pos = SkyCoord(83.63, 22.01, unit=""deg"", frame=""icrs"")\n    exclusion_region = CircleSkyRegion(pos, Angle(0.3, ""deg""))\n    geom = WcsGeom.create(skydir=pos, binsz=0.02, width=10.0)\n    mask = geom.region_mask([exclusion_region], inside=False)\n    return WcsNDMap(geom, data=mask)\n\n\n@pytest.fixture(scope=""session"")\ndef on_region():\n    """"""Example on_region for testing.""""""\n    pos = SkyCoord(83.63, 22.01, unit=""deg"", frame=""icrs"")\n    radius = Angle(0.11, ""deg"")\n    region = CircleSkyRegion(pos, radius)\n    return region\n\n\n@pytest.fixture(scope=""session"")\ndef observations():\n    """"""Example observation list for testing.""""""\n    datastore = DataStore.from_dir(""$GAMMAPY_DATA/hess-dl3-dr1"")\n    obs_ids = [23523, 23526]\n    return datastore.get_observations(obs_ids)\n\n\n@pytest.fixture()\ndef reflected_bkg_maker(exclusion_mask):\n    return ReflectedRegionsBackgroundMaker(exclusion_mask=exclusion_mask)\n\n\nregion_finder_param = [\n    (SkyCoord(83.2, 22.5, unit=""deg""), 15, Angle(""82.592 deg""), 17, 17),\n    (SkyCoord(84.2, 22.5, unit=""deg""), 17, Angle(""83.636 deg""), 19, 19),\n    (SkyCoord(83.2, 21.5, unit=""deg""), 15, Angle(""83.672 deg""), 17, 17),\n]\n\n\n@requires_data()\n@pytest.mark.parametrize(\n    ""pointing_pos, nreg1, reg3_ra, nreg2, nreg3"", region_finder_param\n)\ndef test_find_reflected_regions(\n    exclusion_mask, on_region, pointing_pos, nreg1, reg3_ra, nreg2, nreg3\n):\n    pointing = pointing_pos\n    finder = ReflectedRegionsFinder(\n        center=pointing,\n        region=on_region,\n        exclusion_mask=exclusion_mask,\n        min_distance_input=""0 deg"",\n    )\n    finder.run()\n    regions = finder.reflected_regions\n    assert len(regions) == nreg1\n    assert_quantity_allclose(regions[3].center.icrs.ra, reg3_ra, rtol=1e-2)\n\n    # Test without exclusion\n    finder.exclusion_mask = None\n    finder.run()\n    regions = finder.reflected_regions\n    assert len(regions) == nreg2\n\n    # Test with too small exclusion\n    small_mask = exclusion_mask.cutout(pointing, Angle(""0.1 deg""))\n    finder.exclusion_mask = small_mask\n    finder.run()\n    regions = finder.reflected_regions\n    assert len(regions) == nreg3\n\n    # Test with maximum number of regions\n    finder.max_region_number = 5\n    finder.run()\n    regions = finder.reflected_regions\n    assert len(regions) == 5\n\n    # Test with an other type of region\n    on_ellipse_annulus = EllipseAnnulusSkyRegion(\n        center=on_region.center.galactic,\n        inner_width=0.1 * u.deg,\n        outer_width=0.2 * u.deg,\n        inner_height=0.3 * u.deg,\n        outer_height=0.6 * u.deg,\n        angle=130 * u.deg,\n    )\n    finder.region = on_ellipse_annulus\n    finder.reference_map = None\n    finder.run()\n    regions = finder.reflected_regions\n    assert len(regions) == 5\n\n\ncenter = SkyCoord(0.5, 0.0, unit=""deg"")\nother_region_finder_param = [\n    (RectangleSkyRegion(center, 0.5 * u.deg, 0.5 * u.deg, angle=0 * u.deg), 3),\n    (RectangleSkyRegion(center, 0.5 * u.deg, 1 * u.deg, angle=0 * u.deg), 1),\n    (RectangleSkyRegion(center, 0.5 * u.deg, 1 * u.deg, angle=90 * u.deg), 0),\n    (EllipseSkyRegion(center, 0.1 * u.deg, 1 * u.deg, angle=0 * u.deg), 2),\n    (EllipseSkyRegion(center, 0.1 * u.deg, 1 * u.deg, angle=60 * u.deg), 3),\n    (EllipseSkyRegion(center, 0.1 * u.deg, 1 * u.deg, angle=90 * u.deg), 0),\n]\n\n\n@pytest.mark.parametrize(""region, nreg"", other_region_finder_param)\ndef test_non_circular_regions(region, nreg):\n    pointing = SkyCoord(0.0, 0.0, unit=""deg"")\n\n    finder = ReflectedRegionsFinder(\n        center=pointing, region=region, min_distance_input=""0 deg""\n    )\n    finder.run()\n    regions = finder.reflected_regions\n    assert len(regions) == nreg\n\n\n@requires_dependency(""matplotlib"")\ndef test_bad_on_region(exclusion_mask, on_region):\n    pointing = SkyCoord(83.63, 22.01, unit=""deg"", frame=""icrs"")\n    finder = ReflectedRegionsFinder(\n        center=pointing,\n        region=on_region,\n        exclusion_mask=exclusion_mask,\n        min_distance_input=""0 deg"",\n    )\n    finder.run()\n    regions = finder.reflected_regions\n    assert len(regions) == 0\n\n    # try plotting\n    with mpl_plot_check():\n        finder.plot()\n\n\n@requires_data()\ndef test_reflected_bkg_maker(on_region, reflected_bkg_maker, observations):\n    datasets = []\n\n    e_reco = MapAxis.from_edges(np.logspace(0, 2, 5) * u.TeV, name=""energy"")\n    e_true = MapAxis.from_edges(np.logspace(-0.5, 2, 11) * u.TeV, name=""energy_true"")\n\n    dataset_empty = SpectrumDataset.create(\n        e_reco=e_reco, e_true=e_true, region=on_region\n    )\n\n    maker = SpectrumDatasetMaker(selection=[""counts""])\n\n    for obs in observations:\n        dataset = maker.run(dataset_empty, obs)\n        dataset_on_off = reflected_bkg_maker.run(dataset, obs)\n        datasets.append(dataset_on_off)\n\n    assert_allclose(datasets[0].counts_off.data.sum(), 76)\n    assert_allclose(datasets[1].counts_off.data.sum(), 60)\n\n    regions_0 = compound_region_to_list(datasets[0].counts_off.geom.region)\n    regions_1 = compound_region_to_list(datasets[1].counts_off.geom.region)\n    assert_allclose(len(regions_0), 11)\n    assert_allclose(len(regions_1), 11)\n\n\n@requires_data()\ndef test_reflected_bkg_maker_no_off(reflected_bkg_maker, observations):\n    pos = SkyCoord(83.6333313, 21.51444435, unit=""deg"", frame=""icrs"")\n    radius = Angle(0.11, ""deg"")\n    region = CircleSkyRegion(pos, radius)\n\n    maker = SpectrumDatasetMaker(selection=[""counts""])\n\n    datasets = []\n\n    e_reco = MapAxis.from_edges(np.logspace(0, 2, 5) * u.TeV, name=""energy"")\n    e_true = MapAxis.from_edges(np.logspace(-0.5, 2, 11) * u.TeV, name=""energy_true"")\n    dataset_empty = SpectrumDataset.create(e_reco=e_reco, e_true=e_true, region=region)\n\n    for obs in observations:\n        dataset = maker.run(dataset_empty, obs)\n        dataset_on_off = reflected_bkg_maker.run(dataset, obs)\n        datasets.append(dataset_on_off)\n\n    assert datasets[0].counts_off is None\n    assert_allclose(datasets[0].acceptance_off, 0)\n'"
gammapy/makers/background/tests/test_ring.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nfrom astropy.coordinates import Angle, SkyCoord\nfrom regions import CircleSkyRegion\nfrom gammapy.data import DataStore\nfrom gammapy.datasets import MapDataset\nfrom gammapy.makers import (\n    AdaptiveRingBackgroundMaker,\n    MapDatasetMaker,\n    RingBackgroundMaker,\n    SafeMaskMaker,\n)\nfrom gammapy.maps import MapAxis, WcsGeom, WcsNDMap\nfrom gammapy.utils.testing import requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef observations():\n    """"""Example observation list for testing.""""""\n    datastore = DataStore.from_dir(""$GAMMAPY_DATA/hess-dl3-dr1/"")\n    obs_ids = [23523, 23526]\n    return datastore.get_observations(obs_ids)\n\n\n@pytest.fixture(scope=""session"")\ndef geom():\n    energy_axis = MapAxis.from_edges([1, 10], unit=""TeV"", name=""energy"", interp=""log"")\n    return WcsGeom.create(\n        skydir=SkyCoord(83.633, 22.014, unit=""deg""),\n        binsz=0.02,\n        width=(10, 10),\n        frame=""galactic"",\n        proj=""CAR"",\n        axes=[energy_axis],\n    )\n\n\n@pytest.fixture(scope=""session"")\ndef exclusion_mask(geom):\n    """"""Example mask for testing.""""""\n    pos = SkyCoord(83.633, 22.014, unit=""deg"", frame=""icrs"")\n    region = CircleSkyRegion(pos, Angle(0.15, ""deg""))\n    exclusion = WcsNDMap.from_geom(geom)\n    exclusion.data = geom.region_mask([region], inside=False)\n    return exclusion\n\n\n@requires_data()\ndef test_ring_bkg_maker(geom, observations, exclusion_mask):\n    ring_bkg_maker = RingBackgroundMaker(\n        r_in=""0.2 deg"", width=""0.3 deg"", exclusion_mask=exclusion_mask\n    )\n    safe_mask_maker = SafeMaskMaker(methods=[""offset-max""], offset_max=""2 deg"")\n    map_dataset_maker = MapDatasetMaker(selection=[""counts"", ""background"", ""exposure""])\n\n    reference = MapDataset.create(geom)\n    datasets = []\n\n    for obs in observations:\n        cutout = reference.cutout(obs.pointing_radec, width=""4 deg"")\n        dataset = map_dataset_maker.run(cutout, obs)\n        dataset = safe_mask_maker.run(dataset, obs)\n        dataset = dataset.to_image()\n\n        dataset_on_off = ring_bkg_maker.run(dataset)\n        datasets.append(dataset_on_off)\n\n    mask = dataset.mask_safe\n    assert_allclose(datasets[0].counts_off.data[mask].sum(), 2511333)\n    assert_allclose(datasets[1].counts_off.data[mask].sum(), 2143577.0)\n    assert_allclose(datasets[0].acceptance_off.data[mask].sum(), 2961300, rtol=1e-5)\n    assert_allclose(datasets[1].acceptance_off.data[mask].sum(), 2364657.2, rtol=1e-5)\n    assert_allclose(datasets[0].alpha.data[0][100][100], 0.00063745599, rtol=1e-5)\n    assert_allclose(\n        datasets[0].exposure.data[0][100][100], 806254444.8480084, rtol=1e-5\n    )\n\n\n@pytest.mark.parametrize(\n    ""pars"",\n    [\n        {\n            ""obs_idx"": 0,\n            ""method"": ""fixed_r_in"",\n            ""counts_off"": 2511417.0,\n            ""acceptance_off"": 2960679.91214,\n            ""alpha"": 0.000637456020,\n            ""exposure"": 806254444.8480084,\n        },\n        {\n            ""obs_idx"": 0,\n            ""method"": ""fixed_width"",\n            ""counts_off"": 2511417.0,\n            ""acceptance_off"": 2960679.91214,\n            ""alpha"": 0.000637456020,\n            ""exposure"": 806254444.8480084,\n        },\n        {\n            ""obs_idx"": 1,\n            ""method"": ""fixed_r_in"",\n            ""counts_off"": 2143577.0,\n            ""acceptance_off"": 2364657.352647,\n            ""alpha"": 0.00061841976,\n            ""exposure"": 779613265.2688407,\n        },\n        {\n            ""obs_idx"": 1,\n            ""method"": ""fixed_width"",\n            ""counts_off"": 2143577.0,\n            ""acceptance_off"": 2364657.352647,\n            ""alpha"": 0.00061841976,\n            ""exposure"": 779613265.2688407,\n        },\n    ],\n)\n@requires_data()\ndef test_adaptive_ring_bkg_maker(pars, geom, observations, exclusion_mask):\n    adaptive_ring_bkg_maker = AdaptiveRingBackgroundMaker(\n        r_in=""0.2 deg"",\n        width=""0.3 deg"",\n        r_out_max=""2 deg"",\n        stepsize=""0.2 deg"",\n        exclusion_mask=exclusion_mask,\n        method=pars[""method""],\n    )\n    safe_mask_maker = SafeMaskMaker(methods=[""offset-max""], offset_max=""2 deg"")\n    map_dataset_maker = MapDatasetMaker(selection=[""counts"", ""background"", ""exposure""])\n\n    obs = observations[pars[""obs_idx""]]\n\n    dataset = MapDataset.create(geom).cutout(obs.pointing_radec, width=""4 deg"")\n    dataset = map_dataset_maker.run(dataset, obs)\n    dataset = safe_mask_maker.run(dataset, obs)\n\n    dataset = dataset.to_image()\n    dataset_on_off = adaptive_ring_bkg_maker.run(dataset)\n\n    mask = dataset.mask_safe\n    assert_allclose(dataset_on_off.counts_off.data[mask].sum(), pars[""counts_off""])\n    assert_allclose(\n        dataset_on_off.acceptance_off.data[mask].sum(), pars[""acceptance_off""]\n    )\n    assert_allclose(dataset_on_off.alpha.data[0][100][100], pars[""alpha""])\n    assert_allclose(dataset_on_off.exposure.data[0][100][100], pars[""exposure""])\n'"
gammapy/modeling/models/tests/__init__.py,0,b''
gammapy/modeling/models/tests/test_core.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom gammapy.modeling.models import Model, Parameter, Parameters\nfrom gammapy.datasets import Datasets\nfrom gammapy.utils.testing import requires_data\n\n\nclass MyModel(Model):\n    """"""Simple model example""""""\n\n    x = Parameter(""x"", 1, ""cm"")\n    y = Parameter(""y"", 2)\n\n\nclass CoModel(Model):\n    """"""Compound model example""""""\n\n    norm = Parameter(""norm"", 42, ""cm"")\n\n    def __init__(self, m1, m2, norm=norm.quantity):\n        self.m1 = m1\n        self.m2 = m2\n        super().__init__(norm=norm)\n\n    @property\n    def parameters(self):\n        return Parameters([self.norm]) + self.m1.parameters + self.m2.parameters\n\n\nclass WrapperModel(Model):\n    """"""Wrapper compound model.\n\n    Dynamically generated parameters in `__init__`,\n    and a parameter name conflict with the wrapped\n    model, both have a parameter called ""y"".\n    """"""\n\n    def __init__(self, m1, a=1, y=99):\n        self.m1 = m1\n        a = Parameter(""a"", a)\n        y = Parameter(""y"", y)\n        self.default_parameters = Parameters([a, y])\n        super().__init__(a=a, y=y)\n\n    @property\n    def parameters(self):\n        return Parameters([self.a, self.y]) + self.m1.parameters\n\n\ndef test_model_class():\n    assert isinstance(MyModel.parameters, property)\n    assert MyModel.x.name == ""x""\n    assert MyModel.default_parameters[""x""] is MyModel.x\n\n\ndef test_model_class_par_init():\n    x = Parameter(""x"", 4, ""cm"")\n    y = Parameter(""y"", 10)\n\n    model = MyModel(x=x, y=y)\n\n    assert x is model.x\n    assert y is model.y\n\n\ndef test_model_init():\n    m = MyModel()\n    assert m.x.name == ""x""\n    assert m.x.value == 1\n    assert m.x is m.parameters[0]\n    assert m.y is m.parameters[1]\n    assert m.parameters is not MyModel.default_parameters\n\n    m = MyModel(x=""99 cm"")\n    assert m.x.value == 99\n    assert m.y.value == 2\n\n    # Currently we always convert to the default unit of a parameter\n    # TODO: discuss if this is the behaviour we want, or if we instead\n    # should change to the user-set unit, as long as it\'s compatible\n    m = MyModel(x=99 * u.m)\n    assert_allclose(m.x.value, 9900)\n    assert m.x.unit == ""cm""\n\n    with pytest.raises(u.UnitConversionError):\n        MyModel(x=99)\n\n    with pytest.raises(u.UnitConversionError):\n        MyModel(x=99 * u.s)\n\n\ndef test_wrapper_model():\n    outer = MyModel()\n    m = WrapperModel(outer)\n\n    assert isinstance(m.a, Parameter)\n    assert m.y.value == 99\n\n    assert m.parameters.names == [""a"", ""y"", ""x"", ""y""]\n\n\ndef test_model_parameter():\n    m = MyModel(x=""99 cm"")\n    assert isinstance(m.x, Parameter)\n    assert m.x.value == 99\n    assert m.x.unit == ""cm""\n\n    with pytest.raises(TypeError):\n        m.x = 99\n\n    with pytest.raises(TypeError):\n        m.x = 99 * u.cm\n\n\n# TODO: implement parameter linking. Not working ATM!\n@pytest.mark.xfail()\ndef test_model_parameter_link():\n    # Assigning a parameter should create a link\n    m = MyModel()\n    par = MyModel.x.copy()\n    m.x = par\n    assert isinstance(m.x, Parameter)\n    assert m.x is par\n    # model.parameters should be in sync with attributes\n    assert m.x is m.parameters[""x""]\n\n\ndef test_model_copy():\n    m = MyModel()\n\n    m2 = m.copy()\n\n    # Models should be independent\n    assert m.parameters is not m2.parameters\n    assert m.parameters[0] is not m2.parameters[0]\n\n\ndef test_model_create():\n    spectral_model = Model.create(\n        ""PowerLaw2SpectralModel"", amplitude=""1e-10 cm-2 s-1"", index=3\n    )\n    assert spectral_model.tag == ""PowerLaw2SpectralModel""\n    assert_allclose(spectral_model.index.value, 3)\n\n\ndef test_compound_model():\n    m1 = MyModel()\n    m2 = MyModel(x=10 * u.cm, y=20)\n    m = CoModel(m1, m2)\n    assert len(m.parameters) == 5\n    assert m.parameters.names == [""norm"", ""x"", ""y"", ""x"", ""y""]\n    assert_allclose(m.parameters.values, [42, 1, 2, 10, 20])\n\n\ndef test_parameter_link_init():\n    m1 = MyModel()\n    m2 = MyModel(y=m1.y)\n\n    assert m1.y is m2.y\n\n    m1.y.value = 100\n    assert_allclose(m2.y.value, 100)\n\n\ndef test_parameter_link():\n    m1 = MyModel()\n    m2 = MyModel()\n\n    m2.y = m1.y\n\n    m1.y.value = 100\n    assert_allclose(m2.y.value, 100)\n\n\n@requires_data()\ndef test_models_management(tmp_path):\n    path = ""$GAMMAPY_DATA/tests/models""\n    filedata = ""gc_example_datasets.yaml""\n    filemodel = ""gc_example_models.yaml""\n\n    datasets = Datasets.read(path, filedata, filemodel)\n\n    model1 = datasets.models[0].copy(name=""model1"", datasets_names=None)\n    model2 = datasets.models[0].copy(name=""model2"", datasets_names=[datasets[1].name])\n    model3 = datasets.models[0].copy(name=""model3"", datasets_names=[datasets[0].name])\n\n    model1b = datasets.models[0].copy(name=""model1"", datasets_names=None)\n    model1b.spectral_model.amplitude.value *= 2\n\n    names0 = datasets[0].models.names\n    names1 = datasets[1].models.names\n\n    datasets[0].models.append(model1)\n    _ = datasets[0].models + model2\n    assert datasets[0].models.names == names0 + [""model1"", ""model2""]\n    assert datasets[0].models[""model1""].datasets_names == None\n    assert datasets[0].models[""model2""].datasets_names == [\n        datasets[1].name,\n        datasets[0].name,\n    ]\n    assert datasets[1].models.names == names1 + [""model1"", ""model2""]\n\n    # TODO consistency check at datasets level ?\n    # or force same Models for each dataset._models on datasets init ?\n    # here we have the right behavior: model1 and model2 are also added to dataset1\n    # because serialization create a global model object shared by all datasets\n    # if that was not the case we could have inconsistancies\n    # such as model1.datasets_names == None added only to dataset1\n    # user can still create such inconsistancies if they define datasets\n    # with diferent Models objects for each dataset.\n\n    del datasets[0].models[""model1""]\n    assert datasets[0].models.names == names0 + [""model2""]\n\n    datasets[0].models.remove(model2)\n    assert datasets[0].models.names == names0\n\n    datasets.models.append(model2)\n    assert model2 in datasets.models\n    assert model2 in datasets[1].models\n    assert datasets[0].models.names == names0 + [""model2""]\n\n    datasets[0].models.extend([model1, model3])\n    assert datasets[0].models.names == names0 + [""model2"", ""model1"", ""model3""]\n\n    for m in [model1, model2, model3]:\n        datasets.models.remove(m)\n    assert datasets[0].models.names == names0\n    assert datasets[1].models.names == names1\n    datasets.models.extend([model1, model2, model3])\n    assert datasets[0].models.names == names0 + [""model1"", ""model2"", ""model3""]\n    assert datasets[1].models.names == names1 + [""model1"", ""model2""]\n\n    for m in [model1, model2, model3]:\n        datasets.models.remove(m)\n    _ = datasets.models + [model1, model2]\n    assert datasets[0].models.names == names0 + [""model1"", ""model2""]\n    assert datasets[1].models.names == names1 + [""model1"", ""model2""]\n\n    datasets[0].models[""model2""] = model3\n    assert datasets[0].models.names == names0 + [""model1"", ""model3""]\n    assert datasets[1].models.names == names1 + [""model1""]\n\n    datasets.models.remove(model1)\n    datasets[0].models = model1\n    _ = datasets.models  # auto-update models\n    assert datasets[0].models.names == [""model1"", ""gll_iem_v06_cutout""]\n    # the consistency check added diffuse model contained in the global model\n\n    npred1 = datasets[0].npred().data.sum()\n    datasets.models.remove(model1)\n    npred0 = datasets[0].npred().data.sum()\n    datasets.models.append(model1b)\n    npred1b = datasets[0].npred().data.sum()\n    assert npred1b != npred1\n    assert npred1b != npred0\n    assert_allclose(npred1b, 2147.407023024028)\n'"
gammapy/modeling/models/tests/test_cube.py,12,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.time import Time\nfrom gammapy.data.gti import GTI\nfrom gammapy.datasets.map import MapEvaluator\nfrom gammapy.irf import EDispKernel, PSFKernel\nfrom gammapy.maps import Map, MapAxis, WcsGeom\nfrom gammapy.modeling.models import (\n    BackgroundModel,\n    ConstantSpectralModel,\n    ConstantTemporalModel,\n    GaussianSpatialModel,\n    Models,\n    PointSpatialModel,\n    PowerLawSpectralModel,\n    SkyDiffuseCube,\n    SkyModel,\n    create_fermi_isotropic_diffuse_model,\n)\nfrom gammapy.utils.testing import requires_data\n\n\n@pytest.fixture(scope=""session"")\ndef sky_model():\n    spatial_model = GaussianSpatialModel(\n        lon_0=""3 deg"", lat_0=""4 deg"", sigma=""3 deg"", frame=""galactic""\n    )\n    spectral_model = PowerLawSpectralModel(\n        index=2, amplitude=""1e-11 cm-2 s-1 TeV-1"", reference=""1 TeV""\n    )\n    temporal_model = ConstantTemporalModel()\n    return SkyModel(\n        spatial_model=spatial_model,\n        spectral_model=spectral_model,\n        temporal_model=temporal_model,\n        name=""source-1"",\n    )\n\n\n@pytest.fixture(scope=""session"")\ndef gti():\n    start = [1, 3, 5] * u.day\n    stop = [2, 3.5, 6] * u.day\n    t_ref = Time(55555, format=""mjd"")\n    gti = GTI.create(start, stop, reference_time=t_ref)\n    return gti\n\n\n@pytest.fixture(scope=""session"")\ndef diffuse_model():\n    axis = MapAxis.from_nodes([0.1, 100], name=""energy_true"", unit=""TeV"", interp=""log"")\n    m = Map.create(\n        npix=(4, 3), binsz=2, axes=[axis], unit=""cm-2 s-1 MeV-1 sr-1"", frame=""galactic""\n    )\n    m.data += 42\n    return SkyDiffuseCube(m)\n\n\n@pytest.fixture(scope=""session"")\ndef geom():\n    axis = MapAxis.from_edges(np.logspace(-1, 1, 3), unit=u.TeV, name=""energy"")\n    return WcsGeom.create(skydir=(0, 0), npix=(5, 4), frame=""galactic"", axes=[axis])\n\n\n@pytest.fixture(scope=""session"")\ndef geom_true():\n    axis = MapAxis.from_edges(np.logspace(-1, 1, 4), unit=u.TeV, name=""energy_true"")\n    return WcsGeom.create(skydir=(0, 0), npix=(5, 4), frame=""galactic"", axes=[axis])\n\n\n@pytest.fixture(scope=""session"")\ndef exposure(geom_true):\n    m = Map.from_geom(geom_true)\n    m.quantity = np.ones(geom_true.data_shape) * u.Quantity(""100 m2 s"")\n    m.data[1] *= 10\n    return m\n\n\n@pytest.fixture(scope=""session"")\ndef background(geom):\n    m = Map.from_geom(geom)\n    m.quantity = np.ones(geom.data_shape) * 1e-7\n    return m\n\n\n@pytest.fixture(scope=""session"")\ndef edisp(geom, geom_true):\n    e_reco = geom.get_axis_by_name(""energy"").edges\n    e_true = geom_true.get_axis_by_name(""energy_true"").edges\n    return EDispKernel.from_diagonal_response(e_true=e_true, e_reco=e_reco)\n\n\n@pytest.fixture(scope=""session"")\ndef psf(geom_true):\n    sigma = 0.5 * u.deg\n    return PSFKernel.from_gauss(geom_true, sigma)\n\n\n@pytest.fixture(scope=""session"")\ndef evaluator(sky_model, exposure, psf, edisp, gti):\n    return MapEvaluator(sky_model, exposure, psf=psf, edisp=edisp, gti=gti)\n\n\n@pytest.fixture(scope=""session"")\ndef diffuse_evaluator(diffuse_model, exposure, psf, edisp):\n    return MapEvaluator(diffuse_model, exposure, psf=psf, edisp=edisp)\n\n\n@pytest.fixture(scope=""session"")\ndef sky_models(sky_model):\n    sky_model_2 = sky_model.copy(name=""source-2"")\n    sky_model_3 = sky_model.copy(name=""source-3"")\n    return Models([sky_model_2, sky_model_3])\n\n\n@pytest.fixture(scope=""session"")\ndef sky_models_2(sky_model):\n    sky_model_4 = sky_model.copy(name=""source-4"")\n    sky_model_5 = sky_model.copy(name=""source-5"")\n    return Models([sky_model_4, sky_model_5])\n\n\ndef test_sky_model_init():\n    with pytest.raises(TypeError):\n        spatial_model = GaussianSpatialModel()\n        SkyModel(spectral_model=1234, spatial_model=spatial_model)\n\n    with pytest.raises(TypeError):\n        SkyModel(spectral_model=PowerLawSpectralModel(), spatial_model=1234)\n\n\ndef test_sky_model_spatial_none_io(tmpdir):\n    pwl = PowerLawSpectralModel()\n    model = SkyModel(spectral_model=pwl, name=""test"")\n    models = Models([model])\n\n    filename = tmpdir / ""test-models-none.yaml""\n    models.write(filename)\n\n    models = Models.read(filename)\n\n    assert models[""test""].spatial_model is None\n\n\ndef test_sky_model_spatial_none_evaluate(geom_true, gti):\n    pwl = PowerLawSpectralModel()\n    model = SkyModel(spectral_model=pwl, name=""test"")\n\n    data = model.evaluate_geom(geom_true, gti).to_value(""cm-2 s-1 TeV-1"")\n\n    assert data.shape == (3, 1, 1)\n    assert_allclose(data[0], 1.256774e-11, rtol=1e-6)\n\n\ndef test_skymodel_addition(sky_model, sky_models, sky_models_2, diffuse_model):\n    models = sky_model + sky_model.copy()\n    assert isinstance(models, Models)\n    assert len(models) == 2\n\n    models = sky_model + sky_models\n    assert isinstance(models, Models)\n    assert len(models) == 3\n\n    models = sky_models + sky_model\n    assert isinstance(models, Models)\n    assert len(models) == 3\n\n    models = sky_models + diffuse_model\n    assert isinstance(models, Models)\n    assert len(models) == 3\n\n    models = sky_models + sky_models_2\n    assert isinstance(models, Models)\n    assert len(models) == 4\n\n    models = sky_model + sky_models\n    assert isinstance(models, Models)\n    assert len(models) == 3\n\n\ndef test_background_model(background):\n    bkg1 = BackgroundModel(background, norm=2.0).evaluate()\n    assert_allclose(bkg1.data[0][0][0], background.data[0][0][0] * 2.0, rtol=1e-3)\n    assert_allclose(bkg1.data.sum(), background.data.sum() * 2.0, rtol=1e-3)\n\n    bkg2 = BackgroundModel(\n        background, norm=2.0, tilt=0.2, reference=""1000 GeV""\n    ).evaluate()\n    assert_allclose(bkg2.data[0][0][0], 2.254e-07, rtol=1e-3)\n    assert_allclose(bkg2.data.sum(), 7.352e-06, rtol=1e-3)\n\n\ndef test_background_model_io(tmpdir, background):\n    filename = str(tmpdir / ""test-bkg-file.fits"")\n    bkg = BackgroundModel(background, norm=2.0, filename=filename)\n    bkg.map.write(filename, overwrite=True)\n    bkg_dict = bkg.to_dict()\n    bkg_read = bkg.from_dict(bkg_dict)\n\n    assert_allclose(\n        bkg_read.evaluate().data.sum(), background.data.sum() * 2.0, rtol=1e-3\n    )\n    assert bkg_read.filename == filename\n\n\nclass TestSkyModels:\n    @staticmethod\n    def test_parameters(sky_models):\n        parnames = [\n            ""index"",\n            ""amplitude"",\n            ""reference"",\n            ""lon_0"",\n            ""lat_0"",\n            ""sigma"",\n            ""e"",\n            ""phi"",\n        ] * 2\n        assert sky_models.parameters.names == parnames\n\n        # Check that model parameters are references to the parts\n        p1 = sky_models.parameters[""lon_0""]\n        p2 = sky_models[0].parameters[""lon_0""]\n        assert p1 is p2\n\n    @staticmethod\n    def test_str(sky_models):\n        assert ""Component 0"" in str(sky_models)\n        assert ""Component 1"" in str(sky_models)\n\n    @staticmethod\n    def test_get_item(sky_models):\n        model = sky_models[""source-2""]\n        assert model.name == ""source-2""\n\n        model = sky_models[""source-3""]\n        assert model.name == ""source-3""\n\n        with pytest.raises(ValueError):\n            sky_models[""spam""]\n\n    @staticmethod\n    def test_names(sky_models):\n        assert sky_models.names == [""source-2"", ""source-3""]\n\n\n@requires_data()\ndef test_models_mutation(sky_model, sky_models, sky_models_2):\n    mods = sky_models\n\n    mods.insert(0, sky_model)\n    assert mods.names == [""source-1"", ""source-2"", ""source-3""]\n\n    mods.extend(sky_models_2)\n    assert mods.names == [""source-1"", ""source-2"", ""source-3"", ""source-4"", ""source-5""]\n\n    mod3 = mods[3]\n    mods.remove(mods[3])\n    assert mods.names == [""source-1"", ""source-2"", ""source-3"", ""source-5""]\n    mods.append(mod3)\n    assert mods.names == [""source-1"", ""source-2"", ""source-3"", ""source-5"", ""source-4""]\n    mods.pop(3)\n    assert mods.names == [""source-1"", ""source-2"", ""source-3"", ""source-4""]\n\n    with pytest.raises(ValueError, match=""Model names must be unique""):\n        mods.append(sky_model)\n    with pytest.raises(ValueError, match=""Model names must be unique""):\n        mods.insert(0, sky_model)\n    with pytest.raises(ValueError, match=""Model names must be unique""):\n        mods.extend(sky_models_2)\n    with pytest.raises(ValueError, match=""Model names must be unique""):\n        mods = sky_models + sky_models_2\n\n\nclass TestSkyModel:\n    @staticmethod\n    def test_repr(sky_model):\n        assert ""SkyModel"" in repr(sky_model)\n\n    @staticmethod\n    def test_str(sky_model):\n        assert ""SkyModel"" in str(sky_model)\n\n    @staticmethod\n    def test_parameters(sky_model):\n        # Check that model parameters are references to the spatial and spectral parts\n        p1 = sky_model.parameters[""lon_0""]\n        p2 = sky_model.spatial_model.parameters[""lon_0""]\n        assert p1 is p2\n\n        p1 = sky_model.parameters[""amplitude""]\n        p2 = sky_model.spectral_model.parameters[""amplitude""]\n        assert p1 is p2\n\n    @staticmethod\n    def test_evaluate_scalar(sky_model):\n        lon = 3 * u.deg\n        lat = 4 * u.deg\n        energy = 1 * u.TeV\n\n        q = sky_model.evaluate(lon, lat, energy)\n\n        assert q.unit == ""cm-2 s-1 TeV-1 sr-1""\n        assert np.isscalar(q.value)\n        assert_allclose(q.to_value(""cm-2 s-1 TeV-1 deg-2""), 1.76879232e-13)\n\n    @staticmethod\n    def test_evaluate_array(sky_model):\n        lon = 3 * u.deg * np.ones(shape=(3, 4))\n        lat = 4 * u.deg * np.ones(shape=(3, 4))\n        energy = [1, 1, 1, 1, 1] * u.TeV\n\n        q = sky_model.evaluate(lon, lat, energy[:, np.newaxis, np.newaxis])\n\n        assert q.shape == (5, 3, 4)\n        assert_allclose(q.to_value(""cm-2 s-1 TeV-1 deg-2""), 1.76879232e-13)\n\n    @staticmethod\n    def test_processing(sky_model):\n        assert sky_model.apply_irf == {""exposure"": True, ""psf"": True, ""edisp"": True}\n        out = sky_model.to_dict()\n        assert ""apply_irf"" not in out\n\n        sky_model.apply_irf[""edisp""] = False\n        out = sky_model.to_dict()\n        assert out[""apply_irf""] == {""exposure"": True, ""psf"": True, ""edisp"": False}\n        sky_model.apply_irf[""edisp""] = True\n\n\nclass TestSkyDiffuseCube:\n    @staticmethod\n    def test_evaluate_scalar(diffuse_model):\n        # Check pixel inside map\n        val = diffuse_model.evaluate(0 * u.deg, 0 * u.deg, 10 * u.TeV)\n        assert val.unit == ""cm-2 s-1 MeV-1 sr-1""\n        assert val.shape == (1,)\n        assert_allclose(val.value, 42)\n\n        # Check pixel outside map (spatially)\n        val = diffuse_model.evaluate(100 * u.deg, 0 * u.deg, 10 * u.TeV)\n        assert_allclose(val.value, 0)\n\n        # Check pixel outside energy range\n        val = diffuse_model.evaluate(0 * u.deg, 0 * u.deg, 200 * u.TeV)\n        assert_allclose(val.value, 0)\n\n    @staticmethod\n    def test_evaluate_array(diffuse_model):\n        lon = 1 * u.deg * np.ones(shape=(3, 4))\n        lat = 2 * u.deg * np.ones(shape=(3, 4))\n        energy = [1, 1, 1, 1, 1] * u.TeV\n\n        q = diffuse_model.evaluate(lon, lat, energy[:, np.newaxis, np.newaxis])\n\n        assert q.shape == (5, 3, 4)\n        assert_allclose(q.value.mean(), 42)\n\n    @staticmethod\n    @requires_data()\n    def test_read():\n        model = SkyDiffuseCube.read(\n            ""$GAMMAPY_DATA/tests/unbundled/fermi/gll_iem_v02_cutout.fits""\n        )\n        assert model.map.unit == ""cm-2 s-1 MeV-1 sr-1""\n\n        # Check pixel inside map\n        val = model.evaluate(0 * u.deg, 0 * u.deg, 100 * u.GeV)\n        assert val.unit == ""cm-2 s-1 MeV-1 sr-1""\n        assert val.shape == (1,)\n        assert_allclose(val.value, 1.396424e-12, rtol=1e-5)\n\n    @staticmethod\n    def test_evaluation_radius(diffuse_model):\n        radius = diffuse_model.evaluation_radius\n        assert radius.unit == ""deg""\n        assert_allclose(radius.value, 4)\n\n    @staticmethod\n    def test_frame(diffuse_model):\n        assert diffuse_model.frame == ""galactic""\n\n    @staticmethod\n    def test_processing(diffuse_model):\n        assert diffuse_model.apply_irf == {""exposure"": True, ""psf"": True, ""edisp"": True}\n        out = diffuse_model.to_dict()\n        assert ""apply_irf"" not in out\n\n        diffuse_model.apply_irf[""edisp""] = False\n        out = diffuse_model.to_dict()\n        assert out[""apply_irf""] == {""exposure"": True, ""psf"": True, ""edisp"": False}\n        diffuse_model.apply_irf[""edisp""] = True\n\n    @staticmethod\n    def test_datasets_name(diffuse_model):\n        assert diffuse_model.datasets_names is None\n\n        diffuse_model.datasets_names = [""1"", ""2""]\n        out = diffuse_model.to_dict()\n        assert out[""datasets_names""] == [""1"", ""2""]\n\n        diffuse_model.datasets_names = None\n        out = diffuse_model.to_dict()\n        assert ""datasets_names"" not in out\n\n\nclass TestSkyDiffuseCubeMapEvaluator:\n    @staticmethod\n    def test_compute_dnde(diffuse_evaluator):\n        out = diffuse_evaluator.compute_dnde()\n        assert out.shape == (3, 4, 5)\n        out = out.to(""cm-2 s-1 MeV-1 sr-1"")\n        assert_allclose(out.value.sum(), 2520.0, rtol=1e-5)\n        assert_allclose(out.value[0, 0, 0], 42, rtol=1e-5)\n\n    @staticmethod\n    def test_compute_flux(diffuse_evaluator):\n        out = diffuse_evaluator.compute_flux()\n        assert out.data.shape == (3, 4, 5)\n        out = out.quantity.to(""cm-2 s-1"")\n        assert_allclose(out.value.sum(), 633263.444803, rtol=1e-5)\n        assert_allclose(out.value[0, 0, 0], 1164.656176, rtol=1e-5)\n\n    @staticmethod\n    def test_apply_psf(diffuse_evaluator):\n        flux = diffuse_evaluator.compute_flux()\n        npred = diffuse_evaluator.apply_exposure(flux)\n        out = diffuse_evaluator.apply_psf(npred)\n        assert out.data.shape == (3, 4, 5)\n        assert_allclose(out.data.sum(), 1.106404e12, rtol=1e-5)\n        assert_allclose(out.data[0, 0, 0], 5.586508e08, rtol=1e-5)\n\n    @staticmethod\n    def test_apply_edisp(diffuse_evaluator):\n        flux = diffuse_evaluator.compute_flux()\n        npred = diffuse_evaluator.apply_exposure(flux)\n        out = diffuse_evaluator.apply_edisp(npred)\n        assert out.data.shape == (2, 4, 5)\n        assert_allclose(out.data.sum(), 1.606345e12, rtol=1e-5)\n        assert_allclose(out.data[0, 0, 0], 1.164656e09, rtol=1e-5)\n\n    @staticmethod\n    def test_compute_npred(diffuse_evaluator):\n        out = diffuse_evaluator.compute_npred()\n        assert out.data.shape == (2, 4, 5)\n        assert_allclose(out.data.sum(), 1.106403e12, rtol=1e-5)\n        assert_allclose(out.data[0, 0, 0], 5.586508e08, rtol=1e-5)\n\n\nclass TestSkyModelMapEvaluator:\n    @staticmethod\n    def test_compute_dnde(evaluator):\n        out = evaluator.compute_dnde()\n        assert out.shape == (3, 4, 5)\n        assert out.unit == ""cm-2 s-1 TeV-1 sr-1""\n        assert_allclose(\n            out.to_value(""cm-2 s-1 TeV-1 deg-2"").sum(),\n            1.1788166328203174e-11,\n            rtol=1e-5,\n        )\n        assert_allclose(\n            out.to_value(""cm-2 s-1 TeV-1 deg-2"")[0, 0, 0],\n            5.087056282039508e-13,\n            rtol=1e-5,\n        )\n\n    @staticmethod\n    def test_compute_flux(evaluator):\n        out = evaluator.compute_flux()\n        out = out.quantity.to_value(""cm-2 s-1"")\n        assert out.shape == (3, 4, 5)\n        assert_allclose(out.sum(), 2.213817e-12, rtol=1e-5)\n        assert_allclose(out[0, 0, 0], 7.938388e-14, rtol=1e-5)\n\n    @staticmethod\n    def test_apply_psf(evaluator):\n        flux = evaluator.compute_flux()\n        npred = evaluator.apply_exposure(flux)\n        out = evaluator.apply_psf(npred)\n        assert out.data.shape == (3, 4, 5)\n        assert_allclose(out.data.sum(), 3.862314e-06, rtol=1e-5)\n        assert_allclose(out.data[0, 0, 0], 4.126612e-08, rtol=1e-5)\n\n    @staticmethod\n    def test_apply_edisp(evaluator):\n        flux = evaluator.compute_flux()\n        npred = evaluator.apply_exposure(flux)\n        out = evaluator.apply_edisp(npred)\n        assert out.data.shape == (2, 4, 5)\n        assert_allclose(out.data.sum(), 5.615601e-06, rtol=1e-5)\n        assert_allclose(out.data[0, 0, 0], 7.938388e-08, rtol=1e-5)\n\n    @staticmethod\n    def test_compute_npred(evaluator, gti):\n        out = evaluator.compute_npred()\n        assert out.data.shape == (2, 4, 5)\n        assert_allclose(out.data.sum(), 3.862314e-06, rtol=1e-5)\n        assert_allclose(out.data[0, 0, 0], 4.126612e-08, rtol=1e-5)\n\n\ndef test_sky_point_source():\n    # Test special case of point source. Regression test for GH 2367.\n\n    energy_axis = MapAxis.from_edges(\n        [1, 10], unit=""TeV"", name=""energy_true"", interp=""log""\n    )\n    exposure = Map.create(\n        skydir=(100, 70),\n        npix=(4, 4),\n        binsz=0.1,\n        proj=""AIT"",\n        unit=""cm2 s"",\n        axes=[energy_axis],\n    )\n    exposure.data = np.ones_like(exposure.data)\n\n    spatial_model = PointSpatialModel(\n        lon_0=100.06 * u.deg, lat_0=70.03 * u.deg, frame=""icrs""\n    )\n    # Create a spectral model with integral flux of 1 cm-2 s-1 in this energy band\n    spectral_model = ConstantSpectralModel(const=""1 cm-2 s-1 TeV-1"")\n    spectral_model.const.value /= spectral_model.integral(1 * u.TeV, 10 * u.TeV).value\n    model = SkyModel(spatial_model=spatial_model, spectral_model=spectral_model)\n    evaluator = MapEvaluator(model=model, exposure=exposure)\n    flux = evaluator.compute_flux().quantity.to_value(""cm-2 s-1"")[0]\n\n    expected = [\n        [0, 0, 0, 0],\n        [0, 0.140, 0.058, 0.0],\n        [0, 0.564, 0.236, 0],\n        [0, 0, 0, 0],\n    ]\n    assert_allclose(flux, expected, atol=0.01)\n\n    assert_allclose(flux.sum(), 1)\n\n\n@requires_data()\ndef test_fermi_isotropic():\n    filename = ""$GAMMAPY_DATA/fermi_3fhl/iso_P8R2_SOURCE_V6_v06.txt""\n    model = create_fermi_isotropic_diffuse_model(filename)\n    coords = {""lon"": 0 * u.deg, ""lat"": 0 * u.deg, ""energy"": 50 * u.GeV}\n\n    flux = model(**coords)\n\n    assert_allclose(flux.value, 1.463e-13, rtol=1e-3)\n    assert flux.unit == ""MeV-1 cm-2 s-1 sr-1""\n'"
gammapy/modeling/models/tests/test_io.py,9,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.table import Table\nfrom astropy.utils.data import get_pkg_data_filename\nfrom gammapy.maps import Map, MapAxis\nfrom gammapy.modeling.models import (\n    MODELS,\n    AbsorbedSpectralModel,\n    Absorption,\n    BackgroundModel,\n    Model,\n    Models,\n)\nfrom gammapy.utils.scripts import read_yaml, write_yaml\nfrom gammapy.utils.testing import requires_data\n\n\n@requires_data()\ndef test_dict_to_skymodels():\n    filename = get_pkg_data_filename(""data/examples.yaml"")\n    models_data = read_yaml(filename)\n    models = Models.from_dict(models_data)\n\n    assert len(models) == 5\n\n    model0 = models[0]\n    assert isinstance(model0, BackgroundModel)\n    assert model0.name == ""background_irf""\n\n    model0 = models[1]\n    assert model0.spectral_model.tag == ""ExpCutoffPowerLawSpectralModel""\n    assert model0.spatial_model.tag == ""PointSpatialModel""\n\n    pars0 = model0.parameters\n    assert pars0[""index""].value == 2.1\n    assert pars0[""index""].unit == """"\n    assert np.isnan(pars0[""index""].max)\n    assert np.isnan(pars0[""index""].min)\n    assert not pars0[""index""].frozen\n\n    assert pars0[""lon_0""].value == -0.5\n    assert pars0[""lon_0""].unit == ""deg""\n    assert pars0[""lon_0""].max == 180.0\n    assert pars0[""lon_0""].min == -180.0\n    assert pars0[""lon_0""].frozen\n\n    assert pars0[""lat_0""].value == -0.0005\n    assert pars0[""lat_0""].unit == ""deg""\n    assert pars0[""lat_0""].max == 90.0\n    assert pars0[""lat_0""].min == -90.0\n    assert pars0[""lat_0""].frozen\n\n    assert pars0[""lambda_""].value == 0.006\n    assert pars0[""lambda_""].unit == ""TeV-1""\n    assert np.isnan(pars0[""lambda_""].min)\n    assert np.isnan(pars0[""lambda_""].max)\n\n    model1 = models[2]\n    assert model1.spectral_model.tag == ""PowerLawSpectralModel""\n    assert model1.spatial_model.tag == ""DiskSpatialModel""\n    assert model1.temporal_model.tag == ""LightCurveTemplateTemporalModel""\n\n    pars1 = model1.parameters\n    assert pars1[""index""].value == 2.2\n    assert pars1[""index""].unit == """"\n    assert pars1[""lat_0""].scale == 1.0\n    assert pars1[""lat_0""].factor == pars1[""lat_0""].value\n\n    assert np.isnan(pars1[""index""].max)\n    assert np.isnan(pars1[""index""].min)\n\n    assert pars1[""r_0""].unit == ""deg""\n\n    model2 = models[3]\n    assert_allclose(model2.spectral_model.energy.data, [34.171, 44.333, 57.517])\n    assert model2.spectral_model.energy.unit == ""MeV""\n    assert_allclose(\n        model2.spectral_model.values.data, [2.52894e-06, 1.2486e-06, 6.14648e-06]\n    )\n    assert model2.spectral_model.values.unit == ""1 / (cm2 MeV s sr)""\n\n    assert model2.spectral_model.tag == ""TemplateSpectralModel""\n    assert model2.spatial_model.tag == ""TemplateSpatialModel""\n\n    assert model2.spatial_model.parameters[""norm""].value == 1.0\n    assert not model2.spatial_model.normalize\n    assert model2.spectral_model.parameters[""norm""].value == 2.1\n\n    # TODO problem of duplicate parameter name between TemplateSpatialModel and TemplateSpectralModel\n    # assert model2.parameters[""norm""].value == 2.1 # fail\n\n\n@requires_data()\ndef test_sky_models_io(tmp_path):\n    # TODO: maybe change to a test case where we create a model programatically?\n    filename = get_pkg_data_filename(""data/examples.yaml"")\n    models = Models.read(filename)\n    models.covariance = np.eye(len(models.parameters))\n    models.write(tmp_path / ""tmp.yaml"")\n    models = Models.read(tmp_path / ""tmp.yaml"")\n    assert models._covar_file == ""tmp_covariance.dat""\n    assert_allclose(models.covariance.data, np.eye(len(models.parameters)))\n    assert_allclose(models.parameters[""lat_0""].min, -90.0)\n\n    # TODO: not sure if we should just round-trip, or if we should\n    # check YAML file content (e.g. against a ref file in the repo)\n    # or check serialised dict content\n\n\n@requires_data()\ndef test_absorption_io(tmp_path):\n    dominguez = Absorption.read_builtin(""dominguez"")\n    model = AbsorbedSpectralModel(\n        spectral_model=Model.create(""PowerLawSpectralModel""),\n        absorption=dominguez,\n        redshift=0.5,\n    )\n    assert len(model.parameters) == 5\n\n    model_dict = model.to_dict()\n    parnames = [_[""name""] for _ in model_dict[""parameters""]]\n    assert parnames == [""redshift"", ""alpha_norm""]\n\n    new_model = AbsorbedSpectralModel.from_dict(model_dict)\n\n    assert new_model.redshift.value == 0.5\n    assert new_model.alpha_norm.name == ""alpha_norm""\n    assert new_model.alpha_norm.value == 1\n    assert new_model.spectral_model.tag == ""PowerLawSpectralModel""\n    assert_allclose(new_model.absorption.energy, dominguez.energy)\n    assert_allclose(new_model.absorption.param, dominguez.param)\n    assert len(new_model.parameters) == 5\n\n    test_absorption = Absorption(\n        u.Quantity(range(3), ""keV""),\n        u.Quantity(range(2), """"),\n        u.Quantity(np.ones((2, 3)), """"),\n    )\n    model = AbsorbedSpectralModel(\n        spectral_model=Model.create(""PowerLawSpectralModel""),\n        absorption=test_absorption,\n        redshift=0.5,\n    )\n    model_dict = model.to_dict()\n    new_model = AbsorbedSpectralModel.from_dict(model_dict)\n\n    assert_allclose(new_model.absorption.energy, test_absorption.energy)\n    assert_allclose(new_model.absorption.param, test_absorption.param)\n\n    write_yaml(model_dict, tmp_path / ""tmp.yaml"")\n    read_yaml(tmp_path / ""tmp.yaml"")\n\n\ndef make_all_models():\n    """"""Make an instance of each model, for testing.""""""\n    yield Model.create(""ConstantSpatialModel"")\n    map_constantmodel = Map.create(npix=(10, 20), unit=""sr-1"")\n    yield Model.create(""TemplateSpatialModel"", map=map_constantmodel)\n    yield Model.create(""DiskSpatialModel"", lon_0=""1 deg"", lat_0=""2 deg"", r_0=""3 deg"")\n    yield Model.create(\n        ""GaussianSpatialModel"", lon_0=""1 deg"", lat_0=""2 deg"", sigma=""3 deg""\n    )\n    yield Model.create(""PointSpatialModel"", lon_0=""1 deg"", lat_0=""2 deg"")\n    yield Model.create(\n        ""ShellSpatialModel"", lon_0=""1 deg"", lat_0=""2 deg"", radius=""3 deg"", width=""4 deg""\n    )\n    yield Model.create(""ConstantSpectralModel"", const=""99 cm-2 s-1 TeV-1"")\n    # TODO: yield Model.create(""CompoundSpectralModel"")\n    yield Model.create(""PowerLawSpectralModel"")\n    yield Model.create(""PowerLaw2SpectralModel"")\n    yield Model.create(""ExpCutoffPowerLawSpectralModel"")\n    yield Model.create(""ExpCutoffPowerLaw3FGLSpectralModel"")\n    yield Model.create(""SuperExpCutoffPowerLaw3FGLSpectralModel"")\n    yield Model.create(""SuperExpCutoffPowerLaw4FGLSpectralModel"")\n    yield Model.create(""LogParabolaSpectralModel"")\n    yield Model.create(\n        ""TemplateSpectralModel"", energy=[1, 2] * u.cm, values=[3, 4] * u.cm\n    )  # TODO: add unit validation?\n    yield Model.create(""GaussianSpectralModel"")\n    # TODO: yield Model.create(""AbsorbedSpectralModel"")\n    # TODO: yield Model.create(""NaimaSpectralModel"")\n    # TODO: yield Model.create(""ScaleSpectralModel"")\n    yield Model.create(""ConstantTemporalModel"")\n    yield Model.create(""LightCurveTemplateTemporalModel"", Table())\n    yield Model.create(\n        ""SkyModel"",\n        spatial_model=Model.create(""ConstantSpatialModel""),\n        spectral_model=Model.create(""PowerLawSpectralModel""),\n    )\n    m1 = Map.create(\n        npix=(10, 20, 30), axes=[MapAxis.from_nodes([1, 2] * u.TeV, name=""energy"")]\n    )\n    yield Model.create(""SkyDiffuseCube"", map=m1)\n    m2 = Map.create(\n        npix=(10, 20, 30), axes=[MapAxis.from_edges([1, 2] * u.TeV, name=""energy"")]\n    )\n    yield Model.create(""BackgroundModel"", map=m2)\n\n\n@pytest.mark.parametrize(""model_class"", MODELS)\ndef test_all_model_classes(model_class):\n    assert model_class.tag == model_class.__name__\n\n\n@pytest.mark.parametrize(""model"", make_all_models())\ndef test_all_model_instances(model):\n    assert model.tag == model.__class__.__name__\n\n\n@requires_data()\ndef test_missing_parameters():\n    filename = get_pkg_data_filename(""data/examples.yaml"")\n    models = Models.read(filename)\n    assert models[""source1""].spatial_model.e in models.parameters\n    assert len(models[""source1""].spatial_model.parameters) == 6\n'"
gammapy/modeling/models/tests/test_spatial.py,12,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom astropy.wcs import WCS\nfrom regions import (\n    CircleAnnulusSkyRegion,\n    EllipseSkyRegion,\n    PointSkyRegion,\n    PolygonSkyRegion,\n)\nfrom gammapy.maps import Map, WcsGeom\nfrom gammapy.modeling.models import (\n    ConstantSpatialModel,\n    DiskSpatialModel,\n    GaussianSpatialModel,\n    PointSpatialModel,\n    ShellSpatialModel,\n    TemplateSpatialModel,\n)\nfrom gammapy.utils.testing import mpl_plot_check, requires_data, requires_dependency\n\n\ndef test_sky_point_source():\n    geom = WcsGeom.create(skydir=(2.4, 2.3), npix=(10, 10), binsz=0.3)\n    model = PointSpatialModel(lon_0=""2.5 deg"", lat_0=""2.5 deg"", frame=""icrs"")\n\n    assert model.evaluation_radius.unit == ""deg""\n    assert_allclose(model.evaluation_radius.value, 0)\n\n    assert model.frame == ""icrs""\n\n    assert_allclose(model.position.ra.deg, 2.5)\n    assert_allclose(model.position.dec.deg, 2.5)\n\n    val = model.evaluate_geom(geom)\n    assert val.unit == ""sr-1""\n    assert_allclose(np.sum(val * geom.solid_angle()), 1)\n\n    assert isinstance(model.to_region(), PointSkyRegion)\n\n\ndef test_sky_gaussian():\n    # Test symmetric model\n    sigma = 1 * u.deg\n    model = GaussianSpatialModel(lon_0=""5 deg"", lat_0=""15 deg"", sigma=sigma)\n    assert model.parameters[""sigma""].min == 0\n    val_0 = model(5 * u.deg, 15 * u.deg)\n    val_sigma = model(5 * u.deg, 16 * u.deg)\n    assert val_0.unit == ""sr-1""\n    ratio = val_0 / val_sigma\n    assert_allclose(ratio, np.exp(0.5))\n    radius = model.evaluation_radius\n    assert radius.unit == ""deg""\n    assert_allclose(radius.value, 5 * sigma.value)\n\n    # test the normalization for an elongated Gaussian near the Galactic Plane\n    m_geom_1 = WcsGeom.create(\n        binsz=0.05, width=(20, 20), skydir=(2, 2), frame=""galactic"", proj=""AIT""\n    )\n    coords = m_geom_1.get_coord()\n    solid_angle = m_geom_1.solid_angle()\n    lon = coords.lon\n    lat = coords.lat\n    sigma = 3 * u.deg\n    model_1 = GaussianSpatialModel(\n        lon_0=2 * u.deg, lat_0=2 * u.deg, sigma=sigma, e=0.8, phi=30 * u.deg\n    )\n    vals_1 = model_1(lon, lat)\n    assert vals_1.unit == ""sr-1""\n    assert_allclose(np.sum(vals_1 * solid_angle), 1, rtol=1.0e-3)\n\n    radius = model_1.evaluation_radius\n    assert radius.unit == ""deg""\n    assert_allclose(radius.value, 5 * sigma.value)\n\n    # check the ratio between the value at the peak and on the 1-sigma isocontour\n    sigma = 4 * u.deg\n    semi_minor = 2 * u.deg\n    e = np.sqrt(1 - (semi_minor / sigma) ** 2)\n    model_2 = GaussianSpatialModel(\n        lon_0=0 * u.deg, lat_0=0 * u.deg, sigma=sigma, e=e, phi=0 * u.deg\n    )\n    val_0 = model_2(0 * u.deg, 0 * u.deg)\n    val_major = model_2(0 * u.deg, 4 * u.deg)\n    val_minor = model_2(2 * u.deg, 0 * u.deg)\n    assert val_0.unit == ""sr-1""\n    ratio_major = val_0 / val_major\n    ratio_minor = val_0 / val_minor\n\n    assert_allclose(ratio_major, np.exp(0.5))\n    assert_allclose(ratio_minor, np.exp(0.5))\n\n    # check the rotation\n    model_3 = GaussianSpatialModel(\n        lon_0=0 * u.deg, lat_0=0 * u.deg, sigma=sigma, e=e, phi=90 * u.deg\n    )\n    val_minor_rotated = model_3(0 * u.deg, 2 * u.deg)\n    ratio_minor_rotated = val_0 / val_minor_rotated\n    assert_allclose(ratio_minor_rotated, np.exp(0.5))\n\n    assert isinstance(model.to_region(), EllipseSkyRegion)\n\n\ndef test_sky_disk():\n    # Test the disk case (e=0)\n    r_0 = 2 * u.deg\n    model = DiskSpatialModel(lon_0=""1 deg"", lat_0=""45 deg"", r_0=r_0)\n    lon = [1, 5, 359] * u.deg\n    lat = 46 * u.deg\n    val = model(lon, lat)\n    assert val.unit == ""sr-1""\n    desired = [261.263956, 0, 261.263956]\n    assert_allclose(val.value, desired)\n    radius = model.evaluation_radius\n    assert radius.unit == ""deg""\n    assert_allclose(radius.value, r_0.value)\n\n    # test the normalization for an elongated ellipse near the Galactic Plane\n    m_geom_1 = WcsGeom.create(\n        binsz=0.015, width=(20, 20), skydir=(2, 2), frame=""galactic"", proj=""AIT""\n    )\n    coords = m_geom_1.get_coord()\n    solid_angle = m_geom_1.solid_angle()\n    lon = coords.lon\n    lat = coords.lat\n    r_0 = 10 * u.deg\n    model_1 = DiskSpatialModel(\n        lon_0=2 * u.deg, lat_0=2 * u.deg, r_0=r_0, e=0.4, phi=30 * u.deg\n    )\n    vals_1 = model_1(lon, lat)\n    assert vals_1.unit == ""sr-1""\n    assert_allclose(np.sum(vals_1 * solid_angle), 1, rtol=1.0e-3)\n\n    radius = model_1.evaluation_radius\n    assert radius.unit == ""deg""\n    assert_allclose(radius.value, r_0.value)\n    # test rotation\n    r_0 = 2 * u.deg\n    semi_minor = 1 * u.deg\n    eccentricity = np.sqrt(1 - (semi_minor / r_0) ** 2)\n    model_rot_test = DiskSpatialModel(\n        lon_0=0 * u.deg, lat_0=0 * u.deg, r_0=r_0, e=eccentricity, phi=90 * u.deg\n    )\n    assert_allclose(model_rot_test(0 * u.deg, 1.5 * u.deg).value, 0)\n\n    # test the normalization for a disk (ellipse with e=0) at the Galactic Pole\n    m_geom_2 = WcsGeom.create(\n        binsz=0.1, width=(6, 6), skydir=(0, 90), frame=""galactic"", proj=""AIT""\n    )\n    coords = m_geom_2.get_coord()\n    lon = coords.lon\n    lat = coords.lat\n\n    r_0 = 5 * u.deg\n    disk = DiskSpatialModel(lon_0=0 * u.deg, lat_0=90 * u.deg, r_0=r_0)\n    vals_disk = disk(lon, lat)\n\n    solid_angle = 2 * np.pi * (1 - np.cos(5 * u.deg))\n    assert_allclose(np.max(vals_disk).value * solid_angle, 1)\n\n    assert isinstance(model.to_region(), EllipseSkyRegion)\n\n\ndef test_sky_disk_edge():\n    r_0 = 2 * u.deg\n    model = DiskSpatialModel(lon_0=""0 deg"", lat_0=""0 deg"", r_0=r_0, e=0.5, phi=""0 deg"")\n    value_center = model(0 * u.deg, 0 * u.deg)\n    value_edge = model(0 * u.deg, r_0)\n    assert_allclose((value_edge / value_center).to_value(""""), 0.5)\n\n    edge = model.edge.quantity\n    value_edge_pwidth = model(0 * u.deg, r_0 + edge / 2)\n    assert_allclose((value_edge_pwidth / value_center).to_value(""""), 0.05)\n\n    value_edge_nwidth = model(0 * u.deg, r_0 - edge / 2)\n    assert_allclose((value_edge_nwidth / value_center).to_value(""""), 0.95)\n\n\ndef test_sky_shell():\n    width = 2 * u.deg\n    rad = 2 * u.deg\n    model = ShellSpatialModel(lon_0=""1 deg"", lat_0=""45 deg"", radius=rad, width=width)\n    lon = [1, 2, 4] * u.deg\n    lat = 45 * u.deg\n    val = model(lon, lat)\n    assert val.unit == ""deg-2""\n    desired = [55.979449, 57.831651, 94.919895]\n    assert_allclose(val.to_value(""sr-1""), desired)\n    radius = model.evaluation_radius\n    assert radius.unit == ""deg""\n    assert_allclose(radius.value, rad.value + width.value)\n    assert isinstance(model.to_region(), CircleAnnulusSkyRegion)\n\n\ndef test_sky_diffuse_constant():\n    model = ConstantSpatialModel(value=""42 sr-1"")\n    lon = [1, 2] * u.deg\n    lat = 45 * u.deg\n    val = model(lon, lat)\n    assert val.unit == ""sr-1""\n    assert_allclose(val.value, 42)\n    radius = model.evaluation_radius\n    assert radius is None\n    assert isinstance(model.to_region(), EllipseSkyRegion)\n\n\n@requires_data()\ndef test_sky_diffuse_map():\n    filename = ""$GAMMAPY_DATA/catalogs/fermi/Extended_archive_v18/Templates/RXJ1713_2016_250GeV.fits""\n    model = TemplateSpatialModel.read(filename, normalize=False)\n    lon = [258.5, 0] * u.deg\n    lat = -39.8 * u.deg\n    val = model(lon, lat)\n    assert val.unit == ""sr-1""\n    desired = [3269.178107, 0]\n    assert_allclose(val.value, desired)\n    radius = model.evaluation_radius\n    assert radius.unit == ""deg""\n    assert_allclose(radius.value, 0.64, rtol=1.0e-2)\n    assert model.frame == ""fk5""\n    assert isinstance(model.to_region(), PolygonSkyRegion)\n\n\n@requires_data()\ndef test_sky_diffuse_map_normalize():\n    # define model map with a constant value of 1\n    model_map = Map.create(map_type=""wcs"", width=(10, 5), binsz=0.5)\n    model_map.data += 1.0\n    model_map.unit = ""sr-1""\n    model = TemplateSpatialModel(model_map)\n\n    # define data map with a different spatial binning\n    data_map = Map.create(map_type=""wcs"", width=(10, 5), binsz=1)\n    coords = data_map.geom.get_coord()\n    solid_angle = data_map.geom.solid_angle()\n    vals = model(coords.lon, coords.lat) * solid_angle\n\n    assert vals.unit == """"\n    integral = vals.sum()\n    assert_allclose(integral.value, 1, rtol=1e-4)\n\n\ndef test_evaluate_on_fk5_map():\n    # Check if spatial model can be evaluated on a map with FK5 frame\n    # Regression test for GH-2402\n    header = {}\n    header[""CDELT1""] = 1.0\n    header[""CDELT2""] = 1.0\n    header[""CTYPE1""] = ""RA---TAN""\n    header[""CTYPE2""] = ""DEC--TAN""\n    header[""RADESYS""] = ""FK5""\n    header[""CRVAL1""] = 0\n    header[""CRVAL2""] = 0\n    header[""CRPIX1""] = 5\n    header[""CRPIX2""] = 5\n\n    wcs = WCS(header)\n    geom = WcsGeom(wcs, npix=(10, 10))\n    model = GaussianSpatialModel(lon_0=""0 deg"", lat_0=""0 deg"", sigma=""1 deg"")\n    data = model.evaluate_geom(geom)\n    assert data.sum() > 0\n\n\ndef test_evaluate_fk5_model():\n    geom = WcsGeom.create(width=(5, 5), binsz=0.1, frame=""icrs"")\n    model = GaussianSpatialModel(\n        lon_0=""0 deg"", lat_0=""0 deg"", sigma=""0.1 deg"", frame=""fk5""\n    )\n    data = model.evaluate_geom(geom)\n    assert data.sum() > 0\n\n\n@requires_dependency(""matplotlib"")\ndef test_spatial_model_plot():\n    model = PointSpatialModel()\n    model.covariance = np.diag([0.01, 0.01])\n\n    with mpl_plot_check():\n        ax = model.plot()\n\n    with mpl_plot_check():\n        model.plot_error(ax=ax)\n'"
gammapy/modeling/models/tests/test_spectral.py,8,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom gammapy.maps import MapAxis\nfrom gammapy.modeling.models import (\n    SPECTRAL_MODELS,\n    AbsorbedSpectralModel,\n    Absorption,\n    ConstantSpectralModel,\n    ExpCutoffPowerLaw3FGLSpectralModel,\n    ExpCutoffPowerLawSpectralModel,\n    GaussianSpectralModel,\n    LogParabolaSpectralModel,\n    NaimaSpectralModel,\n    PowerLaw2SpectralModel,\n    PowerLawSpectralModel,\n    SmoothBrokenPowerLawSpectralModel,\n    SuperExpCutoffPowerLaw4FGLSpectralModel,\n    TemplateSpectralModel,\n)\nfrom gammapy.utils.testing import (\n    assert_quantity_allclose,\n    mpl_plot_check,\n    requires_data,\n    requires_dependency,\n)\n\n\ndef table_model():\n    energy = MapAxis.from_energy_bounds(0.1 * u.TeV, 100 * u.TeV, 1000).center\n\n    model = PowerLawSpectralModel(\n        index=2.3, amplitude=""4 cm-2 s-1 TeV-1"", reference=""1 TeV""\n    )\n    dnde = model(energy)\n\n    return TemplateSpectralModel(energy, dnde, 1)\n\n\nTEST_MODELS = [\n    dict(\n        name=""powerlaw"",\n        model=PowerLawSpectralModel(\n            index=2.3 * u.Unit(""""),\n            amplitude=4 / u.cm ** 2 / u.s / u.TeV,\n            reference=1 * u.TeV,\n        ),\n        val_at_2TeV=u.Quantity(4 * 2.0 ** (-2.3), ""cm-2 s-1 TeV-1""),\n        integral_1_10TeV=u.Quantity(2.9227116204223784, ""cm-2 s-1""),\n        eflux_1_10TeV=u.Quantity(6.650836884969039, ""TeV cm-2 s-1""),\n    ),\n    dict(\n        name=""powerlaw"",\n        model=PowerLawSpectralModel(\n            index=2 * u.Unit(""""),\n            amplitude=4 / u.cm ** 2 / u.s / u.TeV,\n            reference=1 * u.TeV,\n        ),\n        val_at_2TeV=u.Quantity(1.0, ""cm-2 s-1 TeV-1""),\n        integral_1_10TeV=u.Quantity(3.6, ""cm-2 s-1""),\n        eflux_1_10TeV=u.Quantity(9.210340371976184, ""TeV cm-2 s-1""),\n    ),\n    dict(\n        name=""powerlaw2"",\n        model=PowerLaw2SpectralModel(\n            amplitude=u.Quantity(2.9227116204223784, ""cm-2 s-1""),\n            index=2.3 * u.Unit(""""),\n            emin=1 * u.TeV,\n            emax=10 * u.TeV,\n        ),\n        val_at_2TeV=u.Quantity(4 * 2.0 ** (-2.3), ""cm-2 s-1 TeV-1""),\n        integral_1_10TeV=u.Quantity(2.9227116204223784, ""cm-2 s-1""),\n        eflux_1_10TeV=u.Quantity(6.650836884969039, ""TeV cm-2 s-1""),\n    ),\n    dict(\n        name=""ecpl"",\n        model=ExpCutoffPowerLawSpectralModel(\n            index=1.6 * u.Unit(""""),\n            amplitude=4 / u.cm ** 2 / u.s / u.TeV,\n            reference=1 * u.TeV,\n            lambda_=0.1 / u.TeV,\n        ),\n        val_at_2TeV=u.Quantity(1.080321705479446, ""cm-2 s-1 TeV-1""),\n        integral_1_10TeV=u.Quantity(3.765838739678921, ""cm-2 s-1""),\n        eflux_1_10TeV=u.Quantity(9.901735870666526, ""TeV cm-2 s-1""),\n        e_peak=4 * u.TeV,\n    ),\n    dict(\n        name=""ecpl_3fgl"",\n        model=ExpCutoffPowerLaw3FGLSpectralModel(\n            index=2.3 * u.Unit(""""),\n            amplitude=4 / u.cm ** 2 / u.s / u.TeV,\n            reference=1 * u.TeV,\n            ecut=10 * u.TeV,\n        ),\n        val_at_2TeV=u.Quantity(0.7349563611124971, ""cm-2 s-1 TeV-1""),\n        integral_1_10TeV=u.Quantity(2.6034046173089, ""cm-2 s-1""),\n        eflux_1_10TeV=u.Quantity(5.340285560055799, ""TeV cm-2 s-1""),\n    ),\n    dict(\n        name=""plsec_4fgl"",\n        model=SuperExpCutoffPowerLaw4FGLSpectralModel(\n            index_1=1.5,\n            index_2=2,\n            amplitude=1 / u.cm ** 2 / u.s / u.TeV,\n            reference=1 * u.TeV,\n            expfactor=1e-2,\n        ),\n        val_at_2TeV=u.Quantity(0.3431043087721737, ""cm-2 s-1 TeV-1""),\n        integral_1_10TeV=u.Quantity(1.2125247, ""cm-2 s-1""),\n        eflux_1_10TeV=u.Quantity(3.38072082, ""TeV cm-2 s-1""),\n    ),\n    dict(\n        name=""logpar"",\n        model=LogParabolaSpectralModel(\n            alpha=2.3 * u.Unit(""""),\n            amplitude=4 / u.cm ** 2 / u.s / u.TeV,\n            reference=1 * u.TeV,\n            beta=0.5 * u.Unit(""""),\n        ),\n        val_at_2TeV=u.Quantity(0.6387956571420305, ""cm-2 s-1 TeV-1""),\n        integral_1_10TeV=u.Quantity(2.255689748270628, ""cm-2 s-1""),\n        eflux_1_10TeV=u.Quantity(3.9586515834989267, ""TeV cm-2 s-1""),\n        e_peak=0.74082 * u.TeV,\n    ),\n    dict(\n        name=""logpar10"",\n        model=LogParabolaSpectralModel.from_log10(\n            alpha=2.3 * u.Unit(""""),\n            amplitude=4 / u.cm ** 2 / u.s / u.TeV,\n            reference=1 * u.TeV,\n            beta=1.151292546497023 * u.Unit(""""),\n        ),\n        val_at_2TeV=u.Quantity(0.6387956571420305, ""cm-2 s-1 TeV-1""),\n        integral_1_10TeV=u.Quantity(2.255689748270628, ""cm-2 s-1""),\n        eflux_1_10TeV=u.Quantity(3.9586515834989267, ""TeV cm-2 s-1""),\n        e_peak=0.74082 * u.TeV,\n    ),\n    dict(\n        name=""constant"",\n        model=ConstantSpectralModel(const=4 / u.cm ** 2 / u.s / u.TeV),\n        val_at_2TeV=u.Quantity(4, ""cm-2 s-1 TeV-1""),\n        integral_1_10TeV=u.Quantity(35.9999999999999, ""cm-2 s-1""),\n        eflux_1_10TeV=u.Quantity(198.00000000000006, ""TeV cm-2 s-1""),\n    ),\n    dict(\n        name=""powerlaw_index1"",\n        model=PowerLawSpectralModel(\n            index=1 * u.Unit(""""),\n            amplitude=2 / u.cm ** 2 / u.s / u.TeV,\n            reference=1 * u.TeV,\n        ),\n        val_at_2TeV=u.Quantity(1.0, ""cm-2 s-1 TeV-1""),\n        integral_1_10TeV=u.Quantity(4.605170185, ""cm-2 s-1""),\n        eflux_1_10TeV=u.Quantity(18.0, ""TeV cm-2 s-1""),\n    ),\n    dict(\n        name=""ecpl_2"",\n        model=ExpCutoffPowerLawSpectralModel(\n            index=2.0 * u.Unit(""""),\n            amplitude=4 / u.cm ** 2 / u.s / u.TeV,\n            reference=1 * u.TeV,\n            lambda_=0.1 / u.TeV,\n        ),\n        val_at_2TeV=u.Quantity(0.81873075, ""cm-2 s-1 TeV-1""),\n        integral_1_10TeV=u.Quantity(2.83075297, ""cm-2 s-1""),\n        eflux_1_10TeV=u.Quantity(6.41406327, ""TeV cm-2 s-1""),\n        e_peak=np.nan * u.TeV,\n    ),\n    dict(\n        name=""GaussianSpectralModel"",\n        model=GaussianSpectralModel(\n            norm=4 / u.cm ** 2 / u.s, mean=2 * u.TeV, sigma=0.2 * u.TeV\n        ),\n        val_at_2TeV=u.Quantity(7.978845608028654, ""cm-2 s-1 TeV-1""),\n        val_at_3TeV=u.Quantity(2.973439029468601e-05, ""cm-2 s-1 TeV-1""),\n        integral_1_10TeV=u.Quantity(3.9999988533937123, ""cm-2 s-1""),\n        integral_infinity=u.Quantity(4, ""cm-2 s-1""),\n        eflux_1_10TeV=u.Quantity(7.999998896163037, ""TeV cm-2 s-1""),\n    ),\n    dict(\n        name=""ecpl"",\n        model=ExpCutoffPowerLawSpectralModel(\n            index=1.8 * u.Unit(""""),\n            amplitude=4 / u.cm ** 2 / u.s / u.TeV,\n            reference=1 * u.TeV,\n            lambda_=0.1 / u.TeV,\n            alpha=0.8,\n        ),\n        val_at_2TeV=u.Quantity(0.871694294554192, ""cm-2 s-1 TeV-1""),\n        integral_1_10TeV=u.Quantity(3.026342, ""cm-2 s-1""),\n        eflux_1_10TeV=u.Quantity(7.38652453, ""TeV cm-2 s-1""),\n        e_peak=1.7677669529663684 * u.TeV,\n    ),\n    dict(\n        name=""sbpl"",\n        model=SmoothBrokenPowerLawSpectralModel(\n            index1=1.5 * u.Unit(""""),\n            index2=2.5 * u.Unit(""""),\n            amplitude=4 / u.cm ** 2 / u.s / u.TeV,\n            ebreak=0.5 * u.TeV,\n            reference=1 * u.TeV,\n            beta=1,\n        ),\n        val_at_2TeV=u.Quantity(0.28284271247461906, ""cm-2 s-1 TeV-1""),\n        integral_1_10TeV=u.Quantity(0.9956923907948155, ""cm-2 s-1""),\n        eflux_1_10TeV=u.Quantity(2.2372256145972207, ""TeV cm-2 s-1""),\n    ),\n    dict(\n        name=""sbpl-hard"",\n        model=SmoothBrokenPowerLawSpectralModel(\n            index1=2.5 * u.Unit(""""),\n            index2=1.5 * u.Unit(""""),\n            amplitude=4 / u.cm ** 2 / u.s / u.TeV,\n            ebreak=0.5 * u.TeV,\n            reference=1 * u.TeV,\n            beta=1,\n        ),\n        val_at_2TeV=u.Quantity(3.5355339059327378, ""cm-2 s-1 TeV-1""),\n        integral_1_10TeV=u.Quantity(13.522782989735022, ""cm-2 s-1""),\n        eflux_1_10TeV=u.Quantity(40.06681812966845, ""TeV cm-2 s-1""),\n    ),\n]\n\n# Add compound models\n\nTEST_MODELS.append(\n    dict(\n        name=""compound3"",\n        model=TEST_MODELS[0][""model""] + TEST_MODELS[0][""model""],\n        val_at_2TeV=TEST_MODELS[0][""val_at_2TeV""] * 2,\n        integral_1_10TeV=TEST_MODELS[0][""integral_1_10TeV""] * 2,\n        eflux_1_10TeV=TEST_MODELS[0][""eflux_1_10TeV""] * 2,\n    )\n)\n\nTEST_MODELS.append(\n    dict(\n        name=""compound6"",\n        model=TEST_MODELS[8][""model""] + u.Quantity(4, ""cm-2 s-1 TeV-1""),\n        val_at_2TeV=TEST_MODELS[8][""val_at_2TeV""] * 2,\n        integral_1_10TeV=TEST_MODELS[8][""integral_1_10TeV""] * 2,\n        eflux_1_10TeV=TEST_MODELS[8][""eflux_1_10TeV""] * 2,\n    )\n)\n\nTEST_MODELS.append(\n    dict(\n        name=""table_model"",\n        model=table_model(),\n        # Values took from power law expectation\n        val_at_2TeV=u.Quantity(4 * 2.0 ** (-2.3), ""cm-2 s-1 TeV-1""),\n        integral_1_10TeV=u.Quantity(2.9227116204223784, ""cm-2 s-1""),\n        eflux_1_10TeV=u.Quantity(6.650836884969039, ""TeV cm-2 s-1""),\n    )\n)\n\n\n@requires_dependency(""scipy"")\n@pytest.mark.parametrize(""spectrum"", TEST_MODELS, ids=lambda _: _[""name""])\ndef test_models(spectrum):\n    model = spectrum[""model""]\n    energy = 2 * u.TeV\n    value = model(energy)\n    assert_quantity_allclose(value, spectrum[""val_at_2TeV""], rtol=1e-7)\n    if ""val_at_3TeV"" in spectrum:\n        energy = 3 * u.TeV\n        value = model(energy)\n        assert_quantity_allclose(value, spectrum[""val_at_3TeV""], rtol=1e-7)\n\n    emin = 1 * u.TeV\n    emax = 10 * u.TeV\n    assert_quantity_allclose(\n        model.integral(emin=emin, emax=emax), spectrum[""integral_1_10TeV""], rtol=1e-5\n    )\n    assert_quantity_allclose(\n        model.energy_flux(emin=emin, emax=emax), spectrum[""eflux_1_10TeV""], rtol=1e-5\n    )\n\n    if ""e_peak"" in spectrum:\n        assert_quantity_allclose(model.e_peak, spectrum[""e_peak""], rtol=1e-2)\n\n    # inverse for ConstantSpectralModel is irrelevant.\n    # inverse for Gaussian has a degeneracy\n    if not (\n        isinstance(model, ConstantSpectralModel)\n        or spectrum[""name""] == ""compound6""\n        or spectrum[""name""] == ""GaussianSpectralModel""\n    ):\n        assert_quantity_allclose(model.inverse(value), 2 * u.TeV, rtol=0.01)\n\n    if ""integral_infinity"" in spectrum:\n        emin = 0 * u.TeV\n        emax = 10000 * u.TeV\n        assert_quantity_allclose(\n            model.integral(emin=emin, emax=emax), spectrum[""integral_infinity""]\n        )\n\n    model.to_dict()\n\n    assert """" in str(model)\n\n    # check that an array evaluation works (otherwise e.g. plotting raises an error)\n    e_array = [2, 10, 20] * u.TeV\n    e_array = e_array[:, np.newaxis, np.newaxis]\n    val = model(e_array)\n    assert val.shape == e_array.shape\n    assert_quantity_allclose(val[0], spectrum[""val_at_2TeV""])\n\n\ndef test_model_unit():\n    pwl = PowerLawSpectralModel()\n    value = pwl(500 * u.MeV)\n    assert value.unit == ""cm-2 s-1 TeV-1""\n\n\n@requires_dependency(""matplotlib"")\ndef test_model_plot():\n    pwl = PowerLawSpectralModel(\n        amplitude=1e-12 * u.Unit(""TeV-1 cm-2 s-1""), reference=1 * u.Unit(""TeV""), index=2\n    )\n    pwl.amplitude.error = 0.1e-12 * u.Unit(""TeV-1 cm-2 s-1"")\n\n    with mpl_plot_check():\n        pwl.plot((1 * u.TeV, 10 * u.TeV))\n\n    with mpl_plot_check():\n        pwl.plot_error((1 * u.TeV, 10 * u.TeV))\n\n\ndef test_to_from_dict():\n    spectrum = TEST_MODELS[0]\n    model = spectrum[""model""]\n\n    model_dict = model.to_dict()\n    model_class = SPECTRAL_MODELS.get_cls(model_dict[""type""])\n    new_model = model_class.from_dict(model_dict)\n\n    assert isinstance(new_model, PowerLawSpectralModel)\n\n    actual = [par.value for par in new_model.parameters]\n    desired = [par.value for par in model.parameters]\n    assert_quantity_allclose(actual, desired)\n\n\n@requires_dependency(""matplotlib"")\n@requires_data()\ndef test_table_model_from_file():\n    filename = ""$GAMMAPY_DATA/ebl/ebl_franceschini.fits.gz""\n    absorption_z03 = TemplateSpectralModel.read_xspec_model(\n        filename=filename, param=0.3\n    )\n    with mpl_plot_check():\n        absorption_z03.plot(energy_range=(0.03, 10), energy_unit=u.TeV, flux_unit="""")\n\n\n@requires_data()\ndef test_absorption():\n    # absorption values for given redshift\n    redshift = 0.117\n    absorption = Absorption.read_builtin(""dominguez"")\n\n    # Spectral model corresponding to PKS 2155-304 (quiescent state)\n    index = 3.53\n    amplitude = 1.81 * 1e-12 * u.Unit(""cm-2 s-1 TeV-1"")\n    reference = 1 * u.TeV\n    pwl = PowerLawSpectralModel(index=index, amplitude=amplitude, reference=reference)\n\n    # EBL + PWL model\n    model = AbsorbedSpectralModel(\n        spectral_model=pwl, absorption=absorption, redshift=redshift\n    )\n\n    desired = u.Quantity(5.140765e-13, ""TeV-1 s-1 cm-2"")\n    assert_quantity_allclose(model(1 * u.TeV), desired, rtol=1e-3)\n    assert model.alpha_norm.value == 1.0\n\n    # EBL + PWL model: test if norm of EBL=0: it mean model =pwl\n    model = AbsorbedSpectralModel(\n        spectral_model=pwl, absorption=absorption, alpha_norm=0, redshift=redshift\n    )\n    assert_quantity_allclose(model(1 * u.TeV), pwl(1 * u.TeV), rtol=1e-3)\n\n    # EBL + PWL model: Test with a norm different of 1\n    model = AbsorbedSpectralModel(\n        spectral_model=pwl, absorption=absorption, alpha_norm=1.5, redshift=redshift\n    )\n    desired = u.Quantity(2.739695e-13, ""TeV-1 s-1 cm-2"")\n    assert model.alpha_norm.value == 1.5\n    assert_quantity_allclose(model(1 * u.TeV), desired, rtol=1e-3)\n\n    # Test error propagation\n    model.spectral_model.amplitude.error = 0.1 * model.spectral_model.amplitude.value\n    dnde, dnde_err = model.evaluate_error(1 * u.TeV)\n    assert_allclose(dnde_err / dnde, 0.1)\n\n\n@requires_data()\ndef test_absorbed_extrapolate():\n    ebl_model = ""dominguez""\n    z = 0.001\n    absorption = Absorption.read_builtin(ebl_model)\n\n    model = absorption.table_model(z)\n    assert_allclose(model(1 * u.TeV), 1)\n\n\ndef test_ecpl_integrate():\n    # regression test to check the numerical integration for small energy bins\n    ecpl = ExpCutoffPowerLawSpectralModel()\n    value = ecpl.integral(1 * u.TeV, 1.1 * u.TeV)\n    assert_quantity_allclose(value, 8.380761e-14 * u.Unit(""s-1 cm-2""))\n\n\ndef test_pwl_pivot_energy():\n    pwl = PowerLawSpectralModel(amplitude=""5.35510540e-11 cm-2 s-1 TeV-1"")\n\n    pwl.covariance = [\n        [0.0318377 ** 2, 6.56889442e-14, 0],\n        [6.56889442e-14, 0, 0],\n        [0, 0, 0],\n    ]\n\n    assert_quantity_allclose(pwl.pivot_energy, 3.3540034240210987 * u.TeV)\n\n\ndef test_TemplateSpectralModel_evaluate_tiny():\n    energy = np.array([1.00000000e06, 1.25892541e06, 1.58489319e06, 1.99526231e06])\n    values = np.array([4.39150790e-38, 1.96639562e-38, 8.80497507e-39, 3.94262401e-39])\n\n    model = TemplateSpectralModel(\n        energy=energy, values=values * u.Unit(""MeV-1 s-1 sr-1"")\n    )\n    result = model.evaluate(energy, norm=1.0, tilt=0.0, reference=1 * u.TeV)\n    tiny = np.finfo(np.float32).tiny\n    mask = abs(values) - tiny > tiny\n    np.testing.assert_allclose(\n        values[mask] / values.max(), result[mask].value / values.max()\n    )\n    mask = abs(result.value) - tiny <= tiny\n    assert np.all(result[mask] == 0.0)\n\n\n@requires_dependency(""naima"")\nclass TestNaimaModel:\n    # Used to test model value at 2 TeV\n    energy = 2 * u.TeV\n\n    # Used to test model integral and energy flux\n    emin = 1 * u.TeV\n    emax = 10 * u.TeV\n\n    # Used to that if array evaluation works\n    e_array = [2, 10, 20] * u.TeV\n    e_array = e_array[:, np.newaxis, np.newaxis]\n\n    def test_pion_decay(self):\n        import naima\n\n        particle_distribution = naima.models.PowerLaw(\n            amplitude=2e33 / u.eV, e_0=10 * u.TeV, alpha=2.5\n        )\n        radiative_model = naima.radiative.PionDecay(\n            particle_distribution, nh=1 * u.cm ** -3\n        )\n        model = NaimaSpectralModel(radiative_model)\n\n        val_at_2TeV = 9.725347355450884e-14 * u.Unit(""cm-2 s-1 TeV-1"")\n        integral_1_10TeV = 3.530537143620737e-13 * u.Unit(""cm-2 s-1"")\n        eflux_1_10TeV = 7.643559573105779e-13 * u.Unit(""TeV cm-2 s-1"")\n\n        value = model(self.energy)\n        assert_quantity_allclose(value, val_at_2TeV)\n        assert_quantity_allclose(\n            model.integral(emin=self.emin, emax=self.emax), integral_1_10TeV\n        )\n        assert_quantity_allclose(\n            model.energy_flux(emin=self.emin, emax=self.emax), eflux_1_10TeV\n        )\n        val = model(self.e_array)\n        assert val.shape == self.e_array.shape\n\n        model.amplitude.error = 0.1 * model.amplitude.value\n\n        out = model.evaluate_error(1 * u.TeV)\n        assert_allclose(out.data, [5.266068e-13, 5.266068e-14], rtol=1e-3)\n\n    def test_ic(self):\n        import naima\n\n        particle_distribution = naima.models.ExponentialCutoffBrokenPowerLaw(\n            amplitude=2e33 / u.eV,\n            e_0=10 * u.TeV,\n            alpha_1=2.5,\n            alpha_2=2.7,\n            e_break=900 * u.GeV,\n            e_cutoff=10 * u.TeV,\n        )\n        radiative_model = naima.radiative.InverseCompton(\n            particle_distribution, seed_photon_fields=[""CMB""]\n        )\n\n        model = NaimaSpectralModel(radiative_model)\n\n        val_at_2TeV = 4.347836316893546e-12 * u.Unit(""cm-2 s-1 TeV-1"")\n        integral_1_10TeV = 1.595813e-11 * u.Unit(""cm-2 s-1"")\n        eflux_1_10TeV = 2.851283e-11 * u.Unit(""TeV cm-2 s-1"")\n\n        value = model(self.energy)\n        assert_quantity_allclose(value, val_at_2TeV)\n        assert_quantity_allclose(\n            model.integral(emin=self.emin, emax=self.emax), integral_1_10TeV, rtol=1e-5\n        )\n        assert_quantity_allclose(\n            model.energy_flux(emin=self.emin, emax=self.emax), eflux_1_10TeV, rtol=1e-5\n        )\n        val = model(self.e_array)\n        assert val.shape == self.e_array.shape\n\n    def test_synchrotron(self):\n        import naima\n\n        particle_distribution = naima.models.LogParabola(\n            amplitude=2e33 / u.eV, e_0=10 * u.TeV, alpha=1.3, beta=0.5\n        )\n        radiative_model = naima.radiative.Synchrotron(particle_distribution, B=2 * u.G)\n\n        model = NaimaSpectralModel(radiative_model)\n\n        val_at_2TeV = 1.0565840392550432e-24 * u.Unit(""cm-2 s-1 TeV-1"")\n        integral_1_10TeV = 4.449303e-13 * u.Unit(""cm-2 s-1"")\n        eflux_1_10TeV = 4.594242e-13 * u.Unit(""TeV cm-2 s-1"")\n\n        value = model(self.energy)\n        assert_quantity_allclose(value, val_at_2TeV)\n        assert_quantity_allclose(\n            model.integral(emin=self.emin, emax=self.emax), integral_1_10TeV, rtol=1e-5\n        )\n        assert_quantity_allclose(\n            model.energy_flux(emin=self.emin, emax=self.emax), eflux_1_10TeV, rtol=1e-5\n        )\n        val = model(self.e_array)\n        assert val.shape == self.e_array.shape\n\n        model.B.value = 3  # update B\n        val_at_2TeV = 5.1985064062296e-16 * u.Unit(""cm-2 s-1 TeV-1"")\n        value = model(self.energy)\n        assert_quantity_allclose(value, val_at_2TeV)\n\n    def test_ssc(self):\n        import naima\n\n        ECBPL = naima.models.ExponentialCutoffBrokenPowerLaw(\n            amplitude=3.699e36 / u.eV,\n            e_0=1 * u.TeV,\n            e_break=0.265 * u.TeV,\n            alpha_1=1.5,\n            alpha_2=3.233,\n            e_cutoff=1863 * u.TeV,\n            beta=2.0,\n        )\n\n        radiative_model = naima.radiative.InverseCompton(\n            ECBPL,\n            seed_photon_fields=[\n                ""CMB"",\n                [""FIR"", 70 * u.K, 0.5 * u.eV / u.cm ** 3],\n                [""NIR"", 5000 * u.K, 1 * u.eV / u.cm ** 3],\n            ],\n            Eemax=50 * u.PeV,\n            Eemin=0.1 * u.GeV,\n        )\n        B = 125 * u.uG\n        radius = 2.1 * u.pc\n        nested_models = {""SSC"": {""B"": B, ""radius"": radius}}\n        model = NaimaSpectralModel(radiative_model, nested_models=nested_models)\n        assert_quantity_allclose(model.B.quantity, B)\n        assert_quantity_allclose(model.radius.quantity, radius)\n        val_at_2TeV = 1.6703761561806372e-11 * u.Unit(""cm-2 s-1 TeV-1"")\n        value = model(self.energy)\n        assert_quantity_allclose(value, val_at_2TeV, rtol=1e-5)\n\n        model.parameters[""B""].value = 100\n        val_at_2TeV = 1.441331153167876e-11 * u.Unit(""cm-2 s-1 TeV-1"")\n        value = model(self.energy)\n        assert_quantity_allclose(value, val_at_2TeV, rtol=1e-5)\n\n    def test_bad_init(self):\n        import naima\n\n        particle_distribution = naima.models.PowerLaw(\n            amplitude=2e33 / u.eV, e_0=10 * u.TeV, alpha=2.5\n        )\n        radiative_model = naima.radiative.PionDecay(\n            particle_distribution, nh=1 * u.cm ** -3\n        )\n        model = NaimaSpectralModel(radiative_model)\n\n        with pytest.raises(NotImplementedError):\n            NaimaSpectralModel.from_dict(model.to_dict())\n        with pytest.raises(NotImplementedError):\n            NaimaSpectralModel.from_parameters(model.parameters)\n\n\nclass TestSpectralModelErrorPropagation:\n    """"""Test spectral model error propagation.\n\n    https://github.com/gammapy/gammapy/blob/master/docs/development/pigs/pig-014.rst#proposal\n    https://nbviewer.jupyter.org/github/gammapy/gammapy-extra/blob/master/experiments/uncertainty_estimation_prototype.ipynb\n    """"""\n\n    def setup(self):\n        self.model = LogParabolaSpectralModel(\n            amplitude=3.76e-11 * u.Unit(""cm-2 s-1 TeV-1""),\n            reference=1 * u.TeV,\n            alpha=2.44,\n            beta=0.25,\n        )\n        self.model.covariance = [\n            [1.31e-23, 0, -6.80e-14, 3.04e-13],\n            [0, 0, 0, 0],\n            [-6.80e-14, 0, 0.00899, 0.00904],\n            [3.04e-13, 0, 0.00904, 0.0284],\n        ]\n\n    def test_evaluate_error_scalar(self):\n        # evaluate_error on scalar\n        out = self.model.evaluate_error(1 * u.TeV)\n        assert isinstance(out, u.Quantity)\n        assert out.unit == ""cm-2 s-1 TeV-1""\n        assert out.shape == (2,)\n        assert_allclose(out.data, [3.7600e-11, 3.6193e-12], rtol=1e-3)\n\n    def test_evaluate_error_array(self):\n        out = self.model.evaluate_error([1, 100] * u.TeV)\n        assert out.shape == (2, 2)\n        expected = [[3.76e-11, 2.469e-18], [3.619e-12, 9.375e-18]]\n        assert_allclose(out.data, expected, rtol=1e-3)\n\n    def test_evaluate_error_unit(self):\n        out = self.model.evaluate_error(1e6 * u.MeV)\n        assert out.unit == ""cm-2 s-1 TeV-1""\n        assert_allclose(out.data, [3.760e-11, 3.6193e-12], rtol=1e-3)\n\n    @pytest.mark.xfail(reason=""FIXME, do we need this method?"")\n    def test_integral_error(self):\n        out = self.model.integral_error(1 * u.TeV, 10 * u.TeV)\n        assert out.unit == ""cm-2 s-1""\n        assert out.shape == (2,)\n        assert_allclose(out.data, [2.197e-11, 2.796e-12], rtol=1e-3)\n\n    @pytest.mark.xfail(reason=""FIXME, do we need this method?"")\n    def test_energy_flux_error(self):\n        out = self.model.energy_flux_error(1 * u.TeV, 10 * u.TeV)\n        assert out.unit == ""TeV cm-2 s-1""\n        assert out.shape == (2,)\n        assert_allclose(out.data, [4.119e-11, 8.157e-12], rtol=1e-3)\n\n    def test_ecpl_model(self):\n        # Regression test for ECPL model\n        # https://github.com/gammapy/gammapy/issues/2007\n        model = ExpCutoffPowerLawSpectralModel(\n            amplitude=2.076183759227292e-12 * u.Unit(""cm-2 s-1 TeV-1""),\n            index=1.8763343736076483,\n            lambda_=0.08703226432146616 * u.Unit(""TeV-1""),\n            reference=1 * u.TeV,\n        )\n        model.covariance = [\n            [0.00204191498, -1.507724e-14, 0.0, -0.001834819, 0.0],\n            [-1.507724e-14, 1.6864740e-25, 0.0, 1.854251e-14, 0.0],\n            [0.0, 0.0, 0.0, 0.0, 0.0],\n            [-0.001834819175, 1.8542517e-14, 0.0, 0.0032559101, 0.0],\n            [0.0, 0.0, 0.0, 0.0, 0.0],\n        ]\n\n        out = model.evaluate_error(1 * u.TeV)\n        assert_allclose(out.data, [1.903129e-12, 2.979976e-13], rtol=1e-3)\n\n        out = model.evaluate_error(0.1 * u.TeV)\n        assert_allclose(out.data, [1.548176e-10, 1.933612e-11], rtol=1e-3)\n'"
gammapy/modeling/models/tests/test_spectral_cosmic_ray.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom gammapy.modeling.models import create_cosmic_ray_spectral_model\n\nCOSMIC_RAY_SPECTRA = [\n    {""name"": ""proton"", ""dnde"": 1.856522e-05, ""flux"": 7.096247e-05, ""index"": 2.70},\n    {""name"": ""N"", ""dnde"": 1.449504e-05, ""flux"": 5.509215e-05, ""index"": 2.64},\n    {""name"": ""Si"", ""dnde"": 5.646618e-06, ""flux"": 2.149887e-05, ""index"": 2.66},\n    {""name"": ""Fe"", ""dnde"": 2.720231e-06, ""flux"": 1.03305e-05, ""index"": 2.63},\n    {""name"": ""electron"", ""dnde"": 1.013671e-08, ""flux"": 4.691975e-08, ""index"": 3.428318},\n]\n\n\n@pytest.mark.parametrize(""spec"", COSMIC_RAY_SPECTRA, ids=lambda _: _[""name""])\ndef test_cosmic_ray_spectrum(spec):\n    cr_spectrum = create_cosmic_ray_spectral_model(particle=spec[""name""])\n\n    dnde = cr_spectrum(2 * u.TeV)\n    assert_allclose(dnde.value, spec[""dnde""], rtol=1e-3)\n    assert dnde.unit == ""cm-2 s-1 TeV-1""\n\n    flux = cr_spectrum.integral(1 * u.TeV, 1e3 * u.TeV)\n    assert_allclose(flux.value, spec[""flux""], rtol=1e-3)\n    assert flux.unit == ""cm-2 s-1""\n\n    index = cr_spectrum.spectral_index(2 * u.TeV)\n    assert_allclose(index.value, spec[""index""], rtol=1e-3)\n    assert index.unit == """"\n'"
gammapy/modeling/models/tests/test_spectral_crab.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport astropy.units as u\nfrom gammapy.modeling.models import create_crab_spectral_model\nfrom gammapy.utils.testing import assert_quantity_allclose\n\nCRAB_SPECTRA = [\n    {\n        ""name"": ""meyer"",\n        ""dnde"": u.Quantity(5.572437502365652e-12, ""cm-2 s-1 TeV-1""),\n        ""flux"": u.Quantity(2.0744425607240974e-11, ""cm-2 s-1""),\n        ""index"": 2.631535530090332,\n    },\n    {\n        ""name"": ""hegra"",\n        ""dnde"": u.Quantity(4.60349681e-12, ""cm-2 s-1 TeV-1""),\n        ""flux"": u.Quantity(1.74688947e-11, ""cm-2 s-1""),\n        ""index"": 2.62000000,\n    },\n    {\n        ""name"": ""hess_pl"",\n        ""dnde"": u.Quantity(5.57327158e-12, ""cm-2 s-1 TeV-1""),\n        ""flux"": u.Quantity(2.11653715e-11, ""cm-2 s-1""),\n        ""index"": 2.63000000,\n    },\n    {\n        ""name"": ""hess_ecpl"",\n        ""dnde"": u.Quantity(6.23714253e-12, ""cm-2 s-1 TeV-1""),\n        ""flux"": u.Quantity(2.267957713046026e-11, ""cm-2 s-1""),\n        ""index"": 2.529860258102417,\n    },\n    {\n        ""name"": ""magic_lp"",\n        ""dnde"": u.Quantity(5.5451060834144166e-12, ""cm-2 s-1 TeV-1""),\n        ""flux"": u.Quantity(2.028222279117e-11, ""cm-2 s-1""),\n        ""index"": 2.614495440236207,\n    },\n    {\n        ""name"": ""magic_ecpl"",\n        ""dnde"": u.Quantity(5.88494595619e-12, ""cm-2 s-1 TeV-1""),\n        ""flux"": u.Quantity(2.070767119534948e-11, ""cm-2 s-1""),\n        ""index"": 2.5433349999859405,\n    },\n]\n\n\n@pytest.mark.parametrize(""spec"", CRAB_SPECTRA, ids=lambda _: _[""name""])\ndef test_crab_spectrum(spec):\n    crab_spectrum = create_crab_spectral_model(reference=spec[""name""])\n\n    dnde = crab_spectrum(2 * u.TeV)\n    assert_quantity_allclose(dnde, spec[""dnde""])\n\n    flux = crab_spectrum.integral(1 * u.TeV, 1e3 * u.TeV)\n    assert_quantity_allclose(flux, spec[""flux""], rtol=1e-6)\n\n    index = crab_spectrum.spectral_index(2 * u.TeV)\n    assert_quantity_allclose(index, spec[""index""], rtol=1e-5)\n\n\ndef test_invalid_format():\n    with pytest.raises(ValueError):\n        create_crab_spectral_model(""spam"")\n'"
gammapy/modeling/models/tests/test_temporal.py,10,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom astropy import units as u\nfrom astropy.table import Table\nfrom astropy.time import Time\nfrom gammapy.data.gti import GTI\nfrom gammapy.modeling.models import (\n    ConstantTemporalModel,\n    ExpDecayTemporalModel,\n    GaussianTemporalModel,\n    LightCurveTemplateTemporalModel,\n    PowerLawSpectralModel,\n    SkyModel,\n)\nfrom gammapy.utils.scripts import make_path\nfrom gammapy.utils.testing import requires_data\n\n\n# TODO: add light-curve test case from scratch\n# only use the FITS one for I/O (or not at all)\n@pytest.fixture(scope=""session"")\ndef light_curve():\n    path = ""$GAMMAPY_DATA/tests/models/light_curve/lightcrv_PKSB1222+216.fits""\n    return LightCurveTemplateTemporalModel.read(path)\n\n\n@requires_data()\ndef test_light_curve_str(light_curve):\n    ss = str(light_curve)\n    assert ""LightCurveTemplateTemporalModel"" in ss\n\n\n@requires_data()\ndef test_light_curve_evaluate(light_curve):\n    t = Time(59500, format=""mjd"")\n    val = light_curve(t)\n    assert_allclose(val, 0.015512, rtol=1e-5)\n\n    t = Time(46300, format=""mjd"")\n    val = light_curve.evaluate(t.mjd, ext=3)\n    assert_allclose(val, 0.01551196, rtol=1e-5)\n\n\ndef rate(x, c=""1e4 s""):\n    c = u.Quantity(c)\n    return np.exp(-x / c)\n\n\ndef ph_curve(x, amplitude=0.5, x0=0.01):\n    return 100.0 + amplitude * np.sin(2 * np.pi * (x - x0) / 1.0)\n\n\ndef test_time_sampling(tmp_path):\n    time = np.arange(0, 10, 0.06) * u.hour\n\n    table = Table()\n    table[""TIME""] = time\n    table[""NORM""] = rate(time)\n    table.meta = dict(MJDREFI=55197.0, MJDREFF=0, TIMEUNIT=""hour"")\n    temporal_model = LightCurveTemplateTemporalModel(table)\n\n    filename = str(make_path(tmp_path / ""tmp.fits""))\n    temporal_model.write(path=filename)\n    model_read = temporal_model.read(filename)\n    assert temporal_model.filename == filename\n    assert model_read.filename == filename\n    assert_allclose(model_read.table[""TIME""].quantity.value, time.value)\n\n    t_ref = ""2010-01-01T00:00:00""\n    t_min = ""2010-01-01T00:00:00""\n    t_max = ""2010-01-01T08:00:00""\n\n    sampler = temporal_model.sample_time(\n        n_events=2, t_min=t_min, t_max=t_max, random_state=0, t_delta=""10 min""\n    )\n\n    sampler = u.Quantity((sampler - Time(t_ref)).sec, ""s"")\n    assert len(sampler) == 2\n    assert_allclose(sampler.value, [12661.65802564, 7826.92991], rtol=1e-5)\n\n    table = Table()\n    table[""TIME""] = time\n    table[""NORM""] = np.ones(len(time))\n    table.meta = dict(MJDREFI=55197.0, MJDREFF=0, TIMEUNIT=""hour"")\n    temporal_model_uniform = LightCurveTemplateTemporalModel(table)\n\n    sampler_uniform = temporal_model_uniform.sample_time(\n        n_events=2, t_min=t_min, t_max=t_max, random_state=0, t_delta=""10 min""\n    )\n    sampler_uniform = u.Quantity((sampler_uniform - Time(t_ref)).sec, ""s"")\n\n    assert len(sampler_uniform) == 2\n    assert_allclose(sampler_uniform.value, [1261.65802564, 6026.9299098], rtol=1e-5)\n\n\ndef test_lightcurve_temporal_model_integral():\n    time = np.arange(0, 10, 0.06) * u.hour\n    table = Table()\n    table[""TIME""] = time\n    table[""NORM""] = np.ones(len(time))\n    table.meta = dict(MJDREFI=55197.0, MJDREFF=0, TIMEUNIT=""hour"")\n    temporal_model = LightCurveTemplateTemporalModel(table)\n\n    start = [1, 3, 5] * u.hour\n    stop = [2, 3.5, 6] * u.hour\n    gti = GTI.create(start, stop, reference_time=Time(""2010-01-01T00:00:00""))\n\n    val = temporal_model.integral(gti.time_start, gti.time_stop)\n    assert len(val) == 3\n    assert_allclose(np.sum(val), 1.0, rtol=1e-5)\n\n\ndef test_constant_temporal_model_sample():\n    temporal_model = ConstantTemporalModel()\n\n    t_ref = ""2010-01-01T00:00:00""\n    t_min = ""2010-01-01T00:00:00""\n    t_max = ""2010-01-01T08:00:00""\n\n    sampler = temporal_model.sample_time(\n        n_events=2, t_min=t_min, t_max=t_max, random_state=0\n    )\n\n    sampler = u.Quantity((sampler - Time(t_ref)).sec, ""s"")\n\n    assert len(sampler) == 2\n    assert_allclose(sampler.value, [15805.82891311, 20597.45375153], rtol=1e-5)\n\n\ndef test_constant_temporal_model_evaluate():\n    temporal_model = ConstantTemporalModel()\n    t = Time(46300, format=""mjd"")\n    val = temporal_model(t)\n    assert_allclose(val, 1.0, rtol=1e-5)\n\n\ndef test_constant_temporal_model_integral():\n    temporal_model = ConstantTemporalModel()\n    start = [1, 3, 5] * u.day\n    stop = [2, 3.5, 6] * u.day\n    gti = GTI.create(start, stop)\n    val = temporal_model.integral(gti.time_start, gti.time_stop)\n    assert len(val) == 3\n    assert_allclose(np.sum(val), 1.0, rtol=1e-5)\n\n\ndef test_exponential_temporal_model_evaluate():\n    t = Time(46301, format=""mjd"")\n    t_ref = 46300 * u.d\n    t0 = 2.0 * u.d\n    temporal_model = ExpDecayTemporalModel(t_ref=t_ref, t0=t0)\n    val = temporal_model(t)\n    assert_allclose(val, 0.6065306597126334, rtol=1e-5)\n\n\ndef test_exponential_temporal_model_integral():\n    t_ref = Time(55555, format=""mjd"")\n\n    temporal_model = ExpDecayTemporalModel(t_ref=t_ref.mjd * u.d)\n    start = [1, 3, 5] * u.day\n    stop = [2, 3.5, 6] * u.day\n    gti = GTI.create(start, stop, reference_time=t_ref)\n    val = temporal_model.integral(gti.time_start, gti.time_stop)\n    assert len(val) == 3\n    assert_allclose(np.sum(val), 0.102557, rtol=1e-5)\n\n\ndef test_gaussian_temporal_model_evaluate():\n    t = Time(46301, format=""mjd"")\n    t_ref = 46300 * u.d\n    sigma = 2.0 * u.d\n    temporal_model = GaussianTemporalModel(t_ref=t_ref, sigma=sigma)\n    val = temporal_model(t)\n    assert_allclose(val, 0.882497, rtol=1e-5)\n\n\ndef test_gaussian_temporal_model_integral():\n    temporal_model = GaussianTemporalModel(t_ref=50003 * u.d, sigma=""2.0 day"")\n    start = [1, 3, 5] * u.day\n    stop = [2, 3.5, 6] * u.day\n    t_ref = Time(50000, format=""mjd"")\n    gti = GTI.create(start, stop, reference_time=t_ref)\n    val = temporal_model.integral(gti.time_start, gti.time_stop)\n    assert len(val) == 3\n    assert_allclose(np.sum(val), 0.682679, rtol=1e-5)\n\n\n@requires_data()\ndef test_to_dict(light_curve):\n\n    out = light_curve.to_dict()\n    assert out[""type""] == ""LightCurveTemplateTemporalModel""\n    assert ""lightcrv_PKSB1222+216.fits"" in out[""filename""]\n\n\n@requires_data()\ndef test_with_skymodel(light_curve):\n\n    sky_model = SkyModel(spectral_model=PowerLawSpectralModel())\n    out = sky_model.to_dict()\n    assert ""temporal"" not in out\n\n    sky_model = SkyModel(\n        spectral_model=PowerLawSpectralModel(), temporal_model=light_curve\n    )\n    assert sky_model.temporal_model.tag == ""LightCurveTemplateTemporalModel""\n\n    out = sky_model.to_dict()\n    assert ""temporal"" in out\n'"
gammapy/utils/coordinates/tests/__init__.py,0,b''
gammapy/utils/coordinates/tests/test_fov.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom numpy.testing import assert_allclose\nimport astropy.units as u\nfrom gammapy.utils.coordinates import fov_to_sky, sky_to_fov\n\n\ndef test_fov_to_sky():\n    # test some simple cases\n    az, alt = fov_to_sky(1 * u.deg, 1 * u.deg, 0 * u.deg, 0 * u.deg)\n    assert_allclose(az.value, 359)\n    assert_allclose(alt.value, 1)\n\n    az, alt = fov_to_sky(-1 * u.deg, 1 * u.deg, 180 * u.deg, 0 * u.deg)\n    assert_allclose(az.value, 181)\n    assert_allclose(alt.value, 1)\n\n    az, alt = fov_to_sky(1 * u.deg, 0 * u.deg, 0 * u.deg, 60 * u.deg)\n    assert_allclose(az.value, 358, rtol=1e-3)\n    assert_allclose(alt.value, 59.985, rtol=1e-3)\n\n    # these are cross-checked with the\n    # transformation as implemented in H.E.S.S.\n    fov_altaz_lon = [0.7145614, 0.86603433, -0.05409698, 2.10295248]\n    fov_altaz_lat = [-1.60829115, -1.19643974, 0.45800984, 3.26844192]\n    az_pointing = [52.42056255, 52.24706061, 52.06655505, 51.86795724]\n    alt_pointing = [51.11908203, 51.23454751, 51.35376141, 51.48385814]\n    az, alt = fov_to_sky(\n        fov_altaz_lon * u.deg,\n        fov_altaz_lat * u.deg,\n        az_pointing * u.deg,\n        alt_pointing * u.deg,\n    )\n    assert_allclose(az.value, [51.320575, 50.899125, 52.154053, 48.233023])\n    assert_allclose(alt.value, [49.505451, 50.030165, 51.811739, 54.700102])\n\n\ndef test_sky_to_fov():\n    # test some simple cases\n    lon, lat = sky_to_fov(1 * u.deg, 1 * u.deg, 0 * u.deg, 0 * u.deg)\n    assert_allclose(lon.value, -1)\n    assert_allclose(lat.value, 1)\n\n    lon, lat = sky_to_fov(269 * u.deg, 0 * u.deg, 270 * u.deg, 0 * u.deg)\n    assert_allclose(lon.value, 1)\n    assert_allclose(lat.value, 0, atol=1e-7)\n\n    lon, lat = sky_to_fov(1 * u.deg, 60 * u.deg, 0 * u.deg, 60 * u.deg)\n    assert_allclose(lon.value, -0.5, rtol=1e-3)\n    assert_allclose(lat.value, 0.003779, rtol=1e-3)\n\n    # these are cross-checked with the\n    # transformation as implemented in H.E.S.S.\n    az = [51.320575, 50.899125, 52.154053, 48.233023]\n    alt = [49.505451, 50.030165, 51.811739, 54.700102]\n    az_pointing = [52.42056255, 52.24706061, 52.06655505, 51.86795724]\n    alt_pointing = [51.11908203, 51.23454751, 51.35376141, 51.48385814]\n    lon, lat = sky_to_fov(\n        az * u.deg, alt * u.deg, az_pointing * u.deg, alt_pointing * u.deg\n    )\n    assert_allclose(\n        lon.value, [0.7145614, 0.86603433, -0.05409698, 2.10295248], rtol=1e-5\n    )\n    assert_allclose(\n        lat.value, [-1.60829115, -1.19643974, 0.45800984, 3.26844192], rtol=1e-5\n    )\n'"
gammapy/utils/coordinates/tests/test_other.py,0,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom astropy.units import Quantity\nfrom gammapy.utils.coordinates import galactic\n\n\ndef test_galactic():\n    x = Quantity(0, ""kpc"")\n    y = Quantity(0, ""kpc"")\n    z = Quantity(0, ""kpc"")\n    reference = (Quantity(8.5, ""kpc""), Quantity(0, ""deg""), Quantity(0, ""deg""))\n    assert galactic(x, y, z) == reference\n'"
gammapy/utils/random/tests/__init__.py,0,b''
gammapy/utils/random/tests/test_inverse_cdf.py,12,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nimport scipy.stats as stats\nfrom numpy.testing import assert_allclose\nfrom gammapy.utils.random import InverseCDFSampler\n\n\ndef uniform_dist(x, a, b):\n    return np.select([x <= a, x >= b], [0, 0], 1 / (b - a))\n\n\ndef gauss_dist(x, mu, sigma):\n    return stats.norm.pdf(x, mu, sigma)\n\n\ndef test_uniform_dist_sampling():\n    n_sampled = 1000\n    x = np.linspace(-2, 2, n_sampled)\n\n    a, b = -1, 1\n    pdf = uniform_dist(x, a=a, b=b)\n    sampler = InverseCDFSampler(pdf=pdf, random_state=0)\n\n    idx = sampler.sample(int(1e4))\n    x_sampled = np.interp(idx, np.arange(n_sampled), x)\n\n    assert_allclose(np.mean(x_sampled), 0.5 * (a + b), atol=0.01)\n    assert_allclose(\n        np.std(x_sampled), np.sqrt(1 / 3 * (a ** 2 + a * b + b ** 2)), rtol=0.01\n    )\n\n\ndef test_norm_dist_sampling():\n    n_sampled = 1000\n    x = np.linspace(-2, 2, n_sampled)\n\n    mu, sigma = 0, 0.1\n    pdf = gauss_dist(x=x, mu=mu, sigma=sigma)\n    sampler = InverseCDFSampler(pdf=pdf, random_state=0)\n\n    idx = sampler.sample(int(1e5))\n    x_sampled = np.interp(idx, np.arange(n_sampled), x)\n\n    assert_allclose(np.mean(x_sampled), mu, atol=0.01)\n    assert_allclose(np.std(x_sampled), sigma, atol=0.005)\n\n\ndef test_axis_sampling():\n    n_sampled = 1000\n    x = np.linspace(-2, 2, n_sampled)\n\n    a, b = -1, 1\n    pdf_uniform = uniform_dist(x, a=a, b=b)\n\n    mu, sigma = 0, 0.1\n    pdf_gauss = gauss_dist(x=x, mu=mu, sigma=sigma)\n\n    pdf = np.vstack([pdf_gauss, pdf_uniform])\n    sampler = InverseCDFSampler(pdf, random_state=0, axis=1)\n\n    idx = sampler.sample_axis()\n    x_sampled = np.interp(idx, np.arange(n_sampled), x)\n\n    assert_allclose(x_sampled, [0.012266, 0.43081], rtol=1e-4)\n'"
gammapy/utils/random/tests/test_random.py,8,"b'# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport numpy as np\nfrom numpy.testing import assert_allclose\nfrom astropy import units as u\nfrom astropy.coordinates import Angle\nfrom gammapy.utils.random import (\n    sample_powerlaw,\n    sample_sphere,\n    sample_sphere_distance,\n    sample_times,\n)\nfrom gammapy.utils.testing import assert_quantity_allclose\n\n\ndef test_sample_sphere():\n    random_state = np.random.RandomState(seed=0)\n\n    # test general case\n    lon, lat = sample_sphere(size=2, random_state=random_state)\n    assert_quantity_allclose(lon, Angle([3.44829694, 4.49366732], ""radian""))\n    assert_quantity_allclose(lat, Angle([0.20700192, 0.08988736], ""radian""))\n\n    # test specify a limited range\n    lon_range = Angle([40.0, 45.0], ""deg"")\n    lat_range = Angle([10.0, 15.0], ""deg"")\n    lon, lat = sample_sphere(\n        size=10, lon_range=lon_range, lat_range=lat_range, random_state=random_state\n    )\n    assert ((lon_range[0] <= lon) & (lon < lon_range[1])).all()\n    assert ((lat_range[0] <= lat) & (lat < lat_range[1])).all()\n\n    # test lon within (-180, 180) deg range\n    lon_range = Angle([-40.0, 0.0], ""deg"")\n    lon, lat = sample_sphere(size=10, lon_range=lon_range, random_state=random_state)\n    assert ((lon_range[0] <= lon) & (lon < lon_range[1])).all()\n    lat_range = Angle([-90.0, 90.0], ""deg"")\n    assert ((lat_range[0] <= lat) & (lat < lat_range[1])).all()\n\n    # test lon range explicitly (0, 360) deg\n    lon_range = Angle([0.0, 360.0], ""deg"")\n    lon, lat = sample_sphere(size=100, lon_range=lon_range, random_state=random_state)\n    # test values in the desired range\n    lat_range = Angle([-90.0, 90.0], ""deg"")\n    assert ((lon_range[0] <= lon) & (lon < lon_range[1])).all()\n    assert ((lat_range[0] <= lat) & (lat < lat_range[1])).all()\n    # test if values are distributed along the whole range\n    nbins = 4\n    lon_delta = (lon_range[1] - lon_range[0]) / nbins\n    lat_delta = (lat_range[1] - lat_range[0]) / nbins\n    for i in np.arange(nbins):\n        assert (\n            (lon_range[0] + i * lon_delta <= lon)\n            & (lon < lon_range[0] + (i + 1) * lon_delta)\n        ).any()\n        assert (\n            (lat_range[0] + i * lat_delta <= lat)\n            & (lat < lat_range[0] + (i + 1) * lat_delta)\n        ).any()\n\n    # test lon range explicitly (-180, 180) deg\n    lon_range = Angle([-180.0, 180.0], ""deg"")\n    lon, lat = sample_sphere(size=100, lon_range=lon_range, random_state=random_state)\n    # test values in the desired range\n    lat_range = Angle([-90.0, 90.0], ""deg"")\n    assert ((lon_range[0] <= lon) & (lon < lon_range[1])).all()\n    assert ((lat_range[0] <= lat) & (lat < lat_range[1])).all()\n    # test if values are distributed along the whole range\n    nbins = 4\n    lon_delta = (lon_range[1] - lon_range[0]) / nbins\n    lat_delta = (lat_range[1] - lat_range[0]) / nbins\n    for i in np.arange(nbins):\n        assert (\n            (lon_range[0] + i * lon_delta <= lon)\n            & (lon < lon_range[0] + (i + 1) * lon_delta)\n        ).any()\n        assert (\n            (lat_range[0] + i * lat_delta <= lat)\n            & (lat < lat_range[0] + (i + 1) * lat_delta)\n        ).any()\n\n    # test box around Galactic center\n    lon_range = Angle([-5.0, 5.0], ""deg"")\n    lon, lat = sample_sphere(size=10, lon_range=lon_range, random_state=random_state)\n    # test if values are distributed along the whole range\n    nbins = 2\n    lon_delta = (lon_range[1] - lon_range[0]) / nbins\n    for i in np.arange(nbins):\n        assert (\n            (lon_range[0] + i * lon_delta <= lon)\n            & (lon < lon_range[0] + (i + 1) * lon_delta)\n        ).any()\n\n    # test box around Galactic anticenter\n    lon_range = Angle([175.0, 185.0], ""deg"")\n    lon, lat = sample_sphere(size=10, lon_range=lon_range, random_state=random_state)\n    # test if values are distributed along the whole range\n    nbins = 2\n    lon_delta = (lon_range[1] - lon_range[0]) / nbins\n    for i in np.arange(nbins):\n        assert (\n            (lon_range[0] + i * lon_delta <= lon)\n            & (lon < lon_range[0] + (i + 1) * lon_delta)\n        ).any()\n\n\ndef test_sample_sphere_distance():\n    random_state = np.random.RandomState(seed=0)\n\n    x = sample_sphere_distance(\n        distance_min=0.1, distance_max=42, size=2, random_state=random_state\n    )\n    assert_allclose(x, [34.386731, 37.559774])\n\n    x = sample_sphere_distance(\n        distance_min=0.1, distance_max=42, size=int(1e3), random_state=random_state\n    )\n    assert x.min() >= 0.1\n    assert x.max() <= 42\n\n\ndef test_sample_powerlaw():\n    random_state = np.random.RandomState(seed=0)\n\n    x = sample_powerlaw(x_min=0.1, x_max=10, gamma=2, size=2, random_state=random_state)\n    assert_allclose(x, [0.14886601, 0.1873559])\n\n\ndef test_random_times():\n    # An example without dead time.\n    rate = u.Quantity(10, ""s^-1"")\n    time = sample_times(size=100, rate=rate, random_state=0)\n    assert_allclose(time[0].sec, 0.07958745081631101)\n    assert_allclose(time[-1].sec, 9.186484131475076)\n\n    # An example with `return_diff=True`\n    rate = u.Quantity(10, ""s^-1"")\n    time = sample_times(size=100, rate=rate, return_diff=True, random_state=0)\n    assert_allclose(time[0].sec, 0.07958745081631101)\n    assert_allclose(time[-1].sec, 0.00047065345706976753)\n\n    # An example with dead time.\n    rate = u.Quantity(10, ""Hz"")\n    dead_time = u.Quantity(0.1, ""second"")\n    time = sample_times(size=100, rate=rate, dead_time=dead_time, random_state=0)\n    assert np.min(time) >= u.Quantity(0.1, ""second"")\n    assert_allclose(time[0].sec, 0.1 + 0.07958745081631101)\n    assert_allclose(time[-1].sec, 0.1 * 100 + 9.186484131475076)\n'"
gammapy/modeling/models/tests/data/make.py,1,"b'""""""Create example model YAML files programmatically.\n\n(some will be also written manually)\n""""""\nfrom pathlib import Path\nimport numpy as np\nimport astropy.units as u\nfrom gammapy.data import DataStore\nfrom gammapy.datasets import Datasets, MapDataset\nfrom gammapy.makers import MapDatasetMaker\nfrom gammapy.maps import MapAxis, WcsGeom\nfrom gammapy.modeling.models import (\n    ExpCutoffPowerLawSpectralModel,\n    GaussianSpatialModel,\n    Models,\n    PointSpatialModel,\n    PowerLawSpectralModel,\n    SkyDiffuseCube,\n    SkyModel,\n)\n\nDATA_PATH = Path(""./"")\n\n\ndef make_example_2():\n    spatial = GaussianSpatialModel(lon_0=""0 deg"", lat_0=""0 deg"", sigma=""1 deg"")\n    model = SkyModel(PowerLawSpectralModel(), spatial)\n    models = Models([model])\n    models.write(DATA_PATH / ""example2.yaml"")\n\n\ndef make_datasets_example():\n    # Define which data to use and print some information\n\n    energy_axis = MapAxis.from_edges(\n        np.logspace(-1.0, 1.0, 4), unit=""TeV"", name=""energy"", interp=""log""\n    )\n    geom0 = WcsGeom.create(\n        skydir=(0, 0),\n        binsz=0.1,\n        width=(1, 1),\n        frame=""galactic"",\n        proj=""CAR"",\n        axes=[energy_axis],\n    )\n    geom1 = WcsGeom.create(\n        skydir=(1, 0),\n        binsz=0.1,\n        width=(1, 1),\n        frame=""galactic"",\n        proj=""CAR"",\n        axes=[energy_axis],\n    )\n    geoms = [geom0, geom1]\n\n    sources_coords = [(0, 0), (0.9, 0.1)]\n    names = [""gc"", ""g09""]\n    models = []\n\n    for idx, (lon, lat) in enumerate(sources_coords):\n        spatial_model = PointSpatialModel(\n            lon_0=lon * u.deg, lat_0=lat * u.deg, frame=""galactic""\n        )\n        spectral_model = ExpCutoffPowerLawSpectralModel(\n            index=2 * u.Unit(""""),\n            amplitude=3e-12 * u.Unit(""cm-2 s-1 TeV-1""),\n            reference=1.0 * u.TeV,\n            lambda_=0.1 / u.TeV,\n        )\n        model_ecpl = SkyModel(\n            spatial_model=spatial_model, spectral_model=spectral_model, name=names[idx]\n        )\n        models.append(model_ecpl)\n\n    # test to link a spectral parameter\n    params0 = models[0].spectral_model.parameters\n    params1 = models[1].spectral_model.parameters\n    params0.link(""reference"", params1[""reference""])\n    # update the sky model\n    models[0].parameters.link(""reference"", params1[""reference""])\n\n    obs_ids = [110380, 111140, 111159]\n    data_store = DataStore.from_dir(""$GAMMAPY_DATA/cta-1dc/index/gps/"")\n\n    diffuse_model = SkyDiffuseCube.read(\n        ""$GAMMAPY_DATA/fermi_3fhl/gll_iem_v06_cutout.fits""\n    )\n\n    datasets_list = []\n    for idx, geom in enumerate(geoms):\n        observations = data_store.get_observations(obs_ids)\n\n        stacked = MapDataset.create(geom=geom)\n        stacked.background_model.name = ""background_irf_"" + names[idx]\n\n        maker = MapDatasetMaker(offset_max=4.0 * u.deg)\n\n        for obs in observations:\n            dataset = maker.run(stacked, obs)\n            stacked.stack(dataset)\n\n        stacked.psf = stacked.psf.get_psf_kernel(\n            position=geom.center_skydir, geom=geom, max_radius=""0.3 deg""\n        )\n\n        stacked.name = names[idx]\n        stacked.models = models[idx] + diffuse_model\n        datasets_list.append(stacked)\n\n    datasets = Datasets(datasets_list)\n\n    dataset0 = datasets[0]\n    print(""dataset0"")\n    print(""counts sum : "", dataset0.counts.data.sum())\n    print(""expo sum : "", dataset0.exposure.data.sum())\n    print(""bkg0 sum : "", dataset0.background_model.evaluate().data.sum())\n\n    datasets.write(""$GAMMAPY_DATA/tests/models"", prefix=""gc_example_"", overwrite=True)\n\n\nif __name__ == ""__main__"":\n    make_example_2()\n    make_datasets_example()\n'"
