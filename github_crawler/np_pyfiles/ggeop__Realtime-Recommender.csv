file_path,api_count,code
src/app/__init__.py,0,b''
src/recommender/__init__.py,0,"b""import os\nimport sys\n\nsys.path.append(os.path.abspath('..'))\n"""
src/recommender/input_transformer.py,0,"b'""""""\nThis prepossessing step is about to getting the input data and\ntransform it in a common format (a list).\n""""""\n\n\nclass Transformer(object):\n    def __init__(self,  target=None):\n        self.target = target\n\n    def transform(self, input_data):\n        pass\n\n\nclass JsonTransformer(Transformer):\n    def transform(self, input_data):\n        return input_data[self.target]\n\n\nclass DataframeTransformer(Transformer):\n    def transform(self, input_data):\n        return list(input_data[self.target])\n\n\nclass DocumentsTransformer(object):\n    def __init__(self):\n        pass\n'"
src/recommender/model_feeder.py,0,"b""import logging\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom recommender.input_transformer import DataframeTransformer\nfrom recommender.utils import SqlConnector, DATABASE\nfrom recommender.resuts_calculator import ResultsCalculator\nfrom recommender.pre_processor import Cleaner\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n\nclass StaticFeeder(object):\n    def __init__(self, model_manager, set_mode=None, input_format=None):\n        self.set_mode = set_mode\n        self.input_format = input_format\n        self.model_manager = model_manager\n\n    def run(self):\n        sql_connection = SqlConnector()\n        input_df = sql_connection.selecting_query()\n        dataframe_transformer = DataframeTransformer(DATABASE['target_column'])\n        input_text = dataframe_transformer.transform(input_df)\n        train_tdm = self.model_manager.create_model(input_text)\n        logging.info('WAITING FOR USER INPUT..')\n        documents = input('INSERT TO THE MODEL: ')\n        results_calculator = ResultsCalculator(dataset=input_df,\n                                               result_column=DATABASE['target_column'],\n                                               vectorizer=self.model_manager.vectorizer,\n                                               similarity_measure=cosine_similarity,\n                                               number_of_recommendations=3)\n\n        result, score = results_calculator.query(train_tdm, [documents])\n        print(result)\n\n"""
src/recommender/model_manager.py,0,"b""import os\nimport logging\nimport pickle\nfrom gensim.test.utils import get_tmpfile\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom recommender.settings import MODEL_DUMPS_PATH\nfrom recommender.models.tfidf_model_1 import args, model_name\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n\nclass ModelManager(object):\n    def __init__(self):\n        self.vectorizer = TfidfVectorizer(**args)\n        self.model_name = model_name\n        self.saved_model = self.model_name + '.model'\n        logging.info('THE MODEL {} INITIALIZED SUCCESSFULLY!'.format(self.model_name))\n\n    def create_model(self, input_text):\n        logging.info('CREATE MODEL - {}..'.format(self.saved_model))\n        model = self.load_model()\n        if not model:\n            vec = self.vectorizer.fit_transform(input_text)\n            logging.info('MODEL CREATED - {}..'.format(self.model_name))\n            self.save_model(vec)\n            return vec\n\n    def load_model(self):\n        logging.info('SEARCH TO LOAD A TRAINED MODEL - {}'.format(self.model_name))\n        model_files = [f for f in os.listdir(MODEL_DUMPS_PATH)]\n        if self.saved_model in model_files:\n            logging.info('TRAINED MODEL EXISTS - {}'.format(self.model_name))\n            model_file = get_tmpfile(os.path.join(MODEL_DUMPS_PATH, self.model_name))\n            return pickle.loads(model_file)\n\n        logging.info('NO TRAINED MODEL EXISTS - {}'.format(self.model_name))\n        return None\n\n    def save_model(self, model):\n        temp_file = get_tmpfile(os.path.join(MODEL_DUMPS_PATH, self.model_name))\n        pickle.dumps(temp_file)\n        logging.info('MODEL SAVED!')\n"""
src/recommender/post_processor.py,0,b'class PostProcessor(object):\n    pass\n'
src/recommender/pre_processor.py,0,"b'import logging\nfrom collections import defaultdict\nfrom recommender.settings import THRESHOLD\n\nlogging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n\n""""""\nClean function:\n-Split the documents to list with words\n-Transform capital to lower\n-Remove stopwords\n-Remove words with frequency lower than the threshold\n""""""\n\n\nclass Cleaner(object):\n    def __init__(self, documents, stop_list=None, lower_case=False, lower_threshold=False):\n        self.documents = documents\n        self.stop_list = stop_list\n        self.lower_case = lower_case\n        self.lower_threshold = lower_threshold\n        logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n\n    def clean(self):\n        if isinstance(self.documents, str):\n            self.documents = [self.documents]\n        if self.stop_list and self.lower_case:\n            texts = [[word for word in document.lower().split() if word not in self.stop_list]\n                     for document in self.documents]\n        elif self.stop_list and not self.lower_case:\n            texts = [[word for word in document.split() if word not in self.stop_list]\n                     for document in self.documents]\n        elif not self.stop_list and self.lower_case:\n            texts = [[word for word in document.lower().split()] for document in self.documents]\n        else:\n            texts = [[word for word in document.split()] for document in self.documents]\n\n        return texts\n\n    @staticmethod\n    def filter(texts):\n        """"""texts format [[\'..\', \'..\'],[\'..\'],[],...]""""""\n        frequency = defaultdict(int)\n        for text in texts:\n            for token in text:\n                frequency[token] += 1\n\n        texts = [[token for token in text if frequency[token] > THRESHOLD]\n                 for text in texts]\n\n        return texts\n'"
src/recommender/resuts_calculator.py,0,"b'\n\nclass ResultsCalculator(object):\n    def __init__(self, dataset, result_column, vectorizer, similarity_measure, number_of_recommendations):\n        self.dataset = dataset\n        self.result_column = result_column\n        self.similarity_measure = similarity_measure\n        self.vectorizer = vectorizer\n        self.number_of_recommendations = number_of_recommendations\n\n    def query(self, train_tdm, new_text):\n        test_tdm = self.vectorizer.transform(new_text)\n        result, score = self.calculate_similarity(train_tdm, test_tdm)\n        return result, score\n\n    def calculate_similarity(self, train_tdm, test_tdm):\n        similarities = self.similarity_measure(train_tdm, test_tdm)\n\n        """"""Extract the most close organization from the sorted list""""""\n        index = similarities.argsort(axis=None)[-self.number_of_recommendations:]\n\n        """"""Extract the similarity score""""""\n        score = similarities[index]\n\n        """"""Return the organization name""""""\n        return self.dataset[self.result_column][index][::-1], score[::-1]\n'"
src/recommender/run_engine.py,0,"b""import logging\nfrom argparse import ArgumentParser\nfrom recommender.model_manager import ModelManager\nfrom recommender.model_feeder import StaticFeeder\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n\ndef main():\n    parser = ArgumentParser()\n\n    parser.add_argument('--set_mode', type=str, required=False, help='Set engine mode type')\n    parser.add_argument('--input_format', type=str, required=False, help='Set the input')\n    parser.add_argument('--model', type=str, required=False, help='Set the type of model')\n\n    args, unknown, = parser.parse_known_args()\n\n    logging.info('RECOMMENDER STARTS..')\n    logging.info('MODEL MODE = {} AND MODEL= {}'.format(args.set_mode, args.model))\n    model_manager = ModelManager()\n    logging.info('APPLICATION STARTS..')\n    if args.set_mode == 'static':\n        feeder = StaticFeeder(model_manager=model_manager)\n        feeder.run()\n\n\nif __name__ == '__main__':\n    main()\n"""
src/recommender/settings.py,0,"b""import sys\nimport os\nfrom gensim.models import Word2Vec, LsiModel\n\n# Gensim Models\nGENSIM = {'Word2Vec': Word2Vec,\n          'LsiModel': LsiModel\n         }\n\n# Minimum number of words in documents\nTHRESHOLD = 2\n\n# Saved models location\nMODEL_DUMPS_PATH = os.path.join(sys.path[0], 'model_dumps')\nif not os.path.exists(MODEL_DUMPS_PATH):\n    os.makedirs(MODEL_DUMPS_PATH, mode=0o777)\n\n# Target Database\nDATABASE = {'host': 'localhost',\n            'user': 'remote',\n            'password': 'remote',\n            'db': '',\n            'target_table': '',\n            'target_column': ''\n            }\n"""
src/recommender/utils.py,0,"b""import mysql.connector\nimport pandas as pd\nimport logging\nfrom recommender.settings import DATABASE\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n\nclass SqlConnector(object):\n    def __init__(self):\n        logging.info('TRY TO CONNECT TO {}..'.format(DATABASE['host']))\n        self.sql_conn = mysql.connector.connect(host=DATABASE['host'],         # your host, usually localhost\n                                                user=DATABASE['user'],         # your username\n                                                passwd=DATABASE['password'],   # your password\n                                                db=DATABASE['db'])             # name of the data base\n\n        logging.info('CONNECTED SUCCESSFULLY TO {}!'.format(DATABASE['host']))\n\n    def selecting_query(self):\n        query = ('SELECT ' + DATABASE['target_column'] + ' FROM ' + DATABASE['db'] + '.' + DATABASE['target_table'])\n        return pd.read_sql(query, self.sql_conn)\n"""
src/recommender/models/__init__.py,0,b''
src/recommender/models/tfidf_model_1.py,0,"b'""""""Model Configuration settings""""""\n\nmodel_name = \'tfidf_model_1\'\nargs = {\n        ""stop_words"": ""english"",\n        ""lowercase"": True,\n        ""norm"": ""l2"",\n        ""use_idf"": True,\n        ""smooth_idf"": True,\n        ""sublinear_tf"": True,\n        ""ngram_range"": (1,2)}  # We take into account all the unigrams and bigrams\n'"
src/recommender/tests/__init__.py,0,"b""import os\nimport sys\n\nsys.path.append(os.path.abspath('...'))\n"""
src/recommender/tests/input_transformer_tests.py,1,"b""import unittest\nimport pandas as pd\nimport numpy as np\n\nfrom recommender.input_transformer import JsonTransformer, DataframeTransformer\n\n\nclass DataframeTransformerTests(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(self):\n        data = np.array([[1, 2], [3, 4]])\n        self.dataframe = pd.DataFrame(data=data, columns=['Target_col', 'Col2'])\n\n    def test_correct_input(self):\n        dataframe_transformer = DataframeTransformer('Target_col')\n        self.assertEqual(dataframe_transformer.transform(self.dataframe), [1, 3])\n\n    def test_wrong_column(self):\n        pass\n\n\nclass DocumentsTransformerTests(unittest.TestCase):\n    @classmethod\n    def setUpClass(self):\n        self.documents = ['word1 word2', 'word3']\n\n    def test_sting_input_no_spaces(self):\n        pass\n\nif __name__ == '__main__':\n    unittest.main()\n"""
src/recommender/tests/model_manager_tests.py,0,"b""import unittest\nfrom gensim import models\nfrom gensim.test.utils import common_dictionary, common_corpus\nfrom recommender.model_manager import ModelManager\n\n\nclass LoadModelTests(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(self):\n        pass\n\n\nclass CreateModelTests(unittest.TestCase):\n\n    def test_create_BoW_model(self):\n        pass\n\n    def test_create_Lsi_model(self):\n        pass\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
src/recommender/tests/pre_processor_tests.py,0,"b""\n\nimport unittest\n\nfrom recommender.pre_processor import Cleaner\n\n\nclass CleanerTests(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(self):\n        self.documents = ['Test documents', 'Stop list words']\n        self.stop_list = ['stop', 'list', 'words']\n\n    def test_stop_list_lower(self):\n        cleaner = Cleaner(documents=self.documents,\n                          stop_list=self.stop_list,\n                          lower_case=True)\n\n        self.assertEqual(cleaner.clean(), [['test', 'documents'], []])\n\n    def test_stop_list(self):\n        cleaner = Cleaner(documents=self.documents,\n                          stop_list=self.stop_list,\n                          lower_case=False)\n\n        self.assertEqual(cleaner.clean(), [['Test', 'documents'], ['Stop']])\n\n    def test_sort(self):\n        cleaner = Cleaner(documents=self.documents,\n                          lower_case=True)\n\n        self.assertEqual(cleaner.clean(), [['test', 'documents'], ['stop', 'list', 'words']])\n\n    def test_no_stop_list_no_sort(self):\n        cleaner = Cleaner(documents=self.documents)\n\n        self.assertEqual(cleaner.clean(), [['Test', 'documents'], ['Stop', 'list', 'words']])\n\n\nclass FilterTests(unittest.TestCase):\n\n    def test_filter_below_threshold(self):\n        documents = ['word1 word2', 'word1']\n        cleaner = Cleaner(documents=documents)\n        texts = cleaner.clean()\n        self.assertEqual(cleaner.filter(texts),[[], []])\n\n    def test_filter_above_threshold(self):\n        documents = ['word1 word1 word2', 'word1']\n        cleaner = Cleaner(documents=documents)\n        texts = cleaner.clean()\n        self.assertEqual(cleaner.filter(texts),[['word1', 'word1'], ['word1']])\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
src/recommender/tests/settings.py,0,b''
