file_path,api_count,code
artificial-neural-network/ann_sample.py,0,"b'# Artificial Neural Network\n# ========================================\n# [] File Name : ann_sample.py\n#\n# [] Creation Date : December 2017\n#\n# [] Created By : Ali Gholami (aligholami7596@gmail.com)\n# ========================================\n#\nimport math\nfrom numpy import array, random, dot, exp\n\n\n# The dataset from the perceptron (which it was failing at classification procedure)\ndataset = array([   \n                    [3, 4, 5],\n                    [3, 4, 6],\n                    [3, 3, 5],\n                    [3, 8, 5],\n                    [4, 4, 5],\n                    [1, 0, 5],\n                    [7, 4, 8],\n                    [1, 6, 4],\n                    [3, 4, 5],\n                    [4, 4, 6],\n                    [9, 5, 5]\n])\n\ndataset_labels = array([[1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [0]])\n\n# Define the initial values\nnumOfIterations = 5000\n\n# Initial weights\nfirstLayerWeights = random.rand(3, 2)\nsecondLayerWeights = random.rand(2, 1)\n\n# The beloved sigmoid function\ndef sigmoid(x):\n    spart = (1 / (1 + exp(-x)))\n    return spart\n\n# The sigmoid curve\ndef sigmoidCurve(x):\n    return x * (1 - x)\n\n# ======================================== #\n# ======== Train The 2 Layer ANN ========= #\n# ======================================== #\ndef trainNeuralNetwork(dataset, flw, slw, numOfIterations):\n    \n    \n    updatedFLW = 0\n    updatedSLW = 0\n\n    for i in range(numOfIterations):\n        firstLayerOutputVector = sigmoid(dot(dataset, flw))\n        secondLayerOutputVector = sigmoid(dot(firstLayerOutputVector, slw))\n\n        # Find the error and delta for the final layer\n        secondLayerError = secondLayerOutputVector - dataset_labels\n        secondLayerDelta = secondLayerError * sigmoidCurve(secondLayerOutputVector)\n\n        # Find the error and delta for the first layer\n        firstLayerError = secondLayerDelta.dot(slw.T)\n        firstLayerDelta = firstLayerError * sigmoidCurve(firstLayerOutputVector)\n\n        # Update the layer 1 and layer 2 weights\n        flw -= secondLayerOutputVector.T.dot(firstLayerDelta)\n        slw -= firstLayerOutputVector.T.dot(secondLayerDelta)\n\n        updatedFLW = flw\n        updatedSLW = slw\n    \n    return [flw, slw]\n\n\n# ======================================== #\n# ======= Start and Plot The Result ====== #\n# ======================================== #\nl1_weight, l2_weight = trainNeuralNetwork(dataset, firstLayerWeights, secondLayerWeights, numOfIterations)\n\nprint(""Layer 1 weights vector was updated to:"")\nprint(l1_weight)\n\nprint(""\\nLayer 2 weights vector was updated to:"")\nprint(l2_weight)'"
gradient-descent/gd_sample.py,0,"b'# Gradient Descent Sample\n# ========================================\n# [] File Name : gd_sample.py\n#\n# [] Creation Date : December 2017\n#\n# [] Created By : Ali Gholami (aligholami7596@gmail.com)\n# ========================================\n#\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Initial Definitions\ncurrent_x = 0.5     \nlearning_rate = 0.02\nnum_iterations = 150\n\n# Goal Function is 5x^4 - 6x^2\ndef findSlopeAtGivenPoint(x):\n    return 5 * x ** 4 - 6 * x ** 2\n\n# ======================================== #\n# ==  Train The Gradient Descent & Plot == #\n# ======================================== #\ndef trainGradientDescent(iter_range, current_x):\n\n    # Gradient Descent Result Array\n    gd_result = []\n    for i in range(iter_range):\n        previous_x = current_x\n        current_x += -learning_rate * findSlopeAtGivenPoint(current_x)\n        gd_result.append(current_x)\n        print(""X was updated to: "", previous_x)\n\n    return gd_result\n\n# ======================================== #\n# ====  Points to Plot The GD Diagram ==== #\n# ======================================== #\ngradientDescentResult = trainGradientDescent(num_iterations, current_x)\n\nfig = plt.figure()\nfig.suptitle(\'Gradient Descent of Y = X^5 - 2X^3 - 2\', fontsize=10, fontweight=\'bold\')\nax = fig.add_subplot(1,1,1)\nax.title.set_text(""Gradient Descent Result"")\n\nax.plot(gradientDescentResult)\nplt.show()'"
least-square/ls_sample.py,0,"b'# Least Square Sample\n# ========================================\n# [] File Name : ls_sample.py\n#\n# [] Creation Date : December 2017\n#\n# [] Created By : Ali Gholami (aligholami7596@gmail.com)\n# ========================================\n#\nimport matplotlib.pyplot as plt\nimport numpy as numpy\n\ndataset = numpy.array([[3,5],[5,3],[8,4],[3,1],[6,4],[5,4],[7,5],[8,3]])\n\nslope_list = [5, 3, 6, 6, 3, 4]\nconstant_list = [6, 1, 4, 8, 4, 7]\n\nplot_titles = [\n    \'y = 5x + 6\',\n    \'y = 3x + 1\',\n    \'y = 6x + 4\',\n    \'y = 6x + 8\',\n    \'y = 3x + 4\',\n    \'y = 4x + 7\'\n]\n\n# ======================================== #\n# ========== Least Square Error ========== #\n# ======================================== #\ndef computeErrorForLineGivenPoints(b, m, coordinates):\n    totalError = 0\n    \n    for i in range(0, len(coordinates)):\n        x = coordinates[i][0]\n        y = coordinates[i][1]\n\n        # Calcuate the error\n        totalError += (y - (m * x + b)) ** 2\n\n    return totalError / float(len(coordinates))\n\n# ======================================== #\n# ============ Test with data ============ #\n# ======================================== #\nerrorlist = []\n\nfor i in range(0, 6):\n    errorlist.append(computeErrorForLineGivenPoints(slope_list[i], constant_list[i], dataset))\n    print(""Hypothesis "" + plot_titles[i] + "" error: "")\n    print(errorlist[i])\n\n# ======================================== #\n# ============ Plot the result =========== #\n# ======================================== #\nfig = plt.figure()\nfig.suptitle(\'Least Square Errors\', fontsize=10, fontweight=\'bold\')\n\nfor i in range(1, 7):\n    ax = fig.add_subplot(3, 2, i)\n    ax.title.set_text(plot_titles[i-1])\n    ax.scatter(dataset[:,0],dataset[:,1])  \n\n    errorLabel = ""Error = ""\n    ax.text(0.95, 0.01, errorLabel + str(errorlist[i-1]),\n            verticalalignment=\'bottom\', horizontalalignment=\'right\',\n            transform=ax.transAxes,\n            color=\'green\', fontsize=12)\n    plt.plot(dataset, dataset/slope_list[i-1] + constant_list[i-1])\n\nplt.show()'"
linear-regression/lr_sample.py,1,"b'# Linear Regression Sample\r\n# ========================================\r\n# [] File Name : lr_sample.py\r\n#\r\n# [] Creation Date : December 2017\r\n#\r\n# [] Created By : Ali Gholami (aligholami7596@gmail.com)\r\n# ========================================\r\n#\r\nimport matplotlib.pyplot as plt \r\nimport numpy as np \r\n\r\ndataset = np.array([[1,2.5],[2,3.5],[3,4.6],[4,4.8],[5,5.9],[6,7.1],[6.5,7.5]])\r\n\r\n# Define the initial values\r\nlearning_rate = 0.02\r\nnumOfIterations = 0\r\ninitialConstant = 5\r\ninitialSlope = 2\r\n\r\n# Testing declarations\r\niterations_array = [10, 30, 60, 150, 500, 1000]\r\n\r\n# ======================================== #\r\n# ====== Train With Gradient Descent ===== #\r\n# ======================================== #\r\ndef trainWithGradientDescent(coordiantes, h_slope, h_constant, learning_rate):\r\n\r\n    # Indicates how much h values must be updated\r\n    slope_gd_rate = 0\r\n    constant_gd_rate = 0\r\n\r\n    # Indicates the new slop and constant for each hypothesis\r\n    updated_h_slope = h_slope\r\n    updated_h_constant = h_constant\r\n\r\n    # Repeat on each single data \r\n    # This is gradient descent obviously :)\r\n    for i in range(0, len(coordiantes)):\r\n\r\n        # Grab the current x and y\r\n        x = coordiantes[i][0]\r\n        y = coordiantes[i][1]\r\n\r\n        constant_gd_rate += -2/len(coordiantes) * (y - (h_slope * x + h_constant))\r\n        slope_gd_rate += -2/len(coordiantes) * x * (y - (h_slope * x + h_constant))\r\n        print(""Constant error: "" + str(constant_gd_rate))\r\n        print(""Slope error: "" + str(slope_gd_rate))\r\n\r\n    updated_h_constant = h_constant - (learning_rate * constant_gd_rate)\r\n    updated_h_slope = h_slope - (learning_rate * slope_gd_rate)\r\n    return [updated_h_constant, updated_h_slope]\r\n\r\n# ======================================== #\r\n# ====== Initialize GD with values ======= #\r\n# ======================================== #\r\ndef gradientDescentInitializer(cooridnates, initial_slope, initial_constant, learning_rate, numOfIterations):\r\n    \r\n    # First value for the trained constant\r\n    # and trained slope\r\n    trained_constant = initial_constant\r\n    trained_slope = initial_slope\r\n    # Start training for numOfIterations times\r\n    for i in range(0, numOfIterations):\r\n        trained_constant, trained_slope = trainWithGradientDescent(cooridnates, trained_slope, trained_constant, learning_rate)\r\n    \r\n    return [trained_constant, trained_slope]\r\n\r\n# ======================================== #\r\n# ======= Start & Plot the result ======== #\r\n# ======================================== #\r\nfig = plt.figure()\r\nfig.suptitle(\'Linear Regression\', fontsize=10, fontweight=\'bold\')\r\n\r\nfor i in range(1, len(iterations_array)+1):\r\n    constant_result, slope_result = gradientDescentInitializer(dataset, initialSlope, initialConstant, learning_rate, iterations_array[i-1])\r\n    ax = fig.add_subplot(3, 2, i)\r\n    ax.scatter(dataset[:,0],dataset[:,1])\r\n    ax.plot(dataset[:,0], dataset[:,0] * slope_result + constant_result)\r\n    ax.text(0.95, 0.01, ""Iterations: "" + str(iterations_array[i-1]),\r\n            verticalalignment=\'bottom\', horizontalalignment=\'right\',\r\n            transform=ax.transAxes,\r\n            color=\'green\', fontsize=10)\r\n    ax.text(0.95, 0.09, ""Learning Rate: "" + str(learning_rate),\r\n            verticalalignment=\'bottom\', horizontalalignment=\'right\',\r\n            transform=ax.transAxes,\r\n            color=\'green\', fontsize=10)\r\n\r\n\r\nplt.show()\r\n'"
perceptron/p_sample.py,0,"b'# Perceptron Sample\n# ========================================\n# [] File Name : p_sample.py\n#\n# [] Creation Date : December 2017\n#\n# [] Created By : Ali Gholami (aligholami7596@gmail.com)\n# ========================================\n#\nimport math\nfrom numpy import array, random, dot\nfrom random import choice\n\n\ndataset = [         (array([3, 4, 5]), 1),\n                    (array([3, 4, 6]), 0),\n                    (array([3, 3, 5]), 0),\n                    (array([3, 8, 5]), 0),\n                    (array([4, 4, 5]), 1),\n                    (array([1, 0, 5]), 1),\n                    (array([7, 4, 8]), 1),\n                    (array([1, 6, 4]), 0),\n                    (array([3, 4, 5]), 0),\n                    (array([4, 4, 6]), 1),\n                    (array([9, 5, 5]), 0)\n]\n\n# Define the initial values\nlearning_rate = 0.01\nweights = random.rand(3)\nnumOfIterations = 250\n\nactivationFunction = lambda x: 0 if x < 0 else 1\n\n# ======================================== #\n# ========= Train The Perceptron ========= #\n# ======================================== #\ndef trainSingleLayerPerceptron(dataset, weights, numOfIterations, learning_rate):\n    for i in range(numOfIterations):\n        # Select randomly from dataset\n        inputVector, label = choice(dataset)\n\n        # Find the dot product of weights4 and input vector\n        result = dot(weights, inputVector)\n\n        # Find the error\n        resultError = label - activationFunction(result)\n\n        # Update the weights\n        weights += learning_rate * resultError * inputVector\n\n    return weights\n\n# Test the trained data (ignore the invalidation of this model for learning purpose)\nlearned_weights = trainSingleLayerPerceptron(dataset, weights, numOfIterations, learning_rate)\n\nfor vector_i, label_i in dataset:\n    test_result = dot(learned_weights, vector_i)\n    print(""Classified "", vector_i, ""as "", activationFunction(test_result))\n'"
