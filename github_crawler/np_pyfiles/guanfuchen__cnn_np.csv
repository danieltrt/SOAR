file_path,api_count,code
convlstm.py,0,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\n# \xe4\xbb\xa3\xe7\xa0\x81\xe6\x9d\xa5\xe6\xba\x90[convlstm.py](https://github.com/ndrplz/ConvLSTM_pytorch/blob/master/convlstm.py)\n# [Convolution_LSTM_PyTorch](https://github.com/automan000/Convolution_LSTM_PyTorch)\n\nclass ConvLSTMCell(nn.Module):\n    def __init__(self, input_size, input_dim, hidden_dim, kernel_size):\n        """"""\n        input_size: \xef\xbc\x88height, width\xef\xbc\x89\xe8\xbe\x93\xe5\x85\xa52D\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\n        input_dim: \xe8\xbe\x93\xe5\x85\xa5\xe7\xbb\xb4\xe5\xba\xa6\n        hidden_dim: hidden_dim\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\xbb\xb4\xe5\xba\xa6\n        kernel_size\xef\xbc\x9a\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xa4\xa7\xe5\xb0\x8f\n        """"""\n        super(ConvLSTMCell, self).__init__()\n        self.height, self.width = input_size\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.kernel_size = kernel_size\n        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim, out_channels=4 * self.hidden_dim, kernel_size=self.kernel_size, padding=self.padding)\n\n    def forward(self, x, cur_state):\n        h_cur, c_cur = cur_state\n        combined = torch.cat([x, h_cur], dim=1)\n\n        combined_conv = self.conv(combined)\n        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n        i = torch.sigmoid(cc_i)\n        f = torch.sigmoid(cc_f)\n        o = torch.sigmoid(cc_o)\n        g = torch.tanh(cc_g)\n\n        c_next = f * c_cur + i * g\n        h_next = o * torch.tanh(c_next)\n\n        return h_next, c_next\n\n    def init_hidden(self, batch_size):\n        h_init = Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width))\n        c_init = Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width))\n        return h_init, c_init\n\n\nclass ConvLSTM(nn.Module):\n    def __init__(self, input_size, input_dim, hidden_dim, kernel_size):\n        """"""\n        input_size: \xef\xbc\x88height, width\xef\xbc\x89\xe8\xbe\x93\xe5\x85\xa52D\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\n        input_dim: \xe8\xbe\x93\xe5\x85\xa5\xe7\xbb\xb4\xe5\xba\xa6\n        hidden_dim: [dim1, dim2, dim3]\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\xbb\xb4\xe5\xba\xa6\n        kernel_size\xef\xbc\x9a\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xa4\xa7\xe5\xb0\x8f\n        """"""\n        super(ConvLSTM, self).__init__()\n        self.height, self.width = input_size\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = len(self.hidden_dim)\n        self.kernel_size = [kernel_size] * self.num_layers\n\n        cell_list = []\n        for i in range(0, self.num_layers):\n            # \xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe6\x98\xafinput_dim\n            if i == 0:\n                cur_input_dim = self.input_dim\n            else:\n                cur_input_dim = self.hidden_dim[i - 1]\n            cell_list.append(ConvLSTMCell(input_size=(self.height, self.width), input_dim=cur_input_dim, hidden_dim=self.hidden_dim[i], kernel_size=self.kernel_size[i]))\n\n        self.cell_list = nn.ModuleList(cell_list)\n\n\n    def forward(self, x):\n        """"""\n        :param x: (t, b, c, h, w)\n        :return:\n        """"""\n        x.permute(1, 0, 2, 3, 4) # (t, b, c, h, w) to (b, t, c, h, w)\n        batch_size, seq_len,  _, _, _ = x.size()\n        hidden_state = self.init_hidden(batch_size=batch_size)\n\n        layer_output_list = []\n        last_state_list = []\n\n        cur_layer_input = x\n\n        for layer_idx in range(self.num_layers):\n            h, c = hidden_state[layer_idx]\n            output_inner = []\n            for t in range(seq_len):\n                h, c = self.cell_list[layer_idx](x=cur_layer_input[:, t, :, :, :], cur_state=[h, c])\n                output_inner.append(h)\n\n            layer_output = torch.stack(output_inner, dim=1)\n            cur_layer_input = layer_output\n\n            layer_output_list.append(layer_output)\n            last_state_list.append([h, c])\n\n        return layer_output_list[-1], last_state_list[-1]\n\n    def init_hidden(self, batch_size):\n        init_states = []\n        for i in range(self.num_layers):\n            init_states.append(self.cell_list[i].init_hidden(batch_size))\n        return init_states\n\n\nif __name__ == \'__main__\':\n    # convlstm = ConvLSTM(input_size=(32, 32), input_dim=3, hidden_dim=[64, 64, 128], kernel_size=(3, 3), num_layers=3, batch_first=True, bias=True, return_all_layers=False)\n    convlstm = ConvLSTM(input_size=(64, 64), input_dim=3, hidden_dim=[64, 64, 128], kernel_size=(3, 3))\n    input_x = Variable(torch.randn(5, 2, 3, 64, 64)) # (t, b, c, h, w)\n    layer_output_list, last_state_list = convlstm(input_x)\n    # print(\'layer_output_list:\', layer_output_list)\n    # print(\'last_state_list:\', last_state_list)\n'"
lenet.py,4,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport torch\nimport numpy as np\nfrom scipy import misc\nimport matplotlib.pyplot as plt\n\n\nfrom layers.module import Module\nfrom layers.utils import im2col\nfrom layers import conv, pool, activation, linear, loss\n\nif __name__ == '__main__':\n    batch_size = 100\n    input_np = np.random.rand(batch_size, 1, 28, 28)\n    target_np = np.random.randint(10, size=batch_size)\n    print('input_np.shape:', input_np.shape)\n    print('target_np.shape:', target_np.shape)\n    conv1 = conv.Conv2d(in_channels=1, out_channels=20, kernel_size=5)\n    maxpool1 = pool.MaxPool2d(kernel_size=2, stride=2)\n    relu1 = activation.ReLU()\n    conv2 = conv.Conv2d(in_channels=20, out_channels=50, kernel_size=5)\n    maxpool2 = pool.MaxPool2d(kernel_size=2, stride=2)\n    relu2 = activation.ReLU()\n\n    # fc1 = linear.Linear(out_flatten.shape[1], 500)\n    fc1 = linear.Linear(800, 500)\n    fc2 = linear.Linear(500, 10)\n    loss_layer = loss.CrossEntropyLoss()\n\n\n    for epoch in range(1000):\n        out = conv1.forward(input_np)\n        out = maxpool1.forward(out)\n        out = relu1.forward(out)\n        out = conv2.forward(out)\n        out = maxpool2.forward(out)\n        out = relu2.forward(out)\n        # print('out.shape:', out.shape)\n        reshape_to_flatten = out.shape\n        out_flatten = out.reshape(batch_size, -1)\n        # print('out_flatten.shape:', out_flatten.shape)\n\n        out = fc1.forward(out_flatten)\n        out = fc2.forward(out)\n\n        # print('out:', out)\n        # print('out.shape:', out.shape)\n        loss_val = loss_layer.forward_loss(out, target_np)\n        print('loss_val:', loss_val)\n\n        theta = loss_layer.calc_gradient_loss()\n        theta = fc2.calc_gradient(theta)\n        theta = fc1.calc_gradient(theta)\n        theta = theta.reshape(reshape_to_flatten)\n        # print('theta:', theta)\n        theta = maxpool2.calc_gradient(theta)\n        theta = conv2.calc_gradient(theta)\n        theta = relu1.calc_gradient(theta)\n        theta = maxpool1.calc_gradient(theta)\n        theta = conv1.calc_gradient(theta)\n\n        fc2.backward()\n        fc1.backward()\n        relu2.backward()\n        maxpool2.backward()\n        conv2.backward()\n        relu1.backward()\n        maxpool1.backward()\n        conv1.backward()\n"""
rnn.py,0,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nif __name__ == '__main__':\n    pass\n"""
layers/__init__.py,0,b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n'
layers/activation.py,8,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport numpy as np\n\nfrom module import Module\n\nclass ReLU(Module):\n    def __init__(self):\n        pass\n\n    def forward(self, x):\n        self.input_map = x\n        relu_out = np.maximum(x, 0)\n        return relu_out\n\n    def calc_gradient(self, error):\n        self.error = error\n        next_error = np.copy(error)\n        next_error[self.input_map<0]=0\n        return next_error\n\n    def backward(self, lr=0.01):\n        pass\n\nclass Softmax(Module):\n    def __init__(self):\n        self.batch_size = 1\n        self.in_features = 1\n        self.out_features = 1\n        self.softmax_out = None\n        pass\n\n    def forward(self, x):\n        self.input_map = x\n        self.batch_size, self.in_features = x.shape\n        self.out_features = self.in_features\n        self.softmax_out = np.zeros((self.batch_size, self.out_features))\n        for batch_i in range(self.batch_size):\n            x_batch_i = x[batch_i, :]\n            x_batch_i_exp = np.exp(x_batch_i)\n            x_batch_i_exp_sum = sum(x_batch_i_exp)\n            self.softmax_out[batch_i, :] = x_batch_i_exp / x_batch_i_exp_sum\n        return self.softmax_out\n\n\n    def calc_gradient(self, error):\n        self.error = error\n        next_error = np.zeros(self.input_map.shape)\n        gradient_ij = np.zeros((self.in_features, self.in_features))\n        for batch_i in range(self.batch_size):\n            error_batch_i = error[batch_i, :]\n            softmax_out_batch_i = self.softmax_out[batch_i, :]\n            for i in range(self.in_features):\n                for j in range(self.in_features):\n                    if i==j:\n                        gradient_ij[i, j] = softmax_out_batch_i[i] * (1-softmax_out_batch_i[j])\n                    else:\n                        gradient_ij[i, j] = - softmax_out_batch_i[i] * softmax_out_batch_i[j]\n            next_error[batch_i] = np.dot(gradient_ij, error_batch_i)\n            # print('gradient_ij:', gradient_ij)\n            # print('error_batch_i:', error_batch_i)\n        return next_error\n\n    def backward(self, lr=0.01):\n        pass\n\nclass LogSoftmax(Module):\n    def __init__(self):\n        self.batch_size = 1\n        self.in_features = 1\n        self.out_features = 1\n        self.softmax_out = None\n        self.logsoftmax_out = None\n        pass\n\n    def forward(self, x):\n        self.input_map = x\n        self.softmax_module = Softmax()\n        self.softmax_out = self.softmax_module.forward(x)\n        self.logsoftmax_out = np.log(self.softmax_out)\n        return self.logsoftmax_out\n\n\n    def calc_gradient(self, error):\n        self.error = error\n        error_1 = 1.0/self.softmax_out * error\n        next_error = self.softmax_module.calc_gradient(error_1)\n        return next_error\n\n    def backward(self, lr=0.01):\n        pass\n"""
layers/conv.py,17,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport torch\nimport numpy as np\nfrom scipy import misc\nimport matplotlib.pyplot as plt\nimport math\n\nfrom module import Module\nfrom utils import im2col\n\nclass Conv2d(Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, init_params=False):\n        """"""\n        :param in_channels: (int) the input channel\n        :param out_channels: (int) the output channel\n        :param kernel_size: (int) the kernel size\n        :param stride: (int) the stirde\n        """"""\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.input_h = None\n        self.input_w = None\n        self.out_h = None\n        self.out_w = None\n\n        self.weight_gradient = 0\n        self.bias_gradient = 0\n\n        self.init_params = init_params\n\n        self.weight = np.random.randn(self.in_channels, self.out_channels, self.kernel_size, self.kernel_size)\n        self.bias = np.random.randn(self.out_channels, 1)\n\n\n        # \xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84batch_size=N\xef\xbc\x8c\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba1\n        self.batch_size = 1\n\n\n\n    def forward(self, x):\n        """"""\n        :param x: (N, C_in, H_in, W_in) \xe9\x80\x9a\xe9\x81\x93*\xe9\xab\x98\xe5\xba\xa6*\xe5\xae\xbd\xe5\xba\xa6\n        :return: \n        """"""\n        self.input_map = x\n\n        if not self.init_params:\n            self.init_params = True\n            weights_scale = math.sqrt(reduce(lambda x, y: x * y, self.input_map.shape) / self.out_channels)\n\n            self.weight = np.random.standard_normal(\n                size=(self.in_channels, self.out_channels, self.kernel_size, self.kernel_size)) / weights_scale\n            self.bias = np.random.standard_normal(size=(self.out_channels, 1)) / weights_scale\n\n        self.batch_size, _, self.input_h, self.input_w = x.shape\n\n        self.out_h = (self.input_h-self.kernel_size)/self.stride + 1\n        self.out_w = (self.input_w-self.kernel_size)/self.stride + 1\n        # print(\'out_h:\', self.out_h)\n        # print(\'out_w:\', self.out_w)\n\n        # \xe5\x9b\xbe\xe5\x83\x8f\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x8cN*(H*W)*(C*K*K)\n        self.col_images = []\n\n        weight_col = self.weight.reshape(self.out_channels, -1)\n        # N * C_out * H_out * W_out\n        conv_out = np.zeros((self.batch_size, self.out_channels, self.out_h, self.out_w))\n        for batch_i in range(self.batch_size):\n            # \xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe7\xac\xaci\xe4\xb8\xaa\xe5\x9b\xbe\xe5\x83\x8fC_in*H_in*W_in\n            image_batch_i = x[batch_i, :]\n            image_batch_i_col = im2col(image_batch_i, self.kernel_size, self.stride)\n\n            self.col_images.append(image_batch_i_col)\n            # print(image_batch_i_col.shape)\n            # print(weight_col.shape)\n            conv_out[batch_i] = np.reshape(np.dot(weight_col, np.transpose(image_batch_i_col))+self.bias, (self.out_channels, self.out_h, self.out_w))\n\n        self.col_images = np.array(self.col_images)\n\n        return conv_out\n\n    # \xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe5\x90\x8c\xe6\x97\xb6\xe5\xb0\x86\xe8\xaf\xaf\xe5\xb7\xae\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xae\xa1\xe7\xae\x97\xe5\x87\xba\xe6\x9d\xa5\xef\xbc\x8c\xe6\xa0\xb9\xe6\x8d\xae\xe5\xbd\x93\xe5\x89\x8d\xe8\xaf\xaf\xe5\xb7\xae\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x8a\xe4\xb8\x80\xe8\xaf\xaf\xe5\xb7\xae\n    def calc_gradient(self, error):\n        self.error = error\n        error_col = self.error.reshape(self.batch_size, self.out_channels, -1)\n        # print(\'self.col_images.shape:\', self.col_images.shape)\n        # print(\'error_col.shape:\', error_col.shape)\n        # print(\'error.shape:\', error.shape)\n\n        for batch_i in range(self.batch_size):\n            self.weight_gradient += np.dot(error_col[batch_i], self.col_images[batch_i]).reshape(self.weight.shape)\n        # \xe5\xb0\x86\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe7\x9b\xb8\xe5\x8a\xa0\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe5\xb0\x86N\xe5\x92\x8c\xe6\x9c\x80\xe5\x90\x8e\xe6\xb1\x82\xe5\x92\x8c\n        self.bias_gradient += np.sum(error_col, axis=(0, 2)).reshape(self.bias.shape)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82error\n\n        error_pad = np.pad(self.error, ((0, 0), (0, 0), (self.kernel_size - 1, self.kernel_size - 1), (self.kernel_size - 1, self.kernel_size - 1)), \'constant\', constant_values=0)\n        # print(\'error_pad.shape:\', error_pad.shape)\n        # print(\'error:\', error)\n        # print(\'error_pad:\', error_pad)\n\n        weight_flip = self.weight[:, :, ::-1, ::-1]\n        weight_flip = np.swapaxes(weight_flip, 0, 1)\n        weight_flip_col = weight_flip.reshape(self.in_channels, -1)\n        # print(\'weight_flip_col.shape:\', weight_flip_col.shape)\n\n\n        next_error = np.zeros((self.batch_size, self.in_channels, self.input_h, self.input_w))\n        for batch_i in range(self.batch_size):\n            # \xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe7\xac\xaci\xe4\xb8\xaa\xe5\x9b\xbe\xe5\x83\x8fC_in*H_in*W_in\n            error_pad_image_batch_i = error_pad[batch_i, :]\n            error_pad_image_batch_i_col = im2col(error_pad_image_batch_i, self.kernel_size, self.stride)\n            # print(\'error_pad_image_batch_i_col.shape:\', error_pad_image_batch_i_col.shape)\n            next_error[batch_i] = np.reshape(np.dot(weight_flip_col, np.transpose(error_pad_image_batch_i_col)), (self.in_channels, self.input_h, self.input_w))\n\n\n        # print(\'error_pad_image_col.shape:\', error_pad_image_col.shape)\n        # print(\'error_pad_image_col.shape:\', error_pad_image_col.shape)\n        # next_error = np.dot(error_pad_image_col, np.transpose(weight_flip_col)).reshape(self.batch_size, self.in_channels, self.input_h, self.input_w)\n        # print(\'next_error.shape:\', next_error.shape)\n        #\n        # conv_out[batch_i] = np.reshape(np.dot(weight_col, np.transpose(image_batch_i_col)) + self.bias,\n        #                                (self.out_channels, self.out_h, self.out_w))\n\n        return next_error\n\n\n    def backward(self, lr=0.01):\n        self.weight -= lr*self.weight_gradient\n        self.bias -= lr*self.bias_gradient\n\n        self.weight_gradient = 0\n        self.bias_gradient = 0\n\n\nif __name__ == \'__main__\':\n    # img = misc.face()\n    # plt.imshow(img)\n    # plt.show()\n    # channel first\n    img = np.ones((64, 64, 3))\n    img = img.transpose((2, 0, 1))\n    img = img[np.newaxis, :]\n    print(img.shape)\n    conv1 = Conv2d(in_channels=3, out_channels=5, kernel_size=3)\n    conv1_forward = conv1.forward(img)\n    print(conv1_forward.shape)\n\n    conv1_forward_real = conv1_forward.copy() + 1\n    conv1.calc_gradient(conv1_forward_real-conv1_forward)\n    # \xe5\xa4\x9a\xe6\xac\xa1\xe6\xa2\xaf\xe5\xba\xa6\xe8\xae\xa1\xe7\xae\x97\n    # conv1.calc_gradient(conv1_forward_real-conv1_forward)\n\n    conv1.backward()\n\n    # img_0_col = im2col(img[0, :], kernel_size=3, stride=1)\n    # print(img_0_col.shape)\n'"
layers/linear.py,6,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\nimport numpy as np\n\nfrom module import Module\n\nclass Linear(Module):\n    def __init__(self, in_features, out_features, init_params=False):\n        """"""\n        :param in_features: (int) \xe8\xbe\x93\xe5\x85\xa5\xe7\x89\xb9\xe5\xbe\x81\xe6\x95\xb0\xe9\x87\x8f\n        :param out_features: (int) \xe8\xbe\x93\xe5\x87\xba\xe7\x89\xb9\xe5\xbe\x81\xe6\x95\xb0\xe9\x87\x8f\n        """"""\n        self.in_features = in_features\n        self.out_features = out_features\n\n        self.weight_gradient = 0\n        self.bias_gradient = 0\n\n        self.init_params = init_params\n\n        self.weight = np.random.standard_normal(size=(self.out_features, self.in_features))/100\n        self.bias = np.random.standard_normal(size=self.out_features)/100\n\n    def forward(self, x):\n        """"""\n        :param x: (N, in_features) batch_size*\xe8\xbe\x93\xe5\x85\xa5\xe7\x89\xb9\xe5\xbe\x81\xe6\x95\xb0\xe9\x87\x8f\n        :return: linear_out: (N, out_features) batch_size*\xe8\xbe\x93\xe5\x87\xba\xe7\x89\xb9\xe5\xbe\x81\xe6\x95\xb0\xe9\x87\x8f\n        """"""\n        self.input_map = x\n        linear_out = np.dot(x, np.transpose(self.weight)) + self.bias\n        return linear_out\n\n    def calc_gradient(self, error):\n        """"""\n        :param error: (N, out_features) batch_size*\xe8\xbe\x93\xe5\x87\xba\xe7\x89\xb9\xe5\xbe\x81\xe6\x95\xb0\xe9\x87\x8f\n        :return: \n        """"""\n        self.error = error\n        self.weight_gradient = np.dot(np.transpose(error), self.input_map)\n        self.bias_gradient = np.sum(np.transpose(self.error), axis=1)\n\n        next_error = np.dot(error, self.weight)\n        return next_error\n\n    def backward(self, lr=0.01):\n        self.weight -= lr*self.weight_gradient\n        self.bias -= lr*self.bias_gradient\n\n        self.weight_gradient = 0\n        self.bias_gradient = 0\n'"
layers/loss.py,1,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport numpy as np\n\nfrom module import Module\nfrom activation import LogSoftmax\nfrom utils import get_one_hot\n\nclass CrossEntropyLoss(Module):\n    def backward(self, lr=0.01):\n        pass\n\n    def calc_gradient(self, error):\n        pass\n\n\n    def forward(self, x):\n        pass\n\n    def forward_loss(self, input, target):\n        self.input = input\n        self.target = target\n\n        self.logsoftmax_module = LogSoftmax()\n        self.logsoftmax_out = self.logsoftmax_module.forward(self.input)\n        self.batch_size, self.target_num = self.input.shape\n\n        # print(\'target:\', self.target)\n        # print(\'batch_size:\', self.batch_size)\n        # print(\'target_num:\', self.target_num)\n\n        self.target_one_hot = get_one_hot(self.target, self.target_num)\n        # print(\'target_one_hot:\', self.target_one_hot)\n        nll_log = -self.logsoftmax_out*self.target_one_hot\n        # print(\'nll_log:\', nll_log)\n        return 1.0/self.batch_size * np.sum(nll_log)\n\n    def calc_gradient_loss(self):\n        error1 = -self.target_one_hot\n        next_error = self.logsoftmax_module.calc_gradient(error1)\n        return 1.0/self.batch_size*next_error\n\n    def __init__(self):\n        """"""\n        \xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5loss\xef\xbc\x8closs\n        """"""\n        self.batch_size = 1\n        self.target_num = 1\n'"
layers/module.py,0,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport abc\n\nclass Module(object):\n    __metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def forward(self, x):\n        pass\n\n    @abc.abstractmethod\n    def calc_gradient(self, error):\n        pass\n\n    @abc.abstractmethod\n    def backward(self, lr=0.01):\n        pass\n'"
layers/pool.py,5,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport numpy as np\n\nfrom module import Module\nfrom utils import im2col\n\nclass MaxPool2d(Module):\n    def __init__(self, kernel_size, stride=None):\n        self.kernel_size = kernel_size\n        self.stride = stride or kernel_size\n\n        # \xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84batch_size=N\xef\xbc\x8c\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba1\n        self.batch_size = 1\n\n        self.in_channels = None\n        self.out_channels = None\n        self.input_h = None\n        self.input_w = None\n        self.out_h = None\n        self.out_w = None\n\n        self.indices = None\n\n    def calc_gradient(self, error):\n        self.error = error\n        next_error = np.repeat(np.repeat(self.error , self.stride, axis=3), self.stride, axis=2) * self.indices\n        return next_error\n\n    def backward(self, lr=0.01):\n        pass\n\n    def forward(self, x):\n        """"""\n        :param x: (N, C_in, H_in, W_in) batch_size*\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93*\xe9\xab\x98\xe5\xba\xa6*\xe5\xae\xbd\xe5\xba\xa6\n        :return: \n        """"""\n        self.input_map = x\n        self.batch_size, self.in_channels, self.input_h, self.input_w = x.shape\n        self.out_channels = self.in_channels\n\n        self.out_h = (self.input_h-self.kernel_size)/self.stride + 1\n        self.out_w = (self.input_w-self.kernel_size)/self.stride + 1\n\n        self.indices = np.zeros(self.input_map.shape)\n\n        pool_out = np.zeros((self.batch_size, self.out_channels, self.out_h, self.out_w))\n\n        for batch_i in range(self.batch_size):\n            # \xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe7\xac\xaci\xe4\xb8\xaa\xe5\x9b\xbe\xe5\x83\x8fC_in*H_in*W_in\n            image_batch_i = x[batch_i, :]\n            for channel_j in range(self.in_channels):\n                image_batch_i_channel_j = image_batch_i[channel_j, :]\n                for h_counter, h_i in enumerate(range(0, self.input_h - self.kernel_size + 1, self.stride)):\n                    for w_counter, w_i in enumerate(range(0, self.input_w - self.kernel_size + 1, self.stride)):\n                        image_batch_i_channel_j_patch = image_batch_i_channel_j[h_i:h_i+self.kernel_size, w_i:w_i+self.kernel_size]\n                        pool_out[batch_i, channel_j, h_counter, w_counter] = np.max(image_batch_i_channel_j_patch)\n                        # print(\'image_batch_i_channel_j_patch:\', image_batch_i_channel_j_patch)\n                        patch_h_max, patch_w_max = np.unravel_index(image_batch_i_channel_j_patch.argmax(), image_batch_i_channel_j_patch.shape)\n                        # print(\'patch_h_max:\', patch_h_max)\n                        # print(\'patch_w_max:\', patch_w_max)\n                        self.indices[batch_i, channel_j, h_i+patch_h_max, w_i+patch_w_max] = 1\n\n        # print(\'indices:\', self.indices.shape)\n\n        return pool_out\n'"
layers/rnn.py,13,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\nimport numpy as np\n\nfrom module import Module\n\nclass RNN(Module):\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.w_ih = np.zeros((self.hidden_size, self.input_size))\n        self.b_ih = np.zeros(self.hidden_size)\n        self.w_hh = np.zeros((self.hidden_size, self.hidden_size))\n        self.b_hh = np.zeros(self.hidden_size)\n\n        self.w_ih_gradient = np.zeros(self.w_ih.shape)\n        self.b_ih_gradient = np.zeros(self.b_ih.shape)\n        self.w_hh_gradient = np.zeros(self.w_hh.shape)\n        self.b_hh_gradient = np.zeros(self.b_hh.shape)\n\n    def calc_gradient(self, error):\n        pass\n        # for t in range(self.seq_len):\n        #     for k in range(t):\n        #         theta = 1\n        #         for i in range(k+1, t+1):\n        #             theta = np.dot(theta, np.dot(np.transpose(self.w_hh), np.diag(1-self.h_batch[i-1, 0, :])))\n        #         theta *= self.input_map[k]\n\n\n    def forward_rnn(self, x, h_init):\n        """"""\n        :param x: \xe8\xbe\x93\xe5\x85\xa5seq_len*batch*input_size\n        :param h_init: \xe9\x9a\x90\xe8\x97\x8fbatch*hidden_size\n        :return: \n        """"""\n        self.seq_len, self.batch_size, self.input_size = x.shape\n        self.input_map = x\n        self.h_batch = np.zeros((self.seq_len, self.batch_size, self.hidden_size))\n        h_prev = h_init\n        o_np = np.zeros((self.seq_len, self.batch_size, self.hidden_size))\n        h_np = np.zeros((self.batch_size, self.hidden_size))\n        for t in range(self.seq_len):\n            x_t = x[t]\n            # h_t\xe6\xa0\xbc\xe5\xbc\x8fbatch*hidden_size\n            h_t = np.tanh(np.dot(x_t, np.transpose(self.w_ih)) + np.dot(h_prev, np.transpose(self.w_hh)) + self.b_ih + self.b_hh)\n            o_t = h_t\n            h_prev = h_t\n            o_np[t] = o_t\n            h_np = h_t\n            self.h_batch[t] = h_t\n        return o_np, h_np\n\n\n    def forward(self, x):\n        pass\n\n    def backward(self, lr=0.01):\n        pass\n'"
layers/utils.py,4,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\nimport numpy as np\n\ndef im2col(img, kernel_size, stride=1):\n    """"""\n    :param img: \xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f C_in H_in W_in\n    :param kernel_size: \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xa4\xa7\xe5\xb0\x8f\n    :param stride: \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe9\x97\xb4\xe8\xb7\x9d\n    :return: img_cols (H*W) * (C*K*K)\n    """"""\n    img_channel, img_h, img_w = img.shape\n    # img_cols_lists = []\n    img_cols = None\n    # print(\'img_h\', img_h)\n    # print(\'img_w\', img_w)\n    for channel_i in range(img_channel):\n        # \xe9\x80\x9a\xe9\x81\x93i\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe6\x98\xaf H W\n        img_channel_i = img[channel_i, :]\n        img_channel_i_cols = []\n        for h_i in range(0, img_h-kernel_size+1, stride):\n            for w_i in range(0, img_w-kernel_size+1, stride):\n                img_channel_i_patch = img_channel_i[h_i:h_i+kernel_size, w_i:w_i+kernel_size]\n                # print(img_channel_i_patch.shape)\n                # \xe5\xb0\x8f\xe7\x9a\x84patch K*K reshape\xe4\xb8\xba\xe8\xa1\x8c\xe5\x90\x91\xe9\x87\x8f\n                img_channel_i_patch_row = img_channel_i_patch.reshape([-1])\n                img_channel_i_cols.append(img_channel_i_patch_row)\n                # print(img_channel_i_patch_row.shape)\n                assert img_channel_i_patch_row.shape ==  (kernel_size*kernel_size, )\n        # print(\'len(img_channel_i_cols):\', len(img_channel_i_cols))\n        img_channel_i_cols = np.array(img_channel_i_cols)\n        # print(\'img_channel_i_cols.shape:\', img_channel_i_cols.shape)\n        # if not img_cols_lists:\n        #     img_cols = img_channel_i_cols\n        # else:\n        #     img_cols = np.hstack(img_cols, img_channel_i_cols)\n        # img_cols_lists.append(img_channel_i_cols)\n        if img_cols is None:\n            img_cols = img_channel_i_cols\n        else:\n            img_cols = np.hstack((img_cols, img_channel_i_cols))\n\n    return img_cols\n\ndef get_one_hot(targets, nb_classes):\n    return np.eye(nb_classes)[np.array(targets).reshape(-1)]\n'"
test/context.py,0,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\n# \xe5\xa2\x9e\xe5\x8a\xa0\xe4\xb8\x8a\xe4\xb8\x8b\xe6\x96\x87package\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nimport layers\n"""
test/test_activation.py,14,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport unittest\nimport torch\nfrom torch import nn\nfrom torch import FloatTensor, LongTensor\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch import optim\nimport numpy as np\n\nfrom context import layers\nfrom layers import activation\n\nclass TestReLU(unittest.TestCase):\n    def test_forward(self):\n        input_var = Variable(torch.randn(3, 3, 4, 4), requires_grad=True)\n        relu_var = nn.ReLU()\n        relu_out_var = relu_var(input_var)\n\n        relu_out_var_np = relu_out_var.data.numpy()\n\n        # print('relu_out_var_np.shape:', relu_out_var_np.shape)\n        # print('relu_out_var_np:', relu_out_var_np)\n\n        input_var_np = input_var.data.numpy()\n        relu_custom = activation.ReLU()\n        relu_out_custom = relu_custom.forward(input_var_np)\n        # print('relu_out_custom.shape:', relu_out_custom.shape)\n        # print('relu_out_custom:', relu_out_custom)\n\n        assert abs(np.sum(relu_out_custom-relu_out_var_np)) < 0.0001\n\n\n    def test_grad(self):\n        input_var = Variable(torch.randn(3, 3, 4, 4), requires_grad=True)\n        relu_var = nn.ReLU()\n        relu_out_var = relu_var(input_var)\n\n        relu_out_var_np = relu_out_var.data.numpy()\n\n        out_var = relu_out_var.sum()\n        out_var.backward()\n\n        relu_out_var_np = relu_out_var.data.numpy()\n\n        relu_var_next_eta = input_var.grad.data.numpy()\n\n        # print('relu_var_next_eta:', relu_var_next_eta)\n\n        input_var_np = input_var.data.numpy()\n        # print('input_var_np:', input_var_np)\n        relu_eta = np.ones(relu_out_var_np.shape)\n\n        relu_custom = activation.ReLU()\n        relu_out_custom = relu_custom.forward(input_var_np)\n        # print('relu_out_custom:', relu_out_custom)\n\n        relu_next_eta = relu_custom.calc_gradient(relu_eta)\n        # print('relu_custom.indices:', relu_custom.indices)\n        # print('relu_next_eta:', relu_next_eta)\n\n        assert abs(np.sum(relu_next_eta-relu_var_next_eta)) < 0.0001\n\n\n    def test_speed(self):\n        pass\n\n\nclass TestSoftmax(unittest.TestCase):\n    def test_forward(self):\n        input_var = Variable(torch.randn(2, 3), requires_grad=True)\n        softmax_var = nn.Softmax()\n        softmax_out_var = softmax_var(input_var)\n\n        softmax_out_var_np = softmax_out_var.data.numpy()\n\n        # print('softmax_out_var_np.shape:', softmax_out_var_np.shape)\n        # print('softmax_out_var_np:', softmax_out_var_np)\n\n        input_var_np = input_var.data.numpy()\n        softmax_custom = activation.Softmax()\n        softmax_out_custom = softmax_custom.forward(input_var_np)\n        # print('input_var_np:', input_var_np)\n        # print('softmax_out_custom.shape:', softmax_out_custom.shape)\n        # print('softmax_out_custom:', softmax_out_custom)\n\n        assert abs(np.sum(softmax_out_custom-softmax_out_var_np)) < 0.0001\n\n\n    def test_grad(self):\n        input_var = Variable(torch.randn(2, 3), requires_grad=True)\n        softmax_var = nn.Softmax()\n        softmax_out_var = softmax_var(input_var)\n\n        softmax_out_var_np = softmax_out_var.data.numpy()\n\n        out_var = softmax_out_var.sum()\n        # out_var = softmax_out_var[:, 0].sum()\n        out_var.backward()\n\n        softmax_var_next_eta = input_var.grad.data.numpy()\n\n        # print('softmax_var_next_eta:', softmax_var_next_eta)\n\n        input_var_np = input_var.data.numpy()\n        # print('input_var_np:', input_var_np)\n        # print('softmax_out_var_np:', softmax_out_var_np)\n        softmax_eta = np.ones(softmax_out_var_np.shape)\n        # softmax_eta = np.zeros(softmax_out_var_np.shape)\n        # softmax_eta[:, 0] = 1\n\n        softmax_custom = activation.Softmax()\n        softmax_out_custom = softmax_custom.forward(input_var_np)\n        # print('softmax_out_custom:', softmax_out_custom)\n\n        softmax_next_eta = softmax_custom.calc_gradient(softmax_eta)\n        # print('softmax_custom.indices:', softmax_custom.indices)\n        # print('softmax_next_eta:', softmax_next_eta)\n\n        assert abs(np.sum(softmax_next_eta-softmax_var_next_eta)) < 0.0001\n\n\n    def test_speed(self):\n        pass\n\nclass TestLogSoftmax(unittest.TestCase):\n    def test_forward(self):\n        input_var = Variable(torch.randn(2, 3), requires_grad=True)\n        logsoftmax_var = nn.LogSoftmax()\n        logsoftmax_out_var = logsoftmax_var(input_var)\n\n        logsoftmax_out_var_np = logsoftmax_out_var.data.numpy()\n\n        # print('logsoftmax_out_var_np.shape:', logsoftmax_out_var_np.shape)\n        # print('logsoftmax_out_var_np:', logsoftmax_out_var_np)\n\n        input_var_np = input_var.data.numpy()\n        logsoftmax_custom = activation.LogSoftmax()\n        logsoftmax_out_custom = logsoftmax_custom.forward(input_var_np)\n        # print('input_var_np:', input_var_np)\n        # print('logsoftmax_out_custom.shape:', logsoftmax_out_custom.shape)\n        # print('logsoftmax_out_custom:', logsoftmax_out_custom)\n\n        assert abs(np.sum(logsoftmax_out_custom-logsoftmax_out_var_np)) < 0.0001\n\n\n    def test_grad(self):\n        input_var = Variable(torch.randn(2, 3), requires_grad=True)\n        logsoftmax_var = nn.LogSoftmax()\n        logsoftmax_out_var = logsoftmax_var(input_var)\n\n        logsoftmax_out_var_np = logsoftmax_out_var.data.numpy()\n\n        out_var = logsoftmax_out_var.sum()\n        # out_var = logsoftmax_out_var[:, 0].sum()\n        out_var.backward()\n\n        logsoftmax_var_next_eta = input_var.grad.data.numpy()\n\n        # print('logsoftmax_var_next_eta:', logsoftmax_var_next_eta)\n\n        input_var_np = input_var.data.numpy()\n        # print('input_var_np:', input_var_np)\n        # print('logsoftmax_out_var_np:', logsoftmax_out_var_np)\n        logsoftmax_eta = np.ones(logsoftmax_out_var_np.shape)\n        # logsoftmax_eta = np.zeros(logsoftmax_out_var_np.shape)\n        # logsoftmax_eta[:, 0] = 1\n\n        logsoftmax_custom = activation.LogSoftmax()\n        logsoftmax_out_custom = logsoftmax_custom.forward(input_var_np)\n        # print('logsoftmax_out_custom:', logsoftmax_out_custom)\n\n        logsoftmax_next_eta = logsoftmax_custom.calc_gradient(logsoftmax_eta)\n        # print('logsoftmax_custom.indices:', logsoftmax_custom.indices)\n        # print('logsoftmax_next_eta:', logsoftmax_next_eta)\n\n        assert abs(np.sum(logsoftmax_next_eta-logsoftmax_var_next_eta)) < 0.0001\n\n\n    def test_speed(self):\n        pass\n\nif __name__ == '__main__':\n    unittest.main()\n"""
test/test_conv.py,27,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport unittest\nimport torch\nfrom torch import nn\nfrom torch import FloatTensor, LongTensor\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch import optim\nimport numpy as np\n\nfrom context import layers\nfrom layers import conv\n\nclass TestConv2d(unittest.TestCase):\n    def test_forward(self):\n        input_var = Variable(torch.randn(3, 3, 5, 5))\n        conv_var = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3)\n        output_var = conv_var(input_var)\n\n        # \xe8\xbd\xac\xe6\x8d\xa2torch\xe5\x8f\x98\xe9\x87\x8f\xe4\xb8\xbanumpy\n        output_np = output_var.data.numpy()\n        input_np = input_var.data.numpy()\n        conv_weight_np = conv_var.weight.data.numpy()\n        conv_bias_np = conv_var.bias.data.numpy()\n        conv_bias_np_vector = np.reshape(conv_bias_np, (conv_bias_np.shape + (1,)))\n\n        # print('conv_weight_np.shape:', conv_weight_np.shape)\n        # print('conv_bias_np.shape:', conv_bias_np.shape)\n        # print('conv_bias_np_vector.shape:', conv_bias_np_vector.shape)\n        # print(output_np)\n\n        conv_custom = conv.Conv2d(in_channels=3, out_channels=3, kernel_size=3, init_params=True)\n\n        conv_custom_weight = conv_custom.weight\n        conv_custom_bias = conv_custom.bias\n        # print('conv_custom_weight.shape:', conv_custom_weight.shape)\n        # print('conv_custom_bias.shape:', conv_custom_bias.shape)\n        np.copyto(dst=conv_custom_weight, src=conv_weight_np)\n        np.copyto(dst=conv_custom_bias, src=conv_bias_np_vector)\n        assert np.array_equal(conv_custom_weight, conv_weight_np)\n        assert np.array_equal(conv_custom_bias, conv_bias_np_vector)\n\n        output_custom = conv_custom.forward(input_np)\n        # print(output_custom)\n\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe8\xae\xa1\xe7\xae\x97\xe7\xbb\x93\xe6\x9e\x9c\xe7\x9b\xb8\xe5\x90\x8c\n        assert abs(np.sum(output_custom-output_np)) < 0.0001\n\n\n        # print(output_custom)\n\n    def test_grad(self):\n        input_var = Variable(torch.randn(3, 3, 5, 5).float(), requires_grad=True)\n        conv_var = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3)\n        conv_out_var = conv_var(input_var)\n\n        conv_out_flatten_shape = 1\n        # shape[0]\xe4\xb8\xbabatch_size\n        for conv_out_var_shape in conv_out_var.data.shape[1:]:\n            conv_out_flatten_shape *= conv_out_var_shape\n\n\n        # output_var = conv_out_var[:, 0, 0, 0].sum()\n        output_var = conv_out_var.sum()\n        # print('output_var.data.shape:', output_var.data.shape)\n        # print('conv_out_var.data.shape:', conv_out_var.data.shape)\n        output_var.backward()\n        conv_var_weight_grad = conv_var.weight.grad\n        conv_var_bias_grad = conv_var.bias.grad\n        conv_var_next_eta = input_var.grad\n\n        # \xe8\xbd\xac\xe6\x8d\xa2torch\xe5\x8f\x98\xe9\x87\x8f\xe4\xb8\xbanumpy\n        input_np = input_var.data.numpy()\n        conv_weight_np = conv_var.weight.data.numpy()\n        conv_bias_np = conv_var.bias.data.numpy()\n        conv_bias_np_vector = np.reshape(conv_bias_np, (conv_bias_np.shape + (1,)))\n        conv_var_weight_grad_np = conv_var_weight_grad.data.numpy()\n        conv_var_bias_grad_np = conv_var_bias_grad.data.numpy()\n        conv_var_bias_grad_np_vector = np.reshape(conv_var_bias_grad_np, (conv_var_bias_grad_np.shape + (1,)))\n        conv_out_var_np = conv_out_var.data.numpy()\n        conv_var_next_eta_np = conv_var_next_eta.data.numpy()\n\n        # print('conv_weight_np.shape:', conv_weight_np.shape)\n        # print('conv_bias_np.shape:', conv_bias_np.shape)\n        # print('conv_bias_np_vector.shape:', conv_bias_np_vector.shape)\n        # print('conv_var_weight_grad_np.shape:', conv_var_weight_grad_np.shape)\n        # print('conv_var_bias_grad_np.shape:', conv_var_bias_grad_np.shape)\n        # print('conv_var_bias_grad_np_vector.shape:', conv_var_bias_grad_np_vector.shape)\n        # print('conv_var_weight_grad_np:', conv_var_weight_grad_np)\n        # print('conv_var_bias_grad_np:', conv_var_bias_grad_np)\n        # print('conv_var_bias_grad_np_vector:', conv_var_bias_grad_np_vector)\n        # print('conv_var_next_eta_np:', conv_var_next_eta_np)\n        # print('conv_var_next_eta_np.shape:', conv_var_next_eta_np.shape)\n\n        conv_custom = conv.Conv2d(in_channels=3, out_channels=3, kernel_size=3, init_params=True)\n        conv_eta = np.ones(conv_out_var_np.shape)\n        # conv_eta = np.zeros(conv_out_var_np.shape)\n        # conv_eta[:, 0, 0, 0] = 1\n        # print('conv_eta.shape:', conv_eta.shape)\n\n        conv_custom_weight = conv_custom.weight\n        conv_custom_bias = conv_custom.bias\n        # print('conv_custom_weight.shape:', conv_custom_weight.shape)\n        # print('conv_custom_bias.shape:', conv_custom_bias.shape)\n        np.copyto(dst=conv_custom_weight, src=conv_weight_np)\n        np.copyto(dst=conv_custom_bias, src=conv_bias_np_vector)\n        assert np.array_equal(conv_custom_weight, conv_weight_np)\n        assert np.array_equal(conv_custom_bias, conv_bias_np_vector)\n\n        output_custom = conv_custom.forward(input_np)\n        assert abs(np.sum(output_custom-conv_out_var_np)) < 0.0001\n\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x85\xac\xe5\xbc\x8f\n        conv_custom_next_eta = conv_custom.calc_gradient(conv_eta)\n        conv_custom_weight_grad = conv_custom.weight_gradient\n        conv_custom_bias_grad = conv_custom.bias_gradient\n        # print('conv_next_eta:', conv_next_eta)\n        # print(input_np[0, 0, 0, 0]+input_np[0, 0, 0, 1]+input_np[0, 0, 0, 2]+input_np[0, 0, 1, 0]+input_np[0, 0, 1, 1]+input_np[0, 0, 1, 2]+input_np[0, 0, 2, 0]+input_np[0, 0, 2, 1]+input_np[0, 0, 2, 2])\n        # print('conv_custom_weight_grad:', conv_custom_weight_grad)\n        # print('conv_custom_bias_grad:', conv_custom_bias_grad)\n        assert abs(np.sum(conv_custom_weight_grad-conv_var_weight_grad_np)) < 0.0001\n        assert abs(np.sum(conv_custom_bias_grad-conv_var_bias_grad_np_vector)) < 0.0001\n        assert abs(np.sum(conv_custom_next_eta-conv_var_next_eta_np)) < 0.0001\n\n    def test_speed(self):\n        import time\n        img = np.ones((64, 64, 3))\n        img = img.transpose((2, 0, 1))\n        img = img[np.newaxis, :]\n        conv1 = layers.conv.Conv2d(in_channels=3, out_channels=5, kernel_size=3)\n        start_time = time.time()\n        conv1_forward = conv1.forward(img)\n        end_time = time.time()\n        print('forward time:', end_time-start_time)\n\n        conv1_forward_real = conv1_forward.copy() + 1\n        start_time = time.time()\n        conv1.calc_gradient(conv1_forward_real - conv1_forward)\n        conv1.backward()\n        end_time = time.time()\n        print('backward time:', end_time-start_time)\n\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
test/test_linear.py,15,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport unittest\nimport torch\nfrom torch import nn\nfrom torch import FloatTensor, LongTensor\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch import optim\nimport numpy as np\n\nfrom context import layers\nfrom layers import linear\n\nclass TestLinear(unittest.TestCase):\n    def test_forward(self):\n        input_var = Variable(torch.randn(128, 20))\n        linear_var = nn.Linear(20, 30)\n        linear_out_var = linear_var(input_var)\n\n        linear_var_weight_np = linear_var.weight.data.numpy()\n        linear_var_bias_np = linear_var.bias.data.numpy()\n\n        input_var_np = input_var.data.numpy()\n        linear_out_var_np = linear_out_var.data.numpy()\n        # print('linear_out_var_np.shape', linear_out_var_np.shape)\n\n        linear_custom = linear.Linear(20, 30)\n\n        np.copyto(linear_custom.weight, linear_var_weight_np)\n        np.copyto(linear_custom.bias, linear_var_bias_np)\n\n        linear_out_custom = linear_custom.forward(input_var_np)\n        # print('linear_out_custom.shape', linear_out_custom.shape)\n\n\n        assert abs(np.sum(linear_out_custom - linear_out_var_np)) < 0.0001\n\n    def test_grad(self):\n        input_var = Variable(torch.randn(128, 20), requires_grad=True)\n        linear_var = nn.Linear(20, 30)\n        linear_out_var = linear_var(input_var)\n\n        output_var = linear_out_var.sum()\n        # output_var = linear_out_var[:, 0].sum()\n        output_var.backward()\n\n        linear_var_weight_np = linear_var.weight.data.numpy()\n        linear_var_bias_np = linear_var.bias.data.numpy()\n\n        linear_var_weight_grad_np = linear_var.weight.grad.data.numpy()\n        linear_var_bias_grad_np = linear_var.bias.grad.data.numpy()\n        linear_var_next_eta_np = input_var.grad.data.numpy()\n\n        input_var_np = input_var.data.numpy()\n        linear_out_var_np = linear_out_var.data.numpy()\n        # print('linear_out_var_np.shape', linear_out_var_np.shape)\n        # print('linear_var_weight_grad_np.shape:', linear_var_weight_grad_np.shape)\n        # print('linear_var_bias_grad_np.shape:', linear_var_bias_grad_np.shape)\n        # print('linear_var_weight_grad_np:', linear_var_weight_grad_np)\n        # print('linear_var_bias_grad_np:', linear_var_bias_grad_np)\n\n        linear_custom = linear.Linear(20, 30)\n\n        np.copyto(linear_custom.weight, linear_var_weight_np)\n        np.copyto(linear_custom.bias, linear_var_bias_np)\n\n        linear_out_custom = linear_custom.forward(input_var_np)\n        # print('linear_out_custom.shape', linear_out_custom.shape)\n\n        assert abs(np.sum(linear_out_custom - linear_out_var_np)) < 0.0001\n\n        linear_eta = np.ones(linear_out_var_np.shape)\n        # linear_eta = np.zeros(linear_out_var_np.shape)\n        # linear_eta[:, 0] = 1\n\n        linear_custom_next_eta = linear_custom.calc_gradient(linear_eta)\n        linear_custom_weight_grad = linear_custom.weight_gradient\n        linear_custom_bias_grad = linear_custom.bias_gradient\n\n        # print('linear_custom_weight_grad.shape:', linear_custom_weight_grad.shape)\n        # print('linear_custom_bias_grad.shape:', linear_custom_bias_grad.shape)\n        # print('linear_custom_weight_grad:', linear_custom_weight_grad)\n        # print('linear_custom_bias_grad:', linear_custom_bias_grad)\n\n        assert abs(np.sum(linear_custom_weight_grad-linear_custom_weight_grad)) < 0.0001\n        assert abs(np.sum(linear_custom_bias_grad-linear_var_bias_grad_np)) < 0.0001\n        assert abs(np.sum(linear_custom_next_eta-linear_var_next_eta_np)) < 0.0001\n\nif __name__ == '__main__':\n    unittest.main()\n"""
test/test_loss.py,4,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport unittest\nimport torch\nfrom torch import nn\nfrom torch import FloatTensor, LongTensor\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch import optim\nimport numpy as np\n\nfrom context import layers\nfrom layers import loss\n\nclass TestCrossEntropyLoss(unittest.TestCase):\n    def test_forward(self):\n        input_var = Variable(torch.randn(3, 5), requires_grad=True)\n        target_var = Variable(torch.LongTensor(3).random_(5))\n        cross_entropy_loss_var = nn.CrossEntropyLoss()\n        cross_entropy_loss_out_var = cross_entropy_loss_var(input_var, target_var)\n\n        cross_entropy_loss_out_var.backward()\n\n        # input_var_grad_np = input_var.grad.data.numpy()\n        # print('input_var_grad_np:', input_var_grad_np)\n\n        cross_entropy_loss_out_var_np = cross_entropy_loss_out_var.data.numpy()\n\n        # print('cross_entropy_loss_out_var_np.shape:', cross_entropy_loss_out_var_np.shape)\n        # print('cross_entropy_loss_out_var_np:', cross_entropy_loss_out_var_np)\n\n        input_var_np = input_var.data.numpy()\n        target_var_np = target_var.data.numpy()\n        # print('input_var_np:', input_var_np)\n        # print('target_var_np:', target_var_np)\n\n        cross_entropy_loss_custom = loss.CrossEntropyLoss()\n        cross_entropy_loss_out_custom = cross_entropy_loss_custom.forward_loss(input_var_np, target_var_np)\n        # print('cross_entropy_loss_out_custom.shape:', cross_entropy_loss_out_custom.shape)\n        # print('cross_entropy_loss_out_custom:', cross_entropy_loss_out_custom)\n\n        assert abs(np.sum(cross_entropy_loss_out_custom-cross_entropy_loss_out_var_np)) < 0.0001\n\n    def test_grad(self):\n        input_var = Variable(torch.randn(3, 5), requires_grad=True)\n        target_var = Variable(torch.LongTensor(3).random_(5))\n        cross_entropy_loss_var = nn.CrossEntropyLoss()\n        cross_entropy_loss_out_var = cross_entropy_loss_var(input_var, target_var)\n\n        cross_entropy_loss_out_var.backward()\n\n        # input_var_grad_np = input_var.grad.data.numpy()\n        # print('input_var_grad_np:', input_var_grad_np)\n\n        cross_entropy_loss_out_var_np = cross_entropy_loss_out_var.data.numpy()\n        cross_entropy_loss_grad_var_np = input_var.grad.data.numpy()\n\n        # print('cross_entropy_loss_out_var_np.shape:', cross_entropy_loss_out_var_np.shape)\n        # print('cross_entropy_loss_out_var_np:', cross_entropy_loss_out_var_np)\n        # print('cross_entropy_loss_grad_var_np:', cross_entropy_loss_grad_var_np)\n\n        input_var_np = input_var.data.numpy()\n        target_var_np = target_var.data.numpy()\n        # print('input_var_np:', input_var_np)\n        # print('target_var_np:', target_var_np)\n\n        cross_entropy_loss_custom = loss.CrossEntropyLoss()\n        cross_entropy_loss_out_custom = cross_entropy_loss_custom.forward_loss(input_var_np, target_var_np)\n        cross_entropy_loss_grad_custom = cross_entropy_loss_custom.calc_gradient_loss()\n        # print('cross_entropy_loss_out_custom.shape:', cross_entropy_loss_out_custom.shape)\n        # print('cross_entropy_loss_out_custom:', cross_entropy_loss_out_custom)\n        # print('cross_entropy_loss_grad_custom:', cross_entropy_loss_grad_custom)\n\n        assert abs(np.sum(cross_entropy_loss_grad_custom-cross_entropy_loss_grad_var_np)) < 0.0001\n\n\n\n    def test_speed(self):\n        pass\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
test/test_pool.py,4,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport unittest\nimport torch\nfrom torch import nn\nfrom torch import FloatTensor, LongTensor\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch import optim\nimport numpy as np\n\nfrom context import layers\nfrom layers import pool\n\nclass TestMaxPool2d(unittest.TestCase):\n    def test_forward(self):\n        input_var = Variable(torch.randn(3, 3, 4, 4), requires_grad=True)\n        maxpool_var = nn.MaxPool2d(kernel_size=2, stride=2)\n        maxpool_out_var = maxpool_var(input_var)\n\n        maxpool_out_var_np = maxpool_out_var.data.numpy()\n\n        # print('maxpool_out_var_np:', maxpool_out_var_np)\n\n        input_var_np = input_var.data.numpy()\n        maxpool_custom = pool.MaxPool2d(kernel_size=2, stride=2)\n        maxpool_out_custom = maxpool_custom.forward(input_var_np)\n        # print('maxpool_out_custom:', maxpool_out_custom)\n\n        assert abs(np.sum(maxpool_out_custom-maxpool_out_var_np)) < 0.0001\n\n\n    def test_grad(self):\n        input_var = Variable(torch.randn(3, 3, 4, 4), requires_grad=True)\n        maxpool_var = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        maxpool_out_var, maxpool_out_indices_var = maxpool_var(input_var)\n\n        maxpool_out_var_np = maxpool_out_var.data.numpy()\n\n        out_var = maxpool_out_var.sum()\n        out_var.backward()\n\n        maxpool_out_var_np = maxpool_out_var.data.numpy()\n        maxpool_out_indices_var_np = maxpool_out_indices_var.data.numpy()\n\n        maxpool_var_next_eta = input_var.grad.data.numpy()\n\n        # print('maxpool_var_next_eta:', maxpool_var_next_eta)\n        # print('maxpool_out_indices_var_np:', maxpool_out_indices_var_np)\n        # print('maxpool_out_indices_var_np.shape:', maxpool_out_indices_var_np.shape)\n\n        input_var_np = input_var.data.numpy()\n        # print('input_var_np:', input_var_np)\n        maxpool_eta = np.ones(maxpool_out_var_np.shape)\n\n        maxpool_custom = pool.MaxPool2d(kernel_size=2, stride=2)\n        maxpool_out_custom = maxpool_custom.forward(input_var_np)\n        # print('maxpool_out_custom:', maxpool_out_custom)\n\n        maxpool_next_eta = maxpool_custom.calc_gradient(maxpool_eta)\n        # print('maxpool_custom.indices:', maxpool_custom.indices)\n        # print('maxpool_next_eta:', maxpool_next_eta)\n\n        assert abs(np.sum(maxpool_next_eta-maxpool_var_next_eta)) < 0.0001\n\n\n    def test_speed(self):\n        pass\n\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
test/test_rnn.py,22,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport unittest\nimport torch\nfrom torch import nn\nfrom torch import FloatTensor, LongTensor\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch import optim\nimport numpy as np\n\nfrom context import layers\nfrom layers import rnn\n\nclass TestRNN(unittest.TestCase):\n    def test_forward(self):\n        input_size = 10\n        hidden_size = 20\n        # \xe8\xbf\x99\xe9\x87\x8c\xe4\xbb\x85\xe4\xbb\x85\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x80\xe5\xb1\x82\n        num_layers = 1\n        batch_size = 3\n        seq_len = 5\n        # input_size\xef\xbc\x8chidden_size\xef\xbc\x8cnum_layers\n        rnn_var = nn.RNN(input_size, hidden_size, num_layers)\n        # \xe8\xbe\x93\xe5\x85\xa5seq_len*batch*input_size\n        input_var = Variable(torch.randn(seq_len, batch_size, input_size))\n        # \xe9\x9a\x90\xe8\x97\x8fnum_layers * num_directions, batch, hidden_size\n        h_init_var = Variable(torch.randn(num_layers, batch_size, hidden_size))\n        o_n_var, h_n_var = rnn_var(input_var, h_init_var)\n\n        o_n_var_np = o_n_var.data.numpy()\n        h_n_var_np = h_n_var.data.numpy()[0] # \xe8\xbf\x99\xe9\x87\x8c\xe6\xaf\x94\xe8\xbe\x83\xe5\x8d\x95\xe5\xb1\x82RNN\n        # print('o_n_var_np:', o_n_var_np)\n        # print('h_n_var_np:', h_n_var_np)\n        # print('o_n_var_np.shape:', o_n_var_np.shape)\n        # print('h_n_var_np.shape:', h_n_var_np.shape)\n\n        # print(rnn_var._all_weights)\n        weight_ih_l0_var = rnn_var.weight_ih_l0\n        weight_hh_l0_var = rnn_var.weight_hh_l0\n        bias_ih_l0_var = rnn_var.bias_ih_l0\n        bias_hh_l0_var = rnn_var.bias_hh_l0\n\n        weight_ih_l0_var_np = weight_ih_l0_var.data.numpy()\n        weight_hh_l0_var_np = weight_hh_l0_var.data.numpy()\n        bias_ih_l0_var_np = bias_ih_l0_var.data.numpy()\n        bias_hh_l0_var_np = bias_hh_l0_var.data.numpy()\n\n        # print('weight_ih_l0_var_np.shape:', weight_ih_l0_var_np.shape)\n        # print('weight_hh_l0_var_np.shape:', weight_hh_l0_var_np.shape)\n        # print('bias_ih_l0_var_np.shape:', bias_ih_l0_var_np.shape)\n        # print('bias_hh_l0_var_np.shape:', bias_hh_l0_var_np.shape)\n\n\n        input_var_np = input_var.data.numpy()\n        h_init_var_np = h_init_var.data.numpy()[0]\n\n        rnn_custom = rnn.RNN(input_size, hidden_size)\n\n        np.copyto(rnn_custom.w_ih, weight_ih_l0_var_np)\n        np.copyto(rnn_custom.b_ih, bias_ih_l0_var_np)\n        np.copyto(rnn_custom.w_hh, weight_hh_l0_var_np)\n        np.copyto(rnn_custom.b_hh, bias_hh_l0_var_np)\n\n        o_n_custom, h_n_custom = rnn_custom.forward_rnn(input_var_np, h_init_var_np)\n        # print('o_n_custom:', o_n_custom)\n        # print('h_n_custom:', h_n_custom)\n        # print('o_n_custom.shape:', o_n_custom.shape)\n        # print('h_n_custom.shape:', h_n_custom.shape)\n\n        assert abs(np.sum(o_n_custom-o_n_var_np)) < 0.0001\n        assert abs(np.sum(h_n_custom-h_n_var_np)) < 0.0001\n\n    def test_grad(self):\n        input_size = 1\n        hidden_size = 2\n        # \xe8\xbf\x99\xe9\x87\x8c\xe4\xbb\x85\xe4\xbb\x85\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x80\xe5\xb1\x82\n        num_layers = 1\n        batch_size = 1\n        seq_len = 5\n        # input_size\xef\xbc\x8chidden_size\xef\xbc\x8cnum_layers\n        rnn_var = nn.RNN(input_size, hidden_size, num_layers)\n        # \xe8\xbe\x93\xe5\x85\xa5seq_len*batch*input_size\n        input_var = Variable(torch.randn(seq_len, batch_size, input_size))\n        # \xe9\x9a\x90\xe8\x97\x8fnum_layers * num_directions, batch, hidden_size\n        h_init_var = Variable(torch.randn(num_layers, batch_size, hidden_size))\n        o_n_var, h_n_var = rnn_var(input_var, h_init_var)\n\n\n\n        o_n_var_np = o_n_var.data.numpy()\n        h_n_var_np = h_n_var.data.numpy()[0]  # \xe8\xbf\x99\xe9\x87\x8c\xe6\xaf\x94\xe8\xbe\x83\xe5\x8d\x95\xe5\xb1\x82RNN\n        # print('o_n_var_np:', o_n_var_np)\n        # print('h_n_var_np:', h_n_var_np)\n        # print('o_n_var_np.shape:', o_n_var_np.shape)\n        # print('h_n_var_np.shape:', h_n_var_np.shape)\n\n        # print(rnn_var._all_weights)\n        weight_ih_l0_var = rnn_var.weight_ih_l0\n        weight_hh_l0_var = rnn_var.weight_hh_l0\n        bias_ih_l0_var = rnn_var.bias_ih_l0\n        bias_hh_l0_var = rnn_var.bias_hh_l0\n\n        weight_ih_l0_var_np = weight_ih_l0_var.data.numpy()\n        weight_hh_l0_var_np = weight_hh_l0_var.data.numpy()\n        bias_ih_l0_var_np = bias_ih_l0_var.data.numpy()\n        bias_hh_l0_var_np = bias_hh_l0_var.data.numpy()\n\n        loss = o_n_var.sum()\n        loss.backward()\n\n        weight_ih_l0_var_grad_np = weight_ih_l0_var.grad.data.numpy()\n        weight_hh_l0_var_grad_np = weight_hh_l0_var.grad.data.numpy()\n        bias_ih_l0_var_grad_np = bias_ih_l0_var.grad.data.numpy()\n        bias_hh_l0_var_grad_np = bias_hh_l0_var.grad.data.numpy()\n\n        # print('weight_ih_l0_var_grad_np:', weight_ih_l0_var_grad_np)\n\n        # print('weight_ih_l0_var_np.shape:', weight_ih_l0_var_np.shape)\n        # print('weight_hh_l0_var_np.shape:', weight_hh_l0_var_np.shape)\n        # print('bias_ih_l0_var_np.shape:', bias_ih_l0_var_np.shape)\n        # print('bias_hh_l0_var_np.shape:', bias_hh_l0_var_np.shape)\n\n\n        input_var_np = input_var.data.numpy()\n        h_init_var_np = h_init_var.data.numpy()[0]\n\n        rnn_custom = rnn.RNN(input_size, hidden_size)\n\n        np.copyto(rnn_custom.w_ih, weight_ih_l0_var_np)\n        np.copyto(rnn_custom.b_ih, bias_ih_l0_var_np)\n        np.copyto(rnn_custom.w_hh, weight_hh_l0_var_np)\n        np.copyto(rnn_custom.b_hh, bias_hh_l0_var_np)\n\n        o_n_custom, h_n_custom = rnn_custom.forward_rnn(input_var_np, h_init_var_np)\n        # print('o_n_custom:', o_n_custom)\n        # print('h_n_custom:', h_n_custom)\n        # print('o_n_custom.shape:', o_n_custom.shape)\n        # print('h_n_custom.shape:', h_n_custom.shape)\n\n\n    def test_speed(self):\n        pass\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
