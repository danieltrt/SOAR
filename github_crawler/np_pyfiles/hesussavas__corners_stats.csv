file_path,api_count,code
analysis.py,5,"b'import random\nfrom collections import defaultdict, OrderedDict\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom corners442.corners442.db import engine\n\n\n# -- ANALYSIS CONSTANTS --\n\nINTERESTING_TEAMS = (\n    (""Manchester United"", 8),\n    (""Manchester City"", 8),\n    (""Chelsea"", 8),\n    (""Tottenham Hotspur"", 8),\n    (""Arsenal"", 8),\n    (""Liverpool"", 8),\n    (""Leicester City"", 8),\n    (""West Bromwich Albion"", 8),\n    (""Juventus"", 21),\n    (""Napoli"", 21),\n    (""Roma"", 21),\n    (""Internazionale"", 21),\n    (""Milan"", 21),\n    (""Barcelona"", 23),\n    (""Real Madrid"", 23),\n    (""Atl\xc3\xa9tico de Madrid"", 23),\n    (""Sevilla"", 23),\n    (""Villarreal"", 23),\n    (""Athletic Club"", 23),\n    (""Celta de Vigo"", 23),\n    (""Valencia"", 23),\n    (""FC Bayern M\xc3\xbcnchen"", 22),\n    (""Borussia Dortmund"", 22),\n    (""Bayer 04 Leverkusen"", 22),\n    (""VfL Wolfsburg"", 22),\n    (""Borussia M\xc3\xb6nchengladbach"", 22),\n    (""Paris Saint-Germain"", 24),\n    (""Lyon"", 24),\n    (""Monaco"", 24),\n)\n\nTOP5_LEAGUES = {22: ""German Bundesliga"",\n                23: ""Spain La Liga"",\n                24: ""France League 1"",\n                8: ""England Premier League"",\n                21: ""Italy Serie A""}\n\nLEAGUES_EXTENDED = dict(TOP5_LEAGUES, **{""98"": ""MLS""})\n\n\nSEASONS = range(2010, 2017)\n\n# boolean parameter means sort in ascending order or not\nFEATURES = (\n    (\'matches_to_score_from_corner\', True),\n    (\'percent_of_goals_scored_from_corners\', False),\n    (\'percent_of_corners_leads_to_goal\', False),\n    (\'percent_of_corners_chances_created_became_a_goal\', False),\n    (\'percent_of_corners_leads_to_nothing\', True),\n    (\'average_corners_per_match\', False),\n)\n\nteam_stats = pd.read_sql_query(\'select * from team_stats;\', con=engine)\n\n\ndef _aggregate(data_set):\n    """"""\n    Aggregates inputted data, by creating some tricky metrics\n    """"""\n    agg = data_set.agg({""corners_assists"": [""sum"", ""mean""],\n                        ""corners_failed"": [""sum"", ""mean""],\n                        ""corners_chances_created"": [""sum"", ""mean""],\n                        ""corners_total"": [""sum"", ""mean""],\n                        ""match_id"": ""count"",\n                        ""team_score"": ""sum"",\n                        })\n    # add more metrics\n    agg[\'matches_to_score_from_corner\'] = 1 / agg[\'corners_assists\'][\'mean\']\n    agg[\'percent_of_goals_scored_from_corners\'] = (agg[\'corners_assists\'][\'sum\'] /\n                                                   agg[\'team_score\'][\'sum\'] * 100)\n    agg[\'percent_of_corners_leads_to_goal\'] = (agg[\'corners_assists\'][\'sum\'] /\n                                               agg[\'corners_total\'][\'sum\'] * 100)\n    agg[\'percent_of_corners_leads_to_nothing\'] = (agg[\'corners_failed\'][\'sum\'] /\n                                                  agg[\'corners_total\'][\'sum\'] * 100)\n    agg[\'percent_of_corners_chances_created_became_a_goal\'] = (agg[\'corners_assists\'][\'sum\'] /\n                                                               agg[\'corners_chances_created\'][\'sum\'] * 100)\n    agg[\'matches_played\'] = agg[\'match_id\'][\'count\']\n    agg[\'average_corners_per_match\'] = agg[\'corners_total\'][\'mean\']\n\n    agg = agg.reset_index()\n\n    # make name more understandable\n    agg = agg.rename(columns={\'team_score\': ""goals_scored""})\n\n    return agg\n\n\ndef get_info(team_stats, league=None, season=None, team_name=None,\n             group_by=None, sort_by=None, asc=True, how_much=25,\n             per_team_stat=False):\n    """"""\n    Gets the dataframe and processed it according to the set of arguments,\n    which corresponds to aggregation, filtering, sorting ad limitation\n\n    :param team_stats: basic data frame with all the collected data\n    :param league: an array (!) of leagues to filter input data with them\n    :param season: filter for a particular season\n    :param team_name: to restrict the result for only 1 concrete team\n    :param group_by: an array of columns for grouping data\n    :param sort_by: an array of columns for sorting data\n    :param asc: Direction of the sorting (only for the 1st column)\n    :param how_much: limiting the output results amount\n    :param per_team_stat: flag which means we need to sort the result by season\n    :return: edited data frame with grouped, agreggated, sorted and limited data\n    """"""\n\n    # league filter\n    if league:\n        team_stats = team_stats[team_stats[\'league_id\'].isin(league)]\n\n    # season filter\n    if season:\n        team_stats = team_stats[team_stats[\'season\'] == str(season)]\n\n    # team_name filter\n    if team_name:\n        team_stats = team_stats[team_stats[\'team_name\'] == team_name]\n\n    # grouping\n    grouped = team_stats.groupby(group_by)\n\n    # aggregate\n    aggregated = _aggregate(grouped)\n\n    resulted_columns = [\'matches_played\',\n                        \'goals_scored\',\n                        \'average_corners_per_match\',\n                        \'corners_assists\',\n                        \'matches_to_score_from_corner\',\n                        \'percent_of_goals_scored_from_corners\',\n                        \'percent_of_corners_leads_to_goal\',\n                        \'percent_of_corners_chances_created_became_a_goal\',\n                        \'percent_of_corners_leads_to_nothing\']\n\n    # append grouped columns\n    for g in group_by:\n        resulted_columns.append(g)\n\n    if per_team_stat:\n        sort_by = \'season\'\n\n    # sorting\n    aggregated = aggregated.sort_values(sort_by, ascending=asc)\n    # cleaning\n    aggregated = aggregated.replace([np.inf, -np.inf], 0)\n\n    return aggregated[resulted_columns][:how_much]\n\n\ndef get_scoring_minutes():\n    """"""\n    Creates a dict with a structure like: {""minute"": goals_scored}, so we\n    could understand what minute is the most popular to score on.\n    """"""\n\n    # use matches without extra times\n    leagues = (8, 21, 22, 23, 24, 98, 214)\n    _team_stats = team_stats[team_stats[\'league_id\'].isin(leagues)]\n    minutes = _team_stats[\'scoring_minutes\'].values\n\n    result = defaultdict(int)\n\n    for mins in minutes:\n        for m in mins:\n            try:\n                result[int(m)] += 1\n            except ValueError:\n                pass\n\n    return OrderedDict(((k, result[k]) for k in sorted(result.keys())))\n\n\ndef create_minutes_plot(data):\n    """"""\n    Draws the plot for scoring minutes\n    """"""\n\n    fig, ax = plt.subplots(figsize=(15, 8))\n\n    barlist = ax.bar(list(data.keys()), data.values())\n    for i, b in enumerate(barlist, start=1):\n        if i % 2 == 0:\n            b.set_color(\'0.75\')\n        else:\n            b.set_color(\'0.55\')\n\n    ax.set_xticks(np.arange(len(data)))\n    ax.set_yticks(np.arange(0, max(data.values()), 25))\n    ax.set_xticklabels(list(data.keys()), rotation=\'vertical\')\n\n    # 100 minutes is enough\n    for i, y in enumerate(list(data.values())[:100]):\n        plt.text(i - 0.5, y, y, fontweight=\'bold\', fontsize=5)\n\n    ax.set_ylim((0, 450))\n    ax.set_xlim((0, 100))\n    plt.xlabel(\'Match minutes\')\n    plt.title(\'Goals scored per minute.\\n\'\n              \'Scored in the first half:{}\\n\'\n              \'Scored in the second half:{}\'.format(\n        sum((value for key, value in data.items() if key <= 45)),\n        sum((value for key, value in data.items() if key > 45))))\n    plt.ylabel(\'Number of goals\')\n    plt.savefig(_get_path(\'goals_minutes\', \'goals_minutes\'), dpi=200)\n\n\ndef create_bar_plots(info, title, filename, x_label, y_label):\n    """"""\n    Draws bar plots and save them as files\n    :param info: processed data frame with aggregated and sorted data\n    :param title: title for a plot\n    :param filename: filename to save concrete plot with\n    :param x_label: what\'s on the x-axis\n    :param y_label: what\'s on the y-axis\n    :return: None\n    """"""\n\n    fig, ax = plt.subplots(figsize=(15, 9))\n    ind = np.arange(len(info))\n    ax.bar(ind, info[y_label], color=random.choice(\'rgbycm\'))\n    ax.set_xlabel(x_label)\n    ax.set_ylabel("" "".join(y_label.split(""_"")))\n\n    for tick in ax.yaxis.get_major_ticks():\n        tick.label1On = False\n        tick.label2On = True\n        tick.label2.set_color(\'green\')\n\n    ax.set_xticks(np.arange(len(info)))\n\n    ax.set_xticklabels(info[x_label].values,\n                       rotation=\'vertical\')\n\n    plt.subplots_adjust(bottom=0.3)\n    for i, y in enumerate(info[y_label]):\n        plt.text(i - 0.3, y, ""{:.2f}"".format(y), fontweight=\'bold\', fontsize=7)\n\n    plt.title(title)\n    plt.savefig(_get_path(y_label, filename), dpi=200)\n    plt.close(\'all\')\n\n\ndef _get_path(subdir, filename):\n    """"""\n    Creates a path for an image\n    :param subdir: directory to store images in\n    :param filename: name of the outputted file\n    :return: string representation of the path\n    """"""\n    path = Path(\'files\', \'images\', subdir)\n    path.mkdir(exist_ok=True, parents=True)\n\n    return str(path / filename)\n\n\ndef build_per_league_stat():\n    for league_id, league_name in LEAGUES_EXTENDED.items():\n        for season in SEASONS:\n            for feature, asc in FEATURES:\n                data = get_info(team_stats, league=[league_id], season=season,\n                                group_by=[\'team_name\', \'season\'],\n                                sort_by=feature, asc=asc)\n                title = ""{} \\n {}"".format(league_name,\n                                          ""{}/{}"".format(season, season + 1))\n                filename = ""{}.png"".format(league_name + ""_"" + str(season))\n                create_bar_plots(data, title, filename, x_label=\'team_name\',\n                                 y_label=feature)\n\n\ndef build_leagues_average_by_season():\n    for feature, asc in FEATURES:\n        for season in SEASONS:\n            data = get_info(team_stats, league=LEAGUES_EXTENDED.keys(),\n                            season=season,\n                            group_by=[\'league_id\', \'season\'],\n                            sort_by=[feature], asc=asc)\n            title = ""Leagues average for {}/{} season. \\n"" \\\n                    ""Mappings: {}"".format(season, season + 1, LEAGUES_EXTENDED)\n            filename = ""Average_by_league_{}.png"".format(season)\n            create_bar_plots(data, title, filename, x_label=\'league_id\',\n                             y_label=feature)\n\n\ndef build_per_team_stat():\n    for team, league in INTERESTING_TEAMS:\n        for feature, asc in FEATURES:\n            # Here we use always asc = True, cause we will iterate through\n            # years (seasons) and they\'d better be in ascending order :)\n            data = get_info(team_stats, league=[league],\n                            group_by=[\'team_name\', \'season\'],\n                            team_name=team,\n                            sort_by=feature, asc=True, how_much=7,\n                            per_team_stat=True)\n            title = ""{} \\n {}"".format(team, TOP5_LEAGUES[league])\n            filename = ""{}_{}.png"".format(team, TOP5_LEAGUES[league])\n            create_bar_plots(data, title, filename, x_label=\'season\',\n                             y_label=feature)\n\n\ndef build_general_results_for_top20_for_all_time():\n    for feature, asc in FEATURES:\n        data = get_info(team_stats, league=TOP5_LEAGUES.keys(),\n                        group_by=[\'team_name\', \'season\'],\n                        sort_by=feature, asc=asc)\n        title = ""TOP-20 results through seasons""\n        filename = ""top20_{}.png"".format(feature)\n        create_bar_plots(data, title, filename,\n                         x_label=[\'team_name\', \'season\'],\n                         y_label=feature)\n\n\ndef build_general_results_for_top20_by_season():\n    for feature, asc in FEATURES:\n        for season in SEASONS:\n            data = get_info(team_stats, league=TOP5_LEAGUES.keys(),\n                            season=season,\n                            group_by=[\'team_name\', \'season\'],\n                            sort_by=feature, asc=asc)\n            title = ""TOP-20 results through for season {}/{}"".format(season,\n                                                                     season + 1)\n            filename = ""top20_{}_{}.png"".format(feature, season)\n            create_bar_plots(data, title, filename,\n                             x_label=\'team_name\',\n                             y_label=feature)\n\n\ndef build_per_team_stat_cl():\n    league_id, league_name = 5, \'Champions League\'\n    for team, _ in INTERESTING_TEAMS:\n        for feature, asc in FEATURES:\n            # asc = True, cause we will iterate through years (seasons)\n            data = get_info(team_stats, league=[league_id],\n                            group_by=[\'team_name\', \'season\'], team_name=team,\n                            sort_by=feature, asc=True, how_much=7,\n                            per_team_stat=True)\n            title = ""{} \\n {}"".format(team, league_name)\n            filename = ""{}_{}.png"".format(team, league_name)\n            create_bar_plots(data, title, filename, x_label=\'season\',\n                             y_label=feature)\n\n\nif __name__ == ""__main__"":\n    create_minutes_plot(get_scoring_minutes())\n    build_per_team_stat_cl()\n    build_per_team_stat()\n    build_per_league_stat()\n    build_leagues_average_by_season()\n    build_general_results_for_top20_by_season()\n    build_general_results_for_top20_for_all_time()\n'"
corners442/__init__.py,0,b''
corners442/schema.py,0,b'from corners442.db import create_schema\n\n# create schema\ncreate_schema()\n\n'
corners442/start_league.py,0,"b'from scrapy import cmdline\ncmdline.execute(""scrapy crawl league"".split())'"
corners442/start_team_stats.py,0,"b'from scrapy import cmdline\ncmdline.execute(""scrapy crawl fourfourtwo"".split())'"
corners442/corners442/__init__.py,0,b''
corners442/corners442/db.py,0,"b'from sqlalchemy.dialects.postgresql.array import ARRAY\nfrom sqlalchemy.engine import create_engine\nfrom sqlalchemy.ext.declarative.api import declarative_base\nfrom sqlalchemy.orm.session import sessionmaker\nfrom sqlalchemy.sql.schema import Column, ForeignKey, UniqueConstraint\nfrom sqlalchemy.sql.sqltypes import Integer, String, Boolean\n\nfrom . import settings\n\nBase = declarative_base()\n\n\ndef get_engine():\n    return create_engine(settings.DEV_PSQL_URI)\n\n\nengine = get_engine()\n\n\ndef create_schema():\n    Base.metadata.create_all(engine)\n\n\ndef create_session():\n    Session = sessionmaker(bind=engine)\n    return Session()\n\n\nclass TeamStats(Base):\n    __tablename__ = \'team_stats\'\n    __table_args__ = (UniqueConstraint(\'match_id\', \'team_name\', name=\'uix_1\'),)\n\n    id = Column(Integer, primary_key=True)\n    match_id = Column(Integer, index=True)\n    league_id = Column(Integer, ForeignKey(""league.league_id""))\n    season = Column(String, index=True)\n    team_name = Column(String, index=True)\n    host_status = Column(Boolean)\n    team_score = Column(Integer)\n    scoring_minutes = Column(ARRAY(String))\n    corners_total = Column(Integer)\n    corners_chances_created = Column(Integer)\n    corners_assists = Column(Integer)\n    corners_failed = Column(Integer)\n\n\nclass League(Base):\n    __tablename__ = \'league\'\n\n    league_id = Column(Integer, primary_key=True)\n    league_name = Column(String)\n\n\ndef get_or_create(session, model, defaults=None, **kwargs):\n\n    instance = session.query(model).filter_by(**kwargs).first()\n    if instance:\n        return instance\n    else:\n        lookups = kwargs.copy()\n        if defaults:\n            for key, value in defaults.items():\n                lookups[key] = value\n        instance = model(**lookups)\n        session.add(instance)\n        session.commit()\n        return instance\n\n\ndef update_or_create(session, model, defaults=None, **kwargs):\n    instance = session.query(model).filter_by(**kwargs).first()\n    if instance:\n        lookups = kwargs.copy()\n        if defaults:\n            for key, value in defaults.items():\n                lookups[key] = value\n        for key, value in lookups.items():\n            setattr(instance, key, value)\n        session.commit()\n        return instance\n    else:\n        lookups = kwargs.copy()\n        if defaults:\n            for key, value in defaults.items():\n                lookups[key] = value\n        instance = model(**lookups)\n        session.add(instance)\n        session.commit()\n        return instance'"
corners442/corners442/items.py,0,"b""# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass Corners442Item(scrapy.Item):\n    # General match info\n    match_id = scrapy.Field()\n    league_id = scrapy.Field()\n    season = scrapy.Field()\n    team_name = scrapy.Field()\n    host_status = scrapy.Field()  # 0 - guest team, 1 - host team\n    team_score = scrapy.Field()  # how many goals the team've scored\n    scoring_minutes = scrapy.Field()  # minutes of the goals been scored\n\n    # Corners info\n    corners_total = scrapy.Field()\n    corners_chances_created = scrapy.Field()\n    corners_assists = scrapy.Field()\n    corners_failed = scrapy.Field()\n\n\nclass LeagueItem(scrapy.Item):\n    league_id = scrapy.Field()\n    league_name = scrapy.Field()"""
corners442/corners442/middlewares.py,0,"b""# -*- coding: utf-8 -*-\n\n# Define here the models for your spider middleware\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/spider-middleware.html\n\nfrom scrapy import signals\n\n\nclass Corners442SpiderMiddleware(object):\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the spider middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_spider_input(response, spider):\n        # Called for each response that goes through the spider\n        # middleware and into the spider.\n\n        # Should return None or raise an exception.\n        return None\n\n    def process_spider_output(response, result, spider):\n        # Called with the results returned from the Spider, after\n        # it has processed the response.\n\n        # Must return an iterable of Request, dict or Item objects.\n        for i in result:\n            yield i\n\n    def process_spider_exception(response, exception, spider):\n        # Called when a spider or process_spider_input() method\n        # (from other spider middleware) raises an exception.\n\n        # Should return either None or an iterable of Response, dict\n        # or Item objects.\n        pass\n\n    def process_start_requests(start_requests, spider):\n        # Called with the start requests of the spider, and works\n        # similarly to the process_spider_output() method, except\n        # that it doesn\xe2\x80\x99t have a response associated.\n\n        # Must return only requests (not items).\n        for r in start_requests:\n            yield r\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n"""
corners442/corners442/pipelines.py,0,"b""# -*- coding: utf-8 -*-\n\n# Define your item pipelines here\n#\n# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html\nfrom sqlalchemy.orm.session import sessionmaker\n\nfrom .db import League, TeamStats, get_or_create, create_session, \\\n    update_or_create\n\n\nclass Corners442Pipeline(object):\n\n    def process_item(self, item, spider):\n        # simply add or update item into database\n        match_id = item.pop('match_id')\n        team_name = item.pop('team_name')\n\n        update_or_create(create_session(), TeamStats, defaults=item,\n                         match_id=match_id,\n                         team_name=team_name)\n\n\nclass LeaguePipeline(object):\n\n    def process_item(self, item, spider):\n        # simply add item to database\n        get_or_create(create_session(), League, **item)\n"""
corners442/corners442/settings.py,0,"b'# -*- coding: utf-8 -*-\n\n# Scrapy settings for corners442 project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     http://doc.scrapy.org/en/latest/topics/settings.html\n#     http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html\n#     http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html\nimport os\n\nBOT_NAME = \'corners442\'\n\nSPIDER_MODULES = [\'corners442.spiders\']\nNEWSPIDER_MODULE = \'corners442.spiders\'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\nUSER_AGENT = \'Mozilla/5.0 (iPad; U; CPU OS 3_2_1 like Mac OS X; en-us) AppleWebKit/531.21.10 (KHTML, like Gecko) Mobile/7B405\'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\nCONCURRENT_REQUESTS = 4\n\n# Configure a delay for requests for the same website (default: 0)\n# See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\nDOWNLOAD_DELAY = 5\n# The download delay setting will honor only one of:\nCONCURRENT_REQUESTS_PER_DOMAIN = 2\nCONCURRENT_REQUESTS_PER_IP = 2\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   \'Accept\': \'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\',\n#   \'Accept-Language\': \'en\',\n#}\n\n# Enable or disable spider middlewares\n# See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    \'corners442.middlewares.Corners442SpiderMiddleware\': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    \'corners442.middlewares.MyCustomDownloaderMiddleware\': 543,\n#}\n\n# Enable or disable extensions\n# See http://scrapy.readthedocs.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    \'scrapy.extensions.telnet.TelnetConsole\': None,\n#}\n\n# Configure item pipelines\n# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html\nITEM_PIPELINES = {\n   \'corners442.pipelines.Corners442Pipeline\': 300,\n}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See http://doc.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = False\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 10\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = \'httpcache\'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = \'scrapy.extensions.httpcache.FilesystemCacheStorage\'\n\nDEV_PSQL_URI = os.getenv(""DEV_PSQL_URI"", ""postgresql://corners:corners@0.0.0.0:8432/corners"")\nBASE_URL = \'https://www.fourfourtwo.com/\''"
corners442/corners442/spiders/__init__.py,0,b'# This package will contain the spiders of your Scrapy project\n#\n# Please refer to the documentation for information on how to create and manage\n# your spiders.\n'
corners442/corners442/spiders/four_four_two.py,0,"b'import re\n\nfrom urllib.parse import urljoin\n\nfrom scrapy.http.request import Request\nfrom scrapy.spiders import Spider\n\nfrom ..settings import BASE_URL\nfrom ..items import Corners442Item\nfrom ..db import create_session, League\n\n\nclass NoTeamStatsHref(Exception):\n    pass\n\n\nclass MySpider(Spider):\n    name = \'fourfourtwo\'\n\n    def start_requests(self):\n        session = create_session()\n        leagues = session.query(League).all()\n\n        for league in leagues:\n            for year in range(2010, 2015):\n                url = urljoin(BASE_URL, \'statszone/results/{}-{}\'.format(\n                    league.league_id, year))\n                request = Request(url, self.parse)\n                request.meta[\'league_id\'] = league.league_id\n                request.meta[\'year\'] = year\n                yield request\n\n\n    def parse(self, response):\n        # get matches_results links:\n        no_content = response.xpath(\n            \'//div[@id=""content""]/div[@class=""clear""]/text()\').re(\n            \'There are no matches\')\n        if no_content:\n            return\n\n        for link in response.xpath(\'//td[@class=""link-to-match""]/a/@href\').extract():\n            request = Request(urljoin(BASE_URL, link), callback=self.get_match_info)\n            request.meta[\'match_id\'] = re.search(r\'^.*[/](\\d+)$\', link).group(1)\n\n            request.meta[\'league_id\'] = response.meta[\'league_id\']\n            request.meta[\'year\'] = response.meta[\'year\']\n            yield request\n\n    def get_match_info(self, response):\n\n        host_team = Corners442Item(host_status=True,\n                                   season=response.meta[\'year\'],\n                                   league_id=response.meta[\'league_id\'],\n                                   match_id=response.meta[\'match_id\'])\n        guest_team = Corners442Item(host_status=False,\n                                    season=response.meta[\'year\'],\n                                    league_id=response.meta[\'league_id\'],\n                                    match_id=response.meta[\'match_id\'])\n\n        host_team[\'team_name\'] = response.xpath(\'//div[@class=""score-wrapper""]/span[@class=""home-head""]/text()\').extract_first().strip()\n        guest_team[\'team_name\'] = response.xpath(\'//div[@class=""score-wrapper""]/span[@class=""away-head""]/text()\').extract_first().strip()\n\n        score = response.xpath(\'//div[@class=""score-wrapper""]/span[@class=""score""]/text()\').extract_first()\n        host_team[\'team_score\'] = int(score.split(\'-\')[0].strip())\n        guest_team[\'team_score\'] = int(score.split(\'-\')[1].strip())\n\n        host_team[\'scoring_minutes\'] = response.xpath(\'//div[@class=""results""]/div[@class=""home""]/span[@class=""goal""]/text()|//div[@class=""results""]/div[@class=""home""]/span[@class=""own_goal""]/text()|//div[@class=""results""]/div[@class=""home""]/span[@class=""penalty""]/text()\').re(r\'\\W*\\w*\\W*(\\d+)\\W*\')\n        guest_team[\'scoring_minutes\'] = response.xpath(\'//div[@class=""results""]/div[@class=""away""]/span[@class=""goal""]/text()|//div[@class=""results""]/div[@class=""away""]/span[@class=""own_goal""]/text()|//div[@class=""results""]/div[@class=""away""]/span[@class=""penalty""]/text()\').re(r\'\\W*(\\d+)\\W*\\w*\\W*\')\n\n        team_stats_link = response.xpath(""//li/a[contains(., \'Team Stats\')]/@href"").extract_first()\n        if not team_stats_link:\n            raise NoTeamStatsHref\n\n        # this will open corners info instead of overall info\n        team_stats_link = team_stats_link.replace(\'OVERALL_01\', \'2_ATTACK_03\')\n        home_team_stats_url = urljoin(BASE_URL, team_stats_link)\n\n        request = Request(home_team_stats_url, callback=self.get_corners_info)\n        request.meta[\'host_team\'] = request.meta[\'team_to_fill\'] = host_team\n        request.meta[\'guest_team\'] = guest_team\n        yield request\n\n    def get_corners_info(self, response):\n        host_team = response.meta[\'host_team\']\n        guest_team = response.meta[\'guest_team\']\n        team_to_fill = response.meta[\'team_to_fill\']\n\n        corners = response.xpath(\n            \'//svg[@id=""pitch""]/line/@marker-end\').extract()\n        team_to_fill[\'corners_total\'] = len(corners)\n        team_to_fill[\'corners_chances_created\'] = len(\n            [i for i in corners if i.find(\'#smalldarkgrey\') != -1])\n        team_to_fill[\'corners_assists\'] = len(\n            [i for i in corners if i.find(\'#smallyellow\') != -1])\n        team_to_fill[\'corners_failed\'] = len(\n            [i for i in corners if i.find(\'#smallred\') != -1])\n\n        guest_team_stat_link = response.xpath(\n            \'//li[@class=""1 last""]/a/@href\').extract_first()\n        if not guest_team_stat_link:\n            # in this case team_to_fill will be equals to the guest_team\n            for team in [host_team, team_to_fill]:\n                yield team\n\n        else:\n            guest_team_stats_url = urljoin(BASE_URL, guest_team_stat_link)\n            request = Request(guest_team_stats_url, callback=self.get_corners_info)\n\n            # if we reached here, then team_to_fill == host_team\n            request.meta[\'host_team\'] = team_to_fill\n            request.meta[\'guest_team\'] = request.meta[\'team_to_fill\'] = guest_team\n            yield request\n'"
corners442/corners442/spiders/leagues.py,0,"b'from urllib.parse import urljoin\n\nfrom scrapy.http.request import Request\nfrom scrapy.spiders import Spider\n\nfrom ..settings import BASE_URL\nfrom ..items import LeagueItem\n\n\nclass MySpider(Spider):\n    name = \'league\'\n    custom_settings = {\n        \'ITEM_PIPELINES\': {\n            \'corners442.pipelines.LeaguePipeline\': 300\n        }\n    }\n\n    def start_requests(self):\n\n        yield Request(urljoin(BASE_URL, \'statszone\'), self.parse)\n\n    def parse(self, response):\n        # get competitions name and id:\n        for option in response.xpath(\'//select[@id=""edit-competitions""]/option\'):\n            if option.xpath(\'@selected\').extract_first(\n                    default=\'\') == \'selected\':\n                continue\n            item = LeagueItem()\n            item[\'league_id\'] = option.xpath(\'@value\').extract_first()\n            item[\'league_name\'] = option.xpath(\'text()\').extract_first()\n            yield item\n'"
