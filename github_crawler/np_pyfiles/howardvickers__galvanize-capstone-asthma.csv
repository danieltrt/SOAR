file_path,api_count,code
src/app.py,0,"b'from flask import Flask, request, render_template\nimport flask\nfrom flask_bootstrap import Bootstrap\nfrom flask import Markup\n\nimport pandas as pd\nimport numpy as np\nimport requests\nimport os\n\nfrom state_color_map import create_map as cm\nfrom data import make_fips_df as mf\nfrom ols_model_hot_one import train_model as ols_tm\nfrom rfr_model_hot_one import train_model as rfr_tm\nfrom svr_model_hot_one import train_model as svr_tm\nfrom lr_model_hot_one import train_model as lr_tm\nfrom knn_model_hot_one import train_model as knn_tm\nfrom comparison import compare_models\nfrom get_results import test_data_preds_table\nfrom get_results import get_county_pred\nfrom get_results import one_county\nfrom get_results import get_state_pred_map\nfrom charts import create_feat_imp_chart\n\napp = Flask(__name__)\n\n@app.route(\'/\', methods =[\'GET\',\'POST\'])\ndef index():\n    return flask.render_template(\'index.html\')\n\n@app.route(\'/data\', methods =[\'GET\',\'POST\'])\ndef data():\n    create_feat_imp_chart()\n\n    return flask.render_template(\'data.html\')\n\n@app.route(\'/models\', methods =[\'GET\',\'POST\'])\ndef models():\n    ols_results = ols_tm()\n    ols_summary = Markup(ols_results)\n\n    comparison_dict = compare_models()\n\n    return flask.render_template(\n                                \'models.html\',\n                                ols_results = ols_summary,\n                                model_comparison_dict = comparison_dict\n                                )\n\n@app.route(\'/state\', methods =[\'GET\',\'POST\'])\ndef state():\n    if request.method == \'POST\':\n        state = \'Colorado\'\n        state_form_results = [  request.form[\'state_uninsur\'],\n                                request.form[\'state_unemploy\'],\n                                request.form[\'state_obs\'],\n                                request.form[\'state_smok\'],\n                                request.form[\'state_partic\']\n                              ]\n        print(\'state_form_results\', state_form_results)\n        div_and_map = get_state_pred_map(state, state_form_results)\n\n        return flask.render_template( ""state.html"",\n                                state_pred_map = div_and_map\n                                )\n\n    return flask.render_template(\'state.html\')\n\n\n@app.route(\'/county\', methods =[\'GET\',\'POST\'])\ndef county():\n    county = \'Adams\'\n    input_county, uninsured, unemployment, obesity, smokers, particulates, y, X = one_county(county)\n\n    # these are for the \'Public Policy and Asthma\' chart\n    if request.method == \'POST\':\n        """"""takes in policy changes and returns single county\'s prediction under policy changes""""""\n\n        form_results = [request.form[\'new_uninsur\'],\n                        request.form[\'new_unemploy\'],\n                        request.form[\'new_obs\'],\n                        request.form[\'new_smok\'],\n                        request.form[\'new_partic\']\n                        ]\n\n        pred, y = get_county_pred(county, form_results)\n\n        return render_template( ""county.html"",\n                                    county = input_county.title(),\n                                    uninsured = uninsured,\n                                    unemployment = unemployment,\n                                    obesity = obesity,\n                                    smokers = smokers,\n                                    particulates = particulates,\n                                    y = y,\n                                    new_y = pred,\n                                )\n\n    return flask.render_template(\'county.html\',\n                                    county = input_county.title(),\n                                    uninsured = uninsured,\n                                    unemployment = unemployment,\n                                    obesity = obesity,\n                                    smokers = smokers,\n                                    particulates = particulates,\n                                    y = y,\n                                    )\n\n@app.route(\'/predictions\', methods =[\'GET\',\'POST\'])\ndef predictions():\n\n    table_dict = test_data_preds_table()\n\n    return flask.render_template(\'predictions.html\',\n                                    table_dict = table_dict\n                                    )\n\n@app.route(\'/about\', methods =[\'GET\',\'POST\'])\ndef about():\n    return flask.render_template(\'about.html\')\n\nif __name__ == \'__main__\':\n\n    Bootstrap(app)\n    app.run(host=\'0.0.0.0\', port=8080, debug=True, use_reloader=False)\n'"
src/ca_data.py,0,"b'import numpy as np\nimport pandas as pd\n# source data: https://data.chhs.ca.gov/dataset/asthma-ed-visit-rates-lghc-indicator-07\n\ndef get_data():\n    # def pm2_5spec_data():\n    pm2_5spec = pd.read_csv(\'../data/daily_88502_2017.csv\')\n    pm2_5spec.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    pm2_5specca = pm2_5spec[pm2_5spec[\'state\'] == \'California\']\n    pm2_5speccaslim = pm2_5specca.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    pm2_5speccaslim = pm2_5speccaslim.reset_index()\n    pm2_5speccaslim = pm2_5speccaslim.drop([\'index\'], axis=1)\n    pm2_5speccaslim[\'obs_x_mean\'] = pm2_5speccaslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    pm2_5speccaslim.county = pm2_5speccaslim.county.str.lower()\n    pm2_5speccagroupby = pm2_5speccaslim.groupby(\'county\').sum()\n    pm2_5speccagroupby = pm2_5speccagroupby.reset_index()\n    pm2_5speccagroupby[\'new_mean\'] = pm2_5speccagroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    pm2_5speccafinal = pm2_5speccagroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    pm2_5speccafinal.columns = [\'county\', \'pm2_5spec_mean\']\n\n\n    # def pm2_5non_data():\n    pm2_5non = pd.read_csv(\'../data/daily_88502_2017.csv\')\n    pm2_5non.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    pm2_5nonca = pm2_5non[pm2_5non[\'state\'] == \'California\']\n    pm2_5noncaslim = pm2_5nonca.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    pm2_5noncaslim = pm2_5noncaslim.reset_index()\n    pm2_5noncaslim = pm2_5noncaslim.drop([\'index\'], axis=1)\n    pm2_5noncaslim[\'obs_x_mean\'] = pm2_5noncaslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    pm2_5noncaslim.county = pm2_5noncaslim.county.str.lower()\n    pm2_5noncagroupby = pm2_5noncaslim.groupby(\'county\').sum()\n    pm2_5noncagroupby = pm2_5noncagroupby.reset_index()\n    pm2_5noncagroupby[\'new_mean\'] = pm2_5noncagroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    pm2_5noncafinal = pm2_5noncagroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    pm2_5noncafinal.columns = [\'county\', \'pm2_5non_mean\']\n\n\n    # def pm2_5_data():\n    pm2_5 = pd.read_csv(\'../data/daily_88101_2017.csv\')\n    pm2_5.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    pm2_5ca = pm2_5[pm2_5[\'state\'] == \'California\']\n    pm2_5caslim = pm2_5ca.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    pm2_5caslim = pm2_5caslim.reset_index()\n    pm2_5caslim = pm2_5caslim.drop([\'index\'], axis=1)\n    pm2_5caslim[\'obs_x_mean\'] = pm2_5caslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    pm2_5caslim.county = pm2_5caslim.county.str.lower()\n    pm2_5cagroupby = pm2_5caslim.groupby(\'county\').sum()\n    pm2_5cagroupby = pm2_5cagroupby.reset_index()\n    pm2_5cagroupby[\'new_mean\'] = pm2_5cagroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    pm2_5cafinal = pm2_5cagroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    pm2_5cafinal.columns = [\'county\', \'pm2_5_mean\']\n\n\n    # def pm10_data():\n    pm10 = pd.read_csv(\'../data/daily_81102_2017.csv\')\n    pm10.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    pm10ca = pm10[pm10[\'state\'] == \'California\']\n    pm10caslim = pm10ca.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    pm10caslim = pm10caslim.reset_index()\n    pm10caslim = pm10caslim.drop([\'index\'], axis=1)\n    pm10caslim[\'obs_x_mean\'] = pm10caslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    pm10caslim.county = pm10caslim.county.str.lower()\n    pm10cagroupby = pm10caslim.groupby(\'county\').sum()\n    pm10cagroupby = pm10cagroupby.reset_index()\n    pm10cagroupby[\'new_mean\'] = pm10cagroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    pm10cafinal = pm10cagroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    pm10cafinal.columns = [\'county\', \'pm10_mean\']\n\n\n    # def lead_data():\n    lead = pd.read_csv(\'../data/daily_LEAD_2014.csv\')\n    lead.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    leadca = lead[lead[\'state\'] == \'California\']\n    leadcaslim = leadca.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    leadcaslim = leadcaslim.reset_index()\n    leadcaslim = leadcaslim.drop([\'index\'], axis=1)\n    leadcaslim[\'obs_x_mean\'] = leadcaslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    leadcaslim.county = leadcaslim.county.str.lower()\n    leadcagroupby = leadcaslim.groupby(\'county\').sum()\n    leadcagroupby = leadcagroupby.reset_index()\n    leadcagroupby[\'new_mean\'] = leadcagroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    leadcafinal = leadcagroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    leadcafinal.columns = [\'county\', \'lead_mean\']\n\n\n    # def nonox_data():\n    nonox = pd.read_csv(\'../data/daily_NONOxNOy_2017.csv\')\n    nonox.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    nonoxca = nonox[nonox[\'state\'] == \'California\']\n    nonoxcacoslim = nonoxca.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    nonoxcacoslim = nonoxcacoslim.reset_index()\n    nonoxcacoslim = nonoxcacoslim.drop([\'index\'], axis=1)\n    nonoxcacoslim[\'obs_x_mean\'] = nonoxcacoslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    nonoxcacoslim.county = nonoxcacoslim.county.str.lower()\n    nonoxcagroupby = nonoxcacoslim.groupby(\'county\').sum()\n    nonoxcagroupby = nonoxcagroupby.reset_index()\n    nonoxcagroupby[\'new_mean\'] = nonoxcagroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    nonoxcafinal = nonoxcagroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    nonoxcafinal.columns = [\'county\', \'nonox_mean\']\n\n\n\n    # def haps_data():\n    haps = pd.read_csv(\'../data/daily_HAPS_2017.csv\')\n    haps.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    hapsca = haps[haps[\'state\'] == \'California\']\n    hapscaslim = hapsca.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    hapscaslim = hapscaslim.reset_index()\n    hapscaslim = hapscaslim.drop([\'index\'], axis=1)\n    hapscaslim[\'obs_x_mean\'] = hapscaslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    hapscaslim.county = hapscaslim.county.str.lower()\n    hapscagroupby = hapscaslim.groupby(\'county\').sum()\n    hapscagroupby = hapscagroupby.reset_index()\n    hapscagroupby[\'new_mean\'] = hapscagroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    hapscafinal = hapscagroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    hapscafinal.columns = [\'county\', \'haps_mean\']\n\n\n\n\n    # def vocs_data():\n    # vocs16 = pd.read_csv(\'daily_VOCS_2016.csv\')\n    vocs = pd.read_csv(\'../data/daily_VOCS_2017.csv\')\n    vocs.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    vocsca = vocs[vocs[\'state\'] == \'California\']\n    vocscaslim = vocsca.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    vocscaslim = vocscaslim.reset_index()\n    vocscaslim = vocscaslim.drop([\'index\'], axis=1)\n    vocscaslim[\'obs_x_mean\'] = vocscaslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    vocscaslim.county = vocscaslim.county.str.lower()\n    vocscagroupby = vocscaslim.groupby(\'county\').sum()\n    vocscagroupby = vocscagroupby.reset_index()\n    vocscagroupby[\'new_mean\'] = vocscagroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    vocscafinal = vocscagroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    vocscafinal.columns = [\'county\', \'vocs_mean\']\n\n\n    # def co_data():\n    co = pd.read_csv(\'../data/daily_42101_2017.csv\')\n    co.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    coca = co[co[\'state\'] == \'California\']\n    cocaslim = coca.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    cocaslim = cocaslim.reset_index()\n    cocaslim = cocaslim.drop([\'index\'], axis=1)\n    cocaslim[\'obs_x_mean\'] = cocaslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    cocaslim.county = cocaslim.county.str.lower()\n    cocagroupby = cocaslim.groupby(\'county\').sum()\n    cocagroupby = cocagroupby.reset_index()\n    cocagroupby[\'new_mean\'] = cocagroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    cocafinal = cocagroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    cocafinal.columns = [\'county\', \'co_mean\']\n\n\n    # def so2_data():\n    so2 = pd.read_csv(\'../data/daily_42401_2017.csv\')\n    so2.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    soca = so2[so2[\'state\'] == \'New Jersey\']\n    socaslim = soca.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    socaslim = socaslim.reset_index()\n    socaslim = socaslim.drop([\'index\'], axis=1)\n    socaslim[\'obs_x_mean\'] = socaslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    socaslim.county = socaslim.county.str.lower()\n    socagroupby = socaslim.groupby(\'county\').sum()\n    socagroupby = socagroupby.reset_index()\n    socagroupby[\'new_mean\'] = socagroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    socafinal = socagroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    socafinal.columns = [\'county\', \'so_mean\']\n\n    # def no2_data():\n    no2 = pd.read_csv(\'../data/daily_42602_2017.csv\')\n    no2.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    noca = no2[no2[\'state\'] == \'Colorado\']\n    nocaslim = noca.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    nocaslim.county = nocaslim.county.str.lower()\n    nocaslim = nocaslim.reset_index()\n    nocaslim = nocaslim.drop([\'index\'], axis=1)\n    nocaslim[\'obs_x_mean\'] = nocaslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    nocagroupby = nocaslim.groupby(\'county\').sum()\n    nocagroupby = nocagroupby.reset_index()\n    nocagroupby[\'new_mean\'] = nocagroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    nocafinal = nocagroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    nocafinal.columns = [\'county\', \'no_mean\']\n\n    # def ozone_data():\n    ozo = pd.read_csv(\'../data/daily_44201_2017.csv\')\n    ozoslim = ozo.drop([\'State Code\', \'County Code\', \'Site Num\', \'POC\',\n           \'Latitude\', \'Longitude\', \'Datum\', \'Sample Duration\',\n           \'Pollutant Standard\', \'Date Local\', \'Units of Measure\', \'Event Type\',\n            \'Observation Percent\',\n           \'1st Max Value\', \'1st Max Hour\', \'AQI\', \'Method Code\', \'Method Name\',\n           \'Local Site Name\', \'Address\', \'City Name\',\n           \'CBSA Name\', \'Date of Last Change\'], axis=1)\n    ozoslim.columns = [\'param_cd\', \'param\', \'obs_count\', \'mean_avg\', \'state\', \'county\']\n    ozoca = ozoslim[ozoslim[\'state\'] == \'Colorado\']\n    ozoca.county = ozoca.county.str.lower()\n    ozoca = ozoca.reset_index()\n    ozoca = ozoca.drop([\'index\', \'param\', \'state\', \'param_cd\'], axis= 1)\n    ozoca[\'obs_x_mean\']  = ozoca.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    ozocagroupby = ozoca.groupby(\'county\').sum()\n    ozocagroupby = ozocagroupby.reset_index()\n    ozocagroupby[\'new_mean\'] = ozocagroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    ozocafinal = ozocagroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\'], axis=1)\n    ozocafinal.columns = [\'county\', \'ozo_mean\']\n\n    # def asthma_ca():\n    # https://data.chhs.ca.gov/dataset/asthma-emergency-department-visit-rates-by-zip-code\n    asthmahospca = pd.read_csv(\'../data/asthma_hospitization_ca.csv\')\n    asthmahospca.columns = [\'year\', \'zip\', \'age\', \'visits\', \'asthma_rate\', \'fips\', \'county\']\n    asthmahospca = asthmahospca.drop([\'zip\', \'age\', \'visits\', \'fips\'], axis=1)\n    asthmahospca2015 = asthmahospca[asthmahospca[\'year\']==2015]\n    asthmahospca2015 = asthmahospca2015.drop(\'year\', axis=1)\n    asthmahospca2015.county = asthmahospca2015.county.str.lower()\n    asthmahospca2015 = asthmahospca2015.reset_index()\n    asthmahospca2015 = asthmahospca2015.drop(\'index\', axis=1)\n    asthmahospca2015 = asthmahospca2015.groupby(\'county\').mean()\n    asthmahospca2015 = asthmahospca2015.reset_index()\n    asthmaca = asthmahospca2015\n\n    # def merge_all():\n    asozo_ca                       = asthmaca.merge(ozocafinal, how=""left"", on=""county"")\n    asozono_ca                     = asozo_ca.merge(nocafinal, how=""left"", on=""county"")\n    asozonoco_ca                   = asozono_ca.merge(cocafinal, how=""left"", on=""county"")\n    asozonocoso_ca                 = asozonoco_ca.merge(socafinal, how=""left"", on=""county"")\n    asozonocosovocs_ca             = asozonocoso_ca.merge(vocscafinal, how=""left"", on=""county"")\n    asozonocosovocshaps_ca         = asozonocosovocs_ca.merge(hapscafinal, how=""left"", on=""county"")\n    asozonocosovocshapsnonox_ca    = asozonocosovocshaps_ca.merge(nonoxcafinal, how=""left"", on=""county"")\n\n    asozonocosovocshapsnonoxlead_ca                         = asozonocosovocshaps_ca.merge(leadcafinal, how=""left"", on=""county"")\n    asozonocosovocshapsnonoxpm10_ca                         = asozonocosovocshapsnonoxlead_ca.merge(pm10cafinal, how=""left"", on=""county"")\n    asozonocosovocshapsnonoxpm10pm2_5_ca                    = asozonocosovocshapsnonoxpm10_ca.merge(pm2_5cafinal, how=""left"", on=""county"")\n    asozonocosovocshapsnonoxpm10pm2_5pm2_5non_ca            = asozonocosovocshapsnonoxpm10pm2_5_ca.merge(pm2_5noncafinal, how=""left"", on=""county"")\n    asozonocosovocshapsnonoxpm10pm2_5pm2_5nonpm2_5spec_ca   = asozonocosovocshapsnonoxpm10pm2_5pm2_5non_ca.merge(pm2_5speccafinal, how=""left"", on=""county"")\n\n    return asozonocosovocshapsnonoxpm10pm2_5pm2_5nonpm2_5spec_ca\n'"
src/charts.py,0,"b""import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom get_feat_imps import get_feat_imps\nfrom data_processing import nice_column_names\nfrom data_processing import show_columns\n\ndef get_imps_names():\n    _, feat_imps, cols = get_feat_imps()\n\n    col_dict = nice_column_names()\n    X_train_dot_columns = cols\n    something = [col_dict.get(x, x) for x in X_train_dot_columns]\n    imps, names = zip(*sorted(zip(feat_imps, [col_dict.get(x) for x in X_train_dot_columns])))\n    return imps, names\n\ndef create_feat_imp_chart():\n    imps, names = get_imps_names()\n\n    plt.style.use('bmh')\n    plt.barh(range(len(names)), imps, align='center')\n    plt.yticks(range(len(names)), names)\n    plt.xlabel('Relative Importance of Features', fontsize=10)\n    plt.ylabel('Features', fontsize=10)\n    plt.tight_layout()\n    plt.savefig('static/images/feat_imps.png')\n\nif __name__ == '__main__':\n    create_feat_imp_chart()\n"""
src/co_data.py,0,"b'import numpy as np\nimport pandas as pd\n# source data: https://data-cdphe.opendata.arcgis.com/datasets/asthma-hospitalization-rate-counties\n\ndef get_data():\n    # def pm2_5spec_data():\n    pm2_5spec = pd.read_csv(\'../data/daily_88502_2017.csv\')\n    pm2_5spec.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    pm2_5specco = pm2_5spec[pm2_5spec[\'state\'] == \'Colorado\']\n    pm2_5speccoslim = pm2_5specco.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    pm2_5speccoslim = pm2_5speccoslim.reset_index()\n    pm2_5speccoslim = pm2_5speccoslim.drop([\'index\'], axis=1)\n    pm2_5speccoslim[\'obs_x_mean\'] = pm2_5speccoslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    pm2_5speccoslim.county = pm2_5speccoslim.county.str.lower()\n    pm2_5speccogroupby = pm2_5speccoslim.groupby(\'county\').sum()\n    pm2_5speccogroupby = pm2_5speccogroupby.reset_index()\n    pm2_5speccogroupby[\'new_mean\'] = pm2_5speccogroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    pm2_5speccofinal = pm2_5speccogroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    pm2_5speccofinal.columns = [\'county\', \'pm2_5spec_mean\']\n\n    # def pm2_5non_data():\n    pm2_5non = pd.read_csv(\'../data/daily_88502_2017.csv\')\n    pm2_5non.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    pm2_5nonco = pm2_5non[pm2_5non[\'state\'] == \'Colorado\']\n    pm2_5noncoslim = pm2_5nonco.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    pm2_5noncoslim = pm2_5noncoslim.reset_index()\n    pm2_5noncoslim = pm2_5noncoslim.drop([\'index\'], axis=1)\n    pm2_5noncoslim[\'obs_x_mean\'] = pm2_5noncoslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    pm2_5noncoslim.county = pm2_5noncoslim.county.str.lower()\n    pm2_5noncogroupby = pm2_5noncoslim.groupby(\'county\').sum()\n    pm2_5noncogroupby = pm2_5noncogroupby.reset_index()\n    pm2_5noncogroupby[\'new_mean\'] = pm2_5noncogroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    pm2_5noncofinal = pm2_5noncogroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    pm2_5noncofinal.columns = [\'county\', \'pm2_5non_mean\']\n\n    # def pm2_5_data():\n    pm2_5 = pd.read_csv(\'../data/daily_88101_2017.csv\')\n    pm2_5.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    pm2_5co = pm2_5[pm2_5[\'state\'] == \'Colorado\']\n    pm2_5coslim = pm2_5co.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    pm2_5coslim = pm2_5coslim.reset_index()\n    pm2_5coslim = pm2_5coslim.drop([\'index\'], axis=1)\n    pm2_5coslim[\'obs_x_mean\'] = pm2_5coslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    pm2_5coslim.county = pm2_5coslim.county.str.lower()\n    pm2_5cogroupby = pm2_5coslim.groupby(\'county\').sum()\n    pm2_5cogroupby = pm2_5cogroupby.reset_index()\n    pm2_5cogroupby[\'new_mean\'] = pm2_5cogroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    pm2_5cofinal = pm2_5cogroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    pm2_5cofinal.columns = [\'county\', \'pm2_5_mean\']\n\n    # def pm10_data():\n    pm10 = pd.read_csv(\'../data/daily_81102_2017.csv\')\n    pm10.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    pm10co = pm10[pm10[\'state\'] == \'Colorado\']\n    pm10coslim = pm10co.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    pm10coslim = pm10coslim.reset_index()\n    pm10coslim = pm10coslim.drop([\'index\'], axis=1)\n    pm10coslim[\'obs_x_mean\'] = pm10coslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    pm10coslim.county = pm10coslim.county.str.lower()\n    pm10cogroupby = pm10coslim.groupby(\'county\').sum()\n    pm10cogroupby = pm10cogroupby.reset_index()\n    pm10cogroupby[\'new_mean\'] = pm10cogroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    pm10cofinal = pm10cogroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    pm10cofinal.columns = [\'county\', \'pm10_mean\']\n\n    # def lead_data():\n    lead = pd.read_csv(\'../data/daily_LEAD_2014.csv\')\n    lead.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    leadco = lead[lead[\'state\'] == \'Colorado\']\n    leadcoslim = leadco.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    leadcoslim = leadcoslim.reset_index()\n    leadcoslim = leadcoslim.drop([\'index\'], axis=1)\n    leadcoslim[\'obs_x_mean\'] = leadcoslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    leadcoslim.county = leadcoslim.county.str.lower()\n    leadcogroupby = leadcoslim.groupby(\'county\').sum()\n    leadcogroupby = leadcogroupby.reset_index()\n    leadcogroupby[\'new_mean\'] = leadcogroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    leadcofinal = leadcogroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    leadcofinal.columns = [\'county\', \'lead_mean\']\n\n    # def nonox_data():\n    nonox = pd.read_csv(\'../data/daily_NONOxNOy_2017.csv\')\n    nonox.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    nonoxco = nonox[nonox[\'state\'] == \'Colorado\']\n    nonoxcocoslim = nonoxco.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    nonoxcocoslim = nonoxcocoslim.reset_index()\n    nonoxcocoslim = nonoxcocoslim.drop([\'index\'], axis=1)\n    nonoxcocoslim[\'obs_x_mean\'] = nonoxcocoslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    nonoxcocoslim.county = nonoxcocoslim.county.str.lower()\n    nonoxcogroupby = nonoxcocoslim.groupby(\'county\').sum()\n    nonoxcogroupby = nonoxcogroupby.reset_index()\n    nonoxcogroupby[\'new_mean\'] = nonoxcogroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    nonoxcofinal = nonoxcogroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    nonoxcofinal.columns = [\'county\', \'nonox_mean\']\n\n    # def haps_data():\n    haps = pd.read_csv(\'../data/daily_HAPS_2017.csv\')\n    haps.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    hapsco = haps[haps[\'state\'] == \'Colorado\']\n    hapscoslim = hapsco.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    hapscoslim = hapscoslim.reset_index()\n    hapscoslim = hapscoslim.drop([\'index\'], axis=1)\n    hapscoslim[\'obs_x_mean\'] = hapscoslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    hapscoslim.county = hapscoslim.county.str.lower()\n    hapscogroupby = hapscoslim.groupby(\'county\').sum()\n    hapscogroupby = hapscogroupby.reset_index()\n    hapscogroupby[\'new_mean\'] = hapscogroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    hapscofinal = hapscogroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    hapscofinal.columns = [\'county\', \'haps_mean\']\n\n    # def vocs_data():\n    # vocs16 = pd.read_csv(\'daily_VOCS_2016.csv\')\n    vocs = pd.read_csv(\'../data/daily_VOCS_2017.csv\')\n    vocs.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    vocsco = vocs[vocs[\'state\'] == \'Colorado\']\n    vocscoslim = vocsco.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    vocscoslim = vocscoslim.reset_index()\n    vocscoslim = vocscoslim.drop([\'index\'], axis=1)\n    vocscoslim[\'obs_x_mean\'] = vocscoslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    vocscoslim.county = vocscoslim.county.str.lower()\n    vocscogroupby = vocscoslim.groupby(\'county\').sum()\n    vocscogroupby = vocscogroupby.reset_index()\n    vocscogroupby[\'new_mean\'] = vocscogroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    vocscofinal = vocscogroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    vocscofinal.columns = [\'county\', \'vocs_mean\']\n\n    # def co_data():\n    co = pd.read_csv(\'../data/daily_42101_2017.csv\')\n    co.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    coco = co[co[\'state\'] == \'Colorado\']\n    cocoslim = coco.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    cocoslim = cocoslim.reset_index()\n    cocoslim = cocoslim.drop([\'index\'], axis=1)\n    cocoslim[\'obs_x_mean\'] = cocoslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    cocoslim.county = cocoslim.county.str.lower()\n    cocogroupby = cocoslim.groupby(\'county\').sum()\n    cocogroupby = cocogroupby.reset_index()\n    cocogroupby[\'new_mean\'] = cocogroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    cocofinal = cocogroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    cocofinal.columns = [\'county\', \'co_mean\']\n\n    # def so2_data():\n    so2 = pd.read_csv(\'../data/daily_42401_2017.csv\')\n    so2.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    soco = so2[so2[\'state\'] == \'New Jersey\']\n    socoslim = soco.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    socoslim = socoslim.reset_index()\n    socoslim = socoslim.drop([\'index\'], axis=1)\n    socoslim[\'obs_x_mean\'] = socoslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    socoslim.county = socoslim.county.str.lower()\n    socogroupby = socoslim.groupby(\'county\').sum()\n    socogroupby = socogroupby.reset_index()\n    socogroupby[\'new_mean\'] = socogroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    socofinal = socogroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    socofinal.columns = [\'county\', \'so_mean\']\n\n    # def no2_data():\n    no2 = pd.read_csv(\'../data/daily_42602_2017.csv\')\n    no2.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    noco = no2[no2[\'state\'] == \'Colorado\']\n    nocoslim = noco.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    nocoslim.county = nocoslim.county.str.lower()\n    nocoslim = nocoslim.reset_index()\n    nocoslim = nocoslim.drop([\'index\'], axis=1)\n    nocoslim[\'obs_x_mean\'] = nocoslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    nocogroupby = nocoslim.groupby(\'county\').sum()\n    nocogroupby = nocogroupby.reset_index()\n    nocogroupby[\'new_mean\'] = nocogroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    nocofinal = nocogroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    nocofinal.columns = [\'county\', \'no_mean\']\n\n    # def ozone_data():\n    ozo = pd.read_csv(\'../data/daily_44201_2017.csv\')\n    ozoslim = ozo.drop([\'State Code\', \'County Code\', \'Site Num\', \'POC\',\n           \'Latitude\', \'Longitude\', \'Datum\', \'Sample Duration\',\n           \'Pollutant Standard\', \'Date Local\', \'Units of Measure\', \'Event Type\',\n            \'Observation Percent\',\n           \'1st Max Value\', \'1st Max Hour\', \'AQI\', \'Method Code\', \'Method Name\',\n           \'Local Site Name\', \'Address\', \'City Name\',\n           \'CBSA Name\', \'Date of Last Change\'], axis=1)\n    ozoslim.columns = [\'param_cd\', \'param\', \'obs_count\', \'mean_avg\', \'state\', \'county\']\n    ozoco = ozoslim[ozoslim[\'state\'] == \'Colorado\']\n    ozoco.county = ozoco.county.str.lower()\n    ozoco = ozoco.reset_index()\n    ozoco = ozoco.drop([\'index\', \'param\', \'state\', \'param_cd\'], axis= 1)\n    ozoco[\'obs_x_mean\']  = ozoco.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    ozocogroupby = ozoco.groupby(\'county\').sum()\n    ozocogroupby = ozocogroupby.reset_index()\n    ozocogroupby[\'new_mean\'] = ozocogroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    ozocofinal = ozocogroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\'], axis=1)\n    ozocofinal.columns = [\'county\', \'ozo_mean\']\n\n    # def asthma_co():\n    # https://data-cdphe.opendata.arcgis.com/datasets/asthma-hospitalization-rate-counties\n    asthmahospco = pd.read_csv(\'../data/asthma_hospitization_co.csv\')\n    asthmahospco.columns = asthmahospco.columns.str.lower()\n    asthmahospco = asthmahospco.drop([\'objectid\', \'county_fips\', \'l95ci\', \'u95ci\',\n           \'stateadjrate\', \'sl95ci\', \'su95ci\', \'display\'], axis=1)\n    asthmahospco.county_name = asthmahospco.county_name.str.lower()\n    asthmahospco.columns = [\'county\', \'asthma_rate\']\n    asthmahospco[\'asthma_rate\']= asthmahospco.apply(lambda row: row.asthma_rate / 10, axis=1)\n\n    asthmaco = asthmahospco\n\n    # def merge_all():\n    asozo_co                       = asthmaco.merge(ozocofinal, how=""left"", on=""county"")\n    asozono_co                     = asozo_co.merge(nocofinal, how=""left"", on=""county"")\n    asozonoco_co                   = asozono_co.merge(cocofinal, how=""left"", on=""county"")\n    asozonocoso_co                 = asozonoco_co.merge(socofinal, how=""left"", on=""county"")\n    asozonocosovocs_co             = asozonocoso_co.merge(vocscofinal, how=""left"", on=""county"")\n    asozonocosovocshaps_co         = asozonocosovocs_co.merge(hapscofinal, how=""left"", on=""county"")\n    asozonocosovocshapsnonox_co    = asozonocosovocshaps_co.merge(nonoxcofinal, how=""left"", on=""county"")\n\n    asozonocosovocshapsnonoxlead_co                         = asozonocosovocshaps_co.merge(leadcofinal, how=""left"", on=""county"")\n    asozonocosovocshapsnonoxpm10_co                         = asozonocosovocshapsnonoxlead_co.merge(pm10cofinal, how=""left"", on=""county"")\n    asozonocosovocshapsnonoxpm10pm2_5_co                    = asozonocosovocshapsnonoxpm10_co.merge(pm2_5cofinal, how=""left"", on=""county"")\n    asozonocosovocshapsnonoxpm10pm2_5pm2_5non_co            = asozonocosovocshapsnonoxpm10pm2_5_co.merge(pm2_5noncofinal, how=""left"", on=""county"")\n    asozonocosovocshapsnonoxpm10pm2_5pm2_5nonpm2_5spec_co   = asozonocosovocshapsnonoxpm10pm2_5pm2_5non_co.merge(pm2_5speccofinal, how=""left"", on=""county"")\n\n    return asozonocosovocshapsnonoxpm10pm2_5pm2_5nonpm2_5spec_co\n'"
src/color_map.py,0,"b'import csv\nfrom bs4 import BeautifulSoup\nfrom data import asthma_ca\nfrom data import asthma_co\nfrom data import asthma_fl\nfrom data import asthma_nj\n\n\n\nco = asthma_co()[1]\nd = co.set_index(\'fips\').to_dict()[\'asthma_rate\']\nprint(\'Asthma Map Dictionary: \', d)\n\n# Load the SVG map\nsvg = open(\'../data/colorado_election.svg\', \'r\').read()\n\n# Load into Beautiful Soup\nsoup = BeautifulSoup(svg, selfClosingTags=[\'defs\',\'sodipodi:namedview\'])\n\n# Find counties\npaths = soup.findAll(\'path\')\n\n# Map colors\ncolors = [""#f7fbff"", ""#deebf7"", ""#c6dbef"", ""#9ecae1"", ""#6baed6"", ""#4292c6"", ""#2171b5"", ""#08519c"", ""#08306b""]\n# colors = [""#fff7f3"", ""#fde0dd"", ""#fcc5c0"", ""#fa9fb5"", ""#f768a1"", ""#dd3497"", ""#ae017e"", ""#7a0177"", ""#49006a""]\n\n\n\n\n# County style\npath_style = \'font-size:12px;fill-rule:nonzero;stroke:#FFFFFF;stroke-opacity:1; stroke-width:0.1;stroke-miterlimit:4;stroke-dasharray:none;stroke-linecap:butt; marker-start:none;stroke-linejoin:bevel;fill:\'\n\n# Color the counties based on unemployment rate\nfor p in paths:\n\n    if p[\'id\'] not in [""State_Lines"", ""separator""]:\n        # pass\n        try:\n            rate = d[p[\'id\']]\n        except:\n            continue\n\n        if rate > 80:\n            color_class = 8\n        elif rate > 70:\n            color_class = 7\n        elif rate > 60:\n            color_class = 6\n        elif rate > 50:\n            color_class = 5\n        elif rate > 40:\n            color_class = 4\n        elif rate > 30:\n            color_class = 3\n        elif rate > 20:\n            color_class = 2\n        elif rate > 10:\n            color_class = 1\n        else:\n            color_class = 0\n\n        color = colors[color_class]\n        p[\'style\'] = path_style + color\n\n# Output map\nmymap = soup.prettify()\nprint(type(mymap))\n\nopen(\'static/images/colorado_asthma.svg\', \'w\').write(mymap)\n'"
src/color_map_pollutant.py,0,"b'import csv\nfrom bs4 import BeautifulSoup\n\nfrom data import all_socio_econ_data\n\n_, fips_lookup = all_socio_econ_data()\nfips_feature_dict = fips_lookup.set_index(\'fips\').to_dict()\n\nfips_lookup_co = fips_lookup[fips_lookup.state == \'colorado\']\nfips_feature_dict_co = fips_lookup_co.set_index(\'fips\').to_dict()\n\nchart_list = [\'obese_adult\', \'smoke_adult\', \'uninsured\', \'air_poll_partic\', \'unemployment\']\n\n# colors from http://colorbrewer2.org\ncolors_dict = { \'obese_adult\'       : [""#fff5f0"", ""#fee0d2"", ""#fcbba1"", ""#fc9272"", ""#fb6a4a"", ""#ef3b2c"", ""#cb181d"", ""#a50f15"", ""#67000d""],\n                \'smoke_adult\'       : [""#fcfbfd"", ""#efedf5"", ""#dadaeb"", ""#bcbddc"", ""#9e9ac8"", ""#807dba"", ""#6a51a3"", ""#54278f"", ""#3f007d""],\n                \'uninsured\'         : [""#fff5eb"", ""#fee6ce"", ""#fdd0a2"", ""#fdae6b"", ""#fd8d3c"", ""#f16913"", ""#d94801"", ""#a63603"", ""#7f2704""],\n                \'air_poll_partic\'   : [""#f7fcf5"", ""#e5f5e0"", ""#c7e9c0"", ""#a1d99b"", ""#74c476"", ""#41ab5d"", ""#238b45"", ""#006d2c"", ""#00441b""],\n                \'unemployment\'      : [""#f7fbff"", ""#deebf7"", ""#c6dbef"", ""#9ecae1"", ""#6baed6"", ""#4292c6"", ""#2171b5"", ""#08519c"", ""#08306b""]\n                }\n\n# load SVG map\nsvg = open(\'static/images/co_counties_blank.svg\', \'r\').read()\n\n# load into Beautiful Soup\nsoup = BeautifulSoup(svg, selfClosingTags=[\'defs\',\'sodipodi:namedview\'])\n\n# find counties\npaths = soup.findAll(\'path\')\n\n# county style\npath_style = \'font-size:12px;fill-rule:nonzero;stroke:#FFFFFF;stroke-opacity:1; stroke-width:0.1;stroke-miterlimit:4;stroke-dasharray:none;stroke-linecap:butt; marker-start:none;stroke-linejoin:bevel;fill:\'\n\n# create each map\nfor feature in chart_list:\n    # d = fips_feature_dict[feature]\n    d = fips_feature_dict_co[feature]\n\n    # get colors for this map\n    colors = colors_dict[feature]\n\n    # find min and max values\n    key_min = min(d.keys(), key=(lambda k: d[k]))\n    val_min = d[key_min]\n    key_max = max(d.keys(), key=(lambda k: d[k]))\n    val_max = d[key_max]\n\n    # calculate 10% steps\n    val_10 = val_min + (val_max-val_min)*1/10\n    val_20 = val_min + (val_max-val_min)*2/10\n    val_30 = val_min + (val_max-val_min)*3/10\n    val_40 = val_min + (val_max-val_min)*4/10\n    val_50 = val_min + (val_max-val_min)*5/10\n    val_60 = val_min + (val_max-val_min)*6/10\n    val_70 = val_min + (val_max-val_min)*7/10\n    val_80 = val_min + (val_max-val_min)*8/10\n    val_90 = val_min + (val_max-val_min)*9/10\n\n    # color the counties based on feature rate\n    for p in paths:\n\n        if p[\'id\'] not in [""State_Lines"", ""separator""]:\n            # pass\n            try:\n                rate = d[p[\'id\']]\n\n            except:\n                continue\n\n            if rate > val_90:\n                color_class = 8\n            elif rate > val_80:\n                color_class = 7\n            elif rate > val_70:\n                color_class = 6\n            elif rate > val_60:\n                color_class = 5\n            elif rate > val_40:\n                color_class = 4\n            elif rate > val_30:\n                color_class = 3\n            elif rate > val_20:\n                color_class = 2\n            elif rate > val_10:\n                color_class = 1\n            else:\n                color_class = 0\n\n            color = colors[color_class]\n            p[\'style\'] = path_style + color\n\n    # output and save each map\n    mymap = soup.prettify()\n    filename = \'static/images/co_{}.svg\'.format(feature)\n    open(filename, \'w\').write(mymap)\n'"
src/combine_data.py,0,"b'import numpy as np\nimport pandas as pd\n\n\ndef join_data():\n    from co_data import get_data as colorado\n    from ca_data import get_data as california\n    from nj_data import get_data as newjersey\n    from fl_data import get_data as florida\n    from us_data import get_data as socio_economic\n\n    # def combine_tables():\n    table1 = colorado()\n    table2 = california()\n    table3 = newjersey()\n    table4 = florida()\n    socioecon = socio_economic()\n\n    join_cocanjfl = pd.concat([table1, table2, table3, table4])\n    join_cocanjfl = join_cocanjfl.reset_index()\n    join_cocanjfl = join_cocanjfl.drop([\'index\'], axis=1)\n\n    socio_pollute = socioecon.merge(join_cocanjfl, how=""left"", on=""county"")\n\n    print(\'socio_pollute.head():\', socio_pollute.head())\n    print(\'states in data:\', socio_pollute.state.unique())\n    return socio_pollute\n\n\nif __name__ == \'__main__\':\n    join_data()\n'"
src/comparison.py,0,"b""import pandas as pd\nimport numpy as np\n\nfrom ols_model_hot_one import train_model as ols_tm\nfrom rfr_model_hot_one import train_model as rfr_tm\nfrom rfr_model_hot_one import eval_model\nfrom svr_model_hot_one import train_model as svr_tm\nfrom lr_model_hot_one import train_model as lr_tm\nfrom knn_model_hot_one import train_model as knn_tm\n\n\ndef compare_models():\n    model_dict = {  'Random Forest': rfr_tm,\n                    }\n\n    comparison_dict = {}\n    for model_name, model in model_dict.items():\n        _, rmse_train, rmse_test, r2score, _ = eval_model()\n        comparison_dict[model_name] =  rmse_train.round(2), rmse_test.round(2), r2score.round(2)\n\n    return comparison_dict\n"""
src/data.py,0,"b'import numpy as np\nimport pandas as pd\n\n# source data: https://data.chhs.ca.gov/dataset/asthma-ed-visit-rates-lghc-indicator-07\n\n# ****************** CREATE VARIABLES ******************\nepa_raw =    { \'pm10\'      : \'daily_81102_2017\',\n                \'pm25\'      : \'daily_88101_2017\',\n                \'pm25non\'   : \'daily_88502_2017\',\n                \'pm25spec\'  : \'daily_SPEC_2017\',\n                \'co\'        : \'daily_42101_2017\',\n                \'so2\'       : \'daily_42401_2017\',\n                \'no2\'       : \'daily_42602_2017\',\n                \'ozo\'       : \'daily_44201_2017\',\n                \'nonox\'     : \'daily_NONOxNOy_2017\',\n                \'lead\'      : \'daily_LEAD_2014\',\n                \'haps\'      : \'daily_HAPS_2017\',\n                \'vocs\'      : \'daily_VOCS_2017\'\n                }\n\ncolumn_list =   [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n   \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n   \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n   \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n   \'county\', \'city\', \'cbsa\', \'last_change_date\']\n\ncolumns_to_drop = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n   \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n   \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n   \'city\', \'cbsa\', \'last_change_date\']\n\ncolumns_to_drop2 = [\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\']\n\nstates_codes = {    \'California\': \'ca\',\n                    \'Colorado\'  : \'co\',\n                    \'Florida\'   : \'fl\',\n                    \'New Jersey\': \'nj\'\n                    }\n\n# ****************** CREATE INITIAL DATASETS ******************\ndef all_socio_econ_data():\n\n    # source data: http://www.countyhealthrankings.org/rankings/data\n    xls = pd.ExcelFile(\'../data/2016CountyHealthRankingsData.xls\')\n    df = pd.read_excel(xls, \'Ranked Measure Data\') # select tab with data\n\n    csv_file_path = \'../data/newcsv.csv\'\n    df = df.to_csv(csv_file_path)\n    df = pd.read_csv(csv_file_path)\n    df = df.drop(\'Unnamed: 0\', axis=1)\n\n    df = df.iloc[:, [1,2,27,31,63,68,95,105,116,135]] # get: state, county, adult smoking, adult obesity, uninsured, PCP (doctors) rate, high school graduation, unemployment, income inequality, air pollution,\n    df.columns = [\'state\', \'county\',\'smoke_adult\', \'obese_adult\', \'uninsured\', \'pcp\', \'high_sch_grad\', \'unemployment\', \'income_ineq\', \'air_poll_partic\']\n    df = df.drop([0]) # drop first row (previously header)\n    df.reset_index(level=0, drop=True, inplace=True) # reset index and drop the old index as column\n    df.county = df.county.str.lower()\n    df.state = df.state.str.lower()\n    df_num = df.iloc[:, 2:10].apply(pd.to_numeric) # make all other columns numerical (this will enable .describe() for each column)\n\n    # join back up all columns in a way that avoids numeric columns becoming all ""NaN""\n    df_soc_econ = pd.concat([df.state.reset_index(drop=True), df.county.reset_index(drop=True), df_num.reset_index(drop=True)], axis=1)\n\n    fips = make_fips_df()\n    fips = fips.drop([\'state\'], axis=1)\n    socio_econ_fips = df_soc_econ.merge(fips, how=""left"", on=""county"")\n    socio_econ_fips = socio_econ_fips.drop([\'county\'], axis=1)\n\n    return df_soc_econ, socio_econ_fips # this is all the socio-economic data in one dataframe\n\ndef asthma_ca():\n    # https://data.chhs.ca.gov/dataset/asthma-emergency-department-visit-rates-by-zip-code\n    # asthma rate quoted as per 10,000 people\n    asthma_ca_drop_lst = [\'zip\', \'age\', \'visits\', \'fips\']\n    df = pd.read_csv(\'../data/asthma_hospitization_ca.csv\')\n    df.columns = [\'year\', \'zip\', \'age\', \'visits\', \'asthma_rate\', \'fips\', \'county\']\n    df = df.drop(asthma_ca_drop_lst, axis=1)\n    df = df[df[\'year\']==2015]\n    df = df.drop(\'year\', axis=1)\n    df.county = df.county.str.lower()\n    df = df.reset_index()\n    df = df.drop(\'index\', axis=1)\n    df = df.groupby(\'county\').mean()\n    df = df.reset_index()\n\n    fips = make_fips_df()[make_fips_df()[\'state\']==\'california\']\n    fips = fips.drop([\'state\'], axis=1)\n    asthma_fips = df.merge(fips, how=""left"", on=""county"")\n    asthma_fips = asthma_fips.drop([\'county\'], axis=1)\n    return df, asthma_fips # this is asthma data for this state\n\ndef asthma_co():\n    # https://data-cdphe.opendata.arcgis.com/datasets/asthma-hospitalization-rate-counties\n    # asthma rate quoted as per 100,000 people\n    asthma_co_drop_lst = [  \'objectid\', \'county_fips\', \'l95ci\', \'u95ci\',\n                            \'stateadjrate\', \'sl95ci\', \'su95ci\', \'display\']\n    df = pd.read_csv(\'../data/asthma_hospitization_co.csv\')\n    df.columns = df.columns.str.lower()\n    df = df.drop(asthma_co_drop_lst, axis=1)\n    df.county_name = df.county_name.str.lower()\n    df.columns = [\'county\', \'asthma_rate\']\n    df[\'asthma_rate\']= df.apply(lambda row: row.asthma_rate / 10, axis=1)\n\n    fips = make_fips_df()[make_fips_df()[\'state\']==\'colorado\']\n    fips = fips.drop([\'state\'], axis=1)\n    asthma_fips = df.merge(fips, how=""left"", on=""county"")\n    asthma_fips = asthma_fips.drop([\'county\'], axis=1)\n    return df, asthma_fips # this is asthma data for this state\n\ndef asthma_fl():\n    # http://www.flhealthcharts.com/charts/OtherIndicators/NonVitalIndDataViewer.aspx?cid=9755\n    # asthma rate quoted as per 100,000 people\n    df = pd.read_csv(\'../data/asthma_hospitization_fl.csv\')\n    df[\'asthma_rate\']= df.apply(lambda row: row.asthma_rate +.01, axis=1) # remove any zeros from \'asthma_rate\' column\n    df[\'asthma_rate\']= df.apply(lambda row: row.asthma_rate / 10, axis=1) # convert rate to per 10,000 (from per 100,000)\n    df = df.drop(\'Unnamed: 0\', axis=1)\n\n    fips = make_fips_df()[make_fips_df()[\'state\']==\'florida\']\n    fips = fips.drop([\'state\'], axis=1)\n    asthma_fips = df.merge(fips, how=""left"", on=""county"")\n    asthma_fips = asthma_fips.drop([\'county\'], axis=1)\n    return df, asthma_fips # this is asthma data for this state\n\ndef asthma_nj():\n    # https://www26.state.nj.us/doh-shad/indicator/view/NJASTHMAHOSP.stateAAR.html\n    # asthma rate quoted as per 10,000 people\n    df = pd.read_csv(\'../data/asthma_hospitization_nj.csv\')\n    df.county = df.county.str.lower()\n\n    fips = make_fips_df()[make_fips_df()[\'state\']==\'new jersey\']\n    fips = fips.drop([\'state\'], axis=1)\n    asthma_fips = df.merge(fips, how=""left"", on=""county"")\n    asthma_fips = asthma_fips.drop([\'county\'], axis=1)\n    return df, asthma_fips # this is asthma data for this state\n\ndef make_fips_df():\n    fips = pd.ExcelFile(\'../data/fips.xls\')\n    fips = pd.read_excel(fips)\n    fips.columns = [\'state\', \'county\', \'fips_state\', \'fips_county\']\n    fips = fips.iloc[1:,:]\n    fips.reset_index(level=0, drop=True, inplace=True)\n    fips[\'fips\'] = fips.fips_state + fips.fips_county\n    fips = fips.drop([\'fips_state\', \'fips_county\'], axis=1)\n    fips.state = fips.state.str.lower()\n    fips.county = fips.county.str.lower()\n    return fips\n\ndef make_fips_dicts(state, code):\n    # for state, code in states_codes.items():\n    df = \'fips_{}\'.format(code)\n    df = make_fips_df()\n    df = df[df[\'state\']==state]\n    d = \'fips_dict_{}\'.format(code)\n    d = df.set_index(\'county\').to_dict()[\'fips\']\n    return d\n\n# ****************** ASSEMBLE DATASETS ******************\ndef choose_states():\n    states = [\'California\', \'Colorado\', \'Florida\', \'New Jersey\']\n    # states = [\'California\', \'Colorado\', \'Florida\']\n    # states = [\'California\', \'Colorado\', \'New Jersey\']\n    # states = [\'California\', \'Colorado\']\n    return states # this is just a list of states\n\ndef populate_epa_state(state_name, state_code):\n    epa_state = \'epa_{}\'.format(state_code)\n    epa_state = dict()\n    for pollutant, filename in epa_raw.items():\n        dfname = \'{}_{}\'.format(pollutant, state_code)\n        epa_state[dfname] = make_pollutant_df(filename, state_name, pollutant)\n    # print(\'populate_epa_state:\', epa_state)\n    return epa_state # this is dictionary with epa pollutants for this state\n\ndef make_pollutant_df(file, state_name, pollutant):\n    df = pd.read_csv(\'../data/{}.csv\'.format(file))\n    df.columns = column_list\n    df = df[df[\'state\'] == state_name]\n    df = df.drop(columns_to_drop, axis=1)\n    df = df.reset_index()\n    df = df.drop([\'index\'], axis=1)\n    df[\'obs_x_mean\'] = df.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    df.county = df.county.str.lower()\n    df = df.groupby(\'county\').sum()\n    df = df.reset_index()\n    df[\'new_mean\'] = df.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    df = df.drop(columns_to_drop2, axis=1)\n    df.columns = [\'county\', \'{}_mean\'.format(pollutant)]\n    print(\'make_pollutant_df:\', df.columns)\n    return df # this is a dataframe for a pollutant for a given state\n\ndef join_side_by_side(state):\n    asthma_datasets = { \'California\'    :   asthma_ca()[0],\n                        \'Colorado\'      :   asthma_co()[0],\n                        \'Florida\'       :   asthma_fl()[0],\n                        \'New Jersey\'    :   asthma_nj()[0]\n                        }\n\n    df = asthma_datasets[state]\n    for name, dataset in populate_epa_state(state, states_codes[state]).items():\n        df = df.merge(dataset, how=""left"", on=""county"")\n    print(\'join_side_by_side:\', df.columns)\n    return df # this is a dataframe with asthma and pollutant data for given state\n\ndef get_each_state_data(state):\n    # socio-economic data for this state\n    data, _ = all_socio_econ_data()\n    state_socio_econ_data = data[data[\'state\'] == state.lower()]\n\n    # asthma-pollutant data for this state\n    asthma_pollutants = join_side_by_side(state)\n\n    # join socio-economic and asthma and pollutant data into one df\n    df = asthma_pollutants.merge(state_socio_econ_data, how=""left"", on=""county"")\n    print(\'get_each_state_data:\', df.columns)\n    return df # this is all the data for this state\n\n\ndef add_states_one_hot_cols(data):\n    """"""takes data, adds a one-hot-encode column for each state and returns data""""""\n    for state, abrev in states_codes.items():\n        data[abrev] = data.state == state.lower()\n\n    return data\n\ndef join_data():\n    list_of_each_states_data = []\n    for state in choose_states():\n        each_state_data = get_each_state_data(state)\n        list_of_each_states_data.append(each_state_data)\n    df = pd.concat(list_of_each_states_data)\n    df = df.reset_index()\n    df = df.drop([\'index\'], axis=1)\n    print(\'join_data:\', df.columns)\n    df = add_states_one_hot_cols(df)\n\n    return df # this is all the data stacked for the chosen states\n\n\nif __name__ == \'__main__\':\n    join_data()\n'"
src/data_processing.py,0,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        from data import join_data as data\n        data = data()\n        data.to_csv(csv_file_path, index=False)\n\n    return data\n\ndef nice_column_names():\n    col_dict = { \'air_poll_partic\': \'Particulate Air Pollution\',\n                 \'asthma_rate\': \'Asthma Rate\',\n                 \'co_mean\': \'CO - Pollutant\',\n                 \'county\': \'County\',\n                 \'haps_mean\': \'HAPS - Pollutant\',\n                 \'high_sch_grad\': \'High School Grads\',\n                 \'income_ineq\': \'Income Inequality\',\n                 \'lead_mean\': \'Lead - Pollutant\',\n                 \'no2_mean\': \'NO2 - Pollutant\',\n                 \'nonox_mean\': \'NONOxNOy - Pollutant\',\n                 \'obese_adult\': \'Obesity (Adult)\',\n                 \'ozo_mean\': \'Ozone - Pollutant\',\n                 \'pcp\': \'PCP\',\n                 \'pm10_mean\': \'PM10 - Pollutant\',\n                 \'pm25_mean\': \'PM2.5 Pollutant\',\n                 \'pm25non_mean\': \'PM2.5 non FRM Pollutant\',\n                 \'pm25spec_mean\': \'PM2.5 Spec - Pollutant\',\n                 \'smoke_adult\': \'Smokers (Adult)\',\n                 \'so2_mean\': \'SO2 - Pollutant\',\n                 \'state\': \'State\',\n                 \'unemployment\': \'Unemployment\',\n                 \'uninsured\': \'Uninsured Rate\',\n                 \'vocs_mean\': \'VOCS - Pollutant\',\n                 \'co\': \'Colo.\',\n                 \'fl\': \'Fla.\',\n                 \'nj\': \'N.J.\',\n                 \'ca\': \'Calif.\'\n                 }\n\n    return col_dict\n\ndef feature_selection(data):\n\n    all_columns = [\'pm10_mean\', \'pm25_mean\', \'pm25non_mean\', \'pm25spec_mean\', \'co_mean\',\n       \'so2_mean\', \'no2_mean\', \'ozo_mean\', \'nonox_mean\', \'lead_mean\',\n       \'haps_mean\', \'vocs_mean\', \'smoke_adult\', \'obese_adult\', \'uninsured\',\n       \'pcp\', \'high_sch_grad\', \'unemployment\', \'income_ineq\', \'air_poll_partic\']\n\n    drop_columns = [\'pm10_mean\', \'pm25_mean\', \'pm25non_mean\', \'pm25spec_mean\', \'co_mean\',\n        \'no2_mean\', \'ozo_mean\', \'nonox_mean\', \'lead_mean\',\n       \'haps_mean\', \'vocs_mean\', \'pcp\', \'high_sch_grad\', \'income_ineq\', \'co\', \'ca\']\n\n    data = data.drop(drop_columns, axis=1)\n\n    return data, data.columns\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    return X_train.columns\n\ndef column_names(X_train):\n    column_names = X_train.columns\n\n    nice_labels = [\'County\', \'Asthma Rate\', \'PM10 - Pollutant\', \'PM2.5 Pollutant\', \'PM2.5 non FRM Pollutant\',\n       \'PM2.5 Spec - Pollutant\', \'CO - Pollutant\', \'SO2 - Pollutant\', \'NO2 - Pollutant\', \'Ozone - Pollutant\',\n       \'NONOxNOy - Pollutant\', \'Lead - Pollutant\', \'HAPS - Pollutant\', \'VOCS - Pollutant\', \'State\',\n       \'Smokers (Adult)\', \'Obesity (Adult)\', \'Uninsured Rate\', \'PCP\', \'High School Grads\',\n       \'Unemployment\', \'Income Inequality\', \'Particulate Air Pollution\']\n    label_dict = dict(zip(column_names, nice_labels))\n\n    return label_dict\n\ndef fill_nans(data):\n    data = data[data.asthma_rate != 0]\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(df):\n    """"""take data and returns X and y """"""\n    X = df.drop(\'asthma_rate\', axis=1)\n    y = df.asthma_rate\n    return X, y\n\ndef single_county_data(county):\n    """"""takes data and returns X and y for a single county """"""\n    data = get_data()\n    data, _ = feature_selection(data)\n    cln_data = fill_nans(data)\n    single_county = cln_data[cln_data[\'county\'] == county]\n    X, y = X_y(single_county)\n    X, y = remove_county_state(X, y)\n    return X, y\n\ndef get_state_data(state):\n    """"""takes data and returns X and y for a single state """"""\n    data = get_data()\n    data, _ = feature_selection(data)\n    cln_data = fill_nans(data)\n    single_state = cln_data[cln_data[\'state\'] == state]\n    X, y = X_y(single_state)\n    \n    return X, y\n\ndef split_data(data):\n    pass\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n\n    return X, y\n\ndef data_for_gridsearch():\n    data = get_data()\n    data, _ = feature_selection(data)\n    cln_data = fill_nans(data)\n    X, y = X_y(cln_data)\n    X, y = remove_county_state(X, y)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\n    return X_train, X_test, y_train, y_test\n\n\ndef data_for_predictions():\n    data = get_data()\n    data, _ = feature_selection(data)\n    cln_data = fill_nans(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\n    return X_train, X_test, y_train, y_test\n\nif __name__ == \'__main__\':\n    train_model()\n'"
src/en_model.py,3,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print(\'Model Performance Indicators\')\n    print(\'MAE: {:0.3f}\'.format(mae))\n    print(\'MAPE: {:0.3f}\'.format(mape))\n    print(\'Accuracy = {:0.3f}%\'.format(accuracy))\n\n    print(\'RMSE (test):\', rmse_test)\n    print(\'RMSE (train):\', rmse_train)\n    if rmse_test > rmse_train:\n        print(\'Overfit\')\n    else:\n        print(\'Underfit\')\n    return accuracy\n\n\ndef train_model():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    X_test, y_test = remove_county_state(X_test, y_test)\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             EN())\n\n    # set hyperparameters\n    hyperparameters = { \'elasticnet__alpha\': [1,0.1,0.01,0.001], # equivalent to lambda; alpha=0 means no regularization, ie linear regression\n                        \'elasticnet__l1_ratio\': [0.5, 0.7, 0.8, 0.9, 1], # l1=1 means L1 penalty, ie Lasso (not L2/Ridge)\n                        \'elasticnet__max_iter\': [10000],\n                        }\n\n    # tune model via pipeline\n    clf = GridSearchCV(pipeline, hyperparameters, cv=3)\n\n    clf.fit(X_train, y_train)\n\n    pred = clf.predict(X_test)\n    # print(\'feature importances:\', clf.feature_importances_)\n    print (\'r2 score:\',r2_score(y_test, pred))\n    print (\'mse:\',mean_squared_error(y_test, pred))\n    print(\'*\'*20)\n    print(\'best params:\',clf.best_params_)\n    print(\'best grid:\', clf.best_estimator_)\n    print(\'^\'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print(\'#\'*20)\n    print(\'score\', clf.score)\n    return clf\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    return X_train.columns\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(data_nas):\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n    data_nas = clean_data(data)\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\nif __name__ == \'__main__\':\n    train_model()\n'"
src/en_model_hot_one.py,3,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print(\'Model Performance Indicators\')\n    print(\'MAE: {:0.3f}\'.format(mae))\n    print(\'MAPE: {:0.3f}\'.format(mape))\n    print(\'Accuracy = {:0.3f}%\'.format(accuracy))\n\n    print(\'RMSE (test):\', rmse_test)\n    print(\'RMSE (train):\', rmse_train)\n    if rmse_test > rmse_train:\n        print(\'Overfit\')\n    else:\n        print(\'Underfit\')\n    return accuracy\n\n\ndef train_model():\n    data = get_data()\n    data[\'co\'] = data.state == \'colorado\'\n    data[\'fl\'] = data.state == \'florida\'\n    data[\'nj\'] = data.state == \'new jersey\'\n    data[\'ca\'] = data.state == \'california\'\n    print(data.shape)\n\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    X_test, y_test = remove_county_state(X_test, y_test)\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             EN())\n\n    # set hyperparameters\n    hyperparameters = { \'elasticnet__alpha\': [1,0.1,0.01,0.001], # equivalent to lambda; alpha=0 means no regularization, ie linear regression\n                        \'elasticnet__l1_ratio\': [0.5, 0.7, 0.8, 0.9, 1], # l1=1 means L1 penalty, ie Lasso (not L2/Ridge)\n                        \'elasticnet__max_iter\': [10000],\n                        }\n\n    # tune model via pipeline\n    clf = GridSearchCV(pipeline, hyperparameters, cv=3)\n\n    clf.fit(X_train, y_train)\n\n    pred = clf.predict(X_test)\n    # print(\'feature importances:\', clf.feature_importances_)\n    print (\'r2 score:\',r2_score(y_test, pred))\n    print (\'mse:\',mean_squared_error(y_test, pred))\n    print(\'*\'*20)\n    print(\'best params:\',clf.best_params_)\n    print(\'best grid:\', clf.best_estimator_)\n    print(\'^\'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print(\'#\'*20)\n    print(\'score\', clf.score)\n    return clf\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    return X_train.columns\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(data_nas):\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n    data_nas = clean_data(data)\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\nif __name__ == \'__main__\':\n    train_model()\n'"
src/fl_data.py,0,"b'import numpy as np\nimport pandas as pd\n# source data: http://www.flhealthcharts.com/charts/OtherIndicators/NonVitalIndDataViewer.aspx?cid=0341\n\ndef get_data():\n    # def pm2_5spec_data():\n    pm2_5spec = pd.read_csv(\'../data/daily_88502_2017.csv\')\n    pm2_5spec.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    pm2_5specfl = pm2_5spec[pm2_5spec[\'state\'] == \'Florida\']\n    pm2_5specflslim = pm2_5specfl.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    pm2_5specflslim = pm2_5specflslim.reset_index()\n    pm2_5specflslim = pm2_5specflslim.drop([\'index\'], axis=1)\n    pm2_5specflslim[\'obs_x_mean\'] = pm2_5specflslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    pm2_5specflslim.county = pm2_5specflslim.county.str.lower()\n    pm2_5specflgroupby = pm2_5specflslim.groupby(\'county\').sum()\n    pm2_5specflgroupby = pm2_5specflgroupby.reset_index()\n    pm2_5specflgroupby[\'new_mean\'] = pm2_5specflgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    pm2_5specflfinal = pm2_5specflgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    pm2_5specflfinal.columns = [\'county\', \'pm2_5spec_mean\']\n\n    # def pm2_5non_data():\n    pm2_5non = pd.read_csv(\'../data/daily_88502_2017.csv\')\n    pm2_5non.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    pm2_5nonfl = pm2_5non[pm2_5non[\'state\'] == \'Florida\']\n    pm2_5nonflslim = pm2_5nonfl.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    pm2_5nonflslim = pm2_5nonflslim.reset_index()\n    pm2_5nonflslim = pm2_5nonflslim.drop([\'index\'], axis=1)\n    pm2_5nonflslim[\'obs_x_mean\'] = pm2_5nonflslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    pm2_5nonflslim.county = pm2_5nonflslim.county.str.lower()\n    pm2_5nonflgroupby = pm2_5nonflslim.groupby(\'county\').sum()\n    pm2_5nonflgroupby = pm2_5nonflgroupby.reset_index()\n    pm2_5nonflgroupby[\'new_mean\'] = pm2_5nonflgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    pm2_5nonflfinal = pm2_5nonflgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    pm2_5nonflfinal.columns = [\'county\', \'pm2_5non_mean\']\n\n    # def pm2_5_data():\n    pm2_5 = pd.read_csv(\'../data/daily_88101_2017.csv\')\n    pm2_5.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    pm2_5fl = pm2_5[pm2_5[\'state\'] == \'Florida\']\n    pm2_5flslim = pm2_5fl.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    pm2_5flslim = pm2_5flslim.reset_index()\n    pm2_5flslim = pm2_5flslim.drop([\'index\'], axis=1)\n    pm2_5flslim[\'obs_x_mean\'] = pm2_5flslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    pm2_5flslim.county = pm2_5flslim.county.str.lower()\n    pm2_5flgroupby = pm2_5flslim.groupby(\'county\').sum()\n    pm2_5flgroupby = pm2_5flgroupby.reset_index()\n    pm2_5flgroupby[\'new_mean\'] = pm2_5flgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    pm2_5flfinal = pm2_5flgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    pm2_5flfinal.columns = [\'county\', \'pm2_5_mean\']\n\n    # def pm10_data():\n    pm10 = pd.read_csv(\'../data/daily_81102_2017.csv\')\n    pm10.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    pm10fl = pm10[pm10[\'state\'] == \'Florida\']\n    pm10flslim = pm10fl.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    pm10flslim = pm10flslim.reset_index()\n    pm10flslim = pm10flslim.drop([\'index\'], axis=1)\n    pm10flslim[\'obs_x_mean\'] = pm10flslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    pm10flslim.county = pm10flslim.county.str.lower()\n    pm10flgroupby = pm10flslim.groupby(\'county\').sum()\n    pm10flgroupby = pm10flgroupby.reset_index()\n    pm10flgroupby[\'new_mean\'] = pm10flgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    pm10flfinal = pm10flgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    pm10flfinal.columns = [\'county\', \'pm10_mean\']\n\n    # def lead_data():\n    lead = pd.read_csv(\'../data/daily_LEAD_2014.csv\')\n    lead.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    leadfl = lead[lead[\'state\'] == \'Florida\']\n    leadflslim = leadfl.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    leadflslim = leadflslim.reset_index()\n    leadflslim = leadflslim.drop([\'index\'], axis=1)\n    leadflslim[\'obs_x_mean\'] = leadflslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    leadflslim.county = leadflslim.county.str.lower()\n    leadflgroupby = leadflslim.groupby(\'county\').sum()\n    leadflgroupby = leadflgroupby.reset_index()\n    leadflgroupby[\'new_mean\'] = leadflgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    leadflfinal = leadflgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    leadflfinal.columns = [\'county\', \'lead_mean\']\n\n    # def nonox_data():\n    nonox = pd.read_csv(\'../data/daily_NONOxNOy_2017.csv\')\n    nonox.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    nonoxfl = nonox[nonox[\'state\'] == \'Florida\']\n    nonoxflcoslim = nonoxfl.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    nonoxflcoslim = nonoxflcoslim.reset_index()\n    nonoxflcoslim = nonoxflcoslim.drop([\'index\'], axis=1)\n    nonoxflcoslim[\'obs_x_mean\'] = nonoxflcoslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    nonoxflcoslim.county = nonoxflcoslim.county.str.lower()\n    nonoxflgroupby = nonoxflcoslim.groupby(\'county\').sum()\n    nonoxflgroupby = nonoxflgroupby.reset_index()\n    nonoxflgroupby[\'new_mean\'] = nonoxflgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    nonoxflfinal = nonoxflgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    nonoxflfinal.columns = [\'county\', \'nonox_mean\']\n\n    # def haps_data():\n    haps = pd.read_csv(\'../data/daily_HAPS_2017.csv\')\n    haps.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    hapsfl = haps[haps[\'state\'] == \'Florida\']\n    hapsflslim = hapsfl.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    hapsflslim = hapsflslim.reset_index()\n    hapsflslim = hapsflslim.drop([\'index\'], axis=1)\n    hapsflslim[\'obs_x_mean\'] = hapsflslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    hapsflslim.county = hapsflslim.county.str.lower()\n    hapsflgroupby = hapsflslim.groupby(\'county\').sum()\n    hapsflgroupby = hapsflgroupby.reset_index()\n    hapsflgroupby[\'new_mean\'] = hapsflgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    hapsflfinal = hapsflgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    hapsflfinal.columns = [\'county\', \'haps_mean\']\n\n    # def vocs_data():\n    # vocs16 = pd.read_csv(\'daily_VOCS_2016.csv\')\n    vocs = pd.read_csv(\'../data/daily_VOCS_2017.csv\')\n    vocs.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    vocsfl = vocs[vocs[\'state\'] == \'Florida\']\n    vocsflslim = vocsfl.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    vocsflslim = vocsflslim.reset_index()\n    vocsflslim = vocsflslim.drop([\'index\'], axis=1)\n    vocsflslim[\'obs_x_mean\'] = vocsflslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    vocsflslim.county = vocsflslim.county.str.lower()\n    vocsflgroupby = vocsflslim.groupby(\'county\').sum()\n    vocsflgroupby = vocsflgroupby.reset_index()\n    vocsflgroupby[\'new_mean\'] = vocsflgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    vocsflfinal = vocsflgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    vocsflfinal.columns = [\'county\', \'vocs_mean\']\n\n    # def co_data():\n    co = pd.read_csv(\'../data/daily_42101_2017.csv\')\n    co.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    cofl = co[co[\'state\'] == \'Florida\']\n    coflslim = cofl.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    coflslim = coflslim.reset_index()\n    coflslim = coflslim.drop([\'index\'], axis=1)\n    coflslim[\'obs_x_mean\'] = coflslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    coflslim.county = coflslim.county.str.lower()\n    coflgroupby = coflslim.groupby(\'county\').sum()\n    coflgroupby = coflgroupby.reset_index()\n    coflgroupby[\'new_mean\'] = coflgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    coflfinal = coflgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    coflfinal.columns = [\'county\', \'co_mean\']\n\n    # def so2_data():\n    so2 = pd.read_csv(\'../data/daily_42401_2017.csv\')\n    so2.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    sofl = so2[so2[\'state\'] == \'Florida\']\n    soflslim = sofl.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    soflslim = soflslim.reset_index()\n    soflslim = soflslim.drop([\'index\'], axis=1)\n    soflslim[\'obs_x_mean\'] = soflslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    soflslim.county = soflslim.county.str.lower()\n    soflgroupby = soflslim.groupby(\'county\').sum()\n    soflgroupby = soflgroupby.reset_index()\n    soflgroupby[\'new_mean\'] = soflgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    soflfinal = soflgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    soflfinal.columns = [\'county\', \'so_mean\']\n\n    # def no2_data():\n    no2 = pd.read_csv(\'../data/daily_42602_2017.csv\')\n    no2.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    nofl = no2[no2[\'state\'] == \'Florida\']\n    noflslim = nofl.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    noflslim.county = noflslim.county.str.lower()\n    noflslim = noflslim.reset_index()\n    noflslim = noflslim.drop([\'index\'], axis=1)\n    noflslim[\'obs_x_mean\'] = noflslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    noflgroupby = noflslim.groupby(\'county\').sum()\n    noflgroupby = noflgroupby.reset_index()\n    noflgroupby[\'new_mean\'] = noflgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    noflfinal = noflgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    noflfinal.columns = [\'county\', \'no_mean\']\n\n    # def ozone_data():\n    ozo = pd.read_csv(\'../data/daily_44201_2017.csv\')\n    ozoslim = ozo.drop([\'State Code\', \'County Code\', \'Site Num\', \'POC\',\n           \'Latitude\', \'Longitude\', \'Datum\', \'Sample Duration\',\n           \'Pollutant Standard\', \'Date Local\', \'Units of Measure\', \'Event Type\',\n            \'Observation Percent\',\n           \'1st Max Value\', \'1st Max Hour\', \'AQI\', \'Method Code\', \'Method Name\',\n           \'Local Site Name\', \'Address\', \'City Name\',\n           \'CBSA Name\', \'Date of Last Change\'], axis=1)\n    ozoslim.columns = [\'param_cd\', \'param\', \'obs_count\', \'mean_avg\', \'state\', \'county\']\n    ozofl = ozoslim[ozoslim[\'state\'] == \'Florida\']\n    ozofl.county = ozofl.county.str.lower()\n    ozofl = ozofl.reset_index()\n    ozofl = ozofl.drop([\'index\', \'param\', \'state\', \'param_cd\'], axis= 1)\n    ozofl[\'obs_x_mean\']  = ozofl.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    ozoflgroupby = ozofl.groupby(\'county\').sum()\n    ozoflgroupby = ozoflgroupby.reset_index()\n    ozoflgroupby[\'new_mean\'] = ozoflgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    ozoflfinal = ozoflgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\'], axis=1)\n    ozoflfinal.columns = [\'county\', \'ozo_mean\']\n\n    # def asthma_fl():\n    # http://www.flhealthcharts.com/charts/OtherIndicators/NonVitalIndDataViewer.aspx?cid=9755\n    asthmahospfl = pd.read_csv(\'../data/asthma_hospitization_fl.csv\')\n    # asthmahospfl.columns = asthmahospfl.columns.str.lower()\n    asthmahospfl.county = asthmahospfl.county.str.lower()\n    # asthmahospfl.columns = [\'county\', \'asthma_rate\']\n    asthmahospfl[\'asthma_rate\']= asthmahospfl.apply(lambda row: row.asthma_rate / 10, axis=1)\n\n    asthmafl = asthmahospfl.drop(\'Unnamed: 0\', axis=1)\n\n    # def merge_all():\n    asozo_fl                       = asthmafl.merge(ozoflfinal, how=""left"", on=""county"")\n    asozono_fl                     = asozo_fl.merge(noflfinal, how=""left"", on=""county"")\n    asozonofl_fl                   = asozono_fl.merge(coflfinal, how=""left"", on=""county"")\n    asozonoflso_fl                 = asozonofl_fl.merge(soflfinal, how=""left"", on=""county"")\n    asozonoflsovocs_fl             = asozonoflso_fl.merge(vocsflfinal, how=""left"", on=""county"")\n    asozonoflsovocshaps_fl         = asozonoflsovocs_fl.merge(hapsflfinal, how=""left"", on=""county"")\n    asozonoflsovocshapsnonox_fl    = asozonoflsovocshaps_fl.merge(nonoxflfinal, how=""left"", on=""county"")\n\n    asozonoflsovocshapsnonoxlead_fl                         = asozonoflsovocshaps_fl.merge(leadflfinal, how=""left"", on=""county"")\n    asozonoflsovocshapsnonoxpm10_fl                         = asozonoflsovocshapsnonoxlead_fl.merge(pm10flfinal, how=""left"", on=""county"")\n    asozonoflsovocshapsnonoxpm10pm2_5_fl                    = asozonoflsovocshapsnonoxpm10_fl.merge(pm2_5flfinal, how=""left"", on=""county"")\n    asozonoflsovocshapsnonoxpm10pm2_5pm2_5non_fl            = asozonoflsovocshapsnonoxpm10pm2_5_fl.merge(pm2_5nonflfinal, how=""left"", on=""county"")\n    asozonoflsovocshapsnonoxpm10pm2_5pm2_5nonpm2_5spec_fl   = asozonoflsovocshapsnonoxpm10pm2_5pm2_5non_fl.merge(pm2_5specflfinal, how=""left"", on=""county"")\n\n    return asozonoflsovocshapsnonoxpm10pm2_5pm2_5nonpm2_5spec_fl\n'"
src/gbr_model.py,3,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print(\'Model Performance Indicators\')\n    print(\'MAE: {:0.3f}\'.format(mae))\n    print(\'MAPE: {:0.3f}\'.format(mape))\n    print(\'Accuracy = {:0.3f}%\'.format(accuracy))\n\n    print(\'RMSE (test):\', rmse_test)\n    print(\'RMSE (train):\', rmse_train)\n    if rmse_test > rmse_train:\n        print(\'Overfit\')\n    else:\n        print(\'Underfit\')\n    return accuracy\n\n\ndef train_model():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    X_test, y_test = remove_county_state(X_test, y_test)\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             GBR())\n\n    # set hyperparameters\n    hyperparameters = { \'gradientboostingregressor__n_estimators\' :     [100, 600, 700, 800],\n                        \'gradientboostingregressor__max_depth\':         [3, 4, 5, 10, 20],\n                        \'gradientboostingregressor__min_samples_split\': [3, 4, 5, 10, 20],\n                        \'gradientboostingregressor__learning_rate\':     [0.01, 0.05, 0.1],\n                        \'gradientboostingregressor__loss\':              [\'ls\'],\n                        }\n\n\n    # tune model via pipeline\n    clf = GridSearchCV(pipeline, hyperparameters, cv=3)\n\n    clf.fit(X_train, y_train)\n\n    pred = clf.predict(X_test)\n    # print(\'feature importances:\', clf.feature_importances_)\n    print (\'r2 score:\',r2_score(y_test, pred))\n    print (\'mse:\',mean_squared_error(y_test, pred))\n    print(\'*\'*20)\n    print(\'best params:\',clf.best_params_)\n    print(\'best grid:\', clf.best_estimator_)\n    print(\'^\'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print(\'#\'*20)\n    print(\'score\', clf.score)\n    return clf\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    return X_train.columns\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(data_nas):\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n    data_nas = clean_data(data)\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\nif __name__ == \'__main__\':\n    train_model()\n'"
src/gbr_model_hot_one.py,3,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print(\'Model Performance Indicators\')\n    print(\'MAE: {:0.3f}\'.format(mae))\n    print(\'MAPE: {:0.3f}\'.format(mape))\n    print(\'Accuracy = {:0.3f}%\'.format(accuracy))\n\n    print(\'RMSE (test):\', rmse_test)\n    print(\'RMSE (train):\', rmse_train)\n    if rmse_test > rmse_train:\n        print(\'Overfit\')\n    else:\n        print(\'Underfit\')\n    return accuracy\n\n\ndef train_model():\n    data = get_data()\n    data[\'co\'] = data.state == \'colorado\'\n    data[\'fl\'] = data.state == \'florida\'\n    data[\'nj\'] = data.state == \'new jersey\'\n    data[\'ca\'] = data.state == \'california\'\n    print(data.shape)\n\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    X_test, y_test = remove_county_state(X_test, y_test)\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             GBR())\n\n    # set hyperparameters\n    hyperparameters = { \'gradientboostingregressor__n_estimators\' :     [100, 600, 700, 800],\n                        \'gradientboostingregressor__max_depth\':         [3, 4, 5, 10, 20],\n                        \'gradientboostingregressor__min_samples_split\': [3, 4, 5, 10, 20],\n                        \'gradientboostingregressor__learning_rate\':     [0.01, 0.05, 0.1],\n                        \'gradientboostingregressor__loss\':              [\'ls\'],\n                        }\n\n\n    # tune model via pipeline\n    clf = GridSearchCV(pipeline, hyperparameters, cv=3)\n\n    clf.fit(X_train, y_train)\n\n    pred = clf.predict(X_test)\n    # print(\'feature importances:\', clf.feature_importances_)\n    print (\'r2 score:\',r2_score(y_test, pred))\n    print (\'mse:\',mean_squared_error(y_test, pred))\n    print(\'*\'*20)\n    print(\'best params:\',clf.best_params_)\n    print(\'best grid:\', clf.best_estimator_)\n    print(\'^\'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print(\'#\'*20)\n    print(\'score\', clf.score)\n    return clf\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    return X_train.columns\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(data_nas):\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n    data_nas = clean_data(data)\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\nif __name__ == \'__main__\':\n    train_model()\n'"
src/get_feat_imps.py,3,"b""import numpy as np\nimport pandas as pd\n\nfrom sklearn.ensemble import RandomForestRegressor as RFR\n\nfrom data_processing import get_data\nfrom data_processing import split_data\nfrom data_processing import remove_county_state\nfrom data_processing import feature_selection\nfrom data_processing import data_for_gridsearch\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    errors_for_plot = ypred - y_test\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n    rmse_test   = calc_rmse(ypred, y_test)\n\n    print('MAE: {:0.3f}'.format(mae))\n    print('MAPE: {:0.3f}'.format(mape))\n\n    print('RMSE (train):', rmse_train.round(2))\n    print('RMSE (test):', rmse_test.round(2))\n    if rmse_test > rmse_train:\n        print('Overfit')\n    else:\n        print('Underfit')\n\n    return rmse_train, rmse_test, errors_for_plot\n\ndef get_feat_imps():\n\n    X_train, X_test, y_train, y_test = data_for_gridsearch()\n    column_names = X_train.columns\n\n    model = RFR(max_features        = 'auto',\n                max_depth           = None,\n                bootstrap           = True,\n                min_samples_leaf    = 5,\n                min_samples_split   = 10,\n                n_estimators        = 100\n                )\n\n    model = model.fit(X_train, y_train)\n\n    model_params    = model.get_params()\n    feat_imps       = model.feature_importances_\n\n    print('model_params', model_params)\n    print('feat_imps', feat_imps)\n\n    rmse_train, rmse_test, errors_for_plot = eval_model(model, X_train, y_train, X_test, y_test)\n    print('RMSE train/test: ', rmse_train, rmse_test)\n\n    return model_params, feat_imps, column_names\n\n\nif __name__ == '__main__':\n    get_feat_imps()\n"""
src/get_results.py,1,"b'import pandas as pd\nimport numpy as np\nfrom flask import Markup\n\nfrom modclass import remove_county_state as rcs\n\nfrom modclass import county_data as cd\nfrom modclass import get_data as gd\nfrom modclass import split_data as sd\nfrom modclass import X_y as xy\nfrom data_processing import remove_county_state\nfrom state_color_map import create_map as cm\nfrom charts import create_feat_imp_chart\nfrom data import make_fips_df as mf\nfrom ols_model_hot_one import train_model as ols_tm\nfrom rfr_model_hot_one import train_model as rfr_tm\nfrom rfr_model_hot_one import predict\nfrom svr_model_hot_one import train_model as svr_tm\nfrom lr_model_hot_one import train_model as lr_tm\nfrom knn_model_hot_one import train_model as knn_tm\nfrom comparison import compare_models\nfrom data_processing import single_county_data\nfrom data_processing import get_state_data\nfrom data_processing import data_for_predictions\n\n\ndef train_the_model():\n    model = rfr_tm()\n\n    return model\n\ndef predict(X_test):\n    model = train_the_model()\n    prediction = model.predict(X_test)\n\n    return prediction\n\n\ndef test_predict():\n    X_train, X_test, y_train, y_test = data_for_predictions()\n    counties_train = X_train[\'county\']\n    counties_test = X_test[\'county\']\n    X_test, y_test = remove_county_state(X_test, y_test)\n    ypred = predict(X_test)\n\n    X = X_test.round(2)\n    y = y_test.round(2)\n    uninsured = X[\'uninsured\'].values\n    unemployment = X[\'unemployment\'].values\n    obesity = X[\'obese_adult\'].values\n    smokers = X[\'smoke_adult\'].values\n    particulates = X[\'air_poll_partic\'].values\n    y = y.values\n\n    return counties_test, uninsured, unemployment, obesity, smokers, particulates, y, ypred\n\ndef test_data_preds_table():\n    """"""returns dictionary to populate table on predictions page """"""\n    county_tst, uninsured_tst, unemployment_tst, obesity_tst, smokers_tst, particulates_tst, y_tst, ypred = test_predict()\n\n    # these are for the \'Actual and Predicted Asthma Rates\' chart...\n    county_tst = list(county_tst.str.title()),\n    uninsured_tst = uninsured_tst,\n    unemployment_tst = unemployment_tst,\n    obesity_tst = obesity_tst,\n    smokers_tst = smokers_tst,\n    particulates_tst = particulates_tst,\n    y_tst = y_tst,\n    ypred = np.round(ypred, 2)\n    num_results = len(ypred)\n    v_list = []\n    for i in range(num_results):\n        temp_list = [uninsured_tst[0][i], unemployment_tst[0][i], obesity_tst[0][i],\n        smokers_tst[0][i], particulates_tst[0][i], y_tst[0][i], ypred[i]]\n        v_list.append(temp_list)\n    table_dict = dict(zip(county_tst[0], v_list))\n\n    return table_dict\n\ndef update_county_policy(row, results):\n    print(\'row\', row)\n    X = row\n    print(\'X\', X)\n    print(\'type(X)\', type(X))\n    print(\'results\', results)\n    results_nums = []\n    for result in results:\n        if result == \'plus10\':\n            result = 1.2\n        elif result == \'minus10\':\n            result = 0.8\n        else:\n            result = 1\n        results_nums.append(result)\n    print(\'results_nums\', results_nums)\n    X[\'uninsured\'] *= results_nums[0]\n    X[\'unemployment\'] *= results_nums[1]\n    X[\'obese_adult\'] *= results_nums[2]\n    X[\'smoke_adult\'] *= results_nums[3]\n    X[\'air_poll_partic\'] *= results_nums[4]\n    print(\'updated X\', X)\n    return X\n\n\ndef get_county_pred(county, form_results):\n    X, y = single_county_data(county.lower())\n    row = update_county_policy(X, form_results)\n    # row = row.drop([\'county\', \'state\'], axis=1)\n    pred = predict(row)\n    pred = pred[0].round(2)\n    y = y.values[0].round(2)\n\n    return pred, y\n\n\ndef one_county(input_county):\n    county = input_county.lower()\n    X, y = single_county_data(county)\n    X = X.round(2)\n    y = y.round(2)\n    uninsured = X[\'uninsured\'].values[0]\n    unemployment = X[\'unemployment\'].values[0]\n    obesity = X[\'obese_adult\'].values[0]\n    smokers = X[\'smoke_adult\'].values[0]\n    particulates = X[\'air_poll_partic\'].values[0]\n    y = y.values[0]\n\n    return county, uninsured, unemployment, obesity, smokers, particulates, y, X\n\ndef get_state_pred_map(state, state_form_results):\n    print(\'state\'*20)\n    print(state)\n    state_data = get_state_data(state.lower())\n\n    X_state = update_state_policy(state_data, state_form_results)\n\n    X_state_counties = X_state.county\n    X_state = X_state.drop([\'county\', \'state\'], axis=1)\n    state_pred_arr = predict(X_state)\n    state_pred_arr = state_pred_arr.round(2)\n\n    df_state_pred = pd.DataFrame(state_pred_arr)\n    df_state_pred.columns = [\'pred\']\n    df_X_state_counties = pd.DataFrame(X_state_counties)\n    df_X_state_counties = df_X_state_counties.reset_index(drop=True)\n    county_state_pred = df_state_pred.join(df_X_state_counties)\n\n    fips = mf()\n    fips_state = fips[fips[\'state\']==state.lower()]\n\n    pred_fips = county_state_pred.merge(fips_state, how=""left"", on=""county"")\n    pred_fips = pred_fips.drop([\'county\', \'state\'], axis=1)\n\n    # create map with predicted values\n    state_pred_map_svg = cm(pred_fips)\n\n    # prepare embed for html\n    dub = \'""\'\n    state_pred_map = \'<embed class=""d-block w-100"" src={}{}{} alt=""Predicted Asthma Map"">\'.format(dub, state_pred_map_svg, dub)\n    div_and_map = Markup(\n                    \'<div id=""mapPrediction"" class=""card"" style=""width: 30rem;"">\'+\n                    state_pred_map+\n                    \'<div class=""card-body""><h2 class=""card-text"">Predicted</h2><p class=""card-text"">Map showing predicted asthma hospitalization rates by county in Colorado.  Darker colors represent higher rates.</p></div></div>\'\n                    )\n    # print(div_and_map)\n    return div_and_map\n\ndef update_state_policy(data_tuple, results):\n    X = data_tuple[0]\n    # X = data_tuple\n    results_nums = []\n    for result in results:\n        if result == \'plus10\':\n            result = 1.2\n        elif result == \'minus10\':\n            result = 0.8\n        else:\n            result = 1\n        results_nums.append(result)\n    X[\'uninsured\'] *= results_nums[0]\n    X[\'unemployment\'] *= results_nums[1]\n    X[\'obese_adult\'] *= results_nums[2]\n    X[\'smoke_adult\'] *= results_nums[3]\n    X[\'air_poll_partic\'] *= results_nums[4]\n\n    return X\n\n\nif __name__ == \'__main__\':\n\n    csv_file_path = \'../data/the_data_file.csv\'\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n    else:\n        print(""Data file not found, assembling dataset..."")\n        # from combine_data import join_data as data\n        from data import join_data as data\n        # data, labels = data()\n        data = data()\n        data.to_csv(csv_file_path, index=False)\n'"
src/k_nearest_neighbors_regressor.py,3,"b""import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils import shuffle\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print('Model Performance Indicators')\n    print('MAE: {:0.3f}'.format(mae))\n    print('MAPE: {:0.3f}'.format(mape))\n    print('Accuracy = {:0.3f}%'.format(accuracy))\n\n    print('RMSE (test):', rmse_test)\n    print('RMSE (train):', rmse_train)\n    if rmse_test > rmse_train:\n        print('Overfit')\n    else:\n        print('Underfit')\n    return accuracy\n\n\ndef knn_regress(data):\n    # drop problematic row with zero for asthma_rate\n    # data = data.drop([171])\n    data_nas = data.fillna(0)\n    no_counties = data_nas.drop(['county', 'state'], axis=1)\n\n    # X = no_counties.drop('asthma_rate', axis=1)\n    # X = no_counties.drop(['asthma_rate', 'ozo_mean','unemployment','uninsured','pcp','pm2_5_mean','pm2_5non_mean','pm2_5spec_mean','air_poll_partic','income_ineq', 'high_sch_grad', 'obese_adult'], axis=1)\n    X = no_counties.drop(['asthma_rate', 'pm2_5_mean','pm10_mean','haps_mean','no_mean','vocs_mean','pm2_5non_mean','co_mean','pm2_5spec_mean','so_mean', 'lead_mean','income_ineq', 'high_sch_grad', 'obese_adult'], axis=1)\n    y = no_counties.asthma_rate\n\n    # train/test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                        test_size=0.2,\n                                                        random_state=123)\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             KNeighborsRegressor())\n\n    # set hyperparameters\n    hyperparameters = { 'kneighborsregressor__n_neighbors' : [20, 10, 5, 3, 2],\n                        'kneighborsregressor__weights': ['uniform', 'distance'],\n                       }\n\n    # tune model via pipeline\n    clf = GridSearchCV(pipeline, hyperparameters, cv=3)\n\n    clf.fit(X_train, y_train)\n\n    # evaluate models with test data\n    pred = clf.predict(X_test)\n    # print('feature importances:', clf.feature_importances_)\n    print ('r2 score:',r2_score(y_test, pred))\n    print ('mse:',mean_squared_error(y_test, pred))\n    print('*'*20)\n    print('best params:',clf.best_params_)\n    print('best grid:', clf.best_estimator_)\n    print('^'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print('#'*20)\n\nif __name__ == '__main__':\n    # get data\n    from combine_data import join_data as data\n    data = data()\n    # run k nearest neighbors model\n    knn_regress(data)\n"""
src/knn_model.py,3,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print(\'Model Performance Indicators\')\n    print(\'MAE: {:0.3f}\'.format(mae))\n    print(\'MAPE: {:0.3f}\'.format(mape))\n    print(\'Accuracy = {:0.3f}%\'.format(accuracy))\n\n    print(\'RMSE (test):\', rmse_test)\n    print(\'RMSE (train):\', rmse_train)\n    if rmse_test > rmse_train:\n        print(\'Overfit\')\n    else:\n        print(\'Underfit\')\n    return accuracy\n\n\ndef train_model():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    X_test, y_test = remove_county_state(X_test, y_test)\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             KNR())\n\n    # set hyperparameters\n    hyperparameters = { \'kneighborsregressor__n_neighbors\' : [100, 50, 20, 15, 10, 5, 3, 2],\n                        \'kneighborsregressor__weights\': [\'uniform\', \'distance\'],\n                       }\n\n    # tune model via pipeline\n    clf = GridSearchCV(pipeline, hyperparameters, cv=3)\n\n    clf.fit(X_train, y_train)\n\n    pred = clf.predict(X_test)\n    # print(\'feature importances:\', clf.feature_importances_)\n    print (\'r2 score:\',r2_score(y_test, pred))\n    print (\'mse:\',mean_squared_error(y_test, pred))\n    print(\'*\'*20)\n    print(\'best params:\',clf.best_params_)\n    print(\'best grid:\', clf.best_estimator_)\n    print(\'^\'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print(\'#\'*20)\n    print(\'score\', clf.score)\n    return clf\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    return X_train.columns\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(data_nas):\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n    data_nas = clean_data(data)\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\nif __name__ == \'__main__\':\n    train_model()\n'"
src/knn_model_hot_one.py,3,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print(\'Model Performance Indicators\')\n    print(\'MAE: {:0.3f}\'.format(mae))\n    print(\'MAPE: {:0.3f}\'.format(mape))\n    print(\'Accuracy = {:0.3f}%\'.format(accuracy))\n\n    print(\'RMSE (test):\', rmse_test)\n    print(\'RMSE (train):\', rmse_train)\n    if rmse_test > rmse_train:\n        print(\'Overfit\')\n    else:\n        print(\'Underfit\')\n    return accuracy\n\n\ndef train_model():\n    data = get_data()\n    data[\'co\'] = data.state == \'colorado\'\n    data[\'fl\'] = data.state == \'florida\'\n    data[\'nj\'] = data.state == \'new jersey\'\n    data[\'ca\'] = data.state == \'california\'\n    print(data.shape)\n\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    X_test, y_test = remove_county_state(X_test, y_test)\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             KNR())\n\n    # set hyperparameters\n    hyperparameters = { \'kneighborsregressor__n_neighbors\' : [100, 50, 20, 15, 10, 5, 3, 2],\n                        \'kneighborsregressor__weights\': [\'uniform\', \'distance\'],\n                       }\n\n    # tune model via pipeline\n    clf = GridSearchCV(pipeline, hyperparameters, cv=3)\n\n    clf.fit(X_train, y_train)\n\n    pred = clf.predict(X_test)\n    # print(\'feature importances:\', clf.feature_importances_)\n    print (\'r2 score:\',r2_score(y_test, pred))\n    print (\'mse:\',mean_squared_error(y_test, pred))\n    print(\'*\'*20)\n    print(\'best params:\',clf.best_params_)\n    print(\'best grid:\', clf.best_estimator_)\n    print(\'^\'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print(\'#\'*20)\n    print(\'score\', clf.score)\n    return clf\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    return X_train.columns\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(data_nas):\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n    data_nas = clean_data(data)\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\nif __name__ == \'__main__\':\n    train_model()\n'"
src/linear_regression.py,3,"b""import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression\n\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print('Model Performance Indicators')\n    print('MAE: {:0.3f}'.format(mae))\n    print('MAPE: {:0.3f}'.format(mape))\n    print('Accuracy = {:0.3f}%'.format(accuracy))\n\n    print('RMSE (test):', rmse_test)\n    print('RMSE (train):', rmse_train)\n    if rmse_test > rmse_train:\n        print('Overfit')\n    else:\n        print('Underfit')\n    return accuracy\n\n\n\ndef lin_regress(data):\n    # drop problematic row with zero for asthma_rate\n    # data = data.drop([171])\n    # replacing nans with zeros\n    data_nas = data.fillna(0)\n    no_counties = data_nas.drop(['county', 'state'], axis=1)\n    X = no_counties.drop('asthma_rate', axis=1)\n    y = no_counties.asthma_rate\n\n    # consider replacing nans with mean (previously tried)...\n    # train/test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                        test_size=0.2,\n                                                        random_state=123)\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             LinearRegression())\n\n    hyperparameters = { } # use defaults only\n\n    clf = GridSearchCV(pipeline, hyperparameters, cv=3) # cv=3 is same as cv=None (default)\n\n    clf.fit(X_train, y_train)\n\n\n\n    # evaluate models with test data\n    pred = clf.predict(X_test)\n    # print('feature importances:', clf.feature_importances_)\n    print ('r2 score:',r2_score(y_test, pred))\n    print ('mse:',mean_squared_error(y_test, pred))\n    print('*'*20)\n    print('best params:',clf.best_params_)\n    print('best grid:', clf.best_estimator_)\n    print('^'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print('#'*20)\n\n    X_train = sm.add_constant(X_train)\n    model = sm.OLS(y_train, X_train)\n    results = model.fit()\n    print(results.summary())\n\n\nif __name__ == '__main__':\n    from combine_data import join_data as data\n    data = data()\n    lin_regress(data)\n"""
src/logistic_regression.py,0,"b""import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cross_validation import train_test_split\n\n\ndef log_regress(data):\n    # need to create threshold for binary classification \n    data_nas = data.fillna(0)\n    no_counties = data_nas.drop(['county', 'state'], axis=1)\n    X = no_counties.drop('asthma_rate', axis=1)\n    y = no_counties.asthma_rate\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    model.predict(X_test)\n    score = model.score(X_test, y_test)\n    print(score)\n\nif __name__ == '__main__':\n    from combine_data import join_data as data\n    data = data()\n    log_regress(data)\n"""
src/lr_model.py,3,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print(\'Model Performance Indicators\')\n    print(\'MAE: {:0.3f}\'.format(mae))\n    print(\'MAPE: {:0.3f}\'.format(mape))\n    print(\'Accuracy = {:0.3f}%\'.format(accuracy))\n\n    print(\'RMSE (test):\', rmse_test)\n    print(\'RMSE (train):\', rmse_train)\n    if rmse_test > rmse_train:\n        print(\'Overfit\')\n    else:\n        print(\'Underfit\')\n    return accuracy\n\n\ndef train_model():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    X_test, y_test = remove_county_state(X_test, y_test)\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             LR())\n\n    # set hyperparameters\n    hyperparameters = {\n                       }\n\n\n    # tune model via pipeline\n    clf = GridSearchCV(pipeline, hyperparameters, cv=3)\n\n    clf.fit(X_train, y_train)\n\n    pred = clf.predict(X_test)\n    # print(\'feature importances:\', clf.feature_importances_)\n    print (\'r2 score:\',r2_score(y_test, pred))\n    print (\'mse:\',mean_squared_error(y_test, pred))\n    print(\'*\'*20)\n    print(\'best params:\',clf.best_params_)\n    print(\'best grid:\', clf.best_estimator_)\n    print(\'^\'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print(\'#\'*20)\n    print(\'score\', clf.score)\n    return clf\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    return X_train.columns\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(data_nas):\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n    data_nas = clean_data(data)\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\nif __name__ == \'__main__\':\n    train_model()\n'"
src/lr_model_hot_one.py,3,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print(\'Model Performance Indicators\')\n    print(\'MAE: {:0.3f}\'.format(mae))\n    print(\'MAPE: {:0.3f}\'.format(mape))\n    print(\'Accuracy = {:0.3f}%\'.format(accuracy))\n\n    print(\'RMSE (test):\', rmse_test)\n    print(\'RMSE (train):\', rmse_train)\n    if rmse_test > rmse_train:\n        print(\'Overfit\')\n    else:\n        print(\'Underfit\')\n    return accuracy\n\n\ndef train_model():\n    data = get_data()\n    data[\'co\'] = data.state == \'colorado\'\n    data[\'fl\'] = data.state == \'florida\'\n    data[\'nj\'] = data.state == \'new jersey\'\n    data[\'ca\'] = data.state == \'california\'\n    print(data.shape)\n\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    X_test, y_test = remove_county_state(X_test, y_test)\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             LR())\n\n    # set hyperparameters\n    hyperparameters = {\n                       }\n\n\n    # tune model via pipeline\n    clf = GridSearchCV(pipeline, hyperparameters, cv=3)\n\n    clf.fit(X_train, y_train)\n\n    pred = clf.predict(X_test)\n    # print(\'feature importances:\', clf.feature_importances_)\n    print (\'r2 score:\',r2_score(y_test, pred))\n    print (\'mse:\',mean_squared_error(y_test, pred))\n    print(\'*\'*20)\n    print(\'best params:\',clf.best_params_)\n    print(\'best grid:\', clf.best_estimator_)\n    print(\'^\'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print(\'#\'*20)\n    print(\'score\', clf.score)\n    return clf\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    return X_train.columns\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(data_nas):\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n    data_nas = clean_data(data)\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\nif __name__ == \'__main__\':\n    train_model()\n'"
src/modclass.py,0,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n \n\ndef train_model():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    model = RFR(    max_features        = \'auto\',\n                    max_depth           = None,\n                    bootstrap           = True,\n                    min_samples_leaf    = 5,\n                    min_samples_split   = 10,\n                    n_estimators        = 10\n                                )\n    trained_model = model.fit(X_train, y_train)\n    return trained_model\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    return X_train.columns\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(data_nas):\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n    data_nas = clean_data(data)\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\nif __name__ == \'__main__\':\n    pass\n'"
src/modclass2.py,0,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\ndef train_model():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    model = RFR(    max_features        = \'sqrt\',\n                    max_depth           = 100,\n                    bootstrap           = False,\n                    min_samples_leaf    = 1,\n                    min_samples_split   = 2,\n                    n_estimators        = 200\n                                )\n    trained_model = model.fit(X_train, y_train)\n\n    return trained_model\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n\n    return X_train.columns\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        # from combine_data import join_data as data\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n    # from data import join_data as data\n    # data = data()\n    # data.to_csv(csv_file_path, index=False)\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    # replacing nans with zeros\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(data_nas):\n    # counties_states = data_nas[[\'county\', \'state\']]\n    # no_counties_states = data_nas.drop([\'county\', \'state\'], axis=1)\n\n    # create X and y datasets\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n\n    data_nas = clean_data(data)\n\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n\n    # note that X now includes county and state\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\n    # note that X_train, X_test, y_train and y_test all now include county and state\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\nclass FinalModel(object):\n    def __init__(self):\n        # self._vectorizer = TfidfVectorizer(stop_words=\'english\')\n        self._regressor = RFR(  max_features        = \'sqrt\',\n                                max_depth           = 100,\n                                bootstrap           = False,\n                                min_samples_leaf    = 1,\n                                min_samples_split   = 2,\n                                n_estimators        = 200\n                                )\n\n    def fit(self, X, y):\n        X, y = remove_county_state(X, y)\n        # StandardScaler here\n        self._regressor.fit(X, y)\n\n        # self.X_vectors = self._vectorizer.fit_transform(X)\n        # self._classifier.fit(self.X_vectors, y)\n        return self\n\n    def predict(self, X):\n        return self._regressor.predict(X)\n\n    # def score(self, X, y):\n    #     X_vectors = self._vectorizer.transform(X)\n    #     return self._classifier.score(X_vectors, y)\n\nif __name__ == \'__main__\':\n    Boulder_X, Boulder_y[1] = county_data(data, \'boulder\')\n    print(Boulder_X.shape)\n    print(Boulder_y.shape)\n    print(type(Boulder_X))\n    print(type(Boulder_y))\n    train_model()\n\n    fm = FinalModel()\n    X_train, X_test, y_train, y_test = split_data(data)\n    fm.fit(X_train, y_train)\n    fm.predict(X_test)\n    fm.feat_imps = fm._regressor.feature_importances_\n    # pg.score(X_test, y_test)\n    print(\'Feature Importances: \', fm._regressor.feature_importances_)\n'"
src/modclass3.py,0,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\ndef train_model():\n    data = get_data()\n    # print(type(data))\n    X_train, X_test, y_train, y_test = split_data(data)\n    # print(type(X_train))\n    # print(X_train.shape)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    # print(X_train.shape)\n\n    hyperpara_dict = {  LR  : {}, # use defaults only\n                        RFR : { \'randomforestregressor__max_features\' : [\'auto\', \'sqrt\'],\n                                \'randomforestregressor__max_depth\': [10, 20, 30, 50, 80, 100, None],\n                                \'randomforestregressor__bootstrap\': [True, False],\n                                \'randomforestregressor__min_samples_leaf\': [1, 2, 4, 10],\n                                \'randomforestregressor__min_samples_split\': [2, 5, 10],\n                                \'randomforestregressor__n_estimators\': [100, 200, 500, 1000, 1500, 2000],\n                                },\n                        GBR : { \'gradientboostingregressor__n_estimators\' :     [100, 600, 700, 800],\n                                \'gradientboostingregressor__max_depth\':         [3, 4, 5, 10, 20],\n                                \'gradientboostingregressor__min_samples_split\': [3, 4, 5, 10, 20],\n                                \'gradientboostingregressor__learning_rate\':     [0.01, 0.05, 0.1],\n                                \'gradientboostingregressor__loss\':              [\'ls\'],\n                                },\n                        KNR : { \'kneighborsregressor__n_neighbors\' : [1, 2, 3, 4, 5, 6, 10],\n                                \'kneighborsregressor__weights\': [\'uniform\', \'distance\'],\n                                },\n                        SVR : { \'svr__kernel\': [\'rbf\'],\n                                \'svr__C\': [50],\n                                \'svr__epsilon\': [5],\n                                },\n                        EN : { \'elasticnet__alpha\': [1], # equivalent to lambda; alpha=0 means no regularization, ie linear regression\n                                \'elasticnet__l1_ratio\': [0.9], # l1=1 means L1 penalty, ie Lasso (not L2/Ridge)\n                                \'elasticnet__max_iter\': [10000],\n                                }\n                    }\n\n\n    models = [RFR]\n    # print(\'models for loop\')\n    for model in models:\n        # print(\'this is the model\', model)\n\n        # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n        pipeline = make_pipeline(   StandardScaler(),\n                                    model()\n                                 )\n\n        hyperparameters = hyperpara_dict[model]\n\n        clf = GridSearchCV(pipeline, hyperparameters, cv=3) # cv=3 is same as cv=None (default)\n        # print(\'past clf GridSearchCV\')\n        # print(X_train)\n        # print(y_train)\n        trained_model = clf.fit(X_train, y_train)\n        # print(\'trained model passed \')\n\n        # evaluate models with test data\n        # pred = clf.predict(X_test)\n\n\n    # model = RFR(    max_features        = \'sqrt\',\n    #                 max_depth           = 100,\n    #                 bootstrap           = False,\n    #                 min_samples_leaf    = 1,\n    #                 min_samples_split   = 2,\n    #                 n_estimators        = 200\n    #                             )\n    # print(\'just RFR\')\n    # # X_train, y_train = standard_scaler(X_train, y_train)\n    # trained_model = model.fit(X_train, y_train)\n\n    return trained_model\n\n# def standard_scaler(X, y):\n#     # standardize with StandardScaler\n#     # scaler_X = StandardScaler().fit(X.values)\n#     # scaler_y = StandardScaler().fit(y.values)\n#     # X_scaled = scaler_X.transform(X.values)\n#     # y_scaled = scaler_y.transform(y.values.reshape(1,-1))\n#\n#     X_scaled = X.sub(X.mean())/X.get_values().std()\n#     y_scaled = y.sub(y.mean())/y.get_values().std()\n#\n#     return X_scaled, y_scaled\n\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n\n    return X_train.columns\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        # from combine_data import join_data as data\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n    # from data import join_data as data\n    # data = data()\n    # data.to_csv(csv_file_path, index=False)\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    # replacing nans with zeros\n    data_nas = data.fillna(0)\n    # print(\'clean_data\')\n    # print(type(data_nas))\n    return data_nas\n\ndef X_y(data_nas):\n    # create X and y datasets\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    # print(\'X_y\')\n    # print(type(X))\n    # print(type(y))\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n    data_nas = clean_data(data)\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n\n    # note that X now includes county and state\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    # standard_scaler(X, y)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\n    # note that X_train, X_test, y_train and y_test all now include county and state\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\n# class FinalModel(object):\n#     def __init__(self):\n#         self._scaler = SS()\n#         self._regressor = RFR(  max_features        = \'sqrt\',\n#                                 max_depth           = 100,\n#                                 bootstrap           = False,\n#                                 min_samples_leaf    = 1,\n#                                 min_samples_split   = 2,\n#                                 n_estimators        = 200\n#                                 )\n#\n#     def fit(self, X, y):\n#         X, y = remove_county_state(X, y)\n#\n#         # standardize with StandardScaler\n#         scaler_X = _scaler.fit(X)\n#         scaler_y = _scaler.fit(y)\n#         X_scaled = scaler_X.transform(X)\n#         y_scaled = scaler_y.transform(y)\n\n    #     # fit model\n    #     self._regressor.fit(X_scaled, y_scaled)\n    #\n    #     return self\n    #\n    # def predict(self, X):\n    #     return self._regressor.predict(X)\n\n\nif __name__ == \'__main__\':\n    # train_model()\n    #\n    # fm = FinalModel()\n    # X_train, X_test, y_train, y_test = split_data(data)\n    # fm.fit(X_train, y_train)\n    # fm.predict(X_test)\n    # fm.feat_imps = fm._regressor.feature_importances_\n'"
src/modclass_hot_one.py,0,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\n\ndef train_model():\n    data = get_data()\n    data[\'co\'] = data.state == \'colorado\'\n    data[\'fl\'] = data.state == \'florida\'\n    data[\'nj\'] = data.state == \'new jersey\'\n    data[\'ca\'] = data.state == \'california\'\n    print(data.shape)\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    model = RFR(    max_features        = \'auto\',\n                    max_depth           = None,\n                    bootstrap           = True,\n                    min_samples_leaf    = 5,\n                    min_samples_split   = 10,\n                    n_estimators        = 10\n                                )\n    trained_model = model.fit(X_train, y_train)\n    return trained_model\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    return X_train.columns\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(data_nas):\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n    data_nas = clean_data(data)\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\nif __name__ == \'__main__\':\n    pass\n'"
src/model.py,3,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\n\n\n\ndef column_names(X_train):\n    column_names = X_train.columns\n\n    nice_labels = [\'County\', \'Asthma Rate\', \'PM10 - Pollutant\', \'PM2.5 Pollutant\', \'PM2.5 non FRM Pollutant\',\n       \'PM2.5 Spec - Pollutant\', \'CO - Pollutant\', \'SO2 - Pollutant\', \'NO2 - Pollutant\', \'Ozone - Pollutant\',\n       \'NONOxNOy - Pollutant\', \'Lead - Pollutant\', \'HAPS - Pollutant\', \'VOCS - Pollutant\', \'State\',\n       \'Smokers (Adult)\', \'Obesity (Adult)\', \'Uninsured Rate\', \'PCP\', \'High School Grads\',\n       \'Unemployment\', \'Income Inequality\', \'Particulate Air Pollution\']\n    label_dict = dict(zip(column_names, nice_labels))\n\n    return label_dict\n\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print(\'Model Performance Indicators\')\n    print(\'Model:\', model)\n    print(\'MAE: {:0.3f}\'.format(mae))\n    print(\'MAPE: {:0.3f}\'.format(mape))\n    print(\'Accuracy = {:0.3f}%\'.format(accuracy))\n\n    print(\'RMSE (test):\', rmse_test)\n    print(\'RMSE (train):\', rmse_train)\n    if rmse_test > rmse_train:\n        print(\'Overfit\')\n    else:\n        print(\'Underfit\')\n    return accuracy\n\ndef split_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    # replacing nans with zeros\n    data_nas = data.fillna(0)\n    no_counties = data_nas.drop([\'county\', \'state\'], axis=1)\n    X = no_counties.drop(\'asthma_rate\', axis=1)\n    y = no_counties.asthma_rate\n\n    # consider replacing nans with mean (previously tried)...\n    # train/test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\n    return X_train, X_test, y_train, y_test\n\ndef do_model(X_train, X_test, y_train, y_test):\n\n    model = RFR(  max_features        = \'sqrt\',\n                max_depth           = 100,\n                bootstrap           = False,\n                min_samples_leaf    = 1,\n                min_samples_split   = 2,\n                n_estimators        = 200\n                )\n\n    model = model.fit(X_train, y_train)\n\n    ypred           = model.predict(X_test)\n    ytrainpred      = model.predict(X_train)\n\n    model_params    = model.get_params()\n    feat_imps       = model.feature_importances_\n\n    return model, model_params, feat_imps, ypred, ytrainpred\n\ndef policy_predict(X_train, X_test, y_train, y_test):\n    model, _, _, _, _,  = do_model(X_train, X_test, y_train, y_test)\n    policy_result = model.predict()\n\ndef chart_feature_importances(X_train, X_test, y_train, y_test):\n\n    _, feat_imps, _, _, _, = do_model(X_train, X_test, y_train, y_test)\n\n    # from data import column_names as cols\n    col_dict = column_names(X_train)\n\n    imps, names = zip(*sorted(zip(feat_imps, [col_dict.get(x, x) for x in X_train.columns])))\n\n    plt.style.use(\'bmh\')\n    # plt.style.use(\'seaborn-deep\')\n    # plt.style.use(\'seaborn-dark-palette\')\n    # plt.style.use(\'seaborn-dark-palette\')\n    # fig = plt.figure()\n    # plt.axis([0, 0.4, 0, 1])\n    plt.barh(range(len(names)), imps, align=\'center\')\n    plt.yticks(range(len(names)), names)\n    # plt.xticks(range(len(imps)), imps)\n    plt.xlabel(\'Relative Importance of Features\', fontsize=14)\n    plt.ylabel(\'Features\', fontsize=14)\n    # plt.title(\'Which Factors Drive Asthma Rates?\', fontsize=24)\n    plt.tight_layout()\n    # plt.show()\n    plt.savefig(\'static/images/feat_imps2.png\')\n\n\n\nif __name__ == \'__main__\':\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        # from combine_data import join_data as data\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n    # from data import join_data as data\n    # data = data()\n    # data.to_csv(csv_file_path, index=False)\n    X_train, X_test, y_train, y_test = split_data(data)\n\n    do_model(X_train, X_test, y_train, y_test)\n    chart_feature_importances(X_train, X_test, y_train, y_test)\n'"
src/models.py,3,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\ndef column_names(data):\n    column_names = data.columns\n    nice_labels = [\'County\', \'Asthma Rate\', \'PM10 - Pollutant\', \'PM2.5 Pollutant\', \'PM2.5 non FRM Pollutant\',\n       \'PM2.5 Spec - Pollutant\', \'CO - Pollutant\', \'SO2 - Pollutant\', \'NO2 - Pollutant\', \'Ozone - Pollutant\',\n       \'NONOxNOy - Pollutant\', \'Lead - Pollutant\', \'HAPS - Pollutant\', \'VOCS - Pollutant\', \'State\',\n       \'Smokers (Adult)\', \'Obesity (Adult)\', \'Uninsured Rate\', \'PCP\', \'High School Grads\',\n       \'Unemployment\', \'Income Inequality\', \'Particulate Air Pollution\']\n    label_dict = dict(zip(column_names, nice_labels))\n\n    return label_dict\n\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print(\'Model Performance Indicators\')\n    print(\'Model:\', model)\n    print(\'MAE: {:0.3f}\'.format(mae))\n    print(\'MAPE: {:0.3f}\'.format(mape))\n    print(\'Accuracy = {:0.3f}%\'.format(accuracy))\n\n    print(\'RMSE (test):\', rmse_test)\n    print(\'RMSE (train):\', rmse_train)\n    if rmse_test > rmse_train:\n        print(\'Overfit\')\n    else:\n        print(\'Underfit\')\n    return accuracy\n\ndef all_regress(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    # replacing nans with zeros\n    data_nas = data.fillna(0)\n    no_counties = data_nas.drop([\'county\', \'state\'], axis=1)\n    X = no_counties.drop(\'asthma_rate\', axis=1)\n    y = no_counties.asthma_rate\n    print(X.columns)\n    all_columns = [\'pm10_mean\', \'pm25_mean\', \'pm25non_mean\', \'pm25spec_mean\', \'co_mean\',\n       \'so2_mean\', \'no2_mean\', \'ozo_mean\', \'nonox_mean\', \'lead_mean\',\n       \'haps_mean\', \'vocs_mean\', \'smoke_adult\', \'obese_adult\', \'uninsured\',\n       \'pcp\', \'high_sch_grad\', \'unemployment\', \'income_ineq\', \'air_poll_partic\']\n    drop_columns = []\n    X = X.drop(drop_columns, axis=1)\n\n    # consider replacing nans with mean (previously tried)...\n    # train/test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                        test_size=0.2,\n                                                        random_state=123)\n\n\n    hyperpara_dict = {  LR  : {}, # use defaults only\n                        RFR : { \'randomforestregressor__max_features\' : [\'auto\', \'sqrt\'],\n                                \'randomforestregressor__max_depth\': [10, None],\n                                \'randomforestregressor__bootstrap\': [True, False],\n                                \'randomforestregressor__min_samples_leaf\': [1, 2, 4],\n                                \'randomforestregressor__min_samples_split\': [2, 5],\n                                \'randomforestregressor__n_estimators\': [10, 100],\n                                },\n                        GBR : { \'gradientboostingregressor__n_estimators\' :     [100, 600, 700, 800],\n                                \'gradientboostingregressor__max_depth\':         [3, 4, 5, 10, 20],\n                                \'gradientboostingregressor__min_samples_split\': [3, 4, 5, 10, 20],\n                                \'gradientboostingregressor__learning_rate\':     [0.01, 0.05, 0.1],\n                                \'gradientboostingregressor__loss\':              [\'ls\'],\n                                },\n                        KNR : { \'kneighborsregressor__n_neighbors\' : [1, 2, 3, 4, 5, 6, 10],\n                                \'kneighborsregressor__weights\': [\'uniform\', \'distance\'],\n                                },\n                        SVR : { \'svr__kernel\': [\'linear\', \'poly\', \'sigmoid\', \'rbf\'],\n                                \'svr__C\': [0.5, 5, 10, 15, 20, 50, 100, 200, 300, 500, 1000],\n                                \'svr__epsilon\': [0.1, 1.5, 2, 5, 10, 30, 50],\n                                },\n                        EN : { \'elasticnet__alpha\': [1,0.1,0.01,0.001], # equivalent to lambda; alpha=0 means no regularization, ie linear regression\n                                \'elasticnet__l1_ratio\': [0.5, 0.7, 0.8, 0.9, 1], # l1=1 means L1 penalty, ie Lasso (not L2/Ridge)\n                                \'elasticnet__max_iter\': [10000],\n                                }\n                        }\n\n    # models = [LR, RFR, GBR, KNR, SVR, EN]\n    models = [EN]\n    # mod_dict = {RFR:\'rfr\'}\n    # best_params_dict = {}\n    # for model, tag in mod_dict.items():\n    #     print(\'MODEL \'*12)\n    #     print(model)\n    #     print(tag)\n\n    print(\'Check if any y_train zeros:\', y_train[y_train==0])\n    print(\'Check if any y_test zeros:\', y_test[y_test==0])\n    for model in models:\n\n        # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n        pipeline = make_pipeline(   StandardScaler(),\n                                    model()\n                                 )\n\n        hyperparameters = hyperpara_dict[model]\n\n        clf = GridSearchCV(pipeline, hyperparameters, cv=3) # cv=3 is same as cv=None (default)\n\n        clf.fit(X_train, y_train)\n\n        # evaluate models with test data\n        pred = clf.predict(X_test)\n        # print (\'r2 score:\',r2_score(y_test, pred))\n        # print (\'mse:\',mean_squared_error(y_test, pred))\n        print(\'*\'*20)\n        print(\'best params:\',clf.best_params_)\n        print(\'best grid:\', clf.best_estimator_)\n        print(\'X_test-y_test Score: \', clf.score(X_test, y_test))\n        print(\'^\'*20)\n        eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n        # print(\'#\'*20)\n\n    #     best_params_dict[tag] = clf.best_params_\n    # print(\'BEST PARAMS \'*10)\n    #\n    # print(\'best_params_dict\', best_params_dict)\n    # print(\'END PARAMS \'*10)\n\n\n\n    # mod = RFR(  max_features        = best_params_dict[tag][\'randomforestregressor__max_features\'],\n    #             max_depth           = best_params_dict[tag][\'randomforestregressor__max_depth\'],\n    #             bootstrap           = best_params_dict[tag][\'randomforestregressor__bootstrap\'],\n    #             min_samples_leaf    = best_params_dict[tag][\'randomforestregressor__min_samples_leaf\'],\n    #             min_samples_split   = best_params_dict[tag][\'randomforestregressor__min_samples_split\']\n    #             )\n    #\n    #\n    # mod.fit(X_train, y_train)\n    # print(\'get_params()\'*20)\n    # print(mod.get_params())\n    # mod.predict(X_test)\n    # print(mod.feature_importances_)\n\n\n    # from data import column_names as cols\n    # col_dict = column_names(data)\n    # imps, names = zip(*sorted(zip(mod.feature_importances_, [col_dict.get(x, x) for x in X_train.columns])))\n\n    # plt.style.use(\'bmh\')\n    # plt.style.use(\'seaborn-deep\')\n    # plt.style.use(\'seaborn-dark-palette\')\n    # plt.style.use(\'seaborn-notebook\')\n    # plt.style.use(\'seaborn-pastel\')\n    # plt.style.use(\'ggplot\')\n\n    # fig = plt.figure()\n    # plt.axis([0, 0.4, 0, 1])\n    # plt.barh(range(len(names)), imps, align=\'center\')\n    # plt.yticks(range(len(names)), names)\n    # plt.xlabel(\'Relative Importance of Features\', fontsize=18)\n    # plt.ylabel(\'Features\', fontsize=18)\n    # plt.title(\'Which Factors Drive Asthma Rates?\', fontsize=24)\n    #\n    # plt.tight_layout()\n    # plt.show()\n    # plt.savefig(\'../saved_images/feat_imps.png\')\n\n\n    X_train = sm.add_constant(X_train)\n    model = sm.OLS(y_train, X_train)\n    results = model.fit()\n    print(results.summary())\n\n\nif __name__ == \'__main__\':\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        # from combine_data import join_data as data\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n    # from data import join_data as data\n    # data = data()\n    # data.to_csv(csv_file_path, index=False)\n\n    all_regress(data)\n'"
src/nj_data.py,0,"b'import numpy as np\nimport pandas as pd\n# source data: https://www26.state.nj.us/doh-shad/indicator/view/NJASTHMAHOSP.countyAAR.html\n\ndef get_data():\n    # def pm2_5spec_data():\n    pm2_5spec = pd.read_csv(\'../data/daily_88502_2017.csv\')\n    pm2_5spec.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    pm2_5specnj = pm2_5spec[pm2_5spec[\'state\'] == \'New Jersey\']\n    pm2_5specnjslim = pm2_5specnj.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    pm2_5specnjslim = pm2_5specnjslim.reset_index()\n    pm2_5specnjslim = pm2_5specnjslim.drop([\'index\'], axis=1)\n    pm2_5specnjslim[\'obs_x_mean\'] = pm2_5specnjslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    pm2_5specnjslim.county = pm2_5specnjslim.county.str.lower()\n    pm2_5specnjgroupby = pm2_5specnjslim.groupby(\'county\').sum()\n    pm2_5specnjgroupby = pm2_5specnjgroupby.reset_index()\n    pm2_5specnjgroupby[\'new_mean\'] = pm2_5specnjgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    pm2_5specnjfinal = pm2_5specnjgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    pm2_5specnjfinal.columns = [\'county\', \'pm2_5spec_mean\']\n\n    # def pm2_5non_data():\n    pm2_5non = pd.read_csv(\'../data/daily_88502_2017.csv\')\n    pm2_5non.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    pm2_5nonnj = pm2_5non[pm2_5non[\'state\'] == \'New Jersey\']\n    pm2_5nonnjslim = pm2_5nonnj.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    pm2_5nonnjslim = pm2_5nonnjslim.reset_index()\n    pm2_5nonnjslim = pm2_5nonnjslim.drop([\'index\'], axis=1)\n    pm2_5nonnjslim[\'obs_x_mean\'] = pm2_5nonnjslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    pm2_5nonnjslim.county = pm2_5nonnjslim.county.str.lower()\n    pm2_5nonnjgroupby = pm2_5nonnjslim.groupby(\'county\').sum()\n    pm2_5nonnjgroupby = pm2_5nonnjgroupby.reset_index()\n    pm2_5nonnjgroupby[\'new_mean\'] = pm2_5nonnjgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    pm2_5nonnjfinal = pm2_5nonnjgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    pm2_5nonnjfinal.columns = [\'county\', \'pm2_5non_mean\']\n\n    # def pm2_5_data():\n    pm2_5 = pd.read_csv(\'../data/daily_88101_2017.csv\')\n    pm2_5.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    pm2_5nj = pm2_5[pm2_5[\'state\'] == \'New Jersey\']\n    pm2_5njslim = pm2_5nj.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    pm2_5njslim = pm2_5njslim.reset_index()\n    pm2_5njslim = pm2_5njslim.drop([\'index\'], axis=1)\n    pm2_5njslim[\'obs_x_mean\'] = pm2_5njslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    pm2_5njslim.county = pm2_5njslim.county.str.lower()\n    pm2_5njgroupby = pm2_5njslim.groupby(\'county\').sum()\n    pm2_5njgroupby = pm2_5njgroupby.reset_index()\n    pm2_5njgroupby[\'new_mean\'] = pm2_5njgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    pm2_5njfinal = pm2_5njgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    pm2_5njfinal.columns = [\'county\', \'pm2_5_mean\']\n\n    # def pm10_data():\n    pm10 = pd.read_csv(\'../data/daily_81102_2017.csv\')\n    pm10.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    pm10nj = pm10[pm10[\'state\'] == \'New Jersey\']\n    pm10njslim = pm10nj.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    pm10njslim = pm10njslim.reset_index()\n    pm10njslim = pm10njslim.drop([\'index\'], axis=1)\n    pm10njslim[\'obs_x_mean\'] = pm10njslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    pm10njslim.county = pm10njslim.county.str.lower()\n    pm10njgroupby = pm10njslim.groupby(\'county\').sum()\n    pm10njgroupby = pm10njgroupby.reset_index()\n    pm10njgroupby[\'new_mean\'] = pm10njgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    pm10njfinal = pm10njgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    pm10njfinal.columns = [\'county\', \'pm10_mean\']\n\n    # def lead_data():\n    lead = pd.read_csv(\'../data/daily_LEAD_2014.csv\')\n    lead.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    leadnj = lead[lead[\'state\'] == \'New Jersey\']\n    leadnjslim = leadnj.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    leadnjslim = leadnjslim.reset_index()\n    leadnjslim = leadnjslim.drop([\'index\'], axis=1)\n    leadnjslim[\'obs_x_mean\'] = leadnjslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    leadnjslim.county = leadnjslim.county.str.lower()\n    leadnjgroupby = leadnjslim.groupby(\'county\').sum()\n    leadnjgroupby = leadnjgroupby.reset_index()\n    leadnjgroupby[\'new_mean\'] = leadnjgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    leadnjfinal = leadnjgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    leadnjfinal.columns = [\'county\', \'lead_mean\']\n\n    # def nonox_data():\n    nonox = pd.read_csv(\'../data/daily_NONOxNOy_2017.csv\')\n    nonox.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    nonoxnj = nonox[nonox[\'state\'] == \'New Jersey\']\n    nonoxnjcoslim = nonoxnj.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    nonoxnjcoslim = nonoxnjcoslim.reset_index()\n    nonoxnjcoslim = nonoxnjcoslim.drop([\'index\'], axis=1)\n    nonoxnjcoslim[\'obs_x_mean\'] = nonoxnjcoslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    nonoxnjcoslim.county = nonoxnjcoslim.county.str.lower()\n    nonoxnjgroupby = nonoxnjcoslim.groupby(\'county\').sum()\n    nonoxnjgroupby = nonoxnjgroupby.reset_index()\n    nonoxnjgroupby[\'new_mean\'] = nonoxnjgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    nonoxnjfinal = nonoxnjgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    nonoxnjfinal.columns = [\'county\', \'nonox_mean\']\n\n    # def haps_data():\n    haps = pd.read_csv(\'../data/daily_HAPS_2017.csv\')\n    haps.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    hapsnj = haps[haps[\'state\'] == \'New Jersey\']\n    hapsnjslim = hapsnj.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    hapsnjslim = hapsnjslim.reset_index()\n    hapsnjslim = hapsnjslim.drop([\'index\'], axis=1)\n    hapsnjslim[\'obs_x_mean\'] = hapsnjslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    hapsnjslim.county = hapsnjslim.county.str.lower()\n    hapsnjgroupby = hapsnjslim.groupby(\'county\').sum()\n    hapsnjgroupby = hapsnjgroupby.reset_index()\n    hapsnjgroupby[\'new_mean\'] = hapsnjgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    hapsnjfinal = hapsnjgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    hapsnjfinal.columns = [\'county\', \'haps_mean\']\n\n    # def vocs_data():\n    # vocs16 = pd.read_csv(\'daily_VOCS_2016.csv\')\n    vocs = pd.read_csv(\'../data/daily_VOCS_2017.csv\')\n    vocs.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    vocsnj = vocs[vocs[\'state\'] == \'New Jersey\']\n    vocsnjslim = vocsnj.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    vocsnjslim = vocsnjslim.reset_index()\n    vocsnjslim = vocsnjslim.drop([\'index\'], axis=1)\n    vocsnjslim[\'obs_x_mean\'] = vocsnjslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    vocsnjslim.county = vocsnjslim.county.str.lower()\n    vocsnjgroupby = vocsnjslim.groupby(\'county\').sum()\n    vocsnjgroupby = vocsnjgroupby.reset_index()\n    vocsnjgroupby[\'new_mean\'] = vocsnjgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    vocsnjfinal = vocsnjgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    vocsnjfinal.columns = [\'county\', \'vocs_mean\']\n\n    # def co_data():\n    co = pd.read_csv(\'../data/daily_42101_2017.csv\')\n    co.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    conj = co[co[\'state\'] == \'New Jersey\']\n    conjslim = conj.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    conjslim = conjslim.reset_index()\n    conjslim = conjslim.drop([\'index\'], axis=1)\n    conjslim[\'obs_x_mean\'] = conjslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    conjslim.county = conjslim.county.str.lower()\n    conjgroupby = conjslim.groupby(\'county\').sum()\n    conjgroupby = conjgroupby.reset_index()\n    conjgroupby[\'new_mean\'] = conjgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    conjfinal = conjgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    conjfinal.columns = [\'county\', \'co_mean\']\n\n    # def so2_data():\n    so2 = pd.read_csv(\'../data/daily_42401_2017.csv\')\n    so2.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    sonj = so2[so2[\'state\'] == \'New Jersey\']\n    sonjslim = sonj.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    sonjslim = sonjslim.reset_index()\n    sonjslim = sonjslim.drop([\'index\'], axis=1)\n    sonjslim[\'obs_x_mean\'] = sonjslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    sonjslim.county = sonjslim.county.str.lower()\n    sonjgroupby = sonjslim.groupby(\'county\').sum()\n    sonjgroupby = sonjgroupby.reset_index()\n    sonjgroupby[\'new_mean\'] = sonjgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    sonjfinal = sonjgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    sonjfinal.columns = [\'county\', \'so_mean\']\n\n    # def no2_data():\n    no2 = pd.read_csv(\'../data/daily_42602_2017.csv\')\n    no2.columns = [\'st_cd\', \'cnt_cd\', \'site_nm\', \'param_cd\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'units\', \'event_type\',\n           \'obs_count\', \'obs_perc\', \'mean_avg\', \'first_max_val\', \'first_max_hour\',\n           \'aqi\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'county\', \'city\', \'cbsa\', \'last_change_date\']\n    nonj = no2[no2[\'state\'] == \'New Jersey\']\n    nonjslim = nonj.drop([\'st_cd\', \'cnt_cd\', \'site_nm\', \'poc\', \'lat\', \'lon\', \'datum\',\n           \'param\', \'duration\', \'pollutant\', \'date\', \'event_type\', \'units\', \'aqi\',\n           \'obs_perc\', \'first_max_val\', \'first_max_hour\', \'method_cd\', \'method\', \'local_site\', \'address\', \'state\',\n           \'city\', \'cbsa\', \'last_change_date\'], axis=1)\n    nonjslim.county = nonjslim.county.str.lower()\n    nonjslim = nonjslim.reset_index()\n    nonjslim = nonjslim.drop([\'index\'], axis=1)\n    nonjslim[\'obs_x_mean\'] = nonjslim.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    nonjgroupby = nonjslim.groupby(\'county\').sum()\n    nonjgroupby = nonjgroupby.reset_index()\n    nonjgroupby[\'new_mean\'] = nonjgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    nonjfinal = nonjgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\', \'param_cd\'], axis=1)\n    nonjfinal.columns = [\'county\', \'no_mean\']\n\n    # def ozone_data():\n    ozo = pd.read_csv(\'../data/daily_44201_2017.csv\')\n    ozoslim = ozo.drop([\'State Code\', \'County Code\', \'Site Num\', \'POC\',\n           \'Latitude\', \'Longitude\', \'Datum\', \'Sample Duration\',\n           \'Pollutant Standard\', \'Date Local\', \'Units of Measure\', \'Event Type\',\n            \'Observation Percent\',\n           \'1st Max Value\', \'1st Max Hour\', \'AQI\', \'Method Code\', \'Method Name\',\n           \'Local Site Name\', \'Address\', \'City Name\',\n           \'CBSA Name\', \'Date of Last Change\'], axis=1)\n    ozoslim.columns = [\'param_cd\', \'param\', \'obs_count\', \'mean_avg\', \'state\', \'county\']\n    ozonj = ozoslim[ozoslim[\'state\'] == \'New Jersey\']\n    ozonj.county = ozonj.county.str.lower()\n    ozonj = ozonj.reset_index()\n    ozonj = ozonj.drop([\'index\', \'param\', \'state\', \'param_cd\'], axis= 1)\n    ozonj[\'obs_x_mean\']  = ozonj.apply(lambda row: row.obs_count * row.mean_avg, axis=1)\n    ozonjgroupby = ozonj.groupby(\'county\').sum()\n    ozonjgroupby = ozonjgroupby.reset_index()\n    ozonjgroupby[\'new_mean\'] = ozonjgroupby.apply(lambda row: row.obs_x_mean / row.obs_count, axis=1)\n    ozonjfinal = ozonjgroupby.drop([\'obs_count\', \'mean_avg\', \'obs_x_mean\'], axis=1)\n    ozonjfinal.columns = [\'county\', \'ozo_mean\']\n\n    # def asthma_nj():\n    # https://data-cdphe.opendata.arcgis.com/datasets/asthma-hospitalization-rate-counties\n    asthmahospnj = pd.read_csv(\'../data/asthma_hospitization_nj.csv\')\n    # asthmahospnj.columns = asthmahospnj.columns.str.lower()\n    asthmahospnj.county = asthmahospnj.county.str.lower()\n    # asthmahospnj.columns = [\'county\', \'asthma_rate\']\n\n    asthmanj = asthmahospnj\n\n    # def merge_all():\n    asozo_nj                       = asthmanj.merge(ozonjfinal, how=""left"", on=""county"")\n    asozono_nj                     = asozo_nj.merge(nonjfinal, how=""left"", on=""county"")\n    asozononj_nj                   = asozono_nj.merge(conjfinal, how=""left"", on=""county"")\n    asozononjso_nj                 = asozononj_nj.merge(sonjfinal, how=""left"", on=""county"")\n    asozononjsovocs_nj             = asozononjso_nj.merge(vocsnjfinal, how=""left"", on=""county"")\n    asozononjsovocshaps_nj         = asozononjsovocs_nj.merge(hapsnjfinal, how=""left"", on=""county"")\n    asozononjsovocshapsnonox_nj    = asozononjsovocshaps_nj.merge(nonoxnjfinal, how=""left"", on=""county"")\n\n    asozononjsovocshapsnonoxlead_nj                         = asozononjsovocshaps_nj.merge(leadnjfinal, how=""left"", on=""county"")\n    asozononjsovocshapsnonoxpm10_nj                         = asozononjsovocshapsnonoxlead_nj.merge(pm10njfinal, how=""left"", on=""county"")\n    asozononjsovocshapsnonoxpm10pm2_5_nj                    = asozononjsovocshapsnonoxpm10_nj.merge(pm2_5njfinal, how=""left"", on=""county"")\n    asozononjsovocshapsnonoxpm10pm2_5pm2_5non_nj            = asozononjsovocshapsnonoxpm10pm2_5_nj.merge(pm2_5nonnjfinal, how=""left"", on=""county"")\n    asozononjsovocshapsnonoxpm10pm2_5pm2_5nonpm2_5spec_nj   = asozononjsovocshapsnonoxpm10pm2_5pm2_5non_nj.merge(pm2_5specnjfinal, how=""left"", on=""county"")\n\n    return asozononjsovocshapsnonoxpm10pm2_5pm2_5nonpm2_5spec_nj\n'"
src/ols_model_hot_one.py,3,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print(\'Model Performance Indicators\')\n    print(\'MAE: {:0.3f}\'.format(mae))\n    print(\'MAPE: {:0.3f}\'.format(mape))\n    print(\'Accuracy = {:0.3f}%\'.format(accuracy))\n\n    print(\'RMSE (test):\', rmse_test)\n    print(\'RMSE (train):\', rmse_train)\n    if rmse_test > rmse_train:\n        print(\'Overfit\')\n    else:\n        print(\'Underfit\')\n\n    return accuracy\n\n\ndef train_model():\n    data = get_data()\n    data[\'co\'] = data.state == \'colorado\'\n    data[\'fl\'] = data.state == \'florida\'\n    data[\'nj\'] = data.state == \'new jersey\'\n    data[\'ca\'] = data.state == \'california\'\n    data.co = data.co.astype(int)\n    data.fl = data.fl.astype(int)\n    data.ca = data.ca.astype(int)\n    data.nj = data.nj.astype(int)\n    # print(data.co)\n\n    all_columns = [\'pm10_mean\', \'pm25_mean\', \'pm25non_mean\', \'pm25spec_mean\', \'co_mean\',\n       \'so2_mean\', \'no2_mean\', \'ozo_mean\', \'nonox_mean\', \'lead_mean\',\n       \'haps_mean\', \'vocs_mean\', \'smoke_adult\', \'obese_adult\', \'uninsured\',\n       \'pcp\', \'high_sch_grad\', \'unemployment\', \'income_ineq\', \'air_poll_partic\']\n    drop_columns = [\'pm10_mean\', \'pm25_mean\', \'pm25non_mean\', \'pm25spec_mean\', \'co_mean\',\n        \'no2_mean\', \'ozo_mean\', \'nonox_mean\', \'lead_mean\',\n       \'haps_mean\', \'vocs_mean\', \'pcp\', \'high_sch_grad\', \'income_ineq\', \'co\', \'ca\', \'smoke_adult\']\n    data = data.drop(drop_columns, axis=1)\n\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    X_test, y_test = remove_county_state(X_test, y_test)\n\n    X_train = sm.add_constant(X_train)\n\n    model = sm.OLS(y_train, X_train, missing=\'drop\')\n    results = model.fit()\n    print(type(results.summary()))\n    print(results.summary().as_html())\n    print(results.summary())\n\n    return results.summary().as_html()\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n\n    return X_train.columns\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(data_nas):\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n    data_nas = clean_data(data)\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\nif __name__ == \'__main__\':\n    train_model()\n'"
src/random_forest.py,3,"b""import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print('Model Performance Indicators')\n    print('MAE: {:0.3f}'.format(mae))\n    print('MAPE: {:0.3f}'.format(mape))\n    print('Accuracy = {:0.3f}%'.format(accuracy))\n\n    print('RMSE (test):', rmse_test)\n    print('RMSE (train):', rmse_train)\n    if rmse_test > rmse_train:\n        print('Overfit')\n    else:\n        print('Underfit')\n    return accuracy\n\n\n\ndef rand_forest(data):\n    # drop problematic row with zero for asthma_rate\n    # data = data.drop([171])\n    data_nas = data.fillna(0)\n    no_counties = data_nas.drop(['county', 'state'], axis=1)\n    # X = no_counties.drop('asthma_rate', axis=1)\n    X = no_counties.drop(['asthma_rate', 'pm2_5_mean','pm10_mean','haps_mean','no_mean','vocs_mean','pm2_5non_mean','co_mean','pm2_5spec_mean','so_mean', 'lead_mean','income_ineq', 'high_sch_grad', 'obese_adult'], axis=1)\n    y = no_counties.asthma_rate\n\n    # train/test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                        test_size=0.2,\n                                                        random_state=123)\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             RandomForestRegressor())\n\n    # set hyperparameters\n    # hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n    #                     'randomforestregressor__max_depth': [None, 20, 10, 5, 2],\n    #                     'randomforestregressor__bootstrap': [True],\n    #                     'randomforestregressor__min_samples_leaf': [3, 4, 5],\n    #                     'randomforestregressor__min_samples_split': [8, 10, 12],\n    #                     'randomforestregressor__n_estimators': [100, 200, 300]\n    #                     }\n\n    hyperparameters = { 'randomforestregressor__max_features' : ['auto'],\n                        'randomforestregressor__max_depth': [None, 5, 2],\n                        'randomforestregressor__bootstrap': [True],\n                        'randomforestregressor__min_samples_leaf': [4],\n                        'randomforestregressor__min_samples_split': [10],\n                        'randomforestregressor__n_estimators': [200]\n                        }\n\n    # tune model via pipeline\n    clf = GridSearchCV(pipeline, hyperparameters, cv=3)\n\n    clf.fit(X_train, y_train)\n\n    # evaluate models with test data\n    pred = clf.predict(X_test)\n    # print('feature importances:', clf.feature_importances_)\n    print ('r2 score:',r2_score(y_test, pred))\n    print ('mse:',mean_squared_error(y_test, pred))\n    print('*'*20)\n    print('best params:',clf.best_params_)\n    print('best grid:', clf.best_estimator_)\n    print('^'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print('#'*20)\n\n    # print('pipeline.steps[1]', pipeline.steps[1])\n    # print('pipeline.steps[1][1].feature_importances_', pipeline.steps[1][1].feature_importances_)\n    # print('pipeline.steps[0][1].get_feature_names()', pipeline.steps[0][1].get_feature_names())\n\n\nif __name__ == '__main__':\n    # get data\n    from combine_data import join_data as data\n    data = data()\n    # run random forest model\n    rand_forest(data)\n"""
src/random_forest2.py,3,"b""import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print('Model Performance Indicators')\n    print('MAE: {:0.3f}'.format(mae))\n    print('MAPE: {:0.3f}'.format(mape))\n    print('Accuracy = {:0.3f}%'.format(accuracy))\n\n    print('RMSE (test):', rmse_test)\n    print('RMSE (train):', rmse_train)\n    if rmse_test > rmse_train:\n        print('Overfit')\n    else:\n        print('Underfit')\n    return accuracy\n\n\n\ndef rand_forest(data):\n    # drop problematic row with zero for asthma_rate\n    # data = data.drop([171])\n    data_nas = data.fillna(-1)\n    no_counties = data_nas.drop(['county', 'state'], axis=1)\n    # X = no_counties.drop('asthma_rate', axis=1)\n    X = no_counties.drop(['asthma_rate', 'pm2_5_mean','pm10_mean','haps_mean','no_mean','vocs_mean','pm2_5non_mean','co_mean','pm2_5spec_mean','so_mean', 'lead_mean','income_ineq', 'high_sch_grad', 'obese_adult'], axis=1)\n    y = no_counties.asthma_rate\n\n    # train/test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                        test_size=0.2,\n                                                        random_state=123)\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             RandomForestRegressor())\n\n    # set hyperparameters\n    hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n                        'randomforestregressor__max_depth': [None, 5, 2],\n                        'randomforestregressor__bootstrap': [True],\n                        'randomforestregressor__min_samples_leaf': [3, 4, 5],\n                        'randomforestregressor__min_samples_split': [10, 12, 15],\n                        'randomforestregressor__n_estimators': [10, 50, 100, 150]\n                        }\n\n    # hyperparameters = { 'randomforestregressor__max_features' : ['auto'],\n    #                     'randomforestregressor__max_depth': [None, 5, 2],\n    #                     'randomforestregressor__bootstrap': [True],\n    #                     'randomforestregressor__min_samples_leaf': [4],\n    #                     'randomforestregressor__min_samples_split': [10],\n    #                     'randomforestregressor__n_estimators': [200]\n    #                     }\n\n    # tune model via pipeline\n    clf = GridSearchCV(pipeline, hyperparameters, cv=3)\n\n    clf.fit(X_train, y_train)\n\n    # evaluate models with test data\n    pred = clf.predict(X_test)\n    # print('feature importances:', clf.feature_importances_)\n    print('best_estimator_.feature importances:', clf.best_estimator_.feature_importances_)\n    print ('r2 score:',r2_score(y_test, pred))\n    print ('mse:',mean_squared_error(y_test, pred))\n    print('*'*20)\n    print('best params:',clf.best_params_)\n    print('best grid:', clf.best_estimator_)\n    print('^'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print('#'*20)\n\n    # print('pipeline.steps[1]', pipeline.steps[1])\n    # print('pipeline.steps[1][1].feature_importances_', pipeline.steps[1][1].feature_importances_)\n    # print('pipeline.steps[0][1].get_feature_names()', pipeline.steps[0][1].get_feature_names())\n\n\nif __name__ == '__main__':\n    # get data\n    from combine_data import join_data as data\n    data = data()\n    # run random forest model\n    rand_forest(data)\n"""
src/rfr_model.py,3,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n    errors_for_plot = ypred - y_test\n    print(\'errors_for_plot\', list(errors_for_plot))\n\n    print(\'Model Performance Indicators\')\n    print(\'MAE: {:0.3f}\'.format(mae))\n    print(\'MAPE: {:0.3f}\'.format(mape))\n    print(\'Accuracy = {:0.3f}%\'.format(accuracy))\n\n    print(\'RMSE (test):\', rmse_test)\n    print(\'RMSE (train):\', rmse_train)\n    if rmse_test > rmse_train:\n        print(\'Overfit\')\n    else:\n        print(\'Underfit\')\n    return accuracy, rmse_train, rmse_test, errors_for_plot\n\n\ndef train_model():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    X_test, y_test = remove_county_state(X_test, y_test)\n\n    print(\'y_train\', list(y_train))\n    print(\'y_test\', list(y_test))\n    print(\'all y\', list(y_train)+list(y_test))\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             RFR())\n\n    # set hyperparameters\n    hyperparameters = {\n                        # \'randomforestregressor__max_features\' : [\'auto\', \'sqrt\'],\n                        # \'randomforestregressor__max_depth\': [3, 5, None],\n                        # \'randomforestregressor__bootstrap\': [True, False],\n                        # \'randomforestregressor__min_samples_leaf\': [3, 5, 7],\n                        # \'randomforestregressor__min_samples_split\': [5, 10, 15],\n                        # \'randomforestregressor__n_estimators\': [5, 8, 10, 15],\n\n                        # \'randomforestregressor__max_features\' : [\'sqrt\'],\n                        # \'randomforestregressor__max_depth\': [100],\n                        # \'randomforestregressor__bootstrap\': [ False],\n                        # \'randomforestregressor__min_samples_leaf\': [1],\n                        # \'randomforestregressor__min_samples_split\': [2],\n                        # \'randomforestregressor__n_estimators\': [200],\n\n                        #\n                        # \'randomforestregressor__max_leaf_nodes\': [None],\n                        # \'randomforestregressor__min_impurity_decrease\': [0.0],\n                        # \'randomforestregressor__min_impurity_split\':[None],\n                        # \'randomforestregressor__min_weight_fraction_leaf\':[0.0],\n\n                        \'randomforestregressor__max_features\': [\'auto\'],\n                        \'randomforestregressor__max_depth\': [None],\n                        \'randomforestregressor__bootstrap\': [True],\n                        \'randomforestregressor__min_samples_leaf\': [5],\n                        \'randomforestregressor__min_samples_split\': [10],\n                        \'randomforestregressor__n_estimators\':[10],\n\n                        # \'randomforestregressor__max_features\': [\'auto\'],\n                        # \'randomforestregressor__max_depth\': [None],\n                        # \'randomforestregressor__bootstrap\': [True],\n                        # \'randomforestregressor__min_samples_leaf\': [5],\n                        # \'randomforestregressor__min_samples_split\': [10],\n                        # \'randomforestregressor__n_estimators\':[10, 30, 50, 70, 100],\n                        }\n\n\n\n\n    # tune model via pipeline\n    clf = GridSearchCV(pipeline, hyperparameters, cv=3)\n\n    clf.fit(X_train, y_train)\n\n    pred = clf.predict(X_test)\n    # print(\'feature importances:\', clf.feature_importances_)\n    print (\'r2 score:\',r2_score(y_test, pred))\n    print (\'mse:\',mean_squared_error(y_test, pred))\n    print(\'*\'*20)\n    print(\'best params:\',clf.best_params_)\n    print(\'best grid:\', clf.best_estimator_)\n    print(\'^\'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print(\'#\'*20)\n    print(\'score\', clf.score)\n    return clf\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    return X_train.columns\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(data_nas):\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n    data_nas = clean_data(data)\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\nif __name__ == \'__main__\':\n    train_model()\n'"
src/rfr_model_hot_one.py,3,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n    errors_for_plot = ypred - y_test\n    print(\'errors_for_plot\', list(errors_for_plot))\n\n    print(\'Model Performance Indicators\')\n    print(\'MAE: {:0.3f}\'.format(mae))\n    print(\'MAPE: {:0.3f}\'.format(mape))\n    print(\'Accuracy = {:0.3f}%\'.format(accuracy))\n\n    print(\'RMSE (test):\', rmse_test)\n    print(\'RMSE (train):\', rmse_train)\n    if rmse_test > rmse_train:\n        print(\'Overfit\')\n    else:\n        print(\'Underfit\')\n    return accuracy, rmse_train, rmse_test, errors_for_plot\n\n\ndef train_model():\n    data = get_data()\n    data[\'co\'] = data.state == \'colorado\'\n    data[\'fl\'] = data.state == \'florida\'\n    data[\'nj\'] = data.state == \'new jersey\'\n    data[\'ca\'] = data.state == \'california\'\n    print(data.shape)\n\n\n    all_columns = [\'pm10_mean\', \'pm25_mean\', \'pm25non_mean\', \'pm25spec_mean\', \'co_mean\',\n       \'so2_mean\', \'no2_mean\', \'ozo_mean\', \'nonox_mean\', \'lead_mean\',\n       \'haps_mean\', \'vocs_mean\', \'smoke_adult\', \'obese_adult\', \'uninsured\',\n       \'pcp\', \'high_sch_grad\', \'unemployment\', \'income_ineq\', \'air_poll_partic\']\n    drop_columns = [\'pm10_mean\', \'pm25_mean\', \'pm25non_mean\', \'pm25spec_mean\', \'co_mean\',\n       \'so2_mean\', \'no2_mean\', \'ozo_mean\', \'nonox_mean\', \'lead_mean\',\n       \'haps_mean\', \'vocs_mean\', \'pcp\', \'high_sch_grad\', \'income_ineq\', \'co\', \'ca\', \'smoke_adult\']\n    data = data.drop(drop_columns, axis=1)\n\n\n\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    X_test, y_test = remove_county_state(X_test, y_test)\n\n    print(\'y_train\', list(y_train))\n    print(\'y_test\', list(y_test))\n    print(\'all y\', list(y_train)+list(y_test))\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             RFR())\n\n    # set hyperparameters\n    hyperparameters = {\n                        # even better performance\n                        # \'randomforestregressor__max_features\' : [\'auto\', \'sqrt\'],\n                        # \'randomforestregressor__max_depth\': [3, 5, None],\n                        # \'randomforestregressor__bootstrap\': [True, False],\n                        # \'randomforestregressor__min_samples_leaf\': [3, 5, 7],\n                        # \'randomforestregressor__min_samples_split\': [5, 10, 15],\n                        # \'randomforestregressor__n_estimators\': [5, 8, 10, 15, 50, 100],\n\n                        # \'randomforestregressor__max_features\' : [\'sqrt\'],\n                        # \'randomforestregressor__max_depth\': [100],\n                        # \'randomforestregressor__bootstrap\': [ False],\n                        # \'randomforestregressor__min_samples_leaf\': [1],\n                        # \'randomforestregressor__min_samples_split\': [2],\n                        # \'randomforestregressor__n_estimators\': [200],\n\n                        # best performance:\n                        # \'randomforestregressor__max_features\': [\'auto\'],\n                        # \'randomforestregressor__max_depth\': [None],\n                        # \'randomforestregressor__bootstrap\': [True],\n                        # \'randomforestregressor__min_samples_leaf\': [5],\n                        # \'randomforestregressor__min_samples_split\': [10],\n                        # \'randomforestregressor__n_estimators\':[10],\n\n                        \'randomforestregressor__max_features\': [\'auto\'],\n                        \'randomforestregressor__max_depth\': [None],\n                        \'randomforestregressor__bootstrap\': [True],\n                        \'randomforestregressor__min_samples_leaf\': [2, 5, 10],\n                        \'randomforestregressor__min_samples_split\': [5, 10, 15],\n                        \'randomforestregressor__n_estimators\':[5, 8, 9, 10, 11, 12, 30, 50, 70, 100, 200],\n                        }\n\n\n\n\n    # tune model via pipeline\n    clf = GridSearchCV(pipeline, hyperparameters, cv=3)\n\n    clf.fit(X_train, y_train)\n\n    pred = clf.predict(X_test)\n    # print(\'feature importances:\', clf.feature_importances_)\n    print (\'r2 score:\',r2_score(y_test, pred))\n    print (\'mse:\',mean_squared_error(y_test, pred))\n    print(\'*\'*20)\n    print(\'best params:\',clf.best_params_)\n    print(\'best grid:\', clf.best_estimator_)\n    print(\'^\'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print(\'#\'*20)\n    print(\'score\', clf.score)\n    return clf\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    return X_train.columns\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(data_nas):\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n    data_nas = clean_data(data)\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\nif __name__ == \'__main__\':\n    train_model()\n'"
src/state_color_map.py,0,"b'import csv\nfrom bs4 import BeautifulSoup\nimport time\n\n\ndef create_map(state_data):\n\n    a = state_data.set_index(\'fips\').to_dict()\n    d = a[\'pred\']\n\n    print(\'This is the dictionary for the state_pred_map: \', d)\n\n    # load SVG map\n    svg = open(\'static/images/co_counties_blank.svg\', \'r\').read()\n\n    # load into Beautiful Soup\n    soup = BeautifulSoup(svg, selfClosingTags=[\'defs\',\'sodipodi:namedview\'])\n\n    # find counties\n    paths = soup.findAll(\'path\')\n\n    # map colors\n    # colors = [""#f7fcf5"", ""#e5f5e0"", ""#c7e9c0"", ""#a1d99b"", ""#74c476"", ""#41ab5d"", ""#238b45"", ""#006d2c"", ""#00441b""]\n    colors = [""#f7fbff"", ""#deebf7"", ""#c6dbef"", ""#9ecae1"", ""#6baed6"", ""#4292c6"", ""#2171b5"", ""#08519c"", ""#08306b""]\n\n    # county style\n    path_style = \'font-size:12px;fill-rule:nonzero;stroke:#FFFFFF;stroke-opacity:1; stroke-width:0.1;stroke-miterlimit:4;stroke-dasharray:none;stroke-linecap:butt; marker-start:none;stroke-linejoin:bevel;fill:\'\n\n    # color the counties based on asthma rate\n    for p in paths:\n\n        if p[\'id\'] not in [""State_Lines"", ""separator""]:\n            # pass\n            try:\n                rate = d[p[\'id\']]\n            except:\n                continue\n\n            if rate > 80:\n                color_class = 8\n            elif rate > 70:\n                color_class = 7\n            elif rate > 60:\n                color_class = 6\n            elif rate > 50:\n                color_class = 5\n            elif rate > 40:\n                color_class = 4\n            elif rate > 30:\n                color_class = 3\n            elif rate > 20:\n                color_class = 2\n            elif rate > 10:\n                color_class = 1\n            else:\n                color_class = 0\n\n            color = colors[color_class]\n            p[\'style\'] = path_style + color\n\n\n    ts = time.time()\n    print(ts)\n    # output map\n    mymap = soup.prettify()\n    state_pred_map = \'static/images/state_pred_map_{}.svg\'.format(ts)\n    open(state_pred_map, \'w\').write(mymap)\n    return state_pred_map\n'"
src/support_vector_regressor.py,4,"b""import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom math import sqrt\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils import shuffle\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print('Model Performance Indicators')\n    print('MAE: {:0.3f}'.format(mae))\n    print('MAPE: {:0.3f}'.format(mape))\n    print('Accuracy = {:0.3f}%'.format(accuracy))\n\n    print('RMSE (test):', rmse_test)\n    print('RMSE (train):', rmse_train)\n    if rmse_test > rmse_train:\n        print('Overfit')\n    else:\n        print('Underfit')\n    return accuracy\n\n\ndef sup_vec_regress(data):\n    # drop problematic row with zero for asthma_rate\n    # data = data.drop([171])\n    data_nas = data.fillna(0)\n    no_counties = data_nas.drop(['county', 'state'], axis=1)\n    # X = no_counties.drop('asthma_rate', axis=1)\n    # X = no_counties.drop(['asthma_rate', 'ozo_mean','unemployment','uninsured','pcp','pm2_5_mean','pm2_5non_mean','pm2_5spec_mean','air_poll_partic','income_ineq', 'high_sch_grad', 'obese_adult'], axis=1)\n    X = no_counties.drop(['asthma_rate', 'pm2_5_mean','pm10_mean','haps_mean','no_mean','vocs_mean','pm2_5non_mean','co_mean','pm2_5spec_mean','so_mean', 'lead_mean','income_ineq', 'high_sch_grad', 'obese_adult'], axis=1)\n    y = no_counties.asthma_rate\n\n    # train/test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                        test_size=0.2,\n                                                        random_state=123)\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             SVR())\n\n    # set hyperparameters\n    # hyperparameters =  [{'kernel': ['rbf'],\n    #                     'gamma': [1e-4, 1e-3, 0.01, 0.1, 0.2, 0.5],\n    #                     'C': [0.1, 1, 10, 100, 1000]},\n    #                     {'kernel': ['linear'],\n    #                     'C': [1, 10, 100, 1000]}\n    #                     ]\n\n    hyperparameters = { 'svr__kernel': ['linear', 'rbf'],\n                        'svr__C': [0.1, 1, 5, 10, 20, 50, 100, 1000]\n                        }\n\n    # cv =                {2, 3, 5, 10}\n\n    # tune model via pipeline\n    clf = GridSearchCV(pipeline, hyperparameters, cv=5)\n\n    clf.fit(X_train, y_train)\n\n    # evaluate models with test data\n    pred = clf.predict(X_test)\n    # print('feature importances:', clf.feature_importances_)\n    print ('r2 score:',r2_score(y_test, pred))\n    print ('mse:',mean_squared_error(y_test, pred))\n    print('Rmse:', np.sqrt(mean_squared_error(y_test, pred)))\n    print('*'*20)\n    print('best params:',clf.best_params_)\n    print('best grid:', clf.best_estimator_)\n    print('^'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print('#'*20)\n\n\nif __name__ == '__main__':\n    # get data\n    from combine_data import join_data as data\n    data = data()\n    # run suport vector regression model\n    sup_vec_regress(data)\n"""
src/svr_model.py,3,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print(\'Model Performance Indicators\')\n    print(\'MAE: {:0.3f}\'.format(mae))\n    print(\'MAPE: {:0.3f}\'.format(mape))\n    print(\'Accuracy = {:0.3f}%\'.format(accuracy))\n\n    print(\'RMSE (test):\', rmse_test)\n    print(\'RMSE (train):\', rmse_train)\n    if rmse_test > rmse_train:\n        print(\'Overfit\')\n    else:\n        print(\'Underfit\')\n    return accuracy\n\n\ndef train_model():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    X_test, y_test = remove_county_state(X_test, y_test)\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             SVR())\n\n    # set hyperparameters\n    hyperparameters = { \'svr__kernel\': [\'linear\', \'poly\', \'sigmoid\', \'rbf\'],\n                        \'svr__C\': [0.5, 5, 10, 15, 20, 50, 100, 200, 300, 500, 1000],\n                        \'svr__epsilon\': [0.1, 1.5, 2, 5, 10, 30, 50],\n                       }\n \n\n    # tune model via pipeline\n    clf = GridSearchCV(pipeline, hyperparameters, cv=3)\n\n    clf.fit(X_train, y_train)\n\n    pred = clf.predict(X_test)\n    # print(\'feature importances:\', clf.feature_importances_)\n    print (\'r2 score:\',r2_score(y_test, pred))\n    print (\'mse:\',mean_squared_error(y_test, pred))\n    print(\'*\'*20)\n    print(\'best params:\',clf.best_params_)\n    print(\'best grid:\', clf.best_estimator_)\n    print(\'^\'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print(\'#\'*20)\n    print(\'score\', clf.score)\n    return clf\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    return X_train.columns\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(data_nas):\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n    data_nas = clean_data(data)\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\nif __name__ == \'__main__\':\n    train_model()\n'"
src/svr_model_hot_one.py,3,"b'import numpy as np\nimport pandas as pd\nimport os\nimport statsmodels.api as sm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR as SVR\nfrom sklearn.linear_model import ElasticNet as EN\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nimport matplotlib.pyplot as plt\n\ndef calc_rmse(yhat, y):\n    return np.sqrt(((yhat-y)**2).mean())\n\ndef calc_mae(yhat, y):\n    return np.mean(abs(yhat-y))\n\ndef calc_mape(yhat, y):\n    return 100*np.mean(abs((yhat-y)/y))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    ypred = model.predict(X_test)\n    ytrainpred = model.predict(X_train)\n\n    mae = calc_mae(ypred, y_test)\n    mape = calc_mape(ypred, y_test)\n    accuracy = 100 - mape\n\n    rmse_test   = calc_rmse(ypred, y_test)\n    rmse_train  = calc_rmse(ytrainpred, y_train)\n\n    print(\'Model Performance Indicators\')\n    print(\'MAE: {:0.3f}\'.format(mae))\n    print(\'MAPE: {:0.3f}\'.format(mape))\n    print(\'Accuracy = {:0.3f}%\'.format(accuracy))\n\n    print(\'RMSE (test):\', rmse_test)\n    print(\'RMSE (train):\', rmse_train)\n    if rmse_test > rmse_train:\n        print(\'Overfit\')\n    else:\n        print(\'Underfit\')\n    return accuracy\n\n\ndef train_model():\n    data = get_data()\n    data[\'co\'] = data.state == \'colorado\'\n    data[\'fl\'] = data.state == \'florida\'\n    data[\'nj\'] = data.state == \'new jersey\'\n    data[\'ca\'] = data.state == \'california\'\n    print(data.shape)\n\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    X_test, y_test = remove_county_state(X_test, y_test)\n\n    # data preprocessing (removing mean and scaling to unit variance with StandardScaler)\n    pipeline = make_pipeline(StandardScaler(),\n                             SVR())\n\n    # set hyperparameters\n    hyperparameters = { \'svr__kernel\': [\'linear\', \'poly\', \'sigmoid\', \'rbf\'],\n                        \'svr__C\': [0.5, 5, 10, 15, 20, 50, 100, 200, 300, 500, 1000],\n                        \'svr__epsilon\': [0.1, 1.5, 2, 5, 10, 30, 50],\n                       }\n\n\n    # tune model via pipeline\n    clf = GridSearchCV(pipeline, hyperparameters, cv=3)\n\n    clf.fit(X_train, y_train)\n\n    pred = clf.predict(X_test)\n    # print(\'feature importances:\', clf.feature_importances_)\n    print (\'r2 score:\',r2_score(y_test, pred))\n    print (\'mse:\',mean_squared_error(y_test, pred))\n    print(\'*\'*20)\n    print(\'best params:\',clf.best_params_)\n    print(\'best grid:\', clf.best_estimator_)\n    print(\'^\'*20)\n    eval_model(clf.best_estimator_, X_train, y_train, X_test, y_test)\n    print(\'#\'*20)\n    print(\'score\', clf.score)\n    return clf\n\ndef show_columns():\n    data = get_data()\n    X_train, X_test, y_train, y_test = split_data(data)\n    X_train, y_train = remove_county_state(X_train, y_train)\n    return X_train.columns\n\n\ndef get_data():\n    csv_file_path = \'../data/the_data_file.csv\'\n\n    if os.path.exists(csv_file_path):\n        print(""Data file found, loading data..."")\n        with open(csv_file_path, ""r"") as f:\n            data = pd.read_csv(f)\n\n    else:\n        print(""Data file not found, assembling dataset..."")\n        from data import join_data as data\n        data, labels = data()\n        data.to_csv(csv_file_path, index=False)\n\n    return data\n\ndef clean_data(data):\n    # drop problematic row with zero for asthma_rate\n    data = data.drop([141, 142, 149, 153, 158])\n    data_nas = data.fillna(0)\n    return data_nas\n\ndef X_y(data_nas):\n    X = data_nas.drop(\'asthma_rate\', axis=1)\n    y = data_nas.asthma_rate\n    return X, y\n\ndef county_data(county):\n    data = get_data()\n    data_nas = clean_data(data)\n    single_county = data_nas[data_nas[\'county\'] == county]\n    X, y = X_y(single_county)\n    return X, y\n\ndef split_data(data):\n    cln_data = clean_data(data)\n    X, y = X_y(cln_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n    return X_train, X_test, y_train, y_test\n\ndef remove_county_state(X, y):\n    X = X.drop([\'county\', \'state\'], axis=1)\n    return X, y\n\nif __name__ == \'__main__\':\n    train_model()\n'"
src/us_data.py,0,"b'import numpy as np\nimport pandas as pd\n\n# source data: http://www.countyhealthrankings.org/rankings/data\n\ndef get_data():\n\n    # def socio_econ_data():\n    # read in excel file\n    xls = pd.ExcelFile(\'../data/2017CountyHealthRankingsData.xls\')\n    # select tab with data and create new df\n    df = pd.read_excel(xls, \'Ranked Measure Data\')\n    # get: state, county, adult smoking, adult obesity, uninsured, PCP (doctors) rate, high school graduation, unemployment, income inequality, air pollution,\n    df = df.iloc[:, [1,2,27,31,63,68,95,105,116,135]]\n    # change column names\n    df.columns = [\'state\', \'county\',\'smoke_adult\', \'obese_adult\', \'uninsured\', \'pcp\', \'high_sch_grad\', \'unemployment\', \'income_ineq\', \'air_poll_partic\']\n    # drop first row (previously header)\n    df = df.drop([0])\n    # reset index and drop the old index as column\n    df.reset_index(level=0, drop=True, inplace=True)\n\n    # make county and state columns lowercase\n    df.county = df.county.str.lower()\n    df.state = df.state.str.lower()\n\n    # make all other columns numerical (this will enable .describe() for each column)\n    df_num = df.iloc[:, 2:10].apply(pd.to_numeric)\n\n    # join back up all columns in a way that avoids numeric columns becoming all ""NaN""\n    dfc = pd.concat([df.state.reset_index(drop=True), df.county.reset_index(drop=True), df_num.reset_index(drop=True)], axis=1)\n\n    # combine for all states selected\n    cocanjfl = dfc[(dfc[\'state\'] == \'colorado\') | (dfc[\'state\'] == \'california\') | (dfc[\'state\'] == \'new jersey\') | (dfc[\'state\'] == \'florida\')]\n    cocafl = dfc[(dfc[\'state\'] == \'colorado\') | (dfc[\'state\'] == \'california\') | (dfc[\'state\'] == \'florida\')]\n    cocanj = dfc[(dfc[\'state\'] == \'colorado\') | (dfc[\'state\'] == \'california\') | (dfc[\'state\'] == \'new jersey\') ]\n    coca = dfc[(dfc[\'state\'] == \'colorado\') | (dfc[\'state\'] == \'california\') ]\n\n    return coca\n'"
