file_path,api_count,code
example1.py,24,"b'#####################\n###   BASE LINE   ###\n#####################\n\nimport numpy as np\nimport mnist_loader\n\n### Data Loading\n\ntraining_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n\n### Parameters\n\nn_epoch = 30\nlearning_rate = 3\nbatch_size = 10\n\n### Network Architecture\n\nn_node_input = 784\nn_node_hidden = 100\nn_node_output = 10\n\n### Weight & Bias\n\nW2=np.random.randn(n_node_hidden, n_node_input)\nb2=np.random.randn(n_node_hidden, 1)\n\nW3=np.random.randn(n_node_output, n_node_hidden)\nb3=np.random.randn(n_node_output, 1)\n\n### Activation Functions\n\ndef sigmoid(z):\n    """"""The sigmoid function.""""""\n    return 1.0/(1.0+np.exp(-z))\n\ndef sigmoid_prime(z):\n    """"""Derivative of the sigmoid function.""""""\n    return sigmoid(z)*(1-sigmoid(z))\n\n### Training\ntest_errors = []\ntraining_errors = []\nn = len(training_data)\n\nfile_name_common = \'mse\'+\'_nHidden\'+str(n_node_hidden)+\'.txt\'\n\ntry:\n    training_errors = np.loadtxt(fname=\'tr_\'+file_name_common)\n    test_errors = np.loadtxt(fname=\'test_\'+file_name_common)\nexcept:\n    for j in range(n_epoch):\n\n        ## Stochastic Gradient Descent\n        np.random.shuffle(training_data)\n\n        # for each batch\n        sum_of_training_error = 0\n        for k in range(0, n, batch_size):\n            batch = training_data[k:k+batch_size]\n\n            # average gradient for samples in a batch\n            sum_gradient_b3 = 0\n            sum_gradient_b2 = 0\n            sum_gradient_W3 = 0\n            sum_gradient_W2 = 0\n\n            # for each sample\n            for x, y in batch:\n                ## Feed forward\n\n                a1 = x\n                z2 = np.dot(W2, a1) + b2\n                a2 = sigmoid(z2)\n                z3 = np.dot(W3, a2) + b3\n                a3 = sigmoid(z3)\n\n                ## Backpropagation\n\n                # Step 1: Error at the output layer [Quadratic Cost]\n                delta_3 = (a3-y)*sigmoid_prime(z3)\n                # Step 2: Error relationship between two adjacent layers\n                delta_2 =  sigmoid_prime(z2)*np.dot(W3.transpose(), delta_3)\n                # Step 3: Gradient of C in terms of bias\n                gradient_b3 = delta_3\n                gradient_b2 = delta_2\n                # Step 4: Gradient of C in terms of weight\n                gradient_W3 = np.dot(delta_3, a2.transpose())\n                gradient_W2 = np.dot(delta_2, a1.transpose())\n\n                # update gradients\n                sum_gradient_b3 += gradient_b3\n                sum_gradient_b2 += gradient_b2\n                sum_gradient_W3 += gradient_W3\n                sum_gradient_W2 += gradient_W2\n\n                ## Training Error\n                sum_of_training_error += int(np.argmax(a3) != np.argmax(y))\n\n\n            # update weights & biases\n            b3 -= learning_rate * sum_gradient_b3 / batch_size\n            b2 -= learning_rate * sum_gradient_b2 / batch_size\n            W3 -= learning_rate * sum_gradient_W3 / batch_size\n            W2 -= learning_rate * sum_gradient_W2 / batch_size\n\n        # Report Training Error\n        print(""[TRAIN_ERROR] Epoch %02d: %5d / %05d"" % (j, sum_of_training_error, n))\n        training_errors.append(np.float(sum_of_training_error) / n)\n\n        ### Test\n        n_test = len(test_data)\n        sum_of_test_error = 0\n        for x, y in test_data:\n            ## Feed forward\n\n            a1 = x\n            z2 = np.dot(W2, a1) + b2\n            a2 = sigmoid(z2)\n            z3 = np.dot(W3, a2) + b3\n            a3 = sigmoid(z3)\n\n            ## Test Error\n            # in test data, label info is a number not one-hot vector as in training data\n            sum_of_test_error += int(np.argmax(a3) != y)\n\n        # Report Test Error\n        print(""[ TEST_ERROR] Epoch %02d: %5d / %05d"" % (j, sum_of_test_error, n_test))\n\n        test_errors.append(np.float(sum_of_test_error)/n_test)\n\n    ## Save Results\n\n    np.savetxt(\'tr_\'+file_name_common, np.array(training_errors), fmt=\'%.5f\')\n    np.savetxt(\'test_\'+file_name_common, np.array(test_errors), fmt=\'%.5f\')\n\n### Plot results\n\nimport matplotlib.pyplot as plt\nidx = np.arange(1,n_epoch+1)\n\nplt.plot(idx,np.array(test_errors)*100,\'ro-\', label=\'test error\')\nplt.plot(idx,np.array(training_errors)*100,\'bo-\', label=\'training error\')\nlegend = plt.legend(loc=\'upper center\', shadow=True)\nfont = {\'family\' : \'normal\',\n        \'weight\' : \'bold\',\n        \'size\'   : 15}\nplt.rc(\'font\', **font)\nplt.xlabel(\'Epoch\', fontsize=22)\nplt.ylabel(\'Error rate [%]\', fontsize=22)\nplt.grid(True)\nplt.show()'"
example2.py,25,"b'#########################################\n###   BASE LINE + Cross Entropy Loss  ###\n#########################################\n\nimport numpy as np\nimport mnist_loader\n\n### Data Loading\n\ntraining_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n\n### Parameters\n\nn_epoch = 30\nlearning_rate = 0.5\nbatch_size = 10\n\n### Network Architecture\n\nn_node_input = 784\nn_node_hidden = 100\nn_node_output = 10\n\n### Weight & Bias\n\nW2=np.random.randn(n_node_hidden, n_node_input)\nb2=np.random.randn(n_node_hidden, 1)\n\nW3=np.random.randn(n_node_output, n_node_hidden)\nb3=np.random.randn(n_node_output, 1)\n\n### Activation Functions\n\ndef sigmoid(z):\n    """"""The sigmoid function.""""""\n    return 1.0/(1.0+np.exp(-z))\n\ndef sigmoid_prime(z):\n    """"""Derivative of the sigmoid function.""""""\n    return sigmoid(z)*(1-sigmoid(z))\n\n### Training\ntest_errors = []\ntraining_errors = []\nn = len(training_data)\n\nfile_name_common = \'ce\'+\'_nHidden\'+str(n_node_hidden)+\'.txt\'\n\ntry:\n    training_errors = np.loadtxt(fname=\'tr_\'+file_name_common)\n    test_errors = np.loadtxt(fname=\'test_\'+file_name_common)\nexcept:\n    for j in range(n_epoch):\n\n        ## Stochastic Gradient Descent\n        np.random.shuffle(training_data)\n\n        # for each batch\n        sum_of_training_error = 0\n        for k in range(0, n, batch_size):\n            batch = training_data[k:k+batch_size]\n\n            # average gradient for samples in a batch\n            sum_gradient_b3 = 0\n            sum_gradient_b2 = 0\n            sum_gradient_W3 = 0\n            sum_gradient_W2 = 0\n\n            # for each sample\n            for x, y in batch:\n                ## Feed forward\n\n                a1 = x\n                z2 = np.dot(W2, a1) + b2\n                a2 = sigmoid(z2)\n                z3 = np.dot(W3, a2) + b3\n                a3 = sigmoid(z3)\n\n                ## Backpropagation\n\n                # Step 1: Error at the output layer [Cross-Entropy Cost]\n                delta_3 = (a3-y)\n                # Step 2: Error relationship between two adjacent layers\n                delta_2 =  sigmoid_prime(z2)*np.dot(W3.transpose(), delta_3)\n                # Step 3: Gradient of C in terms of bias\n                gradient_b3 = delta_3\n                gradient_b2 = delta_2\n                # Step 4: Gradient of C in terms of weight\n                gradient_W3 = np.dot(delta_3, a2.transpose())\n                gradient_W2 = np.dot(delta_2, a1.transpose())\n\n                # update gradients\n                sum_gradient_b3 += gradient_b3\n                sum_gradient_b2 += gradient_b2\n                sum_gradient_W3 += gradient_W3\n                sum_gradient_W2 += gradient_W2\n\n                ## Training Error\n                sum_of_training_error += int(np.argmax(a3) != np.argmax(y))\n\n            # update weights & biases\n            b3 -= learning_rate * sum_gradient_b3 / batch_size\n            b2 -= learning_rate * sum_gradient_b2 / batch_size\n            W3 -= learning_rate * sum_gradient_W3 / batch_size\n            W2 -= learning_rate * sum_gradient_W2 / batch_size\n\n        # Report Training Error\n        print(""[TRAIN_ERROR] Epoch %02d: %5d / %05d"" % (j, sum_of_training_error, n))\n        training_errors.append(np.float(sum_of_training_error) / n)\n\n        ### Test\n        n_test = len(test_data)\n        sum_of_test_error = 0\n        for x, y in test_data:\n            ## Feed forward\n\n            a1 = x\n            z2 = np.dot(W2, a1) + b2\n            a2 = sigmoid(z2)\n            z3 = np.dot(W3, a2) + b3\n            a3 = sigmoid(z3)\n\n            ## Test Error\n            # in test data, label info is a number not one-hot vector as in training data\n            sum_of_test_error += int(np.argmax(a3) != y)\n\n        # Report Test Error\n        print(""[ TEST_ERROR] Epoch %02d: %5d / %05d"" % (j, sum_of_test_error, n_test))\n\n        test_errors.append(np.float(sum_of_test_error)/n_test)\n\n    ## Save Results\n    np.savetxt(\'tr_\'+file_name_common, np.array(training_errors), fmt=\'%.5f\')\n    np.savetxt(\'test_\'+file_name_common, np.array(test_errors), fmt=\'%.5f\')\n\n### Plot results\nimport matplotlib.pyplot as plt\nidx = np.arange(1,n_epoch+1)\n\nplt.plot(idx, np.array(test_errors)*100,\'ro-\', label=\'cross-entropy\')\ntry:\n    # Load baseline\n    file_name_common = \'mse\'+\'_nHidden\'+str(n_node_hidden)+\'.txt\'\n    mse = np.loadtxt(fname=\'test_\'+file_name_common)\n    plt.plot(idx,np.array(mse)*100,\'bo-\', label=\'quadratic\')\nexcept:\n    print (\'There is no result of baseline\')\n\nlegend = plt.legend(loc=\'upper center\', shadow=True)\nfont = {\'family\' : \'normal\',\n        \'weight\' : \'bold\',\n        \'size\'   : 15}\nplt.rc(\'font\', **font)\nplt.xlabel(\'Epoch\', fontsize=22)\nplt.ylabel(\'Test error rate [%]\', fontsize=22)\nplt.grid(True)\nplt.show()'"
example3.py,25,"b'#############################################################\n###   BASE LINE + Cross Entropy Loss  + L2 Regularization ###\n#############################################################\n\nimport numpy as np\nimport mnist_loader\n\n### Data Loading\n\ntraining_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n\n### Parameters\n\nn_epoch = 30\nlearning_rate = 0.5\nbatch_size = 10\nlamda = 5\n\n### Network Architecture\n\nn_node_input = 784\nn_node_hidden = 30\nn_node_output = 10\n\n### Weight & Bias\n\nW2=np.random.randn(n_node_hidden, n_node_input)\nb2=np.random.randn(n_node_hidden, 1)\n\nW3=np.random.randn(n_node_output, n_node_hidden)\nb3=np.random.randn(n_node_output, 1)\n\n### Activation Functions\n\ndef sigmoid(z):\n    """"""The sigmoid function.""""""\n    return 1.0/(1.0+np.exp(-z))\n\ndef sigmoid_prime(z):\n    """"""Derivative of the sigmoid function.""""""\n    return sigmoid(z)*(1-sigmoid(z))\n\n### Training\ntest_errors = []\ntraining_errors = []\nn = len(training_data)\n\nfile_name_common = \'l2reg_ce\'+\'_nHidden\'+str(n_node_hidden)+\'.txt\'\n\ntry:\n    training_errors = np.loadtxt(fname=\'tr_\'+file_name_common)\n    test_errors = np.loadtxt(fname=\'test_\'+file_name_common)\nexcept:\n    for j in range(n_epoch):\n\n        ## Stochastic Gradient Descent\n        np.random.shuffle(training_data)\n\n        # for each batch\n        sum_of_training_error = 0\n        for k in range(0, n, batch_size):\n            batch = training_data[k:k+batch_size]\n\n            # average gradient for samples in a batch\n            sum_gradient_b3 = 0\n            sum_gradient_b2 = 0\n            sum_gradient_W3 = 0\n            sum_gradient_W2 = 0\n\n            # for each sample\n            for x, y in batch:\n                ## Feed forward\n\n                a1 = x\n                z2 = np.dot(W2, a1) + b2\n                a2 = sigmoid(z2)\n                z3 = np.dot(W3, a2) + b3\n                a3 = sigmoid(z3)\n\n                ## Backpropagation\n\n                # Step 1: Error at the output layer [Cross-Entropy Cost]\n                delta_3 = (a3-y)\n                # Step 2: Error relationship between two adjacent layers\n                delta_2 =  sigmoid_prime(z2)*np.dot(W3.transpose(), delta_3)\n                # Step 3: Gradient of C in terms of bias\n                gradient_b3 = delta_3\n                gradient_b2 = delta_2\n                # Step 4: Gradient of C in terms of weight\n                gradient_W3 = np.dot(delta_3, a2.transpose())\n                gradient_W2 = np.dot(delta_2, a1.transpose())\n\n                # update gradients\n                sum_gradient_b3 += gradient_b3\n                sum_gradient_b2 += gradient_b2\n                sum_gradient_W3 += gradient_W3\n                sum_gradient_W2 += gradient_W2\n\n                ## Training Error\n                sum_of_training_error += int(np.argmax(a3) != np.argmax(y))\n\n            # Update Biases\n            b3 -= learning_rate * sum_gradient_b3 / batch_size\n            b2 -= learning_rate * sum_gradient_b2 / batch_size\n\n            # Update Weights\n            # L2 regularization\n            W3 *= 1 - learning_rate * lamda / n\n            W2 *= 1 - learning_rate * lamda / n\n            # update\n            W3 -= learning_rate * sum_gradient_W3 / batch_size\n            W2 -= learning_rate * sum_gradient_W2 / batch_size\n\n        # Report Training Error\n        print(""[TRAIN_ERROR] Epoch %02d: %5d / %05d"" % (j, sum_of_training_error, n))\n        training_errors.append(np.float(sum_of_training_error) / n)\n\n        ### Test\n        n_test = len(test_data)\n        sum_of_test_error = 0\n        for x, y in test_data:\n            ## Feed forward\n\n            a1 = x\n            z2 = np.dot(W2, a1) + b2\n            a2 = sigmoid(z2)\n            z3 = np.dot(W3, a2) + b3\n            a3 = sigmoid(z3)\n\n            ## Test Error\n            # in test data, label info is a number not one-hot vector as in training data\n            sum_of_test_error += int(np.argmax(a3) != y)\n\n        # Report Test Error\n        print(""[ TEST_ERROR] Epoch %02d: %5d / %05d"" % (j, sum_of_test_error, n_test))\n\n        test_errors.append(np.float(sum_of_test_error)/n_test)\n\n    ## Save Results\n    np.savetxt(\'tr_\'+file_name_common, np.array(training_errors), fmt=\'%.5f\')\n    np.savetxt(\'test_\'+file_name_common, np.array(test_errors), fmt=\'%.5f\')\n\n### Plot results\nimport matplotlib.pyplot as plt\nidx = np.arange(1,n_epoch+1)\n\nplt.plot(idx, np.array(test_errors)*100,\'ro-\', label=\'with L2 regularization\')\ntry:\n    # Load baseline\n    file_name_common = \'ce\'+\'_nHidden\'+str(n_node_hidden)+\'.txt\'\n    mse = np.loadtxt(fname=\'test_\'+file_name_common)\n    plt.plot(idx,np.array(mse)*100,\'bo-\', label=\'without L2 regularization\')\nexcept:\n    print (\'There is no result of baseline\')\n\nlegend = plt.legend(loc=\'upper center\', shadow=True)\nfont = {\'family\' : \'normal\',\n        \'weight\' : \'bold\',\n        \'size\'   : 15}\nplt.rc(\'font\', **font)\nplt.xlabel(\'Epoch\', fontsize=22)\nplt.ylabel(\'Test error rate [%]\', fontsize=22)\nplt.grid(True)\nplt.show()'"
example4.py,27,"b'#############################################################\n###   BASE LINE + Cross Entropy Loss  + L1 Regularization ###\n#############################################################\n\nimport numpy as np\nimport mnist_loader\n\n### Data Loading\n\ntraining_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n\n### Parameters\n\nn_epoch = 30\nlearning_rate = 0.5\nbatch_size = 10\nlamda = 5\n\n### Network Architecture\n\nn_node_input = 784\nn_node_hidden = 30\nn_node_output = 10\n\n### Weight & Bias\n\nW2=np.random.randn(n_node_hidden, n_node_input)\nb2=np.random.randn(n_node_hidden, 1)\n\nW3=np.random.randn(n_node_output, n_node_hidden)\nb3=np.random.randn(n_node_output, 1)\n\n### Activation Functions\n\ndef sigmoid(z):\n    """"""The sigmoid function.""""""\n    return 1.0/(1.0+np.exp(-z))\n\ndef sigmoid_prime(z):\n    """"""Derivative of the sigmoid function.""""""\n    return sigmoid(z)*(1-sigmoid(z))\n\n### Training\ntest_errors = []\ntraining_errors = []\nn = len(training_data)\n\nfile_name_common = \'l1reg_ce\'+\'_nHidden\'+str(n_node_hidden)+\'.txt\'\n\ntry:\n    training_errors = np.loadtxt(fname=\'tr_\'+file_name_common)\n    test_errors = np.loadtxt(fname=\'test_\'+file_name_common)\nexcept:\n    for j in range(n_epoch):\n\n        ## Stochastic Gradient Descent\n        np.random.shuffle(training_data)\n\n        # for each batch\n        sum_of_training_error = 0\n        for k in range(0, n, batch_size):\n            batch = training_data[k:k+batch_size]\n\n            # average gradient for samples in a batch\n            sum_gradient_b3 = 0\n            sum_gradient_b2 = 0\n            sum_gradient_W3 = 0\n            sum_gradient_W2 = 0\n\n            # for each sample\n            for x, y in batch:\n                ## Feed forward\n\n                a1 = x\n                z2 = np.dot(W2, a1) + b2\n                a2 = sigmoid(z2)\n                z3 = np.dot(W3, a2) + b3\n                a3 = sigmoid(z3)\n\n                ## Backpropagation\n\n                # Step 1: Error at the output layer [Cross-Entropy Cost]\n                delta_3 = (a3-y)\n                # Step 2: Error relationship between two adjacent layers\n                delta_2 =  sigmoid_prime(z2)*np.dot(W3.transpose(), delta_3)\n                # Step 3: Gradient of C in terms of bias\n                gradient_b3 = delta_3\n                gradient_b2 = delta_2\n                # Step 4: Gradient of C in terms of weight\n                gradient_W3 = np.dot(delta_3, a2.transpose())\n                gradient_W2 = np.dot(delta_2, a1.transpose())\n\n                # update gradients\n                sum_gradient_b3 += gradient_b3\n                sum_gradient_b2 += gradient_b2\n\n                sum_gradient_W3 += gradient_W3\n                sum_gradient_W2 += gradient_W2\n\n                ## Training Error\n                sum_of_training_error += int(np.argmax(a3) != np.argmax(y))\n\n            # Update Biases\n            b3 -= learning_rate * sum_gradient_b3 / batch_size\n            b2 -= learning_rate * sum_gradient_b2 / batch_size\n\n            # Update Weights\n            # L1 regularization\n            W3 -= (learning_rate * lamda / n)*np.sign(W3)\n            W2 -= (learning_rate * lamda / n)*np.sign(W2)\n            # update\n            W3 -= learning_rate * sum_gradient_W3 / batch_size\n            W2 -= learning_rate * sum_gradient_W2 / batch_size\n\n        # Report Training Error\n        print(""[TRAIN_ERROR] Epoch %02d: %5d / %05d"" % (j, sum_of_training_error, n))\n        training_errors.append(np.float(sum_of_training_error) / n)\n\n        ### Test\n        n_test = len(test_data)\n        sum_of_test_error = 0\n        for x, y in test_data:\n            ## Feed forward\n\n            a1 = x\n            z2 = np.dot(W2, a1) + b2\n            a2 = sigmoid(z2)\n            z3 = np.dot(W3, a2) + b3\n            a3 = sigmoid(z3)\n\n            ## Test Error\n            # in test data, label info is a number not one-hot vector as in training data\n            sum_of_test_error += int(np.argmax(a3) != y)\n\n        # Report Test Error\n        print(""[ TEST_ERROR] Epoch %02d: %5d / %05d"" % (j, sum_of_test_error, n_test))\n\n        test_errors.append(np.float(sum_of_test_error)/n_test)\n\n    ## Save Results\n    np.savetxt(\'tr_\'+file_name_common, np.array(training_errors), fmt=\'%.5f\')\n    np.savetxt(\'test_\'+file_name_common, np.array(test_errors), fmt=\'%.5f\')\n\n### Plot results\nimport matplotlib.pyplot as plt\nidx = np.arange(1,n_epoch+1)\n\nplt.plot(idx, np.array(test_errors)*100,\'ro-\', label=\'with L1 regularization\')\ntry:\n    # Load baseline\n    file_name_common = \'ce\'+\'_nHidden\'+str(n_node_hidden)+\'.txt\'\n    mse = np.loadtxt(fname=\'test_\'+file_name_common)\n    plt.plot(idx,np.array(mse)*100,\'bo-\', label=\'without L1 regularization\')\nexcept:\n    print (\'There is no result of baseline\')\n\nlegend = plt.legend(loc=\'upper center\', shadow=True)\nfont = {\'family\' : \'normal\',\n        \'weight\' : \'bold\',\n        \'size\'   : 15}\nplt.rc(\'font\', **font)\nplt.xlabel(\'Epoch\', fontsize=22)\nplt.ylabel(\'Test error rate [%]\', fontsize=22)\nplt.grid(True)\nplt.show()'"
example5.py,26,"b'###################################################\n###   BASE LINE + Cross Entropy Loss  + Dropout ###\n###################################################\n\nimport numpy as np\nimport mnist_loader\n\n### Data Loading\n\ntraining_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n\n### Parameters\n\nn_epoch = 30\nlearning_rate = 0.5\nbatch_size = 10\nlamda = 0   # regularization parameter\np = 0.9     # dropout\n\n### Network Architecture\n\nn_node_input = 784\nn_node_hidden = 30\nn_node_output = 10\n\n### Weight & Bias\n\nW2=np.random.randn(n_node_hidden, n_node_input)\nb2=np.random.randn(n_node_hidden, 1)\n\nW3=np.random.randn(n_node_output, n_node_hidden)\nb3=np.random.randn(n_node_output, 1)\n\n### Activation Functions\n\ndef sigmoid(z):\n    """"""The sigmoid function.""""""\n    return 1.0/(1.0+np.exp(-z))\n\ndef sigmoid_prime(z):\n    """"""Derivative of the sigmoid function.""""""\n    return sigmoid(z)*(1-sigmoid(z))\n\n### Training\ntest_errors = []\ntraining_errors = []\nn = len(training_data)\n\nfile_name_common = \'do_ce\'+\'_nHidden\'+str(n_node_hidden)+\'.txt\'\n\ntry:\n    training_errors = np.loadtxt(fname=\'tr_\'+file_name_common)\n    test_errors = np.loadtxt(fname=\'test_\'+file_name_common)\nexcept:\n    for j in range(n_epoch):\n\n        ## Stochastic Gradient Descent\n        np.random.shuffle(training_data)\n\n        # for each batch\n        sum_of_training_error = 0\n        for k in range(0, n, batch_size):\n            batch = training_data[k:k+batch_size]\n\n            # average gradient for samples in a batch\n            sum_gradient_b3 = 0\n            sum_gradient_b2 = 0\n            sum_gradient_W3 = 0\n            sum_gradient_W2 = 0\n\n            # for each sample\n            for x, y in batch:\n                ## Feed forward\n\n                a1 = x\n                z2 = np.dot(W2, a1) + b2\n                a2 = sigmoid(z2)\n                m2 = (np.random.rand(*a2.shape) < p) / p\n                a2 *= m2\n\n                z3 = np.dot(W3, a2) + b3\n                a3 = sigmoid(z3)\n\n                ## Backpropagation\n\n                # Step 1: Error at the output layer [Cross-Entropy Cost]\n                delta_3 = (a3-y)\n                # Step 2: Error relationship between two adjacent layers\n                delta_2 =  sigmoid_prime(z2)*np.dot(W3.transpose(), delta_3)\n                delta_2 *= m2 # due to dropout\n                # Step 3: Gradient of C in terms of bias\n                gradient_b3 = delta_3\n                gradient_b2 = delta_2\n                # Step 4: Gradient of C in terms of weight\n                gradient_W3 = np.dot(delta_3, a2.transpose())\n                gradient_W2 = np.dot(delta_2, a1.transpose())\n\n                # update gradients\n                sum_gradient_b3 += gradient_b3\n                sum_gradient_b2 += gradient_b2\n\n                sum_gradient_W3 += gradient_W3\n                sum_gradient_W2 += gradient_W2\n\n                ## Training Error\n                sum_of_training_error += int(np.argmax(a3) != np.argmax(y))\n\n            # Update Weights & Biases\n            b3 -= learning_rate * sum_gradient_b3 / batch_size\n            b2 -= learning_rate * sum_gradient_b2 / batch_size\n\n            # Update Weights\n            # L2 regularization\n            W3 *= 1 - learning_rate * lamda / n\n            W2 *= 1 - learning_rate * lamda / n\n            # update\n            W3 -= learning_rate * sum_gradient_W3 / batch_size\n            W2 -= learning_rate * sum_gradient_W2 / batch_size\n\n        # Report Training Error\n        print(""[TRAIN_ERROR] Epoch %02d: %5d / %05d"" % (j, sum_of_training_error, n))\n        training_errors.append(np.float(sum_of_training_error) / n)\n\n        ### Test\n        n_test = len(test_data)\n        sum_of_test_error = 0\n        for x, y in test_data:\n            ## Feed forward\n\n            a1 = x\n            z2 = np.dot(W2, a1) + b2\n            a2 = sigmoid(z2)\n            z3 = np.dot(W3, a2) + b3\n            a3 = sigmoid(z3)\n\n            ## Test Error\n            # in test data, label info is a number not one-hot vector as in training data\n            sum_of_test_error += int(np.argmax(a3) != y)\n\n        # Report Test Error\n        print(""[ TEST_ERROR] Epoch %02d: %5d / %05d"" % (j, sum_of_test_error, n_test))\n\n        test_errors.append(np.float(sum_of_test_error)/n_test)\n\n    ## Save Results\n    np.savetxt(\'tr_\'+file_name_common, np.array(training_errors), fmt=\'%.5f\')\n    np.savetxt(\'test_\'+file_name_common, np.array(test_errors), fmt=\'%.5f\')\n\n### Plot results\nimport matplotlib.pyplot as plt\nidx = np.arange(1,n_epoch+1)\n\nplt.plot(idx, np.array(test_errors)*100,\'ro-\', label=\'with Dropuout\')\ntry:\n    # Load baseline\n    file_name_common = \'ce\'+\'_nHidden\'+str(n_node_hidden)+\'.txt\'\n    mse = np.loadtxt(fname=\'test_\'+file_name_common)\n    plt.plot(idx,np.array(mse)*100,\'bo-\', label=\'without Dropout\')\nexcept:\n    print (\'There is no result of baseline\')\n\nlegend = plt.legend(loc=\'upper center\', shadow=True)\nfont = {\'family\' : \'normal\',\n        \'weight\' : \'bold\',\n        \'size\'   : 15}\nplt.rc(\'font\', **font)\nplt.xlabel(\'Epoch\', fontsize=22)\nplt.ylabel(\'Test error rate [%]\', fontsize=22)\nplt.grid(True)\nplt.show()'"
example6.py,31,"b'##############################################################\n###   BASE LINE + Cross Entropy Loss  + Xavier Initializer ###\n##############################################################\n\nimport numpy as np\nimport mnist_loader\n\n### Data Loading\n\ntraining_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n\n### Parameters\n\nn_epoch = 30\nlearning_rate = 0.5\nbatch_size = 10\n\nW_init = \'xavier\'\nb_init = \'zero\'\n\n### Network Architecture\n\nn_node_input = 784\nn_node_hidden = 100\nn_node_output = 10\n\n### Weight & Bias\nif W_init == \'normal\':\n    W2=np.random.randn(n_node_hidden, n_node_input)\n    W3 = np.random.randn(n_node_output, n_node_hidden)\nelif W_init == \'xavier\':\n    W2 = np.random.randn(n_node_hidden, n_node_input) / np.sqrt(n_node_input)\n    W3 = np.random.randn(n_node_output, n_node_hidden) / np.sqrt(n_node_hidden)\nelse : # \'he\'\n    W2 = np.random.randn(n_node_hidden, n_node_input) / np.sqrt(n_node_input/2)\n    W3 = np.random.randn(n_node_output, n_node_hidden) / np.sqrt(n_node_hidden/2)\n\nif b_init == \'normal\':\n    b2=np.random.randn(n_node_hidden, 1)\n    b3=np.random.randn(n_node_output, 1)\nelse: # \'zero\'\n    b2 = np.zeros([n_node_hidden, 1])\n    b3 = np.zeros([n_node_output, 1])\n\n### Activation Functions\n\ndef sigmoid(z):\n    """"""The sigmoid function.""""""\n    return 1.0/(1.0+np.exp(-z))\n\ndef sigmoid_prime(z):\n    """"""Derivative of the sigmoid function.""""""\n    return sigmoid(z)*(1-sigmoid(z))\n\n### Training\ntest_errors = []\ntraining_errors = []\nn = len(training_data)\n\nfile_name_common = \'XavierInit_ce\'+\'_nHidden\'+str(n_node_hidden)+\'.txt\'\n\ntry:\n    training_errors = np.loadtxt(fname=\'tr_\'+file_name_common)\n    test_errors = np.loadtxt(fname=\'test_\'+file_name_common)\nexcept:\n    for j in range(n_epoch):\n\n        ## Stochastic Gradient Descent\n        np.random.shuffle(training_data)\n\n        # for each batch\n        sum_of_training_error = 0\n        for k in range(0, n, batch_size):\n            batch = training_data[k:k+batch_size]\n\n            # average gradient for samples in a batch\n            sum_gradient_b3 = 0\n            sum_gradient_b2 = 0\n            sum_gradient_W3 = 0\n            sum_gradient_W2 = 0\n\n            # for each sample\n            for x, y in batch:\n                ## Feed forward\n\n                a1 = x\n                z2 = np.dot(W2, a1) + b2\n                a2 = sigmoid(z2)\n                z3 = np.dot(W3, a2) + b3\n                a3 = sigmoid(z3)\n\n                ## Backpropagation\n\n                # Step 1: Error at the output layer [Cross-Entropy Cost]\n                delta_3 = (a3-y)\n                # Step 2: Error relationship between two adjacent layers\n                delta_2 =  sigmoid_prime(z2)*np.dot(W3.transpose(), delta_3)\n                # Step 3: Gradient of C in terms of bias\n                gradient_b3 = delta_3\n                gradient_b2 = delta_2\n                # Step 4: Gradient of C in terms of weight\n                gradient_W3 = np.dot(delta_3, a2.transpose())\n                gradient_W2 = np.dot(delta_2, a1.transpose())\n\n                # update gradients\n                sum_gradient_b3 += gradient_b3\n                sum_gradient_b2 += gradient_b2\n                sum_gradient_W3 += gradient_W3\n                sum_gradient_W2 += gradient_W2\n\n                ## Training Error\n                sum_of_training_error += int(np.argmax(a3) != np.argmax(y))\n\n            # update weights & biases\n            b3 -= learning_rate * sum_gradient_b3 / batch_size\n            b2 -= learning_rate * sum_gradient_b2 / batch_size\n            W3 -= learning_rate * sum_gradient_W3 / batch_size\n            W2 -= learning_rate * sum_gradient_W2 / batch_size\n\n        # Report Training Error\n        print(""[TRAIN_ERROR] Epoch %02d: %5d / %05d"" % (j, sum_of_training_error, n))\n        training_errors.append(np.float(sum_of_training_error) / n)\n\n        ### Test\n        n_test = len(test_data)\n        sum_of_test_error = 0\n        for x, y in test_data:\n            ## Feed forward\n\n            a1 = x\n            z2 = np.dot(W2, a1) + b2\n            a2 = sigmoid(z2)\n            z3 = np.dot(W3, a2) + b3\n            a3 = sigmoid(z3)\n\n            ## Test Error\n            # in test data, label info is a number not one-hot vector as in training data\n            sum_of_test_error += int(np.argmax(a3) != y)\n\n        # Report Test Error\n        print(""[ TEST_ERROR] Epoch %02d: %5d / %05d"" % (j, sum_of_test_error, n_test))\n\n        test_errors.append(np.float(sum_of_test_error)/n_test)\n\n    ## Save Results\n    np.savetxt(\'tr_\'+file_name_common, np.array(training_errors), fmt=\'%.5f\')\n    np.savetxt(\'test_\'+file_name_common, np.array(test_errors), fmt=\'%.5f\')\n\n### Plot results\nimport matplotlib.pyplot as plt\nidx = np.arange(1,n_epoch+1)\n\nplt.plot(idx, np.array(test_errors)*100,\'ro-\', label=\'with Xaiver initialization\')\ntry:\n    # Load baseline\n    file_name_common = \'ce\'+\'_nHidden\'+str(n_node_hidden)+\'.txt\'\n    mse = np.loadtxt(fname=\'test_\'+file_name_common)\n    plt.plot(idx,np.array(mse)*100,\'bo-\', label=\'without Xaiver initialization\')\nexcept:\n    print (\'There is no result of baseline\')\n\nlegend = plt.legend(loc=\'upper center\', shadow=True)\nfont = {\'family\' : \'normal\',\n        \'weight\' : \'bold\',\n        \'size\'   : 15}\nplt.rc(\'font\', **font)\nplt.xlabel(\'Epoch\', fontsize=22)\nplt.ylabel(\'Test error rate [%]\', fontsize=22)\nplt.grid(True)\nplt.show()'"
example7.py,25,"b'##############################################################\n###   BASE LINE + Cross Entropy Loss  + Momentum based SGD ###\n##############################################################\n\nimport numpy as np\nimport mnist_loader\n\n### Data Loading\n\ntraining_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n\n### Parameters\n\nn_epoch = 30\nlearning_rate = 0.1\nbatch_size = 10\n\n# for momentum-based SGD\nmu = 0.9 # momentum\nv_b2 = 0\nv_b3 = 0\nv_W2 = 0\nv_W3 = 0\n\n### Network Architecture\n\nn_node_input = 784\nn_node_hidden = 100\nn_node_output = 10\n\n### Weight & Bias\n\nW2=np.random.randn(n_node_hidden, n_node_input)\nb2=np.random.randn(n_node_hidden, 1)\n\nW3=np.random.randn(n_node_output, n_node_hidden)\nb3=np.random.randn(n_node_output, 1)\n\n### Activation Functions\n\ndef sigmoid(z):\n    """"""The sigmoid function.""""""\n    return 1.0/(1.0+np.exp(-z))\n\ndef sigmoid_prime(z):\n    """"""Derivative of the sigmoid function.""""""\n    return sigmoid(z)*(1-sigmoid(z))\n\n### Training\ntest_errors = []\ntraining_errors = []\nn = len(training_data)\n\nfile_name_common = \'msgd_ce\'+\'_nHidden\'+str(n_node_hidden)+\'.txt\'\n\ntry:\n    training_errors = np.loadtxt(fname=\'tr_\'+file_name_common)\n    test_errors = np.loadtxt(fname=\'test_\'+file_name_common)\nexcept:\n    for j in range(n_epoch):\n\n        ## Stochastic Gradient Descent\n        np.random.shuffle(training_data)\n\n        # for each batch\n        sum_of_training_error = 0\n        for k in range(0, n, batch_size):\n            batch = training_data[k:k+batch_size]\n\n            # average gradient for samples in a batch\n            sum_gradient_b3 = 0\n            sum_gradient_b2 = 0\n            sum_gradient_W3 = 0\n            sum_gradient_W2 = 0\n\n            # for each sample\n            for x, y in batch:\n                ## Feed forward\n\n                a1 = x\n                z2 = np.dot(W2, a1) + b2\n                a2 = sigmoid(z2)\n                z3 = np.dot(W3, a2) + b3\n                a3 = sigmoid(z3)\n\n                ## Backpropagation\n\n                # Step 1: Error at the output layer [Cross-Entropy Cost]\n                delta_3 = (a3-y)\n                # Step 2: Error relationship between two adjacent layers\n                delta_2 =  sigmoid_prime(z2)*np.dot(W3.transpose(), delta_3)\n                # Step 3: Gradient of C in terms of bias\n                gradient_b3 = delta_3\n                gradient_b2 = delta_2\n                # Step 4: Gradient of C in terms of weight\n                gradient_W3 = np.dot(delta_3, a2.transpose())\n                gradient_W2 = np.dot(delta_2, a1.transpose())\n\n                # update gradients\n                sum_gradient_b3 += gradient_b3\n                sum_gradient_b2 += gradient_b2\n                sum_gradient_W3 += gradient_W3\n                sum_gradient_W2 += gradient_W2\n\n                ## Training Error\n                sum_of_training_error += int(np.argmax(a3) != np.argmax(y))\n\n            # update weights & biases via momentum-based SGD\n            v_b3 = mu * v_b3 - learning_rate * sum_gradient_b3 / batch_size\n            b3 += v_b3\n            v_b2 = mu*v_b2 - learning_rate * sum_gradient_b2 / batch_size\n            b2 += v_b2\n            v_W3 = mu * v_W3 - learning_rate * sum_gradient_W3 / batch_size\n            W3 += v_W3\n            v_W2 = mu * v_W2 - learning_rate * sum_gradient_W2 / batch_size\n            W2 += v_W2\n\n        # Report Training Error\n        print(""[TRAIN_ERROR] Epoch %02d: %5d / %05d"" % (j, sum_of_training_error, n))\n        training_errors.append(np.float(sum_of_training_error) / n)\n\n        ### Test\n        n_test = len(test_data)\n        sum_of_test_error = 0\n        for x, y in test_data:\n            ## Feed forward\n\n            a1 = x\n            z2 = np.dot(W2, a1) + b2\n            a2 = sigmoid(z2)\n            z3 = np.dot(W3, a2) + b3\n            a3 = sigmoid(z3)\n\n            ## Test Error\n            # in test data, label info is a number not one-hot vector as in training data\n            sum_of_test_error += int(np.argmax(a3) != y)\n\n        # Report Test Error\n        print(""[ TEST_ERROR] Epoch %02d: %5d / %05d"" % (j, sum_of_test_error, n_test))\n\n        test_errors.append(np.float(sum_of_test_error)/n_test)\n\n    ## Save Results\n    np.savetxt(\'tr_\'+file_name_common, np.array(training_errors), fmt=\'%.5f\')\n    np.savetxt(\'test_\'+file_name_common, np.array(test_errors), fmt=\'%.5f\')\n\n### Plot results\nimport matplotlib.pyplot as plt\nidx = np.arange(1,n_epoch+1)\n\nplt.plot(idx, np.array(test_errors)*100,\'ro-\', label=\'Momentum SGD\')\ntry:\n    # Load baseline\n    file_name_common = \'ce\'+\'_nHidden\'+str(n_node_hidden)+\'.txt\'\n    mse = np.loadtxt(fname=\'test_\'+file_name_common)\n    plt.plot(idx,np.array(mse)*100,\'bo-\', label=\'SGD\')\nexcept:\n    print (\'There is no result of baseline\')\n\nlegend = plt.legend(loc=\'upper center\', shadow=True)\nfont = {\'family\' : \'normal\',\n        \'weight\' : \'bold\',\n        \'size\'   : 15}\nplt.rc(\'font\', **font)\nplt.xlabel(\'Epoch\', fontsize=22)\nplt.ylabel(\'Test error rate [%]\', fontsize=22)\nplt.grid(True)\nplt.show()'"
example8.py,32,"b'#####################################################################\n###   BASE LINE + Cross Entropy Loss  + Xavier Initializer + ReLU ###\n#####################################################################\n\nimport numpy as np\nimport mnist_loader\n\n### Data Loading\n\ntraining_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n\n### Parameters\n\nn_epoch = 30\nlearning_rate = 0.03\nbatch_size = 10\n\nW_init = \'xavier\'\nb_init = \'zero\'\n\n### Network Architecture\n\nn_node_input = 784\nn_node_hidden = 30\nn_node_output = 10\n\n### Weight & Bias\nif W_init == \'normal\':\n    W2 = np.random.randn(n_node_hidden, n_node_input)\n    W3 = np.random.randn(n_node_output, n_node_hidden)\nelif W_init == \'xavier\':\n    W2 = np.random.randn(n_node_hidden, n_node_input) / np.sqrt(n_node_input)\n    W3 = np.random.randn(n_node_output, n_node_hidden) / np.sqrt(n_node_hidden)\nelse : # \'he\'\n    W2 = np.random.randn(n_node_hidden, n_node_input) / np.sqrt(n_node_input/2)\n    W3 = np.random.randn(n_node_output, n_node_hidden) / np.sqrt(n_node_hidden/2)\n\nif b_init == \'normal\':\n    b2 = np.random.randn(n_node_hidden, 1)\n    b3 = np.random.randn(n_node_output, 1)\nelse: # \'zero\'\n    b2 = np.zeros([n_node_hidden, 1])\n    b3 = np.zeros([n_node_output, 1])\n\n### Activation Functions\n\ndef relu(z):\n    """"""The relu function.""""""\n    return np.maximum(z, 0)\n\ndef relu_prime(z):\n    """"""Derivative of the relu function.""""""\n    return np.where(z > 0, 1.0, 0.0)\n\n### Training\ntest_errors = []\ntraining_errors = []\nn = len(training_data)\n\nfile_name_common = \'relu_ce\'+\'_nHidden\'+str(n_node_hidden)+\'.txt\'\n\ntry:\n    training_errors = np.loadtxt(fname=\'tr_\'+file_name_common)\n    test_errors = np.loadtxt(fname=\'test_\'+file_name_common)\nexcept:\n    for j in range(n_epoch):\n\n        ## Stochastic Gradient Descent\n        np.random.shuffle(training_data)\n\n        # for each batch\n        sum_of_training_error = 0\n        for k in range(0, n, batch_size):\n            batch = training_data[k:k+batch_size]\n\n            # average gradient for samples in a batch\n            sum_gradient_b3 = 0\n            sum_gradient_b2 = 0\n            sum_gradient_W3 = 0\n            sum_gradient_W2 = 0\n\n            # for each sample\n            for x, y in batch:\n                ## Feed forward\n\n                a1 = x\n                z2 = np.dot(W2, a1) + b2\n                a2 = relu(z2)\n                z3 = np.dot(W3, a2) + b3\n                a3 = relu(z3)\n\n                ## Backpropagation\n\n                # Step 1: Error at the output layer [Cross-Entropy Cost]\n                delta_3 = (a3-y)\n                # Step 2: Error relationship between two adjacent layers\n                delta_2 = relu_prime(z2)*np.dot(W3.transpose(), delta_3)\n                # Step 3: Gradient of C in terms of bias\n                gradient_b3 = delta_3\n                gradient_b2 = delta_2\n                # Step 4: Gradient of C in terms of weight\n                gradient_W3 = np.dot(delta_3, a2.transpose())\n                gradient_W2 = np.dot(delta_2, a1.transpose())\n\n                # update gradients\n                sum_gradient_b3 += gradient_b3\n                sum_gradient_b2 += gradient_b2\n                sum_gradient_W3 += gradient_W3\n                sum_gradient_W2 += gradient_W2\n\n                ## Training Error\n                sum_of_training_error += int(np.argmax(a3) != np.argmax(y))\n\n            # update weights & biases via SGD\n            b3 -= learning_rate * sum_gradient_b3 / batch_size\n            b2 -= learning_rate * sum_gradient_b2 / batch_size\n            W3 -= learning_rate * sum_gradient_W3 / batch_size\n            W2 -= learning_rate * sum_gradient_W2 / batch_size\n\n        # Report Training Error\n        print(""[TRAIN_ERROR] Epoch %02d: %5d / %05d"" % (j, sum_of_training_error, n))\n        training_errors.append(np.float(sum_of_training_error) / n)\n\n        ### Test\n        n_test = len(test_data)\n        sum_of_test_error = 0\n        for x, y in test_data:\n            ## Feed forward\n\n            a1 = x\n            z2 = np.dot(W2, a1) + b2\n            a2 = relu(z2)\n            z3 = np.dot(W3, a2) + b3\n            a3 = relu(z3)\n\n            ## Test Error\n            # in test data, label info is a number not one-hot vector as in training data\n            sum_of_test_error += int(np.argmax(a3) != y)\n\n        # Report Test Error\n        print(""[ TEST_ERROR] Epoch %02d: %5d / %05d"" % (j, sum_of_test_error, n_test))\n\n        test_errors.append(np.float(sum_of_test_error)/n_test)\n\n    ## Save Results\n    np.savetxt(\'tr_\'+file_name_common, np.array(training_errors), fmt=\'%.5f\')\n    np.savetxt(\'test_\'+file_name_common, np.array(test_errors), fmt=\'%.5f\')\n\n### Plot results\nimport matplotlib.pyplot as plt\nidx = np.arange(1,n_epoch+1)\n\nplt.plot(idx, np.array(test_errors)*100,\'ro-\', label=\'ReLU\')\ntry:\n    # Load baseline\n    file_name_common = \'XavierInit_ce\'+\'_nHidden\'+str(n_node_hidden)+\'.txt\'\n    mse = np.loadtxt(fname=\'test_\'+file_name_common)\n    plt.plot(idx,np.array(mse)*100,\'bo-\', label=\'Sigmoid\')\nexcept:\n    print (\'There is no result of baseline\')\n\nlegend = plt.legend(loc=\'upper center\', shadow=True)\nfont = {\'family\' : \'normal\',\n        \'weight\' : \'bold\',\n        \'size\'   : 15}\nplt.rc(\'font\', **font)\nplt.xlabel(\'Epoch\', fontsize=22)\nplt.ylabel(\'Test error rate [%]\', fontsize=22)\nplt.grid(True)\nplt.show()'"
launcher_package1.py,1,"b""# Package 1\n# from https://github.com/mnielsen/neural-networks-and-deep-learning\n\nimport layer\nimport mnist_loader\n\n### Data Loading\n\ntraining_data, validation_data, test_data = mnist_loader.load_data_wrapper(augmentation=False)\n\n### Parameters\n\nn_epoch = 3\nlearning_rate = 0.5\nbatch_size = 50\n\n### Network Architecture\n\nn_node_input = 784\nn_node_hidden = 30\nn_node_output = 10\n\nnet = layer.Network([n_node_input, n_node_hidden, n_node_output],\n                       W_init='xavier',             # or normal\n                       b_init='zero',               # or normal\n                       cost=layer.CrossEntropyCost, # or QuadraticCost\n                       act_fn=layer.Sigmoid         # or Relu\n                       )\n\n### Training\n\n# SGD\nevaluation_cost, evaluation_accuracy, \\\ntraining_cost, training_accuracy = \\\nnet.SGD(training_data, n_epoch, batch_size, learning_rate,\n        lmbda=0.0,                          # L2 regularization\n        evaluation_data=test_data,\n        monitor_evaluation_cost=False,\n        monitor_evaluation_accuracy=True,\n        monitor_training_cost=False,\n        monitor_training_accuracy=True)\n\n### Plot results\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nidx = np.arange(1,n_epoch+1)\n\nplt.plot(idx, evaluation_accuracy,'ro-', label='test acc.')\nplt.plot(idx, training_accuracy,'bo-', label='training acc.')\n\nlegend = plt.legend(loc='upper center', shadow=True)\nfont = {'family' : 'normal',\n        'weight' : 'bold',\n        'size'   : 15}\nplt.rc('font', **font)\nplt.xlabel('Epoch', fontsize=22)\nplt.ylabel('Accuracy [%]', fontsize=22)\nplt.grid(True)\nplt.show()"""
launcher_package2.py,4,"b'# Package 2\n# from https://github.com/cthorey/CS231/tree/master/assignment2\n\nimport numpy as np\nimport mnist_loader\n\nimport matplotlib.pyplot as plt\nfrom cs231n.classifiers.fc_net import *\nfrom cs231n.solver import Solver\n\n\n### Data Loading\n\ntraining_data, validation_data, test_data = mnist_loader.load_data_wrapper(augmentation=False)\n\n\nX_train, y_train = zip(*training_data)\nX_val, y_val = zip(*test_data)\n\nX_train = np.array(X_train)\ny_train = np.array(np.squeeze(np.argmax(y_train, axis=1)))\nX_val = np.array(X_val)\ny_val = np.array(y_val)\n\n### Parameters\n\nn_epoch = 30\nlearning_rate = 0.001\nbatch_size = 10\n\n### Network Architecture\n\nn_node_input = 784\nhidden_dims = [100]\nn_node_output = 10\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n"""""" Batchnorm for deep networks  """"""\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n# Try training a very deep net with batchnorm\nhidden_dims = [100]\n\nfull_data = {\n  \'X_train\': X_train,\n  \'y_train\': y_train,\n  \'X_val\': X_val,\n  \'y_val\': y_val,\n}\n\nweight_scale = 2e-2 # this is for weight-initializer\nbn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=True, input_dim=n_node_input)\nmodel = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=False, input_dim=n_node_input)\n\nbn_solver = Solver(bn_model, full_data,\n                num_epochs=n_epoch, batch_size=batch_size,\n                update_rule=\'adam\',\n                optim_config={\n                  \'learning_rate\': learning_rate,\n                },\n                verbose=True, print_every=200)\nbn_solver.train()\n\nsolver = Solver(model, full_data,\n                num_epochs=n_epoch, batch_size=batch_size,\n                update_rule=\'adam\',\n                optim_config={\n                  \'learning_rate\': learning_rate,\n                },\n                verbose=True, print_every=200)\nsolver.train()\n\n### Plot Results\n#matplotlib inline\nplt.rcParams[\'figure.figsize\'] = (10.0, 8.0) # set default size of plots\nplt.rcParams[\'image.interpolation\'] = \'nearest\'\nplt.rcParams[\'image.cmap\'] = \'gray\'\n\nplt.subplot(2, 1, 1)\nplt.title(\'Training accuracy\')\nplt.xlabel(\'Iteration\')\n\nplt.subplot(2, 1, 2)\nplt.title(\'Validation accuracy\')\nplt.xlabel(\'Epoch\')\n\nplt.subplot(2, 1, 1)\nplt.plot(solver.train_acc_history, \'-o\', label=\'baseline\')\nplt.plot(bn_solver.train_acc_history, \'-o\', label=\'batchnorm\')\n\nplt.subplot(2, 1, 2)\nplt.plot(solver.val_acc_history, \'-o\', label=\'baseline\')\nplt.plot(bn_solver.val_acc_history, \'-o\', label=\'batchnorm\')\n\nfor i in [1, 2]:\n    plt.subplot(2, 1, i)\n    plt.legend(loc=\'upper center\', ncol=4)\nplt.gcf().set_size_inches(15, 15)\nplt.show()'"
layer.py,29,"b'# From https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network2.py\n\n# Third-party libraries\nimport numpy as np\n\n#### Define non-linear activation functions\nclass Sigmoid(object):\n    @staticmethod\n    def activation(z):\n        return 1.0/(1.0+np.exp(-z))\n\n    @staticmethod\n    def prime(z):\n        z = 1.0/(1.0+np.exp(-z))\n        return z*(1-z)\n\nclass Relu(object):\n    @staticmethod\n    def activation(z):\n        return np.maximum(z, 0)\n\n    @staticmethod\n    def prime(z):\n        return np.where(z > 0, 1.0, 0.0)\n\n#### Define the quadratic and cross-entropy cost functions\nclass QuadraticCost(object):\n\n    @staticmethod\n    def fn(a, y):\n        return 0.5*np.linalg.norm(a-y)**2\n\n    @staticmethod\n    def delta(act_fn_prime_z, a, y):\n        return (a-y) * act_fn_prime_z\n\n\nclass CrossEntropyCost(object):\n\n    @staticmethod\n    def fn(a, y):\n        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n\n    @staticmethod\n    def delta(act_fn_prime_z, a, y):\n        return (a-y)\n\n\n#### Main Network class\nclass Network(object):\n\n    def __init__(self, sizes,\n                 W_init=\'xavier\',\n                 b_init=\'zero\',\n                 cost=CrossEntropyCost,\n                 act_fn=Relu):\n        self.num_layers = len(sizes)\n        self.sizes = sizes\n        self.weight_initializer(W_init)\n        self.bias_initializer(b_init)\n        self.cost=cost\n        self.act_fn=act_fn\n\n    def weight_initializer(self, W_init):\n        if W_init == \'xavier\':\n            self.weights = [np.random.randn(y, x) / np.sqrt(x)\n                            for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n        elif W_init == \'he\':\n            self.weights = [np.random.randn(y, x) / np.sqrt(x/2)\n                            for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n        elif W_init == \'normal\':\n            self.weights = [np.random.randn(y, x)\n                            for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n        else:\n            NotImplementedError()\n\n    def bias_initializer(self, b_init):\n        if b_init == \'normal\':\n            self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n        elif b_init == \'zero\':\n            self.biases = [np.zeros([y, 1]) for y in self.sizes[1:]]\n        else:\n            NotImplementedError()\n\n\n    def feedforward(self, a):\n        for b, W in zip(self.biases, self.weights):\n            a = self.act_fn.activation(np.dot(W, a) + b)\n        return a\n\n    def SGD(self, training_data, epochs, mini_batch_size, eta,\n            lmbda = 0.0,\n            evaluation_data=None,\n            monitor_evaluation_cost=False,\n            monitor_evaluation_accuracy=False,\n            monitor_training_cost=False,\n            monitor_training_accuracy=False):\n\n        if evaluation_data: n_data = len(evaluation_data)\n        n = len(training_data)\n        evaluation_cost, evaluation_accuracy = [], []\n        training_cost, training_accuracy = [], []\n        for j in range(epochs):\n            np.random.shuffle(training_data)\n            mini_batches = [\n                training_data[k:k+mini_batch_size]\n                for k in range(0, n, mini_batch_size)]\n            for mini_batch in mini_batches:\n                self.update_mini_batch(\n                    mini_batch, eta, lmbda, len(training_data))\n            print(""Epoch %s training complete"" % j)\n            if monitor_training_cost:\n                cost = self.total_cost(training_data, lmbda)\n                training_cost.append(cost)\n                print(""Cost on training data: {}"".format(cost))\n            if monitor_training_accuracy:\n                accuracy = self.accuracy(training_data, convert=True)\n                training_accuracy.append(accuracy/np.float(n))\n                print(""Accuracy on training data: {} / {}"".format(\n                    accuracy, n))\n            if monitor_evaluation_cost:\n                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n                evaluation_cost.append(cost)\n                print(""Cost on evaluation data: {}"".format(cost))\n            if monitor_evaluation_accuracy:\n                accuracy = self.accuracy(evaluation_data)\n                evaluation_accuracy.append(accuracy/np.float(n_data))\n                print(""Accuracy on evaluation data: {} / {}"".format(\n                    self.accuracy(evaluation_data), n_data))\n            print\n        return evaluation_cost, evaluation_accuracy, \\\n            training_cost, training_accuracy\n\n    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n        x, y = zip(*mini_batch)\n        x = np.squeeze(x)\n        y = np.squeeze(y)\n\n        nabla_b, nabla_w = self.backprop(x, y)\n\n        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n                        for w, nw in zip(self.weights, nabla_w)]\n        self.biases = [b-(eta/len(mini_batch))*nb\n                       for b, nb in zip(self.biases, nabla_b)]\n\n    def backprop(self, x, y):\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        # feedforward\n        activation = x\n        activations = [x] # list to store all the activations, layer by layer\n        zs = [] # list to store all the z vectors, layer by layer\n        for b, W in zip(self.biases, self.weights):\n            z = np.dot(activation, W.transpose()) + b.transpose()\n            zs.append(z)\n            activation = self.act_fn.activation(z)\n            activations.append(activation)\n        # backward pass\n        delta = (self.cost).delta(self.act_fn.prime(zs[-1]), activations[-1], y)\n        nabla_b[-1] = np.expand_dims(np.sum(delta,axis=0),axis=1)\n        nabla_w[-1] = np.dot(delta.transpose(),activations[-2])\n        for l in range(2, self.num_layers):\n            z = zs[-l]\n            sp = self.act_fn.prime(z)\n            delta = sp * np.dot(delta, self.weights[-l+1])\n            nabla_b[-l] = np.expand_dims(np.sum(delta,axis=0),axis=1)\n            nabla_w[-l] = np.dot(delta.transpose(), activations[-l-1])\n        return (nabla_b, nabla_w)\n\n    def accuracy(self, data, convert=False):\n        if convert:\n            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n                       for (x, y) in data]\n        else:\n            results = [(np.argmax(self.feedforward(x)), y)\n                        for (x, y) in data]\n        return sum(int(x == y) for (x, y) in results)\n\n    def total_cost(self, data, lmbda, convert=False):\n        cost = 0.0\n        for x, y in data:\n            a = self.feedforward(x)\n            if convert: y = vectorized_result(y)\n            cost += self.cost.fn(a, y)/len(data)\n        cost += 0.5*(lmbda/len(data))*sum(\n            np.linalg.norm(w)**2 for w in self.weights)\n        return cost\n\n#### Miscellaneous functions\ndef vectorized_result(j):\n    e = np.zeros((10, 1))\n    e[j] = 1.0\n    return e'"
mnist_loader.py,10,"b'""""""\nmnist_loader\n~~~~~~~~~~~~\nA library to load the MNIST image data.  For details of the data\nstructures that are returned, see the doc strings for ``load_data``\nand ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\nfunction usually called by our neural network code.\n""""""\n\n#### Libraries\n# Standard library\nimport _pickle as cPickle\nimport gzip\n\n# Third-party libraries\nimport numpy as np\nfrom scipy import ndimage\n\ndef load_data():\n    """"""Return the MNIST data as a tuple containing the training data,\n    the validation data, and the test data.\n    The ``training_data`` is returned as a tuple with two entries.\n    The first entry contains the actual training images.  This is a\n    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n    numpy ndarray with 784 values, representing the 28 * 28 = 784\n    pixels in a single MNIST image.\n    The second entry in the ``training_data`` tuple is a numpy ndarray\n    containing 50,000 entries.  Those entries are just the digit\n    values (0...9) for the corresponding images contained in the first\n    entry of the tuple.\n    The ``validation_data`` and ``test_data`` are similar, except\n    each contains only 10,000 images.\n    This is a nice data format, but for use in neural networks it\'s\n    helpful to modify the format of the ``training_data`` a little.\n    That\'s done in the wrapper function ``load_data_wrapper()``, see\n    below.\n    """"""\n    f = gzip.open(\'data/mnist.pkl.gz\', \'rb\')\n    training_data, validation_data, test_data = cPickle.load(f, encoding=\'latin1\')\n    f.close()\n    return (training_data, validation_data, test_data)\n\ndef load_data_wrapper(augmentation=False):\n    """"""Return a tuple containing ``(training_data, validation_data,\n    test_data)``. Based on ``load_data``, but the format is more\n    convenient for use in our implementation of neural networks.\n    In particular, ``training_data`` is a list containing 50,000\n    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n    containing the input image.  ``y`` is a 10-dimensional\n    numpy.ndarray representing the unit vector corresponding to the\n    correct digit for ``x``.\n    ``validation_data`` and ``test_data`` are lists containing 10,000\n    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n    numpy.ndarry containing the input image, and ``y`` is the\n    corresponding classification, i.e., the digit values (integers)\n    corresponding to ``x``.\n    Obviously, this means we\'re using slightly different formats for\n    the training data and the validation / test data.  These formats\n    turn out to be the most convenient for use in our neural network\n    code.""""""\n    tr_d, va_d, te_d = load_data()\n    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n    training_results = [vectorized_result(y) for y in tr_d[1]]\n    if augmentation: training_inputs, training_results = expend_training_data(training_inputs, training_results)\n    training_data = list(zip(training_inputs, training_results))\n    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n    validation_data = list(zip(validation_inputs, va_d[1]))\n    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n    test_data = list(zip(test_inputs, te_d[1]))\n    return (training_data, validation_data, test_data)\n\n\ndef vectorized_result(j):\n    """"""Return a 10-dimensional unit vector with a 1.0 in the jth\n    position and zeroes elsewhere.  This is used to convert a digit\n    (0...9) into a corresponding desired output from the neural\n    network.""""""\n    e = np.zeros((10, 1))\n    e[j] = 1.0\n    return e\n\n# Augment training data\ndef expend_training_data(images, labels):\n\n    expanded_images = []\n    expanded_labels = []\n\n    j = 0 # counter\n    for x, y in zip(images, labels):\n        j = j+1\n        if j%100==0:\n            print (\'expanding data : %03d / %03d\' % (j,np.size(images,0)))\n\n        # register original data\n        expanded_images.append(x)\n        expanded_labels.append(y)\n\n        # get a value for the background\n        # zero is the expected value, but median() is used to estimate background\'s value\n        bg_value = np.median(x) # this is regarded as background\'s value\n        image = np.reshape(x, (-1, 28))\n\n        for i in range(4):\n\n            ## Augmentation via Rotation\n            # rotate the image with random degree\n            angle = np.random.randint(-15,15,1)\n            new_img = ndimage.rotate(image,angle,reshape=False, cval=bg_value)\n\n            # shift the image with random distance\n            shift = np.random.randint(-2, 2, 2)\n            new_img_ = ndimage.shift(new_img,shift, cval=bg_value)\n\n            # register new training data\n            expanded_images.append(np.reshape(new_img_, [784,1]))\n            expanded_labels.append(y)\n\n\n    return expanded_images, expanded_labels'"
cs231n/__init__.py,0,b''
cs231n/data_utils.py,13,"b'import _pickle as pickle\nimport numpy as np\nimport os\nfrom scipy.misc import imread\nDIR_CS231n = \'/Users/thorey/Documents/MLearning/CS231/assignment2/\'\n\n\ndef load_CIFAR_batch(filename):\n    """""" load single batch of cifar """"""\n    with open(filename, \'rb\') as f:\n        datadict = pickle.load(f, encoding=\'latin1\')\n        X = datadict[\'data\']\n        Y = datadict[\'labels\']\n        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype(""float"")\n        Y = np.array(Y)\n        return X, Y\n\n\ndef load_CIFAR10(ROOT):\n    """""" load all of cifar """"""\n    xs = []\n    ys = []\n    for b in range(1, 6):\n        f = os.path.join(ROOT, \'data_batch_%d\' % (b, ))\n        X, Y = load_CIFAR_batch(f)\n        xs.append(X)\n        ys.append(Y)\n    Xtr = np.concatenate(xs)\n    Ytr = np.concatenate(ys)\n    del X, Y\n    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, \'test_batch\'))\n    return Xtr, Ytr, Xte, Yte\n\n\ndef get_CIFAR10_data(dir_path, num_training=49000, num_validation=1000, num_test=1000):\n    """"""\n    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n    it for classifiers. These are the same steps as we used for the SVM, but\n    condensed to a single function.\n    """"""\n    # Load the raw CIFAR-10 data\n    cifar10_dir = os.path.join(dir_path, \'cs231n/datasets/cifar-10-batches-py\')\n    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n\n    # Subsample the data\n    mask = range(num_training, num_training + num_validation)\n    X_val = X_train[mask]\n    y_val = y_train[mask]\n    mask = range(num_training)\n    X_train = X_train[mask]\n    y_train = y_train[mask]\n    mask = range(num_test)\n    X_test = X_test[mask]\n    y_test = y_test[mask]\n\n    # Normalize the data: subtract the mean image\n    mean_image = np.mean(X_train, axis=0)\n    X_train -= mean_image\n    X_val -= mean_image\n    X_test -= mean_image\n\n    # Transpose so that channels come first\n    X_train = X_train.transpose(0, 3, 1, 2).copy()\n    X_val = X_val.transpose(0, 3, 1, 2).copy()\n    X_test = X_test.transpose(0, 3, 1, 2).copy()\n\n    # Package data into a dictionary\n    return {\n        \'X_train\': X_train, \'y_train\': y_train,\n        \'X_val\': X_val, \'y_val\': y_val,\n        \'X_test\': X_test, \'y_test\': y_test,\n    }\n\n\ndef load_tiny_imagenet(path, dtype=np.float32):\n    """"""\n    Load TinyImageNet. Each of TinyImageNet-100-A, TinyImageNet-100-B, and\n    TinyImageNet-200 have the same directory structure, so this can be used\n    to load any of them.\n\n    Inputs:\n    - path: String giving path to the directory to load.\n    - dtype: numpy datatype used to load the data.\n\n    Returns: A tuple of\n    - class_names: A list where class_names[i] is a list of strings giving the\n      WordNet names for class i in the loaded dataset.\n    - X_train: (N_tr, 3, 64, 64) array of training images\n    - y_train: (N_tr,) array of training labels\n    - X_val: (N_val, 3, 64, 64) array of validation images\n    - y_val: (N_val,) array of validation labels\n    - X_test: (N_test, 3, 64, 64) array of testing images.\n    - y_test: (N_test,) array of test labels; if test labels are not available\n      (such as in student code) then y_test will be None.\n    """"""\n    # First load wnids\n    with open(os.path.join(path, \'wnids.txt\'), \'r\') as f:\n        wnids = [x.strip() for x in f]\n\n    # Map wnids to integer labels\n    wnid_to_label = {wnid: i for i, wnid in enumerate(wnids)}\n\n    # Use words.txt to get names for each class\n    with open(os.path.join(path, \'words.txt\'), \'r\') as f:\n        wnid_to_words = dict(line.split(\'\\t\') for line in f)\n        for wnid, words in wnid_to_words.iteritems():\n            wnid_to_words[wnid] = [w.strip() for w in words.split(\',\')]\n    class_names = [wnid_to_words[wnid] for wnid in wnids]\n\n    # Next load training data.\n    X_train = []\n    y_train = []\n    for i, wnid in enumerate(wnids):\n        if (i + 1) % 20 == 0:\n            print(\'loading training data for synset %d / %d\' % (i + 1, len(wnids)))\n        # To figure out the filenames we need to open the boxes file\n        boxes_file = os.path.join(path, \'train\', wnid, \'%s_boxes.txt\' % wnid)\n        with open(boxes_file, \'r\') as f:\n            filenames = [x.split(\'\\t\')[0] for x in f]\n        num_images = len(filenames)\n\n        X_train_block = np.zeros((num_images, 3, 64, 64), dtype=dtype)\n        y_train_block = wnid_to_label[wnid] * \\\n            np.ones(num_images, dtype=np.int64)\n        for j, img_file in enumerate(filenames):\n            img_file = os.path.join(path, \'train\', wnid, \'images\', img_file)\n            img = imread(img_file)\n            if img.ndim == 2:\n                # grayscale file\n                img.shape = (64, 64, 1)\n            X_train_block[j] = img.transpose(2, 0, 1)\n        X_train.append(X_train_block)\n        y_train.append(y_train_block)\n\n    # We need to concatenate all training data\n    X_train = np.concatenate(X_train, axis=0)\n    y_train = np.concatenate(y_train, axis=0)\n\n    # Next load validation data\n    with open(os.path.join(path, \'val\', \'val_annotations.txt\'), \'r\') as f:\n        img_files = []\n        val_wnids = []\n        for line in f:\n            img_file, wnid = line.split(\'\\t\')[:2]\n            img_files.append(img_file)\n            val_wnids.append(wnid)\n        num_val = len(img_files)\n        y_val = np.array([wnid_to_label[wnid] for wnid in val_wnids])\n        X_val = np.zeros((num_val, 3, 64, 64), dtype=dtype)\n        for i, img_file in enumerate(img_files):\n            img_file = os.path.join(path, \'val\', \'images\', img_file)\n            img = imread(img_file)\n            if img.ndim == 2:\n                img.shape = (64, 64, 1)\n            X_val[i] = img.transpose(2, 0, 1)\n\n    # Next load test images\n    # Students won\'t have test labels, so we need to iterate over files in the\n    # images directory.\n    img_files = os.listdir(os.path.join(path, \'test\', \'images\'))\n    X_test = np.zeros((len(img_files), 3, 64, 64), dtype=dtype)\n    for i, img_file in enumerate(img_files):\n        img_file = os.path.join(path, \'test\', \'images\', img_file)\n        img = imread(img_file)\n        if img.ndim == 2:\n            img.shape = (64, 64, 1)\n        X_test[i] = img.transpose(2, 0, 1)\n\n    y_test = None\n    y_test_file = os.path.join(path, \'test\', \'test_annotations.txt\')\n    if os.path.isfile(y_test_file):\n        with open(y_test_file, \'r\') as f:\n            img_file_to_wnid = {}\n            for line in f:\n                line = line.split(\'\\t\')\n                img_file_to_wnid[line[0]] = line[1]\n        y_test = [wnid_to_label[img_file_to_wnid[img_file]]\n                  for img_file in img_files]\n        y_test = np.array(y_test)\n\n    return class_names, X_train, y_train, X_val, y_val, X_test, y_test\n\n\ndef load_models(models_dir):\n    """"""\n    Load saved models from disk. This will attempt to unpickle all files in a\n    directory; any files that give errors on unpickling (such as README.txt) will\n    be skipped.\n\n    Inputs:\n    - models_dir: String giving the path to a directory containing model files.\n      Each model file is a pickled dictionary with a \'model\' field.\n\n    Returns:\n    A dictionary mapping model file names to models.\n    """"""\n    models = {}\n    for model_file in os.listdir(models_dir):\n        with open(os.path.join(models_dir, model_file), \'rb\') as f:\n            try:\n                models[model_file] = pickle.load(f)[\'model\']\n            except pickle.UnpicklingError:\n                continue\n    return models\n'"
cs231n/fast_layers.py,17,"b'import numpy as np\ntry:\n  from cs231n.im2col_cython import col2im_cython, im2col_cython\n  from cs231n.im2col_cython import col2im_6d_cython\nexcept ImportError:\n  print(\'run the following from the cs231n directory and try again:\')\n  print(\'python setup.py build_ext --inplace\')\n  print(\'You may also need to restart your iPython kernel\')\n\nfrom cs231n.im2col import *\n\n\ndef conv_forward_im2col(x, w, b, conv_param):\n  """"""\n  A fast implementation of the forward pass for a convolutional layer\n  based on im2col and col2im.\n  """"""\n  N, C, H, W = x.shape\n  num_filters, _, filter_height, filter_width = w.shape\n  stride, pad = conv_param[\'stride\'], conv_param[\'pad\']\n\n  # Check dimensions\n  assert (W + 2 * pad - filter_width) % stride == 0, \'width does not work\'\n  assert (H + 2 * pad - filter_height) % stride == 0, \'height does not work\'\n\n  # Create output\n  out_height = (H + 2 * pad - filter_height) / stride + 1\n  out_width = (W + 2 * pad - filter_width) / stride + 1\n  out = np.zeros((N, num_filters, out_height, out_width), dtype=x.dtype)\n\n  # x_cols = im2col_indices(x, w.shape[2], w.shape[3], pad, stride)\n  x_cols = im2col_cython(x, w.shape[2], w.shape[3], pad, stride)\n  res = w.reshape((w.shape[0], -1)).dot(x_cols) + b.reshape(-1, 1)\n\n  out = res.reshape(w.shape[0], out.shape[2], out.shape[3], x.shape[0])\n  out = out.transpose(3, 0, 1, 2)\n\n  cache = (x, w, b, conv_param, x_cols)\n  return out, cache\n\n\ndef conv_forward_strides(x, w, b, conv_param):\n  N, C, H, W = x.shape\n  F, _, HH, WW = w.shape\n  stride, pad = conv_param[\'stride\'], conv_param[\'pad\']\n\n  # Check dimensions\n  assert (W + 2 * pad - WW) % stride == 0, \'width does not work\'\n  assert (H + 2 * pad - HH) % stride == 0, \'height does not work\'\n\n  # Pad the input\n  p = pad\n  x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode=\'constant\')\n  \n  # Figure out output dimensions\n  H += 2 * pad\n  W += 2 * pad\n  out_h = (H - HH) / stride + 1\n  out_w = (W - WW) / stride + 1\n\n  # Perform an im2col operation by picking clever strides\n  shape = (C, HH, WW, N, out_h, out_w)\n  strides = (H * W, W, 1, C * H * W, stride * W, stride)\n  strides = x.itemsize * np.array(strides)\n  x_stride = np.lib.stride_tricks.as_strided(x_padded,\n                shape=shape, strides=strides)\n  x_cols = np.ascontiguousarray(x_stride)\n  x_cols.shape = (C * HH * WW, N * out_h * out_w)\n\n  # Now all our convolutions are a big matrix multiply\n  res = w.reshape(F, -1).dot(x_cols) + b.reshape(-1, 1)\n\n  # Reshape the output\n  res.shape = (F, N, out_h, out_w)\n  out = res.transpose(1, 0, 2, 3)\n\n  # Be nice and return a contiguous array\n  # The old version of conv_forward_fast doesn\'t do this, so for a fair\n  # comparison we won\'t either\n  out = np.ascontiguousarray(out)\n\n  cache = (x, w, b, conv_param, x_cols)\n  return out, cache\n  \n\ndef conv_backward_strides(dout, cache):\n  x, w, b, conv_param, x_cols = cache\n  stride, pad = conv_param[\'stride\'], conv_param[\'pad\']\n\n  N, C, H, W = x.shape\n  F, _, HH, WW = w.shape\n  _, _, out_h, out_w = dout.shape\n\n  db = np.sum(dout, axis=(0, 2, 3))\n\n  dout_reshaped = dout.transpose(1, 0, 2, 3).reshape(F, -1)\n  dw = dout_reshaped.dot(x_cols.T).reshape(w.shape)\n\n  dx_cols = w.reshape(F, -1).T.dot(dout_reshaped)\n  dx_cols.shape = (C, HH, WW, N, out_h, out_w)\n  dx = col2im_6d_cython(dx_cols, N, C, H, W, HH, WW, pad, stride)\n\n  return dx, dw, db\n\n\ndef conv_backward_im2col(dout, cache):\n  """"""\n  A fast implementation of the backward pass for a convolutional layer\n  based on im2col and col2im.\n  """"""\n  x, w, b, conv_param, x_cols = cache\n  stride, pad = conv_param[\'stride\'], conv_param[\'pad\']\n\n  db = np.sum(dout, axis=(0, 2, 3))\n\n  num_filters, _, filter_height, filter_width = w.shape\n  dout_reshaped = dout.transpose(1, 2, 3, 0).reshape(num_filters, -1)\n  dw = dout_reshaped.dot(x_cols.T).reshape(w.shape)\n\n  dx_cols = w.reshape(num_filters, -1).T.dot(dout_reshaped)\n  # dx = col2im_indices(dx_cols, x.shape, filter_height, filter_width, pad, stride)\n  dx = col2im_cython(dx_cols, x.shape[0], x.shape[1], x.shape[2], x.shape[3],\n                     filter_height, filter_width, pad, stride)\n\n  return dx, dw, db\n\n\nconv_forward_fast = conv_forward_strides\nconv_backward_fast = conv_backward_strides\n\n\ndef max_pool_forward_fast(x, pool_param):\n  """"""\n  A fast implementation of the forward pass for a max pooling layer.\n\n  This chooses between the reshape method and the im2col method. If the pooling\n  regions are square and tile the input image, then we can use the reshape\n  method which is very fast. Otherwise we fall back on the im2col method, which\n  is not much faster than the naive method.\n  """"""\n  N, C, H, W = x.shape\n  pool_height, pool_width = pool_param[\'pool_height\'], pool_param[\'pool_width\']\n  stride = pool_param[\'stride\']\n\n  same_size = pool_height == pool_width == stride\n  tiles = H % pool_height == 0 and W % pool_width == 0\n  if same_size and tiles:\n    out, reshape_cache = max_pool_forward_reshape(x, pool_param)\n    cache = (\'reshape\', reshape_cache)\n  else:\n    out, im2col_cache = max_pool_forward_im2col(x, pool_param)\n    cache = (\'im2col\', im2col_cache)\n  return out, cache\n\n\ndef max_pool_backward_fast(dout, cache):\n  """"""\n  A fast implementation of the backward pass for a max pooling layer.\n\n  This switches between the reshape method an the im2col method depending on\n  which method was used to generate the cache.\n  """"""\n  method, real_cache = cache\n  if method == \'reshape\':\n    return max_pool_backward_reshape(dout, real_cache)\n  elif method == \'im2col\':\n    return max_pool_backward_im2col(dout, real_cache)\n  else:\n    raise ValueError(\'Unrecognized method ""%s""\' % method)\n\n\ndef max_pool_forward_reshape(x, pool_param):\n  """"""\n  A fast implementation of the forward pass for the max pooling layer that uses\n  some clever reshaping.\n\n  This can only be used for square pooling regions that tile the input.\n  """"""\n  N, C, H, W = x.shape\n  pool_height, pool_width = pool_param[\'pool_height\'], pool_param[\'pool_width\']\n  stride = pool_param[\'stride\']\n  assert pool_height == pool_width == stride, \'Invalid pool params\'\n  assert H % pool_height == 0\n  assert W % pool_height == 0\n  x_reshaped = x.reshape(N, C, H / pool_height, pool_height,\n                         W / pool_width, pool_width)\n  out = x_reshaped.max(axis=3).max(axis=4)\n\n  cache = (x, x_reshaped, out)\n  return out, cache\n\n\ndef max_pool_backward_reshape(dout, cache):\n  """"""\n  A fast implementation of the backward pass for the max pooling layer that\n  uses some clever broadcasting and reshaping.\n\n  This can only be used if the forward pass was computed using\n  max_pool_forward_reshape.\n\n  NOTE: If there are multiple argmaxes, this method will assign gradient to\n  ALL argmax elements of the input rather than picking one. In this case the\n  gradient will actually be incorrect. However this is unlikely to occur in\n  practice, so it shouldn\'t matter much. One possible solution is to split the\n  upstream gradient equally among all argmax elements; this should result in a\n  valid subgradient. You can make this happen by uncommenting the line below;\n  however this results in a significant performance penalty (about 40% slower)\n  and is unlikely to matter in practice so we don\'t do it.\n  """"""\n  x, x_reshaped, out = cache\n\n  dx_reshaped = np.zeros_like(x_reshaped)\n  out_newaxis = out[:, :, :, np.newaxis, :, np.newaxis]\n  mask = (x_reshaped == out_newaxis)\n  dout_newaxis = dout[:, :, :, np.newaxis, :, np.newaxis]\n  dout_broadcast, _ = np.broadcast_arrays(dout_newaxis, dx_reshaped)\n  dx_reshaped[mask] = dout_broadcast[mask]\n  dx_reshaped /= np.sum(mask, axis=(3, 5), keepdims=True)\n  dx = dx_reshaped.reshape(x.shape)\n\n  return dx\n\n\ndef max_pool_forward_im2col(x, pool_param):\n  """"""\n  An implementation of the forward pass for max pooling based on im2col.\n\n  This isn\'t much faster than the naive version, so it should be avoided if\n  possible.\n  """"""\n  N, C, H, W = x.shape\n  pool_height, pool_width = pool_param[\'pool_height\'], pool_param[\'pool_width\']\n  stride = pool_param[\'stride\']\n\n  assert (H - pool_height) % stride == 0, \'Invalid height\'\n  assert (W - pool_width) % stride == 0, \'Invalid width\'\n\n  out_height = (H - pool_height) / stride + 1\n  out_width = (W - pool_width) / stride + 1\n\n  x_split = x.reshape(N * C, 1, H, W)\n  x_cols = im2col(x_split, pool_height, pool_width, padding=0, stride=stride)\n  x_cols_argmax = np.argmax(x_cols, axis=0)\n  x_cols_max = x_cols[x_cols_argmax, np.arange(x_cols.shape[1])]\n  out = x_cols_max.reshape(out_height, out_width, N, C).transpose(2, 3, 0, 1)\n\n  cache = (x, x_cols, x_cols_argmax, pool_param)\n  return out, cache\n\n\ndef max_pool_backward_im2col(dout, cache):\n  """"""\n  An implementation of the backward pass for max pooling based on im2col.\n\n  This isn\'t much faster than the naive version, so it should be avoided if\n  possible.\n  """"""\n  x, x_cols, x_cols_argmax, pool_param = cache\n  N, C, H, W = x.shape\n  pool_height, pool_width = pool_param[\'pool_height\'], pool_param[\'pool_width\']\n  stride = pool_param[\'stride\']\n\n  dout_reshaped = dout.transpose(2, 3, 0, 1).flatten()\n  dx_cols = np.zeros_like(x_cols)\n  dx_cols[x_cols_argmax, np.arange(dx_cols.shape[1])] = dout_reshaped\n  dx = col2im_indices(dx_cols, (N * C, 1, H, W), pool_height, pool_width,\n              padding=0, stride=stride)\n  dx = dx.reshape(x.shape)\n\n  return dx\n'"
cs231n/gradient_check.py,10,"b'import numpy as np\nfrom random import randrange\n\ndef eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n  """""" \n  a naive implementation of numerical gradient of f at x \n  - f should be a function that takes a single argument\n  - x is the point (numpy array) to evaluate the gradient at\n  """""" \n\n  fx = f(x) # evaluate function value at original point\n  grad = np.zeros_like(x)\n  # iterate over all indexes in x\n  it = np.nditer(x, flags=[\'multi_index\'], op_flags=[\'readwrite\'])\n  while not it.finished:\n\n    # evaluate function at x+h\n    ix = it.multi_index\n    oldval = x[ix]\n    x[ix] = oldval + h # increment by h\n    fxph = f(x) # evalute f(x + h)\n    x[ix] = oldval - h\n    fxmh = f(x) # evaluate f(x - h)\n    x[ix] = oldval # restore\n\n    # compute the partial derivative with centered formula\n    grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n    if verbose:\n      print(ix, grad[ix])\n    it.iternext() # step to next dimension\n\n  return grad\n\n\ndef eval_numerical_gradient_array(f, x, df, h=1e-5):\n  """"""\n  Evaluate a numeric gradient for a function that accepts a numpy\n  array and returns a numpy array.\n  """"""\n  grad = np.zeros_like(x)\n  it = np.nditer(x, flags=[\'multi_index\'], op_flags=[\'readwrite\'])\n  while not it.finished:\n    ix = it.multi_index\n    \n    oldval = x[ix]\n    x[ix] = oldval + h\n    pos = f(x).copy()\n    x[ix] = oldval - h\n    neg = f(x).copy()\n    x[ix] = oldval\n    \n    grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n    it.iternext()\n  return grad\n\n\ndef eval_numerical_gradient_blobs(f, inputs, output, h=1e-5):\n  """"""\n  Compute numeric gradients for a function that operates on input\n  and output blobs.\n  \n  We assume that f accepts several input blobs as arguments, followed by a blob\n  into which outputs will be written. For example, f might be called like this:\n\n  f(x, w, out)\n  \n  where x and w are input Blobs, and the result of f will be written to out.\n\n  Inputs: \n  - f: function\n  - inputs: tuple of input blobs\n  - output: output blob\n  - h: step size\n  """"""\n  numeric_diffs = []\n  for input_blob in inputs:\n    diff = np.zeros_like(input_blob.diffs)\n    it = np.nditer(input_blob.vals, flags=[\'multi_index\'],\n                   op_flags=[\'readwrite\'])\n    while not it.finished:\n      idx = it.multi_index\n      orig = input_blob.vals[idx]\n\n      input_blob.vals[idx] = orig + h\n      f(*(inputs + (output,)))\n      pos = np.copy(output.vals)\n      input_blob.vals[idx] = orig - h\n      f(*(inputs + (output,)))\n      neg = np.copy(output.vals)\n      input_blob.vals[idx] = orig\n      \n      diff[idx] = np.sum((pos - neg) * output.diffs) / (2.0 * h)\n\n      it.iternext()\n    numeric_diffs.append(diff)\n  return numeric_diffs\n\n\ndef eval_numerical_gradient_net(net, inputs, output, h=1e-5):\n  return eval_numerical_gradient_blobs(lambda *args: net.forward(),\n              inputs, output, h=h)\n\n\ndef grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5):\n  """"""\n  sample a few random elements and only return numerical\n  in this dimensions.\n  """"""\n\n  for i in range(num_checks):\n    ix = tuple([randrange(m) for m in x.shape])\n\n    oldval = x[ix]\n    x[ix] = oldval + h # increment by h\n    fxph = f(x) # evaluate f(x + h)\n    x[ix] = oldval - h # increment by h\n    fxmh = f(x) # evaluate f(x - h)\n    x[ix] = oldval # reset\n\n    grad_numerical = (fxph - fxmh) / (2 * h)\n    grad_analytic = analytic_grad[ix]\n    rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n    print(\'numerical: %f analytic: %f, relative error: %e\' % (grad_numerical, grad_analytic, rel_error))\n\n'"
cs231n/im2col.py,10,"b'import numpy as np\n\n\ndef get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n  # First figure out what the size of the output should be\n  N, C, H, W = x_shape\n  assert (H + 2 * padding - field_height) % stride == 0\n  assert (W + 2 * padding - field_height) % stride == 0\n  out_height = (H + 2 * padding - field_height) / stride + 1\n  out_width = (W + 2 * padding - field_width) / stride + 1\n\n  i0 = np.repeat(np.arange(field_height), field_width)\n  i0 = np.tile(i0, C)\n  i1 = stride * np.repeat(np.arange(out_height), out_width)\n  j0 = np.tile(np.arange(field_width), field_height * C)\n  j1 = stride * np.tile(np.arange(out_width), out_height)\n  i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n  j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n\n  k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n\n  return (k, i, j)\n\n\ndef im2col_indices(x, field_height, field_width, padding=1, stride=1):\n  """""" An implementation of im2col based on some fancy indexing """"""\n  # Zero-pad the input\n  p = padding\n  x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode=\'constant\')\n\n  k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding,\n                               stride)\n\n  cols = x_padded[:, k, i, j]\n  C = x.shape[1]\n  cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n  return cols\n\n\ndef col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1,\n                   stride=1):\n  """""" An implementation of col2im based on fancy indexing and np.add.at """"""\n  N, C, H, W = x_shape\n  H_padded, W_padded = H + 2 * padding, W + 2 * padding\n  x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n  k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding,\n                               stride)\n  cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n  cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n  np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n  if padding == 0:\n    return x_padded\n  return x_padded[:, :, padding:-padding, padding:-padding]\n\npass\n'"
cs231n/layer_utils.py,0,"b'from cs231n.layers import *\nfrom cs231n.fast_layers import *\n\n\ndef affine_relu_forward(x, w, b):\n    """"""\n    Convenience layer that perorms an affine transform followed by a ReLU\n\n    Inputs:\n    - x: Input to the affine layer\n    - w, b: Weights for the affine layer\n\n    Returns a tuple of:\n    - out: Output from the ReLU\n    - cache: Object to give to the backward pass\n    """"""\n\n    a, fc_cache = affine_forward(x, w, b)\n    out, relu_cache = relu_forward(a)\n    cache = (fc_cache, relu_cache)\n    return out, cache\n\n\ndef affine_relu_backward(dout, cache):\n    """"""\n    Backward pass for the affine-relu convenience layer\n    """"""\n    fc_cache, relu_cache = cache\n    da = relu_backward(dout, relu_cache)\n    dx, dw, db = affine_backward(da, fc_cache)\n    return dx, dw, db\n\n\ndef affine_norm_relu_forward(x, w, b, gamma, beta, bn_param):\n    """"""\n    Convenience layer that perorms an affine transform followed by a ReLU\n\n    Inputs:\n    - x: Input to the affine layer\n    - w, b: Weights for the affine layer\n    - gamma, beta : Weight for the batch norm regularization\n    - bn_params : Contain variable use to batch norml, running_mean and var\n\n    Returns a tuple of:\n    - out: Output from the ReLU\n    - cache: Object to give to the backward pass\n    """"""\n\n    h, h_cache = affine_forward(x, w, b)\n    hnorm, hnorm_cache = batchnorm_forward(h, gamma, beta, bn_param)\n    hnormrelu, relu_cache = relu_forward(hnorm)\n    cache = (h_cache, hnorm_cache, relu_cache)\n\n    return hnormrelu, cache\n\n\ndef affine_norm_relu_backward(dout, cache):\n    """"""\n    Backward pass for the affine-relu convenience layer\n    """"""\n    h_cache, hnorm_cache, relu_cache = cache\n\n    dhnormrelu = relu_backward(dout, relu_cache)\n    dhnorm, dgamma, dbeta = batchnorm_backward_alt(dhnormrelu, hnorm_cache)\n    dx, dw, db = affine_backward(dhnorm, h_cache)\n\n    return dx, dw, db, dgamma, dbeta\n\n\ndef conv_relu_forward(x, w, b, conv_param):\n    """"""\n    A convenience layer that performs a convolution followed by a ReLU.\n\n    Inputs:\n    - x: Input to the convolutional layer\n    - w, b, conv_param: Weights and parameters for the convolutional layer\n\n    Returns a tuple of:\n    - out: Output from the ReLU\n    - cache: Object to give to the backward pass\n    """"""\n    a, conv_cache = conv_forward_fast(x, w, b, conv_param)\n    out, relu_cache = relu_forward(a)\n\n    cache = (conv_cache, relu_cache)\n    return out, cache\n\n\ndef conv_relu_backward(dout, cache):\n    """"""\n    Backward pass for the conv-relu convenience layer.\n    """"""\n    conv_cache, relu_cache = cache\n    da = relu_backward(dout, relu_cache)\n    dx, dw, db = conv_backward_fast(da, conv_cache)\n    return dx, dw, db\n\n\ndef conv_relu_pool_forward(x, w, b, conv_param, pool_param):\n    """"""\n    Convenience layer that performs a convolution, a ReLU, and a pool.\n\n    Inputs:\n    - x: Input to the convolutional layer\n    - w, b, conv_param: Weights and parameters for the convolutional layer\n    - pool_param: Parameters for the pooling layer\n\n    Returns a tuple of:\n    - out: Output from the pooling layer\n    - cache: Object to give to the backward pass\n    """"""\n    a, conv_cache = conv_forward_fast(x, w, b, conv_param)\n    s, relu_cache = relu_forward(a)\n    out, pool_cache = max_pool_forward_fast(s, pool_param)\n    cache = (conv_cache, relu_cache, pool_cache)\n    return out, cache\n\n\ndef conv_relu_pool_backward(dout, cache):\n    """"""\n    Backward pass for the conv-relu-pool convenience layer\n    """"""\n    conv_cache, relu_cache, pool_cache = cache\n    ds = max_pool_backward_fast(dout, pool_cache)\n    da = relu_backward(ds, relu_cache)\n    dx, dw, db = conv_backward_fast(da, conv_cache)\n    return dx, dw, db\n\n\ndef conv_norm_relu_forward(x, w, b, conv_param, gamma, beta, bn_param):\n    """"""Convenience layer that performs a convolution, spatial\n    batchnorm, a ReLU, and a pool.\n\n    Inputs:\n    - x: Input to the convolutional layer\n    - w, b, conv_param: Weights and parameters for the convolutional layer\n    - pool_param: Parameters for the pooling layer\n\n    Returns a tuple of:\n    - out: Output from the pooling layer\n    - cache: Object to give to the backward pass\n\n    """"""\n    conv, conv_cache = conv_forward_fast(x, w, b, conv_param)\n    norm, norm_cache = spatial_batchnorm_forward(conv, gamma, beta, bn_param)\n    out, relu_cache = relu_forward(norm)\n\n    cache = (conv_cache, norm_cache, relu_cache)\n\n    return out, cache\n\n\ndef conv_norm_relu_backward(dout, cache):\n    """"""\n    Backward pass for the conv-relu-pool convenience layer\n    """"""\n    conv_cache, norm_cache, relu_cache = cache\n\n    drelu = relu_backward(dout, relu_cache)\n    dnorm, dgamma, dbeta = spatial_batchnorm_backward(drelu, norm_cache)\n    dx, dw, db = conv_backward_fast(dnorm, conv_cache)\n\n    return dx, dw, db, dgamma, dbeta\n\n\ndef conv_norm_relu_pool_forward(x, w, b, conv_param, pool_param, gamma, beta, bn_param):\n    """"""Convenience layer that performs a convolution, spatial\n    batchnorm, a ReLU, and a pool.\n\n    Inputs:\n    - x: Input to the convolutional layer\n    - w, b, conv_param: Weights and parameters for the convolutional layer\n    - pool_param: Parameters for the pooling layer\n\n    Returns a tuple of:\n    - out: Output from the pooling layer\n    - cache: Object to give to the backward pass\n\n    """"""\n    conv, conv_cache = conv_forward_fast(x, w, b, conv_param)\n    norm, norm_cache = spatial_batchnorm_forward(conv, gamma, beta, bn_param)\n    relu, relu_cache = relu_forward(norm)\n    out, pool_cache = max_pool_forward_fast(relu, pool_param)\n\n    cache = (conv_cache, norm_cache, relu_cache, pool_cache)\n\n    return out, cache\n\n\ndef conv_norm_relu_pool_backward(dout, cache):\n    """"""\n    Backward pass for the conv-relu-pool convenience layer\n    """"""\n    conv_cache, norm_cache, relu_cache, pool_cache = cache\n\n    dpool = max_pool_backward_fast(dout, pool_cache)\n    drelu = relu_backward(dpool, relu_cache)\n    dnorm, dgamma, dbeta = spatial_batchnorm_backward(drelu, norm_cache)\n    dx, dw, db = conv_backward_fast(dnorm, conv_cache)\n\n    return dx, dw, db, dgamma, dbeta\n'"
cs231n/layers.py,66,"b'import numpy as np\n\n\ndef affine_forward(x, w, b):\n    """"""\n    Computes the forward pass for an affine (fully-connected) layer.\n\n    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n    then transform it to an output vector of dimension M.\n\n    Inputs:\n    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n    - w: A numpy array of weights, of shape (D, M)\n    - b: A numpy array of biases, of shape (M,)\n\n    Returns a tuple of:\n    - out: output, of shape (N, M)\n    - cache: (x, w, b)\n    """"""\n    # dimension\n    N = x.shape[0]\n    D = np.prod(x.shape[1:])\n    x2 = np.reshape(x, (N, D))\n    out = np.dot(x2, w) + b\n    cache = (x, w, b)\n\n    return out, cache\n\n\ndef affine_backward(dout, cache):\n    """"""\n    Computes the backward pass for an affine layer.\n\n    Inputs:\n    - dout: Upstream derivative, of shape (N, M)\n    - cache: Tuple of:\n      - x: Input data, of shape (N, d_1, ... d_k)\n      - w: Weights, of shape (D, M)\n\n    Returns a tuple of:\n    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n    - dw: Gradient with respect to w, of shape (D, M)\n    - db: Gradient with respect to b, of shape (M,)\n    """"""\n    x, w, b = cache\n    dx = np.dot(dout, w.T).reshape(x.shape)\n    dw = np.dot(x.reshape(x.shape[0], np.prod(x.shape[1:])).T, dout)\n    db = np.sum(dout, axis=0)\n\n    return dx, dw, db\n\n\ndef relu_forward(x):\n    """"""\n    Computes the forward pass for a layer of rectified linear units (ReLUs).\n\n    Input:\n    - x: Inputs, of any shape\n\n    Returns a tuple of:\n    - out: Output, of the same shape as x\n    - cache: x\n    """"""\n\n    out = np.maximum(0, x)\n    cache = x\n\n    return out, cache\n\n\ndef relu_backward(dout, cache):\n    """"""\n    Computes the backward pass for a layer of rectified linear units (ReLUs).\n\n    Input:\n    - dout: Upstream derivatives, of any shape\n    - cache: Input x, of same shape as dout\n\n    Returns:\n    - dx: Gradient with respect to x\n    """"""\n\n    x = cache\n    dx = np.array(dout, copy=True)\n    dx[x <= 0] = 0\n\n    return dx\n\n\ndef batchnorm_forward(x, gamma, beta, bn_param):\n    """"""\n    Forward pass for batch normalization.\n\n    During training the sample mean and (uncorrected) sample variance are\n    computed from minibatch statistics and used to normalize the incoming data.\n    During training we also keep an exponentially decaying running mean of the mean\n    and variance of each feature, and these averages are used to normalize data\n    at test-time.\n\n    At each timestep we update the running averages for mean and variance using\n    an exponential decay based on the momentum parameter:\n\n    running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n    running_var = momentum * running_var + (1 - momentum) * sample_var\n\n    Note that the batch normalization paper suggests a different test-time\n    behavior: they compute sample mean and variance for each feature using a\n    large number of training images rather than using a running average. For\n    this implementation we have chosen to use running averages instead since\n    they do not require an additional estimation step; the torch7 implementation\n    of batch normalization also uses running averages.\n\n    Input:\n    - x: Data of shape (N, D)\n    - gamma: Scale parameter of shape (D,)\n    - beta: Shift paremeter of shape (D,)\n    - bn_param: Dictionary with the following keys:\n      - mode: \'train\' or \'test\'; required\n      - eps: Constant for numeric stability\n      - momentum: Constant for running mean / variance.\n      - running_mean: Array of shape (D,) giving running mean of features\n      - running_var Array of shape (D,) giving running variance of features\n\n    Returns a tuple of:\n    - out: of shape (N, D)\n    - cache: A tuple of values needed in the backward pass\n    """"""\n    mode = bn_param[\'mode\']\n    eps = bn_param.get(\'eps\', 1e-5)\n    momentum = bn_param.get(\'momentum\', 0.9)\n\n    N, D = x.shape\n    running_mean = bn_param.get(\'running_mean\', np.zeros(D, dtype=x.dtype))\n    running_var = bn_param.get(\'running_var\', np.zeros(D, dtype=x.dtype))\n\n    out, cache = None, None\n    if mode == \'train\':\n        #######################################################################\n        # TODO: Implement the training-time forward pass for batch normalization.   #\n        # Use minibatch statistics to compute the mean and variance, use these      #\n        # statistics to normalize the incoming data, and scale and shift the        #\n        # normalized data using gamma and beta.                                     #\n        #                                                                           #\n        # You should store the output in the variable out. Any intermediates that   #\n        # you need for the backward pass should be stored in the cache variable.    #\n        #                                                                           #\n        # You should also use your computed sample mean and variance together with  #\n        # the momentum variable to update the running mean and running variance,    #\n        # storing your result in the running_mean and running_var variables.        #\n        #######################################################################\n\n        # Forward pass\n        # Step 1 - shape of mu (D,)\n        mu = 1 / float(N) * np.sum(x, axis=0)\n\n        # Step 2 - shape of var (N,D)\n        xmu = x - mu\n\n        # Step 3 - shape of carre (N,D)\n        carre = xmu**2\n\n        # Step 4 - shape of var (D,)\n        var = 1 / float(N) * np.sum(carre, axis=0)\n\n        # Step 5 - Shape sqrtvar (D,)\n        sqrtvar = np.sqrt(var + eps)\n\n        # Step 6 - Shape invvar (D,)\n        invvar = 1. / sqrtvar\n\n        # Step 7 - Shape va2 (N,D)\n        va2 = xmu * invvar\n\n        # Step 8 - Shape va3 (N,D)\n        va3 = gamma * va2\n\n        # Step 9 - Shape out (N,D)\n        out = va3 + beta\n\n        running_mean = momentum * running_mean + (1.0 - momentum) * mu\n        running_var = momentum * running_var + (1.0 - momentum) * var\n\n        cache = (mu, xmu, carre, var, sqrtvar, invvar,\n                 va2, va3, gamma, beta, x, bn_param)\n    elif mode == \'test\':\n        #######################################################################\n        # TODO: Implement the test-time forward pass for batch normalization. Use   #\n        # the running mean and variance to normalize the incoming data, then scale  #\n        # and shift the normalized data using gamma and beta. Store the result in   #\n        # the out variable.                                                         #\n        #######################################################################\n        mu = running_mean\n        var = running_var\n        xhat = (x - mu) / np.sqrt(var + eps)\n        out = gamma * xhat + beta\n        cache = (mu, var, gamma, beta, bn_param)\n\n    else:\n        raise ValueError(\'Invalid forward batchnorm mode ""%s""\' % mode)\n\n    # Store the updated running means back into bn_param\n    bn_param[\'running_mean\'] = running_mean\n    bn_param[\'running_var\'] = running_var\n\n    return out, cache\n\n\ndef batchnorm_backward(dout, cache):\n    """"""\n    Backward pass for batch normalization.\n\n    For this implementation, you should write out a computation graph for\n    batch normalization on paper and propagate gradients backward through\n    intermediate nodes.\n\n    Inputs:\n    - dout: Upstream derivatives, of shape (N, D)\n    - cache: Variable of intermediates from batchnorm_forward.\n\n    Returns a tuple of:\n    - dx: Gradient with respect to inputs x, of shape (N, D)\n    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n    """"""\n    dx, dgamma, dbeta = None, None, None\n\n    ##########################################################################\n    # TODO: Implement the backward pass for batch normalization. Store the      #\n    # results in the dx, dgamma, and dbeta variables.                           #\n    ##########################################################################\n    mu, xmu, carre, var, sqrtvar, invvar, va2, va3, gamma, beta, x, bn_param = cache\n    eps = bn_param.get(\'eps\', 1e-5)\n    N, D = dout.shape\n\n    # Backprop Step 9\n    dva3 = dout\n    dbeta = np.sum(dout, axis=0)\n\n    # Backprop step 8\n    dva2 = gamma * dva3\n    dgamma = np.sum(va2 * dva3, axis=0)\n\n    # Backprop step 7\n    dxmu = invvar * dva2\n    dinvvar = np.sum(xmu * dva2, axis=0)\n\n    # Backprop step 6\n    dsqrtvar = -1. / (sqrtvar**2) * dinvvar\n\n    # Backprop step 5\n    dvar = 0.5 * (var + eps)**(-0.5) * dsqrtvar\n\n    # Backprop step 4\n    dcarre = 1 / float(N) * np.ones((carre.shape)) * dvar\n\n    # Backprop step 3\n    dxmu += 2 * xmu * dcarre\n\n    # Backprop step 2\n    dx = dxmu\n    dmu = - np.sum(dxmu, axis=0)\n\n    # Basckprop step 1\n    dx += 1 / float(N) * np.ones((dxmu.shape)) * dmu\n\n    return dx, dgamma, dbeta\n\n\ndef batchnorm_backward_alt(dout, cache):\n    """"""\n    Alternative backward pass for batch normalization.\n\n    For this implementation you should work out the derivatives for the batch\n    normalizaton backward pass on paper and simplify as much as possible. You\n    should be able to derive a simple expression for the backward pass.\n\n    Note: This implementation should expect to receive the same cache variable\n    as batchnorm_backward, but might not use all of the values in the cache.\n\n    Inputs / outputs: Same as batchnorm_backward\n    """"""\n    dx, dgamma, dbeta = None, None, None\n\n    ##########################################################################\n    # TODO: Implement the backward pass for batch normalization. Store the      #\n    # results in the dx, dgamma, and dbeta variables.                           #\n    #                                                                           #\n    # After computing the gradient with respect to the centered inputs, you     #\n    # should be able to compute gradients with respect to the inputs in a       #\n    # single statement; our implementation fits on a single 80-character line.  #\n    ##########################################################################\n    mu, xmu, carre, var, sqrtvar, invvar, va2, va3, gamma, beta, x, bn_param = cache\n    eps = bn_param.get(\'eps\', 1e-5)\n    N, D = dout.shape\n\n    dbeta = np.sum(dout, axis=0)\n    dgamma = np.sum((x - mu) * (var + eps)**(-1. / 2.) * dout, axis=0)\n    dx = (1. / N) * gamma * (var + eps)**(-1. / 2.) * (N * dout - np.sum(dout, axis=0)\n                                                       - (x - mu) * (var + eps)**(-1.0) * np.sum(dout * (x - mu), axis=0))\n\n    return dx, dgamma, dbeta\n\n\ndef dropout_forward(x, dropout_param):\n    """"""\n    Performs the forward pass for (inverted) dropout.\n\n    Inputs:\n    - x: Input data, of any shape\n    - dropout_param: A dictionary with the following keys:\n      - p: Dropout parameter. We drop each neuron output with probability p.\n      - mode: \'test\' or \'train\'. If the mode is train, then perform dropout;\n        if the mode is test, then just return the input.\n      - seed: Seed for the random number generator. Passing seed makes this\n        function deterministic, which is needed for gradient checking but not in\n        real networks.\n\n    Outputs:\n    - out: Array of the same shape as x.\n    - cache: A tuple (dropout_param, mask). In training mode, mask is the dropout\n      mask that was used to multiply the input; in test mode, mask is None.\n    """"""\n\n    p, mode = dropout_param[\'p\'], dropout_param[\'mode\']\n    if \'seed\' in dropout_param:\n        np.random.seed(dropout_param[\'seed\'])\n\n    mask = None\n    out = None\n\n    if mode == \'train\':\n        #######################################################################\n        # TODO: Implement the training phase forward pass for inverted dropout.   #\n        # Store the dropout mask in the mask variable.                            #\n        #######################################################################\n        mask = (np.random.rand(*x.shape) < p) / p\n        out = x * mask\n\n    elif mode == \'test\':\n        ###################################################################\n        # TODO: Implement the test phase forward pass for inverted dropout.       #\n        ###################################################################\n        mask = None\n        out = x\n\n    cache = (dropout_param, mask)\n    out = out.astype(x.dtype, copy=False)\n\n    return out, cache\n\n\ndef dropout_backward(dout, cache):\n    """"""\n    Perform the backward pass for (inverted) dropout.\n\n    Inputs:\n    - dout: Upstream derivatives, of any shape\n    - cache: (dropout_param, mask) from dropout_forward.\n    """"""\n    dropout_param, mask = cache\n    mode = dropout_param[\'mode\']\n\n    dx = None\n    if mode == \'train\':\n        #######################################################################\n        # TODO: Implement the training phase backward pass for inverted dropout.  #\n        #######################################################################\n        dx = dout * mask\n\n    elif mode == \'test\':\n        dx = dout\n    return dx\n\n\ndef conv_forward_naive(x, w, b, conv_param):\n    """"""\n    A naive implementation of the forward pass for a convolutional layer.\n\n    The input consists of N data points, each with C channels, height H and width\n    W. We convolve each input with F different filters, where each filter spans\n    all C channels and has height HH and width HH.\n\n    Input:\n    - x: Input data of shape (N, C, H, W)\n    - w: Filter weights of shape (F, C, HH, WW)\n    - b: Biases, of shape (F,)\n    - conv_param: A dictionary with the following keys:\n      - \'stride\': The number of pixels between adjacent receptive fields in the\n        horizontal and vertical directions.\n      - \'pad\': The number of pixels that will be used to zero-pad the input.\n\n    Returns a tuple of:\n    - out: Output data, of shape (N, F, H\', W\') where H\' and W\' are given by\n      H\' = 1 + (H + 2 * pad - HH) / stride\n      W\' = 1 + (W + 2 * pad - WW) / stride\n    - cache: (x, w, b, conv_param)\n    """"""\n    out = None\n    ##########################################################################\n    # TODO: Implement the convolutional forward pass.                           #\n    # Hint: you can use the function np.pad for padding.                        #\n    ##########################################################################\n    N, C, H, W = x.shape\n    F, C, HH, WW = w.shape\n    S = conv_param[\'stride\']\n    P = conv_param[\'pad\']\n\n    # Add padding to each image\n    x_pad = np.pad(x, ((0,), (0,), (P,), (P,)), \'constant\')\n    # Size of the output\n    Hh = 1 + (H + 2 * P - HH) / S\n    Hw = 1 + (W + 2 * P - WW) / S\n\n    out = np.zeros((N, F, Hh, Hw))\n\n    for n in range(N):  # First, iterate over all the images\n        for f in range(F):  # Second, iterate over all the kernels\n            for k in range(Hh):\n                for l in range(Hw):\n                    out[n, f, k, l] = np.sum(\n                        x_pad[n, :, k * S:k * S + HH, l * S:l * S + WW] * w[f, :]) + b[f]\n\n    cache = (x, w, b, conv_param)\n    return out, cache\n\n\ndef conv_backward_naive(dout, cache):\n    """"""\n    A naive implementation of the backward pass for a convolutional layer.\n\n    Inputs:\n    - dout: Upstream derivatives.\n    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n\n    Returns a tuple of:\n    - dx: Gradient with respect to x\n    - dw: Gradient with respect to w\n    - db: Gradient with respect to b\n    """"""\n    dx, dw, db = None, None, None\n    ##########################################################################\n    # TODO: Implement the convolutional backward pass.                          #\n    ##########################################################################\n    x, w, b, conv_param = cache\n    P = conv_param[\'pad\']\n    x_pad = np.pad(x, ((0,), (0,), (P,), (P,)), \'constant\')\n\n    N, C, H, W = x.shape\n    F, C, HH, WW = w.shape\n    N, F, Hh, Hw = dout.shape\n    S = conv_param[\'stride\']\n\n    # For dw: Size (C,HH,WW)\n    # Brut force love the loops !\n    dw = np.zeros((F, C, HH, WW))\n    for fprime in range(F):\n        for cprime in range(C):\n            for i in range(HH):\n                for j in range(WW):\n                    sub_xpad = x_pad[:, cprime, i:i + Hh * S:S, j:j + Hw * S:S]\n                    dw[fprime, cprime, i, j] = np.sum(\n                        dout[:, fprime, :, :] * sub_xpad)\n\n    # For db : Size (F,)\n    db = np.zeros((F))\n    for fprime in range(F):\n        db[fprime] = np.sum(dout[:, fprime, :, :])\n\n    dx = np.zeros((N, C, H, W))\n    for nprime in range(N):\n        for i in range(H):\n            for j in range(W):\n                for f in range(F):\n                    for k in range(Hh):\n                        for l in range(Hw):\n                            mask1 = np.zeros_like(w[f, :, :, :])\n                            mask2 = np.zeros_like(w[f, :, :, :])\n                            if (i + P - k * S) < HH and (i + P - k * S) >= 0:\n                                mask1[:, i + P - k * S, :] = 1.0\n                            if (j + P - l * S) < WW and (j + P - l * S) >= 0:\n                                mask2[:, :, j + P - l * S] = 1.0\n                            w_masked = np.sum(\n                                w[f, :, :, :] * mask1 * mask2, axis=(1, 2))\n                            dx[nprime, :, i, j] += dout[nprime, f, k, l] * w_masked\n\n    return dx, dw, db\n\n\ndef max_pool_forward_naive(x, pool_param):\n    """"""\n    A naive implementation of the forward pass for a max pooling layer.\n\n    Inputs:\n    - x: Input data, of shape (N, C, H, W)\n    - pool_param: dictionary with the following keys:\n      - \'pool_height\': The height of each pooling region\n      - \'pool_width\': The width of each pooling region\n      - \'stride\': The distance between adjacent pooling regions\n\n    Returns a tuple of:\n    - out: Output data (N,C,H1,W1)\n    - cache: (x, pool_param)\n\n    where H1 = (H-Hp)/S+1\n    and W1 = (W-Wp)/S+1\n\n    """"""\n\n    ##########################################################################\n    # TODO: Implement the max pooling forward pass                              #\n    ##########################################################################\n\n    Hp = pool_param[\'pool_height\']\n    Wp = pool_param[\'pool_width\']\n    S = pool_param[\'stride\']\n    N, C, H, W = x.shape\n    H1 = (H - Hp) / S + 1\n    W1 = (W - Wp) / S + 1\n\n    out = np.zeros((N, C, H1, W1))\n    for n in range(N):\n        for c in range(C):\n            for k in range(H1):\n                for l in range(W1):\n                    out[n, c, k, l] = np.max(\n                        x[n, c, k * S:k * S + Hp, l * S:l * S + Wp])\n\n    cache = (x, pool_param)\n    return out, cache\n\n\ndef max_pool_backward_naive(dout, cache):\n    """"""\n    A naive implementation of the backward pass for a max pooling layer.\n\n    Inputs:\n    - dout: Upstream derivatives\n    - cache: A tuple of (x, pool_param) as in the forward pass.\n\n    Returns:\n    - dx: Gradient with respect to x\n    """"""\n    dx = None\n    ##########################################################################\n    # TODO: Implement the max pooling backward pass                             #\n    ##########################################################################\n    x, pool_param = cache\n    Hp = pool_param[\'pool_height\']\n    Wp = pool_param[\'pool_width\']\n    S = pool_param[\'stride\']\n    N, C, H, W = x.shape\n    H1 = (H - Hp) / S + 1\n    W1 = (W - Wp) / S + 1\n\n    dx = np.zeros((N, C, H, W))\n    for nprime in range(N):\n        for cprime in range(C):\n            for k in range(H1):\n                for l in range(W1):\n                    x_pooling = x[nprime, cprime, k *\n                                  S:k * S + Hp, l * S:l * S + Wp]\n                    maxi = np.max(x_pooling)\n                    x_mask = x_pooling == maxi\n                    dx[nprime, cprime, k * S:k * S + Hp, l * S:l *\n                        S + Wp] += dout[nprime, cprime, k, l] * x_mask\n    return dx\n\n\ndef spatial_batchnorm_forward(x, gamma, beta, bn_param):\n    """"""\n    Computes the forward pass for spatial batch normalization.\n\n    Inputs:\n    - x: Input data of shape (N, C, H, W)\n    - gamma: Scale parameter, of shape (C,)\n    - beta: Shift parameter, of shape (C,)\n    - bn_param: Dictionary with the following keys:\n      - mode: \'train\' or \'test\'; required\n      - eps: Constant for numeric stability\n      - momentum: Constant for running mean / variance. momentum=0 means that\n        old information is discarded completely at every time step, while\n        momentum=1 means that new information is never incorporated. The\n        default of momentum=0.9 should work well in most situations.\n      - running_mean: Array of shape (D,) giving running mean of features\n      - running_var Array of shape (D,) giving running variance of features\n\n    Returns a tuple of:\n    - out: Output data, of shape (N, C, H, W)\n    - cache: Values needed for the backward pass\n    """"""\n    out, cache = None, None\n\n    ##########################################################################\n    # TODO: Implement the forward pass for spatial batch normalization.         #\n    #                                                                           #\n    # HINT: You can implement spatial batch normalization using the vanilla     #\n    # version of batch normalization defined above. Your implementation should  #\n    # be very short; ours is less than five lines.                              #\n    ##########################################################################\n\n    N, C, H, W = x.shape\n    mode = bn_param[\'mode\']\n    eps = bn_param.get(\'eps\', 1e-5)\n    momentum = bn_param.get(\'momentum\', 0.9)\n\n    running_mean = bn_param.get(\'running_mean\', np.zeros(C, dtype=x.dtype))\n    running_var = bn_param.get(\'running_var\', np.zeros(C, dtype=x.dtype))\n\n    if mode == \'train\':\n        # Step 1 , calcul the average for each channel\n        mu = (1. / (N * H * W) * np.sum(x, axis=(0, 2, 3))).reshape(1, C, 1, 1)\n        var = (1. / (N * H * W) * np.sum((x - mu)**2,\n                                         axis=(0, 2, 3))).reshape(1, C, 1, 1)\n        xhat = (x - mu) / (np.sqrt(eps + var))\n        out = gamma.reshape(1, C, 1, 1) * xhat + beta.reshape(1, C, 1, 1)\n\n        running_mean = momentum * running_mean + \\\n            (1.0 - momentum) * np.squeeze(mu)\n        running_var = momentum * running_var + \\\n            (1.0 - momentum) * np.squeeze(var)\n\n        cache = (mu, var, x, xhat, gamma, beta, bn_param)\n\n        # Store the updated running means back into bn_param\n        bn_param[\'running_mean\'] = running_mean\n        bn_param[\'running_var\'] = running_var\n\n    elif mode == \'test\':\n        mu = running_mean.reshape(1, C, 1, 1)\n        var = running_var.reshape(1, C, 1, 1)\n\n        xhat = (x - mu) / (np.sqrt(eps + var))\n        out = gamma.reshape(1, C, 1, 1) * xhat + beta.reshape(1, C, 1, 1)\n        cache = (mu, var, x, xhat, gamma, beta, bn_param)\n\n    else:\n        raise ValueError(\'Invalid forward batchnorm mode ""%s""\' % mode)\n\n    return out, cache\n\n\ndef spatial_batchnorm_backward(dout, cache):\n    """"""\n    Computes the backward pass for spatial batch normalization.\n\n    Inputs:\n    - dout: Upstream derivatives, of shape (N, C, H, W)\n    - cache: Values from the forward pass\n\n    Returns a tuple of:\n    - dx: Gradient with respect to inputs, of shape (N, C, H, W)\n    - dgamma: Gradient with respect to scale parameter, of shape (C,)\n    - dbeta: Gradient with respect to shift parameter, of shape (C,)\n    """"""\n    dx, dgamma, dbeta = None, None, None\n\n    ##########################################################################\n    # TODO: Implement the backward pass for spatial batch normalization.        #\n    #                                                                           #\n    # HINT: You can implement spatieal batch normalization using the vanilla     #\n    # version of batch normalization defined above. Your implementation should  #\n    # be very short; ours is less than five lines.                              #\n    ##########################################################################\n\n    mu, var, x, xhat, gamma, beta, bn_param = cache\n    N, C, H, W = x.shape\n    mode = bn_param[\'mode\']\n    eps = bn_param.get(\'eps\', 1e-5)\n\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta = beta.reshape(1, C, 1, 1)\n\n    dbeta = np.sum(dout, axis=(0, 2, 3))\n    dgamma = np.sum(dout * xhat, axis=(0, 2, 3))\n\n    Nt = N * H * W\n    dx = (1. / Nt) * gamma * (var + eps)**(-1. / 2.) * (\n        Nt * dout\n        - np.sum(dout, axis=(0, 2, 3)).reshape(1, C, 1, 1)\n        - (x - mu) * (var + eps)**(-1.0) * np.sum(dout * (x - mu), axis=(0, 2, 3)).reshape(1, C, 1, 1))\n\n    return dx, dgamma, dbeta\n\n\ndef svm_loss(x, y):\n    """"""\n    Computes the loss and gradient using for multiclass SVM classification.\n\n    Inputs:\n    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n      for the ith input.\n    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n      0 <= y[i] < C\n\n    Returns a tuple of:\n    - loss: Scalar giving the loss\n    - dx: Gradient of the loss with respect to x\n    """"""\n    N = x.shape[0]\n    correct_class_scores = x[np.arange(N), y]\n    margins = np.maximum(0, x - correct_class_scores[:, np.newaxis] + 1.0)\n    margins[np.arange(N), y] = 0\n    loss = np.sum(margins) / N\n    num_pos = np.sum(margins > 0, axis=1)\n    dx = np.zeros_like(x)\n    dx[margins > 0] = 1\n    dx[np.arange(N), y] -= num_pos\n    dx /= N\n    return loss, dx\n\n\ndef softmax_loss(x, y):\n    """"""\n    Computes the loss and gradient for softmax classification.\n\n    Inputs:\n    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n      for the ith input.\n    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n      0 <= y[i] < C\n\n    Returns a tuple of:\n    - loss: Scalar giving the loss\n    - dx: Gradient of the loss with respect to x\n    """"""\n    probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n    probs /= np.sum(probs, axis=1, keepdims=True)\n    N = x.shape[0]\n    loss = -np.sum(np.log(probs[np.arange(N), y])) / N\n    dx = probs.copy()\n    dx[np.arange(N), y] -= 1\n    dx /= N\n    return loss, dx\n'"
cs231n/optim.py,6,"b'import numpy as np\n\n""""""\nThis file implements various first-order update rules that are commonly used for\ntraining neural networks. Each update rule accepts current weights and the\ngradient of the loss with respect to those weights and produces the next set of\nweights. Each update rule has the same interface:\n\ndef update(w, dw, config=None):\n\nInputs:\n  - w: A numpy array giving the current weights.\n  - dw: A numpy array of the same shape as w giving the gradient of the\n    loss with respect to w.\n  - config: A dictionary containing hyperparameter values such as learning rate,\n    momentum, etc. If the update rule requires caching values over many\n    iterations, then config will also hold these cached values.\n\nReturns:\n  - next_w: The next point after the update.\n  - config: The config dictionary to be passed to the next iteration of the\n    update rule.\n\nNOTE: For most update rules, the default learning rate will probably not perform\nwell; however the default values of the other hyperparameters should work well\nfor a variety of different problems.\n\nFor efficiency, update rules may perform in-place updates, mutating w and\nsetting next_w equal to w.\n""""""\n\n\ndef sgd(w, dw, config=None):\n    """"""\n    Performs vanilla stochastic gradient descent.\n\n    config format:\n    - learning_rate: Scalar learning rate.\n    """"""\n    if config is None:\n        config = {}\n    config.setdefault(\'learning_rate\', 1e-2)\n\n    w -= config[\'learning_rate\'] * dw\n    return w, config\n\n\ndef sgd_momentum(w, dw, config=None):\n    """"""\n    Performs stochastic gradient descent with momentum.\n\n    config format:\n    - learning_rate: Scalar learning rate.\n    - momentum: Scalar between 0 and 1 giving the momentum value.\n      Setting momentum = 0 reduces to sgd.\n    - velocity: A numpy array of the same shape as w and dw used to store a moving\n      average of the gradients.\n\n    """"""\n\n    if config is None:\n        config = {}\n    config.setdefault(\'learning_rate\', 1e-2)\n    config.setdefault(\'momentum\', 0.9)\n    v = config.get(\'velocity\', np.zeros_like(w))\n    next_v = config[\'momentum\'] * v - config[\'learning_rate\'] * dw\n    next_w = w + next_v\n\n    ##########################################################################\n    # TODO: Implement the momentum update formula. Store the updated value in   #\n    # the next_w variable. You should also use and update the velocity v.       #\n    ##########################################################################\n    pass\n    ##########################################################################\n    #                             END OF YOUR CODE                              #\n    ##########################################################################\n    config[\'velocity\'] = next_v\n\n    return next_w, config\n\n\ndef rmsprop(x, dx, config=None):\n    """"""\n    Uses the RMSProp update rule, which uses a moving average of squared gradient\n    values to set adaptive per-parameter learning rates.\n\n    config format:\n    - learning_rate: Scalar learning rate.\n    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared\n      gradient cache.\n    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n    - cache: Moving average of second moments of gradients.\n    """"""\n\n    if config is None:\n        config = {}\n    config.setdefault(\'learning_rate\', 1e-2)\n    config.setdefault(\'decay_rate\', 0.99)\n    config.setdefault(\'epsilon\', 1e-8)\n    config.setdefault(\'cache\', np.zeros_like(x))\n\n    config[\'cache\'] = config[\'cache\'] * config[\'decay_rate\'] +\\\n        (1 - config[\'decay_rate\']) * dx**2\n    next_x = x - config[\'learning_rate\'] * dx / (np.sqrt(config[\'cache\']\n                                                         + config[\'epsilon\']))\n\n    ##########################################################################\n    # TODO: Implement the RMSprop update formula, storing the next value of x   #\n    # in the next_x variable. Don\'t forget to update cache value stored in      #\n    # config[\'cache\'].                                                          #\n    ##########################################################################\n    pass\n    ##########################################################################\n    #                             END OF YOUR CODE                              #\n    ##########################################################################\n\n    return next_x, config\n\n\ndef adam(x, dx, config=None):\n    """"""\n    Uses the Adam update rule, which incorporates moving averages of both the\n    gradient and its square and a bias correction term.\n\n    config format:\n    - learning_rate: Scalar learning rate.\n    - beta1: Decay rate for moving average of first moment of gradient.\n    - beta2: Decay rate for moving average of second moment of gradient.\n    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n    - m: Moving average of gradient.\n    - v: Moving average of squared gradient.\n    - t: Iteration number.\n    """"""\n    if config is None:\n        config = {}\n    config.setdefault(\'learning_rate\', 1e-3)\n    config.setdefault(\'beta1\', 0.9)\n    config.setdefault(\'beta2\', 0.999)\n    config.setdefault(\'epsilon\', 1e-8)\n    config.setdefault(\'m\', np.zeros_like(x))\n    config.setdefault(\'v\', np.zeros_like(x))\n    config.setdefault(\'t\', 0)\n\n    learning_rate = config[\'learning_rate\']\n    beta1 = config[\'beta1\']\n    beta2 = config[\'beta2\']\n    epsilon = config[\'epsilon\']\n\n    # Value after the update\n    config[\'t\'] += 1\n    config[\'m\'] = beta1 * config[\'m\'] + (1 - beta1) * dx\n    config[\'v\'] = beta2 * config[\'v\'] + (1 - beta2) * dx**2\n    mt_hat = config[\'m\'] / (1 - (beta1)**config[\'t\'])\n    vt_hat = config[\'v\'] / (1 - (beta2)**config[\'t\'])\n    next_x = x - learning_rate * mt_hat / (np.sqrt(vt_hat + epsilon))\n\n    ##########################################################################\n    # TODO: Implement the Adam update formula, storing the next value of x in   #\n    # the next_x variable. Don\'t forget to update the m, v, and t variables     #\n    # stored in config.                                                         #\n    ##########################################################################\n    pass\n    ##########################################################################\n    #                             END OF YOUR CODE                              #\n    ##########################################################################\n\n    return next_x, config\n'"
cs231n/setup.py,0,"b""from distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Build import cythonize\nimport numpy\n\nextensions = [\n  Extension('im2col_cython', ['im2col_cython.pyx'],\n            include_dirs = [numpy.get_include()]\n  ),\n]\n\nsetup(\n    ext_modules = cythonize(extensions),\n)\n"""
cs231n/solver.py,10,"b'import numpy as np\nimport os\nfrom sklearn.externals import joblib\nfrom cs231n import optim\n\n\nclass SolverCheckpoints(object):\n    """"""\n    A Solver encapsulates all the logic necessary for training classification\n    models. The Solver performs stochastic gradient descent using different\n    update rules defined in optim.py.\n\n    The solver accepts both training and validataion data and labels so it can\n    periodically check classification accuracy on both training and validation\n    data to watch out for overfitting.\n\n    To train a model, you will first construct a Solver instance, passing the\n    model, dataset, and various optoins (learning rate, batch size, etc) to the\n    constructor. You will then call the train() method to run the optimization\n    procedure and train the model.\n\n    After the train() method returns, model.params will contain the parameters\n    that performed best on the validation set over the course of training.\n    In addition, the instance variable solver.loss_history will contain a list\n    of all losses encountered during training and the instance variables\n    solver.train_acc_history and solver.val_acc_history will be lists containing\n    the accuracies of the model on the training and validation set at each epoch.\n\n    Example usage might look something like this:\n\n    data = {\n      \'X_train\': # training data\n      \'y_train\': # training labels\n      \'X_val\': # validation data\n      \'X_train\': # validation labels\n    }\n    model = MyAwesomeModel(hidden_size=100, reg=10)\n    solver = Solver(model, data,\n                    update_rule=\'sgd\',\n                    optim_config={\n                      \'learning_rate\': 1e-3,\n                    },\n                    lr_decay=0.95,\n                    num_epochs=10, batch_size=100,\n                    print_every=100)\n    solver.train()\n\n\n    A Solver works on a model object that must conform to the following API:\n\n    - model.params must be a dictionary mapping string parameter names to numpy\n      arrays containing parameter values.\n\n    - model.loss(X, y) must be a function that computes training-time loss and\n      gradients, and test-time classification scores, with the following inputs\n      and outputs:\n\n      Inputs:\n      - X: Array giving a minibatch of input data of shape (N, d_1, ..., d_k)\n      - y: Array of labels, of shape (N,) giving labels for X where y[i] is the\n        label for X[i].\n\n      Returns:\n      If y is None, run a test-time forward pass and return:\n      - scores: Array of shape (N, C) giving classification scores for X where\n        scores[i, c] gives the score of class c for X[i].\n\n      If y is not None, run a training time forward and backward pass and return\n      a tuple of:\n      - loss: Scalar giving the loss\n      - grads: Dictionary with the same keys as self.params mapping parameter\n        names to gradients of the loss with respect to those parameters.\n    """"""\n\n    def __init__(self, model, data, path,  **kwargs):\n        """"""\n        Construct a new Solver instance.\n\n        Required arguments:\n        - model: A model object conforming to the API described above\n        - data: A dictionary of training and validation data with the following:\n          \'X_train\': Array of shape (N_train, d_1, ..., d_k) giving training images\n          \'X_val\': Array of shape (N_val, d_1, ..., d_k) giving validation images\n          \'y_train\': Array of shape (N_train,) giving labels for training images\n          \'y_val\': Array of shape (N_val,) giving labels for validation images\n\n        Optional arguments:\n        - update_rule: A string giving the name of an update rule in optim.py.\n          Default is \'sgd\'.\n        - optim_config: A dictionary containing hyperparameters that will be\n          passed to the chosen update rule. Each update rule requires different\n          hyperparameters (see optim.py) but all update rules require a\n          \'learning_rate\' parameter so that should always be present.\n        - lr_decay: A scalar for learning rate decay; after each epoch the learning\n          rate is multiplied by this value.\n        - batch_size: Size of minibatches used to compute loss and gradient during\n          training.\n        - num_epochs: The number of epochs to run for during training.\n        - print_every: Integer; training losses will be printed every print_every\n          iterations.\n        - verbose: Boolean; if set to false then no output will be printed during\n          training.\n        """"""\n\n        self.model = model\n        self.path = path\n        self.path_checkpoints = os.path.join(self.path, \'checkpoints\')\n        self.X_train = data[\'X_train\']\n        self.y_train = data[\'y_train\']\n        self.X_val = data[\'X_val\']\n        self.y_val = data[\'y_val\']\n\n        # Unpack keyword arguments\n        self.update_rule = kwargs.pop(\'update_rule\', \'sgd\')\n        self.optim_config = kwargs.pop(\'optim_config\', {})\n        self.lr_decay = kwargs.pop(\'lr_decay\', 1.0)\n        self.batch_size = kwargs.pop(\'batch_size\', 100)\n        self.num_epochs = kwargs.pop(\'num_epochs\', 10)\n\n        self.check_points_every = kwargs.pop(\'check_points_every\', 1)\n        self.print_every = kwargs.pop(\'print_every\', 10)\n        self.verbose = kwargs.pop(\'verbose\', True)\n\n        # Throw an error if there are extra keyword arguments\n        if len(kwargs) > 0:\n            extra = \', \'.join(\'""%s""\' % k for k in kwargs.keys())\n            raise ValueError(\'Unrecognized arguments %s\' % extra)\n\n        # Make sure the update rule exists, then replace the string\n        # name with the actual function\n        if not hasattr(optim, self.update_rule):\n            raise ValueError(\'Invalid update_rule ""%s""\' % self.update_rule)\n        self.update_rule = getattr(optim, self.update_rule)\n\n        self._reset()\n\n    def _reset(self):\n        """"""\n        Set up some book-keeping variables for optimization. Don\'t call this\n        manually.\n        """"""\n        # Set up some variables for book-keeping\n        self.epoch = 0\n        self.best_val_acc = 0\n        self.best_params = {}\n        self.loss_history = []\n        self.train_acc_history = []\n        self.val_acc_history = []\n\n        # Make a deep copy of the optim_config for each parameter\n        self.optim_configs = {}\n        for p in self.model.params:\n            d = {k: v for k, v in self.optim_config.items()}\n            self.optim_configs[p] = d\n\n    def _step(self):\n        """"""\n        Make a single gradient update. This is called by train() and should not\n        be called manually.\n        """"""\n        # Make a minibatch of training data\n        num_train = self.X_train.shape[0]\n        batch_mask = np.random.choice(num_train, self.batch_size)\n        X_batch = self.X_train[batch_mask]\n        y_batch = self.y_train[batch_mask]\n\n        # Compute loss and gradient\n        loss, grads = self.model.loss(X_batch, y_batch)\n        self.loss_history.append(loss)\n\n        # Perform a parameter update\n        for p, w in self.model.params.items():\n            dw = grads[p]\n            config = self.optim_configs[p]\n            next_w, next_config = self.update_rule(w, dw, config)\n            self.model.params[p] = next_w\n            self.optim_configs[p] = next_config\n\n    def load_current_checkpoints(self):\n        \'\'\' Return the current checkpoint \'\'\'\n\n        checkpoints = os.listdir(self.path_checkpoints)\n        num = max([int(f.split(\'_\')[1]) for f in checkpoints])\n        name = \'check_\' + str(num)\n        return num, joblib.load(os.path.join(self.path_checkpoints, name, name + \'.pkl\'))\n\n    def make_check_point(self):\n\n        num, last_checkpoints = self.load_current_checkpoints()\n\n        if self.best_val_acc > last_checkpoints[\'best_val_acc\']:\n            best_val_acc = self.best_val_acc\n            best_params = self.best_params\n        else:\n            best_val_acc = last_checkpoints[\'best_val_acc\']\n            best_params = last_checkpoints[\'best_params\']\n\n        checkpoints = {\n            \'model\': self.model,\n            \'epoch\': self.epoch,\n            \'best_params\': best_params,\n            \'best_val_acc\': best_val_acc,\n            \'loss_history\': self.loss_history,\n            \'train_acc_history\': self.train_acc_history,\n            \'val_acc_history\': self.val_acc_history}\n\n        name = \'check_\' + str(num + 1)\n        os.mkdir(os.path.join(self.path_checkpoints, name))\n        joblib.dump(checkpoints, os.path.join(\n            self.path_checkpoints, name, name + \'.pkl\'))\n\n    def check_accuracy(self, X, y, num_samples=None, batch_size=100):\n        """"""\n        Check accuracy of the model on the provided data.\n\n        Inputs:\n        - X: Array of data, of shape (N, d_1, ..., d_k)\n        - y: Array of labels, of shape (N,)\n        - num_samples: If not None, subsample the data and only test the model\n          on num_samples datapoints.\n        - batch_size: Split X and y into batches of this size to avoid using too\n          much memory.\n\n        Returns:\n        - acc: Scalar giving the fraction of instances that were correctly\n          classified by the model.\n        """"""\n\n        # Maybe subsample the data\n        N = X.shape[0]\n        if num_samples is not None and N > num_samples:\n            mask = np.random.choice(N, num_samples)\n            N = num_samples\n            X = X[mask]\n            y = y[mask]\n\n        # Compute predictions in batches\n        num_batches = N / batch_size\n        if N % batch_size != 0:\n            num_batches += 1\n        y_pred = []\n        for i in range(num_batches):\n            start = i * batch_size\n            end = (i + 1) * batch_size\n            scores = self.model.loss(X[start:end])\n            y_pred.append(np.argmax(scores, axis=1))\n        y_pred = np.hstack(y_pred)\n        acc = np.mean(y_pred == y)\n\n        return acc\n\n    def train(self):\n        """"""\n        Run optimization to train the model.\n        """"""\n        num_train = self.X_train.shape[0]\n        iterations_per_epoch = max(num_train / self.batch_size, 1)\n        num_iterations = self.num_epochs * iterations_per_epoch\n\n        for t in range(num_iterations):\n            self._step()\n\n            # Maybe print training loss\n            if self.verbose and t % self.print_every == 0:\n                print(\'(Iteration %d / %d) loss: %f\' % (\n                    t + 1, num_iterations, self.loss_history[-1]))\n\n            # At the end of every epoch, increment the epoch counter and decay the\n            # learning rate.\n            epoch_end = (t + 1) % iterations_per_epoch == 0\n            if epoch_end:\n                self.epoch += 1\n                for k in self.optim_configs:\n                    self.optim_configs[k][\'learning_rate\'] *= self.lr_decay\n\n            # Check train and val accuracy on the first iteration, the last\n            # iteration, and at the end of each epoch.\n            first_it = (t == 0)\n            last_it = (t == num_iterations + 1)\n            if first_it or last_it or epoch_end:\n                train_acc = self.check_accuracy(self.X_train, self.y_train,\n                                                num_samples=1000)\n                val_acc = self.check_accuracy(self.X_val, self.y_val)\n                self.train_acc_history.append(train_acc)\n                self.val_acc_history.append(val_acc)\n\n                if self.verbose:\n                    print(\'(Epoch %d / %d) train acc: %f; val_acc: %f\' % (\n                        self.epoch, self.num_epochs, train_acc, val_acc))\n\n                # Keep track of the best model\n                if val_acc > self.best_val_acc:\n                    self.best_val_acc = val_acc\n                    self.best_params = {}\n                    for k, v in self.model.params.items():\n                        self.best_params[k] = v.copy()\n\n            # Make a checkpoint of the model every check_points_every epoch\n            if epoch_end:\n                self.make_check_point()\n\n        # At the end of training swap the best params into the model\n        self.model.params = self.best_params\n\n\nclass Solver(object):\n    """"""\n    A Solver encapsulates all the logic necessary for training classification\n    models. The Solver performs stochastic gradient descent using different\n    update rules defined in optim.py.\n\n    The solver accepts both training and validataion data and labels so it can\n    periodically check classification accuracy on both training and validation\n    data to watch out for overfitting.\n\n    To train a model, you will first construct a Solver instance, passing the\n    model, dataset, and various optoins (learning rate, batch size, etc) to the\n    constructor. You will then call the train() method to run the optimization\n    procedure and train the model.\n\n    After the train() method returns, model.params will contain the parameters\n    that performed best on the validation set over the course of training.\n    In addition, the instance variable solver.loss_history will contain a list\n    of all losses encountered during training and the instance variables\n    solver.train_acc_history and solver.val_acc_history will be lists containing\n    the accuracies of the model on the training and validation set at each epoch.\n\n    Example usage might look something like this:\n\n    data = {\n      \'X_train\': # training data\n      \'y_train\': # training labels\n      \'X_val\': # validation data\n      \'X_train\': # validation labels\n    }\n    model = MyAwesomeModel(hidden_size=100, reg=10)\n    solver = Solver(model, data,\n                    update_rule=\'sgd\',\n                    optim_config={\n                      \'learning_rate\': 1e-3,\n                    },\n                    lr_decay=0.95,\n                    num_epochs=10, batch_size=100,\n                    print_every=100)\n    solver.train()\n\n\n    A Solver works on a model object that must conform to the following API:\n\n    - model.params must be a dictionary mapping string parameter names to numpy\n      arrays containing parameter values.\n\n    - model.loss(X, y) must be a function that computes training-time loss and\n      gradients, and test-time classification scores, with the following inputs\n      and outputs:\n\n      Inputs:\n      - X: Array giving a minibatch of input data of shape (N, d_1, ..., d_k)\n      - y: Array of labels, of shape (N,) giving labels for X where y[i] is the\n        label for X[i].\n\n      Returns:\n      If y is None, run a test-time forward pass and return:\n      - scores: Array of shape (N, C) giving classification scores for X where\n        scores[i, c] gives the score of class c for X[i].\n\n      If y is not None, run a training time forward and backward pass and return\n      a tuple of:\n      - loss: Scalar giving the loss\n      - grads: Dictionary with the same keys as self.params mapping parameter\n        names to gradients of the loss with respect to those parameters.\n    """"""\n\n    def __init__(self, model, data,   **kwargs):\n        """"""\n        Construct a new Solver instance.\n\n        Required arguments:\n        - model: A model object conforming to the API described above\n        - data: A dictionary of training and validation data with the following:\n          \'X_train\': Array of shape (N_train, d_1, ..., d_k) giving training images\n          \'X_val\': Array of shape (N_val, d_1, ..., d_k) giving validation images\n          \'y_train\': Array of shape (N_train,) giving labels for training images\n          \'y_val\': Array of shape (N_val,) giving labels for validation images\n\n        Optional arguments:\n        - update_rule: A string giving the name of an update rule in optim.py.\n          Default is \'sgd\'.\n        - optim_config: A dictionary containing hyperparameters that will be\n          passed to the chosen update rule. Each update rule requires different\n          hyperparameters (see optim.py) but all update rules require a\n          \'learning_rate\' parameter so that should always be present.\n        - lr_decay: A scalar for learning rate decay; after each epoch the learning\n          rate is multiplied by this value.\n        - batch_size: Size of minibatches used to compute loss and gradient during\n          training.\n        - num_epochs: The number of epochs to run for during training.\n        - print_every: Integer; training losses will be printed every print_every\n          iterations.\n        - verbose: Boolean; if set to false then no output will be printed during\n          training.\n        """"""\n\n        self.model = model\n        self.X_train = data[\'X_train\']\n        self.y_train = data[\'y_train\']\n        self.X_val = data[\'X_val\']\n        self.y_val = data[\'y_val\']\n\n        # Unpack keyword arguments\n        self.update_rule = kwargs.pop(\'update_rule\', \'sgd\')\n        self.optim_config = kwargs.pop(\'optim_config\', {})\n        self.lr_decay = kwargs.pop(\'lr_decay\', 0.95)\n        self.batch_size = kwargs.pop(\'batch_size\', 100)\n        self.num_epochs = kwargs.pop(\'num_epochs\', 10)\n\n        self.check_points_every = kwargs.pop(\'check_point_every\', 1)\n        self.print_every = kwargs.pop(\'print_every\', 10)\n        self.verbose = kwargs.pop(\'verbose\', True)\n\n        # Throw an error if there are extra keyword arguments\n        if len(kwargs) > 0:\n            extra = \', \'.join(\'""%s""\' % k for k in kwargs.keys())\n            raise ValueError(\'Unrecognized arguments %s\' % extra)\n\n        # Make sure the update rule exists, then replace the string\n        # name with the actual function\n        if not hasattr(optim, self.update_rule):\n            raise ValueError(\'Invalid update_rule ""%s""\' % self.update_rule)\n        self.update_rule = getattr(optim, self.update_rule)\n\n        self._reset()\n\n    def _reset(self):\n        """"""\n        Set up some book-keeping variables for optimization. Don\'t call this\n        manually.\n        """"""\n        # Set up some variables for book-keeping\n        self.epoch = 0\n        self.best_val_acc = 0\n        self.best_params = {}\n        self.loss_history = []\n        self.train_acc_history = []\n        self.val_acc_history = []\n\n        # Make a deep copy of the optim_config for each parameter\n        self.optim_configs = {}\n        for p in self.model.params:\n            d = {k: v for k, v in self.optim_config.items()}\n            self.optim_configs[p] = d\n\n    def _step(self):\n        """"""\n        Make a single gradient update. This is called by train() and should not\n        be called manually.\n        """"""\n        # Make a minibatch of training data\n        num_train = self.X_train.shape[0]\n        batch_mask = np.random.choice(num_train, self.batch_size)\n        X_batch = self.X_train[batch_mask]\n        y_batch = self.y_train[batch_mask]\n\n        # Compute loss and gradient\n        loss, grads = self.model.loss(X_batch, y_batch)\n        self.loss_history.append(loss)\n\n        # Perform a parameter update\n        for p, w in self.model.params.items():\n            dw = grads[p]\n            config = self.optim_configs[p]\n            next_w, next_config = self.update_rule(w, dw, config)\n            self.model.params[p] = next_w\n            self.optim_configs[p] = next_config\n\n    def check_accuracy(self, X, y, num_samples=None, batch_size=100):\n        """"""\n        Check accuracy of the model on the provided data.\n\n        Inputs:\n        - X: Array of data, of shape (N, d_1, ..., d_k)\n        - y: Array of labels, of shape (N,)\n        - num_samples: If not None, subsample the data and only test the model\n          on num_samples datapoints.\n        - batch_size: Split X and y into batches of this size to avoid using too\n          much memory.\n\n        Returns:\n        - acc: Scalar giving the fraction of instances that were correctly\n          classified by the model.\n        """"""\n\n        # Maybe subsample the data\n        N = X.shape[0]\n        if num_samples is not None and N > num_samples:\n            mask = np.random.choice(N, num_samples)\n            N = num_samples\n            X = X[mask]\n            y = y[mask]\n\n        # Compute predictions in batches\n        num_batches = N / batch_size\n        if N % batch_size != 0:\n            num_batches += 1\n        y_pred = []\n        for i in range(int(num_batches)):\n            start = i * batch_size\n            end = (i + 1) * batch_size\n            scores = self.model.loss(X[start:end])\n            y_pred.append(np.argmax(scores, axis=1))\n        y_pred = np.hstack(y_pred)\n        acc = np.mean(y_pred == y)\n\n        return acc\n\n    def train(self):\n        """"""\n        Run optimization to train the model.\n        """"""\n        num_train = self.X_train.shape[0]\n        iterations_per_epoch = max(num_train / self.batch_size, 1)\n        num_iterations = self.num_epochs * iterations_per_epoch\n\n        for t in range(int(num_iterations)):\n            self._step()\n\n            # Maybe print training loss\n            if self.verbose and t % self.print_every == 0:\n                print(\'(Iteration %d / %d) loss: %f\' % (\n                    t + 1, num_iterations, self.loss_history[-1]))\n\n            # At the end of every epoch, increment the epoch counter and decay the\n            # learning rate.\n            epoch_end = (t + 1) % iterations_per_epoch == 0\n            if epoch_end:\n                self.epoch += 1\n                for k in self.optim_configs:\n                    self.optim_configs[k][\'learning_rate\'] *= self.lr_decay\n\n            # Check train and val accuracy on the first iteration, the last\n            # iteration, and at the end of each epoch.\n            first_it = (t == 0)\n            last_it = (t == num_iterations + 1)\n            if first_it or last_it or epoch_end:\n                train_acc = self.check_accuracy(self.X_train, self.y_train,\n                                                num_samples=1000)\n                val_acc = self.check_accuracy(self.X_val, self.y_val)\n                self.train_acc_history.append(train_acc)\n                self.val_acc_history.append(val_acc)\n\n                if self.verbose:\n                    print(\'(Epoch %d / %d) train acc: %f; val_acc: %f\' % (\n                        self.epoch, self.num_epochs, train_acc, val_acc))\n\n                # Keep track of the best model\n                if val_acc > self.best_val_acc:\n                    self.best_val_acc = val_acc\n                    self.best_params = {}\n                    for k, v in self.model.params.items():\n                        self.best_params[k] = v.copy()\n\n        # At the end of training swap the best params into the model\n        self.model.params = self.best_params\n'"
cs231n/vis_utils.py,7,"b'from math import sqrt, ceil\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pylab as plt\n\n\ndef visualize_grid(Xs, ubound=255.0, padding=1):\n    """"""\n    Reshape a 4D tensor of image data to a grid for easy visualization.\n\n    Inputs:\n    - Xs: Data of shape (N, H, W, C)\n    - ubound: Output grid will have values scaled to the range [0, ubound]\n    - padding: The number of blank pixels between elements of the grid\n    """"""\n    (N, H, W, C) = Xs.shape\n    grid_size = int(ceil(sqrt(N)))\n    grid_height = H * grid_size + padding * (grid_size - 1)\n    grid_width = W * grid_size + padding * (grid_size - 1)\n    grid = np.zeros((grid_height, grid_width, C))\n    next_idx = 0\n    y0, y1 = 0, H\n    for y in xrange(grid_size):\n        x0, x1 = 0, W\n        for x in xrange(grid_size):\n            if next_idx < N:\n                img = Xs[next_idx]\n                low, high = np.min(img), np.max(img)\n                grid[y0:y1, x0:x1] = ubound * (img - low) / (high - low)\n                # grid[y0:y1, x0:x1] = Xs[next_idx]\n                next_idx += 1\n            x0 += W + padding\n            x1 += W + padding\n        y0 += H + padding\n        y1 += H + padding\n    # grid_max = np.max(grid)\n    # grid_min = np.min(grid)\n    # grid = ubound * (grid - grid_min) / (grid_max - grid_min)\n    return grid\n\n\ndef vis_grid(Xs):\n    """""" visualize a grid of images """"""\n    (N, H, W, C) = Xs.shape\n    A = int(ceil(sqrt(N)))\n    G = np.ones((A * H + A, A * W + A, C), Xs.dtype)\n    G *= np.min(Xs)\n    n = 0\n    for y in range(A):\n        for x in range(A):\n            if n < N:\n                G[y * H + y:(y + 1) * H + y, x * W + x:(x + 1)\n                  * W + x, :] = Xs[n, :, :, :]\n                n += 1\n    # normalize to [0,1]\n    maxg = G.max()\n    ming = G.min()\n    G = (G - ming) / (maxg - ming)\n    return G\n\n\ndef vis_nn(rows):\n    """""" visualize array of arrays of images """"""\n    N = len(rows)\n    D = len(rows[0])\n    H, W, C = rows[0][0].shape\n    Xs = rows[0][0]\n    G = np.ones((N * H + N, D * W + D, C), Xs.dtype)\n    for y in range(N):\n        for x in range(D):\n            G[y * H + y:(y + 1) * H + y, x * W + x:(x + 1)\n              * W + x, :] = rows[y][x]\n    # normalize to [0,1]\n    maxg = G.max()\n    ming = G.min()\n    G = (G - ming) / (maxg - ming)\n    return G\n\n\ndef inspect_checkpoint(checks):\n    \'\'\' solvers is a dict with key:solver.\n    Return a plot with the different loss on of top of another \'\'\'\n\n    fig = plt.figure(figsize=(10, 30))\n    height = len(checks) + 1\n    ax = plt.subplot(height, 1, 1)\n    axs = [plt.subplot(height, 1, i + 2) for i in range(len(checks))]\n\n    i = 0\n    for key, check in checks.iteritems():\n        ax.plot(check[\'loss_history\'], label=key)\n        ax.legend(fontsize=24)\n        ax.tick_params(labelsize=24)\n        ax.set_xlabel(\'epoch\')\n        ax.set_ylabel(\'accuracy\')\n\n        axs[i].plot(check[\'train_acc_history\'])\n        axs[i].plot(check[\'val_acc_history\'])\n        axs[i].legend([\'train\', \'val\'], loc=\'upper left\')\n        axs[i].set_xlabel(\'epoch\')\n        axs[i].set_ylabel(\'accuracy\')\n        axs[i].set_title(key, fontsize=24)\n        i += 1\n'"
tf_code_mnist/mnist_data.py,0,"b'# Some code was borrowed from https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/models/image/mnist/convolutional.py\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\n\nimport numpy\nfrom scipy import ndimage\n\nfrom six.moves import urllib\n\nimport tensorflow as tf\n\nSOURCE_URL = \'http://yann.lecun.com/exdb/mnist/\'\nDATA_DIRECTORY = ""data""\n\n# Params for MNIST\nIMAGE_SIZE = 28\nNUM_CHANNELS = 1\nPIXEL_DEPTH = 255\nNUM_LABELS = 10\nVALIDATION_SIZE = 5000  # Size of the validation set.\n\n# Download MNIST data\ndef maybe_download(filename):\n    """"""Download the data from Yann\'s website, unless it\'s already here.""""""\n    if not tf.gfile.Exists(DATA_DIRECTORY):\n        tf.gfile.MakeDirs(DATA_DIRECTORY)\n    filepath = os.path.join(DATA_DIRECTORY, filename)\n    if not tf.gfile.Exists(filepath):\n        filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n        with tf.gfile.GFile(filepath) as f:\n            size = f.size()\n        print(\'Successfully downloaded\', filename, size, \'bytes.\')\n    return filepath\n\n# Extract the images\ndef extract_data(filename, num_images):\n    """"""Extract the images into a 4D tensor [image index, y, x, channels].\n\n    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n    """"""\n    print(\'Extracting\', filename)\n    with gzip.open(filename) as bytestream:\n        bytestream.read(16)\n        buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n        data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n        data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n        data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n        data = numpy.reshape(data, [num_images, -1])\n    return data\n\n# Extract the labels\ndef extract_labels(filename, num_images):\n    """"""Extract the labels into a vector of int64 label IDs.""""""\n    print(\'Extracting\', filename)\n    with gzip.open(filename) as bytestream:\n        bytestream.read(8)\n        buf = bytestream.read(1 * num_images)\n        labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)\n        num_labels_data = len(labels)\n        one_hot_encoding = numpy.zeros((num_labels_data,NUM_LABELS))\n        one_hot_encoding[numpy.arange(num_labels_data),labels] = 1\n        one_hot_encoding = numpy.reshape(one_hot_encoding, [-1, NUM_LABELS])\n    return one_hot_encoding\n\n# Augment training data\ndef expend_training_data(images, labels):\n\n    expanded_images = []\n    expanded_labels = []\n\n    j = 0 # counter\n    for x, y in zip(images, labels):\n        j = j+1\n        if j%100==0:\n            print (\'expanding data : %03d / %03d\' % (j,numpy.size(images,0)))\n\n        # register original data\n        expanded_images.append(x)\n        expanded_labels.append(y)\n\n        # get a value for the background\n        # zero is the expected value, but median() is used to estimate background\'s value \n        bg_value = numpy.median(x) # this is regarded as background\'s value        \n        image = numpy.reshape(x, (-1, 28))\n\n        #shift_amout = ([1,0],[-1,0],[0,1],[0,-1])\n        #for i, s in zip(range(4),shift_amout):\n        for i in range(4):\n            # # rotate the image with random degree\n            angle = numpy.random.randint(-15,15,1)\n            new_img = ndimage.rotate(image,angle,reshape=False, cval=bg_value)\n\n            # shift the image with random distance\n            shift = numpy.random.randint(-2, 2, 2)\n            new_img_ = ndimage.shift(image, shift, cval=bg_value)\n\n            # register new training data\n            expanded_images.append(numpy.reshape(new_img_, 784))\n            expanded_labels.append(y)\n\n    # images and labels are concatenated for random-shuffle at each epoch\n    # notice that pair of image and label should not be broken\n    expanded_train_total_data = numpy.concatenate((expanded_images, expanded_labels), axis=1)\n    numpy.random.shuffle(expanded_train_total_data)\n\n    return expanded_train_total_data\n\n# Prepare MNISt data\ndef prepare_MNIST_data(use_data_augmentation=True):\n    # Get the data.\n    train_data_filename = maybe_download(\'train-images-idx3-ubyte.gz\')\n    train_labels_filename = maybe_download(\'train-labels-idx1-ubyte.gz\')\n    test_data_filename = maybe_download(\'t10k-images-idx3-ubyte.gz\')\n    test_labels_filename = maybe_download(\'t10k-labels-idx1-ubyte.gz\')\n\n    # Extract it into numpy arrays.\n    train_data = extract_data(train_data_filename, 60000)\n    train_labels = extract_labels(train_labels_filename, 60000)\n    test_data = extract_data(test_data_filename, 10000)\n    test_labels = extract_labels(test_labels_filename, 10000)\n\n    # Generate a validation set.\n    validation_data = train_data[:VALIDATION_SIZE, :]\n    validation_labels = train_labels[:VALIDATION_SIZE,:]\n    train_data = train_data[VALIDATION_SIZE:, :]\n    train_labels = train_labels[VALIDATION_SIZE:,:]\n\n    # Concatenate train_data & train_labels for random shuffle\n    if use_data_augmentation:\n        train_total_data = expend_training_data(train_data, train_labels)\n    else:\n        train_total_data = numpy.concatenate((train_data, train_labels), axis=1)\n\n    train_size = train_total_data.shape[0]\n\n    return train_total_data, train_size, validation_data, validation_labels, test_data, test_labels\n\n\n'"
tf_code_mnist/models.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.contrib.layers.python.layers import initializers\nfrom tensorflow.python.ops import init_ops\n\ndef version_0(inputs, is_training):\n    with tf.name_scope('version_0'):\n        n_hidden = 100\n        n_out = 10\n\n        # 100 sigmoid neurons\n        net = slim.fully_connected(inputs, n_hidden, scope='fc1',\n                                   activation_fn=tf.nn.sigmoid,\n                                   weights_initializer=initializers.xavier_initializer(),\n                                   biases_initializer=init_ops.zeros_initializer())\n\n        # 10 neurons (softmax)\n        logits = slim.fully_connected(net, n_out, activation_fn=None, scope='fco',\n                                      weights_initializer=initializers.xavier_initializer(),\n                                      biases_initializer=init_ops.zeros_initializer())\n        out_layer = tf.nn.softmax(logits)\n\n    return out_layer, logits\n\ndef version_1(inputs, is_training):\n    with tf.name_scope('version_1'):\n        n_filter = 20\n        n_hidden = 100\n        n_out = 10\n\n        # Reshaping for convolutional operation\n        x = tf.reshape(inputs, [-1, 28, 28, 1])\n\n        # Convolutional layer\n        net = slim.conv2d(x, n_filter, [5, 5], padding='VALID', activation_fn = tf.nn.sigmoid, scope='conv1')\n\n        # Pooling layer\n        net = slim.max_pool2d(net, [2, 2], stride=2, padding='VALID', scope='pool1')\n\n        # Flatten for fully-connected layer\n        net = slim.flatten(net, scope='flatten3')\n\n        # 100 sigmoid neurons\n        net = slim.fully_connected(net, n_hidden, scope='fc1',\n                                   activation_fn=tf.nn.sigmoid,\n                                   weights_initializer=initializers.xavier_initializer(),\n                                   biases_initializer=init_ops.zeros_initializer())\n\n        # 10 neurons (softmax)\n        logits = slim.fully_connected(net, n_out, activation_fn=None, scope='fco',\n                                      weights_initializer=initializers.xavier_initializer(),\n                                      biases_initializer=init_ops.zeros_initializer())\n        out_layer = tf.nn.softmax(logits)\n\n    return out_layer, logits\n\ndef version_2(inputs, is_training):\n    with tf.name_scope('version_2'):\n        n_filter1 = 20\n        n_filter2 = 40\n        n_hidden = 100\n        n_out = 10\n\n        # Reshaping for convolutional operation\n        x = tf.reshape(inputs, [-1, 28, 28, 1])\n\n        # Convolutional layer\n        net = slim.conv2d(x, n_filter1, [5, 5], padding='VALID', activation_fn = tf.nn.sigmoid, scope='conv1')\n\n        # Pooling layer\n        net = slim.max_pool2d(net, [2, 2], stride=2, padding='VALID', scope='pool1')\n\n        # Convolutional layer\n        net = slim.conv2d(net, n_filter2, [5, 5], padding='VALID', activation_fn=tf.nn.sigmoid, scope='conv2')\n\n        # Pooling layer\n        net = slim.max_pool2d(net, [2, 2], stride=2, padding='VALID', scope='pool2')\n\n        # Flatten for fully-connected layer\n        net = slim.flatten(net, scope='flatten3')\n\n        # 100 sigmoid neurons\n        net = slim.fully_connected(net, n_hidden, scope='fc1',\n                                   activation_fn=tf.nn.sigmoid,\n                                   weights_initializer=initializers.xavier_initializer(),\n                                   biases_initializer=init_ops.zeros_initializer())\n\n        # 10 neurons (softmax)\n        logits = slim.fully_connected(net, n_out, activation_fn=None, scope='fco',\n                                      weights_initializer=initializers.xavier_initializer(),\n                                      biases_initializer=init_ops.zeros_initializer())\n        out_layer = tf.nn.softmax(logits)\n\n    return out_layer, logits\n\ndef version_3(inputs, is_training):\n    with tf.name_scope('version_3'):\n        n_filter1 = 20\n        n_filter2 = 40\n        n_hidden = 100\n        n_out = 10\n\n        # L2 Regularizer\n        with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                            weights_regularizer=slim.l2_regularizer(0.1)):\n\n            # Reshaping for convolutional operation\n            x = tf.reshape(inputs, [-1, 28, 28, 1])\n\n            # Convolutional layer\n            net = slim.conv2d(x, n_filter1, [5, 5], padding='VALID', activation_fn = tf.nn.relu, scope='conv1')\n\n            # Pooling layer\n            net = slim.max_pool2d(net, [2, 2], stride=2, padding='VALID', scope='pool1')\n\n            # Convolutional layer\n            net = slim.conv2d(net, n_filter2, [5, 5], padding='VALID', activation_fn=tf.nn.relu, scope='conv2')\n\n            # Pooling layer\n            net = slim.max_pool2d(net, [2, 2], stride=2, padding='VALID', scope='pool2')\n\n            # Flatten for fully-connected layer\n            net = slim.flatten(net, scope='flatten3')\n\n            # 100 ReLu neurons\n            net = slim.fully_connected(net, n_hidden, scope='fc1',\n                                       activation_fn=tf.nn.relu,\n                                       weights_initializer=initializers.xavier_initializer(),\n                                       biases_initializer=init_ops.zeros_initializer())\n\n            # 10 neurons (softmax)\n            logits = slim.fully_connected(net, n_out, activation_fn=None, scope='fco',\n                                          weights_initializer=initializers.xavier_initializer(),\n                                          biases_initializer=init_ops.zeros_initializer())\n            out_layer = tf.nn.softmax(logits)\n\n    return out_layer, logits\n\ndef version_5(inputs, is_training):\n    with tf.name_scope('version_5'):\n        n_filter1 = 20\n        n_filter2 = 40\n        n_hidden1 = 100\n        n_hidden2 = 100\n        n_out = 10\n\n        # L2 Regularizer\n        with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                            weights_regularizer=slim.l2_regularizer(0.1)):\n            # Reshaping for convolutional operation\n            x = tf.reshape(inputs, [-1, 28, 28, 1])\n\n            # Convolutional layer\n            net = slim.conv2d(x, n_filter1, [5, 5], padding='VALID', activation_fn = tf.nn.relu, scope='conv1')\n\n            # Pooling layer\n            net = slim.max_pool2d(net, [2, 2], stride=2, padding='VALID', scope='pool1')\n\n            # Convolutional layer\n            net = slim.conv2d(net, n_filter2, [5, 5], padding='VALID', activation_fn=tf.nn.relu, scope='conv2')\n\n            # Pooling layer\n            net = slim.max_pool2d(net, [2, 2], stride=2, padding='VALID', scope='pool2')\n\n            # Flatten for fully-connected layer\n            net = slim.flatten(net, scope='flatten3')\n\n            # 100 ReLu neurons\n            net = slim.fully_connected(net, n_hidden1, scope='fc1',\n                                       activation_fn=tf.nn.relu,\n                                       weights_initializer=initializers.xavier_initializer(),\n                                       biases_initializer=init_ops.zeros_initializer())\n\n            # 100 ReLu neurons\n            net = slim.fully_connected(net, n_hidden2, scope='fc2',\n                                       activation_fn=tf.nn.relu,\n                                       weights_initializer=initializers.xavier_initializer(),\n                                       biases_initializer=init_ops.zeros_initializer())\n\n            # 10 neurons (softmax)\n            logits = slim.fully_connected(net, n_out, activation_fn=None, scope='fco',\n                                          weights_initializer=initializers.xavier_initializer(),\n                                          biases_initializer=init_ops.zeros_initializer())\n            out_layer = tf.nn.softmax(logits)\n\n    return out_layer, logits\n\ndef version_6(inputs, is_training):\n    with tf.name_scope('version_6'):\n        n_filter1 = 20\n        n_filter2 = 40\n        n_hidden1 = 100\n        n_hidden2 = 100\n        n_out = 10\n\n        # # L2 Regularizer\n        # with slim.arg_scope([slim.conv2d, slim.fully_connected],\n        #                     weights_regularizer=slim.l2_regularizer(0.1)):\n        # batch_norm_params = {'is_training': is_training, 'decay': 0.9, 'updates_collections': None}\n        # with slim.arg_scope([slim.conv2d, slim.fully_connected],\n        #                     normalizer_fn=slim.batch_norm,\n        #                     normalizer_params=batch_norm_params,\n        #                     weights_regularizer=slim.l2_regularizer(0.1)):\n\n        # Reshaping for convolutional operation\n        x = tf.reshape(inputs, [-1, 28, 28, 1])\n\n        # Convolutional layer\n        net = slim.conv2d(x, n_filter1, [5, 5], padding='VALID', activation_fn = tf.nn.relu, scope='conv1')\n\n        # Pooling layer\n        net = slim.max_pool2d(net, [2, 2], stride=2, padding='VALID', scope='pool1')\n\n        # Convolutional layer\n        net = slim.conv2d(net, n_filter2, [5, 5], padding='VALID', activation_fn=tf.nn.relu, scope='conv2')\n\n        # Pooling layer\n        net = slim.max_pool2d(net, [2, 2], stride=2, padding='VALID', scope='pool2')\n\n        # Flatten for fully-connected layer\n        net = slim.flatten(net, scope='flatten3')\n\n        # 100 ReLu neurons\n        net = slim.fully_connected(net, n_hidden1, scope='fc1',\n                                   activation_fn=tf.nn.relu,\n                                   weights_initializer=initializers.xavier_initializer(),\n                                   biases_initializer=init_ops.zeros_initializer())\n        net = slim.dropout(net, is_training=is_training, scope='dropout1')  # 0.5 by default\n\n        # 100 ReLu neurons\n        net = slim.fully_connected(net, n_hidden2, scope='fc2',\n                                   activation_fn=tf.nn.relu,\n                                   weights_initializer=initializers.xavier_initializer(),\n                                   biases_initializer=init_ops.zeros_initializer())\n        net = slim.dropout(net, is_training=is_training, scope='dropout2')  # 0.5 by default\n\n        # 10 neurons (softmax)\n        net = slim.fully_connected(net, n_out, activation_fn=None, scope='fco',\n                                      weights_initializer=initializers.xavier_initializer(),\n                                      biases_initializer=init_ops.zeros_initializer())\n        logits = slim.dropout(net, is_training=is_training, scope='dropout3')  # 0.5 by default\n        out_layer = tf.nn.softmax(logits)\n\n    return out_layer, logits"""
tf_code_mnist/train.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport models\nimport mnist_data\nimport numpy as np\n\nSOURCE_URL = \'http://yann.lecun.com/exdb/mnist/\'\nDATA_DIRECTORY = ""data""\nLOGS_DIRECTORY = ""logs/train""\n\n# user input\nimport argparse\n\n""""""parsing and configuration""""""\ndef parse_args():\n    desc = ""Tensorflow implementation of various cnn models""\n    parser = argparse.ArgumentParser(description=desc)\n\n    parser.add_argument(\'--model\', type=str, default=\'GAN\',\n                        choices=[\'v0\', \'v1\', \'v2\', \'v3\', \'v4\', \'v5\', \'v6\'],\n                        help=\'The type of model\', required=True)\n\n    return parser.parse_args()\n\n# main function\ndef main():\n    # parse arguments\n    args = parse_args()\n\n    # Some parameters\n    data_augmentation = False\n    batch_size = 10\n\n    training_epochs = 25\n    num_labels = mnist_data.NUM_LABELS\n\n    # Choose model\n    if args.model == \'v0\':\n        model = models.version_0\n        learning_rate = 0.1\n        display_step = 500\n    elif args.model == \'v1\':\n        model = models.version_1\n        learning_rate = 0.1\n        display_step = 500\n    elif args.model == \'v2\':\n        model = models.version_2\n        learning_rate = 0.1\n        display_step = 500\n    elif args.model == \'v3\':\n        model = models.version_3\n        learning_rate = 0.03\n        display_step = 500\n    elif args.model == \'v4\':\n        model = models.version_3\n        learning_rate = 0.03\n        data_augmentation = True\n        display_step = 2500\n    elif args.model == \'v5\':\n        model = models.version_5\n        learning_rate = 0.03\n        data_augmentation = True\n        display_step = 2500\n    elif args.model == \'v6\':\n        model = models.version_6\n        learning_rate = 0.03\n        data_augmentation = True\n        display_step = 5500\n    else:\n        NotImplementedError()\n\n    # Prepare mnist data\n    train_total_data, train_size, validation_data, validation_labels, test_data, test_labels = mnist_data.prepare_MNIST_data(\n        data_augmentation)\n\n    # Boolean for MODE of train or test\n    is_training = tf.placeholder(tf.bool, name=\'MODE\')\n\n    # tf Graph input\n    x = tf.placeholder(tf.float32, [None, 784])\n    y = tf.placeholder(tf.float32, [None, 10]) #answer\n\n    # Predict\n    pred, pred_logit = model(x, is_training)\n\n    # Get loss of model\n    with tf.name_scope(""LOSS""):\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=pred_logit))\n\n    # Define optimizer\n    with tf.name_scope(""ADAM""):\n        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n\n    # Get accuracy of model\n    with tf.name_scope(""ACC""):\n        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(pred, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    # Create a summary to monitor loss tensor\n    tf.summary.scalar(\'loss\', loss)\n\n    # Create a summary to monitor accuracy tensor\n    tf.summary.scalar(\'acc\', accuracy)\n\n    # Merge all summaries into a single op\n    merged_summary_op = tf.summary.merge_all()\n\n    # Add ops to save and restore all the variables\n    sess = tf.InteractiveSession()\n    sess.run(tf.global_variables_initializer(), feed_dict={is_training: True})\n\n    # Training cycle\n    total_batch = int(train_size / batch_size)\n\n    # op to write logs to Tensorboard\n    summary_writer = tf.summary.FileWriter(LOGS_DIRECTORY, graph=tf.get_default_graph())\n\n    # Loop for epoch\n    for epoch in range(training_epochs):\n\n        # Random shuffling\n        np.random.shuffle(train_total_data)\n        train_data_ = train_total_data[:, :-num_labels]\n        train_labels_ = train_total_data[:, -num_labels:]\n\n        # Loop over all batches\n        for i in range(total_batch):\n            # Compute the offset of the current minibatch in the data.\n            offset = (i * batch_size) % (train_size)\n            batch_xs = train_data_[offset:(offset + batch_size), :]\n            batch_ys = train_labels_[offset:(offset + batch_size), :]\n\n            # Run optimization op (backprop), loss op (to get loss value)\n            # and summary nodes\n            _, train_accuracy, summary = sess.run([train_step, accuracy, merged_summary_op] , feed_dict={x: batch_xs, y: batch_ys, is_training: True})\n\n            # Write logs at every iteration\n            summary_writer.add_summary(summary, epoch * total_batch + i)\n\n            # Display logs\n            if i % display_step == 0:\n                print(""Epoch:"", \'%02d,\' % (epoch + 1),\n                      ""batch_index %4d/%4d, training accuracy %.5f"" % (i, total_batch, train_accuracy))\n\n        # Get accuracy for test data\n        print(""Test accuracy at Epoch %02d : %g"" % (epoch+1, accuracy.eval(\n            feed_dict={x: test_data, y: test_labels, is_training: False})))\n\n    # Calculate accuracy for all mnist test images\n    print(""Test accuracy for the latest result: %g"" % accuracy.eval(\n        feed_dict={x: test_data, y: test_labels, is_training: False}))\n\nif __name__ == \'__main__\':\n    main()'"
cs231n/classifiers/__init__.py,0,b''
cs231n/classifiers/cnn.py,35,"b'import numpy as np\n\nfrom cs231n.layers import *\nfrom cs231n.fast_layers import *\nfrom cs231n.layer_utils import *\n\n\nclass FirstConvNet(object):\n    """"""\n    A L-layer convolutional network with the following architecture:\n\n    [conv-relu-pool2x2]xL - [affine - relu]xM - affine - softmax\n\n    The network operates on minibatches of data that have shape (N, C, H, W)\n    consisting of N images, each with height H and width W and with C input\n    channels.\n    """"""\n\n    def __init__(self, input_dim=(3, 32, 32), num_filters=[16, 32], filter_size=3,\n                 hidden_dims=[100, 100], num_classes=10, weight_scale=1e-3, reg=0.0,\n                 dtype=np.float32, use_batchnorm=False):\n        """"""\n        Initialize a new network.\n\n        Inputs:\n        - input_dim: Tuple (C, H, W) giving size of input data\n        - num_filters: List of size  Nbconv+1 with the number of filters\n        to use in each convolutional layer\n        - filter_size: Size of filters to use in the convolutional layer\n        - hidden_dims: Number of units to use in the fully-connected hidden layer\n        - num_classes: Number of scores to produce from the final affine layer.\n        - weight_scale: Scalar giving standard deviation for random initialization\n          of weights.\n        - reg: Scalar giving L2 regularization strength\n        - dtype: numpy datatype to use for computation.\n        """"""\n\n        self.use_batchnorm = use_batchnorm\n        self.params = {}\n        self.reg = reg\n        self.dtype = dtype\n        self.bn_params = {}\n\n        self.filter_size = filter_size\n        self.L = len(num_filters)  # Number of weights\n        self.M = len(hidden_dims)  # Number of conv/relu/pool blocks\n\n        # Size of the input\n        Cinput, Hinput, Winput = input_dim\n        stride_conv = 1  # stride\n\n        # Initialize the weight for the conv layers\n        F = [Cinput] + num_filters\n        for i in xrange(self.L):\n            idx = i + 1\n            W = weight_scale * \\\n                np.random.randn(\n                    F[i + 1], F[i], self.filter_size, self.filter_size)\n            b = np.zeros(F[i + 1])\n            self.params.update({\'W\' + str(idx): W,\n                                \'b\' + str(idx): b})\n            if self.use_batchnorm:\n                bn_param = {\'mode\': \'train\',\n                            \'running_mean\': np.zeros(F[i + 1]),\n                            \'running_var\': np.zeros(F[i + 1])}\n                gamma = np.ones(F[i + 1])\n                beta = np.zeros(F[i + 1])\n                self.bn_params.update({\n                    \'bn_param\' + str(idx): bn_param})\n                self.params.update({\n                    \'gamma\' + str(idx): gamma,\n                    \'beta\' + str(idx): beta})\n\n        # Initialize the weights for the affine-relu layers\n        # Size of the last activation\n        Hconv, Wconv = self.Size_Conv(\n            stride_conv, self.filter_size, Hinput, Winput, self.L)\n        dims = [Hconv * Wconv * F[-1]] + hidden_dims\n        for i in xrange(self.M):\n            idx = self.L + i + 1\n            W = weight_scale * \\\n                np.random.randn(dims[i], dims[i + 1])\n            b = np.zeros(dims[i + 1])\n            self.params.update({\'W\' + str(idx): W,\n                                \'b\' + str(idx): b})\n            if self.use_batchnorm:\n                bn_param = {\'mode\': \'train\',\n                            \'running_mean\': np.zeros(dims[i + 1]),\n                            \'running_var\': np.zeros(dims[i + 1])}\n                gamma = np.ones(dims[i + 1])\n                beta = np.zeros(dims[i + 1])\n                self.bn_params.update({\n                    \'bn_param\' + str(idx): bn_param})\n                self.params.update({\n                    \'gamma\' + str(idx): gamma,\n                    \'beta\' + str(idx): beta})\n\n        # Scoring layer\n        W = weight_scale * np.random.randn(dims[-1], num_classes)\n        b = np.zeros(num_classes)\n        self.params.update({\'W\' + str(self.L + self.M + 1): W,\n                            \'b\' + str(self.L + self.M + 1): b})\n\n        for k, v in self.params.iteritems():\n            self.params[k] = v.astype(dtype)\n\n    def Size_Conv(self, stride_conv, filter_size, H, W, Nbconv):\n        P = (filter_size - 1) / 2  # padd\n        Hc = (H + 2 * P - filter_size) / stride_conv + 1\n        Wc = (W + 2 * P - filter_size) / stride_conv + 1\n        width_pool = 2\n        height_pool = 2\n        stride_pool = 2\n        Hp = (Hc - height_pool) / stride_pool + 1\n        Wp = (Wc - width_pool) / stride_pool + 1\n        if Nbconv == 1:\n            return Hp, Wp\n        else:\n            H = Hp\n            W = Wp\n            return self.Size_Conv(stride_conv, filter_size, H, W, Nbconv - 1)\n\n    def loss(self, X, y=None):\n        """"""\n        Evaluate loss and gradient for the three-layer convolutional network.\n\n        Input / output: Same API as TwoLayerNet in fc_net.py.\n        """"""\n        X = X.astype(self.dtype)\n        mode = \'test\' if y is None else \'train\'\n\n        N = X.shape[0]\n\n        # pass conv_param to the forward pass for the convolutional layer\n        filter_size = self.filter_size\n        conv_param = {\'stride\': 1, \'pad\': (filter_size - 1) / 2}\n\n        # pass pool_param to the forward pass for the max-pooling layer\n        pool_param = {\'pool_height\': 2, \'pool_width\': 2, \'stride\': 2}\n\n        if self.use_batchnorm:\n            for key, bn_param in self.bn_params.iteritems():\n                bn_param[mode] = mode\n\n        scores = None\n        #######################################################################\n        # TODO: Implement the forward pass for the three-layer convolutional net,  #\n        # computing the class scores for X and storing them in the scores          #\n        # variable.                                                                #\n        #######################################################################\n\n        blocks = {}\n        blocks[\'h0\'] = X\n        # Forward into the conv blocks\n        for i in xrange(self.L):\n            idx = i + 1\n            w = self.params[\'W\' + str(idx)]\n            b = self.params[\'b\' + str(idx)]\n            h = blocks[\'h\' + str(idx - 1)]\n            if self.use_batchnorm:\n                beta = self.params[\'beta\' + str(idx)]\n                gamma = self.params[\'gamma\' + str(idx)]\n                bn_param = self.bn_params[\'bn_param\' + str(idx)]\n                h, cache_h = conv_norm_relu_pool_forward(\n                    h, w, b, conv_param, pool_param, gamma, beta, bn_param)\n            else:\n                h, cache_h = conv_relu_pool_forward(\n                    h, w, b, conv_param, pool_param)\n            blocks[\'h\' + str(idx)] = h\n            blocks[\'cache_h\' + str(idx)] = cache_h\n\n        # Forward into the linear blocks\n        for i in xrange(self.M):\n            idx = self.L + i + 1\n            h = blocks[\'h\' + str(idx - 1)]\n            if i == 0:\n                h = h.reshape(N, np.product(h.shape[1:]))\n            w = self.params[\'W\' + str(idx)]\n            b = self.params[\'b\' + str(idx)]\n            if self.use_batchnorm:\n                beta = self.params[\'beta\' + str(idx)]\n                gamma = self.params[\'gamma\' + str(idx)]\n                bn_param = self.bn_params[\'bn_param\' + str(idx)]\n                h, cache_h = affine_norm_relu_forward(h, w, b, gamma,\n                                                      beta, bn_param)\n            else:\n                h, cache_h = affine_relu_forward(h, w, b)\n            blocks[\'h\' + str(idx)] = h\n            blocks[\'cache_h\' + str(idx)] = cache_h\n\n        # Fnally Forward into the score\n        idx = self.L + self.M + 1\n        w = self.params[\'W\' + str(idx)]\n        b = self.params[\'b\' + str(idx)]\n        h = blocks[\'h\' + str(idx - 1)]\n        h, cache_h = affine_forward(h, w, b)\n        blocks[\'h\' + str(idx)] = h\n        blocks[\'cache_h\' + str(idx)] = cache_h\n\n        scores = blocks[\'h\' + str(idx)]\n\n        if y is None:\n            return scores\n\n        loss, grads = 0, {}\n        #######################################################################\n        # TODO: Implement the backward pass for the three-layer convolutional net, #\n        # storing the loss and gradients in the loss and grads variables. Compute  #\n        # data loss using softmax, and make sure that grads[k] holds the gradients #\n        # for self.params[k]. Don\'t forget to add L2 regularization!               #\n        #######################################################################\n\n        # Computing of the loss\n        data_loss, dscores = softmax_loss(scores, y)\n        reg_loss = 0\n        for w in [self.params[f] for f in self.params.keys() if f[0] == \'W\']:\n            reg_loss += 0.5 * self.reg * np.sum(w * w)\n\n        loss = data_loss + reg_loss\n\n        # Backward pass\n        # print \'Backward pass\'\n        # Backprop into the scoring layer\n        idx = self.L + self.M + 1\n        dh = dscores\n        h_cache = blocks[\'cache_h\' + str(idx)]\n        dh, dw, db = affine_backward(dh, h_cache)\n        blocks[\'dh\' + str(idx - 1)] = dh\n        blocks[\'dW\' + str(idx)] = dw\n        blocks[\'db\' + str(idx)] = db\n\n        # Backprop into the linear blocks\n        for i in range(self.M)[::-1]:\n            idx = self.L + i + 1\n            dh = blocks[\'dh\' + str(idx)]\n            h_cache = blocks[\'cache_h\' + str(idx)]\n            if self.use_batchnorm:\n                dh, dw, db, dgamma, dbeta = affine_norm_relu_backward(\n                    dh, h_cache)\n                blocks[\'dbeta\' + str(idx)] = dbeta\n                blocks[\'dgamma\' + str(idx)] = dgamma\n            else:\n                dh, dw, db = affine_relu_backward(dh, h_cache)\n            blocks[\'dh\' + str(idx - 1)] = dh\n            blocks[\'dW\' + str(idx)] = dw\n            blocks[\'db\' + str(idx)] = db\n\n        # Backprop into the conv blocks\n        for i in range(self.L)[::-1]:\n            idx = i + 1\n            dh = blocks[\'dh\' + str(idx)]\n            h_cache = blocks[\'cache_h\' + str(idx)]\n            if i == max(range(self.L)[::-1]):\n                dh = dh.reshape(*blocks[\'h\' + str(idx)].shape)\n            if self.use_batchnorm:\n                dh, dw, db, dgamma, dbeta = conv_norm_relu_pool_backward(\n                    dh, h_cache)\n                blocks[\'dbeta\' + str(idx)] = dbeta\n                blocks[\'dgamma\' + str(idx)] = dgamma\n            else:\n                dh, dw, db = conv_relu_pool_backward(dh, h_cache)\n            blocks[\'dh\' + str(idx - 1)] = dh\n            blocks[\'dW\' + str(idx)] = dw\n            blocks[\'db\' + str(idx)] = db\n\n        # w gradients where we add the regulariation term\n        list_dw = {key[1:]: val + self.reg * self.params[key[1:]]\n                   for key, val in blocks.iteritems() if key[:2] == \'dW\'}\n        # Paramerters b\n        list_db = {key[1:]: val for key, val in blocks.iteritems() if key[:2] ==\n                   \'db\'}\n        # Parameters gamma\n        list_dgamma = {key[1:]: val for key, val in blocks.iteritems() if key[\n            :6] == \'dgamma\'}\n        # Paramters beta\n        list_dbeta = {key[1:]: val for key, val in blocks.iteritems() if key[\n            :5] == \'dbeta\'}\n\n        grads = {}\n        grads.update(list_dw)\n        grads.update(list_db)\n        grads.update(list_dgamma)\n        grads.update(list_dbeta)\n\n        return loss, grads\n\n\nclass ThreeLayerConvNet(object):\n    """"""\n    A three-layer convolutional network with the following architecture:\n\n    conv - relu - 2x2 max pool - affine - relu - affine - softmax\n\n    The network operates on minibatches of data that have shape (N, C, H, W)\n    consisting of N images, each with height H and width W and with C input\n    channels.\n    """"""\n\n    def __init__(self, input_dim=(3, 32, 32), num_filters=32, filter_size=7,\n                 hidden_dim=100, num_classes=10, weight_scale=1e-3, reg=0.0,\n                 dtype=np.float32, use_batchnorm=False):\n        """"""\n        Initialize a new network.\n\n        Inputs:\n        - input_dim: Tuple (C, H, W) giving size of input data\n        - num_filters: Number of filters to use in the convolutional layer\n        - filter_size: Size of filters to use in the convolutional layer\n        - hidden_dim: Number of units to use in the fully-connected hidden layer\n        - num_classes: Number of scores to produce from the final affine layer.\n        - weight_scale: Scalar giving standard deviation for random initialization\n          of weights.\n        - reg: Scalar giving L2 regularization strength\n        - dtype: numpy datatype to use for computation.\n        """"""\n        self.use_batchnorm = use_batchnorm\n        self.params = {}\n        self.reg = reg\n        self.dtype = dtype\n        self.bn_params = {}\n\n        #######################################################################\n        # TODO: Initialize weights and biases for the three-layer convolutional    #\n        # network. Weights should be initialized from a Gaussian with standard     #\n        # deviation equal to weight_scale; biases should be initialized to zero.   #\n        # All weights and biases should be stored in the dictionary self.params.   #\n        # Store weights and biases for the convolutional layer using the keys \'W1\' #\n        # and \'b1\'; use keys \'W2\' and \'b2\' for the weights and biases of the       #\n        # hidden affine layer, and keys \'W3\' and \'b3\' for the weights and biases   #\n        # of the output affine layer.                                              #\n        #######################################################################\n\n        # Size of the input\n        C, H, W = input_dim\n\n        # Conv layer\n        # The parameters of the conv is of size (F,C,HH,WW) with\n        # F give the nb of filters, C,HH,WW characterize the size of\n        # each filter\n        # Input size : (N,C,H,W)\n        # Output size : (N,F,Hc,Wc)\n        F = num_filters\n        filter_height = filter_size\n        filter_width = filter_size\n        stride_conv = 1  # stride\n        P = (filter_size - 1) / 2  # padd\n        Hc = (H + 2 * P - filter_height) / stride_conv + 1\n        Wc = (W + 2 * P - filter_width) / stride_conv + 1\n\n        W1 = weight_scale * np.random.randn(F, C, filter_height, filter_width)\n        b1 = np.zeros(F)\n\n        # Pool layer : 2*2\n        # The pool layer has no parameters but is important in the\n        # count of dimension.\n        # Input : (N,F,Hc,Wc)\n        # Ouput : (N,F,Hp,Wp)\n\n        width_pool = 2\n        height_pool = 2\n        stride_pool = 2\n        Hp = (Hc - height_pool) / stride_pool + 1\n        Wp = (Wc - width_pool) / stride_pool + 1\n\n        # Hidden Affine layer\n        # Size of the parameter (F*Hp*Wp,H1)\n        # Input: (N,F*Hp*Wp)\n        # Output: (N,Hh)\n\n        Hh = hidden_dim\n        W2 = weight_scale * np.random.randn(F * Hp * Wp, Hh)\n        b2 = np.zeros(Hh)\n\n        # Output affine layer\n        # Size of the parameter (Hh,Hc)\n        # Input: (N,Hh)\n        # Output: (N,Hc)\n\n        Hc = num_classes\n        W3 = weight_scale * np.random.randn(Hh, Hc)\n        b3 = np.zeros(Hc)\n\n        self.params.update({\'W1\': W1,\n                            \'W2\': W2,\n                            \'W3\': W3,\n                            \'b1\': b1,\n                            \'b2\': b2,\n                            \'b3\': b3})\n\n        # With batch normalization we need to keep track of running means and\n        # variances, so we need to pass a special bn_param object to each batch\n        # normalization layer. You should pass self.bn_params[0] to the forward pass\n        # of the first batch normalization layer, self.bn_params[1] to the forward\n        # pass of the second batch normalization layer, etc.\n\n        if self.use_batchnorm:\n            print \'We use batchnorm here\'\n            bn_param1 = {\'mode\': \'train\',\n                         \'running_mean\': np.zeros(F),\n                         \'running_var\': np.zeros(F)}\n            gamma1 = np.ones(F)\n            beta1 = np.zeros(F)\n\n            bn_param2 = {\'mode\': \'train\',\n                         \'running_mean\': np.zeros(Hh),\n                         \'running_var\': np.zeros(Hh)}\n            gamma2 = np.ones(Hh)\n            beta2 = np.zeros(Hh)\n\n            self.bn_params.update({\'bn_param1\': bn_param1,\n                                   \'bn_param2\': bn_param2})\n\n            self.params.update({\'beta1\': beta1,\n                                \'beta2\': beta2,\n                                \'gamma1\': gamma1,\n                                \'gamma2\': gamma2})\n\n        for k, v in self.params.iteritems():\n            self.params[k] = v.astype(dtype)\n\n    def loss(self, X, y=None):\n        """"""\n        Evaluate loss and gradient for the three-layer convolutional network.\n\n        Input / output: Same API as TwoLayerNet in fc_net.py.\n        """"""\n        X = X.astype(self.dtype)\n        mode = \'test\' if y is None else \'train\'\n\n        if self.use_batchnorm:\n            for key, bn_param in self.bn_params.iteritems():\n                bn_param[mode] = mode\n\n        N = X.shape[0]\n\n        W1, b1 = self.params[\'W1\'], self.params[\'b1\']\n        W2, b2 = self.params[\'W2\'], self.params[\'b2\']\n        W3, b3 = self.params[\'W3\'], self.params[\'b3\']\n        if self.use_batchnorm:\n            bn_param1, gamma1, beta1 = self.bn_params[\n                \'bn_param1\'], self.params[\'gamma1\'], self.params[\'beta1\']\n            bn_param2, gamma2, beta2 = self.bn_params[\n                \'bn_param2\'], self.params[\'gamma2\'], self.params[\'beta2\']\n\n        # pass conv_param to the forward pass for the convolutional layer\n        filter_size = W1.shape[2]\n        conv_param = {\'stride\': 1, \'pad\': (filter_size - 1) / 2}\n\n        # pass pool_param to the forward pass for the max-pooling layer\n        pool_param = {\'pool_height\': 2, \'pool_width\': 2, \'stride\': 2}\n\n        scores = None\n        #######################################################################\n        # TODO: Implement the forward pass for the three-layer convolutional net,  #\n        # computing the class scores for X and storing them in the scores          #\n        # variable.                                                                #\n        #######################################################################\n\n        # Forward into the conv layer\n        x = X\n        w = W1\n        b = b1\n        if self.use_batchnorm:\n            beta = beta1\n            gamma = gamma1\n            bn_param = bn_param1\n            conv_layer, cache_conv_layer = conv_norm_relu_pool_forward(\n                x, w, b, conv_param, pool_param, gamma, beta, bn_param)\n        else:\n            conv_layer, cache_conv_layer = conv_relu_pool_forward(\n                x, w, b, conv_param, pool_param)\n\n        N, F, Hp, Wp = conv_layer.shape  # output shape\n\n        # Forward into the hidden layer\n        x = conv_layer.reshape((N, F * Hp * Wp))\n        w = W2\n        b = b2\n        if self.use_batchnorm:\n            gamma = gamma2\n            beta = beta2\n            bn_param = bn_param2\n            hidden_layer, cache_hidden_layer = affine_norm_relu_forward(\n                x, w, b, gamma, beta, bn_param)\n        else:\n            hidden_layer, cache_hidden_layer = affine_relu_forward(x, w, b)\n        N, Hh = hidden_layer.shape\n\n        # Forward into the linear output layer\n        x = hidden_layer\n        w = W3\n        b = b3\n        scores, cache_scores = affine_forward(x, w, b)\n\n        if y is None:\n            return scores\n\n        loss, grads = 0, {}\n        #######################################################################\n        # TODO: Implement the backward pass for the three-layer convolutional net, #\n        # storing the loss and gradients in the loss and grads variables. Compute  #\n        # data loss using softmax, and make sure that grads[k] holds the gradients #\n        # for self.params[k]. Don\'t forget to add L2 regularization!               #\n        #######################################################################\n\n        data_loss, dscores = softmax_loss(scores, y)\n        reg_loss = 0.5 * self.reg * np.sum(W1**2)\n        reg_loss += 0.5 * self.reg * np.sum(W2**2)\n        reg_loss += 0.5 * self.reg * np.sum(W3**2)\n        loss = data_loss + reg_loss\n\n        # Backpropagation\n        grads = {}\n        # Backprop into output layer\n        dx3, dW3, db3 = affine_backward(dscores, cache_scores)\n        dW3 += self.reg * W3\n\n        # Backprop into first layer\n        if self.use_batchnorm:\n            dx2, dW2, db2, dgamma2, dbeta2 = affine_norm_relu_backward(\n                dx3, cache_hidden_layer)\n        else:\n            dx2, dW2, db2 = affine_relu_backward(dx3, cache_hidden_layer)\n\n        dW2 += self.reg * W2\n\n        # Backprop into the conv layer\n        dx2 = dx2.reshape(N, F, Hp, Wp)\n        if self.use_batchnorm:\n            dx, dW1, db1, dgamma1, dbeta1 = conv_norm_relu_pool_backward(\n                dx2, cache_conv_layer)\n        else:\n            dx, dW1, db1 = conv_relu_pool_backward(dx2, cache_conv_layer)\n\n        dW1 += self.reg * W1\n\n        grads.update({\'W1\': dW1,\n                      \'b1\': db1,\n                      \'W2\': dW2,\n                      \'b2\': db2,\n                      \'W3\': dW3,\n                      \'b3\': db3})\n\n        if self.use_batchnorm:\n            grads.update({\'beta1\': dbeta1,\n                          \'beta2\': dbeta2,\n                          \'gamma1\': dgamma1,\n                          \'gamma2\': dgamma2})\n\n        #######################################################################\n        #                             END OF YOUR CODE                             #\n        #######################################################################\n\n        return loss, grads\n\n\npass\n'"
cs231n/classifiers/convnet.py,10,"b'import numpy as np\n\nfrom cs231n.layers import *\nfrom cs231n.fast_layers import *\nfrom cs231n.layer_utils import *\n\n\nclass ConvNet(object):\n    """"""\n    A convolutional network with the following architecture:\n\n    conv - relu - 2x2 max pool - affine - relu - affine - softmax\n\n    The network operates on minibatches of data that have shape (N, C, H, W)\n    consisting of N images, each with height H and width W and with C input\n    channels.\n    """"""\n\n    def __init__(self, input_dim=(3, 32, 32), num_filters=32, filter_size=7,\n                 hidden_dim=100, num_classes=10, weight_scale=1e-3, reg=0.0,\n                 dtype=np.float32, use_batchnorm=False):\n        """"""\n        Initialize a new network.\n\n        Inputs:\n        - input_dim: Tuple (C, H, W) giving size of input data\n        - num_filters: Number of filters to use in the convolutional layer\n        - filter_size: Size of filters to use in the convolutional layer\n        - hidden_dim: Number of units to use in the fully-connected hidden layer\n        - num_classes: Number of scores to produce from the final affine layer.\n        - weight_scale: Scalar giving standard deviation for random initialization\n          of weights.\n        - reg: Scalar giving L2 regularization strength\n        - dtype: numpy datatype to use for computation.\n        """"""\n        self.use_batchnorm = use_batchnorm\n        self.params = {}\n        self.reg = reg\n        self.dtype = dtype\n\n        #######################################################################\n        # TODO: Initialize weights and biases for the three-layer convolutional    #\n        # network. Weights should be initialized from a Gaussian with standard     #\n        # deviation equal to weight_scale; biases should be initialized to zero.   #\n        # All weights and biases should be stored in the dictionary self.params.   #\n        # Store weights and biases for the convolutional layer using the keys \'W1\' #\n        # and \'b1\'; use keys \'W2\' and \'b2\' for the weights and biases of the       #\n        # hidden affine layer, and keys \'W3\' and \'b3\' for the weights and biases   #\n        # of the output affine layer.                                              #\n        #######################################################################\n\n        # Size of the input\n        C, H, W = input_dim\n\n        # Conv layer\n        # The parameters of the conv is of size (F,C,HH,WW) with\n        # F give the nb of filters, C,HH,WW characterize the size of\n        # each filter\n        # Input size : (N,C,H,W)\n        # Output size : (N,F,Hc,Wc)\n        F = num_filters\n        filter_height = filter_size\n        filter_width = filter_size\n        stride_conv = 1  # stride\n        P = (filter_size - 1) / 2  # padd\n        Hc = (H + 2 * P - filter_height) / stride_conv + 1\n        Wc = (W + 2 * P - filter_width) / stride_conv + 1\n\n        W1 = weight_scale * np.random.randn(F, C, filter_height, filter_width)\n        b1 = np.zeros(F)\n\n        # Pool layer : 2*2\n        # The pool layer has no parameters but is important in the\n        # count of dimension.\n        # Input : (N,F,Hc,Wc)\n        # Ouput : (N,F,Hp,Wp)\n\n        width_pool = 2\n        height_pool = 2\n        stride_pool = 2\n        Hp = (Hc - height_pool) / stride_pool + 1\n        Wp = (Wc - width_pool) / stride_pool + 1\n\n        # Hidden Affine layer\n        # Size of the parameter (F*Hp*Wp,H1)\n        # Input: (N,F*Hp*Wp)\n        # Output: (N,Hh)\n\n        Hh = hidden_dim\n        W2 = weight_scale * np.random.randn(F * Hp * Wp, Hh)\n        b2 = np.zeros(Hh)\n\n        # Output affine layer\n        # Size of the parameter (Hh,C)\n        # Input: (N,Hh)\n        # Output: (N,C)\n\n        C = num_classes\n        W3 = weight_scale * np.random.randn(Hh, C)\n        b3 = np.zeros(C)\n\n        self.params.update({\'W1\': W1,\n                            \'W2\': W2,\n                            \'W3\': W3,\n                            \'b1\': b1,\n                            \'b2\': b2,\n                            \'b3\': b3})\n\n        for k, v in self.params.iteritems():\n            self.params[k] = v.astype(dtype)\n\n    def loss(self, X, y=None):\n        """"""\n        Evaluate loss and gradient for the three-layer convolutional network.\n\n        Input / output: Same API as TwoLayerNet in fc_net.py.\n        """"""\n\n        N = X.shape[0]\n\n        W1, b1 = self.params[\'W1\'], self.params[\'b1\']\n        W2, b2 = self.params[\'W2\'], self.params[\'b2\']\n        W3, b3 = self.params[\'W3\'], self.params[\'b3\']\n\n        # pass conv_param to the forward pass for the convolutional layer\n        filter_size = W1.shape[2]\n        conv_param = {\'stride\': 1, \'pad\': (filter_size - 1) / 2}\n\n        # pass pool_param to the forward pass for the max-pooling layer\n        pool_param = {\'pool_height\': 2, \'pool_width\': 2, \'stride\': 2}\n\n        scores = None\n        #######################################################################\n        # TODO: Implement the forward pass for the three-layer convolutional net,  #\n        # computing the class scores for X and storing them in the scores          #\n        # variable.                                                                #\n        #######################################################################\n\n        # Forward into the conv layer\n        x = X\n        w = W1\n        b = b1\n        conv_layer, cache_conv_layer = conv_relu_pool_forward(\n            x, w, b, conv_param, pool_param)\n        N, F, Hp, Wp = conv_layer.shape  # output shape\n\n        # Forward into the hidden layer\n        x = conv_layer.reshape((N, F * Hp * Wp))\n        w = W2\n        b = b2\n        hidden_layer, cache_hidden_layer = affine_relu_forward(x, w, b)\n        N, Hh = hidden_layer.shape\n\n        # Forward into the linear output layer\n        x = hidden_layer\n        w = W3\n        b = b3\n        scores, cache_scores = affine_forward(x, w, b)\n\n        if y is None:\n            return scores\n\n        loss, grads = 0, {}\n        #######################################################################\n        # TODO: Implement the backward pass for the three-layer convolutional net, #\n        # storing the loss and gradients in the loss and grads variables. Compute  #\n        # data loss using softmax, and make sure that grads[k] holds the gradients #\n        # for self.params[k]. Don\'t forget to add L2 regularization!               #\n        #######################################################################\n\n        data_loss, dscores = softmax_loss(scores, y)\n        reg_loss = 0.5 * self.reg * np.sum(W1**2)\n        reg_loss += 0.5 * self.reg * np.sum(W2**2)\n        reg_loss += 0.5 * self.reg * np.sum(W3**2)\n        loss = data_loss + reg_loss\n\n        # Backpropagation\n        grads = {}\n        # Backprop into output layer\n        dx3, dW3, db3 = affine_backward(dscores, cache_scores)\n        dW3 += self.reg * W3\n\n        # Backprop into first layer\n        dx2, dW2, db2 = affine_relu_backward(dx3, cache_hidden_layer)\n        dW2 += self.reg * W2\n\n        # Backprop into the conv layer\n        dx2 = dx2.reshape(N, F, Hp, Wp)\n        dx, dW1, db1 = conv_relu_pool_backward(dx2, cache_conv_layer)\n        dW1 += self.reg * W1\n\n        grads.update({\'W1\': dW1,\n                      \'b1\': db1,\n                      \'W2\': dW2,\n                      \'b2\': db2,\n                      \'W3\': dW3,\n                      \'b3\': db3})\n\n        #######################################################################\n        #                             END OF YOUR CODE                             #\n        #######################################################################\n\n        return loss, grads\n\n\npass\n'"
cs231n/classifiers/fc_net.py,15,"b'import numpy as np\n\nfrom cs231n.layers import *\nfrom cs231n.layer_utils import *\n\n\nclass TwoLayerNet(object):\n    """"""\n    A two-layer fully-connected neural network with ReLU nonlinearity and\n    softmax loss that uses a modular layer design. We assume an input dimension\n    of D, a hidden dimension of H, and perform classification over C classes.\n\n    The architecure should be affine - relu - affine - softmax.\n\n    Note that this class does not implement gradient descent; instead, it\n    will interact with a separate Solver object that is responsible for running\n    optimization.\n\n    The learnable parameters of the model are stored in the dictionary\n    self.params that maps parameter names to numpy arrays.\n    """"""\n\n    def __init__(self, input_dim=3 * 32 * 32, hidden_dim=100, num_classes=10,\n                 weight_scale=1e-3, reg=0.0):\n        """"""\n        Initialize a new network.\n\n        Inputs:\n        - input_dim: An integer giving the size of the input\n        - hidden_dim: An integer giving the size of the hidden layer\n        - num_classes: An integer giving the number of classes to classify\n        - dropout: Scalar between 0 and 1 giving dropout strength.\n        - weight_scale: Scalar giving the standard deviation for random\n          initialization of the weights.\n        - reg: Scalar giving L2 regularization strength.\n        """"""\n        self.params = {}\n        self.reg = reg\n        self.D = input_dim\n        self.M = hidden_dim\n        self.C = num_classes\n        self.reg = reg\n\n        w1 = weight_scale * np.random.randn(self.D, self.M)\n        b1 = np.zeros(hidden_dim)\n        w2 = weight_scale * np.random.randn(self.M, self.C)\n        b2 = np.zeros(self.C)\n\n        self.params.update({\'W1\': w1,\n                            \'W2\': w2,\n                            \'b1\': b1,\n                            \'b2\': b2})\n\n    def loss(self, X, y=None):\n        """"""\n        Compute loss and gradient for a minibatch of data.\n\n        Inputs:\n        - X: Array of input data of shape (N, d_1, ..., d_k)\n        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n\n        Returns:\n        If y is None, then run a test-time forward pass of the model and return:\n        - scores: Array of shape (N, C) giving classification scores, where\n          scores[i, c] is the classification score for X[i] and class c.\n\n        If y is not None, then run a training-time forward and backward pass and\n        return a tuple of:\n        - loss: Scalar value giving the loss\n        - grads: Dictionary with the same keys as self.params, mapping parameter\n          names to gradients of the loss with respect to those parameters.\n        """"""\n\n        #######################################################################\n        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n        # in the loss variable and gradients in the grads dictionary. Compute data #\n        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n        # self.params[k]. Don\'t forget to add L2 regularization!                   #\n        #                                                                          #\n        # NOTE: To ensure that your implementation matches ours and you pass the   #\n        # automated tests, make sure that your L2 regularization includes a factor #\n        # of 0.5 to simplify the expression for the gradient.                      #\n        #######################################################################\n\n        W1, b1, W2, b2 = self.params[\'W1\'], self.params[\n            \'b1\'], self.params[\'W2\'], self.params[\'b2\']\n\n        X = X.reshape(X.shape[0], self.D)\n        # Forward into first layer\n        hidden_layer, cache_hidden_layer = affine_relu_forward(X, W1, b1)\n        # Forward into second layer\n        scores, cache_scores = affine_forward(hidden_layer, W2, b2)\n\n        # If y is None then we are in test mode so just return scores\n        if y is None:\n            return scores\n\n        data_loss, dscores = softmax_loss(scores, y)\n        reg_loss = 0.5 * self.reg * np.sum(W1**2)\n        reg_loss += 0.5 * self.reg * np.sum(W2**2)\n        loss = data_loss + reg_loss\n\n        # Backpropagaton\n        grads = {}\n        # Backprop into second layer\n        dx1, dW2, db2 = affine_backward(dscores, cache_scores)\n        dW2 += self.reg * W2\n\n        # Backprop into first layer\n        dx, dW1, db1 = affine_relu_backward(\n            dx1, cache_hidden_layer)\n        dW1 += self.reg * W1\n\n        grads.update({\'W1\': dW1,\n                      \'b1\': db1,\n                      \'W2\': dW2,\n                      \'b2\': db2})\n\n        return loss, grads\n\n\nclass FullyConnectedNet(object):\n    """"""\n    A fully-connected neural network with an arbitrary number of hidden layers,\n    ReLU nonlinearities, and a softmax loss function. This will also implement\n    dropout and batch normalization as options. For a network with L layers,\n    the architecture will be\n\n    {affine - [batch norm] - relu - [dropout]} x (L - 1) - affine - softmax\n\n    where batch normalization and dropout are optional, and the {...} block is\n    repeated L - 1 times.\n\n    Similar to the TwoLayerNet above, learnable parameters are stored in the\n    self.params dictionary and will be learned using the Solver class.\n    """"""\n\n    def __init__(self, hidden_dims, input_dim=3 * 32 * 32, num_classes=10,\n                 dropout=0, use_batchnorm=False, reg=0.0,\n                 weight_scale=1e-2, dtype=np.float32, seed=None):\n        """"""\n\n        Initialize a new FullyConnectedNet.\n\n        Inputs:\n        - hidden_dims: A list of integers giving the size of each hidden layer.\n        - input_dim: An integer giving the size of the input.\n        - num_classes: An integer giving the number of classes to classify.\n        - dropout: Scalar between 0 and 1 giving dropout strength. If dropout=0 then\n          the network should not use dropout at all.\n        - use_batchnorm: Whether or not the network should use batch normalization.\n        - reg: Scalar giving L2 regularization strength.\n        - weight_scale: Scalar giving the standard deviation for random\n          initialization of the weights.\n        - dtype: A numpy datatype object; all computations will be performed using\n          this datatype. float32 is faster but less accurate, so you should use\n          float64 for numeric gradient checking.\n        - seed: If not None, then pass this random seed to the dropout layers. This\n          will make the dropout layers deteriminstic so we can gradient check the\n          model.\n        """"""\n        self.use_batchnorm = use_batchnorm\n        self.use_dropout = dropout > 0\n        self.reg = reg\n        self.num_layers = 1 + len(hidden_dims)\n        self.dtype = dtype\n        self.params = {}\n\n        #######################################################################\n        # TODO: Initialize the parameters of the network, storing all values in    #\n        # the self.params dictionary. Store weights and biases for the first layer #\n        # in W1 and b1; for the second layer use W2 and b2, etc. Weights should be #\n        # initialized from a normal distribution with standard deviation equal to  #\n        # weight_scale and biases should be initialized to zero.                   #\n        #                                                                          #\n        # When using batch normalization, store scale and shift parameters for the #\n        # first layer in gamma1 and beta1; for the second layer use gamma2 and     #\n        # beta2, etc. Scale parameters should be initialized to one and shift      #\n        # parameters should be initialized to zero.                                #\n        #######################################################################\n        if type(hidden_dims) != list:\n            raise ValueError(\'hidden_dim has to be a list\')\n\n        self.L = len(hidden_dims) + 1\n        self.N = input_dim\n        self.C = num_classes\n        dims = [self.N] + hidden_dims + [self.C]\n        Ws = {\'W\' + str(i + 1):\n              weight_scale * np.random.randn(dims[i], dims[i + 1]) for i in range(len(dims) - 1)}\n        b = {\'b\' + str(i + 1): np.zeros(dims[i + 1])\n             for i in range(len(dims) - 1)}\n\n        self.params.update(b)\n        self.params.update(Ws)\n\n        # When using dropout we need to pass a dropout_param dictionary to each\n        # dropout layer so that the layer knows the dropout probability and the mode\n        # (train / test). You can pass the same dropout_param to each dropout layer.\n\n        self.dropout_param = {}\n        if self.use_dropout:\n            self.dropout_param = {\'mode\': \'train\', \'p\': dropout}\n            print(\'We use dropout with p =%f\' % (self.dropout_param[\'p\']))\n            if seed is not None:\n                self.dropout_param[\'seed\'] = seed\n\n        # With batch normalization we need to keep track of running means and\n        # variances, so we need to pass a special bn_param object to each batch\n        # normalization layer. You should pass self.bn_params[0] to the forward pass\n        # of the first batch normalization layer, self.bn_params[1] to the forward\n        # pass of the second batch normalization layer, etc.\n\n        if self.use_batchnorm:\n            print(\'We use batchnorm here\')\n            self.bn_params = {\'bn_param\' + str(i + 1): {\'mode\': \'train\',\n                                                        \'running_mean\': np.zeros(dims[i + 1]),\n                                                        \'running_var\': np.zeros(dims[i + 1])}\n                              for i in range(len(dims) - 2)}\n            gammas = {\'gamma\' + str(i + 1):\n                      np.ones(dims[i + 1]) for i in range(len(dims) - 2)}\n            betas = {\'beta\' + str(i + 1): np.zeros(dims[i + 1])\n                     for i in range(len(dims) - 2)}\n\n            self.params.update(betas)\n            self.params.update(gammas)\n        # Cast all parameters to the correct datatype\n        for k, v in self.params.items():\n            self.params[k] = v.astype(dtype)\n\n    def loss(self, X, y=None):\n        """"""\n        Compute loss and gradient for the fully-connected net.\n\n        Input / output: Same as TwoLayerNet above.\n        """"""\n        X = X.astype(self.dtype)\n        mode = \'test\' if y is None else \'train\'\n\n        # Set train/test mode for batchnorm params and dropout param since they\n        # behave differently during training and testing.\n        if self.dropout_param is not None:\n            self.dropout_param[\'mode\'] = mode\n        if self.use_batchnorm:\n            for key, bn_param in self.bn_params.items():\n                bn_param[mode] = mode\n\n        #######################################################################\n        # TODO: Implement the forward pass for the fully-connected net, computing  #\n        # the class scores for X and storing them in the scores variable.          #\n        #                                                                          #\n        # When using dropout, you\'ll need to pass self.dropout_param to each       #\n        # dropout forward pass.                                                    #\n        #                                                                          #\n        # When using batch normalization, you\'ll need to pass self.bn_params[0] to #\n        # the forward pass for the first batch normalization layer, pass           #\n        # self.bn_params[1] to the forward pass for the second batch normalization #\n        # layer, etc.                                                              #\n        #######################################################################\n\n        # We are gonna store everythin in a dictionnary hidden\n        hidden = {}\n        hidden[\'h0\'] = X.reshape(X.shape[0], np.prod(X.shape[1:]))\n        if self.use_dropout:\n            # dropout on the input layer\n            hdrop, cache_hdrop = dropout_forward(\n                hidden[\'h0\'], self.dropout_param)\n            hidden[\'hdrop0\'], hidden[\'cache_hdrop0\'] = hdrop, cache_hdrop\n\n        for i in range(self.L):\n            idx = i + 1\n            # Naming of the variable\n            w = self.params[\'W\' + str(idx)]\n            b = self.params[\'b\' + str(idx)]\n            h = hidden[\'h\' + str(idx - 1)]\n            if self.use_dropout:\n                h = hidden[\'hdrop\' + str(idx - 1)]\n            if self.use_batchnorm and idx != self.L:\n                gamma = self.params[\'gamma\' + str(idx)]\n                beta = self.params[\'beta\' + str(idx)]\n                bn_param = self.bn_params[\'bn_param\' + str(idx)]\n\n            # Computing of the forward pass.\n            # Special case of the last layer (output)\n            if idx == self.L:\n                h, cache_h = affine_forward(h, w, b)\n                hidden[\'h\' + str(idx)] = h\n                hidden[\'cache_h\' + str(idx)] = cache_h\n\n            # For all other layers\n            else:\n                if self.use_batchnorm:\n                    h, cache_h = affine_norm_relu_forward(\n                        h, w, b, gamma, beta, bn_param)\n                    hidden[\'h\' + str(idx)] = h\n                    hidden[\'cache_h\' + str(idx)] = cache_h\n                else:\n                    h, cache_h = affine_relu_forward(h, w, b)\n                    hidden[\'h\' + str(idx)] = h\n                    hidden[\'cache_h\' + str(idx)] = cache_h\n\n                if self.use_dropout:\n                    h = hidden[\'h\' + str(idx)]\n                    hdrop, cache_hdrop = dropout_forward(h, self.dropout_param)\n                    hidden[\'hdrop\' + str(idx)] = hdrop\n                    hidden[\'cache_hdrop\' + str(idx)] = cache_hdrop\n\n        scores = hidden[\'h\' + str(self.L)]\n\n        # If test mode return early\n        if mode == \'test\':\n            return scores\n\n        loss, grads = 0.0, {}\n\n        #######################################################################\n        # TODO: Implement the backward pass for the fully-connected net. Store the #\n        # loss in the loss variable and gradients in the grads dictionary. Compute #\n        # data loss using softmax, and make sure that grads[k] holds the gradients #\n        # for self.params[k]. Don\'t forget to add L2 regularization!               #\n        #                                                                          #\n        # When using batch normalization, you don\'t need to regularize the scale   #\n        # and shift parameters.Good to know                                        #\n        #                                                                          #\n        # NOTE: To ensure that your implementation matches ours and you pass the   #\n        # automated tests, make sure that your L2 regularization includes a factor #\n        # of 0.5 to simplify the expression for the gradient.                      #\n        #######################################################################\n\n        # Computing of the loss\n        data_loss, dscores = softmax_loss(scores, y)\n        reg_loss = 0\n        for w in [self.params[f] for f in self.params.keys() if f[0] == \'W\']:\n            reg_loss += 0.5 * self.reg * np.sum(w * w)\n\n        loss = data_loss + reg_loss\n\n        # Backward pass\n\n        hidden[\'dh\' + str(self.L)] = dscores\n        for i in range(self.L)[::-1]:\n            idx = i + 1\n            dh = hidden[\'dh\' + str(idx)]\n            h_cache = hidden[\'cache_h\' + str(idx)]\n            if idx == self.L:\n                dh, dw, db = affine_backward(dh, h_cache)\n                hidden[\'dh\' + str(idx - 1)] = dh\n                hidden[\'dW\' + str(idx)] = dw\n                hidden[\'db\' + str(idx)] = db\n\n            else:\n                if self.use_dropout:\n                    # First backprop in the dropout layer\n                    cache_hdrop = hidden[\'cache_hdrop\' + str(idx)]\n                    dh = dropout_backward(dh, cache_hdrop)\n                if self.use_batchnorm:\n                    dh, dw, db, dgamma, dbeta = affine_norm_relu_backward(\n                        dh, h_cache)\n                    hidden[\'dh\' + str(idx - 1)] = dh\n                    hidden[\'dW\' + str(idx)] = dw\n                    hidden[\'db\' + str(idx)] = db\n                    hidden[\'dgamma\' + str(idx)] = dgamma\n                    hidden[\'dbeta\' + str(idx)] = dbeta\n                else:\n                    dh, dw, db = affine_relu_backward(dh, h_cache)\n                    hidden[\'dh\' + str(idx - 1)] = dh\n                    hidden[\'dW\' + str(idx)] = dw\n                    hidden[\'db\' + str(idx)] = db\n\n        # w gradients where we add the regulariation term\n        list_dw = {key[1:]: val + self.reg * self.params[key[1:]]\n                   for key, val in hidden.items() if key[:2] == \'dW\'}\n        # Paramerters b\n        list_db = {key[1:]: val for key, val in hidden.items() if key[:2] ==\n                   \'db\'}\n        # Parameters gamma\n        list_dgamma = {key[1:]: val for key, val in hidden.items() if key[\n            :6] == \'dgamma\'}\n        # Paramters beta\n        list_dbeta = {key[1:]: val for key, val in hidden.items() if key[\n            :5] == \'dbeta\'}\n\n        grads = {}\n        grads.update(list_dw)\n        grads.update(list_db)\n        grads.update(list_dgamma)\n        grads.update(list_dbeta)\n        return loss, grads\n\n# Auxiliary function\n\n\ndef affine_norm_relu_forward(x, w, b, gamma, beta, bn_param):\n    """"""\n    Convenience layer that perorms an affine transform followed by a ReLU\n\n    Inputs:\n    - x: Input to the affine layer\n    - w, b: Weights for the affine layer\n    - gamma, beta : Weight for the batch norm regularization\n    - bn_params : Contain variable use to batch norml, running_mean and var\n\n    Returns a tuple of:\n    - out: Output from the ReLU\n    - cache: Object to give to the backward pass\n    """"""\n\n    h, h_cache = affine_forward(x, w, b)\n    hnorm, hnorm_cache = batchnorm_forward(h, gamma, beta, bn_param)\n    hnormrelu, relu_cache = relu_forward(hnorm)\n    cache = (h_cache, hnorm_cache, relu_cache)\n\n    return hnormrelu, cache\n\n\ndef affine_norm_relu_backward(dout, cache):\n    """"""\n    Backward pass for the affine-relu convenience layer\n    """"""\n    h_cache, hnorm_cache, relu_cache = cache\n\n    dhnormrelu = relu_backward(dout, relu_cache)\n    dhnorm, dgamma, dbeta = batchnorm_backward_alt(dhnormrelu, hnorm_cache)\n    dx, dw, db = affine_backward(dhnorm, h_cache)\n\n    return dx, dw, db, dgamma, dbeta\n'"
