file_path,api_count,code
codebase/__init__.py,0,b''
codebase/data_preprocessing.py,0,"b'import pandas as pd\nimport os\nimport json\nimport bert.extract_features as xf\nimport tokenization\nclass DataPreProcessing(object):\n    def __init__(self):\n        self.data_set_loc = ""dataset""\n        self.fake_news_loc = os.path.join( "".."",self.data_set_loc, ""BuzzFeed"",  ""FakeNewsContent"")\n        self.real_news_loc = os.path.join( "".."", self.data_set_loc, ""BuzzFeed"", ""RealNewsContent"")\n        self.fake_news_desc_list = []\n        self.real_news_desc_list = []\n\n\n    def extract_desc_from_files(self, file_path, data_list):\n        with open(file_path) as fd:\n            json_data = json.load(fd)\n           # data_list.append(xf.convert_examples_to_features(json_data[\'text\'], 1, tokenizer))\n            data_list.append(json_data[\'text\'].replace(""\\n"", "" ""))\n    def read_json_news_files(self):\n        for file in os.listdir(self.fake_news_loc):\n            if file.endswith("".json""):\n                self.extract_desc_from_files(os.path.join(self.fake_news_loc, file), self.fake_news_desc_list)\n        for file in os.listdir(self.real_news_loc):\n            if file.endswith("".json""):\n                self.extract_desc_from_files(os.path.join(self.real_news_loc, file), self.real_news_desc_list)\n        fake_news_df = pd.DataFrame({0: self.fake_news_desc_list})\n        fake_news_df.to_csv(""fake_news.csv"", index=False, header=False)\n        fake_news_df.columns = [""text""]\n        fake_news_df[""label""] = ""Fake""\n        real_news_df = pd.DataFrame({0: self.real_news_desc_list})\n        real_news_df.columns = [""text""]\n        real_news_df[""label""] = ""Real""\n        combined_df = pd.concat([real_news_df, fake_news_df])\n        a=1\n\n    def test_bert(self):\n        examples = xf.read_examples(os.path.join(self.fake_news_loc,""fake_news.csv""))\n        features = xf.convert_examples_to_features(examples, 91, tokenization.Tokenizer)\n        a=1\n\nif __name__ == ""__main__"":\n    ob = DataPreProcessing()\n    ob.test_bert()'"
codebase/execute_bf_pf.py,17,"b'import numpy as np\nimport tensorflow as tf\nimport argparse\n\nfrom models import GAT\nfrom utils import process\nfrom gat_adj_features import GATInputGenerator\n\nparser = argparse.ArgumentParser(\n    description=\'Task: Fake news detection, File: execute_bf_pf.py Datasets: BuzzFeed, PolitiFact\',\n)\nparser.add_argument(\'dataset\', action=""store"", choices=set((\'BuzzFeed\', \'PolitiFact\')))\ninput = vars(parser.parse_args())\ndataset = input[\'dataset\']\n\nif dataset == ""BuzzFeed"":\n    checkpt_file = \'pre_trained/BuzzFeed/mod_BuzzFeed.ckpt\'\n    lr = 0.01\n    l2_coef = 0.0005\nelse:\n    checkpt_file = \'pre_trained/PolitiFact/mod_PolitiFact.ckpt\'\n    lr = 0.01\n    l2_coef = 0.005\n\n# training params\nbatch_size = 1\nnb_epochs = 10000\npatience = 100\n# lr = 0.01  # learning rate\n# l2_coef = 0.0005  # weight decay\nhid_units = [8] # numbers of hidden units per each attention head in each layer\nn_heads = [8, 1] # additional entry for the output layer\nresidual = False\nnonlinearity = tf.nn.elu\nmodel = GAT\n\nprint(\'Dataset: \' + dataset)\nprint(\'----- Opt. hyperparams -----\')\nprint(\'lr: \' + str(lr))\nprint(\'l2_coef: \' + str(l2_coef))\nprint(\'----- Archi. hyperparams -----\')\nprint(\'nb. layers: \' + str(len(hid_units)))\nprint(\'nb. units per layer: \' + str(hid_units))\nprint(\'nb. attention heads: \' + str(n_heads))\nprint(\'residual: \' + str(residual))\nprint(\'nonlinearity: \' + str(nonlinearity))\nprint(\'model: \' + str(model))\n\ngat_ip = GATInputGenerator()\n\nadj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = gat_ip.getComps(dataset=dataset)\nfeatures, spars = process.preprocess_features(features)\n\nnb_nodes = adj.shape[0] # = features.shape[0]\nnb_feature_rows = features.shape[0]\nft_size = features.shape[1]\ny_train = np.array([np.array(yi) for yi in y_train])\nnb_classes = y_train.shape[1]\n\nadj = adj.todense()\n\nfeatures = features[np.newaxis]\nadj = adj[np.newaxis]\ny_train = y_train[np.newaxis]\ny_val = np.array([np.array(yi) for yi in y_val])\ny_val = y_val[np.newaxis]\ny_test = np.array([np.array(yi) for yi in y_test])\ny_test = y_test[np.newaxis]\ntrain_mask = np.array(train_mask)\ntrain_mask = train_mask[np.newaxis]\nval_mask = np.array(val_mask)\nval_mask = val_mask[np.newaxis]\ntest_mask = np.array(test_mask)\ntest_mask = test_mask[np.newaxis]\n\nbiases = process.adj_to_bias(adj, [nb_nodes], nhood=1)\n\nwith tf.Graph().as_default():\n    with tf.name_scope(\'input\'):\n        ftr_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, ft_size))\n        bias_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, nb_nodes))\n        lbl_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes, nb_classes))\n        msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes))\n        attn_drop = tf.placeholder(dtype=tf.float32, shape=())\n        ffd_drop = tf.placeholder(dtype=tf.float32, shape=())\n        is_train = tf.placeholder(dtype=tf.bool, shape=())\n\n    logits = model.inference(ftr_in, nb_classes, nb_nodes, is_train,\n                                attn_drop, ffd_drop,\n                                bias_mat=bias_in,\n                                hid_units=hid_units, n_heads=n_heads,\n                                residual=residual, activation=nonlinearity)\n    log_resh = tf.reshape(logits, [-1, nb_classes])\n    lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n    msk_resh = tf.reshape(msk_in, [-1])\n    loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n    accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n\n    train_op = model.training(loss, lr, l2_coef)\n\n    saver = tf.train.Saver()\n\n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n\n    vlss_mn = np.inf\n    vacc_mx = 0.0\n    curr_step = 0\n\n    with tf.Session() as sess:\n        sess.run(init_op)\n\n        train_loss_avg = 0\n        train_acc_avg = 0\n        val_loss_avg = 0\n        val_acc_avg = 0\n\n        for epoch in range(nb_epochs):\n            tr_step = 0\n            tr_size = features.shape[0]\n\n            while tr_step * batch_size < tr_size:\n                _, loss_value_tr, acc_tr = sess.run([train_op, loss, accuracy],\n                    feed_dict={\n                        ftr_in: features[tr_step*batch_size:(tr_step+1)*batch_size],\n                        bias_in: biases[tr_step*batch_size:(tr_step+1)*batch_size],\n                        lbl_in: y_train[tr_step*batch_size:(tr_step+1)*batch_size],\n                        msk_in: train_mask[tr_step*batch_size:(tr_step+1)*batch_size],\n                        is_train: True,\n                        attn_drop: 0.6, ffd_drop: 0.6})\n                train_loss_avg += loss_value_tr\n                train_acc_avg += acc_tr\n                tr_step += 1\n\n            vl_step = 0\n            vl_size = features.shape[0]\n\n            while vl_step * batch_size < vl_size:\n                loss_value_vl, acc_vl = sess.run([loss, accuracy],\n                    feed_dict={\n                        ftr_in: features[vl_step*batch_size:(vl_step+1)*batch_size],\n                        bias_in: biases[vl_step*batch_size:(vl_step+1)*batch_size],\n                        lbl_in: y_val[vl_step*batch_size:(vl_step+1)*batch_size],\n                        msk_in: val_mask[vl_step*batch_size:(vl_step+1)*batch_size],\n                        is_train: False,\n                        attn_drop: 0.0, ffd_drop: 0.0})\n                val_loss_avg += loss_value_vl\n                val_acc_avg += acc_vl\n                vl_step += 1\n\n            print(\'Training: loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f\' %\n                    (train_loss_avg/tr_step, train_acc_avg/tr_step,\n                    val_loss_avg/vl_step, val_acc_avg/vl_step))\n\n            if val_acc_avg/vl_step >= vacc_mx or val_loss_avg/vl_step <= vlss_mn:\n                if val_acc_avg/vl_step >= vacc_mx and val_loss_avg/vl_step <= vlss_mn:\n                    vacc_early_model = val_acc_avg/vl_step\n                    vlss_early_model = val_loss_avg/vl_step\n                    saver.save(sess, checkpt_file)\n                vacc_mx = np.max((val_acc_avg/vl_step, vacc_mx))\n                vlss_mn = np.min((val_loss_avg/vl_step, vlss_mn))\n                curr_step = 0\n            else:\n                curr_step += 1\n                if curr_step == patience:\n                    print(\'Early stop! Min loss: \', vlss_mn, \', Max accuracy: \', vacc_mx)\n                    print(\'Early stop model validation loss: \', vlss_early_model, \', accuracy: \', vacc_early_model)\n                    break\n\n            train_loss_avg = 0\n            train_acc_avg = 0\n            val_loss_avg = 0\n            val_acc_avg = 0\n\n        saver.restore(sess, checkpt_file)\n\n        ts_size = features.shape[0]\n        ts_step = 0\n        ts_loss = 0.0\n        ts_acc = 0.0\n\n        while ts_step * batch_size < ts_size:\n            loss_value_ts, acc_ts = sess.run([loss, accuracy],\n                feed_dict={\n                    ftr_in: features[ts_step*batch_size:(ts_step+1)*batch_size],\n                    bias_in: biases[ts_step*batch_size:(ts_step+1)*batch_size],\n                    lbl_in: y_test[ts_step*batch_size:(ts_step+1)*batch_size],\n                    msk_in: test_mask[ts_step*batch_size:(ts_step+1)*batch_size],\n                    is_train: False,\n                    attn_drop: 0.0, ffd_drop: 0.0})\n            ts_loss += loss_value_ts\n            ts_acc += acc_ts\n            ts_step += 1\n\n        print(\'Test loss:\', ts_loss/ts_step, \'; Test accuracy:\', ts_acc/ts_step)\n\n        sess.close()\n'"
codebase/execute_cora_sparse.py,17,"b""import time\nimport scipy.sparse as sp\nimport numpy as np\nimport tensorflow as tf\nimport argparse\nfrom codebase.models import GAT\nfrom codebase.models import SpGAT\nfrom codebase.utils import process\nfrom codebase.models import GAT\nfrom codebase.utils import process\nfrom codebase.gat_adj_features import GATInputGenerator\n\ncheckpt_file = 'pre_trained/cora/mod_cora.ckpt'\n\ndataset = 'cora'\n\n# training params\nbatch_size = 1\nnb_epochs = 100000\npatience = 100\nlr = 0.005  # learning rate\nl2_coef = 0.0005  # weight decay\nhid_units = [8] # numbers of hidden units per each attention head in each layer\nn_heads = [8, 1] # additional entry for the output layer\nresidual = False\nnonlinearity = tf.nn.elu\n# model = GAT\nmodel = SpGAT\n\nprint('Dataset: ' + dataset)\nprint('----- Opt. hyperparams -----')\nprint('lr: ' + str(lr))\nprint('l2_coef: ' + str(l2_coef))\nprint('----- Archi. hyperparams -----')\nprint('nb. layers: ' + str(len(hid_units)))\nprint('nb. units per layer: ' + str(hid_units))\nprint('nb. attention heads: ' + str(n_heads))\nprint('residual: ' + str(residual))\nprint('nonlinearity: ' + str(nonlinearity))\nprint('model: ' + str(model))\n\nsparse = True\ngat_ip = GATInputGenerator()\n\nadj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = gat_ip.getComps()\nfeatures, spars = process.preprocess_features(features)\n\nnb_nodes = adj.shape[0] # = features.shape[0]\nnb_feature_rows = features.shape[0]\nft_size = features.shape[1]\ny_train = np.array([np.array(yi) for yi in y_train])\ny_val = np.array([np.array(yi) for yi in y_val])\ny_test = np.array([np.array(yi) for yi in y_test])\nnb_classes = y_train.shape[1]\ntrain_mask = np.array(train_mask)\nval_mask = np.array(val_mask)\ntest_mask = np.array(test_mask)\n\nfeatures = features[np.newaxis]\ny_train = y_train[np.newaxis]\ny_val = y_val[np.newaxis]\ny_test = y_test[np.newaxis]\ntrain_mask = train_mask[np.newaxis]\nval_mask = val_mask[np.newaxis]\ntest_mask = test_mask[np.newaxis]\n\nif sparse:\n    biases = process.preprocess_adj_bias(adj)\nelse:\n    adj = adj.todense()\n    adj = adj[np.newaxis]\n    biases = process.adj_to_bias(adj, [nb_nodes], nhood=1)\n\nwith tf.Graph().as_default():\n    with tf.name_scope('input'):\n        ftr_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, ft_size))\n        if sparse:\n            #bias_idx = tf.placeholder(tf.int64)\n            #bias_val = tf.placeholder(tf.float32)\n            #bias_shape = tf.placeholder(tf.int64)\n            bias_in = tf.sparse_placeholder(dtype=tf.float32)\n        else:\n            bias_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, nb_nodes))\n        lbl_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes, nb_classes))\n        msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes))\n        attn_drop = tf.placeholder(dtype=tf.float32, shape=())\n        ffd_drop = tf.placeholder(dtype=tf.float32, shape=())\n        is_train = tf.placeholder(dtype=tf.bool, shape=())\n\n    logits = model.inference(ftr_in, nb_classes, nb_nodes, is_train,\n                                attn_drop, ffd_drop,\n                                bias_mat=bias_in,\n                                hid_units=hid_units, n_heads=n_heads,\n                                residual=residual, activation=nonlinearity)\n    log_resh = tf.reshape(logits, [-1, nb_classes])\n    lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n    msk_resh = tf.reshape(msk_in, [-1])\n    loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n    accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n\n    train_op = model.training(loss, lr, l2_coef)\n\n    saver = tf.train.Saver()\n\n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n\n    vlss_mn = np.inf\n    vacc_mx = 0.0\n    curr_step = 0\n\n    with tf.Session() as sess:\n        sess.run(init_op)\n\n        train_loss_avg = 0\n        train_acc_avg = 0\n        val_loss_avg = 0\n        val_acc_avg = 0\n\n        for epoch in range(nb_epochs):\n            tr_step = 0\n            tr_size = features.shape[0]\n\n            while tr_step * batch_size < tr_size:\n                if sparse:\n                    bbias = biases\n                else:\n                    bbias = biases[tr_step*batch_size:(tr_step+1)*batch_size]\n\n                _, loss_value_tr, acc_tr = sess.run([train_op, loss, accuracy],\n                    feed_dict={\n                        ftr_in: features[tr_step*batch_size:(tr_step+1)*batch_size],\n                        bias_in: bbias,\n                        lbl_in: y_train[tr_step*batch_size:(tr_step+1)*batch_size],\n                        msk_in: train_mask[tr_step*batch_size:(tr_step+1)*batch_size],\n                        is_train: True,\n                        attn_drop: 0.6, ffd_drop: 0.6})\n                train_loss_avg += loss_value_tr\n                train_acc_avg += acc_tr\n                tr_step += 1\n\n            vl_step = 0\n            vl_size = features.shape[0]\n\n            while vl_step * batch_size < vl_size:\n                if sparse:\n                    bbias = biases\n                else:\n                    bbias = biases[vl_step*batch_size:(vl_step+1)*batch_size]\n                loss_value_vl, acc_vl = sess.run([loss, accuracy],\n                    feed_dict={\n                        ftr_in: features[vl_step*batch_size:(vl_step+1)*batch_size],\n                        bias_in: bbias,\n                        lbl_in: y_val[vl_step*batch_size:(vl_step+1)*batch_size],\n                        msk_in: val_mask[vl_step*batch_size:(vl_step+1)*batch_size],\n                        is_train: False,\n                        attn_drop: 0.0, ffd_drop: 0.0})\n                val_loss_avg += loss_value_vl\n                val_acc_avg += acc_vl\n                vl_step += 1\n\n            print('Training: loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' %\n                    (train_loss_avg/tr_step, train_acc_avg/tr_step,\n                    val_loss_avg/vl_step, val_acc_avg/vl_step))\n\n            if val_acc_avg/vl_step >= vacc_mx or val_loss_avg/vl_step <= vlss_mn:\n                if val_acc_avg/vl_step >= vacc_mx and val_loss_avg/vl_step <= vlss_mn:\n                    vacc_early_model = val_acc_avg/vl_step\n                    vlss_early_model = val_loss_avg/vl_step\n                    saver.save(sess, checkpt_file)\n                vacc_mx = np.max((val_acc_avg/vl_step, vacc_mx))\n                vlss_mn = np.min((val_loss_avg/vl_step, vlss_mn))\n                curr_step = 0\n            else:\n                curr_step += 1\n                if curr_step == patience:\n                    print('Early stop! Min loss: ', vlss_mn, ', Max accuracy: ', vacc_mx)\n                    print('Early stop model validation loss: ', vlss_early_model, ', accuracy: ', vacc_early_model)\n                    break\n\n            train_loss_avg = 0\n            train_acc_avg = 0\n            val_loss_avg = 0\n            val_acc_avg = 0\n\n        saver.restore(sess, checkpt_file)\n\n        ts_size = features.shape[0]\n        ts_step = 0\n        ts_loss = 0.0\n        ts_acc = 0.0\n\n        while ts_step * batch_size < ts_size:\n            if sparse:\n                bbias = biases\n            else:\n                bbias = biases[ts_step*batch_size:(ts_step+1)*batch_size]\n            loss_value_ts, acc_ts = sess.run([loss, accuracy],\n                feed_dict={\n                    ftr_in: features[ts_step*batch_size:(ts_step+1)*batch_size],\n                    bias_in: bbias,\n                    lbl_in: y_test[ts_step*batch_size:(ts_step+1)*batch_size],\n                    msk_in: test_mask[ts_step*batch_size:(ts_step+1)*batch_size],\n                    is_train: False,\n                    attn_drop: 0.0, ffd_drop: 0.0})\n            ts_loss += loss_value_ts\n            ts_acc += acc_ts\n            ts_step += 1\n\n        print('Test loss:', ts_loss/ts_step, '; Test accuracy:', ts_acc/ts_step)\n\n        sess.close()\n"""
codebase/feature_matrix.py,0,"b'import json\nimport os\n\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom bert import run_classifier\nfrom bert import tokenization\n\n\nclass FeatureMatrix:\n    def __init__(self, base_path):\n        self.base_path = base_path\n\n    def get_data_from_file(self, file):\n        with open(file) as json_file:\n            data = json.load(json_file)\n            if file.split(""/"")[-2] == ""FakeNewsContent"":\n                return [data[\'text\'], 1]\n            else:\n                return [data[\'text\'], 0]\n\n    def get_folder_data(self, folder):\n        data_list = []\n        for subfolder in [""FakeNewsContent"", ""RealNewsContent""]:\n            for file in os.listdir(self.base_path + folder + ""/"" + subfolder):\n                if file.endswith("".json""):\n                    data_list.append(self.get_data_from_file(self.base_path + folder + ""/"" + subfolder + ""/"" + file))\n\n        data_df = pd.DataFrame(data_list, columns=[""text"", ""label""])\n\n        return data_df\n\n    def get_all_data(self):\n        print(""Fetching BuzzFeed data"")\n        bf_data_df = self.get_folder_data(""BuzzFeed"")\n\n        # print(""Fetching PolitiFact data"")\n        # pf_data_df = self.get_folder_data(""PolitiFact"")\n\n        # all_data_df = pd.concat([bf_data_df, pf_data_df])\n\n        return bf_data_df\n\n    def create_tokenizer_from_hub_module(self):\n        with tf.Graph().as_default():\n            bert_module = hub.Module(""https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1"")\n            tokenization_info = bert_module(signature=""tokenization_info"", as_dict=True)\n            with tf.Session() as sess:\n                vocab_file, do_lower_case = sess.run([tokenization_info[""vocab_file""],\n                                                      tokenization_info[""do_lower_case""]])\n\n        return tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n\n    def get_feature_matrix(self, dataset = ""BuzzFeed""):\n        all_data_df = self.get_folder_data(folder=dataset)\n        all_data_df = all_data_df.sample(frac=1)\n\n        inputExamples = all_data_df.apply(lambda x: run_classifier.InputExample(guid=None,\n                                                                                text_a=x[\'text\'],\n                                                                                text_b=None,\n                                                                                label=x[\'label\']),\n                                          axis=1)\n        tokenizer = self.create_tokenizer_from_hub_module()\n        features = run_classifier.convert_examples_to_features(inputExamples, [0, 1], 128, tokenizer)\n\n        train_features_list = []\n        for item in features:\n            temp = item.input_ids\n            temp.append(item.label_id)\n            train_features_list.append(temp)\n        column_names = [""feature"" + str(i) for i in range(128)]\n        column_names.append(""label"")\n        features_df = pd.DataFrame(train_features_list, columns=column_names)\n\n        return features_df\n\n\nif __name__ == ""__main__"":\n    base_path = ""../dataset/""\n\n    adj = FeatureMatrix(base_path)\n    res = adj.get_feature_matrix(""PolitiFact"")\n'"
codebase/gat_adj_features.py,0,"b'import scipy.sparse as sp\nimport pandas as pd\nfrom feature_matrix import FeatureMatrix\nimport random\n\nclass GATInputGenerator:\n    def __init__(self):\n        self.gat = ""cora""\n        self.FM = FeatureMatrix(base_path = ""../dataset/"")\n        self.label_zip = None\n\n    def getAdj(self, dataset=""BuzzFeed""):\n        if dataset == ""BuzzFeed"":\n            adj_np = pd.read_csv(""../dataset/news_news_bf_adjacency_matrix.csv"", header=None).values\n        else:\n            adj_np = pd.read_csv(""../dataset/news_news_pf_adjacency_matrix.csv"", header=None).values\n\n        return sp.csr_matrix(adj_np, dtype=int)\n\n    def getFeatures(self, dataset=""BuzzFeed""):\n        feature_df = self.FM.get_feature_matrix(dataset)\n        label = feature_df[\'label\'].tolist()\n        label_comp = [0 if each else 1 for each in label]\n        self.label_zip = list(zip(label_comp, label))\n        feature_df.drop([\'label\'], axis=1)\n        feature_np = feature_df.values\n\n        return sp.csr_matrix(feature_np, dtype=float).tolil()\n\n\n    def getYs(self, dataset=""BuzzFeed""):\n        if dataset == ""BuzzFeed"":\n            random.seed(1)\n        else:\n            random.seed(1)\n        yTrain = self.label_zip[:]\n        yVal = self.label_zip[:]\n        yTest = self.label_zip[:]\n        train_mask = [False] * len(yTrain)\n        val_mask = [False] * len(yTrain)\n        test_mask = [False] * len(yTrain)\n        n = len(yTrain)\n\n        set_of_records_range = set(range(n))\n\n        train_range = set(random.sample(set_of_records_range, k=int(n * 0.6)))\n        set_of_records_range = set_of_records_range - train_range\n\n        val_range = set(random.sample(set_of_records_range, k=int(n * 0.2)))\n        set_of_records_range = set_of_records_range - train_range\n\n        test_range = set(random.sample(set_of_records_range, k=int(n * 0.2)))\n\n        for i in train_range:\n            yVal[i] = (0,0)\n            yTest[i] = (0,0)\n            train_mask[i] = True\n        for i in val_range:\n            yTrain[i] = (0,0)\n            yTest[i] = (0,0)\n            val_mask[i] = True\n        for i in test_range:\n            yVal[i] = (0,0)\n            yTrain[i] = (0,0)\n            test_mask[i] = True\n\n        return yTrain, yVal, yTest, train_mask, val_mask, test_mask\n\n    def getComps(self, dataset=""BuzzFeed""):\n        print(dataset)\n        adj = self.getAdj(dataset)\n        features = self.getFeatures(dataset)\n\n        yTrain, yVal, yTest, train_mask, val_mask, test_mask = self.getYs(dataset)\n\n        return adj, features, yTrain, yVal, yTest, train_mask, val_mask, test_mask\n\nif __name__ == ""__main__"":\n    obj = GATInputGenerator()\n    obj.getComps(dataset=""BuzzFeed"")'"
codebase/new_adj_mat_separately.py,2,"b'import numpy as np\nimport pandas as pd\n\n\nclass AdjacencyMatrix:\n    def __init__(self, base_path):\n        self.base_path = base_path\n\n    def get_folder_data(self, folder):\n        news_df = pd.read_csv(self.base_path + folder + ""/News.txt"", header=None)\n        news_list = list(news_df[0])\n\n        users_df = pd.read_csv(self.base_path + folder + ""/User.txt"", header=None)\n        users_list = list(users_df[0])\n\n        news_user_df = pd.read_csv(self.base_path + folder + ""/"" + folder + ""NewsUser.txt"", header=None, sep=""\\t"")\n        result_list = []\n        for index, row in news_user_df.iterrows():\n            news_index = row[0]\n            user_index = row[1]\n            result_list.append([news_list[news_index - 1], users_list[user_index - 1]])\n\n        result_df = pd.DataFrame(result_list, columns=[""News"", ""Users""])\n\n        return result_df, news_df, users_df\n\n    def get_all_data(self):\n        # print(""Fetching BuzzFeed data"")\n        # bf_res_df, bf_news_df, bf_users_df = self.get_folder_data(""BuzzFeed"")\n\n        print(""Fetching PolitiFact data"")\n        pf_res_df, pf_news_df, pf_users_df = self.get_folder_data(""PolitiFact"")\n        #\n        # news_user_df = pd.concat([bf_res_df, pf_res_df])\n        # news_df = pd.concat([bf_news_df, pf_news_df])\n        # users_df = pd.concat([bf_users_df, pf_users_df])\n        #\n        # return news_user_df, news_df, users_df\n\n        return pf_res_df, pf_news_df, pf_users_df\n\n    # def get_adjacency_matrix(self):\n    #     news_user_df, news_df, users_df = self.get_all_data()\n    #     news_list = list(news_df[0].unique())\n    #     users_list = list(users_df[0].unique())\n    #\n    #     nodes = news_list + users_list\n    #     result = np.empty((len(nodes), len(nodes)))\n    #\n    #     print(""Generating Adjacency Matrix"")\n    #     for index, row in news_user_df.iterrows():\n    #         news = row[0]\n    #         user = row[1]\n    #         result[nodes.index(news)][nodes.index(user)] = 1\n    #         result[nodes.index(user)][nodes.index(news)] = 1\n    #\n    #     result_df = pd.DataFrame(result, columns=nodes, index=nodes)\n    #\n    #     print(""Dumping the Adjacency Matrix to CSV"")\n    #     result_df.to_csv(self.base_path + ""adjacency_matrix.csv"")\n    #\n    #     print(""Done"")\n    #\n    #     return result_df\n\n    def get_connected_news(self, news, df):\n        res = set()\n        user_for_news_df = df[df[""News""] == news]\n        users = user_for_news_df[""Users""].unique()\n\n        for user in users:\n            user_df = df[df[""Users""] == user]\n            for item in user_df[""News""].unique():\n                res.add(item)\n\n        return list(res)\n\n    def get_adjacency_matrix(self):\n        news_user_df, news_df, users_df = self.get_all_data()\n        news_list = list(news_df[0].unique())\n\n        result = np.empty((len(news_list), len(news_list)))\n\n        print(""Generating Adjacency Matrix"")\n        for news in news_list:\n            connected_news = self.get_connected_news(news, news_user_df)\n            for connected_news in connected_news:\n                result[news_list.index(news)][news_list.index(connected_news)] = 1\n                result[news_list.index(connected_news)][news_list.index(news)] = 1\n\n        result_df = pd.DataFrame(result, columns=news_list, index=news_list)\n\n        print(""Dumping the Adjacency Matrix to CSV"")\n        result_df.to_csv(self.base_path + ""news_news_pf_adjacency_matrix.csv"")\n\n        print(""Done"")\n\n        return result_df\n\n\nif __name__ == ""__main__"":\n    base_path = ""../dataset/""\n\n    adj = AdjacencyMatrix(base_path)\n    res = adj.get_adjacency_matrix()'"
codebase/models/__init__.py,0,b'from .gat import GAT\nfrom .sp_gat import SpGAT\n'
codebase/models/base_gattn.py,0,"b'import tensorflow as tf\n\nclass BaseGAttN:\n    def loss(logits, labels, nb_classes, class_weights):\n        sample_wts = tf.reduce_sum(tf.multiply(tf.one_hot(labels, nb_classes), class_weights), axis=-1)\n        xentropy = tf.multiply(tf.nn.sparse_softmax_cross_entropy_with_logits(\n                labels=labels, logits=logits), sample_wts)\n        return tf.reduce_mean(xentropy, name=\'xentropy_mean\')\n\n    def training(loss, lr, l2_coef):\n        # weight decay\n        vars = tf.trainable_variables()\n        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars if v.name not\n                           in [\'bias\', \'gamma\', \'b\', \'g\', \'beta\']]) * l2_coef\n\n        # optimizer\n        opt = tf.train.AdamOptimizer(learning_rate=lr)\n\n        # training op\n        train_op = opt.minimize(loss+lossL2)\n        \n        return train_op\n\n    def preshape(logits, labels, nb_classes):\n        new_sh_lab = [-1]\n        new_sh_log = [-1, nb_classes]\n        log_resh = tf.reshape(logits, new_sh_log)\n        lab_resh = tf.reshape(labels, new_sh_lab)\n        return log_resh, lab_resh\n\n    def confmat(logits, labels):\n        preds = tf.argmax(logits, axis=1)\n        return tf.confusion_matrix(labels, preds)\n\n##########################\n# Adapted from tkipf/gcn #\n##########################\n\n    def masked_softmax_cross_entropy(logits, labels, mask):\n        """"""Softmax cross-entropy loss with masking.""""""\n        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n        mask = tf.cast(mask, dtype=tf.float32)\n        mask /= tf.reduce_mean(mask)\n        loss *= mask\n        return tf.reduce_mean(loss)\n\n    def masked_sigmoid_cross_entropy(logits, labels, mask):\n        """"""Softmax cross-entropy loss with masking.""""""\n        labels = tf.cast(labels, dtype=tf.float32)\n        loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)\n        loss=tf.reduce_mean(loss,axis=1)\n        mask = tf.cast(mask, dtype=tf.float32)\n        mask /= tf.reduce_mean(mask)\n        loss *= mask\n        return tf.reduce_mean(loss)\n\n    def masked_accuracy(logits, labels, mask):\n        """"""Accuracy with masking.""""""\n        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n        accuracy_all = tf.cast(correct_prediction, tf.float32)\n        mask = tf.cast(mask, dtype=tf.float32)\n        mask /= tf.reduce_mean(mask)\n        accuracy_all *= mask\n        return tf.reduce_mean(accuracy_all)\n\n    def micro_f1(logits, labels, mask):\n        """"""Accuracy with masking.""""""\n        predicted = tf.round(tf.nn.sigmoid(logits))\n\n        # Use integers to avoid any nasty FP behaviour\n        predicted = tf.cast(predicted, dtype=tf.int32)\n        labels = tf.cast(labels, dtype=tf.int32)\n        mask = tf.cast(mask, dtype=tf.int32)\n\n        # expand the mask so that broadcasting works ([nb_nodes, 1])\n        mask = tf.expand_dims(mask, -1)\n        \n        # Count true positives, true negatives, false positives and false negatives.\n        tp = tf.count_nonzero(predicted * labels * mask)\n        tn = tf.count_nonzero((predicted - 1) * (labels - 1) * mask)\n        fp = tf.count_nonzero(predicted * (labels - 1) * mask)\n        fn = tf.count_nonzero((predicted - 1) * labels * mask)\n\n        # Calculate accuracy, precision, recall and F1 score.\n        precision = tp / (tp + fp)\n        recall = tp / (tp + fn)\n        fmeasure = (2 * precision * recall) / (precision + recall)\n        fmeasure = tf.cast(fmeasure, tf.float32)\n        return fmeasure\n'"
codebase/models/gat.py,0,"b'import numpy as np\nimport tensorflow as tf\n\nfrom utils import layers\nfrom models.base_gattn import BaseGAttN\n\nclass GAT(BaseGAttN):\n    def inference(inputs, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n            bias_mat, hid_units, n_heads, activation=tf.nn.elu, residual=False):\n        attns = []\n        for _ in range(n_heads[0]):\n            attns.append(layers.attn_head(inputs, bias_mat=bias_mat,\n                out_sz=hid_units[0], activation=activation,\n                in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n        h_1 = tf.concat(attns, axis=-1)\n        for i in range(1, len(hid_units)):\n            h_old = h_1\n            attns = []\n            for _ in range(n_heads[i]):\n                attns.append(layers.attn_head(h_1, bias_mat=bias_mat,\n                    out_sz=hid_units[i], activation=activation,\n                    in_drop=ffd_drop, coef_drop=attn_drop, residual=residual))\n            h_1 = tf.concat(attns, axis=-1)\n        out = []\n        for i in range(n_heads[-1]):\n            out.append(layers.attn_head(h_1, bias_mat=bias_mat,\n                out_sz=nb_classes, activation=lambda x: x,\n                in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n        logits = tf.add_n(out) / n_heads[-1]\n    \n        return logits\n'"
codebase/models/sp_gat.py,0,"b'import numpy as np\nimport tensorflow as tf\n\nfrom utils import layers\nfrom models.base_gattn import BaseGAttN\n\nclass SpGAT(BaseGAttN):\n    def inference(inputs, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n            bias_mat, hid_units, n_heads, activation=tf.nn.elu, \n            residual=False):\n        attns = []\n        for _ in range(n_heads[0]):\n            attns.append(layers.sp_attn_head(inputs,  \n                adj_mat=bias_mat,\n                out_sz=hid_units[0], activation=activation, nb_nodes=nb_nodes,\n                in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n        h_1 = tf.concat(attns, axis=-1)\n        for i in range(1, len(hid_units)):\n            h_old = h_1\n            attns = []\n            for _ in range(n_heads[i]):\n                attns.append(layers.sp_attn_head(h_1,  \n                    adj_mat=bias_mat,\n                    out_sz=hid_units[i], activation=activation, nb_nodes=nb_nodes,\n                    in_drop=ffd_drop, coef_drop=attn_drop, residual=residual))\n            h_1 = tf.concat(attns, axis=-1)\n        out = []\n        for i in range(n_heads[-1]):\n            out.append(layers.sp_attn_head(h_1, adj_mat=bias_mat,\n                out_sz=nb_classes, activation=lambda x: x, nb_nodes=nb_nodes,\n                in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n        logits = tf.add_n(out) / n_heads[-1]\n    \n        return logits\n'"
codebase/utils/__init__.py,0,b''
codebase/utils/layers.py,0,"b""import numpy as np\nimport tensorflow as tf\n\nconv1d = tf.layers.conv1d\n\ndef attn_head(seq, out_sz, bias_mat, activation, in_drop=0.0, coef_drop=0.0, residual=False):\n    with tf.name_scope('my_attn'):\n        if in_drop != 0.0:\n            seq = tf.nn.dropout(seq, 1.0 - in_drop)\n\n        seq_fts = tf.layers.conv1d(seq, out_sz, 1, use_bias=False)\n\n        # simplest self-attention possible\n        f_1 = tf.layers.conv1d(seq_fts, 1, 1)\n        f_2 = tf.layers.conv1d(seq_fts, 1, 1)\n        logits = f_1 + tf.transpose(f_2, [0, 2, 1])\n        coefs = tf.nn.softmax(tf.nn.leaky_relu(logits) + bias_mat)\n\n        if coef_drop != 0.0:\n            coefs = tf.nn.dropout(coefs, 1.0 - coef_drop)\n        if in_drop != 0.0:\n            seq_fts = tf.nn.dropout(seq_fts, 1.0 - in_drop)\n\n        vals = tf.matmul(coefs, seq_fts)\n        ret = tf.contrib.layers.bias_add(vals)\n\n        # residual connection\n        if residual:\n            if seq.shape[-1] != ret.shape[-1]:\n                ret = ret + conv1d(seq, ret.shape[-1], 1) # activation\n            else:\n                ret = ret + seq\n\n        return activation(ret)  # activation\n\n# Experimental sparse attention head (for running on datasets such as Pubmed)\n# N.B. Because of limitations of current TF implementation, will work _only_ if batch_size = 1!\ndef sp_attn_head(seq, out_sz, adj_mat, activation, nb_nodes, in_drop=0.0, coef_drop=0.0, residual=False):\n    with tf.name_scope('sp_attn'):\n        if in_drop != 0.0:\n            seq = tf.nn.dropout(seq, 1.0 - in_drop)\n\n        seq_fts = tf.layers.conv1d(seq, out_sz, 1, use_bias=False)\n\n        # simplest self-attention possible\n        f_1 = tf.layers.conv1d(seq_fts, 1, 1)\n        f_2 = tf.layers.conv1d(seq_fts, 1, 1)\n        \n        f_1 = tf.reshape(f_1, (nb_nodes, 1))\n        f_2 = tf.reshape(f_2, (nb_nodes, 1))\n\n        f_1 = adj_mat*f_1\n        f_2 = adj_mat * tf.transpose(f_2, [1,0])\n\n        logits = tf.sparse_add(f_1, f_2)\n        lrelu = tf.SparseTensor(indices=logits.indices, \n                values=tf.nn.leaky_relu(logits.values), \n                dense_shape=logits.dense_shape)\n        coefs = tf.sparse_softmax(lrelu)\n\n        if coef_drop != 0.0:\n            coefs = tf.SparseTensor(indices=coefs.indices,\n                    values=tf.nn.dropout(coefs.values, 1.0 - coef_drop),\n                    dense_shape=coefs.dense_shape)\n        if in_drop != 0.0:\n            seq_fts = tf.nn.dropout(seq_fts, 1.0 - in_drop)\n\n        # As tf.sparse_tensor_dense_matmul expects its arguments to have rank-2,\n        # here we make an assumption that our input is of batch size 1, and reshape appropriately.\n        # The method will fail in all other cases!\n        coefs = tf.sparse_reshape(coefs, [nb_nodes, nb_nodes])\n        seq_fts = tf.squeeze(seq_fts)\n        vals = tf.sparse_tensor_dense_matmul(coefs, seq_fts)\n        vals = tf.expand_dims(vals, axis=0)\n        vals.set_shape([1, nb_nodes, out_sz])\n        ret = tf.contrib.layers.bias_add(vals)\n\n        # residual connection\n        if residual:\n            if seq.shape[-1] != ret.shape[-1]:\n                ret = ret + conv1d(seq, ret.shape[-1], 1) # activation\n            else:\n                ret = ret + seq\n\n        return activation(ret)  # activation\n\n"""
codebase/utils/process.py,33,"b'import numpy as np\nimport pickle as pkl\nimport networkx as nx\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg.eigen.arpack import eigsh\nimport sys\n\n""""""\n Prepare adjacency matrix by expanding up to a given neighbourhood.\n This will insert loops on every node.\n Finally, the matrix is converted to bias vectors.\n Expected shape: [graph, nodes, nodes]\n""""""\ndef adj_to_bias(adj, sizes, nhood=1):\n    nb_graphs = adj.shape[0]\n    mt = np.empty(adj.shape)\n    for g in range(nb_graphs):\n        mt[g] = np.eye(adj.shape[1])\n        for _ in range(nhood):\n            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))\n        for i in range(sizes[g]):\n            for j in range(sizes[g]):\n                if mt[g][i][j] > 0.0:\n                    mt[g][i][j] = 1.0\n    return -1e9 * (1.0 - mt)\n\n\n###############################################\n# This section of code adapted from tkipf/gcn #\n###############################################\n\ndef parse_index_file(filename):\n    """"""Parse index file.""""""\n    index = []\n    for line in open(filename):\n        index.append(int(line.strip()))\n    return index\n\ndef sample_mask(idx, l):\n    """"""Create mask.""""""\n    mask = np.zeros(l)\n    mask[idx] = 1\n    return np.array(mask, dtype=np.bool)\n\ndef load_data(dataset_str): # {\'pubmed\', \'citeseer\', \'cora\'}\n    """"""Load data.""""""\n    names = [\'x\', \'y\', \'tx\', \'ty\', \'allx\', \'ally\', \'graph\']\n    objects = []\n    for i in range(len(names)):\n        with open(""data/ind.{}.{}"".format(dataset_str, names[i]), \'rb\') as f:\n            if sys.version_info > (3, 0):\n                objects.append(pkl.load(f, encoding=\'latin1\'))\n            else:\n                objects.append(pkl.load(f))\n\n    x, y, tx, ty, allx, ally, graph = tuple(objects)\n    test_idx_reorder = parse_index_file(""data/ind.{}.test.index"".format(dataset_str))\n    test_idx_range = np.sort(test_idx_reorder)\n\n    if dataset_str == \'citeseer\':\n        # Fix citeseer dataset (there are some isolated nodes in the graph)\n        # Find isolated nodes, add them as zero-vecs into the right position\n        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n        tx = tx_extended\n        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n        ty = ty_extended\n\n    features = sp.vstack((allx, tx)).tolil()\n    features[test_idx_reorder, :] = features[test_idx_range, :]\n    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n\n    labels = np.vstack((ally, ty))\n    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n\n    idx_test = test_idx_range.tolist()\n    idx_train = range(len(y))\n    idx_val = range(len(y), len(y)+500)\n\n    train_mask = sample_mask(idx_train, labels.shape[0])\n    val_mask = sample_mask(idx_val, labels.shape[0])\n    test_mask = sample_mask(idx_test, labels.shape[0])\n\n    y_train = np.zeros(labels.shape)\n    y_val = np.zeros(labels.shape)\n    y_test = np.zeros(labels.shape)\n    y_train[train_mask, :] = labels[train_mask, :]\n    y_val[val_mask, :] = labels[val_mask, :]\n    y_test[test_mask, :] = labels[test_mask, :]\n\n    print(adj.shape)\n    print(features.shape)\n\n    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n\ndef load_random_data(size):\n\n    adj = sp.random(size, size, density=0.002) # density similar to cora\n    features = sp.random(size, 1000, density=0.015)\n    int_labels = np.random.randint(7, size=(size))\n    labels = np.zeros((size, 7)) # Nx7\n    labels[np.arange(size), int_labels] = 1\n\n    train_mask = np.zeros((size,)).astype(bool)\n    train_mask[np.arange(size)[0:int(size/2)]] = 1\n\n    val_mask = np.zeros((size,)).astype(bool)\n    val_mask[np.arange(size)[int(size/2):]] = 1\n\n    test_mask = np.zeros((size,)).astype(bool)\n    test_mask[np.arange(size)[int(size/2):]] = 1\n\n    y_train = np.zeros(labels.shape)\n    y_val = np.zeros(labels.shape)\n    y_test = np.zeros(labels.shape)\n    y_train[train_mask, :] = labels[train_mask, :]\n    y_val[val_mask, :] = labels[val_mask, :]\n    y_test[test_mask, :] = labels[test_mask, :]\n  \n    # sparse NxN, sparse NxF, norm NxC, ..., norm Nx1, ...\n    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n\ndef sparse_to_tuple(sparse_mx):\n    """"""Convert sparse matrix to tuple representation.""""""\n    def to_tuple(mx):\n        if not sp.isspmatrix_coo(mx):\n            mx = mx.tocoo()\n        coords = np.vstack((mx.row, mx.col)).transpose()\n        values = mx.data\n        shape = mx.shape\n        return coords, values, shape\n\n    if isinstance(sparse_mx, list):\n        for i in range(len(sparse_mx)):\n            sparse_mx[i] = to_tuple(sparse_mx[i])\n    else:\n        sparse_mx = to_tuple(sparse_mx)\n\n    return sparse_mx\n\ndef standardize_data(f, train_mask):\n    """"""Standardize feature matrix and convert to tuple representation""""""\n    # standardize data\n    f = f.todense()\n    mu = f[train_mask == True, :].mean(axis=0)\n    sigma = f[train_mask == True, :].std(axis=0)\n    f = f[:, np.squeeze(np.array(sigma > 0))]\n    mu = f[train_mask == True, :].mean(axis=0)\n    sigma = f[train_mask == True, :].std(axis=0)\n    f = (f - mu) / sigma\n    return f\n\ndef preprocess_features(features):\n    """"""Row-normalize feature matrix and convert to tuple representation""""""\n    rowsum = np.array(features.sum(1))\n    r_inv = np.power(rowsum, -1, dtype=float).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    features = r_mat_inv.dot(features)\n    return features.todense(), sparse_to_tuple(features)\n\ndef normalize_adj(adj):\n    """"""Symmetrically normalize adjacency matrix.""""""\n    adj = sp.coo_matrix(adj)\n    rowsum = np.array(adj.sum(1))\n    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n\n\ndef preprocess_adj(adj):\n    """"""Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.""""""\n    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n    return sparse_to_tuple(adj_normalized)\n\ndef preprocess_adj_bias(adj):\n    num_nodes = adj.shape[0]\n    adj = adj + sp.eye(num_nodes)  # self-loop\n    adj[adj > 0.0] = 1.0\n    if not sp.isspmatrix_coo(adj):\n        adj = adj.tocoo()\n    adj = adj.astype(np.float32)\n    indices = np.vstack((adj.col, adj.row)).transpose()  # This is where I made a mistake, I used (adj.row, adj.col) instead\n    # return tf.SparseTensor(indices=indices, values=adj.data, dense_shape=adj.shape)\n    return indices, adj.data, adj.shape\n'"
codebase/utils/process_ppi.py,29,"b""import numpy as np\nimport json\nimport networkx as nx\nfrom networkx.readwrite import json_graph\nimport scipy.sparse as sp\nimport pdb\nimport sys\nsys.setrecursionlimit(99999)\n\n\ndef run_dfs(adj, msk, u, ind, nb_nodes):\n    if msk[u] == -1:\n        msk[u] = ind\n        #for v in range(nb_nodes):\n        for v in adj[u,:].nonzero()[1]:\n            #if adj[u,v]== 1:\n            run_dfs(adj, msk, v, ind, nb_nodes)\n\n# Use depth-first search to split a graph into subgraphs\ndef dfs_split(adj):\n    # Assume adj is of shape [nb_nodes, nb_nodes]\n    nb_nodes = adj.shape[0]\n    ret = np.full(nb_nodes, -1, dtype=np.int32)\n\n    graph_id = 0\n\n    for i in range(nb_nodes):\n        if ret[i] == -1:\n            run_dfs(adj, ret, i, graph_id, nb_nodes)\n            graph_id += 1\n\n    return ret\n\ndef test(adj, mapping):\n    nb_nodes = adj.shape[0]\n    for i in range(nb_nodes):\n        #for j in range(nb_nodes):\n        for j in adj[i, :].nonzero()[1]:\n            if mapping[i] != mapping[j]:\n              #  if adj[i,j] == 1:\n                 return False\n    return True\n\n\n\ndef find_split(adj, mapping, ds_label):\n    nb_nodes = adj.shape[0]\n    dict_splits={}\n    for i in range(nb_nodes):\n        #for j in range(nb_nodes):\n        for j in adj[i, :].nonzero()[1]:\n            if mapping[i]==0 or mapping[j]==0:\n                dict_splits[0]=None\n            elif mapping[i] == mapping[j]:\n                if ds_label[i]['val'] == ds_label[j]['val'] and ds_label[i]['test'] == ds_label[j]['test']:\n\n                    if mapping[i] not in dict_splits.keys():\n                        if ds_label[i]['val']:\n                            dict_splits[mapping[i]] = 'val'\n\n                        elif ds_label[i]['test']:\n                            dict_splits[mapping[i]]='test'\n\n                        else:\n                            dict_splits[mapping[i]] = 'train'\n\n                    else:\n                        if ds_label[i]['test']:\n                            ind_label='test'\n                        elif ds_label[i]['val']:\n                            ind_label='val'\n                        else:\n                            ind_label='train'\n                        if dict_splits[mapping[i]]!= ind_label:\n                            print ('inconsistent labels within a graph exiting!!!')\n                            return None\n                else:\n                    print ('label of both nodes different, exiting!!')\n                    return None\n    return dict_splits\n\n\n\n\ndef process_p2p():\n\n\n    print ('Loading G...')\n    with open('p2p_dataset/ppi-G.json') as jsonfile:\n        g_data = json.load(jsonfile)\n    print (len(g_data))\n    G = json_graph.node_link_graph(g_data)\n\n    #Extracting adjacency matrix\n    adj=nx.adjacency_matrix(G)\n\n    prev_key=''\n    for key, value in g_data.items():\n        if prev_key!=key:\n            print (key)\n            prev_key=key\n\n    print ('Loading id_map...')\n    with open('p2p_dataset/ppi-id_map.json') as jsonfile:\n        id_map = json.load(jsonfile)\n    print (len(id_map))\n\n    id_map = {int(k):int(v) for k,v in id_map.items()}\n    for key, value in id_map.items():\n        id_map[key]=[value]\n    print (len(id_map))\n\n    print ('Loading features...')\n    features_=np.load('p2p_dataset/ppi-feats.npy')\n    print (features_.shape)\n\n    #standarizing features\n    from sklearn.preprocessing import StandardScaler\n\n    train_ids = np.array([id_map[n] for n in G.nodes() if not G.node[n]['val'] and not G.node[n]['test']])\n    train_feats = features_[train_ids[:,0]]\n    scaler = StandardScaler()\n    scaler.fit(train_feats)\n    features_ = scaler.transform(features_)\n\n    features = sp.csr_matrix(features_).tolil()\n\n\n    print ('Loading class_map...')\n    class_map = {}\n    with open('p2p_dataset/ppi-class_map.json') as jsonfile:\n        class_map = json.load(jsonfile)\n    print (len(class_map))\n    \n    #pdb.set_trace()\n    #Split graph into sub-graphs\n    print ('Splitting graph...')\n    splits=dfs_split(adj)\n\n    #Rearrange sub-graph index and append sub-graphs with 1 or 2 nodes to bigger sub-graphs\n    print ('Re-arranging sub-graph IDs...')\n    list_splits=splits.tolist()\n    group_inc=1\n\n    for i in range(np.max(list_splits)+1):\n        if list_splits.count(i)>=3:\n            splits[np.array(list_splits) == i] =group_inc\n            group_inc+=1\n        else:\n            #splits[np.array(list_splits) == i] = 0\n            ind_nodes=np.argwhere(np.array(list_splits) == i)\n            ind_nodes=ind_nodes[:,0].tolist()\n            split=None\n            \n            for ind_node in ind_nodes:\n                if g_data['nodes'][ind_node]['val']:\n                    if split is None or split=='val':\n                        splits[np.array(list_splits) == i] = 21\n                        split='val'\n                    else:\n                        raise ValueError('new node is VAL but previously was {}'.format(split))\n                elif g_data['nodes'][ind_node]['test']:\n                    if split is None or split=='test':\n                        splits[np.array(list_splits) == i] = 23\n                        split='test'\n                    else:\n                        raise ValueError('new node is TEST but previously was {}'.format(split))\n                else:\n                    if split is None or split == 'train':\n                        splits[np.array(list_splits) == i] = 1\n                        split='train'\n                    else:\n                        pdb.set_trace()\n                        raise ValueError('new node is TRAIN but previously was {}'.format(split))\n\n    #counting number of nodes per sub-graph\n    list_splits=splits.tolist()\n    nodes_per_graph=[]\n    for i in range(1,np.max(list_splits) + 1):\n        nodes_per_graph.append(list_splits.count(i))\n\n    #Splitting adj matrix into sub-graphs\n    subgraph_nodes=np.max(nodes_per_graph)\n    adj_sub=np.empty((len(nodes_per_graph), subgraph_nodes, subgraph_nodes))\n    feat_sub = np.empty((len(nodes_per_graph), subgraph_nodes, features.shape[1]))\n    labels_sub = np.empty((len(nodes_per_graph), subgraph_nodes, 121))\n\n    for i in range(1, np.max(list_splits) + 1):\n        #Creating same size sub-graphs\n        indexes = np.where(splits == i)[0]\n        subgraph_=adj[indexes,:][:,indexes]\n\n        if subgraph_.shape[0]<subgraph_nodes or subgraph_.shape[1]<subgraph_nodes:\n            subgraph=np.identity(subgraph_nodes)\n            feats=np.zeros([subgraph_nodes, features.shape[1]])\n            labels=np.zeros([subgraph_nodes,121])\n            #adj\n            subgraph = sp.csr_matrix(subgraph).tolil()\n            subgraph[0:subgraph_.shape[0],0:subgraph_.shape[1]]=subgraph_\n            adj_sub[i-1,:,:]=subgraph.todense()\n            #feats\n            feats[0:len(indexes)]=features[indexes,:].todense()\n            feat_sub[i-1,:,:]=feats\n            #labels\n            for j,node in enumerate(indexes):\n                labels[j,:]=np.array(class_map[str(node)])\n            labels[indexes.shape[0]:subgraph_nodes,:]=np.zeros([121])\n            labels_sub[i - 1, :, :] = labels\n\n        else:\n            adj_sub[i - 1, :, :] = subgraph_.todense()\n            feat_sub[i - 1, :, :]=features[indexes,:].todense()\n            for j,node in enumerate(indexes):\n                labels[j,:]=np.array(class_map[str(node)])\n            labels_sub[i-1, :, :] = labels\n\n    # Get relation between id sub-graph and tran,val or test set\n    dict_splits = find_split(adj, splits, g_data['nodes'])\n\n    # Testing if sub graphs are isolated\n    print ('Are sub-graphs isolated?')\n    print (test(adj, splits))\n\n    #Splitting tensors into train,val and test\n    train_split=[]\n    val_split=[]\n    test_split=[]\n    for key, value in dict_splits.items():\n        if dict_splits[key]=='train':\n            train_split.append(int(key)-1)\n        elif dict_splits[key] == 'val':\n            val_split.append(int(key)-1)\n        elif dict_splits[key] == 'test':\n            test_split.append(int(key)-1)\n\n    train_adj=adj_sub[train_split,:,:]\n    val_adj=adj_sub[val_split,:,:]\n    test_adj=adj_sub[test_split,:,:]\n\n    train_feat=feat_sub[train_split,:,:]\n    val_feat = feat_sub[val_split, :, :]\n    test_feat = feat_sub[test_split, :, :]\n\n    train_labels = labels_sub[train_split, :, :]\n    val_labels = labels_sub[val_split, :, :]\n    test_labels = labels_sub[test_split, :, :]\n\n    train_nodes=np.array(nodes_per_graph[train_split[0]:train_split[-1]+1])\n    val_nodes = np.array(nodes_per_graph[val_split[0]:val_split[-1]+1])\n    test_nodes = np.array(nodes_per_graph[test_split[0]:test_split[-1]+1])\n\n\n    #Masks with ones\n\n    tr_msk = np.zeros((len(nodes_per_graph[train_split[0]:train_split[-1]+1]), subgraph_nodes))\n    vl_msk = np.zeros((len(nodes_per_graph[val_split[0]:val_split[-1] + 1]), subgraph_nodes))\n    ts_msk = np.zeros((len(nodes_per_graph[test_split[0]:test_split[-1]+1]), subgraph_nodes))\n\n    for i in range(len(train_nodes)):\n        for j in range(train_nodes[i]):\n            tr_msk[i][j] = 1\n\n    for i in range(len(val_nodes)):\n        for j in range(val_nodes[i]):\n            vl_msk[i][j] = 1\n\n    for i in range(len(test_nodes)):\n        for j in range(test_nodes[i]):\n            ts_msk[i][j] = 1\n\n    return train_adj,val_adj,test_adj,train_feat,val_feat,test_feat,train_labels,val_labels, test_labels, train_nodes, val_nodes, test_nodes, tr_msk, vl_msk, ts_msk\n"""
