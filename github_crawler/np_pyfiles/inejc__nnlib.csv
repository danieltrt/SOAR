file_path,api_count,code
setup.py,0,"b""from setuptools import setup, find_packages\n\ndescription = ('A pure Python and NumPy implementation of a neural networks'\n               'library developed for educational purposes.')\n\nwith open('README.md') as f:\n    long_description = f.read()\n\nsetup(\n    name='nnlib',\n    version='0.0.1',\n    author='Nejc Ilenic',\n    description=description,\n    long_description=long_description,\n    license='MIT',\n    keywords='neural-networks educational machine-learning deep-learning',\n    install_requires=['numpy>=1.12.0'],\n    packages=find_packages(),\n    test_suite='tests',\n    classifiers=[\n        'Intended Audience :: Education',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        'License :: OSI Approved :: MIT License',\n        'Natural Language :: English',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n    ],\n)\n"""
nnlib/__init__.py,0,b'from .model import Model\n'
nnlib/model.py,1,"b'import numpy as np\n\nfrom nnlib.utils import yield_data_in_batches, classification_accuracy\n\n_LAST_LAYER = -1\n\n\nclass Model(object):\n    """"""A computational graph that maintains the connectivity of the layers.""""""\n\n    def __init__(self):\n        self._layers = []\n        self._optimizer = None\n        self._verbose = False\n\n    def add(self, layer):\n        """"""Adds a new layer to the computational graph. The order of\n        the layers should be the same as in the forward pass.\n\n        Parameters\n        ----------\n        layer: nnlib.layers.Layer\n            A new layer added to the model.\n        """"""\n        self._layers.append(layer)\n\n    def compile(self, optimizer):\n        """"""Prepares the model for training.\n\n        Parameters\n        ----------\n        optimizer: nnlib.optimizers.Optimizer\n            Optimizer used during the training process.\n        """"""\n        self._optimizer = optimizer\n\n        for layer in self._layers:\n            if layer.has_updatable_params():\n                self._optimizer.register_layer(layer)\n\n    def fit(self, X, y, batch_size, num_epochs, shuffle=True, verbose=False):\n        """"""Trains the model.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            The input data.\n\n        y: array-like, shape (n_samples,)\n            The target values.\n\n        batch_size: int\n            Number of examples per single gradient update.\n\n        num_epochs: int\n            Number of iterations to train the model (i.e. number\n            of times every example is seen during training).\n\n        shuffle: bool, default True\n            Whether the examples are shuffled or not at each epoch\n            before putting them in batches.\n\n        verbose: bool, default False\n            Whether to report the training loss and accuracy during\n            training.\n        """"""\n        self._verbose = verbose\n\n        for epoch_index in range(num_epochs):\n            self._train_one_epoch(X, y, batch_size, shuffle, epoch_index)\n\n    def _train_one_epoch(self, X, y, batch_size, shuffle, epoch_index):\n        """"""Trains the model for one epoch. For parameters see the\n        train() method.""""""\n        X_y_batches = yield_data_in_batches(\n            X=X, y=y,\n            batch_size=batch_size,\n            shuffle=shuffle\n        )\n\n        for X_batch, y_batch in X_y_batches:\n            self._forward(X_batch, y_batch)\n            self._backward()\n            self._optimizer.update_layers()\n\n        if self._verbose:\n            self._report_after_epoch(X, y, epoch_index)\n\n    def _report_after_epoch(self, X, y, epoch_index):  # pragma: no cover\n        """"""Logs the loss and classification accuracy of the training\n        set to stdout.""""""\n        training_loss = self._forward(X, y)\n        y_pred = self.predict(X)\n        training_acc = classification_accuracy(y, y_pred)\n\n        report = ""EPOCH {:d}: training loss: {:f} ~~~ training acc: {:f}""\n        report = report.format(epoch_index, training_loss, training_acc)\n        print(report)\n\n    def predict(self, X):\n        """"""Makes predictions and returns the predicted classes.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            The input data.\n        """"""\n        probs = self.predict_proba(X)\n        return np.argmax(probs, axis=1)\n\n    def predict_proba(self, X):\n        """"""Makes predictions and returns the probabilities.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            The input data.\n        """"""\n        return self._forward(X)\n\n    def _forward(self, X, y=None):\n        """"""Performs a complete forward pass on all layers in the\n        computational graph.""""""\n        regularization_loss = 0\n\n        for layer in self._layers[:_LAST_LAYER]:\n            X = layer.forward(X)\n\n            if layer.is_regularized():\n                regularization_loss += layer.get_regularization_loss()\n\n        if y is None:\n            # probabilities are returned from the last layer if\n            # ground truths are not passed to the forward function\n            return self._layers[_LAST_LAYER].forward(X)\n\n        loss = self._layers[_LAST_LAYER].forward(X, y)\n        return loss + regularization_loss\n\n    def _backward(self):\n        """"""Performs a complete backward pass on all layers in the\n        computational graph.""""""\n        # no gradient from the top at the beginning of the backward pass\n        grad_top = self._layers[_LAST_LAYER].backward()\n\n        for layer in reversed(self._layers[:_LAST_LAYER]):\n            grad_top = layer.backward(grad_top)\n'"
nnlib/regularizers.py,1,"b'from collections import namedtuple\n\nimport numpy as np\n\n# data container for regularization loss and gradient functions\nRegularizer = namedtuple(\'Regularizer\', [\'loss\', \'grad\'])\n\n\ndef l2(lambda_):\n    """"""Constructs an L2 nnlib.regularizers.Regularizer namedtuple.\n\n    lambda_: float >= 0\n        Regularization strength. Zero equals no regularization,\n        higher values mean stronger regularization.\n    """"""\n    return Regularizer(\n        loss=lambda W: _l2(W, lambda_),\n        grad=lambda W: _d_l2(W, lambda_)\n    )\n\n\ndef _l2(W, lambda_):\n    """"""Computes the L2 (ridge) regularization loss (i.e. weights penalty\n    for the W weights).""""""\n    # the 0.5 constant simplifies the gradient expression below\n    # (the _d_l2 function)\n    return 0.5 * lambda_ * np.sum(W * W)\n\n\ndef _d_l2(W, lambda_):\n    """"""Computes the L2 (ridge) regularization gradient on the W weights.""""""\n    return lambda_ * W\n'"
nnlib/utils.py,2,"b'from itertools import islice\n\nimport numpy as np\nfrom numpy.random import permutation\n\n\ndef yield_data_in_batches(batch_size, X, y=None, shuffle=True):\n    """"""Generates batches of input data.\n\n    Parameters\n    ----------\n    batch_size: int\n        Number of examples in a single batch.\n\n    X: array-like, shape (n_samples, n_features)\n        The input data.\n\n    y: array-like, shape (n_samples,)\n        The target values. Can be omitted.\n\n    shuffle: bool, default True\n        Whether the examples are shuffled or not before\n        put into batches.\n    """"""\n    num_rows = X.shape[0]\n\n    if shuffle:\n        indices_gen = (i for i in permutation(num_rows))\n    else:\n        indices_gen = (i for i in np.arange(num_rows))\n\n    num_yielded = 0\n\n    while True:\n        batch_indices = list(islice(indices_gen, batch_size))\n        num_yielded += len(batch_indices)\n\n        if y is None:\n            yield X[batch_indices]\n        else:\n            yield X[batch_indices], y[batch_indices]\n\n        if num_yielded == num_rows:\n            return\n\n\ndef classification_accuracy(y, y_pred):\n    """"""Computes the classification accuracy.\n\n    Parameters\n    ----------\n    y: array-like, shape (n_samples,)\n        The true target values (i.e. the ground truths).\n\n    y_pred: array-like, shape (n_samples,)\n        The predicted target values.\n    """"""\n    return np.mean(np.equal(y, y_pred))\n'"
tests/__init__.py,0,b''
tests/test_fully_connected.py,5,"b'from unittest import TestCase\n\nimport numpy as np\nfrom numpy.testing import assert_array_equal, assert_array_almost_equal\n\nfrom nnlib.layers import FullyConnected\nfrom tests.utils import numerical_grad\n\n\nclass FullyConnectedTest(TestCase):\n    input_dim_1d = 3\n    num_neurons = 4\n\n    def setUp(self):\n        self.X = np.array([\n            [1, -2, 1],\n            [0, 1, 5]], dtype=float)\n\n        self.expected_Z = np.array([\n            [6, 5, 7, 4],\n            [-1, -18, 17, 27]], dtype=float)\n\n        self.grad_top = np.ones(self.expected_Z.shape)\n\n        self.W = np.array([\n            [1, -1, 5, 0],\n            [-2, -4, 1, 1],\n            [0, -3, 3, 5]], dtype=float)\n\n        self.b = np.ones((1, self.num_neurons))\n        self.layer = FullyConnected(self.input_dim_1d, self.num_neurons)\n        self.layer.W = self.W\n        self.layer.b = self.b\n\n    def test_forward(self):\n        Z = self.layer.forward(self.X)\n        assert_array_equal(Z, self.expected_Z)\n        assert_array_equal(self.layer._X_cache, self.X)\n\n    # test backward\n    def test_grad_on_W(self):\n        self.layer.forward(self.X)\n        self.layer.backward(self.grad_top)\n        d_W = self.layer.d_W\n\n        layer = self.layer\n\n        def forward_as_func_of_W(W_):\n            layer.W = W_\n            return layer.forward(self.X)\n\n        assert_array_almost_equal(\n            numerical_grad(forward_as_func_of_W, self.W),\n            d_W\n        )\n\n    def test_grad_on_b(self):\n        self.layer.forward(self.X)\n        self.layer.backward(self.grad_top)\n        d_b = self.layer.d_b\n\n        layer = self.layer\n\n        def forward_as_func_of_b(b_):\n            layer.b = b_\n            return layer.forward(self.X)\n\n        assert_array_almost_equal(\n            numerical_grad(forward_as_func_of_b, self.b),\n            d_b\n        )\n\n    def test_grad_on_X(self):\n        self.layer.forward(self.X)\n        d_X = self.layer.backward(self.grad_top)\n\n        assert_array_almost_equal(\n            numerical_grad(self.layer.forward, self.X),\n            d_X\n        )\n\n    def test_regularization_disabled(self):\n        self.assertEqual(self.layer.get_regularization_loss(), 0)\n'"
tests/test_layer.py,0,b'from unittest import TestCase\n\nfrom nnlib.layers import Layer\n\n\nclass SimpleDummyLayer(Layer):\n\n    def forward(self):\n        pass\n\n    def backward(self):\n        pass\n\n\nclass FullDummyLayer(Layer):\n\n    def forward(self):\n        pass\n\n    def backward(self):\n        pass\n\n    def get_regularization_loss(self):\n        pass\n\n    def get_updatable_params_grads_names(self):\n        pass\n\n\nclass LayerTest(TestCase):\n\n    def test_is_regularized(self):\n        layer = SimpleDummyLayer()\n        self.assertFalse(layer.is_regularized())\n\n        layer = FullDummyLayer()\n        self.assertTrue(layer.is_regularized())\n\n    def test_has_updatable_params(self):\n        layer = SimpleDummyLayer()\n        self.assertFalse(layer.has_updatable_params())\n\n        layer = FullDummyLayer()\n        self.assertTrue(layer.has_updatable_params())\n'
tests/test_model.py,2,"b'from unittest import TestCase\n\nimport numpy as np\nfrom numpy.testing import assert_array_almost_equal\n\nfrom nnlib import Model\nfrom nnlib.layers import FullyConnected, ReLU, PReLU, SoftmaxWithCrossEntropy\nfrom nnlib.optimizers import SGD\nfrom nnlib.regularizers import l2\n\n\nclass ModelTest(TestCase):\n\n    def setUp(self):\n        self.n = 50   # number of examples (data points)\n        self.d = 2    # number of features (dimensionality of the data)\n        self.h = 50  # number of neurons in the hidden layer\n        self.k = 2    # number of classes\n\n        self.X = np.random.randn(self.n, self.d)\n        self.model = Model()\n\n    def test_add_layers(self):\n        self.assertEqual(len(self.model._layers), 0)\n        self.model.add(ReLU())\n        self.assertEqual(len(self.model._layers), 1)\n        self.model.add(ReLU())\n        self.assertEqual(len(self.model._layers), 2)\n        self.model.add(ReLU())\n\n    def test_compile(self):\n        optimizer = SGD()\n        self.assertIsNone(self.model._optimizer)\n\n        self.model.add(ReLU())\n        self.model.add(PReLU())\n        self.model.compile(optimizer)\n\n        self.assertEqual(self.model._optimizer, optimizer)\n        self.assertEqual(len(self.model._optimizer._layers), 1)\n\n    def test_predict(self):\n        self.model.add(FullyConnected(self.d, self.h))\n        self.model.add(PReLU())\n        self.model.add(FullyConnected(self.h, self.k))\n        self.model.add(SoftmaxWithCrossEntropy())\n\n        y_pred = self.model.predict(self.X)\n        self.assertEqual(y_pred.shape, (self.n,))\n\n    def test_predict_proba(self):\n        self.model.add(FullyConnected(self.d, self.h, regularizer=l2(0.5)))\n        self.model.add(PReLU())\n        self.model.add(FullyConnected(self.h, self.k, regularizer=l2(0.5)))\n        self.model.add(SoftmaxWithCrossEntropy())\n\n        probs = self.model.predict_proba(self.X)\n        self.assertEqual(probs.shape, (self.n, self.k))\n        assert_array_almost_equal(np.sum(probs, axis=1), np.ones((self.n,)))\n'"
tests/test_optimizers.py,0,"b""from unittest import TestCase\n\nfrom nnlib.layers import Layer, ParamGradNames\nfrom nnlib.optimizers import SGD, SGDMomentum\n\n\nclass DummyLayer(Layer):\n\n    def __init__(self):\n        self.dummy_param0 = 10\n        self.dummy_grad0 = 2\n\n        self.dummy_param1 = 5\n        self.dummy_grad1 = 1\n\n    def forward(self):\n        pass\n\n    def backward(self):\n        pass\n\n    def get_updatable_params_grads_names(self):\n        return [\n            ParamGradNames(param_name='dummy_param0', grad_name='dummy_grad0'),\n            ParamGradNames(param_name='dummy_param1', grad_name='dummy_grad1')\n        ]\n\n\nclass SGDTest(TestCase):\n\n    def setUp(self):\n        self.sgd = SGD(lr=0.5)\n        self.sgd_m = SGDMomentum(lr=0.5, nesterov=False)\n        self.sgd_m_n = SGDMomentum(lr=0.5)\n        self.layer = DummyLayer()\n\n    def test_register_layer_sgd(self):\n        self.assertEqual(len(self.sgd._layers), 0)\n        self.sgd.register_layer(self.layer)\n        self.assertEqual(len(self.sgd._layers), 1)\n        self.sgd.register_layer(self.layer)\n        self.assertEqual(len(self.sgd._layers), 2)\n\n    def test_register_layer_sgd_m(self):\n        self.assertEqual(len(self.sgd_m._layers_caches), 0)\n        self.sgd_m.register_layer(self.layer)\n        self.assertEqual(len(self.sgd_m._layers_caches), 1)\n        self.sgd_m.register_layer(self.layer)\n        self.assertEqual(len(self.sgd_m._layers_caches), 2)\n\n    def test_register_layer_sgd_m_n(self):\n        self.assertEqual(len(self.sgd_m_n._layers_caches), 0)\n        self.sgd_m_n.register_layer(self.layer)\n        self.assertEqual(len(self.sgd_m_n._layers_caches), 1)\n        self.sgd_m_n.register_layer(self.layer)\n        self.assertEqual(len(self.sgd_m_n._layers_caches), 2)\n\n    def test_make_updates_sgd(self):\n        self.sgd.register_layer(self.layer)\n        self.sgd.update_layers()\n        self.assertEqual(self.layer.dummy_param0, 9)\n        self.assertEqual(self.layer.dummy_param1, 4.5)\n\n    def test_make_updates_sgd_m(self):\n        self.sgd_m.register_layer(self.layer)\n        self.sgd_m.update_layers()\n        self.assertEqual(self.layer.dummy_param0, 9)\n        self.assertEqual(self.layer.dummy_param1, 4.5)\n        self.sgd_m.update_layers()\n        self.assertEqual(self.layer.dummy_param0, 7.1)\n        self.assertEqual(self.layer.dummy_param1, 3.55)\n\n    def test_make_updates_sgd_m_n(self):\n        self.sgd_m_n.register_layer(self.layer)\n        self.sgd_m_n.update_layers()\n        self.assertEqual(self.layer.dummy_param0, 8.1)\n        self.assertEqual(self.layer.dummy_param1, 4.05)\n        self.sgd_m_n.update_layers()\n        self.assertEqual(self.layer.dummy_param0, 5.39)\n        self.assertEqual(self.layer.dummy_param1, 2.695)\n"""
tests/test_regularizers.py,1,"b'from unittest import TestCase\n\nimport numpy as np\nfrom numpy.testing import assert_array_almost_equal\n\nfrom nnlib.regularizers import l2\nfrom tests.utils import numerical_grad\n\n\nclass L2Test(TestCase):\n\n    def setUp(self):\n        self.W = np.array([\n            [1, -1, 5, 0],\n            [-2, -4, 1, 1],\n            [0, -3, 3, 5]], dtype=float)\n\n    def test_L2(self):\n        regularizer = l2(lambda_=0)\n        self.assertEqual(regularizer.loss(self.W), 0)\n        regularizer = l2(lambda_=2)\n        self.assertEqual(regularizer.loss(self.W), 92)\n\n    def test_L2_grad(self):\n        regularizer = l2(lambda_=0)\n        assert_array_almost_equal(\n            numerical_grad(regularizer.loss, self.W),\n            regularizer.grad(self.W)\n        )\n\n        regularizer = l2(lambda_=2)\n        assert_array_almost_equal(\n            numerical_grad(regularizer.loss, self.W),\n            regularizer.grad(self.W)\n        )\n'"
tests/test_relu.py,5,"b""from unittest import TestCase\n\nimport numpy as np\nfrom numpy.testing import assert_array_equal, assert_array_almost_equal\n\nfrom nnlib.layers import ReLU, LeakyReLU, PReLU\nfrom tests.utils import numerical_grad\n\n# don't use zeros as inputs since this introduces the kink in\n# the function (i.e. the function is non-differentiable at x=0)\nX = np.array([\n    [1, -2, 7],\n    [-1, 1, -5]], dtype=float)\n\ngrad_top = np.ones(X.shape)\n\n\nclass ReLUTest(TestCase):\n\n    def setUp(self):\n        self.X = X.copy()\n\n        self.expected_Z = np.array([\n            [1, 0, 7],\n            [0, 1, 0]], dtype=float)\n\n        self.grad_top = grad_top.copy()\n        self.layer = ReLU()\n\n    def test_forward(self):\n        Z = self.layer.forward(self.X)\n        assert_array_equal(Z, self.expected_Z)\n        assert_array_equal(self.layer._X_cache, self.X)\n\n    def test_backward(self):\n        self.layer.forward(self.X)\n        d_X = self.layer.backward(self.grad_top)\n\n        assert_array_almost_equal(\n            numerical_grad(self.layer.forward, self.X),\n            d_X\n        )\n\n\nclass LeakyReLUTest(TestCase):\n\n    def setUp(self):\n        self.X = X.copy()\n\n        self.expected_Z = np.array([\n            [1, -0.02, 7],\n            [-0.01, 1, -0.05]], dtype=float)\n\n        self.grad_top = grad_top.copy()\n        self.layer = LeakyReLU()\n\n    def test_forward(self):\n        Z = self.layer.forward(self.X)\n        assert_array_almost_equal(Z, self.expected_Z)\n\n    def test_backward(self):\n        self.layer.forward(self.X)\n        d_X = self.layer.backward(self.grad_top)\n\n        assert_array_almost_equal(\n            numerical_grad(self.layer.forward, self.X),\n            d_X\n        )\n\n\nclass PReLUTest(TestCase):\n\n    def setUp(self):\n        self.X = X.copy()\n\n        self.expected_Z = np.array([\n            [1, -0.1, 7],\n            [-0.05, 1, -0.25]], dtype=float)\n\n        self.grad_top = grad_top.copy()\n        self.layer = PReLU()\n        self.leakiness = 0.05\n        self.layer.leakiness = self.leakiness\n\n    def test_forward(self):\n        Z = self.layer.forward(self.X)\n        assert_array_almost_equal(Z, self.expected_Z)\n\n    def test_grad_on_leakiness(self):\n        self.layer.forward(self.X)\n        self.layer.backward(self.grad_top)\n        d_leakiness = self.layer.d_leakiness\n\n        layer = self.layer\n\n        def forward_as_func_of_leakiness(leakiness_):\n            layer.leakiness = leakiness_\n            return layer.forward(self.X)\n\n        self.assertAlmostEqual(\n            numerical_grad(forward_as_func_of_leakiness, self.leakiness),\n            d_leakiness\n        )\n\n    def test_grad_on_X(self):\n        self.layer.forward(self.X)\n        d_X = self.layer.backward(self.grad_top)\n\n        assert_array_almost_equal(\n            numerical_grad(self.layer.forward, self.X),\n            d_X\n        )\n"""
tests/test_softmax.py,3,"b'from unittest import TestCase\n\nimport numpy as np\nfrom numpy.testing import assert_array_almost_equal, assert_array_equal\n\nfrom nnlib.layers import SoftmaxWithCrossEntropy\nfrom tests.utils import numerical_grad\n\n\nclass SoftmaxWithCrossEntropyTest(TestCase):\n\n    def setUp(self):\n        self.X = np.array([\n            [1, -2, 1],\n            [0, 1, 5]], dtype=float)\n\n        self.y = np.array([0, 1])\n\n        self.expected_probs = np.array([\n            [0.48785555, 0.0242889, 0.48785555],\n            [0.00657326, 0.0178679, 0.97555875]], dtype=float)\n\n        self.expected_loss = 2.37124\n        self.layer = SoftmaxWithCrossEntropy()\n\n    def test_forward_with_ground_truths(self):\n        loss = self.layer.forward(self.X, self.y)\n\n        self.assertAlmostEqual(loss, self.expected_loss, places=6)\n        assert_array_almost_equal(self.layer._probs_cache, self.expected_probs)\n        assert_array_equal(self.layer._y_cache, self.y)\n\n    def test_forward_no_ground_truths(self):\n        probs = self.layer.forward(self.X, None)\n\n        self.assertEqual(probs.shape, self.expected_probs.shape)\n        self.assertIsNone(self.layer._probs_cache)\n        self.assertIsNone(self.layer._y_cache)\n\n    def test_backward(self):\n        self.layer.forward(self.X, self.y)\n        d_X = self.layer.backward()\n\n        layer = self.layer\n\n        def forward_as_func_of_X(X):\n            return layer.forward(X, self.y)\n\n        assert_array_almost_equal(\n            numerical_grad(forward_as_func_of_X, self.X),\n            d_X\n        )\n'"
tests/test_utils.py,10,"b'from unittest import TestCase\n\nimport numpy as np\nfrom numpy.testing import assert_array_almost_equal, assert_array_equal\n\nfrom nnlib.utils import classification_accuracy\nfrom nnlib.utils import yield_data_in_batches\nfrom tests.utils import numerical_grad\n\n\nclass UtilsTest(TestCase):\n\n    def setUp(self):\n        self.Y = np.array([\n            [1, 2, 2, 1],\n            [-1, -2, -1, -1],\n            [4, 5, 1, -2],\n            [8, -10, 12, 1],\n            [0, 10, -1, 2]], dtype=float)\n\n    def test_numerical_grad_ndarray(self):\n        X = np.array([\n            [-5, 1, -1, 10, -2],\n            [8, 10, -12, 3, 1],\n            [0, 0, 2, -1, 5]], dtype=float)\n\n        def dot_Y(X_):\n            return np.dot(X_, self.Y)\n\n        expected_grad_X = np.array([\n            [6, -5, 8, 11, 11],\n            [6, -5, 8, 11, 11],\n            [6, -5, 8, 11, 11]], dtype=float)\n\n        assert_array_almost_equal(\n            numerical_grad(dot_Y, X),\n            expected_grad_X\n        )\n\n    def test_numerical_grad_scalar(self):\n\n        def times_5(x_):\n            return x_ * 5\n\n        self.assertAlmostEqual(numerical_grad(times_5, 12), 5)\n\n    def test_yield_data_in_batches_no_shuffle(self):\n        batches = yield_data_in_batches(batch_size=2, X=self.Y, shuffle=False)\n\n        for i, batch in enumerate(batches):\n            if i == 0:\n                expected_batch = np.array([\n                    [1, 2, 2, 1],\n                    [-1, -2, -1, -1]], dtype=float)\n                assert_array_equal(batch, expected_batch)\n            elif i == 1:\n                expected_batch = np.array([\n                    [4, 5, 1, -2],\n                    [8, -10, 12, 1]], dtype=float)\n                assert_array_equal(batch, expected_batch)\n            else:\n                expected_batch = np.array([\n                    [0, 10, -1, 2]], dtype=float)\n                assert_array_equal(batch, expected_batch)\n\n    def test_yield_data_in_batches_shuffle(self):\n        batches = yield_data_in_batches(batch_size=2, X=self.Y)\n\n        for i, batch in enumerate(batches):\n            if i == 0 or i == 1:\n                self.assertEqual(batch.shape, (2, 4))\n            else:\n                self.assertEqual(batch.shape, (1, 4))\n\n    def test_yield_data_in_batches_shuffle_with_y(self):\n        y = np.arange(self.Y.shape[0])\n        batches = yield_data_in_batches(batch_size=2, X=self.Y, y=y)\n\n        for i, batch in enumerate(batches):\n            X_batch, y_batch = batch\n            assert_array_equal(X_batch, self.Y[y_batch])\n\n    def test_classification_accuracy(self):\n        y = np.array([3, 0, 0, 1])\n        y_pred = np.array([3, 1, 0, 2])\n        acc = classification_accuracy(y, y_pred)\n        self.assertEqual(acc, 0.5)\n'"
nnlib/layers/__init__.py,0,b'from .layer import *\nfrom .fully_connected import *\nfrom .relu import *\nfrom .softmax import *\n'
nnlib/layers/fully_connected.py,6,"b'import numpy as np\n\nfrom nnlib.layers import Layer, ParamGradNames\n\n\nclass FullyConnected(Layer):\n    """"""A fully connected or dense layer.\n\n    Parameters\n    ----------\n    num_input_neurons: int\n        Number of input neurons (i.e. the dimensionality of the input\n        data).\n\n    num_neurons: int\n        Number of neurons in this layer (i.e. output dimensionality).\n\n    regularizer: nnlib.regularizers.Regularizer, default None\n        Weight regularizer (i.e. regularizer that computes the weights\n        penalty).\n    """"""\n\n    def __init__(self, num_input_neurons, num_neurons, regularizer=None):\n        self.W = 0.01 * np.random.rand(num_input_neurons, num_neurons)\n        self.b = np.zeros((1, num_neurons))\n\n        self._X_cache = None\n        self.d_W = None\n        self.d_b = None\n\n        self._regularizer = regularizer\n\n    def forward(self, X):\n        # cache the input so that we can use it at the\n        # backward pass when computing the gradient on W\n        self._X_cache = X\n\n        Z = np.dot(X, self.W) + self.b\n        return Z\n\n    def backward(self, grad_top):\n        self.d_W = np.dot(self._X_cache.T, grad_top)\n\n        # add the regularization gradient if regularization is enabled\n        if self._regularizer is not None:\n            self.d_W += self._regularizer.grad(self.W)\n\n        self.d_b = np.sum(grad_top, axis=0, keepdims=True)\n\n        # the gradient on input is the new gradient from the\n        # top for the next layer during the backward pass\n        d_X = np.dot(grad_top, self.W.T)\n        return d_X\n\n    def get_regularization_loss(self):\n        if self._regularizer is None:\n            return 0\n\n        return self._regularizer.loss(self.W)\n\n    def get_updatable_params_grads_names(self):\n        return [\n            ParamGradNames(param_name=\'W\', grad_name=\'d_W\'),\n            ParamGradNames(param_name=\'b\', grad_name=\'d_b\')\n        ]\n'"
nnlib/layers/layer.py,0,"b'from abc import ABC, abstractmethod\nfrom collections import namedtuple\n\n# data container for parameters and gradients name pairs (object\'s named\n# attributes) returned by layers whose parameters are backproped into\n# (see the updatable_params_grads_names() method of the nnlib.layers.Layer\n# base class for more details)\nParamGradNames = namedtuple(\'ParamGradNames\', [\'param_name\', \'grad_name\'])\n\n\nclass Layer(ABC):\n    """"""Base class for all layers implementations.""""""\n\n    @abstractmethod\n    def forward(self, *args):\n        """"""Should perform a local forward pass of the layer. Parameters\n        should be the inputs to the layer and the outputs of the layer\n        should be returned.""""""\n        pass\n\n    @abstractmethod\n    def backward(self, *args):\n        """"""Should perform a local backward pass of the layer. Parameters\n        should be the gradients from the top (i.e. how layer\'s outputs\n        influence the loss) and the gradients on inputs to the layer should\n        be returned.""""""\n        pass\n\n    def is_regularized(self):\n        """"""Indicates whether the layer has weights that contribute to the\n        loss value (i.e. exposes weight penalty).""""""\n        try:\n            self.get_regularization_loss()\n        except NotImplementedError:\n            return False\n\n        return True\n\n    def get_regularization_loss(self):\n        """"""Should return the value of the regularization loss (i.e. the\n        weight penalty).""""""\n        raise NotImplementedError()\n\n    def has_updatable_params(self):\n        """"""Indicates whether the layer has parameters that should be\n        backproped into or not.""""""\n        try:\n            self.get_updatable_params_grads_names()\n        except NotImplementedError:\n            return False\n\n        return True\n\n    def get_updatable_params_grads_names(self):\n        """"""Should expose all parameters and gradients name pairs (object\'s\n        named attributes) that are backproped into. Should return a list\n        of nnlib.layers.ParamGradNames namedtuples.\n        """"""\n        raise NotImplementedError()\n'"
nnlib/layers/relu.py,4,"b'import numpy as np\n\nfrom nnlib.layers import Layer, ParamGradNames\n\n\nclass ReLU(Layer):\n    """"""A rectified linear unit layer.""""""\n\n    def __init__(self):\n        self._X_cache = None\n\n    def forward(self, X):\n        # cache the input so that we can use it at the\n        # backward pass when computing the gradient on input\n        self._X_cache = X\n\n        Z = np.maximum(0, X)\n        return Z\n\n    def backward(self, grad_top):\n        d_X = grad_top\n        d_X[self._X_cache < 0] = 0\n        return d_X\n\n\nclass LeakyReLU(Layer):\n    """"""A leaky rectified linear unit layer.\n\n    Parameters\n    ----------\n    leakiness: float, default 0.01\n        Slope in the negative part, usually between 0 and 1.\n    """"""\n\n    def __init__(self, leakiness=0.01):\n        self._leakiness = leakiness\n        self._X_cache = None\n\n    def forward(self, X):\n        # cache the input so that we can use it at the\n        # backward pass when computing the gradient on input\n        self._X_cache = X\n\n        Z = X.copy()\n        np.putmask(Z, X < 0, self._leakiness * X)\n        return Z\n\n    def backward(self, grad_top):\n        d_X = grad_top\n        d_X[self._X_cache < 0] *= self._leakiness\n        return d_X\n\n\nclass PReLU(Layer):\n    """"""A parametric rectified linear unit layer.""""""\n\n    def __init__(self):\n        self.leakiness = 0\n\n        self._X_cache = None\n        self.d_leakiness = None\n\n    def forward(self, X):\n        # cache the input so that we can use it at the\n        # backward pass when computing the gradient on input\n        self._X_cache = X\n\n        Z = X.copy()\n        np.putmask(Z, X < 0, self.leakiness * X)\n        return Z\n\n    def backward(self, grad_top):\n        d_leakiness = self._X_cache.copy()\n        d_leakiness[self._X_cache >= 0] = 0\n        d_leakiness *= grad_top\n        self.d_leakiness = np.sum(d_leakiness)\n\n        # the gradient on input is the new gradient from the\n        # top for the next layer during the backward pass\n        d_X = grad_top\n        d_X[self._X_cache < 0] *= self.leakiness\n        return d_X\n\n    def get_updatable_params_grads_names(self):\n        return [\n            ParamGradNames(param_name=\'leakiness\', grad_name=\'d_leakiness\')\n        ]\n'"
nnlib/layers/softmax.py,5,"b'import numpy as np\n\nfrom nnlib.layers import Layer\n\n\nclass SoftmaxWithCrossEntropy(Layer):\n    """"""A softmax layer with the cross entropy loss on top. The two layers are\n    merged to avoid the computation of a full Jacobian matrix (only ground\n    truth scores influence the value of the loss function).""""""\n\n    def __init__(self):\n        self._y_cache = None\n        self._probs_cache = None\n\n    def forward(self, X, y=None):\n        # shift the input so that the highest value is\n        # zero (improve numerical stability)\n        X -= np.max(X, axis=1).reshape((-1, 1))\n\n        probs = np.exp(X)\n        probs /= np.sum(probs, axis=1, keepdims=True)\n\n        # ground truths are not present during test time so we can\'t\n        # compute the value of the loss function and just return the\n        # probabilities instead\n        if y is None:\n            return probs\n\n        # cache the class vector and the output of the softmax\n        # layer so that we can use them at the backward pass\n        # when computing the gradient on input\n        self._y_cache = y\n        self._probs_cache = probs\n\n        # compute the value of the loss function\n        num_examples = X.shape[0]\n\n        loss_i = - np.log(probs[range(num_examples), y])\n        loss = np.mean(loss_i)\n        return loss\n\n    def backward(self):\n        num_examples = self._probs_cache.shape[0]\n\n        d_X = self._probs_cache.copy()\n        d_X[range(num_examples), self._y_cache] -= 1\n        d_X /= num_examples\n        return d_X\n'"
nnlib/optimizers/__init__.py,0,b'from .optimizer import *\nfrom .sgd import *\n'
nnlib/optimizers/optimizer.py,0,"b'from abc import ABC, abstractmethod\n\n\nclass Optimizer(ABC):\n    """"""Base class for all optimizers.""""""\n\n    def __init__(self, lr):\n        self._lr = lr\n\n    @abstractmethod\n    def register_layer(self, updatable_layer):\n        """"""Should index the layer and init everything needed to update\n        it later.""""""\n        pass\n\n    @abstractmethod\n    def update_layers(self):\n        """"""Should update all registered (updatable) layers\' parameters.""""""\n        pass\n'"
nnlib/optimizers/sgd.py,0,"b'from nnlib.optimizers import Optimizer\n\n\nclass SGD(Optimizer):\n    """"""Vanilla stochastic gradient descent optimizer.\n\n    Parameters\n    ----------\n    lr: float >= 0, default 0.01\n        Learning rate.\n    """"""\n\n    def __init__(self, lr=0.01):\n        super().__init__(lr)\n        self._layers = []\n\n    def register_layer(self, updatable_layer):\n        self._layers.append(updatable_layer)\n\n    def update_layers(self):\n        for layer in self._layers:\n            self._update_layer(layer)\n\n    def _update_layer(self, layer):\n        params_grads_names = layer.get_updatable_params_grads_names()\n\n        for param_name, grad_name in params_grads_names:\n            param = getattr(layer, param_name)\n            grad = getattr(layer, grad_name)\n\n            param += - self._lr * grad\n            setattr(layer, param_name, param)\n\n\nclass SGDMomentum(Optimizer):\n    """"""Stochastic gradient descent optimizer with momentum updates.\n\n    Parameters\n    ----------\n    lr: float >= 0, default 0.01\n        Learning rate.\n\n    momentum: float > 0 and <= 1, default 0.9\n        Hyperparameter that controls the momentum, could be interpreted\n        as the coefficient of friction.\n\n    nesterov: bool, default True\n        Whether to use the Nesterov momentum update.\n    """"""\n\n    def __init__(self, lr=0.01, momentum=0.9, nesterov=True):\n        super().__init__(lr)\n        self._layers_caches = []\n        self._momentum = momentum\n        self._nesterov = nesterov\n\n    def register_layer(self, updatable_layer):\n        num_params = len(updatable_layer.get_updatable_params_grads_names())\n        layer_cache = {\n            \'layer\': updatable_layer,\n            \'velocities\': [0 for _ in range(num_params)]\n        }\n\n        self._layers_caches.append(layer_cache)\n\n    def update_layers(self):\n        for layer_cache in self._layers_caches:\n            self._update_layer(layer_cache)\n\n    def _update_layer(self, layer_cache):\n        layer = layer_cache[\'layer\']\n        velocities = layer_cache[\'velocities\']\n\n        params_grads_names = layer.get_updatable_params_grads_names()\n\n        for v_i, param_grad_name in enumerate(params_grads_names):\n            param_name, grad_name = param_grad_name\n            param = getattr(layer, param_name)\n            grad = getattr(layer, grad_name)\n\n            v = velocities[v_i]\n\n            if self._nesterov is True:\n                v_prev = v\n\n            # integrate velocity and store it\n            v = self._momentum * v - self._lr * grad\n            velocities[v_i] = v\n\n            # perform actual parameter update\n            if self._nesterov is True:\n                param += - self._momentum * v_prev + (1 + self._momentum) * v\n            else:\n                param += v\n\n            setattr(layer, param_name, param)\n'"
tests/integration_tests/__init__.py,0,b''
tests/integration_tests/test_xor_data.py,1,"b'from unittest import TestCase\n\nimport numpy as np\n\nfrom nnlib import Model\nfrom nnlib.layers import FullyConnected, ReLU, SoftmaxWithCrossEntropy\nfrom nnlib.optimizers import SGD\nfrom nnlib.utils import classification_accuracy\nfrom tests.utils import xor_data\n\n\nclass XorDataTest(TestCase):\n\n    def test_training_accuracy(self):\n        n = 50   # number of examples (data points)\n        d = 2    # number of features (dimensionality of the data)\n        h = 50   # number of neurons in the hidden layer\n        k = 2    # number of classes\n\n        np.random.seed(0)\n        X, y = xor_data(num_examples=n)\n\n        model = Model()\n        model.add(FullyConnected(num_input_neurons=d, num_neurons=h))\n        model.add(ReLU())\n        model.add(FullyConnected(num_input_neurons=h, num_neurons=k))\n        model.add(SoftmaxWithCrossEntropy())\n        model.compile(SGD(lr=1))\n\n        model.fit(X, y, batch_size=n, num_epochs=180)\n        y_pred = model.predict(X)\n\n        acc = classification_accuracy(y, y_pred)\n        self.assertEqual(acc, 1)\n'"
tests/utils/__init__.py,0,b'from .datasets import *\nfrom .misc import *\n'
tests/utils/datasets.py,1,"b'import numpy as np\nfrom numpy.random import randn\n\n\ndef xor_data(num_examples, noise=None):\n    X = randn(num_examples, 2)\n\n    if noise is None:\n        X_ = X\n    else:\n        X_ = X + noise * randn(num_examples, 2)\n\n    y = np.logical_xor(X_[:, 0] > 0, X_[:, 1] > 0).astype(int)\n    return X, y\n'"
tests/utils/misc.py,5,"b'import numpy as np\n\n\ndef numerical_grad(func, input_, h=1e-6):\n    """"""Computes partial derivatives of func wrt. input_ using the\n    center divided difference method. Used to gradient check\n    analytical solutions.\n\n    Parameters\n    ----------\n    func: callable\n        A function whose derivatives should be computed.\n\n    input_: scalar or array-like\n        Partial derivatives are computed wrt. input_.\n\n    h: float, default 1e-6\n        A spacing used when computing the difference, should\n        be small.\n\n    Returns\n    -------\n        grad: scalar or array-like of shape input_.shape\n    """"""\n\n    if np.isscalar(input_):\n        return np.sum((func(input_ + h) - func(input_ - h)) / (2 * h))\n\n    grad = np.zeros(input_.shape)\n\n    for i in np.ndindex(input_.shape):\n        forward = input_.copy()\n        forward[i] += h\n\n        backward = input_.copy()\n        backward[i] -= h\n\n        center_divided_diff = (func(forward) - func(backward)) / (2 * h)\n        grad[i] = np.sum(center_divided_diff)\n\n    return grad\n'"
