file_path,api_count,code
plot.py,1,"b'# to- do\nimport matplotlib.pyplot as plt; plt.rcdefaults()\nimport numpy as np\n\nobjects =[ \'Nk\',\'prafful\',\'pravar\',\'ahaan\']\ny_pos = np.arrange(len(objects))\nper =[10,20,30,25]\n\nplt.bar(y_pos,per,align=\'center\',alpha=0.5)\n\nplt.xticks(y_pos,object)\nplt.ylabel(\'Her\')\nplt.title(""Hello World"")\n\nplt.imshow()'"
Base/beat.py,0,"b""# beats per min goes here\n#from __future__ import print_function\n\nimport librosa\n\nimport librosa\n\n\n#add default location here\nfilename = librosa.util.example_audio_file()\n\ny, s = librosa.load(filename)\n\ntemp, frames = librosa.beat.beat_track(y=y, sr=s)\n\nprint('Estimated tempo: {:.2f} beats per minute'.format(temp))\n\nbpm = librosa.frames_to_time(frames, sr=s)\n\nprint('Saving output to bpm.csv')\nlibrosa.output.times_csv('bpm.csv', bpm)\n"""
Base/detect.py,0,"b""# to - do\n\n'''\n\nadd matplotlib to plot detected accuracy\n\nNote : No learning done .\n\nto - sklern\n\n'''\nimport  scipy.spatial as sp\nimport  numpy as np\n\n\n"""
Base/extract_features.py,4,"b'# exracting faetures\n\n""""""\n\nRead doc - https://librosa.github.io/librosa/install.html#pypi\n\nTerminlogy :\n    sr = sampling rate ( default : 22050 hz)\n    frame = short audio clip / snippet\n    n_fft = No of samples per audio frame ( 2048 )\n    hop_lenght : samples between frames ( 512 )\n\nuse - Freesounds.org -> .wav\n\n""""""\nimport  sklearn\nimport  matplotlib\nimport numpy as np\nimport librosa.display\n\n# Load from dir - .wav file : , init example\nprint(librosa.util.example_audio_file())\ny, sr = librosa.load(librosa.util.example_audio_file())\n\nhop = 512\n\n# components\nharmonic,percussive = librosa.effects.hpss(y)\n\ntempo, beat_frames = librosa.beat.beat_track(y=y,\n                                             sr=sr)\n\n# Compute mfcc features from the raw signals from components\nmfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop, n_mfcc=13)\n\n# first-order differences (delta features)\nmfcc_delta = librosa.feature.delta(mfcc)\n\n# Stack and synchronize between beat events\nbeat_mfcc_delta = librosa.display.util.sync(np.vstack([mfcc, mfcc_delta]),beat_frames)\n\n# Compute chroma features from the harmonic signal\nchromagram = librosa.feature.chroma_cqt(y=harmonic,\n                                        sr=sr)\n\n# Aggregate chroma features between beat events . use median value of each feature between beat frames\nbeat_chroma = librosa.display.util.sync(chromagram,\n                                        beat_frames,\n                                        aggregate=np.median)\n\n# stack all beat-synchronous features together\nbeat_features = np.vstack([beat_chroma, beat_mfcc_delta])\n\n# saving as .mfcc files\n\n#saving numpy.nd in output.mfcc\n\n\n# remember to extract only [1st , middle , last]\n# now i have rows= 38 , column = dependencies on duration\n\ntranspose = beat_features.transpose()\n\noutput = \'test.mfcc\'\nnp.savetxt(output,transpose[0])\nprint(transpose[0])'"
Base/record.py,0,"b'# record here\n# automate this to record test data\n\nimport os\nimport pyaudio\nimport wave\nimport array\nimport numpy\nimport time\n\nCHUNK = 1225\nFORMAT = pyaudio.paInt16\nCHANNELS = 2\nRATE = 44100\nRECORD_SECONDS = 40\nWAVE_OUTPUT_FILENAME = \'output.wav\'\n\nthreshold = 200\n\np = pyaudio.PyAudio()\n\nos.system(\'say Say something\')\ntime.sleep(3)\nstream = p.open(format=FORMAT,\n                channels=CHANNELS,\n                rate=RATE,\n                input=True,\n                frames_per_buffer=CHUNK)\n\n\n\n\nprint(""* recording"")\nframes = []\n\nnum_sil_frames = 0\n\n# The device will record for 20 seconds until there has been 0.5\n# second of audio (18 frames) where the energy is less than the threshold\n\nfor _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n    data = stream.read(CHUNK)\n    frames.append(data)\n    nums = array.array(\'h\', data)\n    left = numpy.array(nums[1::2])\n    energy = 10 * numpy.log(numpy.sum(numpy.power(left, 2)))\n    # print(energy)\n\n    if (energy < threshold):\n        num_sil_frames = num_sil_frames + 1\n    else:\n        num_sil_frames = 0\n\n    if num_sil_frames >= 37:\n        break\n\nos.system(\'say Recording done\')\nprint(""* done recording"")\n\nstream.stop_stream()\nstream.close()\np.terminate()\n\nwf = wave.open(WAVE_OUTPUT_FILENAME, \'wb\')\nwf.setnchannels(CHANNELS)\nwf.setsampwidth(p.get_sample_size(FORMAT))\nwf.setframerate(RATE)\nwf.writeframes(b\'\'.join(frames))\nwf.close()\ntime.sleep(3)\n'"
Intro/beat.py,0,"b""# beats per min goes here\n#from __future__ import print_function\n\nimport librosa\n\nimport librosa\n\n\n#add default location here\nfilename = librosa.util.example_audio_file()\n\ny, s = librosa.load(filename)\n\ntemp, frames = librosa.beat.beat_track(y=y, sr=s)\n\nprint('Estimated tempo: {:.2f} beats per minute'.format(temp))\n\nbpm = librosa.frames_to_time(frames, sr=s)\n\nprint('Saving output to bpm.csv')\nlibrosa.output.times_csv('bpm.csv', bpm)\n"""
Intro/detect.py,0,"b""# to - do\n\n'''\n\nadd matplotlib to plot detected accuracy\n\nNote : No learning done .\n\nto - sklern\n\n'''\nimport  scipy.spatial as sp\nimport  numpy as np\n\n\n"""
Intro/extract_features.py,4,"b'# exracting faetures\n\n""""""\n\nRead doc - https://librosa.github.io/librosa/install.html#pypi\n\nTerminlogy :\n    sr = sampling rate ( default : 22050 hz)\n    frame = short audio clip / snippet\n    n_fft = No of samples per audio frame ( 2048 )\n    hop_lenght : samples between frames ( 512 )\n\nuse - Freesounds.org -> .wav\n\n""""""\nimport  sklearn\nimport  matplotlib\nimport numpy as np\nimport librosa.display\n\n# Load from dir - .wav file : , init example\nprint(librosa.util.example_audio_file())\ny, sr = librosa.load(librosa.util.example_audio_file())\n\nhop = 512\n\n# components\nharmonic,percussive = librosa.effects.hpss(y)\n\ntempo, beat_frames = librosa.beat.beat_track(y=y,\n                                             sr=sr)\n\n# Compute mfcc features from the raw signals from components\nmfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop, n_mfcc=13)\n\n# first-order differences (delta features)\nmfcc_delta = librosa.feature.delta(mfcc)\n\n# Stack and synchronize between beat events\nbeat_mfcc_delta = librosa.display.util.sync(np.vstack([mfcc, mfcc_delta]),beat_frames)\n\n# Compute chroma features from the harmonic signal\nchromagram = librosa.feature.chroma_cqt(y=harmonic,\n                                        sr=sr)\n\n# Aggregate chroma features between beat events . use median value of each feature between beat frames\nbeat_chroma = librosa.display.util.sync(chromagram,\n                                        beat_frames,\n                                        aggregate=np.median)\n\n# stack all beat-synchronous features together\nbeat_features = np.vstack([beat_chroma, beat_mfcc_delta])\n\n# saving as .mfcc files\n\n#saving numpy.nd in output.mfcc\n\n\n# remember to extract only [1st , middle , last]\n# now i have rows= 38 , column = dependencies on duration\n\ntranspose = beat_features.transpose()\n\noutput = \'test.mfcc\'\nnp.savetxt(output,transpose[0])\nprint(transpose[0])'"
Intro/record.py,0,"b'# record here\n# automate this to record test data\n\nimport os\nimport pyaudio\nimport wave\nimport array\nimport numpy\nimport time\n\nCHUNK = 1225\nFORMAT = pyaudio.paInt16\nCHANNELS = 2\nRATE = 44100\nRECORD_SECONDS = 40\nWAVE_OUTPUT_FILENAME = \'output.wav\'\n\nthreshold = 200\n\np = pyaudio.PyAudio()\n\nos.system(\'say Say something\')\ntime.sleep(3)\nstream = p.open(format=FORMAT,\n                channels=CHANNELS,\n                rate=RATE,\n                input=True,\n                frames_per_buffer=CHUNK)\n\n\n\n\nprint(""* recording"")\nframes = []\n\nnum_sil_frames = 0\n\n# The device will record for 20 seconds until there has been 0.5\n# second of audio (18 frames) where the energy is less than the threshold\n\nfor _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n    data = stream.read(CHUNK)\n    frames.append(data)\n    nums = array.array(\'h\', data)\n    left = numpy.array(nums[1::2])\n    energy = 10 * numpy.log(numpy.sum(numpy.power(left, 2)))\n    # print(energy)\n\n    if (energy < threshold):\n        num_sil_frames = num_sil_frames + 1\n    else:\n        num_sil_frames = 0\n\n    if num_sil_frames >= 37:\n        break\n\nos.system(\'say Recording done\')\nprint(""* done recording"")\n\nstream.stop_stream()\nstream.close()\np.terminate()\n\nwf = wave.open(WAVE_OUTPUT_FILENAME, \'wb\')\nwf.setnchannels(CHANNELS)\nwf.setsampwidth(p.get_sample_size(FORMAT))\nwf.setframerate(RATE)\nwf.writeframes(b\'\'.join(frames))\nwf.close()\ntime.sleep(3)\n'"
animals/beat.py,0,"b""# beats per min goes here\n#from __future__ import print_function\n\nimport librosa\n\nimport librosa\n\n\n#add default location here\nfilename = librosa.util.example_audio_file()\n\ny, s = librosa.load(filename)\n\ntemp, frames = librosa.beat.beat_track(y=y, sr=s)\n\nprint('Estimated tempo: {:.2f} beats per minute'.format(temp))\n\nbpm = librosa.frames_to_time(frames, sr=s)\n\nprint('Saving output to bpm.csv')\nlibrosa.output.times_csv('bpm.csv', bpm)\n"""
animals/detect.py,0,"b""# to - do\n\n'''\n\nadd matplotlib to plot detected accuracy\n\nNote : No learning done .\n\nto - sklern\n\n'''\n# !/usr/bin/python\n\nimport sys\nimport numpy\nimport scipy.spatial as scip\nimport time\nimport random\n\nNUMBER_OF_TRAINING_INSTANCES = 10\n\n\n# Compute DTW Distance\ndef dtwdis(temp, inp):\n    # Use eclidean / mahalanobis distance\n    dmat = scip.distance.cdist(temp, inp, 'euclidean')\n    m, n = numpy.shape(dmat)\n\n    dcost = numpy.ones((m + 2, n + 1))\n    dcost = dcost + numpy.inf\n\n    # Computing Dynamic Time Warping Table. Not doing pruning for the demo.\n    dcost[2, 1] = dmat[0, 0]\n    k = 3\n    for j in range(2, n + 1):\n        for i in range(2, min(2 + k, m + 2)):\n            dcost[i, j] = min(dcost[i, j - 1], dcost[i - 1, j - 1], dcost[i - 2, j - 1]) + dmat[i - 2, j - 1]\n        k = k + 2\n    return (dcost[m + 1, n])\n\n\ninp = 'test.mfcc'\n\ndef main():\n    st = time.clock()\n    inpdat = numpy.loadtxt(inp)\n    cost = []\n    trl = range(1, NUMBER_OF_TRAINING_INSTANCES)\n    animal_array = ['dogm', 'catm','lionm']\n    for i in range(0, 3):\n        cos = []\n        for m in trl:\n            temdat = numpy.loadtxt('train/' + animal_array[i] + '/' + str(m) + '.mfcc')\n            fcost = dtwdis(temdat, inpdat)\n            print('{0}_{1}  distance from input : --->  {2}'.format(animal_array[i], m, fcost))\n            cos[len(cos):] = [fcost]\n        print()\n        cost[len(cost):] = [sum(cos)//NUMBER_OF_TRAINING_INSTANCES]\n    print(cost)\n    # print '\\n'\n    animal_index = cost.index(min(cost))\n    print('Animal recognised as {0}'.format(animal_array[animal_index][:-1]))\n    et = time.clock()\n    print('{0} seconds'.format(et - st))\n\n    # Deleting -r mode for demo\n\n\nif __name__ == '__main__':\n    main()"""
animals/extract_features.py,6,"b'# exracting faetures\n""""""\nversion:\nRead doc - https://librosa.github.io/librosa/install.html#pypi\n\nTerminlogy :\n    sr = sampling rate ( default : 22050 hz)\n    frame = short audio clip / snippet\n    n_fft = No of samples per audio frame ( 2048 )\n    hop_lenght : samples between frames ( 512 )\n\nuse - Freesounds.org -> .wav\nto-add matplotlib\n\n""""""\n\nimport  sklearn\nimport timeit\nimport  matplotlib\nimport numpy as np\nimport librosa.display\n\n# Load from dir - .wav file : , init example\n# dir = train/cats .mp3 train/dogs .mp3\n\ndata = [""cat"",""dog"",""lion""]\n#version count\nstart=1\nstop = int(input(""Enter number of training instances (min =1 , max = 10 for now)""))\n\nst = timeit.default_timer()\nfor animal_name in data:\n    for v in range(start,stop):\n        #dir here\n        if animal_name==\'cat\' or animal_name==\'dog\':\n            y, sr = librosa.load(\'train/\'+animal_name+\'/sound\'+str(v)+\'.wav\')\n        else :\n            y,sr = librosa.load(\'train/\'+animal_name+\'/\'+str(v)+\'.mp3\')\n        hop = 512\n        harmonic, percussive = librosa.effects.hpss(y)\n\n        tempo, beat_frames = librosa.beat.beat_track(y=y,\n                                                     sr=sr)\n        mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop, n_mfcc=13)\n\n        mfcc_delta = librosa.feature.delta(mfcc)\n\n        beat_mfcc_delta = librosa.display.util.sync(np.vstack([mfcc, mfcc_delta]), beat_frames)\n\n        chromagram = librosa.feature.chroma_cqt(y=harmonic,\n                                                sr=sr)\n        beat_chroma = librosa.display.util.sync(chromagram,\n                                                beat_frames,\n                                                aggregate=np.median)\n        beat_features = np.vstack([beat_chroma, beat_mfcc_delta])\n\n        transpose = beat_features.transpose()\n\n        # output dir -  mono/animal-name\n        # saving as .mfcc files\n        # saving numpy.nd in output.mfcc\n        # remember to extract only [1st , middle , last]\n        # now i have rows = 38 , column = dependencies on duration\n        #print(transpose)\n        output =  ""train/""+animal_name+\'m\'+\'/\'+ str(v)+\'.mfcc\'\n        data=np.vstack([transpose[1],transpose[len(transpose)//2],transpose[len(transpose)-2]])\n        data = np.transpose(data)\n        np.savetxt(output, data)\n\nsp=timeit.default_timer()\nprint(""time taken = "",(sp-st))'"
animals/test.py,6,"b'\nimport  sklearn\nimport timeit\nimport  matplotlib\nimport numpy as np\nimport librosa.display\n\n# dir here\nprint(""Detect animal "")\nan= input(""Select animal for testing (dog,cat,lion): "")\nnu = int(input(""Enter any number less than or equal to 10""))\ny, sr = librosa.load(""train/""+an+""/sound""+str(10)+"".wav"")\n\nhop = 512\nharmonic, percussive = librosa.effects.hpss(y)\n\ntempo, beat_frames = librosa.beat.beat_track(y=y,\n                                             sr=sr)\nmfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop, n_mfcc=13)\n\nmfcc_delta = librosa.feature.delta(mfcc)\n\nbeat_mfcc_delta = librosa.display.util.sync(np.vstack([mfcc, mfcc_delta]), beat_frames)\n\nchromagram = librosa.feature.chroma_cqt(y=harmonic,\n                                        sr=sr)\nbeat_chroma = librosa.display.util.sync(chromagram,\n                                        beat_frames,\n                                        aggregate=np.median)\nbeat_features = np.vstack([beat_chroma, beat_mfcc_delta])\n\ntranspose = beat_features.transpose()\n\n# output dir -  mono/animal-name\n# saving as .mfcc files\n# saving numpy.nd in output.mfcc\n# remember to extract only [1st , middle , last]\n# now i have rows = 38 , column = dependencies on duration\n# print(transpose)\noutput =  \'test.mfcc\'\ndata = np.vstack([transpose[1], transpose[len(transpose) // 2], transpose[len(transpose) - 2]])\ndata = np.transpose(data)\nnp.savetxt(output, data)\n\nprint(""done"")'"
music/beat.py,0,"b""# beats per min goes here\n#from __future__ import print_function\n\nimport librosa\n\nimport librosa\n\n\n#add default location here\nfilename = 'train/edmtest.mp3'\n\ny, s = librosa.load(filename)\n\ntemp, frames = librosa.beat.beat_track(y=y, sr=s)\n\nprint('Estimated tempo: {:.2f} beats per minute'.format(temp))\n\nbpm = librosa.frames_to_time(frames, sr=s)\n\nprint('Saving output to bpm.csv')\n#librosa.output.times_csv('bpm.csv', bpm)"""
music/detect.py,0,"b'# !/usr/bin/python\nimport sys\nimport numpy\nimport scipy.spatial as scip\nimport time\nimport random\n\nNUMBER_OF_TRAINING_INSTANCES = 5\n\n# Compute DTW Distance\ndef dtwdis(temp, inp):\n    # Use eclidean / mahalanobis distance\n    dmat = scip.distance.cdist(temp, inp, \'euclidean\')\n    m, n = numpy.shape(dmat)\n\n    dcost = numpy.ones((m + 2, n + 1))\n    dcost = dcost + numpy.inf\n\n    # Computing Dynamic Time Warping Table. Not doing pruning for the demo.\n    dcost[2, 1] = dmat[0, 0]\n    k = 3\n    for j in range(2, n + 1):\n        for i in range(2, min(2 + k, m + 2)):\n            dcost[i, j] = min(dcost[i, j - 1], dcost[i - 1, j - 1], dcost[i - 2, j - 1]) + dmat[i - 2, j - 1]\n        k = k + 2\n    return (dcost[m + 1, n])\n\ninp = \'test.mfcc\'\n\ndef main():\n    st = time.clock()\n    inpdat = numpy.loadtxt(inp)\n    cost = []\n    trl = range(1, NUMBER_OF_TRAINING_INSTANCES)\n    data = [""rap"",""rock"",""hiphop"",""metal"",""rnb"",""opera""]\n    for i in range(5):\n        cos = []\n        for m in trl:\n            temdat = numpy.loadtxt(\'train/\' + data[i] + \'m/\' + str(m) + \'.mfcc\')\n            fcost = dtwdis(temdat, inpdat)\n            print(\'{0}_{1}  distance from input : --->  {2}\'.format(data[i], m, fcost))\n            cos[len(cos):] = [fcost]\n        print()\n        cost[len(cost):] = [sum(cos)//(NUMBER_OF_TRAINING_INSTANCES-1)]\n    print(cost)\n    # print \'\\n\'\n    animal_index = cost.index(min(cost))\n    print(\'genre recognised as {0}\'.format(data[animal_index]))\n    et = time.clock()\n    print(\'Time Taken {0} seconds\'.format(et - st))\n\n    # Deleting -r mode for demo\nif __name__ == \'__main__\':\n    main()'"
music/extract_features.py,6,"b'# exracting faetures\n\n""""""\n\nRead doc - https://librosa.github.io/librosa/install.html#pypi\n\nTerminlogy :\n    sr = sampling rate ( default : 22050 hz)\n    frame = short audio clip / snippet\n    n_fft = No of samples per audio frame ( 2048 )\n    hop_lenght : samples between frames ( 512 )\n\nuse - Freesounds.org -> .wav\n\n""""""\nimport  sklearn\nimport timeit\nimport  matplotlib\nimport numpy as np\nimport librosa.display\n\n# Load from dir - .wav file : , init example\n# dir = train/cats .mp3 train/dogs .mp3\n\ndata = [""rap"",""rock"",""hiphop"",""metal"",""rnb"",\'opera\']\n\n#version count\nstart= 1\nstop = 5\n\nst = timeit.default_timer()\nfor name in data:\n    for v in range(start,stop):\n        #dir her\n        if name==\'hiphop\' or name == \'rnb\' or name==\'metal\':\n            y, sr = librosa.load(\'train/\' + name + \'/\' + name + str(v) + \'.m4a\')\n        else:\n            y,sr = librosa.load(\'train/\'+name+\'/\'+name+str(v)+\'.mp3\')\n        hop = 512\n        harmonic, percussive = librosa.effects.hpss(y)\n\n        tempo, beat_frames = librosa.beat.beat_track(y=y,\n                                                     sr=sr)\n        mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop, n_mfcc=13)\n\n        mfcc_delta = librosa.feature.delta(mfcc)\n\n        beat_mfcc_delta = librosa.display.util.sync(np.vstack([mfcc, mfcc_delta]), beat_frames)\n\n        chromagram = librosa.feature.chroma_cqt(y=harmonic,\n                                                sr=sr)\n        beat_chroma = librosa.display.util.sync(chromagram,\n                                                beat_frames,\n                                                aggregate=np.median)\n        beat_features = np.vstack([beat_chroma, beat_mfcc_delta])\n\n        transpose = beat_features.transpose()\n\n        # output dir -  mono/animal-name\n        # saving as .mfcc files\n        # saving numpy.nd in output.mfcc\n        # remember to extract only [1st , middle , last]\n        # now i have rows = 38 , column = dependencies on duration\n        #print(transpose)\n        output =  ""train/""+name+\'m/\'+ str(v)+\'.mfcc\'\n        data=np.vstack([transpose[4],transpose[len(transpose)//2],transpose[len(transpose)-4]])\n        data = np.transpose(data)\n        np.savetxt(output, data)\n    print(""Done with ""+ name )\n\nsp=timeit.default_timer()\nprint(""time taken = "",(sp-st))'"
music/test.py,6,"b'\nimport  sklearn\nimport timeit\nimport  matplotlib\nimport numpy as np\nimport librosa.display\n\n# dir here\ngen = input(""Enter genre to test (rap,rock,metal,hiphop,rnb,opera) : "")\nif gen in [""hiphop"",""metal"",""rnb"",""opera""]:\n    y, sr = librosa.load(""train/test""+gen+\'.m4a\')\nelse :\n    y, sr = librosa.load(""train/test"" + gen + \'.mp3\')\nhop = 512\nharmonic, percussive = librosa.effects.hpss(y)\n\ntempo, beat_frames = librosa.beat.beat_track(y=y,\n                                             sr=sr)\nmfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop, n_mfcc=13)\n\nmfcc_delta = librosa.feature.delta(mfcc)\n\nbeat_mfcc_delta = librosa.display.util.sync(np.vstack([mfcc, mfcc_delta]), beat_frames)\n\nchromagram = librosa.feature.chroma_cqt(y=harmonic,\n                                        sr=sr)\nbeat_chroma = librosa.display.util.sync(chromagram,\n                                        beat_frames,\n                                        aggregate=np.median)\nbeat_features = np.vstack([beat_chroma, beat_mfcc_delta])\n\ntranspose = beat_features.transpose()\n\n# output dir -  mono/animal-name\n# saving as .mfcc files\n# saving numpy.nd in output.mfcc\n# remember to extract only [1st , middle , last]\n# now i have rows = 38 , column = dependencies on duration\n# print(transpose)\noutput =  \'test.mfcc\'\ndata = np.vstack([transpose[5], transpose[len(transpose) // 2], transpose[len(transpose) -2]])\ndata = np.transpose(data)\nnp.savetxt(output, data)\n\nprint(""done creating test file "")'"
voice/beat.py,0,"b""# beats per min goes here\n#from __future__ import print_function\n\nimport librosa\n\nimport librosa\n\n\n#add default location here\nfilename = librosa.util.example_audio_file()\n\ny, s = librosa.load(filename)\n\ntemp, frames = librosa.beat.beat_track(y=y, sr=s)\n\nprint('Estimated tempo: {:.2f} beats per minute'.format(temp))\n\nbpm = librosa.frames_to_time(frames, sr=s)\n\nprint('Saving output to bpm.csv')\nlibrosa.output.times_csv('bpm.csv', bpm)\n"""
voice/detect.py,0,"b'\n\'\'\'\nadd matplotlib to plot detected accuracy\nNote : No learning done .\n\'\'\'\n# !/usr/bin/python\n\nimport sys\nimport os\nimport numpy\nimport scipy.spatial as scip\nimport time\nimport random\n\nNUMBER_OF_TRAINING_INSTANCES = 10\n\n\n# Compute DTW Distance\ndef dtwdis(temp, inp):\n    # Use eclidean / mahalanobis distance\n    dmat = scip.distance.cdist(temp, inp, \'euclidean\')\n    m, n = numpy.shape(dmat)\n\n    dcost = numpy.ones((m + 2, n + 1))\n    dcost = dcost + numpy.inf\n\n    # Computing Dynamic Time Warping Table. Not doing pruning for the demo.\n    dcost[2, 1] = dmat[0, 0]\n    k = 3\n    for j in range(2, n + 1):\n        for i in range(2, min(2 + k, m + 2)):\n            dcost[i, j] = min(dcost[i, j - 1], dcost[i - 1, j - 1], dcost[i - 2, j - 1]) + dmat[i - 2, j - 1]\n        k = k + 2\n    return (dcost[m + 1, n])\n\n\ninp = \'test.mfcc\'\n\ndef main():\n    st = time.clock()\n    inpdat = numpy.loadtxt(inp)\n    cost = []\n    trl = range(0, NUMBER_OF_TRAINING_INSTANCES)\n    array = [\'divya\',\'nishchith\']\n    for i in range(0, 2):\n        cos = []\n        for m in trl:\n            l=[]\n            for k in range(0,5):\n                temdat = numpy.loadtxt(\'train/\' + array[i] + \'_m/\' + str(m)+\'_\'+str(k) + \'.mfcc\')\n                fcost = dtwdis(temdat, inpdat)\n                l.append(fcost)\n            print(\'{0} version {1}  distance from input : --->  {2}\'.format(array[i], m, min(l)))\n            cos.append(min(l))\n        print()\n        cost[len(cost):] = [min(cos)]\n    print(cost)\n    if abs(cost[0]-cost[1])<=10:\n        print(""Please be clear "")\n        # print \'\\n\'\n    else :\n        index = cost.index(min(cost))\n        name = ""male""\n        if index ==0 :\n            name = ""female""\n        print(\'gender recognised as {0}\'.format(name))\n        et = time.clock()\n        os.system(\'say gender detected as \'+name)\n        print(\'{0} seconds\'.format(et - st))\n\nif __name__ == \'__main__\':\n    main()'"
voice/extract_features.py,6,"b'# exracting faetures\n\n""""""\n\nRead doc - https://librosa.github.io/librosa/install.html#pypi\n\nTerminlogy :\n    sr = sampling rate ( default : 22050 hz)\n    frame = short audio clip / snippet\n    n_fft = No of samples per audio frame ( 2048 )\n    hop_lenght : samples between frames ( 512 )\n\nuse - Freesounds.org -> .wav\n\n""""""\nimport  sklearn\nimport  matplotlib\nimport numpy as np\nimport librosa.display\n\n# Load from dir - .wav file : , init example\n\ny, sr = librosa.load(\'test.wav\')\n\nhop = 512\n\n# components\nharmonic,percussive = librosa.effects.hpss(y)\n\ntempo, beat_frames = librosa.beat.beat_track(y=y,\n                                             sr=sr)\n\n# Compute mfcc features from the raw signals from components\nmfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop, n_mfcc=13)\n\n# first-order differences (delta features)\nmfcc_delta = librosa.feature.delta(mfcc)\n\n# Stack and synchronize between beat events\nbeat_mfcc_delta = librosa.display.util.sync(np.vstack([mfcc, mfcc_delta]),beat_frames)\n\n# Compute chroma features from the harmonic signal\nchromagram = librosa.feature.chroma_cqt(y=harmonic,\n                                        sr=sr)\n\n# Aggregate chroma features between beat events . use median value of each feature between beat frames\nbeat_chroma = librosa.display.util.sync(chromagram,\n                                        beat_frames,\n                                        aggregate=np.median)\n\n# stack all beat-synchronous features together\nbeat_features = np.vstack([beat_chroma, beat_mfcc_delta])\n\n# saving as .mfcc files\n\n#saving numpy.nd in output.mfcc\n\n\n# remember to extract only [1st , middle , last]\n# now i have rows= 38 , column = dependencies on duration\n\ntranspose = beat_features.transpose()\n\noutput = \'test.mfcc\'\ndata = np.vstack([transpose[1], transpose[len(transpose) // 2], transpose[len(transpose) - 2]])\ndata = np.transpose(data)\nnp.savetxt(output, data)'"
voice/record.py,0,"b'# record here\n# automate this to record test data\n\nimport os\nimport pyaudio\nimport wave\nimport array\nimport numpy\nimport time\n\nCHUNK = 1225\nFORMAT = pyaudio.paInt16\nCHANNELS = 2\nRATE = 44100\nRECORD_SECONDS = 40\nWAVE_OUTPUT_FILENAME = \'test.wav\'\n\nthreshold = 200\n\np = pyaudio.PyAudio()\n\nprint(""say some number"")\ntime.sleep(3)\nstream = p.open(format=FORMAT,\n                channels=CHANNELS,\n                rate=RATE,\n                input=True,\n                frames_per_buffer=CHUNK)\n\n\n\n\nprint(""* recording"")\nframes = []\n\nnum_sil_frames = 0\n\n# The device will record for 20 seconds until there has been 0.5\n# second of audio (18 frames) where the energy is less than the threshold\n\nfor _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n    data = stream.read(CHUNK)\n    frames.append(data)\n    nums = array.array(\'h\', data)\n    left = numpy.array(nums[1::2])\n    energy = 10 * numpy.log(numpy.sum(numpy.power(left, 2)))\n    # print(energy)\n\n    if (energy < threshold):\n        num_sil_frames = num_sil_frames + 1\n    else:\n        num_sil_frames = 0\n\n    if num_sil_frames >= 37:\n        break\n\nos.system(\'say Recording done\')\nprint(""* done recording"")\n\nstream.stop_stream()\nstream.close()\np.terminate()\n\nwf = wave.open(WAVE_OUTPUT_FILENAME, \'wb\')\nwf.setnchannels(CHANNELS)\nwf.setsampwidth(p.get_sample_size(FORMAT))\nwf.setframerate(RATE)\nwf.writeframes(b\'\'.join(frames))\nwf.close()\ntime.sleep(3)\n'"
voice/test.py,6,"b'# exracting faetures\n\n""""""\nversion:\nRead doc - https://librosa.github.io/librosa/install.html#pypi\n\nTerminlogy :\n    sr = sampling rate ( default : 22050 hz)\n    frame = short audio clip / snippet\n    n_fft = No of samples per audio frame ( 2048 )\n    hop_lenght : samples between frames ( 512 )\n\nuse - Freesounds.org -> .wav\nto-add matplotlib\n\n""""""\n\nimport  sklearn\nimport timeit\nimport  matplotlib\nimport numpy as np\nimport librosa.display\n\n# Load from dir - .wav file : , init example\n# dir = train/cats .mp3 train/dogs .mp3\n\n#version count\nstart= 0\nstop = 10\ndata=[\'divya\',\'nishchith\']\n\nst = timeit.default_timer()\nfor name in data:\n    for v in range(start,stop):\n        for j in range(5):\n            # dir here\n            y, sr = librosa.load(\'train/\' + name + \'/\' + str(v)+\'_\'+str(j) + \'.wav\')\n            hop = 512\n            harmonic, percussive = librosa.effects.hpss(y)\n\n            tempo, beat_frames = librosa.beat.beat_track(y=y,\n                                                         sr=sr)\n            mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop, n_mfcc=13)\n\n            mfcc_delta = librosa.feature.delta(mfcc)\n\n            beat_mfcc_delta = librosa.display.util.sync(np.vstack([mfcc, mfcc_delta]), beat_frames)\n\n            chromagram = librosa.feature.chroma_cqt(y=harmonic,\n                                                    sr=sr)\n            beat_chroma = librosa.display.util.sync(chromagram,\n                                                    beat_frames,\n                                                    aggregate=np.median)\n            beat_features = np.vstack([beat_chroma, beat_mfcc_delta])\n\n            transpose = beat_features.transpose()\n\n            # output dir -  mono/animal-name\n            # saving as .mfcc files\n            # saving numpy.nd in output.mfcc\n            # remember to extract only [1st , middle , last]\n            # now i have rows = 38 , column = dependencies on duration\n            # print(transpose)\n            output = ""train/"" + name + \'_m/\' + str(v)+\'_\'+str(j) + \'.mfcc\'\n            data = np.vstack([transpose[1], transpose[len(transpose) // 2], transpose[len(transpose) - 2]])\n            data = np.transpose(data)\n            np.savetxt(output, data)\n    print(""done with ""+name)\n\nsp=timeit.default_timer()\nprint(""time taken = "",(sp-st))'"
voice/versions.py,0,"b'# record here\n# automate this to record test data\n\nimport os\nimport pyaudio\nimport wave\nimport array\nimport numpy\nimport time\n\nCHUNK = 1225\nFORMAT = pyaudio.paInt16\nCHANNELS = 2\nRATE = 44100\nRECORD_SECONDS = 40\n\n\nthreshold = 200\n\ndata=[\'divya\',\'prafful\',\'ahaan\',\'nishchith\',\'pravar\']\nversions = 10\n#no of versions = 5\n\ninit = data[3]\n#enter dirs\nfor i in range(versions):\n    for j in range(5):\n        os.system(\'say Say\'+ str(i))\n        p = pyaudio.PyAudio()\n        WAVE_OUTPUT_FILENAME = \'train/\'+init+\'/\'+ str(i)+\'_\'+ str(j)+ \'.wav\'\n        time.sleep(0.2)\n        stream = p.open(format=FORMAT,\n                        channels=CHANNELS,\n                        rate=RATE,\n                        input=True,\n                        frames_per_buffer=CHUNK)\n\n        print(""* recording"")\n        frames = []\n\n        num_sil_frames = 0\n\n        # The device will record for 20 seconds until there has been 0.5\n        # second of audio (18 frames) where the energy is less than the threshold\n\n        for _ in range(0, int(RATE // CHUNK * RECORD_SECONDS)):\n            data = stream.read(CHUNK)\n            frames.append(data)\n            nums = array.array(\'h\', data)\n            left = numpy.array(nums[1::2])\n            energy = 10 * numpy.log(numpy.sum(numpy.power(left, 2)))\n            # print(energy)\n\n            if (energy < threshold):\n                num_sil_frames = num_sil_frames + 1\n            else:\n                num_sil_frames = 0\n\n            if num_sil_frames >= 37:\n                break\n\n        os.system(\'say Recording done\')\n        print(""* done recording"")\n\n        stream.stop_stream()\n        stream.close()\n        p.terminate()\n\n        wf = wave.open(WAVE_OUTPUT_FILENAME, \'wb\')\n        wf.setnchannels(CHANNELS)\n        wf.setsampwidth(p.get_sample_size(FORMAT))\n        wf.setframerate(RATE)\n        wf.writeframes(b\'\'.join(frames))\n        wf.close()\n        time.sleep(3)'"
