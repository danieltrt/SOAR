file_path,api_count,code
Week 1/customplot.py,1,"b""#\n# First, let us create some utility functions for Plotting\n#\n\n\ndef pd_centers(featuresUsed, centers):\n\tfrom itertools import cycle, islice\n\tfrom pandas.tools.plotting import parallel_coordinates\n\timport matplotlib.pyplot as plt\n\timport pandas as pd\n\timport numpy as np\n\n\tcolNames = list(featuresUsed)\n\tcolNames.append('prediction')\n\n\t# Zip with a column called 'prediction' (index)\n\tZ = [np.append(A, index) for index, A in enumerate(centers)]\n\n\t# Convert to pandas for plotting\n\tP = pd.DataFrame(Z, columns=colNames)\n\tP['prediction'] = P['prediction'].astype(int)\n\treturn P\n\ndef parallel_plot(data):\n\tfrom itertools import cycle, islice\n\tfrom pandas.tools.plotting import parallel_coordinates\n\timport matplotlib.pyplot as plt\n\n\tmy_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k']), None, len(data)))\n\tplt.figure(figsize=(15,8)).gca().axes.set_ylim([-2.5,+2.5])\n\tparallel_coordinates(data, 'prediction', color = my_colors, marker='o')"""
Python for Data Science/Python Word Count/word_cloud.py,0,"b'# Python 3\n\n# Be sure you have followed the instructions to download the 98-0.txt,\n# the text of A Tale of Two Cities, by Charles Dickens\n\nimport collections\n\nfile=open(\'98-0.txt\')\n\n# if you want to use stopwords, here\'s an example of how to do this\n# stopwords = set(line.strip() for line in open(\'stopwords\'))\n\n# create your data structure here.  F\nwordcount={}\n\n# Instantiate a dictionary, and for every word in the file, add to \n# the dictionary if it doesn\'t exist. If it does, increase the count.\n\n# Hint: To eliminate duplicates, remember to split by punctuation, \n# and use case demiliters. The functions lower() and split() will be useful!\n\nfor word in file.read().lower().split():\n    word = word.replace(""."","""")\n    word = word.replace("","","""")\n    word = word.replace(""\\"""","""")\n    word = word.replace(""\xe2\x80\x9c"","""")\n    if word not in stopwords:\n        if word not in wordcount:\n            wordcount[word] = 1\n        else:\n            wordcount[word] += 1\n\n# after building your wordcount, you can then sort it and return the first\n# n words.  If you want, collections.Counter may be useful.\n\nd = collections.Counter(wordcount)\n\n#print(d.most_common(10))\nfor word, count in d.most_common(10):\n\tprint(word, "": "", count)\n'"
Python for Data Science/Soocer Data Analysis  - kaggle/customplot.py,1,"b""#\n# First, let us create some utility functions for Plotting\n#\n\n\ndef pd_centers(featuresUsed, centers):\n\tfrom itertools import cycle, islice\n\tfrom pandas.tools.plotting import parallel_coordinates\n\timport matplotlib.pyplot as plt\n\timport pandas as pd\n\timport numpy as np\n\n\tcolNames = list(featuresUsed)\n\tcolNames.append('prediction')\n\n\t# Zip with a column called 'prediction' (index)\n\tZ = [np.append(A, index) for index, A in enumerate(centers)]\n\n\t# Convert to pandas for plotting\n\tP = pd.DataFrame(Z, columns=colNames)\n\tP['prediction'] = P['prediction'].astype(int)\n\treturn P\n\ndef parallel_plot(data):\n\tfrom itertools import cycle, islice\n\tfrom pandas.tools.plotting import parallel_coordinates\n\timport matplotlib.pyplot as plt\n\n\tmy_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k']), None, len(data)))\n\tplt.figure(figsize=(15,8)).gca().axes.set_ylim([-2.5,+2.5])\n\tparallel_coordinates(data, 'prediction', color = my_colors, marker='o')"""
Week 2/Python Word Count/word_cloud.py,0,"b'# Python 3\n\n# Be sure you have followed the instructions to download the 98-0.txt,\n# the text of A Tale of Two Cities, by Charles Dickens\n\nimport collections\n\nfile=open(\'98-0.txt\')\n\n# if you want to use stopwords, here\'s an example of how to do this\n# stopwords = set(line.strip() for line in open(\'stopwords\'))\n\n# create your data structure here.  F\nwordcount={}\n\n# Instantiate a dictionary, and for every word in the file, add to \n# the dictionary if it doesn\'t exist. If it does, increase the count.\n\n# Hint: To eliminate duplicates, remember to split by punctuation, \n# and use case demiliters. The functions lower() and split() will be useful!\n\nfor word in file.read().lower().split():\n    word = word.replace(""."","""")\n    word = word.replace("","","""")\n    word = word.replace(""\\"""","""")\n    word = word.replace(""\xe2\x80\x9c"","""")\n    if word not in stopwords:\n        if word not in wordcount:\n            wordcount[word] = 1\n        else:\n            wordcount[word] += 1\n\n# after building your wordcount, you can then sort it and return the first\n# n words.  If you want, collections.Counter may be useful.\n\nd = collections.Counter(wordcount)\n\n#print(d.most_common(10))\nfor word, count in d.most_common(10):\n\tprint(word, "": "", count)\n'"
