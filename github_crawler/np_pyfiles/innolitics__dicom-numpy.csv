file_path,api_count,code
setup.py,0,"b'""""""\nA setuptools based setup module.\n""""""\n\nfrom setuptools import setup, find_packages\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\nwith open(path.join(here, \'README.rst\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nsetup(\n    name=\'dicom_numpy\',\n    version=\'0.3.0\',\n    description=\'Extract image data into a 3D numpy array from a set of DICOM files.\',\n    long_description=long_description,\n    url=\'https://github.com/innolitics/dicom-numpy\',\n    author=\'Innolitics, LLC\',\n    author_email=\'info@innolitics.com\',\n    license=\'MIT\',\n    classifiers=[\n        \'Development Status :: 5 - Production/Stable\',\n\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Healthcare Industry\',\n        \'Topic :: Software Development :: Build Tools\',\n        \'Topic :: Scientific/Engineering :: Medical Science Apps.\',\n\n        \'License :: OSI Approved :: MIT License\',\n\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n    ],\n\n    keywords=\'dicom numpy\',\n\n    packages=find_packages(exclude=[\'contrib\', \'docs\', \'tests\']),\n\n    install_requires=[\n        \'pydicom\',\n        \'numpy\',\n    ],\n\n    extras_require={\n        \'dev\': [\'check-manifest\', \'sphinx\', \'sphinx-autobuild\', \'mock\'],\n        \'test\': [\'coverage\', \'pytest\'],\n    },\n\n    package_data={},\n    data_files=[],\n    entry_points={},\n)\n'"
dicom_numpy/__init__.py,0,"b""from .combine_slices import combine_slices\nfrom .exceptions import DicomImportException\n\n__all__ = [\n    'combine_slices',\n    'DicomImportException',\n]\n"""
dicom_numpy/combine_slices.py,20,"b'import logging\n\nimport numpy as np\n\nfrom .utils import isclose\nfrom .exceptions import DicomImportException\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef combine_slices(slice_datasets, rescale=None):\n    """"""\n    Given a list of pydicom datasets for an image series, stitch them together into a\n    three-dimensional numpy array.  Also calculate a 4x4 affine transformation\n    matrix that converts the ijk-pixel-indices into the xyz-coordinates in the\n    DICOM patient\'s coordinate system.\n\n    Returns a two-tuple containing the 3D-ndarray and the affine matrix.\n\n    If `rescale` is set to `None` (the default), then the image array dtype\n    will be preserved, unless any of the DICOM images contain either the\n    `Rescale Slope\n    <https://dicom.innolitics.com/ciods/ct-image/ct-image/00281053>`_ or the\n    `Rescale Intercept <https://dicom.innolitics.com/ciods/ct-image/ct-image/00281052>`_\n    attributes.  If either of these attributes are present, they will be\n    applied to each slice individually.\n\n    If `rescale` is `True` the voxels will be cast to `float32`, if set to\n    `False`, the original dtype will be preserved even if DICOM rescaling information is present.\n\n    The returned array has the column-major byte-order.\n\n    This function requires that the datasets:\n\n    - Be in same series (have the same\n      `Series Instance UID <https://dicom.innolitics.com/ciods/ct-image/general-series/0020000e>`_,\n      `Modality <https://dicom.innolitics.com/ciods/ct-image/general-series/00080060>`_,\n      and `SOP Class UID <https://dicom.innolitics.com/ciods/ct-image/sop-common/00080016>`_).\n    - The binary storage of each slice must be the same (have the same\n      `Bits Allocated <https://dicom.innolitics.com/ciods/ct-image/image-pixel/00280100>`_,\n      `Bits Stored <https://dicom.innolitics.com/ciods/ct-image/image-pixel/00280101>`_,\n      `High Bit <https://dicom.innolitics.com/ciods/ct-image/image-pixel/00280102>`_, and\n      `Pixel Representation <https://dicom.innolitics.com/ciods/ct-image/image-pixel/00280103>`_).\n    - The image slice must approximately form a grid. This means there can not\n      be any missing internal slices (missing slices on the ends of the dataset\n      are not detected).\n    - It also means that  each slice must have the same\n      `Rows <https://dicom.innolitics.com/ciods/ct-image/image-pixel/00280010>`_,\n      `Columns <https://dicom.innolitics.com/ciods/ct-image/image-pixel/00280011>`_,\n      `Pixel Spacing <https://dicom.innolitics.com/ciods/ct-image/image-plane/00280030>`_, and\n      `Image Orientation (Patient) <https://dicom.innolitics.com/ciods/ct-image/image-plane/00200037>`_\n      attribute values.\n    - The direction cosines derived from the\n      `Image Orientation (Patient) <https://dicom.innolitics.com/ciods/ct-image/image-plane/00200037>`_\n      attribute must, within 1e-4, have a magnitude of 1.  The cosines must\n      also be approximately perpendicular (their dot-product must be within\n      1e-4 of 0).  Warnings are displayed if any of these approximations are\n      below 1e-8, however, since we have seen real datasets with values up to\n      1e-4, we let them pass.\n    - The `Image Position (Patient) <https://dicom.innolitics.com/ciods/ct-image/image-plane/00200032>`_\n      values must approximately form a line.\n\n    If any of these conditions are not met, a `dicom_numpy.DicomImportException` is raised.\n    """"""\n    if len(slice_datasets) == 0:\n        raise DicomImportException(""Must provide at least one DICOM dataset"")\n\n    _validate_slices_form_uniform_grid(slice_datasets)\n\n    voxels = _merge_slice_pixel_arrays(slice_datasets, rescale)\n    transform = _ijk_to_patient_xyz_transform_matrix(slice_datasets)\n\n    return voxels, transform\n\n\ndef _merge_slice_pixel_arrays(slice_datasets, rescale=None):\n    sorted_slice_datasets = _sort_by_slice_position(slice_datasets)\n\n    if rescale is None:\n        rescale = any(_requires_rescaling(d) for d in sorted_slice_datasets)\n\n    first_dataset = sorted_slice_datasets[0]\n    slice_dtype = first_dataset.pixel_array.dtype\n    slice_shape = first_dataset.pixel_array.T.shape\n    num_slices = len(sorted_slice_datasets)\n\n    voxels_shape = slice_shape + (num_slices,)\n    voxels_dtype = np.float32 if rescale else slice_dtype\n    voxels = np.empty(voxels_shape, dtype=voxels_dtype, order=\'F\')\n\n    for k, dataset in enumerate(sorted_slice_datasets):\n        pixel_array = dataset.pixel_array.T\n        if rescale:\n            slope = float(getattr(dataset, \'RescaleSlope\', 1))\n            intercept = float(getattr(dataset, \'RescaleIntercept\', 0))\n            pixel_array = pixel_array.astype(np.float32) * slope + intercept\n        voxels[..., k] = pixel_array\n\n    return voxels\n\n\ndef _requires_rescaling(dataset):\n    return hasattr(dataset, \'RescaleSlope\') or hasattr(dataset, \'RescaleIntercept\')\n\n\ndef _ijk_to_patient_xyz_transform_matrix(slice_datasets):\n    first_dataset = _sort_by_slice_position(slice_datasets)[0]\n    image_orientation = first_dataset.ImageOrientationPatient\n    row_cosine, column_cosine, slice_cosine = _extract_cosines(image_orientation)\n\n    row_spacing, column_spacing = first_dataset.PixelSpacing\n    slice_spacing = _slice_spacing(slice_datasets)\n\n    transform = np.identity(4, dtype=np.float32)\n\n    transform[:3, 0] = row_cosine * column_spacing\n    transform[:3, 1] = column_cosine * row_spacing\n    transform[:3, 2] = slice_cosine * slice_spacing\n\n    transform[:3, 3] = first_dataset.ImagePositionPatient\n\n    return transform\n\n\ndef _validate_slices_form_uniform_grid(slice_datasets):\n    """"""\n    Perform various data checks to ensure that the list of slices form a\n    evenly-spaced grid of data.\n    Some of these checks are probably not required if the data follows the\n    DICOM specification, however it seems pertinent to check anyway.\n    """"""\n    invariant_properties = [\n        \'Modality\',\n        \'SOPClassUID\',\n        \'SeriesInstanceUID\',\n        \'Rows\',\n        \'Columns\',\n        \'SamplesPerPixel\',\n        \'PixelSpacing\',\n        \'PixelRepresentation\',\n        \'BitsAllocated\',\n    ]\n\n    for property_name in invariant_properties:\n        _slice_attribute_equal(slice_datasets, property_name)\n\n    _validate_image_orientation(slice_datasets[0].ImageOrientationPatient)\n    _slice_ndarray_attribute_almost_equal(slice_datasets, \'ImageOrientationPatient\', 1e-5)\n\n    slice_positions = _slice_positions(slice_datasets)\n    _check_for_missing_slices(slice_positions)\n\n\ndef _validate_image_orientation(image_orientation):\n    """"""\n    Ensure that the image orientation is supported\n    - The direction cosines have magnitudes of 1 (just in case)\n    - The direction cosines are perpendicular\n    """"""\n    row_cosine, column_cosine, slice_cosine = _extract_cosines(image_orientation)\n\n    if not _almost_zero(np.dot(row_cosine, column_cosine), 1e-4):\n        raise DicomImportException(""Non-orthogonal direction cosines: {}, {}"".format(row_cosine, column_cosine))\n    elif not _almost_zero(np.dot(row_cosine, column_cosine), 1e-8):\n        logger.warning(""Direction cosines aren\'t quite orthogonal: {}, {}"".format(row_cosine, column_cosine))\n\n    if not _almost_one(np.linalg.norm(row_cosine), 1e-4):\n        raise DicomImportException(""The row direction cosine\'s magnitude is not 1: {}"".format(row_cosine))\n    elif not _almost_one(np.linalg.norm(row_cosine), 1e-8):\n        logger.warning(""The row direction cosine\'s magnitude is not quite 1: {}"".format(row_cosine))\n\n    if not _almost_one(np.linalg.norm(column_cosine), 1e-4):\n        raise DicomImportException(""The column direction cosine\'s magnitude is not 1: {}"".format(column_cosine))\n    elif not _almost_one(np.linalg.norm(column_cosine), 1e-8):\n        logger.warning(""The column direction cosine\'s magnitude is not quite 1: {}"".format(column_cosine))\n\n\ndef _almost_zero(value, abs_tol):\n    return isclose(value, 0.0, abs_tol=abs_tol)\n\n\ndef _almost_one(value, abs_tol):\n    return isclose(value, 1.0, abs_tol=abs_tol)\n\n\ndef _extract_cosines(image_orientation):\n    row_cosine = np.array(image_orientation[:3])\n    column_cosine = np.array(image_orientation[3:])\n    slice_cosine = np.cross(row_cosine, column_cosine)\n    return row_cosine, column_cosine, slice_cosine\n\n\ndef _slice_attribute_equal(slice_datasets, property_name):\n    initial_value = getattr(slice_datasets[0], property_name, None)\n    for dataset in slice_datasets[1:]:\n        value = getattr(dataset, property_name, None)\n        if value != initial_value:\n            msg = \'All slices must have the same value for ""{}"": {} != {}\'\n            raise DicomImportException(msg.format(property_name, value, initial_value))\n\n\ndef _slice_ndarray_attribute_almost_equal(slice_datasets, property_name, abs_tol):\n    initial_value = getattr(slice_datasets[0], property_name, None)\n    for dataset in slice_datasets[1:]:\n        value = getattr(dataset, property_name, None)\n        if not np.allclose(value, initial_value, atol=abs_tol):\n            msg = \'All slices must have the same value for ""{}"" within ""{}"": {} != {}\'\n            raise DicomImportException(msg.format(property_name, abs_tol, value, initial_value))\n\n\ndef _slice_positions(slice_datasets):\n    image_orientation = slice_datasets[0].ImageOrientationPatient\n    row_cosine, column_cosine, slice_cosine = _extract_cosines(image_orientation)\n    return [np.dot(slice_cosine, d.ImagePositionPatient) for d in slice_datasets]\n\n\ndef _check_for_missing_slices(slice_positions):\n    if len(slice_positions) > 1:\n        slice_positions_diffs = np.diff(sorted(slice_positions))\n        if not np.allclose(slice_positions_diffs, slice_positions_diffs[0], atol=0, rtol=1e-5):\n            # TODO: figure out how we should handle non-even slice spacing\n            msg = ""The slice spacing is non-uniform. Slice spacings:\\n{}""\n            logger.warning(msg.format(slice_positions_diffs))\n\n        if not np.allclose(slice_positions_diffs, slice_positions_diffs[0], atol=0, rtol=1e-1):\n            raise DicomImportException(\'It appears there are missing slices\')\n\n\ndef _slice_spacing(slice_datasets):\n    if len(slice_datasets) > 1:\n        slice_positions = _slice_positions(slice_datasets)\n        slice_positions_diffs = np.diff(sorted(slice_positions))\n        return np.mean(slice_positions_diffs)\n\n    return getattr(slice_datasets[0], \'SpacingBetweenSlices\', 0)\n\n\ndef _sort_by_slice_position(slice_datasets):\n    slice_positions = _slice_positions(slice_datasets)\n    return [d for (s, d) in sorted(zip(slice_positions, slice_datasets))]\n'"
dicom_numpy/exceptions.py,0,b'class DicomImportException(Exception):\n    pass\n'
dicom_numpy/utils.py,0,"b'def isclose(a, b, rel_tol=1e-9, abs_tol=0.0):\n    """"""\n    This function is implemented in Python 3.\n\n    To support Python 2, we include our own implementation.\n    """"""\n    return abs(a-b) <= max(rel_tol*max(abs(a), abs(b)), abs_tol)\n'"
dicom_numpy/zip_archive.py,0,"b'import zipfile\nimport logging\nimport tempfile\n\ntry:\n    import pydicom as dicom\nexcept ImportError:\n    import dicom\n\nfrom .exceptions import DicomImportException\nfrom .combine_slices import combine_slices\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef combined_series_from_zip(zip_filename):\n    logger.info(\'Extracting voxel data from ""{}""\'.format(zip_filename))\n\n    if not zipfile.is_zipfile(zip_filename):\n        raise DicomImportException(\'Invalid zipfile {}\'.format(zip_filename))\n\n    with zipfile.ZipFile(zip_filename, \'r\') as zip_file:\n        datasets = dicom_datasets_from_zip(zip_file)\n\n    voxels, ijk_to_xyz = combine_slices(datasets)\n    return voxels, ijk_to_xyz\n\n\ndef dicom_datasets_from_zip(zip_file):\n    datasets = []\n    for entry in zip_file.namelist():\n        if entry.endswith(\'/\'):\n            continue  # skip directories\n\n        entry_pseudo_file = zip_file.open(entry)\n\n        # the pseudo file does not support `seek`, which is required by\n        # dicom\'s lazy loading mechanism; use temporary files to get around this;\n        # relies on the temporary files not being removed until the temp\n        # file is garbage collected, which should be the case because the\n        # dicom datasets should retain a reference to the temp file\n        temp_file = tempfile.TemporaryFile()\n        temp_file.write(entry_pseudo_file.read())\n        temp_file.flush()\n        temp_file.seek(0)\n\n        try:\n            dataset = dicom.read_file(temp_file)\n            datasets.append(dataset)\n        except dicom.errors.InvalidDicomError as e:\n            msg = \'Skipping invalid DICOM file ""{}"": {}\'\n            logger.info(msg.format(entry, e))\n\n    if len(datasets) == 0:\n        raise DicomImportException(\'Zipfile does not contain any valid DICOM files\')\n\n    return datasets\n'"
tests/__init__.py,0,b''
tests/conftest.py,1,"b'import pytest\nimport numpy as np\n\n\n# direction cosines\nx_cos = (1, 0, 0)\ny_cos = (0, 1, 0)\nz_cos = (0, 0, 1)\nnegative_x_cos = (-1, 0, 0)\nnegative_y_cos = (0, -1, 0)\nnegative_z_cos = (0, 0, -1)\n\narbitrary_shape = (10, 11)\narbitrary_rgb_shape = (10, 11, 3)\n\n\nclass MockSlice:\n    """"""\n    A minimal DICOM dataset representing a dataslice at a particular\n    slice location.  The `slice_position` is the coordinate value along the\n    remaining unused axis (i.e. the axis perpendicular to the direction\n    cosines).\n    """"""\n\n    def __init__(self, pixel_array, slice_position, row_cosine=None, column_cosine=None, **kwargs):\n        if row_cosine is None:\n            row_cosine = x_cos\n\n        if column_cosine is None:\n            column_cosine = y_cos\n\n        shape = pixel_array.shape\n        if len(shape) == 2:\n            num_columns, num_rows = shape\n            samples_per_pixel = 1\n        else:\n            num_columns, num_rows, samples_per_pixel = shape\n\n        self.pixel_array = pixel_array\n\n        self.SeriesInstanceUID = \'arbitrary uid\'\n        self.SOPClassUID = \'arbitrary sopclass uid\'\n        self.PixelSpacing = [1.0, 1.0]\n        self.Columns = num_columns\n        self.Rows = num_rows\n        self.SamplesPerPixel = samples_per_pixel\n        self.Modality = \'MR\'\n\n        # assume that the images are centered on the remaining unused axis\n        a_component = [-num_columns/2.0*c for c in row_cosine]\n        b_component = [-num_rows/2.0*c for c in column_cosine]\n        c_component = [(slice_position if c == 0 and cc == 0 else 0) for c, cc in zip(row_cosine, column_cosine)]\n        patient_position = [a + b + c for a, b, c in zip(a_component, b_component, c_component)]\n\n        self.ImagePositionPatient = patient_position\n\n        self.ImageOrientationPatient = list(row_cosine) + list(column_cosine)\n\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\n\n@pytest.fixture\ndef axial_slices():\n    return [\n        MockSlice(randi(*arbitrary_shape), 0),\n        MockSlice(randi(*arbitrary_shape), 1),\n        MockSlice(randi(*arbitrary_shape), 2),\n        MockSlice(randi(*arbitrary_shape), 3),\n    ]\n\n\n@pytest.fixture\ndef axial_rgb_slices():\n    return [\n        MockSlice(randi(*arbitrary_rgb_shape), 0),\n        MockSlice(randi(*arbitrary_rgb_shape), 1),\n        MockSlice(randi(*arbitrary_rgb_shape), 2),\n        MockSlice(randi(*arbitrary_rgb_shape), 3),\n    ]\n\n\ndef randi(*shape):\n    return np.random.randint(1000, size=shape, dtype=\'uint16\')\n'"
tests/create_golden_values.py,1,"b'""""""\nGenerate a golden NPZ file from a dicom ZIP archive.\n""""""\nimport argparse\n\nimport numpy as np\n\nfrom dicom_numpy.zip_archive import combined_series_from_zip\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-o\', \'--output\', help=\'Output golden NPZ file\', required=False)\n    parser.add_argument(\'input\', help=""Input DICOM zip archive"")\n    return parser.parse_args()\n\n\ndef generate_golden_values(input_zip, output_path=\'golden_values\'):\n    """"""\n    Generate a golden NPZ file for a given DICOM zip archive.\n    """"""\n    voxels, ijk_to_xyz = combined_series_from_zip(input_zip)\n    np.savez_compressed(output_path, voxels=voxels, ijk_to_xyz=ijk_to_xyz)\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    if args.output:\n        generate_golden_values(args.input, args.output)\n    else:\n        generate_golden_values(args.input)\n'"
tests/test_combine_from_zip.py,3,"b'import os\n\nimport numpy as np\n\nfrom dicom_numpy.zip_archive import combined_series_from_zip\n\nTEST_DIR = os.path.dirname(__file__)\nTEST_DICOM_ZIP_PATH = os.path.join(TEST_DIR, \'test_dicom.zip\')\nGOLDEN_FILE_PATH = os.path.join(TEST_DIR, \'golden_values.npz\')\n\n\ndef test_combine_from_zip():\n    """"""\n    An integration test checking that a known DICOM zip archive can be\n    processed and produces a known golden value.\n    """"""\n    voxels, ijk_to_xyz = combined_series_from_zip(TEST_DICOM_ZIP_PATH)\n    with np.load(GOLDEN_FILE_PATH) as dataset:\n        np.testing.assert_array_equal(voxels, dataset[\'voxels\'])\n        np.testing.assert_array_equal(ijk_to_xyz, dataset[\'ijk_to_xyz\'])\n'"
tests/test_combine_slices.py,19,"b'import numpy as np\nimport pytest\n\nfrom dicom_numpy.combine_slices import (\n    combine_slices,\n    _validate_slices_form_uniform_grid,\n    _merge_slice_pixel_arrays,\n)\nfrom dicom_numpy.exceptions import DicomImportException\nfrom .conftest import MockSlice\n\n\nclass TestCombineSlices:\n    def test_simple_axial_set(self, axial_slices):\n        combined, _ = combine_slices(axial_slices[0:2])\n\n        manually_combined = np.dstack((axial_slices[0].pixel_array.T, axial_slices[1].pixel_array.T))\n        assert np.array_equal(combined, manually_combined)\n\n    def test_single_slice(self, axial_slices):\n        dataset = axial_slices[-1]\n        array, _ = combine_slices([dataset])\n        assert np.array_equal(array, dataset.pixel_array.T[:, :, None])\n\n    def test_single_slice_spacing(self, axial_slices):\n        slice_spacing = 0.65\n        dataset = axial_slices[0]\n        dataset.SpacingBetweenSlices = slice_spacing\n        array, affine = combine_slices([dataset])\n        assert np.array_equal(array, dataset.pixel_array.T[:, :, None])\n        assert np.isclose(np.linalg.norm(affine[:, 2]), np.abs(slice_spacing))\n\n    def test_rgb_axial_set(self, axial_rgb_slices):\n        combined, _ = combine_slices(axial_rgb_slices)\n\n        manually_combined = np.stack([ds.pixel_array for ds in axial_rgb_slices], axis=0).T\n        assert np.array_equal(combined, manually_combined)\n\n\nclass TestMergeSlicePixelArrays:\n    def test_casting_if_only_rescale_slope(self):\n        """"""\n        If the `RescaleSlope` DICOM attribute is present, the\n        `RescaleIntercept` attribute should also be present, however, we handle\n        this case anyway.\n        """"""\n        slices = [\n            MockSlice(np.ones((10, 20), dtype=np.uint8), 0, RescaleSlope=2),\n            MockSlice(np.ones((10, 20), dtype=np.uint8), 1, RescaleSlope=2),\n        ]\n\n        voxels = _merge_slice_pixel_arrays(slices)\n        assert voxels.dtype == np.dtype(\'float32\')\n        assert voxels[0, 0, 0] == 2.0\n\n    def test_casting_rescale_slope_and_intercept(self):\n        """"""\n        Some DICOM modules contain the `RescaleSlope` and `RescaleIntercept` DICOM attributes.\n        """"""\n        slices = [\n            MockSlice(np.ones((10, 20), dtype=np.uint8), 0, RescaleSlope=2, RescaleIntercept=3),\n            MockSlice(np.ones((10, 20), dtype=np.uint8), 1, RescaleSlope=2, RescaleIntercept=3),\n        ]\n\n        voxels = _merge_slice_pixel_arrays(slices)\n        assert voxels.dtype == np.dtype(\'float32\')\n        assert voxels[0, 0, 0] == 5.0\n\n    def test_robust_to_ordering(self, axial_slices):\n        """"""\n        The DICOM slices should be able to be passed in in any order, and they\n        should be recombined appropriately.\n        """"""\n        assert np.array_equal(\n            _merge_slice_pixel_arrays([axial_slices[0], axial_slices[1], axial_slices[2]]),\n            _merge_slice_pixel_arrays([axial_slices[1], axial_slices[0], axial_slices[2]])\n        )\n\n        assert np.array_equal(\n            _merge_slice_pixel_arrays([axial_slices[0], axial_slices[1], axial_slices[2]]),\n            _merge_slice_pixel_arrays([axial_slices[2], axial_slices[0], axial_slices[1]])\n        )\n\n    def test_rescales_if_forced_true(self):\n        slice_datasets = [MockSlice(np.ones((10, 20), dtype=np.uint8), 0)]\n        voxels = _merge_slice_pixel_arrays(slice_datasets, rescale=True)\n        assert voxels.dtype == np.float32\n\n    def test_no_rescale_if_forced_false(self):\n        slice_datasets = [MockSlice(np.ones((10, 20), dtype=np.uint8), 0, RescaleSlope=2, RescaleIntercept=3)]\n        voxels = _merge_slice_pixel_arrays(slice_datasets, rescale=False)\n        assert voxels.dtype == np.uint8\n\n\nclass TestValidateSlicesFormUniformGrid:\n    def test_missing_middle_slice(self, axial_slices):\n        """"""\n        All slices must be present.  Slice position is determined using the\n        ImagePositionPatient (0020,0032) tag.\n        """"""\n        with pytest.raises(DicomImportException):\n            _validate_slices_form_uniform_grid([axial_slices[0], axial_slices[2], axial_slices[3]])\n\n    def test_insignificant_difference_in_direction_cosines(self, axial_slices):\n        """"""\n        We have seen DICOM series in the field where slices have lightly\n        different direction cosines.\n        """"""\n        axial_slices[0].ImageOrientationPatient[0] += 1e-6\n        _validate_slices_form_uniform_grid(axial_slices)\n\n    def test_significant_difference_in_direction_cosines(self, axial_slices):\n        axial_slices[0].ImageOrientationPatient[0] += 1e-4\n        with pytest.raises(DicomImportException):\n            _validate_slices_form_uniform_grid(axial_slices)\n\n    def test_slices_from_different_series(self, axial_slices):\n        """"""\n        As a sanity check, slices that don\'t come from the same DICOM series should\n        be rejected.\n        """"""\n        axial_slices[2].SeriesInstanceUID += \'Ooops\'\n        with pytest.raises(DicomImportException):\n            _validate_slices_form_uniform_grid(axial_slices)\n\n    @pytest.mark.xfail(reason=\'Not sure how to detect this in DICOM\')\n    def test_missing_end_slice(self, axial_slices):\n        """"""\n        Ideally, we would detect missing edge slices, however given that we don\'t\n        know any way to determine the number of slices are in a DICOM series, this\n        seems impossible.\n        """"""\n        with pytest.raises(DicomImportException):\n            _validate_slices_form_uniform_grid([axial_slices[0], axial_slices[1], axial_slices[2]])\n'"
docs/source/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# DICOM-Numpy documentation build configuration file, created by\n# sphinx-quickstart on Fri Apr 28 10:43:23 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\ntry:\n    from unittest.mock import MagicMock\nexcept ImportError:\n    from mock import Mock as MagicMock\n\nsys.path.insert(0, os.path.abspath(\'../..\'))\n\n\nclass Mock(MagicMock):\n    @classmethod\n    def __getattr__(cls, name):\n        return MagicMock()\n\n\nMOCK_MODULES = [\'numpy\', \'dicom\']\nsys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'DICOM-Numpy\'\ncopyright = \'2017, Innolitics\'\nauthor = \'J. David Giese\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'0.1.1\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'0.1.1\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'DICOM-Numpydoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'DICOM-Numpy.tex\', \'DICOM-Numpy Documentation\',\n     \'J. David Giese\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'dicom-numpy\', \'DICOM-Numpy Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'DICOM-Numpy\', \'DICOM-Numpy Documentation\',\n     author, \'DICOM-Numpy\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n'"
