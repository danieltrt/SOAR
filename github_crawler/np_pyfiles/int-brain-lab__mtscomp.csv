file_path,api_count,code
benchmark.py,2,"b'from itertools import product, repeat\nfrom pathlib import Path\nimport time\n\nfrom tabulate import tabulate\nimport numpy as np\nfrom joblib import Memory\nfrom tqdm import tqdm\n\nfrom mtscomp import compress, decompress, load_raw_data\n\n\ndtype = np.uint16\n\n\ndef mtscomp_perf(**kwargs):\n    ds = kwargs.pop(\'ds\', None)\n    assert ds\n\n    name, n_channels, sample_rate, duration = ds\n\n    # Compress the file.\n    path = Path(\'data/\' + name)\n    out = path.parent / \'data.cbin\'\n    outmeta = path.parent / \'data.ch\'\n    t0 = time.perf_counter()\n    compress(\n        path, out, outmeta, sample_rate=sample_rate, n_channels=n_channels, dtype=dtype,\n        check_after_compress=False, **kwargs)\n    t1 = time.perf_counter()\n    wt = t1 - t0\n\n    # Decompress the file and write it to disk.\n    out2 = path.with_suffix(\'.decomp.bin\')\n    t0 = time.perf_counter()\n    decompress(out, outmeta, out2, check_after_decompress=False)\n    t1 = time.perf_counter()\n    rtc = t1 - t0\n\n    # Read the uncompressed file.\n    t0 = time.perf_counter()\n    x = load_raw_data(path, n_channels=n_channels, dtype=dtype, mmap=False)\n    assert x.size\n    t1 = time.perf_counter()\n    rtdec = t1 - t0\n\n    orig_size = path.stat().st_size\n    compressed_size = out.stat().st_size\n\n    return {\n        \'read_time_compressed\': rtc,\n        \'read_time_decompressed\': rtdec,\n        \'write_time\': wt,\n        \'ratio\': 100 - 100 * compressed_size / orig_size,\n    }\n\n\nparams = {\n    \'ds\': {\n        \'title\': \'dataset\',\n        \'values\': [\n            # (\'imec_385_1s.bin\', 385, 3e4, 1.),\n            # (\'imec_385_10s.bin\', 385, 3e4, 10.),\n            (\'imec_385_100s.bin\', 385, 3e4, 100.),\n            # (\'pierre_10s.bin\', 256, 2e4, 10.),\n        ],\n    },\n    \'n_threads\': {\n        \'title\': \'n_threads\',\n        \'values\': [1, 4, 8]\n    },\n    # \'do_time_diff\': {\n    #     \'title\': \'time diff\',\n    #     \'values\': [False, True],\n    # },\n    # \'do_spatial_diff\': {\n    #     \'title\': \'spatial diff\',\n    #     \'values\': [False, True],\n    # },\n\n    # \'compression_level\': {\n    #     \'title\': \'Compression level\',\n    #     \'values\': [-1],\n    # },\n    # \'chunk_duration\': {\n    #     \'title\': \'Chunk duration (s)\',\n    #     \'values\': [.1, 1, 10],\n    # },\n}\n\ntargets = {\n    \'values\': [\'write_time\', \'read_time_compressed\', \'read_time_decompressed\', \'ratio\'],\n}\n\n\ndef _iter_param_set(params):\n    """"""Iterate over all combinations of parameters as dictionaries {param: value}.""""""\n    yield from map(\n        dict, product(*(zip(repeat(param), info[\'values\']) for param, info in params.items())))\n\n\nclass PlotParams:\n    def __init__(self, fun, params, targets):\n        self.fun = fun\n        self.params = params\n        self.targets = targets\n\n        self.plot_param = self._get_param_for_plotdim(\'plot\')\n        self.row_param = self._get_param_for_plotdim(\'row\')\n        self.column_param = self._get_param_for_plotdim(\'column\')\n        self.group_param = self._get_param_for_plotdim(\'group\')\n        self.bar_param = self._get_param_for_plotdim(\'bar\')\n\n        self.target_plotdim = self.targets.get(\'plotdim\', \'\')\n        self.target_values = self.targets.get(\'values\', [])\n\n        self.plot_values = self._get_param_values(self.plot_param)\n        self.row_values = self._get_param_values(self.row_param)\n        self.column_values = self._get_param_values(self.column_param)\n        self.group_values = self._get_param_values(self.group_param)\n        self.bar_values = self._get_param_values(self.bar_param)\n\n        self.n_plots = len(self.plot_values) or 1\n        self.n_rows = len(self.row_values) or 1\n        self.n_columns = len(self.column_values) or 1\n        self.n_groups = len(self.group_values) or 1\n        self.n_bars = len(self.bar_values) or 1\n\n    def _get_param_values(self, param):\n        if param == \'target\':\n            return self.target_values\n        return self.params.get(param, {}).get(\'values\', [])\n\n    def _get_param_for_plotdim(self, plotdim):\n        if self.targets.get(\'plotdim\', \'\') == plotdim:\n            return \'target\'\n        try:\n            return next(p for p, i in self.params.items() if i.get(\'plotdim\', \'\') == plotdim)\n        except StopIteration:\n            return\n\n    def _get_target(self, plot_idx=0, row=0, column=0, group_idx=0, bar_idx=0):\n        if self.target_plotdim == \'plot\':\n            target_idx = plot_idx\n        elif self.target_plotdim == \'row\':\n            target_idx = row\n        elif self.target_plotdim == \'column\':\n            target_idx = column\n        elif self.target_plotdim == \'group\':\n            target_idx = group_idx\n        return target_idx\n\n    def get_plot_value(self, plot_idx=0, row=0, column=0, group_idx=0, bar_idx=0):\n        target_idx = self._get_target(\n            plot_idx=plot_idx, row=row, column=column, group_idx=group_idx, bar_idx=bar_idx)\n        params = {\n            self.row_param: self.row_values[row],\n            self.column_param: self.column_values[column],\n            self.group_param: self.group_values[group_idx],\n            self.plot_param: self.plot_values[plot_idx],\n            self.bar_param: self.bar_values[bar_idx],\n        }\n        params.pop(\'target\', None)\n        return self.fun(**params).get(self.target_values[target_idx], 0)\n\n    def make(self):\n        import matplotlib.pyplot as plt\n        fig, axes = plt.subplots(\n            self.n_rows, self.n_columns,\n        )\n        index = np.arange(self.n_groups)\n        bar_width = .75 / self.n_bars\n\n        for row in range(self.n_rows):\n            for column in range(self.n_columns):\n                if self.n_columns == self.n_rows == 1:\n                    ax = axes\n                elif self.n_columns == 1:\n                    ax = axes[row]\n                else:\n                    ax = axes[row, column]\n                target_idx = self._get_target(\n                    plot_idx=0, row=row, column=column)\n                for bar in range(self.n_bars):\n                    values = [\n                        self.get_plot_value(\n                            row=row, column=column, group_idx=group, bar_idx=bar)\n                        for group in range(self.n_groups)\n                    ]\n                    label = self.bar_values[bar]\n                    ax.bar(index + bar_width * bar, values, bar_width, label=label)\n\n                ax.set_xlabel(self._get_param_for_plotdim(\'group\'))\n                ax.set_ylabel(self.target_values[target_idx])\n                ax.set_xticks(index + bar_width, map(str, self.group_values))\n        return fig\n\n\ndef benchmark_plots(fun, params=None, targets=None, output_dir=None):\n    params = params or {}\n    pp = PlotParams(fun, params, targets)\n    fig = pp.make()\n    fig.tight_layout()\n    return fig\n\n\nif __name__ == \'__main__\':\n    location = \'.cache\'\n    memory = Memory(location, verbose=0)\n    fun = memory.cache(mtscomp_perf)\n    N = len(list(_iter_param_set(params)))\n    table = []\n    for param_set in tqdm(_iter_param_set(params), total=N):\n        output = fun(**param_set)\n        d = {k: v for k, v in param_set.items() if len(params[k][\'values\']) > 1}\n        d.update(output)\n        table.append(d)\n\n    print(tabulate(table, headers=\'keys\'))\n'"
mtscomp.py,31,"b'# -*- coding: utf-8 -*-\n\n""""""mtscomp: multichannel time series lossless compression in Python.""""""\n\n\n#------------------------------------------------------------------------------\n# Imports\n#------------------------------------------------------------------------------\n\nimport argparse\nimport bisect\nfrom functools import lru_cache\nimport hashlib\nimport json\nimport logging\nimport multiprocessing as mp\nfrom multiprocessing.dummy import Pool as ThreadPool\nimport os\nimport os.path as op\nfrom pathlib import Path\nimport sys\nfrom threading import Lock\nimport zlib\n\n# import traceback\n\nfrom tqdm import tqdm\nimport numpy as np\n\nlogger = logging.getLogger(__name__)\n\n\nlock = Lock()  # use for concurrent read on the same file with multithreaded decompression\n\n\n#------------------------------------------------------------------------------\n# Global variables\n#------------------------------------------------------------------------------\n\n__version__ = \'1.0.1\'\nFORMAT_VERSION = \'1.0\'\n\n__all__ = (\'load_raw_data\', \'Writer\', \'Reader\', \'compress\', \'decompress\')\n\n\nDEFAULT_CONFIG = list(dict(\n    algorithm=\'zlib\',  # only algorithm supported currently\n    cache_size=10,  # number of chunks to keep in memory while reading the data\n    check_after_compress=True,  # check the integrity of the compressed file\n    check_after_decompress=True,  # check the integrity of the decompressed file saved to disk\n    chunk_duration=1.,  # in seconds\n    chunk_order=\'F\',  # leads to slightly better compression than C order\n    comp_level=-1,  # zlib compression level\n    do_spatial_diff=False,  # benchmarks seem to show no compression performance benefits\n    do_time_diff=True,\n    n_threads=mp.cpu_count(),\n).items())  # convert to a list to ensure this dictionary is read-only\n\nCHECK_ATOL = 1e-16  # tolerance for floating point array comparison check\nCRITICAL_ERROR_URL = \\\n    ""https://github.com/int-brain-lab/mtscomp/issues/new?title=Critical+error""\n\n\n#------------------------------------------------------------------------------\n# Misc utils\n#------------------------------------------------------------------------------\n\n# Set a null handler on the root logger\nlogger = logging.getLogger(\'mtscomp\')\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.NullHandler())\n\n_logger_fmt = \'%(asctime)s.%(msecs)03d [%(levelname)s] %(caller)s %(message)s\'\n_logger_date_fmt = \'%H:%M:%S\'\n\n\nclass _Formatter(logging.Formatter):\n    def format(self, record):\n        # Only keep the first character in the level name.\n        record.levelname = record.levelname[0]\n        filename = op.splitext(op.basename(record.pathname))[0]\n        record.caller = \'{:s}:{:d}\'.format(filename, record.lineno).ljust(20)\n        message = super(_Formatter, self).format(record)\n        color_code = {\'D\': \'90\', \'I\': \'0\', \'W\': \'33\', \'E\': \'31\'}.get(record.levelname, \'7\')\n        message = \'\\33[%sm%s\\33[0m\' % (color_code, message)\n        return message\n\n\ndef add_default_handler(level=\'INFO\', logger=logger):\n    handler = logging.StreamHandler()\n    handler.setLevel(level)\n\n    formatter = _Formatter(fmt=_logger_fmt, datefmt=_logger_date_fmt)\n    handler.setFormatter(formatter)\n\n    logger.addHandler(handler)\n\n\nclass Bunch(dict):\n    """"""A subclass of dictionary with an additional dot syntax.""""""\n    def __init__(self, *args, **kwargs):\n        super(Bunch, self).__init__(*args, **kwargs)\n        self.__dict__ = self\n\n\ndef _clip(x, a, b):\n    return max(a, min(b, x))\n\n\n#------------------------------------------------------------------------------\n# I/O utils\n#------------------------------------------------------------------------------\n\ndef load_raw_data(path=None, n_channels=None, dtype=None, offset=None, mmap=True):\n    """"""Load raw data at a given path.""""""\n    path = Path(path)\n    assert path.exists(), ""File %s does not exist."" % path\n    assert dtype, ""The data type must be provided.""\n    n_channels = n_channels or 1\n    # Compute the array shape.\n    item_size = np.dtype(dtype).itemsize\n    offset = offset or 0\n    f_size = op.getsize(str(path))\n    n_samples = (f_size - offset) // (item_size * n_channels)\n    size = n_samples * n_channels\n    if size * item_size != (f_size - offset):\n        raise ValueError(\n            (""The file size (%d bytes) is incompatible with the specified parameters "" % f_size) +\n            (""(n_channels=%d, dtype=%s, offset=%d)"" % (n_channels, dtype, offset)))\n    if size == 0:\n        return np.zeros((0, n_channels), dtype=dtype)\n    shape = (n_samples, n_channels)\n    # Memmap the file into a NumPy-like array.\n    if mmap:\n        return np.memmap(str(path), dtype=dtype, shape=shape, offset=offset)\n    else:\n        if offset > 0:  # pragma: no cover\n            raise NotImplementedError()  # TODO\n        return np.fromfile(str(path), dtype).reshape(shape)\n\n\ndef diff_along_axis(chunk, axis=None):\n    """"""Perform a diff along a given axis in a 2D array.\n    Keep the first line/column identical.\n    """"""\n    if axis is None:\n        return chunk\n    assert 0 <= axis < chunk.ndim\n    chunkd = np.diff(chunk, axis=axis)\n    # The first row is the same (we need to keep the initial values in order to reconstruct\n    # the original array from the diff).\n    if axis == 0:\n        logger.log(5, ""Performing time diff."")\n        chunkd = np.concatenate((chunk[0, :][np.newaxis, :], chunkd), axis=axis)\n    elif axis == 1:\n        logger.debug(""Performing spatial diff."")\n        chunkd = np.concatenate((chunk[:, 0][:, np.newaxis], chunkd), axis=axis)\n    return chunkd\n\n\ndef cumsum_along_axis(chunk, axis=None):\n    """"""Perform a cumsum (inverse of diff) along a given axis in a 2D array.""""""\n    if axis is None:\n        return chunk\n    assert 0 <= axis < chunk.ndim\n    chunki = np.empty_like(chunk)\n    np.cumsum(chunk, axis=axis, out=chunki)\n    return chunki\n\n\n#------------------------------------------------------------------------------\n# Config\n#------------------------------------------------------------------------------\n\ndef config_path():\n    """"""Path to the configuration file.""""""\n    path = Path(\'~\') / \'.mtscomp\'\n    path = path.expanduser()\n    return path\n\n\nCONFIG_PATH = config_path()\n\n\ndef read_config(**kwargs):\n    """"""Return the configuration dictionary, with default values and values set by the user\n    in the configuration file.""""""\n    params = dict(DEFAULT_CONFIG)\n\n    if CONFIG_PATH.exists():\n        with CONFIG_PATH.open(\'r\') as f:\n            user_config = json.load(f)\n    else:\n        user_config = {}\n    # Update the user defaults, then the values passed to the function.\n    # We only update non-None values.\n    for d in (user_config, kwargs):\n        params.update({k: v for k, v in d.items() if v is not None})\n    return Bunch(params)\n\n\ndef write_config(**kwargs):\n    """"""Save some configuration key-values in the configuration file.""""""\n    config = read_config(**kwargs)\n    CONFIG_PATH.parent.mkdir(exist_ok=True, parents=True)\n    with CONFIG_PATH.open(\'w\') as f:\n        json.dump(config, f, indent=2, sort_keys=True)\n    return config\n\n\n#------------------------------------------------------------------------------\n# Low-level API\n#------------------------------------------------------------------------------\n\nclass Writer:\n    """"""Handle compression of a raw data file.\n\n    Constructor\n    -----------\n\n    chunk_duration : float\n        Duration of the chunks, in seconds.\n    algorithm : str\n        Name of the compression algorithm. Only `zlib` is supported at the moment.\n    comp_level : int\n        Compression level of the chosen algorithm.\n    do_time_diff : bool\n        Whether to compute the time-wise diff of the array before compressing.\n    do_spatial_diff : bool\n        Whether to compute the spatial diff of the array before compressing.\n    n_threads : int\n        Number of CPUs to use for compression. By default, use all of them.\n    before_check : function\n        A callback method that could be called just before the integrity check.\n    check_after_compress : bool\n        Whether to perform the automatic check after compression.\n\n    """"""\n    def __init__(self, before_check=None, **kwargs):\n        self.pool = None\n        self.quiet = kwargs.pop(\'quiet\', False)\n        config = read_config(**kwargs)\n        self.config = config\n        self.chunk_duration = config.chunk_duration\n        self.algorithm = config.algorithm\n        assert self.algorithm == \'zlib\', ""Only zlib is currently supported.""\n        self.comp_level = config.comp_level\n        self.do_time_diff = config.do_time_diff\n        self.do_spatial_diff = config.do_spatial_diff\n        self.n_threads = config.n_threads\n        self.before_check = before_check or (lambda x: None)\n        self.check_after_compress = config.check_after_compress\n        self.chunk_order = config.chunk_order\n\n    def open(\n            self, data_path, sample_rate=None, n_channels=None, dtype=None,\n            offset=None, mmap=True):\n        """"""Open a raw data (memmapped) from disk in order to compress it.\n\n        Parameters\n        ----------\n\n        data_path : str or Path\n            Path to the raw binary array.\n        sample_rate : float\n            Sample rate of the data.\n        n_channels : int\n            Number of columns (channels) in the data array.\n            The shape of the data is `(n_samples, n_channels)`.\n        dtype : dtype\n            NumPy data type of the data array.\n        offset : int\n            Offset, in bytes, of the data within the binary file.\n        mmap : bool\n            Whether the data should be memmapped.\n\n        """"""\n\n        self.data_path = Path(data_path)\n\n        # Get default parameters from the config file, if it exists.\n        sample_rate = sample_rate or self.config.get(\'sample_rate\', None)\n        if not sample_rate:\n            raise ValueError(""Please provide a sample rate (-s option in the command-line)."")\n\n        if str(data_path).endswith(\'.npy\'):\n            # NPY files.\n            self.data = np.load(data_path, mmap_mode=\'r\')\n            if self.data.ndim >= 3:\n                self.data = np.reshape(self.data, (-1, self.data.shape[-1]))\n            self.dtype = dtype = self.data.dtype\n            self.n_channels = n_channels = self.data.shape[1]\n        else:\n            # Raw binary files.\n            n_channels = n_channels or self.config.get(\'n_channels\', None)\n            if not n_channels:\n                raise ValueError(""Please provide n_channels (-n option in the command-line)."")\n            dtype = dtype or self.config.get(\'dtype\', None)\n            if not dtype:\n                raise ValueError(""Please provide a dtype (-d option in the command-line)."")\n            self.dtype = np.dtype(dtype)\n            self.data = load_raw_data(data_path, n_channels=n_channels, dtype=self.dtype)\n\n        self.sample_rate = sample_rate\n        assert sample_rate > 0\n        assert n_channels > 0\n        self.file_size = self.data.size * self.data.itemsize\n        assert self.data.ndim == 2\n        self.n_samples, self.n_channels = self.data.shape\n        assert self.n_samples > 0\n        assert self.n_channels > 0\n        assert n_channels == self.n_channels\n        duration = self.data.shape[0] / self.sample_rate\n        logger.info(\n            ""Opening %s, duration %.1fs, %d channels."", data_path, duration, self.n_channels)\n        self._compute_chunk_bounds()\n        self.sha1_compressed = hashlib.sha1()\n        self.sha1_uncompressed = hashlib.sha1()\n\n    def _compute_chunk_bounds(self):\n        """"""Compute the chunk bounds, in number of time samples.""""""\n        chunk_size = int(np.round(self.chunk_duration * self.sample_rate))\n        chunk_bounds = list(range(0, self.n_samples, chunk_size))\n        if chunk_bounds[-1] < self.n_samples:\n            chunk_bounds.append(self.n_samples)\n        # One element more than the number of chunks, the chunk is in\n        # chunk_bounds[i]:chunk_bounds[i+1] (first element included, last element excluded).\n        self.chunk_bounds = chunk_bounds\n        self.n_chunks = len(self.chunk_bounds) - 1\n        assert self.chunk_bounds[0] == 0\n        assert self.chunk_bounds[-1] == self.n_samples\n        logger.log(5, ""Chunk bounds: %s"", self.chunk_bounds)\n        # Batches.\n        self.batch_size = self.n_threads  # in each batch, there is 1 chunk per thread.\n        self.n_batches = int(np.ceil(self.n_chunks / self.batch_size))\n\n    def get_cmeta(self):\n        """"""Return the metadata of the compressed file.""""""\n        return {\n            \'version\': FORMAT_VERSION,\n            \'algorithm\': self.algorithm,\n            \'comp_level\': self.comp_level,\n            \'do_time_diff\': self.do_time_diff,\n            \'do_spatial_diff\': self.do_spatial_diff,\n            \'dtype\': str(np.dtype(self.dtype)),\n            \'n_channels\': self.n_channels,\n            \'sample_rate\': self.sample_rate,\n            \'chunk_bounds\': self.chunk_bounds,\n            \'chunk_offsets\': self.chunk_offsets,\n            \'chunk_order\': self.chunk_order,\n            \'sha1_compressed\': self.sha1_compressed.hexdigest(),\n            \'sha1_uncompressed\': self.sha1_uncompressed.hexdigest(),\n        }\n\n    def get_chunk(self, chunk_idx):\n        """"""Return a given chunk as a NumPy array with shape `(n_samples_chk, n_channels)`.\n\n        Parameters\n        ----------\n\n        chunk_idx : int\n            Index of the chunk, from 0 to `n_chunks - 1`.\n\n        """"""\n        assert 0 <= chunk_idx <= self.n_chunks - 1\n        i0 = self.chunk_bounds[chunk_idx]\n        i1 = self.chunk_bounds[chunk_idx + 1]\n        return self.data[i0:i1, :]\n\n    def _compress_chunk(self, chunk_idx):\n        # Retrieve the chunk data as a 2D NumPy array.\n        chunk = self.get_chunk(chunk_idx)\n        assert chunk.ndim == 2\n        assert chunk.shape[1] == self.n_channels\n        # Compute the diff along the time and/or spatial axis.\n        chunkd = diff_along_axis(chunk, axis=0 if self.do_time_diff else None)\n        chunkd = diff_along_axis(chunkd, axis=1 if self.do_spatial_diff else None)\n        assert chunkd.shape == chunk.shape\n        assert chunkd.dtype == chunk.dtype\n        # Check first line/column of the diffed chunk.\n        assert chunkd[0, 0] == chunk[0, 0]\n        if self.do_time_diff and not self.do_spatial_diff:\n            assert np.array_equal(chunkd[0, :], chunk[0, :])\n        elif not self.do_time_diff and self.do_spatial_diff:\n            assert np.array_equal(chunkd[:, 0], chunk[:, 0])\n        # Compress the diff.\n        logger.log(5, ""Compressing %d MB..."", (chunkd.size * chunk.itemsize) / 1024. ** 2)\n        # order=Fortran: Transposing (demultiplexing) the chunk may save a few %.\n        chunkdc = zlib.compress(chunkd.tobytes(order=self.chunk_order))\n        ratio = 100 - 100 * len(chunkdc) / (chunk.size * chunk.itemsize)\n        logger.debug(""Chunk %d/%d: -%.3f%%."", chunk_idx + 1, self.n_chunks, ratio)\n        return chunk_idx, (chunk, chunkdc)\n\n    def compress_batch(self, first_chunk, last_chunk):\n        """"""Write a given chunk into the output file.\n\n        Parameters\n        ----------\n\n        first_chunk : int\n            Index of the first chunk in the batch (included).\n        last_chunk : int\n            Index of the last chunk in the batch (excluded).\n\n        Returns\n        -------\n\n        chunks : dict\n            A dictionary mapping chunk indices to compressed chunks.\n\n        """"""\n        assert 0 <= first_chunk < last_chunk <= self.n_chunks\n        if self.n_threads == 1:\n            chunks = [\n                self._compress_chunk(chunk_idx) for chunk_idx in range(first_chunk, last_chunk)]\n        elif self.n_threads >= 2:\n            chunks = self.pool.map(self._compress_chunk, range(first_chunk, last_chunk))\n        return dict(chunks)\n\n    def write(self, out, outmeta):\n        """"""Write the compressed data in a compressed binary file, and a compression header file\n        in JSON.\n\n        Parameters\n        ----------\n\n        out : str or Path\n            Path to the compressed data binary file (typically \xcc\x80.cbin` file extension).\n        outmeta : str or Path\n            Path to the compression header JSON file (typicall `.ch` file extension).\n\n        Returns\n        -------\n\n        ratio : float\n            The ratio of the size of the compressed binary file versus the size of the\n            original binary file.\n\n        """"""\n        # Default file extension for output files.\n        if not out:\n            out = self.data_path.with_suffix(\'.c\' + self.data_path.suffix[1:])\n        if not outmeta:\n            outmeta = self.data_path.with_suffix(\'.ch\')\n        # Ensure the parent directory exists.\n        Path(out).parent.mkdir(exist_ok=True, parents=True)\n        # Write all chunks.\n        offset = 0\n        self.chunk_offsets = [0]\n        # Create the thread pool.\n        self.pool = ThreadPool(self.batch_size)\n        logger.debug(\'\\n\'.join(\'%s = %s\' % (k, v) for (k, v) in self.config.items()))\n        ts = \' on %d CPUs.\' % self.n_threads if self.n_threads > 1 else \'.\'\n        logger.info(""Starting compression"" + ts)\n        with open(out, \'wb\') as fb:\n            for batch in tqdm(range(self.n_batches), desc=\'Compressing\', disable=self.quiet):\n                first_chunk = self.batch_size * batch  # first included\n                last_chunk = min(self.batch_size * (batch + 1), self.n_chunks)  # last excluded\n                assert 0 <= first_chunk < last_chunk <= self.n_chunks\n                logger.debug(\n                    ""Processing batch #%d/%d with chunks %s."",\n                    batch + 1, self.n_batches, \', \'.join(map(str, range(first_chunk, last_chunk))))\n                # Compress all chunks in the batch.\n                compressed_chunks = self.compress_batch(first_chunk, last_chunk)\n                # Return a dictionary chunk_idx: compressed_buffer\n                assert set(compressed_chunks.keys()) <= set(range(first_chunk, last_chunk))\n                # Write the batch chunks to disk.\n                # Warning: we need to process the chunks in order.\n                for chunk_idx in sorted(compressed_chunks.keys()):\n                    uncompressed_chunk, compressed_chunk = compressed_chunks[chunk_idx]\n                    fb.write(compressed_chunk)\n                    # Append the chunk offsets.\n                    length = len(compressed_chunk)\n                    offset += length\n                    self.chunk_offsets.append(offset)\n                    # Compute the SHA1 hashes.\n                    self.sha1_uncompressed.update(uncompressed_chunk)\n                    self.sha1_compressed.update(compressed_chunk)\n            # Final size of the file.\n            csize = fb.tell()\n        assert self.chunk_offsets[-1] == csize\n        # Close the thread pool.\n        self.pool.close()\n        self.pool.join()\n        # Compute the compression ratio.\n        ratio = csize / self.file_size\n        logger.info(""Wrote %s (%.1f GB, -%.3f%%)."", out, csize / 1024 ** 3, 100 - 100 * ratio)\n        # Write the metadata file.\n        with open(outmeta, \'w\') as f:\n            json.dump(self.get_cmeta(), f, indent=2, sort_keys=True)\n        # Check that the written file matches the original file (once decompressed).\n        if self.check_after_compress:\n            # Callback function before checking.\n            self.before_check(self)\n            try:\n                check(self.data, out, outmeta)\n            except AssertionError:\n                raise RuntimeError(\n                    ""CRITICAL ERROR: automatic check failed when compressing the data. ""\n                    ""Report immediately to "" + CRITICAL_ERROR_URL)\n            logger.debug(""Automatic integrity check after compression PASSED."")\n        return ratio\n\n    def close(self):\n        """"""Close all file handles.""""""\n        self.data._mmap.close()\n\n\nclass Reader:\n    """"""Handle decompression of a compressed data file.\n\n    Constructor\n    -----------\n\n    cache_size : int\n        Maximum number of chunks to keep in memory while reading. Every chunk kept in cache\n        may take a few dozens of MB in RAM.\n    check_after_decompress : bool\n        Whether to perform the automatic check after decompression.\n\n    """"""\n    def __init__(self, **kwargs):\n        self.pool = None\n        self.quiet = kwargs.pop(\'quiet\', False)\n        self.config = read_config(**kwargs)\n        self.cache_size = self.config.cache_size\n        self.check_after_decompress = self.config.check_after_decompress\n\n    def open(self, cdata, cmeta=None):\n        """"""Open a compressed data file.\n\n        Parameters\n        ----------\n\n        cdata : str or Path\n            Path to the compressed data file.\n        cmeta : str or Path or dict\n            Path to the compression header JSON file, or its contents as a Python dictionary.\n\n        """"""\n        # Read metadata file.\n        if cmeta is None:\n            cmeta = Path(cdata).with_suffix(\'.ch\')\n        if not isinstance(cmeta, dict):\n            with open(cmeta, \'r\') as f:\n                cmeta = json.load(f)\n        assert isinstance(cmeta, dict)\n        self.cmeta = Bunch(cmeta)\n        # Read some values from the metadata file.\n        self.n_channels = self.cmeta.n_channels\n        self.sample_rate = self.cmeta.sample_rate\n        self.dtype = np.dtype(self.cmeta.dtype)\n        self.chunk_offsets = self.cmeta.chunk_offsets\n        self.chunk_bounds = self.cmeta.chunk_bounds\n        self.chunk_order = self.cmeta.chunk_order\n        self.n_samples = self.chunk_bounds[-1]\n        self.n_chunks = len(self.chunk_bounds) - 1\n        self.shape = (self.n_samples, self.n_channels)\n        self.ndim = 2\n\n        # Batches.\n        self.batch_size = self.config.n_threads  # in each batch, there is 1 chunk per thread.\n        self.n_batches = int(np.ceil(self.n_chunks / self.batch_size))\n\n        # Open data.\n        if isinstance(cdata, (str, Path)):\n            cdata = open(cdata, \'rb\')\n        self.cdata = cdata\n\n        self.set_cache_size()\n\n    def set_cache_size(self, cache_size=None):\n        """"""Set the LRU cache size for self.read_chunk().""""""\n        if cache_size != self.cache_size:\n            cache_size = cache_size or self.cache_size\n            assert cache_size > 0\n            self.read_chunk = lru_cache(maxsize=cache_size)(self.read_chunk)\n            self.cache_size = cache_size\n\n    def iter_chunks(self, first_chunk=0, last_chunk=None):\n        """"""Iterate over the compressed chunks.\n\n        Yield tuples `(chunk_idx, chunk_start, chunk_length)`.\n\n        """"""\n        last_chunk = last_chunk if last_chunk is not None else self.n_chunks - 1\n        for idx, (i0, i1) in enumerate(\n                zip(self.chunk_offsets[first_chunk:last_chunk + 1],\n                    self.chunk_offsets[first_chunk + 1:last_chunk + 2])):\n            yield first_chunk + idx, i0, i1 - i0\n\n    def read_chunk(self, chunk_idx, chunk_start, chunk_length):\n        """"""Read a compressed chunk and return a NumPy array.""""""\n        logger.debug(f""Reading compressed chunk {chunk_idx}, {chunk_start}, {chunk_length}"")\n        # Load the compressed chunk from the file.\n        if hasattr(os, \'pread\'):\n            # On UNIX, we use an atomic system call to read N bytes of data from the file so that\n            # this call is thread-safe.\n            cbuffer = os.pread(self.cdata.fileno(), chunk_length, chunk_start)\n        else:  # pragma: no cover\n            # Otherwise, we have to use two system calls, a seek and a read, and we need to\n            # put a lock so that we\'re sure that this pair of calls is atomic across threads.\n            with lock:\n                self.cdata.seek(chunk_start)\n                cbuffer = self.cdata.read(chunk_length)\n        assert len(cbuffer) == chunk_length\n        # Decompress the chunk.\n        buffer = zlib.decompress(cbuffer)\n        chunk = np.frombuffer(buffer, self.dtype)\n        assert chunk.dtype == self.dtype\n        # Find the chunk shape.\n        i0, i1 = self.chunk_bounds[chunk_idx:chunk_idx + 2]\n        assert i0 <= i1\n        n_samples_chunk = i1 - i0\n        assert chunk.size == n_samples_chunk * self.n_channels\n        # Reshape the chunk.\n        chunk = chunk.reshape((n_samples_chunk, self.n_channels), order=self.chunk_order)\n        chunki = cumsum_along_axis(chunk, axis=1 if self.cmeta.do_spatial_diff else None)\n        chunki = cumsum_along_axis(chunki, axis=0 if self.cmeta.do_time_diff else None)\n        assert chunki.dtype == chunk.dtype\n        assert chunki.shape == chunk.shape == (n_samples_chunk, self.n_channels)\n        return np.ascontiguousarray(chunki)  # needed when using F ordering in compression\n\n    def _decompress_chunk(self, chunk_idx):\n        """"""Decompress a chunk.""""""\n        logger.debug(""Starting decompression of chunk %d."", chunk_idx)\n        assert 0 <= chunk_idx <= self.n_chunks - 1\n        chunk_start = self.chunk_offsets[chunk_idx]\n        chunk_length = self.chunk_offsets[chunk_idx + 1] - chunk_start\n        return chunk_idx, self.read_chunk(chunk_idx, chunk_start, chunk_length)\n\n    def decompress_chunks(self, chunk_ids, pool=None):\n        # Return a dictionary chunk_idx: decompressed_chunk\n        assert pool\n        out = dict(pool.map(self._decompress_chunk, chunk_ids))\n        assert set(out.keys()) == set(chunk_ids)\n        return out\n\n    def _validate_index(self, i, value_for_none=0):\n        if i is None:\n            i = value_for_none\n        elif i < 0:\n            i += self.n_samples\n        i = _clip(i, 0, self.n_samples)\n        assert 0 <= i <= self.n_samples\n        return i\n\n    def _chunks_for_interval(self, i0, i1):\n        """"""Find the first and last chunks to be loaded in order to get the data between\n        time samples `i0` and `i1`.""""""\n\n        i0 = _clip(i0, 0, self.n_samples - 1)\n        i1 = _clip(i1, i0, self.n_samples - 1)\n        assert 0 <= i0 <= i1 <= self.n_samples\n\n        first_chunk = _clip(\n            bisect.bisect_right(self.chunk_bounds, i0) - 1, 0, self.n_chunks - 1)\n        assert 0 <= first_chunk < self.n_chunks\n        assert self.chunk_bounds[first_chunk] <= i0\n        # Ensure we don\'t load unnecessary chunks.\n        assert self.chunk_bounds[first_chunk + 1] > i0\n\n        last_chunk = _clip(\n            bisect.bisect_right(self.chunk_bounds, i1, lo=first_chunk) - 1, 0, self.n_chunks - 1)\n        assert 0 <= last_chunk < self.n_chunks\n        assert self.chunk_bounds[last_chunk + 1] >= i1\n        # Ensure we don\'t load unnecessary chunks.\n        assert self.chunk_bounds[last_chunk] <= i1\n\n        assert 0 <= first_chunk <= last_chunk <= self.n_chunks - 1\n        return first_chunk, last_chunk\n\n    def start_thread_pool(self):\n        """"""Start the thread pool for multithreaded decompression.""""""\n        if self.pool:  # \xc2\xa0pragma: no cover\n            return self.pool\n        logging.debug(""Starting thread pool with %d CPUs."", self.batch_size)\n        self.pool = ThreadPool(self.batch_size)\n        return self.pool\n\n    def stop_thread_pool(self):\n        """"""Stop the thread pool.""""""\n        logger.debug(""Stopping thread pool."")\n        self.pool.close()\n        self.pool.join()\n        self.pool = None\n\n    def tofile(self, out, overwrite=False):\n        """"""Write the decompressed array to disk.""""""\n        if out is None:\n            out = Path(self.cdata.name).with_suffix(\'.bin\')\n        out = Path(out)\n        # Handle overwriting.\n        if not overwrite and out.exists():  # pragma: no cover\n            raise ValueError(\n                ""The output file %s already exists, use --overwrite or specify another ""\n                ""output path."" % out)\n        elif overwrite and out.exists():\n            # NOTE: for some reason, on my computer (Ubuntu 19.04 on fresh ext4 HDD), closing the\n            # output file is very slow if it is being overwritten, rather than if it\'s a new file.\n            # So deleting the file to be overwritten before overwriting it saves ~10 seconds.\n            logger.debug(""Deleting %s."", out)\n            out.unlink()\n        # Create the thread pool.\n        self.start_thread_pool()\n        with open(out, \'wb\') as fb:\n            for batch in tqdm(range(self.n_batches), desc=\'Decompressing\', disable=self.quiet):\n                first_chunk = self.batch_size * batch  # first included\n                last_chunk = min(self.batch_size * (batch + 1), self.n_chunks)  # last excluded\n                assert 0 <= first_chunk < last_chunk <= self.n_chunks\n                logger.debug(\n                    ""Processing batch #%d/%d with chunks %s."",\n                    batch + 1, self.n_batches, \', \'.join(map(str, range(first_chunk, last_chunk))))\n                # Decompress all chunks in the batch.\n                decompressed_chunks = self.decompress_chunks(\n                    range(first_chunk, last_chunk), self.pool)\n                # Write the batch chunks to disk.\n                # Warning: we need to process the chunks in order.\n                for chunk_idx in sorted(decompressed_chunks.keys()):\n                    decompressed_chunk = decompressed_chunks[chunk_idx]\n                    fb.write(decompressed_chunk)\n            dsize = fb.tell()\n        assert dsize == self.chunk_bounds[-1] * self.n_channels * self.dtype.itemsize\n        # Close the thread pool.\n        self.stop_thread_pool()\n        logger.info(""Wrote %s (%.1f GB)."", out, dsize / 1024 ** 3)\n        if self.check_after_decompress:\n            decompressed = load_raw_data(out, n_channels=self.n_channels, dtype=self.dtype)\n            check(decompressed, self.cdata, self.cmeta)\n            logger.debug(""Automatic integrity check after decompression PASSED."")\n\n    def close(self):\n        """"""Close all file handles.""""""\n        self.cdata.close()\n\n    def __getitem__(self, item):\n        """"""Implement NumPy array slicing, return a regular in-memory NumPy array.""""""\n        fallback = np.zeros((0, self.n_channels), dtype=self.dtype)\n        if isinstance(item, slice):\n            # Slice indexing.\n            i0 = self._validate_index(item.start, 0)\n            i1 = self._validate_index(item.stop, self.n_samples)\n            if i1 <= i0:\n                return fallback\n            assert i0 < i1\n            first_chunk, last_chunk = self._chunks_for_interval(i0, i1)\n            chunks = []\n            for chunk_idx, chunk_start, chunk_length in self.iter_chunks(first_chunk, last_chunk):\n                chunk = self.read_chunk(chunk_idx, chunk_start, chunk_length)\n                chunks.append(chunk)\n            if not chunks:  # pragma: no cover\n                return fallback\n            if first_chunk < last_chunk:\n                # Concatenate all chunks.\n                ns = sum(chunk.shape[0] for chunk in chunks)\n                arr = np.empty((ns, self.n_channels), dtype=self.dtype)\n                arr = np.concatenate(chunks, out=arr)\n            else:\n                assert len(chunks) == 1\n                arr = chunks[0]\n            assert arr.ndim == 2\n            assert arr.shape[1] == self.n_channels\n            assert arr.shape[0] == (\n                self.chunk_bounds[last_chunk + 1] - self.chunk_bounds[first_chunk])\n            # Subselect in the chunk.\n            a = i0 - self.chunk_bounds[first_chunk]\n            b = i1 - self.chunk_bounds[first_chunk]\n            assert 0 <= a <= b <= arr.shape[0]\n            out = arr[a:b:item.step, :]\n            # Ensure the shape of the output is the expected shape from the slice length.\n            assert out.shape[0] == len(range(i0, i1, item.step or 1))\n            return out\n        elif isinstance(item, tuple):\n            # Multidimensional indexing.\n            if len(item) == 1:\n                return self[item[0]]\n            elif len(item) == 2 and np.isscalar(item[0]):\n                return self[item[0]][item[1]]\n            elif len(item) == 2:\n                return self[item[0]][:, item[1]]\n        elif isinstance(item, int):\n            if item < 0:\n                # Deal with negative indices.\n                k = -int(np.floor(item / self.n_samples))\n                item = item + self.n_samples * k\n                assert 0 <= item < self.n_samples\n            if not 0 <= item < self.n_samples:  # pragma: no cover\n                raise IndexError(\n                    ""index %d is out of bounds for axis 0 with size %d"" % (item, self.n_samples))\n            out = self[item:item + 1]\n            return out[0]\n        elif isinstance(item, (list, np.ndarray)):  # pragma: no cover\n            raise NotImplementedError(""Indexing with multiple values is currently unsupported."")\n        return fallback  # pragma: no cover\n\n    def __del__(self):\n        self.close()\n\n\n#------------------------------------------------------------------------------\n# High-level API\n#------------------------------------------------------------------------------\n\ndef check(data, out, outmeta):\n    """"""Check that the compressed data matches the original array byte per byte.""""""\n    unc = decompress(out, outmeta)\n    # Read all chunks.\n    for chunk_idx, chunk_start, chunk_length in tqdm(\n            unc.iter_chunks(), total=unc.n_chunks, desc=\'Checking\'):\n        chunk = unc.read_chunk(chunk_idx, chunk_start, chunk_length)\n        # Find the corresponding chunk from the original data array.\n        i0, i1 = unc.chunk_bounds[chunk_idx], unc.chunk_bounds[chunk_idx + 1]\n        expected = data[i0:i1]\n        # Check the dtype and shape match.\n        assert chunk.dtype == expected.dtype\n        assert chunk.shape == expected.shape\n        if np.issubdtype(chunk.dtype, np.integer):\n            # For integer dtypes, check that the array are exactly equal.\n            assert np.array_equal(chunk, expected)\n        else:\n            # For floating point dtypes, check that the array are almost equal\n            # (diff followed by cumsum does not yield exactly the same floating point numbers).\n            assert np.allclose(chunk, expected, atol=CHECK_ATOL)\n\n\ndef compress(\n        path, out=None, outmeta=None, sample_rate=None, n_channels=None, dtype=None, **kwargs):\n    """"""Compress a NumPy-like array (may be memmapped) into a compressed format\n    (two files, out and outmeta).\n\n    Parameters\n    ----------\n\n    path : str or Path\n        Path to a raw data binary file.\n    out : str or Path\n        Path the to compressed data file.\n    outmeta : str or Path\n        Path to the compression header JSON file.\n    sample_rate : float\n        Sampling rate, in Hz.\n    dtype : dtype\n        The data type of the array in the raw data file.\n    n_channels : int\n        Number of channels in the file.\n    chunk_duration : float\n        Duration of the chunks, in seconds.\n    algorithm : str\n        Name of the compression algorithm. Only `zlib` is supported at the moment.\n    comp_level : int\n        Compression level of the chosen algorithm.\n    do_time_diff : bool\n        Whether to compute the time-wise diff of the array before compressing.\n    do_spatial_diff : bool\n        Whether to compute the spatial diff of the array before compressing.\n    n_threads : int\n        Number of CPUs to use for compression. By default, use all of them.\n    check_after_compress : bool\n        Whether to perform the automatic check after compression.\n\n    Returns\n    -------\n\n    length : int\n        Number of bytes written.\n\n    Metadata dictionary\n    -------------------\n\n    Saved in the cmeta file as JSON.\n\n    version : str\n        Version number of the compression format.\n    algorithm : str\n        Name of the compression algorithm. Only `zlib` is supported at the moment.\n    comp_level : str\n        Compression level to be passed to the compression function.\n    n_channels : int\n        Number of channels.\n    sample_rate : float\n        Sampling rate, in Hz.\n    chunk_bounds : list of ints\n        Offsets of the chunks in time samples.\n    chunk_offsets : list of ints\n        Offsets of the chunks within the compressed raw buffer.\n\n    """"""\n\n    w = Writer(**kwargs)\n    w.open(path, sample_rate=sample_rate, n_channels=n_channels, dtype=dtype)\n    length = w.write(out, outmeta)\n    w.close()\n    return length\n\n\ndef decompress(cdata, cmeta=None, out=None, write_output=False, overwrite=False, **kwargs):\n    """"""Read an array from a compressed dataset (two files, cdata and cmeta), and\n    return a NumPy-like array (memmapping the compressed data file, and decompressing on the fly).\n\n    Note: the reader should be closed after use.\n\n    Parameters\n    ----------\n\n    cdata : str or Path\n        Path to the compressed data file.\n    cmeta : str or Path\n        Path to the compression header JSON file.\n    out : str or Path\n        Path to the decompressed file to be written.\n    check_after_decompress : bool\n        Whether to perform the automatic check after decompression.\n    write_output : bool\n        Whether to write the output to a file.\n    overwrite : bool\n        Whether to overwrite the output file if it already exists.\n\n    Returns\n    -------\n\n    reader : Reader instance\n        This object implements the NumPy slicing syntax to access\n        parts of the actual data as NumPy arrays.\n\n    """"""\n    if out:\n        write_output = True\n    r = Reader(**kwargs)\n    r.open(cdata, cmeta)\n    if write_output:\n        r.tofile(out, overwrite=overwrite)\n    return r\n\n\n#------------------------------------------------------------------------------\n# Command-line API utils\n#------------------------------------------------------------------------------\n\ndef exception_handler(\n        exception_type, exception, traceback, debug_hook=sys.excepthook):  # pragma: no cover\n    if \'--debug\' in sys.argv or \'-v\' in sys.argv:\n        debug_hook(exception_type, exception, traceback)\n    else:\n        print(""%s: %s"" % (exception_type.__name__, exception))\n\n\ndef _shared_options(parser):\n    parser.add_argument(\'-nc\', \'--no-check\', action=\'store_false\', help=\'no check\')\n    parser.add_argument(\'-v\', \'--debug\', action=\'store_true\', help=\'verbose\')\n    parser.add_argument(\'-p\', \'--cpus\', type=int, help=\'number of CPUs to use\')\n\n\ndef _args_to_config(parser, args, compress=True):\n    pargs = parser.parse_args(args)\n    # parser.nc=True means that the flag was not given => switch to default (True or config)\n    check_after = None if pargs.no_check is True else False\n    kwargs = dict(\n        n_threads=pargs.cpus,\n    )\n    if compress:\n        kwargs.update(\n            sample_rate=pargs.sample_rate,\n            n_channels=pargs.n_channels,\n            dtype=pargs.dtype.strip() if pargs.dtype else pargs.dtype,\n            chunk_duration=pargs.chunk,\n            check_after_compress=check_after,\n        )\n    else:\n        kwargs.update(\n            check_after_decompress=check_after,\n        )\n    config = read_config(**kwargs)\n    return pargs, config\n\n\n#------------------------------------------------------------------------------\n# Command-line API: mtscomp\n#------------------------------------------------------------------------------\n\ndef mtscomp_parser():\n    """"""Command-line interface to compress a file.""""""\n    parser = argparse.ArgumentParser(description=\'Compress a raw binary file.\')\n\n    parser.add_argument(\n        \'path\', type=str, help=\'input path of a raw binary file\')\n\n    parser.add_argument(\n        \'out\', type=str, nargs=\'?\',\n        help=\'output path of the compressed binary file (.cbin)\')\n\n    parser.add_argument(\n        \'outmeta\', type=str, nargs=\'?\',\n        help=\'output path of the compression metadata JSON file (.ch)\')\n\n    parser.add_argument(\'-d\', \'--dtype\', type=str, help=\'data type\')\n    parser.add_argument(\'-s\', \'--sample-rate\', type=float, help=\'sample rate\')\n    parser.add_argument(\'-n\', \'--n-channels\', type=int, help=\'number of channels\')\n    parser.add_argument(\'-c\', \'--chunk\', type=int, help=\'chunk duration\')\n\n    _shared_options(parser)\n\n    parser.add_argument(\n        \'--set-default\', action=\'store_true\', help=\'set the specified parameters as the default\')\n\n    return parser\n\n\ndef mtscomp(args=None):\n    """"""Compress a file.""""""\n    sys.excepthook = exception_handler\n    parser = mtscomp_parser()\n    pargs, config = _args_to_config(\n        parser, args or sys.argv[1:], compress=True)\n    add_default_handler(\'DEBUG\' if pargs.debug else \'INFO\')\n    if pargs.set_default:\n        write_config(**config)\n    compress(pargs.path, pargs.out, pargs.outmeta, **config)\n\n\n#------------------------------------------------------------------------------\n# Command-line API: mtsdecomp\n#------------------------------------------------------------------------------\n\ndef mtsdecomp_parser():\n    """"""Command-line interface to decompress a file.""""""\n    parser = argparse.ArgumentParser(description=\'Decompress a raw binary file.\')\n\n    parser.add_argument(\n        \'cdata\', type=str,\n        help=\'path to the input compressed binary file (.cbin)\')\n\n    parser.add_argument(\n        \'cmeta\', type=str, nargs=\'?\',\n        help=\'path to the input compression metadata JSON file (.ch)\')\n\n    parser.add_argument(\n        \'-o\', \'--out\', type=str, nargs=\'?\',\n        help=\'path to the output decompressed file (.bin)\')\n\n    parser.add_argument(\'--overwrite\', \'-f\', action=\'store_true\', help=\'overwrite existing output\')\n\n    _shared_options(parser)\n\n    return parser\n\n\ndef mtsdecomp(args=None):\n    """"""Decompress a file.""""""\n    sys.excepthook = exception_handler\n    parser = mtsdecomp_parser()\n    pargs, config = _args_to_config(parser, args or sys.argv[1:], compress=False)\n    add_default_handler(\'DEBUG\' if pargs.debug else \'INFO\')\n    decompress(\n        pargs.cdata, pargs.cmeta, out=pargs.out,\n        # check_after_decompress=config.check_after_compress,\n        write_output=True,\n        overwrite=pargs.overwrite,\n        **config\n    )\n\n\n#------------------------------------------------------------------------------\n# Command-line API: mtsdesc\n#------------------------------------------------------------------------------\n\ndef mtsdesc(args=None):\n    """"""Describe a compressed file.""""""\n    sys.excepthook = exception_handler\n    parser = mtsdecomp_parser()\n    pargs = parser.parse_args(args or sys.argv[1:])\n    r = Reader()\n    r.open(pargs.cdata, pargs.cmeta)\n    sr = float(r.cmeta.sample_rate)\n    info = dict(\n        dtype=r.dtype,\n        sample_rate=sr,\n        n_channels=r.n_channels,\n        duration=\'%.1fs\' % (r.n_samples / sr),\n        n_samples=r.n_samples,\n        chunk_duration=\'%.1fs\' % (np.diff(r.chunk_bounds).mean() / sr),\n        n_chunks=r.n_chunks,\n    )\n    for k, v in info.items():\n        print(\'{:<15}\'.format(k), str(v))\n'"
setup.py,0,"b'# -*- coding: utf-8 -*-\n# flake8: noqa\n\n""""""Installation script.""""""\n\n\n#------------------------------------------------------------------------------\n# Imports\n#------------------------------------------------------------------------------\n\nimport os.path as op\nimport re\n\nfrom setuptools import setup, find_packages\n\n\n#------------------------------------------------------------------------------\n# Setup\n#------------------------------------------------------------------------------\n\ncurdir = op.dirname(op.realpath(__file__))\nwith open(op.join(curdir, \'README.md\')) as f:\n    readme = f.read()\n\n\n# Find version number from `__init__.py` without executing it.\nfilename = op.join(curdir, \'mtscomp.py\')\nwith open(filename, \'r\') as f:\n    version = re.search(r""__version__ = \'([^\']+)\'"", f.read()).group(1)\n\n\nsetup(\n    name=\'mtscomp\',\n    version=version,\n    license=""BSD"",\n    description=\'Lossless compression for electrophysiology time-series\',\n    long_description=readme,\n    long_description_content_type=\'text/markdown\',\n    author=\'Cyrille Rossant (International Brain Laboratory)\',\n    author_email=\'cyrille.rossant@gmail.com\',\n    url=\'https://github.com/int-brain-lab/mtscomp\',\n    py_modules=[\'mtscomp\'],\n    packages=find_packages(),\n    package_dir={\'mtscomp\': \'mtscomp\'},\n    package_data={\n        \'mtscomp\': [],\n    },\n    entry_points={\n        \'console_scripts\': [\n            \'mtscomp=mtscomp:mtscomp\',\n            \'mtsdecomp=mtscomp:mtsdecomp\',\n            \'mtsdesc=mtscomp:mtsdesc\',\n        ],\n    },\n    include_package_data=True,\n    keywords=\'lossless,compresssion,electrophysiology,neuroscience\',\n    classifiers=[\n        \'Development Status :: 4 - Beta\',\n        \'Intended Audience :: Developers\',\n        \'License :: OSI Approved :: BSD License\',\n        \'Natural Language :: English\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.7\',\n    ],\n)\n'"
tests.py,40,"b'# -*- coding: utf-8 -*-\n\n""""""mtscomp tests.""""""\n\n\n#------------------------------------------------------------------------------\n# Imports\n#------------------------------------------------------------------------------\n\nfrom contextlib import redirect_stdout\nimport io\nfrom itertools import product\nimport json\nimport hashlib\nimport logging\nimport os\nimport os.path as op\nfrom pathlib import Path\nimport re\n\nimport numpy as np\nfrom pytest import fixture, raises, mark\n\nimport mtscomp as mtscomp_mod\nfrom mtscomp import (\n    add_default_handler,\n    Writer, Reader, load_raw_data, diff_along_axis, cumsum_along_axis,\n    mtscomp_parser, mtsdecomp_parser, _args_to_config, read_config,\n    compress, decompress, mtsdesc, mtscomp, mtsdecomp,\n    CHECK_ATOL)\n\nlogger = logging.getLogger(__name__)\n\n\n#------------------------------------------------------------------------------\n# Fixtures\n#------------------------------------------------------------------------------\n\nn_channels = 19\nsample_rate = 1234.\nduration = 5.67\nnormal_std = .25\ntime = np.arange(0, duration, 1. / sample_rate)\nn_samples = len(time)\n\n\nadd_default_handler(\'DEBUG\')\n\n\n#------------------------------------------------------------------------------\n# Fixtures\n#------------------------------------------------------------------------------\n\n@fixture\ndef tmp_path_(tmp_path):\n    # HACK: do not use user config path in tests\n    mtscomp_mod.CONFIG_PATH = tmp_path / \'.mtscomp\'\n    return tmp_path\n\n\n@fixture\ndef path(tmp_path_):\n    return Path(tmp_path_) / \'data.bin\'\n\n\ndef zeros():\n    return np.zeros((n_samples, n_channels), dtype=np.float32)\n\n\ndef randn():\n    return np.random.normal(loc=0, scale=normal_std, size=(n_samples, n_channels))\n\n\ndef randn_custom(ns, nc):\n    return np.random.normal(loc=0, scale=normal_std, size=(ns, nc))\n\n\ndef white_sine():\n    return np.sin(10 * time)[:, np.newaxis] + randn()\n\n\ndef colored_sine():\n    arr = white_sine()\n    try:\n        from scipy.signal import filtfilt, butter\n    except ImportError:\n        logger.debug(""Skip the filtering as SciPy is not available."")\n        return arr\n    b, a = butter(3, 0.05)\n    arr = filtfilt(b, a, arr, axis=0)\n    assert arr.shape == (n_samples, n_channels)\n    return arr\n\n\n@fixture(params=(\'zeros\', \'randn\', \'white_sine\', \'colored_sine\'))\ndef arr(request):\n    return globals()[request.param]()\n\n\n@fixture(params=(\'uint8\', \'uint16\', \'int8\', \'int16\', \'int32\'))\ndef dtype(request):\n    return np.dtype(request.param)\n\n\n#------------------------------------------------------------------------------\n# Test utils\n#------------------------------------------------------------------------------\n\n_INT16_MAX = 32766\n\n\ndef _write_arr(path, arr):\n    """"""Write an array.""""""\n    with open(path, \'wb\') as f:\n        arr.tofile(f)\n\n\ndef _to_int16(arr, M=None):\n    M = M or np.abs(arr).max()\n    arr = arr / M if M > 0 else arr\n    assert np.all(np.abs(arr) <= 1.)\n    arr16 = (arr * _INT16_MAX).astype(np.int16)\n    return arr16\n\n\ndef _from_int16(arr, M):\n    return arr * float(M / _INT16_MAX)\n\n\ndef _round_trip(path, arr, **ckwargs):\n    _write_arr(path, arr)\n    out = path.parent / \'data.cbin\'\n    outmeta = path.parent / \'data.ch\'\n    compress(\n        path, out, outmeta, sample_rate=sample_rate, n_channels=arr.shape[1],\n        dtype=arr.dtype, **ckwargs)\n    unc = decompress(out, outmeta)\n    assert np.allclose(unc[:], arr)\n    return unc\n\n\ndef sha1(buf):\n    sha = hashlib.sha1()\n    sha.update(buf)\n    return sha.hexdigest()\n\n\n#------------------------------------------------------------------------------\n# Misc tests\n#------------------------------------------------------------------------------\n\ndef test_config_1(tmp_path_):\n    """"""Test default options/""""""\n    config = read_config()\n    assert config.check_after_compress\n    assert config.check_after_decompress\n    assert config.do_time_diff\n    assert not config.do_spatial_diff\n\n\ndef test_load_raw_data(path):\n    arrs = [\n        np.zeros((0, 1)),\n        np.zeros((1, 1)),\n        np.zeros((10, 1)),\n        np.zeros((10, 10)),\n        np.random.randn(100, 10),\n\n        np.random.randn(100, 10).astype(np.float32),\n        (np.random.randn(100, 10) * 100).astype(np.int16),\n        np.random.randint(low=0, high=255, size=(100, 10)).astype(np.uint8),\n    ]\n    for arr in arrs:\n        for mmap in (True, False):\n            with open(path, \'wb\') as f:\n                arr.tofile(f)\n            n_channels = arr.shape[1] if arr.ndim >= 2 else 1\n            loaded = load_raw_data(path=path, n_channels=n_channels, dtype=arr.dtype, mmap=mmap)\n            assert np.array_equal(arr, loaded)\n\n\ndef test_int16(arr):\n    M = np.abs(arr).max()\n    arr16 = _to_int16(arr, M=M)\n    arr_ = _from_int16(arr16, M)\n    assert np.allclose(arr_, arr, atol=1e-4)\n\n\n@mark.parametrize(\'ax1\', [None, 0, 1])\n@mark.parametrize(\'ax2\', [None, 0, 1])\ndef test_diff_cumsum_1(arr, ax1, ax2):\n    if ax1 == ax2 and ax1 is not None:\n        # Skip double diff along the same axis.\n        return\n    arrd = diff_along_axis(arr, axis=ax1)\n    arrd = diff_along_axis(arrd, axis=ax2)\n    arr2 = cumsum_along_axis(arrd, axis=ax2)\n    arr2 = cumsum_along_axis(arr2, axis=ax1)\n    assert arr.shape == arr2.shape\n    assert arr.dtype == arr2.dtype\n    if np.issubdtype(arr.dtype, np.integer):\n        assert np.array_equal(arr, arr2)\n    else:\n        assert np.allclose(arr, arr2, atol=CHECK_ATOL)\n\n\n#------------------------------------------------------------------------------\n# Read/write tests\n#------------------------------------------------------------------------------\n\ndef test_low(path, arr):\n    out = path.parent / \'data.cbin\'\n    outmeta = path.parent / \'data.ch\'\n\n    # Write the array into a raw data binary file.\n    _write_arr(path, arr)\n\n    # Compress the file.\n    w = Writer()\n    w.open(path, sample_rate=sample_rate, n_channels=arr.shape[1], dtype=arr.dtype)\n    # Note: out and outmeta should be set by default to the values specified above.\n    w.write(None, None)\n    w.close()\n\n    # Load the compressed file.\n    r = Reader()\n    r.open(out, outmeta)\n    decomp = r[:]\n    r.close()\n\n    # Ensure the array are equal.\n    assert np.allclose(arr, decomp)\n\n\ndef test_high(path, arr):\n    _round_trip(path, arr)\n\n\ndef test_dtypes(path, dtype):\n    # Test various int dtypes.\n    arr = np.array(np.random.randint(low=0, high=255, size=(1000, 100)), dtype=dtype).T\n    _round_trip(path, arr)\n\n\ndef test_reader_indexing_1(path, arr):\n    # Write the array into a raw data binary file.\n    M = np.abs(arr).max()\n    arr16 = _to_int16(arr, M)\n    unc = _round_trip(path, arr16)\n    # Index with many different items.\n    N = n_samples\n\n    # First, degenerate slices.\n    items = [\n        slice(start, stop, step) for start, stop, step in product(\n            (None, 0, 1, -1), (None, 0, 1, -1), (None, 2, 3, N // 2, N))]\n\n    # Other slices with random numbers.\n    X = np.random.randint(low=-100, high=2 * N, size=(100, 3))\n    items.extend([slice(start, stop, step) for start, stop, step in X])\n\n    items.extend([\n        (slice(None, None, None),),\n        (slice(None, None, None), slice(1, -1, 2)),\n        (slice(None, None, None), [1, 5, 3]),\n        (slice(None, None, None), 1),\n        (1, slice(None, None, None)),\n        (2, 1),\n    ])\n\n    # Single integers.\n    items.extend([0, 1, N - 2, N - 1])  # N, N + 1, -1, -2])\n    items.extend(np.random.randint(low=-N, high=N, size=100).tolist())\n\n    # For every item, check the decompression.\n    for s in items:\n        if isinstance(s, slice) and s.step is not None and s.step <= 0:\n            continue\n        # If the indexing fails, ensures the same indexing fails on the Reader.\n        try:\n            expected = arr16[s]\n        except IndexError:\n            with raises(IndexError):\n                unc[s]\n                continue\n        sliced = unc[s]\n        assert sliced.dtype == expected.dtype\n        assert sliced.shape == expected.shape\n        assert np.array_equal(sliced, expected)\n        assert np.allclose(_from_int16(sliced, M), arr[s], atol=1e-4)\n\n\ndef test_reader_indexing_2(path, arr):\n    # Write the array into a raw data binary file.\n    M = np.abs(arr).max()\n    arr16 = _to_int16(arr, M)\n    unc = _round_trip(path, arr16)\n\n    expected = [\n        (-1, 2, 0, 0),\n        (0, 0, 0, 0),\n        (0, 1, 0, 0),\n        (1, 1, 0, 0),\n        (1, 1, 0, 0),\n        (2, 1, 0, 0),\n        (2, -1, 0, 0),\n        (2, 2, 0, 0),\n        (2, 2, 0, 0),\n\n        (1233, 1233, 0, 0),\n        (1233, 1234, 0, 1),\n        (1234, 1234, 1, 1),\n        (1234, 1235, 1, 1),\n\n\n        (1233, 1233, 0, 0),\n        (1233, 1234, 0, 1),\n        (1234, 1234, 1, 1),\n        (1234, 1235, 1, 1),\n\n        (-10000, 10000, 0, 5),\n        (0, 10000, 0, 5),\n\n        (1233, 10000, 0, 5),\n        (1234, 10000, 1, 5),\n\n        (6996, 10000, 5, 5),\n        (6997, 10000, 5, 5),\n        (6998, 10000, 5, 5),\n    ]\n\n    for i0, i1, c0, c1 in expected:\n        assert unc._chunks_for_interval(i0, i1) == (c0, c1)\n\n\ndef test_check_fail(path, arr):\n    """"""Check that compression fails if we change one byte in the original file before finishing\n    the write() method.""""""\n    out = path.parent / \'data.cbin\'\n    outmeta = path.parent / \'data.ch\'\n\n    # Write the array into a raw data binary file.\n    _write_arr(path, arr)\n\n    def before_check(writer):\n        # Change one byte in the original file.\n        # First, we need to close the original memmapped file before we can write to it.\n        writer.close()\n        # Then, we change one byte in it.\n        f_size = op.getsize(path)\n        # WARNING: must open in r+b mode in order to modify bytes in-place in the middle of\n        # the file.\n        with open(str(path), \'r+b\') as f:\n            f.seek(f_size // 2)\n            # Also, it\'s better to change multiple bytes at the same time to be sure that\n            # the underlying number (e.g. float64) is fully modified.\n            f.write(os.urandom(8))\n        assert not np.allclose(np.fromfile(str(path), dtype=arr.dtype), arr.ravel())\n        # The file size should be the same, although one byte has been changed.\n        assert op.getsize(path) == f_size\n        # Finally, we open it again before the check.\n        writer.open(path, sample_rate=sample_rate, n_channels=arr.shape[1], dtype=arr.dtype)\n\n    # Compress the file.\n    with raises(RuntimeError):\n        w = Writer(before_check=before_check)\n        w.open(path, sample_rate=sample_rate, n_channels=arr.shape[1], dtype=arr.dtype)\n        w.write(out, outmeta)\n        w.close()\n\n\ndef test_comp_decomp(path):\n    """"""Compress and decompress a random binary file with integer data type, and check the files\n    are byte to byte equal. This would not work for floating-point data types.""""""\n    arr = np.array(np.random.randint(low=0, high=255, size=(1000, 1000)), dtype=np.int16).T\n    _write_arr(path, arr)\n    out = path.parent / \'data.cbin\'\n    outmeta = path.parent / \'data.ch\'\n    compress(\n        path, out, outmeta, sample_rate=sample_rate, n_channels=arr.shape[1], dtype=arr.dtype,\n    )\n    decompressed_path = path.with_suffix(\'.decomp.bin\')\n    decompress(out, outmeta, out=decompressed_path)\n\n    # Check the files are equal.\n    with open(str(path), \'rb\') as f:\n        buf1 = f.read()\n        sha1_original = sha1(buf1)\n    with open(str(decompressed_path), \'rb\') as f:\n        buf2 = f.read()\n        sha1_decompressed = sha1(buf2)\n    assert buf1 == buf2\n\n    # Check the SHA1s.\n    with open(str(out), \'rb\') as f:\n        sha1_compressed = sha1(f.read())\n    with open(str(outmeta), \'r\') as f:\n        meta = json.load(f)\n\n    assert meta[\'sha1_compressed\'] == sha1_compressed\n    assert meta[\'sha1_uncompressed\'] == sha1_decompressed == sha1_original\n\n\ndef test_decompress_pool(path, arr):\n    _write_arr(path, arr)\n    out = path.parent / \'data.cbin\'\n    outmeta = path.parent / \'data.ch\'\n    compress(\n        path, out, outmeta, sample_rate=sample_rate, n_channels=arr.shape[1], dtype=arr.dtype,\n        check_after_compress=False)\n    reader = decompress(out, outmeta, cache_size=2)\n\n    pool = reader.start_thread_pool()\n    d1 = reader.decompress_chunks([0, 1, 2], pool=pool)\n    d2 = reader.decompress_chunks([1, 2, 3], pool=pool)\n    d3 = reader.decompress_chunks([0, 1, 3], pool=pool)\n    reader.stop_thread_pool()\n\n    assert sorted(d1.keys()) == [0, 1, 2]\n    assert sorted(d2.keys()) == [1, 2, 3]\n    assert sorted(d3.keys()) == [0, 1, 3]\n\n\n#------------------------------------------------------------------------------\n# Read/write tests with different parameters\n#------------------------------------------------------------------------------\n\n@mark.parametrize(\'chunk_duration\', [.01, .1, 1., 10.])\ndef test_chunk_duration(path, arr, chunk_duration):\n    _round_trip(path, arr, chunk_duration=chunk_duration)\n\n\n@mark.parametrize(\'ns\', [0, 1, 100, 10000])\n@mark.parametrize(\'nc\', [0, 1, 10, 100])\ndef test_n_channels(path, ns, nc):\n    arr = randn_custom(ns, nc)\n    if 0 in (ns, nc):\n        with raises(Exception):\n            _round_trip(path, arr)\n    else:\n        _round_trip(path, arr)\n\n\n@mark.parametrize(\'do_time_diff\', [True, False])\n@mark.parametrize(\'do_spatial_diff\', [True, False])\n@mark.parametrize(\'comp_level\', [1, 6, 9])\ndef test_comp_levels_do_diff(path, arr, comp_level, do_time_diff, do_spatial_diff):\n    _round_trip(\n        path, arr, compression_level=comp_level,\n        do_time_diff=do_time_diff, do_spatial_diff=do_spatial_diff)\n\n\n@mark.parametrize(\'n_threads\', [1, 2, 4, None])\ndef test_n_threads(path, arr, n_threads):\n    _round_trip(path, arr, n_threads=n_threads)\n\n\n#------------------------------------------------------------------------------\n# CLI tests\n#------------------------------------------------------------------------------\n\ndef test_cliargs_0(tmp_path_):\n    """"""Test default parameters.""""""\n    parser = mtscomp_parser()\n\n    args = [\'somefile\']\n    pargs, config = _args_to_config(parser, args)\n    assert config.algorithm == \'zlib\'\n    assert config.check_after_compress\n    assert config.check_after_decompress\n    assert config.do_time_diff\n    assert not config.do_spatial_diff\n\n    pargs, config = _args_to_config(parser, args + [\'-p 3\'])\n    assert config.n_threads == 3\n    assert config.check_after_compress\n    assert config.check_after_decompress\n\n    pargs, config = _args_to_config(parser, args + [\'-c 2\', \'-s 10000\', \'-n 123\', \'-d uint8\'])\n    assert config.chunk_duration == 2\n    assert config.sample_rate == 10000\n    assert config.n_channels == 123\n    assert config.dtype == \'uint8\'\n    assert config.check_after_compress\n    assert config.check_after_decompress\n    assert not pargs.debug\n\n    pargs, config = _args_to_config(parser, args + [\'-c 2\', \'-nc\', \'--debug\'])\n    assert not config.check_after_compress\n    assert config.check_after_decompress\n    assert pargs.debug\n\n\ndef test_cliargs_1(tmp_path_):\n    """"""Test default parameters.""""""\n    parser = mtsdecomp_parser()\n\n    args = [\'somefile\']\n    pargs, config = _args_to_config(parser, args, compress=False)\n    assert config.check_after_compress\n    assert config.check_after_decompress\n\n    pargs, config = _args_to_config(parser, args + [\'-nc\'], compress=False)\n    assert config.check_after_compress\n    assert not config.check_after_decompress\n\n\ndef test_cli_1(path, arr):\n    _write_arr(path, arr)\n    out = path.parent / \'data.cbin\'\n    outmeta = path.parent / \'data.ch\'\n    path2 = path.parent / \'data2.bin\'\n\n    with raises(ValueError):\n        # Wrong number of channels: should raise an error because the file size is not a\n        # multiple of that wrong number of channels.\n        mtscomp([\n            str(path), \'-d\', str(arr.dtype), \'-s\', str(sample_rate), \'-n\', str(arr.shape[1] + 1)])\n    # Compress.\n    mtscomp([str(path), \'-d\', str(arr.dtype), \'-s\', str(sample_rate), \'-n\', str(arr.shape[1])])\n\n    # Capture the compressed dataset description.\n    f = io.StringIO()\n    with redirect_stdout(f):\n        mtsdesc([str(out), str(outmeta)])\n    desc = f.getvalue()\n    dt = np.dtype(re.search(r\'dtype[ ]+(\\S+)\', desc).group(1))\n    nc = int(re.search(r\'n_channels[ ]+([0-9]+)\', desc).group(1))\n\n    # Decompress.\n    mtsdecomp([str(out), str(outmeta), \'-o\', str(path2)])\n\n    # Extract n_channels and dtype from the description.\n    decompressed = load_raw_data(path=path2, n_channels=nc, dtype=dt)\n\n    # Check that the decompressed and original arrays match.\n    np.allclose(decompressed, arr)\n\n\ndef test_cli_2(path, arr):\n    _write_arr(path, arr)\n    parser = mtscomp_parser()\n    args = [\n        str(path), \'-d\', str(arr.dtype), \'-s\', str(sample_rate), \'-n\', str(arr.shape[1]),\n        \'-nc\']\n\n    # Error raised if params are not given.\n    with raises(ValueError):\n        mtscomp(args[:1] + [\'--debug\'])\n    mtscomp(args)\n    with raises(ValueError):\n        mtscomp(args[:1] + args[3:])\n\n    # Now, we use --set-default\n    mtscomp(args + [\'--set-default\'])\n    # This call should not fail this time.\n    mtscomp(args[:1])\n\n    # Check the saved default config.\n    pargs, config = _args_to_config(parser, args[:1])\n    assert config.dtype == str(arr.dtype)\n    assert config.check_after_compress is False\n    assert config.n_channels == 19\n    assert config.sample_rate == 1234\n\n    mtsdecomp([str(path.with_suffix(\'.cbin\')), \'-f\', \'--debug\'])\n\n    # Test override of a parameter despite set_default.\n    pargs, config = _args_to_config(parser, args[:1] + [\'-s 100\'])\n    assert config.sample_rate == 100\n\n\ndef test_cli_3(path, arr):\n    _write_arr(path, arr)\n    parser = mtscomp_parser()\n    args = [str(path), \'-d\', str(arr.dtype), \'-s\', str(sample_rate)]\n\n    # Error raised if params are not given.\n    with raises(ValueError):\n        mtscomp(args)\n    with raises(ValueError):\n        mtscomp(args[:1] + [\'-n\', \'19\'])\n\n    # Now, we use --set-default\n    with raises(ValueError):\n        mtscomp(args + [\'--set-default\'])\n    # This should still fail.\n    with raises(ValueError):\n        mtscomp(args[:1])\n    # Should not fail\n    mtscomp(args[:1] + [\'-n\', \'19\'])\n\n    # Check the saved default config.\n    pargs, config = _args_to_config(parser, args[:1])\n    assert config.dtype == str(arr.dtype)\n    assert config.check_after_compress is True\n    assert config.get(\'n_channels\', None) is None\n    assert config.sample_rate == 1234\n\n\ndef test_cli_4(tmp_path_, arr):\n\n    arr = np.stack([arr, arr], axis=2)\n\n    root = Path(tmp_path_)\n    path = root / \'data.npy\'\n    pathu = root / \'data.bin\'\n\n    np.save(path, arr)\n    args = [str(path), \'-s\', str(sample_rate)]\n    mtscomp(args)\n    assert \'data.cnpy\' in (p.name for p in path.parent.iterdir())\n\n    assert not pathu.exists()\n    pathc = root / \'data.cnpy\'\n    args = [str(pathc)]\n    mtsdecomp(args)\n    assert pathu.exists()\n\n    arru = np.fromfile(str(pathu), dtype=arr.dtype).reshape(arr.shape)\n    assert np.allclose(arr, arru)\n'"
