file_path,api_count,code
setup.py,1,"b'# -*- coding: utf-8 -*-\n\nimport sys\nimport os\nimport ast\n\nfrom setuptools import setup\nfrom setuptools.extension import Extension\n\n\ndef build_ext(*args, **kwargs):\n    import Cython.Build\n    return Cython.Build.build_ext(*args, **kwargs)\n\n\nlibname = ""wildboar""\nbuild_type = ""optimized""\n\nSHORTDESC = ""wildboar is the fundamental package for time series classification with Python""\n\nDESC = """"""\nIt provides:\n\n * Shapelet tree classification and regression\n * Random shapelet forest classification and regression\n * Fast dynamic time warning searching\n * Fast euclidean distance searching\n\nThe package is provided under the GPLv3 license.\n""""""\n\ndatadirs = (""test"",)\ndataexts = ("".py"", "".pyx"", "".pxd"", "".c"", "".cpp"", "".h"", "".sh"", "".lyx"", "".tex"",\n            "".txt"", "".pdf"")\n\nstandard_docs = [""README"", ""LICENSE"", ""TODO"", ""CHANGELOG"", ""AUTHORS""]\nstandard_doc_exts = ["".md"", "".rst"", "".txt"", """", "".org""]\n\nif sys.version_info < (3, 4):\n    sys.exit(\'Sorry, Python < 3.4 is not supported\')\n\nextra_compile_args_math_optimized = [\n    \'-march=native\',\n    \'-O2\',\n    \'-msse\',\n    \'-msse2\',\n    \'-mfma\',\n    \'-mfpmath=sse\',\n]\nextra_compile_args_math_debug = [\n    \'-march=native\',\n    \'-O0\',\n    \'-g\',\n]\n\nextra_link_args_math_optimized = []\nextra_link_args_math_debug = []\n\nextra_compile_args_nonmath_optimized = [\'-O2\']\nextra_compile_args_nonmath_debug = [\'-O0\', \'-g\']\nextra_link_args_nonmath_optimized = []\nextra_link_args_nonmath_debug = []\n\nopenmp_compile_args = [\'-fopenmp\']\nopenmp_link_args = [\'-fopenmp\']\n\n\n# Lazy loading\nclass np_include_dirs(str):\n    def __str__(self):\n        import numpy as np\n        return np.get_include()\n\n\nmy_include_dirs = [np_include_dirs()]\n\nif build_type == \'optimized\':\n    my_extra_compile_args_math = extra_compile_args_math_optimized\n    my_extra_compile_args_nonmath = extra_compile_args_nonmath_optimized\n    my_extra_link_args_math = extra_link_args_math_optimized\n    my_extra_link_args_nonmath = extra_link_args_nonmath_optimized\n    my_debug = False\n    print(""build configuration selected: optimized"")\nelif build_type == \'debug\':\n    my_extra_compile_args_math = extra_compile_args_math_debug\n    my_extra_compile_args_nonmath = extra_compile_args_nonmath_debug\n    my_extra_link_args_math = extra_link_args_math_debug\n    my_extra_link_args_nonmath = extra_link_args_nonmath_debug\n    my_debug = True\n    print(""build configuration selected: debug"")\nelse:\n    raise ValueError(\n        ""Unknown build configuration \'%s\'; valid: \'optimized\', \'debug\'"" %\n        (build_type))\n\n\ndef declare_cython_extension(extName,\n                             use_math=False,\n                             use_openmp=False,\n                             include_dirs=None,\n                             extra_lib=None):\n    extPath = extName.replace(""."", os.path.sep) + "".pyx""\n    if use_math:\n        compile_args = list(my_extra_compile_args_math)  # copy\n        link_args = list(my_extra_link_args_math)\n        import platform\n        if platform.system() == ""Windows"":\n            libraries = None\n        else:\n            libraries = [""m""]\n    else:\n        compile_args = list(my_extra_compile_args_nonmath)\n        link_args = list(my_extra_link_args_nonmath)\n        libraries = None\n\n    if use_openmp:\n        compile_args.insert(0, openmp_compile_args)\n        link_args.insert(0, openmp_link_args)\n\n    if extra_lib is not None:\n        if libraries is None:\n            libraries = []\n        for lib in extra_lib:\n            libraries.append(lib)\n\n    return Extension(\n        extName, [extPath],\n        extra_compile_args=compile_args,\n        extra_link_args=link_args,\n        include_dirs=include_dirs,\n        libraries=libraries)\n\n\ndatafiles = []\n\n\ndef getext(filename):\n    return os.path.splitext(filename)[1]\n\n\nfor datadir in datadirs:\n    datafiles.extend(\n        [(root,\n          [os.path.join(root, f) for f in files if getext(f) in dataexts])\n         for root, dirs, files in os.walk(datadir)])\n\ndetected_docs = []\nfor docname in standard_docs:\n    for ext in standard_doc_exts:\n        filename = """".join((docname, ext))\n        if os.path.isfile(filename):\n            detected_docs.append(filename)\ndatafiles.append((\'.\', detected_docs))\n\ninit_py_path = os.path.join(libname, \'__init__.py\')\nversion = \'0.0.unknown\'\ntry:\n    with open(init_py_path) as f:\n        for line in f:\n            if line.startswith(\'__version__\'):\n                version = ast.parse(line).body[0].value.s\n                break\n        else:\n            print(\n                ""WARNING: Version information not found""\n                "" in \'%s\', using placeholder \'%s\'"" % (init_py_path, version),\n                file=sys.stderr)\nexcept FileNotFoundError:\n    print(\n        ""WARNING: Could not find file \'%s\',""\n        ""using placeholder version information \'%s\'"" % (init_py_path, version),\n        file=sys.stderr)\n\next_module_utils = declare_cython_extension(\n    ""wildboar._utils"",\n    use_math=True,\n    use_openmp=False,\n    include_dirs=my_include_dirs)\n\next_module_distance = declare_cython_extension(\n    ""wildboar._distance"",\n    use_math=True,\n    use_openmp=False,\n    include_dirs=my_include_dirs)\n\next_module_euclidean_distance = declare_cython_extension(\n    ""wildboar._euclidean_distance"",\n    use_math=True,\n    use_openmp=False,\n    include_dirs=my_include_dirs)\n\next_module_dtw_distance = declare_cython_extension(\n    ""wildboar._dtw_distance"",\n    use_math=True,\n    use_openmp=False,\n    include_dirs=my_include_dirs)\n\n# ext_module_mass_distance = declare_cython_extension(\n#     ""wildboar._mass_distance"",\n#     use_math=True,\n#     use_openmp=False,\n#     include_dirs=my_include_dirs,\n#     extra_lib=[""fftw3""])\n\next_module_impurity = declare_cython_extension(\n    ""wildboar._impurity"",\n    use_math=True,\n    use_openmp=False,\n    include_dirs=my_include_dirs)\n\next_module_tree_builder = declare_cython_extension(\n    ""wildboar._tree_builder"",\n    use_math=True,\n    use_openmp=False,\n    include_dirs=my_include_dirs)\n\next_module_distance_api = declare_cython_extension(\n    ""wildboar.distance"",\n    use_math=False,\n    use_openmp=False,\n    include_dirs=my_include_dirs)\n\ncython_ext_modules = [\n    ext_module_utils,\n    ext_module_distance,\n    ext_module_euclidean_distance,\n    ext_module_dtw_distance,\n    #    ext_module_mass_distance,\n    ext_module_distance_api,\n    ext_module_impurity,\n    ext_module_tree_builder,\n]\n\nsetup(\n    name=""wildboar"",\n    version=version,\n    author=""Isak Samsten"",\n    author_email=""isak@samsten.se"",\n    url=""https://github.com/isakkarlsson/wildboar"",\n    description=SHORTDESC,\n    long_description=DESC,\n    license=""GPLv3"",\n    #    platforms=[""Linux""],\n    classifiers=[\n        ""Development Status :: 4 - Beta"",\n        ""Environment :: Console"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)"",\n        ""Operating System :: POSIX :: Linux"",\n        ""Operating System :: Microsoft :: Windows"",\n        ""Operating System :: MacOS"",\n        ""Programming Language :: Cython"",\n        ""Programming Language :: Python"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.4"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Programming Language :: Python :: 3.8"",\n        ""Programming Language :: Python :: 3 :: Only"",\n        ""Programming Language :: Python :: Implementation :: CPython"",\n        ""Topic :: Scientific/Engineering"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n        ""Topic :: Software Development :: Libraries"",\n    ],\n    setup_requires=[\n        \'cython>=0.28\',\n        \'numpy>=1.14.2\',\n        \'setuptools>=18.0\',\n    ],\n    install_requires=[""scikit-learn""],\n    python_requires="">=3.4.0"",\n    provides=[""wildboar""],\n    keywords=[""machine learning"", ""time series distance""],\n    ext_modules=cython_ext_modules,\n    packages=[""wildboar""],\n    package_data={\n        \'wildboar\': [\'*.pxd\', \'*.pyx\', \'*.c\'],\n    },\n    zip_safe=False,\n    #    cmdclass={\'build_ext\': build_ext},\n\n    # Custom data files not inside a Python package\n    data_files=datafiles,\n)\n'"
examples/distance.py,2,"b'import numpy as np\n\nfrom wildboar.distance import distance\nfrom wildboar.distance import matches\n\nfrom scipy.stats import norm\n\nn_samples = 1000\nn_features = 10000\nn_classes = 2\n\nrng = np.random.RandomState(41)\nnp.zeros([10, 10])\n\ndelta = 0.5\ndt = 1\n\nX = (norm.rvs(\n    scale=delta**2 * dt,\n    size=n_samples * n_features,\n    random_state=rng,\n).reshape((n_samples, n_features)))\n\nx = X[0, :]\ndata = X[1:, :]\n\nd, i = distance(\n    x[0:10],\n    data,\n    dim=0,\n    metric=""scaled_dtw"",\n    metric_params={""r"": 3},\n    sample=None,\n    return_index=True,\n)\n\nd, i = matches(\n    x[0:10],\n    data,\n    0.37,\n    dim=0,\n    metric=""euclidean"",\n    sample=10,\n    return_distance=True,\n)\n\nprint(d)\nprint(i)\n'"
examples/example1.py,18,"b'import numpy as np\nimport time\n\nfrom sklearn.ensemble import BaggingClassifier\n# from sklearn.ensemble.weight_boosting import AdaBoostClassifier\n# from sklearn.model_selection import cross_val_score\n# from sklearn.tree import DecisionTreeClassifier\n\nfrom wildboar.tree import ShapeletTreeClassifier\nfrom wildboar.ensemble import ShapeletForestClassifier\nfrom wildboar._utils import print_tree\n\n\ndef testit():\n    train = np.loadtxt(""synthetic_control_TRAIN"")\n    test = np.loadtxt(""synthetic_control_TEST"")\n\n    y = train[:, 0].astype(np.intp)\n    x = train[:, 1:].astype(np.float64)\n    tree = ShapeletTreeClassifier(n_shapelets=100, max_depth=None)\n    tree.fit(x, y)\n    tree.score(test[:, 0], train[:, 1:])\n\n\nif __name__ == ""__main__"":\n\n    x = [\n        [0, 0, 1, 10, 1],\n        [0, 0, 1, 10, 1],\n        [0, 1, 9, 1, 0],\n        [1, 9, 1, 0, 0],\n        [0, 1, 9, 1, 0],\n        [0, 1, 2, 3, 4],\n        [1, 2, 3, 0, 0],\n        [0, 0, 0, 1, 2],\n        [0, 0, -1, 0, 1],\n        [1, 2, 3, 0, 1],\n    ]\n    x = np.array(x, dtype=np.float64)\n    x = np.hstack([x, x]).reshape(-1, 2, x.shape[-1])\n    y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n\n    random_state = np.random.RandomState(123)\n    order = np.arange(10)\n    random_state.shuffle(order)\n\n    x = x[order, :]\n    y = y[order]\n\n    print(x)\n    print(y)\n\n    tree = ShapeletTreeClassifier(random_state=10, metric=""scaled_dtw"")\n    tree.fit(x, y, sample_weight=np.ones(x.shape[0]) / x.shape[0])\n    print_tree(tree.root_node_)\n\n    print(""Score"")\n    print(tree.score(x, y))\n    print(""score_done"")\n\n    train = np.loadtxt(""data/synthetic_control_TRAIN"", delimiter="","")\n    test = np.loadtxt(""data/synthetic_control_TEST"", delimiter="","")\n\n    y = train[:, 0].astype(np.intp)\n    x = train[:, 1:].astype(np.float64)\n    i = np.arange(x.shape[0])\n\n    np.random.shuffle(i)\n\n    x_test = test[:, 1:].astype(np.float64)\n    y_test = test[:, 0].astype(np.intp)\n\n    f = ShapeletForestClassifier(\n        n_shapelets=1, metric=""scaled_dtw"", metric_params={""r"": 0.1})\n    f.fit(x, y)\n    c = time.time()\n    f.fit(x, y)\n    print(f.classes_)\n    print(""acc:"", f.score(x_test, y_test))\n    print(round(time.time() - c) * 1000)\n'"
examples/example2.py,4,"b'import numpy as np\nimport time\n\nfrom wildboar.ensemble import ShapeletForestClassifier\n\n# Example 2: multivariate shapelet forest\n\nx = [\n    # Example 1\n    [\n        [0, 0, 1, 10, 1],  # dimension 1\n        [0, 0, 1, 10, 1]\n    ],  # dimension 2 \n\n    # Example 2\n    [\n        [0, 1, 9, 1, 0],  # dimension 1\n        [1, 9, 1, 0, 0]\n    ],  # dimension 2 \n\n    # etc...\n    [[0, 1, 9, 1, 0], [0, 1, 2, 3, 4]],\n    [[1, 2, 3, 0, 0], [0, 0, 0, 1, 2]],\n    [[0, 0, -1, 0, 1], [1, 2, 3, 0, 1]],\n]\n\n# `x` is an array of shape `[5, 2, 5]`, i.e., 5 examples with 2\n# dimensions consisting of 5 timesteps\nx = np.array(x, dtype=np.float64)\nn_samples, n_dimensions, n_timesteps = x.shape\n\n# `y` is the output target\ny = np.array([0, 0, 1, 1, 0])\n\nrandom_state = np.random.RandomState(123)\norder = np.arange(n_samples)\nrandom_state.shuffle(order)\n\nx = x[order, :, :]\ny = y[order]\n\nf = ShapeletForestClassifier(random_state=random_state, n_shapelets=1)\nc = time.time()\nf.fit(x, y)\n\npredict = 1\nprint(""Predicting the class of example:"")\nprint(x[predict, :].reshape(-1, n_dimensions, n_timesteps))\nprint(""The true class is:"", y[predict], ""and the predicted class is:"",\n      f.predict(x[predict, :].reshape(1, n_dimensions, -1))[0])\n'"
examples/regression_test.py,8,"b'from sklearn.utils import check_random_state\n\nfrom wildboar.tree import ShapeletTreeRegressor\nimport numpy as np\n\ntrain = np.loadtxt(""data/synthetic_control_TRAIN"", delimiter="","")\ntest = np.loadtxt(""data/synthetic_control_TEST"", delimiter="","")\n\ntrain = np.vstack([train, test])\n\ny = train[:, 0].astype(np.float64)\nrandom_state = check_random_state(123)\n# for k in np.unique(y):\n#     y[y == k] = random_state.randn(y[y == k].shape[0]) + k\ny = np.ascontiguousarray(y)\nx = np.ascontiguousarray(train[:, 1:].astype(np.float64))\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(\n    x, y, test_size=0.3, random_state=random_state)\n\nrandom_state = check_random_state(123)\n# tree_builder = RegressionShapeletTreeBuilder(\n#     1,\n#     2,\n#     60,\n#     1000,\n#     DISTANCE_MEASURE[""euclidean""](150),\n#     x,\n#     y,\n#     None,\n#     random_state,\n# )\n\n# tree = tree_builder.build_tree()\n# print_tree(tree)\n\n# pred = RegressionShapeletTreePredictor(x, DISTANCE_MEASURE[""euclidean""](150))\n# print(np.linalg.norm(pred.predict(tree) - y))\n\nr = ShapeletTreeRegressor(\n    n_shapelets=1,\n    metric=""euclidean"",\n    max_depth=None,\n    min_samples_split=2,\n    random_state=check_random_state(123))\n\nfrom sklearn.ensemble import BaggingRegressor\nfrom wildboar.ensemble import ShapeletForestRegressor\n\nb = ShapeletForestRegressor(\n    random_state=check_random_state(123),\n    n_jobs=8,\n    bootstrap=False,\n    metric=""euclidean"",\n    metric_params={""r"": 0.2})\n\nb.fit(x_train, y_train)\nprint(b.predict(x_test))\nprint(b.score(x_test, y_test))\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nprint(\n    KNeighborsRegressor(n_neighbors=1).fit(x_train, y_train).score(\n        x_test, y_test))\n'"
tests/test_tree.py,2,"b'import unittest\nimport numpy as np\nimport itertools\n\nclass TestTree(unittest.TestCase):\n\n    def test_tree(self):\n        from pypf.tree import PfTree\n\n        x = np.random.randn(10, 10)\n        y = np.random.randint(0, 2, size=10)\n\n        tree = PfTree()\n        print(tree.fit(x, y).proba)\n'"
wildboar/__init__.py,0,"b'# This file is part of wildboar\n#\n# wildboar is free software: you can redistribute it and/or modify it\n# under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# wildboar is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see <http://www.gnu.org/licenses/>.\n\n# Authors: Isak Samsten\n\n__version__ = ""0.3.4""\n'"
wildboar/ensemble.py,20,"b'# This file is part of wildboar\n#\n# wildboar is free software: you can redistribute it and/or modify it\n# under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# wildboar is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see <http://www.gnu.org/licenses/>.\n\n# Authors: Isak Samsten\n\nimport numpy as np\n\nfrom wildboar.tree import ShapeletTreeClassifier\nfrom wildboar.tree import ShapeletTreeRegressor\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import BaggingRegressor\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import ClassifierMixin\nfrom sklearn.base import RegressorMixin\n\nfrom sklearn.utils import check_random_state\nfrom sklearn.utils import check_array\n\n\nclass ShapeletForestClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self,\n                 n_estimators=100,\n                 max_depth=None,\n                 min_samples_split=2,\n                 n_shapelets=10,\n                 min_shapelet_size=0,\n                 max_shapelet_size=1,\n                 metric=\'euclidean\',\n                 metric_params=None,\n                 bootstrap=True,\n                 n_jobs=None,\n                 random_state=None):\n        """"""A shapelet forest classifier\n        """"""\n        self.n_estimators = n_estimators\n        self.bootstrap = bootstrap\n        self.n_jobs = n_jobs\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.n_shapelets = n_shapelets\n        self.min_shapelet_size = min_shapelet_size\n        self.max_shapelet_size = max_shapelet_size\n        self.metric = metric\n        self.metric_params = metric_params\n        self.random_state = random_state\n\n    def predict(self, X, check_input=True):\n        return self.classes_[np.argmax(\n            self.predict_proba(X, check_input=check_input), axis=1)]\n\n    def predict_proba(self, X, check_input=True):\n        if X.ndim < 2 or X.ndim > 3:\n            raise ValueError(""illegal input dimensions X.ndim ({})"".format(\n                X.ndim))\n\n        if self.n_dims_ > 1 and X.ndim != 3:\n            raise ValueError(""illegal input dimensions X.ndim != 3"")\n\n        if X.shape[-1] != self.n_timestep_:\n            raise ValueError(""illegal input shape ({} != {})"".format(\n                X.shape[-1], self.n_timestep_))\n\n        if X.ndim > 2 and X.shape[1] != self.n_dims_:\n            raise ValueError(""illegal input shape ({} != {}"".format(\n                X.shape[1], self.n_dims))\n\n        if check_input:\n            X = check_array(X, dtype=np.float64, allow_nd=True, order=""C"")\n\n        if X.dtype != np.float64 or not X.flags.contiguous:\n            X = np.ascontiguousarray(X, dtype=np.float64)\n\n        X = X.reshape(X.shape[0], self.n_dims_ * self.n_timestep_)\n        return self.bagging_classifier_.predict_proba(X)\n\n    def fit(self, X, y, sample_weight=None, check_input=True):\n        """"""Fit a random shapelet forest classifier\n        """"""\n        random_state = check_random_state(self.random_state)\n        if check_input:\n            X = check_array(X, dtype=np.float64, allow_nd=True, order=""C"")\n            y = check_array(y, ensure_2d=False)\n\n        if X.ndim < 2 or X.ndim > 3:\n            raise ValueError(""illegal input dimension"")\n\n        n_samples = X.shape[0]\n        self.n_timestep_ = X.shape[-1]\n        if X.ndim > 2:\n            n_dims = X.shape[1]\n        else:\n            n_dims = 1\n\n        self.n_dims_ = n_dims\n\n        if y.ndim == 1:\n            self.classes_, y = np.unique(y, return_inverse=True)\n        else:\n            _, y = np.nonzero(y)\n            if len(y) != n_samples:\n                raise ValueError(""Single label per sample expected."")\n            self.classes_ = np.unique(y)\n\n        if len(y) != n_samples:\n            raise ValueError(""Number of labels={} does not match ""\n                             ""number of samples={}"".format(len(y), n_samples))\n\n        if X.dtype != np.float64 or not X.flags.contiguous:\n            X = np.ascontiguousarray(X, dtype=np.float64)\n\n        if not y.flags.contiguous:\n            y = np.ascontiguousarray(y, dtype=np.intp)\n\n        shapelet_tree_classifier = ShapeletTreeClassifier(\n            max_depth=self.max_depth,\n            min_samples_split=self.min_samples_split,\n            n_shapelets=self.n_shapelets,\n            min_shapelet_size=self.min_shapelet_size,\n            max_shapelet_size=self.max_shapelet_size,\n            metric=self.metric,\n            metric_params=self.metric_params,\n            random_state=random_state,\n        )\n\n        if n_dims > 1:\n            shapelet_tree_classifier.force_dim = n_dims\n\n        self.bagging_classifier_ = BaggingClassifier(\n            base_estimator=shapelet_tree_classifier,\n            bootstrap=self.bootstrap,\n            n_jobs=self.n_jobs,\n            n_estimators=self.n_estimators,\n            random_state=self.random_state,\n        )\n        X = X.reshape(n_samples, n_dims * self.n_timestep_)\n        self.bagging_classifier_.fit(X, y, sample_weight=sample_weight)\n        return self\n\n\nclass ShapeletForestRegressor(BaseEstimator, RegressorMixin):\n    def __init__(self,\n                 n_estimators=100,\n                 max_depth=None,\n                 min_samples_split=2,\n                 n_shapelets=10,\n                 min_shapelet_size=0,\n                 max_shapelet_size=1,\n                 metric=\'euclidean\',\n                 metric_params=None,\n                 bootstrap=True,\n                 n_jobs=None,\n                 random_state=None):\n        """"""A shapelet forest regressor\n        """"""\n        self.n_estimators = n_estimators\n        self.bootstrap = bootstrap\n        self.n_jobs = n_jobs\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.n_shapelets = n_shapelets\n        self.min_shapelet_size = min_shapelet_size\n        self.max_shapelet_size = max_shapelet_size\n        self.metric = metric\n        self.metric_params = metric_params\n        self.random_state = random_state\n\n    def predict(self, X, check_input=True):\n        if X.ndim < 2 or X.ndim > 3:\n            raise ValueError(""illegal input dimensions X.ndim ({})"".format(\n                X.ndim))\n\n        if self.n_dims_ > 1 and X.ndim != 3:\n            raise ValueError(""illegal input dimensions X.ndim != 3"")\n\n        if X.shape[-1] != self.n_timestep_:\n            raise ValueError(""illegal input shape ({} != {})"".format(\n                X.shape[-1], self.n_timestep_))\n\n        if X.ndim > 2 and X.shape[1] != self.n_dims_:\n            raise ValueError(""illegal input shape ({} != {}"".format(\n                X.shape[1], self.n_dims))\n\n        if check_input:\n            X = check_array(X, dtype=np.float64, allow_nd=True, order=""C"")\n\n        if X.dtype != np.float64 or not X.flags.contiguous:\n            X = np.ascontiguousarray(X, dtype=np.float64)\n\n        X = X.reshape(X.shape[0], self.n_dims_ * self.n_timestep_)\n        return self.bagging_regressor_.predict(X)\n\n    def fit(self, X, y, sample_weight=None, check_input=True):\n        """"""Fit a random shapelet forest regressor\n        """"""\n        random_state = check_random_state(self.random_state)\n        if check_input:\n            X = check_array(X, dtype=np.float64, allow_nd=True, order=""C"")\n            y = check_array(y, dtype=np.float64, ensure_2d=False, order=""C"")\n\n        if X.ndim < 2 or X.ndim > 3:\n            raise ValueError(""illegal input dimension"")\n\n        n_samples = X.shape[0]\n        self.n_timestep_ = X.shape[-1]\n        if X.ndim > 2:\n            n_dims = X.shape[1]\n        else:\n            n_dims = 1\n\n        self.n_dims_ = n_dims\n\n        if len(y) != n_samples:\n            raise ValueError(""Number of labels={} does not match ""\n                             ""number of samples={}"".format(len(y), n_samples))\n\n        if X.dtype != np.float64 or not X.flags.contiguous:\n            X = np.ascontiguousarray(X, dtype=np.float64)\n\n        if y.dtype != np.float64 or not y.flags.contiguous:\n            y = np.ascontiguousarray(y, dtype=np.float64)\n\n        shapelet_tree_regressor = ShapeletTreeRegressor(\n            max_depth=self.max_depth,\n            min_samples_split=self.min_samples_split,\n            n_shapelets=self.n_shapelets,\n            min_shapelet_size=self.min_shapelet_size,\n            max_shapelet_size=self.max_shapelet_size,\n            metric=self.metric,\n            metric_params=self.metric_params,\n            random_state=random_state,\n        )\n\n        if n_dims > 1:\n            shapelet_tree_regressor.force_dim = n_dims\n\n        self.bagging_regressor_ = BaggingRegressor(\n            base_estimator=shapelet_tree_regressor,\n            bootstrap=self.bootstrap,\n            n_jobs=self.n_jobs,\n            n_estimators=self.n_estimators,\n            random_state=self.random_state,\n        )\n        X = X.reshape(n_samples, n_dims * self.n_timestep_)\n        self.bagging_regressor_.fit(X, y, sample_weight=sample_weight)\n        return self\n'"
wildboar/tree.py,26,"b'# This file is part of wildboar\n#\n# wildboar is free software: you can redistribute it and/or modify it\n# under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# wildboar is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see <http://www.gnu.org/licenses/>.\n\n# Authors: Isak Samsten\n\nimport numpy as np\n\nfrom wildboar._tree_builder import ClassificationShapeletTreeBuilder\nfrom wildboar._tree_builder import ClassificationShapeletTreePredictor\nfrom wildboar._tree_builder import RegressionShapeletTreeBuilder\nfrom wildboar._tree_builder import RegressionShapeletTreePredictor\n\nfrom sklearn.base import ClassifierMixin, RegressorMixin\nfrom sklearn.base import BaseEstimator\nfrom sklearn.utils import check_random_state\nfrom sklearn.utils import check_array\n\nfrom wildboar.distance import DISTANCE_MEASURE\n\n__all__ = [""ShapeletTreeClassifier"",\n           ""ShapeletTreeRegressor""]\n\n\nclass ShapeletTreeRegressor(BaseEstimator, RegressorMixin):\n    """"""A shapelet tree classifier.""""""\n\n    def __init__(self,\n                 max_depth=None,\n                 min_samples_split=2,\n                 n_shapelets=10,\n                 min_shapelet_size=0,\n                 max_shapelet_size=1,\n                 metric=\'euclidean\',\n                 metric_params=None,\n                 force_dim=None,\n                 random_state=None):\n        """"""A shapelet decision tree regressor\n\n        :param max_depth: The maximum depth of the tree. If `None` the\n           tree is expanded until all leafs are pure or until all\n           leafs contain less than `min_samples_split` samples\n           (default: None).\n\n        :param min_samples_split: The minimum number of samples to\n           split an internal node (default: 2).\n\n        :param n_shapelets: The number of shapelets to sample at each\n           node (default: 10).\n\n        :param min_shapelet_size: The minimum length of a sampled\n           shapelet expressed as a fraction, computed as\n           `min(ceil(X.shape[-1] * min_shapelet_size), 2)` (default:\n           0).\n\n        :param max_shapelet_size: The maximum length of a sampled\n           shapelet, expressed as a fraction and computed as\n           `ceil(X.shape[-1] * max_shapelet_size)`.\n\n        :param metric: Distance metric used to identify the best\n           match. (default: `\'euclidean\'`)\n\n        :param metric_params: Paramters to the distace measure\n\n        :param force_dim: Force the number of dimensions (default:\n           None). If `int`, `force_dim` reshapes the input to the\n           shape `[n_samples, force_dim, -1]` to support the\n           `BaggingClassifier` interface.\n\n        :param random_state: If `int`, `random_state` is the seed used\n           by the random number generator; If `RandomState` instance,\n           `random_state` is the random number generator; If `None`,\n           the random number generator is the `RandomState` instance\n           used by `np.random`.\n\n        """"""\n        if min_shapelet_size < 0 or min_shapelet_size > max_shapelet_size:\n            raise ValueError(\n                ""`min_shapelet_size` {0} <= 0 or {0} > {1}"".format(\n                    min_shapelet_size, max_shapelet_size))\n        if max_shapelet_size > 1:\n            raise ValueError(\n                ""`max_shapelet_size` {0} > 1"".format(max_shapelet_size))\n\n        self.max_depth = max_depth\n        self.max_depth = max_depth or 2 ** 31\n        self.min_samples_split = min_samples_split\n        self.random_state = check_random_state(random_state)\n        self.n_shapelets = n_shapelets\n        self.min_shapelet_size = min_shapelet_size\n        self.max_shapelet_size = max_shapelet_size\n        self.metric = metric\n        self.metric_params = metric_params\n        self.force_dim = force_dim\n\n    def fit(self, X, y, sample_weight=None, check_input=True):\n        """"""Fit a shapelet tree regressor from the training set (X, y)\n\n        :param X: array-like, shape `[n_samples, n_timesteps]` or\n           `[n_samples, n_dimensions, n_timesteps]`. The training time\n           series.\n\n        :param y: array-like, `[n_samples]`. Target values are\n        floating point values.\n\n        :param sample_weight: If `None`, then samples are equally\n            weighted. Splits that would create child nodes with net\n            zero or negative weight are ignored while searching for a\n            split in each node. Splits are also ignored if they would\n            result in any single class carrying a negative weight in\n            either child node.\n\n        :param check_input: Allow to bypass several input checking.\n            Don\'t use this parameter unless you know what you do.\n\n        :returns: `self`\n\n        """"""\n        random_state = check_random_state(self.random_state)\n\n        if check_input:\n            X = check_array(X, dtype=np.float64, allow_nd=True, order=""C"")\n            y = check_array(y, dtype=np.float64, ensure_2d=False)\n\n        if X.ndim < 2 or X.ndim > 3:\n            raise ValueError(""illegal input dimensions"")\n\n        n_samples = X.shape[0]\n        if isinstance(self.force_dim, int):\n            X = np.reshape(X, [n_samples, self.force_dim, -1])\n\n        n_timesteps = X.shape[-1]\n\n        if X.ndim > 2:\n            n_dims = X.shape[1]\n        else:\n            n_dims = 1\n\n        if len(y) != n_samples:\n            raise ValueError(""Number of labels={} does not match ""\n                             ""number of samples={}"".format(len(y), n_samples))\n\n        if X.dtype != np.float64 or not X.flags.contiguous:\n            X = np.ascontiguousarray(X, dtype=np.float64)\n\n        if y.dtype != np.float64 or not y.flags.contiguous:\n            y = np.ascontiguousarray(y, dtype=np.float64)\n\n        metric_params = self.metric_params\n        if self.metric_params is None:\n            metric_params = {}\n\n        distance_measure = DISTANCE_MEASURE[self.metric](n_timesteps,\n                                                         **metric_params)\n\n        max_shapelet_size = int(n_timesteps * self.max_shapelet_size)\n        min_shapelet_size = int(n_timesteps * self.min_shapelet_size)\n\n        if min_shapelet_size < 2:\n            min_shapelet_size = 2\n\n        min_sample_split = self.min_samples_split\n\n        self.n_timestep_ = n_timesteps\n        self.n_dims_ = n_dims\n        tree_builder = RegressionShapeletTreeBuilder(\n            self.n_shapelets,\n            min_shapelet_size,\n            max_shapelet_size,\n            self.max_depth,\n            min_sample_split,\n            distance_measure,\n            X,\n            y,\n            sample_weight,\n            random_state,\n        )\n\n        self.root_node_ = tree_builder.build_tree()\n        return self\n\n    def predict(self, X, check_input=True):\n        """"""Predict the regression of the input samples X.\n\n        :param X: array-like, shape `[n_samples, n_timesteps]` or\n           `[n_samples, n_dimensions, n_timesteps]`. The input time\n           series.\n\n        :param check_input: Allow to bypass several input checking.\n            Don\'t use this parameter unless you know what you do.\n\n        :returns: array of `shape = [n_samples]`.\n\n        """"""\n        if X.ndim < 2 or X.ndim > 3:\n            raise ValueError(""illegal input dimensions X.ndim ({})"".format(\n                X.ndim))\n\n        if isinstance(self.force_dim, int):\n            X = np.reshape(X, [X.shape[0], self.force_dim, -1])\n\n        if X.shape[-1] != self.n_timestep_:\n            raise ValueError(""illegal input shape ({} != {})"".format(\n                X.shape[-1], self.n_timestep_))\n\n        if X.ndim > 2 and X.shape[1] != self.n_dims_:\n            raise ValueError(""illegal input shape ({} != {}"".format(\n                X.shape[1], self.n_dims))\n\n        if check_input:\n            X = check_array(X, dtype=np.float64, allow_nd=True, order=""C"")\n\n        if X.dtype != np.float64 or not X.flags.contiguous:\n            X = np.ascontiguousarray(X, dtype=np.float64)\n\n        metric_params = self.metric_params\n        if self.metric_params is None:\n            metric_params = {}\n\n        distance_measure = DISTANCE_MEASURE[self.metric](self.n_timestep_,\n                                                         **metric_params)\n\n        predictor = RegressionShapeletTreePredictor(X, distance_measure)\n        return predictor.predict(self.root_node_)\n\n\nclass ShapeletTreeClassifier(BaseEstimator, ClassifierMixin):\n    """"""A shapelet tree classifier.""""""\n\n    def __init__(self,\n                 max_depth=None,\n                 min_samples_split=2,\n                 n_shapelets=10,\n                 min_shapelet_size=0,\n                 max_shapelet_size=1,\n                 metric=\'euclidean\',\n                 metric_params=None,\n                 force_dim=None,\n                 random_state=None):\n        """"""A shapelet decision tree\n\n        :param max_depth: The maximum depth of the tree. If `None` the\n           tree is expanded until all leafs are pure or until all\n           leafs contain less than `min_samples_split` samples\n           (default: None).\n\n        :param min_samples_split: The minimum number of samples to\n           split an internal node (default: 2).\n\n        :param n_shapelets: The number of shapelets to sample at each\n           node (default: 10).\n\n        :param min_shapelet_size: The minimum length of a sampled\n           shapelet expressed as a fraction, computed as\n           `min(ceil(X.shape[-1] * min_shapelet_size), 2)` (default:\n           0).\n\n        :param max_shapelet_size: The maximum length of a sampled\n           shapelet, expressed as a fraction and computed as\n           `ceil(X.shape[-1] * max_shapelet_size)`.\n\n        :param metric: Distance metric used to identify the best\n           match. (default: `\'euclidean\'`)\n\n        :param metric_params: Paramters to the distace measure\n\n        :param force_dim: Force the number of dimensions (default:\n           None). If `int`, `force_dim` reshapes the input to the\n           shape `[n_samples, force_dim, -1]` to support the\n           `BaggingClassifier` interface.\n\n        :param random_state: If `int`, `random_state` is the seed used\n           by the random number generator; If `RandomState` instance,\n           `random_state` is the random number generator; If `None`,\n           the random number generator is the `RandomState` instance\n           used by `np.random`.\n\n        """"""\n        if min_shapelet_size < 0 or min_shapelet_size > max_shapelet_size:\n            raise ValueError(\n                ""`min_shapelet_size` {0} <= 0 or {0} > {1}"".format(\n                    min_shapelet_size, max_shapelet_size))\n        if max_shapelet_size > 1:\n            raise ValueError(\n                ""`max_shapelet_size` {0} > 1"".format(max_shapelet_size))\n\n        self.max_depth = max_depth\n        self.max_depth = max_depth or 2 ** 31\n        self.min_samples_split = min_samples_split\n        self.random_state = check_random_state(random_state)\n        self.n_shapelets = n_shapelets\n        self.min_shapelet_size = min_shapelet_size\n        self.max_shapelet_size = max_shapelet_size\n        self.metric = metric\n        self.metric_params = metric_params\n        self.force_dim = force_dim\n\n    def fit(self, X, y, sample_weight=None, check_input=True):\n        """"""Fit a shapelet tree classifier from the training set (X, y)\n\n        :param X: array-like, shape `[n_samples, n_timesteps]` or\n           `[n_samples, n_dimensions, n_timesteps]`. The training time\n           series.\n\n        :param y: array-like, shape `[n_samples, n_classes]` or\n           `[n_classes]`. Target values (class labels) as integers or\n           strings.\n\n        :param sample_weight: If `None`, then samples are equally\n            weighted. Splits that would create child nodes with net\n            zero or negative weight are ignored while searching for a\n            split in each node. Splits are also ignored if they would\n            result in any single class carrying a negative weight in\n            either child node.\n\n        :param check_input: Allow to bypass several input checking.\n            Don\'t use this parameter unless you know what you do.\n\n        :returns: `self`\n\n        """"""\n        random_state = check_random_state(self.random_state)\n\n        if check_input:\n            X = check_array(X, dtype=np.float64, allow_nd=True, order=""C"")\n            y = check_array(y, ensure_2d=False)\n\n        if X.ndim < 2 or X.ndim > 3:\n            raise ValueError(""illegal input dimensions"")\n\n        n_samples = X.shape[0]\n        if isinstance(self.force_dim, int):\n            X = np.reshape(X, [n_samples, self.force_dim, -1])\n\n        n_timesteps = X.shape[-1]\n\n        if X.ndim > 2:\n            n_dims = X.shape[1]\n        else:\n            n_dims = 1\n\n        if y.ndim == 1:\n            self.classes_, y = np.unique(y, return_inverse=True)\n        else:\n            _, y = np.nonzero(y)\n            if len(y) != n_samples:\n                raise ValueError(""Single label per sample expected."")\n            self.classes_ = np.unique(y)\n\n        if len(y) != n_samples:\n            raise ValueError(""Number of labels={} does not match ""\n                             ""number of samples={}"".format(len(y), n_samples))\n\n        if X.dtype != np.float64 or not X.flags.contiguous:\n            X = np.ascontiguousarray(X, dtype=np.float64)\n\n        if not y.flags.contiguous:\n            y = np.ascontiguousarray(y, dtype=np.intp)\n\n        metric_params = self.metric_params\n        if self.metric_params is None:\n            metric_params = {}\n\n        distance_measure = DISTANCE_MEASURE[self.metric](n_timesteps,\n                                                         **metric_params)\n\n        max_shapelet_size = int(n_timesteps * self.max_shapelet_size)\n        min_shapelet_size = int(n_timesteps * self.min_shapelet_size)\n\n        if min_shapelet_size < 2:\n            min_shapelet_size = 2\n        min_sample_split = self.min_samples_split\n        self.n_classes_ = len(self.classes_)\n        self.n_timestep_ = n_timesteps\n        self.n_dims_ = n_dims\n\n        tree_builder = ClassificationShapeletTreeBuilder(\n            self.n_shapelets,\n            min_shapelet_size,\n            max_shapelet_size,\n            self.max_depth,\n            min_sample_split,\n            distance_measure,\n            X,\n            y,\n            sample_weight,\n            random_state,\n            self.n_classes_,\n        )\n\n        self.root_node_ = tree_builder.build_tree()\n        return self\n\n    def predict(self, X, check_input=True):\n        """"""Predict the class for X\n\n        :param X: array-like, shape `[n_samples, n_timesteps]` or\n            `[n_samples, n_dimensions, n_timesteps]`. The input time\n            series.\n\n        :param check_input: Allow to bypass several input checking.\n            Don\'t use this parameter unless you know what you do.\n\n        :returns: array of `shape = [n_samples]`. The predicted\n            classes\n\n        """"""\n        return self.classes_[np.argmax(\n            self.predict_proba(X, check_input=check_input), axis=1)]\n\n    def predict_proba(self, X, check_input=True):\n        """"""Predict class probabilities of the input samples X.  The predicted\n        class probability is the fraction of samples of the same class\n        in a leaf.\n\n        :param X: array-like, shape `[n_samples, n_timesteps]` or\n           `[n_samples, n_dimensions, n_timesteps]`. The input time\n           series.\n\n        :param check_input: Allow to bypass several input checking.\n            Don\'t use this parameter unless you know what you do.\n\n        :returns: array of `shape = [n_samples, n_classes]`. The\n            class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute `classes_`.\n        """"""\n        if X.ndim < 2 or X.ndim > 3:\n            raise ValueError(""illegal input dimensions X.ndim ({})"".format(\n                X.ndim))\n\n        if isinstance(self.force_dim, int):\n            X = np.reshape(X, [X.shape[0], self.force_dim, -1])\n\n        if X.shape[-1] != self.n_timestep_:\n            raise ValueError(""illegal input shape ({} != {})"".format(\n                X.shape[-1], self.n_timestep_))\n\n        if X.ndim > 2 and X.shape[1] != self.n_dims_:\n            raise ValueError(""illegal input shape ({} != {}"".format(\n                X.shape[1], self.n_dims))\n\n        if check_input:\n            X = check_array(X, dtype=np.float64, allow_nd=True, order=""C"")\n\n        if X.dtype != np.float64 or not X.flags.contiguous:\n            X = np.ascontiguousarray(X, dtype=np.float64)\n\n        metric_params = self.metric_params\n        if self.metric_params is None:\n            metric_params = {}\n\n        distance_measure = DISTANCE_MEASURE[self.metric](self.n_timestep_,\n                                                         **metric_params)\n\n        predictor = ClassificationShapeletTreePredictor(\n            X, distance_measure, len(self.classes_))\n        return predictor.predict(self.root_node_)\n'"
