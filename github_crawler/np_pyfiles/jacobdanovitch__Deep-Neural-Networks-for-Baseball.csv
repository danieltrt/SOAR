file_path,api_count,code
FIP/FIP-DNN.py,8,"b'import numpy as np\nimport pandas as pd\n\n\nclass Layer:\n    def __init__(self, position, max_position, alpha):\n        """"""\n        :param position: Layer\'s position in the overall network\n        :param max_position: Network\'s width\n        :param alpha: Learning rate\n        """"""\n        self.position = position\n        self.max_position = max_position\n        self.alpha = alpha\n\n        self.weights = []  # weights of layer\n        self.prediction = None  # output of layer\n        self.delta = None  # delta for backprop weight adjustment\n        self.cost = None  # cost (only used for final output layer)\n\n        print(""Layer"",position,""initialized.\\n"")\n\n    def generate_weights(self, size):\n        self.weights = np.random.random(size)  # randomly initialize weights given tuple\n        print(""Layer"", self.position,""weights:"",self.weights,\'\\n \')  # print for confirmation\n\n    def get_weights(self):\n        return self.weights\n\n    def update_weights(self, delta):\n        self.weights -= self.alpha * delta\n\n    def relu(self, x, deriv=False):\n        if not deriv:\n            return x * (x > 0)  # Sets output to 0 if input is negative\n        else:\n            return x > 0  # Output is 1 if x is positive\n\n    def predict(self, x, return_pred=False):\n        if self.position == self.max_position:\n            # Output layer prediction; no relu activation\n            self.prediction = np.dot(x, self.weights)\n        else:\n            # Hidden layer predictions; use relu activation\n            self.prediction = self.relu(np.dot(x, self.weights))\n\n        if return_pred:\n            return self.prediction\n\n    def get_prediction(self):\n        return self.prediction\n\n    def calculate_cost(self, prediction, values):\n        m = len(prediction)\n        err = np.square(prediction - values).sum()\n        return err / (2 * m)\n\n    def get_cost(self):\n        return self.cost\n\n    def get_delta(self):\n        return self.delta\n\n    def calculate_delta(self, y=None, prev_layer=None):\n        if self.position == self.max_position:\n            # only for final output layer\n            self.delta = self.prediction - y  # subtracts y/truth vector from prediction vector\n            self.cost = self.calculate_cost(self.prediction, y.values)  # adds cost\n        else:\n            """"""\n            delta calculation for backpropagating through hidden layers\n            \n            dot product of last layer\'s delta and transposed weights, \n            multiplied by relu derivative to freeze negative weights \n            """"""\n            self.delta = np.dot(prev_layer.get_delta(), prev_layer.get_weights().transpose()) * self.relu(self.prediction,\n                                                                                                          deriv=True)\n\n\nclass NeuralNetwork:\n    def __init__(self, num_layers, x, y, max_iter=1000, alpha=0.01, random_seed=1234):\n        \'\'\'\n        HYPERPARAMS\n        :param num_layers: total layers in network\n        :param max_iter: iterations for gradient descent\n        :param alpha: learning rate (passed to layers)\n        :param random_seed: random seed for weight generation (Change this if your network sucks!!)\n        \'\'\'\n\n        self.alpha = alpha\n\n        self.num_layers = num_layers\n        self.layers = []  # list containing all Layer objects\n\n        self.max_iter = max_iter\n        self.random_seed = random_seed\n\n        \'\'\'\n        DATA\n        :param x: input vector/matrix\n        :param features: shape of input / # of features\n        :param y: ground truth vector/matrix\n        \'\'\'\n\n        self.x = x\n        self.features = len(x.iloc[0])\n        self.y = y\n\n    def init_layers(self):\n        np.random.seed(self.random_seed)  # for deterministic weight generation\n        max_position = self.num_layers - 1  # width of network\n\n        # null initialize layers\n        self.layers = [Layer(i, max_position, alpha=self.alpha) for i in range(self.num_layers)]\n\n        # Generate weights for first layer (will always be as wide as # of features)\n        self.layers[0].generate_weights((self.features, self.num_layers))\n\n        # If user specifies for more hidden layers\n        if self.num_layers > 1:\n            # Generate weights for middle hidden layers (won\'t run for num_layers == 2)\n            for layer in self.layers[1:max_position]:\n                # Need to make number of neurons editable\n                layer.generate_weights((self.num_layers, self.num_layers))\n\n            # Generate weight for output layer (should have single output for regression)\n            self.layers[-1].generate_weights((self.num_layers, 1))\n\n    def forward_prop(self):\n        pred = self.layers[0].predict(self.x, return_pred=True) # initial prediction from input vector\n        for l, layer in enumerate(self.layers[1:]):\n            # update prediction iterating through rest of layers, chaining prediction\n            pred = layer.predict(pred, return_pred=True)\n\n    def back_prop(self):\n        for l, layer in reversed(list(enumerate(self.layers))):\n            # for final output layer\n            if l == len(self.layers) - 1:\n                # calculate initial delta for backprop\n                layer.calculate_delta(y=self.y)\n            # for rest of backprop\n            else:\n                # use previous layer to backprop and calculate delta\n                layer.calculate_delta(prev_layer=self.layers[l + 1])\n\n    def update_weights(self):\n        for l, layer in enumerate(self.layers):\n            # weight update for first layer (multiplied by alpha inside function)\n            if l == 0:\n                # input vector transposed, dotted with input vector\'s delta\n                layer.update_weights(np.dot(self.x.transpose(), layer.get_delta()))\n            else:\n                # uses previous layer to update weights\n                prev_layer = self.layers[l - 1]\n                layer.update_weights(np.dot(prev_layer.get_prediction().transpose(), layer.get_delta()))\n\n    def train(self):\n        # initialize layers\n        self.init_layers()\n\n        for epoch in range(1,self.max_iter+1):\n            # forward, back, update\n            self.forward_prop()\n            self.back_prop()\n            self.update_weights()\n\n            # print every thousand\n            if epoch % 1000 == 0:\n                self.print_layers(epoch)\n\n    def print_layers(self,epoch=-1):\n        if epoch >= 0:\n            print(""Epoch"",epoch,""complete."")\n\n        for l, layer in enumerate(self.layers):\n            print(""Layer"",l)\n            print(layer.get_weights())\n            print()\n\n        print(""Cost:"", self.layers[-1].get_cost())\n\n    def fit(self, x):\n        # this is broke ngl\n        pred = self.layers[0].predict(x)\n        for l, layer in enumerate(self.layers[1:]):\n            pred = layer.predict(pred, return_pred=True)\n        return pred\n\n\ndef main():\n    """"""\n    This is just project-specific stuff\n    """"""\n    data = pd.read_csv(\'TestFIPData.csv\')\n    train_data = data.sample(frac=0.7)\n    test_data = data.loc[~data.index.isin(train_data.index)]\n\n    x_train = train_data[[\'K9\', \'BB9\', \'HR9\']]\n    x_test = test_data[[\'K9\', \'BB9\', \'HR9\']]\n\n    y_train = train_data[[\'ERA\']]\n    y_test = test_data[[\'ERA\']]\n\n    neural_net = NeuralNetwork(3, x_train, y_train, max_iter=20000, alpha=0.0000001)\n    neural_net.train()\n\n    \'\'\'\n    this doesn\'t work yet\n    predictions = neural_net.fit(x_test)\n    print(predictions)\n    \'\'\'\n\n    \'\'\'\n    boiler plate code for saving to xlsx\n    \n    test_df = pd.concat([x_test, y_test], axis=1)\n    test_df.loc[:,\'gdFIP\'] = predict(test_df.iloc[:,:3], weights)\n    print(test_df.head(10))\n    \n\n    data.loc[:,\'nnERA\'] = neural_network(x_train.values, y_train.values, 4, verbose=True,\n                                                 alpha=0.0000001, predict_data=data[[\'K9\', \'BB9\', \'HR9\']])\n\n    writer = pd.ExcelWriter(\'nnERA.xlsx\')\n    data.to_excel(writer,\'nnERA\')\n    writer.save()\n    \'\'\'\n\n\n# execute\nmain()\n'"
FIP/FIPGradientDescent.py,7,"b'import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef predict(x, weights):\n    return np.dot(x, weights)\n\ndef cost(prediction, values):\n    m = len(prediction)\n    err = np.square(prediction - values).sum()\n    return err / (2*m)\n\ndef gradient_descent(x, y, alpha=0.0001, max_iter=10000, random_seed=1234, verbose=False):\n    np.random.seed(random_seed)\n    weights = np.random.random((len(x[0]), 1))\n\n    m = len(y)\n    cost_history = []\n\n    for i in range(max_iter):\n        pred = predict(x, weights)\n        delta = pred - y\n\n        weight_deltas = np.dot(x.transpose(), delta)\n        weights -= alpha*weight_deltas\n\n        \'\'\'\n        for i in range(len(x)):\n            pred = np.dot(x[i], weights)\n            delta = (pred - y[i])\n            weight_deltas = alpha*np.array([delta*input for input in x[i]])\n            weights = weights - weight_deltas\n        \'\'\'\n\n        cost_history.append(cost(pred, y))\n\n        if i % 100 == 0 and verbose:\n            print(""Iteration"", i)\n            print(""Cost:"", cost_history[-1],end=""\\n \\n"")\n\n    print(""Iteration"", max_iter)\n    print(""Cost:"", cost_history[-1])\n\n    return [i[0] for i in weights]\n\ndata = pd.read_csv(\'TestFIPData.csv\')\ntrain_data = data.sample(frac=0.7)\ntest_data = data.loc[~data.index.isin(train_data.index)]\n\nx_train = train_data[[\'K9\', \'BB9\', \'HR9\']]\nx_test = test_data[[\'K9\', \'BB9\', \'HR9\']]\n\ny_train = train_data[[\'ERA\']]\ny_test = test_data[[\'ERA\']]\n\nweights = gradient_descent(x_train.values, y_train.values, alpha=0.000001)\n\n\'\'\'\ntest_df = pd.concat([x_test, y_test], axis=1)\ntest_df.loc[:,\'gdFIP\'] = predict(test_df.iloc[:,:3], weights)\nprint(test_df.head(10))\n\'\'\'\n\ndata.loc[:,\'gdFIP\'] = predict(data[[\'K9\', \'BB9\', \'HR9\']], weights)\n\nwriter = pd.ExcelWriter(\'validation_rows.xlsx\')\ndata.to_excel(writer,\'gdFIP\')\nwriter.save()\n\n\n'"
FIP/FIPNN.py,7,"b'import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pprint\n\ndef predict(x, weights):\n    return np.dot(x, weights)\n\ndef cost(prediction, values):\n    m = len(prediction)\n    err = np.square(prediction - values).sum()\n    return err / (2*m)\n\ndef relu(x, deriv=False):\n    if not deriv:\n        return x * (x > 0)\n    else:\n        return x>0\n\ndef generate_weights(size):\n    return np.random.random(size)\n\ndef neural_network(x, y, hidden_layers, alpha=0.0001, max_iter=10000, verbose=False, predict_data=[], random_seed=1234):\n    np.random.seed(random_seed)\n    layer0_weights = generate_weights((len(x[0]), hidden_layers)) #layer 0 -> 1\n    layer1_weights = generate_weights((hidden_layers,1)) #layer 1 -> 2\n\n    m = len(y)\n    cost_history = []\n\n    for i in range(max_iter):\n        layer0_output = relu(predict(x, layer0_weights))\n        layer1_output = predict(layer0_output, layer1_weights)\n\n        layer1_deltas = layer1_output - y\n        layer0_deltas = np.dot(layer1_deltas, layer1_weights.transpose()) * relu(layer0_output, deriv=True)\n\n        layer1_weights -= alpha * np.dot(layer0_output.transpose(), layer1_deltas)\n        layer0_weights -= alpha * np.dot(x.transpose(), layer0_deltas)\n\n        cost_history.append(cost(layer1_output, y))\n\n        if i % 10**(len(str(max_iter))-2) == 0 and (i == 0 or i >= 10**(len(str(max_iter))-2)) and verbose:\n            print(""Iteration"", i)\n            print(""Cost:"", cost_history[-1],end=""\\n \\n"")\n\n    print(""Iteration"", max_iter)\n    print(""Cost:"", cost_history[-1])\n\n    if len(predict_data) > 0:\n        layer0_output = relu(predict(predict_data, layer0_weights))\n        layer1_output = predict(layer0_output, layer1_weights)\n        return layer1_output\n\n    model = {\n        \'L0\':layer0_weights.tolist(),\n        \'L1\':layer1_weights.tolist()\n    }\n\n    if verbose:\n        pprint.pprint(model)\n\n    return model\n\n\ndef main():\n    data = pd.read_csv(\'TestFIPData.csv\')\n    train_data = data.sample(frac=0.7)\n    test_data = data.loc[~data.index.isin(train_data.index)]\n\n    x_train = train_data[[\'K9\', \'BB9\', \'HR9\']]\n    x_test = test_data[[\'K9\', \'BB9\', \'HR9\']]\n\n    y_train = train_data[[\'ERA\']]\n    y_test = test_data[[\'ERA\']]\n\n    \'\'\'\n    test_df = pd.concat([x_test, y_test], axis=1)\n    test_df.loc[:,\'gdFIP\'] = predict(test_df.iloc[:,:3], weights)\n    print(test_df.head(10))\n    \'\'\'\n\n    data.loc[:,\'nnERA\'] = neural_network(x_train.values, y_train.values, 4, verbose=True,\n                                                 alpha=0.0000001, predict_data=data[[\'K9\', \'BB9\', \'HR9\']])\n\n    writer = pd.ExcelWriter(\'nnERA.xlsx\')\n    data.to_excel(writer,\'nnERA\')\n    writer.save()\n\nmain()\n\n\n\n'"
