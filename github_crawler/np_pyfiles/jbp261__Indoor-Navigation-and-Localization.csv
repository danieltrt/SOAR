file_path,api_count,code
QNetwork.py,0,"b'from keras import layers, models, optimizers\nfrom keras.utils import plot_model\nfrom keras import backend as K\n\nclass QNetwork:\n    """"""QNetwork Model.""""""\n\n    def __init__(self, state_size, action_size):\n        """"""Initialize parameters and build model.\n\n        Params\n        ======\n            state_size (int): Dimension of each state\n            action_size (int): Dimension of each action\n        """"""\n        self.state_size = state_size\n        self.action_size = action_size\n\n        self.build_model()\n\n    def build_model(self):\n        """"""Build a Q network that maps (state, action) pairs -> Q-values.""""""\n        # Define input layers\n        states = layers.Input(shape=(self.state_size,), name=\'states\')\n        actions = layers.Input(shape=(self.action_size,), name=\'actions\')\n\n        # Add hidden layer(s) for state pathway\n        net_states = layers.Dense(units=128, activation=\'relu\')(states)\n        net_states = layers.Dense(units=128, activation=\'relu\')(net_states)\n\n        # Add hidden layer(s) for action pathway\n        net_actions = layers.Dense(units=128, activation=\'relu\')(actions)\n        net_actions = layers.Dense(units=128, activation=\'relu\')(net_actions)\n\n        # Combine state and action pathways\n        net = layers.Add()([net_states, net_actions])\n        net = layers.Activation(\'relu\')(net)\n\n        # Add final output layer to prduce action values (Q values)\n        Q_values = layers.Dense(units=1, name=\'q_values\')(net)\n\n        # Create Keras model\n        self.model = models.Model(inputs=[states, actions], outputs=Q_values)\n        #plot_model(QNetwork, to_file=\'QNetwork.png\', show_shapes=True)\n\n        # Define optimizer and compile model for training with built-in loss function\n        self.model.compile(optimizer=\'adam\', loss=\'mse\')'"
ReplayBuffer.py,0,"b'import random\nfrom collections import namedtuple, deque\n\nclass ReplayBuffer:\n    """"""Fixed-size buffer to store experience tuples.""""""\n\n    def __init__(self, buffer_size, batch_size):\n        """"""Initialize a ReplayBuffer object.\n        Params\n        ======\n            buffer_size: maximum size of buffer\n            batch_size: size of each training batch\n        """"""\n        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n        self.batch_size = batch_size\n        self.experience = namedtuple(""Experience"", field_names=[""state"", ""action"", ""reward"", ""next_state"", ""done""])\n\n    def add(self, state, action, reward, next_state, done):\n        """"""Add a new experience to memory.""""""\n        e = self.experience(state, action, reward, next_state, done)\n        self.memory.append(e)\n\n    def sample(self, batch_size=64):\n        """"""Randomly sample a batch of experiences from memory.""""""\n        return random.sample(self.memory, k=self.batch_size)\n\n    def __len__(self):\n        """"""Return the current size of internal memory.""""""\n        return len(self.memory)'"
VAE.py,0,"b'import keras\nfrom keras import layers, models, optimizers\nfrom keras import metrics\nfrom keras.utils import plot_model\nimport pydot\n#import graphviz \nfrom keras import backend as K \n\nclass VAE:\n    \n    #""""""VAE (Variational Auto Encoder) Model""""""\n    \n    \n    #""""""Initialize parameters and build model.\n\n     # Arguments:\n    #    input_size (int): Dimension of each state\n    #""""""\n    def __init__ (self, input_size):\n        \n        self.input_size = input_size\n        self.stddev = 1.0\n        self.vae_trained = False\n        self.build_model()\n         \n    def build_model(self):\n        #""""""Build VAE network such that VAE model = encoder + decoder.""""""\n        \n        # Define input layer & build encoder model\n  \n        self.inputs = layers.Input(shape=(self.input_size, ), name=\'encoder_input\')\n        \n        self.encoder_model()\n        self.decoder_model()\n        \n        # instantiate VAE model\n        outputs = self.decoder(self.encoder(self.inputs)[2])\n        self.vae = models.Model(self.inputs, outputs, name=\'vae\')\n        #plot_model(self.vae, to_file=\'images/vae_model.png\', show_shapes=True)\n                \n   \n    def sampling(self,args):\n        #""""""Reparameterization by sampling from an isotropic unit Gaussian.\n        # Arguments:\n        #    args (tensor): mean and log of variance of Q(z|X)\n        # Returns:\n        #    z (tensor): sampled latent vector\n        #""""""\n        z_mean, z_log_var = args\n        batch = K.shape(z_mean)[0]\n        dim = K.int_shape(z_mean)[1]\n        #random_normal: mean=0 and std=1.0\n        #instead of sampling from Q(z|X), sample eps = N(0,I)\n        epsilon = K.random_normal(shape=(batch, dim),stddev = self.stddev)\n        # z = z_mean + sqrt(var)*eps\n        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\n    def encoder_model(self):\n        #""""""Build an encoder model with dense layers""""""\n        # Add hidden layers\n       \n        x1 = layers.Dense(32, kernel_initializer=\'random_normal\', activation=\'relu\',name=\'dense1\')(self.inputs)\n        x1 = layers.Dense(64, kernel_initializer=\'random_normal\', activation=\'relu\',name=\'dense2\')(x1)\n        x1 = layers.Dense(64, kernel_initializer=\'random_normal\', activation=\'relu\',name=\'dense3\')(x1)\n        \n        # Mean and log_variation layers\n   \n        self.z_mean = layers.Dense(2, kernel_initializer=\'random_normal\', name=\'z_mean\')(x1)\n        self.z_log_var = layers.Dense(2, kernel_initializer=\'random_normal\', name=\'z_log_var\')(x1)\n\n        # use reparameterization trick to push the sampling out as input\n        # note that ""output_shape"" isn\'t necessary with the TensorFlow backend\n        self.z = layers.Lambda(self.sampling, output_shape=(2,), name=\'z\')([self.z_mean, self.z_log_var])\n\n        # instantiate encoder model\n        self.encoder = models.Model(self.inputs, [self.z_mean, self.z_log_var, self.z], name=\'encoder\')\n        #plot_model(self.encoder, to_file=\'images/vae_encoder.png\', show_shapes=True)\n        \n    def decoder_model(self):\n        #""""""Build a decoder model with dense layers""""""\n        # build decoder model\n        latent_inputs = layers.Input(shape=(2,), name=\'z_sampling\')\n        x2 = layers.Dense(64, kernel_initializer=\'random_normal\', activation=\'relu\',name=\'dense4\')(latent_inputs)\n        x2 = layers.Dense(64, kernel_initializer=\'random_normal\', activation=\'relu\',name=\'dense5\')(x2)\n        x2 = layers.Dense(32, kernel_initializer=\'random_normal\', activation=\'relu\',name=\'dense6\')(x2)\n        outputs = layers.Dense(self.input_size, kernel_initializer=\'random_normal\', activation=\'sigmoid\')(x2)\n\n        # instantiate decoder model\n        self.decoder = models.Model(latent_inputs, outputs, name=\'decoder\')\n        #plot_model(self.decoder, to_file=\'images/vae_decoder.png\', show_shapes=True)\n        \n    def train_off(self):\n        #""""""turn all the layers of the model to not-trainable""""""\n        self.vae.trainable = False\n   \n    def loss_vae(self,y_true, y_pred):\n        #""""""Custom loss function for VAE model based on KL divergence principle""""""\n        xent_loss = keras.metrics.mse(y_true, y_pred)\n        # KL divergence\n        kl_loss = -5e-4 * K.mean(1 + self.z_log_var - K.square(self.z_mean) - K.exp(self.z_log_var), axis=-1)\n        \n        loss = K.mean(xent_loss + kl_loss)\n        \n        return loss\n        '"
VAE_action.py,0,"b'import keras\nfrom keras import layers, models, optimizers\nfrom keras import metrics\nfrom keras.utils import plot_model\nfrom keras import backend as K\nimport pydot\n#import graphviz \nfrom VAE import VAE\n\nclass VAE_action:\n    #""""""VAE Action Model to direct the agent for an appropriate action given input""""""\n\n    def __init__(self, state_size, action_size, action_categories):\n        #""""""Initialize parameters and build model.\n\n        #Argumets:\n        #    state_size (int): Dimension of each state\n        #    action_size (int): Dimension of each action\n        #""""""\n        self.state_size = state_size\n        self.action_size = action_size\n        self.action_categories = action_categories\n        self.vae_model = VAE(self.state_size)\n        self.lr = 0.001\n        #vae_model.vae.load_weights(\'model_weights/weights.vae.h5\')\n        #vae_model.vae.trainable = False\n        #vae_model.encoder.trainable = False\n        #vae_model.decoder.trainable = False\n        self.build_model()\n\n    def build_model(self):\n        #""""""Build VAE network such that VAE model = encoder + decoder.""""""\n        #l = self.vae_model.vae.output\n        #input layer\n        inputs = layers.Input(shape=(self.state_size, ), name=\'action_input\')\n        #dense layers\n        l = layers.Dense(64, kernel_initializer=\'random_normal\', activation=""relu"")(inputs)\n        l = layers.Dense(128, kernel_initializer=\'random_normal\', activation=""relu"")(l)\n        l = layers.Dense(128, kernel_initializer=\'random_normal\', activation=""relu"")(l)\n        #output layer\n        actions = layers.Dense(self.action_categories, kernel_initializer=\'random_normal\', activation=\'linear\', name=\'actions\')(l)\n        \n        #model creation\n        self.model = models.Model(inputs, actions, name=\'action_model\')\n        plot_model(self.model, to_file=\'images/vae_action_model.png\', show_shapes=True)\n        \n        #model compilation\n        self.model.compile(loss=\'mse\',optimizer=optimizers.Adam(lr=self.lr))\n        '"
VAE_old.py,0,"b'import keras\nfrom keras import layers, models, optimizers\nfrom keras import metrics\nfrom keras.utils import plot_model\nfrom keras import backend as K \n\nclass VAE:\n    \n    """"""VAE Model.""""""\n    """"""Initialize parameters and build model.\n\n    Params\n    ======\n        input_size (int): Dimension of each state\n        action_size (int): Dimension of each action\n    """"""\n    def __init__ (self, input_size, output_size):\n        \n        self.input_size = input_size\n        self.action_size = output_size\n        self.stddev = 1.0\n        self.vae_trained = False\n        self.build_model()\n         \n    def build_model(self):\n        """"""Build VAE network such that VAE model = encoder + decoder.""""""\n        \n        # Define input layer & build encoder model\n  \n        self.inputs = layers.Input(shape=(self.input_size, ), name=\'encoder_input\')\n        \n        self.encoder_model()\n        self.decoder_model()\n        \n        # instantiate VAE model\n        outputs = self.decoder(self.encoder(self.inputs)[2])\n        self.vae = models.Model(self.inputs, outputs, name=\'vae\')\n        #plot_model(vae, to_file=\'vae_model.png\', show_shapes=True)\n        \n        self.action_dense = layers.Dense(1024, activation=\'relu\', name = \'action_dense\')(outputs)\n        \n        actions = layers.Dense(self.action_size, activation=\'sigmoid\', name=\'actions\')(self.action_dense)\n        \n        self.model = models.Model(self.inputs, actions, name=\'model\')\n        #plot_model(model, to_file=\'vae_action_model.png\', show_shapes=True)\n        \n        #custom loss function\n        state_gradients = layers.Input(shape=(self.input_size,))\n        z_decoded = self.decoder(self.z)\n        # Reconstruction loss\n        xent_loss = keras.metrics.binary_crossentropy(state_gradients, z_decoded)\n        # KL divergence\n        kl_loss = -5e-4 * K.mean(1 + self.z_log_var - K.square(self.z_mean) - K.exp(self.z_log_var), axis=-1)\n        loss_vae = K.mean(xent_loss + kl_loss)\n        \n        # Define optimizer and training functions\n       \n        #The optimizer and training function for the VAE model\n        self.vae.compile(loss=loss_vae,optimizer=\'adam\')\n        #updates_op_vae = optimizer.get_updates(params=self.vae.trainable_weights, loss=loss_vae)       \n            \n        self.model.compile(loss=\'categorical_crossentropy\',optimizer=\'adam\',metrics=[\'categorical_accuracy\'])\n        \n        \n    # reparameterization \n    # instead of sampling from Q(z|X), sample eps = N(0,I)\n    # z = z_mean + sqrt(var)*eps\n    def sampling(self,args):\n        """"""Reparameterization by sampling from an isotropic unit Gaussian.\n        # Arguments:\n            args (tensor): mean and log of variance of Q(z|X)\n        # Returns:\n            z (tensor): sampled latent vector\n        """"""\n        z_mean, z_log_var = args\n        batch = K.shape(z_mean)[0]\n        dim = K.int_shape(z_mean)[1]\n        # by default, random_normal has mean=0 and std=1.0\n        epsilon = K.random_normal(shape=(batch, dim),stddev = self.stddev)\n        \n        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\n    def encoder_model(self):\n        # Add hidden layers\n       \n        x1 = layers.Dense(256, activation=\'relu\')(self.inputs)\n        x1 = layers.Dense(512, activation=\'relu\')(x1)\n        x1 = layers.Dense(256, activation=\'relu\')(x1)\n        \n        # Mean and log_variation layers\n   \n        self.z_mean = layers.Dense(2, name=\'z_mean\')(x1)\n        self.z_log_var = layers.Dense(2, name=\'z_log_var\')(x1)\n\n        # use reparameterization trick to push the sampling out as input\n        # note that ""output_shape"" isn\'t necessary with the TensorFlow backend\n        self.z = layers.Lambda(self.sampling, output_shape=(2,), name=\'z\')([self.z_mean, self.z_log_var])\n\n        # instantiate encoder model\n        self.encoder = models.Model(self.inputs, [self.z_mean, self.z_log_var, self.z], name=\'encoder\')\n        #plot_model(encoder, to_file=\'vae_encoder.png\', show_shapes=True)\n        \n    def decoder_model(self):\n        # build decoder model\n        latent_inputs = layers.Input(shape=(2,), name=\'z_sampling\')\n        x2 = layers.Dense(256, activation=\'relu\')(latent_inputs)\n        outputs = layers.Dense(self.input_size, activation=\'sigmoid\')(x2)\n\n        # instantiate decoder model\n        self.decoder = models.Model(latent_inputs, outputs, name=\'decoder\')\n        #plot_model(decoder, to_file=\'vae_decoder.png\', show_shapes=True)\n        \n        \n    def tain_off(self):\n        if (self.vae_trained is True):\n            self.vae.trainable = False'"
agent.py,5,"b'from QNetwork import QNetwork\nfrom ReplayBuffer import ReplayBuffer\nfrom task import Task\nfrom VAE_action import VAE_action\nimport numpy as np\n\nclass Agent():\n    #""""""Reinforcement Learning agent that learns using Q-Learning and experiance replay.""""""\n    def __init__(self, task):\n        self.task = task\n        self.state_size = task.state_size\n        self.action_size = task.action_size\n        self.action_categories = task.action_categories\n\n        #VAE model\n        self.VAE_act = VAE_action(self.state_size, self.action_size, self.action_categories)\n        self.VAE_act.model.load_weights(\'model_weights/weights.trainedagent_2.h5\')\n        # Replay memory\n        self.buffer_size = 10000\n        self.batch_size = 80\n        self.memory = ReplayBuffer(self.buffer_size, self.batch_size)\n        \n        self.steps = 0\n        # Algorithm parameters\n        self.gamma = 0.95  # discount factor\n\n    def reset_episode(self):\n        #\'\'\'reset the task and agent variables\'\'\'\n        state = self.task.reset()\n        self.steps = 0\n        self.last_state = state\n        return state\n\n    def step(self, action, reward, next_state, done):\n        #\'\'\'save and learn from experiences\'\'\'\n        self.steps += 1\n         # Save experience / reward\n        self.memory.add(self.last_state, action, reward, next_state, done)\n\n        # Learn, if enough samples are available in memory\n        if len(self.memory) > self.batch_size:\n            experiences = self.memory.sample()\n            self.learn(experiences)\n\n        # Roll over last state and action\n        self.last_state = next_state\n\n    def act(self, state):\n        #""""""Returns actions for given state(s) as per current policy.""""""\n        input_state = np.reshape(state, [-1, self.state_size])\n        action = np.argmax(self.VAE_act.model.predict(input_state)[0])\n        return action\n\n    def learn(self, experiences):\n        #""""""Update policy and value parameters using given batch of experience tuples.""""""\n        \n        for state, action, reward, next_state, done in experiences:\n            # if done, make our target reward\n            target = reward\n            if not done:\n              # predict the future discounted reward\n                input_state = np.reshape(next_state, [-1, self.state_size])\n                target = reward + self.gamma * np.max(self.VAE_act.model.predict(input_state)[0])\n            # make the agent to approximately map\n            # the current state to future discounted reward\n            # save the value as target_final\n            input_state = np.reshape(state, [-1, self.state_size])\n            target_final = self.VAE_act.model.predict(input_state)\n            target_final[0][action] = target\n            # Train the Neural Net with the state and target_final\n            self.VAE_act.model.fit(input_state, target_final, epochs=1, verbose=0)\n'"
environment.py,36,"b""import numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\nfrom keras.models import load_model\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom scipy.spatial import distance\nimport random\nimport csv\n\n\n\nclass Environment():\n    #'''Approximate Environment model in which the agent operates'''\n    def __init__(self, grid_file,iBeacon_loc,labeled_data, runtime=5., init_pose=None):\n        \n        self.init_pose = init_pose\n        self.grid = np.load(grid_file)\n        self.runtime = runtime\n        self.b_loc = np.load(iBeacon_loc)\n        #Data with 13 BLE values and the encoded columns and rows\n        self.data = pd.read_csv(labeled_data)\n        self.dt = 1 / 10.0  # Timestep\n        \n        self.lower_bounds = np.array([0,0])\n        self.upper_bounds = np.array([self.grid.shape[0]-1,self.grid.shape[1]-1])\n        #self.env_model = load_model('model_weights/weights.environment.h5')\n        self.reset()\n\n    def reset(self):\n        #'''reset or initialize all the environment variables'''\n        self.time = 0.0\n        self.pose = np.array([2, 14]) if self.init_pose is None else np.copy(self.init_pose)\n        self.cols = self.grid.shape[1]\n        self.rows = self.grid.shape[0]\n        self.distance = 0\n        self.BLE_vals = self.calc_dis_BLE (self.pose)#self.calc_BLE (self.pose)\n        self.done = False\n        self.last_pose = np.array([2, 14]) if self.init_pose is None else np.copy(self.init_pose)\n        \n    def deep_inferred_BLE(self,position):\n        #'''prediction of 13 iBeacon values for a given position based on deep neural network model'''\n        ph1 = []\n        ph1.clear()\n        ph1.append(position[1])\n        ph1.append(position[0])\n        for j in range(0,self.b_loc.shape[0]):\n            ph1.append(3 * distance.euclidean(np.array([position[1],position[0]]), self.b_loc[j]))#Column first!\n        x = np.array(ph1).reshape(-1,15)\n        prediction = self.env_model.predict(x)\n        for i in range(0,prediction.shape[1]):\n            if (prediction[0,i] < 0.25):\n                prediction[0,i] = 0\n            else:\n                prediction[0,i] = 1 - ((x[0,i+2])/24)\n            if (prediction[0,i] < 0):\n                prediction[0,i] = 0\n        return np.array(prediction)\n    \n    def inferred_BLE(self,position):\n        #'''prediction of 13 iBeacon values for a given position based on mathematical model'''\n        ph2 = []\n        ph2.clear()\n        for j in range(0,self.b_loc.shape[0]):\n            ph2.append(3 * distance.euclidean(np.array([position[1],position[0]]), self.b_loc[j]))#Column first!\n        array = np.array(ph2)\n        array = array - array[np.argmin(array)]\n        min_index = np.argmin(array)\n        min_val  = array[min_index]\n        array[min_index] = 1000\n        s_min_index = np.argmin(array)\n        s_min_val  = array[s_min_index]\n        array[s_min_index] = 1000\n        if s_min_val > 5.5:\n            s_min_val  = 0\n        t_min_index = np.argmin(array)\n        t_min_val  = array[t_min_index]\n        if t_min_val > 5.5:\n            t_min_val  = 0\n        result = np.zeros((array.shape))\n        result[min_index] = (1.1/np.exp(min_val*0.1/1))-(0.03*5)\n        if s_min_val > 0:\n            result[s_min_index] = (1.1/np.exp(s_min_val*0.1/1))-(0.03*5)\n        if t_min_val > 0:\n            result[t_min_index] = (1.1/np.exp(t_min_val*0.1/1))-(0.03*5)\n        return result\n    \n    def calc_BLE (self,position):\n        #'''assign 13 iBeacon values for a given position'''\n        search = self.data[(self.data['col']==position[1]) & (self.data['row']==position[0])]\n        search_arr = search.values\n        if search_arr.shape[0] > 0:\n            rn = random.randint(0,search_arr.shape[0]-1)\n            return search_arr[rn,0:13]\n        else:\n            return self.inferred_BLE(position)\n           #return self.deep_inferred_BLE(position)\n    \n    def calc_dis_BLE (self,position):\n        #'''calculate distance between a given position and the 13 iBeacon locations '''\n        ph2 = []\n        ph2.clear()\n        for j in range(0,self.b_loc.shape[0]):\n            ph2.append(3 * distance.euclidean(np.array([position[1],position[0]]), self.b_loc[j]))#Column first!\n        return np.array(ph2)\n    \n    def next_timestep(self, direction):\n       \n       # '''\n        #if direction == 0: #move east\n        #    position = np.array([self.pose[0],(self.pose[1]+1)])\n        #elif direction == 1: #move south-east\n        #    position = np.array([self.pose[0]+1,self.pose[1]+1])    \n        #elif direction == 2: #move south\n        #    position = np.array([self.pose[0]+1,self.pose[1]])  \n        #elif direction == 3: #move south-west\n        #    position = np.array([self.pose[0]+1,self.pose[1]-1]) \n        #elif direction == 4: #move west\n        #    position = np.array([self.pose[0],self.pose[1]-1]) \n        #elif direction == 5: #move north-west\n        #    position = np.array([self.pose[0]-1,self.pose[1]-1])     \n        #elif direction == 6: #move north\n        #    position = np.array([self.pose[0]-1,self.pose[1]]) \n        #elif direction == 7: #move north-east\n        #    position = np.array([self.pose[0]-1,self.pose[1]+1]) \n        #else:\n        #    position = self.pose\n        #'''\n        #change the position based on a given action (direction)\n        if direction == 0: #move east\n            position = np.array([self.pose[0],(self.pose[1]+1)])   \n        elif direction == 1: #move south\n            position = np.array([self.pose[0]+1,self.pose[1]])\n        elif direction == 2: #move west\n            position = np.array([self.pose[0],self.pose[1]-1])     \n        elif direction == 3: #move north\n            position = np.array([self.pose[0]-1,self.pose[1]]) \n        #else:\n        #    position = self.pose\n        \n       #check for out of bounds\n        for ii in range(2):\n            if position[ii] < self.lower_bounds[ii]:\n                self.done = True\n                self.BLE_vals = np.zeros((1,13))\n            elif position[ii] >= self.upper_bounds[ii]:\n                self.done = True\n                self.BLE_vals = np.zeros((1,13))\n        # calculate BLE RSSI values for a given position\n        if not self.done:\n            self.BLE_vals = self.calc_dis_BLE (position)#self.calc_BLE (position)\n        \n        #check to see if the position is occupied with an object\n        if (self.grid[position[0],position[1]] == -10):\n            self.done = True\n        #check to see if the position has a datapoint     \n        #if (self.grid[position[0],position[1]] == 0):\n        #    self.done = True\n        \n        self.pose = position\n        self.time += self.dt\n        #increment and check the time\n        if self.time > self.runtime:\n            self.done = True\n        \n        if self.done is True:\n            self.last_pose = position\n            self.distace = 3 * distance.euclidean(self.last_pose, np.array([2, 14]))\n        return self.done"""
task.py,6,"b'import numpy as np\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom scipy.spatial import distance\nfrom environment import Environment\n\nclass Task():\n    #""""""Task (environment) that defines the goal and provides feedback to the agent.""""""\n    def __init__(self, grid_file,iBeacon_loc,labeled_data,runtime=5., target_pos=None, init_pose=None):\n        #""""""Initialize a Task object.\n         # Arguments:\n        #    init_pose: initial position of the user in (x,y) dimensions \n        #    runtime: time limit for each episode\n        #    target_pos: target/goal (x,y) position for the agent\n        #""""""\n        # Simulation\n        self.sim = Environment(grid_file,iBeacon_loc,labeled_data, runtime, init_pose) \n        self.action_repeat = 1\n        #""""""\n        #States: The state of the agent is represented as a tuple of these observations.\n        #1) A vector of 13 RSSI values.\n        #2) Current location (identified by row and column numbers).\n        #3) Distance to the target.\n        #""""""\n        self.state_size = self.action_repeat * 16\n        self.action_size = 1\n        self.action_categories = 4\n        #Statistics data variables\n        self.prev_dis = 0\n        self.total_dis = 0\n        self.positions = []\n        self.best_pos = []\n        self.best_score = -np.inf\n        self.score = 0\n        # Goal\n        self.target_pos = target_pos if target_pos is not None else np.array([17, 10])\n        self.init_dis = self.calc_distance(self.sim.pose, self.target_pos)\n        self.dis_to_target = self.calc_distance(self.sim.pose, self.target_pos)\n        \n    def calc_distance(self,a,b):\n        return 3 * distance.euclidean(a,b)\n    \n    def get_reward(self, done):\n        #""""""Uses current pose of sim to return reward.""""""\n        reward = 0\n        distance = self.calc_distance(self.sim.pose, self.target_pos)\n        self.total_dis += abs(self.prev_dis - distance)\n   \n        #positive reward for getting close to the target and neagtive for getting far\n        if self.prev_dis > distance:\n            reward += (self.prev_dis - distance)\n        if (distance < 12 and distance != 0): #reward for being close to target\n            reward += (10)\n        elif distance <= 4: # reward for getting to the target\n            reward += 20\n        #elif distance is not 0:\n        #    reward += 1/distance\n        else:\n            reward -= 1 #penalty for hovering away from the target\n            \n        #penalty for being done without reaching target\n        if done is True:\n            if not np.array_equal(self.sim.pose,self.target_pos):\n                reward -= 0\n        \n        self.prev_dis = distance\n        return reward\n  \n    \n    def step(self, direction):\n        #""""""Uses action to obtain next state, reward, done.""""""\n        reward = 0\n        list_ = []\n        \n        for _ in range(self.action_repeat):\n            done = self.sim.next_timestep(direction)\n            self.dis_to_target = self.calc_distance(self.sim.pose, self.target_pos)\n            if np.array_equal(self.sim.pose,self.target_pos):\n                done = True\n            reward += self.get_reward(done)\n            self.score += reward\n            self.positions.append(self.sim.pose)\n            y1 = self.sim.BLE_vals.reshape(-1,)\n            for i in range(0,13):\n                list_.append(y1[i])\n            y2 = self.sim.pose.reshape(-1,)\n            for i in range(0,2):\n                list_.append(y2[i])\n                \n            list_.append((3 * distance.euclidean(self.sim.pose,self.target_pos)))\n            \n        next_state =np.array(list_)\n        list_.clear()\n        \n        if done is True:\n            self.update_positions(self.score)\n        return next_state, reward, done\n\n    def reset(self):\n        #""""""Reset the sim to start a new episode.""""""\n        self.sim.reset()\n        self.prev_dis = 0\n        self.total_dis = 0\n        self.score = 0\n        self.dis_to_target = self.calc_distance(self.sim.pose, self.target_pos)\n        list_ = []\n        y1 = self.sim.BLE_vals.reshape(-1,)\n        for i in range(0,13):\n            list_.append(y1[i])\n        y2 = self.sim.pose.reshape(-1,)\n        for i in range(0,2):\n            list_.append(y2[i])    \n\n        list_.append((3 * distance.euclidean(self.sim.pose,self.target_pos)))\n\n        state = np.concatenate([np.array(list_)]* self.action_repeat)\n        list_.clear()\n        return state\n    \n    def update_positions(self,reward):\n        #""""""saves the best path found in terms of rewards""""""\n        if reward > self.best_score:\n            self.best_pos.clear()\n            self.best_pos = self.positions\n            self.positions.clear()\n            self.best_score = reward\n        \n        \n        \n        \n        '"
