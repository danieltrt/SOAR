file_path,api_count,code
activationFunction.py,0,"b'""""""\n\tEnum holding the names of the possible activation functions\n""""""\n\nfrom enum import Enum, auto\n\n\nclass ActivationFunction(Enum):\n\t""""""\n\t\tEnum holding the names of the possible activation functions\n\t""""""\n\tsigmoid = auto()\n\ttanh = auto()'"
dataSet.py,7,"b'""""""\n\tDataSets is responsible for processing, normalising and providing dataSets\n\tentires to other classes for training of the network.\n""""""\n\nimport numpy as np\n# import linecache  # Get a specific line of a file\n\nimport os.path  # check if a file exists at a certain path\nimport random  # Shuffle lines in dataset\n\n\nclass DataSet():\n\t""""""\n\t\tDataSets is responsible for processing, normalising and providing\n\t\tdataSets entires to other classes for training of the network.\n\t""""""\n\n\tfullDataSetPath = None\n\ttrainingDataSetPath = None\n\ttestDataSetPath = None\n\n\tdef __init__(self, fullDataSetPath, inputLabelNumber, trainingTestRatio=[9, 1]):\n\t\t""""""\n\t\t\tInitiate dataSet with a fullDataSetPath, inputLabelNumber an array\n\t\t\trepresenting the ration between inputs and lables. Optionally a\n\t\t\ttrainingTestRatio array cen be given which determines the ration\n\t\t\tbetween training and test data. Default is 9:1\n\t\t""""""\n\n\t\tself.inputLabelNumber = inputLabelNumber\n\t\tself.trainingTestRatio = trainingTestRatio\n\n\t\t# Check if path is valid and file exists\n\t\tif os.path.exists(fullDataSetPath):\n\t\t\tself.fullDataSetPath = fullDataSetPath\n\n\t\t\t# Check if the trainingDataSetPath and testDataSetPath file already exists\n\t\t\ttrainingDataSetPath = self.fullDataSetPath[:self.fullDataSetPath.rfind(""."")] + ""_training.txt""\n\t\t\ttestDataSetPath = self.fullDataSetPath[:self.fullDataSetPath.rfind(""."")] + ""_test.txt""\n\n\t\t\t# Assign them to attribute if they exists\n\t\t\tif os.path.exists(trainingDataSetPath) and os.path.exists(testDataSetPath):\n\n\t\t\t\tprint(""trainingDataSetPath and testDataSetPath exist, assigning them to attributes"")\n\n\t\t\t\tself.trainingDataSetPath = trainingDataSetPath\n\t\t\t\tself.testDataSetPath = testDataSetPath\n\n\t\t\t# Generate them if they do not exists yet\n\t\t\telse:\n\t\t\t\tself.splitDataSet()\n\n\t\telse:\n\t\t\tprint(""Given path is invalid. Reasign right path to attribute"")\n\n\tdef normalizeInput(self, vector):\n\t\t""""""\n\t\t\tNormalizes the vector by return a vector with the reciprocal value\n\t\t\tof each element in vector\n\t\t""""""\n\n\t\treturn np.divide(1, vector, out=np.zeros_like(vector), where=vector != 0)\n\n\tdef splitDataSet(self):\n\t\t""""""\n\t\t\tSplit the fullDataSetPath by the trainingTestRation into two files,\n\t\t\twhich are saved in the same path as the fullDataSetPath but with the\n\t\t\tending ""_training.txt"" resp. ""_test.txt"".\n\t\t""""""\n\n\t\tprint(""Splitting fullDataSetPath into trainingDataSetPath and testDataSetPath"")\n\n\t\t# Get number of lines(=data) in the fullDataSetPath\n\t\tnumberOfLines = 0\n\n\t\twith open(self.fullDataSetPath, ""r"") as ff:\n\t\t\tfor line in ff:\n\t\t\t\tnumberOfLines += 1\n\n\t\tself.trainingDataSetPath = self.fullDataSetPath[:self.fullDataSetPath.rfind(""."")] + ""_training.txt""\n\t\tself.testDataSetPath = self.fullDataSetPath[:self.fullDataSetPath.rfind(""."")] + ""_test.txt""\n\n\t\t# Get the number of elements for the training set (testset equals the remainder)\n\t\tsplitRatioSum = float(self.trainingTestRatio[0] + self.trainingTestRatio[1])\n\t\tnumberTrainingEntities = int(round(float(self.trainingTestRatio[0]) * numberOfLines / splitRatioSum))\n\n\t\t# Split the entites of the fullDataSetPath into the two files\n\t\twith open(self.fullDataSetPath, ""r"") as ff:\n\n\t\t\tfor (i, line) in enumerate(ff):\n\t\t\t\tif i < numberTrainingEntities:\n\t\t\t\t\twith open(self.trainingDataSetPath, ""a"") as trf:\n\t\t\t\t\t\ttrf.write(line)\n\n\t\t\t\tif i >= numberTrainingEntities:\n\t\t\t\t\twith open(self.testDataSetPath, ""a"") as tef:\n\t\t\t\t\t\ttef.write(line)\n\n\t\t\tprint(""Done creating training and test dataSet"")\n\n\tdef shuffleDataSet(self, dataSetPath):\n\t\t""""""\n\t\t\tdataSetPath is the path to the dataset which is then shuffled and\n\t\t\tsaved\n\t\t""""""\n\n\t\twith open(dataSetPath, ""r+"") as f:\n\t\t\tlines = f.readlines()\n\t\t\trandom.shuffle(lines)\n\t\t\tf.seek(0)\n\t\t\tf.writelines(lines)\n\n\tdef getStats(self):\n\t\t""""""\n\t\t\tAnalyses the dataset and gives the following statis about it:\n\t\t\tExtrema of each collumn, mean of each collumn\n\t\t""""""\n\n\t\tprint(""Analysing dataset"")\n\n\t\twith open(self.fullDataSetPath, ""r"") as ff:\n\n\t\t\t# Get the first line in order to get the number of columns and set the extrema to the values of the first line\n\t\t\tfirstLine = ff.readline().strip()\n\t\t\tfirstLineEntities = np.array([float(i) for i in firstLine.split(""\\t"")], dtype=np.float128)\n\n\t\t\tnumberOfColumns = firstLine.count(""\\t"") + 1\n\n\t\t\t# Holds the max value of each column in the first matrix row and the min value in the second row\n\t\t\t# For initialisation set the firstLine\'s entities as the extremas\n\t\t\textremaVector = np.array([firstLineEntities, firstLineEntities], dtype=np.float128)\n\n\t\t\t# Holds the sum of each column\n\t\t\tsumVector = np.zeros(numberOfColumns)\n\n\t\t\tnumberOfLines = 0\n\n\t\t\t# Get one line after another\n\t\t\tfor line in ff:\n\n\t\t\t\tlineEntities = np.array([float(i) for i in line.split(""\\t"")])\n\n\t\t\t\tsumVector = np.add(lineEntities, sumVector)\n\n\t\t\t\t# Check each entity if it is a extrema and assign it to the extremaVector if so\n\t\t\t\tfor (i, entity) in enumerate(lineEntities):\n\n\t\t\t\t\t# If max\n\t\t\t\t\tif entity > extremaVector[0][i]:\n\t\t\t\t\t\textremaVector[0][i] = entity\n\n\t\t\t\t\t# If min\n\t\t\t\t\tif entity < extremaVector[1][i]:\n\t\t\t\t\t\textremaVector[1][i] = entity\n\n\t\t\t\tnumberOfLines += 1\n\n\t\tprint(""NumberOfColumns: {0},\\nMaxValue: {1},\\nMinValue: {2},\\nNumberOfLines: {3},\\nMeanValue: {4}"".format(numberOfColumns, extremaVector[0], extremaVector[1], numberOfLines, np.divide(sumVector, numberOfLines)))\n'"
experimentNetTF.py,8,"b'import tensorflow as tf\nimport numpy as np\nimport os\nimport shutil\n\nfrom functools import wraps\n\ndef callOnce(inputFunc):\n\tattribute = ""_cache_"" + inputFunc.__name__\n\n\t@property\n\t@wraps(inputFunc)\n\tdef checkAttribute(self):\n\t\tif not hasattr(self, attribute):\n\t\t\tsetattr(self, attribute, inputFunc(self))\n\t\treturn getattr(self, attribute)\n\n\treturn checkAttribute\n\n\nclass ExperimentNetTF:\n\n\tdef __init__(self, shape, learningRate):\n\t\tself.shape = shape\n\t\tself.x = tf.placeholder(tf.float32, shape=[None, self.shape[0]], name=""InputData"")\n\t\tself.y = tf.placeholder(tf.float32, shape=[None, self.shape[-1]], name=""LabelData"")\n\n\t\tself.weights = self._getInitWeights()\n\n\t\tself.logDir = ""./log/experiment""\n\t\tshutil.rmtree(self.logDir)\n\t\tos.makedirs(self.logDir)\n\n\n\n\t\tself.learningRate = learningRate\n\n\t\tself.summaryWriter = tf.summary.FileWriter(self.logDir, graph=tf.get_default_graph())\n\n\t\tself.sess = tf.Session()\n\t\tself.sess.run(tf.global_variables_initializer())\n\n\t\tself.output\n\t\tself.optimizer\n\t\tself.loss\n\n\t\ttf.summary.scalar(""loss"", self.loss)\n\t\tself.mergedSummary = tf.summary.merge_all()\n\n\tdef _getInitWeights(self):\n\t\treturn [tf.Variable(tf.truncated_normal([fromLayer, toLayer], stddev=0.1, name=""Weight{}"".format(i))) for i, (fromLayer, toLayer) in enumerate(zip(self.shape[:-1], self.shape[1:]))]\n\n\tdef train(self, datasetPath, epochs=1):\n\n\t\tcostMeanList = []\n\n\t\tfor epoch in range(epochs):\n\t\t\tprint(f""Epoch {epoch + 1}"")\n\t\t\twith open(datasetPath, ""r"") as ds:\n\n\t\t\t\tcostList = []\n\n\t\t\t\tfor i, line in enumerate(ds):\n\t\t\t\t\tlineEntities = np.array([float(i) for i in line.split("","")], dtype=np.float128)\n\n\t\t\t\t\tinputs = np.reshape(lineEntities[:3], (1, 3))\n\t\t\t\t\tlabels = np.reshape(np.divide(lineEntities[3:], 25), (1, 1))\n\n\t\t\t\t\t# inputs = np.reshape(lineEntities[:2], (1, 2))\n\t\t\t\t\t# labels = np.reshape(lineEntities[2:], (1, 1))\n\n\t\t\t\t\t_, loss, summary = self.sess.run([self.optimizer, self.loss, self.mergedSummary], {self.x: inputs, self.y: labels})\n\n\t\t\t\t\tcostList.append(loss)\n\t\t\t\t\tself.summaryWriter.add_summary(summary, epoch * 1000 + epoch + i)\n\n\t\t\t\ttempList = np.array(costList)\n\t\t\t\tcostMeanList.append(np.mean(tempList))\n\n\t\t\t\taddListSummary = tf.Summary()\n\t\t\t\taddListSummary.value.add(tag=""MeanLoss"", simple_value=np.mean(tempList))\n\t\t\t\tself.summaryWriter.add_summary(addListSummary, epoch)\n\t\t\t\tself.summaryWriter.flush()\n\n\t\tself.saveTrainingData(""./experimentSave/test.txt"", costMeanList)\n\n\n\tdef getPrediction(self, xData):\n\t\treturn self.sess.run(self.output, {self.x: xData})\n\n\t@callOnce\n\tdef output(self):\n\t\tlayerInput = self.x\n\n\t\tfor weight in self.weights:\n\t\t\tlayerInput = tf.math.tanh(tf.matmul(layerInput, weight))\n\n\t\treturn layerInput\n\n\t@callOnce\n\tdef loss(self):\n\t\treturn tf.reduce_mean(tf.square(self.y - self.output))\n\t\t# return tf.square(self.y - self.output)\n\n\t@callOnce\n\tdef optimizer(self):\n\t\treturn tf.train.GradientDescentOptimizer(self.learningRate).minimize(self.loss)\n\n\tdef saveTrainingData(self, filePath, lossList):\n\n\t\tfile = open(filePath, ""a"")\n\n\t\tfor loss in lossList:\n\n\t\t\tfile.write(str(loss) + ""\\n"")\n\n\t\tfile.close()\n\n\tdef doSave(self, step):\n\t\tsavePath = self.saver.save(self.sess, os.path.join(self.savePath, ""model""), global_step = step)\n\t\tprint(""Saved current model to {}"".format(savePath))\n\n\nif __name__ == \'__main__\':\n\tnet = ExperimentNetTF([3, 10, 1], learningRate=0.0005)\n\n\tnet.train(""simulation/dataset/trackMaster1k.txt"", epochs=10)\n\n\t# net.train(""simulation/dataset/testAnd.txt"", epochs=100)'"
fullyConnected.py,18,"b'""""""\n\tFullyConnected represents a deep fullyConnected artificial network (dff). At\n\tinitialisation a list where each elements represents the number of neurons\n\tin each layer. With the evaluate function a given input can be evaluated and\n\twith train the network can be trained\n""""""\n\nimport numpy as np\nfrom activationFunction import ActivationFunction\n\n\nclass FullyConnected():\n\t""""""\n\t\tFullyConnected represents a deep fullyConnected artificial network\n\t\t(dff). At initialisation a list where each elements represents the\n\t\tnumber of neurons in each layer. With the evaluate function a given\n\t\tinput can be evaluated and with train the network can be trained\n\t""""""\n\n\tshape = None\n\tsize = None\n\tweights = None\n\tactivation = None\n\n\tdef __init__(self, shape, activation=ActivationFunction.tanh):\n\t\t""""""\n\t\t\tInitiate network with a shape list, where each element represents\n\t\t\tthe number of fully connected nodes in a specific layer.\n\t\t\tFor each link weights are created at random.\n\t\t""""""\n\n\t\tself.shape = np.array(shape, ndmin=2)\n\t\tself.size = len(shape)\n\n\t\tself.activation = activation\n\n\t\tself.weights = [np.random.normal(0, 1, size=(y, x)) for x, y in zip(shape[:-1], shape[1:])]\n\n\tdef evaluate(self, inputVector, getLayerValues=False):\n\t\t""""""\n\t\t\tTakes a vector with shape (1, n) as a input, transpose it to shape\n\t\t\t(n, 1) evaluates it in the neural network and returns eighter the\n\t\t\touput layer vector or a tuple containing the output vector of shape\n\t\t\t(n, 1) and a list containing all layer\'s node values (as vectors,\n\t\t\tused in the training method) depending whether getLayerValues is\n\t\t\ttrue or false (Default is false)\n\t\t""""""\n\n\t\tlayerVector = inputVector.reshape(len(inputVector.flatten()), 1)\n\n\t\tnetworkLayerValues = [inputVector]\n\n\t\t# For all layers n (except in input layer) sum up the weights commecting layer n and n-1 times ouput of layer n-1 and pass the output as the new input to the next layer\n\t\tfor layerWeights in self.weights:\n\n\t\t\t# Sum up all the inputs * weights for all node in layer n\n\t\t\tsummed = np.dot(layerWeights, layerVector)\n\n\t\t\t# Apply the activation function to the summed input in order to get the output of layer n\n\t\t\tif self.activation == ActivationFunction.sigmoid:\n\t\t\t\tlayerVector = self.sigmoid(summed)\n\n\t\t\telif self.activation == ActivationFunction.tanh:\n\t\t\t\tlayerVector = self.tanh(summed)\n\n\t\t\telse:\n\t\t\t\tprint(""Error: Activationfunction not found"")\n\t\t\t\treturn None\n\n\t\t\tnetworkLayerValues.append(layerVector)\n\n\t\t# Return eighter a tuple of just the output vector\n\t\tif getLayerValues:\n\t\t\treturn (layerVector, networkLayerValues)\n\n\t\treturn layerVector\n\n\tdef train(self, inputs, labels, learningRate=0.5):\n\t\t""""""\n\t\t\tinputs is the inputlayer vector, where labels is a vector holding\n\t\t\tthe associated labels. A deltaError is calculated and the weights\n\t\t\tare updated. An optional learningRate can be given (Default is 0.5)\n\t\t""""""\n\n\t\t# Bring vectors to the right shape: (n, 1)\n\t\tinputs = inputs.reshape(len(inputs), 1)\n\t\tlabels = labels.reshape(len(np.ravel(labels)), 1)\n\n\t\t# Get the outputs tuple of the network\n\t\tnetworkOutputs = self.evaluate(inputs, getLayerValues=True)\n\n\t\tnetworkErrors = self.backpropagate(networkOutputs[0], labels)\n\n\t\t# Iterate over the network, calculate deltaError and update the weights\n\t\tfor index in range(self.size - 1):\n\t\t\terrorL0 = networkErrors[len(networkErrors) - 1 - index]\n\t\t\toutputL0 = networkOutputs[1][len(networkOutputs[1]) - 1 - index]\n\t\t\toutputL1 = networkOutputs[1][len(networkOutputs[1]) - 2 - index]\n\n\t\t\t# dE Formula: np.dot(-errorL0 * outputL0 * (1.0 - outputL0), outputL1.T))\n\n\t\t\t# Update weights\n\t\t\t# same as self.weights[-index] which doesn\'t work for some reason\n\n\t\t\tif self.activation == ActivationFunction.sigmoid:\n\t\t\t\tdeltaWeight = learningRate * np.dot(-errorL0 * outputL0 * (1.0 - outputL0), outputL1.T)\n\n\t\t\telif self.activation == ActivationFunction.tanh:\n\t\t\t\tdeltaWeight = learningRate * np.dot(-errorL0 * (1.0 - np.square(outputL0)), outputL1.T)  # tanh\n\t\t\telse:\n\t\t\t\tprint(""Error: Activationfunction not found"")\n\t\t\t\treturn\n\n\t\t\tself.weights[len(self.weights) - 1 - index] -= deltaWeight\n\n\t\t# Return cost\n\t\treturn (networkOutputs[0] - labels)**2\n\n\t\t# todo Check if performance improved with the new weights. If so save the new weights, if not restore the old ones\n\n\tdef backpropagate(self, outputs, labels):\n\t\t""""""\n\t\t\tTakes the outputs vector and labels vector of the output,\n\t\t\tcomputes the error of the output and backpropagages it, returning a\n\t\t\tlist showing the error of each node\n\t\t""""""\n\n\t\t# Calculate error at the output layer\n\t\terror = np.array(labels - outputs)\n\n\t\t# List containing arrays of the errors of all nodes from inputLayer to the outputLayer\n\t\terrorVector = [error]\n\n\t\t# Start at the ouput player and go backwards to the input layer\n\t\tfor layerWeights in reversed(self.weights):\n\t\t\t# Get layernodes error\n\t\t\terror = np.dot(layerWeights.T, error)\n\n\t\t\t# Prepend (since we start at the outputlayer and move to the input layer) error to the errorVector\n\t\t\terrorVector.insert(0, error)\n\n\t\treturn np.array(errorVector)\n\n\tdef sigmoid(self, z):\n\t\t""""""\n\t\t\tApplies the sigmoid function elementwise to the vector z with shape\n\t\t\t(1, n) or (n, 1) and return a vector of the same shape\n\t\t""""""\n\n\t\t# Cast to float128 else we get an overflow error\n\t\tz = z.astype(np.float128)\n\n\t\treturn 1.0 / (1.0 + np.exp(-z))\n\n\tdef tanh(self, z):\n\t\t""""""\n\t\t\tApplies the tanh function elementwise to the vector z with shape\n\t\t\t(1, n) or (n, 1) and return a vector of the same shape\n\t\t""""""\n\n\t\t# Cast to float128 else we get an overflow error\n\t\tz = z.astype(np.float128)\n\n\t\treturn np.tanh(z)\n\n\t\t# ez = np.exp(z)\n\t\t# enz = np.exp(-z)\n\t\t# a = ez - enz\n\t\t# b = ez + enz\n\n\t\t# return np.divide(a, b, out=np.zeros_like(a), where=b!=0)\n\n\t\t# return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n'"
main.py,0,"b'#! /usr/bin/env python3\n\n""""""\n\tMain script of the sensorCar project. Interpreting this file will start up\n\tthe socket server, neural network etc. Depending on whether we need to load,\n\tsave etc. a trained network this file has to be altered\n""""""\n\nfrom network import Network\nfrom networkTF import NetworkTF\nfrom dataSet import DataSet\nfrom sensorCarController import SensorCarController\nfrom activationFunction import ActivationFunction\nfrom experimentNetTF import ExperimentNetTF\n\nfrom multiprocessing import Pool\n\nimport tensorflow as tf\nimport os\nimport re\n\n\ndef runCar(networkPath):\n\tstart = re.search(""/\\d"", networkPath).start() + 1\n\tend = re.search(""\\d_"", networkPath).start() - 1\n\tshape = [int(e) for e in networkPath[start:end].split(""-"")]\n\n# Vanilla\n\t# network = Network()\n\t# network.loadNet(networkPath)\n\t# network.dff.activation = ActivationFunction.tanh\n\n# TF\n\tnetwork = NetworkTF(shape)\n\tnetwork.doLoad(networkPath)\n\n\tsensorCarController = SensorCarController(network)\n\tsensorCarController.startServer()\n\ndef processTrainNetwork(data):\n\ttrainNetwork(data[0], data[1], data[2], data[3])\n\ndef trainNetwork(hiddenLayerShape, learningRate, dataSetPath, epochs):\n\n\tdataSet = DataSet(dataSetPath, [3, 1])\n\n# Vanilla\n\t# network = Network([3] + hiddenLayerShape + [1], ActivationFunction.tanh, dataSet)\n\t# network.train(epochs=epochs, learningRate=learningRate, verbosity=10, saveNet=10)\n\n# TF\n\tnetwork = NetworkTF([3] + hiddenLayerShape + [1], learningRate=learningRate, dataSet=dataSet)\n\tnetwork.train(epochs=epochs, saveStep=150, verbosity=0)\n\n\nif __name__ == \'__main__\':\n\n\tdsPath = ""./simulation/dataset/trackMaster""\n\tds = ""./simulation/dataset/trackMaster.txt""\n\n\tdata = []\n\n\t# 1: Dataset size\n\t# data += [([100, 50, 10], 0.001, dsPath + ""1k.txt"", 150), ([100, 50, 10], 0.001, dsPath + ""2k.txt"", 150), ([100, 50, 10], 0.001, dsPath + ""4k.txt"", 150), ([100, 50, 10], 0.001, dsPath + ""7k.txt"", 150), ([100, 50, 10], 0.001, dsPath + "".txt"", 150)]\n\n\t# 2: Learningrate\n\t# data += [([100, 50, 10], 0.1, ds, 150), ([100, 50, 10], 0.01, ds, 150), ([100, 50, 10], 0.001, ds, 150), ([100, 50, 10], 0.0001, ds, 150), ([100, 50, 10], 0.00001, ds, 150)]\n\t# data += [([100, 50, 10], 0.1, ds, 150), ([100, 50, 10], 0.01, ds, 150)]\n\n\t# 3: Size\n\t# data += [([50, 25], 0.001, ds, 150), ([75, 35, 10], 0.001, ds, 150), ([150, 75, 25], 0.001, ds, 150), ([150, 100, 25, 10], 0.001, ds, 150)]\n\t# data += [([150, 75, 25], 0.001, ds, 1), ([150, 100, 25, 10], 0.001, ds, 1)]\n\n\t# 4: Repetition\n\t# data += [([100, 50, 10], 0.001, ds, 150),([100, 50, 10], 0.001, ds, 150),([100, 50, 10], 0.001, ds, 150),([100, 50, 10], 0.001, ds, 150),([100, 50, 10], 0.001, ds, 150)]\n\t# data += [([100, 50, 10], 0.001, ds, 150),([100, 50, 10], 0.001, ds, 150),([100, 50, 10], 0.001, ds, 150),([100, 50, 10], 0.001, ds, 150)]\n\n\tp = Pool()\n\tp.map(processTrainNetwork, data)\n\n\t# trainNetwork([10], 0.001, ""./simulation/dataset/trackMaster.txt"", 200)\n\n\t# runCar(""./savedNetTF/3-10-1-0_0005-trackMaster1kII/model-9"")\n\n'"
mainCli.py,0,"b'#! /usr/bin/env python3\n\n""""""\n\tMain script of the sensorCar project. Interpreting this file will start up\n\tthe socket server, neural network etc. Depending on whether we need to load,\n\tsave etc. a trained network this file has to be altered\n""""""\nfrom network import Network\nfrom activationFunction import ActivationFunction\nfrom networkTF import NetworkTF\nfrom dataSet import DataSet\nfrom sensorCarController import SensorCarController\n\nfrom argparse import ArgumentParser\n\nimport tensorflow as tf\nimport os\nimport re\n\nfrom experimentNetTF import ExperimentNetTF\nfrom multiprocessing import Pool\n\nparser = ArgumentParser()\nparser.add_argument(\'-n\',\'--network\',\n\tdest=\'networktype\',\n\tchoices=[\'np\', \'tf\'],\n\tdefault=\'tf\',\n\thelp=\'determine whether the network should be numpy or tensorflow based\')\n\nparser.add_argument(\'-l\',\'--learningrate\',\n\tdest=\'learningrate\',\n\tdefault=0.3,\n\ttype=float,\n\thelp=\'learningrate used in conjunction with --training\')\n\nparser.add_argument(\'-e\',\'--epochs\',\n\tdest=\'epochs\',\n\tdefault=1,\n\ttype=int,\n\thelp=\'epochs used in conjunction with --training\')\n\nparser.add_argument(\'-s\',\'--shape\',\n\tdest=\'shape\',\n\tnargs=\'+\',\n\ttype=int,\n\thelp=\'shape used in conjunction with --training\')\n\nparser.add_argument(\'-g\',\'--graphic\',\n\tdest=\'tensorboard\',\n\tnargs=\'*\',\n\thelp=\'enable tensorboard representation and optionally provide a log directory\')\n\nparser.add_argument(\'-i\',\'--saveinterval\',\n\tdest=\'saveinterval\',\n\ttype=int,\n\thelp=\'determine after how many epochs the networks is saved\')\n\nparser.add_argument(\'-p\',\'--savepath\',\n    dest=\'savepath\',\n    nargs=1,\n    help=\'set folder save path for the network model\')\n\n\ngroup = parser.add_mutually_exclusive_group(required=True)\ngroup.add_argument(\'-t\',\'--training\',\n\tdest=\'datasetpath\',\n\tnargs=1,\n\thelp=\'Path to the dataset for training mode\')\ngroup.add_argument(\'-d\',\'--drive\',\n\tdest=\'networkpath\',\n\tnargs=1,\n\thelp=\'Path to the network model file drive mode\')\n\nargs = parser.parse_args()\n\ndef runCar(networkPath, networkType):\n\tstart = re.search(""/\\d"", networkPath).start() + 1\n\tend = re.search(""\\d_"", networkPath).start() - 1\n\tshape = [int(e) for e in networkPath[start:end].split(""-"")]\n\n\tnetwork = None\n\n\tif networkType == \'np\':\n\t\tnetwork = Network()\n\t\tnetwork.loadNet(networkPath)\n\n\telif networkType == \'tf\':\n\t\tnetwork = NetworkTF(shape)\n\t\tnetwork.doLoad(networkPath)\n\n\tsensorCarController = SensorCarController(network)\n\tsensorCarController.startServer()\n\ndef trainNetwork(shape, learningRate, dataSetPath, epochs, networkType, tensorboard, saveinterval, savepath):\n\n\tdataSet = DataSet(dataSetPath, [shape[0], shape[-1]])\n\n\tif networkType == \'np\':\n\t\tnetwork = Network(shape, ActivationFunction.tanh, dataSet)\n\t\tnetwork.train(epochs=epochs, learningRate=learningRate, verbosity=10, saveStep=saveinterval)\n\n\telif networkType == \'tf\':\n\n\t\ttensorboardPath = None if tensorboard is None else ((tensorboard[0] if tensorboard[0][-1] == \'/\' else tensorboard[0] + \'/\') if len(tensorboard) > 0 else \'./log/\')\n\n\t\tnetwork = NetworkTF(shape, learningRate=learningRate, dataSet=dataSet, tensorboard=tensorboardPath)\n\t\tnetwork.train(epochs=epochs, verbosity=10, saveStep=saveinterval, savePath=savepath)\n\nif __name__ == \'__main__\':\n\n\tif args.datasetpath:\n\t\tif args.shape is None:\n\t\t\tparser.error(""--training requires --shape"")\n\n\t\telse:\n\t\t\tsavePath = None if args.savepath is None else None if len(args.savepath) < 1 else args.savepath[0]\n\n\t\t\ttrainNetwork(args.shape, args.learningrate, args.datasetpath[0], args.epochs, args.networktype, args.tensorboard, args.saveinterval, savePath)\n\n\telif args.networkpath:\n\t\trunCar(args.networkpath[0], args.networktype)'"
network.py,6,"b'""""""\n\tNetwork acts like a wrapper for an artificial neural network. (atm only\n\tdff). Is used for evaluating a net. Compined with a dataSet instance it can\n\tbe used to train and get the performance of the net.\n""""""\n\nimport numpy as np\nimport pickle  # Save instance of class\nimport time  # Measure time\n\nimport os\n\nfrom fullyConnected import FullyConnected\n\n\nclass Network():\n\t""""""\n\t\tNetwork acts like a wrapper for an artificial neural network. (atm only\n\t\tdff). Is used for evaluating a net. Compined with a dataSet instance it\n\t\tcan be used to train and get the performance of the net.\n\t""""""\n\n\tdataSet = None\n\n\tdef __init__(self, dffShape=False, activation=False, dataSet=False):\n\t\t""""""\n\t\t\tdffShape is a array where each element represents the number of\n\t\t\tnodes in each layer of the dff. dataSet is an instance of DataSet.\n\t\t\tWhen it is not provided network cannot be used to train and get the\n\t\t\tperformance of the net\n\t\t""""""\n\n\t\tif dffShape and activation:\n\t\t\tself.dff = FullyConnected(dffShape, activation)\n\n\t\tif dataSet is not False:\n\t\t\tself.dataSet = dataSet\n\n\tdef train(self, epochs=1, learningRate=0.3, verbosity=1, saveStep=None, savePath=None):\n\t\t""""""\n\t\t\tIf a dataSet is initiated and assigned the dff is trained.\n\t\t\tActivation determines the activationfunction and can eigheter be\n\t\t\t""sigmoid"" or ""tamh"". An optional epochs (Default is 1) and\n\t\t\tlearningRate (Defualt is 0.3) can be given. Verbosity determines the\n\t\t\tnumber of epochs after which some information is printed out. The\n\t\t\toptional saveStep is the number of epochs after which the network\n\t\t\tinstance is saved to the savePath if provided. Else it will be saved\n\t\t\tto a default path.\n\t\t\tReturns a list containing all costfunction values of each epoch\n\t\t""""""\n\n\t\t# Check if training possible\n\t\tif self.dataSet is None:\n\t\t\tprint(""Training not possbile since no dataSet is assigned"")\n\t\t\treturn\n\n\t\tprint(""{0}\\nTraining started\\nnumberOfEpochs: {1}"".format(15 * ""-"", epochs))\n\n\t\tif saveStep is not None:\n\t\t\tds = self.dataSet.fullDataSetPath\n\n\t\t\tsavePath = savePath if savePath is not None else \'./savedNetTF/\'\n\t\t\tsavePath = savePath if savePath[-1] == \'/\' else savePath + \'/\'\n\n\t\t\tsavePath = savePath + """".join(str(e) + ""-"" for e in self.dff.shape[0].tolist()) + str(learningRate).replace(\'.\', \'_\') + ""-"" + ds[ds.rfind(""/"") + 1: ds.rfind(""."")] + ""/""\n\n\t\t\t# Check if dir already exists, if so add roman letters behinde it\n\t\t\twhile os.path.exists(savePath):\n\t\t\t\tsavePath = savePath[:savePath.rfind(""/"")] + ""I"" + savePath[savePath.rfind(""/""):]\n\n\t\t\tos.makedirs(savePath)\n\n\t\t\tif saveStep == -1:\n\t\t\t\tpreviousCost = 1e309\n\n\t\tstartTrainingTime = time.time()  # Used for calculating used time\n\t\tcostList = []  # Holds the cost value of each epoch\n\n\t\tdeltaEpochTrainingTime = 0  # time between two epochs\n\n\t\tdeltaPrintTrainingTime = 0  # time between two prints\n\t\tpreviousPrintTrainingTime = time.time()  # time of the previous print. Used for calculating the deltaPrintTime\n\n\t\tdeltaEpochCost = 0\n\t\tpreviousEpochCost = 0\n\n\t\tdeltaPrintCost = 0\n\t\tpreviousPrintCost = 0\n\n\t\tfor epoch in range(epochs):\n\n\t\t\t# Shuffle Dataset\n\t\t\t# self.dataSet.shuffleDataSet(self.dataSet.trainingDataSetPath)\n\n\t\t\t# Read trainingfile and train on it line by line\n\t\t\twith open(self.dataSet.trainingDataSetPath, ""r"") as trf:\n\n\t\t\t\tcostSum = 0  # costSum of one epoch\n\t\t\t\tnumberOfLines = 0\n\n\t\t\t\tstartEpochTrainingTime = time.time()\n\n\t\t\t\tdoPrint = False\n\n\t\t\t\t# Determine whether this epoch data should be printed\n\t\t\t\tif verbosity is not 0 and ((epoch + 1) % verbosity == 0 or epoch == 0):\n\n\t\t\t\t\tprint(""{2}\\nEpoch {0}/{1}"".format(epoch + 1, epochs, 15 * ""-""))\n\n\t\t\t\t\tdoPrint = True\n\n\t\t\t\tfor line in trf:\n\t\t\t\t\t# Split the line entities into an array and normalize it\n\t\t\t\t\tlineEntities = np.array([float(i) for i in line.split("","")], dtype=np.float128)\n\n\t\t\t\t\t# todo Normalisation\n\t\t\t\t\tinputs = lineEntities[:self.dataSet.inputLabelNumber[0]]\n\t\t\t\t\tlabels = np.divide(lineEntities[-self.dataSet.inputLabelNumber[1]:], 25)\n\t\t\t\t\t# labels = lineEntities[-self.dataSet.inputLabelNumber[1]:]\n\n\t\t\t\t\tcostSum += self.dff.train(inputs, labels, learningRate)\n\n\t\t\t\t\tnumberOfLines += 1\n\n\t\t\tcostList.append(costSum[0][0] / numberOfLines)\n\n\t\t\tcost = costSum / numberOfLines\n\n\t\t\tdeltaEpochCost = previousEpochCost - cost\n\t\t\tpreviousEpochCost = cost\n\n\t\t\tif doPrint:\n\n\t\t\t\tdeltaTrainingTime = time.time() - startTrainingTime\n\n\t\t\t\tdeltaEpochTrainingTime = time.time() - startEpochTrainingTime\n\n\t\t\t\tdeltaPrintTrainingTime = time.time() - previousPrintTrainingTime\n\t\t\t\tpreviousPrintTrainingTime = time.time()\n\n\t\t\t\tdeltaPrintCost = previousPrintCost - cost\n\t\t\t\tpreviousPrintCost = cost\n\n\t\t\t\tprint(""{0}{1}{0}"".format(5 * ""-"", self.dff.shape))\n\t\t\t\tprint(""deltaTrainingTime:\\t{},\\ndeltaEpochTrainingTime:\\t{},\\ndeltaPrintTrainingTime:\\t{},\\ncost:\\t{},\\ndeltaEpochCost:\\t{},\\ndeltaPrintCost:\\t{}"".format(deltaTrainingTime, deltaEpochTrainingTime, deltaPrintTrainingTime, cost, deltaEpochCost, deltaPrintCost))\n\n\t\t\tif saveStep is not None:\n\t\t\t\tself.saveTrainingData(savePath + ""training.txt"", str(cost))\n\n\t\t\t\tif ((epoch == 0) or (epoch == epochs - 1) or epoch % saveStep == 0):\n\t\t\t\t\tself.saveNet(savePath + str(epoch + 1) + "".txt"")\n\n\t\t\tif saveStep == -1 and (previousCost > cost):\n\t\t\t\tself.saveNet(savePath + str(epoch + 1) + "".txt"")\n\t\t\t\tpreviousCost = cost\n\n\t\tprint(""{0}\\nepochs: {1},\\ncost: {3},\\ntrainingTime: {2}\\n{0}"".format(20 * ""-"", epochs, time.time() - startTrainingTime, costList[-1]))\n\n\t\treturn costList\n\n\tdef evaluate(self, inputVector, normalize=False):\n\t\t""""""\n\t\t\tEvaluates a given inputVector in the dff and returns the ouputVector\n\t\t""""""\n\n# todo Normalisation\n\t\t# if normalize:\n\t\t# \t# Normalize input\n\t\t# \t# inputVector = self.normalize(inputVector)\n\n\t\t# \t# Evaluate\n\t\t# \toutputVector = self.dff.evaluate(inputVector)\n\n\t\t# \treturn outputVector\n\n\t\t# \t# Normalize output\n\t\t# \treturn self.normalize(outputVector)\n\n\t\treturn self.dff.evaluate(inputVector)\n\n\tdef normalize(self, vector):\n\t\t""""""\n\t\t\tNormalizes the vector by return a vector with the reciprocal value\n\t\t\tof each element in vector\n\t\t""""""\n\n\t\treturn np.divide(1, vector, out=np.zeros_like(vector), where=vector != 0)\n\n\tdef getCost(self, inputVector, labelsVector):\n\t\t""""""\n\t\t\tReturns the MSE\n\t\t""""""\n\n\t\treturn (self.evaluate(inputVector) - labelsVector)**2\n\n\tdef getPerformance(self):\n\t\t""""""\n\t\t\tEvaluate how well the net does by taking the abs value of the\n\t\t\tsubtraction of the lable by the evaluated value\n\t\t""""""\n\n\t\t# Open the testDataSetPath and calculate the difference line by line\n\t\twith open(self.dataSet.testDataSetPath, ""r"") as tef:\n\n\t\t\t# Used for calculating the mean\n\t\t\tnumberOfLines = 0\n\t\t\tdifferenceSum = 0.0\n\n\t\t\t# Get the difference line by line and add it to the differenceSum\n\t\t\tfor line in tef:\n\n\t\t\t\t# lineEntities = self.normalize(np.array([float(i) for i in line.sit(""\\t"")], dtype=np.float128))\n\n\t\t\t\tlineEntities = np.array([float(i) for i in line.split(""\\t"")], dtype=np.float128)\n\n\t\t\t\tinputs = lineEntities[:self.dataSet.inputLabelNumber[0]]\n\n# todo Normalisation\n\t\t\t\t# labels = self.normalize(lineEntities[-self.dataSet.inputLabelNumber[1]:])\n\t\t\t\tlabels = (lineEntities[-self.dataSet.inputLabelNumber[1]:])\n\n\t\t\t\toutputs = self.dff.evaluate(inputs)[0][0]\n\n\t\t\t\tdifferenceSum += np.abs(np.subtract(outputs, labels))\n\n\t\t\t\tnumberOfLines += 1\n\n\t\t\tprint(""The mean difference is {}"".format(differenceSum / numberOfLines))\n\n\tdef saveNet(self, filePath):\n\t\t""""""\n\t\t\tSave the instance of this class to the given filePath\n\t\t""""""\n\n\t\tprint(""Saving network instance to {}"".format(filePath))\n\n\t\twith open(filePath, ""wb"") as f:\n\t\t\tf.write(pickle.dumps(self.__dict__))\n\n\tdef saveTrainingData(self, filePath, toSave):\n\t\t""""""\n\t\t\tSaves toSave to the text file specified in the filePath\n\t\t""""""\n\n\t\tprint(""Saving network training data to {}"".format(filePath))\n\n\t\twith open(filePath, ""a"") as f:\n\t\t\tf.write(""\\n"" + toSave)\n\n\tdef loadNet(self, filePath):\n\t\t""""""\n\t\t\tLoad the instance of this class from the given filePath\n\t\t""""""\n\n\t\tprint(""Loading network instance from {}"".format(filePath))\n\n\t\twith open(filePath, ""rb"") as f:\n\t\t\tself.__dict__ = pickle.load(f)\n'"
networkTF.py,3,"b'""""""\n\n""""""\n\nfrom functools import wraps\n\nimport numpy as np\nimport tensorflow as tf\nimport os\nimport shutil\nimport pathlib\nimport time\n\nclass NetworkTF:\n\n\tdef propertyWithCheck(inputFunc):\n\t\tattribute = ""_cache_"" + inputFunc.__name__\n\n\t\t@property\n\t\t@wraps(inputFunc)\n\t\tdef check_attr(self):\n\t\t\tif not hasattr(self, attribute):\n\t\t\t\tsetattr(self, attribute, inputFunc(self))\n\t\t\treturn getattr(self, attribute)\n\n\t\treturn check_attr\n\n\tdataSet = None\n\tsavePath = None\n\n\tdef __init__(self, shape, learningRate=0.3, dataSet=None, tensorboard=None):\n\n\t\tself.tensorboardEnabled = True if tensorboard is not None else False\n\n\t\ttf.reset_default_graph()\n\t\tself.shape = shape\n\n\t\tself.learningRate = learningRate\n\n\t\tif dataSet is not None:\n\t\t\tself.dataSet = dataSet\n\n\t\t\tds = self.dataSet.fullDataSetPath\n\n\t\t\tself.uid =  """".join(str(e) + ""-"" for e in self.shape) + str(self.learningRate).replace(\'.\', \'_\') + ""-"" + ds[ds.rfind(""/"") + 1: ds.rfind(""."")]\n\n\t\telse:\n\t\t\tself.uid = """".join(str(e) + ""-"" for e in self.shape) + str(self.learningRate).replace(\'.\', \'_\')\n\n\t\t# Tensorflow attributes\n\n\t\tself.x = tf.placeholder(tf.float32, shape=[None, self.shape[0]], name=""InputData"")\n\t\tself.y = tf.placeholder(tf.float32, shape=[None, self.shape[-1]], name=""LabelData"")\n\n\t\tself.weights = self._getInitWeights()\n\n\t\tself.saver = self.saver()\n\n\t\tself.predict\n\t\tself.optimizer\n\t\tself.loss\n\n\t\tif self.tensorboardEnabled:\n\t\t\tself.logDir = tensorboard + self.uid + str(int(time.time()))\n\t\t\tpathlib.Path(self.logDir).mkdir(parents=True, exist_ok=True)\n\n\t\t\ttf.summary.scalar(""loss"", self.loss)\n\t\t\tself.mergedSummary = tf.summary.merge_all()\n\n\t\tself.sess = tf.Session()\n\t\tself.sess.run(tf.global_variables_initializer())\n\n\t\tif self.tensorboardEnabled:\n\t\t\tself.summaryWriter = tf.summary.FileWriter(self.logDir, graph=tf.get_default_graph())\n\n\tdef train(self, epochs=1, verbosity=1, saveStep=None, savePath=None):\n\n\t\t# Check if training possible\n\t\tif self.dataSet is None:\n\t\t\tprint(""Training not possbile since no dataSet is assigned"")\n\t\t\treturn\n\n\t\tprint(""{0}\\nTraining started\\nnumberOfEpochs: {1}"".format(15 * ""-"", epochs))\n\n\t\tif saveStep is not None:\n\t\t\tds = self.dataSet.fullDataSetPath\n\n\t\t\tsavePath = savePath if savePath is not None else \'./savedNetTF/\'\n\t\t\tsavePath = savePath if savePath[-1] == \'/\' else savePath + \'/\'\n\n\t\t\tself.savePath = savePath + self.uid + \'/\'\n\n\t\t\twhile os.path.exists(self.savePath):\n\t\t\t\tself.savePath = self.savePath[:self.savePath.rfind(""/"")] + ""I"" + self.savePath[self.savePath.rfind(""/""):]\n\n\t\t\tos.makedirs(self.savePath)\n\n\t\t\tself.saver\n\n\t\t\tif saveStep == -1:\n\t\t\t\tpreviousCost = 1e309\n\n\t\tstartTrainingTime = time.time()  # Used for calculating used time\n\t\tcostList = []  # Holds the cost value of each epoch\n\n\t\tdeltaEpochTrainingTime = 0  # time between two epochs\n\n\t\tdeltaPrintTrainingTime = 0  # time between two prints\n\t\tpreviousPrintTrainingTime = time.time()  # time of the previous print. Used for calculating the deltaPrintTime\n\n\t\tdeltaEpochCost = 0\n\t\tpreviousEpochCost = 0\n\n\t\tdeltaPrintCost = 0\n\t\tpreviousPrintCost = 0\n\n\t\tcounter = 0\n\n\t\tfor epoch in range(epochs):\n\n\t\t\twith open(self.dataSet.trainingDataSetPath, ""r"") as trf:\n\n\t\t\t\tcostSum = 0  # costSum of one epoch\n\t\t\t\tnumberOfLines = 0\n\n\t\t\t\tstartEpochTrainingTime = time.time()\n\n\t\t\t\tdoPrint = False\n\n\t\t\t\t# Determine whether this epoch data should be printed\n\t\t\t\tif verbosity is not 0 and ((epoch + 1) % verbosity == 0 or epoch == 0):\n\n\t\t\t\t\tprint(""{2}\\nEpoch {0}/{1}"".format(epoch + 1, epochs, 15 * ""-""))\n\n\t\t\t\t\tdoPrint = True\n\n\t\t\t\tfor line in trf:\n\n\t\t\t\t\tlineEntities = np.array([float(i) for i in line.split("","")], dtype=np.float128)\n\t\t\t\t\tinputs = np.array(lineEntities[:self.dataSet.inputLabelNumber[0]], ndmin=2)\n\t\t\t\t\tlabels = np.array(np.divide(lineEntities[-self.dataSet.inputLabelNumber[1]:], 25), ndmin=2)\n\n\t\t\t\t\tloss = self.sess.run(self.loss, {self.x: inputs, self.y: labels})\n\n\t\t\t\t\tif self.tensorboardEnabled:\n\t\t\t\t\t\tsummary = self.sess.run(self.mergedSummary, {self.x: inputs, self.y: labels})\n\t\t\t\t\t\tself.summaryWriter.add_summary(summary, counter)\n\n\t\t\t\t\t_ = self.sess.run(self.optimizer, {self.x: inputs, self.y: labels})\n\n\t\t\t\t\tcostSum += loss\n\n\t\t\t\t\tnumberOfLines += 1\n\t\t\t\t\tcounter += 1\n\n\t\t\tcostList.append(costSum / numberOfLines)\n\n\t\t\tcost = costSum / numberOfLines\n\n\t\t\tif saveStep is not None:\n\t\t\t\tname = self.savePath[:self.savePath.rfind(""/"")][self.savePath[:self.savePath.rfind(""/"")].rfind(""/"") + 1:] + "".txt""\n\n\t\t\t\tself.saveTrainingData(self.savePath + name, cost)\n\n\t\t\tif self.tensorboardEnabled:\n\t\t\t\taddListSummary = tf.Summary()\n\t\t\t\taddListSummary.value.add(tag=""MeanLoss"", simple_value=cost)\n\t\t\t\tself.summaryWriter.add_summary(addListSummary, epoch)\n\t\t\t\tself.summaryWriter.flush()\n\n\t\t\tdeltaEpochCost = previousEpochCost - cost\n\t\t\tpreviousEpochCost = cost\n\n\t\t\tif doPrint:\n\n\t\t\t\tdeltaTrainingTime = time.time() - startTrainingTime\n\n\t\t\t\tdeltaEpochTrainingTime = time.time() - startEpochTrainingTime\n\n\t\t\t\tdeltaPrintTrainingTime = time.time() - previousPrintTrainingTime\n\t\t\t\tpreviousPrintTrainingTime = time.time()\n\n\t\t\t\tdeltaPrintCost = previousPrintCost - cost\n\t\t\t\tpreviousPrintCost = cost\n\n\t\t\t\tprint(""{0}{1}{0}"".format(5 * ""-"", self.shape))\n\t\t\t\tprint(""deltaTrainingTime:\\t{},\\ndeltaEpochTrainingTime:\\t{},\\ndeltaPrintTrainingTime:\\t{},\\ncost:\\t{},\\ndeltaEpochCost:\\t{},\\ndeltaPrintCost:\\t{}"".format(deltaTrainingTime, deltaEpochTrainingTime, deltaPrintTrainingTime, cost, deltaEpochCost, deltaPrintCost))\n\n\t\t\tif saveStep is not None and ((epoch == 0) or (epoch == epochs - 1) or epoch % saveStep == 0):\n\t\t\t\tself.doSave(epoch)\n\n\t\t\tif saveStep == -1 and (previousCost > cost):\n\t\t\t\tself.doSave(epoch)\n\t\t\t\tpreviousCost = cost\n\n\tdef getPrediction(self, xData):\n\t\treturn self.sess.run(self.predict, feed_dict={self.x: xData})\n\n\tdef _getInitWeights(self):\n\t\treturn [tf.Variable(tf.truncated_normal([fromLayer, toLayer], stddev=0.1), name=""Weight{}"".format(i)) for i, (fromLayer, toLayer) in enumerate(zip(self.shape[:-1], self.shape[1:]))]\n\n\t@propertyWithCheck\n\tdef predict(self):\n\t\tlayerInput = self.x\n\n\t\tfor weight in self.weights:\n\t\t\tlayerInput = tf.math.tanh(tf.matmul(layerInput, weight))\n\n\t\treturn layerInput\n\n\t@propertyWithCheck\n\tdef loss(self):\n\t\treturn tf.reduce_mean(tf.square(self.y - self.predict))\n\n\t@propertyWithCheck\n\tdef optimizer(self):\n\t\treturn tf.train.GradientDescentOptimizer(self.learningRate).minimize(self.loss)\n\n\tdef saver(self):\n\t\treturn tf.train.Saver(max_to_keep=10000)\n\n\tdef evaluate(self, xData):\n\t\treturn self.getPrediction(xData)\n\n\tdef doSave(self, step):\n\t\tsavePath = self.saver.save(self.sess, os.path.join(self.savePath, ""model""), global_step = step)\n\t\tprint(""Saved current model to {}"".format(savePath))\n\n\tdef doLoad(self, modelPath):\n\t\tself.saver.restore(self.sess, modelPath)\n\t\tprint(""Loaded model from {}"".format(modelPath))\n\n\tdef saveTrainingData(self, filePath, toSave):\n\t\t# os.makedirs(filePath)\n\n\t\t# print(""Saving network training data to {}"".format(filePath))\n\n\t\twith open(filePath, ""a"") as f:\n\t\t\tf.write(str(toSave) + ""\\n"")\n\n\n'"
sensorCarController.py,2,"b'""""""\n\tCombines the network with a socketServer. Received data is evaluated in\n\tthe network and emitted back via the socketServer\n""""""\n\nimport socketio\nfrom flask import Flask\nimport eventlet\n\nimport numpy as np\n\n\nclass SensorCarController():\n\t""""""\n\t\tCombines the network with the socketServer. Received data is evaluated\n\t\tin the network and emitted back via the socketServer\n\t""""""\n\n\tnetwork = None\n\n\tdef __init__(self, network):\n\t\t""""""\n\t\t\tInitiate sensorCarController with a instance of network which is\n\t\t\tused for evaluating sensorInformation\n\t\t""""""\n\n\t\tself.network = network\n\n\t\tself.sio = socketio.Server()\n\n\t\t# Define events\n\t\t@self.sio.on(\'connect\')\n\t\tdef connect(sid, environ):\n\t\t\tprint(""Client connected: "" + str(sid))\n\n\t\t@self.sio.on(\'disconnect\')\n\t\tdef disconnect(sid):\n\t\t\tprint(""Client disconnected: "" + str(sid))\n\n\t\t@self.sio.on(\'evaluate\')\n\t\tdef evaluate(sid, data):\n\t\t\tprint(""Received package for evaluation"")\n\n\t\t\tself.receive(data)\n\n\tdef startServer(self):\n\t\t""""""\n\t\t\tStarts the socketserver which listens for specific events\n\t\t""""""\n\n\t\twsgiApp = Flask(__name__)\n\t\tapp = socketio.Middleware(self.sio, wsgiApp)\n\n\t\ttry:\n\t\t\teventlet.wsgi.server(eventlet.listen(("""", 4567)), app)\n\t\t\tprint(""SocketServer started"")\n\n\t\texcept KeyboardInterrupt:\n\t\t\tprint(""SocketServer stopped"")\n\n\tdef receive(self, data):\n\t\t""""""\n\t\t\tCalled in the evaluate event. data is the JSON object received from\n\t\t\tthe simulation. Has to be converted into an array and them evaluated\n\t\t\tin the net. In the end the evaluated data is emitted\n\t\t""""""\n\n\t\t# Convert string of json to float list\n\t\t# inputVector = np.array([float(data[key]) for key in data], dtype=np.float32)\n\t\tinputVector = np.array([[float(data[key]) for key in data]], dtype=np.float32)\n\n\t\t# inputVector.ex\n\n\t\tprint(""inputV"", inputVector)\n\n\t\t# evaluate in net\n\t\toutputVector = self.network.evaluate(inputVector)[0][0]\n\n\t\t# todo make general\n\t\tprint(inputVector, ""->"", outputVector)\n\n\t\t# returne evaluated values to simulation\n\t\tself.sio.emit(\'steer\', data={\'steering_angle\': str(25 * outputVector)}, skip_sid=True)\n'"
