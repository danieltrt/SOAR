file_path,api_count,code
setup.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport io\nimport os\nimport sys\nfrom setuptools import setup, Extension\nfrom setuptools import find_packages\n\nNAME            = 'ztlearn'\nDESCRIPTION     = 'Minimalistic Python Machine Learning Toolkit.'\nURL             = 'https://github.com/jefkine/zeta-learn'\nDOWLOAD_URL     = 'https://github.com/jefkine/zeta-learn/archive/master.zip'\nEMAIL           = 'jefkine@gmail.com'\nAUTHOR          = 'Jefkine Kafunah'\nREQUIRES_PYTHON = '>=3.5.0'\nVERSION         = None\nREQUIRED        = ['numpy', 'matplotlib', 'scipy']\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\nwith io.open(os.path.join(here, 'README.rst'), encoding='utf-8') as f:\n    long_description = '\\n' + f.read()\n\nabout = {}\nif not VERSION:\n    with open(os.path.join(here, NAME, '__version__.py')) as f:\n        exec(f.read(), about)\nelse:\n    about['__version__'] = VERSION\n\nsetup(\n    name                          = NAME,\n    version                       = about['__version__'],\n    description                   = DESCRIPTION,\n    long_description              = long_description,\n    long_description_content_type = 'text/markdown',\n    author                        = AUTHOR,\n    author_email                  = EMAIL,\n    python_requires               = REQUIRES_PYTHON,\n    url                           = URL,\n    install_requires              = REQUIRED,\n    include_package_data          = True,\n    license                       = 'MIT',\n    classifiers                   = [\n        'Development Status :: 4 - Beta',\n        'Intended Audience :: Developers',\n        'Intended Audience :: Education',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: MIT License',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.6',\n        'Topic :: Software Development :: Libraries',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence'\n    ],\n    packages             = find_packages(exclude=('docs',))\n)\n"""
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/stable/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\nsys.path.insert(0, os.path.abspath(\'../\'))\n\n# -- Project information -----------------------------------------------------\n\nproject = \'zeta-learn\'\ncopyright = \'2018, zeta-team\'\nauthor = \'zeta-team\'\n\n# The short X.Y version\nversion = \'\'\n# The full version, including alpha/beta/rc tags\nrelease = \'1.0.0\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\n\nhtml_context = {\n  ""display_github"": True, # Add \'Edit on Github\' link instead of \'View page source\'\n  ""github_user"": ""jefkine"", # Username\n  ""github_repo"": ""zeta-learn"", # Repo name\n  ""github_version"": ""master"", # Version\n  ""conf_py_path"": ""/docs/"", # Path in the checkout to the docs root\n  ""last_updated"": True,\n  ""commit"": False,\n}\n\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n    \'sphinxcontrib.googleanalytics\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n# html_theme = \'alabaster\'\n\n# import guzzle_sphinx_theme\n#\n# html_theme_path = guzzle_sphinx_theme.html_theme_path()\n# html_theme = \'guzzle_sphinx_theme\'\n#\n# # Register the theme as an extension to generate a sitemap.xml\n# extensions.append(""guzzle_sphinx_theme"")\n\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\nhtml_theme_options = {\n    \'canonical_url\': \'https://zeta-learn.com/\',\n    \'analytics_id\': \'UA-120729203-1\',\n    \'logo_only\': False,\n    \'display_version\': True,\n    \'prev_next_buttons_location\': \'bottom\',\n    \'style_external_links\': False,\n    \'vcs_pageview_mode\': \'\',\n    # Toc options\n    \'collapse_navigation\': False,\n    \'sticky_navigation\': True,\n    \'navigation_depth\': 4,\n    \'includehidden\': True,\n    \'titles_only\': False\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'zeta-learndoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'zeta-learn.tex\', \'zeta-learn Documentation\',\n     \'zeta-team\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'zeta-learn\', \'zeta-learn Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'zeta-learn\', \'zeta-learn Documentation\',\n     author, \'zeta-learn\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n# google analytics\ngoogleanalytics_id = \'UA-120729203-1\'\n'"
examples/test.py,0,b''
ztlearn/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import modules\nfrom . import dl\nfrom . import ml\nfrom . import utils\nfrom . import toolkit\nfrom . import datasets\nfrom . import decayers\nfrom . import objectives\nfrom . import optimizers\nfrom . import activations\nfrom . import initializers\nfrom . import regularizers\n\n\n# optimizers\nfrom .optimizers import SGD\nfrom .optimizers import Adam\nfrom .optimizers import Adamax\nfrom .optimizers import AdaGrad\nfrom .optimizers import RMSprop\nfrom .optimizers import Adadelta\nfrom .optimizers import SGDMomentum\nfrom .optimizers import NesterovAcceleratedGradient\n'
ztlearn/__version__.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nVERSION = (1, 1, 5)\n\n__version__ = '.'.join(map(str, VERSION))\n"""
ztlearn/activations.py,21,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\n\nclass ELU:\n\n    """"""\n    **Exponential Linear Units (ELUs)**\n\n    ELUs are exponential functions which have negative values that allow them to\n    push mean unit activations closer to zero like batch normalization but  with\n    lower computational complexity.\n\n    References:\n        [1] Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\n            * [Djork-Arn\xc3\xa9 Clevert et. al., 2016] https://arxiv.org/abs/1511.07289\n            * [PDF] https://arxiv.org/pdf/1511.07289.pdf\n\n    Args:\n        alpha (float32): controls the value to which an ELU saturates for negative net inputs\n    """"""\n\n    def __init__(self, activation_dict):\n        self.alpha = activation_dict[\'alpha\'] if \'alpha\' in activation_dict else 0.1\n\n    def activation(self, input_signal):\n\n        """"""\n        ELU activation applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the ELU function applied to the input\n        """"""\n\n        return np.where(input_signal >= 0.0, input_signal, np.multiply(np.expm1(input_signal), self.alpha))\n\n    def derivative(self, input_signal):\n\n        """"""\n        ELU derivative applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the ELU derivative applied to the input\n        """"""\n\n        return np.where(input_signal >= 0.0, 1, self.activation(input_signal) + self.alpha)\n\n    @property\n    def activation_name(self):\n        return self.__class__.__name__\n\n\nclass SELU:\n\n    """"""\n    **Scaled Exponential Linear Units (SELUs)**\n\n    SELUs are activations which induce self-normalizing properties and are  used\n    in Self-Normalizing Neural Networks (SNNs). SNNs  enable high-level abstract\n    representations that  tend to  automatically  converge towards zero mean and\n    unit variance.\n\n    References:\n        [1] Self-Normalizing Neural Networks (SELUs)\n            * [Klambauer, G., et. al., 2017] https://arxiv.org/abs/1706.02515\n            * [PDF] https://arxiv.org/pdf/1706.02515.pdf\n    Args:\n        ALPHA (float32)  : 1.6732632423543772848170429916717\n        _LAMBDA (float32): 1.0507009873554804934193349852946\n    """"""\n\n    ALPHA   = 1.6732632423543772848170429916717\n    _LAMBDA = 1.0507009873554804934193349852946\n\n    def __init__(self, activation_dict): pass\n\n    def activation(self, input_signal):\n\n        """"""\n        SELU activation applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the SELU function applied to the input\n        """"""\n\n        return SELU._LAMBDA * np.where(input_signal >= 0.0,\n                                                            input_signal,\n                                                            np.multiply(SELU.ALPHA, np.exp(input_signal)) - SELU.ALPHA)\n\n    def derivative(self, input_signal):\n\n        """"""\n        SELU derivative applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the SELU derivative applied to the input\n        """"""\n\n        return SELU._LAMBDA * np.where(input_signal >= 0.0,\n                                                            1.0,\n                                                            np.multiply(np.exp(input_signal), SELU.ALPHA))\n\n    @property\n    def activation_name(self):\n        return self.__class__.__name__\n\n\nclass ReLU:\n\n    """"""\n    **Rectified Linear Units (ReLUs)**\n\n    Rectifying neurons are an  even better model of biological  neurons yielding\n    equal or better performance than hyperbolic tangent networks in-spite of the\n    hard non-linearity  and  non-differentiability at zero hence creating sparse\n    representations with true zeros which seem remarkably suitable for naturally\n    sparse data.\n\n    References:\n        [1] Deep Sparse Rectifier Neural Networks\n            * [Xavier Glorot., et. al., 2011] http://proceedings.mlr.press/v15/glorot11a.html\n            * [PDF] http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf\n\n        [2] Delving Deep into Rectifiers\n            * [Kaiming He, et. al., 2015] https://arxiv.org/abs/1502.01852\n            * [PDF] https://arxiv.org/pdf/1502.01852.pdf\n    """"""\n\n    def __init__(self, activation_dict): pass\n\n    def activation(self, input_signal):\n\n        """"""\n        ReLU activation applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the ReLU function applied to the input\n        """"""\n\n        return np.where(input_signal >= 0.0, input_signal, 0.0)\n\n    def derivative(self, input_signal):\n\n        """"""\n        ReLU derivative applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the ReLU derivative applied to the input\n        """"""\n\n        return np.where(input_signal >= 0.0, 1.0, 0.0)\n\n    @property\n    def activation_name(self):\n        return self.__class__.__name__\n\n\nclass TanH:\n\n    """"""\n    **Tangent Hyperbolic (TanH)**\n\n    The Tangent Hyperbolic function, a rescaled  version of the sigmoid function\n    that produces  outputs in scale of  [-1, +1]. As  an  activation function it\n    gives an output for every input value hence making is a continuous function.\n\n    References:\n        [1] Hyperbolic Functions\n            * [Mathematics Education Centre] https://goo.gl/4Dkkrd\n            * [PDF] https://goo.gl/xPSnif\n    """"""\n\n    def __init__(self, activation_dict): pass\n\n    def activation(self, input_signal):\n\n        """"""\n        TanH activation applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the TanH function applied to the input\n        """"""\n\n        return np.tanh(input_signal)\n\n    def derivative(self, input_signal):\n\n        """"""\n        TanH derivative applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the TanH derivative applied to the input\n        """"""\n\n        return 1 - np.square(self.activation(input_signal))\n\n    @property\n    def activation_name(self):\n        return self.__class__.__name__\n\n\nclass Sigmoid:\n\n    """"""\n    **Sigmoid Activation Function**\n\n    A Sigmoid function, often used as  the output activation function for binary\n    classification problems as it  outputs  values that  are in the range (0, 1).\n    Sigmoid functions are real-valued and differentiable, producing a curve that\n    is \'S-shaped\' and feature one local minimum, and one local maximum\n\n    References:\n        [1] The influence of the sigmoid function parameters on the speed of backpropagation learning\n            * [PDF] https://goo.gl/MavJjj\n    """"""\n\n    def __init__(self, activation_dict): pass\n\n    def activation(self, input_signal):\n\n        """"""\n        Sigmoid activation applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the Sigmoid function applied to the input\n        """"""\n\n        return np.exp(-np.logaddexp(0, -input_signal))\n\n    def derivative(self, input_signal):\n\n        """"""\n        Sigmoid derivative applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the Sigmoid derivative applied to the input\n        """"""\n\n        output_signal = self.activation(input_signal)\n\n        return np.multiply(output_signal, 1 - output_signal)\n\n    @property\n    def activation_name(self):\n        return self.__class__.__name__\n\n\nclass SoftPlus:\n\n    """"""\n    **SoftPlus Activation Function**\n\n    A Softplus function  is a smooth approximation to the rectifier linear units\n    (ReLUs). Near point 0, it is  smooth and differentiable and produces outputs\n    in scale of (0, +inf).\n\n    References:\n        [1] Incorporating Second-Order Functional Knowledge for Better Option Pricing\n            * [Charles Dugas, et. al., 2001] https://goo.gl/z3jeYc\n            * [PDF] https://goo.gl/z3jeYc\n    """"""\n\n    def __init__(self, activation_dict): pass\n\n    def activation(self, input_signal):\n\n        """"""\n        SoftPlus activation applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the SoftPlus function applied to the input\n        """"""\n\n        return np.logaddexp(0, input_signal)\n\n    def derivative(self, input_signal):\n\n        """"""\n        SoftPlus derivative applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the SoftPlus derivative applied to the input\n        """"""\n\n        return Sigmoid().activation(input_signal)\n\n    @property\n    def activation_name(self):\n        return self.__class__.__name__\n\n\nclass Softmax:\n\n    """"""\n    **Softmax Activation Function**\n\n    The Softmax Activation Function is a generalization of the logistic function\n    that  squashes  the outputs of each  unit to real values in the range [0, 1]\n    but it  also  divides each output such that  the total sum of the outputs is\n    equal to 1.\n\n    References:\n        [1] Softmax Regression\n            * [UFLDL Tutorial] https://goo.gl/1qgqdg\n\n        [2] Deep Learning using Linear Support Vector Machines\n            * [Yichuan Tang, 2015] https://arxiv.org/abs/1306.0239\n            * [PDF] https://arxiv.org/pdf/1306.0239.pdf\n\n        [3] Probabilistic Interpretation of Feedforward Network Outputs\n            * [Mario Costa, 1989] [PDF] https://goo.gl/ZhBY4r\n    """"""\n\n    def __init__(self, activation_dict): pass\n\n    def activation(self, input_signal):\n\n        """"""\n        Softmax activation applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the Softmax function applied to the input\n        """"""\n\n        probs = np.exp(input_signal - np.max(input_signal, axis = -1, keepdims = True))\n\n        return probs / np.sum(probs, axis = -1, keepdims = True)\n\n    def derivative(self, input_signal):\n\n        """"""\n        Softmax derivative applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the Softmax derivative applied to the input\n        """"""\n\n        output_signal = self.activation(input_signal)\n\n        return np.multiply(output_signal, 1 - output_signal)\n\n    @property\n    def activation_name(self):\n        return self.__class__.__name__\n\n\nclass LeakyReLU:\n\n    """"""\n    **LeakyReLU Activation Functions**\n\n    Leaky ReLUs allow a small non-zero gradient to propagate through the network\n    when the  unit is  not active  hence  avoiding bottlenecks that can  prevent\n    learning in the Neural Network.\n\n    References:\n        [1] Rectifier Nonlinearities Improve Neural Network Acoustic Models\n            * [Andrew L. Mass, et. al., 2013] https://goo.gl/k9fhEZ\n            * [PDF] https://goo.gl/v48yXT\n\n        [2] Empirical Evaluation of Rectified Activations in Convolutional Network\n            * [Bing Xu, et. al., 2015] https://arxiv.org/abs/1505.00853\n            * [PDF] https://arxiv.org/pdf/1505.00853.pdf\n\n    Args:\n        alpha (float32): provides for a small non-zero gradient (e.g. 0.01) when the unit is not active.\n    """"""\n\n    def __init__(self, activation_dict):\n        self.alpha = activation_dict[\'alpha\'] if \'alpha\' in activation_dict else 0.01\n\n    def activation(self, input_signal):\n\n        """"""\n        LeakyReLU activation applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the LeakyReLU function applied to the input\n        """"""\n\n        return np.where(input_signal >= 0, input_signal, np.multiply(input_signal, self.alpha))\n\n    def derivative(self, input_signal):\n\n        """"""\n        LeakyReLU derivative applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the LeakyReLU derivative applied to the input\n        """"""\n\n        return np.where(input_signal >= 0, 1, self.alpha)\n\n    @property\n    def activation_name(self):\n        return self.__class__.__name__\n\n\nclass ElliotSigmoid:\n\n    """"""\n    **Elliot Sigmoid Activation Function**\n\n    Elliot Sigmoid squashes each element of the input from  the interval ranging\n    [-inf, inf] to the interval ranging [-1, 1] with an \'S-shaped\' function. The\n    fucntion is fast to  calculate  on simple  computing hardware as it does not\n    require any exponential or trigonometric functions\n\n    References:\n        [1] A better Activation Function for Artificial Neural Networks\n            * [David L. Elliott, et. al., 1993] https://goo.gl/qqBdne\n            * [PDF] https://goo.gl/fPLPcr\n    """"""\n\n    def __init__(self, activation_dict): pass\n\n    def activation(self, input_signal):\n\n        """"""\n        ElliotSigmoid activation applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the ElliotSigmoid function applied to the input\n        """"""\n\n        return np.multiply(input_signal, 0.5) / (1 + np.abs(input_signal)) + 0.5\n\n    def derivative(self, input_signal):\n\n        """"""\n        ElliotSigmoid derivative applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the ElliotSigmoid derivative applied to the input\n        """"""\n\n        return 0.5 / np.square(1 + np.abs(input_signal))\n\n    @property\n    def activation_name(self):\n        return self.__class__.__name__\n\n\nclass Linear:\n\n    """"""\n    **Linear Activation Function**\n\n    Linear Activation  applies  identity  operation on your  data such  that the\n    output data is  proportional to the input data. The function always  returns\n    the same value that was used as its argument.\n\n    References:\n        [1] Identity Function\n            * [Wikipedia Article] https://en.wikipedia.org/wiki/Identity_function\n    """"""\n\n    def __init__(self, activation_dict): pass\n\n    def activation(self, input_signal):\n\n        """"""\n        Linear activation applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the Linear function applied to the input\n        """"""\n\n        return input_signal\n\n    def derivative(self, input_signal):\n\n        """"""\n        Linear derivative applied to input provided\n\n        Args:\n            input_signal (numpy.array): the input numpy array\n\n        Returns:\n            numpy.array: the output of the Linear derivative applied to the input\n        """"""\n\n        return np.eye(input_signal.shape[0], input_signal.shape[1], dtype = np.float32)\n\n    @property\n    def activation_name(self):\n        return self.__class__.__name__\n\n\nclass ActivationFunction:\n\n    _functions = {\n        \'elu\'            : ELU,\n        \'selu\'           : SELU,\n        \'relu\'           : ReLU,\n        \'tanh\'           : TanH,\n        \'linear\'         : Linear,\n        \'identity\'       : Linear,\n        \'sigmoid\'        : Sigmoid,\n        \'softmax\'        : Softmax,\n        \'softplus\'       : SoftPlus,\n        \'leaky_relu\'     : LeakyReLU,\n        \'elliot_sigmoid\' : ElliotSigmoid\n    }\n\n    def __init__(self, name, activation_dict = {}):\n        if name not in self._functions.keys():\n            raise Exception(\'Activation function must be either one of the following: {}.\'.format(\', \'.join(self._functions.keys())))\n        self.activation_func = self._functions[name](activation_dict)\n\n    @property\n    def name(self):\n        return self.activation_func.activation_name\n\n    def forward(self, input_signal):\n        return self.activation_func.activation(input_signal) # returns tuples\n\n    def backward(self, input_signal):\n        return self.activation_func.derivative(input_signal)\n'"
ztlearn/decayers.py,5,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\n\nclass Decay(object):\n\n    def __init__(self, lr, decay, epoch, min_lr, max_lr):\n        self.lr     = lr\n        self.decay  = decay\n        self.epoch  = epoch\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n\n    @property\n    def clip_lr(self):\n        return np.clip(self.lr, self.min_lr, self.max_lr)\n\n\nclass InverseTimeDecay(Decay):\n\n    def __init__(self, lr, decay, epoch, min_lr, max_lr, step_size):\n        super(InverseTimeDecay, self).__init__(lr, decay, epoch, min_lr, max_lr)\n\n    @property\n    def decompose(self):\n        self.lr *= (1. / (1 + self.decay * self.epoch))\n\n        return super(InverseTimeDecay, self).clip_lr\n\n    @property\n    def decay_name(self):\n        return self.__class__.__name__\n\n\nclass StepDecay(Decay):\n    """""" decay the learning rate every after step_size steps """"""\n\n    def __init__(self, lr, decay, epoch, min_lr, max_lr, step_size):\n        super(StepDecay, self).__init__(lr, decay, epoch, min_lr, max_lr)\n        self.step_size = step_size\n\n    @property\n    def decompose(self):\n        self.lr *= np.power(self.decay, ((1 + self.epoch) // self.step_size))\n\n        return super(StepDecay, self).clip_lr\n\n    @property\n    def decay_name(self):\n        return self.__class__.__name__\n\n\nclass ExponetialDecay(Decay):\n\n    def __init__(self, lr, decay, epoch, min_lr, max_lr, step_size):\n        super(ExponetialDecay, self).__init__(lr, decay, epoch, min_lr, max_lr)\n\n    @property\n    def decompose(self):\n        self.lr *= np.power(self.decay, self.epoch)\n\n        return super(ExponetialDecay, self).clip_lr\n\n    @property\n    def decay_name(self):\n        return self.__class__.__name__\n\n\nclass NaturalExponentialDecay(Decay):\n\n    def __init__(self, lr, decay, epoch, min_lr, max_lr, step_size):\n        super(NaturalExponentialDecay, self).__init__(lr, decay, epoch, min_lr, max_lr)\n\n    @property\n    def decompose(self):\n        self.lr *= np.exp(-self.decay * self.epoch)\n\n        return super(NaturalExponentialDecay, self).clip_lr\n\n    @property\n    def decay_name(self):\n        return self.__class__.__name__\n\n\nclass DecayFunction:\n\n    _functions = {\n        \'step\'                      : StepDecay,\n        \'step_decay\'                : StepDecay,\n        \'exponential\'               : ExponetialDecay,\n        \'exponential_decay\'         : ExponetialDecay,\n        \'inverse\'                   : InverseTimeDecay,\n        \'inverse_time_decay\'        : InverseTimeDecay,        \n        \'nat_exponential\'           : NaturalExponentialDecay,\n        \'natural_exponential_decay\' : NaturalExponentialDecay\n    }\n\n    def __init__(self,\n                       lr        = 0.001,\n                       name      = \'inverse\',\n                       decay     = 1e-6,\n                       epoch     = 1,\n                       min_lr    = 0.,\n                       max_lr    = np.inf,\n                       step_size = 10.0):\n\n        if name not in self._functions.keys():\n            raise Exception(\'Decay function must be either one of the following: {}.\'.format(\', \'.join(self._functions.keys())))\n        self.decay_func = self._functions[name](lr, decay, epoch, min_lr, max_lr, step_size)\n\n    @property\n    def name(self):\n        return self.decay_func.decay_name\n\n    @property\n    def decompose(self):\n        return self.decay_func.decompose\n'"
ztlearn/initializers.py,28,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\n\nclass WeightInitializer:\n\n    def compute_fans(self, shape):\n\n        """"""\n        func: compute_fans adapted from keras: https://github.com/fchollet/keras/blob/master/keras/initializers.py\n        copyright held by fchollet(keras-team), 2017 as part of Keras project\n        licence: MIT\n        """"""\n\n        # kernel shape: (\'NF\': Total Filters, \'CF\': Filter Channels, \'HF\': Filter Height \'WF\': Filter Width)\n\n        shape                = (shape[0], 1) if len(shape) ==  1 else shape\n        receptive_field_size = np.prod(shape[:2])\n        fan_out              = shape[0] * receptive_field_size # NF *receptive_field_size\n        fan_in               = shape[1] * receptive_field_size # CF *receptive_field_size\n\n        return fan_in, fan_out\n\n\nclass HeNormal(WeightInitializer):\n\n    """"""\n    **He Normal (HeNormal)**\n\n    HeNormal is a robust initialization  method that  particularly considers the\n    rectifier nonlinearities.  He normal is an  implementation based on Gaussian\n    distribution\n\n    References:\n        [1] Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n            * [Kaiming He, 2015] https://arxiv.org/abs/1502.01852\n            * [PDF] https://arxiv.org/pdf/1502.01852.pdf\n\n        [2] Initialization Of Deep Networks Case of Rectifiers\n            * [DeepGrid Article - Jefkine Kafunah] https://goo.gl/TBNw5t\n    """"""\n\n    def weights(self, shape, random_seed):\n        fan_in, fan_out = self.compute_fans(shape)\n        scale           = np.sqrt(2. / fan_in)\n\n        np.random.seed(random_seed)\n\n        return np.random.normal(loc = 0.0, scale = scale, size = shape)\n\n    @property\n    def init_name(self):\n        return self.__class__.__name__\n\n\nclass HeUniform(WeightInitializer):\n\n    """"""\n    **He Normal (HeNormal)**\n\n    HeNormal is a robust  initialization method  that particularly considers the\n    rectifier  nonlinearities. He uniform  is an implementation based on Uniform\n    distribution\n\n    References:\n        [1] Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n            * [Kaiming He, 2015] https://arxiv.org/abs/1502.01852\n            * [PDF] https://arxiv.org/pdf/1502.01852.pdf\n\n        [2] Initialization Of Deep Networks Case of Rectifiers\n            * [DeepGrid Article - Jefkine Kafunah] https://goo.gl/TBNw5t\n    """"""\n\n    def weights(self, shape, random_seed):\n        fan_in, fan_out = self.compute_fans(shape)\n        scale           = np.sqrt(6. / fan_in)\n\n        np.random.seed(random_seed)\n\n        return np.random.uniform(low = -scale, high = scale, size = shape)\n\n    @property\n    def init_name(self):\n        return self.__class__.__name__\n\n\nclass GlorotNormal(WeightInitializer):\n\n    """"""\n    **Glorot Normal (GlorotNormal)**\n\n    GlorotNormal, more famously known as the  Xavier initialization is  based on\n    the effort to try mantain the  same variance of the gradients of the weights\n    for all  the  layers. Glorot normal is an implementation  based  on Gaussian\n    distribution\n\n    References:\n        [1] Understanding the difficulty of training deep feedforward neural networks\n            * [Xavier Glorot, 2010] http://proceedings.mlr.press/v9/glorot10a.html\n            * [PDF] http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n\n        [2] Initialization Of Deep Feedfoward Networks\n            * [DeepGrid Article - Jefkine Kafunah] https://goo.gl/E2XrGe\n    """"""\n\n    def weights(self, shape, random_seed):\n        fan_in, fan_out = self.compute_fans(shape)\n        scale           = np.sqrt(2. / (fan_in + fan_out))\n\n        np.random.seed(random_seed)\n\n        return np.random.normal(loc = 0.0, scale = scale, size = shape)\n\n    @property\n    def init_name(self):\n        return self.__class__.__name__\n\n\nclass GlorotUniform(WeightInitializer):\n\n    """"""\n    **Glorot Uniform (GlorotUniform)**\n\n    GlorotUniform, more famously known as  the Xavier initialization is based on\n    the effort to try mantain the same  variance of the gradients of the weights\n    for all the layers. Glorot uniform is  an  implementation based  on  Uniform\n    distribution\n\n    References:\n        [1] Understanding the difficulty of training deep feedforward neural networks\n            * [Xavier Glorot, 2010] http://proceedings.mlr.press/v9/glorot10a.html\n            * [PDF] http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n\n        [2] Initialization Of Deep Feedfoward Networks\n            * [DeepGrid Article - Jefkine Kafunah] https://goo.gl/E2XrGe\n    """"""\n\n    def weights(self, shape, random_seed):\n        fan_in, fan_out = self.compute_fans(shape)\n        scale           = np.sqrt(6. / (fan_in + fan_out))\n\n        np.random.seed(random_seed)\n\n        return np.random.uniform(low = -scale, high = scale, size = shape)\n\n    @property\n    def init_name(self):\n        return self.__class__.__name__\n\n\nclass LeCunUniform(WeightInitializer):\n\n    """"""\n    **LeCun Uniform (LeCunUniform)**\n\n    Weights  should be  randomly chosen  but in  such a way that the sigmoid  is\n    primarily activated in its linear region. LeCun uniform is an implementation\n    based on Uniform distribution\n\n    References:\n        [1] Efficient Backprop\n            * [LeCun, 1998][PDF] http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n    """"""\n\n    def weights(self, shape, random_seed):\n        fan_in, fan_out = self.compute_fans(shape)\n        scale           = np.sqrt(3. / fan_in)\n\n        np.random.seed(random_seed)\n\n        return np.random.uniform(low = -scale, high = scale, size = shape)\n\n    @property\n    def init_name(self):\n        return self.__class__.__name__\n\n\nclass LeCunNormal(WeightInitializer):\n\n    """"""\n    **LeCun Normal (LeCunNormal)**\n\n    Weights  should  be  randomly chosen  but in such a  way that the sigmoid is\n    primarily activated in its linear region. LeCun uniform is an implementation\n    based on Gaussian distribution\n\n    References:\n        [1] Efficient Backprop\n            * [LeCun, 1998][PDF] http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n    """"""\n\n    def weights(self, shape, random_seed):\n        fan_in, fan_out = self.compute_fans(shape)\n        scale           = np.sqrt(1. / fan_in)\n\n        np.random.seed(random_seed)\n\n        return np.random.normal(loc = -scale, scale = scale, size = shape)\n\n    @property\n    def init_name(self):\n        return self.__class__.__name__\n\n\nclass RandomUniform(WeightInitializer):\n\n    """"""\n    **Random Uniform (RandomUniform)**\n\n    Random uniform, an implementation  of weight initialization based on Uniform\n    distribution\n    """"""\n\n    def weights(self, shape, random_seed):\n        fan_in, fan_out = self.compute_fans(shape)\n        scale           = np.sqrt(1. / (fan_in + fan_out))\n\n        np.random.seed(random_seed)\n\n        return np.random.uniform(low = -scale, high = scale, size = shape)\n\n    @property\n    def init_name(self):\n        return self.__class__.__name__\n\n\nclass RandomNormal(WeightInitializer):\n\n    """"""\n    **Random Normal (RandomNormal)**\n\n    Random uniform, an implementation of weight initialization based on Gaussian\n    distribution\n    """"""\n\n    def weights(self, shape, random_seed):\n        fan_in, fan_out = self.compute_fans(shape)\n        scale           = np.sqrt(1. / (fan_in + fan_out))\n\n        np.random.seed(random_seed)\n\n        return np.random.normal(loc = 0.0, scale = scale, size = shape)\n\n    @property\n    def init_name(self):\n        return self.__class__.__name__\n\n\nclass Zero(WeightInitializer):\n\n    """"""\n    **Zero (Zero)**\n\n    Zero is an implementation of weight initialization that returns all zeros\n    """"""\n\n    def weights(self, shape, random_seed):\n        return np.zeros(shape = shape)\n\n    @property\n    def init_name(self):\n        return self.__class__.__name__\n\n\nclass One(WeightInitializer):\n\n    """"""\n    **One (One)**\n\n    One is an implementation of weight initialization that returns all ones\n    """"""\n\n    def weights(self, shape, random_seed):\n        return np.ones(shape = shape)\n\n    @property\n    def init_name(self):\n        return self.__class__.__name__\n\n\nclass Identity(WeightInitializer):\n\n    """"""\n    **Identity (Identity)**\n\n    Identity is an implementation of weight initialization that returns an\n    identity matrix of size shape\n    """"""\n\n    def weights(self, shape, random_seed):\n        return np.eye(shape[0], shape[1], dtype = np.float32)\n\n    @property\n    def init_name(self):\n        return self.__class__.__name__\n\n\nclass InitializeWeights:\n\n    _methods = {\n        \'ones\'           : One,\n        \'zeros\'          : Zero,\n        \'identity\'       : Identity,\n        \'he_normal\'      : HeNormal,\n        \'he_uniform\'     : HeUniform,\n        \'lecun_normal\'   : LeCunNormal,\n        \'lecun_uniform\'  : LeCunUniform,\n        \'random_normal\'  : RandomNormal,\n        \'glorot_normal\'  : GlorotNormal,\n        \'random_uniform\' : RandomUniform,\n        \'glorot_uniform\' : GlorotUniform\n    }\n\n    def __init__(self, name):\n        if name not in self._methods.keys():\n            raise Exception(\'Weight initialization method must be either one of the following: {}.\'.format(\', \'.join(self._methods.keys())))\n        self.init_method = self._methods[name]()\n\n    @property\n    def name(self):\n        return self.init_method.init_name\n\n    def initialize_weights(self, shape, random_seed = None):\n        return self.init_method.weights(shape, random_seed)\n'"
ztlearn/objectives.py,36,"b'# -*- coding: utf-8 -*-\n\nimport math as mt\nimport numpy as np\n\n\nclass Objective(object):\n\n    def clip(self, predictions, epsilon = 1e-15):\n        clipped_predictions = np.clip(predictions, epsilon, 1 - epsilon)\n        clipped_divisor     = np.maximum(np.multiply(predictions, 1 - predictions), epsilon)\n\n        return clipped_predictions, clipped_divisor\n\n    def error(self, predictions, targets):\n        error     = targets - predictions\n        abs_error = np.absolute(error)\n\n        return error, abs_error\n\n    def add_fuzz_factor(self, np_array, epsilon = 1e-05):\n        return np.add(np_array, epsilon)\n\n    @property\n    def objective_name(self):\n        return self.__class__.__name__\n\n\nclass MeanSquaredError:\n\n    """"""\n    **Mean Squared error (MSE)**\n\n    MSE measures  the average squared difference between the predictions and the\n    targets. The closer  the predictions are to  the targets  the more efficient\n    the estimator.\n\n    References:\n        [1] Mean Squared error\n            * [Wikipedia Article] https://en.wikipedia.org/wiki/Mean_squared_error\n    """"""\n\n    def loss(self, predictions, targets, np_type):\n\n        """"""\n        Applies the MeanSquaredError Loss to prediction and targets provided\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.array: the output of MeanSquaredError Loss to prediction and targets\n        """"""\n\n        return 0.5 * np.mean(np.sum(np.square(predictions - targets), axis = 1))\n\n    def derivative(self, predictions, targets, np_type):\n\n        """"""\n        Applies the MeanSquaredError Derivative to prediction and targets provided\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.array: the output of MeanSquaredError Derivative to prediction and targets\n        """"""\n\n        return predictions - targets\n\n    def accuracy(self, predictions, targets, threshold = 0.5):\n        return 0\n\n    @property\n    def objective_name(self):\n        return self.__class__.__name__\n\n\nclass HellingerDistance:\n\n    """"""\n    **Hellinger Distance**\n\n    Hellinger Distance is used to quantify the similarity between two probability\n    distributions.\n\n    References:\n        [1] Hellinger Distance\n            * [Wikipedia Article] https://en.wikipedia.org/wiki/Hellinger_distance\n    """"""\n\n    SQRT_2 = np.sqrt(2)\n\n    def sqrt_difference(self, predictions, targets):\n        return np.sqrt(predictions) - np.sqrt(targets)\n\n    def loss(self, predictions, targets, np_type):\n\n        """"""\n        Applies the HellingerDistance Loss to prediction and targets provided\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.array: the output of HellingerDistance Loss to prediction and targets\n        """"""\n\n        root_difference = self.sqrt_difference(predictions, targets)\n\n        return np.mean(np.true_divide(np.sum(np.square(root_difference), axis = 1), HellingerDistance.SQRT_2))\n\n    def derivative(self, predictions, targets, np_type):\n\n        """"""\n        Applies the HellingerDistance Derivative to prediction and targets provided\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.array: the output of HellingerDistance Derivative to prediction and targets\n        """"""\n\n        root_difference = self.sqrt_difference(predictions, targets)\n\n        return np.true_divide(root_difference, np.multiply(HellingerDistance.SQRT_2, np.sqrt(predictions)))\n\n\n    def accuracy(self, predictions, targets, threshold = 0.5):\n        return 0\n\n    @property\n    def objective_name(self):\n        return self.__class__.__name__\n\n\nclass HingeLoss:\n\n    """"""\n    **Hinge Loss**\n\n    Hinge Loss  also known  as  SVM Loss is used ""maximum-margin"" classification,\n    most notably for support vector machines (SVMs)\n\n    References:\n        [1] Hinge loss\n            * [Wikipedia Article] https://en.wikipedia.org/wiki/Hinge_loss\n    """"""\n\n    def loss(self, predictions, targets, np_type):\n\n        """"""\n        Applies the Hinge-Loss to Loss prediction and targets provided\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.array: the output of Hinge-Loss Loss to prediction and targets\n        """"""\n\n        correct_class = predictions[np.arange(predictions.shape[0]), np.argmax(targets, axis = 1)]\n        margins       = np.maximum(0, predictions - correct_class[:, np.newaxis] + 1.0) # delta = 1.0\n\n        margins[np.arange(predictions.shape[0]), np.argmax(targets, axis = 1)] = 0\n\n        return np.mean(np.sum(margins, axis = 0))\n\n    def derivative(self, predictions, targets, np_type):\n\n        """"""\n        Applies the Hinge-Loss Derivative to prediction and targets provided\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.array: the output of Hinge-Loss Derivative to prediction and targets\n        """"""\n\n        correct_class = predictions[np.arange(predictions.shape[0]), np.argmax(targets, axis = 1)]\n        binary        = np.maximum(0, predictions - correct_class[:, np.newaxis] + 1.0) # delta = 1.0\n\n        binary[binary > 0] = 1\n        incorrect_class    = np.sum(binary, axis = 1)\n\n        binary[np.arange(predictions.shape[0]), np.argmax(targets, axis = 1)] = -incorrect_class\n\n        return binary\n\n    def accuracy(self, predictions, targets, threshold = 0.5):\n\n        """"""\n        Calculates the Hinge-Loss Accuracy Score given prediction and targets\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.float32: the output of Hinge-Loss Accuracy Score\n        """"""\n\n        return np.mean(np.argmax(predictions, axis = 1) == np.argmax(targets, axis = 1))\n\n    @property\n    def objective_name(self):\n        return self.__class__.__name__\n\n\nclass BinaryCrossEntropy(Objective):\n\n    """"""\n    **Binary Cross Entropy**\n\n    Binary CrossEntropy measures the performance of a classification model whose\n    output is a probability value between 0 & 1. \'Binary\' is meant for  discrete\n    classification  tasks in which  the classes are independent and not mutually\n    exclusive. Targets here could be either 0 or 1 scalar\n\n    References:\n        [1] Cross Entropy\n            * [Wikipedia Article] https://en.wikipedia.org/wiki/Cross_entropy\n    """"""\n\n    def loss(self, predictions, targets, np_type):\n\n        """"""\n        Applies the BinaryCrossEntropy Loss to prediction and targets provided\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.array: the output of BinaryCrossEntropy Loss to prediction and targets\n        """"""\n\n        clipped_predictions, _ = super(BinaryCrossEntropy, self).clip(predictions)\n\n        return  np.mean(\n                    -np.sum(\n                        (\n                            np.multiply(targets, np.log(clipped_predictions)),\n                            np.multiply((1 - targets), np.log(1 - clipped_predictions))\n                        ),  axis = 1\n                    )\n                )\n\n    def derivative(self, predictions, targets, np_type):\n\n        """"""\n        Applies the BinaryCrossEntropy Derivative to prediction and targets provided\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.array: the output of BinaryCrossEntropy Derivative to prediction and targets\n        """"""\n\n        clipped_predictions, clipped_divisor = super(BinaryCrossEntropy, self).clip(predictions)\n\n        return np.true_divide((clipped_predictions - targets), clipped_divisor)\n\n    def accuracy(self, predictions, targets, threshold = 0.5):\n\n        """"""\n        Calculates the BinaryCrossEntropy Accuracy Score given prediction and targets\n\n        Args:\n            predictions (numpy.array)  : the predictions numpy array\n            targets     (numpy.array)  : the targets numpy array\n            threshold   (numpy.float32): the threshold value\n\n        Returns:\n            numpy.float32: the output of BinaryCrossEntropy Accuracy Score\n        """"""\n\n        return 1 - np.true_divide(np.count_nonzero((predictions > threshold) == targets), float(targets.size))\n\n    @property\n    def objective_name(self):\n        return self.__class__.__name__\n\n\nclass CategoricalCrossEntropy(Objective):\n\n    """"""\n    **Categorical Cross Entropy**\n\n    Categorical Cross Entropy measures the performance of a classification model\n    whose  output  is a  probability  value  between  0 and 1.  \'Categorical\' is\n    meant for discrete  classification tasks  in which  the classes are mutually\n    exclusive.\n\n    References:\n        [1] Cross Entropy\n            * [Wikipedia Article] https://en.wikipedia.org/wiki/Cross_entropy\n    """"""\n\n    def loss(self, predictions, targets, np_type):\n\n        """"""\n        Applies the CategoricalCrossEntropy Loss to prediction and targets provided\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.array: the output of CategoricalCrossEntropy Loss to prediction and targets\n        """"""\n\n        clipped_predictions, _ = super(CategoricalCrossEntropy, self).clip(predictions)\n\n        return np.mean(-np.sum(np.multiply(targets, np.log(clipped_predictions)), axis = 1))\n\n    def derivative(self, predictions, targets, np_type):\n\n        """"""\n        Applies the CategoricalCrossEntropy Derivative to prediction and targets provided\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.array: the output of CategoricalCrossEntropy Derivative to prediction and targets\n        """"""\n\n        clipped_predictions, _ = super(CategoricalCrossEntropy, self).clip(predictions)\n\n        return clipped_predictions - targets\n\n    def accuracy(self, predictions, targets):\n\n        """"""\n        Calculates the CategoricalCrossEntropy Accuracy Score given prediction and targets\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.float32: the output of CategoricalCrossEntropy Accuracy Score\n        """"""\n\n        return np.mean(np.argmax(predictions, axis = 1) == np.argmax(targets, axis = 1))\n\n    @property\n    def objective_name(self):\n        return self.__class__.__name__\n\n\nclass KLDivergence(Objective):\n\n    """"""\n    **KL Divergence**\n\n     Kullback\xe2\x80\x93Leibler  divergence (also called relative entropy) is a measure of\n     divergence between two probability distributions.\n\n    """"""\n\n    def loss(self, predictions, targets, np_type):\n\n        """"""\n        Applies the KLDivergence Loss to prediction and targets provided\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.array: the output of KLDivergence Loss to prediction and targets\n        """"""\n\n        targets     = super(KLDivergence, self).add_fuzz_factor(targets)\n        predictions = super(KLDivergence, self).add_fuzz_factor(predictions)\n\n        return np.sum(np.multiply(targets, np.log(np.true_divide(targets, predictions))), axis = 1)\n\n    def derivative(self, predictions, targets, np_type):\n\n        """"""\n        Applies the KLDivergence Derivative to prediction and targets provided\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.array: the output of KLDivergence Derivative to prediction and targets\n        """"""\n\n        targets     = super(KLDivergence, self).add_fuzz_factor(targets)\n        predictions = super(KLDivergence, self).add_fuzz_factor(predictions)\n\n        d_log_diff = np.multiply((predictions - targets), (np.log(np.true_divide(targets, predictions))))\n\n        return np.multiply((1 + np.log(np.true_divide(targets, predictions))), d_log_diff)\n\n    def accuracy(self, predictions, targets):\n\n        """"""\n        Calculates the KLDivergence Accuracy Score given prediction and targets\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.float32: the output of KLDivergence Accuracy Score\n        """"""\n\n        return np.mean(np.argmax(predictions, axis = 1) == np.argmax(targets, axis = 1))\n\n    @property\n    def objective_name(self):\n        return self.__class__.__name__\n\n\nclass HuberLoss(Objective):\n\n    """"""\n    **Huber Loss**\n\n     Huber Loss: is a loss function  used in robust regression where it is found\n     to be less sensitive to outliers in data than the squared error loss.\n\n     References:\n         [1] Huber Loss\n             * [Wikipedia Article] https://en.wikipedia.org/wiki/Huber_loss\n\n         [2] Huber loss\n             * [Wikivisually Article] https://wikivisually.com/wiki/Huber_loss\n\n    """"""\n\n    def loss(self, predictions, targets, np_type, delta = 1.):\n\n        """"""\n        Applies the HuberLoss Loss to prediction and targets provided\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.array: the output of KLDivergence Loss to prediction and targets\n        """"""\n\n        error, abs_error = super(HuberLoss, self).error(predictions, targets)\n\n        return np.sum(np.where(abs_error < delta,\n                                                  0.5 * (np.square(error)),\n                                                  delta * abs_error - 0.5 * (mt.pow(delta, 2))))\n\n    def derivative(self, predictions, targets, np_type, delta = 1.):\n\n        """"""\n        Applies the HuberLoss Derivative to prediction and targets provided\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.array: the output of KLDivergence Derivative to prediction and targets\n        """"""\n\n        error, abs_error = super(HuberLoss, self).error(predictions, targets)\n\n        return np.sum(np.where(abs_error > delta, delta * np.sign(error), error))\n\n    def accuracy(self, predictions, targets):\n\n        """"""\n        Calculates the HuberLoss Accuracy Score given prediction and targets\n\n        Args:\n            predictions (numpy.array): the predictions numpy array\n            targets     (numpy.array): the targets numpy array\n\n        Returns:\n            numpy.float32: the output of KLDivergence Accuracy Score\n        """"""\n\n        return np.mean(predictions - targets)\n\n    @property\n    def objective_name(self):\n        return self.__class__.__name__\n\n\nclass ObjectiveFunction:\n\n    _functions = {\n        \'svm\'                         : HingeLoss,\n        \'hinge\'                       : HingeLoss,\n        \'hinge_loss\'                  : HingeLoss,\n        \'huber\'                       : HuberLoss,\n        \'huber_loss\'                  : HuberLoss,        \n        \'kld\'                         : KLDivergence,\n        \'kullback_leibler_divergence\' : KLDivergence,\n        \'mse\'                         : MeanSquaredError,\n        \'mean_squared_error\'          : MeanSquaredError,\n        \'hld\'                         : HellingerDistance,\n        \'hellinger_distance\'          : HellingerDistance,\n        \'bce\'                         : BinaryCrossEntropy,\n        \'binary_crossentropy\'         : BinaryCrossEntropy,\n        \'cce\'                         : CategoricalCrossEntropy,\n        \'categorical_crossentropy\'    : CategoricalCrossEntropy\n    }\n\n    def __init__(self, name):\n        if name not in self._functions.keys():\n            raise Exception(\'Objective function must be either one of the following: {}.\'.format(\', \'.join(self._functions.keys())))\n        self.objective_func = self._functions[name]()\n\n    @property\n    def name(self):\n        return self.objective_func.objective_name\n\n    def forward(self, predictions, targets, np_type = np.float32):\n        return self.objective_func.loss(predictions, targets, np_type)\n\n    def backward(self, predictions, targets, np_type = np.float32):\n        return self.objective_func.derivative(predictions, targets, np_type)\n\n    def accuracy(self, predictions, targets):\n        return self.objective_func.accuracy(predictions, targets)\n'"
ztlearn/optimizers.py,48,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom .decayers import DecayFunction as decayer\n\n\nclass Optimizer(object):\n\n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\n\n    def get_learning_rate(self, current_epoch):\n        self.min_lr     = self.min_lr     if hasattr(self, \'min_lr\')     else 0\n        self.max_lr     = self.max_lr     if hasattr(self, \'max_lr\')     else np.inf\n        self.decay_rate = self.decay_rate if hasattr(self, \'decay_rate\') else 1e-6\n        self.decay_func = self.decay_func if hasattr(self, \'decay_func\') else \'inverse\'\n        self.decay_lr   = self.decay_lr   if hasattr(self, \'decay_lr\')   else True\n\n        if self.decay_lr is False: return self.lr\n\n        if current_epoch == 1: return self.lr\n\n        if hasattr(self, \'step_size\') and isinstance(self.step_size, (int, np.integer)):\n\n            return decayer(self.lr,\n                                    self.decay_func,\n                                    self.decay_rate,\n                                    current_epoch,\n                                    self.min_lr,\n                                    self.max_lr,\n                                    self.step_size).decompose\n\n        return decayer(self.lr,\n                                self.decay_func,\n                                self.decay_rate,\n                                current_epoch,\n                                self.min_lr,\n                                self.max_lr).decompose\n\n\nclass GD:\n\n    """"""\n    **Gradient Descent (GD)**\n\n    GD optimizes parameters theta of an objective function J(theta) by  updating\n    all of the  training samples in the  dataset. The update is perfomed in  the\n    opposite  direction of  the  gradient of the  objective  function  d/d_theta\n    J(theta) - with respect  to  the parameters (theta).  The learning rate  eta\n    helps determine the size of teh steps we take to the minima\n\n    References:\n        [1] An overview of gradient descent optimization algorithms\n            * [Sebastien Ruder, 2016] https://arxiv.org/abs/1609.04747\n            * [PDF] https://arxiv.org/pdf/1609.04747.pdf\n    """"""\n\n    def __init__(self): pass\n\n\nclass SGD(Optimizer):\n\n    """"""\n    **Stochastic Gradient Descent (SGD)**\n\n    SGD optimizes parameters theta of an objective function J(theta) by updating\n    each of the training samples inputs(i) and targets(i) for all samples in the\n    dataset. The update is perfomed in the opposite direction of the gradient of\n    the objective function d/d_theta J(theta) - with respect  to the  parameters\n    (theta). The learning rate eta helps determine the size of the steps we take\n    to the minima\n\n    References:\n        [1] An overview of gradient descent optimization algorithms\n            * [Sebastien Ruder, 2016] https://arxiv.org/abs/1609.04747\n            * [PDF] https://arxiv.org/pdf/1609.04747.pdf\n\n        [2] Large-Scale Machine Learning with Stochastic Gradient Descent\n            * [Leon Botou, 2011][PDF] http://leon.bottou.org/publications/pdf/compstat-2010.pdf\n\n    Args:\n        kwargs: Arbitrary keyword arguments.\n    """"""\n\n    def __init__(self, **kwargs):\n        super(SGD, self).__init__(**kwargs)\n        self.lr = kwargs[\'lr\'] if \'lr\' in kwargs else 0.01\n\n    def update(self, weights, grads, epoch_num, batch_num, batch_size):\n        self.weights = weights\n        self.grads   = grads\n\n        self.weights -= np.multiply(super(SGD, self).get_learning_rate(epoch_num), self.grads, dtype = np.float128)\n\n        return self.weights\n\n    @property\n    def optimization_name(self):\n        return self.__class__.__name__\n\n\nclass SGDMomentum(Optimizer):\n\n    """"""\n    **Stochastic Gradient Descent with Momentum (SGDMomentum)**\n\n    The objective function  regularly forms  places on  the contour map in which\n    the surface  curves more steeply  than  others (ravines). Standard SGD  will\n    tend to  oscillate across the  narrow  ravine since  the  negative  gradient\n    will  point  down one  of  the  steep  sides  rather than  along the  ravine\n    towards  the  optimum.  Momentum  hepls to  push the  objective more quickly\n    along the shallow ravine towards the global minima\n\n    References:\n        [1] An overview of gradient descent optimization algorithms\n            * [Sebastien Ruder, 2016] https://arxiv.org/abs/1609.04747\n            * [PDF] https://arxiv.org/pdf/1609.04747.pdf\n\n        [2] On the Momentum Term in Gradient Descent Learning Algorithms\n            * [Ning Qian, 199] https://goo.gl/7fhr14\n            * [PDF] https://goo.gl/91HtDt\n\n        [3] Two problems with backpropagation and other steepest-descent learning procedures for networks.\n            * [Sutton, R. S., 1986][PDF] https://goo.gl/M3VFM1\n\n    Args:\n        kwargs: Arbitrary keyword arguments.\n    """"""\n\n    def __init__(self, **kwargs):\n        super(SGDMomentum, self).__init__(**kwargs)\n        self.lr       = kwargs[\'lr\']       if \'lr\'       in kwargs else 0.01\n        self.momentum = kwargs[\'momentum\'] if \'momemtum\' in kwargs else 0.1\n        self.velocity = None\n\n    def update(self, weights, grads, epoch_num, batch_num, batch_size):\n        self.weights = weights\n        self.grads   = grads\n\n        if self.velocity is None:\n            self.velocity = np.zeros_like(self.weights)\n\n        self.velocity  = np.subtract(\n                            np.multiply(self.momentum, self.velocity),\n                            np.multiply(super(SGDMomentum, self).get_learning_rate(epoch_num), self.grads)\n                         )\n        self.weights  += self.velocity\n\n        return self.weights\n\n    @property\n    def optimization_name(self):\n        return self.__class__.__name__\n\n\nclass Adam(Optimizer):\n\n    """"""\n    **Adaptive Moment Estimation (Adam)**\n\n    Adam computes adaptive  learning rates for by  updating each of the training\n    samples while storing  an  exponentially  decaying  average of past  squared\n    gradients. Adam  also  keeps  an  exponentially  decaying  average  of  past\n    gradients.\n\n    References:\n        [1] An overview of gradient descent optimization algorithms\n            * [Sebastien Ruder, 2016] https://arxiv.org/abs/1609.04747\n            * [PDF] https://arxiv.org/pdf/1609.04747.pdf\n\n        [2] Adam: A Method for Stochastic Optimization\n            * [Diederik P. Kingma et. al., 2014] https://arxiv.org/abs/1412.6980\n            * [PDF] https://arxiv.org/pdf/1412.6980.pdf\n\n    Args:\n        kwargs: Arbitrary keyword arguments.\n    """"""\n\n    def __init__(self, **kwargs):\n        super(Adam, self).__init__(**kwargs)\n        self.lr      = kwargs[\'lr\']      if \'lr\'      in kwargs else 0.001\n        self.epsilon = kwargs[\'epsilon\'] if \'epsilon\' in kwargs else 1e-8\n        self.beta1   = kwargs[\'beta1\']   if \'beta1\'   in kwargs else 0.9\n        self.beta2   = kwargs[\'beta2\']   if \'beta2\'   in kwargs else 0.999\n        self.m       = None\n        self.v       = None\n        self.t       = 1\n\n    def update(self, weights, grads, epoch_num, batch_num, batch_size):\n        self.weights = weights\n        self.grads   = grads\n        self.t       = batch_num\n\n        if self.m is None:\n            self.m = np.zeros_like(self.weights)\n\n        if self.v is None:\n            self.v = np.zeros_like(self.weights)\n\n        self.m = np.multiply(self.beta1, self.m) + np.multiply((1 - self.beta1), self.grads)\n        m_hat  = np.true_divide(self.m, (1 - np.power(self.beta1, self.t)))\n\n        self.v = np.multiply(self.beta2, self.v)  + np.multiply((1 - self.beta2), np.square(self.grads))\n        v_hat  = np.true_divide(self.v, (1 - np.power(self.beta2, self.t)))\n\n        self.weights -= np.true_divide(\n                           np.multiply(super(Adam, self).get_learning_rate(epoch_num), m_hat),\n                           (np.sqrt(v_hat) + self.epsilon)\n                        )\n\n        return self.weights\n\n    @property\n    def optimization_name(self):\n        return self.__class__.__name__\n\n\nclass Adamax(Optimizer):\n\n    """"""\n    **Admax**\n\n    AdaMax is a variant of Adam based on the infinity norm. The Adam update rule\n    for individual weights is to scale their gradients inversely proportional to\n    a (scaled) L2  norm of  their  individual c urrent and  past  gradients. For\n    Adamax we generalize the L2 norm based update rule to a Lp norm based update\n    rule. These variants are numerically unstable for large p.  but have special\n    cases where as p tens to infinity, a simple and stable algorithm emerges.\n\n    References:\n        [1] An overview of gradient descent optimization algorithms\n            * [Sebastien Ruder, 2016] https://arxiv.org/abs/1609.04747\n            * [PDF] https://arxiv.org/pdf/1609.04747.pdf\n\n        [2] Adam: A Method for Stochastic Optimization\n            * [Diederik P. Kingma et. al., 2014] https://arxiv.org/abs/1412.6980\n            * [PDF] https://arxiv.org/pdf/1412.6980.pdf\n\n    Args:\n        kwargs: Arbitrary keyword arguments.\n    """"""\n\n    def __init__(self, **kwargs):\n        super(Adamax, self).__init__(**kwargs)\n        self.lr      = kwargs[\'lr\']      if \'lr\'      in kwargs else 0.02\n        self.epsilon = kwargs[\'epsilon\'] if \'epsilon\' in kwargs else 1e-8\n        self.beta1   = kwargs[\'beta1\']   if \'beta1\'   in kwargs else 0.9\n        self.beta2   = kwargs[\'beta2\']   if \'beta2\'   in kwargs else 0.999\n        self.m       = None\n        self.u       = None\n        self.t       = 1\n\n    def update(self, weights, grads, epoch_num, batch_num, batch_size):\n        self.weights = weights\n        self.grads   = np.true_divide(grads, batch_size)\n        self.t       = batch_num\n\n        if self.m is None:\n            self.m = np.zeros_like(self.weights)\n\n        if self.u is None:\n            self.u = np.zeros_like(self.weights)\n\n        lr_t = np.true_divide(super(Adamax, self).get_learning_rate(epoch_num),\n                              1. - np.power(self.beta1, self.t))\n\n        m_hat = np.multiply(self.beta1, self.m) + np.multiply((1. - self.beta1), self.grads)\n        u_hat = np.maximum(np.multiply(self.beta2, self.u), np.abs(self.grads))\n\n        self.weights -= np.true_divide(np.multiply(lr_t, m_hat), (u_hat + self.epsilon))\n\n        return self.weights\n\n    @property\n    def optimization_name(self):\n        return self.__class__.__name__\n\n\nclass AdaGrad(Optimizer):\n\n    """"""\n    **Adaptive Gradient Algorithm (AdaGrad)**\n\n    AdaGrad is an  optimization method  that  allows  different  step  sizes for\n    different  features.  It increases  the  influence of  rare  but informative\n    features\n\n    References:\n        [1] An overview of gradient descent optimization algorithms\n            * [Sebastien Ruder, 2016] https://arxiv.org/abs/1609.04747\n            * [PDF] https://arxiv.org/pdf/1609.04747.pdf\n\n        [2] Adaptive Subgradient Methods for Online Learning and Stochastic Optimization\n            * [John Duchi et. al., 2011] http://jmlr.org/papers/v12/duchi11a.html\n            * [PDF] http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\n\n    Args:\n        kwargs: Arbitrary keyword arguments.\n    """"""\n\n    def __init__(self, **kwargs):\n        super(AdaGrad, self).__init__(**kwargs)\n        self.lr      = kwargs[\'lr\']      if \'lr\'      in kwargs else 0.01\n        self.epsilon = kwargs[\'epsilon\'] if \'epsilon\' in kwargs else 1e-8\n        self.cache   = None\n\n    def update(self, weights, grads, epoch_num, batch_num, batch_size):\n        self.weights = weights\n        self.grads   = np.true_divide(grads, batch_size)\n\n        if self.cache is None:\n            self.cache = np.zeros_like(self.grads)\n\n        self.cache   += np.square(self.grads)\n        self.weights -= np.multiply(\n                            super(AdaGrad, self).get_learning_rate(epoch_num),\n                            np.true_divide(self.grads, np.sqrt(self.cache) + self.epsilon)\n                        )\n\n        return self.weights\n\n    @property\n    def optimization_name(self):\n        return self.__class__.__name__\n\n\nclass Adadelta(Optimizer):\n\n    """"""\n    **An Adaptive Learning Rate Method (Adadelta)**\n\n    Adadelta is an extension of Adagrad that seeks to avoid setting  the learing\n    rate to an aggresively  monotonically  decreasing rate. This is achieved via\n    a dynamic learning rate i.e a diffrent learning  rate is computed  for  each\n    training sample\n\n    References:\n        [1] An overview of gradient descent optimization algorithms\n            * [Sebastien Ruder, 2016] https://arxiv.org/abs/1609.04747\n            * [PDF] https://arxiv.org/pdf/1609.04747.pdf\n\n        [2] ADADELTA: An Adaptive Learning Rate Method\n            * [Matthew D. Zeiler, 2012] https://arxiv.org/abs/1212.5701\n            * [PDF] https://arxiv.org/pdf/1212.5701.pdf\n\n    Args:\n        kwargs: Arbitrary keyword arguments.\n    """"""\n\n    def __init__(self, **kwargs):\n        super(Adadelta, self).__init__(**kwargs)\n        self.lr      = kwargs[\'lr\']      if \'lr\'      in kwargs else 1.0\n        self.epsilon = kwargs[\'epsilon\'] if \'epsilon\' in kwargs else 1e-6\n        self.rho     = kwargs[\'rho\']     if \'rho\'     in kwargs else 0.9\n        self.cache   = None\n        self.delta   = None\n\n    def update(self, weights, grads, epoch_num, batch_num, batch_size):\n        self.weights = weights\n        self.grads   = np.true_divide(grads, batch_size) # grads\n\n        if self.cache is None:\n            self.cache = np.zeros_like(self.weights)\n\n        if self.delta is None:\n            self.delta = np.zeros_like(self.weights)\n\n        self.cache = np.multiply(self.rho, self.cache) + np.multiply(1 - self.rho, np.square(self.grads))\n\n        RMSE_grad  = np.sqrt(self.cache + self.epsilon)\n        RMSE_delta = np.sqrt(self.delta + self.epsilon)\n\n        update = np.multiply(self.grads, np.true_divide(RMSE_delta, RMSE_grad))\n\n        self.weights -= np.multiply(super(Adadelta, self).get_learning_rate(epoch_num), update)\n        self.delta    = np.multiply(self.rho, self.delta) + np.multiply((1 - self.rho), np.square(update))\n\n        return self.weights\n\n    @property\n    def optimization_name(self):\n        return self.__class__.__name__\n\n\nclass RMSprop(Optimizer):\n\n    """"""\n    **Root Mean Squared Propagation (RMSprop)**\n\n    RMSprop utilizes  the magnitude of recent gradients to  normalize  gradients.\n    A moving average over the root mean squared (RMS) gradients is kept and then\n    divided by  the current  gradient. Parameters are  recomended to  be  set as\n    follows rho = 0.9 and eta (learning rate) = 0.001\n\n    References:\n        [1] An overview of gradient descent optimization algorithms\n            * [Sebastien Ruder, 2016] https://arxiv.org/abs/1609.04747\n            * [PDF] https://arxiv.org/pdf/1609.04747.pdf\n\n        [2] Lecture 6.5 - rmsprop, COURSERA: Neural Networks for Machine Learning\n            * [Tieleman, T. and Hinton, G. 2012][PDF] https://goo.gl/Dhkvpk\n\n    Args:\n        kwargs: Arbitrary keyword arguments.\n    """"""\n\n    def __init__(self, **kwargs):\n        super(RMSprop, self).__init__(**kwargs)\n        self.lr      = kwargs[\'lr\']      if \'lr\'      in kwargs else 0.001\n        self.epsilon = kwargs[\'epsilon\'] if \'epsilon\' in kwargs else 1e-6\n        self.rho     = kwargs[\'rho\']     if \'rho\'     in kwargs else 0.9\n        self.cache   = None\n\n    def update(self, weights, grads, epoch_num, batch_num, batch_size):\n        self.weights = weights\n        self.grads   = np.true_divide(grads, batch_size) # grads\n\n        if self.cache is None:\n            self.cache = np.zeros_like(self.weights)\n\n        self.cache    = np.multiply(self.rho, np.multiply(self.cache + (1 - self.rho), np.square(self.grads)))\n        self.weights -= np.multiply(self.lr, np.true_divide(self.grads, (np.sqrt(self.cache) + self.epsilon)))\n\n        return self.weights\n\n    @property\n    def optimization_name(self):\n        return self.__class__.__name__\n\n\nclass NesterovAcceleratedGradient(Optimizer):\n\n    """"""\n    **Nesterov Accelerated Gradient (NAG)**\n\n    NAG is an improvement in SGDMomentum where the the previous parameter values\n    are smoothed and a gradient  descent  step is taken from this smoothed value.\n    This enables a more intelligent way of arriving at the minima\n\n    References:\n        [1] An overview of gradient descent optimization algorithms\n            * [Sebastien Ruder, 2016] https://arxiv.org/abs/1609.04747\n            * [PDF] https://arxiv.org/pdf/1609.04747.pdf\n\n        [2] A method for unconstrained convex minimization problem with the rate of convergence\n            * [Nesterov, Y. 1983][PDF] https://goo.gl/X8313t\n\n        [3] Nesterov\'s Accelerated Gradient and Momentum as approximations to Regularised Update Descent\n            * [Aleksandar Botev, 2016] https://arxiv.org/abs/1607.01981\n            * [PDF] https://arxiv.org/pdf/1607.01981.pdf\n\n    Args:\n        kwargs: Arbitrary keyword arguments.\n    """"""\n\n    def __init__(self, **kwargs):\n        super(NesterovAcceleratedGradient, self).__init__(**kwargs)\n        self.lr            = kwargs[\'lr\']       if \'lr\'       in kwargs else 0.001\n        self.momentum      = kwargs[\'momentum\'] if \'momemtum\' in kwargs else 0.9\n        self.velocity_prev = None\n        self.velocity      = None\n\n    def update(self, weights, grads, epoch_num, batch_num, batch_size):\n        self.weights = weights\n        self.grads   = grads\n\n        if self.velocity_prev is None:\n            self.velocity_prev = np.zeros_like(self.weights)\n\n        if self.velocity is None:\n            self.velocity = np.zeros_like(self.weights)\n\n        self.velocity_prev  = self.velocity\n        self.velocity       = np.subtract(\n                                 np.multiply(self.momentum, self.velocity),\n                                 np.multiply(super(NesterovAcceleratedGradient, self).get_learning_rate(epoch_num), self.grads)\n                              )\n        self.weights       += np.multiply(-self.momentum, self.velocity_prev) + np.multiply(1 + self.momentum, self.velocity)\n\n        return self.weights\n\n    @property\n    def optimization_name(self):\n        return self.__class__.__name__\n\n\nclass OptimizationFunction:\n\n    _optimizers = {\n        \'sgd\'          : SGD,\n        \'adam\'         : Adam,\n        \'adamax\'       : Adamax,\n        \'adagrad\'      : AdaGrad,\n        \'rmsprop\'      : RMSprop,\n        \'adadelta\'     : Adadelta,\n        \'sgd_momentum\' : SGDMomentum,\n        \'nestrov\'      : NesterovAcceleratedGradient\n    }\n\n    def __init__(self, optimizer_kwargs):\n\n        # 1. using class types: check if optimizer_kwargs is an instance of any of the classes in _optimizers.values dict\n        if any(isinstance(optimizer_kwargs, cls_type) for cls_type in list(self._optimizers.values())):\n            import copy\n            self.optimization_func = copy.copy(optimizer_kwargs)\n\n        # 2. using string types: check if optimizer_kwargs is an instance of any of type string and is in _optimizers.keys dict\n        elif any(isinstance(optimizer_kwargs, str) for cls_type in list(self._optimizers.keys())):\n            if optimizer_kwargs not in self._optimizers.keys():\n                raise Exception(\'Optimization function must be either one of the following: {}.\'.format(\', \'.join(self._optimizers.keys())))\n            self.optimization_func = self._optimizers[optimizer_kwargs]()\n\n        # 3. using kwargs: we have a dictionary of keyword arguments from the register_opt func\n        else:\n            if optimizer_kwargs[\'optimizer_name\'] not in self._optimizers.keys():\n                raise Exception(\'Optimization function must be either one of the following: {}.\'.format(\', \'.join(self._optimizers.keys())))\n            self.optimization_func = self._optimizers[optimizer_kwargs[\'optimizer_name\']](**optimizer_kwargs)\n\n    @property\n    def name(self):\n        return self.optimization_func.optimization_name\n\n    def update(self, weights, grads, epoch_num, batch_num, batch_size):\n        return self.optimization_func.update(weights, grads, epoch_num, batch_num, batch_size)\n\n\ndef register_opt(**kwargs):\n\n    # ensure that key optimizer_name is present\n    if \'optimizer_name\' not in kwargs:\n        raise Exception(\'optimizer_name must be included in the register_opt func to deduce the optimization type to be used\')\n\n    allowed_kwargs = {\n        \'lr\',\n        \'rho\',\n        \'beta2\',\n        \'beta1\',\n        \'epsilon\',\n        \'epsilon\',\n        \'decay_lr\',\n        \'momentum\',\n        \'velocity\',\n        \'step_size\',\n        \'decay_rate\',\n        \'decay_func\',\n        \'optimizer_name\'\n    }\n\n    for kwrd in kwargs:\n        if kwrd not in allowed_kwargs:\n            raise TypeError(\'Unexpected keyword argument passed to optimizer: \' + str(kwrd))\n    return kwargs\n'"
ztlearn/regularizers.py,7,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\n# Note: careful as np.multiply does an elementwise multiply on numpy arrays\n#       asterisk (*) does the same but will perfom matrix multiplication on mat (numpy matrices)\n\nclass L1Regularization:\n\n    """"""\n    **Lasso Regression (L1Regularization)**\n\n    L1Regularization adds sum  of the absolute value magnitudes of parameters as\n    penalty term to the loss function\n\n    References:\n        [1] Regularization (mathematics)\n            * [Wikipedia Article] https://en.wikipedia.org/wiki/Regularization_(mathematics)\n\n        [2] Regression shrinkage and selection via the lasso\n            * [R Tibshirani, 1996] https://goo.gl/Yh9bBU\n            * [PDF] https://goo.gl/mQP5mA\n\n        [3] Feature selection, L1 vs. L2 regularization, and rotational invariance\n            * [Andrew Y. Ng, ] [PDF] https://goo.gl/rbwNCt\n\n    Args:\n        _lambda  (float32): controls the weight of the penalty term\n    """"""\n\n    def __init__(self, _lambda, **kwargs):\n        self._lambda = _lambda\n\n    def regulate(self, weights):\n        return np.multiply(self._lambda, np.linalg.norm(weights))\n\n    def derivative(self, weights):\n        return np.multiply(self._lambda, np.sign(weights))\n\n    @property\n    def regulation_name(self):\n        return self.__class__.__name__\n\n\nclass L2Regularization:\n\n    """"""\n    **Lasso Regression (L2Regularization)**\n\n    L1Regularization adds sum of the squared magnitudes of parameters as penalty\n    term to the loss function\n\n    References:\n        [1] Regularization (mathematics)\n            * [Wikipedia Article] https://en.wikipedia.org/wiki/Regularization_(mathematics)\n\n        [2] Regression shrinkage and selection via the lasso\n            * [R Tibshirani, 1996] https://goo.gl/Yh9bBU\n            * [PDF] https://goo.gl/mQP5mA\n\n        [3] Feature selection, L1 vs. L2 regularization, and rotational invariance\n            * [Andrew Y. Ng, ] [PDF] https://goo.gl/rbwNCt\n\n    Args:\n        _lambda (float32): controls the weight of the penalty term\n    """"""\n\n    def __init__(self, _lambda, **kwargs):\n        self._lambda = _lambda\n\n    def regulate(self, weights):\n        return np.multiply(self._lambda, (0.5 * weights.T.dot(weights)))\n\n    def derivative(self, weights):\n        return np.multiply(self._lambda, weights)\n\n    @property\n    def regulation_name(self):\n        return self.__class__.__name__\n\n\nclass ElasticNetRegularization:\n\n    """"""\n    **Elastic Net Regularization (ElasticNetRegularization)**\n\n    ElasticNetRegularization  adds both absolute  value of magnitude and squared\n    magnitude of coefficient as penalty term to the loss function\n\n    References:\n        [1] Regularization (mathematics)\n            * [Wikipedia Article] https://en.wikipedia.org/wiki/Regularization_(mathematics)\n\n    Args:\n        _lambda  (float32): controls the weight of the penalty term\n        l1_ratio (float32): controls the value l1 penalty as a ratio of total penalty added to the loss function\n    """"""\n\n    def __init__(self, _lambda, l1_ratio):\n        self._lambda  = _lambda\n        self.l1_ratio = l1_ratio\n\n    def regulate(self, weights):\n        return np.multiply(self._lambda, (((self.l1_ratio * 0.5) * weights.T.dot(weights)) + ((1 - self.l1_ratio) * np.linalg.norm(weights))))\n\n    def derivative(self, weights):\n        return np.multiply(self._lambda, (((self.l1_ratio * 0.5) * weights) + ((1 - self.l1_ratio) *  np.sign(weights))))\n\n    @property\n    def regulation_name(self):\n        return self.__class__.__name__\n\n\nclass RegularizationFunction:\n\n    _regularizers = {\n        \'l1\'          : L1Regularization,\n        \'lasso\'       : L1Regularization,\n        \'l2\'          : L2Regularization,    \n        \'ridge\'       : L2Regularization,\n        \'elastic\'     : ElasticNetRegularization,\n        \'elastic_net\' : ElasticNetRegularization\n    }\n\n    def __init__(self, name = \'lasso\', _lambda = 0.5, l1_ratio = 0.5):\n        if name not in self._regularizers.keys():\n            raise Exception(\'Regularization function must be either one of the following: {}.\'.format(\', \'.join(self._regularizers.keys())))\n        self.regularization_func = self._regularizers[name](_lambda, l1_ratio = l1_ratio)\n\n    @property\n    def name(self):\n        return self.regularization_func.regularization_name\n\n    def regulate(self, weights):\n        return self.regularization_func.regulate(weights)\n\n    def derivative(self, weights):\n        return self.regularization_func.derivative(weights)\n'"
examples/boston/boston_elastic_regression.py,2,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.boston import fetch_boston\nfrom ztlearn.ml.regression import ElasticNetRegression\nfrom ztlearn.objectives import ObjectiveFunction as objective\n\ndata = fetch_boston()\n\n# take the boston data\ninput_data  = z_score(data.data[:,[5]]) # work with only one of the features: RM\ninput_label = data.target\n\ntrain_data, test_data, train_label, test_label = train_test_split(input_data,\n                                                                  input_label,\n                                                                  test_size = 0.3)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'sgd', momentum = 0.01, lr = 0.001)\n\n# model definition\nmodel = ElasticNetRegression(degree         = 3,\n                             epochs         = 100,\n                             optimizer      = opt,\n                             penalty        = 'elastic',\n                             penalty_weight = 0.01,\n                             l1_ratio       = 0.5)\n\nfit_stats   = model.fit(train_data, train_label)\ntargets     = np.expand_dims(test_label, axis = 1)\npredictions = np.expand_dims(model.predict(test_data), axis = 1)\nmse         = objective('mean_squared_error').forward(predictions, targets)\n\nprint('Mean Squared Error: {:.2f}'.format(mse))\n\nmodel_name = 'boston_elastic_regression'\nplot_metric('accuracy_loss',\n                             len(fit_stats['train_loss']),\n                             fit_stats['train_acc'],\n                             fit_stats['train_loss'],\n                             model_name = model_name,\n                             legend     = ['acc', 'loss'])\n\nplot_regression_results(train_data,\n                                    train_label,\n                                    test_data,\n                                    test_label,\n                                    input_data,\n                                    model.predict(input_data),\n                                    mse, 'Elastic Regression',\n                                    'Median House Price',\n                                    'Average Number of Rooms',\n                                    model_name = model_name)\n"""
examples/boston/boston_linear_regression.py,2,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.boston import fetch_boston\nfrom ztlearn.ml.regression import LinearRegression\nfrom ztlearn.objectives import ObjectiveFunction as objective\n\ndata = fetch_boston()\n\ninput_data  = z_score(data.data[:,[5]]) # normalize and work with only one of the features\ninput_label = data.target\n\ntrain_data, test_data, train_label, test_label = train_test_split(input_data,\n                                                                  input_label,\n                                                                  test_size = 0.3)\n\nopt = register_opt(optimizer_name = 'sgd', momentum = 0.01, lr = 0.001)\n\n# model definition\nmodel     = LinearRegression(epochs = 100, optimizer = opt, penalty = 'l1', penalty_weight = 0.8)\nfit_stats = model.fit(train_data, train_label)\n\n# fit_stats = model.fit_OLS(train_data, train_label) # ---- Ordinary Least Squares Method\n\ntargets     = np.expand_dims(test_label, axis = 1)\npredictions = np.expand_dims(model.predict(test_data), axis = 1)\nmse         = objective('mean_squared_error').forward(predictions, targets)\n\nprint('Mean Squared Error: {:.2f}'.format(mse))\n\nmodel_name = 'boston_linear_regression'\nplot_metric('accuracy_loss',\n                             len(fit_stats['train_loss']),\n                             fit_stats['train_acc'],\n                             fit_stats['train_loss'],\n                             model_name = model_name,\n                             legend     = ['acc', 'loss'])\n\nplot_regression_results(train_data,\n                                    train_label,\n                                    test_data,\n                                    test_label,\n                                    input_data,\n                                    model.predict(input_data),\n                                    mse, 'Linear Regression',\n                                    'Median House Price',\n                                    'Average Number of Rooms',\n                                    model_name = model_name)\n"""
examples/boston/boston_polynomial_regression.py,2,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.boston import fetch_boston\nfrom ztlearn.ml.regression import PolynomialRegression\nfrom ztlearn.objectives import ObjectiveFunction as objective\n\ndata = fetch_boston()\n\ninput_data  = z_score(data.data[:,[5]]) # normalize and work with only one of the features\ninput_label = data.target\n\ntrain_data, test_data, train_label, test_label = train_test_split(input_data,\n                                                                  input_label,\n                                                                  test_size = 0.3)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'sgd', momentum = 0.01, lr = 0.001)\n\n# model definition\nmodel = PolynomialRegression(degree         = 5,\n                             epochs         = 100,\n                             optimizer      = opt,\n                             penalty        = 'elastic',\n                             penalty_weight = 0.01,\n                             l1_ratio       = 0.3)\n\nfit_stats   = model.fit(train_data, train_label)\ntargets     = np.expand_dims(test_label, axis = 1)\npredictions = np.expand_dims(model.predict(test_data), axis = 1)\nmse         = objective('mean_squared_error').forward(predictions, targets)\n\nprint('Mean Squared Error: {:.2f}'.format(mse))\n\nmodel_name = 'boston_polynomial_regression'\nplot_metric('accuracy_loss',\n                             len(fit_stats['train_loss']),\n                             fit_stats['train_acc'],\n                             fit_stats['train_loss'],\n                             model_name = model_name,\n                             legend     = ['acc', 'loss'])\n\nplot_regression_results(train_data,\n                                    train_label,\n                                    test_data,\n                                    test_label,\n                                    input_data,\n                                    model.predict(input_data),\n                                    mse, 'Polynomial Regression',\n                                    'Median House Price',\n                                    'Average Number of Rooms',\n                                    model_name = model_name)\n"""
examples/cifar_10/cifar_10_autoencoder.py,4,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.cifar import fetch_cifar_10\nfrom ztlearn.dl.layers import BatchNormalization, Dense\n\nimg_rows   = 32\nimg_cols   = 32\nimg_dim    = 3072 # channels * img_rows * img_cols\nchannels   = 3    # red channel + blue channel + green channel\nlatent_dim = 4\ninit_type  = 'he_uniform'\n\ndef stack_encoder_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(512, activation = 'relu', input_shape = (img_dim,)))\n    model.add(BatchNormalization())\n    model.add(Dense(256, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(latent_dim, activation = 'relu'))\n\n    return model\n\ndef stack_decoder_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(256, activation = 'relu', input_shape = (latent_dim,)))\n    model.add(BatchNormalization())\n    model.add(Dense(512, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(img_dim, activation = 'sigmoid'))\n\n    return model\n\nencoder = stack_encoder_layers(init = init_type)\ndecoder = stack_decoder_layers(init = init_type)\n\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.001)\n\nautoencoder = Sequential(init_method = init_type)\nautoencoder.layers.extend(encoder.layers)\nautoencoder.layers.extend(decoder.layers)\nautoencoder.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nencoder.summary('cifar-10 encoder')\ndecoder.summary('cifar-10 decoder')\n\nautoencoder.summary('cifar-10 autoencoder')\n\ndata   = fetch_cifar_10()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  data.data,\n                                                                  test_size   = 0.2,\n                                                                  random_seed = 5,\n                                                                  cut_off     = 2000)\n\n# plot samples of training data\nplot_img_samples(train_data, None, dataset = 'cifar', channels = 3)\n\ntransformed_image_dims  = img_dim\ntransformed_train_data  = z_score(train_data.reshape(train_data.shape[0], transformed_image_dims).astype(np.float32))\ntransformed_train_label = z_score(train_label.reshape(train_label.shape[0], transformed_image_dims).astype(np.float32))\ntransformed_test_data   = z_score(test_data.reshape(test_data.shape[0], transformed_image_dims).astype(np.float32))\ntransformed_test_label  = z_score(test_label.reshape(test_label.shape[0], transformed_image_dims).astype(np.float32))\n\nmodel_epochs = 500\nfit_stats = autoencoder.fit(transformed_train_data,\n                            transformed_train_label,\n                            batch_size      = 128,\n                            epochs          = model_epochs,\n                            validation_data = (transformed_test_data, transformed_test_label),\n                            shuffle_data    = True)\n\n# generate non rescaled test labels for use in generated digits plot (use the same random_seed as above)\n_, _, _, test_label = train_test_split(data.data, data.target, test_size = 0.2, random_seed = 5)\npredictions         = autoencoder.predict(transformed_test_data).reshape((-1, channels, img_rows, img_cols))\n\nmodel_name = autoencoder.model_name\nplot_generated_img_samples(unhot(one_hot(test_label)),\n                                                        predictions,\n                                                        dataset    = 'cifar',\n                                                        channels   = 3,\n                                                        to_save    = False,\n                                                        iteration  = model_epochs,\n                                                        model_name = model_name)\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model_name)\n"""
examples/cifar_10/cifar_10_cnn.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.cifar import fetch_cifar_10\nfrom ztlearn.dl.layers import BatchNormalization, Conv2D\nfrom ztlearn.dl.layers import Dropout, Dense, Flatten, MaxPooling2D\n\n\ndata = fetch_cifar_10()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  data.target,\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 5,\n                                                                  cut_off     = 2000)\n\n# plot samples of training data\nplot_img_samples(train_data, train_label, dataset = 'cifar', channels = 3)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.0001)\n\n# model definition\nmodel = Sequential(init_method = 'he_uniform')\nmodel.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', input_shape = (3, 32, 32), padding = 'same'))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(MaxPooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('cifar-10 cnn')\n\nmodel_epochs = 12 # change to 12 epochs\nfit_stats = model.fit(train_data.reshape(-1, 3, 32, 32),\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data.reshape(-1, 3, 32, 32), one_hot(test_label)),\n                      shuffle_data    = True)\n\npredictions = unhot(model.predict(test_data.reshape(-1, 3, 32, 32), True))\nprint_results(predictions, test_label)\nplot_img_results(test_data, test_label, predictions, dataset = 'cifar', channels = 3)\n\nmodel_name = model.model_name\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model_name)\n"""
examples/cifar_10/cifar_10_mlp.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.cifar import fetch_cifar_10\nfrom ztlearn.dl.layers import Dropout, Dense, BatchNormalization, Activation\n\ndata = fetch_cifar_10()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  data.target,\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 3)\n\n# plot samples of training data\nplot_img_samples(train_data, train_label, dataset = 'cifar', channels = 3)\n\ntransformed_image_dims = 3 * 32 * 32 # ==> (channels * height * width)\ntransformed_train_data = z_score(train_data.reshape(train_data.shape[0], transformed_image_dims).astype('float32'))\ntransformed_test_data  = z_score(test_data.reshape(test_data.shape[0], transformed_image_dims).astype('float32'))\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.0001)\n\nmodel = Sequential()\nmodel.add(Dense(1024, input_shape = (3072, )))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\nmodel.compile(loss = 'cce', optimizer = opt)\n\nmodel.summary(model_name = 'cifar-10 mlp')\n\nmodel_epochs = 200 # change to 200 epochs\nfit_stats = model.fit(transformed_train_data,\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (transformed_test_data, one_hot(test_label)),\n                      shuffle_data    = True)\n\neval_stats  = model.evaluate(transformed_test_data, one_hot(test_label))\npredictions = unhot(model.predict(transformed_test_data, True))\nprint_results(predictions, test_label)\n\nplot_img_results(test_data, test_label, predictions, dataset = 'cifar', channels = 3)\n\nmodel_name = model.model_name\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model_name)\nplot_metric('evaluation',\n                          eval_stats['valid_batches'],\n                          eval_stats['valid_loss'],\n                          eval_stats['valid_acc'],\n                          model_name = model_name,\n                          legend     = ['loss', 'acc'])\n"""
examples/cifar_10/cifar_10_pca.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import z_score\nfrom ztlearn.utils import plot_pca\nfrom ztlearn.ml.decomposition import PCA\nfrom ztlearn.datasets.cifar import fetch_cifar_10\n\ndata = fetch_cifar_10()\nreshaped_image_dims = 3 * 32 * 32 # ==> (channels * height * width)\nreshaped_data       = z_score(data.data.reshape(-1, reshaped_image_dims).astype('float32'))\n\npca        = PCA(n_components = 2)\ncomponents = pca.fit_transform(reshaped_data)\n\nplot_pca(components, n_components = 2, colour_array = data.target, model_name = 'CIFAR-10 PCA')\n"""
examples/cifar_10/cifar_10_perceptron.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.ml.classification import Perceptron\nfrom ztlearn.datasets.cifar import fetch_cifar_10\n\ndata = fetch_cifar_10()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  one_hot(data.target),\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 5)\n\n# plot samples of training data\nplot_img_samples(train_data, unhot(train_label), dataset = \'cifar\', channels = 3)\n\nreshaped_image_dims = 3 * 1024 # ==> (channels * (height * width))\nreshaped_train_data = z_score(train_data.reshape(train_data.shape[0], reshaped_image_dims).astype(\'float32\'))\nreshaped_test_data  = z_score(test_data.reshape(test_data.shape[0], reshaped_image_dims).astype(\'float32\'))\n\n# optimizer definition\nopt = register_opt(optimizer_name = \'adam\', momentum = 0.001, lr = 0.0001)\n\n# model definition\nmodel     = Perceptron(epochs = 300, activation = \'relu\', loss = \'cce\', init_method = \'he_normal\', optimizer = opt)\nfit_stats = model.fit(reshaped_train_data, train_label)\n\npredictions = unhot(model.predict(reshaped_test_data))\nprint_results(predictions, unhot(test_label))\n\nplot_img_results(test_data, unhot(test_label), predictions, dataset = \'cifar\', channels = 3)\nplot_metric(\'accuracy_loss\',\n                              len(fit_stats[""train_loss""]),\n                              fit_stats[\'train_acc\'],\n                              fit_stats[\'train_loss\'],\n                              model_name = \'cifa_10_perceptron\',\n                              legend     = [\'acc\', \'loss\'])\n'"
examples/cifar_10/cifar_10_rnn.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.cifar import fetch_cifar_10\nfrom ztlearn.dl.layers import RNN, Dense, Flatten\n\ndata = fetch_cifar_10()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  data.target,\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 5,\n                                                                  cut_off     = 10000)\n\n# plot samples of training data\nplot_img_samples(train_data, train_label, dataset = 'cifar', channels = 3)\n\nreshaped_image_dims = 3 * 1024 # ==> (channels * (height * width))\nreshaped_train_data = z_score(train_data.reshape(train_data.shape[0], reshaped_image_dims).astype('float32'))\nreshaped_test_data  = z_score(test_data.reshape(test_data.shape[0], reshaped_image_dims).astype('float32'))\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.0001)\n\n# model definition\nmodel = Sequential()\nmodel.add(RNN(256, activation = 'tanh', bptt_truncate = 5, input_shape = (3, 1024)))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary(model_name = 'cifar-10 rnn')\n\nmodel_epochs = 100 # add more epochs\nfit_stats = model.fit(reshaped_train_data.reshape(-1, 3, 1024),\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (reshaped_test_data.reshape(-1, 3, 1024), one_hot(test_label)),\n                      shuffle_data    = True)\n\npredictions = unhot(model.predict(reshaped_test_data.reshape(-1, 3, 1024), True))\nprint_results(predictions, test_label)\nplot_img_results(test_data, test_label, predictions, dataset = 'cifar', channels = 3)\n\nmodel_name = model.model_name\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model_name)\n"""
examples/cifar_100/cifar_100_autoencoder.py,4,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.cifar import fetch_cifar_100\nfrom ztlearn.dl.layers import BatchNormalization, Dense\n\nimg_rows   = 32\nimg_cols   = 32\nimg_dim    = 3072 # channels * img_rows * img_cols\nchannels   = 3    # Red Blue Green\nlatent_dim = 4\ninit_type  = 'he_uniform'\n\ndef stack_encoder_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(512, activation = 'relu', input_shape = (img_dim,)))\n    model.add(BatchNormalization())\n    model.add(Dense(256, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(latent_dim, activation = 'relu'))\n\n    return model\n\ndef stack_decoder_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(256, activation = 'relu', input_shape = (latent_dim,)))\n    model.add(BatchNormalization())\n    model.add(Dense(512, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(img_dim, activation = 'sigmoid'))\n\n    return model\n\nencoder = stack_encoder_layers(init = init_type)\ndecoder = stack_decoder_layers(init = init_type)\n\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.001)\n\nautoencoder = Sequential(init_method = init_type)\nautoencoder.layers.extend(encoder.layers)\nautoencoder.layers.extend(decoder.layers)\nautoencoder.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nencoder.summary('cifar-100 encoder')\ndecoder.summary('cifar-100 decoder')\n\nautoencoder.summary('cifar-100 autoencoder')\n\ndata   = fetch_cifar_100()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  data.data,\n                                                                  test_size   = 0.2,\n                                                                  random_seed = 5,\n                                                                  cut_off     = 2000)\n\n# plot samples of training data\nplot_img_samples(train_data, None, dataset = 'cifar', channels = 3)\n\ntransformed_image_dims  = img_dim\ntransformed_train_data  = z_score(train_data.reshape(train_data.shape[0], transformed_image_dims).astype(np.float32))\ntransformed_train_label = z_score(train_label.reshape(train_label.shape[0], transformed_image_dims).astype(np.float32))\ntransformed_test_data   = z_score(test_data.reshape(test_data.shape[0], transformed_image_dims).astype(np.float32))\ntransformed_test_label  = z_score(test_label.reshape(test_label.shape[0], transformed_image_dims).astype(np.float32))\n\nmodel_epochs = 500\nfit_stats = autoencoder.fit(transformed_train_data,\n                            transformed_train_label,\n                            batch_size      = 128,\n                            epochs          = model_epochs,\n                            validation_data = (transformed_test_data, transformed_test_label),\n                            shuffle_data    = True)\n\n# generate non rescaled test labels for use in generated digits plot (use the same random_seed as above)\n_, _, _, test_label = train_test_split(data.data, data.target, test_size = 0.2, random_seed = 5)\npredictions         = autoencoder.predict(transformed_test_data).reshape((-1, channels, img_rows, img_cols))\n\nplot_generated_img_samples(unhot(one_hot(test_label)),\n                                                        predictions,\n                                                        dataset    = 'cifar',\n                                                        channels   = 3,\n                                                        to_save    = False,\n                                                        iteration  = model_epochs,\n                                                        model_name = autoencoder.model_name)\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = autoencoder.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = autoencoder.model_name)\n"""
examples/cifar_100/cifar_100_cnn.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.cifar import fetch_cifar_100\nfrom ztlearn.dl.layers import BatchNormalization, Conv2D\nfrom ztlearn.dl.layers import Dropout, Dense, Flatten, MaxPooling2D\n\n\ndata = fetch_cifar_100()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  data.target,\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 5,\n                                                                  cut_off     = 2000)\n\n# plot samples of training data\nplot_img_samples(train_data, train_label, dataset = 'cifar', channels = 3)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.0001)\n\n# model definition\nmodel = Sequential(init_method = 'he_uniform')\nmodel.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', input_shape = (3, 32, 32), padding = 'same'))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(MaxPooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Dense(100, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('cifar-100 cnn')\n\nmodel_epochs = 2 # change to 12 epochs\nfit_stats = model.fit(train_data.reshape(-1, 3, 32, 32),\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data.reshape(-1, 3, 32, 32), one_hot(test_label)),\n                      shuffle_data    = True)\n\npredictions = unhot(model.predict(test_data.reshape(-1, 3, 32, 32), True))\nprint_results(predictions, test_label)\nplot_img_results(test_data, test_label, predictions, dataset = 'cifar', channels = 3)\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/cifar_100/cifar_100_mlp.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.cifar import fetch_cifar_100\nfrom ztlearn.dl.layers import Dropout, Dense, BatchNormalization, Activation\n\ndata = fetch_cifar_100()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  data.target,\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 3)\n\n# plot samples of training data\nplot_img_samples(train_data, train_label, dataset = 'cifar', channels = 3)\n\nreshaped_image_dims = 3 * 32 * 32 # ==> (channels * height * width)\nreshaped_train_data = z_score(train_data.reshape(train_data.shape[0], reshaped_image_dims).astype('float32'))\nreshaped_test_data  = z_score(test_data.reshape(test_data.shape[0], reshaped_image_dims).astype('float32'))\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.0001)\n\nmodel = Sequential()\nmodel.add(Dense(1024, input_shape = (3072, )))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Dense(100))\nmodel.add(Activation('softmax'))\nmodel.compile(loss = 'cce', optimizer = opt)\n\nmodel.summary(model_name = 'cifar-100 mlp')\n\nmodel_epochs = 12 # change to 200 epochs\nfit_stats = model.fit(reshaped_train_data,\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (reshaped_test_data, one_hot(test_label)),\n                      shuffle_data    = True)\n\neval_stats  = model.evaluate(reshaped_test_data, one_hot(test_label))\npredictions = unhot(model.predict(reshaped_test_data, True))\nprint_results(predictions, test_label)\n\nplot_img_results(test_data, test_label, predictions, dataset = 'cifar', channels = 3)\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\nplot_metric('evaluation',\n                          eval_stats['valid_batches'],\n                          eval_stats['valid_loss'],\n                          eval_stats['valid_acc'],\n                          model_name = model_name,\n                          legend     = ['loss', 'acc'])\n"""
examples/cifar_100/cifar_100_pca.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import z_score\nfrom ztlearn.utils import plot_pca\nfrom ztlearn.ml.decomposition import PCA\nfrom ztlearn.datasets.cifar import fetch_cifar_100\n\ndata = fetch_cifar_100()\nreshaped_image_dims = 3 * 32 * 32 # ==> (channels * height * width)\nreshaped_data       = z_score(data.data.reshape(-1, reshaped_image_dims).astype('float32'))\n\npca        = PCA(n_components = 2)\ncomponents = pca.fit_transform(reshaped_data)\n\nplot_pca(components, n_components = 2, colour_array = data.target, model_name = 'CIFAR-100 PCA')\n"""
examples/cifar_100/cifar_100_perceptron.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.ml.classification import Perceptron\nfrom ztlearn.datasets.cifar import fetch_cifar_100\n\ndata = fetch_cifar_100()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  one_hot(data.target),\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 5)\n\n# plot samples of training data\nplot_img_samples(train_data, unhot(train_label), dataset = \'cifar\', channels = 3)\n\nreshaped_image_dims = 3 * 1024 # ==> (channels * (height * width))\nreshaped_train_data = z_score(train_data.reshape(train_data.shape[0], reshaped_image_dims).astype(\'float32\'))\nreshaped_test_data  = z_score(test_data.reshape(test_data.shape[0], reshaped_image_dims).astype(\'float32\'))\n\n# optimizer definition\nopt = register_opt(optimizer_name = \'adam\', momentum = 0.001, lr = 0.0001)\n\n# model definition\nmodel     = Perceptron(epochs = 300, activation = \'relu\', loss = \'cce\', init_method = \'he_normal\', optimizer = opt)\nfit_stats = model.fit(reshaped_train_data, train_label)\n\npredictions = unhot(model.predict(reshaped_test_data))\nprint_results(predictions, unhot(test_label))\n\nplot_img_results(test_data, unhot(test_label), predictions, dataset = \'cifar\', channels = 3)\nplot_metric(\'accuracy_loss\',\n                              len(fit_stats[""train_loss""]),\n                              fit_stats[\'train_acc\'],\n                              fit_stats[\'train_loss\'],\n                              model_name = \'cifa_100_perceptron\',\n                              legend     = [\'acc\', \'loss\'])\n'"
examples/cifar_100/cifar_100_rnn.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.dl.layers import RNN, Dense, Flatten\nfrom ztlearn.datasets.cifar import fetch_cifar_100\n\ndata = fetch_cifar_100()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  data.target,\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 5)\n\n# plot samples of training data\nplot_img_samples(train_data, train_label, dataset = 'cifar', channels = 3)\n\nreshaped_image_dims = 3 * 1024 # ==> (channels * (height * width))\nreshaped_train_data = z_score(train_data.reshape(train_data.shape[0], reshaped_image_dims).astype('float32'))\nreshaped_test_data  = z_score(test_data.reshape(test_data.shape[0], reshaped_image_dims).astype('float32'))\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.0001)\n\n# model definition\nmodel = Sequential()\nmodel.add(RNN(256, activation = 'tanh', bptt_truncate = 5, input_shape = (3, 1024)))\nmodel.add(Flatten())\nmodel.add(Dense(100, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary(model_name = 'cifar-100 rnn')\n\nmodel_epochs = 10 # add more epochs\nfit_stats = model.fit(reshaped_train_data.reshape(-1, 3, 1024),\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (reshaped_test_data.reshape(-1, 3, 1024), one_hot(test_label)),\n                      shuffle_data    = True)\n\npredictions = unhot(model.predict(reshaped_test_data.reshape(-1, 3, 1024), True))\nprint_results(predictions, test_label)\nplot_img_results(test_data, test_label, predictions, dataset = 'cifar', channels = 3)\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/clusters/kmeans_cluestering.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom sklearn.datasets import make_blobs\n\nfrom ztlearn.utils import plot_kmeans\nfrom ztlearn.ml.clustering import KMeans\n\n# generate fake data\ndata, labels = make_blobs(n_samples = 1000, n_features = 2, centers = 4)\n\n# model definition\nmodel = KMeans(n_clusters = 4, max_iter = 2500)\ncentroids = model.fit(data)\n\n# plot clusters and centroids\nplot_kmeans(data, labels, centroids, model_clusters = 4)\n'"
examples/digits/digits_autoencoder.py,1,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.digits import fetch_digits\nfrom ztlearn.dl.layers import BatchNormalization, Dense\n\nimg_rows   = 8\nimg_cols   = 8\nimg_dim    = 64  # img_rows * img_cols\nlatent_dim = 4\ninit_type  = 'he_uniform'\n\ndef stack_encoder_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(256, activation = 'relu', input_shape = (img_dim,)))\n    model.add(BatchNormalization())\n    model.add(Dense(128, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(latent_dim, activation = 'relu'))\n\n    return model\n\ndef stack_decoder_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(128, activation = 'relu', input_shape = (latent_dim,)))\n    model.add(BatchNormalization())\n    model.add(Dense(256, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(img_dim, activation = 'sigmoid'))\n\n    return model\n\nencoder = stack_encoder_layers(init = init_type)\ndecoder = stack_decoder_layers(init = init_type)\n\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.001)\n\nautoencoder = Sequential(init_method = init_type)\nautoencoder.layers.extend(encoder.layers)\nautoencoder.layers.extend(decoder.layers)\nautoencoder.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nencoder.summary('digits encoder')\ndecoder.summary('digits decoder')\n\nautoencoder.summary('digits autoencoder')\n\ndata   = fetch_digits()\nimages = range_normalize(data.data.astype(np.float32), 0, 1)  # rescale to range [0, 1]\ntrain_data, test_data, train_label, test_label = train_test_split(images,\n                                                                  images,\n                                                                  test_size   = 0.2,\n                                                                  random_seed = 5)\n\n# plot samples of training data\nplot_img_samples(train_data, None)\n\nmodel_epochs = 500\nfit_stats = autoencoder.fit(train_data,\n                            train_label,\n                            batch_size      = 64,\n                            epochs          = model_epochs,\n                            validation_data = (test_data, test_label),\n                            shuffle_data    = True)\n\n# generate non rescaled test labels for use in generated digits plot (use the same random_seed as above)\n_, _, _, test_label = train_test_split(data.data, data.target, test_size = 0.2, random_seed = 5)\npredictions         = autoencoder.predict(test_data).reshape((-1, img_rows, img_cols))\n\nplot_generated_img_samples(unhot(one_hot(test_label)),\n                                                        predictions,\n                                                        to_save    = False,\n                                                        iteration  = model_epochs,\n                                                        model_name = autoencoder.model_name)\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = autoencoder.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = autoencoder.model_name)\n"""
examples/digits/digits_cnn.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.optimizers import Adam\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.digits import fetch_digits\nfrom ztlearn.dl.layers import BatchNormalization, Conv2D\nfrom ztlearn.dl.layers import Dropout, Dense, Flatten, MaxPooling2D\n\n\ndata = fetch_digits()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  data.target,\n                                                                  test_size   = 0.33,\n                                                                  random_seed = 5)\n\n# plot samples of training data\nplot_img_samples(train_data, train_label)\n\n# optimizer definition\nopt = Adam(lr = 0.001)\n\n# model definition\nmodel = Sequential(init_method = 'he_uniform')\nmodel.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', input_shape = (1, 8, 8), padding = 'same'))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(MaxPooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('digits cnn')\n\nmodel_epochs = 12\nfit_stats = model.fit(train_data.reshape(-1, 1, 8, 8),\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data.reshape(-1, 1, 8, 8), one_hot(test_label)),\n                      shuffle_data    = True)\n\npredictions = unhot(model.predict(test_data.reshape(-1, 1, 8, 8), True))\nprint_results(predictions, test_label)\nplot_img_results(test_data, test_label, predictions)\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/digits/digits_dcgan.py,10,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.digits import fetch_digits\nfrom ztlearn.dl.layers import Activation, BatchNormalization, Conv2D\nfrom ztlearn.dl.layers import Dense, Dropout, Flatten, Reshape, UpSampling2D\n\n\ndata = fetch_digits()\n\n# plot samples of training data\nplot_img_samples(data.data, None)\n\nimg_rows     = 8\nimg_cols     = 8\nimg_channels = 1\nimg_dims     = (img_channels, img_rows, img_cols)\n\nlatent_dim = 100\nbatch_size = 128\nhalf_batch = int(batch_size * 0.5)\n\nverbose   = True\ninit_type = 'he_uniform'\n\ngen_epoch = 50\ngen_noise = np.random.normal(0, 1, (36, latent_dim)) # for tiles 6 by 6 i.e (36) image generation\n\nmodel_epochs = 600\nmodel_stats  = {'d_train_loss': [], 'd_train_acc': [], 'g_train_loss': [], 'g_train_acc': []}\n\nd_opt = register_opt(optimizer_name = 'adam', beta1 = 0.5, lr = 0.0002)\ng_opt = register_opt(optimizer_name = 'adam', beta1 = 0.5, lr = 0.0002)\n\n\ndef stack_generator_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(64*2*2, input_shape = (latent_dim,)))\n    model.add(Activation('leaky_relu'))\n    model.add(BatchNormalization(momentum = 0.8))\n    model.add(Reshape((64, 2, 2)))\n    model.add(UpSampling2D())\n    model.add(Conv2D(32, kernel_size = (3, 3), padding = 'same'))\n    model.add(BatchNormalization(momentum = 0.8))\n    model.add(Activation('leaky_relu'))\n    model.add(UpSampling2D())\n    model.add(Conv2D(img_channels, kernel_size = (3, 3), padding = 'same'))\n    model.add(Activation('tanh'))\n\n    return model\n\n\ndef stack_discriminator_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Conv2D(32, kernel_size = (3, 3), padding = 'same', input_shape = img_dims))\n    model.add(Activation('leaky_relu'))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(64, kernel_size = (3, 3), padding = 'same'))\n    model.add(Activation('leaky_relu'))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(2))\n    model.add(Activation('sigmoid'))\n\n    return model\n\n\n# stack and compile the generator\ngenerator = stack_generator_layers(init = init_type)\ngenerator.compile(loss = 'cce', optimizer = g_opt)\n\n# stack and compile the discriminator\ndiscriminator = stack_discriminator_layers(init = init_type)\ndiscriminator.compile(loss = 'cce', optimizer = d_opt)\n\n# stack and compile the generator_discriminator\ngenerator_discriminator = Sequential(init_method = init_type)\ngenerator_discriminator.layers.extend(generator.layers)\ngenerator_discriminator.layers.extend(discriminator.layers)\ngenerator_discriminator.compile(loss = 'cce', optimizer = g_opt)\n\ngenerator.summary('digits generator')\ndiscriminator.summary('digits discriminator')\n\ngenerator_discriminator.summary('digits_dcgan')\nmodel_name = generator_discriminator.model_name\n\n# rescale to range [-1, 1]\nimages = range_normalize(data.data.reshape((-1,) + img_dims).astype(np.float32))\n\nfor epoch_idx in range(model_epochs):\n\n    # set the epoch id for print out\n    epoch_idx_p1 = epoch_idx + 1\n\n    # set the discriminator to trainable\n    discriminator.trainable = True\n\n    for epoch_k in range(1):\n\n        # set the epoch id for print out\n        epoch_k_p1 = epoch_k + 1\n\n        # draw random samples from real images\n        index = np.random.choice(images.shape[0], half_batch, replace = False)\n\n        imgs = images[index]\n\n        d_noise = np.random.normal(0, 1, (half_batch, latent_dim))\n\n        # generate a batch of new images\n        gen_imgs = generator.predict(d_noise)\n\n        # valid = [1, 0], fake = [0, 1]\n        d_valid = np.concatenate((np.ones((half_batch, 1)), np.zeros((half_batch, 1))), axis = 1)\n        d_fake  = np.concatenate((np.zeros((half_batch, 1)), np.ones((half_batch, 1))), axis = 1)\n\n        # discriminator training\n        d_loss_real, d_acc_real = discriminator.train_on_batch(imgs, d_valid, epoch_num = epoch_k_p1, batch_num = epoch_k_p1, batch_size = half_batch)\n        d_loss_fake, d_acc_fake = discriminator.train_on_batch(gen_imgs, d_fake, epoch_num = epoch_k_p1, batch_num = epoch_k_p1, batch_size = half_batch)\n\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n        d_acc  = 0.5 * np.add(d_acc_real, d_acc_fake)\n\n        if verbose:\n            print('Epoch {} K:{} Discriminator Loss: {:2.4f}, Acc: {:2.4f}.'.format(epoch_idx_p1, epoch_k+1, d_loss, d_acc))\n\n    # end of for epoch_k in range(1):\n\n    model_stats['d_train_loss'].append(d_loss)\n    model_stats['d_train_acc'].append(d_acc)\n\n    # set the discriminator to not trainable\n    discriminator.trainable = False\n\n    # discriminator training\n    g_noise = np.random.normal(0, 1, (batch_size, latent_dim))\n\n    # g_valid = [1, 0]\n    g_valid = np.concatenate((np.ones((batch_size, 1)), np.zeros((batch_size, 1))), axis = 1)\n\n    # train the generator\n    g_loss, g_acc = generator_discriminator.train_on_batch(g_noise, g_valid, epoch_num = epoch_idx_p1, batch_num = epoch_idx_p1, batch_size = batch_size)\n\n    model_stats['g_train_loss'].append(g_loss)\n    model_stats['g_train_acc'].append(g_acc)\n\n    if epoch_idx % gen_epoch == 0 and epoch_idx > 0:\n        plot_generated_img_samples(None,\n                                          generator.predict(gen_noise).reshape((-1, img_rows, img_cols)),\n                                          to_save    = True,\n                                          iteration  = epoch_idx,\n                                          model_name = model_name)\n\n    if verbose:\n        print('{}Epoch {} Discriminator Loss: {:2.4f}, Acc: {:2.4f}.'.format(print_pad(1), epoch_idx_p1, d_loss, d_acc))\n        print('Epoch {} Generator Loss: {:2.4f}, Acc: {:2.4f}.{}'.format(epoch_idx_p1, g_loss, g_acc, print_pad(1)))\n    else:\n        computebar(model_epochs, epoch_idx)\n\nplot_metric('loss', model_epochs, model_stats['d_train_loss'], model_stats['g_train_loss'], legend = ['D', 'G'], model_name = model_name)\nplot_metric('accuracy', model_epochs, model_stats['d_train_acc'], model_stats['g_train_acc'], legend = ['D', 'G'], model_name = model_name)\n\nplot_generated_img_samples(None,\n                                  generator.predict(gen_noise).reshape((-1, img_rows, img_cols)),\n                                  to_save    = False,\n                                  iteration  = model_epochs,\n                                  model_name = model_name)\n"""
examples/digits/digits_gan.py,10,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.digits import fetch_digits\nfrom ztlearn.dl.layers import BatchNormalization, Dense, Dropout, Activation\n\n\ndata = fetch_digits()\nplot_img_samples(data.data, None)\n\nimg_rows = 8\nimg_cols = 8\nimg_dim  = 64  # is the product (img_rows * img_cols)\n\nlatent_dim = 100\nbatch_size = 128\nhalf_batch = int(batch_size * 0.5)\n\nverbose   = True\ninit_type = 'he_uniform'\n\ngen_epoch = 500\ngen_noise = np.random.normal(0, 1, (36, latent_dim))  # 36 as batch size and is also the number of sample to be generated at the prediction stage\n\nmodel_epochs = 8000\nmodel_stats  = {'d_train_loss': [], 'd_train_acc': [], 'g_train_loss': [], 'g_train_acc': []}\n\nd_opt = register_opt(optimizer_name = 'adam', beta1 = 0.5, lr = 0.0001)\ng_opt = register_opt(optimizer_name = 'adam', beta1 = 0.5, lr = 0.00001)\n\ndef stack_generator_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(128, input_shape = (latent_dim,)))\n    model.add(Activation('leaky_relu'))\n    model.add(BatchNormalization(momentum = 0.8))\n    model.add(Dense(256))\n    model.add(Activation('leaky_relu'))\n    model.add(BatchNormalization(momentum = 0.8))\n    model.add(Dense(512))\n    model.add(Activation('leaky_relu'))\n    model.add(BatchNormalization(momentum = 0.8))\n    model.add(Dense(img_dim, activation = 'tanh'))\n\n    return model\n\ndef stack_discriminator_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(256, input_shape = (img_dim,)))\n    model.add(Activation('leaky_relu', alpha = 0.2))\n    model.add(Dropout(0.25))\n    model.add(Dense(128))\n    model.add(Activation('leaky_relu', alpha = 0.2))\n    model.add(Dropout(0.25))\n    model.add(Dense(2, activation = 'sigmoid'))\n\n    return model\n\n# stack and compile the generator\ngenerator = stack_generator_layers(init = init_type)\ngenerator.compile(loss = 'cce', optimizer = g_opt)\n\n# stack and compile the discriminator\ndiscriminator = stack_discriminator_layers(init = init_type)\ndiscriminator.compile(loss = 'cce', optimizer = d_opt)\n\n# stack and compile the generator_discriminator\ngenerator_discriminator = Sequential(init_method = init_type)\ngenerator_discriminator.layers.extend(generator.layers)\ngenerator_discriminator.layers.extend(discriminator.layers)\ngenerator_discriminator.compile(loss = 'cce', optimizer = g_opt)\n\ngenerator.summary('digits generator')\ndiscriminator.summary('digits discriminator')\n\ngenerator_discriminator.summary('digits gan')\nmodel_name = generator_discriminator.model_name\n\n# rescale to range [-1, 1]\nimages = range_normalize(data.data.astype(np.float32))\n\nfor epoch_idx in range(model_epochs):\n\n    # set the epoch id for print out\n    epoch_idx_p1 = epoch_idx + 1\n\n    # set the discriminator to trainable\n    discriminator.trainable = True\n\n    for epoch_k in range(10):\n\n        # set the epoch id for print out\n        epoch_k_p1 = epoch_k + 1\n\n        # draw random samples from real images\n        index = np.random.choice(images.shape[0], half_batch, replace = False)\n\n        imgs = images[index]\n\n        d_noise = np.random.normal(0, 1, (half_batch, latent_dim))\n\n        # generate a batch of new images\n        gen_imgs = generator.predict(d_noise)\n\n        # valid = [1, 0], fake = [0, 1]\n        d_valid = np.concatenate((np.ones((half_batch, 1)), np.zeros((half_batch, 1))), axis = 1)\n        d_fake  = np.concatenate((np.zeros((half_batch, 1)), np.ones((half_batch, 1))), axis = 1)\n\n        # discriminator training\n        d_loss_real, d_acc_real = discriminator.train_on_batch(imgs, d_valid, epoch_num = epoch_k_p1, batch_num = epoch_k_p1, batch_size = half_batch)\n        d_loss_fake, d_acc_fake = discriminator.train_on_batch(gen_imgs, d_fake, epoch_num = epoch_k_p1, batch_num = epoch_k_p1, batch_size = half_batch)\n\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n        d_acc  = 0.5 * np.add(d_acc_real, d_acc_fake)\n\n        if verbose:\n            print('Epoch {} K:{} Discriminator Loss: {:2.4f}, Acc: {:2.4f}.'.format(epoch_idx_p1, epoch_k+1, d_loss, d_acc))\n\n    # end of for epoch_k in range(10):\n\n    model_stats['d_train_loss'].append(d_loss)\n    model_stats['d_train_acc'].append(d_acc)\n\n    # set the discriminator to not trainable\n    discriminator.trainable = False\n\n    # discriminator training\n    g_noise = np.random.normal(0, 1, (batch_size, latent_dim))\n\n    # g_valid = [1, 0]\n    g_valid = np.concatenate((np.ones((batch_size, 1)), np.zeros((batch_size, 1))), axis = 1)\n\n    # train the generator\n    g_loss, g_acc = generator_discriminator.train_on_batch(g_noise, g_valid, epoch_num = epoch_idx_p1, batch_num = epoch_idx_p1, batch_size = batch_size)\n\n    model_stats['g_train_loss'].append(g_loss)\n    model_stats['g_train_acc'].append(g_acc)\n\n    if epoch_idx % gen_epoch == 0 and epoch_idx > 0:\n        plot_generated_img_samples(None,\n                                          generator.predict(gen_noise).reshape((-1, img_rows, img_cols)),\n                                          to_save    = True,\n                                          iteration  = epoch_idx,\n                                          model_name = model_name)\n\n    if verbose:\n        print('{}Epoch {} Discriminator Loss: {:2.4f}, Acc: {:2.4f}.'.format(print_pad(1), epoch_idx_p1, d_loss, d_acc))\n        print('Epoch {} Generator Loss: {:2.4f}, Acc: {:2.4f}.{}'.format(epoch_idx_p1, g_loss, g_acc, print_pad(1)))\n    else:\n        computebar(model_epochs, epoch_idx)\n\nplot_metric('loss', model_epochs, model_stats['d_train_loss'], model_stats['g_train_loss'], legend = ['D', 'G'], model_name = model_name)\nplot_metric('accuracy', model_epochs, model_stats['d_train_acc'], model_stats['g_train_acc'], legend = ['D', 'G'], model_name = model_name)\n\nplot_generated_img_samples(None,\n                                  generator.predict(gen_noise).reshape((-1, img_rows, img_cols)),\n                                  to_save    = False,\n                                  iteration  = model_epochs,\n                                  model_name = model_name)\n"""
examples/digits/digits_gru.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.digits import fetch_digits\nfrom ztlearn.dl.layers import GRU, Dense, Flatten\n\ndata = fetch_digits()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  data.target,\n                                                                  test_size   = 0.33,\n                                                                  random_seed = 15)\n\n# plot samples of training data\nplot_img_samples(train_data, train_label)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'rmsprop', momentum = 0.01, lr = 0.001)\n\n# model definition\nmodel = Sequential()\nmodel.add(GRU(128, activation = 'tanh', input_shape = (8, 8)))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('digits_gru')\n\nmodel_epochs = 100\nfit_stats = model.fit(train_data.reshape(-1, 8, 8),\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data.reshape(-1, 8, 8), one_hot(test_label)),\n                      shuffle_data    = True)\n\npredictions = unhot(model.predict(test_data.reshape(-1, 8, 8), True))\nprint_results(predictions, test_label)\nplot_img_results(test_data, test_label, predictions)\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/digits/digits_lstm.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.digits import fetch_digits\nfrom ztlearn.dl.layers import LSTM, Dense, Flatten\n\ndata = fetch_digits()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  data.target,\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 15)\n\n# plot samples of training data\nplot_img_samples(train_data, train_label)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.001)\n\n# Model definition\nmodel = Sequential()\nmodel.add(LSTM(128, activation = 'tanh', input_shape = (8, 8)))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('digits lstm')\n\nmodel_epochs = 100\nfit_stats = model.fit(train_data.reshape(-1, 8, 8),\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data.reshape(-1, 8, 8), one_hot(test_label)),\n                      shuffle_data    = True)\n\npredictions = unhot(model.predict(test_data.reshape(-1, 8, 8), True))\nprint_results(predictions, test_label)\nplot_img_results(test_data, test_label, predictions)\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/digits/digits_mlp.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.digits import fetch_digits\nfrom ztlearn.dl.layers import Dropout, Dense, BatchNormalization\n\n# NOTE: Check the random_seed seeding for improperly shuffled data.\ndata = fetch_digits()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  data.target,\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 3)\n\n# plot samples of training data\nplot_tiled_img_samples(train_data, train_label)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.001)\n# opt = register_opt(optimizer_name = 'nestrov', momentum = 0.01, lr = 0.0001)\n\n# 1. model definition\nmodel = Sequential(init_method = 'he_normal')\nmodel.add(Dense(256, activation = 'relu', input_shape = (64,)))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation = 'relu')) # 10 digits classes\nmodel.compile(loss = 'cce', optimizer = opt)\n\n'''\n# 2. model definition\nmodel = Sequential()\nmodel.add(Dense(256, activation = 'tanh', input_shape=(64,)))\nmodel.add(Dense(10, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'cce', optimizer = opt)\n'''\n\nmodel.summary(model_name = 'digits mlp')\n\nmodel_epochs = 12\nfit_stats = model.fit(train_data,\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data, one_hot(test_label)),\n                      shuffle_data    = True)\n\neval_stats  = model.evaluate(test_data, one_hot(test_label))\npredictions = unhot(model.predict(test_data, True))\nprint_results(predictions, test_label)\n\nplot_img_results(test_data, test_label, predictions)\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\nplot_metric('evaluation',\n                          eval_stats['valid_batches'],\n                          eval_stats['valid_loss'],\n                          eval_stats['valid_acc'],\n                          model_name = model.model_name,\n                          legend     = ['loss', 'acc'])\n"""
examples/digits/digits_pca.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import plot_pca\nfrom ztlearn.ml.decomposition import PCA\nfrom ztlearn.datasets.digits import fetch_digits\n\ndata = fetch_digits()\n\npca        = PCA(n_components = 2)\ncomponents = pca.fit_transform(data.data)\n\nplot_pca(components, n_components = 2, colour_array = data.target, model_name = 'DIGITS PCA')\n"""
examples/digits/digits_perceptron.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.digits import fetch_digits\nfrom ztlearn.ml.classification import Perceptron\n\ndata = fetch_digits()\ntrain_data, test_data, train_label, test_label = train_test_split(normalize(data.data),\n                                                                  one_hot(data.target),\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 5)\n\n# plot samples of training data\nplot_img_samples(train_data, unhot(train_label))\n\n# optimizer definition\nopt = register_opt(optimizer_name = \'sgd_momentum\', momentum = 0.01, lr = 0.01)\n\n# model definition\nmodel     = Perceptron(epochs = 1500, activation = \'softmax\', loss = \'cce\', init_method = \'he_uniform\', optimizer = opt)\nfit_stats = model.fit(train_data, train_label)\n\npredictions = unhot(model.predict(test_data))\nprint_results(predictions, unhot(test_label))\n\nplot_img_results(test_data, unhot(test_label), predictions)\nplot_metric(\'accuracy_loss\',\n                              len(fit_stats[""train_loss""]),\n                              fit_stats[\'train_acc\'],\n                              fit_stats[\'train_loss\'],\n                              model_name = \'digits_perceptron\',\n                              legend     = [\'acc\', \'loss\'])\n'"
examples/digits/digits_rnn.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.digits import fetch_digits\nfrom ztlearn.dl.layers import RNN, Dense, Flatten\n\ndata = fetch_digits()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  data.target,\n                                                                  test_size   = 0.4,\n                                                                  random_seed = 5)\n\n# plot samples of training data\nplot_img_samples(train_data, train_label)\n\n# model definition\nmodel = Sequential()\nmodel.add(RNN(128, activation = 'tanh', bptt_truncate = 5, input_shape = (8, 8)))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n# NOTE: in model.compile you could use a string to define the optimization type\n\nmodel.summary(model_name = 'digits rnn')\n\nmodel_epochs = 200\nfit_stats = model.fit(train_data.reshape(-1, 8, 8),\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data.reshape(-1, 8, 8), one_hot(test_label)),\n                      shuffle_data    = True)\n\npredictions = unhot(model.predict(test_data.reshape(-1, 8, 8), True))\nprint_results(predictions, test_label)\nplot_img_results(test_data, test_label, predictions)\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/fashion_mnist/fashion_mnist_autoencoder.py,1,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.dl.layers import BatchNormalization, Dense\nfrom ztlearn.datasets.fashion import fetch_fashion_mnist\n\nimg_rows   = 28\nimg_cols   = 28\nimg_dim    = 784  # img_rows * img_cols\nlatent_dim = 8\ninit_type  = 'he_normal'\n\ndef stack_encoder_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(256, activation = 'relu', input_shape = (img_dim,)))\n    model.add(BatchNormalization())\n    model.add(Dense(128, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(latent_dim, activation = 'relu'))\n\n    return model\n\ndef stack_decoder_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(128, activation = 'relu', input_shape = (latent_dim,)))\n    model.add(BatchNormalization())\n    model.add(Dense(256, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(img_dim, activation = 'sigmoid'))\n\n    return model\n\nencoder = stack_encoder_layers(init = init_type)\ndecoder = stack_decoder_layers(init = init_type)\n\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.0001)\n\nautoencoder = Sequential(init_method = init_type)\nautoencoder.layers.extend(encoder.layers)\nautoencoder.layers.extend(decoder.layers)\nautoencoder.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nencoder.summary('fashion mnist encoder')\ndecoder.summary('fashion mnist decoder')\n\nautoencoder.summary('fashion mnist autoencoder')\n\nmodel_name = autoencoder.model_name\n\nfashion_mnist  = fetch_fashion_mnist()\nimages = range_normalize(fashion_mnist.data.astype(np.float32), 0, 1)  # rescale to range [0, 1]\n\ntrain_data, test_data, train_label, test_label = train_test_split(images,\n                                                                  images,\n                                                                  test_size   = 0.2,\n                                                                  random_seed = 15,\n                                                                  cut_off     = 2000)\nplot_img_samples(train_data[:40], None, dataset = 'mnist')\n\nmodel_epochs = 500\nfit_stats = autoencoder.fit(train_data,\n                            train_label,\n                            batch_size      = 64,\n                            epochs          = model_epochs,\n                            validation_data = (test_data, test_label),\n                            shuffle_data    = True)\n\n# generate non rescaled test labels for use in generated digits plot\n_, _, _, test_label = train_test_split(fashion_mnist.data, fashion_mnist.target.astype('int'), test_size = 0.2, random_seed = 15)\npredictions         = autoencoder.predict(test_data).reshape((-1, img_rows, img_cols))\n\nplot_generated_img_samples(unhot(one_hot(test_label)),\n                                                        predictions,\n                                                        to_save    = False,\n                                                        iteration  = model_epochs,\n                                                        model_name = model_name)\n\nplot_metric('loss',     model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'],  fit_stats['valid_acc'],  model_name = model_name)\n"""
examples/fashion_mnist/fashion_mnist_cnn.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.fashion import fetch_fashion_mnist\nfrom ztlearn.dl.layers import BatchNormalization, Conv2D\nfrom ztlearn.dl.layers import Dropout, Dense, Flatten, MaxPooling2D\n\n\nfashion_mnist = fetch_fashion_mnist()\ninput_data, input_label = shuffle_data(fashion_mnist.data, fashion_mnist.target, random_seed = 5)\ntrain_data, test_data, train_label, test_label = train_test_split(input_data,\n                                                                  input_label.astype('int'),\n                                                                  test_size   = 0.33,\n                                                                  random_seed = 5,\n                                                                  cut_off     = 2000)\n\n# normalize to range [0, 1]\ntrain_data = range_normalize(train_data.astype('float32'), 0, 1)\ntest_data  = range_normalize(test_data.astype('float32'), 0, 1)\n\n# plot samples of training data\nplot_img_samples(train_data[:40], train_label[:40], dataset = 'mnist')\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.001)\n\n# model definition\nmodel = Sequential(init_method = 'he_uniform')\nmodel.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', input_shape = (1, 28, 28), padding = 'same'))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(MaxPooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary(model_name = 'fashion mnist cnn')\n\nmodel_epochs = 12\nfit_stats = model.fit(train_data.reshape(-1, 1, 28, 28),\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data.reshape(-1, 1, 28, 28), one_hot(test_label)),\n                      shuffle_data    = True)\n\npredictions = unhot(model.predict(test_data.reshape(-1, 1, 28, 28), True))\nprint_results(predictions, test_label)\nplot_img_results(test_data[:40], test_label[:40], predictions, dataset = 'mnist')\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/fashion_mnist/fashion_mnist_dcgan.py,10,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.fashion import fetch_fashion_mnist\nfrom ztlearn.dl.layers import Activation, BatchNormalization, Conv2D\nfrom ztlearn.dl.layers import Dense, Dropout, Flatten, Reshape, UpSampling2D\n\n\nfashion_mnist       = fetch_fashion_mnist()\nmnist_data, _, _, _ = train_test_split(fashion_mnist.data, fashion_mnist.target, test_size = 0.0, cut_off = 2000)\n\n# plot samples of training data\nplot_img_samples(mnist_data[:40], None, dataset = 'mnist')\n\nimg_rows     = 28\nimg_cols     = 28\nimg_channels = 1\nimg_dims     = (img_channels, img_rows, img_cols)\n\nlatent_dim = 100\nbatch_size = 128\nhalf_batch = int(batch_size * 0.5)\n\nverbose   = True\ninit_type = 'he_uniform'\n\ngen_epoch = 50\ngen_noise = np.random.normal(0, 1, (36, latent_dim)) # for tiles 6 by 6 i.e (36) image generation\n\nmodel_epochs = 400\nmodel_stats  = {'d_train_loss': [], 'd_train_acc': [], 'g_train_loss': [], 'g_train_acc': []}\n\nd_opt = register_opt(optimizer_name = 'adam', beta1 = 0.5, lr = 0.0002)\ng_opt = register_opt(optimizer_name = 'adam', beta1 = 0.5, lr = 0.0002)\n\n\ndef stack_generator_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(128*7*7, input_shape = (latent_dim,)))\n    model.add(Activation('leaky_relu'))\n    model.add(BatchNormalization(momentum = 0.8))\n    model.add(Reshape((128, 7, 7)))\n    model.add(UpSampling2D())\n    model.add(Conv2D(64, kernel_size = (5, 5), padding = 'same'))\n    model.add(BatchNormalization(momentum = 0.8))\n    model.add(Activation('leaky_relu'))\n    model.add(UpSampling2D())\n    model.add(Conv2D(img_channels, kernel_size = (5, 5), padding = 'same'))\n    model.add(Activation('tanh'))\n\n    return model\n\n\ndef stack_discriminator_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Conv2D(64, kernel_size = (5, 5), padding = 'same', input_shape = img_dims))\n    model.add(Activation('leaky_relu'))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(128, kernel_size = (5, 5), padding = 'same'))\n    model.add(Activation('leaky_relu'))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(2))\n    model.add(Activation('sigmoid'))\n\n    return model\n\n\n# stack and compile the generator\ngenerator = stack_generator_layers(init = init_type)\ngenerator.compile(loss = 'cce', optimizer = g_opt)\n\n# stack and compile the discriminator\ndiscriminator = stack_discriminator_layers(init = init_type)\ndiscriminator.compile(loss = 'cce', optimizer = d_opt)\n\n# stack and compile the generator_discriminator\ngenerator_discriminator = Sequential(init_method = init_type)\ngenerator_discriminator.layers.extend(generator.layers)\ngenerator_discriminator.layers.extend(discriminator.layers)\ngenerator_discriminator.compile(loss = 'cce', optimizer = g_opt)\n\ngenerator.summary('fashion mnist generator')\ndiscriminator.summary('fashion mnist discriminator')\n\ngenerator_discriminator.summary('fashion mnist dcgan')\nmodel_name = generator_discriminator.model_name\n\n# rescale to range [-1, 1]\nimages = range_normalize(mnist_data.reshape((-1,) + img_dims).astype(np.float32))\n\nfor epoch_idx in range(model_epochs):\n\n    # set the epoch id for print out\n    epoch_idx_p1 = epoch_idx + 1\n\n    # set the discriminator to trainable\n    discriminator.trainable = True\n\n    for epoch_k in range(1):\n\n        # set the epoch id for print out\n        epoch_k_p1 = epoch_k + 1\n\n        # draw random samples from real images\n        index = np.random.choice(images.shape[0], half_batch, replace = False)\n\n        imgs = images[index]\n\n        d_noise = np.random.normal(0, 1, (half_batch, latent_dim))\n\n        # generate a batch of new images\n        gen_imgs = generator.predict(d_noise)\n\n        # valid = [1, 0], fake = [0, 1]\n        d_valid = np.concatenate((np.ones((half_batch, 1)), np.zeros((half_batch, 1))), axis = 1)\n        d_fake  = np.concatenate((np.zeros((half_batch, 1)), np.ones((half_batch, 1))), axis = 1)\n\n        # discriminator training\n        d_loss_real, d_acc_real = discriminator.train_on_batch(imgs, d_valid, epoch_num = epoch_k_p1, batch_num = epoch_k_p1, batch_size = half_batch)\n        d_loss_fake, d_acc_fake = discriminator.train_on_batch(gen_imgs, d_fake, epoch_num = epoch_k_p1, batch_num = epoch_k_p1, batch_size = half_batch)\n\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n        d_acc  = 0.5 * np.add(d_acc_real, d_acc_fake)\n\n        if verbose:\n            print('Epoch {} K:{} Discriminator Loss: {:2.4f}, Acc: {:2.4f}.'.format(epoch_idx_p1, epoch_k+1, d_loss, d_acc))\n\n    # end of for epoch_k in range(2):\n\n    model_stats['d_train_loss'].append(d_loss)\n    model_stats['d_train_acc'].append(d_acc)\n\n    # set the discriminator to not trainable\n    discriminator.trainable = False\n\n    # discriminator training\n    g_noise = np.random.normal(0, 1, (batch_size, latent_dim))\n\n    # g_valid = [1, 0]\n    g_valid = np.concatenate((np.ones((batch_size, 1)), np.zeros((batch_size, 1))), axis = 1)\n\n    # train the generator\n    g_loss, g_acc = generator_discriminator.train_on_batch(g_noise, g_valid, epoch_num = epoch_idx_p1, batch_num = epoch_idx_p1, batch_size = batch_size)\n\n    model_stats['g_train_loss'].append(g_loss)\n    model_stats['g_train_acc'].append(g_acc)\n\n    if epoch_idx % gen_epoch == 0 and epoch_idx > 0:\n        plot_generated_img_samples(None,\n                                         generator.predict(gen_noise).reshape((-1, img_rows, img_cols)),\n                                         to_save    = True,\n                                         iteration  = epoch_idx,\n                                         model_name = model_name)\n\n    if verbose:\n        print('{}Epoch {} Discriminator Loss: {:2.4f}, Acc: {:2.4f}.'.format(print_pad(1), epoch_idx_p1, d_loss, d_acc))\n        print('Epoch {} Generator Loss: {:2.4f}, Acc: {:2.4f}.{}'.format(epoch_idx_p1, g_loss, g_acc, print_pad(1)))\n    else:\n        computebar(model_epochs, epoch_idx)\n\nplot_metric('loss', model_epochs, model_stats['d_train_loss'], model_stats['g_train_loss'], legend = ['D', 'G'], model_name = model_name)\nplot_metric('accuracy', model_epochs, model_stats['d_train_acc'],  model_stats['g_train_acc'], legend = ['D', 'G'], model_name = model_name)\n\nplot_generated_img_samples(None,\n                                 generator.predict(gen_noise).reshape((-1, img_rows, img_cols)),\n                                 to_save    = False,\n                                 iteration  = model_epochs,\n                                 model_name = model_name)\n"""
examples/fashion_mnist/fashion_mnist_gan.py,11,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.fashion import fetch_fashion_mnist\nfrom ztlearn.dl.layers import BatchNormalization, Dense, Dropout, Activation\n\n\nfashion_mnist       = fetch_fashion_mnist()\nmnist_data, _, _, _ = train_test_split(fashion_mnist.data, fashion_mnist.target, test_size = 0.0, cut_off = 2000)\n\n# plot samples of training data\nplot_img_samples(mnist_data[:40], None, dataset = 'mnist')\n\nimg_rows = 28\nimg_cols = 28\nimg_dim  = 784  # is the product (img_rows * img_cols)\n\nlatent_dim = 100\nbatch_size = 128\nhalf_batch = int(batch_size * 0.5)\n\nverbose   = True\ninit_type = 'he_uniform'\n\ngen_epoch = 500\ngen_noise = np.random.normal(0, 1, (36, latent_dim)) # 36 as batch size and is also the number of sample to be generated at the prediction stage\n\nmodel_epochs = 8000\nmodel_name   = 'mnist_gan'\nmodel_stats  = {'d_train_loss': [], 'd_train_acc': [], 'g_train_loss': [], 'g_train_acc': []}\n\nd_opt = register_opt(optimizer_name = 'adam', beta1 = 0.5, lr = 0.001)\ng_opt = register_opt(optimizer_name = 'adam', beta1 = 0.5, lr = 0.0001)\n\ndef stack_generator_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(256, input_shape = (latent_dim,)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization(momentum = 0.8))\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization(momentum = 0.8))\n    model.add(Dense(1024))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization(momentum = 0.8))\n    model.add(Dense(img_dim, activation = 'tanh'))\n\n    return model\n\ndef stack_discriminator_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(512, input_shape = (img_dim,)))\n    model.add(Activation('leaky_relu', alpha = 0.2))\n    model.add(Dropout(0.25))\n    model.add(Dense(256))\n    model.add(Activation('leaky_relu', alpha = 0.2))\n    model.add(Dropout(0.25))\n    model.add(Dense(2, activation = 'sigmoid'))\n\n    return model\n\n# stack and compile the generator\ngenerator = stack_generator_layers(init = init_type)\ngenerator.compile(loss = 'cce', optimizer = g_opt)\n\n# stack and compile the discriminator\ndiscriminator = stack_discriminator_layers(init = init_type)\ndiscriminator.compile(loss = 'cce', optimizer = d_opt)\n\n# stack and compile the generator_discriminator\ngenerator_discriminator = Sequential(init_method = init_type)\ngenerator_discriminator.layers.extend(generator.layers)\ngenerator_discriminator.layers.extend(discriminator.layers)\ngenerator_discriminator.compile(loss = 'cce', optimizer = g_opt)\n\ngenerator.summary('fashion mnist generator')\ndiscriminator.summary('fashion mnist discriminator')\n\ngenerator_discriminator.summary('fashion mnist gan')\nmodel_name = generator_discriminator.model_name\n\n# rescale to range [-1, 1]\nimages = range_normalize(mnist_data.astype(np.float32))\n\nfor epoch_idx in range(model_epochs):\n\n    # set the epoch id for print out\n    epoch_idx_p1 = epoch_idx + 1\n\n    # set the discriminator to trainable\n    discriminator.trainable = True\n\n    for epoch_k in range(2):\n\n        # set the epoch id for print out\n        epoch_k_p1 = epoch_k + 1\n\n        # draw random samples from real images\n        index = np.random.choice(images.shape[0], half_batch, replace = False)\n\n        # index = np.random.randint(0, images.shape[0], half_batch)\n        imgs = images[index]\n\n        d_noise = np.random.normal(0, 1, (half_batch, latent_dim))\n\n        # generate a batch of new images\n        gen_imgs = generator.predict(d_noise)\n\n        # valid = [1, 0], fake = [0, 1]\n        d_valid = np.concatenate((np.ones((half_batch, 1)), np.zeros((half_batch, 1))), axis = 1)\n        d_fake  = np.concatenate((np.zeros((half_batch, 1)), np.ones((half_batch, 1))), axis = 1)\n\n        # discriminator training\n        d_loss_real, d_acc_real = discriminator.train_on_batch(imgs, d_valid, epoch_num = epoch_k_p1, batch_num = epoch_k_p1, batch_size = half_batch)\n        d_loss_fake, d_acc_fake = discriminator.train_on_batch(gen_imgs, d_fake, epoch_num = epoch_k_p1, batch_num = epoch_k_p1, batch_size = half_batch)\n\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n        d_acc  = 0.5 * np.add(d_acc_real, d_acc_fake)\n\n        if verbose:\n            print('Epoch {} K:{} Discriminator Loss: {:2.4f}, Acc: {:2.4f}.'.format(epoch_idx_p1, epoch_k+1, d_loss, d_acc))\n\n    # end of for epoch_k in range(1):\n\n    model_stats['d_train_loss'].append(d_loss)\n    model_stats['d_train_acc'].append(d_acc)\n\n    # set the discriminator to not trainable\n    discriminator.trainable = False\n\n    # discriminator training\n    g_noise = np.random.normal(0, 1, (batch_size, latent_dim))\n\n    # g_valid = [1, 0]\n    g_valid = np.concatenate((np.ones((batch_size, 1)), np.zeros((batch_size, 1))), axis = 1)\n\n    # train the generator\n    g_loss, g_acc = generator_discriminator.train_on_batch(g_noise, g_valid, epoch_num = epoch_idx_p1, batch_num = epoch_idx_p1, batch_size = batch_size)\n\n    model_stats['g_train_loss'].append(g_loss)\n    model_stats['g_train_acc'].append(g_acc)\n\n    if epoch_idx % gen_epoch == 0 and epoch_idx > 0:\n        plot_generated_img_samples(None,\n                                         generator.predict(gen_noise).reshape((-1, img_rows, img_cols)),\n                                         to_save    = True,\n                                         iteration  = epoch_idx,\n                                         model_name = model_name)\n\n    if verbose:\n        print('{}Epoch {} Discriminator Loss: {:2.4f}, Acc: {:2.4f}.'.format(print_pad(1), epoch_idx_p1, d_loss, d_acc))\n        print('Epoch {} Generator Loss: {:2.4f}, Acc: {:2.4f}.{}'.format(epoch_idx_p1, g_loss, g_acc, print_pad(1)))\n    else:\n        computebar(model_epochs, epoch_idx)\n\nplot_metric('loss', model_epochs, model_stats['d_train_loss'], model_stats['g_train_loss'], legend = ['D', 'G'], model_name = model_name)\nplot_metric('accuracy', model_epochs, model_stats['d_train_acc'], model_stats['g_train_acc'], legend = ['D', 'G'], model_name = model_name)\n\nplot_generated_img_samples(None,\n                                 generator.predict(gen_noise).reshape((-1, img_rows, img_cols)),\n                                 to_save    = False,\n                                 iteration  = model_epochs,\n                                 model_name = model_name)\n"""
examples/fashion_mnist/fashion_mnist_gru.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.dl.layers import GRU, Dense, Flatten\nfrom ztlearn.datasets.fashion import fetch_fashion_mnist\n\nfashion_mnist = fetch_fashion_mnist()\ntrain_data, test_data, train_label, test_label = train_test_split(fashion_mnist.data,\n                                                                  fashion_mnist.target.astype('int'),\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 15,\n                                                                  cut_off     = 2000)\n\n# plot samples of training data\nplot_img_samples(train_data, train_label, dataset = 'mnist')\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'rmsprop', momentum = 0.01, lr = 0.001)\n\n# model definition\nmodel = Sequential()\nmodel.add(GRU(128, activation = 'tanh', input_shape = (28, 28)))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary(model_name = 'fashion mnist gru')\n\nmodel_epochs = 100\nfit_stats = model.fit(train_data.reshape(-1, 28, 28),\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data.reshape(-1, 28, 28), one_hot(test_label)),\n                      shuffle_data    = True)\n\npredictions = unhot(model.predict(test_data.reshape(-1, 28, 28), True))\nprint_results(predictions, test_label)\nplot_img_results(test_data, test_label, predictions, dataset = 'mnist')\n\nmodel_name = model.model_name\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model_name)\n"""
examples/fashion_mnist/fashion_mnist_lstm.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.dl.layers import LSTM, Dense, Flatten\nfrom ztlearn.datasets.fashion import fetch_fashion_mnist\n\nfashion_mnist = fetch_fashion_mnist()\ntrain_data, test_data, train_label, test_label = train_test_split(fashion_mnist.data,\n                                                                  fashion_mnist.target.astype('int'),\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 15,\n                                                                  cut_off     = 2000)\n\n# plot samples of training data\nplot_img_samples(train_data[:40], train_label[:40], dataset = 'mnist')\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'rmsprop', momentum = 0.01, lr = 0.001)\n\n# model definition\nmodel = Sequential()\nmodel.add(LSTM(128, activation = 'tanh', input_shape = (28, 28)))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('fashion mnist lstm')\n\nmodel_epochs = 100\nfit_stats = model.fit(train_data.reshape(-1, 28, 28),\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data.reshape(-1, 28, 28), one_hot(test_label)),\n                      shuffle_data    = True)\n\npredictions = unhot(model.predict(test_data.reshape(-1, 28, 28), True))\nprint_results(predictions, test_label)\nplot_img_results(test_data[:40], test_label[:40], predictions, dataset = 'mnist')\n\nmodel_name = model.model_name\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model_name)\n"""
examples/fashion_mnist/fashion_mnist_mlp.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.fashion import fetch_fashion_mnist\nfrom ztlearn.dl.layers import Dropout, Dense, BatchNormalization\n\nfashion_mnist = fetch_fashion_mnist()\ntrain_data, test_data, train_label, test_label = train_test_split(fashion_mnist.data,\n                                                                  fashion_mnist.target.astype('int'),\n                                                                  test_size   = 0.33,\n                                                                  random_seed = 5,\n                                                                  cut_off     = None)\n\n# plot samples of training data\nplot_tiled_img_samples(train_data[:40], train_label[:40], dataset = 'mnist')\n\n# optimizer definition\n# opt = register_opt(optimizer_name = 'nestrov', momentum = 0.01, lr = 0.0001)\nopt = register_opt(optimizer_name = 'adam', momentum = 0.001, lr = 0.001)\n\n# model definition\nmodel = Sequential()\nmodel.add(Dense(512, activation = 'relu', input_shape = (784,)))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation = 'relu')) # 10 digits classes\nmodel.compile(loss = 'cce', optimizer = opt)\n\nmodel.summary()\n\nmodel_epochs = 5\nfit_stats = model.fit(train_data,\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data, one_hot(test_label)),\n                      shuffle_data    = True)\n\neval_stats = model.evaluate(test_data, one_hot(test_label))\n\npredictions = unhot(model.predict(test_data, True))\nprint_results(predictions, test_label)\nplot_img_results(test_data[:40], test_label[:40], predictions, dataset = 'mnist') # truncate to 40 samples\n\nmodel_name = model.model_name\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model_name)\nplot_metric('evaluation',\n                          eval_stats['valid_batches'],\n                          eval_stats['valid_loss'],\n                          eval_stats['valid_acc'],\n                          model_name = model_name,\n                          legend     = ['loss', 'acc'])\n"""
examples/fashion_mnist/fashion_mnist_pca.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import plot_pca\nfrom ztlearn.ml.decomposition import PCA\nfrom ztlearn.datasets.fashion import fetch_fashion_mnist\n\nfashion_mnist = fetch_fashion_mnist()\n\npca        = PCA(n_components = 2)\ncomponents = pca.fit_transform(fashion_mnist.data.astype('float64'))\n\nplot_pca(components, n_components = 2, colour_array = fashion_mnist.target.astype('int'), model_name = 'FASHION MNIST PCA')\n"""
examples/fashion_mnist/fashion_mnist_peceptron.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.ml.classification import Perceptron\nfrom ztlearn.datasets.fashion import fetch_fashion_mnist\n\nfashion_mnist = fetch_fashion_mnist()\ntrain_data, test_data, train_label, test_label = train_test_split(normalize(fashion_mnist.data.astype(\'float32\')),\n                                                                  one_hot(fashion_mnist.target.astype(\'int\')),\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 15,\n                                                                  cut_off     = 2000)\n\n# plot samples of training data\nplot_img_samples(train_data[:40], unhot(train_label[:40]), dataset = \'mnist\')\n\n# optimizer definition\nopt = register_opt(optimizer_name = \'sgd_momentum\', momentum = 0.01, lr = 0.01)\n\n# model definition\nmodel     = Perceptron(epochs = 4500, activation = \'softmax\', loss = \'cce\', init_method = \'he_normal\', optimizer = opt)\nfit_stats = model.fit(train_data, train_label)\n\npredictions = unhot(model.predict(test_data))\nprint_results(predictions, unhot(test_label))\nplot_img_results(test_data[:40], unhot(test_label[:40]), predictions, dataset = \'mnist\')\nplot_metric(\'accuracy_loss\',\n                             len(fit_stats[""train_loss""]),\n                             fit_stats[\'train_acc\'],\n                             fit_stats[\'train_loss\'],\n                             model_name = \'fashion_mnist_perceptron\',\n                             legend     = [\'acc\', \'loss\'])\n'"
examples/fashion_mnist/fashion_mnist_rnn.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.dl.layers import RNN, Dense, Flatten\nfrom ztlearn.datasets.fashion import fetch_fashion_mnist\n\nfashion_mnist = fetch_fashion_mnist()\ntrain_data, test_data, train_label, test_label = train_test_split(fashion_mnist.data,\n                                                                  fashion_mnist.target.astype('int'),\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 15,\n                                                                  cut_off     = 2000)\n\n# plot samples of training data\nplot_img_samples(train_data[:40], train_label[:40], dataset = 'mnist')\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.001)\n\n# model definition\nmodel = Sequential()\nmodel.add(RNN(128, activation = 'tanh', bptt_truncate = 5, input_shape = (28, 28)))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('fashion mnist rnn')\n\nmodel_epochs = 100\nfit_stats = model.fit(train_data.reshape(-1, 28, 28),\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data.reshape(-1, 28, 28), one_hot(test_label)),\n                      shuffle_data    = True)\n\npredictions = unhot(model.predict(test_data.reshape(-1, 28, 28), True))\n\nprint_results(predictions, test_label)\nplot_img_results(test_data[:40], test_label[:40], predictions, dataset = 'mnist')\n\nmodel_name = model.model_name\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model_name)\n"""
examples/iris/iris_kmeans_clustering.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import plot_kmeans\nfrom ztlearn.ml.clustering import KMeans\nfrom ztlearn.datasets.iris import fetch_iris\n\n\n# fetch dataset\ndata = fetch_iris()\n\n# model definition\nmodel = KMeans(n_clusters = 3, max_iter = 1000)\ncentroids = model.fit(data.data)\n\n# plot clusters and centroids\nplot_kmeans(data.data, data.target, centroids, model_clusters = 3)\n'"
examples/iris/iris_logistic_regression.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.iris import fetch_iris\nfrom ztlearn.ml.regression import LogisticRegression\n\ndata        = fetch_iris()\ninput_data  = normalize(data.data[data.target != 2])\ninput_label = data.target[data.target != 2]\n\ntrain_data, test_data, train_label, test_label = train_test_split(input_data,\n                                                                  input_label,\n                                                                  test_size   = 0.33,\n                                                                  random_seed = 15)\n\n# optimizer definition\nopt = register_opt(optimizer_name = \'sgd\', momentum = 0.01, lr = 0.01)\n\n# model definition\nmodel     = LogisticRegression(epochs = 1500, optimizer = opt)\nfit_stats = model.fit(train_data, train_label)\n\n# fit_stats = model.fit_NR(train_data, train_label) # --- Newton-Raphson Method\n\nprint_results(model.predict(test_data), test_label)\nplot_metric(\'accuracy_loss\',\n                              len(fit_stats[""train_loss""]),\n                              fit_stats[\'train_acc\'],\n                              fit_stats[\'train_loss\'],\n                              model_name = \'iris_logistic_regression\',\n                              legend     = [\'acc\', \'loss\'])\n'"
examples/iris/iris_mlp.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.layers import Dense\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.iris import fetch_iris\n\ndata = fetch_iris()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  data.target,\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 5)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'adam', momentum = 0.1, lr = 0.01)\n\n# model definition\nmodel = Sequential()\nmodel.add(Dense(10, activation = 'sigmoid', input_shape = (train_data.shape[1],)))\nmodel.add(Dense(3, activation = 'sigmoid')) # 3 iris_classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('iris mlp')\n\nmodel_epochs = 75\nfit_stats = model.fit(train_data,\n                      one_hot(train_label),\n                      batch_size      = 10,\n                      epochs          = model_epochs,\n                      validation_data = (test_data, one_hot(test_label)),\n                      shuffle_data    = True)\n\n# eval_stats = model.evaluate(test_data, one_hot(train_label))\npredictions = unhot(model.predict(test_data))\nprint_results(predictions, test_label)\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/iris/iris_pca.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import plot_pca\nfrom ztlearn.ml.decomposition import PCA\nfrom ztlearn.datasets.iris import fetch_iris\n\n# fetch dataset\ndata = fetch_iris()\n\n# model definition\npca        = PCA(n_components = 2)\ncomponents = pca.fit_transform(data.data.astype('float64'))\n\n# plot clusters\nplot_pca(components, n_components = 2, colour_array = data.target.astype('int'), model_name = 'IRIS PCA')\n"""
examples/iris/iris_perceptron.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.iris import fetch_iris\nfrom ztlearn.ml.classification import Perceptron\n\ndata = fetch_iris()\ntrain_data, test_data, train_label, test_label = train_test_split(data.data,\n                                                                  one_hot(data.target),\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 5)\n\n# optimizer definition\nopt = register_opt(optimizer_name = \'sgd_momentum\', momentum = 0.01, lr = 0.001)\n\n# model definition\nmodel     = Perceptron(epochs = 500, activation = \'softmax\', loss = \'cce\', init_method = \'he_normal\', optimizer = opt)\nfit_stats = model.fit(train_data, train_label)\n\nprint_results(unhot(model.predict(test_data)), unhot(test_label))\nplot_metric(\'accuracy_loss\',\n                             len(fit_stats[""train_loss""]),\n                             fit_stats[\'train_acc\'],\n                             fit_stats[\'train_loss\'],\n                             model_name = \'iris_perceptron\',\n                             legend     = [\'acc\', \'loss\'])\n'"
examples/mnist/mnist_autoencoder.py,1,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.mnist import fetch_mnist\nfrom ztlearn.dl.layers import BatchNormalization, Dense\n\nimg_rows   = 28\nimg_cols   = 28\nimg_dim    = 784  # img_rows * img_cols\nlatent_dim = 8\ninit_type  = 'he_normal'\n\ndef stack_encoder_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(256, activation = 'relu', input_shape = (img_dim,)))\n    model.add(BatchNormalization())\n    model.add(Dense(128, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(latent_dim, activation = 'relu'))\n\n    return model\n\ndef stack_decoder_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(128, activation = 'relu', input_shape = (latent_dim,)))\n    model.add(BatchNormalization())\n    model.add(Dense(256, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(img_dim, activation = 'sigmoid'))\n\n    return model\n\nencoder = stack_encoder_layers(init = init_type)\ndecoder = stack_decoder_layers(init = init_type)\n\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.0001)\n\nautoencoder = Sequential(init_method = init_type)\nautoencoder.layers.extend(encoder.layers)\nautoencoder.layers.extend(decoder.layers)\nautoencoder.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nencoder.summary('mnist encoder')\ndecoder.summary('mnist decoder')\n\nautoencoder.summary('mnist autoencoder')\n\nmnist  = fetch_mnist()\nimages = range_normalize(mnist.data.astype(np.float32), 0, 1)  # rescale to range [0, 1]\n\ntrain_data, test_data, train_label, test_label = train_test_split(images,\n                                                                  images,\n                                                                  test_size   = 0.2,\n                                                                  random_seed = 15,\n                                                                  cut_off     = 2000)\nplot_img_samples(train_data[:40], None, dataset = 'mnist')\n\nmodel_epochs = 500\nfit_stats = autoencoder.fit(train_data,\n                            train_label,\n                            batch_size      = 64,\n                            epochs          = model_epochs,\n                            validation_data = (test_data, test_label),\n                            shuffle_data    = True)\n\n# generate non rescaled test labels for use in generated digits plot\n_, _, _, test_label = train_test_split(mnist.data, mnist.target.astype('int'), test_size = 0.2, random_seed = 15)\npredictions         = autoencoder.predict(test_data).reshape((-1, img_rows, img_cols))\n\nplot_generated_img_samples(unhot(one_hot(test_label)),\n                                                        predictions,\n                                                        to_save    = False,\n                                                        iteration  = model_epochs,\n                                                        model_name = autoencoder.model_name)\n\nplot_metric('loss',     model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = autoencoder.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'],  fit_stats['valid_acc'],  model_name = autoencoder.model_name)\n"""
examples/mnist/mnist_cnn.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.mnist import fetch_mnist\nfrom ztlearn.dl.layers import BatchNormalization, Conv2D\nfrom ztlearn.dl.layers import Dropout, Dense, Flatten, MaxPooling2D\n\n\nmnist = fetch_mnist()\ntrain_data, test_data, train_label, test_label = train_test_split(mnist.data,\n                                                                  mnist.target.astype('int'),\n                                                                  test_size   = 0.33,\n                                                                  random_seed = 5,\n                                                                  cut_off     = 2000)\n\n# normalize to range [0, 1]\ntrain_data = range_normalize(train_data.astype('float32'), 0, 1)\ntest_data  = range_normalize(test_data.astype('float32'), 0, 1)\n\n# plot samples of training data\nplot_img_samples(train_data[:40], train_label[:40], dataset = 'mnist')\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.001)\n\n# model definition\nmodel = Sequential(init_method = 'he_uniform')\nmodel.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', input_shape = (1, 28, 28), padding = 'same'))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(MaxPooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary(model_name = 'mnist cnn')\n\nmodel_epochs = 12\nfit_stats = model.fit(train_data.reshape(-1, 1, 28, 28),\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data.reshape(-1, 1, 28, 28), one_hot(test_label)),\n                      shuffle_data    = True)\n\npredictions = unhot(model.predict(test_data.reshape(-1, 1, 28, 28), True))\nprint_results(predictions, test_label)\nplot_img_results(test_data[:40], test_label[:40], predictions, dataset = 'mnist')\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/mnist/mnist_dcgan.py,10,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.mnist import fetch_mnist\nfrom ztlearn.dl.layers import Activation, BatchNormalization, Conv2D\nfrom ztlearn.dl.layers import Dense, Dropout, Flatten, Reshape, UpSampling2D\n\n\nmnist               = fetch_mnist()\nmnist_data, _, _, _ = train_test_split(mnist.data, mnist.target, test_size = 0.0, cut_off = 2000)\n\n# plot samples of training data\nplot_img_samples(mnist_data[:40], None, dataset = 'mnist')\n\nimg_rows     = 28\nimg_cols     = 28\nimg_channels = 1\nimg_dims     = (img_channels, img_rows, img_cols)\n\nlatent_dim = 100\nbatch_size = 128\nhalf_batch = int(batch_size * 0.5)\n\nverbose   = True\ninit_type = 'he_uniform'\n\ngen_epoch = 50\ngen_noise = np.random.normal(0, 1, (36, latent_dim)) # for tiles 6 by 6 i.e (36) image generation\n\nmodel_epochs = 400\nmodel_stats  = {'d_train_loss': [], 'd_train_acc': [], 'g_train_loss': [], 'g_train_acc': []}\n\nd_opt = register_opt(optimizer_name = 'adam', beta1 = 0.5, lr = 0.0002)\ng_opt = register_opt(optimizer_name = 'adam', beta1 = 0.5, lr = 0.0002)\n\n\ndef stack_generator_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(128*7*7, input_shape = (latent_dim,)))\n    model.add(Activation('leaky_relu'))\n    model.add(BatchNormalization(momentum = 0.8))\n    model.add(Reshape((128, 7, 7)))\n    model.add(UpSampling2D())\n    model.add(Conv2D(64, kernel_size = (5, 5), padding = 'same'))\n    model.add(BatchNormalization(momentum = 0.8))\n    model.add(Activation('leaky_relu'))\n    model.add(UpSampling2D())\n    model.add(Conv2D(img_channels, kernel_size = (5, 5), padding = 'same'))\n    model.add(Activation('tanh'))\n\n    return model\n\n\ndef stack_discriminator_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Conv2D(64, kernel_size = (5, 5), padding = 'same', input_shape = img_dims))\n    model.add(Activation('leaky_relu'))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(128, kernel_size = (5, 5), padding = 'same'))\n    model.add(Activation('leaky_relu'))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(2))\n    model.add(Activation('sigmoid'))\n\n    return model\n\n\n# stack and compile the generator\ngenerator = stack_generator_layers(init = init_type)\ngenerator.compile(loss = 'cce', optimizer = g_opt)\n\n# stack and compile the discriminator\ndiscriminator = stack_discriminator_layers(init = init_type)\ndiscriminator.compile(loss = 'cce', optimizer = d_opt)\n\n# stack and compile the generator_discriminator\ngenerator_discriminator = Sequential(init_method = init_type)\ngenerator_discriminator.layers.extend(generator.layers)\ngenerator_discriminator.layers.extend(discriminator.layers)\ngenerator_discriminator.compile(loss = 'cce', optimizer = g_opt)\n\ngenerator.summary('mnist generator')\ndiscriminator.summary('mnist discriminator')\n\ngenerator_discriminator.summary('mnist dcgan')\nmodel_name = generator_discriminator.model_name\n\n# rescale to range [-1, 1]\nimages = range_normalize(mnist_data.reshape((-1,) + img_dims).astype(np.float32))\n\nfor epoch_idx in range(model_epochs):\n\n    # set the epoch id for print out\n    epoch_idx_p1 = epoch_idx + 1\n\n    # set the discriminator to trainable\n    discriminator.trainable = True\n\n    for epoch_k in range(1):\n\n        # set the epoch id for print out\n        epoch_k_p1 = epoch_k + 1\n\n        # draw random samples from real images\n        index = np.random.choice(images.shape[0], half_batch, replace = False)\n\n        imgs = images[index]\n\n        d_noise = np.random.normal(0, 1, (half_batch, latent_dim))\n\n        # generate a batch of new images\n        gen_imgs = generator.predict(d_noise)\n\n        # valid = [1, 0], fake = [0, 1]\n        d_valid = np.concatenate((np.ones((half_batch, 1)), np.zeros((half_batch, 1))), axis = 1)\n        d_fake  = np.concatenate((np.zeros((half_batch, 1)), np.ones((half_batch, 1))), axis = 1)\n\n        # discriminator training\n        d_loss_real, d_acc_real = discriminator.train_on_batch(imgs, d_valid, epoch_num = epoch_k_p1, batch_num = epoch_k_p1, batch_size = half_batch)\n        d_loss_fake, d_acc_fake = discriminator.train_on_batch(gen_imgs, d_fake, epoch_num = epoch_k_p1, batch_num = epoch_k_p1, batch_size = half_batch)\n\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n        d_acc  = 0.5 * np.add(d_acc_real, d_acc_fake)\n\n        if verbose:\n            print('Epoch {} K:{} Discriminator Loss: {:2.4f}, Acc: {:2.4f}.'.format(epoch_idx_p1, epoch_k+1, d_loss, d_acc))\n\n    # end of for epoch_k in range(2):\n\n    model_stats['d_train_loss'].append(d_loss)\n    model_stats['d_train_acc'].append(d_acc)\n\n    # set the discriminator to not trainable\n    discriminator.trainable = False\n\n    # discriminator training\n    g_noise = np.random.normal(0, 1, (batch_size, latent_dim))\n\n    # g_valid = [1, 0]\n    g_valid = np.concatenate((np.ones((batch_size, 1)), np.zeros((batch_size, 1))), axis = 1)\n\n    # train the generator\n    g_loss, g_acc = generator_discriminator.train_on_batch(g_noise, g_valid, epoch_num = epoch_idx_p1, batch_num = epoch_idx_p1, batch_size = batch_size)\n\n    model_stats['g_train_loss'].append(g_loss)\n    model_stats['g_train_acc'].append(g_acc)\n\n    if epoch_idx % gen_epoch == 0 and epoch_idx > 0:\n        plot_generated_img_samples(None,\n                                         generator.predict(gen_noise).reshape((-1, img_rows, img_cols)),\n                                         to_save    = True,\n                                         iteration  = epoch_idx,\n                                         model_name = model_name)\n\n    if verbose:\n        print('{}Epoch {} Discriminator Loss: {:2.4f}, Acc: {:2.4f}.'.format(print_pad(1), epoch_idx_p1, d_loss, d_acc))\n        print('Epoch {} Generator Loss: {:2.4f}, Acc: {:2.4f}.{}'.format(epoch_idx_p1, g_loss, g_acc, print_pad(1)))\n    else:\n        computebar(model_epochs, epoch_idx)\n\nplot_metric('loss', model_epochs, model_stats['d_train_loss'], model_stats['g_train_loss'], legend = ['D', 'G'], model_name = model_name)\nplot_metric('accuracy', model_epochs, model_stats['d_train_acc'],  model_stats['g_train_acc'], legend = ['D', 'G'], model_name = model_name)\n\nplot_generated_img_samples(None,\n                                 generator.predict(gen_noise).reshape((-1, img_rows, img_cols)),\n                                 to_save    = False,\n                                 iteration  = model_epochs,\n                                 model_name = model_name)\n"""
examples/mnist/mnist_gan.py,11,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.mnist import fetch_mnist\nfrom ztlearn.dl.layers import BatchNormalization, Dense, Dropout, Activation\n\n\nmnist               = fetch_mnist()\nmnist_data, _, _, _ = train_test_split(mnist.data, mnist.target, test_size = 0.0, cut_off = 2000)\n\n# plot samples of training data\nplot_img_samples(mnist_data[:40], None, dataset = 'mnist')\n\nimg_rows = 28\nimg_cols = 28\nimg_dim  = 784  # is the product (img_rows * img_cols)\n\nlatent_dim = 100\nbatch_size = 128\nhalf_batch = int(batch_size * 0.5)\n\nverbose   = True\ninit_type = 'he_uniform'\n\ngen_epoch = 500\ngen_noise = np.random.normal(0, 1, (36, latent_dim)) # 36 as batch size and is also the number of sample to be generated at the prediction stage\n\nmodel_epochs = 8000\nmodel_name   = 'mnist_gan'\nmodel_stats  = {'d_train_loss': [], 'd_train_acc': [], 'g_train_loss': [], 'g_train_acc': []}\n\nd_opt = register_opt(optimizer_name = 'adam', beta1 = 0.5, lr = 0.001)\ng_opt = register_opt(optimizer_name = 'adam', beta1 = 0.5, lr = 0.0001)\n\ndef stack_generator_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(256, input_shape = (latent_dim,)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization(momentum = 0.8))\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization(momentum = 0.8))\n    model.add(Dense(1024))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization(momentum = 0.8))\n    model.add(Dense(img_dim, activation = 'tanh'))\n\n    return model\n\ndef stack_discriminator_layers(init):\n    model = Sequential(init_method = init)\n    model.add(Dense(512, input_shape = (img_dim,)))\n    model.add(Activation('leaky_relu', alpha = 0.2))\n    model.add(Dropout(0.25))\n    model.add(Dense(256))\n    model.add(Activation('leaky_relu', alpha = 0.2))\n    model.add(Dropout(0.25))\n    model.add(Dense(2, activation = 'sigmoid'))\n\n    return model\n\n# stack and compile the generator\ngenerator = stack_generator_layers(init = init_type)\ngenerator.compile(loss = 'cce', optimizer = g_opt)\n\n# stack and compile the discriminator\ndiscriminator = stack_discriminator_layers(init = init_type)\ndiscriminator.compile(loss = 'cce', optimizer = d_opt)\n\n# stack and compile the generator_discriminator\ngenerator_discriminator = Sequential(init_method = init_type)\ngenerator_discriminator.layers.extend(generator.layers)\ngenerator_discriminator.layers.extend(discriminator.layers)\ngenerator_discriminator.compile(loss = 'cce', optimizer = g_opt)\n\ngenerator.summary('mnist generator')\ndiscriminator.summary('mnist discriminator')\n\ngenerator_discriminator.summary('mnist gan')\nmodel_name = generator_discriminator.model_name\n\n# rescale to range [-1, 1]\nimages = range_normalize(mnist_data.astype(np.float32))\n\nfor epoch_idx in range(model_epochs):\n\n    # set the epoch id for print out\n    epoch_idx_p1 = epoch_idx + 1\n\n    # set the discriminator to trainable\n    discriminator.trainable = True\n\n    for epoch_k in range(2):\n\n        # draw random samples from real images\n        index = np.random.choice(images.shape[0], half_batch, replace = False)\n        # index = np.random.randint(0, images.shape[0], half_batch)\n        imgs = images[index]\n\n        d_noise = np.random.normal(0, 1, (half_batch, latent_dim))\n\n        # generate a batch of new images\n        gen_imgs = generator.predict(d_noise)\n\n        # valid = [1, 0], fake = [0, 1]\n        d_valid = np.concatenate((np.ones((half_batch, 1)), np.zeros((half_batch, 1))), axis = 1)\n        d_fake  = np.concatenate((np.zeros((half_batch, 1)), np.ones((half_batch, 1))), axis = 1)\n\n        # discriminator training\n        d_loss_real, d_acc_real = discriminator.train_on_batch(imgs, d_valid, epoch_num = epoch_k_p1, batch_num = epoch_k_p1, batch_size = half_batch)\n        d_loss_fake, d_acc_fake = discriminator.train_on_batch(gen_imgs, d_fake, epoch_num = epoch_k_p1, batch_num = epoch_k_p1, batch_size = half_batch)\n\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n        d_acc  = 0.5 * np.add(d_acc_real, d_acc_fake)\n\n        if verbose:\n            print('Epoch {} K:{} Discriminator Loss: {:2.4f}, Acc: {:2.4f}.'.format(epoch_idx_p1, epoch_k+1, d_loss, d_acc))\n\n    # end of for epoch_k in range(1):\n\n    model_stats['d_train_loss'].append(d_loss)\n    model_stats['d_train_acc'].append(d_acc)\n\n    # set the discriminator to not trainable\n    discriminator.trainable = False\n\n    # discriminator training\n    g_noise = np.random.normal(0, 1, (batch_size, latent_dim))\n\n    # g_valid = [1, 0]\n    g_valid = np.concatenate((np.ones((batch_size, 1)), np.zeros((batch_size, 1))), axis = 1)\n\n    # train the generator\n    g_loss, g_acc = generator_discriminator.train_on_batch(g_noise, g_valid, epoch_num = epoch_idx_p1, batch_num = epoch_idx_p1, batch_size = batch_size)\n\n    model_stats['g_train_loss'].append(g_loss)\n    model_stats['g_train_acc'].append(g_acc)\n\n    if epoch_idx % gen_epoch == 0 and epoch_idx > 0:\n        plot_generated_img_samples(None,\n                                         generator.predict(gen_noise).reshape((-1, img_rows, img_cols)),\n                                         to_save    = True,\n                                         iteration  = epoch_idx,\n                                         model_name = model_name)\n\n    if verbose:\n        print('{}Epoch {} Discriminator Loss: {:2.4f}, Acc: {:2.4f}.'.format(print_pad(1), epoch_idx_p1, d_loss, d_acc))\n        print('Epoch {} Generator Loss: {:2.4f}, Acc: {:2.4f}.{}'.format(epoch_idx_p1, g_loss, g_acc, print_pad(1)))\n    else:\n        computebar(model_epochs, epoch_idx)\n\nplot_metric('loss', model_epochs, model_stats['d_train_loss'], model_stats['g_train_loss'], legend = ['D', 'G'], model_name = model_name)\nplot_metric('accuracy', model_epochs, model_stats['d_train_acc'], model_stats['g_train_acc'], legend = ['D', 'G'], model_name = model_name)\n\nplot_generated_img_samples(None,\n                                 generator.predict(gen_noise).reshape((-1, img_rows, img_cols)),\n                                 to_save    = False,\n                                 iteration  = model_epochs,\n                                 model_name = model_name)\n"""
examples/mnist/mnist_gru.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.mnist import fetch_mnist\nfrom ztlearn.dl.layers import GRU, Dense, Flatten\n\nmnist = fetch_mnist()\ntrain_data, test_data, train_label, test_label = train_test_split(mnist.data,\n                                                                  mnist.target.astype('int'),\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 15,\n                                                                  cut_off     = 2000)\n\n# plot samples of training data\nplot_img_samples(train_data, train_label, dataset = 'mnist')\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'rmsprop', momentum = 0.01, lr = 0.001)\n\n# model definition\nmodel = Sequential()\nmodel.add(GRU(128, activation = 'tanh', input_shape = (28, 28)))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary(model_name = 'mnist gru')\n\nmodel_epochs = 100\nfit_stats = model.fit(train_data.reshape(-1, 28, 28),\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data.reshape(-1, 28, 28), one_hot(test_label)),\n                      shuffle_data    = True)\n\npredictions = unhot(model.predict(test_data.reshape(-1, 28, 28), True))\nprint_results(predictions, test_label)\nplot_img_results(test_data, test_label, predictions, dataset = 'mnist')\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/mnist/mnist_lstm.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.mnist import fetch_mnist\nfrom ztlearn.dl.layers import LSTM, Dense, Flatten\n\nmnist = fetch_mnist()\ntrain_data, test_data, train_label, test_label = train_test_split(mnist.data,\n                                                                  mnist.target.astype('int'),\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 15,\n                                                                  cut_off     = 2000)\n\n# plot samples of training data\nplot_img_samples(train_data[:40], train_label[:40], dataset = 'mnist')\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'rmsprop', momentum = 0.01, lr = 0.001)\n\n# model definition\nmodel = Sequential()\nmodel.add(LSTM(128, activation = 'tanh', input_shape = (28, 28)))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('mnist lstm')\n\nmodel_epochs = 100\nfit_stats = model.fit(train_data.reshape(-1, 28, 28),\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data.reshape(-1, 28, 28), one_hot(test_label)),\n                      shuffle_data    = True)\n\npredictions = unhot(model.predict(test_data.reshape(-1, 28, 28), True))\nprint_results(predictions, test_label)\nplot_img_results(test_data[:40], test_label[:40], predictions, dataset = 'mnist')\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/mnist/mnist_mlp.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.optimizers import Adam\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.datasets.mnist import fetch_mnist\nfrom ztlearn.dl.layers import Dropout, Dense, BatchNormalization\n\nmnist = fetch_mnist()\ntrain_data, test_data, train_label, test_label = train_test_split(mnist.data,\n                                                                  mnist.target.astype('int'),\n                                                                  test_size   = 0.33,\n                                                                  random_seed = 5,\n                                                                  cut_off     = 2000)\n\n# plot samples of training data\nplot_tiled_img_samples(train_data[:40], train_label[:40], dataset = 'mnist')\n\n# model definition\nmodel = Sequential()\nmodel.add(Dense(512, activation = 'relu', input_shape = (784,)))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation = 'relu')) # 10 digits classes\nmodel.compile(loss = 'cce', optimizer = Adam())\n\nmodel.summary()\n\nmodel_epochs = 12\nfit_stats = model.fit(train_data,\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data, one_hot(test_label)),\n                      shuffle_data    = True)\n\neval_stats = model.evaluate(test_data, one_hot(test_label))\n\npredictions = unhot(model.predict(test_data, True))\nprint_results(predictions, test_label)\nplot_img_results(test_data[:40], test_label[:40], predictions, dataset = 'mnist') # truncate to 40 samples\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'],  fit_stats['valid_acc'], model_name = model.model_name)\nplot_metric('evaluation',\n                          eval_stats['valid_batches'],\n                          eval_stats['valid_loss'],\n                          eval_stats['valid_acc'],\n                          model_name = model.model_name,\n                          legend     = ['loss', 'acc'])\n"""
examples/mnist/mnist_pca.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import plot_pca\nfrom ztlearn.ml.decomposition import PCA\nfrom ztlearn.datasets.mnist import fetch_mnist\n\nmnist = fetch_mnist()\n\npca        = PCA(n_components = 2)\ncomponents = pca.fit_transform(mnist.data.astype('float64'))\n\nplot_pca(components, n_components = 2, colour_array = mnist.target.astype('int'), model_name = 'MNIST PCA')\n"""
examples/mnist/mnist_perceptron.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.mnist import fetch_mnist\nfrom ztlearn.ml.classification import Perceptron\n\nmnist = fetch_mnist()\ntrain_data, test_data, train_label, test_label = train_test_split(normalize(mnist.data.astype(\'float32\')),\n                                                                  one_hot(mnist.target.astype(\'int\')),\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 15,\n                                                                  cut_off     = None)\n\n# plot samples of training data\nplot_img_samples(train_data[:40], unhot(train_label[:40]), dataset = \'mnist\')\n\n# optimizer definition\nopt = register_opt(optimizer_name = \'sgd_momentum\', momentum = 0.01, lr = 0.001)\n\n# model definition\nmodel     = Perceptron(epochs = 2500, activation = \'softmax\', loss = \'cce\', init_method = \'he_normal\', optimizer = opt)\nfit_stats = model.fit(train_data, train_label)\n\npredictions = unhot(model.predict(test_data))\nprint_results(predictions, unhot(test_label))\nplot_img_results(test_data[:40], unhot(test_label[:40]), predictions, dataset = \'mnist\')\nplot_metric(\'accuracy_loss\',\n                              len(fit_stats[""train_loss""]),\n                              fit_stats[\'train_acc\'],\n                              fit_stats[\'train_loss\'],\n                              model_name = \'mnist_perceptron\',\n                              legend     = [\'acc\', \'loss\'])\n'"
examples/mnist/mnist_rnn.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.datasets.mnist import fetch_mnist\nfrom ztlearn.dl.layers import RNN, Dense, Flatten\n\nmnist = fetch_mnist()\ntrain_data, test_data, train_label, test_label = train_test_split(mnist.data,\n                                                                  mnist.target.astype('int'),\n                                                                  test_size   = 0.3,\n                                                                  random_seed = 15,\n                                                                  cut_off     = 2000)\n\n# plot samples of training data\nplot_img_samples(train_data[:40], train_label[:40], dataset = 'mnist')\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.001)\n\n# model definition\nmodel = Sequential()\nmodel.add(RNN(128, activation = 'tanh', bptt_truncate = 5, input_shape = (28, 28)))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation = 'softmax')) # 10 digits classes\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('mnist rnn')\n\nmodel_epochs = 100\nfit_stats = model.fit(train_data.reshape(-1, 28, 28),\n                      one_hot(train_label),\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data.reshape(-1, 28, 28), one_hot(test_label)),\n                      shuffle_data    = True)\n\npredictions = unhot(model.predict(test_data.reshape(-1, 28, 28), True))\n\nprint_results(predictions, test_label)\nplot_img_results(test_data[:40], test_label[:40], predictions, dataset = 'mnist')\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/optimizations/opt_viz.py,3,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.toolkit import GbOptimization as gbopt\n\n\n""""""2D module""""""\n\n# def f(x):\n#    return x[0]**2\n#\n# def df(x):\n#    return np.array([2*x[0]])\n#\n# opt   = register_opt(optimizer_name = \'sgd\', momentum = 0.1, lr = 0.1)\n# optim = gbopt(optimizer = opt, init_method = \'he_normal\')\n# optim.run(f, df, params = 1, epochs = 100)\n# optim.plot_2d(f)\n\n""""""3D module""""""\n\n\'\'\'\ndef f2(x):\n    return x[0]**2 + x[1]**2\n\ndef df2(x):\n    return np.array([2*x[0], 2*x[1]])\n\'\'\'\n\ndef f2(x):\n    return 2*x[0]**3 + x[1]**4\n\ndef df2(x):\n    return np.array([6*x[0]**2, 4*x[1]**3])\n\nopt   = register_opt(optimizer_name = \'sgd_momentum\', momentum = 0.01, lr = 0.001)\noptim = gbopt(optimizer = opt, init_method = \'he_normal\')\noptim.run(f2, df2, params = 2, epochs = 1500)\noptim.plot_3d(f2)\n'"
examples/optimizations/optimization.py,3,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n""""""\n\nFor the function y = theta^2,  we can plot the y  against the  thetas and we can\nsee that for the  range -5 to 6 with a step of 1, we  bypass the minima at (0,0),\nbeyond which we start rising yet again.\n\nThe np.meshgrid function helps us to merge the ys and the thetas so thatthe plot\nthat  follows is a  combination of  the function (curve) and  the evaluations at\nvarious points of y (the red dots)\n\nOur next task is then to show that  we can use methods like SGD and Momentum SGD\nto find the minimum with ease\n\n""""""\n\nf = lambda theta: theta**2\n\ntheta = np.arange(-5, 6, 1)\ny     = f(theta)\n\nthetas, ys = np.meshgrid(theta, y, sparse = True)\n\nplt.figure(figsize=(6, 5))\nplt.plot(theta, y)\n\nplt.scatter(thetas, ys, color=\'r\')\n\nplt.xlabel(r\'$\\theta$\',fontsize = 18)\nplt.ylabel(r\'$y$\',fontsize = 18)\n\nplt.show()\n'"
examples/pima_indians/pima_indians_kmeans_clustering.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import plot_kmeans\nfrom ztlearn.ml.clustering import KMeans\nfrom ztlearn.datasets.pima import fetch_pima_indians\n\n\n# fetch dataset\ndata = fetch_pima_indians()\n\n# model definition\nmodel = KMeans(n_clusters = 2, max_iter = 300)\ncentroids = model.fit(data.data[:,[5,7]])\n\n# plot clusters and centroids\nplot_kmeans(data.data[:,[5,7]], data.target, centroids, model_clusters = 2)\n'"
examples/pima_indians/pima_indians_logistic_regression.py,1,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.ml.regression import LogisticRegression\nfrom ztlearn.datasets.pima import fetch_pima_indians\n\ndata = fetch_pima_indians()\n# data.target -> using all the features (e.g to use only one feature data.target [:, 5:6])\ntrain_data, test_data, train_label, test_label = train_test_split(z_score(data.data),\n                                                                  data.target,\n                                                                  test_size   = 0.2,\n                                                                  random_seed = 0)\n\n# optimizer definition\nopt = register_opt(optimizer_name = \'adam\', momentum = 0.01, lr = 0.001)\n\n# model definition\nmodel     = LogisticRegression(epochs = 1000, optimizer = opt)\nfit_stats = model.fit(train_data, train_label)\n\n# fit_stats = model.fit_NR(train_data, train_label) # --- Newton-Raphson Method\n\nprint_results(model.predict(test_data), np.round(test_label).astype(int))\nplot_metric(\'accuracy_loss\',\n                              len(fit_stats[""train_loss""]),\n                              fit_stats[\'train_acc\'],\n                              fit_stats[\'train_loss\'],\n                              model_name = \'diabetes_logistic_regression\',\n                              legend     = [\'acc\', \'loss\'])\n'"
examples/pima_indians/pima_indians_pca.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import plot_pca\nfrom ztlearn.ml.decomposition import PCA\nfrom ztlearn.datasets.pima import fetch_pima_indians\n\n# fetch dataset\ndata = fetch_pima_indians()\n\n# model definition\npca        = PCA(n_components = 2)\ncomponents = pca.fit_transform(data.data[:,[3,5]].astype('float64'))\n\n# plot clusters\nplot_pca(components, n_components = 2, colour_array = data.target.astype('int'), model_name = 'PIMA INDIANS PCA')\n"""
examples/sequences/seq_gru.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.layers import GRU\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\n\nx, y, seq_len = gen_mult_sequence_xtyt(1000, 10, 10)\ntrain_data, test_data, train_label, test_label = train_test_split(x, y, test_size = 0.4)\n\n# plot samples of training data\nprint_seq_samples(train_data, train_label)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'rmsprop', momentum = 0.01, lr = 0.01)\n# opt = register_opt(optimizer_name = 'adadelta', momentum = 0.01, lr = 1)\n\n# model definition\nmodel = Sequential()\nmodel.add(GRU(10, activation = 'tanh', input_shape = (10, seq_len)))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('seq gru')\n\nmodel_epochs = 100\nfit_stats = model.fit(train_data,\n                      train_label,\n                      batch_size      = 100,\n                      epochs          = model_epochs,\n                      validation_data = (test_data, test_label))\n\nprint_seq_results(model.predict(test_data,(0, 2, 1)), test_label, test_data, unhot_axis = 2)\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/sequences/seq_lstm.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.layers import LSTM\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\n\n\nx, y, seq_len = gen_mult_sequence_xtyt(1000, 10, 10)\ntrain_data, test_data, train_label, test_label = train_test_split(x, y, test_size = 0.4)\n\n# plot samples of training data\nprint_seq_samples(train_data, train_label)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'adagrad', momentum = 0.01, lr = 0.01)\n\n# model definition\nmodel = Sequential()\nmodel.add(LSTM(10, activation = 'tanh', input_shape = (10, seq_len)))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('seq lstm')\n\nmodel_epochs = 100\nfit_stats = model.fit(train_data,\n                      train_label,\n                      batch_size      = 100,\n                      epochs          = model_epochs,\n                      validation_data = (test_data, test_label))\n\nprint_seq_results(model.predict(test_data,(0, 2, 1)), test_label, test_data, unhot_axis = 2)\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/sequences/seq_rnn.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.dl.layers import RNN, Flatten, Dense\n\n\nx, y, seq_len = gen_mult_sequence_xtym(3000, 10, 10)\ntrain_data, test_data, train_label, test_label = train_test_split(x, y, test_size = 0.3)\n\n# plot samples of training data\nprint_seq_samples(train_data, train_label, 0)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'adam', momentum = 0.01, lr = 0.01)\n\n# model definition\nmodel = Sequential()\nmodel.add(RNN(5, activation = 'tanh', bptt_truncate = 5, input_shape = (9, seq_len)))\nmodel.add(Flatten())\nmodel.add(Dense(seq_len, activation = 'softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('seq rnn')\n\nmodel_epochs = 15\nfit_stats = model.fit(train_data,\n                      train_label,\n                      batch_size      = 100,\n                      epochs          = model_epochs,\n                      validation_data = (test_data, test_label))\n\nprint_seq_results(model.predict(test_data), test_label, test_data)\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
ztlearn/datasets/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import module(s)\nfrom . import pima\nfrom . import iris\nfrom . import mnist\nfrom . import cifar\nfrom . import digits\nfrom . import boston\nfrom . import fashion\nfrom . import data_set\n\n# import from data_utils.py\nfrom .data_set import DataSet\n'
ztlearn/datasets/data_set.py,0,"b'# -*- coding: utf-8 -*-\n\nclass DataSet:\n\n    def __init__(self, data, target, describe = None):\n        self.data     = data\n        self.target   = target\n        self.describe = describe\n'"
ztlearn/dl/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import packages(s)\nfrom . import layers\nfrom . import models\n'
ztlearn/ml/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import packages(s)\nfrom . import regression\nfrom . import clustering\nfrom . import decomposition\nfrom . import classification\n'
ztlearn/toolkit/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import file(s)\nfrom . import optviz\n\n# gradient based optimization\nfrom .optviz import GbOptimization\n'
ztlearn/toolkit/optviz.py,10,"b'import numpy as np\n\nfrom ztlearn.utils import plot_opt_viz\nfrom ztlearn.initializers import InitializeWeights as init\nfrom ztlearn.optimizers import OptimizationFunction as optimize\n\nclass GbOptimization(object):\n\n    def __init__(self, optimizer, init_method = \'ones\'):\n        self.optimizer   = optimizer\n        self.init_method = init_method\n\n    def run(self, f, df, params = 1, epochs = 10, tol = 1e-4, scale_factor = 5, verbose = False):\n        self.inputs = init(self.init_method).initialize_weights((params, 1)) * scale_factor\n        self.f0     = f(self.inputs) # initial function value (fsolve)\n        self.epochs = epochs\n\n        self.fsolve  = np.zeros((self.epochs, 1))\n        self.weights = np.zeros((self.epochs, 1, params))\n\n        for i in np.arange(self.epochs):\n            self.inputs         = optimize(self.optimizer).update(self.inputs, df(self.inputs))\n            self.weights[i,:,:] = self.inputs.T\n\n            f_solution       = f(self.inputs)\n            self.fsolve[i,:] = f_solution\n\n            if verbose:\n                if i%5 == 0:\n                    print(\'Epoch-{} weights: {:.20}\'.format(i+1, self.npstring(self.inputs.T)))\n                    print(\'Epoch-{} eps: {:.20}\'.format(i+1, self.npstring(self.f0 - f_solution)))\n                # if np.linalg.norm(self.inputs, axis = 0) > tol: break\n\n    def npstring(self, np_array):\n        return np.array2string(np_array, formatter = {\'float_kind\':\'{0:.4f}\'.format})\n\n    def plot_3d(self, f):\n        """""" plot a 3d visualization """"""\n        theta = np.arange(-4.0, 4.0, 0.1)\n\n        x_grid = np.meshgrid(theta, theta)\n        z = f(x_grid)\n\n        weights = self.weights.reshape(self.epochs, -1)\n\n        vis_type = [\'wireframe\', \'contour\']\n        for vis in vis_type:\n            plot_opt_viz(3, x_grid, weights, z, self.fsolve, overlay = vis)\n\n    def plot_2d(self, f):\n        """""" plot a 2d visualization """"""\n        theta = np.expand_dims(np.arange(-5.0, 6.0, 1.0), axis = 1)\n\n        y = np.zeros_like(theta)\n        for i in np.arange(theta.shape[0]):\n            y[i,:] = f(theta[i,:])\n\n        weights = self.weights.reshape(self.epochs, -1)\n\n        plot_opt_viz(2, theta, y, weights, self.fsolve, overlay = \'plot\')\n        \n'"
ztlearn/utils/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\n# import file(s)\nfrom . import data_utils\nfrom . import text_utils\nfrom . import plot_utils\nfrom . import conv_utils\nfrom . import im2col_utils\nfrom . import sequence_utils\nfrom . import time_deco_utils\n\n# import from data_utils.py\nfrom .data_utils import unhot\nfrom .data_utils import one_hot\nfrom .data_utils import min_max\nfrom .data_utils import z_score\nfrom .data_utils import print_pad\nfrom .data_utils import normalize\nfrom .data_utils import computebar\nfrom .data_utils import minibatches\nfrom .data_utils import kfold_split\nfrom .data_utils import custom_tuple\nfrom .data_utils import shuffle_data\nfrom .data_utils import extract_files\nfrom .data_utils import print_results\nfrom .data_utils import clip_gradients\nfrom .data_utils import maybe_download\nfrom .data_utils import eucledian_norm\nfrom .data_utils import accuracy_score\nfrom .data_utils import range_normalize\nfrom .data_utils import train_test_split\nfrom .data_utils import print_seq_samples\nfrom .data_utils import print_seq_results\nfrom .data_utils import imbalanced_dataset\nfrom .data_utils import polynomial_features\n\n# import from text_utils.py\nfrom .text_utils import pad_sequence\nfrom .text_utils import get_sentence_tokens\nfrom .text_utils import gen_char_sequence_xtym\nfrom .text_utils import gen_char_sequence_xtyt\n\n# import from plot_utils.py\nfrom .plot_utils import plot_pca\nfrom .plot_utils import plot_kmeans\nfrom .plot_utils import plot_metric\nfrom .plot_utils import plot_opt_viz\nfrom .plot_utils import plot_img_samples\nfrom .plot_utils import plot_img_results\nfrom .plot_utils import plot_tiled_img_samples\nfrom .plot_utils import plot_regression_results\nfrom .plot_utils import plot_generated_img_samples\n\n# import from time_deco_utils.py\nfrom .time_deco_utils import LogIfBusy\n\n# import from sequence_utils.py\nfrom .sequence_utils import gen_mult_sequence_xtyt\nfrom .sequence_utils import gen_mult_sequence_xtym\n\n# import from im2col_utils.py\nfrom .im2col_utils import get_pad\nfrom .im2col_utils import im2col_indices\nfrom .im2col_utils import col2im_indices\n\n# import from conv_utils.py\nfrom .conv_utils import unroll_inputs\nfrom .conv_utils import get_output_dims\n\n__all__ = [\n\n            # From sequence_utils.py\n            'gen_mult_sequence_xtyt','gen_mult_sequence_xtym',\n\n            # From text_utils.py -- import nothing\n\n            # From plot_utils.py\n            'plot_metric','plot_kmeans','plot_pca','plot_regression_results',\n            'plot_img_samples','plot_img_results','plot_generated_img_samples',\n            'plot_tiled_img_samples',\n\n            # From data_utils.py\n            'unhot','one_hot','min_max','z_score','normalize','print_pad','custom_tuple',\n            'minibatches','shuffle_data','computebar','clip_gradients','range_normalize',\n            'accuracy_score','train_test_split','print_seq_samples','print_seq_results',\n            'print_results'\n\n          ]\n"""
ztlearn/utils/conv_utils.py,7,"b'# -*- coding: utf-8 -*-\n\nimport math as mt\nimport numpy as np\n\ndef alt_get_output_dims(input_height, input_width, kernel_size, strides, pad_height, pad_width):\n\n    """"""\n    FORMULA: [((W - Kernel_W + 2P) / S_W) + 1] and [((H - Kernel_H + 2P) / S_H) + 1]\n    FORMULA: [((W - Pool_W + 2P)   / S_W) + 1] and [((H - Pool_H + 2P)   / S_H) + 1]\n    """"""\n\n    output_height = ((input_height - kernel_size[0] + np.sum(pad_height)) / strides[0]) + 1\n    output_width  = ((input_width  - kernel_size[1] + np.sum(pad_width))  / strides[1]) + 1\n\n    return output_height, output_width\n\ndef get_output_dims(input_height, input_width, kernel_size, strides, padding_type = \'valid\'):\n\n    """"""\n    **SAME and VALID Padding**\n\n    VALID: No padding is applied.  Assume that all  dimensions are valid so that input image\n           gets fully covered by filter and stride you specified.\n\n    SAME:  Padding is applied to input (if needed) so that input image gets fully covered by\n           filter and stride you specified. For stride 1, this will ensure that output image\n           size is same as input.\n\n    References:\n        [1] SAME and VALID Padding: http://bit.ly/2MtGgBM\n    """"""\n\n    if padding_type == \'same\':\n        output_height = mt.ceil(float(input_height) / float(strides[0]))\n        output_width  = mt.ceil(float(input_width)  / float(strides[1]))\n\n    if padding_type == \'valid\':\n        output_height = mt.ceil(float(input_height - kernel_size[0] + 1) / float(strides[0]))\n        output_width  = mt.ceil(float(input_width  - kernel_size[1] + 1) / float(strides[1]))\n\n    return output_height, output_width\n\n# unroll for toeplitz\ndef unroll_inputs(padded_inputs,\n                                 batch_num,\n                                 filter_num,\n                                 output_height,\n                                 output_width,\n                                 kernel_size):\n\n    unrolled_inputs = np.zeros((batch_num,\n                                           filter_num,\n                                           output_height * output_width,\n                                           kernel_size**2))\n\n    offset = 0\n    for h in np.arange(output_height): # output height\n        for w in np.arange(output_width): # output width\n            for b in np.arange(batch_num): # batch number\n                for f in np.arange(filter_num): # filter number\n                     unrolled_inputs[b, f, offset, :] = padded_inputs[b,\n                                                                      f,\n                                                                      h:h+kernel_size,\n                                                                      w:w+kernel_size].flatten()\n            offset += 1\n\n    return unrolled_inputs.reshape(filter_num * kernel_size**2, -1)\n'"
ztlearn/utils/data_utils.py,22,"b'# -*- coding: utf-8 -*-\n\nimport os\nimport sys\nimport gzip\nimport urllib\nimport tarfile\nimport zipfile\nimport numpy as np\n\nfrom itertools import chain\nfrom itertools import combinations\nfrom itertools import combinations_with_replacement\n\n#-----------------------------------------------------------------------------#\n#                       DATA UTILITY FUNCTIONS                                #\n#-----------------------------------------------------------------------------#\n\ndef eucledian_norm(vec_a, vec_b):\n    """""" compute the eucledian distance between two vectors """"""\n    distance = vec_a - vec_b\n    # return np.sqrt(np.sum(np.square(distance)))\n    # return np.sqrt(np.einsum(\'ij, ij->i\', distance, distance))\n    return np.linalg.norm(distance, ord = \'fro\', axis = 1)\n\ndef clip_gradients(grad, g_min = -1., g_max = 1.):\n    """""" enforce min and max bounderies on a given gradient """"""\n    return np.clip(grad, g_min, g_max, out = grad)\n\ndef accuracy_score(predictions, targets):\n    """""" compute an average accuracy score of prediction vs targets """"""\n    return np.mean(predictions == targets)\n\ndef one_hot(labels, num_classes = None):\n    """""" generate one hot encoding for a given set labels """"""\n    num_classes    = np.max(labels.astype(\'int\')) + 1 if not num_classes else num_classes\n    one_hot_labels = np.zeros([labels.size, num_classes])\n\n    one_hot_labels[np.arange(labels.size), labels.astype(\'int\')] = 1.\n\n    return one_hot_labels\n\ndef unhot(one_hot, unhot_axis = 1):\n    """""" reverse one hot encoded data back to labels """"""\n    return np.argmax(one_hot, axis = unhot_axis)\n\ndef imbalanced_dataset(input_data, input_label, digit, remainder):   \n    """""" generate unbalanced dataset """"""\n    input_label_ids = np.squeeze(np.array(np.where(input_label == digit)), axis = 0)\n    remainder_ids   = input_label_ids[remainder:]       \n    return np.delete(input_data, remainder_ids, 0), np.delete(input_label, remainder_ids, 0)\n\ndef shuffle_data(input_data, input_label, random_seed = None):\n    """""" perfom randomized shuffle on a given input dataset """"""\n    assert input_data.shape[0] == input_label.shape[0], \'input data and label sizes do not match!\'\n\n    if random_seed is not None: # confirm that random seed has been set\n        np.random.seed(random_seed)\n\n    indices = np.arange(input_data.shape[0])\n    np.random.shuffle(indices)\n\n    return input_data[indices], input_label[indices]\n\ndef train_test_split(samples, labels, test_size = 0.2, shuffle = True, random_seed = None, cut_off = None):\n    """""" generate a train vs test split given a test size """"""\n    if shuffle:\n        samples, labels = shuffle_data(samples, labels, random_seed)\n\n    split_ratio = int((1.0 - test_size) * len(samples))\n\n    samples_train, samples_test = samples[:split_ratio], samples[split_ratio:]\n    labels_train, labels_test   = labels[:split_ratio], labels[split_ratio:]\n\n    if cut_off is not None and isinstance(cut_off, (int, np.integer)):\n        return samples_train[:cut_off], samples_test[:cut_off], labels_train[:cut_off], labels_test[:cut_off]\n\n    return samples_train, samples_test, labels_train, labels_test\n\ndef kfold_split(samples, labels, n_splits = 5, shuffle = False, random_seed = None):\n    """""" generate K folds for cross validation testing """"""\n    if shuffle:\n        samples, labels = shuffle_data(samples, labels, random_seed)\n\n    def get_folds(input_data, num_rows):\n        fold_size = int(num_rows / n_splits)\n        for idx in range(0, num_rows, fold_size):\n            yield input_data[idx:idx + fold_size]\n\n    sample_folds = list(get_folds(samples, samples.shape[0]))\n    label_folds  = list(get_folds(labels, samples.shape[0]))\n\n    # @@TODO: split the sample_folds and label_folds into sizes K-1 and 1 for test and training sets\n    # this returns two lists which can be accessed by the item index e.g sample_folds[0] for fold 1\n    return sample_folds, label_folds\n\n\ndef minibatches(input_data, input_label, batch_size, shuffle):\n    """""" generate minibatches on a given input data matrix """"""\n    assert input_data.shape[0] == input_label.shape[0], \'input data and label sizes do not match!\'\n    minibatches = []\n    indices     = np.arange(input_data.shape[0])\n\n    if shuffle:\n        np.random.shuffle(indices)\n\n    for idx in range(0, input_data.shape[0], batch_size):\n        mini_batch = indices[idx:idx + batch_size]\n        minibatches.append((input_data[mini_batch], input_label[mini_batch]))\n\n    return minibatches\n\ndef normalize(input_data, axis = -1, order = 2):\n    """""" compute normalization (order) for a given input matrix, order and axis """"""\n    l2 = np.linalg.norm(input_data, order, axis, keepdims = True)\n    l2[l2 == 0] = 1\n\n    return input_data / l2\n\ndef range_normalize(input_data, a = -1, b = 1, axis = None):\n    """""" compute the range normalization for a given input matrix, range [a,b] and axis """"""\n    return (((b - a) * ((input_data - input_data.min(axis = axis, keepdims = True)) / np.ptp(input_data, axis = axis))) + a)\n\ndef min_max(input_data, axis = None):\n    """""" compute the min max standardization for a given input matrix and axis """"""\n    return (input_data - input_data.min(axis = axis, keepdims = True)) / np.ptp(input_data, axis = axis)\n\ndef z_score(input_data, axis = None):\n    """""" compute the z score for a given input matrix and axis """"""\n    input_mean = input_data.mean(axis = axis, keepdims = True)\n    input_std  = input_data.std(axis = axis, keepdims = True)\n\n    return (input_data - input_mean) / input_std\n\ndef print_results(predictions, test_labels, num_samples = 20):\n    """""" print model targeted vs predicted results """"""\n    print(\'Targeted  : {}\'.format(test_labels[:num_samples]))\n    print(\'Predicted : {}{}\'.format(predictions[:num_samples], print_pad(1)))\n    print(\'Model Accuracy : {:2.2f}% {}\'.format(accuracy_score(predictions, test_labels)*100, print_pad(1)))\n\ndef print_seq_samples(train_data, train_label, unhot_axis = 1, sample_num = 0):\n    """""" print generated sequence samples """"""\n    print(print_pad(1) + \'Sample Sequence : {}\'.format(unhot(train_data[sample_num])))\n    print(\'Next Entry      : {} {}\'.format(unhot(train_label[sample_num], unhot_axis), print_pad(1)))\n\ndef print_seq_results(predicted, test_label, test_data, unhot_axis = 1, interval = 5):\n    """""" print results for a model predicting a sequence """"""\n    predictions = unhot(predicted, unhot_axis)\n    targets     = unhot(test_label, unhot_axis)\n\n    for i in range(interval):\n        print(\'Sequence  : {}\'.format(unhot(test_data[i])))\n        print(\'Targeted  : {}\'.format(targets[i]))\n        print(\'Predicted : {} {}\'.format(predictions[i], print_pad(1)))\n\n    print(\'Model Accuracy : {:2.2f}%\'.format(accuracy_score(predictions, targets)*100))\n\ndef computebar(total, curr, size = 45, sign = ""#"", prefix = ""Computing""):\n    """""" generate a graphical loading bar [####---] for a given iteration """"""\n    progress = float((curr + 1) / total)\n    update   = int(round(size * progress))\n\n    bar = ""\\r{}: [{}] {:d}% {}"".format(prefix,\n                                       sign * update + ""-"" * (size - update),\n                                       int(round(progress * 100)),\n                                       """" if progress < 1. else print_pad(1, ""\\r\\n""))\n\n    sys.stdout.write(bar)\n    sys.stdout.flush()\n\ndef print_pad(pad_count, pad_char = ""\\n""):\n    """""" pad strings with a total of n = pad_count, pad_char type characters """"""\n    padding = """"\n    for i in range(pad_count):\n        padding += pad_char\n    return padding\n\ndef custom_tuple(tup):\n    """""" customize tuple to have comma separated numbers """"""\n    tuple_string = ""(""\n    for itup in tup:\n        tuple_string += ""{:,d}"".format(itup) + "", ""\n\n    if len(tup) == 1:\n        return tuple_string[:-2] + "",)""\n    return tuple_string[:-2] + "")""\n\ndef maybe_download(path, url, print_log = False):\n    """""" download the data from url, or return existing """"""\n    path = os.path.expanduser(path)\n    if not os.path.exists(path):\n        os.makedirs(path)\n    filepath = os.path.join(path, url.rpartition(\'/\')[2])\n    if os.path.exists(filepath):\n        if print_log:\n            print(\'{} already exists\'.format(filepath))\n    else:\n        print(print_pad(1) +\'Required Dataset Not Found! Will Proceed To Download.\')\n        print(print_pad(1) +\'Downloading. Please Wait ...\')\n        filepath, _ = urllib.request.urlretrieve(url, filepath)\n        print(print_pad(1) +\'Succesful Download. See : {}\'.format(filepath))\n\n    return filepath\n\ndef extract_files(path, filepath):\n    """""" extract files from a detected compressed format """"""\n    opener, mode = None, None\n    if filepath.endswith(\'.tar.bz2\') or filepath.endswith(\'.tbz\'):\n        opener, mode = tarfile.open, \'r:bz2\'\n    elif zipfile.is_zipfile(filepath):\n        opener, mode = zipfile.ZipFile, \'r\'\n    elif filepath.endswith(\'.tar.gz\') :\n        opener, mode = tarfile.open, \'r:gz\'\n    elif filepath.endswith(\'.gz\'): # @@TODO: write a more robust method for this\n        opener, mode = gzip.open, \'rb\'\n\n    if opener is not None:\n        if opener is gzip.open: # OR if mode == \'rb\':\n            with opener(filepath, mode) as f:\n                contents = f.read()\n            with open(os.path.splitext(filepath)[0], \'wb\') as f:\n                f.write(contents)\n        else:\n            with opener(filepath, mode) as f:\n                f.extractall(path)\n    else:\n        raise NotImplementedError(\'Extraction Method For This Filetype Not Implemented\')\n\ndef polynomial_features(inputs, degree = 2, repeated_elems = False, with_bias = True):\n    """""" generate feature matrix of all polynomial combinations for degrees upto <= degree """"""\n    num_samples, num_features = np.shape(inputs)\n\n    def feature_combinations():\n        combination_type = (combinations if repeated_elems else combinations_with_replacement)\n        start = 0 if repeated_elems == False else 1\n        combs = [list(combination_type(range(num_features), i)) for i in range(start, degree + 1)]\n        flatten_combs =  list(chain(*combs))\n        return flatten_combs, len(flatten_combs)\n\n    flat_combs, num_output_features = feature_combinations()\n    inputs_poly = np.empty((num_samples, num_output_features), dtype = inputs.dtype)\n    for i, flat_comb in enumerate(flat_combs):\n        inputs_poly[:, i] = inputs[:, flat_comb].prod(1)\n\n    return inputs_poly\n'"
ztlearn/utils/im2col_utils.py,16,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\ndef get_pad(padding, input_height, input_width, stride_height, stride_width, kernel_height, kernel_width):\n    if padding == \'valid\':\n        return (0, 0), (0, 0)\n\n    elif padding == \'same\':\n\n        if (input_height % stride_height == 0):\n          pad_along_height = max(kernel_height - stride_height, 0)\n        else:\n          pad_along_height = max(kernel_height - (input_height % stride_height), 0)\n\n        if (input_width % stride_width == 0):\n          pad_along_width = max(kernel_width - stride_width, 0)\n        else:\n          pad_along_width = max(kernel_width - (input_width % stride_width), 0)\n\n        pad_top    = pad_along_height // 2\n        pad_bottom = pad_along_height - pad_top\n        pad_left   = pad_along_width // 2\n        pad_right  = pad_along_width - pad_left\n\n        return (pad_top, pad_bottom), (pad_left, pad_right)\n\ndef get_im2col_indices(x_shape, field_height = 3, field_width = 3, padding = ((0, 0), (0, 0)), stride = 1):\n\n    """"""\n    func: get_im2col_indices adapted from CS231n Stanford: http://cs231n.github.io/assignments2017/assignment2/\n    copyright held by Stanford, 2017 as part of CS231n Convolutional Neural Networks for Visual Recognition\n    """"""\n\n    # first figure out what the size of the output should be\n    N, C, H, W            = x_shape\n    pad_height, pad_width = padding\n\n    assert (H + np.sum(pad_height) - field_height) % stride == 0\n    assert (W + np.sum(pad_width)  - field_height) % stride == 0\n\n    out_height = (H + np.sum(pad_height) - field_height) / stride + 1\n    out_width  = (W + np.sum(pad_width)  - field_width)  / stride + 1\n\n    i0 = np.repeat(np.arange(field_height, dtype = \'int32\'), field_width)\n    i0 = np.tile(i0, C)\n    i1 = stride * np.repeat(np.arange(out_height, dtype = \'int32\'), out_width)\n    j0 = np.tile(np.arange(field_width), field_height * C)\n    j1 = stride * np.tile(np.arange(out_width, dtype = \'int32\'), int(out_height))\n    i  = i0.reshape(-1, 1) + i1.reshape(1, -1)\n    j  = j0.reshape(-1, 1) + j1.reshape(1, -1)\n\n    k = np.repeat(np.arange(C, dtype=\'int32\'), field_height * field_width).reshape(-1, 1)\n\n    return (k, i, j)\n\ndef im2col_indices(x, field_height, field_width, padding, stride = 1):\n\n    """"""\n    func: im2col_indices adapted from CS231n Stanford: http://cs231n.github.io/assignments2017/assignment2/\n    copyright held by Stanford, 2017 as part of CS231n Convolutional Neural Networks for Visual Recognition\n    """"""\n\n    """""" An implementation of im2col based on some fancy indexing """"""\n    pad_height, pad_width = padding\n\n    x_padded = np.pad(x, ((0, 0), (0, 0), pad_height, pad_width), mode = \'constant\')\n    k, i, j  = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n    cols     = x_padded[:, k, i, j]\n    C        = x.shape[1]\n    cols     = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n\n    return cols\n\ndef col2im_indices(cols, x_shape, field_height = 3, field_width = 3, padding = ((0, 0), (0, 0)), stride = 1):\n\n    """"""\n    func: col2im_indices adapted from CS231n Stanford: http://cs231n.github.io/assignments2017/assignment2/\n    copyright held by Stanford, 2017 as part of CS231n Convolutional Neural Networks for Visual Recognition\n    """"""\n\n    """""" An implementation of col2im based on fancy indexing and np.add.at """"""\n    N, C, H, W            = x_shape\n    pad_height, pad_width = padding\n    H_padded, W_padded    = H + np.sum(pad_height), W + np.sum(pad_width)\n\n    x_padded = np.zeros((N, C, H_padded, W_padded), dtype = cols.dtype)\n    k, i, j  = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n\n    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n\n    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n\n    pad_size = (np.sum(pad_height)/2).astype(int)\n    if pad_size == 0:\n        return x_padded\n    return x_padded[:, :, pad_size:-pad_size, pad_size:-pad_size]\n\npass\n'"
ztlearn/utils/plot_utils.py,0,"b'# -*- coding: utf-8 -*-\n\nimport time\nimport matplotlib.pyplot as plt\n\nSMALL_FONT = 10\nLARGE_FONT = 14\nFIG_SIZE   = (8, 6)\n\nimg_specs = {\n    \'mnist\' :  {\n        \'pix_row\'    : 1,\n        \'pix_col\'    : 26,\n        \'img_width\'  : 28,\n        \'img_height\' : 28\n    },\n    \'cifar\' :  {\n        \'pix_row\'    : 1,\n        \'pix_col\'    : 30,\n        \'img_width\'  : 32,\n        \'img_height\' : 32\n    },\n    \'digits\':  {\n        \'pix_row\'    : 0,\n        \'pix_col\'    : 7,\n        \'img_width\'  : 8,\n        \'img_height\' : 8\n    }\n}\n\n\ndef plotter(x,\n               y           = [],\n               plot_dict   = {},\n               fig_dims    = (7, 5),\n               title       = \'Model\',\n               title_dict  = {},\n               ylabel      = \'y-axis\',\n               ylabel_dict = {},\n               xlabel      = \'x-axis\',\n               xlabel_dict = {},\n               legend      = [], # [\'train\', \'valid\'],\n               legend_dict = {},\n               file_path   = \'\',\n               to_save     = False,\n               plot_type   = \'line\',\n               cmap_name   = None,\n               cmap_number = 10,\n               grid_on     = True):\n\n    fig, ax = plt.subplots()\n    fig.set_size_inches(fig_dims)\n\n    ax.set_axisbelow(True)\n    ax.minorticks_on()\n\n    if grid_on:\n        ax.grid(which = \'major\', linestyle = \'-\', linewidth = 0.5, color = \'grey\')\n        ax.grid(which = \'minor\', linestyle = \':\', linewidth = 0.5, color = \'red\')\n\n    if plot_type == \'line\':\n        for i in range(len(y)):\n            ax.plot(x, y[i], **plot_dict)\n\n    if plot_type == \'scatter\':\n        if cmap_name is not None:\n            plot_dict.update(cmap = plt.cm.get_cmap(cmap_name, cmap_number))\n            plot = ax.scatter(x[:, 0], x[:, 1], **plot_dict)\n            fig.colorbar(plot, ax = ax)\n        else:\n            ax.scatter(x[:, 0], x[:, 1], **plot_dict)\n        if y is not None:\n            ax.scatter(y[:, 0], y[:, 1], **{\'c\' : \'red\'}) # centroids for k-means\n\n    ax.set_title(title, **title_dict)\n\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n\n    ax.legend(legend, **legend_dict)\n\n    if to_save:\n        fig.savefig(file_path)\n\n    return plt\n\n\ndef plot_pca(components,\n                         n_components = 2,\n                         colour_array = None,\n                         model_name   = \'PCA\',\n                         to_save      = False,\n                         fig_dims     = FIG_SIZE, #(10, 8),\n                         title_dict   = {\'size\' : SMALL_FONT}):\n\n    file_path = \'../plots/decompositions/\'+(\'{}{}{}{}{}\'.format(model_name,\n                                                                \'_\',\n                                                                n_components,\n                                                                \'_Components_\',\n                                                                time.strftime(""%Y-%m-%d_%H-%M-%S""),\'.png\'))\n\n    plt_dict = {\n        \'c\'         : colour_array,\n        \'edgecolor\' : \'none\',\n        \'alpha\'     : 0.5,\n        \'s\'         : 50\n    }\n\n    plt = plotter(components,\n                              y           = None,\n                              plot_dict   = plt_dict,\n                              fig_dims    = fig_dims,\n                              title       = \'Model {}\'.format(model_name.upper()),\n                              title_dict  = title_dict,\n                              xlabel      = \'PC 1\',\n                              ylabel      = \'PC 2\',\n                              file_path   = file_path,\n                              to_save     = to_save,\n                              plot_type   = \'scatter\',\n                              cmap_name   = \'tab10\',\n                              cmap_number = 10,\n                              grid_on     = False)\n\n    plt.show()\n\n\ndef plot_kmeans(data,\n                      labels         = None,\n                      centroids      = None,\n                      model_name     = \'K-Means\',\n                      model_clusters = 1,\n                      to_save        = False,\n                      fig_dims       = FIG_SIZE,\n                      title_dict     = {\'size\' : SMALL_FONT}):\n\n    file_path = \'../plots/clusters/\'+(\'{}{}{}{}{}\'.format(model_name,\n                                                          \'_\',\n                                                          model_clusters,\n                                                          \'_Clusters_\',\n                                                          time.strftime(""%Y-%m-%d_%H-%M-%S""),\'.png\'))\n\n    plt = plotter(data,\n                        y          = centroids,\n                        plot_dict  = {\'c\' : labels},\n                        fig_dims   = fig_dims,\n                        title      = \'Model {}\'.format(model_name.title()),\n                        title_dict = title_dict,\n                        file_path  = file_path,\n                        to_save    = to_save,\n                        plot_type  = \'scatter\')\n\n    plt.show()\n\n\ndef plot_metric(metric,\n                        epoch,\n                        train,\n                        valid,\n                        model_name  = \'\',\n                        to_save     = False,\n                        plot_dict   = {\'linewidth\' : 0.8},\n                        fig_dims    = FIG_SIZE,\n                        title_dict  = {\'size\' : SMALL_FONT},\n                        ylabel_dict = {\'size\' : SMALL_FONT},\n                        xlabel_dict = {\'size\' : SMALL_FONT},\n                        legend      = [\'train\', \'valid\'],\n                        legend_dict = {\'loc\' : \'upper right\'}):\n\n    file_path = \'../plots/metrics/\'+(\'{}{}{}{}{}\'.format(model_name,\n                                                         \'_\',\n                                                         metric,\n                                                         \'_\',\n                                                         time.strftime(""%Y-%m-%d_%H-%M-%S""),\'.png\'))\n\n    plt = plotter(range(epoch),\n                                [train, valid],\n                                plot_dict   = plot_dict,\n                                fig_dims    = fig_dims,\n                                title       = \'Model {}\'.format(metric.title()),\n                                title_dict  = title_dict,\n                                ylabel      = metric.title(),\n                                ylabel_dict = ylabel_dict,\n                                xlabel      = \'Iterations\',\n                                xlabel_dict = xlabel_dict,\n                                legend      = legend,\n                                legend_dict = legend_dict,\n                                file_path   = file_path,\n                                to_save     = to_save)\n\n    plt.show()\n\n\ndef plot_opt_viz(dims,\n                       x,\n                       y,\n                       z,\n                       f_solution,\n                       overlay     = \'plot\',\n                       to_save     = False,\n                       title       = \'Optimization\',\n                       title_dict  = {\'size\' : LARGE_FONT},\n                       fig_dims    = FIG_SIZE,\n                       xticks_dict = {\'size\' : LARGE_FONT},\n                       yticks_dict = {\'size\' : LARGE_FONT},\n                       xlabel      = r\'$\\theta^1$\',\n                       xlabel_dict = {\'size\' : LARGE_FONT},\n                       ylabel      = r\'$\\theta^2$\',\n                       ylabel_dict = {\'size\' : LARGE_FONT},\n                       legend      = [\'train\', \'valid\'],\n                       legend_dict = {}):\n\n    if dims == 3:\n        fig = plt.figure(figsize = fig_dims)\n\n        if overlay == \'wireframe\':\n            from mpl_toolkits.mplot3d import axes3d # for 3d projections\n            ax = fig.add_subplot(111, projection = \'3d\')\n            plt.scatter(y[:,0], y[:,1], s = f_solution, c = \'r\')\n            ax.plot_wireframe(x[0], x[1], z, rstride = 5, cstride = 5, linewidth = 0.5)\n\n        elif overlay == \'contour\':\n            ax = fig.add_subplot(111)\n            plt.scatter(y[:,0], y[:,1], s = f_solution, c = \'r\')\n            ax.contour(x[0], x[1], z, 20, cmap = plt.cm.jet)\n\n        ax.set_xlabel(xlabel, **xlabel_dict)\n        ax.set_ylabel(ylabel, **ylabel_dict)\n\n    elif dims == 2:\n        plt.figure(figsize = fig_dims)\n\n        plt.xticks(**xticks_dict)\n        plt.yticks(**yticks_dict)\n\n        plt.plot(x, y)\n        plt.scatter(z, f_solution, color = \'r\')\n\n        plt.xlabel(xlabel, **xlabel_dict)\n        plt.ylabel(ylabel, **ylabel_dict)\n\n    if to_save:\n        plt.suptitle((\'{}{}\'.format(dims, \'D Surfaces\')), fontsize = 14)\n        plt.savefig(\'../plots/\'+(\'{}{}{}{}\'.format(overlay, \'_\', dims, \'d.png\')))\n\n    plt.show()\n\n\ndef plot_img_samples(train_data, train_target = None, fig_dims = (6, 6), dataset = \'digits\', channels = 1):\n    fig = plt.figure(figsize = fig_dims)\n    fig.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1, hspace = 0.05, wspace = 0.05)\n\n\n    for i in range(36):\n        digit = fig.add_subplot(6, 6, i+1, xticks = [], yticks = [])\n\n        if channels == 3:\n            color_img = train_data[i].reshape(channels,\n                                              img_specs[dataset][\'img_height\'],\n                                              img_specs[dataset][\'img_width\']).transpose([1, 2, 0])\n            digit.imshow(color_img, interpolation = \'nearest\')\n        else:\n            digit.imshow(train_data[i].reshape(img_specs[dataset][\'img_height\'],\n                                               img_specs[dataset][\'img_width\']),\n                                               cmap = plt.cm.binary, interpolation = \'nearest\')\n\n        if train_target is not None:\n            digit.text(img_specs[dataset][\'pix_row\'],\n                       img_specs[dataset][\'pix_col\'],\n                       str(train_target.astype(\'int\')[i]))\n\n    plt.show()\n\n\ndef plot_tiled_img_samples(train_data, train_target = None, fig_dims = (6, 6), dataset = \'digits\', channels = 1):\n    fig = plt.figure(figsize = fig_dims)\n    fig.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1, hspace = 0.05, wspace = 0.05)\n\n    for i in range(36):\n        digit = fig.add_subplot(6, 6, i+1)\n\n        digit.grid(which = \'major\', linestyle = \':\', linewidth = 0.5, color = \'blue\')\n        digit.grid(which = \'minor\', linestyle = \':\', linewidth = 0.5, color = \'blue\')\n\n        digit.xaxis.set_ticklabels([])\n        digit.yaxis.set_ticklabels([])\n\n        digit.minorticks_on()\n\n        if channels == 3:\n            color_img = train_data[i].reshape(channels, 32, 32).transpose([1, 2, 0])\n            digit.imshow(color_img, interpolation = \'nearest\')\n        else:\n            digit.imshow(train_data[i].reshape(img_specs[dataset][\'img_height\'],\n                                               img_specs[dataset][\'img_width\']),\n                                               cmap = plt.cm.binary, interpolation = \'nearest\')\n\n        if train_target is not None:\n            digit.text(img_specs[dataset][\'pix_row\'],\n                       img_specs[dataset][\'pix_col\'],\n                       str(train_target.astype(\'int\')[i]))\n\n    plt.show()\n\n\ndef plot_img_results(test_data, test_label, predictions, fig_dims = (6, 6), dataset = \'digits\', channels = 1):\n    fig = plt.figure(figsize = fig_dims)\n    fig.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1, hspace = 0.05, wspace = 0.05)\n\n    for i in range(36):\n        digit = fig.add_subplot(6, 6, i + 1, xticks = [], yticks = [])\n\n        if channels == 3:\n            color_img = test_data[i].reshape(channels,\n                                             img_specs[dataset][\'img_height\'],\n                                             img_specs[dataset][\'img_width\']).transpose([1, 2, 0])\n            digit.imshow(color_img, interpolation = \'nearest\')\n        else:\n            digit.imshow(test_data.reshape(-1,\n                                           img_specs[dataset][\'img_height\'],\n                                           img_specs[dataset][\'img_width\'])[i],\n                                           cmap = plt.cm.binary, interpolation = \'nearest\')\n\n        if predictions[i] == test_label[i]:\n            digit.text(img_specs[dataset][\'pix_row\'],\n                       img_specs[dataset][\'pix_col\'],\n                       str(predictions[i]), color = \'green\')\n\n        else:\n            digit.text(img_specs[dataset][\'pix_row\'],\n                       img_specs[dataset][\'pix_col\'],\n                       str(predictions[i]), color = \'red\')\n\n    plt.show()\n\n\ndef plot_generated_img_samples(test_label,\n                                           predictions,\n                                           fig_dims   = (6, 6),\n                                           dataset    = \'digits\',\n                                           channels   = 1,\n                                           to_save    = False,\n                                           iteration  = 0,\n                                           model_name = \'\'):\n\n    fig = plt.figure(figsize = fig_dims)\n    fig.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1, hspace = 0.05, wspace = 0.05)\n\n    for i in range(36):\n        digit = fig.add_subplot(6, 6, i+1, xticks = [], yticks = [])\n\n        if channels == 3:\n            color_img = predictions[i].reshape(channels,\n                                               img_specs[dataset][\'img_height\'],\n                                               img_specs[dataset][\'img_width\']).transpose([1, 2, 0])\n            digit.imshow(color_img, interpolation = \'nearest\')\n        else:\n            digit.imshow(predictions.reshape(-1,\n                                             img_specs[dataset][\'img_height\'],\n                                             img_specs[dataset][\'img_width\'])[i],\n                                             cmap = plt.cm.binary, interpolation = \'nearest\')\n\n        if test_label is not None:\n            digit.text(img_specs[dataset][\'pix_row\'],\n                       img_specs[dataset][\'pix_col\'],\n                       str(test_label[i]), color = \'blue\')\n\n    if to_save:\n        plt.suptitle((\'{}{}\'.format(\'Generator Epoch: \', iteration)), y = 1.05, fontsize = 12).set_color(\'blue\')\n        plt.savefig(\'../plots/generated/\'+(\'{}{}{}\'.format(model_name, \'_\', iteration, \'.png\')))\n\n    plt.show(block = False) if to_save else plt.show(block = True)\n\n\ndef plot_regression_results(train_data,\n                                        train_label,\n                                        test_data,\n                                        test_label,\n                                        input_data,\n                                        pred_line,\n                                        mse,\n                                        super_title,\n                                        y_label,\n                                        x_label,\n                                        model_name = \'\',\n                                        to_save    = False,\n                                        fig_dims   = FIG_SIZE,\n                                        font_size  = 10):\n\n    plt.figure(figsize = fig_dims)\n\n    cmap  = plt.get_cmap(\'summer\')\n    train = plt.scatter(train_data, train_label, color = cmap(0.8), s = 12)\n    test  = plt.scatter(test_data,  test_label,  color = cmap(0.4), s = 12)\n\n    # minimum parameters: plt.plot(input_data, pred_line, \'*\', color = \'green\', markersize = 4)\n    plt.plot(input_data, pred_line, marker = \'*\', color = \'green\', markersize = 4, linestyle = \'none\')\n    plt.suptitle(super_title)\n\n    if mse is not None:\n        plt.title(""MSE: {:4.2f}"".format(mse), size = font_size)\n\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.legend((train, test), (""Train"", ""Test""), loc = \'upper left\')\n\n    if to_save:\n        plt.savefig(\'../plots/metrics/\'+(\'{}{}{:4.2f}{}{}{}\'.format(model_name,\n                                                                    \'_mse_\',\n                                                                    mse,\n                                                                    \'_\',\n                                                                    time.strftime(""%Y-%m-%d_%H-%M-%S""),\'.png\')))\n\n    plt.show()\n'"
ztlearn/utils/sequence_utils.py,12,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom .data_utils import one_hot\n\n#-----------------------------------------------------------------------------#\n#                     GENERATE SYNTHETIC SEQUENCES DATA                       #\n#-----------------------------------------------------------------------------#\n\ndef gen_mult_sequence_xtyt(nums, cols = 10, factor = 10, tensor_dtype = np.int):\n    assert factor >= cols, 'factor should be more than or equal to cols'\n    lookup = cols * factor\n\n    x = np.zeros([nums, cols, lookup], dtype = tensor_dtype)\n    y = np.zeros([nums, cols, lookup], dtype = tensor_dtype)\n\n    for i in range(nums):\n        start = np.random.randint(1, cols)\n        seq   = np.arange(start, (start*cols)+1, start)\n        x[i]  = one_hot(seq, lookup)\n        y[i]  = np.roll(x[i], -1, axis=0)\n\n    y[:, -1, 1] = 1\n\n    return x, y, lookup\n\ndef gen_mult_sequence_xtym(nums, cols = 10, factor = 10, tensor_dtype = np.int):\n    assert factor >= cols, 'factor should be more than or equal to cols'\n    lookup = cols * factor\n    cols_p = cols - 1\n\n    x   = np.zeros([nums, cols, lookup], dtype = tensor_dtype)\n    x_p = np.zeros([nums, cols_p, lookup], dtype = tensor_dtype)\n    y   = np.zeros([nums, lookup], dtype = np.int)\n\n    for i in range(nums):\n        start  = np.random.randint(1, cols)\n        seq    = np.arange(start, (start*cols)+1, start)\n        x[i]   = one_hot(seq, lookup)\n        x_p[i] = x[i,:-1,:]\n        y[i]   = x[i,cols_p,:]\n\n    return x_p, y, lookup\n"""
ztlearn/utils/text_utils.py,9,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\n#-----------------------------------------------------------------------------#\n#                       TEXT UTILITY FUNCTIONS                                #\n#-----------------------------------------------------------------------------#\n\ndef gen_char_sequence_xtym(text, maxlen, step, tensor_dtype = np.int):\n    chars     = sorted(list(set(text)))\n    len_chars = len(chars)\n    len_text  = len(text)\n\n    char_to_indices = {c: i for i, c in enumerate(chars)}\n    # indices_to_char = {i: c for i, c in enumerate(chars)}\n\n    sentences, next_chars = [], []\n    for i in range(0, len_text - maxlen, step):\n        sentences.append(text[i: i + maxlen])\n        next_chars.append(text[i + maxlen])\n\n    len_sentences = len(sentences)\n\n    x = np.zeros((len_sentences, maxlen, len_chars), dtype = tensor_dtype)\n    y = np.zeros((len_sentences, len_chars), dtype = tensor_dtype)\n\n    for i, sentence in enumerate(sentences):\n        for t, char in enumerate(sentence):\n            x[i, t, char_to_indices[char]] = 1\n        y[i, char_to_indices[next_chars[i]]] = 1\n\n    return x, y, len_chars\n    \n\ndef gen_char_sequence_xtyt(text, maxlen, step, tensor_dtype = np.int):\n    chars     = sorted(list(set(text)))\n    len_chars = len(chars)\n    len_text  = len(text)\n\n    char_to_indices = {c: i for i, c in enumerate(chars)}\n    # indices_to_char = {i: c for i, c in enumerate(chars)}\n\n    sentences, next_chars = [], []\n    for i in range(0, len_text - maxlen + 1, step):\n        sentences.append(text[i : i + maxlen])\n        next_chars.append(text[i+1 : i+1 + maxlen])\n\n    len_sentences = len(sentences)\n\n    x = np.zeros((len_sentences, maxlen, len_chars), dtype = tensor_dtype)\n    y = np.zeros((len_sentences, maxlen, len_chars), dtype = tensor_dtype)\n\n    for i, sentence in enumerate(sentences):\n        for t, char in enumerate(sentence):\n            x[i, t, char_to_indices[char]] = 1\n\n    for i, sentence in enumerate(next_chars):\n        for t, char in enumerate(sentence):\n            y[i, t, char_to_indices[char]] = 1\n\n    return x, y, len_chars\n\n\ndef pad_sequence(sequence,\n                           maxlen     = None,\n                           dtype      = \'int32\',\n                           padding    = \'pre\',\n                           truncating = \'pre\',\n                           value      = 0.0):\n    """""" pad or truncate sequences depending on size of maxlen - longest sequence """"""\n    if (maxlen is None):  return sequence\n\n    np_sequence   = np.array(sequence)\n    sequence_size = np_sequence.size\n\n    if (maxlen > sequence_size):\n        pad_size = maxlen - sequence_size\n        if (padding == \'pre\'):  front_pad, back_pad = pad_size, 0\n        if (padding == \'post\'): front_pad, back_pad = 0, pad_size\n\n        paded = np.pad(sequence,\n                                 (front_pad, back_pad),\n                                 \'constant\',\n                                 constant_values = (value, value))\n        return paded\n\n    if (sequence_size > maxlen):\n        trunc_size = sequence_size - maxlen\n        if (truncating == \'pre\'):  truncated = np_sequence[:-trunc_size]\n        if (truncating == \'post\'): truncated = np_sequence[trunc_size:]\n\n        return truncated\n\n    if(sequence_size == maxlen): return np_sequence\n\n\ndef get_sentence_tokens(text_list, maxlen = None, dtype = \'int32\'):\n    unique_words = list(set(text_list.split())) # get the unique words\n\n    import re\n    sentences = re.split(r\'(?<=[.?!])\\s+\', text_list)\n\n    if maxlen is None:\n        maxlen = max(longest_sentence(sentences))\n\n    sentence_index_list = []\n    for _, s in enumerate(sentences):\n        sentence_index = list(map(unique_words.index, s.split()))\n        padded_index   = pad_sequence(sentence_index, maxlen)\n        sentence_index_list.append(padded_index)\n\n    return np.array(sentence_index_list), len(unique_words), maxlen\n\n\ndef longest_sentence(sentences):\n    """""" find longest sentence in a list of sentences """"""\n    if isinstance(sentences, list):\n        yield len(sentences)\n        for sentence in sentences:\n            yield from longest_sentence(sentence)\n'"
ztlearn/utils/time_deco_utils.py,0,"b'# -*- coding: utf-8 -*-\n\nimport time\nimport types\nfrom functools import wraps\nfrom datetime import timedelta\n\n\nclass LogIfBusy:\n\n    def __init__(self, func):\n        wraps(func)(self)\n\n    def __call__(self, *args, **kwargs):\n        print(\'\\nSTART: {}\\n\'.format(time.strftime(""%a, %d %b %Y %H:%M:%S"")))\n        start  = time.time()\n        result = self.__wrapped__(*args, **kwargs)\n        stop   = time.time()\n        print(\'\\nFINISH: {}\\n\'.format(time.strftime(""%a, %d %b %Y %H:%M:%S"")))\n        print(\'TIMER: {} operation took: {} (h:mm:ss) to complete.\\n\'.format(self.__wrapped__.__name__,\n                                                                             timedelta(seconds = timedelta(seconds = (stop-start)).seconds)))\n\n        return result\n\n    def __get__(self, instance, cls):\n        if instance is None:\n            return self\n        else:\n            return types.MethodType(self, instance)\n'"
examples/text/nietzsche/nietzsche_gru.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.layers import GRU\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.utils import gen_char_sequence_xtyt\n\n\ntext = open('../../../ztlearn/datasets/text/nietzsche_short.txt').read().lower()\nx, y, len_chars = gen_char_sequence_xtyt(text, maxlen = 30, step = 1)\ndel text\n\ntrain_data, test_data, train_label, test_label = train_test_split(x, y, test_size = 0.4)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'rmsprop', momentum = 0.1, lr = 0.01)\n\n# model definition\nmodel = Sequential()\nmodel.add(GRU(128, activation = 'tanh', input_shape = (30, len_chars)))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('nietzsche gru')\n\nmodel_epochs = 20\nfit_stats = model.fit(train_data,\n                      train_label,\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data, test_label),\n                      verbose         = False)\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/text/nietzsche/nietzsche_lstm.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.utils import gen_char_sequence_xtym\nfrom ztlearn.dl.layers import LSTM, Flatten, Dense\n\n\ntext = open('../../../ztlearn/datasets/text/nietzsche_short.txt').read().lower()\nx, y, len_chars = gen_char_sequence_xtym(text, maxlen = 30, step = 1)\ndel text\n\ntrain_data, test_data, train_label, test_label = train_test_split(x, y, test_size = 0.4)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'rmsprop', momentum = 0.1, lr = 0.01)\n\n# model definition\nmodel = Sequential()\nmodel.add(LSTM(128, activation = 'tanh', input_shape = (30, len_chars)))\nmodel.add(Flatten())\nmodel.add(Dense(len_chars, activation = 'softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('nietzsche lstm')\n\nmodel_epochs = 20\nfit_stats = model.fit(train_data,\n                      train_label,\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data, test_label))\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/text/nietzsche/nietzsche_rnn.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.utils import gen_char_sequence_xtym\nfrom ztlearn.dl.layers import RNN, Flatten, Dense\n\n\ntext = open('../../../ztlearn/datasets/text/nietzsche_short.txt').read().lower()\nx, y, len_chars = gen_char_sequence_xtym(text, maxlen = 30, step = 1)\ndel text\n\ntrain_data, test_data, train_label, test_label = train_test_split(x, y, test_size = 0.4)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'rmsprop', momentum = 0.1, lr = 0.01)\n\n# model definition\nmodel = Sequential()\nmodel.add(RNN(128, activation = 'tanh', bptt_truncate = 24, input_shape = (30, len_chars)))\nmodel.add(Flatten())\nmodel.add(Dense(len_chars, activation = 'softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('nietzsche rnn')\n\nmodel_epochs = 20\nfit_stats = model.fit(train_data,\n                      train_label,\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data, test_label))\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/text/shakespeare/shakespeare_gru.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.layers import GRU\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.utils import gen_char_sequence_xtyt\n\n\ntext = open('../../../ztlearn/datasets/text/tinyshakespeare_short.txt').read().lower()\nx, y, len_chars = gen_char_sequence_xtyt(text, maxlen = 30, step = 1)\ndel text\n\ntrain_data, test_data, train_label, test_label = train_test_split(x, y, test_size = 0.4)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'rmsprop', momentum = 0.1, lr = 0.01)\n\n# model definition\nmodel = Sequential()\nmodel.add(GRU(128, activation = 'tanh', input_shape = (30, len_chars)))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('shakespeare gru')\n\nmodel_epochs = 20\nfit_stats = model.fit(train_data,\n                      train_label,\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data, test_label))\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/text/shakespeare/shakespeare_lstm.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.utils import gen_char_sequence_xtym\nfrom ztlearn.dl.layers import LSTM, Flatten, Dense\n\n\ntext = open('../../../ztlearn/datasets/text/tinyshakespeare_short.txt').read().lower()\nx, y, len_chars = gen_char_sequence_xtym(text, maxlen = 30, step = 1)\ndel text\n\ntrain_data, test_data, train_label, test_label = train_test_split(x, y, test_size = 0.4)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'rmsprop', momentum = 0.1, lr = 0.01)\n\n# model definition\nmodel = Sequential()\nmodel.add(LSTM(128, activation = 'tanh', input_shape = (30, len_chars)))\nmodel.add(Flatten())\nmodel.add(Dense(len_chars,  activation = 'softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('shakespeare lstm')\n\nmodel_epochs = 20\nfit_stats = model.fit(train_data,\n                      train_label,\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data, test_label))\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/text/shakespeare/shakespeare_rnn.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.utils import gen_char_sequence_xtym\nfrom ztlearn.dl.layers import RNN, Flatten, Dense\n\n\ntext = open('../../../ztlearn/datasets/text/tinyshakespeare_short.txt').read().lower()\nx, y, len_chars = gen_char_sequence_xtym(text, maxlen = 30, step = 1)\ndel text\n\ntrain_data, test_data, train_label, test_label = train_test_split(x, y, test_size = 0.4)\n\n# optimizer definition\nopt = register_opt(optimizer_name = 'rmsprop', momentum = 0.1, lr = 0.01)\n\n# model definition\nmodel = Sequential()\nmodel.add(RNN(128, activation = 'tanh', bptt_truncate = 24, input_shape = (30, len_chars)))\nmodel.add(Flatten())\nmodel.add(Dense(len_chars,  activation = 'softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = opt)\n\nmodel.summary('shakespeare rnn')\n\nmodel_epochs = 20\nfit_stats = model.fit(train_data,\n                      train_label,\n                      batch_size      = 128,\n                      epochs          = model_epochs,\n                      validation_data = (test_data, test_label))\n\nplot_metric('loss', model_epochs, fit_stats['train_loss'], fit_stats['valid_loss'], model_name = model.model_name)\nplot_metric('accuracy', model_epochs, fit_stats['train_acc'], fit_stats['valid_acc'], model_name = model.model_name)\n"""
examples/text/word_embedding/sentiment_analysis_mlp.py,3,"b'import numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.utils import get_sentence_tokens\nfrom ztlearn.dl.layers import Embedding, Flatten, Dense\n\ntext_list = [\n    \'I like rally cars.\',\n    \'This is good.\',\n    \'This is bad.\',\n    \'Rainy days are the worst.\',\n    \'Mercedes is a good brand.\',\n    \'Nobody has the patience for bad food.\',\n    \'I believe Maasai Mara is the best.\',\n    \'This is definately a bad idea.\',\n    \'The trains in the city center suck!\',\n    \'I have a new shinny app love it!\',\n    \'Roads here are bad.\',\n    \'The hospital has good service.\',\n]\n\nparagraph = \' \'.join(text_list)\nsentences_tokens, vocab_size, longest_sentence = get_sentence_tokens(paragraph)\nsentence_targets = one_hot(np.array([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1]))\n\ntrain_data, test_data, train_label, test_label = train_test_split(sentences_tokens,\n                                                                  sentence_targets,\n                                                                  test_size   = 0.2,\n                                                                  random_seed = 5)\n\n# optimizer definition\nopt = register_opt(optimizer_name = \'sgd_momentum\', momentum = 0.01, lr = 0.01)\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 4, input_length = longest_sentence))\nmodel.add(Flatten())\nmodel.add(Dense(2, activation = \'relu\'))\nmodel.compile(loss = \'cce\', optimizer = opt)\n\nmodel.summary(\'embedded sentences mlp\')\n\n""""""\nNOTE:\nbatch size should be equal the size of embedding\nvectors and divisible  by the training  set size\n""""""\n\nmodel_epochs = 150\nfit_stats = model.fit(train_data,\n                      train_label,\n                      batch_size = 4,\n                      epochs     = model_epochs,\n                      validation_data = (test_data, test_label))\n\nplot_metric(\'loss\', model_epochs, fit_stats[\'train_loss\'], fit_stats[\'valid_loss\'], model_name = model.model_name)\nplot_metric(\'accuracy\', model_epochs, fit_stats[\'train_acc\'], fit_stats[\'valid_acc\'], model_name = model.model_name)\n\n# test out with the first sentence - sentences_tokens[0]\noutput_array = model.predict(np.expand_dims(sentences_tokens[0], axis=0))\nprint(np.argmax(output_array))\n'"
examples/text/word_embedding/sentiment_analysis_rnn.py,3,"b'import numpy as np\n\nfrom ztlearn.utils import *\nfrom ztlearn.dl.models import Sequential\nfrom ztlearn.optimizers import register_opt\nfrom ztlearn.utils import get_sentence_tokens\nfrom ztlearn.dl.layers import Embedding, Flatten, Dense, RNN\n\ntext_list = [\n    \'I like rally cars.\',\n    \'This is good.\',\n    \'This is bad.\',\n    \'Rainy days are the worst.\',\n    \'Mercedes is a good brand.\',\n    \'Nobody has the patience for bad food.\',\n    \'I believe Maasai Mara is the best.\',\n    \'This is definately a bad idea.\',\n    \'The trains in the city center suck!\',\n    \'I have a new shinny app love it!\',\n    \'Roads here are bad.\',\n    \'The hospital has good service.\',\n]\n\nparagraph = \' \'.join(text_list)\nsentences_tokens, vocab_size, longest_sentence = get_sentence_tokens(paragraph)\nsentence_targets = one_hot(np.array([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1]))\n\ntrain_data, test_data, train_label, test_label = train_test_split(sentences_tokens,\n                                                                  sentence_targets,\n                                                                  test_size   = 0.2,\n                                                                  random_seed = 5)\n\n# optimizer definition\nopt = register_opt(optimizer_name = \'adamax\', momentum = 0.01, lr = 0.001)\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 2, input_length = longest_sentence))\nmodel.add(RNN(5, activation = \'tanh\', bptt_truncate = 2, input_shape = (2, longest_sentence)))\nmodel.add(Flatten())\nmodel.add(Dense(2, activation = \'softmax\'))\nmodel.compile(loss = \'bce\', optimizer = opt)\n\nmodel.summary(\'embedded sentences rnn\')\n\n""""""\nNOTE:\nbatch size should be equal the size of embedding\nvectors and divisible  by the training  set size\n""""""\n\nmodel_epochs = 500\nfit_stats = model.fit(train_data,\n                      train_label,\n                      batch_size = 2,\n                      epochs     = model_epochs,\n                      validation_data = (test_data, test_label))\n\nplot_metric(\'loss\', model_epochs, fit_stats[\'train_loss\'], fit_stats[\'valid_loss\'], model_name = model.model_name)\nplot_metric(\'accuracy\', model_epochs, fit_stats[\'train_acc\'], fit_stats[\'valid_acc\'], model_name = model.model_name)\n\n# test out with the first sentence - sentences_tokens[0]\noutput_array = model.predict(np.expand_dims(sentences_tokens[0], axis=0))\nprint(np.argmax(output_array))\n'"
ztlearn/datasets/boston/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import file(s)\nfrom . import boston_housing\n\n# import from digits.py\nfrom .boston_housing import fetch_boston\n'
ztlearn/datasets/boston/boston_housing.py,0,"b""import os\nimport pandas as pd\n\nfrom ztlearn.utils import maybe_download\nfrom ztlearn.utils import train_test_split\nfrom ztlearn.datasets.data_set import DataSet\n\nURL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'\n\ndef fetch_boston(data_target = True, custom_path = os.getcwd()):\n    file_path = maybe_download(custom_path + '/../../ztlearn/datasets/boston/', URL)\n    describe  = [\n        'CRIM',\n        'ZN',\n        'INDUS',\n        'CHAS',\n        'NOX',\n        'RM',\n        'AGE',\n        'DIS',\n        'RAD',\n        'TAX',\n        'PTRATIO',\n        'B',\n        'LSTAT',\n        'MEDV'\n    ]\n\n    dataframe    = pd.read_csv(file_path, delim_whitespace = True, names = describe)\n    data, target = dataframe.values[:,0:13], dataframe.values[:,13]\n\n    if data_target:\n        return DataSet(data, target, describe)\n    else:\n        return train_test_split(data, target, test_size = 0.2, random_seed = 2)\n"""
ztlearn/datasets/cifar/__init__.py,0,b'# -*- coding: utf-8 -*-\n\nfrom . import cifar_10\nfrom . import cifar_100\n\n# import from cifar_10.py\nfrom .cifar_10.cifar_10 import fetch_cifar_10\n\n# import from cifar_100.py\nfrom .cifar_100.cifar_100 import fetch_cifar_100\n'
ztlearn/datasets/digits/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import file(s)\nfrom . import digits\n\n# import from digits.py\nfrom .digits import fetch_digits\n'
ztlearn/datasets/digits/digits.py,2,"b""import os\nimport gzip\nimport numpy as np\n\nfrom ztlearn.utils import maybe_download\nfrom ztlearn.utils import train_test_split\nfrom ztlearn.datasets.data_set import DataSet\n\nURL = 'https://github.com/scikit-learn/scikit-learn/raw/master/sklearn/datasets/data/digits.csv.gz'\n\ndef fetch_digits(data_target = True, custom_path = os.getcwd()):\n    file_path = maybe_download(custom_path+'/../../ztlearn/datasets/digits/', URL)\n\n    with gzip.open(file_path, 'rb') as digits_path:\n        digits_data = np.loadtxt(digits_path, delimiter=',')\n\n    data, target = digits_data[:, :-1], digits_data[:, -1].astype(np.int)\n\n    if data_target:\n        return DataSet(data, target)\n    else:\n        return train_test_split(data, target, test_size = 0.33, random_seed = 5)\n"""
ztlearn/datasets/fashion/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import file(s)\nfrom . import fashion_mnist\n\n# import from fashion_mnist.py\nfrom .fashion_mnist import fetch_fashion_mnist\n'
ztlearn/datasets/fashion/fashion_mnist.py,6,"b""import os\nimport gzip\nimport numpy as np\n\nfrom ztlearn.utils import maybe_download\nfrom ztlearn.datasets.data_set import DataSet\n\nURL = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/'\n\ntrain_files = {\n    'train_labels' : 'train-labels-idx1-ubyte.gz',\n    'train_data'   : 'train-images-idx3-ubyte.gz'\n}\n\ntest_files = {\n    'test_labels' : 't10k-labels-idx1-ubyte.gz',\n    'test_data'   : 't10k-images-idx3-ubyte.gz'\n}\n\ndef fetch_fashion_mnist(data_target = True, custom_path = os.getcwd()):\n    train_dict = {}\n    for file_key, file_value in train_files.items():\n        train_dict.update({file_key : maybe_download(custom_path + '/../../ztlearn/datasets/fashion/', URL + file_value)})\n\n    with gzip.open(list(train_dict.values())[0], 'rb') as label_path:\n        train_label = np.frombuffer(label_path.read(), dtype = np.uint8, offset = 8)\n\n    with gzip.open(list(train_dict.values())[1], 'rb') as data_path:\n        train_data = np.frombuffer(data_path.read(), dtype = np.uint8, offset = 16).reshape(len(train_label), 784)\n\n    test_dict = {}\n    for file_key, file_value in test_files.items():\n        test_dict.update({file_key : maybe_download(custom_path + '/../../ztlearn/datasets/fashion/', URL + file_value)})\n\n    with gzip.open(list(test_dict.values())[0], 'rb') as label_path:\n        test_label = np.frombuffer(label_path.read(), dtype = np.uint8, offset = 8)\n\n    with gzip.open(list(test_dict.values())[1], 'rb') as data_path:\n        test_data = np.frombuffer(data_path.read(), dtype = np.uint8, offset = 16).reshape(len(test_label), 784)\n\n    if data_target:\n        return DataSet(np.concatenate((train_data,  test_data),  axis = 0),\n                       np.concatenate((train_label, test_label), axis = 0))\n    else:\n        return train_data, test_data, train_label, test_label\n"""
ztlearn/datasets/iris/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import file(s)\nfrom . import iris\n\n# import from iris.py\nfrom .iris import fetch_iris\n'
ztlearn/datasets/iris/iris.py,0,"b""import os\nimport pandas as pd\n\nfrom ztlearn.utils import maybe_download\nfrom ztlearn.utils import train_test_split\nfrom ztlearn.datasets.data_set import DataSet\n\nURL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n\ndef fetch_iris(data_target = True, custom_path = os.getcwd()):\n    file_path = maybe_download(custom_path + '/../../ztlearn/datasets/iris/', URL)\n    describe  = [\n        'sepal-length (cm)',\n        'sepal-width (cm)',\n        'petal-length (cm)',\n        'petal-width (cm)',\n        'petal_type'\n    ]\n\n    dataframe = pd.read_csv(file_path, names = describe)\n\n    # convert petal type column to categorical data i.e {0:'Iris-setosa', 1:'Iris-versicolor', 2:'Iris-virginica'}\n    dataframe.petal_type    = pd.Categorical(dataframe.petal_type)\n    dataframe['petal_type'] = dataframe.petal_type.cat.codes\n\n    data, target = dataframe.values[:,0:4], dataframe.values[:,4].astype('int')\n\n    if data_target:\n        return DataSet(data, target, describe)\n    else:\n        return train_test_split(data, target, test_size = 0.2, random_seed = 2)\n"""
ztlearn/datasets/mnist/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import file(s)\nfrom . import mnist\n\n# import from mnist.py\nfrom .mnist import fetch_mnist\n'
ztlearn/datasets/mnist/mnist.py,6,"b""import os\nimport gzip\nimport numpy as np\n\nfrom ztlearn.utils import maybe_download\nfrom ztlearn.datasets.data_set import DataSet\n\nURL = 'https://storage.googleapis.com/cvdf-datasets/mnist/'\n\ntrain_files = {\n    'train_labels' : 'train-labels-idx1-ubyte.gz',\n    'train_data'   : 'train-images-idx3-ubyte.gz'\n}\n\ntest_files = {\n    'test_labels' : 't10k-labels-idx1-ubyte.gz',\n    'test_data'   : 't10k-images-idx3-ubyte.gz'\n}\n\ndef fetch_mnist(data_target = True, custom_path = os.getcwd()):\n    train_dict = {}\n    for file_key, file_value in train_files.items():\n        train_dict.update({file_key : maybe_download(custom_path + '/../../ztlearn/datasets/mnist/', URL + file_value)})\n\n    with gzip.open(list(train_dict.values())[0], 'rb') as label_path:\n        train_label = np.frombuffer(label_path.read(), dtype = np.uint8, offset = 8)\n\n    with gzip.open(list(train_dict.values())[1], 'rb') as data_path:\n        train_data = np.frombuffer(data_path.read(), dtype = np.uint8, offset = 16).reshape(len(train_label), 784)\n\n    test_dict = {}\n    for file_key, file_value in test_files.items():\n        test_dict.update({file_key : maybe_download(custom_path + '/../../ztlearn/datasets/mnist/', URL + file_value)})\n\n    with gzip.open(list(test_dict.values())[0], 'rb') as label_path:\n        test_label = np.frombuffer(label_path.read(), dtype = np.uint8, offset = 8)\n\n    with gzip.open(list(test_dict.values())[1], 'rb') as data_path:\n        test_data = np.frombuffer(data_path.read(), dtype = np.uint8, offset = 16).reshape(len(test_label), 784)\n\n    if data_target:\n        return DataSet(np.concatenate((train_data,  test_data),  axis = 0),\n                       np.concatenate((train_label, test_label), axis = 0))\n    else:\n        return train_data, test_data, train_label, test_label\n"""
ztlearn/datasets/pima/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import file(s)\nfrom . import pima_indians\n\n# import from pima_indians.py\nfrom .pima_indians import fetch_pima_indians\n'
ztlearn/datasets/pima/pima_indians.py,0,"b""import os\nimport pandas as pd\n\nfrom ztlearn.utils import maybe_download\nfrom ztlearn.utils import train_test_split\nfrom ztlearn.datasets.data_set import DataSet\n\nURL = 'http://ftp.ics.uci.edu/pub/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data'\n\ndef fetch_pima_indians(data_target = True, custom_path = os.getcwd()):\n    file_path = maybe_download(custom_path + '/../../ztlearn/datasets/pima/', URL)\n    describe  = [\n        'Pregnancies',\n        'Glucose',\n        'BloodPressure',\n        'SkinThickness',\n        'DiabetesPedigreeFunction',\n        'Age',\n        'Insulin',\n        'BMI',\n        'Outcome (0 or 1)'\n    ]\n\n    dataframe    = pd.read_csv(file_path, names = describe)\n    data, target = dataframe.values[:,0:8], dataframe.values[:,8]\n\n    if data_target:\n        return DataSet(data, target, describe)\n    else:\n        return train_test_split(data, target, test_size = 0.2, random_seed = 2)\n"""
ztlearn/dl/layers/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import packages(s)\nfrom . import recurrent\n\n# import file(s)\nfrom . import base\nfrom . import core\nfrom . import pooling\nfrom . import embedding\nfrom . import convolutional\nfrom . import normalization\n\n# base layer(s)\nfrom .base import Layer\n\n# common layer(s)\nfrom .core import Dense\nfrom .core import Dropout\nfrom .core import Flatten\nfrom .core import Reshape\nfrom .core import Activation\nfrom .core import UpSampling2D\n\n# embedding layer(s)\nfrom .embedding import Embedding\n\n# pooling layer(s)\nfrom .pooling import MaxPooling2D\nfrom .pooling import AveragePool2D\n\n# convolutional layer(s)\nfrom .convolutional import Conv2D\nfrom .convolutional import ConvLoop2D\nfrom .convolutional import ConvToeplitzMat\n\n# normalization layer(s)\nfrom .normalization import BatchNormalization\nfrom .normalization import LayerNormalization1D\n\n# recurrent layer(s)\nfrom .recurrent import RNN\nfrom .recurrent import GRU\nfrom .recurrent import LSTM\n'
ztlearn/dl/layers/base.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom abc import ABC, abstractmethod\n\nclass Layer(ABC):\n\n    def __init__(self, layer_name = 'zeta_squential'):\n        self.layer_name = layer_name\n\n    @property\n    def input_shape(self):\n        return self.__input_shape\n\n    @input_shape.setter\n    def input_shape(self, input_shape):\n        self.__input_shape = input_shape\n\n    @property\n    def output_shape(self):\n        return self.input_shape\n\n    @property\n    def layer_parameters(self):\n        return 0\n\n    @property\n    def layer_name(self):\n        return self.__class__.__name__\n\n    @abstractmethod\n    def pass_forward(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def pass_backward(self):\n        raise NotImplementedError\n"""
ztlearn/dl/layers/convolutional.py,35,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom .base import Layer\nfrom ztlearn.utils import get_pad\nfrom ztlearn.utils import unroll_inputs\nfrom ztlearn.utils import im2col_indices\nfrom ztlearn.utils import col2im_indices\nfrom ztlearn.utils import get_output_dims\nfrom ztlearn.initializers import InitializeWeights as init\nfrom ztlearn.optimizers import OptimizationFunction as optimizer\n\n\nclass Conv(Layer):\n\n    def __init__(self,\n                       filters     = 32,\n                       kernel_size = (3, 3),\n                       activation  = None,\n                       input_shape = (1, 8, 8),\n                       strides     = (1, 1),\n                       padding     = 'valid'):\n\n        self.filters     = filters\n        self.strides     = strides\n        self.padding     = padding\n        self.activation  = activation\n        self.kernel_size = kernel_size\n        self.input_shape = input_shape\n\n        self.init_method      = None\n        self.optimizer_kwargs = None\n\n        self.is_trainable = True\n\n    @property\n    def trainable(self):\n        return self.is_trainable\n\n    @trainable.setter\n    def trainable(self, is_trainable):\n        self.is_trainable = is_trainable\n\n    @property\n    def weight_initializer(self):\n        return self.init_method\n\n    @weight_initializer.setter\n    def weight_initializer(self, init_method):\n        self.init_method = init_method\n\n    @property\n    def weight_optimizer(self):\n        return self.optimizer_kwargs\n\n    @weight_optimizer.setter\n    def weight_optimizer(self, optimizer_kwargs = {}):\n        self.optimizer_kwargs = optimizer_kwargs\n\n    @property\n    def layer_activation(self):\n        return self.activation\n\n    @layer_activation.setter\n    def layer_activation(self, activation):\n        self.activation = activation\n\n    @property\n    def layer_parameters(self):\n        return sum([np.prod(param.shape) for param in [self.weights, self.bias]])\n\n    @property\n    def output_shape(self):\n        pad_height, pad_width = get_pad(self.padding,\n                                                      self.input_shape[1],\n                                                      self.input_shape[2],\n                                                      self.strides[0],\n                                                      self.strides[1],\n                                                      self.kernel_size[0],\n                                                      self.kernel_size[1])\n\n        output_height, output_width = get_output_dims(self.input_shape[1],\n                                                                           self.input_shape[2],\n                                                                           self.kernel_size,\n                                                                           self.strides,\n                                                                           self.padding)\n\n        return self.filters, int(output_height), int(output_width)\n\n    def prep_layer(self):\n        self.kernel_shape = (self.filters, self.input_shape[0], self.kernel_size[0], self.kernel_size[1])\n        self.weights      = init(self.weight_initializer).initialize_weights(self.kernel_shape)\n        self.bias         = np.zeros((self.kernel_shape[0], 1))\n\n\nclass Conv2D(Conv):\n\n    def __init__(self,\n                       filters     = 32,\n                       kernel_size = (3, 3),\n                       activation  = None,\n                       input_shape = (1, 8, 8),\n                       strides     = (1, 1),\n                       padding     = 'valid'):\n\n        super(Conv2D, self).__init__(filters, kernel_size, activation, input_shape, strides, padding)\n\n    def pass_forward(self, inputs, train_mode = True, **kwargs):\n        self.filter_num, _, _, _  = self.weights.shape\n        self.input_shape          = inputs.shape\n        self.inputs               = inputs\n\n        input_num, input_depth, input_height, input_width = inputs.shape\n\n        pad_height, pad_width = get_pad(self.padding,\n                                                      input_height,\n                                                      input_width,\n                                                      self.strides[0],\n                                                      self.strides[1],\n                                                      self.kernel_size[0],\n                                                      self.kernel_size[1])\n\n        # confirm dimensions\n        assert (input_height + np.sum(pad_height) - self.kernel_size[0]) % self.strides[0] == 0, 'height does not work'\n        assert (input_width  + np.sum(pad_width)  - self.kernel_size[1]) % self.strides[1] == 0, 'width does not work'\n\n        # compute output_height and output_width\n        output_height, output_width = get_output_dims(input_height, input_width, self.kernel_size, self.strides, self.padding)\n\n        # convert to columns\n        self.input_col = im2col_indices(inputs,\n                                                self.kernel_size[0],\n                                                self.kernel_size[1],\n                                                padding = (pad_height, pad_width),\n                                                stride  = 1)\n\n        self.weight_col =  self.weights.reshape(self.filter_num, -1)\n\n        # calculate ouput\n        output = self.weight_col @ self.input_col + self.bias\n        output = output.reshape(self.filter_num, int(output_height), int(output_width), input_num)\n\n        return output.transpose(3, 0, 1, 2)\n\n    def pass_backward(self, grad, epoch_num, batch_num, batch_size):\n        input_num, input_depth, input_height, input_width = self.input_shape\n        doutput_reshaped = grad.transpose(1, 2, 3, 0).reshape(self.filter_num, -1)\n\n        if self.is_trainable:\n\n            dbias = np.sum(grad, axis = (0, 2, 3))\n            dbias = dbias.reshape(self.filter_num, -1)\n\n            dweights = doutput_reshaped @ self.input_col.T\n            dweights = dweights.reshape(self.weights.shape)\n\n            # optimize the weights and bias\n            self.weights = optimizer(self.weight_optimizer).update(self.weights, dweights, epoch_num, batch_num, batch_size)\n            self.bias    = optimizer(self.weight_optimizer).update(self.bias, dbias, epoch_num, batch_num, batch_size)\n\n        # endif self.is_trainable\n\n        weight_reshape = self.weights.reshape(self.filter_num, -1)\n        dinput_col     = weight_reshape.T @ doutput_reshaped\n\n        pad_height, pad_width = get_pad(self.padding,\n                                                      input_height,\n                                                      input_width,\n                                                      self.strides[0],\n                                                      self.strides[1],\n                                                      self.kernel_size[0],\n                                                      self.kernel_size[1])\n\n        dinputs = col2im_indices(dinput_col,\n                                             self.input_shape,\n                                             self.kernel_size[0],\n                                             self.kernel_size[1],\n                                             padding = (pad_height, pad_width),\n                                             stride = self.strides[0])\n\n        return dinputs\n\n\nclass ConvLoop2D(Conv):\n\n    def __init__(self,\n                       filters     = 32,\n                       kernel_size = (3, 3),\n                       activation  = None,\n                       input_shape = (1, 8, 8),\n                       strides     = (1, 1),\n                       padding     = 'valid'):\n\n        super(ConvLoop2D, self).__init__(filters, kernel_size, activation, input_shape, strides, padding)\n\n    def pass_forward(self, inputs, train_mode = True, **kwargs):\n        self.filter_num, _, _, _ = self.weights.shape\n        self.input_shape         = inputs.shape\n        self.inputs              = inputs\n\n        input_num, input_depth, input_height, input_width = inputs.shape\n\n        pad_height, pad_width = get_pad(self.padding,\n                                                      input_height,\n                                                      input_width,\n                                                      self.strides[0],\n                                                      self.strides[1],\n                                                      self.kernel_size[0],\n                                                      self.kernel_size[1])\n\n        x_padded = np.pad(self.inputs, ((0, 0), (0, 0), pad_height, pad_width), mode = 'constant')\n\n        # confirm dimensions\n        assert (input_height + np.sum(pad_height) - self.kernel_size[0]) % self.strides[0]  == 0, 'height does not work'\n        assert (input_width  + np.sum(pad_width)  - self.kernel_size[1]) % self.strides[1]  == 0, 'width does not work'\n\n        # compute output_height and output_width\n        output_height, output_width = get_output_dims(input_height, input_width, self.kernel_size, self.strides, self.padding)\n\n        output = np.zeros((input_num, self.filter_num, output_height, output_width))\n\n        # convolutions\n        for b in np.arange(input_num): # batch number\n            for f in np.arange(self.filter_num): # filter number\n                for h in np.arange(output_height): # output height\n                    for w in np.arange(output_width): # output width\n                        h_stride, w_stride = h * self.strides[0], w * self.strides[1]\n                        x_patch            = x_padded[b, :, h_stride: h_stride + self.kernel_size[0],\n                                                            w_stride: w_stride + self.kernel_size[1]]\n                        output[b, f, h, w] = np.sum(x_patch * self.weights[f]) + self.bias[f]\n\n        return output\n\n    def pass_backward(self, grad, epoch_num, batch_num, batch_size):\n        input_num, input_depth, input_height, input_width = self.inputs.shape\n\n        # initialize the gradient(s)\n        dinputs = np.zeros(self.inputs.shape)\n\n        if self.is_trainable:\n\n            # initialize the gradient(s)\n            dweights = np.zeros(self.weights.shape)\n            dbias    = np.zeros(self.bias.shape)\n\n            pad_height, pad_width = get_pad(self.padding,\n                                                          input_height,\n                                                          input_width,\n                                                          self.strides[0],\n                                                          self.strides[1],\n                                                          self.kernel_size[0],\n                                                          self.kernel_size[1])\n\n            pad_size = (np.sum(pad_height)/2).astype(int)\n            if pad_size != 0:\n                grad = grad[:, :, pad_size: -pad_size, pad_size: -pad_size]\n\n            # dweights\n            for f in np.arange(self.filter_num): # filter number\n                for c in np.arange(input_depth): # input depth (channels)\n                    for h in np.arange(self.kernel_size[0]): # kernel height\n                        for w in np.arange(self.kernel_size[1]): # kernel width\n                            input_patch = self.inputs[:,\n                                                      c,\n                                                      h: input_height - self.kernel_size[0] + h + 1: self.strides[0],\n                                                      w: input_width  - self.kernel_size[1] + w + 1: self.strides[1]]\n\n                            grad_patch           = grad[:, f]\n                            dweights[f, c, h, w] = np.sum(input_patch * grad_patch) / input_num\n\n            # dbias\n            for f in np.arange(self.filter_num): # filter number\n                dbias[f] = np.sum(grad[:, f]) / input_num\n\n            # optimize the weights and bias\n            self.weights = optimizer(self.weight_optimizer).update(self.weights, dweights, epoch_num, batch_num, batch_size)\n            self.bias    = optimizer(self.weight_optimizer).update(self.bias, dbias, epoch_num, batch_num, batch_size)\n\n        # endif self.is_trainable\n\n        # dinputs\n        for b in np.arange(input_num): # batch number\n            for f in np.arange(self.filter_num): # filter number\n                for c in np.arange(input_depth): # input depth (channels)\n                    for h in np.arange(self.kernel_size[0]): # kernel height\n                        for w in np.arange(self.kernel_size[1]): # kernel width\n                            h_stride, w_stride = h * self.strides[0], w * self.strides[1]\n                            dinputs[b,\n                                    c,\n                                    h_stride: h_stride + self.kernel_size[0],\n                                    w_stride: w_stride + self.kernel_size[1]] += self.weights[f, c] * grad[b, f, h, w]\n\n        return dinputs\n\n\nclass ConvToeplitzMat(Conv):\n\n    def __init__(self,\n                       filters     = 32,\n                       kernel_size = (3, 3),\n                       activation  = None,\n                       input_shape = (1, 8, 8),\n                       strides     = (1, 1),\n                       padding     = 'valid'):\n\n        super(ConvToeplitzMat, self).__init__(filters, kernel_size, activation, input_shape, strides, padding)\n\n    def pass_forward(self, inputs, train_mode = True, **kwargs):\n        self.filter_num, _, _, _ = self.weights.shape\n        self.input_shape         = inputs.shape\n        self.inputs              = inputs\n\n        input_num, input_depth, input_height, input_width = inputs.shape\n\n        pad_height, pad_width = get_pad(self.padding,\n                                                      input_height,\n                                                      input_width,\n                                                      self.strides[0],\n                                                      self.strides[1],\n                                                      self.kernel_size[0],\n                                                      self.kernel_size[1])\n\n        x_padded = np.pad(self.inputs, ((0, 0), (0, 0), pad_height, pad_width), mode = 'constant')\n\n        # confirm dimensions\n        assert (input_height + np.sum(pad_height) - self.kernel_size[0]) % self.strides[0]  == 0, 'height does not work'\n        assert (input_width  + np.sum(pad_width)  - self.kernel_size[1]) % self.strides[1]  == 0, 'width does not work'\n\n        # compute output_height and output_width\n        output_height, output_width = get_output_dims(input_height, input_width, self.kernel_size, self.strides, self.padding)\n\n        output = np.zeros((input_num, self.filter_num, output_height, output_width))\n\n        self.input_col = unroll_inputs(x_padded,\n                                                 x_padded.shape[0],\n                                                 x_padded.shape[1],\n                                                 output_height,\n                                                 output_width,\n                                                 self.kernel_size[0])\n\n        # TODO: weights need to be rearraged in a way to have a matrix\n        #       multiplication with the generated toeplitz matrix\n        self.weight_col = self.weights.reshape(self.filter_num, -1)\n\n        # calculate ouput\n        output = self.weight_col @ self.input_col + self.bias\n        # output = np.matmul(self.weight_col, self.input_col) + self.bias\n        output = output.reshape(self.filter_num, int(output_height), int(output_width), input_num)\n\n        return output.transpose(3, 0, 1, 2)\n\n    def pass_backward(self, grad, epoch_num, batch_num, batch_size): pass\n"""
ztlearn/dl/layers/core.py,10,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom .base import Layer\nfrom ztlearn.initializers import InitializeWeights as init\nfrom ztlearn.optimizers import OptimizationFunction as optimizer\nfrom ztlearn.activations import ActivationFunction as activation\n\n\nclass Activation(Layer):\n\n    def __init__(self, function_name, input_shape = None, **kwargs):\n        self.input_shape     = input_shape\n        self.activation_name = function_name\n\n        allowed_kwargs = {\'alpha\'}\n        for kwrd in kwargs:\n            if kwrd not in allowed_kwargs:\n                raise TypeError(\'Unexpected keyword argument passed to activation: \' + str(kwrd))\n\n        self.activation_func = activation(self.activation_name, kwargs)\n\n        self.is_trainable = True\n\n    @property\n    def trainable(self):\n        return self.is_trainable\n\n    @trainable.setter\n    def trainable(self, is_trainable):\n        self.is_trainable = is_trainable\n\n    @property\n    def output_shape(self):\n        return self.input_shape\n\n    @property\n    def layer_name(self):\n        return ""Activation: {}"".format("" "".join(self.activation_name.upper().split(""_"")))\n\n    def prep_layer(self): pass\n\n    def pass_forward(self, input_signal, train_mode = True, **kwargs):\n        self.input_signal = input_signal\n        return self.activation_func.forward(input_signal)\n\n    def pass_backward(self, grad, epoch_num, batch_num, batch_size):\n        return grad * self.activation_func.backward(self.input_signal, epoch_num, batch_num, batch_size)\n\n\nclass Dense(Layer):\n\n    def __init__(self, units, activation = None, input_shape = None):\n        self.units       = units\n        self.activation  = activation\n        self.input_shape = input_shape\n\n        self.bias             = None\n        self.weights          = None\n        self.init_method      = None\n        self.optimizer_kwargs = None\n\n        self.is_trainable = True\n\n    @property\n    def trainable(self):\n        return self.is_trainable\n\n    @trainable.setter\n    def trainable(self, is_trainable):\n        self.is_trainable = is_trainable\n\n    @property\n    def weight_initializer(self):\n        return self.init_method\n\n    @weight_initializer.setter\n    def weight_initializer(self, init_method):\n        self.init_method = init_method\n\n    @property\n    def weight_optimizer(self):\n        return self.optimizer_kwargs\n\n    @weight_optimizer.setter\n    def weight_optimizer(self, optimizer_kwargs = {}):\n        self.optimizer_kwargs = optimizer_kwargs\n\n    @property\n    def layer_activation(self):\n        return self.activation\n\n    @layer_activation.setter\n    def layer_activation(self, activation):\n        self.activation = activation\n\n    @property\n    def layer_parameters(self):\n        return sum([np.prod(param.shape) for param in [self.weights, self.bias]])\n\n    @property\n    def output_shape(self):\n        return (self.units,)\n\n    def prep_layer(self):\n        self.kernel_shape = (self.input_shape[0], self.units)\n        self.weights      = init(self.weight_initializer).initialize_weights(self.kernel_shape)\n        self.bias         = np.zeros((1, self.units))\n        # @@DEPRECATED: initialize bias using the chosen weight initializers\n        # self.bias       = init(self.weight_initializer).initialize_weights((1, self.units))\n\n    def pass_forward(self, inputs, train_mode = True):\n        self.inputs = inputs\n\n        return inputs @ self.weights + self.bias\n\n    def pass_backward(self, grad, epoch_num, batch_num, batch_size):\n        prev_weights = self.weights\n\n        if self.is_trainable:\n\n            dweights = self.inputs.T @ grad\n            dbias    = np.sum(grad, axis = 0, keepdims = True)\n\n            self.weights = optimizer(self.weight_optimizer).update(self.weights, dweights, epoch_num, batch_num, batch_size)\n            self.bias    = optimizer(self.weight_optimizer).update(self.bias, dbias, epoch_num, batch_num, batch_size)\n\n        # endif self.is_trainable\n\n        return grad @ prev_weights.T\n\n\nclass Dropout(Layer):\n\n    def __init__(self, drop = 0.5):\n        self.drop = drop\n        self.mask = None\n\n        self.is_trainable = True\n\n    @property\n    def trainable(self):\n        return self.is_trainable\n\n    @trainable.setter\n    def trainable(self, is_trainable):\n        self.is_trainable = is_trainable\n\n    @property\n    def output_shape(self):\n        return self.input_shape\n\n    def prep_layer(self): pass\n\n    def pass_forward(self, inputs, train_mode = True, **kwargs):\n        if 0. < self.drop < 1.:\n            keep_prob = (1 - self.drop)\n            if train_mode:\n                self.mask = np.random.binomial(1, keep_prob, size = inputs.shape) / keep_prob\n                keep_prob = self.mask\n            return inputs * keep_prob\n        else:\n            return inputs\n\n    def pass_backward(self, grad, epoch_num, batch_num, batch_size):\n        if 0. < self.drop < 1.:\n            return grad * self.mask\n        else:\n            return grad\n\n\nclass Flatten(Layer):\n\n    def __init__(self, input_shape = None):\n        self.input_shape = input_shape\n        self.prev_shape  = None\n\n        self.is_trainable = True\n\n    @property\n    def trainable(self):\n        return self.is_trainable\n\n    @trainable.setter\n    def trainable(self, is_trainable):\n        self.is_trainable = is_trainable\n\n    @property\n    def output_shape(self):\n        return (np.prod(self.input_shape),)\n\n    def prep_layer(self): pass\n\n    def pass_forward(self, inputs, train_mode = True, **kwargs):\n        self.prev_shape = inputs.shape\n        return np.reshape(inputs, (inputs.shape[0], -1))\n\n    def pass_backward(self, grad, epoch_num, batch_num, batch_size):\n        return np.reshape(grad, self.prev_shape)\n\n\nclass UpSampling2D(Layer):\n\n    def __init__(self, size = (2, 2), input_shape = None):\n        self.h_scale, self.w_scale = size[0], size[1]\n        self.input_shape           = input_shape\n        self.prev_shape            = None\n\n        self.is_trainable = True\n\n    @property\n    def trainable(self):\n        return self.is_trainable\n\n    @trainable.setter\n    def trainable(self, is_trainable):\n        self.is_trainable = is_trainable\n\n    @property\n    def output_shape(self):\n        input_depth, input_height, input_width = self.input_shape\n        return input_depth, self.h_scale * input_height, self.w_scale * input_width\n\n    def prep_layer(self): pass\n\n    def pass_forward(self, inputs, train_mode = True, **kwargs):\n        self.prev_shape = inputs.shape\n        return np.repeat(np.repeat(inputs, self.h_scale, axis = 2), self.w_scale, axis = 3)\n\n    def pass_backward(self, grad, epoch_num, batch_num, batch_size):\n        grad = grad[:, :, ::self.h_scale, ::self.w_scale]\n        assert grad.shape == self.prev_shape, \'grad shape incorrect\'\n\n        return grad\n\n\nclass Reshape(Layer):\n\n    def __init__(self, target_shape, input_shape = None):\n        self.target_shape = target_shape\n        self.input_shape  = input_shape\n        self.prev_shape   = None\n\n        self.is_trainable = True\n\n    @property\n    def trainable(self):\n        return self.is_trainable\n\n    @trainable.setter\n    def trainable(self, is_trainable):\n        self.is_trainable = is_trainable\n\n    @property\n    def output_shape(self):\n        return self.target_shape\n\n    def prep_layer(self): pass\n\n    def pass_forward(self, inputs, train_mode = True, **kwargs):\n        self.prev_shape = inputs.shape\n        return np.reshape(inputs, (inputs.shape[0],) + self.target_shape)\n\n    def pass_backward(self, grad, epoch_num, batch_num, batch_size):\n        return np.reshape(grad, self.prev_shape)\n'"
ztlearn/dl/layers/embedding.py,6,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom .base import Layer\nfrom ztlearn.utils import one_hot\nfrom ztlearn.utils import get_sentence_tokens\nfrom ztlearn.initializers import InitializeWeights as init\nfrom ztlearn.optimizers import OptimizationFunction as optimizer\n\n\nclass Embedding(Layer):\n\n    def __init__(self,\n                       input_dim,                   # number of unique words in the text dataset\n                       output_dim,                  # size of the embedding vectors\n                       embeddings_init = 'uniform', # init type for the embedding matrix (weights)\n                       input_length    = 10):       # size of input sentences\n\n        self.input_dim    = input_dim\n        self.output_dim   = output_dim\n        self.input_length = input_length\n        self.input_shape  = None # required by the base class\n\n        self.init_method      = None\n        self.optimizer_kwargs = None\n\n        self.is_trainable = True\n\n    @property\n    def trainable(self):\n        return self.is_trainable\n\n    @trainable.setter\n    def trainable(self, is_trainable):\n        self.is_trainable = is_trainable\n\n    @property\n    def weight_initializer(self):\n        return self.init_method\n\n    @weight_initializer.setter\n    def weight_initializer(self, init_method):\n        self.init_method = init_method\n\n    @property\n    def weight_optimizer(self):\n        return self.optimizer_kwargs\n\n    @weight_optimizer.setter\n    def weight_optimizer(self, optimizer_kwargs = {}):\n        self.optimizer_kwargs = optimizer_kwargs\n\n    @property\n    def layer_parameters(self):\n        return sum([np.prod(param.shape) for param in [self.weights]])\n\n    @property\n    def output_shape(self):\n        return (self.input_length, self.output_dim)\n\n    def prep_layer(self):\n        self.uniques_one_hot = one_hot(np.arange(self.input_dim)) # master one hot matrix\n        self.kernel_shape    = (self.input_dim, self.output_dim)\n        self.weights         = init(self.weight_initializer).initialize_weights(self.kernel_shape) # embeddings\n\n    # inputs should be gotten from sentences_tokens = get_sentence_tokens(text_input)\n    def pass_forward(self, inputs, train_mode = True, **kwargs):\n        self.inputs = inputs # tokenized inputs\n\n        embeded_inputs = []\n        for _, tokens in enumerate(self.inputs.tolist()):\n\n            for i, word_index in enumerate(tokens):\n                embed     = np.expand_dims(self.uniques_one_hot[word_index,:], 1).T.dot(self.weights)\n                tokens[i] = list(np.array(embed).flat)\n\n            embeded_inputs.append(tokens)\n\n        return np.array(embeded_inputs)\n\n    def pass_backward(self, grad, epoch_num, batch_num, batch_size):\n        prev_weights = self.weights\n\n        if self.is_trainable:\n\n            dweights     = np.sum(grad @ self.weights.T, axis = 1)\n            self.weights = optimizer(self.weight_optimizer).update(self.weights, dweights.T, epoch_num, batch_num, batch_size)\n\n        # endif self.is_trainable\n\n        return grad @ prev_weights.T\n"""
ztlearn/dl/layers/normalization.py,19,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom .base import Layer\nfrom ztlearn.optimizers import OptimizationFunction as optimizer\n\n\nclass BatchNormalization(Layer):\n\n    """"""\n    **Batch Normalization**\n\n    Batch  Normalization seeks to reduce internal  covariate  shift by computing\n    the mean  and variance  used to be for  normalization from all of the summed\n    inputs to  the  neurons  in a  mini-batch on  a single training  case. Batch\n    Normalization enables  higher learning rates and also regularizes the model.\n\n    References:\n        [1] Layer Normalization\n            * [Sergey Ioffe, 2015] https://arxiv.org/abs/1502.03167\n            * [PDF] https://arxiv.org/pdf/1502.03167.pdf\n    """"""\n\n    def __init__(self, eps = 0.01, momentum = 0.99):\n        self.eps      = eps\n        self.momentum = momentum\n\n        self.running_var      = None\n        self.running_mean     = None\n        self.optimizer_kwargs = None\n\n        self.is_trainable = True\n\n    @property\n    def trainable(self):\n        return self.is_trainable\n\n    @trainable.setter\n    def trainable(self, is_trainable):\n        self.is_trainable = is_trainable\n\n    @property\n    def weight_optimizer(self):\n        return self.optimizer_kwargs\n\n    @weight_optimizer.setter\n    def weight_optimizer(self, optimizer_kwargs = {}):\n        self.optimizer_kwargs = optimizer_kwargs\n\n    @property\n    def layer_parameters(self):\n        return sum([np.prod(param.shape) for param in [self.gamma, self.beta]])\n\n    @property\n    def output_shape(self):\n        return self.input_shape\n\n    def prep_layer(self):\n        self.gamma = np.ones(self.input_shape)\n        self.beta  = np.zeros(self.input_shape)\n\n    def pass_forward(self, inputs, train_mode = True, **kwargs):\n        if self.running_var is None:\n            self.running_var = np.var(inputs, axis = 0)\n\n        if self.running_mean is None:\n            self.running_mean = np.mean(inputs, axis = 0)\n\n        if train_mode and self.is_trainable:\n            self.var  = np.var(inputs, axis = 0)\n            self.mean = np.mean(inputs, axis = 0)\n\n            self.running_var  = self.momentum * self.running_var  + (1 - self.momentum) * self.var\n            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n        else:\n            self.var  = self.running_var\n            self.mean = self.running_mean\n\n        self.input_mean = inputs - self.mean\n        self.inv_stddev = np.reciprocal(np.sqrt(self.var + self.eps))\n        self.input_norm = self.input_mean * self.inv_stddev\n\n        return self.gamma * self.input_norm + self.beta\n\n    def pass_backward(self, grad, epoch_num, batch_num, batch_size):\n        dinput_norm = grad * self.gamma\n\n        if self.is_trainable:\n\n            dbeta  = np.sum(grad, axis = 0)\n            dgamma = np.sum(grad * self.input_norm, axis = 0)\n\n            self.gamma = optimizer(self.weight_optimizer).update(self.gamma, dgamma, epoch_num, batch_num, batch_size)\n            self.beta  = optimizer(self.weight_optimizer).update(self.beta, dbeta, epoch_num, batch_num, batch_size)\n\n        # endif self.is_trainable\n\n        dinput = np.divide(1., grad.shape[0]) * self.inv_stddev * (grad.shape[0] * dinput_norm - np.sum(dinput_norm, axis = 0) - self.input_norm * np.sum(dinput_norm * self.input_norm, axis = 0))\n\n        return dinput\n\n\nclass LayerNormalization1D(Layer):\n\n    """"""\n    **Layer Normalization**\n\n    The Layer Normalization is a transpose of Batch Normalization which computes\n    the mean and variance used  for normalization from  all of the summed inputs\n    to the neurons in a layer on a single training case. Layer  normalization is\n    very effective at stabilizing the hidden state dynamics in recurrent networks\n\n    References:\n        [1] Layer Normalization\n            * [Jimmy Lei Ba, 2016] https://arxiv.org/abs/1607.06450\n            * [PDF] https://arxiv.org/pdf/1607.06450.pdf\n\n    """"""\n\n    def __init__(self, eps = 1e-5):\n        self.eps              = eps\n        self.optimizer_kwargs = None\n\n        self.is_trainable = True\n\n    @property\n    def trainable(self):\n        return self.is_trainable\n\n    @trainable.setter\n    def trainable(self, is_trainable):\n        self.is_trainable = is_trainable\n\n    @property\n    def weight_optimizer(self):\n        return self.optimizer_kwargs\n\n    @weight_optimizer.setter\n    def weight_optimizer(self, optimizer_kwargs = {}):\n        self.optimizer_kwargs = optimizer_kwargs\n\n    @property\n    def output_shape(self):\n        return self.input_shape\n\n    def prep_layer(self):\n        self.gamma = np.ones(self.input_shape) # * 0.2 (reduce gamma to 0.2 incase of NaNs)\n        self.beta  = np.zeros(self.input_shape)\n\n    def pass_forward(self, inputs, train_mode = True, **kwargs):\n        self.mean = np.mean(inputs, axis = -1, keepdims = True)\n        self.std  = np.std(inputs,  axis = -1, keepdims = True)\n\n        self.input_mean = inputs - self.mean\n        self.inv_stddev = np.reciprocal(self.std + self.eps)\n        self.input_norm = self.input_mean * self.inv_stddev\n\n        return self.input_norm * self.gamma + self.beta\n\n    def pass_backward(self, grad, epoch_num, batch_num, batch_size):\n        dinput_norm = grad * self.gamma\n\n        if self.is_trainable:\n\n            dbeta  = np.sum(grad, axis = 0)\n            dgamma = np.sum(grad * self.input_norm, axis = 0)\n\n            self.gamma = optimizer(self.weight_optimizer).update(self.gamma, dgamma, epoch_num, batch_num, batch_size)\n            self.beta  = optimizer(self.weight_optimizer).update(self.beta, dbeta, epoch_num, batch_num, batch_size)\n\n        # endif self.is_trainable\n\n        dinput = np.divide(1., grad.shape[0]) * self.inv_stddev * (grad.shape[0] * dinput_norm - np.sum(dinput_norm, axis = 0) - self.input_norm * np.sum(dinput_norm * self.input_norm, axis = 0))\n\n        return dinput\n'"
ztlearn/dl/layers/pooling.py,3,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom .base import Layer\nfrom ztlearn.utils import get_pad\nfrom ztlearn.utils import im2col_indices\nfrom ztlearn.utils import col2im_indices\nfrom ztlearn.utils import get_output_dims\n\n\nclass Pool(Layer):\n\n    def __init__(self, pool_size = (2, 2), strides = (1, 1), padding = 'valid'):\n        self.pool_size = pool_size\n        self.strides   = strides\n        self.padding   = padding\n\n        self.is_trainable = True\n\n    @property\n    def trainable(self):\n        return self.is_trainable\n\n    @trainable.setter\n    def trainable(self, is_trainable):\n        self.is_trainable = is_trainable\n\n    @property\n    def output_shape(self):\n        input_channels, input_height, input_width = self.input_shape\n\n        self.pad_height, self.pad_width = get_pad(self.padding,\n                                                                input_height,\n                                                                input_width,\n                                                                self.strides[0],\n                                                                self.strides[1],\n                                                                self.pool_size[0],\n                                                                self.pool_size[1])\n\n        output_height, output_width = get_output_dims(input_height, input_width, self.pool_size, self.strides, self.padding)\n\n        assert output_height % 1 == 0\n        assert output_width % 1  == 0\n\n        return input_channels, int(output_height), int(output_width)\n\n    def prep_layer(self): pass\n\n    def pass_forward(self, inputs, train_mode = True, **kwargs):\n        input_num, input_depth, input_height, input_width = inputs.shape\n        self.inputs = inputs\n\n        assert (input_height - self.pool_size[0]) % self.strides[0] == 0, 'Invalid height'\n        assert (input_width  - self.pool_size[1]) % self.strides[1] == 0, 'Invalid width'\n\n        output_height, output_width = get_output_dims(input_height, input_width, self.pool_size, self.strides)\n\n        input_reshaped = inputs.reshape(input_num * input_depth, 1, input_height, input_width)\n        self.input_col = im2col_indices(input_reshaped,\n                                                        self.pool_size[0],\n                                                        self.pool_size[1],\n                                                        padding = (self.pad_height, self.pad_width),\n                                                        stride  = self.strides[0])\n\n        output, self.pool_cache = self.pool_forward(self.input_col)\n\n        output = output.reshape(int(output_height), int(output_width), input_num, input_depth)\n\n        return output.transpose(2, 3, 0, 1)\n\n    def pass_backward(self, grad, epoch_num, batch_num, batch_size):\n        input_num, input_depth, input_height, input_width = self.inputs.shape\n\n        d_input_col = np.zeros_like(self.input_col)\n        grad_col    = grad.transpose(2, 3, 0, 1).ravel()\n\n        d_input_col = self.pool_backward(d_input_col, grad_col, self.pool_cache)\n        d_input     = col2im_indices(d_input_col,\n                                                  (input_num * input_depth, 1, input_height, input_width),\n                                                  self.pool_size[0],\n                                                  self.pool_size[1],\n                                                  padding = (self.pad_height, self.pad_width),\n                                                  stride  = self.strides[0])\n\n        return d_input.reshape(self.inputs.shape)\n\n\nclass MaxPooling2D(Pool):\n\n    def __init__(self, pool_size = (2, 2), strides = (1, 1), padding = 'valid'):\n        super(MaxPooling2D, self).__init__(pool_size, strides, padding)\n\n    def pool_forward(self, input_col):\n        max_id = np.argmax(input_col, axis = 0)\n        out    = input_col[max_id, range(max_id.size)]\n\n        return out, max_id\n\n    def pool_backward(self, d_input_col, grad_col, pool_cache):\n        d_input_col[pool_cache, range(grad_col.size)] = grad_col\n\n        return d_input_col\n\n\nclass AveragePool2D(Pool):\n\n    def __init__(self, pool_size = (2, 2), strides = (1, 1), padding = 'valid'):\n        super(AveragePool2D, self).__init__(pool_size, strides, padding)\n\n    def pool_forward(self, input_col):\n        out = np.mean(input_col, axis = 0)\n\n        return out, None\n\n    def pool_backward(self, d_input_col, grad_col, pool_cache = None):\n        d_input_col[:, range(grad_col.size)] = 1. / d_input_col.shape[0] * grad_col\n\n        return d_input_col\n"""
ztlearn/dl/models/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import file\nfrom . import sequential\n\n# sequential\nfrom .sequential import Sequential\n'
ztlearn/dl/models/sequential.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom .trainer import Trainer\nfrom ztlearn.utils import print_pad\nfrom ztlearn.utils import custom_tuple\nfrom ztlearn.dl.layers import Activation\n\n\nclass Sequential(Trainer):\n\n    __slots__ = [\'layers\', \'layer_num\', \'init_method\', \'model_name\', \'is_trainable\']\n\n    def __init__(self, init_method = \'he_normal\', model_name = \'ztlearn_model\'):\n        self.layers      = []\n        self.layer_num   = 0\n        self.model_name  = model_name\n        self.init_method = init_method\n\n        self.is_trainable = True\n\n    @property\n    def trainable(self):\n        return self.is_trainable\n\n    @trainable.setter\n    def trainable(self, is_trainable):\n        self.is_trainable = is_trainable\n        for layer in self.layers:\n            layer.trainable = self.is_trainable\n\n    @property\n    def added_layers(self):\n        return self.layers\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        try:\n            collection = self.layers[self.layer_num]\n        except IndexError:\n            raise StopIteration\n        self.layer_num += 1\n\n        return collection\n\n    def __str__(self):\n        layer_names, layer_params, layer_output = [\'LAYER TYPE\'], [\'PARAMS\'], [\'OUTPUT SHAPE\']\n\n        for _, layer in enumerate(self.layers):\n            layer_names.append(layer.layer_name)\n            layer_output.append(custom_tuple(layer.output_shape))\n            layer_params.append(""{:,}"".format(layer.layer_parameters))\n\n        max_name   = len(max(layer_names,  key = len))\n        max_params = len(max(layer_params, key = len))\n        max_output = len(max(layer_output, key = len))\n\n        liner  = [max_name, max_params, max_output]\n        lining = """"\n        for col_size in liner:\n            lining += ""+"" + (""-"" * (col_size + 2))\n        lining += ""+""\n\n        total_params  = 0\n        model_layers  = print_pad(1)+ "" "" + self.model_name.upper() + print_pad(1)\n        model_layers += print_pad(1)+ "" Input Shape: "" + str(self.layers[0].input_shape) + print_pad(1)\n        for i, layer in enumerate(layer_names, start = 0): # loop through layer_names (has same size as self.layers + column names)\n            if i < 2:\n                model_layers += lining + print_pad(1)\n            model_layers += ""\xc2\xa6 {:<{max_name}} \xc2\xa6 {:>{max_params}} \xc2\xa6 {:>{max_output}} \xc2\xa6 "".format(layer_names[i],\n                                                                                               layer_params[i],\n                                                                                               layer_output[i],\n                                                                                               max_name   = max_name,\n                                                                                               max_params = max_params,\n                                                                                               max_output = max_output)\n            model_layers += print_pad(1)\n\n            if i > 0:\n                total_params += int(layer_params[i].replace(\',\', \'\'))\n\n        model_layers += lining + print_pad(1)\n        model_layers += print_pad(1) + "" TOTAL PARAMETERS: "" + ""{:,}"".format(total_params) + print_pad(1)\n\n        return model_layers\n\n    def summary(self, model_name = \'ztlearn_model\'):\n        self.model_name = model_name\n        print(self.__str__())\n\n    def add(self, layer):\n        if self.layers:\n            layer.input_shape = self.layers[-1].output_shape\n\n        if hasattr(layer, \'weight_initializer\'):\n            layer.weight_initializer = self.init_method\n        self.append_layer(layer)\n\n        if hasattr(layer, \'layer_activation\') and layer.layer_activation is not None:\n            self.append_layer(Activation(layer.layer_activation, input_shape = self.layers[-1].output_shape))\n\n    def append_layer(self, layer):\n        layer.prep_layer()\n        self.layers.append(layer)\n\n    def compile(self, loss = \'categorical_crossentropy\', optimizer = {}):\n        self.loss = loss\n        for layer in self.layers:\n            if hasattr(layer, \'weight_optimizer\'):\n                layer.weight_optimizer = optimizer\n\n    def foward_pass(self, inputs, train_mode = False):\n        layer_output = inputs\n        for layer in self.layers:\n            layer_output = layer.pass_forward(layer_output, train_mode)\n        return layer_output\n\n    def backward_pass(self, loss_grad, epoch_num, batch_num, batch_size):\n        for layer in reversed(self.layers):\n            loss_grad = layer.pass_backward(loss_grad, epoch_num, batch_num, batch_size)\n'"
ztlearn/dl/models/trainer.py,7,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import LogIfBusy\nfrom ztlearn.utils import computebar\nfrom ztlearn.utils import minibatches\nfrom ztlearn.objectives import ObjectiveFunction as objective\n\n\nclass Trainer:\n\n    def __init__(self):\n        self.loss = 'categorical_crossentropy' # default objective\n\n    @LogIfBusy\n    def fit(self,\n                  train_data,\n                  train_label,\n                  batch_size,\n                  epochs,\n                  validation_data = (),\n                  shuffle_data    = True,\n                  verbose         = False):\n\n        fit_stats = {'train_loss': [], 'train_acc': [], 'valid_loss': [], 'valid_acc': []}\n\n        batch_num = 0\n        for epoch_idx in np.arange(epochs):\n            batch_stats = {'batch_loss': [], 'batch_acc': []}\n\n            for train_batch_data, train_batch_label in minibatches(train_data, train_label, batch_size, shuffle_data):\n                batch_num += 1\n                loss, acc  = self.train_on_batch(train_batch_data,\n                                                                   train_batch_label,\n                                                                   epoch_num  = epoch_idx,\n                                                                   batch_num  = batch_num,\n                                                                   batch_size = batch_size)\n\n                batch_stats['batch_loss'].append(loss)\n                batch_stats['batch_acc'].append(acc)\n\n                if verbose:\n                    print('TRAINING: Epoch-{} loss: {:2.4f} accuracy: {:2.4f}'.format(epoch_idx+1, loss, acc))\n\n            fit_stats['train_loss'].append(np.mean(batch_stats['batch_loss']))\n            fit_stats['train_acc'].append(np.mean(batch_stats['batch_acc']))\n\n            if validation_data:\n                val_loss, val_acc = self.test_on_batch(validation_data[0], validation_data[1])\n\n                fit_stats['valid_loss'].append(val_loss)\n                fit_stats['valid_acc'].append(val_acc)\n\n                if verbose:\n                    print('VALIDATION: Epoch-{} loss: {:2.4f} accuracy: {:2.4f}'.format(epoch_idx+1, val_loss, val_acc))\n\n            if not verbose:\n                computebar(epochs, epoch_idx)\n\n        return fit_stats\n\n    def train_on_batch(self,\n                              train_batch_data,\n                              train_batch_label,\n                              epoch_num  = 0,\n                              batch_num  = 0,\n                              batch_size = 1):\n\n        predictions = self.foward_pass(train_batch_data, train_mode = True)\n\n        loss = np.mean(objective(self.loss).forward(predictions, train_batch_label))\n        acc  = objective(self.loss).accuracy(predictions, train_batch_label)\n\n        self.backward_pass(objective(self.loss).backward(predictions, train_batch_label),\n                                                                                          epoch_num,\n                                                                                          batch_num,\n                                                                                          batch_size)\n\n        return loss, acc\n\n    def test_on_batch(self,\n                            test_batch_data,\n                            test_batch_label,\n                            train_mode = False):\n\n        predictions = self.foward_pass(test_batch_data, train_mode = train_mode)\n\n        loss = np.mean(objective(self.loss).forward(predictions, test_batch_label))\n        acc  = objective(self.loss).accuracy(predictions, test_batch_label)\n\n        return loss, acc\n\n    @LogIfBusy\n    def evaluate(self,\n                       test_data,\n                       test_label,\n                       batch_size   = 128,\n                       shuffle_data = True,\n                       verbose      = False):\n                       \n        eval_stats = {'valid_batches' : 0, 'valid_loss': [], 'valid_acc': []}\n\n        batches = minibatches(test_data, test_label, batch_size, shuffle_data)\n        eval_stats['valid_batches'] = len(batches)\n\n        for idx, (test_data_batch_data, test_batch_label) in enumerate(batches):\n            loss, acc = self.test_on_batch(test_data_batch_data, test_batch_label)\n\n            eval_stats['valid_loss'].append(np.mean(loss))\n            eval_stats['valid_acc'].append(np.mean(acc))\n\n            if verbose:\n                print('VALIDATION: loss: {:2.4f} accuracy: {:2.4f}'.format(eval_stats['valid_loss'], eval_stats['valid_acc']))\n            else:\n                computebar(eval_stats['valid_batches'], idx)\n\n        return eval_stats\n\n    def predict(self, sample_input, train_mode = False):\n        return self.foward_pass(sample_input, train_mode = train_mode)\n"""
ztlearn/ml/classification/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import file(s)\nfrom . import perceptron\n\n# perceptron model\nfrom .perceptron import Perceptron'
ztlearn/ml/classification/knn.py,0,b'\n\nclass KNN:\n\n    def __init__(): pass\n'
ztlearn/ml/classification/perceptron.py,4,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import LogIfBusy\nfrom ztlearn.utils import computebar\nfrom ztlearn.initializers import InitializeWeights as init\nfrom ztlearn.objectives import ObjectiveFunction as objective\nfrom ztlearn.activations import ActivationFunction as activate\nfrom ztlearn.optimizers import OptimizationFunction as optimize\nfrom ztlearn.regularizers import RegularizationFunction as regularize\n\n\nclass Perceptron:\n\n    def __init__(self,\n                       epochs,\n                       activation     = \'sigmoid\',\n                       loss           = \'categorical_crossentropy\',\n                       init_method    = \'he_normal\',\n                       optimizer      = {},\n                       penalty        = \'lasso\',\n                       penalty_weight = 0,\n                       l1_ratio       = 0.5):\n\n        self.epochs         = epochs\n        self.activate       = activate(activation)\n        self.loss           = objective(loss)\n        self.init_method    = init(init_method)\n        self.optimizer      = optimizer\n        self.regularization = regularize(penalty, penalty_weight, l1_ratio = l1_ratio)\n\n    @LogIfBusy\n    def fit(self, inputs, targets, verbose = False):\n        fit_stats = {""train_loss"": [], ""train_acc"": [], ""valid_loss"": [], ""valid_acc"": []}\n\n        self.weights = self.init_method.initialize_weights((inputs.shape[1], targets.shape[1]))\n        self.bias    = np.zeros((1, targets.shape[1]))\n\n        for i in range(self.epochs):\n            linear_predictions = inputs.dot(self.weights) + self.bias\n            predictions        = self.activate.forward(linear_predictions)\n\n            loss = self.loss.forward(predictions, targets) + self.regularization.regulate(self.weights)\n            acc  = self.loss.accuracy(predictions, targets)\n\n            fit_stats[""train_loss""].append(np.mean(loss))\n            fit_stats[""train_acc""].append(np.mean(acc))\n\n            grad      = self.loss.backward(predictions, targets) * self.activate.backward(linear_predictions)\n            d_weights = inputs.T.dot(grad) + self.regularization.derivative(self.weights)\n            d_bias    = np.sum(grad, axis = 0, keepdims = True) + self.regularization.derivative(self.bias)\n\n            self.weights = optimize(self.optimizer).update(self.weights, d_weights, i, 1, 1)\n            self.bias    = optimize(self.optimizer).update(self.bias, d_bias, i, 1, 1)\n\n            if verbose:\n                print(\'TRAINING: Epoch-{} loss: {:2.4f} acc: {:2.4f}\'.format(i+1, loss, acc))\n            else:\n                computebar(self.epochs, i)\n\n        return fit_stats\n\n    def predict(self, inputs):\n        # return self.activate.forward(inputs.dot(self.weights) + self.bias)\n        return inputs.dot(self.weights) + self.bias\n\n    @property\n    def model_weights(self):\n        return self.weights\n'"
ztlearn/ml/clustering/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import file(s)\nfrom . import kmeans\n\n# K-Means model\nfrom .kmeans import KMeans\n\n'
ztlearn/ml/clustering/kmeans.py,13,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\n# implementation based on techniques as seen here: https://github.com/goldsborough/k-means/blob/master/python/k_means.py\nclass KMeans:\n\n    def __init__(self, n_clusters = 2, max_iter = 300, random_state = None):\n        self.n_clusters   = n_clusters\n        self.max_iter     = max_iter\n        self.random_state = random_state\n\n    def _initialize_centroids(self, inputs):\n        self.n_samples, self.n_features = np.shape(inputs)\n\n        if self.random_state is not None:\n            np.random.seed(self.random_state)\n\n        # get random indices for centroid and use them to initialize centroids\n        centroid_indices = np.random.choice(range(self.n_samples), self.n_clusters)\n        self.centroids   = inputs[centroid_indices].T\n\n        # stack inputs to form a tensor of dim [self.n_samples, self.n_features, self.n_clusters]\n        self.stacked_inputs = np.stack([inputs] * self.n_clusters, axis = -1)\n\n        self.all_rows    = np.arange(self.n_samples) # get all row indices in an np array\n        self.sparse_data = np.zeros([self.n_samples, self.n_clusters, self.n_features])\n\n    def fit(self, inputs):\n\n        self._initialize_centroids(inputs) # initialize the centroids randomly\n\n        for i in range(self.max_iter):\n\n            # calculate distances of data_points from centroids\n            # returns a distance tensor of dim [no of data_points, distance_from_each_centroid]\n            distances = np.linalg.norm(self.stacked_inputs - self.centroids, axis = 1)\n\n            # given n centroids the 'distance_from_each_centroid' metric consists of n diffrent distances to the n centroids\n            # to find the minimum distance amongst the n diffrent distance we use the np.argmin function on this axis.\n            # closest_centroid = np.argmin(distances, axis = -1). closest_centroid is a tensor of dim [no of data_points]\n            # closest_centroid tensor consists of a collection of the indices of closest centroids.\n\n            # for each data_point, the [row number, closest_centroid] index is its position in the sparse_data tensor\n            # this operation fills in the sparse_data tensor positions at [all_rows, closest_centroid] with data from the inputs\n            self.sparse_data[self.all_rows, np.argmin(distances, axis = -1)] = inputs\n\n            # save current centroids for model convergence check\n            prior_centroids = self.centroids\n\n            # calculate the mean of all the newly formed clusters\n            # get the sum of elements on the first axis (i.e axis = 0)\n            # divide by the count of non zero elements in the sparse_data tensor on the first axis (i.e axis = 0)\n            # also clip at a minimum of 1 to avoid division by zero\n            self.centroids = np.divide(np.sum(self.sparse_data, axis = 0),\n                                       np.clip(np.count_nonzero(self.sparse_data, axis = 0), a_min = 1, a_max = None)).T\n\n            # determine if current centroids are diffrent with prior centroids. break if not\n            if not np.any(self.centroids - prior_centroids):\n                break\n\n        return self.centroids.T\n"""
ztlearn/ml/decomposition/__init__.py,0,b'# import file(s)\nfrom . import pca\n\n# pca model\nfrom .pca import PCA\n'
ztlearn/ml/decomposition/pca.py,13,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\n\nclass PCA(object):\n\n    def __init__(self, n_components = 2):\n        self.n_components = n_components\n\n    def fit(self, inputs):\n        self.n_samples, self.n_features = np.shape(inputs)\n\n        self.mean = np.mean(inputs, axis = 0)\n        self.standardized_inputs = np.subtract(inputs, self.mean)\n\n        self.U, self.S, self.V = np.linalg.svd(self.standardized_inputs, full_matrices = False)\n\n        self.components          = self.V[:self.n_components]\n        components_variance      = np.divide(np.square(self.S), (self.n_samples - 1))\n        self.components_variance = components_variance[:self.n_components]\n\n        return self\n\n    def fit_transform(self, inputs):\n        self.n_samples, self.n_features = np.shape(inputs)\n\n        self.mean = np.mean(inputs, axis = 0)\n        self.standardized_inputs = np.subtract(inputs, self.mean)\n\n        U, S, V = np.linalg.svd(self.standardized_inputs, full_matrices = False)\n\n        self.components    = V[:self.n_components]\n        transformed_inputs = np.multiply(U[:, :self.n_components],\n                                         S[:self.n_components])\n\n        components_variance      = np.divide(np.square(S), (self.n_samples - 1))\n        self.components_variance = components_variance[:self.n_components]\n\n        return transformed_inputs\n\n    @property\n    def transform(self):\n        transformed_inputs = np.multiply(self.U[:, :self.n_components],\n                                         self.S[:self.n_components])\n\n        return transformed_inputs\n\n    def inverse_transform(self, transformed_inputs):\n        return np.dot(transformed_inputs, self.components) + self.mean\n'"
ztlearn/ml/regression/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import file(s)\nfrom . import base\nfrom . import linear\nfrom . import logistic\nfrom . import polynomial\nfrom . import elasticnet\n\n# base model\nfrom .base import Regression\n\n# linear model\nfrom .linear import LinearRegression\n\n# logistic model\nfrom .logistic import LogisticRegression\n\n# polynomial model\nfrom .polynomial import PolynomialRegression\n\n# elastic net model\nfrom .elasticnet import ElasticNetRegression\n'
ztlearn/ml/regression/base.py,7,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import LogIfBusy\nfrom ztlearn.utils import computebar\nfrom ztlearn.initializers import InitializeWeights as init\nfrom ztlearn.objectives import ObjectiveFunction as objective\nfrom ztlearn.optimizers import OptimizationFunction as optimize\nfrom ztlearn.regularizers import RegularizationFunction as regularize\n\n\nclass Regression(object):\n\n    def __init__(self,\n                       epochs,\n                       loss           = \'mean_squared_error\',\n                       init_method    = \'he_uniform\',\n                       optimizer      = {},\n                       penalty        = \'ridge\',\n                       penalty_weight = 0.5,\n                       l1_ratio       = 0.5):\n\n        self.epochs         = epochs\n        self.loss           = objective(loss)\n        self.init_method    = init(init_method)\n        self.optimizer      = optimize(optimizer)\n        self.regularization = regularize(penalty, penalty_weight, l1_ratio = l1_ratio)\n\n    @LogIfBusy\n    def fit(self, inputs, targets, verbose = False):\n        fit_stats    = {""train_loss"": [], ""train_acc"": [], ""valid_loss"": [], ""valid_acc"": []}\n        inputs       = np.column_stack((np.ones(inputs.shape[0]), inputs))\n        self.weights = self.init_method.initialize_weights((inputs.shape[1], ))\n\n        for i in range(self.epochs):\n            predictions = inputs.dot(self.weights)\n            mse         = np.sum(\n                             (\n                                self.loss.forward(\n                                     np.expand_dims(predictions, axis = 1),\n                                     np.expand_dims(targets, axis = 1)\n                                ),\n                                self.regularization.regulate(self.weights)\n                             )\n                          )\n            acc         = self.loss.accuracy(predictions, targets)\n\n            fit_stats[""train_loss""].append(np.mean(mse))\n            fit_stats[""train_acc""].append(np.mean(acc))\n\n            cost_gradient = self.loss.backward(predictions, targets)\n            d_weights     = cost_gradient.dot(inputs) + self.regularization.derivative(self.weights)\n            self.weights  = self.optimizer.update(self.weights, d_weights, i, 1, 1)\n\n            if verbose:\n                print(\'TRAINING: Epoch-{} loss: {:2.4f} acc: {:2.4f}\'.format(i+1, mse, acc))\n            else:\n                computebar(self.epochs, i)\n\n        return fit_stats\n\n    def predict(self, inputs):\n        inputs = np.column_stack((np.ones(inputs.shape[0]), inputs))\n\n        return inputs.dot(self.weights)\n'"
ztlearn/ml/regression/elasticnet.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom .base import Regression\nfrom ztlearn.utils import normalize\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\nclass ElasticNetRegression(Regression):\n\n    def __init__(self,\n                       degree         = 2,\n                       epochs         = 100,\n                       loss           = 'mean_squared_error',\n                       init_method    = 'random_normal',\n                       optimizer      = {},\n                       penalty        = 'elastic',\n                       penalty_weight = 0.5,\n                       l1_ratio       = 0.5):\n\n        self.degree = degree\n        super(ElasticNetRegression, self).__init__(epochs         = epochs,\n                                                   loss           = loss,\n                                                   init_method    = init_method,\n                                                   optimizer      = optimizer,\n                                                   penalty        = 'elastic', # force to elastic\n                                                   penalty_weight = penalty_weight,\n                                                   l1_ratio       = l1_ratio)\n\n    def fit(self, inputs, targets, verbose = False, normalized = True):\n        polynomial_inputs = PolynomialFeatures(degree = self.degree).fit_transform(inputs)\n\n        if normalized:\n            polynomial_inputs = normalize(polynomial_inputs)\n\n        fit_stats = super(ElasticNetRegression, self).fit(polynomial_inputs, targets, verbose)\n\n        return fit_stats\n\n    def predict(self, inputs, normalized = True):\n        polynomial_inputs = PolynomialFeatures(degree = self.degree).fit_transform(inputs)\n\n        if normalized:\n            polynomial_inputs = normalize(polynomial_inputs)\n\n        return super(ElasticNetRegression, self).predict(polynomial_inputs)\n"""
ztlearn/ml/regression/linear.py,2,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom .base import Regression\nfrom ztlearn.utils import LogIfBusy\n\n\nclass LinearRegression(Regression):\n\n    def __init__(self,\n                       epochs         = 100,\n                       loss           = \'mean_squared_error\',\n                       init_method    = \'random_normal\',\n                       optimizer      = {},\n                       penalty        = \'ridge\',\n                       penalty_weight = 0.5,\n                       l1_ratio       = 0.5):\n\n        super(LinearRegression, self).__init__(epochs         = epochs,\n                                               loss           = loss,\n                                               init_method    = init_method,\n                                               optimizer      = optimizer,\n                                               penalty        = penalty,\n                                               penalty_weight = penalty_weight,\n                                               l1_ratio       = l1_ratio)\n\n    def fit(self, inputs, targets, verbose =  False):\n        fit_stats = super(LinearRegression, self).fit(inputs, targets, verbose)\n\n        return fit_stats\n\n    @LogIfBusy\n    def fit_OLS(self, inputs, targets, verbose = True):\n        fit_stats    = {""train_loss"": [], ""train_acc"": [], ""valid_loss"": [], ""valid_acc"": []}\n        inputs       = np.column_stack((np.ones(inputs.shape[0]), inputs))\n        self.weights = np.linalg.inv(inputs.T.dot(inputs)).dot(inputs.T).dot(targets)\n\n        return fit_stats\n'"
ztlearn/ml/regression/logistic.py,13,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ztlearn.utils import LogIfBusy\nfrom ztlearn.utils import computebar\nfrom ztlearn.initializers import InitializeWeights as init\nfrom ztlearn.objectives import ObjectiveFunction as objective\nfrom ztlearn.optimizers import OptimizationFunction as optimize\nfrom ztlearn.activations import ActivationFunction as activation\nfrom ztlearn.regularizers import RegularizationFunction as regularize\n\n\nclass LogisticRegression:\n\n    def __init__(self,\n                       epochs,\n                       loss           = \'binary_crossentropy\',\n                       init_method    = \'he_normal\',\n                       optimizer      = {},\n                       penalty        = \'lasso\',\n                       penalty_weight = 0,\n                       l1_ratio       = 0.5):\n\n        self.epochs         = epochs\n        self.loss           = objective(loss)\n        self.init_method    = init(init_method)\n        self.optimizer      = optimize(optimizer)\n        self.activate       = activation(\'sigmoid\')\n        self.regularization = regularize(penalty, penalty_weight, l1_ratio = l1_ratio)\n\n    @LogIfBusy\n    def fit(self, inputs, targets, verbose = False):\n        fit_stats    = {""train_loss"": [], ""train_acc"": [], ""valid_loss"": [], ""valid_acc"": []}\n        self.weights = self.init_method.initialize_weights((inputs.shape[1], ))\n\n        for i in range(self.epochs):\n            predictions = self.activate.forward(inputs.dot(self.weights))\n            cost        = np.sum(\n                             (\n                                self.loss.forward(\n                                     np.expand_dims(predictions, axis = 1),\n                                     np.expand_dims(targets, axis = 1)\n                                ),\n                                self.regularization.regulate(self.weights)\n                             )\n                          )\n            acc         = self.loss.accuracy(predictions, targets)\n\n            fit_stats[""train_loss""].append(np.mean(cost))\n            fit_stats[""train_acc""].append(np.mean(acc))\n\n            cost_gradient = self.loss.backward(predictions, targets)\n            d_weights     = inputs.T.dot(cost_gradient) + self.regularization.derivative(self.weights)\n            self.weights  = self.optimizer.update(self.weights, d_weights, i, 1, 1)\n\n            if verbose:\n                print(\'TRAINING: Epoch-{} loss: {:.2f} acc: {:.2f}\'.format(i+1, cost, acc))\n            else:\n                computebar(self.epochs, i)\n\n        return fit_stats\n\n    @LogIfBusy\n    def fit_NR(self, inputs, targets, verbose = False):\n        \'\'\' Newton-Raphson Method \'\'\'\n        fit_stats = {""train_loss"": [], ""train_acc"": [], ""valid_loss"": [], ""valid_acc"": []}\n        self.weights = self.init_method.initialize_weights((inputs.shape[1], ))\n\n        for i in range(self.epochs):\n            predictions = self.activate.forward(inputs.dot(self.weights))\n            cost        = np.sum(\n                            (\n                                self.loss.forward(\n                                     np.expand_dims(predictions, axis = 1),\n                                     np.expand_dims(targets, axis = 1)\n                                ),\n                                self.regularization.regulate(self.weights)\n                            )\n                          )\n            acc         = self.loss.accuracy(predictions, targets)\n\n            fit_stats[""train_loss""].append(np.mean(cost))\n            fit_stats[""train_acc""].append(np.mean(acc))\n\n            diag_grad     = np.diag(self.activate.backward(inputs.dot(self.weights)))\n            self.weights += np.linalg.pinv(inputs.T.dot(diag_grad).dot(inputs) +\n                            self.regularization.derivative(self.weights)).dot(inputs.T.dot(diag_grad)).dot((targets - predictions))\n\n            if verbose:\n                print(\'TRAINING: Epoch-{} loss: {:2.4f} acc: {:2.4f}\'.format(i+1, cost, acc))\n            else:\n                computebar(self.epochs, i)\n\n        return fit_stats\n\n    def predict(self, inputs):\n        return np.round(self.activate.forward(inputs.dot(self.weights))).astype(int)\n'"
ztlearn/ml/regression/polynomial.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom .base import Regression\nfrom ztlearn.utils import normalize\nfrom ztlearn.utils import polynomial_features\n\n\nclass PolynomialRegression(Regression):\n\n    def __init__(self,\n                       degree         = 2,\n                       epochs         = 100,\n                       loss           = 'mean_squared_error',\n                       init_method    = 'random_normal',\n                       optimizer      = {},\n                       penalty        = 'ridge',\n                       penalty_weight = 0.5,\n                       l1_ratio       = 0.5):\n\n        self.degree = degree\n        super(PolynomialRegression, self).__init__(epochs         = epochs,\n                                                   loss           = loss,\n                                                   init_method    = init_method,\n                                                   optimizer      = optimizer,\n                                                   penalty        = penalty,\n                                                   penalty_weight = penalty_weight,\n                                                   l1_ratio       = l1_ratio)\n\n    def fit(self, inputs, targets, verbose = False, normalized = True):\n        polynomial_inputs = polynomial_features(inputs, degree = self.degree)\n\n        if normalized:\n            polynomial_inputs = normalize(polynomial_inputs)\n\n        fit_stats = super(PolynomialRegression, self).fit(polynomial_inputs, targets, verbose)\n\n        return fit_stats\n\n    def predict(self, inputs, normalized = True):\n        polynomial_inputs = polynomial_features(inputs, degree = self.degree)\n\n        if normalized:\n            polynomial_inputs = normalize(polynomial_inputs)\n\n        return super(PolynomialRegression, self).predict(polynomial_inputs)\n"""
ztlearn/datasets/cifar/cifar_10/__init__.py,0,b''
ztlearn/datasets/cifar/cifar_10/cifar_10.py,6,"b""import os\nimport gzip\nimport numpy as np\nfrom six.moves import cPickle\n\nfrom ztlearn.utils import extract_files\nfrom ztlearn.utils import maybe_download\nfrom ztlearn.utils import train_test_split\nfrom ztlearn.datasets.data_set import DataSet\n\nURL = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n\nCIFAR_10_BASE_PATH      = '/../../ztlearn/datasets/cifar/cifar_10'\nCIFAR_10_BATCHES_FOLDER = 'cifar-10-batches-py'\n\ntrain_files = [\n    'data_batch_1',\n    'data_batch_2',\n    'data_batch_3',\n    'data_batch_4',\n    'data_batch_5'\n]\ntest_files = ['test_batch']\n\n\ndef fetch_cifar_10(data_target = True, custom_path = os.getcwd()):\n    extract_files(custom_path + CIFAR_10_BASE_PATH, maybe_download(custom_path + CIFAR_10_BASE_PATH, URL))\n\n    for train_file in train_files:\n        if not os.path.exists(os.path.join(custom_path + CIFAR_10_BASE_PATH, CIFAR_10_BATCHES_FOLDER, train_file)):\n            raise FileNotFoundError('{} File Not Found'.format(train_file)) # dont continue\n\n    train_data  = np.zeros((50000, 3, 32, 32), dtype = 'uint8')\n    train_label = np.zeros((50000,), dtype = 'uint8')\n    for idx, train_file in enumerate(train_files):\n\n        with open(os.path.join(custom_path + CIFAR_10_BASE_PATH, CIFAR_10_BATCHES_FOLDER, train_file),'rb') as file:\n            data        = cPickle.load(file, encoding = 'latin1')\n            batch_data  = data['data'].reshape((-1, 3, 32, 32)).astype('uint8')\n            batch_label = np.reshape(data['labels'], len(data['labels'],))\n\n        train_data[idx * 10000: (idx + 1) * 10000, ...] = batch_data\n        train_label[idx * 10000: (idx + 1) * 10000]     = batch_label\n\n    with open(os.path.join(custom_path + CIFAR_10_BASE_PATH, CIFAR_10_BATCHES_FOLDER, test_files[0]),'rb') as file:\n        data       = cPickle.load(file, encoding = 'latin1')\n        test_data  = data['data'].reshape((-1, 3, 32, 32)).astype('uint8')\n        test_label = np.reshape(data['labels'], len(data['labels'],))\n\n    if data_target:\n        return DataSet(np.concatenate((train_data,  test_data),  axis = 0),\n                       np.concatenate((train_label, test_label), axis = 0))\n    else:\n        return train_data, test_data, train_label, test_label\n"""
ztlearn/datasets/cifar/cifar_100/__init__.py,0,b''
ztlearn/datasets/cifar/cifar_100/cifar_100.py,6,"b""import os\nimport gzip\nimport numpy as np\nfrom six.moves import cPickle\n\nfrom ztlearn.utils import extract_files\nfrom ztlearn.utils import maybe_download\nfrom ztlearn.utils import train_test_split\nfrom ztlearn.datasets.data_set import DataSet\n\nURL = 'http://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'\n\nCIFAR_100_BASE_PATH      = '/../../ztlearn/datasets/cifar/cifar_100'\nCIFAR_100_BATCHES_FOLDER = 'cifar-100-python'\n\ntrain_files = ['train']\ntest_files  = ['test']\n\ndef fetch_cifar_100(data_target = True, custom_path = os.getcwd()):\n    extract_files(custom_path + CIFAR_100_BASE_PATH, maybe_download(custom_path + CIFAR_100_BASE_PATH, URL))\n\n    if not os.path.exists(os.path.join(custom_path + CIFAR_100_BASE_PATH, CIFAR_100_BATCHES_FOLDER, train_files[0])):\n        raise FileNotFoundError('{} File Not Found'.format(train_files[0])) # dont continue\n\n    if not os.path.exists(os.path.join(custom_path + CIFAR_100_BASE_PATH, CIFAR_100_BATCHES_FOLDER, test_files[0])):\n        raise FileNotFoundError('{} File Not Found'.format(test_files[0])) # dont continue\n\n    with open(os.path.join(custom_path + CIFAR_100_BASE_PATH, CIFAR_100_BATCHES_FOLDER, train_files[0]),'rb') as file:\n        data        = cPickle.load(file, encoding = 'latin1')\n        train_data  = np.reshape(data['data'], (data['data'].shape[0], 3, 32, 32))\n        train_label = np.reshape(data['fine_labels'], len(data['fine_labels'],))\n\n    with open(os.path.join(custom_path + CIFAR_100_BASE_PATH, CIFAR_100_BATCHES_FOLDER, test_files[0]),'rb') as file:\n        data       = cPickle.load(file, encoding = 'latin1')\n        test_data  = np.reshape(data['data'], (data['data'].shape[0], 3, 32, 32))\n        test_label = np.reshape(data['fine_labels'], len(data['fine_labels'],))\n\n    if data_target:\n        return DataSet(np.concatenate((train_data,  test_data),  axis = 0),\n                       np.concatenate((train_label, test_label), axis = 0))\n    else:\n        return train_data, test_data, train_label, test_label\n"""
ztlearn/dl/layers/recurrent/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# import file(s)\nfrom . import rnn\nfrom . import gru\nfrom . import lstm\n\n# RNN layer(s)\nfrom .rnn import RNN\n\n# GRU layer(s)\nfrom .gru import GRU\n\n# LSTM layer(s)\nfrom .lstm import LSTM\n'
ztlearn/dl/layers/recurrent/gru.py,53,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ..base import Layer\nfrom ztlearn.utils import clip_gradients as cg\nfrom ztlearn.initializers import InitializeWeights as init\nfrom ztlearn.activations import ActivationFunction as activate\nfrom ztlearn.optimizers import OptimizationFunction as optimizer\n\n\nclass GRU(Layer):\n\n    def __init__(self, h_units, activation = None, input_shape = None, gate_activation = 'sigmoid'):\n        self.h_units         = h_units # number of hidden states\n        self.activation      = activation # should be tanh by default\n        self.input_shape     = input_shape\n        self.gate_activation = gate_activation\n\n        self.init_method      = None # just added\n        self.optimizer_kwargs = None # just added\n\n        # gate weights\n        self.W_update = None\n        self.W_reset  = None\n        self.W_states = None\n\n        # gate bias\n        self.b_update = None\n        self.b_reset  = None\n        self.b_states = None\n\n        # final output to nodes weights\n        self.W_final = None\n\n        # final output to nodes bias\n        self.b_final = None\n\n        self.is_trainable = True\n\n    @property\n    def trainable(self):\n        return self.is_trainable\n\n    @trainable.setter\n    def trainable(self, is_trainable):\n        self.is_trainable = is_trainable\n\n    @property\n    def weight_initializer(self):\n        return self.init_method\n\n    @weight_initializer.setter\n    def weight_initializer(self, init_method):\n        self.init_method = init_method\n\n    @property\n    def weight_optimizer(self):\n        return self.optimizer_kwargs\n\n    @weight_optimizer.setter\n    def weight_optimizer(self, optimizer_kwargs = {}):\n        self.optimizer_kwargs = optimizer_kwargs\n\n    @property\n    def layer_activation(self):\n        return self.activation\n\n    @layer_activation.setter\n    def layer_activation(self, activation):\n        self.activation = activation\n\n    @property\n    def layer_parameters(self):\n        parameters = [\n            self.W_update,\n            self.W_reset,\n            self.W_cell,\n            self.W_states,\n            self.W_final,\n\n            self.b_update,\n            self.b_reset,\n            self.b_cell,\n            self.b_states,\n            self.b_final\n        ]\n        return sum([np.prod(param.shape) for param in parameters])\n\n    @property\n    def output_shape(self):\n        return self.input_shape\n\n    def prep_layer(self):\n        _, input_dim = self.input_shape\n        z_dim        = self.h_units + input_dim # concatenate (h_units, vocabulary_size) vector\n\n        # gate weights\n        self.W_update = init(self.init_method).initialize_weights((z_dim, self.h_units))\n        self.W_reset  = init(self.init_method).initialize_weights((z_dim, self.h_units))\n        self.W_cell   = init(self.init_method).initialize_weights((z_dim, self.h_units))\n        self.W_states = init(self.init_method).initialize_weights((z_dim, self.h_units))\n\n        # gate hidden bias\n        self.b_update = np.zeros((self.h_units,))\n        self.b_reset  = np.zeros((self.h_units,))\n        self.b_cell   = np.zeros((self.h_units,))\n        self.b_states = np.zeros((self.h_units,))\n\n        # final output to nodes weights (input_dim is the vocab size and also the ouput size)\n        self.W_final = init(self.init_method).initialize_weights((self.h_units, input_dim))\n\n        # final output to nodes bias (input_dim is the vocab size and also the ouput size)\n        self.b_final = np.zeros((input_dim,))\n\n    def pass_forward(self, inputs, train_mode = True):\n        self.inputs = inputs\n        batch_size, time_steps, input_dim = inputs.shape\n\n        self.update = np.zeros((batch_size, time_steps, self.h_units))\n        self.reset  = np.zeros((batch_size, time_steps, self.h_units))\n        self.cell   = np.zeros((batch_size, time_steps, self.h_units))\n        self.states = np.zeros((batch_size, time_steps, self.h_units))\n        self.final  = np.zeros((batch_size, time_steps, input_dim))\n\n        self.z       = np.concatenate((self.inputs, self.states), axis = 2)\n        self.z_tilde = np.zeros_like(self.z)\n\n        for t in range(time_steps):\n            self.update[:, t]  = activate(self.gate_activation).forward(np.dot(self.z[:, t], self.W_update) + self.b_update)\n            self.reset[:, t]   = activate(self.gate_activation).forward(np.dot(self.z[:, t], self.W_reset) + self.b_reset)\n            self.z_tilde[:, t] = np.concatenate((self.reset[:, t] * self.states[:, t - 1], self.inputs[:, t]), axis = 1)\n            self.cell[:, t]    = activate(self.activation).forward(np.dot(self.z_tilde[:, t - 1], self.W_cell) + self.b_cell)\n            self.states[:, t]  = (1. - self.update[:, t]) * self.states[:, t - 1]  + self.update[:, t] * self.cell[:, t]\n\n            self.final[:, t] = np.dot(self.states[:, t], self.W_final) + self.b_final # logits\n\n        if not train_mode:\n            return activate('softmax').forward(self.final) # if mode is not training\n\n        return self.final\n\n    def pass_backward(self, grad, epoch_num, batch_num, batch_size):\n        _, time_steps, _ = grad.shape\n        next_grad        = np.zeros_like(grad)\n\n        if self.is_trainable:\n\n            dW_update = np.zeros_like(self.W_update)\n            dW_reset  = np.zeros_like(self.W_reset)\n            dW_cell   = np.zeros_like(self.W_cell)\n            dW_final  = np.zeros_like(self.W_final)\n\n            db_update = np.zeros_like(self.b_update)\n            db_reset  = np.zeros_like(self.b_reset)\n            db_cell   = np.zeros_like(self.b_cell)\n            db_final  = np.zeros_like(self.b_final)\n\n            dstates       = np.zeros_like(self.states)\n            dstate_a      = np.zeros_like(self.states)\n            dstate_b      = np.zeros_like(self.states)\n            dstate_c      = np.zeros_like(self.states)\n            dstates_next  = np.zeros_like(self.states)\n            dstates_prime = np.zeros_like(self.states)\n\n            dz_cell = np.zeros_like(self.cell)\n            dcell   = np.zeros_like(self.cell)\n\n            dz_reset = np.zeros_like(self.reset)\n            dreset   = np.zeros_like(self.reset)\n\n            dz_update = np.zeros_like(self.update)\n            dupdate   = np.zeros_like(self.update)\n\n            for t in np.arange(time_steps)[::-1]: # reversed\n\n                dW_final += np.dot(self.states[:, t].T, grad[:, t])\n                db_final += np.sum(grad[:, t], axis = 0)\n\n                dstates[:, t]  = np.dot(grad[:, t], self.W_final.T)\n                dstates[:, t] += dstates_next[:, t]\n                next_grad      = np.dot(dstates, self.W_final)\n\n                dcell[:, t]    = self.update[:, t] * dstates[:, t]\n                dstate_a[:, t] = (1. - self.update[:, t]) * dstates[:, t]\n                dupdate[:, t]  = self.cell[:, t] * dstates[:, t] - self.states[:, t - 1] * dstates[:, t]\n\n                dcell[:, t]  = activate(self.activation).backward(self.cell[:, t]) * dcell[:, t]\n                dW_cell     += np.dot(self.z_tilde[:, t - 1].T, dcell[:, t])\n                db_cell     += np.sum(dcell[:, t], axis = 0)\n                dz_cell      = np.dot(dcell[:, t], self.W_cell.T)\n\n                dstates_prime[:, t] = dz_cell[:, :self.h_units]\n                dstate_b[:, t]      = self.reset[:, t] * dstates_prime[:, t]\n\n                dreset[:, t]  = self.states[:, t - 1] * dstates_prime[:, t]\n                dreset[:, t]  = activate(self.gate_activation).backward(self.reset[:, t]) * dreset[:, t]\n                dW_reset     += np.dot(self.z[:, t].T, dreset[:, t])\n                db_reset     += np.sum(dreset[:, t], axis = 0)\n                dz_reset      = np.dot(dreset[:, t], self.W_reset.T)\n\n                dupdate[:, t]  = activate(self.gate_activation).backward(self.update[:, t]) * dupdate[:, t]\n                dW_update     += np.dot(self.z[:, t].T, dupdate[:, t])\n                db_update     += np.sum(dupdate[:, t], axis = 0)\n                dz_update      = np.dot(dupdate[:, t], self.W_update.T)\n\n                dz             = dz_reset + dz_update\n                dstate_c[:, t] = dz[:, :self.h_units]\n\n                dstates_next = dstate_a + dstate_b + dstate_c\n\n            # optimize weights and bias\n            self.W_final  = optimizer(self.optimizer_kwargs).update(self.W_final, cg(dW_final), epoch_num, batch_num, batch_size)\n            self.b_final  = optimizer(self.optimizer_kwargs).update(self.b_final, cg(db_final), epoch_num, batch_num, batch_size)\n\n            self.W_cell   = optimizer(self.optimizer_kwargs).update(self.W_cell, cg(dW_cell), epoch_num, batch_num, batch_size)\n            self.b_cell   = optimizer(self.optimizer_kwargs).update(self.b_cell, cg(db_cell), epoch_num, batch_num, batch_size)\n\n            self.W_reset  = optimizer(self.optimizer_kwargs).update(self.W_reset, cg(dW_reset), epoch_num, batch_num, batch_size)\n            self.b_reset  = optimizer(self.optimizer_kwargs).update(self.b_reset, cg(db_reset), epoch_num, batch_num, batch_size)\n\n            self.W_update = optimizer(self.optimizer_kwargs).update(self.W_update, cg(dW_update), epoch_num, batch_num, batch_size)\n            self.b_update = optimizer(self.optimizer_kwargs).update(self.b_update, cg(db_update), epoch_num, batch_num, batch_size)\n\n        # endif self.is_trainable\n\n        return next_grad\n"""
ztlearn/dl/layers/recurrent/lstm.py,55,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ..base import Layer\nfrom ztlearn.utils import clip_gradients as cg\nfrom ztlearn.initializers import InitializeWeights as init\nfrom ztlearn.activations import ActivationFunction as activate\nfrom ztlearn.optimizers import OptimizationFunction as optimizer\n\n\nclass LSTM(Layer):\n\n    # (time_steps, input_dim) = input_shape\n    # input_dim ==> vocabulary size\n\n    def __init__(self, h_units, activation = None, input_shape = None, gate_activation = 'sigmoid'):\n        self.h_units         = h_units # number of hidden states\n        self.activation      = activation # should be tanh by default\n        self.input_shape     = input_shape\n        self.gate_activation = gate_activation\n\n        self.init_method      = None\n        self.optimizer_kwargs = None\n\n        # gate weights\n        self.W_input  = None\n        self.W_forget = None\n        self.W_output = None\n\n        # gate bias\n        self.b_input  = None\n        self.b_forget = None\n        self.b_output = None\n\n        # cell weights\n        self.W_cell = None\n\n        # cell bias\n        self.b_cell = None\n\n        # final output weights\n        self.W_final = None\n\n        # final output bias\n        self.b_final = None\n\n        self.is_trainable = True\n\n    @property\n    def trainable(self):\n        return self.is_trainable\n\n    @trainable.setter\n    def trainable(self, is_trainable):\n        self.is_trainable = is_trainable\n\n    @property\n    def weight_initializer(self):\n        return self.init_method\n\n    @weight_initializer.setter\n    def weight_initializer(self, init_method):\n        self.init_method = init_method\n\n    @property\n    def weight_optimizer(self):\n        return self.optimizer_kwargs\n\n    @weight_optimizer.setter\n    def weight_optimizer(self, optimizer_kwargs = {}):\n        self.optimizer_kwargs = optimizer_kwargs\n\n    @property\n    def layer_activation(self):\n        return self.activation\n\n    @layer_activation.setter\n    def layer_activation(self, activation):\n        self.activation = activation\n\n    @property\n    def layer_parameters(self):\n        parameters = [\n            self.W_input,\n            self.W_forget,\n            self.W_output,\n            self.W_cell,\n            self.W_final,\n\n            self.b_input,\n            self.b_forget,\n            self.b_output,\n            self.b_cell,\n            self.b_final\n        ]\n        return sum([np.prod(param.shape) for param in parameters])\n\n    @property\n    def output_shape(self):\n        return self.input_shape\n\n    def prep_layer(self):\n        _, input_dim = self.input_shape\n        z_dim        = self.h_units + input_dim # concatenate (h_units, vocabulary_size) vector\n\n        # gate weights\n        self.W_input  = init(self.init_method).initialize_weights((z_dim, self.h_units))\n        self.W_forget = init(self.init_method).initialize_weights((z_dim, self.h_units))\n        self.W_output = init(self.init_method).initialize_weights((z_dim, self.h_units))\n\n        # gate bias\n        self.b_input  = np.zeros((self.h_units,))\n        self.b_forget = np.zeros((self.h_units,))\n        self.b_output = np.zeros((self.h_units,))\n\n        # cell weights\n        self.W_cell = init(self.init_method).initialize_weights((z_dim, self.h_units))\n\n        # cell bias\n        self.b_cell = np.zeros((self.h_units,))\n\n        # final output weights\n        self.W_final = init(self.init_method).initialize_weights((self.h_units, input_dim))\n\n        # final output bias\n        self.b_final = np.zeros((input_dim,))\n\n    def pass_forward(self, inputs, train_mode = True):\n        self.inputs = inputs\n        batch_size, time_steps, input_dim = inputs.shape\n\n        self.forget     = np.zeros((batch_size, time_steps, self.h_units))\n        self.input      = np.zeros((batch_size, time_steps, self.h_units))\n        self.output     = np.zeros((batch_size, time_steps, self.h_units))\n        self.states     = np.zeros((batch_size, time_steps, self.h_units))\n        self.cell_tilde = np.zeros((batch_size, time_steps, self.h_units))\n        self.cell       = np.zeros((batch_size, time_steps, self.h_units))\n        self.final      = np.zeros((batch_size, time_steps, input_dim))\n\n        self.z = np.concatenate((self.inputs, self.states), axis = 2)\n\n        for t in range(time_steps):\n            self.forget[:, t]     = activate(self.gate_activation).forward(np.dot(self.z[:, t], self.W_forget) + self.b_forget)\n            self.input[:, t]      = activate(self.gate_activation).forward(np.dot(self.z[:, t], self.W_input) + self.b_input)\n            self.cell_tilde[:, t] = activate(self.activation).forward(np.dot(self.z[:, t], self.W_cell) + self.b_cell)\n            self.cell[:, t]       = self.forget[:, t] * self.cell[:, t - 1] + self.input[:, t] * self.cell_tilde[:, t]\n            self.output[:, t]     = activate(self.gate_activation).forward(np.dot(self.z[:, t], self.W_output) + self.b_output)\n            self.states[:, t]     = self.output[:, t] * activate(self.activation).forward(self.cell[:, t])\n\n            # logits\n            self.final[:, t] = np.dot(self.states[:, t], self.W_final) + self.b_final\n\n        if not train_mode:\n            return activate('softmax').forward(self.final) # if mode is not training\n\n        return self.final\n\n    def pass_backward(self, grad, epoch_num, batch_num, batch_size):\n        _, time_steps, _ = grad.shape\n        next_grad = np.zeros_like(grad)\n\n        if self.is_trainable:\n\n            dW_forget = np.zeros_like(self.W_forget)\n            dW_input  = np.zeros_like(self.W_input)\n            dW_output = np.zeros_like(self.W_output)\n            dW_cell   = np.zeros_like(self.W_cell)\n            dW_final  = np.zeros_like(self.W_final)\n\n            db_forget = np.zeros_like(self.b_forget)\n            db_input  = np.zeros_like(self.b_input)\n            db_output = np.zeros_like(self.b_output)\n            db_cell   = np.zeros_like(self.b_cell)\n            db_final  = np.zeros_like(self.b_final)\n\n            dstates     = np.zeros_like(self.states)\n            dcell       = np.zeros_like(self.cell)\n            dcell_tilde = np.zeros_like(self.cell_tilde)\n            dforget     = np.zeros_like(self.forget)\n            dinput      = np.zeros_like(self.input)\n            doutput     = np.zeros_like(self.output)\n\n            dcell_next   = np.zeros_like(self.cell)\n            dstates_next = np.zeros_like(self.states)\n\n            for t in np.arange(time_steps)[::-1]: # reversed\n\n                dW_final += np.dot(self.states[:, t].T, grad[:, t])\n                db_final += np.sum(grad[:, t], axis = 0)\n\n                dstates[:, t]  = np.dot(grad[:, t], self.W_final.T)\n                dstates[:, t] += dstates_next[:, t]\n                next_grad      = np.dot(dstates, self.W_final)\n\n                doutput[:,t]  = activate(self.activation).backward(self.cell[:, t]) * dstates[:, t]\n                doutput[:,t]  = activate(self.gate_activation).backward(self.output[:, t]) * doutput[:,t]\n                dW_output    += np.dot(self.z[:, t].T, doutput[:, t])\n                db_output    += np.sum(doutput[:, t], axis = 0)\n\n                dcell[:, t]       += self.output[:, t] * dstates[:, t] * activate(self.activation).backward(self.cell[:, t])\n                dcell[:, t]       += dcell_next[:, t]\n                dcell_tilde[:, t]  = dcell[:, t] * self.input[:, t]\n                dcell_tilde[:, t]  = dcell_tilde[:, t] * activate(self.activation).backward(dcell_tilde[:, t])\n                dW_cell           += np.dot(self.z[:, t].T, dcell[:, t])\n                db_cell           += np.sum(dcell[:, t], axis = 0)\n\n                dinput[:, t]  = self.cell_tilde[:, t] * dcell[:, t]\n                dinput[:, t]  = activate(self.gate_activation).backward(self.input[:, t]) * dinput[:, t]\n                dW_input     += np.dot(self.z[:, t].T, dinput[:, t])\n                db_input     += np.sum(dinput[:, t], axis = 0)\n\n                dforget[:, t]  = self.cell[:, t - 1] * dcell[:, t]\n                dforget[:, t]  = activate(self.gate_activation).backward(self.forget[:, t]) * dforget[:, t]\n                dW_forget     += np.dot(self.z[:, t].T, dforget[:, t])\n                db_forget     += np.sum(dforget[:, t], axis = 0)\n\n                dz_forget = np.dot(dforget[:, t], self.W_forget.T)\n                dz_input  = np.dot(dinput[:, t], self.W_input.T)\n                dz_output = np.dot(doutput[:, t], self.W_output.T)\n                dz_cell   = np.dot(dcell[:, t], self.W_cell.T)\n\n                dz                 = dz_forget + dz_input + dz_output + dz_cell\n                dstates_next[:, t] = dz[:,:self.h_units]\n                dcell_next         = self.forget * dcell\n\n            # optimize weights and bias\n            self.W_final  = optimizer(self.optimizer_kwargs).update(self.W_final, cg(dW_final), epoch_num, batch_num, batch_size)\n            self.b_final  = optimizer(self.optimizer_kwargs).update(self.b_final, cg(db_final), epoch_num, batch_num, batch_size)\n\n            self.W_forget = optimizer(self.optimizer_kwargs).update(self.W_forget, cg(dW_forget), epoch_num, batch_num, batch_size)\n            self.b_forget = optimizer(self.optimizer_kwargs).update(self.b_forget, cg(db_forget), epoch_num, batch_num, batch_size)\n\n            self.W_input  = optimizer(self.optimizer_kwargs).update(self.W_input, cg(dW_input), epoch_num, batch_num, batch_size)\n            self.b_input  = optimizer(self.optimizer_kwargs).update(self.b_input, cg(db_input), epoch_num, batch_num, batch_size)\n\n            self.W_output = optimizer(self.optimizer_kwargs).update(self.W_output, cg(dW_output), epoch_num, batch_num, batch_size)\n            self.b_output = optimizer(self.optimizer_kwargs).update(self.b_output, cg(db_output), epoch_num, batch_num, batch_size)\n\n            self.W_cell   = optimizer(self.optimizer_kwargs).update(self.W_cell, cg(dW_cell), epoch_num, batch_num, batch_size)\n            self.b_cell   = optimizer(self.optimizer_kwargs).update(self.b_cell, cg(db_cell), epoch_num, batch_num, batch_size)\n\n        # endif self.is_trainable\n\n        return next_grad\n"""
ztlearn/dl/layers/recurrent/rnn.py,24,"b""# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ..base import Layer\nfrom ztlearn.utils import clip_gradients as cg\nfrom ztlearn.initializers import InitializeWeights as init\nfrom ztlearn.activations import ActivationFunction as activate\nfrom ztlearn.optimizers import OptimizationFunction as optimizer\n\n\nclass RNN(Layer):\n\n    def __init__(self, h_units, activation = None, bptt_truncate = 5, input_shape = None):\n        self.h_units       = h_units # number of hidden states\n        self.activation    = activation # should be tanh by default\n        self.bptt_truncate = bptt_truncate\n        self.input_shape   = input_shape\n\n        self.init_method      = None\n        self.optimizer_kwargs = None\n\n        self.W_input  = None\n        self.W_output = None\n        self.W_recur  = None\n\n        self.b_output = None\n        self.b_input  = None\n\n        self.is_trainable = True\n\n    @property\n    def trainable(self):\n        return self.is_trainable\n\n    @trainable.setter\n    def trainable(self, is_trainable):\n        self.is_trainable = is_trainable\n\n    @property\n    def weight_initializer(self):\n        return self.init_method\n\n    @weight_initializer.setter\n    def weight_initializer(self, init_method):\n        self.init_method = init_method\n\n    @property\n    def weight_optimizer(self):\n        return self.optimizer_kwargs\n\n    @weight_optimizer.setter\n    def weight_optimizer(self, optimizer_kwargs = {}):\n        self.optimizer_kwargs = optimizer_kwargs\n\n    @property\n    def layer_activation(self):\n        return self.activation\n\n    @layer_activation.setter\n    def layer_activation(self, activation):\n        self.activation = activation\n\n    @property\n    def layer_parameters(self):\n        parameters = [\n            self.W_input,\n            self.W_output,\n            self.W_recur,\n\n            self.b_output,\n            self.b_input\n        ]\n        return sum([np.prod(param.shape) for param in parameters])\n\n    @property\n    def output_shape(self):\n        return self.input_shape\n\n    def prep_layer(self):\n        _, input_dim = self.input_shape\n\n        self.W_input  = init(self.init_method).initialize_weights((self.h_units, input_dim))\n        self.W_output = init(self.init_method).initialize_weights((input_dim, self.h_units))\n        self.W_recur  = init(self.init_method).initialize_weights((self.h_units, self.h_units))\n\n        self.b_output = np.zeros((input_dim,))\n        self.b_input  = np.zeros((self.h_units,))\n\n    # implementation based on techniques as seen here: https://github.com/dennybritz/rnn-tutorial-rnnlm/blob/master/RNNLM.ipynb\n    def pass_forward(self, inputs, train_mode = True):\n        self.inputs = inputs\n        batch_size, time_steps, input_dim = inputs.shape\n\n        self.state_inputs  = np.zeros((batch_size, time_steps, self.h_units))\n        self.states        = np.zeros((batch_size, time_steps + 1, self.h_units)) # additional(+1) last column containing the final state also set to zero\n        self.state_outputs = np.zeros((batch_size, time_steps, input_dim))\n\n        for t in range(time_steps):\n            self.state_inputs[:, t]  = (np.dot(inputs[:, t], self.W_input.T) + np.dot(self.states[:, t - 1], self.W_recur.T)) + self.b_input\n            self.states[:, t]        = activate(self.activation).forward(self.state_inputs[:, t])\n            self.state_outputs[:, t] = np.dot(self.states[:, t], self.W_output.T) + self.b_output\n\n        if not train_mode:\n            return activate('softmax').forward(self.state_outputs) # if mode is not training\n\n        return self.state_outputs\n\n    # implementation based on techniques as seen here: https://github.com/dennybritz/rnn-tutorial-rnnlm/blob/master/RNNLM.ipynb\n    def pass_backward(self, grad, epoch_num, batch_num, batch_size):\n        _, time_steps, _ = grad.shape\n        next_grad        = np.zeros_like(grad)\n\n        if self.is_trainable:\n\n            dW_input  = np.zeros_like(self.W_input)\n            dW_recur  = np.zeros_like(self.W_recur)\n            dW_output = np.zeros_like(self.W_output)\n\n            db_input  = np.zeros_like(self.b_input)\n            db_output = np.zeros_like(self.b_output)\n\n            for t in np.arange(time_steps)[::-1]: # reversed\n                dW_output       += np.dot(grad[:, t].T, self.states[:, t])\n                db_output       += np.sum(grad[:, t], axis = 0)\n                dstate           = np.dot(grad[:, t], self.W_output) * activate(self.activation).backward(self.state_inputs[:, t])\n                next_grad[:, t]  = np.dot(dstate, self.W_input)\n\n                for tt in np.arange(max(0, t - self.bptt_truncate), t + 1)[::-1]: # reversed\n                    dW_input += np.dot(dstate.T, self.inputs[:, tt])\n                    dW_recur += np.dot(dstate.T, self.states[:, tt - 1])\n                    db_input += np.sum(dstate, axis = 0)\n                    dstate    = np.dot(dstate, self.W_recur) * activate(self.activation).backward(self.state_inputs[:, tt - 1])\n\n            # optimize weights and bias\n            self.W_input  = optimizer(self.optimizer_kwargs).update(self.W_input,  cg(dW_input), epoch_num, batch_num, batch_size)\n            self.W_output = optimizer(self.optimizer_kwargs).update(self.W_output, cg(dW_output), epoch_num, batch_num, batch_size)\n            self.W_recur  = optimizer(self.optimizer_kwargs).update(self.W_recur,  cg(dW_recur), epoch_num, batch_num, batch_size)\n\n            self.b_input  = optimizer(self.optimizer_kwargs).update(self.b_input,  cg(db_input), epoch_num, batch_num, batch_size)\n            self.b_output = optimizer(self.optimizer_kwargs).update(self.b_output, cg(db_output), epoch_num, batch_num, batch_size)\n\n        # endif self.is_trainable\n\n        return next_grad\n"""
