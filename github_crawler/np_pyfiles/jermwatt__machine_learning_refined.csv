file_path,api_count,code
html/prettify.py,0,"b'import sys\nimport re\nfrom bs4 import BeautifulSoup\n\n# extract filename\nfilename = str(sys.argv[1])\nsoup = BeautifulSoup(open(filename),""lxml"")\n\n# page_title becomes the page title and series becomes the series name (duh!)\nfoo = soup.find_all(\'h1\')[0]\npage_title = foo.next_element\n\nfoo = soup.find_all(\'h2\')[0]\nseries = foo.next_element\n\n# name will become the filename: eg, name.html and name.ipynb\nname = soup.html.head.title.string\n\n# change title to page_title\nsoup.html.head.title.string = page_title\n \n   \nseries_dict = {\n\t""Chapter 2:"": ""2_Zero_order_methods"",\n\t""Chapter 3:"": ""3_First_order_methods"",\n\t""Chapter 4:"": ""4_Second_order_methods"",\n\t""Chapter 5:"": ""5_Linear_regression"",\n\t""Chapter 6:"": ""6_Linear_twoclass_classification"",\n\t""Chapter 7:"": ""7_Linear_multiclass_classification"",\n\t""Chapter 8:"": ""8_Linear_unsupervised_learning"",\n\t""Chapter 9:"": ""9_Feature_engineer_select"",\n\t""Chapter 10"": ""10_Nonlinear_intro"",\n\t""Chapter 11"": ""11_Feature_learning"",\n\t""Chapter 13"": ""13_Multilayer_perceptrons"",\n\t""Chapter 16"": ""16_Linear_algebra""\n\t}\n\t\nseries_url = series_dict[series[0:10]]\t\n        \n         \n\n# This script adds navigation bar + sharing logos + title\nscript_1 = \'\'\'\n<!-- uncomment to add back menu\n<div style=""text-align:center !important; padding-top:58px;"">\n\n\t\t\t\t<a href=""../../../index.html"" style=""font-family: inherit; font-weight: 200; letter-spacing: 1.5px; color: #222; font-size: 97%;"">HOME</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=""../../../about.html"" style=""font-family: inherit; font-weight: 200; letter-spacing: 1.5px; color: #222; font-size: 97%;"">ABOUT</a>\n\n\n</div> -->\n\n<br><br><br>\n\n<!-- share buttons -->\n<div style=""width: 63%; margin:auto;"">\n\n\t<div id=""1"" style=""width: 70%; float:left;"">\n\t\t<span style=""color:black; font-family:\'lato\', sans-serif; font-size: 18px;"">code</span>\n\t\t<div style=""width: 95px; border-bottom: solid 1px; border-color:black;"">\n\n\t\t\t<div class=""logo-share""></div>\n\t\t\t<div class=""logo-share""></div>\n\n\t\t\t<div class=""logo-share"">\n\t\t\t\t<!-- github -->\n\t\t\t\t<a target=""_blank"" href=""https://github.com/jermwatt/machine_learning_refined"">\n\t\t\t\t\t<img src=""../../html/pics/github.png"" width=28 height=28 onmouseover=""this.src=\'../../html/pics/github_filled.png\';"" onmouseout=""this.src=\'../../html/pics/github.png\';"">\n\t\t\t\t</a>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n\n\t<div id=""2"" style=""width: 30%; float:left;"">\n\t\t<span style=""color:black; font-family:\'lato\', sans-serif; font-size: 18px;"">share</span>\n\t\t<div style=""width: 280px; border-bottom: solid 1px; border-color:black;"">\n\n\t\t\t<div class=""logo-share""></div>\n\t\t\t<div class=""logo-share""></div>\n\n\t\t\t<div class=""logo-share"">\n\t\t\t\t<!-- linkedin -->\n\t\t\t\t<a target=""_blank"" href=""https://www.linkedin.com/cws/share?url=https%3A%2F%2Fjermwatt.github.io%2Fmachine_learning_refined%2Fnotes%2F\'\'\'+series_url+\'\'\'%2F\'\'\'+ name+\'\'\'.html"">\n\t\t\t\t\t<img src=""../../html/pics/linkedin.png"" width=28 height=28 onmouseover=""this.src=\'../../html/pics/linkedin_filled.png\';"" onmouseout=""this.src=\'../../html/pics/linkedin.png\';"">\n\t\t\t\t</a>\n\t\t\t</div>\n\n\t\t\t<div class=""logo-share""></div>\n\n\t\t\t<div class=""logo-share"">\n\t\t\t\t<!-- twitter -->\n\t\t\t\t<a target=""_blank"" href=""https://twitter.com/intent/tweet?ref_src=twsrc%5Etfw&tw_p=tweetbutton&url=https%3A%2F%2Fjermwatt.github.io%2Fmachine_learning_refined%2Fnotes%2F\'\'\'+series_url+\'\'\'%2F\'\'\'+ name+\'\'\'.html"">\n\t\t\t\t\t<img src=""../../html/pics/twitter.png"" width=28 height=28 onmouseover=""this.src=\'../../html/pics/twitter_filled.png\';"" onmouseout=""this.src=\'../../html/pics/twitter.png\';"">\n\t\t\t\t</a>\n\t\t\t</div>\n\n\t\t\t<div class=""logo-share""></div>\n\n\t\t\t<div class=""logo-share"">\n\t\t\t\t<!-- facebook -->\n\t\t\t\t<a target=""_blank"" href=""https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fjermwatt.github.io%2Fmachine_learning_refined%2Fnotes%2F\'\'\'+series_url+\'\'\'%2F\'\'\'+ name+\'\'\'.html"">\n\t\t\t\t\t<img src=""../../html/pics/facebook.png"" width=28 height=28 onmouseover=""this.src=\'../../html/pics/facebook_filled.png\';"" onmouseout=""this.src=\'../../html/pics/facebook.png\';"">\n\t\t\t\t</a>\n\t\t\t</div>\n\t\t\t\n\t\t\t<div class=""logo-share""></div>\n\n\t\t\t<div class=""logo-share"">\n\t\t\t\t<!-- reddit -->\n\t\t\t\t<a target=""_blank"" href=""https://www.reddit.com/submit?url=https%3A%2F%2Fjermwatt.github.io%2Fmachine_learning_refined%2Fnotes%2F\'\'\'+series_url+\'\'\'%2F\'\'\'+ name+\'\'\'.html"">\n\t\t\t\t\t<img src=""../../html/pics/reddit.png"" width=28 height=28 onmouseover=""this.src=\'../../html/pics/reddit_filled.png\';"" onmouseout=""this.src=\'../../html/pics/reddit.png\';"">\n\t\t\t\t</a>\n\t\t\t</div>\n\t\t\t\n\t\t</div>\n\n\t</div>\n</div>\n\n<br><br>\n<div class=""page-title"" style=""text-align: center !important;"">\n<div><a href=""https://github.com/jermwatt/machine_learning_refined"" style=""text-decoration: none"" target=""_blank""><button class=""btn-star"">\xe2\x98\x85 Our Project On GitHub</button></a></div>\n\t<br><br>\n\t<mark style=""padding: 0px; background-color: #f9f3c2;"">\'\'\'+ page_title +\'\'\'*</mark>\n</div>\n<center>\n<div style=""text-align: left !important; font-size:16px; width:64%; color: #333""><br><br><br><br>\n* The following is part of an early draft of the second edition of <strong>Machine Learning Refined</strong>. The published text (with revised material) is now available on <a target=""_blank"" href=""https://www.amazon.com/Machine-Learning-Refined-Foundations-Applications/dp/1108480721"">Amazon</a> as well as other major book retailers. Instructors may request an examination copy from <a target=""_blank"" href=""https://www.cambridge.org/us/academic/subjects/engineering/communications-and-signal-processing/machine-learning-refined-foundations-algorithms-and-applications-2nd-edition?format=HB"">Cambridge University Press</a>.\n</div>\n</center>\n<br>\'\'\'\n\n# parse script as BeautifulSoup object\nhtml_1 = BeautifulSoup(script_1,\'html.parser\')\n\n# insert it as the first element of the body tag, hence [0]\nsoup.body.insert(0, html_1)\n\n\n# # This script adds comment section to the bottom of the page\n# script_2 = \'\'\'\n# <br><br><br><br><br><br>\n\n# <!-- comment section -->\n# <div id=""disqus_thread"" style=""width:70%; height:auto; margin:auto;""></div>\n# <script>\n# (function() { // DON\'T EDIT BELOW THIS LINE\n# var d = document, s = d.createElement(\'script\');\n# s.src = \'https://machine_learning_refined.disqus.com/embed.js\';\n# s.setAttribute(\'data-timestamp\', +new Date());\n# (d.head || d.body).appendChild(s);\n# })();\n# </script>\n# <noscript>Please enable JavaScript to view the <a href=""https://disqus.com/?ref_noscript"">comments powered by Disqus.</a></noscript>\n# \'\'\'\n\n# # parse script as BeautifulSoup object\n# html_2 = BeautifulSoup(script_2,\'html.parser\')\n\n# # insert it as the last element of body tag, hence: -1\n# soup.body.insert(-1, html_2)\n\nprint(page_title)\n\n# This script changes default LateX font to a prettier version\nscript_3 = \'\'\'\n\t<meta property=""og:title"" content=""\'\'\'+page_title+\'\'\'"">\n\t<meta property=""og:image"" content=""https://github.com/jermwatt/machine_learning_refined/blob/gh-pages/html/pics/meta.png"">\n\t<meta property=""og:url"" content=""https://jermwatt.github.io/machine_learning_refined/notes/\'\'\'+series_url+\'\'\'/\'\'\'+ name+\'\'\'.html"">\n\t<meta name=""twitter:card"" content=""summary_large_image"">\t\n\n    <script type=""text/x-mathjax-config"">\n    MathJax.Hub.Config({\n    \tTeX: { equationNumbers: { autoNumber: ""AMS"" } },\n        tex2jax: {\n            inlineMath: [ [\'$\',\'$\'], [""\\\\\\("",""\\\\\\)""] ],\n            displayMath: [ [\'$$\',\'$$\'], [""\\\\\\["",""\\\\\\]""] ],\n            processEscapes: true,\n            processEnvironments: true\n        },\n        // Center justify equations in code and markdown cells. Elsewhere\n        // we use CSS to left justify single line equations in code cells.\n        displayAlign: \'center\',\n        ""HTML-CSS"": {\n            availableFonts: [""TeX""],\n            preferredFont: ""TeX"",\n            styles: {\'.MathJax_Display\': {""margin"": 0}},\n            linebreaks: { automatic: true }\n        }\n    });\n    </script>\n\n    <link href=""../../html/CSS/custom.css"" rel=""stylesheet""/>\n\n    <style>\n        p {\n            text-align: justify !important;\n            text-justify: inter-word !important;\n        }\n    </style>\n\n    \'\'\'\n# parse script as BeautifulSoup object\nhtml_3 = BeautifulSoup(script_3, \'html.parser\')\n\n# replace the old font with the new font\nsoup.head.find(text=re.compile(r\'HTML-CSS\')).parent.replace_with(html_3);\n\n\n# you have to render soup again (for some reason) before you can search it\nsoup = BeautifulSoup(soup.renderContents(),""lxml"")\n\n# remove old title\nsoup.body.find_all(\'h1\')[0].decompose()\n\n# remove old series title\nsoup.body.find_all(\'h2\')[0].decompose()\n\n# remove code cells that contain the following message\n# \'in the HTML version\'\nfor cell in soup.body.find_all(text=re.compile(\'in the HTML version\')):\n\tcell.parent.parent.parent.parent.decompose()\n\n\n# finish by spiting out modified soup as html\nwith open(filename, ""wt"") as file:\n    file.write(str(soup))\n\nprint(\'----------------\')\nprint(\'Conversion done!\')\nprint(\' \')\nprint(\'   \xc2\xaf\\\\_(\xe3\x83\x84)_/\xc2\xaf\')\nprint(\' \')\nprint(\'----------------\')\n'"
mlrefined_libraries/__init__.py,0,"b""#!/usr/bin/env python\nimport pkgutil\n__path__ = pkgutil.extend_path(__path__, __name__)\n  \nfor importer, modname, ispkg in pkgutil.walk_packages(\n        path=__path__,\n        prefix=__name__+'.',\n        onerror=(lambda x: None)):\n    __import__(modname)\n  """
mlrefined_libraries/calculus_library/__init__.py,0,b''
mlrefined_libraries/calculus_library/ascent_visualizer.py,16,"b'import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import gridspec\nfrom IPython.display import display, HTML, clear_output\nimport copy\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\nimport matplotlib.animation as animation\nimport time\nimport math\n\n### animate 2d slope visualization ###\n# animator for recursive function\ndef animate_visualize2d(func,num_frames):\n    # define input space\n    w = np.linspace(-10,10,500)\n    guides = \'on\'\n    \n    # define slopes\n    func_orig = func\n    s = func(1) - func(0)  # slope of input function\n    slopes = np.linspace(-abs(s),abs(s),num_frames)\n\n    # construct figure\n    fig = plt.figure(figsize = (12,4))\n    artist = fig\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,3, 1]) \n    ax1 = plt.subplot(gs[0]); ax1.axis(\'off\');\n    ax3 = plt.subplot(gs[2]); ax3.axis(\'off\');\n\n    # plot input function\n    ax2 = plt.subplot(gs[1])    \n\n    # animate\n    def animate(k):\n        # clear the panel\n        ax2.cla()\n        \n        # setup function\n        slope = slopes[k]\n        func = lambda w: slope*w + func_orig(0)\n        \n        # print rendering update\n        if np.mod(k+1,25) == 0:\n            print (\'rendering animation frame \' + str(k+1) + \' of \' + str(num_frames))\n        if k == num_frames - 1:\n            print (\'animation rendering complete!\')\n            time.sleep(1.5)\n            clear_output()\n            \n        # plot function\n        ax2.plot(w,func(w), c=\'r\', linewidth=2,zorder = 3)\n     \n        ### plot slope as vector\n        if abs(func(1) - func(0)) > 0.2:\n            head_width = 0.166*(func(1) - func(0))\n            head_length = 0.25*(func(1) - func(0))\n        \n            # annotate arrow and annotation\n            if func(1)-func(0) > 0.1:\n                ax2.arrow(0, 0, func(1)-func(0),0, head_width=head_width, head_length=head_length, fc=\'k\', ec=\'k\',linewidth=2.5,zorder = 3)\n                \n                ax2.annotate(\'$a$\', xy=(2, 1), xytext=(func(1.3)-func(0),0),fontsize=15\n            )\n            elif func(1)-func(0) < -0.1:\n                ax2.arrow(0, 0, func(1)-func(0),0, head_width=-head_width, head_length=-head_length, fc=\'k\', ec=\'k\',linewidth=2.5,zorder = 3)\n                \n                ax2.annotate(\'$a$\', xy=(2, 1), xytext=(func(1.5)-func(0),0),fontsize=15\n            )\n            \n        # set viewing limits\n        wgap = (max(w) - min(w))*0.3\n        ax2.set_xlim([-5,5])\n        ax2.set_ylim([-5,5])\n\n        # plot x and y axes, and clean up\n        ax2.grid(True, which=\'both\')\n        \n        # label plot\n        ax2.set_xlabel(\'$w$\',fontsize = 15)\n        ax2.set_ylabel(\'$g(w)$\',fontsize = 15,rotation = 0,labelpad = 20)\n        ax2.set_title(r\'$g(w) = {:.1f}\'.format(slope) + \'w + {:.1f}\'.format(func_orig(0))+\'$\',fontsize = 18)\n        return artist,\n        \n    anim = animation.FuncAnimation(fig, animate,frames=num_frames, interval=num_frames, blit=True)\n        \n    return(anim)\n\n### animate 3d slope visualization ###\n# animator for recursive function\ndef animate_visualize3d(func,**kwargs):\n    \n    num_frames = 10\n    if \'num_frames\' in kwargs:\n        num_frames = kwargs[\'num_frames\']\n    \n    view = [20,-50]\n    if \'view\' in kwargs:\n        view = kwargs[\'view\']\n       \n    # construct figure\n    fig = plt.figure(figsize = (5,5))\n    artist = fig\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 1, width_ratios=[1]) \n    ax = plt.subplot(gs[0],projection=\'3d\'); \n\n    # determine slope range of input function\n    func_orig = func\n    bias = func([0,0])\n    s = func([1,0]) - bias   # slope 1 of input\n    t = func([0,1]) - bias   # slope 2 of input\n    slopes1 = np.linspace(s,-s,num_frames)\n    slopes1.shape = (len(slopes1),1)\n    slopes2 = np.linspace(t,-t,num_frames)\n    slopes2.shape = (len(slopes2),1)\n    slopes = np.concatenate((slopes1,slopes2),axis=1)\n    \n    # define input space\n    w_in = np.linspace(-2,2,200)\n    w1_vals, w2_vals = np.meshgrid(w_in,w_in)\n    w1_vals.shape = (len(w_in)**2,1)\n    w2_vals.shape = (len(w_in)**2,1)\n    g_vals_orig = func([w1_vals,w2_vals]) \n    zmin = np.min(g_vals_orig)\n    zmax = np.max(g_vals_orig)\n    \n    # animate\n    def animate(k):\n        # clear the panel\n        ax.cla()\n        \n        # print rendering update\n        if np.mod(k+1,25) == 0:\n            print (\'rendering animation frame \' + str(k+1) + \' of \' + str(num_frames))\n        if k == num_frames - 1:\n            print (\'animation rendering complete!\')\n            time.sleep(1.5)\n            clear_output()\n            \n        # create mesh for surface\n        w1_vals.shape = (len(w_in)**2,1)\n        w2_vals.shape = (len(w_in)**2,1)\n        \n        # create and evaluate function\n        slope = slopes[k,:]\n        func = lambda w: slope[0]*w[0] + slope[1]*w[1]  + bias\n        g_vals = func([w1_vals,w2_vals]) \n\n        # vals for cost surface, reshape for plot_surface function\n        w1_vals.shape = (len(w_in),len(w_in))\n        w2_vals.shape = (len(w_in),len(w_in))\n        g_vals.shape = (len(w_in),len(w_in))\n        \n        ### plot function and z=0 for visualization ###\n        ax.plot_surface(w1_vals, w2_vals, g_vals, alpha = 0.3,color = \'r\',rstride=25, cstride=25,linewidth=0.7,edgecolor = \'k\',zorder = 2)\n\n        ax.plot_surface(w1_vals, w2_vals, g_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.5,edgecolor = \'k\') \n        \n        ### add arrows and annotations ###\n        # add arrow for slope visualization\n        s = func([1,0]) - func([0,0])\n        if abs(s) > 0.5:\n            # draw arrow\n            a = Arrow3D([0, s], [0, 0], [0, 0], mutation_scale=20,\n                    lw=2, arrowstyle=""-|>"", color=""b"")\n            ax.add_artist(a)\n            \n            # label arrow\n            q = func([1.3,0]) - func([0,0])\n            annotate3D(ax, s=\'$(a_1,0)$\', xyz=[q,0,0], fontsize=14, xytext=(-3,3),\n               textcoords=\'offset points\', ha=\'center\',va=\'center\') \n\n        t = func([0,1]) - func([0,0])\n        if abs(t) > 0.5:\n            # draw arrow\n            a = Arrow3D([0, 0], [0, t], [0, 0], mutation_scale=20,\n                    lw=2, arrowstyle=""-|>"", color=""b"")\n            ax.add_artist(a)  \n            \n            # label arrow\n            q = func([0,1.3]) - func([0,0])\n            annotate3D(ax, s=\'$(0,a_2)$\', xyz=[0,q,0], fontsize=14, xytext=(-3,3),\n               textcoords=\'offset points\', ha=\'center\',va=\'center\') \n                \n        # full gradient\n        if abs(s) > 0.5 and abs(t) > 0.5:\n            a = Arrow3D([0, func([1,0])- func([0,0])], [0, func([0,1])- func([0,0])], [0, 0], mutation_scale=20,\n                    lw=2, arrowstyle=""-|>"", color=""k"")\n            ax.add_artist(a)  \n            \n            s = func([1.2,0]) - func([0,0])\n            t = func([0,1.2]) - func([0,0])\n            annotate3D(ax, s=\'$(a_1,a_2)$\', xyz=[s,t,0], fontsize=14, xytext=(-3,3),\n               textcoords=\'offset points\', ha=\'center\',va=\'center\') \n\n        ### clean up plot ###\n        # plot x and y axes, and clean up\n        ax.grid(False)\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n        \n        # remove axes lines and tickmarks\n        ax.w_zaxis.line.set_lw(0.)\n        ax.set_zticks([])\n        ax.w_xaxis.line.set_lw(0.)\n        ax.set_xticks([])\n        ax.w_yaxis.line.set_lw(0.)\n        ax.set_yticks([])\n        \n        # set viewing angle\n        ax.view_init(view[0],view[1])\n        \n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n        # set vewing limits\n        y = 2.3\n        ax.set_xlim([-y,y])\n        ax.set_ylim([-y,y])\n        ax.set_zlim([zmin,zmax])\n\n        # label plot\n        fontsize = 15\n        ax.set_xlabel(r\'$w_1$\',fontsize = fontsize,labelpad = -10)\n        ax.set_ylabel(r\'$w_2$\',fontsize = fontsize,rotation = 0,labelpad=-10)\n        sig = \'+\'\n        if slope[1] < 0:\n            sig = \'-\'\n        sig2 = \'+\'\n        if bias < 0:\n            sig2 = \'-\'\n            \n        part1 = \'{:.1f}\'.format(slope[0]) + \'w_1 \'\n        if abs(slope[0]) < 0.01:\n            part1 = \'\'\n            \n        part2 = \'{:.1f}\'.format(abs(slope[1])) + \'w_2\'\n        if abs(slope[1]) < 0.01:\n            part2 = \'\'\n        \n        part3 = sig2 + \'{:.1f}\'.format(abs(bias))\n        if abs(bias) < 0.01:\n            part3 = \'\'\n        \n        ax.set_title(r\'$g(w_1,w_2) = \' + part1 + sig + part2  + part3 + \'$\' ,fontsize = 13)\n\n        return artist,\n        \n    anim = animation.FuncAnimation(fig, animate,frames=num_frames, interval=num_frames, blit=True)\n        \n    return(anim)\n    \n### static slope visualizer functions ###\n# custom plot for spiffing up plot of a single mathematical function\ndef visualize2d(func,**kwargs):\n    # define input space\n    w = np.linspace(-10,10,500)\n    if \'w\' in kwargs:\n        w = kwargs[\'w\']\n    guides = \'on\'\n    if \'guides\' in kwargs:\n        guides = kwargs[\'guides\']\n    \n    # construct figure\n    fig = plt.figure(figsize = (12,4))\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,3, 1]) \n    ax1 = plt.subplot(gs[0]); ax1.axis(\'off\');\n    ax3 = plt.subplot(gs[2]); ax3.axis(\'off\');\n\n    # plot input function\n    ax2 = plt.subplot(gs[1])\n\n    # plot function\n    ax2.plot(w,func(w), c=\'r\', linewidth=2,zorder = 3)\n     \n    ### plot slope as vector\n    if abs(func(1)) > 0.2:\n        head_width = 0.166*func(1)\n        head_length = 0.25*func(1)\n        \n        # plot slope guide as arrow\n        ax2.arrow(0, 0, func(1),0, head_width=head_width, head_length=head_length, fc=\'k\', ec=\'k\',linewidth=2.5,zorder = 3)\n    \n        # annotate arrow\n        if func(1) > 0.1:\n            ax2.annotate(\'$a$\', xy=(2, 1), xytext=(func(1.3),0),fontsize=15\n            )\n        elif func(1) < -0.1:\n            ax2.annotate(\'$a$\', xy=(2, 1), xytext=(func(1.5),0),fontsize=15\n            )\n            \n    # set viewing limits\n    wgap = (max(w) - min(w))*0.3\n    ax2.set_xlim([-5,5])\n    ax2.set_ylim([-5,5])\n\n    # plot x and y axes, and clean up\n    ax2.grid(True, which=\'both\')\n    #ax2.axhline(y=0, color=\'k\', linewidth=1)\n    #ax2.axvline(x=0, color=\'k\', linewidth=1)\n        \n    # label plot\n    ax2.set_xlabel(\'$w$\',fontsize = 15)\n    ax2.set_ylabel(\'$g(w)$\',fontsize = 15,rotation = 0,labelpad = 20)\n    plt.show()\n    \n# custom plot for spiffing up plot of a single mathematical function\ndef visualize3d(func1,func2,func3,**kwargs):\n    # define input space\n    w = np.linspace(-2,2,200)\n\n    if \'w\' in kwargs:\n        w = kwargs[\'w\']\n    guides = \'on\'\n    if \'guides\' in kwargs:\n        guides = kwargs[\'guides\']\n        \n    view = [20,20]\n    if \'view\' in kwargs:\n        view = kwargs[\'view\']\n        \n    # create mesh\n    w1_vals,w2_vals = np.meshgrid(w,w)\n    w1_vals.shape = (len(w)**2,1)\n    w2_vals.shape = (len(w)**2,1)\n    g_vals1 = func1([w1_vals,w2_vals])\n    g_vals2 = func2([w1_vals,w2_vals])\n    g_vals3 = func3([w1_vals,w2_vals])\n\n    # vals for cost surface\n    w1_vals.shape = (len(w),len(w))\n    w2_vals.shape = (len(w),len(w))\n    g_vals1.shape = (len(w),len(w))\n    g_vals2.shape = (len(w),len(w))\n    g_vals3.shape = (len(w),len(w))\n       \n    # construct figure\n    fig = plt.figure(figsize = (15,6),edgecolor = \'k\')\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,1,1]) \n    ax1 = plt.subplot(gs[0],projection=\'3d\'); \n    ax2 = plt.subplot(gs[1],projection=\'3d\')\n    ax3 = plt.subplot(gs[2],projection=\'3d\')\n    \n    for i in range(3):\n        ax = 0\n        func = 0\n        g_vals = 0\n        if i == 0:\n            ax = ax1\n            func = func1\n            g_vals = g_vals1\n        if i == 1:\n            ax = ax2\n            func = func2\n            g_vals = g_vals2\n        if i == 2:\n            ax = ax3\n            func = func3\n            g_vals = g_vals3\n            \n        # add arrow for slope visualization\n        s = func([1,0]) - func([0,0])\n        if abs(s) > 0.5:\n            # draw arrow\n            a = Arrow3D([0, s], [0, 0], [0, 0], mutation_scale=20,\n                    lw=2, arrowstyle=""-|>"", color=""b"")\n            ax.add_artist(a)\n            \n            # label arrow\n            s = func([1.3,0]) - func([0,0])\n            annotate3D(ax, s=\'$(a_1,0)$\', xyz=[s,0,0], fontsize=14, xytext=(-3,3),\n               textcoords=\'offset points\', ha=\'center\',va=\'center\') \n\n        t = func([0,1]) - func([0,0])\n        if abs(t) > 0.5:\n            # draw arrow\n            a = Arrow3D([0, 0], [0, t], [0, 0], mutation_scale=20,\n                    lw=2, arrowstyle=""-|>"", color=""b"")\n            ax.add_artist(a)  \n            \n            # label arrow\n            t = func([0,1.3]) - func([0,0])\n            annotate3D(ax, s=\'$(0,a_2)$\', xyz=[0,t,0], fontsize=14, xytext=(-3,3),\n               textcoords=\'offset points\', ha=\'center\',va=\'center\') \n                \n        # full gradient\n        if abs(s) > 0.5 and abs(t) > 0.5:\n            a = Arrow3D([0, func([1,0])- func([0,0])], [0, func([0,1])- func([0,0])], [0, 0], mutation_scale=20,\n                    lw=2, arrowstyle=""-|>"", color=""k"")\n            ax.add_artist(a)  \n            \n            b = func([1.2,0]) - func([0,0])\n            c = func([0,1.2]) - func([0,0])\n            annotate3D(ax, s=\'$(a_1,a_2)$\', xyz=[b,c,0], fontsize=14, xytext=(-3,3),\n               textcoords=\'offset points\', ha=\'center\',va=\'center\') \n            \n        # plot function        \n        ax.plot_surface(w1_vals, w2_vals, g_vals, alpha = 0.5,color = \'r\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)\n\n        ax.plot_surface(w1_vals, w2_vals, g_vals*0, alpha = 0.3,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n\n        # plot x and y axes, and clean up\n        ax.grid(False)\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        # remove axes lines and tickmarks\n        ax.w_zaxis.line.set_lw(0.)\n        ax.set_zticks([])\n        ax.w_xaxis.line.set_lw(0.)\n        ax.set_xticks([])\n        ax.w_yaxis.line.set_lw(0.)\n        ax.set_yticks([])\n        \n        # set viewing angle\n        ax.view_init(view[0],view[1])\n        \n        # set vewing limits\n        y = 2.3\n        ax.set_xlim([-y,y])\n        ax.set_ylim([-y,y])\n        s = np.min(np.min(g_vals))\n        t = np.max(np.max(g_vals))\n        ax.set_zlim([s,t])\n        \n        # label plot\n        fontsize = 15\n        ax.set_xlabel(r\'$w_1$\',fontsize = fontsize,labelpad = -10)\n        ax.set_ylabel(r\'$w_2$\',fontsize = fontsize,rotation = 0,labelpad=-10)\n        \n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n    plt.show()\n    \n    \n#### custom 3d arrow and annotator functions ###    \n# nice arrow maker from https://stackoverflow.com/questions/11140163/python-matplotlib-plotting-a-3d-cube-a-sphere-and-a-vector\nclass Arrow3D(FancyArrowPatch):\n\n    def __init__(self, xs, ys, zs, *args, **kwargs):\n        FancyArrowPatch.__init__(self, (0, 0), (0, 0), *args, **kwargs)\n        self._verts3d = xs, ys, zs\n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n        FancyArrowPatch.draw(self, renderer)\n\n# great solution for annotating 3d objects - from https://datascience.stackexchange.com/questions/11430/how-to-annotate-labels-in-a-3d-matplotlib-scatter-plot\nclass Annotation3D(Annotation):\n    \'\'\'Annotate the point xyz with text s\'\'\'\n\n    def __init__(self, s, xyz, *args, **kwargs):\n        Annotation.__init__(self,s, xy=(0,0), *args, **kwargs)\n        self._verts3d = xyz        \n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.xy=(xs,ys)\n        Annotation.draw(self, renderer)        \n\ndef annotate3D(ax, s, *args, **kwargs):\n    \'\'\'add anotation text s to to Axes3d ax\'\'\'\n\n    tag = Annotation3D(s, *args, **kwargs)\n    ax.add_artist(tag)'"
mlrefined_libraries/calculus_library/derivative_3d_plotter.py,11,"b""import sys\nsys.path.append('../')\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import gridspec\nfrom IPython.display import display, HTML\nimport copy\nimport autograd.numpy as np\nfrom autograd import jacobian\nfrom autograd import grad\n\n# a short function for plotting function and derivative values over a large range for input function g\ndef autograd_3d_derval_plot(g,**kwargs):\n    # use autograd to compute gradient\n    gradient = grad(g)\n    \n    # specify range of input for our function and its derivative\n    plot_size = 20\n    if 'plot_size' in kwargs:\n        plot_size = kwargs['plot_size']\n    w = np.linspace(-1,1,plot_size) \n    if 'w' in kwargs:\n        w = kwargs['w']\n        \n    # determine vertical plotting limit\n    xx,yy = np.meshgrid(w,w)\n    xx.shape = (1,xx.size)\n    yy.shape = (1,yy.size)\n    h = np.vstack((xx,yy))\n\n    # compute cost func and gradient values\n    vals = g(h)    \n    grad_vals = np.array([gradient(v) for v in h.T])\n    ders1 = grad_vals[:,0]\n    ders2 = grad_vals[:,1]\n        \n    # re-shape everything\n    xx.shape = (plot_size,plot_size)\n    yy.shape = (plot_size,plot_size)\n    vals.shape = (plot_size,plot_size)\n    ders1.shape = (plot_size,plot_size)\n    ders2.shape = (plot_size,plot_size)\n    \n     # plot the functions \n    fig = plt.figure(figsize = (9,4))\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3) \n    ax1 = plt.subplot(gs[0],projection='3d'); ax1.axis('off')\n    ax2 = plt.subplot(gs[1],projection='3d'); ax2.axis('off')\n    ax3 = plt.subplot(gs[2],projection='3d'); ax3.axis('off')\n    color = 'r'\n\n    # plot surfaces\n    ax1.plot_surface(xx, yy, vals, alpha = 0.2,color = color,rstride=2, cstride=2,linewidth=2,edgecolor = 'k')\n    ax2.plot_surface(xx, yy, ders1, alpha = 0.2,color = color,rstride=2, cstride=2,linewidth=2,edgecolor = 'k')\n    ax3.plot_surface(xx, yy, ders2, alpha = 0.2,color = color,rstride=2, cstride=2,linewidth=2,edgecolor = 'k')\n    \n    # titles\n    ax1.set_title(r'$g$',fontsize = 20)\n    ax2.set_title(r'$\\frac{\\mathrm{d}}{\\mathrm{d}w_1}g$',fontsize = 20)\n    ax3.set_title(r'$\\frac{\\mathrm{d}}{\\mathrm{d}w_2}g$',fontsize = 20)\n\n    plt.show()\n    \n# a short function for plotting function and derivative values over a large range for input function g\ndef ad_3d_derval_plot(MyTuple,g,**kwargs):\n    # specify range of input for our function and its derivative\n    plot_size = 20\n    if 'plot_size' in kwargs:\n        plot_size = kwargs['plot_size']\n    w = np.linspace(-1,1,plot_size) \n    if 'w' in kwargs:\n        w = kwargs['w']\n        \n    # determine vertical plotting limit\n    xx,yy = np.meshgrid(w,w)\n    xx.shape = (xx.size,1)\n    yy.shape = (yy.size,1)\n\n    # initialize objects\n    vals = []\n    ders1 = []\n    ders2 = []\n    for i in range(xx.size):\n        u = xx[i]; v = yy[i];\n        \n        w_1 = MyTuple(val = u,der = np.array([1,0]))\n        w_2 = MyTuple(val = v,der = np.array([0,1]))   \n        \n        s = g(w_1,w_2)\n        \n        # extract val and der values\n        val = s.val\n        der = s.der\n        vals.append(val)\n        ders1.append(der[0])\n        ders2.append(der[1])\n\n    # array-afy all output lists\n    vals = np.array(vals)\n    ders1 = np.array(ders1)\n    ders2 = np.array(ders2)\n    \n    # re-shape everything\n    xx.shape = (plot_size,plot_size)\n    yy.shape = (plot_size,plot_size)\n    vals.shape = (plot_size,plot_size)\n    ders1.shape = (plot_size,plot_size)\n    ders2.shape = (plot_size,plot_size)\n    \n     # plot the functions \n    fig = plt.figure(figsize = (9,4))\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3) \n    ax1 = plt.subplot(gs[0],projection='3d'); ax1.axis('off')\n    ax2 = plt.subplot(gs[1],projection='3d'); ax2.axis('off')\n    ax3 = plt.subplot(gs[2],projection='3d'); ax3.axis('off')\n    color = 'r'\n\n    # plot function surfaces\n    ax1.plot_surface(xx, yy, vals, alpha = 0.2,color = color,rstride=2, cstride=2,linewidth=2,edgecolor = 'k')\n    ax2.plot_surface(xx, yy, ders1, alpha = 0.2,color = color,rstride=2, cstride=2,linewidth=2,edgecolor = 'k')\n    ax3.plot_surface(xx, yy, ders2, alpha = 0.2,color = color,rstride=2, cstride=2,linewidth=2,edgecolor = 'k')\n    \n    # titles\n    ax1.set_title(r'$g$',fontsize = 20)\n    ax2.set_title(r'$\\frac{\\mathrm{d}}{\\mathrm{d}w_1}g$',fontsize = 20)\n    ax3.set_title(r'$\\frac{\\mathrm{d}}{\\mathrm{d}w_2}g$',fontsize = 20)\n    \n    plt.show()\n    \n    """
mlrefined_libraries/calculus_library/derivative_ascent_visualizer.py,27,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nimport time\nfrom matplotlib import gridspec\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\n\ndef compare_2d3d(func1,func2,**kwargs):\n    view = [20,-50]\n    if \'view\' in kwargs:\n        view = kwargs[\'view\']\n        \n    # construct figure\n    fig = plt.figure(figsize = (12,4))\n          \n    # remove whitespace from figure\n    fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n    fig.subplots_adjust(wspace=0.01,hspace=0.01)\n        \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,2,4]) \n  \n    ### draw 2d version ###\n    ax1 = plt.subplot(gs[1]); \n    grad = compute_grad(func1)\n    \n    # generate a range of values over which to plot input function, and derivatives\n    w_plot = np.linspace(-3,3,200)                  # input range for original function\n    g_plot = func1(w_plot)\n    g_range = max(g_plot) - min(g_plot)             # used for cleaning up final plot\n    ggap = g_range*0.2\n    w_vals = np.linspace(-2.5,2.5,200)      \n\n    # grab the next input/output tangency pair, the center of the next approximation(s)\n    w_val = float(0)\n    g_val = func1(w_val)\n\n    # plot original function\n    ax1.plot(w_plot,g_plot,color = \'k\',zorder = 1,linewidth=2)                       \n\n    # plot axis\n    ax1.plot(w_plot,g_plot*0,color = \'k\',zorder = 1,linewidth=1)                       \n    # plot the input/output tangency point\n    ax1.scatter(w_val,g_val,s = 80,c = \'lime\',edgecolor = \'k\',linewidth = 2,zorder = 3)            # plot point of tangency\n\n    #### plot first order approximation ####\n    # plug input into the first derivative\n    g_grad_val = grad(w_val)\n\n    # determine width to plot the approximation -- so its length == width\n    width = 4\n    div = float(1 + g_grad_val**2)\n    w1 = w_val - math.sqrt(width/div)\n    w2 = w_val + math.sqrt(width/div)\n\n    # compute first order approximation\n    wrange = np.linspace(w1,w2, 100)\n    h = g_val + g_grad_val*(wrange - w_val)\n\n    # plot the first order approximation\n    ax1.plot(wrange,h,color = \'lime\',alpha = 0.5,linewidth = 3,zorder = 2)      # plot approx\n    \n    #### clean up panel ####\n    # fix viewing limits on panel\n    v = 5\n    ax1.set_xlim([-v,v])\n    ax1.set_ylim([-1 - 0.3,v - 0.3])\n\n    # label axes\n    ax1.set_xlabel(\'$w$\',fontsize = 12,labelpad = -60)\n    ax1.set_ylabel(\'$g(w)$\',fontsize = 25,rotation = 0,labelpad = 50)\n    ax1.grid(False)\n    ax1.yaxis.set_visible(False)\n    ax1.spines[\'right\'].set_visible(False)\n    ax1.spines[\'top\'].set_visible(False)\n    ax1.spines[\'left\'].set_visible(False)\n    \n    ### draw 3d version ###\n    ax2 = plt.subplot(gs[2],projection=\'3d\'); \n    grad = compute_grad(func2)\n    w_val = [float(0),float(0)]\n    \n    # define input space\n    w_in = np.linspace(-2,2,200)\n    w1_vals, w2_vals = np.meshgrid(w_in,w_in)\n    w1_vals.shape = (len(w_in)**2,1)\n    w2_vals.shape = (len(w_in)**2,1)\n    w_vals = np.concatenate((w1_vals,w2_vals),axis=1).T\n    g_vals = func2(w_vals) \n      \n    # evaluation points\n    w_val = np.array([float(w_val[0]),float(w_val[1])])\n    w_val.shape = (2,1)\n    g_val = func2(w_val)\n    grad_val = grad(w_val)\n    grad_val.shape = (2,1)  \n\n    # create and evaluate tangent hyperplane\n    w_tan = np.linspace(-1,1,200)\n    w1tan_vals, w2tan_vals = np.meshgrid(w_tan,w_tan)\n    w1tan_vals.shape = (len(w_tan)**2,1)\n    w2tan_vals.shape = (len(w_tan)**2,1)\n    wtan_vals = np.concatenate((w1tan_vals,w2tan_vals),axis=1).T\n\n    #h = lambda weh: g_val +  np.dot( (weh - w_val).T,grad_val)\n    h = lambda weh: g_val + (weh[0]-w_val[0])*grad_val[0] + (weh[1]-w_val[1])*grad_val[1]     \n    h_vals = h(wtan_vals + w_val)\n    zmin = min(np.min(h_vals),-0.5)\n    zmax = max(np.max(h_vals),+0.5)\n\n    # vals for cost surface, reshape for plot_surface function\n    w1_vals.shape = (len(w_in),len(w_in))\n    w2_vals.shape = (len(w_in),len(w_in))\n    g_vals.shape = (len(w_in),len(w_in))\n    w1tan_vals += w_val[0]\n    w2tan_vals += w_val[1]\n    w1tan_vals.shape =  (len(w_tan),len(w_tan))\n    w2tan_vals.shape =  (len(w_tan),len(w_tan))\n    h_vals.shape = (len(w_tan),len(w_tan))\n\n    ### plot function ###\n    ax2.plot_surface(w1_vals, w2_vals, g_vals, alpha = 0.5,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)\n\n    ### plot z=0 plane ###\n    ax2.plot_surface(w1_vals, w2_vals, g_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n\n    ### plot tangent plane ###\n    ax2.plot_surface(w1tan_vals, w2tan_vals, h_vals, alpha = 0.4,color = \'lime\',zorder = 1,rstride=50, cstride=50,linewidth=1,edgecolor = \'k\')     \n\n    # scatter tangency \n    ax2.scatter(w_val[0],w_val[1],g_val,s = 70,c = \'lime\',edgecolor = \'k\',linewidth = 2)\n    \n    ### clean up plot ###\n    # plot x and y axes, and clean up\n    ax2.xaxis.pane.fill = False\n    ax2.yaxis.pane.fill = False\n    ax2.zaxis.pane.fill = False\n\n    ax2.xaxis.pane.set_edgecolor(\'white\')\n    ax2.yaxis.pane.set_edgecolor(\'white\')\n    ax2.zaxis.pane.set_edgecolor(\'white\')\n\n    # remove axes lines and tickmarks\n    ax2.w_zaxis.line.set_lw(0.)\n    ax2.set_zticks([])\n    ax2.w_xaxis.line.set_lw(0.)\n    ax2.set_xticks([])\n    ax2.w_yaxis.line.set_lw(0.)\n    ax2.set_yticks([])\n\n    # set viewing angle\n    ax2.view_init(20,-65)\n\n    # set vewing limits\n    y = 4\n    ax2.set_xlim([-y,y])\n    ax2.set_ylim([-y,y])\n    ax2.set_zlim([zmin,zmax])\n\n    # label plot\n    fontsize = 12\n    ax2.set_xlabel(r\'$w_1$\',fontsize = fontsize,labelpad = -35)\n    ax2.set_ylabel(r\'$w_2$\',fontsize = fontsize,rotation = 0,labelpad=-40)\n        \n    plt.show()\n    \n# animator for recursive function\ndef visualize3d(func,**kwargs):\n    grad = compute_grad(func)           # gradient of input function\n    colors = [[0,1,0.25],[0,0.75,1]]    # set of custom colors used for plotting\n        \n    num_frames = 10\n    if \'num_frames\' in kwargs:\n        num_frames = kwargs[\'num_frames\']\n    \n    view = [20,-50]\n    if \'view\' in kwargs:\n        view = kwargs[\'view\']\n        \n    plot_descent = False\n    if \'plot_descent\' in kwargs:\n        plot_descent = kwargs[\'plot_descent\']\n        \n    pt1 = [0,0]\n    pt2 = [-0.5,0.5]\n    if \'pt\' in kwargs:\n        pt1 = kwargs[\'pt\']\n    if \'pt2\' in kwargs:\n        pt2 = kwargs[\'pt2\']\n       \n    # construct figure\n    fig = plt.figure(figsize = (9,6))\n          \n    # remove whitespace from figure\n    fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n    fig.subplots_adjust(wspace=0.01,hspace=0.01)\n        \n    # create subplotting mechanism\n    gs = gridspec.GridSpec(1, 1) \n    ax1 = plt.subplot(gs[0],projection=\'3d\'); \n    \n    # define input space\n    w_in = np.linspace(-2,2,200)\n    w1_vals, w2_vals = np.meshgrid(w_in,w_in)\n    w1_vals.shape = (len(w_in)**2,1)\n    w2_vals.shape = (len(w_in)**2,1)\n    w_vals = np.concatenate((w1_vals,w2_vals),axis=1).T\n    g_vals = func(w_vals) \n    cont = 1\n    for pt in [pt1]:\n        # create axis for plotting\n        if cont == 1:\n            ax = ax1\n        if cont == 2:\n            ax = ax2\n\n        cont+=1\n        # evaluation points\n        w_val = np.array([float(pt[0]),float(pt[1])])\n        w_val.shape = (2,1)\n        g_val = func(w_val)\n        grad_val = grad(w_val)\n        grad_val.shape = (2,1)  \n\n        # create and evaluate tangent hyperplane\n        w_tan = np.linspace(-1,1,200)\n        w1tan_vals, w2tan_vals = np.meshgrid(w_tan,w_tan)\n        w1tan_vals.shape = (len(w_tan)**2,1)\n        w2tan_vals.shape = (len(w_tan)**2,1)\n        wtan_vals = np.concatenate((w1tan_vals,w2tan_vals),axis=1).T\n\n        #h = lambda weh: g_val +  np.dot( (weh - w_val).T,grad_val)\n        h = lambda weh: g_val + (weh[0]-w_val[0])*grad_val[0] + (weh[1]-w_val[1])*grad_val[1]     \n        h_vals = h(wtan_vals + w_val)\n        zmin = min(np.min(h_vals),-0.5)\n        zmax = max(np.max(h_vals),+0.5)\n\n        # vals for cost surface, reshape for plot_surface function\n        w1_vals.shape = (len(w_in),len(w_in))\n        w2_vals.shape = (len(w_in),len(w_in))\n        g_vals.shape = (len(w_in),len(w_in))\n        w1tan_vals += w_val[0]\n        w2tan_vals += w_val[1]\n        w1tan_vals.shape =  (len(w_tan),len(w_tan))\n        w2tan_vals.shape =  (len(w_tan),len(w_tan))\n        h_vals.shape = (len(w_tan),len(w_tan))\n\n        ### plot function ###\n        ax.plot_surface(w1_vals, w2_vals, g_vals, alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)\n\n        ### plot z=0 plane ###\n        ax.plot_surface(w1_vals, w2_vals, g_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n\n        ### plot tangent plane ###\n        ax.plot_surface(w1tan_vals, w2tan_vals, h_vals, alpha = 0.1,color = \'lime\',zorder = 1,rstride=50, cstride=50,linewidth=1,edgecolor = \'k\')     \n\n        ### plot particular points - origins and tangency ###\n        # scatter origin\n        ax.scatter(0,0,0,s = 60,c = \'k\',edgecolor = \'w\',linewidth = 2)\n\n        # scatter tangency \n        ax.scatter(w_val[0],w_val[1],g_val,s = 70,c = \'lime\',edgecolor = \'k\',linewidth = 2)\n\n        ##### add arrows and annotations for steepest ascent direction #####\n        # re-assign func variable to tangent\n        cutoff_val = 0.1\n        an = 1.7\n        pname = \'g(\' + str(pt[0]) + \',\' + str(pt[1]) + \')\'\n        s = h([1,0]) - h([0,0])\n        if abs(s) > cutoff_val:\n            # draw arrow\n            a = Arrow3D([0, s], [0, 0], [0, 0], mutation_scale=20,\n                        lw=2, arrowstyle=""-|>"", color=""b"")\n            ax.add_artist(a)\n\n            # label arrow\n            q = h([an,0]) - h([0,0])\n            name = r\'$\\left(\\frac{\\mathrm{d}}{\\mathrm{d}w_1}\' + pname + r\',0\\right)$\'\n            annotate3D(ax, s=name, xyz=[q,0,0], fontsize=12, xytext=(-3,3),textcoords=\'offset points\', ha=\'center\',va=\'center\') \n\n        t = h([0,1]) - h([0,0])\n        if abs(t) > cutoff_val:\n            # draw arrow\n            a = Arrow3D([0, 0], [0, t], [0, 0], mutation_scale=20,\n                        lw=2, arrowstyle=""-|>"", color=""b"")\n            ax.add_artist(a)  \n\n            # label arrow\n            q = h([0,an]) - h([0,0])\n            name = r\'$\\left(0,\\frac{\\mathrm{d}}{\\mathrm{d}w_2}\' + pname + r\'\\right)$\'\n            annotate3D(ax, s=name, xyz=[0,q,0], fontsize=12, xytext=(-3,3),textcoords=\'offset points\', ha=\'center\',va=\'center\') \n\n        # full gradient\n        if abs(s) > cutoff_val and abs(t) > cutoff_val:\n            a = Arrow3D([0, h([1,0])- h([0,0])], [0, h([0,1])- h([0,0])], [0, 0], mutation_scale=20,\n                        lw=2, arrowstyle=""-|>"", color=""k"")\n            ax.add_artist(a)  \n\n            s = h([an+0.2,0]) - h([0,0])\n            t = h([0,an+0.2]) - h([0,0])\n            name = r\'$\\left(\\frac{\\mathrm{d}}{\\mathrm{d}w_1}\' + pname + r\',\\frac{\\mathrm{d}}{\\mathrm{d}w_2}\' + pname + r\'\\right)$\'\n            annotate3D(ax, s=name, xyz=[s,t,0], fontsize=12, xytext=(-3,3),textcoords=\'offset points\', ha=\'center\',va=\'center\') \n\n        ###### add arrow and text for steepest descent direction #####\n        if plot_descent == True:\n            # full negative gradient\n            if abs(s) > cutoff_val and abs(t) > cutoff_val:\n                a = Arrow3D([0, - (h([1,0])- h([0,0]))], [0, - (h([0,1])- h([0,0]))], [0, 0], mutation_scale=20,\n                        lw=2, arrowstyle=""-|>"", color=""r"")\n                ax.add_artist(a)  \n\n                s = - (h([an+0.2,0]) - h([0,0]))\n                t = - (h([0,an+0.2]) - h([0,0]))\n                name = r\'$\\left(-\\frac{\\mathrm{d}}{\\mathrm{d}w_1}\' + pname + r\',-\\frac{\\mathrm{d}}{\\mathrm{d}w_2}\' + pname + r\'\\right)$\'\n                annotate3D(ax, s=name, xyz=[s,t,0], fontsize=12, xytext=(-3,3),textcoords=\'offset points\', ha=\'center\',va=\'center\') \n\n            \n        ### clean up plot ###\n        # plot x and y axes, and clean up\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        # remove axes lines and tickmarks\n        ax.w_zaxis.line.set_lw(0.)\n        ax.set_zticks([])\n        ax.w_xaxis.line.set_lw(0.)\n        ax.set_xticks([])\n        ax.w_yaxis.line.set_lw(0.)\n        ax.set_yticks([])\n\n        # set viewing angle\n        ax.view_init(view[0],view[1])\n\n        # set vewing limits\n        y = 4.5\n        ax.set_xlim([-y,y])\n        ax.set_ylim([-y,y])\n        ax.set_zlim([zmin,zmax])\n\n        # label plot\n        fontsize = 14\n        ax.set_xlabel(r\'$w_1$\',fontsize = fontsize,labelpad = -20)\n        ax.set_ylabel(r\'$w_2$\',fontsize = fontsize,rotation = 0,labelpad=-30)\n    # plot\n    plt.show()\n\n \n# animate ascent direction given by derivative for single input function over a range of values\ndef animate_visualize2d(savepath,**kwargs):\n    g = kwargs[\'g\']                       # input function\n    grad = compute_grad(g)         # gradient of input function\n    colors = [[0,1,0.25],[0,0.75,1]]    # set of custom colors used for plotting\n         \n    num_frames = 300                          # number of slides to create - the input range [-3,3] is divided evenly by this number\n    if \'num_frames\' in kwargs:\n        num_frames = kwargs[\'num_frames\']\n         \n    plot_descent = False\n    if \'plot_descent\' in kwargs:\n        plot_descent = kwargs[\'plot_descent\']\n             \n    # initialize figure\n    fig = plt.figure(figsize = (16,8))\n    artist = fig\n \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,4, 1]) \n    ax1 = plt.subplot(gs[0]); ax1.axis(\'off\');\n    ax3 = plt.subplot(gs[2]); ax3.axis(\'off\');\n \n    # plot input function\n    ax = plt.subplot(gs[1])\n \n    # generate a range of values over which to plot input function, and derivatives\n    w_plot = np.linspace(-3,3,200)                  # input range for original function\n    g_plot = g(w_plot)\n    g_range = max(g_plot) - min(g_plot)             # used for cleaning up final plot\n    ggap = g_range*0.2\n    w_vals = np.linspace(-3,3,num_frames)       # range of values over which to plot first / second order approximations\n                     \n    # animation sub-function\n    print (\'starting animation rendering...\')\n    def animate(k):\n        # clear the panel\n        ax.cla()\n \n        # print rendering update            \n        if np.mod(k+1,25) == 0:\n            print (\'rendering animation frame \' + str(k+1) + \' of \' + str(num_frames))\n        if k == num_frames - 1:\n            print (\'animation rendering complete!\')\n            time.sleep(1.5)\n            clear_output()\n \n        # grab the next input/output tangency pair, the center of the next approximation(s)\n        w_val = w_vals[k]\n        g_val = g(w_val)\n \n        # plot original function\n        ax.plot(w_plot,g_plot,color = \'k\',zorder = 1,linewidth=4)                           # plot function\n \n        # plot the input/output tangency point\n        ax.scatter(w_val,g_val,s = 200,c = \'lime\',edgecolor = \'k\',linewidth = 2,zorder = 3)            # plot point of tangency\n \n        #### plot first order approximation ####\n        # plug input into the first derivative\n        g_grad_val = grad(w_val)\n \n        # determine width to plot the approximation -- so its length == width\n        width = 1\n        div = float(1 + g_grad_val**2)\n        w1 = w_val - math.sqrt(width/div)\n        w2 = w_val + math.sqrt(width/div)\n \n        # compute first order approximation\n        wrange = np.linspace(w1,w2, 100)\n        h = g_val + g_grad_val*(wrange - w_val)\n \n        # plot the first order approximation\n        ax.plot(wrange,h,color = \'lime\',alpha = 0.5,linewidth = 6,zorder = 2)      # plot approx\n             \n        #### plot derivative as vector ####\n        func = lambda w: g_val + g_grad_val*w\n        name = r\'$\\frac{\\mathrm{d}}{\\mathrm{d}w}g(\' + r\'{:.2f}\'.format(w_val) + r\')$\'    \n        if abs(func(1) - func(0)) >=0:\n            head_width = 0.08*(func(1) - func(0))\n            head_length = 0.2*(func(1) - func(0))\n \n            # annotate arrow and annotation\n            if func(1)-func(0) >= 0:\n                ax.arrow(0, 0, func(1)-func(0),0, head_width=head_width, head_length=head_length, fc=\'k\', ec=\'k\',linewidth=2.5,zorder = 3)\n         \n                ax.annotate(name, xy=(2, 1), xytext=(func(1 + 0.3)-func(0),0),fontsize=22)\n            elif func(1)-func(0) < 0:\n                ax.arrow(0, 0, func(1)-func(0),0, head_width=-head_width, head_length=-head_length, fc=\'k\', ec=\'k\',linewidth=2.5,zorder = 3)\n                 \n                ax.annotate(name, xy=(2, 1), xytext=(func(1+0.3) - 1.3 - func(0),0),fontsize=22)\n             \n        #### plot negative derivative as vector ####\n        if plot_descent == True:\n            ax.scatter(0,0,c = \'k\',edgecolor = \'w\',s = 100, linewidth = 0.5,zorder = 4)\n             \n            func = lambda w: g_val - g_grad_val*w\n            name = r\'$-\\frac{\\mathrm{d}}{\\mathrm{d}w}g(\' + r\'{:.2f}\'.format(w_val) + r\')$\'    \n            if abs(func(1) - func(0)) >=0:\n                head_width = 0.08*(func(1) - func(0))\n                head_length = 0.2*(func(1) - func(0))\n \n                # annotate arrow and annotation\n                if func(1)-func(0) >= 0:\n                    ax.arrow(0, 0, func(1)-func(0),0, head_width=head_width, head_length=head_length, fc=\'r\', ec=\'r\',linewidth=2.5,zorder = 3)\n         \n                    ax.annotate(name, xy=(2, 1), xytext=(func(1 + 0.3)-0.2-func(0),0),fontsize=22)\n                elif func(1)-func(0) < 0:\n                    ax.arrow(0, 0, func(1)-func(0),0, head_width=-head_width, head_length=-head_length, fc=\'r\', ec=\'r\',linewidth=2.5,zorder = 3)\n                 \n                    ax.annotate(name, xy=(2, 1), xytext=(func(1+0.3) - 1.6 - func(0),0),fontsize=22)     \n             \n        #### clean up panel ####\n        # fix viewing limits on panel\n        ax.set_xlim([-5,5])\n        ax.set_ylim([min(min(g_plot) - ggap,-0.5),max(max(g_plot) + ggap,0.5)])\n \n        # label axes\n        ax.set_xlabel(\'$w$\',fontsize = 25)\n        ax.set_ylabel(\'$g(w)$\',fontsize = 25,rotation = 0,labelpad = 50)\n        ax.grid(False)\n        ax.yaxis.set_visible(False)\n        ax.spines[\'right\'].set_visible(False)\n        ax.spines[\'top\'].set_visible(False)\n        ax.spines[\'left\'].set_visible(False)\n        for tick in ax.xaxis.get_major_ticks():\n            tick.label.set_fontsize(18) \n \n        return artist,\n         \n    anim = animation.FuncAnimation(fig, animate,frames=len(w_vals), interval=len(w_vals), blit=True)\n         \n    # produce animation and save\n    fps = 50\n    if \'fps\' in kwargs:\n        fps = kwargs[\'fps\']\n    anim.save(savepath, fps=fps, extra_args=[\'-vcodec\', \'libx264\'])\n    clear_output()           \n\n#### custom 3d arrow and annotator functions ###    \n# nice arrow maker from https://stackoverflow.com/questions/11140163/python-matplotlib-plotting-a-3d-cube-a-sphere-and-a-vector\nclass Arrow3D(FancyArrowPatch):\n\n    def __init__(self, xs, ys, zs, *args, **kwargs):\n        FancyArrowPatch.__init__(self, (0, 0), (0, 0), *args, **kwargs)\n        self._verts3d = xs, ys, zs\n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n        FancyArrowPatch.draw(self, renderer)\n\n# great solution for annotating 3d objects - from https://datascience.stackexchange.com/questions/11430/how-to-annotate-labels-in-a-3d-matplotlib-scatter-plot\nclass Annotation3D(Annotation):\n    \'\'\'Annotate the point xyz with text s\'\'\'\n\n    def __init__(self, s, xyz, *args, **kwargs):\n        Annotation.__init__(self,s, xy=(0,0), *args, **kwargs)\n        self._verts3d = xyz        \n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.xy=(xs,ys)\n        Annotation.draw(self, renderer)        \n\ndef annotate3D(ax, s, *args, **kwargs):\n    \'\'\'add anotation text s to to Axes3d ax\'\'\'\n\n    tag = Annotation3D(s, *args, **kwargs)\n    ax.add_artist(tag)'"
mlrefined_libraries/calculus_library/derivative_tree.py,12,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nimport time\nfrom matplotlib import gridspec\nfrom mpl_toolkits.mplot3d import proj3d\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\nfrom IPython.display import display\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nfrom autograd import hessian\nimport autograd.numpy as np\nimport math\nimport copy\n        \n'''\nPlot a user-defined function taking in two inputs, along with first and second derivative functions\n'''\ndef draw_it(func,**kwargs):\n    view = [10,150]\n    if 'view' in kwargs:\n        view = kwargs['view']\n    \n    # generate input space for plotting\n    w_in = np.linspace(-5,5,100)\n    w1_vals, w2_vals = np.meshgrid(w_in,w_in)\n    w1_vals.shape = (len(w_in)**2,1)\n    w2_vals.shape = (len(w_in)**2,1)\n    w_vals = np.concatenate((w1_vals,w2_vals),axis=1).T\n    w1_vals.shape = (len(w_in),len(w_in))\n    w2_vals.shape = (len(w_in),len(w_in))\n    \n    # compute grad vals\n    grad = compute_grad(func)\n    grad_vals = [grad(s) for s in w_vals.T]\n    grad_vals = np.asarray(grad_vals)\n\n    # compute hessian\n    hess = hessian(func)\n    hess_vals = [hess(s) for s in w_vals.T]\n    \n    # define figure\n    fig = plt.figure(figsize = (9,6))\n\n    ###  plot original function ###\n    ax1 = plt.subplot2grid((3, 6), (0, 3), colspan=1,projection='3d')\n\n    # evaluate function, reshape\n    g_vals = func(w_vals)   \n    g_vals.shape = (len(w_in),len(w_in))\n\n    # plot function surface\n    ax1.plot_surface(w1_vals, w2_vals, g_vals, alpha = 0.1,color = 'w',zorder = 1,rstride=15, cstride=15,linewidth=0.5,edgecolor = 'k') \n    ax1.set_title(r'$g(w_1,w_2)$',fontsize = 10)\n    \n    # cleanup axis\n    cleanup(g_vals,view,ax1)\n    \n    ### plot first derivative functions ###\n    ax2 = plt.subplot2grid((3, 6), (1, 2), colspan=1,projection='3d')\n    ax3 = plt.subplot2grid((3, 6), (1, 4), colspan=1,projection='3d')\n\n    # plot first function\n    grad_vals1 = grad_vals[:,0]\n    grad_vals1.shape = (len(w_in),len(w_in))\n    ax2.plot_surface(w1_vals, w2_vals, grad_vals1, alpha = 0.1,color = 'w',zorder = 1,rstride=15, cstride=15,linewidth=0.5,edgecolor = 'k') \n    ax2.set_title(r'$\\frac{\\partial}{\\partial w_1}g(w_1,w_2)$',fontsize = 10)\n        \n    # cleanup axis\n    cleanup(grad_vals1,view,ax2)\n    \n    # plot second\n    grad_vals1 = grad_vals[:,1]\n    grad_vals1.shape = (len(w_in),len(w_in))\n    ax3.plot_surface(w1_vals, w2_vals, grad_vals1, alpha = 0.1,color = 'w',zorder = 1,rstride=15, cstride=15,linewidth=0.5,edgecolor = 'k') \n    ax3.set_title(r'$\\frac{\\partial}{\\partial w_2}g(w_1,w_2)$',fontsize = 10)\n\n    # cleanup axis\n    cleanup(grad_vals1,view,ax3)\n    \n    ### plot second derivatives ###\n    ax4 = plt.subplot2grid((3, 6), (2, 1), colspan=1,projection='3d')\n    ax5 = plt.subplot2grid((3, 6), (2, 3), colspan=1,projection='3d')\n    ax6 = plt.subplot2grid((3, 6), (2, 5), colspan=1,projection='3d')\n\n    # plot first hessian function\n    hess_vals1 = np.asarray([s[0,0] for s in hess_vals])\n    hess_vals1.shape = (len(w_in),len(w_in))\n    ax4.plot_surface(w1_vals, w2_vals, hess_vals1, alpha = 0.1,color = 'w',zorder = 1,rstride=15, cstride=15,linewidth=0.5,edgecolor = 'k') \n    ax4.set_title(r'$\\frac{\\partial}{\\partial w_1}\\frac{\\partial}{\\partial w_1}g(w_1,w_2)$',fontsize = 10)\n\n    # cleanup axis\n    cleanup(hess_vals1,view,ax4)\n    \n    # plot second hessian function\n    hess_vals1 = np.asarray([s[1,0] for s in hess_vals])\n    hess_vals1.shape = (len(w_in),len(w_in))\n    ax5.plot_surface(w1_vals, w2_vals, hess_vals1, alpha = 0.1,color = 'w',zorder = 1,rstride=15, cstride=15,linewidth=0.5,edgecolor = 'k') \n    ax5.set_title(r'$\\frac{\\partial}{\\partial w_1}\\frac{\\partial}{\\partial w_2}g(w_1,w_2)=\\frac{\\partial}{\\partial w_2}\\frac{\\partial}{\\partial w_1}g(w_1,w_2)$',fontsize = 10)\n    \n    # cleanup axis\n    cleanup(hess_vals1,view,ax5)\n    \n    # plot first hessian function\n    hess_vals1 = np.asarray([s[1,1] for s in hess_vals])\n    hess_vals1.shape = (len(w_in),len(w_in))\n    ax6.plot_surface(w1_vals, w2_vals, hess_vals1, alpha = 0.1,color = 'w',zorder = 1,rstride=15, cstride=15,linewidth=0.5,edgecolor = 'k') \n    ax6.set_title(r'$\\frac{\\partial}{\\partial w_2}\\frac{\\partial}{\\partial w_2}g(w_1,w_2)$',fontsize = 10)\n    \n    # cleanup axis\n    cleanup(hess_vals1,view,ax6)\n    plt.show()\n    \n# cleanup an input panel\ndef cleanup(g_vals,view,ax):\n    ### clean up plot ###\n    # plot x and y axes, and clean up\n    ax.xaxis.pane.fill = False\n    ax.yaxis.pane.fill = False\n    ax.zaxis.pane.fill = False\n\n    ax.xaxis.pane.set_edgecolor('white')\n    ax.yaxis.pane.set_edgecolor('white')\n    ax.zaxis.pane.set_edgecolor('white')\n\n    ### plot z=0 plane ###\n    w_zplane = np.linspace(-3,3,200)\n    w1_zplane_vals, w2_zplane_vals = np.meshgrid(w_zplane,w_zplane)\n    ax.plot_surface(w1_zplane_vals, w2_zplane_vals, np.zeros(np.shape(w1_zplane_vals)), alpha = 0.1,color = 'w',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = 'k') \n    \n    # bolden axis on z=0 plane\n    ax.plot(w_zplane,w_zplane*0,w_zplane*0,color = 'k',linewidth = 0.25)\n    ax.plot(w_zplane*0,w_zplane,w_zplane*0,color = 'k',linewidth = 0.25)\n\n    # remove axes lines and tickmarks\n    #ax.w_zaxis.line.set_lw(0.)\n    #ax.set_zticks([])\n    ax.w_xaxis.line.set_lw(0.)\n    ax.set_xticks([])\n    ax.w_yaxis.line.set_lw(0.)\n    ax.set_yticks([])\n\n    # set viewing angle\n    ax.view_init(view[0],view[1])\n\n    # set vewing limits\n    y = 3\n    ax.set_xlim([-y,y])\n    ax.set_ylim([-y,y])\n    zmin = min(np.min(g_vals),-0.5)\n    zmax = max(np.max(g_vals),+0.5)\n    ax.set_zlim([zmin,zmax])\n        """
mlrefined_libraries/calculus_library/derivative_visualizer.py,28,"b""\n# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nfrom matplotlib import gridspec\nimport time\nfrom matplotlib import gridspec\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport copy\n\n# visualize derivatie\ndef compare_2d3d(func1,func2,**kwargs):\n    # input arguments\n    view = [20,-65]\n    if 'view' in kwargs:\n        view = kwargs['view']\n        \n    # define input space\n    w = np.linspace(-3,3,200)                  # input range for original function\n    if 'w' in kwargs:\n        w = kwargs['w']\n        \n    # define pts\n    pt1 = 0\n    if 'pt1' in kwargs:\n        pt1 = kwargs['pt1']\n        \n    pt2 = [0,0]\n    if 'pt2' in kwargs:\n        pt2 = kwargs['pt2']\n    \n    # construct figure\n    fig = plt.figure(figsize = (6,3))\n          \n    # remove whitespace from figure\n    fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n    fig.subplots_adjust(wspace=0.01,hspace=0.01)\n        \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1,2]) \n  \n    ### draw 2d version ###\n    ax1 = plt.subplot(gs[0]); \n    grad = compute_grad(func1)\n    \n    # generate a range of values over which to plot input function, and derivatives\n    g_plot = func1(w)\n    g_range = max(g_plot) - min(g_plot)             # used for cleaning up final plot\n    ggap = g_range*0.2\n    \n    # grab the next input/output tangency pair, the center of the next approximation(s)\n    pt1 = float(pt1)\n    g_val = func1(pt1)\n\n    # plot original function\n    ax1.plot(w,g_plot,color = 'k',zorder = 1,linewidth=2)                          \n    \n    # plot the input/output tangency point\n    ax1.scatter(pt1,g_val,s = 60,c = 'lime',edgecolor = 'k',linewidth = 2,zorder = 3)            # plot point of tangency\n\n    #### plot first order approximation ####\n    # plug input into the first derivative\n    g_grad_val = grad(pt1)\n\n    # compute first order approximation\n    w1 = pt1 - 3\n    w2 = pt1 + 3\n    wrange = np.linspace(w1,w2, 100)\n    h = g_val + g_grad_val*(wrange - pt1)\n\n    # plot the first order approximation\n    ax1.plot(wrange,h,color = 'lime',alpha = 0.5,linewidth = 3,zorder = 2)      # plot approx\n    \n    # make new x-axis\n    ax1.plot(w,g_plot*0,linewidth=3,color = 'k')\n    \n    #### clean up panel ####\n    # fix viewing limits on panel\n    ax1.set_xlim([min(w),max(w)])\n    ax1.set_ylim([min(min(g_plot) - ggap,-4),max(max(g_plot) + ggap,0.5)])\n\n    # label axes\n    ax1.set_xlabel('$w$',fontsize = 12,labelpad = -50)\n    ax1.set_ylabel('$g(w)$',fontsize = 25,rotation = 0,labelpad = 50)\n    \n    ax1.grid(False)\n    ax1.yaxis.set_visible(False)\n    ax1.spines['right'].set_visible(False)\n    ax1.spines['top'].set_visible(False)\n    ax1.spines['left'].set_visible(False)\n    \n    \n    ### draw 3d version ###\n    ax2 = plt.subplot(gs[1],projection='3d'); \n    grad = compute_grad(func2)\n    w_val = [float(0),float(0)]\n    \n    # define input space\n    w1_vals, w2_vals = np.meshgrid(w,w)\n    w1_vals.shape = (len(w)**2,1)\n    w2_vals.shape = (len(w)**2,1)\n    w_vals = np.concatenate((w1_vals,w2_vals),axis=1).T\n    g_vals = func2(w_vals) \n      \n    # evaluation points\n    w_val = np.array([float(pt2[0]),float(pt2[1])])\n    w_val.shape = (2,1)\n    g_val = func2(w_val)\n    grad_val = grad(w_val)\n    grad_val.shape = (2,1)  \n\n    # create and evaluate tangent hyperplane\n    w1tan_vals, w2tan_vals = np.meshgrid(w,w)\n    w1tan_vals.shape = (len(w)**2,1)\n    w2tan_vals.shape = (len(w)**2,1)\n    wtan_vals = np.concatenate((w1tan_vals,w2tan_vals),axis=1).T\n\n    #h = lambda weh: g_val +  np.dot( (weh - w_val).T,grad_val)\n    h = lambda weh: g_val + (weh[0]-w_val[0])*grad_val[0] + (weh[1]-w_val[1])*grad_val[1]     \n    h_vals = h(wtan_vals + w_val)\n\n    # vals for cost surface, reshape for plot_surface function\n    w1_vals.shape = (len(w),len(w))\n    w2_vals.shape = (len(w),len(w))\n    g_vals.shape = (len(w),len(w))\n    w1tan_vals += w_val[0]\n    w2tan_vals += w_val[1]\n    w1tan_vals.shape =  (len(w),len(w))\n    w2tan_vals.shape =  (len(w),len(w))\n    h_vals.shape = (len(w),len(w))\n\n    ### plot function ###\n    ax2.plot_surface(w1_vals, w2_vals, g_vals, alpha = 0.5,color = 'w',rstride=25, cstride=25,linewidth=1,edgecolor = 'k',zorder = 2)\n\n    ### plot z=0 plane ###\n    ax2.plot_surface(w1_vals, w2_vals, g_vals*0, alpha = 0.1,color = 'w',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = 'k') \n\n    ### plot tangent plane ###\n    ax2.plot_surface(w1tan_vals, w2tan_vals, h_vals, alpha = 0.4,color = 'lime',zorder = 1,rstride=50, cstride=50,linewidth=1,edgecolor = 'k')     \n\n    # scatter tangency \n    ax2.scatter(w_val[0],w_val[1],g_val,s = 70,c = 'lime',edgecolor = 'k',linewidth = 2)\n    \n    ### clean up plot ###\n    # plot x and y axes, and clean up\n    ax2.xaxis.pane.fill = False\n    ax2.yaxis.pane.fill = False\n    ax2.zaxis.pane.fill = False\n\n    #ax2.xaxis.pane.set_edgecolor('white')\n    ax2.yaxis.pane.set_edgecolor('white')\n    ax2.zaxis.pane.set_edgecolor('white')\n\n    # remove axes lines and tickmarks\n    ax2.w_zaxis.line.set_lw(0.)\n    ax2.set_zticks([])\n    ax2.w_xaxis.line.set_lw(0.)\n    ax2.set_xticks([])\n    ax2.w_yaxis.line.set_lw(0.)\n    ax2.set_yticks([])\n\n    # set viewing angle\n    ax2.view_init(view[0],view[1])\n\n    # set vewing limits\n    wgap = (max(w) - min(w))*0.4\n    y = max(w) + wgap\n    ax2.set_xlim([-y,y])\n    ax2.set_ylim([-y,y])\n    \n    zmin = min(np.min(g_vals),-0.5)\n    zmax = max(np.max(g_vals),+0.5)\n    ax2.set_zlim([zmin,zmax])\n\n    # label plot\n    fontsize = 12\n    ax2.set_xlabel(r'$w_1$',fontsize = fontsize,labelpad = -30)\n    ax2.set_ylabel(r'$w_2$',fontsize = fontsize,rotation = 0,labelpad=-30)\n        \n    plt.show()\n\n    \ndef show_stationary_1func(func,**kwargs):\n    '''\n    Input one functions, draw each highlighting its stationary points \n    '''\n        \n    # define input space\n    wmax = -3\n    if 'wmax' in kwargs:\n        wmax = kwargs['wmax']\n    w = np.linspace(-wmax,wmax,5000)                  # input range for original function\n\n    # construct figure\n    fig = plt.figure(figsize = (6,3))\n          \n    # remove whitespace from figure\n    #fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n    fig.subplots_adjust(wspace=0.3,hspace=0.4)\n       \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n  \n    ###### draw function, tangent lines, etc., ######\n    ax = plt.subplot(gs[0]); \n    ax2 =  plt.subplot(gs[1],sharey=ax);  \n\n    # generate a range of values over which to plot input function, and derivatives\n    g_plot = func(w)\n    grad = compute_grad(func)\n    grad_plot = np.array([grad(s) for s in w])\n    wgap = (max(w) - min(w))*0.1\n    ggap = (max(g_plot) - min(g_plot))*0.1\n    grad_gap = (max(grad_plot) - min(grad_plot))*0.1\n        \n    # plot first in top panel, derivative in bottom panel\n    ax.plot(w,g_plot,color = 'k',zorder = 1,linewidth=2)  \n    ax.set_title(r'$g(w)$',fontsize = 12)\n    ax.set_xlim([min(w)-wgap,max(w)+wgap])\n    ax.set_ylim([min(g_plot) - ggap, max(g_plot) + ggap])\n        \n    # plot function with stationary points marked \n    ax2.plot(w,g_plot,color = 'k',zorder = 1,linewidth = 2) \n    ax2.set_title(r'$g(w)$',fontsize = 12)\n    ax2.set_ylim([min(g_plot) - ggap, max(g_plot) + ggap])\n\n    # clean up and label axes \n    ax.tick_params(labelsize=6)\n    ax2.tick_params(labelsize=6)\n\n    # determine zero derivative points 'visually'\n    grad_station = copy.deepcopy(grad_plot)\n    grad_station = np.sign(grad_station)\n    ind = []\n    for i in range(len(grad_station)-1):\n        pt1 = grad_station[i]\n        pt2 = grad_station[i+1]\n        plot_pt1 = grad_plot[i]\n        plot_pt2 = grad_plot[i+1]\n\n        # if either point is zero add to list\n        if pt1 == 0 or abs(plot_pt1) < 10**-5:\n            ind.append(i)\n        if pt2 == 0:\n            ind.append(i+1)\n\n        # if grad difference is small then sign change has taken place, add to list\n        gap = abs(pt1 + pt2)\n        if gap < 2 and pt1 !=0 and pt2 != 0:\n            ind.append(i)\n\n    # keep unique pts\n    ind = np.unique(ind)\n        \n    # plot the input/output tangency points and tangent line\n    wtan = np.linspace(-1,1,500)                  # input range for original function\n    for pt in ind:\n        # plot point\n        w_val = w[pt]\n        g_val = func(w_val)\n        grad_val = grad(w_val)\n        ax2.scatter(w_val,g_val,s = 40,c = 'lime',edgecolor = 'k',linewidth = 2,zorder = 3)            # plot point of tangency\n    plt.show()\n    \n    \n    \ndef show_stationary(func1,func2,func3,**kwargs):\n    '''\n    Input three functions, draw each highlighting their stationary points and draw tangent lines, mark evaluations on first derivative as well\n    '''\n        \n    # define input space\n    w = np.linspace(-3,3,5000)                  # input range for original function\n    if 'w' in kwargs:\n        w = kwargs['w']\n\n    # construct figure\n    fig = plt.figure(figsize = (7,5))\n          \n    # remove whitespace from figure\n    #fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n    fig.subplots_adjust(wspace=0.3,hspace=0.4)\n       \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(2, 3, width_ratios=[1,1,1]) \n  \n    ###### draw function, tangent lines, etc., ######\n    for k in range(3):\n        ax = plt.subplot(gs[k]); \n        ax2 =  plt.subplot(gs[k+3],sharex=ax);  \n        \n        func = func1\n        if k == 1:\n            func = func2\n        if k == 2:\n            func = func3\n\n        # generate a range of values over which to plot input function, and derivatives\n        g_plot = func(w)\n        grad = compute_grad(func)\n        grad_plot = np.array([grad(s) for s in w])\n        wgap = (max(w) - min(w))*0.1\n        ggap = (max(g_plot) - min(g_plot))*0.1\n        grad_gap = (max(grad_plot) - min(grad_plot))*0.1\n        \n        # plot first in top panel, derivative in bottom panel\n        ax.plot(w,g_plot,color = 'k',zorder = 1,linewidth=2)   \n        ax.set_title(r'$g(w)$',fontsize = 12)\n        ax.set_xlim([min(w)-wgap,max(w)+wgap])\n        ax.set_ylim([min(g_plot) - ggap, max(g_plot) + ggap])\n        \n        # plot derivative and horizontal axis\n        ax2.plot(w,grad_plot,color = 'k',zorder = 1,linewidth = 2) \n        ax2.plot(w,grad_plot*0,color = 'k',zorder = 1,linewidth = 1,linestyle = '--') \n        ax2.set_title(r'$\\frac{\\mathrm{d}}{\\mathrm{d}w}g(w)$',fontsize = 12)\n        ax2.set_ylim([min(grad_plot) - grad_gap, max(grad_plot) + grad_gap])\n\n        # clean up and label axes \n        ax.tick_params(labelsize=6)\n        ax2.tick_params(labelsize=6)\n\n        # determine zero derivative points 'visually'\n        grad_station = copy.deepcopy(grad_plot)\n        grad_station = np.sign(grad_station)\n        ind = []\n        for i in range(len(grad_station)-1):\n            pt1 = grad_station[i]\n            pt2 = grad_station[i+1]\n            plot_pt1 = grad_plot[i]\n            plot_pt2 = grad_plot[i+1]\n\n            # if either point is zero add to list\n            if pt1 == 0 or abs(plot_pt1) < 10**-5:\n                ind.append(i)\n            if pt2 == 0:\n                ind.append(i+1)\n\n            # if grad difference is small then sign change has taken place, add to list\n            gap = abs(pt1 + pt2)\n            if gap < 2 and pt1 !=0 and pt2 != 0:\n                ind.append(i)\n\n        # keep unique pts\n        ind = np.unique(ind)\n        \n        # plot the input/output tangency points and tangent line\n        wtan = np.linspace(-1,1,500)                  # input range for original function\n        for pt in ind:\n            # plot point\n            w_val = w[pt]\n            g_val = func(w_val)\n            grad_val = grad(w_val)\n            ax.scatter(w_val,g_val,s = 40,c = 'lime',edgecolor = 'k',linewidth = 2,zorder = 3)            # plot point of tangency\n            ax2.scatter(w_val,grad_val,s = 40,c = 'lime',edgecolor = 'k',linewidth = 2,zorder = 3)            # plot point of tangency\n\n            # plot tangent line in original space\n            w1 = w_val - 1\n            w2 = w_val + 1\n            wrange = np.linspace(w1,w2, 100)\n            h = g_val + 0*(wrange - w_val)\n            ax.plot(wrange,h,color = 'lime',alpha = 0.5,linewidth = 1.5,zorder = 2)      # plot approx\n    plt.show()\n    \ndef show_stationary_v2(func1,func2,func3,**kwargs):\n    '''\n    Input three functions, draw each highlighting their stationary points and draw tangent lines, draw the first and second derivatives stationary point evaluations  on each as well\n    '''\n        \n    # define input space\n    w = np.linspace(-3,3,5000)                  # input range for original function\n    if 'w' in kwargs:\n        w = kwargs['w']\n\n    # construct figure\n    fig = plt.figure(figsize = (7,5))\n          \n    # remove whitespace from figure\n    #fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n    fig.subplots_adjust(wspace=0.2,hspace=0.8)\n       \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(3, 3, width_ratios=[1,1,1]) \n  \n    ###### draw function, tangent lines, etc., ######\n    for k in range(3):\n        ax = plt.subplot(gs[k]); \n        ax2 =  plt.subplot(gs[k+3],sharex=ax);  \n        ax3 =  plt.subplot(gs[k+6],sharex=ax);  \n        \n        func = func1\n        if k == 1:\n            func = func2\n        if k == 2:\n            func = func3\n\n        # generate a range of values over which to plot input function, and derivatives\n        g_plot = func(w)\n        grad = compute_grad(func)\n        grad_plot = np.array([grad(s) for s in w])\n        wgap = (max(w) - min(w))*0.1\n        ggap = (max(g_plot) - min(g_plot))*0.1\n        grad_gap = (max(grad_plot) - min(grad_plot))*0.1\n\n        hess = compute_grad(grad)\n        hess_plot = np.array([hess(s) for s in w])\n        hess_gap = (max(hess_plot) - min(hess_plot))*0.1\n            \n        # plot first in top panel, derivative in bottom panel\n        ax.plot(w,g_plot,color = 'k',zorder = 1,linewidth=2)   \n        ax.set_title(r'$g(w)$',fontsize = 12)\n        ax.set_xlim([min(w)-wgap,max(w)+wgap])\n        ax.set_ylim([min(g_plot) - ggap, max(g_plot) + ggap])\n        \n        # plot derivative and horizontal axis\n        ax2.plot(w,grad_plot,color = 'k',zorder = 1,linewidth = 2) \n        ax2.plot(w,grad_plot*0,color = 'k',zorder = 1,linewidth = 1,linestyle = '--') \n        ax2.set_title(r'$\\frac{\\mathrm{d}}{\\mathrm{d}w}g(w)$',fontsize = 12)\n        ax2.set_ylim([min(grad_plot) - grad_gap, max(grad_plot) + grad_gap])\n\n        # plot second derivative and horizontal axis\n        ax3.plot(w,hess_plot,color = 'k',zorder = 1,linewidth = 2) \n        ax3.plot(w,hess_plot*0,color = 'k',zorder = 1,linewidth = 1,linestyle = '--') \n        ax3.set_title(r'$\\frac{\\mathrm{d}^2}{\\mathrm{d}w^2}g(w)$',fontsize = 12)\n        ax3.set_ylim([min(hess_plot) - hess_gap, max(hess_plot) + hess_gap])\n       \n        # clean up and label axes \n        ax.tick_params(labelsize=6)\n        ax2.tick_params(labelsize=6)\n        ax3.tick_params(labelsize=6)\n\n        # determine zero derivative points 'visually'\n        grad_station = copy.deepcopy(grad_plot)\n        grad_station = np.sign(grad_station)\n        ind = []\n        for i in range(len(grad_station)-1):\n            pt1 = grad_station[i]\n            pt2 = grad_station[i+1]\n            plot_pt1 = grad_plot[i]\n            plot_pt2 = grad_plot[i+1]\n\n            # if either point is zero add to list\n            if pt1 == 0 or abs(plot_pt1) < 10**-5:\n                ind.append(i)\n            if pt2 == 0:\n                ind.append(i+1)\n\n            # if grad difference is small then sign change has taken place, add to list\n            gap = abs(pt1 + pt2)\n            if gap < 2 and pt1 !=0 and pt2 != 0:\n                ind.append(i)\n\n        # keep unique pts\n        ind = np.unique(ind)\n        \n        # plot the input/output tangency points and tangent line\n        wtan = np.linspace(-1,1,500)                  # input range for original function\n        for pt in ind:\n            # plot point\n            w_val = w[pt]\n            g_val = func(w_val)\n            grad_val = grad(w_val)\n            hess_val = hess(w_val)\n            ax.scatter(w_val,g_val,s = 40,c = 'lime',edgecolor = 'k',linewidth = 2,zorder = 3)            # plot point of tangency\n            ax2.scatter(w_val,grad_val,s = 40,c = 'lime',edgecolor = 'k',linewidth = 2,zorder = 3)            # plot point of tangency\n            ax3.scatter(w_val,hess_val,s = 40,c = 'lime',edgecolor = 'k',linewidth = 2,zorder = 3)            # plot point of tangency\n            \n            # plot tangent line in original space\n            w1 = w_val - 1\n            w2 = w_val + 1\n            wrange = np.linspace(w1,w2, 100)\n            h = g_val + 0*(wrange - w_val)\n            ax.plot(wrange,h,color = 'lime',alpha = 0.5,linewidth = 1.5,zorder = 2)      # plot approx\n    plt.show()\n                \n   """
mlrefined_libraries/calculus_library/display_derivative_functions.py,1,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\n\n# For any input function display its first three derivative functions.\nclass visualizer:\n    '''\n    For any input function display its first three derivative functions.\n    '''\n    def __init__(self,**args):\n        self.g = args['g']                                              # input function\n        self.first_derivative = compute_grad(self.g)                    # first derivative of input function\n        self.second_derivative = compute_grad(self.first_derivative)    # second derivative of input function\n        self.third_derivative = compute_grad(self.second_derivative)    # third derivative of input function\n        self.fourth_derivative = compute_grad(self.third_derivative)    # fourth derivative of input function\n\n        self.colors = [[0,1,0.25],[0,0.75,1],[1,0.75,0],[1,0,0.75]]     # custom colors\n\n    # draw the derivative functions\n    def draw_it(self,**args):\n        # how many frames to plot\n        num_frames = 5\n        if 'num_frames' in args:\n            num_frames = args['num_frames']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (12,3))\n        artist = fig\n   \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 4, width_ratios=[1,1,1,1]) \n  \n        ### draw 2d version ###\n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]); \n        ax3 = plt.subplot(gs[2]); \n        ax4 = plt.subplot(gs[3]); \n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(-3,3,200)       # range to plot over\n        g_plot = self.g(w_plot)\n        \n        # create first, second, and third derivatives to plot\n        g_first_der = []\n        g_second_der = []\n        g_third_der = []\n        for w in w_plot:\n            g_first_der.append(self.first_derivative(w))\n            g_second_der.append(self.second_derivative(w))\n            g_third_der.append(self.third_derivative(w))\n            \n        # plot function in each panel\n        ax1.plot(w_plot,g_plot,color = 'k',zorder = 0)                           \n        ax2.plot(w_plot,g_first_der,color = self.colors[0],zorder = 0)                         \n        ax3.plot(w_plot,g_second_der,color = self.colors[1],zorder = 0)                       \n        ax4.plot(w_plot,g_third_der,color = self.colors[2],zorder = 0)                       \n  \n        #### fix viewing limits ####\n        ax1.set_xlim([-3,3])\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.5\n        ax1.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n        ax1.set_yticks([],[])\n        ax1.set_title('original function',fontsize = 12)\n\n        ax2.set_xlim([-3,3])\n        g_range = max(g_first_der) - min(g_first_der)\n        ggap = g_range*0.5\n        ax2.set_ylim([min(g_first_der) - ggap,max(g_first_der) + ggap])\n        ax2.set_yticks([],[])\n        ax2.set_title('first derivative',fontsize = 12)\n\n        ax3.set_xlim([-3,3])\n        g_range = max(g_second_der) - min(g_second_der)\n        ggap = g_range*0.5\n        ax3.set_ylim([min(g_second_der) - ggap,max(g_second_der) + ggap])\n        ax3.set_yticks([],[])\n        ax3.set_title('second derivative',fontsize = 12)\n\n        ax4.set_xlim([-3,3])\n        g_range = max(g_third_der) - min(g_third_der)\n        ggap = g_range*0.5\n        ax4.set_ylim([min(g_third_der) - ggap,max(g_third_der) + ggap])\n        ax4.set_yticks([],[])\n        ax4.set_title('third derivative',fontsize = 12)\n        \n        plt.show()"""
mlrefined_libraries/calculus_library/forward_mode.py,7,"b""import numpy as np\n\nclass MyTuple:\n    '''\n    The basic object representing the input variable 'w'\n    represents the core of our AD calculator.  An instance \n    of this class is a tuple containining one function/derivative\n    evaluation of the variable 'w'.  Because it is meant to \n    represent the simple variable 'w' the derivative 'der' is\n    preset to 1.  The value 'val' can be set to 0 by default.  \n    '''\n    def __init__(self,**kwargs):\n        # variables for the value (val) and derivative (der) of our input function \n        self.val = 0\n        self.der = 1    \n        \n        # re-assign these default values \n        if 'val' in kwargs:\n            self.val = kwargs['val']\n        if 'der' in kwargs:\n            self.der = kwargs['der']\n            \n##### basic arithmetic functions #####         \n### our implementation of the addition rules from Table 2 ###\ndef add(a,b):\n    # Create output evaluation and derivative object\n    c = MyTuple()\n    \n    # switch to determine if a or b is a constant\n    if type(a) != MyTuple:\n        c.val = a + b.val\n        c.der = b.der\n    elif type(b) != MyTuple:\n        c.val = a.val + b\n        c.der = a.der\n    else: # both inputs are MyTuple objects, i.e., functions\n        c.val = a.val + b.val\n        c.der = a.der + b.der\n    \n    # Return updated object\n    return c\n\n# this next line overloads the addition operator for our MyTuple objects, or in other words adds the 'add' function to our MyTuple class definition on the fly\nMyTuple.__add__ = add\n\n# overload the reverse direction so that a + b = b + a\nMyTuple.__radd__ = add\n\n### our implementation of the addition rules from Table 2 ###\ndef multiply(a,b):\n    # Create output evaluation and derivative object\n    c = MyTuple()\n\n    # switch to determine if a or b is a constant\n    if type(a) != MyTuple:\n        c.val = a*b.val\n        c.der = a*b.der\n    elif type(b) != MyTuple:\n        c.val = a.val*b\n        c.der = a.der*b\n\n    else: # both inputs are MyTuple objects i.e., functions\n        c.val = a.val*b.val\n        c.der = a.der*b.val + a.val*b.der     # product rule\n    \n    # Return updated object\n    return c\n\n# create two MyTuple objects and try to use Python's built in function assigned to the * operator on them\nMyTuple.__mul__ = multiply\n\n# overload the 'reverse multiplication' so that a*b = b*a\nMyTuple.__rmul__ = multiply    \n\n### our implementation of the power rule from Table 1 ###\ndef power(a,n):\n    # Create output evaluation and derivative object\n    b = MyTuple()\n    \n    # Produce new function value\n    b.val = a.val**n\n\n    # Produce new derivative value - we need to use the chain rule here!\n    b.der = n*(a.val**(n-1))*a.der\n    \n    # Return updated object\n    return b\n\n# create two MyTuple objects and try to use Python's built in function assigned to the ** operator on them\nMyTuple.__pow__ = power\n\n##### elementary functions #####         \n# our implementation of the sinusoid rule from Table 1\ndef log(a):\n    # Create output evaluation and derivative object\n    b = MyTuple()\n    \n    # Produce new function value\n    b.val = np.log(a.val)\n\n    # Produce new derivative value\n    b.der = (1/a.val)*a.der\n    \n    # Return updated object\n    return b\n\n# our implementation of the power rule from Table 1 \ndef tanh(a):\n    # Create output evaluation and derivative object\n    b = MyTuple()\n    \n    # Produce new function value\n    b.val = np.tanh(a.val)\n\n    # Produce new derivative value\n    b.der = (1 - np.tanh(a.val)**2)*a.der\n    \n    # Return updated object\n    return b\n\n# our implementation of the cosine rule from Table 1\ndef cos(a):\n    # Create output evaluation and derivative object\n    b = MyTuple()\n    \n    # Produce new function value\n    b.val = np.cos(a.val)\n\n    # Produce new derivative value - we need to use the chain rule here!\n    b.der = -np.sin(a.val)*a.der\n    \n    # Return updated object\n    return b\n\n# our implementation of the sinusoid rule from Table 1\ndef sin(a):\n    # Create output evaluation and derivative object\n    b = MyTuple()\n    \n    # Produce new function value\n    b.val = np.sin(a.val)\n\n    # Produce new derivative value - we need to use the chain rule here!\n    b.der = np.cos(a.val)*a.der\n    \n    # Return updated object\n    return b"""
mlrefined_libraries/calculus_library/function_derivative_joint_visualizer.py,4,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nimport time\nfrom matplotlib import gridspec\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport time\n\nclass visualizer:\n    '''\n    Using a single input function this illustrates the construction of its corresponding first derivative function using a slider mechanism.  As the slider is run left to right a point on the input function, along with its tangent line / derivative are shown in the left panel, while the value of the derivative is plotted simultaneously in the right panel.\n    '''\n    def __init__(self,**args):\n        self.g = args['g']                       # input function\n        self.grad = compute_grad(self.g)         # gradient of input function\n        self.colors = [[0,1,0.25],[0,0.75,1]]    # set of custom colors used for plotting\n\n    # compute first order approximation\n    def draw_it(self,**args):\n        num_frames = 300                          # number of slides to create - the input range [-3,3] is divided evenly by this number\n        if 'num_frames' in args:\n            num_frames = args['num_frames']\n        \n        # initialize figure\n        fig = plt.figure(figsize = (16,8))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1],wspace=0.3, hspace=0.05) \n        ax1 = plt.subplot(gs[0]);\n        ax2 = plt.subplot(gs[1]); \n\n        # generate a range of values over which to plot input function, and derivatives\n        w_plot = np.linspace(-3,3,200)                  # input range for original function\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)             # used for cleaning up final plot\n        ggap = g_range*0.5\n        w_vals = np.linspace(-2.5,2.5,num_frames)       # range of values over which to plot first / second order approximations\n        \n        # create the derivative function to plot, figure out its plotting range, etc.,\n        grad_plot = []\n        for w in w_plot:\n            grad_plot.append(self.grad(w))\n            \n        grad_range = max(grad_plot) - min(grad_plot)\n        grad_gap = grad_range*0.5\n        print ('beginning animation rendering...')\n               \n        # animation sub-function\n        def animate(k):\n            # clear the panel\n            ax1.cla()\n            ax2.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            # grab the next input/output tangency pair, the center of the next approximation(s)\n            w_val = w_vals[k]\n            g_val = self.g(w_val)\n            grad_val = self.grad(w_val)\n\n            #### left plot: plot the original function ####\n            ax1.plot(w_plot,g_plot,color = 'k',zorder = 3)                           # plot function\n            \n            # plot the input/output tangency point\n            ax1.scatter(w_val,g_val,s = 90,c = 'r',edgecolor = 'k',linewidth = 0.7,zorder = 4)            # plot point of tangency\n\n            # determine width to plot the approximation\n            w1 = w_val - 2\n            w2 = w_val + 2\n\n            # compute first order approximation\n            wrange = np.linspace(w1,w2, 100)\n            h = g_val + grad_val*(wrange - w_val)\n\n            # plot the first order approximation\n            ax1.plot(wrange,h,color = self.colors[0],linewidth = 2,zorder = 2)      # plot approx\n\n            # fix viewing limits on panel\n            ax1.set_xlim([-3,3])\n            ax1.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            \n            # label axes\n            ax1.set_xlabel('$w$',fontsize = 20)\n            ax1.set_ylabel('$g(w)$',fontsize = 20,rotation = 0,labelpad = 25)\n            \n            #### right plot: plot the derivative function ####\n            # get current values plotted so far\n            vals = w_vals[:k+1]\n            grad_vals = []\n            for w in vals:\n                grad_vals.append(self.grad(w))\n            \n            # plot all derivative values passed over so far, including current value\n            ax2.plot(vals,grad_vals,color = self.colors[0],zorder = 3)                           # plot function\n\n            # place marker on final point plotted\n            ax2.scatter(vals[-1],grad_vals[-1],s = 90,color = self.colors[0],edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n            \n            # fix viewing limits on panel\n            ax2.set_xlim([-3,3])\n            ax2.set_ylim([min(grad_plot) - grad_gap,max(grad_plot) + grad_gap])\n            \n            # label axes\n            ax2.set_xlabel('$w$',fontsize = 20)\n            ax2.set_ylabel(r'$\\frac{\\mathrm{d}}{\\mathrm{d}w}g(w)$',fontsize = 20,rotation = 0,labelpad = 25)\n            return artist,\n        \n        anim = animation.FuncAnimation(fig, animate,frames=len(w_vals), interval=len(w_vals), blit=True)\n        \n        return(anim)"""
mlrefined_libraries/calculus_library/numder_silder.py,2,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nimport time\nfrom matplotlib import gridspec\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\n\nclass visualizer:\n    '''\n    This slider toy allows you to experiment with the value of epsilon in the\n    definition of the numerical derivative affects its accuracy.\n    '''\n        \n    def __init__(self,**kwargs):\n        self.g = kwargs['g']                          # input function\n        self.grad = compute_grad(self.g)            # gradient of input function\n        self.colors = [[0,1,0.25],[0,0.75,1]]       # custom colors for visualization purposes\n                \n    # compute derivative approximation and return\n    def numerical_derivative(self, w,epsilon):\n        return (self.g(w+epsilon) - self.g(w))/epsilon\n        \n    # draw numerical derivative\n    def draw_it(self,**kwargs):\n        # number of frames to show in animation - evenly divides the input region [-3,3]\n            \n        epsilon_range = np.logspace(0, -17, 18)\n\n        # initialize figure\n        fig = plt.figure(figsize = (7,3))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 1, width_ratios=[1]) \n        ax = plt.subplot(gs[0]); ax.axis('off');\n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(-3,3,2000)          # input range for the original function\n        g_plot = self.g(w_plot)                 # original function evaluated over input region\n        true_grad = [self.grad(w) for w in w_plot]        # true derivative \n        \n        print ('starting animation rendering...')\n\n        # animation sub-function\n        def animate(k):\n            # clear the current slide\n            ax.cla()\n            \n            # print rendering update\n            if k == 17 - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n\n            if k == 0:\n                # plot original function\n                ax.plot(w_plot,g_plot,color = 'k',zorder = 0,label = 'function')  \n            \n            # plot everything on top of function after first slide\n            if k > 0:\n                # select epsilon\n                epsilon = epsilon_range[k-1]\n                \n                # plot original function\n                ax.plot(w_plot,g_plot,color = 'k',zorder = 0,label = 'function') \n                \n                # compute numerical derivative over input range\n                dervals = [self.numerical_derivative(w,epsilon) for w in w_plot]\n                \n                # plot numerical derivative\n                ax.plot(w_plot,dervals,color = 'r',zorder = 3,label = 'numerical derivative') \n                # plot numerical derivative\n                ax.plot(w_plot,true_grad,color = 'b',linestyle ='--',zorder = 2, label = 'true derivative') \n                \n                # set legend\n                h, l = ax.get_legend_handles_labels()\n                tra = '$\\epsilon = 10^{-' + str(k) + '}$'\n                ax.set_title(tra,fontsize=13)\n                ax.legend(bbox_to_anchor=[0, 0.9],loc='center', ncol=1,fontsize = 12)\n\n            # fix viewing limits\n            ax.set_xlim([-3,3])\n            \n            # label axes\n            ax.set_xlabel('$w$',fontsize = 12)\n            ax.set_ylabel('$g(w)$',fontsize = 12,rotation = 0,labelpad = 25)\n                \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate,frames=len(epsilon_range)+1, interval=len(epsilon_range)+1, blit=True)\n\n        return(anim)"""
mlrefined_libraries/calculus_library/perp_gradient_viewer.py,20,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad    \nimport autograd.numpy as np\nfrom autograd import hessian \nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\n\n'''\nThis method visualizes the contours of a function taking in two inputs.  Then for a set of input points\nthe gradient is computed (at each point) and drawn as an arrow on top of the contour plot\n'''\n\ndef rotation_matrix(theta):\n    return np.array([[np.cos(theta), -np.sin(theta)],[np.sin(theta),np.cos(theta)]]).T\n    \ndef illustrate_gradients(g,pts,**kwargs):   \n    # user defined args\n    pts_max = np.max(np.max(pts)) + 3\n    viewmax = max(3,pts_max)\n    colors = ['lime','magenta','orangered']\n\n    if 'viewmax' in kwargs:\n        viewmax = kwargs['viewmax']\n\n    num_contours = 15\n    if 'num_contours' in kwargs:\n        num_contours = kwargs['num_contours']  \n        \n    ##### setup figure to plot #####\n    # initialize figure\n    fig = plt.figure(figsize = (8,4))\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,1]) \n    ax1 = plt.subplot(gs[0]); ax1.axis('off')\n    ax2 = plt.subplot(gs[1]); ax2.set_aspect('equal')\n    ax3 = plt.subplot(gs[2]); ax3.axis('off')\n\n    ### compute gradient of input function ###\n    nabla_g = grad(g)\n    \n    # loop over points and determine levels\n    num_pts = pts.shape[1]\n    levels = []\n    for t in range(num_pts):\n        pt = pts[:,t]\n        g_val = g(pt)\n        levels.append(g_val)\n    levels = np.array(levels)\n    inds =  np.argsort(levels, axis=None)\n    pts = pts[:,inds]\n    levels = levels[inds]    \n    \n    # evaluate all input points through gradient function\n    grad_pts = []\n    num_pts = pts.shape[1]\n    for t in range(num_pts):\n        # point\n        color = colors[t]\n        pt = pts[:,t]\n        nabla_pt = nabla_g(pt)\n        nabla_pt /= np.linalg.norm(nabla_pt)\n        \n        # plot original points\n        ax2.scatter(pt[0],pt[1],s = 80,c = color,edgecolor = 'k',linewidth = 2,zorder = 3)\n\n        ### draw 2d arrow in right plot ###\n        # create gradient vector\n        grad_pt = pt - nabla_pt\n        \n        # plot gradient direction\n        scale = 0.3\n        arrow_pt = (grad_pt - pt)*0.78*viewmax*scale\n        ax2.arrow(pt[0],pt[1],arrow_pt[0],arrow_pt[1], head_width=0.1, head_length=0.1, fc='k', ec='k',linewidth=4,zorder = 2,length_includes_head=True)\n        ax2.arrow(pt[0],pt[1],arrow_pt[0],arrow_pt[1], head_width=0.1, head_length=0.1, fc=color, ec=color,linewidth=2.75,zorder = 2,length_includes_head=True)\n        \n        ### compute orthogonal line to contour ###\n        # compute slope of gradient direction\n        slope = float(arrow_pt[1])/float(arrow_pt[0])\n        perp_slope = -1/slope\n        perp_inter = pt[1] - perp_slope*pt[0]        \n        \n        # find points on orthog line approx 'scale' away in both directions (lazy quadratic formula)\n        scale = 1.5\n        s = np.linspace(pt[0] - 5, pt[0] + 5,1000)\n        y2 = perp_slope*s + perp_inter\n        dists = np.abs(((s - pt[0])**2 + (y2 - pt[1])**2)**0.5 - scale)\n        ind = np.argmin(dists)\n        x2 = s[ind]\n    \n        # plot tangent line to contour\n        if x2 < pt[0]:\n            s = np.linspace(x2,pt[0] + abs(x2 - pt[0]),200)\n        else:\n            s = np.linspace(pt[0] - abs(x2 - pt[0]),x2,200)\n\n        v = perp_slope*s + perp_inter\n        ax2.plot(s,v,zorder = 2,c = 'k',linewidth = 3)\n        ax2.plot(s,v,zorder = 2,c = colors[t],linewidth = 1)\n        \n    # generate viewing range \n    contour_plot(ax2,g,pts,viewmax,num_contours,colors,levels)\n    plt.show()\n\n### visualize contour plot of cost function ###\ndef contour_plot(ax,g,pts,wmax,num_contours,my_colors,pts_levels):\n    #### define input space for function and evaluate ####\n    w1 = np.linspace(-wmax,wmax,100)\n    w2 = np.linspace(-wmax,wmax,100)\n    w1_vals, w2_vals = np.meshgrid(w1,w2)\n    w1_vals.shape = (len(w1)**2,1)\n    w2_vals.shape = (len(w2)**2,1)\n    h = np.concatenate((w1_vals,w2_vals),axis=1)\n    func_vals = np.asarray([g(s) for s in h])\n    w1_vals.shape = (len(w1),len(w1))\n    w2_vals.shape = (len(w2),len(w2))\n    func_vals.shape = (len(w1),len(w2)) \n\n    ### make contour right plot - as well as horizontal and vertical axes ###\n    # set level ridges\n    levelmin = min(func_vals.flatten())\n    levelmax = max(func_vals.flatten())\n    cutoff = 0.3\n    cutoff = (levelmax - levelmin)*cutoff\n    numper = 3\n    levels1 = np.linspace(cutoff,levelmax,numper)\n    num_contours -= numper\n\n    ##### plot filled contours with generic contour lines #####\n    # produce generic contours\n    levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n    levels = np.unique(np.append(levels1,levels2))\n    num_contours -= numper\n    while num_contours > 0:\n        cutoff = levels[1]\n        levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n        levels = np.unique(np.append(levels2,levels))\n        num_contours -= numper\n    \n    # plot the contours\n    ax.contour(w1_vals, w2_vals, func_vals,levels = levels[1:],colors = 'k')\n    ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = 'Blues')\n    \n    ###### add contour curves based on input points #####\n    # add to this list the contours passing through input points\n    ax.contour(w1_vals, w2_vals, func_vals,levels = pts_levels,colors = 'k',linewidths = 3)\n    ax.contour(w1_vals, w2_vals, func_vals,levels = pts_levels,colors = my_colors, linewidths = 2.5)\n\n    ###### clean up plot ######\n    ax.set_xlabel('$w_0$',fontsize = 12)\n    ax.set_ylabel('$w_1$',fontsize = 12,rotation = 0)\n    ax.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n    ax.axvline(x=0, color='k',zorder = 0,linewidth = 0.5)"""
mlrefined_libraries/calculus_library/plotter.py,17,"b""import sys\nsys.path.append('../')\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import gridspec\nfrom IPython.display import display, HTML\nimport copy\n\n# a short function for plotting function and derivative values over a large range for input function g\ndef ad_derval_plot(MyTuple,g,**kwargs):\n    # specify range of input for our function and its derivative\n    w = np.linspace(-10,10,1000) \n    if 'w' in kwargs:\n        w = kwargs['w']\n  \n    # recurse to create higher order derivative object\n    order = 1\n    if 'order' in kwargs:\n        order = kwargs['order']\n    \n    # initialize objects\n    valder_objs = []\n    for u in w:\n        # loop over and make deep object for higher order derivatives\n        s = MyTuple(val = u)\n        for i in range(order-1):\n            s = MyTuple(val = s)\n        valder_objs.append(s)\n\n    # collect function and derivative values to plot\n    results = [g(w) for w in valder_objs]\n    \n    # loop over and collect final derivative value\n    g = []\n    dgdw = []\n    for r in results:\n        val = r.val\n        der = r.der\n        for i in range(order-1):\n            val = val.val\n            der = der.der\n        g.append(val)\n        dgdw.append(der)\n\n    # generate original function\n    function_table = np.stack((w,g), axis=1) \n\n    # generate derivative function\n    derivative_table = np.stack((w,dgdw), axis=1) \n\n    # use custom plotter to show both functions\n    ylabel_2 = r'$\\frac{\\mathrm{d}^' + str(order) +  '}{\\mathrm{d}w^' + str(order) +  '}g(w)$'\n    if order == 1: # remove superscripts from label\n        ylabel_2 = r'$\\frac{\\mathrm{d}}{\\mathrm{d}w}g(w)$' \n    \n    baslib.basics_plotter.double_plot(table1 = function_table, table2 = derivative_table,plot_type = 'continuous',xlabel = '$w$',ylabel_1 = '$g(w)$',ylabel_2 = ylabel_2,fontsize = 12)\n\n# plotter for function and derivative equations\ndef derval_eq_plot(g,dgdw,**kwargs):\n    # specify range of input for our function and its derivative\n    w = np.linspace(-10,10,1000) \n    if 'w' in kwargs:\n        w = kwargs['w']\n        \n    # make real function / derivative values\n    g_vals = g(w)\n    dgdw_vals = dgdw(w)\n\n    # generate original function\n    function_table = np.stack((w,g_vals), axis=1) \n\n    # generate derivative function\n    derivative_table = np.stack((w,dgdw_vals), axis=1) \n\n    # use custom plotter to show both functions\n    baslib.basics_plotter.double_plot(table1 = function_table, table2 = derivative_table,plot_type = 'continuous',xlabel = '$w$',ylabel_1 = '$g(w)$',ylabel_2 = r'$\\frac{\\mathrm{d}}{\\mathrm{d}w}g(w)$',fontsize = 14)\n    \n    \n# custom plot for spiffing up plot of a two mathematical functions\ndef double_2d_plot(func1,func2,**kwargs): \n    # get labeling arguments\n    xlabel = '$w$'\n    ylabel_1 = ''\n    ylabel_2 = ''\n    title1=  ''\n    title2 = ''\n    fontsize = 13\n    color = 'r'\n    w = np.linspace(-2,2,1000)\n    if 'xlabel' in kwargs:\n        xlabel = kwargs['xlabel']\n    if 'ylabel_1' in kwargs:\n        ylabel_1 = kwargs['ylabel_1']\n    if 'ylabel_2' in kwargs:\n        ylabel_2 = kwargs['ylabel_2']\n    if 'fontsize' in kwargs:\n        fontsize = kwargs['fontsize']\n    if 'title1' in kwargs:\n        title1 = kwargs['title1']\n    if 'title2' in kwargs:\n        title2 = kwargs['title2']\n    if 'w' in kwargs:\n        w = kwargs['w']\n    if 'color' in kwargs:\n        color = kwargs['color']\n        \n    # determine vertical plotting limit\n    f1 = func1(w)\n    f2 = func2(w)\n    ymax = max(max(f1),max(f2))\n    ymin = min(min(f1),min(f2))\n    ygap = (ymax - ymin)*0.2\n    ymax += ygap\n    ymin -= ygap\n        \n    # plot the functions \n    fig = plt.figure(figsize = (8,3))\n    ax1 = fig.add_subplot(121); ax2 = fig.add_subplot(122);    \n    ax1.plot(w, f1, c=color, linewidth=2,zorder = 3)\n    ax2.plot(w, f2, c=color, linewidth=2,zorder = 3)\n\n    # plot x and y axes, and clean up\n    ax1.set_xlabel(xlabel,fontsize = fontsize)\n    ax1.set_ylabel(ylabel_1,fontsize = fontsize,rotation = 0,labelpad = 20)\n    ax2.set_xlabel(xlabel,fontsize = fontsize)\n    ax2.set_ylabel(ylabel_2,fontsize = fontsize,rotation = 0,labelpad = 20)\n    ax1.set_title(title1[1:])\n    ax2.set_title(title2[1:])\n    ax1.set_ylim([ymin,ymax])\n    ax2.set_ylim([ymin,ymax])\n    \n    ax1.grid(True, which='both'), ax2.grid(True, which='both')\n    ax1.axhline(y=0, color='k', linewidth=1), ax2.axhline(y=0, color='k', linewidth=1)\n    ax1.axvline(x=0, color='k', linewidth=1), ax2.axvline(x=0, color='k', linewidth=1)\n    plt.show()\n    \n# custom plot for spiffing up plot of a two mathematical functions\ndef double_2d3d_plot(func1,func2,**kwargs): \n    # get labeling arguments\n    xlabel = '$w$'\n    ylabel_1 = ''\n    ylabel_2 = ''\n    title1=  ''\n    title2 = ''\n    fontsize = 15\n    color = 'r'\n    if 'fontsize' in kwargs:\n        fontsize = kwargs['fontsize']\n    if 'title1' in kwargs:\n        title1 = kwargs['title1']\n    if 'title2' in kwargs:\n        title2 = kwargs['title2']\n    if 'w' in kwargs:\n        w = kwargs['w']\n    if 'color' in kwargs:\n        color = kwargs['color']\n        \n    # determine vertical plotting limit\n    w = np.linspace(-2,2,500)\n    xx,yy = np.meshgrid(w,w)\n    xx.shape = (xx.size,1)\n    yy.shape = (yy.size,1)\n    w3d = np.concatenate((xx,yy),axis=1)\n    f1 = func1(w)\n    f2 = func2(w3d.T)\n    xx.shape = (500,500)\n    yy.shape = (500,500)\n    f2.shape = (500,500)\n        \n    # plot the functions \n    fig = plt.figure(figsize = (8,4))\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n    ax1 = plt.subplot(gs[0]);\n    ax2 = plt.subplot(gs[1],projection='3d'); \n    \n    \n    ax1.plot(w, f1, c=color, linewidth=2,zorder = 3)\n    ax2.plot_surface(xx, yy, f2, alpha = 0.3,color = color,rstride=50, cstride=50,linewidth=2,edgecolor = 'k')\n        \n    # plot x and y axes, and clean up\n    ax1.set_xlabel(xlabel,fontsize = fontsize)\n    ax1.set_ylabel(ylabel_1,fontsize = fontsize,rotation = 0,labelpad = 20)\n    ax2.set_xlabel(r'$w_1$',fontsize = fontsize,labelpad = 10)\n    ax2.set_ylabel(r'$w_2$',fontsize = fontsize,rotation = 0,labelpad = 20)\n    ax2.set_yticks(np.arange(min(w), max(w)+1, 1.0))\n    ax1.set_title(title1[1:])\n    ax2.set_title(title2[:],y=1.08)\n    ax2.view_init(20,-60)\n    \n    ax1.grid(True, which='both'), ax2.grid(True, which='both')\n    ax1.axhline(y=0, color='k', linewidth=1)\n    ax1.axvline(x=0, color='k', linewidth=1)\n    plt.show()\n    \n# custom plot for spiffing up plot of a two mathematical functions\ndef triple_3dsum_plot(func1,func2,**kwargs): \n    # get labeling arguments\n    xlabel = '$w$'\n    ylabel_1 = ''\n    ylabel_2 = ''\n    title1=  ''\n    title2 = ''\n    title3 = ''\n    fontsize = 15\n    if 'fontsize' in kwargs:\n        fontsize = kwargs['fontsize']\n    if 'title1' in kwargs:\n        title1 = kwargs['title1']\n    if 'title2' in kwargs:\n        title2 = kwargs['title2']\n    if 'title3' in kwargs:\n        title3 = kwargs['title3']\n        \n    if 'w' in kwargs:\n        w = kwargs['w']\n        \n    # determine vertical plotting limit\n    w = np.linspace(-2,2,500)\n    xx,yy = np.meshgrid(w,w)\n    xx.shape = (xx.size,1)\n    yy.shape = (yy.size,1)\n    w3d = np.concatenate((xx,yy),axis=1)\n    f1 = func1(w3d.T)\n    f2 = func2(w3d.T)\n    xx.shape = (500,500)\n    yy.shape = (500,500)\n    f1.shape = (500,500)\n    f2.shape = (500,500)\n        \n    # plot the functions \n    fig = plt.figure(figsize = (15,4))\n    plt.style.use('ggplot')\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,1,1]) \n    ax1 = plt.subplot(gs[0],projection='3d');\n    ax2 = plt.subplot(gs[1],projection='3d'); \n    ax3 = plt.subplot(gs[2],projection='3d'); \n   \n    # plot surfaces\n    ax1.plot_surface(xx, yy, f1, alpha = 0.3,color = 'r',rstride=50, cstride=50,linewidth=2,edgecolor = 'k')\n    ax2.plot_surface(xx, yy, f2, alpha = 0.3,color = 'r',rstride=50, cstride=50,linewidth=2,edgecolor = 'k')\n    ax3.plot_surface(xx, yy,f1 + f2, alpha = 0.3,color = 'r',rstride=50, cstride=50,linewidth=2,edgecolor = 'k')\n        \n    # plot x and y axes, and clean up\n    ax1.set_xlabel(r'$w_1$',fontsize = fontsize,labelpad = 5)\n    ax1.set_ylabel(r'$w_2$',fontsize = fontsize,rotation = 0,labelpad = 5)\n    ax1.set_yticks(np.arange(min(w), max(w)+1, 1.0))\n    ax1.set_title(title1[:],y=1.08)\n    ax1.view_init(20,-60)\n    \n    ax2.set_xlabel(r'$w_1$',fontsize = fontsize,labelpad = 5)\n    ax2.set_ylabel(r'$w_2$',fontsize = fontsize,rotation = 0,labelpad = 5)\n    ax2.set_yticks(np.arange(min(w), max(w)+1, 1.0))\n    ax2.set_title(title2[:],y=1.08)\n    ax2.view_init(20,-60)\n    \n    ax3.set_xlabel(r'$w_1$',fontsize = fontsize,labelpad = 5)\n    ax3.set_ylabel(r'$w_2$',fontsize = fontsize,rotation = 0,labelpad = 5)\n    ax3.set_yticks(np.arange(min(w), max(w)+1, 1.0))\n    ax3.set_title(title3[:],y=1.08)\n    ax3.view_init(20,-60)\n    \n    plt.show()"""
mlrefined_libraries/calculus_library/secant_to_tangent.py,6,"b""# import standard plotting and animation\nfrom IPython.display import clear_output\nimport time\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nfrom matplotlib import gridspec\n\n# 3d function\nfrom mpl_toolkits.mplot3d import proj3d\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\n\n# go from a secant to tangent line at a pre-defined point self.w_init\nclass visualizer:\n    '''\n    Using the input anchor point self.w_init, peruse over a course set of other points\n    in a neighborhood around the anchor, drawing the secant line passing through the anchor and each\n    such neighboring point.  When the neighboring point == the anchor point the secant\n    line becomes a tangent one, and this is shown graphically.  Peruse the various secant lines using\n    a custom slider widget.\n    '''\n        \n    def __init__(self,**kwargs):\n        self.g = kwargs['g']                          # input function\n        self.grad = compute_grad(self.g)            # gradient of input function\n        self.colors = [[0,1,0.25],[0,0.75,1]]       # custom colors for visualization purposes\n\n    # animate the secant line passing through self.w_init and all surrounding points\n    def draw_it(self,**kwargs):\n        # number of frames to show in animation - evenly divides the input region [-3,3]\n        num_frames = 100\n        if 'num_frames' in kwargs:\n            num_frames = kwargs['num_frames']\n            \n        self.w_init = 0\n        if 'w_init' in kwargs:\n            self.w_init = kwargs['w_init']                # input point - where we draw the tangent line\n\n        self.mark_tangent = True\n        if 'mark_tangent' in kwargs:\n            self.mark_tangent = kwargs['mark_tangent']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (16,8))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,2, 1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis('off');\n        ax3 = plt.subplot(gs[2]); ax3.axis('off');\n\n        # plot input function\n        ax = plt.subplot(gs[1])\n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(-3,3,200)          # input range for the original function\n        g_plot = self.g(w_plot)                 # original function evaluated over input region\n        g_range = max(g_plot) - min(g_plot)     # metric used for adjusting plotting area of panel\n        ggap = g_range*0.5\n        \n        # define values over which to draw secant line connecting to (self.w_init,self.g(self.w_init))\n        w_vals = np.linspace(max(-2.6 + self.w_init,-3),min(2.6 + self.w_init,3),num_frames)\n        \n        # re-assign w_init to closest point in this range (for animation purposes)\n        ind = np.argmin((w_vals - self.w_init)**2)\n        self.w_init = w_vals[ind]\n        print ('starting animation rendering...')\n\n        # animation sub-function\n        def animate(k):\n            # clear the current slide\n            ax.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n\n            # plot original function\n            g_init = self.g(self.w_init)\n            ax.plot(w_plot,g_plot,color = 'k',zorder = 0) \n            \n            # plot the chosen point\n            ax.scatter(self.w_init,g_init,s = 120,c = self.colors[0],edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency  \n            \n            # plot everything on top of function after first slide\n            if k > 0:\n                # current second point through which we draw the secant line passing through (self.w_init, self.g(self.w_init))\n                w_val = w_vals[k-1]\n                g_val = self.g(w_val)\n            \n                # make slope of the current secant line\n                slope = 0\n                line_color = 'k'\n                \n                # switch colors and guides - when the secant point becomes the tangent point remove the vertical guides and color the line green\n                line_color = 'r'\n                if abs(self.w_init - w_val) > 10**-6:\n                    slope = (g_init - g_val)/float(self.w_init - w_val)\n                    \n                    # plot vertical guiding lines at w_init and w_val\n                    s = np.linspace(min(g_plot) - ggap,max(g_plot) + ggap,100)\n                    o = np.ones(100)\n                    ax.plot(o*w_val,s,linewidth = 1,alpha = 0.3,color = 'k',linestyle = '--')\n                    ax.plot(o*self.w_init,s,linewidth = 1,alpha = 0.3,color = 'k',linestyle = '--')\n                \n                elif abs(self.w_init - w_val) < 10**-6 and self.mark_tangent == True:\n                    slope = self.grad(self.w_init)\n                    line_color = self.colors[0]\n\n                # use point-slope form to create line (here the pt = (w_val,g_val))\n                h = g_val + slope*(w_plot - w_val)\n\n                # plot the approximation\n                ax.plot(w_plot,h,color = line_color,linewidth = 2,zorder = 1)      # plot approx\n \n                # plot other point of intersection of secant line and cost function\n                ax.scatter(w_val,g_val,s = 120,c = 'b',edgecolor = 'k',linewidth = 0.7,zorder = 2)            # plot point of tangency                \n                \n            # fix viewing limits\n            ax.set_xlim([-3,3])\n            ax.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            \n            # label axes\n            ax.set_xlabel('$w$',fontsize = 25)\n            ax.set_ylabel('$g(w)$',fontsize = 25,rotation = 0,labelpad = 25)\n                \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate,frames=len(w_vals)+1, interval=len(w_vals)+1, blit=True)\n\n        return(anim)"""
mlrefined_libraries/calculus_library/secant_to_tangent_3d.py,31,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nimport time\nfrom matplotlib import gridspec\nfrom mpl_toolkits.mplot3d import proj3d\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport copy\n        \n# function for producing fixed image of tangency along each input axis, along with full tangent hyperplane (first order Taylor Series approximation)\ndef draw_it(func,**kwargs):\n    view =  [33,50]\n    if 'view' in kwargs:\n        view = kwargs['view']\n    \n    # compute gradient, points\n    anchor = [0,0]\n    anchor = np.array([float(anchor[0]),float(anchor[1])])\n    anchor.shape = (2,1)\n    g_anchor = func(anchor)\n    \n    # file tracer\n    tracer = np.asarray([0,10**-5])\n    tracer = np.array([float(tracer[0]),float(tracer[1])])\n    tracer.shape = (2,1)\n    g_tracer = func(tracer) \n\n    # construct figure\n    fig = plt.figure(figsize = (9,3))\n    artist = fig\n    \n    # remove whitespace from figure\n    fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n    fig.subplots_adjust(wspace=0.01,hspace=0.01)\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,1,1]) \n    ax1 = plt.subplot(gs[0],projection='3d'); \n    ax2 = plt.subplot(gs[1],projection='3d');     \n    ax3 = plt.subplot(gs[2],projection='3d');     \n    \n    ### first panel - partial with respect to w_1 ###\n    # scatter anchor point \n    ax1.scatter(anchor[0],anchor[1],g_anchor,s = 50,c = 'lime',edgecolor = 'k',linewidth = 1) \n    \n    # plot hyperplane connecting the anchor to tracer\n    secant(func,anchor,tracer,ax1)\n                \n    # plot function\n    plot_func(func,view,ax1)\n    \n    ### second panel - partial with respect to w_2 ###\n    tracer = np.flipud(tracer)\n    \n    ax2.scatter(anchor[0],anchor[1],g_anchor,s = 50,c = 'lime',edgecolor = 'k',linewidth = 1) \n    \n    # plot hyperplane connecting the anchor to tracer\n    secant(func,anchor,tracer,ax2)\n                \n    # plot function\n    plot_func(func,view,ax2)    \n\n    ### third panel - plot full tangent hyperplane at anchor ###\n    ax3.scatter(anchor[0],anchor[1],g_anchor,s = 50,c = 'lime',edgecolor = 'k',linewidth = 1) \n    \n    # plot hyperplane connecting the anchor to tracer\n    tangent(func,anchor,ax3)\n                \n    # plot function\n    plot_func(func,view,ax3)    \n    \n    \n# main function for plotting individual axes tangent approximations\ndef animate_it(func,**kwargs):\n    view =  [33,50]\n    if 'view' in kwargs:\n        view = kwargs['view']\n        \n    num_frames = 10\n    if 'num_frames' in kwargs:\n        num_frames = kwargs['num_frames']\n    \n    # compute gradient, points\n    anchor = [0,0]\n    anchor = np.array([float(anchor[0]),float(anchor[1])])\n    anchor.shape = (2,1)\n    g_anchor = func(anchor)\n    \n    # compute tracer range\n    z = np.zeros((num_frames,1))\n    tracer_range = np.linspace(-2.5,2.5,num_frames)\n    ind = np.argmin(abs(tracer_range))\n    tracer_range[ind] = 10**-5\n    tracer_range.shape = (num_frames,1)\n    tracer_range = np.concatenate((tracer_range,z),axis=1)\n    \n    # construct figure\n    fig = plt.figure(figsize = (9,4))\n    artist = fig\n    \n    # remove whitespace from figure\n    fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n    fig.subplots_adjust(wspace=0.01,hspace=0.01)\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n    ax1 = plt.subplot(gs[0],projection='3d'); \n    ax2 = plt.subplot(gs[1],projection='3d'); \n    \n    # start animation\n    def animate(k):\n        # clear the panels\n        ax1.cla()\n        ax2.cla()\n        \n        # print rendering update\n        if np.mod(k+1,25) == 0:\n            print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n        if k == num_frames - 1:\n            print ('animation rendering complete!')\n            time.sleep(1.5)\n            clear_output()\n\n        if k > 0:\n            # pull current tracer\n            tracer = tracer_range[k-1]\n            tracer = np.array([float(tracer[0]),float(tracer[1])])\n            tracer.shape = (2,1)\n            g_tracer = func(tracer)\n                \n        ### draw 3d version ###\n        for ax in [ax1,ax2]:\n            # plot function\n            plot_func(func,view,ax)\n            \n            if k > 0:\n                # scatter anchor point \n                ax.scatter(anchor[0],anchor[1],g_anchor,s = 50,c = 'lime',edgecolor = 'k',linewidth = 1)  \n                # plot hyperplane connecting the anchor to tracer\n                secant(func,anchor,tracer,ax)\n                \n                # reset tracer\n                tracer = np.flipud(tracer)\n            \n        return artist,\n    \n    anim = animation.FuncAnimation(fig, animate,frames=num_frames+1, interval=num_frames+1, blit=True)\n        \n    return(anim)\n\n# plot secant hyperplane, as well as guides for both anchor and tracer point\ndef secant(func,anchor,tracer,ax): \n    # evaluate function at anchor and tracer\n    g_anchor = func(anchor)  \n    g_tracer = func(tracer) \n    anchor_orig = copy.deepcopy(anchor)\n    tracer_orig = copy.deepcopy(tracer)\n    \n    # determine non-zero component of tracer, compute slope of secant line\n    anchor = anchor.flatten()\n    tracer = tracer.flatten()\n    ind = np.argwhere(tracer != 0) \n    anchor = anchor[ind]\n    tracer = tracer[ind]\n   \n    # plot secant plane\n    color = 'lime'\n    if abs(anchor - tracer) > 10**-4:\n        # scatter tracer point\n        ax.scatter(tracer_orig[0],tracer_orig[1],g_tracer,s = 50,c = 'b',edgecolor = 'k',linewidth = 1)   \n        \n        # change color to red\n        color = 'r'\n        \n        # plot visual guide for tracer\n        w = np.linspace(0,g_tracer,100)\n        o = np.ones(100)\n        ax.plot(o*tracer_orig[0],o*tracer_orig[1],w,linewidth = 1.5,alpha = 1,color = 'k',linestyle = '--')\n\n        w = np.linspace(0,g_anchor,100)\n        o = np.ones(100)\n        ax.plot(o*anchor_orig[0],o*anchor_orig[1],w,linewidth = 1.5,alpha = 1,color = 'k',linestyle = '--')\n        \n    # compute slope of secant plane\n    slope = (g_anchor - g_tracer)/float(anchor - tracer)\n    \n    # create function for hyperplane connecting anchor to tracer\n    w_tan = np.linspace(-2.5,2.5,200)\n\n    w1tan_vals, w2tan_vals = np.meshgrid(w_tan,w_tan)\n    w1tan_vals.shape = (len(w_tan)**2,1)\n    w2tan_vals.shape = (len(w_tan)**2,1)\n    wtan_vals = np.concatenate((w1tan_vals,w2tan_vals),axis=1).T    \n    \n    # create tangent hyperplane formula, evaluate  \n    h = lambda w: g_anchor + slope*(w[ind] - anchor)\n    h_vals = h(wtan_vals) \n\n    # reshape everything and prep for plotting\n    w1tan_vals.shape = (len(w_tan),len(w_tan))\n    w2tan_vals.shape = (len(w_tan),len(w_tan))\n    h_vals.shape = (len(w_tan),len(w_tan))\n\n    # plot hyperplane and guides based on proximity of tracer to anchor\n    ax.plot_surface(w1tan_vals, w2tan_vals, h_vals, alpha = 0.2,color = color,zorder = 3,rstride=50, cstride=50,linewidth=0.5,edgecolor = 'k')  \n\n# form tangent hyperplane\ndef tangent(func,anchor,ax):\n    # compute gradient\n    grad = compute_grad(func)\n    grad_val = grad(anchor)\n    grad_val.shape = (2,1)  \n    g_val = func(anchor)\n    \n    # create input for tangent hyperplane\n    w_tan = np.linspace(-2.5,2.5,200)\n    w1tan_vals, w2tan_vals = np.meshgrid(w_tan,w_tan)\n    w1tan_vals.shape = (len(w_tan)**2,1)\n    w2tan_vals.shape = (len(w_tan)**2,1)\n    wtan_vals = np.concatenate((w1tan_vals,w2tan_vals),axis=1).T    \n    \n    # create tangent hyperplane formula, evaluate\n    h = lambda weh: g_val + (weh[0]-anchor[0])*grad_val[0] + (weh[1]-anchor[1])*grad_val[1]     \n    h_vals = h(wtan_vals + anchor)\n\n    # vals for tangent\n    w1tan_vals += anchor[0]\n    w2tan_vals += anchor[1]\n    w1tan_vals.shape = (len(w_tan),len(w_tan))\n    w2tan_vals.shape = (len(w_tan),len(w_tan))\n    h_vals.shape = (len(w_tan),len(w_tan))\n\n    ### plot tangent plane ###\n    ax.plot_surface(w1tan_vals, w2tan_vals, h_vals, alpha = 0.4,color = 'lime',zorder = 1,rstride=50, cstride=50,linewidth=0.5,edgecolor = 'k')      \n    \n# plot the input function and clean up panel\ndef plot_func(func,view,ax):\n    # define input space\n    w_func = np.linspace(-2.5,2.5,200)\n    w1_vals, w2_vals = np.meshgrid(w_func,w_func)\n    w1_vals.shape = (len(w_func)**2,1)\n    w2_vals.shape = (len(w_func)**2,1)\n    w_vals = np.concatenate((w1_vals,w2_vals),axis=1).T\n    g_vals = func(w_vals) \n    w1_vals.shape = (len(w_func),len(w_func))\n    w2_vals.shape = (len(w_func),len(w_func))\n    g_vals.shape = (len(w_func),len(w_func))\n    \n    ### plot function ###\n    ax.plot_surface(w1_vals, w2_vals, g_vals, alpha = 0.1,color = 'w',rstride=25, cstride=25,linewidth=0.75,edgecolor = 'k',zorder = 2)\n    \n    # clean up the plot while you're at it\n    cleanup(g_vals,view,ax)\n    \n# cleanup an input panel\ndef cleanup(g_vals,view,ax):\n    ### clean up plot ###\n    # plot x and y axes, and clean up\n    ax.xaxis.pane.fill = False\n    ax.yaxis.pane.fill = False\n    ax.zaxis.pane.fill = False\n\n    ax.xaxis.pane.set_edgecolor('white')\n    ax.yaxis.pane.set_edgecolor('white')\n    ax.zaxis.pane.set_edgecolor('white')\n\n    ### plot z=0 plane ###\n    w_zplane = np.linspace(-3,3,200)\n    w1_zplane_vals, w2_zplane_vals = np.meshgrid(w_zplane,w_zplane)\n    ax.plot_surface(w1_zplane_vals, w2_zplane_vals, np.zeros(np.shape(w1_zplane_vals)), alpha = 0.1,color = 'w',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = 'k') \n    \n    # bolden axis on z=0 plane\n    ax.plot(w_zplane,w_zplane*0,w_zplane*0,color = 'k',linewidth = 1.5)\n    ax.plot(w_zplane*0,w_zplane,w_zplane*0,color = 'k',linewidth = 1.5)\n\n    # remove axes lines and tickmarks\n    ax.w_zaxis.line.set_lw(0.)\n    ax.set_zticks([])\n    ax.w_xaxis.line.set_lw(0.)\n    ax.set_xticks([])\n    ax.w_yaxis.line.set_lw(0.)\n    ax.set_yticks([])\n\n    # set viewing angle\n    ax.view_init(view[0],view[1])\n\n    # set vewing limits\n    y = 3\n    ax.set_xlim([-y,y])\n    ax.set_ylim([-y,y])\n    zmin = min(np.min(g_vals),-0.5)\n    zmax = max(np.max(g_vals),+0.5)\n    ax.set_zlim([zmin,zmax])\n\n    # label plot\n    fontsize = 12\n    ax.set_xlabel(r'$w_1$',fontsize = fontsize,labelpad = -20)\n    ax.set_ylabel(r'$w_2$',fontsize = fontsize,rotation = 0,labelpad=-20)\n        """
mlrefined_libraries/calculus_library/second_order_convexity.py,5,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nimport time\nfrom matplotlib import gridspec\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\n\n# illustrate first and second order taylor series approximations to one-parameter input functions\nclass visualizer:\n    '''\n    Illustrate first and second order Taylor series approximations to a given input function at a\n    coarsely chosen set of points.  Transition between the points using a custom slider mechanism\n    to peruse how the approximations change from point-to-point.\n    '''\n    def __init__(self,**args):\n        self.g = args['g']                       # input function\n        self.grad = compute_grad(self.g)         # gradient of input function\n        self.hess = compute_grad(self.grad)      # hessian of input function\n        self.colors = [[0,1,0.25],[0,0.75,1]]    # set of custom colors used for plotting\n\n    # compute first order approximation\n    def draw_it(self,savepath,**kwargs):\n        num_frames = 300                          # number of slides to create - the input range [-3,3] is divided evenly by this number\n        if 'num_frames' in kwargs:\n            num_frames = kwargs['num_frames']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (16,8))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        fig.subplots_adjust(wspace=0.5,hspace=0.01)\n\n        # plot input function\n        ax = plt.subplot(gs[0])\n        ax2 = plt.subplot(gs[1])\n        \n        # range over which to plot second derivative\n        max_val = 2.5\n        if 'max_val' in kwargs:\n            max_val = kwargs['max_val']\n        w_vals = np.linspace(-max_val,max_val,num_frames)       # range of values over which to plot first / second order approximations\n        \n        # generate a range of values over which to plot input function, and derivatives\n        w_plot = np.linspace(-max_val,max_val,200)                  # input range for original function\n       \n        ### function evaluation and second derivative evaluation ###\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)             # used for cleaning up final plot\n        ggap = g_range*0.5\n        \n        hess_plot = np.array([self.hess(v) for v in w_vals])\n        hess_range = max(hess_plot) - min(hess_plot)\n        hess_gap = hess_range*0.25\n                 \n        # animation sub-function\n        print ('starting animation rendering...')\n        def animate(k):\n            # clear the panel\n            ax.cla()\n            ax2.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            # grab the next input/output tangency pair, the center of the next approximation(s)\n            w_val = w_vals[k]\n            g_val = self.g(w_val)\n            \n            ##### plot function and second order approximation in left panel #####\n            # plot original function\n            ax.plot(w_plot,g_plot,color = 'k',zorder = 0,linewidth=3)                           # plot function\n            \n            # plot the input/output tangency point\n            ax.scatter(w_val,g_val,s = 120,c = 'lime',edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n            \n            # label axes\n            ax.set_xlabel('$w$',fontsize = 25)\n            ax.set_ylabel(r'$g(w)$',fontsize=25,rotation = 0,labelpad = 50)\n            \n            # plug input value into the second derivative\n            g_grad_val = self.grad(w_val)\n            g_hess_val = self.hess(w_val)\n\n            # determine width of plotting area for second order approximator\n            width = 1\n            if g_hess_val < 0:\n                width = - width\n\n            # setup quadratic formula params\n            a = 0.5*g_hess_val\n            b = g_grad_val - 2*0.5*g_hess_val*w_val\n            c = 0.5*g_hess_val*w_val**2 - g_grad_val*w_val - width\n\n            # solve for zero points of the quadratic (for plotting purposes only)\n            w1 = (-b + math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n            w2 = (-b - math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n\n            # create the second order approximation\n            wrange = np.linspace(w1,w2, 100)\n            h = g_val + g_grad_val*(wrange - w_val) + 0.5*g_hess_val*(wrange - w_val)**2 \n\n            # plot the second order approximation\n            ax.plot(wrange,h,color = self.colors[1],linewidth = 5,zorder = 1)      # plot approx\n\n            # fix viewing limits on panel\n            ax.set_xlim([-max_val,max_val])\n            ax.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            \n            ##### plot second derivative value in right panel #####\n            ax2.plot(w_vals[:k+1],hess_plot[:k+1],color = self.colors[1],linewidth = 3,zorder = 1) \n            ax2.plot(w_plot,w_plot*0,color = 'k',zorder = 1,linewidth = 1,linestyle = '--') \n            \n            # fix viewing limits on panel\n            ax2.set_xlim([-max_val,max_val])\n            ax2.set_ylim([min(hess_plot) - hess_gap,max(hess_plot) + hess_gap])\n            ax2.set_xlabel('$w$',fontsize = 25)            \n            ax2.set_ylabel(r'$\\frac{\\partial^2}{\\partial w}g(w)$',fontsize=25,rotation = 0,labelpad = 50)           \n            \n            return artist,\n        \n        anim = animation.FuncAnimation(fig, animate,frames=len(w_vals), interval=len(w_vals), blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()"""
mlrefined_libraries/calculus_library/slope_visualizer.py,16,"b'import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import gridspec\nfrom IPython.display import display, HTML, clear_output\nimport copy\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\nfrom matplotlib import gridspec\nfrom matplotlib.ticker import FormatStrFormatter\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\nimport matplotlib.ticker as ticker\n\n# import custom JS animator\n# from mlrefined_libraries.JSAnimation_slider_only import IPython_display_slider_only\nimport time\nimport math\n\n### animate 2d slope visualization ###\n# animator for recursive function\ndef animate_visualize2d(func,num_frames,savepath,**kwargs):\n    # define input space\n    w = np.linspace(-10,10,500)\n    guides = \'on\'\n    \n    # define slopes\n    func_orig = func\n    s = func(1) - func(0)  # slope of input function\n    slopes = np.linspace(-abs(s),abs(s),num_frames)\n\n    # construct figure\n    fig = plt.figure(figsize = (12,4))\n    artist = fig\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,3, 1]) \n    ax1 = plt.subplot(gs[0]); ax1.axis(\'off\');\n    ax3 = plt.subplot(gs[2]); ax3.axis(\'off\');\n\n    # plot input function\n    ax2 = plt.subplot(gs[1])    \n\n    # animate\n    def animate(k):\n        # clear the panel\n        ax2.cla()\n        \n        # setup function\n        slope = slopes[k]\n        func = lambda w: slope*w + func_orig(0)\n        \n        # print rendering update\n        if np.mod(k+1,25) == 0:\n            print (\'rendering animation frame \' + str(k+1) + \' of \' + str(num_frames))\n        if k == num_frames - 1:\n            print (\'animation rendering complete!\')\n            time.sleep(1.5)\n            clear_output()\n            \n        # plot function\n        ax2.plot(w,func(w), c=\'lime\', linewidth=2,zorder = 3)\n     \n        ### plot slope as vector\n        if abs(func(1) - func(0)) > 0.2:\n            head_width = 0.166*(func(1) - func(0))\n            head_length = 0.25*(func(1) - func(0))\n        \n            # annotate arrow and annotation\n            if func(1)-func(0) > 0.1:\n                ax2.arrow(0, 0, func(1)-func(0),0, head_width=head_width, head_length=head_length, fc=\'k\', ec=\'k\',linewidth=2.5,zorder = 3)\n                \n                ax2.annotate(\'$b$\', xy=(2, 1), xytext=(func(1.3)-func(0),0),fontsize=15\n            )\n            elif func(1)-func(0) < -0.1:\n                ax2.arrow(0, 0, func(1)-func(0),0, head_width=-head_width, head_length=-head_length, fc=\'k\', ec=\'k\',linewidth=2.5,zorder = 3)\n                \n                ax2.annotate(\'$b$\', xy=(2, 1), xytext=(func(1.5)-func(0),0),fontsize=15\n            )\n                \n        ### plot negative slope as vector\n        if abs(func(1) - func(0)) >=0:\n            head_width = 0.166*(func(0) - func(1))\n            head_length = -0.25*(func(0) - func(1))\n\n            # annotate arrow and annotation\n            if func(1)-func(0) >= 0:\n                ax2.arrow(0, 0, func(0)-func(1),0, head_width=head_width, head_length=head_length, fc=\'r\', ec=\'r\',linewidth=2.5,zorder = 3)\n        \n                ax2.annotate(\'$-b$\', xy=(2, 1), xytext=(func(0) - func(1) - head_length - 0.7,0),fontsize=15)\n        \n            elif func(1)-func(0) < 0:\n                ax2.arrow(0, 0, func(0)-func(1),0, head_width=-head_width, head_length=-head_length, fc=\'r\', ec=\'r\',linewidth=2.5,zorder = 3)\n                \n                ax2.annotate(\'$-b$\', xy=(2, 1), xytext=( func(0) - func(1+0.3),0),fontsize=15)      \n                    \n        # set viewing limits\n        wgap = (max(w) - min(w))*0.3\n        ax2.set_xlim([-5,5])\n        ax2.set_ylim([-5,5])\n\n        # plot x and y axes, and clean up\n        ax2.grid(True, which=\'both\')\n        \n        # label plot\n        ax2.set_xlabel(\'$w$\',fontsize = 15)\n        ax2.set_ylabel(\'$g(w)$\',fontsize = 15,rotation = 0,labelpad = 20)\n        title = r\'$g(w) = {:.1f}\'.format(func_orig(0)) + \'+ {:.1f}\'.format(slope)+\'w$\'\n        if slope < 0:\n            title = r\'$g(w) = {:.1f}\'.format(func_orig(0)) + \'{:.1f}\'.format(slope)+\'w$\'\n\n        ax2.set_title(title,fontsize = 18)\n        return artist,        \n        \n    anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n\n    # produce animation and save\n    fps = 50\n    if \'fps\' in kwargs:\n        fps = kwargs[\'fps\']\n    anim.save(savepath, fps=fps, extra_args=[\'-vcodec\', \'libx264\'])\n    clear_output()        \n        \n    return(anim)\n\n### animate 3d slope visualization ###\n# animator for recursive function\ndef animate_visualize3d(func,savepath,**kwargs):\n    \n    num_frames = 10\n    if \'num_frames\' in kwargs:\n        num_frames = kwargs[\'num_frames\']\n    \n    view = [20,-50]\n    if \'view\' in kwargs:\n        view = kwargs[\'view\']\n       \n    # construct figure\n    fig = plt.figure(figsize = (6,6))\n    artist = fig\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 1, width_ratios=[1]) \n    ax = plt.subplot(gs[0],projection=\'3d\'); \n\n    # determine slope range of input function\n    func_orig = func\n    bias = func([0,0])\n    s = func([1,0]) - bias   # slope 1 of input\n    t = func([0,1]) - bias   # slope 2 of input\n    slopes1 = np.linspace(s,-s,num_frames)\n    slopes1.shape = (len(slopes1),1)\n    slopes2 = np.linspace(t,-t,num_frames)\n    slopes2.shape = (len(slopes2),1)\n    slopes = np.concatenate((slopes1,slopes2),axis=1)\n    \n    # define input space\n    w_in = np.linspace(-2,2,200)\n    w1_vals, w2_vals = np.meshgrid(w_in,w_in)\n    w1_vals.shape = (len(w_in)**2,1)\n    w2_vals.shape = (len(w_in)**2,1)\n    g_vals_orig = func([w1_vals,w2_vals]) \n    zmin = np.min(g_vals_orig)\n    zmax = np.max(g_vals_orig)\n    \n    # animate\n    def animate(k):\n        # clear the panel\n        ax.cla()\n        \n        # print rendering update\n        if np.mod(k+1,25) == 0:\n            print (\'rendering animation frame \' + str(k+1) + \' of \' + str(num_frames))\n        if k == num_frames - 1:\n            print (\'animation rendering complete!\')\n            time.sleep(1.5)\n            clear_output()\n            \n        # create mesh for surface\n        w1_vals.shape = (len(w_in)**2,1)\n        w2_vals.shape = (len(w_in)**2,1)\n        \n        # create and evaluate function\n        slope = slopes[k,:]\n        func = lambda w: slope[0]*w[0] + slope[1]*w[1]  + bias\n        g_vals = func([w1_vals,w2_vals]) \n\n        # vals for cost surface, reshape for plot_surface function\n        w1_vals.shape = (len(w_in),len(w_in))\n        w2_vals.shape = (len(w_in),len(w_in))\n        g_vals.shape = (len(w_in),len(w_in))\n        \n        ### plot function and z=0 for visualization ###\n        ax.plot_surface(w1_vals, w2_vals, g_vals, alpha = 0.3,color = \'lime\',rstride=25, cstride=25,linewidth=0.7,edgecolor = \'k\',zorder = 2)\n\n        ax.plot_surface(w1_vals, w2_vals, g_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.5,edgecolor = \'k\') \n        \n        ### add arrows and annotations ###\n        # add arrow for slope visualization\n        s = func([1,0]) - func([0,0])\n        if abs(s) > 0.5:\n            # draw arrow\n            a = Arrow3D([0, s], [0, 0], [0, 0], mutation_scale=20,\n                    lw=2, arrowstyle=""-|>"", color=""b"")\n            ax.add_artist(a)\n            \n            # label arrow\n            q = func([1.3,0]) - func([0,0])\n            annotate3D(ax, s=\'$(b_1,0)$\', xyz=[q,0,0], fontsize=13, xytext=(-3,3),\n               textcoords=\'offset points\', ha=\'center\',va=\'center\') \n\n        t = func([0,1]) - func([0,0])\n        if abs(t) > 0.5:\n            # draw arrow\n            a = Arrow3D([0, 0], [0, t], [0, 0], mutation_scale=20,\n                    lw=2, arrowstyle=""-|>"", color=""b"")\n            ax.add_artist(a)  \n            \n            # label arrow\n            q = func([0,1.3]) - func([0,0])\n            annotate3D(ax, s=\'$(0,b_2)$\', xyz=[0,q,0], fontsize=13, xytext=(-3,3),\n               textcoords=\'offset points\', ha=\'center\',va=\'center\') \n                \n        # full gradient\n        if abs(s) > 0.5 and abs(t) > 0.5:\n            a = Arrow3D([0, func([1,0])- func([0,0])], [0, func([0,1])- func([0,0])], [0, 0], mutation_scale=20,\n                    lw=2, arrowstyle=""-|>"", color=""k"")\n            ax.add_artist(a)  \n            \n            s = func([1.2,0]) - func([0,0])\n            t = func([0,1.2]) - func([0,0])\n            annotate3D(ax, s=\'$(b_1,b_2)$\', xyz=[s,t,0], fontsize=13, xytext=(-3,3),\n               textcoords=\'offset points\', ha=\'center\',va=\'center\') \n            \n        # full negative gradient\n        if abs(s) > 0.5 and abs(t) > 0.5:\n            a = Arrow3D([0, - (func([1,0])- func([0,0]))], [0, - (func([0,1])- func([0,0]))], [0, 0], mutation_scale=20,\n                        lw=2, arrowstyle=""-|>"", color=""r"")\n            ax.add_artist(a)  \n            an = 1\n            s = - (func([an+0.2,0]) - func([0,0]))\n            t = - (func([0,an+0.2]) - func([0,0]))\n            name = \'$-(b_1,b_2)$\'\n            annotate3D(ax, s=name, xyz=[s,t,0], fontsize= 13, xytext=(-3,3),textcoords=\'offset points\', ha=\'center\',va=\'center\') \n\n        # draw negative coordinate-wise slope vector        \n        if abs(s) > 0.5 and abs(t) < 0.5:\n            a = Arrow3D([0, - (func([1,0])- func([0,0]))], [0, - (func([0,1])- func([0,0]))], [0, 0], mutation_scale=20,\n                        lw=2, arrowstyle=""-|>"", color=""r"")\n            ax.add_artist(a)  \n            an = 1.2\n            s = - (func([an+0.2,0]) - func([0,0]))\n            t = - (func([0,an+0.2]) - func([0,0]))\n            name = \'$-(b_1,0)$\'\n            annotate3D(ax, s=name, xyz=[s,t,0], fontsize=13, xytext=(-3,3),textcoords=\'offset points\', ha=\'center\',va=\'center\') \n \n            \n        # draw negative coordinate-wise slope vector        \n        if abs(t) > 0.5 and abs(s) < 0.5:\n            a = Arrow3D([0, - (func([1,0])- func([0,0]))], [0, - (func([0,1])- func([0,0]))], [0, 0], mutation_scale=20,\n                        lw=2, arrowstyle=""-|>"", color=""r"")\n            ax.add_artist(a)  \n            an = 1.2\n            s = - (func([an+0.2,0]) - func([0,0]))\n            t = - (func([0,an+0.2]) - func([0,0]))\n            name = \'$-(0,b_2)$\'\n            annotate3D(ax, s=name, xyz=[s,t,0], fontsize=13, xytext=(-3,3),textcoords=\'offset points\', ha=\'center\',va=\'center\') \n           \n            \n        ### clean up plot ###\n        # plot x and y axes, and clean up\n        ax.grid(False)\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n        \n        # remove axes lines and tickmarks\n        ax.w_zaxis.line.set_lw(0.)\n        ax.set_zticks([])\n        ax.w_xaxis.line.set_lw(0.)\n        ax.set_xticks([])\n        ax.w_yaxis.line.set_lw(0.)\n        ax.set_yticks([])\n        \n        # set viewing angle\n        ax.view_init(view[0],view[1])\n        \n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n        # set vewing limits\n        y = 2.3\n        ax.set_xlim([-y,y])\n        ax.set_ylim([-y,y])\n        ax.set_zlim([zmin,zmax])\n\n        # label plot\n        fontsize = 15\n        ax.set_xlabel(r\'$w_1$\',fontsize = fontsize,labelpad = -10)\n        ax.set_ylabel(r\'$w_2$\',fontsize = fontsize,rotation = 0,labelpad=-10)\n        \n        \n        sig = \'+\'\n        if slope[0] < 0:\n            sig = \'-\'\n        sig2 = \'+\'\n        if slope[1] < 0:\n            sig2 = \'-\'\n       \n            \n        part2 = sig + \'{:.1f}\'.format(abs(slope[0])) + \'w_1 \'\n        if abs(slope[0]) < 0.01:\n            part2 = \'\'\n            \n        part3 = sig2 + \'{:.1f}\'.format(abs(slope[1])) + \'w_2\'\n        if abs(slope[1]) < 0.01:\n            part3 = \'\'\n        \n        part1 = \'{:.1f}\'.format(abs(bias))\n        if abs(bias) < 0.01:\n            part3 = \'\'\n        \n        \n        ax.set_title(r\'$g(w_1,w_2) = \' + part1 + part2  + part3 + \'$\' ,fontsize = 15)\n\n        return artist,\n              \n    anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n\n    # produce animation and save\n    fps = 50\n    if \'fps\' in kwargs:\n        fps = kwargs[\'fps\']\n    anim.save(savepath, fps=fps, extra_args=[\'-vcodec\', \'libx264\'])\n    clear_output()            \n        \n    return(anim)\n    \n### static slope visualizer functions ###\n# custom plot for spiffing up plot of a single mathematical function\ndef visualize2d(func,**kwargs):\n    # define input space\n    w = np.linspace(-10,10,500)\n    if \'w\' in kwargs:\n        w = kwargs[\'w\']\n    guides = \'on\'\n    if \'guides\' in kwargs:\n        guides = kwargs[\'guides\']\n    \n    # construct figure\n    fig = plt.figure(figsize = (12,4))\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,3, 1]) \n    ax1 = plt.subplot(gs[0]); ax1.axis(\'off\');\n    ax3 = plt.subplot(gs[2]); ax3.axis(\'off\');\n\n    # plot input function\n    ax2 = plt.subplot(gs[1])\n\n    # plot function\n    ax2.plot(w,func(w), c=\'r\', linewidth=2,zorder = 3)\n     \n    ### plot slope as vector\n    if abs(func(1)) > 0.2:\n        head_width = 0.166*func(1)\n        head_length = 0.25*func(1)\n        \n        # plot slope guide as arrow\n        ax2.arrow(0, 0, func(1),0, head_width=head_width, head_length=head_length, fc=\'k\', ec=\'k\',linewidth=2.5,zorder = 3)\n    \n        # annotate arrow\n        if func(1) > 0.1:\n            ax2.annotate(\'$b$\', xy=(2, 1), xytext=(func(1.3),0),fontsize=15\n            )\n        elif func(1) < -0.1:\n            ax2.annotate(\'$b$\', xy=(2, 1), xytext=(func(1.5),0),fontsize=15\n            )\n            \n    # set viewing limits\n    wgap = (max(w) - min(w))*0.3\n    ax2.set_xlim([-5,5])\n    ax2.set_ylim([-5,5])\n\n    # plot x and y axes, and clean up\n    ax2.grid(True, which=\'both\')\n    #ax2.axhline(y=0, color=\'k\', linewidth=1)\n    #ax2.axvline(x=0, color=\'k\', linewidth=1)\n        \n    # label plot\n    ax2.set_xlabel(\'$w$\',fontsize = 15)\n    ax2.set_ylabel(\'$g(w)$\',fontsize = 15,rotation = 0,labelpad = 20)\n    plt.show()\n    \n# custom plot for spiffing up plot of a single mathematical function\ndef visualize3d(func1,func2,func3,**kwargs):\n    # define input space\n    w = np.linspace(-2,2,200)\n\n    if \'w\' in kwargs:\n        w = kwargs[\'w\']\n    guides = \'on\'\n    if \'guides\' in kwargs:\n        guides = kwargs[\'guides\']\n        \n    view = [20,20]\n    if \'view\' in kwargs:\n        view = kwargs[\'view\']\n        \n    # create mesh\n    w1_vals,w2_vals = np.meshgrid(w,w)\n    w1_vals.shape = (len(w)**2,1)\n    w2_vals.shape = (len(w)**2,1)\n    g_vals1 = func1([w1_vals,w2_vals])\n    g_vals2 = func2([w1_vals,w2_vals])\n    g_vals3 = func3([w1_vals,w2_vals])\n\n    # vals for cost surface\n    w1_vals.shape = (len(w),len(w))\n    w2_vals.shape = (len(w),len(w))\n    g_vals1.shape = (len(w),len(w))\n    g_vals2.shape = (len(w),len(w))\n    g_vals3.shape = (len(w),len(w))\n       \n    # construct figure\n    fig = plt.figure(figsize = (9,4),edgecolor = \'k\')\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,1,1]) \n    ax1 = plt.subplot(gs[0],projection=\'3d\'); \n    ax2 = plt.subplot(gs[1],projection=\'3d\')\n    ax3 = plt.subplot(gs[2],projection=\'3d\')\n    \n    for i in range(3):\n        ax = 0\n        func = 0\n        g_vals = 0\n        if i == 0:\n            ax = ax1\n            func = func1\n            g_vals = g_vals1\n        if i == 1:\n            ax = ax2\n            func = func2\n            g_vals = g_vals2\n        if i == 2:\n            ax = ax3\n            func = func3\n            g_vals = g_vals3\n            \n        # add arrow for slope visualization\n        s = func([1,0]) - func([0,0])\n        t = func([0,1]) - func([0,0])\n\n        # plot coordinate-wise slope vector\n        if abs(s) > 0.5:\n            # draw arrow\n            a = Arrow3D([0, s], [0, 0], [0, 0], mutation_scale=20,\n                    lw=2, arrowstyle=""-|>"", color=""b"")\n            ax.add_artist(a)\n            \n            # label arrow\n            s = func([1.5,0]) - func([0,0])\n            annotate3D(ax, s=\'$(b_1,0)$\', xyz=[s,0,0], fontsize=14, xytext=(-3,3),\n               textcoords=\'offset points\', ha=\'center\',va=\'center\') \n            \n        # draw negative coordinate-wise slope vector        \n        if abs(s) > 0.5 and abs(t) < 0.5:\n            a = Arrow3D([0, - (func([1,0])- func([0,0]))], [0, - (func([0,1])- func([0,0]))], [0, 0], mutation_scale=20,\n                        lw=2, arrowstyle=""-|>"", color=""r"")\n            ax.add_artist(a)  \n            an = 1.2\n            s = - (func([an+0.2,0]) - func([0,0]))\n            t = - (func([0,an+0.2]) - func([0,0]))\n            name = \'$-(b_1,0)$\'\n            annotate3D(ax, s=name, xyz=[s,t,0], fontsize=14, xytext=(-3,3),textcoords=\'offset points\', ha=\'center\',va=\'center\') \n \n        # plot coordinate-wise slope vector\n        if abs(t) > 0.5:\n            # draw arrow\n            a = Arrow3D([0, 0], [0, t], [0, 0], mutation_scale=20,\n                    lw=2, arrowstyle=""-|>"", color=""b"")\n            ax.add_artist(a)  \n            \n            # label arrow\n            t = func([0,1.5]) - func([0,0])\n            annotate3D(ax, s=\'$(0,b_2)$\', xyz=[0,t,0], fontsize=14, xytext=(-3,3),\n               textcoords=\'offset points\', ha=\'center\',va=\'center\') \n            \n        # draw negative coordinate-wise slope vector        \n        if abs(t) > 0.5 and abs(s) < 0.5:\n            a = Arrow3D([0, - (func([1,0])- func([0,0]))], [0, - (func([0,1])- func([0,0]))], [0, 0], mutation_scale=20,\n                        lw=2, arrowstyle=""-|>"", color=""r"")\n            ax.add_artist(a)  \n            an = 1.2\n            s = - (func([an+0.2,0]) - func([0,0]))\n            t = - (func([0,an+0.2]) - func([0,0]))\n            name = \'$-(0,b_2)$\'\n            annotate3D(ax, s=name, xyz=[s,t,0], fontsize=14, xytext=(-3,3),textcoords=\'offset points\', ha=\'center\',va=\'center\') \n         \n                \n        # full gradient\n        if abs(s) > 0.5 and abs(t) > 0.5:\n            a = Arrow3D([0, func([1,0])- func([0,0])], [0, func([0,1])- func([0,0])], [0, 0], mutation_scale=20,\n                    lw=2, arrowstyle=""-|>"", color=""k"")\n            ax.add_artist(a)  \n            \n            b = func([1.2,0]) - func([0,0])\n            c = func([0,1.2]) - func([0,0])\n            annotate3D(ax, s=\'$(b_1,b_2)$\', xyz=[b,c,0], fontsize=14, xytext=(-3,3),\n               textcoords=\'offset points\', ha=\'center\',va=\'center\') \n    \n        # full negative gradient\n        if abs(s) > 0.5 and abs(t) > 0.5:\n            a = Arrow3D([0, - (func([1,0])- func([0,0]))], [0, - (func([0,1])- func([0,0]))], [0, 0], mutation_scale=20,\n                        lw=2, arrowstyle=""-|>"", color=""r"")\n            ax.add_artist(a)  \n            an = 1\n            s = - (func([an+0.2,0]) - func([0,0]))\n            t = - (func([0,an+0.2]) - func([0,0]))\n            name = \'$-(b_1,b_2)$\'\n            annotate3D(ax, s=name, xyz=[s,t,0], fontsize=14, xytext=(-3,3),textcoords=\'offset points\', ha=\'center\',va=\'center\') \n \n        # plot function        \n        ax.plot_surface(w1_vals, w2_vals, g_vals, alpha = 0.3,color = \'lime\',rstride=25, cstride=25,linewidth=0.5,edgecolor = \'k\',zorder = 2)\n\n        ax.plot_surface(w1_vals, w2_vals, g_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n\n        # plot x and y axes, and clean up\n        ax.grid(False)\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        # remove axes lines and tickmarks\n        ax.w_zaxis.line.set_lw(0.)\n        ax.set_zticks([])\n        ax.w_xaxis.line.set_lw(0.)\n        ax.set_xticks([])\n        ax.w_yaxis.line.set_lw(0.)\n        ax.set_yticks([])\n        \n        # set viewing angle\n        ax.view_init(view[0],view[1])\n        \n        # set vewing limits\n        y = 2.3\n        ax.set_xlim([-y,y])\n        ax.set_ylim([-y,y])\n        s = np.min(np.min(g_vals))\n        t = np.max(np.max(g_vals))\n        ax.set_zlim([s,t])\n        \n        # label plot\n        fontsize = 15\n        ax.set_xlabel(r\'$w_1$\',fontsize = fontsize,labelpad = -10)\n        ax.set_ylabel(r\'$w_2$\',fontsize = fontsize,rotation = 0,labelpad=-10)\n        \n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n    plt.show()\n    \n    \n#### custom 3d arrow and annotator functions ###    \n# nice arrow maker from https://stackoverflow.com/questions/11140163/python-matplotlib-plotting-a-3d-cube-a-sphere-and-a-vector\nclass Arrow3D(FancyArrowPatch):\n\n    def __init__(self, xs, ys, zs, *args, **kwargs):\n        FancyArrowPatch.__init__(self, (0, 0), (0, 0), *args, **kwargs)\n        self._verts3d = xs, ys, zs\n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n        FancyArrowPatch.draw(self, renderer)\n\n# great solution for annotating 3d objects - from https://datascience.stackexchange.com/questions/11430/how-to-annotate-labels-in-a-3d-matplotlib-scatter-plot\nclass Annotation3D(Annotation):\n    \'\'\'Annotate the point xyz with text s\'\'\'\n\n    def __init__(self, s, xyz, *args, **kwargs):\n        Annotation.__init__(self,s, xy=(0,0), *args, **kwargs)\n        self._verts3d = xyz        \n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.xy=(xs,ys)\n        Annotation.draw(self, renderer)        \n\ndef annotate3D(ax, s, *args, **kwargs):\n    \'\'\'add anotation text s to to Axes3d ax\'\'\'\n\n    tag = Annotation3D(s, *args, **kwargs)\n    ax.add_artist(tag)'"
mlrefined_libraries/calculus_library/taylor2d_test.py,5,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nimport time\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\n\n# illustrate first and second order taylor series approximations to one-parameter input functions\nclass visualizer:\n    '''\n    Illustrate first and second order Taylor series approximations to a given input function at a\n    coarsely chosen set of points.  Transition between the points using a custom slider mechanism\n    to peruse how the approximations change from point-to-point.\n    '''\n    def __init__(self,**args):\n        self.g = args['g']                       # input function\n        self.grad = compute_grad(self.g)         # gradient of input function\n        self.hess = compute_grad(self.grad)      # hessian of input function\n        self.colors = [[0,1,0.25],[0,0.75,1]]    # set of custom colors used for plotting\n\n    # compute first order approximation\n    def draw_it(self,**args):\n        num_frames = 300                          # number of slides to create - the input range [-3,3] is divided evenly by this number\n        if 'num_frames' in args:\n            num_frames = args['num_frames']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (6,6))\n        artist = fig\n        ax = fig.add_subplot(111)\n\n        # generate a range of values over which to plot input function, and derivatives\n        w_plot = np.linspace(-3,3,200)                  # input range for original function\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)             # used for cleaning up final plot\n        ggap = g_range*0.5\n        w_vals = np.linspace(-2.5,2.5,num_frames)       # range of values over which to plot first / second order approximations\n        \n        # which approximations to plot with the function?  Two switches: first_order and second_order\n        first_order = False\n        second_order = False\n        if 'first_order' in args:\n            first_order = args['first_order']\n        if 'second_order' in args:\n            second_order = args['second_order']\n        print ('starting animation rendering...')\n            \n        # animation sub-function\n        def animate(k):\n            # clear the panel\n            ax.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            # grab the next input/output tangency pair, the center of the next approximation(s)\n            w_val = w_vals[k]\n            g_val = self.g(w_val)\n\n            # plot original function\n            ax.plot(w_plot,g_plot,color = 'k',zorder = 0)                           # plot function\n            \n            # plot the input/output tangency point\n            ax.scatter(w_val,g_val,s = 90,c = 'r',edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n            \n            # label axes\n            ax.set_xlabel('$w$',fontsize = 17)\n            ax.set_ylabel('$g(w)$',fontsize = 17)\n\n            #### should we plot first order approximation? ####\n            if first_order == True:\n                # plug input into the first derivative\n                g_grad_val = self.grad(w_val)\n\n                # determine width to plot the approximation -- so its length == width\n                width = 1\n                div = float(1 + g_grad_val**2)\n                w1 = w_val - math.sqrt(width/div)\n                w2 = w_val + math.sqrt(width/div)\n\n                # compute first order approximation\n                wrange = np.linspace(w1,w2, 100)\n                h = g_val + g_grad_val*(wrange - w_val)\n\n                # plot the first order approximation\n                ax.plot(wrange,h,color = self.colors[0],linewidth = 2,zorder = 2)      # plot approx\n\n            #### should we plot second order approximation? ####\n            if second_order == True:\n                # plug input value into the second derivative\n                g_grad_val = self.grad(w_val)\n                g_hess_val = self.hess(g_grad_val)\n\n                # determine width of plotting area for second order approximator\n                width = 1\n                if g_hess_val < 0:\n                    width = - width\n\n                # setup quadratic formula params\n                a = 0.5*g_hess_val\n                b = g_grad_val - 2*0.5*g_hess_val*w_val\n                c = 0.5*g_hess_val*w_val**2 - g_grad_val*w_val - width\n\n                # solve for zero points of the quadratic (for plotting purposes only)\n                w1 = (-b + math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n                w2 = (-b - math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n\n                # create the second order approximation\n                wrange = np.linspace(w1,w2, 100)\n                h = g_val + g_grad_val*(wrange - w_val) + 0.5*g_hess_val*(wrange - w_val)**2 \n\n                # plot the second order approximation\n                ax.plot(wrange,h,color = self.colors[1],linewidth = 3,zorder = 1)      # plot approx\n\n            # fix viewing limits on panel\n            ax.set_xlim([-3,3])\n            ax.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n                \n            return artist,\n        \n        anim = animation.FuncAnimation(fig, animate,frames=len(w_vals), interval=len(w_vals), blit=True)\n        \n        return(anim)"""
mlrefined_libraries/calculus_library/taylor2d_viz.py,10,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nimport time\nfrom matplotlib import gridspec\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\n\n# illustrate first and second order taylor series approximations to one-parameter input functions\nclass visualizer:\n    '''\n    Illustrate first and second order Taylor series approximations to a given input function at a\n    coarsely chosen set of points.  Transition between the points using a custom slider mechanism\n    to peruse how the approximations change from point-to-point.\n    '''\n    def __init__(self,**args):\n        self.g = args['g']                       # input function\n        self.grad = compute_grad(self.g)         # gradient of input function\n        self.hess = compute_grad(self.grad)      # hessian of input function\n        self.colors = [[0,1,0.25],[0,0.75,1]]    # set of custom colors used for plotting\n\n    # compute first order approximation\n    def draw_it(self,savepath,**kwargs):\n        num_frames = 300                          # number of slides to create - the input range [-3,3] is divided evenly by this number\n        if 'num_frames' in kwargs:\n            num_frames = kwargs['num_frames']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (10,5))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,3, 1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis('off');\n        ax3 = plt.subplot(gs[2]); ax3.axis('off');\n\n        # plot input function\n        ax = plt.subplot(gs[1],aspect = 'equal')\n        \n        max_val = 2.5\n        if 'max_val' in kwargs:\n            max_val = kwargs['max_val']\n        w_vals = np.linspace(-max_val + 0.2,max_val - 0.2,num_frames)       # range of values over which to plot first / second order approximations\n        \n        # generate a range of values over which to plot input function, and derivatives\n        w_plot = np.linspace(-max_val-0.5,max_val+0.5,200)                  # input range for original function\n        \n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)             # used for cleaning up final plot\n        ggap = g_range*0.5\n     \n        # which approximations to plot with the function?  Two switches: first_order and second_order\n        first_order = False\n        second_order = False\n        if 'first_order' in kwargs:\n            first_order = kwargs['first_order']\n        if 'second_order' in kwargs:\n            second_order = kwargs['second_order']\n        print ('starting animation rendering...')\n            \n        # animation sub-function\n        def animate(k):\n            # clear the panel\n            ax.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            # grab the next input/output tangency pair, the center of the next approximation(s)\n            w_val = w_vals[k]\n            g_val = self.g(w_val)\n\n            # plot original function\n            ax.plot(w_plot,g_plot,color = 'k',zorder = 0,linewidth=1)                           # plot function\n            \n            # plot the input/output tangency point\n            ax.scatter(w_val,g_val,s = 80,c = 'red',edgecolor = 'k',linewidth = 1,zorder = 3,marker = 'X')            # plot point of tangency\n            ax.scatter(w_val,0,s = 100,c = 'red',edgecolor = 'k',linewidth = 1,zorder = 3)            # plot point of tangency\n            tempy = np.linspace(0,g_val,100)\n            tempx = w_val*np.ones((100))\n            ax.plot(tempx,tempy,linewidth = 0.7,color = 'k',linestyle = '--',zorder = 1)\n\n            #### should we plot first order approximation? ####\n            if first_order == True:\n                # plug input into the first derivative\n                g_grad_val = self.grad(w_val)\n\n                '''\n                # determine width to plot the approximation -- so its length == width\n                width = 10\n                div = float(1 + g_grad_val**2)\n                w1 = w_val - math.sqrt(width/div)\n                w2 = w_val + math.sqrt(width/div)\n                '''\n                \n                # or just constant width\n                w1 = w_val - 0.5\n                w2 = w_val + 0.5\n\n                # compute first order approximation\n                wrange = np.linspace(w1,w2, 100)\n                h = g_val + g_grad_val*(wrange - w_val)\n\n                # plot the first order approximation\n                ax.plot(wrange,h,color = self.colors[0],linewidth = 3,zorder = 1)      # plot approx\n\n            #### should we plot second order approximation? ####\n            if second_order == True:\n                # plug input value into the second derivative\n                g_grad_val = self.grad(w_val)\n                g_hess_val = self.hess(w_val)\n\n                # determine width of plotting area for second order approximator\n                width = 1\n                if g_hess_val < 0:\n                    width = - width\n\n                # setup quadratic formula params\n                a = 0.5*g_hess_val\n                b = g_grad_val - 2*0.5*g_hess_val*w_val\n                c = 0.5*g_hess_val*w_val**2 - g_grad_val*w_val - width\n\n                # solve for zero points of the quadratic (for plotting purposes only)\n                w1 = (-b + math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n                w2 = (-b - math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n\n                # create the second order approximation\n                wrange = np.linspace(w1,w2, 100)\n                h = g_val + g_grad_val*(wrange - w_val) + 0.5*g_hess_val*(wrange - w_val)**2 \n\n                # plot the second order approximation\n                ax.plot(wrange,h,color = self.colors[1],linewidth = 3,zorder = 2)      # plot approx\n\n            # fix viewing limits on panel\n            ax.set_xlim([-max_val,max_val])\n            ax.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            \n            # set tickmarks\n            ax.set_xticks(-np.arange(-round(max_val), round(max_val) + 1, 1.0))\n            ax.set_yticks(np.arange(round(min(g_plot) - ggap), round(max(g_plot) + ggap) + 1, 1.0))\n            \n            # label axes\n            ax.set_xlabel('$w$',fontsize = 18)\n            ax.set_ylabel('$g(w)$',fontsize = 18,rotation = 0,labelpad = 25)\n            ax.set_title(r'$w^0 = ' + str(np.round(w_val,2)) +  '$',fontsize = 19)\n                \n            # set axis \n            ax.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n            \n            return artist,\n        \n        anim = animation.FuncAnimation(fig, animate,frames=len(w_vals), interval=len(w_vals), blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()"""
mlrefined_libraries/calculus_library/taylor3d_viz.py,15,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import gridspec\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import jacobian\nfrom autograd import hessian\nimport math\n\nclass visualizer:\n    '''\n    Illustrate first and second order Taylor series approximations to a given input function at a\n    user defined point in 3-dimensions.\n    '''\n    def __init__(self,**args):\n        self.g = args['g']                      # user-defined input function\n        self.grad = jacobian(self.g)            # first derivative of input point\n        self.hess = hessian(self.g)             # second derivative of input point\n        \n        # default colors\n        self.colors = [[0,1,0.25],[0,0.75,1]]   # custom colors\n\n    # draw taylor series approximation to 3d function\n    def draw_it(self,**kwargs):\n        # which approximations to plot with the function?  first_order == True (draw first order approximation), second_order == True (draw second order approximation)\n        first_order = False\n        second_order = False\n        if 'first_order' in kwargs:\n            first_order = kwargs['first_order']\n        if 'second_order' in kwargs:\n            second_order = kwargs['second_order']\n        view = [20,20]\n        if 'view' in kwargs:\n            view = kwargs['view']\n            \n        # get user-defined point\n        w_val = kwargs['w_val']\n        \n        # initialize figure\n        fig = plt.figure(figsize = (9,3))\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,2, 1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis('off');\n        ax3 = plt.subplot(gs[2]); ax3.axis('off');\n\n        # create panel for input function\n        ax2 = plt.subplot(gs[1], projection='3d')\n\n        ax2.set_xlabel('$w_1$',fontsize = 17)\n        ax2.set_ylabel('$w_2$',fontsize = 17)\n        ax2.zaxis.set_rotate_label(False)  # disable automatic rotation\n        ax2.set_zlabel('$g(w_1,w_2)$',fontsize = 17,labelpad = 30,rotation = 0)\n        \n        # create plotting range\n        r = np.linspace(-3,3,100)\n\n        # create grid from plotting range\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        g_vals = self.g([w1_vals,w2_vals])\n\n        # plot cost surface\n        w1_vals.shape = (len(r),len(r))\n        w2_vals.shape = (len(r),len(r))\n        g_vals.shape = (len(r),len(r))\n        ax2.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = 'w',rstride=15, cstride=15,linewidth=1,edgecolor = 'k')\n        \n        # get input/output pairs\n        w_val = [float(a) for a in w_val]\n        w_val = np.asarray(w_val)\n        w1_val = w_val[0]\n        w2_val = w_val[1]\n        g_val = self.g(w_val)\n        grad_val = self.grad(w_val)\n        grad_val.shape = (2,1)\n        \n       # plot tangency point\n        ax2.scatter(w1_val,w2_val,g_val,s = 50,c = 'lime',edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n\n        # plot first order approximation\n        if first_order == True:\n            # compute first order approximation\n            t1 = np.linspace(-1.5, 1.5,100)\n            t2 = np.linspace(-1.5, 1.5,100)\n            wrange1,wrange2 = np.meshgrid(t1,t2)\n            wrange1.shape = (len(t1)**2,1) \n            wrange2.shape = (len(t1)**2,1) \n            wrange =  np.hstack((wrange1,wrange2))\n\n            # first order function\n            h = lambda weh: g_val + np.dot(weh - w_val ,grad_val)\n            h_val = h(wrange + w_val)\n\n            # # plot all\n            wrange1 = wrange1 + w_val[0]\n            wrange2 = wrange2 + w_val[1]\n            wrange1.shape = (len(t1),len(t1)) \n            wrange2.shape = (len(t1),len(t1)) \n            h_val.shape = (len(t1),len(t1))\n            ax2.plot_surface(wrange1,wrange2,h_val,alpha = 0.2,color = 'lime',rstride=15, cstride=15,linewidth=1,edgecolor = 'k')\n\n        # print second order approximation\n        if second_order == True:\n            # compute hessian at input point\n            hess = self.hess(w_val)\n            \n            # setup grid - without compensation \n            t1 = np.linspace(-1.5, 1.5,100)\n            t2 = np.linspace(-1.5, 1.5,100)\n            wrange1,wrange2 = np.meshgrid(t1,t2)\n            wrange1.shape = (len(t1)**2,1) \n            wrange2.shape = (len(t1)**2,1) \n            wrange =  np.hstack((wrange1,wrange2))\n            temp =  0.5*np.dot(np.dot(wrange - w_val,hess).T,wrange - w_val)\n            \n            # first order function\n            h = lambda weh: g_val + np.dot(weh - w_val ,grad_val) + 0.5*np.dot(np.dot(weh - w_val,hess).T,weh - w_val)\n            h_val = []\n            for i in range(len(wrange)):\n                pt = wrange[i] + w_val\n                h_pt = h(pt)\n                h_val.append(h_pt)\n            h_val = np.asarray(h_val)\n\n            # # plot all\n            wrange1 = wrange1 + w_val[0]\n            wrange2 = wrange2 + w_val[1]\n            wrange1.shape = (len(t1),len(t1)) \n            wrange2.shape = (len(t1),len(t1)) \n            h_val.shape = (len(t1),len(t1))\n            ax2.plot_surface(wrange1,wrange2,h_val,alpha = 0.4,color = self.colors[1],rstride=15, cstride=15,linewidth=1,edgecolor = 'k')\n\n        # clean up plot\n        ax2.grid(False);\n        ax2.view_init(view[0],view[1]);\n        plt.show()"""
mlrefined_libraries/calculus_library/taylor_series_simultaneous_approximations.py,4,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport time\nfrom IPython.display import clear_output\n\nclass visualizer: \n    '''\n    Draw first through fourth Taylor series approximation to a function over points on \n    the interval [-3,3] and animate with a custom slider mechanism, allowing you to browse these\n    approximations over various points of the input interval\n    '''\n    \n    def __init__(self,**args):\n        self.g = args['g']                                                # input function\n        self.first_derivative = compute_grad(self.g)                      # first derivative of input function\n        self.second_derivative = compute_grad(self.first_derivative)      # second derivative of input function\n        self.third_derivative = compute_grad(self.second_derivative)      # third derivative of input function\n        self.fourth_derivative = compute_grad(self.third_derivative)      # fourth derivative of input function\n\n        self.colors = [[0,1,0.25],[0,0.75,1],[1,0.75,0],[1,0,0.75]]       # custom colors for plotting\n\n    # draw taylor series approximations over a range of points and animate\n    def draw_it(self,**args):\n        # how many frames to plot\n        num_frames = 5\n        if 'num_frames' in args:\n            num_frames = args['num_frames']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (10,3))\n        artist = fig\n        gs = gridspec.GridSpec(1, 4) \n        ax1 = fig.add_subplot(141)\n        ax2 = fig.add_subplot(142)\n        ax3 = fig.add_subplot(143)\n        ax4 = fig.add_subplot(144)\n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(-3,3,200)\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.5\n        w_vals = np.linspace(-2.5,2.5,num_frames)\n        print ('beginning animation rendering...')\n        \n        # animation sub-function\n        def animate_it(k):\n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n                \n            # clear out each panel for next slide\n            ax1.cla()\n            ax2.cla()\n            ax3.cla()\n            ax4.cla()\n\n            # pick current value to plot approximations\n            w_val = w_vals[k]\n            g_val = self.g(w_val)\n\n            # plot the original function in each panel\n            ax1.plot(w_plot,g_plot,color = 'k',zorder = 0)                           \n            ax2.plot(w_plot,g_plot,color = 'k',zorder = 0)                         \n            ax3.plot(w_plot,g_plot,color = 'k',zorder = 0)                       \n            ax4.plot(w_plot,g_plot,color = 'k',zorder = 0)                       \n            \n            # plot tangency point in each panel\n            ax1.scatter(w_val,g_val,s = 90,c = 'r',edgecolor = 'k',linewidth = 0.7,zorder = 3)            \n            ax2.scatter(w_val,g_val,s = 90,c = 'r',edgecolor = 'k',linewidth = 0.7,zorder = 3)            \n            ax3.scatter(w_val,g_val,s = 90,c = 'r',edgecolor = 'k',linewidth = 0.7,zorder = 3)            \n            ax4.scatter(w_val,g_val,s = 90,c = 'r',edgecolor = 'k',linewidth = 0.7,zorder = 3)            \n\n            #### plot first order approximation? ####\n            # plug in value into func and derivative\n            g_first_val = self.first_derivative(w_val)\n\n            # compute first order approximation\n            wrange = np.linspace(-3,3,100)\n            h = g_val + g_first_val*(wrange - w_val)\n\n            # plot first order approximation in first panel\n            ax1.plot(wrange,h,color = self.colors[0],linewidth = 2,zorder = 2)      # plot approx\n\n            #### plot second order approximation? ####\n            # plug in value into func and derivative\n            g_second_val = self.second_derivative(w_val)\n\n            # compute second order approximation\n            h += 1/float(2)*g_second_val*(wrange - w_val)**2 \n\n            # plot second order approximation in second panel\n            ax2.plot(wrange,h,color = self.colors[1],linewidth = 3,zorder = 1)      # plot approx\n\n            \n            #### plot third order approximation? ####\n            # plug in value into func and derivative\n            g_third_val = self.third_derivative(w_val)\n\n            # compute second order approximation\n            h += 1/float(2*3)*g_third_val*(wrange - w_val)**3\n\n            # plot second order approximation in second panel\n            ax3.plot(wrange,h,color = self.colors[2],linewidth = 3,zorder = 1)      # plot approx\n\n            \n            #### plot fourth order approximation? ####\n            # plug in value into func and derivative\n            g_fourth_val = self.fourth_derivative(w_val)\n\n            # compute second order approximation\n            h += 1/float(2*3*4)*g_fourth_val*(wrange - w_val)**4\n\n            # plot second order approximation in second panel\n            ax4.plot(wrange,h,color = self.colors[3],linewidth = 3,zorder = 1)      # plot approx\n\n            \n            #### fix viewing limits ####\n            ax1.set_xlim([-3,3])\n            ax1.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            ax1.set_yticks([],[])\n            ax1.set_title('first order approximation',fontsize = 13)\n            \n            ax2.set_xlim([-3,3])\n            ax2.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            ax2.set_yticks([],[])\n            ax2.set_title('second order approximation',fontsize = 13)\n    \n            ax3.set_xlim([-3,3])\n            ax3.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            ax3.set_yticks([],[])\n            ax3.set_title('third order approximation',fontsize = 13)\n        \n            ax4.set_xlim([-3,3])\n            ax4.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            ax4.set_yticks([],[])\n            ax4.set_title('fourth order approximation',fontsize = 13)\n              \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate_it,frames=len(w_vals), interval=len(w_vals), blit=True)\n\n        return(anim)"""
mlrefined_libraries/linear_algebra_library/__init__.py,0,b''
mlrefined_libraries/linear_algebra_library/plotters.py,11,"b""import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import gridspec\nfrom IPython.display import display, HTML\nimport copy\n\n# custom plot for spiffing up plot of a two mathematical functions\ndef double_2d_plot(func1,func2,**kwargs): \n    # get labeling arguments\n    xlabel = '$w$'\n    ylabel_1 = ''\n    ylabel_2 = ''\n    title1=  ''\n    title2 = ''\n    fontsize = 15\n    color = 'r'\n    w = np.linspace(-2,2,1000)\n    if 'xlabel' in kwargs:\n        xlabel = kwargs['xlabel']\n    if 'ylabel_1' in kwargs:\n        ylabel_1 = kwargs['ylabel_1']\n    if 'ylabel_2' in kwargs:\n        ylabel_2 = kwargs['ylabel_2']\n    if 'fontsize' in kwargs:\n        fontsize = kwargs['fontsize']\n    if 'title1' in kwargs:\n        title1 = kwargs['title1']\n    if 'title2' in kwargs:\n        title2 = kwargs['title2']\n    if 'w' in kwargs:\n        w = kwargs['w']\n    if 'color' in kwargs:\n        color = kwargs['color']\n        \n    # determine vertical plotting limit\n    f1 = func1(w)\n    f2 = func2(w)\n    ymax = max(max(f1),max(f2))\n    ymin = min(min(f1),min(f2))\n    ygap = (ymax - ymin)*0.2\n    ymax += ygap\n    ymin -= ygap\n        \n    # plot the functions \n    fig = plt.figure(figsize = (8,4))\n    ax1 = fig.add_subplot(121); ax2 = fig.add_subplot(122);    \n    ax1.plot(w, f1, c=color, linewidth=2,zorder = 3)\n    ax2.plot(w, f2, c=color, linewidth=2,zorder = 3)\n\n    # plot x and y axes, and clean up\n    ax1.set_xlabel(xlabel,fontsize = fontsize)\n    ax1.set_ylabel(ylabel_1,fontsize = fontsize,rotation = 0,labelpad = 20)\n    ax2.set_xlabel(xlabel,fontsize = fontsize)\n    ax2.set_ylabel(ylabel_2,fontsize = fontsize,rotation = 0,labelpad = 20)\n    ax1.set_title(title1[1:])\n    ax2.set_title(title2[1:])\n    ax1.set_ylim([ymin,ymax])\n    ax2.set_ylim([ymin,ymax])\n    \n    ax1.grid(True, which='both'), ax2.grid(True, which='both')\n    ax1.axhline(y=0, color='k', linewidth=1), ax2.axhline(y=0, color='k', linewidth=1)\n    ax1.axvline(x=0, color='k', linewidth=1), ax2.axvline(x=0, color='k', linewidth=1)\n    plt.show()\n    \n# custom plot for spiffing up plot of a two mathematical functions\ndef double_2d3d_plot(func1,func2,**kwargs): \n    # get labeling arguments\n    xlabel = '$w$'\n    ylabel_1 = ''\n    ylabel_2 = ''\n    title1=  ''\n    title2 = ''\n    fontsize = 15\n    color = 'r'\n    if 'fontsize' in kwargs:\n        fontsize = kwargs['fontsize']\n    if 'title1' in kwargs:\n        title1 = kwargs['title1']\n    if 'title2' in kwargs:\n        title2 = kwargs['title2']\n    if 'w' in kwargs:\n        w = kwargs['w']\n    if 'color' in kwargs:\n        color = kwargs['color']\n        \n    # determine vertical plotting limit\n    w = np.linspace(-2,2,500)\n    xx,yy = np.meshgrid(w,w)\n    xx.shape = (xx.size,1)\n    yy.shape = (yy.size,1)\n    w3d = np.concatenate((xx,yy),axis=1)\n    f1 = func1(w)\n    f2 = func2(w3d.T)\n    xx.shape = (500,500)\n    yy.shape = (500,500)\n    f2.shape = (500,500)\n        \n    # plot the functions \n    fig = plt.figure(figsize = (8,4))\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n    ax1 = plt.subplot(gs[0]);\n    ax2 = plt.subplot(gs[1],projection='3d'); \n    \n    \n    ax1.plot(w, f1, c=color, linewidth=2,zorder = 3)\n    ax2.plot_surface(xx, yy, f2, alpha = 0.3,color = color,rstride=50, cstride=50,linewidth=2,edgecolor = 'k')\n        \n    # plot x and y axes, and clean up\n    ax1.set_xlabel(xlabel,fontsize = fontsize)\n    ax1.set_ylabel(ylabel_1,fontsize = fontsize,rotation = 0,labelpad = 20)\n    ax2.set_xlabel(r'$w_1$',fontsize = fontsize,labelpad = 10)\n    ax2.set_ylabel(r'$w_2$',fontsize = fontsize,rotation = 0,labelpad = 20)\n    ax2.set_yticks(np.arange(min(w), max(w)+1, 1.0))\n    ax1.set_title(title1[1:])\n    ax2.set_title(title2[:],y=1.08)\n    ax2.view_init(20,-60)\n    \n    ax1.grid(True, which='both'), ax2.grid(True, which='both')\n    ax1.axhline(y=0, color='k', linewidth=1)\n    ax1.axvline(x=0, color='k', linewidth=1)\n    plt.show()\n    \n# custom plot for spiffing up plot of a two mathematical functions\ndef triple_3dsum_plot(func1,func2,**kwargs): \n    # get labeling arguments\n    xlabel = '$w$'\n    ylabel_1 = ''\n    ylabel_2 = ''\n    title1=  ''\n    title2 = ''\n    title3 = ''\n    fontsize = 15\n    color = 'r'\n    if 'fontsize' in kwargs:\n        fontsize = kwargs['fontsize']\n    if 'title1' in kwargs:\n        title1 = kwargs['title1']\n    if 'title2' in kwargs:\n        title2 = kwargs['title2']\n    if 'title3' in kwargs:\n        title3 = kwargs['title3']\n        \n    if 'w' in kwargs:\n        w = kwargs['w']\n    if 'color' in kwargs:\n        color = kwargs['color']\n        \n    # determine vertical plotting limit\n    w = np.linspace(-2,2,500)\n    xx,yy = np.meshgrid(w,w)\n    xx.shape = (xx.size,1)\n    yy.shape = (yy.size,1)\n    w3d = np.concatenate((xx,yy),axis=1)\n    f1 = func1(w3d.T)\n    f2 = func2(w3d.T)\n    xx.shape = (500,500)\n    yy.shape = (500,500)\n    f1.shape = (500,500)\n    f2.shape = (500,500)\n        \n    # plot the functions \n    fig = plt.figure(figsize = (9,3))\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,1,1]) \n    ax1 = plt.subplot(gs[0],projection='3d');\n    ax2 = plt.subplot(gs[1],projection='3d'); \n    ax3 = plt.subplot(gs[2],projection='3d'); \n   \n    # plot surfaces\n    ax1.plot_surface(xx, yy, f1, alpha = 0.3,color = color,rstride=50, cstride=50,linewidth=2,edgecolor = 'k')\n    ax2.plot_surface(xx, yy, f2, alpha = 0.3,color = color,rstride=50, cstride=50,linewidth=2,edgecolor = 'k')\n    ax3.plot_surface(xx, yy,f1 + f2, alpha = 0.3,color = color,rstride=50, cstride=50,linewidth=2,edgecolor = 'k')\n        \n    # plot x and y axes, and clean up\n    ax1.set_xlabel(r'$w_1$',fontsize = fontsize,labelpad = 5)\n    ax1.set_ylabel(r'$w_2$',fontsize = fontsize,rotation = 0,labelpad = 5)\n    ax1.set_yticks(np.arange(min(w), max(w)+1, 1.0))\n    ax1.set_title(title1[:],y=1.08)\n    ax1.view_init(20,-60)\n    \n    ax2.set_xlabel(r'$w_1$',fontsize = fontsize,labelpad = 5)\n    ax2.set_ylabel(r'$w_2$',fontsize = fontsize,rotation = 0,labelpad = 5)\n    ax2.set_yticks(np.arange(min(w), max(w)+1, 1.0))\n    ax2.set_title(title2[:],y=1.08)\n    ax2.view_init(20,-60)\n    \n    ax3.set_xlabel(r'$w_1$',fontsize = fontsize,labelpad = 5)\n    ax3.set_ylabel(r'$w_2$',fontsize = fontsize,rotation = 0,labelpad = 5)\n    ax3.set_yticks(np.arange(min(w), max(w)+1, 1.0))\n    ax3.set_title(title3[:],y=1.08)\n    ax3.view_init(20,-60)\n    \n    plt.show()"""
mlrefined_libraries/linear_algebra_library/quadratic_3d_flexer.py,4,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\nclass visualizer:\n    '''\n    Draw 3d quadratic ranging from convex\n    '''\n\n    # compute first order approximation\n    def draw_it(self,**args):  \n        ### other options\n        # size of figure\n        set_figsize = 7\n        if 'set_figsize' in args:\n            set_figsize = args['set_figsize']\n            \n        # turn axis on or off\n        set_axis = 'on'\n        if 'set_axis' in args:\n            set_axis = args['set_axis']\n            \n        # plot title\n        set_title = ''\n        if 'set_title' in args:\n            set_title = args['set_title']\n            \n        # horizontal and vertical axis labels\n        horiz_1_label = ''\n        if 'horiz_1_label' in args:\n            horiz_1_label = args['horiz_1_label']\n            \n        horiz_2_label = ''\n        if 'horiz_2_label' in args:\n            horiz_2_label = args['horiz_2_label']\n            \n        vert_label = ''\n        if 'vert_label' in args:\n            vert_label = args['vert_label']\n            \n        # set width of plot\n        input_range = np.linspace(-3,3,100)                  # input range for original function\n        if 'input_range' in args:\n            input_range = args['input_range']\n            \n        # set viewing angle on plot\n        view = [20,50]\n        if 'view' in args:\n            view = args['view']\n        \n        num_slides = 100\n        if 'num_slides' in args:\n            num_frames = args['num_slides']\n            \n        alpha_values = np.linspace(-2,2,num_frames)\n        \n        # initialize figure\n        fig = plt.figure(figsize = (set_figsize,set_figsize))\n        artist = fig\n        ax = fig.add_subplot(111, projection='3d')\n\n        # animation sub-function\n        def animate(k):\n            ax.cla()\n            \n            # quadratic to plot\n            alpha = alpha_values[k]\n            g = lambda w: w[0]**2 + alpha*w[1]**2\n            \n            # create grid from plotting range\n            w1_vals,w2_vals = np.meshgrid(input_range,input_range)\n            w1_vals.shape = (len(input_range)**2,1)\n            w2_vals.shape = (len(input_range)**2,1)\n            g_vals = g([w1_vals,w2_vals])\n        \n            # vals for cost surface\n            w1_vals.shape = (len(input_range),len(input_range))\n            w2_vals.shape = (len(input_range),len(input_range))\n            g_vals.shape = (len(input_range),len(input_range))\n\n            g_range = np.amax(g_vals) - np.amin(g_vals)             # used for cleaning up final plot\n            ggap = g_range*0.5\n\n            # plot original function\n            ax.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = 'k',rstride=15, cstride=15,linewidth=2,edgecolor = 'k') \n\n            # clean up plotting area\n            ax.set_title(set_title,fontsize = 15)\n            ax.set_xlabel(horiz_1_label,fontsize = 15)\n            ax.set_ylabel(horiz_2_label,fontsize = 15)\n            ax.set_zlabel(vert_label,fontsize = 15)\n            ax.view_init(view[0],view[1])\n            ax.axis(set_axis)\n \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=len(alpha_values), interval=len(alpha_values), blit=True)\n        \n        return(anim)\n        """
mlrefined_libraries/linear_algebra_library/span_animation.py,64,"b""import numpy as np\nfrom matplotlib import gridspec\nfrom IPython.display import display, HTML\nimport copy\nimport math\n \n# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nimport time\nfrom matplotlib import gridspec\nimport copy\n \n# func,\ndef perfect_visualize(savepath,C,**kwargs):\n    vec1 = C[:,0]\n    vec2 = C[:,1]\n    \n    # size up vecs\n    vec1 = np.asarray(vec1)\n    vec2 = np.asarray(vec2)\n    vec1copy = copy.deepcopy(vec1)\n    vec1copy.shape = (len(vec1copy),1)\n    vec2copy = copy.deepcopy(vec2)\n    vec2copy.shape = (len(vec2copy),1)\n     \n    # renderer    \n    fig = plt.figure(figsize = (14,7))\n    artist = fig\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,3, 1]) \n    ax1 = plt.subplot(gs[0]); ax1.axis('off');\n    ax3 = plt.subplot(gs[2]); ax3.axis('off');\n \n    # plot input function\n    ax2 = plt.subplot(gs[1])\n     \n    ### create grid of points ###\n    s = np.linspace(-5,5,10)\n    xx,yy = np.meshgrid(s,s)\n    xx.shape = (xx.size,1)\n    yy.shape = (yy.size,1)\n    pts = np.concatenate((xx,yy),axis=1)\n    pts = np.flipud(pts)\n     \n    # decide on num_frames\n    num_frames = 10\n    if 'num_frames' in kwargs:\n        num_frames = kwargs['num_frames']\n        num_frames = min(num_frames,len(xx))\n     \n    # animate\n    print ('starting animation rendering...')\n     \n    def animate(k):\n        # clear the panel\n        ax2.cla()\n         \n        # print rednering update\n        if np.mod(k+1,25) == 0:\n            print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n        if k == num_frames - 1:\n            print ('animation rendering complete!')\n            time.sleep(1.5)\n            clear_output()  \n         \n        ### take pt of grid and estimate with inputs ###        \n        # scatter every point up to k\n        for i in range(k+1):\n            pt = pts[i,:]\n            ax2.scatter(pt[0],pt[1],s = 100, c = 'k',edgecolor = 'w',linewidth = 1)\n             \n        # get current point and solve for weights\n        vec3 = pts[k,:]   \n        vec3.shape = (len(vec3),1)\n        A = np.concatenate((vec1copy,vec2copy),axis=1)\n        b = vec3\n        alpha = np.linalg.solve(A,b)\n \n        # plot original vectors\n        vector_draw(vec1copy.flatten(),ax2)\n        vector_draw(vec2copy.flatten(),ax2)\n \n        # send axis to vector adder for plotting\n        vec1 = np.asarray([alpha[0]*vec1copy[0],alpha[0]*vec1copy[1]]).flatten()\n        vec2 = np.asarray([alpha[1]*vec2copy[0],alpha[1]*vec2copy[1]]).flatten()\n        vector_add_plot(vec1,vec2,ax2)\n  \n        ax2.set_title(r'$w_1 = ' + str(round(alpha[0][0],3)) + ',\\,\\,\\,\\,\\,' + 'w_2 = ' + str(round(alpha[1][0],3)) +   '$',fontsize = 30)\n            \n        # plot x and y axes, and clean up\n        ax2.grid(True, which='both')\n        ax2.axhline(y=0, color='k', linewidth=1.5,zorder = 1)\n        ax2.axvline(x=0, color='k', linewidth=1,zorder = 1)\n \n        # set viewing limits\n        ax2.set_xlim([-6,6])\n        ax2.set_ylim([-6,6])\n         \n        # set tick label fonts\n        for tick in ax2.xaxis.get_major_ticks():\n            tick.label.set_fontsize(20) \n             \n        for tick in ax2.yaxis.get_major_ticks():\n            tick.label.set_fontsize(20) \n \n        # turn off grid\n        ax2.grid('off')\n         \n        # return artist\n        return artist,\n     \n    anim = animation.FuncAnimation(fig, animate,frames=num_frames, interval=num_frames, blit=True)\n         \n    # produce animation and save\n    fps = 50\n    if 'fps' in kwargs:\n        fps = kwargs['fps']\n    anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n    \n    clear_output()    \n\n # func,\ndef perfect_visualize_transform(savepath,vec1,vec2,**kwargs):\n    # size up vecs\n    vec1 = np.asarray(vec1)\n    vec2 = np.asarray(vec2)\n    vec1copy = copy.deepcopy(vec1)\n    vec1copy.shape = (len(vec1copy),1)\n    vec2copy = copy.deepcopy(vec2)\n    vec2copy.shape = (len(vec2copy),1)\n     \n    # renderer    \n    fig = plt.figure(figsize = (14,7))\n    artist = fig\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 2) \n    ax1 = plt.subplot(gs[0]);\n    ax2 = plt.subplot(gs[1]); \n    # gs.tight_layout(fig, rect=[0, 0.03, 1, 0.97]) \n     \n    ### create grid of points ###\n    s = np.linspace(-5,5,10)\n    xx,yy = np.meshgrid(s,s)\n    xx.shape = (xx.size,1)\n    yy.shape = (yy.size,1)\n    pts = np.concatenate((xx,yy),axis=1)\n    pts = np.flipud(pts)\n     \n    if 'pts' in kwargs:\n        pts = kwargs['pts']\n     \n    # decide on num_frames\n    num_frames = 10\n    if 'num_frames' in kwargs:\n        num_frames = kwargs['num_frames']\n        num_frames = min(num_frames,len(xx))\n     \n    # swing through points and compute coeffecients\n    alphas = []\n    for k in range(num_frames):\n        vec3 = pts[k,:]   \n        vec3.shape = (len(vec3),1)\n        A = np.concatenate((vec1copy,vec2copy),axis=1)\n        b = vec3\n        alpha = np.linalg.solve(A,b)\n        alphas.append(alpha)\n         \n    # set viewing limits\n    alpha_xmin = np.min([a[0][0] for a in alphas])\n    alpha_xmax = np.max([a[0][0] for a in alphas])\n    alpha_xgap = (alpha_xmax - alpha_xmin)*0.15\n    alpha_xmin -= alpha_xgap\n    alpha_xmin = np.min([-0.5,alpha_xmin])\n    alpha_xmax += alpha_xgap\n    alpha_xmax = np.max([1.5,alpha_xmax])\n    alpha_ymin = np.min([a[1][0] for a in alphas])\n    alpha_ymax = np.max([a[1][0] for a in alphas])\n    alpha_ygap = (alpha_ymax - alpha_ymin)*0.15\n    alpha_ymin -= alpha_ygap\n    alpha_ymin = np.min([-0.5,alpha_ymin])\n    alpha_ymax += alpha_ygap\n    alpha_ymax = np.max([1.5,alpha_ymax])\n \n    # animate\n    print ('starting animation rendering...')\n    def animate(k):\n        # clear the panel\n        ax1.cla()\n        ax2.cla()\n         \n        # print rednering update\n        if np.mod(k+1,25) == 0:\n            print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n        if k == num_frames - 1:\n            print ('animation rendering complete!')\n            time.sleep(1.5)\n            clear_output()  \n         \n        ### take pt of grid and estimate with inputs ###        \n        # scatter every point up to k\n        for i in range(k+1):\n            # plot original point\n            pt = pts[i,:]\n            ax1.scatter(pt[0],pt[1],s = 100, c = 'k',edgecolor = 'w',linewidth = 1)\n             \n            # plot transformed plot\n            pt = alphas[i]\n            ax2.scatter(pt[0],pt[1],s = 100, c = 'k',edgecolor = 'w',linewidth = 1)\n \n        # plot original vectors\n        vector_draw(vec1copy.flatten(),ax1)\n        vector_draw(vec2copy.flatten(),ax1)\n \n        # send axis to vector adder for plotting\n        alpha = alphas[k]\n        vec1 = np.asarray([alpha[0]*vec1copy[0],alpha[0]*vec1copy[1]]).flatten()\n        vec2 = np.asarray([alpha[1]*vec2copy[0],alpha[1]*vec2copy[1]]).flatten()\n        vector_add_plot(vec1,vec2,ax1)\n         \n        # now the transformed versions\n        vec1 = np.array([1,0]).flatten()\n        vec2 = np.array([0,1]).flatten()\n        vector_draw(vec1.flatten(),ax2)\n        vector_draw(vec2.flatten(),ax2)\n        vec1 = np.array([alpha[0][0],0]).flatten()\n        vec2 = np.array([0,alpha[1][0]]).flatten()\n        vector_add_plot(vec1,vec2,ax2)\n  \n        # set titles\n        title = r'$w_1 = ' + str(round(alpha[0][0],3)) + ',\\,\\,\\,\\,\\,' + 'w_2 = ' + str(round(alpha[1][0],3)) +   '$'\n        fig.suptitle(title, fontsize=23)\n            \n        # plot x and y axes, and clean up\n        ax1.grid(True, which='both')\n        ax1.axhline(y=0, color='k', linewidth=1.5,zorder = 1)\n        ax1.axvline(x=0, color='k', linewidth=1,zorder = 1)\n        ax1.set_xlim([-6,6])\n        ax1.set_ylim([-6,6])\n        ax1.grid('off')\n        ax1.set_xlabel(r'$x_1$',fontsize = 24)\n        ax1.set_ylabel(r'$x_2$',fontsize = 24,rotation = 0)\n \n        ax2.grid(True, which='both')\n        ax2.axhline(y=0, color='k', linewidth=1.5,zorder = 1)\n        ax2.axvline(x=0, color='k', linewidth=1,zorder = 1)\n        ax2.set_xlim([alpha_xmin,alpha_xmax])\n        ax2.set_ylim([alpha_ymin,alpha_ymax])\n        ax2.grid('off')\n        ax2.set_xlabel(r'$c_1$',fontsize = 24)\n        ax2.set_ylabel(r'$c_2$',fontsize = 24,rotation = 0)\n         \n        # set tick label fonts\n        for tick in ax1.xaxis.get_major_ticks():\n            tick.label.set_fontsize(20) \n        for tick in ax1.yaxis.get_major_ticks():\n            tick.label.set_fontsize(20) \n             \n        for tick in ax2.xaxis.get_major_ticks():\n            tick.label.set_fontsize(20) \n        for tick in ax2.yaxis.get_major_ticks():\n            tick.label.set_fontsize(20) \n         \n        # return artist\n        return artist,\n     \n    anim = animation.FuncAnimation(fig, animate,frames=num_frames, interval=num_frames, blit=True)\n         \n    # produce animation and save\n    fps = 50\n    if 'fps' in kwargs:\n        fps = kwargs['fps']\n    anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n    \n    clear_output()    \n\n# func,\ndef perfect_visualize_transform_static(vec1,vec2,**kwargs):\n    # size up vecs\n    vec1 = np.asarray(vec1)\n    vec2 = np.asarray(vec2)\n    vec1copy = copy.deepcopy(vec1)\n    vec1copy.shape = (len(vec1copy),1)\n    vec2copy = copy.deepcopy(vec2)\n    vec2copy.shape = (len(vec2copy),1)\n     \n    # renderer    \n    fig = plt.figure(figsize = (10,4))\n    artist = fig\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 2) \n    ax1 = plt.subplot(gs[0],aspect = 'equal');\n    ax2 = plt.subplot(gs[1],aspect = 'equal'); \n     \n    ### create grid of points ###\n    s = np.linspace(-5,5,10)\n    xx,yy = np.meshgrid(s,s)\n    xx.shape = (xx.size,1)\n    yy.shape = (yy.size,1)\n    pts = np.concatenate((xx,yy),axis=1)\n    pts = np.flipud(pts)\n     \n    if 'pts' in kwargs:\n        pts = kwargs['pts']\n        pts = pts.T\n             \n    # swing through points and compute coeffecients\n    alphas = []\n    for k in range(pts.shape[0]):\n        vec3 = pts[k,:]   \n        vec3.shape = (len(vec3),1)\n        A = np.concatenate((vec1copy,vec2copy),axis=1)\n        b = vec3\n        alpha = np.linalg.solve(A,b)\n        alphas.append(alpha)\n         \n    # set viewing limits for originals\n    xmin = np.min(pts[:,0])\n    xmax = np.max(pts[:,0])\n    xgap = (xmax - xmin)*0.15\n    xmin -= xgap\n    xmax += xgap\n    ymin = np.min(pts[:,1])\n    ymax = np.max(pts[:,1])\n    ygap = (ymax - ymin)*0.15\n    ymin -= ygap\n    ymax += ygap\n    \n    # set viewing limits for transformed space\n    alpha_xmin = np.min([a[0][0] for a in alphas])\n    alpha_xmax = np.max([a[0][0] for a in alphas])\n    alpha_xgap = (alpha_xmax - alpha_xmin)*0.15\n    alpha_xmin -= alpha_xgap\n    alpha_xmin = np.min([-0.5,alpha_xmin])\n    alpha_xmax += alpha_xgap\n    alpha_xmax = np.max([1.5,alpha_xmax])\n    alpha_ymin = np.min([a[1][0] for a in alphas])\n    alpha_ymax = np.max([a[1][0] for a in alphas])\n    alpha_ygap = (alpha_ymax - alpha_ymin)*0.15\n    alpha_ymin -= alpha_ygap\n    alpha_ymin = np.min([-0.5,alpha_ymin])\n    alpha_ymax += alpha_ygap\n    alpha_ymax = np.max([1.5,alpha_ymax])\n\n    ### take pt of grid and estimate with inputs ###        \n    # scatter every point up to k\n    for i in range(pts.shape[0]):\n        # plot original point\n        pt = pts[i,:]\n        ax1.scatter(pt[0],pt[1],s = 100, c = 'k',edgecolor = 'w',linewidth = 1)\n\n        # plot transformed plot\n        pt = alphas[i]\n        ax2.scatter(pt[0],pt[1],s = 100, c = 'k',edgecolor = 'w',linewidth = 1)\n \n    # plot original vectors\n    vector_draw(vec1copy.flatten(),ax1,color = 'red',zorder = 1)\n    vector_draw(vec2copy.flatten(),ax1,color = 'red',zorder = 1)\n\n    # send axis to vector adder for plotting         \n    vec1 = np.array([1,0]).flatten()\n    vec2 = np.array([0,1]).flatten()\n    vector_draw(vec1.flatten(),ax2,color = 'red',zorder = 1)\n    vector_draw(vec2.flatten(),ax2,color = 'red',zorder = 1)\n            \n    # plot x and y axes, and clean up\n    ax1.grid(True, which='both')\n    ax1.axhline(y=0, color='k', linewidth=1.5,zorder = 1)\n    ax1.axvline(x=0, color='k', linewidth=1,zorder = 1)\n    ax1.set_xlim([xmin,xmax])\n    ax1.set_ylim([ymin,ymax])\n    ax1.grid('off')\n    ax1.set_xlabel(r'$x_1$',fontsize = 22)\n    ax1.set_ylabel(r'$x_2$',fontsize = 22,rotation = 0,labelpad = 10)\n    ax1.set_title('original space',fontsize = 24)\n\n    ax2.grid(True, which='both')\n    ax2.axhline(y=0, color='k', linewidth=1.5,zorder = 1)\n    ax2.axvline(x=0, color='k', linewidth=1,zorder = 1)\n    ax2.set_xlim([alpha_xmin,alpha_xmax])\n    ax2.set_ylim([alpha_ymin,alpha_ymax])\n    ax2.grid('off')\n    ax2.set_xlabel(r'$c_1$',fontsize = 22)\n    ax2.set_ylabel(r'$c_2$',fontsize = 22,rotation = 0,labelpad = 10)\n    ax2.set_title('transformed space',fontsize = 24)\n         \n    # set tick label fonts\n    for tick in ax1.xaxis.get_major_ticks():\n        tick.label.set_fontsize(12) \n    for tick in ax1.yaxis.get_major_ticks():\n        tick.label.set_fontsize(12) \n             \n    for tick in ax2.xaxis.get_major_ticks():\n        tick.label.set_fontsize(12) \n    for tick in ax2.yaxis.get_major_ticks():\n        tick.label.set_fontsize(12) \n      \n# func,\ndef imperfect_visualize(savepath,vec1,**kwargs):\n    # size up vecs\n    vec1 = np.asarray(vec1)\n    vec1copy = copy.deepcopy(vec1)\n    vec1copy.shape = (len(vec1copy),1)\n     \n    # renderer    \n    fig = plt.figure(figsize = (4,4))\n    artist = fig\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 1) \n    #ax1 = plt.subplot(gs[0]); ax1.axis('off');\n    #ax3 = plt.subplot(gs[2]); ax3.axis('off');\n \n    # plot input function\n    ax2 = plt.subplot(gs[0]); # ax2.axis('equal');\n \n    ### create grid of points ###\n    s = np.linspace(-5,5,10)\n    xx,yy = np.meshgrid(s,s)\n    xx.shape = (xx.size,1)\n    yy.shape = (yy.size,1)\n    pts = np.concatenate((xx,yy),axis=1)\n    pts = np.flipud(pts)\n     \n    # decide on num_frames\n    num_frames = 10\n    if 'num_frames' in kwargs:\n        num_frames = kwargs['num_frames']\n        num_frames = min(num_frames,len(xx))\n     \n    # animate\n    print ('starting animation rendering...')\n     \n    def animate(k):\n        # clear the panel\n        ax2.cla()\n         \n        # print rednering update\n        if np.mod(k+1,25) == 0:\n            print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n        if k == num_frames - 1:\n            print ('animation rendering complete!')\n            time.sleep(1.5)\n            clear_output()  \n         \n        ### take pt of grid and estimate with inputs ###        \n        # scatter every point up to k\n        for i in range(k+1):\n            pt = pts[i,:]\n            ax2.scatter(pt[0],pt[1],s = 100, c = 'k',edgecolor = 'w',linewidth = 1)\n             \n        # get current point and solve for weights\n        vec3 = pts[k,:]   \n        vec3.shape = (len(vec3),1)\n        alpha = np.dot(vec1copy.T,vec3)/np.dot(vec1copy.T,vec1copy)  # /np.dot(A.T,A)\n \n        # plot original vectors\n        vector_draw(vec1copy.flatten(),ax2)\n \n        # send axis to vector adder for plotting\n        vec2 = np.asarray([alpha[0][0]*vec1copy[0],alpha[0][0]*vec1copy[1]]).flatten()\n        vector_scale(vec2,pts[k,:],ax2)\n        ax2.set_title(r'$w_1 = ' + str(round(alpha[0][0],3)) + '$',fontsize = 30)\n            \n        # plot x and y axes, and clean up\n        ax2.grid(True, which='both')\n        ax2.axhline(y=0, color='k', linewidth=1.5,zorder = 1)\n        ax2.axvline(x=0, color='k', linewidth=1,zorder = 1)\n \n        # set viewing limits\n        ax2.set_xlim([-6,6])\n        ax2.set_ylim([-6,6])\n         \n        # set tick label fonts\n        for tick in ax2.xaxis.get_major_ticks():\n            tick.label.set_fontsize(20) \n             \n        for tick in ax2.yaxis.get_major_ticks():\n            tick.label.set_fontsize(20) \n \n        # turn off grid\n        ax2.grid('off')\n        plt.axes().set_aspect('equal')\n         \n        # return artist\n        return artist,\n     \n    anim = animation.FuncAnimation(fig, animate,frames=num_frames, interval=num_frames, blit=True)\n         \n    # produce animation and save\n    fps = 50\n    if 'fps' in kwargs:\n        fps = kwargs['fps']\n    anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n    \n    clear_output()    \n\n# draw a vector\ndef vector_draw(vec,ax,**kwargs):\n    color = 'k'\n    if 'color' in kwargs:\n        color = kwargs['color']\n    zorder = 3 \n    if 'zorder' in kwargs:\n        zorder = kwargs['zorder']\n        \n    head_length = 0.5\n    head_width = 0.5\n    veclen = math.sqrt(vec[0]**2 + vec[1]**2)\n    vec_orig = copy.deepcopy(vec)\n    vec = (veclen - head_length)/veclen*vec\n    ax.arrow(0, 0, vec[0],vec[1], head_width=head_width, head_length=head_length, fc=color, ec=color,linewidth=2,zorder = zorder)\n      \ndef vector_scale(vec1,vec2,ax):\n    # plot each vector\n    head_length = 0.5\n    head_width = 0.5\n    veclen = math.sqrt(vec1[0]**2 + vec1[1]**2)\n    vec1_orig = copy.deepcopy(vec1)\n    vec1 = (veclen - head_length)/veclen*vec1\n    ax.arrow(0, 0, vec1[0],vec1[1], head_width=head_width, head_length=head_length, fc='r', ec='r',linewidth=2,zorder = 2)\n    ax.scatter(vec1[0],vec1[1],s = 100, c = 'b',edgecolor = 'w',linewidth = 1,zorder = 3)\n \n    # connect them\n    ax.plot([vec1[0],vec2[0]],[vec1[1],vec2[1]],linestyle= '--',c='b',zorder=3,linewidth = 1)\n \n     \n# simple plot of 2d vector addition / paralellagram law\ndef vector_add_plot(vec1,vec2,ax):   \n    # plot each vector\n    head_length = 0.5\n    head_width = 0.5\n    veclen = math.sqrt(vec1[0]**2 + vec1[1]**2)\n    vec1_orig = copy.deepcopy(vec1)\n    vec1 = (veclen - head_length)/veclen*vec1\n    veclen = math.sqrt(vec2[0]**2 + vec2[1]**2)\n    vec2_orig = copy.deepcopy(vec2)\n    vec2 = (veclen - head_length)/veclen*vec2\n    ax.arrow(0, 0, vec1[0],vec1[1], head_width=head_width, head_length=head_length, fc='b', ec='b',linewidth=2,zorder = 2)\n    ax.arrow(0, 0, vec2[0],vec2[1], head_width=head_width, head_length=head_length, fc='b', ec='b',linewidth=2,zorder = 2)\n     \n    # plot the sum of the two vectors\n    vec3 = vec1_orig + vec2_orig\n    vec3_orig = copy.deepcopy(vec3)\n    veclen = math.sqrt(vec3[0]**2 + vec3[1]**2)\n    vec3 = (veclen - math.sqrt(head_length))/veclen*vec3\n    ax.arrow(0, 0, vec3[0],vec3[1], head_width=head_width, head_length=head_length, fc='r', ec='r',linewidth=3,zorder=2)\n    \n    # connect them\n    ax.plot([vec1_orig[0],vec3_orig[0]],[vec1_orig[1],vec3_orig[1]],linestyle= '--',c='b',zorder=2,linewidth = 1)\n    ax.plot([vec2_orig[0],vec3_orig[0]],[vec2_orig[1],vec3_orig[1]],linestyle= '--',c='b',zorder=2,linewidth = 1)\n\n    """
mlrefined_libraries/linear_algebra_library/transform_animators.py,47,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nimport time\nfrom matplotlib import gridspec\nimport copy\n\n# import autograd functionality\nimport numpy as np\nimport math\n\n# make the adjustable grid\ndef make_warpable_grid(horz_min,horz_max,vert_min,vert_max):\n    s = np.linspace(-10,10,40)\n    s.shape = (len(s),1)\n    g = np.array([-10,-10])\n    g.shape = (1,len(g))\n    e = np.linspace(-10,10,200)\n    e.shape = (len(e),1)\n    f = np.ones((200,1))\n    f.shape = (len(f),1)\n    for a in s:\n        t = a*f\n        h = np.concatenate((e,t),axis = 1)\n        i = np.concatenate((t,e),axis = 1)\n        j = np.concatenate((h,i),axis = 0)\n        g = np.concatenate((g,j),axis = 0)\n\n    grid = g[1:,:]\n    return grid\n\n# animator for showing grid of points transformed by linear transform\ndef transform2d_animator(savepath,mat1,**kwargs):  \n    if len(mat1.shape) > 2 or len(np.argwhere(np.asarray(mat1.shape) > 2)) > 0:\n        print ('input matrix must be 2x2')\n        return \n    orig_mat1 = copy.deepcopy(mat1)\n                                    \n    # define number of frames\n    num_frames = 100\n    if 'num_frames' in kwargs:\n        num_frames = kwargs['num_frames']\n        \n    # create points to help visualize transformation - default is a circle\n    s = np.linspace(0,2*np.pi,1000)\n    x = 2*np.cos(s)\n    x.shape = (len(s),1)\n    y = 2*np.sin(s)\n    y.shape = (len(s),1)\n    pts = np.concatenate((x,y),axis=1)\n    orig_pts = copy.deepcopy(pts)\n    \n    # grab points if input\n    if 'pts' in kwargs:\n        pts = kwargs['pts']\n        orig_pts = copy.deepcopy(pts)\n\n    # type of plot - continuous or scatter\n    plot_type = 'continuous'\n    if 'plot_type' in kwargs:\n        plot_type = kwargs['plot_type']\n        \n    # draw on eigenvectors?\n    eigvecs_on = False\n    if 'eigvecs_on' in kwargs:\n        eigvecs_on = kwargs['eigvecs_on']\n        \n    # define convex-combo parameter - via num_frames\n    alphas = np.linspace(0,1,num_frames)\n\n    # define grid of points via meshgrid\n    viewx = 4\n    viewgap = 0.1*viewx\n    viewx2 = 10\n    grid = make_warpable_grid(horz_min=-viewx2,horz_max=viewx2,vert_min=-viewx2,vert_max=viewx2)\n    orig_grid = copy.deepcopy(grid)\n    \n    # initialize figure\n    fig = plt.figure(figsize = (16,8))\n    artist = fig\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,3, 1]) \n    ax1 = plt.subplot(gs[0]); ax1.axis('off');\n    ax3 = plt.subplot(gs[2]); ax3.axis('off');\n    \n    # plot input function\n    ax = plt.subplot(gs[1])\n\n    # animate\n    def animate(k):\n        # clear the panel\n        ax.cla()\n        \n        # print rednering update\n        if np.mod(k+1,25) == 0:\n            print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n        if k == num_frames - 1:\n            print ('animation rendering complete!')\n            time.sleep(1.5)\n            clear_output()  \n        \n        # get current lambda, define current matrix\n        alpha = alphas[k]\n        mat1 = alpha*orig_mat1 + (1 - alpha)*np.eye(2)\n\n        # compute current transformation of points and plot\n        grid = np.dot(mat1,orig_grid.T).T\n            \n        # plot warped grid\n        for i in range(80):\n            ax.plot(grid[200*i:(i+1)*200,0],grid[200*i:(i+1)*200,1],color = [0.75,0.75,0.75],linewidth = 1,zorder = 0)  \n            \n        # plot input points\n        if len(orig_pts) > 0:\n            pts = np.dot(mat1,orig_pts.T).T\n            \n            # switch for plot type\n            if plot_type == 'continuous':\n                ax.plot(pts[:,0],pts[:,1],c = 'k',linewidth = 3)\n            elif plot_type == 'scatter':\n                ax.scatter(pts[:,0],pts[:,1],c = 'k',edgecolor = 'w',s = 50,linewidth = 1)\n        \n        # plot eigenvectors?\n        if eigvecs_on == True and k > 0:\n            vals, vecs = np.linalg.eig(mat1)\n            \n            # plot first eigenvector\n            head_length = 0.4\n            vec1 = vals[0]*vecs[:,0]\n            ax.arrow(0, 0, vec1[0],vec1[1], head_width=0.25, head_length=head_length, fc='k', ec='k',linewidth=2,zorder = 3)\n\n            # plot first eigenvector\n            vec2 = vals[1]*vecs[:,1]\n            ax.arrow(0, 0, vec2[0],vec2[1], head_width=0.25, head_length=head_length, fc='k', ec='k',linewidth=2,zorder = 3)\n        \n        # plot x and y axes, and clean up\n        plt.grid(True, which='both')\n        plt.axhline(y=0, color='k', linewidth=1)\n        plt.axvline(x=0, color='k', linewidth=1)\n   \n        # return artist to render\n        ax.set_xlim([-viewx - viewgap,viewx + viewgap])\n        ax.set_ylim([-viewx - viewgap,viewx + viewgap])\n        \n        return artist,\n        \n    anim = animation.FuncAnimation(fig, animate,frames=num_frames, interval=num_frames, blit=True)\n        \n    # produce animation and save\n    fps = 50\n    if 'fps' in kwargs:\n        fps = kwargs['fps']\n    anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n    \n    clear_output()   \n\n# animator for showing grid of points transformed by linear transform\ndef nonlinear_transform2d_animator(func,savepath,**kwargs):  \n    # define number of frames\n    num_frames = 100\n    if 'num_frames' in kwargs:\n        num_frames = kwargs['num_frames']\n        \n    # create points to help visualize transformation - default is a circle\n    s = np.linspace(0,2*np.pi,1000)\n    x = 2*np.cos(s)\n    x.shape = (len(s),1)\n    y = 2*np.sin(s)\n    y.shape = (len(s),1)\n    pts = np.concatenate((x,y),axis=1)\n    orig_pts = copy.deepcopy(pts)\n    \n    # grab points if input\n    if 'pts' in kwargs:\n        pts = kwargs['pts']\n        orig_pts = copy.deepcopy(pts)\n\n    # type of plot - continuous or scatter\n    plot_type = 'continuous'\n    if 'plot_type' in kwargs:\n        plot_type = kwargs['plot_type']\n        \n    # draw on eigenvectors?\n    eigvecs_on = False\n    if 'eigvecs_on' in kwargs:\n        eigvecs_on = kwargs['eigvecs_on']\n        \n    # define convex-combo parameter - via num_frames\n    alphas = np.linspace(0,1,num_frames)\n\n    # define grid of points via meshgrid\n    viewx = 4\n    viewgap = 0.1*viewx\n    viewx2 = 10\n    grid = make_warpable_grid(horz_min=-viewx2,horz_max=viewx2,vert_min=-viewx2,vert_max=viewx2)\n    orig_grid = copy.deepcopy(grid)\n    \n    # evaluate both the grid and input points through function\n    func_orig_grid = func(orig_grid.T).T\n    func_orig_pts = func(pts.T).T\n    \n    # initialize figure\n    fig = plt.figure(figsize = (16,8))\n    artist = fig\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,3, 1]) \n    ax1 = plt.subplot(gs[0]); ax1.axis('off');\n    ax3 = plt.subplot(gs[2]); ax3.axis('off');\n    \n    # plot input function\n    ax = plt.subplot(gs[1])\n    \n    # print update\n    print ('starting animation rendering...')\n    \n    # animate\n    def animate(k):\n        # clear the panel\n        ax.cla()\n        \n        # print rednering update\n        if np.mod(k+1,25) == 0:\n            print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n        if k == num_frames - 1:\n            print ('animation rendering complete!')\n            time.sleep(1.5)\n            clear_output()  \n        \n        # get current lambda, define current matrix\n        alpha = alphas[k]\n\n        # compute current transformation of points and plot\n        grid = (1-alpha)*orig_grid + alpha*func_orig_grid\n            \n        # plot warped grid\n        for i in range(80):\n            ax.plot(grid[200*i:(i+1)*200,0],grid[200*i:(i+1)*200,1],color = [0.75,0.75,0.75],linewidth = 1,zorder = 0)  \n            \n        # plot input points\n        if len(orig_pts) > 0:\n            pts = (1-alpha)*orig_pts + alpha*func_orig_pts\n            \n            # switch for plot type\n            if plot_type == 'continuous':\n                ax.plot(pts[:,0],pts[:,1],c = 'k',linewidth = 3)\n            elif plot_type == 'scatter':\n                ax.scatter(pts[:,0],pts[:,1],c = 'k',edgecolor = 'w',s = 50,linewidth = 1)\n        \n        # plot eigenvectors?\n        if eigvecs_on == True and k > 0:\n            vals, vecs = np.linalg.eig(mat1)\n            \n            # plot first eigenvector\n            head_length = 0.4\n            vec1 = vals[0]*vecs[:,0]\n            ax.arrow(0, 0, vec1[0],vec1[1], head_width=0.25, head_length=head_length, fc='k', ec='k',linewidth=2,zorder = 3)\n\n            # plot first eigenvector\n            vec2 = vals[1]*vecs[:,1]\n            ax.arrow(0, 0, vec2[0],vec2[1], head_width=0.25, head_length=head_length, fc='k', ec='k',linewidth=2,zorder = 3)\n        \n        # plot x and y axes, and clean up\n        plt.grid(True, which='both')\n        plt.axhline(y=0, color='k', linewidth=1)\n        plt.axvline(x=0, color='k', linewidth=1)\n   \n        # return artist to render\n        ax.set_xlim([-viewx - viewgap,viewx + viewgap])\n        ax.set_ylim([-viewx - viewgap,viewx + viewgap])\n        \n        return artist,\n        \n    anim = animation.FuncAnimation(fig, animate,frames=num_frames, interval=num_frames, blit=True)\n        \n    # produce animation and save\n    fps = 50\n    if 'fps' in kwargs:\n        fps = kwargs['fps']\n    anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n    \n        \n# animate the method\ndef inner_product_visualizer(savepath,**kwargs):\n    # set number of frames for animation\n    num_frames = 300                          # number of slides to create - the input range [-3,3] is divided evenly by this number\n    if 'num_frames' in kwargs:\n        num_frames = kwargs['num_frames']\n    \n    # initialize figure\n    fig = plt.figure(figsize = (16,8))\n    artist = fig\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1,1],wspace=0.3, hspace=0.05) \n    ax1 = plt.subplot(gs[0]);\n    ax2 = plt.subplot(gs[1]); \n    \n    # create dataset for unit circle\n    v = np.linspace(0,2*np.pi,100)\n    s = np.sin(v)\n    s.shape = (len(s),1)\n    t = np.cos(v)\n    t.shape = (len(t),1)\n    \n    # user defined starting point on the circle\n    start = 0\n    if 'start' in kwargs:\n        start = kwargs['start']\n        \n    # create span of angles to plot over\n    v = np.linspace(start,2*np.pi + start,num_frames)\n    y = 0.87*np.sin(v)\n    x = 0.87*np.cos(v)\n    \n    # create linspace for sine/cosine plots\n    w = np.linspace(start,2*np.pi + start,300)\n    \n    # define colors for sine / cosine\n    colors = ['salmon','cornflowerblue']\n    \n    # print update\n    print ('starting animation rendering...')\n    \n    # animation sub-function\n    def animate(k):\n        # clear panels for next slide\n        ax1.cla()\n        ax2.cla()\n        \n        # print rendering update\n        if np.mod(k+1,25) == 0:\n            print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n        if k == num_frames - 1:\n            print ('animation rendering complete!')\n            time.sleep(1.5)\n            clear_output()\n        \n        ### setup left panel ###\n        # plot circle with lines in left panel\n        ax1.plot(s,t,color = 'k',linewidth = 3)\n        \n        # plot moving arrow\n        ax1.arrow(0, 0, x[k], y[k], head_width=0.1, head_length=0.1, fc='k', color = colors[1],linewidth=3,zorder = 3)\n        \n        # plot fixed arrow\n        ax1.arrow(0, 0, 0.87, 0, head_width=0.1, head_length=0.1, fc='k', color = 'k',linewidth=3,zorder = 3)\n\n        # clean up panel\n        ax1.grid(True, which='both')\n        ax1.axhline(y=0, color='k')\n        ax1.axvline(x=0, color='k')\n        \n        ### setup right panel ###\n        # determine closest value in space of sine/cosine input\n        current_angle = v[k]\n        ind = np.argmin(np.abs(w - current_angle))\n        p = w[:ind+1]\n        \n        # plot sine wave so far\n        ax2.plot(p,np.cos(p),color = colors[1],linewidth=4,zorder = 3)\n        \n        # cleanup plot\n        ax2.grid(True, which='both')\n        ax2.axhline(y=0, color='k')\n        ax2.axvline(x=0, color='k')   \n        ax2.set_xlim([-0.3 + start,2*np.pi + 0.3 + start])\n        ax2.set_ylim([-1.2,1.2])\n        \n        # add legend\n        ax2.legend([r'cos$(\\theta)$'],loc='center left', bbox_to_anchor=(0.33, 1.05),fontsize=18,ncol=2)\n\n        return artist,\n\n    anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n\n    # produce animation and save\n    fps = 50\n    if 'fps' in kwargs:\n        fps = kwargs['fps']\n    anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n    \nclass quadratic_3d_flexer:\n    '''\n    Draw 3d quadratic ranging from convex\n    '''\n\n    # compute first order approximation\n    def draw_it(savepath,**kwargs):  \n        ### other options\n        # size of figure\n        set_figsize = 4\n        if 'set_figsize' in kwargs:\n            set_figsize = kwargs['set_figsize']\n            \n        # turn axis on or off\n        set_axis = 'off'\n        if 'set_axis' in kwargs:\n            set_axis = kwargs['set_axis']\n            \n        # plot title\n        set_title = ''\n        if 'set_title' in kwargs:\n            set_title = kwargs['set_title']\n            \n        # horizontal and vertical axis labels\n        horiz_1_label = ''\n        if 'horiz_1_label' in kwargs:\n            horiz_1_label = kwargs['horiz_1_label']\n            \n        horiz_2_label = ''\n        if 'horiz_2_label' in kwargs:\n            horiz_2_label = kwargs['horiz_2_label']\n            \n        vert_label = ''\n        if 'vert_label' in kwargs:\n            vert_label = kwargs['vert_label']\n            \n        # set width of plot\n        input_range = np.linspace(-2,2,1000)                  # input range for original function\n        if 'input_range' in kwargs:\n            input_range = kwargs['input_range']\n            \n        # set viewing angle on plot\n        view = [-20,60]\n        if 'view' in kwargs:\n            view = kwargs['view']\n        \n        num_slides = 100\n        if 'num_slides' in kwargs:\n            num_frames = kwargs['num_slides']\n            \n        alpha_values = np.linspace(-1,1,num_frames)\n        \n        # initialize figure\n        fig = plt.figure(figsize = (set_figsize,set_figsize))\n        fig.subplots_adjust(left=0,right=1,bottom=0,top=1)   # remove whitespace around 3d figure\n        artist = fig\n        ax = fig.add_subplot(111, projection='3d')\n        \n        # print update\n        print ('starting animation rendering...')\n        \n        # animation sub-function\n        def animate(k):\n            ax.cla()\n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            # quadratic to plot\n            alpha = alpha_values[k]\n            g = lambda w: w[0]**2 + alpha*w[1]**2\n            \n            # create grid from plotting range\n            w1_vals,w2_vals = np.meshgrid(input_range,input_range)\n            w1_vals.shape = (len(input_range)**2,1)\n            w2_vals.shape = (len(input_range)**2,1)\n            g_vals = g([w1_vals,w2_vals])\n        \n            # vals for cost surface\n            w1_vals.shape = (len(input_range),len(input_range))\n            w2_vals.shape = (len(input_range),len(input_range))\n            g_vals.shape = (len(input_range),len(input_range))\n\n            g_range = np.amax(g_vals) - np.amin(g_vals)             # used for cleaning up final plot\n            ggap = g_range*0.5\n\n            # plot original function\n            ax.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.3,color = 'lime',rstride=100, cstride=100,linewidth=1,edgecolor = 'k') \n\n            # clean up plotting area\n            ax.set_title(set_title,fontsize = 15)\n            ax.set_xlabel(horiz_1_label,fontsize = 15)\n            ax.set_ylabel(horiz_2_label,fontsize = 15)\n            ax.set_zlabel(vert_label,fontsize = 15)\n            ax.view_init(view[0],view[1])\n            ax.axis(set_axis)\n            #ax.set_zlim([-2.5,2.5])\n \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=len(alpha_values), interval=len(alpha_values), blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()\n\n    \nclass quadratic_3d_rotater:\n    '''\n    Draw 3d quadratic and rotate\n    '''\n\n    # compute first order approximation\n    def draw_it(savepath,**kwargs):  \n        # get user-defined function\n        g = kwargs['func']\n        \n        ### other options\n        # size of figure\n        set_figsize = 4\n        if 'set_figsize' in kwargs:\n            set_figsize = kwargs['set_figsize']\n            \n        # turn axis on or off\n        set_axis = 'off'\n        if 'set_axis' in kwargs:\n            set_axis = kwargs['set_axis']\n            \n        # plot title\n        set_title = ''\n        if 'set_title' in kwargs:\n            set_title = kwargs['set_title']\n            \n        # horizontal and vertical axis labels\n        horiz_1_label = ''\n        if 'horiz_1_label' in kwargs:\n            horiz_1_label = kwargs['horiz_1_label']\n            \n        horiz_2_label = ''\n        if 'horiz_2_label' in kwargs:\n            horiz_2_label = kwargs['horiz_2_label']\n            \n        vert_label = ''\n        if 'vert_label' in kwargs:\n            vert_label = kwargs['vert_label']\n            \n        # set width of plot\n        input_range = np.linspace(-2,2,1000)                  # input range for original function\n        if 'input_range' in kwargs:\n            input_range = kwargs['input_range']\n            \n        # set viewing angle on plot\n        view = [-20,60]\n        if 'view' in kwargs:\n            view = kwargs['view']\n        \n        num_slides = 100\n        if 'num_slides' in kwargs:\n            num_frames = kwargs['num_slides']\n        color = 'r'\n        if 'color' in kwargs:\n            color = kwargs['color']\n            \n        theta_values = np.linspace(0,180,num_frames)\n        \n        # initialize figure\n        fig = plt.figure(figsize = (set_figsize,set_figsize))\n        fig.subplots_adjust(left=0,right=1,bottom=0,top=1)   # remove whitespace around 3d figure\n        artist = fig\n        ax = fig.add_subplot(111, projection='3d')\n        \n        # create grid from plotting range\n        w1_vals_orig,w2_vals_orig = np.meshgrid(input_range,input_range)\n        w1_vals_orig.shape = (len(input_range)**2,1)\n        w2_vals_orig.shape = (len(input_range)**2,1)\n        w_both = np.concatenate((w1_vals_orig,w2_vals_orig),axis=1).T\n        \n        # vals for cost surface\n        g_vals = g(w_both)\n        g_vals.shape = (len(input_range),len(input_range))\n        w1_vals_orig.shape = (len(input_range),len(input_range))\n        w2_vals_orig.shape = (len(input_range),len(input_range))\n\n        # print update\n        print ('starting animation rendering...')\n        \n        # animation sub-function\n        def animate(k):\n            ax.cla()\n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n\n            # plot original function\n            ax.plot_surface(w1_vals_orig,w2_vals_orig,g_vals,alpha = 0.3,color = color,rstride=100, cstride=100,linewidth=1,edgecolor = 'k') \n            # rotate input\n            theta = theta_values[k]\n            ax.view_init(view[0],view[1] + theta)\n            \n            # clean up plotting area\n            ax.set_title(set_title,fontsize = 15)\n            ax.set_xlabel(horiz_1_label,fontsize = 15)\n            ax.set_ylabel(horiz_2_label,fontsize = 15)\n            ax.set_zlabel(vert_label,fontsize = 15)\n            ax.axis(set_axis)\n \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=len(theta_values), interval=len(theta_values), blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()"""
mlrefined_libraries/linear_algebra_library/vector_plots.py,0,"b""import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import gridspec\nfrom IPython.display import display, HTML\nimport copy\nimport math\n\n# simple plot of 2d vector addition / paralellagram law\ndef single_plot(vec1,**kwargs): \n    guides = False\n    if 'guides' in kwargs:\n        guides = kwargs['guides']\n    \n    # create figure\n    fig = plt.figure(figsize = (12,4))\n\n    # create subplot with 2 panels\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1]) \n    ax1 = plt.subplot(gs[0]); \n    ax2 = plt.subplot(gs[1]);\n\n    ### plot point in left panel\n    ax1.scatter(vec1[0],vec1[1],c = 'k',edgecolor = 'w',s = 50,linewidth = 1)\n    \n    ### plot arrow in right panel\n    head_length = 0.4\n    veclen = math.sqrt(vec1[0]**2 + vec1[1]**2)\n    vec1_orig = copy.deepcopy(vec1)\n    vec1 = (veclen - head_length)/veclen*vec1\n\n    ax2.arrow(0, 0, vec1[0],vec1[1], head_width=0.25, head_length=head_length, fc='k', ec='k',linewidth=2,zorder = 3)\n\n    # draw guides?\n    if guides == True:\n        ax1.plot([0,vec1_orig[0]],[vec1_orig[1],vec1_orig[1]],linestyle= '--',c='b',zorder=2,linewidth = 0.75)\n        ax1.plot([vec1_orig[0],vec1_orig[0]],[0,vec1_orig[1]],linestyle= '--',c='b',zorder=2,linewidth = 0.75)\n        ax2.plot([0,vec1_orig[0]],[vec1_orig[1],vec1_orig[1]],linestyle= '--',c='b',zorder=2,linewidth = 0.75)\n        ax2.plot([vec1_orig[0],vec1_orig[0]],[0,vec1_orig[1]],linestyle= '--',c='b',zorder=2,linewidth = 0.75)\n\n    # plot x and y axes, and clean up\n    ax1.grid(True, which='both')\n    ax1.axhline(y=0, color='k', linewidth=1,zorder = 1)\n    ax1.axvline(x=0, color='k', linewidth=1,zorder = 1)\n    \n    ax2.grid(True, which='both')\n    ax2.axhline(y=0, color='k', linewidth=1,zorder = 1)\n    ax2.axvline(x=0, color='k', linewidth=1,zorder = 1)\n        \n    # set viewing limits\n    xmax = max(vec1[0],0)\n    xmin = min(vec1[0],0)\n    xgap = (xmax - xmin)*0.3\n    xmax = xmax + xgap\n    xmin = xmin - xgap\n\n    ymax = max(vec1[1],0)\n    ymin = min(vec1[1],0)\n    ygap = (ymax - ymin)*0.3\n    ymax = ymax + ygap\n    ymin = ymin - ygap\n    ax1.set_xlim([xmin,xmax])\n    ax1.set_ylim([ymin,ymax])\n    ax2.set_xlim([xmin,xmax])\n    ax2.set_ylim([ymin,ymax])\n    \n    # renderer\n    plt.style.use('ggplot')\n    plt.show()\n\n# simple plot of 2d vector addition / paralellagram law\ndef vector_add_plot(vec1,vec2):     \n    # renderer\n    plt.style.use('ggplot')\n    \n    fig = plt.figure(figsize = (12,4))\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,3, 1]) \n    ax1 = plt.subplot(gs[0]); ax1.axis('off');\n    ax3 = plt.subplot(gs[2]); ax3.axis('off');\n\n    # plot input function\n    ax2 = plt.subplot(gs[1])\n\n    # plot each vector\n    head_length = 0.4\n    veclen = math.sqrt(vec1[0]**2 + vec1[1]**2)\n    vec1_orig = copy.deepcopy(vec1)\n    vec1 = (veclen - head_length)/veclen*vec1\n    veclen = math.sqrt(vec2[0]**2 + vec2[1]**2)\n    vec2_orig = copy.deepcopy(vec2)\n    vec2 = (veclen - head_length)/veclen*vec2\n    ax2.arrow(0, 0, vec1[0],vec1[1], head_width=0.25, head_length=head_length, fc='k', ec='k',linewidth=2,zorder = 3)\n    ax2.arrow(0, 0, vec2[0],vec2[1], head_width=0.25, head_length=head_length, fc='k', ec='k',linewidth=2,zorder = 3)\n    \n    # plot the sum of the two vectors\n    vec3 = vec1_orig + vec2_orig\n    vec3_orig = copy.deepcopy(vec3)\n    veclen = math.sqrt(vec3[0]**2 + vec3[1]**2)\n    vec3 = (veclen - head_length)/veclen*vec3\n    ax2.arrow(0, 0, vec3[0],vec3[1], head_width=0.25, head_length=head_length, fc='r', ec='r',linewidth=2,zorder=3)\n    \n    # connect them\n    ax2.plot([vec1_orig[0],vec3_orig[0]],[vec1_orig[1],vec3_orig[1]],linestyle= '--',c='b',zorder=2,linewidth = 0.75)\n    ax2.plot([vec2_orig[0],vec3_orig[0]],[vec2_orig[1],vec3_orig[1]],linestyle= '--',c='b',zorder=2,linewidth = 0.75)\n\n    # plot x and y axes, and clean up\n    ax2.grid(True, which='both')\n    ax2.axhline(y=0, color='k', linewidth=1,zorder = 1)\n    ax2.axvline(x=0, color='k', linewidth=1,zorder = 1)\n        \n    # set viewing limits\n    xmax = max(vec1[0],vec2[0],vec3[0],0)\n    xmin = min(vec1[0],vec2[0],vec3[0],0)\n    xgap = (xmax - xmin)*0.3\n    xmax = xmax + xgap\n    xmin = xmin - xgap\n\n    ymax = max(vec1[1],vec2[1],vec3[1],0)\n    ymin = min(vec1[1],vec2[1],vec3[1],0)\n    ygap = (ymax - ymin)*0.3\n    ymax = ymax + ygap\n    ymin = ymin - ygap\n    ax2.set_xlim([xmin,xmax])\n    ax2.set_ylim([ymin,ymax])\n\n    plt.show()\n    \n    \n# simple plot of 2d vector linear combination / paralellagram law\ndef vector_linear_combination_plot(vec1, vec2, alpha1, alpha2):     \n    # renderer\n    fig = plt.figure(figsize = (12,4))\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,3, 1]) \n    ax1 = plt.subplot(gs[0]); ax1.axis('off');\n    ax3 = plt.subplot(gs[2]); ax3.axis('off');\n\n    # plot input function\n    ax2 = plt.subplot(gs[1])\n\n    # plot each vector\n    head_length = 0.4\n    veclen = math.sqrt(vec1[0]**2 + vec1[1]**2)\n    vec1_orig = copy.deepcopy(vec1)\n    vec1 = (veclen - head_length)/veclen*vec1\n    veclen = math.sqrt(vec2[0]**2 + vec2[1]**2)\n    vec2_orig = copy.deepcopy(vec2)\n    vec2 = (veclen - head_length)/veclen*vec2\n    ax2.arrow(0, 0, vec1[0],vec1[1], head_width=0.25, head_length=head_length, fc='k', ec='k',linewidth=2,zorder = 3)\n    ax2.arrow(0, 0, vec2[0],vec2[1], head_width=0.25, head_length=head_length, fc='k', ec='k',linewidth=2,zorder = 3)\n    \n    # plot the linear combination of the two vectors\n    vec3 = alpha1*vec1_orig + alpha2*vec2_orig\n    vec3_orig = copy.deepcopy(vec3)\n    veclen = math.sqrt(vec3[0]**2 + vec3[1]**2)\n    vec3 = (veclen - head_length)/veclen*vec3\n    ax2.arrow(0, 0, vec3[0],vec3[1], head_width=0.25, head_length=head_length, fc='r', ec='r',linewidth=2,zorder=3)\n    \n    # connect them\n    ax2.plot([vec1_orig[0],alpha1*vec1_orig[0]],[vec1_orig[1],alpha1*vec1_orig[1]],linestyle= '--',c='k',zorder=2,linewidth = 0.75)\n    ax2.plot([vec2_orig[0],alpha2*vec2_orig[0]],[vec2_orig[1],alpha2*vec2_orig[1]],linestyle= '--',c='k',zorder=2,linewidth = 0.75)\n    \n    # connect them\n    ax2.plot([alpha1*vec1_orig[0],vec3_orig[0]],[alpha1*vec1_orig[1],vec3_orig[1]],linestyle= '--',c='b',zorder=2,linewidth = 0.75)\n    ax2.plot([alpha2*vec2_orig[0],vec3_orig[0]],[alpha2*vec2_orig[1],vec3_orig[1]],linestyle= '--',c='b',zorder=2,linewidth = 0.75)\n\n    # plot x and y axes, and clean up\n    ax2.grid(True, which='both')\n    ax2.axhline(y=0, color='k', linewidth=1,zorder = 1)\n    ax2.axvline(x=0, color='k', linewidth=1,zorder = 1)\n        \n    # set viewing limits\n    xmax = max(vec1[0], alpha1*vec1[0], vec2[0], alpha2*vec2[0], vec3[0], 0)\n    xmin = min(vec1[0], alpha1*vec1[0], vec2[0], alpha2*vec2[0], vec3[0], 0)\n    xgap = (xmax - xmin)*0.3\n    xmax = xmax + xgap\n    xmin = xmin - xgap\n\n    ymax = max(vec1[1],vec2[1],vec3[1],0)\n    ymin = min(vec1[1],vec2[1],vec3[1],0)\n    ygap = (ymax - ymin)*0.3\n    ymax = ymax + ygap\n    ymin = ymin - ygap\n    ax2.set_xlim([xmin,xmax])\n    ax2.set_ylim([ymin,ymax])\n\n    plt.show()    \n    """
mlrefined_libraries/math_optimization_library/__init__.py,0,b''
mlrefined_libraries/math_optimization_library/animation_plotter.py,26,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   \nfrom autograd import hessian as compute_hess\nimport autograd.numpy as np\nimport math\nimport time\n\nclass Visualizer:\n    '''\n    Animate runs of gradient descent and Newton's method, showing the correspnoding Taylor Series approximations as you go along.\n    Run the algorithm first, and input the resulting weight history into this wrapper.\n    ''' \n\n    ##### animate gradient descent method using single-input function #####\n    def gradient_descent(self,g,w_hist,savepath,**kwargs):\n        # compute gradient of input function\n        grad = compute_grad(g)              # gradient of input function\n\n        # decide on viewing range\n        wmin = -3.1\n        wmax = 3.1\n        if 'wmin' in kwargs:            \n            wmin = kwargs['wmin']\n        if 'wmax' in kwargs:\n            wmax = kwargs['wmax']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n        \n        # remove whitespace from figure\n        #fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        #fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,4,1]) \n\n        ax1 = plt.subplot(gs[0]); ax1.axis('off')\n        ax3 = plt.subplot(gs[2]); ax3.axis('off')\n        ax = plt.subplot(gs[1]); \n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(wmin,wmax,200)\n        g_plot = g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.1\n        width = 30\n        \n        # make color spectrum for points\n        colorspec = self.make_colorspec(w_hist)\n        \n        # animation sub-function\n        num_frames = 2*len(w_hist)+2\n        print ('starting animation rendering...')\n        def animate(t):\n            ax.cla()\n            k = math.floor((t+1)/float(2))\n            \n            # print rendering update            \n            if np.mod(t+1,25) == 0:\n                print ('rendering animation frame ' + str(t+1) + ' of ' + str(num_frames))\n            if t == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            # plot function\n            ax.plot(w_plot,g_plot,color = 'k',zorder = 2)                           # plot function\n            \n            # plot initial point and evaluation\n            if k == 0:\n                w_val = w_hist[0]\n                g_val = g(w_val)\n                ax.scatter(w_val,g_val,s = 90,c = colorspec[k],edgecolor = 'k',linewidth = 0.5*((1/(float(k) + 1)))**(0.4),zorder = 3,marker = 'X')            # evaluation on function\n                ax.scatter(w_val,0,s = 90,facecolor = colorspec[k],edgecolor = 'k',linewidth = 0.5*((1/(float(k) + 1)))**(0.4), zorder = 3)\n                \n                # draw dashed line connecting w axis to point on cost function\n                s = np.linspace(0,g_val)\n                o = np.ones((len(s)))\n                ax.plot(o*w_val,s,'k--',linewidth=1)\n\n            # plot all input/output pairs generated by algorithm thus far\n            if k > 0:\n                # plot all points up to this point\n                for j in range(min(k-1,len(w_hist))):  \n                    w_val = w_hist[j]\n                    g_val = g(w_val)\n                    ax.scatter(w_val,g_val,s = 90,c = colorspec[j],edgecolor = 'k',linewidth = 0.5*((1/(float(j) + 1)))**(0.4),zorder = 3,marker = 'X')            # plot point of tangency\n                    ax.scatter(w_val,0,s = 90,facecolor = colorspec[j],edgecolor = 'k',linewidth =  0.5*((1/(float(j) + 1)))**(0.4), zorder = 2)\n                    \n            # plot surrogate function and travel-to point\n            if k > 0 and k < len(w_hist) + 1:          \n                # grab historical weight, compute function and derivative evaluations\n                w = w_hist[k-1]\n                g_eval = g(w)\n                grad_eval = float(grad(w))\n            \n                # determine width to plot the approximation -- so its length == width defined above\n                div = float(1 + grad_eval**2)\n                w1 = w - math.sqrt(width/div)\n                w2 = w + math.sqrt(width/div)\n\n                # use point-slope form of line to plot\n                wrange = np.linspace(w1,w2, 100)\n                h = g_eval + grad_eval*(wrange - w)\n\n                # plot tangent line\n                ax.plot(wrange,h,color = colorspec[k-1],linewidth = 2,zorder = 1)      # plot approx\n\n                # plot tangent point\n                ax.scatter(w,g_eval,s = 100,c = 'm',edgecolor = 'k',linewidth = 0.7,zorder = 3,marker = 'X')            # plot point of tangency\n            \n                # plot next point learned from surrogate\n                if np.mod(t,2) == 0 and k < len(w_hist) -1:\n                    # create next point information\n                    w_zero = w_hist[k]\n                    g_zero = g(w_zero)\n                    h_zero = g_eval + grad_eval*(w_zero - w)\n\n                    # draw dashed line connecting the three\n                    vals = [0,h_zero,g_zero]\n                    vals = np.sort(vals)\n\n                    s = np.linspace(vals[0],vals[2])\n                    o = np.ones((len(s)))\n                    ax.plot(o*w_zero,s,'k--',linewidth=1)\n\n                    # draw intersection at zero and associated point on cost function you hop back too\n                    ax.scatter(w_zero,h_zero,s = 100,c = 'k', zorder = 3,marker = 'X')\n                    ax.scatter(w_zero,0,s = 100,c = 'm',edgecolor = 'k',linewidth = 0.7, zorder = 3)\n                    ax.scatter(w_zero,g_zero,s = 100,c = 'm',edgecolor = 'k',linewidth = 0.7,zorder = 3, marker = 'X')            # plot point of tangency\n                 \n            # fix viewing limits\n            ax.set_xlim([wmin-0.1,wmax+0.1])\n            ax.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            ax.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n            \n            # place title\n            ax.set_xlabel(r'$w$',fontsize = 14)\n            ax.set_ylabel(r'$g(w)$',fontsize = 14,rotation = 0,labelpad = 25)\n\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n\n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        \n        clear_output()           \n            \n    ###### animate the method ######\n    def newtons_method(self,g,w_hist,savepath,**kwargs):\n        # compute gradient and hessian of input\n        grad = compute_grad(g)              # gradient of input function\n        hess = compute_hess(g)           # hessian of input function\n        \n        # set viewing range\n        wmax = 3\n        if 'wmax' in kwargs:\n            wmax = kwargs['wmax']\n        wmin = -wmax\n        if 'wmin' in kwargs:\n            wmin = kwargs['wmin']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,4,1]) \n\n        ax1 = plt.subplot(gs[0]); ax1.axis('off')\n        ax3 = plt.subplot(gs[2]); ax3.axis('off')\n        ax = plt.subplot(gs[1]); \n        \n        # generate function for plotting on each slide\n        w_plot = np.linspace(wmin,wmax,1000)\n        g_plot = g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.1\n        w_vals = np.linspace(-2.5,2.5,50)\n        width = 1\n        \n        # make color spectrum for points\n        colorspec = self.make_colorspec(w_hist)\n        \n        # animation sub-function\n        print ('starting animation rendering...')\n        num_frames = 2*len(w_hist)+2\n        def animate(t):\n            ax.cla()\n            k = math.floor((t+1)/float(2))\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if t == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n\n            # plot function\n            ax.plot(w_plot,g_plot,color = 'k',zorder = 1)                           # plot function\n            \n            # plot initial point and evaluation\n            if k == 0:\n                w_val = w_hist[0]\n                g_val = g(w_val)\n                ax.scatter(w_val,g_val,s = 100,c = colorspec[k],edgecolor = 'k',linewidth = 0.7, marker = 'X',zorder = 2)            # plot point of tangency\n                ax.scatter(w_val,0,s = 100,c = colorspec[k],edgecolor = 'k',linewidth = 0.7, zorder = 2)\n                # draw dashed line connecting w axis to point on cost function\n                s = np.linspace(0,g_val)\n                o = np.ones((len(s)))\n                ax.plot(o*w_val,s,'k--',linewidth=1,zorder = 0)\n                \n            # plot all input/output pairs generated by algorithm thus far\n            if k > 0:\n                # plot all points up to this point\n                for j in range(min(k-1,len(w_hist))):  \n                    w_val = w_hist[j]\n                    g_val = g(w_val)\n                    ax.scatter(w_val,g_val,s = 90,c = colorspec[j],edgecolor = 'k',marker = 'X',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                    ax.scatter(w_val,0,s = 90,facecolor = colorspec[j],edgecolor = 'k',linewidth = 0.7, zorder = 2)\n                          \n            # plot surrogate function and travel-to point\n            if k > 0 and k < len(w_hist) + 1:          \n                # grab historical weight, compute function and derivative evaluations    \n                w_eval = w_hist[k-1]\n                if type(w_eval) != float:\n                    w_eval = float(w_eval)\n\n                # plug in value into func and derivative\n                g_eval = g(w_eval)\n                g_grad_eval = grad(w_eval)\n                g_hess_eval = hess(w_eval)\n\n                # determine width of plotting area for second order approximator\n                width = 0.5\n                if g_hess_eval < 0:\n                    width = - width\n\n                # setup quadratic formula params\n                a = 0.5*g_hess_eval\n                b = g_grad_eval - 2*0.5*g_hess_eval*w_eval\n                c = 0.5*g_hess_eval*w_eval**2 - g_grad_eval*w_eval - width\n\n                # solve for zero points\n                w1 = (-b + math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n                w2 = (-b - math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n\n                # compute second order approximation\n                wrange = np.linspace(w1,w2, 100)\n                h = g_eval + g_grad_eval*(wrange - w_eval) + 0.5*g_hess_eval*(wrange - w_eval)**2 \n\n                # plot tangent curve\n                ax.plot(wrange,h,color = colorspec[k-1],linewidth = 2,zorder = 2)      # plot approx\n\n                # plot tangent point\n                ax.scatter(w_eval,g_eval,s = 100,c = 'm',edgecolor = 'k', marker = 'X',linewidth = 0.7,zorder = 3)            # plot point of tangency\n            \n                # plot next point learned from surrogate\n                if np.mod(t,2) == 0:\n                    # create next point information\n                    w_zero = w_eval - g_grad_eval/(g_hess_eval + 10**-5)\n                    g_zero = g(w_zero)\n                    h_zero = g_eval + g_grad_eval*(w_zero - w_eval) + 0.5*g_hess_eval*(w_zero - w_eval)**2\n\n                    # draw dashed line connecting the three\n                    vals = [0,h_zero,g_zero]\n                    vals = np.sort(vals)\n\n                    s = np.linspace(vals[0],vals[2])\n                    o = np.ones((len(s)))\n                    ax.plot(o*w_zero,s,'k--',linewidth=1)\n\n                    # draw intersection at zero and associated point on cost function you hop back too\n                    ax.scatter(w_zero,h_zero,s = 100,c = 'b',linewidth=0.7, marker = 'X',edgecolor = 'k',zorder = 3)\n                    ax.scatter(w_zero,0,s = 100,c = 'm',edgecolor = 'k',linewidth = 0.7, zorder = 3)\n                    ax.scatter(w_zero,g_zero,s = 100,c = 'm',edgecolor = 'k',linewidth = 0.7, marker = 'X',zorder = 3)            # plot point of tangency\n            \n            # fix viewing limits on panel\n            ax.set_xlim([wmin,wmax])\n            ax.set_ylim([min(-0.3,min(g_plot) - ggap),max(max(g_plot) + ggap,0.3)])\n            \n            # add horizontal axis\n            ax.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n            \n            # label axes\n            ax.set_xlabel(r'$w$',fontsize = 14)\n            ax.set_ylabel(r'$g(w)$',fontsize = 14,rotation = 0,labelpad = 25)\n            \n            # set tickmarks\n            ax.set_xticks(np.arange(round(wmin), round(wmax) + 1, 1.0))\n            ax.set_yticks(np.arange(round(min(g_plot) - ggap), round(max(g_plot) + ggap) + 1, 1.0))\n\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n\n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()\n            \n    ### makes color spectrum for plotted run points - from green (start) to red (stop)\n    def make_colorspec(self,w_hist):\n        # make color range for path\n        s = np.linspace(0,1,len(w_hist[:round(len(w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(w_hist[round(len(w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        colorspec = []\n        colorspec = np.concatenate((s,np.flipud(s)),1)\n        colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n        return colorspec"""
mlrefined_libraries/math_optimization_library/convservative_steplength_demos.py,26,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport time\n\nclass visualizer:\n    '''\n    Illustrates how conservative steplength rules work in general.\n    ''' \n     \n    ######## gradient descent ########\n    # run gradient descent \n    def run_gradient_descent(self):\n        w = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w)\n        w_old = np.inf\n        j = 0\n        for j in range(int(self.max_its)):\n            # update old w and index\n            w_old = w\n            \n            # plug in value into func and derivative\n            grad_eval = self.grad(w)\n            \n            ### normalized or unnormalized? ###\n            if self.version == 'normalized':\n                grad_norm = np.linalg.norm(grad_eval)\n                if grad_norm == 0:\n                    grad_norm += 10**-6*np.sign(2*np.random.rand(1) - 1)\n                grad_eval /= grad_norm\n                \n            ### check what sort of steplength rule to employ ###\n            alpha = 0\n            if self.steplength == 'diminishing':\n                alpha = 1/(1 + j)\n            elif self.steplength == 'backtracking':\n                alpha = self.backtracking(w,grad_eval)\n            elif self.steplength == 'exact': \n                alpha = self.exact(w,grad_eval)\n            else:\n                alpha = float(self.steplength)            \n            \n            # take gradient descent step\n            w = w - alpha*grad_eval\n            \n            # record\n            self.w_hist.append(w)\n\n    # backtracking linesearch module\n    def backtracking(self,w,grad_eval):\n        # set input parameters\n        alpha = 1\n        t = 0.8\n        \n        # compute initial function and gradient values\n        func_eval = self.g(w)\n        grad_norm = np.linalg.norm(grad_eval)**2\n        \n        # loop over and tune steplength\n        while self.g(w - alpha*grad_eval) > func_eval - alpha*0.5*grad_norm:\n            alpha = t*alpha\n        return alpha\n\n    # exact linesearch module\n    def exact(self,w,grad_eval):\n        # set parameters of linesearch at each step\n        valmax = 10\n        num_evals = 3000\n        \n        # set alpha range\n        alpha_range = np.linspace(0,valmax,num_evals)\n        \n        # evaluate function over direction and alpha range, grab alpha giving lowest eval\n        steps = [(w - alpha*grad_eval) for alpha in alpha_range]\n        func_evals = np.array([self.g(s) for s in steps])\n        ind = np.argmin(func_evals)\n        best_alpha = alpha_range[ind]\n        \n        return best_alpha\n    \n    # visualize descent on multi-input function\n    def run(self,g,w_init,steplength_vals,max_its,**kwargs):\n        # count up steplength vals\n        step_count = len(steplength_vals)\n        \n        ### input arguments ###        \n        self.g = g\n        self.max_its = max_its\n        self.grad = compute_grad(self.g)              # gradient of input function\n        self.w_init = w_init\n        \n        pts = 'off'\n        if 'pts' in kwargs:\n            pts = 'off'\n            \n        linewidth = 2.5\n        if 'linewidth' in kwargs:\n            linewidth = kwargs['linewidth']\n            \n        view = [20,-50]\n        if 'view' in kwargs:\n            view = kwargs['view']\n\n        axes = False\n        if 'axes' in kwargs:\n            axes = kwargs['axes']\n\n        plot_final = False\n        if 'plot_final' in kwargs:\n            plot_final = kwargs['plot_final']\n\n        num_contours = 15\n        if 'num_contours' in kwargs:\n            num_contours = kwargs['num_contours']\n\n        # version of gradient descent to use (normalized or unnormalized)\n        self.version = 'unnormalized'\n        if 'version' in kwargs:\n            self.version = kwargs['version']\n            \n        # get initial point \n        if np.size(self.w_init) == 2:\n            self.w_init = np.asarray([float(s) for s in self.w_init])\n        else:\n            self.w_init = float(self.w_init)\n            \n        # take in user defined maximum number of iterations\n        self.max_its = max_its\n            \n        ##### construct figure with panels #####\n        # loop over steplengths, plot panels for each\n        count = 0\n        for step in steplength_vals:\n            # construct figure\n            fig, axs = plt.subplots(1, 2, figsize=(9,4))\n\n            # create subplot with 3 panels, plot input function in center plot\n            gs = gridspec.GridSpec(1, 2, width_ratios=[2,1]) \n            ax = plt.subplot(gs[0],aspect = 'equal'); \n            ax2 = plt.subplot(gs[1]) #  ,sharey = ax); \n\n            #### run local random search algorithm ####\n            self.w_hist = []\n            self.steplength = steplength_vals[count]\n            self.run_gradient_descent()\n            count+=1\n            \n            # colors for points\n            s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n            s.shape = (len(s),1)\n            t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n            t.shape = (len(t),1)\n            s = np.vstack((s,t))\n            colorspec = []\n            colorspec = np.concatenate((s,np.flipud(s)),1)\n            colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n    \n            #### define input space for function and evaluate ####\n            if np.size(self.w_init) == 2:           # function is multi-input, plot 3d function contour\n                # set viewing limits on contour plot\n                xvals = [self.w_hist[s][0] for s in range(len(self.w_hist))]\n                xvals.append(self.w_init[0])\n                yvals = [self.w_hist[s][1] for s in range(len(self.w_hist))]\n                yvals.append(self.w_init[1])\n                xmax = max(xvals)\n                xmin = min(xvals)\n                xgap = (xmax - xmin)*0.1\n                ymax = max(yvals)\n                ymin = min(yvals)\n                ygap = (ymax - ymin)*0.1\n                xmin -= xgap\n                xmax += xgap\n                ymin -= ygap\n                ymax += ygap\n\n                if 'xmin' in kwargs:\n                    xmin = kwargs['xmin']\n                if 'xmax' in kwargs:\n                    xmax = kwargs['xmax']\n                if 'ymin' in kwargs:\n                    ymin = kwargs['ymin']\n                if 'ymax' in kwargs:\n                    ymax = kwargs['ymax']  \n\n                w1 = np.linspace(xmin,xmax,400)\n                w2 = np.linspace(ymin,ymax,400)\n                w1_vals, w2_vals = np.meshgrid(w1,w2)\n                w1_vals.shape = (len(w1)**2,1)\n                w2_vals.shape = (len(w2)**2,1)\n                h = np.concatenate((w1_vals,w2_vals),axis=1)\n                func_vals = np.asarray([g(s) for s in h])\n                w1_vals.shape = (len(w1),len(w1))\n                w2_vals.shape = (len(w2),len(w2))\n                func_vals.shape = (len(w1),len(w2)) \n\n                ### make contour right plot - as well as horizontal and vertical axes ###\n                # set level ridges\n                num_contours = kwargs['num_contours']\n                levelmin = min(func_vals.flatten())\n                levelmax = max(func_vals.flatten())\n                cutoff = 0.5\n                cutoff = (levelmax - levelmin)*cutoff\n                numper = 3\n                levels1 = np.linspace(cutoff,levelmax,numper)\n                num_contours -= numper\n\n                levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n                levels = np.unique(np.append(levels1,levels2))\n                num_contours -= numper\n                while num_contours > 0:\n                    cutoff = levels[1]\n                    levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n                    levels = np.unique(np.append(levels2,levels))\n                    num_contours -= numper\n\n                a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = 'k')\n                ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = 'Blues')\n                \n                # plot points on contour\n                for j in range(len(self.w_hist)):  \n                    w_val = self.w_hist[j]\n                    g_val = self.g(w_val)\n\n                    # plot in left panel\n                    if pts == 'on':\n                        ax.scatter(w_val[0],w_val[1],s = 30,c = colorspec[j],edgecolor = 'k',linewidth = 1.5*math.sqrt((1/(float(j) + 1))),zorder = 3)\n\n                        ax2.scatter(j,g_val,s = 30,c = colorspec[j],edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n\n                    # plot connector between points for visualization purposes\n                    if j > 0:\n                        w_old = self.w_hist[j-1]\n                        w_new = self.w_hist[j]\n                        g_old = self.g(w_old)\n                        g_new = self.g(w_new)\n     \n                        ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = colorspec[j],linewidth = linewidth,alpha = 1,zorder = 2)      # plot approx\n                        ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = 'k',linewidth = linewidth + 0.4,alpha = 1,zorder = 1)      # plot approx\n                        ax2.plot([j-1,j],[g_old,g_new],color = colorspec[j],linewidth = 2,alpha = 1,zorder = 2)      # plot approx\n                        ax2.plot([j-1,j],[g_old,g_new],color = 'k',linewidth = 2.5,alpha = 1,zorder = 1)      # plot approx\n            \n                # clean up panel\n                ax.set_xlabel('$w_1$',fontsize = 12)\n                ax.set_ylabel('$w_2$',fontsize = 12,rotation = 0)\n                ax.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n                ax.axvline(x=0, color='k',zorder = 0,linewidth = 0.5)\n                ax.set_xlim([xmin,xmax])\n                ax.set_ylim([ymin,ymax])\n                \n                \n            else:    # function is single input, plot curve\n                if 'xmin' in kwargs:\n                    xmin = kwargs['xmin']\n                if 'xmax' in kwargs:\n                    xmax = kwargs['xmax']\n                    \n                w_plot = np.linspace(xmin,xmax,500)\n                g_plot = self.g(w_plot)\n                ax.plot(w_plot,g_plot,color = 'k',linewidth = 2,zorder = 2)\n                \n                # set viewing limits\n                ymin = min(g_plot)\n                ymax = max(g_plot)\n                ygap = (ymax - ymin)*0.2\n                ymin -= ygap\n                ymax += ygap\n                ax.set_ylim([ymin,ymax])\n                \n                # clean up panel\n                ax.axhline(y=0, color='k',zorder = 1,linewidth = 0.25)\n                ax.axvline(x=0, color='k',zorder = 1,linewidth = 0.25)\n                ax.set_xlabel(r'$w$',fontsize = 13)\n                ax.set_ylabel(r'$g(w)$',fontsize = 13,rotation = 0,labelpad = 25)   \n                \n                # function single-input, plot input and evaluation points on function\n                for j in range(len(self.w_hist)):  \n                    w_val = self.w_hist[j]\n                    g_val = self.g(w_val)\n            \n                    ax.scatter(w_val,g_val,s = 90,c = colorspec[j],edgecolor = 'k',linewidth = 0.5*((1/(float(j) + 1)))**(0.4),zorder = 3,marker = 'X')            # evaluation on function\n                    ax.scatter(w_val,0,s = 90,facecolor = colorspec[j],edgecolor = 'k',linewidth = 0.5*((1/(float(j) + 1)))**(0.4), zorder = 3)\n                    \n                    ax2.scatter(j,g_val,s = 30,c = colorspec[j],edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                    \n                    # plot connector between points for visualization purposes\n                    if j > 0:\n                        w_old = self.w_hist[j-1]\n                        w_new = self.w_hist[j]\n                        g_old = self.g(w_old)\n                        g_new = self.g(w_new)\n     \n                        ax2.plot([j-1,j],[g_old,g_new],color = colorspec[j],linewidth = 2,alpha = 1,zorder = 2)      # plot approx\n                        ax2.plot([j-1,j],[g_old,g_new],color = 'k',linewidth = 2.5,alpha = 1,zorder = 1)      # plot approx\n            \n            if axes == True:\n                ax.axhline(linestyle = '--', color = 'k',linewidth = 1)\n                ax.axvline(linestyle = '--', color = 'k',linewidth = 1)\n\n            # clean panels\n            title = self.steplength\n            if type(self.steplength) == float or type(self.steplength) == int:\n                title = r'$\\alpha = $' + str(self.steplength)\n            ax.set_title(title,fontsize = 12)\n\n            ax2.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n            ax2.set_xlabel('iteration',fontsize = 12)\n            ax2.set_ylabel(r'$g(w)$',fontsize = 12,rotation = 0,labelpad = 25)\n            \n            ax.set(aspect = 'equal')\n            a = ax.get_position()\n            yr = ax.get_position().y1 - ax.get_position().y0\n            xr = ax.get_position().x1 - ax.get_position().x0\n            aspectratio=1.25*xr/yr# + min(xr,yr)\n            ratio_default=(ax2.get_xlim()[1]-ax2.get_xlim()[0])/(ax2.get_ylim()[1]-ax2.get_ylim()[0])\n            ax2.set_aspect(ratio_default*aspectratio)\n            \n            # plot\n            plt.show()\n   """
mlrefined_libraries/math_optimization_library/coord_descent_plotter.py,27,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nimport numpy as np\nimport math\nimport time\nimport copy\n\nclass Visualizer:\n    \'\'\'\n    Simple plotter for 3d function - from surface and contour perspective\n    \'\'\'             \n\n    # my cost history plotter\n    def plot_cost(self,g,w_history):\n        # make a figure\n        fig,ax= plt.subplots(1,1,figsize = (8,3))\n\n        # compute cost vales\n        cost_vals = [g(w) for w in w_history]\n\n        # plot the cost values\n        ax.plot(cost_vals)\n\n        # cleanup graph and label axes\n        ax.set_xlabel(\'num of (outer loop) iterations\')\n        ax.set_ylabel(\'cost function value\')\n        ax.set_title(\'boosting descent\')\n        \n    # my cost history plotter\n    def plot_coord_descent_cost_history(self,g,w_history):\n        # make a figure\n        fig,ax= plt.subplots(1,1,figsize = (8,3))\n\n        # compute cost vales\n        cost_vals = [g(w) for w in w_history]\n\n        # plot the cost values\n        ax.plot(cost_vals)\n\n        # cleanup graph and label axes\n        ax.set_xlabel(\'num of (outer loop) iterations\')\n        ax.set_ylabel(\'cost function value\')\n\n        tickrange =  np.arange(0,len(w_history),len(w_history[-1]))\n        tickrange2 = [int(v/float(len(w_history[-1]))) for v in tickrange]\n        if len(tickrange2) > 10:\n            f = lambda m, n: [i*n//m + n//(2*m) for i in range(m)]\n            tickrange = f(10,len(w_history))\n            tickrange2 = [int(v/float(len(w_history[-1]))) for v in tickrange]\n        ax.set_xticks(tickrange) # choose which x locations to have ticks\n        ax.set_xticklabels(tickrange2) # set the labels to display at those ticks\n    \n    # show contour plot of input function\n    def draw_setup(self,g,**kwargs):\n        self.g = g                         # input function        \n        wmin = -3.1\n        wmax = 3.1\n        view = [50,50]\n        num_contours = 10\n        if \'wmin\' in kwargs:            \n            wmin = kwargs[\'wmin\']\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\']\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']   \n            \n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (9,3))\n\n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,2]) \n        ax = plt.subplot(gs[0],projection=\'3d\'); \n        ax2 = plt.subplot(gs[1],aspect=\'equal\'); \n\n        #### define input space for function and evaluate ####\n        w = np.linspace(-wmax,wmax,200)\n        w1_vals, w2_vals = np.meshgrid(w,w)\n        w1_vals.shape = (len(w)**2,1)\n        w2_vals.shape = (len(w)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([g(s) for s in h])\n        w1_vals.shape = (len(w),len(w))\n        w2_vals.shape = (len(w),len(w))\n        func_vals.shape = (len(w),len(w))\n\n        ### plot function as surface ### \n        ax.plot_surface(w1_vals, w2_vals, func_vals, alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)\n\n        # plot z=0 plane \n        ax.plot_surface(w1_vals, w2_vals, func_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n        \n        ### plot function as contours ###\n        # set level ridges\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cutoff = 0.5\n        cutoff = (levelmax - levelmin)*cutoff\n        numper = 3\n        levels1 = np.linspace(cutoff,levelmax,numper)\n        num_contours -= numper\n\n        levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n        levels = np.unique(np.append(levels1,levels2))\n        num_contours -= numper\n        while num_contours > 0:\n            cutoff = levels[1]\n            levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n            levels = np.unique(np.append(levels2,levels))\n            num_contours -= numper\n\n        ax2.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = \'k\')\n        ax2.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = \'Blues\')\n        \n        ### cleanup panels ###\n        ax.set_xlabel(\'$w_1$\',fontsize = 12)\n        ax.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n        ax.set_title(\'$g(w_1,w_2)$\',fontsize = 12)\n        ax.view_init(view[0],view[1])\n\n        ax2.set_xlabel(\'$w_1$\',fontsize = 12)\n        ax2.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n        ax2.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax2.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax2.set_xticks(np.arange(-round(wmax),round(wmax)+1))\n        ax2.set_yticks(np.arange(-round(wmax),round(wmax)+1))\n\n        # clean up axis\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        # plot\n        plt.show()\n        \n    # function to plot contour of input function g and path on contours (only works for N = 2 dim input functions)\n    def show_path(self,g,w_history,**kwargs):\n        self.g = g\n        self.w_hist = w_history\n        \n        ### user args\n        # number of contours to show in contour plot\n        self.num_contours = 15\n        if \'num_contours\' in kwargs:\n            self.num_contours = kwargs[\'num_contours\']\n       \n        ### setup figure and plot\n        fig, axs = plt.subplots(1, 3, figsize=(9,4))\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,3,1]) \n        ax = plt.subplot(gs[1],aspect = \'equal\'); \n        ax2 = plt.subplot(gs[0]); ax2.axis(\'off\');\n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\');\n        \n        ### draw contours\n        self.draw_contour_plot(ax,fig)\n        \n        ### draw path on contours\n        self.draw_weight_path(ax)\n            \n        ### cleanup panel\n        ax.set_xlabel(\'$w_1$\',fontsize = 12)\n        ax.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n        \n    ### function for drawing weight history path\n    def draw_weight_path(self,ax):\n        # make color range for path\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        colorspec = []\n        colorspec = np.concatenate((s,np.flipud(s)),1)\n        colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n\n        ### plot function decrease plot in right panel\n        for j in range(len(self.w_hist)):  \n            w_val = self.w_hist[j]\n            g_val = self.g(w_val)\n\n            # plot each weight set as a point\n            ax.scatter(w_val[0],w_val[1],s = 30,c = colorspec[j],edgecolor = \'k\',linewidth = 2*math.sqrt((1/(float(j) + 1))),zorder = 3)\n\n            # plot connector between points for visualization purposes\n            if j > 0:\n                w_old = self.w_hist[j-1]\n                w_new = self.w_hist[j]\n                g_old = self.g(w_old)\n                g_new = self.g(w_new)\n         \n                ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = colorspec[j],linewidth = 2,alpha = 1,zorder = 2)      # plot approx\n                ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = \'k\',linewidth = 2 + 0.4,alpha = 1,zorder = 1)      # plot approx\n             \n    ### function for creating contour plot\n    def draw_contour_plot(self,ax,fig):\n        # set viewing limits on contour plot\n        xvals = [self.w_hist[s][0] for s in range(len(self.w_hist))]\n        yvals = [self.w_hist[s][1] for s in range(len(self.w_hist))]\n        xmax = copy.deepcopy(max(xvals))\n        xmin = copy.deepcopy(min(xvals))\n        xgap = (xmax - xmin)*0.5\n        ymax = copy.deepcopy(max(yvals))\n        ymin = copy.deepcopy(min(yvals))\n        ygap = (ymax - ymin)*0.5\n        xmin -= xgap\n        xmax += xgap\n        ymin -= ygap\n        ymax += ygap\n            \n        #### define input space for function and evaluate ####\n        w1 = np.linspace(xmin,xmax,400)\n        w2 = np.linspace(ymin,ymax,400)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([ self.g(np.reshape(s,(2,1))) for s in h])\n\n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cutoff = 0.5\n        cutoff = (levelmax - levelmin)*cutoff\n        numper = 3\n        levels1 = np.linspace(cutoff,levelmax,numper)\n        self.num_contours -= numper\n\n        levels2 = np.linspace(levelmin,cutoff,min(self.num_contours,numper))\n        levels = np.unique(np.append(levels1,levels2))\n        self.num_contours -= numper\n        while self.num_contours > 0:\n            cutoff = levels[1]\n            levels2 = np.linspace(levelmin,cutoff,min(self.num_contours,numper))\n            levels = np.unique(np.append(levels2,levels))\n            self.num_contours -= numper\n\n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = \'k\')\n        b = ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = \'Blues\')'"
mlrefined_libraries/math_optimization_library/coordinate_descent.py,22,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport time\nimport copy\nfrom matplotlib.collections import LineCollection\nfrom matplotlib.colors import ListedColormap, BoundaryNorm\n\nclass visualizer:\n    '''\n    Illustrates how brute force coordinate descent w/linesearch works\n    ''' \n\n    # run coordinate descent - in this case for the demo we use coordinate gradient descent with exact line search\n    def run_coordinate_descent(self):\n        w = copy.deepcopy(self.w_init)\n        self.w_hist = []\n        self.w_hist.append(copy.deepcopy(w))\n        j = 0\n        for j in range(int(self.max_its)):            \n            # plug in value into func and derivative\n            grad_eval = self.grad(w)\n                    \n            # loop over coordinates\n            for k in range(len(w)):                \n                # strip gradient of k^th coordinate\n                coord_grad = copy.deepcopy(grad_eval[k])\n                                                     \n                # normalize direction or no?\n                if self.version == 'normalized':\n                    # normaize direction, if everything is perfectly zero then move in random direction\n                    grad_norm = np.linalg.norm(coord_grad)\n                    if grad_norm == 0:\n                        coord_grad = np.sign(2*np.random.rand(1) - 1)[0]\n                    else:\n                        coord_grad = np.sign(coord_grad)\n                 \n                ### check what sort of steplength rule to employ ###\n                alpha = 0\n                grad_temp = copy.deepcopy(grad_eval)\n                grad_temp[k] = coord_grad\n                alpha = self.exact(w,grad_temp)\n                \n                # take coordinate descent step - update single weight\n                w[k] -= alpha*coord_grad\n\n                # record each coordinate descent step for visualization\n                self.w_hist.append(copy.deepcopy(w))            \n                \n    # exact linesearch module for brute coordinate descent\n    def exact(self,w,grad_eval):\n        # set parameters of linesearch at each step\n        valmax = 10\n        num_evals = 3000\n        \n        # set alpha range\n        alpha_range = np.linspace(0,valmax,num_evals)\n        \n        # evaluate function over direction and alpha range, grab alpha giving lowest eval\n        steps = [(w - alpha*grad_eval) for alpha in alpha_range]\n        func_evals = np.array([self.g(s) for s in steps])\n        ind = np.argmin(func_evals)\n        best_alpha = alpha_range[ind]\n        \n        return best_alpha\n    \n    # visualize descent on multi-input function\n    def run(self,g,w_init,max_its,**kwargs):        \n        ### input arguments ###        \n        self.g = g\n        self.max_its = max_its\n        self.grad = compute_grad(self.g)              # gradient of input function\n\n        pts = 'off'\n        if 'pts' in kwargs:\n            pts = 'off'\n            \n        linewidth = 2.5\n        if 'linewidth' in kwargs:\n            linewidth = kwargs['linewidth']\n            \n        view = [20,-50]\n        if 'view' in kwargs:\n            view = kwargs['view']\n\n        axes = False\n        if 'axes' in kwargs:\n            axes = kwargs['axes']\n\n        plot_final = False\n        if 'plot_final' in kwargs:\n            plot_final = kwargs['plot_final']\n\n        num_contours = 15\n        if 'num_contours' in kwargs:\n            num_contours = kwargs['num_contours']\n\n        # version of gradient descent to use (normalized or unnormalized)\n        self.version = 'unnormalized'\n        if 'version' in kwargs:\n            self.version = kwargs['version']\n            \n        # get initial point \n        self.w_init = np.asarray([float(s) for s in w_init])\n            \n        # take in user defined maximum number of iterations\n        self.max_its = max_its\n\n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(9,4))\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[2,1]) \n        ax = plt.subplot(gs[0],aspect = 'equal'); \n        ax2 = plt.subplot(gs[1]) #  ,sharey = ax); \n\n        #### run local random search algorithm ####\n        self.w_hist = []\n        self.steplength = 'exact'\n        self.run_coordinate_descent()\n        \n        # set viewing limits on contour plot\n        xvals = [self.w_hist[s][0] for s in range(len(self.w_hist))]\n        xvals.append(self.w_init[0])\n        yvals = [self.w_hist[s][1] for s in range(len(self.w_hist))]\n        yvals.append(self.w_init[1])\n        xmax = max(xvals)\n        xmin = min(xvals)\n        xgap = (xmax - xmin)*0.1\n        ymax = max(yvals)\n        ymin = min(yvals)\n        ygap = (ymax - ymin)*0.1\n        xmin -= xgap\n        xmax += xgap\n        ymin -= ygap\n        ymax += ygap\n            \n        if 'xmin' in kwargs:\n            xmin = kwargs['xmin']\n        if 'xmax' in kwargs:\n            xmax = kwargs['xmax']\n        if 'ymin' in kwargs:\n            ymin = kwargs['ymin']\n        if 'ymax' in kwargs:\n            ymax = kwargs['ymax']  \n            \n        #### define input space for function and evaluate ####\n        w1 = np.linspace(xmin,xmax,400)\n        w2 = np.linspace(ymin,ymax,400)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([g(s) for s in h])\n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        num_contours = kwargs['num_contours']\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cutoff = 0.5\n        cutoff = (levelmax - levelmin)*cutoff\n        numper = 3\n        levels1 = np.linspace(cutoff,levelmax,numper)\n        num_contours -= numper\n\n        levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n        levels = np.unique(np.append(levels1,levels2))\n        num_contours -= numper\n        while num_contours > 0:\n            cutoff = levels[1]\n            levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n            levels = np.unique(np.append(levels2,levels))\n            num_contours -= numper\n\n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = 'k')\n        ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = 'Blues')\n\n        # label contour lines?\n        #ax.clabel(a, inline=1, fontsize=10)\n\n        if axes == True:\n            ax.axhline(linestyle = '--', color = 'k',linewidth = 1)\n            ax.axvline(linestyle = '--', color = 'k',linewidth = 1)\n\n        # colors for points\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        colorspec = []\n        colorspec = np.concatenate((s,np.flipud(s)),1)\n        colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n\n        ### plot function decrease plot in right panel\n        for j in range(len(self.w_hist)):  \n            w_val = self.w_hist[j]\n            g_val = self.g(w_val)\n\n            # plot in left panel\n            if pts == 'True':\n                ax.scatter(w_val[0],w_val[1],s = 30,c = colorspec[j],edgecolor = 'k',linewidth = 1.5*math.sqrt((1/(float(j) + 1))),zorder = 3)\n\n                ax2.scatter(j,g_val,s = 30,c = colorspec[j],edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n\n            # plot connector between points for visualization purposes\n            if j > 0:\n                w_old = self.w_hist[j-1]\n                w_new = self.w_hist[j]\n                g_old = self.g(w_old)\n                g_new = self.g(w_new)\n         \n                ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = colorspec[j],linewidth = linewidth,alpha = 1,zorder = 2)      # plot approx\n                ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = 'k',linewidth = linewidth + 0.4,alpha = 1,zorder = 1)      # plot approx\n                ax2.plot([j-1,j],[g_old,g_new],color = colorspec[j],linewidth = 2,alpha = 1,zorder = 2)      # plot approx\n                ax2.plot([j-1,j],[g_old,g_new],color = 'k',linewidth = 2.5,alpha = 1,zorder = 1)      # plot approx\n               \n        # clean panels\n        title = self.steplength\n        if type(self.steplength) == float:\n            title = r'$\\alpha = $' + str(self.steplength)\n        #ax.set_title(title,fontsize = 12)\n        ax.set_xlabel('$w_1$',fontsize = 12)\n        ax.set_ylabel('$w_2$',fontsize = 12,rotation = 0)\n        ax.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color='k',zorder = 0,linewidth = 0.5)\n        ax2.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n        ax2.set_xlabel('iteration',fontsize = 12)\n        ax2.set_ylabel(r'$g(w)$',fontsize = 12,rotation = 0,labelpad = 25)\n               \n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n            \n        ax.set(aspect = 'equal')\n        a = ax.get_position()\n        yr = ax.get_position().y1 - ax.get_position().y0\n        xr = ax.get_position().x1 - ax.get_position().x0\n        aspectratio=1.25*xr/yr# + min(xr,yr)\n        ratio_default=(ax2.get_xlim()[1]-ax2.get_xlim()[0])/(ax2.get_ylim()[1]-ax2.get_ylim()[0])\n        ax2.set_aspect(ratio_default*aspectratio)\n            \n        # plot\n        plt.show()\n   """
mlrefined_libraries/math_optimization_library/coordinate_gradient_descent.py,26,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\nimport copy\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport time\n\nclass visualizer:\n    '''\n    Compares steepest descent using L1, L2, and Linfinity norms.\n    ''' \n     \n    ######## gradient descent functions ########\n    # run gradient descent \n    def run_gradient_descent(self):\n        w = copy.deepcopy(self.w_init)\n        self.w_hist = []\n        self.w_hist.append(w)\n        j = 0\n        for j in range(int(self.max_its)):            \n            # plug in value into func and derivative\n            grad_eval = self.grad(w)\n           \n            if self.version == 'normalized':\n                # normaize direction, if everything is perfectly zero then move in random direction\n                grad_norm = np.linalg.norm(grad_eval)\n                if grad_norm == 0:\n                    grad_eval = 2*np.random.rand(len(w)) - 1\n                    grad_norm = np.linalg.norm(grad_eval)\n                    grad_eval /= grad_norm\n                else:\n                    grad_eval /= grad_norm\n               \n            ### check what sort of steplength rule to employ ###\n            alpha = 0\n            if self.steplength == 'diminishing':\n                alpha = 1/(1 + j)\n            elif self.steplength == 'backtracking':\n                alpha = self.backtracking(w,grad_eval)\n            elif self.steplength == 'exact': \n                alpha = self.exact(w,grad_eval)\n            else:\n                alpha = float(self.steplength)            \n            \n            # take gradient descent step\n            w = w - alpha*grad_eval\n            \n            # record\n            self.w_hist.append(w)\n    \n    # run coordinate descent \n    def run_coordinate_descent(self):\n        w = copy.deepcopy(self.w_init)\n        self.w_hist = []\n        self.w_hist.append(copy.deepcopy(w))\n        j = 0\n        for j in range(int(self.max_its)):            \n            # plug in value into func and derivative\n            grad_eval = self.grad(w)\n                    \n            # loop over coordinates\n            for k in range(len(w)):                \n                # strip gradient of k^th coordinate\n                coord_grad = copy.deepcopy(grad_eval[k])\n                                                     \n                # normalize direction or no?\n                if self.version == 'normalized':\n                    # normaize direction, if everything is perfectly zero then move in random direction\n                    grad_norm = np.linalg.norm(coord_grad)\n                    if grad_norm == 0:\n                        coord_grad = np.sign(2*np.random.rand(1) - 1)[0]\n                    else:\n                        coord_grad = np.sign(coord_grad)\n                 \n                ### check what sort of steplength rule to employ ###\n                alpha = 0\n                grad_temp = copy.deepcopy(grad_eval)\n                grad_temp[k] = coord_grad\n                if self.steplength == 'diminishing':\n                    alpha = 1/(1 + j)\n                elif self.steplength == 'backtracking':\n                    alpha = self.backtracking(w,grad_temp)\n                elif self.steplength == 'exact': \n                    alpha = self.exact(w,grad_temp)\n                else:\n                    alpha = float(self.steplength) \n                \n                # take coordinate descent step - update single weight\n                w[k] -= alpha*coord_grad\n\n                # record each coordinate descent step for visualization\n                self.w_hist.append(copy.deepcopy(w))\n\n                \n    # backtracking linesearch module\n    def backtracking(self,w,grad_eval):\n        # set input parameters\n        alpha = 1\n        t = 0.5\n        \n        # compute initial function and gradient values\n        func_eval = self.g(w)\n        grad_norm = np.linalg.norm(grad_eval)**2\n        \n        # loop over and tune steplength\n        while self.g(w - alpha*grad_eval) > func_eval - alpha*0.5*grad_norm:\n            alpha = t*alpha\n        return alpha\n\n    # exact linesearch module\n    def exact(self,w,grad_eval):\n        # set parameters of linesearch at each step\n        valmax = 10\n        num_evals = 3000\n        \n        # set alpha range\n        alpha_range = np.linspace(0,valmax,num_evals)\n        \n        # evaluate function over direction and alpha range, grab alpha giving lowest eval\n        steps = [(w - alpha*grad_eval) for alpha in alpha_range]\n        func_evals = np.array([self.g(s) for s in steps])\n        ind = np.argmin(func_evals)\n        best_alpha = alpha_range[ind]\n        \n        return best_alpha\n    \n    # visualize descent on multi-input function\n    def run(self,g,w_init,steplength,max_its,**kwargs):\n        ### input arguments ###        \n        self.g = g\n        self.max_its = max_its\n        self.grad = compute_grad(self.g)              # gradient of input function\n            \n        plot_final = False\n        if 'plot_final' in kwargs:\n            plot_final = kwargs['plot_final']\n\n        num_contours = 15\n        if 'num_contours' in kwargs:\n            num_contours = kwargs['num_contours']\n\n        # version of gradient descent to use (normalized or unnormalized)\n        self.version = 'unnormalized'\n        if 'version' in kwargs:\n            self.version = kwargs['version']\n        \n        # steplength\n        self.steplength = steplength\n            \n        # get initial point \n        self.w_init = np.asarray([float(s) for s in w_init])\n            \n        # take in user defined maximum number of iterations\n        self.max_its = max_its\n           \n        # loop over steplengths, plot panels for each\n        count = 0\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(9,4))\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0],aspect = 'equal');\n        ax2 = plt.subplot(gs[1],aspect = 'equal'); \n\n        #### run local random search algorithm ####\n       \n        # choose version\n        self.version = 'normalized'\n        if 'normalized' in kwargs:\n            self.version = kwargs['normalized']\n         \n        # run gradient descent\n        self.run_gradient_descent()\n        title = 'gradient descent'\n        self.draw_panel(ax1,title,**kwargs)\n        \n        # run coordinate descent\n        self.run_coordinate_descent()\n        title = 'coordinate descent'\n        self.draw_panel(ax2,title,**kwargs)\n\n        # plot\n        plt.show()\n     \n    # draw panel \n    def draw_panel(self,ax,title,**kwargs):\n        # set viewing limits on contour plot\n        xvals = [self.w_hist[s][0] for s in range(len(self.w_hist))]\n        xvals.append(self.w_init[0])\n        yvals = [self.w_hist[s][1] for s in range(len(self.w_hist))]\n        yvals.append(self.w_init[1])\n        xmax = max(xvals)\n        xmin = min(xvals)\n        xgap = (xmax - xmin)*0.1\n        ymax = max(yvals)\n        ymin = min(yvals)\n        ygap = (ymax - ymin)*0.1\n        xmin -= xgap\n        xmax += xgap\n        ymin -= ygap\n        ymax += ygap\n\n        if 'xmin' in kwargs:\n            xmin = kwargs['xmin']\n        if 'xmax' in kwargs:\n            xmax = kwargs['xmax']\n        if 'ymin' in kwargs:\n            ymin = kwargs['ymin']\n        if 'ymax' in kwargs:\n            ymax = kwargs['ymax'] \n        axes = False\n        if 'axes' in kwargs:\n            axes = kwargs['ymax']\n        pts = False\n        if 'pts' in kwargs:\n            pts = kwargs['pts']  \n        \n        pts = False\n        if 'pts' in kwargs:\n            pts = kwargs['pts']  \n            \n        linewidth = 2.5\n        if 'linewidth' in kwargs:\n            linewidth = kwargs['linewidth']\n            \n        #### define input space for function and evaluate ####\n        w1 = np.linspace(xmin,xmax,400)\n        w2 = np.linspace(ymin,ymax,400)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([self.g(s) for s in h])\n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        num_contours = kwargs['num_contours']\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cutoff = 0.5\n        cutoff = (levelmax - levelmin)*cutoff\n        numper = 3\n        levels1 = np.linspace(cutoff,levelmax,numper)\n        num_contours -= numper\n\n        levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n        levels = np.unique(np.append(levels1,levels2))\n        num_contours -= numper\n        while num_contours > 0:\n            cutoff = levels[1]\n            levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n            levels = np.unique(np.append(levels2,levels))\n            num_contours -= numper\n\n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = 'k')\n        ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = 'Blues')\n\n        if axes == True:\n            ax.axhline(linestyle = '--', color = 'k',linewidth = 1)\n            ax.axvline(linestyle = '--', color = 'k',linewidth = 1)\n\n        # colors for points\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        colorspec = []\n        colorspec = np.concatenate((s,np.flipud(s)),1)\n        colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n\n        ### plot function decrease plot in right panel\n        for j in range(len(self.w_hist)):  \n            w_val = self.w_hist[j]\n            g_val = self.g(w_val)\n\n            # plot in left panel\n            if pts == 'True':\n                ax.scatter(w_val[0],w_val[1],s = 30,c = colorspec[j],edgecolor = 'k',linewidth = 1.5*math.sqrt((1/(float(j) + 1))),zorder = 3)\n\n            # plot connector between points for visualization purposes\n            if j > 0:\n                w_old = self.w_hist[j-1]\n                w_new = self.w_hist[j]     \n                \n                ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = colorspec[j],linewidth = linewidth,alpha = 1,zorder = 2)      # plot approx\n                ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = 'k',linewidth = linewidth + 0.4,alpha = 1,zorder = 1)      # plot approx\n    \n        # clean panel\n        ax.set_title(title,fontsize = 12)\n        ax.set_xlabel('$w_1$',fontsize = 12)\n        ax.set_ylabel('$w_2$',fontsize = 12,rotation = 0)\n        ax.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color='k',zorder = 0,linewidth = 0.5)               \n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])"""
mlrefined_libraries/math_optimization_library/exponential_average_animator.py,6,"b""# import standard plotting and animation\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom matplotlib.ticker import FormatStrFormatter\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import standard libraries\nimport math\nimport time\nimport copy\nfrom inspect import signature\n\nclass Visualizer:\n    '''\n    animators for time series\n    '''\n\n    #### animate exponential average ####\n    def animate_exponential_ave(self,x,y,savepath,**kwargs):\n        # produce figure\n        fig = plt.figure(figsize = (9,4))\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,7,1]) \n        ax = plt.subplot(gs[0]); ax.axis('off')\n        ax1 = plt.subplot(gs[1]); \n        ax2 = plt.subplot(gs[2]); ax2.axis('off')\n        artist = fig\n        \n        # view limits\n        xmin = -3\n        xmax = len(x) + 3\n        ymin = np.min(x)\n        ymax = np.max(x) \n        ygap = (ymax - ymin)*0.15\n        ymin -= ygap\n        ymax += ygap\n            \n        # start animation\n        num_frames = len(y) \n        print ('starting animation rendering...')\n        def animate(k):\n            # clear panels\n            ax1.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n                \n            # plot x\n            ax1.plot(np.arange(1,x.size + 1),x,alpha = 1,c = 'k',linewidth = 2,zorder = 2);\n\n            # plot exponential average - initial conditions\n            if k == 1:\n                ax1.plot(np.arange(1,2), y[:1], alpha = 0.75, c = 'darkorange',linewidth = 4,zorder = 3);\n                \n            # plot moving average - everything after and including initial conditions\n            if k > 1:\n                # plot \n                ax1.plot(np.arange(1,k+1),y[:k],alpha = 0.7,c = 'darkorange',linewidth = 4,zorder = 3);\n                \n            # label axes\n            ax1.set_xlim([xmin,xmax])\n            ax1.set_ylim([ymin,ymax])\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()"""
mlrefined_libraries/math_optimization_library/function_addition_3d.py,4,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nimport time\n\nclass visualizer:\n    \'\'\'\n    This file illlustrates the sum of two functions in 3d.  Both functions are defined by the user.\n    \'\'\' \n\n    # animate the method\n    def draw_it(self,h1,h2,savepath,**kwargs):\n        # user input functions to add\n        self.h1 = h1                            # input function\n        self.h2 = h2                            # input function\n        num_frames = 100\n        if \'num_frames\' in kwargs:\n            num_frames = kwargs[\'num_frames\']\n            \n        # turn axis on or off\n        set_axis = \'on\'\n        if \'set_axis\' in kwargs:\n            set_axis = kwargs[\'set_axis\']\n  \n        # set viewing angle on plot\n        view = [20,50]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n            \n        epsmax = 2\n        if \'epsmax\' in kwargs:\n            epsmax = kwargs[\'epsmax\']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (15,5))\n        artist = fig\n        \n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,1, 1]) \n        ax1 = plt.subplot(gs[0],projection=\'3d\');\n        ax2 = plt.subplot(gs[1],projection=\'3d\');\n        ax3 = plt.subplot(gs[2],projection=\'3d\');\n        \n        # generate input range for functions\n        r = np.linspace(-3,3,100)\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        h1_vals = self.h1([w1_vals,w2_vals])\n        h2_vals = self.h2([w1_vals,w2_vals])\n        \n        # vals for cost surface\n        w1_vals.shape = (len(r),len(r))\n        w2_vals.shape = (len(r),len(r))\n        h1_vals.shape = (len(r),len(r))\n        h2_vals.shape = (len(r),len(r))\n\n        # decide on number of slides\n        epsilon_vals = np.linspace(0,epsmax,num_frames)\n\n        # animation sub-function\n        print (\'starting animation rendering...\')\n        def animate(t):\n            # clear panels for next slide\n            ax1.cla()\n            ax2.cla()\n            ax3.cla()\n            \n            # print rendering update\n            if np.mod(t+1,25) == 0:\n                print (\'rendering animation frame \' + str(t+1) + \' of \' + str(num_frames))\n            if t == num_frames - 1:\n                print (\'animation rendering complete!\')\n                time.sleep(1.5)\n                clear_output()\n              \n            # plot function 1\n            ax1.plot_surface(w1_vals,w2_vals,h1_vals,alpha = 0.3,color = \'w\',rstride=10, cstride=10,linewidth=2,edgecolor = \'k\') \n            ax1.set_title(""$h_1$"",fontsize = 15)\n            ax1.view_init(view[0],view[1])\n            ax1.grid(False)\n\n\n            # plot function 2\n            ax2.plot_surface(w1_vals,w2_vals,h2_vals,alpha = 0.3,color = \'w\',rstride=10, cstride=10,linewidth=2,edgecolor = \'k\') \n            ax2.set_title(""$h_2$"",fontsize = 15)\n            ax2.view_init(view[0],view[1])\n            ax2.grid(False)\n                \n            # plot combination of both\n            epsilon = epsilon_vals[t]\n            h3_vals = h1_vals + epsilon*h2_vals\n            ax3.plot_surface(w1_vals,w2_vals,h3_vals,alpha = 0.3,color = \'w\',rstride=10, cstride=10,linewidth=2,edgecolor = \'k\')\n            ax3.grid(False)\n\n            title = r\'$h_1 + \' +  r\'{:.2f}\'.format(epsilon)  + \'h_2$\'\n            ax3.set_title(title,fontsize = 15)\n            ax3.view_init(view[0],view[1])\n            \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n\n        # produce animation and save\n        fps = 50\n        if \'fps\' in kwargs:\n            fps = kwargs[\'fps\']\n        anim.save(savepath, fps=fps, extra_args=[\'-vcodec\', \'libx264\'])\n        clear_output()'"
mlrefined_libraries/math_optimization_library/grad_descent_as_quadratic.py,17,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nimport time\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nfrom matplotlib import gridspec\n\n# simple first order taylor series visualizer\nclass visualizer:\n    '''\n    Illustrate gradient descent as a minimization technique using simple quadratic surrogates.  \n    Visual comparison with the standard linear surrogate view.\n    ''' \n    def __init__(self,**args):\n        self.g = args['g']                            # input function\n        self.grad = compute_grad(self.g)              # gradient of input function\n        self.hess = compute_grad(self.grad)           # hessian of input function\n        self.w_init =float( -2)                       # user-defined initial point (adjustable when calling each algorithm)\n        self.alpha = 10**-4                           # user-defined step length for gradient descent (adjustable when calling gradient descent)\n        self.max_its = 20                             # max iterations to run for each algorithm\n        self.colors = [[0,1,0.25],[0,0.75,1],[1,0.75,0],[1,0,0.75]]       # custom colors for plotting\n        \n    ######## gradient descent ########\n    # run gradient descent \n    def run_gradient_descent(self):\n        w = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w)\n        w_old = np.inf\n        j = 0\n        while (w_old - w)**2 > 10**-5 and j < self.max_its:\n            # update old w and index\n            w_old = w\n            j+=1\n            \n            # plug in value into func and derivative\n            grad_eval = float(self.grad(w))\n            \n            # take gradient descent step\n            w = w - self.alpha*grad_eval\n            \n            # record\n            self.w_hist.append(w)\n\n    # animate the gradient descent method\n    def animate_it(self,savepath,**kwargs):\n        # get new initial point if desired\n        if 'w_init' in kwargs:\n            self.w_init = float(kwargs['w_init'])\n            \n        # take in user defined step length\n        if 'alpha' in kwargs:\n            self.alpha = float(kwargs['alpha'])\n            \n        # take in user defined maximum number of iterations\n        if 'max_its' in kwargs:\n            self.max_its = float(kwargs['max_its'])\n            \n        # viewing range\n        wmax = 5\n        if 'wmax' in kwargs:\n            wmax = kwargs['wmax']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (10,5))\n        artist = fig\n        \n       # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5, 1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis('off');\n        ax3 = plt.subplot(gs[2]); ax3.axis('off');\n\n        # plot input function\n        ax = plt.subplot(gs[1],aspect = 'equal')\n        \n        # generate function for plotting on each slide\n        w_plot = np.linspace(-wmax,wmax,1000)\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.25\n        width = 30\n        \n        # run gradient descent method\n        self.w_hist = []\n        self.run_gradient_descent()\n        \n        # colors for points --> green as the algorithm begins, yellow as it converges, red at final point\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # animation sub-function\n        print ('starting animation rendering...')\n        num_frames = len(self.w_hist)\n        def animate(k):\n            ax.cla()\n            \n            # print rendering update            \n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            # plot function\n            ax.plot(w_plot,g_plot,color = 'k',zorder = 2)                           # plot function\n            \n            # plot initial point and evaluation\n            if k == 0:\n                w_val = self.w_init\n                g_val = self.g(w_val)\n                ax.scatter(w_val,g_val,s = 100,c = 'r',edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                ax.scatter(w_val,0,s = 100,c = 'r',edgecolor = 'k',linewidth = 0.7, zorder = 3, marker = 'X')\n                # draw dashed line connecting w axis to point on cost function\n                s = np.linspace(0,g_val)\n                o = np.ones((len(s)))\n                ax.plot(o*w_val,s,'k--',linewidth=1)\n\n            # plot all input/output pairs generated by algorithm thus far\n            if k > 0 and k < len(self.w_hist) + 1:\n                # plot all points up to this point\n                for j in range(min(k,len(self.w_hist))):  \n                    alpha_val = 1\n                    if j < k-1:\n                        alpha_val = 0.1\n                        \n                    # get next value of weight, function and gradient evaluation\n                    w_val = self.w_hist[j]\n                    g_val = self.g(w_val)\n                    grad_val = float(self.grad(w_val))\n                    \n                    # plot current point\n                    ax.scatter(w_val,g_val,s = 90,c = 'r',edgecolor = 'k',linewidth = 0.7,zorder = 3,alpha = alpha_val)            # plot point of tangency\n                    ax.scatter(w_val,0,s = 90,facecolor = 'r',marker = 'X',edgecolor = 'k',linewidth = 0.7, zorder = 3,alpha = alpha_val)\n                    \n                    #### plot linear surrogate ####\n                    # determine width to plot the approximation -- so its length == width defined above\n                    div = float(1 + grad_val**2)\n                    w1 = w_val - math.sqrt(width/div)\n                    w2 = w_val + math.sqrt(width/div)\n\n                    # use point-slope form of line to plot\n                    wrange = np.linspace(w1,w2, 100)\n                    h = g_val + grad_val*(wrange - w_val)\n\n                    # plot tangent line\n                    ax.plot(wrange,h,color = self.colors[0],linewidth = 2,zorder = 1,alpha = alpha_val)      # plot approx\n\n                    # plot tangent point\n                    ax.scatter(w_val,g_val,s = 100,c = 'r',edgecolor = 'k',linewidth = 0.7,zorder = 3,alpha = alpha_val)            # plot point of tangency\n\n                    # plot next point learned from surrogate\n                    # create next point information\n                    w_zero = w_val - self.alpha*grad_val\n                    g_zero = self.g(w_zero)\n                    h_zero = g_val + grad_val*(w_zero - w_val)\n\n                    # draw intersection at zero and associated point on cost function you hop back too\n                    ax.scatter(w_zero,h_zero,s = 100,c = self.colors[0],edgecolor = 'k',linewidth = 0.7, zorder = 3, marker = 'X',alpha = alpha_val)\n                    ax.scatter(w_zero,0,s = 100,c = 'r',edgecolor = 'k',linewidth = 0.7, zorder = 3, marker = 'X',alpha = alpha_val)\n                    ax.scatter(w_zero,g_zero,s = 100,c = 'r',edgecolor = 'k',linewidth = 0.7,zorder = 3,alpha = alpha_val)            # plot point of tangency\n                    \n                    ### draw simple quadratic surrogate ###\n                    # decide on range for quadratic so it looks nice\n                    quad_term = 1/float(2*self.alpha)\n                    a = 0.5*quad_term\n                    b = grad_val - 2*0.5*quad_term*w_val\n                    c = 0.5*quad_term*w_val**2 - grad_val*w_val - width\n\n                    # solve for zero points\n                    w1 = (-b + math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n                    w2 = (-b - math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n                    wrange = np.linspace(w1,w2, 100)\n\n                    # create simple quadratic surrogate\n                    h = g_val + grad_val*(wrange - w_val) + quad_term*(wrange - w_val)**2\n                    \n                    # plot simple quadratic surrogate\n                    ax.plot(wrange,h,color = self.colors[1],linewidth = 2,zorder = 1,alpha = alpha_val)      # plot approx\n\n                    # plot point of intersection - next gradient descent step - on simple quadratic surrogate\n                    h_zero_2 = g_val + grad_val*(w_zero - w_val) + 1/float(2*self.alpha)*(w_zero - w_val)**2\n\n                    ax.scatter(w_zero,h_zero_2,s = 100,c = self.colors[1],edgecolor = 'k',linewidth = 0.7, zorder = 3, marker = 'X',alpha = alpha_val)\n                    \n                    # draw dashed line connecting w axis to point on cost function\n                    s = np.linspace(0,g_val)\n                    o = np.ones((len(s)))\n                    ax.plot(o*w_val,s,'k--',linewidth=1,alpha = alpha_val)\n\n                    vals = [0,h_zero,h_zero_2,g_zero]\n                    vals = np.sort(vals)\n\n                    s = np.linspace(vals[0],vals[3])\n                    o = np.ones((len(s)))\n                    w_val = self.w_hist[j+1]\n                    ax.plot(o*w_val,s,'k--',linewidth=1,alpha = alpha_val)\n                 \n            # fix viewing limits\n            ax.set_xlim([-wmax,wmax])\n            ax.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            ax.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n            ax.set_xlabel(r'$w$',fontsize=12)\n            ax.set_ylabel(r'$g(w)$',fontsize=12,rotation = 0,labelpad = 20)\n            \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()"""
mlrefined_libraries/math_optimization_library/grad_descent_steplength_adjuster_2d.py,14,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport time\nfrom IPython.display import clear_output\nfrom matplotlib import gridspec\n\n# simple first order taylor series visualizer\nclass visualizer:\n    '''\n    Illustrator for the affect of step size choice on the behavior of gradient descent.  User chooses\n       a) an input function\n       b) an initial point \n       c) a range of step length values to try\n    Several runs of gradient descent are then executed - one for each choice of step length to try -\n    and a custom slider widget is used to visualize each completed run.  As the slider is moved from \n    left to right a different run - with another step size - is illustrated graphically.  Points in each\n    run are colored green (if near the start of the run) to yellow (as the run approaches its maximum number\n    of iterations) to red (when near completion).  Points are shown both plotted on the cost function itself,\n    as well as a cost function history plotted per-iteration.\n    ''' \n             \n    ######## gradient descent ########\n     # run gradient descent \n    def run_gradient_descent(self,alpha):\n        w = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w)\n        w_old = np.inf\n        j = 0\n        for j in range(int(self.max_its)):\n            # update old w and index\n            w_old = w\n            \n            # plug in value into func and derivative\n            grad_eval = float(self.grad(w))\n            \n            # normalized or unnormalized?\n            if self.version == 'normalized':\n                grad_norm = abs(grad_eval)\n                if grad_norm == 0:\n                    grad_norm += 10**-6*np.sign(2*np.random.rand(1) - 1)\n                grad_eval /= grad_norm\n                         \n            # take gradient descent step\n            w = w - alpha*grad_eval\n            \n            # record\n            self.w_hist.append(w)\n           \n            \n    # adaptive plotting for input function\n    def plot_function(self,ax):\n        big_val = np.amax(np.asarray([abs(a) for a in self.w_hist]))\n        big_val = max(big_val,3)\n        \n        # create plotting range\n        w_plot = np.linspace(-big_val,big_val,500)\n        g_plot = self.g(w_plot)\n        \n        # plot function\n        ax.plot(w_plot,g_plot,color = 'k',zorder = 0)               # plot function\n            \n    # animate the method\n    def animate_it(self,savepath,**kwargs):\n        # presets\n        self.g = kwargs['g']                            # input function\n        self.grad = compute_grad(self.g)              # gradient of input function\n        self.w_init =float( -2)                       # user-defined initial point (adjustable when calling each algorithm)\n        self.max_its = 20                             # max iterations to run for each algorithm\n        self.w_hist = []                              # container for algorithm path\n        wmin = -3.1                                   # max and min viewing\n        wmax = 3.1  \n        self.steplength_range = np.linspace(10**-4,1,20)      # default range of alpha (step length) values to try, adjustable\n        \n        # adjust range of step values to illustrate as well as initial point for all runs\n        if 'steplength_range' in kwargs:\n            self.steplength_range = kwargs['steplength_range']\n        if 'wmin' in kwargs:            \n            wmin = kwargs['wmin']\n        if 'wmax' in kwargs:\n            wmax = kwargs['wmax']\n        \n        # get new initial point if desired\n        if 'w_init' in kwargs:\n            self.w_init = kwargs['w_init']\n            \n        # take in user defined step length\n        if 'steplength' in kwargs:\n            self.steplength = kwargs['steplength']\n            \n        # take in user defined maximum number of iterations\n        if 'max_its' in kwargs:\n            self.max_its = float(kwargs['max_its'])\n            \n        # version of gradient descent to use (normalized or unnormalized)\n        self.version = 'unnormalized'\n        if 'version' in kwargs:\n            self.version = kwargs['version']\n            \n        # turn on first order approximation illustrated at each step\n        tracers = 'off'\n        if 'tracers' in kwargs:\n            tracers = kwargs['tracers']\n           \n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n        \n        # create subplot with 2 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0]);\n        ax2 = plt.subplot(gs[1],sharey=ax1); \n        gs.update(wspace=0.5, hspace=0.1) \n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(wmin,wmax,500)\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.1\n        width = 30\n        \n        # animation sub-function\n        num_frames = len(self.steplength_range)+1\n        print ('starting animation rendering...')\n        def animate(k):\n            ax1.cla()\n            ax2.cla()\n            \n            # print rendering update            \n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n                \n            # plot initial point and evaluation\n            if k == 0:\n                w_val = self.w_init\n                g_val = self.g(w_val)\n                ax1.scatter(w_val,g_val,s = 100,c = 'm',edgecolor = 'k',linewidth = 0.7,zorder = 2)            # plot point of tangency\n                # ax1.scatter(w_val,0,s = 100,c = 'm',edgecolor = 'k',linewidth = 0.7, zorder = 2, marker = 'X')\n                # plot function \n                ax1.plot(w_plot,g_plot,color = 'k',zorder = 0)               # plot function\n\n            # plot function alone first along with initial point\n            if k > 0:\n                alpha = self.steplength_range[k-1]\n                \n                # run gradient descent method\n                self.w_hist = []\n                self.run_gradient_descent(alpha = alpha)\n                \n                # plot function\n                self.plot_function(ax1)\n        \n                # colors for points\n                s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n                s.shape = (len(s),1)\n                t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n                t.shape = (len(t),1)\n                s = np.vstack((s,t))\n                self.colorspec = []\n                self.colorspec = np.concatenate((s,np.flipud(s)),1)\n                self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n                # plot everything for each iteration \n                for j in range(len(self.w_hist)):  \n                    w_val = self.w_hist[j]\n                    g_val = self.g(w_val)\n                    grad_val = self.grad(w_val)\n                    ax1.scatter(w_val,g_val,s = 90,c = self.colorspec[j],edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                    \n                    # ax1.scatter(w_val,0,s = 90,facecolor = self.colorspec[j],marker = 'X',edgecolor = 'k',linewidth = 0.7, zorder = 2)\n                    \n                    # determine width to plot the approximation -- so its length == width defined above\n                    div = float(1 + grad_val**2)\n                    w1 = w_val - math.sqrt(width/div)\n                    w2 = w_val + math.sqrt(width/div)\n\n                    # use point-slope form of line to plot\n                    wrange = np.linspace(w1,w2, 100)\n                    h = g_val + grad_val*(wrange - w_val)\n                \n                    # plot tracers connecting consecutive points on the cost (for visualization purposes)\n                    if tracers == 'on':\n                        if j > 0:\n                            w_old = self.w_hist[j-1]\n                            w_new = self.w_hist[j]\n                            g_old = self.g(w_old)\n                            g_new = self.g(w_new)\n                            ax1.quiver(w_old, g_old, w_new - w_old, g_new - g_old, scale_units='xy', angles='xy', scale=1, color = self.colorspec[j],linewidth = 1.5,alpha = 0.2,linestyle = '-',headwidth = 4.5,edgecolor = 'k',headlength = 10,headaxislength = 7)\n            \n                    ### plot all on cost function decrease plot\n                    ax2.scatter(j,g_val,s = 90,c = self.colorspec[j],edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                    \n                    # clean up second axis, set title on first\n                    ax2.set_xticks(np.arange(len(self.w_hist)))\n                    ax1.set_title(r'$\\alpha = $' + r'{:.2f}'.format(alpha),fontsize = 14)\n\n                    # plot connector between points for visualization purposes\n                    if j > 0:\n                        w_old = self.w_hist[j-1]\n                        w_new = self.w_hist[j]\n                        g_old = self.g(w_old)\n                        g_new = self.g(w_new)\n                        ax2.plot([j-1,j],[g_old,g_new],color = self.colorspec[j],linewidth = 2,alpha = 0.4,zorder = 1)      # plot approx\n \n            ### clean up function plot ###\n            # fix viewing limits on function plot\n            #ax1.set_xlim([-3,3])\n            #ax1.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            \n            # draw axes and labels\n            ax1.set_xlabel(r'$w$',fontsize = 13)\n            ax1.set_ylabel(r'$g(w)$',fontsize = 13,rotation = 0,labelpad = 25)   \n\n            ax2.set_xlabel('iteration',fontsize = 13)\n            ax2.set_ylabel(r'$g(w)$',fontsize = 13,rotation = 0,labelpad = 25)\n            ax1.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n            ax2.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n\n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        \n        clear_output()    """
mlrefined_libraries/math_optimization_library/grad_descent_steplength_adjuster_3d.py,19,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import jacobian\nfrom autograd import hessian\nimport math\nimport time\nfrom matplotlib import gridspec\n\n# simple first order taylor series visualizer\nclass visualizer:\n    '''\n    Illustrator for the affect of step size choice on the behavior of gradient descent on a 3d cost function (2 inputs).  User chooses\n       a) an input function\n       b) an initial point \n       c) a range of step length values to try\n    Several runs of gradient descent are then executed - one for each choice of step length to try -\n    and a custom slider widget is used to visualize each completed run.  As the slider is moved from \n    left to right a different run - with another step size - is illustrated graphically.  Points in each\n    run are colored green (if near the start of the run) to yellow (as the run approaches its maximum number\n    of iterations) to red (when near completion).  Points are shown both plotted on the cost function itself,\n    as well as a cost function history plotted per-iteration.\n    ''' \n     \n    ######## gradient descent ########\n    # run gradient descent\n    def run_gradient_descent(self,alpha):\n        w_val = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w_val)\n        w_old = np.inf\n        j = 0\n        while np.linalg.norm(w_old - w_val)**2 > 10**-5 and j < self.max_its:\n            # update old w and index\n            w_old = w_val\n            j+=1\n            \n            # plug in value into func and derivative\n            grad_val = self.grad(w_val)\n            grad_val.shape = (2,1)\n            \n            # take newtons step\n            w_val = w_val - alpha*grad_val\n            \n            # record\n            self.w_hist.append(w_val)\n            \n    # determine plotting area for function based on current gradient descent run\n    def plot_function(self,ax):\n        big_val1 = np.amax(np.asarray([abs(a[0]) for a in self.w_hist]))\n        big_val2 = np.amax(np.asarray([abs(a[1]) for a in self.w_hist]))\n        big_val = max(big_val1,big_val2,3)\n        \n        # create plotting range\n        r = np.linspace(-big_val,big_val,100)\n\n        # create grid from plotting range\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        g_vals = self.g([w1_vals,w2_vals])\n\n        # vals for cost surface\n        w1_vals.shape = (len(r),len(r))\n        w2_vals.shape = (len(r),len(r))\n        g_vals.shape = (len(r),len(r))\n        \n        # vals for plotting range\n        gmin = np.amin(g_vals)\n        gmax = np.amax(g_vals)\n        ggap = (gmax - gmin)*0.1\n        gmin = gmin - ggap\n        gmax = gmax + ggap\n        \n        # plot and fix up panel\n        strider = int(round(45/float(big_val)))\n        strider = max(strider,2)\n        ax.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = 'k',rstride=strider, cstride=strider ,linewidth=1,edgecolor = 'k')  \n        \n    # animate the method\n    def animate_it(self,savepath,**kwargs):\n        self.g = kwargs['g']                               # input function defined by user\n        self.grad = compute_grad(self.g)                 # first derivative of input\n        self.hess = compute_grad(self.grad)              # second derivative of input\n        self.alpha_range = np.linspace(10**-4,1,20)      # default range of alpha (step length) values to try, adjustable\n        self.max_its = 20\n        \n        # adjust range of step values to illustrate as well as initial point for all runs\n        if 'alpha_range' in kwargs:\n            self.alpha_range = kwargs['alpha_range']\n            \n        if 'max_its' in kwargs:\n            self.max_its = kwargs['max_its']\n \n        if 'w_init' in kwargs:\n            w_init = kwargs['w_init']\n            w_init = [float(a) for a in w_init]\n            self.w_init = np.asarray(w_init)\n            self.w_init.shape = (2,1)\n            \n        view = [10,50]\n        if 'view' in kwargs:\n            view = kwargs['view']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (9,5))\n        artist = fig\n        \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[3,1]) \n        ax1 = plt.subplot(gs[0],projection='3d'); \n        ax2 = plt.subplot(gs[1]); \n\n        # animation sub-function\n        print ('starting animation rendering...')\n        num_frames = len(self.alpha_range)+1\n        def animate(k):\n            ax1.cla()\n            ax2.cla()\n            \n            # print rendering update            \n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            # plot initial point and evaluation\n            if k == 0:\n                w_val = self.w_init\n                g_val = self.g(w_val)\n                ax1.scatter(w_val[0],w_val[1],g_val,s = 100,c = 'm',edgecolor = 'k',linewidth = 0.7,zorder = 2)            # plot point of tangency\n                \n                # plot function \n                r = np.linspace(-3,3,100)\n\n                # create grid from plotting range\n                w1_vals,w2_vals = np.meshgrid(r,r)\n                w1_vals.shape = (len(r)**2,1)\n                w2_vals.shape = (len(r)**2,1)\n                g_vals = self.g([w1_vals,w2_vals])\n\n                # vals for cost surface\n                w1_vals.shape = (len(r),len(r))\n                w2_vals.shape = (len(r),len(r))\n                g_vals.shape = (len(r),len(r))\n\n                ax1.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = 'k',rstride=15, cstride=15,linewidth=1,edgecolor = 'k')    \n\n            # plot function alone first along with initial point\n            if k > 0:\n                alpha = self.alpha_range[k-1]\n                \n                # setup axes\n                ax1.set_title(r'$\\alpha = $' + r'{:.2f}'.format(alpha),fontsize = 14)\n                ax2.set_xlabel('iteration',fontsize = 13)\n                ax2.set_ylabel('cost function value',fontsize = 13)          \n                \n                # run gradient descent method\n                self.w_hist = []\n                self.run_gradient_descent(alpha = alpha)\n                \n                # plot function\n                self.plot_function(ax1)\n        \n                # colors for points\n                s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n                s.shape = (len(s),1)\n                t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n                t.shape = (len(t),1)\n                s = np.vstack((s,t))\n                self.colorspec = []\n                self.colorspec = np.concatenate((s,np.flipud(s)),1)\n                self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n                # plot everything for each iteration \n                for j in range(len(self.w_hist)):  \n                    w_val = self.w_hist[j]\n                    g_val = self.g(w_val)\n                    grad_val = self.grad(w_val)\n                    ax1.scatter(w_val[0],w_val[1],g_val,s = 90,c = self.colorspec[j],edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n            \n                    ### plot all on cost function decrease plot\n                    ax2.scatter(j,g_val,s = 90,c = self.colorspec[j],edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                    \n                    # clean up second axis\n                    ax2.set_xticks(np.arange(len(self.w_hist)))\n                    \n                    # plot connector between points for visualization purposes\n                    if j > 0:\n                        w_old = self.w_hist[j-1]\n                        w_new = self.w_hist[j]\n                        g_old = self.g(w_old)\n                        g_new = self.g(w_new)\n                        ax2.plot([j-1,j],[g_old,g_new],color = self.colorspec[j],linewidth = 2,alpha = 0.4,zorder = 1)      # plot approx\n                        \n            # clean up plot\n            ax1.view_init(view[0],view[1])\n            ax1.set_axis_off()\n \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        \n        clear_output()    """
mlrefined_libraries/math_optimization_library/gradient_descent_demos.py,49,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport time\n\nclass visualizer:\n    \'\'\'\n    Illustrate gradient descent, Newton method, and Secant method for minimizing an input function, illustrating\n    surrogate functions at each step.  A custom slider mechanism is used to progress each algorithm, and points are\n    colored from green at the start of an algorithm, to yellow as it converges, and red as the final point.\n    \'\'\' \n     \n    ######## gradient descent ########\n    # run gradient descent \n    def run_gradient_descent(self):\n        w = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w)\n        w_old = np.inf\n        j = 0\n        for j in range(int(self.max_its)):\n            # update old w and index\n            w_old = w\n            \n            # plug in value into func and derivative\n            grad_eval = self.grad(w)\n            \n            # normalized or unnormalized?\n            if self.version == \'normalized\':\n                grad_norm = np.linalg.norm(grad_eval)\n                if grad_norm == 0:\n                    grad_norm += 10**-6*np.sign(2*np.random.rand(1) - 1)\n                grad_eval /= grad_norm\n                \n           # check if diminishing steplength rule used\n            alpha = 0\n            if self.steplength == \'diminishing\':\n                alpha = 1/(1 + j)\n            else:\n                alpha = float(self.steplength)            \n            \n            # take gradient descent step\n            w = w - alpha*grad_eval\n            \n            # record\n            self.w_hist.append(w)\n\n    ##### draw still image of gradient descent on single-input function ####       \n    def draw_cost(self,**kwargs):\n        self.g = kwargs[\'g\']                            # input function\n        wmin = -3.1\n        wmax = 3.1\n        if \'wmin\' in kwargs:            \n            wmin = kwargs[\'wmin\']\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\']\n                    \n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,4,1]) \n\n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\')\n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n        ax = plt.subplot(gs[1]); \n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(wmin,wmax,500)\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.1\n        width = 30\n        \n        # plot function, axes lines\n        ax.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n        ax.axhline(y=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax.axvline(x=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax.set_xlabel(r\'$w$\',fontsize = 13)\n        ax.set_ylabel(r\'$g(w)$\',fontsize = 13,rotation = 0,labelpad = 25)            \n \n    ##### draw still image of gradient descent on single-input function ####       \n    def draw_2d(self,**kwargs):\n        self.g = kwargs[\'g\']                            # input function\n        self.grad = compute_grad(self.g)              # gradient of input function\n        self.w_init =float( -2)                       # user-defined initial point (adjustable when calling each algorithm)\n        self.alpha = 10**-4                           # user-defined step length for gradient descent (adjustable when calling gradient descent)\n        self.max_its = 20                             # max iterations to run for each algorithm\n        self.w_hist = []                              # container for algorithm path\n        \n        wmin = -3.1\n        wmax = 3.1\n        if \'wmin\' in kwargs:            \n            wmin = kwargs[\'wmin\']\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\']\n        \n        # get new initial point if desired\n        if \'w_inits\' in kwargs:\n            self.w_inits = kwargs[\'w_inits\']\n            self.w_inits = [float(s) for s in self.w_inits]\n            \n        # take in user defined step length\n        if \'steplength\' in kwargs:\n            self.steplength = kwargs[\'steplength\']\n            \n        # take in user defined maximum number of iterations\n        if \'max_its\' in kwargs:\n            self.max_its = float(kwargs[\'max_its\'])\n            \n        # version of gradient descent to use (normalized or unnormalized)\n        self.version = \'unnormalized\'\n        if \'version\' in kwargs:\n            self.version = kwargs[\'version\']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n        \n        # remove whitespace from figure\n        #fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        #fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n        # create subplot with 2 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n\n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]); \n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(wmin,wmax,500)\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.1\n        width = 30\n       \n        #### loop over all initializations, run gradient descent algorithm for each and plot results ###\n        for j in range(len(self.w_inits)):\n            # get next initialization\n            self.w_init = self.w_inits[j]\n            \n            # run grad descent for this init\n            self.w_hist = []\n            self.run_gradient_descent()\n        \n            # colors for points --> green as the algorithm begins, yellow as it converges, red at final point\n            s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n            s.shape = (len(s),1)\n            t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n            t.shape = (len(t),1)\n            s = np.vstack((s,t))\n            self.colorspec = []\n            self.colorspec = np.concatenate((s,np.flipud(s)),1)\n            self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n            # plot function, axes lines\n            ax1.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n            ax1.axhline(y=0, color=\'k\',zorder = 1,linewidth = 0.25)\n            ax1.axvline(x=0, color=\'k\',zorder = 1,linewidth = 0.25)\n            ax1.set_xlabel(r\'$w$\',fontsize = 13)\n            ax1.set_ylabel(r\'$g(w)$\',fontsize = 13,rotation = 0,labelpad = 25)            \n            \n            ax2.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n            ax2.axhline(y=0, color=\'k\',zorder = 1,linewidth = 0.25)\n            ax2.axvline(x=0, color=\'k\',zorder = 1,linewidth = 0.25)\n            ax2.set_xlabel(r\'$w$\',fontsize = 13)\n            ax2.set_ylabel(r\'$g(w)$\',fontsize = 13,rotation = 0,labelpad = 25)            \n            \n            ### plot all gradient descent points ###\n            for k in range(len(self.w_hist)):\n                # pick out current weight and function value from history, then plot\n                w_val = self.w_hist[k]\n                g_val = self.g(w_val)\n            \n                ax2.scatter(w_val,g_val,s = 90,c = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4),zorder = 3,marker = \'X\')            # evaluation on function\n                ax2.scatter(w_val,0,s = 90,facecolor = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4), zorder = 3)\n                    \n\n    ##### draw still image of gradient descent on single-input function ####       \n    def compare_versions_2d(self,**kwargs):\n        self.g = kwargs[\'g\']                            # input function\n        self.grad = compute_grad(self.g)              # gradient of input function\n        self.w_init =float( -2)                       # user-defined initial point (adjustable when calling each algorithm)\n        self.alpha = 10**-4                           # user-defined step length for gradient descent (adjustable when calling gradient descent)\n        self.max_its = 20                             # max iterations to run for each algorithm\n        self.w_hist = []                              # container for algorithm path\n        \n        # get new initial point if desired\n        if \'w_init\' in kwargs:\n            self.w_init = float(kwargs[\'w_init\'])\n            \n        # take in user defined step length\n        if \'steplength\' in kwargs:\n            self.steplength = kwargs[\'steplength\']\n            \n        # take in user defined maximum number of iterations\n        if \'max_its\' in kwargs:\n            self.max_its = float(kwargs[\'max_its\'])\n            \n        # version of gradient descent to use (normalized or unnormalized)\n        self.version = \'unnormalized\'\n        if \'version\' in kwargs:\n            self.version = kwargs[\'version\']\n            \n        # define viewing min and max\n        wmin = -3.1\n        wmax = 3.1\n        if \'wmin\' in kwargs:\n            wmin = kwargs[\'wmin\']\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n        \n        # remove whitespace from figure\n        #fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        #fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n        # create subplot with 2 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n\n        ax1 = plt.subplot(gs[0]);\n        ax2 = plt.subplot(gs[1]); \n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(wmin,wmax,500)\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.1\n        width = 30       \n        \n        # plot function, axes lines\n        for ax in [ax1,ax2]:\n            ax.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n            ax.axhline(y=0, color=\'k\',zorder = 1,linewidth = 0.25)\n            ax.axvline(x=0, color=\'k\',zorder = 1,linewidth = 0.25)\n            ax.set_xlabel(r\'$w$\',fontsize = 13)\n            ax.set_ylabel(r\'$g(w)$\',fontsize = 13,rotation = 0,labelpad = 25)            \n        \n        ax1.set_title(\'normalized gradient descent\',fontsize = 13)\n        ax2.set_title(\'unnormalized gradient descent\',fontsize = 13)\n\n        ### run normalized gradient descent and plot results ###\n        \n        # run normalized gradient descent method\n        self.version = \'normalized\'\n        self.w_hist = []\n        self.run_gradient_descent()\n        \n        # colors for points --> green as the algorithm begins, yellow as it converges, red at final point\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # plot results\n        for k in range(len(self.w_hist)):\n            # pick out current weight and function value from history, then plot\n            w_val = self.w_hist[k]\n            g_val = self.g(w_val)\n            \n            ax1.scatter(w_val,g_val,s = 90,c = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4),zorder = 3,marker = \'X\')            # evaluation on function\n            ax1.scatter(w_val,0,s = 90,facecolor = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4), zorder = 3)\n            \n        # run unnormalized gradient descent method\n        self.version = \'unnormalized\'\n        self.w_hist = []\n        self.run_gradient_descent()\n        \n        # plot results\n        for k in range(len(self.w_hist)):\n            # pick out current weight and function value from history, then plot\n            w_val = self.w_hist[k]\n            g_val = self.g(w_val)\n            \n            ax2.scatter(w_val,g_val,s = 90,c = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4),zorder = 3,marker = \'X\')            # evaluation on function\n            ax2.scatter(w_val,0,s = 90,facecolor = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4), zorder = 3)\n            \n            \n                \n    ##### animate gradient descent method using single-input function #####\n    def animate_2d(self,g,w_hist,**kwargs):\n        self.g = g                                    # input function\n        self.w_hist = w_hist                          # input weight history\n        self.grad = compute_grad(self.g)              # gradient of input function\n        self.w_init = self.w_hist[0]                  # user-defined initial point (adjustable when calling each algorithm)\n         \n        wmin = -3.1\n        wmax = 3.1\n        if \'wmin\' in kwargs:            \n            wmin = kwargs[\'wmin\']\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n        \n        # remove whitespace from figure\n        #fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        #fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,4,1]) \n\n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\')\n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n        ax = plt.subplot(gs[1]); \n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(wmin,wmax,200)\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.1\n        width = 30\n        \n        # colors for points --> green as the algorithm begins, yellow as it converges, red at final point\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # animation sub-function\n        num_frames = 2*len(self.w_hist)+2\n        print (\'starting animation rendering...\')\n        def animate(t):\n            ax.cla()\n            k = math.floor((t+1)/float(2))\n            \n            # print rendering update            \n            if np.mod(t+1,25) == 0:\n                print (\'rendering animation frame \' + str(t+1) + \' of \' + str(num_frames))\n            if t == num_frames - 1:\n                print (\'animation rendering complete!\')\n                time.sleep(1.5)\n                clear_output()\n            \n            # plot function\n            ax.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n            \n            # plot initial point and evaluation\n            if k == 0:\n                w_val = self.w_init\n                g_val = self.g(w_val)\n                ax.scatter(w_val,g_val,s = 90,c = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4),zorder = 3,marker = \'X\')            # evaluation on function\n                ax.scatter(w_val,0,s = 90,facecolor = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4), zorder = 3)\n                \n                # draw dashed line connecting w axis to point on cost function\n                s = np.linspace(0,g_val)\n                o = np.ones((len(s)))\n                ax.plot(o*w_val,s,\'k--\',linewidth=1)\n\n            # plot all input/output pairs generated by algorithm thus far\n            if k > 0:\n                # plot all points up to this point\n                for j in range(min(k-1,len(self.w_hist))):  \n                    w_val = self.w_hist[j]\n                    g_val = self.g(w_val)\n                    ax.scatter(w_val,g_val,s = 90,c = self.colorspec[j],edgecolor = \'k\',linewidth = 0.5*((1/(float(j) + 1)))**(0.4),zorder = 3,marker = \'X\')            # plot point of tangency\n                    ax.scatter(w_val,0,s = 90,facecolor = self.colorspec[j],edgecolor = \'k\',linewidth =  0.5*((1/(float(j) + 1)))**(0.4), zorder = 2)\n                    \n            # plot surrogate function and travel-to point\n            if k > 0 and k < len(self.w_hist) + 1:          \n                # grab historical weight, compute function and derivative evaluations\n                w = self.w_hist[k-1]\n                g_eval = self.g(w)\n                grad_eval = float(self.grad(w))\n            \n                # determine width to plot the approximation -- so its length == width defined above\n                div = float(1 + grad_eval**2)\n                w1 = w - math.sqrt(width/div)\n                w2 = w + math.sqrt(width/div)\n\n                # use point-slope form of line to plot\n                wrange = np.linspace(w1,w2, 100)\n                h = g_eval + grad_eval*(wrange - w)\n\n                # plot tangent line\n                ax.plot(wrange,h,color = self.colorspec[k-1],linewidth = 2,zorder = 1)      # plot approx\n\n                # plot tangent point\n                ax.scatter(w,g_eval,s = 100,c = \'m\',edgecolor = \'k\',linewidth = 0.7,zorder = 3,marker = \'X\')            # plot point of tangency\n            \n                # plot next point learned from surrogate\n                if np.mod(t,2) == 0 and k < len(self.w_hist) -1:\n                    # create next point information\n                    w_zero = self.w_hist[k]\n                    g_zero = self.g(w_zero)\n                    h_zero = g_eval + grad_eval*(w_zero - w)\n\n                    # draw dashed line connecting the three\n                    vals = [0,h_zero,g_zero]\n                    vals = np.sort(vals)\n\n                    s = np.linspace(vals[0],vals[2])\n                    o = np.ones((len(s)))\n                    ax.plot(o*w_zero,s,\'k--\',linewidth=1)\n\n                    # draw intersection at zero and associated point on cost function you hop back too\n                    ax.scatter(w_zero,h_zero,s = 100,c = \'k\', zorder = 3,marker = \'X\')\n                    ax.scatter(w_zero,0,s = 100,c = \'m\',edgecolor = \'k\',linewidth = 0.7, zorder = 3)\n                    ax.scatter(w_zero,g_zero,s = 100,c = \'m\',edgecolor = \'k\',linewidth = 0.7,zorder = 3, marker = \'X\')            # plot point of tangency\n                 \n            # fix viewing limits\n            ax.set_xlim([wmin-0.1,wmax+0.1])\n            ax.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            \n            # place title\n            ax.set_xlabel(r\'$w$\',fontsize = 14)\n            ax.set_ylabel(r\'$g(w)$\',fontsize = 14,rotation = 0,labelpad = 25)\n\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        return(anim)\n\n    # visualize descent on multi-input function\n    def visualize3d(self,g,w_hist,**kwargs):\n        ### input arguments ###        \n        self.g = g\n        self.grad = compute_grad(self.g)              # gradient of input function\n        self.w_hist = w_hist\n            \n        wmax = 1\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\'] + 0.5\n\n        view = [20,-50]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n\n        axes = False\n        if \'axes\' in kwargs:\n            axes = kwargs[\'axes\']\n\n        plot_final = False\n        if \'plot_final\' in kwargs:\n            plot_final = kwargs[\'plot_final\']\n\n        num_contours = 10\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']\n            \n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (11,3))\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,10]) \n        ax = plt.subplot(gs[1],projection=\'3d\'); \n        ax2 = plt.subplot(gs[2],aspect=\'equal\'); \n        \n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n        \n        #### define input space for function and evaluate ####\n        w = np.linspace(-wmax,wmax,200)\n        w1_vals, w2_vals = np.meshgrid(w,w)\n        w1_vals.shape = (len(w)**2,1)\n        w2_vals.shape = (len(w)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([g(s) for s in h])\n        w1_vals.shape = (len(w),len(w))\n        w2_vals.shape = (len(w),len(w))\n        func_vals.shape = (len(w),len(w))\n\n        # plot function \n        ax.plot_surface(w1_vals, w2_vals, func_vals, alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)\n\n        # plot z=0 plane \n        ax.plot_surface(w1_vals, w2_vals, func_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        ax2.contour(w1_vals, w2_vals, func_vals,num_contours,colors = \'k\')\n        if axes == True:\n            ax2.axhline(linestyle = \'--\', color = \'k\',linewidth = 1)\n            ax2.axvline(linestyle = \'--\', color = \'k\',linewidth = 1)\n\n        # colors for points\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        colorspec = []\n        colorspec = np.concatenate((s,np.flipud(s)),1)\n        colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n\n        #### scatter path points ####\n        for k in range(len(self.w_hist)):\n            w_now = self.w_hist[k]\n            ax.scatter(w_now[0],w_now[1],0,s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 0.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n\n            ax2.scatter(w_now[0],w_now[1],s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 1.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n\n        #### connect points with arrows ####\n        if len(self.w_hist) < 10:\n            for i in range(len(self.w_hist)-1):\n                pt1 = self.w_hist[i]\n                pt2 = self.w_hist[i+1]\n\n                # draw arrow in left plot\n                a = Arrow3D([pt1[0],pt2[0]], [pt1[1],pt2[1]], [0, 0], mutation_scale=10, lw=2, arrowstyle=""-|>"", color=""k"")\n                ax.add_artist(a)\n\n                # draw 2d arrow in right plot\n                ax2.arrow(pt1[0],pt1[1],(pt2[0] - pt1[0])*0.78,(pt2[1] - pt1[1])*0.78, head_width=0.1, head_length=0.1, fc=\'k\', ec=\'k\',linewidth=3,zorder = 2,length_includes_head=True)\n\n        ### cleanup panels ###\n        ax.set_xlabel(\'$w_1$\',fontsize = 12)\n        ax.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n        ax.set_title(\'$g(w_1,w_2)$\',fontsize = 12)\n        ax.view_init(view[0],view[1])\n\n        ax2.set_xlabel(\'$w_1$\',fontsize = 12)\n        ax2.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n        ax2.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax2.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax2.set_xlim([-wmax,wmax])\n        ax2.set_ylim([-wmax,wmax])\n\n        # clean up axis\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        # plot\n        plt.show()\n        \n    # compare normalized and unnormalized grad descent on 3d example\n    def compare_versions_3d(self,g,w_init,steplength,max_its,**kwargs):\n        ### input arguments ###        \n        self.g = g\n        self.steplength = steplength\n        self.max_its = max_its\n        self.grad = compute_grad(self.g)              # gradient of input function\n\n        wmax = 1\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\'] + 0.5\n\n        view = [20,-50]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n\n        axes = False\n        if \'axes\' in kwargs:\n            axes = kwargs[\'axes\']\n\n        plot_final = False\n        if \'plot_final\' in kwargs:\n            plot_final = kwargs[\'plot_final\']\n\n        num_contours = 10\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']\n            \n        # get initial point \n        self.w_init = np.asarray([float(s) for s in w_init])\n                    \n        # take in user defined step length\n        self.steplength = steplength\n            \n        # take in user defined maximum number of iterations\n        self.max_its = max_its\n            \n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (12,6))\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(2, 3, width_ratios=[1,5,10]) \n        ax3 = plt.subplot(gs[1],projection=\'3d\'); \n        ax4 = plt.subplot(gs[2],aspect=\'equal\'); \n        ax5 = plt.subplot(gs[4],projection=\'3d\'); \n        ax6 = plt.subplot(gs[5],aspect=\'equal\'); \n        \n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        \n        #### define input space for function and evaluate ####\n        w = np.linspace(-wmax,wmax,200)\n        w1_vals, w2_vals = np.meshgrid(w,w)\n        w1_vals.shape = (len(w)**2,1)\n        w2_vals.shape = (len(w)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([g(s) for s in h])\n        w1_vals.shape = (len(w),len(w))\n        w2_vals.shape = (len(w),len(w))\n        func_vals.shape = (len(w),len(w))\n\n        #### run local random search algorithms ####\n        for algo in [\'normalized\',\'unnormalized\']:\n            # switch normalized / unnormalized\n            self.version = algo\n            title = \'\'\n            if self.version == \'normalized\':\n                ax = ax3\n                ax2 = ax4\n                title = \'normalized gradient descent\'\n            else:\n                ax = ax5\n                ax2 = ax6\n                title = \'unnormalized gradient descent\'\n            \n           # plot function \n            ax.plot_surface(w1_vals, w2_vals, func_vals, alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)\n\n            # plot z=0 plane \n            ax.plot_surface(w1_vals, w2_vals, func_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n\n            ### make contour right plot - as well as horizontal and vertical axes ###\n            ax2.contour(w1_vals, w2_vals, func_vals,num_contours,colors = \'k\')\n            if axes == True:\n                ax2.axhline(linestyle = \'--\', color = \'k\',linewidth = 1)\n                ax2.axvline(linestyle = \'--\', color = \'k\',linewidth = 1)\n            \n            self.w_hist = []\n            self.run_gradient_descent()\n\n            # colors for points\n            s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n            s.shape = (len(s),1)\n            t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n            t.shape = (len(t),1)\n            s = np.vstack((s,t))\n            colorspec = []\n            colorspec = np.concatenate((s,np.flipud(s)),1)\n            colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n\n            #### scatter path points ####\n            for k in range(len(self.w_hist)):\n                w_now = self.w_hist[k]\n                ax.scatter(w_now[0],w_now[1],0,s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 0.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n\n                ax2.scatter(w_now[0],w_now[1],s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 1.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n\n            #### connect points with arrows ####\n            if len(self.w_hist) < 10:\n                for i in range(len(self.w_hist)-1):\n                    pt1 = self.w_hist[i]\n                    pt2 = self.w_hist[i+1]\n        \n                    # draw arrow in left plot\n                    a = Arrow3D([pt1[0],pt2[0]], [pt1[1],pt2[1]], [0, 0], mutation_scale=10, lw=2, arrowstyle=""-|>"", color=""k"")\n                    ax.add_artist(a)\n\n                    # draw 2d arrow in right plot\n                    ax2.arrow(pt1[0],pt1[1],(pt2[0] - pt1[0])*0.78,(pt2[1] - pt1[1])*0.78, head_width=0.1, head_length=0.1, fc=\'k\', ec=\'k\',linewidth=3,zorder = 2,length_includes_head=True)\n\n            ### cleanup panels ###\n            ax.set_xlabel(\'$w_1$\',fontsize = 12)\n            ax.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n            ax.set_title(title,fontsize = 12)\n            ax.view_init(view[0],view[1])\n    \n            ax2.set_xlabel(\'$w_1$\',fontsize = 12)\n            ax2.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n            ax2.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            ax2.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n\n            # clean up axis\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        # plot\n        plt.show()      \n        \n        \n#### custom 3d arrow and annotator functions ###    \n# nice arrow maker from https://stackoverflow.com/questions/11140163/python-matplotlib-plotting-a-3d-cube-a-sphere-and-a-vector\nclass Arrow3D(FancyArrowPatch):\n\n    def __init__(self, xs, ys, zs, *args, **kwargs):\n        FancyArrowPatch.__init__(self, (0, 0), (0, 0), *args, **kwargs)\n        self._verts3d = xs, ys, zs\n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n        FancyArrowPatch.draw(self, renderer)'"
mlrefined_libraries/math_optimization_library/lipschitz_majorizer.py,11,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nimport time\nfrom matplotlib import gridspec\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\n\nclass visualizer:\n    '''\n    Illustrate majorization of lipschitz gradient-based quadratic majorizer of an input function\n    '''\n    def __init__(self,**args):\n        self.g = args['g']                       # input function\n        self.grad = compute_grad(self.g)         # gradient of input function\n        self.hess = compute_grad(self.grad)      # hessian of input function\n        self.colors = [[0,1,0.25],[0,0.75,1]]    # set of custom colors used for plotting\n\n    # compute first order approximation\n    def animate_it(self,savepath,**kwargs):\n        num_frames = 100                          # number of slides to create - the input range [-3,3] is divided evenly by this number\n        if 'num_frames' in kwargs:\n            num_frames = kwargs['num_frames']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (10,5))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5, 1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis('off');\n        ax3 = plt.subplot(gs[2]); ax3.axis('off');\n\n        # plot input function\n        ax = plt.subplot(gs[1],aspect = 'equal')\n        \n        max_val = 2.5\n        if 'max_val' in kwargs:\n            max_val = kwargs['max_val']\n        w_vals = np.linspace(-max_val,max_val,num_frames)       # range of values over which to plot first / second order approximations\n        \n        # generate a range of values over which to plot input function, and derivatives\n        w_plot = np.linspace(-max_val-0.5,max_val+0.5,200)                  # input range for original function\n        \n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)             # used for cleaning up final plot\n        ggap = g_range*0.1\n        \n        # estimate Lipschitz constant over input range\n        w_temp = np.linspace(-max_val-0.5,max_val+0.5,2000)\n        hess_vals = [abs(self.hess(s)) for s in w_temp]\n        L = max(hess_vals)\n        alpha = 1/float(L)\n     \n        # animation sub-function\n        print ('starting animation rendering...')\n        def animate(k):\n            # clear the panel\n            ax.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            # grab the next input/output tangency pair, minimum of gradient quadratic\n            w_val = w_vals[k]\n            g_val = self.g(w_val)\n            grad_val = self.grad(w_val)\n            w_step = w_val - alpha*grad_val\n            g_step = self.g(w_step)\n\n            # plot original function\n            ax.plot(w_plot,g_plot,color = 'k',zorder = 0,linewidth=1)                           # plot function\n       \n            # create and Lipschitz majorizer centered on w_val\n            h = lambda w: g_val + grad_val*(w - w_val) + 1/(2*alpha)*(w - w_val)**2\n            width = 2*max_val\n            w_major = np.linspace(w_step - width,w_step + width,200)\n            h_major = h(w_major)\n            h_step = h(w_step)\n            \n            # plot majorizer\n            ax.plot(w_major,h_major,color = self.colors[1],zorder = 1,linewidth=2)   \n            \n            # plot all points\n            ax.scatter(w_step,h_step,s = 60,c = 'blue',edgecolor = 'k',linewidth = 0.7,marker = 'X',zorder = 3)           \n            ax.scatter(w_step,g_step,s = 60,c = 'lime',edgecolor = 'k',linewidth = 0.7,marker = 'X',zorder = 3)            # plot point of tangency\n            ax.scatter(w_step,0,s = 80,c = 'lime',edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n            ax.scatter(w_val,g_val,s = 60,c = 'r',edgecolor = 'k',linewidth = 0.7,marker = 'X',zorder = 3)            # plot point of tangency\n            ax.scatter(w_val,0,s = 80,c = 'r',edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n            \n            # plot visual aid for old point\n            tempy = np.linspace(0,g_val,100)\n            tempx = w_val*np.ones((100))\n            ax.plot(tempx,tempy,linewidth = 0.7,color = 'k',linestyle = '--',zorder = 1)\n            \n            # plot visual aid for new point\n            tempy = np.linspace(0,h_step,100)\n            tempx = w_step*np.ones((100))\n            ax.plot(tempx,tempy,linewidth = 0.7,color = 'k',linestyle = '--',zorder = 1)\n            \n            # label axes\n            ax.set_xlabel('$w$',fontsize = 12)\n            ax.set_ylabel('$g(w)$',fontsize = 12,rotation = 0,labelpad = 12)\n\n            # fix viewing limits on panel\n            ax.set_xlim([-max_val,max_val])\n            ax.set_ylim([min(-0.3,min(g_plot) - ggap),max(max(g_plot) + ggap,0.3)])\n            \n            # set tickmarks\n            ax.set_xticks(-np.arange(-round(max_val), round(max_val) + 1, 1.0))\n            ax.set_yticks(np.arange(round(min(g_plot) - ggap), round(max(g_plot) + ggap) + 1, 1.0))\n                \n            # set axis \n            ax.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n            \n            return artist,\n        \n        anim = animation.FuncAnimation(fig, animate,frames=len(w_vals), interval=len(w_vals), blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()"""
mlrefined_libraries/math_optimization_library/local_method_visualizer.py,50,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport time\n\nclass Visualizer:\n    \'\'\'\n    Illustrate gradient descent, Newton method, and Secant method for minimizing an input function, illustrating\n    surrogate functions at each step.  A custom slider mechanism is used to progress each algorithm, and points are\n    colored from green at the start of an algorithm, to yellow as it converges, and red as the final point.\n    \'\'\' \n     \n    ######## gradient descent ########\n    # run gradient descent \n    def run_gradient_descent(self):\n        w = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w)\n        w_old = np.inf\n        j = 0\n        for j in range(int(self.max_its)):\n            # update old w and index\n            w_old = w\n            \n            # plug in value into func and derivative\n            grad_eval = self.grad(w)\n            \n            # normalized or unnormalized?\n            if self.version == \'normalized\':\n                grad_norm = np.linalg.norm(grad_eval)\n                if grad_norm == 0:\n                    grad_norm += 10**-6*np.sign(2*np.random.rand(1) - 1)\n                grad_eval /= grad_norm\n                \n           # check if diminishing steplength rule used\n            alpha = 0\n            if self.steplength == \'diminishing\':\n                alpha = 1/(1 + j)\n            else:\n                alpha = float(self.steplength)            \n            \n            # take gradient descent step\n            w = w - alpha*grad_eval\n            \n            # record\n            self.w_hist.append(w)\n\n    ##### draw still image of gradient descent on single-input function ####       \n    def draw_cost(self,**kwargs):\n        self.g = kwargs[\'g\']                            # input function\n        wmin = -3.1\n        wmax = 3.1\n        if \'wmin\' in kwargs:            \n            wmin = kwargs[\'wmin\']\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\']\n                    \n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,4,1]) \n\n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\')\n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n        ax = plt.subplot(gs[1]); \n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(wmin,wmax,500)\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.1\n        width = 30\n        \n        # plot function, axes lines\n        ax.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n        ax.axhline(y=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax.axvline(x=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax.set_xlabel(r\'$w$\',fontsize = 13)\n        ax.set_ylabel(r\'$g(w)$\',fontsize = 13,rotation = 0,labelpad = 25)            \n \n    ##### draw still image of gradient descent on single-input function ####       \n    def draw_2d(self,**kwargs):\n        self.g = kwargs[\'g\']                            # input function\n        self.grad = compute_grad(self.g)              # gradient of input function\n        self.w_init =float( -2)                       # user-defined initial point (adjustable when calling each algorithm)\n        self.alpha = 10**-4                           # user-defined step length for gradient descent (adjustable when calling gradient descent)\n        self.max_its = 20                             # max iterations to run for each algorithm\n        self.w_hist = []                              # container for algorithm path\n        \n        wmin = -3.1\n        wmax = 3.1\n        if \'wmin\' in kwargs:            \n            wmin = kwargs[\'wmin\']\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\']\n        \n        # get new initial point if desired\n        if \'w_inits\' in kwargs:\n            self.w_inits = kwargs[\'w_inits\']\n            self.w_inits = [float(s) for s in self.w_inits]\n            \n        # take in user defined step length\n        if \'steplength\' in kwargs:\n            self.steplength = kwargs[\'steplength\']\n            \n        # take in user defined maximum number of iterations\n        if \'max_its\' in kwargs:\n            self.max_its = float(kwargs[\'max_its\'])\n            \n        # version of gradient descent to use (normalized or unnormalized)\n        self.version = \'unnormalized\'\n        if \'version\' in kwargs:\n            self.version = kwargs[\'version\']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n        \n        # remove whitespace from figure\n        #fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        #fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n        # create subplot with 2 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n\n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]); \n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(wmin,wmax,500)\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.1\n        width = 30\n       \n        #### loop over all initializations, run gradient descent algorithm for each and plot results ###\n        for j in range(len(self.w_inits)):\n            # get next initialization\n            self.w_init = self.w_inits[j]\n            \n            # run grad descent for this init\n            self.w_hist = []\n            self.run_gradient_descent()\n        \n            # colors for points --> green as the algorithm begins, yellow as it converges, red at final point\n            s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n            s.shape = (len(s),1)\n            t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n            t.shape = (len(t),1)\n            s = np.vstack((s,t))\n            self.colorspec = []\n            self.colorspec = np.concatenate((s,np.flipud(s)),1)\n            self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n            # plot function, axes lines\n            ax1.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n            ax1.axhline(y=0, color=\'k\',zorder = 1,linewidth = 0.25)\n            ax1.axvline(x=0, color=\'k\',zorder = 1,linewidth = 0.25)\n            ax1.set_xlabel(r\'$w$\',fontsize = 13)\n            ax1.set_ylabel(r\'$g(w)$\',fontsize = 13,rotation = 0,labelpad = 25)            \n            \n            ax2.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n            ax2.axhline(y=0, color=\'k\',zorder = 1,linewidth = 0.25)\n            ax2.axvline(x=0, color=\'k\',zorder = 1,linewidth = 0.25)\n            ax2.set_xlabel(r\'$w$\',fontsize = 13)\n            ax2.set_ylabel(r\'$g(w)$\',fontsize = 13,rotation = 0,labelpad = 25)            \n            \n            ### plot all gradient descent points ###\n            for k in range(len(self.w_hist)):\n                # pick out current weight and function value from history, then plot\n                w_val = self.w_hist[k]\n                g_val = self.g(w_val)\n            \n                ax2.scatter(w_val,g_val,s = 90,c = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4),zorder = 3,marker = \'X\')            # evaluation on function\n                ax2.scatter(w_val,0,s = 90,facecolor = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4), zorder = 3)\n                    \n\n    ##### draw still image of gradient descent on single-input function ####       \n    def compare_versions_2d(self,**kwargs):\n        self.g = kwargs[\'g\']                            # input function\n        self.grad = compute_grad(self.g)              # gradient of input function\n        self.w_init =float( -2)                       # user-defined initial point (adjustable when calling each algorithm)\n        self.alpha = 10**-4                           # user-defined step length for gradient descent (adjustable when calling gradient descent)\n        self.max_its = 20                             # max iterations to run for each algorithm\n        self.w_hist = []                              # container for algorithm path\n        \n        # get new initial point if desired\n        if \'w_init\' in kwargs:\n            self.w_init = float(kwargs[\'w_init\'])\n            \n        # take in user defined step length\n        if \'steplength\' in kwargs:\n            self.steplength = kwargs[\'steplength\']\n            \n        # take in user defined maximum number of iterations\n        if \'max_its\' in kwargs:\n            self.max_its = float(kwargs[\'max_its\'])\n            \n        # version of gradient descent to use (normalized or unnormalized)\n        self.version = \'unnormalized\'\n        if \'version\' in kwargs:\n            self.version = kwargs[\'version\']\n            \n        # define viewing min and max\n        wmin = -3.1\n        wmax = 3.1\n        if \'wmin\' in kwargs:\n            wmin = kwargs[\'wmin\']\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n        \n        # remove whitespace from figure\n        #fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        #fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n        # create subplot with 2 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n\n        ax1 = plt.subplot(gs[0]);\n        ax2 = plt.subplot(gs[1]); \n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(wmin,wmax,500)\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.1\n        width = 30       \n        \n        # plot function, axes lines\n        for ax in [ax1,ax2]:\n            ax.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n            ax.axhline(y=0, color=\'k\',zorder = 1,linewidth = 0.25)\n            ax.axvline(x=0, color=\'k\',zorder = 1,linewidth = 0.25)\n            ax.set_xlabel(r\'$w$\',fontsize = 13)\n            ax.set_ylabel(r\'$g(w)$\',fontsize = 13,rotation = 0,labelpad = 25)            \n        \n        ax1.set_title(\'normalized gradient descent\',fontsize = 13)\n        ax2.set_title(\'unnormalized gradient descent\',fontsize = 13)\n\n        ### run normalized gradient descent and plot results ###\n        \n        # run normalized gradient descent method\n        self.version = \'normalized\'\n        self.w_hist = []\n        self.run_gradient_descent()\n        \n        # colors for points --> green as the algorithm begins, yellow as it converges, red at final point\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # plot results\n        for k in range(len(self.w_hist)):\n            # pick out current weight and function value from history, then plot\n            w_val = self.w_hist[k]\n            g_val = self.g(w_val)\n            \n            ax1.scatter(w_val,g_val,s = 90,c = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4),zorder = 3,marker = \'X\')            # evaluation on function\n            ax1.scatter(w_val,0,s = 90,facecolor = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4), zorder = 3)\n            \n        # run unnormalized gradient descent method\n        self.version = \'unnormalized\'\n        self.w_hist = []\n        self.run_gradient_descent()\n        \n        # plot results\n        for k in range(len(self.w_hist)):\n            # pick out current weight and function value from history, then plot\n            w_val = self.w_hist[k]\n            g_val = self.g(w_val)\n            \n            ax2.scatter(w_val,g_val,s = 90,c = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4),zorder = 3,marker = \'X\')            # evaluation on function\n            ax2.scatter(w_val,0,s = 90,facecolor = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4), zorder = 3)\n            \n            \n                \n    ##### animate gradient descent method using single-input function #####\n    def animate_2d(self,g,w_hist,**kwargs):\n        self.g = g                                    # input function\n        self.w_hist = w_hist                          # input weight history\n        self.grad = compute_grad(self.g)              # gradient of input function\n        self.w_init = self.w_hist[0]                  # user-defined initial point (adjustable when calling each algorithm)\n         \n        wmin = -3.1\n        wmax = 3.1\n        if \'wmin\' in kwargs:            \n            wmin = kwargs[\'wmin\']\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n        \n        # remove whitespace from figure\n        #fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        #fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,4,1]) \n\n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\')\n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n        ax = plt.subplot(gs[1]); \n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(wmin,wmax,200)\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.1\n        width = 30\n        \n        # colors for points --> green as the algorithm begins, yellow as it converges, red at final point\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # animation sub-function\n        num_frames = 2*len(self.w_hist)+2\n        print (\'starting animation rendering...\')\n        def animate(t):\n            ax.cla()\n            k = math.floor((t+1)/float(2))\n            \n            # print rendering update            \n            if np.mod(t+1,25) == 0:\n                print (\'rendering animation frame \' + str(t+1) + \' of \' + str(num_frames))\n            if t == num_frames - 1:\n                print (\'animation rendering complete!\')\n                time.sleep(1.5)\n                clear_output()\n            \n            # plot function\n            ax.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n            \n            # plot initial point and evaluation\n            if k == 0:\n                w_val = self.w_init\n                g_val = self.g(w_val)\n                ax.scatter(w_val,g_val,s = 90,c = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4),zorder = 3,marker = \'X\')            # evaluation on function\n                ax.scatter(w_val,0,s = 90,facecolor = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4), zorder = 3)\n                \n                # draw dashed line connecting w axis to point on cost function\n                s = np.linspace(0,g_val)\n                o = np.ones((len(s)))\n                ax.plot(o*w_val,s,\'k--\',linewidth=1)\n\n            # plot all input/output pairs generated by algorithm thus far\n            if k > 0:\n                # plot all points up to this point\n                for j in range(min(k-1,len(self.w_hist))):  \n                    w_val = self.w_hist[j]\n                    g_val = self.g(w_val)\n                    ax.scatter(w_val,g_val,s = 90,c = self.colorspec[j],edgecolor = \'k\',linewidth = 0.5*((1/(float(j) + 1)))**(0.4),zorder = 3,marker = \'X\')            # plot point of tangency\n                    ax.scatter(w_val,0,s = 90,facecolor = self.colorspec[j],edgecolor = \'k\',linewidth =  0.5*((1/(float(j) + 1)))**(0.4), zorder = 2)\n                    \n            # plot surrogate function and travel-to point\n            if k > 0 and k < len(self.w_hist) + 1:          \n                # grab historical weight, compute function and derivative evaluations\n                w = self.w_hist[k-1]\n                g_eval = self.g(w)\n                grad_eval = float(self.grad(w))\n            \n                # determine width to plot the approximation -- so its length == width defined above\n                div = float(1 + grad_eval**2)\n                w1 = w - math.sqrt(width/div)\n                w2 = w + math.sqrt(width/div)\n\n                # use point-slope form of line to plot\n                wrange = np.linspace(w1,w2, 100)\n                h = g_eval + grad_eval*(wrange - w)\n\n                # plot tangent line\n                ax.plot(wrange,h,color = \'lime\',linewidth = 2,zorder = 1)      # plot approx\n\n                # plot tangent point\n                ax.scatter(w,g_eval,s = 100,c = \'m\',edgecolor = \'k\',linewidth = 0.7,zorder = 3,marker = \'X\')            # plot point of tangency\n            \n                # plot next point learned from surrogate\n                if np.mod(t,2) == 0 and k < len(self.w_hist) -1:\n                    # create next point information\n                    w_zero = self.w_hist[k]\n                    g_zero = self.g(w_zero)\n                    h_zero = g_eval + grad_eval*(w_zero - w)\n\n                    # draw dashed line connecting the three\n                    vals = [0,h_zero,g_zero]\n                    vals = np.sort(vals)\n\n                    s = np.linspace(vals[0],vals[2])\n                    o = np.ones((len(s)))\n                    ax.plot(o*w_zero,s,\'k--\',linewidth=1)\n\n                    # draw intersection at zero and associated point on cost function you hop back too\n                    ax.scatter(w_zero,h_zero,s = 100,c = \'k\', zorder = 3,marker = \'X\')\n                    ax.scatter(w_zero,0,s = 100,c = \'m\',edgecolor = \'k\',linewidth = 0.7, zorder = 3)\n                    ax.scatter(w_zero,g_zero,s = 100,c = \'m\',edgecolor = \'k\',linewidth = 0.7,zorder = 3, marker = \'X\')            # plot point of tangency\n                 \n            # fix viewing limits\n            ax.set_xlim([wmin-0.1,wmax+0.1])\n            ax.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            \n            # place title\n            ax.set_xlabel(r\'$w$\',fontsize = 14)\n            ax.set_ylabel(r\'$g(w)$\',fontsize = 14,rotation = 0,labelpad = 25)\n\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        return(anim)\n\n    # visualize descent on multi-input function\n    def visualize3d(self,g,w_init,steplength,max_its,**kwargs):\n        ### input arguments ###        \n        self.g = g\n        self.steplength = steplength\n        self.max_its = max_its\n        self.grad = compute_grad(self.g)              # gradient of input function\n\n        wmax = 1\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\'] + 0.5\n\n        view = [20,-50]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n\n        axes = False\n        if \'axes\' in kwargs:\n            axes = kwargs[\'axes\']\n\n        plot_final = False\n        if \'plot_final\' in kwargs:\n            plot_final = kwargs[\'plot_final\']\n\n        num_contours = 10\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']\n\n        # version of gradient descent to use (normalized or unnormalized)\n        self.version = \'unnormalized\'\n        if \'version\' in kwargs:\n            self.version = kwargs[\'version\']\n            \n        # get initial point \n        self.w_init = np.asarray([float(s) for s in w_init])\n                    \n        # take in user defined step length\n        self.steplength = steplength\n            \n        # take in user defined maximum number of iterations\n        self.max_its = max_its\n            \n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (11,3))\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,10]) \n        ax = plt.subplot(gs[1],projection=\'3d\'); \n        ax2 = plt.subplot(gs[2],aspect=\'equal\'); \n        \n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n\n        #### define input space for function and evaluate ####\n        w = np.linspace(-wmax,wmax,200)\n        w1_vals, w2_vals = np.meshgrid(w,w)\n        w1_vals.shape = (len(w)**2,1)\n        w2_vals.shape = (len(w)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([g(s) for s in h])\n        w1_vals.shape = (len(w),len(w))\n        w2_vals.shape = (len(w),len(w))\n        func_vals.shape = (len(w),len(w))\n\n        # plot function \n        ax.plot_surface(w1_vals, w2_vals, func_vals, alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)\n\n        # plot z=0 plane \n        ax.plot_surface(w1_vals, w2_vals, func_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        ax2.contour(w1_vals, w2_vals, func_vals,num_contours,colors = \'k\')\n        if axes == True:\n            ax2.axhline(linestyle = \'--\', color = \'k\',linewidth = 1)\n            ax2.axvline(linestyle = \'--\', color = \'k\',linewidth = 1)\n\n        #### run local random search algorithm ####\n        self.w_hist = []\n        self.run_gradient_descent()\n\n        # colors for points\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        colorspec = []\n        colorspec = np.concatenate((s,np.flipud(s)),1)\n        colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n\n        #### scatter path points ####\n        for k in range(len(self.w_hist)):\n            w_now = self.w_hist[k]\n            ax.scatter(w_now[0],w_now[1],0,s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 0.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n\n            ax2.scatter(w_now[0],w_now[1],s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 1.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n\n        #### connect points with arrows ####\n        if len(self.w_hist) < 10:\n            for i in range(len(self.w_hist)-1):\n                pt1 = self.w_hist[i]\n                pt2 = self.w_hist[i+1]\n\n                # draw arrow in left plot\n                a = Arrow3D([pt1[0],pt2[0]], [pt1[1],pt2[1]], [0, 0], mutation_scale=10, lw=2, arrowstyle=""-|>"", color=""k"")\n                ax.add_artist(a)\n\n                # draw 2d arrow in right plot\n                ax2.arrow(pt1[0],pt1[1],(pt2[0] - pt1[0])*0.78,(pt2[1] - pt1[1])*0.78, head_width=0.1, head_length=0.1, fc=\'k\', ec=\'k\',linewidth=3,zorder = 2,length_includes_head=True)\n\n        ### cleanup panels ###\n        ax.set_xlabel(\'$w_1$\',fontsize = 12)\n        ax.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n        ax.set_title(\'$g(w_1,w_2)$\',fontsize = 12)\n        ax.view_init(view[0],view[1])\n\n        ax2.set_xlabel(\'$w_1$\',fontsize = 12)\n        ax2.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n        ax2.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax2.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n\n        # clean up axis\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        # plot\n        plt.show()\n        \n    # compare normalized and unnormalized grad descent on 3d example\n    def compare_versions_3d(self,g,w_init,steplength,max_its,**kwargs):\n        ### input arguments ###        \n        self.g = g\n        self.steplength = steplength\n        self.max_its = max_its\n        self.grad = compute_grad(self.g)              # gradient of input function\n\n        wmax = 1\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\'] + 0.5\n\n        view = [20,-50]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n\n        axes = False\n        if \'axes\' in kwargs:\n            axes = kwargs[\'axes\']\n\n        plot_final = False\n        if \'plot_final\' in kwargs:\n            plot_final = kwargs[\'plot_final\']\n\n        num_contours = 10\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']\n            \n        # get initial point \n        self.w_init = np.asarray([float(s) for s in w_init])\n                    \n        # take in user defined step length\n        self.steplength = steplength\n            \n        # take in user defined maximum number of iterations\n        self.max_its = max_its\n            \n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (12,6))\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(2, 3, width_ratios=[1,5,10]) \n        ax3 = plt.subplot(gs[1],projection=\'3d\'); \n        ax4 = plt.subplot(gs[2],aspect=\'equal\'); \n        ax5 = plt.subplot(gs[4],projection=\'3d\'); \n        ax6 = plt.subplot(gs[5],aspect=\'equal\'); \n        \n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        \n        #### define input space for function and evaluate ####\n        w = np.linspace(-wmax,wmax,200)\n        w1_vals, w2_vals = np.meshgrid(w,w)\n        w1_vals.shape = (len(w)**2,1)\n        w2_vals.shape = (len(w)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([g(s) for s in h])\n        w1_vals.shape = (len(w),len(w))\n        w2_vals.shape = (len(w),len(w))\n        func_vals.shape = (len(w),len(w))\n\n        #### run local random search algorithms ####\n        for algo in [\'normalized\',\'unnormalized\']:\n            # switch normalized / unnormalized\n            self.version = algo\n            title = \'\'\n            if self.version == \'normalized\':\n                ax = ax3\n                ax2 = ax4\n                title = \'normalized gradient descent\'\n            else:\n                ax = ax5\n                ax2 = ax6\n                title = \'unnormalized gradient descent\'\n            \n           # plot function \n            ax.plot_surface(w1_vals, w2_vals, func_vals, alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)\n\n            # plot z=0 plane \n            ax.plot_surface(w1_vals, w2_vals, func_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n\n            ### make contour right plot - as well as horizontal and vertical axes ###\n            ax2.contour(w1_vals, w2_vals, func_vals,num_contours,colors = \'k\')\n            if axes == True:\n                ax2.axhline(linestyle = \'--\', color = \'k\',linewidth = 1)\n                ax2.axvline(linestyle = \'--\', color = \'k\',linewidth = 1)\n            \n            self.w_hist = []\n            self.run_gradient_descent()\n\n            # colors for points\n            s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n            s.shape = (len(s),1)\n            t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n            t.shape = (len(t),1)\n            s = np.vstack((s,t))\n            colorspec = []\n            colorspec = np.concatenate((s,np.flipud(s)),1)\n            colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n\n            #### scatter path points ####\n            for k in range(len(self.w_hist)):\n                w_now = self.w_hist[k]\n                ax.scatter(w_now[0],w_now[1],0,s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 0.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n\n                ax2.scatter(w_now[0],w_now[1],s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 1.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n\n            #### connect points with arrows ####\n            if len(self.w_hist) < 10:\n                for i in range(len(self.w_hist)-1):\n                    pt1 = self.w_hist[i]\n                    pt2 = self.w_hist[i+1]\n        \n                    # draw arrow in left plot\n                    a = Arrow3D([pt1[0],pt2[0]], [pt1[1],pt2[1]], [0, 0], mutation_scale=10, lw=2, arrowstyle=""-|>"", color=""k"")\n                    ax.add_artist(a)\n\n                    # draw 2d arrow in right plot\n                    ax2.arrow(pt1[0],pt1[1],(pt2[0] - pt1[0])*0.78,(pt2[1] - pt1[1])*0.78, head_width=0.1, head_length=0.1, fc=\'k\', ec=\'k\',linewidth=3,zorder = 2,length_includes_head=True)\n\n            ### cleanup panels ###\n            ax.set_xlabel(\'$w_1$\',fontsize = 12)\n            ax.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n            ax.set_title(title,fontsize = 12)\n            ax.view_init(view[0],view[1])\n    \n            ax2.set_xlabel(\'$w_1$\',fontsize = 12)\n            ax2.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n            ax2.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            ax2.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n\n            # clean up axis\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        # plot\n        plt.show()      \n        \n        \n#### custom 3d arrow and annotator functions ###    \n# nice arrow maker from https://stackoverflow.com/questions/11140163/python-matplotlib-plotting-a-3d-cube-a-sphere-and-a-vector\nclass Arrow3D(FancyArrowPatch):\n\n    def __init__(self, xs, ys, zs, *args, **kwargs):\n        FancyArrowPatch.__init__(self, (0, 0), (0, 0), *args, **kwargs)\n        self._verts3d = xs, ys, zs\n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n        FancyArrowPatch.draw(self, renderer)'"
mlrefined_libraries/math_optimization_library/minimize_zero_find_simultaneous.py,28,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nfrom IPython.display import clear_output\nimport time\nimport copy\n\n\n# simple first order taylor series visualizer\nclass visualizer:\n    '''\n    Illustrate Newton's and Secant method for zero-finding with a customized slider mechanism\n    to let user control progression of algorithms.  Both function minimization and derivative\n    zero-finding side-by-side simultaneously.\n    ''' \n    def __init__(self,**args):\n        self.g = args['g']                             # input function\n        self.grad = compute_grad(self.g)               # first derivative of input function\n        self.hess = compute_grad(self.grad)            # second derivative of input function\n        self.w_init =float( -3)                        # user-defined initial point\n        self.max_its = 20\n     \n    ######## newton's method ########\n    # run newton's method\n    def run_newtons_method(self):\n        w = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w)\n        w_old = np.inf\n        j = 0\n        while (w_old - w)**2 > 10**-5 and j < self.max_its:\n            # update old w and index\n            w_old = w\n            j+=1\n            \n            # plug in value into func and derivative\n            grad_eval = float(self.grad(w))\n            hess_eval = float(self.hess(w))\n\n            # take newtons step\n            w = w - grad_eval/(hess_eval + 10**-5)\n            \n            # record\n            self.w_hist.append(w)\n\n    # animate the method\n    def draw_it_newtons(self,savepath,**kwargs):\n        # user-defined input point\n        if 'w_init' in kwargs:\n            self.w_init = float(kwargs['w_init'])\n            \n        # user-defined max_its\n        if 'max_its' in kwargs:\n            self.max_its = float(kwargs['max_its'])\n            \n        # initialize figure\n        fig = plt.figure(figsize = (12,4))\n        artist = fig\n        \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]);       \n        artist = fig\n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(-3.1,3.1,200)\n        g_plot = self.g(w_plot)\n        grad_plot = [self.grad(v) for v in w_plot]\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.1\n        w_vals = np.linspace(-2.5,2.5,50)\n        \n        # run newtons method\n        self.w_hist = []\n        self.run_newtons_method()\n        \n        # colors for points\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # animation sub-function\n        print ('beginning animation rendering...')\n        def animate(k):\n            ax1.cla()\n            ax2.cla()\n                        \n            # print rendering update\n            if k == len(self.w_hist):\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            # plot functions\n            ax1.plot(w_plot,g_plot,color = 'k',zorder = 0)               # plot function\n            ax2.plot(w_plot,grad_plot,color = 'k',zorder = 2)                           # plot function\n            \n            # plot initial point and evaluation\n            if k == 0:\n                w_val = self.w_init\n                g_val = self.g(w_val)\n                ax1.scatter(w_val,g_val,s = 120,c = 'w',edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                ax1.scatter(w_val,0,s = 120,c = 'w',edgecolor = 'k',linewidth = 0.7, zorder = 2, marker = 'X')\n\n                g_val = self.grad(w_val)\n                ax2.scatter(w_val,g_val,s = 120,c = 'w',edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                ax2.scatter(w_val,0,s = 120,c = 'w',edgecolor = 'k',linewidth = 0.7, zorder = 2, marker = 'X')\n            \n            # draw functions first, then start animating process\n            if k > 0:\n                #### cost function (minimizing) view ####\n                w_val = self.w_hist[k-1]\n\n                # plug in value into func and derivative\n                g_val = self.g(w_val)\n                g_grad_val = self.grad(w_val)\n                g_hess_val = self.hess(w_val)\n\n                # determine width of plotting area for second order approximator\n                width = 5\n                if g_hess_val < 0:\n                    width = - width\n\n                # setup quadratic formula params\n                a = 0.5*g_hess_val\n                b = g_grad_val - 2*0.5*g_hess_val*w_val\n                c = 0.5*g_hess_val*w_val**2 - g_grad_val*w_val - width\n\n                # solve for zero points\n                w1 = (-b + math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n                w2 = (-b - math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n\n                # compute second order approximation\n                wrange = np.linspace(w1,w2, 100)\n                h = g_val + g_grad_val*(wrange - w_val) + 0.5*g_hess_val*(wrange - w_val)**2 \n\n                # create next point information\n                w_zero = w_val - g_grad_val/(g_hess_val + 10**-5)\n                g_zero = self.g(w_zero)\n                h_zero = g_val + g_grad_val*(w_zero - w_val) + 0.5*g_hess_val*(w_zero - w_val)**2\n\n                # draw dashed linen connecting the three\n                vals = [0,h_zero,g_zero]\n                vals = np.sort(vals)\n                s = np.linspace(vals[0],vals[2])\n                o = np.ones((len(s)))\n\n                # plot all\n                ax1.plot(wrange,h,color = self.colorspec[k-1],linewidth = 2,zorder = 1)      # plot approx\n\n                # plot tangent point\n                ax1.scatter(w_val, g_val, s = 120, c='w',edgecolor = 'k',linewidth = 1,zorder = 3)\n\n                # created dashed linen connecting the three            \n                ax1.plot(o*w_zero,s,'k--',linewidth=1)\n\n                # draw intersection at zero and associated point on cost function you hop back too\n                # ax1.scatter(w_zero,h_zero,s = 120,c = 'k', zorder = 2)\n                ax1.scatter(w_zero,g_zero,s = 120,c = self.colorspec[k-1],edgecolor = 'k',linewidth = 1,zorder = 3)            # plot point of tangency\n                ax1.scatter(w_zero,0,s = 120,facecolor = self.colorspec[k-1],marker = 'X',edgecolor = 'k',linewidth = 1, zorder = 2)\n\n                #### derivative (zero-crossing) view ####\n                # grab historical weight, compute function and derivative evaluations\n                g_val = float(self.grad(w_val))\n                grad_val = float(self.hess(w_val))\n                h = g_val + grad_val*(wrange - w_val)\n\n                # draw points\n                w_zero = -g_val/grad_val + w_val\n                g_zero = self.grad(w_zero)\n                s = np.linspace(0,g_zero)\n                o = np.ones((len(s)))\n\n                # plot tangent line\n                ax2.plot(wrange,h,color = self.colorspec[k-1],linewidth = 2,zorder = 1)      # plot approx\n\n                # plot tangent point\n                ax2.scatter(w_val, g_val, s = 120, c='w',edgecolor = 'k',linewidth = 1,zorder = 3)\n\n                # draw dashed lines to highlight zero crossing point\n                ax2.plot(o*w_zero,s,'k--',linewidth=1)\n\n                # draw intersection at zero and associated point on cost function you hop back too\n                ax2.scatter(w_zero,g_zero,s = 120,c = self.colorspec[k-1],edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                ax2.scatter(w_zero,0,s = 120,facecolor = self.colorspec[k-1],marker = 'X',edgecolor = 'k',linewidth = 0.7, zorder = 2)\n             \n            # fix viewing limits\n            ax1.set_xlim([-3,3])\n            ax1.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n                \n            # fix viewing limits\n            ax2.set_xlim([-3,3])\n            ax2.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n\n            # set titles\n            ax1.set_title('cost function (minimizing) view',fontsize = 15)\n            ax2.set_title('gradient (zero-crossing) view',fontsize = 15)\n   \n            # draw axes\n            ax1.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n            ax2.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate,frames=len(self.w_hist)+1, interval=len(self.w_hist)+1, blit=True)\n\n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()\n\n    ######## secant method #########\n    # run secant method\n    def run_secant_method(self):\n        # get initial point\n        w2 = self.w_init\n        \n        # create second point nearby w_old\n        w1 = w2 - 0.5\n        g2 = self.g(w2)\n        g1 = self.g(w1)\n        if g1 > g2:\n            w1 = w2 + 0.5\n        \n        # setup container for history\n        self.w_hist = []\n        self.w_hist.append(w2)\n        self.w_hist.append(w1)\n        \n        # start loop\n        w_old = np.inf\n        j = 0\n        while abs(w1 - w2) > 10**-5 and j < self.max_its:  \n            # plug in value into func and derivative\n            g1 = float(self.grad(w1))\n            g2 = float(self.grad(w2))\n                        \n            # take newtons step\n            w = w1 - g1*(w1 - w2)/(g1 - g2 + 10**-6)\n            \n            # record\n            self.w_hist.append(w)\n            \n            # update old w and index\n            j+=1\n            w2 = w1\n            w1 = w\n    \n    # animate the method\n    def draw_it_secant(self,savepath,**kwargs):\n        # user-defined input point\n        if 'w_init' in kwargs:\n            self.w_init = float(kwargs['w_init'])\n        \n        # user-defined max_its\n        if 'max_its' in kwargs:\n            self.max_its = float(kwargs['max_its'])\n            \n        # initialize figure\n        fig = plt.figure(figsize = (12,4))\n        \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]); \n\n        artist = fig\n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(-3.1,3.1,200)\n        g_plot = self.g(w_plot)\n        grad_plot = [self.grad(v) for v in w_plot]\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.1\n        w_vals = np.linspace(-2.5,2.5,50)\n        width = 5\n        \n        # run newtons method\n        self.w_hist = []\n        self.run_secant_method()\n    \n        # colors for points\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # animation sub-function\n        print ('beginning animation rendering...')\n        def animate(k):\n            ax1.cla()\n            ax2.cla()\n            \n            # print rendering update\n            if k == len(self.w_hist)-1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            # plot functions\n            ax1.plot(w_plot,g_plot,color = 'k',zorder = 0)               # plot function\n            ax2.plot(w_plot,grad_plot,color = 'k',zorder = 2)                           # plot function\n\n            # plot initial point and evaluation\n            if k == 0:\n                w_val = self.w_init\n                g_val = self.g(w_val)\n                ax1.scatter(w_val,g_val,s = 120,c = 'w',edgecolor = 'k',linewidth = 1.0,zorder = 3)            # plot point of tangency\n                ax1.scatter(w_val,0,s = 120,c = 'w',edgecolor = 'k',linewidth = 1.0, zorder = 2, marker = 'X')\n\n                g_val = self.grad(w_val)\n                ax2.scatter(w_val,g_val,s = 120,c = 'w',edgecolor = 'k',linewidth = 1.0,zorder = 3)            # plot point of tangency\n                ax2.scatter(w_val,0,s = 120,c = 'w',edgecolor = 'k',linewidth = 1.0, zorder = 2, marker = 'X')\n            \n            # plot functions first for one slide\n            if k > 0:\n                #### cost function (minimizing) view ####\n                # grab historical weights, form associated secant line\n                w2 = self.w_hist[k-1]\n                w1 = self.w_hist[k]\n                g2 = self.g(w2)\n                g1 = self.g(w1)\n                grad2 = self.grad(w2)\n                grad1 = self.grad(w1)\n\n                # determine width of plotting area for second order approximator\n                width = 5\n                g_hess_val = (grad1 - grad2)/(w1 - w2)\n                if g_hess_val < 0:\n                    width = - width\n\n                # setup quadratic formula params\n                a = 0.5*g_hess_val\n                b = grad1 - 2*0.5*g_hess_val*w1\n                c = 0.5*g_hess_val*w1**2 - grad1*w1 - width\n\n                # solve for zero points\n                wa = (-b + math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n                wb = (-b - math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n\n                # compute second order approximation\n                wrange = np.linspace(wa,wb, 100)\n                h = g1 + grad1*(wrange - w1) + 0.5*g_hess_val*(wrange - w1)**2 \n\n                # create next point information\n                w_zero = w1 - grad1/(g_hess_val + 10**-5)\n                g_zero = self.g(w_zero)\n                h_zero = g1 + grad1*(w_zero - w1) + 0.5*g_hess_val*(w_zero - w1)**2\n\n                # draw dashed linen connecting the three\n                vals = [0,h_zero,g_zero]\n                vals = np.sort(vals)\n                s = np.linspace(vals[0],vals[2])\n                o = np.ones((len(s)))\n\n                # plot all\n                ax1.plot(wrange,h,color =  self.colorspec[k-1] ,linewidth = 2,zorder = 1)      # plot approx\n                ax1.scatter(w1,g1,s = 120,c = 'w',edgecolor = 'k',linewidth = 1.0,zorder = 3)           # plot point of tangency\n                ax1.scatter(w2,g2,s = 120,c = 'w',edgecolor = 'k',linewidth = 1.0,zorder = 3)           # plot point of tangency\n                ax1.plot(o*w_zero,s,'k--',linewidth=1)\n\n                # draw intersection at zero and associated point on cost function you hop back too\n                # ax1.scatter(w_zero,h_zero,s = 120,c = 'k', zorder = 2)\n                ax1.scatter(w_zero,g_zero,s = 120,c = self.colorspec[k-1],edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                ax1.scatter(w_zero,0,s = 120,facecolor = self.colorspec[k-1],marker = 'X',edgecolor = 'k',linewidth = 0.7, zorder = 2)\n\n                #### derivative (zero-crossing) view ####\n                # grab historical weights, form associated secant line\n                w2 = self.w_hist[k-1]\n                w1 = self.w_hist[k]\n                g2 = self.grad(w2)\n                g1 = self.grad(w1)\n                m = (g1 - g2)/(w1 - w2)\n\n                # use point-slope form of line to plot\n                h = g1 + m*(wrange - w1)\n\n                # create dashed line and associated points\n                w_zero = -g1/m + w1\n                g_zero = self.grad(w_zero)\n                s = np.linspace(0,g_zero)\n                o = np.ones((len(s)))\n\n                # plot secant line \n                ax2.plot(wrange,h,color =  self.colorspec[k-1],linewidth = 2,zorder = 1)      # plot approx\n\n                # plot intersection points\n                ax2.scatter(w1,g1,s = 120,c = 'w',edgecolor = 'k',linewidth = 1.0,zorder = 3)           # plot point of tangency\n                ax2.scatter(w2,g2,s = 120,c = 'w',edgecolor = 'k',linewidth = 1.0,zorder = 3)           # plot point of tangency\n\n                # plot dashed line\n                ax2.plot(o*w_zero,s,'k--',linewidth=1)\n\n                # draw zero intersection, and associated point on cost function you hop back too\n                ax2.scatter(w_zero,g_zero,s = 120,c = self.colorspec[k-1],edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                ax2.scatter(w_zero,0,s = 120,facecolor = self.colorspec[k-1],marker = 'X',edgecolor = 'k',linewidth = 0.7, zorder = 2)\n\n            # fix viewing limits\n            ax1.set_xlim([-3,3])\n            ax1.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            \n            ax2.set_xlim([-3.1,3.1])\n            ax2.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            \n            # draw axes\n            ax1.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n            ax2.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n                \n            # place title\n            ax1.set_title('cost function (minimizing) view',fontsize = 15)\n            ax2.set_title('gradient (zero-crossing) view',fontsize = 15)\n\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate,frames=len(self.w_hist), interval=len(self.w_hist), blit=True)\n\n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()"""
mlrefined_libraries/math_optimization_library/newton_secant_zero_finder.py,26,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nfrom IPython.display import clear_output\nimport time\nimport copy\n\nclass visualizer:\n    \'\'\'\n    Illustrate Newton\'s and Secant method for zero-finding with a customized slider mechanism\n    to let user control progression of algorithms.\n    \'\'\' \n    def __init__(self,**args):\n        self.g = args[\'g\']                            # input function\n        self.grad = compute_grad(self.g)              # gradient of input function\n        self.w_init =float( -3)                       # input initial point\n        self.w_hist = []\n        self.colorspec = []                           # container for colors --> when algorithm begins, colored green, as it ends color turns yellow, then red\n        \n    # run newton\'s method\n    def run_newtons(self):\n        w = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w)\n        w_old = np.inf\n        j = 0\n        while (w_old - w)**2 > 10**-4 and j < 20:\n            # update old w and index\n            w_old = w\n            j+=1\n            \n            # plug in value into func and derivative\n            g_eval = self.g(w)\n            grad_eval = float(self.grad(w))\n            \n            # take newtons step\n            w = w - g_eval/(grad_eval + 10**-4)\n            \n            # record\n            self.w_hist.append(w)\n\n    # animate the method\n    def draw_it_newton(self,savepath,**kwargs):\n        if \'w_init\' in kwargs:\n            self.w_init = float(kwargs[\'w_init\'])\n        \n        # initialize figure\n        fig = plt.figure(figsize = (4,4))\n        artist = fig\n        ax = fig.add_subplot(111)\n        \n        # run newtons method and collect path history\n        self.w_hist = []\n        self.run_newtons()\n        \n        # set viewing range\n        wmax = max([v for v in self.w_hist])\n        wmin = min([v for v in self.w_hist])\n        wgap = (wmax - wmin)*0.5\n        wmax += wgap\n        wmin -= wgap\n        \n        w_plot = np.linspace(wmin,wmax,200)\n        g_plot = self.g(w_plot)\n        width = 30\n        \n        # set range for function plotting\n        w_plot1 = np.linspace(-3,3)\n        g_plot1 = self.g(w_plot1)\n        gmin = min(copy.deepcopy(g_plot1))\n        gmax = max(copy.deepcopy(g_plot1))\n        ggap = (gmax - gmin)*0.2\n        gmin -= ggap\n        gmax += ggap\n        \n        # colors for points\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # animation sub-function\n        print (\'beginning animation rendering...\')\n        def animate(k):\n            ax.cla()\n            \n            # print rendering update\n            if k == len(self.w_hist):\n                print (\'animation rendering complete!\')\n                time.sleep(1.5)\n                clear_output()\n \n            # plot function\n            ax.plot(w_plot1,g_plot1,color = \'k\',zorder = 1)                           # plot function\n\n            # plot all input/output pairs generated by algorithm thus far\n            if k > 0:\n                # plot all points up to this point\n                for j in range(0,min(k+1,len(self.w_hist))):  \n                    w_val = self.w_hist[j]\n                    g_val = self.g(w_val)\n                    \n\n                    if j == k-1:\n                        # draw guide line to visua\n                        s = np.linspace(0,g_val)\n                        o = np.ones((len(s)))\n                        ax.plot(o*w_val,s,\'k--\',linewidth=1,zorder = 1)\n                        \n                        ax.scatter(w_val,g_val,s = 90,c = self.colorspec[j],edgecolor = \'k\',linewidth = 1,zorder = 3)            # plot point of tangency\n                        ax.scatter(w_val,0,s = 90,facecolor = self.colorspec[j],marker = \'X\',edgecolor = \'k\',linewidth = 1, zorder = 2)\n                     \n                    if j == k:\n                        # draw guide line to visua\n                        s = np.linspace(0,g_val)\n                        o = np.ones((len(s)))\n                        ax.plot(o*w_val,s,\'k--\',linewidth=1,zorder = 1)\n                        \n                        ax.scatter(w_val,g_val,s = 90,c = \'w\',edgecolor = \'k\',linewidth = 1,zorder = 3)            # plot point of tangency\n                        ax.scatter(w_val,0,s = 90,facecolor = \'w\',marker = \'X\',edgecolor = \'k\',linewidth = 1, zorder = 2)\n\n            # plot surrogate function and travel-to point\n            if k > 0 and k < len(self.w_hist) + 1:\n                # grab historical weight, compute function and derivative evaluations\n                w = self.w_hist[k-1]\n                g_eval = self.g(w)\n                grad_eval = float(self.grad(w))\n\n                # determine width to plot the approximation -- so its length == width defined above\n                div = float(1 + grad_eval**2)\n                w1 = w - math.sqrt(width/div)\n                w2 = w + math.sqrt(width/div)\n\n                # use point-slope form of line to plot\n                wrange = np.linspace(w1,w2, 100)\n                h = g_eval + grad_eval*(wrange - w)\n\n                # plot tangent line\n                ax.plot(wrange,h,color = self.colorspec[k-1],linewidth = 2,zorder = 1)      # plot approx\n\n                # plot tangent point\n                #ax.scatter(w,g_eval,s = 100,c = \'m\',edgecolor = \'k\',linewidth = 0.7,zorder = 2)            # plot point of tangency\n\n                # plot go-too line on surrogate\n                w_zero = -g_eval/grad_eval + w\n                #ax.scatter(w_zero,0,s = 100,c = \'m\',edgecolor = \'k\',linewidth = 0.7, zorder = 2, marker = \'X\')\n                \n                \'\'\'\n                # plot next point learned from surrogate\n                if k > 0:\n                    # draw dashed lines to highlight zero crossing point\n                    g_zero = self.g(w_zero)\n\n                    s = np.linspace(0,g_zero)\n                    o = np.ones((len(s)))\n                    ax.plot(o*w_zero,s,\'k--\',linewidth=1,zorder = 1)\n\n                    \n                    # draw associated point on cost function you hop back too\n                    #ax.scatter(w_zero,g_zero,s = 100,c = \'m\',edgecolor = \'k\',linewidth = 0.7,zorder = 2)            # plot point of tangency\n               \'\'\'\n            # fix viewing limits\n            ax.set_xlim([wmin,wmax])\n            ax.set_ylim([gmin,gmax])\n\n            # draw axes\n            # ax.grid(True, which=\'both\')\n            ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            # ax.axvline(x=0, color=\'k\')\n\n            # place title\n            ax.set_title(""Newton\'s method (zero finding)"",fontsize = 12)\n            \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=len(self.w_hist)+1, interval=len(self.w_hist)+1, blit=True)\n\n        # produce animation and save\n        fps = 50\n        if \'fps\' in kwargs:\n            fps = kwargs[\'fps\']\n        anim.save(savepath, fps=fps, extra_args=[\'-vcodec\', \'libx264\'])\n        clear_output()\n\n    # secant method\n    def run_secant(self):\n        # get initial point\n        w2 = self.w_init\n        \n        # create second point nearby w_old\n        w1 = w2 - 0.5\n        g2 = self.g(w2)\n        g1 = self.g(w1)\n        if g1 > g2:\n            w1 = w2 + 0.5\n        \n        # setup container for history\n        self.w_hist = []\n        self.w_hist.append(w2)\n        self.w_hist.append(w1)\n        \n        # start loop\n        w_old = np.inf\n        j = 0\n        while abs(w1 - w2) > 10**-5 and j < 20:  \n            # plug in value into func and derivative\n            g1 = float(self.g(w1))\n            g2 = float(self.g(w2))\n                        \n            # take newtons step\n            w = w1 - g1*(w1 - w2)/(g1 - g2 + 10**-4)\n            \n            # record\n            self.w_hist.append(w)\n            \n            # update old w and index\n            j+=1\n            w2 = w1\n            w1 = w\n\n    # animate the method\n    def draw_it_secant(self,**args):\n        if \'w_init\' in args:\n            self.w_init = float(args[\'w_init\'])\n            \n        # initialize figure\n        fig = plt.figure(figsize = (6,6))\n        artist = fig\n        ax = fig.add_subplot(111)\n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(-3.1,3.1,200)\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.1\n        width = 30\n        \n        # run newtons method\n        self.w_hist = []\n        self.run_secant()\n        \n        # colors for points\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # animation sub-function\n        print (\'beginning animation rendering...\')\n        def animate(t):\n            ax.cla()\n            k = math.floor((t+1)/float(2))\n            \n            # print rendering update\n            if k == 2*len(self.w_hist)-1:\n                print (\'animation rendering complete!\')\n                time.sleep(1.5)\n                clear_output()\n            \n            # plot function\n            ax.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n           \n            # plot initial point and evaluation\n            if k == 0:\n                w_val = self.w_init\n                g_val = self.g(w_val)\n                ax.scatter(w_val,g_val,s = 100,c = \'m\',edgecolor = \'k\',linewidth = 0.7,zorder = 2)            # plot point of tangency\n                ax.scatter(w_val,0,s = 100,c = \'m\',edgecolor = \'k\',linewidth = 0.7, zorder = 2, marker = \'X\')\n\n            # plot all input/output pairs generated by algorithm thus far\n            if k > 0:\n                # plot all points up to this point\n                for j in range(min(k-1,len(self.w_hist))):  \n                    w_val = self.w_hist[j]\n                    g_val = self.g(w_val)\n                    \n                    ax.scatter(w_val,g_val,s = 90,c = self.colorspec[j],edgecolor = \'k\',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                    ax.scatter(w_val,0,s = 90,facecolor = self.colorspec[j],marker = \'X\',edgecolor = \'k\',linewidth = 0.7, zorder = 2)\n\n            # plot surrogate function and travel-to point\n            if k > 0 and k < len(self.w_hist):\n            \n                # grab historical weights, form associated secant line\n                w2 = self.w_hist[k-1]\n                w1 = self.w_hist[k]\n                g2 = self.g(w2)\n                g1 = self.g(w1)\n                m = (g1 - g2)/(w1 - w2)\n            \n                # determine width to plot the approximation -- so its length == width defined above\n                div = float(1 + m**2)\n                wa = w1 - math.sqrt(width/div)\n                wb = w1 + math.sqrt(width/div)\n            \n                # use point-slope form of line to plot\n                wrange = np.linspace(wa,wb, 100)\n                h = g1 + m*(wrange - w1)\n\n                # plot secant line \n                ax.plot(wrange,h,color = \'b\',linewidth = 2,zorder = 1)      # plot approx\n\n                # plot intersection points\n                ax.scatter(w2, g2, s = 100, c=\'m\',edgecolor = \'k\',linewidth = 0.7,zorder = 3)\n                ax.scatter(w1, g1, s = 100, c=\'m\',edgecolor = \'k\',linewidth = 0.7,zorder = 3)\n\n                # plot next point learned from surrogate\n                if np.mod(t,2) == 0:\n          \n                    # draw dashed lines to highlight zero crossing point\n                    w_zero = -g1/m + w1\n                    g_zero = self.g(w_zero)\n                    s = np.linspace(0,g_zero)\n                    o = np.ones((len(s)))\n                    ax.plot(o*w_zero,s,\'k--\',linewidth=1,zorder = 1)\n\n                    # draw zero intersection, and associated point on cost function you hop back too\n                    ax.scatter(w_zero,g_zero,s = 100,c = \'m\',edgecolor = \'k\',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                    ax.scatter(w_zero,0,s = 100,c = \'m\',edgecolor = \'k\',linewidth = 0.7, zorder = 3, marker = \'X\')\n\n            # fix viewing limits\n            ax.set_xlim([-3.1,3.1])\n            ax.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n\n            # draw axes\n            # ax.grid(True, which=\'both\')\n            ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            # ax.axvline(x=0, color=\'k\',linewidth = 0.5)\n            \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=2*len(self.w_hist), interval=2*len(self.w_hist), blit=True)\n\n        return(anim)'"
mlrefined_libraries/math_optimization_library/newtons_method_demos.py,42,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   \nfrom autograd import hessian as compute_hess\n\nimport autograd.numpy as np\nimport math\nfrom IPython.display import clear_output\nimport time\nfrom matplotlib import gridspec\n\n# simple first order taylor series visualizer\nclass visualizer:\n    '''\n    Illustrate gradient descent, Newton method, and Secant method for minimizing an input function, illustrating\n    surrogate functions at each step.  A custom slider mechanism is used to progress each algorithm, and points are\n    colored from green at the start of an algorithm, to yellow as it converges, and red as the final point.\n    ''' \n    def __init__(self,**args):\n        self.g = args['g']                            # input function\n        self.grad = compute_grad(self.g)              # gradient of input function\n        self.hess = compute_hess(self.g)           # hessian of input function\n        self.w_init =float( -2)                       # user-defined initial point (adjustable when calling each algorithm)\n        self.alpha = 10**-4                           # user-defined step length for gradient descent (adjustable when calling gradient descent)\n        self.max_its = 20                             # max iterations to run for each algorithm\n        self.w_hist = []                              # container for algorithm path\n        self.colors = [[0,1,0.25],[0,0.75,1]]    # set of custom colors used for plotting\n        self.beta = 0\n\n    ######## newton's method ########\n    # run newton's method\n    def run_newtons_method(self):\n        w = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w)\n        for k in range(self.max_its):\n            # plug in value into func and derivative\n            grad_eval = self.grad(w)\n            hess_eval = self.hess(w)\n            \n            # reshape for numpy linalg functionality\n            hess_eval.shape = (int((np.size(hess_eval))**(0.5)),int((np.size(hess_eval))**(0.5)))\n\n            # solve linear system for weights\n            w = w - np.dot(np.linalg.pinv(hess_eval + self.beta*np.eye(np.size(w))),grad_eval)\n                                \n            # record\n            self.w_hist.append(w)\n\n    # animate the method\n    def animate_it(self,**kwargs):\n        # get new initial point if desired\n        if 'w_init' in kwargs:\n            self.w_init = float(kwargs['w_init'])\n            \n        # take in user defined maximum number of iterations\n        if 'max_its' in kwargs:\n            self.max_its = int(kwargs['max_its'])\n            \n        wmax = 3\n        if 'wmax' in kwargs:\n            wmax = kwargs['wmax']\n        wmin = -wmax\n        if 'wmin' in kwargs:\n            wmin = kwargs['wmin']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (10,5))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5, 1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis('off');\n        ax3 = plt.subplot(gs[2]); ax3.axis('off');\n\n        # plot input function\n        ax = plt.subplot(gs[1],aspect = 'equal')\n        \n        # generate function for plotting on each slide\n        w_plot = np.linspace(wmin,wmax,1000)\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.1\n        w_vals = np.linspace(-2.5,2.5,50)\n        width = 1\n        \n        # run newtons method\n        self.w_hist = []\n        self.run_newtons_method()\n        \n        # colors for points --> green as the algorithm begins, yellow as it converges, red at final point\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # animation sub-function\n        print ('starting animation rendering...')\n        num_frames = 2*len(self.w_hist)+2\n        def animate(t):\n            ax.cla()\n            k = math.floor((t+1)/float(2))\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if t == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n\n            # plot function\n            ax.plot(w_plot,g_plot,color = 'k',zorder = 1)                           # plot function\n            \n            # plot initial point and evaluation\n            if k == 0:\n                w_val = self.w_init\n                g_val = self.g(w_val)\n                ax.scatter(w_val,g_val,s = 100,c = 'm',edgecolor = 'k',linewidth = 0.7, marker = 'X',zorder = 2)            # plot point of tangency\n                ax.scatter(w_val,0,s = 100,c = 'm',edgecolor = 'k',linewidth = 0.7, zorder = 2)\n                # draw dashed line connecting w axis to point on cost function\n                s = np.linspace(0,g_val)\n                o = np.ones((len(s)))\n                ax.plot(o*w_val,s,'k--',linewidth=1,zorder = 0)\n                \n            # plot all input/output pairs generated by algorithm thus far\n            if k > 0:\n                # plot all points up to this point\n                for j in range(min(k-1,len(self.w_hist))):  \n                    w_val = self.w_hist[j]\n                    g_val = self.g(w_val)\n                    ax.scatter(w_val,g_val,s = 90,c = self.colorspec[j],edgecolor = 'k',marker = 'X',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                    ax.scatter(w_val,0,s = 90,facecolor = self.colorspec[j],edgecolor = 'k',linewidth = 0.7, zorder = 2)\n                          \n            # plot surrogate function and travel-to point\n            if k > 0 and k < len(self.w_hist) + 1:          \n                # grab historical weight, compute function and derivative evaluations    \n                w_eval = self.w_hist[k-1]\n                if type(w_eval) != float:\n                    w_eval = float(w_eval[0][0])\n\n                # plug in value into func and derivative\n                g_eval = self.g(w_eval)\n                g_grad_eval = self.grad(w_eval)\n                g_hess_eval = self.hess(w_eval)\n\n                # determine width of plotting area for second order approximator\n                width = 0.5\n                if g_hess_eval < 0:\n                    width = - width\n\n                # setup quadratic formula params\n                a = 0.5*g_hess_eval\n                b = g_grad_eval - 2*0.5*g_hess_eval*w_eval\n                c = 0.5*g_hess_eval*w_eval**2 - g_grad_eval*w_eval - width\n\n                # solve for zero points\n                w1 = (-b + math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n                w2 = (-b - math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n\n                # compute second order approximation\n                wrange = np.linspace(w1,w2, 100)\n                h = g_eval + g_grad_eval*(wrange - w_eval) + 0.5*g_hess_eval*(wrange - w_eval)**2 \n\n                # plot tangent line\n                ax.plot(wrange,h,color = self.colors[1],linewidth = 2,zorder = 2)      # plot approx\n\n                # plot tangent point\n                ax.scatter(w_eval,g_eval,s = 100,c = 'm',edgecolor = 'k', marker = 'X',linewidth = 0.7,zorder = 3)            # plot point of tangency\n            \n                # plot next point learned from surrogate\n                if np.mod(t,2) == 0:\n                    # create next point information\n                    w_zero = w_eval - g_grad_eval/(g_hess_eval + 10**-5)\n                    g_zero = self.g(w_zero)\n                    h_zero = g_eval + g_grad_eval*(w_zero - w_eval) + 0.5*g_hess_eval*(w_zero - w_eval)**2\n\n                    # draw dashed line connecting the three\n                    vals = [0,h_zero,g_zero]\n                    vals = np.sort(vals)\n\n                    s = np.linspace(vals[0],vals[2])\n                    o = np.ones((len(s)))\n                    ax.plot(o*w_zero,s,'k--',linewidth=1)\n\n                    # draw intersection at zero and associated point on cost function you hop back too\n                    ax.scatter(w_zero,h_zero,s = 100,c = 'b',linewidth=0.7, marker = 'X',edgecolor = 'k',zorder = 3)\n                    ax.scatter(w_zero,0,s = 100,c = 'm',edgecolor = 'k',linewidth = 0.7, zorder = 3)\n                    ax.scatter(w_zero,g_zero,s = 100,c = 'm',edgecolor = 'k',linewidth = 0.7, marker = 'X',zorder = 3)            # plot point of tangency\n            \n            # fix viewing limits on panel\n            ax.set_xlim([wmin,wmax])\n            ax.set_ylim([min(-0.3,min(g_plot) - ggap),max(max(g_plot) + ggap,0.3)])\n            \n            # add horizontal axis\n            ax.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n            \n            # label axes\n            ax.set_xlabel('$w$',fontsize = 12)\n            ax.set_ylabel('$g(w)$',fontsize = 12,rotation = 0,labelpad = 12)\n            \n            # set tickmarks\n            ax.set_xticks(np.arange(round(wmin), round(wmax) + 1, 1.0))\n            ax.set_yticks(np.arange(round(min(g_plot) - ggap), round(max(g_plot) + ggap) + 1, 1.0))\n\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n\n        return(anim)\n    \n    # visualize descent on multi-input function\n    def draw_it(self,w_init,max_its,**kwargs):\n        ### input arguments ###        \n        self.max_its = max_its\n        self.grad = compute_grad(self.g)              # gradient of input function\n        self.w_init = w_init\n        \n        if 'beta' in kwargs:\n            self.beta = kwargs['beta']\n            \n        pts = 'off'\n        if 'pts' in kwargs:\n            pts = 'off'\n            \n        linewidth = 2.5\n        if 'linewidth' in kwargs:\n            linewidth = kwargs['linewidth']\n            \n        view = [20,-50]\n        if 'view' in kwargs:\n            view = kwargs['view']\n\n        axes = False\n        if 'axes' in kwargs:\n            axes = kwargs['axes']\n\n        plot_final = False\n        if 'plot_final' in kwargs:\n            plot_final = kwargs['plot_final']\n\n        num_contours = 15\n        if 'num_contours' in kwargs:\n            num_contours = kwargs['num_contours']\n            \n        # get initial point \n        self.w_init = w_init\n        if np.size(self.w_init) == 2:\n            self.w_init = np.asarray([float(s) for s in self.w_init])\n        else:\n            self.w_init = np.asarray([float(self.w_init)])\n        \n        # take in user defined maximum number of iterations\n        self.max_its = max_its\n            \n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(9,4))\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[2,1]) \n        ax = plt.subplot(gs[0],aspect = 'equal'); \n        ax2 = plt.subplot(gs[1]) #  ,sharey = ax); \n\n        #### run local random search algorithm ####\n        self.w_hist = []\n        self.run_newtons_method()\n\n        # colors for points\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        colorspec = []\n        colorspec = np.concatenate((s,np.flipud(s)),1)\n        colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n    \n        #### define input space for function and evaluate ####\n        if np.size(self.w_init) == 2:           # function is multi-input, plot 3d function contour\n            # set viewing limits on contour plot\n            xvals = [self.w_hist[s][0] for s in range(len(self.w_hist))]\n            xvals.append(self.w_init[0])\n            yvals = [self.w_hist[s][1] for s in range(len(self.w_hist))]\n            yvals.append(self.w_init[1])\n            xmax = max(xvals)\n            xmin = min(xvals)\n            xgap = (xmax - xmin)*0.1\n            ymax = max(yvals)\n            ymin = min(yvals)\n            ygap = (ymax - ymin)*0.1\n            xmin -= xgap\n            xmax += xgap\n            ymin -= ygap\n            ymax += ygap\n\n            if 'xmin' in kwargs:\n                xmin = kwargs['xmin']\n            if 'xmax' in kwargs:\n                xmax = kwargs['xmax']\n            if 'ymin' in kwargs:\n                ymin = kwargs['ymin']\n            if 'ymax' in kwargs:\n                ymax = kwargs['ymax']  \n\n            w1 = np.linspace(xmin,xmax,400)\n            w2 = np.linspace(ymin,ymax,400)\n            w1_vals, w2_vals = np.meshgrid(w1,w2)\n            w1_vals.shape = (len(w1)**2,1)\n            w2_vals.shape = (len(w2)**2,1)\n            h = np.concatenate((w1_vals,w2_vals),axis=1)\n            func_vals = np.asarray([self.g(s) for s in h])\n            w1_vals.shape = (len(w1),len(w1))\n            w2_vals.shape = (len(w2),len(w2))\n            func_vals.shape = (len(w1),len(w2)) \n\n            ### make contour right plot - as well as horizontal and vertical axes ###\n            # set level ridges\n            num_contours = kwargs['num_contours']\n            levelmin = min(func_vals.flatten())\n            levelmax = max(func_vals.flatten())\n            cutoff = 0.5\n            cutoff = (levelmax - levelmin)*cutoff\n            numper = 3\n            levels1 = np.linspace(cutoff,levelmax,numper)\n            num_contours -= numper\n\n            levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n            levels = np.unique(np.append(levels1,levels2))\n            num_contours -= numper\n            while num_contours > 0:\n                cutoff = levels[1]\n                levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n                levels = np.unique(np.append(levels2,levels))\n                num_contours -= numper\n\n            a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = 'k')\n            ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = 'Blues')\n                \n            # plot points on contour\n            for j in range(len(self.w_hist)):  \n                w_val = self.w_hist[j]\n                g_val = self.g(w_val)\n\n                # plot in left panel\n                if pts == 'on':\n                    ax.scatter(w_val[0],w_val[1],s = 30,c = colorspec[j],edgecolor = 'k',linewidth = 1.5*math.sqrt((1/(float(j) + 1))),zorder = 3)\n\n                    ax2.scatter(j,g_val,s = 30,c = colorspec[j],edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n\n                # plot connector between points for visualization purposes\n                if j > 0:\n                    w_old = self.w_hist[j-1]\n                    w_new = self.w_hist[j]\n                    g_old = self.g(w_old)\n                    g_new = self.g(w_new)\n\n                    ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = colorspec[j],linewidth = linewidth,alpha = 1,zorder = 2)      # plot approx\n                    ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = 'k',linewidth = linewidth + 0.4,alpha = 1,zorder = 1)      # plot approx\n                    ax2.plot([j-1,j],[g_old,g_new],color = colorspec[j],linewidth = 2,alpha = 1,zorder = 2)      # plot approx\n                    ax2.plot([j-1,j],[g_old,g_new],color = 'k',linewidth = 2.5,alpha = 1,zorder = 1)      # plot approx\n            \n            # clean up panel\n            ax.set_xlabel('$w_1$',fontsize = 12)\n            ax.set_ylabel('$w_2$',fontsize = 12,rotation = 0,labelpad = 15)\n            ax.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n            ax.axvline(x=0, color='k',zorder = 0,linewidth = 0.5)\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            \n            # set tickmarks\n            ax.set_xticks(np.arange(round(xmin), round(xmax) + 1, 1.0))\n            ax.set_yticks(np.arange(round(ymin), round(ymax) + 1, 1.0))\n            \n        else:    # function is single input, plot curve\n            xmin = -2\n            xmax = 2\n            if 'xmin' in kwargs:\n                xmin = kwargs['xmin']\n            if 'xmax' in kwargs:\n                xmax = kwargs['xmax']\n                    \n            w_plot = np.linspace(xmin,xmax,500)\n            g_plot = np.asarray([self.g(s) for s in w_plot])\n            ax.plot(w_plot,g_plot,color = 'k',linewidth = 2,zorder = 2)\n                \n            # set viewing limits\n            ymin = min(g_plot)\n            ymax = max(g_plot)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap\n            ax.set_ylim([ymin,ymax])\n                \n            # clean up panel\n            ax.axhline(y=0, color='k',zorder = 1,linewidth = 0.25)\n            ax.axvline(x=0, color='k',zorder = 1,linewidth = 0.25)\n            ax.set_xlabel(r'$w$',fontsize = 13)\n            ax.set_ylabel(r'$g(w)$',fontsize = 13,rotation = 0,labelpad = 25)   \n                \n            # function single-input, plot input and evaluation points on function\n            for j in range(len(self.w_hist)):  \n                w_val = self.w_hist[j]\n                g_val = self.g(w_val)\n            \n                ax.scatter(w_val,g_val,s = 90,c = colorspec[j],edgecolor = 'k',linewidth = 0.5*((1/(float(j) + 1)))**(0.4),zorder = 3,marker = 'X')            # evaluation on function\n                ax.scatter(w_val,0,s = 90,facecolor = colorspec[j],edgecolor = 'k',linewidth = 0.5*((1/(float(j) + 1)))**(0.4), zorder = 3)\n                    \n                ax2.scatter(j,g_val,s = 30,c = colorspec[j],edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                    \n                # plot connector between points for visualization purposes\n                if j > 0:\n                    w_old = self.w_hist[j-1][0]\n                    w_new = self.w_hist[j][0]\n                    g_old = self.g(w_old)\n                    g_new = self.g(w_new)\n     \n                    ax2.plot([j-1,j],[g_old,g_new],color = colorspec[j],linewidth = 2,alpha = 1,zorder = 2)      # plot approx\n                    ax2.plot([j-1,j],[g_old,g_new],color = 'k',linewidth = 2.5,alpha = 1,zorder = 1)      # plot approx\n      \n\n        # clean panels\n        ax2.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n        ax2.set_xlabel('iteration',fontsize = 12)\n        ax2.set_ylabel(r'$g(w)$',fontsize = 12,rotation = 0,labelpad = 25)\n            \n        ax.set(aspect = 'equal')\n        a = ax.get_position()\n        yr = ax.get_position().y1 - ax.get_position().y0\n        xr = ax.get_position().x1 - ax.get_position().x0\n        aspectratio=1.25*xr/yr# + min(xr,yr)\n        ratio_default=(ax2.get_xlim()[1]-ax2.get_xlim()[0])/(ax2.get_ylim()[1]-ax2.get_ylim()[0])\n        ax2.set_aspect(ratio_default*aspectratio)\n            \n        # plot\n        plt.show()    """
mlrefined_libraries/math_optimization_library/norm_visualizer.py,58,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport time\nimport copy\n\nclass visualizer:\n    \'\'\'\n    Visualizes norm ball from L1 to Linf, as well as how the distance between two user defined points is defined in that norm.\n    \'\'\' \n            \n    ##### draws still figure of L1, L2, and Linf ball with visualized distance between 2 points ####       \n    def draw_it(self,pt1,pt2,**kwargs):\n        ### initialize figure and panels ###\n        fig = plt.figure(figsize = (9,4))\n       \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0],aspect = \'equal\')\n        ax2 = plt.subplot(gs[1],aspect = \'equal\');         \n        \n        ### make unit balls ###\n        w = np.linspace(0,1,2000)\n        \n        # make l2 ball\n        x = np.cos(2*np.pi*w)\n        x.shape = (len(x),1)\n        y = np.sin(2*np.pi*w)\n        y.shape = (len(y),1)\n        l2 = np.concatenate((x,y),axis=1)\n        \n        # make l1 ball \n        l1 = []\n        for s in l2:\n            s = s/np.sum(abs(s))\n            l1.append(s)\n        l1 = np.asarray(l1)\n        \n        # make Linf ball\n        linf = []\n        for s in l2:\n            s = s/np.max(abs(s))\n            linf.append(s)\n        linf = np.asarray(linf) \n        \n        ### plot norm balls ###\n        ax1.plot(l2[:,0],l2[:,1],color = \'k\',zorder = 1)                           # plot function\n        ax1.plot(l1[:,0],l1[:,1],color = \'r\',zorder = 1)                           # plot function\n        ax1.plot(linf[:,0],linf[:,1],color = \'b\',zorder = 1)                           # plot function\n        \n        ### clean up panel ###\n        ax1.axhline(y=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax1.axvline(x=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax1.set_xticks([-1,-0.5,0,0.5,1])\n        ax1.set_yticks([-1,-0.5,0,0.5,1])\n        ax1.set_xlim([-1.2,1.2])\n        ax1.set_ylim([-1.2,1.2])\n        \n        ### scatter two points, show length in norm for l1, l2, and linf ###\n        # create l2 distance curve\n        pt1 = np.asarray(pt1)\n        pt2 = np.asarray(pt2)\n        l2_dist = [(1-s)*pt1 + s*pt2 for s in w]\n        l2_dist = np.asarray(l2_dist)\n        l2_dist.shape = (len(l2_dist),2)\n        \n        # create l1 distance curve\n        pt1_temp = np.asarray([pt1[0],pt1[1]])\n        pt2_temp = np.asarray([pt2[0],pt1[1]])\n        l1_dist = [(1-s)*pt1_temp + s*pt2_temp for s in w]\n        l1_dist = np.asarray(l1_dist)\n        l1_dist.shape = (len(l1_dist),2)\n\n        pt1_temp = np.asarray([pt2[0],pt1[1]])\n        pt2_temp = np.asarray([pt2[0],pt2[1]])\n        l1_dist2 = [(1-s)*pt1_temp + s*pt2_temp for s in w]\n        l1_dist2 = np.asarray(l1_dist2)\n        l1_dist2.shape = (len(l1_dist2),2)\n        l1_dist = np.concatenate((l1_dist,l1_dist2),axis=0)        \n\n        # create linf distance curve\n        linf_dist = abs(pt1 - pt2)\n        ind = np.argmax(linf_dist)  \n        pt1_temp = 0\n        pt2_temp = 0\n        if ind == 0:\n            pt1_temp = np.asarray([pt1[0],pt2[1]])\n            pt2_temp = np.asarray([pt2[0],pt2[1]]) \n        elif ind == 1:\n            pt1_temp = np.asarray([pt1[0],pt1[1]])\n            pt2_temp = np.asarray([pt1[0],pt2[1]]) \n            \n        linf_dist = [(1-s)*pt1_temp + s*pt2_temp for s in w]\n        linf_dist = np.asarray(linf_dist)\n        linf_dist.shape = (len(linf_dist),2)\n        \n        ### plot points and distances ###\n        ax2.scatter(pt1[0],pt1[1],s = 60,linewidth = 1,edgecolor = \'w\',color = \'k\',zorder = 2)\n        ax2.scatter(pt2[0],pt2[1],s = 60,linewidth = 1,edgecolor = \'w\',color = \'k\',zorder = 2)\n                \n        ### plot distances ###\n        ax2.plot(l2_dist[:,0],l2_dist[:,1],color = \'k\',linestyle = \'--\',linewidth=2,zorder = 1)                           \n        ax2.plot(l1_dist[:,0],l1_dist[:,1],color = \'r\',linestyle = \'--\',linewidth=2,zorder = 1)       \n        ax2.plot(linf_dist[:,0],linf_dist[:,1],color = \'b\',linestyle = \'--\',linewidth=2,zorder = 1)               \n       \n        ### clean up panel ###\n        ax2.axhline(y=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax2.axvline(x=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax2.set_xticks([-1,-0.5,0,0.5,1])\n        ax2.set_yticks([-1,-0.5,0,0.5,1])\n        ax2.set_xlim([-1.2,1.2])\n        ax2.set_ylim([-1.2,1.2])\n        ax2.legend([\'l2\',\'l1\',\'linf\'])\n\n        \n        \n    ##### animate gradient descent method using single-input function #####\n    def animate_2d(self,**kwargs):\n        self.g = kwargs[\'g\']                          # input function\n        self.grad = compute_grad(self.g)              # gradient of input function\n        self.w_init =float( -2)                       # user-defined initial point (adjustable when calling each algorithm)\n        self.alpha = 10**-4                           # user-defined step length for gradient descent (adjustable when calling gradient descent)\n        self.max_its = 20                             # max iterations to run for each algorithm\n        self.w_hist = []                              # container for algorithm path\n        \n        wmin = -3.1\n        wmax = 3.1\n        if \'wmin\' in kwargs:            \n            wmin = kwargs[\'wmin\']\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\']\n            \n        # version of gradient descent to use (normalized or unnormalized)\n        self.version = \'unnormalized\'\n        if \'version\' in kwargs:\n            self.version = kwargs[\'version\']\n            \n        # get new initial point if desired\n        if \'w_init\' in kwargs:\n            self.w_init = float(kwargs[\'w_init\'])\n            \n        # take in user defined step length\n        if \'steplength\' in kwargs:\n            self.steplength = kwargs[\'steplength\']\n            \n        # take in user defined maximum number of iterations\n        if \'max_its\' in kwargs:\n            self.max_its = float(kwargs[\'max_its\'])\n            \n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n        \n        # remove whitespace from figure\n        #fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        #fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,4,1]) \n\n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\')\n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n        ax = plt.subplot(gs[1]); \n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(wmin,wmax,200)\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.1\n        width = 30\n        \n        # run gradient descent method\n        self.w_hist = []\n        self.run_gradient_descent()\n        \n        # colors for points --> green as the algorithm begins, yellow as it converges, red at final point\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # animation sub-function\n        num_frames = 2*len(self.w_hist)+2\n        print (\'starting animation rendering...\')\n        def animate(t):\n            ax.cla()\n            k = math.floor((t+1)/float(2))\n            \n            # print rendering update            \n            if np.mod(t+1,25) == 0:\n                print (\'rendering animation frame \' + str(t+1) + \' of \' + str(num_frames))\n            if t == num_frames - 1:\n                print (\'animation rendering complete!\')\n                time.sleep(1.5)\n                clear_output()\n            \n            # plot function\n            ax.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n            \n            # plot initial point and evaluation\n            if k == 0:\n                w_val = self.w_init\n                g_val = self.g(w_val)\n                ax.scatter(w_val,g_val,s = 90,c = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4),zorder = 3,marker = \'X\')            # evaluation on function\n                ax.scatter(w_val,0,s = 90,facecolor = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4), zorder = 3)\n                \n                # draw dashed line connecting w axis to point on cost function\n                s = np.linspace(0,g_val)\n                o = np.ones((len(s)))\n                ax.plot(o*w_val,s,\'k--\',linewidth=1)\n\n            # plot all input/output pairs generated by algorithm thus far\n            if k > 0:\n                # plot all points up to this point\n                for j in range(min(k-1,len(self.w_hist))):  \n                    w_val = self.w_hist[j]\n                    g_val = self.g(w_val)\n                    ax.scatter(w_val,g_val,s = 90,c = self.colorspec[j],edgecolor = \'k\',linewidth = 0.5*((1/(float(j) + 1)))**(0.4),zorder = 3,marker = \'X\')            # plot point of tangency\n                    ax.scatter(w_val,0,s = 90,facecolor = self.colorspec[j],edgecolor = \'k\',linewidth =  0.5*((1/(float(j) + 1)))**(0.4), zorder = 2)\n                    \n            # plot surrogate function and travel-to point\n            if k > 0 and k < len(self.w_hist) + 1:          \n                # grab historical weight, compute function and derivative evaluations\n                w = self.w_hist[k-1]\n                g_eval = self.g(w)\n                grad_eval = float(self.grad(w))\n            \n                # determine width to plot the approximation -- so its length == width defined above\n                div = float(1 + grad_eval**2)\n                w1 = w - math.sqrt(width/div)\n                w2 = w + math.sqrt(width/div)\n\n                # use point-slope form of line to plot\n                wrange = np.linspace(w1,w2, 100)\n                h = g_eval + grad_eval*(wrange - w)\n\n                # plot tangent line\n                ax.plot(wrange,h,color = \'lime\',linewidth = 2,zorder = 1)      # plot approx\n\n                # plot tangent point\n                ax.scatter(w,g_eval,s = 100,c = \'m\',edgecolor = \'k\',linewidth = 0.7,zorder = 3,marker = \'X\')            # plot point of tangency\n            \n                # plot next point learned from surrogate\n                if np.mod(t,2) == 0:\n                    # create next point information\n                    w_zero = w - self.alpha*grad_eval\n                    g_zero = self.g(w_zero)\n                    h_zero = g_eval + grad_eval*(w_zero - w)\n\n                    # draw dashed line connecting the three\n                    vals = [0,h_zero,g_zero]\n                    vals = np.sort(vals)\n\n                    s = np.linspace(vals[0],vals[2])\n                    o = np.ones((len(s)))\n                    ax.plot(o*w_zero,s,\'k--\',linewidth=1)\n\n                    # draw intersection at zero and associated point on cost function you hop back too\n                    ax.scatter(w_zero,h_zero,s = 100,c = \'k\', zorder = 3,marker = \'X\')\n                    ax.scatter(w_zero,0,s = 100,c = \'m\',edgecolor = \'k\',linewidth = 0.7, zorder = 3)\n                    ax.scatter(w_zero,g_zero,s = 100,c = \'m\',edgecolor = \'k\',linewidth = 0.7,zorder = 3, marker = \'X\')            # plot point of tangency\n                 \n            # fix viewing limits\n            ax.set_xlim([wmin-0.1,wmax+0.1])\n            ax.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            \n            # place title\n            ax.set_xlabel(r\'$w$\',fontsize = 14)\n            ax.set_ylabel(r\'$g(w)$\',fontsize = 14,rotation = 0,labelpad = 25)\n\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        return(anim)\n\n    # visualize descent on multi-input function\n    def visualize3d(self,g,w_init,steplength,max_its,**kwargs):\n        ### input arguments ###        \n        self.g = g\n        self.steplength = steplength\n        self.max_its = max_its\n        self.grad = compute_grad(self.g)              # gradient of input function\n\n        wmax = 1\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\'] + 0.5\n\n        view = [20,-50]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n\n        axes = False\n        if \'axes\' in kwargs:\n            axes = kwargs[\'axes\']\n\n        plot_final = False\n        if \'plot_final\' in kwargs:\n            plot_final = kwargs[\'plot_final\']\n\n        num_contours = 10\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']\n\n        # version of gradient descent to use (normalized or unnormalized)\n        self.version = \'unnormalized\'\n        if \'version\' in kwargs:\n            self.version = kwargs[\'version\']\n            \n        # get initial point \n        self.w_init = np.asarray([float(s) for s in w_init])\n                    \n        # take in user defined step length\n        self.steplength = steplength\n            \n        # take in user defined maximum number of iterations\n        self.max_its = max_its\n            \n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (9,3))\n\n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,2]) \n        ax = plt.subplot(gs[0],projection=\'3d\'); \n        ax2 = plt.subplot(gs[1],aspect=\'equal\'); \n\n        #### define input space for function and evaluate ####\n        w = np.linspace(-wmax,wmax,200)\n        w1_vals, w2_vals = np.meshgrid(w,w)\n        w1_vals.shape = (len(w)**2,1)\n        w2_vals.shape = (len(w)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([g(s) for s in h])\n        w1_vals.shape = (len(w),len(w))\n        w2_vals.shape = (len(w),len(w))\n        func_vals.shape = (len(w),len(w))\n\n        # plot function \n        ax.plot_surface(w1_vals, w2_vals, func_vals, alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)\n\n        # plot z=0 plane \n        ax.plot_surface(w1_vals, w2_vals, func_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        ax2.contour(w1_vals, w2_vals, func_vals,num_contours,colors = \'k\')\n        if axes == True:\n            ax2.axhline(linestyle = \'--\', color = \'k\',linewidth = 1)\n            ax2.axvline(linestyle = \'--\', color = \'k\',linewidth = 1)\n\n        #### run local random search algorithm ####\n        self.w_hist = []\n        self.run_gradient_descent()\n\n        # colors for points\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        colorspec = []\n        colorspec = np.concatenate((s,np.flipud(s)),1)\n        colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n\n        #### scatter path points ####\n        for k in range(len(self.w_hist)):\n            w_now = self.w_hist[k]\n            ax.scatter(w_now[0],w_now[1],0,s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 0.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n\n            ax2.scatter(w_now[0],w_now[1],s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 1.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n\n        #### connect points with arrows ####\n        if len(self.w_hist) < 10:\n            for i in range(len(self.w_hist)-1):\n                pt1 = self.w_hist[i]\n                pt2 = self.w_hist[i+1]\n\n                # draw arrow in left plot\n                a = Arrow3D([pt1[0],pt2[0]], [pt1[1],pt2[1]], [0, 0], mutation_scale=10, lw=2, arrowstyle=""-|>"", color=""k"")\n                ax.add_artist(a)\n\n                # draw 2d arrow in right plot\n                ax2.arrow(pt1[0],pt1[1],(pt2[0] - pt1[0])*0.78,(pt2[1] - pt1[1])*0.78, head_width=0.1, head_length=0.1, fc=\'k\', ec=\'k\',linewidth=3,zorder = 2,length_includes_head=True)\n\n        ### cleanup panels ###\n        ax.set_xlabel(\'$w_1$\',fontsize = 12)\n        ax.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n        ax.set_title(\'$g(w_1,w_2)$\',fontsize = 12)\n        ax.view_init(view[0],view[1])\n\n        ax2.set_xlabel(\'$w_1$\',fontsize = 12)\n        ax2.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n        ax2.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax2.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n\n        # clean up axis\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        # plot\n        plt.show()\n        \n    # compare normalized and unnormalized grad descent on 3d example\n    def compare_versions_3d(self,g,w_init,steplength,max_its,**kwargs):\n        ### input arguments ###        \n        self.g = g\n        self.steplength = steplength\n        self.max_its = max_its\n        self.grad = compute_grad(self.g)              # gradient of input function\n\n        wmax = 1\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\'] + 0.5\n\n        view = [20,-50]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n\n        axes = False\n        if \'axes\' in kwargs:\n            axes = kwargs[\'axes\']\n\n        plot_final = False\n        if \'plot_final\' in kwargs:\n            plot_final = kwargs[\'plot_final\']\n\n        num_contours = 10\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']\n            \n        # get initial point \n        self.w_init = np.asarray([float(s) for s in w_init])\n                    \n        # take in user defined step length\n        self.steplength = steplength\n            \n        # take in user defined maximum number of iterations\n        self.max_its = max_its\n            \n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (12,6))\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(2, 2, width_ratios=[1,4]) \n        ax3 = plt.subplot(gs[0],projection=\'3d\'); \n        ax4 = plt.subplot(gs[1],aspect=\'equal\'); \n        ax5 = plt.subplot(gs[2],projection=\'3d\'); \n        ax6 = plt.subplot(gs[3],aspect=\'equal\'); \n        \n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        \n        #### define input space for function and evaluate ####\n        w = np.linspace(-wmax,wmax,200)\n        w1_vals, w2_vals = np.meshgrid(w,w)\n        w1_vals.shape = (len(w)**2,1)\n        w2_vals.shape = (len(w)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([g(s) for s in h])\n        w1_vals.shape = (len(w),len(w))\n        w2_vals.shape = (len(w),len(w))\n        func_vals.shape = (len(w),len(w))\n\n        #### run local random search algorithms ####\n        for algo in [\'normalized\',\'unnormalized\']:\n            # switch normalized / unnormalized\n            self.version = algo\n            title = \'\'\n            if self.version == \'normalized\':\n                ax = ax3\n                ax2 = ax4\n                title = \'normalized gradient descent\'\n            else:\n                ax = ax5\n                ax2 = ax6\n                title = \'unnormalized gradient descent\'\n            \n           # plot function \n            ax.plot_surface(w1_vals, w2_vals, func_vals, alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)\n\n            # plot z=0 plane \n            ax.plot_surface(w1_vals, w2_vals, func_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n\n            ### make contour right plot - as well as horizontal and vertical axes ###\n            ax2.contour(w1_vals, w2_vals, func_vals,num_contours,colors = \'k\')\n            if axes == True:\n                ax2.axhline(linestyle = \'--\', color = \'k\',linewidth = 1)\n                ax2.axvline(linestyle = \'--\', color = \'k\',linewidth = 1)\n            \n            self.w_hist = []\n            self.run_gradient_descent()\n\n            # colors for points\n            s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n            s.shape = (len(s),1)\n            t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n            t.shape = (len(t),1)\n            s = np.vstack((s,t))\n            colorspec = []\n            colorspec = np.concatenate((s,np.flipud(s)),1)\n            colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n\n            #### scatter path points ####\n            for k in range(len(self.w_hist)):\n                w_now = self.w_hist[k]\n                ax.scatter(w_now[0],w_now[1],0,s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 0.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n\n                ax2.scatter(w_now[0],w_now[1],s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 1.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n\n            #### connect points with arrows ####\n            if len(self.w_hist) < 10:\n                for i in range(len(self.w_hist)-1):\n                    pt1 = self.w_hist[i]\n                    pt2 = self.w_hist[i+1]\n        \n                    # draw arrow in left plot\n                    a = Arrow3D([pt1[0],pt2[0]], [pt1[1],pt2[1]], [0, 0], mutation_scale=10, lw=2, arrowstyle=""-|>"", color=""k"")\n                    ax.add_artist(a)\n\n                    # draw 2d arrow in right plot\n                    ax2.arrow(pt1[0],pt1[1],(pt2[0] - pt1[0])*0.78,(pt2[1] - pt1[1])*0.78, head_width=0.1, head_length=0.1, fc=\'k\', ec=\'k\',linewidth=3,zorder = 2,length_includes_head=True)\n\n            ### cleanup panels ###\n            ax.set_xlabel(\'$w_1$\',fontsize = 12)\n            ax.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n            ax.set_title(title,fontsize = 12)\n            ax.view_init(view[0],view[1])\n    \n            ax2.set_xlabel(\'$w_1$\',fontsize = 12)\n            ax2.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n            ax2.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            ax2.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n\n            # clean up axis\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        # plot\n        plt.show()      \n        \n        \n#### custom 3d arrow and annotator functions ###    \n# nice arrow maker from https://stackoverflow.com/questions/11140163/python-matplotlib-plotting-a-3d-cube-a-sphere-and-a-vector\nclass Arrow3D(FancyArrowPatch):\n\n    def __init__(self, xs, ys, zs, *args, **kwargs):\n        FancyArrowPatch.__init__(self, (0, 0), (0, 0), *args, **kwargs)\n        self._verts3d = xs, ys, zs\n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n        FancyArrowPatch.draw(self, renderer)'"
mlrefined_libraries/math_optimization_library/optimizers.py,19,"b""# import autograd functionality\nimport autograd.numpy as np\nfrom autograd import value_and_grad \nfrom autograd import hessian\nfrom autograd.misc.flatten import flatten_func\n\n# random search function\ndef random_search(g,alpha_choice,max_its,w,num_samples):\n    # run random search\n    weight_history = []         # container for weight history\n    cost_history = []           # container for corresponding cost function history\n    alpha = 0\n    for k in range(1,max_its+1):        \n        # check if diminishing steplength rule used\n        if alpha_choice == 'diminishing':\n            alpha = 1/float(k)\n        else:\n            alpha = alpha_choice\n            \n        # record weights and cost evaluation\n        weight_history.append(w)\n        cost_history.append(g(w))\n        \n        # construct set of random unit directions\n        directions = np.random.randn(num_samples,np.size(w))\n        norms = np.sqrt(np.sum(directions*directions,axis = 1))[:,np.newaxis]\n        directions = directions/norms   \n        \n        ### pick best descent direction\n        # compute all new candidate points\n        w_candidates = w + alpha*directions\n        \n        # evaluate all candidates\n        evals = np.array([g(w_val) for w_val in w_candidates])\n\n        # if we find a real descent direction take the step in its direction\n        ind = np.argmin(evals)\n        if g(w_candidates[ind]) < g(w):\n            # pluck out best descent direction\n            d = directions[ind,:]\n        \n            # take step\n            w = w + alpha*d\n        \n    # record weights and cost evaluation\n    weight_history.append(w)\n    cost_history.append(g(w))\n    return weight_history,cost_history\n\n# zero order coordinate search\ndef coordinate_search(g,alpha_choice,max_its,w):\n    # construct set of all coordinate directions\n    directions_plus = np.eye(np.size(w),np.size(w))\n    directions_minus = - np.eye(np.size(w),np.size(w))\n    directions = np.concatenate((directions_plus,directions_minus),axis=0)\n        \n    # run coordinate search\n    weight_history = []         # container for weight history\n    cost_history = []           # container for corresponding cost function history\n    alpha = 0\n    for k in range(1,max_its+1):        \n        # check if diminishing steplength rule used\n        if alpha_choice == 'diminishing':\n            alpha = 1/float(k)\n        else:\n            alpha = alpha_choice\n            \n        # record weights and cost evaluation\n        weight_history.append(w)\n        cost_history.append(g(w))\n        \n        ### pick best descent direction\n        # compute all new candidate points\n        w_candidates = w + alpha*directions\n        \n        # evaluate all candidates\n        evals = np.array([g(w_val) for w_val in w_candidates])\n\n        # if we find a real descent direction take the step in its direction\n        ind = np.argmin(evals)\n        if g(w_candidates[ind]) < g(w):\n            # pluck out best descent direction\n            d = directions[ind,:]\n        \n            # take step\n            w = w + alpha*d\n        \n    # record weights and cost evaluation\n    weight_history.append(w)\n    cost_history.append(g(w))\n    return weight_history,cost_history\n\n# zero order coordinate search\ndef coordinate_descent_zero_order(g,alpha_choice,max_its,w):  \n    # run coordinate search\n    N = np.size(w)\n    weight_history = []         # container for weight history\n    cost_history = []           # container for corresponding cost function history\n    alpha = 0\n    for k in range(1,max_its+1):        \n        # check if diminishing steplength rule used\n        if alpha_choice == 'diminishing':\n            alpha = 1/float(k)\n        else:\n            alpha = alpha_choice\n        \n        # random shuffle of coordinates\n        c = np.random.permutation(N)\n        \n        # forming the dirction matrix out of the loop\n        DIRECTION = np.eye(N)\n        cost = g(w)\n        \n        # loop over each coordinate direction\n        for n in range(N):\n            #direction = np.zeros((N,1))\n            #direction[c[n]] = 1\n            direction = DIRECTION[:,[c[n]]]\n     \n            # record weights and cost evaluation\n            weight_history.append(w)\n            cost_history.append(cost)\n\n            # evaluate all candidates\n            evals =  [g(w + alpha*direction)]\n            evals.append(g(w - alpha*direction))\n            evals = np.array(evals)\n\n            # if we find a real descent direction take the step in its direction\n            ind = np.argmin(evals)\n            if evals[ind] < cost_history[-1]:\n                # take step\n                w = w + ((-1)**(ind))*alpha*direction\n                cost = evals[ind]\n        \n    # record weights and cost evaluation\n    weight_history.append(w)\n    cost_history.append(g(w))\n    return weight_history,cost_history\n\n# gradient descent function - inputs: g (input function), alpha (steplength parameter), max_its (maximum number of iterations), w (initialization)\ndef gradient_descent(g,alpha_choice,max_its,w):\n    # flatten the input function to more easily deal with costs that have layers of parameters\n    g_flat, unflatten, w = flatten_func(g, w) # note here the output 'w' is also flattened\n\n    # compute the gradient function of our input function - note this is a function too\n    # that - when evaluated - returns both the gradient and function evaluations (remember\n    # as discussed in Chapter 3 we always ge the function evaluation 'for free' when we use\n    # an Automatic Differntiator to evaluate the gradient)\n    gradient = value_and_grad(g_flat)\n\n    # run the gradient descent loop\n    weight_history = []      # container for weight history\n    cost_history = []        # container for corresponding cost function history\n    alpha = 0\n    for k in range(1,max_its+1):\n        # check if diminishing steplength rule used\n        if alpha_choice == 'diminishing':\n            alpha = 1/float(k)\n        else:\n            alpha = alpha_choice\n        \n        # evaluate the gradient, store current (unflattened) weights and cost function value\n        cost_eval,grad_eval = gradient(w)\n        weight_history.append(unflatten(w))\n        cost_history.append(cost_eval)\n\n        # take gradient descent step\n        w = w - alpha*grad_eval\n            \n    # collect final weights\n    weight_history.append(unflatten(w))\n    # compute final cost function value via g itself (since we aren't computing \n    # the gradient at the final step we don't get the final cost function value \n    # via the Automatic Differentiatoor) \n    cost_history.append(g_flat(w))  \n    return weight_history,cost_history\n\n# newtons method function - inputs: g (input function), max_its (maximum number of iterations), w (initialization)\ndef newtons_method(g,max_its,w,**kwargs):\n    # flatten input funciton, in case it takes in matrices of weights\n    flat_g, unflatten, w = flatten_func(g, w)\n    \n    # compute the gradient / hessian functions of our input function -\n    # note these are themselves functions.  In particular the gradient - \n    # - when evaluated - returns both the gradient and function evaluations (remember\n    # as discussed in Chapter 3 we always ge the function evaluation 'for free' when we use\n    # an Automatic Differntiator to evaluate the gradient)\n    gradient = value_and_grad(flat_g)\n    hess = hessian(flat_g)\n    \n    # set numericxal stability parameter / regularization parameter\n    epsilon = 10**(-7)\n    if 'epsilon' in kwargs:\n        epsilon = kwargs['epsilon']\n\n    # run the newtons method loop\n    weight_history = []      # container for weight history\n    cost_history = []        # container for corresponding cost function history\n    for k in range(max_its):\n        # evaluate the gradient, store current weights and cost function value\n        cost_eval,grad_eval = gradient(w)\n        weight_history.append(unflatten(w))\n        cost_history.append(cost_eval)\n\n        # evaluate the hessian\n        hess_eval = hess(w)\n\n        # reshape for numpy linalg functionality\n        hess_eval.shape = (int((np.size(hess_eval))**(0.5)),int((np.size(hess_eval))**(0.5)))\n        \n        # solve second order system system for weight update\n        #w = w - np.dot(np.linalg.pinv(hess_eval + epsilon*np.eye(np.size(w))),grad_eval)\n\n        # solve second order system system for weight update\n        A = hess_eval + epsilon*np.eye(np.size(w))\n        b = grad_eval\n        w = np.linalg.lstsq(A,np.dot(A,w) - b)[0]\n            \n    # collect final weights\n    weight_history.append(unflatten(w))\n    # compute final cost function value via g itself (since we aren't computing \n    # the gradient at the final step we don't get the final cost function value \n    # via the Automatic Differentiatoor) \n    cost_history.append(flat_g(w))  \n    \n    return weight_history,cost_history"""
mlrefined_libraries/math_optimization_library/plot3d.py,11,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nimport autograd.numpy as np\nimport math\nimport time\n\nclass visualizer:\n    \'\'\'\n    Simple plotter for 3d function - from surface and contour perspective\n    \'\'\'             \n\n    def draw_2d(self,g,**kwargs):\n        self.g = g                         # input function        \n        wmin = -3.1\n        wmax = 3.1\n        view = [50,50]\n        num_contours = 10\n        if \'wmin\' in kwargs:            \n            wmin = kwargs[\'wmin\']\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\']\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']   \n            \n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (9,3))\n\n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,2]) \n        ax = plt.subplot(gs[0],projection=\'3d\'); \n        ax2 = plt.subplot(gs[1],aspect=\'equal\'); \n\n        #### define input space for function and evaluate ####\n        w = np.linspace(-wmax,wmax,200)\n        w1_vals, w2_vals = np.meshgrid(w,w)\n        w1_vals.shape = (len(w)**2,1)\n        w2_vals.shape = (len(w)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([g(s) for s in h])\n        w1_vals.shape = (len(w),len(w))\n        w2_vals.shape = (len(w),len(w))\n        func_vals.shape = (len(w),len(w))\n\n        ### plot function as surface ### \n        ax.plot_surface(w1_vals, w2_vals, func_vals, alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)\n\n        # plot z=0 plane \n        ax.plot_surface(w1_vals, w2_vals, func_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n        \n        ### plot function as contours ###\n        # set level ridges\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cutoff = 0.5\n        cutoff = (levelmax - levelmin)*cutoff\n        numper = 3\n        levels1 = np.linspace(cutoff,levelmax,numper)\n        num_contours -= numper\n\n        levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n        levels = np.unique(np.append(levels1,levels2))\n        num_contours -= numper\n        while num_contours > 0:\n            cutoff = levels[1]\n            levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n            levels = np.unique(np.append(levels2,levels))\n            num_contours -= numper\n\n        ax2.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = \'k\')\n        ax2.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = \'Blues\')\n        \n        ### cleanup panels ###\n        ax.set_xlabel(\'$w_1$\',fontsize = 12)\n        ax.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n        ax.set_title(\'$g(w_1,w_2)$\',fontsize = 12)\n        ax.view_init(view[0],view[1])\n\n        ax2.set_xlabel(\'$w_1$\',fontsize = 12)\n        ax2.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n        ax2.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax2.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax2.set_xticks(np.arange(-round(wmax),round(wmax)+1))\n        ax2.set_yticks(np.arange(-round(wmax),round(wmax)+1))\n\n        # clean up axis\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        # plot\n        plt.show()'"
mlrefined_libraries/math_optimization_library/random_local_search.py,34,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nimport time\nfrom matplotlib import gridspec\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport copy\n\n# random local search function\ndef random_local_search(func,pt,max_steps,num_samples,steplength):\n    # starting point evaluation\n    current_eval = func(pt)\n    current_pt = pt\n    \n    # loop over max_its descend until no improvement or max_its reached\n    pt_history = [current_pt]\n    eval_history = [current_eval]\n    for i in range(max_steps):\n        # loop over num_samples, randomly sample direction and evaluate, move to best evaluation\n        swap = 0\n        keeper_pt = current_pt\n        \n        # check if diminishing steplength rule used\n        if steplength == \'diminish\':\n            steplength_temp = 1/(1 + i)\n        else:\n            steplength_temp = steplength\n        \n        for j in range(num_samples):            \n            # produce direction\n            theta = np.random.rand(1)\n            x = steplength_temp*np.cos(2*np.pi*theta)\n            y = steplength_temp*np.sin(2*np.pi*theta)\n            new_pt = np.asarray([x,y])\n            temp_pt = copy.deepcopy(keeper_pt) \n            new_pt += temp_pt\n            \n            # evaluate new point\n            new_eval = func(new_pt)\n            if new_eval < current_eval:\n                current_pt = new_pt\n                current_eval = new_eval\n                swap = 1\n        \n        # if nothing has changed\n        if swap == 1:\n            pt_history.append(current_pt)\n            eval_history.append(current_eval)\n    \n    # translate to array, reshape appropriately\n    pt_history = np.asarray(pt_history)\n    pt_history.shape = (np.shape(pt_history)[0],np.shape(pt_history)[1])\n\n    eval_history = np.asarray(eval_history)\n    eval_history.shape = (np.shape(eval_history)[0],np.shape(eval_history)[1])\n\n    return pt_history,eval_history\n\n# random local search function\ndef random_local_search_2d(func,pt,max_steps,num_samples,steplength):\n    # starting point evaluation\n    current_eval = func(pt)\n    current_pt = pt\n    \n    # loop over max_its descend until no improvement or max_its reached\n    pt_history = [current_pt]\n    eval_history = [current_eval]\n    for i in range(max_steps):\n        # loop over num_samples, randomly sample direction and evaluate, move to best evaluation\n        swap = 0\n        keeper_pt = current_pt\n        \n        # check if diminishing steplength rule used\n        if steplength == \'diminish\':\n            steplength_temp = 1/(1 + i)\n        else:\n            steplength_temp = steplength\n        \n        for j in range(num_samples):            \n            # produce direction\n            new_pt = steplength*np.sign(2*np.random.rand(1) - 1)\n            temp_pt = copy.deepcopy(keeper_pt) \n            new_pt += temp_pt\n            \n            # evaluate new point\n            new_eval = func(new_pt)\n            if new_eval < current_eval:\n                current_pt = new_pt\n                current_eval = new_eval\n                swap = 1\n        \n            # if nothing has changed\n            if swap == 1:\n                pt_history.append(current_pt)\n                eval_history.append(current_eval)\n    \n    # translate to array, reshape appropriately\n    pt_history = np.asarray(pt_history)\n    eval_history = np.asarray(eval_history)\n\n    return pt_history,eval_history\n\n##### draw still image of gradient descent on single-input function ####       \ndef draw_2d(g,steplength,max_steps,w_inits,num_samples,**kwargs):\n    wmin = -3.1\n    wmax = 3.1\n    if \'wmin\' in kwargs:            \n        wmin = kwargs[\'wmin\']\n    if \'wmax\' in kwargs:\n        wmax = kwargs[\'wmax\'] \n        \n    # initialize figure\n    fig = plt.figure(figsize = (9,4))\n    artist = fig\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,4,1]) \n    ax1 = plt.subplot(gs[0]); ax1.axis(\'off\')\n    ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n    ax = plt.subplot(gs[1]); \n\n    # generate function for plotting on each slide\n    w_plot = np.linspace(wmin,wmax,500)\n    g_plot = [g(s) for s in w_plot]\n    g_range = max(g_plot) - min(g_plot)\n    ggap = g_range*0.1\n    width = 30\n       \n    #### loop over all initializations, run gradient descent algorithm for each and plot results ###\n    for j in range(len(w_inits)):\n        # get next initialization\n        w_init = w_inits[j]\n\n        # run grad descent for this init\n        func = g\n        pt_history,eval_history = random_local_search_2d(func,w_init,max_steps,num_samples,steplength)\n\n        # colors for points --> green as the algorithm begins, yellow as it converges, red at final point\n        s = np.linspace(0,1,len(pt_history[:round(len(eval_history)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(eval_history[round(len(eval_history)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        colorspec = []\n        colorspec = np.concatenate((s,np.flipud(s)),1)\n        colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n        \n        # plot function, axes lines\n        ax.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n        ax.axhline(y=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax.axvline(x=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax.set_xlabel(r\'$w$\',fontsize = 13)\n        ax.set_ylabel(r\'$g(w)$\',fontsize = 13,rotation = 0,labelpad = 25)            \n            \n        ### plot all local search points ###\n        for k in range(len(eval_history)):\n            # pick out current weight and function value from history, then plot\n            w_val = pt_history[k]\n            g_val = eval_history[k]\n\n            ax.scatter(w_val,g_val,s = 90,c = colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4),zorder = 3,marker = \'X\')            # evaluation on function\n            ax.scatter(w_val,0,s = 90,facecolor = colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4), zorder = 3)\n\n            \n# animator for random local search\ndef visualize3d(func,pt_history,eval_history,**kwargs):\n    ### input arguments ###        \n    wmax = 1\n    if \'wmax\' in kwargs:\n        wmax = kwargs[\'wmax\'] + 0.5\n        \n    view = [20,-50]\n    if \'view\' in kwargs:\n        view = kwargs[\'view\']\n        \n    axes = False\n    if \'axes\' in kwargs:\n        axes = kwargs[\'axes\']\n       \n    plot_final = False\n    if \'plot_final\' in kwargs:\n        plot_final = kwargs[\'plot_final\']\n      \n    num_contours = 10\n    if \'num_contours\' in kwargs:\n        num_contours = kwargs[\'num_contours\']\n        \n    pt = [0,0]\n    if \'pt\' in kwargs:\n        pt = kwargs[\'pt\']\n    pt = np.asarray(pt)\n    pt.shape = (2,1)\n     \n    max_steps = 10\n    if \'max_steps\' in kwargs:\n        max_steps = kwargs[\'max_steps\']\n    num_samples = 10\n    if \'num_samples\' in kwargs:\n        num_samples = kwargs[\'num_samples\'] \n    steplength = 1\n    if \'steplength\' in kwargs:\n        steplength = kwargs[\'steplength\']     \n        \n    ##### construct figure with panels #####\n    # construct figure\n    fig = plt.figure(figsize = (9,3))\n          \n    # remove whitespace from figure\n    fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1,2]) \n    ax = plt.subplot(gs[0],projection=\'3d\'); \n    ax2 = plt.subplot(gs[1],aspect=\'equal\'); \n    \n    #### define input space for function and evaluate ####\n    w = np.linspace(-wmax,wmax,200)\n    w1_vals, w2_vals = np.meshgrid(w,w)\n    w1_vals.shape = (len(w)**2,1)\n    w2_vals.shape = (len(w)**2,1)\n    h = np.concatenate((w1_vals,w2_vals),axis=1)\n    func_vals = np.asarray([func(s) for s in h])\n    w1_vals.shape = (len(w),len(w))\n    w2_vals.shape = (len(w),len(w))\n    func_vals.shape = (len(w),len(w))\n    \n    # plot function \n    ax.plot_surface(w1_vals, w2_vals, func_vals, alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)\n\n    # plot z=0 plane \n    ax.plot_surface(w1_vals, w2_vals, func_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n    \n    ### make contour right plot - as well as horizontal and vertical axes ###\n    ax2.contour(w1_vals, w2_vals, func_vals,num_contours,colors = \'k\')\n    if axes == True:\n        ax2.axhline(linestyle = \'--\', color = \'k\',linewidth = 1)\n        ax2.axvline(linestyle = \'--\', color = \'k\',linewidth = 1)\n    \n    ### plot circle on which point lies, as well as step length circle - used only for simple quadratic\n    if plot_final == True:\n        # plot contour of quadratic on which final point was plotted\n        f = pt_history[-1]\n        val = np.linalg.norm(f)\n        theta = np.linspace(0,1,400)\n        x = val*np.cos(2*np.pi*theta) \n        y = val*np.sin(2*np.pi*theta) \n        ax2.plot(x,y,color = \'r\',linestyle = \'--\',linewidth = 1)\n\n        # plot direction sampling circle centered at final point\n        x = steplength*np.cos(2*np.pi*theta) + f[0]\n        y = steplength*np.sin(2*np.pi*theta) + f[1]\n        ax2.plot(x,y,color = \'b\',linewidth = 1)    \n    \n    # colors for points\n    s = np.linspace(0,1,len(eval_history[:round(len(eval_history)/2)]))\n    s.shape = (len(s),1)\n    t = np.ones(len(eval_history[round(len(eval_history)/2):]))\n    t.shape = (len(t),1)\n    s = np.vstack((s,t))\n    colorspec = []\n    colorspec = np.concatenate((s,np.flipud(s)),1)\n    colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n    \n    #### scatter path points ####\n    for k in range(len(eval_history)):\n        ax.scatter(pt_history[k][0],pt_history[k][1],0,s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 0.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n        \n        ax2.scatter(pt_history[k][0],pt_history[k][1],s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 1.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n\n    #### connect points with arrows ####\n    for i in range(len(eval_history)-1):\n        pt1 = pt_history[i]\n        pt2 = pt_history[i+1]\n\n        if np.linalg.norm(pt1 - pt2) > 0.5:\n            # draw arrow in left plot\n            a = Arrow3D([pt1[0],pt2[0]], [pt1[1],pt2[1]], [0, 0], mutation_scale=10, lw=2, arrowstyle=""-|>"", color=""k"")\n            ax.add_artist(a)\n\n            # draw 2d arrow in right plot\n            ax2.arrow(pt1[0],pt1[1],(pt2[0] - pt1[0])*0.78,(pt2[1] - pt1[1])*0.78, head_width=0.1, head_length=0.1, fc=\'k\', ec=\'k\',linewidth=3,zorder = 2,length_includes_head=True)\n\n    ### cleanup panels ###\n    ax.set_xlabel(\'$w_0$\',fontsize = 12)\n    ax.set_ylabel(\'$w_1$\',fontsize = 12,rotation = 0)\n    ax.set_title(\'$g(w_0,w_1)$\',fontsize = 12)\n    ax.view_init(view[0],view[1])\n    \n    ax2.set_xlabel(\'$w_0$\',fontsize = 12)\n    ax2.set_ylabel(\'$w_1$\',fontsize = 12,rotation = 0)\n    \n    # clean up axis\n    ax.xaxis.pane.fill = False\n    ax.yaxis.pane.fill = False\n    ax.zaxis.pane.fill = False\n\n    ax.xaxis.pane.set_edgecolor(\'white\')\n    ax.yaxis.pane.set_edgecolor(\'white\')\n    ax.zaxis.pane.set_edgecolor(\'white\')\n    \n    ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n    ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n    ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n    \n    # plot\n    plt.show()\n\n#### custom 3d arrow and annotator functions ###    \n# nice arrow maker from https://stackoverflow.com/questions/11140163/python-matplotlib-plotting-a-3d-cube-a-sphere-and-a-vector\nclass Arrow3D(FancyArrowPatch):\n\n    def __init__(self, xs, ys, zs, *args, **kwargs):\n        FancyArrowPatch.__init__(self, (0, 0), (0, 0), *args, **kwargs)\n        self._verts3d = xs, ys, zs\n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n        FancyArrowPatch.draw(self, renderer)\n\n# great solution for annotating 3d objects - from https://datascience.stackexchange.com/questions/11430/how-to-annotate-labels-in-a-3d-matplotlib-scatter-plot\nclass Annotation3D(Annotation):\n    \'\'\'Annotate the point xyz with text s\'\'\'\n\n    def __init__(self, s, xyz, *args, **kwargs):\n        Annotation.__init__(self,s, xy=(0,0), *args, **kwargs)\n        self._verts3d = xyz        \n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.xy=(xs,ys)\n        Annotation.draw(self, renderer)        \n\ndef annotate3D(ax, s, *args, **kwargs):\n    \'\'\'add anotation text s to to Axes3d ax\'\'\'\n\n    tag = Annotation3D(s, *args, **kwargs)\n    ax.add_artist(tag)'"
mlrefined_libraries/math_optimization_library/random_method_experiments.py,29,"b'import autograd.numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import gridspec\nfrom IPython.display import display, HTML\nimport copy\nimport time\nimport math\nfrom IPython.display import clear_output\n        \n# custom plot for spiffing up plot of a two mathematical functions\ndef double_plot(func,num_samples,**kwargs):    \n    ## arguments user can tweak from control panel ##\n    wmax = 1\n    if \'wmax\' in kwargs:\n        wmax = kwargs[\'wmax\']   \n    view = [10,50]\n    if \'view\' in kwargs:\n        view = kwargs[\'view\']\n    \n    #### setup figure ####\n    fig = plt.figure(figsize = (8,6)) \n    gs = gridspec.GridSpec(2, 2,wspace=0.3, hspace=0.8) \n\n    ax1 = plt.subplot(gs[0])\n    ax2 = plt.subplot(gs[1],projection=\'3d\')\n    ax3 = plt.subplot(gs[2])\n    ax4 = plt.subplot(gs[3],projection=\'3d\')\n\n    ###### create 2d panels ######\n    # range over which to plot\n    w = np.linspace(-wmax,wmax,1000)\n     \n    # create even grid of sample points, random sample points\n    w_even = np.linspace(-wmax,wmax,num_samples)\n    w_rand = 2*wmax*np.random.rand(num_samples) - wmax\n\n    ### plot first 2d function - with even grid of points ###\n    f = [func(val) for val in w]\n    ax1.plot(w,f,color = \'k\',zorder = 2, linewidth = 2)\n    ax1.plot(w,[s*0 for s in f],color = \'k\',zorder = 1, linewidth = 1)    # horizontal axis\n   \n    ax3.plot(w,f,color = \'k\',zorder = 2, linewidth = 2)\n    ax3.plot(w,[s*0 for s in f],color = \'k\',zorder = 1, linewidth = 1)    # horizontal axis\n\n    f_even = [func(val) for val in w_even]\n    ax1.scatter(w_even,f_even,s = 50,c = \'lime\',edgecolor = \'k\',linewidth = 0.7,zorder = 3)       \n    ax1.scatter(w_even,[s*0 for s in w_even],s = 50,c = \'b\',edgecolor = \'k\',linewidth = 0.7,zorder = 3)      \n    \n    ### plot second 2d function - with random grid of points ###\n    f_rand = [func(val) for val in w_rand]\n    ax3.scatter(w_rand,f_rand,s = 50,c = \'lime\',edgecolor = \'k\',linewidth = 0.7,zorder = 3)            \n    ax3.scatter(w_rand,[s*0 for s in w_rand],s = 50,c = \'b\',edgecolor = \'k\',linewidth = 0.7,zorder = 3)   \n    \n    ### cleanup panels ###\n    # label 2d panels, put axis lines on, etc.,\n    ax1.set_xlabel(\'$w$\',fontsize = 12)\n    ax1.set_title(\'$g(w)$\',fontsize = 12)\n    ax1.grid(False, which=\'both\')\n   \n    ax3.set_xlabel(\'$w$\',fontsize = 12)\n    ax3.set_title(\'$g(w)$\',fontsize = 12)\n    ax3.grid(False, which=\'both\')\n    \n    \n    ###### create 3d panels ######   \n    w = np.linspace(-wmax,wmax,200)\n    w1_vals, w2_vals = np.meshgrid(w,w)\n    w1_vals.shape = (len(w)**2,1)\n    w2_vals.shape = (len(w)**2,1)\n    h = np.concatenate((w1_vals,w2_vals),axis=1)\n    func_vals = np.asarray([func(s) for s in h])\n\n    w1_vals.shape = (len(w),len(w))\n    w2_vals.shape = (len(w),len(w))\n    func_vals.shape = (len(w),len(w))\n\n    ### plot function and z=0 for visualization ###\n    ax2.plot_surface(w1_vals, w2_vals, func_vals, alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=0.7,edgecolor = \'k\',zorder = 2)\n        \n    ax4.plot_surface(w1_vals, w2_vals, func_vals, alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=0.7,edgecolor = \'k\',zorder = 2)       \n        \n    ### plot even vals ###\n    w = np.linspace(-wmax,wmax,num_samples)\n    w1_vals, w2_vals = np.meshgrid(w,w)\n    w1_vals.shape = (len(w)**2,1)\n    w2_vals.shape = (len(w)**2,1)\n    h = np.concatenate((w1_vals,w2_vals),axis=1)\n    f_even = np.asarray([func(s) for s in h])\n    ax2.scatter(w1_vals,w2_vals,f_even,s = 50,c = \'lime\',edgecolor = \'k\',linewidth = 0.7,zorder = 3)       \n    f_even = [s*0 for s in f_even]\n    ax2.scatter(w1_vals,w2_vals,f_even,s = 50,c = \'b\',edgecolor = \'k\',linewidth = 0.7,zorder = 3)       \n    \n    ### plot random samples ###\n    w_rand1 = 2*wmax*np.random.rand(num_samples**2) - wmax\n    w_rand1.shape = (len(w_rand1),1)\n    w_rand2 = 2*wmax*np.random.rand(num_samples**2) - wmax\n    w_rand2.shape = (len(w_rand2),1)\n    h = np.concatenate((w_rand1,w_rand2),axis=1)\n    f_rand = [func(val) for val in h]\n    ax4.scatter(w_rand1,w_rand2,f_rand,s = 50,c = \'lime\',edgecolor = \'k\',linewidth = 0.7,zorder = 3)       \n    f_rand = [s*0 for s in f_rand]\n    ax4.scatter(w_rand1,w_rand2,f_rand,s = 50,c = \'b\',edgecolor = \'k\',linewidth = 0.7,zorder = 3)       \n    \n        \n    ### cleanup panels ###\n    ax2.set_xlabel(\'$w_1$\',fontsize = 12)\n    ax2.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n    ax2.set_title(\'$g(w_1,w_2)$\',fontsize = 12)\n    ax2.view_init(view[0],view[1])\n    \n    # clean up axis\n    ax2.xaxis.pane.fill = False\n    ax2.yaxis.pane.fill = False\n    ax2.zaxis.pane.fill = False\n\n    ax2.xaxis.pane.set_edgecolor(\'white\')\n    ax2.yaxis.pane.set_edgecolor(\'white\')\n    ax2.zaxis.pane.set_edgecolor(\'white\')\n    \n    ax2.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n    ax2.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n    ax2.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n    ax2.set_xticks([-1,0,1])\n    ax2.set_xticklabels([-1,0,1])\n    \n    ax2.set_yticks([-1,0,1])\n    ax2.set_yticklabels([-1,0,1])\n    \n    ax2.set_zticks([0,1,2])\n    ax2.set_zticklabels([0,1,2])\n    \n    # label axis\n    ax4.set_xlabel(\'$w_1$\',fontsize = 12)\n    ax4.set_ylabel(\'$w_2$\',fontsize = 12,rotation = 0)\n    ax4.set_title(\'$g(w_1,w_2)$\',fontsize = 12)\n    ax4.view_init(view[0],view[1])\n    \n    # clean up axis\n    ax4.xaxis.pane.fill = False\n    ax4.yaxis.pane.fill = False\n    ax4.zaxis.pane.fill = False\n\n    ax4.xaxis.pane.set_edgecolor(\'white\')\n    ax4.yaxis.pane.set_edgecolor(\'white\')\n    ax4.zaxis.pane.set_edgecolor(\'white\')\n    \n    ax4.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n    ax4.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n    ax4.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n    \n    ax4.set_xticks([-1,0,1])\n    ax4.set_xticklabels([-1,0,1])\n    \n    ax4.set_yticks([-1,0,1])\n    ax4.set_yticklabels([-1,0,1])\n  \n    ax4.set_zticks([0,1,2])\n    ax4.set_zticklabels([0,1,2])\n\ndef random_eval_experiment():\n    \'\'\'\n    Experiment illutrating how quickly global random evaluation will fail as a method of optimization.  Output is minimum value attained by random sampling over the cube [-1,1] x [-1,1] x... [-1,1] evaluating simple quadratic for 100, 1000, or 10000 times.  The dimension is increased from 1 to 100 and the minimum plotted for each dimension.\n    \'\'\'    \n    # define symmetric quadratic N-dimensional\n    g = lambda w: np.dot(w.T,w)\n\n    # loop over dimensions, sample directions, evaluate\n    mean_evals = []\n    big_dim = 100\n    num_pts = 10000\n    pt_stops = [100,1000,10000]\n    for dim in range(big_dim):\n        dim_eval = []\n        m_eval = []\n        for pt in range(num_pts):\n            # generate random direction using normalized gaussian                    \n            direction = np.random.randn(dim + 1,1)\n            norms = np.sqrt(np.sum(direction*direction,axis = 1))[:,np.newaxis]\n            direction = direction/norms   \n            \n            e = g(direction)\n            dim_eval.append(e)\n\n            # record mean and std of so many pts\n            if (pt+1) in pt_stops:\n                m_eval.append(np.min(dim_eval))\n        mean_evals.append(m_eval)\n\n    # convert to array for easy access\n    mean_evals_global = np.asarray(mean_evals)\n\n    fig = plt.figure(figsize = (6,3))\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 1, width_ratios=[1]) \n    fig.subplots_adjust(wspace=0.5,hspace=0.01)\n\n    # plot input function\n    ax = plt.subplot(gs[0])\n\n    for k in range(len(pt_stops)):\n        mean_evals = mean_evals_global[:,k]\n\n        # scatter plot mean value\n        ax.plot(np.arange(big_dim)+1,mean_evals)\n\n        # clean up plot - label axes, etc.,\n        ax.set_xlabel(\'dimension of input\')\n        ax.set_ylabel(\'funciton value\')\n\n    # draw legend\n    t = [str(p) for p in pt_stops]\n    ax.legend(t, bbox_to_anchor=(1, 0.5))\n\n    # draw horizontal axis\n    ax.plot(np.arange(big_dim) + 1,np.arange(big_dim)*0,linewidth=1,linestyle=\'--\',color = \'k\')\n    plt.show()\n\ndef random_local_experiment():\n    \'\'\'\n    Experiment illustrating the ultimate shortcoming of local random search.   Output is fraction of directions that are decreasing on a simple quadratic centered at the point [1,0,0...] as we increase the dimension of the function\n    \n    \'\'\'\n    # define symmetric quadratic N-dimensional\n    g = lambda w: np.dot(w.T,w)\n    \n    # loop over dimensions, sample points, evaluate\n    mean_evals = []\n    big_dim = 25\n    num_pts = 10000\n    pt_stops = [100,1000,10000]\n    for dim in range(big_dim):\n        # containers for evaluation\n        dim_eval = []\n        m_eval = []\n\n        # starting vector\n        start = np.zeros((dim+1,1))\n        start[0] = 1\n\n        for pt in range(num_pts):\n            # generate random point on n-sphere\n            r = np.random.randn(dim+1)\n            r.shape = (len(r),1)\n            pt_norm = math.sqrt(np.dot(r.T,r))\n            r = [b/pt_norm for b in r]\n            r+=start\n\n            # compare new direction to original point\n            if g(r) < g(start):\n                dim_eval.append(1)\n            else:\n                dim_eval.append(0)\n\n            # record mean and std of so many pts\n            if (pt+1) in pt_stops:\n                m_eval.append(np.mean(dim_eval))\n\n        # store average number of descent directions\n        mean_evals.append(m_eval)\n\n    # convert to array for easy access\n    mean_evals_global = np.asarray(mean_evals)\n\n    fig = plt.figure(figsize = (6,3))\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 1, width_ratios=[1]) \n    fig.subplots_adjust(wspace=0.5,hspace=0.01)\n\n    # plot input function\n    ax = plt.subplot(gs[0])\n\n    for k in range(len(pt_stops)):\n        mean_evals = mean_evals_global[:,k]\n\n        # scatter plot mean value\n        ax.plot(np.arange(big_dim)+1,mean_evals)\n\n        # clean up plot - label axes, etc.,\n        ax.set_xlabel(\'dimension of input\')\n        ax.set_ylabel(\'fraction of directions descennding\')\n\n    # draw legend\n    t = [str(p) for p in pt_stops]\n    ax.legend(t, bbox_to_anchor=(1, 0.5))\n\n    # draw horizontal axis\n    ax.plot(np.arange(big_dim) + 1,np.arange(big_dim)*0,linewidth=1,linestyle=\'--\',color = \'k\')\n\n    plt.show()'"
mlrefined_libraries/math_optimization_library/regularized_newtons_method.py,14,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nimport time\n\n# simple first order taylor series visualizer\nclass visualizer:\n    '''\n    Illustrating how to regularize Newton's method to deal with nonconvexity.  Using a custom slider\n    widget we can visualize the result of adding a pure weighted quadratic to the second derivative\n    at each step of Newton's method.  Each time the slider is moved a new complete run of regularized\n    Newton's method is illustrated, where at each step going from left to right the weight on the \n    pure quadratic is increased.\n    \n    For a non-convex function we can see how that - without reglarizing - we will climb to a local maximum,\n    since at each step the quadratic approximation is concave.  However if the regularization parameter is set\n    large enough the sum quadratic is made convex, and we can descend.  If the weight is made too high we \n    completely drown out second derivative and have gradient descent.\n    ''' \n    \n    def __init__(self,**args):\n        self.g = args['g']                          # input function\n        self.grad = compute_grad(self.g)            # first derivative of input function\n        self.hess = compute_grad(self.grad)         # second derivative of input function\n        self.w_init = float( -2.3)                  # initial point\n        self.w_hist = []\n        self.epsilon_range = np.linspace(0,2,20)       # range of regularization parameter to try\n        self.max_its = 10\n        \n    ######## newton's method ########\n    # run newton's method\n    def run_newtons_method(self,epsilon):\n        w_val = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w_val)\n        w_old = np.inf\n        for j in range(int(self.max_its)):\n            # update old w and index\n            w_old = w_val\n            \n            # plug in value into func and derivative\n            grad_val = float(self.grad(w_val))\n            hess_val = float(self.hess(w_val))\n\n            # take newtons step\n            curvature = hess_val + epsilon\n            if abs(curvature) > 10**-6:\n                w_val = w_val - grad_val/curvature\n            \n            # record\n            self.w_hist.append(w_val)\n\n    # animate the method\n    def animate_it(self,epsilon_range,savepath,**kwargs):\n        self.epsilon_range = epsilon_range\n        if 'w_init' in kwargs:\n            self.w_init = float(kwargs['w_init'])\n        if 'max_its' in kwargs:\n            self.max_its = float(kwargs['max_its'])\n        wmax = 3\n        if 'wmax' in kwargs:\n            wmax = kwargs['wmax']\n    \n        # initialize figure\n        fig = plt.figure(figsize = (10,4))\n        artist = fig\n        \n        gs = gridspec.GridSpec(1, 2, width_ratios=[2,1]) \n        ax1 = plt.subplot(gs[0],aspect = 'auto');\n        ax2 = plt.subplot(gs[1],sharey=ax1);\n\n        # generate function for plotting on each slide\n        w_plot = np.linspace(-wmax,wmax,1000)\n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)\n        ggap = g_range*0.5\n        w_vals = np.linspace(-2.5,2.5,50)\n \n        # animation sub-function\n        print ('starting animation rendering...')\n        num_frames = len(self.epsilon_range) + 1\n        def animate(k):\n            # clear the previous panel for next slide\n            ax1.cla()\n            ax2.cla()\n            \n            # plot function \n            ax1.plot(w_plot,g_plot,color = 'k',zorder = 0)               # plot function\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n                \n            # plot initial point and evaluation\n            if k == 0:\n                w_val = self.w_init\n                g_val = self.g(w_val)\n                ax1.scatter(w_val,g_val,s = 100,c = 'm',edgecolor = 'k',linewidth = 0.7,zorder = 2)            # plot point of tangency\n                ax1.scatter(w_val,0,s = 100,c = 'm',edgecolor = 'k',linewidth = 0.7, zorder = 2, marker = 'X')\n\n            # plot function alone first along with initial point\n            if k > 0:\n                epsilon = self.epsilon_range[k-1]\n                \n                # run gradient descent method\n                self.w_hist = []\n                self.run_newtons_method(epsilon)\n        \n                # colors for points\n                s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n                s.shape = (len(s),1)\n                t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n                t.shape = (len(t),1)\n                s = np.vstack((s,t))\n                self.colorspec = []\n                self.colorspec = np.concatenate((s,np.flipud(s)),1)\n                self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n                # plot everything for each iteration \n                for j in range(len(self.w_hist)):  \n                    w_val = self.w_hist[j]\n                    g_val = self.g(w_val)\n                    ax1.scatter(w_val,g_val,s = 90,c = self.colorspec[j],edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                    ax1.scatter(w_val,0,s = 90,facecolor = self.colorspec[j],marker = 'X',edgecolor = 'k',linewidth = 0.7, zorder = 2)\n                    \n                    # plug in value into func and derivative\n                    g_val = self.g(w_val)\n                    g_grad_val = self.grad(w_val)\n                    g_hess_val = self.hess(w_val)\n\n                    # determine width of plotting area for second order approximator\n                    width = 0.5\n                    if g_hess_val < 0:\n                        width = - width\n\n                    # compute second order approximation\n                    wrange = np.linspace(w_val - 3,w_val + 3, 100)\n                    h = g_val + g_grad_val*(wrange - w_val) + 0.5*(g_hess_val + epsilon)*(wrange - w_val)**2 \n\n                    # plot all\n                    ax1.plot(wrange,h,color = self.colorspec[j],linewidth = 2,alpha = 0.4,zorder = 1)      # plot approx\n            \n                    ### plot all on cost function decrease plot\n                    ax2.scatter(j,g_val,s = 90,c = self.colorspec[j],edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n                    \n                    # place title\n                    title = r'$\\epsilon = $' + r'{:.2f}'.format(epsilon)\n                    ax1.set_title(title,fontsize = 15)\n            \n                    # plot connector between points for visualization purposes\n                    if j > 0:\n                        w_old = self.w_hist[j-1]\n                        w_new = self.w_hist[j]\n                        g_old = self.g(w_old)\n                        g_new = self.g(w_new)\n                        ax2.plot([j-1,j],[g_old,g_new],color = self.colorspec[j],linewidth = 2,alpha = 0.4,zorder = 1)      # plot approx\n            else:\n                title = r'$\\,\\,\\,$'\n                ax1.set_title(title,fontsize = 15)\n\n            # clean up axis in each panel\n            ax2.set_xlabel('iteration',fontsize = 13)\n            ax2.set_ylabel(r'$g(w)$',fontsize = 13,labelpad = 15,rotation = 0)\n            ax1.set_xlabel(r'$w$',fontsize = 13)\n            ax1.set_ylabel(r'$g(w)$',fontsize = 13,labelpad = 15,rotation = 0)\n                    \n            # fix viewing limits\n            ax1.set_xlim([-wmax,wmax])\n            ax1.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            \n            ax2.set_xlim([-0.5,self.max_its + 0.5])\n            ax2.set_ylim([min(g_plot) - ggap,max(g_plot) + ggap])\n            \n            # set tickmarks\n            ax1.set_xticks(np.arange(-round(wmax), round(wmax) + 1, 1.0))\n            ax1.set_yticks(np.arange(round(min(g_plot) - ggap), round(max(g_plot) + ggap) + 1, 1.0))\n           \n            ax2.set_xticks(np.arange(0,self.max_its + 1, 1.0))\n                        \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate,frames=num_frames, interval=num_frames, blit=True)\n\n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()"""
mlrefined_libraries/math_optimization_library/second_order_majorizer.py,10,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nimport time\nfrom matplotlib import gridspec\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\n\nclass visualizer:\n    '''\n    Illustrate majorization of second order Taylor series approximations to a function\n    '''\n    def __init__(self,**args):\n        self.g = args['g']                       # input function\n        self.grad = compute_grad(self.g)         # gradient of input function\n        self.hess = compute_grad(self.grad)      # hessian of input function\n        self.colors = [[0,1,0.25],[0,0.75,1]]    # set of custom colors used for plotting\n\n    # compute first order approximation\n    def animate_it(self,savepath,**kwargs):\n        num_frames = 300                          # number of slides to create - the input range [-3,3] is divided evenly by this number\n        if 'num_frames' in kwargs:\n            num_frames = kwargs['num_frames']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (10,5))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5, 1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis('off');\n        ax3 = plt.subplot(gs[2]); ax3.axis('off');\n\n        # plot input function\n        ax = plt.subplot(gs[1],aspect = 'equal')\n        \n        max_val = 2.5\n        if 'max_val' in kwargs:\n            max_val = kwargs['max_val']\n        w_vals = np.linspace(-max_val+0.1,max_val-0.1,num_frames)       # range of values over which to plot first / second order approximations\n        \n        # generate a range of values over which to plot input function, and derivatives\n        w_plot = np.linspace(-max_val,max_val,1000)                  # input range for original function\n        \n        g_plot = self.g(w_plot)\n        g_range = max(g_plot) - min(g_plot)             # used for cleaning up final plot\n        ggap = g_range*0.1\n     \n        # animation sub-function\n        print ('starting animation rendering...')\n        def animate(k):\n            # clear the panel\n            ax.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            # grab the next input/output tangency pair, minimum of gradient quadratic\n            w_val = w_vals[k]\n            g_val = self.g(w_val)\n            grad_val = self.grad(w_val)\n            hess_val = self.hess(w_val)\n\n            # plot original function\n            ax.plot(w_plot,g_plot,color = 'k',zorder = 0,linewidth=1)                           # plot function\n            \n            # plot the input/output tangency point\n            ax.scatter(w_val,g_val,s = 60,c = 'r',edgecolor = 'k',linewidth = 0.7,marker = 'X',zorder = 3)            # plot point of tangency\n            ax.scatter(w_val,0,s = 80,c = 'r',edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n            \n            # plot visual aid for old point\n            tempy = np.linspace(0,g_val,100)\n            tempx = w_val*np.ones((100))\n            ax.plot(tempx,tempy,linewidth = 0.7,color = 'k',linestyle = '--',zorder = 1)\n\n            # plug input value into the second derivative\n            g_grad_val = self.grad(w_val)\n            g_hess_val = self.hess(w_val)\n\n            # determine width of plotting area for second order approximator\n            width = 1\n            if g_hess_val < 0:\n                width = - width\n\n            # setup quadratic formula params\n            a = 0.5*g_hess_val\n            b = g_grad_val - 2*0.5*g_hess_val*w_val\n            c = 0.5*g_hess_val*w_val**2 - g_grad_val*w_val - width\n\n            # solve for zero points of the quadratic (for plotting purposes only)\n            w1 = (-b + math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n            w2 = (-b - math.sqrt(b**2 - 4*a*c))/float(2*a + 0.00001)\n\n            # create the second order approximation\n            w_major = np.linspace(w1,w2, 1000)\n            h = lambda w: g_val + g_grad_val*(w - w_val) + 0.5*g_hess_val*(w - w_val)**2 \n            h_major = h(w_major)\n            \n            # compute minimum point\n            eps = 0\n            if abs(hess_val) < 10**-5:\n                eps = 10**-5\n            w_step = w_val - grad_val/(hess_val + eps)\n            h_step = h(w_step)\n            g_step = self.g(w_step)\n            \n            ax.scatter(w_step,h_step,s = 60,c = 'blue',edgecolor = 'k',linewidth = 0.7,marker = 'X',zorder = 3)           \n            ax.scatter(w_step,g_step,s = 60,c = 'lime',edgecolor = 'k',linewidth = 0.7,marker = 'X',zorder = 3)            # plot point of tangency\n            ax.scatter(w_step,0,s = 80,c = 'lime',edgecolor = 'k',linewidth = 0.7,zorder = 3)            # plot point of tangency\n            \n            # plot visual aid for new point\n            tempy = np.linspace(0,h_step,100)\n            tempx = w_step*np.ones((100))\n            ax.plot(tempx,tempy,linewidth = 0.7,color = 'k',linestyle = '--',zorder = 1)\n            \n            # plot approximation\n            ax.plot(w_major,h_major,color = self.colors[1],zorder = 1,linewidth=2)                           # plot function\n          \n            # label axes\n            ax.set_xlabel('$w$',fontsize = 12)\n            ax.set_ylabel('$g(w)$',fontsize = 12,rotation = 0,labelpad = 12)\n\n            # fix viewing limits on panel\n            ax.set_xlim([-max_val,max_val])\n            ax.set_ylim([min(-0.3,min(g_plot) - ggap),max(max(g_plot) + ggap,0.3)])\n            \n            # set tickmarks\n            ax.set_xticks(-np.arange(-round(max_val), round(max_val) + 1, 1.0))\n            ax.set_yticks(np.arange(round(min(g_plot) - ggap), round(max(g_plot) + ggap) + 1, 1.0))\n                \n            # set axis \n            ax.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n            \n            return artist,\n        \n        anim = animation.FuncAnimation(fig, animate,frames=len(w_vals), interval=len(w_vals), blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()"""
mlrefined_libraries/math_optimization_library/static_plotter.py,42,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport time\nimport copy\n\nclass Visualizer:\n    \'\'\'\n    Illustrate a run of your preferred optimization algorithm on a one or two-input function.  Run\n    the algorithm first, and input the resulting weight history into this wrapper.\n    \'\'\' \n\n    ##### draw picture of function and run for single-input function ####       \n    def single_input_plot(self,g,weight_histories,cost_histories,**kwargs):        \n        # adjust viewing range\n        wmin = -3.1\n        wmax = 3.1\n        if \'wmin\' in kwargs:            \n            wmin = kwargs[\'wmin\']\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\']\n            \n        onerun_perplot = False\n        if \'onerun_perplot\' in kwargs:\n            onerun_perplot = kwargs[\'onerun_perplot\']\n            \n        ### initialize figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n        \n        # remove whitespace from figure\n        #fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        #fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n        # create subplot with 2 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]); \n\n        ### plot function in both panels\n        w_plot = np.linspace(wmin,wmax,500)\n        g_plot = g(w_plot)\n        gmin = np.min(g_plot)\n        gmax = np.max(g_plot)\n        g_range = gmax - gmin\n        ggap = g_range*0.1\n        gmin -= ggap\n        gmax += ggap\n        \n        # plot function, axes lines\n        ax1.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n        ax1.axhline(y=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax1.axvline(x=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax1.set_xlabel(r\'$w$\',fontsize = 13)\n        ax1.set_ylabel(r\'$g(w)$\',fontsize = 13,rotation = 0,labelpad = 25)            \n        ax1.set_xlim(wmin,wmax)\n        ax1.set_ylim(gmin,gmax)\n        \n        ax2.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n        ax2.axhline(y=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax2.axvline(x=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax2.set_xlabel(r\'$w$\',fontsize = 13)\n        ax2.set_ylabel(r\'$g(w)$\',fontsize = 13,rotation = 0,labelpad = 25)   \n        ax2.set_xlim(wmin,wmax)\n        ax2.set_ylim(gmin,gmax)        \n       \n        #### loop over histories and plot each\n        for j in range(len(weight_histories)):\n            w_hist = weight_histories[j]\n            c_hist = cost_histories[j]\n            \n            # colors for points --> green as the algorithm begins, yellow as it converges, red at final point\n            s = np.linspace(0,1,len(w_hist[:round(len(w_hist)/2)]))\n            s.shape = (len(s),1)\n            t = np.ones(len(w_hist[round(len(w_hist)/2):]))\n            t.shape = (len(t),1)\n            s = np.vstack((s,t))\n            self.colorspec = []\n            self.colorspec = np.concatenate((s,np.flipud(s)),1)\n            self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n            \n            ### plot all history points\n            ax = ax2\n            if onerun_perplot == True:\n                if j == 0:\n                    ax = ax1\n                if j == 1:\n                    ax = ax2\n            for k in range(len(w_hist)):\n                # pick out current weight and function value from history, then plot\n                w_val = w_hist[k]\n                g_val = c_hist[k]\n                ax.scatter(w_val,g_val,s = 90,c = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4),zorder = 3,marker = \'X\')            # evaluation on function\n                ax.scatter(w_val,0,s = 90,facecolor = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4), zorder = 3)\n\n    ##### draw picture of function and run for two-input function ####       \n    def two_input_surface_contour_plot(self,g,w_hist,**kwargs):\n        ### input arguments ###        \n        num_contours = 10\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']\n           \n        view = [20,20]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n            \n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (11,5))\n        self.edgecolor = \'k\'\n\n        # create subplot with 3 panels, plot input function in center plot\n        # this seems to be the best option for whitespace management when using\n        # both a surface and contour plot in the same figure\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,10]) \n        ax1 = plt.subplot(gs[1],projection=\'3d\'); \n        ax2 = plt.subplot(gs[2],aspect=\'equal\'); \n        \n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n        \n        # plot 3d surface and path in left panel\n        self.draw_surface(g,ax1,**kwargs)\n        self.show_inputspace_path(w_hist,ax1)\n        ax1.view_init(view[0],view[1])\n        \n        ### make contour right plot - as well as horizontal and vertical axes ###\n        self.contour_plot_setup(g,ax2,**kwargs)  # draw contour plot\n        self.draw_weight_path(ax2,w_hist)              # draw path on contour plot\n\n        # plot\n        plt.show()\n \n\n\n    ##### draw picture of function and run for two-input function ####       \n    def two_input_original_contour_plot(self,g,**kwargs):\n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (10,4.5))\n\n        # create figure with single plot for contour\n        gs = gridspec.GridSpec(1, 1) \n        ax1 = plt.subplot(gs[0],aspect=\'equal\'); \n\n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n        \n        ### make contour right plot - as well as horizontal and vertical axes ###\n        self.contour_plot_setup(g,ax1,**kwargs)  # draw contour plot\n\n        # plot\n        plt.show()\n        \n        \n    ##### draw picture of function and run for two-input function ####       \n    def two_input_contour_plot(self,g,w_hist,**kwargs):\n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (10,4.5))\n        \n        # show original contour function as well?\n        show_original = True\n        if \'show_original\' in kwargs:\n            show_original = kwargs[\'show_original\']\n\n        # create figure with single plot for contour\n        gs = gridspec.GridSpec(1, 2) \n        ax1 = plt.subplot(gs[0],aspect=\'equal\'); \n        ax2 = plt.subplot(gs[1],aspect=\'equal\'); \n        \n        if show_original == False:\n            gs = gridspec.GridSpec(1, 1) \n            ax2 = plt.subplot(gs[0],aspect=\'equal\'); \n\n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n        \n        ### make contour right plot - as well as horizontal and vertical axes ###\n        self.contour_plot_setup(g,ax2,**kwargs)  # draw contour plot\n        self.edgecolor = \'k\'\n        self.draw_weight_path(ax2,w_hist,**kwargs)        # draw path on contour plot\n        \n        if show_original == True:\n            self.contour_plot_setup(g,ax1,**kwargs)  # draw contour plot\n\n        # plot\n        plt.show()\n \n    ##### draw picture of function and run for two-input function ####       \n    def two_input_contour_horiz_plots(self,g,histories,**kwargs):\n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (10,4.5))\n\n        # create figure with single plot for contour\n        num_plots = len(histories)\n        axs = gridspec.GridSpec(1, num_plots) \n        \n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n        \n        # define edgecolors \n        edgecolors = [\'k\',\'magenta\',\'aqua\',\'blueviolet\',\'chocolate\']\n        \n        # loop over histories and plot\n        for j in range(num_plots):\n            # get next weight history\n            w_hist = histories[j]\n            \n            # create subplot\n            ax = plt.subplot(axs[j],aspect=\'equal\'); \n\n            ### make contour right plot - as well as horizontal and vertical axes ###\n            self.contour_plot_setup(g,ax,**kwargs)           # draw contour plot\n            self.edgecolor = edgecolors[j]\n            self.draw_weight_path(ax,w_hist,**kwargs)        # draw path on contour plot\n\n        # plot\n        plt.show()\n\n    ##### draw picture of function and run for two-input function ####       \n    def two_input_contour_vert_plots(self,gs,histories,**kwargs):\n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (10,7))\n\n        # create figure with single plot for contour\n        num_plots = len(histories)\n        axs = gridspec.GridSpec(num_plots,1) \n        \n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n        \n        # define edgecolors \n        edgecolors = [\'k\',\'k\',\'k\',\'k\',\'k\']\n        \n        # loop over histories and plot\n        for j in range(num_plots):\n            # get next weight history\n            w_hist = histories[j]\n            g = gs[j]\n            \n            # create subplot\n            ax = plt.subplot(axs[j],aspect=\'equal\'); \n\n            ### make contour right plot - as well as horizontal and vertical axes ###\n            self.contour_plot_setup(g,ax,**kwargs)           # draw contour plot\n            self.edgecolor = edgecolors[j]\n            self.draw_weight_path(ax,w_hist,**kwargs)        # draw path on contour plot\n\n        # plot\n        plt.show()\n        \n    ##### draw picture of function and run for two-input function ####       \n    def compare_runs_contour_plots(self,g,weight_histories,**kwargs):\n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (10,4.5))\n        self.edgecolor = \'k\'\n         \n        # create figure with single plot for contour\n        gs = gridspec.GridSpec(1, 2) \n        ax1 = plt.subplot(gs[0],aspect=\'equal\'); \n        ax2 = plt.subplot(gs[1],aspect=\'equal\'); \n\n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n        \n        ### make contour right plot - as well as horizontal and vertical axes ###\n        self.contour_plot_setup(g,ax1,**kwargs)  # draw contour plot\n        w_hist = weight_histories[0]\n        self.draw_weight_path(ax1,w_hist)        # draw path on contour plot\n        \n        self.contour_plot_setup(g,ax2,**kwargs)  # draw contour plot\n        w_hist = weight_histories[1]\n        self.draw_weight_path(ax2,w_hist)        # draw path on contour plot\n        \n        # plot\n        plt.show()   \n        \n        \n    # compare cost histories from multiple runs\n    def plot_cost_histories(self,histories,start,**kwargs):\n        # plotting colors\n        colors = [\'k\',\'magenta\',\'aqua\',\'blueviolet\',\'chocolate\']\n        \n        # initialize figure\n        fig = plt.figure(figsize = (10,3))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); \n        \n        # any labels to add?        \n        labels = [\' \',\' \']\n        if \'labels\' in kwargs:\n            labels = kwargs[\'labels\']\n            \n        # plot points on cost function plot too?\n        points = False\n        if \'points\' in kwargs:\n            points = kwargs[\'points\']\n\n        # run through input histories, plotting each beginning at \'start\' iteration\n        for c in range(len(histories)):\n            history = histories[c]\n            label = 0\n            if c == 0:\n                label = labels[0]\n            else:\n                label = labels[1]\n                \n            # check if a label exists, if so add it to the plot\n            if np.size(label) == 0:\n                ax.plot(np.arange(start,len(history),1),history[start:],linewidth = 3*(0.8)**(c),color = colors[c]) \n            else:               \n                ax.plot(np.arange(start,len(history),1),history[start:],linewidth = 3*(0.8)**(c),color = colors[c],label = label) \n                \n            # check if points should be plotted for visualization purposes\n            if points == True:\n                ax.scatter(np.arange(start,len(history),1),history[start:],s = 90,color = colors[c],edgecolor = \'w\',linewidth = 2,zorder = 3) \n\n\n        # clean up panel\n        xlabel = \'step $k$\'\n        if \'xlabel\' in kwargs:\n            xlabel = kwargs[\'xlabel\']\n        ylabel = r\'$g\\left(\\mathbf{w}^k\\right)$\'\n        if \'ylabel\' in kwargs:\n            ylabel = kwargs[\'ylabel\']\n        ax.set_xlabel(xlabel,fontsize = 14)\n        ax.set_ylabel(ylabel,fontsize = 14,rotation = 0,labelpad = 25)\n        if np.size(label) > 0:\n            anchor = (1,1)\n            if \'anchor\' in kwargs:\n                anchor = kwargs[\'anchor\']\n            plt.legend(loc=\'upper right\', bbox_to_anchor=anchor)\n            #leg = ax.legend(loc=\'upper left\', bbox_to_anchor=(1.02, 1), borderaxespad=0)\n\n        ax.set_xlim([start - 0.5,len(history) - 0.5])\n        \n       # fig.tight_layout()\n        plt.show()\n\n    # get directions good\n    def plot_grad_directions(self,histories,**kwargs):\n        # loop over histories and plot grad directions\n        num_histories = len(histories)\n       \n        # construct figure\n        fig = plt.figure(figsize = (10,5))\n\n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n            \n        # create figure with single plot for contour\n        axs = gridspec.GridSpec(1,num_histories) \n        \n        for j in range(num_histories):\n            # create directions out of weight history\n            w_hist = histories[j]\n            num_weights = len(w_hist)\n            directions = []\n            for i in range(num_weights - 1):\n                w_old = w_hist[i]\n                w_new = w_hist[i+1]\n\n                # form direction \n                direction = w_new - w_old\n\n                # normalize\n                direction /= np.sqrt(np.sum([r**2 for r in direction]))\n\n                # store \n                directions.append(direction)\n\n            # plot directions as arrows\n            ax = plt.subplot(axs[j],aspect=\'equal\'); \n            self.draw_grads(ax,directions,**kwargs)        # draw path on contour plot\n\n            # set viewlimits\n            ax.set_xlim([-1.25,1.25])\n            ax.set_ylim([-1.25,1.25])\n\n        # plot\n        plt.show()\n\n    # get directions good\n    def plot_grad_directions_v2(self,history,**kwargs):\n        # loop over histories and plot grad directions\n        num_grads = np.minimum(len(history),9)\n       \n        # construct figure\n        fig = plt.figure(figsize = (6,6))\n\n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n            \n        # create figure with single plot for contour\n        axs = gridspec.GridSpec(3,3) \n        \n        # create directions out of weight history\n        directions = []\n        for i in range(len(history) - 1):\n            w_old = history[i]\n            w_new = history[i+1]\n\n            # form direction \n            direction = w_new - w_old\n\n            # normalize\n            direction /= np.sqrt(np.sum([r**2 for r in direction]))\n\n            # store \n            directions.append(direction)\n        \n        # plot directions as arrows\n        self.colorspec = self.make_colorspec(directions[:num_grads+1])\n        for j in range(num_grads):\n            ax = plt.subplot(axs[j],aspect=\'equal\'); \n            self.draw_grads_v2(ax,directions[:j+1],**kwargs)        # draw path on contour plot\n\n            # set viewlimits\n            ax.set_xlim([-1.25,1.25])\n            ax.set_ylim([-1.25,1.25])\n            \n            # set title\n            title = \'step \' + str(j+1) + \' direction\'\n            ax.set_title(title)\n\n        # plot\n        plt.show()\n        \n    ########################################################################################\n    #### utility functions - for setting up / making contour plots, 3d surface plots, etc., ####\n    # show contour plot of input function\n    def contour_plot_setup(self,g,ax,**kwargs):\n        xmin = -3.1\n        xmax = 3.1\n        ymin = -3.1\n        ymax = 3.1\n        if \'xmin\' in kwargs:            \n            xmin = kwargs[\'xmin\']\n        if \'xmax\' in kwargs:\n            xmax = kwargs[\'xmax\']\n        if \'ymin\' in kwargs:            \n            ymin = kwargs[\'ymin\']\n        if \'ymax\' in kwargs:\n            ymax = kwargs[\'ymax\']      \n        num_contours = 20\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']   \n            \n        # choose viewing range using weight history?\n        if \'view_by_weights\' in kwargs:\n            view_by_weights = True\n            weight_history = kwargs[\'weight_history\']\n            if view_by_weights == True:\n                xmin = min([v[0] for v in weight_history])[0]\n                xmax = max([v[0] for v in weight_history])[0]\n                xgap = (xmax - xmin)*0.25\n                xmin -= xgap\n                xmax += xgap\n\n                ymin = min([v[1] for v in weight_history])[0]\n                ymax = max([v[1] for v in weight_history])[0]\n                ygap = (ymax - ymin)*0.25\n                ymin -= ygap\n                ymax += ygap\n \n        ### plot function as contours ###\n        self.draw_contour_plot(g,ax,num_contours,xmin,xmax,ymin,ymax)\n        \n        ### cleanup panel ###\n        ax.set_xlabel(\'$w_0$\',fontsize = 14)\n        ax.set_ylabel(\'$w_1$\',fontsize = 14,labelpad = 15,rotation = 0)\n        ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        # ax.set_xticks(np.arange(round(xmin),round(xmax)+1))\n        # ax.set_yticks(np.arange(round(ymin),round(ymax)+1))\n        \n        # set viewing limits\n        ax.set_xlim(xmin,xmax)\n        ax.set_ylim(ymin,ymax)\n\n    ### function for creating contour plot\n    def draw_contour_plot(self,g,ax,num_contours,xmin,xmax,ymin,ymax):\n        #### define input space for function and evaluate ####\n        w1 = np.linspace(xmin,xmax,400)\n        w2 = np.linspace(ymin,ymax,400)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([ g(np.reshape(s,(2,1))) for s in h])\n\n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n        \n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cutoff = 1\n        cutoff = (levelmax - levelmin)*cutoff\n        numper = 4\n        levels1 = np.linspace(cutoff,levelmax,numper)\n        num_contours -= numper\n\n        # produce generic contours\n        levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n        levels = np.unique(np.append(levels1,levels2))\n        num_contours -= numper\n        while num_contours > 0:\n            cutoff = levels[1]\n            levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n            levels = np.unique(np.append(levels2,levels))\n            num_contours -= numper\n   \n        # plot the contours\n        ax.contour(w1_vals, w2_vals, func_vals,levels = levels[1:],colors = \'k\')\n        ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = \'Blues\')\n\n        ###### clean up plot ######\n        ax.set_xlabel(\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        \n        \n    ### makes color spectrum for plotted run points - from green (start) to red (stop)\n    def make_colorspec(self,w_hist):\n        # make color range for path\n        s = np.linspace(0,1,len(w_hist[:round(len(w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(w_hist[round(len(w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        colorspec = []\n        colorspec = np.concatenate((s,np.flipud(s)),1)\n        colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n        return colorspec\n\n\n    ### function for drawing weight history path\n    def draw_grads(self,ax,directions,**kwargs):\n        # make colors for plot\n        colorspec = self.make_colorspec(directions)\n\n        arrows = True\n        if \'arrows\' in kwargs:\n            arrows = kwargs[\'arrows\']\n            \n        # plot axes\n        ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        \n        ### plot function decrease plot in right panel\n        for j in range(len(directions)):  \n            # get current direction\n            direction = directions[j]\n            \n            # draw arrows connecting pairwise points\n            head_length = 0.1\n            head_width = 0.1\n            ax.arrow(0,0,direction[0],direction[1], head_width=head_width, head_length=head_length, fc=\'k\', ec=\'k\',linewidth=1,zorder = 2,length_includes_head=True)\n            ax.arrow(0,0,direction[0],direction[1], head_width=0.1, head_length=head_length, fc=colorspec[j], ec=colorspec[j],linewidth=0.25,zorder = 2,length_includes_head=True)\n\n    ### function for drawing weight history path\n    def draw_grads_v2(self,ax,directions,**kwargs):\n        arrows = True\n        if \'arrows\' in kwargs:\n            arrows = kwargs[\'arrows\']\n            \n        # plot axes\n        ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        \n        ### plot function decrease plot in right panel\n        head_length = 0.1\n        head_width = 0.1\n        alpha = 0.1\n        for j in range(len(directions)-1):  \n            # get current direction\n            direction = directions[j]\n            \n            # draw arrows connecting pairwise points\n            ax.arrow(0,0,direction[0],direction[1], head_width=head_width, head_length=head_length, fc=\'k\', ec=\'k\',linewidth=3.5,zorder = 2,length_includes_head=True,alpha = alpha)\n            ax.arrow(0,0,direction[0],direction[1], head_width=0.1, head_length=head_length, fc=self.colorspec[j], ec=self.colorspec[j],linewidth=3,zorder = 2,length_includes_head=True,alpha = alpha)\n            \n        # plot most recent direction\n        direction = directions[-1]\n        num_dirs = len(directions)\n  \n        # draw arrows connecting pairwise points\n        ax.arrow(0,0,direction[0],direction[1], head_width=head_width, head_length=head_length, fc=\'k\', ec=\'k\',linewidth=4,zorder = 2,length_includes_head=True)\n        ax.arrow(0,0,direction[0],direction[1], head_width=0.1, head_length=head_length, fc=self.colorspec[num_dirs], ec=self.colorspec[num_dirs],linewidth=3,zorder = 2,length_includes_head=True)            \n            \n    ### function for drawing weight history path\n    def draw_weight_path(self,ax,w_hist,**kwargs):\n        # make colors for plot\n        colorspec = self.make_colorspec(w_hist)\n        \n        arrows = True\n        if \'arrows\' in kwargs:\n            arrows = kwargs[\'arrows\']\n\n        ### plot function decrease plot in right panel\n        for j in range(len(w_hist)):  \n            w_val = w_hist[j]\n\n            # plot each weight set as a point\n            ax.scatter(w_val[0],w_val[1],s = 80,c = colorspec[j],edgecolor = self.edgecolor,linewidth = 2*math.sqrt((1/(float(j) + 1))),zorder = 3)\n\n            # plot connector between points for visualization purposes\n            if j > 0:\n                pt1 = w_hist[j-1]\n                pt2 = w_hist[j]\n                \n                # produce scalar for arrow head length\n                pt_length = np.linalg.norm(pt1 - pt2)\n                head_length = 0.1\n                alpha = (head_length - 0.35)/pt_length + 1\n                \n                # if points are different draw error\n                if np.linalg.norm(pt1 - pt2) > head_length and arrows == True:\n                    if np.ndim(pt1) > 1:\n                        pt1 = pt1.flatten()\n                        pt2 = pt2.flatten()\n                        \n                        \n                    # draw color connectors for visualization\n                    w_old = pt1\n                    w_new = pt2\n                    ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = colorspec[j],linewidth = 2,alpha = 1,zorder = 2)      # plot approx\n                    ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = \'k\',linewidth = 3,alpha = 1,zorder = 1)      # plot approx\n                \n                \n                    # draw arrows connecting pairwise points\n                    #ax.arrow(pt1[0],pt1[1],(pt2[0] - pt1[0])*alpha,(pt2[1] - pt1[1])*alpha, head_width=0.1, head_length=head_length, fc=\'k\', ec=\'k\',linewidth=4,zorder = 2,length_includes_head=True)\n                    #ax.arrow(pt1[0],pt1[1],(pt2[0] - pt1[0])*alpha,(pt2[1] - pt1[1])*alpha, head_width=0.1, head_length=head_length, fc=\'w\', ec=\'w\',linewidth=0.25,zorder = 2,length_includes_head=True)\n        \n    ### draw surface plot\n    def draw_surface(self,g,ax,**kwargs):\n        xmin = -3.1\n        xmax = 3.1\n        ymin = -3.1\n        ymax = 3.1\n        if \'xmin\' in kwargs:            \n            xmin = kwargs[\'xmin\']\n        if \'xmax\' in kwargs:\n            xmax = kwargs[\'xmax\']\n        if \'ymin\' in kwargs:            \n            ymin = kwargs[\'ymin\']\n        if \'ymax\' in kwargs:\n            ymax = kwargs[\'ymax\']   \n            \n        #### define input space for function and evaluate ####\n        w1 = np.linspace(xmin,xmax,200)\n        w2 = np.linspace(ymin,ymax,200)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([g(np.reshape(s,(2,1))) for s in h])\n\n        ### plot function as surface ### \n        w1_vals.shape = (len(w1),len(w2))\n        w2_vals.shape = (len(w1),len(w2))\n        func_vals.shape = (len(w1),len(w2))\n        ax.plot_surface(w1_vals, w2_vals, func_vals, alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)\n\n        # plot z=0 plane \n        ax.plot_surface(w1_vals, w2_vals, func_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n                \n        # clean up axis\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        \n        ax.set_xlabel(\'$w_0$\',fontsize = 14)\n        ax.set_ylabel(\'$w_1$\',fontsize = 14,rotation = 0)\n        ax.set_title(\'$g(w_0,w_1)$\',fontsize = 14)\n        \n\n    ### plot points and connectors in input space in 3d plot        \n    def show_inputspace_path(self,w_hist,ax):\n        # make colors for plot\n        colorspec = self.make_colorspec(w_hist)\n        \n        for k in range(len(w_hist)):\n            pt1 = w_hist[k]\n            ax.scatter(pt1[0],pt1[1],0,s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 0.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n            if k < len(w_hist)-1:\n                pt2 = w_hist[k+1]\n                if np.linalg.norm(pt1 - pt2) > 10**(-3):\n                    # draw arrow in left plot\n                    a = Arrow3D([pt1[0],pt2[0]], [pt1[1],pt2[1]], [0, 0], mutation_scale=10, lw=2, arrowstyle=""-|>"", color=""k"")\n                    ax.add_artist(a)\n        \n#### custom 3d arrow and annotator functions ###    \n# nice arrow maker from https://stackoverflow.com/questions/11140163/python-matplotlib-plotting-a-3d-cube-a-sphere-and-a-vector\nclass Arrow3D(FancyArrowPatch):\n\n    def __init__(self, xs, ys, zs, *args, **kwargs):\n        FancyArrowPatch.__init__(self, (0, 0), (0, 0), *args, **kwargs)\n        self._verts3d = xs, ys, zs\n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n        FancyArrowPatch.draw(self, renderer)'"
mlrefined_libraries/math_optimization_library/static_plotter_old.py,35,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport time\n\nclass Visualizer:\n    \'\'\'\n    Illustrate a run of your preferred optimization algorithm on a one or two-input function.  Run\n    the algorithm first, and input the resulting weight history into this wrapper.\n    \'\'\' \n\n    ##### draw picture of function and run for single-input function ####       \n    def single_input_plot(self,g,weight_histories,cost_histories,**kwargs):        \n        # adjust viewing range\n        wmin = -3.1\n        wmax = 3.1\n        if \'wmin\' in kwargs:            \n            wmin = kwargs[\'wmin\']\n        if \'wmax\' in kwargs:\n            wmax = kwargs[\'wmax\']\n            \n        onerun_perplot = False\n        if \'onerun_perplot\' in kwargs:\n            onerun_perplot = kwargs[\'onerun_perplot\']\n            \n        ### initialize figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n        \n        # remove whitespace from figure\n        #fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        #fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n        # create subplot with 2 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]); \n\n        ### plot function in both panels\n        w_plot = np.linspace(wmin,wmax,500)\n        g_plot = g(w_plot)\n        gmin = np.min(g_plot)\n        gmax = np.max(g_plot)\n        g_range = gmax - gmin\n        ggap = g_range*0.1\n        gmin -= ggap\n        gmax += ggap\n        \n        # plot function, axes lines\n        ax1.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n        ax1.axhline(y=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax1.axvline(x=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax1.set_xlabel(r\'$w$\',fontsize = 13)\n        ax1.set_ylabel(r\'$g(w)$\',fontsize = 13,rotation = 0,labelpad = 25)            \n        ax1.set_xlim(wmin,wmax)\n        ax1.set_ylim(gmin,gmax)\n        \n        ax2.plot(w_plot,g_plot,color = \'k\',zorder = 2)                           # plot function\n        ax2.axhline(y=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax2.axvline(x=0, color=\'k\',zorder = 1,linewidth = 0.25)\n        ax2.set_xlabel(r\'$w$\',fontsize = 13)\n        ax2.set_ylabel(r\'$g(w)$\',fontsize = 13,rotation = 0,labelpad = 25)   \n        ax2.set_xlim(wmin,wmax)\n        ax2.set_ylim(gmin,gmax)        \n       \n        #### loop over histories and plot each\n        for j in range(len(weight_histories)):\n            w_hist = weight_histories[j]\n            c_hist = cost_histories[j]\n            \n            # colors for points --> green as the algorithm begins, yellow as it converges, red at final point\n            s = np.linspace(0,1,len(w_hist[:round(len(w_hist)/2)]))\n            s.shape = (len(s),1)\n            t = np.ones(len(w_hist[round(len(w_hist)/2):]))\n            t.shape = (len(t),1)\n            s = np.vstack((s,t))\n            self.colorspec = []\n            self.colorspec = np.concatenate((s,np.flipud(s)),1)\n            self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n            \n            ### plot all history points\n            ax = ax2\n            if onerun_perplot == True:\n                if j == 0:\n                    ax = ax1\n                if j == 1:\n                    ax = ax2\n            for k in range(len(w_hist)):\n                # pick out current weight and function value from history, then plot\n                w_val = w_hist[k]\n                g_val = c_hist[k]\n                ax.scatter(w_val,g_val,s = 90,c = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4),zorder = 3,marker = \'X\')            # evaluation on function\n                ax.scatter(w_val,0,s = 90,facecolor = self.colorspec[k],edgecolor = \'k\',linewidth = 0.5*((1/(float(k) + 1)))**(0.4), zorder = 3)\n\n    ##### draw picture of function and run for two-input function ####       \n    def two_input_surface_contour_plot(self,g,w_hist,**kwargs):\n        ### input arguments ###        \n        num_contours = 10\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']\n           \n        view = [20,20]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n            \n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (11,5))\n\n        # create subplot with 3 panels, plot input function in center plot\n        # this seems to be the best option for whitespace management when using\n        # both a surface and contour plot in the same figure\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,10]) \n        ax1 = plt.subplot(gs[1],projection=\'3d\'); \n        ax2 = plt.subplot(gs[2],aspect=\'equal\'); \n        \n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n        \n        # plot 3d surface and path in left panel\n        self.draw_surface(g,ax1,**kwargs)\n        self.show_inputspace_path(w_hist,ax1)\n        ax1.view_init(view[0],view[1])\n        \n        ### make contour right plot - as well as horizontal and vertical axes ###\n        self.contour_plot_setup(g,ax2,**kwargs)  # draw contour plot\n        self.draw_weight_path(ax2,w_hist)              # draw path on contour plot\n\n        # plot\n        plt.show()\n        \n    ##### draw picture of function and run for two-input function ####       \n    def two_input_contour_plot(self,g,w_hist,**kwargs):\n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (10,4.5))\n        \n        # show original contour function as well?\n        show_original = True\n        if \'show_original\' in kwargs:\n            show_original = kwargs[\'show_original\']\n\n        # create figure with single plot for contour\n        gs = gridspec.GridSpec(1, 2) \n        ax1 = plt.subplot(gs[0],aspect=\'equal\'); \n        ax2 = plt.subplot(gs[1],aspect=\'equal\'); \n        \n        if show_original == False:\n            gs = gridspec.GridSpec(1, 1) \n            ax2 = plt.subplot(gs[0],aspect=\'equal\'); \n\n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n        \n        ### make contour right plot - as well as horizontal and vertical axes ###\n        self.contour_plot_setup(g,ax2,**kwargs)  # draw contour plot\n        self.draw_weight_path(ax2,w_hist,**kwargs)        # draw path on contour plot\n        \n        if show_original == True:\n            self.contour_plot_setup(g,ax1,**kwargs)  # draw contour plot\n\n        # plot\n        plt.show()\n        \n    ##### draw picture of function and run for two-input function ####       \n    def compare_runs_contour_plots(self,g,weight_histories,**kwargs):\n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (10,4.5))\n         \n        # create figure with single plot for contour\n        gs = gridspec.GridSpec(1, 2) \n        ax1 = plt.subplot(gs[0],aspect=\'equal\'); \n        ax2 = plt.subplot(gs[1],aspect=\'equal\'); \n\n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n        \n        ### make contour right plot - as well as horizontal and vertical axes ###\n        self.contour_plot_setup(g,ax1,**kwargs)  # draw contour plot\n        w_hist = weight_histories[0]\n        self.draw_weight_path(ax1,w_hist)        # draw path on contour plot\n        \n        self.contour_plot_setup(g,ax2,**kwargs)  # draw contour plot\n        w_hist = weight_histories[1]\n        self.draw_weight_path(ax2,w_hist)        # draw path on contour plot\n        \n        # plot\n        plt.show()   \n        \n        \n    # compare cost histories from multiple runs\n    def plot_cost_histories(self,histories,start,**kwargs):\n        # plotting colors\n        colors = [\'k\',\'magenta\',\'springgreen\',\'blueviolet\',\'chocolate\']\n        \n        # initialize figure\n        fig = plt.figure(figsize = (10,3))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); \n        \n        # any labels to add?        \n        labels = [\' \',\' \']\n        if \'labels\' in kwargs:\n            labels = kwargs[\'labels\']\n            \n        # plot points on cost function plot too?\n        points = False\n        if \'points\' in kwargs:\n            points = kwargs[\'points\']\n\n        # run through input histories, plotting each beginning at \'start\' iteration\n        for c in range(len(histories)):\n            history = histories[c]\n            label = 0\n            if c == 0:\n                label = labels[0]\n            else:\n                label = labels[1]\n                \n            # check if a label exists, if so add it to the plot\n            if np.size(label) == 0:\n                ax.plot(np.arange(start,len(history),1),history[start:],linewidth = 3*(0.8)**(c),color = colors[c]) \n            else:               \n                ax.plot(np.arange(start,len(history),1),history[start:],linewidth = 3*(0.8)**(c),color = colors[c],label = label) \n                \n            # check if points should be plotted for visualization purposes\n            if points == True:\n                ax.scatter(np.arange(start,len(history),1),history[start:],s = 90,color = colors[c],edgecolor = \'w\',linewidth = 2,zorder = 3) \n\n\n        # clean up panel\n        xlabel = \'step $k$\'\n        if \'xlabel\' in kwargs:\n            xlabel = kwargs[\'xlabel\']\n        ylabel = r\'$g\\left(\\mathbf{w}^k\\right)$\'\n        if \'ylabel\' in kwargs:\n            ylabel = kwargs[\'ylabel\']\n        ax.set_xlabel(xlabel,fontsize = 14)\n        ax.set_ylabel(ylabel,fontsize = 14,rotation = 0,labelpad = 25)\n        if np.size(label) > 0:\n            anchor = (1,1)\n            if \'anchor\' in kwargs:\n                anchor = kwargs[\'anchor\']\n            plt.legend(loc=\'upper right\', bbox_to_anchor=anchor)\n            #leg = ax.legend(loc=\'upper left\', bbox_to_anchor=(1.02, 1), borderaxespad=0)\n\n        ax.set_xlim([start - 0.5,len(history) - 0.5])\n \n        \n       # fig.tight_layout()\n        plt.show()\n\n\n    ########################################################################################\n    #### utility functions - for setting up / making contour plots, 3d surface plots, etc., ####\n    # show contour plot of input function\n    def contour_plot_setup(self,g,ax,**kwargs):\n        xmin = -3.1\n        xmax = 3.1\n        ymin = -3.1\n        ymax = 3.1\n        if \'xmin\' in kwargs:            \n            xmin = kwargs[\'xmin\']\n        if \'xmax\' in kwargs:\n            xmax = kwargs[\'xmax\']\n        if \'ymin\' in kwargs:            \n            ymin = kwargs[\'ymin\']\n        if \'ymax\' in kwargs:\n            ymax = kwargs[\'ymax\']      \n        num_contours = 20\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']   \n            \n        # choose viewing range using weight history?\n        if \'view_by_weights\' in kwargs:\n            view_by_weights = True\n            weight_history = kwargs[\'weight_history\']\n            if view_by_weights == True:\n                xmin = min([v[0] for v in weight_history])[0]\n                xmax = max([v[0] for v in weight_history])[0]\n                xgap = (xmax - xmin)*0.25\n                xmin -= xgap\n                xmax += xgap\n\n                ymin = min([v[1] for v in weight_history])[0]\n                ymax = max([v[1] for v in weight_history])[0]\n                ygap = (ymax - ymin)*0.25\n                ymin -= ygap\n                ymax += ygap\n \n        ### plot function as contours ###\n        self.draw_contour_plot(g,ax,num_contours,xmin,xmax,ymin,ymax)\n        \n        ### cleanup panel ###\n        ax.set_xlabel(\'$w_0$\',fontsize = 14)\n        ax.set_ylabel(\'$w_1$\',fontsize = 14,labelpad = 15,rotation = 0)\n        ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        # ax.set_xticks(np.arange(round(xmin),round(xmax)+1))\n        # ax.set_yticks(np.arange(round(ymin),round(ymax)+1))\n        \n        # set viewing limits\n        ax.set_xlim(xmin,xmax)\n        ax.set_ylim(ymin,ymax)\n\n    ### function for creating contour plot\n    def draw_contour_plot(self,g,ax,num_contours,xmin,xmax,ymin,ymax):\n        #### define input space for function and evaluate ####\n        w1 = np.linspace(xmin,xmax,400)\n        w2 = np.linspace(ymin,ymax,400)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([ g(np.reshape(s,(2,1))) for s in h])\n\n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cut = 0.4\n        cutoff = (levelmax - levelmin)\n        levels = [levelmin + cutoff*cut**(num_contours - i) for i in range(0,num_contours+1)]\n        levels = [levelmin] + levels\n        levels = np.asarray(levels)\n   \n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = \'k\')\n        b = ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = \'Blues\')\n        \n        \n    ### makes color spectrum for plotted run points - from green (start) to red (stop)\n    def make_colorspec(self,w_hist):\n        # make color range for path\n        s = np.linspace(0,1,len(w_hist[:round(len(w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(w_hist[round(len(w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        colorspec = []\n        colorspec = np.concatenate((s,np.flipud(s)),1)\n        colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n        return colorspec\n        \n    ### function for drawing weight history path\n    def draw_weight_path(self,ax,w_hist,**kwargs):\n        # make colors for plot\n        colorspec = self.make_colorspec(w_hist)\n        \n        arrows = True\n        if \'arrows\' in kwargs:\n            arrows = kwargs[\'arrows\']\n\n        ### plot function decrease plot in right panel\n        for j in range(len(w_hist)):  \n            w_val = w_hist[j]\n\n            # plot each weight set as a point\n            ax.scatter(w_val[0],w_val[1],s = 80,c = colorspec[j],edgecolor = \'k\',linewidth = 2*math.sqrt((1/(float(j) + 1))),zorder = 3)\n\n            # plot connector between points for visualization purposes\n            if j > 0:\n                pt1 = w_hist[j-1]\n                pt2 = w_hist[j]\n                \n                # produce scalar for arrow head length\n                pt_length = np.linalg.norm(pt1 - pt2)\n                head_length = 0.1\n                alpha = (head_length - 0.35)/pt_length + 1\n                \n                # if points are different draw error\n                if np.linalg.norm(pt1 - pt2) > head_length and arrows == True:\n                    if np.ndim(pt1) > 1:\n                        pt1 = pt1.flatten()\n                        pt2 = pt2.flatten()\n                    \n                    ax.arrow(pt1[0],pt1[1],(pt2[0] - pt1[0])*alpha,(pt2[1] - pt1[1])*alpha, head_width=0.1, head_length=head_length, fc=\'k\', ec=\'k\',linewidth=4,zorder = 2,length_includes_head=True)\n                    ax.arrow(pt1[0],pt1[1],(pt2[0] - pt1[0])*alpha,(pt2[1] - pt1[1])*alpha, head_width=0.1, head_length=head_length, fc=\'w\', ec=\'w\',linewidth=0.25,zorder = 2,length_includes_head=True)\n        \n    ### draw surface plot\n    def draw_surface(self,g,ax,**kwargs):\n        xmin = -3.1\n        xmax = 3.1\n        ymin = -3.1\n        ymax = 3.1\n        if \'xmin\' in kwargs:            \n            xmin = kwargs[\'xmin\']\n        if \'xmax\' in kwargs:\n            xmax = kwargs[\'xmax\']\n        if \'ymin\' in kwargs:            \n            ymin = kwargs[\'ymin\']\n        if \'ymax\' in kwargs:\n            ymax = kwargs[\'ymax\']   \n            \n        #### define input space for function and evaluate ####\n        w1 = np.linspace(xmin,xmax,200)\n        w2 = np.linspace(ymin,ymax,200)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([g(np.reshape(s,(2,1))) for s in h])\n\n        ### plot function as surface ### \n        w1_vals.shape = (len(w1),len(w2))\n        w2_vals.shape = (len(w1),len(w2))\n        func_vals.shape = (len(w1),len(w2))\n        ax.plot_surface(w1_vals, w2_vals, func_vals, alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)\n\n        # plot z=0 plane \n        ax.plot_surface(w1_vals, w2_vals, func_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n                \n        # clean up axis\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        \n        ax.set_xlabel(\'$w_0$\',fontsize = 14)\n        ax.set_ylabel(\'$w_1$\',fontsize = 14,rotation = 0)\n        ax.set_title(\'$g(w_0,w_1)$\',fontsize = 14)\n        \n\n    ### plot points and connectors in input space in 3d plot        \n    def show_inputspace_path(self,w_hist,ax):\n        # make colors for plot\n        colorspec = self.make_colorspec(w_hist)\n        \n        for k in range(len(w_hist)):\n            pt1 = w_hist[k]\n            ax.scatter(pt1[0],pt1[1],0,s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 0.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n            if k < len(w_hist)-1:\n                pt2 = w_hist[k+1]\n                if np.linalg.norm(pt1 - pt2) > 10**(-3):\n                    # draw arrow in left plot\n                    a = Arrow3D([pt1[0],pt2[0]], [pt1[1],pt2[1]], [0, 0], mutation_scale=10, lw=2, arrowstyle=""-|>"", color=""k"")\n                    ax.add_artist(a)\n        \n#### custom 3d arrow and annotator functions ###    \n# nice arrow maker from https://stackoverflow.com/questions/11140163/python-matplotlib-plotting-a-3d-cube-a-sphere-and-a-vector\nclass Arrow3D(FancyArrowPatch):\n\n    def __init__(self, xs, ys, zs, *args, **kwargs):\n        FancyArrowPatch.__init__(self, (0, 0), (0, 0), *args, **kwargs)\n        self._verts3d = xs, ys, zs\n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n        FancyArrowPatch.draw(self, renderer)'"
mlrefined_libraries/math_optimization_library/steepest_descent.py,33,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport time\n\nclass visualizer:\n    '''\n    Compares steepest descent using L1, L2, and Linfinity norms.\n    ''' \n     \n    ######## gradient descent ########\n    # run gradient descent \n    def run_gradient_descent(self):\n        w = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w)\n        j = 0\n        for j in range(int(self.max_its)):            \n            # plug in value into func and derivative\n            grad_eval = self.grad(w)\n\n            ### L1, L2, or Linf? L2 by default ###\n            # L1 steepest descent\n            if self.version == 'L1':\n                # take absolute value of each entry in grad vector\n                grad_abs = np.abs(grad_eval)\n                best_val = np.max(grad_abs)\n                ind_best = np.argwhere(grad_abs == best_val)\n                new_grad = np.zeros((len(grad_eval)))\n                new_grad[ind_best] = np.sign(grad_eval[ind_best])\n                grad_eval = new_grad\n    \n            # Linf steepest descent\n            elif self.version == 'Linf':\n                grad_eval = np.sign(grad_eval)\n                \n            # normaize direction, if everything is perfectly zero then move in random direction\n            grad_norm = np.linalg.norm(grad_eval)\n            if grad_norm == 0:\n                grad_eval = 2*np.random.rand(len(w)) - 1\n                grad_norm = np.linalg.norm(grad_eval)\n                grad_eval /= grad_norm\n            else:\n                grad_eval /= grad_norm\n               \n            ### check what sort of steplength rule to employ ###\n            alpha = 0\n            if self.steplength == 'diminishing':\n                alpha = 1/(1 + j)\n            elif self.steplength == 'backtracking':\n                alpha = self.backtracking(w,grad_eval)\n            elif self.steplength == 'exact': \n                alpha = self.exact(w,grad_eval)\n            else:\n                alpha = float(self.steplength)            \n            \n            # take gradient descent step\n            w = w - alpha*grad_eval\n            \n            # record\n            self.w_hist.append(w)\n\n    # backtracking linesearch module\n    def backtracking(self,w,grad_eval):\n        # set input parameters\n        alpha = 1\n        t = 0.5\n        \n        # compute initial function and gradient values\n        func_eval = self.g(w)\n        grad_norm = np.linalg.norm(grad_eval)**2\n        \n        # loop over and tune steplength\n        while self.g(w - alpha*grad_eval) > func_eval - alpha*0.5*grad_norm:\n            alpha = t*alpha\n        return alpha\n\n    # exact linesearch module\n    def exact(self,w,grad_eval):\n        # set parameters of linesearch at each step\n        valmax = 10\n        num_evals = 3000\n        \n        # set alpha range\n        alpha_range = np.linspace(0,valmax,num_evals)\n        \n        # evaluate function over direction and alpha range, grab alpha giving lowest eval\n        steps = [(w - alpha*grad_eval) for alpha in alpha_range]\n        func_evals = np.array([self.g(s) for s in steps])\n        ind = np.argmin(func_evals)\n        best_alpha = alpha_range[ind]\n        \n        return best_alpha\n    \n    # visualize descent on multi-input function\n    def L2(self,g,w_init,steplength,max_its,**kwargs):\n        ### input arguments ###        \n        self.g = g\n        self.max_its = max_its\n        self.grad = compute_grad(self.g)              # gradient of input function\n            \n        plot_final = False\n        if 'plot_final' in kwargs:\n            plot_final = kwargs['plot_final']\n\n        num_contours = 15\n        if 'num_contours' in kwargs:\n            num_contours = kwargs['num_contours']\n\n        # version of gradient descent to use (normalized or unnormalized)\n        self.version = 'unnormalized'\n        if 'version' in kwargs:\n            self.version = kwargs['version']\n        \n        # steplength\n        self.steplength = steplength\n            \n        # get initial point \n        self.w_init = np.asarray([float(s) for s in w_init])\n            \n        # take in user defined maximum number of iterations\n        self.max_its = max_its\n           \n        # loop over steplengths, plot panels for each\n        count = 0\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(11,5))\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,1,1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis('off')\n        ax2 = plt.subplot(gs[1],aspect = 'equal'); \n        ax3 = plt.subplot(gs[2]); ax3.axis('off')\n\n        #### run local random search algorithm ####\n        \n        # run L2 and plot\n        self.version = 'L2'\n        self.run_gradient_descent()\n        title = '$L_2$ steepest descent'\n        self.draw_panel(ax2,title,**kwargs)\n        \n         \n    # visualize descent on multi-input function\n    def L1(self,g,w_init,steplength,max_its,**kwargs):\n        ### input arguments ###        \n        self.g = g\n        self.max_its = max_its\n        self.grad = compute_grad(self.g)              # gradient of input function\n            \n        plot_final = False\n        if 'plot_final' in kwargs:\n            plot_final = kwargs['plot_final']\n\n        num_contours = 15\n        if 'num_contours' in kwargs:\n            num_contours = kwargs['num_contours']\n\n        # version of gradient descent to use (normalized or unnormalized)\n        self.version = 'unnormalized'\n        if 'version' in kwargs:\n            self.version = kwargs['version']\n        \n        # steplength\n        self.steplength = steplength\n            \n        # get initial point \n        self.w_init = np.asarray([float(s) for s in w_init])\n            \n        # take in user defined maximum number of iterations\n        self.max_its = max_its\n           \n        # loop over steplengths, plot panels for each\n        count = 0\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(11,5))\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,1,1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis('off')\n        ax2 = plt.subplot(gs[1],aspect = 'equal'); \n        ax3 = plt.subplot(gs[2]); ax3.axis('off')\n\n        # run L1 first and plot\n        self.version = 'L1'\n        self.run_gradient_descent()\n        title = '$L_1$ steepest descent'\n        self.draw_panel(ax2,title,**kwargs)\n        \n        \n    # visualize descent on multi-input function\n    def Linf(self,g,w_init,steplength,max_its,**kwargs):\n        ### input arguments ###        \n        self.g = g\n        self.max_its = max_its\n        self.grad = compute_grad(self.g)              # gradient of input function\n            \n        plot_final = False\n        if 'plot_final' in kwargs:\n            plot_final = kwargs['plot_final']\n\n        num_contours = 15\n        if 'num_contours' in kwargs:\n            num_contours = kwargs['num_contours']\n\n        # version of gradient descent to use (normalized or unnormalized)\n        self.version = 'unnormalized'\n        if 'version' in kwargs:\n            self.version = kwargs['version']\n        \n        # steplength\n        self.steplength = steplength\n            \n        # get initial point \n        self.w_init = np.asarray([float(s) for s in w_init])\n            \n        # take in user defined maximum number of iterations\n        self.max_its = max_its\n           \n        # loop over steplengths, plot panels for each\n        count = 0\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(11,5))\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,1,1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis('off')\n        ax2 = plt.subplot(gs[1],aspect = 'equal'); \n        ax3 = plt.subplot(gs[2]); ax3.axis('off')\n        \n        # run Linf first and plot\n        self.version = 'Linf'\n        self.run_gradient_descent()\n        title = '$L_{\\infty}$ steepest descent'\n        self.draw_panel(ax2,title,**kwargs)\n    \n        # plot\n        plt.show()\n     \n    # draw panel \n    def draw_panel(self,ax,title,**kwargs):\n        # set viewing limits on contour plot\n        xvals = [self.w_hist[s][0] for s in range(len(self.w_hist))]\n        xvals.append(self.w_init[0])\n        yvals = [self.w_hist[s][1] for s in range(len(self.w_hist))]\n        yvals.append(self.w_init[1])\n        xmax = max(xvals)\n        xmin = min(xvals)\n        xgap = (xmax - xmin)*0.1\n        ymax = max(yvals)\n        ymin = min(yvals)\n        ygap = (ymax - ymin)*0.1\n        xmin -= xgap\n        xmax += xgap\n        ymin -= ygap\n        ymax += ygap\n\n        if 'xmin' in kwargs:\n            xmin = kwargs['xmin']\n        if 'xmax' in kwargs:\n            xmax = kwargs['xmax']\n        if 'ymin' in kwargs:\n            ymin = kwargs['ymin']\n        if 'ymax' in kwargs:\n            ymax = kwargs['ymax'] \n        axes = False\n        if 'axes' in kwargs:\n            axes = kwargs['ymax']\n        pts = False\n        if 'pts' in kwargs:\n            pts = kwargs['pts']  \n        \n        pts = False\n        if 'pts' in kwargs:\n            pts = kwargs['pts']  \n            \n        linewidth = 2.5\n        if 'linewidth' in kwargs:\n            linewidth = kwargs['linewidth']\n            \n        #### define input space for function and evaluate ####\n        w1 = np.linspace(xmin,xmax,400)\n        w2 = np.linspace(ymin,ymax,400)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([self.g(s) for s in h])\n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        num_contours = kwargs['num_contours']\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cutoff = 0.5\n        cutoff = (levelmax - levelmin)*cutoff\n        numper = 3\n        levels1 = np.linspace(cutoff,levelmax,numper)\n        num_contours -= numper\n\n        levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n        levels = np.unique(np.append(levels1,levels2))\n        num_contours -= numper\n        while num_contours > 0:\n            cutoff = levels[1]\n            levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n            levels = np.unique(np.append(levels2,levels))\n            num_contours -= numper\n\n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = 'k')\n        ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = 'Blues')\n\n        if axes == True:\n            ax.axhline(linestyle = '--', color = 'k',linewidth = 1)\n            ax.axvline(linestyle = '--', color = 'k',linewidth = 1)\n\n        # colors for points\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        colorspec = []\n        colorspec = np.concatenate((s,np.flipud(s)),1)\n        colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n\n        ### plot function decrease plot in right panel\n        for j in range(len(self.w_hist)):  \n            w_val = self.w_hist[j]\n            g_val = self.g(w_val)\n\n            # plot in left panel\n            if pts == 'True':\n                ax.scatter(w_val[0],w_val[1],s = 30,c = colorspec[j],edgecolor = 'k',linewidth = 1.5*math.sqrt((1/(float(j) + 1))),zorder = 3)\n\n            # plot connector between points for visualization purposes\n            if j > 0:\n                w_old = self.w_hist[j-1]\n                w_new = self.w_hist[j]    \n                \n                ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = colorspec[j],linewidth = linewidth,alpha = 1,zorder = 2)      # plot approx\n                ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = 'k',linewidth = linewidth + 0.4,alpha = 1,zorder = 1)      # plot approx\n                \n        # clean panel\n        ax.set_title(title,fontsize = 12)\n        ax.set_xlabel('$w_1$',fontsize = 12)\n        ax.set_ylabel('$w_2$',fontsize = 12,rotation = 0)\n        ax.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color='k',zorder = 0,linewidth = 0.5)               \n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n        ax.set_xticks(np.arange(round(xmin), round(xmax) + 1, 1.0))\n        ax.set_yticks(np.arange(round(ymin), round(ymax) + 1, 1.0))\n            """
mlrefined_libraries/math_optimization_library/steepest_descent_comparison.py,29,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport time\n\nclass visualizer:\n    '''\n    Compares steepest descent using L1, L2, and Linfinity norms.\n    ''' \n     \n    ######## gradient descent ########\n    # run gradient descent \n    def run_gradient_descent(self):\n        w = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w)\n        j = 0\n        for j in range(int(self.max_its)):            \n            # plug in value into func and derivative\n            grad_eval = self.grad(w)\n            \n            ### L1, L2, or Linf? L2 by default ###\n            # L1 steepest descent\n            if self.version == 'L1':\n                # take absolute value of each entry in grad vector\n                grad_abs = np.abs(grad_eval)\n                best_val = np.max(grad_abs)\n                ind_best = np.argwhere(grad_abs == best_val)\n                new_grad = np.zeros((len(grad_eval)))\n                new_grad[ind_best] = np.sign(grad_eval[ind_best])\n                grad_eval = new_grad\n    \n            # Linf steepest descent\n            elif self.version == 'Linf':\n                grad_eval = np.sign(grad_eval)\n                \n            # normaize direction, if everything is perfectly zero then move in random direction\n            grad_norm = np.linalg.norm(grad_eval)\n            if grad_norm == 0:\n                grad_eval = 2*np.random.rand(len(w)) - 1\n                grad_norm = np.linalg.norm(grad_eval)\n                grad_eval /= grad_norm\n            else:\n                grad_eval /= grad_norm\n               \n            ### check what sort of steplength rule to employ ###\n            alpha = 0\n            if self.steplength == 'diminishing':\n                alpha = 1/(1 + j)\n            elif self.steplength == 'backtracking':\n                alpha = self.backtracking(w,grad_eval)\n            elif self.steplength == 'exact': \n                alpha = self.exact(w,grad_eval)\n            else:\n                alpha = float(self.steplength)            \n            \n            # take gradient descent step\n            w = w - alpha*grad_eval\n            \n            # record\n            self.w_hist.append(w)\n\n    # backtracking linesearch module\n    def backtracking(self,w,grad_eval):\n        # set input parameters\n        alpha = 1\n        t = 0.5\n        \n        # compute initial function and gradient values\n        func_eval = self.g(w)\n        grad_norm = np.linalg.norm(grad_eval)**2\n        \n        # loop over and tune steplength\n        while self.g(w - alpha*grad_eval) > func_eval - alpha*0.5*grad_norm:\n            alpha = t*alpha\n        return alpha\n\n    # exact linesearch module\n    def exact(self,w,grad_eval):\n        # set parameters of linesearch at each step\n        valmax = 10\n        num_evals = 3000\n        \n        # set alpha range\n        alpha_range = np.linspace(0,valmax,num_evals)\n        \n        # evaluate function over direction and alpha range, grab alpha giving lowest eval\n        steps = [(w - alpha*grad_eval) for alpha in alpha_range]\n        func_evals = np.array([self.g(s) for s in steps])\n        ind = np.argmin(func_evals)\n        best_alpha = alpha_range[ind]\n        \n        return best_alpha\n    \n    # visualize descent on multi-input function\n    def run(self,g,w_init,steplength,max_its,**kwargs):\n        ### input arguments ###        \n        self.g = g\n        self.max_its = max_its\n        self.grad = compute_grad(self.g)              # gradient of input function\n            \n        plot_final = False\n        if 'plot_final' in kwargs:\n            plot_final = kwargs['plot_final']\n\n        num_contours = 15\n        if 'num_contours' in kwargs:\n            num_contours = kwargs['num_contours']\n\n        # version of gradient descent to use (normalized or unnormalized)\n        self.version = 'unnormalized'\n        if 'version' in kwargs:\n            self.version = kwargs['version']\n        \n        # steplength\n        self.steplength = steplength\n            \n        # get initial point \n        self.w_init = np.asarray([float(s) for s in w_init])\n            \n        # take in user defined maximum number of iterations\n        self.max_its = max_its\n           \n        # loop over steplengths, plot panels for each\n        count = 0\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(9,4))\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,1,1]) \n        ax1 = plt.subplot(gs[0],aspect = 'equal'); \n        ax2 = plt.subplot(gs[1],aspect = 'equal'); \n        ax3 = plt.subplot(gs[2],aspect = 'equal'); \n\n        #### run local random search algorithm ####\n        \n        # run L2 first and plot\n        self.version = 'L2'\n        self.run_gradient_descent()\n        title = '$L_2$ steepest descent'\n        self.draw_panel(ax1,title,**kwargs)\n        \n         \n        # run L2 first and plot\n        self.version = 'L1'\n        self.run_gradient_descent()\n        title = '$L_1$ steepest descent'\n        self.draw_panel(ax2,title,**kwargs)\n        \n        # run Linf first and plot\n        self.version = 'Linf'\n        self.run_gradient_descent()\n        title = '$L_{\\infty}$ steepest descent'\n        self.draw_panel(ax3,title,**kwargs)\n    \n        # plot\n        plt.show()\n     \n    # draw panel \n    def draw_panel(self,ax,title,**kwargs):\n        # set viewing limits on contour plot\n        xvals = [self.w_hist[s][0] for s in range(len(self.w_hist))]\n        xvals.append(self.w_init[0])\n        yvals = [self.w_hist[s][1] for s in range(len(self.w_hist))]\n        yvals.append(self.w_init[1])\n        xmax = max(xvals)\n        xmin = min(xvals)\n        xgap = (xmax - xmin)*0.1\n        ymax = max(yvals)\n        ymin = min(yvals)\n        ygap = (ymax - ymin)*0.1\n        xmin -= xgap\n        xmax += xgap\n        ymin -= ygap\n        ymax += ygap\n\n        if 'xmin' in kwargs:\n            xmin = kwargs['xmin']\n        if 'xmax' in kwargs:\n            xmax = kwargs['xmax']\n        if 'ymin' in kwargs:\n            ymin = kwargs['ymin']\n        if 'ymax' in kwargs:\n            ymax = kwargs['ymax'] \n        axes = False\n        if 'axes' in kwargs:\n            axes = kwargs['ymax']\n        pts = False\n        if 'pts' in kwargs:\n            pts = kwargs['pts']  \n        \n        pts = False\n        if 'pts' in kwargs:\n            pts = kwargs['pts']  \n            \n        linewidth = 2.5\n        if 'linewidth' in kwargs:\n            linewidth = kwargs['linewidth']\n            \n        #### define input space for function and evaluate ####\n        w1 = np.linspace(xmin,xmax,400)\n        w2 = np.linspace(ymin,ymax,400)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([self.g(s) for s in h])\n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        num_contours = kwargs['num_contours']\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cutoff = 0.5\n        cutoff = (levelmax - levelmin)*cutoff\n        numper = 3\n        levels1 = np.linspace(cutoff,levelmax,numper)\n        num_contours -= numper\n\n        levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n        levels = np.unique(np.append(levels1,levels2))\n        num_contours -= numper\n        while num_contours > 0:\n            cutoff = levels[1]\n            levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n            levels = np.unique(np.append(levels2,levels))\n            num_contours -= numper\n\n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = 'k')\n        ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = 'Blues')\n\n        if axes == True:\n            ax.axhline(linestyle = '--', color = 'k',linewidth = 1)\n            ax.axvline(linestyle = '--', color = 'k',linewidth = 1)\n\n        # colors for points\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        colorspec = []\n        colorspec = np.concatenate((s,np.flipud(s)),1)\n        colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n\n        ### plot function decrease plot in right panel\n        for j in range(len(self.w_hist)):  \n            w_val = self.w_hist[j]\n            g_val = self.g(w_val)\n\n            # plot in left panel\n            if pts == 'True':\n                ax.scatter(w_val[0],w_val[1],s = 30,c = colorspec[j],edgecolor = 'k',linewidth = 1.5*math.sqrt((1/(float(j) + 1))),zorder = 3)\n\n            # plot connector between points for visualization purposes\n            if j > 0:\n                w_old = self.w_hist[j-1]\n                w_new = self.w_hist[j]    \n                \n                ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = colorspec[j],linewidth = linewidth,alpha = 1,zorder = 2)      # plot approx\n                ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = 'k',linewidth = linewidth + 0.4,alpha = 1,zorder = 1)      # plot approx\n                \n        # clean panel\n        ax.set_title(title,fontsize = 12)\n        ax.set_xlabel('$w_1$',fontsize = 12)\n        ax.set_ylabel('$w_2$',fontsize = 12,rotation = 0)\n        ax.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color='k',zorder = 0,linewidth = 0.5)               \n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])"""
mlrefined_libraries/math_optimization_library/steepest_descent_direction.py,60,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nimport matplotlib.animation as animation\nimport math\nimport numpy as np\nfrom IPython.display import clear_output\nimport time\n\n'''\nA collection of animations illustrating the steepest descent direction under the L2, L1, and Linfinity norms.\n'''\ndef L2(pt,num_frames,savepath,**kwargs):\n    # initialize figure\n    fig = plt.figure(figsize = (16,8))\n    artist = fig\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1,1],wspace=0.3, hspace=0.05) \n    ax1 = plt.subplot(gs[0],aspect ='equal');\n    ax2 = plt.subplot(gs[1],aspect = 'equal'); \n\n    # create dataset for unit circle\n    v = np.linspace(0,2*np.pi,1000)    \n    s = np.sin(v)\n    s.shape = (len(s),1)\n    t = np.cos(v)\n    t.shape = (len(t),1)\n    \n    # create span of angles to plot over\n    v = np.linspace(0,2*np.pi,num_frames)\n    a = np.arccos(pt[0]/(pt[0]**2 + pt[1]**2)**(0.5)) + np.pi\n    v = np.append(v,a)\n    v = np.sort(v)\n    v = np.unique(v)\n    y = np.sin(v)\n    x = np.cos(v)\n    \n    # create inner product plot\n    obj = [(x[s]*pt[0] + y[s]*pt[1]) for s in range(len(v))]\n    ind_min = np.argmin(obj)\n    \n    # rescale directions for plotting with arrows\n    x = 0.96*x\n    y = 0.96*y\n    \n    # create linspace for left panel\n    w = np.linspace(0,2*np.pi,300)\n            \n    \n    # print update\n    num_frames = len(v)\n    print ('starting animation rendering...')\n    # animation sub-function\n    def animate(k):\n        # clear panels for next slide\n        ax1.cla()\n        ax2.cla()\n        \n        # print rendering update\n        if np.mod(k+1,25) == 0:\n            print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n        if k == num_frames - 1:\n            print ('animation rendering complete!')\n            time.sleep(1.5)\n            clear_output()\n           \n        ### setup left panel ###\n        # plot circle with lines in left panel\n        ax1.plot(s,t,color = 'k',linewidth = 3)\n        \n        # plot rotation arrow\n        ax1.arrow(0, 0, x[k], y[k], head_width=0.1, head_length=0.1, fc='k', ec='k',linewidth=3,zorder = 3,length_includes_head = True)\n\n        # plot input point as arrow\n        ax1.arrow(0, 0, pt[0], pt[1], head_width=0.1, head_length=0.1, fc='r', ec='r',linewidth=3,zorder = 3,length_includes_head = True)\n        ax1.arrow(0, 0, pt[0], pt[1], head_width=0.11, head_length=0.1, fc='k', ec='k',linewidth=5,zorder = 2,length_includes_head = True)\n\n        # clean up panel\n        ax1.grid(True, which='both')\n        ax1.axhline(y=0, color='k')\n        ax1.axvline(x=0, color='k')\n        ax1.set_xlim([-1.5,1.5])\n        ax1.set_ylim([-1.5,1.5])\n        \n        ### setup right panel ###\n        current_angle = v[k]\n        ind = np.argmin(np.abs(w - current_angle))\n        p = w[:ind+1]\n        \n        # plot normalized objective thus far\n        ax2.plot(v[:k+1],obj[:k+1],color ='k',linewidth=4,zorder = 2)\n        \n        # if we have reached the minimum plot it on all slides going further\n        if k >= ind_min:\n            # plot direction\n            ax1.arrow(0, 0, x[ind_min], y[ind_min], head_width=0.1, head_length=0.1, fc='lime', ec='lime',linewidth=3,zorder = 3,length_includes_head = True)\n            ax1.arrow(0, 0, x[ind_min], y[ind_min], head_width=0.11, head_length=0.1, fc='k', ec='k',linewidth=5,zorder = 2,length_includes_head = True)\n        \n            # mark objective plot\n            ax2.scatter(v[ind_min],obj[ind_min],color ='lime',s = 200,linewidth = 1,edgecolor = 'k',zorder = 3)\n        \n        # cleanup plot\n        ax2.grid(True, which='both')\n        ax2.axhline(y=0, color='k')\n        ax2.axvline(x=0, color='k')   \n        ax2.set_xlim([-0.1,2*np.pi + 0.1])\n        ax2.set_ylim([min(obj) - 0.2,max(obj) + 0.2])\n        \n        # add legend\n        ax2.legend([r'$\\nabla g(\\mathbf{v})^T \\mathbf{d}$'],loc='center left', bbox_to_anchor=(0.13, 1.05),fontsize=18,ncol=2)\n\n        return artist,\n\n    anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n\n    # produce animation and save\n    fps = 50\n    if 'fps' in kwargs:\n        fps = kwargs['fps']\n    anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n    clear_output()\n\ndef L1(pt,num_frames,savepath,**kwargs):\n    # initialize figure\n    fig = plt.figure(figsize = (16,8))\n    artist = fig\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1,1],wspace=0.3, hspace=0.05) \n    ax1 = plt.subplot(gs[0],aspect ='equal');\n    ax2 = plt.subplot(gs[1],aspect = 'equal'); \n\n    # create dataset for unit diamond\n    v = np.linspace(0,2*np.pi,2000)\n    s = np.sin(v)\n    s.shape = (len(s),1)\n    t = np.cos(v)\n    t.shape = (len(t),1)\n    diamond = np.concatenate((s,t),axis=1)\n    news = []\n    for a in diamond:\n        a = a/np.sum(abs(a))\n        news.append(a)\n    news = np.asarray(news)\n    s = news[:,0]\n    t = news[:,1]\n    \n    ### create span of angles to plot over\n    v = np.linspace(0,2*np.pi,num_frames)\n    \n    # make sure corners of the diamond are included\n    v = np.append(v,np.pi*0.5)\n    v = np.append(v,np.pi)\n    v = np.append(v,3/float(2)*np.pi)\n    v = np.sort(v)\n    v = np.unique(v)\n    y = np.sin(v)\n    x = np.cos(v)\n    \n    # make l2 ball\n    x.shape = (len(x),1)\n    y.shape = (len(y),1)\n    l2 = np.concatenate((x,y),axis=1)\n        \n    # make l1 ball \n    l1 = []\n    for a in l2:\n        a = a/np.sum(abs(a))\n        l1.append(a)\n    l1 = np.asarray(l1)\n    x = l1[:,0]\n    y = l1[:,1]\n    \n    # create inner product plot\n    obj = [(x[s]*pt[0] + y[s]*pt[1]) for s in range(len(v))]\n    ind_min = np.argmin(obj)\n    \n    # rescale directions for plotting with arrows\n    x = 0.96*x\n    y = 0.96*y\n    \n    # create linspace for left panel\n    w = np.linspace(0,2*np.pi,300)\n    pt = [0.975*a for a in pt]\n\n    # print update\n    num_frames = len(v)\n    print ('starting animation rendering...')\n    # animation sub-function\n    def animate(k):\n        # clear panels for next slide\n        ax1.cla()\n        ax2.cla()\n        \n        # print rendering update\n        if np.mod(k+1,25) == 0:\n            print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n        if k == num_frames - 1:\n            print ('animation rendering complete!')\n            time.sleep(1.5)\n            clear_output()\n           \n        ### setup left panel ###\n        # plot circle with lines in left panel\n        ax1.plot(s,t,color = 'k',linewidth = 3)\n        \n        # plot rotation arrow\n        ax1.arrow(0, 0, x[k], y[k], head_width=0.1, head_length=0.1, fc='k', ec='k',linewidth=3,zorder = 3,length_includes_head = True)\n\n        # plot input point as arrow\n        ax1.arrow(0, 0, pt[0], pt[1], head_width=0.1, head_length=0.1, fc='r', ec='r',linewidth=3,zorder = 3,length_includes_head = True)\n        ax1.arrow(0, 0, pt[0], pt[1], head_width=0.11, head_length=0.1, fc='k', ec='k',linewidth=5,zorder = 2,length_includes_head = True)\n\n        # clean up panel\n        ax1.grid(True, which='both')\n        ax1.axhline(y=0, color='k')\n        ax1.axvline(x=0, color='k')\n        ax1.set_xlim([-1.5,1.5])\n        ax1.set_ylim([-1.5,1.5])\n        \n        ### setup right panel ###\n        current_angle = v[k]\n        ind = np.argmin(np.abs(w - current_angle))\n        p = w[:ind+1]\n        \n        # plot normalized objective thus far\n        ax2.plot(v[:k+1],obj[:k+1],color ='k',linewidth=4,zorder = 2)\n        \n        # if we have reached the minimum plot it on all slides going further\n        if k >= ind_min:\n            # plot direction\n            ax1.arrow(0, 0, x[ind_min], y[ind_min], head_width=0.1, head_length=0.1, fc='lime', ec='lime',linewidth=3,zorder = 3,length_includes_head = True)\n            ax1.arrow(0, 0, x[ind_min], y[ind_min], head_width=0.11, head_length=0.1, fc='k', ec='k',linewidth=5,zorder = 2,length_includes_head = True)\n        \n            # mark objective plot\n            ax2.scatter(v[ind_min],obj[ind_min],color ='lime',s = 200,linewidth = 1,edgecolor = 'k',zorder = 3)\n        \n        # cleanup plot\n        ax2.grid(True, which='both')\n        ax2.axhline(y=0, color='k')\n        ax2.axvline(x=0, color='k')   \n        ax2.set_xlim([-0.1,2*np.pi + 0.1])\n        ax2.set_ylim([min(obj) - 0.2,max(obj) + 0.2])\n        \n        # add legend\n        ax2.legend([r'$\\nabla g(\\mathbf{v})^T \\mathbf{d}$'],loc='center left', bbox_to_anchor=(0.13, 1.05),fontsize=18,ncol=2)\n\n        return artist,\n\n    anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n\n    # produce animation and save\n    fps = 50\n    if 'fps' in kwargs:\n        fps = kwargs['fps']\n    anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n    clear_output()\n\ndef Linf(pt,num_frames,savepath,**kwargs):\n    # initialize figure\n    fig = plt.figure(figsize = (16,8))\n    artist = fig\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1,1],wspace=0.3, hspace=0.05) \n    ax1 = plt.subplot(gs[0],aspect ='equal');\n    ax2 = plt.subplot(gs[1],aspect = 'equal'); \n\n    # create dataset for unit square\n    v = np.linspace(0,2*np.pi,2000)\n    s = np.sin(v)\n    s.shape = (len(s),1)\n    t = np.cos(v)\n    t.shape = (len(t),1)\n    square = np.concatenate((s,t),axis=1)\n    news = []\n    for a in square:\n        a = a/np.max(abs(a))\n        news.append(a)\n    news = np.asarray(news)\n    s = news[:,0]\n    t = news[:,1]\n    \n    ### create span of angles to plot over\n    v = np.linspace(0,2*np.pi,num_frames)\n    \n    # make sure corners of the square are included\n    v = np.append(v,np.pi/float(4))\n    v = np.append(v,np.pi*3/float(4))\n    v = np.append(v,np.pi*5/float(4))\n    v = np.append(v,np.pi*7/float(4))\n    v = np.sort(v)\n    v = np.unique(v)\n    y = np.sin(v)\n    x = np.cos(v)\n    \n    # make l2 ball\n    x.shape = (len(x),1)\n    y.shape = (len(y),1)\n    l2 = np.concatenate((x,y),axis=1)\n        \n    # make Linf ball\n    linf = []\n    for a in l2:\n        a = a/np.max(abs(a))\n        linf.append(a)\n    linf = np.asarray(linf) \n    x = linf[:,0]\n    y = linf[:,1]\n    \n    # create inner product plot\n    obj = [(x[s]*pt[0] + y[s]*pt[1]) for s in range(len(v))]\n    ind_min = np.argmin(obj)\n    \n    # rescale directions for plotting with arrows\n    x = 0.96*x\n    y = 0.96*y\n    \n    # create linspace for left panel\n    w = np.linspace(0,2*np.pi,300)\n    pt = [0.975*a for a in pt]\n\n    # print update\n    num_frames = len(v)\n    print ('starting animation rendering...')\n    # animation sub-function\n    def animate(k):\n        # clear panels for next slide\n        ax1.cla()\n        ax2.cla()\n        \n        # print rendering update\n        if np.mod(k+1,25) == 0:\n            print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n        if k == num_frames - 1:\n            print ('animation rendering complete!')\n            time.sleep(1.5)\n            clear_output()\n           \n        ### setup left panel ###\n        # plot circle with lines in left panel\n        ax1.plot(s,t,color = 'k',linewidth = 3)\n        \n        # plot rotation arrow\n        ax1.arrow(0, 0, x[k], y[k], head_width=0.1, head_length=0.1, fc='k', ec='k',linewidth=3,zorder = 3,length_includes_head = True)\n\n        # plot input point as arrow\n        ax1.arrow(0, 0, pt[0], pt[1], head_width=0.1, head_length=0.1, fc='r', ec='r',linewidth=3,zorder = 3,length_includes_head = True)\n        ax1.arrow(0, 0, pt[0], pt[1], head_width=0.11, head_length=0.1, fc='k', ec='k',linewidth=5,zorder = 2,length_includes_head = True)\n\n        # clean up panel\n        ax1.grid(True, which='both')\n        ax1.axhline(y=0, color='k')\n        ax1.axvline(x=0, color='k')\n        ax1.set_xlim([-1.5,1.5])\n        ax1.set_ylim([-1.5,1.5])\n        \n        ### setup right panel ###\n        current_angle = v[k]\n        ind = np.argmin(np.abs(w - current_angle))\n        p = w[:ind+1]\n        \n        # plot normalized objective thus far\n        ax2.plot(v[:k+1],obj[:k+1],color ='k',linewidth=4,zorder = 2)\n        \n        # if we have reached the minimum plot it on all slides going further\n        if k >= ind_min:\n            # plot direction\n            ax1.arrow(0, 0, x[ind_min], y[ind_min], head_width=0.1, head_length=0.1, fc='lime', ec='lime',linewidth=3,zorder = 3,length_includes_head = True)\n            ax1.arrow(0, 0, x[ind_min], y[ind_min], head_width=0.11, head_length=0.1, fc='k', ec='k',linewidth=5,zorder = 2,length_includes_head = True)\n        \n            # mark objective plot\n            ax2.scatter(v[ind_min],obj[ind_min],color ='lime',s = 200,linewidth = 1,edgecolor = 'k',zorder = 3)\n        \n        # cleanup plot\n        ax2.grid(True, which='both')\n        ax2.axhline(y=0, color='k')\n        ax2.axvline(x=0, color='k')   \n        ax2.set_xlim([-0.1,2*np.pi + 0.1])\n        ax2.set_ylim([min(obj) - 0.2,max(obj) + 0.2])\n        \n        # add legend\n        ax2.legend([r'$\\nabla g(\\mathbf{v})^T \\mathbf{d}$'],loc='center left', bbox_to_anchor=(0.13, 1.05),fontsize=18,ncol=2)\n\n        return artist,\n\n    anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n\n    # produce animation and save\n    fps = 50\n    if 'fps' in kwargs:\n        fps = kwargs['fps']\n    anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n    clear_output()"""
mlrefined_libraries/multilayer_perceptron_library/__init__.py,0,b''
mlrefined_libraries/multilayer_perceptron_library/architectures.py,36,"b""# import autograd functionality\nfrom autograd import grad as compute_grad  \nimport autograd.numpy as np\nimport copy\n\nclass Setup:\n    '''\n    Normalized multilayer perceptron / feedforward network architectures\n    '''\n \n    def choose_architecture(self,activation_name):\n        self.activation_name = activation_name\n        \n        # set activation\n        if activation_name == 'relu':\n            self.activation = self.relu\n        if activation_name == 'maxout':\n            self.activation = self.maxout\n        if activation_name == 'tanh':\n            self.activation = self.tanh\n        if activation_name == 'linear':\n            self.activation = self.linear\n            \n        # set architecture and initializer (basically just a switch for maxout vs others)\n        self.training_architecture = self.compute_general_network_features\n        self.initializer = self.initialize_general_network_weights\n        self.testing_architecture = self.compute_network_features_testing\n        if self.activation_name == 'maxout':\n            self.training_architecture = self.compute_maxout_network_features            \n            self.initializer = self.initialize_maxout_network_weights   \n            self.testing_architecture = self.compute_maxout_network_features_testing\n            \n    # our normalization function\n    def normalize(self,data,data_mean,data_std):\n        normalized_data = (data - data_mean)/(data_std + 10**(-5))\n        return normalized_data\n\n    ########## architectures ##########\n    def compute_general_network_features(self,x, inner_weights):        \n        # pad data with ones to deal with bias\n        o = np.ones((np.shape(x)[0],1))\n        a_padded = np.concatenate((o,x),axis = 1)\n\n        # loop through weights and update each layer of the network\n        for W in inner_weights:            \n            # output of layer activation\n            a = self.activation(np.dot(a_padded,W))\n\n            ### normalize output of activation\n            # compute the mean and standard deviation of the activation output distributions\n            a_means = np.mean(a,axis = 0)\n            a_stds = np.std(a,axis = 0)\n\n            # normalize the activation outputs\n            a_normed = self.normalize(a,a_means,a_stds)\n\n            # pad with ones for bias\n            o = np.ones((np.shape(a_normed)[0],1))\n            a_padded = np.concatenate((o,a_normed),axis = 1)\n\n        return a_padded\n\n    def compute_maxout_network_features(self,x, inner_weights):\n        # pad data with ones to deal with bias\n        o = np.ones((np.shape(x)[0],1))\n        a_padded = np.concatenate((o,x),axis = 1)\n\n        # loop through weights and update each layer of the network\n        for W1,W2 in inner_weights:                                 \n            # output of layer activation  \n            a = self.activation(np.dot(a_padded,W1),np.dot(a_padded,W2))  \n\n            ### normalize output of activation\n            # compute the mean and standard deviation of the activation output distributions\n            a_means = np.mean(a,axis = 0)\n            a_stds = np.std(a,axis = 0)\n\n            # normalize the activation outputs\n            a_normed = self.normalize(a,a_means,a_stds)\n\n            # pad with ones for bias\n            o = np.ones((np.shape(a_normed)[0],1))\n            a_padded = np.concatenate((o,a_normed),axis = 1)\n\n        return a_padded\n\n    ########## test versions of the architecture to extract stats ##########\n    def compute_network_features_testing(self,x, inner_weights,stats):\n        '''\n        An adjusted normalized architecture compute function that collects network statistics as the training data\n        passes through each layer, and applies them to properly normalize test data.\n        '''\n        # are you using this to compute stats on training data (stats empty) or to normalize testing data (stats not empty)\n        switch =  'testing'\n        if np.size(stats) == 0:\n            switch = 'training'\n\n        # pad data with ones to deal with bias\n        o = np.ones((np.shape(x)[0],1))\n        a_padded = np.concatenate((o,x),axis = 1)\n\n        # loop through weights and update each layer of the network\n        c = 0\n        for W in inner_weights:\n            # output of layer activation\n            a = self.activation(np.dot(a_padded,W))\n\n            ### normalize output of activation\n            a_means = 0\n            a_stds = 0\n            if switch == 'training':\n                # compute the mean and standard deviation of the activation output distributions\n                a_means = np.mean(a,axis = 0)\n                a_stds = np.std(a,axis = 0)\n                stats.append([a_means,a_stds])\n            elif switch == 'testing':\n                a_means = stats[c][0]\n                a_stds = stats[c][1]\n\n            # normalize the activation outputs\n            a_normed = self.normalize(a,a_means,a_stds)\n\n            # pad with ones for bias\n            o = np.ones((np.shape(a_normed)[0],1))\n            a_padded = np.concatenate((o,a_normed),axis = 1)\n            c+=1\n\n        return a_padded,stats\n\n    def compute_maxout_network_features_testing(self,x,inner_weights,stats):\n        '''\n        An adjusted normalized architecture compute function that collects network statistics as the training data\n        passes through each layer, and applies them to properly normalize test data.\n        '''\n        # are you using this to compute stats on training data (stats empty) or to normalize testing data (stats not empty)\n        switch =  'testing'\n        if np.size(stats) == 0:\n            switch = 'training'\n\n        # pad data with ones to deal with bias\n        o = np.ones((np.shape(x)[0],1))\n        a_padded = np.concatenate((o,x),axis = 1)\n\n        # loop through weights and update each layer of the network\n        c = 0\n        for W1,W2 in inner_weights:                                  \n            # output of layer activation\n            a = self.activation(np.dot(a_padded,W1),np.dot(a_padded,W2))     \n\n            ### normalize output of activation\n            a_means = 0\n            a_stds = 0\n            if switch == 'training':\n                # compute the mean and standard deviation of the activation output distributions\n                a_means = np.mean(a,axis = 0)\n                a_stds = np.std(a,axis = 0)\n                stats.append([a_means,a_stds])\n            elif switch == 'testing':\n                a_means = stats[c][0]\n                a_stds = stats[c][1]\n\n            # normalize the activation outputs\n            a_normed = self.normalize(a,a_means,a_stds)\n\n            # pad with ones for bias\n            o = np.ones((np.shape(a_normed)[0],1))\n            a_padded = np.concatenate((o,a_normed),axis = 1)\n            c+=1\n\n        return a_padded,stats\n\n    ########## weight initializers ##########\n    # create initial weights for arbitrary feedforward network\n    def initialize_general_network_weights(self,layer_sizes,scale):\n        # container for entire weight tensor\n        weights = []\n\n        # loop over desired layer sizes and create appropriately sized initial \n        # weight matrix for each layer\n        for k in range(len(layer_sizes)-1):\n            # get layer sizes for current weight matrix\n            U_k = layer_sizes[k]\n            U_k_plus_1 = layer_sizes[k+1]\n\n            # make weight matrix\n            weight = scale*np.random.randn(U_k + 1,U_k_plus_1)\n            weights.append(weight)\n\n        # re-express weights so that w_init[0] = omega_inner contains all \n        # internal weight matrices, and w_init[1] = w contains weights of \n        # final linear combination in predict function\n        w_init = [weights[:-1],weights[-1]]\n\n        return w_init\n    \n    # create initial weights for maxout feedforward network\n    def initialize_maxout_network_weights(self,layer_sizes,scale):\n        # container for entire weight tensor\n        weights = []\n\n        # loop over desired layer sizes and create appropriately sized initial \n        # weight matrix for each layer\n        for k in range(len(layer_sizes)-1):\n            # get layer sizes for current weight matrix\n            U_k = layer_sizes[k]\n            U_k_plus_1 = layer_sizes[k+1]\n\n            # make weight matrix\n            weight1 = scale*np.random.randn(U_k + 1,U_k_plus_1)\n\n            # add second matrix for inner weights\n            if k < len(layer_sizes)-2:\n                weight2 = scale*np.random.randn(U_k + 1,U_k_plus_1)\n                weights.append([weight1,weight2])\n            else:\n                weights.append(weight1)\n\n        # re-express weights so that w_init[0] = omega_inner contains all \n        # internal weight matrices, and w_init = w contains weights of \n        # final linear combination in predict function\n        w_init = [weights[:-1],weights[-1]]\n\n        return w_init\n    \n    ########## activation functions ##########\n    def maxout(self,t1,t2):\n        # maxout activation\n        f = np.maximum(t1,t2)\n        return f\n    \n    def relu(self,t):\n        # relu activation\n        f = np.maximum(0,t)\n        return f    \n    \n    def tanh(self,t):\n        # tanh activation\n        f = np.tanh(t)\n        return f    \n    \n    def linear(self,t):\n        # linear activation\n        f = t\n        return f """
mlrefined_libraries/multilayer_perceptron_library/autoencoder_demos.py,29,"b"" # import autograd functionality to bulid function's properly for optimizers\nimport autograd.numpy as np\nimport math\nimport copy\n\n# import matplotlib functionality\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\n# import standard plotting and animation\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom matplotlib.ticker import FormatStrFormatter\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\nfrom matplotlib.ticker import MaxNLocator, FuncFormatter\n\n# import autograd functionality\nimport autograd.numpy as np\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\nfrom matplotlib.ticker import FormatStrFormatter\nfrom inspect import signature\n        \n# static image maker\ndef show_encode_decode(x,wrapper,**kwargs):\n    # strip instruments off autoencoder wrapper\n    cost_history = wrapper.train_cost_histories[0]\n    weight_history = wrapper.weight_histories[0]\n    encoder = wrapper.encoder\n    decoder = wrapper.decoder\n    normalizer = wrapper.normalizer\n    inverse_normalizer = wrapper.inverse_normalizer\n    \n    # show projection map or not\n    projmap = False\n    if 'projmap' in kwargs:\n        projmap = kwargs['projmap']\n\n    # for projection map drawing - arrow size\n    scale = 14\n    if 'scale' in kwargs:\n        scale = kwargs['scale']\n\n    # pluck out best weights from run\n    ind = np.argmin(cost_history)\n    w_best = weight_history[ind]\n\n    ###### figure 1 - original data, encoded data, decoded data ######\n    fig = plt.figure(figsize = (10,3))\n    gs = gridspec.GridSpec(1, 3) \n    ax1 = plt.subplot(gs[0],aspect = 'equal'); \n    ax2 = plt.subplot(gs[1],aspect = 'equal'); \n    ax3 = plt.subplot(gs[2],aspect = 'equal'); \n\n    # scatter original data with pc\n    ax1.scatter(x[0,:],x[1,:],c = 'k',s = 60,linewidth = 0.75,edgecolor = 'w')\n\n    ### plot encoded and decoded data ###\n    # create encoded vectors\n    v = encoder(normalizer(x),w_best[0])\n\n    # decode onto basis\n    p = inverse_normalizer(decoder(v,w_best[1]))\n\n    # plot decoded data \n    ax3.scatter(p[0,:],p[1,:],c = 'k',s = 60,linewidth = 0.75,edgecolor = 'r')\n\n    # define range for manifold\n    xmin1 = np.min(x[0,:])\n    xmax1 = np.max(x[0,:])\n    xmin2 = np.min(x[1,:])\n    xmax2 = np.max(x[1,:])\n    xgap1 = (xmax1 - xmin1)*0.2\n    xgap2 = (xmax2 - xmin2)*0.2\n    xmin1 -= xgap1\n    xmax1 += xgap1\n    xmin2 -= xgap2\n    xmax2 += xgap2\n    \n    # plot learned manifold\n    a = np.linspace(xmin1,xmax1,200)\n    b = np.linspace(xmin2,xmax2,200)\n    s,t = np.meshgrid(a,b)\n    s.shape = (1,len(a)**2)\n    t.shape = (1,len(b)**2)\n    z = np.vstack((s,t))\n    \n    # create encoded vectors\n    v = encoder(normalizer(z),w_best[0])\n\n    # decode onto basis\n    p = inverse_normalizer(decoder(v,w_best[1]))\n    \n    # scatter\n    ax2.scatter(p[0,:],p[1,:],c = 'k',s = 1.5,edgecolor = 'r',linewidth = 1,zorder = 0)\n    ax3.scatter(p[0,:],p[1,:],c = 'k',s = 1.5,edgecolor = 'r',linewidth = 1,zorder = 0)\n         \n    for ax in [ax1,ax2,ax3]:\n        ax.set_xlim([xmin1,xmax1])\n        ax.set_ylim([xmin2,xmax2])\n        ax.set_xlabel(r'$x_1$',fontsize = 16)\n        ax.set_ylabel(r'$x_2$',fontsize = 16,rotation = 0,labelpad = 10)\n        ax.axvline(linewidth=0.5, color='k',zorder = 0)\n        ax.axhline(linewidth=0.5, color='k',zorder = 0)\n\n    ax1.set_title('original data',fontsize = 18)\n    ax2.set_title('learned manifold',fontsize = 18)\n    ax3.set_title('decoded data',fontsize = 18)\n\n    # set whitespace\n    #fgs.update(wspace=0.01, hspace=0.5) # set the spacing between axes. \n        \n    ##### bottom panels - plot subspace and quiver plot of projections ####\n    if projmap == True:\n        fig = plt.figure(figsize = (10,4))\n        gs = gridspec.GridSpec(1, 1) \n        ax1 = plt.subplot(gs[0],aspect = 'equal'); \n        ax1.scatter(p[0,:],p[1,:],c = 'r',s = 9.5)\n        ax1.scatter(p[0,:],p[1,:],c = 'k',s = 1.5)\n        \n        ### create quiver plot of how data is projected ###\n        new_scale = 0.75\n        a = np.linspace(xmin1 - xgap1*new_scale,xmax1 + xgap1*new_scale,20)\n        b = np.linspace(xmin2 - xgap2*new_scale,xmax2 + xgap2*new_scale,20)\n        s,t = np.meshgrid(a,b)\n        s.shape = (1,len(a)**2)\n        t.shape = (1,len(b)**2)\n        z = np.vstack((s,t))\n        \n        v = 0\n        p = 0\n        # create encoded vectors\n        v = encoder(normalizer(z),w_best[0])\n\n        # decode onto basis\n        p = inverse_normalizer(decoder(v,w_best[1]))\n\n        # get directions\n        d = []\n        for i in range(p.shape[1]):\n            dr = (p[:,i] - z[:,i])[:,np.newaxis]\n            d.append(dr)\n        d = 2*np.array(d)\n        d = d[:,:,0].T\n        M = np.hypot(d[0,:], d[1,:])\n        ax1.quiver(z[0,:], z[1,:], d[0,:], d[1,:],M,alpha = 0.5,width = 0.01,scale = scale,cmap='autumn') \n        ax1.quiver(z[0,:], z[1,:], d[0,:], d[1,:],edgecolor = 'k',linewidth = 0.25,facecolor = 'None',width = 0.01,scale = scale) \n\n        #### clean up and label panels ####\n        for ax in [ax1]:\n            #ax.set_xlim([xmin1 - xgap1*new_scale,xmax1 + xgap1*new_scale])\n            #ax.set_ylim([xmin2 - xgap2*new_scale,xmax2 + xgap1*new_scale])\n            \n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            \n            ax.set_xlabel(r'$x_1$',fontsize = 16)\n            ax.set_ylabel(r'$x_2$',fontsize = 16,rotation = 0,labelpad = 10)\n\n        ax1.set_title('projection map',fontsize = 18)\n\n        # set whitespace\n        gs.update(wspace=0.01, hspace=0.5) # set the spacing between axes. \n    \n# draw a vector - for projection panel\ndef vector_draw(vec,ax,**kwargs):\n    color = 'k'\n    if 'color' in kwargs:\n        color = kwargs['color']\n    zorder = 3 \n    if 'zorder' in kwargs:\n        zorder = kwargs['zorder']\n        \n    veclen = math.sqrt(vec[0]**2 + vec[1]**2)\n    head_length = 0.25\n    head_width = 0.25\n    vec_orig = copy.deepcopy(vec)\n    vec = (veclen - head_length)/veclen*vec\n    ax.arrow(0, 0, vec[0],vec[1], head_width=head_width, head_length=head_length, fc=color, ec=color,linewidth=3,zorder = zorder)\n    \n    \n#### animate multiple runs on single regression ####\ndef animate_crossvals(savepath,x,runs,**kwargs):\n    weight_history = []\n    train_errors = []\n    valid_errors = []\n    for run in runs:\n        # get histories\n        train_counts = run.train_cost_histories[0]\n        valid_counts = run.val_cost_histories[0]\n        weights = run.weight_histories[0]\n\n        # select based on minimum training\n        ind = np.argmin(train_counts)\n        train_count = train_counts[ind]\n        valid_count = valid_counts[ind]\n        weight = weights[ind]\n\n        # store\n        train_errors.append(train_count)\n        valid_errors.append(valid_count)\n        weight_history.append(weight)\n        \n    ###### figure 1 - original data, encoded data, decoded data ######\n    fig = plt.figure(figsize = (9,5))\n    artist = fig\n\n    gs = gridspec.GridSpec(2, 3) \n    ax1 = plt.subplot(gs[0],aspect = 'equal'); \n    ax2 = plt.subplot(gs[1],aspect = 'equal'); \n    ax3 = plt.subplot(gs[2],aspect = 'equal'); \n    ax5 = plt.subplot(gs[4]); \n        \n    # start animation\n    num_frames = len(runs)        \n    print ('starting animation rendering...')\n    def animate(k):\n        print (k)\n        # clear panels\n        ax1.cla()\n        ax2.cla()\n        ax3.cla()\n        ax5.cla()\n\n        # print rendering update\n        if np.mod(k+1,25) == 0:\n            print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n        if k == num_frames - 1:\n            print ('animation rendering complete!')\n            time.sleep(1.5)\n            clear_output()\n            \n\n        # grab current encoder / decoder pair\n        wrapper = runs[k]\n        cost_history = wrapper.train_cost_histories[0]\n        weight_history = wrapper.weight_histories[0]\n        encoder = wrapper.encoder\n        decoder = wrapper.decoder\n        normalizer = wrapper.normalizer\n        inverse_normalizer = wrapper.inverse_normalizer    \n        \n        # pluck out best weights from run\n        ind = np.argmin(cost_history)\n        w_best = weight_history[ind]\n            \n        # scatter original data with pc\n        ax1.scatter(x[0,:],x[1,:],c = 'k',s = 60,linewidth = 0.75,edgecolor = 'w')\n\n        ### plot encoded and decoded data ###\n        # create encoded vectors\n        v = encoder(normalizer(x),w_best[0])\n\n        # decode onto basis\n        p = inverse_normalizer(decoder(v,w_best[1]))\n\n        # plot decoded data \n        ax3.scatter(p[0,:],p[1,:],c = 'k',s = 60,linewidth = 0.75,edgecolor = 'r')\n\n        # define range for manifold\n        xmin1 = np.min(x[0,:])\n        xmax1 = np.max(x[0,:])\n        xmin2 = np.min(x[1,:])\n        xmax2 = np.max(x[1,:])\n        xgap1 = (xmax1 - xmin1)*0.2\n        xgap2 = (xmax2 - xmin2)*0.2\n        xmin1 -= xgap1\n        xmax1 += xgap1\n        xmin2 -= xgap2\n        xmax2 += xgap2\n\n        # plot learned manifold\n        a = np.linspace(xmin1,xmax1,200)\n        b = np.linspace(xmin2,xmax2,200)\n        s,t = np.meshgrid(a,b)\n        s.shape = (1,len(a)**2)\n        t.shape = (1,len(b)**2)\n        z = np.vstack((s,t))\n\n        # create encoded vectors\n        v = encoder(normalizer(z),w_best[0])\n\n        # decode onto basis\n        p = inverse_normalizer(decoder(v,w_best[1]))\n\n        # scatter\n        ax2.scatter(p[0,:],p[1,:],c = 'k',s = 1.5,edgecolor = 'r',linewidth = 1,zorder = 0)\n        ax3.scatter(p[0,:],p[1,:],c = 'k',s = 1.5,edgecolor = 'r',linewidth = 1,zorder = 0)\n\n        for ax in [ax1,ax2,ax3]:\n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            ax.set_xlabel(r'$x_1$',fontsize = 16)\n            ax.set_ylabel(r'$x_2$',fontsize = 16,rotation = 0,labelpad = 10)\n            ax.axvline(linewidth=0.5, color='k',zorder = 0)\n            ax.axhline(linewidth=0.5, color='k',zorder = 0)\n\n        ax1.set_title('original data',fontsize = 18)\n        ax2.set_title('learned manifold',fontsize = 18)\n        ax3.set_title('decoded data',fontsize = 18)            \n            \n        #### plot training / validation errors ####\n        train_errors = wrapper.train_cost_histories\n        valid_errors = wrapper.val_cost_histories\n        \n        #plot_train_valid_errors(ax5,k,train_errors,valid_errors)\n        return artist,\n\n    anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n    # produce animation and save\n    fps = 50\n    if 'fps' in kwargs:\n        fps = kwargs['fps']\n    anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n    \n    clear_output()    \n\ndef plot_train_valid_errors(ax,k,train_errors,valid_errors):\n    train_errors = train_errors[0]\n    valid_errors = valid_errors[0]\n    num_elements = np.arange(len(train_errors))\n\n    ax.plot([v+1 for v in num_elements[:k+1]] ,train_errors[:k+1],color = [0,0.7,1],linewidth = 1.5,zorder = 1,label = 'training')\n    ax.scatter([v+1  for v in num_elements[:k+1]] ,train_errors[:k+1],color = [0,0.7,1],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n\n    ax.plot([v+1  for v in num_elements[:k+1]] ,valid_errors[:k+1],color = [1,0.8,0.5],linewidth = 1.5,zorder = 1,label = 'validation')\n    ax.scatter([v+1  for v in num_elements[:k+1]] ,valid_errors[:k+1],color= [1,0.8,0.5],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n    ax.set_title('errors',fontsize = 15)\n\n    # cleanup\n    ax.set_xlabel('number of units',fontsize = 12)\n\n    # cleanp panel                \n    num_iterations = len(train_errors)\n    minxc = 0.5\n    maxxc = len(num_elements) + 0.5\n    minc = min(min(copy.deepcopy(train_errors)),min(copy.deepcopy(valid_errors)))\n    maxc = max(max(copy.deepcopy(train_errors[:5])),max(copy.deepcopy(valid_errors[:5])))\n    gapc = (maxc - minc)*0.25\n    minc -= gapc\n    maxc += gapc\n\n    ax.set_xlim([minxc,maxxc])\n    ax.set_ylim([minc,maxc])\n\n    #ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax.set_xticks(np.arange(1,len(num_elements)+1))\n"""
mlrefined_libraries/multilayer_perceptron_library/cost_functions.py,8,"b""# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport copy\n\nclass Setup:\n    '''\n    Cost functinos\n    '''\n    def choose_cost(self,cost_name,predict,x,y,**kwargs):\n        self.predict = predict\n        self.x = x\n        self.y = y\n        if cost_name == 'least_squares':\n            self.cost = self.least_squares\n        if cost_name == 'twoclass_softmax':\n            self.cost = self.twoclass_softmax\n        if cost_name == 'multiclass_softmax':\n            self.cost = self.multiclass_softmax\n         \n        if cost_name == 'twoclass_counter':\n            self.cost = self.twoclass_counter\n        if cost_name == 'multiclass_counter':\n            self.cost = self.multiclass_counter\n\n    ########## cost functions ##########\n    # least squares cost\n    def least_squares(self,w):\n        cost = np.sum((self.predict(self.x,w) - self.y)**2)\n        return cost\n    \n    # two-class softmax / logistic regression cost\n    def twoclass_softmax(self,w):\n        cost = np.sum(np.log(1 + np.exp((-self.y)*(self.predict(self.x,w)))))\n        return cost\n\n    # multiclass softmaax regularized by the summed length of all normal vectors\n    def multiclass_softmax(self,W):        \n        # pre-compute predictions on all points\n        all_evals = self.predict(self.x,W)\n\n        # compute cost in compact form using numpy broadcasting\n        a = np.log(np.sum(np.exp(all_evals),axis = 1)) \n        b = all_evals[np.arange(len(self.y)),self.y]\n        cost = np.sum(a - b)\n        return cost\n    \n    ### misclassification counters ###\n    # two-class\n    def twoclass_counter(self,w):\n        misclassifications = 0.25*np.sum((np.sign(self.predict(self.x,w)) - self.y)**2)\n        return misclassifications\n\n    # multiclass\n    def multiclass_counter(self,W):\n        '''\n        fusion rule for counting number of misclassifications on an input multiclass dataset\n        '''\n\n        # create predicted labels\n        y_predict = np.argmax(self.predict(self.x,W),axis = 1) \n\n        # compare to actual labels\n        misclassifications = int(np.sum([abs(np.sign(a - b)) for a,b in zip(self.y,y_predict)]))\n        return misclassifications"""
mlrefined_libraries/multilayer_perceptron_library/early_stop_classification_animator.py,31,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom matplotlib.ticker import FormatStrFormatter\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\nfrom matplotlib.ticker import MaxNLocator, FuncFormatter\n\n# import autograd functionality\nimport autograd.numpy as np\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\nfrom matplotlib.ticker import FormatStrFormatter\nfrom inspect import signature\n\nclass Visualizer:\n    '''\n    Visualize cross validation performed on N = 2 dimensional input classification datasets\n    '''\n    #### initialize ####\n    def __init__(self,csvname):\n        # grab input\n        data = np.loadtxt(csvname,delimiter = ',')\n        self.x = data[:-1,:]\n        self.y = data[-1:,:] \n\n        self.colors = ['salmon','cornflowerblue','lime','bisque','mediumaquamarine','b','m','g']\n    \n    #### animate multiple runs on single regression ####\n    def animate_trainval_earlystop(self,savepath,run,frames,**kwargs):\n        train_errors = run.train_count_histories[0]\n        valid_errors = run.valid_count_histories[0]\n        weight_history = run.weight_histories[0]\n        num_units = len(weight_history)\n\n        # select subset of runs\n        inds = np.arange(0,len(weight_history),int(len(weight_history)/float(frames)))\n        weight_history = [weight_history[v] for v in inds]\n        train_errors = [train_errors[v] for v in inds]\n        valid_errors = [valid_errors[v] for v in inds]\n       \n        # construct figure\n        fig = plt.figure(figsize = (6,6))\n        artist = fig\n\n        # create subplot with 4 panels, plot input function in center plot\n        gs = gridspec.GridSpec(2, 2)\n        ax = plt.subplot(gs[0]); \n        ax1 = plt.subplot(gs[2]); \n        ax2 = plt.subplot(gs[3]); \n        ax3 = plt.subplot(gs[1]); \n        \n        # start animation\n        num_frames = len(inds)        \n        print ('starting animation rendering...')\n        def animate(k):\n            # clear panels\n            ax.cla()\n            ax1.cla()\n            ax2.cla()\n            ax3.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            #### plot training, testing, and full data ####            \n            # pluck out current weights \n            w_best = weight_history[k]\n            \n            # produce static img\n            self.static_N2_simple(ax,w_best,run,train_valid = 'original')\n            self.draw_boundary(ax,run,w_best,train_valid = 'train')\n            \n            self.static_N2_simple(ax1,w_best,run,train_valid = 'train')\n            self.draw_boundary(ax1,run,w_best,train_valid = 'train')\n            self.static_N2_simple(ax2,w_best,run,train_valid = 'validate')\n            self.draw_boundary(ax2,run,w_best,train_valid = 'validate')\n\n            #### plot training / validation errors ####\n            self.plot_train_valid_errors(ax3,k,train_errors,valid_errors,num_units)\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        \n        clear_output()  \n\n    ##### draw boundary #####\n    def draw_boundary(self,ax,run,w,train_valid):\n        ### create boundary data ###\n        # get visual boundary\n        xmin1 = np.min(self.x[0,:])\n        xmax1 = np.max(self.x[0,:])\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        xmin2 = np.min(self.x[1,:])\n        xmax2 = np.max(self.x[1,:])\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2    \n        \n        # plot boundary for 2d plot\n        r1 = np.linspace(xmin1,xmax1,300)\n        r2 = np.linspace(xmin2,xmax2,300)\n        s,t = np.meshgrid(r1,r2)\n        s = np.reshape(s,(np.size(s),1))\n        t = np.reshape(t,(np.size(t),1))\n        h = np.concatenate((s,t),axis = 1)\n        \n        # plot total fit\n        cost = run.cost\n        model = run.model\n        normalizer = run.normalizer\n\n        if train_valid == 'validate':\n            model = run.valid_model\n        a = model(normalizer(h.T),w)\n        \n        # compute model on train data\n        z1 = np.sign(a)\n\n        # reshape it\n        s.shape = (np.size(r1),np.size(r2))\n        t.shape = (np.size(r1),np.size(r2))     \n        z1.shape = (np.size(r1),np.size(r2))\n        \n        #### plot contour, color regions ####\n        ax.contour(s,t,z1,colors='k', linewidths=2.5,levels = [0],zorder = 2)\n        ax.contourf(s,t,z1,colors = [self.colors[1],self.colors[0]],alpha = 0.15,levels = range(-1,2))\n        \n    ######## show N = 2 static image ########\n    # show coloring of entire space\n    def static_N2_simple(self,ax,w_best,runner,train_valid):\n        cost = runner.cost\n        predict = runner.model\n        feat = runner.feature_transforms\n        normalizer = runner.normalizer\n        inverse_nornalizer = runner.inverse_normalizer\n      \n        # or just take last weights\n        self.w = w_best\n        \n        ### create boundary data ###\n        xmin1 = np.min(self.x[0,:])\n        xmax1 = np.max(self.x[0,:])\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        xmin2 = np.min(self.x[1,:])\n        xmax2 = np.max(self.x[1,:])\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2    \n\n        ### loop over two panels plotting each ###\n        # plot training points\n        if train_valid == 'train':\n            # reverse normalize data\n            x_train = inverse_nornalizer(runner.x_train).T\n            y_train = runner.y_train\n            \n            # plot data\n            ind0 = np.argwhere(y_train == +1)\n            ind0 = [v[1] for v in ind0]\n            ax.scatter(x_train[ind0,0],x_train[ind0,1],s = 45, color = self.colors[0], edgecolor = [0,0.7,1],linewidth = 1,zorder = 3)\n\n            ind1 = np.argwhere(y_train == -1)\n            ind1 = [v[1] for v in ind1]\n            ax.scatter(x_train[ind1,0],x_train[ind1,1],s = 45, color = self.colors[1], edgecolor = [0,0.7,1],linewidth = 1,zorder = 3)\n            ax.set_title('training data',fontsize = 15)\n\n        if train_valid == 'validate':\n            # reverse normalize data\n            x_valid = inverse_nornalizer(runner.x_valid).T\n            y_valid = runner.y_valid\n        \n            # plot testing points\n            ind0 = np.argwhere(y_valid == +1)\n            ind0 = [v[1] for v in ind0]\n            ax.scatter(x_valid[ind0,0],x_valid[ind0,1],s = 45, color = self.colors[0], edgecolor = [1,0.8,0.5],linewidth = 1,zorder = 3)\n\n            ind1 = np.argwhere(y_valid == -1)\n            ind1 = [v[1] for v in ind1]\n            ax.scatter(x_valid[ind1,0],x_valid[ind1,1],s = 45, color = self.colors[1], edgecolor = [1,0.8,0.5],linewidth = 1,zorder = 3)\n            ax.set_title('validation data',fontsize = 15)\n                \n        if train_valid == 'original':\n            # plot all points\n            ind0 = np.argwhere(self.y == +1)\n            ind0 = [v[1] for v in ind0]\n            ax.scatter(self.x[0,ind0],self.x[1,ind0],s = 55, color = self.colors[0], edgecolor = 'k',linewidth = 1,zorder = 3)\n\n            ind1 = np.argwhere(self.y == -1)\n            ind1 = [v[1] for v in ind1]\n            ax.scatter(self.x[0,ind1],self.x[1,ind1],s = 55, color = self.colors[1], edgecolor = 'k',linewidth = 1,zorder = 3)\n            ax.set_title('original data',fontsize = 15)\n\n        # cleanup panel\n        ax.set_xlabel(r'$x_1$',fontsize = 15)\n        ax.set_ylabel(r'$x_2$',fontsize = 15,rotation = 0,labelpad = 20)\n        ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n        ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n        \n    def plot_train_valid_errors(self,ax,k,train_errors,valid_errors,num_units):\n        num_elements = np.arange(len(train_errors))\n\n        ax.plot([v+1 for v in num_elements[:k+1]] ,train_errors[:k+1],color = [0,0.7,1],linewidth = 2.5,zorder = 1,label = 'training')\n        #ax.scatter([v+1  for v in num_elements[:k+1]] ,train_errors[:k+1],color = [0,0.7,1],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n\n        ax.plot([v+1  for v in num_elements[:k+1]] ,valid_errors[:k+1],color = [1,0.8,0.5],linewidth = 2.5,zorder = 1,label = 'validation')\n        #ax.scatter([v+1  for v in num_elements[:k+1]] ,valid_errors[:k+1],color= [1,0.8,0.5],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n        ax.set_title('misclassifications',fontsize = 15)\n\n        # cleanup\n        ax.set_xlabel('step',fontsize = 12)\n\n        # cleanp panel                \n        num_iterations = len(train_errors)\n        minxc = 0.5\n        maxxc = len(num_elements) + 0.5\n        minc = min(min(copy.deepcopy(train_errors)),min(copy.deepcopy(valid_errors)))\n        maxc = max(max(copy.deepcopy(train_errors[:10])),max(copy.deepcopy(valid_errors[:10])))\n        gapc = (maxc - minc)*0.25\n        minc -= gapc\n        maxc += gapc\n        \n        ax.set_xlim([minxc,maxxc])\n        ax.set_ylim([minc,maxc])\n        \n        tics = np.arange(1,len(num_elements)+1 + len(num_elements)/float(5),len(num_elements)/float(5))\n        labels = np.arange(1,num_units+1 + num_units/float(5),num_units/float(5))\n        labels = [int(np.around(v,decimals=-1)) for v in labels]\n\n        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n        ax.set_xticks(tics)\n        ax.set_xticklabels(labels)\n\n\n        """
mlrefined_libraries/multilayer_perceptron_library/early_stop_regression_animator.py,14,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom matplotlib.ticker import FormatStrFormatter\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\nfrom matplotlib.ticker import MaxNLocator, FuncFormatter\n\n# import autograd functionality\nimport autograd.numpy as np\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\nfrom matplotlib.ticker import FormatStrFormatter\nfrom inspect import signature\n\nclass Visualizer:\n    '''\n    Visualize cross validation performed on N = 2 dimensional input classification datasets\n    '''\n    #### initialize ####\n    def __init__(self,csvname):\n        # grab input\n        data = np.loadtxt(csvname,delimiter = ',')\n        self.x = data[:-1,:]\n        self.y = data[-1:,:] \n        \n        self.colors = ['salmon','cornflowerblue','lime','bisque','mediumaquamarine','b','m','g']\n\n    \n    #### animate multiple runs on single regression ####\n    def animate_trainval_earlystop(self,savepath,run,frames,**kwargs):\n        train_errors = run.train_cost_histories[0]\n        valid_errors = run.valid_cost_histories[0]\n        weight_history = run.weight_histories[0]\n        num_units = len(weight_history)\n\n        # select subset of runs\n        inds = np.arange(0,len(weight_history),int(len(weight_history)/float(frames)))\n        weight_history = [weight_history[v] for v in inds]\n        train_errors = [train_errors[v] for v in inds]\n        valid_errors = [valid_errors[v] for v in inds]\n       \n        # construct figure\n        fig = plt.figure(figsize = (6,6))\n        artist = fig\n\n        # create subplot with 4 panels, plot input function in center plot\n        gs = gridspec.GridSpec(2, 2)\n        ax = plt.subplot(gs[0]); \n        ax1 = plt.subplot(gs[2]); \n        ax2 = plt.subplot(gs[3]); \n        ax3 = plt.subplot(gs[1]); \n        \n        # start animation\n        num_frames = len(inds)        \n        print ('starting animation rendering...')\n        def animate(k):\n            # clear panels\n            ax.cla()\n            ax1.cla()\n            ax2.cla()\n            ax3.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            #### plot training, testing, and full data ####            \n            # pluck out current weights \n            w_best = weight_history[k]\n            \n            # produce static img\n            self.draw_data(ax,w_best,run,train_valid = 'original')\n            self.draw_fit(ax,run,w_best,train_valid = 'train')\n            \n            self.draw_data(ax1,w_best,run,train_valid = 'train')\n            self.draw_fit(ax1,run,w_best,train_valid = 'train')\n            self.draw_data(ax2,w_best,run,train_valid = 'validate')\n            self.draw_fit(ax2,run,w_best,train_valid = 'validate')\n\n            #### plot training / validation errors ####\n            self.plot_train_valid_errors(ax3,k,train_errors,valid_errors,num_units)\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        \n        clear_output()   \n    \n    ##### draw boundary #####\n    def draw_fit(self,ax,run,w,train_valid):\n        ### create boundary data ###\n        # get visual boundary\n        xmin1 = np.min(copy.deepcopy(self.x))\n        xmax1 = np.max(copy.deepcopy(self.x))\n        xgap1 = (xmax1 - xmin1)*0.1\n        xmin1 -= xgap1\n        xmax1 += xgap1 \n        \n        ymin1 = np.min(copy.deepcopy(self.y))\n        ymax1 = np.max(copy.deepcopy(self.y))\n        ygap1 = (ymax1 - ymin1)*0.3\n        ymin1 -= ygap1\n        ymax1 += ygap1 \n        \n        # plot boundary for 2d plot\n        s = np.linspace(xmin1,xmax1,300)[np.newaxis,:]\n        \n        # plot total fit\n        cost = run.cost\n        model = run.model\n        normalizer = run.normalizer\n        t = model(normalizer(s),w)\n        \n        #### plot contour, color regions ####\n        ax.plot(s.flatten(),t.flatten(),c = 'magenta',linewidth = 2.5,zorder = 3)  \n        ax.set_xlim([xmin1,xmax1])\n        ax.set_ylim([ymin1,ymax1])\n        \n    ######## show N = 2 static image ########\n    # show coloring of entire space\n    def draw_data(self,ax,w_best,runner,train_valid):\n        cost = runner.cost\n        predict = runner.model\n        feat = runner.feature_transforms\n        normalizer = runner.normalizer\n        inverse_nornalizer = runner.inverse_normalizer\n      \n        # or just take last weights\n        self.w = w_best\n        \n        ### create boundary data ###\n        xmin1 = np.min(copy.deepcopy(self.x))\n        xmax1 = np.max(copy.deepcopy(self.x))\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        ### loop over two panels plotting each ###\n        # plot training points\n        if train_valid == 'train':\n            # reverse normalize data\n            x_train = inverse_nornalizer(runner.x_train).T\n            y_train = runner.y_train\n            \n            # plot data\n            ax.scatter(x_train,y_train,s = 45, color = [0,0.7,1], edgecolor = 'k',linewidth = 1,zorder = 3)\n            ax.set_title('training data',fontsize = 15)\n\n        if train_valid == 'validate':\n            # reverse normalize data\n            x_valid = inverse_nornalizer(runner.x_valid).T\n            y_valid = runner.y_valid\n        \n            # plot testing points\n            ax.scatter(x_valid,y_valid,s = 45, color = [1,0.8,0.5], edgecolor = 'k',linewidth = 1,zorder = 3)\n\n            ax.set_title('validation data',fontsize = 15)\n                \n        if train_valid == 'original':\n            # plot all points\n            ax.scatter(self.x,self.y,s = 55, color = 'k', edgecolor = 'w',linewidth = 1,zorder = 3)\n            ax.set_title('original data',fontsize = 15)\n\n        # cleanup panel\n        ax.set_xlabel(r'$x_1$',fontsize = 15)\n        ax.set_ylabel(r'$x_2$',fontsize = 15,rotation = 0,labelpad = 20)\n        ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n        ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n        \n    def plot_train_valid_errors(self,ax,k,train_errors,valid_errors,num_units):\n        num_elements = np.arange(len(train_errors))\n\n        ax.plot([v+1 for v in num_elements[:k+1]] ,train_errors[:k+1],color = [0,0.7,1],linewidth = 2.5,zorder = 1,label = 'training')\n        #ax.scatter([v+1  for v in num_elements[:k+1]] ,train_errors[:k+1],color = [0,0.7,1.0],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n\n        ax.plot([v+1  for v in num_elements[:k+1]] ,valid_errors[:k+1],color = [1,0.8,0.5],linewidth = 2.5,zorder = 1,label = 'validation')\n        #ax.scatter([v+1  for v in num_elements[:k+1]] ,valid_errors[:k+1],color= [1,0.8,0.5],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n        ax.set_title('errors',fontsize = 15)\n\n        # cleanup\n        ax.set_xlabel('step',fontsize = 12)\n\n        # cleanp panel                \n        num_iterations = len(train_errors)\n        minxc = 0.5\n        maxxc = len(num_elements) + 0.5\n        minc = min(min(copy.deepcopy(train_errors)),min(copy.deepcopy(valid_errors)))\n        maxc = max(max(copy.deepcopy(train_errors[:10])),max(copy.deepcopy(valid_errors[:10])))\n        gapc = (maxc - minc)*0.25\n        minc -= gapc\n        maxc += gapc\n        \n        ax.set_xlim([minxc,maxxc])\n        ax.set_ylim([minc,maxc])\n        \n        tics = np.arange(1,len(num_elements)+1 + len(num_elements)/float(5),len(num_elements)/float(5))\n        labels = np.arange(1,num_units+1 + num_units/float(5),num_units/float(5))\n        labels = [int(np.around(v,decimals=-1)) for v in labels]\n\n        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n        ax.set_xticks(tics)\n        ax.set_xticklabels(labels)\n\n\n        """
mlrefined_libraries/multilayer_perceptron_library/network_learner.py,14,"b""# clear display\nfrom IPython.display import clear_output\n\n# import autograd functionality\nimport autograd.numpy as np\n\n# aome basic libraries\nimport math\nimport time\nimport copy\n\n# plotting functionality\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\n##### import network functionality #####\nfrom . import optimizers\nfrom . import cost_functions\nfrom . import architectures\n\nclass Network:\n    '''\n    Normalized multilayer perceptron / feedforward network learner\n    '''    \n    \n    ###### load in training/testing data ######\n    def input_data(self,train_data,test_data,normalize):\n        self.train_data = train_data\n        self.x_train = train_data[:,:-1]\n        self.y_train = train_data[:,-1:]\n        \n        # normalize training data?\n        if normalize == True:\n            # training data\n            self.x_means = np.mean(self.x_train,axis = 0)\n            self.x_stds = np.std(self.x_train,axis = 0)\n            self.x_train = self.normalize(self.x_train,self.x_means,self.x_stds)\n                            \n        # test data included?\n        if np.size(test_data) > 0:\n            self.test_data = test_data\n            self.x_test = test_data[:,:-1]\n            self.y_test = test_data[:,-1:]\n            self.x_test = self.normalize(self.x_test,self.x_means,self.x_stds)  \n        else:\n            self.test_data = []\n            self.x_test = []\n            self.y_test = []\n            \n            \n    # our normalization function\n    def normalize(self,data,data_mean,data_std):\n        normalized_data = (data - data_mean)/(data_std + 10**(-5))\n        return normalized_data          \n    \n    ###### setup architecture ######\n    def architecture_settings(self,activation_name,layer_sizes):\n        # create instance of architectures\n        self.architectures = architectures.Setup()\n        \n        # setup architecture\n        self.activation_name = activation_name\n        self.layer_sizes = layer_sizes\n        self.architectures.choose_architecture(activation_name)\n        \n    ###### chose cost #####\n    def choose_cost(self,cost_name):\n        self.cost_name = cost_name\n            \n        # if cost is multiclass softmax then flatten output, this is done because the compact multiclass softmax \n        # written with broadcasting with y, and necessary for y to be flattened for this\n        if self.cost_name == 'multiclass_softmax':\n            self.y_train = np.asarray([int(v) for v in self.y_train])\n            self.y_train = self.y_train.flatten()\n            \n            self.y_test = np.asarray([int(v) for v in self.y_test])\n            self.y_test = self.y_test.flatten()\n        \n        # create instance of cost functions\n        cost_function = cost_functions.Setup()\n        \n        # determine the right predict function\n        # create instance of cost\n        cost_function.choose_cost(cost_name,self.predict_training,self.x_train,self.y_train)\n        self.training_cost = cost_function.cost\n    \n    ########## predict functions ##########\n    # predict for training\n    def predict_training(self,x,w):\n        # feature trasnsformations\n        f = self.architectures.training_architecture(x,w[0])\n\n        # compute linear model\n        vals = np.dot(f,w[1])\n        return vals\n    \n    # predict for testing \n    def predict_testing(self,x,w):     \n        # feature trasnsformations\n        f,stats = self.architectures.testing_architecture(x,w[0],self.train_stats)\n\n        # compute linear model\n        vals = np.dot(f,w[1])\n        return vals\n        \n    ###### setup optimizer ######\n    def optimizer_settings(self,alpha,max_its,**kwargs):\n        # generate initial weights\n        scale = 0.1\n        if 'scale' in kwargs:\n            scale = kwargs['scale']\n        self.w_init = self.architectures.initializer(self.layer_sizes,scale)\n        \n        # other settings\n        self.alpha = alpha\n        self.max_its =  max_its\n        self.beta = 0\n        if 'beta' in kwargs:\n            self.beta = kwargs['beta']\n        self.version = 'normalized'\n        if 'version' in kwargs:\n            self.version = kwargs['version']\n        \n        # create instance of optimizers\n        self.opt = optimizers.Setup()\n        \n    def fit(self,**kwargs):\n        verbose = False\n        if 'verbose' in kwargs:\n            verbose = kwargs['verbose']\n            \n        # run optimizer\n        self.weight_history = self.opt.gradient_descent(self.training_cost,self.w_init,self.alpha,self.max_its,self.beta,self.version,verbose=verbose)\n        \n    ####### show cost function plots #######\n    def compute_cost_plots(self): \n        # create instance of cost functions\n        cost_function2 = cost_functions.Setup()\n        \n        # loop over weights in history and construct cost function plots for training and testing data\n        self.train_cost_history = []\n        self.test_cost_history = []\n        self.training_stats = []\n        \n        # classification?  then record count history as well\n        if self.cost_name == 'twoclass_softmax' or self.cost_name == 'multiclass_softmax':\n            self.train_count_history = []\n            self.test_count_history = []\n        \n        # loop over weights and record cost values\n        for w in self.weight_history:\n            # use testing architecture to gather stats on training data network normalization\n            a_padded,self.train_stats = self.architectures.testing_architecture(self.x_train,w[0],[])\n            self.training_stats.append(self.train_stats)\n\n            # evalaute both training and testing data using testing predictor, which will normalize network\n            # with respect to current weights and training data\n            cost_function2.choose_cost(self.cost_name,self.predict_testing,self.x_train,self.y_train)\n            testing_cost = cost_function2.cost   \n            self.train_cost_history.append(testing_cost(w))\n            \n            # classification?  then record misclassification data too\n            if self.cost_name == 'twoclass_softmax':\n                cost_function2.choose_cost('twoclass_counter',self.predict_testing,self.x_train,self.y_train)\n                testing_cost = cost_function2.cost   \n                self.train_count_history.append(testing_cost(w))\n            if self.cost_name == 'multiclass_softmax':\n                cost_function2.choose_cost('multiclass_counter',self.predict_testing,self.x_train,self.y_train)\n                testing_cost = cost_function2.cost   \n                self.train_count_history.append(testing_cost(w))   \n            \n            # was test data included?  then compute error on this\n            if np.size(self.test_data) > 0:\n                cost_function2.choose_cost(self.cost_name,self.predict_testing,self.x_test,self.y_test)\n                testing_cost = cost_function2.cost\n                self.test_cost_history.append(testing_cost(w))\n                \n                # classification?  then record misclassification data too\n                if self.cost_name == 'twoclass_softmax':\n                    cost_function2.choose_cost('twoclass_counter',self.predict_testing,self.x_test,self.y_test)\n                    testing_cost = cost_function2.cost   \n                    self.test_count_history.append(testing_cost(w))\n                if self.cost_name == 'multiclass_softmax':\n                    cost_function2.choose_cost('multiclass_counter',self.predict_testing,self.x_test,self.y_test)\n                    testing_cost = cost_function2.cost   \n                    self.test_count_history.append(testing_cost(w))   \n                \n            \n            ### if performing classification record number of micclassifications as well ###\n            \n    # plot cost function histories    \n    def plot_histories(self,start):\n        ### plot\n        # initialize figure\n        fig = plt.figure(figsize = (8,3))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); \n        \n        if self.cost_name == 'twoclass_softmax' or self.cost_name == 'multiclass_softmax':\n            gs = gridspec.GridSpec(1, 2) \n            ax = plt.subplot(gs[0]); \n            ax2 = plt.subplot(gs[1]); \n\n        # now plot each, one per panel\n        ax.plot(np.arange(start,len(self.train_cost_history),1),self.train_cost_history[start:],linewidth = 3*(0.8)**(1)) \n        ax.plot(np.arange(start,len(self.test_cost_history),1),self.test_cost_history[start:],linewidth = 3*(0.8)**(2)) \n\n        # label up\n        ax.set_xlabel('iteration')\n        ax.set_ylabel('cost function val')\n        ax.set_title('cost function history')\n        \n        # test data included?  make sure to include correct labeling\n        if np.size(self.test_data) > 0:\n            ax.legend(['training','testing'],loc='upper right')\n        else:\n            ax.legend(['training'],loc='upper right')\n            \n        # classification?  then record count history as well\n        if self.cost_name == 'twoclass_softmax' or self.cost_name == 'multiclass_softmax':\n           # now plot each, one per panel\n            ax2.plot(np.arange(start,len(self.train_count_history),1),self.train_count_history[start:],linewidth = 3*(0.8)**(1)) \n            ax2.plot(np.arange(start,len(self.test_count_history),1),self.test_count_history[start:],linewidth = 3*(0.8)**(2)) \n            \n            # label up\n            ax2.set_xlabel('iteration')\n            ax2.set_ylabel('misclassifications')\n            ax2.set_title('misclassification history')\n            \n            # test data included?  make sure to include correct labeling\n            if np.size(self.test_data) > 0:\n                ax2.legend(['training','testing'],loc='upper right')\n            else:\n                ax2.legend(['training'],loc='upper right')\n"""
mlrefined_libraries/multilayer_perceptron_library/nonlinear_classification_visualizer.py,92,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nimport autograd.numpy as np\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\nfrom matplotlib.ticker import FormatStrFormatter\nfrom inspect import signature\n\nclass Visualizer:\n    \'\'\'\n    Visualize linear regression in 2 and 3 dimensions.  For single input cases (2 dimensions) the path of gradient descent on the cost function can be animated.\n    \'\'\'\n    #### initialize ####\n    def __init__(self,csvname):\n        # grab input\n        data = np.loadtxt(csvname,delimiter = \',\')\n        data = data.T\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n\n        self.colors = [\'salmon\',\'cornflowerblue\',\'lime\',\'bisque\',\'mediumaquamarine\',\'b\',\'m\',\'g\']\n    \n    ######## show N = 1 static image ########\n    # show coloring of entire space\n    def static_N1_img(self,w_best,cost,predict,**kwargs):\n        # or just take last weights\n        self.w = w_best\n        \n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        \n        show_cost = False\n        if show_cost == True:   \n            gs = gridspec.GridSpec(1, 3,width_ratios = [1,1,1],height_ratios = [1]) \n            \n            # create third panel for cost values\n            ax3 = plt.subplot(gs[2],aspect = \'equal\')\n            \n        else:\n            gs = gridspec.GridSpec(1, 2,width_ratios = [1,1]) \n\n        #### left plot - data and fit in original space ####\n        # setup current axis\n        ax = plt.subplot(gs[0]);\n        ax2 = plt.subplot(gs[1],aspect = \'equal\');\n        \n        # scatter original points\n        self.scatter_pts(ax,self.x)\n        ax.set_xlabel(r\'$x$\', fontsize = 14,labelpad = 10)\n        ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 14,labelpad = 10)\n        \n        # create fit\n        gapx = (max(self.x) - min(self.x))*0.1\n        s = np.linspace(min(self.x) - gapx,max(self.x) + gapx,100)\n        t = [np.tanh(predict(np.asarray([v]),self.w)) for v in s]\n        \n        # plot fit\n        ax.plot(s,t,c = \'lime\')\n        ax.axhline(linewidth=0.5, color=\'k\',zorder = 1)\n\n        #### plot data in new space in middle panel (or right panel if cost function decrease plot shown ) #####\n        if \'f_x\' in kwargs:\n            f_x = kwargs[\'f_x\']\n\n            # scatter points\n            self.scatter_pts(ax2,f_x)\n\n            # create and plot fit\n            s = np.linspace(min(f_x) - 0.1,max(f_x) + 0.1,100)\n            t = np.tanh(self.w[0] + self.w[1]*s)\n            ax2.plot(s,t,c = \'lime\')\n            ax2.set_xlabel(r\'$f\\,(x)$\', fontsize = 14,labelpad = 10)\n            ax2.set_ylabel(r\'$y$\', rotation = 0,fontsize = 14,labelpad = 10)\n        \n        if \'f2_x\' in kwargs:\n            ax2 = plt.subplot(gs[1],projection = \'3d\');   \n            view = kwargs[\'view\']\n            \n            # get input\n            f1_x = kwargs[\'f1_x\']\n            f2_x = kwargs[\'f2_x\']\n\n            # scatter points\n            f1_x = np.asarray(f1_x)\n            f1_x.shape = (len(f1_x),1)\n            f2_x = np.asarray(f2_x)\n            f2_x.shape = (len(f2_x),1)\n            xtran = np.concatenate((f1_x,f2_x),axis = 1)\n            self.scatter_pts(ax2,xtran)\n\n            # create and plot fit\n            s1 = np.linspace(min(f1_x) - 0.1,max(f1_x) + 0.1,100)\n            s2 = np.linspace(min(f2_x) - 0.1,max(f2_x) + 0.1,100)\n            t1,t2 = np.meshgrid(s1,s2)\n            \n            # compute fitting hyperplane\n            t1.shape = (len(s1)**2,1)\n            t2.shape = (len(s2)**2,1)\n            r = np.tanh(self.w[0] + self.w[1]*t1 + self.w[2]*t2)\n            \n            # reshape for plotting\n            t1.shape = (len(s1),len(s1))\n            t2.shape = (len(s2),len(s2))\n            r.shape = (len(s1),len(s2))\n            ax2.plot_surface(t1,t2,r,alpha = 0.1,color = \'lime\',rstride=10, cstride=10,linewidth=0.5,edgecolor = \'k\')\n                \n            # label axes\n            self.move_axis_left(ax2)\n            ax2.set_xlabel(r\'$f_1(x)$\', fontsize = 12,labelpad = 5)\n            ax2.set_ylabel(r\'$f_2(x)$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax2.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n            ax2.view_init(view[0],view[1])\n            \n        # plot cost function decrease\n        if  show_cost == True: \n            # compute cost eval history\n            g = cost\n            cost_evals = []\n            for i in range(len(w_hist)):\n                W = w_hist[i]\n                cost = g(W)\n                cost_evals.append(cost)\n     \n            # plot cost path - scale to fit inside same aspect as classification plots\n            num_iterations = len(w_hist)\n            minx = min(self.x)\n            maxx = max(self.x)\n            gapx = (maxx - minx)*0.1\n            minc = min(cost_evals)\n            maxc = max(cost_evals)\n            gapc = (maxc - minc)*0.1\n            minc -= gapc\n            maxc += gapc\n            \n            s = np.linspace(minx + gapx,maxx - gapx,num_iterations)\n            scaled_costs = [c/float(max(cost_evals))*(maxx-gapx) - (minx+gapx) for c in cost_evals]\n            ax3.plot(s,scaled_costs,color = \'k\',linewidth = 1.5)\n            ax3.set_xlabel(\'iteration\',fontsize = 12)\n            ax3.set_title(\'cost function plot\',fontsize = 12)\n            \n            # rescale number of iterations and cost function value to fit same aspect ratio as other two subplots\n            ax3.set_xlim(minx,maxx)\n            #ax3.set_ylim(minc,maxc)\n            \n            ### set tickmarks for both axes - requries re-scaling   \n            # x axis\n            marks = range(0,num_iterations,round(num_iterations/5.0))\n            ax3.set_xticks(s[marks])\n            labels = [item.get_text() for item in ax3.get_xticklabels()]\n            ax3.set_xticklabels(marks)\n            \n            ### y axis\n            r = (max(scaled_costs) - min(scaled_costs))/5.0\n            marks = [min(scaled_costs) + m*r for m in range(6)]\n            ax3.set_yticks(marks)\n            labels = [item.get_text() for item in ax3.get_yticklabels()]\n            \n            r = (max(cost_evals) - min(cost_evals))/5.0\n            marks = [int(min(cost_evals) + m*r) for m in range(6)]\n            ax3.set_yticklabels(marks)\n\n    ######## show N = 2 static image ########\n    # show coloring of entire space\n    def static_N2_img(self,w_best,runner,**kwargs):\n        cost = runner.cost\n        predict = runner.model\n        feat = runner.feature_transforms\n        normalizer = runner.normalizer\n                \n        # count parameter layers of input to feature transform\n        sig = signature(feat)\n        sig = len(sig.parameters)\n\n        # or just take last weights\n        self.w = w_best\n        \n        zplane = \'on\'\n        if \'zplane\' in kwargs:\n            zplane = kwargs[\'zplane\']\n        view1 = [20,45]\n        if \'view1\' in kwargs:\n            view1 = kwargs[\'view1\']\n        view2 = [20,30]\n        if \'view2\' in kwargs:\n            view2 = kwargs[\'view2\']  \n            \n        # initialize figure\n        fig = plt.figure(figsize = (10,9))\n        gs = gridspec.GridSpec(2, 2,width_ratios = [1,1]) \n\n        #### left plot - data and fit in original space ####\n        # setup current axis\n        ax = plt.subplot(gs[0],aspect = \'equal\');\n        ax2 = plt.subplot(gs[1],aspect = \'equal\');\n        ax3 = plt.subplot(gs[2],projection = \'3d\');\n        ax4 = plt.subplot(gs[3],projection = \'3d\');\n        \n        ### cleanup left plots, create max view ranges ###\n        xmin1 = np.min(self.x[:,0])\n        xmax1 = np.max(self.x[:,0])\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n        ax.set_xlim([xmin1,xmax1])\n        ax3.set_xlim([xmin1,xmax1])\n\n        xmin2 = np.min(self.x[:,1])\n        xmax2 = np.max(self.x[:,1])\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2\n        ax.set_ylim([xmin2,xmax2])\n        ax3.set_ylim([xmin2,xmax2])\n\n        ymin = np.min(self.y)\n        ymax = np.max(self.y)\n        ygap = (ymax - ymin)*0.05\n        ymin -= ygap\n        ymax += ygap\n        ax3.set_zlim([ymin,ymax])\n        \n        ax3.axis(\'off\')\n        ax3.view_init(view1[0],view1[1])\n\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_xlabel(r\'$x_1$\',fontsize = 15)\n        ax.set_ylabel(r\'$x_2$\',fontsize = 15,rotation = 0,labelpad = 20)\n            \n        #### plot left panels ####\n        # plot points in 2d and 3d\n        ind0 = np.argwhere(self.y == +1)\n        ax.scatter(self.x[ind0,0],self.x[ind0,1],s = 55, color = self.colors[0], edgecolor = \'k\')\n        ax3.scatter(self.x[ind0,0],self.x[ind0,1],self.y[ind0],s = 55, color = self.colors[0], edgecolor = \'k\')\n\n        ind1 = np.argwhere(self.y == -1)\n        ax.scatter(self.x[ind1,0],self.x[ind1,1],s = 55, color = self.colors[1], edgecolor = \'k\')\n        ax3.scatter(self.x[ind1,0],self.x[ind1,1],self.y[ind1],s = 55, color = self.colors[1], edgecolor = \'k\')\n       \n        # plot boundary for 2d plot\n        r1 = np.linspace(xmin1,xmax1,100)\n        r2 = np.linspace(xmin2,xmax2,100)\n        s,t = np.meshgrid(r1,r2)\n        s = np.reshape(s,(np.size(s),1))\n        t = np.reshape(t,(np.size(t),1))\n        h = np.concatenate((s,t),axis = 1)\n        z = predict(normalizer(h.T),self.w)\n        z = np.tanh(z)\n        \n        # reshape it\n        s.shape = (np.size(r1),np.size(r2))\n        t.shape = (np.size(r1),np.size(r2))     \n        z.shape = (np.size(r1),np.size(r2))\n        \n        #### plot contour, color regions ####\n        ax.contour(s,t,z,colors=\'k\', linewidths=2.5,levels = [0],zorder = 2)\n        ax.contourf(s,t,z,colors = [self.colors[1],self.colors[0]],alpha = 0.15,levels = range(-1,2))\n        ax3.plot_surface(s,t,z,alpha = 0.1,color = \'w\',rstride=10, cstride=10,linewidth=0.5,edgecolor = \'k\')\n\n        # plot zplane = 0 in left 3d panel - showing intersection of regressor with z = 0 (i.e., its contour, the separator, in the 3d plot too)?\n        if zplane == \'on\':\n            # plot zplane\n            ax3.plot_surface(s,t,z*0,alpha = 0.1,rstride=20, cstride=20,linewidth=0.15,color = \'w\',edgecolor = \'k\') \n            \n            # plot separator curve in left plot\n            ax3.contour(s,t,z,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n            ax3.contourf(s,t,z,colors = self.colors[1],levels = [0,1],zorder = 1,alpha = 0.1)\n            ax3.contourf(s,t,z+1,colors = self.colors[0],levels = [0,1],zorder = 1,alpha = 0.1)\n        \n        #### plot right panel scatter ####\n        # transform data\n        f = 0 \n        if sig == 1:\n            f = feat(normalizer(self.x.T)).T\n        else:\n            f = feat(normalizer(self.x.T),self.w[0]).T\n        x1 = f[:,0]\n        x2 = f[:,1]\n        #x1 = [f1(e) for e in self.x]\n        #x2 = [f2(e) for e in self.x]\n        ind0 = [v[0] for v in ind0]\n        ind1 = [v[0] for v in ind1]\n\n        # plot points on desired panel\n        v1 = [x1[e] for e in ind0]\n        v2 = [x2[e] for e in ind0]\n        ax2.scatter(v1,v2,s = 55, color = self.colors[0], edgecolor = \'k\')\n        ax4.scatter(v1,v2,self.y[ind0],s = 55, color = self.colors[0], edgecolor = \'k\')\n\n        v1 = [x1[e] for e in ind1]\n        v2 = [x2[e] for e in ind1]        \n        ax2.scatter(v1,v2,s = 55, color = self.colors[1], edgecolor = \'k\')\n        ax4.scatter(v1,v2,self.y[ind1],s = 55, color = self.colors[1], edgecolor = \'k\')\n        \n        ### cleanup right panels - making max viewing ranges ###\n        \n        xmin1 = np.min(x1)\n        xmax1 = np.max(x1)\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n        ax2.set_xlim([xmin1,xmax1])\n        ax4.set_xlim([xmin1,xmax1])\n\n        xmin2 = np.min(x2)\n        xmax2 = np.max(x2)\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2\n        \n        ax2.set_yticklabels([])\n        ax2.set_xticklabels([])\n        ax2.set_xticks([])\n        ax2.set_yticks([])\n        ax2.set_xlabel(r\'$f\\,_1\\left(\\mathbf{x}\\right)$\',fontsize = 15)\n        ax2.set_ylabel(r\'$f\\,_2\\left(\\mathbf{x}\\right)$\',fontsize = 15)        \n        \n        ### plot right panel 3d scatter ###\n        #### make right plot contour ####\n        r1 = np.linspace(xmin1,xmax1,100)\n        r2 = np.linspace(xmin2,xmax2,100)\n        s,t = np.meshgrid(r1,r2)\n        \n        s.shape = (1,len(r1)**2)\n        t.shape = (1,len(r2)**2)\n       # h = np.vstack((s,t))\n       # h = feat(normalizer(h))  \n       # s = h[0,:]\n       # t = h[1,:]\n        z = 0\n        if sig == 1:\n            z = self.w[0] + self.w[1]*s + self.w[2]*t\n        else:\n            z = self.w[1][0] + self.w[1][1]*s + self.w[1][2]*t\n        z = np.tanh(np.asarray(z))\n        \n        s.shape = (np.size(r1),np.size(r2))\n        t.shape = (np.size(r1),np.size(r2))\n        z.shape = (np.size(r1),np.size(r2))\n\n        ax2.contour(s,t,z,colors=\'k\', linewidths=2.5,levels = [0],zorder = 2)\n        ax2.contourf(s,t,z,colors = [self.colors[1],self.colors[0]],alpha = 0.15,levels = range(-1,2))\n\n        #### plot right surface plot ####\n        # plot regression surface\n        ax4.plot_surface(s,t,z,alpha = 0.1,color = \'w\',rstride=10, cstride=10,linewidth=0.5,edgecolor = \'k\')\n            \n        # plot zplane = 0 in left 3d panel - showing intersection of regressor with z = 0 (i.e., its contour, the separator, in the 3d plot too)?\n        if zplane == \'on\':\n            # plot zplane\n            ax4.plot_surface(s,t,z*0,alpha = 0.1,rstride=20, cstride=20,linewidth=0.15,color = \'w\',edgecolor = \'k\') \n            \n            # plot separator curve in left plot\n            ax4.contour(s,t,z,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n            ax4.contourf(s,t,z,colors = self.colors[0],levels = [0,1],zorder = 1,alpha = 0.1)\n            ax4.contourf(s,t,z+1,colors = self.colors[1],levels = [0,1],zorder = 1,alpha = 0.1)\n   \n        ax2.set_ylim([xmin2,xmax2])\n        ax4.set_ylim([xmin2,xmax2])\n\n        ax4.axis(\'off\')\n        ax4.view_init(view2[0],view2[1])\n        ax4.set_zlim([ymin,ymax])\n\n    ######## show N = 2 static image ########\n    # show coloring of entire space\n    def static_N2_simple(self,w_best,runner,**kwargs):\n        cost = runner.cost\n        predict = runner.model\n        feat = runner.feature_transforms\n        normalizer = runner.normalizer\n                \n        # count parameter layers of input to feature transform\n        sig = signature(feat)\n        sig = len(sig.parameters)\n\n        # or just take last weights\n        self.w = w_best\n\n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(9,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 2) \n        ax2 = plt.subplot(gs[1],aspect = \'equal\'); \n        ax1 = plt.subplot(gs[0],projection=\'3d\'); \n\n        # scatter points\n        self.scatter_pts(ax1,self.x)\n\n        ### from above\n        ax2.set_xlabel(r\'$x_1$\',fontsize = 15)\n        ax2.set_ylabel(r\'$x_2$\',fontsize = 15,rotation = 0,labelpad = 20)\n        ax2.xaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n        ax2.yaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n\n        # plot points in 2d and 3d\n        C = len(np.unique(self.y))\n        if C == 2:\n            ind0 = np.argwhere(self.y == +1)\n            ax2.scatter(self.x[ind0,0],self.x[ind0,1],s = 55, color = self.colors[0], edgecolor = \'k\')\n\n            ind1 = np.argwhere(self.y == -1)\n            ax2.scatter(self.x[ind1,0],self.x[ind1,1],s = 55, color = self.colors[1], edgecolor = \'k\')\n        else:\n            for c in range(C):\n                ind0 = np.argwhere(self.y == c)\n                ax2.scatter(self.x[ind0,0],self.x[ind0,1],s = 55, color = self.colors[c], edgecolor = \'k\')\n\n        self.move_axis_left(ax1)\n        ax1.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 5)\n        ax1.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n        ax1.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n\n        ### create surface and boundary plot ###\n        xmin1 = np.min(self.x[:,0])\n        xmax1 = np.max(self.x[:,0])\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        xmin2 = np.min(self.x[:,1])\n        xmax2 = np.max(self.x[:,1])\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2    \n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n            ax1.view_init(view[0],view[1])\n\n        # plot boundary for 2d plot\n        r1 = np.linspace(xmin1,xmax1,300)\n        r2 = np.linspace(xmin2,xmax2,300)\n        s,t = np.meshgrid(r1,r2)\n        s = np.reshape(s,(np.size(s),1))\n        t = np.reshape(t,(np.size(t),1))\n        h = np.concatenate((s,t),axis = 1)\n        z = predict(normalizer(h.T),self.w)\n        z = np.sign(z)\n        \n        # reshape it\n        s.shape = (np.size(r1),np.size(r2))\n        t.shape = (np.size(r1),np.size(r2))     \n        z.shape = (np.size(r1),np.size(r2))\n        \n        #### plot contour, color regions ####\n        ax2.contour(s,t,z,colors=\'k\', linewidths=2.5,levels = [0],zorder = 2)\n        ax2.contourf(s,t,z,colors = [self.colors[1],self.colors[0]],alpha = 0.15,levels = range(-1,2))\n        ax1.plot_surface(s,t,z,alpha = 0.1,color = \'w\',rstride=30, cstride=30,linewidth=0.5,edgecolor = \'k\')\n\n            \n    ###### plot plotting functions ######\n    def plot_data(self):\n        fig = 0\n        # plot data in two and one-d\n        if np.shape(self.x)[1] < 2:\n            # construct figure\n            fig, axs = plt.subplots(2,1, figsize=(4,4))\n            gs = gridspec.GridSpec(2,1,height_ratios = [6,1]) \n            ax1 = plt.subplot(gs[0],aspect = \'equal\');\n            ax2 = plt.subplot(gs[1],sharex = ax1); \n            \n            # set plotting limits\n            xmax = copy.deepcopy(max(self.x))\n            xmin = copy.deepcopy(min(self.x))\n            xgap = (xmax - xmin)*0.2\n            xmin -= xgap\n            xmax += xgap\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.5\n            ymin -= ygap\n            ymax += ygap    \n\n            ### plot in 2-d\n            ax1.scatter(self.x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n\n            # clean up panel\n            ax1.set_xlim([xmin,xmax])\n            ax1.set_ylim([ymin,ymax])\n            ax1.axhline(linewidth=0.5, color=\'k\',zorder = 1)\n            \n            ### plot in 1-d\n            ind0 = np.argwhere(self.y == +1)\n            ax2.scatter(self.x[ind0],np.zeros((len(self.x[ind0]))),s = 55, color = self.colors[0], edgecolor = \'k\',zorder = 3)\n\n            ind1 = np.argwhere(self.y == -1)\n            ax2.scatter(self.x[ind1],np.zeros((len(self.x[ind1]))),s = 55, color = self.colors[1], edgecolor = \'k\',zorder = 3)\n            ax2.set_yticks([0])\n            ax2.axhline(linewidth=0.5, color=\'k\',zorder = 1)\n        \n            ax2.xaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            ax2.yaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            \n        if np.shape(self.x)[1] == 2:\n            # construct figure\n            fig, axs = plt.subplots(1, 2, figsize=(9,4))\n\n            # create subplot with 2 panels\n            gs = gridspec.GridSpec(1, 2) \n            ax2 = plt.subplot(gs[1],aspect = \'equal\'); \n            ax1 = plt.subplot(gs[0],projection=\'3d\'); \n\n            # scatter points\n            self.scatter_pts(ax1,self.x)\n            \n            ### from above\n            ax2.set_xlabel(r\'$x_1$\',fontsize = 15)\n            ax2.set_ylabel(r\'$x_2$\',fontsize = 15,rotation = 0,labelpad = 20)\n            ax2.xaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            ax2.yaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            \n            # plot points in 2d and 3d\n            C = len(np.unique(self.y))\n            if C == 2:\n                ind0 = np.argwhere(self.y == +1)\n                ax2.scatter(self.x[ind0,0],self.x[ind0,1],s = 55, color = self.colors[0], edgecolor = \'k\')\n\n                ind1 = np.argwhere(self.y == -1)\n                ax2.scatter(self.x[ind1,0],self.x[ind1,1],s = 55, color = self.colors[1], edgecolor = \'k\')\n            else:\n                for c in range(C):\n                    ind0 = np.argwhere(self.y == c)\n                    ax2.scatter(self.x[ind0,0],self.x[ind0,1],s = 55, color = self.colors[c], edgecolor = \'k\')\n                    \n        \n            self.move_axis_left(ax1)\n            ax1.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 5)\n            ax1.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax1.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n        \n    # scatter points\n    def scatter_pts(self,ax,x):\n        if np.shape(x)[1] == 1:\n            # set plotting limits\n            xmax = copy.deepcopy(max(x))\n            xmin = copy.deepcopy(min(x))\n            xgap = (xmax - xmin)*0.2\n            xmin -= xgap\n            xmax += xgap\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n\n            # clean up panel\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            \n        if np.shape(x)[1] == 2:\n            # set plotting limits\n            xmax1 = copy.deepcopy(max(x[:,0]))\n            xmin1 = copy.deepcopy(min(x[:,0]))\n            xgap1 = (xmax1 - xmin1)*0.1\n            xmin1 -= xgap1\n            xmax1 += xgap1\n            \n            xmax2 = copy.deepcopy(max(x[:,1]))\n            xmin2 = copy.deepcopy(min(x[:,1]))\n            xgap2 = (xmax2 - xmin2)*0.1\n            xmin2 -= xgap2\n            xmax2 += xgap2\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(x[:,0],x[:,1],self.y.flatten(),s = 40,color = \'k\', edgecolor = \'w\',linewidth = 0.9)\n\n            # clean up panel\n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            ax.set_zlim([ymin,ymax])\n            \n            ax.set_xticks(np.arange(round(xmin1), round(xmax1)+1, 1.0))\n            ax.set_yticks(np.arange(round(xmin2), round(xmax2)+1, 1.0))\n            ax.set_zticks(np.arange(round(ymin), round(ymax)+1, 1.0))\n           \n            # clean up panel\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n           \n    # set axis in left panel\n    def move_axis_left(self,ax):\n        tmp_planes = ax.zaxis._PLANES \n        ax.zaxis._PLANES = ( tmp_planes[2], tmp_planes[3], \n                             tmp_planes[0], tmp_planes[1], \n                             tmp_planes[4], tmp_planes[5])\n        view_1 = (25, -135)\n        view_2 = (25, -45)\n        init_view = view_2\n        ax.view_init(*init_view) \n        \n        \n    # toy plot\n    def multiclass_plot(self,run,w,**kwargs):\n        model = run.model\n        normalizer = run.normalizer\n        \n        # grab args\n        view = [20,-70]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n \n        ### plot all input data ###\n        # generate input range for functions\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n\n        r = np.linspace(minx,maxx,600)\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        h = np.concatenate([w1_vals,w2_vals],axis = 1).T\n\n        g_vals = model(normalizer(h),w)\n        g_vals = np.asarray(g_vals)\n        g_vals = np.argmax(g_vals,axis = 0)\n\n        # vals for cost surface\n        w1_vals.shape = (len(r),len(r))\n        w2_vals.shape = (len(r),len(r))\n        g_vals.shape = (len(r),len(r))\n\n        # create figure to plot\n        fig = plt.figure(num=None, figsize=(12,5), dpi=80, facecolor=\'w\', edgecolor=\'k\')\n\n        ### create 3d plot in left panel\n        ax1 = plt.subplot(121,projection = \'3d\')\n        ax2 = plt.subplot(122)\n\n        fig.subplots_adjust(left=0,right=1,bottom=0,top=1)   # remove whitespace around 3d figure\n\n        # scatter points in both panels\n        class_nums = np.unique(self.y)\n        C = len(class_nums)\n        for c in range(C):\n            ind = np.argwhere(self.y == class_nums[c])\n            ind = [v[0] for v in ind]\n            ax1.scatter(self.x[ind,0],self.x[ind,1],self.y[ind],s = 80,color = self.colors[c],edgecolor = \'k\',linewidth = 1.5)\n            ax2.scatter(self.x[ind,0],self.x[ind,1],s = 110,color = self.colors[c],edgecolor = \'k\', linewidth = 2)\n            \n        # switch for 2class / multiclass view\n        if C == 2:\n            # plot regression surface\n            ax1.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'k\',rstride=20, cstride=20,linewidth=0,edgecolor = \'k\') \n\n            # plot zplane = 0 in left 3d panel - showing intersection of regressor with z = 0 (i.e., its contour, the separator, in the 3d plot too)?\n            ax1.plot_surface(w1_vals,w2_vals,g_vals*0,alpha = 0.1,rstride=20, cstride=20,linewidth=0.15,color = \'k\',edgecolor = \'k\') \n            \n            # plot separator in left plot z plane\n            ax1.contour(w1_vals,w2_vals,g_vals,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n\n            # color parts of plane with correct colors\n            ax1.contourf(w1_vals,w2_vals,g_vals+1,colors = self.colors[:],alpha = 0.1,levels = range(0,2))\n            ax1.contourf(w1_vals,w2_vals,-g_vals+1,colors = self.colors[1:],alpha = 0.1,levels = range(0,2))\n    \n            # plot separator in right plot\n            ax2.contour(w1_vals,w2_vals,g_vals,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n\n            # adjust height of regressor to plot filled contours\n            ax2.contourf(w1_vals,w2_vals,g_vals+1,colors = self.colors[:],alpha = 0.1,levels = range(0,C+1))\n\n            ### clean up panels\n            # set viewing limits on vertical dimension for 3d plot           \n            minz = min(copy.deepcopy(self.y))\n            maxz = max(copy.deepcopy(self.y))\n\n            gapz = (maxz - minz)*0.1\n            minz -= gapz\n            maxz += gapz\n\n        # multiclass view\n        else:   \n            ax1.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'w\',rstride=45, cstride=45,linewidth=0.25,edgecolor = \'k\')\n\n            for c in range(C):\n                # plot separator curve in left plot z plane\n                ax1.contour(w1_vals,w2_vals,g_vals - c,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n\n                # color parts of plane with correct colors\n                ax1.contourf(w1_vals,w2_vals,g_vals - c +0.5,colors = self.colors[c],alpha = 0.4,levels = [0,1])\n             \n                \n            # plot separator in right plot\n            ax2.contour(w1_vals,w2_vals,g_vals,colors = \'k\',levels = range(0,C+1),linewidths = 3,zorder = 1)\n            \n            # adjust height of regressor to plot filled contours\n            ax2.contourf(w1_vals,w2_vals,g_vals+0.5,colors = self.colors[:],alpha = 0.2,levels = range(0,C+1))\n\n            ### clean up panels\n            # set viewing limits on vertical dimension for 3d plot \n            minz = 0\n            maxz = max(copy.deepcopy(self.y))\n            gapz = (maxz - minz)*0.1\n            minz -= gapz\n            maxz += gapz\n            ax1.set_zlim([minz,maxz])\n\n            ax1.view_init(view[0],view[1]) \n\n        # clean up panel\n        ax1.xaxis.pane.fill = False\n        ax1.yaxis.pane.fill = False\n        ax1.zaxis.pane.fill = False\n\n        ax1.xaxis.pane.set_edgecolor(\'white\')\n        ax1.yaxis.pane.set_edgecolor(\'white\')\n        ax1.zaxis.pane.set_edgecolor(\'white\')\n\n        ax1.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax1.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax1.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        self.move_axis_left(ax1)\n        ax1.set_xlabel(r\'$x_1$\', fontsize = 16,labelpad = 5)\n        ax1.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 16,labelpad = 5)\n        ax1.set_zlabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 5)\n\n        ax2.set_xlabel(r\'$x_1$\', fontsize = 18,labelpad = 10)\n        ax2.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 18,labelpad = 15)\n        \n        \n    # toy plot\n    def show_individual_classifiers(self,run,w,**kwargs):\n        model = run.model\n        normalizer = run.normalizer\n        feat = run.feature_transforms\n        \n        # grab args\n        view = [20,-70]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n \n        ### plot all input data ###\n        # generate input range for functions\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n\n        r = np.linspace(minx,maxx,600)\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        h = np.concatenate([w1_vals,w2_vals],axis = 1).T\n\n        g_vals = model(normalizer(h),w)\n        g_vals = np.asarray(g_vals)\n        g_new = copy.deepcopy(g_vals).T\n        g_vals = np.argmax(g_vals,axis = 0)\n\n        # vals for cost surface\n        w1_vals.shape = (len(r),len(r))\n        w2_vals.shape = (len(r),len(r))\n        g_vals.shape = (len(r),len(r))\n\n        # count points\n        class_nums = np.unique(self.y)\n        C = int(len(class_nums))\n        \n        fig = plt.figure(figsize = (10,7))\n        gs = gridspec.GridSpec(2, C) \n\n        #### left plot - data and fit in original space ####\n        # setup current axis\n        ax1 = plt.subplot(gs[C],projection = \'3d\');\n        ax2 = plt.subplot(gs[C+1],aspect = \'equal\');\n        fig.subplots_adjust(left=0,right=1,bottom=0,top=1)   # remove whitespace around 3d figure\n\n        ##### plot top panels ####\n        for d in range(C):\n            # create panel\n            ax = plt.subplot(gs[d],aspect = \'equal\');\n                       \n            for c in range(C):\n                # plot points\n                ind = np.argwhere(self.y == class_nums[c])\n                ind = [v[0] for v in ind]\n                ax.scatter(self.x[ind,0],self.x[ind,1],s = 50,color = self.colors[c],edgecolor = \'k\', linewidth = 2)\n            \n            g_2 = np.sign(g_new[:,d])\n            g_2.shape = (len(r),len(r))\n\n            # plot separator curve \n            ax.contour(w1_vals,w2_vals,g_2+1,colors = \'k\',levels = [-1,1],linewidths = 4.5,zorder = 1,linestyle = \'-\')\n            ax.contour(w1_vals,w2_vals,g_2+1,colors = self.colors[d],levels = [-1,1],linewidths = 2.5,zorder = 1,linestyle = \'-\')\n                \n            ax.set_xlabel(r\'$x_1$\', fontsize = 18,labelpad = 10)\n            ax.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 18,labelpad = 15)\n        \n        ##### plot bottom panels ###\n        # scatter points in both bottom panels\n        for c in range(C):\n            ind = np.argwhere(self.y == class_nums[c])\n            ind = [v[0] for v in ind]\n            ax1.scatter(self.x[ind,0],self.x[ind,1],self.y[ind],s = 50,color = self.colors[c],edgecolor = \'k\',linewidth = 1.5)\n            ax2.scatter(self.x[ind,0],self.x[ind,1],s = 50,color = self.colors[c],edgecolor = \'k\', linewidth = 2)\n      \n        ax1.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'w\',rstride=45, cstride=45,linewidth=0.25,edgecolor = \'k\')\n\n        for c in range(C):\n            # plot separator curve in left plot z plane\n            ax1.contour(w1_vals,w2_vals,g_vals - c,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n\n            # color parts of plane with correct colors\n            ax1.contourf(w1_vals,w2_vals,g_vals - c +0.5,colors = self.colors[c],alpha = 0.4,levels = [0,1])\n             \n        # plot separator in right plot\n        ax2.contour(w1_vals,w2_vals,g_vals,colors = \'k\',levels = range(0,C+1),linewidths = 3,zorder = 1)\n\n        # adjust height of regressor to plot filled contours\n        ax2.contourf(w1_vals,w2_vals,g_vals+0.5,colors = self.colors[:],alpha = 0.2,levels = range(0,C+1))\n\n        ### clean up panels\n        # set viewing limits on vertical dimension for 3d plot \n        minz = 0\n        maxz = max(copy.deepcopy(self.y))\n        gapz = (maxz - minz)*0.1\n        minz -= gapz\n        maxz += gapz\n        ax1.set_zlim([minz,maxz])\n\n        ax1.view_init(view[0],view[1]) \n\n        # clean up panel\n        ax1.xaxis.pane.fill = False\n        ax1.yaxis.pane.fill = False\n        ax1.zaxis.pane.fill = False\n\n        ax1.xaxis.pane.set_edgecolor(\'white\')\n        ax1.yaxis.pane.set_edgecolor(\'white\')\n        ax1.zaxis.pane.set_edgecolor(\'white\')\n\n        ax1.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax1.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax1.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        self.move_axis_left(ax1)\n        ax1.set_xlabel(r\'$x_1$\', fontsize = 16,labelpad = 5)\n        ax1.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 16,labelpad = 5)\n        ax1.set_zlabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 5)\n\n        ax2.set_xlabel(r\'$x_1$\', fontsize = 18,labelpad = 10)\n        ax2.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 18,labelpad = 15)'"
mlrefined_libraries/multilayer_perceptron_library/optimizers.py,4,"b""# clear display\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad  \nimport autograd.numpy as np\nimport math\nimport time\nimport copy\nfrom autograd.misc.flatten import flatten_func\n\nclass Setup:\n    '''\n    Optimizer(s) for multilayer perceptron function\n    '''    \n        \n    ########## optimizer ##########\n    # gradient descent function\n    def gradient_descent(self,g,w,alpha,max_its,beta,version,**kwargs):\n        verbose = False\n        if 'verbose' in kwargs:\n            verbose = kwargs['verbose']\n        \n        # flatten the input function, create gradient based on flat function\n        g_flat, unflatten, w = flatten_func(g, w)\n        grad = compute_grad(g_flat)\n\n        # record history\n        w_hist = []\n        w_hist.append(unflatten(w))\n\n        # start gradient descent loop\n        z = np.zeros((np.shape(w)))      # momentum term\n\n        if verbose == True:\n            print ('starting optimization...')\n            \n        # over the line\n        for k in range(max_its):   \n            # plug in value into func and derivative\n            grad_eval = grad(w)\n            grad_eval.shape = np.shape(w)\n\n            ### normalized or unnormalized descent step? ###\n            if version == 'normalized':\n                grad_norm = np.linalg.norm(grad_eval)\n                if grad_norm == 0:\n                    grad_norm += 10**-6*np.sign(2*np.random.rand(1) - 1)\n                grad_eval /= grad_norm\n\n            # take descent step with momentum\n            z = beta*z + grad_eval\n            w = w - alpha*z\n\n            # record weight update\n            w_hist.append(unflatten(w))\n\n        if verbose == True:\n            print ('...optimization complete!')\n            time.sleep(1.5)\n            clear_output()\n            \n        return w_hist"""
mlrefined_libraries/multilayer_perceptron_library/perceptron_scaling_tools.py,25,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   \nfrom autograd import hessian as compute_hess\nimport autograd.numpy as np\nimport math\nimport time\nimport copy\n\nclass Visualizer:    \n    # standard normalization function \n    def standard_normalizer(self,x):\n        # compute the mean and standard deviation of the input\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n        \n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(x_stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((x_stds.shape))\n            adjust[ind] = 1.0\n            x_stds += adjust\n\n        # create standard normalizer function\n        normalizer = lambda data: (data - x_means)/x_stds\n\n        # create inverse standard normalizer\n        inverse_normalizer = lambda data: data*x_stds + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer\n\n    # fully evaluate our network features using the tensor of weights in w\n    def feature_transforms(self,a, w):    \n        # loop through each layer matrix\n        self.all_activations = []\n        for W in w:\n            #  pad with ones (to compactly take care of bias) for next layer computation        \n            o = np.ones((1,np.shape(a)[1]))\n            a = np.vstack((o,a))\n\n            # compute inner product with current layer weights\n            a = np.dot(a.T, W).T\n\n            # output of layer activation\n            a = self.activation(a)\n            \n            # normalized architecture or not?\n            if self.normalize == True:\n                normalizer,inverse_normalizer = self.standard_normalizer(a)\n                a = normalizer(a)\n                \n            # store activation for plotting\n            self.all_activations.append(a)\n        return a\n    \n    \n    def scatter_activations(self,ax):\n        g = self.all_activations\n        num_layers = len(g)\n        for b in range(num_layers):\n            f = g[b]\n            label = r'($f^{(' + str(b+1) + ')}_1,\\,f^{(' + str(b+1) + ')}_2$)'\n            ax.scatter(f[0,:],f[1,:], c = self.colors[b],s = 60, edgecolor = 'k',linewidth = 1,label=label)\n        \n        if num_layers == 1:\n            ax.set_xlabel(r'$f^{(1)}_1$', fontsize = 14,labelpad = 10)\n            ax.set_ylabel(r'$f^{(1)}_2$', rotation = 0,fontsize = 14,labelpad = 10)\n        \n    def shifting_distribution(self,savepath,run,frames,x,**kwargs):\n        self.colors = ['cyan','magenta','lime','orange']\n        \n        # select inds of history to plot\n        weight_history = run.weight_histories[0]\n        cost_history = run.cost_histories[0]\n        inds = np.arange(0,len(weight_history),int(len(weight_history)/float(frames)))\n        weight_history_sample = [weight_history[v] for v in inds]\n        cost_history_sample = [cost_history[v] for v in inds]\n        start = inds[0]\n        feature_transforms = run.feature_transforms\n                \n        # define activations\n        self.activation = np.tanh\n        if 'activation' in kwargs:\n            self.activation = kwargs['activation']\n        self.normalize = False\n        if 'normalize' in kwargs:\n            self.normalize = kwargs['normalize']\n        xmin = 0\n        xmax = 1\n        \n        ### set viewing limits for scatter plot ###\n        xmins = []\n        xmaxs = []\n        ymins = []\n        ymaxs = []\n        \n        for k in range(len(inds)):\n            current_ind = inds[k]\n            w_best = weight_history[current_ind]\n            bl = self.feature_transforms(x,w_best[0])\n            \n            # get all activations and look over each layer\n            g = self.all_activations\n            num_layers = len(g)\n            for b in range(num_layers):\n                f = g[b]\n    \n                xmin = np.min(copy.deepcopy(f[0,:]))\n                xmax = np.max(copy.deepcopy(f[0,:]))\n\n                ymin = np.min(copy.deepcopy(f[1,:]))\n                ymax = np.max(copy.deepcopy(f[1,:]))\n\n                xmins.append(xmin)\n                xmaxs.append(xmax)\n                ymins.append(ymin)\n                ymaxs.append(ymax)\n\n        xmin = min(xmins)\n        xmax = max(xmaxs)\n        xgap = (xmax - xmin)*0.1\n        xmin -= xgap\n        xmax += xgap\n        ymin = min(ymins)\n        ymax = max(ymaxs)\n        ygap = (ymax - ymin)*0.1    \n        ymin -= ygap\n        ymax += ygap\n            \n         # create figure \n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n\n        # create subplots\n        show_history = False\n        if 'show_history' in kwargs:\n            show_history = kwargs['show_history']\n\n        gs = gridspec.GridSpec(1,1)\n        ax = plt.subplot(gs[0]); \n        axs = [ax]\n        if show_history == True:\n            gs = gridspec.GridSpec(1,2)\n            ax = plt.subplot(gs[0]); \n            axs = [ax]\n            ax1 = plt.subplot(gs[1]); \n            axs.append(ax1)\n            c = 0\n\n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n        # start animation\n        num_frames = len(inds)\n        print ('starting animation rendering...')\n        def animate(k):        \n            # get current index to plot\n            current_ind = inds[k]\n\n            if show_history == True:\n                ax = axs[-1]\n                ax.cla()\n                ax.scatter(current_ind,cost_history[current_ind],s = 60,color = 'r',edgecolor = 'k',zorder = 3)\n                self.plot_cost_history(ax,cost_history,start=0)\n\n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n\n            # pluck out current weights \n            w_best = weight_history[current_ind]\n            f = self.feature_transforms(x,w_best[0])\n\n            # produce static img\n            ax = axs[0]\n            ax.cla()\n            self.scatter_activations(ax)\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            #ax.legend(loc=0, borderaxespad=0.)\n        \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate,frames=num_frames,interval = 25,blit=False)\n\n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()           \n            \n\n    # a small Python function for plotting the distributions of input features\n    def single_layer_animation(self,savepath,run,frames,x,**kwargs):\n        # select inds of history to plot\n        weight_history = run.weight_history\n        cost_history = run.cost_history\n        inds = np.arange(0,len(weight_history),int(len(weight_history)/float(frames)))\n        weight_history_sample = [weight_history[v] for v in inds]\n        cost_history_sample = [cost_history[v] for v in inds]\n        start = inds[0]\n        feature_transforms = run.feature_transforms\n                \n        # define activations\n        self.activation = np.tanh\n        if 'activation' in kwargs:\n            self.activation = kwargs['activation']\n        self.normalize = False\n        if 'normalize' in kwargs:\n            self.normalize = kwargs['normalize']\n\n        # create figure \n        fig = plt.figure(figsize = (9,6))\n        artist = fig\n\n        # create subplots\n        N = np.shape(feature_transforms(x,weight_history[0][0]))[0]\n        layer_sizes = []\n        self.feature_transforms(x,weight_history[0][0])\n        layer_sizes = [np.shape(v)[0] for v in self.all_activations]\n        num_layers = len(layer_sizes)\n        max_units = max(layer_sizes)\n        \n        show_history = False\n        if 'show_history' in kwargs:\n            show_history = kwargs['show_history']\n\n        gs = gridspec.GridSpec(num_layers,max_units)\n        axs = []\n        for n in range(num_layers*max_units):\n            ax = plt.subplot(gs[n]); \n            axs.append(ax)\n        if show_history == True:\n            gs = gridspec.GridSpec(num_layers + 1,max_units)\n            ax = plt.subplot(gs[0,:]); \n            axs = [ax]\n            c = 0\n            for n in range(num_layers):\n                current_layer = layer_sizes[n]\n                current_axs = []\n                for m in range(current_layer):\n                    ax = plt.subplot(gs[n+1,m]); \n                    current_axs.append(ax)\n                axs.append(current_axs)\n\n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n        # start animation\n        num_frames = len(inds)\n        print ('starting animation rendering...')\n        def animate(k):        \n            # get current index to plot\n            current_ind = inds[k]\n\n            if show_history == True:\n                ax = axs[0]\n                ax.cla()\n                ax.scatter(current_ind,cost_history[current_ind],s = 60,color = 'r',edgecolor = 'k',zorder = 3)\n                self.plot_cost_history(ax,cost_history,start=0)\n\n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n\n            # pluck out current weights \n            w_best = weight_history[current_ind]\n            f = self.feature_transforms(x,w_best[0])\n\n            # produce static img\n            for u in range(num_layers):\n                bl = self.all_activations[u]\n                local_axs = axs[u+1]\n                for ax in local_axs:\n                    ax.cla()\n                self.single_layer_distributions(u,bl,local_axs)\n\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate,frames=num_frames,interval = 25,blit=False)\n\n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()\n        \n    def single_layer_distributions(self,u,x,axs):\n        # loop over input and plot each individual input dimension value\n        all_bins = []\n        N = x.shape[0]\n        for n in range(N):\n            hist, bins = np.histogram(x[n,:], bins=30)\n            all_bins.append(bins.ravel())\n\n        # determine range for all subplots\n        maxview = np.max(all_bins)\n        minview = np.min(all_bins)\n        viewrange = (maxview - minview)*0.1\n        maxview += viewrange\n        minview -= viewrange\n\n        for n in range(N):\n            ax = axs[n]\n            hist, bins = np.histogram(x[n,:], bins=30)\n            width = 0.7 * (bins[1] - bins[0])\n            center = (bins[:-1] + bins[1:]) / 2\n            ax.barh(center, hist,width)\n            ax.set_title(r'$f_' + str(n+1) + '^{(' + str(u+1) + ')}$',fontsize=14)\n            ax.set_ylim([minview,maxview])\n\n    #### compare cost function histories ####\n    def plot_cost_history(self,ax,history,start):\n        # plotting colors\n        colors = ['k']\n\n        # plot cost function history\n        ax.plot(np.arange(start,len(history),1),history[start:],linewidth = 3,color = 'k') \n\n        # clean up panel / axes labels\n        xlabel = 'step $k$'\n        ylabel = r'$g\\left(\\mathbf{w}^k\\right)$'\n        ax.set_xlabel(xlabel,fontsize = 14)\n        ax.set_ylabel(ylabel,fontsize = 14,rotation = 0,labelpad = 25)\n        title = 'cost history'\n        ax.set_title(title,fontsize = 18)\n\n        # plotting limits\n        xmin = 0; xmax = len(history); xgap = xmax*0.05; \n\n        xmin -= xgap; xmax += xgap;\n        ymin = np.min(history); ymax = np.max(history); ygap = (ymax - ymin)*0.1;\n        ymin -= ygap; ymax += ygap;\n\n        ax.set_xlim([xmin,xmax]) \n        ax.set_ylim([ymin,ymax]) """
mlrefined_libraries/multilayer_perceptron_library/plotters.py,23,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\nfrom matplotlib import gridspec\nfrom autograd.misc.flatten import flatten_func\nfrom autograd import grad as compute_grad   \n\n\n# import autograd functionality\nimport numpy as np\nimport math\nimport time\nimport copy\n\nclass Visualizer:\n    '''\n    Various plotting and visualization functions for illustrating training / fitting of nonlinear regression and classification\n    '''             \n    \n    # compare regression cost histories from multiple runs\n    def compare_regression_histories(self, histories, start,**kwargs):\n        # initialize figure\n        fig = plt.figure(figsize = (8,3))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); \n        \n        # any labels to add?        \n        labels = [' ',' ']\n        if 'labels' in kwargs:\n            labels = kwargs['labels']\n\n        # run through input histories, plotting each beginning at 'start' iteration\n        for c in range(len(histories)):\n            history = histories[c]\n            \n            label = 0\n            if c == 0:\n                label = labels[0]\n            else:\n                label = labels[1]\n                \n            # check if a label exists, if so add it to the plot\n            if np.size(label) == 0:\n                ax.plot(np.arange(start,len(history),1),history[start:],linewidth = 3*(0.8)**(c)) \n            else:               \n                ax.plot(np.arange(start,len(history),1),history[start:],linewidth = 3*(0.8)**(c),label = label) \n\n        # clean up panel\n        ax.set_xlabel('iteration',fontsize = 12)\n        ax.set_ylabel('cost function value',fontsize = 12)\n        ax.set_title('cost function value at each step of gradient descent',fontsize = 15)\n        if np.size(label) > 0:\n            plt.legend(loc='upper right')\n        ax.set_xlim([start - 1,len(history)+1])\n        plt.show()\n    \n    \n    def compare_classification_histories(self, g, x, y, **kwargs):\n        '''\n        A module for computing / plotting the cost and misclassification histories for a given run of gradient descent.\n        '''\n        \n        num_pts = len(y)\n        \n        max_its = 100\n        if 'max_its' in kwargs:\n            max_its = kwargs['max_its']\n\n        alpha = 1e-4\n        if 'alpha' in kwargs:\n            alpha = kwargs['alpha']     \n            \n        batch_size = 50\n        if 'batch_size' in kwargs:\n            batch_size = kwargs['batch_size']  \n            \n        version = 'unnormalized'\n        if 'version' in kwargs:\n            version = kwargs['version']       \n        \n        multiclass = False\n        # initialize w based on number of classes\n        if len(np.unique(y))>2:\n            # multiclass\n            multiclass = True \n            w = 0.1*np.random.randn(np.shape(x)[0]+1,len(np.unique(y)))\n        else:\n            # binary\n            w = .1*np.random.randn(np.shape(x)[0]+1,1)\n            \n        \n        # compute linear combination of input point\n        def model(x,w):\n            # tack a 1 onto the top of each input point all at once\n            o = np.ones((1,np.shape(x)[1]))\n            x = np.vstack((o,x))\n\n            # compute linear combination and return\n            a = np.dot(x.T,w)\n            return a    \n       \n        \n        def minibatch_gradient_descent(g, w, alpha, num_pts, batch_size, max_its, version):    \n            # flatten the input function, create gradient based on flat function\n            g_flat, unflatten, w = flatten_func(g, w)\n            grad = compute_grad(g_flat)\n\n            # record history\n            w_hist = []\n            w_hist.append(unflatten(w))\n\n            # how many mini-batches equal the entire dataset?\n            num_batches = int(np.ceil(np.divide(num_pts, batch_size)))\n            # over the line\n            for k in range(max_its):   \n                # loop over each minibatch\n                for b in range(num_batches):\n                    # collect indices of current mini-batch\n                    batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_pts))\n\n                    # plug in value into func and derivative\n                    grad_eval = grad(w, batch_inds)\n                    grad_eval.shape = np.shape(w)\n\n                    ### normalized or unnormalized descent step? ###\n                    if version == 'normalized':\n                        grad_norm = np.linalg.norm(grad_eval)\n                        if grad_norm == 0:\n                            grad_norm += 10**-6*np.sign(2*np.random.rand(1) - 1)\n                        grad_eval /= grad_norm\n\n                    # take descent step with momentum\n                    w = w - alpha*grad_eval\n\n                # record weight update\n                w_hist.append(unflatten(w))\n\n            return w_hist\n        \n        \n        \n        # local copies of the softmax cost function written more compactly, for scoping issues\n        softmax = lambda w: np.sum(np.log(1 + np.exp((-y)*(w[0] + np.dot(x.T,w[1:])))))\n        binary_count = lambda w: 0.25*np.sum((np.sign(w[0] + np.dot(x.T,w[1:])) - y)**2)\n        \n        \n        # multiclass counting cost\n        def multiclass_count(w):                \n            # pre-compute predictions on all points\n            all_evals = model(x,w)\n\n            # compute predictions of each input point\n            y_predict = (np.argmax(all_evals,axis = 1))[:,np.newaxis]\n\n            # compare predicted label to actual label\n            count = np.sum(np.abs(np.sign(y - y_predict)))\n\n            # return number of misclassifications\n            return count\n\n        def multiclass_perceptron(w):\n\n            lam = 10**-3\n            # pre-compute predictions on all points\n            all_evals = model(x,w)\n\n            # compute maximum across data points\n            a =  np.max(all_evals, axis = 1)        \n\n            # compute cost in compact form using numpy broadcasting\n            b = all_evals[np.arange(len(y)), y.astype(int).flatten()]\n            cost = np.sum(a - b)\n\n            # add regularizer\n            cost = cost + lam*np.linalg.norm(w[1:,:],'fro')**2\n\n            # return average\n            return cost/float(len(y))\n        \n        \n        # specify cost and count functions\n        cost_function = softmax\n        count_function = binary_count\n        if multiclass == True:\n            cost_function = multiclass_perceptron\n            count_function = multiclass_count\n\n        \n        # run batch gradient descent\n        batch = num_pts\n        batch_weights = minibatch_gradient_descent(g, w, alpha, num_pts, batch, max_its, version)\n\n        # run mini-batch gradient descent\n        batch = batch_size\n        minibatch_weights = minibatch_gradient_descent(g, w, alpha, num_pts, batch, max_its, version)\n\n        # collect the weights \n        weight_histories = [batch_weights, minibatch_weights]\n\n\n        # initialize figure\n        fig = plt.figure(figsize = (9,3))\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]);\n\n        # loop over histories and plot all\n        c = 1\n        for weight_history in weight_histories:\n            # loop over input weight history and create associated cost and misclassification histories\n            cost_history = []\n            count_history = []\n            for weight in weight_history:\n                cost_val = cost_function(weight)\n                cost_history.append(cost_val)\n\n                count_val = count_function(weight)\n                count_history.append(count_val)\n\n            # now plot each, one per panel\n            ax1.plot(cost_history)  \n            label = 'batch'\n            if c == 2:\n                label = 'mini-batch'\n            if c == 3:\n                label = 'stochastic'\n            ax2.plot(count_history,label = label)\n            c+=1\n\n        # label each panel\n        ax1.set_xlabel('iteration')\n        ax1.set_ylabel('cost function value')\n        ax1.set_title('cost function history')\n\n        ax2.set_xlabel('iteration')\n        ax2.set_ylabel('misclassifications')\n        ax2.set_title('number of misclassificaions')\n\n        ax2.legend()\n\n        plt.show()\n        \n       \n    \n    \n    \n  """
mlrefined_libraries/multilayer_perceptron_library/squiggle_generator.py,19,"b""import autograd.numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nclass Setup:\n    def __init__(self,**kwargs):\n        ### basic parameters of squiggle ###\n        # An example 4 hidden layer network, with 10 units in each layer\n        U_1 = 10\n        U_2 = 10\n        U_3 = 10\n        U_4 = 10\n        U_5 = 10\n\n        # the list defines our network architecture\n        self.encoder_layer_sizes = [2,U_1,1]\n        self.decoder_layer_sizes = [1, U_1,U_2,U_3,U_4,U_5,2]\n        self.activation = np.sinc\n        self.scale = 0.5\n        \n        if 'activation' in kwargs:\n            self.activation = kwargs['activation']\n        if 'encoder_layer_sizes' in kwargs:\n            self.encoder_layer_sizes = kwargs['encoder_layer_sizes']\n        if 'decoder_layer_sizes' in kwargs:\n            self.decoder_layer_sizes = kwargs['decoder_layer_sizes']\n        if 'scale' in kwargs:\n            self.scale = kwargs['scale']\n\n    ####### squiggle maker functions ######\n    def make_squiggle(self):\n        # generate weights\n        w1 = self.initialize_network_weights(self.encoder_layer_sizes, self.scale)\n        w2 = self.initialize_network_weights(self.decoder_layer_sizes, self.scale)\n        self.w = [w1,w2]\n        \n        # evaluate autoencoder over fine range of input\n        a = np.linspace(-1,1,200)\n        b = np.linspace(-1,1,200)\n        s,t = np.meshgrid(a,b)\n        s.shape = (1,len(a)**2)\n        t.shape = (1,len(b)**2)\n        z = np.vstack((s,t))\n\n        # create encoded vectors\n        v = self.encoder(z,self.w[0])\n\n        # decode onto basis\n        self.squiggle = self.decoder(v,self.w[1])\n        \n        # plot squiggle\n        fig = plt.figure(figsize = (9,4))\n        gs = gridspec.GridSpec(1,1) \n        ax = plt.subplot(gs[0]);ax.axis('off'); \n        #ax.set_xlabel(r'$x_1$',fontsize = 15);ax.set_ylabel(r'$x_2$',fontsize = 15,rotation = 0);\n        ax.scatter(self.squiggle[0,:],self.squiggle[1,:],c = 'k',s = 5.5,edgecolor = 'k',linewidth = 0.5,zorder = 0)\n        ax.scatter(self.squiggle[0,:],self.squiggle[1,:],c = 'r',s = 1.5,edgecolor = 'r',linewidth = 0.5,zorder = 0)\n        plt.show()\n        \n    def make_so_many_squiggles(self):\n        # evaluate autoencoder over fine range of input\n        a = np.linspace(-1,1,200)\n        b = np.linspace(-1,1,200)\n        s,t = np.meshgrid(a,b)\n        s.shape = (1,len(a)**2)\n        t.shape = (1,len(b)**2)\n        z = np.vstack((s,t))\n        fig = plt.figure(figsize = (9,6))\n        gs = gridspec.GridSpec(3,3) \n        for i in range(9):\n            # generate weights\n            w1 = self.initialize_network_weights(self.encoder_layer_sizes, self.scale)\n            w2 = self.initialize_network_weights(self.decoder_layer_sizes, self.scale)\n            self.w = [w1,w2]\n\n            # create encoded vectors\n            v = self.encoder(z,self.w[0])\n\n            # decode onto basis\n            self.squiggle = self.decoder(v,self.w[1])\n\n            # plot squiggle\n            ax = plt.subplot(gs[i]);ax.axis('off'); \n            #ax.set_xlabel(r'$x_1$',fontsize = 15);ax.set_ylabel(r'$x_2$',fontsize = 15,rotation = 0);\n            ax.scatter(self.squiggle[0,:],self.squiggle[1,:],c = 'k',s = 5.5,edgecolor = 'k',linewidth = 0.5,zorder = 0)\n            ax.scatter(self.squiggle[0,:],self.squiggle[1,:],c = 'r',s = 1.5,edgecolor = 'r',linewidth = 0.5,zorder = 0)\n        plt.show()\n        \n    ####### network functions ######\n    # create initial weights for arbitrary feedforward network\n    def initialize_network_weights(self,layer_sizes, scale):\n        # container for entire weight tensor\n        weights = []\n\n        # loop over desired layer sizes and create appropriately sized initial \n        # weight matrix for each layer\n        for k in range(len(layer_sizes)-1):\n            # get layer sizes for current weight matrix\n            U_k = layer_sizes[k]\n            U_k_plus_1 = layer_sizes[k+1]\n\n            # make weight matrix\n            weight = scale*np.random.randn(U_k+1,U_k_plus_1)\n            weights.append(weight)\n\n        # re-express weights so that w_init[0] = omega_inner contains all \n        # internal weight matrices, and w_init = w contains weights of \n        # final linear combination in predict function\n        w_init = [weights[:-1],weights[-1]]\n\n        return w_init\n\n    # fully evaluate our network features using the tensor of weights in w\n    def feature_transforms(self,a, w):    \n        # loop through each layer matrix\n        for W in w:\n            #  pad with ones (to compactly take care of bias) for next layer computation        \n            o = np.ones((1,np.shape(a)[1]))\n            a = np.vstack((o,a))\n\n            # compute inner product with current layer weights\n            a = np.dot(a.T, W).T\n\n            # output of layer activation\n            a = self.activation(a)\n        return a\n    \n    def encoder(self,x,w):    \n        # feature transformation \n        f = self.feature_transforms(x,w[0])\n\n        # tack a 1 onto the top of each input point all at once\n        o = np.ones((1,np.shape(f)[1]))\n        f = np.vstack((o,f))\n\n        # compute linear combination and return\n        a = np.dot(f.T,w[1])\n        return a.T\n    \n    def decoder(self,v,w):\n        # feature transformation \n        f = self.feature_transforms(v,w[0])\n\n        # tack a 1 onto the top of each input point all at once\n        o = np.ones((1,np.shape(f)[1]))\n        f = np.vstack((o,f))\n\n        # compute linear combination and return\n        a = np.dot(f.T,w[1])\n        return a.T"""
mlrefined_libraries/nonlinear_superlearn_library/__init__.py,0,b''
mlrefined_libraries/nonlinear_superlearn_library/basic_runner.py,39,"b""import autograd.numpy as np\nfrom autograd import value_and_grad \nfrom autograd import hessian\nfrom autograd.misc.flatten import flatten_func\nimport copy\nfrom inspect import signature\n\n'''\nA list of cost functions for supervised learning.  Use the choose_cost function\nto choose the desired cost with input data  \n'''\nclass Setup:\n    def __init__(self,x,y,feature_transforms,cost,**kwargs):\n        normalize = 'standard'\n        if 'normalize' in kwargs:\n            normalize = kwargs['normalize']\n        if normalize == 'standard':\n            # create normalizer\n            self.normalizer,self.inverse_normalizer = self.standard_normalizer(x)\n\n            # normalize input \n            self.x = self.normalizer(x)\n        elif normalize == 'sphere':\n            # create normalizer\n            self.normalizer,self.inverse_normalizer = self.PCA_sphereing(x)\n\n            # normalize input \n            self.x = self.normalizer(x)\n        else:\n            self.x = x\n            self.normalizer = lambda data: data\n            self.inverse_normalizer = lambda data: data\n            \n        # make any other variables not explicitly input into cost functions globally known\n        self.y = y\n        self.feature_transforms = feature_transforms\n        \n        # count parameter layers of input to feature transform\n        self.sig = signature(self.feature_transforms)\n\n        self.lam = 0\n        if 'lam' in kwargs:\n            self.lam = kwargs['lam']\n\n        # make cost function choice\n        cost_func = 0\n        if cost == 'least_squares':\n            self.cost_func = self.least_squares\n        if cost == 'least_absolute_deviations':\n            self.cost_func = self.least_absolute_deviations\n        if cost == 'softmax':\n            self.cost_func = self.softmax\n        if cost == 'relu':\n            self.cost_func = self.relu\n        if cost == 'counter':\n            self.cost_func = self.counting_cost\n        if cost == 'multiclass_perceptron':\n            self.cost_func = self.multiclass_perceptron\n        if cost == 'multiclass_softmax':\n            self.cost_func = self.multiclass_softmax\n        if cost == 'multiclass_counter':\n            self.cost_func = self.multiclass_counting_cost\n            \n        # for autoencoder\n        if cost == 'autoencoder':\n            self.feature_transforms_2 = kwargs['feature_transforms_2']\n            self.cost_func = self.autoencoder\n\n    # run optimization\n    def fit(self,**kwargs):\n        # basic parameters for gradient descent run\n        max_its = 500; alpha_choice = 10**(-1);\n        w = 0.1*np.random.randn(np.shape(self.x)[0] + 1,1)\n        algo = 'gradient_descent'\n\n        # set parameters by hand\n        if 'algo' in kwargs:\n            algo = kwargs['algo']\n        if 'max_its' in kwargs:\n            max_its = kwargs['max_its']\n        if 'alpha_choice' in kwargs:\n            alpha_choice = kwargs['alpha_choice']\n        if 'w' in kwargs:\n            w = kwargs['w']\n\n        # run gradient descent\n        if algo == 'gradient_descent':\n            self.weight_history, self.cost_history = self.gradient_descent(self.cost_func,alpha_choice,max_its,w)\n        if algo == 'newtons_method':  \n            self.weight_history, self.cost_history = self.newtons_method(self.cost_func,max_its,w)\n\n    ###### cost functions #####\n    # compute linear combination of input point\n    def model(self,x,w):   \n        # feature transformation - switch for dealing\n        # with feature transforms that either do or do\n        # not have internal parameters\n        f = 0\n        if len(self.sig.parameters) == 2:\n            f = self.feature_transforms(x,w[0])\n        else: \n            f = self.feature_transforms(x)    \n\n        # compute linear combination and return\n        # switch for dealing with feature transforms that either \n        # do or do not have internal parameters\n        a = 0\n        if len(self.sig.parameters) == 2:\n            a = w[1][0] + np.dot(f.T,w[1][1:])\n        else:\n            a = w[0] + np.dot(f.T,w[1:])\n        return a.T\n    \n    ###### regression costs #######\n    # an implementation of the least squares cost function for linear regression\n    def least_squares(self,w):\n        cost = np.sum((self.model(self.x,w) - self.y)**2)\n        return cost/float(np.size(self.y))\n\n    # a compact least absolute deviations cost function\n    def least_absolute_deviations(self,w):\n        cost = np.sum(np.abs(self.model(self.x,w) - self.y))\n        return cost/float(np.size(self.y))\n\n    ###### two-class classification costs #######\n    # the convex softmax cost function\n    def softmax(self,w):\n        cost = np.sum(np.log(1 + np.exp(-self.y*self.model(self.x,w))))\n        return cost/float(np.size(self.y))\n\n    # the convex relu cost function\n    def relu(self,w):\n        cost = np.sum(np.maximum(0,-self.y*self.model(self.x,w)))\n        return cost/float(np.size(self.y))\n\n    # the counting cost function\n    def counting_cost(self,w):\n        cost = np.sum((np.sign(self.model(self.x,w)) - self.y)**2)\n        return 0.25*cost \n\n    ###### multiclass classification costs #######\n    # multiclass perceptron\n    def multiclass_perceptron(self,w):        \n        # pre-compute predictions on all points\n        all_evals = self.model(self.x,w)\n\n        # compute maximum across data points\n        a = np.max(all_evals,axis = 0)    \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[self.y.astype(int).flatten(),np.arange(np.size(self.y))]\n        cost = np.sum(a - b)\n\n        # return average\n        return cost/float(np.size(self.y))\n\n    # multiclass softmax\n    def multiclass_softmax(self,w):        \n        # pre-compute predictions on all points\n        all_evals = self.model(self.x,w)\n\n        # compute softmax across data points\n        a = np.log(np.sum(np.exp(all_evals),axis = 0)) \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[self.y.astype(int).flatten(),np.arange(np.size(self.y))]\n        cost = np.sum(a - b)\n\n        # return average\n        return cost/float(np.size(self.y))\n    \n    # multiclass misclassification cost function - aka the fusion rule\n    def multiclass_counting_cost(self,w):                \n        # pre-compute predictions on all points\n        all_evals = self.model(self.x,w)\n\n        # compute predictions of each input point\n        y_predict = (np.argmax(all_evals,axis = 0))[np.newaxis,:]\n\n        # compare predicted label to actual label\n        count = np.sum(np.abs(np.sign(self.y - y_predict)))\n\n        # return number of misclassifications\n        return count\n    \n    ### for autoencoder ###\n    def encoder(self,x,w):    \n        # feature transformation \n        f = self.feature_transforms(x,w[0])\n\n        # tack a 1 onto the top of each input point all at once\n        o = np.ones((1,np.shape(f)[1]))\n        f = np.vstack((o,f))\n\n        # compute linear combination and return\n        a = np.dot(f.T,w[1])\n        return a.T\n\n    def decoder(self,v,w):\n        # feature transformation \n        f = self.feature_transforms_2(v,w[0])\n\n        # tack a 1 onto the top of each input point all at once\n        o = np.ones((1,np.shape(f)[1]))\n        f = np.vstack((o,f))\n\n        # compute linear combination and return\n        a = np.dot(f.T,w[1])\n        return a.T\n    \n    def autoencoder(self,w):\n        # encode input\n        a = self.encoder(self.x,w[0])\n        \n        # decode result\n        b = self.decoder(a,w[1])\n        \n        # compute Least Squares error\n        cost = np.sum((b - self.x)**2)\n        return cost/float(self.x.shape[1])\n    \n    ##### optimizer ####\n    # gradient descent function - inputs: g (input function), alpha (steplength parameter), max_its (maximum number of iterations), w (initialization)\n    def gradient_descent(self,g,alpha_choice,max_its,w):\n        # flatten the input function to more easily deal with costs that have layers of parameters\n        g_flat, unflatten, w = flatten_func(g, w) # note here the output 'w' is also flattened\n\n        # compute the gradient function of our input function - note this is a function too\n        # that - when evaluated - returns both the gradient and function evaluations (remember\n        # as discussed in Chapter 3 we always ge the function evaluation 'for free' when we use\n        # an Automatic Differntiator to evaluate the gradient)\n        gradient = value_and_grad(g_flat)\n\n        # run the gradient descent loop\n        weight_history = []      # container for weight history\n        cost_history = []        # container for corresponding cost function history\n        alpha = 0\n        for k in range(1,max_its+1):\n            # check if diminishing steplength rule used\n            if alpha_choice == 'diminishing':\n                alpha = 1/float(k)\n            else:\n                alpha = alpha_choice\n\n            # evaluate the gradient, store current (unflattened) weights and cost function value\n            cost_eval,grad_eval = gradient(w)\n            weight_history.append(unflatten(w))\n            cost_history.append(cost_eval)\n\n            # take gradient descent step\n            w = w - alpha*grad_eval\n\n        # collect final weights\n        weight_history.append(unflatten(w))\n        # compute final cost function value via g itself (since we aren't computing \n        # the gradient at the final step we don't get the final cost function value \n        # via the Automatic Differentiatoor) \n        cost_history.append(g_flat(w))  \n        return weight_history,cost_history\n \n    # newtons method function - inputs: g (input function), max_its (maximum number of iterations), w (initialization)\n    def newtons_method(self,g,max_its,w,**kwargs):\n        # flatten input funciton, in case it takes in matrices of weights\n        flat_g, unflatten, w = flatten_func(g, w)\n\n        # compute the gradient / hessian functions of our input function -\n        # note these are themselves functions.  In particular the gradient - \n        # - when evaluated - returns both the gradient and function evaluations (remember\n        # as discussed in Chapter 3 we always ge the function evaluation 'for free' when we use\n        # an Automatic Differntiator to evaluate the gradient)\n        gradient = value_and_grad(flat_g)\n        hess = hessian(flat_g)\n\n        # set numericxal stability parameter / regularization parameter\n        epsilon = 10**(-7)\n        if 'epsilon' in kwargs:\n            epsilon = kwargs['epsilon']\n\n        # run the newtons method loop\n        weight_history = []      # container for weight history\n        cost_history = []        # container for corresponding cost function history\n        for k in range(max_its):\n            # evaluate the gradient, store current weights and cost function value\n            cost_eval,grad_eval = gradient(w)\n            weight_history.append(unflatten(w))\n            cost_history.append(cost_eval)\n\n            # evaluate the hessian\n            hess_eval = hess(w)\n\n            # reshape for numpy linalg functionality\n            hess_eval.shape = (int((np.size(hess_eval))**(0.5)),int((np.size(hess_eval))**(0.5)))\n\n            # solve second order system system for weight update\n            w = w - np.dot(np.linalg.pinv(hess_eval + epsilon*np.eye(np.size(w))),grad_eval)\n\n        # collect final weights\n        weight_history.append(unflatten(w))\n        # compute final cost function value via g itself (since we aren't computing \n        # the gradient at the final step we don't get the final cost function value \n        # via the Automatic Differentiatoor) \n        cost_history.append(flat_g(w))  \n\n        return weight_history,cost_history\n\n    ###### normalizers #####\n    # standard normalization function \n    def standard_normalizer(self,x):\n        # compute the mean and standard deviation of the input\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n\n        # create standard normalizer function\n        normalizer = lambda data: (data - x_means)/x_stds\n\n        # create inverse standard normalizer\n        inverse_normalizer = lambda data: data*x_stds + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer\n\n    # compute eigendecomposition of data covariance matrix\n    def PCA(self,x,**kwargs):\n        # regularization parameter for numerical stability\n        lam = 10**(-7)\n        if 'lam' in kwargs:\n            lam = kwargs['lam']\n\n        # create the correlation matrix\n        P = float(x.shape[1])\n        Cov = 1/P*np.dot(x,x.T) + lam*np.eye(x.shape[0])\n\n        # use numpy function to compute eigenvalues / vectors of correlation matrix\n        D,V = np.linalg.eigh(Cov)\n        return D,V\n\n    # PCA-sphereing - use PCA to normalize input features\n    def PCA_sphereing(self,x,**kwargs):\n        # Step 1: mean-center the data\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_centered = x - x_means\n\n        # Step 2: compute pca transform on mean-centered data\n        d,V = self.PCA(x_centered,**kwargs)\n\n        # Step 3: divide off standard deviation of each (transformed) input, \n        # which are equal to the returned eigenvalues in 'd'.  \n        stds = (d[:,np.newaxis])**(0.5)\n        normalizer = lambda data: np.dot(V.T,data - x_means)/stds\n\n        # create inverse normalizer\n        inverse_normalizer = lambda data: np.dot(V,data*stds) + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer"""
mlrefined_libraries/nonlinear_superlearn_library/boosting_classification_animator.py,31,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom matplotlib.ticker import FormatStrFormatter\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\nfrom matplotlib.ticker import MaxNLocator, FuncFormatter\n\n# import autograd functionality\nimport autograd.numpy as np\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\nfrom matplotlib.ticker import FormatStrFormatter\nfrom inspect import signature\n\nclass Visualizer:\n    '''\n    Visualize cross validation performed on N = 2 dimensional input classification datasets\n    '''\n    #### initialize ####\n    def __init__(self,csvname):\n        # grab input\n        data = np.loadtxt(csvname,delimiter = ',')\n        self.x = data[:-1,:]\n        self.y = data[-1:,:] \n\n        self.colors = ['salmon','cornflowerblue','lime','bisque','mediumaquamarine','b','m','g']\n    \n    #### animate multiple runs on single regression ####\n    def animate_trainval_boosting(self,savepath,runner,num_frames,**kwargs):        \n        # select subset of runs\n        inds = np.arange(0,len(runner.models),int(len(runner.models)/float(num_frames)))\n        labels = np.arange(0,len(runner.models),int(len(runner.models)/float(5)))\n\n        # global names for train / valid sets\n        train_inds = runner.train_inds\n        valid_inds = runner.valid_inds\n        \n        self.x_train = self.x[:,train_inds]\n        self.y_train = self.y[:,train_inds]\n        \n        self.x_valid = self.x[:,valid_inds]\n        self.y_valid = self.y[:,valid_inds]\n        \n        self.normalizer = runner.normalizer\n        train_errors = runner.train_count_vals\n        valid_errors = runner.valid_count_vals\n        num_units = len(runner.models)\n       \n        # construct figure\n        fig = plt.figure(figsize = (6,6))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(2, 2)\n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]); \n        ax3 = plt.subplot(gs[2]); \n        ax4 = plt.subplot(gs[3]);\n        \n        # start animation\n        num_frames = len(inds) -2       \n        print ('starting animation rendering...')\n        def animate(k):\n            # clear panels\n            ax1.cla()\n            ax2.cla()\n            ax3.cla()\n            ax4.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            #### plot training, testing, and full data ####            \n            # pluck out current weights \n            a = inds[k]\n            steps = runner.best_steps[:a+1]\n            \n            # produce static img\n            self.draw_boundary(ax1,steps,a)\n            self.static_N2_simple(ax1,train_valid = 'original')\n            self.draw_boundary(ax2,steps,a)\n\n            self.static_N2_simple(ax2,train_valid = 'train')\n            self.draw_boundary(ax3,steps,a)\n            self.static_N2_simple(ax3,train_valid = 'validate')\n\n            #### plot training / validation errors ####\n            self.plot_train_valid_errors(ax4,a,train_errors,valid_errors,inds)\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()   \n        \n    ##### draw boundary #####\n    def draw_boundary(self,ax,steps,ind):\n        ### create boundary data ###\n        # get visual boundary\n        xmin1 = np.min(self.x[0,:])\n        xmax1 = np.max(self.x[0,:])\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        xmin2 = np.min(self.x[1,:])\n        xmax2 = np.max(self.x[1,:])\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2    \n        \n        # plot boundary for 2d plot\n        r1 = np.linspace(xmin1,xmax1,300)\n        r2 = np.linspace(xmin2,xmax2,300)\n        s,t = np.meshgrid(r1,r2)\n        s = np.reshape(s,(np.size(s),1))\n        t = np.reshape(t,(np.size(t),1))\n        h = np.concatenate((s,t),axis = 1).T\n        \n        # plot total fit\n        model = lambda x: np.sum([v(x) for v in steps],axis=0)\n        z = model(self.normalizer(h))\n        z1 = np.sign(z)        \n\n        # reshape it\n        s.shape = (np.size(r1),np.size(r2))\n        t.shape = (np.size(r1),np.size(r2))     \n        z1.shape = (np.size(r1),np.size(r2))\n        \n        #### plot contour, color regions ####\n        ax.contour(s,t,z1,colors='k', linewidths=2.5,levels = [0],zorder = 2)\n        ax.contourf(s,t,z1,colors = [self.colors[1],self.colors[0]],alpha = 0.15,levels = range(-1,2))\n        \n    ######## show N = 2 static image ########\n    # show coloring of entire space\n    def static_N2_simple(self,ax,train_valid):        \n        ### create boundary data ###\n        xmin1 = np.min(self.x[0,:])\n        xmax1 = np.max(self.x[0,:])\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        xmin2 = np.min(self.x[1,:])\n        xmax2 = np.max(self.x[1,:])\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2    \n\n        ### loop over two panels plotting each ###\n        # plot training points\n        if train_valid == 'train':\n            # reverse normalize data\n            x_train = self.x_train\n            y_train = self.y_train\n            \n            # plot data\n            ind0 = np.argwhere(y_train == +1)\n            ind0 = [v[1] for v in ind0]\n            ax.scatter(x_train[0,ind0],x_train[1,ind0],s = 45, color = self.colors[0], edgecolor = [0,0.7,1],linewidth = 1,zorder = 3)\n\n            ind1 = np.argwhere(y_train == -1)\n            ind1 = [v[1] for v in ind1]\n            ax.scatter(x_train[0,ind1],x_train[1,ind1],s = 45, color = self.colors[1], edgecolor = [0,0.7,1],linewidth = 1,zorder = 3)\n            ax.set_title('training data',fontsize = 15)\n\n        if train_valid == 'validate':\n            # reverse normalize data\n            x_valid = self.x_valid\n            y_valid = self.y_valid\n        \n            # plot testing points\n            ind0 = np.argwhere(y_valid == +1)\n            ind0 = [v[1] for v in ind0]\n            ax.scatter(x_valid[0,ind0],x_valid[1,ind0],s = 45, color = self.colors[0], edgecolor = [1,0.8,0.5],linewidth = 1,zorder = 3)\n\n            ind1 = np.argwhere(y_valid == -1)\n            ind1 = [v[1] for v in ind1]\n            ax.scatter(x_valid[0,ind1],x_valid[1,ind1],s = 45, color = self.colors[1], edgecolor = [1,0.8,0.5],linewidth = 1,zorder = 3)\n            ax.set_title('validation data',fontsize = 15)\n                \n        if train_valid == 'original':\n            # plot all points\n            ind0 = np.argwhere(self.y == +1)\n            ind0 = [v[1] for v in ind0]\n            ax.scatter(self.x[0,ind0],self.x[1,ind0],s = 55, color = self.colors[0], edgecolor = 'k',linewidth = 1,zorder = 3)\n\n            ind1 = np.argwhere(self.y == -1)\n            ind1 = [v[1] for v in ind1]\n            ax.scatter(self.x[0,ind1],self.x[1,ind1],s = 55, color = self.colors[1], edgecolor = 'k',linewidth = 1,zorder = 3)\n            ax.set_title('original data',fontsize = 15)\n\n        # cleanup panel\n        ax.set_xlabel(r'$x_1$',fontsize = 15)\n        ax.set_ylabel(r'$x_2$',fontsize = 15,rotation = 0,labelpad = 20)\n        ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n        ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n        \n    def plot_train_valid_errors(self,ax,k,train_errors,valid_errors,inds):\n        num_elements = np.arange(len(train_errors))\n\n        ax.plot([v+1 for v in inds[:k+1]] ,[train_errors[v] for v in inds[:k+1]],color = [0,0.7,1],linewidth = 2.5,zorder = 1,label = 'training')\n        # ax.scatter([v+1  for v in inds[:k+1]] ,train_errors[:k+1],color = [0,0.7,1],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n\n        ax.plot([v+1  for v in inds[:k+1]] ,[valid_errors[v] for v in inds[:k+1]],color = [1,0.8,0.5],linewidth = 2.5,zorder = 1,label = 'validation')\n        # ax.scatter([v+1  for v in inds[:k+1]] ,valid_errors[:k+1],color= [1,0.8,0.5],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n        ax.set_title('misclassifications',fontsize = 15)\n\n        # cleanup\n        ax.set_xlabel('number of units',fontsize = 12)\n        ax.set_ylabel('number of misclassifications',fontsize = 12)\n\n        # cleanp panel                \n        num_iterations = len(train_errors)\n        minxc = 0.5\n        maxxc = len(num_elements) + 0.5\n        minc = min(min(copy.deepcopy(train_errors)),min(copy.deepcopy(valid_errors)))\n        maxc = max(max(copy.deepcopy(train_errors[:10])),max(copy.deepcopy(valid_errors[:10])))\n        gapc = (maxc - minc)*0.25\n        minc -= gapc\n        maxc += gapc\n        \n        ax.set_xlim([minxc- 0.5,maxxc + 0.5])\n        ax.set_ylim([minc,maxc])\n        \n        # ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n        #labels = [str(v) for v in num_units]\n        #ax.set_xticks(np.arange(1,len(num_elements)+1))\n       # ax.set_xticklabels(num_units)\n\n\n        """
mlrefined_libraries/nonlinear_superlearn_library/boosting_regression_animators.py,15,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom matplotlib.ticker import FormatStrFormatter\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nimport autograd.numpy as np\n\n# import standard libraries\nimport math\nimport time\nimport copy\nfrom inspect import signature\n\nclass Visualizer:\n    '''\n    Visualize linear regression in 2 and 3 dimensions.  For single input cases (2 dimensions) the path of gradient descent on the cost function can be animated.\n    '''\n    #### initialize ####\n    def __init__(self,csvname):\n        # grab input\n        data = np.loadtxt(csvname,delimiter = ',')\n        self.x = data[:-1,:]\n        self.y = data[-1:,:] \n        \n        self.colors = [[1,0.8,0.5],[0,0.7,1]]\n        \n        # if 1-d regression data make sure points are sorted\n        if np.shape(self.x)[1] == 1:\n            ind = np.argsort(self.x.flatten())\n            self.x = self.x[ind,:]\n            self.y = self.y[ind,:]\n            \n    ########## show boosting crossval on 1d regression, with fit to residual ##########\n    def animate_trainval_boosting(self,savepath,runner,num_frames,**kwargs):\n        # construct figure\n        fig = plt.figure(figsize=(9,4))\n        artist = fig\n        \n        ### get inds for each run ###\n        inds = np.arange(0,len(runner.models),int(len(runner.models)/float(num_frames)))\n        \n        # create subplot with 1 active panel\n        gs = gridspec.GridSpec(1, 2) \n        ax = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]); \n        \n        # global names for train / valid sets\n        train_inds = runner.train_inds\n        valid_inds = runner.valid_inds\n        \n        self.x_train = self.x[:,train_inds]\n        self.y_train = self.y[:,train_inds]\n        \n        self.x_valid = self.x[:,valid_inds]\n        self.y_valid = self.y[:,valid_inds]\n        \n        self.normalizer = runner.normalizer\n        train_errors = runner.train_cost_vals\n        valid_errors = runner.valid_cost_vals\n        num_units = len(runner.models)\n        \n        # set plotting limits\n        xmax = np.max(copy.deepcopy(self.x))\n        xmin = np.min(copy.deepcopy(self.x))\n        xgap = (xmax - xmin)*0.1\n        xmin -= xgap\n        xmax += xgap\n\n        ymax = np.max(copy.deepcopy(self.y))\n        ymin = np.min(copy.deepcopy(self.y))\n        ygap = (ymax - ymin)*0.1\n        ymin -= ygap\n        ymax += ygap  \n        \n        # start animation\n        print ('starting animation rendering...')\n        def animate(k):\n            # clear panels\n            ax.cla()\n            ax2.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            ####### plot total model on original dataset #######      \n            ax.scatter(self.x_train,self.y_train,color = 'k',s = 60,edgecolor = self.colors[1] ,linewidth = 1.5)\n        \n            ax.scatter(self.x_valid,self.y_valid,color = 'k',s = 60,edgecolor = self.colors[0] ,linewidth = 1.5)\n                \n            # plot fit\n            if k > 0:\n                # plot current fit\n                a = inds[k-1] \n                steps = runner.best_steps[:a+1]\n                self.draw_fit(ax,steps,a)\n                    \n                # plot train / valid errors up to this point\n                self.plot_train_valid_errors(ax2,k-1,train_errors,valid_errors,inds)\n                \n            # make panel size\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames+1, interval=num_frames+1, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()   \n             \n    # 1d regression demo\n    def draw_fit(self,ax,steps,ind):\n        # set plotting limits\n        xmax = np.max(copy.deepcopy(self.x))\n        xmin = np.min(copy.deepcopy(self.x))\n        xgap = (xmax - xmin)*0.1\n        xmin -= xgap\n        xmax += xgap\n        \n        # plot fit\n        s = np.linspace(xmin,xmax,2000)[np.newaxis,:]\n        model = lambda x: np.sum([v(x) for v in steps],axis=0)\n        t = model(self.normalizer(s))\n\n        ax.plot(s.T,t.T,linewidth = 4,c = 'k')\n        ax.plot(s.T,t.T,linewidth = 2,c = 'r')\n\n    # plot training / validation errors\n    def plot_train_valid_errors(self,ax,k,train_errors,valid_errors,inds):\n        num_elements = np.arange(len(train_errors))\n\n        ax.plot([v+1 for v in inds[:k+1]] ,train_errors[:k+1],color = [0,0.7,1],linewidth = 2.5,zorder = 1,label = 'training')\n        #ax.scatter([v+1  for v in inds[:k+1]] ,train_errors[:k+1],color = [0,0.7,1],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n\n        ax.plot([v+1  for v in inds[:k+1]] ,valid_errors[:k+1],color = [1,0.8,0.5],linewidth = 2.5,zorder = 1,label = 'validation')\n        #ax.scatter([v+1  for v in inds[:k+1]] ,valid_errors[:k+1],color= [1,0.8,0.5],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n        ax.set_title('cost function history',fontsize = 15)\n\n        # cleanup\n        ax.set_xlabel('number of units',fontsize = 12)\n\n        # cleanp panel                \n        num_iterations = len(train_errors)\n        minxc = 0.5\n        maxxc = len(num_elements) + 0.5\n        minc = min(min(copy.deepcopy(train_errors)),min(copy.deepcopy(valid_errors)))\n        maxc = max(max(copy.deepcopy(train_errors[:10])),max(copy.deepcopy(valid_errors[:10])))\n        gapc = (maxc - minc)*0.25\n        minc -= gapc\n        maxc += gapc\n        \n        ax.set_xlim([minxc,maxxc])\n        ax.set_ylim([minc,maxc])\n        \n        # ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n        #labels = [str(v) for v in num_units]\n        #ax.set_xticks(np.arange(1,len(num_elements)+1))\n       # ax.set_xticklabels(num_units)"""
mlrefined_libraries/nonlinear_superlearn_library/boosting_regression_animators_v3.py,10,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom matplotlib.ticker import FormatStrFormatter\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nimport autograd.numpy as np\n\n# import standard libraries\nimport math\nimport time\nimport copy\nfrom inspect import signature\n\nclass Visualizer:\n    '''\n    Visualize linear regression in 2 and 3 dimensions.  For single input cases (2 dimensions) the path of gradient descent on the cost function can be animated.\n    '''\n    #### initialize ####\n    def __init__(self,csvname):\n        # grab input\n        data = np.loadtxt(csvname,delimiter = ',')\n        self.x = data[:-1,:]\n        self.y = data[-1:,:] \n        \n        self.colors = [[1,0.8,0.5],[0,0.7,1]]\n        \n        # if 1-d regression data make sure points are sorted\n        if np.shape(self.x)[1] == 1:\n            ind = np.argsort(self.x.flatten())\n            self.x = self.x[ind,:]\n            self.y = self.y[ind,:]\n            \n\n    ########## show boosting results on 1d regression, with fit to residual ##########\n    def animate_boosting(self,savepath,runs,frames,**kwargs):\n        # select subset of runs\n        inds = np.arange(0,len(runs),int(len(runs)/float(frames)))\n            \n        # select inds of history to plot\n        num_runs = frames\n\n        # construct figure\n        fig = plt.figure(figsize=(9,4))\n        artist = fig\n        \n        # parse any input args\n        scatter = 'none'\n        if 'scatter' in kwargs:\n            scatter = kwargs['scatter']\n\n        # create subplot with 1 active panel\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,0.01,1]) \n        ax = plt.subplot(gs[0]); \n        ax1 = plt.subplot(gs[2]);\n        ax3 = plt.subplot(gs[1]); ax3.axis('off')\n        \n        # start animation\n        num_frames = num_runs\n        print ('starting animation rendering...')\n        def animate(k):\n            # clear panels\n            ax.cla()\n            ax1.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            # get current run for cost function history plot\n            a = inds[k]\n            run = runs[a]\n            \n            # pluck out current weights \n            self.draw_fit(ax,ax1,runs,a)\n                \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()   \n        \n    # 1d regression demo\n    def draw_fit(self,ax,ax1,runs,ind):\n        # set plotting limits\n        xmax = np.max(copy.deepcopy(self.x))\n        xmin = np.min(copy.deepcopy(self.x))\n        xgap = (xmax - xmin)*0.1\n        xmin -= xgap\n        xmax += xgap\n\n        ymax = np.max(copy.deepcopy(self.y))\n        ymin = np.min(copy.deepcopy(self.y))\n        ygap = (ymax - ymin)*0.1\n        ymin -= ygap\n        ymax += ygap    \n\n        ####### plot total model on original dataset #######\n        # scatter original data\n        ax.scatter(self.x.flatten(),self.y.flatten(),color = 'k',s = 60,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n        \n        # plot fit on residual\n        s = np.linspace(xmin,xmax,2000)[np.newaxis,:]\n           \n        # plot total fit\n        t = 0\n        for i in range(ind+1):\n            # get current run\n            run = runs[i]\n            model = run.model\n            normalizer = run.normalizer\n\n            # get best weights                     \n            t += model(normalizer(s))\n\n        ax.plot(s.T,t.T,linewidth = 4,c = 'k',zorder = 1)\n        ax.plot(s.T,t.T,linewidth = 2,c = 'r',zorder = 1)\n        \n        ##### plot residual info #####\n        # get all functions from final step\n        run = runs[ind]\n        model = run.model\n        inverse_normalizer = run.inverse_normalizer\n        normalizer = run.normalizer\n        x_temp = inverse_normalizer(runs[ind].x)\n        y_temp = runs[ind].y\n        \n        # scatter residual data\n        ax1.scatter(x_temp,y_temp,color = 'k',s = 60,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n\n        # make prediction\n        t = model(normalizer(s))\n        \n        # plot fit to residual\n        ax1.plot(s.T,t.T,linewidth = 4,c = 'k',zorder = 1)\n        ax1.plot(s.T,t.T,linewidth = 2,c = 'r',zorder = 1)\n        \n        ### clean up panels ###             \n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n    \n        ax1.set_xlim([xmin,xmax])\n        ax1.set_ylim([ymin,ymax])\n        \n        # label axes\n        ax.set_xlabel(r'$x$', fontsize = 14)\n        ax.set_ylabel(r'$y$', rotation = 0,fontsize = 14,labelpad = 15)\n        ax.set_title('model ' + str(ind+1) + ' fit to original',fontsize = 14)\n        \n        ax1.set_xlabel(r'$x$', fontsize = 16)\n        ax1.set_ylabel(r'$y$', rotation = 0,fontsize = 16,labelpad = 15)\n        ax1.set_title('unit ' + str(ind+1) + ' fit to residual',fontsize = 14)\n\n     """
mlrefined_libraries/nonlinear_superlearn_library/crossval_classification_animator.py,56,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom matplotlib.ticker import FormatStrFormatter\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\nfrom matplotlib.ticker import MaxNLocator, FuncFormatter\n\n# import autograd functionality\nimport autograd.numpy as np\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\nfrom matplotlib.ticker import FormatStrFormatter\nfrom inspect import signature\n\nclass Visualizer:\n    '''\n    Visualize cross validation performed on N = 2 dimensional input classification datasets\n    '''\n    #### initialize ####\n    def __init__(self,csvname):\n        # grab input\n        data = np.loadtxt(csvname,delimiter = ',')\n        data = data.T\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n\n        self.colors = ['salmon','cornflowerblue','lime','bisque','mediumaquamarine','b','m','g']\n        \n    # the counting cost function\n    def counting_cost(self,run,x,y,w):\n        cost = np.sum((np.sign(model(x,w)) -y)**2)\n        return 0.25*cost \n    \n    #### animate multiple runs on single regression ####\n    def animate_crossval_classifications(self,savepath,runs,**kwargs):\n        weight_history = []\n        train_errors = []\n        valid_errors = []\n        for run in runs:\n            # get histories\n            train_counts = run.train_count_histories[0]\n            valid_counts = run.valid_count_histories[0]\n            weights = run.weight_histories[0]\n            \n            # select based on minimum training\n            ind = np.argmin(train_counts)\n            train_count = train_counts[ind]\n            valid_count = valid_counts[ind]\n            weight = weights[ind]\n            \n            # store\n            train_errors.append(train_count)\n            valid_errors.append(valid_count)\n            weight_history.append(weight)\n       \n        # construct figure\n        fig = plt.figure(figsize = (6,6))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(2, 2)\n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]); \n        ax3 = plt.subplot(gs[2]); \n        ax4 = plt.subplot(gs[3]);\n        \n        # start animation\n        num_frames = len(runs)        \n        print ('starting animation rendering...')\n        def animate(k):\n            print (k)\n            # clear panels\n            ax1.cla()\n            ax2.cla()\n            ax3.cla()\n            ax4.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            #### plot training, testing, and full data ####            \n            # pluck out current weights \n            w_best = weight_history[k]\n            run = runs[k]\n            \n            # produce static img\n            self.static_N2_simple(ax1,w_best,run,train_valid = 'original')\n            self.static_N2_simple(ax2,w_best,run,train_valid = 'train')\n            self.static_N2_simple(ax3,w_best,run,train_valid = 'validate')\n\n            #### plot training / validation errors ####\n            self.plot_train_valid_errors(ax4,k,train_errors,valid_errors)\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()        \n    \n\n    def plot_train_valid_errors(self,ax,k,train_errors,valid_errors):\n        num_elements = np.arange(len(train_errors))\n\n        ax.plot([v+1 for v in num_elements[:k+1]] ,train_errors[:k+1],color = [0,0.7,1],linewidth = 1.5,zorder = 1,label = 'training')\n        ax.scatter([v+1  for v in num_elements[:k+1]] ,train_errors[:k+1],color = [0,0.7,1],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n\n        ax.plot([v+1  for v in num_elements[:k+1]] ,valid_errors[:k+1],color = [1,0.8,0.5],linewidth = 1.5,zorder = 1,label = 'validation')\n        ax.scatter([v+1  for v in num_elements[:k+1]] ,valid_errors[:k+1],color= [1,0.8,0.5],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n        ax.set_title('misclassifications',fontsize = 15)\n\n        # cleanup\n        ax.set_xlabel('model',fontsize = 12)\n\n        # cleanp panel                \n        num_iterations = len(train_errors)\n        minxc = 0.5\n        maxxc = len(num_elements) + 0.5\n        minc = min(min(copy.deepcopy(train_errors)),min(copy.deepcopy(valid_errors)))\n        maxc = max(max(copy.deepcopy(train_errors[:])),max(copy.deepcopy(valid_errors[:])))\n        gapc = (maxc - minc)*0.1\n        minc -= gapc\n        maxc += gapc\n        \n        ax.set_xlim([minxc,maxxc])\n        ax.set_ylim([minc,maxc])\n        \n        #ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n        #labels = [str(v) for v in num_elements]\n        ax.set_xticks(np.arange(1,len(num_elements)+1))\n        #ax.set_xticklabels(num_units)\n\n            \n    ######## show N = 2 static image ########\n    # show coloring of entire space\n    def static_N2_simple(self,ax,w_best,runner,train_valid):\n        cost = runner.cost\n        predict = runner.model\n        feat = runner.feature_transforms\n        normalizer = runner.normalizer\n        inverse_nornalizer = runner.inverse_normalizer\n      \n        # or just take last weights\n        self.w = w_best\n        \n        ### create boundary data ###\n        xmin1 = np.min(self.x[:,0])\n        xmax1 = np.max(self.x[:,0])\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        xmin2 = np.min(self.x[:,1])\n        xmax2 = np.max(self.x[:,1])\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2    \n\n        # plot boundary for 2d plot\n        r1 = np.linspace(xmin1,xmax1,300)\n        r2 = np.linspace(xmin2,xmax2,300)\n        s,t = np.meshgrid(r1,r2)\n        s = np.reshape(s,(np.size(s),1))\n        t = np.reshape(t,(np.size(t),1))\n        h = np.concatenate((s,t),axis = 1)\n        \n        # compute model on train data\n        z1 = predict(normalizer(h.T),self.w)\n        z1 = np.sign(z1)\n\n        # reshape it\n        s.shape = (np.size(r1),np.size(r2))\n        t.shape = (np.size(r1),np.size(r2))     \n        z1.shape = (np.size(r1),np.size(r2))\n\n        ### loop over two panels plotting each ###\n        # plot training points\n        if train_valid == 'train':\n            # reverse normalize data\n            x_train = inverse_nornalizer(runner.x_train).T\n            y_train = runner.y_train\n            \n            # plot data\n            ind0 = np.argwhere(y_train == +1)\n            ind0 = [v[1] for v in ind0]\n            ax.scatter(x_train[ind0,0],x_train[ind0,1],s = 45, color = self.colors[0], edgecolor = [0,0.7,1],linewidth = 1,zorder = 3)\n\n            ind1 = np.argwhere(y_train == -1)\n            ind1 = [v[1] for v in ind1]\n            ax.scatter(x_train[ind1,0],x_train[ind1,1],s = 45, color = self.colors[1], edgecolor = [0,0.7,1],linewidth = 1,zorder = 3)\n            ax.set_title('training data',fontsize = 15)\n\n        if train_valid == 'validate':\n            # reverse normalize data\n            x_valid = inverse_nornalizer(runner.x_valid).T\n            y_valid = runner.y_valid\n        \n            # plot testing points\n            ind0 = np.argwhere(y_valid == +1)\n            ind0 = [v[1] for v in ind0]\n            ax.scatter(x_valid[ind0,0],x_valid[ind0,1],s = 45, color = self.colors[0], edgecolor = [1,0.8,0.5],linewidth = 1,zorder = 3)\n\n            ind1 = np.argwhere(y_valid == -1)\n            ind1 = [v[1] for v in ind1]\n            ax.scatter(x_valid[ind1,0],x_valid[ind1,1],s = 45, color = self.colors[1], edgecolor = [1,0.8,0.5],linewidth = 1,zorder = 3)\n            ax.set_title('validation data',fontsize = 15)\n                \n        if train_valid == 'original':\n            # plot all points\n            ind0 = np.argwhere(self.y == +1)\n            ax.scatter(self.x[ind0,0],self.x[ind0,1],s = 55, color = self.colors[0], edgecolor = 'k',linewidth = 1,zorder = 3)\n\n            ind1 = np.argwhere(self.y == -1)\n            ax.scatter(self.x[ind1,0],self.x[ind1,1],s = 55, color = self.colors[1], edgecolor = 'k',linewidth = 1,zorder = 3)\n            ax.set_title('original data',fontsize = 15)\n\n        #### plot contour, color regions ####\n        ax.contour(s,t,z1,colors='k', linewidths=2.5,levels = [0],zorder = 2)\n        ax.contourf(s,t,z1,colors = [self.colors[1],self.colors[0]],alpha = 0.15,levels = range(-1,2))\n\n        # cleanup panel\n        ax.set_xlabel(r'$x_1$',fontsize = 15)\n        ax.set_ylabel(r'$x_2$',fontsize = 15,rotation = 0,labelpad = 20)\n        ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n        ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n        \n      \n    \n    \n    \n   \n    \n    # toy plot\n    def static_MULTI_simple(self,run,w,**kwargs):\n        model = run.model\n        normalizer = run.normalizer\n        feat = run.feature_transforms\n        \n        # grab args\n        view = [20,-70]\n        if 'view' in kwargs:\n            view = kwargs['view']\n \n        ### plot all input data ###\n        # generate input range for functions\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n\n        r = np.linspace(minx,maxx,600)\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        h = np.concatenate([w1_vals,w2_vals],axis = 1).T\n\n        g_vals = model(normalizer(h),w)\n        g_vals = np.asarray(g_vals)\n        g_new = copy.deepcopy(g_vals).T\n        g_vals = np.argmax(g_vals,axis = 0)\n\n        # vals for cost surface\n        w1_vals.shape = (len(r),len(r))\n        w2_vals.shape = (len(r),len(r))\n        g_vals.shape = (len(r),len(r))\n\n        # count points\n        class_nums = np.unique(self.y)\n        C = int(len(class_nums))\n        \n        fig = plt.figure(figsize = (10,7))\n        gs = gridspec.GridSpec(2, C) \n\n        #### left plot - data and fit in original space ####\n        # setup current axis\n        #ax1 = plt.subplot(gs[C],projection = '3d');\n        ax2 = plt.subplot(gs[C+1],aspect = 'equal');\n        fig.subplots_adjust(left=0,right=1,bottom=0,top=1)   # remove whitespace around 3d figure\n\n        ##### plot top panels ####\n        for d in range(C):\n            # create panel\n            ax = plt.subplot(gs[d],aspect = 'equal');\n                       \n            for c in range(C):\n                # plot points\n                ind = np.argwhere(self.y == class_nums[c])\n                ind = [v[0] for v in ind]\n                ax.scatter(self.x[ind,0],self.x[ind,1],s = 50,color = self.colors[c],edgecolor = 'k', linewidth = 2)\n            \n            g_2 = np.sign(g_new[:,d])\n            g_2.shape = (len(r),len(r))\n\n            # plot separator curve \n            ax.contour(w1_vals,w2_vals,g_2+1,colors = 'k',levels = [-1,1],linewidths = 4.5,zorder = 1,linestyle = '-')\n            ax.contour(w1_vals,w2_vals,g_2+1,colors = self.colors[d],levels = [-1,1],linewidths = 2.5,zorder = 1,linestyle = '-')\n                \n            ax.set_xlabel(r'$x_1$', fontsize = 18,labelpad = 10)\n            ax.set_ylabel(r'$x_2$', rotation = 0,fontsize = 18,labelpad = 15)\n        \n        ##### plot bottom panels ###\n        # scatter points in both bottom panels\n        for c in range(C):\n            ind = np.argwhere(self.y == class_nums[c])\n            ind = [v[0] for v in ind]\n            #ax1.scatter(self.x[ind,0],self.x[ind,1],self.y[ind],s = 50,color = self.colors[c],edgecolor = 'k',linewidth = 1.5)\n            ax2.scatter(self.x[ind,0],self.x[ind,1],s = 50,color = self.colors[c],edgecolor = 'k', linewidth = 2)\n      \n        #ax1.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = 'w',rstride=45, cstride=45,linewidth=0.25,edgecolor = 'k')\n\n        #for c in range(C):\n            # plot separator curve in left plot z plane\n            #ax1.contour(w1_vals,w2_vals,g_vals - c,colors = 'k',levels = [0],linewidths = 3,zorder = 1)\n\n            # color parts of plane with correct colors\n            #ax1.contourf(w1_vals,w2_vals,g_vals - c +0.5,colors = self.colors[c],alpha = 0.4,levels = [0,1])\n             \n        # plot separator in right plot\n        ax2.contour(w1_vals,w2_vals,g_vals,colors = 'k',levels = range(0,C+1),linewidths = 3,zorder = 1)\n\n        # adjust height of regressor to plot filled contours\n        ax2.contourf(w1_vals,w2_vals,g_vals+0.5,colors = self.colors[:],alpha = 0.2,levels = range(0,C+1))\n        \n\n    #### animate multiple runs on single regression ####\n    def animate_multiclass_crossval_classifications(self,runs,**kwargs):\n        weight_history = []\n        train_errors = []\n        valid_errors = []\n        for run in runs:\n            # get histories\n            train_counts = run.train_count_histories[0]\n            valid_counts = run.valid_count_histories[0]\n            weights = run.weight_histories[0]\n            \n            # select based on minimum training\n            ind = np.argmin(train_counts)\n            train_count = train_counts[ind]\n            valid_count = valid_counts[ind]\n            weight = weights[ind]\n            \n            # store\n            train_errors.append(train_count)\n            valid_errors.append(valid_count)\n            weight_history.append(weight)\n       \n        # construct figure\n        fig = plt.figure(figsize = (6,6))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(2, 2)\n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]); \n        ax3 = plt.subplot(gs[2]); \n        ax4 = plt.subplot(gs[3]);\n        \n        # start animation\n        num_frames = len(runs)        \n        print ('starting animation rendering...')\n        def animate(k):\n            print (k)\n            # clear panels\n            ax1.cla()\n            ax2.cla()\n            ax3.cla()\n            ax4.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            #### plot training, testing, and full data ####            \n            # pluck out current weights \n            w_best = weight_history[k]\n            run = runs[k]\n            \n            # produce static img\n            self.static_MULTI_simple(ax1,w_best,run,train_valid = 'original')\n            self.static_MULTI_simple(ax2,w_best,run,train_valid = 'train')\n            self.static_MULTI_simple(ax3,w_best,run,train_valid = 'validate')\n\n            #### plot training / validation errors ####\n            self.plot_train_valid_errors(ax4,k,train_errors,valid_errors)\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        return(anim)\n\n  \n     \n    \n    \n    ######## show N = 2 static image ########\n    # show coloring of entire space\n    def static_MULTI_simple(self,ax,w_best,runner,train_valid):\n        cost = runner.cost\n        predict = runner.model\n        feat = runner.feature_transforms\n        normalizer = runner.normalizer\n        inverse_nornalizer = runner.inverse_normalizer\n      \n        # or just take last weights\n        self.w = w_best\n        \n        ### create boundary data ###\n        xmin1 = np.min(self.x[:,0])\n        xmax1 = np.max(self.x[:,0])\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        xmin2 = np.min(self.x[:,1])\n        xmax2 = np.max(self.x[:,1])\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2    \n\n        # plot boundary for 2d plot\n        r1 = np.linspace(xmin1,xmax1,800)\n        r2 = np.linspace(xmin2,xmax2,800)\n        s,t = np.meshgrid(r1,r2)\n        s = np.reshape(s,(np.size(s),1))\n        t = np.reshape(t,(np.size(t),1))\n        h = np.concatenate((s,t),axis = 1)\n        \n        # compute model on train data\n        C = len(np.unique(self.y))\n   \n        z1 = predict(normalizer(h.T),self.w)\n        z1 = np.asarray(z1)\n        z1 = np.argmax(z1,axis = 0)\n\n        # reshape it\n        s.shape = (np.size(r1),np.size(r2))\n        t.shape = (np.size(r1),np.size(r2))     \n        z1.shape = (np.size(r1),np.size(r2))\n\n        ### loop over two panels plotting each ###\n        # plot training points\n        if train_valid == 'train':\n            # reverse normalize data\n            x_train = inverse_nornalizer(runner.x_train).T\n            y_train = runner.y_train\n            \n            # scatter data\n            for c in range(C):\n                # plot points\n                ind0 = np.argwhere(y_train == c)\n                ind0 = [v[1] for v in ind0]\n                ax.scatter(x_train[ind0,0],x_train[ind0,1],s = 45,color = self.colors[c], edgecolor = [0,0.7,1],linewidth = 1,zorder = 3)     \n            \n            ax.set_title('training data',fontsize = 15)\n\n        if train_valid == 'validate':\n            # reverse normalize data\n            x_valid = inverse_nornalizer(runner.x_valid).T\n            y_valid = runner.y_valid\n            \n            # scatter data\n            for c in range(C):\n                # plot points\n                ind0 = np.argwhere(y_valid == c)\n                ind0 = [v[1] for v in ind0]\n                ax.scatter(x_valid[ind0,0],x_valid[ind0,1],s = 45,color = self.colors[c], edgecolor = [1,0.8,0.5],linewidth = 1,zorder = 3)\n \n            ax.set_title('validation data',fontsize = 15)\n                \n        if train_valid == 'original':\n            \n            # scatter data\n            for c in range(C):\n                # plot points\n                ind0 = np.argwhere(self.y == c)\n                ind0 = [v[0] for v in ind0]\n                ax.scatter(self.x[ind0,0],self.x[ind0,1],s = 55,color = self.colors[c], edgecolor = 'k',linewidth = 1,zorder = 3)\n\n            ax.set_title('original data',fontsize = 15)\n        \n        #### plot contour, color regions ####\n        for c in range(C):\n            # plot separator curve \n            ax.contour(s,t,z1+1,colors = 'k',levels = [-1,1],linewidths = 1.5,zorder = 1,linestyle = '-')\n            #ax.contour(s,t,z1+1,colors = self.colors[c],levels = [-1,1],linewidths = 2.5,zorder = 1,linestyle = '-')\n                \n        # plot separator in right plot\n        ax.contour(s,t,z1,colors = 'k',levels = range(0,C+1),linewidths = 3,zorder = 1)\n            \n        # adjust height of regressor to plot filled contours\n        ax.contourf(s,t,z1+0.5,colors = self.colors[:],alpha = 0.2,levels = range(0,C+1))\n        \n      \n\n        \n        \n        \n        \n        \n        \n        \n        # cleanup panel\n        ax.set_xlabel(r'$x_1$',fontsize = 15)\n        ax.set_ylabel(r'$x_2$',fontsize = 15,rotation = 0,labelpad = 20)\n        ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n        ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))        """
mlrefined_libraries/nonlinear_superlearn_library/crossval_classification_visualizer.py,34,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nimport autograd.numpy as np\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\nfrom matplotlib.ticker import FormatStrFormatter\nfrom inspect import signature\n\nclass Visualizer:\n    \'\'\'\n    Visualize cross validation performed on N = 2 dimensional input classification datasets\n    \'\'\'\n    #### initialize ####\n    def __init__(self,csvname):\n        # grab input\n        data = np.loadtxt(csvname,delimiter = \',\')\n        data = data.T\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n\n        self.colors = [\'salmon\',\'cornflowerblue\',\'lime\',\'bisque\',\'mediumaquamarine\',\'b\',\'m\',\'g\']\n\n    ######## show N = 2 static image ########\n    # show coloring of entire space\n    def static_N2_simple(self,w_best,runner,**kwargs):\n        cost = runner.cost\n        predict = runner.model\n        full_predict = runner.full_model\n        feat = runner.feature_transforms\n        normalizer = runner.normalizer\n        inverse_nornalizer = runner.inverse_normalizer\n        x_train = inverse_nornalizer(runner.x_train).T\n        y_train = runner.y_train\n\n        x_test = inverse_nornalizer(runner.x_test).T\n        y_test = runner.y_test\n      \n        # or just take last weights\n        self.w = w_best\n\n        # construct figure\n        fig, axs = plt.subplots(1, 1, figsize=(9,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 2,width_ratios = [3,1]) \n        ax2 = plt.subplot(gs[0],aspect = \'equal\')\n        ax3 = plt.subplot(gs[1]); ax3.axis(\'off\')\n        \n        ### create boundary data ###\n        xmin1 = np.min(self.x[:,0])\n        xmax1 = np.max(self.x[:,0])\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        xmin2 = np.min(self.x[:,1])\n        xmax2 = np.max(self.x[:,1])\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2    \n\n        # plot boundary for 2d plot\n        r1 = np.linspace(xmin1,xmax1,300)\n        r2 = np.linspace(xmin2,xmax2,300)\n        s,t = np.meshgrid(r1,r2)\n        s = np.reshape(s,(np.size(s),1))\n        t = np.reshape(t,(np.size(t),1))\n        h = np.concatenate((s,t),axis = 1)\n        \n        # compute model on train data\n        z1 = predict(normalizer(h.T),self.w)\n        z1 = np.sign(z1)\n\n        # reshape it\n        s.shape = (np.size(r1),np.size(r2))\n        t.shape = (np.size(r1),np.size(r2))     \n        z1.shape = (np.size(r1),np.size(r2))\n\n        ### loop over two panels plotting each ###\n        for ax in [ax2]:\n            # plot training points\n            ind0 = np.argwhere(y_train == +1)\n            ind0 = [v[1] for v in ind0]\n            ax.scatter(x_train[ind0,0],x_train[ind0,1],s = 55, color = self.colors[0], edgecolor = \'k\',linewidth = 2.5,zorder = 3)\n\n            ind1 = np.argwhere(y_train == -1)\n            ind1 = [v[1] for v in ind1]\n            ax.scatter(x_train[ind1,0],x_train[ind1,1],s = 55, color = self.colors[1], edgecolor = \'k\',linewidth = 2.5,zorder = 3)\n\n            # plot testing points\n            ind0 = np.argwhere(y_test == +1)\n            ind0 = [v[1] for v in ind0]\n            ax.scatter(x_test[ind0,0],x_test[ind0,1],s = 55, color = self.colors[0], edgecolor = [1,0.8,0.5],linewidth = 2.5,zorder = 3)\n\n            ind1 = np.argwhere(y_test == -1)\n            ind1 = [v[1] for v in ind1]\n            ax.scatter(x_test[ind1,0],x_test[ind1,1],s = 55, color = self.colors[1], edgecolor = [1,0.8,0.5],linewidth = 2.5,zorder = 3)\n\n            #### plot contour, color regions ####\n            ax.contour(s,t,z1,colors=\'k\', linewidths=2.5,levels = [0],zorder = 2)\n            ax.contourf(s,t,z1,colors = [self.colors[1],self.colors[0]],alpha = 0.15,levels = range(-1,2))\n  \n            # cleanup panel\n            ax.set_xlabel(r\'$x_1$\',fontsize = 15)\n            ax.set_ylabel(r\'$x_2$\',fontsize = 15,rotation = 0,labelpad = 20)\n            ax.xaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            ax.yaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n        \n    ###### plot plotting functions ######\n    def plot_data(self):\n        fig = 0\n        # plot data in two and one-d\n        if np.shape(self.x)[1] < 2:\n            # construct figure\n            fig, axs = plt.subplots(2,1, figsize=(4,4))\n            gs = gridspec.GridSpec(2,1,height_ratios = [6,1]) \n            ax1 = plt.subplot(gs[0],aspect = \'equal\');\n            ax2 = plt.subplot(gs[1],sharex = ax1); \n            \n            # set plotting limits\n            xmax = copy.deepcopy(max(self.x))\n            xmin = copy.deepcopy(min(self.x))\n            xgap = (xmax - xmin)*0.2\n            xmin -= xgap\n            xmax += xgap\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.5\n            ymin -= ygap\n            ymax += ygap    \n\n            ### plot in 2-d\n            ax1.scatter(self.x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n\n            # clean up panel\n            ax1.set_xlim([xmin,xmax])\n            ax1.set_ylim([ymin,ymax])\n            ax1.axhline(linewidth=0.5, color=\'k\',zorder = 1)\n            \n            ### plot in 1-d\n            ind0 = np.argwhere(self.y == +1)\n            ax2.scatter(self.x[ind0],np.zeros((len(self.x[ind0]))),s = 55, color = self.colors[0], edgecolor = \'k\',zorder = 3)\n\n            ind1 = np.argwhere(self.y == -1)\n            ax2.scatter(self.x[ind1],np.zeros((len(self.x[ind1]))),s = 55, color = self.colors[1], edgecolor = \'k\',zorder = 3)\n            ax2.set_yticks([0])\n            ax2.axhline(linewidth=0.5, color=\'k\',zorder = 1)\n        \n            ax2.xaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            ax2.yaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            \n        if np.shape(self.x)[1] == 2:\n            # construct figure\n            fig, axs = plt.subplots(1, 2, figsize=(9,4))\n\n            # create subplot with 2 panels\n            gs = gridspec.GridSpec(1, 2) \n            ax2 = plt.subplot(gs[1],aspect = \'equal\'); \n            ax1 = plt.subplot(gs[0],projection=\'3d\'); \n\n            # scatter points\n            self.scatter_pts(ax1,self.x)\n            \n            ### from above\n            ax2.set_xlabel(r\'$x_1$\',fontsize = 15)\n            ax2.set_ylabel(r\'$x_2$\',fontsize = 15,rotation = 0,labelpad = 20)\n            ax2.xaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            ax2.yaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            \n            # plot points in 2d and 3d\n            C = len(np.unique(self.y))\n            if C == 2:\n                ind0 = np.argwhere(self.y == +1)\n                ax2.scatter(self.x[ind0,0],self.x[ind0,1],s = 55, color = self.colors[0], edgecolor = \'k\')\n\n                ind1 = np.argwhere(self.y == -1)\n                ax2.scatter(self.x[ind1,0],self.x[ind1,1],s = 55, color = self.colors[1], edgecolor = \'k\')\n            else:\n                for c in range(C):\n                    ind0 = np.argwhere(self.y == c)\n                    ax2.scatter(self.x[ind0,0],self.x[ind0,1],s = 55, color = self.colors[c], edgecolor = \'k\')\n                    \n        \n            self.move_axis_left(ax1)\n            ax1.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 5)\n            ax1.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax1.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n        \n    # scatter points\n    def scatter_pts(self,ax,x):\n        if np.shape(x)[1] == 1:\n            # set plotting limits\n            xmax = copy.deepcopy(max(x))\n            xmin = copy.deepcopy(min(x))\n            xgap = (xmax - xmin)*0.2\n            xmin -= xgap\n            xmax += xgap\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n\n            # clean up panel\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            \n        if np.shape(x)[1] == 2:\n            # set plotting limits\n            xmax1 = copy.deepcopy(max(x[:,0]))\n            xmin1 = copy.deepcopy(min(x[:,0]))\n            xgap1 = (xmax1 - xmin1)*0.1\n            xmin1 -= xgap1\n            xmax1 += xgap1\n            \n            xmax2 = copy.deepcopy(max(x[:,1]))\n            xmin2 = copy.deepcopy(min(x[:,1]))\n            xgap2 = (xmax2 - xmin2)*0.1\n            xmin2 -= xgap2\n            xmax2 += xgap2\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(x[:,0],x[:,1],self.y.flatten(),s = 40,color = \'k\', edgecolor = \'w\',linewidth = 0.9)\n\n            # clean up panel\n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            ax.set_zlim([ymin,ymax])\n            \n            ax.set_xticks(np.arange(round(xmin1), round(xmax1)+1, 1.0))\n            ax.set_yticks(np.arange(round(xmin2), round(xmax2)+1, 1.0))\n            ax.set_zticks(np.arange(round(ymin), round(ymax)+1, 1.0))\n           \n            # clean up panel\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n'"
mlrefined_libraries/nonlinear_superlearn_library/main_classification_animators.py,33,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\n\nclass Visualizer:\n    '''\n    Compare various basis units on 3d classification\n    '''\n    #### initialize ####\n    def __init__(self,csvname):\n        # grab input\n        data = np.loadtxt(csvname,delimiter = ',')\n        self.x = data[:-1,:]\n        self.y = data[-1,:]\n        self.y.shape = (1,len(self.y))\n        self.colors = ['salmon','cornflowerblue','lime','bisque','mediumaquamarine','b','m','g']\n  \n    ########## animate results of nonlinear classification ##########\n    def animate_classifications(self,savepath,runs,frames,**kwargs):\n        # select subset of runs\n        inds = np.arange(0,len(runs),int(len(runs)/float(frames)))\n        \n        # pull out cost vals\n        cost_evals = []\n        for run in runs:\n            # get current run histories\n            cost_history = run.train_cost_histories[0]\n            weight_history = run.weight_histories[0]\n\n            # get best weights                \n            win = np.argmin(cost_history)\n            \n            # get best cost val\n            cost = cost_history[win]\n            cost_evals.append(cost)\n                    \n        # select inds of history to plot\n        num_runs = frames\n\n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[2,1,0.25]) \n        ax1 = plt.subplot(gs[0]); ax1.set_aspect('equal'); ax1.axis('off');\n        ax2 = plt.subplot(gs[1]); ax2.axis('off');\n        ax3 = plt.subplot(gs[2]); ax3.axis('off');\n        \n        # setup viewing range\n        num_elements = len(cost_evals)\n        minxc = 0.5\n        maxxc = num_elements + 0.5\n\n        ymax = max(copy.deepcopy(cost_evals))\n        ymin = min(copy.deepcopy(cost_evals))\n        ygap = (ymax - ymin)*0.1\n        ymax += ygap\n        ymin -= ygap\n        \n        # plot cost path - scale to fit inside same aspect as classification plots        \n        # start animation\n        num_frames = num_runs\n        print ('starting animation rendering...')\n        def animate(k):\n            # clear panels\n            ax1.cla()\n            ax2.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            #### scatter data ####\n            # plot points in 2d and 3d\n            ind0 = np.argwhere(self.y == +1)\n            ind0 = [e[1] for e in ind0]\n            ax1.scatter(self.x[0,ind0],self.x[1,ind0],s = 55, color = self.colors[0], edgecolor = 'k')\n                        \n            ind1 = np.argwhere(self.y == -1)\n            ind1 = [e[1] for e in ind1]\n            ax1.scatter(self.x[0,ind1],self.x[1,ind1],s = 55, color = self.colors[1], edgecolor = 'k')\n            \n            # plot boundary\n            if k > 0:\n                # get current run for cost function history plot\n                a = inds[k-1]\n                run = runs[a]\n                \n                # pluck out current weights \n                self.draw_fit(ax1,run,a)\n                \n                # cost function value\n                ax2.plot(np.arange(1,num_elements + 1),cost_evals,color = 'k',linewidth = 2.5,zorder = 1)\n                ax2.scatter(a + 1,cost_evals[a],color = self.colors[0],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n            \n            # cleanup panels\n            ax1.set_yticklabels([])\n            ax1.set_xticklabels([])\n            ax1.set_xticks([])\n            ax1.set_yticks([])\n            ax1.set_xlabel(r'$x_1$',fontsize = 15)\n            ax1.set_ylabel(r'$x_2$',fontsize = 15,rotation = 0,labelpad = 20)  \n            \n            ax2.set_xlabel('number of units',fontsize = 12)\n            ax2.set_title('cost function plot',fontsize = 14)\n            ax2.set_xlim([minxc,maxxc])\n            ax2.set_ylim([ymin,ymax])\n        \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames+1, interval=num_frames+1, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()   \n            \n    def draw_fit(self,ax,run,ind):\n        # viewing ranges\n        xmin1 = min(copy.deepcopy(self.x[0,:]))\n        xmax1 = max(copy.deepcopy(self.x[0,:]))\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        xmin2 = min(copy.deepcopy(self.x[1,:]))\n        xmax2 = max(copy.deepcopy(self.x[1,:]))\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2\n\n        ymin = min(copy.deepcopy(self.y))\n        ymax = max(copy.deepcopy(self.y))\n        ygap = (ymax - ymin)*0.05\n        ymin -= ygap\n        ymax += ygap\n             \n        # plot boundary for 2d plot\n        r1 = np.linspace(xmin1,xmax1,300)\n        r2 = np.linspace(xmin2,xmax2,300)\n        s,t = np.meshgrid(r1,r2)\n        s = np.reshape(s,(np.size(s),1))\n        t = np.reshape(t,(np.size(t),1))\n        h = np.concatenate((s,t),axis = 1).T\n        \n        # plot total fit\n        cost = run.cost\n        model = run.model\n        feat = run.feature_transforms\n        normalizer = run.normalizer\n        cost_history = run.train_cost_histories[0]\n        weight_history = run.weight_histories[0]\n\n        # get best weights                \n        win = np.argmin(cost_history)\n        w = weight_history[win]        \n        \n        model = lambda b: run.model(normalizer(b),w)\n        z = model(h)\n        z = np.sign(z)\n\n        # reshape it\n        s.shape = (np.size(r1),np.size(r2))\n        t.shape = (np.size(r1),np.size(r2))     \n        z.shape = (np.size(r1),np.size(r2))\n\n        #### plot contour, color regions ####\n        ax.contour(s,t,z,colors='k', linewidths=2.5,levels = [0],zorder = 2)\n        ax.contourf(s,t,z,colors = [self.colors[1],self.colors[0]],alpha = 0.15,levels = range(-1,2))\n                \n        ### cleanup left plots, create max view ranges ###\n        ax.set_xlim([xmin1,xmax1])\n        ax.set_ylim([xmin2,xmax2])\n        ax.set_title(str(ind+1) + ' units fit to data',fontsize = 14)\n        \n        \n    ########## animate results of boosting ##########\n    def animate_boosting_classifications(self,run,frames,**kwargs):\n        # select subset of runs\n        inds = np.arange(0,len(run.models),int(len(run.models)/float(frames)))\n\n        # select inds of history to plot\n        num_runs = frames\n\n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n        \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0],aspect = 'equal'); ax.axis('off');\n        \n        # start animation\n        num_frames = num_runs\n        print ('starting animation rendering...')\n        def animate(k):\n            # clear panels\n            ax.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            #### scatter data ####\n            # plot points in 2d and 3d\n            ind0 = np.argwhere(self.y == +1)\n            ind0 = [e[1] for e in ind0]\n            ax.scatter(self.x[0,ind0],self.x[1,ind0],s = 55, color = self.colors[0], edgecolor = 'k')\n                        \n            ind1 = np.argwhere(self.y == -1)\n            ind1 = [e[1] for e in ind1]\n            ax.scatter(self.x[0,ind1],self.x[1,ind1],s = 55, color = self.colors[1], edgecolor = 'k')\n            \n            # plot boundary\n            if k > 0:\n                # get current run for cost function history plot\n                a = inds[k-1]\n                model = run.models[a]\n                steps = run.best_steps[:a+1]\n                \n                # pluck out current weights \n                self.draw_boosting_fit(ax,steps,a)\n            \n            # cleanup panel\n            ax.set_yticklabels([])\n            ax.set_xticklabels([])\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_xlabel(r'$x_1$',fontsize = 15)\n            ax.set_ylabel(r'$x_2$',fontsize = 15,rotation = 0,labelpad = 20)  \n        \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames+1, interval=num_frames+1, blit=True)\n        \n        return(anim)\n    \n    def draw_boosting_fit(self,ax,steps,ind):\n        # viewing ranges\n        xmin1 = min(copy.deepcopy(self.x[0,:]))\n        xmax1 = max(copy.deepcopy(self.x[0,:]))\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        xmin2 = min(copy.deepcopy(self.x[1,:]))\n        xmax2 = max(copy.deepcopy(self.x[1,:]))\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2\n\n        ymin = min(copy.deepcopy(self.y))\n        ymax = max(copy.deepcopy(self.y))\n        ygap = (ymax - ymin)*0.05\n        ymin -= ygap\n        ymax += ygap\n             \n        # plot boundary for 2d plot\n        r1 = np.linspace(xmin1,xmax1,30)\n        r2 = np.linspace(xmin2,xmax2,30)\n        s,t = np.meshgrid(r1,r2)\n        s = np.reshape(s,(np.size(s),1))\n        t = np.reshape(t,(np.size(t),1))\n        h = np.concatenate((s,t),axis = 1).T\n        \n        model = lambda x: np.sum([v(x) for v in steps],axis=0)\n        z = model(h)\n        z = np.sign(z)\n\n        # reshape it\n        s.shape = (np.size(r1),np.size(r2))\n        t.shape = (np.size(r1),np.size(r2))     \n        z.shape = (np.size(r1),np.size(r2))\n\n        #### plot contour, color regions ####\n        ax.contour(s,t,z,colors='k', linewidths=2.5,levels = [0],zorder = 2)\n        ax.contourf(s,t,z,colors = [self.colors[1],self.colors[0]],alpha = 0.15,levels = range(-1,2))\n                \n        ### cleanup left plots, create max view ranges ###\n        ax.set_xlim([xmin1,xmax1])\n        ax.set_ylim([xmin2,xmax2])\n        ax.set_title(str(ind+1) + ' units fit to data',fontsize = 14)\n        \n """
mlrefined_libraries/nonlinear_superlearn_library/main_classification_comparison.py,29,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom matplotlib.ticker import FormatStrFormatter\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\n\n# import autograd functionality\nimport autograd.numpy as np\n\n# import standard libraries\nimport math\nimport time\nimport copy\nfrom inspect import signature\n\n# custom fit libraries\nfrom . import intro_boost_library\nfrom . import intro_general_library\n\nclass Visualizer:\n    '''\n    Visualize linear regression in 2 and 3 dimensions.  For single input cases (2 dimensions) the path of gradient descent on the cost function can be animated.\n    '''\n    #### initialize ####\n    def __init__(self,csvname):\n        # grab input\n        data = np.loadtxt(csvname,delimiter = ',')\n        self.x = data[:-1,:]\n        self.y = data[-1,:]\n        self.y.shape = (1,len(self.y))\n        self.colors = ['salmon','cornflowerblue','lime','bisque','mediumaquamarine','b','m','g']\n        \n    ####### fit each type of universal approximator #######\n    def run_approximators(self,**kwargs):\n        # define number of units per approximator\n        poly_units = 5\n        net_units = 8\n        tree_units = 8\n        \n        if 'poly_units' in kwargs:\n            poly_units = kwargs['poly_units']\n        if 'net_units' in kwargs:\n            net_units = kwargs['net_units']\n        if 'tree_units' in kwargs:\n            tree_units = kwargs['tree_units']\n            \n        # run polys\n        self.runs1 = self.run_poly(poly_units)\n        \n        # run nets\n        self.runs2 = self.run_net(net_units)\n        \n        # run trees\n        self.runs3 = self.run_trees(tree_units) \n\n    ### run classification with poly features ###\n    def run_poly(self,num_units):\n        runs = []\n        for j in range(num_units):\n            print ('fitting ' + str(j + 1) + ' poly units')\n\n            # import the v1 library\n            mylib = intro_general_library.superlearn_setup.Setup(self.x,self.y)\n\n            # choose features\n            mylib.choose_features(name = 'polys',degree = j+1)\n\n            # choose normalizer\n            mylib.choose_normalizer(name = 'none')\n\n            # pick training set\n            mylib.make_train_valid_split(train_portion=1)\n\n            # choose cost\n            mylib.choose_cost(name = 'softmax')\n\n            # fit an optimization\n            mylib.fit(max_its = 5,optimizer = 'newtons_method',epsilon = 10**(-5))\n\n            # add model to list\n            runs.append(copy.deepcopy(mylib))\n            \n        print ('poly run complete')\n        time.sleep(1.5)\n        clear_output()\n        return runs \n\n    ### run classification with net features ###\n    def run_net(self,num_units):\n        runs = []\n        for j in range(num_units):\n            print ('fitting ' + str(j + 1) + ' net units')\n\n            # import the v1 library\n            mylib = intro_general_library.superlearn_setup.Setup(self.x,self.y)\n\n            # choose features\n            mylib.choose_features(name = 'multilayer_perceptron',layer_sizes = [2,j+1,1],activation = 'tanh')\n\n            # choose normalizer\n            mylib.choose_normalizer(name = 'standard')\n\n            # pick training set\n            mylib.make_train_valid_split(train_portion=1)\n\n            # choose cost\n            mylib.choose_cost(name = 'softmax')\n\n            # fit an optimization\n            mylib.fit(max_its = 1000,alpha_choice = 10**(0),optimizer = 'gradient_descent')\n\n            # add model to list\n            runs.append(copy.deepcopy(mylib))\n            \n        print ('net run complete')\n        time.sleep(1.5)\n        clear_output()\n        return runs \n    \n    ### run classification with tree features ###\n    def run_trees(self,num_rounds):\n        # import booster\n        mylib = intro_boost_library.stump_booster.Setup(self.x,self.y)\n\n        # choose normalizer\n        mylib.choose_normalizer(name = 'none')\n\n        # pick training set\n        mylib.make_train_valid_split(train_portion=1)\n\n        # choose cost|\n        mylib.choose_cost(name = 'softmax')\n\n        # choose optimizer\n        mylib.choose_optimizer('newtons_method',max_its=1)\n\n        # run boosting\n        mylib.boost(num_rounds)\n\n        return mylib\n  \n    ########## show classification results ##########\n    def animate_comparisons(self,savepath,frames,**kwargs):\n        pt_size = 55\n        if 'pt_size' in kwargs:\n            pt_size = kwargs['pt_size']\n            \n        ### get inds for each run ###\n        runs1 = self.runs1; runs2 = self.runs2; runs3 = self.runs3;\n        inds1 = np.arange(0,len(runs1),int(len(runs1)/float(frames)))\n        inds2 = np.arange(0,len(runs2),int(len(runs2)/float(frames)))\n        inds3 = np.arange(0,len(runs3.models),int(len(runs3.models)/float(frames)))\n            \n        # select inds of history to plot\n        num_runs = frames\n\n        # construct figure\n        fig = plt.figure(figsize=(9,4))\n        artist = fig\n\n        # create subplot with 1 active panel\n        gs = gridspec.GridSpec(1, 3, width_ratios = [1,1,1]) \n        ax1 = plt.subplot(gs[0]); ax1.set_aspect('equal'); \n        ax1.axis('off');\n        ax1.xaxis.set_visible(False) # Hide only x axis\n        ax1.yaxis.set_visible(False) # Hide only x axis\n        \n        ax2 = plt.subplot(gs[1]); ax2.set_aspect('equal'); \n        ax2.axis('off');\n        ax2.xaxis.set_visible(False) # Hide only x axis\n        ax2.yaxis.set_visible(False) # Hide only x axis\n        \n        ax3 = plt.subplot(gs[2]); ax3.set_aspect('equal'); \n        ax3.axis('off');\n        ax3.xaxis.set_visible(False) # Hide only x axis\n        ax3.yaxis.set_visible(False) # Hide only x axis\n        \n        # viewing ranges\n        xmin1 = min(copy.deepcopy(self.x[0,:]))\n        xmax1 = max(copy.deepcopy(self.x[0,:]))\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        xmin2 = min(copy.deepcopy(self.x[1,:]))\n        xmax2 = max(copy.deepcopy(self.x[1,:]))\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2\n\n        # start animation\n        num_frames = num_runs\n        print ('starting animation rendering...')\n        def animate(k):\n            # clear panels\n            ax1.cla()\n            ax2.cla()\n            ax3.cla()\n\n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            # scatter data\n            ind0 = np.argwhere(self.y == +1)\n            ind0 = [e[1] for e in ind0]\n            ind1 = np.argwhere(self.y == -1)\n            ind1 = [e[1] for e in ind1]\n            for ax in [ax1,ax2,ax3]:\n                ax.scatter(self.x[0,ind0],self.x[1,ind0],s = pt_size, color = self.colors[0], edgecolor = 'k',antialiased=True)\n                ax.scatter(self.x[0,ind1],self.x[1,ind1],s = pt_size, color = self.colors[1], edgecolor = 'k',antialiased=True)\n                \n            if k == 0:\n                ax1.set_title(str(0) + ' units fit to data',fontsize = 14,color = 'w')\n                ax1.set_title(str(0) + ' units fit to data',fontsize = 14,color = 'w')\n                ax1.set_title(str(0) + ' units fit to data',fontsize = 14,color = 'w')\n                \n                ax1.set_xlim([xmin1,xmax1])\n                ax1.set_ylim([xmin2,xmax2])\n                ax2.set_xlim([xmin1,xmax1])\n                ax2.set_ylim([xmin2,xmax2])                \n                ax3.set_xlim([xmin1,xmax1])\n                ax3.set_ylim([xmin2,xmax2])\n                \n            # plot fit\n            if k > 0:\n                # get current run\n                a1 = inds1[k-1] \n                a2 = inds2[k-1] \n                a3 = inds3[k-1] \n\n                run1 = runs1[a1]\n                a1 = len(run1.w_init) - 1\n\n                run2 = runs2[a2]\n                model3 = runs3.models[a3]\n                steps = runs3.best_steps[:a3+1]\n                \n                # plot models to data\n                self.draw_fit(ax1,run1,a1)\n                self.draw_fit(ax2,run2,a2 + 1)\n                self.draw_boosting_fit(ax3,steps,a3)\n                \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames+1, interval=num_frames+1, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()\n\n\n    def draw_fit(self,ax,run,num_units):\n        # viewing ranges\n        xmin1 = min(copy.deepcopy(self.x[0,:]))\n        xmax1 = max(copy.deepcopy(self.x[0,:]))\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        xmin2 = min(copy.deepcopy(self.x[1,:]))\n        xmax2 = max(copy.deepcopy(self.x[1,:]))\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2\n\n        ymin = min(copy.deepcopy(self.y))\n        ymax = max(copy.deepcopy(self.y))\n        ygap = (ymax - ymin)*0.05\n        ymin -= ygap\n        ymax += ygap\n             \n        # plot boundary for 2d plot\n        r1 = np.linspace(xmin1,xmax1,300)\n        r2 = np.linspace(xmin2,xmax2,300)\n        s,t = np.meshgrid(r1,r2)\n        s = np.reshape(s,(np.size(s),1))\n        t = np.reshape(t,(np.size(t),1))\n        h = np.concatenate((s,t),axis = 1).T\n        \n        # plot total fit\n        cost = run.cost\n        model = run.model\n        feat = run.feature_transforms\n        normalizer = run.normalizer\n        cost_history = run.train_cost_histories[0]\n        weight_history = run.weight_histories[0]\n\n        # get best weights                \n        win = np.argmin(cost_history)\n        w = weight_history[win]        \n        \n        model = lambda b: run.model(normalizer(b),w)\n        z = model(h)\n        z = np.sign(z)\n\n        # reshape it\n        s.shape = (np.size(r1),np.size(r2))\n        t.shape = (np.size(r1),np.size(r2))     \n        z.shape = (np.size(r1),np.size(r2))\n\n        #### plot contour, color regions ####\n        ax.contour(s,t,z,colors='k', linewidths=2.5,levels = [0],zorder = 2)\n        ax.contourf(s,t,z,colors = [self.colors[1],self.colors[0]],alpha = 0.15,levels = range(-1,2))\n                \n        ### cleanup left plots, create max view ranges ###\n        ax.set_xlim([xmin1,xmax1])\n        ax.set_ylim([xmin2,xmax2])\n        ax.set_title(str(num_units) + ' units fit to data',fontsize = 14)\n    \n    def draw_boosting_fit(self,ax,steps,ind):\n        # viewing ranges\n        xmin1 = min(copy.deepcopy(self.x[0,:]))\n        xmax1 = max(copy.deepcopy(self.x[0,:]))\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        xmin2 = min(copy.deepcopy(self.x[1,:]))\n        xmax2 = max(copy.deepcopy(self.x[1,:]))\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2\n\n        ymin = min(copy.deepcopy(self.y))\n        ymax = max(copy.deepcopy(self.y))\n        ygap = (ymax - ymin)*0.05\n        ymin -= ygap\n        ymax += ygap\n\n        # plot boundary for 2d plot\n        r1 = np.linspace(xmin1,xmax1,30)\n        r2 = np.linspace(xmin2,xmax2,30)\n        s,t = np.meshgrid(r1,r2)\n        s = np.reshape(s,(np.size(s),1))\n        t = np.reshape(t,(np.size(t),1))\n        h = np.concatenate((s,t),axis = 1).T\n\n        model = lambda x: np.sum([v(x) for v in steps],axis=0)\n        z = model(h)\n        z = np.sign(z)\n\n        # reshape it\n        s.shape = (np.size(r1),np.size(r2))\n        t.shape = (np.size(r1),np.size(r2))     \n        z.shape = (np.size(r1),np.size(r2))\n\n        #### plot contour, color regions ####\n        ax.contour(s,t,z,colors='k', linewidths=2.5,levels = [0],zorder = 2)\n        ax.contourf(s,t,z,colors = [self.colors[1],self.colors[0]],alpha = 0.15,levels = range(-1,2))\n\n        ### cleanup left plots, create max view ranges ###\n        ax.set_xlim([xmin1,xmax1])\n        ax.set_ylim([xmin2,xmax2])\n        ax.set_title(str(ind+1) + ' units fit to data',fontsize = 14)"""
mlrefined_libraries/nonlinear_superlearn_library/nonlinear_classification_demos.py,43,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\nfrom inspect import signature\nfrom matplotlib.ticker import FormatStrFormatter\n\nclass Visualizer:\n    \'\'\'\n    Visualize linear regression in 2 and 3 dimensions.  For single input cases (2 dimensions) the path of gradient descent on the cost function can be animated.\n    \'\'\'\n    #### initialize ####\n    def __init__(self,data):\n        # grab input\n        data = data.T\n        self.x = data[:,:-1].T\n        self.y = data[:,-1:] \n \n    ###### plot plotting functions ######\n    def plot_data(self):\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(9,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\') \n        ax = plt.subplot(gs[1]); \n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n        \n        # plot 2d points\n        xmin,xmax,ymin,ymax = self.scatter_pts_2d(self.x,ax)\n\n        # clean up panel\n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n\n        # label axes\n        ax.set_xlabel(r\'$x$\', fontsize = 16)\n        ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 15)\n            \n    # plot regression fits\n    def plot_fit(self,w,model,**kwargs):        \n        # construct figure\n        fig = plt.figure(figsize=(9,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,1]) \n        ax = plt.subplot(gs[0]); ax.axis(\'off\')\n        ax2 = plt.subplot(gs[2]); ax2.axis(\'off\')\n        ax1 = plt.subplot(gs[1]);\n        \n        view = [20,20]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n\n        ##### plot left panel in original space ####\n        # scatter points\n        xmin,xmax,ymin,ymax = self.scatter_pts_2d(self.x,ax1)\n\n        # clean up panel\n        ax1.set_xlim([xmin,xmax])\n        ax1.set_ylim([ymin,ymax])\n\n        # label axes\n        ax1.set_xlabel(r\'$x$\', fontsize = 16)\n        ax1.set_ylabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 10)\n        \n        # create fit\n        s = np.linspace(xmin,xmax,300)[np.newaxis,:]\n \n        normalizer = lambda a: a\n        if \'normalizer\' in kwargs:\n            normalizer = kwargs[\'normalizer\']\n\n        t = np.tanh(model(normalizer(s),w))\n        ax1.plot(s.flatten(),t.flatten(),linewidth = 2,c = \'lime\')    \n         \n    # plot regression fits\n    def plot_fit_and_feature_space(self,w,model,feat,**kwargs):        \n        # construct figure\n        fig = plt.figure(figsize=(9,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0]);\n        ax2 = plt.subplot(gs[1]); \n        \n        view = [20,20]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n\n        ##### plot left panel in original space ####\n        # scatter points\n        xmin,xmax,ymin,ymax = self.scatter_pts_2d(self.x,ax1)\n\n        # clean up panel\n        ax1.set_xlim([xmin,xmax])\n        ax1.set_ylim([ymin,ymax])\n\n        # label axes\n        ax1.set_xlabel(r\'$x$\', fontsize = 16)\n        ax1.set_ylabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 10)\n        \n        # create fit\n        s = np.linspace(xmin,xmax,300)[np.newaxis,:]\n \n        normalizer = lambda a: a\n        if \'normalizer\' in kwargs:\n            normalizer = kwargs[\'normalizer\']\n\n        t = np.tanh(model(normalizer(s),w))\n        ax1.plot(s.flatten(),t.flatten(),linewidth = 2,c = \'lime\')    \n        \n        #### plot fit in transformed feature space #####\n        # check if feature transform has internal parameters\n        x_transformed = 0\n        sig = signature(feat)\n        if len(sig.parameters) == 2:\n            if np.shape(w)[1] == 1:\n                x_transformed = feat(normalizer(self.x),w)\n            else:\n                x_transformed = feat(normalizer(self.x),w[0])\n        else: \n            x_transformed = feat(normalizer(self.x))\n        \n        # two dimensional transformed feature space\n        if x_transformed.shape[0] == 1:\n            s = np.linspace(xmin,xmax,300)[np.newaxis,:]\n            \n            # scatter points\n            xmin,xmax,ymin,ymax = self.scatter_pts_2d(x_transformed,ax2)\n        \n            # produce plot\n            s2 = copy.deepcopy(s)\n            if len(sig.parameters) == 2:\n                if np.shape(w)[1] == 1:\n                    s2 = feat(normalizer(s),w)\n                else:\n                    s2 = feat(normalizer(s),w[0])\n            else: \n                s2 = feat(normalizer(s))\n            t = np.tanh(model(normalizer(s),w))\n            ax2.plot(s2.flatten(),t.flatten(),linewidth = 2,c = \'lime\')    \n            \n            # label axes\n            ax2.set_xlabel(r\'$f\\left(x,\\mathbf{w}^{\\star}\\right)$\', fontsize = 16)\n            ax2.set_ylabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 10)\n            \n        # three dimensional transformed feature space\n        if x_transformed.shape[0] == 2:\n            # create panel\n            ax2 = plt.subplot(gs[1],projection = \'3d\');  \n            s = np.linspace(xmin,xmax,100)[np.newaxis,:]\n\n            # plot data in 3d\n            xmin,xmax,xmin1,xmax1,ymin,ymax = self.scatter_3d_points(x_transformed,ax2)\n\n            # create and plot fit\n            s2 = copy.deepcopy(s)\n            if len(sig.parameters) == 2:\n                s2 = feat(normalizer(s),w[0])\n            else: \n                s2 = feat(normalizer(s))\n \n            # reshape for plotting\n            a = s2[0,:]\n            b = s2[1,:]\n            a = np.linspace(xmin,xmax,100)\n            b = np.linspace(xmin1,xmax1,100)\n            a,b = np.meshgrid(a,b)\n            \n            # get firstem\n            a.shape = (1,np.size(s)**2)\n            f1 = feat(normalizer(a))[0,:]\n            \n            # secondm\n            b.shape = (1,np.size(s)**2)\n            f2 = feat(normalizer(b))[1,:]\n            \n            # tack a 1 onto the top of each input point all at once\n            c = np.vstack((a,b))\n            o = np.ones((1,np.shape(c)[1]))\n            c = np.vstack((o,c))\n            r = np.tanh(np.dot(c.T,w))\n            \n            # various\n            a.shape = (np.size(s),np.size(s))\n            b.shape = (np.size(s),np.size(s))\n            r.shape = (np.size(s),np.size(s))\n            ax2.plot_surface(a,b,r,alpha = 0.1,color = \'lime\',rstride=15, cstride=15,linewidth=0.5,edgecolor = \'k\')\n            ax2.set_xlim([np.min(a),np.max(a)])\n            ax2.set_ylim([np.min(b),np.max(b)])\n            \n            \'\'\'\n            a,b = np.meshgrid(t1,t2)\n            a.shape = (1,np.size(s)**2)\n            b.shape = (1,np.size(s)**2)\n            \'\'\'\n \n            \'\'\'\n            c = np.vstack((a,b))\n            o = np.ones((1,np.shape(c)[1]))\n            c = np.vstack((o,c))\n\n            # tack a 1 onto the top of each input point all at once\n            r = (np.dot(c.T,w))\n\n            a.shape = (np.size(s),np.size(s))\n            b.shape = (np.size(s),np.size(s))\n            r.shape = (np.size(s),np.size(s))\n            ax2.plot_surface(a,b,r,alpha = 0.1,color = \'lime\',rstride=15, cstride=15,linewidth=0.5,edgecolor = \'k\')\n            \'\'\'\n            \n            # label axes\n            #self.move_axis_left(ax2)\n            ax2.set_xlabel(r\'$f_1(x)$\', fontsize = 12,labelpad = 5)\n            ax2.set_ylabel(r\'$f_2(x)$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax2.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = 0)\n            self.move_axis_left(ax2)\n            ax2.xaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            ax2.yaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            ax2.view_init(view[0],view[1])\n \n    def scatter_pts_2d(self,x,ax):\n        # set plotting limits\n        xmax = copy.deepcopy(np.max(x))\n        xmin = copy.deepcopy(np.min(x))\n        xgap = (xmax - xmin)*0.2\n        xmin -= xgap\n        xmax += xgap\n\n        ymax = copy.deepcopy(np.max(self.y))\n        ymin = copy.deepcopy(np.min(self.y))\n        ygap = (ymax - ymin)*0.2\n        ymin -= ygap\n        ymax += ygap    \n\n        # initialize points\n        ax.scatter(x.flatten(),self.y.flatten(),color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n\n        # clean up panel\n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n        \n        return xmin,xmax,ymin,ymax\n    \n    ### visualize the surface plot of cost function ###\n    def scatter_3d_points(self,x,ax):\n        # set plotting limits\n        xmax = copy.deepcopy(np.max(x[0,:]))\n        xmin = copy.deepcopy(np.min(x[0,:]))\n        xgap = (xmax - xmin)*0.2\n        xmin -= xgap\n        xmax += xgap\n        \n        xmax1 = copy.deepcopy(np.max(x[1,:]))\n        xmin1 = copy.deepcopy(np.min(x[1,:]))\n        xgap1 = (xmax1 - xmin1)*0.2\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        ymax = copy.deepcopy(np.max(self.y))\n        ymin = copy.deepcopy(np.min(self.y))\n        ygap = (ymax - ymin)*0.2\n        ymin -= ygap\n        ymax += ygap   \n        \n        # plot data\n        ax.scatter(x[0,:].flatten(),x[1,:].flatten(),self.y.flatten(),color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n            \n        # clean up panel\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n      \n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        \n        \n        return xmin,xmax,xmin1,xmax1,ymin,ymax\n\n    # set axis in left panel\n    def move_axis_left(self,ax):\n        tmp_planes = ax.zaxis._PLANES \n        ax.zaxis._PLANES = ( tmp_planes[2], tmp_planes[3], \n                             tmp_planes[0], tmp_planes[1], \n                             tmp_planes[4], tmp_planes[5])\n        view_1 = (25, -135)\n        view_2 = (25, -45)\n        init_view = view_2\n        ax.view_init(*init_view) '"
mlrefined_libraries/nonlinear_superlearn_library/nonlinear_classification_visualizer.py,92,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nimport autograd.numpy as np\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\nfrom matplotlib.ticker import FormatStrFormatter\nfrom inspect import signature\n\nclass Visualizer:\n    \'\'\'\n    Visualize linear regression in 2 and 3 dimensions.  For single input cases (2 dimensions) the path of gradient descent on the cost function can be animated.\n    \'\'\'\n    #### initialize ####\n    def __init__(self,csvname):\n        # grab input\n        data = np.loadtxt(csvname,delimiter = \',\')\n        data = data.T\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n\n        self.colors = [\'cornflowerblue\',\'salmon\',\'lime\',\'bisque\',\'mediumaquamarine\',\'b\',\'m\',\'g\']\n    \n    ######## show N = 1 static image ########\n    # show coloring of entire space\n    def static_N1_img(self,w_best,cost,predict,**kwargs):\n        # or just take last weights\n        self.w = w_best\n        \n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        \n        show_cost = False\n        if show_cost == True:   \n            gs = gridspec.GridSpec(1, 3,width_ratios = [1,1,1],height_ratios = [1]) \n            \n            # create third panel for cost values\n            ax3 = plt.subplot(gs[2],aspect = \'equal\')\n            \n        else:\n            gs = gridspec.GridSpec(1, 2,width_ratios = [1,1]) \n\n        #### left plot - data and fit in original space ####\n        # setup current axis\n        ax = plt.subplot(gs[0]);\n        ax2 = plt.subplot(gs[1],aspect = \'equal\');\n        \n        # scatter original points\n        self.scatter_pts(ax,self.x)\n        ax.set_xlabel(r\'$x$\', fontsize = 14,labelpad = 10)\n        ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 14,labelpad = 10)\n        \n        # create fit\n        gapx = (max(self.x) - min(self.x))*0.1\n        s = np.linspace(min(self.x) - gapx,max(self.x) + gapx,100)\n        t = [np.tanh(predict(np.asarray([v]),self.w)) for v in s]\n        \n        # plot fit\n        ax.plot(s,t,c = \'lime\')\n        ax.axhline(linewidth=0.5, color=\'k\',zorder = 1)\n\n        #### plot data in new space in middle panel (or right panel if cost function decrease plot shown ) #####\n        if \'f_x\' in kwargs:\n            f_x = kwargs[\'f_x\']\n\n            # scatter points\n            self.scatter_pts(ax2,f_x)\n\n            # create and plot fit\n            s = np.linspace(min(f_x) - 0.1,max(f_x) + 0.1,100)\n            t = np.tanh(self.w[0] + self.w[1]*s)\n            ax2.plot(s,t,c = \'lime\')\n            ax2.set_xlabel(r\'$f\\,(x)$\', fontsize = 14,labelpad = 10)\n            ax2.set_ylabel(r\'$y$\', rotation = 0,fontsize = 14,labelpad = 10)\n        \n        if \'f2_x\' in kwargs:\n            ax2 = plt.subplot(gs[1],projection = \'3d\');   \n            view = kwargs[\'view\']\n            \n            # get input\n            f1_x = kwargs[\'f1_x\']\n            f2_x = kwargs[\'f2_x\']\n\n            # scatter points\n            f1_x = np.asarray(f1_x)\n            f1_x.shape = (len(f1_x),1)\n            f2_x = np.asarray(f2_x)\n            f2_x.shape = (len(f2_x),1)\n            xtran = np.concatenate((f1_x,f2_x),axis = 1)\n            self.scatter_pts(ax2,xtran)\n\n            # create and plot fit\n            s1 = np.linspace(min(f1_x) - 0.1,max(f1_x) + 0.1,100)\n            s2 = np.linspace(min(f2_x) - 0.1,max(f2_x) + 0.1,100)\n            t1,t2 = np.meshgrid(s1,s2)\n            \n            # compute fitting hyperplane\n            t1.shape = (len(s1)**2,1)\n            t2.shape = (len(s2)**2,1)\n            r = np.tanh(self.w[0] + self.w[1]*t1 + self.w[2]*t2)\n            \n            # reshape for plotting\n            t1.shape = (len(s1),len(s1))\n            t2.shape = (len(s2),len(s2))\n            r.shape = (len(s1),len(s2))\n            ax2.plot_surface(t1,t2,r,alpha = 0.1,color = \'lime\',rstride=10, cstride=10,linewidth=0.5,edgecolor = \'k\')\n                \n            # label axes\n            self.move_axis_left(ax2)\n            ax2.set_xlabel(r\'$f_1(x)$\', fontsize = 12,labelpad = 5)\n            ax2.set_ylabel(r\'$f_2(x)$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax2.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n            ax2.view_init(view[0],view[1])\n            \n        # plot cost function decrease\n        if  show_cost == True: \n            # compute cost eval history\n            g = cost\n            cost_evals = []\n            for i in range(len(w_hist)):\n                W = w_hist[i]\n                cost = g(W)\n                cost_evals.append(cost)\n     \n            # plot cost path - scale to fit inside same aspect as classification plots\n            num_iterations = len(w_hist)\n            minx = min(self.x)\n            maxx = max(self.x)\n            gapx = (maxx - minx)*0.1\n            minc = min(cost_evals)\n            maxc = max(cost_evals)\n            gapc = (maxc - minc)*0.1\n            minc -= gapc\n            maxc += gapc\n            \n            s = np.linspace(minx + gapx,maxx - gapx,num_iterations)\n            scaled_costs = [c/float(max(cost_evals))*(maxx-gapx) - (minx+gapx) for c in cost_evals]\n            ax3.plot(s,scaled_costs,color = \'k\',linewidth = 1.5)\n            ax3.set_xlabel(\'iteration\',fontsize = 12)\n            ax3.set_title(\'cost function plot\',fontsize = 12)\n            \n            # rescale number of iterations and cost function value to fit same aspect ratio as other two subplots\n            ax3.set_xlim(minx,maxx)\n            #ax3.set_ylim(minc,maxc)\n            \n            ### set tickmarks for both axes - requries re-scaling   \n            # x axis\n            marks = range(0,num_iterations,round(num_iterations/5.0))\n            ax3.set_xticks(s[marks])\n            labels = [item.get_text() for item in ax3.get_xticklabels()]\n            ax3.set_xticklabels(marks)\n            \n            ### y axis\n            r = (max(scaled_costs) - min(scaled_costs))/5.0\n            marks = [min(scaled_costs) + m*r for m in range(6)]\n            ax3.set_yticks(marks)\n            labels = [item.get_text() for item in ax3.get_yticklabels()]\n            \n            r = (max(cost_evals) - min(cost_evals))/5.0\n            marks = [int(min(cost_evals) + m*r) for m in range(6)]\n            ax3.set_yticklabels(marks)\n\n    ######## show N = 2 static image ########\n    # show coloring of entire space\n    def static_N2_img(self,w_best,runner,**kwargs):\n        cost = runner.cost_func\n        predict = runner.model\n        feat = runner.feature_transforms\n        normalizer = runner.normalizer\n                \n        # count parameter layers of input to feature transform\n        sig = signature(feat)\n        sig = len(sig.parameters)\n\n        # or just take last weights\n        self.w = w_best\n        \n        zplane = \'on\'\n        if \'zplane\' in kwargs:\n            zplane = kwargs[\'zplane\']\n        view1 = [20,45]\n        if \'view1\' in kwargs:\n            view1 = kwargs[\'view1\']\n        view2 = [20,30]\n        if \'view2\' in kwargs:\n            view2 = kwargs[\'view2\']  \n            \n        # initialize figure\n        fig = plt.figure(figsize = (10,9))\n        gs = gridspec.GridSpec(2, 2,width_ratios = [1,1]) \n\n        #### left plot - data and fit in original space ####\n        # setup current axis\n        ax = plt.subplot(gs[0],aspect = \'equal\');\n        ax2 = plt.subplot(gs[1],aspect = \'equal\');\n        ax3 = plt.subplot(gs[2],projection = \'3d\');\n        ax4 = plt.subplot(gs[3],projection = \'3d\');\n        \n        ### cleanup left plots, create max view ranges ###\n        xmin1 = np.min(self.x[:,0])\n        xmax1 = np.max(self.x[:,0])\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n        ax.set_xlim([xmin1,xmax1])\n        ax3.set_xlim([xmin1,xmax1])\n\n        xmin2 = np.min(self.x[:,1])\n        xmax2 = np.max(self.x[:,1])\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2\n        ax.set_ylim([xmin2,xmax2])\n        ax3.set_ylim([xmin2,xmax2])\n\n        ymin = np.min(self.y)\n        ymax = np.max(self.y)\n        ygap = (ymax - ymin)*0.05\n        ymin -= ygap\n        ymax += ygap\n        ax3.set_zlim([ymin,ymax])\n        \n        ax3.axis(\'off\')\n        ax3.view_init(view1[0],view1[1])\n\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_xlabel(r\'$x_1$\',fontsize = 15)\n        ax.set_ylabel(r\'$x_2$\',fontsize = 15,rotation = 0,labelpad = 20)\n            \n        #### plot left panels ####\n        # plot points in 2d and 3d\n        ind0 = np.argwhere(self.y == +1)\n        ax.scatter(self.x[ind0,0],self.x[ind0,1],s = 55, color = self.colors[0], edgecolor = \'k\')\n        ax3.scatter(self.x[ind0,0],self.x[ind0,1],self.y[ind0],s = 55, color = self.colors[0], edgecolor = \'k\')\n\n        ind1 = np.argwhere(self.y == -1)\n        ax.scatter(self.x[ind1,0],self.x[ind1,1],s = 55, color = self.colors[1], edgecolor = \'k\')\n        ax3.scatter(self.x[ind1,0],self.x[ind1,1],self.y[ind1],s = 55, color = self.colors[1], edgecolor = \'k\')\n       \n        # plot boundary for 2d plot\n        r1 = np.linspace(xmin1,xmax1,100)\n        r2 = np.linspace(xmin2,xmax2,100)\n        s,t = np.meshgrid(r1,r2)\n        s = np.reshape(s,(np.size(s),1))\n        t = np.reshape(t,(np.size(t),1))\n        h = np.concatenate((s,t),axis = 1)\n        z = predict(normalizer(h.T),self.w)\n        z = np.tanh(z)\n        \n        # reshape it\n        s.shape = (np.size(r1),np.size(r2))\n        t.shape = (np.size(r1),np.size(r2))     \n        z.shape = (np.size(r1),np.size(r2))\n        \n        #### plot contour, color regions ####\n        ax.contour(s,t,z,colors=\'k\', linewidths=2.5,levels = [0],zorder = 2)\n        ax.contourf(s,t,z,colors = [self.colors[1],self.colors[0]],alpha = 0.15,levels = range(-1,2))\n        ax3.plot_surface(s,t,z,alpha = 0.1,color = \'w\',rstride=10, cstride=10,linewidth=0.5,edgecolor = \'k\')\n\n        # plot zplane = 0 in left 3d panel - showing intersection of regressor with z = 0 (i.e., its contour, the separator, in the 3d plot too)?\n        if zplane == \'on\':\n            # plot zplane\n            ax3.plot_surface(s,t,z*0,alpha = 0.1,rstride=20, cstride=20,linewidth=0.15,color = \'w\',edgecolor = \'k\') \n            \n            # plot separator curve in left plot\n            ax3.contour(s,t,z,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n            ax3.contourf(s,t,z,colors = self.colors[1],levels = [0,1],zorder = 1,alpha = 0.1)\n            ax3.contourf(s,t,z+1,colors = self.colors[0],levels = [0,1],zorder = 1,alpha = 0.1)\n        \n        #### plot right panel scatter ####\n        # transform data\n        f = 0 \n        if sig == 1:\n            f = feat(normalizer(self.x.T)).T\n        else:\n            f = feat(normalizer(self.x.T),self.w[0]).T\n        x1 = f[:,0]\n        x2 = f[:,1]\n        #x1 = [f1(e) for e in self.x]\n        #x2 = [f2(e) for e in self.x]\n        ind0 = [v[0] for v in ind0]\n        ind1 = [v[0] for v in ind1]\n\n        # plot points on desired panel\n        v1 = [x1[e] for e in ind0]\n        v2 = [x2[e] for e in ind0]\n        ax2.scatter(v1,v2,s = 55, color = self.colors[0], edgecolor = \'k\')\n        ax4.scatter(v1,v2,self.y[ind0],s = 55, color = self.colors[0], edgecolor = \'k\')\n\n        v1 = [x1[e] for e in ind1]\n        v2 = [x2[e] for e in ind1]        \n        ax2.scatter(v1,v2,s = 55, color = self.colors[1], edgecolor = \'k\')\n        ax4.scatter(v1,v2,self.y[ind1],s = 55, color = self.colors[1], edgecolor = \'k\')\n        \n        ### cleanup right panels - making max viewing ranges ###\n        \n        xmin1 = np.min(x1)\n        xmax1 = np.max(x1)\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n        ax2.set_xlim([xmin1,xmax1])\n        ax4.set_xlim([xmin1,xmax1])\n\n        xmin2 = np.min(x2)\n        xmax2 = np.max(x2)\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2\n        \n        ax2.set_yticklabels([])\n        ax2.set_xticklabels([])\n        ax2.set_xticks([])\n        ax2.set_yticks([])\n        ax2.set_xlabel(r\'$f\\,_1\\left(\\mathbf{x}\\right)$\',fontsize = 15)\n        ax2.set_ylabel(r\'$f\\,_2\\left(\\mathbf{x}\\right)$\',fontsize = 15)        \n        \n        ### plot right panel 3d scatter ###\n        #### make right plot contour ####\n        r1 = np.linspace(xmin1,xmax1,100)\n        r2 = np.linspace(xmin2,xmax2,100)\n        s,t = np.meshgrid(r1,r2)\n        \n        s.shape = (1,len(r1)**2)\n        t.shape = (1,len(r2)**2)\n       # h = np.vstack((s,t))\n       # h = feat(normalizer(h))  \n       # s = h[0,:]\n       # t = h[1,:]\n        z = 0\n        if sig == 1:\n            z = self.w[0] + self.w[1]*s + self.w[2]*t\n        else:\n            z = self.w[1][0] + self.w[1][1]*s + self.w[1][2]*t\n        z = np.tanh(np.asarray(z))\n        \n        s.shape = (np.size(r1),np.size(r2))\n        t.shape = (np.size(r1),np.size(r2))\n        z.shape = (np.size(r1),np.size(r2))\n\n        ax2.contour(s,t,z,colors=\'k\', linewidths=2.5,levels = [0],zorder = 2)\n        ax2.contourf(s,t,z,colors = [self.colors[1],self.colors[0]],alpha = 0.15,levels = range(-1,2))\n\n        #### plot right surface plot ####\n        # plot regression surface\n        ax4.plot_surface(s,t,z,alpha = 0.1,color = \'w\',rstride=10, cstride=10,linewidth=0.5,edgecolor = \'k\')\n            \n        # plot zplane = 0 in left 3d panel - showing intersection of regressor with z = 0 (i.e., its contour, the separator, in the 3d plot too)?\n        if zplane == \'on\':\n            # plot zplane\n            ax4.plot_surface(s,t,z*0,alpha = 0.1,rstride=20, cstride=20,linewidth=0.15,color = \'w\',edgecolor = \'k\') \n            \n            # plot separator curve in left plot\n            ax4.contour(s,t,z,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n            ax4.contourf(s,t,z,colors = self.colors[0],levels = [0,1],zorder = 1,alpha = 0.1)\n            ax4.contourf(s,t,z+1,colors = self.colors[1],levels = [0,1],zorder = 1,alpha = 0.1)\n   \n        ax2.set_ylim([xmin2,xmax2])\n        ax4.set_ylim([xmin2,xmax2])\n\n        ax4.axis(\'off\')\n        ax4.view_init(view2[0],view2[1])\n        ax4.set_zlim([ymin,ymax])\n\n    ######## show N = 2 static image ########\n    # show coloring of entire space\n    def static_N2_simple(self,w_best,runner,**kwargs):\n        cost = runner.cost_func\n        predict = runner.model\n        feat = runner.feature_transforms\n        normalizer = runner.normalizer\n                \n        # count parameter layers of input to feature transform\n        sig = signature(feat)\n        sig = len(sig.parameters)\n\n        # or just take last weights\n        self.w = w_best\n\n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(9,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 2) \n        ax2 = plt.subplot(gs[1],aspect = \'equal\'); \n        ax1 = plt.subplot(gs[0],projection=\'3d\'); \n\n        # scatter points\n        self.scatter_pts(ax1,self.x)\n\n        ### from above\n        ax2.set_xlabel(r\'$x_1$\',fontsize = 15)\n        ax2.set_ylabel(r\'$x_2$\',fontsize = 15,rotation = 0,labelpad = 20)\n        ax2.xaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n        ax2.yaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n\n        # plot points in 2d and 3d\n        C = len(np.unique(self.y))\n        if C == 2:\n            ind0 = np.argwhere(self.y == +1)\n            ax2.scatter(self.x[ind0,0],self.x[ind0,1],s = 55, color = self.colors[0], edgecolor = \'k\')\n\n            ind1 = np.argwhere(self.y == -1)\n            ax2.scatter(self.x[ind1,0],self.x[ind1,1],s = 55, color = self.colors[1], edgecolor = \'k\')\n        else:\n            for c in range(C):\n                ind0 = np.argwhere(self.y == c)\n                ax2.scatter(self.x[ind0,0],self.x[ind0,1],s = 55, color = self.colors[c], edgecolor = \'k\')\n\n        self.move_axis_left(ax1)\n        ax1.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 5)\n        ax1.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n        ax1.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n\n        ### create surface and boundary plot ###\n        xmin1 = np.min(self.x[:,0])\n        xmax1 = np.max(self.x[:,0])\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        xmin2 = np.min(self.x[:,1])\n        xmax2 = np.max(self.x[:,1])\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2    \n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n            ax1.view_init(view[0],view[1])\n\n        # plot boundary for 2d plot\n        r1 = np.linspace(xmin1,xmax1,300)\n        r2 = np.linspace(xmin2,xmax2,300)\n        s,t = np.meshgrid(r1,r2)\n        s = np.reshape(s,(np.size(s),1))\n        t = np.reshape(t,(np.size(t),1))\n        h = np.concatenate((s,t),axis = 1)\n        z = predict(normalizer(h.T),self.w)\n        z = np.sign(z)\n        \n        # reshape it\n        s.shape = (np.size(r1),np.size(r2))\n        t.shape = (np.size(r1),np.size(r2))     \n        z.shape = (np.size(r1),np.size(r2))\n        \n        #### plot contour, color regions ####\n        ax2.contour(s,t,z,colors=\'k\', linewidths=2.5,levels = [0],zorder = 2)\n        ax2.contourf(s,t,z,colors = [self.colors[1],self.colors[0]],alpha = 0.15,levels = range(-1,2))\n        ax1.plot_surface(s,t,z,alpha = 0.1,color = \'w\',rstride=30, cstride=30,linewidth=0.5,edgecolor = \'k\')\n\n            \n    ###### plot plotting functions ######\n    def plot_data(self):\n        fig = 0\n        # plot data in two and one-d\n        if np.shape(self.x)[1] < 2:\n            # construct figure\n            fig, axs = plt.subplots(2,1, figsize=(4,4))\n            gs = gridspec.GridSpec(2,1,height_ratios = [6,1]) \n            ax1 = plt.subplot(gs[0],aspect = \'equal\');\n            ax2 = plt.subplot(gs[1],sharex = ax1); \n            \n            # set plotting limits\n            xmax = copy.deepcopy(max(self.x))\n            xmin = copy.deepcopy(min(self.x))\n            xgap = (xmax - xmin)*0.05\n            xmin -= xgap\n            xmax += xgap\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.05\n            ymin -= ygap\n            ymax += ygap    \n\n            ### plot in 2-d\n            ax1.scatter(self.x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n\n            # clean up panel\n            ax1.set_xlim([xmin,xmax])\n            ax1.set_ylim([ymin,ymax])\n            ax1.axhline(linewidth=0.5, color=\'k\',zorder = 1)\n            \n            ### plot in 1-d\n            ind0 = np.argwhere(self.y == +1)\n            ax2.scatter(self.x[ind0],np.zeros((len(self.x[ind0]))),s = 55, color = self.colors[1], edgecolor = \'k\',zorder = 3)\n\n            ind1 = np.argwhere(self.y == -1)\n            ax2.scatter(self.x[ind1],np.zeros((len(self.x[ind1]))),s = 55, color = self.colors[0], edgecolor = \'k\',zorder = 3)\n            ax2.set_yticks([0])\n            ax2.axhline(linewidth=0.5, color=\'k\',zorder = 1)\n        \n            ax2.xaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            ax2.yaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            \n            \n        if np.shape(self.x)[1] == 2:\n            \n            # initialize figure\n            fig = plt.figure(figsize = (9,3))\n            gs = gridspec.GridSpec(1, 2,width_ratios = [1,1]) \n            ax2 = plt.subplot(gs[1],aspect = \'equal\');\n            ax1 = plt.subplot(gs[0],projection = \'3d\');\n\n            # scatter points\n            self.scatter_pts(ax1,self.x)\n            \n            ### from above\n            ax2.set_xlabel(r\'$x_1$\',fontsize = 15)\n            ax2.set_ylabel(r\'$x_2$\',fontsize = 15,rotation = 0,labelpad = 20)\n            ax2.xaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            ax2.yaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            \n            # plot points in 2d and 3d\n            C = len(np.unique(self.y))\n            if C == 2:\n                ind0 = np.argwhere(self.y == +1)\n                ax2.scatter(self.x[ind0,0],self.x[ind0,1],s = 55, color = self.colors[1], edgecolor = \'k\')\n\n                ind1 = np.argwhere(self.y == -1)\n                ax2.scatter(self.x[ind1,0],self.x[ind1,1],s = 55, color = self.colors[0], edgecolor = \'k\')\n            else:\n                for c in range(C):\n                    ind0 = np.argwhere(self.y == c)\n                    ax2.scatter(self.x[ind0,0],self.x[ind0,1],s = 50, color = self.colors[c], edgecolor = \'k\',linewidth=2)\n                    \n                  \n        \n            self.move_axis_left(ax1)\n            ax1.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 5)\n            ax1.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax1.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n            view = [20,45]\n            ax1.view_init(view[0],view[1])\n            \n    # scatter points\n    def scatter_pts(self,ax,x):\n        if np.shape(x)[1] == 1:\n            # set plotting limits\n            xmax = copy.deepcopy(max(x))\n            xmin = copy.deepcopy(min(x))\n            xgap = (xmax - xmin)*0.2\n            xmin -= xgap\n            xmax += xgap\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n\n            # clean up panel\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            \n        if np.shape(x)[1] == 2:\n            # set plotting limits\n            xmax1 = copy.deepcopy(max(x[:,0]))\n            xmin1 = copy.deepcopy(min(x[:,0]))\n            xgap1 = (xmax1 - xmin1)*0.1\n            xmin1 -= xgap1\n            xmax1 += xgap1\n            \n            xmax2 = copy.deepcopy(max(x[:,1]))\n            xmin2 = copy.deepcopy(min(x[:,1]))\n            xgap2 = (xmax2 - xmin2)*0.1\n            xmin2 -= xgap2\n            xmax2 += xgap2\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(x[:,0],x[:,1],self.y.flatten(),s = 40,color = \'k\', edgecolor = \'w\',linewidth = 0.9)\n\n            # clean up panel\n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            ax.set_zlim([ymin,ymax])\n            \n            ax.set_xticks(np.arange(round(xmin1), round(xmax1)+1, 1.0))\n            ax.set_yticks(np.arange(round(xmin2), round(xmax2)+1, 1.0))\n            ax.set_zticks(np.arange(round(ymin), round(ymax)+1, 1.0))\n           \n            # clean up panel\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n           \n    # set axis in left panel\n    def move_axis_left(self,ax):\n        tmp_planes = ax.zaxis._PLANES \n        ax.zaxis._PLANES = ( tmp_planes[2], tmp_planes[3], \n                             tmp_planes[0], tmp_planes[1], \n                             tmp_planes[4], tmp_planes[5])\n        view_1 = (25, -135)\n        view_2 = (25, -45)\n        init_view = view_2\n        ax.view_init(*init_view) \n        \n        \n    # toy plot\n    def multiclass_plot(self,run,w,**kwargs):\n        model = run.model\n        normalizer = run.normalizer\n        \n        # grab args\n        view = [20,-70]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n \n        ### plot all input data ###\n        # generate input range for functions\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n\n        r = np.linspace(minx,maxx,600)\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        h = np.concatenate([w1_vals,w2_vals],axis = 1).T\n\n        g_vals = model(normalizer(h),w)\n        g_vals = np.asarray(g_vals)\n        g_vals = np.argmax(g_vals,axis = 0)\n\n        # vals for cost surface\n        w1_vals.shape = (len(r),len(r))\n        w2_vals.shape = (len(r),len(r))\n        g_vals.shape = (len(r),len(r))\n\n        # create figure to plot\n        fig = plt.figure(num=None, figsize=(12,4), dpi=80, facecolor=\'w\', edgecolor=\'k\')\n\n        ### create 3d plot in left panel\n        #ax1 = plt.subplot(121,projection = \'3d\')\n        ax2 = plt.subplot(132)\n\n        fig.subplots_adjust(left=0,right=1,bottom=0,top=1)   # remove whitespace around 3d figure\n\n        # scatter points in both panels\n        class_nums = np.unique(self.y)\n        C = len(class_nums)\n        for c in range(C):\n            ind = np.argwhere(self.y == class_nums[c])\n            ind = [v[0] for v in ind]\n            #ax1.scatter(self.x[ind,0],self.x[ind,1],self.y[ind],s = 80,color = self.colors[c],edgecolor = \'k\',linewidth = 1.5)\n            ax2.scatter(self.x[ind,0],self.x[ind,1],s = 110,color = self.colors[c],edgecolor = \'k\', linewidth = 2)\n            \n        # switch for 2class / multiclass view\n        if C == 2:\n            # plot regression surface\n            #ax1.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'k\',rstride=20, cstride=20,linewidth=0,edgecolor = \'k\') \n\n            # plot zplane = 0 in left 3d panel - showing intersection of regressor with z = 0 (i.e., its contour, the separator, in the 3d plot too)?\n            #ax1.plot_surface(w1_vals,w2_vals,g_vals*0,alpha = 0.1,rstride=20, cstride=20,linewidth=0.15,color = \'k\',edgecolor = \'k\') \n            \n            # plot separator in left plot z plane\n            #ax1.contour(w1_vals,w2_vals,g_vals,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n\n            # color parts of plane with correct colors\n            #ax1.contourf(w1_vals,w2_vals,g_vals+1,colors = self.colors[:],alpha = 0.1,levels = range(0,2))\n            #ax1.contourf(w1_vals,w2_vals,-g_vals+1,colors = self.colors[1:],alpha = 0.1,levels = range(0,2))\n    \n            # plot separator in right plot\n            ax2.contour(w1_vals,w2_vals,g_vals,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n\n            # adjust height of regressor to plot filled contours\n            ax2.contourf(w1_vals,w2_vals,g_vals+1,colors = self.colors[:],alpha = 0.1,levels = range(0,C+1))\n\n            ### clean up panels\n            # set viewing limits on vertical dimension for 3d plot           \n            minz = min(copy.deepcopy(self.y))\n            maxz = max(copy.deepcopy(self.y))\n\n            gapz = (maxz - minz)*0.1\n            minz -= gapz\n            maxz += gapz\n\n        \n        # multiclass view\n        else:   \n            \'\'\'\n            ax1.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'w\',rstride=45, cstride=45,linewidth=0.25,edgecolor = \'k\')\n\n            for c in range(C):\n                # plot separator curve in left plot z plane\n                ax1.contour(w1_vals,w2_vals,g_vals - c,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n\n                # color parts of plane with correct colors\n                ax1.contourf(w1_vals,w2_vals,g_vals - c +0.5,colors = self.colors[c],alpha = 0.4,levels = [0,1])\n             \n            \'\'\'\n            \n            # plot separator in right plot\n            ax2.contour(w1_vals,w2_vals,g_vals,colors = \'k\',levels = range(0,C+1),linewidths = 3,zorder = 1)\n            \n            # adjust height of regressor to plot filled contours\n            ax2.contourf(w1_vals,w2_vals,g_vals+0.5,colors = self.colors[:],alpha = 0.2,levels = range(0,C+1))\n\n            ### clean up panels\n            # set viewing limits on vertical dimension for 3d plot \n            minz = 0\n            maxz = max(copy.deepcopy(self.y))\n            gapz = (maxz - minz)*0.1\n            minz -= gapz\n            maxz += gapz\n            #ax1.set_zlim([minz,maxz])\n\n            #ax1.view_init(view[0],view[1]) \n\n        \'\'\'\n        # clean up panel\n        ax1.xaxis.pane.fill = False\n        ax1.yaxis.pane.fill = False\n        ax1.zaxis.pane.fill = False\n\n        ax1.xaxis.pane.set_edgecolor(\'white\')\n        ax1.yaxis.pane.set_edgecolor(\'white\')\n        ax1.zaxis.pane.set_edgecolor(\'white\')\n\n        ax1.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax1.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax1.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        self.move_axis_left(ax1)\n        ax1.set_xlabel(r\'$x_1$\', fontsize = 16,labelpad = 5)\n        ax1.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 16,labelpad = 5)\n        ax1.set_zlabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 5)\n        \'\'\'\n        \n        ax2.set_xlabel(r\'$x_1$\', fontsize = 18,labelpad = 10)\n        ax2.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 18,labelpad = 15)\n        \n        \n    # toy plot\n    def show_individual_classifiers(self,run,w,**kwargs):\n        model = run.model\n        normalizer = run.normalizer\n        feat = run.feature_transforms\n        \n        # grab args\n        view = [20,-70]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n \n        ### plot all input data ###\n        # generate input range for functions\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n\n        r = np.linspace(minx,maxx,600)\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        h = np.concatenate([w1_vals,w2_vals],axis = 1).T\n\n        g_vals = model(normalizer(h),w)\n        g_vals = np.asarray(g_vals)\n        g_new = copy.deepcopy(g_vals).T\n        g_vals = np.argmax(g_vals,axis = 0)\n\n        # vals for cost surface\n        w1_vals.shape = (len(r),len(r))\n        w2_vals.shape = (len(r),len(r))\n        g_vals.shape = (len(r),len(r))\n\n        # count points\n        class_nums = np.unique(self.y)\n        C = int(len(class_nums))\n        \n        fig = plt.figure(figsize = (10,7))\n        gs = gridspec.GridSpec(2, C) \n\n        #### left plot - data and fit in original space ####\n        # setup current axis\n        #ax1 = plt.subplot(gs[C],projection = \'3d\');\n        ax2 = plt.subplot(gs[C+1],aspect = \'equal\');\n        fig.subplots_adjust(left=0,right=1,bottom=0,top=1)   # remove whitespace around 3d figure\n\n        ##### plot top panels ####\n        for d in range(C):\n            # create panel\n            ax = plt.subplot(gs[d],aspect = \'equal\'); ax.axis(\'off\')\n                       \n            for c in range(C):\n                # plot points\n                ind = np.argwhere(self.y == class_nums[c])\n                ind = [v[0] for v in ind]\n                ax.scatter(self.x[ind,0],self.x[ind,1],s = 50,color = self.colors[c],edgecolor = \'k\', linewidth = 2)\n            \n            g_2 = np.sign(g_new[:,d])\n            g_2.shape = (len(r),len(r))\n\n            # plot separator curve \n            ax.contour(w1_vals,w2_vals,g_2+1,colors = \'k\',levels = [-1,1],linewidths = 4.5,zorder = 1,linestyle = \'-\')\n            ax.contour(w1_vals,w2_vals,g_2+1,colors = self.colors[d],levels = [-1,1],linewidths = 2.5,zorder = 1,linestyle = \'-\')\n                \n            ax.set_xlabel(r\'$x_1$\', fontsize = 18,labelpad = 10)\n            ax.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 18,labelpad = 15)\n        \n        ##### plot bottom panels ###\n        # scatter points in both bottom panels\n        for c in range(C):\n            ind = np.argwhere(self.y == class_nums[c])\n            ind = [v[0] for v in ind]\n            #ax1.scatter(self.x[ind,0],self.x[ind,1],self.y[ind],s = 50,color = self.colors[c],edgecolor = \'k\',linewidth = 1.5)\n            ax2.scatter(self.x[ind,0],self.x[ind,1],s = 50,color = self.colors[c],edgecolor = \'k\', linewidth = 2)\n      \n        #ax1.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'w\',rstride=45, cstride=45,linewidth=0.25,edgecolor = \'k\')\n\n        #for c in range(C):\n            # plot separator curve in left plot z plane\n            #ax1.contour(w1_vals,w2_vals,g_vals - c,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n\n            # color parts of plane with correct colors\n            #ax1.contourf(w1_vals,w2_vals,g_vals - c +0.5,colors = self.colors[c],alpha = 0.4,levels = [0,1])\n             \n        # plot separator in right plot\n        ax2.contour(w1_vals,w2_vals,g_vals,colors = \'k\',levels = range(0,C+1),linewidths = 3,zorder = 1)\n\n        # adjust height of regressor to plot filled contours\n        ax2.contourf(w1_vals,w2_vals,g_vals+0.5,colors = self.colors[:],alpha = 0.2,levels = range(0,C+1))\n\n        ### clean up panels\n        # set viewing limits on vertical dimension for 3d plot \n        minz = 0\n        maxz = max(copy.deepcopy(self.y))\n        gapz = (maxz - minz)*0.1\n        minz -= gapz\n        maxz += gapz\n        ax2.axis(\'off\')\n        \n        \'\'\'\n        ax1.set_zlim([minz,maxz])\n\n        ax1.view_init(view[0],view[1]) \n\n        # clean up panel\n        ax1.xaxis.pane.fill = False\n        ax1.yaxis.pane.fill = False\n        ax1.zaxis.pane.fill = False\n\n        ax1.xaxis.pane.set_edgecolor(\'white\')\n        ax1.yaxis.pane.set_edgecolor(\'white\')\n        ax1.zaxis.pane.set_edgecolor(\'white\')\n\n        ax1.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax1.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax1.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        self.move_axis_left(ax1)\n        ax1.set_xlabel(r\'$x_1$\', fontsize = 16,labelpad = 5)\n        ax1.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 16,labelpad = 5)\n        ax1.set_zlabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 5)\n        \'\'\'\n        \n       # ax2.set_xlabel(r\'$x_1$\', fontsize = 18,labelpad = 10)\n       # ax2.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 18,labelpad = 15)'"
mlrefined_libraries/nonlinear_superlearn_library/nonlinear_classification_visualizer_multiple_panels.py,51,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nimport autograd.numpy as np\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\nfrom matplotlib.ticker import FormatStrFormatter\nfrom inspect import signature\n\nclass Visualizer:\n    \'\'\'\n    Visualize linear regression in 2 and 3 dimensions.  For single input cases (2 dimensions) the path of gradient descent on the cost function can be animated.\n    \'\'\'\n    #### initialize ####\n    def __init__(self,csvname):\n        # grab input\n        data = np.loadtxt(csvname,delimiter = \',\')\n        data = data.T\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n\n        self.colors = [\'salmon\',\'cornflowerblue\',\'lime\',\'bisque\',\'mediumaquamarine\',\'b\',\'m\',\'g\']\n\n    ######## show N = 2 static image ########\n    # show coloring of entire space\n    def plot_three_fits(self,run1,run2,run3,**kwargs):\n        ## strip off model, normalizer, etc., ##\n        model1 = run1.model\n        model2 = run2.model\n        model3 = run3.model\n\n        normalizer1 = run1.normalizer\n        normalizer2 = run2.normalizer\n        normalizer3 = run3.normalizer\n\n        # get weights\n        cost_history1 = run1.cost_histories[0]\n        ind1 = np.argmin(cost_history1)\n        w1 = run1.weight_histories[0][ind1]\n        cost_history2 = run2.cost_histories[0]\n        ind2 = np.argmin(cost_history2)\n        w2 = run2.weight_histories[0][ind2]\n        cost_history3 = run3.cost_histories[0]\n        ind3 = np.argmin(cost_history3)\n        w3 = run3.weight_histories[0][ind3]\n\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(10,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3) \n        ax1 = plt.subplot(gs[0],aspect = \'equal\'); \n        ax2 = plt.subplot(gs[1],aspect = \'equal\'); \n        ax3 = plt.subplot(gs[2],aspect = \'equal\'); \n\n        # loop over axes\n        for ax in [ax1,ax2,ax3]:\n            ### from above\n            ax.set_xlabel(r\'$x_1$\',fontsize = 15)\n            ax.set_ylabel(r\'$x_2$\',fontsize = 15,rotation = 0,labelpad = 20)\n            ax.xaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            ax.yaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n\n            # plot points in 2d \n            ind0 = np.argwhere(self.y == +1)\n            ax.scatter(self.x[ind0,0],self.x[ind0,1],s = 55, color = self.colors[0], edgecolor = \'k\')\n\n            ind1 = np.argwhere(self.y == -1)\n            ax.scatter(self.x[ind1,0],self.x[ind1,1],s = 55, color = self.colors[1], edgecolor = \'k\')\n\n            ### create surface and boundary plot ###\n            xmin1 = np.min(self.x[:,0])\n            xmax1 = np.max(self.x[:,0])\n            xgap1 = (xmax1 - xmin1)*0.05\n            xmin1 -= xgap1\n            xmax1 += xgap1\n\n            xmin2 = np.min(self.x[:,1])\n            xmax2 = np.max(self.x[:,1])\n            xgap2 = (xmax2 - xmin2)*0.05\n            xmin2 -= xgap2\n            xmax2 += xgap2    \n\n            # plot boundary for 2d plot\n            r1 = np.linspace(xmin1,xmax1,300)\n            r2 = np.linspace(xmin2,xmax2,300)\n            s,t = np.meshgrid(r1,r2)\n            s = np.reshape(s,(np.size(s),1))\n            t = np.reshape(t,(np.size(t),1))\n            h = np.concatenate((s,t),axis = 1)\n            \n            # plot model\n            z = 0\n            if ax == ax1:\n                z = model1(normalizer1(h.T),w1)\n                ax.set_title(\'underfitting\',fontsize = 14)\n            if ax == ax2:\n                z = model2(normalizer2(h.T),w2)\n                ax.set_title(\'overfitting\',fontsize = 14)\n            if ax == ax3:\n                z = model3(normalizer3(h.T),w3)\n                ax.set_title(r\'""just right""\',fontsize = 14)\n            z = np.sign(z)\n\n            # reshape it\n            s.shape = (np.size(r1),np.size(r2))\n            t.shape = (np.size(r1),np.size(r2))     \n            z.shape = (np.size(r1),np.size(r2))\n\n            #### plot contour, color regions ####\n            ax.contour(s,t,z,colors=\'k\', linewidths=2.5,levels = [0],zorder = 2)\n            ax.contourf(s,t,z,colors = [self.colors[1],self.colors[0]],alpha = 0.15,levels = range(-1,2))\n            \n    ###### plot plotting functions ######\n    def plot_data(self):\n        fig = 0\n        # plot data in two and one-d\n        if np.shape(self.x)[1] < 2:\n            # construct figure\n            fig, axs = plt.subplots(2,1, figsize=(4,4))\n            gs = gridspec.GridSpec(2,1,height_ratios = [6,1]) \n            ax1 = plt.subplot(gs[0],aspect = \'equal\');\n            ax2 = plt.subplot(gs[1],sharex = ax1); \n            \n            # set plotting limits\n            xmax = copy.deepcopy(max(self.x))\n            xmin = copy.deepcopy(min(self.x))\n            xgap = (xmax - xmin)*0.2\n            xmin -= xgap\n            xmax += xgap\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.5\n            ymin -= ygap\n            ymax += ygap    \n\n            ### plot in 2-d\n            ax1.scatter(self.x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n\n            # clean up panel\n            ax1.set_xlim([xmin,xmax])\n            ax1.set_ylim([ymin,ymax])\n            ax1.axhline(linewidth=0.5, color=\'k\',zorder = 1)\n            \n            ### plot in 1-d\n            ind0 = np.argwhere(self.y == +1)\n            ax2.scatter(self.x[ind0],np.zeros((len(self.x[ind0]))),s = 55, color = self.colors[0], edgecolor = \'k\',zorder = 3)\n\n            ind1 = np.argwhere(self.y == -1)\n            ax2.scatter(self.x[ind1],np.zeros((len(self.x[ind1]))),s = 55, color = self.colors[1], edgecolor = \'k\',zorder = 3)\n            ax2.set_yticks([0])\n            ax2.axhline(linewidth=0.5, color=\'k\',zorder = 1)\n        \n            ax2.xaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            ax2.yaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            \n        if np.shape(self.x)[1] == 2:\n            # construct figure\n            fig, axs = plt.subplots(1, 2, figsize=(9,4))\n\n            # create subplot with 2 panels\n            gs = gridspec.GridSpec(1, 2) \n            ax2 = plt.subplot(gs[1],aspect = \'equal\'); \n            ax1 = plt.subplot(gs[0],projection=\'3d\'); \n\n            # scatter points\n            self.scatter_pts(ax1,self.x)\n            \n            ### from above\n            ax2.set_xlabel(r\'$x_1$\',fontsize = 15)\n            ax2.set_ylabel(r\'$x_2$\',fontsize = 15,rotation = 0,labelpad = 20)\n            ax2.xaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            ax2.yaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            \n            # plot points in 2d and 3d\n            C = len(np.unique(self.y))\n            if C == 2:\n                ind0 = np.argwhere(self.y == +1)\n                ax2.scatter(self.x[ind0,0],self.x[ind0,1],s = 55, color = self.colors[0], edgecolor = \'k\')\n\n                ind1 = np.argwhere(self.y == -1)\n                ax2.scatter(self.x[ind1,0],self.x[ind1,1],s = 55, color = self.colors[1], edgecolor = \'k\')\n            else:\n                for c in range(C):\n                    ind0 = np.argwhere(self.y == c)\n                    ax2.scatter(self.x[ind0,0],self.x[ind0,1],s = 55, color = self.colors[c], edgecolor = \'k\')\n                    \n        \n            self.move_axis_left(ax1)\n            ax1.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 5)\n            ax1.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax1.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n        \n    # scatter points\n    def scatter_pts(self,ax,x):\n        if np.shape(x)[1] == 1:\n            # set plotting limits\n            xmax = copy.deepcopy(max(x))\n            xmin = copy.deepcopy(min(x))\n            xgap = (xmax - xmin)*0.2\n            xmin -= xgap\n            xmax += xgap\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n\n            # clean up panel\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            \n        if np.shape(x)[1] == 2:\n            # set plotting limits\n            xmax1 = copy.deepcopy(max(x[:,0]))\n            xmin1 = copy.deepcopy(min(x[:,0]))\n            xgap1 = (xmax1 - xmin1)*0.1\n            xmin1 -= xgap1\n            xmax1 += xgap1\n            \n            xmax2 = copy.deepcopy(max(x[:,1]))\n            xmin2 = copy.deepcopy(min(x[:,1]))\n            xgap2 = (xmax2 - xmin2)*0.1\n            xmin2 -= xgap2\n            xmax2 += xgap2\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(x[:,0],x[:,1],self.y.flatten(),s = 40,color = \'k\', edgecolor = \'w\',linewidth = 0.9)\n\n            # clean up panel\n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            ax.set_zlim([ymin,ymax])\n            \n            ax.set_xticks(np.arange(round(xmin1), round(xmax1)+1, 1.0))\n            ax.set_yticks(np.arange(round(xmin2), round(xmax2)+1, 1.0))\n            ax.set_zticks(np.arange(round(ymin), round(ymax)+1, 1.0))\n           \n            # clean up panel\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n           \n    # set axis in left panel\n    def move_axis_left(self,ax):\n        tmp_planes = ax.zaxis._PLANES \n        ax.zaxis._PLANES = ( tmp_planes[2], tmp_planes[3], \n                             tmp_planes[0], tmp_planes[1], \n                             tmp_planes[4], tmp_planes[5])\n        view_1 = (25, -135)\n        view_2 = (25, -45)\n        init_view = view_2\n        ax.view_init(*init_view) \n        \n        \n    # toy plot\n    def multiclass_plot(self,run,w,**kwargs):\n        model = run.model\n        normalizer = run.normalizer\n        \n        # grab args\n        view = [20,-70]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n \n        ### plot all input data ###\n        # generate input range for functions\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n\n        r = np.linspace(minx,maxx,600)\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        h = np.concatenate([w1_vals,w2_vals],axis = 1).T\n\n        g_vals = model(normalizer(h),w)\n        g_vals = np.asarray(g_vals)\n        g_vals = np.argmax(g_vals,axis = 0)\n\n        # vals for cost surface\n        w1_vals.shape = (len(r),len(r))\n        w2_vals.shape = (len(r),len(r))\n        g_vals.shape = (len(r),len(r))\n\n        # create figure to plot\n        fig = plt.figure(num=None, figsize=(12,5), dpi=80, facecolor=\'w\', edgecolor=\'k\')\n\n        ### create 3d plot in left panel\n        ax1 = plt.subplot(121,projection = \'3d\')\n        ax2 = plt.subplot(122)\n\n        fig.subplots_adjust(left=0,right=1,bottom=0,top=1)   # remove whitespace around 3d figure\n\n        # scatter points in both panels\n        class_nums = np.unique(self.y)\n        C = len(class_nums)\n        for c in range(C):\n            ind = np.argwhere(self.y == class_nums[c])\n            ind = [v[0] for v in ind]\n            ax1.scatter(self.x[ind,0],self.x[ind,1],self.y[ind],s = 80,color = self.colors[c],edgecolor = \'k\',linewidth = 1.5)\n            ax2.scatter(self.x[ind,0],self.x[ind,1],s = 110,color = self.colors[c],edgecolor = \'k\', linewidth = 2)\n            \n        # switch for 2class / multiclass view\n        if C == 2:\n            # plot regression surface\n            ax1.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'k\',rstride=20, cstride=20,linewidth=0,edgecolor = \'k\') \n\n            # plot zplane = 0 in left 3d panel - showing intersection of regressor with z = 0 (i.e., its contour, the separator, in the 3d plot too)?\n            ax1.plot_surface(w1_vals,w2_vals,g_vals*0,alpha = 0.1,rstride=20, cstride=20,linewidth=0.15,color = \'k\',edgecolor = \'k\') \n            \n            # plot separator in left plot z plane\n            ax1.contour(w1_vals,w2_vals,g_vals,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n\n            # color parts of plane with correct colors\n            ax1.contourf(w1_vals,w2_vals,g_vals+1,colors = self.colors[:],alpha = 0.1,levels = range(0,2))\n            ax1.contourf(w1_vals,w2_vals,-g_vals+1,colors = self.colors[1:],alpha = 0.1,levels = range(0,2))\n    \n            # plot separator in right plot\n            ax2.contour(w1_vals,w2_vals,g_vals,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n\n            # adjust height of regressor to plot filled contours\n            ax2.contourf(w1_vals,w2_vals,g_vals+1,colors = self.colors[:],alpha = 0.1,levels = range(0,C+1))\n\n            ### clean up panels\n            # set viewing limits on vertical dimension for 3d plot           \n            minz = min(copy.deepcopy(self.y))\n            maxz = max(copy.deepcopy(self.y))\n\n            gapz = (maxz - minz)*0.1\n            minz -= gapz\n            maxz += gapz\n\n        # multiclass view\n        else:   \n            ax1.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'w\',rstride=45, cstride=45,linewidth=0.25,edgecolor = \'k\')\n\n            for c in range(C):\n                # plot separator curve in left plot z plane\n                ax1.contour(w1_vals,w2_vals,g_vals - c,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n\n                # color parts of plane with correct colors\n                ax1.contourf(w1_vals,w2_vals,g_vals - c +0.5,colors = self.colors[c],alpha = 0.4,levels = [0,1])\n             \n                \n            # plot separator in right plot\n            ax2.contour(w1_vals,w2_vals,g_vals,colors = \'k\',levels = range(0,C+1),linewidths = 3,zorder = 1)\n            \n            # adjust height of regressor to plot filled contours\n            ax2.contourf(w1_vals,w2_vals,g_vals+0.5,colors = self.colors[:],alpha = 0.2,levels = range(0,C+1))\n\n            ### clean up panels\n            # set viewing limits on vertical dimension for 3d plot \n            minz = 0\n            maxz = max(copy.deepcopy(self.y))\n            gapz = (maxz - minz)*0.1\n            minz -= gapz\n            maxz += gapz\n            ax1.set_zlim([minz,maxz])\n\n            ax1.view_init(view[0],view[1]) \n\n        # clean up panel\n        ax1.xaxis.pane.fill = False\n        ax1.yaxis.pane.fill = False\n        ax1.zaxis.pane.fill = False\n\n        ax1.xaxis.pane.set_edgecolor(\'white\')\n        ax1.yaxis.pane.set_edgecolor(\'white\')\n        ax1.zaxis.pane.set_edgecolor(\'white\')\n\n        ax1.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax1.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax1.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        self.move_axis_left(ax1)\n        ax1.set_xlabel(r\'$x_1$\', fontsize = 16,labelpad = 5)\n        ax1.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 16,labelpad = 5)\n        ax1.set_zlabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 5)\n\n        ax2.set_xlabel(r\'$x_1$\', fontsize = 18,labelpad = 10)\n        ax2.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 18,labelpad = 15)\n        \n        \n    # toy plot\n    def show_individual_classifiers(self,run,w,**kwargs):\n        model = run.model\n        normalizer = run.normalizer\n        feat = run.feature_transforms\n        \n        # grab args\n        view = [20,-70]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n \n        ### plot all input data ###\n        # generate input range for functions\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n\n        r = np.linspace(minx,maxx,600)\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        h = np.concatenate([w1_vals,w2_vals],axis = 1).T\n\n        g_vals = model(normalizer(h),w)\n        g_vals = np.asarray(g_vals)\n        g_new = copy.deepcopy(g_vals).T\n        g_vals = np.argmax(g_vals,axis = 0)\n\n        # vals for cost surface\n        w1_vals.shape = (len(r),len(r))\n        w2_vals.shape = (len(r),len(r))\n        g_vals.shape = (len(r),len(r))\n\n        # count points\n        class_nums = np.unique(self.y)\n        C = int(len(class_nums))\n        \n        fig = plt.figure(figsize = (10,7))\n        gs = gridspec.GridSpec(2, C) \n\n        #### left plot - data and fit in original space ####\n        # setup current axis\n        ax1 = plt.subplot(gs[C],projection = \'3d\');\n        ax2 = plt.subplot(gs[C+1],aspect = \'equal\');\n        fig.subplots_adjust(left=0,right=1,bottom=0,top=1)   # remove whitespace around 3d figure\n\n        ##### plot top panels ####\n        for d in range(C):\n            # create panel\n            ax = plt.subplot(gs[d],aspect = \'equal\');\n                       \n            for c in range(C):\n                # plot points\n                ind = np.argwhere(self.y == class_nums[c])\n                ind = [v[0] for v in ind]\n                ax.scatter(self.x[ind,0],self.x[ind,1],s = 50,color = self.colors[c],edgecolor = \'k\', linewidth = 2)\n            \n            g_2 = np.sign(g_new[:,d])\n            g_2.shape = (len(r),len(r))\n\n            # plot separator curve \n            ax.contour(w1_vals,w2_vals,g_2+1,colors = \'k\',levels = [-1,1],linewidths = 4.5,zorder = 1,linestyle = \'-\')\n            ax.contour(w1_vals,w2_vals,g_2+1,colors = self.colors[d],levels = [-1,1],linewidths = 2.5,zorder = 1,linestyle = \'-\')\n                \n            ax.set_xlabel(r\'$x_1$\', fontsize = 18,labelpad = 10)\n            ax.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 18,labelpad = 15)\n        \n        ##### plot bottom panels ###\n        # scatter points in both bottom panels\n        for c in range(C):\n            ind = np.argwhere(self.y == class_nums[c])\n            ind = [v[0] for v in ind]\n            ax1.scatter(self.x[ind,0],self.x[ind,1],self.y[ind],s = 50,color = self.colors[c],edgecolor = \'k\',linewidth = 1.5)\n            ax2.scatter(self.x[ind,0],self.x[ind,1],s = 50,color = self.colors[c],edgecolor = \'k\', linewidth = 2)\n      \n        ax1.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'w\',rstride=45, cstride=45,linewidth=0.25,edgecolor = \'k\')\n\n        for c in range(C):\n            # plot separator curve in left plot z plane\n            ax1.contour(w1_vals,w2_vals,g_vals - c,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n\n            # color parts of plane with correct colors\n            ax1.contourf(w1_vals,w2_vals,g_vals - c +0.5,colors = self.colors[c],alpha = 0.4,levels = [0,1])\n             \n        # plot separator in right plot\n        ax2.contour(w1_vals,w2_vals,g_vals,colors = \'k\',levels = range(0,C+1),linewidths = 3,zorder = 1)\n\n        # adjust height of regressor to plot filled contours\n        ax2.contourf(w1_vals,w2_vals,g_vals+0.5,colors = self.colors[:],alpha = 0.2,levels = range(0,C+1))\n\n        ### clean up panels\n        # set viewing limits on vertical dimension for 3d plot \n        minz = 0\n        maxz = max(copy.deepcopy(self.y))\n        gapz = (maxz - minz)*0.1\n        minz -= gapz\n        maxz += gapz\n        ax1.set_zlim([minz,maxz])\n\n        ax1.view_init(view[0],view[1]) \n\n        # clean up panel\n        ax1.xaxis.pane.fill = False\n        ax1.yaxis.pane.fill = False\n        ax1.zaxis.pane.fill = False\n\n        ax1.xaxis.pane.set_edgecolor(\'white\')\n        ax1.yaxis.pane.set_edgecolor(\'white\')\n        ax1.zaxis.pane.set_edgecolor(\'white\')\n\n        ax1.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax1.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax1.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        self.move_axis_left(ax1)\n        ax1.set_xlabel(r\'$x_1$\', fontsize = 16,labelpad = 5)\n        ax1.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 16,labelpad = 5)\n        ax1.set_zlabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 5)\n\n        ax2.set_xlabel(r\'$x_1$\', fontsize = 18,labelpad = 10)\n        ax2.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 18,labelpad = 15)'"
mlrefined_libraries/nonlinear_superlearn_library/nonlinear_regression_demos.py,40,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\nfrom inspect import signature\nfrom matplotlib.ticker import FormatStrFormatter\n\nclass Visualizer:\n    \'\'\'\n    Visualize linear regression in 2 and 3 dimensions.  For single input cases (2 dimensions) the path of gradient descent on the cost function can be animated.\n    \'\'\'\n    #### initialize ####\n    def __init__(self,data):\n        # grab input\n        data = data.T\n        self.x = data[:,:-1].T\n        self.y = data[:,-1:] \n \n    ###### plot plotting functions ######\n    def plot_data(self):\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(9,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\') \n        ax = plt.subplot(gs[1]); \n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n        \n        # plot 2d points\n        xmin,xmax,ymin,ymax = self.scatter_pts_2d(self.x,ax)\n\n        # clean up panel\n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n\n        # label axes\n        ax.set_xlabel(r\'$x$\', fontsize = 16)\n        ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 15)\n            \n    # plot regression fits\n    def plot_fit(self,w,model,**kwargs):        \n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(9,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\') \n        ax = plt.subplot(gs[1]); \n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n  \n        # scatter points\n        xmin,xmax,ymin,ymax = self.scatter_pts_2d(self.x,ax)\n\n        # clean up panel\n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n\n        # label axes\n        ax.set_xlabel(r\'$x$\', fontsize = 16)\n        ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 15)\n        \n        # create fit\n        s = np.linspace(xmin,xmax,300)[np.newaxis,:]\n        colors = [\'k\',\'magenta\']\n        if \'colors\' in kwargs:\n            colors = kwargs[\'colors\']\n        c = 0\n \n        normalizer = lambda a: a\n        if \'normalizer\' in kwargs:\n            normalizer = kwargs[\'normalizer\']\n\n        t = model(normalizer(s),w)\n        ax.plot(s.T,t.T,linewidth = 4,c = \'k\',zorder = 0)\n        ax.plot(s.T,t.T,linewidth = 2,c = \'lime\',zorder = 0)\n         \n    # plot regression fits\n    def plot_fit_and_feature_space(self,w,model,feat,**kwargs):        \n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(9,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0]);\n        ax2 = plt.subplot(gs[1]); \n        \n        view = [20,20]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n\n        ##### plot left panel in original space ####\n        # scatter points\n        xmin,xmax,ymin,ymax = self.scatter_pts_2d(self.x,ax1)\n\n        # clean up panel\n        ax1.set_xlim([xmin,xmax])\n        ax1.set_ylim([ymin,ymax])\n\n        # label axes\n        ax1.set_xlabel(r\'$x$\', fontsize = 16)\n        ax1.set_ylabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 10)\n        \n        # create fit\n        s = np.linspace(xmin,xmax,300)[np.newaxis,:]\n \n        normalizer = lambda a: a\n        if \'normalizer\' in kwargs:\n            normalizer = kwargs[\'normalizer\']\n\n        t = model(normalizer(s),w)\n        \n        ax1.plot(s.flatten(),t.flatten(),linewidth = 4,c = \'k\',zorder = 0)    \n        ax1.plot(s.flatten(),t.flatten(),linewidth = 2,c = \'lime\',zorder = 0)\n\n        #### plot fit in transformed feature space #####\n        # check if feature transform has internal parameters\n        x_transformed = 0\n        sig = signature(feat)\n        if len(sig.parameters) == 2:\n            if np.shape(w)[1] == 1:\n                x_transformed = feat(normalizer(self.x),w)\n            else:\n                x_transformed = feat(normalizer(self.x),w[0])\n        else: \n            x_transformed = feat(normalizer(self.x))\n        \n        # two dimensional transformed feature space\n        if x_transformed.shape[0] == 1:\n            s = np.linspace(xmin,xmax,300)[np.newaxis,:]\n            \n            # scatter points\n            xmin,xmax,ymin,ymax = self.scatter_pts_2d(x_transformed,ax2)\n        \n            # produce plot\n            s2 = copy.deepcopy(s)\n            if len(sig.parameters) == 2:\n                if np.shape(w)[1] == 1:\n                    s2 = feat(normalizer(s),w)\n                else:\n                    s2 = feat(normalizer(s),w[0])\n            else: \n                s2 = feat(normalizer(s))\n            t = model(normalizer(s),w)\n            \n            ax2.plot(s2.flatten(),t.flatten(),linewidth = 4,c = \'k\',zorder = 0)    \n            ax2.plot(s2.flatten(),t.flatten(),linewidth = 2,c = \'lime\',zorder = 0)\n            \n            # label axes\n            ax2.set_xlabel(r\'$f\\left(x,\\mathbf{w}^{\\star}\\right)$\', fontsize = 16)\n            ax2.set_ylabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 10)\n            \n        # three dimensional transformed feature space\n        if x_transformed.shape[0] == 2:\n            # create panel\n            ax2 = plt.subplot(gs[1],projection = \'3d\');  \n            s = np.linspace(xmin,xmax,100)[np.newaxis,:]\n\n            # plot data in 3d\n            xmin,xmax,xmin1,xmax1,ymin,ymax = self.scatter_3d_points(x_transformed,ax2)\n\n            # create and plot fit\n            s2 = copy.deepcopy(s)\n            if len(sig.parameters) == 2:\n                s2 = feat(normalizer(s),w[0])\n            else: \n                s2 = feat(normalizer(s))\n \n            # reshape for plotting\n            a = s2[0,:]\n            b = s2[1,:]\n            a = np.linspace(xmin,xmax,100)\n            b = np.linspace(xmin1,xmax1,100)\n            a,b = np.meshgrid(a,b)\n            \n            # get firstem\n            a.shape = (1,np.size(s)**2)\n            f1 = feat(normalizer(a))[0,:]\n            \n            # secondm\n            b.shape = (1,np.size(s)**2)\n            f2 = feat(normalizer(b))[1,:]\n            \n            # tack a 1 onto the top of each input point all at once\n            c = np.vstack((a,b))\n            o = np.ones((1,np.shape(c)[1]))\n            c = np.vstack((o,c))\n            r = (np.dot(c.T,w))\n            \n            # various\n            a.shape = (np.size(s),np.size(s))\n            b.shape = (np.size(s),np.size(s))\n            r.shape = (np.size(s),np.size(s))\n            ax2.plot_surface(a,b,r,alpha = 0.1,color = \'lime\',rstride=15, cstride=15,linewidth=0.5,edgecolor = \'k\')\n            ax2.set_xlim([np.min(a),np.max(a)])\n            ax2.set_ylim([np.min(b),np.max(b)])\n            \n            \'\'\'\n            a,b = np.meshgrid(t1,t2)\n            a.shape = (1,np.size(s)**2)\n            b.shape = (1,np.size(s)**2)\n            \'\'\'\n \n            \'\'\'\n            c = np.vstack((a,b))\n            o = np.ones((1,np.shape(c)[1]))\n            c = np.vstack((o,c))\n\n            # tack a 1 onto the top of each input point all at once\n            r = (np.dot(c.T,w))\n\n            a.shape = (np.size(s),np.size(s))\n            b.shape = (np.size(s),np.size(s))\n            r.shape = (np.size(s),np.size(s))\n            ax2.plot_surface(a,b,r,alpha = 0.1,color = \'lime\',rstride=15, cstride=15,linewidth=0.5,edgecolor = \'k\')\n            \'\'\'\n            \n            # label axes\n            #self.move_axis_left(ax2)\n            ax2.set_xlabel(r\'$f_1(x)$\', fontsize = 12,labelpad = 5)\n            ax2.set_ylabel(r\'$f_2(x)$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax2.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = 0)\n            self.move_axis_left(ax2)\n            ax2.xaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            ax2.yaxis.set_major_formatter(FormatStrFormatter(\'%.1f\'))\n            ax2.view_init(view[0],view[1])\n \n    def scatter_pts_2d(self,x,ax):\n        # set plotting limits\n        xmax = copy.deepcopy(np.max(x))\n        xmin = copy.deepcopy(np.min(x))\n        xgap = (xmax - xmin)*0.2\n        xmin -= xgap\n        xmax += xgap\n\n        ymax = copy.deepcopy(np.max(self.y))\n        ymin = copy.deepcopy(np.min(self.y))\n        ygap = (ymax - ymin)*0.2\n        ymin -= ygap\n        ymax += ygap    \n\n        # initialize points\n        ax.scatter(x.flatten(),self.y.flatten(),color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 60,zorder = 3)\n\n        # clean up panel\n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n        \n        return xmin,xmax,ymin,ymax\n    \n    ### visualize the surface plot of cost function ###\n    def scatter_3d_points(self,x,ax):\n        # set plotting limits\n        xmax = copy.deepcopy(np.max(x[0,:]))\n        xmin = copy.deepcopy(np.min(x[0,:]))\n        xgap = (xmax - xmin)*0.2\n        xmin -= xgap\n        xmax += xgap\n        \n        xmax1 = copy.deepcopy(np.max(x[1,:]))\n        xmin1 = copy.deepcopy(np.min(x[1,:]))\n        xgap1 = (xmax1 - xmin1)*0.2\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        ymax = copy.deepcopy(np.max(self.y))\n        ymin = copy.deepcopy(np.min(self.y))\n        ygap = (ymax - ymin)*0.2\n        ymin -= ygap\n        ymax += ygap   \n        \n        # plot data\n        ax.scatter(x[0,:].flatten(),x[1,:].flatten(),self.y.flatten(),color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n            \n        # clean up panel\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n      \n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        \n        \n        return xmin,xmax,xmin1,xmax1,ymin,ymax\n\n    # set axis in left panel\n    def move_axis_left(self,ax):\n        tmp_planes = ax.zaxis._PLANES \n        ax.zaxis._PLANES = ( tmp_planes[2], tmp_planes[3], \n                             tmp_planes[0], tmp_planes[1], \n                             tmp_planes[4], tmp_planes[5])\n        view_1 = (25, -135)\n        view_2 = (25, -45)\n        init_view = view_2\n        ax.view_init(*init_view) '"
mlrefined_libraries/nonlinear_superlearn_library/nonlinear_regression_demos_multiple_panels.py,8,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\nfrom inspect import signature\nfrom matplotlib.ticker import FormatStrFormatter\n\nclass Visualizer:\n    \'\'\'\n    Visualize linear regression in 2 and 3 dimensions.  For single input cases (2 dimensions) the path of gradient descent on the cost function can be animated.\n    \'\'\'\n    #### initialize ####\n    def __init__(self,data):\n        # grab input\n        data = data.T\n        self.x = data[:,:-1].T\n        self.y = data[:,-1:] \n \n    ###### plot plotting functions ######\n    def plot_data(self):\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(9,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\') \n        ax = plt.subplot(gs[1]); \n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n        \n        # plot 2d points\n        xmin,xmax,ymin,ymax = self.scatter_pts_2d(self.x,ax)\n\n        # clean up panel\n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n\n        # label axes\n        ax.set_xlabel(r\'$x$\', fontsize = 16)\n        ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 15)\n            \n    # plot regression fits\n    def plot_three_fits(self,run1,run2,run3,**kwargs):   \n        ## strip off model, normalizer, etc., ##\n        model1 = run1.model\n        model2 = run2.model\n        model3 = run3.model\n        \n        all_colors = [[1,0,0.4], [ 0, 0.4, 1],[0, 1, 0.5],[1, 0.7, 0.5],[0.7, 0.6, 0.5],\'mediumaquamarine\']\n\n        normalizer1 = run1.normalizer\n        normalizer2 = run2.normalizer\n        normalizer3 = run3.normalizer\n\n        # get weights\n        cost_history1 = run1.cost_histories[0]\n        ind1 = np.argmin(cost_history1)\n        w1 = run1.weight_histories[0][ind1]\n        cost_history2 = run2.cost_histories[0]\n        ind2 = np.argmin(cost_history2)\n        w2 = run2.weight_histories[0][ind2]\n        cost_history3 = run3.cost_histories[0]\n        ind3 = np.argmin(cost_history3)\n        w3 = run3.weight_histories[0][ind3]\n        \n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(10,3))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,1,1]) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]); \n        ax3 = plt.subplot(gs[2]); \n\n        for ax in [ax1,ax2,ax3]:\n            # scatter points\n            xmin,xmax,ymin,ymax = self.scatter_pts_2d(self.x,ax)\n\n            # clean up panel\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n\n            # label axes\n            ax.set_xlabel(r\'$x$\', fontsize = 16)\n            ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 15)\n\n            # create fit\n            s = np.linspace(xmin,xmax,300)[np.newaxis,:]\n            colors = [\'k\',\'magenta\']\n            if \'colors\' in kwargs:\n                colors = kwargs[\'colors\']\n            c = 0\n\n            # plot model\n            t = 0\n            if ax == ax1:\n                t = model1(normalizer1(s),w1)\n                ax.set_title(\'underfitting\',fontsize = 14)\n            if ax == ax2:\n                t = model2(normalizer2(s),w2)\n                ax.set_title(\'overfitting\',fontsize = 14)\n            if ax == ax3:\n                t = model3(normalizer3(s),w3)\n                ax.set_title(\'""just right""\',fontsize = 14)\n\n            ax.plot(s.T,t.T,linewidth = 3,c = \'k\')\n            ax.plot(s.T,t.T,linewidth = 2.5,c = all_colors[2])\n \n    def scatter_pts_2d(self,x,ax):\n        # set plotting limits\n        xmax = copy.deepcopy(np.max(x))\n        xmin = copy.deepcopy(np.min(x))\n        xgap = (xmax - xmin)*0.2\n        xmin -= xgap\n        xmax += xgap\n\n        ymax = copy.deepcopy(np.max(self.y))\n        ymin = copy.deepcopy(np.min(self.y))\n        ygap = (ymax - ymin)*0.2\n        ymin -= ygap\n        ymax += ygap    \n\n        # initialize points\n        ax.scatter(x.flatten(),self.y.flatten(),color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 60)\n\n        # clean up panel\n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n        \n        return xmin,xmax,ymin,ymax'"
mlrefined_libraries/nonlinear_superlearn_library/regression_basis_comparison_2d.py,30,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\nfrom matplotlib import gridspec\nimport autograd.numpy as np\nimport copy\nimport time\nimport bisect\n\n\nclass Visualizer:\n    '''\n    Class for visualizing nonlinear regression fits to N = 1 dimensional input datasets\n    '''\n\n    # load target function\n    def load_data(self,csvname):\n        data = np.loadtxt(csvname,delimiter = ',').T\n        self.x = data[:,0]\n        self.y = data[:,1]\n        self.y.shape = (len(self.y),1)\n        \n    # initialize after animation call\n    def dial_settings(self):        \n        #### initialize split points for trees ####\n        # sort data by values of input\n        self.x_t = copy.deepcopy(self.x)\n        self.y_t = copy.deepcopy(self.y)\n        sorted_inds = np.argsort(self.x_t,axis = 0)\n        self.x_t = self.x_t[sorted_inds]\n        self.y_t = self.y_t[sorted_inds]\n\n        # create temp copy of data, sort from smallest to largest\n        splits = []\n        levels = []\n        residual = copy.deepcopy(self.y_t)\n\n        ## create simple 'weak learner' between each consecutive pair of points ##\n        for p in range(len(self.x_t) - 1):\n            # determine points on each side of split\n            split = (self.x_t[p] + self.x_t[p+1])/float(2)\n            splits.append(split)\n\n            # gather points to left and right of split\n            pts_left  = [t for t in self.x_t if t <= split]\n            resid_left = residual[:len(pts_left)]\n            resid_right = residual[len(pts_left):]\n\n            # compute average on each side\n            ave_left = np.mean(resid_left)\n            ave_right = np.mean(resid_right)\n            levels.append([ave_left,ave_right])\n                \n        # randomize splits for this experiment\n        self.splits = splits\n        self.levels = levels\n                \n        # generate features\n        self.F_tree = self.tree_feats()\n        \n    # least squares\n    def least_squares(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = self.x[p]\n            y_p = self.y[p]\n            cost +=(self.predict(x_p,w) - y_p)**2\n        return cost\n    \n    ##### transformation functions #####\n    # poly features\n    def poly_feats(self,D):\n        F = []\n        for deg in range(D+1):\n            F.append(self.x**deg)\n        F = np.asarray(F)\n        F.shape = (D+1,len(self.x))\n        return F.T\n    \n    # tanh features\n    def tanh_feats(self,D):\n        F = [np.ones((len(self.x)))]\n        for deg in range(D):\n            F.append(np.tanh(self.R[deg,0] + self.R[deg,1]*self.x))\n        F = np.asarray(F)\n        F.shape = (D+1,len(self.x))\n        return F.T\n    \n    # stump-tree feats\n    def tree_feats(self):\n        # feat matrix container\n        F = []\n\n        # loop over points and create feature vector based on stump for each\n        for pt in self.x:\n            f = [1]\n            for i in range(len(self.splits)):\n                # get current stump\n                split = self.splits[i]\n                level = self.levels[i]\n                \n                # check - which side of this split does the pt lie?\n                if pt <= split:  # lies to the left - so evaluate at left level\n                    f.append(level[0])\n                else:\n                    f.append(level[1])\n            \n            # save stump evaluations - this is our feature vector for pt\n            F.append(f)\n                    \n        # make numpy array\n        F = np.asarray(F)\n        return F\n\n    ##### prediction functions #####\n    # standard polys\n    def poly_predict(self,pt,w):\n        # linear combo\n        val = w[0] + sum([w[i]*pt**i for i in range(1,self.D+1)])\n        return val\n    \n    # single hidden layer tanh network with fixed random weights\n    def tanh_predict(self,pt,w):\n        # linear combo\n        val = w[0] + sum([w[i]*np.tanh(self.R[i-1,0] + self.R[i-1,1]*pt)  for i in range(1,self.D+1)])\n        return val\n    \n    # tree prediction\n    def tree_predict(self,pt,w): \n        # our return prediction\n        val = copy.deepcopy(w[0])\n        \n        # loop over current stumps and collect weighted evaluation\n        for i in range(len(self.splits)):\n            # get current stump\n            split = self.splits[i]\n            levels = self.levels[i]\n                \n            # check - which side of this split does the pt lie?\n            if pt <= split:  # lies to the left - so evaluate at left level\n                val += w[i+1]*levels[0]\n            else:\n                val += w[i+1]*levels[1]\n        return val\n\n    \n    ###### optimizer ######\n    def boosting(self,F,y,its):\n        '''\n        Alternating descent wrapper for general Least Squares function\n        '''\n        g = lambda w: np.linalg.norm(np.dot(F,w) - y)\n\n        # settings \n        tol = 10**(-8)                  # tolerance to between sweeps to stop (optional)\n        N = np.shape(F)[1]                        # length of weights\n        w = np.zeros((N,1))        # initialization\n        w_history = [copy.deepcopy(w)]              # record each weight for plotting\n\n        # outer loop - each is a sweep through every variable once\n        i = 0\n        g_change = np.inf; gval1 = g(w);\n        r = np.copy(y)\n        r.shape = (len(r),1)\n        for i in range(its):\n            # what value do we get?\n            vals = np.dot(F.T,r)\n\n            # determine best ind\n            n = np.argmax(np.abs(vals))\n            f_n = np.asarray(F[:,n])\n            num = sum([a*b for a,b in zip(f_n,r)])[0]\n            den = sum([a**2 for a in f_n])\n            w_n = num/den  \n            r = np.asarray([a - w_n*b for a,b in zip(r,f_n)])\n            w[n] += w_n\n\n            # record weights at each step for kicks\n            w_history.append(copy.deepcopy(w))\n\n            i+=1\n        return w_history\n    \n    \n    ###### fit and compare ######\n    def brows_fits(self,savepath,**kwargs):\n        self.colors = [[1,0,0.4], [ 0, 0.4, 1],[0, 1, 0.5],[1, 0.7, 0.5],[0.7, 0.6, 0.5],'mediumaquamarine']\n\n        # parse input args\n        num_elements = [1,10,len(self.y)]\n        if 'num_elements' in kwargs:\n            num_elements = kwargs['num_elements']\n        scatter = 'off'\n        if 'scatter' in kwargs:\n            scatter = kwargs['scatter']\n        \n        # set dials for tanh network and trees\n        num_elements = [v+1 for v in num_elements]\n        self.num_elements = max(num_elements)\n        self.dial_settings()\n\n        ### run through each feature type, boost, collect associated weight histories\n        # make full poly and tanh feature matrices\n        self.F_poly = self.poly_feats(self.num_elements)\n           \n        # random weights for tanh network, tanh transform \n        scale = 1\n        self.R = scale*np.random.randn(self.num_elements,2)\n        self.F_tanh = self.tanh_feats(self.num_elements)\n            \n        # collect poly and tanh weights over each desired level\n        poly_weight_history = []\n        tanh_weight_history = []\n        for element in num_elements:\n            # fit weights to data\n            w = np.linalg.lstsq(self.F_poly[:,:element], self.y)[0]\n            \n            # store weights\n            poly_weight_history.append(w)\n            \n            # fit weights to data\n            w = np.linalg.lstsq(self.F_tanh[:,:element], self.y)[0]\n            \n            # store weights\n            tanh_weight_history.append(w)          \n            \n        ### create tree weight history via boosting - then filter out for desired levels\n        stump_weight_history = self.boosting(self.F_tree,self.y,its = 3000)\n        \n        # compute number of non-zeros per weight in history\n        nonzs = [len(np.argwhere(w != 0)) for w in stump_weight_history]\n            \n        # find unique additions\n        huh = np.asarray([np.sign(abs(nonzs[p] - nonzs[p+1])) for p in range(len(nonzs)-1)])\n        inds = np.argwhere(huh == 1)\n        inds = [v[0] for v in inds]\n\n        # sift through, make sure to pick the best fit\n        new_inds = []\n        for j in range(len(inds)-1):\n            val = inds[j+1] - inds[j]\n            if val > 2:\n                new_inds.append(inds[j+1] - 1)\n            else:\n                new_inds.append(inds[j])\n        new_inds.append(inds[-1])\n        stump_weight_history = [stump_weight_history[ind] for ind  in new_inds]\n            \n        ### plot it\n        # construct figure\n        fig = plt.figure(figsize = (10,4))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,1, 1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis('off');\n        ax2 = plt.subplot(gs[1]); ax2.axis('off');\n        ax3 = plt.subplot(gs[2]); ax3.axis('off');\n\n        # set viewing range for all 3 panels\n        xmax = max(copy.deepcopy(self.x))\n        xmin = min(copy.deepcopy(self.x))\n        xgap = (xmax - xmin)*0.0\n        xmax += xgap\n        xmin -= xgap\n        ymax = max(copy.deepcopy(self.y))[0]\n        ymin = min(copy.deepcopy(self.y))[0]\n        ygap = (ymax - ymin)*0.4\n        ymax += ygap\n        ymin -= ygap\n        \n        # animate\n        print ('beginning animation rendering...')\n        num_frames = len(num_elements)\n        def animate(k):\n            # clear the panel\n            ax1.cla()\n            ax2.cla()\n            ax3.cla()\n\n            # print rendering update\n            if np.mod(k+1,5) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(len(num_elements)))\n            if k == len(num_elements) - 1:\n                print ('animation rendering complete!')\n                time.sleep(1)\n                clear_output()\n                \n            # loop over panels, produce plots\n            self.D = num_elements[k] \n            cs = 0\n            for ax in [ax1,ax2,ax3]:\n                # fit to data\n                F = 0\n                predict = 0\n                w = 0\n                if ax == ax1: \n                    w = poly_weight_history[k]\n                    self.D = len(w) - 1\n                    ax.set_title(str(self.D) + ' poly units',fontsize = 14)\n                    self.predict = self.poly_predict\n                                       \n                elif ax == ax2:\n                    w = tanh_weight_history[k]\n                    self.D = len(w) - 1\n                    ax.set_title(str(self.D) + ' tanh units',fontsize = 14)\n                    self.predict = self.tanh_predict\n                    \n                elif ax == ax3:\n                    item = min(len(self.y)-1, num_elements[k]-1,len(stump_weight_history)-1) \n                    w = stump_weight_history[item]\n                    ax.set_title(str(item) + ' tree units',fontsize = 14)\n                    self.predict = self.tree_predict\n\n                ####### plot all and dress panel ######\n                # produce learned predictor\n                s = np.linspace(xmin,xmax,400)\n                t = [self.predict(np.asarray([v]),w) for v in s]\n    \n                # plot approximation and data in panel\n                if scatter == 'off':\n                    ax.plot(self.x,self.y,c = 'k',linewidth = 2)\n                elif scatter == 'on':\n                    ax.scatter(self.x,self.y,c = 'k',edgecolor = 'w',s = 30,zorder = 1)\n\n                ax.plot(s,t,linewidth = 2.75,color = self.colors[cs],zorder = 3)\n                cs += 1\n                \n                # cleanup panel\n                ax.set_xlim([xmin,xmax])\n                ax.set_ylim([ymin,ymax])\n                ax.set_xlabel(r'$x$', fontsize = 14,labelpad = 10)\n                ax.set_ylabel(r'$y$', rotation = 0,fontsize = 14,labelpad = 10)\n                ax.set_xticks(np.arange(round(xmin), round(xmax)+1, 1.0))\n                ax.set_yticks(np.arange(round(ymin), round(ymax)+1, 1.0))\n\n            return artist,\n            \n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()\n \n\n """
mlrefined_libraries/nonlinear_superlearn_library/regression_basis_single.py,62,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\nfrom matplotlib import gridspec\nimport autograd.numpy as np\nimport copy\nimport time\nimport bisect\nfrom matplotlib.ticker import MaxNLocator\n\nclass Visualizer:\n    '''\n    Class for visualizing nonlinear regression fits to N = 1 dimensional input datasets\n    '''\n\n    # load target function\n    def load_data(self,csvname):\n        data = np.loadtxt(csvname,delimiter = ',').T\n        self.x = data[:,0]\n        self.y = data[:,1]\n        self.y.shape = (len(self.y),1)\n        \n    # initialize after animation call\n    def dial_settings(self):        \n        #### initialize split points for trees ####\n        # sort data by values of input\n        self.x_t = copy.deepcopy(self.x)\n        self.y_t = copy.deepcopy(self.y)\n        sorted_inds = np.argsort(self.x_t,axis = 0)\n        self.x_t = self.x_t[sorted_inds]\n        self.y_t = self.y_t[sorted_inds]\n\n        # create temp copy of data, sort from smallest to largest\n        splits = []\n        levels = []\n        residual = copy.deepcopy(self.y_t)\n\n        ## create simple 'weak learner' between each consecutive pair of points ##\n        for p in range(len(self.x_t) - 1):\n            # determine points on each side of split\n            split = (self.x_t[p] + self.x_t[p+1])/float(2)\n            splits.append(split)\n\n            # gather points to left and right of split\n            pts_left  = [t for t in self.x_t if t <= split]\n            resid_left = residual[:len(pts_left)]\n            resid_right = residual[len(pts_left):]\n\n            # compute average on each side\n            ave_left = np.mean(resid_left)\n            ave_right = np.mean(resid_right)\n            levels.append([ave_left,ave_right])\n                \n        # randomize splits for this experiment\n        self.splits = splits\n        self.levels = levels\n                \n        # generate features\n        self.F_tree = self.tree_feats()\n        \n    # least squares\n    def least_squares(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = self.x[p]\n            y_p = self.y[p]\n            cost +=(self.predict(x_p,w) - y_p)**2\n        return cost\n    \n    ##### transformation functions #####\n    # poly features\n    def poly_feats(self,D):\n        F = []\n        for deg in range(D+1):\n            F.append(self.x**deg)\n        F = np.asarray(F)\n        F.shape = (D+1,len(self.x))\n        return F.T\n    \n    # tanh features\n    def tanh_feats(self,D):\n        F = [np.ones((len(self.x)))]\n        for deg in range(D):\n            F.append(np.tanh(self.R[deg,0] + self.R[deg,1]*self.x))\n        F = np.asarray(F)\n        F.shape = (D+1,len(self.x))\n        return F.T\n    \n    # stump-tree feats\n    def tree_feats(self):\n        # feat matrix container\n        F = []\n\n        # loop over points and create feature vector based on stump for each\n        for pt in self.x:\n            f = [1]\n            for i in range(len(self.splits)):\n                # get current stump\n                split = self.splits[i]\n                level = self.levels[i]\n                \n                # check - which side of this split does the pt lie?\n                if pt <= split:  # lies to the left - so evaluate at left level\n                    f.append(level[0])\n                else:\n                    f.append(level[1])\n            \n            # save stump evaluations - this is our feature vector for pt\n            F.append(f)\n                    \n        # make numpy array\n        F = np.asarray(F)\n        return F\n\n    ##### prediction functions #####\n    # standard polys\n    def poly_predict(self,pt,w):\n        # linear combo\n        val = w[0] + sum([w[i]*pt**i for i in range(1,self.D+1)])\n        return val\n    \n    # single hidden layer tanh network with fixed random weights\n    def tanh_predict(self,pt,w):\n        # linear combo\n        val = w[0] + sum([w[i]*np.tanh(self.R[i-1,0] + self.R[i-1,1]*pt)  for i in range(1,self.D+1)])\n        return val\n    \n    # tree prediction\n    def tree_predict(self,pt,w): \n        # our return prediction\n        val = copy.deepcopy(w[0])\n        \n        # loop over current stumps and collect weighted evaluation\n        for i in range(len(self.splits)):\n            # get current stump\n            split = self.splits[i]\n            levels = self.levels[i]\n                \n            # check - which side of this split does the pt lie?\n            if pt <= split:  # lies to the left - so evaluate at left level\n                val += w[i+1]*levels[0]\n            else:\n                val += w[i+1]*levels[1]\n        return val\n\n    \n    ###### optimizer ######\n    def boosting(self,F,y,its):\n        '''\n        Alternating descent wrapper for general Least Squares function\n        '''\n        g = lambda w: np.linalg.norm(np.dot(F,w) - y)\n\n        # settings \n        tol = 10**(-8)                  # tolerance to between sweeps to stop (optional)\n        N = np.shape(F)[1]                        # length of weights\n        w = np.zeros((N,1))        # initialization\n        w_history = [copy.deepcopy(w)]              # record each weight for plotting\n\n        # outer loop - each is a sweep through every variable once\n        i = 0\n        g_change = np.inf; gval1 = g(w);\n        r = np.copy(y)\n        r.shape = (len(r),1)\n        for i in range(its):\n            # what value do we get?\n            vals = np.dot(F.T,r)\n\n            # determine best ind\n            n = np.argmax(np.abs(vals))\n            f_n = np.asarray(F[:,n])\n            num = sum([a*b for a,b in zip(f_n,r)])[0]\n            den = sum([a**2 for a in f_n])\n            w_n = num/den  \n            r = np.asarray([a - w_n*b for a,b in zip(r,f_n)])\n            w[n] += w_n\n\n            # record weights at each step for kicks\n            w_history.append(copy.deepcopy(w))\n\n            i+=1\n        return w_history\n    \n    ###### fit and compare ######\n    def browse_single_fit(self,savepath,**kwargs):\n        # parse input args\n        num_elements = [1,10,len(self.y)]\n        if 'num_units' in kwargs:\n            num_elements = kwargs['num_units']\n            \n        basis = kwargs['basis']\n       \n        self.colors = [[1,0,0.4], [ 0, 0.4, 1],[0, 1, 0.5],[1, 0.7, 0.5],[0.7, 0.6, 0.5],'mediumaquamarine']\n        \n        # construct figure\n        fig = plt.figure(figsize = (9,4))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 3, width_ratios=[2,1,0.25]) \n        ax1 = plt.subplot(gs[0]); ax1.axis('off');\n        ax2 = plt.subplot(gs[1]); ax2.axis('off');\n        ax3 = plt.subplot(gs[2]); ax3.axis('off');\n\n        # set dials for tanh network and trees\n        num_elements = [v+1 for v in num_elements]\n        self.num_elements = max(num_elements)\n\n        # choose basis type\n        self.F = []\n        weight_history = []\n        if basis == 'poly':\n            self.F = self.poly_feats(self.num_elements)\n            \n            # collect poly and tanh weights over each desired level\n            for element in num_elements:\n                # fit weights to data\n                w = np.linalg.lstsq(self.F[:,:element], self.y,rcond = 10**(-15))[0]\n\n                # store weights\n                weight_history.append(w)\n                \n            self.predict = self.poly_predict\n\n        if basis == 'tanh':\n            # random weights for tanh network, tanh transform \n            scale = 1\n            self.R = scale*np.random.randn(self.num_elements,2)\n            self.F = self.tanh_feats(self.num_elements)\n            \n            # collect poly and tanh weights over each desired level\n            for element in num_elements:\n                # fit weights to data\n                w = np.linalg.lstsq(self.F[:,:element], self.y)[0]\n\n                # store weights\n                weight_history.append(w)\n            \n            self.predict = self.tanh_predict\n\n        if basis == 'tree':\n            self.dial_settings()\n            self.F = self.F_tree\n            weight_history = self.boosting(self.F,self.y,its = 3000)\n\n            # compute number of non-zeros per weight in history\n            nonzs = [len(np.argwhere(w != 0)) for w in weight_history]\n\n            # find unique additions\n            huh = np.asarray([np.sign(abs(nonzs[p] - nonzs[p+1])) for p in range(len(nonzs)-1)])\n            inds = np.argwhere(huh == 1)\n            inds = [v[0] for v in inds]\n\n            # sift through, make sure to pick the best fit\n            new_inds = []\n            for j in range(len(inds)-1):\n                val = inds[j+1] - inds[j]\n                if val > 2:\n                    new_inds.append(inds[j+1] - 1)\n                else:\n                    new_inds.append(inds[j])\n            new_inds.append(inds[-1])\n            weight_history = [weight_history[ind] for ind  in new_inds]\n            weight_history = [weight_history[ind - 2] for ind in num_elements]\n            self.predict = self.tree_predict\n            \n            # generate three panels - one to show current basis element being fit\n            gs = gridspec.GridSpec(1, 3, width_ratios=[1,1,1]) \n            ax = plt.subplot(gs[0]); ax1.axis('off');\n            ax1 = plt.subplot(gs[1]); ax2.axis('off');\n            ax2 = plt.subplot(gs[2]); ax2.axis('off');\n\n        # compute cost eval history\n        cost_evals = []\n        for i in range(len(weight_history)):\n            item = copy.deepcopy(i)\n            if basis == 'tree':\n                item = min(len(self.y)-1, num_elements[i]-1,len(weight_history)-1) \n            w = weight_history[item]\n            self.D = len(w) - 1\n\n            cost = self.least_squares(w)\n            cost_evals.append(cost)\n     \n        # plot cost path - scale to fit inside same aspect as classification plots\n        cost_evals = [v/float(np.size(self.y)) for v in cost_evals]\n        num_iterations = len(weight_history)\n        minxc = min(num_elements)-1\n        maxxc = max(num_elements)+1\n        gapxc = (maxxc - minxc)*0.1\n        minxc -= gapxc\n        maxxc += gapxc\n        minc = min(copy.deepcopy(cost_evals))\n        maxc = max(copy.deepcopy(cost_evals))\n        gapc = (maxc - minc)*0.1\n        minc -= gapc\n        maxc += gapc\n\n        ### plot it\n        # set viewing range for all 3 panels\n        xmax = max(copy.deepcopy(self.x))\n        xmin = min(copy.deepcopy(self.x))\n        xgap = (xmax - xmin)*0.05\n        xmax += xgap\n        xmin -= xgap\n        ymax = max(copy.deepcopy(self.y))[0]\n        ymin = min(copy.deepcopy(self.y))[0]\n        ygap = (ymax - ymin)*0.1\n        ymax += ygap\n        ymin -= ygap\n\n        # animate\n        print ('beginning animation rendering...')\n        def animate(k):\n            # clear the panel\n            ax1.cla()\n            ax2.cla()\n            \n            # print rendering update\n            if np.mod(k+1,5) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(len(num_elements)))\n            if k == len(num_elements):\n                print ('animation rendering complete!')\n                time.sleep(1)\n                clear_output()\n                \n            # loop over panels, produce plots\n            if k > 0:\n                self.D = num_elements[k-1] + 1\n                cs = 0\n\n                # fit to data\n                F = 0\n                predict = 0\n                w = 0\n                if basis == 'poly': \n                    w = weight_history[k-1]\n                    self.D = len(w) - 1\n                    ax1.set_title(str(self.D) + ' poly units',fontsize = 14)\n                    self.predict = self.poly_predict\n\n                elif basis == 'tanh':\n                    w = weight_history[k-1]\n                    self.D = len(w) - 1\n                    ax1.set_title(str(self.D) + ' tanh units',fontsize = 14)\n                    self.predict = self.tanh_predict\n\n                elif basis == 'tree':\n                    item = min(len(self.y)-1, num_elements[k]-1,len(weight_history)-1) \n                    w = weight_history[item]\n                    ax1.set_title(str(item) + ' tree units',fontsize = 14)\n                    self.predict = self.tree_predict\n\n                ####### plot all and dress panel ######\n                # produce learned predictor\n                s = np.linspace(xmin,xmax,400)\n                t = [self.predict(np.asarray([v]),w) for v in s]\n                ax1.plot(s,t,linewidth = 2.75,color = self.colors[2],zorder = 3)\n                            \n                # cost function value\n                ax2.plot(num_elements,cost_evals,color = 'k',linewidth = 2.5,zorder = 1)\n                ax2.scatter(num_elements[k-1],cost_evals[k-1],color = self.colors[2],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n\n                ax2.set_xlabel('model',fontsize = 12)\n                ax2.set_title('cost function plot',fontsize = 12)\n\n                # cleanp panel\n                ax2.set_xlim([minxc,maxxc])\n                ax2.set_ylim([minc,maxc])\n                #ax2.xaxis.set_major_locator(MaxNLocator(integer=True))\n\n            # plot approximation and data in panel\n            ax1.scatter(self.x,self.y,c = 'k',edgecolor = 'w',s = 50,zorder = 1)\n            #cs += 1\n\n            # cleanup panel\n            ax1.set_xlim([xmin,xmax])\n            ax1.set_ylim([ymin,ymax])\n            ax1.set_xlabel(r'$x$', fontsize = 14,labelpad = 10)\n            ax1.set_ylabel(r'$y$', rotation = 0,fontsize = 14,labelpad = 10)\n            ax1.set_xticks(np.arange(round(xmin), round(xmax)+1, 1.0))\n            #ax1.set_yticks(np.arange(round(ymin), round(ymax)+1, 1.0))\n            \n            ### if basis == tree, show the most recently added element as well\n            if basis == 'tree':\n                ax.cla()\n                \n                # plot data\n                ax.scatter(self.x,self.y,c = 'k',edgecolor = 'w',s = 50,zorder = 1)\n\n                # plot tree\n                item = min(len(self.y)-1, num_elements[k]-1,len(weight_history)-1)\n                w = 0\n                if k == 0:   # on the first slide just show first stump\n                    w = np.sign(weight_history[item])\n                else:        # show most recently added\n                    w1 = weight_history[item]\n                    w2 = weight_history[item-1]\n                    w = w1 - w2\n                    ind = np.argmax(np.abs(w))\n                    w2 = np.zeros((len(w),1))\n                    w2[ind] = 1\n                    w = w2\n                ax.set_title('best fit tree unit',fontsize = 14)\n                self.predict = self.tree_predict\n                s = np.linspace(xmin,xmax,400)\n                t = [self.predict(np.asarray([v]),w) for v in s]\n                ax.plot(s,t,linewidth = 2.75,color = self.colors[0],zorder = 3)\n                \n                # cleanup panel\n                ax.set_xlim([xmin,xmax])\n                ax.set_ylim([ymin,ymax])\n                ax.set_xlabel(r'$x$', fontsize = 14,labelpad = 10)\n                ax.set_ylabel(r'$y$', rotation = 0,fontsize = 14,labelpad = 10)\n                ax.set_xticks(np.arange(round(xmin), round(xmax)+1, 1.0))\n                ax.set_yticks(np.arange(round(ymin), round(ymax)+1, 1.0))\n            return artist,\n                \n        anim = animation.FuncAnimation(fig, animate,frames = len(num_elements)+1, interval = len(num_elements)+1, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()    \n    \n    ########### cross-validation functionality ###########\n    # function for splitting dataset into k folds\n    def split_data(self,folds):\n        # split data into k equal (as possible) sized sets\n        L = np.size(self.y)\n        order = np.random.permutation(L)\n        c = np.ones((L,1))\n        L = int(np.round((1/folds)*L))\n        for s in np.arange(0,folds-2):\n            c[order[s*L:(s+1)*L]] = s + 2\n        c[order[(folds-1)*L:]] = folds\n        return c\n    \n    ###### fit and compare ######\n    def brows_single_cross_val(self,savepath,**kwargs):\n        # parse input args\n        num_elements = [1,10,len(self.y)]\n        if 'num_elements' in kwargs:\n            num_elements = kwargs['num_elements']\n        basis = kwargs['basis']\n        self.colors = [[1,0,0.4], [ 0, 0.4, 1],[0, 1, 0.5],[1, 0.7, 0.5],[0.7, 0.6, 0.5],'mediumaquamarine']\n        folds = kwargs['folds']\n        \n        # make indices for split --> keep first fold for test, last k-1 for training\n        c = self.split_data(folds)\n        train_inds = np.argwhere(c > 1)\n        train_inds = [v[0] for v in train_inds]\n       \n        test_inds = np.argwhere(c == 1)\n        test_inds = [v[0] for v in test_inds]\n        \n        # split up points this way\n        self.x_train = copy.deepcopy(self.x[train_inds])\n        self.x_test = copy.deepcopy(self.x[test_inds])\n        self.y_train = copy.deepcopy(self.y[train_inds])\n        self.y_test = copy.deepcopy(self.y[test_inds])\n        \n        # set dials for tanh network and trees\n        num_elements = [v+1 for v in num_elements]\n        self.num_elements = max(num_elements)\n\n        # choose basis type\n        self.F = []\n        weight_history = []\n        if basis == 'poly':\n            self.F = self.poly_feats(self.num_elements)\n            self.F_train = self.F[train_inds,:]\n            self.F_test = self.F[test_inds,:]\n            \n            # collect poly and tanh weights over each desired level\n            for element in num_elements:\n                # fit weights to data\n                w = np.linalg.lstsq(self.F_train[:,:element], self.y_train)[0]\n\n                # store weights\n                weight_history.append(w)\n                \n            self.predict = self.poly_predict\n\n        if basis == 'tanh':\n            # random weights for tanh network, tanh transform \n            scale = 1\n            self.R = scale*np.random.randn(self.num_elements,2)\n            self.F = self.tanh_feats(self.num_elements)\n            self.F_train = self.F[train_inds,:]\n            self.F_test = self.F[test_inds,:]\n            \n            # collect poly and tanh weights over each desired level\n            for element in num_elements:\n                # fit weights to data\n                w = np.linalg.lstsq(self.F_train[:,:element], self.y_train)[0]\n\n                # store weights\n                weight_history.append(w)\n            \n            self.predict = self.tanh_predict\n\n        if basis == 'tree':\n            self.dial_settings()\n            self.F = self.F_tree\n            self.F_train = self.F[train_inds,:]\n            self.F_test = self.F[test_inds,:]\n            weight_history = self.boosting(self.F_train,self.y_train,its = 3000)\n\n            # compute number of non-zeros per weight in history\n            nonzs = [len(np.argwhere(w != 0)) for w in weight_history]\n\n            # find unique additions\n            huh = np.asarray([np.sign(abs(nonzs[p] - nonzs[p+1])) for p in range(len(nonzs)-1)])\n            inds = np.argwhere(huh == 1)\n            inds = [v[0] for v in inds]\n\n            # sift through, make sure to pick the best fit\n            new_inds = []\n            for j in range(len(inds)-1):\n                val = inds[j+1] - inds[j]\n                if val > 2:\n                    new_inds.append(inds[j+1] - 1)\n                else:\n                    new_inds.append(inds[j])\n            new_inds.append(inds[-1])\n            weight_history = [weight_history[ind] for ind  in new_inds]\n            weight_history = [weight_history[ind - 2] for ind in num_elements]\n            self.predict = self.tree_predict\n\n        \n        ### compute training and testing cost eval history ###\n        train_errors = []\n        test_errors = []\n        for i in range(len(weight_history)):\n            item = copy.deepcopy(i)\n            if basis == 'tree':\n                item = min(len(self.y)-1, num_elements[i]-1,len(weight_history)-1) \n            w = weight_history[item]\n            self.D = len(w) - 1\n\n            # compute training error \n            self.x_orig = copy.deepcopy(self.x)\n            self.x = self.x_train\n            self.y_orig = copy.deepcopy(self.y)\n            self.y = self.y_train           \n            train_error = (self.least_squares(w)/float(len(self.y_train)))**(0.5)\n            train_errors.append(train_error)\n            \n            # compute testing error\n            self.x = copy.deepcopy(self.x_orig)\n            self.x_orig = copy.deepcopy(self.x)\n            self.x = self.x_test\n            self.y = copy.deepcopy(self.y_orig)\n            self.y_orig = copy.deepcopy(self.y)\n            self.y = self.y_test          \n            test_error = (self.least_squares(w)/float(len(self.y_test)))**(0.5)\n            \n            self.y = copy.deepcopy(self.y_orig)\n            self.x = copy.deepcopy(self.x_orig)\n\n            # store training and testing errors\n            test_error = self.least_squares(w)\n            test_errors.append(test_error)\n     \n        # normalize training and validation errors\n        train_errors = [v/float(np.size(self.y_train)) for v in train_errors]\n        test_errors = [v/float(np.size(self.y_test)) for v in test_errors]\n\n        # plot cost path - scale to fit inside same aspect as classification plots\n        num_iterations = len(weight_history)\n        minxc = min(num_elements)-1\n        maxxc = max(num_elements)-1\n        gapxc = (maxxc - minxc)*0.1\n        minxc -= gapxc\n        maxxc += gapxc\n        minc = min(min(copy.deepcopy(train_errors)),min(copy.deepcopy(test_errors)))\n        maxc = max(max(copy.deepcopy(train_errors[:])),max(copy.deepcopy(test_errors[:5])))\n        gapc = (maxc - minc)*0.1\n        minc -= gapc\n        maxc += gapc\n\n        ### plot it\n        # construct figure\n        fig = plt.figure(figsize = (5,5))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(2, 2)#, width_ratios=[1,1,1,1]) \n        ax = plt.subplot(gs[0]); ax.axis('off');\n        ax1 = plt.subplot(gs[1]); ax1.axis('off');\n        ax2 = plt.subplot(gs[2]); ax2.axis('off');\n        ax3 = plt.subplot(gs[3]); ax2.axis('off');\n\n        # set viewing range for all 3 panels\n        xmax = max(copy.deepcopy(self.x))\n        xmin = min(copy.deepcopy(self.x))\n        xgap = (xmax - xmin)*0.05\n        xmax += xgap\n        xmin -= xgap\n        ymax = max(copy.deepcopy(self.y))[0]\n        ymin = min(copy.deepcopy(self.y))[0]\n        ygap = (ymax - ymin)*0.4\n        ymax += ygap\n        ymin -= ygap\n        \n        # animate\n        print ('beginning animation rendering...')\n        def animate(k):\n            # clear the panel\n            ax.cla()\n            ax1.cla()\n            ax2.cla()\n            ax3.cla()\n            \n            # print rendering update\n            if np.mod(k+1,5) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(len(num_elements)))\n            if k == len(num_elements):\n                print ('animation rendering complete!')\n                time.sleep(1)\n                clear_output()\n                \n                \n            #### plot data and clean up panels ####\n            # scatter data\n            ax.scatter(self.x,self.y,color = 'k',edgecolor = 'w',s = 50,zorder = 1)\n            ax1.scatter(self.x_train,self.y_train,color = [0,0.7,1],edgecolor = 'k',s = 60,zorder = 1)\n            ax2.scatter(self.x_test,self.y_test,color = [1,0.8,0.5],edgecolor = 'k',s = 60,zorder = 1)\n\n            # cleanup panels\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            ax.set_xlabel(r'$x$', fontsize = 14,labelpad = 0)\n            ax.set_ylabel(r'$y$', rotation = 0,fontsize = 14,labelpad = 5)\n            ax.set_xticks(np.arange(round(xmin), round(xmax)+1, 1.0))\n            ax.set_yticks(np.arange(round(ymin), round(ymax)+1, 1.0))\n            ax.set_title('original data',fontsize = 15)\n\n            ax1.set_xlim([xmin,xmax])\n            ax1.set_ylim([ymin,ymax])\n            ax1.set_xlabel(r'$x$', fontsize = 14,labelpad = 0)\n            ax1.set_ylabel(r'$y$', rotation = 0,fontsize = 14,labelpad = 5)\n            ax1.set_xticks(np.arange(round(xmin), round(xmax)+1, 1.0))\n            ax1.set_yticks(np.arange(round(ymin), round(ymax)+1, 1.0))\n            ax1.set_title('training data',fontsize = 15)\n\n            ax2.set_xlim([xmin,xmax])\n            ax2.set_ylim([ymin,ymax])\n            ax2.set_xlabel(r'$x$', fontsize = 14,labelpad = 0)\n            ax2.set_ylabel(r'$y$', rotation = 0,fontsize = 14,labelpad = 5)\n            ax2.set_xticks(np.arange(round(xmin), round(xmax)+1, 1.0))\n            ax2.set_yticks(np.arange(round(ymin), round(ymax)+1, 1.0))\n            ax2.set_title('validation data',fontsize = 15)\n                          \n             # cleanup\n            ax3.set_xlabel('model',fontsize = 12)\n            ax3.set_title('errors',fontsize = 15)\n           \n            # cleanp panel\n            ax3.set_xlim([minxc,maxxc])\n            ax3.set_ylim([minc,maxc])\n            ax3.xaxis.set_major_locator(MaxNLocator(integer=True))\n                     \n            if k > 0:\n                # loop over panels, produce plots\n                self.D = num_elements[k-1] \n                cs = 0\n\n                # fit to data\n                F = 0\n                predict = 0\n                w = 0\n                if basis == 'poly': \n                    w = weight_history[k-1]\n                    self.D = len(w) - 1\n                    #ax1.set_title(str(self.D) + ' poly units',fontsize = 14)\n                    self.predict = self.poly_predict\n\n                elif basis == 'tanh':\n                    w = weight_history[k-1]\n                    self.D = len(w) - 1\n                    #ax1.set_title(str(self.D) + ' tanh units',fontsize = 14)\n                    self.predict = self.tanh_predict\n\n                elif basis == 'tree':\n                    item = min(len(self.y)-1, num_elements[k-1]-1,len(weight_history)-1) \n                    w = weight_history[item]\n                    #ax1.set_title(str(item) + ' tree units',fontsize = 14)\n                    self.predict = self.tree_predict\n\n\n                # produce learned predictor\n                s = np.linspace(xmin,xmax,400)\n                t = [self.predict(np.asarray([v]),w) for v in s]\n\n                # plot approximation and data in panel\n                ax.plot(s,t,linewidth = 2.75,color = self.colors[cs],zorder = 3)\n                ax1.plot(s,t,linewidth = 2.75,color = self.colors[cs],zorder = 3)\n                ax2.plot(s,t,linewidth = 2.75,color = self.colors[cs],zorder = 3)\n                cs += 1\n\n                ### plot training and testing errors  \n                ax3.plot([v-1 for v in num_elements[:k]],train_errors[:k],color = [0,0.7,1],linewidth = 1.5,zorder = 1,label = 'training')\n                ax3.scatter([v-1 for v in num_elements[:k]],train_errors[:k],color = [0,0.7,1],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n\n                ax3.plot([v-1 for v in num_elements[:k]],test_errors[:k],color = [1,0.8,0.5],linewidth = 1.5,zorder = 1,label = 'validation')\n                ax3.scatter([v-1 for v in num_elements[:k]],test_errors[:k],color= [1,0.8,0.5],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n                #legend = ax3.legend(loc='upper right')\n\n            return artist,\n            \n        anim = animation.FuncAnimation(fig, animate,frames = len(num_elements)+1, interval = len(num_elements)+1, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()    \n"""
mlrefined_libraries/nonlinear_superlearn_library/regularization_regression_animators.py,23,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom matplotlib.ticker import FormatStrFormatter\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nimport autograd.numpy as np\n\n# import standard libraries\nimport math\nimport time\nimport copy\nfrom inspect import signature\n\nclass Visualizer:\n    '''\n    Visualize linear regression in 2 and 3 dimensions.  For single input cases (2 dimensions) the path of gradient descent on the cost function can be animated.\n    '''\n    #### initialize ####\n    def __init__(self,csvname):\n        # grab input\n        data = np.loadtxt(csvname,delimiter = ',')\n        self.x = data[:-1,:]\n        self.y = data[-1:,:] \n        \n        self.colors = [[1,0.8,0.5],[0,0.7,1]]\n        \n        # if 1-d regression data make sure points are sorted\n        if np.shape(self.x)[1] == 1:\n            ind = np.argsort(self.x.flatten())\n            self.x = self.x[ind,:]\n            self.y = self.y[ind,:]\n            \n    ########## show boosting crossval on 1d regression, with fit to residual ##########\n    def animate_trainval_regularization(self,savepath,runs,frames,num_units,**kwargs):\n        # get training / validation errors\n        train_errors = []\n        valid_errors = []\n        for run in runs:\n            # get histories\n            train_costs = run.train_cost_histories[0]\n            valid_costs = run.valid_cost_histories[0]\n            weights = run.weight_histories[0]\n            \n            # select based on minimum training\n            ind = np.argmin(train_costs)\n            train_cost = train_costs[ind]\n            valid_cost = valid_costs[ind]\n            weight = weights[ind]\n            \n            # store\n            train_errors.append(train_cost)\n            valid_errors.append(valid_cost)\n            \n        # select subset of runs\n        inds = np.arange(0,len(runs),int(len(runs)/float(frames)))\n        train_errors = [train_errors[v] for v in inds]\n        valid_errors = [valid_errors[v] for v in inds]\n        labels = []\n        for f in range(frames):\n            run = runs[f]\n            labels.append(np.round(run.lam,2))\n        \n        # select inds of history to plot\n        num_runs = frames\n\n        # construct figure\n        fig = plt.figure(figsize=(9,4))\n        artist = fig\n        \n        # parse any input args\n        scatter = 'none'\n        if 'scatter' in kwargs:\n            scatter = kwargs['scatter']\n        show_history = False\n        if 'show_history' in kwargs:\n            show_history = kwargs['show_history']\n\n        # create subplot with 1 active panel\n        gs = gridspec.GridSpec(1, 3, width_ratios=[5,5,1]) \n        ax = plt.subplot(gs[0]); \n        ax1 = plt.subplot(gs[1]);\n        ax3 = plt.subplot(gs[2]); ax3.axis('off')\n        \n        if show_history == True:\n            # create subplot with 2 active panels\n            gs = gridspec.GridSpec(1, 2, width_ratios=[1.3,1]) \n            ax = plt.subplot(gs[0]); \n            ax1 = plt.subplot(gs[1]); \n        \n        # start animation\n        num_frames = num_runs + 1\n        print ('starting animation rendering...')\n        def animate(k):\n            # clear panels\n            ax.cla()\n            ax1.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n           \n            if k == 0:\n                # get current run for cost function history plot\n                a = inds[k]\n                run = runs[a]\n\n                # pluck out current weights \n                plot_fit = False\n                self.draw_fit_trainval(ax,run,plot_fit)\n                                \n                # show cost function history\n                if show_history == True:\n                    ax1.cla()\n                    plot = False\n                    self.plot_train_valid_errors(ax1,k,train_errors,valid_errors,labels,plot)\n                    #ax1.set_yscale('log',basey=10)\n            else:\n                # get current run for cost function history plot\n                a = inds[k-1]\n                run = runs[a]\n           \n                # pluck out current weights \n                plot_fit = True\n                self.draw_fit_trainval(ax,run,plot_fit)\n                \n                # show cost function history\n                if show_history == True:\n                    ax1.cla()\n                    plot = True\n                    self.plot_train_valid_errors(ax1,k-1,train_errors,valid_errors,labels,plot)\n                    #ax1.set_yscale('log',basey=10)\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()   \n\n\n    def plot_train_valid_errors(self,ax,k,train_errors,valid_errors,labels,plot):      \n        if plot == True:\n            ax.plot(labels[:k+1] ,train_errors[:k+1],color = [0,0.7,1],linewidth = 2.5,zorder = 1,label = 'training')\n            #ax.scatter(labels[:k+1],train_errors[:k+1],color = [0,0.7,1],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n\n            ax.plot(labels[:k+1] ,valid_errors[:k+1],color = [1,0.8,0.5],linewidth = 2.5,zorder = 1,label = 'validation')\n            #ax.scatter(labels[:k+1] ,valid_errors[:k+1],color= [1,0.8,0.5],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n\n        # cleanup\n        ax.set_xlabel(r'$\\lambda$',fontsize = 12)\n        ax.set_title('errors',fontsize = 15)\n\n        # cleanp panel                \n        num_iterations = len(train_errors)\n        minxc = -0.05\n        maxxc = max(labels) + 0.05\n        minc = min(min(copy.deepcopy(train_errors)),min(copy.deepcopy(valid_errors)))\n        maxc = max(max(copy.deepcopy(train_errors[:])),max(copy.deepcopy(valid_errors[5:])))\n        \n        \n        gapc = (maxc - minc)*0.1\n        minc -= gapc\n        maxc += gapc\n        maxc = min(maxc,35)\n        \n        ax.set_xlim([minxc,maxxc])\n        ax.set_ylim([minc,maxc])\n        \n    # 1d regression demo\n    def draw_fit_trainval(self,ax,run,plot_fit):\n        # set plotting limits\n        xmax = np.max(copy.deepcopy(self.x))\n        xmin = np.min(copy.deepcopy(self.x))\n        xgap = (xmax - xmin)*0.1\n        xmin -= xgap\n        xmax += xgap\n\n        ymax = np.max(copy.deepcopy(self.y))\n        ymin = np.min(copy.deepcopy(self.y))\n        ygap = (ymax - ymin)*0.3\n        ymin -= ygap\n        ymax += ygap    \n\n        ####### plot total model on original dataset #######\n        # scatter original data - training and validation sets\n        train_inds = run.train_inds\n        valid_inds = run.valid_inds\n        ax.scatter(self.x[:,train_inds],self.y[:,train_inds],color = self.colors[1],s = 40,edgecolor = 'k',linewidth = 0.9)\n        ax.scatter(self.x[:,valid_inds],self.y[:,valid_inds],color = self.colors[0],s = 40,edgecolor = 'k',linewidth = 0.9)\n        \n        if plot_fit == True:\n            # plot fit on residual\n            s = np.linspace(xmin,xmax,2000)[np.newaxis,:]\n\n            # plot total fit\n            t = 0\n\n            # get current run\n            cost = run.cost\n            model = run.model\n            feat = run.feature_transforms\n            normalizer = run.normalizer\n            cost_history = run.train_cost_histories[0]\n            weight_history = run.weight_histories[0]\n\n            # get best weights                \n            win = np.argmin(cost_history)\n            w = weight_history[win]        \n            t = model(normalizer(s),w)\n\n            ax.plot(s.T,t.T,linewidth = 4,c = 'k')\n            ax.plot(s.T,t.T,linewidth = 2,c = 'r')\n            \n            lam = run.lam\n            ax.set_title( 'lam = ' + str(np.round(lam,2)) + ' and fit to original',fontsize = 14)\n\n        if plot_fit == False:\n            ax.set_title('test',fontsize = 14,color = 'w')\n\n        ### clean up panels ###             \n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n        \n        # label axes\n        ax.set_xlabel(r'$x$', fontsize = 14)\n        ax.set_ylabel(r'$y$', rotation = 0,fontsize = 14,labelpad = 15)\n\n    ########## show boosting results on 1d regression, with fit to residual ##########\n    def animate_boosting(self,runs,frames,**kwargs):\n        # select subset of runs\n        inds = np.arange(0,len(runs),int(len(runs)/float(frames)))\n            \n        # select inds of history to plot\n        num_runs = frames\n\n        # construct figure\n        fig = plt.figure(figsize=(9,4))\n        artist = fig\n        \n        # parse any input args\n        scatter = 'none'\n        if 'scatter' in kwargs:\n            scatter = kwargs['scatter']\n        show_history = False\n        if 'show_history' in kwargs:\n            show_history = kwargs['show_history']\n\n        # create subplot with 1 active panel\n        gs = gridspec.GridSpec(1, 3, width_ratios=[5,5,1]) \n        ax = plt.subplot(gs[0]); \n        ax1 = plt.subplot(gs[1]);\n        ax3 = plt.subplot(gs[2]); ax3.axis('off')\n        \n        if show_history == True:\n            # create subplot with 2 active panels\n            gs = gridspec.GridSpec(1, 2, width_ratios=[2,1]) \n            ax = plt.subplot(gs[0]); \n            ax1 = plt.subplot(gs[1]); \n        \n        # start animation\n        num_frames = num_runs\n        print ('starting animation rendering...')\n        def animate(k):\n            # clear panels\n            ax.cla()\n            ax1.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            # get current run for cost function history plot\n            a = inds[k]\n            run = runs[a]\n            \n            # pluck out current weights \n            self.draw_fit(ax,ax1,runs,a)\n            \n            # show cost function history\n            if show_history == True:\n                ax1.cla()\n                ax1.scatter(current_ind,cost_history[current_ind],s = 60,color = 'r',edgecolor = 'k',zorder = 3)\n                self.plot_cost_history(ax1,cost_history,start)\n                \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        return(anim)\n\n    # 1d regression demo\n    def draw_fit(self,ax,ax1,runs,ind):\n        # set plotting limits\n        xmax = np.max(copy.deepcopy(self.x))\n        xmin = np.min(copy.deepcopy(self.x))\n        xgap = (xmax - xmin)*0.1\n        xmin -= xgap\n        xmax += xgap\n\n        ymax = np.max(copy.deepcopy(self.y))\n        ymin = np.min(copy.deepcopy(self.y))\n        ygap = (ymax - ymin)*0.1\n        ymin -= ygap\n        ymax += ygap    \n\n        ####### plot total model on original dataset #######\n        # scatter original data\n        ax.scatter(self.x.flatten(),self.y.flatten(),color = 'k',s = 40,edgecolor = 'w',linewidth = 0.9)\n        \n        # plot fit on residual\n        s = np.linspace(xmin,xmax,2000)[np.newaxis,:]\n           \n        # plot total fit\n        t = 0\n        for i in range(ind+1):\n            # get current run\n            run = runs[i]\n            cost = run.cost\n            model = run.model\n            feat = run.feature_transforms\n            normalizer = run.normalizer\n            cost_history = run.cost_histories[0]\n            weight_history = run.weight_histories[0]\n\n            # get best weights                \n            win = np.argmin(cost_history)\n            w = weight_history[win]        \n            t += model(normalizer(s),w)\n\n        ax.plot(s.T,t.T,linewidth = 4,c = 'k')\n        ax.plot(s.T,t.T,linewidth = 2,c = 'r')\n        \n        ##### plot residual info #####\n        # get all functions from final step\n        run = runs[ind]\n        model = run.model\n        inverse_normalizer = run.inverse_normalizer\n        normalizer = run.normalizer\n        cost_history = run.cost_histories[0]\n        weight_history = run.weight_histories[0]\n        win = np.argmin(cost_history)\n        w = weight_history[win]      \n        x_temp = inverse_normalizer(runs[ind].x)\n        y_temp = runs[ind].y\n        \n        # scatter residual data\n        ax1.scatter(x_temp,y_temp,color = 'k',s = 40,edgecolor = 'w',linewidth = 0.9)\n\n        # make prediction\n        t = model(normalizer(s),w)\n        \n        # plot fit to residual\n        ax1.plot(s.T,t.T,linewidth = 4,c = 'k')\n        ax1.plot(s.T,t.T,linewidth = 2,c = 'r')\n        \n        ### clean up panels ###             \n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n    \n        ax1.set_xlim([xmin,xmax])\n        ax1.set_ylim([ymin,ymax])\n        \n        # label axes\n        ax.set_xlabel(r'$x$', fontsize = 14)\n        ax.set_ylabel(r'$y$', rotation = 0,fontsize = 14,labelpad = 15)\n        ax.set_title('model ' + str(ind+1) + ' fit to original',fontsize = 14)\n        \n        ax1.set_xlabel(r'$x$', fontsize = 16)\n        ax1.set_ylabel(r'$y$', rotation = 0,fontsize = 16,labelpad = 15)\n        ax1.set_title('unit ' + str(ind+1) + ' fit to residual',fontsize = 14)\n\n     """
mlrefined_libraries/superlearn_library/LS_sigmoid.py,18,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\n\nclass Visualizer:\n    \'\'\'\n    Visualize an input cost function based on data.\n    \'\'\'\n    \n    #### initialize ####\n    def __init__(self,data):\n        # grab input\n        data = data.T\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n    \n    # sigmoid\n    def sigmoid(self,t):\n        return 1/(1 + np.exp(-t))\n    \n    # sigmoid non-convex least squares\n    def sigmoid_least_squares(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = self.x[p,:]\n            y_p = self.y[p]\n            a_p = w[0] + np.sum([u*v for (u,v) in zip(x_p,w[1:])])\n            cost +=(self.sigmoid(a_p) - y_p)**2\n        return cost\n\n    ###### function plotting functions #######\n    def plot_costs(self,**kwargs):    \n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(6,3))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 2, width_ratios=[.75,1]) \n        ax1 = plt.subplot(gs[0]);\n        self.scatter_pts(ax1)\n        ax2 = plt.subplot(gs[1],projection=\'3d\'); \n\n        \n        # pull user-defined args\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        label_axes = True\n        if \'label_axes\' in kwargs:\n            label_axes = kwargs[\'label_axes\']\n        \n        # make contour plot in each panel\n        g = self.sigmoid_least_squares\n        self.surface_plot(g,ax2,viewmax,view)\n        \n        if label_axes == True:\n            ax2.set_xlabel(r\'$w_0$\',fontsize = 12)\n            ax2.set_ylabel(r\'$w_1$\',fontsize = 12,rotation = 0)\n        plt.show()\n        \n    ### visualize the surface plot of cost function ###\n    def surface_plot(self,g,ax,wmax,view):\n        ##### Produce cost function surface #####\n        r = np.linspace(-wmax,wmax,300)\n\n        # create grid from plotting range\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        w_ = np.concatenate((w1_vals,w2_vals),axis = 1)\n        g_vals = []\n        for i in range(len(r)**2):\n            g_vals.append(g(w_[i,:]))\n        g_vals = np.asarray(g_vals)\n        \n        w1_vals.shape = (np.size(r),np.size(r))\n        w2_vals.shape = (np.size(r),np.size(r))\n        \n        ### is this a counting cost?  if so re-calculate ###\n        levels = np.unique(g_vals)\n        if np.size(levels) < 30:\n            # plot each level of the counting cost\n            levels = np.unique(g_vals)\n            for u in levels:\n                # make copy of cost and nan out all non level entries\n                z = g_vals.copy()\n                ind = np.argwhere(z != u)\n                ind = [v[0] for v in ind]\n                z[ind] = np.nan\n\n                # plot the current level\n                z.shape = (len(r),len(r)) \n                ax.plot_surface(w1_vals,w2_vals,z,alpha = 1,color = \'#696969\',zorder = 0,shade = True,linewidth=0)\n\n        else: # smooth cost function, plot usual\n            # reshape and plot the surface, as well as where the zero-plane is\n            g_vals.shape = (np.size(r),np.size(r))\n\n            # plot cost surface\n            ax.plot_surface(w1_vals,w2_vals,g_vals,alpha = 1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)  \n        \n        ### clean up panel ###\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        ax.view_init(view[0],view[1])\n        \n        \n    # scatter points\n    def scatter_pts(self,ax):\n        if np.shape(self.x)[1] == 1:\n            # set plotting limits\n            xmax = max(copy.deepcopy(self.x))\n            xmin = min(copy.deepcopy(self.x))\n            xgap = (xmax - xmin)*0.4\n            xmin -= xgap\n            xmax += xgap\n            \n            ymax = max(copy.deepcopy(self.y))\n            ymin = min(copy.deepcopy(self.y))\n            ygap = (ymax - ymin)*0.4\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(self.x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n\n            # clean up panel\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            \n            # label axes\n            ax.set_xlabel(r\'$x$\', fontsize = 12)\n            ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 12)\n            \n            ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            \n        if np.shape(self.x)[1] == 2:\n            # set plotting limits\n            xmax1 = copy.deepcopy(max(self.x[:,0]))\n            xmin1 = copy.deepcopy(min(self.x[:,0]))\n            xgap1 = (xmax1 - xmin1)*0.35\n            xmin1 -= xgap1\n            xmax1 += xgap1\n            \n            xmax2 = copy.deepcopy(max(self.x[:,0]))\n            xmin2 = copy.deepcopy(min(self.x[:,0]))\n            xgap2 = (xmax2 - xmin2)*0.35\n            xmin2 -= xgap2\n            xmax2 += xgap2\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(self.x[:,0],self.x[:,1],self.y,s = 40,color = \'k\', edgecolor = \'w\',linewidth = 0.9)\n\n            # clean up panel\n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            ax.set_zlim([ymin,ymax])\n            \n            ax.set_xticks(np.arange(round(xmin1) +1, round(xmax1), 1.0))\n            ax.set_yticks(np.arange(round(xmin2) +1, round(xmax2), 1.0))\n\n            # label axes\n            ax.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 5)\n            ax.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n\n            # clean up panel\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)        \n   '"
mlrefined_libraries/superlearn_library/LS_tanh.py,18,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\n\nclass Visualizer:\n    \'\'\'\n    Visualize an input cost function based on data.\n    \'\'\'\n    \n    #### initialize ####\n    def __init__(self,data):\n        # grab input\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n    \n    # tanh non-convex least squares\n    def tanh_least_squares(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = self.x[p,:]\n            y_p = self.y[p]\n            a_p = w[0] + np.sum([u*v for (u,v) in zip(x_p,w[1:])])\n            cost +=(np.tanh(a_p) - y_p)**2\n        return cost\n\n    ###### function plotting functions #######\n    def plot_costs(self,**kwargs):    \n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(6,3))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 2, width_ratios=[.75,1]) \n        ax1 = plt.subplot(gs[0]);\n        self.scatter_pts(ax1)\n        ax2 = plt.subplot(gs[1],projection=\'3d\'); \n\n        \n        # pull user-defined args\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        \n        # make contour plot in each panel\n        g = self.tanh_least_squares\n        self.surface_plot(g,ax2,viewmax,view)\n        plt.show()\n        \n    ### visualize the surface plot of cost function ###\n    def surface_plot(self,g,ax,wmax,view):\n        ##### Produce cost function surface #####\n        r = np.linspace(-wmax,wmax,300)\n\n        # create grid from plotting range\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        w_ = np.concatenate((w1_vals,w2_vals),axis = 1)\n        g_vals = []\n        for i in range(len(r)**2):\n            g_vals.append(g(w_[i,:]))\n        g_vals = np.asarray(g_vals)\n        \n        w1_vals.shape = (np.size(r),np.size(r))\n        w2_vals.shape = (np.size(r),np.size(r))\n        \n        ### is this a counting cost?  if so re-calculate ###\n        levels = np.unique(g_vals)\n        if np.size(levels) < 30:\n            # plot each level of the counting cost\n            levels = np.unique(g_vals)\n            for u in levels:\n                # make copy of cost and nan out all non level entries\n                z = g_vals.copy()\n                ind = np.argwhere(z != u)\n                ind = [v[0] for v in ind]\n                z[ind] = np.nan\n\n                # plot the current level\n                z.shape = (len(r),len(r)) \n                ax.plot_surface(w1_vals,w2_vals,z,alpha = 0.4,color = \'#696969\',zorder = 0,shade = True,linewidth=0)\n\n        else: # smooth cost function, plot usual\n            # reshape and plot the surface, as well as where the zero-plane is\n            g_vals.shape = (np.size(r),np.size(r))\n\n            # plot cost surface\n            ax.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)  \n        \n        ### clean up panel ###\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        ax.set_xlabel(r\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(r\'$w_1$\',fontsize = 12,rotation = 0)\n\n        ax.view_init(view[0],view[1])\n        \n        \n    # scatter points\n    def scatter_pts(self,ax):\n        if np.shape(self.x)[1] == 1:\n            # set plotting limits\n            xmax = copy.deepcopy(max(self.x))\n            xmin = copy.deepcopy(min(self.x))\n            xgap = (xmax - xmin)*0.4\n            xmin -= xgap\n            xmax += xgap\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.4\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(self.x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n\n            # clean up panel\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            \n            # label axes\n            ax.set_xlabel(r\'$x$\', fontsize = 12)\n            ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 12)\n            \n            ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            \n        if np.shape(self.x)[1] == 2:\n            # set plotting limits\n            xmax1 = copy.deepcopy(max(self.x[:,0]))\n            xmin1 = copy.deepcopy(min(self.x[:,0]))\n            xgap1 = (xmax1 - xmin1)*0.35\n            xmin1 -= xgap1\n            xmax1 += xgap1\n            \n            xmax2 = copy.deepcopy(max(self.x[:,0]))\n            xmin2 = copy.deepcopy(min(self.x[:,0]))\n            xgap2 = (xmax2 - xmin2)*0.35\n            xmin2 -= xgap2\n            xmax2 += xgap2\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(self.x[:,0],self.x[:,1],self.y,s = 40,color = \'k\', edgecolor = \'w\',linewidth = 0.9)\n\n            # clean up panel\n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            ax.set_zlim([ymin,ymax])\n            \n            ax.set_xticks(np.arange(round(xmin1) +1, round(xmax1), 1.0))\n            ax.set_yticks(np.arange(round(xmin2) +1, round(xmax2), 1.0))\n\n            # label axes\n            ax.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 5)\n            ax.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n\n            # clean up panel\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)        \n   '"
mlrefined_libraries/superlearn_library/__init__.py,0,b''
mlrefined_libraries/superlearn_library/classification_2d_demos.py,47,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\n\nclass Visualizer:\n    \'\'\'\n    Visualize logistic regression applied to a 2-class dataset with N = 2\n    \'\'\'\n    #### initialize ####\n    def __init__(self,data,g):\n        # grab input\n        data = data.T\n        self.data = data\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n        self.g = g\n        \n        # colors for viewing classification data \'from above\'\n        self.colors = [\'cornflowerblue\',\'salmon\',\'lime\',\'bisque\',\'mediumaquamarine\',\'b\',\'m\',\'g\']\n        \n    # the counting cost function - for determining best weights from input weight history\n    def counting_cost(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = self.x[p]\n            y_p = self.y[p]\n            a_p = w[0] + sum([a*b for a,b in zip(w[1:],x_p)])\n            cost += (np.sign(a_p) - y_p)**2\n        return 0.25*cost\n    \n    ######## 2d functions ########\n    # animate gradient descent or newton\'s method\n    def animate_run(self,savepath,w_hist,**kwargs):     \n        self.w_hist = w_hist\n        \n        ##### setup figure to plot #####\n        # initialize figure\n        fig = plt.figure(figsize = (8,3))\n        artist = fig\n        \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]);\n\n        # produce color scheme\n        s = np.linspace(0,1,len(self.w_hist[:round(len(w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # seed left panel plotting range\n        xmin = copy.deepcopy(min(self.x))\n        xmax = copy.deepcopy(max(self.x))\n        xgap = (xmax - xmin)*0.1\n        xmin-=xgap\n        xmax+=xgap\n        x_fit = np.linspace(xmin,xmax,300)\n        \n        # seed right panel contour plot\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        num_contours = 15\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']        \n        self.contour_plot(ax2,viewmax,num_contours)\n        \n        # start animation\n        num_frames = len(self.w_hist)\n        print (\'starting animation rendering...\')\n        def animate(k):\n            # clear panels\n            ax1.cla()\n            \n            # current color\n            color = self.colorspec[k]\n\n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print (\'rendering animation frame \' + str(k+1) + \' of \' + str(num_frames))\n            if k == num_frames - 1:\n                print (\'animation rendering complete!\')\n                time.sleep(1.5)\n                clear_output()\n            \n            ###### make left panel - plot data and fit ######\n            # initialize fit\n            w = self.w_hist[k]\n            y_fit = np.tanh(w[0] + x_fit*w[1])\n            \n            # scatter data\n            self.scatter_pts(ax1)\n            \n            # plot fit to data\n            ax1.plot(x_fit,y_fit,color = color,linewidth = 2) \n\n            ###### make right panel - plot contour and steps ######\n            if k == 0:\n                ax2.scatter(w[0],w[1],s = 90,facecolor = color,edgecolor = \'k\',linewidth = 0.5, zorder = 3)\n            if k > 0 and k < num_frames:\n                self.plot_pts_on_contour(ax2,k,color)\n            if k == num_frames -1:\n                ax2.scatter(w[0],w[1],s = 90,facecolor = color,edgecolor = \'k\',linewidth = 0.5, zorder = 3)\n               \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if \'fps\' in kwargs:\n            fps = kwargs[\'fps\']\n        anim.save(savepath, fps=fps, extra_args=[\'-vcodec\', \'libx264\'])\n        clear_output()\n            \n    # produce static image of gradient descent or newton\'s method run\n    def static_fig(self,w_hist,**kwargs):\n        self.w_hist = w_hist\n        ind = -1\n        show_path = True\n        if np.size(w_hist) == 0:\n            show_path = False\n        w = 0\n        if show_path:\n            w = w_hist[ind]\n        \n        ##### setup figure to plot #####\n        # initialize figure\n        fig = plt.figure(figsize = (8,3))\n        artist = fig\n        \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]);\n\n        # produce color scheme\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # seed left panel plotting range\n        xmin = copy.deepcopy(min(self.x))\n        xmax = copy.deepcopy(max(self.x))\n        xgap = (xmax - xmin)*0.1\n        xmin-=xgap\n        xmax+=xgap\n        x_fit = np.linspace(xmin,xmax,300)\n        \n        # seed right panel contour plot\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        num_contours = 15\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']   \n            \n        ### contour plot in right panel ###\n        self.contour_plot(ax2,viewmax,num_contours)\n        \n        ### make left panel - plot data and fit ###\n        # scatter data\n        self.scatter_pts(ax1)\n        \n        if show_path:\n            # initialize fit\n            y_fit = np.tanh(w[0] + x_fit*w[1])\n\n            # plot fit to data\n            color = self.colorspec[-1]\n            ax1.plot(x_fit,y_fit,color = color,linewidth = 2) \n\n            # add points to right panel contour plot\n            num_frames = len(self.w_hist)\n            for k in range(num_frames):\n                # current color\n                color = self.colorspec[k]\n\n                # current weights\n                w = self.w_hist[k]\n\n                ###### make right panel - plot contour and steps ######\n                if k == 0:\n                    ax2.scatter(w[0],w[1],s = 90,facecolor = color,edgecolor = \'k\',linewidth = 0.5, zorder = 3)\n                if k > 0 and k < num_frames:\n                    self.plot_pts_on_contour(ax2,k,color)\n                if k == num_frames -1:\n                    ax2.scatter(w[0],w[1],s = 90,facecolor = color,edgecolor = \'k\',linewidth = 0.5, zorder = 3)\n        \n        plt.show()\n            \n    \n    ###### plot plotting functions ######\n    def plot_data(self,**kwargs):\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(9,3))\n\n        if np.shape(self.x)[1] == 1:\n            # create subplot with 2 panels\n            gs = gridspec.GridSpec(1, 3, width_ratios=[1,2,1]) \n            ax1 = plt.subplot(gs[0]); ax1.axis(\'off\') \n            ax2 = plt.subplot(gs[1]); \n            ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n        \n            # scatter points\n            self.scatter_pts(ax2)\n            \n        if np.shape(self.x)[1] == 2:\n            gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n            ax1 = plt.subplot(gs[0],projection=\'3d\'); \n            ax2 = plt.subplot(gs[1],aspect = \'equal\'); \n            #gs.update(wspace=0.025, hspace=0.05) # set spacing between axes. \n\n            \n            # plot points - first in 3d, then from above\n            self.scatter_pts(ax1)\n            self.separator_view(ax2)\n            \n            # set zaxis to the left\n            self.move_axis_left(ax1)\n            \n            # set view\n            if \'view\' in kwargs:\n                view = kwargs[\'view\']\n                ax1.view_init(view[0],view[1])\n        \n    # scatter points\n    def scatter_pts(self,ax):\n        if np.shape(self.x)[1] == 1:\n            # set plotting limits\n            xmax = copy.deepcopy(max(self.x))\n            xmin = copy.deepcopy(min(self.x))\n            xgap = (xmax - xmin)*0.2\n            xmin -= xgap\n            xmax += xgap\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(self.x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n\n            # clean up panel\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            \n            # label axes\n            ax.set_xlabel(r\'$x$\', fontsize = 12)\n            ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 12)\n            ax.set_title(\'data\', fontsize = 13)\n            \n            ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            \n        if np.shape(self.x)[1] == 2:\n            # set plotting limits\n            xmax1 = copy.deepcopy(max(self.x[:,0]))\n            xmin1 = copy.deepcopy(min(self.x[:,0]))\n            xgap1 = (xmax1 - xmin1)*0.35\n            xmin1 -= xgap1\n            xmax1 += xgap1\n            \n            xmax2 = copy.deepcopy(max(self.x[:,0]))\n            xmin2 = copy.deepcopy(min(self.x[:,0]))\n            xgap2 = (xmax2 - xmin2)*0.35\n            xmin2 -= xgap2\n            xmax2 += xgap2\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # scatter points in both panels\n            class_nums = np.unique(self.y)\n            C = len(class_nums)\n            for c in range(C):\n                ind = np.argwhere(self.y == class_nums[c])\n                ind = [v[0] for v in ind]\n                ax.scatter(self.x[ind,0],self.x[ind,1],self.y[ind],s = 80,color = self.colors[c],edgecolor = \'k\',linewidth = 1.5)\n\n            # clean up panel\n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            ax.set_zlim([ymin,ymax])\n            \n            ax.set_xticks(np.arange(round(xmin1) +1, round(xmax1), 1.0))\n            ax.set_yticks(np.arange(round(xmin2) +1, round(xmax2), 1.0))\n            ax.set_zticks([-1,0,1])\n            \n            # label axes\n            ax.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 5)\n            ax.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n\n            # clean up panel\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n    # plot data \'from above\' in seperator view\n    def separator_view(self,ax):\n        # set plotting limits\n        xmax1 = copy.deepcopy(max(self.x[:,0]))\n        xmin1 = copy.deepcopy(min(self.x[:,0]))\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n            \n        xmax2 = copy.deepcopy(max(self.x[:,0]))\n        xmin2 = copy.deepcopy(min(self.x[:,0]))\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2\n            \n        ymax = max(self.y)\n        ymin = min(self.y)\n        ygap = (ymax - ymin)*0.2\n        ymin -= ygap\n        ymax += ygap    \n\n        # scatter points\n        classes = np.unique(self.y)\n        count = 0\n        for num in classes:\n            inds = np.argwhere(self.y == num)\n            inds = [s[0] for s in inds]\n            plt.scatter(self.data[inds,0],self.data[inds,1],color = self.colors[int(count)],linewidth = 1,marker = \'o\',edgecolor = \'k\',s = 50)\n            count+=1\n            \n        # clean up panel\n        ax.set_xlim([xmin1,xmax1])\n        ax.set_ylim([xmin2,xmax2])\n\n        ax.set_xticks(np.arange(round(xmin1), round(xmax1) + 1, 1.0))\n        ax.set_yticks(np.arange(round(xmin2), round(xmax2) + 1, 1.0))\n\n        # label axes\n        ax.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 0)\n        ax.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            \n    # plot points on contour\n    def plot_pts_on_contour(self,ax,j,color):\n        # plot connector between points for visualization purposes\n        w_old = self.w_hist[j-1]\n        w_new = self.w_hist[j]\n        g_old = self.g(w_old)\n        g_new = self.g(w_new)\n     \n        ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = color,linewidth = 3,alpha = 1,zorder = 2)      # plot approx\n        ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = \'k\',linewidth = 3 + 1,alpha = 1,zorder = 1)      # plot approx\n    \n    ###### function plotting functions #######\n    def plot_ls_cost(self,**kwargs):\n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(8,3))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0],aspect = \'equal\'); \n        ax2 = plt.subplot(gs[1],projection=\'3d\'); \n        \n        # pull user-defined args\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        num_contours = 15\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']\n        \n        # make contour plot in left panel\n        self.contour_plot(ax1,viewmax,num_contours)\n        \n        # make contour plot in right panel\n        self.surface_plot(ax2,viewmax,view)\n        \n        plt.show()\n        \n    ### visualize the surface plot of cost function ###\n    def surface_plot(self,ax,wmax,view):\n        ##### Produce cost function surface #####\n        wmax += wmax*0.1\n        r = np.linspace(-wmax,wmax,200)\n\n        # create grid from plotting range\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        w_ = np.concatenate((w1_vals,w2_vals),axis = 1)\n        g_vals = []\n        for i in range(len(r)**2):\n            g_vals.append(self.g(w_[i,:]))\n        g_vals = np.asarray(g_vals)\n\n        # reshape and plot the surface, as well as where the zero-plane is\n        w1_vals.shape = (np.size(r),np.size(r))\n        w2_vals.shape = (np.size(r),np.size(r))\n        g_vals.shape = (np.size(r),np.size(r))\n        \n        # plot cost surface\n        ax.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)  \n        \n        # clean up panel\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        ax.set_xlabel(r\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(r\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.set_title(r\'$g\\left(w_0,w_1\\right)$\',fontsize = 13)\n\n        ax.view_init(view[0],view[1])\n        \n    ### visualize contour plot of cost function ###\n    def contour_plot(self,ax,wmax,num_contours):\n        #### define input space for function and evaluate ####\n        w1 = np.linspace(-wmax,wmax,100)\n        w2 = np.linspace(-wmax,wmax,100)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([ self.g(np.reshape(s,(2,1))) for s in h])\n        #func_vals = np.asarray([self.g(s) for s in h])\n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cutoff = 0.5\n        cutoff = (levelmax - levelmin)*cutoff\n        numper = 3\n        levels1 = np.linspace(cutoff,levelmax,numper)\n        num_contours -= numper\n\n        levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n        levels = np.unique(np.append(levels1,levels2))\n        num_contours -= numper\n        while num_contours > 0:\n            cutoff = levels[1]\n            levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n            levels = np.unique(np.append(levels2,levels))\n            num_contours -= numper\n\n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = \'k\')\n        ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = \'Blues\')\n                \n        # clean up panel\n        ax.set_xlabel(\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.set_title(r\'$g\\left(w_0,w_1\\right)$\',fontsize = 13)\n\n        ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.set_xlim([-wmax,wmax])\n        ax.set_ylim([-wmax,wmax])'"
mlrefined_libraries/superlearn_library/classification_2d_demos_entropy.py,46,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\n\nclass Visualizer:\n    \'\'\'\n    Visualize logistic regression applied to a 2-class dataset with N = 2\n    \'\'\'\n    #### initialize ####\n    def __init__(self,data,g):\n        # grab input\n        data = data.T\n        self.data = data\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n        self.g = g\n        \n        # colors for viewing classification data \'from above\'\n        self.colors = [\'cornflowerblue\',\'salmon\',\'lime\',\'bisque\',\'mediumaquamarine\',\'b\',\'m\',\'g\']\n    \n    ### logistic functionality ###\n    def identity(self,t):\n        val = 0\n        if t > 0.5:\n            val = 1\n        return val\n    \n    # define sigmoid function\n    def sigmoid(self,t):\n        return 1/(1 + np.exp(-t))\n    \n    ######## 2d functions ########\n    # animate gradient descent or newton\'s method\n    def animate_run(self,savepath,w_hist,**kwargs):     \n        self.w_hist = w_hist\n        \n        ##### setup figure to plot #####\n        # initialize figure\n        fig = plt.figure(figsize = (8,3))\n        artist = fig\n        \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]);\n\n        # produce color scheme\n        s = np.linspace(0,1,len(self.w_hist[:round(len(w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # seed left panel plotting range\n        xmin = copy.deepcopy(min(self.x))\n        xmax = copy.deepcopy(max(self.x))\n        xgap = (xmax - xmin)*0.1\n        xmin-=xgap\n        xmax+=xgap\n        x_fit = np.linspace(xmin,xmax,300)\n        \n        # seed right panel contour plot\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        num_contours = 15\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']        \n        self.contour_plot(ax2,viewmax,num_contours)\n        \n        # start animation\n        num_frames = len(self.w_hist)\n        print (\'starting animation rendering...\')\n        def animate(k):\n            # clear panels\n            ax1.cla()\n            \n            # current color\n            color = self.colorspec[k]\n\n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print (\'rendering animation frame \' + str(k+1) + \' of \' + str(num_frames))\n            if k == num_frames - 1:\n                print (\'animation rendering complete!\')\n                time.sleep(1.5)\n                clear_output()\n            \n            ###### make left panel - plot data and fit ######\n            # initialize fit\n            w = self.w_hist[k]\n            y_fit = self.sigmoid(w[0] + x_fit*w[1])\n            \n            # scatter data\n            self.scatter_pts(ax1)\n            \n            # plot fit to data\n            ax1.plot(x_fit,y_fit,color = color,linewidth = 2) \n\n            ###### make right panel - plot contour and steps ######\n            if k == 0:\n                ax2.scatter(w[0],w[1],s = 90,facecolor = color,edgecolor = \'k\',linewidth = 0.5, zorder = 3)\n            if k > 0 and k < num_frames:\n                self.plot_pts_on_contour(ax2,k,color)\n            if k == num_frames -1:\n                ax2.scatter(w[0],w[1],s = 90,facecolor = color,edgecolor = \'k\',linewidth = 0.5, zorder = 3)\n               \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if \'fps\' in kwargs:\n            fps = kwargs[\'fps\']\n        anim.save(savepath, fps=fps, extra_args=[\'-vcodec\', \'libx264\'])\n        clear_output()\n            \n    def sigmoid(self,t):\n        return 1/(1 + np.exp(-t))\n    \n    # produce static image of gradient descent or newton\'s method run\n    def static_fig(self,w_hist,**kwargs):\n        self.w_hist = w_hist\n        ind = -1\n        show_path = True\n        if np.size(w_hist) == 0:\n            show_path = False\n        w = 0\n        if show_path:\n            w = w_hist[ind]\n        \n        ##### setup figure to plot #####\n        # initialize figure\n        fig = plt.figure(figsize = (8,3))\n        artist = fig\n        \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]);\n\n        # produce color scheme\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # seed left panel plotting range\n        xmin = copy.deepcopy(min(self.x))\n        xmax = copy.deepcopy(max(self.x))\n        xgap = (xmax - xmin)*0.1\n        xmin-=xgap\n        xmax+=xgap\n        x_fit = np.linspace(xmin,xmax,300)\n        \n        # seed right panel contour plot\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        num_contours = 15\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']   \n            \n        ### contour plot in right panel ###\n        self.contour_plot(ax2,viewmax,num_contours)\n        \n        ### make left panel - plot data and fit ###\n        # scatter data\n        self.scatter_pts(ax1)\n        \n        if show_path:\n            # initialize fit\n            y_fit = self.sigmoid(w[0] + x_fit*w[1])\n\n            # plot fit to data\n            color = self.colorspec[-1]\n            ax1.plot(x_fit,y_fit,color = color,linewidth = 2) \n\n            # add points to right panel contour plot\n            num_frames = len(self.w_hist)\n            for k in range(num_frames):\n                # current color\n                color = self.colorspec[k]\n\n                # current weights\n                w = self.w_hist[k]\n\n                ###### make right panel - plot contour and steps ######\n                if k == 0:\n                    ax2.scatter(w[0],w[1],s = 90,facecolor = color,edgecolor = \'k\',linewidth = 0.5, zorder = 3)\n                if k > 0 and k < num_frames:\n                    self.plot_pts_on_contour(ax2,k,color)\n                if k == num_frames -1:\n                    ax2.scatter(w[0],w[1],s = 90,facecolor = color,edgecolor = \'k\',linewidth = 0.5, zorder = 3)\n        \n        plt.show()\n            \n    \n    ###### plot plotting functions ######\n    def plot_data(self,**kwargs):\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(9,3))\n\n        if np.shape(self.x)[1] == 1:\n            # create subplot with 2 panels\n            gs = gridspec.GridSpec(1, 3, width_ratios=[1,2,1]) \n            ax1 = plt.subplot(gs[0]); ax1.axis(\'off\') \n            ax2 = plt.subplot(gs[1]); \n            ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n        \n            # scatter points\n            self.scatter_pts(ax2)\n            \n        if np.shape(self.x)[1] == 2:\n            gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n            ax1 = plt.subplot(gs[0],projection=\'3d\'); \n            ax2 = plt.subplot(gs[1],aspect = \'equal\'); \n            #gs.update(wspace=0.025, hspace=0.05) # set spacing between axes. \n\n            \n            # plot points - first in 3d, then from above\n            self.scatter_pts(ax1)\n            self.separator_view(ax2)\n            \n            # set zaxis to the left\n            self.move_axis_left(ax1)\n            \n            # set view\n            if \'view\' in kwargs:\n                view = kwargs[\'view\']\n                ax1.view_init(view[0],view[1])\n        \n    # scatter points\n    def scatter_pts(self,ax):\n        if np.shape(self.x)[1] == 1:\n            # set plotting limits\n            xmax = copy.deepcopy(max(self.x))\n            xmin = copy.deepcopy(min(self.x))\n            xgap = (xmax - xmin)*0.2\n            xmin -= xgap\n            xmax += xgap\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(self.x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n\n            # clean up panel\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            \n            # label axes\n            ax.set_xlabel(r\'$x$\', fontsize = 12)\n            ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 12)\n            ax.set_title(\'data\', fontsize = 13)\n            \n            ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            \n        if np.shape(self.x)[1] == 2:\n            # set plotting limits\n            xmax1 = copy.deepcopy(max(self.x[:,0]))\n            xmin1 = copy.deepcopy(min(self.x[:,0]))\n            xgap1 = (xmax1 - xmin1)*0.35\n            xmin1 -= xgap1\n            xmax1 += xgap1\n            \n            xmax2 = copy.deepcopy(max(self.x[:,0]))\n            xmin2 = copy.deepcopy(min(self.x[:,0]))\n            xgap2 = (xmax2 - xmin2)*0.35\n            xmin2 -= xgap2\n            xmax2 += xgap2\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # scatter points in both panels\n            class_nums = np.unique(self.y)\n            C = len(class_nums)\n            for c in range(C):\n                ind = np.argwhere(self.y == class_nums[c])\n                ind = [v[0] for v in ind]\n                ax.scatter(self.x[ind,0],self.x[ind,1],self.y[ind],s = 80,color = self.colors[c],edgecolor = \'k\',linewidth = 1.5)\n\n            # clean up panel\n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            ax.set_zlim([ymin,ymax])\n            \n            ax.set_xticks(np.arange(round(xmin1) +1, round(xmax1), 1.0))\n            ax.set_yticks(np.arange(round(xmin2) +1, round(xmax2), 1.0))\n            ax.set_zticks([-1,0,1])\n            \n            # label axes\n            ax.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 5)\n            ax.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n\n            # clean up panel\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n    # plot data \'from above\' in seperator view\n    def separator_view(self,ax):\n        # set plotting limits\n        xmax1 = copy.deepcopy(max(self.x[:,0]))\n        xmin1 = copy.deepcopy(min(self.x[:,0]))\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n            \n        xmax2 = copy.deepcopy(max(self.x[:,0]))\n        xmin2 = copy.deepcopy(min(self.x[:,0]))\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2\n            \n        ymax = max(self.y)\n        ymin = min(self.y)\n        ygap = (ymax - ymin)*0.2\n        ymin -= ygap\n        ymax += ygap    \n\n        # scatter points\n        classes = np.unique(self.y)\n        count = 0\n        for num in classes:\n            inds = np.argwhere(self.y == num)\n            inds = [s[0] for s in inds]\n            plt.scatter(self.data[inds,0],self.data[inds,1],color = self.colors[int(count)],linewidth = 1,marker = \'o\',edgecolor = \'k\',s = 50)\n            count+=1\n            \n        # clean up panel\n        ax.set_xlim([xmin1,xmax1])\n        ax.set_ylim([xmin2,xmax2])\n\n        ax.set_xticks(np.arange(round(xmin1), round(xmax1) + 1, 1.0))\n        ax.set_yticks(np.arange(round(xmin2), round(xmax2) + 1, 1.0))\n\n        # label axes\n        ax.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 0)\n        ax.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            \n    # plot points on contour\n    def plot_pts_on_contour(self,ax,j,color):\n        # plot connector between points for visualization purposes\n        w_old = self.w_hist[j-1]\n        w_new = self.w_hist[j]\n        g_old = self.g(w_old)\n        g_new = self.g(w_new)\n     \n        ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = color,linewidth = 3,alpha = 1,zorder = 2)      # plot approx\n        ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = \'k\',linewidth = 3 + 1,alpha = 1,zorder = 1)      # plot approx\n    \n    ###### function plotting functions #######\n    def plot_ls_cost(self,**kwargs):\n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(8,3))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0],aspect = \'equal\'); \n        ax2 = plt.subplot(gs[1],projection=\'3d\'); \n        \n        # pull user-defined args\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        num_contours = 15\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']\n        \n        # make contour plot in left panel\n        self.contour_plot(ax1,viewmax,num_contours)\n        \n        # make contour plot in right panel\n        self.surface_plot(ax2,viewmax,view)\n        \n        plt.show()\n        \n    ### visualize the surface plot of cost function ###\n    def surface_plot(self,ax,wmax,view):\n        ##### Produce cost function surface #####\n        wmax += wmax*0.1\n        r = np.linspace(-wmax,wmax,200)\n\n        # create grid from plotting range\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        w_ = np.concatenate((w1_vals,w2_vals),axis = 1)\n        g_vals = []\n        for i in range(len(r)**2):\n            g_vals.append(self.g(w_[i,:]))\n        g_vals = np.asarray(g_vals)\n\n        # reshape and plot the surface, as well as where the zero-plane is\n        w1_vals.shape = (np.size(r),np.size(r))\n        w2_vals.shape = (np.size(r),np.size(r))\n        g_vals.shape = (np.size(r),np.size(r))\n        \n        # plot cost surface\n        ax.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)  \n        \n        # clean up panel\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        ax.set_xlabel(r\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(r\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.set_title(r\'$g\\left(w_0,w_1\\right)$\',fontsize = 13)\n\n        ax.view_init(view[0],view[1])\n        \n    ### visualize contour plot of cost function ###\n    def contour_plot(self,ax,wmax,num_contours):\n        \n        #### define input space for function and evaluate ####\n        w1 = np.linspace(-wmax,wmax,100)\n        w2 = np.linspace(-wmax,wmax,100)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([ self.g(np.reshape(s,(2,1))) for s in h])\n\n        #func_vals = np.asarray([self.g(s) for s in h])\n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cutoff = 0.5\n        cutoff = (levelmax - levelmin)*cutoff\n        numper = 3\n        levels1 = np.linspace(cutoff,levelmax,numper)\n        num_contours -= numper\n\n        levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n        levels = np.unique(np.append(levels1,levels2))\n        num_contours -= numper\n        while num_contours > 0:\n            cutoff = levels[1]\n            levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n            levels = np.unique(np.append(levels2,levels))\n            num_contours -= numper\n\n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = \'k\')\n        ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = \'Blues\')\n                \n        # clean up panel\n        ax.set_xlabel(\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.set_title(r\'$g\\left(w_0,w_1\\right)$\',fontsize = 13)\n\n        ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.set_xlim([-wmax,wmax])\n        ax.set_ylim([-wmax,wmax])'"
mlrefined_libraries/superlearn_library/classification_3d_demos.py,28,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\n\nclass Visualizer:\n    \'\'\'\n    Visualize classification on a 2-class dataset with N = 2\n    \'\'\'\n    #### initialize ####\n    def __init__(self,data):\n        # grab input\n        data = data.T\n        self.data = data\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n        \n        # colors for viewing classification data \'from above\'\n        self.colors = [\'cornflowerblue\',\'salmon\',\'lime\',\'bisque\',\'mediumaquamarine\',\'b\',\'m\',\'g\']\n\n    def center_data(self):\n        # center data\n        self.x = self.x - np.mean(self.x)\n        self.y = self.y - np.mean(self.y)\n        \n    # the counting cost function - for determining best weights from input weight history\n    def counting_cost(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = self.x[p]\n            y_p = self.y[p]\n            a_p = w[0] + sum([a*b for a,b in zip(w[1:],x_p)])\n            cost += (np.sign(a_p) - y_p)**2\n        return 0.25*cost\n                    \n     ######## 3d static and animation functions ########\n    # produce static image of gradient descent or newton\'s method run\n    def static_fig(self,w,**kwargs):      \n        # grab args\n        zplane = \'on\'\n        if \'zplane\' in kwargs:\n            zplane = kwargs[\'zplane\']\n            \n        cost_plot = \'off\'\n        if \'cost_plot\' in kwargs:\n            cost_plot = kwargs[\'cost_plot\']     \n         \n        g = 0\n        if \'g\' in kwargs:\n            g = kwargs[\'g\']             \n                \n        ### plot all input data ###\n        # generate input range for functions\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n\n        r = np.linspace(minx,maxx,400)\n        x1_vals,x2_vals = np.meshgrid(r,r)\n        x1_vals.shape = (len(r)**2,1)\n        x2_vals.shape = (len(r)**2,1)\n        h = np.concatenate([x1_vals,x2_vals],axis = 1)\n        g_vals = np.tanh( w[0] + w[1]*x1_vals + w[2]*x2_vals )\n        g_vals = np.asarray(g_vals)\n\n        # vals for cost surface\n        x1_vals.shape = (len(r),len(r))\n        x2_vals.shape = (len(r),len(r))\n        g_vals.shape = (len(r),len(r))\n\n        # create figure to plot\n        num_panels = 2\n        fig_len = 9\n        widths = [1,1]\n        if cost_plot == \'on\':\n            num_panels = 3\n            fig_len = 8\n            widths = [2,2,1]\n        fig, axs = plt.subplots(1, num_panels, figsize=(fig_len,4))\n        gs = gridspec.GridSpec(1, num_panels, width_ratios=widths) \n        ax1 = plt.subplot(gs[0],projection=\'3d\'); \n        ax2 = plt.subplot(gs[1],aspect = \'equal\'); \n        ax3 = 0\n        if cost_plot == \'on\':\n            ax3 = plt.subplot(gs[2],aspect = 0.5); \n\n        fig.subplots_adjust(left=0,right=1,bottom=0,top=1)   # remove whitespace around 3d figure\n            \n        # plot points - first in 3d, then from above\n        self.scatter_pts(ax1)\n        self.separator_view(ax2)\n            \n        # set zaxis to the left\n        self.move_axis_left(ax1)\n            \n        # set view\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n            ax1.view_init(view[0],view[1])\n            \n        class_nums = np.unique(self.y)\n        C = len(class_nums)\n            \n        # plot regression surface\n        ax1.plot_surface(x1_vals,x2_vals,g_vals,alpha = 0.1,color = \'k\',rstride=20, cstride=20,linewidth=0,edgecolor = \'k\') \n            \n        # plot zplane = 0 in left 3d panel - showing intersection of regressor with z = 0 (i.e., its contour, the separator, in the 3d plot too)?\n        if zplane == \'on\':\n            ax1.plot_surface(x1_vals,x2_vals,g_vals*0,alpha = 0.1,rstride=20, cstride=20,linewidth=0.15,color = \'w\',edgecolor = \'k\') \n            # plot separator curve in left plot\n            ax1.contour(x1_vals,x2_vals,g_vals,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n                \n            if C == 2:\n                ax1.contourf(x1_vals,x2_vals,g_vals,colors = self.colors[1],levels = [0,1],zorder = 1,alpha = 0.1)\n                ax1.contourf(x1_vals,x2_vals,g_vals+1,colors = self.colors[0],levels = [0,1],zorder = 1,alpha = 0.1)\n\n            \n        # plot separator in right plot\n        ax2.contour(x1_vals,x2_vals,g_vals,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n            \n        # plot color filled contour based on separator\n        if C == 2:\n            g_vals = np.sign(g_vals) + 1\n            ax2.contourf(x1_vals,x2_vals,g_vals,colors = self.colors[:],alpha = 0.1,levels = range(0,C+1))\n        else:\n            ax2.contourf(x1_vals,x2_vals,g_vals,colors = self.colors[:],alpha = 0.1,levels = range(0,C+1))\n     \n        # plot cost function value\n        if cost_plot == \'on\':\n            # plot cost function history\n            g_hist = []\n            for j in range(len(w_hist)):\n                w = w_hist[j]\n                g_eval = g(w)\n                g_hist.append(g_eval)\n                \n            g_hist = np.asarray(g_hist).flatten()\n            \n            # plot cost function history\n            ax3.plot(np.arange(len(g_hist)),g_hist,linewidth = 2)\n            ax3.set_xlabel(\'iteration\',fontsize = 13)\n            ax3.set_title(\'cost value\',fontsize = 12)\n    \n        plt.show()\n  \n    # produce static image of gradient descent or newton\'s method run\n    def static_fig_topview(self,w,**kwargs):               \n        ### plot all input data ###\n        # generate input range for functions\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n\n        r = np.linspace(minx,maxx,400)\n        x1_vals,x2_vals = np.meshgrid(r,r)\n        x1_vals.shape = (len(r)**2,1)\n        x2_vals.shape = (len(r)**2,1)\n        h = np.concatenate([x1_vals,x2_vals],axis = 1)\n        g_vals = np.tanh( w[0] + w[1]*x1_vals + w[2]*x2_vals )\n        g_vals = np.asarray(g_vals)\n\n        # vals for cost surface\n        x1_vals.shape = (len(r),len(r))\n        x2_vals.shape = (len(r),len(r))\n        g_vals.shape = (len(r),len(r))\n\n        # create figure to plot\n        ### initialize figure\n        fig = plt.figure(figsize = (9,4))\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,1]) \n        ax1 = plt.subplot(gs[0]);  ax1.axis(\'off\')\n        ax2 = plt.subplot(gs[1],aspect = \'equal\');\n        ax3 = plt.subplot(gs[2]);  ax3.axis(\'off\')\n            \n        # plot points - first in 3d, then from above\n        self.separator_view(ax2)\n        class_nums = np.unique(self.y)\n        C = len(class_nums)\n         \n        # plot separator in right plot\n        ax2.contour(x1_vals,x2_vals,g_vals,colors = \'k\',levels = [0],linewidths = 3,zorder = 1)\n            \n        # plot color filled contour based on separator\n        if C == 2:\n            g_vals = np.sign(g_vals) + 1\n            ax2.contourf(x1_vals,x2_vals,g_vals,colors = self.colors[:],alpha = 0.1,levels = range(0,C+1))\n        else:\n            ax2.contourf(x1_vals,x2_vals,g_vals,colors = self.colors[:],alpha = 0.1,levels = range(0,C+1))\n  \n        plt.show()\n        \n\n    # set axis in left panel\n    def move_axis_left(self,ax):\n        tmp_planes = ax.zaxis._PLANES \n        ax.zaxis._PLANES = ( tmp_planes[2], tmp_planes[3], \n                             tmp_planes[0], tmp_planes[1], \n                             tmp_planes[4], tmp_planes[5])\n        view_1 = (25, -135)\n        view_2 = (25, -45)\n        init_view = view_2\n        ax.view_init(*init_view)\n    \n    ###### plot plotting functions ######\n    def plot_data(self,**kwargs):\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(9,4))\n        fig.subplots_adjust(left=0,right=1,bottom=0,top=1)   # remove whitespace around 3d figure\n\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0],projection=\'3d\'); \n        ax2 = plt.subplot(gs[1],aspect = \'equal\'); \n\n        # plot points - first in 3d, then from above\n        self.scatter_pts(ax1)\n        self.separator_view(ax2)\n\n        # set zaxis to the left\n        self.move_axis_left(ax1)\n            \n        # set view\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n            ax1.view_init(view[0],view[1])\n        \n    # scatter points\n    def scatter_pts(self,ax):\n        if np.shape(self.x)[1] == 2:\n            # set plotting limits\n            xmax1 = copy.deepcopy(max(self.x[:,0]))\n            xmin1 = copy.deepcopy(min(self.x[:,0]))\n            xgap1 = (xmax1 - xmin1)*0.35\n            xmin1 -= xgap1\n            xmax1 += xgap1\n            \n            xmax2 = copy.deepcopy(max(self.x[:,0]))\n            xmin2 = copy.deepcopy(min(self.x[:,0]))\n            xgap2 = (xmax2 - xmin2)*0.35\n            xmin2 -= xgap2\n            xmax2 += xgap2\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # scatter points in both panels\n            class_nums = np.unique(self.y)\n            C = len(class_nums)\n            for c in range(C):\n                ind = np.argwhere(self.y == class_nums[c])\n                ind = [v[0] for v in ind]\n                ax.scatter(self.x[ind,0],self.x[ind,1],self.y[ind],s = 80,color = self.colors[c],edgecolor = \'k\',linewidth = 1.5)\n\n            # clean up panel\n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            ax.set_zlim([ymin,ymax])\n            \n            ax.set_xticks(np.arange(round(xmin1) +1, round(xmax1), 1.0))\n            ax.set_yticks(np.arange(round(xmin2) +1, round(xmax2), 1.0))\n            ax.set_zticks([-1,0,1])\n            \n            # label axes\n            ax.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 5)\n            ax.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n\n            # clean up panel\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n    # plot data \'from above\' in seperator view\n    def separator_view(self,ax):\n        # set plotting limits\n        xmax1 = copy.deepcopy(max(self.x[:,0]))\n        xmin1 = copy.deepcopy(min(self.x[:,0]))\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n            \n        xmax2 = copy.deepcopy(max(self.x[:,0]))\n        xmin2 = copy.deepcopy(min(self.x[:,0]))\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2\n            \n        ymax = max(self.y)\n        ymin = min(self.y)\n        ygap = (ymax - ymin)*0.2\n        ymin -= ygap\n        ymax += ygap    \n\n        # scatter points\n        classes = np.unique(self.y)\n        count = 0\n        for num in classes:\n            inds = np.argwhere(self.y == num)\n            inds = [s[0] for s in inds]\n            ax.scatter(self.data[inds,0],self.data[inds,1],color = self.colors[int(count)],linewidth = 1,marker = \'o\',edgecolor = \'k\',s = 50)\n            count+=1\n            \n        # clean up panel\n        ax.set_xlim([xmin1,xmax1])\n        ax.set_ylim([xmin2,xmax2])\n\n        ax.set_xticks(np.arange(round(xmin1), round(xmax1) + 1, 1.0))\n        ax.set_yticks(np.arange(round(xmin2), round(xmax2) + 1, 1.0))\n\n        # label axes\n        ax.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 0)\n        ax.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            '"
mlrefined_libraries/superlearn_library/classification_bits.py,5,"b""# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nimport copy\nfrom autograd.misc.flatten import flatten_func\n    \n#### newton's method ####            \ndef newtons_method(g,w,x,y,beta,max_its):        \n    # flatten gradient for simpler-written descent loop\n    flat_g, unflatten, w = flatten_func(g, w)\n\n    grad = compute_grad(flat_g)\n    hess = compute_hess(flat_g)  \n\n    # create container for weight history \n    w_hist = []\n    w_hist.append(unflatten(w))\n    \n    g_hist = []\n    geval_old = flat_g(w,x,y,beta)\n    g_hist.append(geval_old)\n\n    # main loop\n    epsilon = 10**(-7)\n    for k in range(max_its):\n        # compute gradient and hessian\n        grad_val = grad(w,x,y,beta)\n        hess_val = hess(w,x,y,beta)\n        hess_val.shape = (np.size(w),np.size(w))\n\n        # solve linear system for weights\n        w = w - np.dot(np.linalg.pinv(hess_val + epsilon*np.eye(np.size(w))),grad_val)\n\n        # eject from process if reaching singular system\n        geval_new = flat_g(w,x,y,beta)\n        if k > 2 and geval_new > geval_old:\n            print ('singular system reached')\n            time.sleep(1.5)\n            clear_output()\n            return w_hist\n        else:\n            geval_old = geval_new\n\n        # record current weights\n        w_hist.append(unflatten(w))\n        g_hist.append(geval_new)\n\n    return w_hist,g_hist\n\n# compute linear combination of input points\ndef model(x,w):\n    a = w[0] + np.dot(x.T,w[1:])\n    return a.T\n   \n# softmax cost\ndef softmax(w,x,y,beta):\n    # compute cost over batch        \n    cost = np.sum(beta*np.log(1 + np.exp(-y*model(x,w))))\n    return cost/float(np.size(y))"""
mlrefined_libraries/superlearn_library/classification_plotter.py,12,"b""import matplotlib.pyplot as plt\nimport autograd.numpy as np                 # Thinly-wrapped numpy\nfrom mpl_toolkits.mplot3d import Axes3D\n \nclass define_plot:\n    def __init__(self,**kwargs):\n        self.x = kwargs['inputs']\n        self.y = kwargs['outputs']\n        self.model = kwargs['model']\n\n    # plot the cost history\n    def plot_cost_history(self,ghist):\n        plt.plot(self.ghist)\n        plt.show()\n\n    # toy plot\n    def toy_plot(self,**kwargs):\n        # grab args\n        zplane = 'off'\n        if 'zplane' in kwargs:\n            zplane = kwargs['zplane']\n        \n        # colors for plot \n        color_opts = np.array([[1,0,0.4], [ 0, 0.4, 1],[0, 1, 0.5],[1, 0.7, 0.5],[0.7, 0.6, 0.5]])\n        \n        # 2d or 3d plot?\n        switch = 1\n        if np.shape(self.x)[1] == 2:\n            switch = 2\n        if np.shape(self.x)[1] > 2:\n            print ('this works only for input data that is 1 or 2 dimensional!')\n            return\n\n        # a 2d plot of data with predictor\n        if switch == 1:\n            # lets see our prediction on the data\n            plt.scatter(self.x,self.y)\n            s = np.linspace(min(self.x),max(self.x))\n            s.shape = (len(s),1)\n            t = self.model.predict(s)\n            plt.plot(s,t,c = 'r',linewidth = 2)\n            plt.show()\n\n        # a 3d plot of data with predictor\n        if switch == 2:\n            ### plot all input data ###\n            # generate input range for functions\n            minx = min(min(self.x[:,0]),min(self.x[:,1]))\n            maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n            gapx = (maxx - minx)*0.1\n            minx -= gapx\n            maxx += gapx\n\n            r = np.linspace(minx,maxx,200)\n            w1_vals,w2_vals = np.meshgrid(r,r)\n            w1_vals.shape = (len(r)**2,1)\n            w2_vals.shape = (len(r)**2,1)\n            h = np.concatenate([w1_vals,w2_vals],axis = 1)\n            g_vals = self.model.predict(h)\n            g_vals = np.asarray(g_vals)\n            print (np.shape(g_vals))\n\n            # vals for cost surface\n            w1_vals.shape = (len(r),len(r))\n            w2_vals.shape = (len(r),len(r))\n            g_vals.shape = (len(r),len(r))\n\n            # create figure to plot\n            fig = plt.figure(num=None, figsize=(12,5), dpi=80, facecolor='w', edgecolor='k')\n            \n            ### create 3d plot in left panel\n            ax1 = plt.subplot(121,projection = '3d')\n            ax2 = plt.subplot(122)\n\n            fig.subplots_adjust(left=0,right=1,bottom=0,top=1)   # remove whitespace around 3d figure\n            \n            # scatter points in both panels\n            class_nums = np.unique(self.y)\n            C = len(class_nums)\n            for c in range(C):\n                ind = np.argwhere(self.y == class_nums[c])\n                ind = [v[0] for v in ind]\n                ax1.scatter(self.x[ind,0],self.x[ind,1],self.y[ind],s = 80,color = color_opts[c],edgecolor = 'k',linewidth = 1.5)\n                ax2.scatter(self.x[ind,0],self.x[ind,1],s = 110,color = color_opts[c],edgecolor = 'k', linewidth = 2)\n\n            # plot regression surface\n            ax1.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = 'k',rstride=20, cstride=20,linewidth=0,edgecolor = 'k') \n            \n            # plot zplane = 0 in left 3d panel - showing intersection of regressor with z = 0 (i.e., its contour, the separator, in the 3d plot too)?\n            if zplane == 'on':\n                ax1.plot_surface(w1_vals,w2_vals,g_vals*0,alpha = 0.1,rstride=20, cstride=20,linewidth=0.15,color = 'lime',edgecolor = 'k') \n            \n            # plot separator curve in right plot\n            ax1.contour(w1_vals,w2_vals,g_vals,colors = 'k',levels = [0],linewidths = 3,zorder = 1)\n            ax2.contour(w1_vals,w2_vals,g_vals,colors = 'k',levels = [0],linewidths = 3,zorder = 1)\n            \n            # plot color filled contour based on separator\n            if C == 2:\n                g_vals = np.sign(g_vals) + 1\n                ax2.contourf(w1_vals,w2_vals,g_vals,colors = color_opts[:],alpha = 0.1,levels = range(0,C+1))\n            else:\n                ax2.contourf(w1_vals,w2_vals,g_vals,colors = color_opts[:],alpha = 0.1,levels = range(0,C+1))\n                \n\n            # clean up panels\n            ax1.view_init(20,-70)   \n            ax2.axis('off')\n     \n            plt.show()\n       """
mlrefined_libraries/superlearn_library/classification_static_plotter.py,5,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom autograd import numpy as np\n\nclass Visualizer:\n    '''\n    Illustrate a run of your preferred optimization algorithm for classification.  Run\n    the algorithm first, and input the resulting weight history into this wrapper.\n    ''' \n\n    # compare cost histories from multiple runs\n    def plot_histories(self,cost_histories,count_histories,start,**kwargs):        \n        # plotting colors\n        colors = ['k','magenta','springgreen','blueviolet','chocolate']\n        \n        # initialize figure\n        fig = plt.figure(figsize = (10,3))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 2) \n        ax = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]); \n\n        # any labels to add?        \n        labels = [' ',' ']\n        if 'labels' in kwargs:\n            labels = kwargs['labels']\n            \n        # plot points on cost function plot too?\n        points = False\n        if 'points' in kwargs:\n            points = kwargs['points']\n\n        # run through input histories, plotting each beginning at 'start' iteration\n        for c in range(len(cost_histories)):\n            history = cost_histories[c]\n            count_hist = count_histories[c]\n            label = labels[c]\n                \n            # check if a label exists, if so add it to the plot\n            ax.plot(np.arange(start,len(history),1),history[start:],linewidth = 3*(0.8)**(c),color = colors[c]) \n            if np.size(label) == 0:\n                ax2.plot(np.arange(start,len(history),1),count_hist[start:],linewidth = 3*(0.8)**(c),color = colors[c]) \n            else:          \n                ax2.plot(np.arange(start,len(history),1),count_hist[start:],linewidth = 3*(0.8)**(c),color = colors[c],label = label) \n                \n        # clean up panel\n        xlabel = 'step $k$'\n        ylabel = r'$g\\left(\\mathbf{w}^k\\right)$'\n        ylabel2 = 'num misclassifications'\n\n        ax.set_xlabel(xlabel,fontsize = 14)\n        ax.set_ylabel(ylabel,fontsize = 14,rotation = 0,labelpad = 25)\n        ax.set_title('cost function history',fontsize = 14)\n        ax2.set_xlabel(xlabel,fontsize = 14)\n        ax2.set_ylabel(ylabel2,fontsize = 12,rotation = 90,labelpad = 10)\n        ax2.set_title('misclassification history',fontsize = 14)\n \n        if np.size(label) > 0:\n            anchor = (1,1)\n            if 'anchor' in kwargs:\n                anchor = kwargs['anchor']\n            plt.legend(loc='upper right', bbox_to_anchor=anchor)\n            #leg = ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1), borderaxespad=0)\n\n        ax.set_xlim([start - 0.5,len(history) - 0.5])\n        ax2.set_xlim([start - 0.5,len(history) - 0.5])\n\n       # fig.tight_layout()\n        plt.show()"""
mlrefined_libraries/superlearn_library/cost_comparisons.py,20,"b""# import standard plotting \nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\n\n# other basic libraries\nimport math\nimport time\nimport copy\nimport autograd.numpy as np\n\n# import optimizer class from same library\nfrom mlrefined_libraries.math_optimization_library import optimizers\n\nclass Visualizer:\n    '''\n    Compare cost functions for two-class classification\n    \n    '''\n    \n    #### initialize ####\n    def __init__(self,data):        \n        # grab input\n        self.data = data\n        self.x = data[:-1,:]\n        self.y = data[-1:,:] \n        \n    ### cost functions ###\n    def counting_cost(self,w):\n        # compute predicted labels\n        y_hat = np.sign(self.model(self.x,w))\n                \n        # compare to true labels\n        ind = np.argwhere(self.y != y_hat)\n        ind = [v[1] for v in ind]\n       \n        cost = np.sum(len(ind))\n        return cost\n    \n    # compute stats for F1 score\n    def confusion_matrix(self,w):\n        # compute predicted labels\n        y_hat = np.sign(self.model(self.x,w))\n        \n        # determine indices of real and predicted label values\n        ind1 = np.argwhere(self.y == +1)\n        ind1 = [v[1] for v in ind1]\n\n        ind2 = np.argwhere(self.y == -1)\n        ind2 = [v[1] for v in ind2]\n        \n        ind3 = np.argwhere(y_hat == +1)\n        ind3 = [v[1] for v in ind3]\n\n        ind4 = np.argwhere(y_hat == -1)\n        ind4 = [v[1] for v in ind4]    \n        \n        # compute elements of confusion matrix\n        A = len(list(set.intersection(*[set(ind1), set(ind3)])))\n        B = len(list(set.intersection(*[set(ind1), set(ind4)])))\n        C = len(list(set.intersection(*[set(ind2), set(ind3)])))\n        D = len(list(set.intersection(*[set(ind2), set(ind4)])))\n        return A,B,C,D\n        \n    # compute balanced accuracy\n    def compute_balanced_accuracy(self,w):\n        # compute confusion matrix\n        A,B,C,D = self.confusion_matrix(w)\n        \n        # compute precision and recall\n        precision = 0\n        if A > 0:\n            precision = A/(A + B)\n            \n        specif = 0\n        if D > 0:\n            specif = D/(C + D)\n        \n        # compute balanced accuracy\n        balanced_accuracy = (precision + specif)/2\n        return balanced_accuracy\n    \n    # compute linear combination of input point\n    def model(self,x,w):\n        a = w[0] + np.dot(x.T,w[1:])\n        return a.T\n    \n    # the perceptron relu cost\n    def relu(self,w):\n        cost = np.sum(np.maximum(0,-self.y*self.model(self.x,w)))\n        return cost/float(np.size(y))\n\n    # the convex softmax cost function\n    def softmax(self,w):\n        cost = np.sum(np.log(1 + np.exp(-self.y*self.model(self.x,w))))\n        return cost/float(np.size(self.y))\n                   \n    ### compare grad descent runs - given cost to counting cost ###\n    def compare_to_counting(self,cost,**kwargs):\n        # parse args\n        num_runs = 1\n        if 'num_runs' in kwargs:\n            num_runs = kwargs['num_runs']\n        max_its = 200\n        if 'max_its' in kwargs:\n            max_its = kwargs['max_its']\n        alpha = 10**-3\n        if 'alpha' in kwargs:\n            alpha = kwargs['alpha']  \n         \n        #### perform all optimizations ###\n        g = self.softmax\n        if cost == 'softmax':\n            g = self.softmax\n        if cost == 'relu':\n            g = self.relu\n        g_count = self.counting_cost\n\n        big_w_hist = []\n        for j in range(num_runs):\n            # construct random init\n            w_init = np.random.randn(np.shape(self.x)[0]+1,1)\n            \n            # run optimizer\n            w_hist,g_hist = optimizers.gradient_descent(g = g, alpha_choice = alpha,max_its = max_its,w = w_init)\n            \n            # store history\n            big_w_hist.append(w_hist)\n            \n        ##### setup figure to plot #####\n        # initialize figure\n        fig = plt.figure(figsize = (8,4))\n        artist = fig\n        \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]);\n        \n        #### start runs and plotting ####\n        for j in range(num_runs):\n            w_hist = big_w_hist[j]\n            \n            # evaluate counting cost / other cost for each weight in history, then plot\n            count_evals = []\n            cost_evals = []\n            for k in range(len(w_hist)):\n                w = w_hist[k]\n                g_eval = g(w)\n                cost_evals.append(g_eval)\n                \n                count_eval = g_count(w)\n                count_evals.append(count_eval)\n                \n            # plot each \n            ax1.plot(np.arange(0,len(w_hist)),count_evals[:len(w_hist)],linewidth = 2)\n            ax2.plot(np.arange(0,len(w_hist)),cost_evals[:len(w_hist)],linewidth = 2)\n                \n        #### cleanup plots ####\n        # label axes\n        ax1.set_xlabel('iteration',fontsize = 13)\n        ax1.set_ylabel('num misclassifications',rotation = 90,fontsize = 13)\n        ax1.set_title('number of misclassifications',fontsize = 14)\n        ax1.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n        \n        ax2.set_xlabel('iteration',fontsize = 13)\n        ax2.set_ylabel('cost value',rotation = 90,fontsize = 13)\n        title = cost + ' cost'\n        ax2.set_title(title,fontsize = 14)\n        ax2.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n        \n        plt.show()\n        \n    ### compare grad descent runs - given cost to counting cost ###\n    def compare_to_balanced_accuracy(self,cost,**kwargs):\n        # parse args\n        num_runs = 1\n        if 'num_runs' in kwargs:\n            num_runs = kwargs['num_runs']\n        max_its = 200\n        if 'max_its' in kwargs:\n            max_its = kwargs['max_its']\n        alpha = 10**-3\n        if 'alpha' in kwargs:\n            alpha = kwargs['alpha']  \n         \n        #### perform all optimizations ###\n        g = self.softmax\n        if cost == 'softmax':\n            g = self.softmax\n        if cost == 'relu':\n            g = self.relu\n        computer = self.compute_balanced_accuracy\n        g_count = self.counting_cost\n\n        self.big_w_hist = []\n        for j in range(num_runs):\n            # construct random init\n            w_init = np.random.randn(np.shape(self.x)[0]+1,1)\n            \n            # run optimizer\n            w_hist,g_hist = optimizers.gradient_descent(g = g, alpha_choice = alpha,max_its = max_its,w = w_init)\n            \n            # store history\n            self.big_w_hist.append(w_hist)\n            \n        ##### setup figure to plot #####\n        # initialize figure\n        fig = plt.figure(figsize = (9,3))\n        artist = fig\n        \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]);\n        \n        #### start runs and plotting ####\n        for j in range(num_runs):\n            w_hist = self.big_w_hist[j]\n            \n            # evaluate counting cost / other cost for each weight in history, then plot\n            self.balanced_vals = []\n            cost_evals = []\n            self.count_evals = []\n            for k in range(len(w_hist)):\n                w = w_hist[k]\n                g_eval = g(w)\n                cost_evals.append(g_eval)\n                \n                count_eval = 1 - g_count(w)/self.y.size\n                self.count_evals.append(count_eval)\n                \n                balanced_accuracy = computer(w)\n                self.balanced_vals.append(balanced_accuracy)\n                \n            # plot each             \n            ax1.plot(np.arange(0,len(w_hist)),self.count_evals[:len(w_hist)],linewidth = 2,label = 'accuracy')\n            ax1.plot(np.arange(0,len(w_hist)),self.balanced_vals[:len(w_hist)],linewidth = 2,label = 'balanced accuracy')\n            ax1.legend(loc = 4)\n            \n            ax2.plot(np.arange(0,len(w_hist)),cost_evals[:len(w_hist)],linewidth = 2)\n                \n        #### cleanup plots ####\n        # label axes      \n        ax1.set_xlabel('iteration',fontsize = 13)\n        ax1.set_title('metrics',fontsize = 14)\n        ax1.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n        \n        ax2.set_xlabel('iteration',fontsize = 13)\n        ax2.set_ylabel('cost value',rotation = 90,fontsize = 13)\n        title = cost + ' cost'\n        ax2.set_title(title,fontsize = 14)\n        ax2.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n        \n        plt.show()   """
mlrefined_libraries/superlearn_library/cost_functions.py,20,"b""import autograd.numpy as np\n\n'''\nA list of cost functions for supervised learning.  Use the choose_cost function\nto choose the desired cost with input data (x_in,y_in).  The aim here was to \ncreate a library of cost functions while keeping things as simple as possible \n(i.e., without the use of object oriented programming).  \n'''\n\ndef choose_cost(x_in,y_in,cost,**kwargs):\n    # define x and y as globals so all cost functions are aware of them\n    global x,y\n    x = x_in\n    y = y_in\n    \n    # make any other variables not explicitly input into cost functions globally known\n    global lam\n    lam = 0\n    if 'lam' in kwargs:\n        lam = kwargs['lam']\n    \n    # make cost function choice\n    cost_func = 0\n    if cost == 'least_squares':\n        cost_func = least_squares\n    if cost == 'least_absolute_deviations':\n        cost_func = least_absolute_deviations\n    if cost == 'softmax':\n        cost_func = softmax\n    if cost == 'relu':\n        cost_func = relu\n    if cost == 'counter':\n        cost_func = counting_cost\n\n    if cost == 'multiclass_perceptron':\n        cost_func = multiclass_perceptron\n    if cost == 'multiclass_softmax':\n        cost_func = multiclass_softmax\n    if cost == 'multiclass_counter':\n        cost_func = multiclass_counting_cost\n        \n    return cost_func\n\n###### basic model ######\n# compute linear combination of input point\ndef model(x,w):\n    a = w[0] + np.dot(x.T,w[1:])\n    return a.T\n\n###### cost functions #####\n# an implementation of the least squares cost function for linear regression\ndef least_squares(w):\n    cost = np.sum((model(x,w) - y)**2)\n    return cost/float(np.size(y))\n\n# a compact least absolute deviations cost function\ndef least_absolute_deviations(w):\n    cost = np.sum(np.abs(model(x,w) - y))\n    return cost/float(np.size(y))\n\n# the convex softmax cost function\ndef softmax(w):\n    cost = np.sum(np.log(1 + np.exp(-y*model(x,w))))\n    return cost/float(np.size(y))\n\n# the convex relu cost function\ndef relu(w):\n    cost = np.sum(np.maximum(0,-y*model(x,w)))\n    return cost/float(np.size(y))\n\n# the counting cost function\ndef counting_cost(w):\n    cost = np.sum((np.sign(model(x,w)) - y)**2)\n    return 0.25*cost \n\n# multiclass perceptron\ndef multiclass_perceptron(w):        \n    # pre-compute predictions on all points\n    all_evals = model(x,w)\n    \n    # compute maximum across data points\n    a = np.max(all_evals,axis = 0)    \n\n    # compute cost in compact form using numpy broadcasting\n    b = all_evals[y.astype(int).flatten(),np.arange(np.size(y))]\n    cost = np.sum(a - b)\n\n    # return average\n    return cost/float(np.size(y))\n\n# multiclass softmax\ndef multiclass_softmax(w):        \n    # pre-compute predictions on all points\n    all_evals = model(x,w)\n    \n    # compute softmax across data points\n    a = np.log(np.sum(np.exp(all_evals),axis = 0)) \n    \n    # compute cost in compact form using numpy broadcasting\n    b = all_evals[y.astype(int).flatten(),np.arange(np.size(y))]\n    cost = np.sum(a - b)\n\n    # return average\n    return cost/float(np.size(y))\n\n# multiclass misclassification cost function - aka the fusion rule\ndef multiclass_counting_cost(w):                \n    # pre-compute predictions on all points\n    all_evals = model(x,w)\n\n    # compute predictions of each input point\n    y_predict = (np.argmax(all_evals,axis = 0))[np.newaxis,:]\n\n    # compare predicted label to actual label\n    count = np.sum(np.abs(np.sign(y - y_predict)))\n\n    # return number of misclassifications\n    return count"""
mlrefined_libraries/superlearn_library/cost_viewer.py,18,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\n\nclass Visualizer:\n    \'\'\'\n    Visualize an input cost function based on data.\n    \'\'\'\n    \n    #### initialize ####\n    def __init__(self,data):\n        # grab input\n        data = data.T\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n\n    # least squares\n    def counting_cost(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = self.x[p,:]\n            y_p = self.y[p]\n            a_p = w[0] + np.sum([u*v for (u,v) in zip(x_p,w[1:])])\n            e = 0\n            if np.sign(a_p) != y_p:\n                cost += 1\n        return float(cost)\n    \n    # log-loss\n    def log_loss(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = self.x[p,:]\n            y_p = self.y[p]\n            a_p = w[0] + np.sum([u*v for (u,v) in zip(x_p,w[1:])])\n            cost += np.log(1 + np.exp(-y_p*a_p))\n        return cost\n    \n    # tanh non-convex least squares\n    def tanh_least_squares(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = self.x[p,:]\n            y_p = self.y[p]\n            a_p = w[0] + np.sum([u*v for (u,v) in zip(x_p,w[1:])])\n            cost +=(np.tanh(a_p) - y_p)**2\n        return cost\n\n    ###### function plotting functions #######\n    def plot_costs(self,**kwargs):    \n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(8,3))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,1,1]) \n        ax1 = plt.subplot(gs[0],projection=\'3d\');\n        ax2 = plt.subplot(gs[1],projection=\'3d\'); \n        ax3 = plt.subplot(gs[2],projection=\'3d\');\n        \n        # pull user-defined args\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        \n        # make contour plot in each panel\n        g = self.counting_cost\n        self.surface_plot(g,ax1,viewmax,view)\n        g = self.tanh_least_squares\n        self.surface_plot(g,ax2,viewmax,view)\n        g = self.log_loss\n        self.surface_plot(g,ax3,viewmax,view)\n        plt.show()\n        \n    ### visualize the surface plot of cost function ###\n    def surface_plot(self,g,ax,wmax,view):\n        ##### Produce cost function surface #####\n        r = np.linspace(-wmax,wmax,300)\n\n        # create grid from plotting range\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        w_ = np.concatenate((w1_vals,w2_vals),axis = 1)\n        g_vals = []\n        for i in range(len(r)**2):\n            g_vals.append(g(w_[i,:]))\n        g_vals = np.asarray(g_vals)\n        \n        w1_vals.shape = (np.size(r),np.size(r))\n        w2_vals.shape = (np.size(r),np.size(r))\n        \n        ### is this a counting cost?  if so re-calculate ###\n        levels = np.unique(g_vals)\n        if np.size(levels) < 30:\n            # plot each level of the counting cost\n            levels = np.unique(g_vals)\n            for u in levels:\n                # make copy of cost and nan out all non level entries\n                z = g_vals.copy()\n                ind = np.argwhere(z != u)\n                ind = [v[0] for v in ind]\n                z[ind] = np.nan\n\n                # plot the current level\n                z.shape = (len(r),len(r)) \n                ax.plot_surface(w1_vals,w2_vals,z,alpha = 0.4,color = \'#696969\',zorder = 0,shade = True,linewidth=0)\n\n        else: # smooth cost function, plot usual\n            # reshape and plot the surface, as well as where the zero-plane is\n            g_vals.shape = (np.size(r),np.size(r))\n\n            # plot cost surface\n            ax.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)  \n        \n        ### clean up panel ###\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        ax.set_xlabel(r\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(r\'$w_1$\',fontsize = 12,rotation = 0)\n\n        ax.view_init(view[0],view[1])\n   '"
mlrefined_libraries/superlearn_library/cost_viewer_entropy.py,18,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\n\nclass Visualizer:\n    \'\'\'\n    Visualize an input cost function based on data.\n    \'\'\'\n    \n    #### initialize ####\n    def __init__(self,data):\n        # grab input\n        data = data.T\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n    \n    def identity(self,t):\n        val = 0\n        if t > 0.5:\n            val = 1\n        return val\n        \n    def counting_cost(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            # strip of pth point\n            x_p = copy.deepcopy(self.x[p,:])\n            x_p.shape = (len(x_p),1)\n            y_p = self.y[p]\n            \n            # produce predicted label\n            val_p = self.identity(w[0] + np.dot(w[1:].T,x_p)) \n\n            # compare with true label\n            if val_p != y_p:\n                cost += 1\n        return float(cost)\n    \n    # sigmoid\n    def sigmoid(self,t):\n        return 1/(1 + np.exp(-t))\n    \n    def my_exp(self,t):\n        if t > 10:\n            t = 10\n        if t < -10:\n            t = -10\n        return np.exp(t)\n    \n    # log-loss\n    def entropy(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            # extract point\n            x_p = self.x[p,:]\n            y_p = self.y[p]\n            \n            # sigmoid of linear combo of input\n            a_p = self.sigmoid(w[0] + np.sum([u*v for (u,v) in zip(x_p,w[1:])]))\n            \n            # compute cost\n            cost -= (y_p*np.log(a_p) + (1-y_p)*np.log(1 - a_p))\n        return cost\n    \n    # sigmoid non-convex least squares\n    def sigmoid_least_squares(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = self.x[p,:]\n            y_p = self.y[p]\n            a_p = w[0] + np.sum([u*v for (u,v) in zip(x_p,w[1:])])\n            cost +=(self.sigmoid(a_p) - y_p)**2\n        return cost\n\n    ###### function plotting functions #######\n    def plot_costs(self,**kwargs):    \n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(8,3))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,1,1]) \n        ax1 = plt.subplot(gs[0],projection=\'3d\');\n        ax2 = plt.subplot(gs[1],projection=\'3d\'); \n        ax3 = plt.subplot(gs[2],projection=\'3d\');\n        \n        # pull user-defined args\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        \n        # make contour plot in each panel\n        g = self.counting_cost\n        self.surface_plot(g,ax1,viewmax,view)\n        g = self.sigmoid_least_squares\n        self.surface_plot(g,ax2,viewmax,view)\n        g = self.entropy\n        self.surface_plot(g,ax3,viewmax,view)\n        plt.show()\n        \n    ### visualize the surface plot of cost function ###\n    def surface_plot(self,g,ax,wmax,view):\n        ##### Produce cost function surface #####\n        r = np.linspace(-wmax,wmax,300) \n\n        # create grid from plotting range\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        w_ = np.concatenate((w1_vals,w2_vals),axis = 1)\n        g_vals = []\n        for i in range(len(r)**2):\n            g_vals.append(g(w_[i,:]))\n        g_vals = np.asarray(g_vals)\n        \n        w1_vals.shape = (np.size(r),np.size(r))\n        w2_vals.shape = (np.size(r),np.size(r))\n        \n        ### is this a counting cost?  if so re-calculate ###\n        levels = np.unique(g_vals)\n        if np.size(levels) < 30:\n            # plot each level of the counting cost\n            levels = np.unique(g_vals)\n            for u in levels:\n                # make copy of cost and nan out all non level entries\n                z = g_vals.copy()\n                ind = np.argwhere(z != u)\n                ind = [v[0] for v in ind]\n                z[ind] = np.nan\n\n                # plot the current level\n                z.shape = (len(r),len(r)) \n                ax.plot_surface(w1_vals,w2_vals,z,alpha = 0.4,color = \'#696969\',zorder = 0,shade = True,linewidth=0)\n\n        else: # smooth cost function, plot usual\n            # reshape and plot the surface, as well as where the zero-plane is\n            g_vals.shape = (np.size(r),np.size(r))\n\n            # plot cost surface\n            ax.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)  \n        \n        ### clean up panel ###\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        ax.set_xlabel(r\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(r\'$w_1$\',fontsize = 12,rotation = 0)\n\n        ax.view_init(view[0],view[1])\n   '"
mlrefined_libraries/superlearn_library/feature_scaling_tools.py,23,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   \nfrom autograd import hessian as compute_hess\nimport autograd.numpy as np\nimport math\nimport time\n\nclass Visualizer:\n    '''\n    Animate how normalizing the input of a single input supervised cost function re-shapes \n    its contours, equalizing the penalty assigned to violating either the ideal bias or slope\n    parameter.\n    ''' \n\n    # load in data, in particular input and normalized input\n    def __init__(self,x,x_normalized,y,cost):\n        self.x_original = x\n        self.x_normalized = x_normalized\n        self.y = y\n        self.cost_func = cost\n        \n        # make cost function choice\n        self.cost_func = 0\n        if cost == 'least_squares':\n            self.cost_func = self.least_squares\n        if cost == 'least_absolute_deviations':\n            self.cost_func = self.least_absolute_deviations\n        if cost == 'softmax':\n            self.cost_func = self.softmax\n        if cost == 'relu':\n            self.cost_func = self.relu\n                            \n    #####   #####\n    def animate_transition(self,savepath,num_frames,**kwargs):\n        # initialize figure\n        fig = plt.figure(figsize = (10,5))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); ax.set_aspect('equal')\n\n        # animation sub-function\n        lams = np.linspace(0,1,num_frames)\n        print ('starting animation rendering...')\n        def animate(k):\n            ax.cla()\n            lam = lams[k]\n            \n            # print rendering update            \n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n                \n            # re-assign inputs as weighted average of original and normalized input\n            self.x = (1 - lam)*self.x_original + lam*self.x_normalized\n            \n            # plot contour\n            self.contour_plot_setup(ax,**kwargs)  # draw contour plot\n            ax.set_title(r'$\\lambda = ' + str(np.round(lam,2)) + '$',fontsize = 14)\n\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()\n            \n    ########################################################################################\n    ###### predict and cost functions #####\n    ###### basic model ######\n    # compute linear combination of input point\n    def model(self,x,w):\n        # tack a 1 onto the top of each input point all at once\n        o = np.ones((1,np.shape(x)[1]))\n        x = np.vstack((o,x))\n\n        # compute linear combination and return\n        a = np.dot(x.T,w)\n        return a\n\n    ###### cost functions #####\n    # an implementation of the least squares cost function for linear regression\n    def least_squares(self,w):\n        cost = np.sum((self.model(self.x,w) - self.y)**2)\n        return cost/float(len(self.y))\n    \n    # a compact least absolute deviations cost function\n    def least_absolute_deviations(self,w):\n        cost = np.sum(np.abs(self.model(self.x,w) - self.y))\n        return cost/float(len(self.y))\n\n    # the convex softmax cost function\n    def softmax(self,w):\n        cost = np.sum(np.log(1 + np.exp(-self.y*self.model(self.x,w))))\n        return cost/float(len(self.y))\n    \n    # the convex relu cost function\n    def relu(self,w):\n        cost = np.sum(np.maximum(0,-self.y*self.model(self.x,w)))\n        return cost/float(len(self.y))\n\n    # the counting cost function\n    def counting_cost(self,w):\n        cost = np.sum((np.sign(self.model(self.x,w)) - self.y)**2)\n        return 0.25*cost \n    \n    \n    ########################################################################################\n    #### utility functions - for setting up / making contour plots, 3d surface plots, etc., ####\n    # show contour plot of input function\n    def contour_plot_setup(self,ax,**kwargs):\n        xmin = -3.1\n        xmax = 3.1\n        ymin = -3.1\n        ymax = 3.1\n        if 'xmin' in kwargs:            \n            xmin = kwargs['xmin']\n        if 'xmax' in kwargs:\n            xmax = kwargs['xmax']\n        if 'ymin' in kwargs:            \n            ymin = kwargs['ymin']\n        if 'ymax' in kwargs:\n            ymax = kwargs['ymax']      \n        num_contours = 20\n        if 'num_contours' in kwargs:\n            num_contours = kwargs['num_contours']   \n            \n        # choose viewing range using weight history?\n        if 'view_by_weights' in kwargs:\n            view_by_weights = True\n            weight_history = kwargs['weight_history']\n            if view_by_weights == True:\n                xmin = min([v[0] for v in weight_history])[0]\n                xmax = max([v[0] for v in weight_history])[0]\n                xgap = (xmax - xmin)*0.25\n                xmin -= xgap\n                xmax += xgap\n\n                ymin = min([v[1] for v in weight_history])[0]\n                ymax = max([v[1] for v in weight_history])[0]\n                ygap = (ymax - ymin)*0.25\n                ymin -= ygap\n                ymax += ygap\n \n        ### plot function as contours ###\n        self.draw_contour_plot(ax,num_contours,xmin,xmax,ymin,ymax)\n        \n        ### cleanup panel ###\n        ax.set_xlabel('$w_0$',fontsize = 14)\n        ax.set_ylabel('$w_1$',fontsize = 14,labelpad = 15,rotation = 0)\n        ax.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color='k',zorder = 0,linewidth = 0.5)\n        # ax.set_xticks(np.arange(round(xmin),round(xmax)+1))\n        # ax.set_yticks(np.arange(round(ymin),round(ymax)+1))\n        \n        # set viewing limits\n        ax.set_xlim(xmin,xmax)\n        ax.set_ylim(ymin,ymax)\n\n    ### function for creating contour plot\n    def draw_contour_plot(self,ax,num_contours,xmin,xmax,ymin,ymax):\n        #### define input space for function and evaluate ####\n        w1 = np.linspace(xmin,xmax,400)\n        w2 = np.linspace(ymin,ymax,400)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([ self.cost_func(np.reshape(s,(2,1))) for s in h])\n\n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cut = 0.4\n        cutoff = (levelmax - levelmin)\n        levels = [levelmin + cutoff*cut**(num_contours - i) for i in range(0,num_contours+1)]\n        levels = [levelmin] + levels\n        levels = np.asarray(levels)\n   \n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = 'k')\n        b = ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = 'Blues')\n      \n    # a small Python function for plotting the distributions of input features\n    def feature_distributions(self,x):\n        # create figure \n        fig = plt.figure(figsize = (10,4))\n\n        # create subplots\n        N = x.shape[0]\n        gs = 0\n        if N <= 5:\n            gs = gridspec.GridSpec(1,N)\n        else:\n            gs = gridspec.GridSpec(2,5)\n\n\n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n        # loop over input and plot each individual input dimension value\n        all_bins = []\n        for n in range(N):\n            hist, bins = np.histogram(x[n,:], bins=30)\n            all_bins.append(bins.ravel())\n            \n        # determine range for all subplots\n        maxview = np.max(all_bins)\n        minview = np.min(all_bins)\n        viewrange = (maxview - minview)*0.1\n        maxview += viewrange\n        minview -= viewrange\n        \n        for n in range(N):\n            # make subplot\n            ax = plt.subplot(gs[n]); \n            hist, bins = np.histogram(x[n,:], bins=30)\n            width = 0.7 * (bins[1] - bins[0])\n            center = (bins[:-1] + bins[1:]) / 2\n            ax.barh(center, hist,width)\n            ax.set_title(r'$x_' + str(n+1) + '$',fontsize=14)\n            ax.set_ylim([minview,maxview])\n        plt.show()"""
mlrefined_libraries/superlearn_library/lin_classification_demos.py,24,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\n\nclass Visualizer:\n    \'\'\'\n    Visualize linear classification and fits in 2d (for N=1 dimensional input datasets)\n    \'\'\'\n    #### initialize ####\n    def __init__(self,data):\n        # grab input\n        data = data.T\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n   \n    ###### plot plotting functions ######\n    def plot_data(self):\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(9,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\') \n        ax2 = plt.subplot(gs[1]); \n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n\n        # scatter points\n        self.scatter_pts(ax2)\n        \n    # plot regression fits\n    def plot_fit(self,plotting_weights,**kwargs):\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(9,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\') \n        ax = plt.subplot(gs[1]); \n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n        \n        # set plotting limits\n        xmax = copy.deepcopy(max(self.x))\n        xmin = copy.deepcopy(min(self.x))\n        xgap = (xmax - xmin)*0.25\n        xmin -= xgap\n        xmax += xgap\n\n        ymax = max(self.y)\n        ymin = min(self.y)\n        ygap = (ymax - ymin)*0.25\n        ymin -= ygap\n        ymax += ygap    \n\n        # initialize points\n        ax.scatter(self.x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 80,zorder = 3)\n\n        # clean up panel\n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n\n        # label axes\n        ax.set_xlabel(r\'$x$\', fontsize = 12)\n        ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 12)\n        \n        # create fit\n        s = np.linspace(xmin,xmax,300)\n        colors = [\'k\',\'magenta\']\n        if \'colors\' in kwargs:\n            colors = kwargs[\'colors\']\n        \n        transformers = [lambda a: a for i in range(len(plotting_weights))]\n        if \'transformers\' in kwargs:\n            transformers = kwargs[\'transformers\']\n\n        for i in range(len(plotting_weights)):\n            weights = plotting_weights[i]\n            transformer = transformers[i]\n            \n            # plot approximation\n            l = weights[0] + weights[1]*transformer(s)\n            t = np.tanh(l).flatten()\n            ax.plot(s,t,linewidth = 2,color = colors[i],zorder = 2)\n        \n        # plot counting cost \n        #t = np.sign(l).flatten()\n        #ax.plot(s,t,linewidth = 4,color = \'b\',zorder = 1)\n    \n    # scatter points\n    def scatter_pts(self,ax):\n        if np.shape(self.x)[1] == 1:\n            # set plotting limits\n            xmax = copy.deepcopy(max(self.x))\n            xmin = copy.deepcopy(min(self.x))\n            xgap = (xmax - xmin)*0.2\n            xmin -= xgap\n            xmax += xgap\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(self.x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 80)\n\n            # clean up panel\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            \n            # label axes\n            ax.set_xlabel(r\'$x$\', fontsize = 16)\n            ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 15)\n            \n        if np.shape(self.x)[1] == 2:\n            # set plotting limits\n            xmax1 = copy.deepcopy(max(self.x[:,0]))\n            xmin1 = copy.deepcopy(min(self.x[:,0]))\n            xgap1 = (xmax1 - xmin1)*0.35\n            xmin1 -= xgap1\n            xmax1 += xgap1\n            \n            xmax2 = copy.deepcopy(max(self.x[:,0]))\n            xmin2 = copy.deepcopy(min(self.x[:,0]))\n            xgap2 = (xmax2 - xmin2)*0.35\n            xmin2 -= xgap2\n            xmax2 += xgap2\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(self.x[:,0],self.x[:,1],self.y,s = 40,color = \'k\', edgecolor = \'w\',linewidth = 0.9)\n\n            # clean up panel\n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            ax.set_zlim([ymin,ymax])\n            \n            ax.set_xticks(np.arange(round(xmin1) +1, round(xmax1), 1.0))\n            ax.set_yticks(np.arange(round(xmin2) +1, round(xmax2), 1.0))\n\n            # label axes\n            ax.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 5)\n            ax.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n\n            # clean up panel\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n           \n    # plot points on contour\n    def plot_pts_on_contour(self,ax,j,color):\n        # plot connector between points for visualization purposes\n        w_old = self.w_hist[j-1]\n        w_new = self.w_hist[j]\n        g_old = self.least_squares(w_old)\n        g_new = self.least_squares(w_new)\n     \n        ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = color,linewidth = 3,alpha = 1,zorder = 2)      # plot approx\n        ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = \'k\',linewidth = 3 + 1,alpha = 1,zorder = 1)      # plot approx\n    \n    ###### function plotting functions #######\n    def plot_ls_cost(self,**kwargs):\n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(8,3))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0],aspect = \'equal\'); \n        ax2 = plt.subplot(gs[1],projection=\'3d\'); \n        \n        # pull user-defined args\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        num_contours = 15\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']\n        \n        # make contour plot in left panel\n        self.contour_plot(ax1,viewmax,num_contours)\n        \n        # make contour plot in right panel\n        self.surface_plot(ax2,viewmax,view)\n        \n        plt.show()\n        \n    ### visualize the surface plot of cost function ###\n    def surface_plot(self,ax,wmax,view):\n        ##### Produce cost function surface #####\n        wmax += wmax*0.1\n        r = np.linspace(-wmax,wmax,200)\n\n        # create grid from plotting range\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        w_ = np.concatenate((w1_vals,w2_vals),axis = 1)\n        g_vals = []\n        for i in range(len(r)**2):\n            g_vals.append(self.least_squares(w_[i,:]))\n        g_vals = np.asarray(g_vals)\n\n        # reshape and plot the surface, as well as where the zero-plane is\n        w1_vals.shape = (np.size(r),np.size(r))\n        w2_vals.shape = (np.size(r),np.size(r))\n        g_vals.shape = (np.size(r),np.size(r))\n        \n        # plot cost surface\n        ax.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)  \n        \n        # clean up panel\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        ax.set_xlabel(r\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(r\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.set_title(r\'$g\\left(w_0,w_1\\right)$\',fontsize = 13)\n\n        ax.view_init(view[0],view[1])\n        \n    ### visualize contour plot of cost function ###\n    def contour_plot(self,ax,wmax,num_contours):\n        #### define input space for function and evaluate ####\n        w1 = np.linspace(-wmax,wmax,100)\n        w2 = np.linspace(-wmax,wmax,100)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([self.least_squares(s) for s in h])\n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cutoff = 0.5\n        cutoff = (levelmax - levelmin)*cutoff\n        numper = 3\n        levels1 = np.linspace(cutoff,levelmax,numper)\n        num_contours -= numper\n\n        levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n        levels = np.unique(np.append(levels1,levels2))\n        num_contours -= numper\n        while num_contours > 0:\n            cutoff = levels[1]\n            levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n            levels = np.unique(np.append(levels2,levels))\n            num_contours -= numper\n\n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = \'k\')\n        ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = \'Blues\')\n                \n        # clean up panel\n        ax.set_xlabel(\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.set_title(r\'$g\\left(w_0,w_1\\right)$\',fontsize = 13)\n\n        ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.set_xlim([-wmax,wmax])\n        ax.set_ylim([-wmax,wmax])'"
mlrefined_libraries/superlearn_library/lin_regression_demos.py,69,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\n\nclass Visualizer:\n    \'\'\'\n    Visualize linear regression in 2 and 3 dimensions.  For single input cases (2 dimensions) the path of gradient descent on the cost function can be animated.\n    \'\'\'\n    #### initialize ####\n    def __init__(self,data):\n        # grab input\n        self.x = data[:-1,:].T\n        self.y = data[-1:,:].T\n        \n    def center_data(self):\n        # center data\n        self.x = self.x - np.mean(self.x)\n        self.y = self.y - np.mean(self.y)\n        \n    ######## linear regression functions ########    \n    def least_squares(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = copy.deepcopy(self.x[p,:])\n            x_p.shape = (len(x_p),1)\n            y_p = self.y[p]\n            cost +=(w[0] + np.dot(w[1:].T,x_p) - y_p)**2\n        return cost/float(np.size(self.y))\n    \n     ######## 3d animation function ########\n    # animate gradient descent or newton\'s method\n    def animate_it_3d(self,savepath,w_hist,**kwargs):         \n        self.w_hist = w_hist \n        \n        ##### setup figure to plot #####\n        # initialize figure\n        fig = plt.figure(figsize = (8,3))\n        artist = fig\n        \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[2,1]) \n        ax1 = plt.subplot(gs[0],projection=\'3d\'); \n        ax2 = plt.subplot(gs[1]);\n\n        # produce color scheme\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # seed left panel plotting range\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        r = np.linspace(-viewmax,viewmax,200)\n\n        # create grid from plotting range\n        x1_vals,x2_vals = np.meshgrid(r,r)\n        x1_vals.shape = (len(r)**2,1)\n        x2_vals.shape = (len(r)**2,1)\n        \n        x1_vals.shape = (np.size(r),np.size(r))\n        x2_vals.shape = (np.size(r),np.size(r))\n\n        # seed left panel view \n        view = [20,50]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        \n        # set zaxis to the left\n        self.move_axis_left(ax1)\n            \n        # start animation\n        num_frames = len(self.w_hist)\n        print (\'starting animation rendering...\')\n        def animate(k):\n            # clear panels\n            ax1.cla()\n            \n            # set axis in left panel\n            self.move_axis_left(ax1)\n            \n            # current color\n            color = self.colorspec[k]\n\n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print (\'rendering animation frame \' + str(k+1) + \' of \' + str(num_frames))\n            if k == num_frames - 1:\n                print (\'animation rendering complete!\')\n                time.sleep(1.5)\n                clear_output()\n            \n            ###### make left panel - plot data and fit ######\n            # initialize fit\n            w = self.w_hist[k]\n                    \n            # reshape and plot the surface, as well as where the zero-plane is        \n            y_fit = w[0] + w[1]*x1_vals + w[2]*x2_vals\n\n            # plot cost surface\n            ax1.plot_surface(x1_vals,x2_vals,y_fit,alpha = 0.1,color = color,rstride=25, cstride=25,linewidth=0.25,edgecolor = \'k\',zorder = 2)  \n            \n            # scatter data\n            self.scatter_pts(ax1)\n            #ax1.view_init(view[0],view[1])\n            \n            # plot connector between points for visualization purposes\n            if k == 0:\n                w_new = self.w_hist[k]\n                g_new = self.least_squares(w_new)[0]\n                ax2.scatter(k,g_new,s = 0.1,color = \'w\',linewidth = 2.5,alpha = 0,zorder = 1)      # plot approx\n                \n            if k > 0:\n                w_old = self.w_hist[k-1]\n                w_new = self.w_hist[k]\n                g_old = self.least_squares(w_old)[0]\n                g_new = self.least_squares(w_new)[0]\n     \n                ax2.plot([k-1,k],[g_old,g_new],color = color,linewidth = 2.5,alpha = 1,zorder = 2)      # plot approx\n                ax2.plot([k-1,k],[g_old,g_new],color = \'k\',linewidth = 3.5,alpha = 1,zorder = 1)      # plot approx\n            \n            # set viewing limits for second panel\n            ax2.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            ax2.set_xlabel(\'iteration\',fontsize = 12)\n            ax2.set_ylabel(r\'$g(\\mathbf{w})$\',fontsize = 12,rotation = 0,labelpad = 25)\n            ax2.set_xlim([-0.5,len(self.w_hist)])\n            \n            # set axis in left panel\n            self.move_axis_left(ax1)\n                \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if \'fps\' in kwargs:\n            fps = kwargs[\'fps\']\n        anim.save(savepath, fps=fps, extra_args=[\'-vcodec\', \'libx264\'])\n        clear_output()\n\n    # set axis in left panel\n    def move_axis_left(self,ax):\n        tmp_planes = ax.zaxis._PLANES \n        ax.zaxis._PLANES = ( tmp_planes[2], tmp_planes[3], \n                             tmp_planes[0], tmp_planes[1], \n                             tmp_planes[4], tmp_planes[5])\n        view_1 = (25, -135)\n        view_2 = (25, -45)\n        init_view = view_2\n        ax.view_init(*init_view)\n    \n    ######## 2d animation function ########\n    # animate gradient descent or newton\'s method\n    def animate_it_2d(self,savepath,w_hist,**kwargs):       \n        self.w_hist = w_hist\n        \n        ##### setup figure to plot #####\n        # initialize figure\n        fig = plt.figure(figsize = (8,3))\n        artist = fig\n        \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]);\n\n        # produce color scheme\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # seed left panel plotting range\n        xmin = np.min(copy.deepcopy(self.x))\n        xmax = np.max(copy.deepcopy(self.x))\n        xgap = (xmax - xmin)*0.1\n        xmin-=xgap\n        xmax+=xgap\n        x_fit = np.linspace(xmin,xmax,300)\n        \n        # seed right panel contour plot\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        num_contours = 15\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']        \n        self.contour_plot(ax2,viewmax,num_contours)\n        \n        # start animation\n        num_frames = len(self.w_hist)\n        print (\'starting animation rendering...\')\n        def animate(k):\n            # clear panels\n            ax1.cla()\n            \n            # current color\n            color = self.colorspec[k]\n\n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print (\'rendering animation frame \' + str(k+1) + \' of \' + str(num_frames))\n            if k == num_frames - 1:\n                print (\'animation rendering complete!\')\n                time.sleep(1.5)\n                clear_output()\n            \n            ###### make left panel - plot data and fit ######\n            # initialize fit\n            w = self.w_hist[k]\n            y_fit = w[0] + x_fit*w[1]\n            \n            # scatter data\n            self.scatter_pts(ax1)\n            \n            # plot fit to data\n            ax1.plot(x_fit,y_fit,color = color,linewidth = 3) \n\n            ###### make right panel - plot contour and steps ######\n            if k == 0:\n                ax2.scatter(w[0],w[1],s = 90,facecolor = color,edgecolor = \'k\',linewidth = 0.5, zorder = 3)\n            if k > 0 and k < num_frames:\n                self.plot_pts_on_contour(ax2,k,color)\n            if k == num_frames -1:\n                ax2.scatter(w[0],w[1],s = 90,facecolor = color,edgecolor = \'k\',linewidth = 0.5, zorder = 3)\n               \n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if \'fps\' in kwargs:\n            fps = kwargs[\'fps\']\n        anim.save(savepath, fps=fps, extra_args=[\'-vcodec\', \'libx264\'])\n        clear_output()\n\n    ### animate only the fit ###\n    def animate_it_2d_fit_only(self,savepath,w_hist,**kwargs):       \n        self.w_hist = w_hist\n        \n        ##### setup figure to plot #####\n        # initialize figure\n        fig = plt.figure(figsize = (4,4))\n        artist = fig\n        \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 1) \n        ax1 = plt.subplot(gs[0]); \n\n        # produce color scheme\n        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        self.colorspec = []\n        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n        \n        # seed left panel plotting range\n        xmin = np.min(copy.deepcopy(self.x))\n        xmax = np.max(copy.deepcopy(self.x))\n        xgap = (xmax - xmin)*0.1\n        xmin-=xgap\n        xmax+=xgap\n        x_fit = np.linspace(xmin,xmax,300)\n        \n        # seed right panel contour plot\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        \n        # start animation\n        num_frames = len(self.w_hist)\n        print (\'starting animation rendering...\')\n        def animate(k):\n            # clear panels\n            ax1.cla()\n            \n            # current color\n            color = self.colorspec[k]\n\n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print (\'rendering animation frame \' + str(k+1) + \' of \' + str(num_frames))\n            if k == num_frames - 1:\n                print (\'animation rendering complete!\')\n                time.sleep(1.5)\n                clear_output()\n            \n            ###### make left panel - plot data and fit ######\n            # initialize fit\n            w = self.w_hist[k]\n            y_fit = w[0] + x_fit*w[1]\n            \n            # scatter data\n            self.scatter_pts(ax1)\n            \n            # plot fit to data\n            ax1.plot(x_fit,y_fit,color = color,linewidth = 3) \n\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if \'fps\' in kwargs:\n            fps = kwargs[\'fps\']\n        anim.save(savepath, fps=fps, extra_args=[\'-vcodec\', \'libx264\'])\n        clear_output()\n         \n    ###### plot plotting functions ######\n    def plot_data(self):\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(9,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\') \n        ax2 = plt.subplot(gs[1]); \n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n        \n        if np.shape(self.x)[1] == 2:\n            ax2 = plt.subplot(gs[1],projection=\'3d\'); \n\n        # scatter points\n        self.scatter_pts(ax2)\n        \n    def plot_regression_fits(self,final_weights):\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(8,3))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,2,1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\') \n        ax2 = plt.subplot(gs[1]); \n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n \n        # scatter points\n        self.scatter_pts(ax2)      \n        \n        # print regression fits\n        for weights in final_weights:\n            ax2.plot_fit(ax2,weights)\n        \n    # plot regression fits\n    def plot_fit(self,plotting_weights,**kwargs):\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(9,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\') \n        ax = plt.subplot(gs[1]); \n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n        \n        # set plotting limits\n        xmin = np.min(copy.deepcopy(self.x))\n        xmax = np.max(copy.deepcopy(self.x))\n        xgap = (xmax - xmin)*0.25\n        xmin -= xgap\n        xmax += xgap\n\n        ymin = np.min(copy.deepcopy(self.y))\n        ymax = np.max(copy.deepcopy(self.y))\n        ygap = (ymax - ymin)*0.25\n        ymin -= ygap\n        ymax += ygap    \n\n        # initialize points\n        ax.scatter(self.x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40,zorder = 0)\n\n        # clean up panel\n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n\n        # label axes\n        ax.set_xlabel(r\'$x$\', fontsize = 12)\n        ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 12)\n        \n        # create fit\n        s = np.linspace(xmin,xmax,300)\n        colors = [\'k\',\'magenta\']\n        if \'colors\' in kwargs:\n            colors = kwargs[\'colors\']\n \n        transformers = [lambda a: a for i in range(len(plotting_weights))]\n        if \'transformers\' in kwargs:\n            transformers = kwargs[\'transformers\']\n\n        for i in range(len(plotting_weights)):\n            weights = plotting_weights[i]\n            transformer = transformers[i]\n            t = weights[0] + weights[1]*transformer(s).flatten()\n            ax.plot(s,t,linewidth = 2,color = colors[i],zorder = 3)\n            c+=1\n    \n    # scatter points\n    def scatter_pts(self,ax):\n        if np.shape(self.x)[1] == 1:\n            # set plotting limits\n            xmin = np.min(copy.deepcopy(self.x))\n            xmax = np.max(copy.deepcopy(self.x))\n            xgap = (xmax - xmin)*0.2\n            xmin -= xgap\n            xmax += xgap\n            \n            ymin = np.min(copy.deepcopy(self.y))\n            ymax = np.max(copy.deepcopy(self.y))\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(self.x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n\n            # clean up panel\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            \n            # label axes\n            ax.set_xlabel(r\'$x$\', fontsize = 16)\n            ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 16,labelpad = 15)\n            \n        if np.shape(self.x)[1] == 2:\n            # set plotting limits\n            xmin1 = np.min(copy.deepcopy(self.x[:,0]))\n            xmax1 = np.max(copy.deepcopy(self.x[:,0])) \n            xgap1 = (xmax1 - xmin1)*0.35\n            xmin1 -= xgap1\n            xmax1 += xgap1\n            \n            xmin2 = np.min(copy.deepcopy(self.x[:,1]))\n            xmax2 = np.max(copy.deepcopy(self.x[:,1])) \n            xgap2 = (xmax2 - xmin2)*0.35\n            xmin2 -= xgap2\n            xmax2 += xgap2\n            \n            ymin = np.min(copy.deepcopy(self.y))\n            ymax = np.max(copy.deepcopy(self.y))\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(self.x[:,0],self.x[:,1],self.y,s = 40,color = \'k\', edgecolor = \'w\',linewidth = 0.9)\n\n            # clean up panel\n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            ax.set_zlim([ymin,ymax])\n            \n            ax.set_xticks(np.arange(round(xmin1) +1, round(xmax1), 1.0))\n            ax.set_yticks(np.arange(round(xmin2) +1, round(xmax2), 1.0))\n\n            # label axes\n            ax.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 5)\n            ax.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n\n            # clean up panel\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n           \n    # plot points on contour\n    def plot_pts_on_contour(self,ax,j,color):\n        # plot connector between points for visualization purposes\n        w_old = self.w_hist[j-1]\n        w_new = self.w_hist[j]\n        g_old = self.least_squares(w_old)\n        g_new = self.least_squares(w_new)\n     \n        ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = color,linewidth = 3,alpha = 1,zorder = 2)      # plot approx\n        ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = \'k\',linewidth = 3 + 1,alpha = 1,zorder = 1)      # plot approx\n    \n    ###### function plotting functions #######\n    def plot_ls_cost(self,**kwargs):\n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(8,3))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0],aspect = \'equal\'); \n        ax2 = plt.subplot(gs[1],projection=\'3d\'); \n        \n        # pull user-defined args\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        num_contours = 15\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']\n        \n        # make contour plot in left panel\n        self.contour_plot(ax1,viewmax,num_contours)\n        \n        # make contour plot in right panel\n        self.surface_plot(ax2,viewmax,view)\n        \n        plt.show()\n        \n    ### visualize the surface plot of cost function ###\n    def surface_plot(self,ax,wmax,view):\n        ##### Produce cost function surface #####\n        wmax += wmax*0.1\n        r = np.linspace(-wmax,wmax,200)\n\n        # create grid from plotting range\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        w_ = np.concatenate((w1_vals,w2_vals),axis = 1)\n        g_vals = []\n        for i in range(len(r)**2):\n            g_vals.append(self.least_squares(w_[i,:]))\n        g_vals = np.asarray(g_vals)\n\n        # reshape and plot the surface, as well as where the zero-plane is\n        w1_vals.shape = (np.size(r),np.size(r))\n        w2_vals.shape = (np.size(r),np.size(r))\n        g_vals.shape = (np.size(r),np.size(r))\n        \n        # plot cost surface\n        ax.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)  \n        \n        # clean up panel\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        ax.set_xlabel(r\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(r\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.set_title(r\'$g\\left(w_0,w_1\\right)$\',fontsize = 13)\n\n        ax.view_init(view[0],view[1])\n        \n    ### visualize contour plot of cost function ###\n    def contour_plot(self,ax,wmax,num_contours):\n        #### define input space for function and evaluate ####\n        w1 = np.linspace(-wmax,wmax,100)\n        w2 = np.linspace(-wmax,wmax,100)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([self.least_squares(s) for s in h])\n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cutoff = 0.5\n        cutoff = (levelmax - levelmin)*cutoff\n        numper = 3\n        levels1 = np.linspace(cutoff,levelmax,numper)\n        num_contours -= numper\n\n        levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n        levels = np.unique(np.append(levels1,levels2))\n        num_contours -= numper\n        while num_contours > 0:\n            cutoff = levels[1]\n            levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n            levels = np.unique(np.append(levels2,levels))\n            num_contours -= numper\n\n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = \'k\')\n        ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = \'Blues\')\n                \n        # clean up panel\n        ax.set_xlabel(\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.set_title(r\'$g\\left(w_0,w_1\\right)$\',fontsize = 13)\n\n        ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.set_xlim([-wmax,wmax])\n        ax.set_ylim([-wmax,wmax])'"
mlrefined_libraries/superlearn_library/logistic_regression_simple_demos.py,46,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\n\n\nclass visualizer:\n    \'\'\'\n    Visualize linear regression applied to a 2-class dataset.\n    \'\'\'\n    #### initialize ####\n    def __init__(self,data):\n        # grab input\n        data = data.T\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n\n    def center_data(self):\n        # center data\n        self.x = self.x - np.mean(self.x)\n        self.y = self.y - np.mean(self.y)\n        \n    def run_algo(self,algo,**kwargs):\n        # Get function and compute gradient\n        self.g = self.linear_least_squares\n        self.grad = compute_grad(self.g)\n        \n        # choose algorithm\n        self.algo = algo\n        if self.algo == \'gradient_descent\':\n            self.alpha = 10**-3\n            if \'alpha\' in kwargs:\n                self.alpha = kwargs[\'alpha\']\n        \n        self.max_its = 10\n        if \'max_its\' in kwargs:\n            self.max_its = kwargs[\'max_its\']\n            \n        self.w_init = np.random.randn(2)\n        if \'w_init\' in kwargs:\n            self.w_init = kwargs[\'w_init\']\n            self.w_init = np.asarray([float(s) for s in self.w_init])\n            self.w_init.shape = (np.size(self.w_init),1)\n            \n        # run algorithm of choice\n        if self.algo == \'gradient_descent\':\n            self.w_hist = []\n            self.gradient_descent()\n        if self.algo == \'newtons_method\':\n            self.hess = compute_hess(self.g)           # hessian of input function\n            self.beta = 0\n            if \'beta\' in kwargs:\n                self.beta = kwargs[\'beta\']\n            self.w_hist = []\n            self.newtons_method()  \n    \n    ######## linear regression functions ########    \n    def linear_least_squares(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = copy.deepcopy(self.x[p,:])\n            x_p.shape = (len(x_p),1)\n            y_p = self.y[p]\n            cost +=(w[0] + np.dot(w[1:].T,x_p) - y_p)**2\n        return cost\n    \n    def counting_cost(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = copy.deepcopy(self.x[p,:])\n            x_p.shape = (len(x_p),1)\n            y_p = self.y[p]\n            cost +=(np.sign(w[0] + np.dot(w[1:].T,x_p)) - y_p)**2\n        return cost\n    \n    # run gradient descent\n    def gradient_descent(self):\n        w = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w)\n        for k in range(self.max_its):   \n            # plug in value into func and derivative\n            grad_eval = self.grad(w)\n            grad_eval.shape = (len(w),1)\n            \n            grad_norm = np.linalg.norm(grad_eval)\n            if grad_norm == 0:\n                grad_norm += 10**-6*np.sign(2*np.random.rand(1) - 1)\n            grad_eval /= grad_norm\n            \n            # decide on alpha\n            alpha = self.alpha\n            if self.alpha == \'backtracking\':\n                alpha = self.backtracking(w,grad_val)\n            \n            # take newtons step\n            w = w - alpha*grad_eval\n            \n            # record\n            self.w_hist.append(w)     \n    \n    #### run newton\'s method ####\n    def newtons_method(self):\n        w = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w)\n        for k in range(self.max_its):\n            # plug in value into func and derivative\n            grad_eval = self.grad(w)\n            hess_eval = self.hess(w)\n            \n            # reshape for numpy linalg functionality\n            hess_eval.shape = (int((np.size(hess_eval))**(0.5)),int((np.size(hess_eval))**(0.5)))\n\n            # solve linear system for weights\n            w = w - np.dot(np.linalg.pinv(hess_eval + self.beta*np.eye(np.size(w))),grad_eval)\n                                \n            # record\n            self.w_hist.append(w)   \n            \n    ### demo 1 - fitting a line to a step dataset, then taking sign of this line ###\n    def naive_fitting_demo(self,**kwargs):\n        ##### setup figure to plot #####\n        # initialize figure\n        fig = plt.figure(figsize = (8,4))\n        artist = fig\n        \n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(2, 1, height_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0],aspect = \'equal\'); \n        ax2 = plt.subplot(gs[1],aspect = \'equal\');\n    \n        #### plot data in both panels ####\n        self.scatter_pts(ax1)\n        self.scatter_pts(ax2)\n        \n        #### fit line to data and plot ####\n        # make plotting range\n        xmin = copy.deepcopy(min(self.x))\n        xmax = copy.deepcopy(max(self.x))\n        xgap = (xmax - xmin)*0.4\n        xmin-=xgap\n        xmax+=xgap\n        \n        # produce fit\n        x_fit = np.linspace(xmin,xmax,300)    \n        w = self.w_hist[-1]\n        y_fit = w[0] + x_fit*w[1]\n        \n        # plot linear fit\n        ax2.plot(x_fit,y_fit,color = \'lime\',linewidth = 1.5) \n        \n        # plot sign version of linear fit\n        f = np.sign(y_fit)\n        bot_ind = np.argwhere(f == -1)\n        bot_ind = [s[0] for s in bot_ind]\n        bot_in = x_fit[bot_ind]\n        bot_out = f[bot_ind]\n        ax2.plot(bot_in,bot_out,color = \'r\',linewidth = 1.5,linestyle = \'--\') \n\n        top_ind = np.argwhere(f == +1)\n        top_ind = [s[0] for s in top_ind]\n        top_in = x_fit[top_ind]\n        top_out = f[top_ind]\n        ax2.plot(top_in,top_out,color = \'r\',linewidth = 1.5,linestyle = \'--\') \n        \n    # scatter points\n    def scatter_pts(self,ax):\n        if np.shape(self.x)[1] == 1:\n            # set plotting limits\n            xmax = copy.deepcopy(max(self.x))\n            xmin = copy.deepcopy(min(self.x))\n            xgap = (xmax - xmin)*0.4\n            xmin -= xgap\n            xmax += xgap\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.4\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(self.x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n\n            # clean up panel\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            \n            # label axes\n            ax.set_xlabel(r\'$x$\', fontsize = 12)\n            ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 12)\n            \n            ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            \n        if np.shape(self.x)[1] == 2:\n            # set plotting limits\n            xmax1 = copy.deepcopy(max(self.x[:,0]))\n            xmin1 = copy.deepcopy(min(self.x[:,0]))\n            xgap1 = (xmax1 - xmin1)*0.35\n            xmin1 -= xgap1\n            xmax1 += xgap1\n            \n            xmax2 = copy.deepcopy(max(self.x[:,0]))\n            xmin2 = copy.deepcopy(min(self.x[:,0]))\n            xgap2 = (xmax2 - xmin2)*0.35\n            xmin2 -= xgap2\n            xmax2 += xgap2\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(self.x[:,0],self.x[:,1],self.y,s = 40,color = \'k\', edgecolor = \'w\',linewidth = 0.9)\n\n            # clean up panel\n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            ax.set_zlim([ymin,ymax])\n            \n            ax.set_xticks(np.arange(round(xmin1) +1, round(xmax1), 1.0))\n            ax.set_yticks(np.arange(round(xmin2) +1, round(xmax2), 1.0))\n\n            # label axes\n            ax.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 5)\n            ax.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n\n            # clean up panel\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n                   \n    ###### function plotting functions #######\n    def plot_cost(self,**kwargs):\n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(8,3))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0],aspect = \'equal\'); \n        ax2 = plt.subplot(gs[1],projection=\'3d\'); \n        \n        # pull user-defined args\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        num_contours = 15\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']\n        cost = \'counting_cost\'\n        if \'cost\' in kwargs:\n            cost = kwargs[\'cost\']\n        \n        # make contour plot in left panel\n        self.contour_plot(ax1,viewmax,num_contours,cost)\n        \n        if cost == \'counting_cost\':\n            self.counting_cost_surface(ax2,viewmax)\n            \n        # make contour plot in right panel\n        #self.surface_plot(ax2,viewmax,view,cost)\n        \n        plt.show()\n        \n    # plot counting cost\n    def plot_counting_cost(self,**kwargs):\n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(8,3))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3,width_ratios=[1,3,1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\'); \n        ax = plt.subplot(gs[1],projection=\'3d\'); \n        ax2 = plt.subplot(gs[2]); ax2.axis(\'off\'); \n\n        # pull user-defined args\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        \n        # define coordinate system\n        r = np.linspace(-viewmax,viewmax,300)    \n        s,t = np.meshgrid(r,r)\n        s.shape = (np.prod(np.shape(s)),1)\n        t.shape = (np.prod(np.shape(t)),1)\n        w_ = np.concatenate((s,t),axis=1)\n\n        # define cost surface\n        g_vals = []\n        for i in range(len(r)**2):\n            g_vals.append(self.counting_cost(w_[i,:]))\n        g_vals = np.asarray(g_vals)\n            \n        # loop over levels and print\n        s.shape = (len(r),len(r))\n        t.shape = (len(r),len(r))\n\n        levels = np.unique(g_vals)\n        for u in levels:\n            # make copy of cost and nan out all non level entries\n            z = g_vals.copy()\n            ind = np.argwhere(z != u)\n            ind = [v[0] for v in ind]\n            z[ind] = np.nan\n\n            # plot the current level\n            z.shape = (len(r),len(r)) \n            ax.plot_surface(s,t,z,alpha = 0.4,color = \'#696969\',zorder = 0,shade = True,linewidth=0)\n\n        # set viewing angle\n        ax.view_init(5,126)\n\n        # clean up panel\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        ax.set_xlabel(r\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(r\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.set_title(r\'$g\\left(w_0,w_1\\right)$\',fontsize = 13)\n        \n        \n    ### visualize the surface plot of cost function ###\n    def surface_plot(self,ax,wmax,view,cost):\n        ##### Produce cost function surface #####\n        wmax += wmax*0.1\n        r = np.linspace(-wmax,wmax,200)\n\n        # create grid from plotting range\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        w_ = np.concatenate((w1_vals,w2_vals),axis = 1)\n        g_vals = []\n        if cost == \'counting_cost\':\n            for i in range(len(r)**2):\n                g_vals.append(self.counting_cost(w_[i,:]))\n            g_vals = np.asarray(g_vals)\n        \n        \'\'\'\n        for i in range(len(r)**2):\n            g_vals.append(self.least_squares(w_[i,:]))\n        g_vals = np.asarray(g_vals)\n        \'\'\'\n\n        # reshape and plot the surface, as well as where the zero-plane is\n        w1_vals.shape = (np.size(r),np.size(r))\n        w2_vals.shape = (np.size(r),np.size(r))\n        g_vals.shape = (np.size(r),np.size(r))\n        \n        # plot cost surface\n        ax.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)  \n        \n        # clean up panel\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        ax.set_xlabel(r\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(r\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.set_title(r\'$g\\left(w_0,w_1\\right)$\',fontsize = 13)\n\n        ax.view_init(view[0],view[1])\n        \n    ### visualize contour plot of cost function ###\n    def contour_plot(self,ax,wmax,num_contours,cost):\n        #### define input space for function and evaluate ####\n        w1 = np.linspace(-wmax,wmax,300)\n        w2 = np.linspace(-wmax,wmax,300)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = []\n        func_vals = np.asarray([self.least_squares(s) for s in h])\n            \n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cutoff = 0.5\n        cutoff = (levelmax - levelmin)*cutoff\n        numper = 3\n        levels1 = np.linspace(cutoff,levelmax,numper)\n        num_contours -= numper\n\n        levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n        levels = np.unique(np.append(levels1,levels2))\n        num_contours -= numper\n        while num_contours > 0:\n            cutoff = levels[1]\n            levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n            levels = np.unique(np.append(levels2,levels))\n            num_contours -= numper\n\n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = \'k\')\n        ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = \'Blues\')\n                \n        # clean up panel\n        ax.set_xlabel(\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.set_title(r\'$g\\left(w_0,w_1\\right)$\',fontsize = 13)\n\n        ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.set_xlim([-wmax,wmax])\n        ax.set_ylim([-wmax,wmax])        '"
mlrefined_libraries/superlearn_library/logistic_regression_simple_demos_entropy.py,46,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\n\n\nclass visualizer:\n    \'\'\'\n    Visualize linear regression applied to a 2-class dataset.\n    \'\'\'\n    #### initialize ####\n    def __init__(self,data):\n        # grab input\n        data = data.T\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n\n    def center_data(self):\n        # center data\n        self.x = self.x - np.mean(self.x)\n        self.y = self.y - np.mean(self.y)\n        \n    def run_algo(self,algo,**kwargs):\n        # Get function and compute gradient\n        self.g = self.linear_least_squares\n        self.grad = compute_grad(self.g)\n        \n        # choose algorithm\n        self.algo = algo\n        if self.algo == \'gradient_descent\':\n            self.alpha = 10**-3\n            if \'alpha\' in kwargs:\n                self.alpha = kwargs[\'alpha\']\n        \n        self.max_its = 10\n        if \'max_its\' in kwargs:\n            self.max_its = kwargs[\'max_its\']\n            \n        self.w_init = np.random.randn(2)\n        if \'w_init\' in kwargs:\n            self.w_init = kwargs[\'w_init\']\n            self.w_init = np.asarray([float(s) for s in self.w_init])\n            self.w_init.shape = (np.size(self.w_init),1)\n            \n        # run algorithm of choice\n        if self.algo == \'gradient_descent\':\n            self.w_hist = []\n            self.gradient_descent()\n        if self.algo == \'newtons_method\':\n            self.hess = compute_hess(self.g)           # hessian of input function\n            self.beta = 0\n            if \'beta\' in kwargs:\n                self.beta = kwargs[\'beta\']\n            self.w_hist = []\n            self.newtons_method()  \n    \n    ######## linear regression functions ########    \n    def linear_least_squares(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = copy.deepcopy(self.x[p,:])\n            x_p.shape = (len(x_p),1)\n            y_p = self.y[p]\n            cost +=(w[0] + np.dot(w[1:].T,x_p) - y_p)**2\n        return cost\n    \n    def identity(self,t):\n        val = 0\n        if t > 0.5:\n            val = 1\n        return val\n        \n    def counting_cost(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            # strip of pth point\n            x_p = copy.deepcopy(self.x[p,:])\n            x_p.shape = (len(x_p),1)\n            y_p = self.y[p]\n            \n            # produce predicted label\n            val_p = self.identity(w[0] + np.dot(w[1:].T,x_p)) \n\n            # compare with true label\n            if val_p != y_p:\n                cost += 1\n        return cost\n    \n    # run gradient descent\n    def gradient_descent(self):\n        w = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w)\n        for k in range(self.max_its):   \n            # plug in value into func and derivative\n            grad_eval = self.grad(w)\n            grad_eval.shape = (len(w),1)\n            \n            grad_norm = np.linalg.norm(grad_eval)\n            if grad_norm == 0:\n                grad_norm += 10**-6*np.sign(2*np.random.rand(1) - 1)\n            grad_eval /= grad_norm\n            \n            # decide on alpha\n            alpha = self.alpha\n            if self.alpha == \'backtracking\':\n                alpha = self.backtracking(w,grad_val)\n            \n            # take newtons step\n            w = w - alpha*grad_eval\n            \n            # record\n            self.w_hist.append(w)     \n    \n    #### run newton\'s method ####\n    def newtons_method(self):\n        w = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w)\n        for k in range(self.max_its):\n            # plug in value into func and derivative\n            grad_eval = self.grad(w)\n            hess_eval = self.hess(w)\n            \n            # reshape for numpy linalg functionality\n            hess_eval.shape = (int((np.size(hess_eval))**(0.5)),int((np.size(hess_eval))**(0.5)))\n\n            # solve linear system for weights\n            w = w - np.dot(np.linalg.pinv(hess_eval + self.beta*np.eye(np.size(w))),grad_eval)\n                                \n            # record\n            self.w_hist.append(w)   \n            \n    ### demo 1 - fitting a line to a step dataset, then taking sign of this line ###\n    def naive_fitting_demo(self,**kwargs):\n        ##### setup figure to plot #####\n        # initialize figure\n        fig = plt.figure(figsize = (8,4))\n        artist = fig\n        \n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(2, 1, height_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0],aspect = \'equal\'); \n        ax2 = plt.subplot(gs[1],aspect = \'equal\');\n    \n        #### plot data in both panels ####\n        self.scatter_pts(ax1)\n        self.scatter_pts(ax2)\n        \n        #### fit line to data and plot ####\n        # make plotting range\n        xmin = copy.deepcopy(min(self.x))\n        xmax = copy.deepcopy(max(self.x))\n        xgap = (xmax - xmin)*0.4\n        xmin-=xgap\n        xmax+=xgap\n        \n        # produce fit\n        x_fit = np.linspace(xmin,xmax,300)    \n        w = self.w_hist[-1]\n        y_fit = w[0] + x_fit*w[1]\n        \n        # plot linear fit\n        ax2.plot(x_fit,y_fit,color = \'lime\',linewidth = 1.5) \n        \n        # plot sign version of linear fit\n        f = np.array([self.identity(v) for v in y_fit])\n        bot_ind = np.argwhere(f == 0)\n        bot_ind = [s[0] for s in bot_ind]\n        bot_in = x_fit[bot_ind]\n        bot_out = f[bot_ind]\n        ax2.plot(bot_in,bot_out,color = \'r\',linewidth = 1.5,linestyle = \'--\') \n\n        top_ind = np.argwhere(f == +1)\n        top_ind = [s[0] for s in top_ind]\n        top_in = x_fit[top_ind]\n        top_out = f[top_ind]\n        ax2.plot(top_in,top_out,color = \'r\',linewidth = 1.5,linestyle = \'--\') \n        \n    # scatter points\n    def scatter_pts(self,ax):\n        if np.shape(self.x)[1] == 1:\n            # set plotting limits\n            xmax = copy.deepcopy(max(self.x))\n            xmin = copy.deepcopy(min(self.x))\n            xgap = (xmax - xmin)*0.4\n            xmin -= xgap\n            xmax += xgap\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.4\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(self.x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 40)\n\n            # clean up panel\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            \n            # label axes\n            ax.set_xlabel(r\'$x$\', fontsize = 12)\n            ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = 10)\n            \n            ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n            \n        if np.shape(self.x)[1] == 2:\n            # set plotting limits\n            xmax1 = copy.deepcopy(max(self.x[:,0]))\n            xmin1 = copy.deepcopy(min(self.x[:,0]))\n            xgap1 = (xmax1 - xmin1)*0.35\n            xmin1 -= xgap1\n            xmax1 += xgap1\n            \n            xmax2 = copy.deepcopy(max(self.x[:,0]))\n            xmin2 = copy.deepcopy(min(self.x[:,0]))\n            xgap2 = (xmax2 - xmin2)*0.35\n            xmin2 -= xgap2\n            xmax2 += xgap2\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(self.x[:,0],self.x[:,1],self.y,s = 40,color = \'k\', edgecolor = \'w\',linewidth = 0.9)\n\n            # clean up panel\n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            ax.set_zlim([ymin,ymax])\n            \n            ax.set_xticks(np.arange(round(xmin1) +1, round(xmax1), 1.0))\n            ax.set_yticks(np.arange(round(xmin2) +1, round(xmax2), 1.0))\n\n            # label axes\n            ax.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 5)\n            ax.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n\n            # clean up panel\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n                   \n    ###### function plotting functions #######\n    def plot_cost(self,**kwargs):\n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(8,3))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0],aspect = \'equal\'); \n        ax2 = plt.subplot(gs[1],projection=\'3d\'); \n        \n        # pull user-defined args\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        num_contours = 15\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']\n        cost = \'counting_cost\'\n        if \'cost\' in kwargs:\n            cost = kwargs[\'cost\']\n        \n        # make contour plot in left panel\n        self.contour_plot(ax1,viewmax,num_contours,cost)\n        \n        if cost == \'counting_cost\':\n            self.counting_cost_surface(ax2,viewmax)\n            \n        # make contour plot in right panel\n        #self.surface_plot(ax2,viewmax,view,cost)\n        \n        plt.show()\n        \n    # plot counting cost\n    def plot_counting_cost(self,**kwargs):\n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(8,3))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3,width_ratios=[1,3,1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\'); \n        ax = plt.subplot(gs[1],projection=\'3d\'); \n        ax2 = plt.subplot(gs[2]); ax2.axis(\'off\'); \n\n        # pull user-defined args\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        \n        # define coordinate system\n        r = np.linspace(-viewmax,viewmax,300)    \n        s,t = np.meshgrid(r,r)\n        s.shape = (np.prod(np.shape(s)),1)\n        t.shape = (np.prod(np.shape(t)),1)\n        w_ = np.concatenate((s,t),axis=1)\n\n        # define cost surface\n        g_vals = []\n        for i in range(len(r)**2):\n            g_vals.append(self.counting_cost(w_[i,:]))\n        g_vals = np.asarray(g_vals)\n            \n        # loop over levels and print\n        s.shape = (len(r),len(r))\n        t.shape = (len(r),len(r))\n\n        levels = np.unique(g_vals)\n        for u in levels:\n            # make copy of cost and nan out all non level entries\n            z = g_vals.copy()\n            ind = np.argwhere(z != u)\n            ind = [v[0] for v in ind]\n            z[ind] = np.nan\n\n            # plot the current level\n            z.shape = (len(r),len(r)) \n            ax.plot_surface(s,t,z,alpha = 0.4,color = \'#696969\',zorder = 0,shade = True,linewidth=0)\n\n        # set viewing angle\n        ax.view_init(5,126)\n\n        # clean up panel\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        ax.set_xlabel(r\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(r\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.set_title(r\'$g\\left(w_0,w_1\\right)$\',fontsize = 13)\n        \n        \n    ### visualize the surface plot of cost function ###\n    def surface_plot(self,ax,wmax,view,cost):\n        ##### Produce cost function surface #####\n        wmax += wmax*0.1\n        r = np.linspace(-wmax,wmax,200)\n\n        # create grid from plotting range\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        w_ = np.concatenate((w1_vals,w2_vals),axis = 1)\n        g_vals = []\n        if cost == \'counting_cost\':\n            for i in range(len(r)**2):\n                g_vals.append(self.counting_cost(w_[i,:]))\n            g_vals = np.asarray(g_vals)\n        \n        \'\'\'\n        for i in range(len(r)**2):\n            g_vals.append(self.least_squares(w_[i,:]))\n        g_vals = np.asarray(g_vals)\n        \'\'\'\n\n        # reshape and plot the surface, as well as where the zero-plane is\n        w1_vals.shape = (np.size(r),np.size(r))\n        w2_vals.shape = (np.size(r),np.size(r))\n        g_vals.shape = (np.size(r),np.size(r))\n        \n        # plot cost surface\n        ax.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)  \n        \n        # clean up panel\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        ax.set_xlabel(r\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(r\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.set_title(r\'$g\\left(w_0,w_1\\right)$\',fontsize = 13)\n\n        ax.view_init(view[0],view[1])\n        \n    ### visualize contour plot of cost function ###\n    def contour_plot(self,ax,wmax,num_contours,cost):\n        #### define input space for function and evaluate ####\n        w1 = np.linspace(-wmax,wmax,300)\n        w2 = np.linspace(-wmax,wmax,300)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = []\n        func_vals = np.asarray([self.least_squares(s) for s in h])\n            \n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cutoff = 0.5\n        cutoff = (levelmax - levelmin)*cutoff\n        numper = 3\n        levels1 = np.linspace(cutoff,levelmax,numper)\n        num_contours -= numper\n\n        levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n        levels = np.unique(np.append(levels1,levels2))\n        num_contours -= numper\n        while num_contours > 0:\n            cutoff = levels[1]\n            levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n            levels = np.unique(np.append(levels2,levels))\n            num_contours -= numper\n\n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = \'k\')\n        ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = \'Blues\')\n                \n        # clean up panel\n        ax.set_xlabel(\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.set_title(r\'$g\\left(w_0,w_1\\right)$\',fontsize = 13)\n\n        ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.set_xlim([-wmax,wmax])\n        ax.set_ylim([-wmax,wmax])        '"
mlrefined_libraries/superlearn_library/multi_lin_classification_demo.py,34,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\n\nclass Visualizer:\n    \'\'\'\n    Visualize linear classification and fits in 2d (for N=1 dimensional input datasets)\n    \'\'\'\n    #### initialize ####\n    def __init__(self,data):\n        # grab input\n        data = data.T\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n   \n    ###### plot plotting functions for single input multiclass dataset ######\n    def plot_data(self):\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(9,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\') \n        ax2 = plt.subplot(gs[1]); \n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n\n        # scatter points\n        xmin,xmax = self.scatter_pts(ax2,self.x,self.y)\n      \n    # scatter points\n    def scatter_pts(self,ax,x,y):\n        # set plotting limits\n        xmax = copy.deepcopy(np.max(x))\n        xmin = copy.deepcopy(np.min(x))\n        xgap = (xmax - xmin)*0.1\n        xmin -= xgap\n        xmax += xgap\n\n        ymax = copy.deepcopy(np.max(y))\n        ymin = copy.deepcopy(np.min(y))\n        ygap = (ymax - ymin)*0.25\n        ymin -= ygap\n        ymax += ygap    \n\n        # initialize points\n        ax.scatter(x,y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 60)\n\n        # clean up panel\n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n\n        # label axes\n        ax.set_xlabel(r\'$x$\', fontsize = 15)\n        ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 15,labelpad = 15)\n        \n        return xmin,xmax\n        \n    # plot regression fits\n    def plot_fit(self,weights,**kwargs):\n        # construct figure\n        fig, axs = plt.subplots(1, 3, figsize=(9,4))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 3, width_ratios=[1,5,1]) \n        ax1 = plt.subplot(gs[0]); ax1.axis(\'off\') \n        ax2 = plt.subplot(gs[1]); \n        ax3 = plt.subplot(gs[2]); ax3.axis(\'off\')\n    \n        # scatter points\n        xmin,xmax = self.scatter_pts(ax2,self.x,self.y)\n        \n        # create fit\n        s = np.linspace(xmin,xmax,300)[np.newaxis,:]\n        colors = [\'k\',\'magenta\']\n        if \'colors\' in kwargs:\n            colors = kwargs[\'colors\']\n        c = 0\n        transformer = lambda a: a\n        if \'transformer\' in kwargs:\n            transformer = kwargs[\'transformer\']\n        a = self.model(transformer(s),weights)\n\n        # plot counting cost \n        t = np.argmax(a,axis = 1).flatten()\n        ax2.plot(s.flatten(),t,linewidth = 4,color = \'b\',zorder = 2)\n    \n    # compute linear combination of input point\n    def model(self,x,w):\n        # tack a 1 onto the top of each input point all at once\n        o = np.ones((1,np.shape(x)[1]))\n        x = np.vstack((o,x))\n\n        # compute linear combination and return\n        a = np.dot(x.T,w)\n        return a\n    \n    ####### plot each individual classifier trained and show on each two-class subproblem ######\n    def plot_subproblem_data(self):\n        C = len(np.unique(self.y))\n        \n        # construct figure\n        fig = plt.figure(figsize=(9,2.5))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, C) \n        \n        # scatter points\n        for c in range(C):\n            # create subproblem data\n            y_temp = copy.deepcopy(self.y)\n            ind = np.argwhere(y_temp.astype(int) == (c))\n            ind = ind[:,0]\n            ind2 = np.argwhere(y_temp.astype(int) != (c))\n            ind2 = ind2[:,0]\n            y_temp[ind] = 1\n            y_temp[ind2] = -1\n        \n            # create new axis to plot\n            ax = plt.subplot(gs[c])\n            xmin,xmax = self.scatter_pts(ax,self.x,y_temp)\n            \n            # pretty up panel\n            title = \'class \' + str(c+1) + \' versus all\'\n            ax.set_title(title,fontsize = 14)\n    \n    # plot subproblem data and plot\n    def plot_subproblem_fits(self,weights,**kwargs):\n        C = len(np.unique(self.y))\n        \n        # construct figure\n        fig = plt.figure(figsize=(9,2.5))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, C) \n        \n        # scatter points\n        for c in range(C):\n            # create subproblem data\n            y_temp = copy.deepcopy(self.y)\n            ind = np.argwhere(y_temp.astype(int) == (c))\n            ind = ind[:,0]\n            ind2 = np.argwhere(y_temp.astype(int) != (c))\n            ind2 = ind2[:,0]\n            y_temp[ind] = 1\n            y_temp[ind2] = -1\n        \n            # create new axis to plot\n            ax = plt.subplot(gs[c])\n            xmin,xmax = self.scatter_pts(ax,self.x,y_temp)\n            \n            # create fit\n            s = np.linspace(xmin,xmax,300)[np.newaxis,:]\n            transformer = lambda a: a\n            if \'transformer\' in kwargs:\n                transformer = kwargs[\'transformer\']\n            a = self.model(transformer(s),weights[:,c])\n\n            # plot counting cost \n            t = np.sign(a).flatten() \n            ax.plot(s.flatten(),t,linewidth = 4,color = \'b\',zorder = 2)\n            \n            # pretty up panel\n            title = \'class \' + str(c+1) + \' versus all\'\n            ax.set_title(title,fontsize = 14)\n\n    # plot points on contour\n    def plot_pts_on_contour(self,ax,j,color):\n        # plot connector between points for visualization purposes\n        w_old = self.w_hist[j-1]\n        w_new = self.w_hist[j]\n        g_old = self.least_squares(w_old)\n        g_new = self.least_squares(w_new)\n     \n        ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = color,linewidth = 3,alpha = 1,zorder = 2)      # plot approx\n        ax.plot([w_old[0],w_new[0]],[w_old[1],w_new[1]],color = \'k\',linewidth = 3 + 1,alpha = 1,zorder = 1)      # plot approx\n    \n    ###### function plotting functions #######\n    def plot_ls_cost(self,**kwargs):\n        # construct figure\n        fig, axs = plt.subplots(1, 2, figsize=(8,3))\n\n        # create subplot with 2 panels\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0],aspect = \'equal\'); \n        ax2 = plt.subplot(gs[1],projection=\'3d\'); \n        \n        # pull user-defined args\n        viewmax = 3\n        if \'viewmax\' in kwargs:\n            viewmax = kwargs[\'viewmax\']\n        view = [20,100]\n        if \'view\' in kwargs:\n            view = kwargs[\'view\']\n        num_contours = 15\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']\n        \n        # make contour plot in left panel\n        self.contour_plot(ax1,viewmax,num_contours)\n        \n        # make contour plot in right panel\n        self.surface_plot(ax2,viewmax,view)\n        \n        plt.show()\n        \n    ### visualize the surface plot of cost function ###\n    def surface_plot(self,ax,wmax,view):\n        ##### Produce cost function surface #####\n        wmax += wmax*0.1\n        r = np.linspace(-wmax,wmax,200)\n\n        # create grid from plotting range\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        w_ = np.concatenate((w1_vals,w2_vals),axis = 1)\n        g_vals = []\n        for i in range(len(r)**2):\n            g_vals.append(self.least_squares(w_[i,:]))\n        g_vals = np.asarray(g_vals)\n\n        # reshape and plot the surface, as well as where the zero-plane is\n        w1_vals.shape = (np.size(r),np.size(r))\n        w2_vals.shape = (np.size(r),np.size(r))\n        g_vals.shape = (np.size(r),np.size(r))\n        \n        # plot cost surface\n        ax.plot_surface(w1_vals,w2_vals,g_vals,alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)  \n        \n        # clean up panel\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n\n        ax.set_xlabel(r\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(r\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.set_title(r\'$g\\left(w_0,w_1\\right)$\',fontsize = 13)\n\n        ax.view_init(view[0],view[1])\n        \n    ### visualize contour plot of cost function ###\n    def contour_plot(self,ax,wmax,num_contours):\n        #### define input space for function and evaluate ####\n        w1 = np.linspace(-wmax,wmax,100)\n        w2 = np.linspace(-wmax,wmax,100)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([self.least_squares(s) for s in h])\n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cutoff = 0.5\n        cutoff = (levelmax - levelmin)*cutoff\n        numper = 3\n        levels1 = np.linspace(cutoff,levelmax,numper)\n        num_contours -= numper\n\n        levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n        levels = np.unique(np.append(levels1,levels2))\n        num_contours -= numper\n        while num_contours > 0:\n            cutoff = levels[1]\n            levels2 = np.linspace(levelmin,cutoff,min(num_contours,numper))\n            levels = np.unique(np.append(levels2,levels))\n            num_contours -= numper\n\n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = \'k\')\n        ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = \'Blues\')\n                \n        # clean up panel\n        ax.set_xlabel(\'$w_0$\',fontsize = 12)\n        ax.set_ylabel(\'$w_1$\',fontsize = 12,rotation = 0)\n        ax.set_title(r\'$g\\left(w_0,w_1\\right)$\',fontsize = 13)\n\n        ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.set_xlim([-wmax,wmax])\n        ax.set_ylim([-wmax,wmax])'"
mlrefined_libraries/superlearn_library/multi_outupt_plotters.py,5,"b""import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d\nimport matplotlib.pyplot as plt \nfrom autograd import numpy as np\n    \n# plot multi-output regression dataset where output dimension C = 2\ndef plot_data(x,y,view1,view2):    \n    # construct panels\n    fig = plt.figure(figsize = (9,4))\n    ax0 = plt.subplot(121,projection='3d')\n    ax0.view_init(view1[0],view1[1])\n    ax0.axis('off')\n\n    ax1 = plt.subplot(122,projection='3d')\n    ax1.view_init(view2[0],view2[1])\n    ax1.axis('off')\n\n    # scatter plot data in each panel\n    ax0.scatter(x[0,:],x[1,:],y[0,:],c='k',edgecolor = 'w',linewidth = 1,s=60)\n    ax1.scatter(x[0,:],x[1,:],y[1,:],c='k',edgecolor = 'w',linewidth = 1,s=60)\n    plt.show()\n   \n# plot multi-output regression dataset with fits provided by 'predictor'\ndef plot_regressions(x,y,predictor,view1,view2):        \n    # import all the requisite libs\n    # construct panels\n    fig = plt.figure(figsize = (9,4))\n    ax0 = plt.subplot(121,projection='3d')\n    ax0.view_init(view1[0],view1[1])\n    ax0.axis('off')\n\n    ax1 = plt.subplot(122,projection='3d')\n    ax1.view_init(view2[0],view2[1])\n    ax1.axis('off')\n\n    # scatter plot data in each panel\n    ax0.scatter(x[0,:],x[1,:],y[0,:],c='k',edgecolor = 'w',linewidth = 1,s=60)\n    ax1.scatter(x[0,:],x[1,:],y[1,:],c='k',edgecolor = 'w',linewidth = 1,s=60)\n\n    # construct input for each model fit\n    a_ = np.linspace(0,1,15)\n    a,b = np.meshgrid(a_,a_)\n    a = a.flatten()[np.newaxis,:]\n    b = b.flatten()[np.newaxis,:]\n    c = np.vstack((a,b))\n\n    # evaluate model \n    p = predictor(c)\n    m1 = p[0,:]\n    m2 = p[1,:]\n\n    # plot each as surface\n    a.shape = (a_.size,a_.size)\n    b.shape = (a_.size,a_.size)\n    m1.shape = (a_.size,a_.size)\n    m2.shape = (a_.size,a_.size)\n\n    ax0.plot_surface(a,b,m1,alpha = 0.25,color = 'lime',cstride = 2,rstride = 2,linewidth = 1,edgecolor ='k')\n    ax1.plot_surface(a,b,m2,alpha = 0.25,color = 'lime',cstride = 2,rstride = 2,linewidth = 1,edgecolor ='k')\n\n    plt.show()"""
mlrefined_libraries/superlearn_library/multiclass_feature_scaling_tools.py,28,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   \nfrom autograd import hessian as compute_hess\nimport autograd.numpy as np\nimport math\nimport time\n\nclass Visualizer:\n    '''\n    Animate how normalizing the input of a single input supervised cost function re-shapes \n    its contours, equalizing the penalty assigned to violating either the ideal bias or slope\n    parameter.\n    ''' \n\n    # load in data, in particular input and normalized input\n    def __init__(self,x,x_normalized,y,cost):\n        self.x_original = x\n        self.x_normalized = x_normalized\n        self.y = y\n        \n        # make cost function choice\n        self.cost_func = 0\n        if cost == 'multiclass_softmax':\n            self.cost_func = self.multiclass_softmax\n        if cost == 'multiclass_perceptron':\n            self.cost_func = self.multiclass_perceptron\n        if cost == 'fusion_rule':\n            self.cost_func = self.fusion_rule\n                            \n    #####   #####\n    def animate_transition(self,savepath,num_frames,**kwargs):\n        # initialize figure\n        fig = plt.figure(figsize = (10,4.5))\n        artist = fig\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); ax.set_aspect('equal')\n\n        # animation sub-function\n        lams = np.linspace(0,1,num_frames)\n        print ('starting animation rendering...')\n        def animate(k):\n            ax.cla()\n            lam = lams[k]\n            \n            # print rendering update            \n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n                \n            # re-assign inputs as weighted average of original and normalized input\n            self.x = (1 - lam)*self.x_original + lam*self.x_normalized\n            \n            # plot contour\n            self.contour_plot_setup(ax,**kwargs)  # draw contour plot\n            ax.set_title(r'$\\lambda = ' + str(np.round(lam,2)) + '$',fontsize = 14)\n\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()\n            \n    ########################################################################################\n    ###### predict and cost functions #####\n    ###### basic model ######\n    # compute linear combination of input point\n    def model(self,x,w):\n        # tack a 1 onto the top of each input point all at once\n        o = np.ones((1,np.shape(x)[1]))\n        x = np.vstack((o,x))\n\n        # compute linear combination and return\n        a = np.dot(x.T,w)\n        return a\n            \n    ###### cost functions #####\n    # multiclass perceptron\n    def multiclass_perceptron(self,w):        \n        # pre-compute predictions on all points\n        all_evals = self.model(self.x,w)\n\n        # compute maximum across data points\n        a =  np.max(all_evals,axis = 1)        \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[np.arange(len(self.y)),self.y.astype(int).flatten()-1]\n        cost = np.sum(a - b)\n\n        # add regularizer\n        cost = cost + self.lam*np.linalg.norm(w[1:,:],'fro')**2\n\n        # return average\n        return cost/float(len(self.y))\n    \n    # multiclass softmax\n    def multiclass_softmax(self,w):        \n        # pre-compute predictions on all points\n        all_evals = self.model(self.x,w)\n\n        # compute softmax across data points\n        a = np.log(np.sum(np.exp(all_evals),axis = 1)) \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[np.arange(len(self.y)),self.y.astype(int).flatten()-1]\n        cost = np.sum(a - b)\n\n        # add regularizer\n        cost = cost + self.lam*np.linalg.norm(w[1:,:],'fro')**2\n\n        # return average\n        return cost/float(len(self.y))\n    \n    # multiclass misclassification cost function - aka the fusion rule\n    def fusion_rule(self,w):        \n        # pre-compute predictions on all points\n        all_evals = self.model(self.x,w)\n\n        # compute predictions of each input point\n        y_predict = (np.argmax(all_evals,axis = 1) + 1)[:,np.newaxis]\n\n        # compare predicted label to actual label\n        count = np.sum(np.abs(np.sign(self.y - y_predict)))\n\n        # return number of misclassifications\n        return count\n\n    ########################################################################################\n    #### utility functions - for setting up / making contour plots, 3d surface plots, etc., ####\n    # show contour plot of input function\n    def contour_plot_setup(self,ax,**kwargs):\n        xmin = -3.1\n        xmax = 3.1\n        ymin = -3.1\n        ymax = 3.1\n        if 'xmin' in kwargs:            \n            xmin = kwargs['xmin']\n        if 'xmax' in kwargs:\n            xmax = kwargs['xmax']\n        if 'ymin' in kwargs:            \n            ymin = kwargs['ymin']\n        if 'ymax' in kwargs:\n            ymax = kwargs['ymax']      \n        num_contours = 20\n        if 'num_contours' in kwargs:\n            num_contours = kwargs['num_contours']   \n            \n        # choose viewing range using weight history?\n        if 'view_by_weights' in kwargs:\n            view_by_weights = True\n            weight_history = kwargs['weight_history']\n            if view_by_weights == True:\n                xmin = min([v[0] for v in weight_history])[0]\n                xmax = max([v[0] for v in weight_history])[0]\n                xgap = (xmax - xmin)*0.25\n                xmin -= xgap\n                xmax += xgap\n\n                ymin = min([v[1] for v in weight_history])[0]\n                ymax = max([v[1] for v in weight_history])[0]\n                ygap = (ymax - ymin)*0.25\n                ymin -= ygap\n                ymax += ygap\n \n        ### plot function as contours ###\n        self.draw_contour_plot(ax,num_contours,xmin,xmax,ymin,ymax)\n        \n        ### cleanup panel ###\n        ax.set_xlabel('$w_0$',fontsize = 14)\n        ax.set_ylabel('$w_1$',fontsize = 14,labelpad = 15,rotation = 0)\n        ax.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color='k',zorder = 0,linewidth = 0.5)\n        # ax.set_xticks(np.arange(round(xmin),round(xmax)+1))\n        # ax.set_yticks(np.arange(round(ymin),round(ymax)+1))\n        \n        # set viewing limits\n        ax.set_xlim(xmin,xmax)\n        ax.set_ylim(ymin,ymax)\n\n    ### function for creating contour plot\n    def draw_contour_plot(self,ax,num_contours,xmin,xmax,ymin,ymax):\n        #### define input space for function and evaluate ####\n        w1 = np.linspace(xmin,xmax,400)\n        w2 = np.linspace(ymin,ymax,400)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([ self.cost_func(np.reshape(s,(2,1))) for s in h])\n\n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cut = 0.4\n        cutoff = (levelmax - levelmin)\n        levels = [levelmin + cutoff*cut**(num_contours - i) for i in range(0,num_contours+1)]\n        levels = [levelmin] + levels\n        levels = np.asarray(levels)\n   \n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = 'k')\n        b = ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = 'Blues')\n      \n    # a small Python function for plotting the distributions of input features\n    def feature_distributions(self,x):\n        # create figure \n        fig = plt.figure(figsize = (10,4))\n\n        # create subplots\n        N = x.shape[0]\n        gs = 0\n        if N <= 5:\n            gs = gridspec.GridSpec(1,N)\n        else:\n            gs = gridspec.GridSpec(2,5)\n\n\n        # remove whitespace from figure\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.01,hspace=0.01)\n\n        # loop over input and plot each individual input dimension value\n        all_bins = []\n        for n in range(N):\n            hist, bins = np.histogram(x[n,:], bins=30)\n            all_bins.append(bins.ravel())\n            \n        # determine range for all subplots\n        maxview = np.max(all_bins)\n        minview = np.min(all_bins)\n        viewrange = (maxview - minview)*0.1\n        maxview += viewrange\n        minview -= viewrange\n        \n        for n in range(N):\n            # make subplot\n            ax = plt.subplot(gs[n]); \n            hist, bins = np.histogram(x[n,:], bins=30)\n            width = 0.7 * (bins[1] - bins[0])\n            center = (bins[:-1] + bins[1:]) / 2\n            ax.barh(center, hist,width)\n            ax.set_title(r'$x_' + str(n+1) + '$',fontsize=14)\n            ax.set_ylim([minview,maxview])\n        plt.show()"""
mlrefined_libraries/superlearn_library/multiclass_illustrator.py,41,"b""# import standard plotting \nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\n\n# other basic libraries\nimport math\nimport time\nimport copy\nimport autograd.numpy as np\n\n# patch / convex hull libraries\nfrom matplotlib.patches import Circle, Wedge, Polygon\nfrom matplotlib.collections import PatchCollection\nfrom scipy.spatial import ConvexHull\n\n# import optimizer class from same library\nfrom . import optimizers\n\nclass Visualizer:\n    '''\n    Demonstrate multiclass logistic regression classification\n    \n    '''\n    \n    #### initialize ####\n    def __init__(self,data):    \n        data = data.T\n        \n        # define the input and output of our dataset\n        self.x = np.asarray(data[:,:-1])\n        self.x.shape = (len(self.x),np.shape(data)[1]-1); self.x = self.x.T;\n        self.y = data[:,-1]\n        self.y.shape = (len(self.y),1)\n    \n        # colors for viewing classification data 'from above'\n        self.colors = [[ 0, 0.4, 1],[1,0,0.4],[0, 1, 0.5],[1, 0.7, 0.5],'violet','mediumaquamarine']\n        \n    ###### cost functions ######\n    # counting cost for multiclass classification - used to determine best weights\n    def counting_cost(self,W):        \n        # pre-compute predictions on all points\n        y_hats = W[0,:] + np.dot(self.x.T,W[1:,:])\n\n        # compute counting cost\n        cost = 0\n        for p in range(len(self.y)):\n            # pluck out current true label, predicted label\n            y_p = int(self.y[p][0])         # subtract off one due to python indexing\n            y_hat_p = int(np.argmax(y_hats[p])) \n\n            # update cost\n            cost += np.abs(np.sign(y_hat_p - y_p))\n        return cost\n    \n    \n    # multiclass softmaax regularized by the summed length of all normal vectors\n    def multiclass_softmax(self,W):        \n        # pre-compute predictions on all points\n        all_evals = W[0,:] + np.dot(self.x.T,W[1:,:])\n\n        # compute counting cost\n        cost = 0\n        for p in range(len(self.y)):\n            # pluck out current true label\n            y_p = int(self.y[p][0])    # subtract off one due to python indexing\n\n            # update cost summand\n            cost +=  np.log(np.sum(np.exp(all_evals[p,:]))) - all_evals[p,y_p]\n\n        # return cost with regularizer added\n        return cost + self.lam*np.linalg.norm(W[1:,:],'fro')**2\n\n    \n    ###### plotting functions ######  \n    # show data\n    def show_dataset(self):\n        # initialize figure\n        fig = plt.figure(figsize = (8,4))\n        artist = fig\n        gs = gridspec.GridSpec(1, 3,width_ratios = [1,3,1]) \n\n        # setup current axis\n        ax = plt.subplot(gs[1],aspect = 'equal'); \n        \n        # run axis through data plotter\n        self.plot_data(ax)\n        \n        # determine plotting ranges\n        minx = min(min(self.x[0,:]),min(self.x[1,:]))\n        maxx = max(max(self.x[0,:]),max(self.x[1,:]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n        \n        # dress panel\n        ax.set_xlim(minx,maxx)\n        ax.set_ylim(minx,maxx)\n        \n        plt.show()\n        \n    # show coloring of entire space\n    def show_complete_coloring(self,w_hist,**kwargs):\n        '''\n        # determine best set of weights from history\n        cost_evals = []\n        for i in range(len(w_hist)):\n            W = w_hist[i]\n            cost = self.counting_cost(W)\n            cost_evals.append(cost)\n        ind = np.argmin(cost_evals)\n        '''\n        \n        # or just take last weights\n        self.W = w_hist[-1]\n        \n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        \n        show_cost = False\n        if 'show_cost' in kwargs:\n            show_cost = kwargs['show_cost']\n        if show_cost == True:   \n            gs = gridspec.GridSpec(1, 3,width_ratios = [1,1,1],height_ratios = [1]) \n            \n            # create third panel for cost values\n            ax3 = plt.subplot(gs[2],aspect = 'equal')\n            \n        else:\n            gs = gridspec.GridSpec(1, 2,width_ratios = [1,1]) \n\n        # setup current axis\n        ax = plt.subplot(gs[0],aspect = 'equal');\n        ax2 = plt.subplot(gs[1],aspect = 'equal');\n        \n        # generate input range for viewing range\n        minx = min(min(self.x[0,:]),min(self.x[1,:]))\n        maxx = max(max(self.x[0,:]),max(self.x[1,:]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n        \n        # plot panel with all data and separators\n        self.plot_data(ax)\n        self.plot_data(ax2)\n        self.plot_all_separators(ax)\n                \n        ### draw multiclass boundary on right panel\n        r = np.linspace(minx,maxx,2000)\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        o = np.ones((len(r)**2,1))\n        h = np.concatenate([o,w1_vals,w2_vals],axis = 1)\n        pts = np.dot(h,self.W)\n        g_vals = np.argmax(pts,axis = 1)\n\n        # vals for cost surface\n        w1_vals.shape = (len(r),len(r))\n        w2_vals.shape = (len(r),len(r))\n        g_vals.shape = (len(r),len(r))\n        \n        # plot contour\n        C = len(np.unique(self.y))\n        ax2.contour(w1_vals,w2_vals,g_vals,colors = 'k',levels = range(0,C+1),linewidths = 2.75,zorder = 4)\n        ax2.contourf(w1_vals,w2_vals,g_vals+1,colors = self.colors[:],alpha = 0.2,levels = range(0,C+1))\n        ax.contourf(w1_vals,w2_vals,g_vals+1,colors = self.colors[:],alpha = 0.2,levels = range(0,C+1))\n        \n        # dress panel\n        ax.set_xlim(minx,maxx)\n        ax.set_ylim(minx,maxx)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        #ax.set_ylabel(r'$x_2$',rotation = 0,fontsize = 12,labelpad = 10)\n        #ax.set_xlabel(r'$x_1$',fontsize = 12)\n        \n        ax2.set_xticks([])\n        ax2.set_yticks([])\n        ax2.set_yticklabels([])\n        ax2.set_xticklabels([])\n        ax2.set_xlim(minx,maxx)\n        ax2.set_ylim(minx,maxx)\n        #ax2.set_ylabel(r'$x_2$',rotation = 0,fontsize = 12,labelpad = 10)\n        #ax2.set_xlabel(r'$x_1$',fontsize = 12)\n\n        \n        if  show_cost == True: \n            # compute cost eval history\n            g = kwargs['cost']\n            cost_evals = []\n            for i in range(len(w_hist)):\n                W = w_hist[i]\n                cost = g(W)\n                cost_evals.append(cost)\n     \n            # plot cost path - scale to fit inside same aspect as classification plots\n            num_iterations = len(w_hist)\n            s = np.linspace(minx + gapx,maxx - gapx,num_iterations)\n            scaled_costs = [c/float(max(cost_evals))*(maxx-gapx) - (minx+gapx) for c in cost_evals]\n            ax3.plot(s,scaled_costs,color = 'k',linewidth = 1.5)\n            ax3.set_xlabel('iteration',fontsize = 12)\n            ax3.set_title('cost function plot',fontsize = 12)\n            \n            # rescale number of iterations and cost function value to fit same aspect ratio as other two subplots\n            ax3.set_xlim(minx,maxx)\n            ax3.set_ylim(minx,maxx)\n            \n            ### set tickmarks for both axes - requries re-scaling   \n            # x axis\n            marks = range(0,num_iterations,round(num_iterations/5.0))\n            ax3.set_xticks(s[marks])\n            labels = [item.get_text() for item in ax3.get_xticklabels()]\n            ax3.set_xticklabels(marks)\n            \n            ### y axis\n            r = (max(scaled_costs) - min(scaled_costs))/5.0\n            marks = [min(scaled_costs) + m*r for m in range(6)]\n            ax3.set_yticks(marks)\n            labels = [item.get_text() for item in ax3.get_yticklabels()]\n            \n            r = (max(cost_evals) - min(cost_evals))/5.0\n            marks = [int(min(cost_evals) + m*r) for m in range(6)]\n            ax3.set_yticklabels(marks)\n   \n\n    # show coloring of entire space\n    def show_discrete_step(self,w_hist,view,**kwargs):\n        '''\n        # determine best set of weights from history\n        cost_evals = []\n        for i in range(len(w_hist)):\n            W = w_hist[i]\n            cost = self.counting_cost(W)\n            cost_evals.append(cost)\n        ind = np.argmin(cost_evals)\n        '''\n        \n        # or just take last weights\n        self.W = w_hist[-1]\n        \n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n        gs = gridspec.GridSpec(1, 2,width_ratios = [1.5,1]) \n            \n        # setup current axis\n        ax = plt.subplot(gs[1],projection = '3d');\n        ax2 = plt.subplot(gs[0],aspect = 'equal');\n        \n        # load in args\n        zplane = 'on'\n        if 'zplane' in kwargs:\n            zplane = kwargs['zplane']\n       \n        # generate input range for viewing range\n        minx = min(min(self.x[0,:]),min(self.x[1,:]))\n        maxx = max(max(self.x[0,:]),max(self.x[1,:]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n        \n        # plot panel with all data and separators\n        # scatter points in both panels\n        class_nums = np.unique(self.y)\n        C = len(class_nums)\n\n        # plot data in right panel from above\n        self.plot_data(ax2)\n                \n        ### draw multiclass boundary on right panel\n        r = np.linspace(minx,maxx,4000)\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        o = np.ones((len(r)**2,1))\n        h = np.concatenate([o,w1_vals,w2_vals],axis = 1)\n        pts = np.dot(h,self.W)\n        g_vals = np.argmax(pts,axis = 1) \n\n        # vals for cost surface\n        w1_vals.shape = (len(r),len(r))\n        w2_vals.shape = (len(r),len(r))\n        g_vals.shape = (len(r),len(r))\n        \n        # plot contour in right panel\n        C = len(np.unique(self.y))\n        ax2.contour(w1_vals,w2_vals,g_vals,colors = 'k',levels = range(-1,C),linewidths = 2.75,zorder = 4)\n        ax2.contourf(w1_vals,w2_vals,g_vals,colors = self.colors[:],alpha = 0.2,levels = range(-1,C))\n        \n        ### plot discrete step function ###\n        # to plot the step function, plot the bottom and top steps separately - z1 and z2\n        steps = np.unique(g_vals)\n        num_steps = np.arange(len(steps))\n        \n        # loop over each step and plot\n        g_vals_copy = copy.deepcopy(g_vals)\n        g_vals_copy.shape = (len(r)**2,1)\n        for step in steps:\n            # copy surface            \n            g_copy = np.zeros((len(r)**2,1))\n            g_copy.fill(np.nan)\n            \n            # find step in copy, nan out all else\n            ind = np.argwhere(g_vals_copy == step)\n            ind = [v[0] for v in ind]\n            for i in ind:\n                g_copy[i] = step\n            \n            # reshape and plot\n            g_copy.shape = (len(r),len(r))\n            ax.plot_surface(w1_vals,w2_vals,g_copy,alpha = 0.25,color = 'w',zorder = 0,edgecolor = 'k',linewidth=0.25,cstride = 200, rstride = 200,shade=10,antialiased=True)\n\n        # plot zplane = 0 in left 3d panel - showing intersection of regressor with z = 0 (i.e., its contour, the separator, in the 3d plot too)?\n        if zplane == 'on':\n            g_vals +=1\n            #ax.plot_surface(w1_vals,w2_vals,g_vals*0-1,alpha = 0.1) \n            \n            # loop over each class and color in z-plane\n            for c in class_nums:\n                # plot separator curve in left plot z plane\n                ax.contour(w1_vals,w2_vals,g_vals - c,colors = 'k',levels = [0],linewidths = 3,zorder = 1)\n                        \n                # color parts of plane with correct colors\n                ax.contourf(w1_vals,w2_vals, g_vals - 0.5 - c ,colors = self.colors[(int(c)):],alpha = 0.1,levels = range(0,2))\n                \n                \n        # scatter points in 3d\n        for c in range(C):\n            ind = np.argwhere(self.y == class_nums[c])\n            ind = [v[0] for v in ind]\n            ax.scatter(self.x[0,ind],self.x[1,ind],self.y[ind],s = 80,color = self.colors[c],edgecolor = 'k',linewidth = 1.5,zorder = 3)\n            \n        # dress panel\n        ax.view_init(view[0],view[1])\n        #ax.axis('off')\n        ax.set_xlim(minx,maxx)\n        ax.set_ylim(minx,maxx)\n        ax.set_zlim(-0.1,C - 1+0.1)\n        \n        ax2.set_xticks([])\n        ax2.set_yticks([])\n        ax2.set_yticklabels([])\n        ax2.set_xticklabels([])\n        ax2.set_xlim(minx,maxx)\n        ax2.set_ylim(minx,maxx)\n        #ax2.set_ylabel(r'$x_2$',rotation = 0,fontsize = 12,labelpad = 10)\n        #ax2.set_xlabel(r'$x_1$',fontsize = 12)\n\n\n    ### compare grad descent runs - given cost to counting cost ###\n    def compare_to_counting(self,**kwargs):\n        # parse args\n        num_runs = 1\n        if 'num_runs' in kwargs:\n            num_runs = kwargs['num_runs']\n        max_its = 200\n        if 'max_its' in kwargs:\n            max_its = kwargs['max_its']\n        alpha = 10**-2\n        if 'alpha' in kwargs:\n            alpha = kwargs['alpha']  \n        steplength_rule = 'none'\n        if 'steplength_rule' in kwargs:\n            steplength_rule = kwargs['steplength_rule']\n        version = 'unnormalized'\n        if 'version' in kwargs:\n            version = kwargs['version'] \n        algo = 'gradient_descent'\n        if 'algo' in kwargs:\n            algo = kwargs['algo']\n         \n        #### perform all optimizations ###\n        self.lam = 10**-3  # our regularization paramter \n        if 'lam' in kwargs:\n            self.lam = kwargs['lam']\n            \n        g = self.multiclass_softmax\n        g_count = self.counting_cost\n        \n        # create instance of optimizers\n        self.opt = optimizers.MyOptimizers()\n        \n        # run optimizer\n        big_w_hist = []\n        C = len(np.unique(self.y))\n        for j in range(num_runs):\n            # create random initialization\n            w_init =  np.random.randn(np.shape(self.x)[0]+1,C)\n\n            # run algo\n            if algo == 'gradient_descent':# run gradient descent\n                w_hist = self.opt.gradient_descent(g = g,w = w_init,version = version,max_its = max_its, alpha = alpha,steplength_rule = steplength_rule)\n            elif algo == 'newtons_method':\n                w_hist = self.opt.newtons_method(g = g,w = w_init,max_its = max_its)\n            big_w_hist.append(w_hist)\n            \n        ##### setup figure to plot #####\n        # initialize figure\n        fig = plt.figure(figsize = (8,4))\n        artist = fig\n        \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]);\n        \n        #### start runs and plotting ####\n        for j in range(num_runs):\n            w_hist = big_w_hist[j]\n            \n            # evaluate counting cost / other cost for each weight in history, then plot\n            count_evals = []\n            cost_evals = []\n            for k in range(len(w_hist)):\n                w = w_hist[k]\n                g_eval = g(w)\n                cost_evals.append(g_eval)\n                \n                count_eval = g_count(w)\n                count_evals.append(count_eval)\n                \n            # plot each \n            ax1.plot(np.arange(0,len(w_hist)),count_evals[:len(w_hist)],linewidth = 2)\n            ax2.plot(np.arange(0,len(w_hist)),cost_evals[:len(w_hist)],linewidth = 2)\n                \n        #### cleanup plots ####\n        # label axes\n        ax1.set_xlabel('iteration',fontsize = 13)\n        ax1.set_ylabel('num misclassifications',rotation = 90,fontsize = 13)\n        ax1.set_title('number of misclassifications',fontsize = 14)\n        ax1.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n        \n        ax2.set_xlabel('iteration',fontsize = 13)\n        ax2.set_ylabel('cost value',rotation = 90,fontsize = 13)\n        title = 'Multiclass Softmax'\n        ax2.set_title(title,fontsize = 14)\n        ax2.axhline(y=0, color='k',zorder = 0,linewidth = 0.5)\n        \n        plt.show()\n        \n    \n    #### utility functions ####           \n    #plot data\n    def plot_data(self,ax):\n        # initialize figure, plot data, and dress up panels with axes labels etc.\n        num_classes = np.size(np.unique(self.y))\n                \n        # color current class\n        for a in range(0,num_classes):\n            t = np.argwhere(self.y == a)\n            t = t[:,0]\n            ax.scatter(self.x[0,t],self.x[1,t], s = 50,color = self.colors[a],edgecolor = 'k',linewidth = 1.5)\n        \n    # plot separators\n    def plot_all_separators(self,ax):\n        # determine plotting ranges\n        minx = min(min(self.x[0,:]),min(self.x[1,:]))\n        maxx = max(max(self.x[0,:]),max(self.x[1,:]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n        \n        # initialize figure, plot data, and dress up panels with axes labels etc.\n        num_classes = np.size(np.unique(self.y))\n                \n        # color current class\n        r = np.linspace(minx,maxx,400)\n        for a in range(0,num_classes):\n            # get current weights\n            w = self.W[:,a]\n            \n            # draw subproblem separator\n            z = - w[0]/w[2] - w[1]/w[2]*r\n            r = np.linspace(minx,maxx,400)\n            ax.plot(r,z,linewidth = 2,color = self.colors[a],zorder = 3)\n            ax.plot(r,z,linewidth = 2.75,color = 'k',zorder = 2)"""
mlrefined_libraries/superlearn_library/multiclass_static_plotter.py,24,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport time\n\nclass Visualizer:\n    \'\'\'\n    Illustrate a run of your preferred optimization algorithm on a one or two-input function.  Run\n    the algorithm first, and input the resulting weight history into this wrapper.\n    \'\'\' \n\n    ##### draw picture of function and run for two-input function ####       \n    def two_input_contour_plot(self,g,weight_history,**kwargs):\n        # compute number of classes\n        C = np.shape(weight_history[0])[1]\n        \n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (10,6))\n\n        # create figure with single plot for contour\n        gs = gridspec.GridSpec(2, 2) \n       \n        ### make contour right plot - as well as horizontal and vertical axes ###\n        for c in range(C):\n            # create panel\n            ax = plt.subplot(gs[c])\n            ax.set_aspect(\'equal\')\n\n            # plot contour and path \n            w_hist = [weight_history[v][:,c][:,np.newaxis] for v in range(len(weight_history))]\n            self.contour_plot_setup(c,C,g,ax,**kwargs)  # draw contour plot\n            self.draw_weight_path(ax,w_hist,**kwargs)        # draw path on contour plot\n            \n            # label axes\n            ax.set_xlabel(r\'$w_0^{(\' + str(c+1) + \')}$\',fontsize = 15)\n            ax.set_ylabel(r\'$w_1^{(\' + str(c+1) + \')}$\',fontsize = 15,labelpad = 15,rotation = 0)\n            \n        # remove whitespace from figure\n        #gs.update(wspace=0.005, hspace=0.15) # set the spacing between axes. \n        #fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.001,hspace=0.001)\n        \n        # plot\n        plt.show()\n\n    ########################################################################################\n    #### utility functions - for setting up / making contour plots, 3d surface plots, etc., ####\n    # show contour plot of input function\n    def contour_plot_setup(self,c,C,g,ax,**kwargs):\n        xmin = -3.1\n        xmax = 3.1\n        ymin = -3.1\n        ymax = 3.1\n        if \'xmin\' in kwargs:            \n            xmin = kwargs[\'xmin\']\n        if \'xmax\' in kwargs:\n            xmax = kwargs[\'xmax\']\n        if \'ymin\' in kwargs:            \n            ymin = kwargs[\'ymin\']\n        if \'ymax\' in kwargs:\n            ymax = kwargs[\'ymax\']      \n        num_contours = 20\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']   \n            \n        # choose viewing range using weight history?\n        if \'view_by_weights\' in kwargs:\n            view_by_weights = True\n            weight_history = kwargs[\'weight_history\']\n            if view_by_weights == True:\n                xmin = min([v[0] for v in weight_history])[0]\n                xmax = max([v[0] for v in weight_history])[0]\n                xgap = (xmax - xmin)*0.25\n                xmin -= xgap\n                xmax += xgap\n\n                ymin = min([v[1] for v in weight_history])[0]\n                ymax = max([v[1] for v in weight_history])[0]\n                ygap = (ymax - ymin)*0.25\n                ymin -= ygap\n                ymax += ygap\n \n        ### plot function as contours ###\n        self.draw_contour_plot(c,C,g,ax,num_contours,xmin,xmax,ymin,ymax)\n        \n        ### cleanup panel ###\n        ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        \n        # set viewing limits\n        ax.set_xlim(xmin,xmax)\n        ax.set_ylim(ymin,ymax)\n\n    ### function for creating contour plot\n    def draw_contour_plot(self,c,C,g,ax,num_contours,xmin,xmax,ymin,ymax):\n        #### define input space for function and evaluate ####\n        w1 = np.linspace(xmin,xmax,100)\n        w2 = np.linspace(ymin,ymax,100)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = []\n        for e in range(len(w1)**2):\n            s = np.reshape(h[e,:],(2,1))\n            z = np.zeros((2,C))\n            z[:,c] = s.flatten()\n            func_vals.append(g(z))\n        func_vals = np.array(func_vals)\n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cut = 0.4\n        cutoff = (levelmax - levelmin)\n        levels = [levelmin + cutoff*cut**(num_contours - i) for i in range(0,num_contours+1)]\n        levels = [levelmin] + levels\n        levels = np.asarray(levels)\n   \n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = \'k\')\n        b = ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = \'Blues\')\n        \n        \n    ### makes color spectrum for plotted run points - from green (start) to red (stop)\n    def make_colorspec(self,w_hist):\n        # make color range for path\n        s = np.linspace(0,1,len(w_hist[:round(len(w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(w_hist[round(len(w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        colorspec = []\n        colorspec = np.concatenate((s,np.flipud(s)),1)\n        colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n        return colorspec\n        \n    ### function for drawing weight history path\n    def draw_weight_path(self,ax,w_hist,**kwargs):\n        # make colors for plot\n        colorspec = self.make_colorspec(w_hist)\n        \n        arrows = True\n        if \'arrows\' in kwargs:\n            arrows = kwargs[\'arrows\']\n\n        ### plot function decrease plot in right panel\n        for j in range(len(w_hist)):  \n            w_val = w_hist[j]\n\n            # plot each weight set as a point\n            ax.scatter(w_val[0],w_val[1],s = 80,c = colorspec[j],edgecolor = \'k\',linewidth = 2*math.sqrt((1/(float(j) + 1))),zorder = 3)\n\n            # plot connector between points for visualization purposes\n            if j > 0:\n                pt1 = w_hist[j-1]\n                pt2 = w_hist[j]\n                \n                # produce scalar for arrow head length\n                pt_length = np.linalg.norm(pt1 - pt2)\n                head_length = 0.1\n                alpha = (head_length - 0.35)/pt_length + 1\n                \n                # if points are different draw error\n                if np.linalg.norm(pt1 - pt2) > head_length and arrows == True:\n                    if np.ndim(pt1) > 1:\n                        pt1 = pt1.flatten()\n                        pt2 = pt2.flatten()\n                    \n                    ax.arrow(pt1[0],pt1[1],(pt2[0] - pt1[0])*alpha,(pt2[1] - pt1[1])*alpha, head_width=0.1, head_length=head_length, fc=\'k\', ec=\'k\',linewidth=4,zorder = 2,length_includes_head=True)\n                    ax.arrow(pt1[0],pt1[1],(pt2[0] - pt1[0])*alpha,(pt2[1] - pt1[1])*alpha, head_width=0.1, head_length=head_length, fc=\'w\', ec=\'w\',linewidth=0.25,zorder = 2,length_includes_head=True)\n        \n    ### draw surface plot\n    def draw_surface(self,g,ax,**kwargs):\n        xmin = -3.1\n        xmax = 3.1\n        ymin = -3.1\n        ymax = 3.1\n        if \'xmin\' in kwargs:            \n            xmin = kwargs[\'xmin\']\n        if \'xmax\' in kwargs:\n            xmax = kwargs[\'xmax\']\n        if \'ymin\' in kwargs:            \n            ymin = kwargs[\'ymin\']\n        if \'ymax\' in kwargs:\n            ymax = kwargs[\'ymax\']   \n            \n        #### define input space for function and evaluate ####\n        w1 = np.linspace(xmin,xmax,200)\n        w2 = np.linspace(ymin,ymax,200)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([g(np.reshape(s,(2,1))) for s in h])\n\n        ### plot function as surface ### \n        w1_vals.shape = (len(w1),len(w2))\n        w2_vals.shape = (len(w1),len(w2))\n        func_vals.shape = (len(w1),len(w2))\n        ax.plot_surface(w1_vals, w2_vals, func_vals, alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)\n\n        # plot z=0 plane \n        ax.plot_surface(w1_vals, w2_vals, func_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n                \n        # clean up axis\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        \n        ax.set_xlabel(\'$w_0$\',fontsize = 14)\n        ax.set_ylabel(\'$w_1$\',fontsize = 14,rotation = 0)\n        ax.set_title(\'$g(w_0,w_1)$\',fontsize = 14)\n        \n\n    ### plot points and connectors in input space in 3d plot        \n    def show_inputspace_path(self,w_hist,ax):\n        # make colors for plot\n        colorspec = self.make_colorspec(w_hist)\n        \n        for k in range(len(w_hist)):\n            pt1 = w_hist[k]\n            ax.scatter(pt1[0],pt1[1],0,s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 0.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n            if k < len(w_hist)-1:\n                pt2 = w_hist[k+1]\n                if np.linalg.norm(pt1 - pt2) > 10**(-3):\n                    # draw arrow in left plot\n                    a = Arrow3D([pt1[0],pt2[0]], [pt1[1],pt2[1]], [0, 0], mutation_scale=10, lw=2, arrowstyle=""-|>"", color=""k"")\n                    ax.add_artist(a)\n        \n#### custom 3d arrow and annotator functions ###    \n# nice arrow maker from https://stackoverflow.com/questions/11140163/python-matplotlib-plotting-a-3d-cube-a-sphere-and-a-vector\nclass Arrow3D(FancyArrowPatch):\n\n    def __init__(self, xs, ys, zs, *args, **kwargs):\n        FancyArrowPatch.__init__(self, (0, 0), (0, 0), *args, **kwargs)\n        self._verts3d = xs, ys, zs\n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n        FancyArrowPatch.draw(self, renderer)'"
mlrefined_libraries/superlearn_library/normalizers.py,2,"b'import autograd.numpy as np\n\n###### standard normalization function ######\ndef standard_normalizer(x):\n    # compute the mean and standard deviation of the input\n    x_means = np.mean(x,axis = 1)[:,np.newaxis]\n    x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n\n    # create standard normalizer based on mean / std\n    normalizer = lambda data: (data - x_means)/x_stds\n\n    # return normalizer and inverse_normalizer\n    return normalizer'"
mlrefined_libraries/superlearn_library/one_versus_all.py,6,"b""# import custom library\nimport copy\nimport sys\nsys.path.append('../')\nfrom mlrefined_libraries import math_optimization_library as optlib\nfrom mlrefined_libraries import superlearn_library as superlearn\nimport autograd.numpy as np\n\n# demos for this notebook\noptimizers = optlib.optimizers\ncost_lib = superlearn.cost_functions\n\n### compare grad descent runs - given cost to counting cost ###\ndef train(x,y,**kwargs):    \n    # get and run optimizer to solve two-class problem\n    N = np.shape(x)[0]\n    C = np.size(np.unique(y))\n    max_its = 100; alpha_choice = 1; cost_name = 'softmax'; w = 0.1*np.random.randn(N+1,1); optimizer = 'gradient_descent';\n    \n    # switches for user choices\n    if 'max_its' in kwargs:\n        max_its = kwargs['max_its']\n    if 'alpha_choice' in kwargs:\n        alpha_choice = kwargs['alpha_choice']\n    if 'cost_name' in kwargs:\n        cost_name = kwargs['cost_name']\n    if 'w' in kwargs:\n        w = kwargs['w']\n    if 'optimizer' in kwargs:\n        optimizer = kwargs['optimizer']\n    epsilon = 10**(-7)\n    if 'epsilon' in kwargs:\n        epsilon = kwargs['epsilon']\n    \n    # loop over subproblems and solve\n    weight_histories = []\n    for c in range(0,C):\n        # prepare temporary C vs notC sub-probem labels\n        y_temp = copy.deepcopy(y)\n        ind = np.argwhere(y_temp.astype(int) == c)\n        ind = ind[:,1]\n        ind2 = np.argwhere(y_temp.astype(int) != c)\n        ind2 = ind2[:,1]\n        y_temp[0,ind] = 1\n        y_temp[0,ind2] = -1\n\n        # store best weight for final classification \n        cost = cost_lib.choose_cost(x,y_temp,cost_name)\n        \n        # run optimizer\n        weight_history = 0; cost_history = 0;\n        if optimizer == 'gradient_descent':\n            weight_history,cost_history = optimizers.gradient_descent(cost,alpha_choice,max_its,w)\n        if optimizer == 'newtons_method':\n            weight_history,cost_history = optimizers.newtons_method(cost,max_its,w=w,epsilon = epsilon)\n\n        # store each weight history\n        weight_histories.append(copy.deepcopy(weight_history))\n        \n    # combine each individual classifier weights into single weight \n    # matrix per step\n    R = len(weight_histories[0])\n    combined_weights = []\n    for r in range(R):\n        a = []\n        for c in range(C):\n            a.append(weight_histories[c][r])\n        a = np.array(a).T\n        a = a[0,:,:]\n        combined_weights.append(a)\n        \n    # run combined weight matrices through fusion rule to calculate\n    # number of misclassifications per step\n    counter = cost_lib.choose_cost(x,y,'multiclass_counter')\n    count_history = [counter(v) for v in combined_weights]\n        \n    return combined_weights, count_history"""
mlrefined_libraries/superlearn_library/optimizers.py,6,"b""# clear display\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nimport copy\nfrom autograd.misc.flatten import flatten_func\n\nclass MyOptimizers:\n    '''\n    A list of current optimizers.  In each case - since these are used for educational purposes - the weights at each step are recorded and returned.\n    '''\n\n    ### gradient descent ###\n    def gradient_descent(self,g,w,**kwargs):                \n        # create gradient function\n        self.g = g\n        self.grad = compute_grad(self.g)\n        \n        # parse optional arguments        \n        max_its = 100\n        if 'max_its' in kwargs:\n            max_its = kwargs['max_its']\n        version = 'unnormalized'\n        if 'version' in kwargs:\n            version = kwargs['version']\n        alpha = 10**-4\n        if 'alpha' in kwargs:\n            alpha = kwargs['alpha']\n        steplength_rule = 'none'    \n        if 'steplength_rule' in kwargs:\n            steplength_rule = kwargs['steplength_rule']\n        projection = 'None'\n        if 'projection' in kwargs:\n            projection = kwargs['projection']\n        verbose = False\n        if 'verbose' in kwargs:\n            verbose = kwargs['verbose']\n       \n        # create container for weight history \n        w_hist = []\n        w_hist.append(w)\n        \n        # start gradient descent loop\n        if verbose == True:\n            print ('starting optimization...')\n        for k in range(max_its):   \n            # plug in value into func and derivative\n            grad_eval = self.grad(w)\n            grad_eval.shape = np.shape(w)\n            \n            ### normalized or unnormalized descent step? ###\n            if version == 'normalized':\n                grad_norm = np.linalg.norm(grad_eval)\n                if grad_norm == 0:\n                    grad_norm += 10**-6*np.sign(2*np.random.rand(1) - 1)\n                grad_eval /= grad_norm\n            \n            # use backtracking line search?\n            if steplength_rule == 'backtracking':\n                alpha = self.backtracking(w,grad_eval)\n                \n            # use a pre-set diminishing steplength parameter?\n            if steplength_rule == 'diminishing':\n                alpha = 1/(float(k + 1))\n            \n            ### take gradient descent step ###\n            w = w - alpha*grad_eval\n            \n            # record\n            w_hist.append(w)     \n         \n        if verbose == True:\n            print ('...optimization complete!')\n            time.sleep(1.5)\n            clear_output()\n        \n        return w_hist\n\n    # backtracking linesearch module\n    def backtracking(self,w,grad_eval):\n        # set input parameters\n        alpha = 1\n        t = 0.8\n        \n        # compute initial function and gradient values\n        func_eval = self.g(w)\n        grad_norm = np.linalg.norm(grad_eval)**2\n        \n        # loop over and tune steplength\n        while self.g(w - alpha*grad_eval) > func_eval - alpha*0.5*grad_norm:\n            alpha = t*alpha\n        return alpha\n            \n    #### newton's method ####            \n    def newtons_method(self,g,w,**kwargs):        \n        # create gradient and hessian functions\n        self.g = g\n        \n        # flatten gradient for simpler-written descent loop\n        flat_g, unflatten, w = flatten_func(self.g, w)\n        \n        self.grad = compute_grad(flat_g)\n        self.hess = compute_hess(flat_g)  \n        \n        # parse optional arguments        \n        max_its = 20\n        if 'max_its' in kwargs:\n            max_its = kwargs['max_its']\n        self.epsilon = 10**(-5)\n        if 'epsilon' in kwargs:\n            self.epsilon = kwargs['epsilon']\n        verbose = False\n        if 'verbose' in kwargs:\n            verbose = kwargs['verbose']\n        \n        # create container for weight history \n        w_hist = []\n        w_hist.append(unflatten(w))\n        \n        # start newton's method loop  \n        if verbose == True:\n            print ('starting optimization...')\n            \n        geval_old = flat_g(w)\n        for k in range(max_its):\n            # compute gradient and hessian\n            grad_val = self.grad(w)\n            hess_val = self.hess(w)\n            hess_val.shape = (np.size(w),np.size(w))\n\n            # solve linear system for weights\n            w = w - np.dot(np.linalg.pinv(hess_val + self.epsilon*np.eye(np.size(w))),grad_val)\n                    \n            # eject from process if reaching singular system\n            geval_new = flat_g(w)\n            if k > 2 and geval_new > geval_old:\n                print ('singular system reached')\n                time.sleep(1.5)\n                clear_output()\n                return w_hist\n            else:\n                geval_old = geval_new\n                \n            # record current weights\n            w_hist.append(unflatten(w))\n            \n        if verbose == True:\n            print ('...optimization complete!')\n            time.sleep(1.5)\n            clear_output()\n        \n        return w_hist"""
mlrefined_libraries/superlearn_library/ova_illustrator.py,87,"b""# import standard plotting \nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\n\n# other basic libraries\nimport math\nimport time\nimport copy\nimport autograd.numpy as np\n\n# patch / convex hull libraries\nfrom matplotlib.patches import Circle, Wedge, Polygon\nfrom matplotlib.collections import PatchCollection\nfrom scipy.spatial import ConvexHull\n\n# import optimizer class from same library\nfrom . import optimizers\n\nclass Visualizer:\n    '''\n    Demonstrate one-versus-all classification\n    \n    '''\n    \n    #### initialize ####\n    def __init__(self,data):        \n        # grab input\n        data = data.T\n        self.data = data\n        self.x = data[:,:-1]\n        if self.x.ndim == 1:\n            self.x.shape = (len(self.x),1)\n        self.y = data[:,-1]\n        self.y.shape = (len(self.y),1)\n        \n        # colors for viewing classification data 'from above'\n        self.colors = [[ 0, 0.4, 1],[1,0,0.4],[0, 1, 0.5],[1, 0.7, 0.5],'violet','mediumaquamarine']\n\n        #self.colors = ['cornflowerblue','salmon','lime','bisque','mediumaquamarine','b','m','g']\n        \n        # create instance of optimizers\n        self.opt = optimizers.MyOptimizers()\n        \n    ### cost functions ###\n    # the counting cost function\n    def counting_cost(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = self.x[p]\n            y_p = self.y[p]\n            a_p = w[0] + sum([a*b for a,b in zip(w[1:],x_p)])\n            cost += (np.sign(a_p) - y_p)**2\n        return 0.25*cost\n    \n    # the perceptron relu cost\n    def relu(self,w):\n        cost = 0\n        for p in range(0,len(self.y_temp)):\n            x_p = self.x[p]\n            y_p = self.y_temp[p]\n            a_p = w[0] + sum([a*b for a,b in zip(w[1:],x_p)])\n            cost += np.maximum(0,-y_p*a_p)\n        return cost\n\n    # the convex softmax cost function\n    def softmax(self,w):\n        cost = 0\n        for p in range(0,len(self.y_temp)):\n            x_p = self.x[p]\n            y_p = self.y_temp[p]\n            a_p = w[0] + sum([a*b for a,b in zip(w[1:],x_p)])\n            cost += np.log(1 + np.exp(-y_p*a_p))\n        return cost\n                   \n    ### compare grad descent runs - given cost to counting cost ###\n    def solve_2class_subproblems(self,**kwargs):\n        # parse args\n        max_its = 5\n        if 'max_its' in kwargs:\n            max_its = kwargs['max_its']\n        alpha = 10**-3\n        if 'alpha' in kwargs:\n            alpha = kwargs['alpha']  \n        steplength_rule = 'none'\n        if 'steplength_rule' in kwargs:\n            steplength_rule = kwargs['steplength_rule']\n        version = 'unnormalized'\n        if 'version' in kwargs:\n            version = kwargs['version'] \n        algo = 'newtons_method'\n        if 'algo' in kwargs:\n            algo = kwargs['algo']\n         \n        #### perform all optimizations ###\n        self.g = self.softmax\n        if 'cost' in kwargs:\n            cost = kwargs['cost']\n            if cost == 'softmax':\n                self.g = self.softmax\n            if cost == 'relu':\n                self.g = self.relu\n\n        # loop over subproblems and solve\n        self.W = []\n        num_classes = np.size(np.unique(self.y))\n        for i in range(0,num_classes):\n            #print ('solving sub-problem number ' + str(i+1))\n            # prepare temporary C vs notC sub-probem labels\n            self.y_temp = copy.deepcopy(self.y)\n            ind = np.argwhere(self.y_temp == (i))\n            ind = ind[:,0]\n            ind2 = np.argwhere(self.y_temp != (i))\n            ind2 = ind2[:,0]\n            self.y_temp[ind] = 1\n            self.y_temp[ind2] = -1\n\n            # solve the current subproblem\n            if algo == 'gradient_descent':# run gradient descent\n                w_hist = self.opt.gradient_descent(g = self.g,w = np.random.randn(np.shape(self.x)[1]+1,1),version = version,max_its = max_its, alpha = alpha,steplength_rule = steplength_rule)\n            elif algo == 'newtons_method':\n                w_hist = self.opt.newtons_method(g = self.g,w = np.random.randn(np.shape(self.x)[1]+1,1),max_its = max_its,epsilon = 10**(-5))\n            \n            # store best weight for final classification \n            g_count = []\n            for j in range(len(w_hist)):\n                w = w_hist[j]\n                gval = self.g(w)\n                g_count.append(gval)\n            ind = np.argmin(g_count)\n            w = w_hist[ind]\n            \n            # normalize normal vectors for each classifier\n            w_norm = sum([v**2 for v in w[1:]])**(0.5)\n            w_1N = [v/w_norm for v in w]\n            self.W.append(w_1N)\n            \n        # reshape\n        self.W = np.asarray(self.W)\n        self.W.shape = (num_classes,np.shape(self.x)[1] + 1)\n    \n    # plotting function for the data and individual separators\n    def plot_data_and_subproblem_separators(self):\n        # determine plotting ranges\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n        \n        # initialize figure, plot data, and dress up panels with axes labels etc.\n        num_classes = np.size(np.unique(self.y))\n        \n        ##### setup figure to plot #####\n        # initialize figure\n        fig = plt.figure(figsize = (9,5))\n        gs = gridspec.GridSpec(2, num_classes) \n        \n        # create subplots for each sub-problem\n        r = np.linspace(minx,maxx,400)\n        for a in range(0,num_classes):\n            # setup current axis\n            ax = plt.subplot(gs[a],aspect = 'equal'); \n\n            # get current weights\n            w = self.W[a]\n                \n            # color current class\n            ax.scatter(self.x[:,0], self.x[:,1], s = 30,color = '0.75')\n            t = np.argwhere(self.y == a)\n            t = t[:,0]\n            ax.scatter(self.x[t,0],self.x[t,1], s = 50,color = self.colors[a],edgecolor = 'k',linewidth = 1.5)\n\n            # draw subproblem separator\n            z = - w[0]/w[2] - w[1]/w[2]*r\n            ax.plot(r,z,linewidth = 2,color = self.colors[a],zorder = 3)\n            ax.plot(r,z,linewidth = 2.75,color = 'k',zorder = 2)\n\n            # dress panel correctly\n            ax.set_xlim(minx,maxx)\n            ax.set_ylim(minx,maxx)\n            ax.axis('off')\n         \n        # plot final panel with all data and separators\n        ax4 = plt.subplot(gs[num_classes + 1],aspect = 'equal'); \n        self.plot_data(ax4)\n        self.plot_all_separators(ax4)\n\n        # dress panel\n        ax4.set_xlim(minx,maxx)\n        ax4.set_ylim(minx,maxx)\n        ax4.axis('off')\n            \n        plt.show()\n           \n    # show data\n    def show_dataset(self):\n        # initialize figure\n        fig = plt.figure(figsize = (8,4))\n        artist = fig\n        gs = gridspec.GridSpec(1, 3,width_ratios = [1,3,1]) \n\n        # setup current axis\n        ax = plt.subplot(gs[1],aspect = 'equal'); \n        \n        # run axis through data plotter\n        self.plot_data(ax)\n        \n        # determine plotting ranges\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n        \n        # dress panel\n        ax.set_xlim(minx,maxx)\n        ax.set_ylim(minx,maxx)\n        \n        plt.show()\n        \n    # color indnividual region using fusion rule\n    def show_fusion(self,region):\n        # generate input range for viewing range\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n        \n        # initialize figure\n        fig = plt.figure(figsize = (8,4))\n        artist = fig\n        gs = gridspec.GridSpec(1, 3,width_ratios = [1,3,1]) \n\n        # setup current axis\n        ax = plt.subplot(gs[1],aspect = 'equal');   \n        \n        # plot panel with all data and separators\n        self.plot_data(ax)\n        self.plot_all_separators(ax)\n        \n        # color region\n        self.region_coloring(region = region,ax = ax)\n        \n        # dress panel\n        ax.set_xlim(minx,maxx)\n        ax.set_ylim(minx,maxx)\n        ax.axis('off')\n        \n    # show coloring of entire space\n    def show_complete_coloring(self):\n        # generate input range for viewing range\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n        \n        # initialize figure\n        fig = plt.figure(figsize = (8,4))\n        gs = gridspec.GridSpec(1, 2,width_ratios = [1,1]) \n\n        # setup current axis\n        ax = plt.subplot(gs[0],aspect = 'equal');\n        ax2 = plt.subplot(gs[1],aspect = 'equal');\n        \n        # plot panel with all data and separators\n        self.plot_data(ax)\n        self.plot_data(ax2)\n        self.plot_all_separators(ax)\n                \n        ### draw multiclass boundary on right panel\n        r = np.linspace(minx,maxx,2000)\n        w1_vals,w2_vals = np.meshgrid(r,r)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(r)**2,1)\n        o = np.ones((len(r)**2,1))\n        h = np.concatenate([o,w1_vals,w2_vals],axis = 1)\n        pts = np.dot(self.W,h.T)\n        g_vals = np.argmax(pts,axis = 0)\n\n        # vals for cost surface\n        w1_vals.shape = (len(r),len(r))\n        w2_vals.shape = (len(r),len(r))\n        g_vals.shape = (len(r),len(r))\n        \n        # plot contour\n        C = len(np.unique(self.y))\n        ax2.contour(w1_vals,w2_vals,g_vals,colors = 'k',levels = range(0,C+1),linewidths = 2.75,zorder = 4)\n        ax2.contourf(w1_vals,w2_vals,g_vals+1,colors = self.colors[:],alpha = 0.2,levels = range(0,C+1))\n        ax.contourf(w1_vals,w2_vals,g_vals+1,colors = self.colors[:],alpha = 0.2,levels = range(0,C+1))\n\n        # dress panel\n        ax.set_xlim(minx,maxx)\n        ax.set_ylim(minx,maxx)\n        ax.axis('off')\n        \n        ax2.set_xlim(minx,maxx)\n        ax2.set_ylim(minx,maxx)\n        ax2.axis('off')     \n    \n    # point and projection illustration\n    def point_and_projection(self,point1,point2):\n        # generate range for viewing limits\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n        \n        # initialize figure\n        fig = plt.figure(figsize = (8,4))\n        gs = gridspec.GridSpec(1, 2,width_ratios = [1,1]) \n\n        # setup current axis\n        ax = plt.subplot(gs[0],aspect = 'equal');\n        ax2 = plt.subplot(gs[1],aspect = 'equal');\n        \n        ### plot left panel - data, separators, and region coloring\n        self.plot_data(ax)\n        self.plot_all_separators(ax)        \n        \n        ### determine projections etc.,\n        point = [1] + point1\n        point = np.asarray(point)\n        point.shape = (len(point),1)\n        y = np.dot(self.W,point)\n        ind = np.argwhere(y > 0)\n        if np.size(ind) == 0:\n            num_classes = len(np.unique(self.y))\n            ind = np.arange(num_classes).tolist()\n        else:\n            ind = [v[0] for v in ind]\n        point = point[1:]\n        ax.scatter(point[0],point[1],c = 'k',edgecolor = 'w',linewidth = 1,s = 90)\n\n        # loop over classifiers and project\n        for i in ind:\n            # get weights\n            w = self.W[i]\n            w = np.asarray(w)\n            w.shape = (len(w),1)\n            w_norm = sum([v**2 for v in w[1:]])\n\n            # make projected point\n            add_on = w[0] + sum([v*a for v,a in zip(point,w[1:])])\n            add_on /= w_norm\n            proj_point = copy.deepcopy(point)\n            proj_point -= add_on*w[1:]\n\n            # projected point\n            ax.scatter(proj_point[0],proj_point[1],c = self.colors[i],edgecolor = 'k',linewidth = 1,s = 60,zorder = 4,marker = 'X')\n                \n            # dashed line\n            l = np.linspace(proj_point[0],point[0],200)\n            b = np.linspace(proj_point[1],point[1],200)\n            ax.plot(l,b,linewidth = 1,linestyle = '--',color = 'k',zorder = 3)\n            \n        # dress panels\n        ax.set_xlim(minx,maxx)\n        ax.set_ylim(minx,maxx)\n        ax.axis('off')\n\n        ### plot left panel - data, separators, and region coloring\n        self.plot_data(ax2)\n        self.plot_all_separators(ax2)        \n        \n        ### determine projections etc.,\n        point = [1] + point2\n        point = np.asarray(point)\n        point.shape = (len(point),1)\n        y = np.dot(self.W,point)\n        ind = np.argwhere(y > 0)\n        if np.size(ind) == 0:\n            num_classes = len(np.unique(self.y))\n            ind = np.arange(num_classes).tolist()\n        else:\n            ind = [v[0] for v in ind]\n        point = point[1:]\n        ax2.scatter(point[0],point[1],c = 'k',edgecolor = 'w',linewidth = 1,s = 90)\n\n        # loop over classifiers and project\n        for i in ind:\n            # get weights\n            w = self.W[i]\n            w = np.asarray(w)\n            w.shape = (len(w),1)\n            w_norm = sum([v**2 for v in w[1:]])\n\n            # make projected point\n            add_on = w[0] + sum([v*a for v,a in zip(point,w[1:])])\n            add_on /= w_norm\n            proj_point = copy.deepcopy(point)\n            proj_point -= add_on*w[1:]\n\n            # projected point\n            ax2.scatter(proj_point[0],proj_point[1],c = self.colors[i],edgecolor = 'k',linewidth = 1,s = 60,zorder = 4,marker = 'X')\n                \n            # dashed line\n            l = np.linspace(proj_point[0],point[0],200)\n            b = np.linspace(proj_point[1],point[1],200)\n            ax2.plot(l,b,linewidth = 1,linestyle = '--',color = 'k',zorder = 3)\n            \n        # dress panels\n        ax2.set_xlim(minx,maxx)\n        ax2.set_ylim(minx,maxx)\n        ax2.axis('off')\n\n    ###### utility functions - individual data/separators plotters ###### \n    # plot regions colored by classification\n    def region_coloring(self,region,ax):        \n        #### color first regions  ####\n        # generate input range for functions\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n        \n        # plot over range\n        r = np.linspace(minx,maxx,200)\n        x1_vals,x2_vals = np.meshgrid(r,r)\n        x1_vals.shape = (len(r)**2,1)\n        x2_vals.shape = (len(r)**2,1)\n        o = np.ones((len(r)**2,1))\n        x = np.concatenate([o,x1_vals,x2_vals],axis = 1)\n        \n        ### for region 1, determine points that are uniquely positive for each classifier ###\n        ind_set = []\n        y = np.dot(self.W,x.T)\n        num_classes = np.size(np.unique(self.y))\n        \n        if region == 1 or region == 'all':\n            for i in range(0,num_classes):       \n                class_inds = np.arange(num_classes)\n                class_inds = np.delete(class_inds,(i),axis = 0)\n\n                # loop over non-current classifier\n                ind = np.argwhere(y[class_inds[0]] < 0).tolist()\n                ind = [s[0] for s in ind]\n                for j in range(1,len(class_inds)):\n                    c_ind = class_inds[j]\n                    ind2 = np.argwhere(y[c_ind] < 0).tolist()\n                    ind2 = [s[0] for s in ind2]\n                    ind = [s for s in ind if s in ind2]                \n\n                ind2 = np.argwhere(y[i] > 0).tolist()\n                ind2 = [s[0] for s in ind2]\n                ind = [s for s in ind if s in ind2]\n\n                # plot polygon over region defined by ind\n                x1_ins = np.asarray([x1_vals[s] for s in ind])\n                x1_ins.shape = (len(x1_ins),1)\n                x2_ins = np.asarray([x2_vals[s] for s in ind])\n                x2_ins.shape = (len(x2_ins),1)\n                h = np.concatenate((x1_ins,x2_ins),axis = 1)\n                vertices = ConvexHull(h).vertices\n                poly = [h[v] for v in vertices]\n                polygon = Polygon(poly, True)   \n                patches = []\n                patches.append(polygon)\n\n                p = PatchCollection(patches, alpha=0.2,color = self.colors[i])\n                ax.add_collection(p)\n                \n        if region == 2 or region == 'all':\n            for i in range(0,num_classes):       \n                class_inds = np.arange(num_classes)\n                class_inds = np.delete(class_inds,(i),axis = 0)\n\n                # loop over non-current classifier\n                ind = np.argwhere(y[class_inds[0]] > 0).tolist()\n                ind = [s[0] for s in ind]\n                for j in range(1,len(class_inds)):\n                    c_ind = class_inds[j]\n                    ind2 = np.argwhere(y[c_ind] > 0).tolist()\n                    ind2 = [s[0] for s in ind2]\n                    ind = [s for s in ind if s in ind2]                \n\n                ind2 = np.argwhere(y[i] < 0).tolist()\n                ind2 = [s[0] for s in ind2]\n                ind = [s for s in ind if s in ind2]\n\n                # plot polygon over region defined by ind\n                x1_ins = np.asarray([x1_vals[s] for s in ind])\n                x1_ins.shape = (len(x1_ins),1)\n                x2_ins = np.asarray([x2_vals[s] for s in ind])\n                x2_ins.shape = (len(x2_ins),1)\n                o = np.ones((len(x2_ins),1))\n                h = np.concatenate((o,x1_ins,x2_ins),axis = 1)\n                \n                # determine regions dominated by one classifier or the other\n                vals = []\n                for c in class_inds:\n                    w = self.W[int(c)]\n                    nv = np.dot(w,h.T)\n                    vals.append(nv)\n                vals = np.asarray(vals)\n                vals.shape = (len(class_inds),len(h))\n                ind = np.argmax(vals,axis = 0)\n\n                for j in range(len(class_inds)):\n                    # make polygon for each subregion\n                    ind1 = np.argwhere(ind == j)\n                    x1_ins2 = np.asarray([x1_ins[s] for s in ind1])\n                    x1_ins2.shape = (len(x1_ins2),1)\n                    x2_ins2 = np.asarray([x2_ins[s] for s in ind1])\n                    x2_ins2.shape = (len(x2_ins2),1)\n                    h = np.concatenate((x1_ins2,x2_ins2),axis = 1)\n                    \n                    # find convex hull of points\n                    vertices = ConvexHull(h).vertices\n                    poly = [h[v] for v in vertices]\n                    polygon = Polygon(poly, True)   \n                    patches = []\n                    patches.append(polygon)\n                    c = class_inds[j]\n                    p = PatchCollection(patches, alpha=0.2,color = self.colors[c])\n                    ax.add_collection(p)\n                    \n        if region == 3 or region == 'all':\n            # find negative zone of all classifiers\n            ind = np.argwhere(y[0] < 0).tolist()\n            ind = [s[0] for s in ind]\n            for i in range(1,num_classes):\n                ind2 = np.argwhere(y[i] < 0).tolist()\n                ind2 = [s[0] for s in ind2]\n                ind = [s for s in ind if s in ind2]                \n\n            # loop over negative zone, find max area of each classifier\n            x1_ins = np.asarray([x1_vals[s] for s in ind])\n            x1_ins.shape = (len(x1_ins),1)\n            x2_ins = np.asarray([x2_vals[s] for s in ind])\n            x2_ins.shape = (len(x2_ins),1)\n            o = np.ones((len(x2_ins),1))\n            h = np.concatenate((o,x1_ins,x2_ins),axis = 1)\n                \n            # determine regions dominated by one classifier or the other\n            vals = []\n            for c in range(num_classes):\n                w = self.W[c]\n                nv = np.dot(w,h.T)\n                vals.append(nv)\n            vals = np.asarray(vals)\n            vals.shape = (num_classes,len(h))\n            ind = np.argmax(vals,axis = 0)\n\n            # loop over each class, construct polygon region for each\n            for c in range(num_classes):\n                # make polygon for each subregion\n                ind1 = np.argwhere(ind == c)\n                x1_ins2 = np.asarray([x1_ins[s] for s in ind1])\n                x1_ins2.shape = (len(x1_ins2),1)\n                x2_ins2 = np.asarray([x2_ins[s] for s in ind1])\n                x2_ins2.shape = (len(x2_ins2),1)\n                h = np.concatenate((x1_ins2,x2_ins2),axis = 1)\n                    \n                # find convex hull of points\n                vertices = ConvexHull(h).vertices\n                poly = [h[v] for v in vertices]\n                polygon = Polygon(poly, True)   \n                patches = []\n                patches.append(polygon)\n                p = PatchCollection(patches, alpha=0.2,color = self.colors[c])\n                ax.add_collection(p)    \n                \n                \n    # plot data\n    def plot_data(self,ax):\n        # initialize figure, plot data, and dress up panels with axes labels etc.\n        num_classes = np.size(np.unique(self.y))\n                \n        # color current class\n        for a in range(0,num_classes):\n            t = np.argwhere(self.y == a)\n            t = t[:,0]\n            ax.scatter(self.x[t,0],self.x[t,1], s = 50,color = self.colors[a],edgecolor = 'k',linewidth = 1.5)\n        \n    # plot separators\n    def plot_all_separators(self,ax):\n        # determine plotting ranges\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n        \n        # initialize figure, plot data, and dress up panels with axes labels etc.\n        num_classes = np.size(np.unique(self.y))\n                \n        # color current class\n        r = np.linspace(minx,maxx,400)\n        for a in range(0,num_classes):\n            # get current weights\n            w = self.W[a]\n            \n            # draw subproblem separator\n            z = - w[0]/w[2] - w[1]/w[2]*r\n            r = np.linspace(minx,maxx,400)\n            ax.plot(r,z,linewidth = 2,color = self.colors[a],zorder = 3)\n            ax.plot(r,z,linewidth = 2.75,color = 'k',zorder = 2)"""
mlrefined_libraries/superlearn_library/ova_multiclass_static_plotter.py,25,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nimport math\nimport time\nimport copy\n\nimport sys\nsys.path.append(\'../\')\nfrom mlrefined_libraries import superlearn_library as superlearn\ncost_lib = superlearn.cost_functions\n\nclass Visualizer:\n    \'\'\'\n    Illustrate a run of your preferred optimization algorithm on a one or two-input function.  Run\n    the algorithm first, and input the resulting weight history into this wrapper.\n    \'\'\' \n\n    ##### draw picture of function and run for two-input function ####       \n    def two_input_contour_plot(self,weight_history,x,y,**kwargs):\n        cost_name = \'softmax\'\n        if \'cost_name\' in kwargs:\n            cost_name = kwargs[\'cost_name\']\n            \n        # compute number of classes\n        C = np.shape(weight_history[0])[1]\n        \n        ##### construct figure with panels #####\n        # construct figure\n        fig = plt.figure(figsize = (10,6))\n\n        # create figure with single plot for contour\n        gs = gridspec.GridSpec(2, 2) \n       \n        ### make contour right plot - as well as horizontal and vertical axes ###\n        for c in range(C):\n            # prepare temporary C vs notC sub-probem labels\n            y_temp = copy.deepcopy(y)\n            ind = np.argwhere(y_temp.astype(int) == c)\n            ind = ind[:,1]\n            ind2 = np.argwhere(y_temp.astype(int) != c)\n            ind2 = ind2[:,1]\n            y_temp[0,ind] = 1\n            y_temp[0,ind2] = -1\n            \n            g = cost_lib.choose_cost(x,y_temp,cost_name)\n            \n            # create panel\n            ax = plt.subplot(gs[c])\n            ax.set_aspect(\'equal\')\n\n            # plot contour and path \n            w_hist = [weight_history[v][:,c][:,np.newaxis] for v in range(len(weight_history))]\n            self.contour_plot_setup(c,C,g,ax,**kwargs)  # draw contour plot\n            self.draw_weight_path(ax,w_hist,**kwargs)        # draw path on contour plot\n            \n            # label axes\n            ax.set_xlabel(r\'$w_0^{(\' + str(c+1) + \')}$\',fontsize = 15)\n            ax.set_ylabel(r\'$w_1^{(\' + str(c+1) + \')}$\',fontsize = 15,labelpad = 15,rotation = 0)\n            \n        # remove whitespace from figure\n        #gs.update(wspace=0.005, hspace=0.15) # set the spacing between axes. \n        #fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # remove whitespace\n        fig.subplots_adjust(wspace=0.001,hspace=0.001)\n        \n        # plot\n        plt.show()\n\n    ########################################################################################\n    #### utility functions - for setting up / making contour plots, 3d surface plots, etc., ####\n    # show contour plot of input function\n    def contour_plot_setup(self,c,C,g,ax,**kwargs):\n        xmin = -3.1\n        xmax = 3.1\n        ymin = -3.1\n        ymax = 3.1\n        if \'xmin\' in kwargs:            \n            xmin = kwargs[\'xmin\']\n        if \'xmax\' in kwargs:\n            xmax = kwargs[\'xmax\']\n        if \'ymin\' in kwargs:            \n            ymin = kwargs[\'ymin\']\n        if \'ymax\' in kwargs:\n            ymax = kwargs[\'ymax\']      \n        num_contours = 20\n        if \'num_contours\' in kwargs:\n            num_contours = kwargs[\'num_contours\']   \n            \n        # choose viewing range using weight history?\n        if \'view_by_weights\' in kwargs:\n            view_by_weights = True\n            weight_history = kwargs[\'weight_history\']\n            if view_by_weights == True:\n                xmin = min([v[0] for v in weight_history])[0]\n                xmax = max([v[0] for v in weight_history])[0]\n                xgap = (xmax - xmin)*0.25\n                xmin -= xgap\n                xmax += xgap\n\n                ymin = min([v[1] for v in weight_history])[0]\n                ymax = max([v[1] for v in weight_history])[0]\n                ygap = (ymax - ymin)*0.25\n                ymin -= ygap\n                ymax += ygap\n \n        ### plot function as contours ###\n        self.draw_contour_plot(c,C,g,ax,num_contours,xmin,xmax,ymin,ymax)\n        \n        ### cleanup panel ###\n        ax.axhline(y=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        ax.axvline(x=0, color=\'k\',zorder = 0,linewidth = 0.5)\n        \n        # set viewing limits\n        ax.set_xlim(xmin,xmax)\n        ax.set_ylim(ymin,ymax)\n\n    ### function for creating contour plot\n    def draw_contour_plot(self,c,C,g,ax,num_contours,xmin,xmax,ymin,ymax):\n        #### define input space for function and evaluate ####\n        w1 = np.linspace(xmin,xmax,100)\n        w2 = np.linspace(ymin,ymax,100)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = []\n        for e in range(len(w1)**2):\n            s = np.reshape(h[e,:],(2,1))\n            func_vals.append(g(s))\n        func_vals = np.array(func_vals)\n        w1_vals.shape = (len(w1),len(w1))\n        w2_vals.shape = (len(w2),len(w2))\n        func_vals.shape = (len(w1),len(w2)) \n\n        ### make contour right plot - as well as horizontal and vertical axes ###\n        # set level ridges\n        levelmin = min(func_vals.flatten())\n        levelmax = max(func_vals.flatten())\n        cut = 0.4\n        cutoff = (levelmax - levelmin)\n        levels = [levelmin + cutoff*cut**(num_contours - i) for i in range(0,num_contours+1)]\n        levels = [levelmin] + levels\n        levels = np.asarray(levels)\n   \n        a = ax.contour(w1_vals, w2_vals, func_vals,levels = levels,colors = \'k\')\n        b = ax.contourf(w1_vals, w2_vals, func_vals,levels = levels,cmap = \'Blues\')\n        \n        \n    ### makes color spectrum for plotted run points - from green (start) to red (stop)\n    def make_colorspec(self,w_hist):\n        # make color range for path\n        s = np.linspace(0,1,len(w_hist[:round(len(w_hist)/2)]))\n        s.shape = (len(s),1)\n        t = np.ones(len(w_hist[round(len(w_hist)/2):]))\n        t.shape = (len(t),1)\n        s = np.vstack((s,t))\n        colorspec = []\n        colorspec = np.concatenate((s,np.flipud(s)),1)\n        colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n        return colorspec\n        \n    ### function for drawing weight history path\n    def draw_weight_path(self,ax,w_hist,**kwargs):\n        # make colors for plot\n        colorspec = self.make_colorspec(w_hist)\n        \n        arrows = True\n        if \'arrows\' in kwargs:\n            arrows = kwargs[\'arrows\']\n\n        ### plot function decrease plot in right panel\n        for j in range(len(w_hist)):  \n            w_val = w_hist[j]\n\n            # plot each weight set as a point\n            ax.scatter(w_val[0],w_val[1],s = 80,c = colorspec[j],edgecolor = \'k\',linewidth = 2*math.sqrt((1/(float(j) + 1))),zorder = 3)\n\n            # plot connector between points for visualization purposes\n            if j > 0:\n                pt1 = w_hist[j-1]\n                pt2 = w_hist[j]\n                \n                # produce scalar for arrow head length\n                pt_length = np.linalg.norm(pt1 - pt2)\n                head_length = 0.1\n                alpha = (head_length - 0.35)/pt_length + 1\n                \n                # if points are different draw error\n                if np.linalg.norm(pt1 - pt2) > head_length and arrows == True:\n                    if np.ndim(pt1) > 1:\n                        pt1 = pt1.flatten()\n                        pt2 = pt2.flatten()\n                    \n                    ax.arrow(pt1[0],pt1[1],(pt2[0] - pt1[0])*alpha,(pt2[1] - pt1[1])*alpha, head_width=0.1, head_length=head_length, fc=\'k\', ec=\'k\',linewidth=4,zorder = 2,length_includes_head=True)\n                    ax.arrow(pt1[0],pt1[1],(pt2[0] - pt1[0])*alpha,(pt2[1] - pt1[1])*alpha, head_width=0.1, head_length=head_length, fc=\'w\', ec=\'w\',linewidth=0.25,zorder = 2,length_includes_head=True)\n        \n    ### draw surface plot\n    def draw_surface(self,g,ax,**kwargs):\n        xmin = -3.1\n        xmax = 3.1\n        ymin = -3.1\n        ymax = 3.1\n        if \'xmin\' in kwargs:            \n            xmin = kwargs[\'xmin\']\n        if \'xmax\' in kwargs:\n            xmax = kwargs[\'xmax\']\n        if \'ymin\' in kwargs:            \n            ymin = kwargs[\'ymin\']\n        if \'ymax\' in kwargs:\n            ymax = kwargs[\'ymax\']   \n            \n        #### define input space for function and evaluate ####\n        w1 = np.linspace(xmin,xmax,200)\n        w2 = np.linspace(ymin,ymax,200)\n        w1_vals, w2_vals = np.meshgrid(w1,w2)\n        w1_vals.shape = (len(w1)**2,1)\n        w2_vals.shape = (len(w2)**2,1)\n        h = np.concatenate((w1_vals,w2_vals),axis=1)\n        func_vals = np.asarray([g(np.reshape(s,(2,1))) for s in h])\n\n        ### plot function as surface ### \n        w1_vals.shape = (len(w1),len(w2))\n        w2_vals.shape = (len(w1),len(w2))\n        func_vals.shape = (len(w1),len(w2))\n        ax.plot_surface(w1_vals, w2_vals, func_vals, alpha = 0.1,color = \'w\',rstride=25, cstride=25,linewidth=1,edgecolor = \'k\',zorder = 2)\n\n        # plot z=0 plane \n        ax.plot_surface(w1_vals, w2_vals, func_vals*0, alpha = 0.1,color = \'w\',zorder = 1,rstride=25, cstride=25,linewidth=0.3,edgecolor = \'k\') \n                \n        # clean up axis\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n\n        ax.xaxis.pane.set_edgecolor(\'white\')\n        ax.yaxis.pane.set_edgecolor(\'white\')\n        ax.zaxis.pane.set_edgecolor(\'white\')\n\n        ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n        \n        ax.set_xlabel(\'$w_0$\',fontsize = 14)\n        ax.set_ylabel(\'$w_1$\',fontsize = 14,rotation = 0)\n        ax.set_title(\'$g(w_0,w_1)$\',fontsize = 14)\n        \n\n    ### plot points and connectors in input space in 3d plot        \n    def show_inputspace_path(self,w_hist,ax):\n        # make colors for plot\n        colorspec = self.make_colorspec(w_hist)\n        \n        for k in range(len(w_hist)):\n            pt1 = w_hist[k]\n            ax.scatter(pt1[0],pt1[1],0,s = 60,c = colorspec[k],edgecolor = \'k\',linewidth = 0.5*math.sqrt((1/(float(k) + 1))),zorder = 3)\n            if k < len(w_hist)-1:\n                pt2 = w_hist[k+1]\n                if np.linalg.norm(pt1 - pt2) > 10**(-3):\n                    # draw arrow in left plot\n                    a = Arrow3D([pt1[0],pt2[0]], [pt1[1],pt2[1]], [0, 0], mutation_scale=10, lw=2, arrowstyle=""-|>"", color=""k"")\n                    ax.add_artist(a)\n        \n#### custom 3d arrow and annotator functions ###    \n# nice arrow maker from https://stackoverflow.com/questions/11140163/python-matplotlib-plotting-a-3d-cube-a-sphere-and-a-vector\nclass Arrow3D(FancyArrowPatch):\n\n    def __init__(self, xs, ys, zs, *args, **kwargs):\n        FancyArrowPatch.__init__(self, (0, 0), (0, 0), *args, **kwargs)\n        self._verts3d = xs, ys, zs\n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n        FancyArrowPatch.draw(self, renderer)'"
mlrefined_libraries/superlearn_library/regression_probabilistic_demos.py,15,"b'# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\n\nclass Visualizer:\n    \'\'\'\n    Visualize linear regression in 2 and 3 dimensions.  For single input cases (2 dimensions) the path of gradient descent on the cost function can be animated.\n    \'\'\'\n    #### initialize ####\n    def __init__(self,data):\n        # grab input\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n        \n    def center_data(self):\n        # center data\n        self.x = self.x - np.mean(self.x)\n        self.y = self.y - np.mean(self.y)\n        \n    def run_algo(self,algo,**kwargs):\n        # Get function and compute gradient\n        self.g = self.least_squares\n        self.grad = compute_grad(self.g)\n        \n        # choose algorithm\n        self.algo = algo\n        if self.algo == \'gradient_descent\':\n            self.alpha = 10**-3\n            if \'alpha\' in kwargs:\n                self.alpha = kwargs[\'alpha\']\n        \n        self.max_its = 10\n        if \'max_its\' in kwargs:\n            self.max_its = kwargs[\'max_its\']\n            \n        self.w_init = np.random.randn(2)\n        if \'w_init\' in kwargs:\n            self.w_init = kwargs[\'w_init\']\n            self.w_init = np.asarray([float(s) for s in self.w_init])\n            self.w_init.shape = (len(self.w_init),1)\n            \n        # run algorithm of choice\n        if self.algo == \'gradient_descent\':\n            self.w_hist = []\n            self.gradient_descent()\n        if self.algo == \'newtons_method\':\n            self.hess = compute_hess(self.g)           # hessian of input function\n            self.beta = 0\n            if \'beta\' in kwargs:\n                self.beta = kwargs[\'beta\']\n            self.w_hist = []\n            self.newtons_method()            \n    \n    ######## linear regression functions ########    \n    def least_squares(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = copy.deepcopy(self.x[p,:])\n            x_p.shape = (len(x_p),1)\n            y_p = self.y[p]\n            cost +=(w[0] + np.dot(w[1:].T,x_p) - y_p)**2\n        return cost\n    \n    def predict(self,w,x_new):\n        model = w[0] + np.dot(w[1:].T,x_new)\n        return model\n    \n    def compute_errors(self,w):\n        errors = []\n        for p in range(len(self.y)):\n            x_p = copy.deepcopy(self.x[p,:])\n            x_p.shape = (len(x_p),1)\n            y_p = self.y[p]\n            y_predict =  w[0] + np.dot(w[1:].T,x_p)\n            error = y_predict - y_p\n            errors.append(error)\n        errors = np.asarray([s[0] for s in errors])\n        return errors\n        \n    #### run newton\'s method ####\n    def newtons_method(self):\n        w = self.w_init\n        self.w_hist = []\n        self.w_hist.append(w)\n        for k in range(self.max_its):\n            # plug in value into func and derivative\n            grad_eval = self.grad(w)\n            hess_eval = self.hess(w)\n            \n            # reshape for numpy linalg functionality\n            hess_eval.shape = (int((np.size(hess_eval))**(0.5)),int((np.size(hess_eval))**(0.5)))\n\n            # solve linear system for weights\n            w = w - np.dot(np.linalg.pinv(hess_eval + self.beta*np.eye(np.size(w))),grad_eval)\n                                \n            # record\n            self.w_hist.append(w)\n            \n    ### plot data, fit, and histogram of errors ###\n    def error_hist(self,**kwargs):\n        fig = plt.figure(figsize = (8,3))\n        \n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]);\n    \n        ### plot data, fit linear regression, plot fit\n        # scatter data in left panel\n        self.scatter_pts(ax1)\n        \n        # find best fit         \n        w = self.w_hist[1]\n        \n        # run model with best weight\n        xmin = copy.deepcopy(min(self.x))\n        xmax = copy.deepcopy(max(self.x))\n        xgap = (xmax - xmin)*0.1\n        xmin-=xgap\n        xmax+=xgap\n        x_fit = np.linspace(xmin,xmax,300)\n        y_fit = w[0] + w[1]*x_fit\n        \n        # plot fit \n        ax1.plot(x_fit,y_fit,color = \'r\',linewidth = 3) \n        \n        ### plot histogram of errors in right plot ###\n        # compute actual error of fully trained model\n        errors = self.compute_errors(w)\n        num_bins = 5\n        if \'num_bins\' in kwargs:\n            num_bins = kwargs[\'num_bins\']\n        ax2.hist(errors, normed=True, bins=num_bins,facecolor=\'blue\', alpha=0.5,edgecolor = \'k\')\n        \n        # label axes\n        if \'xlabel\' in kwargs:\n            xlabel = kwargs[\'xlabel\']\n            ax1.set_xlabel(xlabel,fontsize = 12)\n        if \'ylabel\' in kwargs:\n            ylabel = kwargs[\'ylabel\']\n            ax1.set_ylabel(ylabel,fontsize = 12,rotation = 90)          \n        plt.show()\n\n    # scatter points\n    def scatter_pts(self,ax):\n        if np.shape(self.x)[1] == 1:\n            # set plotting limits\n            xmax = copy.deepcopy(max(self.x))\n            xmin = copy.deepcopy(min(self.x))\n            xgap = (xmax - xmin)*0.2\n            xmin -= xgap\n            xmax += xgap\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(self.x,self.y,color = \'k\', edgecolor = \'w\',linewidth = 0.9,s = 20)\n\n            # clean up panel\n            ax.set_xlim([xmin,xmax])\n            ax.set_ylim([ymin,ymax])\n            \n            # label axes\n            ax.set_xlabel(r\'$x$\', fontsize = 12)\n            ax.set_ylabel(r\'$y$\', rotation = 0,fontsize = 12)\n            ax.set_title(\'data\', fontsize = 13)\n            \n        if np.shape(self.x)[1] == 2:\n            # set plotting limits\n            xmax1 = copy.deepcopy(max(self.x[:,0]))\n            xmin1 = copy.deepcopy(min(self.x[:,0]))\n            xgap1 = (xmax1 - xmin1)*0.35\n            xmin1 -= xgap1\n            xmax1 += xgap1\n            \n            xmax2 = copy.deepcopy(max(self.x[:,0]))\n            xmin2 = copy.deepcopy(min(self.x[:,0]))\n            xgap2 = (xmax2 - xmin2)*0.35\n            xmin2 -= xgap2\n            xmax2 += xgap2\n            \n            ymax = max(self.y)\n            ymin = min(self.y)\n            ygap = (ymax - ymin)*0.2\n            ymin -= ygap\n            ymax += ygap    \n\n            # initialize points\n            ax.scatter(self.x[:,0],self.x[:,1],self.y,s = 40,color = \'k\', edgecolor = \'w\',linewidth = 0.9)\n\n            # clean up panel\n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            ax.set_zlim([ymin,ymax])\n            \n            ax.set_xticks(np.arange(round(xmin1) +1, round(xmax1), 1.0))\n            ax.set_yticks(np.arange(round(xmin2) +1, round(xmax2), 1.0))\n\n            # label axes\n            ax.set_xlabel(r\'$x_1$\', fontsize = 12,labelpad = 5)\n            ax.set_ylabel(r\'$x_2$\', rotation = 0,fontsize = 12,labelpad = 5)\n            ax.set_zlabel(r\'$y$\', rotation = 0,fontsize = 12,labelpad = -3)\n\n            # clean up panel\n            ax.xaxis.pane.fill = False\n            ax.yaxis.pane.fill = False\n            ax.zaxis.pane.fill = False\n\n            ax.xaxis.pane.set_edgecolor(\'white\')\n            ax.yaxis.pane.set_edgecolor(\'white\')\n            ax.zaxis.pane.set_edgecolor(\'white\')\n\n            ax.xaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.yaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n            ax.zaxis._axinfo[""grid""][\'color\'] =  (1,1,1,0)\n'"
mlrefined_libraries/superlearn_library/sparse_feature_selection_animator.py,25,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\nfrom matplotlib import gridspec\nimport matplotlib.patches as mpatches\n\n# import autograd functionality\nimport autograd.numpy as np\nfrom autograd.misc.flatten import flatten_func\nfrom autograd import value_and_grad \n\n# import some basic libs\nimport math\nimport copy\nimport time\n\n# import mlrefined libraries\nfrom mlrefined_libraries import math_optimization_library as optlib\n\nclass Visualizer:\n    '''\n    animations for visualizing sparse feature selection for regression and \n    classification\n    '''\n    def __init__(self,x,y,**kwargs):\n        # get input/output pairs\n        self.x_orig = x\n        self.y_orig = y\n       \n        # normalize input for optimization\n        normalize = False\n        normalize_out = False\n        if 'normalize' in kwargs:\n            normalize = kwargs['normalize']\n        if 'normalize_out' in kwargs:\n            normalize_out = kwargs['normalize_out']\n        if normalize == True:\n            # normalize input?\n            normalizer,inverse_normalizer = self.standard_normalizer(self.x_orig)\n\n            # normalize input by subtracting off mean and dividing by standard deviation\n            self.x = normalizer(self.x_orig)\n        else:\n            self.x = x_orig\n            \n        if normalize_out == True:\n            # normalize input?\n            normalizer,inverse_normalizer = self.standard_normalizer(self.y_orig)\n\n            # normalize input by subtracting off mean and dividing by standard deviation\n            self.y = normalizer(self.y_orig)\n        else:\n            self.y = self.y_orig\n        \n    # standard normalization function \n    def standard_normalizer(self,x):\n        # compute the mean and standard deviation of the input\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n\n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(x_stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((x_stds.shape))\n            adjust[ind] = 1.0\n            x_stds += adjust\n\n        # create standard normalizer function\n        normalizer = lambda data: (data - x_means)/x_stds\n\n        # create inverse standard normalizer\n        inverse_normalizer = lambda data: data*x_stds + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer\n\n    #### main animator #####\n    # compare multiple l1 regularized runs\n    def compare_lams(self,savepath,g,lams,**kwargs):       \n        if 'counter' in kwargs:\n            counter = kwargs['counter']\n            \n        # initialize figure\n        fig = plt.figure(figsize = (9,3))\n        artist = fig\n        num_lams = len(lams)\n        gs = gridspec.GridSpec(1,1) \n        ax = plt.subplot(gs[0])\n        \n        ### run over all input lamdas ###\n        # setup optimizations\n        alpha_choice = 10**(-1)\n        max_its = 1000\n        batch_size = self.y.size\n        algo = 'gradient_descent'\n        w = 0.1*np.random.randn(self.x.shape[0]+1,1)\n        if 'max_its' in kwargs:\n            max_its = kwargs['max_its']\n        if 'alpha' in kwargs:\n            alpha = kwargs['alpha']\n        if 'batch_size' in kwargs:\n            batch_size = kwargs['batch_size']\n        if 'algo' in kwargs:\n            algo = kwargs['algo']\n            \n        # start animation\n        num_frames = len(lams)\n        print ('starting animation rendering...')\n        def animate(k):            \n            # clear panels\n            ax.cla()\n            lam = lams[k]\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n\n            # run optimization\n            if algo == 'gradient_descent':\n                weight_history,cost_history = self.gradient_descent(g,w,self.x,self.y,lam,alpha_choice,max_its,batch_size)\n            if algo == 'RMSprop':\n                weight_history,cost_history = self.RMSprop(g,w,self.x,self.y,lam,alpha_choice,max_its,batch_size)\n\n            # choose set of weights to plot based on lowest cost val\n            ind = np.argmin(cost_history)\n            \n            # classification? then base on accuracy\n            if 'counter' in kwargs:\n                # create counting cost history as well\n                counts = [counter(v,self.x,self.y,lam) for v in weight_history]\n                if k == 0:\n                    ind = np.argmin(counts)\n                count = counts[ind]\n                acc = 1 - count/self.y.size\n                acc = np.round(acc,2)\n                \n            # save lowest misclass weights\n            w_best = weight_history[ind][1:]\n            \n            # plot\n            ax.axhline(c='k',zorder = 2)\n            \n            # make bar plot\n            ax.bar(np.arange(0,len(w_best)), w_best, color='k', alpha=0.5)\n                \n            # dress panel\n            title1 = r'$\\lambda = ' + str(np.round(lam,2)) + '$' \n            costval = cost_history[ind][0]\n            title2 = ', cost val = ' + str(np.round(costval,2))\n            if 'counter' in kwargs:\n                title2 = ', accuracy = ' + str(acc)\n            title = title1 + title2\n            ax.set_title(title)\n            ax.set_xlabel('learned weights')\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()\n\n    \n    # static graphics\n    def plot_regress(self,id1,labels):\n        fig = plt.figure(figsize = (10,3))\n        plt.xlabel(labels[0])\n        plt.ylabel(labels[1])\n        plt.scatter(self.x_orig[id1,:],self.y_orig,color = 'k',edgecolor = 'w',s = 30)\n        #plt.axes().set_aspect('equal', 'datalim')\n        plt.show()\n\n\n    def plot_classif(self, id_1, id_2,labels):\n        # create figure for plotting\n        fig = plt.figure(figsize = (5,5))\n\n        # setup colors / labels for plot\n        red_patch = mpatches.Patch(color='red', label=labels[0])\n        blue_patch = mpatches.Patch(color='blue', label=labels[1])\n        plt.legend(handles=[red_patch, blue_patch])\n        plt.legend(handles=[red_patch, blue_patch], loc = 2)\n        \n        # scatter plot data\n        ind = np.argwhere(self.y == -1)\n        ind = [v[1] for v in ind]\n        plt.scatter(self.x_orig[id_1,ind],self.x_orig[id_2,ind], color='r', s=30) #plotting the data\n        \n        ind = np.argwhere(self.y == +1)\n        ind = [v[1] for v in ind]\n\n        plt.scatter(self.x_orig[id_1,ind],self.x_orig[id_2,ind], color='b', s=30) #plotting the data        \n        plt.show()\n    \n    #### optimizers ####\n    # minibatch gradient descent\n    def gradient_descent(self,g,w,x_train,y_train,lam,alpha_choice,max_its,batch_size): \n        # flatten the input function, create gradient based on flat function\n        g_flat, unflatten, w = flatten_func(g, w)\n        grad = value_and_grad(g_flat)\n\n        # record history\n        num_train = y_train.shape[1]\n        w_hist = [unflatten(w)]\n        train_hist = [g_flat(w,x_train,y_train,lam,np.arange(num_train))]\n        \n        # how many mini-batches equal the entire dataset?\n        num_batches = int(np.ceil(np.divide(num_train, batch_size)))\n\n        # over the line\n        alpha = 0\n        for k in range(max_its):             \n            # check if diminishing steplength rule used\n            if alpha_choice == 'diminishing':\n                alpha = 1/float(k)\n            else:\n                alpha = alpha_choice\n            \n            for b in range(num_batches):\n                # collect indices of current mini-batch\n                batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_train))\n            \n                # plug in value into func and derivative\n                cost_eval,grad_eval = grad(w,x_train,y_train,lam,batch_inds)\n                grad_eval.shape = np.shape(w)\n\n                # take descent step with momentum\n                w = w - alpha*grad_eval\n\n            # update training and validation cost\n            train_cost = g_flat(w,x_train,y_train,lam,np.arange(num_train))\n\n            # record weight update, train and val costs\n            w_hist.append(unflatten(w))\n            train_hist.append(train_cost)\n        return w_hist,train_hist\n\n    # minibatch gradient descent\n    def RMSprop(self,g,w,x_train,y_train,lam,alpha,max_its,batch_size,**kwargs): \n        # rmsprop params\n        gamma=0.9\n        eps=10**-8\n        if 'gamma' in kwargs:\n            gamma = kwargs['gamma']\n        if 'eps' in kwargs:\n            eps = kwargs['eps']\n\n        # flatten the input function, create gradient based on flat function\n        g_flat, unflatten, w = flatten_func(g, w)\n        grad = value_and_grad(g_flat)\n\n        # initialize average gradient\n        avg_sq_grad = np.ones(np.size(w))\n\n        # record history\n        num_train = y_train.size\n        w_hist = [unflatten(w)]\n        train_hist = [g_flat(w,x_train,y_train,lam,np.arange(num_train))]\n\n        # how many mini-batches equal the entire dataset?\n        num_batches = int(np.ceil(np.divide(num_train, batch_size)))\n\n        # over the line\n        for k in range(max_its):                   \n            # loop over each minibatch\n            for b in range(num_batches):\n                # collect indices of current mini-batch\n                batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_train))\n\n                # plug in value into func and derivative\n                cost_eval,grad_eval = grad(w,x_train,y_train,lam,batch_inds)\n                grad_eval.shape = np.shape(w)\n\n                # update exponential average of past gradients\n                avg_sq_grad = gamma*avg_sq_grad + (1 - gamma)*grad_eval**2 \n\n                # take descent step \n                w = w - alpha*grad_eval / (avg_sq_grad**(0.5) + eps)\n\n            # update training and validation cost\n            train_cost = g_flat(w,x_train,y_train,lam,np.arange(num_train))\n\n            # record weight update, train and val costs\n            w_hist.append(unflatten(w))\n            train_hist.append(train_cost)\n\n        return w_hist,train_hist"""
mlrefined_libraries/superlearn_library/sparse_feature_selection_static.py,7,"b""import numpy as np\nimport matplotlib.pyplot as plt\n#.style.use('ggplot')\nimport matplotlib.patches as mpatches\nfrom matplotlib import gridspec\n\ndef plot_genes(X, gene_id_1, gene_id_2):\n    # create figure for plotting\n    fig = plt.figure(figsize = (5,5))\n    \n    # plot all\n    N = int(X.shape[0]/2)\n    plt.xlabel('gene #'+str(gene_id_1))\n    plt.ylabel('gene #'+str(gene_id_2))\n    red_patch = mpatches.Patch(color='red', label='healthy')\n    blue_patch = mpatches.Patch(color='blue', label='afflicted')\n    plt.legend(handles=[red_patch, blue_patch])\n    plt.legend(handles=[red_patch, blue_patch], loc = 2)\n    ax = plt.scatter(X[0:N,gene_id_1], X[0:N,gene_id_2], color='r', s=30) #plotting the data\n    plt.scatter(X[N+1:2*N,gene_id_1], X[N+1:2*N,gene_id_2], color='b', s=30)\n    plt.show()\n\n# compare multiple l1 regularized runs\ndef compare_lams(weights,lams,genes):       \n    # initialize figure\n    fig = plt.figure(figsize = (9,7))\n    artist = fig\n\n    # create subplot with 3 panels, plot input function in center plot\n    num_lams = len(lams)\n    gs = gridspec.GridSpec(num_lams,1) \n    for n in range(num_lams):\n        lam = lams[n]\n        ax = plt.subplot(gs[n]); \n        w = weights[n][1:]\n        \n        # plot weights for this run\n        plot_weights(ax,w,genes,lam)\n       # ax.set_ylim([-0.6,1.7])\n    plt.show()\n\ndef plot_weights(ax,w,genes,lam):\n    # mark all genes\n    plt.bar(np.arange(0,len(w)), w, color='k', alpha=0.2)\n\n    # highlight chosen genes\n    for gene in genes:\n        plt.bar([gene],[w[gene]], color='k', alpha=.7)\n    plt.axhline(c='k',zorder = 3)\n    \n    # dress panel\n    plt.xlabel('genes')\n    plt.ylabel('learned weights')\n    title = r'$\\lambda = ' + str(lam)\n    plt.title(title)\n\ndef compute_grad(X, y, w):\n    #produce gradient for each class weights\n    grad = 0\n    for p in range(0,len(y)):\n        x_p = X[:,p]\n        y_p = y[p]\n        grad+= -1/(1 + np.exp(y_p*np.dot(x_p.T,w)))*y_p*x_p\n    grad.shape = (len(grad),1)\n    return grad\n\n\ndef L1_logistic_regression(X, y, lam):    \n    X = X.T\n    \n    # initialize weights - we choose w = random for illustrative purposes\n    w = np.zeros((X.shape[0],1))\n    print (w.shape)\n   \n    # set maximum number of iterations and step length\n    alpha = 1\n    max_its = 2000\n   \n    # make list to record weights at each step of algorithm\n    w_history = np.zeros((len(w),max_its+1)) \n    w_history[:,0] = w.flatten()\n    print(w_history.shape)\n    # gradient descent loop\n    for k in range(1,max_its+1):  \n\n        # form gradient\n        grad = compute_grad(X,y,w)\n      \n        # take gradient descent step\n        w = w - alpha*grad\n        \n        # take a proximal step\n        w[1:] = proximal_step(w[1:], lam)\n\n        # save new weights\n        w_history[:,k] = w.flatten()\n   \n    # return weights from each step\n    return w_history[:,-1]\n\ndef proximal_step(w, lam):\n    return np.maximum(np.abs(w) - 2*lam,0)*np.sign(w)\n\n\ndef logistic_regression(X, y):    \n    X = X.T\n    \n    # initialize weights - we choose w = random for illustrative purposes\n    w = np.zeros((X.shape[0],1))\n   \n    # set maximum number of iterations and step length\n    alpha = 1\n    max_its = 2000\n   \n    # make list to record weights at each step of algorithm\n    w_history = np.zeros((len(w),max_its+1)) \n    w_history[:,0] = w.flatten()\n    # gradient descent loop\n    for k in range(1,max_its+1):  \n\n        # form gradient\n        grad = compute_grad(X,y,w)\n      \n        # take gradient descent step\n        w = w - alpha*grad\n\n        # save new weights\n        w_history[:,k] = w.flatten()\n   \n    # return weights from each step\n    return w_history[1:,-1]\n\n\n\n\n\n\n"""
mlrefined_libraries/superlearn_library/svm_margin_demo.py,36,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\nfrom . import optimizers\n\nclass Visualizer:\n    '''\n    Visualize classification on a 2-class dataset with N = 2\n    '''\n    #### initialize ####\n    def __init__(self,data):\n        # grab input\n        self.data = data\n        self.x = data[:,:-1]\n        self.y = data[:,-1]\n        \n        # colors for viewing classification data 'from above'\n        self.colors = ['cornflowerblue','salmon','lime','bisque','mediumaquamarine','b','m','g']\n\n    def center_data(self):\n        # center data\n        self.x = self.x - np.mean(self.x)\n        self.y = self.y - np.mean(self.y)\n        \n    # the counting cost function - for determining best weights from input weight history\n    def counting_cost(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = self.x[p]\n            y_p = self.y[p]\n            a_p = w[0] + sum([a*b for a,b in zip(w[1:],x_p)])\n            cost += (np.sign(a_p) - y_p)**2\n        return 0.25*cost\n    \n    # softmargin svm with softmax cost\n    def softmargin(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = self.x[p]\n            y_p = self.y[p]\n            a_p = w[0] + sum([a*b for a,b in zip(w[1:],x_p)])                \n            cost += np.log(1 + np.exp(-y_p*a_p))+ self.lam*np.dot(w[1:].T,w[1:])\n        return cost\n    \n    # margin-perceptron\n    def margin_perceptron(self,w):\n        cost = 0\n        for p in range(0,len(self.y)):\n            x_p = self.x[p]\n            y_p = self.y[p]\n            a_p = w[0] + sum([a*b for a,b in zip(w[1:],x_p)])\n            cost += np.maximum(0,1-y_p*a_p)\n        return cost\n    \n     ######## softmargin vs other method ########\n     # produce static image of gradient descent or newton's method run\n    def svm_comparison(self):\n        # declare an instance of our current our optimizers\n        opt = optimizers.MyOptimizers()\n        \n        ### run all algs ###\n        self.lam = 0\n        self.big_whist = []\n        for i in range(3):\n            # run newton's method\n            w_hist = opt.gradient_descent(g = self.margin_perceptron,w = np.random.randn(np.shape(self.x)[1]+1,1),max_its = 50,steplength_rule = 'diminishing')\n            \n            # find best weights\n            w = w_hist[-1]\n            \n            # store\n            self.big_whist.append(w)\n            \n        # run svm     \n        self.lam = 10**(-3)\n        \n        # run newton's method\n        w_hist = opt.newtons_method(g = self.softmargin,w = np.random.randn(np.shape(self.x)[1]+1,1),max_its = 10,epsilon = 10**-8)\n        w = w_hist[-1]\n        self.big_whist.append(w)\n    \n    # plot comparison figure\n    def svm_comparison_fig(self):      \n        #### left panel - multiple runs ####\n        fig, axs = plt.subplots(1, 2, figsize=(8,4))\n        gs = gridspec.GridSpec(1, 2, width_ratios = [1,1]) \n        ax1 = plt.subplot(gs[0],aspect = 'equal'); \n        ax2 = plt.subplot(gs[1],aspect = 'equal'); \n        \n        # plot points - first in 3d, then from above\n        self.separator_view(ax1)\n               \n        # plot separator\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.25\n        minx -= gapx\n        maxx += gapx\n        \n        s = np.linspace(minx,maxx,400)\n        for i in range(3):\n            w = self.big_whist[i]\n\n            # plot \n            t = - ((w[0])/float(w[2]) + w[1]/float(w[2])*s ) \n            ax1.plot(s,t,linewidth = 2,zorder = 1)\n        \n        #### right panel - svm runs ####\n        self.separator_view(ax2)\n        w = self.big_whist[-1]\n        \n        # create surface\n        r = np.linspace(minx,maxx,400)\n        x1_vals,x2_vals = np.meshgrid(r,r)\n        x1_vals.shape = (len(r)**2,1)\n        x2_vals.shape = (len(r)**2,1)\n        g_vals = np.tanh( w[0] + w[1]*x1_vals + w[2]*x2_vals )\n        g_vals = np.asarray(g_vals)\n\n        # vals for cost surface\n        x1_vals.shape = (len(r),len(r))\n        x2_vals.shape = (len(r),len(r))\n        g_vals.shape = (len(r),len(r))\n        \n        # plot color filled contour based on separator\n        g_vals = np.sign(g_vals) + 1\n        ax2.contourf(x1_vals,x2_vals,g_vals,colors = self.colors[:],alpha = 0.1,levels = range(0,2+1))\n    \n        # plot separator\n        s = np.linspace(minx,maxx,400)\n        t = - ((w[0])/float(w[2]) + w[1]/float(w[2])*s ) \n        ax2.plot(s,t,color = 'k',linewidth = 3,zorder = 1)\n        \n        ### determine margin ###\n        # determine margin\n        margin = self.proj_onto_line(w)\n        \n        # plot margin planes\n        s = np.linspace(minx,maxx,400)\n        t = - ((w[0])/float(w[2]) + w[1]/float(w[2])*s ) + margin\n        ax2.plot(s,t,color = 'k',linewidth = 1,zorder = 1)\n        \n        t = - ((w[0])/float(w[2]) + w[1]/float(w[2])*s ) - margin\n        ax2.plot(s,t,color = 'k',linewidth = 1,zorder = 1)\n            \n        plt.show()  \n    \n                    \n     ######## softmargin static figure ########\n    # produce static image of gradient descent or newton's method run\n    def softmargin_fig(self,w_hist,**kwargs):      \n        # determine best weights based on number of misclassifications\n        g_count = []\n        for j in range(len(w_hist)):\n            w = w_hist[j]\n            count = self.counting_cost(w)\n            g_count.append(count)\n        ind = np.argmin(g_count)\n        if np.size(ind) > 1:\n            w = w_hist[ind[-1]]    \n        else:\n            w = w_hist[ind]\n            \n        w = w_hist[-1]\n        \n        # optional arguments\n        cost_plot = 'off'\n        if 'cost_plot' in kwargs:\n            cost_plot = kwargs['cost_plot']  \n            \n        g = 0\n        if 'g' in kwargs:\n            g = kwargs['g']              \n            \n        ### plot all input data ###\n        # generate input range for functions\n        minx = min(min(self.x[:,0]),min(self.x[:,1]))\n        maxx = max(max(self.x[:,0]),max(self.x[:,1]))\n        gapx = (maxx - minx)*0.25\n        minx -= gapx\n        maxx += gapx\n\n        r = np.linspace(minx,maxx,400)\n        x1_vals,x2_vals = np.meshgrid(r,r)\n        x1_vals.shape = (len(r)**2,1)\n        x2_vals.shape = (len(r)**2,1)\n        g_vals = np.tanh( w[0] + w[1]*x1_vals + w[2]*x2_vals )\n        g_vals = np.asarray(g_vals)\n\n        # vals for cost surface\n        x1_vals.shape = (len(r),len(r))\n        x2_vals.shape = (len(r),len(r))\n        g_vals.shape = (len(r),len(r))\n        \n        # create figure to plot\n        num_panels = 1\n        widths = [1]\n        if cost_plot == 'on':\n            num_panels = 2\n            widths = [2,1]\n        fig, axs = plt.subplots(1, num_panels, figsize=(8,4))\n        gs = gridspec.GridSpec(1, num_panels, width_ratios=widths) \n        ax1 = plt.subplot(gs[0],aspect = 'equal'); \n        if cost_plot == 'on':\n            ax2 = plt.subplot(gs[1]); \n            \n        # plot points - first in 3d, then from above\n        self.separator_view(ax1)\n               \n        # plot color filled contour based on separator\n        g_vals = np.sign(g_vals) + 1\n        ax1.contourf(x1_vals,x2_vals,g_vals,colors = self.colors[:],alpha = 0.1,levels = range(0,2+1))\n    \n        # plot separator\n        s = np.linspace(minx,maxx,400)\n        t = - ((w[0])/float(w[2]) + w[1]/float(w[2])*s ) \n        ax1.plot(s,t,color = 'k',linewidth = 3,zorder = 1)\n        \n        ### determine margin ###\n        # determine margin\n        margin = self.proj_onto_line(w)\n        \n        # plot margin planes\n        s = np.linspace(minx,maxx,400)\n        t = - ((w[0])/float(w[2]) + w[1]/float(w[2])*s ) + margin\n        ax1.plot(s,t,color = 'k',linewidth = 1,zorder = 1)\n        \n        t = - ((w[0])/float(w[2]) + w[1]/float(w[2])*s ) - margin\n        ax1.plot(s,t,color = 'k',linewidth = 1,zorder = 1)\n \n        # plot cost function value\n        if cost_plot == 'on':\n            # plot cost function history\n            g_hist = []\n            for j in range(len(w_hist)):\n                w = w_hist[j]\n                w = np.asarray(w)\n                w.shape = (len(w),1)\n                g_eval = g(w)\n                g_hist.append(g_eval)\n                \n            g_hist = np.asarray(g_hist).flatten()\n            \n            # plot cost function history\n            ax2.plot(np.arange(len(g_hist)),g_hist,linewidth = 2)\n            ax2.set_xlabel('iteration',fontsize = 13)\n            ax2.set_title('cost value',fontsize = 12)\n            \n        plt.show()\n \n    # project onto line\n    def proj_onto_line(self,w):\n        w_c = copy.deepcopy(w)\n        w_0 = -w_c[0]/w_c[2]  # amount to subtract from the vertical of each point\n        \n        # setup line to project onto\n        w_1 = -w_c[1]/w_c[2]\n        line_pt = np.asarray([1,w_1])\n        line_pt.shape = (2,1)\n        line_hat = line_pt / np.linalg.norm(line_pt)\n        line_hat.shape = (2,1)\n\n        # loop over points, compute distance of projections                     \n        dists = []\n        for j in range(len(self.y)):\n            pt = copy.deepcopy(self.x[j])\n            pt[1]-= w_0\n            pt.shape = (2,1)\n            proj = np.dot(line_hat.T,pt)*line_hat  \n            proj.shape = (2,1)\n            d = np.linalg.norm(proj - pt)\n            dists.append(d)                  \n        \n        # find smallest distance to class point\n        ind = np.argmin(dists)\n        pt_min = copy.deepcopy(self.x[ind])\n        \n        # create new intercept coeff\n        pt_min[1] -= w_0\n        w_new = -w_1*pt_min[0] + pt_min[1] \n\n        return w_new\n        \n    # plot data 'from above' in seperator view\n    def separator_view(self,ax):\n        # set plotting limits\n        xmax1 = copy.deepcopy(max(self.x[:,0]))\n        xmin1 = copy.deepcopy(min(self.x[:,0]))\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n            \n        xmax2 = copy.deepcopy(max(self.x[:,0]))\n        xmin2 = copy.deepcopy(min(self.x[:,0]))\n        xgap2 = (xmax2 - xmin2)*0.05\n        xmin2 -= xgap2\n        xmax2 += xgap2\n            \n        ymax = max(self.y)\n        ymin = min(self.y)\n        ygap = (ymax - ymin)*0.2\n        ymin -= ygap\n        ymax += ygap    \n\n        # scatter points\n        classes = np.unique(self.y)\n        count = 0\n        for num in classes:\n            inds = np.argwhere(self.y == num)\n            inds = [s[0] for s in inds]\n            ax.scatter(self.data[inds,0],self.data[inds,1],color = self.colors[int(count)],linewidth = 1,marker = 'o',edgecolor = 'k',s = 50)\n            count+=1\n            \n        # clean up panel\n        ax.set_xlim([round(xmin1)-1,round(xmax1)+1])\n        ax.set_ylim([round(xmin2)-1,round(xmax2)+1])\n\n        ax.set_xticks(np.arange(round(xmin1)-1, round(xmax1) + 2, 1.0))\n        ax.set_yticks(np.arange(round(xmin2)-1, round(xmax2) + 2, 1.0))\n\n        # label axes\n        ax.set_xlabel(r'$x_1$', fontsize = 12,labelpad = 0)\n        ax.set_ylabel(r'$x_2$', rotation = 0,fontsize = 12,labelpad = 5)\n            """
mlrefined_libraries/superlearn_library/weighted_classification_animator.py,17,"b""import matplotlib.pyplot as plt\nimport autograd.numpy as np                 # Thinly-wrapped numpy\n\n# import custom JS animator\nimport sys\nfrom IPython.display import clear_output\n\n# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\n# import classification bits\nfrom . import classification_bits as bits\nimport time\n    \nclass Visualizer:\n    def load_data(self,csvname):\n        data = np.loadtxt(csvname,delimiter = ',')\n        self.data = data\n\n        x = data[0:2,:]\n        y = data[-1,:][np.newaxis,:]\n        \n        special_class = +1\n        return x,y,special_class\n\n    def animate_weightings(self,savepath,csvname,**kwargs):\n        self.x,self.y,special_class = self.load_data(csvname)\n        self.color_opts = np.array([[1,0,0.4], [ 0, 0.4, 1],[0, 1, 0.5],[1, 0.7, 0.5],[0.7, 0.6, 0.5]])\n\n        # pick out user-defined arguments\n        num_slides = 2\n        if 'num_slides' in kwargs:\n            num_slides = kwargs['num_slides']\n\n        # make range for plot\n        base_size = 100\n        size_range = np.linspace(base_size, 20*base_size, num_slides)\n        weight_range = np.linspace(1,10,num_slides)\n        \n        # generate figure to plot onto\n        fig = plt.figure(figsize=(5,5))\n        artist = fig\n        ax = plt.subplot(111)\n        \n        # animation sub-function\n        ind1 = np.argwhere(self.y == special_class)\n        ind1 = [v[1] for v in ind1]\n        \n        # run animator\n        max_its = 5\n        w = 0.1*np.random.randn(3,1)\n        g = bits.softmax\n        def animate(k):\n            ax.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_slides))\n            if k == num_slides - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            # define beta\n            special_size = size_range[k]\n            special_weight = weight_range[k]\n            beta = np.ones((1,self.y.size))\n            beta[:,ind1] = special_weight\n            \n            # run optimizer\n            w_hist,g_hist = bits.newtons_method(g,w,self.x,self.y,beta,max_its)\n\n            # determine minimum classification weightings\n\n\n            w_best = w_hist[-1]\n            self.model = lambda data: bits.model(data,w_best)\n            \n            # scatter plot all data\n            self.plot_data(ax,special_class,special_size)\n            \n            # draw decision boundary\n            self.draw_decision_boundary(ax)\n            return artist,\n        \n        anim = animation.FuncAnimation(fig, animate ,frames=num_slides, interval=num_slides, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()\n                \n    def plot_data(self,ax,special_class,special_size):\n        # scatter points in both panels\n        class_nums = np.unique(self.y)\n        C = len(class_nums)\n        z = 3\n        for c in range(C):\n            ind = np.argwhere(self.y == class_nums[c])\n            ind = [v[1] for v in ind]\n            s = 80\n            if class_nums[c] == special_class:\n                s = special_size\n                z = 0\n            ax.scatter(self.x[0,ind],self.x[1,ind],s = s,color = self.color_opts[c],edgecolor = 'k',linewidth = 1.5,zorder = z)\n            \n        # control viewing limits\n        minx = min(self.x[0,:])\n        maxx = max(self.x[0,:])\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n        \n        miny = min(self.x[1,:])\n        maxy = max(self.x[1,:])\n        gapy = (maxy - miny)*0.1\n        miny -= gapy\n        maxy += gapy\n        \n        ax.set_xlim([minx,maxx])\n        ax.set_ylim([miny,maxy])\n        #ax.axis('equal')\n        ax.axis('off')\n\n    # toy plot\n    def draw_decision_boundary(self,ax,**kwargs):  \n        # control viewing limits\n        minx = min(self.x[0,:])\n        maxx = max(self.x[0,:])\n        gapx = (maxx - minx)*0.1\n        minx -= gapx\n        maxx += gapx\n        \n        miny = min(self.x[1,:])\n        maxy = max(self.x[1,:])\n        gapy = (maxy - miny)*0.1\n        miny -= gapy\n        maxy += gapy\n\n        r = np.linspace(minx,maxx,200)\n        s = np.linspace(miny,maxy,200)\n        w1_vals,w2_vals = np.meshgrid(r,s)\n        w1_vals.shape = (len(r)**2,1)\n        w2_vals.shape = (len(s)**2,1)\n        h = np.concatenate([w1_vals,w2_vals],axis = 1)\n        g_vals = self.model(h.T)\n        g_vals = np.asarray(g_vals)\n\n        # vals for cost surface\n        w1_vals.shape = (len(r),len(s))\n        w2_vals.shape = (len(r),len(s))\n        g_vals.shape = (len(r),len(s))\n        \n        # plot separator curve in right plot\n        ax.contour(w1_vals,w2_vals,g_vals,colors = 'k',levels = [0],linewidths = 3,zorder = 1)\n            \n        # plot color filled contour based on separator\n        g_vals = np.sign(g_vals) + 1\n        ax.contourf(w1_vals,w2_vals,g_vals,colors = self.color_opts[:],alpha = 0.1,levels = range(0,2+1))"""
mlrefined_libraries/superlearn_library/weighted_classification_example.py,14,"b'# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nclass Visualizer:\n    def load_data(self,csvname):\n        # Read census data\n        census_data = pd.read_csv(csvname, \n                      names = [""age"",""workclass"",""education_level"",""education-num"",""marital-status"",""occupation"",""relationship"",""race"",""sex"",""capital-gain"",""capital-loss"",""hours-per-week"",""native-country"",""income""])\n\n        # Extract feature columns\n        feature_cols = list(census_data.columns[:-1])\n        \n        # Extract target column \'income\'\n        target_col = census_data.columns[-1] \n\n        # Separate the data into feature data and target data (X_all and y_all, respectively)\n        X_all = census_data[feature_cols]\n        y_all = census_data[target_col]\n\n        # update data with log of capital-gain and capital-loss values\n        X_all = np.log(X_all[\'capital-gain\'] + 1)\n        \n        # convert labels to numerical value\n        y_all = np.asarray(y_all)\n        y_all.shape = (len(y_all),1)\n        X_all = np.asarray(X_all)\n        ind1 = np.argwhere(y_all == ""<=50K"")\n        ind1 = [s[0] for s in ind1]\n        ind2 = np.argwhere(y_all == "">50K"")\n        ind2 = [s[0] for s in ind2]\n        y_all[ind1] = -1\n        y_all[ind2] = +1\n        y_all = np.asarray([s[0] for s in y_all])\n\n        # keep only the portion of data where capital gain > 0 \n        ind = np.argwhere(X_all > 0)\n        ind = [s[0] for s in ind]\n        y = y_all[ind]\n        x = X_all[ind]\n        x = np.asarray(x, dtype=np.float)    \n\n        return x, y\n\n    # quantizes x using values in the bin_centers\n    def quantize(self,x):\n        # specify bin centers\n        self.bin_centers = np.linspace(4.5, 11.5, 15)\n        x_q = x\n        for i in range(0,len(x)):\n            dist = np.abs(self.bin_centers-x[i])\n            x_q[i] = self.bin_centers[np.argmin(dist)]   \n        return x_q\n\n    def my_scatter(self,x, y, ax, c):\n        # count number of occurances for each element\n        s = np.asarray([sum(x==i) for i in x])\n        # plot data using s as size vector\n        ax.scatter(x, y, s, color=c) \n \n    def plot(self,csvname):\n        x, y = self.load_data(csvname)\n\n        # quantize x\n        x_quantized = self.quantize(x)\n        \n        # seprate positive class and negative class for plotting \n        x_pos = x_quantized[y>0]\n        x_neg = x_quantized[y<0]\n\n        # plot data\n        fig = plt.figure(figsize=(9,5))\n        ax = fig.gca()\n\n        # use my_scatter to plot each class\n        self.my_scatter(x_pos, np.ones(len(x_pos)),ax, c=\'r\')\n        self.my_scatter(x_neg, -np.ones(len(x_neg)),ax, c=\'b\')\n\n        # clean up\n        ax.set_xticks(self.bin_centers)\n        ax.set_xlabel(\'log capital gain\')\n        ax.set_yticks([-1,1])\n        ax.set_ylabel(\'class (make > $50k)\')\n        ax.set_ylim([-2.5,2.5])\n        plt.grid(color=\'gray\', linestyle=\'-\', linewidth=1, alpha=.15)\n        plt.show()'"
mlrefined_libraries/superlearn_library/weighted_regression_animator.py,14,"b""# import custom JS animator\nimport sys\n\n# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\n\n# basic data manipulation libraries\nimport numpy as np\nimport pandas as pd\n\n# class for illustrating regression weighting\nclass Visualizer:\n    ##### a simple data loading function #####\n    def load(self,csvname,**args):\n        sep = ','\n        data = np.loadtxt(csvname,delimiter = ',')\n        self.x = data[:,0]\n        self.y = data[:,1]\n        \n    # generate a dataset if desired\n    def generate_data(self,num_pts,csvname):\n        # load in points (or make them)\n        x = np.random.rand(num_pts)\n        x = x\n        y = x + 0.25*np.random.randn(num_pts)\n        y = y\n        \n        # save \n        x.shape = (len(x),1)\n        y.shape = (len(y),1)\n        data = np.concatenate((x,y),axis = 1)\n        np.savetxt(csvname,data)\n        \n        # put to global\n        x = x.tolist()\n        x = [v[0] for v in x]\n\n        y = y.tolist()\n        y = [v[0] for v in y]\n\n        self.x = x\n        self.y = y\n\n    # weighted linear regression solver\n    def weighted_linear_regression(self,inputs,outputs,special_ind,special_weight):        \n        # setup linear system \n        A = 0\n        b = 0\n        for p in range(len(outputs)):\n            # get pth point\n            x_p = inputs[p]\n            x_p = np.asarray([1,x_p])\n            x_p.shape = (len(x_p),1)\n            y_p = np.asarray(outputs[p])\n            \n            # make pth outer product matrix and solution vector\n            lef = np.outer(x_p,x_p.T)\n            rig = y_p*x_p\n            \n            # weight if special ind\n            if p == special_ind:\n                lef = lef*special_weight\n                rig = rig*special_weight\n                \n            # add to totals\n            A += lef\n            b += rig\n         \n        # solve linear system\n        w = np.linalg.solve(A,b)\n        \n        return w\n        \n    # animate regression weighting\n    def animate_weighting(self,savepath,csvname,**kwargs):\n        data = np.loadtxt(csvname,delimiter = ',')\n        x = data[:,0]\n        y = data[:,1]\n        \n        # pick out user-defined arguments\n        num_slides = kwargs['num_slides']\n        special_ind = kwargs['special_ind']\n\n        # make range for plot\n        base_size = 100\n        size_range = np.linspace(base_size, 20*base_size, num_slides)\n        weight_range = np.linspace(1,100,num_slides)\n        \n        # generate figure to plot onto\n        fig = plt.figure(figsize=(6,6))\n        artist = fig\n        ax = plt.subplot(111)\n        \n        # animation sub-function\n        weights = np.ones((len(x),1))\n        weights = weights.tolist()\n        weights = [s[0] for s in weights]\n        def animate(k):\n            ax.cla()\n            special_size = size_range[k]\n            special_weight = weight_range[k]\n \n            # scatter plot all data\n            ax.scatter(x,y,s = base_size, c = 'k', edgecolor = 'w',zorder = 0)\n\n            # scatter plot weighted point\n            ax.scatter(x[special_ind],y[special_ind],s = special_size,c = 'r',edgecolor = 'w',zorder = 2)\n            ax.axis('off')\n            \n            # compute regression line given weighted data\n            w = self.weighted_linear_regression(inputs = x,outputs = y,special_ind = special_ind, special_weight = special_weight)\n\n            # plot regression line ontop of data\n            s = np.linspace(0,1,100)\n            t = w[0] + w[1]*s\n            ax.plot(s,t,color = 'b',linewidth = 5,zorder = 1)\n            ax.axhline(0.25,c='k',zorder = 1)\n            ax.axvline(0,c='k',zorder = 1)\n            \n            \n            return artist,\n        \n        anim = animation.FuncAnimation(fig, animate ,frames=num_slides, interval=num_slides, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()"""
mlrefined_libraries/superlearn_library/weighted_regression_example.py,4,"b""# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n'''\ncustom scatter plot if there's repreated data, the dots \nrepresenting more frequent points will be larger.\n'''\nclass Visualizer:    \n    def my_scatter(self,x, y, c):\n        # count number of occurances for each element\n        s = np.asarray([sum(x==i) for i in x])\n        # plot data using s as size vector\n        plt.scatter(x, y, s**2, color=c)    \n    \n    def plot_it(self,csvname):\n        # read data\n        data = pd.read_csv(csvname, index_col=0)\n        n_row, n_col = np.shape(data)\n        \n        # plot data\n        fig = plt.figure(figsize=(12,5))\n        ax = fig.gca()\n        colors=['r', 'b', 'g', 'y', 'm']\n\n        # use my_scatter to plot each column of the dataframe\n        for i in range(0,n_col):\n            self.my_scatter(data[data.columns[i]], float(data.columns[i])*np.ones(n_row), c=colors[i])\n\n        # clean up\n        ax.set_xticks(np.arange(3.4, 7.5, .2))\n        ax.set_xlabel('time')\n        ax.set_yticks([.25,.50,.67,.75,1.0])\n        ax.set_ylabel('portion of ramp traveled')\n        ax.set_ylim([.15,1.1])\n        plt.grid(color='gray', linestyle='-', linewidth=1, alpha=.15)\n        plt.show()"""
mlrefined_libraries/unsupervised_library/K_means_demos.py,56,"b""import numpy as np\nfrom matplotlib import gridspec\nfrom IPython.display import display, HTML\nimport copy\nimport math\nimport time\n\n# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nfrom matplotlib import gridspec\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n####### K-means functionality #######\n# function for updating cluster assignments\ndef update_assignments(data,centroids):\n    P = np.shape(data)[1]\n    assignments = []\n    for p in range(P):\n        # get pth point\n        x_p = data[:,p][:,np.newaxis]\n        \n        # compute distance between pth point and all centroids\n        # using numpy broadcasting\n        diffs = np.sum((x_p - centroids)**2,axis = 0)\n        \n        # determine closest centroid\n        ind = np.argmin(diffs)\n        assignments.append(ind)\n    return np.array(assignments)\n       \n# update centroid locations\ndef update_centroids(data,old_centroids,assignments):\n    K = old_centroids.shape[1]\n    # new centroid container\n    centroids = []\n    for k in range(K):\n        # collect indices of points belonging to kth cluster\n        S_k = np.argwhere(assignments == k)\n        \n        # take average of points belonging to this cluster\n        c_k = 0\n        if np.size(S_k) > 0:\n            c_k = np.mean(data[:,S_k],axis = 1)\n        else:  # empty cluster\n            c_k = copy.deepcopy(old_centroids[:,k])[:,np.newaxis]\n        centroids.append(c_k)\n    centroids = np.array(centroids)[:,:,0]\n    return centroids.T\n\n# main k-means function\ndef my_kmeans(data,centroids,max_its):\n    # collect all assignment and centroid updates - containers below\n    all_assignments = []\n    all_centroids = [centroids]\n    \n    # outer loop - alternate between updating assignments / centroids\n    for j in range(max_its):\n        # update cluter assignments\n        assignments = update_assignments(data,centroids)\n        \n        # update centroid locations\n        centroids = update_centroids(data,centroids,assignments)\n        \n        # store all assignments and centroids\n        all_assignments.append(assignments)\n        all_centroids.append(centroids)\n        \n    # final assignment update\n    assignments = update_assignments(data,centroids)\n    all_assignments.append(assignments)\n\n    return all_centroids,all_assignments\n\n####### K-means demo #######\ndef run_animated_demo(savepath,data,centroids,max_its,**kwargs):\n    # run K-means algo\n    all_centroids,all_assignments = my_kmeans(data,centroids,max_its-1)\n\n    P = np.shape(data)[1]\n    K = centroids.shape[1]\n    \n    # with all centroid and assignments in hand we can go forth and animate the process\n    colors =  [[1,0,0.4], [ 0, 0.4, 1],[0, 1, 0.5],[1, 0.7, 0.5],[0.75, 0.75, 0.75],'mediumaquamarine']\n    \n    # determine viewing range for plot\n    pt_xmin = np.min(data[0,:])\n    cent_xmin = np.min([c[0,:] for c in all_centroids])\n    xmin = np.min([pt_xmin,cent_xmin])\n    \n    pt_xmax = np.max(data[0,:])\n    cent_xmax = np.max([c[0,:] for c in all_centroids])\n    xmax = np.max([pt_xmax,cent_xmax])    \n    \n    xgap = (xmax - xmin)*0.2\n    xmin -= xgap\n    xmax += xgap\n    \n    pt_ymin = np.min(data[1,:])\n    cent_ymin = np.min([c[1,:] for c in all_centroids])\n    ymin = np.min([pt_ymin,cent_ymin])\n    \n    pt_ymax = np.max(data[1,:])\n    cent_ymax = np.max([c[1,:] for c in all_centroids])\n    ymax = np.max([pt_ymax,cent_ymax])    \n    \n    ygap = (ymax - ymin)*0.2\n    ymin -= ygap\n    ymax += ygap\n    \n    # initialize figure\n    fig = plt.figure(figsize = (5,5))\n    artist = fig\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 1) \n    ax = plt.subplot(gs[0],aspect = 'equal'); \n\n    # start animation\n    num_frames = 4*len(all_centroids)\n    print ('starting animation rendering...')\n    def animate(j):\n        # clear panel\n        ax.cla()\n        \n        # print rendering update\n        if j == num_frames - 2:\n            print ('rendering animation frame ' + str(j+1) + ' of ' + str(num_frames))\n        if j == num_frames - 1:\n            print ('animation rendering complete!')\n            time.sleep(1.5)\n            clear_output()\n        \n        #### plot first frame - just data #####\n        # gather current centroids and assignments \n        c = int(np.floor(np.divide(j,4)))\n        centroids = all_centroids[c]\n        assignments = all_assignments[c] \n\n        # draw uncolord points\n        ax.scatter(data[0,:],data[1,:],c = 'k',s = 100,edgecolor = 'w',linewidth = 1,zorder = 1) \n        title = 'iteration ' + str(c + 1)\n        ax.set_title(title,fontsize = 17)\n                \n        # plot the centroids \n        if np.mod(j,4) < 3 or j == num_frames - 1:\n            for k in range(K):\n                ax.scatter(centroids[0,k],centroids[1,k],c = colors[k],s = 400,edgecolor ='k',linewidth = 2,marker=(5, 1),zorder = 3)\n        else:\n            for k in range(K):\n                ax.scatter(centroids[0,k],centroids[1,k],c = colors[k],s = 400,edgecolor ='k',linewidth = 2,marker=(5, 1),zorder = 2,alpha = 0.35)\n        \n        # plot guides to updated centroids\n        if np.mod(j,4) == 3 and j < num_frames - 4:\n            next_centroids = all_centroids[c+1]\n            \n            # draw visual guides\n            for k in range(K):\n                ind = np.argwhere(assignments == k)\n                if np.size(ind) > 0:\n                    ind = [s[0] for s in ind]\n                    centroid = next_centroids[:,k]\n                    \n                    # plot new centroid\n                    ax.scatter(centroid[0],centroid[1],c = colors[k],s = 400,edgecolor ='k',linewidth = 2,marker=(5, 1),zorder = 3)\n\n                    # connect point to cluster centroid via dashed guide line\n                    for i in ind:\n                        pt = data[:,i]\n                        ax.plot([pt[0],centroid[0]],[pt[1],centroid[1]],color = colors[k],linestyle = '--',zorder = 0,linewidth = 1)\n                                  \n        # draw points and visual guides between points and their assigned cluster centroids\n        if np.mod(j,4) == 1: \n            # draw visual guides\n            for k in range(K):\n                ind = np.argwhere(assignments == k)\n                if np.size(ind) > 0:\n                    ind = [s[0] for s in ind]\n                    centroid = centroids[:,k]\n                    \n                    # connect point to cluster centroid via dashed guide line\n                    for i in ind:\n                        pt = data[:,i]\n                        ax.plot([pt[0],centroid[0]],[pt[1],centroid[1]],color = colors[k],linestyle = '--',zorder = 0,linewidth = 1)\n                        \n        # scatter plot each cluster of points\n        if np.mod(j,4) == 2 or np.mod(j,4) == 3:\n            # plot the point assignments \n            for k in range(K):\n                ind = np.argwhere(assignments == k)\n                if np.size(ind) > 0:\n                    ind = [s[0] for s in ind]    \n                    ax.scatter(data[0,ind],data[1,ind],color = colors[k],s = 100,edgecolor = 'k',linewidth = 1,zorder = 2) \n                        \n        # set viewing range\n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n        \n        return artist,\n\n    anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n\n    # produce animation and save\n    fps = 50\n    if 'fps' in kwargs:\n        fps = kwargs['fps']\n    anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n    clear_output()\n\n# computer for the average error\ndef compuate_ave(data,centroids,assignments):\n    P = len(assignments)\n    K = np.shape(centroids)[1]\n    error = 0\n    for k in range(K):\n        centroid = centroids[:,k]\n        ind = np.argwhere(assignments == k)\n        if np.size(ind) > 0:\n            ind = [s[0] for s in ind]    \n            for i in ind:\n                pt = data[:,i]\n                error += np.linalg.norm(centroid - pt)\n    # divide by the average\n    error /= float(P)\n    return error\n\n##### static image generator #####\ndef compare_runs(data,starter_centroids,max_its):\n    # constants for run\n    P = np.shape(data)[1]\n    K = starter_centroids[0].shape[1]\n    num_runs = len(starter_centroids)\n        \n    # with all centroid and assignments in hand we can go forth and animate the process\n    colors =  [[1,0,0.4], [ 0, 0.4, 1],[0, 1, 0.5],[1, 0.7, 0.5],[0.75, 0.75, 0.75],'mediumaquamarine']\n    \n    # initialize figure\n    fig = plt.figure(figsize = (9,5))\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, num_runs) \n    \n    # loop over initial centroids and make a run\n    num = 0\n    for centroids in starter_centroids:\n        # run K-means algo\n        all_centroids,all_assignments = my_kmeans(data,centroids,max_its-1)\n        final_centroids = all_centroids[-1]\n        final_assignments = all_assignments[-1]\n        \n        # generate panel\n        ax = plt.subplot(gs[num],aspect = 'equal'); \n\n        # determine viewing range for plot\n        pt_xmin = np.min(data[0,:])\n        cent_xmin = np.min(final_centroids[0,:])\n        xmin = np.min([pt_xmin,cent_xmin])\n\n        pt_xmax = np.max(data[0,:])\n        cent_xmax = np.max(final_centroids[0,:])\n        xmax = np.max([pt_xmax,cent_xmax])    \n\n        xgap = (xmax - xmin)*0.2\n        xmin -= xgap\n        xmax += xgap\n\n        pt_ymin = np.min(data[1,:])\n        cent_ymin = np.min(final_centroids[1,:])\n        ymin = np.min([pt_ymin,cent_ymin])\n\n        pt_ymax = np.max(data[1,:])\n        cent_ymax = np.max(final_centroids[1,:])\n        ymax = np.max([pt_ymax,cent_ymax])    \n\n        ygap = (ymax - ymin)*0.2\n        ymin -= ygap\n        ymax += ygap\n        \n        # set viewing range\n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n        \n        # plot final clustered data \n        for k in range(K):\n            ind = np.argwhere(final_assignments == k)\n            if np.size(ind) > 0:\n                ind = [s[0] for s in ind]    \n                ax.scatter(data[0,ind],data[1,ind],color = colors[k],s = 100,edgecolor = 'k',linewidth = 1,zorder = 2) \n        \n        # plot cluster centroids\n        for k in range(K):\n            ax.scatter(final_centroids[0,k],final_centroids[1,k],c = colors[k],s = 400,edgecolor ='k',linewidth = 2,marker=(5, 1),zorder = 3)\n            \n        # compute average error over dataset\n        error = compuate_ave(data,final_centroids,final_assignments)\n                               \n        # make title\n        title = 'average dist = ' + str(round(error,1))\n        ax.set_title(title,fontsize = 17)\n        num += 1\n        \ndef scree_plot(data,K_range,max_its):\n    # with all centroid and assignments in hand we can go forth and animate the process\n    colors =  [[1,0,0.4], [ 0, 0.4, 1],[0, 1, 0.5],[1, 0.7, 0.5],[0.75, 0.75, 0.75],'mediumaquamarine']\n    \n    # initialize figure\n    fig = plt.figure(figsize = (8,3))\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1,1)\n    ax = plt.subplot(gs[0]); \n    \n    ### outer loop - run K-means for each k ###\n    K_errors = []\n    for k in K_range:\n        errors = []\n        for j in range(5):\n            # initialize\n            P = np.shape(data)[1]\n            random_inds = np.random.permutation(P)[:k]\n            init_centroids = data[:,random_inds]\n\n            # run K-means algo\n            all_centroids,all_assignments = my_kmeans(data,init_centroids,max_its-1)\n            centroids = all_centroids[-1]\n            assignments = all_assignments[-1]\n\n            # compute average error over dataset\n            error = compuate_ave(data,centroids,assignments)\n            errors.append(error)\n            \n        # take final error\n        best_ind = np.argmin(errors)\n        K_errors.append(errors[best_ind])\n    \n    # plot cost function value for each K chosen    \n    ax.plot(K_range,K_errors,'ko-')\n    \n    # dress up panel\n    ax.set_xlabel('number of clusters')\n    ax.set_ylabel('objective value')\n    ax.set_xticks(K_range)"""
mlrefined_libraries/unsupervised_library/K_means_methods.py,32,"b""import random\nimport numpy as np\nimport time\nimport matplotlib.pylab as plt\nfrom IPython import display\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndef K_means_demo(X, C, mode):\n    \n    t = 1   # While-loop counter\n    d = [1] # Container for centroid movements    \n    eps = 1e-3  # Threshold for stopping the algorithm\n    P = X.shape[1]\n    K = np.shape(C)[1]\n    clrs = ['r', 'b', 'g', 'm', 'y'] # Colors\n       \n    if mode == 'just_run_the_alg':\n        \n        while d[-1] > eps:\n            \n            #Cluster assignment\n            cluster_assignments = [] #This list will contain the cluster assignments\n            for p in np.arange(0, P):\n                diff = []\n                for k in np.arange(0, K):\n                    diff.append(np.linalg.norm(X[:, p] - C[2*(t-1):2*t, k]))    \n                cluster_assignments.append(diff.index(min(diff)))\n\n            #Centroid update\n            AVG = np.empty([2,0]) #This array will contain the centroid locations       \n            for k in  np.arange(0, K):\n                ind = [i for i, y in enumerate(cluster_assignments) if y == k] \n                AVG = np.concatenate((AVG, X[:,ind].mean(axis=1).reshape([2,1])), axis=1)   \n            C = np.concatenate((C, AVG), axis=0)\n            d.append(np.linalg.norm(C[2*(t-1):2*t,:] - C[2*t:2*(t+1),:]))\n            t = t+1\n    \n    elif mode == 'plot_the_steps':\n        \n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        plt.axis([-.1, 1.1, -.1, 1.1])\n        plt.scatter(X[0,:], X[1,:], color='k')\n        plt.axis('off')\n        time.sleep(1)\n\n        while d[-1] > eps:\n\n            #Cluster assignment\n            cluster_assignments = [] #This list will contain the cluster assignments\n            for p in np.arange(0, P):\n                diff = []\n                for k in np.arange(0, K):\n                    diff.append(np.linalg.norm(X[:, p] - C[2*(t-1):2*t, k]))    \n                cluster_assignments.append(diff.index(min(diff)))\n\n            #Plotting the centroids \n            for k in np.arange(0, K):\n                plt.scatter(C[2*(t-1), k], C[2*t-1, k], s=120, color=clrs[k], marker=(5, 2))\n                display.display(plt.gcf())\n                display.clear_output(wait=True)  \n            time.sleep(1)\n\n            #Centroid update\n            AVG = np.empty([2,0]) #This array will contain the centroid locations       \n            for k in  np.arange(0, K):\n                ind = [i for i, y in enumerate(cluster_assignments) if y == k]\n                plt.scatter(X[0,ind], X[1,ind], color=clrs[k])\n                display.display(plt.gcf())\n                display.clear_output(wait=True)  \n                AVG = np.concatenate((AVG, X[:,ind].mean(axis=1).reshape([2,1])), axis=1)   \n            C = np.concatenate((C, AVG), axis=0)\n            d.append(np.linalg.norm(C[2*(t-1):2*t,:] - C[2*t:2*(t+1),:]))\n            time.sleep(1)\n\n            for k in np.arange(0, K):\n                fig = plt.plot([C[2*(t-1),k], C[2*t,k]], [C[2*t-1,k], C[2*t+1,k]], '--', color=clrs[k])\n                display.display(plt.gcf())\n                display.clear_output(wait=True) \n                \n            t = t+1\n     \n        #plt.figure() # Plotting the clustered data\n        #for k in np.arange(0, K):\n            #ind = [i for i, x in enumerate(cluster_assignments) if x == k]\n            #plt.scatter(X[0,ind], X[1,ind], s=30, color=clrs[k])\n            #plt.axis([-.1, 1.1, -.1, 1.1])\n            #plt.axis('off')\n\n\n    return cluster_assignments, calc_obj_val(X, C[-2:,:], cluster_assignments)\n\n\ndef calc_obj_val(X, C, cluster_assignments):\n    W = np.zeros((C.shape[1], X.shape[1]))\n    for i, cluster in enumerate(cluster_assignments):\n        W[cluster,i] = 1\n    obj_val = np.linalg.norm(X - np.dot(C,W), 'fro')\n    return obj_val\n\n\ndef scree_plot(X):\n  \n    num_clusters = 10\n    num_runs = 20\n    Results = float('Inf')*np.ones((num_runs, num_clusters))\n\n    for i in np.arange(0,num_runs):\n        for k in np.arange(1,num_clusters+1):\n            foo, obj_val = K_means_demo(X, X[:,random.sample(set(np.arange(0,X.shape[1])),k)], mode='just_run_the_alg') \n            if np.isnan(obj_val) == False:\n                Results[i,k-1] = obj_val\n\n    obj_val = Results.min(axis=0)            \n   \n    plt.figure()\n    plt.style.use('ggplot')\n    plt.xlabel('number of clusters')\n    plt.ylabel('objective value')\n    plt.axis([1-.2, num_clusters+.2, min(obj_val)-.2, max(obj_val)+.2])\n    plt.xticks(np.arange(1, num_clusters+1, 1))\n    foo = plt.plot(np.arange(1,num_clusters+1), obj_val,'ko-') \n    return\n\ndef normalize_blobs(blobs):\n    X = np.transpose(blobs[0])\n    X = (X-X.min())/(X.max()-X.min())\n    return X\n\ndef plot_data(X,C):\n    clrs = ['r', 'b', 'g', 'm', 'y'] # colors\n    plt.axis('off')\n    foo = plt.scatter(X[0,:], X[1,:], s=30, color='k') # plot data\n    K = np.shape(C)[1]\n    for k in np.arange(0, K):\n        foo = plt.scatter(C[0, k], C[1, k], s=120, color=clrs[k], marker=(5, 2)) # plot centroids\n    return    \n        \n"""
mlrefined_libraries/unsupervised_library/PCA_demos.py,43,"b'import numpy as np\nfrom matplotlib import gridspec\nfrom IPython.display import display, HTML\nimport copy\nimport math\nimport time\n\n# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nfrom matplotlib import gridspec\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n########## data generating functions ##########\n# make a gaussian dataset in 2-dimensions\ndef generate_2d_data(num_pts):\n    x_1 = 3*np.random.randn(num_pts,1)\n    x_2 = 1*np.random.randn(num_pts,1)\n\n    # concatenate data into single matrix\n    X = np.concatenate((x_1,x_2),axis = 1).T\n    \n    # rotate a bit\n    theta = -np.pi*0.25\n    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)],[np.sin(theta), np.cos(theta)]])\n    rotation_matrix = np.reshape(rotation_matrix,(2,2))\n    X = np.dot(rotation_matrix,X)\n    \n    # normalize data\n    X_means = np.mean(X,axis=1)\n    X_stds = np.std(X,axis = 1)\n    X_normalized = ((X.T - X_means)/(X_stds + 10**(-7))).T\n    \n    return X,X_normalized\n\n# make a gaussian dataset in 3-dimensions\ndef generate_3d_data(num_pts):\n    x_1 = 0.75*np.random.randn(num_pts,1) + 1\n    x_2 = 0.75*np.random.randn(num_pts,1) + 1\n    x_3 = 7*x_1 + x_2 + 0.35*np.random.randn(num_pts,1)\n\n    # concatenate data into single matrix\n    X = np.concatenate((x_1,x_2),axis = 1)\n    X = np.concatenate((X,x_3),axis=1).T \n    \n    return X\n\n########## data generating functions ##########\ndef frame_3d_plot(data,ax):\n    # strip off each dimension of data\n    x_1 = data[:,0]\n    x_2 = data[:,1]\n    x_3 = data[:,2]\n    \n    # hack to set aspect ratio to \'equal\' in matplotlib 3d plot\n    # Create cubic bounding box to simulate equal aspect ratio\n    max_range = np.array([x_1.max()-x_1.min(), x_2.max()-x_2.min(), x_3.max()-x_3.min()]).max()\n    Xb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][0].flatten() + 0.5*(x_1.max()+x_1.min())\n    Yb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][1].flatten() + 0.5*(x_2.max()+x_2.min())\n    Zb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][2].flatten() + 0.5*(x_3.max()+x_3.min())\n    # Comment or uncomment following both lines to test the fake bounding box:\n    for xb, yb, zb in zip(Xb, Yb, Zb):\n        ax.plot([xb], [yb], [zb], \'w\')\n        \ndef plot_hyperplane(data,slopes,ax):\n    # define input space\n    xmin = np.min(data[:,0])\n    xmax = np.max(data[:,0])\n    xgap = (xmax - xmin)*0.1\n    xmin -= xgap\n    xmax += xgap\n    \n    ymin = np.min(data[:,1])\n    ymax = np.max(data[:,1])\n    ygap = (ymax - ymin)*0.1\n    ymin -= ygap\n    ymax += ygap\n    \n    # create meshgrid\n    xrange = np.linspace(xmin,xmax,200)\n    yrange = np.linspace(ymin,ymax,200)\n    w1_vals, w2_vals = np.meshgrid(xrange,yrange)\n    w1_vals.shape = (len(xrange)**2,1)\n    w2_vals.shape = (len(yrange)**2,1)\n    \n    # compute normal vector to plane\n    normal_vector = np.cross(slopes[:,0], slopes[:,1])\n    normal_vector = normal_vector/(-normal_vector[-1])\n    \n    # hyperplane function\n    func = lambda w: normal_vector[0]*w[0] + normal_vector[1]*w[1]\n\n    # evaluate hyperplane\n    zvals = func([w1_vals,w2_vals]) \n\n    # vals for cost surface, reshape for plot_surface function\n    w1_vals.shape = (len(xrange),len(xrange))\n    w2_vals.shape = (len(yrange),len(yrange))\n    zvals.shape = (len(xrange),len(yrange))\n\n    ### plot function and z=0 for visualization ###\n    ax.plot_surface(w1_vals, w2_vals, zvals, alpha = 0.1,color = \'r\',zorder = 2)\n    \ndef project_data_from_3d_to_2d(X,C,view):\n\n    # create plotting panel\n    fig = plt.figure(figsize = (10,4))\n\n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1.5,1,1.5]) \n    ax1 = plt.subplot(gs[0],projection=\'3d\',aspect = \'equal\');  \n    ax2 = plt.subplot(gs[1],aspect = \'equal\'); \n    ax3 = plt.subplot(gs[2],projection=\'3d\',aspect = \'equal\');  \n\n    #### plot original data ####\n    # scatter normalized data\n    ax1.scatter(X[0,:],X[1,:],X[2,:],c = \'k\',alpha = 0.25)\n\n    # plot principal components\n    a = np.zeros((2,1))\n    ax1.quiver(a,a,a,C[0,:],C[1,:],C[2,:],color = \'r\')\n\n    # draw hyperplane\n    plot_hyperplane(X.T,C,ax1)\n    \n    # clean up panel 1\n    ax1.view_init(view[0],view[1])\n    ax1.set_xlabel(r\'$x_1$\',fontsize = 18,labelpad = 5)\n    ax1.set_ylabel(r\'$x_2$\',fontsize = 18,labelpad = 5)\n    ax1.zaxis.set_rotate_label(False)  # disable automatic rotation\n    ax1.set_zlabel(r\'$x_3$\',fontsize = 18,rotation = 0)\n    ax1.set_title(\'Original data\',fontsize = 20)\n    move_axis_left(ax1)\n\n    #### plot encoded data ####\n    W = np.linalg.solve(np.dot(C.T,C),np.dot(C.T,X))\n\n    # in new coordinate system defined by pcs\n    ax2.scatter(W[0,:],W[1,:],c = \'k\',edgecolor = \'w\',linewidth = 1,s = 50,zorder = 2)\n\n    # paint arrows on data\n    ax2.arrow(0,0,0,1,fc=""r"", ec=""r"",head_width=0.15, head_length=0.15,linewidth = 2,zorder = 3)\n    ax2.arrow(0,0,1,0,fc=""r"", ec=""r"",head_width=0.15, head_length=0.15,linewidth = 2,zorder = 3)   \n    \n    # clean up panel 2\n    ax2.set_xlabel(r\'$c_1$\',fontsize = 18)\n    ax2.set_ylabel(r\'$c_2$\',fontsize = 18,rotation = 0)\n    ax2.axhline(y=0, color=\'k\', linewidth=1.5,zorder = 1)\n    ax2.axvline(x=0, color=\'k\', linewidth=1,zorder = 1)\n    ax2.set_title(\'Encoded data\',fontsize = 20)\n    \n    xmin = np.min([-1.5,np.min(W[0,:])])\n    xmax = np.max([1.5,np.max(W[0,:])])\n    xgap = (xmax - xmin)*0.2\n    xmin -= xgap\n    xmax += xgap\n    \n    ymin = np.min([-1.5,np.min(W[1,:])])\n    ymax = np.max([1.5,np.max(W[1,:])])\n    ygap = (ymax - ymin)*0.2\n    ymin -= ygap\n    ymax += ygap\n   \n    ax2.set_xlim([xmin,xmax])\n    ax2.set_ylim([ymin,ymax])\n    \n    #### plot decoded data ####\n    # scatter decoded data\n    X_d = np.dot(C,W)\n    ax3.scatter(X_d[0,:],X_d[1,:],X_d[2,:],c = \'k\',edgecolor = \'r\',linewidth = 1,alpha = 0.25)\n    \n    # draw hyperplane\n    plot_hyperplane(X.T,C,ax3)\n    \n    # clean up panel 1\n    ax3.view_init(view[0],view[1])\n    ax3.set_xlabel(r\'$x_1$\',fontsize = 18,labelpad = 5)\n    ax3.set_ylabel(r\'$x_2$\',fontsize = 18,labelpad = 5)\n    ax3.zaxis.set_rotate_label(False)  # disable automatic rotation\n    ax3.set_zlabel(r\'$x_3$\',fontsize = 18,rotation = 0)\n    ax3.set_title(\'Decoded data\',fontsize = 20)\n    move_axis_left(ax3)\n    \n    # set viewing range based on original plot\n    vals = ax1.get_zlim()\n    ax3.set_zlim([vals[0],vals[1]])\n    \n# func,\ndef pca_visualizer(X,W,pcs):\n    # renderer    \n    fig = plt.figure(figsize = (10,5))\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 2) \n    ax1 = plt.subplot(gs[0],aspect = \'equal\');\n    ax2 = plt.subplot(gs[1],aspect = \'equal\'); \n                 \n    # sphere the results\n    ars = np.eye(2)\n        \n    # loop over panels and plot each \n    c = 1\n    for ax,pt,ar in zip([ax1,ax2],[X,W],[pcs,ars]): \n        # set viewing limits for originals\n        xmin = np.min(pt[0,:])\n        xmax = np.max(pt[0,:])\n        xgap = (xmax - xmin)*0.15\n        xmin -= xgap\n        xmax += xgap\n        ymin = np.min(pt[1,:])\n        ymax = np.max(pt[1,:])\n        ygap = (ymax - ymin)*0.15\n        ymin -= ygap\n        ymax += ygap\n    \n        # scatter points\n        ax.scatter(pt[0,:],pt[1,:],s = 60, c = \'k\',edgecolor = \'w\',linewidth = 1,zorder = 2)\n   \n        # plot original vectors\n        vector_draw(ar[:,0].flatten(),ax,color = \'red\',zorder = 3)\n        vector_draw(ar[:,1].flatten(),ax,color = \'red\',zorder = 3)\n\n        # plot x and y axes, and clean up\n        ax.grid(True, which=\'both\')\n        ax.axhline(y=0, color=\'k\', linewidth=1.5,zorder = 1)\n        ax.axvline(x=0, color=\'k\', linewidth=1,zorder = 1)\n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n        ax.grid(\'off\')\n\n        # set tick label fonts\n        for tick in ax.xaxis.get_major_ticks():\n            tick.label.set_fontsize(12) \n        for tick in ax.yaxis.get_major_ticks():\n            tick.label.set_fontsize(12) \n        \n        # plot title\n        if c == 1:\n            ax.set_title(\'original space\',fontsize = 22)\n            ax.set_xlabel(r\'$x_1$\',fontsize = 22)\n            ax.set_ylabel(r\'$x_2$\',fontsize = 22,rotation = 0,labelpad = 10)\n        if c == 2:\n            ax.set_title(\'PCA transformed space\',fontsize = 22)\n            ax.set_xlabel(r\'$v_1$\',fontsize = 22)\n            ax.set_ylabel(r\'$v_2$\',fontsize = 22,rotation = 0,labelpad = 10)\n        c+=1\n    \n# func,\ndef sphereing_visualizer(X,V,W,S):\n    # renderer    \n    fig = plt.figure(figsize = (10,5))\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3) \n    ax1 = plt.subplot(gs[0],aspect = \'equal\');\n    ax2 = plt.subplot(gs[1],aspect = \'equal\'); \n    ax3 = plt.subplot(gs[2],aspect = \'equal\'); \n    ars2 = np.eye(2)\n    ars = ars2\n        \n    # loop over panels and plot each \n    c = 1\n    for ax,pt,ar in zip([ax1,ax2,ax3],[X,W,S],[V,ars,ars2]): \n        # set viewing limits for originals\n        xmin = np.min(pt[0,:])\n        xmax = np.max(pt[0,:])\n        xgap = (xmax - xmin)*0.15\n        xmin -= xgap\n        xmax += xgap\n        ymin = np.min(pt[1,:])\n        ymax = np.max(pt[1,:])\n        ygap = (ymax - ymin)*0.15\n        ymin -= ygap\n        ymax += ygap\n    \n        # scatter points\n        ax.scatter(pt[0,:],pt[1,:],s = 60, c = \'k\',edgecolor = \'w\',linewidth = 1,zorder = 2)\n   \n        # plot original vectors\n        vector_draw(ar[:,0].flatten(),ax,color = \'red\',zorder = 3)\n        vector_draw(ar[:,1].flatten(),ax,color = \'red\',zorder = 3)\n\n        # plot x and y axes, and clean up\n        ax.grid(True, which=\'both\')\n        ax.axhline(y=0, color=\'k\', linewidth=1.5,zorder = 1)\n        ax.axvline(x=0, color=\'k\', linewidth=1,zorder = 1)\n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n        ax.grid(\'off\')\n\n        # set tick label fonts\n        for tick in ax.xaxis.get_major_ticks():\n            tick.label.set_fontsize(12) \n        for tick in ax.yaxis.get_major_ticks():\n            tick.label.set_fontsize(12) \n        \n        # plot title\n        if c == 1:\n            ax.set_title(\'original space\',fontsize = 22)\n            ax.set_xlabel(r\'$x_1$\',fontsize = 22)\n            ax.set_ylabel(r\'$x_2$\',fontsize = 22,rotation = 0,labelpad = 10)\n        if c == 2:\n            ax.set_title(\'PCA transformed space\',fontsize = 22)\n            ax.set_xlabel(r\'$v_1$\',fontsize = 22)\n            ax.set_ylabel(r\'$v_2$\',fontsize = 22,rotation = 0,labelpad = 10)\n        if c == 3:\n            ax.set_title(\'Sphered data space\',fontsize = 22)\n            ax.set_xlabel(r\'$\\frac{1}{d_1^{^1/_2}}v_1$\',fontsize = 22)\n            ax.set_ylabel(r\'$\\frac{1}{d_2^{^1/_2}}v_2$\',fontsize = 22,rotation = 0,labelpad = 10)\n        c+=1\n \n# draw a vector\ndef vector_draw(vec,ax,**kwargs):\n    color = \'k\'\n    if \'color\' in kwargs:\n        color = kwargs[\'color\']\n    zorder = 3 \n    if \'zorder\' in kwargs:\n        zorder = kwargs[\'zorder\']\n        \n    veclen = math.sqrt(vec[0]**2 + vec[1]**2)\n    head_length = 0.25\n    head_width = 0.25\n    vec_orig = copy.deepcopy(vec)\n    vec = (veclen - head_length)/veclen*vec\n    ax.arrow(0, 0, vec[0],vec[1], head_width=head_width, head_length=head_length, fc=color, ec=color,linewidth=3,zorder = zorder)\n      \n        \n# set axis in left panel\ndef move_axis_left(ax):\n    tmp_planes = ax.zaxis._PLANES \n    ax.zaxis._PLANES = ( tmp_planes[2], tmp_planes[3], \n                        tmp_planes[0], tmp_planes[1], \n                        tmp_planes[4], tmp_planes[5])   \n    ax.grid(False)\n    \n    ax.xaxis.pane.fill = False\n    ax.yaxis.pane.fill = False\n    ax.zaxis.pane.fill = False\n\n    ax.xaxis.pane.set_edgecolor(\'white\')\n    ax.yaxis.pane.set_edgecolor(\'white\')\n    ax.zaxis.pane.set_edgecolor(\'white\')'"
mlrefined_libraries/unsupervised_library/PCA_functionality.py,20,"b""import numpy as np\nfrom matplotlib import gridspec\nfrom IPython.display import display, HTML\nimport copy\nimport math\nimport time\n\n# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nfrom matplotlib import gridspec\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n######### centering and contrast-normalizing #########\ndef center(X):\n    '''\n    A function for normalizing each feaure dimension of an input array, mean-centering\n    and division by its standard deviation\n    \n    '''\n    X_means = np.mean(X,axis=0)[np.newaxis,:]\n    X_normalized = X - X_means\n\n    return X_normalized\n\ndef contrast_normalize(X):\n    '''\n    A contrast-normalizing function for image data pre-sphereing normalization.\n    \n    '''\n    # compute and subtract off means\n    X_means = np.mean(X,axis=0)[np.newaxis,:]\n    X = X - X_means\n    \n    # divide off std of each image - remove any images deemed constant (whose std = 0)\n    X_stds = np.std(X,axis=0)[np.newaxis,:]\n    ind = np.argwhere(np.abs(X_stds) > 10**(-7))\n    ind = np.array([s[1] for s in ind])\n    X = X[:,ind]\n    X_stds = X_stds[:,ind]\n    X_normalized = X/X_stds\n    \n    # print report to user if any patches deemed constant\n    report = np.shape(X_means)[1] - len(ind) \n    if report > 0:\n        print (str(report) + ' images of ' + str(np.shape(X_means)[1]) + ' imagses found to be constant, and so were removed')\n\n    return X_normalized\n\n########## sphereing pre-processing functionality ##########\ndef compute_pcs(X,lam):\n    '''\n    A function for computing the principal components of an input data matrix.  Both\n    principal components and variance parameters (eigenvectors and eigenvalues of XX^T)\n    are returned\n    '''\n    # create the correlation matrix\n    P = float(X.shape[1])\n    Cov = 1/P*np.dot(X,X.T) + lam*np.eye(X.shape[0])\n\n    # use numpy function to compute eigenvalues / vectors of correlation matrix\n    D,V = np.linalg.eigh(Cov)\n    return V, D\n\ndef pca_transform_data(X,**kwargs):\n    '''\n    A function for producing the full PCA transformation on an input dataset X.  \n    '''\n    # user-determined number of principal components to keep, and regularizer penalty param\n    num_components = X.shape[0]\n    if 'num_components' in kwargs:\n        num_components = kwargs['num_components']\n    lam = 10**(-7)\n    if 'lam' in kwargs:\n        lam = kwargs['lam']\n    \n    # compute principal components\n    V,D = compute_pcs(X,lam)\n    V = V[:,-num_components:]\n    D = D[-num_components:]\n\n    # compute transformed data for PC space: V^T X\n    W = np.dot(V.T,X)\n    return W,V,D\n      \ndef PCA_sphere(X,**kwargs):\n    '''\n    A function for producing the full PCA sphereing on an input dataset X.  \n    '''\n    # compute principal components\n    W,V,D = pca_transform_data(X,**kwargs)\n    \n    # compute transformed data for PC space: V^T X\n    W = np.dot(V.T,X)\n    D_ = np.array([1/d**(0.5) for d in D])\n    D_ = np.diag(D_)\n    S = np.dot(D_,W)\n    return W,S\n\ndef ZCA_sphere(X,**kwargs):\n    '''\n    A function for producing the full PCA sphereing on an input dataset X.  \n    '''   \n    \n    # compute principal components\n    W,V,D = pca_transform_data(X,**kwargs)\n    \n    # PCA-sphere data\n    W = np.dot(V.T,X)\n    D_ = np.array([1/d**(0.5) for d in D])\n    D_ = np.diag(D_)\n    S = np.dot(D_,W)\n    \n    # rotate data back to original orientation - ZCA sphere\n    Z = np.dot(V,S)\n    \n    return W,S,Z\n\n########## plotting functionality ############\ndef show_images(X):\n    '''\n    Function for plotting input images, stacked in columns of input X.\n    '''\n    # plotting mechanism taken from excellent answer from stack overflow: https://stackoverflow.com/questions/20057260/how-to-remove-gaps-between-subplots-in-matplotlib\n    plt.figure(figsize = (9,3))\n    gs1 = gridspec.GridSpec(5, 14)\n    gs1.update(wspace=0, hspace=0.05) # set the spacing between axes. \n    \n    # shape of square version of image\n    square_shape = int((X.shape[0])**(0.5))\n\n    for i in range(min(70,X.shape[1])):\n        # plot image in panel\n        ax = plt.subplot(gs1[i])\n        im = ax.imshow(255 - np.reshape(X[:,i],(square_shape,square_shape)),cmap = 'gray')\n\n        # clean up panel\n        plt.axis('off')\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n\n    plt.show()"""
mlrefined_libraries/unsupervised_library/__init__.py,0,b''
mlrefined_libraries/unsupervised_library/autoencoder_demos.py,23,"b"" # import autograd functionality to bulid function's properly for optimizers\nimport autograd.numpy as np\nimport math\nimport copy\n\n# import matplotlib functionality\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\n\ndef visual_comparison(x,weights):\n    '''\n    Visually compare the results of several runs of PCA applied to two dimensional input and \n    two principal components\n    '''\n    # do weights\n    weights = np.array(weights)\n    num_runs = np.ndim(weights)\n    \n    # plot data\n    fig = plt.figure(figsize = (10,4))\n    gs = gridspec.GridSpec(1, num_runs) \n    for run in range(num_runs):\n        # create subplot\n        ax = plt.subplot(gs[run],aspect = 'equal'); \n        w_best = weights[run]\n\n        # scatter data\n        ax.scatter(x[0,:],x[1,:],c = 'k')\n        \n        # plot pc 1\n        vector_draw(w_best[:,0],ax,color = 'red',zorder = 1)\n        vector_draw(w_best[:,1],ax,color = 'red',zorder = 1)\n\n        # plot vertical / horizontal axes\n        ax.axhline(linewidth=0.5, color='k',zorder = 0)\n        ax.axvline(linewidth=0.5, color='k',zorder = 0)\n        ax.set_title('run ' + str(run + 1),fontsize=16)\n        ax.set_xlabel(r'$x_1$',fontsize = 16)\n        ax.set_ylabel(r'$x_2$',fontsize = 16,rotation = 0,labelpad = 10)\n        \ndef show_encode_decode(x,cost_history,weight_history,**kwargs):\n    '''\n    Examine the results of linear or nonlinear PCA / autoencoder to two-dimensional input.\n    Four panels are shown: \n    - original data (top left panel)\n    - data projected onto lower dimensional curve (top right panel)\n    - lower dimensional curve (lower left panel)\n    - vector field illustrating how points in space are projected onto lower dimensional curve (lower right panel)\n    \n    Inputs: \n    - x: data\n    - encoder: encoding function from autoencoder\n    - decoder: decoding function from autoencoder\n    - cost_history/weight_history: from run of gradient descent minimizing PCA least squares\n    \n    Optinal inputs:\n    - show_pc: show pcs?   Only useful really for linear case.\n    - scale: for vector field / quiver plot, adjusts the length of arrows in vector field\n    '''\n    # user-adjustable args\n    encoder = lambda a,b: np.dot(b.T,a)\n    decoder = lambda a,b: np.dot(b,a)\n    if 'encoder' in kwargs:\n        encoder = kwargs['encoder']\n    if 'decoder' in kwargs:\n        decoder = kwargs['decoder']\n    projmap = False\n    if 'projmap' in kwargs:\n        projmap = kwargs['projmap']\n    show_pc = False\n    if 'show_pc' in kwargs:\n        show_pc = kwargs['show_pc']\n    scale = 14\n    if 'scale' in kwargs:\n        scale = kwargs['scale']\n    encode_label = ''\n    if 'encode_label' in kwargs:\n        encode_label = kwargs['encode_label']\n\n    # pluck out best weights\n    ind = np.argmin(cost_history)\n    w_best = weight_history[ind]\n    num_params = 0\n    if type(w_best)==list:\n        num_params = len(w_best)\n    else:\n        num_params = np.ndim(w_best) - 1\n\n    ###### figure 1 - original data, encoded data, decoded data ######\n    fig = plt.figure(figsize = (10,4))\n    gs = gridspec.GridSpec(1, 3) \n    ax1 = plt.subplot(gs[0],aspect = 'equal'); \n    ax2 = plt.subplot(gs[1],aspect = 'equal'); \n    ax3 = plt.subplot(gs[2],aspect = 'equal'); \n\n    # scatter original data with pc\n    ax1.scatter(x[0,:],x[1,:],c = 'k',s = 60,linewidth = 0.75,edgecolor = 'w')\n\n    if show_pc == True:\n        for pc in range(np.shape(w_best)[1]):\n            ax1.arrow(0, 0, w_best[0,pc], w_best[1,pc], head_width=0.25, head_length=0.5, fc='k', ec='k',linewidth = 4)\n            ax1.arrow(0, 0, w_best[0,pc], w_best[1,pc], head_width=0.25, head_length=0.5, fc='r', ec='r',linewidth = 3)\n\n    ### plot encoded and decoded data ###\n    v = 0\n    p = 0\n    if num_params == 2:\n        # create encoded vectors\n        v = encoder(x,w_best[0])\n\n        # decode onto basis\n        p = decoder(v,w_best[1])\n    else:\n        # create encoded vectors\n        v = encoder(x,w_best)\n\n        # decode onto basis\n        p = decoder(v,w_best)\n\n    # plot encoded data \n    if v.shape[0] == 1:\n        z = np.zeros((1,np.size(v)))\n        ax2.scatter(v,z,c = 'k',s = 60,linewidth = 0.75,edgecolor = 'w')\n    elif v.shape[0] == 2:\n        ax2.scatter(v[0],v[1].flatten(),c = 'k',s = 60,linewidth = 0.75,edgecolor = 'w')\n\n    \n    # plot decoded data \n    ax3.scatter(p[0,:],p[1,:],c = 'k',s = 60,linewidth = 0.75,edgecolor = 'r')\n\n    # clean up panels\n    xmin1 = np.min(x[0,:])\n    xmax1 = np.max(x[0,:])\n    xmin2 = np.min(x[1,:])\n    xmax2 = np.max(x[1,:])\n    xgap1 = (xmax1 - xmin1)*0.2\n    xgap2 = (xmax2 - xmin2)*0.2\n    xmin1 -= xgap1\n    xmax1 += xgap1\n    xmin2 -= xgap2\n    xmax2 += xgap2\n    \n    for ax in [ax1,ax2,ax3]:\n        if ax == ax1 or ax == ax3:\n            ax.set_xlim([xmin1,xmax1])\n            ax.set_ylim([xmin2,xmax2])\n            ax.set_xlabel(r'$x_1$',fontsize = 16)\n            ax.set_ylabel(r'$x_2$',fontsize = 16,rotation = 0,labelpad = 10)\n            ax.axvline(linewidth=0.5, color='k',zorder = 0)\n        else:\n            ax.set_ylim([-1,1])\n            if len(encode_label) > 0:\n                ax.set_xlabel(encode_label,fontsize = 16)\n        ax.axhline(linewidth=0.5, color='k',zorder = 0)\n    \n    ax1.set_title('original data',fontsize = 18)\n    ax2.set_title('encoded data',fontsize = 18)\n    ax3.set_title('decoded data',fontsize = 18)\n    \n    # plot learned manifold\n    a = np.linspace(xmin1,xmax1,400)\n    b = np.linspace(xmin2,xmax2,400)\n    s,t = np.meshgrid(a,b)\n    s.shape = (1,len(a)**2)\n    t.shape = (1,len(b)**2)\n    z = np.vstack((s,t))\n    \n    v = 0\n    p = 0\n    if num_params == 2:\n        # create encoded vectors\n        v = encoder(z,w_best[0])\n\n        # decode onto basis\n        p = decoder(v,w_best[1])\n    else:\n        # create encoded vectors\n        v = encoder(z,w_best)\n\n        # decode onto basis\n        p = decoder(v,w_best)\n    \n    ax3.scatter(p[0,:],p[1,:],c = 'k',s = 1.5,edgecolor = 'r',linewidth = 1,zorder = 0)\n    ax3.axis('off')\n    # set whitespace\n    #fgs.update(wspace=0.01, hspace=0.5) # set the spacing between axes. \n        \n    ##### bottom panels - plot subspace and quiver plot of projections ####\n    if projmap == True:\n        fig = plt.figure(figsize = (10,4))\n        gs = gridspec.GridSpec(1, 1) \n        ax1 = plt.subplot(gs[0],aspect = 'equal'); \n        ax1.scatter(p[0,:],p[1,:],c = 'r',s = 9.5)\n        ax1.scatter(p[0,:],p[1,:],c = 'k',s = 1.5)\n        \n        ### create quiver plot of how data is projected ###\n        new_scale = 0.75\n        a = np.linspace(xmin1 - xgap1*new_scale,xmax1 + xgap1*new_scale,20)\n        b = np.linspace(xmin2 - xgap2*new_scale,xmax2 + xgap2*new_scale,20)\n        s,t = np.meshgrid(a,b)\n        s.shape = (1,len(a)**2)\n        t.shape = (1,len(b)**2)\n        z = np.vstack((s,t))\n        \n        v = 0\n        p = 0\n        if num_params == 2:\n            # create encoded vectors\n            v = encoder(z,w_best[0])\n\n            # decode onto basis\n            p = decoder(v,w_best[1])\n        else:\n            # create encoded vectors\n            v = encoder(z,w_best)\n\n            # decode onto basis\n            p = decoder(v,w_best)\n\n\n        # get directions\n        d = []\n        for i in range(p.shape[1]):\n            dr = (p[:,i] - z[:,i])[:,np.newaxis]\n            d.append(dr)\n        d = 2*np.array(d)\n        d = d[:,:,0].T\n        M = np.hypot(d[0,:], d[1,:])\n        ax1.quiver(z[0,:], z[1,:], d[0,:], d[1,:],M,alpha = 0.5,width = 0.01,scale = scale,cmap='autumn') \n        ax1.quiver(z[0,:], z[1,:], d[0,:], d[1,:],edgecolor = 'k',linewidth = 0.25,facecolor = 'None',width = 0.01,scale = scale) \n\n        #### clean up and label panels ####\n        for ax in [ax1]:\n            ax.set_xlim([xmin1 - xgap1*new_scale,xmax1 + xgap1*new_scale])\n            ax.set_ylim([xmin2 - xgap2*new_scale,xmax2 + xgap1*new_scale])\n            ax.set_xlabel(r'$x_1$',fontsize = 16)\n            ax.set_ylabel(r'$x_2$',fontsize = 16,rotation = 0,labelpad = 10)\n\n        ax1.set_title('projection map',fontsize = 18)\n        #ax.axvline(linewidth=0.5, color='k',zorder = 0)\n        #ax.axhline(linewidth=0.5, color='k',zorder = 0)\n\n        # set whitespace\n        gs.update(wspace=0.01, hspace=0.5) # set the spacing between axes. \n        #ax.set_xlim([xmin1,xmax1])\n        #ax.set_ylim([xmin2,xmax2])\n        ax.axis('off')\n    \n# draw a vector\ndef vector_draw(vec,ax,**kwargs):\n    color = 'k'\n    if 'color' in kwargs:\n        color = kwargs['color']\n    zorder = 3 \n    if 'zorder' in kwargs:\n        zorder = kwargs['zorder']\n        \n    veclen = math.sqrt(vec[0]**2 + vec[1]**2)\n    head_length = 0.25\n    head_width = 0.25\n    vec_orig = copy.deepcopy(vec)\n    vec = (veclen - head_length)/veclen*vec\n    ax.arrow(0, 0, vec[0],vec[1], head_width=head_width, head_length=head_length, fc=color, ec=color,linewidth=3,zorder = zorder)\n      """
mlrefined_libraries/unsupervised_library/normalizers.py,8,"b""import autograd.numpy as np\n\nclass My_Normalizers:\n    '''\n    A class that wraps up the various input normalization schemes\n    we have seen including\n    - mean centering / std normalization\n    - PCA sphereing\n    \n    For each scheme you put in input features, and the following is returned\n    - normalizer: the normalization scheme of your choice, returned as a function that \n    you can then use for future test points\n    \n    You can then normalize the input x of a dataset using the desired normalization scheme\n    by \n    \n    x_normalized = normalizer(x)\n    '''    \n\n    ###### standard normalization function ######\n    def standard(self,x):\n        # compute the mean and standard deviation of the input\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n\n        # create standard normalizer function\n        normalizer = lambda data: (data - x_means)/x_stds\n\n        # return normalizer \n        return normalizer\n\n    ###### standard normalization function ######\n    # compute eigendecomposition of data covariance matrix\n    def PCA(self,x,**kwargs):\n        # regularization parameter for numerical stability\n        lam = 10**(-7)\n        if 'lam' in kwargs:\n            lam = kwargs['lam']\n\n        # create the correlation matrix\n        P = float(x.shape[1])\n        Cov = 1/P*np.dot(x,x.T) + lam*np.eye(x.shape[0])\n\n        # use numpy function to compute eigenvalues / vectors of correlation matrix\n        D,V = np.linalg.eigh(Cov)\n        return D,V\n\n    # PCA-sphereing - use PCA to normalize input features\n    def PCA_sphereing(self,x,**kwargs):\n        # standard normalize the input data\n        standard_normalizer = self.standard(x)\n        x_standard = standard_normalizer(x)\n        \n        # compute pca transform \n        D,V = self.PCA(x_standard,**kwargs)\n        \n        # compute forward sphereing transform\n        D_ = np.array([1/d**(0.5) for d in D])\n        D_ = np.diag(D_)\n        W = np.dot(D_,V.T)\n        pca_sphere_normalizer = lambda data: np.dot(W,standard_normalizer(data))\n\n        # return normalizer \n        return pca_sphere_normalizer\n"""
mlrefined_libraries/unsupervised_library/optimimzers.py,6,"b""# clear display\nfrom IPython.display import clear_output\n\n# import autograd functionality\nfrom autograd import grad as compute_grad   # The only autograd function you may ever need\nimport autograd.numpy as np\nfrom autograd import hessian as compute_hess\nimport math\nimport time\nimport copy\nfrom autograd.misc.flatten import flatten_func\n\nclass MyOptimizers:\n    '''\n    A list of current optimizers.  In each case - since these are used for educational purposes - the weights at each step are recorded and returned.\n    '''\n\n    ### gradient descent ###\n    def gradient_descent(self,g,w,**kwargs):                \n        # create gradient function\n        self.g = g\n        self.grad = compute_grad(self.g)\n        \n        # parse optional arguments        \n        max_its = 100\n        if 'max_its' in kwargs:\n            max_its = kwargs['max_its']\n        version = 'unnormalized'\n        if 'version' in kwargs:\n            version = kwargs['version']\n        alpha = 10**-4\n        if 'alpha' in kwargs:\n            alpha = kwargs['alpha']\n        steplength_rule = 'none'    \n        if 'steplength_rule' in kwargs:\n            steplength_rule = kwargs['steplength_rule']\n        projection = 'None'\n        if 'projection' in kwargs:\n            projection = kwargs['projection']\n        verbose = False\n        if 'verbose' in kwargs:\n            verbose = kwargs['verbose']\n       \n        # create container for weight history \n        w_hist = []\n        w_hist.append(w)\n        \n        # start gradient descent loop\n        if verbose == True:\n            print ('starting optimization...')\n        for k in range(max_its):   \n            # plug in value into func and derivative\n            grad_eval = self.grad(w)\n            grad_eval.shape = np.shape(w)\n            \n            ### normalized or unnormalized descent step? ###\n            if version == 'normalized':\n                grad_norm = np.linalg.norm(grad_eval)\n                if grad_norm == 0:\n                    grad_norm += 10**-6*np.sign(2*np.random.rand(1) - 1)\n                grad_eval /= grad_norm\n            \n            # use backtracking line search?\n            if steplength_rule == 'backtracking':\n                alpha = self.backtracking(w,grad_eval)\n                \n            # use a pre-set diminishing steplength parameter?\n            if steplength_rule == 'diminishing':\n                alpha = 1/(float(k + 1))\n            \n            ### take gradient descent step ###\n            w = w - alpha*grad_eval\n            \n            # record\n            w_hist.append(w)     \n         \n        if verbose == True:\n            print ('...optimization complete!')\n            time.sleep(1.5)\n            clear_output()\n        \n        return w_hist\n\n    # backtracking linesearch module\n    def backtracking(self,w,grad_eval):\n        # set input parameters\n        alpha = 1\n        t = 0.8\n        \n        # compute initial function and gradient values\n        func_eval = self.g(w)\n        grad_norm = np.linalg.norm(grad_eval)**2\n        \n        # loop over and tune steplength\n        while self.g(w - alpha*grad_eval) > func_eval - alpha*0.5*grad_norm:\n            alpha = t*alpha\n        return alpha\n            \n    #### newton's method ####            \n    def newtons_method(self,g,w,**kwargs):        \n        # create gradient and hessian functions\n        self.g = g\n        \n        # flatten gradient for simpler-written descent loop\n        flat_g, unflatten, w = flatten_func(self.g, w)\n        \n        self.grad = compute_grad(flat_g)\n        self.hess = compute_hess(flat_g)  \n        \n        # parse optional arguments        \n        max_its = 20\n        if 'max_its' in kwargs:\n            max_its = kwargs['max_its']\n        self.epsilon = 10**(-5)\n        if 'epsilon' in kwargs:\n            self.epsilon = kwargs['epsilon']\n        verbose = False\n        if 'verbose' in kwargs:\n            verbose = kwargs['verbose']\n        \n        # create container for weight history \n        w_hist = []\n        w_hist.append(unflatten(w))\n        \n        # start newton's method loop  \n        if verbose == True:\n            print ('starting optimization...')\n            \n        geval_old = flat_g(w)\n        for k in range(max_its):\n            # compute gradient and hessian\n            grad_val = self.grad(w)\n            hess_val = self.hess(w)\n            hess_val.shape = (np.size(w),np.size(w))\n\n            # solve linear system for weights\n            w = w - np.dot(np.linalg.pinv(hess_val + self.epsilon*np.eye(np.size(w))),grad_val)\n                    \n            # eject from process if reaching singular system\n            geval_new = flat_g(w)\n            if k > 2 and geval_new > geval_old:\n                print ('singular system reached')\n                time.sleep(1.5)\n                clear_output()\n                return w_hist\n            else:\n                geval_old = geval_new\n                \n            # record current weights\n            w_hist.append(unflatten(w))\n            \n        if verbose == True:\n            print ('...optimization complete!')\n            time.sleep(1.5)\n            clear_output()\n        \n        return w_hist"""
mlrefined_libraries/unsupervised_library/plot_utilities.py,2,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nfrom mpl_toolkits.mplot3d import proj3d\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\n\n# import autograd functionality\nimport numpy as np\nimport math\nimport time\nimport copy\n\nclass Visualizer:\n    '''\n    Various plotting functions \n    '''             \n    \n    # compare cost to counting\n    def plot_cost_history(self,history):\n        ##### setup figure to plot #####\n        # initialize figure\n        fig = plt.figure(figsize = (7,3))\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 1) \n        ax1 = plt.subplot(gs[0]); \n\n        # run through weights, evaluate classification and counting costs, record\n        ax1.plot(history,linewidth = 4*(0.8))\n\n        ax1.set_xlabel('iteration',fontsize = 10)\n        ax1.set_ylabel('cost function value',fontsize = 10)\n        plt.show()\n\n    # compare cost to counting\n    def compare_histories(self,histories,**kwargs):\n        # parse input args\n        label1 = ''; label2 = '';\n        if 'label1' in kwargs:\n            label1 = kwargs['label1']\n        if 'label2' in kwargs:\n            label2 = kwargs['label2']  \n        plot_range = len(histories[0])\n        if 'plot_range' in kwargs:\n            plot_range = kwargs['plot_range']\n        \n        ##### setup figure to plot #####\n        # initialize figure\n        fig = plt.figure(figsize = (7,3))\n\n        # create subplot with 3 panels, plot input function in center plot\n        gs = gridspec.GridSpec(1, 1) \n        ax1 = plt.subplot(gs[0]); \n\n        # run through weights, evaluate classification and counting costs, record\n        c = 1\n        for history in histories:\n            # plot both classification and counting cost histories\n            if c == 1:\n                ax1.plot(np.arange(1,len(history) + 1),history,label = label1,linewidth = 4*(0.8)**(c))\n            else:\n                ax1.plot(np.arange(1,len(history) + 1),history,label = label2,linewidth = 4*(0.8)**(c))\n            c += 1\n\n        ax1.set_xlabel('value of $K$',fontsize = 10)\n        ax1.set_ylabel('cost function value',fontsize = 10)\n        plt.legend(loc='upper right')\n        ax1.set_xticks(plot_range)\n        plt.show()"""
mlrefined_libraries/unsupervised_library/span_animation.py,75,"b""import numpy as np\nfrom matplotlib import gridspec\nfrom IPython.display import display, HTML\nimport copy\nimport math\n \n# import standard plotting and animation\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nimport time\nfrom matplotlib import gridspec\nimport copy\n \n# func,\ndef perfect_visualize(savepath,C,**kwargs):\n    vec1 = C[:,0]\n    vec2 = C[:,1]\n    \n    # size up vecs\n    vec1 = np.asarray(vec1)\n    vec2 = np.asarray(vec2)\n    vec1copy = copy.deepcopy(vec1)\n    vec1copy.shape = (len(vec1copy),1)\n    vec2copy = copy.deepcopy(vec2)\n    vec2copy.shape = (len(vec2copy),1)\n     \n    # renderer    \n    fig = plt.figure(figsize = (14,7))\n    artist = fig\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1,3, 1]) \n    ax1 = plt.subplot(gs[0]); ax1.axis('off');\n    ax3 = plt.subplot(gs[2]); ax3.axis('off');\n \n    # plot input function\n    ax2 = plt.subplot(gs[1])\n     \n    ### create grid of points ###\n    s = np.linspace(-5,5,10)\n    xx,yy = np.meshgrid(s,s)\n    xx.shape = (xx.size,1)\n    yy.shape = (yy.size,1)\n    pts = np.concatenate((xx,yy),axis=1)\n    pts = np.flipud(pts)\n     \n    # decide on num_frames\n    num_frames = 10\n    if 'num_frames' in kwargs:\n        num_frames = kwargs['num_frames']\n        num_frames = min(num_frames,len(xx))\n     \n    # animate\n    print ('starting animation rendering...')\n     \n    def animate(k):\n        # clear the panel\n        ax2.cla()\n         \n        # print rednering update\n        if np.mod(k+1,25) == 0:\n            print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n        if k == num_frames - 1:\n            print ('animation rendering complete!')\n            time.sleep(1.5)\n            clear_output()  \n         \n        ### take pt of grid and estimate with inputs ###        \n        # scatter every point up to k\n        for i in range(k+1):\n            pt = pts[i,:]\n            ax2.scatter(pt[0],pt[1],s = 100, c = 'k',edgecolor = 'w',linewidth = 1)\n             \n        # get current point and solve for weights\n        vec3 = pts[k,:]   \n        vec3.shape = (len(vec3),1)\n        A = np.concatenate((vec1copy,vec2copy),axis=1)\n        b = vec3\n        alpha = np.linalg.solve(A,b)\n \n        # plot original vectors\n        vector_draw(vec1copy.flatten(),ax2)\n        vector_draw(vec2copy.flatten(),ax2)\n \n        # send axis to vector adder for plotting\n        vec1 = np.asarray([alpha[0]*vec1copy[0],alpha[0]*vec1copy[1]]).flatten()\n        vec2 = np.asarray([alpha[1]*vec2copy[0],alpha[1]*vec2copy[1]]).flatten()\n        vector_add_plot(vec1,vec2,ax2)\n  \n        ax2.set_title(r'$w_1 = ' + str(round(alpha[0][0],3)) + ',\\,\\,\\,\\,\\,' + 'w_2 = ' + str(round(alpha[1][0],3)) +   '$',fontsize = 30)\n            \n        # plot x and y axes, and clean up\n        ax2.grid(True, which='both')\n        ax2.axhline(y=0, color='k', linewidth=1.5,zorder = 1)\n        ax2.axvline(x=0, color='k', linewidth=1,zorder = 1)\n \n        # set viewing limits\n        ax2.set_xlim([-6,6])\n        ax2.set_ylim([-6,6])\n         \n        # set tick label fonts\n        for tick in ax2.xaxis.get_major_ticks():\n            tick.label.set_fontsize(20) \n             \n        for tick in ax2.yaxis.get_major_ticks():\n            tick.label.set_fontsize(20) \n \n        # turn off grid\n        ax2.grid('off')\n         \n        # return artist\n        return artist,\n     \n    anim = animation.FuncAnimation(fig, animate,frames=num_frames, interval=num_frames, blit=True)\n         \n    # produce animation and save\n    fps = 50\n    if 'fps' in kwargs:\n        fps = kwargs['fps']\n    anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n    clear_output()\n\n # func,\ndef perfect_visualize_transform(savepath,C,**kwargs):\n    # extract \n    vec1 = C[:,0]\n    vec2 = C[:,1]\n    \n    # size up vecs\n    vec1 = np.asarray(vec1)\n    vec2 = np.asarray(vec2)\n    vec1copy = copy.deepcopy(vec1)\n    vec1copy.shape = (len(vec1copy),1)\n    vec2copy = copy.deepcopy(vec2)\n    vec2copy.shape = (len(vec2copy),1)\n     \n    # renderer    \n    fig = plt.figure(figsize = (14,7))\n    artist = fig\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 2) \n    ax1 = plt.subplot(gs[0]);\n    ax2 = plt.subplot(gs[1]); \n    # gs.tight_layout(fig, rect=[0, 0.03, 1, 0.97]) \n     \n    ### create grid of points ###\n    s = np.linspace(-5,5,10)\n    xx,yy = np.meshgrid(s,s)\n    xx.shape = (xx.size,1)\n    yy.shape = (yy.size,1)\n    pts = np.concatenate((xx,yy),axis=1)\n    pts = np.flipud(pts)\n     \n    if 'pts' in kwargs:\n        pts = kwargs['pts']\n     \n    # decide on num_frames\n    num_frames = 10\n    if 'num_frames' in kwargs:\n        num_frames = kwargs['num_frames']\n        num_frames = min(num_frames,len(xx))\n     \n    # swing through points and compute coeffecients\n    alphas = []\n    for k in range(num_frames):\n        vec3 = pts[k,:]   \n        vec3.shape = (len(vec3),1)\n        A = np.concatenate((vec1copy,vec2copy),axis=1)\n        b = vec3\n        alpha = np.linalg.solve(A,b)\n        alphas.append(alpha)\n         \n    # set viewing limits\n    alpha_xmin = np.min([a[0][0] for a in alphas])\n    alpha_xmax = np.max([a[0][0] for a in alphas])\n    alpha_xgap = (alpha_xmax - alpha_xmin)*0.15\n    alpha_xmin -= alpha_xgap\n    alpha_xmin = np.min([-0.5,alpha_xmin])\n    alpha_xmax += alpha_xgap\n    alpha_xmax = np.max([1.5,alpha_xmax])\n    alpha_ymin = np.min([a[1][0] for a in alphas])\n    alpha_ymax = np.max([a[1][0] for a in alphas])\n    alpha_ygap = (alpha_ymax - alpha_ymin)*0.15\n    alpha_ymin -= alpha_ygap\n    alpha_ymin = np.min([-0.5,alpha_ymin])\n    alpha_ymax += alpha_ygap\n    alpha_ymax = np.max([1.5,alpha_ymax])\n \n    # animate\n    print ('starting animation rendering...')\n    def animate(k):\n        # clear the panel\n        ax1.cla()\n        ax2.cla()\n         \n        # print rednering update\n        if np.mod(k+1,25) == 0:\n            print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n        if k == num_frames - 1:\n            print ('animation rendering complete!')\n            time.sleep(1.5)\n            clear_output()  \n         \n        ### take pt of grid and estimate with inputs ###        \n        # scatter every point up to k\n        for i in range(k+1):\n            # plot original point\n            pt = pts[i,:]\n            ax1.scatter(pt[0],pt[1],s = 60, c = 'k',edgecolor = 'w',linewidth = 1)\n             \n            # plot transformed plot\n            pt = alphas[i]\n            ax2.scatter(pt[0],pt[1],s = 60, c = 'k',edgecolor = 'w',linewidth = 1)\n \n        # plot original vectors\n        vector_draw(vec1copy.flatten(),ax1)\n        vector_draw(vec2copy.flatten(),ax1)\n \n        # send axis to vector adder for plotting\n        alpha = alphas[k]\n        vec1 = np.asarray([alpha[0]*vec1copy[0],alpha[0]*vec1copy[1]]).flatten()\n        vec2 = np.asarray([alpha[1]*vec2copy[0],alpha[1]*vec2copy[1]]).flatten()\n        vector_add_plot(vec1,vec2,ax1)\n         \n        # now the transformed versions\n        vec1 = np.array([1,0]).flatten()\n        vec2 = np.array([0,1]).flatten()\n        vector_draw(vec1.flatten(),ax2)\n        vector_draw(vec2.flatten(),ax2)\n        vec1 = np.array([alpha[0][0],0]).flatten()\n        vec2 = np.array([0,alpha[1][0]]).flatten()\n        vector_add_plot(vec1,vec2,ax2)\n  \n        # place text signifying weight values\n        title = r'$w_1 = ' + str(round(alpha[0][0],3)) + ',\\,\\,\\,\\,\\,' + 'w_2 = ' + str(round(alpha[1][0],3)) +   '$'\n        #props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n        #ax1.text(0.05, 0.95, title,  fontsize=20,transform=ax1.transAxes, verticalalignment='top')\n        ax1.set_title(title,fontsize = 20)\n            \n        # plot x and y axes, and clean up\n        ax1.grid(True, which='both')\n        ax1.axhline(y=0, color='k', linewidth=1.5,zorder = 1)\n        ax1.axvline(x=0, color='k', linewidth=1,zorder = 1)\n        ax1.set_xlim([-6,6])\n        ax1.set_ylim([-6,6])\n        ax1.grid('off')\n        ax1.set_xlabel(r'$x_1$',fontsize = 24)\n        ax1.set_ylabel(r'$x_2$',fontsize = 24,rotation = 0)\n \n        ax2.grid(True, which='both')\n        ax2.axhline(y=0, color='k', linewidth=1.5,zorder = 1)\n        ax2.axvline(x=0, color='k', linewidth=1,zorder = 1)\n        ax2.set_xlim([alpha_xmin,alpha_xmax])\n        ax2.set_ylim([alpha_ymin,alpha_ymax])\n        ax2.grid('off')\n        ax2.set_xlabel(r'$c_1$',fontsize = 24)\n        ax2.set_ylabel(r'$c_2$',fontsize = 24,rotation = 0)\n         \n        # set tick label fonts\n        for tick in ax1.xaxis.get_major_ticks():\n            tick.label.set_fontsize(20) \n        for tick in ax1.yaxis.get_major_ticks():\n            tick.label.set_fontsize(20) \n             \n        for tick in ax2.xaxis.get_major_ticks():\n            tick.label.set_fontsize(20) \n        for tick in ax2.yaxis.get_major_ticks():\n            tick.label.set_fontsize(20) \n         \n        # return artist\n        return artist,\n     \n    anim = animation.FuncAnimation(fig, animate,frames=num_frames, interval=num_frames, blit=True)\n         \n    # produce animation and save\n    fps = 50\n    if 'fps' in kwargs:\n        fps = kwargs['fps']\n    anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n    clear_output()\n\n# func,\ndef sphereing_visualizer(pts,pcs,eigs):\n    # renderer    \n    fig = plt.figure(figsize = (10,5))\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 3) \n    ax1 = plt.subplot(gs[0],aspect = 'equal');\n    ax2 = plt.subplot(gs[1],aspect = 'equal'); \n    ax3 = plt.subplot(gs[2],aspect = 'equal'); \n             \n    # swing through points and compute coeffecients over input basis\n    W = np.linalg.solve(pcs,pts)\n    #ars = np.diag(eigs)\n    #pcs = np.dot(pcs,ars)\n    \n    # sphere the results\n    eigs = np.array([1/a**(0.5) for a in eigs])\n    eigs = np.diag(eigs)\n    sphere_pts = np.dot(eigs,W)\n    ars2 = np.eye(2)\n    ars = ars2\n        \n    # loop over panels and plot each \n    c = 1\n    for ax,pt,ar in zip([ax1,ax2,ax3],[pts,W,sphere_pts],[pcs,ars,ars2]): \n        # set viewing limits for originals\n        xmin = np.min(pt[0,:])\n        xmax = np.max(pt[0,:])\n        xgap = (xmax - xmin)*0.15\n        xmin -= xgap\n        xmax += xgap\n        ymin = np.min(pt[1,:])\n        ymax = np.max(pt[1,:])\n        ygap = (ymax - ymin)*0.15\n        ymin -= ygap\n        ymax += ygap\n    \n        # scatter points\n        ax.scatter(pt[0,:],pt[1,:],s = 60, c = 'k',edgecolor = 'w',linewidth = 1,zorder = 2)\n   \n        # plot original vectors\n        vector_draw(ar[:,0].flatten(),ax,color = 'red',zorder = 3)\n        vector_draw(ar[:,1].flatten(),ax,color = 'red',zorder = 3)\n\n        # plot x and y axes, and clean up\n        ax.grid(True, which='both')\n        ax.axhline(y=0, color='k', linewidth=1.5,zorder = 1)\n        ax.axvline(x=0, color='k', linewidth=1,zorder = 1)\n        ax.set_xlim([xmin,xmax])\n        ax.set_ylim([ymin,ymax])\n        ax.grid('off')\n\n        # set tick label fonts\n        for tick in ax.xaxis.get_major_ticks():\n            tick.label.set_fontsize(12) \n        for tick in ax.yaxis.get_major_ticks():\n            tick.label.set_fontsize(12) \n        \n        # plot title\n        if c == 1:\n            ax.set_title('original space',fontsize = 22)\n            ax.set_xlabel(r'$x_1$',fontsize = 22)\n            ax.set_ylabel(r'$x_2$',fontsize = 22,rotation = 0,labelpad = 10)\n        if c == 2:\n            ax.set_title('PCA transformed space',fontsize = 22)\n            ax.set_xlabel(r'$v_1$',fontsize = 22)\n            ax.set_ylabel(r'$v_2$',fontsize = 22,rotation = 0,labelpad = 10)\n        if c == 3:\n            ax.set_title('Sphered data space',fontsize = 22)\n            ax.set_xlabel(r'$\\frac{1}{d_1}v_1$',fontsize = 22)\n            ax.set_ylabel(r'$\\frac{1}{d_2}v_2$',fontsize = 22,rotation = 0,labelpad = 10)\n        c+=1\n\n\n# func,\ndef perfect_visualize_transform_static(C,**kwargs):\n    vec1 = C[:,0]\n    vec2 = C[:,1]\n    \n    # size up vecs\n    vec1 = np.asarray(vec1)\n    vec2 = np.asarray(vec2)\n    vec1copy = copy.deepcopy(vec1)\n    vec1copy.shape = (len(vec1copy),1)\n    vec2copy = copy.deepcopy(vec2)\n    vec2copy.shape = (len(vec2copy),1)\n     \n    # renderer    \n    fig = plt.figure(figsize = (10,4))\n    artist = fig\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 2) \n    ax1 = plt.subplot(gs[0],aspect = 'equal');\n    ax2 = plt.subplot(gs[1],aspect = 'equal'); \n     \n    ### create grid of points ###\n    s = np.linspace(-5,5,10)\n    xx,yy = np.meshgrid(s,s)\n    xx.shape = (xx.size,1)\n    yy.shape = (yy.size,1)\n    X = np.concatenate((xx,yy),axis=1)\n    X = np.flipud(X)\n    \n    if 'X' in kwargs:\n        X = kwargs['X'].T\n             \n    # swing through points and compute coeffecients\n    alphas = []\n    for k in range(X.shape[0]):\n        vec3 = X[k,:]   \n        vec3.shape = (len(vec3),1)\n        A = np.concatenate((vec1copy,vec2copy),axis=1)\n        b = vec3\n        alpha = np.linalg.solve(A,b)\n        alphas.append(alpha)\n         \n    # set viewing limits for originals\n    xmin = np.min(X[:,0])\n    xmax = np.max(X[:,0])\n    xgap = (xmax - xmin)*0.15\n    xmin -= xgap\n    xmax += xgap\n    ymin = np.min(X[:,1])\n    ymax = np.max(X[:,1])\n    ygap = (ymax - ymin)*0.15\n    ymin -= ygap\n    ymax += ygap\n    \n    # set viewing limits for transformed space\n    alpha_xmin = np.min([a[0][0] for a in alphas])\n    alpha_xmax = np.max([a[0][0] for a in alphas])\n    alpha_xgap = (alpha_xmax - alpha_xmin)*0.15\n    alpha_xmin -= alpha_xgap\n    alpha_xmin = np.min([-0.5,alpha_xmin])\n    alpha_xmax += alpha_xgap\n    alpha_xmax = np.max([1.5,alpha_xmax])\n    alpha_ymin = np.min([a[1][0] for a in alphas])\n    alpha_ymax = np.max([a[1][0] for a in alphas])\n    alpha_ygap = (alpha_ymax - alpha_ymin)*0.15\n    alpha_ymin -= alpha_ygap\n    alpha_ymin = np.min([-0.5,alpha_ymin])\n    alpha_ymax += alpha_ygap\n    alpha_ymax = np.max([1.5,alpha_ymax])\n\n    ### take pt of grid and estimate with inputs ###        \n    # scatter every point up to k\n    for i in range(X.shape[0]):\n        # plot original point\n        pt = X[i,:]\n        ax1.scatter(pt[0],pt[1],s = 60, c = 'k',edgecolor = 'w',linewidth = 1)\n\n        # plot transformed plot\n        pt = alphas[i]\n        ax2.scatter(pt[0],pt[1],s = 60, c = 'k',edgecolor = 'w',linewidth = 1)\n \n    # plot original vectors\n    vector_draw(vec1copy.flatten(),ax1,color = 'red',zorder = 1)\n    vector_draw(vec2copy.flatten(),ax1,color = 'red',zorder = 1)\n\n    # send axis to vector adder for plotting         \n    vec1 = np.array([1,0]).flatten()\n    vec2 = np.array([0,1]).flatten()\n    vector_draw(vec1.flatten(),ax2,color = 'red',zorder = 1)\n    vector_draw(vec2.flatten(),ax2,color = 'red',zorder = 1)\n            \n    # plot x and y axes, and clean up\n    ax1.grid(True, which='both')\n    ax1.axhline(y=0, color='k', linewidth=1.5,zorder = 1)\n    ax1.axvline(x=0, color='k', linewidth=1,zorder = 1)\n    ax1.set_xlim([xmin,xmax])\n    ax1.set_ylim([ymin,ymax])\n    ax1.grid('off')\n    ax1.set_xlabel(r'$x_1$',fontsize = 22)\n    ax1.set_ylabel(r'$x_2$',fontsize = 22,rotation = 0,labelpad = 10)\n    ax1.set_title('original data',fontsize = 24)\n\n    ax2.grid(True, which='both')\n    ax2.axhline(y=0, color='k', linewidth=1.5,zorder = 1)\n    ax2.axvline(x=0, color='k', linewidth=1,zorder = 1)\n    ax2.set_xlim([alpha_xmin,alpha_xmax])\n    ax2.set_ylim([alpha_ymin,alpha_ymax])\n    ax2.grid('off')\n    ax2.set_xlabel(r'$c_1$',fontsize = 22)\n    ax2.set_ylabel(r'$c_2$',fontsize = 22,rotation = 0,labelpad = 10)\n    ax2.set_title('encoded data',fontsize = 24)\n         \n    # set tick label fonts\n    for tick in ax1.xaxis.get_major_ticks():\n        tick.label.set_fontsize(12) \n    for tick in ax1.yaxis.get_major_ticks():\n        tick.label.set_fontsize(12) \n             \n    for tick in ax2.xaxis.get_major_ticks():\n        tick.label.set_fontsize(12) \n    for tick in ax2.yaxis.get_major_ticks():\n        tick.label.set_fontsize(12) \n        \n      \n# func,\ndef imperfect_visualize(savepath,vec1,**kwargs):\n    # size up vecs\n    vec1 = np.asarray(vec1)\n    vec1copy = copy.deepcopy(vec1)\n    vec1copy.shape = (len(vec1copy),1)\n     \n    # renderer    \n    fig = plt.figure(figsize = (4,4))\n    artist = fig\n    \n    # create subplot with 3 panels, plot input function in center plot\n    gs = gridspec.GridSpec(1, 1) \n    #ax1 = plt.subplot(gs[0]); ax1.axis('off');\n    #ax3 = plt.subplot(gs[2]); ax3.axis('off');\n \n    # plot input function\n    ax2 = plt.subplot(gs[0]); # ax2.axis('equal');\n \n    ### create grid of points ###\n    s = np.linspace(-5,5,10)\n    xx,yy = np.meshgrid(s,s)\n    xx.shape = (xx.size,1)\n    yy.shape = (yy.size,1)\n    pts = np.concatenate((xx,yy),axis=1)\n    pts = np.flipud(pts)\n     \n    # decide on num_frames\n    num_frames = 10\n    if 'num_frames' in kwargs:\n        num_frames = kwargs['num_frames']\n        num_frames = min(num_frames,len(xx))\n     \n    # animate\n    print ('starting animation rendering...')\n     \n    def animate(k):\n        # clear the panel\n        ax2.cla()\n         \n        # print rednering update\n        if np.mod(k+1,25) == 0:\n            print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n        if k == num_frames - 1:\n            print ('animation rendering complete!')\n            time.sleep(1.5)\n            clear_output()  \n         \n        ### take pt of grid and estimate with inputs ###        \n        # scatter every point up to k\n        for i in range(k+1):\n            pt = pts[i,:]\n            ax2.scatter(pt[0],pt[1],s = 100, c = 'k',edgecolor = 'w',linewidth = 1)\n             \n        # get current point and solve for weights\n        vec3 = pts[k,:]   \n        vec3.shape = (len(vec3),1)\n        alpha = np.dot(vec1copy.T,vec3)/np.dot(vec1copy.T,vec1copy)  # /np.dot(A.T,A)\n \n        # plot original vectors\n        vector_draw(vec1copy.flatten(),ax2)\n \n        # send axis to vector adder for plotting\n        vec2 = np.asarray([alpha[0][0]*vec1copy[0],alpha[0][0]*vec1copy[1]]).flatten()\n        vector_scale(vec2,pts[k,:],ax2)\n        ax2.set_title(r'$w_1 = ' + str(round(alpha[0][0],3)) + '$',fontsize = 30)\n            \n        # plot x and y axes, and clean up\n        ax2.grid(True, which='both')\n        ax2.axhline(y=0, color='k', linewidth=1.5,zorder = 1)\n        ax2.axvline(x=0, color='k', linewidth=1,zorder = 1)\n \n        # set viewing limits\n        ax2.set_xlim([-6,6])\n        ax2.set_ylim([-6,6])\n         \n        # set tick label fonts\n        for tick in ax2.xaxis.get_major_ticks():\n            tick.label.set_fontsize(20) \n             \n        for tick in ax2.yaxis.get_major_ticks():\n            tick.label.set_fontsize(20) \n \n        # turn off grid\n        ax2.grid('off')\n        plt.axes().set_aspect('equal')\n         \n        # return artist\n        return artist,\n     \n    anim = animation.FuncAnimation(fig, animate,frames=num_frames, interval=num_frames, blit=True)\n         \n    # produce animation and save\n    fps = 50\n    if 'fps' in kwargs:\n        fps = kwargs['fps']\n    anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n    clear_output()\n\n# draw a vector\ndef vector_draw(vec,ax,**kwargs):\n    color = 'k'\n    if 'color' in kwargs:\n        color = kwargs['color']\n    zorder = 3 \n    if 'zorder' in kwargs:\n        zorder = kwargs['zorder']\n        \n    veclen = math.sqrt(vec[0]**2 + vec[1]**2)\n    head_length = 0.25\n    head_width = 0.25\n    vec_orig = copy.deepcopy(vec)\n    vec = (veclen - head_length)/veclen*vec\n    ax.arrow(0, 0, vec[0],vec[1], head_width=head_width, head_length=head_length, fc=color, ec=color,linewidth=3,zorder = zorder)\n      \ndef vector_scale(vec1,vec2,ax):\n    # plot each vector\n    head_length = 0.5\n    head_width = 0.5\n    veclen = math.sqrt(vec1[0]**2 + vec1[1]**2)\n    vec1_orig = copy.deepcopy(vec1)\n    vec1 = (veclen - head_length)/veclen*vec1\n    ax.arrow(0, 0, vec1[0],vec1[1], head_width=head_width, head_length=head_length, fc='r', ec='r',linewidth=2,zorder = 2)\n    ax.scatter(vec1[0],vec1[1],s = 100, c = 'b',edgecolor = 'w',linewidth = 1,zorder = 3)\n \n    # connect them\n    ax.plot([vec1[0],vec2[0]],[vec1[1],vec2[1]],linestyle= '--',c='b',zorder=3,linewidth = 1)\n \n     \n# simple plot of 2d vector addition / paralellagram law\ndef vector_add_plot(vec1,vec2,ax):   \n    # plot each vector\n    head_length = 0.5\n    head_width = 0.5\n    veclen = math.sqrt(vec1[0]**2 + vec1[1]**2)\n    vec1_orig = copy.deepcopy(vec1)\n    vec1 = (veclen - head_length)/veclen*vec1\n    veclen = math.sqrt(vec2[0]**2 + vec2[1]**2)\n    vec2_orig = copy.deepcopy(vec2)\n    vec2 = (veclen - head_length)/veclen*vec2\n    ax.arrow(0, 0, vec1[0],vec1[1], head_width=head_width, head_length=head_length, fc='b', ec='b',linewidth=2,zorder = 2)\n    ax.arrow(0, 0, vec2[0],vec2[1], head_width=head_width, head_length=head_length, fc='b', ec='b',linewidth=2,zorder = 2)\n     \n    # plot the sum of the two vectors\n    vec3 = vec1_orig + vec2_orig\n    vec3_orig = copy.deepcopy(vec3)\n    veclen = math.sqrt(vec3[0]**2 + vec3[1]**2)\n    vec3 = (veclen - math.sqrt(head_length))/veclen*vec3\n    ax.arrow(0, 0, vec3[0],vec3[1], head_width=head_width, head_length=head_length, fc='r', ec='r',linewidth=3,zorder=2)\n    \n    # connect them\n    ax.plot([vec1_orig[0],vec3_orig[0]],[vec1_orig[1],vec3_orig[1]],linestyle= '--',c='b',zorder=2,linewidth = 1)\n    ax.plot([vec2_orig[0],vec3_orig[0]],[vec2_orig[1],vec3_orig[1]],linestyle= '--',c='b',zorder=2,linewidth = 1)\n\n    """
mlrefined_libraries/multilayer_perceptron_library/basic_lib/__init__.py,0,b''
mlrefined_libraries/multilayer_perceptron_library/basic_lib/cost_functions.py,21,"b""import autograd.numpy as np\nfrom inspect import signature\n\nclass Setup:\n    def __init__(self,name,**kwargs):        \n        ### make cost function choice ###\n        # for regression\n        if name == 'least_squares':\n            self.cost = self.least_squares\n        if name == 'least_absolute_deviations':\n            self.cost = self.least_absolute_deviations\n            \n        # for two-class classification\n        if name == 'softmax':\n            self.cost = self.softmax\n        if name == 'perceptron':\n            self.cost = self.perceptron\n        if name == 'twoclass_counter':\n            self.cost = self.counting_cost\n            \n        # for multiclass classification\n        if name == 'multiclass_perceptron':\n            self.cost = self.multiclass_perceptron\n        if name == 'multiclass_softmax':\n            self.cost = self.multiclass_softmax\n        if name == 'multiclass_counter':\n            self.cost = self.multiclass_counting_cost\n            \n        # for autoencoder\n        if name == 'autoencoder':\n            self.feature_transforms = feature_transforms\n            self.feature_transforms_2 = kwargs['feature_transforms_2']\n            self.cost = self.autoencoder\n            \n    ### insert feature transformations to use ###\n    def define_feature_transform(self,feature_transforms):\n        # make copy of feature transformation\n        self.feature_transforms = feature_transforms\n        \n        # count parameter layers of input to feature transform\n        self.sig = signature(self.feature_transforms)\n            \n    ##### models functions #####\n    # compute linear combination of features\n    def model(self,x,w):   \n        # feature transformation - switch for dealing\n        # with feature transforms that either do or do\n        # not have internal parameters\n        f = 0\n        if len(self.sig.parameters) == 2:\n            f = self.feature_transforms(x,w[0])\n        else: \n            f = self.feature_transforms(x)    \n\n        # compute linear combination and return\n        # switch for dealing with feature transforms that either \n        # do or do not have internal parameters\n        a = 0\n        if len(self.sig.parameters) == 2:\n            a = w[1][0] + np.dot(f.T,w[1][1:])\n        else:\n            a = w[0] + np.dot(f.T,w[1:])\n        return a.T\n\n    ###### regression costs #######\n    # an implementation of the least squares cost function for linear regression\n    def least_squares(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n                \n        # compute cost over batch\n        cost = np.sum((self.model(x_p,w) - y_p)**2)\n        return cost/float(np.size(y_p))\n\n    # a compact least absolute deviations cost function\n    def least_absolute_deviations(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n\n        # compute cost over batch\n        cost = np.sum(np.abs(self.model(x_p,w) - y_p))\n        return cost/float(np.size(y_p))\n\n    ###### two-class classification costs #######\n    # the convex softmax cost function\n    def softmax(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.log(1 + np.exp(-y_p*self.model(x_p,w))))\n        return cost/float(np.size(y_p))\n\n    # the convex relu cost function\n    def relu(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.maximum(0,-y_p*self.model(x_p,w)))\n        return cost/float(np.size(y_p))\n\n    # the counting cost function\n    def counting_cost(self,w,x,y):\n        cost = np.sum((np.sign(self.model(x,w)) - y)**2)\n        return 0.25*cost \n\n    ###### multiclass classification costs #######\n    # multiclass perceptron\n    def multiclass_perceptron(self,w,x,y,iter):\n        # get subset of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n\n        # pre-compute predictions on all points\n        all_evals = self.model(x_p,w)\n\n        # compute maximum across data points\n        a =  np.max(all_evals,axis = 0)        \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n        cost = np.sum(a - b)\n\n        # return average\n        return cost/float(np.size(y_p))\n\n    # multiclass softmax\n    def multiclass_softmax(self,w,x,y,iter):     \n        # get subset of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # pre-compute predictions on all points\n        all_evals = self.model(x_p,w)\n\n        # compute softmax across data points\n        a = np.log(np.sum(np.exp(all_evals),axis = 0)) \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n        cost = np.sum(a - b)\n\n        # return average\n        return cost/float(np.size(y_p))\n\n    # multiclass misclassification cost function - aka the fusion rule\n    def multiclass_counting_cost(self,w,x,y):                \n        # pre-compute predictions on all points\n        all_evals = self.model(x,w)\n\n        # compute predictions of each input point\n        y_predict = (np.argmax(all_evals,axis = 0))[np.newaxis,:]\n\n        # compare predicted label to actual label\n        count = np.sum(np.abs(np.sign(y - y_predict)))\n\n        # return number of misclassifications\n        return count"""
mlrefined_libraries/multilayer_perceptron_library/basic_lib/history_plotters.py,8,"b""# import standard plotting and animation\nimport autograd.numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nclass Setup:\n    def __init__(self,train_cost_histories,train_accuracy_histories,val_cost_histories,val_accuracy_histories,start):\n        # plotting colors\n        self.colors = [[0,0.7,1],[1,0.8,0.5]]\n\n        # just plot cost history?\n        if len(train_accuracy_histories) == 0:\n            self.plot_cost_histories(train_cost_histories,val_cost_histories,start)\n        else: # plot cost and count histories\n            self.plot_cost_count_histories(train_cost_histories,train_accuracy_histories,val_cost_histories,val_accuracy_histories,start)\n \n    #### compare cost function histories ####\n    def plot_cost_histories(self,train_cost_histories,val_cost_histories,start):        \n        # initialize figure\n        fig = plt.figure(figsize = (10,3))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); \n\n        # run through input histories, plotting each beginning at 'start' iteration\n        for c in range(len(train_cost_histories)):\n            train_history = train_cost_histories[c]\n            val_history = val_cost_histories[c]\n\n            # plot train cost function history\n            ax.plot(np.arange(start,len(train_history),1),train_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[0],label = 'train cost') \n            \n            if np.size(val_history) > 0:\n                # plot test cost function history\n                ax.plot(np.arange(start,len(val_history),1),val_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[1],label = 'test cost') \n\n        # clean up panel / axes labels\n        xlabel = 'step $k$'\n        ylabel = r'$g\\left(\\mathbf{w}^k\\right)$'\n        ax.set_xlabel(xlabel,fontsize = 14)\n        ax.set_ylabel(ylabel,fontsize = 14,rotation = 0,labelpad = 25)\n        title = 'train vs validation cost histories'\n        ax.set_title(title,fontsize = 18)\n        \n        # plot legend\n        anchor = (1,1)\n        plt.legend(loc='upper right', bbox_to_anchor=anchor)\n        ax.set_xlim([start - 0.5,len(train_history) - 0.5]) \n        plt.show()\n        \n    #### compare multiple histories of cost and misclassification counts ####\n    def plot_cost_count_histories(self,train_cost_histories,train_accuracy_histories,val_cost_histories,val_accuracy_histories,start):        \n        # initialize figure\n        fig = plt.figure(figsize = (10,3))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 2) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]); \n\n        # run through input histories, plotting each beginning at 'start' iteration\n        for c in range(len(train_cost_histories)):\n            train_cost_history = train_cost_histories[c]\n            train_accuracy_history = train_accuracy_histories[c]\n\n            val_cost_history = val_cost_histories[c]\n            val_accuracy_history = val_accuracy_histories[c]\n            \n            # check if a label exists, if so add it to the plot\n            ax1.plot(np.arange(start,len(train_cost_history),1),train_cost_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[0]) \n  \n            ax2.plot(np.arange(start,len(train_accuracy_history),1),train_accuracy_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[0],label = 'train') \n    \n            if np.size(val_cost_history) > 0:\n                ax1.plot(np.arange(start,len(val_cost_history),1),val_cost_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[1]) \n\n                ax2.plot(np.arange(start,len(val_accuracy_history),1),val_accuracy_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[1],label = 'validation') \n            \n        # clean up panel\n        xlabel = 'step $k$'\n        ylabel = r'$g\\left(\\mathbf{w}^k\\right)$'\n        ax1.set_xlabel(xlabel,fontsize = 14)\n        ax1.set_ylabel(ylabel,fontsize = 14,rotation = 0,labelpad = 25)\n        title = 'cost history'\n        ax1.set_title(title,fontsize = 15)\n\n        ylabel = 'accuracy'\n        ax2.set_xlabel(xlabel,fontsize = 14)\n        ax2.set_ylabel(ylabel,fontsize = 14,rotation = 90,labelpad = 10)\n        title = 'accuracy history'\n        ax2.set_title(title,fontsize = 15)\n        \n        anchor = (1,1)\n        plt.legend(loc='lower right')# bbox_to_anchor=anchor)\n        ax1.set_xlim([start - 0.5,len(train_cost_history) - 0.5])\n        ax2.set_xlim([start - 0.5,len(train_cost_history) - 0.5])\n        ax2.set_ylim([0,1.05])\n        plt.show()       \n        """
mlrefined_libraries/multilayer_perceptron_library/basic_lib/multilayer_perceptron.py,11,"b""import autograd.numpy as np\n\nclass Setup:\n    def __init__(self,**kwargs):  \n        # set default values for layer sizes, activation, and scale\n        activation = 'relu'\n\n        # decide on these parameters via user input\n        if 'activation' in kwargs:\n            activation = kwargs['activation']\n\n        # switches\n        if activation == 'linear':\n            self.activation = lambda data: data\n        elif activation == 'tanh':\n            self.activation = lambda data: np.tanh(data)\n        elif activation == 'relu':\n            self.activation = lambda data: np.maximum(0,data)\n        elif activation == 'sinc':\n            self.activation = lambda data: np.sinc(data)\n        elif activation == 'sin':\n            self.activation = lambda data: np.sin(data)\n        elif activation == 'maxout':\n            self.activation = lambda data1,data2: np.maximum(data1,data2)\n                        \n        # get layer sizes\n        layer_sizes = kwargs['layer_sizes']\n        self.layer_sizes = layer_sizes\n        self.scale = 0.1\n        if 'scale' in kwargs:\n            self.scale = kwargs['scale']\n            \n        # assign initializer / feature transforms function\n        if activation == 'linear' or activation == 'tanh' or activation == 'relu' or activation == 'sinc' or activation == 'sin':\n            self.initializer = self.standard_initializer\n            self.feature_transforms = self.standard_feature_transforms\n        elif activation == 'maxout':\n            self.initializer = self.maxout_initializer\n            self.feature_transforms = self.maxout_feature_transforms\n\n    ####### initializers ######\n    # create initial weights for arbitrary feedforward network\n    def standard_initializer(self):\n        # container for entire weight tensor\n        weights = []\n\n        # loop over desired layer sizes and create appropriately sized initial \n        # weight matrix for each layer\n        for k in range(len(self.layer_sizes)-1):\n            # get layer sizes for current weight matrix\n            U_k = self.layer_sizes[k]\n            U_k_plus_1 = self.layer_sizes[k+1]\n\n            # make weight matrix\n            weight = self.scale*np.random.randn(U_k+1,U_k_plus_1)\n            weights.append(weight)\n\n        # re-express weights so that w_init[0] = omega_inner contains all \n        # internal weight matrices, and w_init = w contains weights of \n        # final linear combination in predict function\n        w_init = [weights[:-1],weights[-1]]\n\n        return w_init\n    \n    # create initial weights for arbitrary feedforward network\n    def maxout_initializer(self):\n        # container for entire weight tensor\n        weights = []\n\n        # loop over desired layer sizes and create appropriately sized initial \n        # weight matrix for each layer\n        for k in range(len(self.layer_sizes)-1):\n            # get layer sizes for current weight matrix\n            U_k = self.layer_sizes[k]\n            U_k_plus_1 = self.layer_sizes[k+1]\n\n            # make weight matrix\n            weight1 = self.scale*np.random.randn(U_k + 1,U_k_plus_1)\n\n            # add second matrix for inner weights\n            if k < len(self.layer_sizes)-2:\n                weight2 = self.scale*np.random.randn(U_k + 1,U_k_plus_1)\n                weights.append([weight1,weight2])\n            else:\n                weights.append(weight1)\n\n        # re-express weights so that w_init[0] = omega_inner contains all \n        # internal weight matrices, and w_init = w contains weights of \n        # final linear combination in predict function\n        w_init = [weights[:-1],weights[-1]]\n\n        return w_init\n\n    ####### feature transforms ######\n    # fully evaluate our network features using the tensor of weights in w\n    def standard_feature_transforms(self,a, w):    \n        # loop through each layer matrix\n        for W in w:\n            # compute inner product with current layer weights\n            a = W[0] + np.dot(a.T, W[1:])\n\n            # output of layer activation\n            a = self.activation(a).T\n        return a\n    \n    # fully evaluate our network features using the tensor of weights in w\n    def maxout_feature_transforms(self,a, w):    \n        # loop through each layer matrix\n        for W1,W2 in w:\n            # compute inner product with current layer weights\n            a1 = W1[0][:,np.newaxis] + np.dot(a.T, W1[1:]).T\n            a2 = W2[0][:,np.newaxis] + np.dot(a.T, W2[1:]).T\n\n            # output of layer activation\n            a = self.activation(a1,a2)\n        return a"""
mlrefined_libraries/multilayer_perceptron_library/basic_lib/multilayer_perceptron_batch_normalized.py,22,"b""import autograd.numpy as np\n\nclass Setup:\n    def __init__(self,**kwargs):        \n        # set default values for layer sizes, activation, and scale\n        activation = 'relu'\n\n        # decide on these parameters via user input\n        if 'activation' in kwargs:\n            activation = kwargs['activation']\n\n        # switches\n        if activation == 'linear':\n            self.activation = lambda data: data\n        elif activation == 'tanh':\n            self.activation = lambda data: np.tanh(data)\n        elif activation == 'relu':\n            self.activation = lambda data: np.maximum(0,data)\n        elif activation == 'sinc':\n            self.activation = lambda data: np.sinc(data)\n        elif activation == 'sin':\n            self.activation = lambda data: np.sin(data)\n        elif activation == 'maxout':\n            self.activation = lambda data1,data2: np.maximum(data1,data2)\n                        \n        # get layer sizes\n        layer_sizes = kwargs['layer_sizes']\n        self.layer_sizes = layer_sizes\n        self.scale = 0.1\n        if 'scale' in kwargs:\n            self.scale = kwargs['scale']\n            \n        # assign initializer / feature transforms function\n        if activation == 'linear' or activation == 'tanh' or activation == 'relu' or activation == 'sinc' or activation == 'sin':\n            self.initializer = self.standard_initializer\n            self.feature_transforms = self.standard_feature_transforms\n            self.testing_feature_transforms = self.standard_feature_transforms_testing\n        elif activation == 'maxout':\n            self.initializer = self.maxout_initializer\n            self.feature_transforms = self.maxout_feature_transforms\n            self.testing_feature_transforms = self.maxout_feature_transforms_testing\n\n    ####### initializers ######\n    # create initial weights for arbitrary feedforward network\n    def standard_initializer(self):\n        # container for entire weight tensor\n        weights = []\n\n        # loop over desired layer sizes and create appropriately sized initial \n        # weight matrix for each layer\n        for k in range(len(self.layer_sizes)-1):\n            # get layer sizes for current weight matrix\n            U_k = self.layer_sizes[k]\n            U_k_plus_1 = self.layer_sizes[k+1]\n\n            # make weight matrix\n            weight = self.scale*np.random.randn(U_k+1,U_k_plus_1)\n            weights.append(weight)\n\n        # re-express weights so that w_init[0] = omega_inner contains all \n        # internal weight matrices, and w_init = w contains weights of \n        # final linear combination in predict function\n        w_init = [weights[:-1],weights[-1]]\n\n        return w_init\n    \n    # create initial weights for arbitrary feedforward network\n    def maxout_initializer(self):\n        # container for entire weight tensor\n        weights = []\n\n        # loop over desired layer sizes and create appropriately sized initial \n        # weight matrix for each layer\n        for k in range(len(self.layer_sizes)-1):\n            # get layer sizes for current weight matrix\n            U_k = self.layer_sizes[k]\n            U_k_plus_1 = self.layer_sizes[k+1]\n\n            # make weight matrix\n            weight1 = self.scale*np.random.randn(U_k + 1,U_k_plus_1)\n\n            # add second matrix for inner weights\n            if k < len(self.layer_sizes)-2:\n                weight2 = self.scale*np.random.randn(U_k + 1,U_k_plus_1)\n                weights.append([weight1,weight2])\n            else:\n                weights.append(weight1)\n\n        # re-express weights so that w_init[0] = omega_inner contains all \n        # internal weight matrices, and w_init = w contains weights of \n        # final linear combination in predict function\n        w_init = [weights[:-1],weights[-1]]\n\n        return w_init\n\n    # standard normalization function \n    def standard_normalizer(self,x):\n        # compute the mean and standard deviation of the input\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n\n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(x_stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((x_stds.shape))\n            adjust[ind] = 1.0\n            x_stds += adjust\n\n        # create standard normalizer function\n        normalizer = lambda data: (data - x_means)/x_stds\n\n        # return normalizer \n        return normalizer\n    \n    ####### feature transforms ######\n    # a multilayer perceptron network, note the input w is a tensor of weights, with \n    # activation output normalization\n    def standard_feature_transforms(self,a, w):    \n        # loop through each layer matrix\n        self.normalizers = []\n        for W in w:\n            # compute inner product with current layer weights\n            a = W[0] + np.dot(a.T, W[1:])\n\n            # output of layer activation\n            a = self.activation(a).T\n\n            # NEW - perform standard normalization to the activation outputs\n            normalizer = self.standard_normalizer(a)\n            a = normalizer(a)\n            \n            # store normalizer for testing data\n            self.normalizers.append(normalizer)\n        return a\n    \n    # a multilayer perceptron network, note the input w is a tensor of weights, with \n    # activation output normalization\n    def maxout_feature_transforms(self,a, w):    \n        # loop through each layer matrix\n        self.normalizers = []\n        for W1,W2 in w:\n            # compute inner product with current layer weights\n            a1 = W1[0][:,np.newaxis] + np.dot(a.T, W1[1:])\n            a2 = W2[0][:,np.newaxis] + np.dot(a.T, W2[1:])\n\n            # output of layer activation\n            a = self.activation(a1,a2).T\n\n            # NEW - perform standard normalization to the activation outputs\n            normalizer = self.standard_normalizer(a)\n            a = normalizer(a)\n            \n            # store normalizer for testing data\n            self.normalizers.append(normalizer)\n        return a\n    \n    #### testing side feature transforms ####\n    # a copy of the batch normalized architecture that employs normalizers\n    # at each layer based on statistics from training data and user-chosen\n    # choice of weights w\n    def standard_feature_transforms_testing(self,a, w):    \n        # loop through each layer matrix\n        c=0\n        for W in w:\n            #  pad with ones (to compactly take care of bias) for next layer computation        \n            o = np.ones((1,np.shape(a)[1]))\n            a = np.vstack((o,a))\n\n            # compute linear combination of current layer units\n            a = np.dot(a.T, W).T\n\n            # pass through activation\n            a = self.activation(a)\n\n            # get normalizer for this layer tuned to training data\n            normalizer = self.normalizers[c]\n            a = normalizer(a)\n            c+=1\n        return a\n    \n    # a copy of the batch normalized architecture that employs normalizers\n    # at each layer based on statistics from training data and user-chosen\n    # choice of weights w\n    def maxout_feature_transforms_testing(self,a, w):    \n        # loop through each layer matrix\n        c=0\n        for W1,W2 in w:\n            #  pad with ones (to compactly take care of bias) for next layer computation        \n            o = np.ones((1,np.shape(a)[1]))\n            a = np.vstack((o,a))\n\n            # compute linear combination of current layer units\n            a1 = np.dot(a.T, W1).T\n            a2 = np.dot(a.T, W2).T\n\n            # pass through activation\n            a = self.activation(a1,a2)\n\n            # get normalizer for this layer tuned to training data\n            normalizer = self.normalizers[c]\n            a = normalizer(a)\n            c+=1\n        return a"""
mlrefined_libraries/multilayer_perceptron_library/basic_lib/normalizers.py,18,"b""import autograd.numpy as np\n\nclass Setup:\n    def __init__(self,x,name):\n        normalizer = 0\n        inverse_normalizer = 0\n        \n        # use standard normalizer\n        if name == 'standard':\n            self.normalizer, self.inverse_normalizer = self.standard_normalizer(x)\n            \n        # use PCA sphereing\n        elif name == 'PCA_sphere':\n            # create normalizer\n            self.normalizer, self.inverse_normalizer = self.PCA_sphereing(x)\n        \n        # use ZCA sphereing\n        elif name == 'ZCA_sphere':\n            self.normalizer, self.inverse_normalizer = self.ZCA_sphereing(x)\n            \n        else:\n            self.normalizer = lambda data: data\n            self.inverse_normalizer = lambda data: data\n            \n    # standard normalization function \n    def standard_normalizer(self,x):\n        # compute the mean and standard deviation of the input\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n        \n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(x_stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((x_stds.shape))\n            adjust[ind] = 1.0\n            x_stds += adjust\n\n        # create standard normalizer function\n        normalizer = lambda data: (data - x_means)/x_stds\n\n        # create inverse standard normalizer\n        inverse_normalizer = lambda data: data*x_stds + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer\n\n    # compute eigendecomposition of data covariance matrix for PCA transformation\n    def PCA(self,x,**kwargs):\n        # regularization parameter for numerical stability\n        lam = 10**(-7)\n        if 'lam' in kwargs:\n            lam = kwargs['lam']\n\n        # create the correlation matrix\n        P = float(x.shape[1])\n        Cov = 1/P*np.dot(x,x.T) + lam*np.eye(x.shape[0])\n\n        # use numpy function to compute eigenvalues / vectors of correlation matrix\n        d,V = np.linalg.eigh(Cov)\n        return d,V\n\n    # PCA-sphereing - use PCA to normalize input features\n    def PCA_sphereing(self,x,**kwargs):\n        # Step 1: mean-center the data\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_centered = x - x_means\n\n        # Step 2: compute pca transform on mean-centered data\n        d,V = self.PCA(x_centered,**kwargs)\n\n        # Step 3: divide off standard deviation of each (transformed) input, \n        # which are equal to the returned eigenvalues in 'd'.  \n        stds = (d[:,np.newaxis])**(0.5)\n        \n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((stds.shape))\n            adjust[ind] = 1.0\n            stds += adjust\n        \n        normalizer = lambda data: np.dot(V.T,data - x_means)/stds\n\n        # create inverse normalizer\n        inverse_normalizer = lambda data: np.dot(V,data*stds) + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer\n    \n    \n    # ZCA-sphereing - use ZCA to normalize input features\n    def ZCA_sphereing(self,x,**kwargs):\n        # Step 1: mean-center the data\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_centered = x - x_means\n\n        # Step 2: compute pca transform on mean-centered data\n        d,V = self.PCA(x_centered,**kwargs)\n\n        # Step 3: divide off standard deviation of each (transformed) input, \n        # which are equal to the returned eigenvalues in 'd'.  \n        stds = (d[:,np.newaxis])**(0.5)\n        \n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((stds.shape))\n            adjust[ind] = 1.0\n            stds += adjust\n             \n        normalizer = lambda data: np.dot(V, np.dot(V.T,data - x_means)/stds)\n\n        # create inverse normalizer\n        inverse_normalizer = lambda data: np.dot(V,np.dot(V.T,data)*stds) + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer"""
mlrefined_libraries/multilayer_perceptron_library/basic_lib/super_cost_functions.py,21,"b""import autograd.numpy as np\nfrom inspect import signature\n\nclass Setup:\n    def __init__(self,name,**kwargs):        \n        ### make cost function choice ###\n        # for regression\n        if name == 'least_squares':\n            self.cost = self.least_squares\n        if name == 'least_absolute_deviations':\n            self.cost = self.least_absolute_deviations\n            \n        # for two-class classification\n        if name == 'softmax':\n            self.cost = self.softmax\n        if name == 'perceptron':\n            self.cost = self.perceptron\n        if name == 'twoclass_counter':\n            self.cost = self.counting_cost\n            \n        # for multiclass classification\n        if name == 'multiclass_perceptron':\n            self.cost = self.multiclass_perceptron\n        if name == 'multiclass_softmax':\n            self.cost = self.multiclass_softmax\n        if name == 'multiclass_counter':\n            self.cost = self.multiclass_counting_cost\n            \n        # for autoencoder\n        if name == 'autoencoder':\n            self.feature_transforms = feature_transforms\n            self.feature_transforms_2 = kwargs['feature_transforms_2']\n            self.cost = self.autoencoder\n            \n    ### insert feature transformations to use ###\n    def define_feature_transform(self,feature_transforms):\n        # make copy of feature transformation\n        self.feature_transforms = feature_transforms\n        \n        # count parameter layers of input to feature transform\n        self.sig = signature(self.feature_transforms)\n            \n    ##### models functions #####\n    # compute linear combination of features\n    def model(self,x,w):   \n        # feature transformation - switch for dealing\n        # with feature transforms that either do or do\n        # not have internal parameters\n        f = 0\n        if len(self.sig.parameters) == 2:\n            f = self.feature_transforms(x,w[0])\n        else: \n            f = self.feature_transforms(x)    \n\n        # compute linear combination and return\n        # switch for dealing with feature transforms that either \n        # do or do not have internal parameters\n        a = 0\n        if len(self.sig.parameters) == 2:\n            a = w[1][0] + np.dot(f.T,w[1][1:])\n        else:\n            a = w[0] + np.dot(f.T,w[1:])\n        return a.T\n\n    ###### regression costs #######\n    # an implementation of the least squares cost function for linear regression\n    def least_squares(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n                \n        # compute cost over batch\n        cost = np.sum((self.model(x_p,w) - y_p)**2)\n        return cost/float(np.size(y_p))\n\n    # a compact least absolute deviations cost function\n    def least_absolute_deviations(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n\n        # compute cost over batch\n        cost = np.sum(np.abs(self.model(x_p,w) - y_p))\n        return cost/float(np.size(y_p))\n\n    ###### two-class classification costs #######\n    # the convex softmax cost function\n    def softmax(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.log(1 + np.exp(-y_p*self.model(x_p,w))))\n        return cost/float(np.size(y_p))\n\n    # the convex relu cost function\n    def relu(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.maximum(0,-y_p*self.model(x_p,w)))\n        return cost/float(np.size(y_p))\n\n    # the counting cost function\n    def counting_cost(self,w,x,y):\n        cost = np.sum((np.sign(self.model(x,w)) - y)**2)\n        return 0.25*cost \n\n    ###### multiclass classification costs #######\n    # multiclass perceptron\n    def multiclass_perceptron(self,w,x,y,iter):\n        # get subset of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n\n        # pre-compute predictions on all points\n        all_evals = self.model(x_p,w)\n\n        # compute maximum across data points\n        a =  np.max(all_evals,axis = 0)        \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n        cost = np.sum(a - b)\n\n        # return average\n        return cost/float(np.size(y_p))\n\n    # multiclass softmax\n    def multiclass_softmax(self,w,x,y,iter):     \n        # get subset of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # pre-compute predictions on all points\n        all_evals = self.model(x_p,w)\n\n        # compute softmax across data points\n        a = np.log(np.sum(np.exp(all_evals),axis = 0)) \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n        cost = np.sum(a - b)\n\n        # return average\n        return cost/float(np.size(y_p))\n\n    # multiclass misclassification cost function - aka the fusion rule\n    def multiclass_counting_cost(self,w,x,y):                \n        # pre-compute predictions on all points\n        all_evals = self.model(x,w)\n\n        # compute predictions of each input point\n        y_predict = (np.argmax(all_evals,axis = 0))[np.newaxis,:]\n\n        # compare predicted label to actual label\n        count = np.sum(np.abs(np.sign(y - y_predict)))\n\n        # return number of misclassifications\n        return count"""
mlrefined_libraries/multilayer_perceptron_library/basic_lib/super_optimizers.py,8,"b""import autograd.numpy as np\nfrom autograd import value_and_grad \nfrom autograd import hessian\nfrom autograd.misc.flatten import flatten_func\nfrom IPython.display import clear_output\nfrom timeit import default_timer as timer\nimport time\n\n# minibatch gradient descent\ndef gradient_descent(g,w,x_train,y_train,x_val,y_val,alpha,max_its,batch_size,**kwargs): \n    verbose = True\n    if 'verbose' in kwargs:\n        verbose = kwargs['verbose']\n    \n    # flatten the input function, create gradient based on flat function\n    g_flat, unflatten, w = flatten_func(g, w)\n    grad = value_and_grad(g_flat)\n\n    # record history\n    num_train = y_train.size\n    num_val = y_val.size\n    w_hist = [unflatten(w)]\n    train_hist = [g_flat(w,x_train,y_train,np.arange(num_train))]\n    val_hist = [g_flat(w,x_val,y_val,np.arange(num_val))]\n\n    # how many mini-batches equal the entire dataset?\n    num_batches = int(np.ceil(np.divide(num_train, batch_size)))\n\n    # over the line\n    for k in range(max_its):                   \n        # loop over each minibatch\n        start = timer()\n        train_cost = 0\n        for b in range(num_batches):\n            # collect indices of current mini-batch\n            batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_train))\n            \n            # plug in value into func and derivative\n            cost_eval,grad_eval = grad(w,x_train,y_train,batch_inds)\n            grad_eval.shape = np.shape(w)\n    \n            # take descent step with momentum\n            w = w - alpha*grad_eval\n\n        end = timer()\n        \n        # update training and validation cost\n        train_cost = g_flat(w,x_train,y_train,np.arange(num_train))\n        val_cost = g_flat(w,x_val,y_val,np.arange(num_val))\n\n        # record weight update, train and val costs\n        w_hist.append(unflatten(w))\n        train_hist.append(train_cost)\n        val_hist.append(val_cost)\n\n        if verbose == True:\n            print ('step ' + str(k+1) + ' done in ' + str(np.round(end - start,1)) + ' secs, train cost = ' + str(np.round(train_hist[-1][0],4)) + ', val cost = ' + str(np.round(val_hist[-1][0],4)))\n\n    if verbose == True:\n        print ('finished all ' + str(max_its) + ' steps')\n        #time.sleep(1.5)\n        #clear_output()\n    return w_hist,train_hist,val_hist"""
mlrefined_libraries/multilayer_perceptron_library/basic_lib/super_setup.py,6,"b""import autograd.numpy as np\nfrom . import super_optimizers \nfrom . import super_cost_functions\nfrom . import normalizers\nfrom . import multilayer_perceptron\nfrom . import multilayer_perceptron_batch_normalized\nfrom . import history_plotters\n\nclass Setup:\n    def __init__(self,x,y,**kwargs):\n        # link in data\n        self.x = x\n        self.y = y\n        \n        # make containers for all histories\n        self.weight_histories = []\n        self.train_cost_histories = []\n        self.train_accuracy_histories = []\n        self.val_cost_histories = []\n        self.val_accuracy_histories = []\n        self.train_costs = []\n        self.train_counts = []\n        self.val_costs = []\n        self.val_counts = []\n        \n    #### define preprocessing steps ####\n    def preprocessing_steps(self,**kwargs):        \n        ### produce / use data normalizer ###\n        normalizer_name = 'standard'\n        if 'normalizer_name' in kwargs:\n            normalizer_name = kwargs['normalizer_name']\n        self.normalizer_name = normalizer_name\n\n        # produce normalizer / inverse normalizer\n        s = normalizers.Setup(self.x,normalizer_name)\n        self.normalizer = s.normalizer\n        self.inverse_normalizer = s.inverse_normalizer\n        \n        # normalize input \n        self.x = self.normalizer(self.x)\n       \n    #### split data into training and validation sets ####\n    def make_train_val_split(self,train_portion):\n        # translate desired training portion into exact indecies\n        self.train_portion = train_portion\n        r = np.random.permutation(self.x.shape[1])\n        train_num = int(np.round(train_portion*len(r)))\n        self.train_inds = r[:train_num]\n        self.val_inds = r[train_num:]\n        \n        # define training and testing sets\n        self.x_train = self.x[:,self.train_inds]\n        self.x_val = self.x[:,self.val_inds]\n\n        self.y_train = self.y[:,self.train_inds]\n        self.y_val = self.y[:,self.val_inds]\n     \n    #### define cost function ####\n    def choose_cost(self,name,**kwargs):\n        # create training and testing cost functions\n        self.cost_object = super_cost_functions.Setup(name,**kwargs)\n\n        # if the cost function is a two-class classifier, build a counter too\n        if name == 'softmax' or name == 'perceptron':\n            self.count_object = super_cost_functions.Setup('twoclass_counter',**kwargs)\n                        \n        if name == 'multiclass_softmax' or name == 'multiclass_perceptron':\n            self.count_object = super_cost_functions.Setup('multiclass_counter',**kwargs)\n  \n        self.cost_name = name\n    \n    #### define feature transformation ####\n    def choose_features(self,**kwargs): \n        ### select from pre-made feature transforms ###\n        layer_sizes = [1]\n        if 'layer_sizes' in kwargs:\n            layer_sizes = kwargs['layer_sizes']\n        \n        # add input and output layer sizes\n        input_size = self.x.shape[0]\n        layer_sizes.insert(0, input_size)\n      \n        # add output size\n        if self.cost_name == 'least_squares' or self.cost_name == 'least_absolute_deviations':\n            layer_sizes.append(self.y.shape[0])\n        else:\n            num_labels = len(np.unique(self.y))\n            if num_labels == 2:\n                layer_sizes.append(1)\n            else:\n                layer_sizes.append(num_labels)\n        \n        # multilayer perceptron #\n        feature_name = 'multilayer_perceptron'\n        if 'name' in kwargs:\n            feature_name = kwargs['feature_name']\n           \n        if feature_name == 'multilayer_perceptron':\n            transformer = multilayer_perceptron.Setup(**kwargs)\n            self.feature_transforms = transformer.feature_transforms\n            self.multilayer_initializer = transformer.initializer\n            self.layer_sizes = transformer.layer_sizes\n            \n        if feature_name == 'multilayer_perceptron_batch_normalized':\n            transformer = multilayer_perceptron_batch_normalized.Setup(**kwargs)\n            self.feature_transforms = transformer.feature_transforms\n            self.multilayer_initializer = transformer.initializer\n            self.layer_sizes = transformer.layer_sizes\n            \n        self.feature_name = feature_name\n        \n        ### with feature transformation constructed, pass on to cost function ###\n        self.cost_object.define_feature_transform(self.feature_transforms)\n        self.cost = self.cost_object.cost\n        self.model = self.cost_object.model\n        \n        # if classification performed, inject feature transforms into counter as well\n        if self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n            self.count_object.define_feature_transform(self.feature_transforms)\n            self.counter = self.count_object.cost\n            \n    #### run optimization ####\n    def fit(self,**kwargs):\n        # basic parameters for gradient descent run (default algorithm)\n        max_its = 500; alpha_choice = 10**(-1);\n        \n        # set parameters by hand\n        if 'max_its' in kwargs:\n            self.max_its = kwargs['max_its']\n        if 'alpha_choice' in kwargs:\n            self.alpha_choice = kwargs['alpha_choice']\n        \n        # set initialization\n        self.w_init = self.multilayer_initializer()\n        \n        # batch size for gradient descent?\n        self.train_num = np.size(self.y_train)\n        self.val_num = np.size(self.y_val)\n        self.batch_size = np.size(self.y_train)\n        if 'batch_size' in kwargs:\n            self.batch_size = min(kwargs['batch_size'],self.batch_size)\n        \n        # verbose or not\n        verbose = True\n        if 'verbose' in kwargs:\n            verbose = kwargs['verbose']\n\n        # optimize\n        weight_history = []\n        cost_history = []\n        \n        # run gradient descent\n        weight_history,train_cost_history,val_cost_history = super_optimizers.gradient_descent(self.cost,self.w_init,self.x_train,self.y_train,self.x_val,self.y_val,self.alpha_choice,self.max_its,self.batch_size,verbose=verbose)\n                                                                                         \n        # store all new histories\n        self.weight_histories.append(weight_history)\n        self.train_cost_histories.append(train_cost_history)\n        self.val_cost_histories.append(val_cost_history)\n\n        # if classification produce count history\n        if self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n            train_accuracy_history = [1 - self.counter(v,self.x_train,self.y_train)/float(self.y_train.size) for v in weight_history]\n            val_accuracy_history = [1 - self.counter(v,self.x_val,self.y_val)/float(self.y_val.size) for v in weight_history]\n\n            # store count history\n            self.train_accuracy_histories.append(train_accuracy_history)\n            self.val_accuracy_histories.append(val_accuracy_history)\n \n    #### plot histories ###\n    def show_histories(self,**kwargs):\n        start = 0\n        if 'start' in kwargs:\n            start = kwargs['start']\n        if self.train_portion == 1:\n            self.val_cost_histories = [[] for s in range(len(self.val_cost_histories))]\n            self.val_accuracy_histories = [[] for s in range(len(self.val_accuracy_histories))]\n        history_plotters.Setup(self.train_cost_histories,self.train_accuracy_histories,self.val_cost_histories,self.val_accuracy_histories,start)\n        \n    #### for batch normalized multilayer architecture only - set normalizers to desired settings ####\n    def fix_normalizers(self,w):\n        ### re-set feature transformation ###        \n        # fix normalization at each layer by passing data and specific weight through network\n        self.feature_transforms(self.x,w);\n        \n        # re-assign feature transformation based on these settings\n        self.testing_feature_transforms = self.transformer.testing_feature_transforms\n        \n        ### re-assign cost function (and counter) based on fixed architecture ###\n        funcs = cost_functions.Setup(self.cost_name,self.x,self.y,self.testing_feature_transforms)\n        self.model = funcs.model"""
mlrefined_libraries/multilayer_perceptron_library/basic_lib/unsuper_cost_functions.py,3,"b""import autograd.numpy as np\n\nclass Setup:\n    def __init__(self,name,**kwargs):                    \n        # for autoencoder\n        if name == 'autoencoder':\n            self.cost = self.autoencoder\n            \n    ### insert feature transformations to use ###\n    def define_encoder_decoder(self,feats1,feats2):\n        self.feature_transforms = feats1\n        self.feature_transforms_2 = feats2\n   \n    ### for autoencoder ###\n    def encoder(self,x,w):    \n        # feature transformation \n        f = self.feature_transforms(x,w[0])\n\n        # compute linear combination and return\n        a = w[1][0] + np.dot(f.T,w[1][1:])\n        return a.T\n\n    def decoder(self,v,w):\n        # feature transformation \n        f = self.feature_transforms_2(v,w[0])\n\n        # compute linear combination and return\n        a = w[1][0] + np.dot(f.T,w[1][1:])\n        return a.T\n    \n    def autoencoder(self,w,x,iter):\n        x_p = x[:,iter]\n        \n        # encode input\n        a = self.encoder(x_p,w[0])\n        \n        # decode result\n        b = self.decoder(a,w[1])\n        \n        # compute Least Squares error\n        cost = np.sum((b - x_p)**2)\n        return cost/float(x_p.shape[1])"""
mlrefined_libraries/multilayer_perceptron_library/basic_lib/unsuper_optimizers.py,8,"b""import autograd.numpy as np\nfrom autograd import value_and_grad \nfrom autograd import hessian\nfrom autograd.misc.flatten import flatten_func\nfrom IPython.display import clear_output\nfrom timeit import default_timer as timer\nimport time\n\n# minibatch gradient descent\ndef gradient_descent(g,w,x_train,x_val,alpha,max_its,batch_size,**kwargs): \n    verbose = True\n    if 'verbose' in kwargs:\n        verbose = kwargs['verbose']\n    \n    # flatten the input function, create gradient based on flat function\n    g_flat, unflatten, w = flatten_func(g, w)\n    grad = value_and_grad(g_flat)\n\n    # record history\n    num_train = x_train.shape[1]\n    num_val = x_val.shape[1]\n    w_hist = [unflatten(w)]\n    train_hist = [g_flat(w,x_train,np.arange(num_train))]\n    val_hist = [g_flat(w,x_val,np.arange(num_val))]\n\n    # how many mini-batches equal the entire dataset?\n    num_batches = int(np.ceil(np.divide(num_train, batch_size)))\n\n    # over the line\n    for k in range(max_its):                   \n        # loop over each minibatch\n        start = timer()\n        train_cost = 0\n        for b in range(num_batches):\n            # collect indices of current mini-batch\n            batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_train))\n            \n            # plug in value into func and derivative\n            cost_eval,grad_eval = grad(w,x_train,batch_inds)\n            grad_eval.shape = np.shape(w)\n    \n            # take descent step with momentum\n            w = w - alpha*grad_eval\n\n        end = timer()\n        \n        # update training and validation cost\n        train_cost = g_flat(w,x_train,np.arange(num_train))\n        val_cost = g_flat(w,x_val,np.arange(num_val))\n\n        # record weight update, train and val costs\n        w_hist.append(unflatten(w))\n        train_hist.append(train_cost)\n        val_hist.append(val_cost)\n\n        if verbose == True:\n            print ('step ' + str(k+1) + ' done in ' + str(np.round(end - start,1)) + ' secs, train cost = ' + str(np.round(train_hist[-1][0],4)) + ', val cost = ' + str(np.round(val_hist[-1][0],4)))\n\n    if verbose == True:\n        print ('finished all ' + str(max_its) + ' steps')\n        #time.sleep(1.5)\n        #clear_output()\n    return w_hist,train_hist,val_hist"""
mlrefined_libraries/multilayer_perceptron_library/basic_lib/unsuper_setup.py,5,"b""import autograd.numpy as np\nfrom . import unsuper_optimizers \nfrom . import unsuper_cost_functions\nfrom . import normalizers\nfrom . import multilayer_perceptron\nfrom . import history_plotters\n\nclass Setup:\n    def __init__(self,X,**kwargs):\n        # link in data\n        self.x = X\n        \n        # make containers for all histories\n        self.weight_histories = []\n        self.train_cost_histories = []\n        self.train_accuracy_histories = []\n        self.val_cost_histories = []\n        self.val_accuracy_histories = []\n        self.train_costs = []\n        self.train_counts = []\n        self.val_costs = []\n        self.val_counts = []\n        \n    #### define preprocessing steps ####\n    def preprocessing_steps(self,**kwargs):        \n        ### produce / use data normalizer ###\n        normalizer_name = 'standard'\n        if 'normalizer_name' in kwargs:\n            normalizer_name = kwargs['normalizer_name']\n        self.normalizer_name = normalizer_name\n\n        # produce normalizer / inverse normalizer\n        s = normalizers.Setup(self.x,normalizer_name)\n        self.normalizer = s.normalizer\n        self.inverse_normalizer = s.inverse_normalizer\n        \n        # normalize input \n        self.x = self.normalizer(self.x)\n        \n    #### split data into training and validation sets ####\n    def make_train_val_split(self,train_portion):\n        # translate desired training portion into exact indecies\n        self.train_portion = train_portion\n        r = np.random.permutation(self.x.shape[1])\n        train_num = int(np.round(train_portion*len(r)))\n        self.train_inds = r[:train_num]\n        self.val_inds = r[train_num:]\n        \n        # define training and testing sets\n        self.x_train = self.x[:,self.train_inds]\n        self.x_val = self.x[:,self.val_inds]\n        \n    #### define encoder ####\n    def choose_encoder(self,**kwargs):         \n        feature_name = 'multilayer_perceptron'\n        if 'name' in kwargs:\n            feature_name = kwargs['feature_name']\n        \n        transformer = 0\n        if feature_name == 'multilayer_perceptron':\n            transformer = multilayer_perceptron.Setup(**kwargs)\n        elif feature_name == 'multilayer_perceptron_batch_normalized':\n            transformer = multilayer_perceptron_batch_normalized.Setup(**kwargs)\n            \n        self.feature_transforms = transformer.feature_transforms\n        self.initializer_1 = transformer.initializer\n     \n    # form decoder\n    def choose_decoder(self,**kwargs):         \n        feature_name = 'multilayer_perceptron'\n        if 'name' in kwargs:\n            feature_name = kwargs['feature_name']\n           \n        transformer = 0\n        if feature_name == 'multilayer_perceptron':\n            transformer = multilayer_perceptron.Setup(**kwargs)\n        elif feature_name == 'multilayer_perceptron_batch_normalized':\n            transformer = multilayer_perceptron_batch_normalized.Setup(**kwargs)\n        \n        self.feature_transforms_2 = transformer.feature_transforms\n        self.initializer_2 = transformer.initializer\n     \n    #### define cost function ####\n    def choose_cost(self,name,**kwargs):\n        # pick cost based on user input\n        self.cost_object = unsuper_cost_functions.Setup(name,**kwargs)\n                \n        ### with feature transformation constructed, pass on to cost function ###\n        self.cost_object.define_encoder_decoder(self.feature_transforms,self.feature_transforms_2)\n        self.cost = self.cost_object.cost\n        self.cost_name = name\n        self.encoder = self.cost_object.encoder\n        self.decoder = self.cost_object.decoder\n            \n    #### run optimization ####\n    def fit(self,**kwargs):\n        # basic parameters for gradient descent run (default algorithm)\n        max_its = 500; alpha_choice = 10**(-1);\n        self.w_init_1 = self.initializer_1()\n        self.w_init_2 = self.initializer_2()\n        self.w_init = [self.w_init_1,self.w_init_2]\n        \n        # set parameters by hand\n        if 'max_its' in kwargs:\n            self.max_its = kwargs['max_its']\n        if 'alpha_choice' in kwargs:\n            self.alpha_choice = kwargs['alpha_choice']\n        if 'w' in kwargs:\n            self.w_init = kwargs['w']\n\n        # batch size for gradient descent?\n        self.train_num = np.shape(self.x_train)[1]\n        self.val_num = np.shape(self.x_val)[1]\n        self.batch_size = np.shape(self.x_train)[1]\n        if 'batch_size' in kwargs:\n            self.batch_size = min(kwargs['batch_size'],self.batch_size)\n        \n        # verbose or not\n        verbose = True\n        if 'verbose' in kwargs:\n            verbose = kwargs['verbose']\n        \n        # run gradient descent\n        weight_history,train_cost_history,val_cost_history = unsuper_optimizers.gradient_descent(self.cost,self.w_init,self.x_train,self.x_val,self.alpha_choice,self.max_its,self.batch_size,verbose=verbose)\n        \n        # store all new histories\n        self.weight_histories.append(weight_history)\n        self.train_cost_histories.append(train_cost_history)\n        self.val_cost_histories.append(val_cost_history)\n        \n    #### plot histories ###\n    def show_histories(self,**kwargs):\n        start = 0\n        if 'start' in kwargs:\n            start = kwargs['start']\n        if self.train_portion == 1:\n            self.val_cost_histories = [[] for s in range(len(self.val_cost_histories))]\n            self.val_accuracy_histories = [[] for s in range(len(self.val_accuracy_histories))]\n        history_plotters.Setup(self.train_cost_histories,self.train_accuracy_histories,self.val_cost_histories,self.val_accuracy_histories,start)\n        \n    """
mlrefined_libraries/multilayer_perceptron_library/early_stop_lib/__init__.py,0,b''
mlrefined_libraries/multilayer_perceptron_library/early_stop_lib/cost_functions.py,30,"b""import autograd.numpy as np\nfrom inspect import signature\n\nclass Setup:\n    def __init__(self,name,x,y,feature_transforms,**kwargs):\n        # point to input/output for cost functions\n        self.x = x\n        self.y = y\n       \n        # make copy of feature transformation\n        self.feature_transforms = feature_transforms\n        \n        # count parameter layers of input to feature transform\n        self.sig = signature(self.feature_transforms)\n        \n        ### make cost function choice ###\n        # for regression\n        if name == 'least_squares':\n            self.cost = self.least_squares\n        if name == 'least_absolute_deviations':\n            self.cost = self.least_absolute_deviations\n            \n        # for two-class classification\n        if name == 'softmax':\n            self.cost = self.softmax\n        if name == 'perceptron':\n            self.cost = self.perceptron\n        if name == 'twoclass_counter':\n            self.cost = self.counting_cost\n            \n        # for multiclass classification\n        if name == 'multiclass_perceptron':\n            self.cost = self.multiclass_perceptron\n        if name == 'multiclass_softmax':\n            self.cost = self.multiclass_softmax\n        if name == 'multiclass_counter':\n            self.cost = self.multiclass_counting_cost\n            \n        # for autoencoder\n        if name == 'autoencoder':\n            self.feature_transforms = feature_transforms\n            self.feature_transforms_2 = kwargs['feature_transforms_2']\n            self.cost = self.autoencoder\n\n            \n    ###### cost functions #####\n    # compute linear combination of input point\n    def model(self,x,w):   \n        # feature transformation - switch for dealing\n        # with feature transforms that either do or do\n        # not have internal parameters\n        f = 0\n        if len(self.sig.parameters) == 2:\n            f = self.feature_transforms(x,w[0])\n        else: \n            f = self.feature_transforms(x)    \n\n        # tack a 1 onto the top of each input point all at once\n        o = np.ones((1,np.shape(f)[1]))\n        f = np.vstack((o,f))\n\n        # compute linear combination and return\n        # switch for dealing with feature transforms that either \n        # do or do not have internal parameters\n        a = 0\n        if len(self.sig.parameters) == 2:\n            a = np.dot(f.T,w[1])\n        else:\n            a = np.dot(f.T,w)\n        return a.T\n    \n    ###### regression costs #######\n    # an implementation of the least squares cost function for linear regression\n    def least_squares(self,w,iter):\n        # get batch of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n        \n        # compute cost\n        cost = np.sum((self.model(x_p,w) - y_p)**2)\n        return cost/float(np.size(x_p))\n\n    # a compact least absolute deviations cost function\n    def least_absolute_deviations(self,w,iter):\n        # get batch of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n        \n        # compute cost\n        cost = np.sum(np.abs(self.model(x_p,w) - y_p))\n        return cost/float(np.size(y_p))\n\n    ###### two-class classification costs #######\n    # the convex softmax cost function\n    def softmax(self,w,iter):\n        # get batch of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.log(1 + np.exp(-y_p*self.model(x_p,w))))\n        return cost/float(np.size(y_p))\n\n    # the convex relu cost function\n    def relu(self,w,iter):\n        # get batch of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.maximum(0,-y_p*self.model(x_p,w)))\n        return cost/float(np.size(y_p))\n    \n    # the counting cost function\n    def counting_cost(self,w):\n        cost = np.sum((np.sign(self.model(self.x,w)) - self.y)**2)\n        return 0.25*cost \n\n    ###### multiclass classification costs #######\n    # multiclass perceptron\n    def multiclass_perceptron(self,w,iter):\n        # get subset of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n\n        # pre-compute predictions on all points\n        all_evals = self.model(x_p,w)\n\n        # compute maximum across data points\n        a =  np.max(all_evals,axis = 0)        \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n        cost = np.sum(a - b)\n\n        # return average\n        return cost/float(np.size(y_p))\n\n    # multiclass softmax\n    def multiclass_softmax(self,w,iter):     \n        # get subset of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n        \n        # pre-compute predictions on all points\n        all_evals = self.model(x_p,w)\n\n        # compute softmax across data points\n        a = np.log(np.sum(np.exp(all_evals),axis = 0)) \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n        cost = np.sum(a - b)\n\n        # return average\n        return cost/float(np.size(y_p))\n\n    # multiclass misclassification cost function - aka the fusion rule\n    def multiclass_counting_cost(self,w):                \n        # pre-compute predictions on all points\n        all_evals = self.model(self.x,w)\n\n        # compute predictions of each input point\n        y_predict = (np.argmax(all_evals,axis = 0))[np.newaxis,:]\n\n        # compare predicted label to actual label\n        count = np.sum(np.abs(np.sign(self.y - y_predict)))\n\n        # return number of misclassifications\n        return count\n    \n    ### for autoencoder ###\n    def encoder(self,x,w):    \n        # feature transformation \n        f = self.feature_transforms(x,w[0])\n\n        # tack a 1 onto the top of each input point all at once\n        o = np.ones((1,np.shape(f)[1]))\n        f = np.vstack((o,f))\n\n        # compute linear combination and return\n        a = np.dot(f.T,w[1])\n        return a.T\n\n    def decoder(self,v,w):\n        # feature transformation \n        f = self.feature_transforms_2(v,w[0])\n\n        # tack a 1 onto the top of each input point all at once\n        o = np.ones((1,np.shape(f)[1]))\n        f = np.vstack((o,f))\n\n        # compute linear combination and return\n        a = np.dot(f.T,w[1])\n        return a.T\n    \n    def autoencoder(self,w):\n        # encode input\n        a = self.encoder(self.x,w[0])\n        \n        # decode result\n        b = self.decoder(a,w[1])\n        \n        # compute Least Squares error\n        cost = np.sum((b - self.x)**2)\n        return cost/float(self.x.shape[1])"""
mlrefined_libraries/multilayer_perceptron_library/early_stop_lib/early_stop_regression_animator.py,14,"b""# import standard plotting and animation\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom matplotlib.ticker import FormatStrFormatter\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import clear_output\nfrom matplotlib.ticker import MaxNLocator, FuncFormatter\n\n# import autograd functionality\nimport autograd.numpy as np\nimport math\nimport time\nfrom matplotlib import gridspec\nimport copy\nfrom matplotlib.ticker import FormatStrFormatter\nfrom inspect import signature\n\nclass Visualizer:\n    '''\n    Visualize cross validation performed on N = 2 dimensional input classification datasets\n    '''\n    #### initialize ####\n    def __init__(self,csvname):\n        # grab input\n        data = np.loadtxt(csvname,delimiter = ',')\n        self.x = data[:-1,:]\n        self.y = data[-1:,:] \n        \n        self.colors = ['salmon','cornflowerblue','lime','bisque','mediumaquamarine','b','m','g']\n\n    \n    #### animate multiple runs on single regression ####\n    def animate_trainval_earlystop(self,run,frames,**kwargs):\n        train_errors = run.train_cost_histories[0]\n        valid_errors = run.valid_cost_histories[0]\n        weight_history = run.weight_histories[0]\n        num_units = len(weight_history)\n\n        # select subset of runs\n        inds = np.arange(0,len(weight_history),int(len(weight_history)/float(frames)))\n        weight_history = [weight_history[v] for v in inds]\n        train_errors = [train_errors[v] for v in inds]\n        valid_errors = [valid_errors[v] for v in inds]\n       \n        # construct figure\n        fig = plt.figure(figsize = (6,6))\n        artist = fig\n\n        # create subplot with 4 panels, plot input function in center plot\n        gs = gridspec.GridSpec(2, 2)\n        ax = plt.subplot(gs[0]); \n        ax1 = plt.subplot(gs[2]); \n        ax2 = plt.subplot(gs[3]); \n        ax3 = plt.subplot(gs[1]); \n        \n        # start animation\n        num_frames = len(inds)        \n        print ('starting animation rendering...')\n        def animate(k):\n            print (k)\n            # clear panels\n            ax.cla()\n            ax1.cla()\n            ax2.cla()\n            ax3.cla()\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n            \n            #### plot training, testing, and full data ####            \n            # pluck out current weights \n            w_best = weight_history[k]\n            \n            # produce static img\n            self.draw_data(ax,w_best,run,train_valid = 'original')\n            self.draw_fit(ax,run,w_best,train_valid = 'train')\n            \n            self.draw_data(ax1,w_best,run,train_valid = 'train')\n            self.draw_fit(ax1,run,w_best,train_valid = 'train')\n            self.draw_data(ax2,w_best,run,train_valid = 'validate')\n            self.draw_fit(ax2,run,w_best,train_valid = 'validate')\n\n            #### plot training / validation errors ####\n            self.plot_train_valid_errors(ax3,k,train_errors,valid_errors,num_units)\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        \n        clear_output()   \n    \n    ##### draw boundary #####\n    def draw_fit(self,ax,run,w,train_valid):\n        ### create boundary data ###\n        # get visual boundary\n        xmin1 = np.min(copy.deepcopy(self.x))\n        xmax1 = np.max(copy.deepcopy(self.x))\n        xgap1 = (xmax1 - xmin1)*0.1\n        xmin1 -= xgap1\n        xmax1 += xgap1 \n        \n        ymin1 = np.min(copy.deepcopy(self.y))\n        ymax1 = np.max(copy.deepcopy(self.y))\n        ygap1 = (ymax1 - ymin1)*0.3\n        ymin1 -= ygap1\n        ymax1 += ygap1 \n        \n        # plot boundary for 2d plot\n        s = np.linspace(xmin1,xmax1,300)[np.newaxis,:]\n        \n        # plot total fit\n        cost = run.cost\n        model = run.model\n        normalizer = run.normalizer\n        t = model(normalizer(s),w)\n        \n        #### plot contour, color regions ####\n        ax.plot(s.flatten(),t.flatten(),c = 'magenta',linewidth = 2.5,zorder = 3)  \n        ax.set_xlim([xmin1,xmax1])\n        ax.set_ylim([ymin1,ymax1])\n        \n    ######## show N = 2 static image ########\n    # show coloring of entire space\n    def draw_data(self,ax,w_best,runner,train_valid):\n        cost = runner.cost\n        predict = runner.model\n        feat = runner.feature_transforms\n        normalizer = runner.normalizer\n        inverse_nornalizer = runner.inverse_normalizer\n      \n        # or just take last weights\n        self.w = w_best\n        \n        ### create boundary data ###\n        xmin1 = np.min(copy.deepcopy(self.x))\n        xmax1 = np.max(copy.deepcopy(self.x))\n        xgap1 = (xmax1 - xmin1)*0.05\n        xmin1 -= xgap1\n        xmax1 += xgap1\n\n        ### loop over two panels plotting each ###\n        # plot training points\n        if train_valid == 'train':\n            # reverse normalize data\n            x_train = inverse_nornalizer(runner.x_train).T\n            y_train = runner.y_train\n            \n            # plot data\n            ax.scatter(x_train,y_train,s = 45, color = [0,0.7,1], edgecolor = 'k',linewidth = 1,zorder = 3)\n            ax.set_title('training data',fontsize = 15)\n\n        if train_valid == 'validate':\n            # reverse normalize data\n            x_valid = inverse_nornalizer(runner.x_valid).T\n            y_valid = runner.y_valid\n        \n            # plot testing points\n            ax.scatter(x_valid,y_valid,s = 45, color = [1,0.8,0.5], edgecolor = 'k',linewidth = 1,zorder = 3)\n\n            ax.set_title('validation data',fontsize = 15)\n                \n        if train_valid == 'original':\n            # plot all points\n            ax.scatter(self.x,self.y,s = 55, color = 'k', edgecolor = 'w',linewidth = 1,zorder = 3)\n            ax.set_title('original data',fontsize = 15)\n\n        # cleanup panel\n        ax.set_xlabel(r'$x_1$',fontsize = 15)\n        ax.set_ylabel(r'$x_2$',fontsize = 15,rotation = 0,labelpad = 20)\n        ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n        ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n        \n    def plot_train_valid_errors(self,ax,k,train_errors,valid_errors,num_units):\n        num_elements = np.arange(len(train_errors))\n\n        ax.plot([v+1 for v in num_elements[:k+1]] ,train_errors[:k+1],color = [0,0.7,1],linewidth = 2.5,zorder = 1,label = 'training')\n        #ax.scatter([v+1  for v in num_elements[:k+1]] ,train_errors[:k+1],color = [0,0.7,1.0],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n\n        ax.plot([v+1  for v in num_elements[:k+1]] ,valid_errors[:k+1],color = [1,0.8,0.5],linewidth = 2.5,zorder = 1,label = 'validation')\n        #ax.scatter([v+1  for v in num_elements[:k+1]] ,valid_errors[:k+1],color= [1,0.8,0.5],s = 70,edgecolor = 'w',linewidth = 1.5,zorder = 3)\n        ax.set_title('errors',fontsize = 15)\n\n        # cleanup\n        ax.set_xlabel('step',fontsize = 12)\n\n        # cleanp panel                \n        num_iterations = len(train_errors)\n        minxc = 0.5\n        maxxc = len(num_elements) + 0.5\n        minc = min(min(copy.deepcopy(train_errors)),min(copy.deepcopy(valid_errors)))\n        maxc = max(max(copy.deepcopy(train_errors[:10])),max(copy.deepcopy(valid_errors[:10])))\n        gapc = (maxc - minc)*0.25\n        minc -= gapc\n        maxc += gapc\n        \n        ax.set_xlim([minxc,maxxc])\n        ax.set_ylim([minc,maxc])\n        \n        tics = np.arange(1,len(num_elements)+1 + len(num_elements)/float(5),len(num_elements)/float(5))\n        labels = np.arange(1,num_units+1 + num_units/float(5),num_units/float(5))\n        labels = [int(np.around(v,decimals=-1)) for v in labels]\n\n        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n        ax.set_xticks(tics)\n        ax.set_xticklabels(labels)\n\n\n        """
mlrefined_libraries/multilayer_perceptron_library/early_stop_lib/history_plotters.py,8,"b""# import standard plotting and animation\nimport autograd.numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nclass Setup:\n    def __init__(self,train_cost_histories,train_count_histories,valid_cost_histories,valid_count_histories,start):\n        # plotting colors\n        self.colors = [[0,0.7,1],[1,0.8,0.5]]\n\n        # just plot cost history?\n        if len(train_count_histories) == 0:\n            self.plot_cost_histories(train_cost_histories,valid_cost_histories,start)\n        else: # plot cost and count histories\n            self.plot_cost_count_histories(train_cost_histories,train_count_histories,valid_cost_histories,valid_count_histories,start)\n \n    #### compare cost function histories ####\n    def plot_cost_histories(self,train_cost_histories,valid_cost_histories,start):        \n        # initialize figure\n        fig = plt.figure(figsize = (10,3))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); \n\n        # run through input histories, plotting each beginning at 'start' iteration\n        for c in range(len(train_cost_histories)):\n            train_history = train_cost_histories[c]\n            \n            # plot train cost function history\n            ax.plot(np.arange(start,len(train_history),1),train_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[0],label = 'train cost') \n            \n            if np.size(valid_cost_histories) > 0:\n                val_history = valid_cost_histories[c]\n\n                # plot test cost function history\n                ax.plot(np.arange(start,len(val_history),1),val_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[1],label = 'test cost') \n\n        # clean up panel / axes labels\n        xlabel = 'step $k$'\n        ylabel = r'$g\\left(\\mathbf{w}^k\\right)$'\n        ax.set_xlabel(xlabel,fontsize = 14)\n        ax.set_ylabel(ylabel,fontsize = 14,rotation = 0,labelpad = 25)\n        title = 'train vs validation cost histories'\n        ax.set_title(title,fontsize = 18)\n        \n        # plot legend\n        anchor = (1,1)\n        plt.legend(loc='upper right', bbox_to_anchor=anchor)\n        ax.set_xlim([start - 0.5,len(train_history) - 0.5]) \n        plt.show()\n        \n    #### compare multiple histories of cost and misclassification counts ####\n    def plot_cost_count_histories(self,train_cost_histories,train_count_histories,valid_cost_histories,valid_count_histories,start):        \n        # initialize figure\n        fig = plt.figure(figsize = (10,3))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 2) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]); \n\n        # run through input histories, plotting each beginning at 'start' iteration\n        for c in range(len(train_cost_histories)):\n            train_cost_history = train_cost_histories[c]\n            train_count_history = train_count_histories[c]\n            \n            # check if a label exists, if so add it to the plot\n            ax1.plot(np.arange(start,len(train_cost_history),1),train_cost_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[0]) \n  \n            ax2.plot(np.arange(start,len(train_count_history),1),train_count_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[0],label = 'train') \n    \n           # ax2.plot(train_count_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[0],label = 'train') \n    \n            if np.size(valid_cost_histories) > 0:\n                valid_cost_history = valid_cost_histories[c]\n                valid_count_history = valid_count_histories[c]\n            \n                ax1.plot(np.arange(start,len(valid_cost_history),1),valid_cost_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[1]) \n\n                ax2.plot(np.arange(start,len(valid_count_history),1),valid_count_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[1],label = 'validation') \n            \n        # clean up panel\n        xlabel = 'step $k$'\n        ylabel = r'$g\\left(\\mathbf{w}^k\\right)$'\n        ax1.set_xlabel(xlabel,fontsize = 14)\n        ax1.set_ylabel(ylabel,fontsize = 14,rotation = 0,labelpad = 25)\n        title = 'cost history'\n        ax1.set_title(title,fontsize = 15)\n\n        ylabel = 'misclassification'\n        ax2.set_xlabel(xlabel,fontsize = 14)\n        ax2.set_ylabel(ylabel,fontsize = 14,rotation = 90,labelpad = 10)\n        title = 'misclassification history'\n        ax2.set_title(title,fontsize = 15)\n        \n        anchor = (1,1)\n        plt.legend(loc='upper right')# bbox_to_anchor=anchor)\n        ax1.set_xlim([start - 0.5,len(train_cost_history) - 0.5])\n        ax2.set_xlim([start - 0.5,len(train_cost_history) - 0.5])\n        #ax2.set_ylim([0,1.05])\n        plt.show()       \n        """
mlrefined_libraries/multilayer_perceptron_library/early_stop_lib/multilayer_perceptron.py,11,"b""import autograd.numpy as np\n\nclass Setup:\n    def __init__(self,**kwargs):        \n        # set default values for layer sizes, activation, and scale\n        activation = 'relu'\n\n        # decide on these parameters via user input\n        if 'activation' in kwargs:\n            activation = kwargs['activation']\n\n        # switches\n        if activation == 'linear':\n            self.activation = lambda data: data\n        elif activation == 'tanh':\n            self.activation = lambda data: np.tanh(data)\n        elif activation == 'relu':\n            self.activation = lambda data: np.maximum(0,data)\n        elif activation == 'sinc':\n            self.activation = lambda data: np.sinc(data)\n        elif activation == 'sin':\n            self.activation = lambda data: np.sin(data)\n        elif activation == 'maxout':\n            self.activation = lambda data1,data2: np.maximum(data1,data2)\n                        \n        # select layer sizes and scale\n        self.layer_sizes = kwargs['layer_sizes']\n        self.scale = 0.1\n        if 'scale' in kwargs:\n            self.scale = kwargs['scale']\n            \n        # assign initializer / feature transforms function\n        if activation == 'linear' or activation == 'tanh' or activation == 'relu' or activation == 'sinc' or activation == 'sin':\n            self.initializer = self.standard_initializer\n            self.feature_transforms = self.feature_transforms\n        elif activation == 'maxout':\n            self.initializer = self.maxout_initializer\n            self.feature_transforms = self.maxout_feature_transforms\n\n    ####### initializers ######\n    # create initial weights for arbitrary feedforward network\n    def standard_initializer(self):\n        # container for entire weight tensor\n        weights = []\n\n        # loop over desired layer sizes and create appropriately sized initial \n        # weight matrix for each layer\n        for k in range(len(self.layer_sizes)-1):\n            # get layer sizes for current weight matrix\n            U_k = self.layer_sizes[k]\n            U_k_plus_1 = self.layer_sizes[k+1]\n\n            # make weight matrix\n            weight = self.scale*np.random.randn(U_k+1,U_k_plus_1)\n            weights.append(weight)\n\n        # re-express weights so that w_init[0] = omega_inner contains all \n        # internal weight matrices, and w_init = w contains weights of \n        # final linear combination in predict function\n        w_init = [weights[:-1],weights[-1]]\n\n        return w_init\n    \n    # create initial weights for arbitrary feedforward network\n    def maxout_initializer(self):\n        # container for entire weight tensor\n        weights = []\n\n        # loop over desired layer sizes and create appropriately sized initial \n        # weight matrix for each layer\n        for k in range(len(self.layer_sizes)-1):\n            # get layer sizes for current weight matrix\n            U_k = self.layer_sizes[k]\n            U_k_plus_1 = self.layer_sizes[k+1]\n\n            # make weight matrix\n            weight1 = self.scale*np.random.randn(U_k + 1,U_k_plus_1)\n\n            # add second matrix for inner weights\n            if k < len(self.layer_sizes)-2:\n                weight2 = self.scale*np.random.randn(U_k + 1,U_k_plus_1)\n                weights.append([weight1,weight2])\n            else:\n                weights.append(weight1)\n\n        # re-express weights so that w_init[0] = omega_inner contains all \n        # internal weight matrices, and w_init = w contains weights of \n        # final linear combination in predict function\n        w_init = [weights[:-1],weights[-1]]\n\n        return w_init\n\n    ####### feature transforms ######\n    # a feature_transforms function for computing\n    # U_L L layer perceptron units efficiently\n    def feature_transforms(self,a, w):    \n        # loop through each layer matrix\n        for W in w:\n            # compute inner product with current layer weights\n            a = W[0] + np.dot(a.T, W[1:])\n\n            # output of layer activation\n            a = self.activation(a).T\n        return a\n        \n    # fully evaluate our network features using the tensor of weights in w\n    def maxout_feature_transforms(self,a, w):    \n        # loop through each layer matrix\n        for W1,W2 in w:\n            # compute inner product with current layer weights\n            a1 = W1[0] + np.dot(a.T, W1[1:])\n            a2 = W2[0] + np.dot(a.T, W2[1:])\n\n            # output of layer activation\n            a = self.activation(a1,a2)\n        return a"""
mlrefined_libraries/multilayer_perceptron_library/early_stop_lib/multilayer_perceptron_batch_normalized.py,18,"b""import autograd.numpy as np\n\nclass Setup:\n    def __init__(self,**kwargs):        \n        # set default values for layer sizes, activation, and scale\n        activation = 'relu'\n\n        # decide on these parameters via user input\n        if 'activation' in kwargs:\n            activation = kwargs['activation']\n\n        # switches\n        if activation == 'linear':\n            self.activation = lambda data: data\n        elif activation == 'tanh':\n            self.activation = lambda data: np.tanh(data)\n        elif activation == 'relu':\n            self.activation = lambda data: np.maximum(0,data)\n        elif activation == 'sinc':\n            self.activation = lambda data: np.sinc(data)\n        elif activation == 'sin':\n            self.activation = lambda data: np.sin(data)\n        elif activation == 'maxout':\n            self.activation = lambda data1,data2: np.maximum(data1,data2)\n                        \n        # select layer sizes and scale\n        self.layer_sizes = kwargs['layer_sizes']\n        self.scale = 0.1\n        if 'scale' in kwargs:\n            self.scale = kwargs['scale']\n            \n        # assign initializer / feature transforms function\n        if activation == 'linear' or activation == 'tanh' or activation == 'relu' or activation == 'sinc' or activation == 'sin':\n            self.initializer = self.standard_initializer\n            self.feature_transforms = self.standard_feature_transforms\n            self.testing_feature_transforms = self.standard_feature_transforms_testing\n        elif activation == 'maxout':\n            self.initializer = self.maxout_initializer\n            self.feature_transforms = self.maxout_feature_transforms\n            self.testing_feature_transforms = self.maxout_feature_transforms_testing\n\n    ####### initializers ######\n    # create initial weights for arbitrary feedforward network\n    def standard_initializer(self):\n        # container for entire weight tensor\n        weights = []\n\n        # loop over desired layer sizes and create appropriately sized initial \n        # weight matrix for each layer\n        for k in range(len(self.layer_sizes)-1):\n            # get layer sizes for current weight matrix\n            U_k = self.layer_sizes[k]\n            U_k_plus_1 = self.layer_sizes[k+1]\n\n            # make weight matrix\n            weight = self.scale*np.random.randn(U_k+1,U_k_plus_1)\n            weights.append(weight)\n\n        # re-express weights so that w_init[0] = omega_inner contains all \n        # internal weight matrices, and w_init = w contains weights of \n        # final linear combination in predict function\n        w_init = [weights[:-1],weights[-1]]\n\n        return w_init\n    \n    # create initial weights for arbitrary feedforward network\n    def maxout_initializer(self):\n        # container for entire weight tensor\n        weights = []\n\n        # loop over desired layer sizes and create appropriately sized initial \n        # weight matrix for each layer\n        for k in range(len(self.layer_sizes)-1):\n            # get layer sizes for current weight matrix\n            U_k = self.layer_sizes[k]\n            U_k_plus_1 = self.layer_sizes[k+1]\n\n            # make weight matrix\n            weight1 = self.scale*np.random.randn(U_k + 1,U_k_plus_1)\n\n            # add second matrix for inner weights\n            if k < len(self.layer_sizes)-2:\n                weight2 = self.scale*np.random.randn(U_k + 1,U_k_plus_1)\n                weights.append([weight1,weight2])\n            else:\n                weights.append(weight1)\n\n        # re-express weights so that w_init[0] = omega_inner contains all \n        # internal weight matrices, and w_init = w contains weights of \n        # final linear combination in predict function\n        w_init = [weights[:-1],weights[-1]]\n\n        return w_init\n\n    # standard normalization function \n    def standard_normalizer(self,x):\n        # compute the mean and standard deviation of the input\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n\n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(x_stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((x_stds.shape))\n            adjust[ind] = 1.0\n            x_stds += adjust\n\n        # create standard normalizer function\n        normalizer = lambda data: (data - x_means)/x_stds\n\n        # return normalizer \n        return normalizer\n    \n    ####### feature transforms ######\n    # a multilayer perceptron network, note the input w is a tensor of weights, with \n    # activation output normalization\n    def standard_feature_transforms(self,a, w):    \n        # loop through each layer matrix\n        self.normalizers = []\n        for W in w:\n            # linear combo\n            a = W[0] + np.dot(a.T, W[1:])\n\n            # pass through activation\n            a = self.activation(a)\n\n            # NEW - perform standard normalization to the activation outputs\n            normalizer = self.standard_normalizer(a)\n            a = normalizer(a)\n            \n            # store normalizer for testing data\n            self.normalizers.append(normalizer)\n        return a\n    \n    # a multilayer perceptron network, note the input w is a tensor of weights, with \n    # activation output normalization\n    def maxout_feature_transforms(self,a, w):    \n        # loop through each layer matrix\n        self.normalizers = []\n        for W1,W2 in w:\n            # linear combos\n            a1 = W1[0] + np.dot(a.T, W1[1:])\n            a2 = W2[0] + np.dot(a.T, W2[1:])\n\n            # pass through activation\n            a = self.activation(a1,a2)\n\n            # NEW - perform standard normalization to the activation outputs\n            normalizer = self.standard_normalizer(a)\n            a = normalizer(a)\n            \n            # store normalizer for testing data\n            self.normalizers.append(normalizer)\n        return a\n    \n    #### testing side feature transforms ####\n    # a copy of the batch normalized architecture that employs normalizers\n    # at each layer based on statistics from training data and user-chosen\n    # choice of weights w\n    def standard_feature_transforms_testing(self,a, w):    \n        # loop through each layer matrix\n        c=0\n        for W in w:\n            # linear combo\n            a = W[0] + np.dot(a.T, W[1:])\n\n            # pass through activation\n            a = self.activation(a)\n\n            # get normalizer for this layer tuned to training data\n            normalizer = self.normalizers[c]\n            a = normalizer(a)\n            c+=1\n        return a\n    \n    # a copy of the batch normalized architecture that employs normalizers\n    # at each layer based on statistics from training data and user-chosen\n    # choice of weights w\n    def maxout_feature_transforms_testing(self,a, w):    \n        # loop through each layer matrix\n        c=0\n        for W1,W2 in w:\n            # linear combo\n            a1 = W1[0] + np.dot(a.T, W1[1:])\n            a2 = W2[0] + np.dot(a.T, W2[1:])\n\n            # pass through activation\n            a = self.activation(a1,a2)\n\n            # get normalizer for this layer tuned to training data\n            normalizer = self.normalizers[c]\n            a = normalizer(a)\n            c+=1\n        return a"""
mlrefined_libraries/multilayer_perceptron_library/early_stop_lib/normalizers.py,12,"b""import autograd.numpy as np\n\nclass Setup:\n    def __init__(self,x,name):\n        normalizer = 0\n        inverse_normalizer = 0\n        if name == 'standard':\n            # create normalizer\n            self.normalizer, self.inverse_normalizer = self.standard_normalizer(x)\n            \n        elif name == 'sphere':\n            # create normalizer\n            self.normalizer, self.inverse_normalizer = self.PCA_sphereing(x)\n        else:\n            self.normalizer = lambda data: data\n            self.inverse_normalizer = lambda data: data\n            \n    # standard normalization function \n    def standard_normalizer(self,x):\n        # compute the mean and standard deviation of the input\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n        \n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(x_stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((x_stds.shape))\n            adjust[ind] = 1.0\n            x_stds += adjust\n\n        # create standard normalizer function\n        normalizer = lambda data: (data - x_means)/x_stds\n\n        # create inverse standard normalizer\n        inverse_normalizer = lambda data: data*x_stds + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer\n\n    # compute eigendecomposition of data covariance matrix for PCA transformation\n    def PCA(self,x,**kwargs):\n        # regularization parameter for numerical stability\n        lam = 10**(-7)\n        if 'lam' in kwargs:\n            lam = kwargs['lam']\n\n        # create the correlation matrix\n        P = float(x.shape[1])\n        Cov = 1/P*np.dot(x,x.T) + lam*np.eye(x.shape[0])\n\n        # use numpy function to compute eigenvalues / vectors of correlation matrix\n        d,V = np.linalg.eigh(Cov)\n        return d,V\n\n    # PCA-sphereing - use PCA to normalize input features\n    def PCA_sphereing(self,x,**kwargs):\n        # Step 1: mean-center the data\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_centered = x - x_means\n\n        # Step 2: compute pca transform on mean-centered data\n        d,V = self.PCA(x_centered,**kwargs)\n\n        # Step 3: divide off standard deviation of each (transformed) input, \n        # which are equal to the returned eigenvalues in 'd'.  \n        stds = (d[:,np.newaxis])**(0.5)\n        \n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((stds.shape))\n            adjust[ind] = 1.0\n            stds += adjust\n        \n        normalizer = lambda data: np.dot(V.T,data - x_means)/stds\n\n        # create inverse normalizer\n        inverse_normalizer = lambda data: np.dot(V,data*stds) + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer"""
mlrefined_libraries/multilayer_perceptron_library/early_stop_lib/optimizers.py,16,"b""import autograd.numpy as np\nfrom autograd import value_and_grad \nfrom autograd import hessian\nfrom autograd.misc.flatten import flatten_func\n\n# minibatch gradient descent\ndef gradient_descent(g, alpha, max_its, w, num_pts, batch_size,**kwargs):   \n    # pluck out args\n    beta = 0\n    if 'beta' in kwargs:\n        beta = kwargs['beta']\n    normalize = False\n    if 'normalize' in kwargs:\n        normalize = kwargs['normalize']\n    \n    # flatten the input function, create gradient based on flat function\n    g_flat, unflatten, w = flatten_func(g, w)\n    grad = value_and_grad(g_flat)\n\n    # record history\n    w_hist = []\n    w_hist.append(unflatten(w))\n   \n    # how many mini-batches equal the entire dataset?\n    num_batches = int(np.ceil(np.divide(num_pts, batch_size)))\n    \n    # initialization for momentum direction\n    h = np.zeros((w.shape))\n    \n    # over the line\n    for k in range(max_its):   \n        # loop over each minibatch\n        for b in range(num_batches):\n            # collect indices of current mini-batch\n            batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_pts))\n\n            # plug in value into func and derivative\n            cost_eval,grad_eval = grad(w,batch_inds)\n            grad_eval.shape = np.shape(w)\n            \n            # normalize?\n            if normalize == True:\n                grad_eval = np.sign(grad_eval)\n                \n            # momentum step \n            # h = beta*h - (1 - beta)*grad_eval    \n            \n            # take descent step with momentum\n            w = w - alpha*grad_eval\n\n        # record weight update\n        w_hist.append(unflatten(w))\n\n    return w_hist\n\n\n# RMSprop advanced first order optimizer\ndef RMSprop(g, alpha, max_its, w, num_pts, batch_size,**kwargs):    \n    # rmsprop params\n    gamma=0.9\n    eps=10**-8\n    if 'gamma' in kwargs:\n        gamma = kwargs['gamma']\n    if 'eps' in kwargs:\n        eps = kwargs['eps']\n    \n    # flatten the input function, create gradient based on flat function\n    g_flat, unflatten, w = flatten_func(g, w)\n    grad = value_and_grad(g_flat)\n\n    # initialize average gradient\n    avg_sq_grad = np.ones(np.size(w))\n    \n    # record history\n    w_hist = [unflatten(w)]\n    \n    # how many mini-batches equal the entire dataset?\n    num_batches = int(np.ceil(np.divide(num_pts, batch_size)))\n\n    # over the line\n    for k in range(max_its):                   \n        # loop over each minibatch\n        for b in range(num_batches):\n            # collect indices of current mini-batch\n            batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_pts))\n            \n            # plug in value into func and derivative\n            cost_eval,grad_eval = grad(w,batch_inds)\n            grad_eval.shape = np.shape(w)\n            \n            # update exponential average of past gradients\n            avg_sq_grad = gamma*avg_sq_grad + (1 - gamma)*grad_eval**2 \n    \n            # take descent step \n            w = w - alpha*grad_eval / (avg_sq_grad**(0.5) + eps)\n\n        # record weight update, train and val costs\n        w_hist.append(unflatten(w))\n        \n    return w_hist\n\n\n# newtons method function - inputs: g (input function), max_its (maximum number of iterations), w (initialization)\ndef newtons_method(g, epsilon, max_its, w, num_pts, batch_size,**kwargs):      \n    \n    # flatten the input function, create gradient based on flat function\n    g_flat, unflatten, w = flatten_func(g, w)\n    grad = value_and_grad(g_flat)\n    hess = hessian(g_flat)\n\n    # record history\n    w_hist = []\n    w_hist.append(unflatten(w))\n   \n    # how many mini-batches equal the entire dataset?\n    num_batches = int(np.ceil(np.divide(num_pts, batch_size)))\n    \n    # over the line\n    for k in range(max_its):   \n        for b in range(num_batches):\n            # collect indices of current mini-batch\n            batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_pts))\n            \n            # plug in value into func and derivative\n            cost_eval,grad_eval = grad(w,batch_inds)\n            grad_eval.shape = np.shape(w)\n\n            # evaluate the hessian\n            hess_eval = hess(w,batch_inds)\n\n            # reshape for numpy linalg functionality\n            hess_eval.shape = (int((np.size(hess_eval))**(0.5)),int((np.size(hess_eval))**(0.5)))\n            hess_eval += epsilon*np.eye(np.size(w))\n\n            # solve second order system system for weight update\n            A = hess_eval \n            b = grad_eval\n            w = np.linalg.lstsq(A,np.dot(A,w) - b)[0]            \n        \n        # record weight update, train and val costs\n        w_hist.append(unflatten(w))\n        \n        if np.linalg.norm(w) > 100:\n            return w_hist\n\n    return w_hist\n"""
mlrefined_libraries/multilayer_perceptron_library/early_stop_lib/polys.py,5,"b""import autograd.numpy as np\nimport copy\nimport itertools\n\nclass Setup:\n    def __init__(self,x,y,**kwargs):    \n        # get desired degree\n        self.D = kwargs['degree']\n        self.N = x.shape[0]\n        \n        # all monomial terms degrees\n        degs = np.array(list(itertools.product(list(np.arange(self.D+1)), repeat = self.N)))\n        b = np.sum(degs,axis = 1)\n        ind = np.argwhere(b <= self.D)\n        ind = [v[0] for v in ind]\n        degs = degs[ind,:]     \n        self.degs = degs[1:,:]\n\n        # define initializer\n        self.num_classifiers = 1\n        if 'num_classifiers' in kwargs:\n            self.num_classifiers = kwargs['num_classifiers']\n        self.scale = 0.1\n        if 'scale' in kwargs:\n            self.scale = kwargs['scale']\n\n    # create initial weights for arbitrary feedforward network\n    def initializer(self):\n        w_init = self.scale*np.random.randn(len(self.degs)+1,self.num_classifiers);\n        return w_init\n    \n    # compute transformation on entire set of inputs\n    def feature_transforms(self,x): \n        x_transformed = np.array([np.prod(x**v[:,np.newaxis],axis = 0)[:,np.newaxis] for v in self.degs])[:,:,0]  \n        return x_transformed"""
mlrefined_libraries/multilayer_perceptron_library/early_stop_lib/superlearn_setup.py,6,"b""import autograd.numpy as np\nfrom . import optimizers \nfrom . import cost_functions\nfrom . import normalizers\nfrom . import multilayer_perceptron\nfrom . import multilayer_perceptron_batch_normalized\nfrom . import polys\nfrom . import history_plotters\n\nclass Setup:\n    def __init__(self,x,y,**kwargs):\n        # link in data\n        self.x = x\n        self.y = y\n        \n        # make containers for all histories\n        self.weight_histories = []\n        self.train_cost_histories = []\n        self.train_count_histories = []\n        self.valid_cost_histories = []\n        self.valid_count_histories = []\n        \n    #### define feature transformation ####\n    def choose_features(self,name,**kwargs): \n        ### select from pre-made feature transforms ###\n        # multilayer perceptron #\n        if name == 'multilayer_perceptron':\n            self.transformer = multilayer_perceptron.Setup(**kwargs)\n            self.feature_transforms = self.transformer.feature_transforms\n            self.initializer = self.transformer.initializer\n            self.layer_sizes = self.transformer.layer_sizes\n            \n        if name == 'multilayer_perceptron_batch_normalized':\n            self.transformer = multilayer_perceptron_batch_normalized.Setup(**kwargs)\n            self.feature_transforms = self.transformer.feature_transforms\n            self.initializer = self.transformer.initializer\n            self.layer_sizes = self.transformer.layer_sizes\n            \n        # polynomials #\n        if name == 'polys':\n            self.transformer = polys.Setup(self.x,self.y,**kwargs)\n            self.feature_transforms = self.transformer.feature_transforms\n            self.initializer = self.transformer.initializer\n            self.degs = self.transformer.D            \n            \n        self.feature_name = name\n\n    #### define normalizer ####\n    def choose_normalizer(self,name):\n        # produce normalizer / inverse normalizer\n        s = normalizers.Setup(self.x,name)\n        self.normalizer = s.normalizer\n        self.inverse_normalizer = s.inverse_normalizer\n        \n        # normalize input \n        self.x = self.normalizer(self.x)\n        self.normalizer_name = name\n       \n    #### split data into training and validation sets ####\n    def make_train_valid_split(self,train_portion):\n        # translate desired training portion into exact indecies\n        r = np.random.permutation(self.x.shape[1])\n        train_num = int(np.round(train_portion*len(r)))\n        self.train_inds = r[:train_num]\n        self.valid_inds = r[train_num:]\n        \n        # define training and validation sets\n        self.x_train = self.x[:,self.train_inds]\n        self.x_valid = self.x[:,self.valid_inds]\n        \n        self.y_train = self.y[:,self.train_inds]\n        self.y_valid = self.y[:,self.valid_inds]\n     \n    #### define cost function ####\n    def choose_cost(self,name,**kwargs):\n        # create cost on entire dataset\n        funcs = cost_functions.Setup(name,self.x,self.y,self.feature_transforms,**kwargs)\n        self.full_cost = funcs.cost\n        self.full_model = funcs.model\n\n        # create training and validation cost functions\n        funcs = cost_functions.Setup(name,self.x_train,self.y_train,self.feature_transforms,**kwargs)\n        self.cost = funcs.cost\n        self.model = funcs.model\n        \n        funcs = cost_functions.Setup(name,self.x_valid,self.y_valid,self.feature_transforms,**kwargs)\n        self.valid_cost = funcs.cost\n        self.valid_model = funcs.model\n\n        # if the cost function is a two-class classifier, build a counter too\n        if name == 'softmax' or name == 'perceptron':\n            funcs = cost_functions.Setup('twoclass_counter',self.x_train,self.y_train,self.feature_transforms,**kwargs)\n            self.counter = funcs.cost\n            \n            funcs = cost_functions.Setup('twoclass_counter',self.x_valid,self.y_valid,self.feature_transforms,**kwargs)\n            self.valid_counter = funcs.cost\n            \n        if name == 'multiclass_softmax' or name == 'multiclass_perceptron':\n            funcs = cost_functions.Setup('multiclass_counter',self.x_train,self.y_train,self.feature_transforms,**kwargs)\n            self.counter = funcs.cost\n            \n            funcs = cost_functions.Setup('multiclass_counter',self.x_valid,self.y_valid,self.feature_transforms,**kwargs)\n            self.valid_counter = funcs.cost\n            \n        self.cost_name = name\n            \n    #### run optimization ####\n    def fit(self,**kwargs):\n        # basic parameters for gradient descent run (default algorithm)\n        max_its = 500; alpha_choice = 10**(-1);\n        self.w_init = self.initializer()\n        optimizer = 'gradient_descent'\n        epsilon = 10**(-10)\n        \n        # set parameters by hand\n        if 'max_its' in kwargs:\n            self.max_its = kwargs['max_its']\n        if 'alpha_choice' in kwargs:\n            self.alpha_choice = kwargs['alpha_choice']\n        if 'optimizer' in kwargs:\n            optimizer = kwargs['optimizer']\n        if 'epsilon' in kwargs:\n            epsilon = kwargs['epsilon']\n        if 'init' in kwargs:\n            print ('here')\n            self.w_init = kwargs['init']\n            \n        # batch size for gradient descent?\n        self.num_pts = np.size(self.y_train)\n        self.batch_size = np.size(self.y_train)\n        if 'batch_size' in kwargs:\n            self.batch_size = kwargs['batch_size']\n\n        # optimize\n        weight_history = []\n        \n        # run gradient descent\n        if optimizer == 'gradient_descent':\n            weight_history = optimizers.gradient_descent(self.cost,self.alpha_choice,self.max_its,self.w_init,self.num_pts,self.batch_size)\n        \n        if optimizer == 'RMSprop':\n            weight_history = optimizers.RMSprop(self.cost,self.alpha_choice,self.max_its,self.w_init,self.num_pts,self.batch_size)\n \n        # run gradient descent\n        if optimizer == 'newtons_method':\n            epsilon = 10**(-10)\n            if 'epsilon' in kwargs:\n                epsilon = kwargs['epsilon']\n            weight_history = optimizers.newtons_method(self.cost,epsilon,self.max_its,self.w_init,self.num_pts,self.batch_size)\n        \n\n        # compute training history\n        train_cost_history = [self.cost(v,np.arange(np.size(self.y_train))) for v in weight_history]\n        \n        # store all new histories\n        self.weight_histories.append(weight_history)\n        self.train_cost_histories.append(train_cost_history)\n        \n        # compute validation history\n        if len(self.valid_inds) > 0:\n            valid_cost_history = [self.valid_cost(v,np.arange(np.size(self.y_valid))) for v in weight_history]\n            self.valid_cost_histories.append(valid_cost_history)\n        \n        # if classification produce count history\n        if self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n            train_count_history = [self.counter(v) for v in weight_history]\n            self.train_count_histories.append(train_count_history)\n            \n            if len(self.valid_inds) > 0:\n                valid_count_history = [self.valid_counter(v) for v in weight_history]\n                self.valid_count_histories.append(valid_count_history)\n \n    #### plot histories ###\n    def show_histories(self,**kwargs):\n        start = 0\n        if 'start' in kwargs:\n            start = kwargs['start']\n        history_plotters.Setup(self.train_cost_histories,self.train_count_histories,self.valid_cost_histories,self.valid_count_histories,start)\n        \n    #### for batch normalized multilayer architecture only - set normalizers to desired settings ####\n    def fix_normalizers(self,w):\n        ### re-set feature transformation ###        \n        # fix normalization at each layer by passing data and specific weight through network\n        self.feature_transforms(self.x,w);\n        \n        # re-assign feature transformation based on these settings\n        self.validation_feature_transforms = self.transformer.validation_feature_transforms\n        \n        ### re-assign cost function (and counter) based on fixed architecture ###\n        funcs = cost_functions.Setup(self.cost_name,self.x,self.y,self.validation_feature_transforms)\n        self.model = funcs.model\n    """
mlrefined_libraries/multilayer_perceptron_library/library_v1/__init__.py,0,b''
mlrefined_libraries/multilayer_perceptron_library/library_v1/cost_functions.py,30,"b""import autograd.numpy as np\nfrom inspect import signature\n\nclass Setup:\n    def __init__(self,name,x,y,feature_transforms,**kwargs):\n        # point to input/output for cost functions\n        self.x = x\n        self.y = y\n       \n        # make copy of feature transformation\n        self.feature_transforms = feature_transforms\n        \n        # count parameter layers of input to feature transform\n        self.sig = signature(self.feature_transforms)\n        \n        ### make cost function choice ###\n        # for regression\n        if name == 'least_squares':\n            self.cost = self.least_squares\n        if name == 'least_absolute_deviations':\n            self.cost = self.least_absolute_deviations\n            \n        # for two-class classification\n        if name == 'softmax':\n            self.cost = self.softmax\n        if name == 'perceptron':\n            self.cost = self.perceptron\n        if name == 'twoclass_counter':\n            self.cost = self.counting_cost\n            \n        # for multiclass classification\n        if name == 'multiclass_perceptron':\n            self.cost = self.multiclass_perceptron\n        if name == 'multiclass_softmax':\n            self.cost = self.multiclass_softmax\n        if name == 'multiclass_counter':\n            self.cost = self.multiclass_counting_cost\n            \n        # for autoencoder\n        if name == 'autoencoder':\n            self.feature_transforms = feature_transforms\n            self.feature_transforms_2 = kwargs['feature_transforms_2']\n            self.cost = self.autoencoder\n\n            \n    ###### cost functions #####\n    # compute linear combination of input point\n    def model(self,x,w):   \n        # feature transformation - switch for dealing\n        # with feature transforms that either do or do\n        # not have internal parameters\n        f = 0\n        if len(self.sig.parameters) == 2:\n            f = self.feature_transforms(x,w[0])\n        else: \n            f = self.feature_transforms(x)    \n\n        # tack a 1 onto the top of each input point all at once\n        o = np.ones((1,np.shape(f)[1]))\n        f = np.vstack((o,f))\n\n        # compute linear combination and return\n        # switch for dealing with feature transforms that either \n        # do or do not have internal parameters\n        a = 0\n        if len(self.sig.parameters) == 2:\n            a = np.dot(f.T,w[1])\n        else:\n            a = np.dot(f.T,w)\n        return a.T\n    \n    ###### regression costs #######\n    # an implementation of the least squares cost function for linear regression\n    def least_squares(self,w,iter):\n        # get batch of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum((self.model(x_p,w) - y_p)**2)\n        return cost/float(np.size(y_p))\n\n    # a compact least absolute deviations cost function\n    def least_absolute_deviations(self,w,iter):\n        # get batch of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.abs(self.model(x_p,w) - y_p))\n        return cost/float(np.size(y_p))\n\n    ###### two-class classification costs #######\n    # the convex softmax cost function\n    def softmax(self,w,iter):\n        # get batch of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.log(1 + np.exp(-y_p*self.model(x_p,w))))\n        return cost/float(np.size(y_p))\n\n    # the convex relu cost function\n    def relu(self,w,iter):\n        # get batch of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.maximum(0,-y_p*self.model(x_p,w)))\n        return cost/float(np.size(y_p))\n\n    # the counting cost function\n    def counting_cost(self,w):\n        cost = np.sum((np.sign(self.model(self.x,w)) - self.y)**2)\n        return 0.25*cost \n\n    ###### multiclass classification costs #######\n    # multiclass perceptron\n    def multiclass_perceptron(self,w,iter):\n        # get subset of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n\n        # pre-compute predictions on all points\n        all_evals = self.model(x_p,w)\n\n        # compute maximum across data points\n        a =  np.max(all_evals,axis = 0)        \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n        cost = np.sum(a - b)\n\n        # return average\n        return cost/float(np.size(y_p))\n\n    # multiclass softmax\n    def multiclass_softmax(self,w,iter):     \n        # get subset of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n        \n        # pre-compute predictions on all points\n        all_evals = self.model(x_p,w)\n\n        # compute softmax across data points\n        a = np.log(np.sum(np.exp(all_evals),axis = 0)) \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n        cost = np.sum(a - b)\n\n        # return average\n        return cost/float(np.size(y_p))\n\n    # multiclass misclassification cost function - aka the fusion rule\n    def multiclass_counting_cost(self,w):                \n        # pre-compute predictions on all points\n        all_evals = self.model(self.x,w)\n\n        # compute predictions of each input point\n        y_predict = (np.argmax(all_evals,axis = 0))[np.newaxis,:]\n\n        # compare predicted label to actual label\n        count = np.sum(np.abs(np.sign(self.y - y_predict)))\n\n        # return number of misclassifications\n        return count\n    \n    ### for autoencoder ###\n    def encoder(self,x,w):    \n        # feature transformation \n        f = self.feature_transforms(x,w[0])\n\n        # tack a 1 onto the top of each input point all at once\n        o = np.ones((1,np.shape(f)[1]))\n        f = np.vstack((o,f))\n\n        # compute linear combination and return\n        a = np.dot(f.T,w[1])\n        return a.T\n\n    def decoder(self,v,w):\n        # feature transformation \n        f = self.feature_transforms_2(v,w[0])\n\n        # tack a 1 onto the top of each input point all at once\n        o = np.ones((1,np.shape(f)[1]))\n        f = np.vstack((o,f))\n\n        # compute linear combination and return\n        a = np.dot(f.T,w[1])\n        return a.T\n    \n    def autoencoder(self,w):\n        # encode input\n        a = self.encoder(self.x,w[0])\n        \n        # decode result\n        b = self.decoder(a,w[1])\n        \n        # compute Least Squares error\n        cost = np.sum((b - self.x)**2)\n        return cost/float(self.x.shape[1])"""
mlrefined_libraries/multilayer_perceptron_library/library_v1/history_plotters.py,3,"b""# import standard plotting and animation\nimport autograd.numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nclass Setup:\n    def __init__(self,cost_histories,count_histories,start,labels):\n        # just plot cost history?\n        if len(count_histories) == 0:\n            self.plot_cost_histories(cost_histories,start,labels)\n        else: # plot cost and count histories\n            self.plot_cost_count_histories(cost_histories,count_histories,start,labels)\n \n    #### compare cost function histories ####\n    def plot_cost_histories(self,cost_histories,start,labels):\n        # plotting colors\n        colors = ['k','magenta','springgreen','blueviolet','chocolate']\n        \n        # initialize figure\n        fig = plt.figure(figsize = (10,3))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); \n\n        # run through input histories, plotting each beginning at 'start' iteration\n        for c in range(len(cost_histories)):\n            history = cost_histories[c]\n            label = labels[c]\n                \n            # plot cost function history\n            ax.plot(np.arange(start,len(history),1),history[start:],linewidth = 3*(0.8)**(c),color = colors[c],label = label) \n\n        # clean up panel / axes labels\n        xlabel = 'step $k$'\n        ylabel = r'$g\\left(\\mathbf{w}^k\\right)$'\n        ax.set_xlabel(xlabel,fontsize = 14)\n        ax.set_ylabel(ylabel,fontsize = 14,rotation = 0,labelpad = 25)\n        title = 'cost history'\n        ax.set_title(title,fontsize = 18)\n        \n        # plot legend\n        anchor = (1,1)\n        plt.legend(loc='upper right', bbox_to_anchor=anchor)\n        ax.set_xlim([start - 0.5,len(history) - 0.5]) \n        plt.show()\n        \n    #### compare multiple histories of cost and misclassification counts ####\n    def plot_cost_count_histories(self,cost_histories,count_histories,start,labels):\n        # plotting colors\n        colors = ['k','magenta','springgreen','blueviolet','chocolate']\n        \n        # initialize figure\n        fig = plt.figure(figsize = (10,3))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 2) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]); \n\n        # run through input histories, plotting each beginning at 'start' iteration\n        for c in range(len(cost_histories)):\n            cost_history = cost_histories[c]\n            count_history = count_histories[c]\n            label = labels[c]\n\n            # check if a label exists, if so add it to the plot\n            ax1.plot(np.arange(start,len(cost_history),1),cost_history[start:],linewidth = 3*(0.8)**(c),color = colors[c],label = label) \n            \n            ax2.plot(np.arange(start,len(count_history),1),count_history[start:],linewidth = 3*(0.8)**(c),color = colors[c],label = label) \n\n        # clean up panel\n        xlabel = 'step $k$'\n        ylabel = r'$g\\left(\\mathbf{w}^k\\right)$'\n        ax1.set_xlabel(xlabel,fontsize = 14)\n        ax1.set_ylabel(ylabel,fontsize = 14,rotation = 0,labelpad = 25)\n        title = 'cost history'\n        ax1.set_title(title,fontsize = 18)\n\n        ylabel = 'num misclasses'\n        ax2.set_xlabel(xlabel,fontsize = 14)\n        ax2.set_ylabel(ylabel,fontsize = 14,rotation = 90,labelpad = 10)\n        title = 'misclassification history'\n        ax2.set_title(title,fontsize = 18)\n        \n        anchor = (1,1)\n        plt.legend(loc='upper right', bbox_to_anchor=anchor)\n        ax1.set_xlim([start - 0.5,len(cost_history) - 0.5])\n        ax2.set_xlim([start - 0.5,len(cost_history) - 0.5])\n        plt.show()       \n        """
mlrefined_libraries/multilayer_perceptron_library/library_v1/multilayer_perceptron.py,6,"b""import autograd.numpy as np\n\nclass Setup:\n    def __init__(self,**kwargs):        \n        # set default values for layer sizes, activation, and scale\n        activation = 'relu'\n\n        # decide on these parameters via user input\n        if 'activation' in kwargs:\n            activation = kwargs['activation']\n\n        # switches\n        if activation == 'linear':\n            self.activation = lambda data: data\n        elif activation == 'tanh':\n            self.activation = lambda data: np.tanh(data)\n        elif activation == 'relu':\n            self.activation = lambda data: np.maximum(0,data)\n        elif activation == 'sinc':\n            self.activation = lambda data: np.sinc(data)\n        elif activation == 'sin':\n            self.activation = lambda data: np.sin(data)\n        else: # user-defined activation\n            self.activation = kwargs['activation']\n                        \n        # select layer sizes and scale\n        N = 1; M = 1;\n        U = 10;\n        self.layer_sizes = [N,U,M]\n        self.scale = 0.1\n        if 'layer_sizes' in kwargs:\n            self.layer_sizes = kwargs['layer_sizes']\n        if 'scale' in kwargs:\n            self.scale = kwargs['scale']\n\n    # create initial weights for arbitrary feedforward network\n    def initializer(self):\n        # container for entire weight tensor\n        weights = []\n\n        # loop over desired layer sizes and create appropriately sized initial \n        # weight matrix for each layer\n        for k in range(len(self.layer_sizes)-1):\n            # get layer sizes for current weight matrix\n            U_k = self.layer_sizes[k]\n            U_k_plus_1 = self.layer_sizes[k+1]\n\n            # make weight matrix\n            weight = self.scale*np.random.randn(U_k+1,U_k_plus_1)\n            weights.append(weight)\n\n        # re-express weights so that w_init[0] = omega_inner contains all \n        # internal weight matrices, and w_init = w contains weights of \n        # final linear combination in predict function\n        w_init = [weights[:-1],weights[-1]]\n\n        return w_init\n\n    ####### feature transforms ######\n    # a feature_transforms function for computing\n    # U_L L layer perceptron units efficiently\n    def feature_transforms(self,a, w):    \n        # loop through each layer matrix\n        for W in w:\n            # compute inner product with current layer weights\n            a = W[0] + np.dot(a.T, W[1:])\n\n            # output of layer activation\n            a = self.activation(a).T\n        return a"""
mlrefined_libraries/multilayer_perceptron_library/library_v1/multilayer_perceptron_batch_normalized.py,10,"b""import autograd.numpy as np\n\nclass Setup:\n    def __init__(self,**kwargs):        \n        # set default values for layer sizes, activation, and scale\n        activation = 'relu'\n\n        # decide on these parameters via user input\n        if 'activation' in kwargs:\n            activation = kwargs['activation']\n\n        # switches\n        if activation == 'linear':\n            self.activation = lambda data: data\n        elif activation == 'tanh':\n            self.activation = lambda data: np.tanh(data)\n        elif activation == 'relu':\n            self.activation = lambda data: np.maximum(0,data)\n        elif activation == 'sinc':\n            self.activation = lambda data: np.sinc(data)\n        elif activation == 'sin':\n            self.activation = lambda data: np.sin(data)\n        else: # user-defined activation\n            self.activation = kwargs['activation']\n                        \n        # select layer sizes and scale\n        N = 1; M = 1;\n        U = 10;\n        self.layer_sizes = [N,U,M]\n        self.scale = 0.1\n        if 'layer_sizes' in kwargs:\n            self.layer_sizes = kwargs['layer_sizes']\n        if 'scale' in kwargs:\n            self.scale = kwargs['scale']\n\n    # create initial weights for arbitrary feedforward network\n    def initializer(self):\n        # container for entire weight tensor\n        weights = []\n\n        # loop over desired layer sizes and create appropriately sized initial \n        # weight matrix for each layer\n        for k in range(len(self.layer_sizes)-1):\n            # get layer sizes for current weight matrix\n            U_k = self.layer_sizes[k]\n            U_k_plus_1 = self.layer_sizes[k+1]\n\n            # make weight matrix\n            weight = self.scale*np.random.randn(U_k+1,U_k_plus_1)\n            weights.append(weight)\n\n        # re-express weights so that w_init[0] = omega_inner contains all \n        # internal weight matrices, and w_init = w contains weights of \n        # final linear combination in predict function\n        w_init = [weights[:-1],weights[-1]]\n\n        return w_init\n\n    # standard normalization function \n    def standard_normalizer(self,x):\n        # compute the mean and standard deviation of the input\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n\n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(x_stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((x_stds.shape))\n            adjust[ind] = 1.0\n            x_stds += adjust\n\n        # create standard normalizer function\n        normalizer = lambda data: (data - x_means)/x_stds\n\n        # return normalizer \n        return normalizer\n    \n\n    ####### feature transforms ######\n    # a feature_transforms function for computing\n    # U_L L layer perceptron units efficiently\n    # -- batch normalized\n    def feature_transforms(self,a, w):    \n        # loop through each layer matrix\n        for W in w:\n            # compute inner product with current layer weights\n            a = W[0] + np.dot(a.T, W[1:])\n\n            # output of layer activation\n            a = self.activation(a).T\n\n            # perform standard normalization to the activation outputs\n            normalizer = self.standard_normalizer(a)\n            a = normalizer(a)\n        return a"""
mlrefined_libraries/multilayer_perceptron_library/library_v1/normalizers.py,12,"b""import autograd.numpy as np\n\nclass Setup:\n    def __init__(self,x,name):\n        normalizer = 0\n        inverse_normalizer = 0\n        if name == 'standard':\n            # create normalizer\n            self.normalizer, self.inverse_normalizer = self.standard_normalizer(x)\n            \n        elif name == 'sphere':\n            # create normalizer\n            self.normalizer, self.inverse_normalizer = self.PCA_sphereing(x)\n        else:\n            self.normalizer = lambda data: data\n            self.inverse_normalizer = lambda data: data\n            \n    # standard normalization function \n    def standard_normalizer(self,x):\n        # compute the mean and standard deviation of the input\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n        \n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(x_stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((x_stds.shape))\n            adjust[ind] = 1.0\n            x_stds += adjust\n\n        # create standard normalizer function\n        normalizer = lambda data: (data - x_means)/x_stds\n\n        # create inverse standard normalizer\n        inverse_normalizer = lambda data: data*x_stds + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer\n\n    # compute eigendecomposition of data covariance matrix for PCA transformation\n    def PCA(self,x,**kwargs):\n        # regularization parameter for numerical stability\n        lam = 10**(-7)\n        if 'lam' in kwargs:\n            lam = kwargs['lam']\n\n        # create the correlation matrix\n        P = float(x.shape[1])\n        Cov = 1/P*np.dot(x,x.T) + lam*np.eye(x.shape[0])\n\n        # use numpy function to compute eigenvalues / vectors of correlation matrix\n        d,V = np.linalg.eigh(Cov)\n        return d,V\n\n    # PCA-sphereing - use PCA to normalize input features\n    def PCA_sphereing(self,x,**kwargs):\n        # Step 1: mean-center the data\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_centered = x - x_means\n\n        # Step 2: compute pca transform on mean-centered data\n        d,V = self.PCA(x_centered,**kwargs)\n\n        # Step 3: divide off standard deviation of each (transformed) input, \n        # which are equal to the returned eigenvalues in 'd'.  \n        stds = (d[:,np.newaxis])**(0.5)\n        \n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((stds.shape))\n            adjust[ind] = 1.0\n            stds += adjust\n        \n        normalizer = lambda data: np.dot(V.T,data - x_means)/stds\n\n        # create inverse normalizer\n        inverse_normalizer = lambda data: np.dot(V,data*stds) + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer"""
mlrefined_libraries/multilayer_perceptron_library/library_v1/optimizers.py,22,"b""import autograd.numpy as np\nfrom autograd import value_and_grad \nfrom autograd import hessian\nfrom autograd.misc.flatten import flatten_func\n\n# minibatch gradient descent\ndef RMSprop(g,alpha,max_its,w,num_pts,batch_size,**kwargs): \n    # rmsprop params\n    gamma=0.9\n    eps=10**-8\n    if 'gamma' in kwargs:\n        gamma = kwargs['gamma']\n    if 'eps' in kwargs:\n        eps = kwargs['eps']\n    \n    # flatten the input function, create gradient based on flat function\n    g_flat, unflatten, w = flatten_func(g, w)\n    grad = value_and_grad(g_flat)\n\n    # initialize average gradient\n    avg_sq_grad = np.ones(np.size(w))\n    \n    # record history\n    w_hist = [unflatten(w)]\n    train_hist = [g_flat(w,np.arange(num_pts))]\n    \n    # how many mini-batches equal the entire dataset?\n    num_batches = int(np.ceil(np.divide(num_pts, batch_size)))\n\n    # over the line\n    for k in range(max_its):                   \n        # loop over each minibatch\n        train_cost = 0\n        for b in range(num_batches):\n            # collect indices of current mini-batch\n            batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_pts))\n            \n            # plug in value into func and derivative\n            cost_eval,grad_eval = grad(w,batch_inds)\n            grad_eval.shape = np.shape(w)\n            \n            # update exponential average of past gradients\n            avg_sq_grad = gamma*avg_sq_grad + (1 - gamma)*grad_eval**2 \n    \n            # take descent step \n            w = w - alpha*grad_eval / (avg_sq_grad**(0.5) + eps)\n        \n        # update training and validation cost\n        train_cost = g_flat(w,np.arange(num_pts))\n\n        # record weight update, train and val costs\n        w_hist.append(unflatten(w))\n        train_hist.append(train_cost)\n        \n    return w_hist,train_hist\n\n# minibatch gradient descent\ndef gradient_descent(g, alpha, max_its, w, num_pts, batch_size,**kwargs):    \n    # flatten the input function, create gradient based on flat function\n    g_flat, unflatten, w = flatten_func(g, w)\n    grad = value_and_grad(g_flat)\n\n    # record history\n    w_hist = []\n    w_hist.append(unflatten(w))\n    cost_hist = [g_flat(w,np.arange(num_pts))]\n   \n    # how many mini-batches equal the entire dataset?\n    num_batches = int(np.ceil(np.divide(num_pts, batch_size)))\n    # over the line\n    for k in range(max_its):   \n        # loop over each minibatch\n        for b in range(num_batches):\n            # collect indices of current mini-batch\n            batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_pts))\n\n            # plug in value into func and derivative\n            cost_eval,grad_eval = grad(w,batch_inds)\n            grad_eval.shape = np.shape(w)\n            \n            # take descent step with momentum\n            w = w - alpha*grad_eval\n\n        # record weight update\n        w_hist.append(unflatten(w))\n        cost_hist.append(g_flat(w,np.arange(num_pts)))\n    return w_hist,cost_hist\n\n\n# newtons method function - inputs: g (input function), max_its (maximum number of iterations), w (initialization)\ndef newtons_method(g,max_its,w,num_pts,batch_size,**kwargs):\n    # flatten input funciton, in case it takes in matrices of weights\n    g_flat, unflatten, w = flatten_func(g, w)\n    \n    # compute the gradient / hessian functions of our input function -\n    gradient = value_and_grad(g_flat)\n    hess = hessian(g_flat)\n    \n    # set numericxal stability parameter / regularization parameter\n    epsilon = 10**(-7)\n    if 'epsilon' in kwargs:\n        epsilon = kwargs['epsilon']\n\n    # record history\n    w_hist = []\n    w_hist.append(unflatten(w))\n    cost_hist = [g_flat(w,np.arange(num_pts))]\n\n    # how many mini-batches equal the entire dataset?\n    num_batches = int(np.ceil(np.divide(num_pts, batch_size)))\n    \n    # over the line\n    for k in range(max_its):   \n        # loop over each minibatch\n        for b in range(num_batches):\n            # collect indices of current mini-batch\n            batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_pts))\n            \n            # evaluate the gradient, store current weights and cost function value\n            cost_eval,grad_eval = gradient(w,batch_inds)\n\n            # evaluate the hessian\n            hess_eval = hess(w,batch_inds)\n\n            # reshape for numpy linalg functionality\n            hess_eval.shape = (int((np.size(hess_eval))**(0.5)),int((np.size(hess_eval))**(0.5)))\n            \n            '''\n            # compute minimum eigenvalue of hessian matrix \n            eigs, vecs = np.linalg.eig(hess_eval)\n            smallest_eig = np.min(eigs)\n            adjust = 0\n            if smallest_eig < 0:\n                adjust = np.abs(smallest_eig)\n            '''\n\n            # solve second order system system for weight update\n            A = hess_eval + (epsilon)*np.eye(np.size(w))\n            b = grad_eval\n            w = np.linalg.lstsq(A,np.dot(A,w) - b)[0]\n\n            #w = w - np.dot(np.linalg.pinv(hess_eval + epsilon*np.eye(np.size(w))),grad_eval)\n            \n        # record weights after each epoch\n        w_hist.append(unflatten(w))\n        cost_hist.append(g_flat(w,np.arange(num_pts)))\n\n    return w_hist,cost_hist"""
mlrefined_libraries/multilayer_perceptron_library/library_v1/polys.py,5,"b""import autograd.numpy as np\nimport copy\nimport itertools\n\nclass Setup:\n    def __init__(self,x,y,**kwargs):    \n        # get desired degree\n        self.D = kwargs['degree']\n        self.N = x.shape[0]\n        \n        # all monomial terms degrees\n        degs = np.array(list(itertools.product(list(np.arange(self.D+1)), repeat = self.N)))\n        b = np.sum(degs,axis = 1)\n        ind = np.argwhere(b <= self.D)\n        ind = [v[0] for v in ind]\n        degs = degs[ind,:]     \n        self.degs = degs[1:,:]\n\n        # define initializer\n        self.num_classifiers = 1\n        if 'num_classifiers' in kwargs:\n            self.num_classifiers = kwargs['num_classifiers']\n        self.scale = 0.1\n        if 'scale' in kwargs:\n            self.scale = kwargs['scale']\n\n    # create initial weights for arbitrary feedforward network\n    def initializer(self):\n        w_init = self.scale*np.random.randn(len(self.degs)+1,self.num_classifiers);\n        return w_init\n    \n    # compute transformation on entire set of inputs\n    def feature_transforms(self,x): \n        x_transformed = np.array([np.prod(x**v[:,np.newaxis],axis = 0)[:,np.newaxis] for v in self.degs])[:,:,0]         \n        return x_transformed"""
mlrefined_libraries/multilayer_perceptron_library/library_v1/sines.py,5,"b""import autograd.numpy as np\nimport copy\nimport itertools\n\nclass Setup:\n    def __init__(self,x,y,**kwargs):    \n        # get desired degree\n        self.D = kwargs['degree']\n        self.N = x.shape[0]\n        \n        # all monomial terms degrees\n        degs = np.array(list(itertools.product(list(np.arange(self.D+1)), repeat = self.N)))\n        b = np.sum(degs,axis = 1)\n        ind = np.argwhere(b <= self.D)\n        ind = [v[0] for v in ind]\n        degs = degs[ind,:]     \n        self.degs = degs[1:,:]\n\n        # define initializer\n        self.num_classifiers = 1\n        if 'num_classifiers' in kwargs:\n            self.num_classifiers = kwargs['num_classifiers']\n        self.scale = 0.1\n        if 'scale' in kwargs:\n            self.scale = kwargs['scale']\n\n    # create initial weights for arbitrary feedforward network\n    def initializer(self):\n        w_init = self.scale*np.random.randn(len(self.degs)+1,self.num_classifiers);\n        return w_init\n    \n    # compute transformation on entire set of inputs\n    def feature_transforms(self,x): \n        x_transformed = np.array([np.prod(x**v[:,np.newaxis],axis = 0)[:,np.newaxis] for v in self.degs])[:,:,0]         \n        return x_transformed"""
mlrefined_libraries/multilayer_perceptron_library/library_v1/stumps.py,5,"b""import autograd.numpy as np\nimport copy\n\nclass Setup:\n    def __init__(self,x,y,**kwargs):        \n        # create splits, levels, and dims\n        self.splits,self.levels,self.dims =  self.create_boost_stumps(x,y)\n\n        # define initializer\n        self.num_classifiers = 1\n        if 'num_classifiers' in kwargs:\n            self.num_classifiers = kwargs['num_classifiers']\n        self.scale = 0.1\n        if 'scale' in kwargs:\n            self.scale = kwargs['scale']\n\n    # create initial weights for arbitrary feedforward network\n    def initializer(self):\n        w_init = np.zeros((len(self.splits)+1,self.num_classifiers));\n        return w_init\n    \n    # compute transformation on entire set of inputs\n    def feature_transforms(self,x): \n        # container for stump transformed data\n        N = x.shape[0]\n        P = x.shape[1]\n        S = len(self.splits)\n        x_transformed = np.zeros((S,P))\n\n        # loop over points and transform each individually\n        for pt in range(P):\n            x_n = x[:,pt]\n\n            # loop over the stump collectionand calculate weighted contribution\n            for u in range(len(self.splits)):\n                # get current stump f_u\n                split = self.splits[u]\n                level = self.levels[u]\n                dim = self.dims[u]\n\n                ### our stump function f_u(x)\n                if x_n[dim] <= split:  # lies to the left - so evaluate at left level\n                    x_transformed[u][pt] = level[0]\n                else:\n                    x_transformed[u][pt]  = level[1]\n        return x_transformed\n\n    def create_boost_stumps(self,x,y):\n        '''\n        Create stumps tailored to an input dataset (x,y) based on the naive method of creating\n        a split point between each pair of successive inputs.  \n\n        The input to this function: a dataset (x,y) where the input x has shape \n        (NUMBER OF POINTS by  DIMENSION OF INPUT)\n\n        The output of this function is a set of two lists, one containing the split points and \n        the other the corresponding levels of stumps.\n        '''\n\n        # containers for the split points and levels of our stumps, along with container\n        # for which dimension the stump is defined along\n        splits = []\n        levels = []\n        dims = []\n\n        # important constants: dimension of input N and total number of points P\n        N = np.shape(x)[0]              \n        P = np.size(y)\n\n        ### begin outer loop - loop over each dimension of the input\n        for n in range(N):\n            # make a copy of the n^th dimension of the input data (we will sort after this)\n            x_n = copy.deepcopy(x[n,:])\n            y_n = copy.deepcopy(y)\n\n            # sort x_n and y_n according to ascending order in x_n\n            sorted_inds = np.argsort(x_n,axis = 0)\n            x_n = x_n[sorted_inds]\n            y_n = y_n[:,sorted_inds]\n\n            # loop over points and create stump in between each \n            # in dimension n\n            for p in range(P - 1):\n                # compute split point\n                split = (x_n[p] + x_n[p+1])/float(2)\n\n                ### create non-zero stump to 'left' of split ###\n                # compute and store split point\n                splits.append(split)\n                levels.append([1,0])\n                dims.append(n)\n\n                ### create non-zero stump to 'right' of split ###\n                # compute and store split point\n                splits.append(split)\n                levels.append([0,1])\n                dims.append(n)\n\n        # return items\n        return splits,levels,dims"""
mlrefined_libraries/multilayer_perceptron_library/library_v1/superlearn_setup.py,2,"b""import autograd.numpy as np\nfrom . import optimizers \nfrom . import cost_functions\nfrom . import normalizers\nfrom . import multilayer_perceptron\nfrom . import multilayer_perceptron_batch_normalized\nfrom . import stumps\nfrom . import polys\nfrom . import history_plotters\n\nclass Setup:\n    def __init__(self,x,y,**kwargs):\n        # link in data\n        self.x = x\n        self.y = y\n        \n        # make containers for all histories\n        self.weight_histories = []\n        self.cost_histories = []\n        self.count_histories = []\n        \n    #### define feature transformation ####\n    def choose_features(self,name,**kwargs): \n        ### select from pre-made feature transforms ###\n        # multilayer perceptron #\n        if name == 'multilayer_perceptron':\n            transformer = multilayer_perceptron.Setup(**kwargs)\n            self.feature_transforms = transformer.feature_transforms\n            self.initializer = transformer.initializer\n            self.layer_sizes = transformer.layer_sizes\n            \n        if name == 'multilayer_perceptron_batch_normalized':\n            transformer = multilayer_perceptron_batch_normalized.Setup(**kwargs)\n            self.feature_transforms = transformer.feature_transforms\n            self.initializer = transformer.initializer\n            self.layer_sizes = transformer.layer_sizes\n            \n        # stumps #\n        if name == 'stumps':\n            transformer = stumps.Setup(self.x,self.y,**kwargs)\n            self.feature_transforms = transformer.feature_transforms\n            self.initializer = transformer.initializer\n            \n        # polynomials #\n        if name == 'polys':\n            transformer = polys.Setup(self.x,self.y,**kwargs)\n            self.feature_transforms = transformer.feature_transforms\n            self.initializer = transformer.initializer\n            self.degs = transformer.D\n            \n        # input a custom feature transformation\n        if name == 'custom':\n            self.feature_transforms = kwargs['feature_transforms']\n            self.initializer = kwargs['initializer']\n        self.feature_name = name\n\n    #### define normalizer ####\n    def choose_normalizer(self,name):\n        # produce normalizer / inverse normalizer\n        s = normalizers.Setup(self.x,name)\n        self.normalizer = s.normalizer\n        self.inverse_normalizer = s.inverse_normalizer\n        \n        # normalize input \n        self.x = self.normalizer(self.x)\n        self.normalizer_name = name\n     \n    #### define cost function ####\n    def choose_cost(self,name,**kwargs):\n        # pick cost based on user input\n        funcs = cost_functions.Setup(name,self.x,self.y,self.feature_transforms,**kwargs)\n        self.cost = funcs.cost\n        self.model = funcs.model\n        \n        # if the cost function is a two-class classifier, build a counter too\n        if name == 'softmax' or name == 'perceptron':\n            funcs = cost_functions.Setup('twoclass_counter',self.x,self.y,self.feature_transforms,**kwargs)\n            self.counter = funcs.cost\n        if name == 'multiclass_softmax' or name == 'multiclass_perceptron':\n            funcs = cost_functions.Setup('multiclass_counter',self.x,self.y,self.feature_transforms,**kwargs)\n            self.counter = funcs.cost\n        self.cost_name = name\n            \n    #### run optimization ####\n    def fit(self,**kwargs):\n        # basic parameters for gradient descent run (default algorithm)\n        max_its = 500; alpha_choice = 10**(-1);\n        self.w_init = self.initializer()\n        optimizer = 'gradient descent'\n        if 'optimizer' in kwargs:\n            optimizer = kwargs['optimizer']\n        \n        # set parameters by hand\n        if 'max_its' in kwargs:\n            self.max_its = kwargs['max_its']\n        if 'alpha_choice' in kwargs:\n            self.alpha_choice = kwargs['alpha_choice']\n            \n        # batch size for gradient descent?\n        self.num_pts = np.size(self.y)\n        self.batch_size = np.size(self.y)\n        if 'batch_size' in kwargs:\n            self.batch_size = kwargs['batch_size']\n\n        # optimize\n        weight_history = []\n        cost_history = []\n        \n        if optimizer == 'gradient descent':\n            # run gradient descent\n            weight_history,cost_history = optimizers.gradient_descent(self.cost,self.alpha_choice,self.max_its,self.w_init,self.num_pts,self.batch_size)\n            \n        if optimizer == 'RMSprop':\n            weight_history,cost_history = optimizers.RMSprop(self.cost,self.alpha_choice,self.max_its,self.w_init,self.num_pts,self.batch_size)                \n                \n        if optimizer == 'newtons method':\n            epsilon = 10**(-7)\n            if 'epsilon' in kwargs:\n                epsilon = kwargs['epsilon']\n            weight_history,cost_history = optimizers.newtons_method(self.cost,self.max_its,self.w_init,self.num_pts,self.batch_size,epsilon = epsilon)\n\n        # store all new histories\n        self.weight_histories.append(weight_history)\n        self.cost_histories.append(cost_history)\n        \n        # if classification produce count history\n        if self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n            count_history = [self.counter(v) for v in weight_history]\n            \n            # store count history\n            self.count_histories.append(count_history)\n \n    #### plot histories ###\n    def show_histories(self,**kwargs):\n        start = 0\n        if 'start' in kwargs:\n            start = kwargs['start']\n            \n        # if labels not in input argument, make blank labels\n        labels = []\n        for c in range(len(self.cost_histories)):\n            labels.append('')\n        if 'labels' in kwargs:\n            labels = kwargs['labels']\n        history_plotters.Setup(self.cost_histories,self.count_histories,start,labels)\n        \n    """
mlrefined_libraries/multilayer_perceptron_library/library_v1/unsuperlearn_setup.py,0,"b""import autograd.numpy as np\nfrom . import optimizers \nfrom . import cost_functions\nfrom . import normalizers\nfrom . import multilayer_perceptron\nfrom . import history_plotters\n\nclass Setup:\n    def __init__(self,X,**kwargs):\n        # link in data\n        self.x = X\n        \n        # make containers for all histories\n        self.weight_histories = []\n        self.cost_histories = []\n        self.count_histories = []\n        \n    #### define feature transformation ####\n    def choose_encoder(self,**kwargs): \n        # select from pre-made feature transforms\n        # form encoder\n        transformer = multilayer_perceptron.Setup(**kwargs)\n        self.feature_transforms = transformer.feature_transforms\n        self.initializer_1 = transformer.initializer\n        self.layer_sizes_encoder = transformer.layer_sizes\n        \n    def choose_decoder(self,**kwargs): \n        # form decoder\n        transformer = multilayer_perceptron.Setup(**kwargs)\n        self.feature_transforms_2 = transformer.feature_transforms\n        self.initializer_2 = transformer.initializer\n        self.layer_sizes_decoder = transformer.layer_sizes\n\n    #### define normalizer ####\n    def choose_normalizer(self,name):\n        # produce normalizer / inverse normalizer\n        s = normalizers.Setup(self.x,name)\n        self.normalizer = s.normalizer\n        self.inverse_normalizer = s.inverse_normalizer\n        \n        # normalize input \n        self.x = self.normalizer(self.x)\n        self.normalizer_name = name\n     \n    #### define cost function ####\n    def choose_cost(self,name,**kwargs):\n        # pick cost based on user input\n        funcs = cost_functions.Setup(name,self.x,[],self.feature_transforms,feature_transforms_2 = self.feature_transforms_2,**kwargs)\n        self.cost = funcs.cost\n        self.encoder = funcs.encoder\n        self.decoder = funcs.decoder\n        self.cost_name = name\n            \n    #### run optimization ####\n    def fit(self,**kwargs):\n        # basic parameters for gradient descent run (default algorithm)\n        max_its = 500; alpha_choice = 10**(-1);\n        self.w_init_1 = self.initializer_1()\n        self.w_init_2 = self.initializer_2()\n        self.w_init = [self.w_init_1,self.w_init_2]\n        \n        # set parameters by hand\n        if 'max_its' in kwargs:\n            self.max_its = kwargs['max_its']\n        if 'alpha_choice' in kwargs:\n            self.alpha_choice = kwargs['alpha_choice']\n        if 'w' in kwargs:\n            self.w_init = kwargs['w']\n\n        # run gradient descent\n        weight_history,cost_history = optimizers.gradient_descent(self.cost,self.alpha_choice,self.max_its,self.w_init,self.num_pts,self.batch_size)\n            \n         # store all new histories\n        self.weight_histories.append(self.weight_history)\n        self.cost_histories.append(self.cost_history)\n        \n    #### plot histories ###\n    def show_histories(self,**kwargs):\n        start = 0\n        if 'start' in kwargs:\n            start = kwargs['start']\n            \n        # if labels not in input argument, make blank labels\n        labels = []\n        for c in range(len(self.cost_histories)):\n            labels.append('')\n        if 'labels' in kwargs:\n            labels = kwargs['labels']\n        history_plotters.Setup(self.cost_histories,self.count_histories,start,labels)\n        \n    """
mlrefined_libraries/nonlinear_superlearn_library/intro_boost_library/__init__.py,0,b''
mlrefined_libraries/nonlinear_superlearn_library/intro_boost_library/cost_functions.py,20,"b""import autograd.numpy as np\nfrom inspect import signature\n\nclass Setup:\n    def __init__(self,name):             \n        ### make cost function choice ###\n        # for regression\n        if name == 'least_squares':\n            self.cost = self.least_squares\n        if name == 'least_absolute_deviations':\n            self.cost = self.least_absolute_deviations\n            \n        # for two-class classification\n        if name == 'softmax':\n            self.cost = self.softmax\n        if name == 'perceptron':\n            self.cost = self.perceptron\n        if name == 'twoclass_counter':\n            self.cost = self.counting_cost\n            \n        # for multiclass classification\n        if name == 'multiclass_perceptron':\n            self.cost = self.multiclass_perceptron\n        if name == 'multiclass_softmax':\n            self.cost = self.multiclass_softmax\n        if name == 'multiclass_counter':\n            self.cost = self.multiclass_counting_cost\n            \n    ###### cost functions #####\n    # set model\n    def set_model(self,model):\n        self.model = model\n    \n    ###### regression costs #######\n    # an implementation of the least squares cost function for linear regression\n    def least_squares(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # compute cost\n        cost = np.sum((self.model(x_p,w) - y_p)**2)\n        return cost/float(np.size(y_p))\n\n    # a compact least absolute deviations cost function\n    def least_absolute_deviations(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # compute cost\n        cost = np.sum(np.abs(self.model(x_p,w) - y_p))\n        return cost/float(np.size(y_p))\n\n    ###### two-class classification costs #######\n    # the convex softmax cost function\n    def softmax(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.log(1 + np.exp(-y_p*(self.model(x_p,w)))))\n        return cost/float(np.size(y_p))\n\n    # the convex relu cost function\n    def relu(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.maximum(0,-y_p*self.model(x_p,w)))\n        return cost/float(np.size(y_p))\n    \n    # the counting cost function\n    def counting_cost(self,x,y):\n        y_predict = np.sign(self.model(x))\n        misclass = len(np.argwhere(y_predict != y))\n        return misclass \n\n    ###### multiclass classification costs #######\n    # multiclass perceptron\n    def multiclass_perceptron(self,w,x,y,iter):\n        # get subset of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n\n        # pre-compute predictions on all points\n        all_evals = self.model(x_p,w)\n\n        # compute maximum across data points\n        a =  np.max(all_evals,axis = 0)        \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n        cost = np.sum(a - b)\n\n        # return average\n        return cost/float(np.size(y_p))\n\n    # multiclass softmax\n    def multiclass_softmax(self,w,x,y,iter):\n        # get subset of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # pre-compute predictions on all points\n        all_evals = self.model(x_p,w)\n\n        # compute softmax across data points\n        a = np.log(np.sum(np.exp(all_evals),axis = 0)) \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n        cost = np.sum(a - b)\n\n        # return average\n        return cost/float(np.size(y_p))\n\n    # multiclass misclassification cost function - aka the fusion rule\n    def multiclass_counting_cost(self,w,x,y):            \n        # pre-compute predictions on all points\n        all_evals = self.model(x,w)\n\n        # compute predictions of each input point\n        y_predict = (np.argmax(all_evals,axis = 0))[np.newaxis,:]\n\n        # compare predicted label to actual label\n        count = np.sum(np.abs(np.sign(y - y_predict)))\n\n        # return number of misclassifications\n        return count"""
mlrefined_libraries/nonlinear_superlearn_library/intro_boost_library/kernel_booster.py,18,"b""import autograd.numpy as np\nfrom . import optimizers \nfrom . import cost_functions\nfrom . import normalizers\nimport copy\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nimport time\n\nclass Setup:\n    def __init__(self,x,y,**kwargs):\n        # link in data\n        self.x_orig = x\n        self.y_orig = y\n       \n    #### define normalizer ####\n    def choose_normalizer(self,name):       \n        # produce normalizer / inverse normalizer\n        s = normalizers.Setup(self.x_orig,name)\n        self.normalizer = s.normalizer\n        self.inverse_normalizer = s.inverse_normalizer\n        \n        # normalize input \n        self.x = self.normalizer(self.x_orig)\n        self.normalizer_name = name\n        self.y = self.y_orig\n        \n    #### split into training / validation sets ####    \n    def make_train_valid_split(self,train_portion):\n        # translate desired training portion into exact indecies\n        r = np.random.permutation(self.x.shape[1])\n        train_num = int(np.round(train_portion*len(r)))\n        self.train_inds = r[:train_num]\n        self.valid_inds = r[train_num:]\n        \n        # define training and validation sets\n        self.x_train = self.x[:,self.train_inds]\n        self.x_valid = self.x[:,self.valid_inds]\n        \n        self.y_train = self.y[:,self.train_inds]\n        self.y_valid = self.y[:,self.valid_inds]   \n  \n    #### define cost function ####\n    def choose_cost(self,name,**kwargs):\n        # create cost on entire dataset\n        self.cost = cost_functions.Setup(name)\n                \n        # if the cost function is a two-class classifier, build a counter too\n        if name == 'softmax' or name == 'perceptron':\n            self.counter = cost_functions.Setup('twoclass_counter')\n            \n        if name == 'multiclass_softmax' or name == 'multiclass_perceptron':\n            self.counter = cost_functions.Setup('multiclass_counter')\n            \n        self.cost_name = name\n            \n    #### setup optimization ####\n    def choose_optimizer(self,optimizer_name,**kwargs):\n        # general params for optimizers\n        max_its = 500; \n        alpha_choice = 10**(-1);\n        epsilon = 10**(-10)\n        \n        # set parameters by hand\n        if 'max_its' in kwargs:\n            max_its = kwargs['max_its']\n        if 'alpha_choice' in kwargs:\n            alpha_choice = kwargs['alpha_choice']\n        if 'epsilon' in kwargs:\n            epsilon = kwargs['epsilon']\n            \n        # batch size for gradient descent?\n        self.w = 0.0*np.random.randn(self.x.shape[0] + 1,1)\n        num_pts = np.size(self.y)\n        batch_size = np.size(self.y)\n        if 'batch_size' in kwargs:\n            self.batch_size = kwargs['batch_size']\n        \n        # run gradient descent\n        if optimizer_name == 'gradient_descent':\n            self.optimizer = lambda cost,x,y,w: optimizers.gradient_descent(cost,w,x,y,alpha_choice,max_its,batch_size)\n        \n        if optimizer_name == 'newtons_method':\n            self.optimizer = lambda cost,x,y,w: optimizers.newtons_method(cost,w,x,y,max_its,epsilon=epsilon)\n      \n    \n    ######## boosting demo with monomials  ########\n    ### create prototype steps ###\n    def create_monomials(self,D):\n        N = self.x.shape[0]\n        all_monos = []\n        if N == 1:\n            for d in range(D):\n                mon = lambda x,deg = d: x**deg\n                all_monos.append(copy.deepcopy(mon))\n\n        if N == 2:\n            degs = []\n            for n in range(D):\n                for m in range(D):\n                    if n + m <= D:\n                        degs.append([n,m])\n            \n            for deg in degs:\n                mon = lambda x,n = deg[0], m = deg[1]: (x[0,:][np.newaxis,:]**n)*(x[1,:][np.newaxis,:]**m)\n                all_monos.append(copy.deepcopy(mon))\n        return all_monos\n    \n    ### boost it ###\n    def boost(self,num_rounds,D,**kwargs):        \n        # create monomials\n        all_steps = self.create_monomials(D)        \n        num_steps = len(all_steps)\n        \n        # container for models and cost function histories\n        self.best_steps = []\n        self.train_cost_vals = []\n        self.valid_cost_vals = []\n        self.models = []\n                \n        # tune bias\n        model_0 = lambda x,w: w*np.ones((1,x.shape[1]))\n        self.cost.set_model(model_0)\n        w = 0.1*np.random.randn(1)\n        w_hist,c_hist = self.optimizer(self.cost.cost,self.x_train,self.y_train,w)\n\n        # determine smallest cost value attained\n        ind = np.argmin(c_hist)\n        best_w = w_hist[ind][0]\n\n        # lock in model_0 value\n        model = lambda x,w=best_w: model_0(x,w)\n        self.best_steps.append(copy.deepcopy(model))\n        self.models.append(copy.deepcopy(model))\n  \n        train_cost_val = c_hist[ind]\n        self.train_cost_vals.append(train_cost_val)\n        valid_cost_val = self.cost.cost(best_w,self.x_valid,self.y_valid,np.arange(len(self.y_valid)))\n        self.valid_cost_vals.append(valid_cost_val)\n\n        # pluck counter\n        if  self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n            self.train_count_vals = []\n            self.valid_count_vals = []     \n            \n        # pluck counter\n        if  self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n            self.counter.set_model(model)\n\n            train_count = self.counter.cost(self.x_train,self.y_train)\n            valid_count = self.counter.cost(self.x_valid,self.y_valid)\n\n            self.train_count_vals.append(train_count)\n            self.valid_count_vals.append(valid_count)   \n\n        # index sets to keep track of which feature-touching weights have been used\n        # thus far\n        used = [0]\n        unused = {i for i in range(1,num_steps+1)}\n        for i in range(num_rounds):  \n            print ('starting round ' + str(i+1) + ' of ' + str(num_rounds) + ' of boosting')\n            # loop over unused indices and try out each remaining corresponding weight\n            best_weight = 0\n            best_train_cost = np.inf\n            best_valid_cost = np.inf\n            best_ind = 0\n\n            for n in unused:\n                # get current proto-step to test\n                w = 0.1*np.random.randn(1)\n                current_step = lambda x,w: w*all_steps[n-1](x)\n                \n\n                # construct model to test\n                current_model = lambda x,w: model(x) + current_step(x,w)\n                       \n                # load in current model\n                self.cost.set_model(current_model)\n                w_hist,c_hist = self.optimizer(self.cost.cost,self.x_train,self.y_train,w)\n\n                # determine smallest cost value attained\n                ind = np.argmin(c_hist)         \n                weight = w_hist[ind]\n                train_cost_val = c_hist[ind]\n                valid_cost_val = self.cost.cost(weight,self.x_valid,self.y_valid,np.arange(len(self.y_valid)))\n\n                # update smallest cost val / associated weight\n                if train_cost_val < best_train_cost:\n                    best_w = weight\n                    best_train_cost = train_cost_val\n                    best_valid_cost = valid_cost_val\n                    best_ind = n\n            \n            # after sweeping through and computing minimum for all subproblems\n            # update the best weight value\n            self.train_cost_vals.append(copy.deepcopy(best_train_cost))\n            self.valid_cost_vals.append(copy.deepcopy(best_valid_cost))\n\n            best_step = lambda x,w=best_w,ind=best_ind-1: w[0]*all_steps[ind](x)\n            self.best_steps.append(copy.deepcopy(best_step))\n            \n            # fix next model\n            model = lambda x,steps=self.best_steps: np.sum([v(x) for v in steps],axis = 0)\n            self.models.append(copy.deepcopy(model))    \n\n            # pluck counter\n            if  self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n                self.counter.set_model(model)\n\n                train_count = self.counter.cost(self.x_train,self.y_train)\n                valid_count = self.counter.cost(self.x_valid,self.y_valid)\n                \n                self.train_count_vals.append(train_count)\n                self.valid_count_vals.append(valid_count)   \n            \n            # remove best index from unused set, add to used set\n            #unused -= {best_ind}\n            # used.append(best_ind)\n            \n        # make universals\n        self.used = used\n        \n        print ('boosting complete!')\n        time.sleep(1.5)\n        clear_output()\n   \n    #### plotting functionality ###\n    def plot_history(self):\n        # colors for plotting\n        colors = [[0,0.7,1],[1,0.8,0.5]]\n\n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); \n        \n        ### plot history val ###\n        ax.plot(self.train_cost_vals,linewidth = 2,color = colors[0]) \n        ax.plot(self.valid_cost_vals,linewidth = 2,color = colors[1]) \n        \n        #ax.scatter(np.arange(len(self.cost_vals)).flatten(),self.cost_vals,s = 70,color = colors[0],edgecolor = 'k',linewidth = 1,zorder = 5) \n\n        # change tick labels to used\n        #ax.set_xticks(np.arange(len(self.cost_vals)))\n        #ax.set_xticklabels(self.used)\n        \n        # clean up panel / axes labels\n        xlabel = 'boosting round'\n        ylabel = 'cost value'\n        title = 'cost value at each round of boosting'\n        ax.set_xlabel(xlabel,fontsize = 14)\n        ax.set_ylabel(ylabel,fontsize = 14,rotation = 90,labelpad = 25)\n        ax.set_title(title,fontsize = 16)\n        \n        # histogram plot of each non-bias weight\n        ax.axhline(c='k',zorder = 2)\n        \n    #### plotting functionality ###\n    def plot_misclass_history(self):     \n        # colors for plotting\n        colors = [[0,0.7,1],[1,0.8,0.5]]\n\n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); \n        \n        ### plot history val ###\n        ax.plot(self.train_count_vals,linewidth = 2,color = colors[0]) \n        ax.plot(self.valid_count_vals,linewidth = 2,color = colors[1]) \n        \n        # clean up panel / axes labels\n        xlabel = 'boosting round'\n        ylabel = 'number of misclassifications'\n        title = 'misclassifications at each round of boosting'\n        ax.set_xlabel(xlabel,fontsize = 14)\n        ax.set_ylabel(ylabel,fontsize = 14,rotation = 90,labelpad = 25)\n        ax.set_title(title,fontsize = 16)\n        \n        # histogram plot of each non-bias weight\n        ax.axhline(c='k',zorder = 2)"""
mlrefined_libraries/nonlinear_superlearn_library/intro_boost_library/net_booster.py,21,"b""import autograd.numpy as np\nfrom . import optimizers \nfrom . import cost_functions\nfrom . import normalizers\nimport copy\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nimport time\n\nclass Setup:\n    def __init__(self,x,y,**kwargs):\n        # link in data\n        self.x_orig = x\n        self.y_orig = y\n        \n        # make containers for all histories\n        self.weight_histories = []\n        self.train_cost_histories = []\n        self.train_count_histories = []\n        self.valid_cost_histories = []\n        self.valid_count_histories = []\n\n    #### define normalizer ####\n    def choose_normalizer(self,name):       \n        # produce normalizer / inverse normalizer\n        s = normalizers.Setup(self.x_orig,name)\n        self.normalizer = s.normalizer\n        self.inverse_normalizer = s.inverse_normalizer\n        \n        # normalize input \n        self.x = self.normalizer(self.x_orig)\n        self.normalizer_name = name\n        self.y = self.y_orig\n        \n    #### split into training / validation sets ####    \n    def make_train_valid_split(self,train_portion):\n        # translate desired training portion into exact indecies\n        r = np.random.permutation(self.x.shape[1])\n        train_num = int(np.round(train_portion*len(r)))\n        self.train_inds = r[:train_num]\n        self.valid_inds = r[train_num:]\n        \n        # define training and validation sets\n        self.x_train = self.x[:,self.train_inds]\n        self.x_valid = self.x[:,self.valid_inds]\n        \n        self.y_train = self.y[:,self.train_inds]\n        self.y_valid = self.y[:,self.valid_inds]   \n  \n    #### define cost function ####\n    def choose_cost(self,name,**kwargs):\n        # create cost on entire dataset\n        self.cost = cost_functions.Setup(name)\n                \n        # if the cost function is a two-class classifier, build a counter too\n        if name == 'softmax' or name == 'perceptron':\n            self.counter = cost_functions.Setup('twoclass_counter')\n            \n        if name == 'multiclass_softmax' or name == 'multiclass_perceptron':\n            self.counter = cost_functions.Setup('multiclass_counter')\n            \n        self.cost_name = name\n            \n    #### setup optimization ####\n    def choose_optimizer(self,optimizer_name,**kwargs):\n        # general params for optimizers\n        max_its = 500; \n        alpha_choice = 10**(-1);\n        epsilon = 10**(-10)\n        \n        # set parameters by hand\n        if 'max_its' in kwargs:\n            max_its = kwargs['max_its']\n        if 'alpha_choice' in kwargs:\n            alpha_choice = kwargs['alpha_choice']\n        if 'epsilon' in kwargs:\n            epsilon = kwargs['epsilon']\n            \n        # batch size for gradient descent?\n        self.w = 0.0*np.random.randn(self.x.shape[0] + 1,1)\n        num_pts = np.size(self.y)\n        batch_size = np.size(self.y)\n        if 'batch_size' in kwargs:\n            self.batch_size = kwargs['batch_size']\n        \n        # run gradient descent\n        if optimizer_name == 'gradient_descent':\n            self.optimizer = lambda cost,x,y,w: optimizers.gradient_descent(cost,w,x,y,alpha_choice,max_its,batch_size)\n        \n        if optimizer_name == 'newtons_method':\n            self.optimizer = lambda cost,x,y,w: optimizers.newtons_method(cost,w,x,y,max_its,epsilon=epsilon)\n\n    # define activation\n    def choose_activation(self,activation):\n        if activation == 'tanh':\n            self.activation = lambda data: np.tanh(data)\n        elif activation == 'relu':\n            self.activation = lambda data: np.maximum(0,data)\n      \n    \n    # fully evaluate our network features using the tensor of weights in w\n    def perceptron(self,a, w):    \n        # compute inner product with current layer weights\n        a = w[0][0] + np.dot(a.T, w[0][1:])\n\n        # output of layer activation\n        a = self.activation(a).T\n        \n        # final linear combo \n        a = w[1][0] + np.dot(a.T,w[1][1:])\n        return a.T\n        \n    ### boost it ###\n    def boost(self,num_rounds,**kwargs): \n        verbose = True\n        if 'verbose' in kwargs:\n            verbose = kwargs['verbose']\n        \n        # container for models and cost function histories\n        self.best_steps = []\n        self.train_cost_vals = []\n        self.valid_cost_vals = []\n        self.models = []\n        \n        # tune bias\n        model_0 = lambda x,w: w*np.ones((1,x.shape[1]))\n        self.cost.set_model(model_0)\n        w = 0.1*np.random.randn(1)\n        w_hist,c_hist = self.optimizer(self.cost.cost,self.x_train,self.y_train,w)\n        \n        # determine smallest cost value attained\n        ind = np.argmin(c_hist)\n        w_best = w_hist[ind]\n\n        # lock in model_0 value\n        model = lambda x,w=w_best: model_0(x,w)\n        self.best_steps.append(copy.deepcopy(model))\n        self.models.append(copy.deepcopy(model))\n        model = lambda x,steps=self.best_steps: np.sum([v(x) for v in steps],axis=0)\n        \n        train_cost_val = c_hist[ind]\n        self.train_cost_vals.append(copy.deepcopy(train_cost_val))\n\n        if self.y_valid.size > 0:\n            valid_cost_val = self.cost.cost(w_best,self.x_valid,self.y_valid,np.arange(len(self.y_valid)))\n            self.valid_cost_vals.append(copy.deepcopy(valid_cost_val))\n        \n        # pluck counter\n        if  self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n            self.train_count_vals = []\n            self.valid_count_vals = []     \n            \n        # pluck counter\n        if  self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n            self.counter.set_model(model)\n\n            train_count = self.counter.cost(self.x_train,self.y_train)\n            self.train_count_vals.append(train_count)\n            \n            if self.y_valid.size > 0:\n                valid_count = self.counter.cost(self.x_valid,self.y_valid)\n                self.valid_count_vals.append(valid_count)   \n\n        # boost rounds\n        for i in range(num_rounds):     \n            if verbose: \n                print ('starting round ' + str(i+1) + ' of ' + str(num_rounds) + ' of boosting')\n           \n            # initialize weights\n            scale = 0.1\n            U = 1\n            w = [scale*np.random.randn(self.x.shape[0] + 1,U), scale*np.random.randn(2,U)]\n    \n            # construct model to test\n            next_unit = lambda x,w: self.perceptron(x,w)\n            current_model = lambda x,w: model(x) + next_unit(x,w)\n        \n            # load in current model\n            self.cost.set_model(current_model)\n            w_hist,c_hist = self.optimizer(self.cost.cost,self.x_train,self.y_train,w)\n            \n            # determine smallest cost value attained\n            ind = np.argmin(c_hist)            \n            w_best = w_hist[ind]\n            best_train_cost = c_hist[ind]\n            self.train_cost_vals.append(copy.deepcopy(best_train_cost))\n\n\n            if self.y_valid.size > 0:\n               best_valid_cost = self.cost.cost(w_best,self.x_valid,self.y_valid,np.arange(len(self.y_valid)))\n               self.valid_cost_vals.append(copy.deepcopy(best_valid_cost))\n\n            # pluck counter\n            if  self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n                self.counter.set_model(model)\n\n                train_count = self.counter.cost(self.x_train,self.y_train)\n                self.train_count_vals.append(train_count)\n\n                self.valid_count_vals.append(valid_count)  \n                valid_count = self.counter.cost(self.x_valid,self.y_valid)\n \n            \n            # best_perceptron = lambda x,w=w_best: np.dot(self.perceptron(x,w[0]).T,w[1]).T \n            best_perceptron = lambda x,w=w_best: next_unit(x,w)\n            self.best_steps.append(copy.deepcopy(best_perceptron))\n            \n            # fix next model\n            model = lambda x,steps=self.best_steps: np.sum([v(x) for v in steps],axis=0)\n            self.models.append(copy.deepcopy(model))\n  \n        if verbose:\n            print ('boosting complete!')\n            time.sleep(1.5)\n            clear_output()\n        \n    #### plotting functionality ###\n    def plot_history(self):\n        # colors for plotting\n        colors = [[0,0.7,1],[1,0.8,0.5]]\n\n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); \n        \n        ### plot history val ###\n        ax.plot(self.train_cost_vals,linewidth = 2,color = colors[0]) \n        ax.plot(self.valid_cost_vals,linewidth = 2,color = colors[1]) \n        \n        # clean up panel / axes labels\n        xlabel = 'boosting round'\n        ylabel = 'cost value'\n        title = 'cost value at each round of boosting'\n        ax.set_xlabel(xlabel,fontsize = 14)\n        ax.set_ylabel(ylabel,fontsize = 14,rotation = 90,labelpad = 25)\n        ax.set_title(title,fontsize = 16)\n        \n        # histogram plot of each non-bias weight\n        ax.axhline(c='k',zorder = 2)\n        \n         \n    #### plotting functionality ###\n    def plot_misclass_history(self):     \n        # colors for plotting\n        colors = [[0,0.7,1],[1,0.8,0.5]]\n\n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); \n        \n        ### plot history val ###\n        ax.plot(self.train_count_vals,linewidth = 2,color = colors[0]) \n        ax.plot(self.valid_count_vals,linewidth = 2,color = colors[1]) \n        \n        #ax.scatter(np.arange(len(self.cost_vals)).flatten(),self.cost_vals,s = 70,color = colors[0],edgecolor = 'k',linewidth = 1,zorder = 5) \n\n        # change tick labels to used\n        #ax.set_xticks(np.arange(len(self.cost_vals)))\n        #ax.set_xticklabels(self.used)\n        \n        # clean up panel / axes labels\n        xlabel = 'boosting round'\n        ylabel = 'number of misclassifications'\n        title = 'misclassifications at each round of boosting'\n        ax.set_xlabel(xlabel,fontsize = 14)\n        ax.set_ylabel(ylabel,fontsize = 14,rotation = 90,labelpad = 25)\n        ax.set_title(title,fontsize = 16)\n        \n        # histogram plot of each non-bias weight\n        ax.axhline(c='k',zorder = 2)       \n        """
mlrefined_libraries/nonlinear_superlearn_library/intro_boost_library/normalizers.py,13,"b""import autograd.numpy as np\n\nclass Setup:\n    def __init__(self,x,name):\n        normalizer = 0\n        inverse_normalizer = 0\n        if name == 'standard':\n            # create normalizer\n            self.normalizer, self.inverse_normalizer = self.standard_normalizer(x)\n            \n        elif name == 'sphere':\n            # create normalizer\n            self.normalizer, self.inverse_normalizer = self.PCA_sphereing(x)\n        else:\n            self.normalizer = lambda data: data\n            self.inverse_normalizer = lambda data: data\n \n    # standard normalization function - with nan checker / filler in-er\n    def standard_normalizer(self,x):    \n        # compute the mean and standard deviation of the input\n        x_means = np.nanmean(x,axis = 1)[:,np.newaxis]\n        x_stds = np.nanstd(x,axis = 1)[:,np.newaxis]   \n\n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(x_stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((x_stds.shape))\n            adjust[ind] = 1.0\n            x_stds += adjust\n\n        # fill in any nan values with means \n        ind = np.argwhere(np.isnan(x) == True)\n        for i in ind:\n            x[i[0],i[1]] = x_means[i[0]]\n\n        # create standard normalizer function\n        normalizer = lambda data: (data - x_means)/x_stds\n\n        # create inverse standard normalizer\n        inverse_normalizer = lambda data: data*x_stds + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer\n\n    # compute eigendecomposition of data covariance matrix for PCA transformation\n    def PCA(self,x,**kwargs):\n        # regularization parameter for numerical stability\n        lam = 10**(-7)\n        if 'lam' in kwargs:\n            lam = kwargs['lam']\n\n        # create the correlation matrix\n        P = float(x.shape[1])\n        Cov = 1/P*np.dot(x,x.T) + lam*np.eye(x.shape[0])\n\n        # use numpy function to compute eigenvalues / vectors of correlation matrix\n        d,V = np.linalg.eigh(Cov)\n        return d,V\n\n    # PCA-sphereing - use PCA to normalize input features\n    def PCA_sphereing(self,x,**kwargs):\n        # Step 1: mean-center the data\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_centered = x - x_means\n\n        # Step 2: compute pca transform on mean-centered data\n        d,V = self.PCA(x_centered,**kwargs)\n\n        # Step 3: divide off standard deviation of each (transformed) input, \n        # which are equal to the returned eigenvalues in 'd'.  \n        stds = (d[:,np.newaxis])**(0.5)\n        \n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((stds.shape))\n            adjust[ind] = 1.0\n            stds += adjust\n        \n        normalizer = lambda data: np.dot(V.T,data - x_means)/stds\n\n        # create inverse normalizer\n        inverse_normalizer = lambda data: np.dot(V,data*stds) + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer"""
mlrefined_libraries/nonlinear_superlearn_library/intro_boost_library/optimizers.py,13,"b""import autograd.numpy as np\nfrom autograd import value_and_grad \nfrom autograd import hessian\nfrom autograd.misc.flatten import flatten_func\n\n#### optimizers ####\n# minibatch gradient descent\ndef gradient_descent(g,w,x,y,alpha_choice,max_its,batch_size): \n    # flatten the input function, create gradient based on flat function\n    g_flat, unflatten, w = flatten_func(g, w)\n    grad = value_and_grad(g_flat)\n\n    # record history\n    num_train = y.size\n    w_hist = [unflatten(w)]\n    train_hist = [g_flat(w,x,y,np.arange(num_train))]\n\n    # how many mini-batches equal the entire dataset?\n    num_batches = int(np.ceil(np.divide(num_train, batch_size)))\n\n    # over the line\n    alpha = 0\n\n    for k in range(max_its):             \n        # check if diminishing steplength rule used\n        if alpha_choice == 'diminishing':\n            alpha = 1/float(k)\n        else:\n            alpha = alpha_choice\n            \n        for b in range(num_batches):\n            # collect indices of current mini-batch\n            batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_train))\n\n            # plug in value into func and derivative\n            cost_eval,grad_eval = grad(w,x,y,batch_inds)\n            grad_eval.shape = np.shape(w)\n\n            # take descent step with momentum\n            w = w - alpha*grad_eval\n\n        # update training and validation cost\n        train_cost = g_flat(w,x,y,np.arange(num_train))\n\n        # record weight update, train and val costs\n        w_hist.append(unflatten(w))\n        train_hist.append(train_cost)\n    return w_hist,train_hist\n\n# newtons method function\ndef newtons_method(g,w,x,y,max_its,**kwargs): \n    # flatten input funciton, in case it takes in matrices of weights\n    g_flat, unflatten, w = flatten_func(g, w)\n    \n    # compute the gradient / hessian functions of our input function\n    grad = value_and_grad(g_flat)\n    hess = hessian(g_flat)\n    \n    # set numericxal stability parameter / regularization parameter\n    epsilon = 10**(-7)\n    if 'epsilon' in kwargs:\n        epsilon = kwargs['epsilon']\n    \n    # record history\n    num_train = y.size\n    w_hist = [unflatten(w)]\n    train_hist = [g_flat(w,x,y,np.arange(num_train))]\n\n    # over the line\n    for k in range(max_its):   \n        # evaluate the gradient, store current weights and cost function value\n        cost_eval,grad_eval = grad(w,x,y,np.arange(num_train))\n\n        # evaluate the hessian\n        hess_eval = hess(w,x,y,np.arange(num_train))\n\n        # reshape for numpy linalg functionality\n        hess_eval.shape = (int((np.size(hess_eval))**(0.5)),int((np.size(hess_eval))**(0.5)))\n\n        # solve second order system system for weight update\n        A = hess_eval + epsilon*np.eye(np.size(w))\n        b = grad_eval\n        w = np.linalg.lstsq(A,np.dot(A,w) - b)[0]\n\n        #w = w - np.dot(np.linalg.pinv(hess_eval + epsilon*np.eye(np.size(w))),grad_eval)\n            \n        # update training and validation cost\n        train_cost = g_flat(w,x,y,np.arange(num_train))\n\n        # record weight update, train and val costs\n        w_hist.append(unflatten(w))\n        train_hist.append(train_cost)\n    \n    return w_hist,train_hist"""
mlrefined_libraries/nonlinear_superlearn_library/intro_boost_library/stump_booster.py,24,"b""import autograd.numpy as np\nfrom . import optimizers \nfrom . import cost_functions\nfrom . import normalizers\nimport copy\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom IPython.display import clear_output\nimport time\n\nclass Setup:\n    def __init__(self,x,y,**kwargs):\n        # link in data\n        self.x_orig = x\n        self.y_orig = y\n        \n        # make containers for all histories\n        self.weight_histories = []\n        self.train_cost_histories = []\n        self.train_count_histories = []\n        self.valid_cost_histories = []\n        self.valid_count_histories = []\n\n    #### define normalizer ####\n    def choose_normalizer(self,name):       \n        # produce normalizer / inverse normalizer\n        s = normalizers.Setup(self.x_orig,name)\n        self.normalizer = s.normalizer\n        self.inverse_normalizer = s.inverse_normalizer\n        \n        # normalize input \n        self.x = self.normalizer(self.x_orig)\n        self.normalizer_name = name\n        self.y = self.y_orig\n        \n    #### split into training / validation sets ####    \n    def make_train_valid_split(self,train_portion):\n        # translate desired training portion into exact indecies\n        r = np.random.permutation(self.x.shape[1])\n        train_num = int(np.round(train_portion*len(r)))\n        self.train_inds = r[:train_num]\n        self.valid_inds = r[train_num:]\n        \n        # define training and validation sets\n        self.x_train = self.x[:,self.train_inds]\n        self.x_valid = self.x[:,self.valid_inds]\n        \n        self.y_train = self.y[:,self.train_inds]\n        self.y_valid = self.y[:,self.valid_inds]   \n  \n    #### define cost function ####\n    def choose_cost(self,name,**kwargs):\n        # create cost on entire dataset\n        self.cost = cost_functions.Setup(name)\n                \n        # if the cost function is a two-class classifier, build a counter too\n        if name == 'softmax' or name == 'perceptron':\n            self.counter = cost_functions.Setup('twoclass_counter')\n            \n        if name == 'multiclass_softmax' or name == 'multiclass_perceptron':\n            self.counter = cost_functions.Setup('multiclass_counter')\n            \n        self.cost_name = name\n            \n    #### setup optimization ####\n    def choose_optimizer(self,optimizer_name,**kwargs):\n        # general params for optimizers\n        max_its = 500; \n        alpha_choice = 10**(-1);\n        epsilon = 10**(-10)\n        \n        # set parameters by hand\n        if 'max_its' in kwargs:\n            max_its = kwargs['max_its']\n        if 'alpha_choice' in kwargs:\n            alpha_choice = kwargs['alpha_choice']\n        if 'epsilon' in kwargs:\n            epsilon = kwargs['epsilon']\n            \n        # batch size for gradient descent?\n        self.w = 0.0*np.random.randn(self.x.shape[0] + 1,1)\n        num_pts = np.size(self.y)\n        batch_size = np.size(self.y)\n        if 'batch_size' in kwargs:\n            self.batch_size = kwargs['batch_size']\n        \n        # run gradient descent\n        if optimizer_name == 'gradient_descent':\n            self.optimizer = lambda cost,x,y,w: optimizers.gradient_descent(cost,w,x,y,alpha_choice,max_its,batch_size)\n        \n        if optimizer_name == 'newtons_method':\n            self.optimizer = lambda cost,x,y,w: optimizers.newtons_method(cost,w,x,y,max_its,epsilon=epsilon)\n       \n    \n    ### create prototype steps ###\n    def create_proto_stumps(self):\n        # for which dimension the stump is defined along\n        splits = []\n        dims = []\n\n        # important constants: dimension of input N and total number of points P\n        N = np.shape(self.x)[0]              \n        P = np.size(self.y)\n\n        # begin outer loop - loop over each dimension of the input - create split points and dimensions\n        for n in range(N):\n            # make a copy of the n^th dimension of the input data (we will sort after this)\n            x_n = copy.deepcopy(self.x[n,:])\n            y_n = copy.deepcopy(self.y)\n\n            # sort x_n and y_n according to ascending order in x_n\n            sorted_inds = np.argsort(x_n,axis = 0)\n            x_n = x_n[sorted_inds]\n            y_n = y_n[0,sorted_inds]\n\n            # loop over points and create stump in between each \n            # in dimension n\n            for p in range(P - 1):\n                if y_n[p] != y_n[p+1]:\n                    # compute split point\n                    split = (x_n[p] + x_n[p+1])/float(2)\n\n                    ### create non-zero stump to 'left' of split ###\n                    # compute and store split point\n                    splits.append(split)\n                    dims.append(n)\n\n        ### create stumps out of splits and dims ###\n        all_steps = []\n        for ind in range(len(dims)):\n            # create step function\n            split = splits[ind]\n            dim = dims[ind]\n            step = lambda x,w,split=split,dim=dim: np.array([(w[0] if v <= split else w[1]) for v in x[dim,:]])\n\n            # add to list\n            all_steps.append(copy.deepcopy(step))\n        return all_steps\n    \n    ### boost it ###\n    def boost(self,num_rounds,**kwargs): \n        verbose = True\n        if 'verbose' in kwargs:\n            verbose = kwargs['verbose']\n            \n        # create proto stumps\n        all_steps = self.create_proto_stumps()        \n                \n        # adjust num_rounds based on total number of step features available\n        num_steps = len(all_steps)\n\n        # set maximum number of random steps to check per round\n        max_check = num_steps\n        if 'max_check' in kwargs:\n            max_check = kwargs['max_check']\n\n        # container for models and cost function histories\n        self.best_steps = []\n        self.train_cost_vals = []\n        self.valid_cost_vals = []\n        self.models = []\n                \n        # tune bias\n        model_0 = lambda x,w: w*np.ones((1,x.shape[1]))\n        self.cost.set_model(model_0)\n        w = np.array([0])\n        w_hist,c_hist = self.optimizer(self.cost.cost,self.x_train,self.y_train,w)\n\n        # determine smallest cost value attained\n        ind = np.argmin(c_hist)\n        best_w = w_hist[ind][0]\n\n        # lock in model_0 value\n        model = lambda x,w=best_w: model_0(x,w)\n        self.best_steps.append(copy.deepcopy(model))\n        self.models.append(copy.deepcopy(model))\n        train_cost_val = c_hist[ind]\n        self.train_cost_vals.append(train_cost_val)\n        \n        if self.y_valid.size > 0:\n            valid_cost_val = self.cost.cost(best_w,self.x_valid,self.y_valid,np.arange(len(self.y_valid)))\n            self.valid_cost_vals.append(valid_cost_val)\n\n        # pluck counter\n        if  self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n            self.train_count_vals = []\n            self.valid_count_vals = []     \n            \n        # pluck counter\n        if  self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n            self.counter.set_model(model)\n\n            train_count = self.counter.cost(self.x_train,self.y_train)\n            self.train_count_vals.append(train_count)\n            \n            if self.y_valid.size > 0:\n                valid_count = self.counter.cost(self.x_valid,self.y_valid)\n                self.valid_count_vals.append(valid_count)   \n\n        for i in range(num_rounds):  \n            # index sets to keep track of which feature-touching weights have been used\n            # thus far\n            used = [0]\n            check_inds = np.random.permutation(num_steps)[:max_check] + 1\n            unused = {i for i in check_inds}\n        \n            if verbose == True:\n                print ('starting round ' + str(i+1) + ' of ' + str(num_rounds) + ' of boosting')\n                \n            # loop over unused indices and try out each remaining corresponding weight\n            best_weight = 0\n            best_train_cost = np.inf\n            best_valid_cost = np.inf\n            best_ind = 0\n            for n in unused:\n                # get current proto-step to test\n                current_step = all_steps[n-1]\n                w = np.zeros((2,))\n                \n                # construct model to test\n                current_model = lambda x,w: model(x) + current_step(x,w)\n                       \n                # load in current model\n                self.cost.set_model(current_model)\n                w_hist,c_hist = self.optimizer(self.cost.cost,self.x_train,self.y_train,w)\n\n                # determine smallest cost value attained\n                ind = np.argmin(c_hist)            \n                weight = w_hist[ind]\n                train_cost_val = c_hist[ind]\n                \n                if self.y_valid.size > 0:\n                    valid_cost_val = self.cost.cost(weight,self.x_valid,self.y_valid,np.arange(len(self.y_valid)))\n\n                # update smallest cost val / associated weight\n                if train_cost_val < best_train_cost:\n                    best_w = weight\n                    best_train_cost = train_cost_val\n                    best_ind = n\n\n                    if self.y_valid.size > 0:\n                        best_valid_cost = valid_cost_val\n\n            # after sweeping through and computing minimum for all subproblems\n            # update the best weight value\n            self.train_cost_vals.append(copy.deepcopy(best_train_cost))\n            \n            if self.y_valid.size > 0:\n                self.valid_cost_vals.append(copy.deepcopy(best_valid_cost))\n\n            best_step = lambda x,w=best_w,ind=best_ind-1: all_steps[ind](x,w)\n            self.best_steps.append(copy.deepcopy(best_step))\n            \n            # fix next model\n            model = lambda x,steps=self.best_steps: np.sum([v(x) for v in steps])\n            self.models.append(copy.deepcopy(model))            \n\n            # pluck counter\n            if  self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n                self.counter.set_model(model)\n\n                train_count = self.counter.cost(self.x_train,self.y_train)                \n                self.train_count_vals.append(train_count)\n                \n                if self.y_valid.size > 0:\n                    valid_count = self.counter.cost(self.x_valid,self.y_valid)\n                    self.valid_count_vals.append(valid_count)   \n            \n            # remove best index from unused set, add to used set\n            #unused -= {best_ind}\n            used.append(best_ind)\n            \n        # make universals\n        self.used = used\n        \n        if verbose == True:\n            print ('boosting complete!')\n            time.sleep(1.5)\n            clear_output()\n        \n    #### plotting functionality ###\n    def plot_history(self):\n        # colors for plotting\n        colors = [[0,0.7,1],[1,0.8,0.5]]\n\n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); \n        \n        ### plot history val ###\n        ax.plot(self.train_cost_vals,linewidth = 2,color = colors[0]) \n        ax.plot(self.valid_cost_vals,linewidth = 2,color = colors[1]) \n        \n        #ax.scatter(np.arange(len(self.cost_vals)).flatten(),self.cost_vals,s = 70,color = colors[0],edgecolor = 'k',linewidth = 1,zorder = 5) \n\n        # change tick labels to used\n        #ax.set_xticks(np.arange(len(self.cost_vals)))\n        #ax.set_xticklabels(self.used)\n        \n        # clean up panel / axes labels\n        xlabel = 'boosting round'\n        ylabel = 'cost value'\n        title = 'cost value at each round of boosting'\n        ax.set_xlabel(xlabel,fontsize = 14)\n        ax.set_ylabel(ylabel,fontsize = 14,rotation = 90,labelpad = 25)\n        ax.set_title(title,fontsize = 16)\n        \n        # histogram plot of each non-bias weight\n        ax.axhline(c='k',zorder = 2)\n        \n        \n    #### plotting functionality ###\n    def plot_misclass_history(self):     \n        # colors for plotting\n        colors = [[0,0.7,1],[1,0.8,0.5]]\n\n        # initialize figure\n        fig = plt.figure(figsize = (9,4))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); \n        \n        ### plot history val ###\n        ax.plot(self.train_count_vals,linewidth = 2,color = colors[0]) \n        ax.plot(self.valid_count_vals,linewidth = 2,color = colors[1]) \n        \n        #ax.scatter(np.arange(len(self.cost_vals)).flatten(),self.cost_vals,s = 70,color = colors[0],edgecolor = 'k',linewidth = 1,zorder = 5) \n\n        # change tick labels to used\n        #ax.set_xticks(np.arange(len(self.cost_vals)))\n        #ax.set_xticklabels(self.used)\n        \n        # clean up panel / axes labels\n        xlabel = 'boosting round'\n        ylabel = 'number of misclassifications'\n        title = 'misclassifications at each round of boosting'\n        ax.set_xlabel(xlabel,fontsize = 14)\n        ax.set_ylabel(ylabel,fontsize = 14,rotation = 90,labelpad = 25)\n        ax.set_title(title,fontsize = 16)\n        \n        # histogram plot of each non-bias weight\n        ax.axhline(c='k',zorder = 2)"""
mlrefined_libraries/nonlinear_superlearn_library/intro_general_library/__init__.py,0,b''
mlrefined_libraries/nonlinear_superlearn_library/intro_general_library/cost_functions.py,30,"b""import autograd.numpy as np\nfrom inspect import signature\n\nclass Setup:\n    def __init__(self,name,x,y,feature_transforms,**kwargs):\n        # point to input/output for cost functions\n        self.x = x\n        self.y = y\n       \n        # make copy of feature transformation\n        self.feature_transforms = feature_transforms\n        \n        # count parameter layers of input to feature transform\n        self.sig = signature(self.feature_transforms)\n        \n        ### make cost function choice ###\n        # for regression\n        if name == 'least_squares':\n            self.cost = self.least_squares\n        if name == 'least_absolute_deviations':\n            self.cost = self.least_absolute_deviations\n            \n        # for two-class classification\n        if name == 'softmax':\n            self.cost = self.softmax\n        if name == 'perceptron':\n            self.cost = self.perceptron\n        if name == 'twoclass_counter':\n            self.cost = self.counting_cost\n            \n        # for multiclass classification\n        if name == 'multiclass_perceptron':\n            self.cost = self.multiclass_perceptron\n        if name == 'multiclass_softmax':\n            self.cost = self.multiclass_softmax\n        if name == 'multiclass_counter':\n            self.cost = self.multiclass_counting_cost\n            \n        # for autoencoder\n        if name == 'autoencoder':\n            self.feature_transforms = feature_transforms\n            self.feature_transforms_2 = kwargs['feature_transforms_2']\n            self.cost = self.autoencoder\n\n        # set regularization param\n        self.lam = 0\n        if 'lam' in kwargs:\n            self.lam = kwargs['lam']\n\n            \n    ###### cost functions #####\n    # compute linear combination of input point\n    def model(self,x,w):   \n        # feature transformation - switch for dealing\n        # with feature transforms that either do or do\n        # not have internal parameters\n        f = 0\n        if len(self.sig.parameters) == 2:\n            f = self.feature_transforms(x,w[0])\n        else: \n            f = self.feature_transforms(x)    \n\n        # tack a 1 onto the top of each input point all at once\n        o = np.ones((1,np.shape(f)[1]))\n        f = np.vstack((o,f))\n\n        # compute linear combination and return\n        # switch for dealing with feature transforms that either \n        # do or do not have internal parameters\n        a = 0\n        if len(self.sig.parameters) == 2:\n            a = np.dot(f.T,w[1])\n        else:\n            a = np.dot(f.T,w)\n        return a.T\n    \n    ###### regression costs #######\n    # an implementation of the least squares cost function for linear regression\n    def least_squares(self,w,iter):\n        # get batch of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n        \n        # compute cost\n        cost = np.sum((self.model(x_p,w) - y_p)**2)/y_p.size\n\n        # add l_2 retularizer\n        if self.lam > 0:\n            cost += self.lam*np.sum(w**2)/y_p.size\n        return cost\n\n    # a compact least absolute deviations cost function\n    def least_absolute_deviations(self,w,iter):\n        # get batch of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n        \n        # compute cost\n        cost = np.sum(np.abs(self.model(x_p,w) - y_p))/y_p.size\n\n         # add l_2 retularizer\n        if self.lam > 0:\n            cost += self.lam*np.sum(w**2)/y_p.size\n\n        return cost\n\n    ###### two-class classification costs #######\n    # the convex softmax cost function\n    def softmax(self,w,iter):\n        # get batch of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.log(1 + np.exp(-y_p*self.model(x_p,w))))/y_p.size\n\n        # add l_2 retularizer\n        if self.lam > 0:\n            cost += self.lam*np.sum(w**2)/y_p.size\n        return cost\n\n    # the convex perceptron / relu cost function\n    def perceptron(self,w,iter):\n        # get batch of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.maximum(0,-y_p*self.model(x_p,w)))/y_p.size\n\n        # add l_2 retularizer\n        if self.lam > 0:\n            cost += self.lam*np.sum(w**2)/y_p.size\n        return cost\n    \n    # the counting cost function\n    def counting_cost(self,w):\n        cost = np.sum((np.sign(self.model(self.x,w)) - self.y)**2)\n        return 0.25*cost \n\n    ###### multiclass classification costs #######\n    # multiclass perceptron\n    def multiclass_perceptron(self,w,iter):\n        # get subset of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n\n        # pre-compute predictions on all points\n        all_evals = self.model(x_p,w)\n\n        # compute maximum across data points\n        a =  np.max(all_evals,axis = 0)        \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n        cost = np.sum(a - b)/y_p.size\n\n        # add l_2 retularizer\n        if self.lam > 0:\n            cost += self.lam*np.sum(w**2)/y_p.size\n\n        # return average\n        return cost\n\n    # multiclass softmax\n    def multiclass_softmax(self,w,iter):     \n        # get subset of points\n        x_p = self.x[:,iter]\n        y_p = self.y[:,iter]\n        \n        # pre-compute predictions on all points\n        all_evals = self.model(x_p,w)\n\n        # compute softmax across data points\n        a = np.log(np.sum(np.exp(all_evals),axis = 0)) \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n        cost = np.sum(a - b)/y_p.size\n\n        # add l_2 retularizer\n        if self.lam > 0:\n            cost += self.lam*np.sum(w**2)/y_p.size\n\n        # return average\n        return cost\n\n    # multiclass misclassification cost function - aka the fusion rule\n    def multiclass_counting_cost(self,w):                \n        # pre-compute predictions on all points\n        all_evals = self.model(self.x,w)\n\n        # compute predictions of each input point\n        y_predict = (np.argmax(all_evals,axis = 0))[np.newaxis,:]\n\n        # compare predicted label to actual label\n        count = np.sum(np.abs(np.sign(self.y - y_predict)))\n\n        # return number of misclassifications\n        return count\n    \n    ### for autoencoder ###\n    def encoder(self,x,w):    \n        # feature transformation \n        f = self.feature_transforms(x,w[0])\n\n        # tack a 1 onto the top of each input point all at once\n        o = np.ones((1,np.shape(f)[1]))\n        f = np.vstack((o,f))\n\n        # compute linear combination and return\n        a = np.dot(f.T,w[1])\n        return a.T\n\n    def decoder(self,v,w):\n        # feature transformation \n        f = self.feature_transforms_2(v,w[0])\n\n        # tack a 1 onto the top of each input point all at once\n        o = np.ones((1,np.shape(f)[1]))\n        f = np.vstack((o,f))\n\n        # compute linear combination and return\n        a = np.dot(f.T,w[1])\n        return a.T\n    \n    def autoencoder(self,w):\n        # encode input\n        a = self.encoder(self.x,w[0])\n        \n        # decode result\n        b = self.decoder(a,w[1])\n        \n        # compute Least Squares error\n        cost = np.sum((b - self.x)**2)\n        return cost/float(self.x.shape[1])"""
mlrefined_libraries/nonlinear_superlearn_library/intro_general_library/history_plotters.py,6,"b""# import standard plotting and animation\nimport autograd.numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nclass Setup:\n    def __init__(self,train_cost_histories,train_count_histories,test_cost_histories,test_count_histories,start):\n        # plotting colors\n        self.colors = [[0,0.7,1],[1,0.8,0.5]]\n        \n        # just plot cost history?\n        if len(train_count_histories) == 0:\n            self.plot_cost_histories(train_cost_histories,test_cost_histories,start)\n        else: # plot cost and count histories\n            self.plot_cost_count_histories(train_cost_histories,train_count_histories,test_cost_histories,test_count_histories,start)\n \n    #### compare cost function histories ####\n    def plot_cost_histories(self,train_cost_histories,test_cost_histories,start):        \n        # initialize figure\n        fig = plt.figure(figsize = (10,3))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); \n\n        # run through input histories, plotting each beginning at 'start' iteration\n        for c in range(len(train_cost_histories)):\n            train_history = train_cost_histories[c]\n            test_history = test_cost_histories[c]\n\n            # plot train cost function history\n            ax.plot(np.arange(start,len(train_history),1),train_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[0],label = 'train cost') \n            \n            # plot test cost function history\n            ax.plot(np.arange(start,len(test_history),1),test_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[1],label = 'test_cost') \n\n        # clean up panel / axes labels\n        xlabel = 'step $k$'\n        ylabel = r'$g\\left(\\mathbf{w}^k\\right)$'\n        ax.set_xlabel(xlabel,fontsize = 14)\n        ax.set_ylabel(ylabel,fontsize = 14,rotation = 0,labelpad = 25)\n        title = 'train vs test cost histories'\n        ax.set_title(title,fontsize = 18)\n        \n        # plot legend\n        anchor = (1,1)\n        plt.legend(loc='upper right', bbox_to_anchor=anchor)\n        ax.set_xlim([start - 0.5,len(train_history) - 0.5]) \n        plt.show()\n        \n    #### compare multiple histories of cost and misclassification counts ####\n    def plot_cost_count_histories(self,train_cost_histories,train_count_histories,test_cost_histories,test_count_histories,start):        \n        # initialize figure\n        fig = plt.figure(figsize = (10,3))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 2) \n        ax1 = plt.subplot(gs[0]); \n        ax2 = plt.subplot(gs[1]); \n\n        # run through input histories, plotting each beginning at 'start' iteration\n        for c in range(len(train_cost_histories)):\n            train_cost_history = train_cost_histories[c]\n            train_count_history = train_count_histories[c]\n\n            test_cost_history = test_cost_histories[c]\n            test_count_history = test_count_histories[c]\n            \n            # check if a label exists, if so add it to the plot\n            ax1.plot(np.arange(start,len(train_cost_history),1),train_cost_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[0]) \n            \n            ax1.plot(np.arange(start,len(test_cost_history),1),test_cost_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[1]) \n            \n            ax2.plot(np.arange(start,len(train_count_history),1),train_count_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[0],label = 'train') \n            \n            ax2.plot(np.arange(start,len(test_count_history),1),test_count_history[start:],linewidth = 3*(0.8)**(c),color = self.colors[1],label = 'test') \n                \n        # clean up panel\n        xlabel = 'step $k$'\n        ylabel = r'$g\\left(\\mathbf{w}^k\\right)$'\n        ax1.set_xlabel(xlabel,fontsize = 14)\n        ax1.set_ylabel(ylabel,fontsize = 14,rotation = 0,labelpad = 25)\n        title = 'cost history'\n        ax1.set_title(title,fontsize = 18)\n\n        ylabel = 'num misclasses'\n        ax2.set_xlabel(xlabel,fontsize = 14)\n        ax2.set_ylabel(ylabel,fontsize = 14,rotation = 90,labelpad = 10)\n        title = 'misclassification history'\n        ax2.set_title(title,fontsize = 18)\n        \n        anchor = (1,1)\n        plt.legend(loc='upper right', bbox_to_anchor=anchor)\n        ax1.set_xlim([start - 0.5,len(train_cost_history) - 0.5])\n        ax2.set_xlim([start - 0.5,len(train_cost_history) - 0.5])\n        plt.show()       \n        """
mlrefined_libraries/nonlinear_superlearn_library/intro_general_library/multilayer_perceptron.py,8,"b""import autograd.numpy as np\n\nclass Setup:\n    def __init__(self,**kwargs):        \n        # set default values for layer sizes, activation, and scale\n        activation = 'relu'\n\n        # decide on these parameters via user input\n        if 'activation' in kwargs:\n            activation = kwargs['activation']\n\n        # switches\n        if activation == 'linear':\n            self.activation = lambda data: data\n        elif activation == 'tanh':\n            self.activation = lambda data: np.tanh(data)\n        elif activation == 'relu':\n            self.activation = lambda data: np.maximum(0,data)\n        elif activation == 'sinc':\n            self.activation = lambda data: np.sinc(data)\n        elif activation == 'sin':\n            self.activation = lambda data: np.sin(data)\n        else: # user-defined activation\n            self.activation = kwargs['activation']\n                        \n        # select layer sizes and scale\n        N = 1; M = 1;\n        U = 10;\n        self.layer_sizes = [N,U,M]\n        self.scale = 0.1\n        if 'layer_sizes' in kwargs:\n            self.layer_sizes = kwargs['layer_sizes']\n        if 'scale' in kwargs:\n            self.scale = kwargs['scale']\n\n    # create initial weights for arbitrary feedforward network\n    def initializer(self):\n        # container for entire weight tensor\n        weights = []\n\n        # loop over desired layer sizes and create appropriately sized initial \n        # weight matrix for each layer\n        for k in range(len(self.layer_sizes)-1):\n            # get layer sizes for current weight matrix\n            U_k = self.layer_sizes[k]\n            U_k_plus_1 = self.layer_sizes[k+1]\n\n            # make weight matrix\n            weight = self.scale*np.random.randn(U_k+1,U_k_plus_1)\n            weights.append(weight)\n\n        # re-express weights so that w_init[0] = omega_inner contains all \n        # internal weight matrices, and w_init = w contains weights of \n        # final linear combination in predict function\n        w_init = [weights[:-1],weights[-1]]\n\n        return w_init\n\n    # fully evaluate our network features using the tensor of weights in w\n    def feature_transforms(self,a, w):    \n        # loop through each layer matrix\n        for W in w:\n            #  pad with ones (to compactly take care of bias) for next layer computation        \n            o = np.ones((1,np.shape(a)[1]))\n            a = np.vstack((o,a))\n\n            # compute inner product with current layer weights\n            a = np.dot(a.T, W).T\n\n            # output of layer activation\n            a = self.activation(a)\n        return a"""
mlrefined_libraries/nonlinear_superlearn_library/intro_general_library/multilayer_perceptron_batch_normalized.py,15,"b""import autograd.numpy as np\n\nclass Setup:\n    def __init__(self,**kwargs):        \n        # set default values for layer sizes, activation, and scale\n        activation = 'relu'\n\n        # decide on these parameters via user input\n        if 'activation' in kwargs:\n            activation = kwargs['activation']\n\n        # switches\n        if activation == 'linear':\n            self.activation = lambda data: data\n        elif activation == 'tanh':\n            self.activation = lambda data: np.tanh(data)\n        elif activation == 'relu':\n            self.activation = lambda data: np.maximum(0,data)\n        elif activation == 'sinc':\n            self.activation = lambda data: np.sinc(data)\n        elif activation == 'sin':\n            self.activation = lambda data: np.sin(data)\n        else: # user-defined activation\n            self.activation = kwargs['activation']\n                        \n        # select layer sizes and scale\n        N = 1; M = 1;\n        U = 10;\n        self.layer_sizes = [N,U,M]\n        self.scale = 0.1\n        if 'layer_sizes' in kwargs:\n            self.layer_sizes = kwargs['layer_sizes']\n        if 'scale' in kwargs:\n            self.scale = kwargs['scale']\n\n    # create initial weights for arbitrary feedforward network\n    def initializer(self):\n        # container for entire weight tensor\n        weights = []\n\n        # loop over desired layer sizes and create appropriately sized initial \n        # weight matrix for each layer\n        for k in range(len(self.layer_sizes)-1):\n            # get layer sizes for current weight matrix\n            U_k = self.layer_sizes[k]\n            U_k_plus_1 = self.layer_sizes[k+1]\n\n            # make weight matrix\n            weight = self.scale*np.random.randn(U_k+1,U_k_plus_1)\n            weights.append(weight)\n\n        # re-express weights so that w_init[0] = omega_inner contains all \n        # internal weight matrices, and w_init = w contains weights of \n        # final linear combination in predict function\n        w_init = [weights[:-1],weights[-1]]\n\n        return w_init\n\n    # standard normalization function \n    def standard_normalizer(self,x):\n        # compute the mean and standard deviation of the input\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n\n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(x_stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((x_stds.shape))\n            adjust[ind] = 1.0\n            x_stds += adjust\n\n        # create standard normalizer function\n        normalizer = lambda data: (data - x_means)/x_stds\n\n        # return normalizer \n        return normalizer\n    \n    # a multilayer perceptron network, note the input w is a tensor of weights, with \n    # activation output normalization\n    def feature_transforms(self,a, w):    \n        # loop through each layer matrix\n        self.normalizers = []\n        for W in w:\n            #  pad with ones (to compactly take care of bias) for next layer computation        \n            o = np.ones((1,np.shape(a)[1]))\n            a = np.vstack((o,a))\n\n            # compute linear combination of current layer units\n            a = np.dot(a.T, W).T\n\n            # pass through activation\n            a = self.activation(a)\n\n            # NEW - perform standard normalization to the activation outputs\n            normalizer = self.standard_normalizer(a)\n            a = normalizer(a)\n            \n            # store normalizer for testing data\n            self.normalizers.append(normalizer)\n        return a\n    \n\n    # a copy of the batch normalized architecture that employs normalizers\n    # at each layer based on statistics from training data and user-chosen\n    # choice of weights w\n    def feature_transforms_testing(self,a, w):    \n        # loop through each layer matrix\n        c=0\n        for W in w:\n            #  pad with ones (to compactly take care of bias) for next layer computation        \n            o = np.ones((1,np.shape(a)[1]))\n            a = np.vstack((o,a))\n\n            # compute linear combination of current layer units\n            a = np.dot(a.T, W).T\n\n            # pass through activation\n            a = self.activation(a)\n\n            # get normalizer for this layer tuned to training data\n            normalizer = self.normalizers[c]\n            a = normalizer(a)\n            c+=1\n        return a"""
mlrefined_libraries/nonlinear_superlearn_library/intro_general_library/normalizers.py,12,"b""import autograd.numpy as np\n\nclass Setup:\n    def __init__(self,x,name):\n        normalizer = 0\n        inverse_normalizer = 0\n        if name == 'standard':\n            # create normalizer\n            self.normalizer, self.inverse_normalizer = self.standard_normalizer(x)\n            \n        elif name == 'sphere':\n            # create normalizer\n            self.normalizer, self.inverse_normalizer = self.PCA_sphereing(x)\n        else:\n            self.normalizer = lambda data: data\n            self.inverse_normalizer = lambda data: data\n            \n    # standard normalization function \n    def standard_normalizer(self,x):\n        # compute the mean and standard deviation of the input\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n        \n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(x_stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((x_stds.shape))\n            adjust[ind] = 1.0\n            x_stds += adjust\n\n        # create standard normalizer function\n        normalizer = lambda data: (data - x_means)/x_stds\n\n        # create inverse standard normalizer\n        inverse_normalizer = lambda data: data*x_stds + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer\n\n    # compute eigendecomposition of data covariance matrix for PCA transformation\n    def PCA(self,x,**kwargs):\n        # regularization parameter for numerical stability\n        lam = 10**(-7)\n        if 'lam' in kwargs:\n            lam = kwargs['lam']\n\n        # create the correlation matrix\n        P = float(x.shape[1])\n        Cov = 1/P*np.dot(x,x.T) + lam*np.eye(x.shape[0])\n\n        # use numpy function to compute eigenvalues / vectors of correlation matrix\n        d,V = np.linalg.eigh(Cov)\n        return d,V\n\n    # PCA-sphereing - use PCA to normalize input features\n    def PCA_sphereing(self,x,**kwargs):\n        # Step 1: mean-center the data\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_centered = x - x_means\n\n        # Step 2: compute pca transform on mean-centered data\n        d,V = self.PCA(x_centered,**kwargs)\n\n        # Step 3: divide off standard deviation of each (transformed) input, \n        # which are equal to the returned eigenvalues in 'd'.  \n        stds = (d[:,np.newaxis])**(0.5)\n        \n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((stds.shape))\n            adjust[ind] = 1.0\n            stds += adjust\n        \n        normalizer = lambda data: np.dot(V.T,data - x_means)/stds\n\n        # create inverse normalizer\n        inverse_normalizer = lambda data: np.dot(V,data*stds) + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer"""
mlrefined_libraries/nonlinear_superlearn_library/intro_general_library/optimizers.py,9,"b""import autograd.numpy as np\nfrom autograd import value_and_grad \nfrom autograd import hessian\nfrom autograd.misc.flatten import flatten_func\n\n# minibatch gradient descent\ndef gradient_descent(g, alpha, max_its, w, num_pts, batch_size,**kwargs):    \n    # flatten the input function, create gradient based on flat function    \n    g_flat, unflatten, w = flatten_func(g, w)\n    grad = value_and_grad(g_flat)\n    \n    # record history\n    w_hist = []\n    w_hist.append(unflatten(w))\n   \n    # how many mini-batches equal the entire dataset?\n    num_batches = int(np.ceil(np.divide(num_pts, batch_size)))\n    # over the line\n    for k in range(max_its):   \n        # loop over each minibatch\n        for b in range(num_batches):\n            # collect indices of current mini-batch\n            batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_pts))\n\n            # plug in value into func and derivative\n            cost_eval,grad_eval = grad(w,batch_inds)\n            grad_eval.shape = np.shape(w)\n            \n            # take descent step with momentum\n            w = w - alpha*grad_eval\n\n        # record weight update\n        w_hist.append(unflatten(w))\n\n    return w_hist\n\n# newtons method function - inputs: g (input function), max_its (maximum number of iterations), w (initialization)\ndef newtons_method(g,max_its,w,num_pts,batch_size,**kwargs):\n    # flatten input funciton, in case it takes in matrices of weights\n    flat_g, unflatten, w = flatten_func(g, w)\n    \n    # compute the gradient / hessian functions of our input function -\n    # note these are themselves functions.  In particular the gradient - \n    # - when evaluated - returns both the gradient and function evaluations (remember\n    # as discussed in Chapter 3 we always ge the function evaluation 'for free' when we use\n    # an Automatic Differntiator to evaluate the gradient)\n    gradient = value_and_grad(flat_g)\n    hess = hessian(flat_g)\n    \n    # set numericxal stability parameter / regularization parameter\n    epsilon = 10**(-7)\n    if 'epsilon' in kwargs:\n        epsilon = kwargs['epsilon']\n\n    # record history\n    w_hist = []\n    w_hist.append(unflatten(w))\n    \n    # how many mini-batches equal the entire dataset?\n    num_batches = int(np.ceil(np.divide(num_pts, batch_size)))\n    \n    # over the line\n    for k in range(max_its):   \n        # loop over each minibatch\n        for b in range(num_batches):\n            # collect indices of current mini-batch\n            batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_pts))\n            \n            # evaluate the gradient, store current weights and cost function value\n            cost_eval,grad_eval = gradient(w,batch_inds)\n\n            # evaluate the hessian\n            hess_eval = hess(w,batch_inds)\n\n            # reshape for numpy linalg functionality\n            hess_eval.shape = (int((np.size(hess_eval))**(0.5)),int((np.size(hess_eval))**(0.5)))\n\n            # solve second order system system for weight update\n            A = hess_eval + epsilon*np.eye(np.size(w))\n            b = grad_eval\n            w = np.linalg.lstsq(A,np.dot(A,w) - b)[0]\n\n            #w = w - np.dot(np.linalg.pinv(hess_eval + epsilon*np.eye(np.size(w))),grad_eval)\n            \n        # record weights after each epoch\n        w_hist.append(unflatten(w))\n\n    # collect final weights\n    w_hist.append(unflatten(w))\n    \n    return w_hist"""
mlrefined_libraries/nonlinear_superlearn_library/intro_general_library/polys.py,5,"b""import autograd.numpy as np\nimport copy\nimport itertools\n\nclass Setup:\n    def __init__(self,x,y,**kwargs):    \n        # get desired degree\n        self.D = kwargs['degree']\n        self.N = x.shape[0]\n        \n        # all monomial terms degrees\n        degs = np.array(list(itertools.product(list(np.arange(self.D+1)), repeat = self.N)))\n        b = np.sum(degs,axis = 1)\n        ind = np.argwhere(b <= self.D)\n        ind = [v[0] for v in ind]\n        degs = degs[ind,:]     \n        self.degs = degs[1:,:]\n\n        # define initializer\n        self.num_classifiers = 1\n        if 'num_classifiers' in kwargs:\n            self.num_classifiers = kwargs['num_classifiers']\n        self.scale = 0.1\n        if 'scale' in kwargs:\n            self.scale = kwargs['scale']\n\n    # create initial weights for arbitrary feedforward network\n    def initializer(self):\n        w_init = self.scale*np.random.randn(len(self.degs)+1,self.num_classifiers);\n        return w_init\n    \n    # compute transformation on entire set of inputs\n    def feature_transforms(self,x): \n        x_transformed = np.array([np.prod(x**v[:,np.newaxis],axis = 0)[:,np.newaxis] for v in self.degs])[:,:,0]         \n        return x_transformed"""
mlrefined_libraries/nonlinear_superlearn_library/intro_general_library/sines.py,5,"b""import autograd.numpy as np\nimport copy\nimport itertools\n\nclass Setup:\n    def __init__(self,x,y,**kwargs):    \n        # get desired degree\n        self.D = kwargs['degree']\n        self.N = x.shape[0]\n        \n        # all monomial terms degrees\n        degs = np.array(list(itertools.product(list(np.arange(self.D+1)), repeat = self.N)))\n        b = np.sum(degs,axis = 1)\n        ind = np.argwhere(b <= self.D)\n        ind = [v[0] for v in ind]\n        degs = degs[ind,:]     \n        self.degs = degs[1:,:]\n\n        # define initializer\n        self.num_classifiers = 1\n        if 'num_classifiers' in kwargs:\n            self.num_classifiers = kwargs['num_classifiers']\n        self.scale = 0.1\n        if 'scale' in kwargs:\n            self.scale = kwargs['scale']\n\n    # create initial weights for arbitrary feedforward network\n    def initializer(self):\n        w_init = self.scale*np.random.randn(len(self.degs)+1,self.num_classifiers);\n        return w_init\n    \n    # compute transformation on entire set of inputs\n    def feature_transforms(self,x): \n        x_transformed = np.array([np.prod(x**v[:,np.newaxis],axis = 0)[:,np.newaxis] for v in self.degs])[:,:,0]         \n        return x_transformed"""
mlrefined_libraries/nonlinear_superlearn_library/intro_general_library/stumps.py,5,"b""import autograd.numpy as np\nimport copy\n\nclass Setup:\n    def __init__(self,x,y,**kwargs):        \n        # create splits, levels, and dims\n        self.splits,self.levels,self.dims =  self.create_boost_stumps(x,y)\n\n        # define initializer\n        self.num_classifiers = 1\n        if 'num_classifiers' in kwargs:\n            self.num_classifiers = kwargs['num_classifiers']\n        self.scale = 0.1\n        if 'scale' in kwargs:\n            self.scale = kwargs['scale']\n\n    # create initial weights for arbitrary feedforward network\n    def initializer(self):\n        w_init = np.zeros((len(self.splits)+1,self.num_classifiers));\n        return w_init\n    \n    # compute transformation on entire set of inputs\n    def feature_transforms(self,x): \n        # container for stump transformed data\n        N = x.shape[0]\n        P = x.shape[1]\n        S = len(self.splits)\n        x_transformed = np.zeros((S,P))\n\n        # loop over points and transform each individually\n        for pt in range(P):\n            x_n = x[:,pt]\n\n            # loop over the stump collectionand calculate weighted contribution\n            for u in range(len(self.splits)):\n                # get current stump f_u\n                split = self.splits[u]\n                level = self.levels[u]\n                dim = self.dims[u]\n\n                ### our stump function f_u(x)\n                if x_n[dim] <= split:  # lies to the left - so evaluate at left level\n                    x_transformed[u][pt] = level[0]\n                else:\n                    x_transformed[u][pt]  = level[1]\n        return x_transformed\n\n    def create_boost_stumps(self,x,y):\n        '''\n        Create stumps tailored to an input dataset (x,y) based on the naive method of creating\n        a split point between each pair of successive inputs.  \n\n        The input to this function: a dataset (x,y) where the input x has shape \n        (NUMBER OF POINTS by  DIMENSION OF INPUT)\n\n        The output of this function is a set of two lists, one containing the split points and \n        the other the corresponding levels of stumps.\n        '''\n\n        # containers for the split points and levels of our stumps, along with container\n        # for which dimension the stump is defined along\n        splits = []\n        levels = []\n        dims = []\n\n        # important constants: dimension of input N and total number of points P\n        N = np.shape(x)[0]              \n        P = np.size(y)\n\n        ### begin outer loop - loop over each dimension of the input\n        for n in range(N):\n            # make a copy of the n^th dimension of the input data (we will sort after this)\n            x_n = copy.deepcopy(x[n,:])\n            y_n = copy.deepcopy(y)\n\n            # sort x_n and y_n according to ascending order in x_n\n            sorted_inds = np.argsort(x_n,axis = 0)\n            x_n = x_n[sorted_inds]\n            y_n = y_n[:,sorted_inds]\n\n            # loop over points and create stump in between each \n            # in dimension n\n            for p in range(P - 1):\n                # compute split point\n                split = (x_n[p] + x_n[p+1])/float(2)\n\n                ### create non-zero stump to 'left' of split ###\n                # compute and store split point\n                splits.append(split)\n                levels.append([1,0])\n                dims.append(n)\n\n                ### create non-zero stump to 'right' of split ###\n                # compute and store split point\n                splits.append(split)\n                levels.append([0,1])\n                dims.append(n)\n\n        # return items\n        return splits,levels,dims"""
mlrefined_libraries/nonlinear_superlearn_library/intro_general_library/superlearn_setup.py,6,"b""import autograd.numpy as np\nfrom . import optimizers \nfrom . import cost_functions\nfrom . import normalizers\nfrom . import multilayer_perceptron\nfrom . import multilayer_perceptron_batch_normalized\nfrom . import stumps\nfrom . import polys\nfrom . import history_plotters\n\nclass Setup:\n    def __init__(self,x,y,**kwargs):\n        # link in data\n        self.x = x\n        self.y = y\n        \n        # make containers for all histories\n        self.weight_histories = []\n        self.train_cost_histories = []\n        self.train_count_histories = []\n        self.valid_cost_histories = []\n        self.valid_count_histories = []\n        \n    #### define feature transformation ####\n    def choose_features(self,name,**kwargs):         \n        # multilayer perceptron #\n        if name == 'multilayer_perceptron':\n            self.transformer = multilayer_perceptron.Setup(**kwargs)\n            self.feature_transforms = self.transformer.feature_transforms\n            self.initializer = self.transformer.initializer\n            self.layer_sizes = self.transformer.layer_sizes\n            \n        if name == 'multilayer_perceptron_batch_normalized':\n            self.transformer = multilayer_perceptron_batch_normalized.Setup(**kwargs)\n            self.feature_transforms = self.transformer.feature_transforms\n            self.initializer = self.transformer.initializer\n            self.layer_sizes = self.transformer.layer_sizes\n            \n        # stumps #\n        if name == 'stumps':\n            self.transformer = stumps.Setup(self.x,self.y,**kwargs)\n            self.feature_transforms = self.transformer.feature_transforms\n            self.initializer = self.transformer.initializer\n            \n        # polynomials #\n        if name == 'polys':\n            self.transformer = polys.Setup(self.x,self.y,**kwargs)\n            self.feature_transforms = self.transformer.feature_transforms\n            self.initializer = self.transformer.initializer\n            self.degs = self.transformer.D\n            \n        # input a custom feature transformation\n        if name == 'custom':\n            self.feature_transforms = kwargs['feature_transforms']\n            self.initializer = kwargs['initializer']\n        self.feature_name = name\n\n    #### define normalizer ####\n    def choose_normalizer(self,name):\n        # produce normalizer / inverse normalizer\n        s = normalizers.Setup(self.x,name)\n        self.normalizer = s.normalizer\n        self.inverse_normalizer = s.inverse_normalizer\n        \n        # normalize input \n        self.x = self.normalizer(self.x)\n        self.normalizer_name = name\n       \n    #### split data into training and validation sets ####\n    def make_train_valid_split(self,train_portion):\n        # translate desired training portion into exact indecies\n        r = np.random.permutation(self.x.shape[1])\n        train_num = int(np.round(train_portion*len(r)))\n        self.train_inds = r[:train_num]\n        self.valid_inds = r[train_num:]\n        \n        # define training and validation sets\n        self.x_train = self.x[:,self.train_inds]\n        self.x_valid = self.x[:,self.valid_inds]\n        \n        self.y_train = self.y[:,self.train_inds]\n        self.y_valid = self.y[:,self.valid_inds]\n\n\n    #### define cost function ####\n    def choose_cost(self,name,**kwargs):\n        self.lam = 0\n        if 'lam' in kwargs:\n            self.lam = kwargs['lam']\n\n        # create cost on entire dataset\n        funcs = cost_functions.Setup(name,self.x,self.y,self.feature_transforms,**kwargs)\n        self.full_cost = funcs.cost\n        self.full_model = funcs.model\n\n        # create training and testing cost functions\n        funcs = cost_functions.Setup(name,self.x_train,self.y_train,self.feature_transforms,**kwargs)\n        self.cost = funcs.cost\n        self.model = funcs.model\n        \n        funcs = cost_functions.Setup(name,self.x_valid,self.y_valid,self.feature_transforms,**kwargs)\n        self.valid_cost = funcs.cost\n        \n        # if the cost function is a two-class classifier, build a counter too\n        if name == 'softmax' or name == 'perceptron':\n            funcs = cost_functions.Setup('twoclass_counter',self.x_train,self.y_train,self.feature_transforms,**kwargs)\n            self.counter = funcs.cost\n            \n            funcs = cost_functions.Setup('twoclass_counter',self.x_valid,self.y_valid,self.feature_transforms,**kwargs)\n            self.valid_counter = funcs.cost\n            \n        if name == 'multiclass_softmax' or name == 'multiclass_perceptron':\n            funcs = cost_functions.Setup('multiclass_counter',self.x_train,self.y_train,self.feature_transforms,**kwargs)\n            self.counter = funcs.cost\n            \n            funcs = cost_functions.Setup('multiclass_counter',self.x_valid,self.y_valid,self.feature_transforms,**kwargs)\n            self.valid_counter = funcs.cost\n            \n        self.cost_name = name\n            \n    #### run optimization ####\n    def fit(self,**kwargs):\n        # basic parameters for gradient descent run (default algorithm)\n        max_its = 500; alpha_choice = 10**(-1);\n        self.w_init = self.initializer()\n        optimizer = 'gradient_descent'\n        epsilon = 10**(-10)\n        \n        # set parameters by hand\n        if 'max_its' in kwargs:\n            self.max_its = kwargs['max_its']\n        if 'alpha_choice' in kwargs:\n            self.alpha_choice = kwargs['alpha_choice']\n        if 'optimizer' in kwargs:\n            optimizer = kwargs['optimizer']\n        if 'epsilon' in kwargs:\n            epsilon = kwargs['epsilon']\n            \n        # batch size for gradient descent?\n        self.num_pts = np.size(self.y_train)\n        self.batch_size = np.size(self.y_train)\n        if 'batch_size' in kwargs:\n            self.batch_size = kwargs['batch_size']\n\n        # optimize\n        weight_history = []\n        \n        # run gradient descent\n        if optimizer == 'gradient_descent':\n            weight_history = optimizers.gradient_descent(self.cost,self.alpha_choice,self.max_its,self.w_init,self.num_pts,self.batch_size)\n        \n        if optimizer == 'newtons_method':\n            weight_history = optimizers.newtons_method(self.cost,self.max_its,self.w_init,self.num_pts,self.batch_size,epsilon = epsilon)\n                    \n        # compute training and testing cost histories\n        train_cost_history = [self.cost(v,np.arange(np.size(self.y_train))) for v in weight_history]\n        valid_cost_history = [self.valid_cost(v,np.arange(np.size(self.y_valid))) for v in weight_history]\n        \n\n        # store all new histories\n        self.weight_histories.append(weight_history)\n        self.train_cost_histories.append(train_cost_history)\n        self.valid_cost_histories.append(valid_cost_history)\n        \n        # if classification produce count history\n        if self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n            train_count_history = [self.counter(v) for v in weight_history]\n            valid_count_history = [self.valid_counter(v) for v in weight_history]\n\n            # store count history\n            self.train_count_histories.append(train_count_history)\n            self.valid_count_histories.append(valid_count_history)\n \n    #### plot histories ###\n    def show_histories(self,**kwargs):\n        start = 0\n        if 'start' in kwargs:\n            start = kwargs['start']\n        history_plotters.Setup(self.train_cost_histories,self.train_count_histories,self.valid_cost_histories,self.test_count_histories,start)\n        \n    #### for batch normalized multilayer architecture only - set normalizers to desired settings ####\n    def fix_normalizers(self,w):\n        ### re-set feature transformation ###        \n        # fix normalization at each layer by passing data and specific weight through network\n        self.feature_transforms(self.x,w);\n        \n        # re-assign feature transformation based on these settings\n        self.feature_transforms_validation = self.transformer.feature_transforms_validation\n        \n        ### re-assign cost function (and counter) based on fixed architecture ###\n        funcs = cost_functions.Setup(self.cost_name,self.x,self.y,self.feature_transforms_validation)\n        self.model = funcs.model\n    """
mlrefined_libraries/nonlinear_superlearn_library/intro_general_library/unsuperlearn_setup.py,0,"b""import autograd.numpy as np\nfrom . import optimizers \nfrom . import cost_functions\nfrom . import normalizers\nfrom . import multilayer_perceptron\nfrom . import history_plotters\n\nclass Setup:\n    def __init__(self,X,**kwargs):\n        # link in data\n        self.x = X\n        \n        # make containers for all histories\n        self.weight_histories = []\n        self.cost_histories = []\n        self.count_histories = []\n        \n    #### define feature transformation ####\n    def choose_encoder(self,**kwargs): \n        # select from pre-made feature transforms\n        # form encoder\n        transformer = multilayer_perceptron.Setup(**kwargs)\n        self.feature_transforms = transformer.feature_transforms\n        self.initializer_1 = transformer.initializer\n        self.layer_sizes_encoder = transformer.layer_sizes\n        \n    def choose_decoder(self,**kwargs): \n        # form decoder\n        transformer = multilayer_perceptron.Setup(**kwargs)\n        self.feature_transforms_2 = transformer.feature_transforms\n        self.initializer_2 = transformer.initializer\n        self.layer_sizes_decoder = transformer.layer_sizes\n\n    #### define normalizer ####\n    def choose_normalizer(self,name):\n        # produce normalizer / inverse normalizer\n        s = normalizers.Setup(self.x,name)\n        self.normalizer = s.normalizer\n        self.inverse_normalizer = s.inverse_normalizer\n        \n        # normalize input \n        self.x = self.normalizer(self.x)\n        self.normalizer_name = name\n     \n    #### define cost function ####\n    def choose_cost(self,name,**kwargs):\n        # pick cost based on user input\n        funcs = cost_functions.Setup(name,self.x,[],self.feature_transforms,feature_transforms_2 = self.feature_transforms_2,**kwargs)\n        self.cost = funcs.cost\n        self.encoder = funcs.encoder\n        self.decoder = funcs.decoder\n        self.cost_name = name\n            \n    #### run optimization ####\n    def fit(self,**kwargs):\n        # basic parameters for gradient descent run (default algorithm)\n        max_its = 500; alpha_choice = 10**(-1);\n        self.w_init_1 = self.initializer_1()\n        self.w_init_2 = self.initializer_2()\n        self.w_init = [self.w_init_1,self.w_init_2]\n        \n        # set parameters by hand\n        if 'max_its' in kwargs:\n            self.max_its = kwargs['max_its']\n        if 'alpha_choice' in kwargs:\n            self.alpha_choice = kwargs['alpha_choice']\n        if 'w' in kwargs:\n            self.w_init = kwargs['w']\n\n        # run gradient descent\n        self.weight_history, self.cost_history = optimizers.gradient_descent(self.cost,self.alpha_choice,self.max_its,self.w_init)\n        \n         # store all new histories\n        self.weight_histories.append(self.weight_history)\n        self.cost_histories.append(self.cost_history)\n        \n    #### plot histories ###\n    def show_histories(self,**kwargs):\n        start = 0\n        if 'start' in kwargs:\n            start = kwargs['start']\n            \n        # if labels not in input argument, make blank labels\n        labels = []\n        for c in range(len(self.cost_histories)):\n            labels.append('')\n        if 'labels' in kwargs:\n            labels = kwargs['labels']\n        history_plotters.Setup(self.cost_histories,self.count_histories,start,labels)\n        \n    """
mlrefined_libraries/superlearn_library/boost_lib/__init__.py,0,b''
mlrefined_libraries/superlearn_library/boost_lib/cost_functions.py,19,"b""import autograd.numpy as np\nfrom inspect import signature\n\nclass Setup:\n    def __init__(self,name):             \n        ### make cost function choice ###\n        # for regression\n        if name == 'least_squares':\n            self.cost = self.least_squares\n        if name == 'least_absolute_deviations':\n            self.cost = self.least_absolute_deviations\n            \n        # for two-class classification\n        if name == 'softmax':\n            self.cost = self.softmax\n        if name == 'perceptron':\n            self.cost = self.perceptron\n        if name == 'twoclass_counter':\n            self.cost = self.counting_cost\n            \n        # for multiclass classification\n        if name == 'multiclass_perceptron':\n            self.cost = self.multiclass_perceptron\n        if name == 'multiclass_softmax':\n            self.cost = self.multiclass_softmax\n        if name == 'multiclass_counter':\n            self.cost = self.multiclass_counting_cost\n            \n    ###### cost functions #####\n    # set model\n    def set_model(self,model):\n        self.model = model\n    \n    ###### regression costs #######\n    # an implementation of the least squares cost function for linear regression\n    def least_squares(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # compute cost\n        cost = np.sum((self.model(x_p,w) - y_p)**2)\n        return cost/float(np.size(y_p))\n\n    # a compact least absolute deviations cost function\n    def least_absolute_deviations(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # compute cost\n        cost = np.sum(np.abs(self.model(x_p,w) - y_p))\n        return cost/float(np.size(y_p))\n\n    ###### two-class classification costs #######\n    # the convex softmax cost function\n    def softmax(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.log(1 + np.exp(-y_p*(self.model(x_p,w)))))\n        return cost/float(np.size(y_p))\n\n    # the convex relu cost function\n    def relu(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.maximum(0,-y_p*self.model(x_p,w)))\n        return cost/float(np.size(y_p))\n    \n    # the counting cost function\n    def counting_cost(self,w,x,y,iter):\n        cost = np.sum(np.abs(np.sign(self.model(x,w)) - self.y))\n        return cost \n\n    ###### multiclass classification costs #######\n    # multiclass perceptron\n    def multiclass_perceptron(self,w,x,y,iter):\n        # get subset of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n\n        # pre-compute predictions on all points\n        all_evals = self.model(x_p,w)\n\n        # compute maximum across data points\n        a =  np.max(all_evals,axis = 0)        \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n        cost = np.sum(a - b)\n\n        # return average\n        return cost/float(np.size(y_p))\n\n    # multiclass softmax\n    def multiclass_softmax(self,w,x,y,iter):\n        # get subset of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # pre-compute predictions on all points\n        all_evals = self.model(x_p,w)\n\n        # compute softmax across data points\n        a = np.log(np.sum(np.exp(all_evals),axis = 0)) \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n        cost = np.sum(a - b)\n\n        # return average\n        return cost/float(np.size(y_p))\n\n    # multiclass misclassification cost function - aka the fusion rule\n    def multiclass_counting_cost(self,w,x,y,iter):            \n        # pre-compute predictions on all points\n        all_evals = self.model(x,w)\n\n        # compute predictions of each input point\n        y_predict = (np.argmax(all_evals,axis = 0))[np.newaxis,:]\n\n        # compare predicted label to actual label\n        count = np.sum(np.abs(np.sign(y - y_predict)))\n\n        # return number of misclassifications\n        return count"""
mlrefined_libraries/superlearn_library/boost_lib/normalizers.py,13,"b""import autograd.numpy as np\n\nclass Setup:\n    def __init__(self,x,name):\n        normalizer = 0\n        inverse_normalizer = 0\n        if name == 'standard':\n            # create normalizer\n            self.normalizer, self.inverse_normalizer = self.standard_normalizer(x)\n            \n        elif name == 'sphere':\n            # create normalizer\n            self.normalizer, self.inverse_normalizer = self.PCA_sphereing(x)\n        else:\n            self.normalizer = lambda data: data\n            self.inverse_normalizer = lambda data: data\n \n    # standard normalization function - with nan checker / filler in-er\n    def standard_normalizer(self,x):    \n        # compute the mean and standard deviation of the input\n        x_means = np.nanmean(x,axis = 1)[:,np.newaxis]\n        x_stds = np.nanstd(x,axis = 1)[:,np.newaxis]   \n\n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(x_stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((x_stds.shape))\n            adjust[ind] = 1.0\n            x_stds += adjust\n\n        # fill in any nan values with means \n        ind = np.argwhere(np.isnan(x) == True)\n        for i in ind:\n            x[i[0],i[1]] = x_means[i[0]]\n\n        # create standard normalizer function\n        normalizer = lambda data: (data - x_means)/x_stds\n\n        # create inverse standard normalizer\n        inverse_normalizer = lambda data: data*x_stds + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer\n\n    # compute eigendecomposition of data covariance matrix for PCA transformation\n    def PCA(self,x,**kwargs):\n        # regularization parameter for numerical stability\n        lam = 10**(-7)\n        if 'lam' in kwargs:\n            lam = kwargs['lam']\n\n        # create the correlation matrix\n        P = float(x.shape[1])\n        Cov = 1/P*np.dot(x,x.T) + lam*np.eye(x.shape[0])\n\n        # use numpy function to compute eigenvalues / vectors of correlation matrix\n        d,V = np.linalg.eigh(Cov)\n        return d,V\n\n    # PCA-sphereing - use PCA to normalize input features\n    def PCA_sphereing(self,x,**kwargs):\n        # Step 1: mean-center the data\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_centered = x - x_means\n\n        # Step 2: compute pca transform on mean-centered data\n        d,V = self.PCA(x_centered,**kwargs)\n\n        # Step 3: divide off standard deviation of each (transformed) input, \n        # which are equal to the returned eigenvalues in 'd'.  \n        stds = (d[:,np.newaxis])**(0.5)\n        \n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((stds.shape))\n            adjust[ind] = 1.0\n            stds += adjust\n        \n        normalizer = lambda data: np.dot(V.T,data - x_means)/stds\n\n        # create inverse normalizer\n        inverse_normalizer = lambda data: np.dot(V,data*stds) + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer"""
mlrefined_libraries/superlearn_library/boost_lib/optimizers.py,13,"b""import autograd.numpy as np\nfrom autograd import value_and_grad \nfrom autograd import hessian\nfrom autograd.misc.flatten import flatten_func\n\n#### optimizers ####\n# minibatch gradient descent\ndef gradient_descent(g,w,x,y,alpha_choice,max_its,batch_size): \n    # flatten the input function, create gradient based on flat function\n    g_flat, unflatten, w = flatten_func(g, w)\n    grad = value_and_grad(g_flat)\n\n    # record history\n    num_train = y.size\n    w_hist = [unflatten(w)]\n    train_hist = [g_flat(w,x,y,np.arange(num_train))]\n\n    # how many mini-batches equal the entire dataset?\n    num_batches = int(np.ceil(np.divide(num_train, batch_size)))\n\n    # over the line\n    alpha = 0\n    print ('grads')\n\n    for k in range(max_its):             \n        # check if diminishing steplength rule used\n        if alpha_choice == 'diminishing':\n            alpha = 1/float(k)\n        else:\n            alpha = alpha_choice\n            \n        for b in range(num_batches):\n            # collect indices of current mini-batch\n            batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_train))\n\n            # plug in value into func and derivative\n            cost_eval,grad_eval = grad(w,x,y,batch_inds)\n            grad_eval.shape = np.shape(w)\n\n            # take descent step with momentum\n            w = w - alpha*grad_eval\n\n        # update training and validation cost\n        train_cost = g_flat(w,x,y,np.arange(num_train))\n\n        # record weight update, train and val costs\n        w_hist.append(unflatten(w))\n        train_hist.append(train_cost)\n    return w_hist,train_hist\n\n# newtons method function\ndef newtons_method(g,w,x,y,max_its,**kwargs): \n    # flatten input funciton, in case it takes in matrices of weights\n    g_flat, unflatten, w = flatten_func(g, w)\n    \n    # compute the gradient / hessian functions of our input function\n    grad = value_and_grad(g_flat)\n    hess = hessian(g_flat)\n    \n    # set numericxal stability parameter / regularization parameter\n    epsilon = 10**(-7)\n    if 'epsilon' in kwargs:\n        epsilon = kwargs['epsilon']\n    \n    # record history\n    num_train = y.size\n    w_hist = [unflatten(w)]\n    train_hist = [g_flat(w,x,y,np.arange(num_train))]\n\n    # over the line\n    for k in range(max_its):   \n        # evaluate the gradient, store current weights and cost function value\n        cost_eval,grad_eval = grad(w,x,y,np.arange(num_train))\n\n        # evaluate the hessian\n        hess_eval = hess(w,x,y,np.arange(num_train))\n\n        # reshape for numpy linalg functionality\n        hess_eval.shape = (int((np.size(hess_eval))**(0.5)),int((np.size(hess_eval))**(0.5)))\n\n        # solve second order system system for weight update\n        A = hess_eval + epsilon*np.eye(np.size(w))\n        b = grad_eval\n        w = np.linalg.lstsq(A,np.dot(A,w) - b)[0]\n\n        #w = w - np.dot(np.linalg.pinv(hess_eval + epsilon*np.eye(np.size(w))),grad_eval)\n            \n        # update training and validation cost\n        train_cost = g_flat(w,x,y,np.arange(num_train))\n\n        # record weight update, train and val costs\n        w_hist.append(unflatten(w))\n        train_hist.append(train_cost)\n    \n    return w_hist,train_hist"""
mlrefined_libraries/superlearn_library/boost_lib/superlearn_setup.py,13,"b""import autograd.numpy as np\nfrom . import optimizers \nfrom . import cost_functions\nfrom . import normalizers\nimport copy\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nclass Setup:\n    def __init__(self,x,y,**kwargs):\n        # link in data\n        self.x_orig = x\n        self.y_orig = y\n        \n        # make containers for all histories\n        self.weight_histories = []\n        self.train_cost_histories = []\n        self.train_count_histories = []\n        self.valid_cost_histories = []\n        self.valid_count_histories = []\n\n    #### define normalizer ####\n    def choose_normalizer(self,name):       \n        # produce normalizer / inverse normalizer\n        s = normalizers.Setup(self.x_orig,name)\n        self.normalizer = s.normalizer\n        self.inverse_normalizer = s.inverse_normalizer\n        \n        # normalize input \n        self.x = self.normalizer(self.x_orig)\n        self.normalizer_name = name\n        \n        # produce normalizer / inverse normalizer\n        s = normalizers.Setup(self.y_orig,name)\n        self.normalizer = s.normalizer\n        self.inverse_normalizer = s.inverse_normalizer\n        \n        # normalize input \n        self.y = self.normalizer(self.y_orig)\n  \n    #### define cost function ####\n    def choose_cost(self,name,**kwargs):\n        # create cost on entire dataset\n        self.cost = cost_functions.Setup(name)\n                \n        # if the cost function is a two-class classifier, build a counter too\n        if name == 'softmax' or name == 'perceptron':\n            funcs = cost_functions.Setup('twoclass_counter')\n            self.counter = funcs.cost\n            \n        if name == 'multiclass_softmax' or name == 'multiclass_perceptron':\n            funcs = cost_functions.Setup('multiclass_counter')\n            self.counter = funcs.cost\n            \n        self.cost_name = name\n            \n    #### setup optimization ####\n    def choose_optimizer(self,optimizer_name,**kwargs):\n        # general params for optimizers\n        max_its = 500; \n        alpha_choice = 10**(-1);\n        epsilon = 10**(-10)\n        \n        # set parameters by hand\n        if 'max_its' in kwargs:\n            max_its = kwargs['max_its']\n        if 'alpha_choice' in kwargs:\n            alpha_choice = kwargs['alpha_choice']\n        if 'epsilon' in kwargs:\n            epsilon = kwargs['epsilon']\n            \n        # batch size for gradient descent?\n        self.w = 0.0*np.random.randn(self.x.shape[0] + 1,1)\n        num_pts = np.size(self.y)\n        batch_size = np.size(self.y)\n        if 'batch_size' in kwargs:\n            self.batch_size = kwargs['batch_size']\n        \n        # run gradient descent\n        if optimizer_name == 'gradient_descent':\n            self.optimizer = lambda cost,x,w: optimizers.gradient_descent(cost,w,x,self.y,alpha_choice,max_its,batch_size)\n        \n        if optimizer_name == 'newtons_method':\n            self.optimizer = lambda cost,x,w: optimizers.newtons_method(cost,w,x,self.y,max_its,epsilon=epsilon)\n       \n    ### boost it ###\n    def boost(self,**kwargs):\n        # choose number of rounds\n        num_rounds = self.x.shape[0]\n        if 'num_rounds' in kwargs:\n            num_rounds = min(kwargs['num_rounds'],self.x.shape[0])\n              \n        # reset initialization\n        self.w = 0.0*np.random.randn(self.x.shape[0] + 1,1)\n\n        # container for models and cost function histories\n        self.models = []\n        self.cost_vals = []\n        self.weight_vals = []\n        \n        # tune bias\n        model_0 = lambda x,w: w\n        self.cost.set_model(model_0)\n        w_hist,c_hist = self.optimizer(self.cost.cost,self.x,self.w[0])\n\n        # determine smallest cost value attained\n        ind = np.argmin(c_hist)\n        self.w[0] = w_hist[ind][0]\n        self.cost_vals.append(c_hist[ind])\n        self.weight_vals.append(self.w[0])\n        \n        # lock in model_0 value\n        model_0 = copy.deepcopy(self.w[0])\n        \n        self.models.append(model_0)\n        cost_val = c_hist[ind]\n        \n        # loop over feature-touching weights and update one at a time\n        model = lambda x,w: x*w\n        model_m = lambda x,w: self.models[0] + model(x,w)\n        \n        # index sets to keep track of which feature-touching weights have been used\n        # thus far\n        used = [0]\n        unused = {i for i in range(1,self.x.shape[0]+1)}\n        \n        for i in range(num_rounds):\n            # loop over unused indices and try out each remaining corresponding weight\n            best_weight = 0\n            best_cost = np.inf\n            best_ind = 0\n            for n in unused:\n                # construct model to test\n                current_model = lambda x,w: self.models[-1] + model(x,w)\n\n                # load in current model\n                self.cost.set_model(current_model)\n                w_hist,c_hist = self.optimizer(self.cost.cost,self.x[n-1,:][np.newaxis,:],self.w[n])\n\n                # determine smallest cost value attained\n                ind = np.argmin(c_hist)            \n                weight = w_hist[ind]\n                cost_val = c_hist[ind]\n\n                # update smallest cost val / associated weight\n                if cost_val < best_cost:\n                    best_weight = weight\n                    best_cost = cost_val\n                    best_ind = n\n\n            # after sweeping through and computing minimum for all subproblems\n            # update the best weight value\n            self.w[best_ind] = best_weight\n            self.cost_vals.append(best_cost)\n            self.weight_vals.append(self.w[best_ind])\n            \n            # fix next model\n            model_m = self.models[-1] + self.x[best_ind-1,:][np.newaxis,:]*self.w[best_ind]\n            self.models.append(model_m)\n\n            # update current model\n            model_m = lambda x,w: self.models[-1](x) + model(x,w)\n\n            # remove best index from unused set, add to used set\n            unused -= {best_ind}\n            used.append(best_ind)\n            \n        # make universals\n        self.used = used\n        \n    #### plotting functionality ###\n    def plot_history(self):\n        # colors for plotting\n        colors = [[0,0.7,1],[1,0.8,0.5]]\n\n        # initialize figure\n        fig = plt.figure(figsize = (10,5.5))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(3, 1,height_ratios = [1,0.1,1]) \n        ax = plt.subplot(gs[0]); \n        \n        ### plot history val ###\n        ax.plot(self.cost_vals,linewidth = 2,color = colors[0]) \n        ax.scatter(np.arange(len(self.cost_vals)).flatten(),self.cost_vals,s = 70,color = colors[0],edgecolor = 'k',linewidth = 1,zorder = 5) \n\n        # change tick labels to used\n        ax.set_xticks(np.arange(len(self.cost_vals)))\n        ax.set_xticklabels(self.used)\n        \n        # clean up panel / axes labels\n        xlabel = 'weight index'\n        ylabel = 'cost value'\n        title = 'cost value at each round of boosting'\n        ax.set_xlabel(xlabel,fontsize = 14)\n        ax.set_ylabel(ylabel,fontsize = 14,rotation = 90,labelpad = 25)\n        ax.set_title(title,fontsize = 16)\n        \n        # histogram plot of each non-bias weight\n        ax.axhline(c='k',zorder = 2)\n            \n        ### make bar plot ###\n        ax = plt.subplot(gs[1]); ax.axis('off')\n        ax = plt.subplot(gs[2]); \n        ax.bar(np.arange(1,len(self.weight_vals)), [w[0] for w in self.weight_vals[1:]], color='k', alpha=0.5)\n        ax.axhline(c='k',zorder = 2)\n        \n        # change tick labels to used\n        ax.set_xticks(np.arange(1,len(self.cost_vals)))\n        ax.set_xticklabels(self.used[1:])\n        \n        # dress panel\n        xlabel = 'weight index'\n        ylabel = 'weight value'\n        title = 'weight values learned by boosting'\n        ax.set_xlabel(xlabel,fontsize = 14)\n        ax.set_ylabel(ylabel,fontsize = 14,rotation = 90,labelpad = 25)\n        ax.set_title(title,fontsize = 15)\n        \n    # static graphics\n    def plot_regress(self,id1,labels):\n        # initialize figure\n        fig = plt.figure(figsize = (9,3))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); \n        \n        # scatter plot\n        ax.scatter(self.x[id1-1,:],self.y,color = 'k',edgecolor = 'w',s = 30)\n    \n        # dress panel\n        ax.set_xlabel(labels[0])\n        ax.set_ylabel(labels[1])"""
mlrefined_libraries/superlearn_library/reg_lib/__init__.py,0,b''
mlrefined_libraries/superlearn_library/reg_lib/cost_functions.py,22,"b""import autograd.numpy as np\nfrom inspect import signature\n\nclass Setup:\n    def __init__(self,cost_name,reg_name):             \n        ### make cost function choice ###\n        # for regression\n        if cost_name == 'least_squares':\n            self.cost = self.least_squares\n        if cost_name == 'least_absolute_deviations':\n            self.cost = self.least_absolute_deviations\n            \n        # for two-class classification\n        if cost_name == 'softmax':\n            self.cost = self.softmax\n        if cost_name == 'perceptron':\n            self.cost = self.perceptron\n        if cost_name == 'twoclass_counter':\n            self.cost = self.counting_cost\n            \n        # for multiclass classification\n        if cost_name == 'multiclass_perceptron':\n            self.cost = self.multiclass_perceptron\n        if cost_name == 'multiclass_softmax':\n            self.cost = self.multiclass_softmax\n        if cost_name == 'multiclass_counter':\n            self.cost = self.multiclass_counting_cost\n            \n        # choose regularizer\n        self.lam = 0\n        if reg_name == 'L2':\n            self.reg = self.L2\n        if reg_name == 'L1':\n            self.reg = self.L1\n            \n    ### regularizers ###\n    def L1(self,w):\n         return self.lam*np.sum(np.abs(w[1:]))\n  \n    def L2(self,w):\n         return self.lam*np.sum((w[1:])**2)\n        \n    # set lambda value (regularization penalty)\n    def set_lambda(self,lam):\n        self.lam = lam\n        \n    ### setup model ###\n    def model(self,x,w):\n        a = w[0] + np.dot(x.T,w[1:])\n        return a.T\n    \n    ###### regression costs #######\n    # an implementation of the least squares cost function for linear regression\n    def least_squares(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # compute cost\n        cost = np.sum((self.model(x_p,w) - y_p)**2)\n        \n        # add regularizer \n        cost += self.reg(w)\n        \n        # return average\n        return cost/float(np.size(y_p))\n\n    # a compact least absolute deviations cost function\n    def least_absolute_deviations(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # compute cost\n        cost = np.sum(np.abs(self.model(x_p,w) - y_p))\n        \n        # add regularizer \n        cost += self.reg(w)\n        \n        # return average\n        return cost/float(np.size(y_p))\n\n    ###### two-class classification costs #######\n    # the convex softmax cost function\n    def softmax(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.log(1 + np.exp(-y_p*(self.model(x_p,w)))))\n        \n        # add regularizer \n        cost += self.reg(w)\n        \n        # return average\n        return cost/float(np.size(y_p))\n\n    # the convex relu cost function\n    def relu(self,w,x,y,iter):\n        # get batch of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # compute cost over batch\n        cost = np.sum(np.maximum(0,-y_p*self.model(x_p,w)))\n        \n        # add regularizer \n        cost += self.reg(w)\n        \n        # return average\n        return cost/float(np.size(y_p))\n    \n    # the counting cost function\n    def counting_cost(self,w,x,y,iter):\n        cost = np.sum(np.abs(np.sign(self.model(x,w)) - self.y))\n        return cost \n\n    ###### multiclass classification costs #######\n    # multiclass perceptron\n    def multiclass_perceptron(self,w,x,y,iter):\n        # get subset of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n\n        # pre-compute predictions on all points\n        all_evals = self.model(x_p,w)\n\n        # compute maximum across data points\n        a =  np.max(all_evals,axis = 0)        \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n        cost = np.sum(a - b)\n\n        # add regularizer \n        cost += self.reg(w)\n        \n        # return average\n        return cost/float(np.size(y_p))\n\n    # multiclass softmax\n    def multiclass_softmax(self,w,x,y,iter):\n        # get subset of points\n        x_p = x[:,iter]\n        y_p = y[:,iter]\n        \n        # pre-compute predictions on all points\n        all_evals = self.model(x_p,w)\n\n        # compute softmax across data points\n        a = np.log(np.sum(np.exp(all_evals),axis = 0)) \n\n        # compute cost in compact form using numpy broadcasting\n        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n        cost = np.sum(a - b)\n\n        # add regularizer \n        cost += self.reg(w)\n        \n        # return average\n        return cost/float(np.size(y_p))\n\n    # multiclass misclassification cost function - aka the fusion rule\n    def multiclass_counting_cost(self,w,x,y,iter):            \n        # pre-compute predictions on all points\n        all_evals = self.model(x,w)\n\n        # compute predictions of each input point\n        y_predict = (np.argmax(all_evals,axis = 0))[np.newaxis,:]\n\n        # compare predicted label to actual label\n        count = np.sum(np.abs(np.sign(y - y_predict)))\n\n        # return number of misclassifications\n        return count"""
mlrefined_libraries/superlearn_library/reg_lib/normalizers.py,12,"b""import autograd.numpy as np\n\nclass Setup:\n    def __init__(self,x,name):\n        normalizer = 0\n        inverse_normalizer = 0\n        if name == 'standard':\n            # create normalizer\n            self.normalizer, self.inverse_normalizer = self.standard_normalizer(x)\n            \n        elif name == 'sphere':\n            # create normalizer\n            self.normalizer, self.inverse_normalizer = self.PCA_sphereing(x)\n        else:\n            self.normalizer = lambda data: data\n            self.inverse_normalizer = lambda data: data\n            \n    # standard normalization function \n    def standard_normalizer(self,x):\n        # compute the mean and standard deviation of the input\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n        \n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(x_stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((x_stds.shape))\n            adjust[ind] = 1.0\n            x_stds += adjust\n\n        # create standard normalizer function\n        normalizer = lambda data: (data - x_means)/x_stds\n\n        # create inverse standard normalizer\n        inverse_normalizer = lambda data: data*x_stds + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer\n\n    # compute eigendecomposition of data covariance matrix for PCA transformation\n    def PCA(self,x,**kwargs):\n        # regularization parameter for numerical stability\n        lam = 10**(-7)\n        if 'lam' in kwargs:\n            lam = kwargs['lam']\n\n        # create the correlation matrix\n        P = float(x.shape[1])\n        Cov = 1/P*np.dot(x,x.T) + lam*np.eye(x.shape[0])\n\n        # use numpy function to compute eigenvalues / vectors of correlation matrix\n        d,V = np.linalg.eigh(Cov)\n        return d,V\n\n    # PCA-sphereing - use PCA to normalize input features\n    def PCA_sphereing(self,x,**kwargs):\n        # Step 1: mean-center the data\n        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n        x_centered = x - x_means\n\n        # Step 2: compute pca transform on mean-centered data\n        d,V = self.PCA(x_centered,**kwargs)\n\n        # Step 3: divide off standard deviation of each (transformed) input, \n        # which are equal to the returned eigenvalues in 'd'.  \n        stds = (d[:,np.newaxis])**(0.5)\n        \n        # check to make sure thta x_stds > small threshold, for those not\n        # divide by 1 instead of original standard deviation\n        ind = np.argwhere(stds < 10**(-2))\n        if len(ind) > 0:\n            ind = [v[0] for v in ind]\n            adjust = np.zeros((stds.shape))\n            adjust[ind] = 1.0\n            stds += adjust\n        \n        normalizer = lambda data: np.dot(V.T,data - x_means)/stds\n\n        # create inverse normalizer\n        inverse_normalizer = lambda data: np.dot(V,data*stds) + x_means\n\n        # return normalizer \n        return normalizer,inverse_normalizer"""
mlrefined_libraries/superlearn_library/reg_lib/optimizers.py,13,"b""import autograd.numpy as np\nfrom autograd import value_and_grad \nfrom autograd import hessian\nfrom autograd.misc.flatten import flatten_func\n\n#### optimizers ####\n# minibatch gradient descent\ndef gradient_descent(g,w,x,y,alpha_choice,max_its,batch_size): \n    # flatten the input function, create gradient based on flat function\n    g_flat, unflatten, w = flatten_func(g, w)\n    grad = value_and_grad(g_flat)\n\n    # record history\n    num_train = y.size\n    w_hist = [unflatten(w)]\n    train_hist = [g_flat(w,x,y,np.arange(num_train))]\n\n    # how many mini-batches equal the entire dataset?\n    num_batches = int(np.ceil(np.divide(num_train, batch_size)))\n\n    # over the line\n    alpha = 0\n\n    for k in range(max_its):             \n        # check if diminishing steplength rule used\n        if alpha_choice == 'diminishing':\n            alpha = 1/float(k)\n        else:\n            alpha = alpha_choice\n            \n        for b in range(num_batches):\n            # collect indices of current mini-batch\n            batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_train))\n\n            # plug in value into func and derivative\n            cost_eval,grad_eval = grad(w,x,y,batch_inds)\n            grad_eval.shape = np.shape(w)\n\n            # take descent step with momentum\n            w = w - alpha*grad_eval\n\n        # update training and validation cost\n        train_cost = g_flat(w,x,y,np.arange(num_train))\n\n        # record weight update, train and val costs\n        w_hist.append(unflatten(w))\n        train_hist.append(train_cost)\n    return w_hist,train_hist\n\n# newtons method function\ndef newtons_method(g,w,x,y,max_its,**kwargs): \n    # flatten input funciton, in case it takes in matrices of weights\n    g_flat, unflatten, w = flatten_func(g, w)\n    \n    # compute the gradient / hessian functions of our input function\n    grad = value_and_grad(g_flat)\n    hess = hessian(g_flat)\n    \n    # set numericxal stability parameter / regularization parameter\n    epsilon = 10**(-7)\n    if 'epsilon' in kwargs:\n        epsilon = kwargs['epsilon']\n    \n    # record history\n    num_train = y.size\n    w_hist = [unflatten(w)]\n    train_hist = [g_flat(w,x,y,np.arange(num_train))]\n\n    # over the line\n    for k in range(max_its):   \n        # evaluate the gradient, store current weights and cost function value\n        cost_eval,grad_eval = grad(w,x,y,np.arange(num_train))\n\n        # evaluate the hessian\n        hess_eval = hess(w,x,y,np.arange(num_train))\n\n        # reshape for numpy linalg functionality\n        hess_eval.shape = (int((np.size(hess_eval))**(0.5)),int((np.size(hess_eval))**(0.5)))\n\n        # solve second order system system for weight update\n        A = hess_eval + epsilon*np.eye(np.size(w))\n        b = grad_eval\n        w = np.linalg.lstsq(A,np.dot(A,w) - b)[0]\n\n        #w = w - np.dot(np.linalg.pinv(hess_eval + epsilon*np.eye(np.size(w))),grad_eval)\n            \n        # update training and validation cost\n        train_cost = g_flat(w,x,y,np.arange(num_train))\n\n        # record weight update, train and val costs\n        w_hist.append(unflatten(w))\n        train_hist.append(train_cost)\n    \n    return w_hist,train_hist"""
mlrefined_libraries/superlearn_library/reg_lib/superlearn_setup.py,12,"b""### import basic libs ###\nimport autograd.numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nimport copy\nimport time\n\n### import custom libs ###\nfrom . import optimizers \nfrom . import cost_functions\nfrom . import normalizers\n\n### animation libs ###\nimport matplotlib.animation as animation\nfrom IPython.display import clear_output\nimport matplotlib.patches as mpatches\n\nclass Setup:\n    def __init__(self,x,y,**kwargs):\n        # link in data\n        self.x_orig = x\n        self.y_orig = y\n        \n        # make containers for all histories\n        self.weight_histories = []\n        self.train_cost_histories = []\n        self.train_count_histories = []\n        self.valid_cost_histories = []\n        self.valid_count_histories = []\n\n    #### define normalizer ####\n    def choose_normalizer(self,name):       \n        # produce normalizer / inverse normalizer\n        s = normalizers.Setup(self.x_orig,name)\n        self.normalizer = s.normalizer\n        self.inverse_normalizer = s.inverse_normalizer\n        \n        # normalize input \n        self.x = self.normalizer(self.x_orig)\n        self.normalizer_name = name\n        \n        # normalize input \n        self.y = self.y_orig\n  \n    #### define cost function ####\n    def choose_cost(self,cost_name,reg_name,**kwargs):\n        # create cost on entire dataset\n        self.cost = cost_functions.Setup(cost_name,reg_name)\n                \n        # if the cost function is a two-class classifier, build a counter too\n        if cost_name == 'softmax' or cost_name == 'perceptron':\n            funcs = cost_functions.Setup('twoclass_counter',reg_name)\n            self.counter = funcs.cost\n            \n        if cost_name == 'multiclass_softmax' or cost_name == 'multiclass_perceptron':\n            funcs = cost_functions.Setup('multiclass_counter',reg_name)\n            self.counter = funcs.cost\n            \n        self.cost_name = cost_name\n        self.reg_name = reg_name\n            \n    #### setup optimization ####\n    def choose_optimizer(self,optimizer_name,**kwargs):\n        # general params for optimizers\n        max_its = 500; \n        alpha_choice = 10**(-1);\n        epsilon = 10**(-10)\n        \n        # set parameters by hand\n        if 'max_its' in kwargs:\n            max_its = kwargs['max_its']\n        if 'alpha_choice' in kwargs:\n            alpha_choice = kwargs['alpha_choice']\n        if 'epsilon' in kwargs:\n            epsilon = kwargs['epsilon']\n            \n        # batch size for gradient descent?\n        self.w = 0.0*np.random.randn(self.x.shape[0] + 1,1)\n        num_pts = np.size(self.y)\n        batch_size = np.size(self.y)\n        if 'batch_size' in kwargs:\n            self.batch_size = kwargs['batch_size']\n        \n        # run gradient descent\n        if optimizer_name == 'gradient_descent':\n            self.optimizer = lambda cost,x,w: optimizers.gradient_descent(cost,w,x,self.y,alpha_choice,max_its,batch_size)\n        \n        if optimizer_name == 'newtons_method':\n            self.optimizer = lambda cost,x,w: optimizers.newtons_method(cost,w,x,self.y,max_its,epsilon=epsilon)\n       \n    ### try-out various regularization params ###\n    def tryout_lams(self,lams,**kwargs):\n        # choose number of rounds\n        self.lams = lams\n        num_rounds = len(lams)\n\n        # container for costs and weights \n        self.cost_vals = []\n        self.weights = []\n        \n        # reset initialization\n        self.w_init = 0.1*np.random.randn(self.x.shape[0] + 1,1)\n            \n        # loop over lams and try out each\n        for i in range(num_rounds):        \n            # set lambda\n            lam = self.lams[i]\n            self.cost.set_lambda(lam)\n        \n            # load in current model\n            w_hist,c_hist = self.optimizer(self.cost.cost,self.x,self.w_init)\n            \n            # determine smallest cost value attained\n            ind = np.argmin(c_hist)            \n            weight = w_hist[ind]\n            cost_val = c_hist[ind]\n            self.weights.append(weight)\n            self.cost_vals.append(cost_val)\n            \n        # determine best value of lamba from the above runs\n        ind = np.argmin(self.cost_vals)\n        self.best_lam = self.lams[ind]\n        self.best_weights = self.weights[ind]\n        \n    # compare multiple l1 regularized runs\n    def animate_lams(self,savepath,**kwargs):                   \n        # initialize figure\n        fig = plt.figure(figsize = (9,3))\n        artist = fig\n        gs = gridspec.GridSpec(1,1) \n        ax = plt.subplot(gs[0])\n        \n        ### run over all input lamdas ###\n        # start animation\n        num_frames = len(self.lams)\n        print ('starting animation rendering...')\n        def animate(k):            \n            # clear panels\n            ax.cla()\n            lam = self.lams[k]\n            \n            # print rendering update\n            if np.mod(k+1,25) == 0:\n                print ('rendering animation frame ' + str(k+1) + ' of ' + str(num_frames))\n            if k == num_frames - 1:\n                print ('animation rendering complete!')\n                time.sleep(1.5)\n                clear_output()\n                \n            # save lowest misclass weights\n            w_best = self.weights[k][1:]\n            \n            # plot\n            ax.axhline(c='k',zorder = 2)\n            \n            # make bar plot\n            ax.bar(np.arange(1,len(w_best)+1).flatten(),np.array(w_best).flatten(), color='k', alpha=0.5)\n                \n            # dress panel\n            title1 = r'$\\lambda = ' + str(np.round(lam,2)) + '$' \n            costval = self.cost_vals[k][0]\n            title2 = ', cost val = ' + str(np.round(costval,2))\n            title = title1 + title2\n            ax.set_title(title)\n            ax.set_xlabel('learned weights')\n            \n            # change tick labels to used\n            ax.set_xticks(np.arange(1,self.x.shape[0]+2))\n            ax.set_xticklabels(np.arange(1,self.x.shape[0]+2))\n            \n            # clip viewing range\n            ax.set_xlim([0.25,self.x.shape[0] + 0.75])\n            return artist,\n\n        anim = animation.FuncAnimation(fig, animate ,frames=num_frames, interval=num_frames, blit=True)\n        \n        # produce animation and save\n        fps = 50\n        if 'fps' in kwargs:\n            fps = kwargs['fps']\n        anim.save(savepath, fps=fps, extra_args=['-vcodec', 'libx264'])\n        clear_output()\n\n    # static graphics\n    def plot_regress(self,id1,labels):\n        # initialize figure\n        fig = plt.figure(figsize = (9,3))\n\n        # create subplot with 1 panel\n        gs = gridspec.GridSpec(1, 1) \n        ax = plt.subplot(gs[0]); \n        \n        # scatter plot\n        ax.scatter(self.x[id1-1,:],self.y,color = 'k',edgecolor = 'w',s = 30)\n    \n        # dress panel\n        ax.set_xlabel(labels[0])\n        ax.set_ylabel(labels[1])"""
mlrefined_exercises/ed_1/Python2/Chapter_2/Exercise_2_12/convex_grad_surrogate.py,0,b'This file has been depreciated.  Please see the basic-optimization demonstration notebook in the main github repo to play with an updated version of this code.'
mlrefined_exercises/ed_1/Python2/Chapter_2/Exercise_2_12/nonconvex_grad_surrogate.py,0,b'This file has been depreciated.  Please see the basic-optimization demonstration notebook in the main github repo to play with an updated version of this code.'
mlrefined_exercises/ed_1/Python2/Chapter_2/Exercise_2_13/two_d_grad_wrapper_hw.py,0,"b'# two_d_grad_wrapper_hw is a toy wrapper to illustrate the path\n # taken by gradient descent depending on the learning rate (alpha) chosen.\n # Here alpha is kept fixed and chosen by the use. The corresponding\n # gradient steps, evaluated at the objective, are then plotted.  The plotted points on\n # the objective turn from green to red as the algorithm converges (or\n # reaches a maximum iteration count, preset to 50).\n # (nonconvex) function here is\n #\n # g(w) = -cos(2*pi*w\'*w) + 2*w\'*w\n\n # This file is associated with the book\n # ""Machine Learning Refined"", Cambridge University Press, 2016.\n # by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nfrom pylab import *\nfrom mpl_toolkits.mplot3d import Axes3D\n\n###### ML Algorithm functions ######\ndef gradient_descent(w0,alpha):\n    w = w0\n    g_path = []\n    w_path = []\n    w_path.append(w)\n    g_path.append(-cos(2*pi*dot(w.T,w)) + 2*dot(w.T,w))\n\n    # start gradient descent loop\n    grad = 1\n    iter = 1\n    max_its = 50\n    while linalg.norm(grad) > 10**(-5) and iter <= max_its:\n        # take gradient step\n# --->  grad =\n        w = w - alpha*grad\n\n        # update path containers\n        w_path.append(w)\n        g_path.append(-cos(2*pi*dot(w.T,w)) + 2*dot(w.T,w))\n        iter+= 1\n    g_path = asarray(g_path)\n    g_path.shape = (iter,1)\n    w_path = asarray(w_path)\n    w_path.shape = (iter,2)\n\n    # show final average gradient norm for sanity check\n    s = dot(grad.T,grad)/2\n    s = \'The final average norm of the gradient = \' + str(float(s))\n    print(s)\n\n\n    # # for use in testing if algorithm minimizing/converging properly\n    # plot(asarray(obj_path))\n    # show()\n\n    return (w_path,g_path)\n\n###### plotting functions #######\ndef make_function():\n    global fig,ax1\n\n    # prepare the function for plotting\n    r = linspace(-1.15,1.15,300)\n    s,t = meshgrid(r,r)\n    s = reshape(s,(size(s),1))\n    t = reshape(t,(size(t),1))\n    h = concatenate((s,t),1)\n    h = dot(h*h,ones((2,1)))\n    b = -cos(2*pi*h) + 2*h\n    s = reshape(s,(sqrt(size(s)),sqrt(size(s))))\n    t = reshape(t,(sqrt(size(t)),sqrt(size(t))))\n    b = reshape(b,(sqrt(size(b)),sqrt(size(b))))\n\n    # plot the function\n    fig = plt.figure(facecolor = \'white\')\n    ax1 = fig.add_subplot(111, projection=\'3d\')\n    ax1.plot_surface(s,t,b,cmap = \'Greys\',antialiased=False) # optinal surface-smoothing args rstride=1, cstride=1,linewidth=0\n    ax1.azim = 115\n    ax1.elev = 70\n\n    # pretty the figure up\n    ax1.xaxis.set_rotate_label(False)\n    ax1.yaxis.set_rotate_label(False)\n    ax1.zaxis.set_rotate_label(False)\n    ax1.get_xaxis().set_ticks([-1,1])\n    ax1.get_yaxis().set_ticks([-1,1])\n    ax1.set_xlabel(\'$w_0$   \',fontsize=20,rotation = 0,linespacing = 10)\n    ax1.set_ylabel(\'$w_1$\',fontsize=20,rotation = 0,labelpad = 50)\n    ax1.set_zlabel(\'   $g(\\mathbf{w})$\',fontsize=20,rotation = 0,labelpad = 20)\n\ndef plot_steps(w_path,g_path):\n    # colors for points\n    ax1.plot(w_path[:,0],w_path[:,1],g_path[:,0],color = [1,0,1],linewidth = 5)   # add a little to output path so its visible on top of the surface plot\n    ax1.plot(w_path[-8:-1,0],w_path[-8:-1,1],g_path[-8:-1,0],color = [1,0,0],linewidth = 5)   # add a little to output path so its visible on top of the surface plot\n\n\ndef main():\n    make_function()                             # plot objective function\n\n    # plot first run on surface\n    alpha = 10**-2\n    w0 = array([-0.7,0])\n    w0.shape = (2,1)\n    w_path,g_path = gradient_descent(w0,alpha)    # perform gradient descent\n    plot_steps(w_path,g_path)\n\n    # plot second run on surface\n    w0 = array([0.8,-0.8])\n    w0.shape = (2,1)\n    w_path,g_path = gradient_descent(w0,alpha)    # perform gradient descent\n    plot_steps(w_path,g_path)\n    show()\nmain()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_2/Exercise_2_16/convex_newt_demo.py,0,b'This file has been depreciated.  Please see the basic-optimization demonstration notebook in the main github repo to play with an updated version of this code.'
mlrefined_exercises/ed_1/Python2/Chapter_2/Exercise_2_16/nonconvex_newt_demo.py,0,b'This file has been depreciated.  Please see the basic-optimization demonstration notebook in the main github repo to play with an updated version of this code.'
mlrefined_exercises/ed_1/Python2/Chapter_3/Exercise_3_11/nonconvex_logistic_growth.py,0,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom pylab import *\n\n# load the data\ndef load_data():\n    # load data\n    global X,y,ax1,ax2\n\n    data = matrix(genfromtxt(\'bacteria_data.csv\', delimiter=\',\'))\n    x = asarray(data[:,0])\n    temp = ones((size(x),1))\n    X = concatenate((temp,x),1)\n    y = asarray(data[:,1])\n    y = y/y.max()\n\n    # initialize figure, plot data, and dress up panels with axes labels etc.,\n    fig = plt.figure(facecolor = \'white\')\n    ax1 = fig.add_subplot(121)\n    ax1.set_xlabel(\'$x$\',fontsize=20,labelpad = 20)\n    ax1.set_ylabel(\'$y$\',fontsize=20,rotation = 0,labelpad = 20)\n    plot(x,y,\'ko\')\n    ax1.set_xlim(min(x[:,0])-0.5, max(x[:,0])+0.5)\n    ax1.set_ylim(min(y)-0.1,max(y)+0.1)\n\n    ax2 = fig.add_subplot(122, projection=\'3d\')\n    ax2.xaxis.set_rotate_label(False)\n    ax2.yaxis.set_rotate_label(False)\n    ax2.zaxis.set_rotate_label(False)\n    ax2.get_xaxis().set_ticks([-3,-1,1,3])\n    ax2.get_yaxis().set_ticks([-3,-1,1,3])\n    ax2.set_xlabel(\'$w_0$   \',fontsize=20,rotation = 0,linespacing = 10)\n    ax2.set_ylabel(\'$w_1$\',fontsize=20,rotation = 0,labelpad = 50)\n    ax2.set_zlabel(\'   $g(\\mathbf{w})$\',fontsize=20,rotation = 0,labelpad = 20)\n\n###### ML Algorithm functions ######\n# run gradient descent\ndef gradient_descent(w0):\n    w_path = []         # container for weights learned at each iteration\n    obj_path = []       # container for associated objective values at each iteration\n    w_path.append(w0)\n    obj = calculate_obj(w0)\n    obj_path.append(obj)\n    grad_path = []\n    w = w0\n\n    # start gradient descent loop\n    grad = 1\n    iter = 1\n    max_its = 5000\n    alpha = 10**(-2)\n    while linalg.norm(grad) > 10**(-5) and iter <= max_its:\n        # compute gradient\n# --->  grad =\n\n        # take gradient step\n        w = w - alpha*grad\n\n        # update path containers\n        w_path.append(w)\n        obj = calculate_obj(w)\n        obj_path.append(obj)\n        iter+= 1\n\n    # reshape containers for use in plotting in 3d\n    w_path = asarray(w_path)\n    w_path.shape = (iter,2)\n    obj_path = asarray(obj_path)\n    obj_path.shape = (iter,1)\n\n    return (w_path,obj_path)\n    ## for use in testing if gradient vanishes\n    # grad_path = asarray(grad_path)\n    # grad_path.shape = (iter,1)\n    # plot(asarray(grad_path))\n    # show()\n\n# calculate the objective value for a given input weight w\ndef calculate_obj(w):\n    temp = 1/(1 + my_exp(-dot(X,w))) - y\n    temp = dot(temp.T,temp)\n    return temp\n\n# avoid overflow when using exp - just cutoff after arguments get too large/small\ndef my_exp(u):\n    s = argwhere(u > 100)\n    t = argwhere(u < -100)\n    u[s] = 0\n    u[t] = 0\n    u = exp(u)\n    u[t] = 1\n    return u\n\n###### plotting functions #######\n# make 3d surface plot\ndef plot_logistic_surface():\n    r = linspace(-3,3,150)\n    s,t = meshgrid(r, r)\n    s = reshape(s,(size(s),1))\n    t = reshape(t,(size(t),1))\n    h = concatenate((s,t),1)\n\n    # build 3d surface\n    surf = zeros((size(s),1))\n    max_its = size(y)\n    for i in range(0,max_its):\n        surf = surf + add_layer(X[i,:],y[i],h)\n\n\n    s = reshape(s,(sqrt(size(s)),sqrt(size(s))))\n    t = reshape(t,(sqrt(size(t)),sqrt(size(t))))\n    surf = reshape(surf,(sqrt(size(surf)),sqrt(size(surf))))\n\n    # plot 3d surface\n    ax2.plot_surface(s,t,surf,cmap = \'jet\')\n    ax2.azim = 175\n    ax2.elev = 20\n\n# used by plot_logistic_surface to make objective surface of logistic regression cost function\ndef add_layer(a,b,c):\n    a.shape = (2,1)\n    b.shape = (1,1)\n    z = my_exp(-dot(c,a))\n    z = 1/(1 + z) - b\n    z = z**2\n    return z\n\n# plot fit to data and corresponding gradient descent path onto the logistic regression objective surface\ndef show_fit_and_paths(w_path,obj_path,col):\n    # plot solution of gradient descent fit to original data\n    s = linspace(0,X.max(),100)\n    t = 1/(1 + my_exp(-(w_path[-1,0] + w_path[-1,1]*s)))\n    ax1.plot(s,t,color = col)\n\n    # plot grad descent path onto surface\n    ax2.plot(w_path[:,0],w_path[:,1],obj_path[:,0],color = col,linewidth = 5)   # add a little to output path so its visible on top of the surface plot\n\ndef main():\n\n    load_data() # load in data\n\n    plot_logistic_surface()  # plot objective surface\n\n    ### run gradient descent with first initial point\n    w0 = array([0,2])\n    w0.shape = (2,1)\n    w_path, obj_path = gradient_descent(w0)\n\n    # plot fit to data and path on objective surface\n    show_fit_and_paths(w_path, obj_path,\'m\')\n\n    ### run gradient descent with first initial point\n    w0 = array([0,-2])\n    w0.shape = (2,1)\n    w_path, obj_path = gradient_descent(w0)\n\n    # plot fit to data and path on objective surface\n    show_fit_and_paths(w_path, obj_path,\'c\')\n    show()\n\nmain()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_3/Exercise_3_13/l2reg_nonconvex_logistic_growth.py,0,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom pylab import *\n\n# load the data\ndef load_data():\n    # load data\n    global X,y,ax1,ax2\n\n    data = matrix(genfromtxt(\'bacteria_data.csv\', delimiter=\',\'))\n    x = asarray(data[:,0])\n    temp = ones((size(x),1))\n    X = concatenate((temp,x),1)\n    y = asarray(data[:,1])\n    y = y/y.max()\n\n    # initialize figure, plot data, and dress up panels with axes labels etc.,\n    fig = plt.figure(facecolor = \'white\')\n    ax1 = fig.add_subplot(121)\n    ax1.set_xlabel(\'$x$\',fontsize=20,labelpad = 20)\n    ax1.set_ylabel(\'$y$\',fontsize=20,rotation = 0,labelpad = 20)\n    plot(x,y,\'ko\')\n    ax1.set_xlim(min(x[:,0])-0.5, max(x[:,0])+0.5)\n    ax1.set_ylim(min(y)-0.1,max(y)+0.1)\n\n    ax2 = fig.add_subplot(122, projection=\'3d\')\n    ax2.xaxis.set_rotate_label(False)\n    ax2.yaxis.set_rotate_label(False)\n    ax2.zaxis.set_rotate_label(False)\n    ax2.get_xaxis().set_ticks([-3,-1,1,3])\n    ax2.get_yaxis().set_ticks([-3,-1,1,3])\n    ax2.set_xlabel(\'$w_0$   \',fontsize=20,rotation = 0,linespacing = 10)\n    ax2.set_ylabel(\'$w_1$\',fontsize=20,rotation = 0,labelpad = 50)\n    ax2.set_zlabel(\'   $g(\\mathbf{w})$\',fontsize=20,rotation = 0,labelpad = 20)\n\n###### ML Algorithm functions ######\n# run gradient descent\ndef gradient_descent(w0):\n    w_path = []         # container for weights learned at each iteration\n    obj_path = []       # container for associated objective values at each iteration\n    w_path.append(w0)\n    obj = calculate_obj(w0)\n    obj_path.append(obj)\n    grad_path = []\n    w = w0\n\n    # start gradient descent loop\n    grad = 1\n    iter = 1\n    max_its = 5000\n    alpha = 10**(-2)\n    while linalg.norm(grad) > 10**(-5) and iter <= max_its:\n        # compute gradient\n # ---> grad =\n\n\n        # take gradient step\n        w = w - alpha*grad\n\n        # update path containers\n        w_path.append(w)\n        obj = calculate_obj(w)\n        obj_path.append(obj)\n        iter+= 1\n\n    # reshape containers for use in plotting in 3d\n    w_path = asarray(w_path)\n    w_path.shape = (iter,2)\n    obj_path = asarray(obj_path)\n    obj_path.shape = (iter,1)\n\n    return (w_path,obj_path)\n    ## for use in testing if gradient vanishes\n    # grad_path = asarray(grad_path)\n    # grad_path.shape = (iter,1)\n    # plot(asarray(grad_path))\n    # show()\n\n# calculate the objective value for a given input weight w\ndef calculate_obj(w):\n    temp = 1/(1 + my_exp(-dot(X,w))) - y\n    temp = dot(temp.T,temp)\n    obj = temp + lam*w[1]**2\n    return obj\n\n# avoid overflow when using exp - just cutoff after arguments get too large/small\ndef my_exp(u):\n    s = argwhere(u > 100)\n    t = argwhere(u < -100)\n    u[s] = 0\n    u[t] = 0\n    u = exp(u)\n    u[t] = 1\n    return u\n\n###### plotting functions #######\n# make 3d surface plot\ndef plot_logistic_surface():\n    r = linspace(-3,3,150)\n    s,t = meshgrid(r, r)\n    s = reshape(s,(size(s),1))\n    t = reshape(t,(size(t),1))\n    h = concatenate((s,t),1)\n\n    # build 3d surface\n    surf = zeros((size(s),1))\n    max_its = size(y)\n    for i in range(0,max_its):\n        surf = surf + add_layer(X[i,:],y[i],h)\n    surf = surf + lam*t**2\n\n    s = reshape(s,(sqrt(size(s)),sqrt(size(s))))\n    t = reshape(t,(sqrt(size(t)),sqrt(size(t))))\n    surf = reshape(surf,(sqrt(size(surf)),sqrt(size(surf))))\n\n    # plot 3d surface\n    ax2.plot_surface(s,t,surf,cmap = \'jet\')\n    ax2.azim = 175\n    ax2.elev = 20\n\n# used by plot_logistic_surface to make objective surface of logistic regression cost function\ndef add_layer(a,b,c):\n    a.shape = (2,1)\n    b.shape = (1,1)\n    z = my_exp(-dot(c,a))\n    z = 1/(1 + z) - b\n    z = z**2\n    return z\n\n# plot fit to data and corresponding gradient descent path onto the logistic regression objective surface\ndef show_fit_and_paths(w_path,obj_path,col):\n    # plot solution of gradient descent fit to original data\n    s = linspace(0,X.max(),100)\n    t = 1/(1 + my_exp(-(w_path[-1,0] + w_path[-1,1]*s)))\n    ax1.plot(s,t,color = col)\n\n    # plot grad descent path onto surface\n    ax2.plot(w_path[:,0],w_path[:,1],obj_path[:,0],color = col,linewidth = 5)   # add a little to output path so its visible on top of the surface plot\n\ndef main():\n    global lam\n    lam = 10**-1                  # L2 regularizer parameter, convexifies the objective so gradient descent doesn\'t get stuck in flat areas as much\n    load_data()              # load the data\n    plot_logistic_surface()  # plot objective surface\n\n    ### run gradient descent with first initial point\n    w0 = array([0,2])\n    w0.shape = (2,1)\n    w_path, obj_path = gradient_descent(w0)\n\n    # plot fit to data and path on objective surface\n    show_fit_and_paths(w_path, obj_path,\'m\')\n\n    ### run gradient descent with first initial point\n    w0 = array([0,-2])\n    w0.shape = (2,1)\n    w_path, obj_path = gradient_descent(w0)\n\n    # plot fit to data and path on objective surface\n    show_fit_and_paths(w_path, obj_path,\'c\')\n    show()\n\nmain()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_4/Exercise_4_11/exp_vs_log_demo_hw.py,13,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# load the data\ndef load_data():\n    # load data\n    data = np.matrix(np.genfromtxt(\'exp_vs_log_data.csv\', delimiter=\',\'))\n    X = np.asarray(data[:,0:2])\n    y = np.asarray(data[:,2])\n    y.shape = (np.size(y),1)\n    return X, y\n\n###### ML Algorithm functions ######\n\n# run gradient descent for h1\ndef gradient_descent_soft_cost(X,y,w,alpha):\n\n    # start gradient descent loop\n    grad = 1\n    k = 1\n    max_its = 10000\n\n    while np.linalg.norm(grad) > 10**(-5) and k <= max_its:\n        # compute gradient\n# --->  grad =\n\n        # take gradient step\n        w = w - alpha*grad\n\n        # update path containers\n        k += 1\n\n    return w\n\n\n# run gradient descent for h2\ndef gradient_descent_exp_cost(X,y,w,alpha):\n\n    # start gradient descent loop\n    grad = 1\n    k = 1\n    max_its = 10000\n\n    while np.linalg.norm(grad) > 10**(-5) and k <= max_its:\n        # compute gradient\n# --->  grad =\n\n        # take gradient step\n        w = w - alpha*grad\n\n        # update path containers\n        k += 1\n\n    return w\n\n###### plotting functions #######\ndef plot_all(X,y,w,color,ax1):\n\n    # initialize figure, plot data, and dress up panels with axes labels etc.,\n\n    ax1.set_xlabel(\'$x_1$\',fontsize=20,labelpad = 20)\n    ax1.set_ylabel(\'$x_2$\',fontsize=20,rotation = 0,labelpad = 20)\n    s = np.argwhere(y == 1)\n    s = s[:,0]\n    plt.scatter(X[s,0],X[s,1], s = 30,color = (1, 0, 0.4))\n    s = np.argwhere(y == -1)\n    s = s[:,0]\n    plt.scatter(X[s,0],X[s,1],s = 30, color = (0, 0.4, 1))\n    ax1.set_xlim(0,1.05)\n    ax1.set_ylim(0,1.05)\n\n    # plot separator\n    r = np.linspace(0,1,150)\n    z = -w.item(0)/w.item(2) - w.item(1)/w.item(2)*r\n    ax1.plot(r,z,color = color,linewidth = 2)\n\n# main wrapper\ndef exp_vs_log_demo_hw():\n    # load data\n    X,y = load_data()\n\n    # use compact notation and initialize\n    temp = np.shape(X)\n    temp = np.ones((temp[0],1))\n    X_tilde = np.concatenate((temp,X),1)\n    X_tilde = X_tilde.T\n\n    alpha = 10**(-2)\n    w0 = np.random.randn(3,1)\n\n    # run gradient descent for h1\n    w1 = gradient_descent_soft_cost(X_tilde,y,w0,alpha)\n\n    # run gradient descent for h1\n    w2 = gradient_descent_exp_cost(X_tilde,y,w0,alpha)\n\n    # plot everything\n    fig = plt.figure(facecolor = \'white\')\n    ax1 = fig.add_subplot(111)\n    plot_all(X,y,w1,\'k\',ax1)\n    plot_all(X,y,w2,\'m\',ax1)\n    plt.show()\n\nexp_vs_log_demo_hw()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_4/Exercise_4_14/one_versus_all_demo_hw.py,33,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# load the data\ndef load_data():\n    # load data\n    data = np.matrix(np.genfromtxt(\'four_class_data.csv\', delimiter=\',\'))\n    x = np.asarray(data[:,0:2])\n    temp = np.shape(x)\n    temp = np.ones((temp[0],1))\n    X = np.concatenate((temp,x),1)\n    X = X.T\n    y = np.asarray(data[:,2])\n    y.shape = (np.size(y),1)\n    return X,y\n\n###### ML Algorithm functions ######\n# learn all C separators\ndef learn_separators(X,y):\n    W = []\n    num_classes = np.size(np.unique(y))\n    for i in range(0,num_classes):\n        # prepare temporary C vs notC probem labels\n        y_temp = np.copy(y)\n        ind = np.argwhere(y_temp == (i+1))\n        ind = ind[:,0]\n        ind2 = np.argwhere(y_temp != (i+1))\n        ind2 = ind2[:,0]\n        y_temp[ind] = 1\n        y_temp[ind2] = -1\n        # run descent algorithm to classify C vs notC problem\n        w = newtons_method(np.random.randn(3,1),X,y_temp)\n        W.append(w)\n    W = np.asarray(W)\n    W.shape = (num_classes,3)\n    W = W.T\n    return W\n\n# run newton\'s method\ndef newtons_method(w0,X,y):\n\n# ---> YOU MUST COMPLETE THIS MODULE.    \n\n    return w\n\n# calculate the objective value for a given input weight w\ndef calculate_obj(w,X,y):\n    obj = np.log(1 + my_exp(-y*np.dot(X.T,w)))\n    obj = obj.sum()\n    return obj\n\n# avoid overflow when using exp - just cutoff after arguments get too large/small\ndef my_exp(u):\n    s = np.argwhere(u > 100)\n    t = np.argwhere(u < -100)\n    u[s] = 0\n    u[t] = 0\n    u = np.exp(u)\n    u[t] = 1\n    return u\n\n###### plotting functions #######\ndef plot_data_and_subproblem_separators(X,y,W):\n    # initialize figure, plot data, and dress up panels with axes labels etc.\n    num_classes = np.size(np.unique(y))\n    color_opts = np.array([[1,0,0.4], [ 0, 0.4, 1],[0, 1, 0.5],[1, 0.7, 0.5],[0.7, 0.6, 0.5]])\n    f,axs = plt.subplots(1,num_classes + 1,facecolor = \'white\')\n\n    r = np.linspace(0,1,150)\n    for a in range(0,num_classes):\n        # color current class\n        axs[a].scatter(X[1,],X[2,], s = 30,color = \'0.75\')\n        s = np.argwhere(y == a+1)\n        s = s[:,0]\n        axs[a].scatter(X[1,s],X[2,s], s = 30,color = color_opts[a,:])\n        axs[num_classes].scatter(X[1,s],X[2,s], s = 30,color = color_opts[a,:])\n\n        # draw subproblem separator\n        z = -W[0,a]/W[2,a] - W[1,a]/W[2,a]*r\n        axs[a].plot(r,z,\'-k\',linewidth = 2,color = color_opts[a,:])\n\n        # dress panel correctly\n        axs[a].set_xlim(0,1)\n        axs[a].set_ylim(0,1)\n        axs[a].set(aspect = \'equal\')\n    axs[num_classes].set(aspect = \'equal\')\n\n    return axs\n\n# fuse individual subproblem separators into one joint rule\ndef plot_joint_separator(W,axs,num_classes):\n    r = np.linspace(0,1,300)\n    s,t = np.meshgrid(r,r)\n    s = np.reshape(s,(np.size(s),1))\n    t = np.reshape(t,(np.size(t),1))\n    h = np.concatenate((np.ones((np.size(s),1)),s,t),1)\n    f = np.dot(W.T,h.T)\n    z = np.argmax(f,0)\n    f.shape = (np.size(f),1)\n    s.shape = (np.size(r),np.size(r))\n    t.shape = (np.size(r),np.size(r))\n    z.shape = (np.size(r),np.size(r))\n\n    for i in range(0,num_classes + 1):\n        axs[num_classes].contour(s,t,z,num_classes-1,colors = \'k\',linewidths = 2.25)\n\ndef one_versus_all_demo_hw():\n    # load the data\n    X,y = load_data()\n\n    # learn all C vs notC separators\n    W = learn_separators(X,y)\n\n    # plot data and each subproblem 2-class separator\n    axs = plot_data_and_subproblem_separators(X,y,W)\n\n    # plot fused separator\n    plot_joint_separator(W,axs,np.size(np.unique(y)))\n\n    plt.show()\n\none_versus_all_demo_hw()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_4/Exercise_4_15/softmax_multiclass_grad_hw.py,29,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# load the data\ndef load_data():\n    # load data\n    data = np.matrix(np.genfromtxt(\'4class_data.csv\', delimiter=\',\'))\n    x = np.asarray(data[:,0:2])\n    y = np.asarray(data[:,2])\n    y.shape = (np.size(y),1)\n    return x,y\n\n###### ML Algorithm functions ######\n# learn all C separators running gradient descent\ndef gradient_descent(x,y,alpha):\n    # formulate full input data matrix X\n    temp = np.shape(x)\n    temp = np.ones((temp[0],1))\n    X = np.concatenate((temp,x),1)\n    X = X.T\n    num_classes = len(np.unique(y))\n    num_pts = len(y)\n    W = np.random.randn(3,num_classes)\n\n    # record objective value at each iteration to check that algorithm is working properly\n    obj_path = []\n    obj = calculate_obj(X,y,W)\n    obj_path.append(obj)\n\n    # gradient descent loop\n    iter = 1\n    max_its = 1000\n    while iter < max_its:\n\n        # calculate gradient\n# --->  grad = \n\n        # full gradient completely calculated - take gradient step\n        W = W - alpha*grad\n        iter+= 1\n\n        # update path containers - used to check that algorithm is working\n        obj = calculate_obj(X,y,W)\n        obj_path.append(obj)\n\n    # plot objective value at each iteration to make sure everything works properly\n    obj_path = np.asarray(obj_path)\n    obj_path.shape = (max_its,1)\n    # plt.plot(np.asarray(obj_path))\n\n    return W\n\n# calculate the objective value for a given input weight w\ndef calculate_obj(X,y,W):\n    # loop for cost function\n    cost = 0\n    for p in range(0,len(y)):\n        s = int(y[p])-1\n        p_temp = 0\n        for j in range(0,len(np.unique(y))-1):\n            p_temp += np.exp(np.dot(X[:,p].T,(W[:,j] - W[:,s])))\n        p_temp = np.log(p_temp)\n\n        # update cost\n        cost+=p_temp\n    return cost\n\n# plot data, separators, and fused fule\ndef plot_all(x,y,W):\n    # initialize figure, plot data, and dress up panels with axes labels etc.\n    num_classes = np.size(np.unique(y))\n    color_opts = np.array([[1,0,0.4], [ 0, 0.4, 1],[0, 1, 0.5],[1, 0.7, 0.5],[0.7, 0.6, 0.5]])\n    f,axs = plt.subplots(1,3,facecolor = \'white\')\n    for a in range(0,3):\n        for i in range(0,num_classes):\n            s = np.argwhere(y == i+1)\n            s = s[:,0]\n            axs[a].scatter(x[s,0],x[s,1], s = 30,color = color_opts[i,:])\n\n        # dress panel correctly\n        axs[a].set_xlabel(\'$x_1$\',fontsize=20,labelpad = 20)\n        axs[a].set_ylabel(\'$x_2$\',fontsize=20,rotation = 0,labelpad = 20)\n        axs[a].set_xlim(0,1)\n        axs[a].set_ylim(0,1)\n        axs[a].set(aspect = \'equal\')\n\n    r = np.linspace(0,1,150)\n    for i in range(0,num_classes):\n        z = -W[0,i]/W[2,i] - W[1,i]/W[2,i]*r\n        axs[1].plot(r,z,\'-k\',linewidth = 2,color = color_opts[i,:])\n\n    # fuse individual subproblem separators into one joint rule\n    r = np.linspace(0,1,300)\n    s,t = np.meshgrid(r,r)\n    s = np.reshape(s,(np.size(s),1))\n    t = np.reshape(t,(np.size(t),1))\n    h = np.concatenate((np.ones((np.size(s),1)),s,t),1)\n    f = np.dot(W.T,h.T)\n    z = np.argmax(f,0)\n    f.shape = (np.size(f),1)\n    s.shape = (np.size(r),np.size(r))\n    t.shape = (np.size(r),np.size(r))\n    z.shape = (np.size(r),np.size(r))\n\n    for i in range(0,num_classes + 1):\n        axs[2].contour(s,t,z,num_classes-1,colors = \'k\',linewidths = 2.25)\n\n# main wrapper\ndef softmax_multiclass_grad_hw():\n    # load data\n    x,y = load_data()\n\n    # perform gradient descent on softmax multiclass\n    alpha = 10**(-2)    # step length, tune to your heat\'s desire!\n    W = gradient_descent(x,y,alpha)           # learn all C vs notC separators\n    plot_all(x,y,W)\n    plt.show()\n\nsoftmax_multiclass_grad_hw()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_4/Exercise_4_3/softmax_grad_demo_hw.py,7,"b'# softmax_grad_demo_hw runs the softmax model on a separable two \n# class dataset consisting of two dimensional data features.\n\n# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# sigmoid for softmax/logistic regression minimization\ndef sigmoid(z): \n    y = 1/(1+np.exp(-z))\n    return y\n    \n# import training data \ndef load_data(csvname):\n    # load in dataframe\n    all_data = np.array(np.genfromtxt(csvname,delimiter = \',\'))\n\n    # grab training data and labels\n    X = all_data[:,:-1]      \n    y = all_data[:,-1]\n    \n    return X,y\n\n# gradient descent function for softmax cost/logistic regression \ndef softmax_grad(X,y):\n    # Initializations \n    w = np.random.randn(3,1);        # random initial point\n    alpha = 10**-2\n    k = 1\n    max_its = 2000\n    grad = 1\n    \n    while np.linalg.norm(grad) > 10**-12 and k < max_its:\n        # compute gradient\n        grad = \n        w = w - alpha*grad;\n\n        # update iteration count\n        k = k + 1;\n        \n    return w\n\n# plots everything \ndef plot_all(X,y,w):\n    # custom colors for plotting points\n    red = [1,0,0.4]  \n    blue = [0,0.4,1]\n    \n    # scatter plot points\n    fig = plt.figure(figsize = (4,4))\n    ind = np.argwhere(y==1)\n    ind = [s[0] for s in ind]\n    plt.scatter(X[ind,0],X[ind,1],color = red,edgecolor = \'k\',s = 25)\n    ind = np.argwhere(y==-1)\n    ind = [s[0] for s in ind]\n    plt.scatter(X[ind,0],X[ind,1],color = blue,edgecolor = \'k\',s = 25)\n    plt.grid(\'off\')\n    \n    # plot separator\n    s = np.linspace(0,1,100) \n    plt.plot(s,(-w[0]-w[1]*s)/w[2],color = \'k\',linewidth = 2)\n    plt.show()\n    \n# load in data\nX,y = load_data(\'imbalanced_2class.csv\')\n\n# run gradient descent\nw = softmax_grad(X,y)\n\n# plot points and separator\nplot_all(X,y,w)\n'"
mlrefined_exercises/ed_1/Python2/Chapter_4/Exercise_4_4/softmax_Newton_demo_hw.py,11,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# load the data\ndef load_data():\n    data = np.matrix(np.genfromtxt(\'overlapping_2class.csv\', delimiter=\',\'))\n    x = np.asarray(data[:,0:2])\n    y = np.asarray(data[:,2])\n    y.shape = (np.size(y),1)\n    return x,y\n\n# run newton\'s method\ndef newtons_method(x,y):\n\n    # begin newton\'s method loop\n    k = 1\n    max_its = 20\n\n    while k <= max_its:\n        # take Newton step\n# ----> w =\n\n        # update counter\n        k+=1\n    return w\n\n# calculate the objective value for a given input weight w\ndef calculate_obj(X,y,w):\n    obj = np.log(1 + my_exp(-y*np.dot(X.T,w)))\n    obj = obj.sum()\n    return obj\n\n# avoid overflow when using exp - just cutoff after arguments get too large/small\ndef my_exp(u):\n    s = np.argwhere(u > 100)\n    t = np.argwhere(u < -100)\n    u[s] = 0\n    u[t] = 0\n    u = np.exp(u)\n    u[t] = 1\n    return u\n\n# plotting function\ndef plot_fit(x,y,w):\n\n    # initialize figure, plot data, and dress up panels with axes labels etc.\n    fig = plt.figure(facecolor = \'white\')\n    ax1 = fig.add_subplot(111)\n    ax1.set_xlabel(\'$x_1$\',fontsize=20,labelpad = 20)\n    ax1.set_ylabel(\'$x_2$\',fontsize=20,rotation = 0,labelpad = 20)\n    s = np.argwhere(y == 1)\n    s = s[:,0]\n    plt.scatter(x[s,0],x[s,1], s = 30,color = (1, 0, 0.4))\n    s = np.argwhere(y == -1)\n    s = s[:,0]\n    plt.scatter(x[s,0],x[s,1],s = 30, color = (0, 0.4, 1))\n    ax1.set_xlim(min(x[:,0])-0.1, max(x[:,0])+0.1)\n    ax1.set_ylim(min(x[:,1])-0.1,max(x[:,1])+0.1)\n\n    # plot separator\n    r = np.linspace(0,1,150)\n    z = -w.item(0)/w.item(2) - w.item(1)/w.item(2)*r\n    ax1.plot(r,z,\'-k\',linewidth = 2)\n    plt.show()\n\n# main\ndef main():\n    # load data\n    x,y = load_data()\n\n    # run newtons method to minimize logistic regression or softmax cost\n    w = newtons_method(x,y)\n\n    # plot everything - including data and separator\n    plot_fit(x,y,w)\n    plt.show()\n\nmain()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_4/Exercise_4_7/squared_margin_grad_demo_hw.py,16,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# load the data\ndef load_data():\n    # load data\n    global X,y,ax1\n\n    data = np.matrix(np.genfromtxt(\'imbalanced_2class.csv\', delimiter=\',\'))\n    x = np.asarray(data[:,0:2])\n    temp = np.shape(x)\n    temp = np.ones((temp[0],1))\n    X = np.concatenate((temp,x),1)\n    X = X.T\n    y = np.asarray(data[:,2])\n    y.shape = (np.size(y),1)\n\n    # initialize figure, plot data, and dress up panels with axes labels etc.,\n    fig = plt.figure(facecolor = \'white\')\n    ax1 = fig.add_subplot(111)\n    ax1.set_xlabel(\'$x$\',fontsize=20,labelpad = 20)\n    ax1.set_ylabel(\'$y$\',fontsize=20,rotation = 0,labelpad = 20)\n    s = np.argwhere(y == 1)\n    s = s[:,0]\n    plt.scatter(x[s,0],x[s,1], s = 30,color = (1, 0, 0.4))\n    s = np.argwhere(y == -1)\n    s = s[:,0]\n    plt.scatter(x[s,0],x[s,1],s = 30, color = (0, 0.4, 1))\n    ax1.set_xlim(min(x[:,0])-0.1, max(x[:,0])+0.1)\n    ax1.set_ylim(min(x[:,1])-0.1,max(x[:,1])+0.1)\n\n###### ML Algorithm functions ######\n# run gradient descent\ndef gradient_descent(w0):\n    obj_path = []\n    obj = calculate_obj(w0)\n    obj_path.append(obj)\n    w = w0\n    grad = 1\n    iter = 1\n    max_its = 5000\n    alpha = 10**(-3)\n    while np.linalg.norm(grad) > 10**(-5) and iter <= max_its:\n        # compute gradient\n# --->  grad =\n\n        # take gradient step\n        w = w - alpha*grad\n\n        # update path containers\n        obj = calculate_obj(w)\n        obj_path.append(obj)\n        iter+= 1\n\n    return w\n\n# calculate the objective value for a given input weight w\ndef calculate_obj(w):\n    obj = np.log(1 + my_exp(-y*np.dot(X.T,w)))\n    obj = obj.sum()\n    return obj\n\n# avoid overflow when using exp - just cutoff after arguments get too large/small\ndef my_exp(u):\n    s = np.argwhere(u > 100)\n    t = np.argwhere(u < -100)\n    u[s] = 0\n    u[t] = 0\n    u = np.exp(u)\n    u[t] = 1\n    return u\n\n###### plotting functions #######\ndef plot_fit(w):\n    r = np.linspace(0,1,150)\n    z = -w.item(0)/w.item(2) - w.item(1)/w.item(2)*r\n    ax1.plot(r,z,\'-k\',linewidth = 2)\n\ndef squared_margin_grad_demo_hw():\n    load_data()              # load the data\n\n    ### run gradient descent with first initial point\n    w0 = np.random.randn(3,1)\n    w = gradient_descent(w0)\n    plot_fit(w)\n    plt.show()\n\nsquared_margin_grad_demo_hw()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_4/Exercise_4_8/squared_margin_Newton_demo_hw.py,17,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# load the data\ndef load_data():\n    # load data\n    global X,y,ax1\n\n    data = np.matrix(np.genfromtxt(\'overlapping_2class.csv\', delimiter=\',\'))\n    x = np.asarray(data[:,0:2])\n    temp = np.shape(x)\n    temp = np.ones((temp[0],1))\n    X = np.concatenate((temp,x),1)\n    X = X.T\n    y = np.asarray(data[:,2])\n    y.shape = (np.size(y),1)\n\n    # initialize figure, plot data, and dress up panels with axes labels etc.\n    fig = plt.figure(facecolor = \'white\')\n    ax1 = fig.add_subplot(111)\n    ax1.set_xlabel(\'$x$\',fontsize=20,labelpad = 20)\n    ax1.set_ylabel(\'$y$\',fontsize=20,rotation = 0,labelpad = 20)\n    s = np.argwhere(y == 1)\n    s = s[:,0]\n    plt.scatter(x[s,0],x[s,1], s = 30,color = (1, 0, 0.4))\n    s = np.argwhere(y == -1)\n    s = s[:,0]\n    plt.scatter(x[s,0],x[s,1],s = 30, color = (0, 0.4, 1))\n    ax1.set_xlim(min(x[:,0])-0.1, max(x[:,0])+0.1)\n    ax1.set_ylim(min(x[:,1])-0.1,max(x[:,1])+0.1)\n\n###### ML Algorithm functions ######\n# run newton\'s method\ndef newtons_method(w0):\n    obj_path = []\n    obj = calculate_obj(w0)\n    obj_path.append(obj)\n    w = w0\n    grad = 1\n    iter = 1\n    max_its = 100\n    while np.linalg.norm(grad) > 10**(-5) and iter <= max_its:\n\n        # take Newton step\n# --->  w =\n\n        # update path containers\n        obj = calculate_obj(w)\n        obj_path.append(obj)\n        iter+= 1\n\n    # for use in testing if algorithm minimizing/converging properly\n    obj_path = np.asarray(obj_path)\n    obj_path.shape = (iter,1)\n    #plot(asarray(obj_path))\n\n    return w\n\n\n# calculate the objective value for a given input weight w\ndef calculate_obj(w):\n    obj = np.log(1 + my_exp(-y*np.dot(X.T,w)))\n    obj = obj.sum()\n    return obj\n\n# avoid overflow when using exp - just cutoff after arguments get too large/small\ndef my_exp(u):\n    s = np.argwhere(u > 100)\n    t = np.argwhere(u < -100)\n    u[s] = 0\n    u[t] = 0\n    u = np.exp(u)\n    u[t] = 1\n    return u\n\n###### plotting functions #######\ndef plot_fit(w):\n    r = np.linspace(0,1,150)\n    z = -w.item(0)/w.item(2) - w.item(1)/w.item(2)*r\n    ax1.plot(r,z,\'k-\',linewidth = 2)\n\ndef squared_margin_Newton_demo_hw():\n    load_data()              # load the data\n\n    ### run newtons method with first initial point\n    w0 = np.random.randn(3,1)\n    w = newtons_method(w0)\n    plot_fit(w)\n    plt.show()\n\nsquared_margin_Newton_demo_hw()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_5/Exercise_5_13/compare_maps_regression_hw.py,53,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport numpy as np\nimport numpy.matlib\nimport matplotlib.pyplot as plt\nfrom __future__ import division\n\ndef compare_maps_regression_hw():\n\n    # load in data\n    data = np.array(np.genfromtxt(\'noisy_sin_samples.csv\', delimiter=\',\'))\n    x = np.reshape(data[:,0],(np.size(data[:,0]),1))\n    y = np.reshape(data[:,1],(np.size(data[:,1]),1))\n\n    # true underlying data-generating function\n    global x_true,y_true\n    x_true = np.reshape(np.arange(0,1,.01),(100,1))\n    y_true = np.sin(2*np.pi*x_true)\n\n    # parameters to play with\n    k = 3    # number of folds to use\n\n    # split points into k equal sized sets\n    c = split_data(x,y,k)\n\n    ###################################################\n    # do k-fold cross-validation using polynomial basis\n    poly_degs = np.arange(1,11)           # range of poly models to compare\n    deg = cross_validate_poly(x,y,c,poly_degs,k)\n\n\n    # plot it\n    plt.subplot(1,3,1)\n    plot_poly(x,y,deg,k)\n\n    ###################################################\n    # do k-fold cross-validation using fourier basis\n    fourier_degs = np.arange(1,11)           # range of fourier models to compare\n    deg = cross_validate_fourier(x,y,c,fourier_degs,k)\n\n\n    # plot it\n    plt.subplot(1,3,2)\n    plot_fourier(x,y,deg,k)\n\n    ###################################################\n    # do k-fold cross-validation using tanh basis\n    tanh_degs = np.arange(1,7)           # range of NN models to compare\n    deg = cross_validate_tanh(x,y,c,tanh_degs,k)\n\n    # plot it\n    plt.subplot(1,3,3)\n    plot_tanh(x,y,deg,k)\n    plt.show()\n\n\ndef split_data(x,y,k):\n\n# ---> YOUR CODE GOES HERE.\n\n    return c\n\ndef cross_validate_poly(x,y,c,poly_degs,k):\n\n# ---> YOUR CODE GOES HERE.\n\n    return deg\n\ndef cross_validate_fourier(x,y,c,fourier_degs,k):\n\n# ---> YOUR CODE GOES HERE.\n\n    return deg\n\ndef cross_validate_tanh(x,y,split,tanh_degs,k):\n\n# ---> YOUR CODE GOES HERE.\n\n    return deg\n\n\n#########################################\n\n### takes poly features of the input ###\ndef build_poly(x,D):\n    F = []\n    for m in np.arange(1,D+1):\n        F.append(x**m)\n\n    temp1 = np.reshape(F,(D,np.shape(x)[0])).T\n    temp2 = np.concatenate((np.ones((np.shape(temp1)[0],1)),temp1),1)\n    F = temp2\n    return F\n\n### takes fourier features of the input ###\ndef build_fourier(x,D):\n    F = []\n    for m in np.arange(1,D+1):\n        F.append(np.cos(2*np.pi*m*x))\n        F.append(np.sin(2*np.pi*m*x))\n\n    temp1 = np.reshape(F,(2*D,np.shape(x)[0])).T\n    temp2 = np.concatenate((np.ones((np.shape(temp1)[0],1)),temp1),1)\n    F = temp2\n    return F\n\n### gradient descent for single layer tanh nn ###\ndef tanh_grad_descent(x,y,M):\n\n    # initializations\n    P = np.size(x)\n    b = M*np.random.randn()\n    w = M*np.random.randn(M,1)\n    c = M*np.random.randn(M,1)\n    v = M*np.random.randn(M,1)\n    l_P = np.ones((P,1))\n\n    # stoppers + step length\n    max_its = 10000\n    k = 1\n    alpha = 1e-4\n\n    ### main ###\n    while (k <= max_its):\n        # update gradients\n        q = np.zeros((P,1))\n        for p in np.arange(0,P):\n            q[p] = b + np.dot(w.T,np.tanh(c + v*x[p])) - y[p]\n\n        grad_b = np.dot(l_P.T,q)\n        grad_w = np.zeros((M,1))\n        grad_c = np.zeros((M,1))\n        grad_v = np.zeros((M,1))\n        for m in np.arange(0,M):\n            t = np.tanh(c[m] + x*v[m])\n            s = (1/np.cosh(c[m] + x*v[m]))**2\n            grad_w[m] = 2*np.dot(l_P.T,(q*t))\n            grad_c[m] = 2*np.dot(l_P.T,(q*s)*w[m])\n            grad_v[m] = 2*np.dot(l_P.T,(q*x*s)*w[m])\n\n        # take gradient steps\n        b = b - alpha*grad_b\n        w = w - alpha*grad_w\n        c = c - alpha*grad_c\n        v = v - alpha*grad_v\n\n        # update stopper and container\n        k = k + 1\n\n    return b, w, c, v\n\ndef plot_poly(x,y,deg,k):\n\n    # calculate weights\n    X = build_poly(x,deg)\n    temp = np.linalg.pinv(np.dot(X.T,X))\n    w = np.dot(np.dot(temp,X.T),y)\n\n    # output model\n    in_put = np.reshape(np.arange(0,1,.01),(100,1))\n    out_put = np.zeros(np.shape(in_put))\n    for n in np.arange(0,deg+1):\n        out_put = out_put + w[n]*(in_put**n)\n\n    # plot\n    plt.plot(x_true[0], y_true[0], \'k.\', linewidth=1.5)\n    plt.scatter(x,y,s=40,color = \'k\')\n    plt.plot(in_put,out_put,\'b\',linewidth=2)\n\n    # clean up plot\n    plt.xlim(0,1)\n    plt.ylim(-1.4,1.4)\n    title = \'k = \' + str(k) + \', deg = \' + str(deg)\n    plt.title(title)\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n\ndef plot_fourier(x,y,deg,k):\n\n    # calculate weights\n    X = build_fourier(x,deg)\n    temp = np.linalg.pinv(np.dot(X.T,X))\n    w = np.dot(np.dot(temp,X.T),y)\n\n    # output model\n    period = 1\n    in_put = np.reshape(np.arange(0,1,.01),(100,1))\n    out_put = w[0]*np.ones(np.shape(in_put))\n    for n in np.arange(1,deg+1):\n        out_put = out_put + w[2*n-1]*np.cos((1/period)*2*np.pi*n*in_put)\n        out_put = out_put + w[2*n]*np.sin((1/period)*2*np.pi*n*in_put)\n\n    # plot\n    plt.plot(x_true[0], y_true[0], \'k.\', linewidth=1.5)\n    plt.scatter(x,y,s=40,color = \'k\')\n    plt.plot(in_put,out_put,\'r\',linewidth=2)\n\n    # clean up plot\n    plt.xlim(0,1)\n    plt.ylim(-1.4,1.4)\n    title = \'k = \' + str(k) + \', deg = \' + str(deg)\n    plt.title(title)\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n\ndef plot_tanh(x,y,deg,k):\n\n    # calculate weights\n    colors = [\'m\',\'c\']\n    num_inits = 2\n    for foo in np.arange(0,num_inits):\n        b, w, c, v = tanh_grad_descent(x,y,deg)\n\n        # plot resulting fit\n        plot_approx(b,w,c,v,colors[foo])\n\n    # plot\n    plt.plot(x_true[0], y_true[0], \'k.\', linewidth=1.5)\n    plt.scatter(x,y,s=40,color = \'k\')\n\n    # clean up plot\n    plt.xlim(0,1)\n    plt.ylim(-1.4,1.4)\n    title = \'k = \' + str(k) + \', deg = \' + str(deg)\n    plt.title(title)\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n\n### plot tanh approximation ###\ndef plot_approx(b,w,c,v,color):\n    M = np.size(c)\n    s = np.arange(0,1,.01)\n    t = b\n    for m in np.arange(0,M):\n        t = t + w[m]*np.tanh(c[m] + v[m]*s)\n\n    s = np.reshape(s,np.shape(t))\n    plt.plot(s[0],t[0], color = color, linewidth=2)\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n\ncompare_maps_regression_hw()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_5/Exercise_5_7/poly_regression_hw.py,14,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport numpy as np\nimport numpy.matlib\nimport matplotlib.pyplot as plt\nfrom __future__ import division\nimport pylab\n\ndef poly_regression_hw():\n\n    # load data\n    x, y = load_data()\n    deg = [1,3,5,7,15,20]           # degree polys to try\n\n    # plot data\n    plot_data(x,y,deg)\n\n    # generate nonlinear features\n    mses = []\n\n    for D in np.arange(0,np.size(deg)):\n        # generate poly feature transformation\n        F = poly_features(x,deg[D])\n\n        # get weights\n        temp = np.linalg.pinv(np.dot(F,F.T))\n        w = np.dot(np.dot(temp,F),y)\n        MSE = np.linalg.norm(np.dot(F.T,w)-y)/np.size(y)\n        mses.append(MSE)\n\n        # plot fit to data\n        plt.subplot(2,3,D+1)\n        plot_model(w,deg[D])\n\n\n    # make plot of mse\'s\n    plot_mse(mses,deg)\n    plt.show()\n\n### takes poly features of the input ###\ndef poly_features(x,D):\n\n# ---->  YOUR CODE GOES HERE.\n\n    return F\n\ndef load_data():\n    data = np.array(np.genfromtxt(\'noisy_sin_samples.csv\', delimiter=\',\'))\n    x = np.reshape(data[:,0],(np.size(data[:,0]),1))\n    y = np.reshape(data[:,1],(np.size(data[:,1]),1))\n    return x,y\n\n### plot the D-fit ###\ndef plot_model(w,D):\n\n    # plot determined surface in 3d space\n    s = np.arange(0,1,.01)\n    f = []\n    for m in np.arange(1,D+1):\n        f.append(s**m)\n\n    f = np.reshape(f,(D,np.size(s))).T\n    temp = np.dot(f,np.reshape(w[1:],(np.size(w)-1,1)))\n\n    f = np.sum(temp,1) + w[0]\n\n    # plot contour in original space\n    plt.plot(s,f, color = \'r\', linewidth = 2)\n    plt.ylim(-1.5,1.5)\n    plt.xlim(0,1)\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n\n\n### plot mse\'s over all D tested ###\ndef plot_mse(mses,deg):\n    plt.figure(2)\n    plt.plot(np.arange(1,np.size(mses)+1),mses,\'ro--\')\n    plt.title(\'MSE on entire dataset in D\', fontsize=18)\n    plt.xlabel(\'D\', fontsize=18)\n    plt.ylabel(\'MSE       \', fontsize=18)\n\n### plot data ###\ndef plot_data(x,y,deg):\n    for i in np.arange(1,7):\n        plt.subplot(2,3,i)\n        plt.scatter(x,y,s = 30, color = \'k\')\n\n        # graph info labels\n        s = \'D = \' + str(deg[i-1])\n        plt.title(s, fontsize=15)\n\npoly_regression_hw()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_5/Exercise_5_8/fourier_regression_hw.py,15,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport numpy as np\nimport numpy.matlib\nimport matplotlib.pyplot as plt\nfrom __future__ import division\nimport pylab\n\ndef fourier_regression_hw():\n\n    # load data\n    x, y = load_data()\n    deg = [1,3,5,7,9,15]           # degrees to try\n\n    # plot data\n    plot_data(x,y,deg)\n\n    # generate nonlinear features\n    mses = []\n\n    for D in np.arange(0,np.size(deg)):\n        # generate poly feature transformation\n        F = fourier_features(x,deg[D])\n\n        # get weights\n        temp = np.linalg.pinv(np.dot(F,F.T))\n        w = np.dot(np.dot(temp,F),y)\n        MSE = np.linalg.norm(np.dot(F.T,w)-y)/np.size(y)\n        mses.append(MSE)\n\n        # plot fit to data\n        plt.subplot(2,3,D+1)\n        plot_model(w,deg[D])\n\n\n    # make plot of mse\'s\n    plot_mse(mses,deg)\n    plt.show()\n\n### takes fourier features of the input ###\ndef fourier_features(x,D):\n\n# ----->  YOUR CODE GOES HERE.\n\n    return F\n\ndef load_data():\n    data = np.array(np.genfromtxt(\'noisy_sin_samples.csv\', delimiter=\',\'))\n    x = np.reshape(data[:,0],(np.size(data[:,0]),1))\n    y = np.reshape(data[:,1],(np.size(data[:,1]),1))\n    return x,y\n\n\n## plot the D-fit ###\ndef plot_model(w,D):\n\n    # plot determined surface in 3d space\n    s = np.arange(0,1,.01)\n    f = []\n    for m in np.arange(1,D+1):\n        f.append(np.cos(2*np.pi*m*s))\n        f.append(np.sin(2*np.pi*m*s))\n\n    f = np.reshape(f,(2*D,np.size(s))).T\n    temp = np.dot(f,np.reshape(w[1:],(np.size(w)-1,1)))\n\n    f = np.sum(temp,1) + w[0]\n\n    # plot contour in original space\n    plt.plot(s,f, color = \'r\', linewidth = 2)\n    plt.ylim(-1.5,1.5)\n    plt.xlim(0,1)\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n\n### plot mse\'s over all D tested ###\ndef plot_mse(mses,deg):\n    plt.figure(2)\n    plt.plot(deg,mses,\'ro--\')\n    plt.title(\'MSE on entire dataset in D\', fontsize=18)\n    plt.xlabel(\'D\', fontsize=18)\n    plt.ylabel(\'MSE       \', fontsize=18)\n\n\n### plot data ###\ndef plot_data(x,y,deg):\n    for i in np.arange(1,7):\n        plt.subplot(2,3,i)\n        plt.scatter(x,y,s = 30, color = \'k\')\n\n        # graph info labels\n        s = \'D = \' + str(deg[i-1])\n        plt.title(s, fontsize=15)\n\nfourier_regression_hw()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_5/Exercise_5_9/tanh_regression_hw.py,26,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport numpy as np\nimport numpy.matlib\nimport matplotlib.pyplot as plt\nfrom __future__ import division\n\ndef tanh_regression_hw():\n\n    # load data\n    x, y = load_data()\n\n    # plot data\n    plt.subplot(1,2,1)\n    plot_data(x,y)\n\n    # perform gradient descent to fit tanh basis sum\n    num_runs = 3\n    colors = [\'r\',\'g\',\'b\']\n    for i in np.arange(0,num_runs):\n\n        # minimize least squares cost\n        b,w,c,v,obj_val = tanh_grad_descent(x,y,i)\n\n        # plot resulting fit to data\n        color = colors[i]\n        plt.subplot(1,2,1)\n        plot_approx(b,w,c,v,color)\n\n        # plot objective value decrease for current run\n        plt.subplot(1,2,2)\n        plot_obj(obj_val,color)\n\n    plt.show()\n\n### gradient descent for single layer tanh nn ###\ndef tanh_grad_descent(x,y,i):\n\n    # initialize weights and other items\n    b, w, c, v = initialize(i)\n    P = np.size(x)\n    M = 4\n    alpha = 1e-3\n    l_P = np.ones((P,1))\n\n    # stoppers and containers\n    max_its = 15000\n    k = 1\n    obj_val = []       # container for objective value at each iteration\n\n    ### main ###\n    while (k <= max_its):\n        # update gradients\n# --->  grad_b =\n# --->  grad_w =\n# --->  grad_c =\n# --->  grad_v =\n\n        # take gradient steps\n        b = b - alpha*grad_b\n        w = w - alpha*grad_w\n        c = c - alpha*grad_c\n        v = v - alpha*grad_v\n\n        # update stopper and container\n        k = k + 1\n        obj_val.append(calculate_obj_val(x,y,b,w,c,v))\n\n    return b, w, c, v, obj_val\n\n\ndef load_data():\n    data = np.array(np.genfromtxt(\'noisy_sin_samples.csv\', delimiter=\',\'))\n    x = np.reshape(data[:,0],(np.size(data[:,0]),1))\n    y = np.reshape(data[:,1],(np.size(data[:,1]),1))\n    return x,y\n\n\ndef calculate_obj_val(x,y,b,w,c,v):\n    s = 0\n    P = np.size(x)\n    for p  in np.arange(0,P):\n        s = s + ((b + np.dot(w.T,np.tanh(c + v*x[p])) - y[p])**2)\n    return s[0][0]\n\n### initialize parameters ###\ndef initialize(i):\n    b = 0\n    w = 0\n    c = 0\n    v = 0\n    if (i == 0):\n        b = -0.454\n        w = np.array([[-0.3461],[-0.8727],[0.6312 ],[0.9760]])\n        c = np.array([[-0.6584],[0.7832],[-1.0260],[0.5559]])\n        v = np.array([[-0.8571],[-0.8623],[1.0418],[-0.4081]])\n\n    elif (i == 1):\n        b = -1.1724\n        w = np.array([[.09],[-1.99],[-3.68],[-.64]])\n        c = np.array([[-3.4814],[-0.3177],[-4.7905],[-1.5374]])\n        v = np.array([[-0.7055],[-0.6778],[0.1639],[-2.4117]])\n\n    else:\n        b = 0.1409\n        w = np.array([[0.5207],[-2.1275],[10.7415],[3.5584]])\n        c = np.array([[2.7754],[0.0417],[-5.5907],[-2.5756]])\n        v = np.array([[-1.8030],[0.7578],[-2.4235],[0.6615]])\n\n    return b, w, c, v\n\n\n### plot tanh approximation ###\ndef plot_approx(b,w,c,v,color):\n    M = np.size(c)\n    s = np.arange(0,1,.01)\n    t = b\n    for m in np.arange(0,M):\n        t = t + w[m]*np.tanh(c[m] + v[m]*s)\n\n    s = np.reshape(s,np.shape(t))\n    plt.plot(s[0],t[0], color = color, linewidth=2)\n    plt.xlim(0,1)\n    plt.ylim(-1.4,1.4)\n    plt.xlabel(\'$x$\', fontsize=20)\n    plt.ylabel(\'$y$  \', fontsize=20)\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n\n### plot objective value at each iteration of gradient descent ###\ndef plot_obj(o, color):\n    if np.size(o) == 15000:\n        plt.plot(np.arange(100,np.size(o)), o[100:], color = color, linewidth=2)\n    else:\n        plt.plot(np.arange(1,np.size(o)+1), o, color = color)\n\n    plt.xlabel(\'$k$\', fontsize=20)\n    plt.ylabel(\'$g(w^k)$  \', fontsize=20)\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n\n\n### plot data ###\ndef plot_data(x,y):\n    plt.scatter(x,y,s=30,color=\'k\')\n\ntanh_regression_hw()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_6/Exercise_6_3/poly_classification_hw.py,32,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport numpy as np\nimport numpy.matlib\nimport matplotlib.pyplot as plt\nfrom __future__ import division\n\ndef ploy_classification_hw():\n\n    # parameters to play with\n    poly_degs = np.arange(1,9)    # range of poly models to compare\n\n    # load data\n    X, y = load_data()\n\n    # perform feature transformation + classification\n    classify(X,y,poly_degs)\n    plt.show()\n\n\n### builds (poly) features based on input data ###\ndef poly_features(data, deg):\n\n    # ---> YOUR CODE GOES HERE.\n\n    return F\n\n\n### sigmoid function for use with log_loss_newton ###\ndef sigmoid(z):\n    return 1/(1+np.exp(-z))\n\n\ndef load_data():\n    data = np.array(np.genfromtxt(\'2eggs_data.csv\', delimiter=\',\'))\n    X = data[:,0:-1]\n    y = data[:,-1]\n    return X,y\n\n\n### plots learned model ###\ndef plot_poly(w,deg,color):\n    # Generate poly seperator\n    o = np.arange(0,1,.01)\n    s, t = np.meshgrid(o,o)\n    s = np.reshape(s,(np.size(s),1))\n    t = np.reshape(t,(np.size(t),1))\n    f = np.zeros((np.size(s),1))\n    count = 0\n\n    for n in np.arange(0,deg+1):\n        for m in np.arange(0,deg+1):\n            if (n + m <= deg):\n                f = f + w[count]*((s**n)*(t**m))\n                count = count + 1\n\n    s = np.reshape(s,(np.size(o),np.size(o)))\n    t = np.reshape(t,(np.size(o),np.size(o)))\n    f = np.reshape(f,(np.size(o),np.size(o)))\n\n    # plot contour in original space\n    plt.contour(s,t,f,1, color =\'k\')\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n    plt.hold(True)\n\n### plots points for each fold ###\ndef plot_pts(X,y):\n\n    # plot training set\n    ind = np.nonzero(y==1)[0]\n    plt.plot(X[ind,0],X[ind,1],\'ro\')\n    ind = np.nonzero(y==-1)[0]\n    plt.plot(X[ind,0],X[ind,1],\'bo\')\n    plt.hold(True)\n\n\n### plots training errors ###\ndef plot_errors(poly_degs, errors):\n\n    plt.plot(np.arange(1,np.size(poly_degs)+1), errors,\'m--\')\n    plt.plot(np.arange(1,np.size(poly_degs)+1), errors,\'mo\')\n\n    #ax2.set_aspect(\'equal\')\n    plt.xlabel(\'D\')\n    plt.ylabel(\'error\')\n\n\n### newton\'s method for log-loss classifier ###\ndef log_loss_newton(D,b):\n    # initialize\n    w = np.random.randn(np.shape(D)[0],1)\n\n    # precomputations\n    H = np.dot(np.diag(b),D.T)\n    grad = 1\n    n = 1\n\n    ### main ###\n    while (n <= 30) & (np.linalg.norm(grad) > 1e-5):\n\n        # prep gradient for logistic objective\n        r = sigmoid(-np.dot(H,w))\n        g = r*(1 - r)\n        grad = -np.dot(H.T,r)\n\n\n\n        hess = np.dot(D,np.dot(np.diag(np.ravel(g)),D.T))\n\n        # take Newton step\n        s = np.dot(hess,w) - grad\n        w = np.dot(np.linalg.pinv(hess),s)\n        n = n + 1\n\n    return w\n\n\ndef classify(X,y,poly_degs):\n\n    errors = []\n    fig1 = plt.figure(facecolor = \'white\')\n    # solve for weights and collect errors\n    for i in np.arange(1,np.shape(poly_degs)[0]+1):\n        # generate features\n        poly_deg = poly_degs[i-1]\n        F = poly_features(X,poly_deg)\n\n        # run logistic regression\n        w = log_loss_newton(F.T,y)\n\n        # output model\n        ax1 = fig1.add_subplot(2,4,i)\n        plot_poly(w, poly_deg,\'b\')\n        title = \'D = \' + str(i)\n        plt.title(title, fontsize = 12)\n        plot_pts(X,y)\n\n        # calculate training errors\n        resid = evaluate(F,y,w)\n        errors.append(resid)\n\n    # plot training errors for visualization\n    fig2 = plt.figure(facecolor = \'white\')\n    plot_errors(poly_degs, errors)\n\n\n### evaluates error of a learned model ###\ndef evaluate(A,b,w):\n    s = np.dot(A,w)\n    s[np.nonzero(s>0)] = 1\n    s[np.nonzero(s<=0)] = -1\n    t = s*np.reshape(b,(np.size(b),1))\n    t[np.nonzero(t<0)] = 0\n    score = 1 - (t.sum()/np.size(t))\n    return score\n\n\nploy_classification_hw()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_6/Exercise_6_5/single_layer_classification_hw.py,27,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport numpy as np\nimport numpy.matlib\nimport matplotlib.pyplot as plt\nfrom __future__ import division\n\ndef single_layer_classification_hw():\n\n    # load data\n    X, y = load_data()\n    global M\n    M = 4     # number of hidden units\n\n    # perform gradient descent to fit tanh basis sum\n    b,w,c,V = tanh_softmax(X.T,y,M)\n\n    # plot resulting fit\n    plot_separator(b,w,c,V,X,y)\n    plt.show()\n\n\n### gradient descent for single layer tanh nn ###\ndef tanh_softmax(X,y,M):\n\n    y = np.reshape(y,(np.size(y),1))\n\n    # initializations\n    N = np.shape(X)[0]\n    P = np.shape(X)[1]\n\n    b = np.random.randn()\n    w = np.random.randn(M,1)\n    c = np.random.randn(M,1)\n    V = np.random.randn(N,M)\n    l_P = np.ones((P,1))\n\n    # stoppers\n    max_its = 10000\n    grad = 1\n    count = 1\n\n    ### main ###\n    while (count <= max_its) & (np.linalg.norm(grad) > 1e-5):\n\n        F = obj(c,V,X)\n\n        # calculate gradients\n# --->  grad_b =\n# --->  grad_w =\n# --->  grad_c =\n# --->  grad_V =\n\n        # determine steplength\n        alpha = 1e-2\n\n        # take gradient steps\n        b = b - alpha*grad_b\n        w = w - alpha*grad_w\n        c = c - alpha*grad_c\n        V = V - alpha*grad_V\n\n        # update stoppers\n        count = count + 1\n\n    return b, w, c, V\n\n\n### load data\ndef load_data():\n\n    data = np.array(np.genfromtxt(\'genreg_data.csv\', delimiter=\',\'))\n    A = data[:,0:-1]\n    b = data[:,-1]\n\n    # plot data\n    ind = np.nonzero(b==1)[0]\n    plt.plot(A[ind,0],A[ind,1],\'ro\')\n    ind = np.nonzero(b==-1)[0]\n    plt.plot(A[ind,0],A[ind,1],\'bo\')\n    plt.hold(True)\n\n    return A,b\n\n\n\ndef sigmoid(z):\n    return 1/(1+np.exp(-z))\n\n\n# plot the seprator + surface\ndef plot_separator(b,w,c,V,X,y):\n\n    s = np.arange(-1,1,.01)\n    s1, s2 = np.meshgrid(s,s)\n\n    s1 = np.reshape(s1,(np.size(s1),1))\n    s2 = np.reshape(s2,(np.size(s2),1))\n    g = np.zeros((np.size(s1),1))\n\n    t = np.zeros((2,1))\n    for i in np.arange(0,np.size(s1)):\n        t[0] = s1[i]\n        t[1] = s2[i]\n        F = obj(c,V,t)\n        g[i] = np.tanh(b + np.dot(F.T,w))\n\n    s1 = np.reshape(s1,(np.size(s),np.size(s)))\n    s2 = np.reshape(s2,(np.size(s),np.size(s)))\n    g = np.reshape(g,(np.size(s),np.size(s)))\n\n    # plot contour in original space\n    plt.contour(s1,s2,g,1,color = \'k\')\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n    plt.xlim(0,1)\n    plt.ylim(0,1)\n    plt.hold(True)\n\n\ndef obj(z,H,A):\n    F = np.zeros((M,np.shape(A)[1]))\n    for p in np.arange(0,np.shape(A)[1]):\n        F[:,p] = np.ravel(np.tanh(z + np.dot(H.T,np.reshape(A[:,p],(np.shape(A)[0],1)))))\n\n    return F\n\nsingle_layer_classification_hw()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_6/Exercise_6_7/ova_fixed_basis.py,41,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport numpy as np\nimport numpy.matlib\nimport matplotlib.pyplot as plt\nfrom __future__ import division\nimport copy as cp\n\n\ndef ova_fixed_basis():\n\n    colors = [\'m\',\'b\',\'r\',\'c\']\n\n    # parameters to play with\n    deg = 2    # range of poly models to compare\n\n    # load data\n    X, y = load_data()\n    num_classes = np.size(np.unique(y))  # number of classes = number of separators\n\n    # make individual classifiers for each class\n    ot = np.arange(0,1,.002)\n    t1, t2 = np.meshgrid(ot,ot)\n    t1 = np.reshape(t1,(np.size(t1),1))\n    t2 = np.reshape(t2,(np.size(t2),1))\n    X2 = np.concatenate((t1,t2),1)\n    F = poly_features(X,deg)\n    F2 = poly_features(X2,deg)\n\n    for q in np.arange(1,num_classes+1):\n\n        ind = np.nonzero(y == q)\n        ind2 = np.nonzero(y != q)\n        ytemp = cp.deepcopy(y)\n        ytemp[ind] = 1\n        ytemp[ind2] = -1\n\n        w = log_loss_newton(F.T, ytemp)\n        plt.subplot(1,num_classes+1,q)\n        plot_poly(w,deg,colors[q])\n\n        # calculate val\n        u = np.dot(F2,w)\n\n        if q ==1:\n            M = u\n        else:\n            M = np.concatenate((M,u),1)\n\n    z = np.argmax(M,1)\n\n    # plot max separator on the whole thing\n    t1 = np.reshape(t1,(np.size(ot),np.size(ot)))\n    t2 = np.reshape(t2,(np.size(ot),np.size(ot)))\n    z = np.reshape(z,(np.size(ot),np.size(ot)))\n\n    plt.subplot(1,num_classes + 1,num_classes + 1)\n\n    for i in np.arange(1,num_classes):\n        plt.contour(t1,t2,z,2,color = \'k\')\n\n\n    plt.show()\n\n\n### builds (poly) features based on input data ###\ndef poly_features(data, deg):\n\n# ---> YOUR CODE GOES HERE.\n\n    return F\n\n\n### newton\'s method for log-loss classifier ###\ndef log_loss_newton(D,b):\n    # initialize\n    w = np.random.randn(np.shape(D)[0],1)\n\n    # precomputations\n    H = np.dot(np.diag(b),D.T)\n    grad = 1\n    n = 1\n\n    ### main ###\n    while (n <= 30) & (np.linalg.norm(grad) > 1e-5):\n\n        # prep gradient for logistic objective\n        r = sigmoid(-np.dot(H,w))\n        g = r*(1 - r)\n        grad = -np.dot(H.T,r)\n        hess = np.dot(D,np.dot(np.diag(np.ravel(g)),D.T))\n\n        # take Newton step\n        s = np.dot(hess,w) - grad\n        w = np.dot(np.linalg.pinv(hess),s)\n        n = n + 1\n\n    return w\n\n\n### sigmoid function for use with log_loss_newton ###\ndef sigmoid(z):\n    return 1/(1+np.exp(-z))\n\n\n### plots learned model ###\ndef plot_poly(w,deg,color):\n    # Generate poly seperator\n    o = np.arange(0,1,.01)\n    s, t = np.meshgrid(o,o)\n    s = np.reshape(s,(np.size(s),1))\n    t = np.reshape(t,(np.size(t),1))\n    f = np.zeros((np.size(s),1))\n    count = 0\n\n    for n in np.arange(0,deg+1):\n        for m in np.arange(0,deg+1):\n            if (n + m <= deg):\n                f = f + w[count]*((s**n)*(t**m))\n                count = count + 1\n\n    s = np.reshape(s,(np.size(o),np.size(o)))\n    t = np.reshape(t,(np.size(o),np.size(o)))\n    f = np.reshape(f,(np.size(o),np.size(o)))\n\n    # plot contour in original space\n    plt.contour(s,t,f,1, colors =color)\n\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n    plt.axis(\'equal\')\n\n\n### load data ###\ndef load_data():\n    # load data from file\n    data = np.array(np.genfromtxt(\'bullseye_data.csv\', delimiter=\',\'))\n    X = data[:,0:-1]\n    y = data[:,-1]\n\n    # how many classes in the data?  4 maximum for this toy.\n    class_labels = np.unique(y)         # class labels\n    num_classes = np.size(class_labels)\n\n    fig = plt.figure(facecolor = \'white\')\n    colors = [\'m\',\'b\',\'r\',\'c\']\n\n    for j in np.arange(1,num_classes+1):\n        # plot data\n        plt.subplot(1,num_classes + 1,j)\n        ind = np.nonzero(y != j)\n        plt.scatter(X[ind,0],X[ind,1],s = 30, facecolors = \'None\', edgecolors = \'grey\')\n        ind = np.nonzero(y == j)\n        plt.scatter(X[ind,0],X[ind,1],s = 30, facecolors = \'None\', edgecolors = colors[j])\n\n        plt.subplot(1,num_classes + 1,num_classes + 1)\n        plt.scatter(X[ind,0],X[ind,1],s = 30, facecolors = \'None\', edgecolors = colors[j])\n\n        plt.gca().xaxis.set_major_locator(plt.NullLocator())\n        plt.gca().yaxis.set_major_locator(plt.NullLocator())\n        plt.axis(\'equal\')\n\n    return X,y\n\n\nova_fixed_basis()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_9/Exercise_9_1/K_means_demo.py,6,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport numpy as np\nimport numpy.matlib\nimport matplotlib.pyplot as plt\nfrom __future__ import division\n\ndef K_means_demo():\n\n    # load data\n    X = np.array(np.genfromtxt(\'Kmeans_demo_data.csv\', delimiter=\',\'))\n\n    C0 = np.array([[0,0],[0,.5]])   # initial centroid locations\n\n    # run K-means\n    K = np.shape(C0)[1]\n\n    C, W = your_K_means(X, K, C0)\n\n    # plot results\n    plot_results(X, C, W, C0)\n    plt.show()\n\n\ndef your_K_means(X, K, C0):\n\n# --->  YOUR CODE GOES HERE.\n\n    return C, W\n\ndef plot_results(X, C, W, C0):\n\n    K = np.shape(C)[1]\n\n    # plot original data\n    fig = plt.figure(facecolor = \'white\')\n    ax1 = fig.add_subplot(121)\n    plt.scatter(X[0,:],X[1,:], s = 50, facecolors = \'k\')\n    plt.title(\'original data\')\n    ax1.set_xlim(-.55, .55)\n    ax1.set_ylim(-.55, .55)\n    ax1.set_aspect(\'equal\')\n\n    plt.scatter(C0[0,0],C0[1,0],s = 100, marker=(5, 2), facecolors = \'b\')\n    plt.scatter(C0[0,1],C0[1,1],s = 100, marker=(5, 2), facecolors = \'r\')\n\n    # plot clustered data\n    ax2 = fig.add_subplot(122)\n    colors = [\'b\',\'r\']\n\n    for k in np.arange(0,K):\n        ind = np.nonzero(W[k][:]==1)[0]\n        plt.scatter(X[0,ind],X[1,ind],s = 50, facecolors = colors[k])\n        plt.scatter(C[0,k],C[1,k], s = 100, marker=(5, 2), facecolors = colors[k])\n\n    plt.title(\'clustered data\')\n    ax2.set_xlim(-.55, .55)\n    ax2.set_ylim(-.55, .55)\n    ax2.set_aspect(\'equal\')\n\nK_means_demo()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_9/Exercise_9_2/PCA_demo.py,7,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport numpy as np\nimport numpy.matlib\nimport matplotlib.pyplot as plt\nfrom __future__ import division\n\ndef PCA_demo():\n\n    global n\n\n    # load in data\n    X = np.matrix(np.genfromtxt(\'PCA_demo_data.csv\', delimiter=\',\'))\n    n = np.shape(X)[0]\n    means = np.matlib.repmat(np.mean(X,0), n, 1)\n    X = X - means  # center the data\n    X = X.T\n    K = 1\n\n    # run PCA\n    C, W = your_PCA(X, K)\n\n    # plot results\n    plot_results(X, C)\n    plt.show()\n\n    return\n\n\ndef your_PCA(X, K):\n\n# ---> YOUR CODE GOES HERE.\n\n    return C, W\n\ndef plot_results(X, C):\n\n    # Print points and pcs\n    fig = plt.figure(facecolor = \'white\')\n    ax1 = fig.add_subplot(121)\n    for j in np.arange(0,n):\n        plt.scatter(X[0][:],X[1][:])\n\n    s = np.arange(C[0,0],-C[0,0],.001)\n    m = C[1,0]/C[0,0]\n    ax1.plot(s, m*s, color = \'k\', linewidth = 2)\n\n    ax1.set_xlabel(\'$b_1$\', fontsize = 14)\n    ax1.set_ylabel(\'$b_2$\', fontsize = 14)\n    ax1.set_xlim(-.5, .5)\n    ax1.set_ylim(-.5, .5)\n    ax1.set_aspect(\'equal\')\n\n    # Plot projected data\n    ax2 = fig.add_subplot(122)\n    X_proj = np.dot(C, np.linalg.solve(np.dot(C.T,C),np.dot(C.T,X)))\n    for j in np.arange(0,n):\n        plt.scatter(X_proj[0][:],X_proj[1][:])\n\n    ax2.set_xlabel(\'$b_1$\', fontsize = 14)\n    ax2.set_ylabel(\'$b_2$\', fontsize = 14)\n    ax2.set_xlim(-.5, .5)\n    ax2.set_ylim(-.5, .5)\n    ax2.set_aspect(\'equal\')\n\n    return\n\nPCA_demo()\n'"
mlrefined_exercises/ed_1/Python2/Chapter_9/Exercise_9_4/recommender_demo.py,7,"b'# This file is associated with the book\n# ""Machine Learning Refined"", Cambridge University Press, 2016.\n# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nimport numpy as np\nimport numpy.matlib\nimport matplotlib.pyplot as plt\nfrom __future__ import division\n\ndef recommender_demo():\n    # load in data\n    X = np.array(np.genfromtxt(\'recommender_demo_data_true_matrix.csv\', delimiter=\',\'))\n    X_corrupt = np.array(np.genfromtxt(\'recommender_demo_data_dissolved_matrix.csv\', delimiter=\',\'))\n\n    K = np.linalg.matrix_rank(X)\n\n    # run ALS for matrix completion\n    C, W = matrix_complete(X_corrupt, K)\n\n    # plot results\n    plot_results(X, X_corrupt, C, W)\n    plt.show()\n\n\ndef matrix_complete(X, K):\n\n# ---->  YOUR CODE GOES HERE\n    \n    return C, W\n\ndef plot_results(X, X_corrupt, C, W):\n\n    gaps_x = np.arange(0,np.shape(X)[1])\n    gaps_y = np.arange(0,np.shape(X)[0])\n\n    # plot original matrix\n    fig = plt.figure(facecolor = \'white\')\n    ax1 = fig.add_subplot(131)\n    plt.imshow(X,cmap=\'Greys_r\')\n    plt.title(\'original\')\n\n    # plot corrupted matrix\n    ax2 = fig.add_subplot(132)\n    plt.imshow(X_corrupt,cmap=\'Greys_r\')\n    plt.title(\'corrupted\')\n\n    # plot reconstructed matrix\n    ax3 = fig.add_subplot(133)\n    recon = np.dot(C,W)\n    plt.imshow(recon,cmap=\'Greys_r\')\n    RMSE_mat = np.sqrt(np.linalg.norm(recon - X,\'fro\')/np.size(X))\n    title = \'RMSE-ALS = \' + str(RMSE_mat)\n    plt.title(title,fontsize=10)\n\nrecommender_demo()\n'"
mlrefined_exercises/ed_1/Python3/Chapter_2/Exercise_2_12/convex_grad_surrogate.py,0,b'This file has been depreciated.  Please see the basic-optimization demonstration notebook in the main github repo to play with an updated version of this code.'
mlrefined_exercises/ed_1/Python3/Chapter_2/Exercise_2_12/nonconvex_grad_surrogate.py,0,b'This file has been depreciated.  Please see the basic-optimization demonstration notebook in the main github repo to play with an updated version of this code.'
mlrefined_exercises/ed_1/Python3/Chapter_2/Exercise_2_13/two_d_grad_wrapper_hw.py,0,"b'# two_d_grad_wrapper_hw is a toy wrapper to illustrate the path\n # taken by gradient descent depending on the learning rate (alpha) chosen.\n # Here alpha is kept fixed and chosen by the use. The corresponding\n # gradient steps, evaluated at the objective, are then plotted.  The plotted points on\n # the objective turn from green to red as the algorithm converges (or\n # reaches a maximum iteration count, preset to 50).\n # (nonconvex) function here is\n #\n # g(w) = -cos(2*pi*w\'*w) + 2*w\'*w\n\n # This file is associated with the book\n # ""Machine Learning Refined"", Cambridge University Press, 2016.\n # by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n\nfrom pylab import *\nfrom mpl_toolkits.mplot3d import Axes3D\n\n###### ML Algorithm functions ######\ndef gradient_descent(w0,alpha):\n    w = w0\n    g_path = []\n    w_path = []\n    w_path.append(w)\n    g_path.append(-cos(2*pi*dot(w.T,w)) + 2*dot(w.T,w))\n\n    # start gradient descent loop\n    grad = 1\n    iter = 1\n    max_its = 50\n    while linalg.norm(grad) > 10**(-5) and iter <= max_its:\n        # take gradient step\n# --->  grad =\n        w = w - alpha*grad\n\n        # update path containers\n        w_path.append(w)\n        g_path.append(-cos(2*pi*dot(w.T,w)) + 2*dot(w.T,w))\n        iter+= 1\n    g_path = asarray(g_path)\n    g_path.shape = (iter,1)\n    w_path = asarray(w_path)\n    w_path.shape = (iter,2)\n\n    # show final average gradient norm for sanity check\n    s = dot(grad.T,grad)/2\n    s = \'The final average norm of the gradient = \' + str(float(s))\n    print(s)\n\n\n    # # for use in testing if algorithm minimizing/converging properly\n    # plot(asarray(obj_path))\n    # show()\n\n    return (w_path,g_path)\n\n###### plotting functions #######\ndef make_function():\n    global fig,ax1\n\n    # prepare the function for plotting\n    r = linspace(-1.15,1.15,300)\n    s,t = meshgrid(r,r)\n    s = reshape(s,(size(s),1))\n    t = reshape(t,(size(t),1))\n    h = concatenate((s,t),1)\n    h = dot(h*h,ones((2,1)))\n    b = -cos(2*pi*h) + 2*h\n    s = reshape(s,(sqrt(size(s)),sqrt(size(s))))\n    t = reshape(t,(sqrt(size(t)),sqrt(size(t))))\n    b = reshape(b,(sqrt(size(b)),sqrt(size(b))))\n\n    # plot the function\n    fig = plt.figure(facecolor = \'white\')\n    ax1 = fig.add_subplot(111, projection=\'3d\')\n    ax1.plot_surface(s,t,b,cmap = \'Greys\',antialiased=False) # optinal surface-smoothing args rstride=1, cstride=1,linewidth=0\n    ax1.azim = 115\n    ax1.elev = 70\n\n    # pretty the figure up\n    ax1.xaxis.set_rotate_label(False)\n    ax1.yaxis.set_rotate_label(False)\n    ax1.zaxis.set_rotate_label(False)\n    ax1.get_xaxis().set_ticks([-1,1])\n    ax1.get_yaxis().set_ticks([-1,1])\n    ax1.set_xlabel(\'$w_0$   \',fontsize=20,rotation = 0,linespacing = 10)\n    ax1.set_ylabel(\'$w_1$\',fontsize=20,rotation = 0,labelpad = 50)\n    ax1.set_zlabel(\'   $g(\\mathbf{w})$\',fontsize=20,rotation = 0,labelpad = 20)\n\ndef plot_steps(w_path,g_path):\n    # colors for points\n    ax1.plot(w_path[:,0],w_path[:,1],g_path[:,0],color = [1,0,1],linewidth = 5)   # add a little to output path so its visible on top of the surface plot\n    ax1.plot(w_path[-8:-1,0],w_path[-8:-1,1],g_path[-8:-1,0],color = [1,0,0],linewidth = 5)   # add a little to output path so its visible on top of the surface plot\n\n\ndef main():\n    make_function()                             # plot objective function\n\n    # plot first run on surface\n    alpha = 10**-2\n    w0 = array([-0.7,0])\n    w0.shape = (2,1)\n    w_path,g_path = gradient_descent(w0,alpha)    # perform gradient descent\n    plot_steps(w_path,g_path)\n\n    # plot second run on surface\n    w0 = array([0.8,-0.8])\n    w0.shape = (2,1)\n    w_path,g_path = gradient_descent(w0,alpha)    # perform gradient descent\n    plot_steps(w_path,g_path)\n    show()\nmain()\n'"
mlrefined_exercises/ed_1/Python3/Chapter_2/Exercise_2_16/convex_newt_demo.py,0,b'This file has been depreciated.  Please see the basic-optimization demonstration notebook in the main github repo to play with an updated version of this code.'
mlrefined_exercises/ed_1/Python3/Chapter_2/Exercise_2_16/nonconvex_newt_demo.py,0,b'This file has been depreciated.  Please see the basic-optimization demonstration notebook in the main github repo to play with an updated version of this code.'
