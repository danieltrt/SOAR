file_path,api_count,code
graph.py,0,"b'""""""\n[main.py](main.html) |\n[graph.py](graph.html) |\n[tensor.py](tensor.html) |\n[ops.py](ops.html) |\n[session.py](session.html)\n\n[Previous: Main](main.html) | [Next: Tensors](tensor.html)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nfrom tensor import Tensor\nfrom ops import AddOp, SubOp, MulOp, DivOp, \\\n                DotOp, TransposeOp, SquareOp, NegOp, \\\n                MeanOp, SigmoidOp, AssignOp, GroupOp\n\nclass Graph(object):\n    """"""\n    `Graph` represents a computation to be evaluated by a `Session`. With the\n    exception of `Graph#tensor`, `Graph#convert`, and `Graph#gradients`, most\n    methods simply create an operation and return the output tensor of the\n    operation.\n    """"""\n\n    def tensor(self, initial_value=None, op=None):\n        """"""\n        The `tensor` method defines a new tensor with the given initial value\n        and operation.\n        """"""\n        return Tensor(initial_value=initial_value, graph=self, op=op)\n\n    def convert(self, value):\n        """"""\n        The `convert` method returns the given value if it is a `Tensor`,\n        otherwise convert it to one.\n        """"""\n        if isinstance(value, Tensor):\n            return value\n        return self.tensor(initial_value=value)\n\n    def gradients(self, y, xs):\n        """"""\n        The `gradients` method performs backpropagation using [reverse accumulation](https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation) and the [chain rule](https://en.wikipedia.org/wiki/Chain_rule#Higher_dimensions).\n\n        It traverses the graph from `y` to each `x` in `xs`, accumulating\n        gradients, and returning the partial gradients for each `xs`. We use a\n        queue to keep track of the next tensor for which to compute the\n        gradient and keep a dictionary of the gradients computed thus far.\n        Iteration starts from the target output `y` with an output gradient\n        of 1.\n        """"""\n\n        queue = []\n        queue.append((y, 1))\n\n        grads = {}\n        while len(queue) > 0:\n            y, grad_y = queue.pop(0)\n            grad_y = self.convert(grad_y)\n\n            gradients = y.op.gradient(grad_y)\n            assert len(gradients) == len(y.op.inputs)\n\n            for tensor, gradient in zip(y.op.inputs, gradients):\n                if tensor in grads:\n                    grads[tensor] += gradient\n                else:\n                    grads[tensor] = gradient\n\n                if tensor.op:\n                    queue.append((tensor, gradient))\n\n        return [grads[x] for x in xs]\n\n    # ## Operation Methods\n    # Each operation method defines a new operation with the provided input\n    # tensors and returns the operations\' output.\n\n    def add(self, a, b):\n        op = AddOp([a, b], graph=self)\n        return op.output\n\n    def sub(self, a, b):\n        op = SubOp([a, b], graph=self)\n        return op.output\n\n    def mul(self, a, b):\n        op = MulOp([a, b], graph=self)\n        return op.output\n\n    def div(self, a, b):\n        op = DivOp([a, b], graph=self)\n        return op.output\n\n    def neg(self, x):\n        op = NegOp([x], graph=self)\n        return op.output\n\n    def square(self, x):\n        op = SquareOp([x], graph=self)\n        return op.output\n\n    def sigmoid(self, x):\n        op = SigmoidOp([x], graph=self)\n        return op.output\n\n    def dot(self, a, b):\n        op = DotOp([a, b], graph=self)\n        return op.output\n\n    def transpose(self, x):\n        op = TransposeOp([x], graph=self)\n        return op.output\n\n    def mean(self, x):\n        op = MeanOp([x], graph=self)\n        return op.output\n\n    def assign(self, a, b):\n        op = AssignOp([a, b], graph=self)\n        return op.output\n\n    def group(self, inputs):\n        op = GroupOp(inputs, graph=self)\n        return op.output\n'"
main.py,5,"b'""""""\n# Implementing (parts of) TensorFlow (almost) from Scratch\n## A Walkthrough of Symbolic Differentiation\n### Jim Fleming ([@jimmfleming](https://twitter.com/jimmfleming))\n\n[main.py](main.html) |\n[graph.py](graph.html) |\n[tensor.py](tensor.html) |\n[ops.py](ops.html) |\n[session.py](session.html)\n\n[Next: The Graph](graph.html)\n\nThis [literate programming](https://en.wikipedia.org/wiki/Literate_programming)\nexercise will construct a simple 2-layer feed-forward neural network to compute\nthe [exclusive or](https://en.wikipedia.org/wiki/Exclusive_or), using [symbolic\ndifferentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) to\ncompute the gradients automatically. In total, about 500 lines of code,\nincluding comments. The only functional dependency is numpy. I highly recommend\nreading Chris Olah\'s [Calculus on Computational Graphs:\nBackpropagation](http://colah.github.io/posts/2015-08-Backprop/) for more\nbackground on what this code is doing.\n\nThe XOR task is convenient for a number of reasons: it\'s very fast to compute;\nit is not linearly separable thus requiring at least two layers and making the\ngradient calculation more interesting; it doesn\'t require more complicated\nmatrix-matrix features such as broadcasting.\n\n> (I\'m also working on a more involved example for MNIST but as soon as I added\nsupport for matrix shapes and broadcasting the code ballooned by 5x and it was\nno longer a simple example.)\n\nLet\'s start by going over the architecture. We\'re going to use four main\ncomponents:\n\n  - [`Graph`](graph.html), composed of `Tensor` nodes and `Op` nodes that\n    together represent the computation we want to differentiate.\n  - [`Tensor`](tensor.html) represents a value in the graph. Tensors keep a\n    reference to the operation that produced it, if any.\n  - [`BaseOp`](ops.html) represents a computation to perform and its\n    differentiable components. Operations hold references to their input\n    tensors and an output tensor.\n  - [`Session`](session.html) is used to evaluate tensors in the graph.\n\n**Note** the return from a graph operation is actually a tensor, representing\nthe output of the operation.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport numpy as np\nnp.random.seed(67)\n\nfrom tqdm import trange\n\nfrom graph import Graph\nfrom session import Session\n\ndef main():\n    """"""\n    The main method performs some setup then trains the model, displaying the\n    current loss along the way.\n    """"""\n\n    # Define a new graph\n    graph = Graph()\n\n    # Initialize the training data (XOR truth table)\n    X = graph.tensor(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]))\n    y = graph.tensor(np.array([[0, 1, 1, 0]]))\n\n    # Initialize the model\'s parameters (weights for each layer)\n    weights0 = graph.tensor(np.random.normal(size=(2, 4)))\n    weights1 = graph.tensor(np.random.normal(size=(4, 1)))\n\n    # Define the model\'s activations\n    activations0 = graph.sigmoid(graph.dot(X, weights0))\n    activations1 = graph.sigmoid(graph.dot(activations0, weights1))\n\n    # Define operation for computing the loss\n    # ([mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error))\n    loss_op = graph.mean(graph.square(graph.transpose(y) - activations1))\n\n    # Define operations for the gradients w.r.t. the loss and an update\n    # operation to apply the gradients to the model\'s parameters.\n    parameters = [weights0, weights1]\n    gradients = graph.gradients(loss_op, parameters)\n\n    update_op = graph.group([\n        graph.assign(param, param - grad) \\\n            for param, grad in zip(parameters, gradients)\n    ])\n\n    # Begin training... We iterate for a number of epochs, calling the session\n    # run method each time to compute the update operation and the current\n    # loss. The progress bar\'s description is updated to display the loss.\n    sess = Session(graph)\n    with trange(10000) as pbar_epoch:\n        for _ in pbar_epoch:\n            _, loss = sess.run([update_op, loss_op])\n            pbar_epoch.set_description(\'loss: {:.8f}\'.format(loss))\n\nif __name__ == \'__main__\':\n    main()\n'"
ops.py,5,"b'""""""\n[main.py](main.html) |\n[graph.py](graph.html) |\n[tensor.py](tensor.html) |\n[ops.py](ops.html) |\n[session.py](session.html)\n\n[Previous: Tensors](tensor.html) | [Next: The Session](session.html)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport numpy as np\n\nclass BaseOp(object):\n    """"""\n    `BaseOp` represents an operation that performs computation on tensors.\n    Every operation consists of the following:\n\n      - A list of `inputs`, each converted to ensure they\'re all tensors.\n      - An output tensor to represent the result of the operation (which might\n        be `None`.)\n      - A reference to the graph so that each operation can generate new\n        operations when constructing gradients.\n    """"""\n\n    def __init__(self, inputs, graph):\n        self.inputs = [graph.convert(input_) for input_ in inputs]\n        self.output = graph.tensor(op=self)\n        self.graph = graph\n\n    def compute(self, sess, *args):\n        """"""\n        The `compute` method receives as input the _evaluated_ input tensors\n        and returns the result of performing its operation on the inputs.\n        """"""\n        raise NotImplementedError()\n\n    def gradient(self, grad):\n        """"""\n        The `gradient` method computes the partial derivative w.r.t. each input\n        to the operation. (Most of the derivatives come from\n        [Wikipedia](https://en.wikipedia.org/wiki/Differentiation_rules).)\n        """"""\n        raise NotImplementedError()\n\nclass AddOp(BaseOp):\n    """"""\n    `AddOp` adds a tensor to another tensor. Uses the\n    [sum rule](https://en.wikipedia.org/wiki/Sum_rule_in_differentiation) to\n    compute the partial derivatives.\n    """"""\n\n    def compute(self, sess, a, b):\n        return a + b\n\n    def gradient(self, grad):\n        return [grad, grad]\n\nclass SubOp(BaseOp):\n    """"""\n    `SubOp` subtracts a tensor from another tensor. Also uses the\n    [sum rule](https://en.wikipedia.org/wiki/Sum_rule_in_differentiation) to\n    compute the partial derivatives.\n    """"""\n\n    def compute(self, sess, a, b):\n        return a - b\n\n    def gradient(self, grad):\n        return [grad, -grad]\n\nclass MulOp(BaseOp):\n    """"""\n    `MulOp` multiplies a tensor by another tensor. Uses the\n    [product rule](https://en.wikipedia.org/wiki/Product_rule) to compute the\n    partial derivatives.\n    """"""\n\n    def compute(self, sess, a, b):\n        return a * b\n\n    def gradient(self, grad):\n        a, b = self.inputs\n        return [grad * b, grad * a]\n\nclass DivOp(BaseOp):\n    """"""\n    `DivOp` divides a tensor by another tensor. Uses the\n    [quotient rule](https://en.wikipedia.org/wiki/Quotient_rule) to compute the\n    partial derivatives.\n    """"""\n\n    def compute(self, sess, a, b):\n        return a / b\n\n    def gradient(self, grad):\n        a, b = self.inputs\n        return [grad / b, grad * (-a / self.graph.square(b))]\n\nclass NegOp(BaseOp):\n    """"""\n    `NegOp` negates a tensor.\n    """"""\n\n    def compute(self, sess, x):\n        return -x\n\n    def gradient(self, grad):\n        return [-grad]\n\nclass DotOp(BaseOp):\n    """"""\n    `DotOp` computes the dot product between two tensors. Uses the\n    [product rule](https://en.wikipedia.org/wiki/Product_rule) to compute the\n    partial derivatives. Note that here we need to transpose the terms and\n    perform a dot product, assuming matrices rather than scalars.\n    """"""\n\n    def compute(self, sess, a, b):\n        return np.dot(a, b)\n\n    def gradient(self, grad):\n        a, b = self.inputs\n        return [\n            self.graph.dot(grad, self.graph.transpose(b)),\n            self.graph.dot(self.graph.transpose(a), grad),\n        ]\n\nclass SquareOp(BaseOp):\n    """"""\n    `SquareOp` squares a tensor.\n    """"""\n\n    def compute(self, sess, x):\n        return np.square(x)\n\n    def gradient(self, grad):\n        x = self.inputs[0]\n        return [grad * (2 * x)]\n\nclass TransposeOp(BaseOp):\n    """"""\n    `TransposeOp` tranposes a tensor.\n    """"""\n\n    def compute(self, sess, x):\n        return np.transpose(x)\n\n    def gradient(self, grad):\n        return [self.graph.transpose(grad)]\n\nclass SigmoidOp(BaseOp):\n    """"""\n    `SigmoidOp` implements the\n    [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) and its\n    derivative. Notice that the derivative uses the output of the operation\n    which saves recomputation.\n    """"""\n\n    def compute(self, sess, x):\n        return 1 / (1 + np.exp(-x))\n\n    def gradient(self, grad):\n        y = self.output\n        return [grad * (y * (1 - y))]\n\nclass MeanOp(BaseOp):\n    """"""\n    `MeanOp` computes the mean of a tensor. **Note** the gradient here is\n    intentionally incorrect because computing it requires knowing the shape of\n    the input and output tensors. Fortunately, gradients are fairly malleable\n    in optimization.\n    """"""\n\n    def compute(self, sess, x):\n        return np.mean(x)\n\n    def gradient(self, grad):\n        return [grad]\n\nclass GroupOp(BaseOp):\n    """"""\n    `GroupOp` exploits the fact that each input to the operation is\n    automatically evaluated before computing the operation\'s output, allowing\n    us to group together the evaluation of multiple operations. It\'s input\n    gradients come from simply broadcasting the output gradient.\n    """"""\n\n    def compute(self, sess, *args):\n        return None\n\n    def gradient(self, grad):\n        return [grad] * len(self.inputs)\n\nclass AssignOp(BaseOp):\n    """"""\n    `AssignOp` updates the session\'s current state for a tensor. It is not\n    differentiable in this implementation.\n    """"""\n\n    def compute(self, sess, a, b):\n        assert a.shape == b.shape, \\\n            \'shapes must match to assign: {} != {}\' \\\n                .format(a.shape, b.shape)\n        sess.state[self.inputs[0]] = b\n        return b\n'"
session.py,0,"b'""""""\n[main.py](main.html) |\n[graph.py](graph.html) |\n[tensor.py](tensor.html) |\n[ops.py](ops.html) |\n[session.py](session.html)\n\n[Previous: Operations](ops.html)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport numpy as np\n\nclass Session(object):\n    """"""\n    `Session` performs computation on a graph.\n    """"""\n\n    def __init__(self, graph):\n        """"""\n        Initializing a session with a graph and a state dictionary to hold\n        tensor values.\n        """"""\n        self.graph = graph\n        self.state = {}\n\n    def run_op(self, op, context):\n        """"""\n        `run_op` takes as input an operation to run and a context to fetch\n        pre-evaluted tensors.\n        """"""\n        args = [self.eval_tensor(tensor, context) for tensor in op.inputs]\n        return op.compute(self, *args)\n\n    def eval_tensor(self, tensor, context):\n        """"""\n        `eval_tensor` takes as input a tensor to evaluate and a context to\n        fetch pre-evaluted tensors. If the tensor is not already in the context\n        there are three possibilities for evaluating the tensor:\n\n          - The tensor has an operation and is therefore the result of the\n            operation that must be computed.\n          - The tensor has an active state from another session run that can be\n            fetched.\n          - The tensor has an initial value from its instantiation that can be\n            fetched and added to the state.\n        """"""\n        if tensor not in context:\n            if tensor.op is not None:\n                context[tensor] = self.run_op(tensor.op, context)\n            elif tensor in self.state and self.state[tensor] is not None:\n                context[tensor] = self.state[tensor]\n            elif tensor not in self.state and tensor.initial_value is not None:\n                context[tensor] = self.state[tensor] = tensor.initial_value\n\n        return context[tensor]\n\n    def run(self, tensors, feed_dict=None):\n        """"""\n        `run` takes a list of tensors to evaluate and a feed dictionary that\n        can be used to override tensors.\n        """"""\n        context = {}\n\n        if feed_dict:\n            context.update(feed_dict)\n\n        return [self.eval_tensor(tensor, context) for tensor in tensors]\n'"
tensor.py,0,"b'""""""\n[main.py](main.html) |\n[graph.py](graph.html) |\n[tensor.py](tensor.html) |\n[ops.py](ops.html) |\n[session.py](session.html)\n\n[Previous: The Graph](graph.html) | [Next: Operations](ops.html)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport numpy as np\n\nclass Tensor(object):\n    """"""\n    `Tensor` represents a _value_ in the graph. It\'s just a data container with\n    methods for operator overloading (each of which delegate to the graph). It\n    includes:\n\n      - The initial value of the tensor.\n      - The operation which produced the tensor, if applicable.\n      - A reference to the graph this tensor belongs to.\n    """"""\n\n    def __init__(self, initial_value, op, graph):\n        self.initial_value = initial_value\n        self.graph = graph\n        self.op = op\n\n    # ## [Operator Overloading](https://docs.python.org/2/reference/datamodel.html?highlight=__radd__#emulating-numeric-types)\n    def __add__(self, other):\n        return self.graph.add(self, other)\n\n    def __sub__(self, other):\n        return self.graph.sub(self, other)\n\n    def __mul__(self, other):\n        return self.graph.mul(self, other)\n\n    def __truediv__(self, other):\n        return self.graph.div(self, other)\n\n    def __neg__(self):\n        return self.graph.neg(self)\n\n    # ## [Reverse Operator Overloading](https://docs.python.org/2/reference/datamodel.html?highlight=__radd__#object.__radd__)\n    def __radd__(self, other):\n        return self.graph.add(other, self)\n\n    def __rsub__(self, other):\n        return self.graph.sub(other, self)\n\n    def __rmul__(self, other):\n        return self.graph.mul(other, self)\n\n    def __rtruediv__(self, other):\n        return self.graph.div(other, self)\n'"
tf_test.py,3,"b""from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport numpy as np\nnp.random.seed(67)\n\nimport tensorflow as tf\n\nfrom tqdm import trange\n\ndef main():\n    X = tf.constant([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=tf.float32)\n    y = tf.constant([[0, 1, 1, 0]], dtype=tf.float32)\n\n    weights0 = tf.Variable(np.random.normal(size=(2, 4)), dtype=tf.float32)\n    weights1 = tf.Variable(np.random.normal(size=(4, 1)), dtype=tf.float32)\n\n    activations0 = tf.sigmoid(tf.matmul(X, weights0))\n    activations1 = tf.sigmoid(tf.matmul(activations0, weights1))\n\n    loss_op = tf.reduce_mean(tf.square(tf.transpose(y) - activations1))\n\n    parameters = [weights0, weights1]\n    gradients = tf.gradients(loss_op, parameters)\n\n    update_op = tf.group(*[\n        tf.assign(param, param - grad) \\\n            for param, grad in zip(parameters, gradients)\n    ])\n\n    tf.set_random_seed(67)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        with trange(10000) as pbar_epoch:\n            for _ in pbar_epoch:\n                _, loss = sess.run([update_op, loss_op])\n                pbar_epoch.set_description('loss: {:.8f}'.format(loss))\n\nif __name__ == '__main__':\n    main()\n"""
tests/__init__.py,0,b''
tests/test_gradients.py,6,"b""from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport unittest\nimport numpy as np\n\nfrom graph import Graph\nfrom session import Session\n\nclass GradientsTestCase(unittest.TestCase):\n\n    def test_add_grad(self):\n        graph = Graph()\n\n        a = graph.tensor()\n        b = graph.tensor()\n        c = a + b\n\n        grad_a, grad_b = graph.gradients(c, [a, b])\n\n        sess = Session(graph)\n        grad_a_, grad_b_ = sess.run([grad_a, grad_b], feed_dict={a: 2, b: 1})\n\n        self.assertEqual(grad_a_, 1)\n        self.assertEqual(grad_b_, 1)\n\n    def test_sub_grad(self):\n        graph = Graph()\n\n        a = graph.tensor()\n        b = graph.tensor()\n        c = a - b\n\n        grad_a, grad_b = graph.gradients(c, [a, b])\n\n        sess = Session(graph)\n        grad_a_, grad_b_ = sess.run([grad_a, grad_b], feed_dict={a: 2, b: 1})\n\n        self.assertEqual(grad_a_, 1)\n        self.assertEqual(grad_b_, -1)\n\n    def test_mul_grad(self):\n        graph = Graph()\n\n        a = graph.tensor()\n        b = graph.tensor()\n        c = a * b\n\n        grad_a, grad_b = graph.gradients(c, [a, b])\n\n        sess = Session(graph)\n        grad_a_, grad_b_ = sess.run([grad_a, grad_b], feed_dict={a: 2, b: 3})\n\n        self.assertEqual(grad_a_, 3)\n        self.assertEqual(grad_b_, 2)\n\n    def test_div_grad(self):\n        graph = Graph()\n\n        a = graph.tensor()\n        b = graph.tensor()\n        c = a / b\n\n        grad_a, grad_b = graph.gradients(c, [a, b])\n\n        sess = Session(graph)\n        grad_a_, grad_b_ = sess.run([grad_a, grad_b], feed_dict={a: 2, b: 3})\n\n        self.assertAlmostEqual(grad_a_, 0.3333333)\n        self.assertAlmostEqual(grad_b_, -0.2222222)\n\n    def test_square_grad(self):\n        graph = Graph()\n\n        a = graph.tensor()\n        b = graph.square(a)\n\n        grad, = graph.gradients(b, [a])\n\n        sess = Session(graph)\n        grad_, = sess.run([grad], feed_dict={a: 6})\n\n        self.assertEqual(grad_, 12)\n\n    def test_sigmoid_grad(self):\n        graph = Graph()\n\n        a = graph.tensor()\n        b = graph.sigmoid(a)\n\n        grad, = graph.gradients(b, [a])\n\n        sess = Session(graph)\n        grad_, = sess.run([grad], feed_dict={a: 1})\n\n        self.assertAlmostEqual(grad_, 0.19661193)\n\n    def test_neg_grad(self):\n        graph = Graph()\n\n        a = graph.tensor()\n        b = -a\n\n        grad, = graph.gradients(b, [a])\n\n        sess = Session(graph)\n        grad_, = sess.run([grad], feed_dict={a: 1})\n\n        self.assertEqual(grad_, -1)\n\n    def test_dot_grad(self):\n        graph = Graph()\n\n        a = graph.tensor(np.array([0, 1, 2, 3]).reshape((1, -1)))\n        b = graph.tensor(np.array([0, 1, 2, 3]).reshape((-1, 1)))\n        c = graph.dot(a, b)\n\n        grad_a, grad_b, = graph.gradients(c, [a, b])\n\n        sess = Session(graph)\n        grad_a_, grad_b_ = sess.run([grad_a, grad_b])\n\n        self.assertTrue(np.array_equal(grad_a_, np.array([[0, 1, 2, 3]])))\n        self.assertTrue(np.array_equal(grad_b_, np.array([[0], [1], [2], [3]])))\n\n    def test_transpose_grad(self):\n        graph = Graph()\n\n        a = graph.tensor(np.array([[0, 1, 2, 3]]))\n        b = graph.transpose(a)\n\n        grad, = graph.gradients(b, [a])\n\n        sess = Session(graph)\n        grad_, = sess.run([grad])\n\n        self.assertEqual(grad_, 1)\n\n    def test_mean_grad(self):\n        graph = Graph()\n\n        a = graph.tensor(np.array([[0, 2, 4, 6]]))\n        b = graph.mean(a)\n\n        grad, = graph.gradients(b, [a])\n\n        sess = Session(graph)\n        grad_, = sess.run([grad])\n\n        # XXX: This is intentionally incorrect.\n        self.assertEqual(grad_, 1)\n\n    def test_expression_grad(self):\n        graph = Graph()\n\n        a = graph.tensor()\n        b = graph.tensor()\n\n        c = a + b\n        d = b + 1\n        e = c * d\n\n        de_da, de_db = graph.gradients(e, [a, b])\n\n        sess = Session(graph)\n\n        a_, b_, c_, d_, e_, de_da_, de_db_ = sess.run([a, b, c, d, e, de_da, de_db], feed_dict={a: 2, b: 1})\n\n        self.assertEqual(a_, 2)\n        self.assertEqual(b_, 1)\n        self.assertEqual(c_, 3)\n        self.assertEqual(d_, 2)\n        self.assertEqual(e_, 6)\n        self.assertEqual(de_da_, 2)\n        self.assertEqual(de_db_, 5)\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/test_ops.py,6,"b""from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport unittest\nimport numpy as np\n\nfrom graph import Graph\nfrom session import Session\n\nclass OpsTestCase(unittest.TestCase):\n\n    def test_add(self):\n        graph = Graph()\n\n        a = graph.tensor()\n        b = graph.tensor()\n        c = a + b\n\n        sess = Session(graph)\n\n        a_, b_, c_ = sess.run([a, b, c], feed_dict={a: 2, b: 1})\n\n        self.assertEqual(a_, 2)\n        self.assertEqual(b_, 1)\n        self.assertEqual(c_, 3)\n\n    def test_sub(self):\n        graph = Graph()\n\n        a = graph.tensor()\n        b = graph.tensor()\n        c = a - b\n\n        sess = Session(graph)\n\n        a_, b_, c_ = sess.run([a, b, c], feed_dict={a: 2, b: 3})\n\n        self.assertEqual(a_, 2)\n        self.assertEqual(b_, 3)\n        self.assertEqual(c_, -1)\n\n    def test_mul(self):\n        graph = Graph()\n\n        a = graph.tensor()\n        b = graph.tensor()\n        c = a * b\n\n        sess = Session(graph)\n\n        a_, b_, c_ = sess.run([a, b, c], feed_dict={a: 2, b: 3})\n\n        self.assertEqual(a_, 2)\n        self.assertEqual(b_, 3)\n        self.assertEqual(c_, 6)\n\n    def test_div(self):\n        graph = Graph()\n\n        a = graph.tensor()\n        b = graph.tensor()\n        c = a / b\n\n        sess = Session(graph)\n\n        a_, b_, c_ = sess.run([a, b, c], feed_dict={a: 6, b: 2})\n\n        self.assertEqual(a_, 6)\n        self.assertEqual(b_, 2)\n        self.assertEqual(c_, 3)\n\n    def test_square(self):\n        graph = Graph()\n\n        a = graph.tensor()\n        b = graph.square(a)\n\n        sess = Session(graph)\n\n        a_, b_ = sess.run([a, b], feed_dict={a: 3})\n\n        self.assertEqual(a_, 3)\n        self.assertEqual(b_, 9)\n\n    def test_neg(self):\n        graph = Graph()\n\n        a = graph.tensor()\n        b = graph.neg(a)\n\n        sess = Session(graph)\n\n        a_, b_ = sess.run([a, b], feed_dict={a: 1})\n\n        self.assertEqual(a_, 1)\n        self.assertEqual(b_, -1)\n\n    def test_sigmoid(self):\n        graph = Graph()\n\n        a = graph.tensor()\n        b = graph.sigmoid(a)\n\n        sess = Session(graph)\n\n        a_, b_ = sess.run([a, b], feed_dict={a: 1})\n\n        self.assertEqual(a_, 1)\n        self.assertAlmostEqual(b_, 0.731058579)\n\n    def test_dot(self):\n        graph = Graph()\n\n        a = graph.tensor(np.array([0, 1, 2, 3]).reshape((1, -1)))\n        b = graph.tensor(np.array([0, 1, 2, 3]).reshape((-1, 1)))\n        c = graph.dot(a, b)\n\n        sess = Session(graph)\n\n        c_, = sess.run([c])\n\n        self.assertTrue(np.array_equal(c_, [[14]]))\n\n    def test_transpose(self):\n        graph = Graph()\n\n        a = graph.tensor(np.array([[0, 1, 2, 3]]))\n        b = graph.transpose(a)\n\n        sess = Session(graph)\n\n        b_, = sess.run([b])\n\n        self.assertTrue(np.array_equal(b_, np.array([[0], [1], [2], [3]])))\n\n    def test_mean(self):\n        graph = Graph()\n\n        a = graph.tensor(np.array([[0, 2, 4, 6]]))\n        b = graph.mean(a)\n\n        sess = Session(graph)\n\n        b_, = sess.run([b])\n\n        self.assertEqual(b_, 3)\n\n    def test_assign(self):\n        graph = Graph()\n\n        a = graph.tensor(1)\n        increment_op = graph.assign(a, a + 1)\n\n        sess = Session(graph)\n\n        a0, = sess.run([a])\n        sess.run([increment_op])\n        a1, = sess.run([a])\n\n        self.assertEqual(a0, 1)\n        self.assertEqual(a1, 2)\n\nif __name__ == '__main__':\n    unittest.main()\n"""
