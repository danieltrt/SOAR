file_path,api_count,code
four_layer_network.py,21,"b'import numpy as np \nimport math\nfrom sklearn import datasets\n\ndef relu(X):\n    return np.maximum(X, 0)\n\ndef relu_derivative(X):\n    return 1. * (X > 0)\n\ndef build_model(X,hidden_nodes,output_dim=2):\n    model = {}\n    input_dim = X.shape[1]\n    model[\'W1\'] = np.random.randn(input_dim, hidden_nodes) / np.sqrt(input_dim)\n    model[\'b1\'] = np.zeros((1, hidden_nodes))\n    model[\'W2\'] = np.random.randn(hidden_nodes, hidden_nodes) / np.sqrt(hidden_nodes)\n    model[\'b2\'] = np.zeros((1, hidden_nodes))\n    model[\'W3\'] = np.random.randn(hidden_nodes, output_dim) / np.sqrt(hidden_nodes)\n    model[\'b3\'] = np.zeros((1, output_dim))\n    return model\n\ndef feed_forward(model, x):\n    W1, b1, W2, b2, W3, b3 = model[\'W1\'], model[\'b1\'], model[\'W2\'], model[\'b2\'], model[\'W3\'], model[\'b3\']\n    # Forward propagation\n    z1 = x.dot(W1) + b1\n    #a1 = np.tanh(z1)\n    a1 = relu(z1)\n    z2 = a1.dot(W2) + b2\n    a2 = relu(z2)\n    z3 = a2.dot(W3) + b3\n    exp_scores = np.exp(z3)\n    out = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    return z1, a1, z2, a2, z3, out\n\ndef calculate_loss(model,X,y,reg_lambda):\n    num_examples = X.shape[0]\n    W1, b1, W2, b2, W3, b3 = model[\'W1\'], model[\'b1\'], model[\'W2\'], model[\'b2\'], model[\'W3\'], model[\'b3\']\n    # Forward propagation to calculate our predictions\n    z1, a1, z2, a2, z3, out = feed_forward(model, X)\n    probs = out / np.sum(out, axis=1, keepdims=True)\n    # Calculating the loss\n    corect_logprobs = -np.log(probs[range(num_examples), y])\n    loss = np.sum(corect_logprobs)\n    # Add regulatization term to loss (optional)\n    loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\n    return 1./num_examples * loss\n\ndef backprop(X,y,model,z1,a1,z2,a2,z3,output,reg_lambda):\n    delta3 = output\n    delta3[range(X.shape[0]), y] -= 1  #yhat - y\n    dW3 = (a2.T).dot(delta3)\n    db3 = np.sum(delta3, axis=0, keepdims=True)\n    delta2 = delta3.dot(model[\'W3\'].T) * relu_derivative(a2) #if ReLU\n    dW2 = np.dot(a1.T, delta2)\n    db2 = np.sum(delta2, axis=0)\n    #delta2 = delta3.dot(model[\'W2\'].T) * (1 - np.power(a1, 2)) #if tanh\n    delta1 = delta2.dot(model[\'W2\'].T) * relu_derivative(a1) #if ReLU\n    dW1 = np.dot(X.T, delta1)\n    db1 = np.sum(delta1, axis=0)\n    # Add regularization terms\n    dW3 += reg_lambda * model[\'W3\']\n    dW2 += reg_lambda * model[\'W2\']\n    dW1 += reg_lambda * model[\'W1\']\n    return dW1, dW2, dW3, db1, db2, db3\n\n\ndef train(model, X, y, num_passes=10000, reg_lambda = .1, learning_rate=0.1):\n    # Batch gradient descent\n    done = False\n    previous_loss = float(\'inf\')\n    i = 0\n    losses = []\n    while done == False:  #comment out while performance testing\n    #while i < 1500:\n        #feed forward\n        z1,a1,z2,a2,z3,output = feed_forward(model, X)\n        #backpropagation\n        dW1, dW2, dW3, db1, db2, db3 = backprop(X,y,model,z1,a1,z2,a2,z3,output,reg_lambda)\n        #update weights and biases\n        model[\'W1\'] -= learning_rate * dW1\n        model[\'b1\'] -= learning_rate * db1\n        model[\'W2\'] -= learning_rate * dW2\n        model[\'b2\'] -= learning_rate * db2\n        model[\'W3\'] -= learning_rate * dW3\n        model[\'b3\'] -= learning_rate * db3\n        if i % 1000 == 0:\n            loss = calculate_loss(model, X, y, reg_lambda)\n            losses.append(loss)\n            print ""Loss after iteration %i: %f"" %(i, loss)  #uncomment once testing finished, return mod val to 1000\n            if (previous_loss-loss)/previous_loss < 0.01:\n                done = True\n                #print i\n            previous_loss = loss\n        i += 1\n    return model, losses\n\ndef main():\n    #toy dataset\n    X, y = datasets.make_moons(16, noise=0.10)\n    num_examples = len(X) # training set size\n    nn_input_dim = 2 # input layer dimensionality\n    nn_output_dim = 2 # output layer dimensionality \n    learning_rate = 0.01 # learning rate for gradient descent\n    reg_lambda = 0.01 # regularization strength\n    model = build_model(X,20,2)\n    model, losses = train(model,X, y, reg_lambda=reg_lambda, learning_rate=learning_rate)\n    output = feed_forward(model, X)\n    preds = np.argmax(output[3], axis=1)\n\nif __name__ == ""__main__"":\n    main()\n'"
tests.py,5,"b'from three_layer_network import build_model, relu, relu_derivative, feed_forward, \\\n\t\t\t\t\t\t\t\tcalculate_loss, backprop, train\n#from three_layer_network import *\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport numpy as np\n\n""""""\nto reproduce tests, modify the three_layer_network.py file by commenting out \n\'while done == True\', and uncommenting \'while i < 150\', and then by changing \n\'if i % 1000 == 0\' to \'if i % 150 == 0\'\n""""""\n\n\ndef num_observations():\n\tobs_values = [10, 100, 1000]\n\tnn_input_dim = 2 # input layer dimensionality\n\tnn_output_dim = 2 # output layer dimensionality \n\tlearning_rate = 0.01 # learning rate for gradient descent\n\treg_lambda = 0.01 # regularization strength\n\tlosses_store = []\n\tfor i in obs_values:\n\t\tX, y = datasets.make_moons(i, noise=0.1)\n\t\tnum_examples = len(X) # training set size\n\t\tmodel = build_model(X,32,2)\n\t\tmodel, losses = train(model,X, y, reg_lambda=reg_lambda, learning_rate=learning_rate)\n\t\tlosses_store.append(losses)\n\t\tprint losses\n\tx = np.linspace(0,145,30)\n\tfor i in range(len(losses_store)):\n\t\tlab = \'n_observations = \' + str(obs_values[i])\n\t\tplt.plot(x,losses_store[i],label=lab)\n\tplt.legend()\n\tplt.show()\n\ndef noise():\n\tnoise_values = [0.01, 0.1, 0.2, 0.3, 0.4]\n\tnn_input_dim = 2 # input layer dimensionality\n\tnn_output_dim = 2 # output layer dimensionality \n\tlearning_rate = 0.01 # learning rate for gradient descent\n\treg_lambda = 0.01 # regularization strength\n\tlosses_store = []\n\tfor i in noise_values:\n\t\tX, y = datasets.make_moons(200, noise=i)\n\t\tnum_examples = len(X) # training set size\n\t\tmodel = build_model(X,32,2)\n\t\tmodel, losses = train(model,X, y, reg_lambda=reg_lambda, learning_rate=learning_rate)\n\t\tlosses_store.append(losses)\n\t\tprint losses\n\tx = np.linspace(0,145,30)\n\tfor i in range(len(losses_store)):\n\t\tlab = \'noise_value = \' + str(noise_values[i])\n\t\tplt.plot(x,losses_store[i],label=lab)\n\tplt.legend()\n\tplt.show()\n\ndef reg():\n\treg_values = [0.00, 0.01, 0.1, 0.2, 0.3]\n\tnn_input_dim = 2 # input layer dimensionality\n\tnn_output_dim = 2 # output layer dimensionality \n\tlearning_rate = 0.01 # learning rate for gradient descent\n\tlosses_store = []\n\tfor i in reg_values:\n\t\treg_lambda = i # regularization strength\n\t\tX, y = datasets.make_moons(200, noise=0.2)\n\t\tnum_examples = len(X) # training set size\n\t\tmodel = build_model(X,32,2)\n\t\tmodel, losses = train(model,X, y, reg_lambda=reg_lambda, learning_rate=learning_rate)\n\t\tlosses_store.append(losses)\n\t\tprint losses\n\tx = np.linspace(0,145,30)\n\tfor i in range(len(losses_store)):\n\t\tlab = \'regularization_value = \' + str(reg_values[i])\n\t\tplt.plot(x,losses_store[i],label=lab)\n\tplt.legend()\n\tplt.show()\n\n\ndef lr():\n\tlr_values = [0.001, 0.01, 0.05]\n\tnn_input_dim = 2 # input layer dimensionality\n\tnn_output_dim = 2 # output layer dimensionality \n\treg_lambda = .01 # regularization strength\n\tlosses_store = []\n\tfor i in lr_values:\n\t\tlearning_rate = i\n\t\tX, y = datasets.make_moons(200, noise=0.2)\n\t\tnum_examples = len(X) # training set size\n\t\tmodel = build_model(X,32,2)\n\t\tmodel, losses = train(model,X, y, reg_lambda=reg_lambda, learning_rate=learning_rate)\n\t\tlosses_store.append(losses)\n\t\tprint losses\n\tx = np.linspace(0,145,30)\n\tfor i in range(len(losses_store)):\n\t\tlab = \'learning rate = \' + str(lr_values[i])\n\t\tplt.plot(x,losses_store[i],label=lab)\n\tplt.legend()\n\tplt.show()\n\ndef test_num_nodes():\n\tX, y = datasets.make_moons(400, noise=0.2)\n\tnum_examples = len(X) # training set size\n\tnn_input_dim = 2 # input layer dimensionality\n\tnn_output_dim = 2 # output layer dimensionality \n\tlearning_rate = 0.01 # learning rate for gradient descent\n\treg_lambda = 0.01 # regularization strength\n\tnode_vals = [4,8,16,32,64,128]\n\tlosses_store = []\n\tfor val in node_vals:\n\t\tmodel = build_model(X,val,2)\n\t\tmodel, losses = train(model,X, y, reg_lambda=reg_lambda, learning_rate=learning_rate)\n\t\tlosses_store.append(losses)\n\t\tprint losses\n\tx = np.linspace(0,145,30)\n\tfor i in range(len(losses_store)):\n\t\tlab = \'n_nodes = \' + str(node_vals[i])\n\t\tplt.plot(x,losses_store[i],label=lab)\n\tplt.legend()\n\tplt.show()\n\nprint ""number of observations:""\nnum_observations()\nprint \'noise:\'\nnoise()\nprint \'regularization:\'\nreg()\nprint \'learning rate:\'\nlr()\nprint \'hidden nodes:\'\ntest_num_nodes()'"
three_layer_network.py,17,"b'import numpy as np \nimport math\nfrom sklearn import datasets\n\ndef relu(X):\n\treturn np.maximum(X, 0)\n\ndef relu_derivative(X):\n\treturn 1. * (X > 0)\n\ndef build_model(X,hidden_nodes,output_dim=2):\n    model = {}\n    input_dim = X.shape[1]\n    model[\'W1\'] = np.random.randn(input_dim, hidden_nodes) / np.sqrt(input_dim)\n    model[\'b1\'] = np.zeros((1, hidden_nodes))\n    model[\'W2\'] = np.random.randn(hidden_nodes, output_dim) / np.sqrt(hidden_nodes)\n    model[\'b2\'] = np.zeros((1, output_dim))\n    return model\n\ndef feed_forward(model, x):\n    W1, b1, W2, b2 = model[\'W1\'], model[\'b1\'], model[\'W2\'], model[\'b2\']\n    # Forward propagation\n    z1 = x.dot(W1) + b1\n    #a1 = np.tanh(z1)\n    a1 = relu(z1)\n    z2 = a1.dot(W2) + b2\n    exp_scores = np.exp(z2)\n    out = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    return z1, a1, z2, out\n\ndef calculate_loss(model,X,y,reg_lambda):\n    num_examples = X.shape[0]\n    W1, b1, W2, b2 = model[\'W1\'], model[\'b1\'], model[\'W2\'], model[\'b2\']\n    # Forward propagation to calculate our predictions\n    z1, a1, z2, out = feed_forward(model, X)\n    probs = out / np.sum(out, axis=1, keepdims=True)\n    # Calculating the loss\n    corect_logprobs = -np.log(probs[range(num_examples), y])\n    loss = np.sum(corect_logprobs)\n    # Add regulatization term to loss (optional)\n    loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n    return 1./num_examples * loss\n\ndef backprop(X,y,model,z1,a1,z2,output,reg_lambda):\n    delta3 = output\n    delta3[range(X.shape[0]), y] -= 1  #yhat - y\n    dW2 = (a1.T).dot(delta3)\n    db2 = np.sum(delta3, axis=0, keepdims=True)\n    #delta2 = delta3.dot(model[\'W2\'].T) * (1 - np.power(a1, 2)) #if tanh\n    delta2 = delta3.dot(model[\'W2\'].T) * relu_derivative(a1) #if ReLU\n    dW1 = np.dot(X.T, delta2)\n    db1 = np.sum(delta2, axis=0)\n    # Add regularization terms\n    dW2 += reg_lambda * model[\'W2\']\n    dW1 += reg_lambda * model[\'W1\']\n    return dW1, dW2, db1, db2\n\n\ndef train(model, X, y, num_passes=10000, reg_lambda = .1, learning_rate=0.1):\n    # Batch gradient descent\n    done = False\n    previous_loss = float(\'inf\')\n    i = 0\n    losses = []\n    while done == False:  #comment out while performance testing\n    #while i < 1500:\n    \t#feed forward\n        z1,a1,z2,output = feed_forward(model, X)\n        #backpropagation\n        dW1, dW2, db1, db2 = backprop(X,y,model,z1,a1,z2,output,reg_lambda)\n        #update weights and biases\n        model[\'W1\'] -= learning_rate * dW1\n        model[\'b1\'] -= learning_rate * db1\n        model[\'W2\'] -= learning_rate * dW2\n        model[\'b2\'] -= learning_rate * db2\n        if i % 1000 == 0:\n        \tloss = calculate_loss(model, X, y, reg_lambda)\n        \tlosses.append(loss)\n        \tprint ""Loss after iteration %i: %f"" %(i, loss)  #uncomment once testing finished, return mod val to 1000\n        \tif (previous_loss-loss)/previous_loss < 0.01:\n        \t\tdone = True\n        \t\t#print i\n        \tprevious_loss = loss\n        i += 1\n    return model, losses\n\ndef main():\n\t#toy dataset\n\tX, y = datasets.make_moons(16, noise=0.10)\n\tnum_examples = len(X) # training set size\n\tnn_input_dim = 2 # input layer dimensionality\n\tnn_output_dim = 2 # output layer dimensionality \n\tlearning_rate = 0.01 # learning rate for gradient descent\n\treg_lambda = 0.01 # regularization strength\n\tmodel = build_model(X,20,2)\n\tmodel, losses = train(model,X, y, reg_lambda=reg_lambda, learning_rate=learning_rate)\n\toutput = feed_forward(model, X)\n\tpreds = np.argmax(output[3], axis=1)\n\nif __name__ == ""__main__"":\n    main()\n'"
