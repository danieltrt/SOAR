file_path,api_count,code
main_Seb.py,0,"b'import numpy as np \nimport pandas as pd\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier #imports the dtree classifier\nfrom sklearn.metrics import accuracy_score #using accuracy to rate the model\nfrom sklearn import tree\n\ndf=pd.read_csv(\'data/training-2016-10-01-2016-12-31.csv\') #Reads the dataset... Used the same one as John\n\n# #Data stuff...commented because didn\'t find it useful...\n# print(\'Dataset Shape::\',df.shape) #prints the shape of the dataset...\n# print(\'Dataset Length::\', len(df)) #prints the length of the dataset...\n\n# print(\'Dataset::\' , df.head()) #prints the column heads...\n\n# print(df.dtypes) #Prints the type of data...\n\n#may have to redo the part below...not the prettiest but it works so +1 lol\ndata = df.dropna(subset=[\'post_pos\',\'offset\',\'post\',\'speed\',\'horse_win\',\'horse_wps\',\'horse_roi\',\'driver_win\',\'driver_wps\',\'driver_roi\',\'trainer_win\',\'trainer_roi\',\'min_races\',\'previous_break\',\'days_since\',\'same_track\',\'same_driver\',\'last_race_res\',\'last_race_wps\',\'last_three_races\',\'purse\',\'finish_pos\'])\ndata[\'finish_pos\'] = data[\'finish_pos\'].map(lambda x: 1 if x >= 3 else 0) #used the same function as John...\n\nX = data[[\'post_pos\',\'offset\',\'post\',\'speed\',\'horse_win\',\'horse_wps\',\'horse_roi\',\'driver_win\',\'driver_wps\',\'driver_roi\',\'trainer_win\',\'trainer_roi\',\'min_races\',\'previous_break\',\'days_since\',\'same_track\',\'same_driver\',\'last_race_res\',\'last_race_wps\',\'last_three_races\',\'purse\']].values #X is everything else besides the first two columns and the last one, removed the -1\nY= data[[\'finish_pos\']].values#Y is the finishing Postion... \n\nX_train,X_test,y_train,y_test=train_test_split(X,Y, test_size=0.30,train_size=0.70, random_state=1) #split 30:70...\n\nclf_gini=DecisionTreeClassifier(criterion=""gini"", random_state = 100, max_depth=10, min_samples_leaf=5) #dtree with criterion gini index...\n\nclf_gini.fit(X_train, y_train)\n\nclf_entropy=DecisionTreeClassifier(criterion=""entropy"",random_state=100,max_depth=10,min_samples_leaf=5)#dtree with criterion information gain...\nclf_entropy.fit(X_train, y_train)\n\ny_pred = clf_gini.predict(X_test)#predicition for the gini index...\ny_pred\n\ny_pred_en = clf_entropy.predict(X_test)#prediction for the information gain...\ny_pred_en\n\nprint(""Accuracy is"", accuracy_score(y_test,y_pred)*100) # Prints accuracy score with criterion as gini index...\nprint(""Accuracy is"", accuracy_score(y_test,y_pred_en)*100)#Prints accuracy score with criterion as information gain...\n'"
main_bree.py,3,"b'import numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\n# read in the data file into a pandas dataframe\ndata = pd.read_csv(\'data/training-2016-10-01-2016-12-31.csv\')\n\n# drop some features\ndef remove_strings(df):\n    df= df.drop([\'row_id\', \'entry_id\', \'morning_line\'], axis=1)\n    return df\n\n\n# our y label will be finish position, if it is in the top three (WPS)\n# this function maps top three to 1 and anything other finish position to 0\ndef alter_y_label(df):\n    df[\'finish_pos\'] = df[\'finish_pos\'].apply(lambda x: 1 if x >= 3 else 0)\n    return df\n\n# Apply functions to the data\ndata = remove_strings(data)\ndata = alter_y_label(data)\n\n# Sort dataframe into X and y variables\nX = data.drop([\'finish_pos\'], axis=1)\ny = data[\'finish_pos\']\n\n# Transform X and y variables into np arrays\nX_all = np.array(X.iloc[0:95518, 0:25].values)\ny_all = np.array(data[\'finish_pos\'])\n\n# Divide into train, dev, and test sets\nX_train, X_dev_test, y_train, y_dev_test = train_test_split(X_all, y_all, test_size=0.30, random_state=1)\ny_temp = np.reshape(y_dev_test, len(y_dev_test))\nX_test, X_dev, y_test, y_dev = train_test_split(X_dev_test, y_temp, test_size=0.50, random_state=1)\n\n\n""""""\nuse gridsearch for best value for k\nfor x in range(3,11):\n    for y in range(1,3):\n        knn = KNeighborsClassifier(n_neighbors=x,p=y)\n        knn.fit(X_train, y_train)\n\n        # predict on development\n        predictions = knn.predict(X_dev)\n        expectations = y_dev\n\n        # print performance metric score\n        print(metrics.f1_score(expectations, predictions))\n""""""\n\n#found best hyperparameter value to be 7.\nknn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train, y_train)\n\n# predict on development\npredictions = knn.predict(X_test)\nexpectations = y_test\n\n# print performance metric score\nprint(metrics.accuracy_score(expectations, predictions))'"
main_john.py,4,"b'# Imported the pandas and numpy\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVC\n# Importing the sklearn library and its submodules\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn import preprocessing\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import svm\nfrom sklearn.metrics import f1_score\n# Gets the data from the data folder and convert to datframes\ndf = pd.read_csv(\'data/training-2016-10-01-2016-12-31.csv\')\n# Gets the data we would like to use on each column\ndata = df.dropna(subset=[\'same_track\',\'last_race_res\',\'finish_pos\'])\n# Maps the finish_pos if its metals within top 3 then 1 else 0\ndata[\'finish_pos\'] = data[\'finish_pos\'].map(lambda x: 1 if x >= 3 else 0)\n# Gets the y labels of the finish_pos\ny_sk = data[[\'finish_pos\']].values\n# Gets the data for training \'same_track\',\'last_race_res\'\nX_sk = data[[\'same_track\',\'last_race_res\']].values\n# Gets the text/dev and training dataset / y_labels which will be splitted into dev/test\nX_train, X_test_dev, y_train, y_test_dev = train_test_split(X_sk, y_sk, test_size=0.30,train_size=0.70, random_state=1)\n# Reshapes the y_labels\ny_test_dev_reshape = np.reshape(y_test_dev, len(y_test_dev))\n# Takes X_test_dev and spliting into dev and test X/y_labels\nX_dev, X_test, y_dev, y_test = train_test_split(X_test_dev, y_test_dev_reshape, test_size=0.50,train_size=0.50, random_state=1)\n# Reshapes the y_labels\ny_train_reshape = np.reshape(y_train, len(y_train))\ny_test_reshape = np.reshape(y_test, len(y_test))\ny_dev_reshape = np.reshape(y_dev, len(y_dev))\nprint(""Tuning against the dev dataset/y_label"")\nparam_grid = [\n {\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']},\n {\'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']},\n]\nclfdev = svm.SVC(kernel=\'poly\',C=2.0,random_state=2)\nclfdev.fit(X_train, y_train_reshape)\nc1dev = clfdev.score(X=X_dev,y=y_dev_reshape)\nprint(""Dev Accuracy: {0:.0f}%"".format(c1dev * 100))\nprint(""Testing against best model and hyberparameters"")\nclf = svm.SVC(kernel=\'poly\',C=1.0,random_state=1)\n#clf = GridSearchCV(SVC(), param_grid, cv=5)\nclf.fit(X_train, y_train_reshape)\n#print(""Best parameters set found on development set:"")\n#print(clf.best_params_)\nc1 = clf.score(X=X_test,y=y_test_reshape)\n#y_pred = clf.predict(X_test)\n#print(f1_score(y_test_reshape, y_pred, average=\'micro\'))\nprint(""Test Accuracy: {0:.0f}%"".format(c1 * 100))\n'"
main_noel.py,0,b''
